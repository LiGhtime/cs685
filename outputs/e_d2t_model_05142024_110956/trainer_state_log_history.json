[
    {
        "loss": 7.1789,
        "grad_norm": 9.250138282775879,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00013333333333333334,
        "step": 1
    },
    {
        "loss": 8.076,
        "grad_norm": 9.98422622680664,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.0002666666666666667,
        "step": 2
    },
    {
        "loss": 5.267,
        "grad_norm": 4.747072696685791,
        "learning_rate": 2.4e-05,
        "epoch": 0.0004,
        "step": 3
    },
    {
        "loss": 6.5692,
        "grad_norm": 9.194737434387207,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0005333333333333334,
        "step": 4
    },
    {
        "loss": 5.1156,
        "grad_norm": 5.005303382873535,
        "learning_rate": 4e-05,
        "epoch": 0.0006666666666666666,
        "step": 5
    },
    {
        "loss": 4.6877,
        "grad_norm": 4.331012725830078,
        "learning_rate": 4.8e-05,
        "epoch": 0.0008,
        "step": 6
    },
    {
        "loss": 4.7748,
        "grad_norm": 4.45315408706665,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0009333333333333333,
        "step": 7
    },
    {
        "loss": 4.7913,
        "grad_norm": 5.042860984802246,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0010666666666666667,
        "step": 8
    },
    {
        "loss": 6.5469,
        "grad_norm": Infinity,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0012,
        "step": 9
    },
    {
        "loss": 4.0687,
        "grad_norm": 2.95993971824646,
        "learning_rate": 7.2e-05,
        "epoch": 0.0013333333333333333,
        "step": 10
    },
    {
        "loss": 4.1417,
        "grad_norm": 3.652667284011841,
        "learning_rate": 8e-05,
        "epoch": 0.0014666666666666667,
        "step": 11
    },
    {
        "loss": 7.4352,
        "grad_norm": Infinity,
        "learning_rate": 8e-05,
        "epoch": 0.0016,
        "step": 12
    },
    {
        "loss": 4.0245,
        "grad_norm": 3.9300692081451416,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0017333333333333333,
        "step": 13
    },
    {
        "loss": 5.8175,
        "grad_norm": 11.718360900878906,
        "learning_rate": 9.6e-05,
        "epoch": 0.0018666666666666666,
        "step": 14
    },
    {
        "loss": 4.9497,
        "grad_norm": 7.76549768447876,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.002,
        "step": 15
    },
    {
        "loss": 4.1305,
        "grad_norm": 6.339547157287598,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0021333333333333334,
        "step": 16
    },
    {
        "loss": 3.6326,
        "grad_norm": 4.210788249969482,
        "learning_rate": 0.00012,
        "epoch": 0.002266666666666667,
        "step": 17
    },
    {
        "loss": 4.0704,
        "grad_norm": 6.609633445739746,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0024,
        "step": 18
    },
    {
        "loss": 3.5528,
        "grad_norm": 3.1234328746795654,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.002533333333333333,
        "step": 19
    },
    {
        "loss": 3.1753,
        "grad_norm": 3.0161283016204834,
        "learning_rate": 0.000144,
        "epoch": 0.0026666666666666666,
        "step": 20
    },
    {
        "loss": 3.4227,
        "grad_norm": 4.205460071563721,
        "learning_rate": 0.000152,
        "epoch": 0.0028,
        "step": 21
    },
    {
        "loss": 4.6945,
        "grad_norm": 4.313234329223633,
        "learning_rate": 0.00016,
        "epoch": 0.0029333333333333334,
        "step": 22
    },
    {
        "loss": 3.589,
        "grad_norm": 3.4238481521606445,
        "learning_rate": 0.000168,
        "epoch": 0.0030666666666666668,
        "step": 23
    },
    {
        "loss": 3.7147,
        "grad_norm": 5.207477569580078,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0032,
        "step": 24
    },
    {
        "loss": 3.6356,
        "grad_norm": 7.147646427154541,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.0033333333333333335,
        "step": 25
    },
    {
        "loss": 3.2473,
        "grad_norm": 7.13840913772583,
        "learning_rate": 0.000192,
        "epoch": 0.0034666666666666665,
        "step": 26
    },
    {
        "loss": 2.7523,
        "grad_norm": 5.5821990966796875,
        "learning_rate": 0.0002,
        "epoch": 0.0036,
        "step": 27
    },
    {
        "loss": 3.3548,
        "grad_norm": 2.7872061729431152,
        "learning_rate": 0.00019999998019482965,
        "epoch": 0.0037333333333333333,
        "step": 28
    },
    {
        "loss": 3.0934,
        "grad_norm": 3.620872974395752,
        "learning_rate": 0.00019999992077932637,
        "epoch": 0.0038666666666666667,
        "step": 29
    },
    {
        "loss": 3.245,
        "grad_norm": 10.335738182067871,
        "learning_rate": 0.00019999982175351373,
        "epoch": 0.004,
        "step": 30
    },
    {
        "loss": 3.1966,
        "grad_norm": 6.336019039154053,
        "learning_rate": 0.00019999968311743097,
        "epoch": 0.0041333333333333335,
        "step": 31
    },
    {
        "loss": 2.9928,
        "grad_norm": 2.763995885848999,
        "learning_rate": 0.00019999950487113299,
        "epoch": 0.004266666666666667,
        "step": 32
    },
    {
        "loss": 3.1298,
        "grad_norm": 5.233175754547119,
        "learning_rate": 0.00019999928701469038,
        "epoch": 0.0044,
        "step": 33
    },
    {
        "loss": 2.8952,
        "grad_norm": 7.415555000305176,
        "learning_rate": 0.00019999902954818946,
        "epoch": 0.004533333333333334,
        "step": 34
    },
    {
        "loss": 2.7241,
        "grad_norm": 8.181571960449219,
        "learning_rate": 0.00019999873247173214,
        "epoch": 0.004666666666666667,
        "step": 35
    },
    {
        "loss": 3.1911,
        "grad_norm": 5.356176853179932,
        "learning_rate": 0.0001999983957854362,
        "epoch": 0.0048,
        "step": 36
    },
    {
        "loss": 3.1662,
        "grad_norm": 2.311763048171997,
        "learning_rate": 0.00019999801948943495,
        "epoch": 0.004933333333333333,
        "step": 37
    },
    {
        "loss": 2.9974,
        "grad_norm": 3.753873825073242,
        "learning_rate": 0.00019999760358387745,
        "epoch": 0.005066666666666666,
        "step": 38
    },
    {
        "loss": 2.8852,
        "grad_norm": 2.8274080753326416,
        "learning_rate": 0.00019999714806892845,
        "epoch": 0.0052,
        "step": 39
    },
    {
        "loss": 3.1301,
        "grad_norm": 2.0708649158477783,
        "learning_rate": 0.00019999665294476832,
        "epoch": 0.005333333333333333,
        "step": 40
    },
    {
        "loss": 2.3565,
        "grad_norm": 7.1385297775268555,
        "learning_rate": 0.00019999611821159327,
        "epoch": 0.0054666666666666665,
        "step": 41
    },
    {
        "loss": 3.0287,
        "grad_norm": 3.044475555419922,
        "learning_rate": 0.00019999554386961505,
        "epoch": 0.0056,
        "step": 42
    },
    {
        "loss": 3.1164,
        "grad_norm": 3.057774782180786,
        "learning_rate": 0.00019999492991906116,
        "epoch": 0.005733333333333333,
        "step": 43
    },
    {
        "loss": 2.1929,
        "grad_norm": 1.5863895416259766,
        "learning_rate": 0.00019999427636017484,
        "epoch": 0.005866666666666667,
        "step": 44
    },
    {
        "loss": 2.8488,
        "grad_norm": 7.41654634475708,
        "learning_rate": 0.00019999358319321492,
        "epoch": 0.006,
        "step": 45
    },
    {
        "loss": 2.5165,
        "grad_norm": 1.9489693641662598,
        "learning_rate": 0.00019999285041845597,
        "epoch": 0.0061333333333333335,
        "step": 46
    },
    {
        "loss": 2.9437,
        "grad_norm": 3.1769330501556396,
        "learning_rate": 0.00019999207803618823,
        "epoch": 0.006266666666666667,
        "step": 47
    },
    {
        "loss": 3.4633,
        "grad_norm": 2.9619932174682617,
        "learning_rate": 0.00019999126604671773,
        "epoch": 0.0064,
        "step": 48
    },
    {
        "loss": 2.3905,
        "grad_norm": 2.3487298488616943,
        "learning_rate": 0.00019999041445036597,
        "epoch": 0.006533333333333334,
        "step": 49
    },
    {
        "loss": 2.5199,
        "grad_norm": 3.794624090194702,
        "learning_rate": 0.00019998952324747036,
        "epoch": 0.006666666666666667,
        "step": 50
    },
    {
        "loss": 2.9468,
        "grad_norm": 4.056763648986816,
        "learning_rate": 0.00019998859243838387,
        "epoch": 0.0068,
        "step": 51
    },
    {
        "loss": 1.8943,
        "grad_norm": 1.649922251701355,
        "learning_rate": 0.00019998762202347528,
        "epoch": 0.006933333333333333,
        "step": 52
    },
    {
        "loss": 2.7473,
        "grad_norm": 2.760343551635742,
        "learning_rate": 0.00019998661200312884,
        "epoch": 0.007066666666666666,
        "step": 53
    },
    {
        "loss": 2.1659,
        "grad_norm": 3.8308987617492676,
        "learning_rate": 0.0001999855623777447,
        "epoch": 0.0072,
        "step": 54
    },
    {
        "loss": 2.1317,
        "grad_norm": 3.3332605361938477,
        "learning_rate": 0.00019998447314773863,
        "epoch": 0.007333333333333333,
        "step": 55
    },
    {
        "loss": 2.4123,
        "grad_norm": 3.0945236682891846,
        "learning_rate": 0.00019998334431354205,
        "epoch": 0.007466666666666667,
        "step": 56
    },
    {
        "loss": 1.4299,
        "grad_norm": 5.44379186630249,
        "learning_rate": 0.00019998217587560208,
        "epoch": 0.0076,
        "step": 57
    },
    {
        "loss": 2.6505,
        "grad_norm": 2.9598641395568848,
        "learning_rate": 0.0001999809678343816,
        "epoch": 0.007733333333333333,
        "step": 58
    },
    {
        "loss": 3.0103,
        "grad_norm": 3.0233404636383057,
        "learning_rate": 0.00019997972019035904,
        "epoch": 0.007866666666666666,
        "step": 59
    },
    {
        "loss": 2.3657,
        "grad_norm": 3.8605172634124756,
        "learning_rate": 0.00019997843294402868,
        "epoch": 0.008,
        "step": 60
    },
    {
        "loss": 2.6988,
        "grad_norm": 2.187222719192505,
        "learning_rate": 0.00019997710609590038,
        "epoch": 0.008133333333333333,
        "step": 61
    },
    {
        "loss": 2.671,
        "grad_norm": 3.4954023361206055,
        "learning_rate": 0.00019997573964649962,
        "epoch": 0.008266666666666667,
        "step": 62
    },
    {
        "loss": 2.5222,
        "grad_norm": 2.6427104473114014,
        "learning_rate": 0.0001999743335963678,
        "epoch": 0.0084,
        "step": 63
    },
    {
        "loss": 3.2747,
        "grad_norm": 2.4452810287475586,
        "learning_rate": 0.00019997288794606176,
        "epoch": 0.008533333333333334,
        "step": 64
    },
    {
        "loss": 1.9944,
        "grad_norm": 5.108831405639648,
        "learning_rate": 0.00019997140269615416,
        "epoch": 0.008666666666666666,
        "step": 65
    },
    {
        "loss": 2.6206,
        "grad_norm": 3.414809465408325,
        "learning_rate": 0.00019996987784723328,
        "epoch": 0.0088,
        "step": 66
    },
    {
        "loss": 2.9223,
        "grad_norm": 2.9004178047180176,
        "learning_rate": 0.00019996831339990316,
        "epoch": 0.008933333333333333,
        "step": 67
    },
    {
        "loss": 2.8092,
        "grad_norm": 4.142261505126953,
        "learning_rate": 0.00019996670935478348,
        "epoch": 0.009066666666666667,
        "step": 68
    },
    {
        "loss": 2.0947,
        "grad_norm": 3.35530424118042,
        "learning_rate": 0.0001999650657125096,
        "epoch": 0.0092,
        "step": 69
    },
    {
        "loss": 1.8853,
        "grad_norm": 5.593930244445801,
        "learning_rate": 0.00019996338247373253,
        "epoch": 0.009333333333333334,
        "step": 70
    },
    {
        "loss": 2.8503,
        "grad_norm": 6.728113174438477,
        "learning_rate": 0.0001999616596391191,
        "epoch": 0.009466666666666667,
        "step": 71
    },
    {
        "loss": 2.1602,
        "grad_norm": 3.8799054622650146,
        "learning_rate": 0.00019995989720935164,
        "epoch": 0.0096,
        "step": 72
    },
    {
        "loss": 2.7976,
        "grad_norm": 2.387634038925171,
        "learning_rate": 0.0001999580951851283,
        "epoch": 0.009733333333333333,
        "step": 73
    },
    {
        "loss": 2.7233,
        "grad_norm": 2.9982008934020996,
        "learning_rate": 0.00019995625356716287,
        "epoch": 0.009866666666666666,
        "step": 74
    },
    {
        "loss": 2.0087,
        "grad_norm": 3.5087006092071533,
        "learning_rate": 0.0001999543723561848,
        "epoch": 0.01,
        "step": 75
    },
    {
        "loss": 2.8369,
        "grad_norm": 2.768399953842163,
        "learning_rate": 0.00019995245155293926,
        "epoch": 0.010133333333333333,
        "step": 76
    },
    {
        "loss": 2.1546,
        "grad_norm": 3.7794241905212402,
        "learning_rate": 0.00019995049115818705,
        "epoch": 0.010266666666666667,
        "step": 77
    },
    {
        "loss": 2.6047,
        "grad_norm": 2.655809164047241,
        "learning_rate": 0.00019994849117270475,
        "epoch": 0.0104,
        "step": 78
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.3655285835266113,
        "learning_rate": 0.00019994645159728452,
        "epoch": 0.010533333333333334,
        "step": 79
    },
    {
        "loss": 2.836,
        "grad_norm": 2.730755567550659,
        "learning_rate": 0.00019994437243273425,
        "epoch": 0.010666666666666666,
        "step": 80
    },
    {
        "loss": 2.3346,
        "grad_norm": 6.63829231262207,
        "learning_rate": 0.00019994225367987752,
        "epoch": 0.0108,
        "step": 81
    },
    {
        "loss": 1.2562,
        "grad_norm": 4.378453254699707,
        "learning_rate": 0.00019994009533955357,
        "epoch": 0.010933333333333333,
        "step": 82
    },
    {
        "loss": 2.9036,
        "grad_norm": 3.1050055027008057,
        "learning_rate": 0.00019993789741261727,
        "epoch": 0.011066666666666667,
        "step": 83
    },
    {
        "loss": 2.3895,
        "grad_norm": 3.6530814170837402,
        "learning_rate": 0.00019993565989993934,
        "epoch": 0.0112,
        "step": 84
    },
    {
        "loss": 2.3786,
        "grad_norm": 3.7640209197998047,
        "learning_rate": 0.00019993338280240597,
        "epoch": 0.011333333333333334,
        "step": 85
    },
    {
        "loss": 3.0281,
        "grad_norm": 3.319652795791626,
        "learning_rate": 0.00019993106612091913,
        "epoch": 0.011466666666666667,
        "step": 86
    },
    {
        "loss": 2.545,
        "grad_norm": 4.579065799713135,
        "learning_rate": 0.00019992870985639652,
        "epoch": 0.0116,
        "step": 87
    },
    {
        "loss": 2.3691,
        "grad_norm": 4.920710563659668,
        "learning_rate": 0.00019992631400977146,
        "epoch": 0.011733333333333333,
        "step": 88
    },
    {
        "loss": 2.4595,
        "grad_norm": 4.589212894439697,
        "learning_rate": 0.0001999238785819929,
        "epoch": 0.011866666666666666,
        "step": 89
    },
    {
        "loss": 1.8545,
        "grad_norm": 3.0039284229278564,
        "learning_rate": 0.00019992140357402556,
        "epoch": 0.012,
        "step": 90
    },
    {
        "loss": 2.6033,
        "grad_norm": 4.024564743041992,
        "learning_rate": 0.00019991888898684978,
        "epoch": 0.012133333333333333,
        "step": 91
    },
    {
        "loss": 0.9123,
        "grad_norm": 4.093951225280762,
        "learning_rate": 0.00019991633482146161,
        "epoch": 0.012266666666666667,
        "step": 92
    },
    {
        "loss": 1.6001,
        "grad_norm": 4.8360676765441895,
        "learning_rate": 0.00019991374107887277,
        "epoch": 0.0124,
        "step": 93
    },
    {
        "loss": 0.6773,
        "grad_norm": 2.731657028198242,
        "learning_rate": 0.00019991110776011067,
        "epoch": 0.012533333333333334,
        "step": 94
    },
    {
        "loss": 2.8999,
        "grad_norm": 3.413203716278076,
        "learning_rate": 0.0001999084348662183,
        "epoch": 0.012666666666666666,
        "step": 95
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.08544659614563,
        "learning_rate": 0.00019990572239825447,
        "epoch": 0.0128,
        "step": 96
    },
    {
        "loss": 2.4197,
        "grad_norm": 5.131059646606445,
        "learning_rate": 0.00019990297035729358,
        "epoch": 0.012933333333333333,
        "step": 97
    },
    {
        "loss": 1.458,
        "grad_norm": 5.197146892547607,
        "learning_rate": 0.0001999001787444257,
        "epoch": 0.013066666666666667,
        "step": 98
    },
    {
        "loss": 2.9703,
        "grad_norm": 2.275768756866455,
        "learning_rate": 0.0001998973475607566,
        "epoch": 0.0132,
        "step": 99
    },
    {
        "loss": 2.7677,
        "grad_norm": 3.325263500213623,
        "learning_rate": 0.00019989447680740781,
        "epoch": 0.013333333333333334,
        "step": 100
    },
    {
        "loss": 3.1188,
        "grad_norm": 2.537505626678467,
        "learning_rate": 0.00019989156648551632,
        "epoch": 0.013466666666666667,
        "step": 101
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.6545016765594482,
        "learning_rate": 0.000199888616596235,
        "epoch": 0.0136,
        "step": 102
    },
    {
        "loss": 3.5845,
        "grad_norm": 2.719480037689209,
        "learning_rate": 0.00019988562714073226,
        "epoch": 0.013733333333333334,
        "step": 103
    },
    {
        "loss": 2.9185,
        "grad_norm": 2.489926338195801,
        "learning_rate": 0.00019988259812019229,
        "epoch": 0.013866666666666666,
        "step": 104
    },
    {
        "loss": 3.0577,
        "grad_norm": 2.7600176334381104,
        "learning_rate": 0.0001998795295358148,
        "epoch": 0.014,
        "step": 105
    },
    {
        "loss": 2.2826,
        "grad_norm": 2.725308418273926,
        "learning_rate": 0.0001998764213888154,
        "epoch": 0.014133333333333333,
        "step": 106
    },
    {
        "loss": 2.5437,
        "grad_norm": 3.449169158935547,
        "learning_rate": 0.00019987327368042515,
        "epoch": 0.014266666666666667,
        "step": 107
    },
    {
        "loss": 2.6231,
        "grad_norm": 2.440807342529297,
        "learning_rate": 0.00019987008641189088,
        "epoch": 0.0144,
        "step": 108
    },
    {
        "loss": 2.7229,
        "grad_norm": 5.110931396484375,
        "learning_rate": 0.00019986685958447506,
        "epoch": 0.014533333333333334,
        "step": 109
    },
    {
        "loss": 2.1149,
        "grad_norm": 2.6299667358398438,
        "learning_rate": 0.00019986359319945589,
        "epoch": 0.014666666666666666,
        "step": 110
    },
    {
        "loss": 1.9126,
        "grad_norm": 2.726616144180298,
        "learning_rate": 0.00019986028725812717,
        "epoch": 0.0148,
        "step": 111
    },
    {
        "loss": 3.1373,
        "grad_norm": 2.530208110809326,
        "learning_rate": 0.00019985694176179842,
        "epoch": 0.014933333333333333,
        "step": 112
    },
    {
        "loss": 1.9611,
        "grad_norm": 5.2278289794921875,
        "learning_rate": 0.00019985355671179477,
        "epoch": 0.015066666666666667,
        "step": 113
    },
    {
        "loss": 2.5259,
        "grad_norm": 2.6857008934020996,
        "learning_rate": 0.00019985013210945706,
        "epoch": 0.0152,
        "step": 114
    },
    {
        "loss": 3.0886,
        "grad_norm": 3.7458300590515137,
        "learning_rate": 0.0001998466679561418,
        "epoch": 0.015333333333333332,
        "step": 115
    },
    {
        "loss": 3.039,
        "grad_norm": 3.243386745452881,
        "learning_rate": 0.00019984316425322114,
        "epoch": 0.015466666666666667,
        "step": 116
    },
    {
        "loss": 1.9108,
        "grad_norm": 4.49009895324707,
        "learning_rate": 0.00019983962100208292,
        "epoch": 0.0156,
        "step": 117
    },
    {
        "loss": 3.3471,
        "grad_norm": 4.704023838043213,
        "learning_rate": 0.00019983603820413062,
        "epoch": 0.015733333333333332,
        "step": 118
    },
    {
        "loss": 1.7268,
        "grad_norm": 4.139725208282471,
        "learning_rate": 0.00019983241586078338,
        "epoch": 0.015866666666666668,
        "step": 119
    },
    {
        "loss": 3.1985,
        "grad_norm": 2.313011407852173,
        "learning_rate": 0.00019982875397347606,
        "epoch": 0.016,
        "step": 120
    },
    {
        "loss": 2.5939,
        "grad_norm": 3.7318661212921143,
        "learning_rate": 0.00019982505254365914,
        "epoch": 0.016133333333333333,
        "step": 121
    },
    {
        "loss": 2.2693,
        "grad_norm": 4.096497058868408,
        "learning_rate": 0.00019982131157279877,
        "epoch": 0.016266666666666665,
        "step": 122
    },
    {
        "loss": 1.8665,
        "grad_norm": 3.8660240173339844,
        "learning_rate": 0.00019981753106237675,
        "epoch": 0.0164,
        "step": 123
    },
    {
        "loss": 1.7704,
        "grad_norm": 4.45922327041626,
        "learning_rate": 0.00019981371101389055,
        "epoch": 0.016533333333333334,
        "step": 124
    },
    {
        "loss": 2.6662,
        "grad_norm": 3.504547119140625,
        "learning_rate": 0.00019980985142885332,
        "epoch": 0.016666666666666666,
        "step": 125
    },
    {
        "loss": 2.6981,
        "grad_norm": 3.3660120964050293,
        "learning_rate": 0.00019980595230879384,
        "epoch": 0.0168,
        "step": 126
    },
    {
        "loss": 2.0106,
        "grad_norm": 3.141263246536255,
        "learning_rate": 0.00019980201365525657,
        "epoch": 0.016933333333333335,
        "step": 127
    },
    {
        "loss": 2.6051,
        "grad_norm": 2.334540605545044,
        "learning_rate": 0.0001997980354698016,
        "epoch": 0.017066666666666667,
        "step": 128
    },
    {
        "loss": 2.6849,
        "grad_norm": 2.789156436920166,
        "learning_rate": 0.00019979401775400477,
        "epoch": 0.0172,
        "step": 129
    },
    {
        "loss": 2.2451,
        "grad_norm": 3.2690629959106445,
        "learning_rate": 0.00019978996050945745,
        "epoch": 0.017333333333333333,
        "step": 130
    },
    {
        "loss": 2.2961,
        "grad_norm": 2.727841377258301,
        "learning_rate": 0.00019978586373776676,
        "epoch": 0.017466666666666665,
        "step": 131
    },
    {
        "loss": 2.9618,
        "grad_norm": 2.8066341876983643,
        "learning_rate": 0.0001997817274405554,
        "epoch": 0.0176,
        "step": 132
    },
    {
        "loss": 2.345,
        "grad_norm": 4.076068878173828,
        "learning_rate": 0.0001997775516194618,
        "epoch": 0.017733333333333334,
        "step": 133
    },
    {
        "loss": 2.8438,
        "grad_norm": 2.2066493034362793,
        "learning_rate": 0.00019977333627614006,
        "epoch": 0.017866666666666666,
        "step": 134
    },
    {
        "loss": 3.0045,
        "grad_norm": 2.1565258502960205,
        "learning_rate": 0.00019976908141225983,
        "epoch": 0.018,
        "step": 135
    },
    {
        "loss": 2.6087,
        "grad_norm": 2.9098291397094727,
        "learning_rate": 0.00019976478702950646,
        "epoch": 0.018133333333333335,
        "step": 136
    },
    {
        "loss": 1.1367,
        "grad_norm": 6.50792932510376,
        "learning_rate": 0.00019976045312958108,
        "epoch": 0.018266666666666667,
        "step": 137
    },
    {
        "loss": 2.1911,
        "grad_norm": 2.9487805366516113,
        "learning_rate": 0.00019975607971420026,
        "epoch": 0.0184,
        "step": 138
    },
    {
        "loss": 2.453,
        "grad_norm": 4.2234344482421875,
        "learning_rate": 0.00019975166678509634,
        "epoch": 0.018533333333333332,
        "step": 139
    },
    {
        "loss": 3.0837,
        "grad_norm": 2.4186015129089355,
        "learning_rate": 0.00019974721434401732,
        "epoch": 0.018666666666666668,
        "step": 140
    },
    {
        "loss": 2.8307,
        "grad_norm": 2.2085561752319336,
        "learning_rate": 0.0001997427223927268,
        "epoch": 0.0188,
        "step": 141
    },
    {
        "loss": 2.4501,
        "grad_norm": 3.311047315597534,
        "learning_rate": 0.0001997381909330041,
        "epoch": 0.018933333333333333,
        "step": 142
    },
    {
        "loss": 2.9061,
        "grad_norm": 2.406651735305786,
        "learning_rate": 0.00019973361996664414,
        "epoch": 0.019066666666666666,
        "step": 143
    },
    {
        "loss": 2.4517,
        "grad_norm": 4.955704212188721,
        "learning_rate": 0.00019972900949545745,
        "epoch": 0.0192,
        "step": 144
    },
    {
        "loss": 2.9287,
        "grad_norm": 3.243757963180542,
        "learning_rate": 0.00019972435952127026,
        "epoch": 0.019333333333333334,
        "step": 145
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.48463773727417,
        "learning_rate": 0.0001997196700459245,
        "epoch": 0.019466666666666667,
        "step": 146
    },
    {
        "loss": 2.4265,
        "grad_norm": 3.448218822479248,
        "learning_rate": 0.0001997149410712776,
        "epoch": 0.0196,
        "step": 147
    },
    {
        "loss": 2.3509,
        "grad_norm": 4.287555694580078,
        "learning_rate": 0.00019971017259920283,
        "epoch": 0.019733333333333332,
        "step": 148
    },
    {
        "loss": 2.1438,
        "grad_norm": 2.1834723949432373,
        "learning_rate": 0.00019970536463158892,
        "epoch": 0.019866666666666668,
        "step": 149
    },
    {
        "loss": 2.6286,
        "grad_norm": 2.605320453643799,
        "learning_rate": 0.00019970051717034033,
        "epoch": 0.02,
        "step": 150
    },
    {
        "loss": 2.0236,
        "grad_norm": 3.8043227195739746,
        "learning_rate": 0.00019969563021737718,
        "epoch": 0.020133333333333333,
        "step": 151
    },
    {
        "loss": 2.6858,
        "grad_norm": 2.3323957920074463,
        "learning_rate": 0.00019969070377463517,
        "epoch": 0.020266666666666665,
        "step": 152
    },
    {
        "loss": 2.4585,
        "grad_norm": 2.5889551639556885,
        "learning_rate": 0.00019968573784406575,
        "epoch": 0.0204,
        "step": 153
    },
    {
        "loss": 2.8816,
        "grad_norm": 2.7027652263641357,
        "learning_rate": 0.0001996807324276359,
        "epoch": 0.020533333333333334,
        "step": 154
    },
    {
        "loss": 2.5627,
        "grad_norm": 1.6800085306167603,
        "learning_rate": 0.00019967568752732824,
        "epoch": 0.020666666666666667,
        "step": 155
    },
    {
        "loss": 2.4035,
        "grad_norm": 3.6105120182037354,
        "learning_rate": 0.0001996706031451411,
        "epoch": 0.0208,
        "step": 156
    },
    {
        "loss": 2.6376,
        "grad_norm": 2.246023178100586,
        "learning_rate": 0.0001996654792830885,
        "epoch": 0.020933333333333335,
        "step": 157
    },
    {
        "loss": 2.3467,
        "grad_norm": 3.6198320388793945,
        "learning_rate": 0.00019966031594319993,
        "epoch": 0.021066666666666668,
        "step": 158
    },
    {
        "loss": 2.4015,
        "grad_norm": 2.942560911178589,
        "learning_rate": 0.0001996551131275206,
        "epoch": 0.0212,
        "step": 159
    },
    {
        "loss": 2.7542,
        "grad_norm": 4.265190124511719,
        "learning_rate": 0.00019964987083811143,
        "epoch": 0.021333333333333333,
        "step": 160
    },
    {
        "loss": 3.1171,
        "grad_norm": 2.8239197731018066,
        "learning_rate": 0.00019964458907704885,
        "epoch": 0.021466666666666665,
        "step": 161
    },
    {
        "loss": 2.4617,
        "grad_norm": 3.185765266418457,
        "learning_rate": 0.000199639267846425,
        "epoch": 0.0216,
        "step": 162
    },
    {
        "loss": 3.1291,
        "grad_norm": 2.7668793201446533,
        "learning_rate": 0.00019963390714834766,
        "epoch": 0.021733333333333334,
        "step": 163
    },
    {
        "loss": 2.4927,
        "grad_norm": 3.1706295013427734,
        "learning_rate": 0.0001996285069849402,
        "epoch": 0.021866666666666666,
        "step": 164
    },
    {
        "loss": 2.6183,
        "grad_norm": 1.5702908039093018,
        "learning_rate": 0.00019962306735834165,
        "epoch": 0.022,
        "step": 165
    },
    {
        "loss": 2.3884,
        "grad_norm": 2.3927342891693115,
        "learning_rate": 0.00019961758827070667,
        "epoch": 0.022133333333333335,
        "step": 166
    },
    {
        "loss": 3.0338,
        "grad_norm": 3.307452440261841,
        "learning_rate": 0.00019961206972420552,
        "epoch": 0.022266666666666667,
        "step": 167
    },
    {
        "loss": 2.0546,
        "grad_norm": 2.15852427482605,
        "learning_rate": 0.00019960651172102413,
        "epoch": 0.0224,
        "step": 168
    },
    {
        "loss": 1.8131,
        "grad_norm": 1.8781992197036743,
        "learning_rate": 0.00019960091426336404,
        "epoch": 0.022533333333333332,
        "step": 169
    },
    {
        "loss": 1.3078,
        "grad_norm": 3.544299602508545,
        "learning_rate": 0.00019959527735344246,
        "epoch": 0.02266666666666667,
        "step": 170
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.035982131958008,
        "learning_rate": 0.00019958960099349216,
        "epoch": 0.0228,
        "step": 171
    },
    {
        "loss": 2.3856,
        "grad_norm": 3.152811288833618,
        "learning_rate": 0.00019958388518576153,
        "epoch": 0.022933333333333333,
        "step": 172
    },
    {
        "loss": 2.3586,
        "grad_norm": 2.6805758476257324,
        "learning_rate": 0.00019957812993251468,
        "epoch": 0.023066666666666666,
        "step": 173
    },
    {
        "loss": 2.4901,
        "grad_norm": 3.0536422729492188,
        "learning_rate": 0.00019957233523603126,
        "epoch": 0.0232,
        "step": 174
    },
    {
        "loss": 1.2234,
        "grad_norm": 4.557426452636719,
        "learning_rate": 0.00019956650109860652,
        "epoch": 0.023333333333333334,
        "step": 175
    },
    {
        "loss": 2.3525,
        "grad_norm": 3.5780715942382812,
        "learning_rate": 0.0001995606275225515,
        "epoch": 0.023466666666666667,
        "step": 176
    },
    {
        "loss": 2.5136,
        "grad_norm": 2.721569776535034,
        "learning_rate": 0.00019955471451019264,
        "epoch": 0.0236,
        "step": 177
    },
    {
        "loss": 2.1024,
        "grad_norm": 2.2855851650238037,
        "learning_rate": 0.00019954876206387215,
        "epoch": 0.023733333333333332,
        "step": 178
    },
    {
        "loss": 1.6578,
        "grad_norm": 3.3085036277770996,
        "learning_rate": 0.00019954277018594777,
        "epoch": 0.023866666666666668,
        "step": 179
    },
    {
        "loss": 1.6118,
        "grad_norm": 3.1529922485351562,
        "learning_rate": 0.000199536738878793,
        "epoch": 0.024,
        "step": 180
    },
    {
        "loss": 2.9645,
        "grad_norm": 2.799703359603882,
        "learning_rate": 0.00019953066814479674,
        "epoch": 0.024133333333333333,
        "step": 181
    },
    {
        "loss": 3.3272,
        "grad_norm": 4.184623718261719,
        "learning_rate": 0.00019952455798636368,
        "epoch": 0.024266666666666666,
        "step": 182
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.600721597671509,
        "learning_rate": 0.0001995184084059141,
        "epoch": 0.0244,
        "step": 183
    },
    {
        "loss": 3.0623,
        "grad_norm": 2.0031545162200928,
        "learning_rate": 0.00019951221940588387,
        "epoch": 0.024533333333333334,
        "step": 184
    },
    {
        "loss": 2.3705,
        "grad_norm": 2.544461250305176,
        "learning_rate": 0.00019950599098872443,
        "epoch": 0.024666666666666667,
        "step": 185
    },
    {
        "loss": 3.0916,
        "grad_norm": 3.581313371658325,
        "learning_rate": 0.0001994997231569029,
        "epoch": 0.0248,
        "step": 186
    },
    {
        "loss": 2.9183,
        "grad_norm": 2.885434627532959,
        "learning_rate": 0.00019949341591290201,
        "epoch": 0.02493333333333333,
        "step": 187
    },
    {
        "loss": 2.9226,
        "grad_norm": 3.5669283866882324,
        "learning_rate": 0.00019948706925922003,
        "epoch": 0.025066666666666668,
        "step": 188
    },
    {
        "loss": 2.5268,
        "grad_norm": 2.67629337310791,
        "learning_rate": 0.00019948068319837093,
        "epoch": 0.0252,
        "step": 189
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.0764472484588623,
        "learning_rate": 0.00019947425773288426,
        "epoch": 0.025333333333333333,
        "step": 190
    },
    {
        "loss": 3.0662,
        "grad_norm": 2.282320261001587,
        "learning_rate": 0.00019946779286530514,
        "epoch": 0.025466666666666665,
        "step": 191
    },
    {
        "loss": 2.8654,
        "grad_norm": 3.2660107612609863,
        "learning_rate": 0.00019946128859819434,
        "epoch": 0.0256,
        "step": 192
    },
    {
        "loss": 2.1044,
        "grad_norm": 2.948420286178589,
        "learning_rate": 0.00019945474493412822,
        "epoch": 0.025733333333333334,
        "step": 193
    },
    {
        "loss": 1.7359,
        "grad_norm": 3.2489235401153564,
        "learning_rate": 0.00019944816187569876,
        "epoch": 0.025866666666666666,
        "step": 194
    },
    {
        "loss": 2.145,
        "grad_norm": 3.8205196857452393,
        "learning_rate": 0.00019944153942551352,
        "epoch": 0.026,
        "step": 195
    },
    {
        "loss": 3.0497,
        "grad_norm": 2.59065318107605,
        "learning_rate": 0.00019943487758619565,
        "epoch": 0.026133333333333335,
        "step": 196
    },
    {
        "loss": 1.6765,
        "grad_norm": 4.168922424316406,
        "learning_rate": 0.00019942817636038398,
        "epoch": 0.026266666666666667,
        "step": 197
    },
    {
        "loss": 2.0078,
        "grad_norm": 2.4310834407806396,
        "learning_rate": 0.00019942143575073286,
        "epoch": 0.0264,
        "step": 198
    },
    {
        "loss": 1.8208,
        "grad_norm": 3.0478663444519043,
        "learning_rate": 0.00019941465575991224,
        "epoch": 0.026533333333333332,
        "step": 199
    },
    {
        "loss": 2.7546,
        "grad_norm": 2.3152759075164795,
        "learning_rate": 0.00019940783639060773,
        "epoch": 0.02666666666666667,
        "step": 200
    },
    {
        "loss": 2.1087,
        "grad_norm": 3.0751945972442627,
        "learning_rate": 0.00019940097764552052,
        "epoch": 0.0268,
        "step": 201
    },
    {
        "loss": 2.0969,
        "grad_norm": 3.491203784942627,
        "learning_rate": 0.00019939407952736737,
        "epoch": 0.026933333333333333,
        "step": 202
    },
    {
        "loss": 1.9889,
        "grad_norm": 4.368183135986328,
        "learning_rate": 0.00019938714203888064,
        "epoch": 0.027066666666666666,
        "step": 203
    },
    {
        "loss": 2.6464,
        "grad_norm": 3.5822086334228516,
        "learning_rate": 0.0001993801651828083,
        "epoch": 0.0272,
        "step": 204
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.942551612854004,
        "learning_rate": 0.00019937314896191385,
        "epoch": 0.027333333333333334,
        "step": 205
    },
    {
        "loss": 2.3343,
        "grad_norm": 2.8460333347320557,
        "learning_rate": 0.00019936609337897654,
        "epoch": 0.027466666666666667,
        "step": 206
    },
    {
        "loss": 2.5631,
        "grad_norm": 3.0748465061187744,
        "learning_rate": 0.00019935899843679105,
        "epoch": 0.0276,
        "step": 207
    },
    {
        "loss": 2.5394,
        "grad_norm": 3.518930673599243,
        "learning_rate": 0.00019935186413816774,
        "epoch": 0.027733333333333332,
        "step": 208
    },
    {
        "loss": 2.7696,
        "grad_norm": 2.8330206871032715,
        "learning_rate": 0.0001993446904859325,
        "epoch": 0.027866666666666668,
        "step": 209
    },
    {
        "loss": 1.1955,
        "grad_norm": 4.176533222198486,
        "learning_rate": 0.00019933747748292682,
        "epoch": 0.028,
        "step": 210
    },
    {
        "loss": 2.8489,
        "grad_norm": 2.3055291175842285,
        "learning_rate": 0.00019933022513200785,
        "epoch": 0.028133333333333333,
        "step": 211
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.748627185821533,
        "learning_rate": 0.00019932293343604825,
        "epoch": 0.028266666666666666,
        "step": 212
    },
    {
        "loss": 3.0398,
        "grad_norm": 4.31307315826416,
        "learning_rate": 0.0001993156023979363,
        "epoch": 0.0284,
        "step": 213
    },
    {
        "loss": 2.162,
        "grad_norm": 4.013891696929932,
        "learning_rate": 0.00019930823202057578,
        "epoch": 0.028533333333333334,
        "step": 214
    },
    {
        "loss": 3.0883,
        "grad_norm": 2.4501566886901855,
        "learning_rate": 0.00019930082230688618,
        "epoch": 0.028666666666666667,
        "step": 215
    },
    {
        "loss": 1.3774,
        "grad_norm": 3.2523715496063232,
        "learning_rate": 0.00019929337325980253,
        "epoch": 0.0288,
        "step": 216
    },
    {
        "loss": 2.6374,
        "grad_norm": 2.5091159343719482,
        "learning_rate": 0.0001992858848822754,
        "epoch": 0.028933333333333332,
        "step": 217
    },
    {
        "loss": 2.7748,
        "grad_norm": 2.053847074508667,
        "learning_rate": 0.00019927835717727094,
        "epoch": 0.029066666666666668,
        "step": 218
    },
    {
        "loss": 2.7039,
        "grad_norm": 3.9691333770751953,
        "learning_rate": 0.00019927079014777092,
        "epoch": 0.0292,
        "step": 219
    },
    {
        "loss": 2.0857,
        "grad_norm": 3.6392502784729004,
        "learning_rate": 0.0001992631837967727,
        "epoch": 0.029333333333333333,
        "step": 220
    },
    {
        "loss": 1.9555,
        "grad_norm": 3.406172752380371,
        "learning_rate": 0.0001992555381272891,
        "epoch": 0.029466666666666665,
        "step": 221
    },
    {
        "loss": 2.732,
        "grad_norm": 2.9127910137176514,
        "learning_rate": 0.00019924785314234868,
        "epoch": 0.0296,
        "step": 222
    },
    {
        "loss": 1.8476,
        "grad_norm": 3.1898462772369385,
        "learning_rate": 0.00019924012884499543,
        "epoch": 0.029733333333333334,
        "step": 223
    },
    {
        "loss": 2.9577,
        "grad_norm": 2.2496416568756104,
        "learning_rate": 0.000199232365238289,
        "epoch": 0.029866666666666666,
        "step": 224
    },
    {
        "loss": 1.9922,
        "grad_norm": 3.040173053741455,
        "learning_rate": 0.00019922456232530458,
        "epoch": 0.03,
        "step": 225
    },
    {
        "loss": 2.248,
        "grad_norm": 3.392387866973877,
        "learning_rate": 0.0001992167201091329,
        "epoch": 0.030133333333333335,
        "step": 226
    },
    {
        "loss": 2.6193,
        "grad_norm": 2.832296848297119,
        "learning_rate": 0.00019920883859288034,
        "epoch": 0.030266666666666667,
        "step": 227
    },
    {
        "loss": 1.87,
        "grad_norm": 3.1463053226470947,
        "learning_rate": 0.00019920091777966877,
        "epoch": 0.0304,
        "step": 228
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.919670581817627,
        "learning_rate": 0.0001991929576726356,
        "epoch": 0.030533333333333332,
        "step": 229
    },
    {
        "loss": 0.8372,
        "grad_norm": 3.9460976123809814,
        "learning_rate": 0.00019918495827493394,
        "epoch": 0.030666666666666665,
        "step": 230
    },
    {
        "loss": 3.025,
        "grad_norm": 2.838035821914673,
        "learning_rate": 0.00019917691958973234,
        "epoch": 0.0308,
        "step": 231
    },
    {
        "loss": 2.0154,
        "grad_norm": 3.8469533920288086,
        "learning_rate": 0.00019916884162021495,
        "epoch": 0.030933333333333334,
        "step": 232
    },
    {
        "loss": 2.3566,
        "grad_norm": 3.675671100616455,
        "learning_rate": 0.0001991607243695815,
        "epoch": 0.031066666666666666,
        "step": 233
    },
    {
        "loss": 2.8644,
        "grad_norm": 3.597442388534546,
        "learning_rate": 0.00019915256784104723,
        "epoch": 0.0312,
        "step": 234
    },
    {
        "loss": 2.5606,
        "grad_norm": 3.253075361251831,
        "learning_rate": 0.000199144372037843,
        "epoch": 0.03133333333333333,
        "step": 235
    },
    {
        "loss": 2.5464,
        "grad_norm": 2.754143476486206,
        "learning_rate": 0.00019913613696321517,
        "epoch": 0.031466666666666664,
        "step": 236
    },
    {
        "loss": 3.1127,
        "grad_norm": 1.673103928565979,
        "learning_rate": 0.0001991278626204257,
        "epoch": 0.0316,
        "step": 237
    },
    {
        "loss": 2.507,
        "grad_norm": 4.767696380615234,
        "learning_rate": 0.00019911954901275205,
        "epoch": 0.031733333333333336,
        "step": 238
    },
    {
        "loss": 2.6749,
        "grad_norm": 2.473341941833496,
        "learning_rate": 0.0001991111961434873,
        "epoch": 0.03186666666666667,
        "step": 239
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.073233127593994,
        "learning_rate": 0.00019910280401594005,
        "epoch": 0.032,
        "step": 240
    },
    {
        "loss": 1.222,
        "grad_norm": 3.308091878890991,
        "learning_rate": 0.00019909437263343445,
        "epoch": 0.03213333333333333,
        "step": 241
    },
    {
        "loss": 2.6662,
        "grad_norm": 2.991603374481201,
        "learning_rate": 0.0001990859019993102,
        "epoch": 0.032266666666666666,
        "step": 242
    },
    {
        "loss": 2.4547,
        "grad_norm": 2.470649480819702,
        "learning_rate": 0.00019907739211692254,
        "epoch": 0.0324,
        "step": 243
    },
    {
        "loss": 2.2761,
        "grad_norm": 3.234224796295166,
        "learning_rate": 0.00019906884298964226,
        "epoch": 0.03253333333333333,
        "step": 244
    },
    {
        "loss": 3.1659,
        "grad_norm": 2.395537853240967,
        "learning_rate": 0.00019906025462085569,
        "epoch": 0.03266666666666666,
        "step": 245
    },
    {
        "loss": 2.7955,
        "grad_norm": 2.4878714084625244,
        "learning_rate": 0.00019905162701396473,
        "epoch": 0.0328,
        "step": 246
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.974778652191162,
        "learning_rate": 0.0001990429601723868,
        "epoch": 0.032933333333333335,
        "step": 247
    },
    {
        "loss": 1.7834,
        "grad_norm": 4.436533451080322,
        "learning_rate": 0.0001990342540995549,
        "epoch": 0.03306666666666667,
        "step": 248
    },
    {
        "loss": 3.0108,
        "grad_norm": 3.079716444015503,
        "learning_rate": 0.00019902550879891747,
        "epoch": 0.0332,
        "step": 249
    },
    {
        "loss": 2.4293,
        "grad_norm": 2.9759464263916016,
        "learning_rate": 0.00019901672427393856,
        "epoch": 0.03333333333333333,
        "step": 250
    },
    {
        "loss": 2.363,
        "grad_norm": 3.1707990169525146,
        "learning_rate": 0.00019900790052809782,
        "epoch": 0.033466666666666665,
        "step": 251
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.884395122528076,
        "learning_rate": 0.0001989990375648903,
        "epoch": 0.0336,
        "step": 252
    },
    {
        "loss": 3.4161,
        "grad_norm": 1.902574062347412,
        "learning_rate": 0.00019899013538782665,
        "epoch": 0.03373333333333333,
        "step": 253
    },
    {
        "loss": 2.6088,
        "grad_norm": 3.985529899597168,
        "learning_rate": 0.00019898119400043308,
        "epoch": 0.03386666666666667,
        "step": 254
    },
    {
        "loss": 3.3368,
        "grad_norm": 5.784578323364258,
        "learning_rate": 0.0001989722134062513,
        "epoch": 0.034,
        "step": 255
    },
    {
        "loss": 2.4849,
        "grad_norm": 3.4325010776519775,
        "learning_rate": 0.00019896319360883857,
        "epoch": 0.034133333333333335,
        "step": 256
    },
    {
        "loss": 1.0586,
        "grad_norm": 2.8126494884490967,
        "learning_rate": 0.00019895413461176763,
        "epoch": 0.03426666666666667,
        "step": 257
    },
    {
        "loss": 2.9504,
        "grad_norm": 2.9300434589385986,
        "learning_rate": 0.0001989450364186268,
        "epoch": 0.0344,
        "step": 258
    },
    {
        "loss": 2.0952,
        "grad_norm": 2.2952685356140137,
        "learning_rate": 0.00019893589903301987,
        "epoch": 0.03453333333333333,
        "step": 259
    },
    {
        "loss": 1.6558,
        "grad_norm": 3.8053641319274902,
        "learning_rate": 0.00019892672245856624,
        "epoch": 0.034666666666666665,
        "step": 260
    },
    {
        "loss": 3.0431,
        "grad_norm": 2.2025067806243896,
        "learning_rate": 0.00019891750669890078,
        "epoch": 0.0348,
        "step": 261
    },
    {
        "loss": 1.1936,
        "grad_norm": 3.325864315032959,
        "learning_rate": 0.00019890825175767382,
        "epoch": 0.03493333333333333,
        "step": 262
    },
    {
        "loss": 1.9438,
        "grad_norm": 3.251572847366333,
        "learning_rate": 0.00019889895763855134,
        "epoch": 0.03506666666666667,
        "step": 263
    },
    {
        "loss": 2.6383,
        "grad_norm": 2.930182456970215,
        "learning_rate": 0.00019888962434521474,
        "epoch": 0.0352,
        "step": 264
    },
    {
        "loss": 2.2322,
        "grad_norm": 4.601438999176025,
        "learning_rate": 0.000198880251881361,
        "epoch": 0.035333333333333335,
        "step": 265
    },
    {
        "loss": 1.5833,
        "grad_norm": 4.810059547424316,
        "learning_rate": 0.00019887084025070254,
        "epoch": 0.03546666666666667,
        "step": 266
    },
    {
        "loss": 2.4433,
        "grad_norm": 2.4140355587005615,
        "learning_rate": 0.00019886138945696737,
        "epoch": 0.0356,
        "step": 267
    },
    {
        "loss": 2.7524,
        "grad_norm": 4.142805576324463,
        "learning_rate": 0.00019885189950389896,
        "epoch": 0.03573333333333333,
        "step": 268
    },
    {
        "loss": 0.9987,
        "grad_norm": 3.878312587738037,
        "learning_rate": 0.00019884237039525634,
        "epoch": 0.035866666666666665,
        "step": 269
    },
    {
        "loss": 2.6943,
        "grad_norm": 3.0616531372070312,
        "learning_rate": 0.00019883280213481403,
        "epoch": 0.036,
        "step": 270
    },
    {
        "loss": 2.9743,
        "grad_norm": 2.650900363922119,
        "learning_rate": 0.000198823194726362,
        "epoch": 0.03613333333333334,
        "step": 271
    },
    {
        "loss": 2.054,
        "grad_norm": 4.17992639541626,
        "learning_rate": 0.00019881354817370583,
        "epoch": 0.03626666666666667,
        "step": 272
    },
    {
        "loss": 2.2621,
        "grad_norm": 2.6951684951782227,
        "learning_rate": 0.0001988038624806665,
        "epoch": 0.0364,
        "step": 273
    },
    {
        "loss": 2.8628,
        "grad_norm": 1.8161991834640503,
        "learning_rate": 0.0001987941376510806,
        "epoch": 0.036533333333333334,
        "step": 274
    },
    {
        "loss": 2.2922,
        "grad_norm": 4.480897903442383,
        "learning_rate": 0.00019878437368880017,
        "epoch": 0.03666666666666667,
        "step": 275
    },
    {
        "loss": 1.0046,
        "grad_norm": 2.700214385986328,
        "learning_rate": 0.00019877457059769266,
        "epoch": 0.0368,
        "step": 276
    },
    {
        "loss": 2.5907,
        "grad_norm": 2.5055603981018066,
        "learning_rate": 0.00019876472838164123,
        "epoch": 0.03693333333333333,
        "step": 277
    },
    {
        "loss": 2.2923,
        "grad_norm": 2.4979803562164307,
        "learning_rate": 0.0001987548470445443,
        "epoch": 0.037066666666666664,
        "step": 278
    },
    {
        "loss": 2.7071,
        "grad_norm": 2.0775091648101807,
        "learning_rate": 0.00019874492659031597,
        "epoch": 0.0372,
        "step": 279
    },
    {
        "loss": 2.8598,
        "grad_norm": 3.048051595687866,
        "learning_rate": 0.00019873496702288578,
        "epoch": 0.037333333333333336,
        "step": 280
    },
    {
        "loss": 2.9604,
        "grad_norm": 3.2225680351257324,
        "learning_rate": 0.0001987249683461987,
        "epoch": 0.03746666666666667,
        "step": 281
    },
    {
        "loss": 2.5655,
        "grad_norm": 3.9828174114227295,
        "learning_rate": 0.00019871493056421527,
        "epoch": 0.0376,
        "step": 282
    },
    {
        "loss": 2.0722,
        "grad_norm": 3.767277479171753,
        "learning_rate": 0.00019870485368091148,
        "epoch": 0.037733333333333334,
        "step": 283
    },
    {
        "loss": 2.013,
        "grad_norm": 4.05158805847168,
        "learning_rate": 0.0001986947377002788,
        "epoch": 0.037866666666666667,
        "step": 284
    },
    {
        "loss": 2.2035,
        "grad_norm": 3.8911964893341064,
        "learning_rate": 0.00019868458262632424,
        "epoch": 0.038,
        "step": 285
    },
    {
        "loss": 2.5801,
        "grad_norm": 2.850907325744629,
        "learning_rate": 0.00019867438846307025,
        "epoch": 0.03813333333333333,
        "step": 286
    },
    {
        "loss": 1.9137,
        "grad_norm": 3.3956193923950195,
        "learning_rate": 0.00019866415521455476,
        "epoch": 0.038266666666666664,
        "step": 287
    },
    {
        "loss": 2.5245,
        "grad_norm": 2.554161787033081,
        "learning_rate": 0.00019865388288483119,
        "epoch": 0.0384,
        "step": 288
    },
    {
        "loss": 2.598,
        "grad_norm": 3.091219663619995,
        "learning_rate": 0.00019864357147796847,
        "epoch": 0.038533333333333336,
        "step": 289
    },
    {
        "loss": 2.3651,
        "grad_norm": 2.4021193981170654,
        "learning_rate": 0.00019863322099805095,
        "epoch": 0.03866666666666667,
        "step": 290
    },
    {
        "loss": 2.2361,
        "grad_norm": 3.9910733699798584,
        "learning_rate": 0.00019862283144917852,
        "epoch": 0.0388,
        "step": 291
    },
    {
        "loss": 2.9329,
        "grad_norm": 1.5125889778137207,
        "learning_rate": 0.00019861240283546648,
        "epoch": 0.038933333333333334,
        "step": 292
    },
    {
        "loss": 2.483,
        "grad_norm": 3.164039373397827,
        "learning_rate": 0.0001986019351610457,
        "epoch": 0.039066666666666666,
        "step": 293
    },
    {
        "loss": 2.8562,
        "grad_norm": 3.6609413623809814,
        "learning_rate": 0.00019859142843006242,
        "epoch": 0.0392,
        "step": 294
    },
    {
        "loss": 2.9669,
        "grad_norm": 1.9322967529296875,
        "learning_rate": 0.00019858088264667836,
        "epoch": 0.03933333333333333,
        "step": 295
    },
    {
        "loss": 2.9675,
        "grad_norm": 2.5278565883636475,
        "learning_rate": 0.00019857029781507083,
        "epoch": 0.039466666666666664,
        "step": 296
    },
    {
        "loss": 2.7565,
        "grad_norm": 1.780096411705017,
        "learning_rate": 0.00019855967393943243,
        "epoch": 0.0396,
        "step": 297
    },
    {
        "loss": 2.2845,
        "grad_norm": 3.2193570137023926,
        "learning_rate": 0.00019854901102397135,
        "epoch": 0.039733333333333336,
        "step": 298
    },
    {
        "loss": 3.2492,
        "grad_norm": 2.649170160293579,
        "learning_rate": 0.0001985383090729112,
        "epoch": 0.03986666666666667,
        "step": 299
    },
    {
        "loss": 1.6605,
        "grad_norm": 2.806626319885254,
        "learning_rate": 0.0001985275680904911,
        "epoch": 0.04,
        "step": 300
    },
    {
        "loss": 1.6438,
        "grad_norm": 4.190276145935059,
        "learning_rate": 0.0001985167880809655,
        "epoch": 0.04013333333333333,
        "step": 301
    },
    {
        "loss": 1.318,
        "grad_norm": 3.4350991249084473,
        "learning_rate": 0.00019850596904860451,
        "epoch": 0.040266666666666666,
        "step": 302
    },
    {
        "loss": 2.2536,
        "grad_norm": 2.7837891578674316,
        "learning_rate": 0.0001984951109976935,
        "epoch": 0.0404,
        "step": 303
    },
    {
        "loss": 2.2098,
        "grad_norm": 3.2276716232299805,
        "learning_rate": 0.0001984842139325334,
        "epoch": 0.04053333333333333,
        "step": 304
    },
    {
        "loss": 2.371,
        "grad_norm": 3.896768093109131,
        "learning_rate": 0.0001984732778574406,
        "epoch": 0.04066666666666666,
        "step": 305
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.853935956954956,
        "learning_rate": 0.00019846230277674692,
        "epoch": 0.0408,
        "step": 306
    },
    {
        "loss": 2.7559,
        "grad_norm": 3.4743494987487793,
        "learning_rate": 0.00019845128869479958,
        "epoch": 0.040933333333333335,
        "step": 307
    },
    {
        "loss": 1.6584,
        "grad_norm": 3.426119804382324,
        "learning_rate": 0.00019844023561596132,
        "epoch": 0.04106666666666667,
        "step": 308
    },
    {
        "loss": 1.661,
        "grad_norm": 2.9858362674713135,
        "learning_rate": 0.00019842914354461033,
        "epoch": 0.0412,
        "step": 309
    },
    {
        "loss": 2.6134,
        "grad_norm": 3.1990344524383545,
        "learning_rate": 0.00019841801248514018,
        "epoch": 0.04133333333333333,
        "step": 310
    },
    {
        "loss": 2.9671,
        "grad_norm": 3.924410820007324,
        "learning_rate": 0.00019840684244195993,
        "epoch": 0.041466666666666666,
        "step": 311
    },
    {
        "loss": 1.2383,
        "grad_norm": 4.171603202819824,
        "learning_rate": 0.00019839563341949408,
        "epoch": 0.0416,
        "step": 312
    },
    {
        "loss": 2.2356,
        "grad_norm": 2.489332914352417,
        "learning_rate": 0.00019838438542218255,
        "epoch": 0.04173333333333333,
        "step": 313
    },
    {
        "loss": 1.6443,
        "grad_norm": 3.662292003631592,
        "learning_rate": 0.00019837309845448074,
        "epoch": 0.04186666666666667,
        "step": 314
    },
    {
        "loss": 2.4856,
        "grad_norm": 3.7241058349609375,
        "learning_rate": 0.0001983617725208594,
        "epoch": 0.042,
        "step": 315
    },
    {
        "loss": 2.7231,
        "grad_norm": 2.2615549564361572,
        "learning_rate": 0.00019835040762580482,
        "epoch": 0.042133333333333335,
        "step": 316
    },
    {
        "loss": 1.9379,
        "grad_norm": 3.1911940574645996,
        "learning_rate": 0.0001983390037738187,
        "epoch": 0.04226666666666667,
        "step": 317
    },
    {
        "loss": 2.8986,
        "grad_norm": 3.686292886734009,
        "learning_rate": 0.00019832756096941804,
        "epoch": 0.0424,
        "step": 318
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.6435728073120117,
        "learning_rate": 0.00019831607921713546,
        "epoch": 0.04253333333333333,
        "step": 319
    },
    {
        "loss": 3.0152,
        "grad_norm": 3.753697156906128,
        "learning_rate": 0.00019830455852151891,
        "epoch": 0.042666666666666665,
        "step": 320
    },
    {
        "loss": 1.9729,
        "grad_norm": 2.3812854290008545,
        "learning_rate": 0.0001982929988871318,
        "epoch": 0.0428,
        "step": 321
    },
    {
        "loss": 3.1794,
        "grad_norm": 4.0662760734558105,
        "learning_rate": 0.00019828140031855282,
        "epoch": 0.04293333333333333,
        "step": 322
    },
    {
        "loss": 1.9638,
        "grad_norm": 4.581252574920654,
        "learning_rate": 0.00019826976282037633,
        "epoch": 0.04306666666666667,
        "step": 323
    },
    {
        "loss": 1.8171,
        "grad_norm": 3.695929765701294,
        "learning_rate": 0.00019825808639721193,
        "epoch": 0.0432,
        "step": 324
    },
    {
        "loss": 2.5248,
        "grad_norm": 2.784818410873413,
        "learning_rate": 0.00019824637105368473,
        "epoch": 0.043333333333333335,
        "step": 325
    },
    {
        "loss": 2.3573,
        "grad_norm": 2.810901165008545,
        "learning_rate": 0.00019823461679443515,
        "epoch": 0.04346666666666667,
        "step": 326
    },
    {
        "loss": 2.5211,
        "grad_norm": 2.4475975036621094,
        "learning_rate": 0.00019822282362411917,
        "epoch": 0.0436,
        "step": 327
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.781991481781006,
        "learning_rate": 0.00019821099154740805,
        "epoch": 0.04373333333333333,
        "step": 328
    },
    {
        "loss": 2.3263,
        "grad_norm": 2.586139678955078,
        "learning_rate": 0.00019819912056898853,
        "epoch": 0.043866666666666665,
        "step": 329
    },
    {
        "loss": 1.991,
        "grad_norm": 2.899113178253174,
        "learning_rate": 0.00019818721069356275,
        "epoch": 0.044,
        "step": 330
    },
    {
        "loss": 2.6565,
        "grad_norm": 2.314210891723633,
        "learning_rate": 0.00019817526192584826,
        "epoch": 0.04413333333333333,
        "step": 331
    },
    {
        "loss": 3.0253,
        "grad_norm": 2.913775682449341,
        "learning_rate": 0.000198163274270578,
        "epoch": 0.04426666666666667,
        "step": 332
    },
    {
        "loss": 2.5007,
        "grad_norm": 2.522615671157837,
        "learning_rate": 0.00019815124773250027,
        "epoch": 0.0444,
        "step": 333
    },
    {
        "loss": 2.6134,
        "grad_norm": 2.990861415863037,
        "learning_rate": 0.0001981391823163789,
        "epoch": 0.044533333333333334,
        "step": 334
    },
    {
        "loss": 2.8669,
        "grad_norm": 3.380420446395874,
        "learning_rate": 0.00019812707802699305,
        "epoch": 0.04466666666666667,
        "step": 335
    },
    {
        "loss": 2.3472,
        "grad_norm": 2.582913398742676,
        "learning_rate": 0.0001981149348691372,
        "epoch": 0.0448,
        "step": 336
    },
    {
        "loss": 0.4927,
        "grad_norm": 2.750722885131836,
        "learning_rate": 0.00019810275284762132,
        "epoch": 0.04493333333333333,
        "step": 337
    },
    {
        "loss": 3.2128,
        "grad_norm": 3.0896151065826416,
        "learning_rate": 0.0001980905319672708,
        "epoch": 0.045066666666666665,
        "step": 338
    },
    {
        "loss": 2.9046,
        "grad_norm": 2.5563745498657227,
        "learning_rate": 0.00019807827223292628,
        "epoch": 0.0452,
        "step": 339
    },
    {
        "loss": 2.6237,
        "grad_norm": 3.0828659534454346,
        "learning_rate": 0.00019806597364944396,
        "epoch": 0.04533333333333334,
        "step": 340
    },
    {
        "loss": 2.3936,
        "grad_norm": 3.746436834335327,
        "learning_rate": 0.00019805363622169532,
        "epoch": 0.04546666666666667,
        "step": 341
    },
    {
        "loss": 2.7011,
        "grad_norm": 2.4881558418273926,
        "learning_rate": 0.00019804125995456727,
        "epoch": 0.0456,
        "step": 342
    },
    {
        "loss": 2.7894,
        "grad_norm": 2.148721218109131,
        "learning_rate": 0.00019802884485296208,
        "epoch": 0.045733333333333334,
        "step": 343
    },
    {
        "loss": 2.5877,
        "grad_norm": 3.7489190101623535,
        "learning_rate": 0.00019801639092179745,
        "epoch": 0.04586666666666667,
        "step": 344
    },
    {
        "loss": 2.4367,
        "grad_norm": 2.8246994018554688,
        "learning_rate": 0.00019800389816600634,
        "epoch": 0.046,
        "step": 345
    },
    {
        "loss": 2.7379,
        "grad_norm": 2.8924431800842285,
        "learning_rate": 0.00019799136659053726,
        "epoch": 0.04613333333333333,
        "step": 346
    },
    {
        "loss": 2.3581,
        "grad_norm": 2.400968313217163,
        "learning_rate": 0.00019797879620035396,
        "epoch": 0.046266666666666664,
        "step": 347
    },
    {
        "loss": 2.8191,
        "grad_norm": 2.3152899742126465,
        "learning_rate": 0.00019796618700043564,
        "epoch": 0.0464,
        "step": 348
    },
    {
        "loss": 2.3575,
        "grad_norm": 3.6405694484710693,
        "learning_rate": 0.00019795353899577684,
        "epoch": 0.046533333333333336,
        "step": 349
    },
    {
        "loss": 2.6187,
        "grad_norm": 3.9075567722320557,
        "learning_rate": 0.00019794085219138746,
        "epoch": 0.04666666666666667,
        "step": 350
    },
    {
        "loss": 2.7584,
        "grad_norm": 2.5197837352752686,
        "learning_rate": 0.00019792812659229282,
        "epoch": 0.0468,
        "step": 351
    },
    {
        "loss": 1.0308,
        "grad_norm": 4.123072624206543,
        "learning_rate": 0.00019791536220353356,
        "epoch": 0.046933333333333334,
        "step": 352
    },
    {
        "loss": 2.9076,
        "grad_norm": 3.145556688308716,
        "learning_rate": 0.00019790255903016568,
        "epoch": 0.047066666666666666,
        "step": 353
    },
    {
        "loss": 2.0619,
        "grad_norm": 3.4180898666381836,
        "learning_rate": 0.00019788971707726058,
        "epoch": 0.0472,
        "step": 354
    },
    {
        "loss": 2.7102,
        "grad_norm": 3.397914171218872,
        "learning_rate": 0.00019787683634990498,
        "epoch": 0.04733333333333333,
        "step": 355
    },
    {
        "loss": 3.1996,
        "grad_norm": 3.3864355087280273,
        "learning_rate": 0.000197863916853201,
        "epoch": 0.047466666666666664,
        "step": 356
    },
    {
        "loss": 1.2963,
        "grad_norm": 3.715284824371338,
        "learning_rate": 0.0001978509585922661,
        "epoch": 0.0476,
        "step": 357
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.561511754989624,
        "learning_rate": 0.00019783796157223308,
        "epoch": 0.047733333333333336,
        "step": 358
    },
    {
        "loss": 1.5297,
        "grad_norm": 2.398350954055786,
        "learning_rate": 0.00019782492579825008,
        "epoch": 0.04786666666666667,
        "step": 359
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.7912120819091797,
        "learning_rate": 0.0001978118512754807,
        "epoch": 0.048,
        "step": 360
    },
    {
        "loss": 2.9523,
        "grad_norm": 2.832151412963867,
        "learning_rate": 0.00019779873800910368,
        "epoch": 0.048133333333333334,
        "step": 361
    },
    {
        "loss": 2.8309,
        "grad_norm": 3.8812263011932373,
        "learning_rate": 0.00019778558600431332,
        "epoch": 0.048266666666666666,
        "step": 362
    },
    {
        "loss": 1.4181,
        "grad_norm": 4.423038482666016,
        "learning_rate": 0.00019777239526631916,
        "epoch": 0.0484,
        "step": 363
    },
    {
        "loss": 2.3712,
        "grad_norm": 3.691058397293091,
        "learning_rate": 0.00019775916580034607,
        "epoch": 0.04853333333333333,
        "step": 364
    },
    {
        "loss": 2.9665,
        "grad_norm": 2.4768316745758057,
        "learning_rate": 0.0001977458976116343,
        "epoch": 0.048666666666666664,
        "step": 365
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.3254880905151367,
        "learning_rate": 0.00019773259070543945,
        "epoch": 0.0488,
        "step": 366
    },
    {
        "loss": 2.8944,
        "grad_norm": 2.401392698287964,
        "learning_rate": 0.00019771924508703238,
        "epoch": 0.048933333333333336,
        "step": 367
    },
    {
        "loss": 1.7156,
        "grad_norm": 3.9183008670806885,
        "learning_rate": 0.00019770586076169936,
        "epoch": 0.04906666666666667,
        "step": 368
    },
    {
        "loss": 2.63,
        "grad_norm": 4.373595714569092,
        "learning_rate": 0.00019769243773474197,
        "epoch": 0.0492,
        "step": 369
    },
    {
        "loss": 2.7316,
        "grad_norm": 2.6469767093658447,
        "learning_rate": 0.00019767897601147714,
        "epoch": 0.04933333333333333,
        "step": 370
    },
    {
        "loss": 1.9261,
        "grad_norm": 3.033353090286255,
        "learning_rate": 0.00019766547559723705,
        "epoch": 0.049466666666666666,
        "step": 371
    },
    {
        "loss": 2.036,
        "grad_norm": 3.719278573989868,
        "learning_rate": 0.00019765193649736929,
        "epoch": 0.0496,
        "step": 372
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.2829360961914062,
        "learning_rate": 0.00019763835871723675,
        "epoch": 0.04973333333333333,
        "step": 373
    },
    {
        "loss": 2.9235,
        "grad_norm": 2.7200710773468018,
        "learning_rate": 0.00019762474226221762,
        "epoch": 0.04986666666666666,
        "step": 374
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.7159242630004883,
        "learning_rate": 0.00019761108713770543,
        "epoch": 0.05,
        "step": 375
    },
    {
        "loss": 2.6286,
        "grad_norm": 2.8395371437072754,
        "learning_rate": 0.00019759739334910905,
        "epoch": 0.050133333333333335,
        "step": 376
    },
    {
        "loss": 2.021,
        "grad_norm": 4.384125232696533,
        "learning_rate": 0.00019758366090185257,
        "epoch": 0.05026666666666667,
        "step": 377
    },
    {
        "loss": 0.9773,
        "grad_norm": 4.076915740966797,
        "learning_rate": 0.00019756988980137554,
        "epoch": 0.0504,
        "step": 378
    },
    {
        "loss": 2.5932,
        "grad_norm": 3.309325695037842,
        "learning_rate": 0.00019755608005313265,
        "epoch": 0.05053333333333333,
        "step": 379
    },
    {
        "loss": 2.6707,
        "grad_norm": 2.8235175609588623,
        "learning_rate": 0.00019754223166259407,
        "epoch": 0.050666666666666665,
        "step": 380
    },
    {
        "loss": 2.5617,
        "grad_norm": 2.3185155391693115,
        "learning_rate": 0.00019752834463524516,
        "epoch": 0.0508,
        "step": 381
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.396986722946167,
        "learning_rate": 0.0001975144189765866,
        "epoch": 0.05093333333333333,
        "step": 382
    },
    {
        "loss": 2.7076,
        "grad_norm": 3.4517364501953125,
        "learning_rate": 0.00019750045469213445,
        "epoch": 0.05106666666666667,
        "step": 383
    },
    {
        "loss": 2.3623,
        "grad_norm": 3.1654436588287354,
        "learning_rate": 0.00019748645178741995,
        "epoch": 0.0512,
        "step": 384
    },
    {
        "loss": 2.6107,
        "grad_norm": 2.0964913368225098,
        "learning_rate": 0.00019747241026798972,
        "epoch": 0.051333333333333335,
        "step": 385
    },
    {
        "loss": 2.2336,
        "grad_norm": 4.206197738647461,
        "learning_rate": 0.00019745833013940563,
        "epoch": 0.05146666666666667,
        "step": 386
    },
    {
        "loss": 2.4688,
        "grad_norm": 3.1374735832214355,
        "learning_rate": 0.00019744421140724492,
        "epoch": 0.0516,
        "step": 387
    },
    {
        "loss": 2.2801,
        "grad_norm": 3.453155994415283,
        "learning_rate": 0.00019743005407710004,
        "epoch": 0.05173333333333333,
        "step": 388
    },
    {
        "loss": 2.2039,
        "grad_norm": 2.5673186779022217,
        "learning_rate": 0.00019741585815457875,
        "epoch": 0.051866666666666665,
        "step": 389
    },
    {
        "loss": 2.8224,
        "grad_norm": 2.574087381362915,
        "learning_rate": 0.0001974016236453041,
        "epoch": 0.052,
        "step": 390
    },
    {
        "loss": 1.7048,
        "grad_norm": 4.994707107543945,
        "learning_rate": 0.00019738735055491446,
        "epoch": 0.05213333333333333,
        "step": 391
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.7009592056274414,
        "learning_rate": 0.0001973730388890634,
        "epoch": 0.05226666666666667,
        "step": 392
    },
    {
        "loss": 1.8622,
        "grad_norm": 3.253086805343628,
        "learning_rate": 0.00019735868865341987,
        "epoch": 0.0524,
        "step": 393
    },
    {
        "loss": 2.4231,
        "grad_norm": 2.681342840194702,
        "learning_rate": 0.000197344299853668,
        "epoch": 0.052533333333333335,
        "step": 394
    },
    {
        "loss": 2.4008,
        "grad_norm": 3.71321439743042,
        "learning_rate": 0.00019732987249550726,
        "epoch": 0.05266666666666667,
        "step": 395
    },
    {
        "loss": 2.159,
        "grad_norm": 2.791015148162842,
        "learning_rate": 0.00019731540658465238,
        "epoch": 0.0528,
        "step": 396
    },
    {
        "loss": 2.6604,
        "grad_norm": 2.730814218521118,
        "learning_rate": 0.00019730090212683338,
        "epoch": 0.05293333333333333,
        "step": 397
    },
    {
        "loss": 2.7047,
        "grad_norm": 3.1571404933929443,
        "learning_rate": 0.0001972863591277955,
        "epoch": 0.053066666666666665,
        "step": 398
    },
    {
        "loss": 2.5854,
        "grad_norm": 2.617784023284912,
        "learning_rate": 0.00019727177759329927,
        "epoch": 0.0532,
        "step": 399
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.5058062076568604,
        "learning_rate": 0.0001972571575291205,
        "epoch": 0.05333333333333334,
        "step": 400
    },
    {
        "loss": 2.8727,
        "grad_norm": 3.57842755317688,
        "learning_rate": 0.0001972424989410502,
        "epoch": 0.05346666666666667,
        "step": 401
    },
    {
        "loss": 2.6735,
        "grad_norm": 3.8150603771209717,
        "learning_rate": 0.00019722780183489475,
        "epoch": 0.0536,
        "step": 402
    },
    {
        "loss": 2.1745,
        "grad_norm": 2.3556954860687256,
        "learning_rate": 0.0001972130662164757,
        "epoch": 0.053733333333333334,
        "step": 403
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.5142409801483154,
        "learning_rate": 0.00019719829209162987,
        "epoch": 0.05386666666666667,
        "step": 404
    },
    {
        "loss": 3.6132,
        "grad_norm": 3.1881561279296875,
        "learning_rate": 0.00019718347946620933,
        "epoch": 0.054,
        "step": 405
    },
    {
        "loss": 1.0521,
        "grad_norm": 4.868746280670166,
        "learning_rate": 0.00019716862834608147,
        "epoch": 0.05413333333333333,
        "step": 406
    },
    {
        "loss": 2.687,
        "grad_norm": 2.376161575317383,
        "learning_rate": 0.0001971537387371288,
        "epoch": 0.054266666666666664,
        "step": 407
    },
    {
        "loss": 2.7588,
        "grad_norm": 2.49064564704895,
        "learning_rate": 0.0001971388106452492,
        "epoch": 0.0544,
        "step": 408
    },
    {
        "loss": 2.1589,
        "grad_norm": 2.9617128372192383,
        "learning_rate": 0.0001971238440763557,
        "epoch": 0.054533333333333336,
        "step": 409
    },
    {
        "loss": 1.712,
        "grad_norm": 3.7461209297180176,
        "learning_rate": 0.00019710883903637657,
        "epoch": 0.05466666666666667,
        "step": 410
    },
    {
        "loss": 2.8878,
        "grad_norm": 2.688175678253174,
        "learning_rate": 0.00019709379553125547,
        "epoch": 0.0548,
        "step": 411
    },
    {
        "loss": 2.6594,
        "grad_norm": 2.0077619552612305,
        "learning_rate": 0.00019707871356695108,
        "epoch": 0.054933333333333334,
        "step": 412
    },
    {
        "loss": 1.608,
        "grad_norm": 3.013753890991211,
        "learning_rate": 0.0001970635931494375,
        "epoch": 0.05506666666666667,
        "step": 413
    },
    {
        "loss": 1.1117,
        "grad_norm": 4.942859649658203,
        "learning_rate": 0.00019704843428470392,
        "epoch": 0.0552,
        "step": 414
    },
    {
        "loss": 2.7999,
        "grad_norm": 3.295839309692383,
        "learning_rate": 0.0001970332369787548,
        "epoch": 0.05533333333333333,
        "step": 415
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.2682266235351562,
        "learning_rate": 0.00019701800123760995,
        "epoch": 0.055466666666666664,
        "step": 416
    },
    {
        "loss": 1.5642,
        "grad_norm": 3.195842742919922,
        "learning_rate": 0.00019700272706730417,
        "epoch": 0.0556,
        "step": 417
    },
    {
        "loss": 1.3977,
        "grad_norm": 4.551549434661865,
        "learning_rate": 0.0001969874144738877,
        "epoch": 0.055733333333333336,
        "step": 418
    },
    {
        "loss": 3.0879,
        "grad_norm": 2.285914182662964,
        "learning_rate": 0.00019697206346342588,
        "epoch": 0.05586666666666667,
        "step": 419
    },
    {
        "loss": 2.1901,
        "grad_norm": 2.3136212825775146,
        "learning_rate": 0.00019695667404199927,
        "epoch": 0.056,
        "step": 420
    },
    {
        "loss": 2.7011,
        "grad_norm": 2.7038516998291016,
        "learning_rate": 0.00019694124621570373,
        "epoch": 0.056133333333333334,
        "step": 421
    },
    {
        "loss": 2.3084,
        "grad_norm": 2.5651204586029053,
        "learning_rate": 0.00019692577999065022,
        "epoch": 0.056266666666666666,
        "step": 422
    },
    {
        "loss": 2.7237,
        "grad_norm": 3.0835371017456055,
        "learning_rate": 0.000196910275372965,
        "epoch": 0.0564,
        "step": 423
    },
    {
        "loss": 2.9349,
        "grad_norm": 2.5424323081970215,
        "learning_rate": 0.00019689473236878948,
        "epoch": 0.05653333333333333,
        "step": 424
    },
    {
        "loss": 2.517,
        "grad_norm": 2.920579195022583,
        "learning_rate": 0.00019687915098428035,
        "epoch": 0.056666666666666664,
        "step": 425
    },
    {
        "loss": 1.9747,
        "grad_norm": 3.4498376846313477,
        "learning_rate": 0.00019686353122560937,
        "epoch": 0.0568,
        "step": 426
    },
    {
        "loss": 2.7341,
        "grad_norm": 2.1776134967803955,
        "learning_rate": 0.00019684787309896362,
        "epoch": 0.056933333333333336,
        "step": 427
    },
    {
        "loss": 2.8355,
        "grad_norm": 2.462602376937866,
        "learning_rate": 0.00019683217661054534,
        "epoch": 0.05706666666666667,
        "step": 428
    },
    {
        "loss": 2.0828,
        "grad_norm": 2.464195728302002,
        "learning_rate": 0.00019681644176657197,
        "epoch": 0.0572,
        "step": 429
    },
    {
        "loss": 3.1377,
        "grad_norm": 2.243450164794922,
        "learning_rate": 0.0001968006685732761,
        "epoch": 0.05733333333333333,
        "step": 430
    },
    {
        "loss": 2.5569,
        "grad_norm": 2.8438565731048584,
        "learning_rate": 0.00019678485703690556,
        "epoch": 0.057466666666666666,
        "step": 431
    },
    {
        "loss": 1.8937,
        "grad_norm": 7.1324028968811035,
        "learning_rate": 0.0001967690071637234,
        "epoch": 0.0576,
        "step": 432
    },
    {
        "loss": 2.5828,
        "grad_norm": 2.532607316970825,
        "learning_rate": 0.00019675311896000773,
        "epoch": 0.05773333333333333,
        "step": 433
    },
    {
        "loss": 2.5739,
        "grad_norm": 3.078474760055542,
        "learning_rate": 0.000196737192432052,
        "epoch": 0.057866666666666663,
        "step": 434
    },
    {
        "loss": 3.0546,
        "grad_norm": 3.3328988552093506,
        "learning_rate": 0.0001967212275861647,
        "epoch": 0.058,
        "step": 435
    },
    {
        "loss": 3.2031,
        "grad_norm": 2.6972475051879883,
        "learning_rate": 0.00019670522442866959,
        "epoch": 0.058133333333333335,
        "step": 436
    },
    {
        "loss": 1.8105,
        "grad_norm": 4.850458145141602,
        "learning_rate": 0.00019668918296590558,
        "epoch": 0.05826666666666667,
        "step": 437
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.7175745964050293,
        "learning_rate": 0.0001966731032042267,
        "epoch": 0.0584,
        "step": 438
    },
    {
        "loss": 2.9773,
        "grad_norm": 3.4363629817962646,
        "learning_rate": 0.0001966569851500023,
        "epoch": 0.05853333333333333,
        "step": 439
    },
    {
        "loss": 2.5448,
        "grad_norm": 3.1120076179504395,
        "learning_rate": 0.00019664082880961666,
        "epoch": 0.058666666666666666,
        "step": 440
    },
    {
        "loss": 2.4219,
        "grad_norm": 3.257417917251587,
        "learning_rate": 0.00019662463418946947,
        "epoch": 0.0588,
        "step": 441
    },
    {
        "loss": 2.6936,
        "grad_norm": 2.466876983642578,
        "learning_rate": 0.00019660840129597543,
        "epoch": 0.05893333333333333,
        "step": 442
    },
    {
        "loss": 1.1396,
        "grad_norm": 3.0295028686523438,
        "learning_rate": 0.00019659213013556442,
        "epoch": 0.05906666666666667,
        "step": 443
    },
    {
        "loss": 2.3984,
        "grad_norm": 2.479306936264038,
        "learning_rate": 0.0001965758207146816,
        "epoch": 0.0592,
        "step": 444
    },
    {
        "loss": 1.7064,
        "grad_norm": 2.52888560295105,
        "learning_rate": 0.00019655947303978707,
        "epoch": 0.059333333333333335,
        "step": 445
    },
    {
        "loss": 2.4281,
        "grad_norm": 4.006402015686035,
        "learning_rate": 0.00019654308711735628,
        "epoch": 0.05946666666666667,
        "step": 446
    },
    {
        "loss": 2.8147,
        "grad_norm": 4.1260480880737305,
        "learning_rate": 0.00019652666295387968,
        "epoch": 0.0596,
        "step": 447
    },
    {
        "loss": 2.5872,
        "grad_norm": 3.2573390007019043,
        "learning_rate": 0.00019651020055586304,
        "epoch": 0.05973333333333333,
        "step": 448
    },
    {
        "loss": 2.8168,
        "grad_norm": 3.2920138835906982,
        "learning_rate": 0.00019649369992982707,
        "epoch": 0.059866666666666665,
        "step": 449
    },
    {
        "loss": 2.1633,
        "grad_norm": 3.76646089553833,
        "learning_rate": 0.00019647716108230778,
        "epoch": 0.06,
        "step": 450
    },
    {
        "loss": 2.5704,
        "grad_norm": 2.7515974044799805,
        "learning_rate": 0.0001964605840198562,
        "epoch": 0.06013333333333333,
        "step": 451
    },
    {
        "loss": 2.0719,
        "grad_norm": 2.600325584411621,
        "learning_rate": 0.00019644396874903865,
        "epoch": 0.06026666666666667,
        "step": 452
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.925626039505005,
        "learning_rate": 0.00019642731527643646,
        "epoch": 0.0604,
        "step": 453
    },
    {
        "loss": 2.7038,
        "grad_norm": 2.0408034324645996,
        "learning_rate": 0.00019641062360864614,
        "epoch": 0.060533333333333335,
        "step": 454
    },
    {
        "loss": 1.3591,
        "grad_norm": 3.230139970779419,
        "learning_rate": 0.00019639389375227925,
        "epoch": 0.06066666666666667,
        "step": 455
    },
    {
        "loss": 2.1384,
        "grad_norm": 2.5226593017578125,
        "learning_rate": 0.00019637712571396262,
        "epoch": 0.0608,
        "step": 456
    },
    {
        "loss": 1.8632,
        "grad_norm": 2.4690139293670654,
        "learning_rate": 0.00019636031950033808,
        "epoch": 0.06093333333333333,
        "step": 457
    },
    {
        "loss": 2.3457,
        "grad_norm": 2.396857261657715,
        "learning_rate": 0.00019634347511806262,
        "epoch": 0.061066666666666665,
        "step": 458
    },
    {
        "loss": 3.2041,
        "grad_norm": 1.8030623197555542,
        "learning_rate": 0.00019632659257380844,
        "epoch": 0.0612,
        "step": 459
    },
    {
        "loss": 2.6648,
        "grad_norm": 3.2053818702697754,
        "learning_rate": 0.00019630967187426267,
        "epoch": 0.06133333333333333,
        "step": 460
    },
    {
        "loss": 2.4174,
        "grad_norm": 2.6899490356445312,
        "learning_rate": 0.00019629271302612773,
        "epoch": 0.06146666666666667,
        "step": 461
    },
    {
        "loss": 2.8999,
        "grad_norm": 3.1899242401123047,
        "learning_rate": 0.00019627571603612106,
        "epoch": 0.0616,
        "step": 462
    },
    {
        "loss": 2.7915,
        "grad_norm": 2.9877569675445557,
        "learning_rate": 0.0001962586809109752,
        "epoch": 0.061733333333333335,
        "step": 463
    },
    {
        "loss": 3.3813,
        "grad_norm": 3.243807315826416,
        "learning_rate": 0.00019624160765743782,
        "epoch": 0.06186666666666667,
        "step": 464
    },
    {
        "loss": 2.2335,
        "grad_norm": 3.226785659790039,
        "learning_rate": 0.00019622449628227173,
        "epoch": 0.062,
        "step": 465
    },
    {
        "loss": 2.5856,
        "grad_norm": 2.705794334411621,
        "learning_rate": 0.0001962073467922548,
        "epoch": 0.06213333333333333,
        "step": 466
    },
    {
        "loss": 2.7538,
        "grad_norm": 3.346846580505371,
        "learning_rate": 0.00019619015919417998,
        "epoch": 0.062266666666666665,
        "step": 467
    },
    {
        "loss": 2.0007,
        "grad_norm": 2.0155584812164307,
        "learning_rate": 0.00019617293349485533,
        "epoch": 0.0624,
        "step": 468
    },
    {
        "loss": 2.3549,
        "grad_norm": 4.197046279907227,
        "learning_rate": 0.00019615566970110403,
        "epoch": 0.06253333333333333,
        "step": 469
    },
    {
        "loss": 1.4981,
        "grad_norm": 2.998985767364502,
        "learning_rate": 0.0001961383678197643,
        "epoch": 0.06266666666666666,
        "step": 470
    },
    {
        "loss": 2.8808,
        "grad_norm": 1.6474111080169678,
        "learning_rate": 0.00019612102785768955,
        "epoch": 0.0628,
        "step": 471
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.3753771781921387,
        "learning_rate": 0.00019610364982174808,
        "epoch": 0.06293333333333333,
        "step": 472
    },
    {
        "loss": 2.6613,
        "grad_norm": 2.78731369972229,
        "learning_rate": 0.00019608623371882348,
        "epoch": 0.06306666666666666,
        "step": 473
    },
    {
        "loss": 2.5381,
        "grad_norm": 2.4152064323425293,
        "learning_rate": 0.00019606877955581428,
        "epoch": 0.0632,
        "step": 474
    },
    {
        "loss": 2.7878,
        "grad_norm": 2.709759473800659,
        "learning_rate": 0.00019605128733963417,
        "epoch": 0.06333333333333334,
        "step": 475
    },
    {
        "loss": 2.5886,
        "grad_norm": 2.7837436199188232,
        "learning_rate": 0.00019603375707721185,
        "epoch": 0.06346666666666667,
        "step": 476
    },
    {
        "loss": 2.2323,
        "grad_norm": 2.7496304512023926,
        "learning_rate": 0.00019601618877549112,
        "epoch": 0.0636,
        "step": 477
    },
    {
        "loss": 2.71,
        "grad_norm": 3.1715314388275146,
        "learning_rate": 0.00019599858244143085,
        "epoch": 0.06373333333333334,
        "step": 478
    },
    {
        "loss": 2.8993,
        "grad_norm": 3.205785036087036,
        "learning_rate": 0.00019598093808200498,
        "epoch": 0.06386666666666667,
        "step": 479
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.812913656234741,
        "learning_rate": 0.00019596325570420248,
        "epoch": 0.064,
        "step": 480
    },
    {
        "loss": 2.3541,
        "grad_norm": 2.7592031955718994,
        "learning_rate": 0.0001959455353150274,
        "epoch": 0.06413333333333333,
        "step": 481
    },
    {
        "loss": 2.5379,
        "grad_norm": 2.825174570083618,
        "learning_rate": 0.00019592777692149888,
        "epoch": 0.06426666666666667,
        "step": 482
    },
    {
        "loss": 1.8706,
        "grad_norm": 3.814976215362549,
        "learning_rate": 0.00019590998053065103,
        "epoch": 0.0644,
        "step": 483
    },
    {
        "loss": 2.4217,
        "grad_norm": 3.2522192001342773,
        "learning_rate": 0.0001958921461495331,
        "epoch": 0.06453333333333333,
        "step": 484
    },
    {
        "loss": 1.7811,
        "grad_norm": 2.984968662261963,
        "learning_rate": 0.00019587427378520934,
        "epoch": 0.06466666666666666,
        "step": 485
    },
    {
        "loss": 0.986,
        "grad_norm": 2.6745662689208984,
        "learning_rate": 0.00019585636344475905,
        "epoch": 0.0648,
        "step": 486
    },
    {
        "loss": 2.7151,
        "grad_norm": 1.8120877742767334,
        "learning_rate": 0.00019583841513527657,
        "epoch": 0.06493333333333333,
        "step": 487
    },
    {
        "loss": 2.7571,
        "grad_norm": 2.737593173980713,
        "learning_rate": 0.0001958204288638713,
        "epoch": 0.06506666666666666,
        "step": 488
    },
    {
        "loss": 1.7782,
        "grad_norm": 2.586516857147217,
        "learning_rate": 0.00019580240463766763,
        "epoch": 0.0652,
        "step": 489
    },
    {
        "loss": 2.5453,
        "grad_norm": 3.8195366859436035,
        "learning_rate": 0.00019578434246380507,
        "epoch": 0.06533333333333333,
        "step": 490
    },
    {
        "loss": 2.3528,
        "grad_norm": 3.6229915618896484,
        "learning_rate": 0.00019576624234943807,
        "epoch": 0.06546666666666667,
        "step": 491
    },
    {
        "loss": 2.7095,
        "grad_norm": 3.275062084197998,
        "learning_rate": 0.00019574810430173618,
        "epoch": 0.0656,
        "step": 492
    },
    {
        "loss": 2.6991,
        "grad_norm": 3.255872964859009,
        "learning_rate": 0.0001957299283278839,
        "epoch": 0.06573333333333334,
        "step": 493
    },
    {
        "loss": 2.1972,
        "grad_norm": 2.4241063594818115,
        "learning_rate": 0.0001957117144350808,
        "epoch": 0.06586666666666667,
        "step": 494
    },
    {
        "loss": 2.2383,
        "grad_norm": 3.671052932739258,
        "learning_rate": 0.00019569346263054153,
        "epoch": 0.066,
        "step": 495
    },
    {
        "loss": 1.9481,
        "grad_norm": 2.321066379547119,
        "learning_rate": 0.00019567517292149562,
        "epoch": 0.06613333333333334,
        "step": 496
    },
    {
        "loss": 1.6573,
        "grad_norm": 2.698700428009033,
        "learning_rate": 0.00019565684531518772,
        "epoch": 0.06626666666666667,
        "step": 497
    },
    {
        "loss": 2.5885,
        "grad_norm": 4.081693172454834,
        "learning_rate": 0.00019563847981887743,
        "epoch": 0.0664,
        "step": 498
    },
    {
        "loss": 2.343,
        "grad_norm": 3.0207629203796387,
        "learning_rate": 0.00019562007643983943,
        "epoch": 0.06653333333333333,
        "step": 499
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.419132709503174,
        "learning_rate": 0.00019560163518536332,
        "epoch": 0.06666666666666667,
        "step": 500
    },
    {
        "loss": 3.0246,
        "grad_norm": 2.9558780193328857,
        "learning_rate": 0.00019558315606275377,
        "epoch": 0.0668,
        "step": 501
    },
    {
        "loss": 1.0808,
        "grad_norm": 4.312302112579346,
        "learning_rate": 0.00019556463907933038,
        "epoch": 0.06693333333333333,
        "step": 502
    },
    {
        "loss": 2.6152,
        "grad_norm": 3.1232874393463135,
        "learning_rate": 0.00019554608424242787,
        "epoch": 0.06706666666666666,
        "step": 503
    },
    {
        "loss": 1.396,
        "grad_norm": 4.834222793579102,
        "learning_rate": 0.00019552749155939578,
        "epoch": 0.0672,
        "step": 504
    },
    {
        "loss": 2.7601,
        "grad_norm": 2.943286895751953,
        "learning_rate": 0.0001955088610375988,
        "epoch": 0.06733333333333333,
        "step": 505
    },
    {
        "loss": 3.0231,
        "grad_norm": 1.9718897342681885,
        "learning_rate": 0.00019549019268441656,
        "epoch": 0.06746666666666666,
        "step": 506
    },
    {
        "loss": 0.9615,
        "grad_norm": 3.2766971588134766,
        "learning_rate": 0.00019547148650724358,
        "epoch": 0.0676,
        "step": 507
    },
    {
        "loss": 3.0746,
        "grad_norm": 2.825094699859619,
        "learning_rate": 0.00019545274251348945,
        "epoch": 0.06773333333333334,
        "step": 508
    },
    {
        "loss": 2.2464,
        "grad_norm": 3.5259876251220703,
        "learning_rate": 0.00019543396071057884,
        "epoch": 0.06786666666666667,
        "step": 509
    },
    {
        "loss": 2.0574,
        "grad_norm": 3.5317142009735107,
        "learning_rate": 0.0001954151411059512,
        "epoch": 0.068,
        "step": 510
    },
    {
        "loss": 2.598,
        "grad_norm": 2.0113563537597656,
        "learning_rate": 0.000195396283707061,
        "epoch": 0.06813333333333334,
        "step": 511
    },
    {
        "loss": 2.9948,
        "grad_norm": 2.7051637172698975,
        "learning_rate": 0.0001953773885213778,
        "epoch": 0.06826666666666667,
        "step": 512
    },
    {
        "loss": 2.4653,
        "grad_norm": 3.5489587783813477,
        "learning_rate": 0.00019535845555638599,
        "epoch": 0.0684,
        "step": 513
    },
    {
        "loss": 0.6227,
        "grad_norm": 3.23463773727417,
        "learning_rate": 0.00019533948481958503,
        "epoch": 0.06853333333333333,
        "step": 514
    },
    {
        "loss": 3.121,
        "grad_norm": 2.0896685123443604,
        "learning_rate": 0.00019532047631848926,
        "epoch": 0.06866666666666667,
        "step": 515
    },
    {
        "loss": 2.7932,
        "grad_norm": 2.9802303314208984,
        "learning_rate": 0.00019530143006062805,
        "epoch": 0.0688,
        "step": 516
    },
    {
        "loss": 1.3891,
        "grad_norm": 4.816723823547363,
        "learning_rate": 0.00019528234605354562,
        "epoch": 0.06893333333333333,
        "step": 517
    },
    {
        "loss": 0.8959,
        "grad_norm": 3.724613666534424,
        "learning_rate": 0.00019526322430480129,
        "epoch": 0.06906666666666667,
        "step": 518
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.5228636264801025,
        "learning_rate": 0.0001952440648219692,
        "epoch": 0.0692,
        "step": 519
    },
    {
        "loss": 3.1286,
        "grad_norm": 3.894688129425049,
        "learning_rate": 0.00019522486761263847,
        "epoch": 0.06933333333333333,
        "step": 520
    },
    {
        "loss": 1.4443,
        "grad_norm": 4.424625396728516,
        "learning_rate": 0.00019520563268441324,
        "epoch": 0.06946666666666666,
        "step": 521
    },
    {
        "loss": 2.8899,
        "grad_norm": 2.230360269546509,
        "learning_rate": 0.0001951863600449125,
        "epoch": 0.0696,
        "step": 522
    },
    {
        "loss": 1.9032,
        "grad_norm": 3.5284314155578613,
        "learning_rate": 0.0001951670497017702,
        "epoch": 0.06973333333333333,
        "step": 523
    },
    {
        "loss": 2.2389,
        "grad_norm": 3.086810350418091,
        "learning_rate": 0.00019514770166263523,
        "epoch": 0.06986666666666666,
        "step": 524
    },
    {
        "loss": 0.9867,
        "grad_norm": 4.984787940979004,
        "learning_rate": 0.00019512831593517142,
        "epoch": 0.07,
        "step": 525
    },
    {
        "loss": 1.5635,
        "grad_norm": 4.784347057342529,
        "learning_rate": 0.00019510889252705754,
        "epoch": 0.07013333333333334,
        "step": 526
    },
    {
        "loss": 1.9958,
        "grad_norm": 2.4100301265716553,
        "learning_rate": 0.00019508943144598727,
        "epoch": 0.07026666666666667,
        "step": 527
    },
    {
        "loss": 3.3528,
        "grad_norm": 3.2537832260131836,
        "learning_rate": 0.00019506993269966918,
        "epoch": 0.0704,
        "step": 528
    },
    {
        "loss": 2.702,
        "grad_norm": 2.483752965927124,
        "learning_rate": 0.00019505039629582677,
        "epoch": 0.07053333333333334,
        "step": 529
    },
    {
        "loss": 2.3794,
        "grad_norm": 4.983580589294434,
        "learning_rate": 0.00019503082224219854,
        "epoch": 0.07066666666666667,
        "step": 530
    },
    {
        "loss": 2.7679,
        "grad_norm": 1.9990559816360474,
        "learning_rate": 0.0001950112105465378,
        "epoch": 0.0708,
        "step": 531
    },
    {
        "loss": 2.2897,
        "grad_norm": 2.1758811473846436,
        "learning_rate": 0.00019499156121661286,
        "epoch": 0.07093333333333333,
        "step": 532
    },
    {
        "loss": 1.7543,
        "grad_norm": 3.2746503353118896,
        "learning_rate": 0.00019497187426020682,
        "epoch": 0.07106666666666667,
        "step": 533
    },
    {
        "loss": 2.4433,
        "grad_norm": 2.489814281463623,
        "learning_rate": 0.00019495214968511776,
        "epoch": 0.0712,
        "step": 534
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.260878562927246,
        "learning_rate": 0.0001949323874991587,
        "epoch": 0.07133333333333333,
        "step": 535
    },
    {
        "loss": 2.6947,
        "grad_norm": 4.564321517944336,
        "learning_rate": 0.00019491258771015747,
        "epoch": 0.07146666666666666,
        "step": 536
    },
    {
        "loss": 2.4764,
        "grad_norm": 2.521651268005371,
        "learning_rate": 0.0001948927503259568,
        "epoch": 0.0716,
        "step": 537
    },
    {
        "loss": 1.9574,
        "grad_norm": 3.936685800552368,
        "learning_rate": 0.00019487287535441443,
        "epoch": 0.07173333333333333,
        "step": 538
    },
    {
        "loss": 2.4665,
        "grad_norm": 2.854445695877075,
        "learning_rate": 0.00019485296280340287,
        "epoch": 0.07186666666666666,
        "step": 539
    },
    {
        "loss": 2.217,
        "grad_norm": 2.660874366760254,
        "learning_rate": 0.00019483301268080952,
        "epoch": 0.072,
        "step": 540
    },
    {
        "loss": 1.4217,
        "grad_norm": 4.413167476654053,
        "learning_rate": 0.00019481302499453672,
        "epoch": 0.07213333333333333,
        "step": 541
    },
    {
        "loss": 2.0532,
        "grad_norm": 2.8432838916778564,
        "learning_rate": 0.00019479299975250162,
        "epoch": 0.07226666666666667,
        "step": 542
    },
    {
        "loss": 1.8555,
        "grad_norm": 3.5488972663879395,
        "learning_rate": 0.00019477293696263634,
        "epoch": 0.0724,
        "step": 543
    },
    {
        "loss": 2.4135,
        "grad_norm": 3.2288568019866943,
        "learning_rate": 0.0001947528366328878,
        "epoch": 0.07253333333333334,
        "step": 544
    },
    {
        "loss": 1.1317,
        "grad_norm": 2.6533262729644775,
        "learning_rate": 0.00019473269877121781,
        "epoch": 0.07266666666666667,
        "step": 545
    },
    {
        "loss": 1.9619,
        "grad_norm": 3.4822041988372803,
        "learning_rate": 0.00019471252338560305,
        "epoch": 0.0728,
        "step": 546
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.230327606201172,
        "learning_rate": 0.00019469231048403502,
        "epoch": 0.07293333333333334,
        "step": 547
    },
    {
        "loss": 1.3135,
        "grad_norm": 4.333511829376221,
        "learning_rate": 0.00019467206007452016,
        "epoch": 0.07306666666666667,
        "step": 548
    },
    {
        "loss": 2.1049,
        "grad_norm": 3.311378240585327,
        "learning_rate": 0.00019465177216507974,
        "epoch": 0.0732,
        "step": 549
    },
    {
        "loss": 2.5036,
        "grad_norm": 2.7416818141937256,
        "learning_rate": 0.00019463144676374984,
        "epoch": 0.07333333333333333,
        "step": 550
    },
    {
        "loss": 3.1858,
        "grad_norm": 2.0317203998565674,
        "learning_rate": 0.00019461108387858143,
        "epoch": 0.07346666666666667,
        "step": 551
    },
    {
        "loss": 3.7707,
        "grad_norm": 6.2564826011657715,
        "learning_rate": 0.00019459068351764032,
        "epoch": 0.0736,
        "step": 552
    },
    {
        "loss": 3.1495,
        "grad_norm": 2.345219373703003,
        "learning_rate": 0.00019457024568900715,
        "epoch": 0.07373333333333333,
        "step": 553
    },
    {
        "loss": 2.7784,
        "grad_norm": 2.842146158218384,
        "learning_rate": 0.00019454977040077743,
        "epoch": 0.07386666666666666,
        "step": 554
    },
    {
        "loss": 2.1765,
        "grad_norm": 2.711702346801758,
        "learning_rate": 0.00019452925766106147,
        "epoch": 0.074,
        "step": 555
    },
    {
        "loss": 2.0341,
        "grad_norm": 5.993376731872559,
        "learning_rate": 0.00019450870747798446,
        "epoch": 0.07413333333333333,
        "step": 556
    },
    {
        "loss": 2.0191,
        "grad_norm": 2.3991596698760986,
        "learning_rate": 0.00019448811985968637,
        "epoch": 0.07426666666666666,
        "step": 557
    },
    {
        "loss": 2.2954,
        "grad_norm": 2.5445008277893066,
        "learning_rate": 0.00019446749481432206,
        "epoch": 0.0744,
        "step": 558
    },
    {
        "loss": 2.3114,
        "grad_norm": 3.709003448486328,
        "learning_rate": 0.0001944468323500612,
        "epoch": 0.07453333333333333,
        "step": 559
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.2807514667510986,
        "learning_rate": 0.00019442613247508818,
        "epoch": 0.07466666666666667,
        "step": 560
    },
    {
        "loss": 1.7274,
        "grad_norm": 2.11456561088562,
        "learning_rate": 0.00019440539519760232,
        "epoch": 0.0748,
        "step": 561
    },
    {
        "loss": 2.3187,
        "grad_norm": 2.452828884124756,
        "learning_rate": 0.0001943846205258178,
        "epoch": 0.07493333333333334,
        "step": 562
    },
    {
        "loss": 3.068,
        "grad_norm": 3.4282126426696777,
        "learning_rate": 0.00019436380846796342,
        "epoch": 0.07506666666666667,
        "step": 563
    },
    {
        "loss": 2.6487,
        "grad_norm": 3.423126220703125,
        "learning_rate": 0.00019434295903228302,
        "epoch": 0.0752,
        "step": 564
    },
    {
        "loss": 2.8189,
        "grad_norm": 2.833725690841675,
        "learning_rate": 0.00019432207222703504,
        "epoch": 0.07533333333333334,
        "step": 565
    },
    {
        "loss": 2.509,
        "grad_norm": 2.9424195289611816,
        "learning_rate": 0.0001943011480604929,
        "epoch": 0.07546666666666667,
        "step": 566
    },
    {
        "loss": 3.095,
        "grad_norm": 2.9899325370788574,
        "learning_rate": 0.00019428018654094465,
        "epoch": 0.0756,
        "step": 567
    },
    {
        "loss": 1.8159,
        "grad_norm": 3.8799264430999756,
        "learning_rate": 0.0001942591876766933,
        "epoch": 0.07573333333333333,
        "step": 568
    },
    {
        "loss": 1.2884,
        "grad_norm": 4.863813877105713,
        "learning_rate": 0.0001942381514760565,
        "epoch": 0.07586666666666667,
        "step": 569
    },
    {
        "loss": 2.7721,
        "grad_norm": 2.5403831005096436,
        "learning_rate": 0.0001942170779473668,
        "epoch": 0.076,
        "step": 570
    },
    {
        "loss": 3.3411,
        "grad_norm": 2.6404571533203125,
        "learning_rate": 0.00019419596709897147,
        "epoch": 0.07613333333333333,
        "step": 571
    },
    {
        "loss": 2.0819,
        "grad_norm": 2.93428635597229,
        "learning_rate": 0.00019417481893923265,
        "epoch": 0.07626666666666666,
        "step": 572
    },
    {
        "loss": 2.7249,
        "grad_norm": 6.369941234588623,
        "learning_rate": 0.00019415363347652714,
        "epoch": 0.0764,
        "step": 573
    },
    {
        "loss": 2.7377,
        "grad_norm": 3.0246357917785645,
        "learning_rate": 0.00019413241071924655,
        "epoch": 0.07653333333333333,
        "step": 574
    },
    {
        "loss": 2.0576,
        "grad_norm": 3.743582248687744,
        "learning_rate": 0.00019411115067579737,
        "epoch": 0.07666666666666666,
        "step": 575
    },
    {
        "loss": 2.4896,
        "grad_norm": 3.2913482189178467,
        "learning_rate": 0.00019408985335460073,
        "epoch": 0.0768,
        "step": 576
    },
    {
        "loss": 1.9402,
        "grad_norm": 2.53353214263916,
        "learning_rate": 0.00019406851876409254,
        "epoch": 0.07693333333333334,
        "step": 577
    },
    {
        "loss": 2.4987,
        "grad_norm": 3.0610389709472656,
        "learning_rate": 0.00019404714691272356,
        "epoch": 0.07706666666666667,
        "step": 578
    },
    {
        "loss": 1.0638,
        "grad_norm": 3.488877534866333,
        "learning_rate": 0.00019402573780895923,
        "epoch": 0.0772,
        "step": 579
    },
    {
        "loss": 0.9124,
        "grad_norm": 2.9167261123657227,
        "learning_rate": 0.00019400429146127976,
        "epoch": 0.07733333333333334,
        "step": 580
    },
    {
        "loss": 1.9498,
        "grad_norm": 4.921224594116211,
        "learning_rate": 0.00019398280787818014,
        "epoch": 0.07746666666666667,
        "step": 581
    },
    {
        "loss": 2.9425,
        "grad_norm": 2.6779627799987793,
        "learning_rate": 0.0001939612870681701,
        "epoch": 0.0776,
        "step": 582
    },
    {
        "loss": 2.7926,
        "grad_norm": 2.232550859451294,
        "learning_rate": 0.00019393972903977404,
        "epoch": 0.07773333333333333,
        "step": 583
    },
    {
        "loss": 2.0094,
        "grad_norm": 3.022355318069458,
        "learning_rate": 0.00019391813380153122,
        "epoch": 0.07786666666666667,
        "step": 584
    },
    {
        "loss": 2.8424,
        "grad_norm": 2.6010448932647705,
        "learning_rate": 0.00019389650136199564,
        "epoch": 0.078,
        "step": 585
    },
    {
        "loss": 2.9015,
        "grad_norm": 1.982020616531372,
        "learning_rate": 0.00019387483172973589,
        "epoch": 0.07813333333333333,
        "step": 586
    },
    {
        "loss": 2.1767,
        "grad_norm": 3.388734817504883,
        "learning_rate": 0.00019385312491333541,
        "epoch": 0.07826666666666666,
        "step": 587
    },
    {
        "loss": 3.0814,
        "grad_norm": 3.285825252532959,
        "learning_rate": 0.00019383138092139238,
        "epoch": 0.0784,
        "step": 588
    },
    {
        "loss": 2.9704,
        "grad_norm": 2.067652702331543,
        "learning_rate": 0.00019380959976251962,
        "epoch": 0.07853333333333333,
        "step": 589
    },
    {
        "loss": 2.3926,
        "grad_norm": 3.277916431427002,
        "learning_rate": 0.00019378778144534475,
        "epoch": 0.07866666666666666,
        "step": 590
    },
    {
        "loss": 3.0163,
        "grad_norm": 4.4560112953186035,
        "learning_rate": 0.00019376592597851008,
        "epoch": 0.0788,
        "step": 591
    },
    {
        "loss": 1.6769,
        "grad_norm": 3.415745258331299,
        "learning_rate": 0.00019374403337067263,
        "epoch": 0.07893333333333333,
        "step": 592
    },
    {
        "loss": 1.6493,
        "grad_norm": 4.230783939361572,
        "learning_rate": 0.00019372210363050415,
        "epoch": 0.07906666666666666,
        "step": 593
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.9754106998443604,
        "learning_rate": 0.00019370013676669106,
        "epoch": 0.0792,
        "step": 594
    },
    {
        "loss": 1.8374,
        "grad_norm": 4.054150581359863,
        "learning_rate": 0.0001936781327879345,
        "epoch": 0.07933333333333334,
        "step": 595
    },
    {
        "loss": 1.8721,
        "grad_norm": 3.1250898838043213,
        "learning_rate": 0.00019365609170295039,
        "epoch": 0.07946666666666667,
        "step": 596
    },
    {
        "loss": 2.394,
        "grad_norm": 3.0073578357696533,
        "learning_rate": 0.0001936340135204692,
        "epoch": 0.0796,
        "step": 597
    },
    {
        "loss": 2.0607,
        "grad_norm": 3.008039712905884,
        "learning_rate": 0.00019361189824923622,
        "epoch": 0.07973333333333334,
        "step": 598
    },
    {
        "loss": 2.8141,
        "grad_norm": 2.658879518508911,
        "learning_rate": 0.00019358974589801133,
        "epoch": 0.07986666666666667,
        "step": 599
    },
    {
        "loss": 2.3256,
        "grad_norm": 2.755740165710449,
        "learning_rate": 0.00019356755647556922,
        "epoch": 0.08,
        "step": 600
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.990689992904663,
        "learning_rate": 0.0001935453299906992,
        "epoch": 0.08013333333333333,
        "step": 601
    },
    {
        "loss": 2.4807,
        "grad_norm": 3.001444101333618,
        "learning_rate": 0.00019352306645220517,
        "epoch": 0.08026666666666667,
        "step": 602
    },
    {
        "loss": 1.889,
        "grad_norm": 3.1931440830230713,
        "learning_rate": 0.00019350076586890585,
        "epoch": 0.0804,
        "step": 603
    },
    {
        "loss": 1.6481,
        "grad_norm": 3.1573824882507324,
        "learning_rate": 0.0001934784282496346,
        "epoch": 0.08053333333333333,
        "step": 604
    },
    {
        "loss": 2.4987,
        "grad_norm": 2.8755574226379395,
        "learning_rate": 0.00019345605360323939,
        "epoch": 0.08066666666666666,
        "step": 605
    },
    {
        "loss": 2.0131,
        "grad_norm": 3.2716481685638428,
        "learning_rate": 0.0001934336419385829,
        "epoch": 0.0808,
        "step": 606
    },
    {
        "loss": 1.6687,
        "grad_norm": 5.252993106842041,
        "learning_rate": 0.00019341119326454247,
        "epoch": 0.08093333333333333,
        "step": 607
    },
    {
        "loss": 3.4432,
        "grad_norm": 3.4439263343811035,
        "learning_rate": 0.00019338870759001012,
        "epoch": 0.08106666666666666,
        "step": 608
    },
    {
        "loss": 2.7758,
        "grad_norm": 3.3393125534057617,
        "learning_rate": 0.00019336618492389243,
        "epoch": 0.0812,
        "step": 609
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.465730905532837,
        "learning_rate": 0.0001933436252751108,
        "epoch": 0.08133333333333333,
        "step": 610
    },
    {
        "loss": 3.4813,
        "grad_norm": 2.4668195247650146,
        "learning_rate": 0.00019332102865260114,
        "epoch": 0.08146666666666667,
        "step": 611
    },
    {
        "loss": 3.1852,
        "grad_norm": 1.96367347240448,
        "learning_rate": 0.000193298395065314,
        "epoch": 0.0816,
        "step": 612
    },
    {
        "loss": 2.0515,
        "grad_norm": 3.107571840286255,
        "learning_rate": 0.0001932757245222147,
        "epoch": 0.08173333333333334,
        "step": 613
    },
    {
        "loss": 2.9985,
        "grad_norm": 2.9029123783111572,
        "learning_rate": 0.00019325301703228308,
        "epoch": 0.08186666666666667,
        "step": 614
    },
    {
        "loss": 2.3473,
        "grad_norm": 3.0084855556488037,
        "learning_rate": 0.00019323027260451366,
        "epoch": 0.082,
        "step": 615
    },
    {
        "loss": 2.5499,
        "grad_norm": 2.616152286529541,
        "learning_rate": 0.0001932074912479156,
        "epoch": 0.08213333333333334,
        "step": 616
    },
    {
        "loss": 2.412,
        "grad_norm": 3.6036922931671143,
        "learning_rate": 0.00019318467297151266,
        "epoch": 0.08226666666666667,
        "step": 617
    },
    {
        "loss": 1.6523,
        "grad_norm": 2.5521414279937744,
        "learning_rate": 0.00019316181778434324,
        "epoch": 0.0824,
        "step": 618
    },
    {
        "loss": 3.0389,
        "grad_norm": 2.584150791168213,
        "learning_rate": 0.00019313892569546031,
        "epoch": 0.08253333333333333,
        "step": 619
    },
    {
        "loss": 2.2993,
        "grad_norm": 3.073050022125244,
        "learning_rate": 0.0001931159967139316,
        "epoch": 0.08266666666666667,
        "step": 620
    },
    {
        "loss": 2.0661,
        "grad_norm": 4.501476764678955,
        "learning_rate": 0.00019309303084883933,
        "epoch": 0.0828,
        "step": 621
    },
    {
        "loss": 2.4966,
        "grad_norm": 2.4066901206970215,
        "learning_rate": 0.00019307002810928028,
        "epoch": 0.08293333333333333,
        "step": 622
    },
    {
        "loss": 2.1876,
        "grad_norm": 2.9589269161224365,
        "learning_rate": 0.00019304698850436598,
        "epoch": 0.08306666666666666,
        "step": 623
    },
    {
        "loss": 2.2973,
        "grad_norm": 5.5127692222595215,
        "learning_rate": 0.00019302391204322248,
        "epoch": 0.0832,
        "step": 624
    },
    {
        "loss": 2.9921,
        "grad_norm": 2.5345218181610107,
        "learning_rate": 0.00019300079873499045,
        "epoch": 0.08333333333333333,
        "step": 625
    },
    {
        "loss": 2.3943,
        "grad_norm": 2.801790714263916,
        "learning_rate": 0.00019297764858882514,
        "epoch": 0.08346666666666666,
        "step": 626
    },
    {
        "loss": 2.8175,
        "grad_norm": 3.512465238571167,
        "learning_rate": 0.00019295446161389642,
        "epoch": 0.0836,
        "step": 627
    },
    {
        "loss": 2.6471,
        "grad_norm": 2.582038402557373,
        "learning_rate": 0.00019293123781938872,
        "epoch": 0.08373333333333334,
        "step": 628
    },
    {
        "loss": 2.6419,
        "grad_norm": 3.013720750808716,
        "learning_rate": 0.00019290797721450108,
        "epoch": 0.08386666666666667,
        "step": 629
    },
    {
        "loss": 2.0581,
        "grad_norm": 4.018062591552734,
        "learning_rate": 0.00019288467980844708,
        "epoch": 0.084,
        "step": 630
    },
    {
        "loss": 3.782,
        "grad_norm": 2.982865333557129,
        "learning_rate": 0.0001928613456104549,
        "epoch": 0.08413333333333334,
        "step": 631
    },
    {
        "loss": 2.5257,
        "grad_norm": 3.9127023220062256,
        "learning_rate": 0.0001928379746297673,
        "epoch": 0.08426666666666667,
        "step": 632
    },
    {
        "loss": 2.6758,
        "grad_norm": 3.356558084487915,
        "learning_rate": 0.00019281456687564164,
        "epoch": 0.0844,
        "step": 633
    },
    {
        "loss": 2.2933,
        "grad_norm": 3.333223819732666,
        "learning_rate": 0.00019279112235734977,
        "epoch": 0.08453333333333334,
        "step": 634
    },
    {
        "loss": 2.6908,
        "grad_norm": 2.393747091293335,
        "learning_rate": 0.00019276764108417814,
        "epoch": 0.08466666666666667,
        "step": 635
    },
    {
        "loss": 1.7251,
        "grad_norm": 3.629150629043579,
        "learning_rate": 0.00019274412306542779,
        "epoch": 0.0848,
        "step": 636
    },
    {
        "loss": 2.9093,
        "grad_norm": 2.327314853668213,
        "learning_rate": 0.00019272056831041428,
        "epoch": 0.08493333333333333,
        "step": 637
    },
    {
        "loss": 2.7379,
        "grad_norm": 2.7611441612243652,
        "learning_rate": 0.00019269697682846774,
        "epoch": 0.08506666666666667,
        "step": 638
    },
    {
        "loss": 3.3322,
        "grad_norm": 2.6123673915863037,
        "learning_rate": 0.0001926733486289328,
        "epoch": 0.0852,
        "step": 639
    },
    {
        "loss": 2.3364,
        "grad_norm": 2.0385518074035645,
        "learning_rate": 0.0001926496837211687,
        "epoch": 0.08533333333333333,
        "step": 640
    },
    {
        "loss": 3.0814,
        "grad_norm": 2.5566985607147217,
        "learning_rate": 0.0001926259821145492,
        "epoch": 0.08546666666666666,
        "step": 641
    },
    {
        "loss": 0.6089,
        "grad_norm": 3.3272321224212646,
        "learning_rate": 0.00019260224381846251,
        "epoch": 0.0856,
        "step": 642
    },
    {
        "loss": 1.831,
        "grad_norm": 2.288863182067871,
        "learning_rate": 0.00019257846884231157,
        "epoch": 0.08573333333333333,
        "step": 643
    },
    {
        "loss": 0.8975,
        "grad_norm": 3.1499862670898438,
        "learning_rate": 0.00019255465719551364,
        "epoch": 0.08586666666666666,
        "step": 644
    },
    {
        "loss": 2.1251,
        "grad_norm": 3.998661994934082,
        "learning_rate": 0.00019253080888750063,
        "epoch": 0.086,
        "step": 645
    },
    {
        "loss": 3.2846,
        "grad_norm": 2.1633036136627197,
        "learning_rate": 0.00019250692392771892,
        "epoch": 0.08613333333333334,
        "step": 646
    },
    {
        "loss": 1.6899,
        "grad_norm": 4.021085739135742,
        "learning_rate": 0.00019248300232562943,
        "epoch": 0.08626666666666667,
        "step": 647
    },
    {
        "loss": 1.6892,
        "grad_norm": 4.253148078918457,
        "learning_rate": 0.00019245904409070762,
        "epoch": 0.0864,
        "step": 648
    },
    {
        "loss": 1.0706,
        "grad_norm": 3.881463050842285,
        "learning_rate": 0.00019243504923244337,
        "epoch": 0.08653333333333334,
        "step": 649
    },
    {
        "loss": 2.2072,
        "grad_norm": 2.548654794692993,
        "learning_rate": 0.00019241101776034116,
        "epoch": 0.08666666666666667,
        "step": 650
    },
    {
        "loss": 2.3042,
        "grad_norm": 3.343564748764038,
        "learning_rate": 0.00019238694968391995,
        "epoch": 0.0868,
        "step": 651
    },
    {
        "loss": 2.8232,
        "grad_norm": 3.8970236778259277,
        "learning_rate": 0.00019236284501271316,
        "epoch": 0.08693333333333333,
        "step": 652
    },
    {
        "loss": 0.5956,
        "grad_norm": 3.223599910736084,
        "learning_rate": 0.00019233870375626872,
        "epoch": 0.08706666666666667,
        "step": 653
    },
    {
        "loss": 2.1141,
        "grad_norm": 2.18396258354187,
        "learning_rate": 0.00019231452592414908,
        "epoch": 0.0872,
        "step": 654
    },
    {
        "loss": 2.5613,
        "grad_norm": 3.013654947280884,
        "learning_rate": 0.0001922903115259312,
        "epoch": 0.08733333333333333,
        "step": 655
    },
    {
        "loss": 1.7818,
        "grad_norm": 2.148341178894043,
        "learning_rate": 0.00019226606057120646,
        "epoch": 0.08746666666666666,
        "step": 656
    },
    {
        "loss": 3.0444,
        "grad_norm": 5.836350917816162,
        "learning_rate": 0.0001922417730695807,
        "epoch": 0.0876,
        "step": 657
    },
    {
        "loss": 2.1741,
        "grad_norm": 3.151127815246582,
        "learning_rate": 0.00019221744903067436,
        "epoch": 0.08773333333333333,
        "step": 658
    },
    {
        "loss": 2.9096,
        "grad_norm": 2.285130023956299,
        "learning_rate": 0.0001921930884641222,
        "epoch": 0.08786666666666666,
        "step": 659
    },
    {
        "loss": 2.1577,
        "grad_norm": 4.332294940948486,
        "learning_rate": 0.0001921686913795736,
        "epoch": 0.088,
        "step": 660
    },
    {
        "loss": 2.7853,
        "grad_norm": 3.8505420684814453,
        "learning_rate": 0.00019214425778669224,
        "epoch": 0.08813333333333333,
        "step": 661
    },
    {
        "loss": 3.0111,
        "grad_norm": 3.3731303215026855,
        "learning_rate": 0.00019211978769515642,
        "epoch": 0.08826666666666666,
        "step": 662
    },
    {
        "loss": 2.2872,
        "grad_norm": 2.984395742416382,
        "learning_rate": 0.00019209528111465882,
        "epoch": 0.0884,
        "step": 663
    },
    {
        "loss": 2.0699,
        "grad_norm": 3.7567505836486816,
        "learning_rate": 0.00019207073805490655,
        "epoch": 0.08853333333333334,
        "step": 664
    },
    {
        "loss": 2.0639,
        "grad_norm": 3.0970382690429688,
        "learning_rate": 0.0001920461585256212,
        "epoch": 0.08866666666666667,
        "step": 665
    },
    {
        "loss": 2.8702,
        "grad_norm": 3.6258797645568848,
        "learning_rate": 0.00019202154253653884,
        "epoch": 0.0888,
        "step": 666
    },
    {
        "loss": 1.6066,
        "grad_norm": 2.9375312328338623,
        "learning_rate": 0.00019199689009740992,
        "epoch": 0.08893333333333334,
        "step": 667
    },
    {
        "loss": 2.1495,
        "grad_norm": 3.0586395263671875,
        "learning_rate": 0.00019197220121799935,
        "epoch": 0.08906666666666667,
        "step": 668
    },
    {
        "loss": 2.0648,
        "grad_norm": 2.3742129802703857,
        "learning_rate": 0.0001919474759080865,
        "epoch": 0.0892,
        "step": 669
    },
    {
        "loss": 2.0157,
        "grad_norm": 5.192453861236572,
        "learning_rate": 0.00019192271417746513,
        "epoch": 0.08933333333333333,
        "step": 670
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.346909999847412,
        "learning_rate": 0.00019189791603594345,
        "epoch": 0.08946666666666667,
        "step": 671
    },
    {
        "loss": 2.7353,
        "grad_norm": 2.779545545578003,
        "learning_rate": 0.00019187308149334412,
        "epoch": 0.0896,
        "step": 672
    },
    {
        "loss": 0.9771,
        "grad_norm": 2.8335466384887695,
        "learning_rate": 0.00019184821055950413,
        "epoch": 0.08973333333333333,
        "step": 673
    },
    {
        "loss": 2.7203,
        "grad_norm": 3.2534804344177246,
        "learning_rate": 0.000191823303244275,
        "epoch": 0.08986666666666666,
        "step": 674
    },
    {
        "loss": 2.9342,
        "grad_norm": 2.994774580001831,
        "learning_rate": 0.00019179835955752256,
        "epoch": 0.09,
        "step": 675
    },
    {
        "loss": 3.2401,
        "grad_norm": 3.464888572692871,
        "learning_rate": 0.0001917733795091271,
        "epoch": 0.09013333333333333,
        "step": 676
    },
    {
        "loss": 3.2762,
        "grad_norm": 3.1186394691467285,
        "learning_rate": 0.00019174836310898333,
        "epoch": 0.09026666666666666,
        "step": 677
    },
    {
        "loss": 2.6054,
        "grad_norm": 3.545847177505493,
        "learning_rate": 0.00019172331036700028,
        "epoch": 0.0904,
        "step": 678
    },
    {
        "loss": 2.7979,
        "grad_norm": 2.422506332397461,
        "learning_rate": 0.00019169822129310146,
        "epoch": 0.09053333333333333,
        "step": 679
    },
    {
        "loss": 2.7579,
        "grad_norm": 1.9312657117843628,
        "learning_rate": 0.00019167309589722474,
        "epoch": 0.09066666666666667,
        "step": 680
    },
    {
        "loss": 1.9878,
        "grad_norm": 2.7144224643707275,
        "learning_rate": 0.00019164793418932234,
        "epoch": 0.0908,
        "step": 681
    },
    {
        "loss": 2.5158,
        "grad_norm": 3.251025438308716,
        "learning_rate": 0.00019162273617936098,
        "epoch": 0.09093333333333334,
        "step": 682
    },
    {
        "loss": 2.8774,
        "grad_norm": 4.1855244636535645,
        "learning_rate": 0.00019159750187732158,
        "epoch": 0.09106666666666667,
        "step": 683
    },
    {
        "loss": 2.3158,
        "grad_norm": 3.3616292476654053,
        "learning_rate": 0.0001915722312931996,
        "epoch": 0.0912,
        "step": 684
    },
    {
        "loss": 1.2166,
        "grad_norm": 3.5621178150177,
        "learning_rate": 0.00019154692443700474,
        "epoch": 0.09133333333333334,
        "step": 685
    },
    {
        "loss": 1.3139,
        "grad_norm": 4.039548873901367,
        "learning_rate": 0.0001915215813187612,
        "epoch": 0.09146666666666667,
        "step": 686
    },
    {
        "loss": 2.4652,
        "grad_norm": 2.654444932937622,
        "learning_rate": 0.00019149620194850746,
        "epoch": 0.0916,
        "step": 687
    },
    {
        "loss": 2.9075,
        "grad_norm": 2.254833936691284,
        "learning_rate": 0.00019147078633629632,
        "epoch": 0.09173333333333333,
        "step": 688
    },
    {
        "loss": 2.1023,
        "grad_norm": 6.5052170753479,
        "learning_rate": 0.00019144533449219509,
        "epoch": 0.09186666666666667,
        "step": 689
    },
    {
        "loss": 2.1851,
        "grad_norm": 3.527672529220581,
        "learning_rate": 0.00019141984642628523,
        "epoch": 0.092,
        "step": 690
    },
    {
        "loss": 1.6011,
        "grad_norm": 3.2315964698791504,
        "learning_rate": 0.0001913943221486627,
        "epoch": 0.09213333333333333,
        "step": 691
    },
    {
        "loss": 2.7926,
        "grad_norm": 3.8958852291107178,
        "learning_rate": 0.00019136876166943778,
        "epoch": 0.09226666666666666,
        "step": 692
    },
    {
        "loss": 1.5337,
        "grad_norm": 4.608157157897949,
        "learning_rate": 0.00019134316499873499,
        "epoch": 0.0924,
        "step": 693
    },
    {
        "loss": 1.1389,
        "grad_norm": 5.7744550704956055,
        "learning_rate": 0.00019131753214669334,
        "epoch": 0.09253333333333333,
        "step": 694
    },
    {
        "loss": 3.0394,
        "grad_norm": 2.0382230281829834,
        "learning_rate": 0.00019129186312346602,
        "epoch": 0.09266666666666666,
        "step": 695
    },
    {
        "loss": 2.6415,
        "grad_norm": 2.8816869258880615,
        "learning_rate": 0.00019126615793922067,
        "epoch": 0.0928,
        "step": 696
    },
    {
        "loss": 1.4574,
        "grad_norm": 4.688198089599609,
        "learning_rate": 0.00019124041660413917,
        "epoch": 0.09293333333333334,
        "step": 697
    },
    {
        "loss": 2.7652,
        "grad_norm": 3.813633441925049,
        "learning_rate": 0.00019121463912841774,
        "epoch": 0.09306666666666667,
        "step": 698
    },
    {
        "loss": 1.2862,
        "grad_norm": 5.558698654174805,
        "learning_rate": 0.00019118882552226696,
        "epoch": 0.0932,
        "step": 699
    },
    {
        "loss": 1.1128,
        "grad_norm": 5.323686122894287,
        "learning_rate": 0.0001911629757959117,
        "epoch": 0.09333333333333334,
        "step": 700
    },
    {
        "loss": 2.7336,
        "grad_norm": 1.81003999710083,
        "learning_rate": 0.00019113708995959104,
        "epoch": 0.09346666666666667,
        "step": 701
    },
    {
        "loss": 2.2775,
        "grad_norm": 2.6681113243103027,
        "learning_rate": 0.00019111116802355852,
        "epoch": 0.0936,
        "step": 702
    },
    {
        "loss": 2.0872,
        "grad_norm": 4.675891399383545,
        "learning_rate": 0.00019108520999808196,
        "epoch": 0.09373333333333334,
        "step": 703
    },
    {
        "loss": 1.842,
        "grad_norm": 3.8991260528564453,
        "learning_rate": 0.00019105921589344327,
        "epoch": 0.09386666666666667,
        "step": 704
    },
    {
        "loss": 2.1449,
        "grad_norm": 3.288864850997925,
        "learning_rate": 0.00019103318571993892,
        "epoch": 0.094,
        "step": 705
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.1430699825286865,
        "learning_rate": 0.00019100711948787953,
        "epoch": 0.09413333333333333,
        "step": 706
    },
    {
        "loss": 2.6004,
        "grad_norm": 4.567578315734863,
        "learning_rate": 0.00019098101720759,
        "epoch": 0.09426666666666667,
        "step": 707
    },
    {
        "loss": 2.6215,
        "grad_norm": 1.869030475616455,
        "learning_rate": 0.00019095487888940953,
        "epoch": 0.0944,
        "step": 708
    },
    {
        "loss": 2.1617,
        "grad_norm": 3.436887264251709,
        "learning_rate": 0.00019092870454369163,
        "epoch": 0.09453333333333333,
        "step": 709
    },
    {
        "loss": 2.4831,
        "grad_norm": 2.0115113258361816,
        "learning_rate": 0.000190902494180804,
        "epoch": 0.09466666666666666,
        "step": 710
    },
    {
        "loss": 2.1392,
        "grad_norm": 3.2168684005737305,
        "learning_rate": 0.00019087624781112875,
        "epoch": 0.0948,
        "step": 711
    },
    {
        "loss": 2.3301,
        "grad_norm": 2.355822801589966,
        "learning_rate": 0.00019084996544506202,
        "epoch": 0.09493333333333333,
        "step": 712
    },
    {
        "loss": 1.5986,
        "grad_norm": 2.7055845260620117,
        "learning_rate": 0.00019082364709301444,
        "epoch": 0.09506666666666666,
        "step": 713
    },
    {
        "loss": 3.5579,
        "grad_norm": 2.88128924369812,
        "learning_rate": 0.00019079729276541077,
        "epoch": 0.0952,
        "step": 714
    },
    {
        "loss": 2.232,
        "grad_norm": 4.48588752746582,
        "learning_rate": 0.00019077090247269002,
        "epoch": 0.09533333333333334,
        "step": 715
    },
    {
        "loss": 2.8035,
        "grad_norm": 3.9081318378448486,
        "learning_rate": 0.00019074447622530557,
        "epoch": 0.09546666666666667,
        "step": 716
    },
    {
        "loss": 2.7745,
        "grad_norm": 2.8172266483306885,
        "learning_rate": 0.00019071801403372485,
        "epoch": 0.0956,
        "step": 717
    },
    {
        "loss": 2.708,
        "grad_norm": 3.449740409851074,
        "learning_rate": 0.00019069151590842966,
        "epoch": 0.09573333333333334,
        "step": 718
    },
    {
        "loss": 1.8273,
        "grad_norm": 1.4139034748077393,
        "learning_rate": 0.000190664981859916,
        "epoch": 0.09586666666666667,
        "step": 719
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.9067515134811401,
        "learning_rate": 0.00019063841189869408,
        "epoch": 0.096,
        "step": 720
    },
    {
        "loss": 2.7797,
        "grad_norm": 2.5348899364471436,
        "learning_rate": 0.00019061180603528835,
        "epoch": 0.09613333333333333,
        "step": 721
    },
    {
        "loss": 1.9116,
        "grad_norm": 3.5683536529541016,
        "learning_rate": 0.0001905851642802375,
        "epoch": 0.09626666666666667,
        "step": 722
    },
    {
        "loss": 2.617,
        "grad_norm": 3.01251482963562,
        "learning_rate": 0.00019055848664409445,
        "epoch": 0.0964,
        "step": 723
    },
    {
        "loss": 1.5471,
        "grad_norm": 3.0407721996307373,
        "learning_rate": 0.00019053177313742625,
        "epoch": 0.09653333333333333,
        "step": 724
    },
    {
        "loss": 1.6701,
        "grad_norm": 3.6521384716033936,
        "learning_rate": 0.00019050502377081422,
        "epoch": 0.09666666666666666,
        "step": 725
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.8461780548095703,
        "learning_rate": 0.0001904782385548539,
        "epoch": 0.0968,
        "step": 726
    },
    {
        "loss": 1.391,
        "grad_norm": 3.4834721088409424,
        "learning_rate": 0.000190451417500155,
        "epoch": 0.09693333333333333,
        "step": 727
    },
    {
        "loss": 2.6292,
        "grad_norm": 2.8075785636901855,
        "learning_rate": 0.00019042456061734141,
        "epoch": 0.09706666666666666,
        "step": 728
    },
    {
        "loss": 2.6341,
        "grad_norm": 2.145169734954834,
        "learning_rate": 0.00019039766791705126,
        "epoch": 0.0972,
        "step": 729
    },
    {
        "loss": 2.8164,
        "grad_norm": 2.4854462146759033,
        "learning_rate": 0.00019037073940993682,
        "epoch": 0.09733333333333333,
        "step": 730
    },
    {
        "loss": 2.5744,
        "grad_norm": 3.175964832305908,
        "learning_rate": 0.00019034377510666458,
        "epoch": 0.09746666666666666,
        "step": 731
    },
    {
        "loss": 2.4971,
        "grad_norm": 2.901869773864746,
        "learning_rate": 0.00019031677501791513,
        "epoch": 0.0976,
        "step": 732
    },
    {
        "loss": 2.6304,
        "grad_norm": 3.369183301925659,
        "learning_rate": 0.0001902897391543834,
        "epoch": 0.09773333333333334,
        "step": 733
    },
    {
        "loss": 2.5526,
        "grad_norm": 2.682349443435669,
        "learning_rate": 0.00019026266752677833,
        "epoch": 0.09786666666666667,
        "step": 734
    },
    {
        "loss": 2.5337,
        "grad_norm": 2.8090245723724365,
        "learning_rate": 0.00019023556014582303,
        "epoch": 0.098,
        "step": 735
    },
    {
        "loss": 2.6988,
        "grad_norm": 3.2302775382995605,
        "learning_rate": 0.00019020841702225495,
        "epoch": 0.09813333333333334,
        "step": 736
    },
    {
        "loss": 2.9494,
        "grad_norm": 2.9800922870635986,
        "learning_rate": 0.00019018123816682552,
        "epoch": 0.09826666666666667,
        "step": 737
    },
    {
        "loss": 0.9382,
        "grad_norm": 2.7719688415527344,
        "learning_rate": 0.00019015402359030034,
        "epoch": 0.0984,
        "step": 738
    },
    {
        "loss": 3.0575,
        "grad_norm": 2.9846925735473633,
        "learning_rate": 0.00019012677330345923,
        "epoch": 0.09853333333333333,
        "step": 739
    },
    {
        "loss": 1.5893,
        "grad_norm": 3.6655616760253906,
        "learning_rate": 0.00019009948731709607,
        "epoch": 0.09866666666666667,
        "step": 740
    },
    {
        "loss": 2.1311,
        "grad_norm": 3.432337760925293,
        "learning_rate": 0.00019007216564201907,
        "epoch": 0.0988,
        "step": 741
    },
    {
        "loss": 1.8325,
        "grad_norm": 4.090385437011719,
        "learning_rate": 0.00019004480828905027,
        "epoch": 0.09893333333333333,
        "step": 742
    },
    {
        "loss": 2.1922,
        "grad_norm": 2.6927101612091064,
        "learning_rate": 0.00019001741526902612,
        "epoch": 0.09906666666666666,
        "step": 743
    },
    {
        "loss": 2.2671,
        "grad_norm": 4.438605785369873,
        "learning_rate": 0.00018998998659279704,
        "epoch": 0.0992,
        "step": 744
    },
    {
        "loss": 2.3452,
        "grad_norm": 2.693964958190918,
        "learning_rate": 0.00018996252227122766,
        "epoch": 0.09933333333333333,
        "step": 745
    },
    {
        "loss": 2.8091,
        "grad_norm": 4.911747932434082,
        "learning_rate": 0.00018993502231519664,
        "epoch": 0.09946666666666666,
        "step": 746
    },
    {
        "loss": 2.7419,
        "grad_norm": 3.2432265281677246,
        "learning_rate": 0.00018990748673559685,
        "epoch": 0.0996,
        "step": 747
    },
    {
        "loss": 2.5464,
        "grad_norm": 4.533772945404053,
        "learning_rate": 0.00018987991554333519,
        "epoch": 0.09973333333333333,
        "step": 748
    },
    {
        "loss": 1.7487,
        "grad_norm": 4.042423248291016,
        "learning_rate": 0.00018985230874933275,
        "epoch": 0.09986666666666667,
        "step": 749
    },
    {
        "loss": 0.8894,
        "grad_norm": 2.4725263118743896,
        "learning_rate": 0.00018982466636452465,
        "epoch": 0.1,
        "step": 750
    },
    {
        "loss": 2.6205,
        "grad_norm": 2.753063917160034,
        "learning_rate": 0.00018979698839986014,
        "epoch": 0.10013333333333334,
        "step": 751
    },
    {
        "loss": 2.2853,
        "grad_norm": 2.0148985385894775,
        "learning_rate": 0.00018976927486630252,
        "epoch": 0.10026666666666667,
        "step": 752
    },
    {
        "loss": 2.955,
        "grad_norm": 2.8675966262817383,
        "learning_rate": 0.00018974152577482927,
        "epoch": 0.1004,
        "step": 753
    },
    {
        "loss": 2.2539,
        "grad_norm": 3.474470376968384,
        "learning_rate": 0.00018971374113643187,
        "epoch": 0.10053333333333334,
        "step": 754
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.888779401779175,
        "learning_rate": 0.00018968592096211588,
        "epoch": 0.10066666666666667,
        "step": 755
    },
    {
        "loss": 2.5771,
        "grad_norm": 3.1871652603149414,
        "learning_rate": 0.000189658065262901,
        "epoch": 0.1008,
        "step": 756
    },
    {
        "loss": 1.5382,
        "grad_norm": 3.901804208755493,
        "learning_rate": 0.000189630174049821,
        "epoch": 0.10093333333333333,
        "step": 757
    },
    {
        "loss": 2.7064,
        "grad_norm": 3.227067232131958,
        "learning_rate": 0.00018960224733392366,
        "epoch": 0.10106666666666667,
        "step": 758
    },
    {
        "loss": 2.1105,
        "grad_norm": 7.998367786407471,
        "learning_rate": 0.00018957428512627084,
        "epoch": 0.1012,
        "step": 759
    },
    {
        "loss": 2.7889,
        "grad_norm": 2.4432098865509033,
        "learning_rate": 0.00018954628743793846,
        "epoch": 0.10133333333333333,
        "step": 760
    },
    {
        "loss": 1.2945,
        "grad_norm": 1.4313102960586548,
        "learning_rate": 0.0001895182542800165,
        "epoch": 0.10146666666666666,
        "step": 761
    },
    {
        "loss": 2.6848,
        "grad_norm": 3.8022279739379883,
        "learning_rate": 0.00018949018566360898,
        "epoch": 0.1016,
        "step": 762
    },
    {
        "loss": 2.3296,
        "grad_norm": 3.273981809616089,
        "learning_rate": 0.00018946208159983398,
        "epoch": 0.10173333333333333,
        "step": 763
    },
    {
        "loss": 2.5353,
        "grad_norm": 2.402235507965088,
        "learning_rate": 0.00018943394209982364,
        "epoch": 0.10186666666666666,
        "step": 764
    },
    {
        "loss": 2.0256,
        "grad_norm": 2.141059637069702,
        "learning_rate": 0.0001894057671747241,
        "epoch": 0.102,
        "step": 765
    },
    {
        "loss": 3.1675,
        "grad_norm": 3.228733539581299,
        "learning_rate": 0.00018937755683569554,
        "epoch": 0.10213333333333334,
        "step": 766
    },
    {
        "loss": 2.7902,
        "grad_norm": 2.169469118118286,
        "learning_rate": 0.00018934931109391217,
        "epoch": 0.10226666666666667,
        "step": 767
    },
    {
        "loss": 1.6728,
        "grad_norm": 3.185171365737915,
        "learning_rate": 0.00018932102996056221,
        "epoch": 0.1024,
        "step": 768
    },
    {
        "loss": 2.73,
        "grad_norm": 3.1451451778411865,
        "learning_rate": 0.00018929271344684794,
        "epoch": 0.10253333333333334,
        "step": 769
    },
    {
        "loss": 1.5658,
        "grad_norm": 5.842648029327393,
        "learning_rate": 0.00018926436156398562,
        "epoch": 0.10266666666666667,
        "step": 770
    },
    {
        "loss": 2.035,
        "grad_norm": 2.9007301330566406,
        "learning_rate": 0.0001892359743232055,
        "epoch": 0.1028,
        "step": 771
    },
    {
        "loss": 2.1226,
        "grad_norm": 2.9276955127716064,
        "learning_rate": 0.00018920755173575191,
        "epoch": 0.10293333333333334,
        "step": 772
    },
    {
        "loss": 1.0,
        "grad_norm": 2.7591357231140137,
        "learning_rate": 0.0001891790938128831,
        "epoch": 0.10306666666666667,
        "step": 773
    },
    {
        "loss": 2.6494,
        "grad_norm": 3.085078001022339,
        "learning_rate": 0.00018915060056587138,
        "epoch": 0.1032,
        "step": 774
    },
    {
        "loss": 1.8725,
        "grad_norm": 3.2164196968078613,
        "learning_rate": 0.00018912207200600298,
        "epoch": 0.10333333333333333,
        "step": 775
    },
    {
        "loss": 2.3344,
        "grad_norm": 3.230177879333496,
        "learning_rate": 0.00018909350814457821,
        "epoch": 0.10346666666666667,
        "step": 776
    },
    {
        "loss": 1.9243,
        "grad_norm": 4.7062883377075195,
        "learning_rate": 0.00018906490899291126,
        "epoch": 0.1036,
        "step": 777
    },
    {
        "loss": 2.8317,
        "grad_norm": 2.3876798152923584,
        "learning_rate": 0.00018903627456233037,
        "epoch": 0.10373333333333333,
        "step": 778
    },
    {
        "loss": 2.0398,
        "grad_norm": 3.0927700996398926,
        "learning_rate": 0.00018900760486417775,
        "epoch": 0.10386666666666666,
        "step": 779
    },
    {
        "loss": 2.418,
        "grad_norm": 2.5000851154327393,
        "learning_rate": 0.00018897889990980956,
        "epoch": 0.104,
        "step": 780
    },
    {
        "loss": 2.1102,
        "grad_norm": 4.117123126983643,
        "learning_rate": 0.00018895015971059594,
        "epoch": 0.10413333333333333,
        "step": 781
    },
    {
        "loss": 1.4905,
        "grad_norm": 3.3768043518066406,
        "learning_rate": 0.00018892138427792094,
        "epoch": 0.10426666666666666,
        "step": 782
    },
    {
        "loss": 1.3905,
        "grad_norm": 4.335350513458252,
        "learning_rate": 0.00018889257362318267,
        "epoch": 0.1044,
        "step": 783
    },
    {
        "loss": 2.7432,
        "grad_norm": 3.900330066680908,
        "learning_rate": 0.00018886372775779307,
        "epoch": 0.10453333333333334,
        "step": 784
    },
    {
        "loss": 2.5584,
        "grad_norm": 3.522423028945923,
        "learning_rate": 0.00018883484669317814,
        "epoch": 0.10466666666666667,
        "step": 785
    },
    {
        "loss": 1.9916,
        "grad_norm": 4.248154640197754,
        "learning_rate": 0.0001888059304407777,
        "epoch": 0.1048,
        "step": 786
    },
    {
        "loss": 1.8825,
        "grad_norm": 3.271198272705078,
        "learning_rate": 0.00018877697901204562,
        "epoch": 0.10493333333333334,
        "step": 787
    },
    {
        "loss": 1.7254,
        "grad_norm": 2.5972440242767334,
        "learning_rate": 0.00018874799241844968,
        "epoch": 0.10506666666666667,
        "step": 788
    },
    {
        "loss": 3.3413,
        "grad_norm": 3.8100533485412598,
        "learning_rate": 0.0001887189706714715,
        "epoch": 0.1052,
        "step": 789
    },
    {
        "loss": 3.2302,
        "grad_norm": 3.1769039630889893,
        "learning_rate": 0.00018868991378260676,
        "epoch": 0.10533333333333333,
        "step": 790
    },
    {
        "loss": 2.5306,
        "grad_norm": 3.746894598007202,
        "learning_rate": 0.00018866082176336498,
        "epoch": 0.10546666666666667,
        "step": 791
    },
    {
        "loss": 2.8656,
        "grad_norm": 2.2506625652313232,
        "learning_rate": 0.00018863169462526958,
        "epoch": 0.1056,
        "step": 792
    },
    {
        "loss": 3.1275,
        "grad_norm": 2.0552825927734375,
        "learning_rate": 0.00018860253237985793,
        "epoch": 0.10573333333333333,
        "step": 793
    },
    {
        "loss": 3.1838,
        "grad_norm": 2.6230108737945557,
        "learning_rate": 0.0001885733350386813,
        "epoch": 0.10586666666666666,
        "step": 794
    },
    {
        "loss": 2.6655,
        "grad_norm": 2.1923017501831055,
        "learning_rate": 0.00018854410261330486,
        "epoch": 0.106,
        "step": 795
    },
    {
        "loss": 1.3699,
        "grad_norm": 3.301547050476074,
        "learning_rate": 0.00018851483511530764,
        "epoch": 0.10613333333333333,
        "step": 796
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.663827657699585,
        "learning_rate": 0.00018848553255628266,
        "epoch": 0.10626666666666666,
        "step": 797
    },
    {
        "loss": 2.3028,
        "grad_norm": 4.004059791564941,
        "learning_rate": 0.00018845619494783672,
        "epoch": 0.1064,
        "step": 798
    },
    {
        "loss": 2.5444,
        "grad_norm": 2.5563199520111084,
        "learning_rate": 0.00018842682230159056,
        "epoch": 0.10653333333333333,
        "step": 799
    },
    {
        "loss": 2.6058,
        "grad_norm": 3.8987607955932617,
        "learning_rate": 0.00018839741462917876,
        "epoch": 0.10666666666666667,
        "step": 800
    },
    {
        "loss": 2.5229,
        "grad_norm": 2.5808935165405273,
        "learning_rate": 0.00018836797194224985,
        "epoch": 0.1068,
        "step": 801
    },
    {
        "loss": 2.0323,
        "grad_norm": 2.408393383026123,
        "learning_rate": 0.0001883384942524661,
        "epoch": 0.10693333333333334,
        "step": 802
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.8447468280792236,
        "learning_rate": 0.0001883089815715038,
        "epoch": 0.10706666666666667,
        "step": 803
    },
    {
        "loss": 2.0871,
        "grad_norm": 3.4370529651641846,
        "learning_rate": 0.000188279433911053,
        "epoch": 0.1072,
        "step": 804
    },
    {
        "loss": 2.8703,
        "grad_norm": 2.9892783164978027,
        "learning_rate": 0.0001882498512828176,
        "epoch": 0.10733333333333334,
        "step": 805
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.841336250305176,
        "learning_rate": 0.00018822023369851544,
        "epoch": 0.10746666666666667,
        "step": 806
    },
    {
        "loss": 2.5555,
        "grad_norm": 1.9558210372924805,
        "learning_rate": 0.0001881905811698781,
        "epoch": 0.1076,
        "step": 807
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.7266061305999756,
        "learning_rate": 0.00018816089370865102,
        "epoch": 0.10773333333333333,
        "step": 808
    },
    {
        "loss": 1.6891,
        "grad_norm": 3.176994562149048,
        "learning_rate": 0.00018813117132659356,
        "epoch": 0.10786666666666667,
        "step": 809
    },
    {
        "loss": 2.3912,
        "grad_norm": 3.037172555923462,
        "learning_rate": 0.00018810141403547884,
        "epoch": 0.108,
        "step": 810
    },
    {
        "loss": 1.7741,
        "grad_norm": 3.783292293548584,
        "learning_rate": 0.00018807162184709383,
        "epoch": 0.10813333333333333,
        "step": 811
    },
    {
        "loss": 2.5733,
        "grad_norm": 3.645190954208374,
        "learning_rate": 0.0001880417947732393,
        "epoch": 0.10826666666666666,
        "step": 812
    },
    {
        "loss": 1.9581,
        "grad_norm": 4.082604885101318,
        "learning_rate": 0.00018801193282572988,
        "epoch": 0.1084,
        "step": 813
    },
    {
        "loss": 2.3189,
        "grad_norm": 2.8690836429595947,
        "learning_rate": 0.00018798203601639395,
        "epoch": 0.10853333333333333,
        "step": 814
    },
    {
        "loss": 1.9708,
        "grad_norm": 2.479379653930664,
        "learning_rate": 0.00018795210435707378,
        "epoch": 0.10866666666666666,
        "step": 815
    },
    {
        "loss": 2.8533,
        "grad_norm": 2.530247449874878,
        "learning_rate": 0.0001879221378596254,
        "epoch": 0.1088,
        "step": 816
    },
    {
        "loss": 0.7574,
        "grad_norm": 3.086942434310913,
        "learning_rate": 0.00018789213653591858,
        "epoch": 0.10893333333333333,
        "step": 817
    },
    {
        "loss": 1.4448,
        "grad_norm": 4.501924991607666,
        "learning_rate": 0.00018786210039783703,
        "epoch": 0.10906666666666667,
        "step": 818
    },
    {
        "loss": 0.6323,
        "grad_norm": 2.5081329345703125,
        "learning_rate": 0.00018783202945727812,
        "epoch": 0.1092,
        "step": 819
    },
    {
        "loss": 2.6079,
        "grad_norm": 2.5800716876983643,
        "learning_rate": 0.00018780192372615305,
        "epoch": 0.10933333333333334,
        "step": 820
    },
    {
        "loss": 2.4001,
        "grad_norm": 4.327449798583984,
        "learning_rate": 0.0001877717832163868,
        "epoch": 0.10946666666666667,
        "step": 821
    },
    {
        "loss": 2.5525,
        "grad_norm": 5.236571311950684,
        "learning_rate": 0.0001877416079399182,
        "epoch": 0.1096,
        "step": 822
    },
    {
        "loss": 2.6853,
        "grad_norm": 3.422466278076172,
        "learning_rate": 0.0001877113979086997,
        "epoch": 0.10973333333333334,
        "step": 823
    },
    {
        "loss": 1.8256,
        "grad_norm": 3.198552131652832,
        "learning_rate": 0.00018768115313469758,
        "epoch": 0.10986666666666667,
        "step": 824
    },
    {
        "loss": 2.0975,
        "grad_norm": 3.2336642742156982,
        "learning_rate": 0.000187650873629892,
        "epoch": 0.11,
        "step": 825
    },
    {
        "loss": 2.276,
        "grad_norm": 2.4124276638031006,
        "learning_rate": 0.00018762055940627667,
        "epoch": 0.11013333333333333,
        "step": 826
    },
    {
        "loss": 2.842,
        "grad_norm": 3.6376864910125732,
        "learning_rate": 0.00018759021047585922,
        "epoch": 0.11026666666666667,
        "step": 827
    },
    {
        "loss": 2.3903,
        "grad_norm": 3.3572747707366943,
        "learning_rate": 0.00018755982685066092,
        "epoch": 0.1104,
        "step": 828
    },
    {
        "loss": 2.535,
        "grad_norm": 3.4030745029449463,
        "learning_rate": 0.00018752940854271688,
        "epoch": 0.11053333333333333,
        "step": 829
    },
    {
        "loss": 3.0837,
        "grad_norm": 2.932603597640991,
        "learning_rate": 0.0001874989555640758,
        "epoch": 0.11066666666666666,
        "step": 830
    },
    {
        "loss": 2.2627,
        "grad_norm": 3.3767309188842773,
        "learning_rate": 0.00018746846792680035,
        "epoch": 0.1108,
        "step": 831
    },
    {
        "loss": 3.2149,
        "grad_norm": 2.743098020553589,
        "learning_rate": 0.00018743794564296667,
        "epoch": 0.11093333333333333,
        "step": 832
    },
    {
        "loss": 2.3722,
        "grad_norm": 3.456691026687622,
        "learning_rate": 0.00018740738872466485,
        "epoch": 0.11106666666666666,
        "step": 833
    },
    {
        "loss": 2.9459,
        "grad_norm": 2.5916590690612793,
        "learning_rate": 0.00018737679718399845,
        "epoch": 0.1112,
        "step": 834
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.75714111328125,
        "learning_rate": 0.000187346171033085,
        "epoch": 0.11133333333333334,
        "step": 835
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.442497730255127,
        "learning_rate": 0.00018731551028405553,
        "epoch": 0.11146666666666667,
        "step": 836
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.415992021560669,
        "learning_rate": 0.00018728481494905495,
        "epoch": 0.1116,
        "step": 837
    },
    {
        "loss": 2.4792,
        "grad_norm": 2.236999988555908,
        "learning_rate": 0.00018725408504024178,
        "epoch": 0.11173333333333334,
        "step": 838
    },
    {
        "loss": 1.1937,
        "grad_norm": 3.9943323135375977,
        "learning_rate": 0.00018722332056978817,
        "epoch": 0.11186666666666667,
        "step": 839
    },
    {
        "loss": 1.9891,
        "grad_norm": 2.6021668910980225,
        "learning_rate": 0.00018719252154988009,
        "epoch": 0.112,
        "step": 840
    },
    {
        "loss": 3.4506,
        "grad_norm": 3.1631927490234375,
        "learning_rate": 0.0001871616879927171,
        "epoch": 0.11213333333333333,
        "step": 841
    },
    {
        "loss": 2.471,
        "grad_norm": 3.4194674491882324,
        "learning_rate": 0.00018713081991051256,
        "epoch": 0.11226666666666667,
        "step": 842
    },
    {
        "loss": 2.6738,
        "grad_norm": 2.3360774517059326,
        "learning_rate": 0.00018709991731549332,
        "epoch": 0.1124,
        "step": 843
    },
    {
        "loss": 2.4591,
        "grad_norm": 4.639378547668457,
        "learning_rate": 0.00018706898021990007,
        "epoch": 0.11253333333333333,
        "step": 844
    },
    {
        "loss": 2.888,
        "grad_norm": 2.590714454650879,
        "learning_rate": 0.00018703800863598705,
        "epoch": 0.11266666666666666,
        "step": 845
    },
    {
        "loss": 2.3911,
        "grad_norm": 2.241276264190674,
        "learning_rate": 0.0001870070025760222,
        "epoch": 0.1128,
        "step": 846
    },
    {
        "loss": 2.0718,
        "grad_norm": 2.521768569946289,
        "learning_rate": 0.00018697596205228726,
        "epoch": 0.11293333333333333,
        "step": 847
    },
    {
        "loss": 2.6654,
        "grad_norm": 2.6306028366088867,
        "learning_rate": 0.00018694488707707733,
        "epoch": 0.11306666666666666,
        "step": 848
    },
    {
        "loss": 1.9966,
        "grad_norm": 3.7279534339904785,
        "learning_rate": 0.00018691377766270133,
        "epoch": 0.1132,
        "step": 849
    },
    {
        "loss": 2.2548,
        "grad_norm": 3.0465757846832275,
        "learning_rate": 0.0001868826338214819,
        "epoch": 0.11333333333333333,
        "step": 850
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.607203722000122,
        "learning_rate": 0.00018685145556575513,
        "epoch": 0.11346666666666666,
        "step": 851
    },
    {
        "loss": 1.6697,
        "grad_norm": 4.217003345489502,
        "learning_rate": 0.00018682024290787093,
        "epoch": 0.1136,
        "step": 852
    },
    {
        "loss": 1.1128,
        "grad_norm": 3.507866382598877,
        "learning_rate": 0.00018678899586019264,
        "epoch": 0.11373333333333334,
        "step": 853
    },
    {
        "loss": 1.878,
        "grad_norm": 3.0877532958984375,
        "learning_rate": 0.0001867577144350974,
        "epoch": 0.11386666666666667,
        "step": 854
    },
    {
        "loss": 2.7229,
        "grad_norm": 2.2315421104431152,
        "learning_rate": 0.0001867263986449758,
        "epoch": 0.114,
        "step": 855
    },
    {
        "loss": 2.8117,
        "grad_norm": 2.7695484161376953,
        "learning_rate": 0.0001866950485022322,
        "epoch": 0.11413333333333334,
        "step": 856
    },
    {
        "loss": 2.0053,
        "grad_norm": 3.681802749633789,
        "learning_rate": 0.0001866636640192845,
        "epoch": 0.11426666666666667,
        "step": 857
    },
    {
        "loss": 1.0404,
        "grad_norm": 3.1716020107269287,
        "learning_rate": 0.00018663224520856417,
        "epoch": 0.1144,
        "step": 858
    },
    {
        "loss": 1.6653,
        "grad_norm": 3.778791666030884,
        "learning_rate": 0.00018660079208251634,
        "epoch": 0.11453333333333333,
        "step": 859
    },
    {
        "loss": 1.7127,
        "grad_norm": 3.1652309894561768,
        "learning_rate": 0.00018656930465359964,
        "epoch": 0.11466666666666667,
        "step": 860
    },
    {
        "loss": 2.7944,
        "grad_norm": 1.9439805746078491,
        "learning_rate": 0.00018653778293428643,
        "epoch": 0.1148,
        "step": 861
    },
    {
        "loss": 2.7771,
        "grad_norm": 3.066103219985962,
        "learning_rate": 0.0001865062269370625,
        "epoch": 0.11493333333333333,
        "step": 862
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.5333170890808105,
        "learning_rate": 0.00018647463667442735,
        "epoch": 0.11506666666666666,
        "step": 863
    },
    {
        "loss": 2.6297,
        "grad_norm": 3.290243625640869,
        "learning_rate": 0.0001864430121588939,
        "epoch": 0.1152,
        "step": 864
    },
    {
        "loss": 2.9589,
        "grad_norm": 3.247799873352051,
        "learning_rate": 0.00018641135340298885,
        "epoch": 0.11533333333333333,
        "step": 865
    },
    {
        "loss": 3.1237,
        "grad_norm": 2.7162249088287354,
        "learning_rate": 0.00018637966041925224,
        "epoch": 0.11546666666666666,
        "step": 866
    },
    {
        "loss": 2.2159,
        "grad_norm": 3.3308191299438477,
        "learning_rate": 0.00018634793322023786,
        "epoch": 0.1156,
        "step": 867
    },
    {
        "loss": 2.7612,
        "grad_norm": 3.697699546813965,
        "learning_rate": 0.00018631617181851285,
        "epoch": 0.11573333333333333,
        "step": 868
    },
    {
        "loss": 2.6849,
        "grad_norm": 3.3303918838500977,
        "learning_rate": 0.00018628437622665808,
        "epoch": 0.11586666666666667,
        "step": 869
    },
    {
        "loss": 2.3885,
        "grad_norm": 3.940859794616699,
        "learning_rate": 0.0001862525464572679,
        "epoch": 0.116,
        "step": 870
    },
    {
        "loss": 2.7876,
        "grad_norm": 2.7409210205078125,
        "learning_rate": 0.00018622068252295012,
        "epoch": 0.11613333333333334,
        "step": 871
    },
    {
        "loss": 2.9956,
        "grad_norm": 1.8328496217727661,
        "learning_rate": 0.00018618878443632623,
        "epoch": 0.11626666666666667,
        "step": 872
    },
    {
        "loss": 2.2655,
        "grad_norm": 2.850541830062866,
        "learning_rate": 0.00018615685221003115,
        "epoch": 0.1164,
        "step": 873
    },
    {
        "loss": 2.6841,
        "grad_norm": 3.8340272903442383,
        "learning_rate": 0.00018612488585671336,
        "epoch": 0.11653333333333334,
        "step": 874
    },
    {
        "loss": 2.8285,
        "grad_norm": 3.364901542663574,
        "learning_rate": 0.0001860928853890348,
        "epoch": 0.11666666666666667,
        "step": 875
    },
    {
        "loss": 2.3104,
        "grad_norm": 3.6040990352630615,
        "learning_rate": 0.00018606085081967097,
        "epoch": 0.1168,
        "step": 876
    },
    {
        "loss": 2.7956,
        "grad_norm": 2.398186445236206,
        "learning_rate": 0.00018602878216131093,
        "epoch": 0.11693333333333333,
        "step": 877
    },
    {
        "loss": 1.7043,
        "grad_norm": 4.96256685256958,
        "learning_rate": 0.0001859966794266571,
        "epoch": 0.11706666666666667,
        "step": 878
    },
    {
        "loss": 2.4995,
        "grad_norm": 2.4594500064849854,
        "learning_rate": 0.0001859645426284255,
        "epoch": 0.1172,
        "step": 879
    },
    {
        "loss": 2.427,
        "grad_norm": 3.7836239337921143,
        "learning_rate": 0.0001859323717793457,
        "epoch": 0.11733333333333333,
        "step": 880
    },
    {
        "loss": 2.386,
        "grad_norm": 2.8954100608825684,
        "learning_rate": 0.0001859001668921606,
        "epoch": 0.11746666666666666,
        "step": 881
    },
    {
        "loss": 2.4988,
        "grad_norm": 3.167203664779663,
        "learning_rate": 0.00018586792797962672,
        "epoch": 0.1176,
        "step": 882
    },
    {
        "loss": 2.2568,
        "grad_norm": 3.4304282665252686,
        "learning_rate": 0.00018583565505451394,
        "epoch": 0.11773333333333333,
        "step": 883
    },
    {
        "loss": 2.1399,
        "grad_norm": 3.4915671348571777,
        "learning_rate": 0.00018580334812960575,
        "epoch": 0.11786666666666666,
        "step": 884
    },
    {
        "loss": 3.1187,
        "grad_norm": 2.2960453033447266,
        "learning_rate": 0.00018577100721769898,
        "epoch": 0.118,
        "step": 885
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.361201524734497,
        "learning_rate": 0.000185738632331604,
        "epoch": 0.11813333333333334,
        "step": 886
    },
    {
        "loss": 2.5972,
        "grad_norm": 2.4985198974609375,
        "learning_rate": 0.0001857062234841446,
        "epoch": 0.11826666666666667,
        "step": 887
    },
    {
        "loss": 0.9237,
        "grad_norm": 4.49770975112915,
        "learning_rate": 0.00018567378068815805,
        "epoch": 0.1184,
        "step": 888
    },
    {
        "loss": 2.2626,
        "grad_norm": 3.881243944168091,
        "learning_rate": 0.00018564130395649504,
        "epoch": 0.11853333333333334,
        "step": 889
    },
    {
        "loss": 1.3892,
        "grad_norm": 1.7216848134994507,
        "learning_rate": 0.00018560879330201973,
        "epoch": 0.11866666666666667,
        "step": 890
    },
    {
        "loss": 1.4531,
        "grad_norm": 3.313026189804077,
        "learning_rate": 0.00018557624873760968,
        "epoch": 0.1188,
        "step": 891
    },
    {
        "loss": 0.9818,
        "grad_norm": 5.616176128387451,
        "learning_rate": 0.00018554367027615588,
        "epoch": 0.11893333333333334,
        "step": 892
    },
    {
        "loss": 2.9424,
        "grad_norm": 2.940502166748047,
        "learning_rate": 0.00018551105793056282,
        "epoch": 0.11906666666666667,
        "step": 893
    },
    {
        "loss": 2.2524,
        "grad_norm": 3.0961222648620605,
        "learning_rate": 0.00018547841171374837,
        "epoch": 0.1192,
        "step": 894
    },
    {
        "loss": 2.5043,
        "grad_norm": 2.590562105178833,
        "learning_rate": 0.00018544573163864375,
        "epoch": 0.11933333333333333,
        "step": 895
    },
    {
        "loss": 2.6727,
        "grad_norm": 4.843277931213379,
        "learning_rate": 0.00018541301771819371,
        "epoch": 0.11946666666666667,
        "step": 896
    },
    {
        "loss": 1.1816,
        "grad_norm": 2.9522783756256104,
        "learning_rate": 0.00018538026996535627,
        "epoch": 0.1196,
        "step": 897
    },
    {
        "loss": 2.0534,
        "grad_norm": 2.8777077198028564,
        "learning_rate": 0.00018534748839310302,
        "epoch": 0.11973333333333333,
        "step": 898
    },
    {
        "loss": 1.9491,
        "grad_norm": 2.8414146900177,
        "learning_rate": 0.00018531467301441875,
        "epoch": 0.11986666666666666,
        "step": 899
    },
    {
        "loss": 2.115,
        "grad_norm": 2.2925705909729004,
        "learning_rate": 0.00018528182384230184,
        "epoch": 0.12,
        "step": 900
    },
    {
        "loss": 2.1948,
        "grad_norm": 3.428755760192871,
        "learning_rate": 0.00018524894088976389,
        "epoch": 0.12013333333333333,
        "step": 901
    },
    {
        "loss": 2.3477,
        "grad_norm": 3.393932819366455,
        "learning_rate": 0.00018521602416982997,
        "epoch": 0.12026666666666666,
        "step": 902
    },
    {
        "loss": 2.3852,
        "grad_norm": 2.124194383621216,
        "learning_rate": 0.00018518307369553853,
        "epoch": 0.1204,
        "step": 903
    },
    {
        "loss": 2.0813,
        "grad_norm": 3.6544318199157715,
        "learning_rate": 0.00018515008947994133,
        "epoch": 0.12053333333333334,
        "step": 904
    },
    {
        "loss": 1.2957,
        "grad_norm": 4.960533142089844,
        "learning_rate": 0.00018511707153610356,
        "epoch": 0.12066666666666667,
        "step": 905
    },
    {
        "loss": 2.8753,
        "grad_norm": 2.9597368240356445,
        "learning_rate": 0.00018508401987710373,
        "epoch": 0.1208,
        "step": 906
    },
    {
        "loss": 2.1197,
        "grad_norm": 4.461246013641357,
        "learning_rate": 0.0001850509345160337,
        "epoch": 0.12093333333333334,
        "step": 907
    },
    {
        "loss": 2.7361,
        "grad_norm": 2.399125099182129,
        "learning_rate": 0.00018501781546599868,
        "epoch": 0.12106666666666667,
        "step": 908
    },
    {
        "loss": 2.2481,
        "grad_norm": 3.9356637001037598,
        "learning_rate": 0.0001849846627401173,
        "epoch": 0.1212,
        "step": 909
    },
    {
        "loss": 0.7447,
        "grad_norm": 3.003373622894287,
        "learning_rate": 0.00018495147635152142,
        "epoch": 0.12133333333333333,
        "step": 910
    },
    {
        "loss": 2.1537,
        "grad_norm": 4.050989627838135,
        "learning_rate": 0.0001849182563133563,
        "epoch": 0.12146666666666667,
        "step": 911
    },
    {
        "loss": 2.589,
        "grad_norm": 2.608809471130371,
        "learning_rate": 0.00018488500263878045,
        "epoch": 0.1216,
        "step": 912
    },
    {
        "loss": 2.681,
        "grad_norm": 2.6801674365997314,
        "learning_rate": 0.00018485171534096586,
        "epoch": 0.12173333333333333,
        "step": 913
    },
    {
        "loss": 2.9875,
        "grad_norm": 2.5979762077331543,
        "learning_rate": 0.0001848183944330977,
        "epoch": 0.12186666666666666,
        "step": 914
    },
    {
        "loss": 1.2923,
        "grad_norm": 5.278894424438477,
        "learning_rate": 0.0001847850399283745,
        "epoch": 0.122,
        "step": 915
    },
    {
        "loss": 2.8323,
        "grad_norm": 2.1467456817626953,
        "learning_rate": 0.00018475165184000806,
        "epoch": 0.12213333333333333,
        "step": 916
    },
    {
        "loss": 2.7733,
        "grad_norm": 2.1401166915893555,
        "learning_rate": 0.00018471823018122356,
        "epoch": 0.12226666666666666,
        "step": 917
    },
    {
        "loss": 2.4777,
        "grad_norm": 2.833378314971924,
        "learning_rate": 0.00018468477496525943,
        "epoch": 0.1224,
        "step": 918
    },
    {
        "loss": 3.0132,
        "grad_norm": 3.084988832473755,
        "learning_rate": 0.00018465128620536735,
        "epoch": 0.12253333333333333,
        "step": 919
    },
    {
        "loss": 2.3233,
        "grad_norm": 3.420588493347168,
        "learning_rate": 0.0001846177639148124,
        "epoch": 0.12266666666666666,
        "step": 920
    },
    {
        "loss": 2.2737,
        "grad_norm": 3.9767398834228516,
        "learning_rate": 0.00018458420810687278,
        "epoch": 0.1228,
        "step": 921
    },
    {
        "loss": 2.5237,
        "grad_norm": 2.8258676528930664,
        "learning_rate": 0.00018455061879484014,
        "epoch": 0.12293333333333334,
        "step": 922
    },
    {
        "loss": 1.8949,
        "grad_norm": 5.087184429168701,
        "learning_rate": 0.0001845169959920193,
        "epoch": 0.12306666666666667,
        "step": 923
    },
    {
        "loss": 2.5654,
        "grad_norm": 2.639117956161499,
        "learning_rate": 0.00018448333971172835,
        "epoch": 0.1232,
        "step": 924
    },
    {
        "loss": 1.9361,
        "grad_norm": 4.828971862792969,
        "learning_rate": 0.0001844496499672987,
        "epoch": 0.12333333333333334,
        "step": 925
    },
    {
        "loss": 2.1916,
        "grad_norm": 2.8557167053222656,
        "learning_rate": 0.00018441592677207494,
        "epoch": 0.12346666666666667,
        "step": 926
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.010450839996338,
        "learning_rate": 0.00018438217013941492,
        "epoch": 0.1236,
        "step": 927
    },
    {
        "loss": 1.0262,
        "grad_norm": 3.4780499935150146,
        "learning_rate": 0.00018434838008268977,
        "epoch": 0.12373333333333333,
        "step": 928
    },
    {
        "loss": 2.6429,
        "grad_norm": 2.453537940979004,
        "learning_rate": 0.0001843145566152839,
        "epoch": 0.12386666666666667,
        "step": 929
    },
    {
        "loss": 0.9314,
        "grad_norm": 3.468630075454712,
        "learning_rate": 0.00018428069975059486,
        "epoch": 0.124,
        "step": 930
    },
    {
        "loss": 1.9559,
        "grad_norm": 3.3555409908294678,
        "learning_rate": 0.00018424680950203344,
        "epoch": 0.12413333333333333,
        "step": 931
    },
    {
        "loss": 2.2269,
        "grad_norm": 2.8364906311035156,
        "learning_rate": 0.00018421288588302373,
        "epoch": 0.12426666666666666,
        "step": 932
    },
    {
        "loss": 2.6473,
        "grad_norm": 2.151672601699829,
        "learning_rate": 0.00018417892890700298,
        "epoch": 0.1244,
        "step": 933
    },
    {
        "loss": 2.3174,
        "grad_norm": 5.6421709060668945,
        "learning_rate": 0.00018414493858742161,
        "epoch": 0.12453333333333333,
        "step": 934
    },
    {
        "loss": 2.6441,
        "grad_norm": 2.0852363109588623,
        "learning_rate": 0.0001841109149377434,
        "epoch": 0.12466666666666666,
        "step": 935
    },
    {
        "loss": 1.1013,
        "grad_norm": 4.599701404571533,
        "learning_rate": 0.00018407685797144517,
        "epoch": 0.1248,
        "step": 936
    },
    {
        "loss": 1.3139,
        "grad_norm": 3.2316009998321533,
        "learning_rate": 0.000184042767702017,
        "epoch": 0.12493333333333333,
        "step": 937
    },
    {
        "loss": 2.6624,
        "grad_norm": 3.3698086738586426,
        "learning_rate": 0.00018400864414296216,
        "epoch": 0.12506666666666666,
        "step": 938
    },
    {
        "loss": 2.0277,
        "grad_norm": 2.8523194789886475,
        "learning_rate": 0.0001839744873077972,
        "epoch": 0.1252,
        "step": 939
    },
    {
        "loss": 2.2395,
        "grad_norm": 3.0595474243164062,
        "learning_rate": 0.0001839402972100516,
        "epoch": 0.12533333333333332,
        "step": 940
    },
    {
        "loss": 1.8348,
        "grad_norm": 3.1892571449279785,
        "learning_rate": 0.00018390607386326827,
        "epoch": 0.12546666666666667,
        "step": 941
    },
    {
        "loss": 2.4593,
        "grad_norm": 4.200863838195801,
        "learning_rate": 0.0001838718172810032,
        "epoch": 0.1256,
        "step": 942
    },
    {
        "loss": 2.2707,
        "grad_norm": 3.5581307411193848,
        "learning_rate": 0.0001838375274768255,
        "epoch": 0.12573333333333334,
        "step": 943
    },
    {
        "loss": 0.6829,
        "grad_norm": 3.3853096961975098,
        "learning_rate": 0.0001838032044643175,
        "epoch": 0.12586666666666665,
        "step": 944
    },
    {
        "loss": 3.5074,
        "grad_norm": 2.972658395767212,
        "learning_rate": 0.0001837688482570747,
        "epoch": 0.126,
        "step": 945
    },
    {
        "loss": 2.3765,
        "grad_norm": 3.929173707962036,
        "learning_rate": 0.00018373445886870563,
        "epoch": 0.12613333333333332,
        "step": 946
    },
    {
        "loss": 2.3893,
        "grad_norm": 3.895040988922119,
        "learning_rate": 0.0001837000363128321,
        "epoch": 0.12626666666666667,
        "step": 947
    },
    {
        "loss": 1.1199,
        "grad_norm": 2.6785366535186768,
        "learning_rate": 0.00018366558060308897,
        "epoch": 0.1264,
        "step": 948
    },
    {
        "loss": 2.6892,
        "grad_norm": 3.0980355739593506,
        "learning_rate": 0.00018363109175312427,
        "epoch": 0.12653333333333333,
        "step": 949
    },
    {
        "loss": 2.425,
        "grad_norm": 3.4967029094696045,
        "learning_rate": 0.0001835965697765992,
        "epoch": 0.12666666666666668,
        "step": 950
    },
    {
        "loss": 2.1555,
        "grad_norm": 6.743564128875732,
        "learning_rate": 0.00018356201468718798,
        "epoch": 0.1268,
        "step": 951
    },
    {
        "loss": 2.9932,
        "grad_norm": 2.773303747177124,
        "learning_rate": 0.00018352742649857797,
        "epoch": 0.12693333333333334,
        "step": 952
    },
    {
        "loss": 2.5406,
        "grad_norm": 3.5733821392059326,
        "learning_rate": 0.00018349280522446975,
        "epoch": 0.12706666666666666,
        "step": 953
    },
    {
        "loss": 1.4915,
        "grad_norm": 4.411758899688721,
        "learning_rate": 0.00018345815087857686,
        "epoch": 0.1272,
        "step": 954
    },
    {
        "loss": 2.3695,
        "grad_norm": 2.1654765605926514,
        "learning_rate": 0.00018342346347462608,
        "epoch": 0.12733333333333333,
        "step": 955
    },
    {
        "loss": 3.3435,
        "grad_norm": 3.0259082317352295,
        "learning_rate": 0.00018338874302635714,
        "epoch": 0.12746666666666667,
        "step": 956
    },
    {
        "loss": 2.6199,
        "grad_norm": 2.458852529525757,
        "learning_rate": 0.00018335398954752292,
        "epoch": 0.1276,
        "step": 957
    },
    {
        "loss": 2.843,
        "grad_norm": 2.631679058074951,
        "learning_rate": 0.00018331920305188944,
        "epoch": 0.12773333333333334,
        "step": 958
    },
    {
        "loss": 2.4045,
        "grad_norm": 4.145869255065918,
        "learning_rate": 0.00018328438355323572,
        "epoch": 0.12786666666666666,
        "step": 959
    },
    {
        "loss": 2.7869,
        "grad_norm": 3.303400754928589,
        "learning_rate": 0.0001832495310653539,
        "epoch": 0.128,
        "step": 960
    },
    {
        "loss": 2.7273,
        "grad_norm": 3.240487575531006,
        "learning_rate": 0.00018321464560204915,
        "epoch": 0.12813333333333332,
        "step": 961
    },
    {
        "loss": 1.8506,
        "grad_norm": 1.9957867860794067,
        "learning_rate": 0.00018317972717713974,
        "epoch": 0.12826666666666667,
        "step": 962
    },
    {
        "loss": 2.4994,
        "grad_norm": 3.679830551147461,
        "learning_rate": 0.00018314477580445694,
        "epoch": 0.1284,
        "step": 963
    },
    {
        "loss": 2.1826,
        "grad_norm": 3.89388108253479,
        "learning_rate": 0.00018310979149784514,
        "epoch": 0.12853333333333333,
        "step": 964
    },
    {
        "loss": 2.5502,
        "grad_norm": 4.075028896331787,
        "learning_rate": 0.00018307477427116178,
        "epoch": 0.12866666666666668,
        "step": 965
    },
    {
        "loss": 3.1521,
        "grad_norm": 2.7784321308135986,
        "learning_rate": 0.00018303972413827722,
        "epoch": 0.1288,
        "step": 966
    },
    {
        "loss": 2.6132,
        "grad_norm": 3.0984387397766113,
        "learning_rate": 0.000183004641113075,
        "epoch": 0.12893333333333334,
        "step": 967
    },
    {
        "loss": 2.7,
        "grad_norm": 3.114398241043091,
        "learning_rate": 0.0001829695252094516,
        "epoch": 0.12906666666666666,
        "step": 968
    },
    {
        "loss": 2.3898,
        "grad_norm": 2.284987688064575,
        "learning_rate": 0.00018293437644131653,
        "epoch": 0.1292,
        "step": 969
    },
    {
        "loss": 2.7998,
        "grad_norm": 3.8188133239746094,
        "learning_rate": 0.0001828991948225924,
        "epoch": 0.12933333333333333,
        "step": 970
    },
    {
        "loss": 2.694,
        "grad_norm": 3.1647088527679443,
        "learning_rate": 0.00018286398036721467,
        "epoch": 0.12946666666666667,
        "step": 971
    },
    {
        "loss": 2.1632,
        "grad_norm": 4.4540114402771,
        "learning_rate": 0.00018282873308913202,
        "epoch": 0.1296,
        "step": 972
    },
    {
        "loss": 2.4739,
        "grad_norm": 3.304450035095215,
        "learning_rate": 0.00018279345300230594,
        "epoch": 0.12973333333333334,
        "step": 973
    },
    {
        "loss": 2.3668,
        "grad_norm": 3.877729654312134,
        "learning_rate": 0.00018275814012071103,
        "epoch": 0.12986666666666666,
        "step": 974
    },
    {
        "loss": 2.5479,
        "grad_norm": 2.7731659412384033,
        "learning_rate": 0.0001827227944583348,
        "epoch": 0.13,
        "step": 975
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.8820605278015137,
        "learning_rate": 0.00018268741602917783,
        "epoch": 0.13013333333333332,
        "step": 976
    },
    {
        "loss": 3.0997,
        "grad_norm": 3.1053922176361084,
        "learning_rate": 0.00018265200484725362,
        "epoch": 0.13026666666666667,
        "step": 977
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.6709110736846924,
        "learning_rate": 0.00018261656092658865,
        "epoch": 0.1304,
        "step": 978
    },
    {
        "loss": 3.286,
        "grad_norm": 3.3886537551879883,
        "learning_rate": 0.00018258108428122237,
        "epoch": 0.13053333333333333,
        "step": 979
    },
    {
        "loss": 1.3935,
        "grad_norm": 2.0747439861297607,
        "learning_rate": 0.00018254557492520726,
        "epoch": 0.13066666666666665,
        "step": 980
    },
    {
        "loss": 2.4017,
        "grad_norm": 2.5066165924072266,
        "learning_rate": 0.00018251003287260865,
        "epoch": 0.1308,
        "step": 981
    },
    {
        "loss": 1.9888,
        "grad_norm": 3.3948123455047607,
        "learning_rate": 0.00018247445813750486,
        "epoch": 0.13093333333333335,
        "step": 982
    },
    {
        "loss": 1.9253,
        "grad_norm": 3.9104201793670654,
        "learning_rate": 0.00018243885073398718,
        "epoch": 0.13106666666666666,
        "step": 983
    },
    {
        "loss": 2.4467,
        "grad_norm": 2.3933303356170654,
        "learning_rate": 0.0001824032106761598,
        "epoch": 0.1312,
        "step": 984
    },
    {
        "loss": 2.6406,
        "grad_norm": 3.0911753177642822,
        "learning_rate": 0.0001823675379781399,
        "epoch": 0.13133333333333333,
        "step": 985
    },
    {
        "loss": 2.0753,
        "grad_norm": 3.1523542404174805,
        "learning_rate": 0.00018233183265405756,
        "epoch": 0.13146666666666668,
        "step": 986
    },
    {
        "loss": 1.2793,
        "grad_norm": 4.21197509765625,
        "learning_rate": 0.00018229609471805578,
        "epoch": 0.1316,
        "step": 987
    },
    {
        "loss": 2.2435,
        "grad_norm": 4.322873115539551,
        "learning_rate": 0.00018226032418429045,
        "epoch": 0.13173333333333334,
        "step": 988
    },
    {
        "loss": 2.5754,
        "grad_norm": 3.9258081912994385,
        "learning_rate": 0.0001822245210669304,
        "epoch": 0.13186666666666666,
        "step": 989
    },
    {
        "loss": 2.7622,
        "grad_norm": 3.3218867778778076,
        "learning_rate": 0.0001821886853801574,
        "epoch": 0.132,
        "step": 990
    },
    {
        "loss": 2.8735,
        "grad_norm": 2.190664768218994,
        "learning_rate": 0.00018215281713816606,
        "epoch": 0.13213333333333332,
        "step": 991
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.5302748680114746,
        "learning_rate": 0.0001821169163551639,
        "epoch": 0.13226666666666667,
        "step": 992
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.8819282054901123,
        "learning_rate": 0.0001820809830453714,
        "epoch": 0.1324,
        "step": 993
    },
    {
        "loss": 1.845,
        "grad_norm": 2.9205803871154785,
        "learning_rate": 0.00018204501722302182,
        "epoch": 0.13253333333333334,
        "step": 994
    },
    {
        "loss": 2.2577,
        "grad_norm": 3.5906059741973877,
        "learning_rate": 0.00018200901890236138,
        "epoch": 0.13266666666666665,
        "step": 995
    },
    {
        "loss": 2.6674,
        "grad_norm": 3.2986648082733154,
        "learning_rate": 0.00018197298809764908,
        "epoch": 0.1328,
        "step": 996
    },
    {
        "loss": 1.8441,
        "grad_norm": 3.4219272136688232,
        "learning_rate": 0.0001819369248231569,
        "epoch": 0.13293333333333332,
        "step": 997
    },
    {
        "loss": 1.9045,
        "grad_norm": 4.061783790588379,
        "learning_rate": 0.00018190082909316958,
        "epoch": 0.13306666666666667,
        "step": 998
    },
    {
        "loss": 2.4254,
        "grad_norm": 3.117628574371338,
        "learning_rate": 0.00018186470092198482,
        "epoch": 0.1332,
        "step": 999
    },
    {
        "loss": 3.0627,
        "grad_norm": 4.899734973907471,
        "learning_rate": 0.00018182854032391306,
        "epoch": 0.13333333333333333,
        "step": 1000
    },
    {
        "loss": 2.2172,
        "grad_norm": 3.101271390914917,
        "learning_rate": 0.00018179234731327765,
        "epoch": 0.13346666666666668,
        "step": 1001
    },
    {
        "loss": 1.986,
        "grad_norm": 4.391778469085693,
        "learning_rate": 0.00018175612190441477,
        "epoch": 0.1336,
        "step": 1002
    },
    {
        "loss": 2.7299,
        "grad_norm": 2.6606974601745605,
        "learning_rate": 0.00018171986411167344,
        "epoch": 0.13373333333333334,
        "step": 1003
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.9424004554748535,
        "learning_rate": 0.00018168357394941548,
        "epoch": 0.13386666666666666,
        "step": 1004
    },
    {
        "loss": 2.7165,
        "grad_norm": 2.675398111343384,
        "learning_rate": 0.0001816472514320155,
        "epoch": 0.134,
        "step": 1005
    },
    {
        "loss": 2.7404,
        "grad_norm": 2.500725746154785,
        "learning_rate": 0.00018161089657386107,
        "epoch": 0.13413333333333333,
        "step": 1006
    },
    {
        "loss": 2.2622,
        "grad_norm": 3.1466150283813477,
        "learning_rate": 0.00018157450938935244,
        "epoch": 0.13426666666666667,
        "step": 1007
    },
    {
        "loss": 1.8299,
        "grad_norm": 3.7875478267669678,
        "learning_rate": 0.00018153808989290266,
        "epoch": 0.1344,
        "step": 1008
    },
    {
        "loss": 2.4004,
        "grad_norm": 4.798696041107178,
        "learning_rate": 0.0001815016380989376,
        "epoch": 0.13453333333333334,
        "step": 1009
    },
    {
        "loss": 3.3846,
        "grad_norm": 3.1938796043395996,
        "learning_rate": 0.00018146515402189602,
        "epoch": 0.13466666666666666,
        "step": 1010
    },
    {
        "loss": 2.414,
        "grad_norm": 2.9178884029388428,
        "learning_rate": 0.00018142863767622936,
        "epoch": 0.1348,
        "step": 1011
    },
    {
        "loss": 2.7288,
        "grad_norm": 2.545837879180908,
        "learning_rate": 0.00018139208907640183,
        "epoch": 0.13493333333333332,
        "step": 1012
    },
    {
        "loss": 2.5353,
        "grad_norm": 2.8694629669189453,
        "learning_rate": 0.0001813555082368905,
        "epoch": 0.13506666666666667,
        "step": 1013
    },
    {
        "loss": 2.3283,
        "grad_norm": 2.935431480407715,
        "learning_rate": 0.00018131889517218513,
        "epoch": 0.1352,
        "step": 1014
    },
    {
        "loss": 1.1802,
        "grad_norm": 3.1338791847229004,
        "learning_rate": 0.00018128224989678827,
        "epoch": 0.13533333333333333,
        "step": 1015
    },
    {
        "loss": 1.8828,
        "grad_norm": 4.557453632354736,
        "learning_rate": 0.00018124557242521535,
        "epoch": 0.13546666666666668,
        "step": 1016
    },
    {
        "loss": 1.9192,
        "grad_norm": 3.938453197479248,
        "learning_rate": 0.00018120886277199426,
        "epoch": 0.1356,
        "step": 1017
    },
    {
        "loss": 1.6751,
        "grad_norm": 3.6149415969848633,
        "learning_rate": 0.00018117212095166595,
        "epoch": 0.13573333333333334,
        "step": 1018
    },
    {
        "loss": 2.1646,
        "grad_norm": 2.3701694011688232,
        "learning_rate": 0.00018113534697878394,
        "epoch": 0.13586666666666666,
        "step": 1019
    },
    {
        "loss": 1.9565,
        "grad_norm": 3.8467800617218018,
        "learning_rate": 0.00018109854086791454,
        "epoch": 0.136,
        "step": 1020
    },
    {
        "loss": 2.9044,
        "grad_norm": 2.503638505935669,
        "learning_rate": 0.00018106170263363676,
        "epoch": 0.13613333333333333,
        "step": 1021
    },
    {
        "loss": 2.21,
        "grad_norm": 3.372223377227783,
        "learning_rate": 0.00018102483229054235,
        "epoch": 0.13626666666666667,
        "step": 1022
    },
    {
        "loss": 1.8308,
        "grad_norm": 3.7738966941833496,
        "learning_rate": 0.0001809879298532358,
        "epoch": 0.1364,
        "step": 1023
    },
    {
        "loss": 0.9429,
        "grad_norm": 2.986265182495117,
        "learning_rate": 0.00018095099533633425,
        "epoch": 0.13653333333333334,
        "step": 1024
    },
    {
        "loss": 2.1268,
        "grad_norm": 3.1885173320770264,
        "learning_rate": 0.00018091402875446765,
        "epoch": 0.13666666666666666,
        "step": 1025
    },
    {
        "loss": 2.9596,
        "grad_norm": 2.934636354446411,
        "learning_rate": 0.0001808770301222785,
        "epoch": 0.1368,
        "step": 1026
    },
    {
        "loss": 2.8802,
        "grad_norm": 3.548569917678833,
        "learning_rate": 0.00018083999945442219,
        "epoch": 0.13693333333333332,
        "step": 1027
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.5520102977752686,
        "learning_rate": 0.00018080293676556663,
        "epoch": 0.13706666666666667,
        "step": 1028
    },
    {
        "loss": 2.6908,
        "grad_norm": 3.462364673614502,
        "learning_rate": 0.00018076584207039247,
        "epoch": 0.1372,
        "step": 1029
    },
    {
        "loss": 2.4881,
        "grad_norm": 4.193855285644531,
        "learning_rate": 0.00018072871538359303,
        "epoch": 0.13733333333333334,
        "step": 1030
    },
    {
        "loss": 2.2952,
        "grad_norm": 1.7859172821044922,
        "learning_rate": 0.0001806915567198744,
        "epoch": 0.13746666666666665,
        "step": 1031
    },
    {
        "loss": 3.0132,
        "grad_norm": 3.3112003803253174,
        "learning_rate": 0.0001806543660939552,
        "epoch": 0.1376,
        "step": 1032
    },
    {
        "loss": 2.9582,
        "grad_norm": 2.101930618286133,
        "learning_rate": 0.00018061714352056676,
        "epoch": 0.13773333333333335,
        "step": 1033
    },
    {
        "loss": 2.1191,
        "grad_norm": 4.985789775848389,
        "learning_rate": 0.00018057988901445308,
        "epoch": 0.13786666666666667,
        "step": 1034
    },
    {
        "loss": 1.2795,
        "grad_norm": 5.8046040534973145,
        "learning_rate": 0.00018054260259037078,
        "epoch": 0.138,
        "step": 1035
    },
    {
        "loss": 2.5155,
        "grad_norm": 3.3152167797088623,
        "learning_rate": 0.00018050528426308916,
        "epoch": 0.13813333333333333,
        "step": 1036
    },
    {
        "loss": 2.5552,
        "grad_norm": 2.294085741043091,
        "learning_rate": 0.00018046793404739012,
        "epoch": 0.13826666666666668,
        "step": 1037
    },
    {
        "loss": 2.305,
        "grad_norm": 2.71515154838562,
        "learning_rate": 0.00018043055195806823,
        "epoch": 0.1384,
        "step": 1038
    },
    {
        "loss": 2.0563,
        "grad_norm": 2.9577579498291016,
        "learning_rate": 0.00018039313800993068,
        "epoch": 0.13853333333333334,
        "step": 1039
    },
    {
        "loss": 2.4012,
        "grad_norm": 2.6776773929595947,
        "learning_rate": 0.00018035569221779718,
        "epoch": 0.13866666666666666,
        "step": 1040
    },
    {
        "loss": 2.7881,
        "grad_norm": 3.1812233924865723,
        "learning_rate": 0.00018031821459650021,
        "epoch": 0.1388,
        "step": 1041
    },
    {
        "loss": 2.6879,
        "grad_norm": 2.7657060623168945,
        "learning_rate": 0.00018028070516088476,
        "epoch": 0.13893333333333333,
        "step": 1042
    },
    {
        "loss": 2.749,
        "grad_norm": 2.531081438064575,
        "learning_rate": 0.00018024316392580844,
        "epoch": 0.13906666666666667,
        "step": 1043
    },
    {
        "loss": 2.7328,
        "grad_norm": 3.618903875350952,
        "learning_rate": 0.00018020559090614144,
        "epoch": 0.1392,
        "step": 1044
    },
    {
        "loss": 1.1669,
        "grad_norm": 3.7712225914001465,
        "learning_rate": 0.00018016798611676662,
        "epoch": 0.13933333333333334,
        "step": 1045
    },
    {
        "loss": 2.2769,
        "grad_norm": 2.7020134925842285,
        "learning_rate": 0.00018013034957257932,
        "epoch": 0.13946666666666666,
        "step": 1046
    },
    {
        "loss": 2.1925,
        "grad_norm": 3.9335174560546875,
        "learning_rate": 0.00018009268128848754,
        "epoch": 0.1396,
        "step": 1047
    },
    {
        "loss": 2.7612,
        "grad_norm": 2.5079874992370605,
        "learning_rate": 0.00018005498127941175,
        "epoch": 0.13973333333333332,
        "step": 1048
    },
    {
        "loss": 2.9904,
        "grad_norm": 2.6441879272460938,
        "learning_rate": 0.00018001724956028515,
        "epoch": 0.13986666666666667,
        "step": 1049
    },
    {
        "loss": 1.9599,
        "grad_norm": 3.643120527267456,
        "learning_rate": 0.00017997948614605332,
        "epoch": 0.14,
        "step": 1050
    },
    {
        "loss": 2.2707,
        "grad_norm": 3.8250718116760254,
        "learning_rate": 0.0001799416910516745,
        "epoch": 0.14013333333333333,
        "step": 1051
    },
    {
        "loss": 2.7837,
        "grad_norm": 2.479194164276123,
        "learning_rate": 0.00017990386429211946,
        "epoch": 0.14026666666666668,
        "step": 1052
    },
    {
        "loss": 1.7408,
        "grad_norm": 4.573337554931641,
        "learning_rate": 0.00017986600588237148,
        "epoch": 0.1404,
        "step": 1053
    },
    {
        "loss": 2.2664,
        "grad_norm": 6.799980163574219,
        "learning_rate": 0.00017982811583742647,
        "epoch": 0.14053333333333334,
        "step": 1054
    },
    {
        "loss": 2.5092,
        "grad_norm": 3.1379144191741943,
        "learning_rate": 0.00017979019417229276,
        "epoch": 0.14066666666666666,
        "step": 1055
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.540316104888916,
        "learning_rate": 0.00017975224090199126,
        "epoch": 0.1408,
        "step": 1056
    },
    {
        "loss": 1.8269,
        "grad_norm": 3.021505117416382,
        "learning_rate": 0.0001797142560415554,
        "epoch": 0.14093333333333333,
        "step": 1057
    },
    {
        "loss": 2.306,
        "grad_norm": 4.068480491638184,
        "learning_rate": 0.0001796762396060311,
        "epoch": 0.14106666666666667,
        "step": 1058
    },
    {
        "loss": 3.2969,
        "grad_norm": 2.784348249435425,
        "learning_rate": 0.00017963819161047678,
        "epoch": 0.1412,
        "step": 1059
    },
    {
        "loss": 2.3688,
        "grad_norm": 3.0960958003997803,
        "learning_rate": 0.00017960011206996345,
        "epoch": 0.14133333333333334,
        "step": 1060
    },
    {
        "loss": 2.8632,
        "grad_norm": 2.6667094230651855,
        "learning_rate": 0.00017956200099957445,
        "epoch": 0.14146666666666666,
        "step": 1061
    },
    {
        "loss": 2.9244,
        "grad_norm": 2.551187515258789,
        "learning_rate": 0.00017952385841440576,
        "epoch": 0.1416,
        "step": 1062
    },
    {
        "loss": 2.5825,
        "grad_norm": 3.9218976497650146,
        "learning_rate": 0.0001794856843295658,
        "epoch": 0.14173333333333332,
        "step": 1063
    },
    {
        "loss": 2.8428,
        "grad_norm": 3.812889337539673,
        "learning_rate": 0.00017944747876017542,
        "epoch": 0.14186666666666667,
        "step": 1064
    },
    {
        "loss": 2.1633,
        "grad_norm": 5.482760906219482,
        "learning_rate": 0.00017940924172136802,
        "epoch": 0.142,
        "step": 1065
    },
    {
        "loss": 2.3842,
        "grad_norm": 2.440692663192749,
        "learning_rate": 0.00017937097322828937,
        "epoch": 0.14213333333333333,
        "step": 1066
    },
    {
        "loss": 2.0375,
        "grad_norm": 2.7416293621063232,
        "learning_rate": 0.00017933267329609779,
        "epoch": 0.14226666666666668,
        "step": 1067
    },
    {
        "loss": 1.8576,
        "grad_norm": 4.364641189575195,
        "learning_rate": 0.00017929434193996398,
        "epoch": 0.1424,
        "step": 1068
    },
    {
        "loss": 1.505,
        "grad_norm": 4.3969316482543945,
        "learning_rate": 0.00017925597917507115,
        "epoch": 0.14253333333333335,
        "step": 1069
    },
    {
        "loss": 2.0449,
        "grad_norm": 2.9054675102233887,
        "learning_rate": 0.0001792175850166149,
        "epoch": 0.14266666666666666,
        "step": 1070
    },
    {
        "loss": 2.8709,
        "grad_norm": 3.7049598693847656,
        "learning_rate": 0.0001791791594798033,
        "epoch": 0.1428,
        "step": 1071
    },
    {
        "loss": 3.1288,
        "grad_norm": 2.1830928325653076,
        "learning_rate": 0.0001791407025798568,
        "epoch": 0.14293333333333333,
        "step": 1072
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.274869918823242,
        "learning_rate": 0.00017910221433200843,
        "epoch": 0.14306666666666668,
        "step": 1073
    },
    {
        "loss": 2.083,
        "grad_norm": 2.862344741821289,
        "learning_rate": 0.00017906369475150335,
        "epoch": 0.1432,
        "step": 1074
    },
    {
        "loss": 3.3866,
        "grad_norm": 2.076961040496826,
        "learning_rate": 0.00017902514385359942,
        "epoch": 0.14333333333333334,
        "step": 1075
    },
    {
        "loss": 2.7182,
        "grad_norm": 2.0750129222869873,
        "learning_rate": 0.00017898656165356674,
        "epoch": 0.14346666666666666,
        "step": 1076
    },
    {
        "loss": 1.3926,
        "grad_norm": 3.291661024093628,
        "learning_rate": 0.00017894794816668782,
        "epoch": 0.1436,
        "step": 1077
    },
    {
        "loss": 3.1826,
        "grad_norm": 3.4504752159118652,
        "learning_rate": 0.00017890930340825763,
        "epoch": 0.14373333333333332,
        "step": 1078
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.7466349601745605,
        "learning_rate": 0.00017887062739358348,
        "epoch": 0.14386666666666667,
        "step": 1079
    },
    {
        "loss": 2.9442,
        "grad_norm": 2.5423176288604736,
        "learning_rate": 0.00017883192013798507,
        "epoch": 0.144,
        "step": 1080
    },
    {
        "loss": 2.1458,
        "grad_norm": 3.2662296295166016,
        "learning_rate": 0.00017879318165679449,
        "epoch": 0.14413333333333334,
        "step": 1081
    },
    {
        "loss": 2.4819,
        "grad_norm": 3.042890787124634,
        "learning_rate": 0.0001787544119653562,
        "epoch": 0.14426666666666665,
        "step": 1082
    },
    {
        "loss": 2.9941,
        "grad_norm": 3.148566484451294,
        "learning_rate": 0.00017871561107902695,
        "epoch": 0.1444,
        "step": 1083
    },
    {
        "loss": 1.7459,
        "grad_norm": 2.81083607673645,
        "learning_rate": 0.00017867677901317594,
        "epoch": 0.14453333333333335,
        "step": 1084
    },
    {
        "loss": 3.1257,
        "grad_norm": 3.631840705871582,
        "learning_rate": 0.00017863791578318468,
        "epoch": 0.14466666666666667,
        "step": 1085
    },
    {
        "loss": 2.1824,
        "grad_norm": 4.074713706970215,
        "learning_rate": 0.00017859902140444702,
        "epoch": 0.1448,
        "step": 1086
    },
    {
        "loss": 2.3153,
        "grad_norm": 4.598527908325195,
        "learning_rate": 0.00017856009589236918,
        "epoch": 0.14493333333333333,
        "step": 1087
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.4389469623565674,
        "learning_rate": 0.00017852113926236966,
        "epoch": 0.14506666666666668,
        "step": 1088
    },
    {
        "loss": 2.6603,
        "grad_norm": 3.643399715423584,
        "learning_rate": 0.0001784821515298793,
        "epoch": 0.1452,
        "step": 1089
    },
    {
        "loss": 1.9156,
        "grad_norm": 3.0214669704437256,
        "learning_rate": 0.00017844313271034133,
        "epoch": 0.14533333333333334,
        "step": 1090
    },
    {
        "loss": 2.0664,
        "grad_norm": 2.382924795150757,
        "learning_rate": 0.0001784040828192112,
        "epoch": 0.14546666666666666,
        "step": 1091
    },
    {
        "loss": 2.9802,
        "grad_norm": 3.1243860721588135,
        "learning_rate": 0.0001783650018719567,
        "epoch": 0.1456,
        "step": 1092
    },
    {
        "loss": 2.6628,
        "grad_norm": 2.8502495288848877,
        "learning_rate": 0.00017832588988405795,
        "epoch": 0.14573333333333333,
        "step": 1093
    },
    {
        "loss": 0.7827,
        "grad_norm": 2.9880049228668213,
        "learning_rate": 0.00017828674687100733,
        "epoch": 0.14586666666666667,
        "step": 1094
    },
    {
        "loss": 2.8881,
        "grad_norm": 1.534336805343628,
        "learning_rate": 0.0001782475728483095,
        "epoch": 0.146,
        "step": 1095
    },
    {
        "loss": 2.3816,
        "grad_norm": 2.3552193641662598,
        "learning_rate": 0.00017820836783148146,
        "epoch": 0.14613333333333334,
        "step": 1096
    },
    {
        "loss": 3.4091,
        "grad_norm": 2.3616931438446045,
        "learning_rate": 0.0001781691318360524,
        "epoch": 0.14626666666666666,
        "step": 1097
    },
    {
        "loss": 2.7623,
        "grad_norm": 3.773615837097168,
        "learning_rate": 0.00017812986487756392,
        "epoch": 0.1464,
        "step": 1098
    },
    {
        "loss": 2.593,
        "grad_norm": 4.778055667877197,
        "learning_rate": 0.0001780905669715697,
        "epoch": 0.14653333333333332,
        "step": 1099
    },
    {
        "loss": 1.9648,
        "grad_norm": 2.7025110721588135,
        "learning_rate": 0.00017805123813363584,
        "epoch": 0.14666666666666667,
        "step": 1100
    },
    {
        "loss": 2.4399,
        "grad_norm": 2.7476813793182373,
        "learning_rate": 0.00017801187837934056,
        "epoch": 0.1468,
        "step": 1101
    },
    {
        "loss": 1.2799,
        "grad_norm": 3.597226142883301,
        "learning_rate": 0.00017797248772427447,
        "epoch": 0.14693333333333333,
        "step": 1102
    },
    {
        "loss": 2.7261,
        "grad_norm": 4.180760383605957,
        "learning_rate": 0.0001779330661840403,
        "epoch": 0.14706666666666668,
        "step": 1103
    },
    {
        "loss": 2.4808,
        "grad_norm": 3.017174482345581,
        "learning_rate": 0.00017789361377425306,
        "epoch": 0.1472,
        "step": 1104
    },
    {
        "loss": 1.5058,
        "grad_norm": 4.302717685699463,
        "learning_rate": 0.00017785413051054,
        "epoch": 0.14733333333333334,
        "step": 1105
    },
    {
        "loss": 2.6096,
        "grad_norm": 3.436816453933716,
        "learning_rate": 0.00017781461640854057,
        "epoch": 0.14746666666666666,
        "step": 1106
    },
    {
        "loss": 2.9492,
        "grad_norm": 2.091872215270996,
        "learning_rate": 0.0001777750714839064,
        "epoch": 0.1476,
        "step": 1107
    },
    {
        "loss": 1.779,
        "grad_norm": 3.0264768600463867,
        "learning_rate": 0.00017773549575230145,
        "epoch": 0.14773333333333333,
        "step": 1108
    },
    {
        "loss": 2.595,
        "grad_norm": 3.167654514312744,
        "learning_rate": 0.0001776958892294017,
        "epoch": 0.14786666666666667,
        "step": 1109
    },
    {
        "loss": 2.7156,
        "grad_norm": 3.1052780151367188,
        "learning_rate": 0.00017765625193089552,
        "epoch": 0.148,
        "step": 1110
    },
    {
        "loss": 1.658,
        "grad_norm": 4.285245895385742,
        "learning_rate": 0.00017761658387248334,
        "epoch": 0.14813333333333334,
        "step": 1111
    },
    {
        "loss": 2.5532,
        "grad_norm": 2.658114194869995,
        "learning_rate": 0.00017757688506987783,
        "epoch": 0.14826666666666666,
        "step": 1112
    },
    {
        "loss": 3.3675,
        "grad_norm": 2.0946481227874756,
        "learning_rate": 0.00017753715553880378,
        "epoch": 0.1484,
        "step": 1113
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.4712107181549072,
        "learning_rate": 0.00017749739529499822,
        "epoch": 0.14853333333333332,
        "step": 1114
    },
    {
        "loss": 2.3706,
        "grad_norm": 2.1426196098327637,
        "learning_rate": 0.00017745760435421032,
        "epoch": 0.14866666666666667,
        "step": 1115
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.6839208602905273,
        "learning_rate": 0.00017741778273220145,
        "epoch": 0.1488,
        "step": 1116
    },
    {
        "loss": 2.0786,
        "grad_norm": 3.2353365421295166,
        "learning_rate": 0.00017737793044474498,
        "epoch": 0.14893333333333333,
        "step": 1117
    },
    {
        "loss": 2.5082,
        "grad_norm": 4.062304973602295,
        "learning_rate": 0.00017733804750762663,
        "epoch": 0.14906666666666665,
        "step": 1118
    },
    {
        "loss": 2.4296,
        "grad_norm": 3.6665804386138916,
        "learning_rate": 0.00017729813393664413,
        "epoch": 0.1492,
        "step": 1119
    },
    {
        "loss": 0.7138,
        "grad_norm": 3.1459813117980957,
        "learning_rate": 0.00017725818974760743,
        "epoch": 0.14933333333333335,
        "step": 1120
    },
    {
        "loss": 2.7342,
        "grad_norm": 3.495495557785034,
        "learning_rate": 0.00017721821495633846,
        "epoch": 0.14946666666666666,
        "step": 1121
    },
    {
        "loss": 2.4721,
        "grad_norm": 2.562969923019409,
        "learning_rate": 0.00017717820957867145,
        "epoch": 0.1496,
        "step": 1122
    },
    {
        "loss": 1.6226,
        "grad_norm": 4.036629676818848,
        "learning_rate": 0.00017713817363045265,
        "epoch": 0.14973333333333333,
        "step": 1123
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.330080270767212,
        "learning_rate": 0.00017709810712754047,
        "epoch": 0.14986666666666668,
        "step": 1124
    },
    {
        "loss": 2.1905,
        "grad_norm": 4.99859619140625,
        "learning_rate": 0.0001770580100858053,
        "epoch": 0.15,
        "step": 1125
    },
    {
        "loss": 2.8347,
        "grad_norm": 3.3742258548736572,
        "learning_rate": 0.0001770178825211298,
        "epoch": 0.15013333333333334,
        "step": 1126
    },
    {
        "loss": 2.4907,
        "grad_norm": 1.335975170135498,
        "learning_rate": 0.0001769777244494086,
        "epoch": 0.15026666666666666,
        "step": 1127
    },
    {
        "loss": 2.1991,
        "grad_norm": 3.887660264968872,
        "learning_rate": 0.00017693753588654844,
        "epoch": 0.1504,
        "step": 1128
    },
    {
        "loss": 2.6252,
        "grad_norm": 2.3271024227142334,
        "learning_rate": 0.00017689731684846817,
        "epoch": 0.15053333333333332,
        "step": 1129
    },
    {
        "loss": 1.6444,
        "grad_norm": 2.646176338195801,
        "learning_rate": 0.00017685706735109865,
        "epoch": 0.15066666666666667,
        "step": 1130
    },
    {
        "loss": 2.3425,
        "grad_norm": 2.574817419052124,
        "learning_rate": 0.00017681678741038288,
        "epoch": 0.1508,
        "step": 1131
    },
    {
        "loss": 2.216,
        "grad_norm": 4.12200927734375,
        "learning_rate": 0.00017677647704227587,
        "epoch": 0.15093333333333334,
        "step": 1132
    },
    {
        "loss": 2.2415,
        "grad_norm": 5.366277694702148,
        "learning_rate": 0.0001767361362627447,
        "epoch": 0.15106666666666665,
        "step": 1133
    },
    {
        "loss": 1.8266,
        "grad_norm": 2.393277645111084,
        "learning_rate": 0.0001766957650877685,
        "epoch": 0.1512,
        "step": 1134
    },
    {
        "loss": 1.233,
        "grad_norm": 3.1605544090270996,
        "learning_rate": 0.00017665536353333837,
        "epoch": 0.15133333333333332,
        "step": 1135
    },
    {
        "loss": 1.6275,
        "grad_norm": 3.559431552886963,
        "learning_rate": 0.00017661493161545756,
        "epoch": 0.15146666666666667,
        "step": 1136
    },
    {
        "loss": 1.3536,
        "grad_norm": 4.040065288543701,
        "learning_rate": 0.0001765744693501413,
        "epoch": 0.1516,
        "step": 1137
    },
    {
        "loss": 1.0647,
        "grad_norm": 2.836254835128784,
        "learning_rate": 0.0001765339767534168,
        "epoch": 0.15173333333333333,
        "step": 1138
    },
    {
        "loss": 3.1328,
        "grad_norm": 2.6688594818115234,
        "learning_rate": 0.00017649345384132333,
        "epoch": 0.15186666666666668,
        "step": 1139
    },
    {
        "loss": 2.2699,
        "grad_norm": 2.6590113639831543,
        "learning_rate": 0.00017645290062991213,
        "epoch": 0.152,
        "step": 1140
    },
    {
        "loss": 2.8765,
        "grad_norm": 2.982008457183838,
        "learning_rate": 0.00017641231713524648,
        "epoch": 0.15213333333333334,
        "step": 1141
    },
    {
        "loss": 1.9179,
        "grad_norm": 2.34600830078125,
        "learning_rate": 0.0001763717033734017,
        "epoch": 0.15226666666666666,
        "step": 1142
    },
    {
        "loss": 2.2265,
        "grad_norm": 3.0735435485839844,
        "learning_rate": 0.00017633105936046492,
        "epoch": 0.1524,
        "step": 1143
    },
    {
        "loss": 2.5182,
        "grad_norm": 2.400329113006592,
        "learning_rate": 0.00017629038511253546,
        "epoch": 0.15253333333333333,
        "step": 1144
    },
    {
        "loss": 2.5888,
        "grad_norm": 1.6872222423553467,
        "learning_rate": 0.0001762496806457245,
        "epoch": 0.15266666666666667,
        "step": 1145
    },
    {
        "loss": 2.8819,
        "grad_norm": 3.078801155090332,
        "learning_rate": 0.00017620894597615523,
        "epoch": 0.1528,
        "step": 1146
    },
    {
        "loss": 2.5279,
        "grad_norm": 3.564141035079956,
        "learning_rate": 0.0001761681811199628,
        "epoch": 0.15293333333333334,
        "step": 1147
    },
    {
        "loss": 3.0057,
        "grad_norm": 2.295621871948242,
        "learning_rate": 0.00017612738609329423,
        "epoch": 0.15306666666666666,
        "step": 1148
    },
    {
        "loss": 1.8998,
        "grad_norm": 3.295992136001587,
        "learning_rate": 0.0001760865609123087,
        "epoch": 0.1532,
        "step": 1149
    },
    {
        "loss": 2.3033,
        "grad_norm": 2.953218936920166,
        "learning_rate": 0.0001760457055931771,
        "epoch": 0.15333333333333332,
        "step": 1150
    },
    {
        "loss": 2.7895,
        "grad_norm": 2.9917619228363037,
        "learning_rate": 0.0001760048201520824,
        "epoch": 0.15346666666666667,
        "step": 1151
    },
    {
        "loss": 1.7187,
        "grad_norm": 1.9618088006973267,
        "learning_rate": 0.00017596390460521946,
        "epoch": 0.1536,
        "step": 1152
    },
    {
        "loss": 2.6547,
        "grad_norm": 4.6563849449157715,
        "learning_rate": 0.00017592295896879504,
        "epoch": 0.15373333333333333,
        "step": 1153
    },
    {
        "loss": 2.9403,
        "grad_norm": 1.8751742839813232,
        "learning_rate": 0.0001758819832590279,
        "epoch": 0.15386666666666668,
        "step": 1154
    },
    {
        "loss": 1.628,
        "grad_norm": 3.2996346950531006,
        "learning_rate": 0.00017584097749214864,
        "epoch": 0.154,
        "step": 1155
    },
    {
        "loss": 2.3174,
        "grad_norm": 2.566793918609619,
        "learning_rate": 0.00017579994168439976,
        "epoch": 0.15413333333333334,
        "step": 1156
    },
    {
        "loss": 2.505,
        "grad_norm": 2.4266791343688965,
        "learning_rate": 0.00017575887585203568,
        "epoch": 0.15426666666666666,
        "step": 1157
    },
    {
        "loss": 2.4827,
        "grad_norm": 2.290234327316284,
        "learning_rate": 0.00017571778001132274,
        "epoch": 0.1544,
        "step": 1158
    },
    {
        "loss": 3.2139,
        "grad_norm": 3.960063934326172,
        "learning_rate": 0.00017567665417853916,
        "epoch": 0.15453333333333333,
        "step": 1159
    },
    {
        "loss": 3.3253,
        "grad_norm": 8.493742942810059,
        "learning_rate": 0.00017563549836997498,
        "epoch": 0.15466666666666667,
        "step": 1160
    },
    {
        "loss": 2.2879,
        "grad_norm": 2.7097535133361816,
        "learning_rate": 0.00017559431260193216,
        "epoch": 0.1548,
        "step": 1161
    },
    {
        "loss": 2.6569,
        "grad_norm": 3.3207151889801025,
        "learning_rate": 0.00017555309689072453,
        "epoch": 0.15493333333333334,
        "step": 1162
    },
    {
        "loss": 2.5674,
        "grad_norm": 2.1658618450164795,
        "learning_rate": 0.00017551185125267782,
        "epoch": 0.15506666666666666,
        "step": 1163
    },
    {
        "loss": 2.4705,
        "grad_norm": 2.5317399501800537,
        "learning_rate": 0.0001754705757041295,
        "epoch": 0.1552,
        "step": 1164
    },
    {
        "loss": 2.473,
        "grad_norm": 2.4132819175720215,
        "learning_rate": 0.000175429270261429,
        "epoch": 0.15533333333333332,
        "step": 1165
    },
    {
        "loss": 2.0458,
        "grad_norm": 2.1380317211151123,
        "learning_rate": 0.00017538793494093751,
        "epoch": 0.15546666666666667,
        "step": 1166
    },
    {
        "loss": 1.5075,
        "grad_norm": 2.4443581104278564,
        "learning_rate": 0.00017534656975902815,
        "epoch": 0.1556,
        "step": 1167
    },
    {
        "loss": 0.8519,
        "grad_norm": 2.9447216987609863,
        "learning_rate": 0.00017530517473208574,
        "epoch": 0.15573333333333333,
        "step": 1168
    },
    {
        "loss": 2.6863,
        "grad_norm": 2.8222732543945312,
        "learning_rate": 0.00017526374987650705,
        "epoch": 0.15586666666666665,
        "step": 1169
    },
    {
        "loss": 2.5404,
        "grad_norm": 3.6588780879974365,
        "learning_rate": 0.00017522229520870055,
        "epoch": 0.156,
        "step": 1170
    },
    {
        "loss": 3.2265,
        "grad_norm": 2.252777576446533,
        "learning_rate": 0.0001751808107450866,
        "epoch": 0.15613333333333335,
        "step": 1171
    },
    {
        "loss": 2.5438,
        "grad_norm": 3.1180367469787598,
        "learning_rate": 0.00017513929650209736,
        "epoch": 0.15626666666666666,
        "step": 1172
    },
    {
        "loss": 2.7785,
        "grad_norm": 3.180021286010742,
        "learning_rate": 0.00017509775249617676,
        "epoch": 0.1564,
        "step": 1173
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.4395456314086914,
        "learning_rate": 0.00017505617874378045,
        "epoch": 0.15653333333333333,
        "step": 1174
    },
    {
        "loss": 1.9988,
        "grad_norm": 5.709000110626221,
        "learning_rate": 0.00017501457526137605,
        "epoch": 0.15666666666666668,
        "step": 1175
    },
    {
        "loss": 2.2598,
        "grad_norm": 2.7704544067382812,
        "learning_rate": 0.00017497294206544276,
        "epoch": 0.1568,
        "step": 1176
    },
    {
        "loss": 2.9649,
        "grad_norm": 2.4208714962005615,
        "learning_rate": 0.00017493127917247168,
        "epoch": 0.15693333333333334,
        "step": 1177
    },
    {
        "loss": 2.2151,
        "grad_norm": 3.863250970840454,
        "learning_rate": 0.00017488958659896554,
        "epoch": 0.15706666666666666,
        "step": 1178
    },
    {
        "loss": 2.5178,
        "grad_norm": 3.9442191123962402,
        "learning_rate": 0.00017484786436143903,
        "epoch": 0.1572,
        "step": 1179
    },
    {
        "loss": 2.9084,
        "grad_norm": 2.7006688117980957,
        "learning_rate": 0.00017480611247641838,
        "epoch": 0.15733333333333333,
        "step": 1180
    },
    {
        "loss": 1.7995,
        "grad_norm": 4.710792541503906,
        "learning_rate": 0.00017476433096044167,
        "epoch": 0.15746666666666667,
        "step": 1181
    },
    {
        "loss": 2.8128,
        "grad_norm": 1.1906532049179077,
        "learning_rate": 0.00017472251983005873,
        "epoch": 0.1576,
        "step": 1182
    },
    {
        "loss": 2.6914,
        "grad_norm": 4.701481819152832,
        "learning_rate": 0.00017468067910183106,
        "epoch": 0.15773333333333334,
        "step": 1183
    },
    {
        "loss": 1.6255,
        "grad_norm": 3.3305423259735107,
        "learning_rate": 0.00017463880879233195,
        "epoch": 0.15786666666666666,
        "step": 1184
    },
    {
        "loss": 2.6353,
        "grad_norm": 2.0863451957702637,
        "learning_rate": 0.00017459690891814632,
        "epoch": 0.158,
        "step": 1185
    },
    {
        "loss": 2.3937,
        "grad_norm": 2.7477807998657227,
        "learning_rate": 0.0001745549794958709,
        "epoch": 0.15813333333333332,
        "step": 1186
    },
    {
        "loss": 3.1677,
        "grad_norm": 2.794602870941162,
        "learning_rate": 0.0001745130205421141,
        "epoch": 0.15826666666666667,
        "step": 1187
    },
    {
        "loss": 3.2481,
        "grad_norm": 1.8709101676940918,
        "learning_rate": 0.00017447103207349592,
        "epoch": 0.1584,
        "step": 1188
    },
    {
        "loss": 2.2643,
        "grad_norm": 2.7566170692443848,
        "learning_rate": 0.0001744290141066482,
        "epoch": 0.15853333333333333,
        "step": 1189
    },
    {
        "loss": 1.9508,
        "grad_norm": 3.141164779663086,
        "learning_rate": 0.0001743869666582144,
        "epoch": 0.15866666666666668,
        "step": 1190
    },
    {
        "loss": 2.9159,
        "grad_norm": 2.147141218185425,
        "learning_rate": 0.0001743448897448496,
        "epoch": 0.1588,
        "step": 1191
    },
    {
        "loss": 2.5716,
        "grad_norm": 3.3000688552856445,
        "learning_rate": 0.00017430278338322066,
        "epoch": 0.15893333333333334,
        "step": 1192
    },
    {
        "loss": 1.4435,
        "grad_norm": 2.479477643966675,
        "learning_rate": 0.00017426064759000606,
        "epoch": 0.15906666666666666,
        "step": 1193
    },
    {
        "loss": 2.8305,
        "grad_norm": 2.8931140899658203,
        "learning_rate": 0.0001742184823818959,
        "epoch": 0.1592,
        "step": 1194
    },
    {
        "loss": 2.165,
        "grad_norm": 2.696364641189575,
        "learning_rate": 0.00017417628777559196,
        "epoch": 0.15933333333333333,
        "step": 1195
    },
    {
        "loss": 2.0375,
        "grad_norm": 3.1286284923553467,
        "learning_rate": 0.00017413406378780771,
        "epoch": 0.15946666666666667,
        "step": 1196
    },
    {
        "loss": 2.1384,
        "grad_norm": 3.671098232269287,
        "learning_rate": 0.00017409181043526815,
        "epoch": 0.1596,
        "step": 1197
    },
    {
        "loss": 2.9381,
        "grad_norm": 4.97926664352417,
        "learning_rate": 0.00017404952773471006,
        "epoch": 0.15973333333333334,
        "step": 1198
    },
    {
        "loss": 1.6216,
        "grad_norm": 4.432219505310059,
        "learning_rate": 0.0001740072157028817,
        "epoch": 0.15986666666666666,
        "step": 1199
    },
    {
        "loss": 1.5569,
        "grad_norm": 3.2086334228515625,
        "learning_rate": 0.000173964874356543,
        "epoch": 0.16,
        "step": 1200
    },
    {
        "loss": 2.5538,
        "grad_norm": 2.7191591262817383,
        "learning_rate": 0.00017392250371246558,
        "epoch": 0.16013333333333332,
        "step": 1201
    },
    {
        "loss": 1.9836,
        "grad_norm": 3.7372586727142334,
        "learning_rate": 0.00017388010378743254,
        "epoch": 0.16026666666666667,
        "step": 1202
    },
    {
        "loss": 3.0467,
        "grad_norm": 2.470357894897461,
        "learning_rate": 0.00017383767459823864,
        "epoch": 0.1604,
        "step": 1203
    },
    {
        "loss": 2.6994,
        "grad_norm": 2.9309632778167725,
        "learning_rate": 0.00017379521616169024,
        "epoch": 0.16053333333333333,
        "step": 1204
    },
    {
        "loss": 2.4249,
        "grad_norm": 2.927595376968384,
        "learning_rate": 0.00017375272849460527,
        "epoch": 0.16066666666666668,
        "step": 1205
    },
    {
        "loss": 2.4845,
        "grad_norm": 2.7649667263031006,
        "learning_rate": 0.00017371021161381327,
        "epoch": 0.1608,
        "step": 1206
    },
    {
        "loss": 2.4143,
        "grad_norm": 3.90674090385437,
        "learning_rate": 0.00017366766553615523,
        "epoch": 0.16093333333333334,
        "step": 1207
    },
    {
        "loss": 3.0741,
        "grad_norm": 2.0933969020843506,
        "learning_rate": 0.00017362509027848389,
        "epoch": 0.16106666666666666,
        "step": 1208
    },
    {
        "loss": 1.5585,
        "grad_norm": 2.657525062561035,
        "learning_rate": 0.00017358248585766338,
        "epoch": 0.1612,
        "step": 1209
    },
    {
        "loss": 2.171,
        "grad_norm": 2.8573803901672363,
        "learning_rate": 0.00017353985229056956,
        "epoch": 0.16133333333333333,
        "step": 1210
    },
    {
        "loss": 1.9422,
        "grad_norm": 5.206812381744385,
        "learning_rate": 0.00017349718959408964,
        "epoch": 0.16146666666666668,
        "step": 1211
    },
    {
        "loss": 2.7489,
        "grad_norm": 5.8902130126953125,
        "learning_rate": 0.00017345449778512247,
        "epoch": 0.1616,
        "step": 1212
    },
    {
        "loss": 2.6973,
        "grad_norm": 2.02238392829895,
        "learning_rate": 0.00017341177688057845,
        "epoch": 0.16173333333333334,
        "step": 1213
    },
    {
        "loss": 2.8648,
        "grad_norm": 2.027848482131958,
        "learning_rate": 0.00017336902689737944,
        "epoch": 0.16186666666666666,
        "step": 1214
    },
    {
        "loss": 0.9531,
        "grad_norm": 3.458312749862671,
        "learning_rate": 0.0001733262478524589,
        "epoch": 0.162,
        "step": 1215
    },
    {
        "loss": 2.0242,
        "grad_norm": 3.9614131450653076,
        "learning_rate": 0.0001732834397627617,
        "epoch": 0.16213333333333332,
        "step": 1216
    },
    {
        "loss": 2.3259,
        "grad_norm": 3.312699317932129,
        "learning_rate": 0.00017324060264524435,
        "epoch": 0.16226666666666667,
        "step": 1217
    },
    {
        "loss": 2.0916,
        "grad_norm": 2.6745967864990234,
        "learning_rate": 0.0001731977365168747,
        "epoch": 0.1624,
        "step": 1218
    },
    {
        "loss": 3.1287,
        "grad_norm": 3.9078757762908936,
        "learning_rate": 0.00017315484139463223,
        "epoch": 0.16253333333333334,
        "step": 1219
    },
    {
        "loss": 2.462,
        "grad_norm": 4.256917476654053,
        "learning_rate": 0.0001731119172955078,
        "epoch": 0.16266666666666665,
        "step": 1220
    },
    {
        "loss": 2.2678,
        "grad_norm": 3.2846250534057617,
        "learning_rate": 0.0001730689642365038,
        "epoch": 0.1628,
        "step": 1221
    },
    {
        "loss": 2.6431,
        "grad_norm": 4.033319473266602,
        "learning_rate": 0.0001730259822346341,
        "epoch": 0.16293333333333335,
        "step": 1222
    },
    {
        "loss": 2.8021,
        "grad_norm": 2.9506800174713135,
        "learning_rate": 0.000172982971306924,
        "epoch": 0.16306666666666667,
        "step": 1223
    },
    {
        "loss": 1.917,
        "grad_norm": 3.163954973220825,
        "learning_rate": 0.0001729399314704103,
        "epoch": 0.1632,
        "step": 1224
    },
    {
        "loss": 2.6023,
        "grad_norm": 2.82717227935791,
        "learning_rate": 0.00017289686274214118,
        "epoch": 0.16333333333333333,
        "step": 1225
    },
    {
        "loss": 3.0888,
        "grad_norm": 2.2771997451782227,
        "learning_rate": 0.00017285376513917632,
        "epoch": 0.16346666666666668,
        "step": 1226
    },
    {
        "loss": 1.5204,
        "grad_norm": 4.047907829284668,
        "learning_rate": 0.0001728106386785869,
        "epoch": 0.1636,
        "step": 1227
    },
    {
        "loss": 2.284,
        "grad_norm": 3.5600595474243164,
        "learning_rate": 0.00017276748337745535,
        "epoch": 0.16373333333333334,
        "step": 1228
    },
    {
        "loss": 2.823,
        "grad_norm": 2.660531759262085,
        "learning_rate": 0.00017272429925287573,
        "epoch": 0.16386666666666666,
        "step": 1229
    },
    {
        "loss": 2.7939,
        "grad_norm": 3.6659936904907227,
        "learning_rate": 0.00017268108632195335,
        "epoch": 0.164,
        "step": 1230
    },
    {
        "loss": 1.2095,
        "grad_norm": 4.303375244140625,
        "learning_rate": 0.00017263784460180503,
        "epoch": 0.16413333333333333,
        "step": 1231
    },
    {
        "loss": 2.7559,
        "grad_norm": 3.8746719360351562,
        "learning_rate": 0.00017259457410955892,
        "epoch": 0.16426666666666667,
        "step": 1232
    },
    {
        "loss": 2.9256,
        "grad_norm": 3.849870443344116,
        "learning_rate": 0.0001725512748623547,
        "epoch": 0.1644,
        "step": 1233
    },
    {
        "loss": 2.1016,
        "grad_norm": 3.5199782848358154,
        "learning_rate": 0.0001725079468773433,
        "epoch": 0.16453333333333334,
        "step": 1234
    },
    {
        "loss": 2.2851,
        "grad_norm": 2.8335752487182617,
        "learning_rate": 0.00017246459017168704,
        "epoch": 0.16466666666666666,
        "step": 1235
    },
    {
        "loss": 2.0664,
        "grad_norm": 3.955714225769043,
        "learning_rate": 0.00017242120476255969,
        "epoch": 0.1648,
        "step": 1236
    },
    {
        "loss": 2.5433,
        "grad_norm": 3.257256269454956,
        "learning_rate": 0.0001723777906671464,
        "epoch": 0.16493333333333332,
        "step": 1237
    },
    {
        "loss": 1.6285,
        "grad_norm": 2.9739174842834473,
        "learning_rate": 0.00017233434790264358,
        "epoch": 0.16506666666666667,
        "step": 1238
    },
    {
        "loss": 2.308,
        "grad_norm": 3.8795166015625,
        "learning_rate": 0.0001722908764862591,
        "epoch": 0.1652,
        "step": 1239
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.877751111984253,
        "learning_rate": 0.0001722473764352121,
        "epoch": 0.16533333333333333,
        "step": 1240
    },
    {
        "loss": 2.0803,
        "grad_norm": 2.539868116378784,
        "learning_rate": 0.0001722038477667331,
        "epoch": 0.16546666666666668,
        "step": 1241
    },
    {
        "loss": 2.3626,
        "grad_norm": 3.5495810508728027,
        "learning_rate": 0.000172160290498064,
        "epoch": 0.1656,
        "step": 1242
    },
    {
        "loss": 1.8516,
        "grad_norm": 4.198397636413574,
        "learning_rate": 0.00017211670464645798,
        "epoch": 0.16573333333333334,
        "step": 1243
    },
    {
        "loss": 2.3065,
        "grad_norm": 2.527679443359375,
        "learning_rate": 0.00017207309022917948,
        "epoch": 0.16586666666666666,
        "step": 1244
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.3574953079223633,
        "learning_rate": 0.00017202944726350437,
        "epoch": 0.166,
        "step": 1245
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.867108106613159,
        "learning_rate": 0.00017198577576671976,
        "epoch": 0.16613333333333333,
        "step": 1246
    },
    {
        "loss": 2.5869,
        "grad_norm": 2.80798602104187,
        "learning_rate": 0.00017194207575612407,
        "epoch": 0.16626666666666667,
        "step": 1247
    },
    {
        "loss": 2.6139,
        "grad_norm": 2.6788430213928223,
        "learning_rate": 0.00017189834724902706,
        "epoch": 0.1664,
        "step": 1248
    },
    {
        "loss": 2.0392,
        "grad_norm": 3.2495152950286865,
        "learning_rate": 0.00017185459026274971,
        "epoch": 0.16653333333333334,
        "step": 1249
    },
    {
        "loss": 2.3896,
        "grad_norm": 3.4464571475982666,
        "learning_rate": 0.0001718108048146243,
        "epoch": 0.16666666666666666,
        "step": 1250
    },
    {
        "loss": 2.4543,
        "grad_norm": 5.119032859802246,
        "learning_rate": 0.00017176699092199442,
        "epoch": 0.1668,
        "step": 1251
    },
    {
        "loss": 2.1842,
        "grad_norm": 3.9260241985321045,
        "learning_rate": 0.00017172314860221493,
        "epoch": 0.16693333333333332,
        "step": 1252
    },
    {
        "loss": 2.5044,
        "grad_norm": 3.0613186359405518,
        "learning_rate": 0.00017167927787265183,
        "epoch": 0.16706666666666667,
        "step": 1253
    },
    {
        "loss": 2.5102,
        "grad_norm": 3.624809980392456,
        "learning_rate": 0.00017163537875068257,
        "epoch": 0.1672,
        "step": 1254
    },
    {
        "loss": 2.2395,
        "grad_norm": 3.353663444519043,
        "learning_rate": 0.00017159145125369567,
        "epoch": 0.16733333333333333,
        "step": 1255
    },
    {
        "loss": 2.1324,
        "grad_norm": 3.2510464191436768,
        "learning_rate": 0.000171547495399091,
        "epoch": 0.16746666666666668,
        "step": 1256
    },
    {
        "loss": 2.3908,
        "grad_norm": 3.411982536315918,
        "learning_rate": 0.0001715035112042796,
        "epoch": 0.1676,
        "step": 1257
    },
    {
        "loss": 2.505,
        "grad_norm": 4.247777462005615,
        "learning_rate": 0.0001714594986866838,
        "epoch": 0.16773333333333335,
        "step": 1258
    },
    {
        "loss": 2.1732,
        "grad_norm": 2.680631160736084,
        "learning_rate": 0.00017141545786373706,
        "epoch": 0.16786666666666666,
        "step": 1259
    },
    {
        "loss": 2.5696,
        "grad_norm": 2.2118263244628906,
        "learning_rate": 0.0001713713887528841,
        "epoch": 0.168,
        "step": 1260
    },
    {
        "loss": 2.4664,
        "grad_norm": 4.372470378875732,
        "learning_rate": 0.00017132729137158087,
        "epoch": 0.16813333333333333,
        "step": 1261
    },
    {
        "loss": 2.5112,
        "grad_norm": 3.013909339904785,
        "learning_rate": 0.0001712831657372945,
        "epoch": 0.16826666666666668,
        "step": 1262
    },
    {
        "loss": 1.9984,
        "grad_norm": 3.5904369354248047,
        "learning_rate": 0.0001712390118675033,
        "epoch": 0.1684,
        "step": 1263
    },
    {
        "loss": 1.6521,
        "grad_norm": 3.5026533603668213,
        "learning_rate": 0.00017119482977969672,
        "epoch": 0.16853333333333334,
        "step": 1264
    },
    {
        "loss": 2.8794,
        "grad_norm": 1.8724020719528198,
        "learning_rate": 0.0001711506194913755,
        "epoch": 0.16866666666666666,
        "step": 1265
    },
    {
        "loss": 2.5405,
        "grad_norm": 2.001073122024536,
        "learning_rate": 0.00017110638102005145,
        "epoch": 0.1688,
        "step": 1266
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.6351619958877563,
        "learning_rate": 0.0001710621143832476,
        "epoch": 0.16893333333333332,
        "step": 1267
    },
    {
        "loss": 2.9433,
        "grad_norm": 3.1797802448272705,
        "learning_rate": 0.00017101781959849807,
        "epoch": 0.16906666666666667,
        "step": 1268
    },
    {
        "loss": 3.122,
        "grad_norm": 2.2334654331207275,
        "learning_rate": 0.00017097349668334825,
        "epoch": 0.1692,
        "step": 1269
    },
    {
        "loss": 2.792,
        "grad_norm": 2.70984148979187,
        "learning_rate": 0.00017092914565535456,
        "epoch": 0.16933333333333334,
        "step": 1270
    },
    {
        "loss": 1.3341,
        "grad_norm": 3.0776071548461914,
        "learning_rate": 0.00017088476653208455,
        "epoch": 0.16946666666666665,
        "step": 1271
    },
    {
        "loss": 2.8409,
        "grad_norm": 4.66419792175293,
        "learning_rate": 0.000170840359331117,
        "epoch": 0.1696,
        "step": 1272
    },
    {
        "loss": 2.5931,
        "grad_norm": 2.8838071823120117,
        "learning_rate": 0.00017079592407004177,
        "epoch": 0.16973333333333335,
        "step": 1273
    },
    {
        "loss": 1.8491,
        "grad_norm": 2.0824055671691895,
        "learning_rate": 0.00017075146076645976,
        "epoch": 0.16986666666666667,
        "step": 1274
    },
    {
        "loss": 1.1005,
        "grad_norm": 4.76742696762085,
        "learning_rate": 0.00017070696943798306,
        "epoch": 0.17,
        "step": 1275
    },
    {
        "loss": 2.8093,
        "grad_norm": 1.867425799369812,
        "learning_rate": 0.00017066245010223483,
        "epoch": 0.17013333333333333,
        "step": 1276
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.0829057693481445,
        "learning_rate": 0.00017061790277684934,
        "epoch": 0.17026666666666668,
        "step": 1277
    },
    {
        "loss": 2.3214,
        "grad_norm": 4.54084587097168,
        "learning_rate": 0.00017057332747947193,
        "epoch": 0.1704,
        "step": 1278
    },
    {
        "loss": 1.8253,
        "grad_norm": 3.0957093238830566,
        "learning_rate": 0.00017052872422775907,
        "epoch": 0.17053333333333334,
        "step": 1279
    },
    {
        "loss": 1.6051,
        "grad_norm": 4.370695114135742,
        "learning_rate": 0.00017048409303937817,
        "epoch": 0.17066666666666666,
        "step": 1280
    },
    {
        "loss": 1.8426,
        "grad_norm": 3.8707010746002197,
        "learning_rate": 0.0001704394339320079,
        "epoch": 0.1708,
        "step": 1281
    },
    {
        "loss": 2.1109,
        "grad_norm": 2.5278685092926025,
        "learning_rate": 0.00017039474692333778,
        "epoch": 0.17093333333333333,
        "step": 1282
    },
    {
        "loss": 3.3089,
        "grad_norm": 2.194533348083496,
        "learning_rate": 0.00017035003203106857,
        "epoch": 0.17106666666666667,
        "step": 1283
    },
    {
        "loss": 2.5735,
        "grad_norm": 3.070556879043579,
        "learning_rate": 0.00017030528927291196,
        "epoch": 0.1712,
        "step": 1284
    },
    {
        "loss": 2.0732,
        "grad_norm": 2.9622082710266113,
        "learning_rate": 0.00017026051866659073,
        "epoch": 0.17133333333333334,
        "step": 1285
    },
    {
        "loss": 2.5082,
        "grad_norm": 2.4061636924743652,
        "learning_rate": 0.00017021572022983861,
        "epoch": 0.17146666666666666,
        "step": 1286
    },
    {
        "loss": 2.1635,
        "grad_norm": 3.0927178859710693,
        "learning_rate": 0.00017017089398040048,
        "epoch": 0.1716,
        "step": 1287
    },
    {
        "loss": 1.0576,
        "grad_norm": 3.367366313934326,
        "learning_rate": 0.00017012603993603215,
        "epoch": 0.17173333333333332,
        "step": 1288
    },
    {
        "loss": 2.84,
        "grad_norm": 3.1483840942382812,
        "learning_rate": 0.0001700811581145004,
        "epoch": 0.17186666666666667,
        "step": 1289
    },
    {
        "loss": 1.9965,
        "grad_norm": 2.771055221557617,
        "learning_rate": 0.00017003624853358318,
        "epoch": 0.172,
        "step": 1290
    },
    {
        "loss": 2.3923,
        "grad_norm": 3.5877883434295654,
        "learning_rate": 0.00016999131121106924,
        "epoch": 0.17213333333333333,
        "step": 1291
    },
    {
        "loss": 2.7801,
        "grad_norm": 2.5994298458099365,
        "learning_rate": 0.00016994634616475845,
        "epoch": 0.17226666666666668,
        "step": 1292
    },
    {
        "loss": 0.9952,
        "grad_norm": 3.4050133228302,
        "learning_rate": 0.0001699013534124616,
        "epoch": 0.1724,
        "step": 1293
    },
    {
        "loss": 2.5396,
        "grad_norm": 4.212712287902832,
        "learning_rate": 0.00016985633297200053,
        "epoch": 0.17253333333333334,
        "step": 1294
    },
    {
        "loss": 1.2538,
        "grad_norm": 4.671871185302734,
        "learning_rate": 0.0001698112848612079,
        "epoch": 0.17266666666666666,
        "step": 1295
    },
    {
        "loss": 2.6358,
        "grad_norm": 2.785374879837036,
        "learning_rate": 0.00016976620909792746,
        "epoch": 0.1728,
        "step": 1296
    },
    {
        "loss": 2.7614,
        "grad_norm": 2.290423631668091,
        "learning_rate": 0.00016972110570001388,
        "epoch": 0.17293333333333333,
        "step": 1297
    },
    {
        "loss": 2.6198,
        "grad_norm": 3.5474398136138916,
        "learning_rate": 0.00016967597468533273,
        "epoch": 0.17306666666666667,
        "step": 1298
    },
    {
        "loss": 2.7874,
        "grad_norm": 1.8424519300460815,
        "learning_rate": 0.00016963081607176066,
        "epoch": 0.1732,
        "step": 1299
    },
    {
        "loss": 2.6351,
        "grad_norm": 3.014435052871704,
        "learning_rate": 0.00016958562987718503,
        "epoch": 0.17333333333333334,
        "step": 1300
    },
    {
        "loss": 2.4209,
        "grad_norm": 3.1356866359710693,
        "learning_rate": 0.0001695404161195043,
        "epoch": 0.17346666666666666,
        "step": 1301
    },
    {
        "loss": 2.1612,
        "grad_norm": 3.7249081134796143,
        "learning_rate": 0.0001694951748166278,
        "epoch": 0.1736,
        "step": 1302
    },
    {
        "loss": 2.3175,
        "grad_norm": 3.3086135387420654,
        "learning_rate": 0.00016944990598647577,
        "epoch": 0.17373333333333332,
        "step": 1303
    },
    {
        "loss": 2.4914,
        "grad_norm": 3.116772413253784,
        "learning_rate": 0.00016940460964697934,
        "epoch": 0.17386666666666667,
        "step": 1304
    },
    {
        "loss": 2.5201,
        "grad_norm": 2.8440446853637695,
        "learning_rate": 0.0001693592858160805,
        "epoch": 0.174,
        "step": 1305
    },
    {
        "loss": 2.0546,
        "grad_norm": 4.213351249694824,
        "learning_rate": 0.00016931393451173222,
        "epoch": 0.17413333333333333,
        "step": 1306
    },
    {
        "loss": 2.2206,
        "grad_norm": 2.822906732559204,
        "learning_rate": 0.00016926855575189834,
        "epoch": 0.17426666666666665,
        "step": 1307
    },
    {
        "loss": 2.2973,
        "grad_norm": 2.9998703002929688,
        "learning_rate": 0.00016922314955455345,
        "epoch": 0.1744,
        "step": 1308
    },
    {
        "loss": 2.5468,
        "grad_norm": 2.4422380924224854,
        "learning_rate": 0.0001691777159376832,
        "epoch": 0.17453333333333335,
        "step": 1309
    },
    {
        "loss": 2.0801,
        "grad_norm": 2.033237934112549,
        "learning_rate": 0.0001691322549192839,
        "epoch": 0.17466666666666666,
        "step": 1310
    },
    {
        "loss": 0.9095,
        "grad_norm": 2.755213737487793,
        "learning_rate": 0.00016908676651736292,
        "epoch": 0.1748,
        "step": 1311
    },
    {
        "loss": 2.4984,
        "grad_norm": 3.122253179550171,
        "learning_rate": 0.00016904125074993827,
        "epoch": 0.17493333333333333,
        "step": 1312
    },
    {
        "loss": 1.7759,
        "grad_norm": 3.5826001167297363,
        "learning_rate": 0.00016899570763503894,
        "epoch": 0.17506666666666668,
        "step": 1313
    },
    {
        "loss": 2.6667,
        "grad_norm": 2.213301420211792,
        "learning_rate": 0.00016895013719070475,
        "epoch": 0.1752,
        "step": 1314
    },
    {
        "loss": 2.2489,
        "grad_norm": 2.4334897994995117,
        "learning_rate": 0.00016890453943498626,
        "epoch": 0.17533333333333334,
        "step": 1315
    },
    {
        "loss": 1.9125,
        "grad_norm": 3.389411449432373,
        "learning_rate": 0.00016885891438594492,
        "epoch": 0.17546666666666666,
        "step": 1316
    },
    {
        "loss": 2.9161,
        "grad_norm": 3.149806261062622,
        "learning_rate": 0.00016881326206165292,
        "epoch": 0.1756,
        "step": 1317
    },
    {
        "loss": 2.2855,
        "grad_norm": 3.544620990753174,
        "learning_rate": 0.0001687675824801934,
        "epoch": 0.17573333333333332,
        "step": 1318
    },
    {
        "loss": 2.6669,
        "grad_norm": 3.5557901859283447,
        "learning_rate": 0.00016872187565966014,
        "epoch": 0.17586666666666667,
        "step": 1319
    },
    {
        "loss": 3.0825,
        "grad_norm": 2.9602489471435547,
        "learning_rate": 0.00016867614161815775,
        "epoch": 0.176,
        "step": 1320
    },
    {
        "loss": 2.9279,
        "grad_norm": 3.440009832382202,
        "learning_rate": 0.00016863038037380165,
        "epoch": 0.17613333333333334,
        "step": 1321
    },
    {
        "loss": 2.9339,
        "grad_norm": 3.3165371417999268,
        "learning_rate": 0.00016858459194471804,
        "epoch": 0.17626666666666665,
        "step": 1322
    },
    {
        "loss": 1.6329,
        "grad_norm": 3.565382242202759,
        "learning_rate": 0.0001685387763490438,
        "epoch": 0.1764,
        "step": 1323
    },
    {
        "loss": 2.8001,
        "grad_norm": 2.3659698963165283,
        "learning_rate": 0.0001684929336049268,
        "epoch": 0.17653333333333332,
        "step": 1324
    },
    {
        "loss": 2.6812,
        "grad_norm": 2.3687827587127686,
        "learning_rate": 0.00016844706373052537,
        "epoch": 0.17666666666666667,
        "step": 1325
    },
    {
        "loss": 2.5933,
        "grad_norm": 2.3975119590759277,
        "learning_rate": 0.0001684011667440088,
        "epoch": 0.1768,
        "step": 1326
    },
    {
        "loss": 2.1265,
        "grad_norm": 3.3779232501983643,
        "learning_rate": 0.00016835524266355697,
        "epoch": 0.17693333333333333,
        "step": 1327
    },
    {
        "loss": 3.0222,
        "grad_norm": 2.371495008468628,
        "learning_rate": 0.00016830929150736065,
        "epoch": 0.17706666666666668,
        "step": 1328
    },
    {
        "loss": 2.1293,
        "grad_norm": 3.1440072059631348,
        "learning_rate": 0.00016826331329362117,
        "epoch": 0.1772,
        "step": 1329
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.288228750228882,
        "learning_rate": 0.00016821730804055074,
        "epoch": 0.17733333333333334,
        "step": 1330
    },
    {
        "loss": 2.9781,
        "grad_norm": 3.382760524749756,
        "learning_rate": 0.00016817127576637209,
        "epoch": 0.17746666666666666,
        "step": 1331
    },
    {
        "loss": 2.0573,
        "grad_norm": 4.093057632446289,
        "learning_rate": 0.00016812521648931888,
        "epoch": 0.1776,
        "step": 1332
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.8636629581451416,
        "learning_rate": 0.00016807913022763522,
        "epoch": 0.17773333333333333,
        "step": 1333
    },
    {
        "loss": 2.447,
        "grad_norm": 3.8400089740753174,
        "learning_rate": 0.00016803301699957614,
        "epoch": 0.17786666666666667,
        "step": 1334
    },
    {
        "loss": 2.2498,
        "grad_norm": 2.1243038177490234,
        "learning_rate": 0.00016798687682340722,
        "epoch": 0.178,
        "step": 1335
    },
    {
        "loss": 1.3447,
        "grad_norm": 4.0825324058532715,
        "learning_rate": 0.00016794070971740469,
        "epoch": 0.17813333333333334,
        "step": 1336
    },
    {
        "loss": 2.3584,
        "grad_norm": 1.501424789428711,
        "learning_rate": 0.00016789451569985562,
        "epoch": 0.17826666666666666,
        "step": 1337
    },
    {
        "loss": 2.3432,
        "grad_norm": 2.90451717376709,
        "learning_rate": 0.00016784829478905746,
        "epoch": 0.1784,
        "step": 1338
    },
    {
        "loss": 2.6797,
        "grad_norm": 3.825320243835449,
        "learning_rate": 0.00016780204700331857,
        "epoch": 0.17853333333333332,
        "step": 1339
    },
    {
        "loss": 2.8541,
        "grad_norm": 2.6025350093841553,
        "learning_rate": 0.00016775577236095783,
        "epoch": 0.17866666666666667,
        "step": 1340
    },
    {
        "loss": 2.6038,
        "grad_norm": 2.792156934738159,
        "learning_rate": 0.0001677094708803048,
        "epoch": 0.1788,
        "step": 1341
    },
    {
        "loss": 2.8671,
        "grad_norm": 2.6623151302337646,
        "learning_rate": 0.00016766314257969964,
        "epoch": 0.17893333333333333,
        "step": 1342
    },
    {
        "loss": 2.7446,
        "grad_norm": 2.445728063583374,
        "learning_rate": 0.00016761678747749311,
        "epoch": 0.17906666666666668,
        "step": 1343
    },
    {
        "loss": 2.3654,
        "grad_norm": 2.224475860595703,
        "learning_rate": 0.00016757040559204668,
        "epoch": 0.1792,
        "step": 1344
    },
    {
        "loss": 2.8591,
        "grad_norm": 2.8898215293884277,
        "learning_rate": 0.00016752399694173236,
        "epoch": 0.17933333333333334,
        "step": 1345
    },
    {
        "loss": 1.7514,
        "grad_norm": 2.730936050415039,
        "learning_rate": 0.00016747756154493278,
        "epoch": 0.17946666666666666,
        "step": 1346
    },
    {
        "loss": 2.8625,
        "grad_norm": 2.476842164993286,
        "learning_rate": 0.00016743109942004116,
        "epoch": 0.1796,
        "step": 1347
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.5863115787506104,
        "learning_rate": 0.00016738461058546126,
        "epoch": 0.17973333333333333,
        "step": 1348
    },
    {
        "loss": 2.5635,
        "grad_norm": 4.237292766571045,
        "learning_rate": 0.00016733809505960751,
        "epoch": 0.17986666666666667,
        "step": 1349
    },
    {
        "loss": 1.6627,
        "grad_norm": 3.2458744049072266,
        "learning_rate": 0.00016729155286090486,
        "epoch": 0.18,
        "step": 1350
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.8684115409851074,
        "learning_rate": 0.00016724498400778884,
        "epoch": 0.18013333333333334,
        "step": 1351
    },
    {
        "loss": 2.7728,
        "grad_norm": 3.421274423599243,
        "learning_rate": 0.0001671983885187055,
        "epoch": 0.18026666666666666,
        "step": 1352
    },
    {
        "loss": 2.1866,
        "grad_norm": 2.588632822036743,
        "learning_rate": 0.00016715176641211152,
        "epoch": 0.1804,
        "step": 1353
    },
    {
        "loss": 1.3006,
        "grad_norm": 4.340254306793213,
        "learning_rate": 0.00016710511770647404,
        "epoch": 0.18053333333333332,
        "step": 1354
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.523369789123535,
        "learning_rate": 0.0001670584424202708,
        "epoch": 0.18066666666666667,
        "step": 1355
    },
    {
        "loss": 2.112,
        "grad_norm": 3.3477730751037598,
        "learning_rate": 0.00016701174057199,
        "epoch": 0.1808,
        "step": 1356
    },
    {
        "loss": 1.8461,
        "grad_norm": 3.4444169998168945,
        "learning_rate": 0.0001669650121801304,
        "epoch": 0.18093333333333333,
        "step": 1357
    },
    {
        "loss": 3.7924,
        "grad_norm": 4.15188455581665,
        "learning_rate": 0.00016691825726320133,
        "epoch": 0.18106666666666665,
        "step": 1358
    },
    {
        "loss": 3.0051,
        "grad_norm": 2.8382208347320557,
        "learning_rate": 0.00016687147583972255,
        "epoch": 0.1812,
        "step": 1359
    },
    {
        "loss": 0.8288,
        "grad_norm": 2.7254598140716553,
        "learning_rate": 0.0001668246679282243,
        "epoch": 0.18133333333333335,
        "step": 1360
    },
    {
        "loss": 2.7087,
        "grad_norm": 3.2649359703063965,
        "learning_rate": 0.0001667778335472474,
        "epoch": 0.18146666666666667,
        "step": 1361
    },
    {
        "loss": 1.6734,
        "grad_norm": 4.80494499206543,
        "learning_rate": 0.00016673097271534308,
        "epoch": 0.1816,
        "step": 1362
    },
    {
        "loss": 2.5268,
        "grad_norm": 2.352633476257324,
        "learning_rate": 0.0001666840854510731,
        "epoch": 0.18173333333333333,
        "step": 1363
    },
    {
        "loss": 1.1703,
        "grad_norm": 5.236898899078369,
        "learning_rate": 0.00016663717177300966,
        "epoch": 0.18186666666666668,
        "step": 1364
    },
    {
        "loss": 2.7683,
        "grad_norm": 1.6528363227844238,
        "learning_rate": 0.0001665902316997354,
        "epoch": 0.182,
        "step": 1365
    },
    {
        "loss": 1.0234,
        "grad_norm": 1.4273871183395386,
        "learning_rate": 0.00016654326524984347,
        "epoch": 0.18213333333333334,
        "step": 1366
    },
    {
        "loss": 0.8783,
        "grad_norm": 4.068820953369141,
        "learning_rate": 0.00016649627244193745,
        "epoch": 0.18226666666666666,
        "step": 1367
    },
    {
        "loss": 2.2651,
        "grad_norm": 3.495626211166382,
        "learning_rate": 0.0001664492532946313,
        "epoch": 0.1824,
        "step": 1368
    },
    {
        "loss": 1.6796,
        "grad_norm": 5.670708656311035,
        "learning_rate": 0.00016640220782654952,
        "epoch": 0.18253333333333333,
        "step": 1369
    },
    {
        "loss": 2.4283,
        "grad_norm": 2.346116542816162,
        "learning_rate": 0.000166355136056327,
        "epoch": 0.18266666666666667,
        "step": 1370
    },
    {
        "loss": 1.7127,
        "grad_norm": 3.072279930114746,
        "learning_rate": 0.00016630803800260892,
        "epoch": 0.1828,
        "step": 1371
    },
    {
        "loss": 2.4854,
        "grad_norm": 2.1711552143096924,
        "learning_rate": 0.00016626091368405108,
        "epoch": 0.18293333333333334,
        "step": 1372
    },
    {
        "loss": 2.1087,
        "grad_norm": 3.552940845489502,
        "learning_rate": 0.00016621376311931954,
        "epoch": 0.18306666666666666,
        "step": 1373
    },
    {
        "loss": 2.6825,
        "grad_norm": 3.796234369277954,
        "learning_rate": 0.00016616658632709084,
        "epoch": 0.1832,
        "step": 1374
    },
    {
        "loss": 2.8646,
        "grad_norm": 2.7555277347564697,
        "learning_rate": 0.0001661193833260518,
        "epoch": 0.18333333333333332,
        "step": 1375
    },
    {
        "loss": 2.5878,
        "grad_norm": 2.744004487991333,
        "learning_rate": 0.00016607215413489977,
        "epoch": 0.18346666666666667,
        "step": 1376
    },
    {
        "loss": 2.768,
        "grad_norm": 3.05808162689209,
        "learning_rate": 0.0001660248987723423,
        "epoch": 0.1836,
        "step": 1377
    },
    {
        "loss": 1.5244,
        "grad_norm": 4.065053462982178,
        "learning_rate": 0.00016597761725709747,
        "epoch": 0.18373333333333333,
        "step": 1378
    },
    {
        "loss": 2.8787,
        "grad_norm": 2.8247435092926025,
        "learning_rate": 0.0001659303096078937,
        "epoch": 0.18386666666666668,
        "step": 1379
    },
    {
        "loss": 1.5568,
        "grad_norm": 3.4061779975891113,
        "learning_rate": 0.00016588297584346954,
        "epoch": 0.184,
        "step": 1380
    },
    {
        "loss": 1.9362,
        "grad_norm": 6.2909722328186035,
        "learning_rate": 0.0001658356159825742,
        "epoch": 0.18413333333333334,
        "step": 1381
    },
    {
        "loss": 2.5023,
        "grad_norm": 2.9467697143554688,
        "learning_rate": 0.00016578823004396705,
        "epoch": 0.18426666666666666,
        "step": 1382
    },
    {
        "loss": 2.8224,
        "grad_norm": 3.0921945571899414,
        "learning_rate": 0.00016574081804641782,
        "epoch": 0.1844,
        "step": 1383
    },
    {
        "loss": 1.7595,
        "grad_norm": 3.507376194000244,
        "learning_rate": 0.0001656933800087065,
        "epoch": 0.18453333333333333,
        "step": 1384
    },
    {
        "loss": 1.2843,
        "grad_norm": 4.543293476104736,
        "learning_rate": 0.00016564591594962357,
        "epoch": 0.18466666666666667,
        "step": 1385
    },
    {
        "loss": 2.3875,
        "grad_norm": 2.8640308380126953,
        "learning_rate": 0.00016559842588796962,
        "epoch": 0.1848,
        "step": 1386
    },
    {
        "loss": 2.2551,
        "grad_norm": 2.7833030223846436,
        "learning_rate": 0.0001655509098425557,
        "epoch": 0.18493333333333334,
        "step": 1387
    },
    {
        "loss": 1.6341,
        "grad_norm": 3.5456860065460205,
        "learning_rate": 0.00016550336783220296,
        "epoch": 0.18506666666666666,
        "step": 1388
    },
    {
        "loss": 2.4577,
        "grad_norm": 2.498055934906006,
        "learning_rate": 0.0001654557998757431,
        "epoch": 0.1852,
        "step": 1389
    },
    {
        "loss": 2.121,
        "grad_norm": 4.0967488288879395,
        "learning_rate": 0.00016540820599201782,
        "epoch": 0.18533333333333332,
        "step": 1390
    },
    {
        "loss": 1.5758,
        "grad_norm": 3.5854032039642334,
        "learning_rate": 0.0001653605861998793,
        "epoch": 0.18546666666666667,
        "step": 1391
    },
    {
        "loss": 3.2936,
        "grad_norm": 3.027378559112549,
        "learning_rate": 0.00016531294051818986,
        "epoch": 0.1856,
        "step": 1392
    },
    {
        "loss": 2.7233,
        "grad_norm": 2.4199488162994385,
        "learning_rate": 0.00016526526896582216,
        "epoch": 0.18573333333333333,
        "step": 1393
    },
    {
        "loss": 2.1999,
        "grad_norm": 2.7152106761932373,
        "learning_rate": 0.00016521757156165898,
        "epoch": 0.18586666666666668,
        "step": 1394
    },
    {
        "loss": 2.4592,
        "grad_norm": 3.194429636001587,
        "learning_rate": 0.00016516984832459355,
        "epoch": 0.186,
        "step": 1395
    },
    {
        "loss": 2.316,
        "grad_norm": 3.418968677520752,
        "learning_rate": 0.0001651220992735291,
        "epoch": 0.18613333333333335,
        "step": 1396
    },
    {
        "loss": 2.0888,
        "grad_norm": 3.480813503265381,
        "learning_rate": 0.00016507432442737922,
        "epoch": 0.18626666666666666,
        "step": 1397
    },
    {
        "loss": 1.3084,
        "grad_norm": 3.6735949516296387,
        "learning_rate": 0.0001650265238050677,
        "epoch": 0.1864,
        "step": 1398
    },
    {
        "loss": 1.3675,
        "grad_norm": 4.322793960571289,
        "learning_rate": 0.00016497869742552855,
        "epoch": 0.18653333333333333,
        "step": 1399
    },
    {
        "loss": 2.3433,
        "grad_norm": 3.1027863025665283,
        "learning_rate": 0.00016493084530770595,
        "epoch": 0.18666666666666668,
        "step": 1400
    },
    {
        "loss": 0.6512,
        "grad_norm": 2.958493709564209,
        "learning_rate": 0.00016488296747055426,
        "epoch": 0.1868,
        "step": 1401
    },
    {
        "loss": 2.7621,
        "grad_norm": 2.1047556400299072,
        "learning_rate": 0.00016483506393303804,
        "epoch": 0.18693333333333334,
        "step": 1402
    },
    {
        "loss": 2.8161,
        "grad_norm": 2.8108456134796143,
        "learning_rate": 0.0001647871347141321,
        "epoch": 0.18706666666666666,
        "step": 1403
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.9544423818588257,
        "learning_rate": 0.00016473917983282136,
        "epoch": 0.1872,
        "step": 1404
    },
    {
        "loss": 2.425,
        "grad_norm": 3.0309667587280273,
        "learning_rate": 0.00016469119930810088,
        "epoch": 0.18733333333333332,
        "step": 1405
    },
    {
        "loss": 1.7869,
        "grad_norm": 2.849717855453491,
        "learning_rate": 0.00016464319315897594,
        "epoch": 0.18746666666666667,
        "step": 1406
    },
    {
        "loss": 2.5453,
        "grad_norm": 2.643608331680298,
        "learning_rate": 0.0001645951614044619,
        "epoch": 0.1876,
        "step": 1407
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.2860007286071777,
        "learning_rate": 0.00016454710406358435,
        "epoch": 0.18773333333333334,
        "step": 1408
    },
    {
        "loss": 2.9152,
        "grad_norm": 4.893406391143799,
        "learning_rate": 0.0001644990211553789,
        "epoch": 0.18786666666666665,
        "step": 1409
    },
    {
        "loss": 2.4724,
        "grad_norm": 2.2009661197662354,
        "learning_rate": 0.00016445091269889147,
        "epoch": 0.188,
        "step": 1410
    },
    {
        "loss": 2.6472,
        "grad_norm": 2.682859182357788,
        "learning_rate": 0.00016440277871317783,
        "epoch": 0.18813333333333335,
        "step": 1411
    },
    {
        "loss": 3.0086,
        "grad_norm": 3.2951481342315674,
        "learning_rate": 0.00016435461921730416,
        "epoch": 0.18826666666666667,
        "step": 1412
    },
    {
        "loss": 2.0905,
        "grad_norm": 2.649686336517334,
        "learning_rate": 0.0001643064342303465,
        "epoch": 0.1884,
        "step": 1413
    },
    {
        "loss": 1.9588,
        "grad_norm": 3.1135737895965576,
        "learning_rate": 0.00016425822377139112,
        "epoch": 0.18853333333333333,
        "step": 1414
    },
    {
        "loss": 1.549,
        "grad_norm": 3.090487480163574,
        "learning_rate": 0.0001642099878595343,
        "epoch": 0.18866666666666668,
        "step": 1415
    },
    {
        "loss": 1.3719,
        "grad_norm": 4.362665176391602,
        "learning_rate": 0.0001641617265138826,
        "epoch": 0.1888,
        "step": 1416
    },
    {
        "loss": 2.7632,
        "grad_norm": 4.081661701202393,
        "learning_rate": 0.00016411343975355236,
        "epoch": 0.18893333333333334,
        "step": 1417
    },
    {
        "loss": 2.434,
        "grad_norm": 2.9608659744262695,
        "learning_rate": 0.00016406512759767016,
        "epoch": 0.18906666666666666,
        "step": 1418
    },
    {
        "loss": 2.7444,
        "grad_norm": 1.7487146854400635,
        "learning_rate": 0.00016401679006537262,
        "epoch": 0.1892,
        "step": 1419
    },
    {
        "loss": 2.6496,
        "grad_norm": 2.5673792362213135,
        "learning_rate": 0.00016396842717580643,
        "epoch": 0.18933333333333333,
        "step": 1420
    },
    {
        "loss": 2.4034,
        "grad_norm": 2.337589979171753,
        "learning_rate": 0.00016392003894812827,
        "epoch": 0.18946666666666667,
        "step": 1421
    },
    {
        "loss": 1.9774,
        "grad_norm": 2.8681490421295166,
        "learning_rate": 0.0001638716254015048,
        "epoch": 0.1896,
        "step": 1422
    },
    {
        "loss": 2.3998,
        "grad_norm": 3.33728289604187,
        "learning_rate": 0.00016382318655511297,
        "epoch": 0.18973333333333334,
        "step": 1423
    },
    {
        "loss": 2.419,
        "grad_norm": 2.668409824371338,
        "learning_rate": 0.00016377472242813944,
        "epoch": 0.18986666666666666,
        "step": 1424
    },
    {
        "loss": 1.3925,
        "grad_norm": 3.7759251594543457,
        "learning_rate": 0.00016372623303978102,
        "epoch": 0.19,
        "step": 1425
    },
    {
        "loss": 2.0228,
        "grad_norm": 4.201838970184326,
        "learning_rate": 0.0001636777184092446,
        "epoch": 0.19013333333333332,
        "step": 1426
    },
    {
        "loss": 1.4849,
        "grad_norm": 2.3870339393615723,
        "learning_rate": 0.00016362917855574694,
        "epoch": 0.19026666666666667,
        "step": 1427
    },
    {
        "loss": 2.3158,
        "grad_norm": 2.43806791305542,
        "learning_rate": 0.00016358061349851482,
        "epoch": 0.1904,
        "step": 1428
    },
    {
        "loss": 2.0152,
        "grad_norm": 4.2885332107543945,
        "learning_rate": 0.00016353202325678504,
        "epoch": 0.19053333333333333,
        "step": 1429
    },
    {
        "loss": 1.0345,
        "grad_norm": 3.968635082244873,
        "learning_rate": 0.00016348340784980435,
        "epoch": 0.19066666666666668,
        "step": 1430
    },
    {
        "loss": 2.8434,
        "grad_norm": 3.128328800201416,
        "learning_rate": 0.00016343476729682954,
        "epoch": 0.1908,
        "step": 1431
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.5086638927459717,
        "learning_rate": 0.00016338610161712724,
        "epoch": 0.19093333333333334,
        "step": 1432
    },
    {
        "loss": 2.0432,
        "grad_norm": 3.8331081867218018,
        "learning_rate": 0.00016333741082997412,
        "epoch": 0.19106666666666666,
        "step": 1433
    },
    {
        "loss": 1.1728,
        "grad_norm": 2.9525482654571533,
        "learning_rate": 0.00016328869495465674,
        "epoch": 0.1912,
        "step": 1434
    },
    {
        "loss": 2.3516,
        "grad_norm": 2.89704966545105,
        "learning_rate": 0.00016323995401047161,
        "epoch": 0.19133333333333333,
        "step": 1435
    },
    {
        "loss": 4.1334,
        "grad_norm": 3.2071399688720703,
        "learning_rate": 0.00016319118801672524,
        "epoch": 0.19146666666666667,
        "step": 1436
    },
    {
        "loss": 2.9772,
        "grad_norm": 3.9951932430267334,
        "learning_rate": 0.00016314239699273398,
        "epoch": 0.1916,
        "step": 1437
    },
    {
        "loss": 2.1068,
        "grad_norm": 3.307652711868286,
        "learning_rate": 0.00016309358095782412,
        "epoch": 0.19173333333333334,
        "step": 1438
    },
    {
        "loss": 1.8966,
        "grad_norm": 3.2987473011016846,
        "learning_rate": 0.00016304473993133186,
        "epoch": 0.19186666666666666,
        "step": 1439
    },
    {
        "loss": 1.5872,
        "grad_norm": 3.155724287033081,
        "learning_rate": 0.00016299587393260327,
        "epoch": 0.192,
        "step": 1440
    },
    {
        "loss": 2.7464,
        "grad_norm": 2.1669342517852783,
        "learning_rate": 0.00016294698298099437,
        "epoch": 0.19213333333333332,
        "step": 1441
    },
    {
        "loss": 2.8705,
        "grad_norm": 2.512458562850952,
        "learning_rate": 0.00016289806709587107,
        "epoch": 0.19226666666666667,
        "step": 1442
    },
    {
        "loss": 2.8878,
        "grad_norm": 2.4169225692749023,
        "learning_rate": 0.00016284912629660898,
        "epoch": 0.1924,
        "step": 1443
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.9293742179870605,
        "learning_rate": 0.00016280016060259386,
        "epoch": 0.19253333333333333,
        "step": 1444
    },
    {
        "loss": 3.1389,
        "grad_norm": 3.697192430496216,
        "learning_rate": 0.00016275117003322116,
        "epoch": 0.19266666666666668,
        "step": 1445
    },
    {
        "loss": 2.302,
        "grad_norm": 2.9303712844848633,
        "learning_rate": 0.00016270215460789618,
        "epoch": 0.1928,
        "step": 1446
    },
    {
        "loss": 2.4801,
        "grad_norm": 2.697800636291504,
        "learning_rate": 0.0001626531143460341,
        "epoch": 0.19293333333333335,
        "step": 1447
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.56185245513916,
        "learning_rate": 0.00016260404926705997,
        "epoch": 0.19306666666666666,
        "step": 1448
    },
    {
        "loss": 2.814,
        "grad_norm": 2.625800132751465,
        "learning_rate": 0.00016255495939040854,
        "epoch": 0.1932,
        "step": 1449
    },
    {
        "loss": 2.4068,
        "grad_norm": 3.98793888092041,
        "learning_rate": 0.0001625058447355246,
        "epoch": 0.19333333333333333,
        "step": 1450
    },
    {
        "loss": 1.5284,
        "grad_norm": 4.926736831665039,
        "learning_rate": 0.00016245670532186258,
        "epoch": 0.19346666666666668,
        "step": 1451
    },
    {
        "loss": 2.1239,
        "grad_norm": 3.1886680126190186,
        "learning_rate": 0.00016240754116888674,
        "epoch": 0.1936,
        "step": 1452
    },
    {
        "loss": 2.121,
        "grad_norm": 3.787985324859619,
        "learning_rate": 0.00016235835229607117,
        "epoch": 0.19373333333333334,
        "step": 1453
    },
    {
        "loss": 1.9922,
        "grad_norm": 3.419679880142212,
        "learning_rate": 0.00016230913872289983,
        "epoch": 0.19386666666666666,
        "step": 1454
    },
    {
        "loss": 1.2412,
        "grad_norm": 3.507927656173706,
        "learning_rate": 0.00016225990046886632,
        "epoch": 0.194,
        "step": 1455
    },
    {
        "loss": 1.6514,
        "grad_norm": 3.967228651046753,
        "learning_rate": 0.00016221063755347408,
        "epoch": 0.19413333333333332,
        "step": 1456
    },
    {
        "loss": 1.8654,
        "grad_norm": 3.164762496948242,
        "learning_rate": 0.0001621613499962363,
        "epoch": 0.19426666666666667,
        "step": 1457
    },
    {
        "loss": 2.5794,
        "grad_norm": 2.8993325233459473,
        "learning_rate": 0.00016211203781667602,
        "epoch": 0.1944,
        "step": 1458
    },
    {
        "loss": 1.7967,
        "grad_norm": 2.012657403945923,
        "learning_rate": 0.0001620627010343259,
        "epoch": 0.19453333333333334,
        "step": 1459
    },
    {
        "loss": 2.279,
        "grad_norm": 3.9152932167053223,
        "learning_rate": 0.00016201333966872842,
        "epoch": 0.19466666666666665,
        "step": 1460
    },
    {
        "loss": 2.5101,
        "grad_norm": 2.354278326034546,
        "learning_rate": 0.00016196395373943578,
        "epoch": 0.1948,
        "step": 1461
    },
    {
        "loss": 1.304,
        "grad_norm": 2.7111759185791016,
        "learning_rate": 0.00016191454326600996,
        "epoch": 0.19493333333333332,
        "step": 1462
    },
    {
        "loss": 2.6246,
        "grad_norm": 3.495457887649536,
        "learning_rate": 0.00016186510826802258,
        "epoch": 0.19506666666666667,
        "step": 1463
    },
    {
        "loss": 2.7834,
        "grad_norm": 4.230824947357178,
        "learning_rate": 0.00016181564876505496,
        "epoch": 0.1952,
        "step": 1464
    },
    {
        "loss": 2.852,
        "grad_norm": 2.085498332977295,
        "learning_rate": 0.00016176616477669832,
        "epoch": 0.19533333333333333,
        "step": 1465
    },
    {
        "loss": 1.7938,
        "grad_norm": 4.485021591186523,
        "learning_rate": 0.0001617166563225533,
        "epoch": 0.19546666666666668,
        "step": 1466
    },
    {
        "loss": 2.2114,
        "grad_norm": 2.589911937713623,
        "learning_rate": 0.0001616671234222304,
        "epoch": 0.1956,
        "step": 1467
    },
    {
        "loss": 2.3205,
        "grad_norm": 4.0208964347839355,
        "learning_rate": 0.00016161756609534984,
        "epoch": 0.19573333333333334,
        "step": 1468
    },
    {
        "loss": 1.7374,
        "grad_norm": 3.2693898677825928,
        "learning_rate": 0.00016156798436154138,
        "epoch": 0.19586666666666666,
        "step": 1469
    },
    {
        "loss": 1.98,
        "grad_norm": 3.5852909088134766,
        "learning_rate": 0.00016151837824044448,
        "epoch": 0.196,
        "step": 1470
    },
    {
        "loss": 2.4477,
        "grad_norm": 3.881863594055176,
        "learning_rate": 0.00016146874775170838,
        "epoch": 0.19613333333333333,
        "step": 1471
    },
    {
        "loss": 2.6693,
        "grad_norm": 2.6800293922424316,
        "learning_rate": 0.00016141909291499183,
        "epoch": 0.19626666666666667,
        "step": 1472
    },
    {
        "loss": 2.4764,
        "grad_norm": 2.6837971210479736,
        "learning_rate": 0.0001613694137499633,
        "epoch": 0.1964,
        "step": 1473
    },
    {
        "loss": 2.6198,
        "grad_norm": 3.9381144046783447,
        "learning_rate": 0.00016131971027630085,
        "epoch": 0.19653333333333334,
        "step": 1474
    },
    {
        "loss": 2.1096,
        "grad_norm": 2.2587459087371826,
        "learning_rate": 0.00016126998251369225,
        "epoch": 0.19666666666666666,
        "step": 1475
    },
    {
        "loss": 1.9141,
        "grad_norm": 3.4363317489624023,
        "learning_rate": 0.0001612202304818348,
        "epoch": 0.1968,
        "step": 1476
    },
    {
        "loss": 2.543,
        "grad_norm": 2.279825210571289,
        "learning_rate": 0.00016117045420043543,
        "epoch": 0.19693333333333332,
        "step": 1477
    },
    {
        "loss": 1.7453,
        "grad_norm": 3.2005300521850586,
        "learning_rate": 0.00016112065368921076,
        "epoch": 0.19706666666666667,
        "step": 1478
    },
    {
        "loss": 2.8119,
        "grad_norm": 2.3281378746032715,
        "learning_rate": 0.00016107082896788685,
        "epoch": 0.1972,
        "step": 1479
    },
    {
        "loss": 2.0519,
        "grad_norm": 4.01904296875,
        "learning_rate": 0.00016102098005619955,
        "epoch": 0.19733333333333333,
        "step": 1480
    },
    {
        "loss": 2.93,
        "grad_norm": 4.867825508117676,
        "learning_rate": 0.00016097110697389409,
        "epoch": 0.19746666666666668,
        "step": 1481
    },
    {
        "loss": 3.3265,
        "grad_norm": 3.844489574432373,
        "learning_rate": 0.00016092120974072537,
        "epoch": 0.1976,
        "step": 1482
    },
    {
        "loss": 3.0062,
        "grad_norm": 3.3659584522247314,
        "learning_rate": 0.00016087128837645795,
        "epoch": 0.19773333333333334,
        "step": 1483
    },
    {
        "loss": 2.6533,
        "grad_norm": 3.0027785301208496,
        "learning_rate": 0.00016082134290086578,
        "epoch": 0.19786666666666666,
        "step": 1484
    },
    {
        "loss": 2.6252,
        "grad_norm": 3.5815351009368896,
        "learning_rate": 0.0001607713733337324,
        "epoch": 0.198,
        "step": 1485
    },
    {
        "loss": 1.695,
        "grad_norm": 2.513819932937622,
        "learning_rate": 0.000160721379694851,
        "epoch": 0.19813333333333333,
        "step": 1486
    },
    {
        "loss": 2.1315,
        "grad_norm": 3.312833547592163,
        "learning_rate": 0.0001606713620040242,
        "epoch": 0.19826666666666667,
        "step": 1487
    },
    {
        "loss": 2.817,
        "grad_norm": 2.4205169677734375,
        "learning_rate": 0.00016062132028106418,
        "epoch": 0.1984,
        "step": 1488
    },
    {
        "loss": 2.4101,
        "grad_norm": 1.881455659866333,
        "learning_rate": 0.0001605712545457926,
        "epoch": 0.19853333333333334,
        "step": 1489
    },
    {
        "loss": 2.371,
        "grad_norm": 2.8404829502105713,
        "learning_rate": 0.00016052116481804076,
        "epoch": 0.19866666666666666,
        "step": 1490
    },
    {
        "loss": 2.5609,
        "grad_norm": 2.3194034099578857,
        "learning_rate": 0.0001604710511176493,
        "epoch": 0.1988,
        "step": 1491
    },
    {
        "loss": 2.9916,
        "grad_norm": 2.9079349040985107,
        "learning_rate": 0.0001604209134644684,
        "epoch": 0.19893333333333332,
        "step": 1492
    },
    {
        "loss": 2.585,
        "grad_norm": 2.3015856742858887,
        "learning_rate": 0.00016037075187835784,
        "epoch": 0.19906666666666667,
        "step": 1493
    },
    {
        "loss": 2.6179,
        "grad_norm": 2.588634729385376,
        "learning_rate": 0.00016032056637918672,
        "epoch": 0.1992,
        "step": 1494
    },
    {
        "loss": 2.2897,
        "grad_norm": 3.8012425899505615,
        "learning_rate": 0.00016027035698683374,
        "epoch": 0.19933333333333333,
        "step": 1495
    },
    {
        "loss": 2.5042,
        "grad_norm": 3.991486072540283,
        "learning_rate": 0.00016022012372118703,
        "epoch": 0.19946666666666665,
        "step": 1496
    },
    {
        "loss": 0.8009,
        "grad_norm": 2.2629430294036865,
        "learning_rate": 0.00016016986660214407,
        "epoch": 0.1996,
        "step": 1497
    },
    {
        "loss": 2.0306,
        "grad_norm": 4.325502395629883,
        "learning_rate": 0.0001601195856496119,
        "epoch": 0.19973333333333335,
        "step": 1498
    },
    {
        "loss": 2.7494,
        "grad_norm": 3.0482654571533203,
        "learning_rate": 0.00016006928088350706,
        "epoch": 0.19986666666666666,
        "step": 1499
    },
    {
        "loss": 1.8116,
        "grad_norm": 3.61289644241333,
        "learning_rate": 0.00016001895232375536,
        "epoch": 0.2,
        "step": 1500
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.8483901023864746,
        "learning_rate": 0.00015996859999029215,
        "epoch": 0.20013333333333333,
        "step": 1501
    },
    {
        "loss": 3.0909,
        "grad_norm": 2.638601541519165,
        "learning_rate": 0.0001599182239030621,
        "epoch": 0.20026666666666668,
        "step": 1502
    },
    {
        "loss": 3.1266,
        "grad_norm": 3.301966905593872,
        "learning_rate": 0.00015986782408201943,
        "epoch": 0.2004,
        "step": 1503
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.9294071197509766,
        "learning_rate": 0.00015981740054712764,
        "epoch": 0.20053333333333334,
        "step": 1504
    },
    {
        "loss": 1.8323,
        "grad_norm": 2.3025906085968018,
        "learning_rate": 0.00015976695331835968,
        "epoch": 0.20066666666666666,
        "step": 1505
    },
    {
        "loss": 2.7441,
        "grad_norm": 2.1879308223724365,
        "learning_rate": 0.00015971648241569784,
        "epoch": 0.2008,
        "step": 1506
    },
    {
        "loss": 2.3648,
        "grad_norm": 2.8700480461120605,
        "learning_rate": 0.00015966598785913387,
        "epoch": 0.20093333333333332,
        "step": 1507
    },
    {
        "loss": 2.8622,
        "grad_norm": 3.791722536087036,
        "learning_rate": 0.00015961546966866875,
        "epoch": 0.20106666666666667,
        "step": 1508
    },
    {
        "loss": 2.1025,
        "grad_norm": 3.0870981216430664,
        "learning_rate": 0.000159564927864313,
        "epoch": 0.2012,
        "step": 1509
    },
    {
        "loss": 2.5781,
        "grad_norm": 3.373898506164551,
        "learning_rate": 0.00015951436246608637,
        "epoch": 0.20133333333333334,
        "step": 1510
    },
    {
        "loss": 3.1345,
        "grad_norm": 2.0380711555480957,
        "learning_rate": 0.00015946377349401796,
        "epoch": 0.20146666666666666,
        "step": 1511
    },
    {
        "loss": 3.082,
        "grad_norm": 2.943566083908081,
        "learning_rate": 0.00015941316096814622,
        "epoch": 0.2016,
        "step": 1512
    },
    {
        "loss": 1.9174,
        "grad_norm": 3.333338737487793,
        "learning_rate": 0.00015936252490851902,
        "epoch": 0.20173333333333332,
        "step": 1513
    },
    {
        "loss": 2.0574,
        "grad_norm": 3.001828670501709,
        "learning_rate": 0.0001593118653351934,
        "epoch": 0.20186666666666667,
        "step": 1514
    },
    {
        "loss": 2.0619,
        "grad_norm": 3.474118232727051,
        "learning_rate": 0.00015926118226823583,
        "epoch": 0.202,
        "step": 1515
    },
    {
        "loss": 2.6352,
        "grad_norm": 2.2963197231292725,
        "learning_rate": 0.00015921047572772204,
        "epoch": 0.20213333333333333,
        "step": 1516
    },
    {
        "loss": 2.1926,
        "grad_norm": 2.59302020072937,
        "learning_rate": 0.00015915974573373708,
        "epoch": 0.20226666666666668,
        "step": 1517
    },
    {
        "loss": 2.5818,
        "grad_norm": 2.6538357734680176,
        "learning_rate": 0.0001591089923063752,
        "epoch": 0.2024,
        "step": 1518
    },
    {
        "loss": 2.8386,
        "grad_norm": 2.51649808883667,
        "learning_rate": 0.00015905821546574012,
        "epoch": 0.20253333333333334,
        "step": 1519
    },
    {
        "loss": 1.9948,
        "grad_norm": 3.261455535888672,
        "learning_rate": 0.0001590074152319446,
        "epoch": 0.20266666666666666,
        "step": 1520
    },
    {
        "loss": 1.8224,
        "grad_norm": 4.102803707122803,
        "learning_rate": 0.0001589565916251109,
        "epoch": 0.2028,
        "step": 1521
    },
    {
        "loss": 1.1877,
        "grad_norm": 4.283975124359131,
        "learning_rate": 0.00015890574466537034,
        "epoch": 0.20293333333333333,
        "step": 1522
    },
    {
        "loss": 2.2645,
        "grad_norm": 3.8996410369873047,
        "learning_rate": 0.0001588548743728636,
        "epoch": 0.20306666666666667,
        "step": 1523
    },
    {
        "loss": 1.4681,
        "grad_norm": 4.100818634033203,
        "learning_rate": 0.00015880398076774057,
        "epoch": 0.2032,
        "step": 1524
    },
    {
        "loss": 2.4775,
        "grad_norm": 3.633527994155884,
        "learning_rate": 0.00015875306387016038,
        "epoch": 0.20333333333333334,
        "step": 1525
    },
    {
        "loss": 3.1401,
        "grad_norm": 2.659850835800171,
        "learning_rate": 0.00015870212370029142,
        "epoch": 0.20346666666666666,
        "step": 1526
    },
    {
        "loss": 2.2097,
        "grad_norm": 3.4063563346862793,
        "learning_rate": 0.00015865116027831124,
        "epoch": 0.2036,
        "step": 1527
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.171842336654663,
        "learning_rate": 0.00015860017362440663,
        "epoch": 0.20373333333333332,
        "step": 1528
    },
    {
        "loss": 1.9086,
        "grad_norm": 4.438048839569092,
        "learning_rate": 0.00015854916375877357,
        "epoch": 0.20386666666666667,
        "step": 1529
    },
    {
        "loss": 2.7632,
        "grad_norm": 3.3183794021606445,
        "learning_rate": 0.0001584981307016172,
        "epoch": 0.204,
        "step": 1530
    },
    {
        "loss": 2.2528,
        "grad_norm": 3.730560064315796,
        "learning_rate": 0.00015844707447315197,
        "epoch": 0.20413333333333333,
        "step": 1531
    },
    {
        "loss": 2.7069,
        "grad_norm": 3.192322254180908,
        "learning_rate": 0.0001583959950936014,
        "epoch": 0.20426666666666668,
        "step": 1532
    },
    {
        "loss": 2.3652,
        "grad_norm": 3.6658029556274414,
        "learning_rate": 0.00015834489258319818,
        "epoch": 0.2044,
        "step": 1533
    },
    {
        "loss": 2.7446,
        "grad_norm": 4.336833953857422,
        "learning_rate": 0.0001582937669621842,
        "epoch": 0.20453333333333334,
        "step": 1534
    },
    {
        "loss": 2.5194,
        "grad_norm": 3.655118942260742,
        "learning_rate": 0.0001582426182508105,
        "epoch": 0.20466666666666666,
        "step": 1535
    },
    {
        "loss": 2.1898,
        "grad_norm": 3.509305953979492,
        "learning_rate": 0.00015819144646933724,
        "epoch": 0.2048,
        "step": 1536
    },
    {
        "loss": 1.8264,
        "grad_norm": 3.690519094467163,
        "learning_rate": 0.00015814025163803373,
        "epoch": 0.20493333333333333,
        "step": 1537
    },
    {
        "loss": 0.8818,
        "grad_norm": 3.4596774578094482,
        "learning_rate": 0.0001580890337771785,
        "epoch": 0.20506666666666667,
        "step": 1538
    },
    {
        "loss": 1.8274,
        "grad_norm": 2.907953977584839,
        "learning_rate": 0.00015803779290705903,
        "epoch": 0.2052,
        "step": 1539
    },
    {
        "loss": 3.1706,
        "grad_norm": 3.309884548187256,
        "learning_rate": 0.00015798652904797203,
        "epoch": 0.20533333333333334,
        "step": 1540
    },
    {
        "loss": 2.9687,
        "grad_norm": 2.1271729469299316,
        "learning_rate": 0.00015793524222022326,
        "epoch": 0.20546666666666666,
        "step": 1541
    },
    {
        "loss": 2.6975,
        "grad_norm": 3.286999464035034,
        "learning_rate": 0.00015788393244412768,
        "epoch": 0.2056,
        "step": 1542
    },
    {
        "loss": 2.1513,
        "grad_norm": 2.776747465133667,
        "learning_rate": 0.0001578325997400092,
        "epoch": 0.20573333333333332,
        "step": 1543
    },
    {
        "loss": 1.9762,
        "grad_norm": 3.3985939025878906,
        "learning_rate": 0.0001577812441282009,
        "epoch": 0.20586666666666667,
        "step": 1544
    },
    {
        "loss": 2.7062,
        "grad_norm": 4.552306175231934,
        "learning_rate": 0.00015772986562904493,
        "epoch": 0.206,
        "step": 1545
    },
    {
        "loss": 4.0747,
        "grad_norm": 4.9847588539123535,
        "learning_rate": 0.00015767846426289244,
        "epoch": 0.20613333333333334,
        "step": 1546
    },
    {
        "loss": 1.8829,
        "grad_norm": 3.5128421783447266,
        "learning_rate": 0.00015762704005010378,
        "epoch": 0.20626666666666665,
        "step": 1547
    },
    {
        "loss": 2.5065,
        "grad_norm": 2.4334347248077393,
        "learning_rate": 0.00015757559301104817,
        "epoch": 0.2064,
        "step": 1548
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.641674518585205,
        "learning_rate": 0.00015752412316610398,
        "epoch": 0.20653333333333335,
        "step": 1549
    },
    {
        "loss": 2.0391,
        "grad_norm": 2.452056407928467,
        "learning_rate": 0.0001574726305356586,
        "epoch": 0.20666666666666667,
        "step": 1550
    },
    {
        "loss": 1.5649,
        "grad_norm": 3.3295516967773438,
        "learning_rate": 0.00015742111514010842,
        "epoch": 0.2068,
        "step": 1551
    },
    {
        "loss": 2.127,
        "grad_norm": 4.150802135467529,
        "learning_rate": 0.0001573695769998589,
        "epoch": 0.20693333333333333,
        "step": 1552
    },
    {
        "loss": 2.6603,
        "grad_norm": 4.227504730224609,
        "learning_rate": 0.00015731801613532445,
        "epoch": 0.20706666666666668,
        "step": 1553
    },
    {
        "loss": 2.98,
        "grad_norm": 3.111635208129883,
        "learning_rate": 0.00015726643256692847,
        "epoch": 0.2072,
        "step": 1554
    },
    {
        "loss": 2.5071,
        "grad_norm": 4.186911106109619,
        "learning_rate": 0.00015721482631510342,
        "epoch": 0.20733333333333334,
        "step": 1555
    },
    {
        "loss": 2.4356,
        "grad_norm": 3.531318187713623,
        "learning_rate": 0.00015716319740029073,
        "epoch": 0.20746666666666666,
        "step": 1556
    },
    {
        "loss": 2.4565,
        "grad_norm": 2.2086188793182373,
        "learning_rate": 0.00015711154584294076,
        "epoch": 0.2076,
        "step": 1557
    },
    {
        "loss": 3.2514,
        "grad_norm": 4.1686930656433105,
        "learning_rate": 0.0001570598716635129,
        "epoch": 0.20773333333333333,
        "step": 1558
    },
    {
        "loss": 2.5841,
        "grad_norm": 3.8106119632720947,
        "learning_rate": 0.00015700817488247543,
        "epoch": 0.20786666666666667,
        "step": 1559
    },
    {
        "loss": 2.8428,
        "grad_norm": 3.2051401138305664,
        "learning_rate": 0.00015695645552030567,
        "epoch": 0.208,
        "step": 1560
    },
    {
        "loss": 2.3251,
        "grad_norm": 2.9736788272857666,
        "learning_rate": 0.00015690471359748975,
        "epoch": 0.20813333333333334,
        "step": 1561
    },
    {
        "loss": 2.242,
        "grad_norm": 2.3657758235931396,
        "learning_rate": 0.0001568529491345229,
        "epoch": 0.20826666666666666,
        "step": 1562
    },
    {
        "loss": 2.2563,
        "grad_norm": 3.8305165767669678,
        "learning_rate": 0.00015680116215190917,
        "epoch": 0.2084,
        "step": 1563
    },
    {
        "loss": 2.5171,
        "grad_norm": 3.071478843688965,
        "learning_rate": 0.0001567493526701616,
        "epoch": 0.20853333333333332,
        "step": 1564
    },
    {
        "loss": 2.5871,
        "grad_norm": 2.9086384773254395,
        "learning_rate": 0.000156697520709802,
        "epoch": 0.20866666666666667,
        "step": 1565
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.9975297451019287,
        "learning_rate": 0.00015664566629136135,
        "epoch": 0.2088,
        "step": 1566
    },
    {
        "loss": 2.0167,
        "grad_norm": 4.658627033233643,
        "learning_rate": 0.0001565937894353792,
        "epoch": 0.20893333333333333,
        "step": 1567
    },
    {
        "loss": 3.3219,
        "grad_norm": 5.272056579589844,
        "learning_rate": 0.00015654189016240424,
        "epoch": 0.20906666666666668,
        "step": 1568
    },
    {
        "loss": 2.1083,
        "grad_norm": 3.1563985347747803,
        "learning_rate": 0.00015648996849299392,
        "epoch": 0.2092,
        "step": 1569
    },
    {
        "loss": 2.828,
        "grad_norm": 2.4874331951141357,
        "learning_rate": 0.0001564380244477146,
        "epoch": 0.20933333333333334,
        "step": 1570
    },
    {
        "loss": 3.0127,
        "grad_norm": 3.502659797668457,
        "learning_rate": 0.00015638605804714145,
        "epoch": 0.20946666666666666,
        "step": 1571
    },
    {
        "loss": 2.0385,
        "grad_norm": 4.271702766418457,
        "learning_rate": 0.0001563340693118586,
        "epoch": 0.2096,
        "step": 1572
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.72712779045105,
        "learning_rate": 0.00015628205826245898,
        "epoch": 0.20973333333333333,
        "step": 1573
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.2068188190460205,
        "learning_rate": 0.00015623002491954425,
        "epoch": 0.20986666666666667,
        "step": 1574
    },
    {
        "loss": 1.9348,
        "grad_norm": 3.141781806945801,
        "learning_rate": 0.00015617796930372509,
        "epoch": 0.21,
        "step": 1575
    },
    {
        "loss": 2.9698,
        "grad_norm": 3.429330825805664,
        "learning_rate": 0.0001561258914356208,
        "epoch": 0.21013333333333334,
        "step": 1576
    },
    {
        "loss": 0.9401,
        "grad_norm": 3.874067544937134,
        "learning_rate": 0.00015607379133585978,
        "epoch": 0.21026666666666666,
        "step": 1577
    },
    {
        "loss": 2.2472,
        "grad_norm": 3.460475206375122,
        "learning_rate": 0.00015602166902507886,
        "epoch": 0.2104,
        "step": 1578
    },
    {
        "loss": 2.6847,
        "grad_norm": 2.4573357105255127,
        "learning_rate": 0.00015596952452392397,
        "epoch": 0.21053333333333332,
        "step": 1579
    },
    {
        "loss": 2.9523,
        "grad_norm": 2.8009822368621826,
        "learning_rate": 0.00015591735785304973,
        "epoch": 0.21066666666666667,
        "step": 1580
    },
    {
        "loss": 1.924,
        "grad_norm": 3.1202361583709717,
        "learning_rate": 0.00015586516903311946,
        "epoch": 0.2108,
        "step": 1581
    },
    {
        "loss": 1.1569,
        "grad_norm": 4.511174201965332,
        "learning_rate": 0.00015581295808480544,
        "epoch": 0.21093333333333333,
        "step": 1582
    },
    {
        "loss": 2.4781,
        "grad_norm": 2.9485104084014893,
        "learning_rate": 0.00015576072502878848,
        "epoch": 0.21106666666666668,
        "step": 1583
    },
    {
        "loss": 3.0105,
        "grad_norm": 3.6504344940185547,
        "learning_rate": 0.00015570846988575838,
        "epoch": 0.2112,
        "step": 1584
    },
    {
        "loss": 2.6773,
        "grad_norm": 3.654489278793335,
        "learning_rate": 0.00015565619267641354,
        "epoch": 0.21133333333333335,
        "step": 1585
    },
    {
        "loss": 2.5533,
        "grad_norm": 3.17460560798645,
        "learning_rate": 0.00015560389342146111,
        "epoch": 0.21146666666666666,
        "step": 1586
    },
    {
        "loss": 0.9347,
        "grad_norm": 3.4478824138641357,
        "learning_rate": 0.00015555157214161707,
        "epoch": 0.2116,
        "step": 1587
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.669924259185791,
        "learning_rate": 0.00015549922885760599,
        "epoch": 0.21173333333333333,
        "step": 1588
    },
    {
        "loss": 2.3496,
        "grad_norm": 2.930105686187744,
        "learning_rate": 0.00015544686359016122,
        "epoch": 0.21186666666666668,
        "step": 1589
    },
    {
        "loss": 3.168,
        "grad_norm": 1.9295655488967896,
        "learning_rate": 0.00015539447636002488,
        "epoch": 0.212,
        "step": 1590
    },
    {
        "loss": 2.2415,
        "grad_norm": 3.1423490047454834,
        "learning_rate": 0.00015534206718794774,
        "epoch": 0.21213333333333334,
        "step": 1591
    },
    {
        "loss": 2.8091,
        "grad_norm": 3.9816365242004395,
        "learning_rate": 0.00015528963609468915,
        "epoch": 0.21226666666666666,
        "step": 1592
    },
    {
        "loss": 2.6611,
        "grad_norm": 1.893011212348938,
        "learning_rate": 0.00015523718310101736,
        "epoch": 0.2124,
        "step": 1593
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.1756784915924072,
        "learning_rate": 0.0001551847082277091,
        "epoch": 0.21253333333333332,
        "step": 1594
    },
    {
        "loss": 2.1018,
        "grad_norm": 4.786736011505127,
        "learning_rate": 0.00015513221149554988,
        "epoch": 0.21266666666666667,
        "step": 1595
    },
    {
        "loss": 3.4562,
        "grad_norm": 4.039874076843262,
        "learning_rate": 0.0001550796929253338,
        "epoch": 0.2128,
        "step": 1596
    },
    {
        "loss": 1.4521,
        "grad_norm": 3.391479969024658,
        "learning_rate": 0.0001550271525378637,
        "epoch": 0.21293333333333334,
        "step": 1597
    },
    {
        "loss": 2.8358,
        "grad_norm": 3.544283151626587,
        "learning_rate": 0.000154974590353951,
        "epoch": 0.21306666666666665,
        "step": 1598
    },
    {
        "loss": 2.552,
        "grad_norm": 2.293469190597534,
        "learning_rate": 0.00015492200639441572,
        "epoch": 0.2132,
        "step": 1599
    },
    {
        "loss": 2.4424,
        "grad_norm": 3.0784122943878174,
        "learning_rate": 0.00015486940068008655,
        "epoch": 0.21333333333333335,
        "step": 1600
    },
    {
        "loss": 2.8111,
        "grad_norm": 2.3282604217529297,
        "learning_rate": 0.00015481677323180083,
        "epoch": 0.21346666666666667,
        "step": 1601
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.8139092922210693,
        "learning_rate": 0.00015476412407040445,
        "epoch": 0.2136,
        "step": 1602
    },
    {
        "loss": 2.1105,
        "grad_norm": 3.185385227203369,
        "learning_rate": 0.00015471145321675192,
        "epoch": 0.21373333333333333,
        "step": 1603
    },
    {
        "loss": 2.0576,
        "grad_norm": 3.84332013130188,
        "learning_rate": 0.0001546587606917063,
        "epoch": 0.21386666666666668,
        "step": 1604
    },
    {
        "loss": 2.3746,
        "grad_norm": 3.823817729949951,
        "learning_rate": 0.00015460604651613936,
        "epoch": 0.214,
        "step": 1605
    },
    {
        "loss": 2.5791,
        "grad_norm": 2.7287843227386475,
        "learning_rate": 0.00015455331071093136,
        "epoch": 0.21413333333333334,
        "step": 1606
    },
    {
        "loss": 2.034,
        "grad_norm": 3.471320152282715,
        "learning_rate": 0.00015450055329697105,
        "epoch": 0.21426666666666666,
        "step": 1607
    },
    {
        "loss": 3.0356,
        "grad_norm": 2.6962296962738037,
        "learning_rate": 0.0001544477742951559,
        "epoch": 0.2144,
        "step": 1608
    },
    {
        "loss": 2.707,
        "grad_norm": 3.0607504844665527,
        "learning_rate": 0.00015439497372639183,
        "epoch": 0.21453333333333333,
        "step": 1609
    },
    {
        "loss": 2.0084,
        "grad_norm": 3.0899739265441895,
        "learning_rate": 0.0001543421516115933,
        "epoch": 0.21466666666666667,
        "step": 1610
    },
    {
        "loss": 2.5931,
        "grad_norm": 3.1370270252227783,
        "learning_rate": 0.00015428930797168337,
        "epoch": 0.2148,
        "step": 1611
    },
    {
        "loss": 1.6541,
        "grad_norm": 4.37609338760376,
        "learning_rate": 0.00015423644282759358,
        "epoch": 0.21493333333333334,
        "step": 1612
    },
    {
        "loss": 2.4972,
        "grad_norm": 3.1974000930786133,
        "learning_rate": 0.00015418355620026394,
        "epoch": 0.21506666666666666,
        "step": 1613
    },
    {
        "loss": 2.631,
        "grad_norm": 4.339012145996094,
        "learning_rate": 0.00015413064811064308,
        "epoch": 0.2152,
        "step": 1614
    },
    {
        "loss": 2.269,
        "grad_norm": 2.852412462234497,
        "learning_rate": 0.00015407771857968807,
        "epoch": 0.21533333333333332,
        "step": 1615
    },
    {
        "loss": 2.2941,
        "grad_norm": 4.6749982833862305,
        "learning_rate": 0.00015402476762836444,
        "epoch": 0.21546666666666667,
        "step": 1616
    },
    {
        "loss": 1.5554,
        "grad_norm": 5.298835754394531,
        "learning_rate": 0.0001539717952776463,
        "epoch": 0.2156,
        "step": 1617
    },
    {
        "loss": 2.7489,
        "grad_norm": 2.5065758228302,
        "learning_rate": 0.00015391880154851613,
        "epoch": 0.21573333333333333,
        "step": 1618
    },
    {
        "loss": 2.7652,
        "grad_norm": 2.54190731048584,
        "learning_rate": 0.00015386578646196493,
        "epoch": 0.21586666666666668,
        "step": 1619
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.6317296028137207,
        "learning_rate": 0.00015381275003899217,
        "epoch": 0.216,
        "step": 1620
    },
    {
        "loss": 2.7103,
        "grad_norm": 2.485095977783203,
        "learning_rate": 0.00015375969230060576,
        "epoch": 0.21613333333333334,
        "step": 1621
    },
    {
        "loss": 2.3099,
        "grad_norm": 3.346562385559082,
        "learning_rate": 0.00015370661326782209,
        "epoch": 0.21626666666666666,
        "step": 1622
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.64662504196167,
        "learning_rate": 0.0001536535129616659,
        "epoch": 0.2164,
        "step": 1623
    },
    {
        "loss": 1.6032,
        "grad_norm": 2.970357894897461,
        "learning_rate": 0.00015360039140317034,
        "epoch": 0.21653333333333333,
        "step": 1624
    },
    {
        "loss": 2.8183,
        "grad_norm": 1.9886033535003662,
        "learning_rate": 0.00015354724861337714,
        "epoch": 0.21666666666666667,
        "step": 1625
    },
    {
        "loss": 1.929,
        "grad_norm": 3.634270668029785,
        "learning_rate": 0.0001534940846133363,
        "epoch": 0.2168,
        "step": 1626
    },
    {
        "loss": 2.4411,
        "grad_norm": 2.1632072925567627,
        "learning_rate": 0.0001534408994241063,
        "epoch": 0.21693333333333334,
        "step": 1627
    },
    {
        "loss": 3.3947,
        "grad_norm": 3.6176681518554688,
        "learning_rate": 0.0001533876930667539,
        "epoch": 0.21706666666666666,
        "step": 1628
    },
    {
        "loss": 1.211,
        "grad_norm": 3.0799198150634766,
        "learning_rate": 0.00015333446556235438,
        "epoch": 0.2172,
        "step": 1629
    },
    {
        "loss": 1.4114,
        "grad_norm": 3.1156270503997803,
        "learning_rate": 0.00015328121693199132,
        "epoch": 0.21733333333333332,
        "step": 1630
    },
    {
        "loss": 2.9351,
        "grad_norm": 3.7921438217163086,
        "learning_rate": 0.00015322794719675669,
        "epoch": 0.21746666666666667,
        "step": 1631
    },
    {
        "loss": 2.4882,
        "grad_norm": 2.179060935974121,
        "learning_rate": 0.0001531746563777508,
        "epoch": 0.2176,
        "step": 1632
    },
    {
        "loss": 2.0265,
        "grad_norm": 3.5289406776428223,
        "learning_rate": 0.0001531213444960823,
        "epoch": 0.21773333333333333,
        "step": 1633
    },
    {
        "loss": 2.2526,
        "grad_norm": 3.615811347961426,
        "learning_rate": 0.0001530680115728683,
        "epoch": 0.21786666666666665,
        "step": 1634
    },
    {
        "loss": 2.5249,
        "grad_norm": 2.9886600971221924,
        "learning_rate": 0.00015301465762923404,
        "epoch": 0.218,
        "step": 1635
    },
    {
        "loss": 2.0208,
        "grad_norm": 3.1910533905029297,
        "learning_rate": 0.00015296128268631328,
        "epoch": 0.21813333333333335,
        "step": 1636
    },
    {
        "loss": 2.5665,
        "grad_norm": 2.0589709281921387,
        "learning_rate": 0.00015290788676524793,
        "epoch": 0.21826666666666666,
        "step": 1637
    },
    {
        "loss": 2.6612,
        "grad_norm": 2.5056028366088867,
        "learning_rate": 0.00015285446988718841,
        "epoch": 0.2184,
        "step": 1638
    },
    {
        "loss": 1.2717,
        "grad_norm": 3.3314926624298096,
        "learning_rate": 0.00015280103207329328,
        "epoch": 0.21853333333333333,
        "step": 1639
    },
    {
        "loss": 2.8128,
        "grad_norm": 2.347259759902954,
        "learning_rate": 0.0001527475733447294,
        "epoch": 0.21866666666666668,
        "step": 1640
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.2700324058532715,
        "learning_rate": 0.000152694093722672,
        "epoch": 0.2188,
        "step": 1641
    },
    {
        "loss": 1.8401,
        "grad_norm": 3.3578481674194336,
        "learning_rate": 0.0001526405932283045,
        "epoch": 0.21893333333333334,
        "step": 1642
    },
    {
        "loss": 1.5605,
        "grad_norm": 3.665151596069336,
        "learning_rate": 0.0001525870718828187,
        "epoch": 0.21906666666666666,
        "step": 1643
    },
    {
        "loss": 2.5086,
        "grad_norm": 2.3939335346221924,
        "learning_rate": 0.0001525335297074145,
        "epoch": 0.2192,
        "step": 1644
    },
    {
        "loss": 2.7093,
        "grad_norm": 2.8425729274749756,
        "learning_rate": 0.0001524799667233002,
        "epoch": 0.21933333333333332,
        "step": 1645
    },
    {
        "loss": 2.5146,
        "grad_norm": 3.2357449531555176,
        "learning_rate": 0.00015242638295169224,
        "epoch": 0.21946666666666667,
        "step": 1646
    },
    {
        "loss": 2.2328,
        "grad_norm": 4.175716400146484,
        "learning_rate": 0.00015237277841381538,
        "epoch": 0.2196,
        "step": 1647
    },
    {
        "loss": 3.2685,
        "grad_norm": 2.5293161869049072,
        "learning_rate": 0.00015231915313090253,
        "epoch": 0.21973333333333334,
        "step": 1648
    },
    {
        "loss": 1.7416,
        "grad_norm": 3.511291265487671,
        "learning_rate": 0.00015226550712419483,
        "epoch": 0.21986666666666665,
        "step": 1649
    },
    {
        "loss": 2.2981,
        "grad_norm": 2.5139617919921875,
        "learning_rate": 0.0001522118404149417,
        "epoch": 0.22,
        "step": 1650
    },
    {
        "loss": 2.3131,
        "grad_norm": 7.9205322265625,
        "learning_rate": 0.00015215815302440063,
        "epoch": 0.22013333333333332,
        "step": 1651
    },
    {
        "loss": 2.87,
        "grad_norm": 2.6531035900115967,
        "learning_rate": 0.00015210444497383745,
        "epoch": 0.22026666666666667,
        "step": 1652
    },
    {
        "loss": 2.7103,
        "grad_norm": 4.111503601074219,
        "learning_rate": 0.0001520507162845261,
        "epoch": 0.2204,
        "step": 1653
    },
    {
        "loss": 2.4672,
        "grad_norm": 3.37809419631958,
        "learning_rate": 0.00015199696697774863,
        "epoch": 0.22053333333333333,
        "step": 1654
    },
    {
        "loss": 2.6137,
        "grad_norm": 3.055466413497925,
        "learning_rate": 0.00015194319707479537,
        "epoch": 0.22066666666666668,
        "step": 1655
    },
    {
        "loss": 1.145,
        "grad_norm": 4.805440902709961,
        "learning_rate": 0.00015188940659696475,
        "epoch": 0.2208,
        "step": 1656
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.164229393005371,
        "learning_rate": 0.0001518355955655634,
        "epoch": 0.22093333333333334,
        "step": 1657
    },
    {
        "loss": 2.5201,
        "grad_norm": 3.262834310531616,
        "learning_rate": 0.000151781764001906,
        "epoch": 0.22106666666666666,
        "step": 1658
    },
    {
        "loss": 2.2185,
        "grad_norm": 3.799971342086792,
        "learning_rate": 0.00015172791192731545,
        "epoch": 0.2212,
        "step": 1659
    },
    {
        "loss": 0.9979,
        "grad_norm": 6.233747482299805,
        "learning_rate": 0.00015167403936312273,
        "epoch": 0.22133333333333333,
        "step": 1660
    },
    {
        "loss": 3.3141,
        "grad_norm": 1.7813438177108765,
        "learning_rate": 0.00015162014633066692,
        "epoch": 0.22146666666666667,
        "step": 1661
    },
    {
        "loss": 1.3999,
        "grad_norm": 3.7212486267089844,
        "learning_rate": 0.00015156623285129527,
        "epoch": 0.2216,
        "step": 1662
    },
    {
        "loss": 2.6887,
        "grad_norm": 2.2033936977386475,
        "learning_rate": 0.00015151229894636305,
        "epoch": 0.22173333333333334,
        "step": 1663
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.8906168937683105,
        "learning_rate": 0.00015145834463723375,
        "epoch": 0.22186666666666666,
        "step": 1664
    },
    {
        "loss": 3.0304,
        "grad_norm": 2.6686513423919678,
        "learning_rate": 0.00015140436994527876,
        "epoch": 0.222,
        "step": 1665
    },
    {
        "loss": 2.8293,
        "grad_norm": 2.9290432929992676,
        "learning_rate": 0.00015135037489187768,
        "epoch": 0.22213333333333332,
        "step": 1666
    },
    {
        "loss": 2.4136,
        "grad_norm": 2.832926034927368,
        "learning_rate": 0.00015129635949841816,
        "epoch": 0.22226666666666667,
        "step": 1667
    },
    {
        "loss": 2.3169,
        "grad_norm": 3.206134080886841,
        "learning_rate": 0.00015124232378629584,
        "epoch": 0.2224,
        "step": 1668
    },
    {
        "loss": 3.1383,
        "grad_norm": 2.298231601715088,
        "learning_rate": 0.00015118826777691446,
        "epoch": 0.22253333333333333,
        "step": 1669
    },
    {
        "loss": 3.5189,
        "grad_norm": 6.480528831481934,
        "learning_rate": 0.00015113419149168577,
        "epoch": 0.22266666666666668,
        "step": 1670
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.7848875522613525,
        "learning_rate": 0.00015108009495202963,
        "epoch": 0.2228,
        "step": 1671
    },
    {
        "loss": 2.9233,
        "grad_norm": 3.0878732204437256,
        "learning_rate": 0.00015102597817937382,
        "epoch": 0.22293333333333334,
        "step": 1672
    },
    {
        "loss": 2.711,
        "grad_norm": 3.4858009815216064,
        "learning_rate": 0.0001509718411951542,
        "epoch": 0.22306666666666666,
        "step": 1673
    },
    {
        "loss": 2.7583,
        "grad_norm": 3.161344528198242,
        "learning_rate": 0.00015091768402081458,
        "epoch": 0.2232,
        "step": 1674
    },
    {
        "loss": 1.8471,
        "grad_norm": 2.8045225143432617,
        "learning_rate": 0.00015086350667780683,
        "epoch": 0.22333333333333333,
        "step": 1675
    },
    {
        "loss": 2.0384,
        "grad_norm": 3.3954062461853027,
        "learning_rate": 0.00015080930918759074,
        "epoch": 0.22346666666666667,
        "step": 1676
    },
    {
        "loss": 1.892,
        "grad_norm": 2.6732749938964844,
        "learning_rate": 0.0001507550915716342,
        "epoch": 0.2236,
        "step": 1677
    },
    {
        "loss": 2.0801,
        "grad_norm": 3.1794145107269287,
        "learning_rate": 0.00015070085385141292,
        "epoch": 0.22373333333333334,
        "step": 1678
    },
    {
        "loss": 2.664,
        "grad_norm": 2.073467254638672,
        "learning_rate": 0.00015064659604841068,
        "epoch": 0.22386666666666666,
        "step": 1679
    },
    {
        "loss": 1.4756,
        "grad_norm": 4.300663948059082,
        "learning_rate": 0.00015059231818411917,
        "epoch": 0.224,
        "step": 1680
    },
    {
        "loss": 2.4725,
        "grad_norm": 5.1040802001953125,
        "learning_rate": 0.00015053802028003804,
        "epoch": 0.22413333333333332,
        "step": 1681
    },
    {
        "loss": 1.9684,
        "grad_norm": 3.622201919555664,
        "learning_rate": 0.00015048370235767487,
        "epoch": 0.22426666666666667,
        "step": 1682
    },
    {
        "loss": 2.0334,
        "grad_norm": 3.0222890377044678,
        "learning_rate": 0.00015042936443854517,
        "epoch": 0.2244,
        "step": 1683
    },
    {
        "loss": 3.0015,
        "grad_norm": 3.0091371536254883,
        "learning_rate": 0.00015037500654417239,
        "epoch": 0.22453333333333333,
        "step": 1684
    },
    {
        "loss": 2.0408,
        "grad_norm": 4.614325523376465,
        "learning_rate": 0.00015032062869608788,
        "epoch": 0.22466666666666665,
        "step": 1685
    },
    {
        "loss": 2.5308,
        "grad_norm": 3.6976799964904785,
        "learning_rate": 0.00015026623091583086,
        "epoch": 0.2248,
        "step": 1686
    },
    {
        "loss": 2.5964,
        "grad_norm": 2.8339085578918457,
        "learning_rate": 0.00015021181322494848,
        "epoch": 0.22493333333333335,
        "step": 1687
    },
    {
        "loss": 2.75,
        "grad_norm": 5.4431586265563965,
        "learning_rate": 0.00015015737564499584,
        "epoch": 0.22506666666666666,
        "step": 1688
    },
    {
        "loss": 1.4638,
        "grad_norm": 4.1838531494140625,
        "learning_rate": 0.00015010291819753574,
        "epoch": 0.2252,
        "step": 1689
    },
    {
        "loss": 1.7234,
        "grad_norm": 3.8520474433898926,
        "learning_rate": 0.00015004844090413908,
        "epoch": 0.22533333333333333,
        "step": 1690
    },
    {
        "loss": 2.6231,
        "grad_norm": 4.477336406707764,
        "learning_rate": 0.00014999394378638442,
        "epoch": 0.22546666666666668,
        "step": 1691
    },
    {
        "loss": 1.6201,
        "grad_norm": 3.6457326412200928,
        "learning_rate": 0.00014993942686585826,
        "epoch": 0.2256,
        "step": 1692
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.888446092605591,
        "learning_rate": 0.00014988489016415496,
        "epoch": 0.22573333333333334,
        "step": 1693
    },
    {
        "loss": 3.0287,
        "grad_norm": 2.693270206451416,
        "learning_rate": 0.0001498303337028767,
        "epoch": 0.22586666666666666,
        "step": 1694
    },
    {
        "loss": 2.305,
        "grad_norm": 3.7088241577148438,
        "learning_rate": 0.00014977575750363346,
        "epoch": 0.226,
        "step": 1695
    },
    {
        "loss": 1.0617,
        "grad_norm": 3.257364273071289,
        "learning_rate": 0.0001497211615880431,
        "epoch": 0.22613333333333333,
        "step": 1696
    },
    {
        "loss": 2.0293,
        "grad_norm": 4.6511454582214355,
        "learning_rate": 0.00014966654597773114,
        "epoch": 0.22626666666666667,
        "step": 1697
    },
    {
        "loss": 2.3445,
        "grad_norm": 2.773542642593384,
        "learning_rate": 0.00014961191069433113,
        "epoch": 0.2264,
        "step": 1698
    },
    {
        "loss": 2.4807,
        "grad_norm": 3.0188980102539062,
        "learning_rate": 0.00014955725575948426,
        "epoch": 0.22653333333333334,
        "step": 1699
    },
    {
        "loss": 2.0281,
        "grad_norm": 4.3364057540893555,
        "learning_rate": 0.0001495025811948395,
        "epoch": 0.22666666666666666,
        "step": 1700
    },
    {
        "loss": 1.7633,
        "grad_norm": 3.6073105335235596,
        "learning_rate": 0.0001494478870220536,
        "epoch": 0.2268,
        "step": 1701
    },
    {
        "loss": 1.7745,
        "grad_norm": 3.908503770828247,
        "learning_rate": 0.00014939317326279126,
        "epoch": 0.22693333333333332,
        "step": 1702
    },
    {
        "loss": 1.5759,
        "grad_norm": 4.009030818939209,
        "learning_rate": 0.00014933843993872466,
        "epoch": 0.22706666666666667,
        "step": 1703
    },
    {
        "loss": 1.2958,
        "grad_norm": 4.071892738342285,
        "learning_rate": 0.00014928368707153384,
        "epoch": 0.2272,
        "step": 1704
    },
    {
        "loss": 2.9029,
        "grad_norm": 2.989454746246338,
        "learning_rate": 0.00014922891468290668,
        "epoch": 0.22733333333333333,
        "step": 1705
    },
    {
        "loss": 1.4819,
        "grad_norm": 4.202721118927002,
        "learning_rate": 0.00014917412279453863,
        "epoch": 0.22746666666666668,
        "step": 1706
    },
    {
        "loss": 2.6816,
        "grad_norm": 2.3217368125915527,
        "learning_rate": 0.00014911931142813303,
        "epoch": 0.2276,
        "step": 1707
    },
    {
        "loss": 1.8814,
        "grad_norm": 3.0985326766967773,
        "learning_rate": 0.00014906448060540077,
        "epoch": 0.22773333333333334,
        "step": 1708
    },
    {
        "loss": 2.4338,
        "grad_norm": 1.8164687156677246,
        "learning_rate": 0.00014900963034806058,
        "epoch": 0.22786666666666666,
        "step": 1709
    },
    {
        "loss": 1.9202,
        "grad_norm": 3.389051675796509,
        "learning_rate": 0.0001489547606778388,
        "epoch": 0.228,
        "step": 1710
    },
    {
        "loss": 2.6823,
        "grad_norm": 2.407954692840576,
        "learning_rate": 0.0001488998716164695,
        "epoch": 0.22813333333333333,
        "step": 1711
    },
    {
        "loss": 2.5801,
        "grad_norm": 3.796391248703003,
        "learning_rate": 0.00014884496318569445,
        "epoch": 0.22826666666666667,
        "step": 1712
    },
    {
        "loss": 2.8431,
        "grad_norm": 2.2456696033477783,
        "learning_rate": 0.00014879003540726305,
        "epoch": 0.2284,
        "step": 1713
    },
    {
        "loss": 2.3181,
        "grad_norm": 2.2452807426452637,
        "learning_rate": 0.00014873508830293237,
        "epoch": 0.22853333333333334,
        "step": 1714
    },
    {
        "loss": 2.7263,
        "grad_norm": 3.0271034240722656,
        "learning_rate": 0.00014868012189446717,
        "epoch": 0.22866666666666666,
        "step": 1715
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.17579984664917,
        "learning_rate": 0.00014862513620363978,
        "epoch": 0.2288,
        "step": 1716
    },
    {
        "loss": 2.4281,
        "grad_norm": 3.6333065032958984,
        "learning_rate": 0.00014857013125223024,
        "epoch": 0.22893333333333332,
        "step": 1717
    },
    {
        "loss": 2.8228,
        "grad_norm": 2.6323728561401367,
        "learning_rate": 0.00014851510706202626,
        "epoch": 0.22906666666666667,
        "step": 1718
    },
    {
        "loss": 1.0138,
        "grad_norm": 3.067953586578369,
        "learning_rate": 0.00014846006365482306,
        "epoch": 0.2292,
        "step": 1719
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.7560205459594727,
        "learning_rate": 0.0001484050010524235,
        "epoch": 0.22933333333333333,
        "step": 1720
    },
    {
        "loss": 1.5263,
        "grad_norm": 4.340099811553955,
        "learning_rate": 0.00014834991927663811,
        "epoch": 0.22946666666666668,
        "step": 1721
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.704216241836548,
        "learning_rate": 0.0001482948183492849,
        "epoch": 0.2296,
        "step": 1722
    },
    {
        "loss": 2.3151,
        "grad_norm": 3.368196964263916,
        "learning_rate": 0.00014823969829218963,
        "epoch": 0.22973333333333334,
        "step": 1723
    },
    {
        "loss": 0.9148,
        "grad_norm": 3.9150681495666504,
        "learning_rate": 0.00014818455912718549,
        "epoch": 0.22986666666666666,
        "step": 1724
    },
    {
        "loss": 2.2557,
        "grad_norm": 2.559523105621338,
        "learning_rate": 0.00014812940087611325,
        "epoch": 0.23,
        "step": 1725
    },
    {
        "loss": 2.607,
        "grad_norm": 2.9846134185791016,
        "learning_rate": 0.00014807422356082138,
        "epoch": 0.23013333333333333,
        "step": 1726
    },
    {
        "loss": 2.4028,
        "grad_norm": 3.142411947250366,
        "learning_rate": 0.00014801902720316568,
        "epoch": 0.23026666666666668,
        "step": 1727
    },
    {
        "loss": 1.0836,
        "grad_norm": 4.571589469909668,
        "learning_rate": 0.00014796381182500973,
        "epoch": 0.2304,
        "step": 1728
    },
    {
        "loss": 1.1677,
        "grad_norm": 4.653903961181641,
        "learning_rate": 0.0001479085774482245,
        "epoch": 0.23053333333333334,
        "step": 1729
    },
    {
        "loss": 2.636,
        "grad_norm": 3.2251760959625244,
        "learning_rate": 0.0001478533240946885,
        "epoch": 0.23066666666666666,
        "step": 1730
    },
    {
        "loss": 2.9052,
        "grad_norm": 3.706670045852661,
        "learning_rate": 0.00014779805178628772,
        "epoch": 0.2308,
        "step": 1731
    },
    {
        "loss": 1.8412,
        "grad_norm": 2.56854510307312,
        "learning_rate": 0.0001477427605449158,
        "epoch": 0.23093333333333332,
        "step": 1732
    },
    {
        "loss": 2.0253,
        "grad_norm": 3.5504422187805176,
        "learning_rate": 0.00014768745039247372,
        "epoch": 0.23106666666666667,
        "step": 1733
    },
    {
        "loss": 2.5966,
        "grad_norm": 4.377318382263184,
        "learning_rate": 0.00014763212135087008,
        "epoch": 0.2312,
        "step": 1734
    },
    {
        "loss": 2.4105,
        "grad_norm": 2.2479679584503174,
        "learning_rate": 0.00014757677344202086,
        "epoch": 0.23133333333333334,
        "step": 1735
    },
    {
        "loss": 2.0128,
        "grad_norm": 2.764960765838623,
        "learning_rate": 0.00014752140668784956,
        "epoch": 0.23146666666666665,
        "step": 1736
    },
    {
        "loss": 2.9133,
        "grad_norm": 3.9809329509735107,
        "learning_rate": 0.00014746602111028714,
        "epoch": 0.2316,
        "step": 1737
    },
    {
        "loss": 3.1574,
        "grad_norm": 5.041323184967041,
        "learning_rate": 0.000147410616731272,
        "epoch": 0.23173333333333335,
        "step": 1738
    },
    {
        "loss": 2.3567,
        "grad_norm": 3.2623884677886963,
        "learning_rate": 0.00014735519357275006,
        "epoch": 0.23186666666666667,
        "step": 1739
    },
    {
        "loss": 2.2196,
        "grad_norm": 2.7013473510742188,
        "learning_rate": 0.0001472997516566746,
        "epoch": 0.232,
        "step": 1740
    },
    {
        "loss": 2.5306,
        "grad_norm": 3.7700321674346924,
        "learning_rate": 0.00014724429100500635,
        "epoch": 0.23213333333333333,
        "step": 1741
    },
    {
        "loss": 2.1228,
        "grad_norm": 2.4205808639526367,
        "learning_rate": 0.0001471888116397134,
        "epoch": 0.23226666666666668,
        "step": 1742
    },
    {
        "loss": 2.5519,
        "grad_norm": 2.7886908054351807,
        "learning_rate": 0.00014713331358277138,
        "epoch": 0.2324,
        "step": 1743
    },
    {
        "loss": 1.7184,
        "grad_norm": 3.429253578186035,
        "learning_rate": 0.00014707779685616326,
        "epoch": 0.23253333333333334,
        "step": 1744
    },
    {
        "loss": 2.4287,
        "grad_norm": 3.423661947250366,
        "learning_rate": 0.00014702226148187938,
        "epoch": 0.23266666666666666,
        "step": 1745
    },
    {
        "loss": 2.2339,
        "grad_norm": 2.9226837158203125,
        "learning_rate": 0.0001469667074819175,
        "epoch": 0.2328,
        "step": 1746
    },
    {
        "loss": 1.8725,
        "grad_norm": 3.2705271244049072,
        "learning_rate": 0.00014691113487828278,
        "epoch": 0.23293333333333333,
        "step": 1747
    },
    {
        "loss": 2.7014,
        "grad_norm": 2.919713258743286,
        "learning_rate": 0.00014685554369298767,
        "epoch": 0.23306666666666667,
        "step": 1748
    },
    {
        "loss": 2.3817,
        "grad_norm": 3.1746230125427246,
        "learning_rate": 0.00014679993394805204,
        "epoch": 0.2332,
        "step": 1749
    },
    {
        "loss": 2.2605,
        "grad_norm": 1.9041460752487183,
        "learning_rate": 0.0001467443056655031,
        "epoch": 0.23333333333333334,
        "step": 1750
    },
    {
        "loss": 1.3333,
        "grad_norm": 4.838403701782227,
        "learning_rate": 0.00014668865886737547,
        "epoch": 0.23346666666666666,
        "step": 1751
    },
    {
        "loss": 2.8548,
        "grad_norm": 3.4267568588256836,
        "learning_rate": 0.0001466329935757109,
        "epoch": 0.2336,
        "step": 1752
    },
    {
        "loss": 1.7801,
        "grad_norm": 3.391932964324951,
        "learning_rate": 0.0001465773098125587,
        "epoch": 0.23373333333333332,
        "step": 1753
    },
    {
        "loss": 2.9674,
        "grad_norm": 2.0646886825561523,
        "learning_rate": 0.0001465216075999754,
        "epoch": 0.23386666666666667,
        "step": 1754
    },
    {
        "loss": 2.1051,
        "grad_norm": 3.20064115524292,
        "learning_rate": 0.0001464658869600248,
        "epoch": 0.234,
        "step": 1755
    },
    {
        "loss": 2.9269,
        "grad_norm": 2.507350206375122,
        "learning_rate": 0.000146410147914778,
        "epoch": 0.23413333333333333,
        "step": 1756
    },
    {
        "loss": 3.2396,
        "grad_norm": 2.9231834411621094,
        "learning_rate": 0.00014635439048631352,
        "epoch": 0.23426666666666668,
        "step": 1757
    },
    {
        "loss": 2.6604,
        "grad_norm": 3.713165044784546,
        "learning_rate": 0.00014629861469671703,
        "epoch": 0.2344,
        "step": 1758
    },
    {
        "loss": 1.2806,
        "grad_norm": 4.32274055480957,
        "learning_rate": 0.00014624282056808146,
        "epoch": 0.23453333333333334,
        "step": 1759
    },
    {
        "loss": 2.6757,
        "grad_norm": 2.5409512519836426,
        "learning_rate": 0.0001461870081225071,
        "epoch": 0.23466666666666666,
        "step": 1760
    },
    {
        "loss": 2.6464,
        "grad_norm": 2.7405896186828613,
        "learning_rate": 0.00014613117738210147,
        "epoch": 0.2348,
        "step": 1761
    },
    {
        "loss": 2.919,
        "grad_norm": 3.765873432159424,
        "learning_rate": 0.0001460753283689793,
        "epoch": 0.23493333333333333,
        "step": 1762
    },
    {
        "loss": 2.7493,
        "grad_norm": 2.237291097640991,
        "learning_rate": 0.00014601946110526253,
        "epoch": 0.23506666666666667,
        "step": 1763
    },
    {
        "loss": 2.3807,
        "grad_norm": 2.594881296157837,
        "learning_rate": 0.00014596357561308043,
        "epoch": 0.2352,
        "step": 1764
    },
    {
        "loss": 2.4842,
        "grad_norm": 2.5239081382751465,
        "learning_rate": 0.0001459076719145694,
        "epoch": 0.23533333333333334,
        "step": 1765
    },
    {
        "loss": 2.4425,
        "grad_norm": 3.287363052368164,
        "learning_rate": 0.00014585175003187307,
        "epoch": 0.23546666666666666,
        "step": 1766
    },
    {
        "loss": 2.2256,
        "grad_norm": 5.168591499328613,
        "learning_rate": 0.00014579580998714238,
        "epoch": 0.2356,
        "step": 1767
    },
    {
        "loss": 1.1725,
        "grad_norm": 3.027846097946167,
        "learning_rate": 0.00014573985180253525,
        "epoch": 0.23573333333333332,
        "step": 1768
    },
    {
        "loss": 2.9652,
        "grad_norm": 3.086916446685791,
        "learning_rate": 0.000145683875500217,
        "epoch": 0.23586666666666667,
        "step": 1769
    },
    {
        "loss": 2.0249,
        "grad_norm": 3.1151599884033203,
        "learning_rate": 0.00014562788110236,
        "epoch": 0.236,
        "step": 1770
    },
    {
        "loss": 2.2759,
        "grad_norm": 2.55362868309021,
        "learning_rate": 0.0001455718686311438,
        "epoch": 0.23613333333333333,
        "step": 1771
    },
    {
        "loss": 2.1994,
        "grad_norm": 2.983548879623413,
        "learning_rate": 0.0001455158381087552,
        "epoch": 0.23626666666666668,
        "step": 1772
    },
    {
        "loss": 2.62,
        "grad_norm": 2.7940008640289307,
        "learning_rate": 0.000145459789557388,
        "epoch": 0.2364,
        "step": 1773
    },
    {
        "loss": 2.992,
        "grad_norm": 2.4053919315338135,
        "learning_rate": 0.00014540372299924327,
        "epoch": 0.23653333333333335,
        "step": 1774
    },
    {
        "loss": 3.0925,
        "grad_norm": 5.1397929191589355,
        "learning_rate": 0.00014534763845652912,
        "epoch": 0.23666666666666666,
        "step": 1775
    },
    {
        "loss": 2.784,
        "grad_norm": 3.527531862258911,
        "learning_rate": 0.00014529153595146087,
        "epoch": 0.2368,
        "step": 1776
    },
    {
        "loss": 1.7661,
        "grad_norm": 3.4003918170928955,
        "learning_rate": 0.0001452354155062609,
        "epoch": 0.23693333333333333,
        "step": 1777
    },
    {
        "loss": 2.8807,
        "grad_norm": 3.2991228103637695,
        "learning_rate": 0.00014517927714315876,
        "epoch": 0.23706666666666668,
        "step": 1778
    },
    {
        "loss": 2.8285,
        "grad_norm": 3.7566540241241455,
        "learning_rate": 0.00014512312088439094,
        "epoch": 0.2372,
        "step": 1779
    },
    {
        "loss": 0.6751,
        "grad_norm": 3.8824195861816406,
        "learning_rate": 0.0001450669467522012,
        "epoch": 0.23733333333333334,
        "step": 1780
    },
    {
        "loss": 1.5639,
        "grad_norm": 3.093881845474243,
        "learning_rate": 0.0001450107547688403,
        "epoch": 0.23746666666666666,
        "step": 1781
    },
    {
        "loss": 2.1704,
        "grad_norm": 2.303942918777466,
        "learning_rate": 0.00014495454495656607,
        "epoch": 0.2376,
        "step": 1782
    },
    {
        "loss": 1.8518,
        "grad_norm": 3.128589630126953,
        "learning_rate": 0.0001448983173376434,
        "epoch": 0.23773333333333332,
        "step": 1783
    },
    {
        "loss": 2.6318,
        "grad_norm": 3.3390815258026123,
        "learning_rate": 0.00014484207193434426,
        "epoch": 0.23786666666666667,
        "step": 1784
    },
    {
        "loss": 2.6413,
        "grad_norm": 3.336301565170288,
        "learning_rate": 0.0001447858087689476,
        "epoch": 0.238,
        "step": 1785
    },
    {
        "loss": 1.4704,
        "grad_norm": 3.821659564971924,
        "learning_rate": 0.0001447295278637395,
        "epoch": 0.23813333333333334,
        "step": 1786
    },
    {
        "loss": 2.4956,
        "grad_norm": 4.285684585571289,
        "learning_rate": 0.00014467322924101297,
        "epoch": 0.23826666666666665,
        "step": 1787
    },
    {
        "loss": 1.0173,
        "grad_norm": 3.3198204040527344,
        "learning_rate": 0.00014461691292306817,
        "epoch": 0.2384,
        "step": 1788
    },
    {
        "loss": 2.5187,
        "grad_norm": 2.8278627395629883,
        "learning_rate": 0.0001445605789322121,
        "epoch": 0.23853333333333335,
        "step": 1789
    },
    {
        "loss": 2.3052,
        "grad_norm": 2.8528518676757812,
        "learning_rate": 0.00014450422729075888,
        "epoch": 0.23866666666666667,
        "step": 1790
    },
    {
        "loss": 1.9392,
        "grad_norm": 4.760814666748047,
        "learning_rate": 0.0001444478580210296,
        "epoch": 0.2388,
        "step": 1791
    },
    {
        "loss": 2.326,
        "grad_norm": 2.203842878341675,
        "learning_rate": 0.00014439147114535229,
        "epoch": 0.23893333333333333,
        "step": 1792
    },
    {
        "loss": 0.9614,
        "grad_norm": 2.9871749877929688,
        "learning_rate": 0.000144335066686062,
        "epoch": 0.23906666666666668,
        "step": 1793
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.540729284286499,
        "learning_rate": 0.0001442786446655007,
        "epoch": 0.2392,
        "step": 1794
    },
    {
        "loss": 2.5255,
        "grad_norm": 3.8523056507110596,
        "learning_rate": 0.00014422220510601746,
        "epoch": 0.23933333333333334,
        "step": 1795
    },
    {
        "loss": 2.3463,
        "grad_norm": 4.872057914733887,
        "learning_rate": 0.00014416574802996802,
        "epoch": 0.23946666666666666,
        "step": 1796
    },
    {
        "loss": 0.973,
        "grad_norm": 4.571199893951416,
        "learning_rate": 0.00014410927345971533,
        "epoch": 0.2396,
        "step": 1797
    },
    {
        "loss": 1.098,
        "grad_norm": 4.838171482086182,
        "learning_rate": 0.00014405278141762907,
        "epoch": 0.23973333333333333,
        "step": 1798
    },
    {
        "loss": 2.4098,
        "grad_norm": 4.447336196899414,
        "learning_rate": 0.00014399627192608603,
        "epoch": 0.23986666666666667,
        "step": 1799
    },
    {
        "loss": 2.7554,
        "grad_norm": 3.418370485305786,
        "learning_rate": 0.0001439397450074698,
        "epoch": 0.24,
        "step": 1800
    },
    {
        "loss": 2.7266,
        "grad_norm": 2.6790518760681152,
        "learning_rate": 0.00014388320068417078,
        "epoch": 0.24013333333333334,
        "step": 1801
    },
    {
        "loss": 0.7716,
        "grad_norm": 4.366538047790527,
        "learning_rate": 0.00014382663897858646,
        "epoch": 0.24026666666666666,
        "step": 1802
    },
    {
        "loss": 2.4466,
        "grad_norm": 4.323451995849609,
        "learning_rate": 0.00014377005991312114,
        "epoch": 0.2404,
        "step": 1803
    },
    {
        "loss": 2.7096,
        "grad_norm": 3.733699321746826,
        "learning_rate": 0.00014371346351018592,
        "epoch": 0.24053333333333332,
        "step": 1804
    },
    {
        "loss": 2.6179,
        "grad_norm": 2.648866891860962,
        "learning_rate": 0.00014365684979219886,
        "epoch": 0.24066666666666667,
        "step": 1805
    },
    {
        "loss": 2.7125,
        "grad_norm": 3.689765691757202,
        "learning_rate": 0.0001436002187815848,
        "epoch": 0.2408,
        "step": 1806
    },
    {
        "loss": 2.6134,
        "grad_norm": 1.954459547996521,
        "learning_rate": 0.00014354357050077555,
        "epoch": 0.24093333333333333,
        "step": 1807
    },
    {
        "loss": 1.9935,
        "grad_norm": 2.6875765323638916,
        "learning_rate": 0.00014348690497220968,
        "epoch": 0.24106666666666668,
        "step": 1808
    },
    {
        "loss": 2.2005,
        "grad_norm": 3.6068315505981445,
        "learning_rate": 0.00014343022221833252,
        "epoch": 0.2412,
        "step": 1809
    },
    {
        "loss": 1.8345,
        "grad_norm": 5.202248573303223,
        "learning_rate": 0.00014337352226159638,
        "epoch": 0.24133333333333334,
        "step": 1810
    },
    {
        "loss": 2.3885,
        "grad_norm": 2.78167986869812,
        "learning_rate": 0.00014331680512446031,
        "epoch": 0.24146666666666666,
        "step": 1811
    },
    {
        "loss": 3.2863,
        "grad_norm": 4.895087242126465,
        "learning_rate": 0.0001432600708293901,
        "epoch": 0.2416,
        "step": 1812
    },
    {
        "loss": 1.9064,
        "grad_norm": 4.74770975112915,
        "learning_rate": 0.0001432033193988584,
        "epoch": 0.24173333333333333,
        "step": 1813
    },
    {
        "loss": 2.1689,
        "grad_norm": 3.8782880306243896,
        "learning_rate": 0.00014314655085534477,
        "epoch": 0.24186666666666667,
        "step": 1814
    },
    {
        "loss": 2.4251,
        "grad_norm": 1.8287279605865479,
        "learning_rate": 0.00014308976522133524,
        "epoch": 0.242,
        "step": 1815
    },
    {
        "loss": 1.7922,
        "grad_norm": 1.310929775238037,
        "learning_rate": 0.0001430329625193229,
        "epoch": 0.24213333333333334,
        "step": 1816
    },
    {
        "loss": 2.1197,
        "grad_norm": 3.47953200340271,
        "learning_rate": 0.0001429761427718075,
        "epoch": 0.24226666666666666,
        "step": 1817
    },
    {
        "loss": 2.8154,
        "grad_norm": 2.8678860664367676,
        "learning_rate": 0.00014291930600129546,
        "epoch": 0.2424,
        "step": 1818
    },
    {
        "loss": 2.3328,
        "grad_norm": 3.4394445419311523,
        "learning_rate": 0.0001428624522303001,
        "epoch": 0.24253333333333332,
        "step": 1819
    },
    {
        "loss": 2.4118,
        "grad_norm": 2.3931431770324707,
        "learning_rate": 0.00014280558148134137,
        "epoch": 0.24266666666666667,
        "step": 1820
    },
    {
        "loss": 2.4555,
        "grad_norm": 14.556827545166016,
        "learning_rate": 0.00014274869377694596,
        "epoch": 0.2428,
        "step": 1821
    },
    {
        "loss": 2.1489,
        "grad_norm": 2.1112160682678223,
        "learning_rate": 0.00014269178913964726,
        "epoch": 0.24293333333333333,
        "step": 1822
    },
    {
        "loss": 2.3912,
        "grad_norm": 2.085383176803589,
        "learning_rate": 0.00014263486759198544,
        "epoch": 0.24306666666666665,
        "step": 1823
    },
    {
        "loss": 2.0659,
        "grad_norm": 3.775742292404175,
        "learning_rate": 0.00014257792915650728,
        "epoch": 0.2432,
        "step": 1824
    },
    {
        "loss": 2.6878,
        "grad_norm": 3.4486687183380127,
        "learning_rate": 0.00014252097385576633,
        "epoch": 0.24333333333333335,
        "step": 1825
    },
    {
        "loss": 2.5487,
        "grad_norm": 2.9809188842773438,
        "learning_rate": 0.00014246400171232266,
        "epoch": 0.24346666666666666,
        "step": 1826
    },
    {
        "loss": 2.3986,
        "grad_norm": 2.566331148147583,
        "learning_rate": 0.0001424070127487433,
        "epoch": 0.2436,
        "step": 1827
    },
    {
        "loss": 1.96,
        "grad_norm": 6.12174129486084,
        "learning_rate": 0.00014235000698760165,
        "epoch": 0.24373333333333333,
        "step": 1828
    },
    {
        "loss": 2.1046,
        "grad_norm": 3.7184548377990723,
        "learning_rate": 0.00014229298445147796,
        "epoch": 0.24386666666666668,
        "step": 1829
    },
    {
        "loss": 2.6419,
        "grad_norm": 3.7474048137664795,
        "learning_rate": 0.00014223594516295902,
        "epoch": 0.244,
        "step": 1830
    },
    {
        "loss": 2.0763,
        "grad_norm": 3.3113090991973877,
        "learning_rate": 0.00014217888914463828,
        "epoch": 0.24413333333333334,
        "step": 1831
    },
    {
        "loss": 1.0566,
        "grad_norm": 4.410479545593262,
        "learning_rate": 0.0001421218164191158,
        "epoch": 0.24426666666666666,
        "step": 1832
    },
    {
        "loss": 2.7371,
        "grad_norm": 3.681976079940796,
        "learning_rate": 0.00014206472700899835,
        "epoch": 0.2444,
        "step": 1833
    },
    {
        "loss": 2.7673,
        "grad_norm": 3.49562406539917,
        "learning_rate": 0.00014200762093689916,
        "epoch": 0.24453333333333332,
        "step": 1834
    },
    {
        "loss": 2.5785,
        "grad_norm": 2.361799716949463,
        "learning_rate": 0.0001419504982254382,
        "epoch": 0.24466666666666667,
        "step": 1835
    },
    {
        "loss": 2.9207,
        "grad_norm": 1.7848137617111206,
        "learning_rate": 0.00014189335889724193,
        "epoch": 0.2448,
        "step": 1836
    },
    {
        "loss": 2.2131,
        "grad_norm": 3.1893393993377686,
        "learning_rate": 0.0001418362029749435,
        "epoch": 0.24493333333333334,
        "step": 1837
    },
    {
        "loss": 1.2567,
        "grad_norm": 4.188897132873535,
        "learning_rate": 0.00014177903048118252,
        "epoch": 0.24506666666666665,
        "step": 1838
    },
    {
        "loss": 1.7194,
        "grad_norm": 3.801057815551758,
        "learning_rate": 0.00014172184143860513,
        "epoch": 0.2452,
        "step": 1839
    },
    {
        "loss": 2.3637,
        "grad_norm": 5.5761566162109375,
        "learning_rate": 0.00014166463586986423,
        "epoch": 0.24533333333333332,
        "step": 1840
    },
    {
        "loss": 2.8611,
        "grad_norm": 3.1525135040283203,
        "learning_rate": 0.00014160741379761914,
        "epoch": 0.24546666666666667,
        "step": 1841
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.9272797107696533,
        "learning_rate": 0.00014155017524453562,
        "epoch": 0.2456,
        "step": 1842
    },
    {
        "loss": 2.7541,
        "grad_norm": 2.7390122413635254,
        "learning_rate": 0.0001414929202332861,
        "epoch": 0.24573333333333333,
        "step": 1843
    },
    {
        "loss": 0.6673,
        "grad_norm": 2.9830918312072754,
        "learning_rate": 0.0001414356487865495,
        "epoch": 0.24586666666666668,
        "step": 1844
    },
    {
        "loss": 2.9387,
        "grad_norm": 2.983823537826538,
        "learning_rate": 0.00014137836092701123,
        "epoch": 0.246,
        "step": 1845
    },
    {
        "loss": 1.353,
        "grad_norm": 4.856639385223389,
        "learning_rate": 0.00014132105667736319,
        "epoch": 0.24613333333333334,
        "step": 1846
    },
    {
        "loss": 2.9313,
        "grad_norm": 2.8337032794952393,
        "learning_rate": 0.0001412637360603038,
        "epoch": 0.24626666666666666,
        "step": 1847
    },
    {
        "loss": 2.8653,
        "grad_norm": 3.5804500579833984,
        "learning_rate": 0.000141206399098538,
        "epoch": 0.2464,
        "step": 1848
    },
    {
        "loss": 2.6079,
        "grad_norm": 3.999342918395996,
        "learning_rate": 0.00014114904581477708,
        "epoch": 0.24653333333333333,
        "step": 1849
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.6692097187042236,
        "learning_rate": 0.0001410916762317389,
        "epoch": 0.24666666666666667,
        "step": 1850
    },
    {
        "loss": 1.7551,
        "grad_norm": 6.250932693481445,
        "learning_rate": 0.00014103429037214772,
        "epoch": 0.2468,
        "step": 1851
    },
    {
        "loss": 2.558,
        "grad_norm": 3.3780517578125,
        "learning_rate": 0.00014097688825873437,
        "epoch": 0.24693333333333334,
        "step": 1852
    },
    {
        "loss": 2.9339,
        "grad_norm": 2.5580763816833496,
        "learning_rate": 0.0001409194699142359,
        "epoch": 0.24706666666666666,
        "step": 1853
    },
    {
        "loss": 2.2094,
        "grad_norm": 3.8575961589813232,
        "learning_rate": 0.000140862035361396,
        "epoch": 0.2472,
        "step": 1854
    },
    {
        "loss": 2.5643,
        "grad_norm": 3.5316998958587646,
        "learning_rate": 0.00014080458462296464,
        "epoch": 0.24733333333333332,
        "step": 1855
    },
    {
        "loss": 1.374,
        "grad_norm": 4.781973361968994,
        "learning_rate": 0.0001407471177216983,
        "epoch": 0.24746666666666667,
        "step": 1856
    },
    {
        "loss": 1.6855,
        "grad_norm": 3.827010154724121,
        "learning_rate": 0.00014068963468035972,
        "epoch": 0.2476,
        "step": 1857
    },
    {
        "loss": 1.4631,
        "grad_norm": 3.2079110145568848,
        "learning_rate": 0.00014063213552171827,
        "epoch": 0.24773333333333333,
        "step": 1858
    },
    {
        "loss": 1.8881,
        "grad_norm": 3.3264167308807373,
        "learning_rate": 0.00014057462026854944,
        "epoch": 0.24786666666666668,
        "step": 1859
    },
    {
        "loss": 1.2936,
        "grad_norm": 5.105120658874512,
        "learning_rate": 0.00014051708894363528,
        "epoch": 0.248,
        "step": 1860
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.6300649642944336,
        "learning_rate": 0.00014045954156976414,
        "epoch": 0.24813333333333334,
        "step": 1861
    },
    {
        "loss": 2.7928,
        "grad_norm": 2.746615409851074,
        "learning_rate": 0.00014040197816973072,
        "epoch": 0.24826666666666666,
        "step": 1862
    },
    {
        "loss": 2.7316,
        "grad_norm": 4.082655429840088,
        "learning_rate": 0.00014034439876633612,
        "epoch": 0.2484,
        "step": 1863
    },
    {
        "loss": 2.1045,
        "grad_norm": 4.474847316741943,
        "learning_rate": 0.00014028680338238765,
        "epoch": 0.24853333333333333,
        "step": 1864
    },
    {
        "loss": 2.192,
        "grad_norm": 3.904876470565796,
        "learning_rate": 0.0001402291920406991,
        "epoch": 0.24866666666666667,
        "step": 1865
    },
    {
        "loss": 3.0319,
        "grad_norm": 2.2537543773651123,
        "learning_rate": 0.00014017156476409047,
        "epoch": 0.2488,
        "step": 1866
    },
    {
        "loss": 2.5794,
        "grad_norm": 3.949256181716919,
        "learning_rate": 0.0001401139215753882,
        "epoch": 0.24893333333333334,
        "step": 1867
    },
    {
        "loss": 2.3623,
        "grad_norm": 2.73421573638916,
        "learning_rate": 0.00014005626249742487,
        "epoch": 0.24906666666666666,
        "step": 1868
    },
    {
        "loss": 2.3258,
        "grad_norm": 2.579695701599121,
        "learning_rate": 0.00013999858755303952,
        "epoch": 0.2492,
        "step": 1869
    },
    {
        "loss": 1.1812,
        "grad_norm": 3.505493640899658,
        "learning_rate": 0.0001399408967650773,
        "epoch": 0.24933333333333332,
        "step": 1870
    },
    {
        "loss": 1.9407,
        "grad_norm": 3.7572860717773438,
        "learning_rate": 0.00013988319015638984,
        "epoch": 0.24946666666666667,
        "step": 1871
    },
    {
        "loss": 1.9428,
        "grad_norm": 3.404695987701416,
        "learning_rate": 0.00013982546774983482,
        "epoch": 0.2496,
        "step": 1872
    },
    {
        "loss": 1.4776,
        "grad_norm": 5.381771564483643,
        "learning_rate": 0.00013976772956827632,
        "epoch": 0.24973333333333333,
        "step": 1873
    },
    {
        "loss": 1.1275,
        "grad_norm": 4.235881328582764,
        "learning_rate": 0.00013970997563458466,
        "epoch": 0.24986666666666665,
        "step": 1874
    },
    {
        "loss": 2.4136,
        "grad_norm": 3.8883848190307617,
        "learning_rate": 0.0001396522059716363,
        "epoch": 0.25,
        "step": 1875
    },
    {
        "loss": 2.9795,
        "grad_norm": 2.3322019577026367,
        "learning_rate": 0.00013959442060231406,
        "epoch": 0.2501333333333333,
        "step": 1876
    },
    {
        "loss": 2.3753,
        "grad_norm": 3.200104236602783,
        "learning_rate": 0.0001395366195495069,
        "epoch": 0.2502666666666667,
        "step": 1877
    },
    {
        "loss": 2.0753,
        "grad_norm": 6.151453971862793,
        "learning_rate": 0.00013947880283611005,
        "epoch": 0.2504,
        "step": 1878
    },
    {
        "loss": 2.8803,
        "grad_norm": 2.357327699661255,
        "learning_rate": 0.00013942097048502488,
        "epoch": 0.25053333333333333,
        "step": 1879
    },
    {
        "loss": 1.4581,
        "grad_norm": 3.6905336380004883,
        "learning_rate": 0.00013936312251915895,
        "epoch": 0.25066666666666665,
        "step": 1880
    },
    {
        "loss": 1.5559,
        "grad_norm": 3.4944067001342773,
        "learning_rate": 0.00013930525896142606,
        "epoch": 0.2508,
        "step": 1881
    },
    {
        "loss": 2.108,
        "grad_norm": 2.6605591773986816,
        "learning_rate": 0.00013924737983474618,
        "epoch": 0.25093333333333334,
        "step": 1882
    },
    {
        "loss": 2.0743,
        "grad_norm": 2.2345845699310303,
        "learning_rate": 0.0001391894851620454,
        "epoch": 0.25106666666666666,
        "step": 1883
    },
    {
        "loss": 2.6288,
        "grad_norm": 1.9106420278549194,
        "learning_rate": 0.00013913157496625602,
        "epoch": 0.2512,
        "step": 1884
    },
    {
        "loss": 1.771,
        "grad_norm": 4.521265506744385,
        "learning_rate": 0.00013907364927031647,
        "epoch": 0.25133333333333335,
        "step": 1885
    },
    {
        "loss": 1.1602,
        "grad_norm": 3.8544435501098633,
        "learning_rate": 0.0001390157080971713,
        "epoch": 0.25146666666666667,
        "step": 1886
    },
    {
        "loss": 2.4885,
        "grad_norm": 3.599043607711792,
        "learning_rate": 0.0001389577514697712,
        "epoch": 0.2516,
        "step": 1887
    },
    {
        "loss": 2.185,
        "grad_norm": 1.7211025953292847,
        "learning_rate": 0.000138899779411073,
        "epoch": 0.2517333333333333,
        "step": 1888
    },
    {
        "loss": 2.6857,
        "grad_norm": 2.528247833251953,
        "learning_rate": 0.00013884179194403962,
        "epoch": 0.2518666666666667,
        "step": 1889
    },
    {
        "loss": 2.2062,
        "grad_norm": 4.172942638397217,
        "learning_rate": 0.0001387837890916401,
        "epoch": 0.252,
        "step": 1890
    },
    {
        "loss": 1.9063,
        "grad_norm": 4.075272560119629,
        "learning_rate": 0.0001387257708768496,
        "epoch": 0.2521333333333333,
        "step": 1891
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.3637382984161377,
        "learning_rate": 0.00013866773732264926,
        "epoch": 0.25226666666666664,
        "step": 1892
    },
    {
        "loss": 2.5112,
        "grad_norm": 3.584505558013916,
        "learning_rate": 0.00013860968845202644,
        "epoch": 0.2524,
        "step": 1893
    },
    {
        "loss": 1.8128,
        "grad_norm": 3.2436435222625732,
        "learning_rate": 0.00013855162428797445,
        "epoch": 0.25253333333333333,
        "step": 1894
    },
    {
        "loss": 2.0364,
        "grad_norm": 2.626721143722534,
        "learning_rate": 0.0001384935448534927,
        "epoch": 0.25266666666666665,
        "step": 1895
    },
    {
        "loss": 2.9004,
        "grad_norm": 3.1551036834716797,
        "learning_rate": 0.00013843545017158673,
        "epoch": 0.2528,
        "step": 1896
    },
    {
        "loss": 0.8628,
        "grad_norm": 3.341245651245117,
        "learning_rate": 0.00013837734026526791,
        "epoch": 0.25293333333333334,
        "step": 1897
    },
    {
        "loss": 2.7795,
        "grad_norm": 2.3003950119018555,
        "learning_rate": 0.00013831921515755388,
        "epoch": 0.25306666666666666,
        "step": 1898
    },
    {
        "loss": 2.0239,
        "grad_norm": 2.9148502349853516,
        "learning_rate": 0.00013826107487146817,
        "epoch": 0.2532,
        "step": 1899
    },
    {
        "loss": 3.1263,
        "grad_norm": 5.109363555908203,
        "learning_rate": 0.0001382029194300403,
        "epoch": 0.25333333333333335,
        "step": 1900
    },
    {
        "loss": 2.6776,
        "grad_norm": 3.2677407264709473,
        "learning_rate": 0.00013814474885630592,
        "epoch": 0.2534666666666667,
        "step": 1901
    },
    {
        "loss": 2.2726,
        "grad_norm": 3.5898702144622803,
        "learning_rate": 0.00013808656317330646,
        "epoch": 0.2536,
        "step": 1902
    },
    {
        "loss": 2.5541,
        "grad_norm": 5.1468095779418945,
        "learning_rate": 0.0001380283624040896,
        "epoch": 0.2537333333333333,
        "step": 1903
    },
    {
        "loss": 3.1771,
        "grad_norm": 7.523570537567139,
        "learning_rate": 0.00013797014657170883,
        "epoch": 0.2538666666666667,
        "step": 1904
    },
    {
        "loss": 2.1751,
        "grad_norm": 3.065951108932495,
        "learning_rate": 0.00013791191569922362,
        "epoch": 0.254,
        "step": 1905
    },
    {
        "loss": 2.4549,
        "grad_norm": 3.9689719676971436,
        "learning_rate": 0.0001378536698096994,
        "epoch": 0.2541333333333333,
        "step": 1906
    },
    {
        "loss": 3.039,
        "grad_norm": 3.6282005310058594,
        "learning_rate": 0.00013779540892620756,
        "epoch": 0.25426666666666664,
        "step": 1907
    },
    {
        "loss": 2.1349,
        "grad_norm": 3.1800098419189453,
        "learning_rate": 0.0001377371330718255,
        "epoch": 0.2544,
        "step": 1908
    },
    {
        "loss": 2.3048,
        "grad_norm": 2.470980405807495,
        "learning_rate": 0.00013767884226963643,
        "epoch": 0.25453333333333333,
        "step": 1909
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.8335883617401123,
        "learning_rate": 0.00013762053654272957,
        "epoch": 0.25466666666666665,
        "step": 1910
    },
    {
        "loss": 2.4579,
        "grad_norm": 3.561859369277954,
        "learning_rate": 0.00013756221591419997,
        "epoch": 0.2548,
        "step": 1911
    },
    {
        "loss": 2.3973,
        "grad_norm": 2.3148465156555176,
        "learning_rate": 0.00013750388040714863,
        "epoch": 0.25493333333333335,
        "step": 1912
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.844320774078369,
        "learning_rate": 0.0001374455300446825,
        "epoch": 0.25506666666666666,
        "step": 1913
    },
    {
        "loss": 2.0579,
        "grad_norm": 4.554422855377197,
        "learning_rate": 0.00013738716484991436,
        "epoch": 0.2552,
        "step": 1914
    },
    {
        "loss": 0.6515,
        "grad_norm": 2.839470624923706,
        "learning_rate": 0.00013732878484596278,
        "epoch": 0.25533333333333336,
        "step": 1915
    },
    {
        "loss": 1.7378,
        "grad_norm": 5.883617401123047,
        "learning_rate": 0.00013727039005595235,
        "epoch": 0.2554666666666667,
        "step": 1916
    },
    {
        "loss": 1.8529,
        "grad_norm": 8.379538536071777,
        "learning_rate": 0.00013721198050301346,
        "epoch": 0.2556,
        "step": 1917
    },
    {
        "loss": 2.4004,
        "grad_norm": 1.8963574171066284,
        "learning_rate": 0.00013715355621028225,
        "epoch": 0.2557333333333333,
        "step": 1918
    },
    {
        "loss": 2.315,
        "grad_norm": 2.4073071479797363,
        "learning_rate": 0.00013709511720090081,
        "epoch": 0.2558666666666667,
        "step": 1919
    },
    {
        "loss": 1.9146,
        "grad_norm": 2.1793928146362305,
        "learning_rate": 0.0001370366634980171,
        "epoch": 0.256,
        "step": 1920
    },
    {
        "loss": 2.0445,
        "grad_norm": 3.0164129734039307,
        "learning_rate": 0.00013697819512478474,
        "epoch": 0.2561333333333333,
        "step": 1921
    },
    {
        "loss": 3.2697,
        "grad_norm": 3.6283841133117676,
        "learning_rate": 0.00013691971210436332,
        "epoch": 0.25626666666666664,
        "step": 1922
    },
    {
        "loss": 1.1137,
        "grad_norm": 3.1090381145477295,
        "learning_rate": 0.00013686121445991812,
        "epoch": 0.2564,
        "step": 1923
    },
    {
        "loss": 2.4362,
        "grad_norm": 2.271550416946411,
        "learning_rate": 0.00013680270221462024,
        "epoch": 0.25653333333333334,
        "step": 1924
    },
    {
        "loss": 1.549,
        "grad_norm": 3.903505802154541,
        "learning_rate": 0.00013674417539164666,
        "epoch": 0.25666666666666665,
        "step": 1925
    },
    {
        "loss": 2.2088,
        "grad_norm": 3.3485403060913086,
        "learning_rate": 0.00013668563401418,
        "epoch": 0.2568,
        "step": 1926
    },
    {
        "loss": 0.9088,
        "grad_norm": 4.735055446624756,
        "learning_rate": 0.00013662707810540867,
        "epoch": 0.25693333333333335,
        "step": 1927
    },
    {
        "loss": 2.3757,
        "grad_norm": 2.53483247756958,
        "learning_rate": 0.0001365685076885269,
        "epoch": 0.25706666666666667,
        "step": 1928
    },
    {
        "loss": 2.5573,
        "grad_norm": 2.0560858249664307,
        "learning_rate": 0.00013650992278673466,
        "epoch": 0.2572,
        "step": 1929
    },
    {
        "loss": 3.0122,
        "grad_norm": 2.019508123397827,
        "learning_rate": 0.0001364513234232376,
        "epoch": 0.25733333333333336,
        "step": 1930
    },
    {
        "loss": 3.4475,
        "grad_norm": 4.602937698364258,
        "learning_rate": 0.0001363927096212471,
        "epoch": 0.2574666666666667,
        "step": 1931
    },
    {
        "loss": 2.5833,
        "grad_norm": 2.385150194168091,
        "learning_rate": 0.00013633408140398032,
        "epoch": 0.2576,
        "step": 1932
    },
    {
        "loss": 2.0598,
        "grad_norm": 2.9768242835998535,
        "learning_rate": 0.0001362754387946601,
        "epoch": 0.2577333333333333,
        "step": 1933
    },
    {
        "loss": 2.5267,
        "grad_norm": 3.994138240814209,
        "learning_rate": 0.00013621678181651497,
        "epoch": 0.2578666666666667,
        "step": 1934
    },
    {
        "loss": 2.6723,
        "grad_norm": 2.4099133014678955,
        "learning_rate": 0.00013615811049277914,
        "epoch": 0.258,
        "step": 1935
    },
    {
        "loss": 2.0065,
        "grad_norm": 3.929983377456665,
        "learning_rate": 0.00013609942484669256,
        "epoch": 0.2581333333333333,
        "step": 1936
    },
    {
        "loss": 2.0831,
        "grad_norm": 2.945081949234009,
        "learning_rate": 0.00013604072490150078,
        "epoch": 0.25826666666666664,
        "step": 1937
    },
    {
        "loss": 2.0619,
        "grad_norm": 2.131272077560425,
        "learning_rate": 0.00013598201068045507,
        "epoch": 0.2584,
        "step": 1938
    },
    {
        "loss": 2.5231,
        "grad_norm": 4.849946975708008,
        "learning_rate": 0.00013592328220681234,
        "epoch": 0.25853333333333334,
        "step": 1939
    },
    {
        "loss": 1.7444,
        "grad_norm": 3.9485065937042236,
        "learning_rate": 0.00013586453950383505,
        "epoch": 0.25866666666666666,
        "step": 1940
    },
    {
        "loss": 1.9491,
        "grad_norm": 3.6892967224121094,
        "learning_rate": 0.0001358057825947915,
        "epoch": 0.2588,
        "step": 1941
    },
    {
        "loss": 3.113,
        "grad_norm": 3.774405002593994,
        "learning_rate": 0.0001357470115029555,
        "epoch": 0.25893333333333335,
        "step": 1942
    },
    {
        "loss": 2.8172,
        "grad_norm": 2.420348644256592,
        "learning_rate": 0.00013568822625160642,
        "epoch": 0.25906666666666667,
        "step": 1943
    },
    {
        "loss": 0.9624,
        "grad_norm": 3.683023452758789,
        "learning_rate": 0.0001356294268640293,
        "epoch": 0.2592,
        "step": 1944
    },
    {
        "loss": 2.4727,
        "grad_norm": 2.607818126678467,
        "learning_rate": 0.00013557061336351477,
        "epoch": 0.25933333333333336,
        "step": 1945
    },
    {
        "loss": 2.6383,
        "grad_norm": 3.2479350566864014,
        "learning_rate": 0.0001355117857733591,
        "epoch": 0.2594666666666667,
        "step": 1946
    },
    {
        "loss": 1.8948,
        "grad_norm": 3.77758526802063,
        "learning_rate": 0.0001354529441168641,
        "epoch": 0.2596,
        "step": 1947
    },
    {
        "loss": 1.4862,
        "grad_norm": 3.330364227294922,
        "learning_rate": 0.00013539408841733717,
        "epoch": 0.2597333333333333,
        "step": 1948
    },
    {
        "loss": 2.9221,
        "grad_norm": 2.269218921661377,
        "learning_rate": 0.00013533521869809115,
        "epoch": 0.2598666666666667,
        "step": 1949
    },
    {
        "loss": 1.9171,
        "grad_norm": 2.898500680923462,
        "learning_rate": 0.0001352763349824446,
        "epoch": 0.26,
        "step": 1950
    },
    {
        "loss": 1.6857,
        "grad_norm": 3.4186010360717773,
        "learning_rate": 0.00013521743729372164,
        "epoch": 0.2601333333333333,
        "step": 1951
    },
    {
        "loss": 2.6573,
        "grad_norm": 2.6562068462371826,
        "learning_rate": 0.00013515852565525166,
        "epoch": 0.26026666666666665,
        "step": 1952
    },
    {
        "loss": 3.0061,
        "grad_norm": 2.929765224456787,
        "learning_rate": 0.00013509960009036993,
        "epoch": 0.2604,
        "step": 1953
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.4965367317199707,
        "learning_rate": 0.00013504066062241697,
        "epoch": 0.26053333333333334,
        "step": 1954
    },
    {
        "loss": 2.7131,
        "grad_norm": 3.1522085666656494,
        "learning_rate": 0.00013498170727473897,
        "epoch": 0.26066666666666666,
        "step": 1955
    },
    {
        "loss": 2.288,
        "grad_norm": 2.7900784015655518,
        "learning_rate": 0.00013492274007068753,
        "epoch": 0.2608,
        "step": 1956
    },
    {
        "loss": 1.9463,
        "grad_norm": 3.4109485149383545,
        "learning_rate": 0.00013486375903361972,
        "epoch": 0.26093333333333335,
        "step": 1957
    },
    {
        "loss": 2.518,
        "grad_norm": 4.291278839111328,
        "learning_rate": 0.00013480476418689817,
        "epoch": 0.26106666666666667,
        "step": 1958
    },
    {
        "loss": 1.2796,
        "grad_norm": 4.446793079376221,
        "learning_rate": 0.00013474575555389096,
        "epoch": 0.2612,
        "step": 1959
    },
    {
        "loss": 2.5429,
        "grad_norm": 2.848409414291382,
        "learning_rate": 0.00013468673315797158,
        "epoch": 0.2613333333333333,
        "step": 1960
    },
    {
        "loss": 2.4902,
        "grad_norm": 4.65331506729126,
        "learning_rate": 0.00013462769702251898,
        "epoch": 0.2614666666666667,
        "step": 1961
    },
    {
        "loss": 2.5718,
        "grad_norm": 3.126622438430786,
        "learning_rate": 0.0001345686471709176,
        "epoch": 0.2616,
        "step": 1962
    },
    {
        "loss": 2.8486,
        "grad_norm": 2.220004081726074,
        "learning_rate": 0.0001345095836265573,
        "epoch": 0.2617333333333333,
        "step": 1963
    },
    {
        "loss": 1.6302,
        "grad_norm": 3.846109628677368,
        "learning_rate": 0.00013445050641283331,
        "epoch": 0.2618666666666667,
        "step": 1964
    },
    {
        "loss": 2.421,
        "grad_norm": 3.3090603351593018,
        "learning_rate": 0.00013439141555314637,
        "epoch": 0.262,
        "step": 1965
    },
    {
        "loss": 2.8986,
        "grad_norm": 4.149338722229004,
        "learning_rate": 0.00013433231107090254,
        "epoch": 0.26213333333333333,
        "step": 1966
    },
    {
        "loss": 3.071,
        "grad_norm": 2.5488181114196777,
        "learning_rate": 0.0001342731929895133,
        "epoch": 0.26226666666666665,
        "step": 1967
    },
    {
        "loss": 1.9199,
        "grad_norm": 3.1907286643981934,
        "learning_rate": 0.00013421406133239553,
        "epoch": 0.2624,
        "step": 1968
    },
    {
        "loss": 2.6556,
        "grad_norm": 2.1028103828430176,
        "learning_rate": 0.00013415491612297147,
        "epoch": 0.26253333333333334,
        "step": 1969
    },
    {
        "loss": 2.1208,
        "grad_norm": 3.96437668800354,
        "learning_rate": 0.0001340957573846688,
        "epoch": 0.26266666666666666,
        "step": 1970
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.4547789096832275,
        "learning_rate": 0.00013403658514092045,
        "epoch": 0.2628,
        "step": 1971
    },
    {
        "loss": 2.3546,
        "grad_norm": 3.3642890453338623,
        "learning_rate": 0.0001339773994151647,
        "epoch": 0.26293333333333335,
        "step": 1972
    },
    {
        "loss": 2.9693,
        "grad_norm": 3.2580084800720215,
        "learning_rate": 0.00013391820023084527,
        "epoch": 0.26306666666666667,
        "step": 1973
    },
    {
        "loss": 2.8393,
        "grad_norm": 2.5502185821533203,
        "learning_rate": 0.00013385898761141118,
        "epoch": 0.2632,
        "step": 1974
    },
    {
        "loss": 2.3577,
        "grad_norm": 1.892524003982544,
        "learning_rate": 0.00013379976158031673,
        "epoch": 0.2633333333333333,
        "step": 1975
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.0680294036865234,
        "learning_rate": 0.00013374052216102154,
        "epoch": 0.2634666666666667,
        "step": 1976
    },
    {
        "loss": 1.9437,
        "grad_norm": 3.081296682357788,
        "learning_rate": 0.00013368126937699055,
        "epoch": 0.2636,
        "step": 1977
    },
    {
        "loss": 2.5461,
        "grad_norm": 3.5198569297790527,
        "learning_rate": 0.00013362200325169401,
        "epoch": 0.2637333333333333,
        "step": 1978
    },
    {
        "loss": 1.8556,
        "grad_norm": 3.5965819358825684,
        "learning_rate": 0.0001335627238086074,
        "epoch": 0.2638666666666667,
        "step": 1979
    },
    {
        "loss": 2.7484,
        "grad_norm": 4.014686584472656,
        "learning_rate": 0.00013350343107121156,
        "epoch": 0.264,
        "step": 1980
    },
    {
        "loss": 3.0141,
        "grad_norm": 1.5008978843688965,
        "learning_rate": 0.0001334441250629925,
        "epoch": 0.26413333333333333,
        "step": 1981
    },
    {
        "loss": 2.0294,
        "grad_norm": 3.5888073444366455,
        "learning_rate": 0.00013338480580744148,
        "epoch": 0.26426666666666665,
        "step": 1982
    },
    {
        "loss": 1.9656,
        "grad_norm": 3.5280826091766357,
        "learning_rate": 0.0001333254733280552,
        "epoch": 0.2644,
        "step": 1983
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.7325199842453003,
        "learning_rate": 0.00013326612764833534,
        "epoch": 0.26453333333333334,
        "step": 1984
    },
    {
        "loss": 2.7282,
        "grad_norm": 2.4780781269073486,
        "learning_rate": 0.000133206768791789,
        "epoch": 0.26466666666666666,
        "step": 1985
    },
    {
        "loss": 2.2577,
        "grad_norm": 1.912416934967041,
        "learning_rate": 0.00013314739678192838,
        "epoch": 0.2648,
        "step": 1986
    },
    {
        "loss": 2.4521,
        "grad_norm": 5.3911309242248535,
        "learning_rate": 0.00013308801164227093,
        "epoch": 0.26493333333333335,
        "step": 1987
    },
    {
        "loss": 2.3215,
        "grad_norm": 3.2745466232299805,
        "learning_rate": 0.00013302861339633935,
        "epoch": 0.2650666666666667,
        "step": 1988
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.594188690185547,
        "learning_rate": 0.00013296920206766147,
        "epoch": 0.2652,
        "step": 1989
    },
    {
        "loss": 2.8532,
        "grad_norm": 2.5677754878997803,
        "learning_rate": 0.00013290977767977026,
        "epoch": 0.2653333333333333,
        "step": 1990
    },
    {
        "loss": 2.8207,
        "grad_norm": 2.9413886070251465,
        "learning_rate": 0.000132850340256204,
        "epoch": 0.2654666666666667,
        "step": 1991
    },
    {
        "loss": 1.512,
        "grad_norm": 4.424169540405273,
        "learning_rate": 0.00013279088982050605,
        "epoch": 0.2656,
        "step": 1992
    },
    {
        "loss": 2.6514,
        "grad_norm": 2.1845083236694336,
        "learning_rate": 0.00013273142639622486,
        "epoch": 0.2657333333333333,
        "step": 1993
    },
    {
        "loss": 2.518,
        "grad_norm": 2.483555555343628,
        "learning_rate": 0.0001326719500069142,
        "epoch": 0.26586666666666664,
        "step": 1994
    },
    {
        "loss": 2.4708,
        "grad_norm": 2.8125534057617188,
        "learning_rate": 0.00013261246067613276,
        "epoch": 0.266,
        "step": 1995
    },
    {
        "loss": 2.7754,
        "grad_norm": 3.864309787750244,
        "learning_rate": 0.0001325529584274446,
        "epoch": 0.26613333333333333,
        "step": 1996
    },
    {
        "loss": 1.4712,
        "grad_norm": 2.779252529144287,
        "learning_rate": 0.0001324934432844186,
        "epoch": 0.26626666666666665,
        "step": 1997
    },
    {
        "loss": 2.0731,
        "grad_norm": 3.7119548320770264,
        "learning_rate": 0.00013243391527062906,
        "epoch": 0.2664,
        "step": 1998
    },
    {
        "loss": 2.9953,
        "grad_norm": 3.3319852352142334,
        "learning_rate": 0.00013237437440965515,
        "epoch": 0.26653333333333334,
        "step": 1999
    },
    {
        "loss": 2.458,
        "grad_norm": 3.7424516677856445,
        "learning_rate": 0.00013231482072508118,
        "epoch": 0.26666666666666666,
        "step": 2000
    },
    {
        "loss": 2.7232,
        "grad_norm": 2.9571402072906494,
        "learning_rate": 0.00013225525424049664,
        "epoch": 0.2668,
        "step": 2001
    },
    {
        "loss": 2.8205,
        "grad_norm": 2.484302043914795,
        "learning_rate": 0.00013219567497949603,
        "epoch": 0.26693333333333336,
        "step": 2002
    },
    {
        "loss": 2.427,
        "grad_norm": 2.6011314392089844,
        "learning_rate": 0.0001321360829656788,
        "epoch": 0.2670666666666667,
        "step": 2003
    },
    {
        "loss": 2.4572,
        "grad_norm": 3.2075045108795166,
        "learning_rate": 0.00013207647822264962,
        "epoch": 0.2672,
        "step": 2004
    },
    {
        "loss": 0.7287,
        "grad_norm": 2.7186357975006104,
        "learning_rate": 0.00013201686077401813,
        "epoch": 0.2673333333333333,
        "step": 2005
    },
    {
        "loss": 1.8456,
        "grad_norm": 3.9988393783569336,
        "learning_rate": 0.000131957230643399,
        "epoch": 0.2674666666666667,
        "step": 2006
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.293224573135376,
        "learning_rate": 0.0001318975878544119,
        "epoch": 0.2676,
        "step": 2007
    },
    {
        "loss": 1.8357,
        "grad_norm": 2.7475430965423584,
        "learning_rate": 0.00013183793243068157,
        "epoch": 0.2677333333333333,
        "step": 2008
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.5983786582946777,
        "learning_rate": 0.00013177826439583773,
        "epoch": 0.26786666666666664,
        "step": 2009
    },
    {
        "loss": 2.6016,
        "grad_norm": 3.2097554206848145,
        "learning_rate": 0.00013171858377351506,
        "epoch": 0.268,
        "step": 2010
    },
    {
        "loss": 2.5151,
        "grad_norm": 2.5068678855895996,
        "learning_rate": 0.0001316588905873533,
        "epoch": 0.26813333333333333,
        "step": 2011
    },
    {
        "loss": 2.7353,
        "grad_norm": 3.154287099838257,
        "learning_rate": 0.00013159918486099707,
        "epoch": 0.26826666666666665,
        "step": 2012
    },
    {
        "loss": 2.7625,
        "grad_norm": 2.7791554927825928,
        "learning_rate": 0.00013153946661809607,
        "epoch": 0.2684,
        "step": 2013
    },
    {
        "loss": 3.5042,
        "grad_norm": 3.495511054992676,
        "learning_rate": 0.00013147973588230487,
        "epoch": 0.26853333333333335,
        "step": 2014
    },
    {
        "loss": 2.4048,
        "grad_norm": 3.5216643810272217,
        "learning_rate": 0.000131419992677283,
        "epoch": 0.26866666666666666,
        "step": 2015
    },
    {
        "loss": 2.2488,
        "grad_norm": 2.5505282878875732,
        "learning_rate": 0.00013136023702669503,
        "epoch": 0.2688,
        "step": 2016
    },
    {
        "loss": 3.5774,
        "grad_norm": 3.266235828399658,
        "learning_rate": 0.00013130046895421026,
        "epoch": 0.26893333333333336,
        "step": 2017
    },
    {
        "loss": 2.9507,
        "grad_norm": 3.0437092781066895,
        "learning_rate": 0.00013124068848350312,
        "epoch": 0.2690666666666667,
        "step": 2018
    },
    {
        "loss": 2.9419,
        "grad_norm": 2.9762418270111084,
        "learning_rate": 0.00013118089563825283,
        "epoch": 0.2692,
        "step": 2019
    },
    {
        "loss": 2.5955,
        "grad_norm": 2.968336343765259,
        "learning_rate": 0.00013112109044214346,
        "epoch": 0.2693333333333333,
        "step": 2020
    },
    {
        "loss": 2.561,
        "grad_norm": 3.098515748977661,
        "learning_rate": 0.00013106127291886418,
        "epoch": 0.2694666666666667,
        "step": 2021
    },
    {
        "loss": 2.6275,
        "grad_norm": 2.4591970443725586,
        "learning_rate": 0.00013100144309210888,
        "epoch": 0.2696,
        "step": 2022
    },
    {
        "loss": 2.2338,
        "grad_norm": 3.1607415676116943,
        "learning_rate": 0.0001309416009855763,
        "epoch": 0.2697333333333333,
        "step": 2023
    },
    {
        "loss": 2.8525,
        "grad_norm": 1.8200063705444336,
        "learning_rate": 0.00013088174662297014,
        "epoch": 0.26986666666666664,
        "step": 2024
    },
    {
        "loss": 2.2447,
        "grad_norm": 1.6548181772232056,
        "learning_rate": 0.00013082188002799892,
        "epoch": 0.27,
        "step": 2025
    },
    {
        "loss": 2.0943,
        "grad_norm": 3.4189023971557617,
        "learning_rate": 0.000130762001224376,
        "epoch": 0.27013333333333334,
        "step": 2026
    },
    {
        "loss": 1.6479,
        "grad_norm": 3.091118097305298,
        "learning_rate": 0.0001307021102358196,
        "epoch": 0.27026666666666666,
        "step": 2027
    },
    {
        "loss": 0.6353,
        "grad_norm": 3.1988437175750732,
        "learning_rate": 0.00013064220708605267,
        "epoch": 0.2704,
        "step": 2028
    },
    {
        "loss": 2.2841,
        "grad_norm": 3.003655433654785,
        "learning_rate": 0.00013058229179880313,
        "epoch": 0.27053333333333335,
        "step": 2029
    },
    {
        "loss": 2.8936,
        "grad_norm": 2.7371490001678467,
        "learning_rate": 0.00013052236439780362,
        "epoch": 0.27066666666666667,
        "step": 2030
    },
    {
        "loss": 2.344,
        "grad_norm": 3.0760786533355713,
        "learning_rate": 0.00013046242490679153,
        "epoch": 0.2708,
        "step": 2031
    },
    {
        "loss": 0.6809,
        "grad_norm": 3.401843309402466,
        "learning_rate": 0.00013040247334950914,
        "epoch": 0.27093333333333336,
        "step": 2032
    },
    {
        "loss": 2.0167,
        "grad_norm": 2.1312355995178223,
        "learning_rate": 0.00013034250974970345,
        "epoch": 0.2710666666666667,
        "step": 2033
    },
    {
        "loss": 2.2278,
        "grad_norm": 2.950869560241699,
        "learning_rate": 0.0001302825341311263,
        "epoch": 0.2712,
        "step": 2034
    },
    {
        "loss": 1.8382,
        "grad_norm": 3.4325520992279053,
        "learning_rate": 0.00013022254651753418,
        "epoch": 0.2713333333333333,
        "step": 2035
    },
    {
        "loss": 1.4171,
        "grad_norm": 2.722947835922241,
        "learning_rate": 0.00013016254693268842,
        "epoch": 0.2714666666666667,
        "step": 2036
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.7221760749816895,
        "learning_rate": 0.00013010253540035501,
        "epoch": 0.2716,
        "step": 2037
    },
    {
        "loss": 1.1269,
        "grad_norm": 3.7816731929779053,
        "learning_rate": 0.00013004251194430476,
        "epoch": 0.2717333333333333,
        "step": 2038
    },
    {
        "loss": 2.628,
        "grad_norm": 2.8123998641967773,
        "learning_rate": 0.0001299824765883132,
        "epoch": 0.27186666666666665,
        "step": 2039
    },
    {
        "loss": 2.7752,
        "grad_norm": 2.0687575340270996,
        "learning_rate": 0.0001299224293561605,
        "epoch": 0.272,
        "step": 2040
    },
    {
        "loss": 1.9033,
        "grad_norm": 4.068908214569092,
        "learning_rate": 0.00012986237027163154,
        "epoch": 0.27213333333333334,
        "step": 2041
    },
    {
        "loss": 1.8194,
        "grad_norm": 3.1080143451690674,
        "learning_rate": 0.00012980229935851596,
        "epoch": 0.27226666666666666,
        "step": 2042
    },
    {
        "loss": 2.65,
        "grad_norm": 2.86094069480896,
        "learning_rate": 0.0001297422166406081,
        "epoch": 0.2724,
        "step": 2043
    },
    {
        "loss": 2.4572,
        "grad_norm": 3.2617666721343994,
        "learning_rate": 0.00012968212214170682,
        "epoch": 0.27253333333333335,
        "step": 2044
    },
    {
        "loss": 2.3589,
        "grad_norm": 3.1545591354370117,
        "learning_rate": 0.00012962201588561585,
        "epoch": 0.27266666666666667,
        "step": 2045
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.7813236713409424,
        "learning_rate": 0.00012956189789614347,
        "epoch": 0.2728,
        "step": 2046
    },
    {
        "loss": 2.3496,
        "grad_norm": 3.463343381881714,
        "learning_rate": 0.00012950176819710257,
        "epoch": 0.2729333333333333,
        "step": 2047
    },
    {
        "loss": 2.1609,
        "grad_norm": 2.5982861518859863,
        "learning_rate": 0.0001294416268123108,
        "epoch": 0.2730666666666667,
        "step": 2048
    },
    {
        "loss": 2.9165,
        "grad_norm": 3.4659624099731445,
        "learning_rate": 0.00012938147376559032,
        "epoch": 0.2732,
        "step": 2049
    },
    {
        "loss": 2.3692,
        "grad_norm": 2.4648101329803467,
        "learning_rate": 0.00012932130908076793,
        "epoch": 0.2733333333333333,
        "step": 2050
    },
    {
        "loss": 2.0782,
        "grad_norm": 4.499299049377441,
        "learning_rate": 0.00012926113278167512,
        "epoch": 0.2734666666666667,
        "step": 2051
    },
    {
        "loss": 2.2715,
        "grad_norm": 4.250138759613037,
        "learning_rate": 0.00012920094489214795,
        "epoch": 0.2736,
        "step": 2052
    },
    {
        "loss": 2.8037,
        "grad_norm": 3.6282458305358887,
        "learning_rate": 0.00012914074543602695,
        "epoch": 0.27373333333333333,
        "step": 2053
    },
    {
        "loss": 1.859,
        "grad_norm": 3.3669683933258057,
        "learning_rate": 0.00012908053443715743,
        "epoch": 0.27386666666666665,
        "step": 2054
    },
    {
        "loss": 2.0163,
        "grad_norm": 3.348778486251831,
        "learning_rate": 0.00012902031191938912,
        "epoch": 0.274,
        "step": 2055
    },
    {
        "loss": 2.5255,
        "grad_norm": 3.463860034942627,
        "learning_rate": 0.00012896007790657638,
        "epoch": 0.27413333333333334,
        "step": 2056
    },
    {
        "loss": 2.2252,
        "grad_norm": 2.607762098312378,
        "learning_rate": 0.00012889983242257808,
        "epoch": 0.27426666666666666,
        "step": 2057
    },
    {
        "loss": 1.8541,
        "grad_norm": 3.112370491027832,
        "learning_rate": 0.00012883957549125768,
        "epoch": 0.2744,
        "step": 2058
    },
    {
        "loss": 1.831,
        "grad_norm": 3.5476555824279785,
        "learning_rate": 0.00012877930713648322,
        "epoch": 0.27453333333333335,
        "step": 2059
    },
    {
        "loss": 2.4334,
        "grad_norm": 2.3991401195526123,
        "learning_rate": 0.0001287190273821271,
        "epoch": 0.27466666666666667,
        "step": 2060
    },
    {
        "loss": 2.3713,
        "grad_norm": 4.15354585647583,
        "learning_rate": 0.00012865873625206634,
        "epoch": 0.2748,
        "step": 2061
    },
    {
        "loss": 2.0963,
        "grad_norm": 3.0424416065216064,
        "learning_rate": 0.00012859843377018254,
        "epoch": 0.2749333333333333,
        "step": 2062
    },
    {
        "loss": 2.7694,
        "grad_norm": 3.3293192386627197,
        "learning_rate": 0.00012853811996036167,
        "epoch": 0.2750666666666667,
        "step": 2063
    },
    {
        "loss": 2.5349,
        "grad_norm": 1.9368709325790405,
        "learning_rate": 0.00012847779484649423,
        "epoch": 0.2752,
        "step": 2064
    },
    {
        "loss": 2.1528,
        "grad_norm": 3.409351110458374,
        "learning_rate": 0.00012841745845247526,
        "epoch": 0.2753333333333333,
        "step": 2065
    },
    {
        "loss": 2.6085,
        "grad_norm": 1.6303972005844116,
        "learning_rate": 0.0001283571108022041,
        "epoch": 0.2754666666666667,
        "step": 2066
    },
    {
        "loss": 2.2972,
        "grad_norm": 3.5451886653900146,
        "learning_rate": 0.0001282967519195848,
        "epoch": 0.2756,
        "step": 2067
    },
    {
        "loss": 2.8065,
        "grad_norm": 2.9811577796936035,
        "learning_rate": 0.0001282363818285256,
        "epoch": 0.27573333333333333,
        "step": 2068
    },
    {
        "loss": 2.8584,
        "grad_norm": 2.092024087905884,
        "learning_rate": 0.00012817600055293934,
        "epoch": 0.27586666666666665,
        "step": 2069
    },
    {
        "loss": 1.8785,
        "grad_norm": 3.620330810546875,
        "learning_rate": 0.00012811560811674326,
        "epoch": 0.276,
        "step": 2070
    },
    {
        "loss": 2.7226,
        "grad_norm": 3.2447245121002197,
        "learning_rate": 0.000128055204543859,
        "epoch": 0.27613333333333334,
        "step": 2071
    },
    {
        "loss": 3.294,
        "grad_norm": 3.1629302501678467,
        "learning_rate": 0.00012799478985821264,
        "epoch": 0.27626666666666666,
        "step": 2072
    },
    {
        "loss": 2.1821,
        "grad_norm": 3.3511667251586914,
        "learning_rate": 0.0001279343640837346,
        "epoch": 0.2764,
        "step": 2073
    },
    {
        "loss": 2.4381,
        "grad_norm": 2.0135560035705566,
        "learning_rate": 0.00012787392724435977,
        "epoch": 0.27653333333333335,
        "step": 2074
    },
    {
        "loss": 2.329,
        "grad_norm": 2.9530515670776367,
        "learning_rate": 0.0001278134793640274,
        "epoch": 0.27666666666666667,
        "step": 2075
    },
    {
        "loss": 2.6912,
        "grad_norm": 2.188483238220215,
        "learning_rate": 0.00012775302046668107,
        "epoch": 0.2768,
        "step": 2076
    },
    {
        "loss": 2.4356,
        "grad_norm": 3.057171106338501,
        "learning_rate": 0.0001276925505762688,
        "epoch": 0.2769333333333333,
        "step": 2077
    },
    {
        "loss": 2.7628,
        "grad_norm": 2.9931094646453857,
        "learning_rate": 0.00012763206971674285,
        "epoch": 0.2770666666666667,
        "step": 2078
    },
    {
        "loss": 2.2056,
        "grad_norm": 3.2522222995758057,
        "learning_rate": 0.00012757157791205994,
        "epoch": 0.2772,
        "step": 2079
    },
    {
        "loss": 2.5865,
        "grad_norm": 2.868044137954712,
        "learning_rate": 0.00012751107518618103,
        "epoch": 0.2773333333333333,
        "step": 2080
    },
    {
        "loss": 3.4525,
        "grad_norm": 2.594569683074951,
        "learning_rate": 0.00012745056156307158,
        "epoch": 0.27746666666666664,
        "step": 2081
    },
    {
        "loss": 2.0659,
        "grad_norm": 4.137317657470703,
        "learning_rate": 0.00012739003706670112,
        "epoch": 0.2776,
        "step": 2082
    },
    {
        "loss": 2.5496,
        "grad_norm": 3.782860040664673,
        "learning_rate": 0.00012732950172104364,
        "epoch": 0.27773333333333333,
        "step": 2083
    },
    {
        "loss": 1.0454,
        "grad_norm": 3.918321371078491,
        "learning_rate": 0.0001272689555500774,
        "epoch": 0.27786666666666665,
        "step": 2084
    },
    {
        "loss": 2.7042,
        "grad_norm": 3.064265727996826,
        "learning_rate": 0.00012720839857778497,
        "epoch": 0.278,
        "step": 2085
    },
    {
        "loss": 1.8791,
        "grad_norm": 2.2140052318573,
        "learning_rate": 0.00012714783082815316,
        "epoch": 0.27813333333333334,
        "step": 2086
    },
    {
        "loss": 2.9292,
        "grad_norm": 2.205932140350342,
        "learning_rate": 0.00012708725232517303,
        "epoch": 0.27826666666666666,
        "step": 2087
    },
    {
        "loss": 1.8372,
        "grad_norm": 1.5147137641906738,
        "learning_rate": 0.00012702666309283994,
        "epoch": 0.2784,
        "step": 2088
    },
    {
        "loss": 2.453,
        "grad_norm": 2.256082057952881,
        "learning_rate": 0.00012696606315515356,
        "epoch": 0.27853333333333335,
        "step": 2089
    },
    {
        "loss": 1.0498,
        "grad_norm": 3.9395194053649902,
        "learning_rate": 0.00012690545253611767,
        "epoch": 0.2786666666666667,
        "step": 2090
    },
    {
        "loss": 2.2237,
        "grad_norm": 2.7300500869750977,
        "learning_rate": 0.0001268448312597403,
        "epoch": 0.2788,
        "step": 2091
    },
    {
        "loss": 2.4391,
        "grad_norm": 3.4573378562927246,
        "learning_rate": 0.00012678419935003386,
        "epoch": 0.2789333333333333,
        "step": 2092
    },
    {
        "loss": 2.4768,
        "grad_norm": 3.551987648010254,
        "learning_rate": 0.0001267235568310147,
        "epoch": 0.2790666666666667,
        "step": 2093
    },
    {
        "loss": 3.027,
        "grad_norm": 1.8948715925216675,
        "learning_rate": 0.00012666290372670373,
        "epoch": 0.2792,
        "step": 2094
    },
    {
        "loss": 4.488,
        "grad_norm": 3.9377410411834717,
        "learning_rate": 0.0001266022400611257,
        "epoch": 0.2793333333333333,
        "step": 2095
    },
    {
        "loss": 2.2248,
        "grad_norm": 2.700360059738159,
        "learning_rate": 0.0001265415658583097,
        "epoch": 0.27946666666666664,
        "step": 2096
    },
    {
        "loss": 2.5207,
        "grad_norm": 3.044647693634033,
        "learning_rate": 0.0001264808811422891,
        "epoch": 0.2796,
        "step": 2097
    },
    {
        "loss": 1.8393,
        "grad_norm": 3.960214376449585,
        "learning_rate": 0.0001264201859371012,
        "epoch": 0.27973333333333333,
        "step": 2098
    },
    {
        "loss": 2.5808,
        "grad_norm": 2.473571538925171,
        "learning_rate": 0.00012635948026678764,
        "epoch": 0.27986666666666665,
        "step": 2099
    },
    {
        "loss": 2.4474,
        "grad_norm": 3.3618617057800293,
        "learning_rate": 0.00012629876415539413,
        "epoch": 0.28,
        "step": 2100
    },
    {
        "loss": 2.2884,
        "grad_norm": 5.513077735900879,
        "learning_rate": 0.00012623803762697048,
        "epoch": 0.28013333333333335,
        "step": 2101
    },
    {
        "loss": 2.5552,
        "grad_norm": 3.692318916320801,
        "learning_rate": 0.0001261773007055708,
        "epoch": 0.28026666666666666,
        "step": 2102
    },
    {
        "loss": 2.9122,
        "grad_norm": 2.294044256210327,
        "learning_rate": 0.0001261165534152531,
        "epoch": 0.2804,
        "step": 2103
    },
    {
        "loss": 0.7085,
        "grad_norm": 5.942038059234619,
        "learning_rate": 0.00012605579578007953,
        "epoch": 0.28053333333333336,
        "step": 2104
    },
    {
        "loss": 1.9445,
        "grad_norm": 3.2403275966644287,
        "learning_rate": 0.0001259950278241165,
        "epoch": 0.2806666666666667,
        "step": 2105
    },
    {
        "loss": 2.4145,
        "grad_norm": 2.0116004943847656,
        "learning_rate": 0.00012593424957143442,
        "epoch": 0.2808,
        "step": 2106
    },
    {
        "loss": 2.0181,
        "grad_norm": 3.6907193660736084,
        "learning_rate": 0.00012587346104610768,
        "epoch": 0.2809333333333333,
        "step": 2107
    },
    {
        "loss": 2.7553,
        "grad_norm": 3.349297046661377,
        "learning_rate": 0.00012581266227221485,
        "epoch": 0.2810666666666667,
        "step": 2108
    },
    {
        "loss": 2.438,
        "grad_norm": 2.751444101333618,
        "learning_rate": 0.00012575185327383856,
        "epoch": 0.2812,
        "step": 2109
    },
    {
        "loss": 2.1782,
        "grad_norm": 3.744682550430298,
        "learning_rate": 0.00012569103407506543,
        "epoch": 0.2813333333333333,
        "step": 2110
    },
    {
        "loss": 1.8231,
        "grad_norm": 4.210474014282227,
        "learning_rate": 0.00012563020469998616,
        "epoch": 0.28146666666666664,
        "step": 2111
    },
    {
        "loss": 2.8779,
        "grad_norm": 2.609180450439453,
        "learning_rate": 0.0001255693651726955,
        "epoch": 0.2816,
        "step": 2112
    },
    {
        "loss": 2.226,
        "grad_norm": 2.6006529331207275,
        "learning_rate": 0.00012550851551729214,
        "epoch": 0.28173333333333334,
        "step": 2113
    },
    {
        "loss": 2.7556,
        "grad_norm": 2.3029067516326904,
        "learning_rate": 0.00012544765575787885,
        "epoch": 0.28186666666666665,
        "step": 2114
    },
    {
        "loss": 2.293,
        "grad_norm": 4.276992321014404,
        "learning_rate": 0.00012538678591856244,
        "epoch": 0.282,
        "step": 2115
    },
    {
        "loss": 2.7261,
        "grad_norm": 2.509239673614502,
        "learning_rate": 0.00012532590602345363,
        "epoch": 0.28213333333333335,
        "step": 2116
    },
    {
        "loss": 1.3213,
        "grad_norm": 4.695956707000732,
        "learning_rate": 0.00012526501609666713,
        "epoch": 0.28226666666666667,
        "step": 2117
    },
    {
        "loss": 2.283,
        "grad_norm": 3.6507155895233154,
        "learning_rate": 0.00012520411616232164,
        "epoch": 0.2824,
        "step": 2118
    },
    {
        "loss": 1.7974,
        "grad_norm": 3.1453659534454346,
        "learning_rate": 0.00012514320624453992,
        "epoch": 0.28253333333333336,
        "step": 2119
    },
    {
        "loss": 3.3538,
        "grad_norm": 2.7236504554748535,
        "learning_rate": 0.0001250822863674485,
        "epoch": 0.2826666666666667,
        "step": 2120
    },
    {
        "loss": 3.2967,
        "grad_norm": 3.285945415496826,
        "learning_rate": 0.00012502135655517797,
        "epoch": 0.2828,
        "step": 2121
    },
    {
        "loss": 1.7097,
        "grad_norm": 3.2196741104125977,
        "learning_rate": 0.00012496041683186286,
        "epoch": 0.2829333333333333,
        "step": 2122
    },
    {
        "loss": 1.3682,
        "grad_norm": 3.1093366146087646,
        "learning_rate": 0.0001248994672216416,
        "epoch": 0.2830666666666667,
        "step": 2123
    },
    {
        "loss": 2.1226,
        "grad_norm": 4.3441267013549805,
        "learning_rate": 0.00012483850774865654,
        "epoch": 0.2832,
        "step": 2124
    },
    {
        "loss": 1.6613,
        "grad_norm": 1.9638898372650146,
        "learning_rate": 0.0001247775384370539,
        "epoch": 0.2833333333333333,
        "step": 2125
    },
    {
        "loss": 2.6561,
        "grad_norm": 2.3375487327575684,
        "learning_rate": 0.00012471655931098384,
        "epoch": 0.28346666666666664,
        "step": 2126
    },
    {
        "loss": 3.2096,
        "grad_norm": 2.3927927017211914,
        "learning_rate": 0.00012465557039460047,
        "epoch": 0.2836,
        "step": 2127
    },
    {
        "loss": 2.0133,
        "grad_norm": 3.0532634258270264,
        "learning_rate": 0.00012459457171206165,
        "epoch": 0.28373333333333334,
        "step": 2128
    },
    {
        "loss": 1.1585,
        "grad_norm": 4.334242343902588,
        "learning_rate": 0.00012453356328752914,
        "epoch": 0.28386666666666666,
        "step": 2129
    },
    {
        "loss": 1.8177,
        "grad_norm": 2.9928901195526123,
        "learning_rate": 0.00012447254514516863,
        "epoch": 0.284,
        "step": 2130
    },
    {
        "loss": 2.6899,
        "grad_norm": 4.043809413909912,
        "learning_rate": 0.00012441151730914962,
        "epoch": 0.28413333333333335,
        "step": 2131
    },
    {
        "loss": 2.4921,
        "grad_norm": 2.392552375793457,
        "learning_rate": 0.0001243504798036454,
        "epoch": 0.28426666666666667,
        "step": 2132
    },
    {
        "loss": 2.044,
        "grad_norm": 3.8154146671295166,
        "learning_rate": 0.00012428943265283316,
        "epoch": 0.2844,
        "step": 2133
    },
    {
        "loss": 1.6225,
        "grad_norm": 3.819753885269165,
        "learning_rate": 0.00012422837588089384,
        "epoch": 0.28453333333333336,
        "step": 2134
    },
    {
        "loss": 2.625,
        "grad_norm": 2.3142642974853516,
        "learning_rate": 0.00012416730951201233,
        "epoch": 0.2846666666666667,
        "step": 2135
    },
    {
        "loss": 2.1762,
        "grad_norm": 3.255434513092041,
        "learning_rate": 0.00012410623357037717,
        "epoch": 0.2848,
        "step": 2136
    },
    {
        "loss": 2.0172,
        "grad_norm": 3.5405731201171875,
        "learning_rate": 0.00012404514808018072,
        "epoch": 0.2849333333333333,
        "step": 2137
    },
    {
        "loss": 2.6034,
        "grad_norm": 2.4558587074279785,
        "learning_rate": 0.00012398405306561922,
        "epoch": 0.2850666666666667,
        "step": 2138
    },
    {
        "loss": 2.6896,
        "grad_norm": 3.4695498943328857,
        "learning_rate": 0.00012392294855089256,
        "epoch": 0.2852,
        "step": 2139
    },
    {
        "loss": 2.4057,
        "grad_norm": 4.518703937530518,
        "learning_rate": 0.00012386183456020448,
        "epoch": 0.2853333333333333,
        "step": 2140
    },
    {
        "loss": 2.4685,
        "grad_norm": 3.496718168258667,
        "learning_rate": 0.00012380071111776244,
        "epoch": 0.28546666666666665,
        "step": 2141
    },
    {
        "loss": 2.4375,
        "grad_norm": 2.448556661605835,
        "learning_rate": 0.00012373957824777758,
        "epoch": 0.2856,
        "step": 2142
    },
    {
        "loss": 1.6758,
        "grad_norm": 3.274044990539551,
        "learning_rate": 0.0001236784359744649,
        "epoch": 0.28573333333333334,
        "step": 2143
    },
    {
        "loss": 2.2869,
        "grad_norm": 3.5425844192504883,
        "learning_rate": 0.00012361728432204306,
        "epoch": 0.28586666666666666,
        "step": 2144
    },
    {
        "loss": 2.5234,
        "grad_norm": 3.96298885345459,
        "learning_rate": 0.00012355612331473444,
        "epoch": 0.286,
        "step": 2145
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.868239402770996,
        "learning_rate": 0.00012349495297676505,
        "epoch": 0.28613333333333335,
        "step": 2146
    },
    {
        "loss": 2.5251,
        "grad_norm": 2.7619290351867676,
        "learning_rate": 0.00012343377333236477,
        "epoch": 0.28626666666666667,
        "step": 2147
    },
    {
        "loss": 2.4422,
        "grad_norm": 4.091525077819824,
        "learning_rate": 0.00012337258440576704,
        "epoch": 0.2864,
        "step": 2148
    },
    {
        "loss": 1.4757,
        "grad_norm": 4.979947090148926,
        "learning_rate": 0.00012331138622120896,
        "epoch": 0.2865333333333333,
        "step": 2149
    },
    {
        "loss": 1.6538,
        "grad_norm": 3.3049416542053223,
        "learning_rate": 0.0001232501788029314,
        "epoch": 0.2866666666666667,
        "step": 2150
    },
    {
        "loss": 2.5564,
        "grad_norm": 3.1291778087615967,
        "learning_rate": 0.00012318896217517874,
        "epoch": 0.2868,
        "step": 2151
    },
    {
        "loss": 2.6398,
        "grad_norm": 3.392590284347534,
        "learning_rate": 0.0001231277363621992,
        "epoch": 0.2869333333333333,
        "step": 2152
    },
    {
        "loss": 1.5794,
        "grad_norm": 3.5460357666015625,
        "learning_rate": 0.00012306650138824447,
        "epoch": 0.2870666666666667,
        "step": 2153
    },
    {
        "loss": 2.0048,
        "grad_norm": 3.336139440536499,
        "learning_rate": 0.0001230052572775699,
        "epoch": 0.2872,
        "step": 2154
    },
    {
        "loss": 1.4664,
        "grad_norm": 3.795130729675293,
        "learning_rate": 0.00012294400405443457,
        "epoch": 0.28733333333333333,
        "step": 2155
    },
    {
        "loss": 2.679,
        "grad_norm": 3.07346773147583,
        "learning_rate": 0.00012288274174310102,
        "epoch": 0.28746666666666665,
        "step": 2156
    },
    {
        "loss": 2.8052,
        "grad_norm": 2.8284194469451904,
        "learning_rate": 0.00012282147036783556,
        "epoch": 0.2876,
        "step": 2157
    },
    {
        "loss": 2.5065,
        "grad_norm": 1.620750904083252,
        "learning_rate": 0.00012276018995290787,
        "epoch": 0.28773333333333334,
        "step": 2158
    },
    {
        "loss": 2.9959,
        "grad_norm": 3.796705961227417,
        "learning_rate": 0.00012269890052259142,
        "epoch": 0.28786666666666666,
        "step": 2159
    },
    {
        "loss": 2.6942,
        "grad_norm": 3.6432788372039795,
        "learning_rate": 0.0001226376021011631,
        "epoch": 0.288,
        "step": 2160
    },
    {
        "loss": 2.2929,
        "grad_norm": 2.565809726715088,
        "learning_rate": 0.0001225762947129035,
        "epoch": 0.28813333333333335,
        "step": 2161
    },
    {
        "loss": 2.9136,
        "grad_norm": 2.331695079803467,
        "learning_rate": 0.00012251497838209658,
        "epoch": 0.28826666666666667,
        "step": 2162
    },
    {
        "loss": 2.1466,
        "grad_norm": 4.040998935699463,
        "learning_rate": 0.00012245365313303004,
        "epoch": 0.2884,
        "step": 2163
    },
    {
        "loss": 2.8653,
        "grad_norm": 3.18656849861145,
        "learning_rate": 0.00012239231898999496,
        "epoch": 0.2885333333333333,
        "step": 2164
    },
    {
        "loss": 2.3864,
        "grad_norm": 3.5285468101501465,
        "learning_rate": 0.00012233097597728602,
        "epoch": 0.2886666666666667,
        "step": 2165
    },
    {
        "loss": 1.305,
        "grad_norm": 2.6483302116394043,
        "learning_rate": 0.00012226962411920146,
        "epoch": 0.2888,
        "step": 2166
    },
    {
        "loss": 1.5324,
        "grad_norm": 3.0859487056732178,
        "learning_rate": 0.00012220826344004287,
        "epoch": 0.2889333333333333,
        "step": 2167
    },
    {
        "loss": 1.8364,
        "grad_norm": 2.915522813796997,
        "learning_rate": 0.00012214689396411544,
        "epoch": 0.2890666666666667,
        "step": 2168
    },
    {
        "loss": 2.6736,
        "grad_norm": 2.913654088973999,
        "learning_rate": 0.00012208551571572787,
        "epoch": 0.2892,
        "step": 2169
    },
    {
        "loss": 1.7602,
        "grad_norm": 3.614537000656128,
        "learning_rate": 0.00012202412871919228,
        "epoch": 0.28933333333333333,
        "step": 2170
    },
    {
        "loss": 1.7356,
        "grad_norm": 3.605412721633911,
        "learning_rate": 0.00012196273299882422,
        "epoch": 0.28946666666666665,
        "step": 2171
    },
    {
        "loss": 2.2873,
        "grad_norm": 3.7493999004364014,
        "learning_rate": 0.0001219013285789428,
        "epoch": 0.2896,
        "step": 2172
    },
    {
        "loss": 2.3896,
        "grad_norm": 3.1028714179992676,
        "learning_rate": 0.00012183991548387053,
        "epoch": 0.28973333333333334,
        "step": 2173
    },
    {
        "loss": 2.296,
        "grad_norm": 2.436507225036621,
        "learning_rate": 0.00012177849373793328,
        "epoch": 0.28986666666666666,
        "step": 2174
    },
    {
        "loss": 2.5094,
        "grad_norm": 2.900141954421997,
        "learning_rate": 0.00012171706336546048,
        "epoch": 0.29,
        "step": 2175
    },
    {
        "loss": 1.841,
        "grad_norm": 3.6035428047180176,
        "learning_rate": 0.00012165562439078488,
        "epoch": 0.29013333333333335,
        "step": 2176
    },
    {
        "loss": 1.5857,
        "grad_norm": 5.174264907836914,
        "learning_rate": 0.00012159417683824266,
        "epoch": 0.2902666666666667,
        "step": 2177
    },
    {
        "loss": 2.4259,
        "grad_norm": 4.291200160980225,
        "learning_rate": 0.0001215327207321734,
        "epoch": 0.2904,
        "step": 2178
    },
    {
        "loss": 2.24,
        "grad_norm": 2.4453835487365723,
        "learning_rate": 0.00012147125609692012,
        "epoch": 0.2905333333333333,
        "step": 2179
    },
    {
        "loss": 1.9617,
        "grad_norm": 4.4634599685668945,
        "learning_rate": 0.00012140978295682912,
        "epoch": 0.2906666666666667,
        "step": 2180
    },
    {
        "loss": 1.441,
        "grad_norm": 5.507822513580322,
        "learning_rate": 0.00012134830133625012,
        "epoch": 0.2908,
        "step": 2181
    },
    {
        "loss": 1.4097,
        "grad_norm": 5.033125877380371,
        "learning_rate": 0.00012128681125953626,
        "epoch": 0.2909333333333333,
        "step": 2182
    },
    {
        "loss": 2.7621,
        "grad_norm": 3.4130396842956543,
        "learning_rate": 0.00012122531275104391,
        "epoch": 0.29106666666666664,
        "step": 2183
    },
    {
        "loss": 2.5314,
        "grad_norm": 3.00604248046875,
        "learning_rate": 0.00012116380583513285,
        "epoch": 0.2912,
        "step": 2184
    },
    {
        "loss": 1.9215,
        "grad_norm": 2.9636027812957764,
        "learning_rate": 0.00012110229053616619,
        "epoch": 0.29133333333333333,
        "step": 2185
    },
    {
        "loss": 2.6654,
        "grad_norm": 3.2311341762542725,
        "learning_rate": 0.00012104076687851034,
        "epoch": 0.29146666666666665,
        "step": 2186
    },
    {
        "loss": 2.6962,
        "grad_norm": 4.701249599456787,
        "learning_rate": 0.00012097923488653505,
        "epoch": 0.2916,
        "step": 2187
    },
    {
        "loss": 2.871,
        "grad_norm": 2.623399496078491,
        "learning_rate": 0.00012091769458461333,
        "epoch": 0.29173333333333334,
        "step": 2188
    },
    {
        "loss": 3.0242,
        "grad_norm": 4.177919387817383,
        "learning_rate": 0.00012085614599712147,
        "epoch": 0.29186666666666666,
        "step": 2189
    },
    {
        "loss": 2.0104,
        "grad_norm": 4.703281879425049,
        "learning_rate": 0.00012079458914843915,
        "epoch": 0.292,
        "step": 2190
    },
    {
        "loss": 2.7234,
        "grad_norm": 3.343242645263672,
        "learning_rate": 0.00012073302406294924,
        "epoch": 0.29213333333333336,
        "step": 2191
    },
    {
        "loss": 2.542,
        "grad_norm": 3.8795697689056396,
        "learning_rate": 0.0001206714507650378,
        "epoch": 0.2922666666666667,
        "step": 2192
    },
    {
        "loss": 3.0246,
        "grad_norm": 3.5910792350769043,
        "learning_rate": 0.00012060986927909433,
        "epoch": 0.2924,
        "step": 2193
    },
    {
        "loss": 2.1935,
        "grad_norm": 3.8637866973876953,
        "learning_rate": 0.00012054827962951132,
        "epoch": 0.2925333333333333,
        "step": 2194
    },
    {
        "loss": 2.3522,
        "grad_norm": 3.719716787338257,
        "learning_rate": 0.00012048668184068481,
        "epoch": 0.2926666666666667,
        "step": 2195
    },
    {
        "loss": 1.8602,
        "grad_norm": 2.82308030128479,
        "learning_rate": 0.00012042507593701382,
        "epoch": 0.2928,
        "step": 2196
    },
    {
        "loss": 2.7948,
        "grad_norm": 3.0179011821746826,
        "learning_rate": 0.0001203634619429006,
        "epoch": 0.2929333333333333,
        "step": 2197
    },
    {
        "loss": 2.4108,
        "grad_norm": 3.3864307403564453,
        "learning_rate": 0.00012030183988275076,
        "epoch": 0.29306666666666664,
        "step": 2198
    },
    {
        "loss": 2.2841,
        "grad_norm": 4.3042683601379395,
        "learning_rate": 0.00012024020978097296,
        "epoch": 0.2932,
        "step": 2199
    },
    {
        "loss": 2.5994,
        "grad_norm": 3.0318691730499268,
        "learning_rate": 0.00012017857166197911,
        "epoch": 0.29333333333333333,
        "step": 2200
    },
    {
        "loss": 0.7716,
        "grad_norm": 3.798863410949707,
        "learning_rate": 0.00012011692555018425,
        "epoch": 0.29346666666666665,
        "step": 2201
    },
    {
        "loss": 2.3871,
        "grad_norm": 3.330220937728882,
        "learning_rate": 0.00012005527147000663,
        "epoch": 0.2936,
        "step": 2202
    },
    {
        "loss": 1.7056,
        "grad_norm": 3.1323578357696533,
        "learning_rate": 0.00011999360944586766,
        "epoch": 0.29373333333333335,
        "step": 2203
    },
    {
        "loss": 2.1159,
        "grad_norm": 3.5906777381896973,
        "learning_rate": 0.00011993193950219186,
        "epoch": 0.29386666666666666,
        "step": 2204
    },
    {
        "loss": 2.9676,
        "grad_norm": 2.152604818344116,
        "learning_rate": 0.00011987026166340689,
        "epoch": 0.294,
        "step": 2205
    },
    {
        "loss": 2.1783,
        "grad_norm": 2.0533111095428467,
        "learning_rate": 0.00011980857595394357,
        "epoch": 0.29413333333333336,
        "step": 2206
    },
    {
        "loss": 1.2192,
        "grad_norm": 4.318284511566162,
        "learning_rate": 0.00011974688239823583,
        "epoch": 0.2942666666666667,
        "step": 2207
    },
    {
        "loss": 1.279,
        "grad_norm": 3.9335763454437256,
        "learning_rate": 0.0001196851810207207,
        "epoch": 0.2944,
        "step": 2208
    },
    {
        "loss": 2.6411,
        "grad_norm": 2.440204620361328,
        "learning_rate": 0.00011962347184583827,
        "epoch": 0.2945333333333333,
        "step": 2209
    },
    {
        "loss": 2.9989,
        "grad_norm": 2.0392658710479736,
        "learning_rate": 0.00011956175489803177,
        "epoch": 0.2946666666666667,
        "step": 2210
    },
    {
        "loss": 2.4386,
        "grad_norm": 2.962841749191284,
        "learning_rate": 0.00011950003020174752,
        "epoch": 0.2948,
        "step": 2211
    },
    {
        "loss": 1.8014,
        "grad_norm": 3.2338976860046387,
        "learning_rate": 0.00011943829778143485,
        "epoch": 0.2949333333333333,
        "step": 2212
    },
    {
        "loss": 2.0836,
        "grad_norm": 3.516129732131958,
        "learning_rate": 0.0001193765576615462,
        "epoch": 0.29506666666666664,
        "step": 2213
    },
    {
        "loss": 1.9707,
        "grad_norm": 3.0786678791046143,
        "learning_rate": 0.00011931480986653701,
        "epoch": 0.2952,
        "step": 2214
    },
    {
        "loss": 2.4587,
        "grad_norm": 3.9577393531799316,
        "learning_rate": 0.00011925305442086582,
        "epoch": 0.29533333333333334,
        "step": 2215
    },
    {
        "loss": 2.3206,
        "grad_norm": 2.298243284225464,
        "learning_rate": 0.00011919129134899422,
        "epoch": 0.29546666666666666,
        "step": 2216
    },
    {
        "loss": 2.8285,
        "grad_norm": 3.5064685344696045,
        "learning_rate": 0.0001191295206753867,
        "epoch": 0.2956,
        "step": 2217
    },
    {
        "loss": 2.1646,
        "grad_norm": 2.2304863929748535,
        "learning_rate": 0.00011906774242451087,
        "epoch": 0.29573333333333335,
        "step": 2218
    },
    {
        "loss": 2.6437,
        "grad_norm": 3.0576682090759277,
        "learning_rate": 0.00011900595662083725,
        "epoch": 0.29586666666666667,
        "step": 2219
    },
    {
        "loss": 2.0287,
        "grad_norm": 3.268838405609131,
        "learning_rate": 0.0001189441632888395,
        "epoch": 0.296,
        "step": 2220
    },
    {
        "loss": 2.3346,
        "grad_norm": 6.053372859954834,
        "learning_rate": 0.00011888236245299411,
        "epoch": 0.29613333333333336,
        "step": 2221
    },
    {
        "loss": 2.4478,
        "grad_norm": 2.631795644760132,
        "learning_rate": 0.00011882055413778059,
        "epoch": 0.2962666666666667,
        "step": 2222
    },
    {
        "loss": 2.3829,
        "grad_norm": 2.9337403774261475,
        "learning_rate": 0.00011875873836768145,
        "epoch": 0.2964,
        "step": 2223
    },
    {
        "loss": 2.7729,
        "grad_norm": 2.193192720413208,
        "learning_rate": 0.00011869691516718215,
        "epoch": 0.2965333333333333,
        "step": 2224
    },
    {
        "loss": 2.5197,
        "grad_norm": 2.393646240234375,
        "learning_rate": 0.00011863508456077104,
        "epoch": 0.2966666666666667,
        "step": 2225
    },
    {
        "loss": 2.5644,
        "grad_norm": 2.9187541007995605,
        "learning_rate": 0.00011857324657293943,
        "epoch": 0.2968,
        "step": 2226
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.940469980239868,
        "learning_rate": 0.00011851140122818154,
        "epoch": 0.2969333333333333,
        "step": 2227
    },
    {
        "loss": 2.5448,
        "grad_norm": 2.4652440547943115,
        "learning_rate": 0.00011844954855099458,
        "epoch": 0.29706666666666665,
        "step": 2228
    },
    {
        "loss": 2.5706,
        "grad_norm": 2.1462366580963135,
        "learning_rate": 0.00011838768856587858,
        "epoch": 0.2972,
        "step": 2229
    },
    {
        "loss": 1.8706,
        "grad_norm": 4.657987594604492,
        "learning_rate": 0.00011832582129733644,
        "epoch": 0.29733333333333334,
        "step": 2230
    },
    {
        "loss": 2.0994,
        "grad_norm": 3.857778310775757,
        "learning_rate": 0.00011826394676987409,
        "epoch": 0.29746666666666666,
        "step": 2231
    },
    {
        "loss": 2.4335,
        "grad_norm": 2.7405941486358643,
        "learning_rate": 0.00011820206500800017,
        "epoch": 0.2976,
        "step": 2232
    },
    {
        "loss": 2.9608,
        "grad_norm": 3.059626579284668,
        "learning_rate": 0.00011814017603622626,
        "epoch": 0.29773333333333335,
        "step": 2233
    },
    {
        "loss": 0.9981,
        "grad_norm": 10.826891899108887,
        "learning_rate": 0.00011807827987906684,
        "epoch": 0.29786666666666667,
        "step": 2234
    },
    {
        "loss": 2.3782,
        "grad_norm": 3.614130735397339,
        "learning_rate": 0.00011801637656103912,
        "epoch": 0.298,
        "step": 2235
    },
    {
        "loss": 2.8435,
        "grad_norm": 3.578713893890381,
        "learning_rate": 0.00011795446610666329,
        "epoch": 0.2981333333333333,
        "step": 2236
    },
    {
        "loss": 2.8871,
        "grad_norm": 3.998473882675171,
        "learning_rate": 0.00011789254854046225,
        "epoch": 0.2982666666666667,
        "step": 2237
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.82763934135437,
        "learning_rate": 0.00011783062388696177,
        "epoch": 0.2984,
        "step": 2238
    },
    {
        "loss": 2.7401,
        "grad_norm": 2.8275997638702393,
        "learning_rate": 0.00011776869217069042,
        "epoch": 0.2985333333333333,
        "step": 2239
    },
    {
        "loss": 2.3642,
        "grad_norm": 3.1963415145874023,
        "learning_rate": 0.00011770675341617952,
        "epoch": 0.2986666666666667,
        "step": 2240
    },
    {
        "loss": 2.102,
        "grad_norm": 3.9423365592956543,
        "learning_rate": 0.00011764480764796328,
        "epoch": 0.2988,
        "step": 2241
    },
    {
        "loss": 2.4192,
        "grad_norm": 4.377274513244629,
        "learning_rate": 0.00011758285489057863,
        "epoch": 0.29893333333333333,
        "step": 2242
    },
    {
        "loss": 1.1242,
        "grad_norm": 3.9018075466156006,
        "learning_rate": 0.00011752089516856521,
        "epoch": 0.29906666666666665,
        "step": 2243
    },
    {
        "loss": 2.2046,
        "grad_norm": 3.9266433715820312,
        "learning_rate": 0.00011745892850646552,
        "epoch": 0.2992,
        "step": 2244
    },
    {
        "loss": 2.7195,
        "grad_norm": 3.5801854133605957,
        "learning_rate": 0.00011739695492882479,
        "epoch": 0.29933333333333334,
        "step": 2245
    },
    {
        "loss": 2.4537,
        "grad_norm": 3.112333297729492,
        "learning_rate": 0.00011733497446019091,
        "epoch": 0.29946666666666666,
        "step": 2246
    },
    {
        "loss": 2.5834,
        "grad_norm": 1.8917409181594849,
        "learning_rate": 0.00011727298712511457,
        "epoch": 0.2996,
        "step": 2247
    },
    {
        "loss": 2.4858,
        "grad_norm": 1.759222388267517,
        "learning_rate": 0.00011721099294814917,
        "epoch": 0.29973333333333335,
        "step": 2248
    },
    {
        "loss": 2.0464,
        "grad_norm": 3.0297060012817383,
        "learning_rate": 0.00011714899195385086,
        "epoch": 0.29986666666666667,
        "step": 2249
    },
    {
        "loss": 1.2823,
        "grad_norm": 3.055476427078247,
        "learning_rate": 0.00011708698416677837,
        "epoch": 0.3,
        "step": 2250
    },
    {
        "loss": 2.581,
        "grad_norm": 5.364780902862549,
        "learning_rate": 0.00011702496961149322,
        "epoch": 0.3001333333333333,
        "step": 2251
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.71878719329834,
        "learning_rate": 0.0001169629483125596,
        "epoch": 0.3002666666666667,
        "step": 2252
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.7314627170562744,
        "learning_rate": 0.00011690092029454436,
        "epoch": 0.3004,
        "step": 2253
    },
    {
        "loss": 2.396,
        "grad_norm": 2.9895389080047607,
        "learning_rate": 0.00011683888558201703,
        "epoch": 0.3005333333333333,
        "step": 2254
    },
    {
        "loss": 2.2445,
        "grad_norm": 3.5323328971862793,
        "learning_rate": 0.00011677684419954968,
        "epoch": 0.3006666666666667,
        "step": 2255
    },
    {
        "loss": 2.597,
        "grad_norm": 2.2511203289031982,
        "learning_rate": 0.00011671479617171721,
        "epoch": 0.3008,
        "step": 2256
    },
    {
        "loss": 1.8623,
        "grad_norm": 2.4196746349334717,
        "learning_rate": 0.00011665274152309702,
        "epoch": 0.30093333333333333,
        "step": 2257
    },
    {
        "loss": 2.7879,
        "grad_norm": 2.29093074798584,
        "learning_rate": 0.0001165906802782692,
        "epoch": 0.30106666666666665,
        "step": 2258
    },
    {
        "loss": 3.1214,
        "grad_norm": 3.0966899394989014,
        "learning_rate": 0.00011652861246181639,
        "epoch": 0.3012,
        "step": 2259
    },
    {
        "loss": 2.3564,
        "grad_norm": 1.7455888986587524,
        "learning_rate": 0.00011646653809832381,
        "epoch": 0.30133333333333334,
        "step": 2260
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.851431131362915,
        "learning_rate": 0.00011640445721237943,
        "epoch": 0.30146666666666666,
        "step": 2261
    },
    {
        "loss": 1.0075,
        "grad_norm": 2.2237677574157715,
        "learning_rate": 0.00011634236982857363,
        "epoch": 0.3016,
        "step": 2262
    },
    {
        "loss": 2.0509,
        "grad_norm": 2.3823189735412598,
        "learning_rate": 0.00011628027597149947,
        "epoch": 0.30173333333333335,
        "step": 2263
    },
    {
        "loss": 1.5728,
        "grad_norm": 4.13517951965332,
        "learning_rate": 0.00011621817566575251,
        "epoch": 0.30186666666666667,
        "step": 2264
    },
    {
        "loss": 2.4248,
        "grad_norm": 2.7862324714660645,
        "learning_rate": 0.0001161560689359309,
        "epoch": 0.302,
        "step": 2265
    },
    {
        "loss": 2.9712,
        "grad_norm": 3.8752005100250244,
        "learning_rate": 0.00011609395580663534,
        "epoch": 0.3021333333333333,
        "step": 2266
    },
    {
        "loss": 1.722,
        "grad_norm": 5.2658772468566895,
        "learning_rate": 0.00011603183630246908,
        "epoch": 0.3022666666666667,
        "step": 2267
    },
    {
        "loss": 2.6849,
        "grad_norm": 3.468425989151001,
        "learning_rate": 0.00011596971044803779,
        "epoch": 0.3024,
        "step": 2268
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.7732388973236084,
        "learning_rate": 0.0001159075782679498,
        "epoch": 0.3025333333333333,
        "step": 2269
    },
    {
        "loss": 3.0115,
        "grad_norm": 2.7812392711639404,
        "learning_rate": 0.00011584543978681584,
        "epoch": 0.30266666666666664,
        "step": 2270
    },
    {
        "loss": 2.3035,
        "grad_norm": 3.7648563385009766,
        "learning_rate": 0.0001157832950292492,
        "epoch": 0.3028,
        "step": 2271
    },
    {
        "loss": 2.6002,
        "grad_norm": 3.09165620803833,
        "learning_rate": 0.0001157211440198656,
        "epoch": 0.30293333333333333,
        "step": 2272
    },
    {
        "loss": 2.5314,
        "grad_norm": 3.203690528869629,
        "learning_rate": 0.00011565898678328328,
        "epoch": 0.30306666666666665,
        "step": 2273
    },
    {
        "loss": 2.6348,
        "grad_norm": 2.206465244293213,
        "learning_rate": 0.00011559682334412296,
        "epoch": 0.3032,
        "step": 2274
    },
    {
        "loss": 3.0232,
        "grad_norm": 4.107391834259033,
        "learning_rate": 0.00011553465372700776,
        "epoch": 0.30333333333333334,
        "step": 2275
    },
    {
        "loss": 2.2157,
        "grad_norm": 2.9987845420837402,
        "learning_rate": 0.00011547247795656328,
        "epoch": 0.30346666666666666,
        "step": 2276
    },
    {
        "loss": 2.5774,
        "grad_norm": 2.0165393352508545,
        "learning_rate": 0.00011541029605741758,
        "epoch": 0.3036,
        "step": 2277
    },
    {
        "loss": 2.3571,
        "grad_norm": 2.7479608058929443,
        "learning_rate": 0.00011534810805420109,
        "epoch": 0.30373333333333336,
        "step": 2278
    },
    {
        "loss": 3.2079,
        "grad_norm": 3.949100971221924,
        "learning_rate": 0.00011528591397154671,
        "epoch": 0.3038666666666667,
        "step": 2279
    },
    {
        "loss": 1.6522,
        "grad_norm": 4.665106296539307,
        "learning_rate": 0.00011522371383408972,
        "epoch": 0.304,
        "step": 2280
    },
    {
        "loss": 2.5824,
        "grad_norm": 3.1111388206481934,
        "learning_rate": 0.0001151615076664678,
        "epoch": 0.3041333333333333,
        "step": 2281
    },
    {
        "loss": 1.97,
        "grad_norm": 3.3500890731811523,
        "learning_rate": 0.00011509929549332102,
        "epoch": 0.3042666666666667,
        "step": 2282
    },
    {
        "loss": 1.5372,
        "grad_norm": 3.3184306621551514,
        "learning_rate": 0.00011503707733929188,
        "epoch": 0.3044,
        "step": 2283
    },
    {
        "loss": 2.2402,
        "grad_norm": 3.1005899906158447,
        "learning_rate": 0.00011497485322902515,
        "epoch": 0.3045333333333333,
        "step": 2284
    },
    {
        "loss": 1.2375,
        "grad_norm": 2.4875826835632324,
        "learning_rate": 0.000114912623187168,
        "epoch": 0.30466666666666664,
        "step": 2285
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.3457438945770264,
        "learning_rate": 0.00011485038723837003,
        "epoch": 0.3048,
        "step": 2286
    },
    {
        "loss": 1.5175,
        "grad_norm": 2.9703829288482666,
        "learning_rate": 0.00011478814540728307,
        "epoch": 0.30493333333333333,
        "step": 2287
    },
    {
        "loss": 1.451,
        "grad_norm": 3.8861098289489746,
        "learning_rate": 0.00011472589771856132,
        "epoch": 0.30506666666666665,
        "step": 2288
    },
    {
        "loss": 2.4983,
        "grad_norm": 2.2936935424804688,
        "learning_rate": 0.00011466364419686131,
        "epoch": 0.3052,
        "step": 2289
    },
    {
        "loss": 2.8973,
        "grad_norm": 2.662095308303833,
        "learning_rate": 0.00011460138486684184,
        "epoch": 0.30533333333333335,
        "step": 2290
    },
    {
        "loss": 2.4532,
        "grad_norm": 3.578516960144043,
        "learning_rate": 0.00011453911975316411,
        "epoch": 0.30546666666666666,
        "step": 2291
    },
    {
        "loss": 2.0076,
        "grad_norm": 5.5928215980529785,
        "learning_rate": 0.0001144768488804915,
        "epoch": 0.3056,
        "step": 2292
    },
    {
        "loss": 2.9584,
        "grad_norm": 2.4054148197174072,
        "learning_rate": 0.00011441457227348971,
        "epoch": 0.30573333333333336,
        "step": 2293
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.3094232082366943,
        "learning_rate": 0.00011435228995682672,
        "epoch": 0.3058666666666667,
        "step": 2294
    },
    {
        "loss": 3.0058,
        "grad_norm": 3.2247865200042725,
        "learning_rate": 0.00011429000195517274,
        "epoch": 0.306,
        "step": 2295
    },
    {
        "loss": 3.0171,
        "grad_norm": 2.68800687789917,
        "learning_rate": 0.00011422770829320035,
        "epoch": 0.3061333333333333,
        "step": 2296
    },
    {
        "loss": 2.8076,
        "grad_norm": 2.0564961433410645,
        "learning_rate": 0.00011416540899558423,
        "epoch": 0.3062666666666667,
        "step": 2297
    },
    {
        "loss": 2.2948,
        "grad_norm": 4.672968864440918,
        "learning_rate": 0.0001141031040870013,
        "epoch": 0.3064,
        "step": 2298
    },
    {
        "loss": 2.0837,
        "grad_norm": 4.0204010009765625,
        "learning_rate": 0.0001140407935921308,
        "epoch": 0.3065333333333333,
        "step": 2299
    },
    {
        "loss": 2.6255,
        "grad_norm": 3.077430009841919,
        "learning_rate": 0.00011397847753565411,
        "epoch": 0.30666666666666664,
        "step": 2300
    },
    {
        "loss": 2.3375,
        "grad_norm": 3.3580009937286377,
        "learning_rate": 0.00011391615594225486,
        "epoch": 0.3068,
        "step": 2301
    },
    {
        "loss": 1.2526,
        "grad_norm": 2.8508777618408203,
        "learning_rate": 0.00011385382883661881,
        "epoch": 0.30693333333333334,
        "step": 2302
    },
    {
        "loss": 1.9366,
        "grad_norm": 4.531525135040283,
        "learning_rate": 0.00011379149624343395,
        "epoch": 0.30706666666666665,
        "step": 2303
    },
    {
        "loss": 1.8333,
        "grad_norm": 2.9534175395965576,
        "learning_rate": 0.00011372915818739043,
        "epoch": 0.3072,
        "step": 2304
    },
    {
        "loss": 2.22,
        "grad_norm": 3.8865807056427,
        "learning_rate": 0.00011366681469318061,
        "epoch": 0.30733333333333335,
        "step": 2305
    },
    {
        "loss": 2.7752,
        "grad_norm": 2.4416863918304443,
        "learning_rate": 0.00011360446578549885,
        "epoch": 0.30746666666666667,
        "step": 2306
    },
    {
        "loss": 2.4227,
        "grad_norm": 3.981067419052124,
        "learning_rate": 0.00011354211148904185,
        "epoch": 0.3076,
        "step": 2307
    },
    {
        "loss": 2.4243,
        "grad_norm": 2.2510762214660645,
        "learning_rate": 0.0001134797518285084,
        "epoch": 0.30773333333333336,
        "step": 2308
    },
    {
        "loss": 2.7893,
        "grad_norm": 3.805535078048706,
        "learning_rate": 0.0001134173868285993,
        "epoch": 0.3078666666666667,
        "step": 2309
    },
    {
        "loss": 1.5711,
        "grad_norm": 3.5418291091918945,
        "learning_rate": 0.00011335501651401754,
        "epoch": 0.308,
        "step": 2310
    },
    {
        "loss": 2.4214,
        "grad_norm": 3.4968628883361816,
        "learning_rate": 0.00011329264090946825,
        "epoch": 0.3081333333333333,
        "step": 2311
    },
    {
        "loss": 2.8056,
        "grad_norm": 1.9209389686584473,
        "learning_rate": 0.0001132302600396586,
        "epoch": 0.3082666666666667,
        "step": 2312
    },
    {
        "loss": 2.4013,
        "grad_norm": 3.158050298690796,
        "learning_rate": 0.00011316787392929786,
        "epoch": 0.3084,
        "step": 2313
    },
    {
        "loss": 2.4654,
        "grad_norm": 3.495842695236206,
        "learning_rate": 0.00011310548260309739,
        "epoch": 0.3085333333333333,
        "step": 2314
    },
    {
        "loss": 2.5396,
        "grad_norm": 5.440101146697998,
        "learning_rate": 0.00011304308608577059,
        "epoch": 0.30866666666666664,
        "step": 2315
    },
    {
        "loss": 2.0869,
        "grad_norm": 3.076706886291504,
        "learning_rate": 0.00011298068440203294,
        "epoch": 0.3088,
        "step": 2316
    },
    {
        "loss": 2.1689,
        "grad_norm": 5.426510810852051,
        "learning_rate": 0.00011291827757660201,
        "epoch": 0.30893333333333334,
        "step": 2317
    },
    {
        "loss": 2.6218,
        "grad_norm": 3.0732007026672363,
        "learning_rate": 0.00011285586563419727,
        "epoch": 0.30906666666666666,
        "step": 2318
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.9234540462493896,
        "learning_rate": 0.0001127934485995404,
        "epoch": 0.3092,
        "step": 2319
    },
    {
        "loss": 2.3723,
        "grad_norm": 3.8062191009521484,
        "learning_rate": 0.0001127310264973549,
        "epoch": 0.30933333333333335,
        "step": 2320
    },
    {
        "loss": 2.4408,
        "grad_norm": 3.578913688659668,
        "learning_rate": 0.00011266859935236647,
        "epoch": 0.30946666666666667,
        "step": 2321
    },
    {
        "loss": 1.6001,
        "grad_norm": 3.652940511703491,
        "learning_rate": 0.00011260616718930263,
        "epoch": 0.3096,
        "step": 2322
    },
    {
        "loss": 2.4439,
        "grad_norm": 2.9418275356292725,
        "learning_rate": 0.000112543730032893,
        "epoch": 0.30973333333333336,
        "step": 2323
    },
    {
        "loss": 1.8,
        "grad_norm": 3.648500919342041,
        "learning_rate": 0.0001124812879078692,
        "epoch": 0.3098666666666667,
        "step": 2324
    },
    {
        "loss": 2.4016,
        "grad_norm": 3.090900182723999,
        "learning_rate": 0.00011241884083896472,
        "epoch": 0.31,
        "step": 2325
    },
    {
        "loss": 1.614,
        "grad_norm": 2.5250489711761475,
        "learning_rate": 0.00011235638885091506,
        "epoch": 0.3101333333333333,
        "step": 2326
    },
    {
        "loss": 2.5171,
        "grad_norm": 3.7017312049865723,
        "learning_rate": 0.0001122939319684577,
        "epoch": 0.3102666666666667,
        "step": 2327
    },
    {
        "loss": 2.3644,
        "grad_norm": 2.6464080810546875,
        "learning_rate": 0.00011223147021633194,
        "epoch": 0.3104,
        "step": 2328
    },
    {
        "loss": 2.7627,
        "grad_norm": 4.02199125289917,
        "learning_rate": 0.0001121690036192792,
        "epoch": 0.31053333333333333,
        "step": 2329
    },
    {
        "loss": 3.0695,
        "grad_norm": 3.067704677581787,
        "learning_rate": 0.00011210653220204266,
        "epoch": 0.31066666666666665,
        "step": 2330
    },
    {
        "loss": 1.7264,
        "grad_norm": 5.193386554718018,
        "learning_rate": 0.00011204405598936743,
        "epoch": 0.3108,
        "step": 2331
    },
    {
        "loss": 2.7697,
        "grad_norm": 2.0713605880737305,
        "learning_rate": 0.00011198157500600062,
        "epoch": 0.31093333333333334,
        "step": 2332
    },
    {
        "loss": 2.1047,
        "grad_norm": 5.110881805419922,
        "learning_rate": 0.0001119190892766911,
        "epoch": 0.31106666666666666,
        "step": 2333
    },
    {
        "loss": 2.8135,
        "grad_norm": 2.4033334255218506,
        "learning_rate": 0.0001118565988261897,
        "epoch": 0.3112,
        "step": 2334
    },
    {
        "loss": 2.4243,
        "grad_norm": 3.3793256282806396,
        "learning_rate": 0.00011179410367924911,
        "epoch": 0.31133333333333335,
        "step": 2335
    },
    {
        "loss": 2.9602,
        "grad_norm": 6.806596279144287,
        "learning_rate": 0.00011173160386062385,
        "epoch": 0.31146666666666667,
        "step": 2336
    },
    {
        "loss": 2.3266,
        "grad_norm": 4.967343807220459,
        "learning_rate": 0.00011166909939507036,
        "epoch": 0.3116,
        "step": 2337
    },
    {
        "loss": 2.8046,
        "grad_norm": 2.4430480003356934,
        "learning_rate": 0.00011160659030734682,
        "epoch": 0.3117333333333333,
        "step": 2338
    },
    {
        "loss": 2.8317,
        "grad_norm": 3.2656946182250977,
        "learning_rate": 0.00011154407662221331,
        "epoch": 0.3118666666666667,
        "step": 2339
    },
    {
        "loss": 2.2711,
        "grad_norm": 3.6346211433410645,
        "learning_rate": 0.00011148155836443173,
        "epoch": 0.312,
        "step": 2340
    },
    {
        "loss": 2.1092,
        "grad_norm": 3.3528287410736084,
        "learning_rate": 0.00011141903555876574,
        "epoch": 0.3121333333333333,
        "step": 2341
    },
    {
        "loss": 2.3471,
        "grad_norm": 3.014188528060913,
        "learning_rate": 0.00011135650822998086,
        "epoch": 0.3122666666666667,
        "step": 2342
    },
    {
        "loss": 2.4428,
        "grad_norm": 2.743262767791748,
        "learning_rate": 0.00011129397640284441,
        "epoch": 0.3124,
        "step": 2343
    },
    {
        "loss": 1.4131,
        "grad_norm": 3.7611260414123535,
        "learning_rate": 0.00011123144010212536,
        "epoch": 0.31253333333333333,
        "step": 2344
    },
    {
        "loss": 1.0296,
        "grad_norm": 3.9520163536071777,
        "learning_rate": 0.00011116889935259462,
        "epoch": 0.31266666666666665,
        "step": 2345
    },
    {
        "loss": 2.1633,
        "grad_norm": 4.084494113922119,
        "learning_rate": 0.00011110635417902486,
        "epoch": 0.3128,
        "step": 2346
    },
    {
        "loss": 2.7383,
        "grad_norm": 3.456141948699951,
        "learning_rate": 0.00011104380460619032,
        "epoch": 0.31293333333333334,
        "step": 2347
    },
    {
        "loss": 1.9464,
        "grad_norm": 4.08741569519043,
        "learning_rate": 0.00011098125065886713,
        "epoch": 0.31306666666666666,
        "step": 2348
    },
    {
        "loss": 1.0165,
        "grad_norm": 3.6731560230255127,
        "learning_rate": 0.00011091869236183316,
        "epoch": 0.3132,
        "step": 2349
    },
    {
        "loss": 1.9605,
        "grad_norm": 4.048110485076904,
        "learning_rate": 0.00011085612973986795,
        "epoch": 0.31333333333333335,
        "step": 2350
    },
    {
        "loss": 1.7732,
        "grad_norm": 4.261009216308594,
        "learning_rate": 0.00011079356281775272,
        "epoch": 0.31346666666666667,
        "step": 2351
    },
    {
        "loss": 2.0805,
        "grad_norm": 3.9463303089141846,
        "learning_rate": 0.00011073099162027052,
        "epoch": 0.3136,
        "step": 2352
    },
    {
        "loss": 2.6342,
        "grad_norm": 3.7045066356658936,
        "learning_rate": 0.00011066841617220594,
        "epoch": 0.3137333333333333,
        "step": 2353
    },
    {
        "loss": 2.3164,
        "grad_norm": 3.0023839473724365,
        "learning_rate": 0.00011060583649834537,
        "epoch": 0.3138666666666667,
        "step": 2354
    },
    {
        "loss": 2.5252,
        "grad_norm": 2.8888463973999023,
        "learning_rate": 0.00011054325262347683,
        "epoch": 0.314,
        "step": 2355
    },
    {
        "loss": 2.7504,
        "grad_norm": 3.1926958560943604,
        "learning_rate": 0.00011048066457239,
        "epoch": 0.3141333333333333,
        "step": 2356
    },
    {
        "loss": 1.9238,
        "grad_norm": 2.585136651992798,
        "learning_rate": 0.00011041807236987623,
        "epoch": 0.3142666666666667,
        "step": 2357
    },
    {
        "loss": 1.0627,
        "grad_norm": 3.300985097885132,
        "learning_rate": 0.00011035547604072847,
        "epoch": 0.3144,
        "step": 2358
    },
    {
        "loss": 2.8232,
        "grad_norm": 3.1682074069976807,
        "learning_rate": 0.00011029287560974142,
        "epoch": 0.31453333333333333,
        "step": 2359
    },
    {
        "loss": 2.9838,
        "grad_norm": 2.459465742111206,
        "learning_rate": 0.00011023027110171122,
        "epoch": 0.31466666666666665,
        "step": 2360
    },
    {
        "loss": 2.8606,
        "grad_norm": 3.302395820617676,
        "learning_rate": 0.00011016766254143578,
        "epoch": 0.3148,
        "step": 2361
    },
    {
        "loss": 2.5929,
        "grad_norm": 2.262310743331909,
        "learning_rate": 0.00011010504995371458,
        "epoch": 0.31493333333333334,
        "step": 2362
    },
    {
        "loss": 2.6993,
        "grad_norm": 3.1102516651153564,
        "learning_rate": 0.00011004243336334865,
        "epoch": 0.31506666666666666,
        "step": 2363
    },
    {
        "loss": 2.5837,
        "grad_norm": 2.8796825408935547,
        "learning_rate": 0.00010997981279514067,
        "epoch": 0.3152,
        "step": 2364
    },
    {
        "loss": 2.411,
        "grad_norm": 2.676666498184204,
        "learning_rate": 0.00010991718827389482,
        "epoch": 0.31533333333333335,
        "step": 2365
    },
    {
        "loss": 1.0226,
        "grad_norm": 2.894965171813965,
        "learning_rate": 0.00010985455982441688,
        "epoch": 0.3154666666666667,
        "step": 2366
    },
    {
        "loss": 2.7884,
        "grad_norm": 2.5868709087371826,
        "learning_rate": 0.00010979192747151423,
        "epoch": 0.3156,
        "step": 2367
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.195206880569458,
        "learning_rate": 0.00010972929123999578,
        "epoch": 0.3157333333333333,
        "step": 2368
    },
    {
        "loss": 1.785,
        "grad_norm": 3.4751107692718506,
        "learning_rate": 0.00010966665115467188,
        "epoch": 0.3158666666666667,
        "step": 2369
    },
    {
        "loss": 1.6833,
        "grad_norm": 5.0786519050598145,
        "learning_rate": 0.00010960400724035453,
        "epoch": 0.316,
        "step": 2370
    },
    {
        "loss": 3.069,
        "grad_norm": 2.0777688026428223,
        "learning_rate": 0.00010954135952185718,
        "epoch": 0.3161333333333333,
        "step": 2371
    },
    {
        "loss": 1.4944,
        "grad_norm": 3.783219575881958,
        "learning_rate": 0.00010947870802399482,
        "epoch": 0.31626666666666664,
        "step": 2372
    },
    {
        "loss": 2.3581,
        "grad_norm": 1.5636975765228271,
        "learning_rate": 0.0001094160527715839,
        "epoch": 0.3164,
        "step": 2373
    },
    {
        "loss": 2.6827,
        "grad_norm": 2.105576515197754,
        "learning_rate": 0.0001093533937894424,
        "epoch": 0.31653333333333333,
        "step": 2374
    },
    {
        "loss": 2.1862,
        "grad_norm": 3.573612928390503,
        "learning_rate": 0.00010929073110238975,
        "epoch": 0.31666666666666665,
        "step": 2375
    },
    {
        "loss": 2.2271,
        "grad_norm": 4.3140549659729,
        "learning_rate": 0.00010922806473524683,
        "epoch": 0.3168,
        "step": 2376
    },
    {
        "loss": 3.0257,
        "grad_norm": 2.8402721881866455,
        "learning_rate": 0.00010916539471283607,
        "epoch": 0.31693333333333334,
        "step": 2377
    },
    {
        "loss": 3.0265,
        "grad_norm": 2.487173080444336,
        "learning_rate": 0.0001091027210599812,
        "epoch": 0.31706666666666666,
        "step": 2378
    },
    {
        "loss": 3.0293,
        "grad_norm": 3.339637041091919,
        "learning_rate": 0.00010904004380150749,
        "epoch": 0.3172,
        "step": 2379
    },
    {
        "loss": 2.8021,
        "grad_norm": 2.741853952407837,
        "learning_rate": 0.00010897736296224165,
        "epoch": 0.31733333333333336,
        "step": 2380
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.252936601638794,
        "learning_rate": 0.00010891467856701177,
        "epoch": 0.3174666666666667,
        "step": 2381
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.568674325942993,
        "learning_rate": 0.00010885199064064733,
        "epoch": 0.3176,
        "step": 2382
    },
    {
        "loss": 2.6295,
        "grad_norm": 4.185904502868652,
        "learning_rate": 0.0001087892992079792,
        "epoch": 0.3177333333333333,
        "step": 2383
    },
    {
        "loss": 2.1051,
        "grad_norm": 3.0323822498321533,
        "learning_rate": 0.00010872660429383974,
        "epoch": 0.3178666666666667,
        "step": 2384
    },
    {
        "loss": 2.8114,
        "grad_norm": 2.3765201568603516,
        "learning_rate": 0.00010866390592306257,
        "epoch": 0.318,
        "step": 2385
    },
    {
        "loss": 2.4588,
        "grad_norm": 2.2521095275878906,
        "learning_rate": 0.00010860120412048274,
        "epoch": 0.3181333333333333,
        "step": 2386
    },
    {
        "loss": 2.4367,
        "grad_norm": 2.7482967376708984,
        "learning_rate": 0.00010853849891093664,
        "epoch": 0.31826666666666664,
        "step": 2387
    },
    {
        "loss": 2.3157,
        "grad_norm": 3.560464382171631,
        "learning_rate": 0.00010847579031926205,
        "epoch": 0.3184,
        "step": 2388
    },
    {
        "loss": 1.9181,
        "grad_norm": 3.3607544898986816,
        "learning_rate": 0.00010841307837029801,
        "epoch": 0.31853333333333333,
        "step": 2389
    },
    {
        "loss": 2.7585,
        "grad_norm": 2.326411247253418,
        "learning_rate": 0.000108350363088885,
        "epoch": 0.31866666666666665,
        "step": 2390
    },
    {
        "loss": 2.5798,
        "grad_norm": 2.828948736190796,
        "learning_rate": 0.00010828764449986467,
        "epoch": 0.3188,
        "step": 2391
    },
    {
        "loss": 1.1147,
        "grad_norm": 3.746096611022949,
        "learning_rate": 0.00010822492262808012,
        "epoch": 0.31893333333333335,
        "step": 2392
    },
    {
        "loss": 1.5095,
        "grad_norm": 2.4585890769958496,
        "learning_rate": 0.00010816219749837575,
        "epoch": 0.31906666666666667,
        "step": 2393
    },
    {
        "loss": 2.5278,
        "grad_norm": 3.237391710281372,
        "learning_rate": 0.00010809946913559708,
        "epoch": 0.3192,
        "step": 2394
    },
    {
        "loss": 1.4621,
        "grad_norm": 3.982285737991333,
        "learning_rate": 0.0001080367375645911,
        "epoch": 0.31933333333333336,
        "step": 2395
    },
    {
        "loss": 1.8993,
        "grad_norm": 2.5200839042663574,
        "learning_rate": 0.00010797400281020596,
        "epoch": 0.3194666666666667,
        "step": 2396
    },
    {
        "loss": 2.3858,
        "grad_norm": 2.841815710067749,
        "learning_rate": 0.00010791126489729117,
        "epoch": 0.3196,
        "step": 2397
    },
    {
        "loss": 1.6351,
        "grad_norm": 1.7483378648757935,
        "learning_rate": 0.00010784852385069739,
        "epoch": 0.3197333333333333,
        "step": 2398
    },
    {
        "loss": 2.4451,
        "grad_norm": 3.429391860961914,
        "learning_rate": 0.00010778577969527656,
        "epoch": 0.3198666666666667,
        "step": 2399
    },
    {
        "loss": 1.0725,
        "grad_norm": 3.968940258026123,
        "learning_rate": 0.0001077230324558819,
        "epoch": 0.32,
        "step": 2400
    },
    {
        "loss": 1.866,
        "grad_norm": 3.4240026473999023,
        "learning_rate": 0.00010766028215736774,
        "epoch": 0.3201333333333333,
        "step": 2401
    },
    {
        "loss": 2.1722,
        "grad_norm": 3.3681020736694336,
        "learning_rate": 0.00010759752882458972,
        "epoch": 0.32026666666666664,
        "step": 2402
    },
    {
        "loss": 2.8561,
        "grad_norm": 3.572803020477295,
        "learning_rate": 0.00010753477248240464,
        "epoch": 0.3204,
        "step": 2403
    },
    {
        "loss": 2.0366,
        "grad_norm": 2.4958789348602295,
        "learning_rate": 0.00010747201315567049,
        "epoch": 0.32053333333333334,
        "step": 2404
    },
    {
        "loss": 2.9951,
        "grad_norm": 2.961827278137207,
        "learning_rate": 0.00010740925086924649,
        "epoch": 0.32066666666666666,
        "step": 2405
    },
    {
        "loss": 2.5084,
        "grad_norm": 3.946397304534912,
        "learning_rate": 0.00010734648564799301,
        "epoch": 0.3208,
        "step": 2406
    },
    {
        "loss": 1.8245,
        "grad_norm": 4.899081707000732,
        "learning_rate": 0.00010728371751677147,
        "epoch": 0.32093333333333335,
        "step": 2407
    },
    {
        "loss": 2.4014,
        "grad_norm": 3.613020420074463,
        "learning_rate": 0.00010722094650044461,
        "epoch": 0.32106666666666667,
        "step": 2408
    },
    {
        "loss": 2.8208,
        "grad_norm": 3.562978982925415,
        "learning_rate": 0.00010715817262387629,
        "epoch": 0.3212,
        "step": 2409
    },
    {
        "loss": 1.4331,
        "grad_norm": 4.8369364738464355,
        "learning_rate": 0.00010709539591193136,
        "epoch": 0.32133333333333336,
        "step": 2410
    },
    {
        "loss": 1.9998,
        "grad_norm": 3.559495687484741,
        "learning_rate": 0.00010703261638947592,
        "epoch": 0.3214666666666667,
        "step": 2411
    },
    {
        "loss": 0.8345,
        "grad_norm": 3.6311070919036865,
        "learning_rate": 0.00010696983408137719,
        "epoch": 0.3216,
        "step": 2412
    },
    {
        "loss": 1.3518,
        "grad_norm": 6.255344390869141,
        "learning_rate": 0.00010690704901250343,
        "epoch": 0.3217333333333333,
        "step": 2413
    },
    {
        "loss": 1.6041,
        "grad_norm": 3.2760634422302246,
        "learning_rate": 0.00010684426120772401,
        "epoch": 0.3218666666666667,
        "step": 2414
    },
    {
        "loss": 2.4579,
        "grad_norm": 3.219409704208374,
        "learning_rate": 0.00010678147069190942,
        "epoch": 0.322,
        "step": 2415
    },
    {
        "loss": 2.3468,
        "grad_norm": 2.032012462615967,
        "learning_rate": 0.00010671867748993116,
        "epoch": 0.3221333333333333,
        "step": 2416
    },
    {
        "loss": 2.691,
        "grad_norm": 2.8008413314819336,
        "learning_rate": 0.00010665588162666184,
        "epoch": 0.32226666666666665,
        "step": 2417
    },
    {
        "loss": 2.509,
        "grad_norm": 2.387293577194214,
        "learning_rate": 0.00010659308312697515,
        "epoch": 0.3224,
        "step": 2418
    },
    {
        "loss": 1.94,
        "grad_norm": 3.126796245574951,
        "learning_rate": 0.00010653028201574577,
        "epoch": 0.32253333333333334,
        "step": 2419
    },
    {
        "loss": 2.5527,
        "grad_norm": 3.458770513534546,
        "learning_rate": 0.00010646747831784945,
        "epoch": 0.32266666666666666,
        "step": 2420
    },
    {
        "loss": 2.5905,
        "grad_norm": 3.3219571113586426,
        "learning_rate": 0.0001064046720581629,
        "epoch": 0.3228,
        "step": 2421
    },
    {
        "loss": 2.5945,
        "grad_norm": 3.7834408283233643,
        "learning_rate": 0.00010634186326156396,
        "epoch": 0.32293333333333335,
        "step": 2422
    },
    {
        "loss": 2.223,
        "grad_norm": 4.098388195037842,
        "learning_rate": 0.00010627905195293135,
        "epoch": 0.32306666666666667,
        "step": 2423
    },
    {
        "loss": 2.627,
        "grad_norm": 1.9508428573608398,
        "learning_rate": 0.00010621623815714486,
        "epoch": 0.3232,
        "step": 2424
    },
    {
        "loss": 2.7194,
        "grad_norm": 2.7444260120391846,
        "learning_rate": 0.00010615342189908526,
        "epoch": 0.3233333333333333,
        "step": 2425
    },
    {
        "loss": 2.2147,
        "grad_norm": 7.979691505432129,
        "learning_rate": 0.00010609060320363428,
        "epoch": 0.3234666666666667,
        "step": 2426
    },
    {
        "loss": 1.9968,
        "grad_norm": 3.510889768600464,
        "learning_rate": 0.00010602778209567463,
        "epoch": 0.3236,
        "step": 2427
    },
    {
        "loss": 1.9725,
        "grad_norm": 2.993213176727295,
        "learning_rate": 0.00010596495860008995,
        "epoch": 0.3237333333333333,
        "step": 2428
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.661226987838745,
        "learning_rate": 0.00010590213274176481,
        "epoch": 0.3238666666666667,
        "step": 2429
    },
    {
        "loss": 2.5685,
        "grad_norm": 2.2962207794189453,
        "learning_rate": 0.00010583930454558481,
        "epoch": 0.324,
        "step": 2430
    },
    {
        "loss": 2.6255,
        "grad_norm": 2.541306257247925,
        "learning_rate": 0.0001057764740364364,
        "epoch": 0.32413333333333333,
        "step": 2431
    },
    {
        "loss": 1.9685,
        "grad_norm": 3.57517147064209,
        "learning_rate": 0.00010571364123920691,
        "epoch": 0.32426666666666665,
        "step": 2432
    },
    {
        "loss": 2.3779,
        "grad_norm": 3.4832963943481445,
        "learning_rate": 0.00010565080617878467,
        "epoch": 0.3244,
        "step": 2433
    },
    {
        "loss": 2.8847,
        "grad_norm": 3.2753918170928955,
        "learning_rate": 0.00010558796888005884,
        "epoch": 0.32453333333333334,
        "step": 2434
    },
    {
        "loss": 2.2176,
        "grad_norm": 3.3009486198425293,
        "learning_rate": 0.0001055251293679195,
        "epoch": 0.32466666666666666,
        "step": 2435
    },
    {
        "loss": 2.4891,
        "grad_norm": 2.818321943283081,
        "learning_rate": 0.00010546228766725757,
        "epoch": 0.3248,
        "step": 2436
    },
    {
        "loss": 1.7649,
        "grad_norm": 3.2500617504119873,
        "learning_rate": 0.00010539944380296489,
        "epoch": 0.32493333333333335,
        "step": 2437
    },
    {
        "loss": 2.233,
        "grad_norm": 2.7441275119781494,
        "learning_rate": 0.00010533659779993414,
        "epoch": 0.32506666666666667,
        "step": 2438
    },
    {
        "loss": 2.4543,
        "grad_norm": 2.831021785736084,
        "learning_rate": 0.0001052737496830588,
        "epoch": 0.3252,
        "step": 2439
    },
    {
        "loss": 1.7819,
        "grad_norm": 3.2702434062957764,
        "learning_rate": 0.00010521089947723326,
        "epoch": 0.3253333333333333,
        "step": 2440
    },
    {
        "loss": 1.2803,
        "grad_norm": 3.690333127975464,
        "learning_rate": 0.0001051480472073527,
        "epoch": 0.3254666666666667,
        "step": 2441
    },
    {
        "loss": 1.1819,
        "grad_norm": 4.6149444580078125,
        "learning_rate": 0.00010508519289831305,
        "epoch": 0.3256,
        "step": 2442
    },
    {
        "loss": 1.9624,
        "grad_norm": 3.2590110301971436,
        "learning_rate": 0.00010502233657501119,
        "epoch": 0.3257333333333333,
        "step": 2443
    },
    {
        "loss": 1.8375,
        "grad_norm": 3.3611481189727783,
        "learning_rate": 0.00010495947826234471,
        "epoch": 0.3258666666666667,
        "step": 2444
    },
    {
        "loss": 2.54,
        "grad_norm": 1.7016583681106567,
        "learning_rate": 0.00010489661798521197,
        "epoch": 0.326,
        "step": 2445
    },
    {
        "loss": 1.7749,
        "grad_norm": 3.729719400405884,
        "learning_rate": 0.00010483375576851215,
        "epoch": 0.32613333333333333,
        "step": 2446
    },
    {
        "loss": 2.4172,
        "grad_norm": 2.8045237064361572,
        "learning_rate": 0.00010477089163714524,
        "epoch": 0.32626666666666665,
        "step": 2447
    },
    {
        "loss": 2.5859,
        "grad_norm": 3.7902064323425293,
        "learning_rate": 0.00010470802561601189,
        "epoch": 0.3264,
        "step": 2448
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.5772461891174316,
        "learning_rate": 0.00010464515773001351,
        "epoch": 0.32653333333333334,
        "step": 2449
    },
    {
        "loss": 2.5204,
        "grad_norm": 2.977220296859741,
        "learning_rate": 0.00010458228800405237,
        "epoch": 0.32666666666666666,
        "step": 2450
    },
    {
        "loss": 2.4819,
        "grad_norm": 2.9565787315368652,
        "learning_rate": 0.00010451941646303132,
        "epoch": 0.3268,
        "step": 2451
    },
    {
        "loss": 1.7066,
        "grad_norm": 3.1935055255889893,
        "learning_rate": 0.00010445654313185402,
        "epoch": 0.32693333333333335,
        "step": 2452
    },
    {
        "loss": 2.3671,
        "grad_norm": 4.197956085205078,
        "learning_rate": 0.00010439366803542479,
        "epoch": 0.32706666666666667,
        "step": 2453
    },
    {
        "loss": 2.1586,
        "grad_norm": 3.155527114868164,
        "learning_rate": 0.00010433079119864866,
        "epoch": 0.3272,
        "step": 2454
    },
    {
        "loss": 2.2176,
        "grad_norm": 3.1924614906311035,
        "learning_rate": 0.0001042679126464314,
        "epoch": 0.3273333333333333,
        "step": 2455
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.0109989643096924,
        "learning_rate": 0.00010420503240367939,
        "epoch": 0.3274666666666667,
        "step": 2456
    },
    {
        "loss": 2.4007,
        "grad_norm": 2.517552137374878,
        "learning_rate": 0.00010414215049529971,
        "epoch": 0.3276,
        "step": 2457
    },
    {
        "loss": 1.9627,
        "grad_norm": 4.709094047546387,
        "learning_rate": 0.00010407926694620013,
        "epoch": 0.3277333333333333,
        "step": 2458
    },
    {
        "loss": 2.1988,
        "grad_norm": 2.2132883071899414,
        "learning_rate": 0.00010401638178128898,
        "epoch": 0.32786666666666664,
        "step": 2459
    },
    {
        "loss": 2.4577,
        "grad_norm": 2.8789796829223633,
        "learning_rate": 0.00010395349502547538,
        "epoch": 0.328,
        "step": 2460
    },
    {
        "loss": 2.7058,
        "grad_norm": 2.7636427879333496,
        "learning_rate": 0.00010389060670366891,
        "epoch": 0.32813333333333333,
        "step": 2461
    },
    {
        "loss": 2.6144,
        "grad_norm": 3.1296639442443848,
        "learning_rate": 0.0001038277168407798,
        "epoch": 0.32826666666666665,
        "step": 2462
    },
    {
        "loss": 3.0827,
        "grad_norm": 2.206184148788452,
        "learning_rate": 0.00010376482546171908,
        "epoch": 0.3284,
        "step": 2463
    },
    {
        "loss": 1.7964,
        "grad_norm": 3.6859991550445557,
        "learning_rate": 0.00010370193259139815,
        "epoch": 0.32853333333333334,
        "step": 2464
    },
    {
        "loss": 2.0803,
        "grad_norm": 3.095933198928833,
        "learning_rate": 0.0001036390382547291,
        "epoch": 0.32866666666666666,
        "step": 2465
    },
    {
        "loss": 2.1664,
        "grad_norm": 3.546340227127075,
        "learning_rate": 0.0001035761424766246,
        "epoch": 0.3288,
        "step": 2466
    },
    {
        "loss": 2.6381,
        "grad_norm": 3.260594367980957,
        "learning_rate": 0.00010351324528199786,
        "epoch": 0.32893333333333336,
        "step": 2467
    },
    {
        "loss": 1.61,
        "grad_norm": 3.0559282302856445,
        "learning_rate": 0.00010345034669576272,
        "epoch": 0.3290666666666667,
        "step": 2468
    },
    {
        "loss": 2.6653,
        "grad_norm": 3.274066686630249,
        "learning_rate": 0.00010338744674283351,
        "epoch": 0.3292,
        "step": 2469
    },
    {
        "loss": 2.7796,
        "grad_norm": 3.170522689819336,
        "learning_rate": 0.00010332454544812506,
        "epoch": 0.3293333333333333,
        "step": 2470
    },
    {
        "loss": 2.3925,
        "grad_norm": 2.082096576690674,
        "learning_rate": 0.00010326164283655284,
        "epoch": 0.3294666666666667,
        "step": 2471
    },
    {
        "loss": 2.5957,
        "grad_norm": 2.4695520401000977,
        "learning_rate": 0.0001031987389330328,
        "epoch": 0.3296,
        "step": 2472
    },
    {
        "loss": 2.0599,
        "grad_norm": 3.512434244155884,
        "learning_rate": 0.00010313583376248137,
        "epoch": 0.3297333333333333,
        "step": 2473
    },
    {
        "loss": 2.001,
        "grad_norm": 4.013906955718994,
        "learning_rate": 0.00010307292734981546,
        "epoch": 0.32986666666666664,
        "step": 2474
    },
    {
        "loss": 2.5102,
        "grad_norm": 4.101434707641602,
        "learning_rate": 0.00010301001971995259,
        "epoch": 0.33,
        "step": 2475
    },
    {
        "loss": 3.5619,
        "grad_norm": 3.583442449569702,
        "learning_rate": 0.00010294711089781064,
        "epoch": 0.33013333333333333,
        "step": 2476
    },
    {
        "loss": 2.0282,
        "grad_norm": 3.33313250541687,
        "learning_rate": 0.00010288420090830802,
        "epoch": 0.33026666666666665,
        "step": 2477
    },
    {
        "loss": 1.6711,
        "grad_norm": 3.274505615234375,
        "learning_rate": 0.0001028212897763636,
        "epoch": 0.3304,
        "step": 2478
    },
    {
        "loss": 1.7484,
        "grad_norm": 2.8731558322906494,
        "learning_rate": 0.0001027583775268967,
        "epoch": 0.33053333333333335,
        "step": 2479
    },
    {
        "loss": 1.6681,
        "grad_norm": 4.048531532287598,
        "learning_rate": 0.00010269546418482703,
        "epoch": 0.33066666666666666,
        "step": 2480
    },
    {
        "loss": 2.7366,
        "grad_norm": 2.939164638519287,
        "learning_rate": 0.00010263254977507483,
        "epoch": 0.3308,
        "step": 2481
    },
    {
        "loss": 2.6471,
        "grad_norm": 2.481816053390503,
        "learning_rate": 0.0001025696343225607,
        "epoch": 0.33093333333333336,
        "step": 2482
    },
    {
        "loss": 1.8411,
        "grad_norm": 3.797954797744751,
        "learning_rate": 0.00010250671785220567,
        "epoch": 0.3310666666666667,
        "step": 2483
    },
    {
        "loss": 2.7279,
        "grad_norm": 2.4436140060424805,
        "learning_rate": 0.00010244380038893114,
        "epoch": 0.3312,
        "step": 2484
    },
    {
        "loss": 1.6394,
        "grad_norm": 1.7003295421600342,
        "learning_rate": 0.00010238088195765896,
        "epoch": 0.3313333333333333,
        "step": 2485
    },
    {
        "loss": 2.3015,
        "grad_norm": 3.3176026344299316,
        "learning_rate": 0.00010231796258331132,
        "epoch": 0.3314666666666667,
        "step": 2486
    },
    {
        "loss": 1.874,
        "grad_norm": 3.361880302429199,
        "learning_rate": 0.00010225504229081078,
        "epoch": 0.3316,
        "step": 2487
    },
    {
        "loss": 2.364,
        "grad_norm": 3.6485164165496826,
        "learning_rate": 0.00010219212110508032,
        "epoch": 0.3317333333333333,
        "step": 2488
    },
    {
        "loss": 1.0821,
        "grad_norm": 3.3757898807525635,
        "learning_rate": 0.00010212919905104324,
        "epoch": 0.33186666666666664,
        "step": 2489
    },
    {
        "loss": 2.4456,
        "grad_norm": 2.4058725833892822,
        "learning_rate": 0.00010206627615362314,
        "epoch": 0.332,
        "step": 2490
    },
    {
        "loss": 2.3954,
        "grad_norm": 3.1682021617889404,
        "learning_rate": 0.00010200335243774402,
        "epoch": 0.33213333333333334,
        "step": 2491
    },
    {
        "loss": 2.4082,
        "grad_norm": 3.814918279647827,
        "learning_rate": 0.00010194042792833016,
        "epoch": 0.33226666666666665,
        "step": 2492
    },
    {
        "loss": 1.7074,
        "grad_norm": 4.795654773712158,
        "learning_rate": 0.0001018775026503062,
        "epoch": 0.3324,
        "step": 2493
    },
    {
        "loss": 2.302,
        "grad_norm": 3.2191567420959473,
        "learning_rate": 0.00010181457662859706,
        "epoch": 0.33253333333333335,
        "step": 2494
    },
    {
        "loss": 2.7738,
        "grad_norm": 2.64247465133667,
        "learning_rate": 0.00010175164988812791,
        "epoch": 0.33266666666666667,
        "step": 2495
    },
    {
        "loss": 2.3622,
        "grad_norm": 3.1758852005004883,
        "learning_rate": 0.00010168872245382424,
        "epoch": 0.3328,
        "step": 2496
    },
    {
        "loss": 2.0671,
        "grad_norm": 4.2906622886657715,
        "learning_rate": 0.00010162579435061188,
        "epoch": 0.33293333333333336,
        "step": 2497
    },
    {
        "loss": 2.2983,
        "grad_norm": 2.785433769226074,
        "learning_rate": 0.00010156286560341685,
        "epoch": 0.3330666666666667,
        "step": 2498
    },
    {
        "loss": 3.0339,
        "grad_norm": 2.0624825954437256,
        "learning_rate": 0.00010149993623716544,
        "epoch": 0.3332,
        "step": 2499
    },
    {
        "loss": 2.7743,
        "grad_norm": 2.4494924545288086,
        "learning_rate": 0.00010143700627678414,
        "epoch": 0.3333333333333333,
        "step": 2500
    },
    {
        "loss": 1.7758,
        "grad_norm": 4.5654096603393555,
        "learning_rate": 0.0001013740757471998,
        "epoch": 0.3334666666666667,
        "step": 2501
    },
    {
        "loss": 2.2882,
        "grad_norm": 3.8290975093841553,
        "learning_rate": 0.00010131114467333935,
        "epoch": 0.3336,
        "step": 2502
    },
    {
        "loss": 3.0551,
        "grad_norm": 3.2798709869384766,
        "learning_rate": 0.00010124821308013001,
        "epoch": 0.3337333333333333,
        "step": 2503
    },
    {
        "loss": 2.0602,
        "grad_norm": 3.250361204147339,
        "learning_rate": 0.00010118528099249926,
        "epoch": 0.33386666666666664,
        "step": 2504
    },
    {
        "loss": 2.9297,
        "grad_norm": 3.0379011631011963,
        "learning_rate": 0.00010112234843537463,
        "epoch": 0.334,
        "step": 2505
    },
    {
        "loss": 0.5359,
        "grad_norm": 2.48191499710083,
        "learning_rate": 0.00010105941543368398,
        "epoch": 0.33413333333333334,
        "step": 2506
    },
    {
        "loss": 2.1992,
        "grad_norm": 3.758239984512329,
        "learning_rate": 0.00010099648201235528,
        "epoch": 0.33426666666666666,
        "step": 2507
    },
    {
        "loss": 3.0206,
        "grad_norm": 4.0720319747924805,
        "learning_rate": 0.00010093354819631662,
        "epoch": 0.3344,
        "step": 2508
    },
    {
        "loss": 2.624,
        "grad_norm": 2.634573221206665,
        "learning_rate": 0.00010087061401049635,
        "epoch": 0.33453333333333335,
        "step": 2509
    },
    {
        "loss": 1.4859,
        "grad_norm": 4.298214912414551,
        "learning_rate": 0.00010080767947982292,
        "epoch": 0.33466666666666667,
        "step": 2510
    },
    {
        "loss": 2.3614,
        "grad_norm": 1.7646931409835815,
        "learning_rate": 0.00010074474462922492,
        "epoch": 0.3348,
        "step": 2511
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.1359362602233887,
        "learning_rate": 0.00010068180948363097,
        "epoch": 0.33493333333333336,
        "step": 2512
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.0877556800842285,
        "learning_rate": 0.00010061887406797001,
        "epoch": 0.3350666666666667,
        "step": 2513
    },
    {
        "loss": 2.9393,
        "grad_norm": 4.290462970733643,
        "learning_rate": 0.0001005559384071709,
        "epoch": 0.3352,
        "step": 2514
    },
    {
        "loss": 2.7781,
        "grad_norm": 2.267705202102661,
        "learning_rate": 0.00010049300252616271,
        "epoch": 0.3353333333333333,
        "step": 2515
    },
    {
        "loss": 2.293,
        "grad_norm": 3.2673959732055664,
        "learning_rate": 0.00010043006644987453,
        "epoch": 0.3354666666666667,
        "step": 2516
    },
    {
        "loss": 1.3135,
        "grad_norm": 2.888129711151123,
        "learning_rate": 0.00010036713020323556,
        "epoch": 0.3356,
        "step": 2517
    },
    {
        "loss": 1.7216,
        "grad_norm": 4.023637771606445,
        "learning_rate": 0.00010030419381117504,
        "epoch": 0.33573333333333333,
        "step": 2518
    },
    {
        "loss": 2.4614,
        "grad_norm": 3.0130763053894043,
        "learning_rate": 0.00010024125729862235,
        "epoch": 0.33586666666666665,
        "step": 2519
    },
    {
        "loss": 1.9241,
        "grad_norm": 3.1962411403656006,
        "learning_rate": 0.00010017832069050681,
        "epoch": 0.336,
        "step": 2520
    },
    {
        "loss": 2.0684,
        "grad_norm": 3.2951507568359375,
        "learning_rate": 0.00010011538401175786,
        "epoch": 0.33613333333333334,
        "step": 2521
    },
    {
        "loss": 2.6604,
        "grad_norm": 3.44325852394104,
        "learning_rate": 0.00010005244728730487,
        "epoch": 0.33626666666666666,
        "step": 2522
    },
    {
        "loss": 2.0243,
        "grad_norm": 1.9658576250076294,
        "learning_rate": 9.998951054207739e-05,
        "epoch": 0.3364,
        "step": 2523
    },
    {
        "loss": 2.1712,
        "grad_norm": 2.7431442737579346,
        "learning_rate": 9.992657380100478e-05,
        "epoch": 0.33653333333333335,
        "step": 2524
    },
    {
        "loss": 2.5347,
        "grad_norm": 3.716148853302002,
        "learning_rate": 9.986363708901653e-05,
        "epoch": 0.33666666666666667,
        "step": 2525
    },
    {
        "loss": 2.4857,
        "grad_norm": 4.3252739906311035,
        "learning_rate": 9.98007004310421e-05,
        "epoch": 0.3368,
        "step": 2526
    },
    {
        "loss": 1.7296,
        "grad_norm": 3.3243257999420166,
        "learning_rate": 9.973776385201092e-05,
        "epoch": 0.3369333333333333,
        "step": 2527
    },
    {
        "loss": 2.7533,
        "grad_norm": 3.323256731033325,
        "learning_rate": 9.967482737685237e-05,
        "epoch": 0.3370666666666667,
        "step": 2528
    },
    {
        "loss": 2.4557,
        "grad_norm": 4.2480573654174805,
        "learning_rate": 9.961189103049582e-05,
        "epoch": 0.3372,
        "step": 2529
    },
    {
        "loss": 2.7016,
        "grad_norm": 3.7004191875457764,
        "learning_rate": 9.95489548378705e-05,
        "epoch": 0.3373333333333333,
        "step": 2530
    },
    {
        "loss": 2.9899,
        "grad_norm": 4.272505283355713,
        "learning_rate": 9.948601882390575e-05,
        "epoch": 0.3374666666666667,
        "step": 2531
    },
    {
        "loss": 1.8167,
        "grad_norm": 2.3505022525787354,
        "learning_rate": 9.94230830135307e-05,
        "epoch": 0.3376,
        "step": 2532
    },
    {
        "loss": 2.8529,
        "grad_norm": 3.6086246967315674,
        "learning_rate": 9.936014743167438e-05,
        "epoch": 0.33773333333333333,
        "step": 2533
    },
    {
        "loss": 2.9176,
        "grad_norm": 3.0242767333984375,
        "learning_rate": 9.929721210326591e-05,
        "epoch": 0.33786666666666665,
        "step": 2534
    },
    {
        "loss": 2.4352,
        "grad_norm": 3.1078364849090576,
        "learning_rate": 9.923427705323409e-05,
        "epoch": 0.338,
        "step": 2535
    },
    {
        "loss": 2.1814,
        "grad_norm": 4.084417343139648,
        "learning_rate": 9.917134230650775e-05,
        "epoch": 0.33813333333333334,
        "step": 2536
    },
    {
        "loss": 2.0682,
        "grad_norm": 2.0265331268310547,
        "learning_rate": 9.910840788801552e-05,
        "epoch": 0.33826666666666666,
        "step": 2537
    },
    {
        "loss": 1.9081,
        "grad_norm": 3.962141275405884,
        "learning_rate": 9.904547382268599e-05,
        "epoch": 0.3384,
        "step": 2538
    },
    {
        "loss": 3.0625,
        "grad_norm": 3.3649041652679443,
        "learning_rate": 9.89825401354475e-05,
        "epoch": 0.33853333333333335,
        "step": 2539
    },
    {
        "loss": 3.0062,
        "grad_norm": 2.526933431625366,
        "learning_rate": 9.891960685122835e-05,
        "epoch": 0.33866666666666667,
        "step": 2540
    },
    {
        "loss": 2.4913,
        "grad_norm": 3.0052294731140137,
        "learning_rate": 9.885667399495661e-05,
        "epoch": 0.3388,
        "step": 2541
    },
    {
        "loss": 2.5692,
        "grad_norm": 3.8982858657836914,
        "learning_rate": 9.879374159156019e-05,
        "epoch": 0.3389333333333333,
        "step": 2542
    },
    {
        "loss": 2.6253,
        "grad_norm": 3.023024797439575,
        "learning_rate": 9.873080966596684e-05,
        "epoch": 0.3390666666666667,
        "step": 2543
    },
    {
        "loss": 1.6865,
        "grad_norm": 3.215985059738159,
        "learning_rate": 9.86678782431041e-05,
        "epoch": 0.3392,
        "step": 2544
    },
    {
        "loss": 2.2777,
        "grad_norm": 3.268395185470581,
        "learning_rate": 9.860494734789937e-05,
        "epoch": 0.3393333333333333,
        "step": 2545
    },
    {
        "loss": 2.7768,
        "grad_norm": 2.8181509971618652,
        "learning_rate": 9.854201700527972e-05,
        "epoch": 0.3394666666666667,
        "step": 2546
    },
    {
        "loss": 2.1079,
        "grad_norm": 4.905063629150391,
        "learning_rate": 9.847908724017205e-05,
        "epoch": 0.3396,
        "step": 2547
    },
    {
        "loss": 1.974,
        "grad_norm": 3.903326988220215,
        "learning_rate": 9.841615807750317e-05,
        "epoch": 0.33973333333333333,
        "step": 2548
    },
    {
        "loss": 1.7812,
        "grad_norm": 4.023888111114502,
        "learning_rate": 9.835322954219947e-05,
        "epoch": 0.33986666666666665,
        "step": 2549
    },
    {
        "loss": 2.8266,
        "grad_norm": 2.0830564498901367,
        "learning_rate": 9.829030165918713e-05,
        "epoch": 0.34,
        "step": 2550
    },
    {
        "loss": 2.4059,
        "grad_norm": 5.811280250549316,
        "learning_rate": 9.822737445339213e-05,
        "epoch": 0.34013333333333334,
        "step": 2551
    },
    {
        "loss": 2.5569,
        "grad_norm": 3.6543173789978027,
        "learning_rate": 9.816444794974017e-05,
        "epoch": 0.34026666666666666,
        "step": 2552
    },
    {
        "loss": 2.8157,
        "grad_norm": 3.2428500652313232,
        "learning_rate": 9.810152217315662e-05,
        "epoch": 0.3404,
        "step": 2553
    },
    {
        "loss": 2.5255,
        "grad_norm": 2.2103147506713867,
        "learning_rate": 9.803859714856664e-05,
        "epoch": 0.34053333333333335,
        "step": 2554
    },
    {
        "loss": 2.6479,
        "grad_norm": 3.8008229732513428,
        "learning_rate": 9.797567290089494e-05,
        "epoch": 0.3406666666666667,
        "step": 2555
    },
    {
        "loss": 3.2403,
        "grad_norm": 3.3335745334625244,
        "learning_rate": 9.791274945506614e-05,
        "epoch": 0.3408,
        "step": 2556
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.6971852779388428,
        "learning_rate": 9.784982683600439e-05,
        "epoch": 0.3409333333333333,
        "step": 2557
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.9329447746276855,
        "learning_rate": 9.778690506863357e-05,
        "epoch": 0.3410666666666667,
        "step": 2558
    },
    {
        "loss": 2.8314,
        "grad_norm": 3.180654287338257,
        "learning_rate": 9.772398417787716e-05,
        "epoch": 0.3412,
        "step": 2559
    },
    {
        "loss": 2.5013,
        "grad_norm": 3.217895030975342,
        "learning_rate": 9.76610641886584e-05,
        "epoch": 0.3413333333333333,
        "step": 2560
    },
    {
        "loss": 2.9524,
        "grad_norm": 2.5009608268737793,
        "learning_rate": 9.75981451259001e-05,
        "epoch": 0.34146666666666664,
        "step": 2561
    },
    {
        "loss": 1.6606,
        "grad_norm": 2.0147669315338135,
        "learning_rate": 9.75352270145247e-05,
        "epoch": 0.3416,
        "step": 2562
    },
    {
        "loss": 2.464,
        "grad_norm": 2.704378843307495,
        "learning_rate": 9.747230987945425e-05,
        "epoch": 0.34173333333333333,
        "step": 2563
    },
    {
        "loss": 2.5549,
        "grad_norm": 2.402047872543335,
        "learning_rate": 9.740939374561047e-05,
        "epoch": 0.34186666666666665,
        "step": 2564
    },
    {
        "loss": 2.6839,
        "grad_norm": 2.770423650741577,
        "learning_rate": 9.734647863791468e-05,
        "epoch": 0.342,
        "step": 2565
    },
    {
        "loss": 2.4812,
        "grad_norm": 2.887186050415039,
        "learning_rate": 9.728356458128774e-05,
        "epoch": 0.34213333333333334,
        "step": 2566
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.8422315120697021,
        "learning_rate": 9.722065160065009e-05,
        "epoch": 0.34226666666666666,
        "step": 2567
    },
    {
        "loss": 2.5812,
        "grad_norm": 2.4699270725250244,
        "learning_rate": 9.715773972092184e-05,
        "epoch": 0.3424,
        "step": 2568
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.990605354309082,
        "learning_rate": 9.709482896702256e-05,
        "epoch": 0.34253333333333336,
        "step": 2569
    },
    {
        "loss": 2.0262,
        "grad_norm": 4.4681501388549805,
        "learning_rate": 9.703191936387146e-05,
        "epoch": 0.3426666666666667,
        "step": 2570
    },
    {
        "loss": 0.9297,
        "grad_norm": 2.8210651874542236,
        "learning_rate": 9.69690109363872e-05,
        "epoch": 0.3428,
        "step": 2571
    },
    {
        "loss": 2.7323,
        "grad_norm": 2.6220617294311523,
        "learning_rate": 9.690610370948799e-05,
        "epoch": 0.3429333333333333,
        "step": 2572
    },
    {
        "loss": 2.5347,
        "grad_norm": 3.8311002254486084,
        "learning_rate": 9.684319770809169e-05,
        "epoch": 0.3430666666666667,
        "step": 2573
    },
    {
        "loss": 2.161,
        "grad_norm": 2.720900535583496,
        "learning_rate": 9.678029295711552e-05,
        "epoch": 0.3432,
        "step": 2574
    },
    {
        "loss": 2.1147,
        "grad_norm": 3.156236171722412,
        "learning_rate": 9.671738948147625e-05,
        "epoch": 0.3433333333333333,
        "step": 2575
    },
    {
        "loss": 2.8234,
        "grad_norm": 2.5468780994415283,
        "learning_rate": 9.665448730609023e-05,
        "epoch": 0.34346666666666664,
        "step": 2576
    },
    {
        "loss": 2.1607,
        "grad_norm": 3.3626582622528076,
        "learning_rate": 9.659158645587316e-05,
        "epoch": 0.3436,
        "step": 2577
    },
    {
        "loss": 2.1667,
        "grad_norm": 3.439502239227295,
        "learning_rate": 9.652868695574033e-05,
        "epoch": 0.34373333333333334,
        "step": 2578
    },
    {
        "loss": 3.2198,
        "grad_norm": 6.612525939941406,
        "learning_rate": 9.646578883060643e-05,
        "epoch": 0.34386666666666665,
        "step": 2579
    },
    {
        "loss": 2.2116,
        "grad_norm": 3.105451822280884,
        "learning_rate": 9.64028921053856e-05,
        "epoch": 0.344,
        "step": 2580
    },
    {
        "loss": 2.2774,
        "grad_norm": 3.5110368728637695,
        "learning_rate": 9.633999680499147e-05,
        "epoch": 0.34413333333333335,
        "step": 2581
    },
    {
        "loss": 2.672,
        "grad_norm": 2.024212121963501,
        "learning_rate": 9.627710295433707e-05,
        "epoch": 0.34426666666666667,
        "step": 2582
    },
    {
        "loss": 2.1146,
        "grad_norm": 2.6129403114318848,
        "learning_rate": 9.621421057833489e-05,
        "epoch": 0.3444,
        "step": 2583
    },
    {
        "loss": 1.9955,
        "grad_norm": 3.5849814414978027,
        "learning_rate": 9.615131970189679e-05,
        "epoch": 0.34453333333333336,
        "step": 2584
    },
    {
        "loss": 2.2714,
        "grad_norm": 3.824967622756958,
        "learning_rate": 9.608843034993407e-05,
        "epoch": 0.3446666666666667,
        "step": 2585
    },
    {
        "loss": 2.5957,
        "grad_norm": 3.1331701278686523,
        "learning_rate": 9.602554254735745e-05,
        "epoch": 0.3448,
        "step": 2586
    },
    {
        "loss": 0.7414,
        "grad_norm": 2.7524168491363525,
        "learning_rate": 9.596265631907694e-05,
        "epoch": 0.3449333333333333,
        "step": 2587
    },
    {
        "loss": 2.5422,
        "grad_norm": 2.237440586090088,
        "learning_rate": 9.589977169000203e-05,
        "epoch": 0.3450666666666667,
        "step": 2588
    },
    {
        "loss": 2.9961,
        "grad_norm": 2.940200090408325,
        "learning_rate": 9.58368886850415e-05,
        "epoch": 0.3452,
        "step": 2589
    },
    {
        "loss": 2.926,
        "grad_norm": 2.8585007190704346,
        "learning_rate": 9.577400732910356e-05,
        "epoch": 0.3453333333333333,
        "step": 2590
    },
    {
        "loss": 2.8981,
        "grad_norm": 2.2724037170410156,
        "learning_rate": 9.571112764709573e-05,
        "epoch": 0.34546666666666664,
        "step": 2591
    },
    {
        "loss": 2.5277,
        "grad_norm": 3.300393581390381,
        "learning_rate": 9.564824966392482e-05,
        "epoch": 0.3456,
        "step": 2592
    },
    {
        "loss": 1.8409,
        "grad_norm": 2.7104506492614746,
        "learning_rate": 9.558537340449706e-05,
        "epoch": 0.34573333333333334,
        "step": 2593
    },
    {
        "loss": 2.305,
        "grad_norm": 2.438371181488037,
        "learning_rate": 9.552249889371795e-05,
        "epoch": 0.34586666666666666,
        "step": 2594
    },
    {
        "loss": 2.1246,
        "grad_norm": 3.9559319019317627,
        "learning_rate": 9.54596261564923e-05,
        "epoch": 0.346,
        "step": 2595
    },
    {
        "loss": 2.3375,
        "grad_norm": 2.979032039642334,
        "learning_rate": 9.539675521772419e-05,
        "epoch": 0.34613333333333335,
        "step": 2596
    },
    {
        "loss": 2.5325,
        "grad_norm": 4.71612024307251,
        "learning_rate": 9.5333886102317e-05,
        "epoch": 0.34626666666666667,
        "step": 2597
    },
    {
        "loss": 2.6991,
        "grad_norm": 2.1234042644500732,
        "learning_rate": 9.52710188351734e-05,
        "epoch": 0.3464,
        "step": 2598
    },
    {
        "loss": 1.5505,
        "grad_norm": 2.7591631412506104,
        "learning_rate": 9.520815344119542e-05,
        "epoch": 0.34653333333333336,
        "step": 2599
    },
    {
        "loss": 1.738,
        "grad_norm": 3.609269142150879,
        "learning_rate": 9.514528994528416e-05,
        "epoch": 0.3466666666666667,
        "step": 2600
    },
    {
        "loss": 0.902,
        "grad_norm": 3.683533191680908,
        "learning_rate": 9.508242837234005e-05,
        "epoch": 0.3468,
        "step": 2601
    },
    {
        "loss": 2.6997,
        "grad_norm": 2.0586657524108887,
        "learning_rate": 9.501956874726289e-05,
        "epoch": 0.3469333333333333,
        "step": 2602
    },
    {
        "loss": 2.3969,
        "grad_norm": 4.208935260772705,
        "learning_rate": 9.49567110949515e-05,
        "epoch": 0.3470666666666667,
        "step": 2603
    },
    {
        "loss": 2.1975,
        "grad_norm": 3.4227943420410156,
        "learning_rate": 9.489385544030403e-05,
        "epoch": 0.3472,
        "step": 2604
    },
    {
        "loss": 2.323,
        "grad_norm": 3.104189395904541,
        "learning_rate": 9.48310018082178e-05,
        "epoch": 0.3473333333333333,
        "step": 2605
    },
    {
        "loss": 2.1684,
        "grad_norm": 3.3518857955932617,
        "learning_rate": 9.476815022358937e-05,
        "epoch": 0.34746666666666665,
        "step": 2606
    },
    {
        "loss": 2.8866,
        "grad_norm": 3.0353338718414307,
        "learning_rate": 9.470530071131447e-05,
        "epoch": 0.3476,
        "step": 2607
    },
    {
        "loss": 2.5686,
        "grad_norm": 2.664499044418335,
        "learning_rate": 9.464245329628803e-05,
        "epoch": 0.34773333333333334,
        "step": 2608
    },
    {
        "loss": 2.179,
        "grad_norm": 2.834557056427002,
        "learning_rate": 9.457960800340402e-05,
        "epoch": 0.34786666666666666,
        "step": 2609
    },
    {
        "loss": 2.0545,
        "grad_norm": 5.961121082305908,
        "learning_rate": 9.45167648575558e-05,
        "epoch": 0.348,
        "step": 2610
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.1799988746643066,
        "learning_rate": 9.445392388363573e-05,
        "epoch": 0.34813333333333335,
        "step": 2611
    },
    {
        "loss": 1.473,
        "grad_norm": 3.4521734714508057,
        "learning_rate": 9.439108510653529e-05,
        "epoch": 0.34826666666666667,
        "step": 2612
    },
    {
        "loss": 1.6204,
        "grad_norm": 4.049436092376709,
        "learning_rate": 9.432824855114513e-05,
        "epoch": 0.3484,
        "step": 2613
    },
    {
        "loss": 2.6932,
        "grad_norm": 4.78031063079834,
        "learning_rate": 9.426541424235504e-05,
        "epoch": 0.3485333333333333,
        "step": 2614
    },
    {
        "loss": 2.8381,
        "grad_norm": 3.3942110538482666,
        "learning_rate": 9.420258220505392e-05,
        "epoch": 0.3486666666666667,
        "step": 2615
    },
    {
        "loss": 2.2192,
        "grad_norm": 2.2146875858306885,
        "learning_rate": 9.413975246412974e-05,
        "epoch": 0.3488,
        "step": 2616
    },
    {
        "loss": 2.1889,
        "grad_norm": 3.0941903591156006,
        "learning_rate": 9.407692504446957e-05,
        "epoch": 0.3489333333333333,
        "step": 2617
    },
    {
        "loss": 2.4521,
        "grad_norm": 2.5476911067962646,
        "learning_rate": 9.401409997095957e-05,
        "epoch": 0.3490666666666667,
        "step": 2618
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.963780403137207,
        "learning_rate": 9.395127726848498e-05,
        "epoch": 0.3492,
        "step": 2619
    },
    {
        "loss": 2.3492,
        "grad_norm": 2.6797873973846436,
        "learning_rate": 9.388845696193008e-05,
        "epoch": 0.34933333333333333,
        "step": 2620
    },
    {
        "loss": 2.009,
        "grad_norm": 3.0988612174987793,
        "learning_rate": 9.38256390761782e-05,
        "epoch": 0.34946666666666665,
        "step": 2621
    },
    {
        "loss": 2.5369,
        "grad_norm": 2.9990012645721436,
        "learning_rate": 9.376282363611173e-05,
        "epoch": 0.3496,
        "step": 2622
    },
    {
        "loss": 0.687,
        "grad_norm": 2.717024087905884,
        "learning_rate": 9.370001066661203e-05,
        "epoch": 0.34973333333333334,
        "step": 2623
    },
    {
        "loss": 2.5343,
        "grad_norm": 1.849302053451538,
        "learning_rate": 9.363720019255963e-05,
        "epoch": 0.34986666666666666,
        "step": 2624
    },
    {
        "loss": 1.6853,
        "grad_norm": 3.077758312225342,
        "learning_rate": 9.357439223883388e-05,
        "epoch": 0.35,
        "step": 2625
    },
    {
        "loss": 2.6541,
        "grad_norm": 2.3166677951812744,
        "learning_rate": 9.351158683031326e-05,
        "epoch": 0.35013333333333335,
        "step": 2626
    },
    {
        "loss": 1.4903,
        "grad_norm": 1.9167585372924805,
        "learning_rate": 9.344878399187521e-05,
        "epoch": 0.35026666666666667,
        "step": 2627
    },
    {
        "loss": 2.8418,
        "grad_norm": 2.3303561210632324,
        "learning_rate": 9.338598374839614e-05,
        "epoch": 0.3504,
        "step": 2628
    },
    {
        "loss": 1.1173,
        "grad_norm": 3.368964195251465,
        "learning_rate": 9.332318612475145e-05,
        "epoch": 0.3505333333333333,
        "step": 2629
    },
    {
        "loss": 2.6152,
        "grad_norm": 2.1330583095550537,
        "learning_rate": 9.326039114581551e-05,
        "epoch": 0.3506666666666667,
        "step": 2630
    },
    {
        "loss": 2.5046,
        "grad_norm": 2.936723232269287,
        "learning_rate": 9.319759883646155e-05,
        "epoch": 0.3508,
        "step": 2631
    },
    {
        "loss": 3.0186,
        "grad_norm": 2.4350271224975586,
        "learning_rate": 9.313480922156188e-05,
        "epoch": 0.3509333333333333,
        "step": 2632
    },
    {
        "loss": 3.3232,
        "grad_norm": 5.485174655914307,
        "learning_rate": 9.307202232598772e-05,
        "epoch": 0.3510666666666667,
        "step": 2633
    },
    {
        "loss": 2.8728,
        "grad_norm": 2.7361907958984375,
        "learning_rate": 9.300923817460905e-05,
        "epoch": 0.3512,
        "step": 2634
    },
    {
        "loss": 2.4987,
        "grad_norm": 3.274508476257324,
        "learning_rate": 9.294645679229502e-05,
        "epoch": 0.35133333333333333,
        "step": 2635
    },
    {
        "loss": 3.0266,
        "grad_norm": 2.4076972007751465,
        "learning_rate": 9.288367820391346e-05,
        "epoch": 0.35146666666666665,
        "step": 2636
    },
    {
        "loss": 2.3933,
        "grad_norm": 2.411930561065674,
        "learning_rate": 9.282090243433122e-05,
        "epoch": 0.3516,
        "step": 2637
    },
    {
        "loss": 2.7118,
        "grad_norm": 3.314732313156128,
        "learning_rate": 9.275812950841397e-05,
        "epoch": 0.35173333333333334,
        "step": 2638
    },
    {
        "loss": 1.7467,
        "grad_norm": 3.0649006366729736,
        "learning_rate": 9.269535945102632e-05,
        "epoch": 0.35186666666666666,
        "step": 2639
    },
    {
        "loss": 2.7091,
        "grad_norm": 2.5413448810577393,
        "learning_rate": 9.263259228703167e-05,
        "epoch": 0.352,
        "step": 2640
    },
    {
        "loss": 2.3807,
        "grad_norm": 2.0107574462890625,
        "learning_rate": 9.256982804129231e-05,
        "epoch": 0.35213333333333335,
        "step": 2641
    },
    {
        "loss": 3.1451,
        "grad_norm": 2.8345608711242676,
        "learning_rate": 9.25070667386694e-05,
        "epoch": 0.3522666666666667,
        "step": 2642
    },
    {
        "loss": 2.1329,
        "grad_norm": 3.1934406757354736,
        "learning_rate": 9.244430840402287e-05,
        "epoch": 0.3524,
        "step": 2643
    },
    {
        "loss": 2.9133,
        "grad_norm": 3.4703574180603027,
        "learning_rate": 9.238155306221153e-05,
        "epoch": 0.3525333333333333,
        "step": 2644
    },
    {
        "loss": 2.3399,
        "grad_norm": 2.3666045665740967,
        "learning_rate": 9.2318800738093e-05,
        "epoch": 0.3526666666666667,
        "step": 2645
    },
    {
        "loss": 2.4817,
        "grad_norm": 3.6816680431365967,
        "learning_rate": 9.225605145652369e-05,
        "epoch": 0.3528,
        "step": 2646
    },
    {
        "loss": 0.7358,
        "grad_norm": 3.4733941555023193,
        "learning_rate": 9.219330524235875e-05,
        "epoch": 0.3529333333333333,
        "step": 2647
    },
    {
        "loss": 2.295,
        "grad_norm": 3.2606985569000244,
        "learning_rate": 9.213056212045217e-05,
        "epoch": 0.35306666666666664,
        "step": 2648
    },
    {
        "loss": 2.7592,
        "grad_norm": 2.8771440982818604,
        "learning_rate": 9.206782211565681e-05,
        "epoch": 0.3532,
        "step": 2649
    },
    {
        "loss": 1.7462,
        "grad_norm": 5.90701961517334,
        "learning_rate": 9.200508525282413e-05,
        "epoch": 0.35333333333333333,
        "step": 2650
    },
    {
        "loss": 2.2451,
        "grad_norm": 2.4787449836730957,
        "learning_rate": 9.194235155680439e-05,
        "epoch": 0.35346666666666665,
        "step": 2651
    },
    {
        "loss": 2.7934,
        "grad_norm": 2.556241273880005,
        "learning_rate": 9.187962105244667e-05,
        "epoch": 0.3536,
        "step": 2652
    },
    {
        "loss": 2.7488,
        "grad_norm": 4.5826945304870605,
        "learning_rate": 9.181689376459872e-05,
        "epoch": 0.35373333333333334,
        "step": 2653
    },
    {
        "loss": 2.352,
        "grad_norm": 2.259822368621826,
        "learning_rate": 9.175416971810704e-05,
        "epoch": 0.35386666666666666,
        "step": 2654
    },
    {
        "loss": 1.8116,
        "grad_norm": 3.806408405303955,
        "learning_rate": 9.169144893781685e-05,
        "epoch": 0.354,
        "step": 2655
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.934468984603882,
        "learning_rate": 9.1628731448572e-05,
        "epoch": 0.35413333333333336,
        "step": 2656
    },
    {
        "loss": 0.9637,
        "grad_norm": 3.7506446838378906,
        "learning_rate": 9.156601727521519e-05,
        "epoch": 0.3542666666666667,
        "step": 2657
    },
    {
        "loss": 0.5537,
        "grad_norm": 1.9144892692565918,
        "learning_rate": 9.150330644258762e-05,
        "epoch": 0.3544,
        "step": 2658
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.603123664855957,
        "learning_rate": 9.144059897552935e-05,
        "epoch": 0.3545333333333333,
        "step": 2659
    },
    {
        "loss": 1.6403,
        "grad_norm": 3.2795097827911377,
        "learning_rate": 9.137789489887899e-05,
        "epoch": 0.3546666666666667,
        "step": 2660
    },
    {
        "loss": 2.0283,
        "grad_norm": 1.8169718980789185,
        "learning_rate": 9.131519423747386e-05,
        "epoch": 0.3548,
        "step": 2661
    },
    {
        "loss": 2.4764,
        "grad_norm": 3.245875358581543,
        "learning_rate": 9.125249701614989e-05,
        "epoch": 0.3549333333333333,
        "step": 2662
    },
    {
        "loss": 2.7889,
        "grad_norm": 2.4070792198181152,
        "learning_rate": 9.118980325974165e-05,
        "epoch": 0.35506666666666664,
        "step": 2663
    },
    {
        "loss": 2.3912,
        "grad_norm": 3.6542561054229736,
        "learning_rate": 9.112711299308235e-05,
        "epoch": 0.3552,
        "step": 2664
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.881511926651001,
        "learning_rate": 9.10644262410038e-05,
        "epoch": 0.35533333333333333,
        "step": 2665
    },
    {
        "loss": 1.589,
        "grad_norm": 3.1412408351898193,
        "learning_rate": 9.100174302833651e-05,
        "epoch": 0.35546666666666665,
        "step": 2666
    },
    {
        "loss": 3.002,
        "grad_norm": 2.4318673610687256,
        "learning_rate": 9.093906337990945e-05,
        "epoch": 0.3556,
        "step": 2667
    },
    {
        "loss": 1.8957,
        "grad_norm": 3.5284409523010254,
        "learning_rate": 9.087638732055024e-05,
        "epoch": 0.35573333333333335,
        "step": 2668
    },
    {
        "loss": 1.6637,
        "grad_norm": 3.376357078552246,
        "learning_rate": 9.081371487508513e-05,
        "epoch": 0.35586666666666666,
        "step": 2669
    },
    {
        "loss": 1.7912,
        "grad_norm": 3.8114445209503174,
        "learning_rate": 9.075104606833884e-05,
        "epoch": 0.356,
        "step": 2670
    },
    {
        "loss": 2.6848,
        "grad_norm": 2.5471551418304443,
        "learning_rate": 9.068838092513475e-05,
        "epoch": 0.35613333333333336,
        "step": 2671
    },
    {
        "loss": 2.9551,
        "grad_norm": 2.726372718811035,
        "learning_rate": 9.062571947029468e-05,
        "epoch": 0.3562666666666667,
        "step": 2672
    },
    {
        "loss": 2.9994,
        "grad_norm": 3.524383306503296,
        "learning_rate": 9.056306172863905e-05,
        "epoch": 0.3564,
        "step": 2673
    },
    {
        "loss": 1.6483,
        "grad_norm": 5.386425971984863,
        "learning_rate": 9.050040772498685e-05,
        "epoch": 0.3565333333333333,
        "step": 2674
    },
    {
        "loss": 2.3349,
        "grad_norm": 2.4624991416931152,
        "learning_rate": 9.043775748415552e-05,
        "epoch": 0.3566666666666667,
        "step": 2675
    },
    {
        "loss": 2.622,
        "grad_norm": 2.355494260787964,
        "learning_rate": 9.0375111030961e-05,
        "epoch": 0.3568,
        "step": 2676
    },
    {
        "loss": 2.5755,
        "grad_norm": 2.3437671661376953,
        "learning_rate": 9.031246839021783e-05,
        "epoch": 0.3569333333333333,
        "step": 2677
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.990138292312622,
        "learning_rate": 9.024982958673891e-05,
        "epoch": 0.35706666666666664,
        "step": 2678
    },
    {
        "loss": 1.1291,
        "grad_norm": 3.9195704460144043,
        "learning_rate": 9.018719464533573e-05,
        "epoch": 0.3572,
        "step": 2679
    },
    {
        "loss": 2.2331,
        "grad_norm": 3.2354133129119873,
        "learning_rate": 9.012456359081821e-05,
        "epoch": 0.35733333333333334,
        "step": 2680
    },
    {
        "loss": 2.4701,
        "grad_norm": 2.1521551609039307,
        "learning_rate": 9.006193644799469e-05,
        "epoch": 0.35746666666666665,
        "step": 2681
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.330423593521118,
        "learning_rate": 8.999931324167198e-05,
        "epoch": 0.3576,
        "step": 2682
    },
    {
        "loss": 1.9287,
        "grad_norm": 3.408870220184326,
        "learning_rate": 8.993669399665539e-05,
        "epoch": 0.35773333333333335,
        "step": 2683
    },
    {
        "loss": 1.0891,
        "grad_norm": 3.290323495864868,
        "learning_rate": 8.987407873774863e-05,
        "epoch": 0.35786666666666667,
        "step": 2684
    },
    {
        "loss": 2.5096,
        "grad_norm": 2.675248622894287,
        "learning_rate": 8.981146748975372e-05,
        "epoch": 0.358,
        "step": 2685
    },
    {
        "loss": 1.3455,
        "grad_norm": 2.4763152599334717,
        "learning_rate": 8.97488602774713e-05,
        "epoch": 0.35813333333333336,
        "step": 2686
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.275423049926758,
        "learning_rate": 8.968625712570026e-05,
        "epoch": 0.3582666666666667,
        "step": 2687
    },
    {
        "loss": 2.7226,
        "grad_norm": 2.6048879623413086,
        "learning_rate": 8.96236580592379e-05,
        "epoch": 0.3584,
        "step": 2688
    },
    {
        "loss": 0.8471,
        "grad_norm": 2.660295248031616,
        "learning_rate": 8.956106310287996e-05,
        "epoch": 0.3585333333333333,
        "step": 2689
    },
    {
        "loss": 1.3412,
        "grad_norm": 4.726496696472168,
        "learning_rate": 8.949847228142044e-05,
        "epoch": 0.3586666666666667,
        "step": 2690
    },
    {
        "loss": 1.547,
        "grad_norm": 1.9679336547851562,
        "learning_rate": 8.943588561965187e-05,
        "epoch": 0.3588,
        "step": 2691
    },
    {
        "loss": 2.034,
        "grad_norm": 3.7734427452087402,
        "learning_rate": 8.937330314236501e-05,
        "epoch": 0.3589333333333333,
        "step": 2692
    },
    {
        "loss": 2.6659,
        "grad_norm": 3.7790191173553467,
        "learning_rate": 8.931072487434895e-05,
        "epoch": 0.35906666666666665,
        "step": 2693
    },
    {
        "loss": 2.2215,
        "grad_norm": 4.240268707275391,
        "learning_rate": 8.924815084039121e-05,
        "epoch": 0.3592,
        "step": 2694
    },
    {
        "loss": 2.7979,
        "grad_norm": 2.1844499111175537,
        "learning_rate": 8.918558106527757e-05,
        "epoch": 0.35933333333333334,
        "step": 2695
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.841447114944458,
        "learning_rate": 8.912301557379214e-05,
        "epoch": 0.35946666666666666,
        "step": 2696
    },
    {
        "loss": 2.4828,
        "grad_norm": 2.0393447875976562,
        "learning_rate": 8.906045439071728e-05,
        "epoch": 0.3596,
        "step": 2697
    },
    {
        "loss": 1.9489,
        "grad_norm": 3.2593510150909424,
        "learning_rate": 8.899789754083368e-05,
        "epoch": 0.35973333333333335,
        "step": 2698
    },
    {
        "loss": 2.127,
        "grad_norm": 2.5120127201080322,
        "learning_rate": 8.893534504892033e-05,
        "epoch": 0.35986666666666667,
        "step": 2699
    },
    {
        "loss": 1.4432,
        "grad_norm": 2.773966073989868,
        "learning_rate": 8.887279693975458e-05,
        "epoch": 0.36,
        "step": 2700
    },
    {
        "loss": 1.4342,
        "grad_norm": 3.1270751953125,
        "learning_rate": 8.881025323811186e-05,
        "epoch": 0.36013333333333336,
        "step": 2701
    },
    {
        "loss": 3.0709,
        "grad_norm": 3.1420958042144775,
        "learning_rate": 8.874771396876593e-05,
        "epoch": 0.3602666666666667,
        "step": 2702
    },
    {
        "loss": 1.7232,
        "grad_norm": 4.3870415687561035,
        "learning_rate": 8.868517915648887e-05,
        "epoch": 0.3604,
        "step": 2703
    },
    {
        "loss": 1.5094,
        "grad_norm": 5.479391574859619,
        "learning_rate": 8.86226488260509e-05,
        "epoch": 0.3605333333333333,
        "step": 2704
    },
    {
        "loss": 1.8391,
        "grad_norm": 3.8941986560821533,
        "learning_rate": 8.85601230022205e-05,
        "epoch": 0.3606666666666667,
        "step": 2705
    },
    {
        "loss": 1.5474,
        "grad_norm": 3.616403579711914,
        "learning_rate": 8.849760170976435e-05,
        "epoch": 0.3608,
        "step": 2706
    },
    {
        "loss": 2.6809,
        "grad_norm": 4.253645896911621,
        "learning_rate": 8.843508497344734e-05,
        "epoch": 0.36093333333333333,
        "step": 2707
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.6808953285217285,
        "learning_rate": 8.837257281803259e-05,
        "epoch": 0.36106666666666665,
        "step": 2708
    },
    {
        "loss": 2.1517,
        "grad_norm": 3.050938129425049,
        "learning_rate": 8.83100652682814e-05,
        "epoch": 0.3612,
        "step": 2709
    },
    {
        "loss": 2.347,
        "grad_norm": 2.2135744094848633,
        "learning_rate": 8.824756234895311e-05,
        "epoch": 0.36133333333333334,
        "step": 2710
    },
    {
        "loss": 2.2479,
        "grad_norm": 3.6512179374694824,
        "learning_rate": 8.818506408480548e-05,
        "epoch": 0.36146666666666666,
        "step": 2711
    },
    {
        "loss": 0.8699,
        "grad_norm": 4.282702922821045,
        "learning_rate": 8.812257050059424e-05,
        "epoch": 0.3616,
        "step": 2712
    },
    {
        "loss": 2.6767,
        "grad_norm": 3.09047532081604,
        "learning_rate": 8.806008162107327e-05,
        "epoch": 0.36173333333333335,
        "step": 2713
    },
    {
        "loss": 2.3147,
        "grad_norm": 2.9316444396972656,
        "learning_rate": 8.799759747099467e-05,
        "epoch": 0.36186666666666667,
        "step": 2714
    },
    {
        "loss": 0.9013,
        "grad_norm": 2.99242901802063,
        "learning_rate": 8.793511807510857e-05,
        "epoch": 0.362,
        "step": 2715
    },
    {
        "loss": 3.1417,
        "grad_norm": 2.7534842491149902,
        "learning_rate": 8.787264345816333e-05,
        "epoch": 0.3621333333333333,
        "step": 2716
    },
    {
        "loss": 1.5031,
        "grad_norm": 4.143303394317627,
        "learning_rate": 8.781017364490534e-05,
        "epoch": 0.3622666666666667,
        "step": 2717
    },
    {
        "loss": 1.794,
        "grad_norm": 2.976301431655884,
        "learning_rate": 8.77477086600791e-05,
        "epoch": 0.3624,
        "step": 2718
    },
    {
        "loss": 1.7878,
        "grad_norm": 3.4295578002929688,
        "learning_rate": 8.768524852842719e-05,
        "epoch": 0.3625333333333333,
        "step": 2719
    },
    {
        "loss": 2.502,
        "grad_norm": 2.8258538246154785,
        "learning_rate": 8.762279327469033e-05,
        "epoch": 0.3626666666666667,
        "step": 2720
    },
    {
        "loss": 1.9392,
        "grad_norm": 3.8527190685272217,
        "learning_rate": 8.756034292360723e-05,
        "epoch": 0.3628,
        "step": 2721
    },
    {
        "loss": 1.7122,
        "grad_norm": 3.8547158241271973,
        "learning_rate": 8.749789749991468e-05,
        "epoch": 0.36293333333333333,
        "step": 2722
    },
    {
        "loss": 3.3269,
        "grad_norm": 3.3777403831481934,
        "learning_rate": 8.743545702834756e-05,
        "epoch": 0.36306666666666665,
        "step": 2723
    },
    {
        "loss": 2.2493,
        "grad_norm": 3.201864242553711,
        "learning_rate": 8.737302153363865e-05,
        "epoch": 0.3632,
        "step": 2724
    },
    {
        "loss": 0.9782,
        "grad_norm": 3.862758159637451,
        "learning_rate": 8.731059104051899e-05,
        "epoch": 0.36333333333333334,
        "step": 2725
    },
    {
        "loss": 2.4106,
        "grad_norm": 2.0386486053466797,
        "learning_rate": 8.724816557371746e-05,
        "epoch": 0.36346666666666666,
        "step": 2726
    },
    {
        "loss": 2.3248,
        "grad_norm": 3.047370195388794,
        "learning_rate": 8.718574515796097e-05,
        "epoch": 0.3636,
        "step": 2727
    },
    {
        "loss": 2.0303,
        "grad_norm": 3.991793155670166,
        "learning_rate": 8.71233298179745e-05,
        "epoch": 0.36373333333333335,
        "step": 2728
    },
    {
        "loss": 2.4683,
        "grad_norm": 3.80461049079895,
        "learning_rate": 8.706091957848096e-05,
        "epoch": 0.36386666666666667,
        "step": 2729
    },
    {
        "loss": 2.5217,
        "grad_norm": 3.4866819381713867,
        "learning_rate": 8.699851446420127e-05,
        "epoch": 0.364,
        "step": 2730
    },
    {
        "loss": 2.346,
        "grad_norm": 3.08477520942688,
        "learning_rate": 8.693611449985432e-05,
        "epoch": 0.3641333333333333,
        "step": 2731
    },
    {
        "loss": 2.1141,
        "grad_norm": 2.6580910682678223,
        "learning_rate": 8.687371971015687e-05,
        "epoch": 0.3642666666666667,
        "step": 2732
    },
    {
        "loss": 2.067,
        "grad_norm": 3.7494544982910156,
        "learning_rate": 8.681133011982382e-05,
        "epoch": 0.3644,
        "step": 2733
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.1959824562072754,
        "learning_rate": 8.674894575356787e-05,
        "epoch": 0.3645333333333333,
        "step": 2734
    },
    {
        "loss": 2.6583,
        "grad_norm": 2.3380961418151855,
        "learning_rate": 8.668656663609959e-05,
        "epoch": 0.36466666666666664,
        "step": 2735
    },
    {
        "loss": 2.6551,
        "grad_norm": 2.9396846294403076,
        "learning_rate": 8.662419279212768e-05,
        "epoch": 0.3648,
        "step": 2736
    },
    {
        "loss": 2.5065,
        "grad_norm": 3.08510684967041,
        "learning_rate": 8.656182424635858e-05,
        "epoch": 0.36493333333333333,
        "step": 2737
    },
    {
        "loss": 2.6322,
        "grad_norm": 2.663501262664795,
        "learning_rate": 8.649946102349663e-05,
        "epoch": 0.36506666666666665,
        "step": 2738
    },
    {
        "loss": 2.4811,
        "grad_norm": 3.6483395099639893,
        "learning_rate": 8.643710314824424e-05,
        "epoch": 0.3652,
        "step": 2739
    },
    {
        "loss": 1.9976,
        "grad_norm": 4.079741477966309,
        "learning_rate": 8.637475064530149e-05,
        "epoch": 0.36533333333333334,
        "step": 2740
    },
    {
        "loss": 2.1184,
        "grad_norm": 3.337700128555298,
        "learning_rate": 8.631240353936642e-05,
        "epoch": 0.36546666666666666,
        "step": 2741
    },
    {
        "loss": 2.5989,
        "grad_norm": 3.6598243713378906,
        "learning_rate": 8.625006185513496e-05,
        "epoch": 0.3656,
        "step": 2742
    },
    {
        "loss": 2.7902,
        "grad_norm": 2.85746431350708,
        "learning_rate": 8.618772561730084e-05,
        "epoch": 0.36573333333333335,
        "step": 2743
    },
    {
        "loss": 2.3104,
        "grad_norm": 2.281942367553711,
        "learning_rate": 8.612539485055566e-05,
        "epoch": 0.3658666666666667,
        "step": 2744
    },
    {
        "loss": 2.2125,
        "grad_norm": 3.0058305263519287,
        "learning_rate": 8.606306957958887e-05,
        "epoch": 0.366,
        "step": 2745
    },
    {
        "loss": 2.3981,
        "grad_norm": 2.4922635555267334,
        "learning_rate": 8.600074982908773e-05,
        "epoch": 0.3661333333333333,
        "step": 2746
    },
    {
        "loss": 2.892,
        "grad_norm": 2.917771577835083,
        "learning_rate": 8.593843562373728e-05,
        "epoch": 0.3662666666666667,
        "step": 2747
    },
    {
        "loss": 2.8516,
        "grad_norm": 3.0661656856536865,
        "learning_rate": 8.587612698822037e-05,
        "epoch": 0.3664,
        "step": 2748
    },
    {
        "loss": 2.6739,
        "grad_norm": 2.8529274463653564,
        "learning_rate": 8.581382394721766e-05,
        "epoch": 0.3665333333333333,
        "step": 2749
    },
    {
        "loss": 2.1295,
        "grad_norm": 2.804748296737671,
        "learning_rate": 8.575152652540768e-05,
        "epoch": 0.36666666666666664,
        "step": 2750
    },
    {
        "loss": 2.7377,
        "grad_norm": 2.285928249359131,
        "learning_rate": 8.568923474746657e-05,
        "epoch": 0.3668,
        "step": 2751
    },
    {
        "loss": 2.8467,
        "grad_norm": 3.0690855979919434,
        "learning_rate": 8.562694863806832e-05,
        "epoch": 0.36693333333333333,
        "step": 2752
    },
    {
        "loss": 2.9684,
        "grad_norm": 3.6440699100494385,
        "learning_rate": 8.556466822188471e-05,
        "epoch": 0.36706666666666665,
        "step": 2753
    },
    {
        "loss": 2.6778,
        "grad_norm": 2.112637758255005,
        "learning_rate": 8.550239352358521e-05,
        "epoch": 0.3672,
        "step": 2754
    },
    {
        "loss": 2.648,
        "grad_norm": 3.374998092651367,
        "learning_rate": 8.544012456783704e-05,
        "epoch": 0.36733333333333335,
        "step": 2755
    },
    {
        "loss": 1.9344,
        "grad_norm": 3.3280317783355713,
        "learning_rate": 8.537786137930514e-05,
        "epoch": 0.36746666666666666,
        "step": 2756
    },
    {
        "loss": 2.903,
        "grad_norm": 2.831505298614502,
        "learning_rate": 8.531560398265212e-05,
        "epoch": 0.3676,
        "step": 2757
    },
    {
        "loss": 2.961,
        "grad_norm": 2.280017375946045,
        "learning_rate": 8.525335240253842e-05,
        "epoch": 0.36773333333333336,
        "step": 2758
    },
    {
        "loss": 1.2804,
        "grad_norm": 3.012338638305664,
        "learning_rate": 8.51911066636221e-05,
        "epoch": 0.3678666666666667,
        "step": 2759
    },
    {
        "loss": 2.2081,
        "grad_norm": 2.823120355606079,
        "learning_rate": 8.512886679055889e-05,
        "epoch": 0.368,
        "step": 2760
    },
    {
        "loss": 2.7869,
        "grad_norm": 2.8648335933685303,
        "learning_rate": 8.50666328080022e-05,
        "epoch": 0.3681333333333333,
        "step": 2761
    },
    {
        "loss": 2.5582,
        "grad_norm": 3.380521059036255,
        "learning_rate": 8.500440474060316e-05,
        "epoch": 0.3682666666666667,
        "step": 2762
    },
    {
        "loss": 1.8982,
        "grad_norm": 1.7850191593170166,
        "learning_rate": 8.494218261301052e-05,
        "epoch": 0.3684,
        "step": 2763
    },
    {
        "loss": 2.5746,
        "grad_norm": 4.285818576812744,
        "learning_rate": 8.487996644987062e-05,
        "epoch": 0.3685333333333333,
        "step": 2764
    },
    {
        "loss": 2.4721,
        "grad_norm": 4.822074890136719,
        "learning_rate": 8.481775627582754e-05,
        "epoch": 0.36866666666666664,
        "step": 2765
    },
    {
        "loss": 2.8694,
        "grad_norm": 2.345247745513916,
        "learning_rate": 8.475555211552292e-05,
        "epoch": 0.3688,
        "step": 2766
    },
    {
        "loss": 2.0883,
        "grad_norm": 3.7578530311584473,
        "learning_rate": 8.469335399359606e-05,
        "epoch": 0.36893333333333334,
        "step": 2767
    },
    {
        "loss": 3.0137,
        "grad_norm": 2.4309334754943848,
        "learning_rate": 8.463116193468384e-05,
        "epoch": 0.36906666666666665,
        "step": 2768
    },
    {
        "loss": 2.8485,
        "grad_norm": 2.9333932399749756,
        "learning_rate": 8.456897596342072e-05,
        "epoch": 0.3692,
        "step": 2769
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.8053884506225586,
        "learning_rate": 8.450679610443883e-05,
        "epoch": 0.36933333333333335,
        "step": 2770
    },
    {
        "loss": 1.7825,
        "grad_norm": 4.12523889541626,
        "learning_rate": 8.44446223823678e-05,
        "epoch": 0.36946666666666667,
        "step": 2771
    },
    {
        "loss": 3.0691,
        "grad_norm": 2.6113555431365967,
        "learning_rate": 8.438245482183486e-05,
        "epoch": 0.3696,
        "step": 2772
    },
    {
        "loss": 3.5067,
        "grad_norm": 5.496614933013916,
        "learning_rate": 8.432029344746477e-05,
        "epoch": 0.36973333333333336,
        "step": 2773
    },
    {
        "loss": 2.2268,
        "grad_norm": 3.7788796424865723,
        "learning_rate": 8.425813828387984e-05,
        "epoch": 0.3698666666666667,
        "step": 2774
    },
    {
        "loss": 2.3917,
        "grad_norm": 2.689192771911621,
        "learning_rate": 8.41959893557e-05,
        "epoch": 0.37,
        "step": 2775
    },
    {
        "loss": 2.6763,
        "grad_norm": 3.396723508834839,
        "learning_rate": 8.413384668754263e-05,
        "epoch": 0.3701333333333333,
        "step": 2776
    },
    {
        "loss": 1.4402,
        "grad_norm": 3.8948044776916504,
        "learning_rate": 8.407171030402262e-05,
        "epoch": 0.3702666666666667,
        "step": 2777
    },
    {
        "loss": 2.9014,
        "grad_norm": 2.4657185077667236,
        "learning_rate": 8.400958022975246e-05,
        "epoch": 0.3704,
        "step": 2778
    },
    {
        "loss": 1.8691,
        "grad_norm": 3.774056911468506,
        "learning_rate": 8.394745648934204e-05,
        "epoch": 0.3705333333333333,
        "step": 2779
    },
    {
        "loss": 1.3129,
        "grad_norm": 3.6395514011383057,
        "learning_rate": 8.388533910739882e-05,
        "epoch": 0.37066666666666664,
        "step": 2780
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.4193718433380127,
        "learning_rate": 8.382322810852769e-05,
        "epoch": 0.3708,
        "step": 2781
    },
    {
        "loss": 1.074,
        "grad_norm": 3.1426901817321777,
        "learning_rate": 8.376112351733102e-05,
        "epoch": 0.37093333333333334,
        "step": 2782
    },
    {
        "loss": 2.7684,
        "grad_norm": 2.9853885173797607,
        "learning_rate": 8.369902535840864e-05,
        "epoch": 0.37106666666666666,
        "step": 2783
    },
    {
        "loss": 2.4754,
        "grad_norm": 2.897078037261963,
        "learning_rate": 8.363693365635788e-05,
        "epoch": 0.3712,
        "step": 2784
    },
    {
        "loss": 2.1719,
        "grad_norm": 3.1642005443573,
        "learning_rate": 8.357484843577348e-05,
        "epoch": 0.37133333333333335,
        "step": 2785
    },
    {
        "loss": 2.8444,
        "grad_norm": 2.1144983768463135,
        "learning_rate": 8.351276972124752e-05,
        "epoch": 0.37146666666666667,
        "step": 2786
    },
    {
        "loss": 2.331,
        "grad_norm": 3.744258165359497,
        "learning_rate": 8.34506975373697e-05,
        "epoch": 0.3716,
        "step": 2787
    },
    {
        "loss": 1.7416,
        "grad_norm": 3.402102470397949,
        "learning_rate": 8.338863190872697e-05,
        "epoch": 0.37173333333333336,
        "step": 2788
    },
    {
        "loss": 1.3806,
        "grad_norm": 3.5265841484069824,
        "learning_rate": 8.332657285990375e-05,
        "epoch": 0.3718666666666667,
        "step": 2789
    },
    {
        "loss": 2.7908,
        "grad_norm": 3.068068265914917,
        "learning_rate": 8.326452041548182e-05,
        "epoch": 0.372,
        "step": 2790
    },
    {
        "loss": 2.8315,
        "grad_norm": 2.7409117221832275,
        "learning_rate": 8.320247460004036e-05,
        "epoch": 0.3721333333333333,
        "step": 2791
    },
    {
        "loss": 2.4593,
        "grad_norm": 3.2759978771209717,
        "learning_rate": 8.314043543815596e-05,
        "epoch": 0.3722666666666667,
        "step": 2792
    },
    {
        "loss": 2.3645,
        "grad_norm": 3.2201895713806152,
        "learning_rate": 8.307840295440254e-05,
        "epoch": 0.3724,
        "step": 2793
    },
    {
        "loss": 2.5835,
        "grad_norm": 3.272643566131592,
        "learning_rate": 8.301637717335135e-05,
        "epoch": 0.3725333333333333,
        "step": 2794
    },
    {
        "loss": 2.6369,
        "grad_norm": 5.39808988571167,
        "learning_rate": 8.295435811957105e-05,
        "epoch": 0.37266666666666665,
        "step": 2795
    },
    {
        "loss": 1.7384,
        "grad_norm": 3.603586435317993,
        "learning_rate": 8.289234581762758e-05,
        "epoch": 0.3728,
        "step": 2796
    },
    {
        "loss": 2.276,
        "grad_norm": 2.8641655445098877,
        "learning_rate": 8.283034029208426e-05,
        "epoch": 0.37293333333333334,
        "step": 2797
    },
    {
        "loss": 2.0065,
        "grad_norm": 4.460245132446289,
        "learning_rate": 8.276834156750161e-05,
        "epoch": 0.37306666666666666,
        "step": 2798
    },
    {
        "loss": 2.6886,
        "grad_norm": 3.865433931350708,
        "learning_rate": 8.270634966843756e-05,
        "epoch": 0.3732,
        "step": 2799
    },
    {
        "loss": 1.6546,
        "grad_norm": 3.1091723442077637,
        "learning_rate": 8.264436461944732e-05,
        "epoch": 0.37333333333333335,
        "step": 2800
    },
    {
        "loss": 2.0321,
        "grad_norm": 3.1391618251800537,
        "learning_rate": 8.258238644508346e-05,
        "epoch": 0.37346666666666667,
        "step": 2801
    },
    {
        "loss": 2.7797,
        "grad_norm": 3.564504861831665,
        "learning_rate": 8.252041516989564e-05,
        "epoch": 0.3736,
        "step": 2802
    },
    {
        "loss": 2.3322,
        "grad_norm": 2.688377857208252,
        "learning_rate": 8.245845081843091e-05,
        "epoch": 0.3737333333333333,
        "step": 2803
    },
    {
        "loss": 1.8766,
        "grad_norm": 2.832920789718628,
        "learning_rate": 8.239649341523363e-05,
        "epoch": 0.3738666666666667,
        "step": 2804
    },
    {
        "loss": 3.0944,
        "grad_norm": 1.6707483530044556,
        "learning_rate": 8.23345429848453e-05,
        "epoch": 0.374,
        "step": 2805
    },
    {
        "loss": 1.871,
        "grad_norm": 3.6195645332336426,
        "learning_rate": 8.227259955180465e-05,
        "epoch": 0.3741333333333333,
        "step": 2806
    },
    {
        "loss": 2.4214,
        "grad_norm": 3.0272865295410156,
        "learning_rate": 8.221066314064774e-05,
        "epoch": 0.3742666666666667,
        "step": 2807
    },
    {
        "loss": 1.8836,
        "grad_norm": 4.024409770965576,
        "learning_rate": 8.214873377590775e-05,
        "epoch": 0.3744,
        "step": 2808
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.9248850345611572,
        "learning_rate": 8.208681148211516e-05,
        "epoch": 0.37453333333333333,
        "step": 2809
    },
    {
        "loss": 0.6496,
        "grad_norm": 2.155881643295288,
        "learning_rate": 8.202489628379759e-05,
        "epoch": 0.37466666666666665,
        "step": 2810
    },
    {
        "loss": 1.3289,
        "grad_norm": 4.5005927085876465,
        "learning_rate": 8.19629882054798e-05,
        "epoch": 0.3748,
        "step": 2811
    },
    {
        "loss": 2.5639,
        "grad_norm": 4.277525424957275,
        "learning_rate": 8.190108727168391e-05,
        "epoch": 0.37493333333333334,
        "step": 2812
    },
    {
        "loss": 2.6707,
        "grad_norm": 2.3596839904785156,
        "learning_rate": 8.183919350692899e-05,
        "epoch": 0.37506666666666666,
        "step": 2813
    },
    {
        "loss": 0.9319,
        "grad_norm": 1.9245814085006714,
        "learning_rate": 8.177730693573142e-05,
        "epoch": 0.3752,
        "step": 2814
    },
    {
        "loss": 2.4843,
        "grad_norm": 3.2735767364501953,
        "learning_rate": 8.171542758260464e-05,
        "epoch": 0.37533333333333335,
        "step": 2815
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.473346710205078,
        "learning_rate": 8.165355547205929e-05,
        "epoch": 0.37546666666666667,
        "step": 2816
    },
    {
        "loss": 1.7319,
        "grad_norm": 3.894792318344116,
        "learning_rate": 8.159169062860315e-05,
        "epoch": 0.3756,
        "step": 2817
    },
    {
        "loss": 2.1969,
        "grad_norm": 3.499035120010376,
        "learning_rate": 8.152983307674108e-05,
        "epoch": 0.3757333333333333,
        "step": 2818
    },
    {
        "loss": 1.7562,
        "grad_norm": 3.7380917072296143,
        "learning_rate": 8.146798284097504e-05,
        "epoch": 0.3758666666666667,
        "step": 2819
    },
    {
        "loss": 2.1275,
        "grad_norm": 3.72482967376709,
        "learning_rate": 8.140613994580414e-05,
        "epoch": 0.376,
        "step": 2820
    },
    {
        "loss": 0.7033,
        "grad_norm": 3.6757030487060547,
        "learning_rate": 8.134430441572459e-05,
        "epoch": 0.3761333333333333,
        "step": 2821
    },
    {
        "loss": 2.3454,
        "grad_norm": 2.7117035388946533,
        "learning_rate": 8.128247627522964e-05,
        "epoch": 0.3762666666666667,
        "step": 2822
    },
    {
        "loss": 2.0537,
        "grad_norm": 6.054164409637451,
        "learning_rate": 8.12206555488096e-05,
        "epoch": 0.3764,
        "step": 2823
    },
    {
        "loss": 2.2263,
        "grad_norm": 3.2341296672821045,
        "learning_rate": 8.115884226095191e-05,
        "epoch": 0.37653333333333333,
        "step": 2824
    },
    {
        "loss": 1.6851,
        "grad_norm": 3.1738429069519043,
        "learning_rate": 8.109703643614094e-05,
        "epoch": 0.37666666666666665,
        "step": 2825
    },
    {
        "loss": 2.7483,
        "grad_norm": 1.6240143775939941,
        "learning_rate": 8.10352380988583e-05,
        "epoch": 0.3768,
        "step": 2826
    },
    {
        "loss": 1.5576,
        "grad_norm": 5.684475421905518,
        "learning_rate": 8.097344727358247e-05,
        "epoch": 0.37693333333333334,
        "step": 2827
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.622096061706543,
        "learning_rate": 8.091166398478897e-05,
        "epoch": 0.37706666666666666,
        "step": 2828
    },
    {
        "loss": 1.5328,
        "grad_norm": 4.081793308258057,
        "learning_rate": 8.084988825695044e-05,
        "epoch": 0.3772,
        "step": 2829
    },
    {
        "loss": 2.5959,
        "grad_norm": 2.6277713775634766,
        "learning_rate": 8.07881201145364e-05,
        "epoch": 0.37733333333333335,
        "step": 2830
    },
    {
        "loss": 1.553,
        "grad_norm": 3.2484188079833984,
        "learning_rate": 8.072635958201347e-05,
        "epoch": 0.3774666666666667,
        "step": 2831
    },
    {
        "loss": 2.3492,
        "grad_norm": 2.9759061336517334,
        "learning_rate": 8.06646066838452e-05,
        "epoch": 0.3776,
        "step": 2832
    },
    {
        "loss": 3.323,
        "grad_norm": 2.9417307376861572,
        "learning_rate": 8.060286144449203e-05,
        "epoch": 0.3777333333333333,
        "step": 2833
    },
    {
        "loss": 2.6536,
        "grad_norm": 2.605902910232544,
        "learning_rate": 8.054112388841159e-05,
        "epoch": 0.3778666666666667,
        "step": 2834
    },
    {
        "loss": 2.8347,
        "grad_norm": 1.8734437227249146,
        "learning_rate": 8.04793940400583e-05,
        "epoch": 0.378,
        "step": 2835
    },
    {
        "loss": 2.573,
        "grad_norm": 3.402174472808838,
        "learning_rate": 8.041767192388348e-05,
        "epoch": 0.3781333333333333,
        "step": 2836
    },
    {
        "loss": 2.5054,
        "grad_norm": 2.815214157104492,
        "learning_rate": 8.03559575643356e-05,
        "epoch": 0.37826666666666664,
        "step": 2837
    },
    {
        "loss": 2.8553,
        "grad_norm": 3.0594825744628906,
        "learning_rate": 8.029425098585984e-05,
        "epoch": 0.3784,
        "step": 2838
    },
    {
        "loss": 2.5955,
        "grad_norm": 2.7162880897521973,
        "learning_rate": 8.02325522128984e-05,
        "epoch": 0.37853333333333333,
        "step": 2839
    },
    {
        "loss": 2.4905,
        "grad_norm": 3.1040821075439453,
        "learning_rate": 8.01708612698904e-05,
        "epoch": 0.37866666666666665,
        "step": 2840
    },
    {
        "loss": 0.6518,
        "grad_norm": 1.8552749156951904,
        "learning_rate": 8.010917818127182e-05,
        "epoch": 0.3788,
        "step": 2841
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.9883503913879395,
        "learning_rate": 8.00475029714755e-05,
        "epoch": 0.37893333333333334,
        "step": 2842
    },
    {
        "loss": 2.4284,
        "grad_norm": 3.0872182846069336,
        "learning_rate": 7.998583566493127e-05,
        "epoch": 0.37906666666666666,
        "step": 2843
    },
    {
        "loss": 1.9639,
        "grad_norm": 3.1830430030822754,
        "learning_rate": 7.992417628606574e-05,
        "epoch": 0.3792,
        "step": 2844
    },
    {
        "loss": 2.7648,
        "grad_norm": 1.7759462594985962,
        "learning_rate": 7.986252485930237e-05,
        "epoch": 0.37933333333333336,
        "step": 2845
    },
    {
        "loss": 2.2996,
        "grad_norm": 3.092898368835449,
        "learning_rate": 7.980088140906153e-05,
        "epoch": 0.3794666666666667,
        "step": 2846
    },
    {
        "loss": 2.1684,
        "grad_norm": 4.146812438964844,
        "learning_rate": 7.97392459597604e-05,
        "epoch": 0.3796,
        "step": 2847
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.533501148223877,
        "learning_rate": 7.9677618535813e-05,
        "epoch": 0.3797333333333333,
        "step": 2848
    },
    {
        "loss": 2.3684,
        "grad_norm": 2.188274383544922,
        "learning_rate": 7.961599916163012e-05,
        "epoch": 0.3798666666666667,
        "step": 2849
    },
    {
        "loss": 1.7985,
        "grad_norm": 5.126161575317383,
        "learning_rate": 7.955438786161939e-05,
        "epoch": 0.38,
        "step": 2850
    },
    {
        "loss": 2.0882,
        "grad_norm": 3.505970001220703,
        "learning_rate": 7.949278466018535e-05,
        "epoch": 0.3801333333333333,
        "step": 2851
    },
    {
        "loss": 2.3823,
        "grad_norm": 4.571463584899902,
        "learning_rate": 7.943118958172917e-05,
        "epoch": 0.38026666666666664,
        "step": 2852
    },
    {
        "loss": 2.2135,
        "grad_norm": 3.180121421813965,
        "learning_rate": 7.936960265064886e-05,
        "epoch": 0.3804,
        "step": 2853
    },
    {
        "loss": 3.1341,
        "grad_norm": 2.3042516708374023,
        "learning_rate": 7.930802389133926e-05,
        "epoch": 0.38053333333333333,
        "step": 2854
    },
    {
        "loss": 1.9856,
        "grad_norm": 3.090038776397705,
        "learning_rate": 7.92464533281919e-05,
        "epoch": 0.38066666666666665,
        "step": 2855
    },
    {
        "loss": 4.2114,
        "grad_norm": 6.486077308654785,
        "learning_rate": 7.91848909855951e-05,
        "epoch": 0.3808,
        "step": 2856
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.531625747680664,
        "learning_rate": 7.912333688793391e-05,
        "epoch": 0.38093333333333335,
        "step": 2857
    },
    {
        "loss": 1.9554,
        "grad_norm": 4.124088764190674,
        "learning_rate": 7.906179105959006e-05,
        "epoch": 0.38106666666666666,
        "step": 2858
    },
    {
        "loss": 2.5639,
        "grad_norm": 2.1228859424591064,
        "learning_rate": 7.900025352494214e-05,
        "epoch": 0.3812,
        "step": 2859
    },
    {
        "loss": 2.5136,
        "grad_norm": 2.9155924320220947,
        "learning_rate": 7.893872430836537e-05,
        "epoch": 0.38133333333333336,
        "step": 2860
    },
    {
        "loss": 2.8071,
        "grad_norm": 2.3990702629089355,
        "learning_rate": 7.887720343423167e-05,
        "epoch": 0.3814666666666667,
        "step": 2861
    },
    {
        "loss": 2.6091,
        "grad_norm": 2.402987480163574,
        "learning_rate": 7.881569092690965e-05,
        "epoch": 0.3816,
        "step": 2862
    },
    {
        "loss": 2.3368,
        "grad_norm": 2.656531572341919,
        "learning_rate": 7.875418681076465e-05,
        "epoch": 0.3817333333333333,
        "step": 2863
    },
    {
        "loss": 2.8078,
        "grad_norm": 1.482505440711975,
        "learning_rate": 7.869269111015867e-05,
        "epoch": 0.3818666666666667,
        "step": 2864
    },
    {
        "loss": 2.7899,
        "grad_norm": 2.7673192024230957,
        "learning_rate": 7.863120384945031e-05,
        "epoch": 0.382,
        "step": 2865
    },
    {
        "loss": 1.6644,
        "grad_norm": 3.3304944038391113,
        "learning_rate": 7.856972505299494e-05,
        "epoch": 0.3821333333333333,
        "step": 2866
    },
    {
        "loss": 2.8391,
        "grad_norm": 3.6619484424591064,
        "learning_rate": 7.850825474514447e-05,
        "epoch": 0.38226666666666664,
        "step": 2867
    },
    {
        "loss": 2.1655,
        "grad_norm": 3.892411947250366,
        "learning_rate": 7.844679295024755e-05,
        "epoch": 0.3824,
        "step": 2868
    },
    {
        "loss": 2.858,
        "grad_norm": 1.9234739542007446,
        "learning_rate": 7.838533969264938e-05,
        "epoch": 0.38253333333333334,
        "step": 2869
    },
    {
        "loss": 2.4472,
        "grad_norm": 2.8051629066467285,
        "learning_rate": 7.832389499669178e-05,
        "epoch": 0.38266666666666665,
        "step": 2870
    },
    {
        "loss": 1.2303,
        "grad_norm": 4.232947826385498,
        "learning_rate": 7.826245888671325e-05,
        "epoch": 0.3828,
        "step": 2871
    },
    {
        "loss": 1.7404,
        "grad_norm": 3.8163938522338867,
        "learning_rate": 7.820103138704884e-05,
        "epoch": 0.38293333333333335,
        "step": 2872
    },
    {
        "loss": 2.5949,
        "grad_norm": 4.540097713470459,
        "learning_rate": 7.813961252203019e-05,
        "epoch": 0.38306666666666667,
        "step": 2873
    },
    {
        "loss": 2.3073,
        "grad_norm": 4.766317844390869,
        "learning_rate": 7.807820231598548e-05,
        "epoch": 0.3832,
        "step": 2874
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.8422977924346924,
        "learning_rate": 7.801680079323952e-05,
        "epoch": 0.38333333333333336,
        "step": 2875
    },
    {
        "loss": 2.0189,
        "grad_norm": 3.800168991088867,
        "learning_rate": 7.795540797811368e-05,
        "epoch": 0.3834666666666667,
        "step": 2876
    },
    {
        "loss": 2.2595,
        "grad_norm": 3.2229602336883545,
        "learning_rate": 7.789402389492586e-05,
        "epoch": 0.3836,
        "step": 2877
    },
    {
        "loss": 2.281,
        "grad_norm": 3.220031499862671,
        "learning_rate": 7.783264856799049e-05,
        "epoch": 0.3837333333333333,
        "step": 2878
    },
    {
        "loss": 2.356,
        "grad_norm": 3.4441075325012207,
        "learning_rate": 7.777128202161851e-05,
        "epoch": 0.3838666666666667,
        "step": 2879
    },
    {
        "loss": 2.4424,
        "grad_norm": 3.0149343013763428,
        "learning_rate": 7.770992428011754e-05,
        "epoch": 0.384,
        "step": 2880
    },
    {
        "loss": 2.7768,
        "grad_norm": 2.711289644241333,
        "learning_rate": 7.76485753677915e-05,
        "epoch": 0.3841333333333333,
        "step": 2881
    },
    {
        "loss": 1.8246,
        "grad_norm": 2.942932605743408,
        "learning_rate": 7.758723530894093e-05,
        "epoch": 0.38426666666666665,
        "step": 2882
    },
    {
        "loss": 1.8667,
        "grad_norm": 5.543022632598877,
        "learning_rate": 7.752590412786279e-05,
        "epoch": 0.3844,
        "step": 2883
    },
    {
        "loss": 2.8518,
        "grad_norm": 2.2482144832611084,
        "learning_rate": 7.74645818488506e-05,
        "epoch": 0.38453333333333334,
        "step": 2884
    },
    {
        "loss": 2.6182,
        "grad_norm": 3.30173659324646,
        "learning_rate": 7.740326849619434e-05,
        "epoch": 0.38466666666666666,
        "step": 2885
    },
    {
        "loss": 2.7111,
        "grad_norm": 2.95495867729187,
        "learning_rate": 7.734196409418046e-05,
        "epoch": 0.3848,
        "step": 2886
    },
    {
        "loss": 2.309,
        "grad_norm": 3.276618003845215,
        "learning_rate": 7.728066866709174e-05,
        "epoch": 0.38493333333333335,
        "step": 2887
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.6598427295684814,
        "learning_rate": 7.72193822392076e-05,
        "epoch": 0.38506666666666667,
        "step": 2888
    },
    {
        "loss": 2.2984,
        "grad_norm": 2.6885170936584473,
        "learning_rate": 7.715810483480381e-05,
        "epoch": 0.3852,
        "step": 2889
    },
    {
        "loss": 1.8254,
        "grad_norm": 4.195718288421631,
        "learning_rate": 7.709683647815251e-05,
        "epoch": 0.38533333333333336,
        "step": 2890
    },
    {
        "loss": 0.8741,
        "grad_norm": 2.9735031127929688,
        "learning_rate": 7.703557719352232e-05,
        "epoch": 0.3854666666666667,
        "step": 2891
    },
    {
        "loss": 2.6966,
        "grad_norm": 3.134465456008911,
        "learning_rate": 7.697432700517824e-05,
        "epoch": 0.3856,
        "step": 2892
    },
    {
        "loss": 2.1801,
        "grad_norm": 3.042797565460205,
        "learning_rate": 7.691308593738171e-05,
        "epoch": 0.3857333333333333,
        "step": 2893
    },
    {
        "loss": 1.4326,
        "grad_norm": 4.251634120941162,
        "learning_rate": 7.68518540143905e-05,
        "epoch": 0.3858666666666667,
        "step": 2894
    },
    {
        "loss": 2.4331,
        "grad_norm": 3.371751308441162,
        "learning_rate": 7.679063126045877e-05,
        "epoch": 0.386,
        "step": 2895
    },
    {
        "loss": 2.0583,
        "grad_norm": 4.034668445587158,
        "learning_rate": 7.67294176998371e-05,
        "epoch": 0.38613333333333333,
        "step": 2896
    },
    {
        "loss": 2.8782,
        "grad_norm": 2.1636481285095215,
        "learning_rate": 7.666821335677237e-05,
        "epoch": 0.38626666666666665,
        "step": 2897
    },
    {
        "loss": 1.7859,
        "grad_norm": 4.062241077423096,
        "learning_rate": 7.660701825550787e-05,
        "epoch": 0.3864,
        "step": 2898
    },
    {
        "loss": 1.7719,
        "grad_norm": 4.1299004554748535,
        "learning_rate": 7.654583242028311e-05,
        "epoch": 0.38653333333333334,
        "step": 2899
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.9144583940505981,
        "learning_rate": 7.648465587533403e-05,
        "epoch": 0.38666666666666666,
        "step": 2900
    },
    {
        "loss": 2.2007,
        "grad_norm": 4.152543544769287,
        "learning_rate": 7.642348864489286e-05,
        "epoch": 0.3868,
        "step": 2901
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.2209599018096924,
        "learning_rate": 7.636233075318824e-05,
        "epoch": 0.38693333333333335,
        "step": 2902
    },
    {
        "loss": 1.6415,
        "grad_norm": 2.829958438873291,
        "learning_rate": 7.630118222444494e-05,
        "epoch": 0.38706666666666667,
        "step": 2903
    },
    {
        "loss": 2.5124,
        "grad_norm": 3.077319622039795,
        "learning_rate": 7.624004308288405e-05,
        "epoch": 0.3872,
        "step": 2904
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.5380446910858154,
        "learning_rate": 7.617891335272314e-05,
        "epoch": 0.3873333333333333,
        "step": 2905
    },
    {
        "loss": 1.8893,
        "grad_norm": 2.7800586223602295,
        "learning_rate": 7.611779305817577e-05,
        "epoch": 0.3874666666666667,
        "step": 2906
    },
    {
        "loss": 1.8034,
        "grad_norm": 3.570004940032959,
        "learning_rate": 7.605668222345197e-05,
        "epoch": 0.3876,
        "step": 2907
    },
    {
        "loss": 2.7418,
        "grad_norm": 4.090950012207031,
        "learning_rate": 7.599558087275793e-05,
        "epoch": 0.3877333333333333,
        "step": 2908
    },
    {
        "loss": 2.8664,
        "grad_norm": 2.810629367828369,
        "learning_rate": 7.593448903029606e-05,
        "epoch": 0.3878666666666667,
        "step": 2909
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.0700130462646484,
        "learning_rate": 7.587340672026512e-05,
        "epoch": 0.388,
        "step": 2910
    },
    {
        "loss": 2.697,
        "grad_norm": 3.1592044830322266,
        "learning_rate": 7.581233396686001e-05,
        "epoch": 0.38813333333333333,
        "step": 2911
    },
    {
        "loss": 2.8081,
        "grad_norm": 2.316575765609741,
        "learning_rate": 7.575127079427176e-05,
        "epoch": 0.38826666666666665,
        "step": 2912
    },
    {
        "loss": 2.0442,
        "grad_norm": 3.711217164993286,
        "learning_rate": 7.569021722668783e-05,
        "epoch": 0.3884,
        "step": 2913
    },
    {
        "loss": 2.3004,
        "grad_norm": 2.3553032875061035,
        "learning_rate": 7.562917328829169e-05,
        "epoch": 0.38853333333333334,
        "step": 2914
    },
    {
        "loss": 1.7738,
        "grad_norm": 4.224569320678711,
        "learning_rate": 7.556813900326303e-05,
        "epoch": 0.38866666666666666,
        "step": 2915
    },
    {
        "loss": 2.0885,
        "grad_norm": 5.526219844818115,
        "learning_rate": 7.550711439577778e-05,
        "epoch": 0.3888,
        "step": 2916
    },
    {
        "loss": 2.3725,
        "grad_norm": 3.0183005332946777,
        "learning_rate": 7.544609949000794e-05,
        "epoch": 0.38893333333333335,
        "step": 2917
    },
    {
        "loss": 2.8758,
        "grad_norm": 2.422710657119751,
        "learning_rate": 7.538509431012179e-05,
        "epoch": 0.38906666666666667,
        "step": 2918
    },
    {
        "loss": 3.1792,
        "grad_norm": 1.7073355913162231,
        "learning_rate": 7.532409888028364e-05,
        "epoch": 0.3892,
        "step": 2919
    },
    {
        "loss": 2.3778,
        "grad_norm": 3.3313586711883545,
        "learning_rate": 7.5263113224654e-05,
        "epoch": 0.3893333333333333,
        "step": 2920
    },
    {
        "loss": 1.9094,
        "grad_norm": 3.9062726497650146,
        "learning_rate": 7.52021373673895e-05,
        "epoch": 0.3894666666666667,
        "step": 2921
    },
    {
        "loss": 1.2106,
        "grad_norm": 3.885568857192993,
        "learning_rate": 7.51411713326429e-05,
        "epoch": 0.3896,
        "step": 2922
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.6852035522460938,
        "learning_rate": 7.508021514456305e-05,
        "epoch": 0.3897333333333333,
        "step": 2923
    },
    {
        "loss": 1.5479,
        "grad_norm": 3.933544635772705,
        "learning_rate": 7.501926882729489e-05,
        "epoch": 0.38986666666666664,
        "step": 2924
    },
    {
        "loss": 1.9087,
        "grad_norm": 2.182727813720703,
        "learning_rate": 7.495833240497947e-05,
        "epoch": 0.39,
        "step": 2925
    },
    {
        "loss": 1.5473,
        "grad_norm": 3.360201358795166,
        "learning_rate": 7.489740590175387e-05,
        "epoch": 0.39013333333333333,
        "step": 2926
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.496581554412842,
        "learning_rate": 7.483648934175138e-05,
        "epoch": 0.39026666666666665,
        "step": 2927
    },
    {
        "loss": 1.5498,
        "grad_norm": 2.451096773147583,
        "learning_rate": 7.477558274910119e-05,
        "epoch": 0.3904,
        "step": 2928
    },
    {
        "loss": 2.7187,
        "grad_norm": 3.710789918899536,
        "learning_rate": 7.47146861479286e-05,
        "epoch": 0.39053333333333334,
        "step": 2929
    },
    {
        "loss": 2.8103,
        "grad_norm": 2.247899055480957,
        "learning_rate": 7.4653799562355e-05,
        "epoch": 0.39066666666666666,
        "step": 2930
    },
    {
        "loss": 2.7825,
        "grad_norm": 4.348729133605957,
        "learning_rate": 7.459292301649777e-05,
        "epoch": 0.3908,
        "step": 2931
    },
    {
        "loss": 2.5488,
        "grad_norm": 1.6842657327651978,
        "learning_rate": 7.453205653447029e-05,
        "epoch": 0.39093333333333335,
        "step": 2932
    },
    {
        "loss": 3.049,
        "grad_norm": 2.0020651817321777,
        "learning_rate": 7.447120014038202e-05,
        "epoch": 0.3910666666666667,
        "step": 2933
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.053079128265381,
        "learning_rate": 7.44103538583383e-05,
        "epoch": 0.3912,
        "step": 2934
    },
    {
        "loss": 2.1355,
        "grad_norm": 3.7739152908325195,
        "learning_rate": 7.434951771244067e-05,
        "epoch": 0.3913333333333333,
        "step": 2935
    },
    {
        "loss": 2.2665,
        "grad_norm": 2.041348934173584,
        "learning_rate": 7.428869172678648e-05,
        "epoch": 0.3914666666666667,
        "step": 2936
    },
    {
        "loss": 1.8731,
        "grad_norm": 2.359858274459839,
        "learning_rate": 7.422787592546904e-05,
        "epoch": 0.3916,
        "step": 2937
    },
    {
        "loss": 2.5928,
        "grad_norm": 2.8377809524536133,
        "learning_rate": 7.416707033257784e-05,
        "epoch": 0.3917333333333333,
        "step": 2938
    },
    {
        "loss": 2.3824,
        "grad_norm": 3.247741460800171,
        "learning_rate": 7.410627497219807e-05,
        "epoch": 0.39186666666666664,
        "step": 2939
    },
    {
        "loss": 2.8315,
        "grad_norm": 5.067243576049805,
        "learning_rate": 7.4045489868411e-05,
        "epoch": 0.392,
        "step": 2940
    },
    {
        "loss": 2.4537,
        "grad_norm": 2.2193188667297363,
        "learning_rate": 7.398471504529385e-05,
        "epoch": 0.39213333333333333,
        "step": 2941
    },
    {
        "loss": 2.9148,
        "grad_norm": 2.50793194770813,
        "learning_rate": 7.392395052691971e-05,
        "epoch": 0.39226666666666665,
        "step": 2942
    },
    {
        "loss": 2.6616,
        "grad_norm": 2.221289873123169,
        "learning_rate": 7.386319633735761e-05,
        "epoch": 0.3924,
        "step": 2943
    },
    {
        "loss": 0.9631,
        "grad_norm": 3.354990243911743,
        "learning_rate": 7.380245250067252e-05,
        "epoch": 0.39253333333333335,
        "step": 2944
    },
    {
        "loss": 2.1968,
        "grad_norm": 4.242852210998535,
        "learning_rate": 7.374171904092526e-05,
        "epoch": 0.39266666666666666,
        "step": 2945
    },
    {
        "loss": 0.9725,
        "grad_norm": 2.373697519302368,
        "learning_rate": 7.368099598217254e-05,
        "epoch": 0.3928,
        "step": 2946
    },
    {
        "loss": 3.0076,
        "grad_norm": 3.089315414428711,
        "learning_rate": 7.362028334846701e-05,
        "epoch": 0.39293333333333336,
        "step": 2947
    },
    {
        "loss": 2.8852,
        "grad_norm": 2.0635931491851807,
        "learning_rate": 7.355958116385712e-05,
        "epoch": 0.3930666666666667,
        "step": 2948
    },
    {
        "loss": 2.2643,
        "grad_norm": 2.217329978942871,
        "learning_rate": 7.349888945238725e-05,
        "epoch": 0.3932,
        "step": 2949
    },
    {
        "loss": 2.0367,
        "grad_norm": 4.548252105712891,
        "learning_rate": 7.343820823809755e-05,
        "epoch": 0.3933333333333333,
        "step": 2950
    },
    {
        "loss": 1.6073,
        "grad_norm": 2.146228790283203,
        "learning_rate": 7.337753754502404e-05,
        "epoch": 0.3934666666666667,
        "step": 2951
    },
    {
        "loss": 2.6207,
        "grad_norm": 2.9420814514160156,
        "learning_rate": 7.331687739719868e-05,
        "epoch": 0.3936,
        "step": 2952
    },
    {
        "loss": 2.4248,
        "grad_norm": 2.8540937900543213,
        "learning_rate": 7.325622781864908e-05,
        "epoch": 0.3937333333333333,
        "step": 2953
    },
    {
        "loss": 1.3616,
        "grad_norm": 1.91056489944458,
        "learning_rate": 7.319558883339875e-05,
        "epoch": 0.39386666666666664,
        "step": 2954
    },
    {
        "loss": 2.7235,
        "grad_norm": 2.592829942703247,
        "learning_rate": 7.313496046546704e-05,
        "epoch": 0.394,
        "step": 2955
    },
    {
        "loss": 1.4902,
        "grad_norm": 3.870978593826294,
        "learning_rate": 7.307434273886902e-05,
        "epoch": 0.39413333333333334,
        "step": 2956
    },
    {
        "loss": 2.4737,
        "grad_norm": 2.3612077236175537,
        "learning_rate": 7.30137356776156e-05,
        "epoch": 0.39426666666666665,
        "step": 2957
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.94059419631958,
        "learning_rate": 7.295313930571345e-05,
        "epoch": 0.3944,
        "step": 2958
    },
    {
        "loss": 2.298,
        "grad_norm": 3.463620901107788,
        "learning_rate": 7.289255364716492e-05,
        "epoch": 0.39453333333333335,
        "step": 2959
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.8847663402557373,
        "learning_rate": 7.283197872596828e-05,
        "epoch": 0.39466666666666667,
        "step": 2960
    },
    {
        "loss": 2.5586,
        "grad_norm": 3.151984691619873,
        "learning_rate": 7.277141456611745e-05,
        "epoch": 0.3948,
        "step": 2961
    },
    {
        "loss": 2.5413,
        "grad_norm": 3.499739170074463,
        "learning_rate": 7.271086119160208e-05,
        "epoch": 0.39493333333333336,
        "step": 2962
    },
    {
        "loss": 2.7338,
        "grad_norm": 2.8722426891326904,
        "learning_rate": 7.265031862640757e-05,
        "epoch": 0.3950666666666667,
        "step": 2963
    },
    {
        "loss": 1.909,
        "grad_norm": 3.640554428100586,
        "learning_rate": 7.258978689451508e-05,
        "epoch": 0.3952,
        "step": 2964
    },
    {
        "loss": 1.8478,
        "grad_norm": 2.980820655822754,
        "learning_rate": 7.25292660199014e-05,
        "epoch": 0.3953333333333333,
        "step": 2965
    },
    {
        "loss": 2.8972,
        "grad_norm": 3.541355609893799,
        "learning_rate": 7.246875602653905e-05,
        "epoch": 0.3954666666666667,
        "step": 2966
    },
    {
        "loss": 2.6371,
        "grad_norm": 4.604683876037598,
        "learning_rate": 7.240825693839621e-05,
        "epoch": 0.3956,
        "step": 2967
    },
    {
        "loss": 3.088,
        "grad_norm": 3.393503427505493,
        "learning_rate": 7.234776877943683e-05,
        "epoch": 0.3957333333333333,
        "step": 2968
    },
    {
        "loss": 1.9681,
        "grad_norm": 3.5036075115203857,
        "learning_rate": 7.228729157362047e-05,
        "epoch": 0.39586666666666664,
        "step": 2969
    },
    {
        "loss": 1.2949,
        "grad_norm": 3.6019561290740967,
        "learning_rate": 7.222682534490235e-05,
        "epoch": 0.396,
        "step": 2970
    },
    {
        "loss": 2.289,
        "grad_norm": 2.925867795944214,
        "learning_rate": 7.21663701172333e-05,
        "epoch": 0.39613333333333334,
        "step": 2971
    },
    {
        "loss": 1.8271,
        "grad_norm": 3.3279221057891846,
        "learning_rate": 7.210592591455994e-05,
        "epoch": 0.39626666666666666,
        "step": 2972
    },
    {
        "loss": 3.0195,
        "grad_norm": 3.453758478164673,
        "learning_rate": 7.204549276082435e-05,
        "epoch": 0.3964,
        "step": 2973
    },
    {
        "loss": 2.1847,
        "grad_norm": 2.3196964263916016,
        "learning_rate": 7.198507067996436e-05,
        "epoch": 0.39653333333333335,
        "step": 2974
    },
    {
        "loss": 2.3165,
        "grad_norm": 2.776313066482544,
        "learning_rate": 7.192465969591332e-05,
        "epoch": 0.39666666666666667,
        "step": 2975
    },
    {
        "loss": 1.8599,
        "grad_norm": 5.506622791290283,
        "learning_rate": 7.186425983260021e-05,
        "epoch": 0.3968,
        "step": 2976
    },
    {
        "loss": 2.6579,
        "grad_norm": 3.9525671005249023,
        "learning_rate": 7.180387111394967e-05,
        "epoch": 0.39693333333333336,
        "step": 2977
    },
    {
        "loss": 3.2673,
        "grad_norm": 4.48521089553833,
        "learning_rate": 7.174349356388186e-05,
        "epoch": 0.3970666666666667,
        "step": 2978
    },
    {
        "loss": 2.8497,
        "grad_norm": 1.8298323154449463,
        "learning_rate": 7.168312720631251e-05,
        "epoch": 0.3972,
        "step": 2979
    },
    {
        "loss": 2.5557,
        "grad_norm": 3.4049718379974365,
        "learning_rate": 7.162277206515293e-05,
        "epoch": 0.3973333333333333,
        "step": 2980
    },
    {
        "loss": 1.9626,
        "grad_norm": 2.3461806774139404,
        "learning_rate": 7.156242816431011e-05,
        "epoch": 0.3974666666666667,
        "step": 2981
    },
    {
        "loss": 2.5326,
        "grad_norm": 2.7431118488311768,
        "learning_rate": 7.150209552768637e-05,
        "epoch": 0.3976,
        "step": 2982
    },
    {
        "loss": 2.1356,
        "grad_norm": 4.535625457763672,
        "learning_rate": 7.144177417917968e-05,
        "epoch": 0.3977333333333333,
        "step": 2983
    },
    {
        "loss": 2.5855,
        "grad_norm": 2.115896701812744,
        "learning_rate": 7.138146414268356e-05,
        "epoch": 0.39786666666666665,
        "step": 2984
    },
    {
        "loss": 2.0679,
        "grad_norm": 4.611456394195557,
        "learning_rate": 7.132116544208698e-05,
        "epoch": 0.398,
        "step": 2985
    },
    {
        "loss": 2.4382,
        "grad_norm": 3.6268012523651123,
        "learning_rate": 7.12608781012745e-05,
        "epoch": 0.39813333333333334,
        "step": 2986
    },
    {
        "loss": 2.6154,
        "grad_norm": 3.688460350036621,
        "learning_rate": 7.120060214412617e-05,
        "epoch": 0.39826666666666666,
        "step": 2987
    },
    {
        "loss": 2.5522,
        "grad_norm": 2.1962082386016846,
        "learning_rate": 7.114033759451739e-05,
        "epoch": 0.3984,
        "step": 2988
    },
    {
        "loss": 2.7759,
        "grad_norm": 2.9987173080444336,
        "learning_rate": 7.108008447631928e-05,
        "epoch": 0.39853333333333335,
        "step": 2989
    },
    {
        "loss": 2.8192,
        "grad_norm": 3.1908791065216064,
        "learning_rate": 7.101984281339826e-05,
        "epoch": 0.39866666666666667,
        "step": 2990
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.135226249694824,
        "learning_rate": 7.095961262961624e-05,
        "epoch": 0.3988,
        "step": 2991
    },
    {
        "loss": 3.4955,
        "grad_norm": 4.5285820960998535,
        "learning_rate": 7.089939394883059e-05,
        "epoch": 0.3989333333333333,
        "step": 2992
    },
    {
        "loss": 2.3055,
        "grad_norm": 3.6865954399108887,
        "learning_rate": 7.083918679489415e-05,
        "epoch": 0.3990666666666667,
        "step": 2993
    },
    {
        "loss": 2.3736,
        "grad_norm": 3.150092840194702,
        "learning_rate": 7.077899119165518e-05,
        "epoch": 0.3992,
        "step": 2994
    },
    {
        "loss": 0.7585,
        "grad_norm": 3.17435884475708,
        "learning_rate": 7.07188071629574e-05,
        "epoch": 0.3993333333333333,
        "step": 2995
    },
    {
        "loss": 2.0939,
        "grad_norm": 3.7355196475982666,
        "learning_rate": 7.065863473263983e-05,
        "epoch": 0.3994666666666667,
        "step": 2996
    },
    {
        "loss": 2.5874,
        "grad_norm": 1.6978278160095215,
        "learning_rate": 7.059847392453707e-05,
        "epoch": 0.3996,
        "step": 2997
    },
    {
        "loss": 2.3351,
        "grad_norm": 3.262481451034546,
        "learning_rate": 7.053832476247895e-05,
        "epoch": 0.39973333333333333,
        "step": 2998
    },
    {
        "loss": 1.6901,
        "grad_norm": 3.247262477874756,
        "learning_rate": 7.047818727029081e-05,
        "epoch": 0.39986666666666665,
        "step": 2999
    },
    {
        "loss": 1.2545,
        "grad_norm": 3.0728650093078613,
        "learning_rate": 7.041806147179322e-05,
        "epoch": 0.4,
        "step": 3000
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.711167573928833,
        "learning_rate": 7.035794739080233e-05,
        "epoch": 0.40013333333333334,
        "step": 3001
    },
    {
        "loss": 2.6347,
        "grad_norm": 2.429187297821045,
        "learning_rate": 7.029784505112944e-05,
        "epoch": 0.40026666666666666,
        "step": 3002
    },
    {
        "loss": 2.609,
        "grad_norm": 2.7614667415618896,
        "learning_rate": 7.023775447658137e-05,
        "epoch": 0.4004,
        "step": 3003
    },
    {
        "loss": 2.0738,
        "grad_norm": 3.657014846801758,
        "learning_rate": 7.017767569096016e-05,
        "epoch": 0.40053333333333335,
        "step": 3004
    },
    {
        "loss": 1.1657,
        "grad_norm": 4.100151538848877,
        "learning_rate": 7.011760871806319e-05,
        "epoch": 0.40066666666666667,
        "step": 3005
    },
    {
        "loss": 2.7017,
        "grad_norm": 4.212268352508545,
        "learning_rate": 7.005755358168327e-05,
        "epoch": 0.4008,
        "step": 3006
    },
    {
        "loss": 2.9276,
        "grad_norm": 2.4714879989624023,
        "learning_rate": 6.99975103056084e-05,
        "epoch": 0.4009333333333333,
        "step": 3007
    },
    {
        "loss": 2.8103,
        "grad_norm": 3.4338366985321045,
        "learning_rate": 6.993747891362193e-05,
        "epoch": 0.4010666666666667,
        "step": 3008
    },
    {
        "loss": 1.9986,
        "grad_norm": 3.4450535774230957,
        "learning_rate": 6.987745942950248e-05,
        "epoch": 0.4012,
        "step": 3009
    },
    {
        "loss": 1.6317,
        "grad_norm": 5.963830471038818,
        "learning_rate": 6.981745187702397e-05,
        "epoch": 0.4013333333333333,
        "step": 3010
    },
    {
        "loss": 2.7561,
        "grad_norm": 3.0795648097991943,
        "learning_rate": 6.975745627995566e-05,
        "epoch": 0.4014666666666667,
        "step": 3011
    },
    {
        "loss": 3.2188,
        "grad_norm": 2.263751983642578,
        "learning_rate": 6.969747266206196e-05,
        "epoch": 0.4016,
        "step": 3012
    },
    {
        "loss": 2.1088,
        "grad_norm": 2.6365115642547607,
        "learning_rate": 6.963750104710255e-05,
        "epoch": 0.40173333333333333,
        "step": 3013
    },
    {
        "loss": 2.4932,
        "grad_norm": 2.5746285915374756,
        "learning_rate": 6.95775414588325e-05,
        "epoch": 0.40186666666666665,
        "step": 3014
    },
    {
        "loss": 2.1409,
        "grad_norm": 2.106450080871582,
        "learning_rate": 6.95175939210019e-05,
        "epoch": 0.402,
        "step": 3015
    },
    {
        "loss": 2.4771,
        "grad_norm": 2.2695257663726807,
        "learning_rate": 6.945765845735624e-05,
        "epoch": 0.40213333333333334,
        "step": 3016
    },
    {
        "loss": 1.075,
        "grad_norm": 3.5114455223083496,
        "learning_rate": 6.939773509163611e-05,
        "epoch": 0.40226666666666666,
        "step": 3017
    },
    {
        "loss": 1.8366,
        "grad_norm": 5.974704742431641,
        "learning_rate": 6.933782384757736e-05,
        "epoch": 0.4024,
        "step": 3018
    },
    {
        "loss": 2.5958,
        "grad_norm": 3.174062728881836,
        "learning_rate": 6.927792474891111e-05,
        "epoch": 0.40253333333333335,
        "step": 3019
    },
    {
        "loss": 2.8789,
        "grad_norm": 2.20862078666687,
        "learning_rate": 6.921803781936351e-05,
        "epoch": 0.4026666666666667,
        "step": 3020
    },
    {
        "loss": 2.0863,
        "grad_norm": 3.8445217609405518,
        "learning_rate": 6.915816308265604e-05,
        "epoch": 0.4028,
        "step": 3021
    },
    {
        "loss": 2.5922,
        "grad_norm": 3.240352153778076,
        "learning_rate": 6.909830056250524e-05,
        "epoch": 0.4029333333333333,
        "step": 3022
    },
    {
        "loss": 2.8529,
        "grad_norm": 3.1018617153167725,
        "learning_rate": 6.903845028262291e-05,
        "epoch": 0.4030666666666667,
        "step": 3023
    },
    {
        "loss": 2.9482,
        "grad_norm": 3.228055000305176,
        "learning_rate": 6.897861226671593e-05,
        "epoch": 0.4032,
        "step": 3024
    },
    {
        "loss": 2.0055,
        "grad_norm": 5.199459075927734,
        "learning_rate": 6.891878653848631e-05,
        "epoch": 0.4033333333333333,
        "step": 3025
    },
    {
        "loss": 1.9576,
        "grad_norm": 4.544366836547852,
        "learning_rate": 6.88589731216313e-05,
        "epoch": 0.40346666666666664,
        "step": 3026
    },
    {
        "loss": 1.9061,
        "grad_norm": 3.698395252227783,
        "learning_rate": 6.879917203984307e-05,
        "epoch": 0.4036,
        "step": 3027
    },
    {
        "loss": 2.8587,
        "grad_norm": 5.722721576690674,
        "learning_rate": 6.873938331680915e-05,
        "epoch": 0.40373333333333333,
        "step": 3028
    },
    {
        "loss": 1.9783,
        "grad_norm": 4.605923652648926,
        "learning_rate": 6.867960697621203e-05,
        "epoch": 0.40386666666666665,
        "step": 3029
    },
    {
        "loss": 1.6883,
        "grad_norm": 3.0923871994018555,
        "learning_rate": 6.861984304172927e-05,
        "epoch": 0.404,
        "step": 3030
    },
    {
        "loss": 1.9712,
        "grad_norm": 3.1325528621673584,
        "learning_rate": 6.856009153703363e-05,
        "epoch": 0.40413333333333334,
        "step": 3031
    },
    {
        "loss": 2.4971,
        "grad_norm": 3.521165132522583,
        "learning_rate": 6.850035248579286e-05,
        "epoch": 0.40426666666666666,
        "step": 3032
    },
    {
        "loss": 2.6349,
        "grad_norm": 2.877460241317749,
        "learning_rate": 6.844062591166981e-05,
        "epoch": 0.4044,
        "step": 3033
    },
    {
        "loss": 1.0811,
        "grad_norm": 2.4915382862091064,
        "learning_rate": 6.838091183832238e-05,
        "epoch": 0.40453333333333336,
        "step": 3034
    },
    {
        "loss": 1.1491,
        "grad_norm": 2.968644857406616,
        "learning_rate": 6.832121028940346e-05,
        "epoch": 0.4046666666666667,
        "step": 3035
    },
    {
        "loss": 2.0686,
        "grad_norm": 4.727457523345947,
        "learning_rate": 6.826152128856112e-05,
        "epoch": 0.4048,
        "step": 3036
    },
    {
        "loss": 2.405,
        "grad_norm": 4.100484848022461,
        "learning_rate": 6.820184485943837e-05,
        "epoch": 0.4049333333333333,
        "step": 3037
    },
    {
        "loss": 2.696,
        "grad_norm": 3.4553050994873047,
        "learning_rate": 6.814218102567316e-05,
        "epoch": 0.4050666666666667,
        "step": 3038
    },
    {
        "loss": 2.6999,
        "grad_norm": 3.5979249477386475,
        "learning_rate": 6.808252981089866e-05,
        "epoch": 0.4052,
        "step": 3039
    },
    {
        "loss": 2.2617,
        "grad_norm": 2.8969995975494385,
        "learning_rate": 6.802289123874283e-05,
        "epoch": 0.4053333333333333,
        "step": 3040
    },
    {
        "loss": 2.2593,
        "grad_norm": 3.5095503330230713,
        "learning_rate": 6.796326533282873e-05,
        "epoch": 0.40546666666666664,
        "step": 3041
    },
    {
        "loss": 2.404,
        "grad_norm": 1.5630465745925903,
        "learning_rate": 6.79036521167744e-05,
        "epoch": 0.4056,
        "step": 3042
    },
    {
        "loss": 3.0359,
        "grad_norm": 3.2342824935913086,
        "learning_rate": 6.784405161419286e-05,
        "epoch": 0.40573333333333333,
        "step": 3043
    },
    {
        "loss": 2.2462,
        "grad_norm": 3.296967029571533,
        "learning_rate": 6.778446384869199e-05,
        "epoch": 0.40586666666666665,
        "step": 3044
    },
    {
        "loss": 0.9183,
        "grad_norm": 3.8866307735443115,
        "learning_rate": 6.77248888438748e-05,
        "epoch": 0.406,
        "step": 3045
    },
    {
        "loss": 2.7391,
        "grad_norm": 2.5606119632720947,
        "learning_rate": 6.76653266233391e-05,
        "epoch": 0.40613333333333335,
        "step": 3046
    },
    {
        "loss": 1.7517,
        "grad_norm": 5.06302547454834,
        "learning_rate": 6.76057772106777e-05,
        "epoch": 0.40626666666666666,
        "step": 3047
    },
    {
        "loss": 3.2185,
        "grad_norm": 3.331599235534668,
        "learning_rate": 6.754624062947835e-05,
        "epoch": 0.4064,
        "step": 3048
    },
    {
        "loss": 2.6061,
        "grad_norm": 2.4435300827026367,
        "learning_rate": 6.748671690332365e-05,
        "epoch": 0.40653333333333336,
        "step": 3049
    },
    {
        "loss": 2.6365,
        "grad_norm": 3.352367401123047,
        "learning_rate": 6.742720605579121e-05,
        "epoch": 0.4066666666666667,
        "step": 3050
    },
    {
        "loss": 1.3352,
        "grad_norm": 6.113246917724609,
        "learning_rate": 6.736770811045339e-05,
        "epoch": 0.4068,
        "step": 3051
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.4764256477355957,
        "learning_rate": 6.730822309087756e-05,
        "epoch": 0.4069333333333333,
        "step": 3052
    },
    {
        "loss": 2.9286,
        "grad_norm": 3.2031514644622803,
        "learning_rate": 6.724875102062602e-05,
        "epoch": 0.4070666666666667,
        "step": 3053
    },
    {
        "loss": 1.5628,
        "grad_norm": 4.18702507019043,
        "learning_rate": 6.718929192325579e-05,
        "epoch": 0.4072,
        "step": 3054
    },
    {
        "loss": 3.0461,
        "grad_norm": 1.806584119796753,
        "learning_rate": 6.712984582231879e-05,
        "epoch": 0.4073333333333333,
        "step": 3055
    },
    {
        "loss": 1.7861,
        "grad_norm": 3.338099956512451,
        "learning_rate": 6.707041274136189e-05,
        "epoch": 0.40746666666666664,
        "step": 3056
    },
    {
        "loss": 2.3432,
        "grad_norm": 2.9052953720092773,
        "learning_rate": 6.70109927039267e-05,
        "epoch": 0.4076,
        "step": 3057
    },
    {
        "loss": 1.8921,
        "grad_norm": 3.7159945964813232,
        "learning_rate": 6.695158573354972e-05,
        "epoch": 0.40773333333333334,
        "step": 3058
    },
    {
        "loss": 1.9944,
        "grad_norm": 2.512700080871582,
        "learning_rate": 6.689219185376226e-05,
        "epoch": 0.40786666666666666,
        "step": 3059
    },
    {
        "loss": 2.4031,
        "grad_norm": 2.426896810531616,
        "learning_rate": 6.683281108809034e-05,
        "epoch": 0.408,
        "step": 3060
    },
    {
        "loss": 1.5709,
        "grad_norm": 5.767634868621826,
        "learning_rate": 6.677344346005498e-05,
        "epoch": 0.40813333333333335,
        "step": 3061
    },
    {
        "loss": 2.4159,
        "grad_norm": 3.575730562210083,
        "learning_rate": 6.671408899317193e-05,
        "epoch": 0.40826666666666667,
        "step": 3062
    },
    {
        "loss": 2.1798,
        "grad_norm": 2.794356346130371,
        "learning_rate": 6.665474771095164e-05,
        "epoch": 0.4084,
        "step": 3063
    },
    {
        "loss": 2.6732,
        "grad_norm": 3.128155469894409,
        "learning_rate": 6.659541963689938e-05,
        "epoch": 0.40853333333333336,
        "step": 3064
    },
    {
        "loss": 2.6708,
        "grad_norm": 4.580227851867676,
        "learning_rate": 6.653610479451524e-05,
        "epoch": 0.4086666666666667,
        "step": 3065
    },
    {
        "loss": 2.5611,
        "grad_norm": 2.173039674758911,
        "learning_rate": 6.647680320729403e-05,
        "epoch": 0.4088,
        "step": 3066
    },
    {
        "loss": 2.183,
        "grad_norm": 3.1987931728363037,
        "learning_rate": 6.64175148987253e-05,
        "epoch": 0.4089333333333333,
        "step": 3067
    },
    {
        "loss": 1.9798,
        "grad_norm": 1.697718620300293,
        "learning_rate": 6.635823989229333e-05,
        "epoch": 0.4090666666666667,
        "step": 3068
    },
    {
        "loss": 2.7736,
        "grad_norm": 3.830456495285034,
        "learning_rate": 6.629897821147715e-05,
        "epoch": 0.4092,
        "step": 3069
    },
    {
        "loss": 2.8208,
        "grad_norm": 2.314422369003296,
        "learning_rate": 6.623972987975055e-05,
        "epoch": 0.4093333333333333,
        "step": 3070
    },
    {
        "loss": 0.9156,
        "grad_norm": 2.8987345695495605,
        "learning_rate": 6.618049492058201e-05,
        "epoch": 0.40946666666666665,
        "step": 3071
    },
    {
        "loss": 2.4474,
        "grad_norm": 3.1788716316223145,
        "learning_rate": 6.612127335743461e-05,
        "epoch": 0.4096,
        "step": 3072
    },
    {
        "loss": 2.6636,
        "grad_norm": 2.149885892868042,
        "learning_rate": 6.606206521376632e-05,
        "epoch": 0.40973333333333334,
        "step": 3073
    },
    {
        "loss": 2.291,
        "grad_norm": 3.6059069633483887,
        "learning_rate": 6.600287051302963e-05,
        "epoch": 0.40986666666666666,
        "step": 3074
    },
    {
        "loss": 2.9392,
        "grad_norm": 3.0048604011535645,
        "learning_rate": 6.59436892786718e-05,
        "epoch": 0.41,
        "step": 3075
    },
    {
        "loss": 2.4654,
        "grad_norm": 3.2936947345733643,
        "learning_rate": 6.588452153413465e-05,
        "epoch": 0.41013333333333335,
        "step": 3076
    },
    {
        "loss": 1.826,
        "grad_norm": 1.5307517051696777,
        "learning_rate": 6.582536730285476e-05,
        "epoch": 0.41026666666666667,
        "step": 3077
    },
    {
        "loss": 2.2328,
        "grad_norm": 2.6658425331115723,
        "learning_rate": 6.576622660826334e-05,
        "epoch": 0.4104,
        "step": 3078
    },
    {
        "loss": 2.4732,
        "grad_norm": 3.4970908164978027,
        "learning_rate": 6.570709947378622e-05,
        "epoch": 0.4105333333333333,
        "step": 3079
    },
    {
        "loss": 2.698,
        "grad_norm": 3.9950778484344482,
        "learning_rate": 6.564798592284383e-05,
        "epoch": 0.4106666666666667,
        "step": 3080
    },
    {
        "loss": 2.4733,
        "grad_norm": 3.6786129474639893,
        "learning_rate": 6.558888597885125e-05,
        "epoch": 0.4108,
        "step": 3081
    },
    {
        "loss": 1.711,
        "grad_norm": 2.0611793994903564,
        "learning_rate": 6.552979966521824e-05,
        "epoch": 0.4109333333333333,
        "step": 3082
    },
    {
        "loss": 2.8438,
        "grad_norm": 2.432094097137451,
        "learning_rate": 6.547072700534901e-05,
        "epoch": 0.4110666666666667,
        "step": 3083
    },
    {
        "loss": 1.6938,
        "grad_norm": 3.8478212356567383,
        "learning_rate": 6.541166802264247e-05,
        "epoch": 0.4112,
        "step": 3084
    },
    {
        "loss": 2.139,
        "grad_norm": 3.060051918029785,
        "learning_rate": 6.53526227404921e-05,
        "epoch": 0.41133333333333333,
        "step": 3085
    },
    {
        "loss": 2.6812,
        "grad_norm": 3.4649593830108643,
        "learning_rate": 6.529359118228589e-05,
        "epoch": 0.41146666666666665,
        "step": 3086
    },
    {
        "loss": 2.2434,
        "grad_norm": 3.9411327838897705,
        "learning_rate": 6.523457337140648e-05,
        "epoch": 0.4116,
        "step": 3087
    },
    {
        "loss": 2.0119,
        "grad_norm": 3.528782844543457,
        "learning_rate": 6.517556933123106e-05,
        "epoch": 0.41173333333333334,
        "step": 3088
    },
    {
        "loss": 2.4194,
        "grad_norm": 3.0749125480651855,
        "learning_rate": 6.51165790851312e-05,
        "epoch": 0.41186666666666666,
        "step": 3089
    },
    {
        "loss": 2.5582,
        "grad_norm": 2.761768341064453,
        "learning_rate": 6.505760265647329e-05,
        "epoch": 0.412,
        "step": 3090
    },
    {
        "loss": 1.3025,
        "grad_norm": 4.169056415557861,
        "learning_rate": 6.499864006861804e-05,
        "epoch": 0.41213333333333335,
        "step": 3091
    },
    {
        "loss": 2.1734,
        "grad_norm": 2.7989718914031982,
        "learning_rate": 6.493969134492072e-05,
        "epoch": 0.41226666666666667,
        "step": 3092
    },
    {
        "loss": 2.1486,
        "grad_norm": 3.3648295402526855,
        "learning_rate": 6.488075650873111e-05,
        "epoch": 0.4124,
        "step": 3093
    },
    {
        "loss": 1.7394,
        "grad_norm": 4.582315444946289,
        "learning_rate": 6.48218355833935e-05,
        "epoch": 0.4125333333333333,
        "step": 3094
    },
    {
        "loss": 1.9385,
        "grad_norm": 3.8642423152923584,
        "learning_rate": 6.476292859224669e-05,
        "epoch": 0.4126666666666667,
        "step": 3095
    },
    {
        "loss": 2.247,
        "grad_norm": 4.436244487762451,
        "learning_rate": 6.470403555862394e-05,
        "epoch": 0.4128,
        "step": 3096
    },
    {
        "loss": 2.5742,
        "grad_norm": 3.6829440593719482,
        "learning_rate": 6.464515650585294e-05,
        "epoch": 0.4129333333333333,
        "step": 3097
    },
    {
        "loss": 2.8396,
        "grad_norm": 2.4415535926818848,
        "learning_rate": 6.458629145725596e-05,
        "epoch": 0.4130666666666667,
        "step": 3098
    },
    {
        "loss": 2.217,
        "grad_norm": 2.4884486198425293,
        "learning_rate": 6.452744043614959e-05,
        "epoch": 0.4132,
        "step": 3099
    },
    {
        "loss": 2.4043,
        "grad_norm": 3.6254451274871826,
        "learning_rate": 6.446860346584496e-05,
        "epoch": 0.41333333333333333,
        "step": 3100
    },
    {
        "loss": 2.7464,
        "grad_norm": 2.9411988258361816,
        "learning_rate": 6.440978056964749e-05,
        "epoch": 0.41346666666666665,
        "step": 3101
    },
    {
        "loss": 1.9127,
        "grad_norm": 4.440905570983887,
        "learning_rate": 6.435097177085728e-05,
        "epoch": 0.4136,
        "step": 3102
    },
    {
        "loss": 2.5706,
        "grad_norm": 2.165858268737793,
        "learning_rate": 6.429217709276856e-05,
        "epoch": 0.41373333333333334,
        "step": 3103
    },
    {
        "loss": 1.7274,
        "grad_norm": 3.2512102127075195,
        "learning_rate": 6.423339655867024e-05,
        "epoch": 0.41386666666666666,
        "step": 3104
    },
    {
        "loss": 0.6894,
        "grad_norm": 3.125007390975952,
        "learning_rate": 6.417463019184538e-05,
        "epoch": 0.414,
        "step": 3105
    },
    {
        "loss": 1.3666,
        "grad_norm": 2.9936492443084717,
        "learning_rate": 6.411587801557154e-05,
        "epoch": 0.41413333333333335,
        "step": 3106
    },
    {
        "loss": 1.8505,
        "grad_norm": 4.506079196929932,
        "learning_rate": 6.405714005312074e-05,
        "epoch": 0.41426666666666667,
        "step": 3107
    },
    {
        "loss": 2.2545,
        "grad_norm": 2.149723529815674,
        "learning_rate": 6.399841632775922e-05,
        "epoch": 0.4144,
        "step": 3108
    },
    {
        "loss": 2.7342,
        "grad_norm": 2.346641778945923,
        "learning_rate": 6.393970686274765e-05,
        "epoch": 0.4145333333333333,
        "step": 3109
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.680546283721924,
        "learning_rate": 6.38810116813411e-05,
        "epoch": 0.4146666666666667,
        "step": 3110
    },
    {
        "loss": 2.2704,
        "grad_norm": 3.0435023307800293,
        "learning_rate": 6.382233080678885e-05,
        "epoch": 0.4148,
        "step": 3111
    },
    {
        "loss": 2.8153,
        "grad_norm": 3.263629913330078,
        "learning_rate": 6.376366426233466e-05,
        "epoch": 0.4149333333333333,
        "step": 3112
    },
    {
        "loss": 1.7533,
        "grad_norm": 2.4399147033691406,
        "learning_rate": 6.370501207121656e-05,
        "epoch": 0.41506666666666664,
        "step": 3113
    },
    {
        "loss": 1.2755,
        "grad_norm": 4.2704057693481445,
        "learning_rate": 6.364637425666679e-05,
        "epoch": 0.4152,
        "step": 3114
    },
    {
        "loss": 2.6066,
        "grad_norm": 2.3251514434814453,
        "learning_rate": 6.35877508419121e-05,
        "epoch": 0.41533333333333333,
        "step": 3115
    },
    {
        "loss": 2.196,
        "grad_norm": 3.584541082382202,
        "learning_rate": 6.352914185017335e-05,
        "epoch": 0.41546666666666665,
        "step": 3116
    },
    {
        "loss": 2.6559,
        "grad_norm": 2.5133280754089355,
        "learning_rate": 6.34705473046658e-05,
        "epoch": 0.4156,
        "step": 3117
    },
    {
        "loss": 2.6134,
        "grad_norm": 4.442665100097656,
        "learning_rate": 6.341196722859892e-05,
        "epoch": 0.41573333333333334,
        "step": 3118
    },
    {
        "loss": 2.4932,
        "grad_norm": 3.5062255859375,
        "learning_rate": 6.335340164517648e-05,
        "epoch": 0.41586666666666666,
        "step": 3119
    },
    {
        "loss": 2.7231,
        "grad_norm": 1.9073470830917358,
        "learning_rate": 6.329485057759654e-05,
        "epoch": 0.416,
        "step": 3120
    },
    {
        "loss": 2.4918,
        "grad_norm": 3.406606435775757,
        "learning_rate": 6.323631404905132e-05,
        "epoch": 0.41613333333333336,
        "step": 3121
    },
    {
        "loss": 1.5724,
        "grad_norm": 4.39762544631958,
        "learning_rate": 6.31777920827274e-05,
        "epoch": 0.4162666666666667,
        "step": 3122
    },
    {
        "loss": 1.1685,
        "grad_norm": 3.5242326259613037,
        "learning_rate": 6.311928470180551e-05,
        "epoch": 0.4164,
        "step": 3123
    },
    {
        "loss": 2.0959,
        "grad_norm": 2.851299524307251,
        "learning_rate": 6.306079192946062e-05,
        "epoch": 0.4165333333333333,
        "step": 3124
    },
    {
        "loss": 2.7213,
        "grad_norm": 3.3746702671051025,
        "learning_rate": 6.30023137888619e-05,
        "epoch": 0.4166666666666667,
        "step": 3125
    },
    {
        "loss": 2.9513,
        "grad_norm": 3.542609453201294,
        "learning_rate": 6.294385030317277e-05,
        "epoch": 0.4168,
        "step": 3126
    },
    {
        "loss": 1.6893,
        "grad_norm": 4.732405662536621,
        "learning_rate": 6.288540149555083e-05,
        "epoch": 0.4169333333333333,
        "step": 3127
    },
    {
        "loss": 2.4321,
        "grad_norm": 3.5384840965270996,
        "learning_rate": 6.282696738914772e-05,
        "epoch": 0.41706666666666664,
        "step": 3128
    },
    {
        "loss": 2.5923,
        "grad_norm": 2.594630002975464,
        "learning_rate": 6.27685480071096e-05,
        "epoch": 0.4172,
        "step": 3129
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.630366086959839,
        "learning_rate": 6.271014337257644e-05,
        "epoch": 0.41733333333333333,
        "step": 3130
    },
    {
        "loss": 3.0117,
        "grad_norm": 2.988873243331909,
        "learning_rate": 6.265175350868253e-05,
        "epoch": 0.41746666666666665,
        "step": 3131
    },
    {
        "loss": 1.0631,
        "grad_norm": 3.685478448867798,
        "learning_rate": 6.259337843855633e-05,
        "epoch": 0.4176,
        "step": 3132
    },
    {
        "loss": 2.7602,
        "grad_norm": 2.139151096343994,
        "learning_rate": 6.253501818532039e-05,
        "epoch": 0.41773333333333335,
        "step": 3133
    },
    {
        "loss": 2.5854,
        "grad_norm": 2.782397747039795,
        "learning_rate": 6.247667277209142e-05,
        "epoch": 0.41786666666666666,
        "step": 3134
    },
    {
        "loss": 0.948,
        "grad_norm": 2.820124387741089,
        "learning_rate": 6.241834222198024e-05,
        "epoch": 0.418,
        "step": 3135
    },
    {
        "loss": 2.4607,
        "grad_norm": 3.027623414993286,
        "learning_rate": 6.23600265580917e-05,
        "epoch": 0.41813333333333336,
        "step": 3136
    },
    {
        "loss": 3.0094,
        "grad_norm": 3.975419759750366,
        "learning_rate": 6.230172580352493e-05,
        "epoch": 0.4182666666666667,
        "step": 3137
    },
    {
        "loss": 2.8835,
        "grad_norm": 3.519052505493164,
        "learning_rate": 6.224343998137305e-05,
        "epoch": 0.4184,
        "step": 3138
    },
    {
        "loss": 2.7238,
        "grad_norm": 3.327723979949951,
        "learning_rate": 6.218516911472318e-05,
        "epoch": 0.4185333333333333,
        "step": 3139
    },
    {
        "loss": 2.715,
        "grad_norm": 3.4745469093322754,
        "learning_rate": 6.212691322665674e-05,
        "epoch": 0.4186666666666667,
        "step": 3140
    },
    {
        "loss": 2.4852,
        "grad_norm": 3.4567110538482666,
        "learning_rate": 6.206867234024895e-05,
        "epoch": 0.4188,
        "step": 3141
    },
    {
        "loss": 2.4958,
        "grad_norm": 2.7151811122894287,
        "learning_rate": 6.201044647856935e-05,
        "epoch": 0.4189333333333333,
        "step": 3142
    },
    {
        "loss": 1.9846,
        "grad_norm": 2.4349586963653564,
        "learning_rate": 6.195223566468135e-05,
        "epoch": 0.41906666666666664,
        "step": 3143
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.6141090393066406,
        "learning_rate": 6.189403992164242e-05,
        "epoch": 0.4192,
        "step": 3144
    },
    {
        "loss": 2.7379,
        "grad_norm": 3.0375277996063232,
        "learning_rate": 6.183585927250412e-05,
        "epoch": 0.41933333333333334,
        "step": 3145
    },
    {
        "loss": 2.7052,
        "grad_norm": 3.5561020374298096,
        "learning_rate": 6.177769374031201e-05,
        "epoch": 0.41946666666666665,
        "step": 3146
    },
    {
        "loss": 1.6057,
        "grad_norm": 3.1301491260528564,
        "learning_rate": 6.171954334810567e-05,
        "epoch": 0.4196,
        "step": 3147
    },
    {
        "loss": 2.0262,
        "grad_norm": 3.07458758354187,
        "learning_rate": 6.16614081189186e-05,
        "epoch": 0.41973333333333335,
        "step": 3148
    },
    {
        "loss": 2.1688,
        "grad_norm": 4.294362545013428,
        "learning_rate": 6.160328807577845e-05,
        "epoch": 0.41986666666666667,
        "step": 3149
    },
    {
        "loss": 2.8814,
        "grad_norm": 2.676331043243408,
        "learning_rate": 6.154518324170673e-05,
        "epoch": 0.42,
        "step": 3150
    },
    {
        "loss": 2.735,
        "grad_norm": 2.2284326553344727,
        "learning_rate": 6.148709363971896e-05,
        "epoch": 0.42013333333333336,
        "step": 3151
    },
    {
        "loss": 2.7485,
        "grad_norm": 3.4321365356445312,
        "learning_rate": 6.142901929282462e-05,
        "epoch": 0.4202666666666667,
        "step": 3152
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.488778591156006,
        "learning_rate": 6.137096022402713e-05,
        "epoch": 0.4204,
        "step": 3153
    },
    {
        "loss": 2.3577,
        "grad_norm": 3.8226499557495117,
        "learning_rate": 6.131291645632398e-05,
        "epoch": 0.4205333333333333,
        "step": 3154
    },
    {
        "loss": 2.592,
        "grad_norm": 3.0498595237731934,
        "learning_rate": 6.12548880127064e-05,
        "epoch": 0.4206666666666667,
        "step": 3155
    },
    {
        "loss": 2.1988,
        "grad_norm": 3.5135691165924072,
        "learning_rate": 6.11968749161597e-05,
        "epoch": 0.4208,
        "step": 3156
    },
    {
        "loss": 2.1414,
        "grad_norm": 3.135874032974243,
        "learning_rate": 6.113887718966309e-05,
        "epoch": 0.4209333333333333,
        "step": 3157
    },
    {
        "loss": 1.152,
        "grad_norm": 3.8243601322174072,
        "learning_rate": 6.108089485618963e-05,
        "epoch": 0.42106666666666664,
        "step": 3158
    },
    {
        "loss": 3.3556,
        "grad_norm": 3.9769949913024902,
        "learning_rate": 6.102292793870634e-05,
        "epoch": 0.4212,
        "step": 3159
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.4946985244750977,
        "learning_rate": 6.096497646017412e-05,
        "epoch": 0.42133333333333334,
        "step": 3160
    },
    {
        "loss": 1.6249,
        "grad_norm": 3.672640562057495,
        "learning_rate": 6.090704044354767e-05,
        "epoch": 0.42146666666666666,
        "step": 3161
    },
    {
        "loss": 2.1356,
        "grad_norm": 3.555758476257324,
        "learning_rate": 6.084911991177572e-05,
        "epoch": 0.4216,
        "step": 3162
    },
    {
        "loss": 2.3607,
        "grad_norm": 3.8722620010375977,
        "learning_rate": 6.0791214887800806e-05,
        "epoch": 0.42173333333333335,
        "step": 3163
    },
    {
        "loss": 1.8496,
        "grad_norm": 3.5740952491760254,
        "learning_rate": 6.073332539455927e-05,
        "epoch": 0.42186666666666667,
        "step": 3164
    },
    {
        "loss": 2.1536,
        "grad_norm": 2.9452321529388428,
        "learning_rate": 6.0675451454981326e-05,
        "epoch": 0.422,
        "step": 3165
    },
    {
        "loss": 2.6324,
        "grad_norm": 3.1875343322753906,
        "learning_rate": 6.0617593091991086e-05,
        "epoch": 0.42213333333333336,
        "step": 3166
    },
    {
        "loss": 2.6551,
        "grad_norm": 2.7398934364318848,
        "learning_rate": 6.055975032850643e-05,
        "epoch": 0.4222666666666667,
        "step": 3167
    },
    {
        "loss": 2.0126,
        "grad_norm": 2.96370005607605,
        "learning_rate": 6.050192318743903e-05,
        "epoch": 0.4224,
        "step": 3168
    },
    {
        "loss": 2.2743,
        "grad_norm": 3.4110546112060547,
        "learning_rate": 6.044411169169444e-05,
        "epoch": 0.4225333333333333,
        "step": 3169
    },
    {
        "loss": 0.949,
        "grad_norm": 3.3877599239349365,
        "learning_rate": 6.0386315864171984e-05,
        "epoch": 0.4226666666666667,
        "step": 3170
    },
    {
        "loss": 2.4738,
        "grad_norm": 3.451666831970215,
        "learning_rate": 6.03285357277648e-05,
        "epoch": 0.4228,
        "step": 3171
    },
    {
        "loss": 2.6044,
        "grad_norm": 3.537628650665283,
        "learning_rate": 6.0270771305359795e-05,
        "epoch": 0.42293333333333333,
        "step": 3172
    },
    {
        "loss": 1.9397,
        "grad_norm": 3.224074363708496,
        "learning_rate": 6.021302261983763e-05,
        "epoch": 0.42306666666666665,
        "step": 3173
    },
    {
        "loss": 2.487,
        "grad_norm": 2.781765937805176,
        "learning_rate": 6.01552896940728e-05,
        "epoch": 0.4232,
        "step": 3174
    },
    {
        "loss": 2.7543,
        "grad_norm": 5.078343868255615,
        "learning_rate": 6.00975725509335e-05,
        "epoch": 0.42333333333333334,
        "step": 3175
    },
    {
        "loss": 1.9036,
        "grad_norm": 2.8864004611968994,
        "learning_rate": 6.0039871213281685e-05,
        "epoch": 0.42346666666666666,
        "step": 3176
    },
    {
        "loss": 2.4753,
        "grad_norm": 2.6738033294677734,
        "learning_rate": 5.998218570397303e-05,
        "epoch": 0.4236,
        "step": 3177
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.3996713161468506,
        "learning_rate": 5.992451604585695e-05,
        "epoch": 0.42373333333333335,
        "step": 3178
    },
    {
        "loss": 2.0525,
        "grad_norm": 2.322436809539795,
        "learning_rate": 5.9866862261776634e-05,
        "epoch": 0.42386666666666667,
        "step": 3179
    },
    {
        "loss": 2.7208,
        "grad_norm": 3.3593761920928955,
        "learning_rate": 5.980922437456893e-05,
        "epoch": 0.424,
        "step": 3180
    },
    {
        "loss": 2.8216,
        "grad_norm": 4.166904926300049,
        "learning_rate": 5.9751602407064374e-05,
        "epoch": 0.4241333333333333,
        "step": 3181
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.257568597793579,
        "learning_rate": 5.9693996382087235e-05,
        "epoch": 0.4242666666666667,
        "step": 3182
    },
    {
        "loss": 2.5545,
        "grad_norm": 2.293782949447632,
        "learning_rate": 5.963640632245553e-05,
        "epoch": 0.4244,
        "step": 3183
    },
    {
        "loss": 2.9672,
        "grad_norm": 3.30403733253479,
        "learning_rate": 5.9578832250980796e-05,
        "epoch": 0.4245333333333333,
        "step": 3184
    },
    {
        "loss": 2.419,
        "grad_norm": 2.7624359130859375,
        "learning_rate": 5.952127419046833e-05,
        "epoch": 0.4246666666666667,
        "step": 3185
    },
    {
        "loss": 0.9275,
        "grad_norm": 3.6240158081054688,
        "learning_rate": 5.946373216371709e-05,
        "epoch": 0.4248,
        "step": 3186
    },
    {
        "loss": 2.1729,
        "grad_norm": 3.3590338230133057,
        "learning_rate": 5.940620619351964e-05,
        "epoch": 0.42493333333333333,
        "step": 3187
    },
    {
        "loss": 2.1479,
        "grad_norm": 2.6419482231140137,
        "learning_rate": 5.9348696302662266e-05,
        "epoch": 0.42506666666666665,
        "step": 3188
    },
    {
        "loss": 1.5072,
        "grad_norm": 3.6489362716674805,
        "learning_rate": 5.929120251392482e-05,
        "epoch": 0.4252,
        "step": 3189
    },
    {
        "loss": 2.215,
        "grad_norm": 3.829202890396118,
        "learning_rate": 5.9233724850080694e-05,
        "epoch": 0.42533333333333334,
        "step": 3190
    },
    {
        "loss": 1.4544,
        "grad_norm": 4.715993404388428,
        "learning_rate": 5.917626333389712e-05,
        "epoch": 0.42546666666666666,
        "step": 3191
    },
    {
        "loss": 2.3081,
        "grad_norm": 2.989621162414551,
        "learning_rate": 5.9118817988134766e-05,
        "epoch": 0.4256,
        "step": 3192
    },
    {
        "loss": 2.7097,
        "grad_norm": 1.9241026639938354,
        "learning_rate": 5.906138883554788e-05,
        "epoch": 0.42573333333333335,
        "step": 3193
    },
    {
        "loss": 2.31,
        "grad_norm": 3.7408547401428223,
        "learning_rate": 5.9003975898884365e-05,
        "epoch": 0.42586666666666667,
        "step": 3194
    },
    {
        "loss": 0.7213,
        "grad_norm": 2.994208574295044,
        "learning_rate": 5.8946579200885666e-05,
        "epoch": 0.426,
        "step": 3195
    },
    {
        "loss": 2.9287,
        "grad_norm": 3.50244140625,
        "learning_rate": 5.888919876428687e-05,
        "epoch": 0.4261333333333333,
        "step": 3196
    },
    {
        "loss": 2.6993,
        "grad_norm": 1.8589580059051514,
        "learning_rate": 5.883183461181651e-05,
        "epoch": 0.4262666666666667,
        "step": 3197
    },
    {
        "loss": 0.9013,
        "grad_norm": 2.5668020248413086,
        "learning_rate": 5.877448676619672e-05,
        "epoch": 0.4264,
        "step": 3198
    },
    {
        "loss": 2.551,
        "grad_norm": 1.7957655191421509,
        "learning_rate": 5.8717155250143206e-05,
        "epoch": 0.4265333333333333,
        "step": 3199
    },
    {
        "loss": 2.705,
        "grad_norm": 3.0601747035980225,
        "learning_rate": 5.865984008636518e-05,
        "epoch": 0.4266666666666667,
        "step": 3200
    },
    {
        "loss": 2.3661,
        "grad_norm": 2.3620293140411377,
        "learning_rate": 5.860254129756537e-05,
        "epoch": 0.4268,
        "step": 3201
    },
    {
        "loss": 2.7885,
        "grad_norm": 2.51587176322937,
        "learning_rate": 5.854525890643996e-05,
        "epoch": 0.42693333333333333,
        "step": 3202
    },
    {
        "loss": 2.4137,
        "grad_norm": 2.164775848388672,
        "learning_rate": 5.848799293567879e-05,
        "epoch": 0.42706666666666665,
        "step": 3203
    },
    {
        "loss": 2.3093,
        "grad_norm": 2.7405319213867188,
        "learning_rate": 5.843074340796503e-05,
        "epoch": 0.4272,
        "step": 3204
    },
    {
        "loss": 2.5752,
        "grad_norm": 2.8120012283325195,
        "learning_rate": 5.837351034597548e-05,
        "epoch": 0.42733333333333334,
        "step": 3205
    },
    {
        "loss": 2.1761,
        "grad_norm": 3.881237268447876,
        "learning_rate": 5.831629377238034e-05,
        "epoch": 0.42746666666666666,
        "step": 3206
    },
    {
        "loss": 1.3209,
        "grad_norm": 4.083990573883057,
        "learning_rate": 5.8259093709843236e-05,
        "epoch": 0.4276,
        "step": 3207
    },
    {
        "loss": 2.6926,
        "grad_norm": 2.123903512954712,
        "learning_rate": 5.820191018102133e-05,
        "epoch": 0.42773333333333335,
        "step": 3208
    },
    {
        "loss": 1.6914,
        "grad_norm": 4.1079254150390625,
        "learning_rate": 5.814474320856528e-05,
        "epoch": 0.4278666666666667,
        "step": 3209
    },
    {
        "loss": 2.6375,
        "grad_norm": 1.731994390487671,
        "learning_rate": 5.808759281511905e-05,
        "epoch": 0.428,
        "step": 3210
    },
    {
        "loss": 2.015,
        "grad_norm": 2.8716094493865967,
        "learning_rate": 5.803045902332009e-05,
        "epoch": 0.4281333333333333,
        "step": 3211
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.297569751739502,
        "learning_rate": 5.797334185579931e-05,
        "epoch": 0.4282666666666667,
        "step": 3212
    },
    {
        "loss": 2.3464,
        "grad_norm": 3.417180061340332,
        "learning_rate": 5.7916241335181066e-05,
        "epoch": 0.4284,
        "step": 3213
    },
    {
        "loss": 2.6018,
        "grad_norm": 2.9488370418548584,
        "learning_rate": 5.7859157484082994e-05,
        "epoch": 0.4285333333333333,
        "step": 3214
    },
    {
        "loss": 1.9058,
        "grad_norm": 2.950563907623291,
        "learning_rate": 5.780209032511622e-05,
        "epoch": 0.42866666666666664,
        "step": 3215
    },
    {
        "loss": 2.8017,
        "grad_norm": 3.5285837650299072,
        "learning_rate": 5.77450398808853e-05,
        "epoch": 0.4288,
        "step": 3216
    },
    {
        "loss": 3.3107,
        "grad_norm": 2.8932905197143555,
        "learning_rate": 5.768800617398802e-05,
        "epoch": 0.42893333333333333,
        "step": 3217
    },
    {
        "loss": 1.7592,
        "grad_norm": 2.3033485412597656,
        "learning_rate": 5.763098922701572e-05,
        "epoch": 0.42906666666666665,
        "step": 3218
    },
    {
        "loss": 2.8324,
        "grad_norm": 3.12744402885437,
        "learning_rate": 5.757398906255295e-05,
        "epoch": 0.4292,
        "step": 3219
    },
    {
        "loss": 2.1954,
        "grad_norm": 3.4801669120788574,
        "learning_rate": 5.751700570317759e-05,
        "epoch": 0.42933333333333334,
        "step": 3220
    },
    {
        "loss": 1.0119,
        "grad_norm": 3.3700530529022217,
        "learning_rate": 5.746003917146112e-05,
        "epoch": 0.42946666666666666,
        "step": 3221
    },
    {
        "loss": 2.7722,
        "grad_norm": 2.39910888671875,
        "learning_rate": 5.740308948996804e-05,
        "epoch": 0.4296,
        "step": 3222
    },
    {
        "loss": 1.7715,
        "grad_norm": 4.760138511657715,
        "learning_rate": 5.734615668125639e-05,
        "epoch": 0.42973333333333336,
        "step": 3223
    },
    {
        "loss": 2.7261,
        "grad_norm": 3.1964030265808105,
        "learning_rate": 5.728924076787734e-05,
        "epoch": 0.4298666666666667,
        "step": 3224
    },
    {
        "loss": 2.5676,
        "grad_norm": 2.710655927658081,
        "learning_rate": 5.723234177237564e-05,
        "epoch": 0.43,
        "step": 3225
    },
    {
        "loss": 2.2829,
        "grad_norm": 7.33475399017334,
        "learning_rate": 5.7175459717289105e-05,
        "epoch": 0.4301333333333333,
        "step": 3226
    },
    {
        "loss": 2.843,
        "grad_norm": 2.9986391067504883,
        "learning_rate": 5.7118594625148834e-05,
        "epoch": 0.4302666666666667,
        "step": 3227
    },
    {
        "loss": 1.6229,
        "grad_norm": 3.0330939292907715,
        "learning_rate": 5.70617465184794e-05,
        "epoch": 0.4304,
        "step": 3228
    },
    {
        "loss": 2.8003,
        "grad_norm": 2.947448253631592,
        "learning_rate": 5.7004915419798434e-05,
        "epoch": 0.4305333333333333,
        "step": 3229
    },
    {
        "loss": 2.3563,
        "grad_norm": 2.6111690998077393,
        "learning_rate": 5.694810135161697e-05,
        "epoch": 0.43066666666666664,
        "step": 3230
    },
    {
        "loss": 3.136,
        "grad_norm": 2.069915294647217,
        "learning_rate": 5.68913043364393e-05,
        "epoch": 0.4308,
        "step": 3231
    },
    {
        "loss": 2.6878,
        "grad_norm": 2.920269012451172,
        "learning_rate": 5.6834524396762835e-05,
        "epoch": 0.43093333333333333,
        "step": 3232
    },
    {
        "loss": 1.2624,
        "grad_norm": 2.943563222885132,
        "learning_rate": 5.677776155507833e-05,
        "epoch": 0.43106666666666665,
        "step": 3233
    },
    {
        "loss": 2.8363,
        "grad_norm": 2.5433835983276367,
        "learning_rate": 5.672101583386979e-05,
        "epoch": 0.4312,
        "step": 3234
    },
    {
        "loss": 2.8513,
        "grad_norm": 3.364459276199341,
        "learning_rate": 5.6664287255614354e-05,
        "epoch": 0.43133333333333335,
        "step": 3235
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.9779958724975586,
        "learning_rate": 5.6607575842782355e-05,
        "epoch": 0.43146666666666667,
        "step": 3236
    },
    {
        "loss": 2.4247,
        "grad_norm": 2.1222541332244873,
        "learning_rate": 5.6550881617837435e-05,
        "epoch": 0.4316,
        "step": 3237
    },
    {
        "loss": 2.3026,
        "grad_norm": 3.142045497894287,
        "learning_rate": 5.6494204603236377e-05,
        "epoch": 0.43173333333333336,
        "step": 3238
    },
    {
        "loss": 2.2838,
        "grad_norm": 3.472583532333374,
        "learning_rate": 5.643754482142909e-05,
        "epoch": 0.4318666666666667,
        "step": 3239
    },
    {
        "loss": 2.7254,
        "grad_norm": 2.082895517349243,
        "learning_rate": 5.638090229485873e-05,
        "epoch": 0.432,
        "step": 3240
    },
    {
        "loss": 2.5527,
        "grad_norm": 3.4045631885528564,
        "learning_rate": 5.6324277045961637e-05,
        "epoch": 0.4321333333333333,
        "step": 3241
    },
    {
        "loss": 1.378,
        "grad_norm": 2.6136977672576904,
        "learning_rate": 5.6267669097167187e-05,
        "epoch": 0.4322666666666667,
        "step": 3242
    },
    {
        "loss": 3.4396,
        "grad_norm": 3.4203100204467773,
        "learning_rate": 5.6211078470898014e-05,
        "epoch": 0.4324,
        "step": 3243
    },
    {
        "loss": 2.3648,
        "grad_norm": 3.5283148288726807,
        "learning_rate": 5.615450518956993e-05,
        "epoch": 0.4325333333333333,
        "step": 3244
    },
    {
        "loss": 1.7238,
        "grad_norm": 3.0927438735961914,
        "learning_rate": 5.609794927559171e-05,
        "epoch": 0.43266666666666664,
        "step": 3245
    },
    {
        "loss": 1.9331,
        "grad_norm": 3.4012720584869385,
        "learning_rate": 5.604141075136532e-05,
        "epoch": 0.4328,
        "step": 3246
    },
    {
        "loss": 3.0941,
        "grad_norm": 2.1862709522247314,
        "learning_rate": 5.5984889639285984e-05,
        "epoch": 0.43293333333333334,
        "step": 3247
    },
    {
        "loss": 2.7278,
        "grad_norm": 3.811959981918335,
        "learning_rate": 5.592838596174184e-05,
        "epoch": 0.43306666666666666,
        "step": 3248
    },
    {
        "loss": 2.5326,
        "grad_norm": 2.7289035320281982,
        "learning_rate": 5.5871899741114106e-05,
        "epoch": 0.4332,
        "step": 3249
    },
    {
        "loss": 2.2887,
        "grad_norm": 2.7823684215545654,
        "learning_rate": 5.581543099977733e-05,
        "epoch": 0.43333333333333335,
        "step": 3250
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.8401288986206055,
        "learning_rate": 5.5758979760098895e-05,
        "epoch": 0.43346666666666667,
        "step": 3251
    },
    {
        "loss": 1.8656,
        "grad_norm": 2.780014991760254,
        "learning_rate": 5.570254604443929e-05,
        "epoch": 0.4336,
        "step": 3252
    },
    {
        "loss": 2.3541,
        "grad_norm": 2.896667003631592,
        "learning_rate": 5.5646129875152166e-05,
        "epoch": 0.43373333333333336,
        "step": 3253
    },
    {
        "loss": 2.7541,
        "grad_norm": 2.3312904834747314,
        "learning_rate": 5.558973127458409e-05,
        "epoch": 0.4338666666666667,
        "step": 3254
    },
    {
        "loss": 3.0769,
        "grad_norm": 3.010080099105835,
        "learning_rate": 5.553335026507478e-05,
        "epoch": 0.434,
        "step": 3255
    },
    {
        "loss": 1.8813,
        "grad_norm": 3.180823802947998,
        "learning_rate": 5.547698686895699e-05,
        "epoch": 0.4341333333333333,
        "step": 3256
    },
    {
        "loss": 2.6149,
        "grad_norm": 2.3603310585021973,
        "learning_rate": 5.542064110855636e-05,
        "epoch": 0.4342666666666667,
        "step": 3257
    },
    {
        "loss": 1.7881,
        "grad_norm": 3.1781327724456787,
        "learning_rate": 5.5364313006191706e-05,
        "epoch": 0.4344,
        "step": 3258
    },
    {
        "loss": 2.1592,
        "grad_norm": 3.4971017837524414,
        "learning_rate": 5.530800258417479e-05,
        "epoch": 0.4345333333333333,
        "step": 3259
    },
    {
        "loss": 1.9168,
        "grad_norm": 4.117100715637207,
        "learning_rate": 5.525170986481034e-05,
        "epoch": 0.43466666666666665,
        "step": 3260
    },
    {
        "loss": 2.1266,
        "grad_norm": 1.9183510541915894,
        "learning_rate": 5.5195434870396044e-05,
        "epoch": 0.4348,
        "step": 3261
    },
    {
        "loss": 2.1878,
        "grad_norm": 2.2278125286102295,
        "learning_rate": 5.513917762322266e-05,
        "epoch": 0.43493333333333334,
        "step": 3262
    },
    {
        "loss": 2.637,
        "grad_norm": 2.7285070419311523,
        "learning_rate": 5.508293814557388e-05,
        "epoch": 0.43506666666666666,
        "step": 3263
    },
    {
        "loss": 2.611,
        "grad_norm": 2.690659284591675,
        "learning_rate": 5.5026716459726404e-05,
        "epoch": 0.4352,
        "step": 3264
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.4406654834747314,
        "learning_rate": 5.497051258794973e-05,
        "epoch": 0.43533333333333335,
        "step": 3265
    },
    {
        "loss": 2.0538,
        "grad_norm": 3.0105643272399902,
        "learning_rate": 5.4914326552506454e-05,
        "epoch": 0.43546666666666667,
        "step": 3266
    },
    {
        "loss": 1.5567,
        "grad_norm": 4.54038143157959,
        "learning_rate": 5.48581583756521e-05,
        "epoch": 0.4356,
        "step": 3267
    },
    {
        "loss": 2.3092,
        "grad_norm": 3.7430076599121094,
        "learning_rate": 5.480200807963499e-05,
        "epoch": 0.4357333333333333,
        "step": 3268
    },
    {
        "loss": 2.8887,
        "grad_norm": 4.61463737487793,
        "learning_rate": 5.4745875686696534e-05,
        "epoch": 0.4358666666666667,
        "step": 3269
    },
    {
        "loss": 1.7823,
        "grad_norm": 3.497189521789551,
        "learning_rate": 5.46897612190709e-05,
        "epoch": 0.436,
        "step": 3270
    },
    {
        "loss": 1.8203,
        "grad_norm": 3.194352865219116,
        "learning_rate": 5.4633664698985174e-05,
        "epoch": 0.4361333333333333,
        "step": 3271
    },
    {
        "loss": 2.5707,
        "grad_norm": 2.0960888862609863,
        "learning_rate": 5.457758614865952e-05,
        "epoch": 0.4362666666666667,
        "step": 3272
    },
    {
        "loss": 1.2119,
        "grad_norm": 4.156589508056641,
        "learning_rate": 5.4521525590306766e-05,
        "epoch": 0.4364,
        "step": 3273
    },
    {
        "loss": 1.7247,
        "grad_norm": 2.2831482887268066,
        "learning_rate": 5.44654830461326e-05,
        "epoch": 0.43653333333333333,
        "step": 3274
    },
    {
        "loss": 2.4126,
        "grad_norm": 2.7498326301574707,
        "learning_rate": 5.4409458538335855e-05,
        "epoch": 0.43666666666666665,
        "step": 3275
    },
    {
        "loss": 1.3715,
        "grad_norm": 3.7400519847869873,
        "learning_rate": 5.4353452089107916e-05,
        "epoch": 0.4368,
        "step": 3276
    },
    {
        "loss": 2.5069,
        "grad_norm": 2.7170796394348145,
        "learning_rate": 5.429746372063309e-05,
        "epoch": 0.43693333333333334,
        "step": 3277
    },
    {
        "loss": 2.8994,
        "grad_norm": 3.2713074684143066,
        "learning_rate": 5.4241493455088664e-05,
        "epoch": 0.43706666666666666,
        "step": 3278
    },
    {
        "loss": 0.9948,
        "grad_norm": 4.043556213378906,
        "learning_rate": 5.418554131464456e-05,
        "epoch": 0.4372,
        "step": 3279
    },
    {
        "loss": 2.6578,
        "grad_norm": 2.549389600753784,
        "learning_rate": 5.412960732146365e-05,
        "epoch": 0.43733333333333335,
        "step": 3280
    },
    {
        "loss": 2.496,
        "grad_norm": 3.4713358879089355,
        "learning_rate": 5.407369149770162e-05,
        "epoch": 0.43746666666666667,
        "step": 3281
    },
    {
        "loss": 2.7286,
        "grad_norm": 1.8809536695480347,
        "learning_rate": 5.401779386550683e-05,
        "epoch": 0.4376,
        "step": 3282
    },
    {
        "loss": 2.3696,
        "grad_norm": 4.517948150634766,
        "learning_rate": 5.396191444702058e-05,
        "epoch": 0.4377333333333333,
        "step": 3283
    },
    {
        "loss": 1.9909,
        "grad_norm": 2.7012906074523926,
        "learning_rate": 5.390605326437688e-05,
        "epoch": 0.4378666666666667,
        "step": 3284
    },
    {
        "loss": 2.8706,
        "grad_norm": 2.791567325592041,
        "learning_rate": 5.3850210339702586e-05,
        "epoch": 0.438,
        "step": 3285
    },
    {
        "loss": 2.7921,
        "grad_norm": 1.6283694505691528,
        "learning_rate": 5.3794385695117236e-05,
        "epoch": 0.4381333333333333,
        "step": 3286
    },
    {
        "loss": 1.5475,
        "grad_norm": 3.9355967044830322,
        "learning_rate": 5.373857935273311e-05,
        "epoch": 0.4382666666666667,
        "step": 3287
    },
    {
        "loss": 2.3307,
        "grad_norm": 3.6796200275421143,
        "learning_rate": 5.368279133465533e-05,
        "epoch": 0.4384,
        "step": 3288
    },
    {
        "loss": 0.8015,
        "grad_norm": 3.691908121109009,
        "learning_rate": 5.362702166298177e-05,
        "epoch": 0.43853333333333333,
        "step": 3289
    },
    {
        "loss": 1.0764,
        "grad_norm": 2.9845733642578125,
        "learning_rate": 5.35712703598029e-05,
        "epoch": 0.43866666666666665,
        "step": 3290
    },
    {
        "loss": 2.6349,
        "grad_norm": 3.464175224304199,
        "learning_rate": 5.3515537447202036e-05,
        "epoch": 0.4388,
        "step": 3291
    },
    {
        "loss": 2.2814,
        "grad_norm": 2.888216733932495,
        "learning_rate": 5.345982294725522e-05,
        "epoch": 0.43893333333333334,
        "step": 3292
    },
    {
        "loss": 2.3917,
        "grad_norm": 2.4258153438568115,
        "learning_rate": 5.340412688203109e-05,
        "epoch": 0.43906666666666666,
        "step": 3293
    },
    {
        "loss": 2.5169,
        "grad_norm": 5.984574317932129,
        "learning_rate": 5.3348449273591106e-05,
        "epoch": 0.4392,
        "step": 3294
    },
    {
        "loss": 1.7333,
        "grad_norm": 5.917999267578125,
        "learning_rate": 5.329279014398931e-05,
        "epoch": 0.43933333333333335,
        "step": 3295
    },
    {
        "loss": 1.9738,
        "grad_norm": 3.7132482528686523,
        "learning_rate": 5.3237149515272435e-05,
        "epoch": 0.43946666666666667,
        "step": 3296
    },
    {
        "loss": 2.3029,
        "grad_norm": 2.7382569313049316,
        "learning_rate": 5.318152740948004e-05,
        "epoch": 0.4396,
        "step": 3297
    },
    {
        "loss": 2.2615,
        "grad_norm": 4.063602447509766,
        "learning_rate": 5.3125923848644175e-05,
        "epoch": 0.4397333333333333,
        "step": 3298
    },
    {
        "loss": 2.3326,
        "grad_norm": 3.669098377227783,
        "learning_rate": 5.3070338854789516e-05,
        "epoch": 0.4398666666666667,
        "step": 3299
    },
    {
        "loss": 2.7259,
        "grad_norm": 4.02163028717041,
        "learning_rate": 5.3014772449933625e-05,
        "epoch": 0.44,
        "step": 3300
    },
    {
        "loss": 2.1869,
        "grad_norm": 1.7494885921478271,
        "learning_rate": 5.295922465608646e-05,
        "epoch": 0.4401333333333333,
        "step": 3301
    },
    {
        "loss": 2.2497,
        "grad_norm": 3.7184834480285645,
        "learning_rate": 5.290369549525066e-05,
        "epoch": 0.44026666666666664,
        "step": 3302
    },
    {
        "loss": 1.3876,
        "grad_norm": 4.408283710479736,
        "learning_rate": 5.284818498942155e-05,
        "epoch": 0.4404,
        "step": 3303
    },
    {
        "loss": 2.1203,
        "grad_norm": 3.1357479095458984,
        "learning_rate": 5.279269316058707e-05,
        "epoch": 0.44053333333333333,
        "step": 3304
    },
    {
        "loss": 1.0066,
        "grad_norm": 5.6403632164001465,
        "learning_rate": 5.273722003072764e-05,
        "epoch": 0.44066666666666665,
        "step": 3305
    },
    {
        "loss": 2.091,
        "grad_norm": 3.188150644302368,
        "learning_rate": 5.268176562181641e-05,
        "epoch": 0.4408,
        "step": 3306
    },
    {
        "loss": 1.9507,
        "grad_norm": 5.198896408081055,
        "learning_rate": 5.262632995581909e-05,
        "epoch": 0.44093333333333334,
        "step": 3307
    },
    {
        "loss": 2.7475,
        "grad_norm": 2.7629339694976807,
        "learning_rate": 5.2570913054693836e-05,
        "epoch": 0.44106666666666666,
        "step": 3308
    },
    {
        "loss": 2.7113,
        "grad_norm": 3.6836774349212646,
        "learning_rate": 5.251551494039155e-05,
        "epoch": 0.4412,
        "step": 3309
    },
    {
        "loss": 2.6567,
        "grad_norm": 3.114365816116333,
        "learning_rate": 5.246013563485563e-05,
        "epoch": 0.44133333333333336,
        "step": 3310
    },
    {
        "loss": 1.9941,
        "grad_norm": 1.9687988758087158,
        "learning_rate": 5.240477516002198e-05,
        "epoch": 0.4414666666666667,
        "step": 3311
    },
    {
        "loss": 1.0582,
        "grad_norm": 4.529470443725586,
        "learning_rate": 5.2349433537819024e-05,
        "epoch": 0.4416,
        "step": 3312
    },
    {
        "loss": 2.805,
        "grad_norm": 2.7615416049957275,
        "learning_rate": 5.22941107901678e-05,
        "epoch": 0.4417333333333333,
        "step": 3313
    },
    {
        "loss": 2.7037,
        "grad_norm": 1.8413217067718506,
        "learning_rate": 5.22388069389819e-05,
        "epoch": 0.4418666666666667,
        "step": 3314
    },
    {
        "loss": 2.707,
        "grad_norm": 3.3077690601348877,
        "learning_rate": 5.2183522006167274e-05,
        "epoch": 0.442,
        "step": 3315
    },
    {
        "loss": 3.0726,
        "grad_norm": 1.6504371166229248,
        "learning_rate": 5.21282560136225e-05,
        "epoch": 0.4421333333333333,
        "step": 3316
    },
    {
        "loss": 2.1311,
        "grad_norm": 3.349578857421875,
        "learning_rate": 5.207300898323868e-05,
        "epoch": 0.44226666666666664,
        "step": 3317
    },
    {
        "loss": 2.6326,
        "grad_norm": 2.87379789352417,
        "learning_rate": 5.201778093689926e-05,
        "epoch": 0.4424,
        "step": 3318
    },
    {
        "loss": 2.3151,
        "grad_norm": 2.8404438495635986,
        "learning_rate": 5.196257189648037e-05,
        "epoch": 0.44253333333333333,
        "step": 3319
    },
    {
        "loss": 3.0897,
        "grad_norm": 1.8465220928192139,
        "learning_rate": 5.1907381883850425e-05,
        "epoch": 0.44266666666666665,
        "step": 3320
    },
    {
        "loss": 1.5548,
        "grad_norm": 2.079132556915283,
        "learning_rate": 5.185221092087029e-05,
        "epoch": 0.4428,
        "step": 3321
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.9208827018737793,
        "learning_rate": 5.179705902939358e-05,
        "epoch": 0.44293333333333335,
        "step": 3322
    },
    {
        "loss": 2.7203,
        "grad_norm": 2.099663734436035,
        "learning_rate": 5.1741926231265966e-05,
        "epoch": 0.44306666666666666,
        "step": 3323
    },
    {
        "loss": 1.7039,
        "grad_norm": 1.8930670022964478,
        "learning_rate": 5.168681254832586e-05,
        "epoch": 0.4432,
        "step": 3324
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.2213895320892334,
        "learning_rate": 5.163171800240385e-05,
        "epoch": 0.44333333333333336,
        "step": 3325
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.410048246383667,
        "learning_rate": 5.1576642615323234e-05,
        "epoch": 0.4434666666666667,
        "step": 3326
    },
    {
        "loss": 2.5303,
        "grad_norm": 2.8273823261260986,
        "learning_rate": 5.152158640889947e-05,
        "epoch": 0.4436,
        "step": 3327
    },
    {
        "loss": 2.151,
        "grad_norm": 3.4143548011779785,
        "learning_rate": 5.1466549404940465e-05,
        "epoch": 0.4437333333333333,
        "step": 3328
    },
    {
        "loss": 1.3792,
        "grad_norm": 4.82412576675415,
        "learning_rate": 5.141153162524668e-05,
        "epoch": 0.4438666666666667,
        "step": 3329
    },
    {
        "loss": 2.0837,
        "grad_norm": 3.8183391094207764,
        "learning_rate": 5.1356533091610725e-05,
        "epoch": 0.444,
        "step": 3330
    },
    {
        "loss": 1.9172,
        "grad_norm": 2.154695749282837,
        "learning_rate": 5.130155382581776e-05,
        "epoch": 0.4441333333333333,
        "step": 3331
    },
    {
        "loss": 2.7081,
        "grad_norm": 2.9062411785125732,
        "learning_rate": 5.12465938496453e-05,
        "epoch": 0.44426666666666664,
        "step": 3332
    },
    {
        "loss": 1.8988,
        "grad_norm": 2.6571247577667236,
        "learning_rate": 5.1191653184863095e-05,
        "epoch": 0.4444,
        "step": 3333
    },
    {
        "loss": 3.1654,
        "grad_norm": 3.394153118133545,
        "learning_rate": 5.113673185323338e-05,
        "epoch": 0.44453333333333334,
        "step": 3334
    },
    {
        "loss": 2.5941,
        "grad_norm": 2.0279459953308105,
        "learning_rate": 5.108182987651071e-05,
        "epoch": 0.44466666666666665,
        "step": 3335
    },
    {
        "loss": 3.077,
        "grad_norm": 2.787870407104492,
        "learning_rate": 5.102694727644193e-05,
        "epoch": 0.4448,
        "step": 3336
    },
    {
        "loss": 1.9773,
        "grad_norm": 2.2293314933776855,
        "learning_rate": 5.0972084074766144e-05,
        "epoch": 0.44493333333333335,
        "step": 3337
    },
    {
        "loss": 1.797,
        "grad_norm": 3.041948080062866,
        "learning_rate": 5.091724029321492e-05,
        "epoch": 0.44506666666666667,
        "step": 3338
    },
    {
        "loss": 0.9287,
        "grad_norm": 4.040062427520752,
        "learning_rate": 5.08624159535121e-05,
        "epoch": 0.4452,
        "step": 3339
    },
    {
        "loss": 1.8902,
        "grad_norm": 1.993504285812378,
        "learning_rate": 5.080761107737372e-05,
        "epoch": 0.44533333333333336,
        "step": 3340
    },
    {
        "loss": 1.2511,
        "grad_norm": 4.317448616027832,
        "learning_rate": 5.075282568650821e-05,
        "epoch": 0.4454666666666667,
        "step": 3341
    },
    {
        "loss": 2.6932,
        "grad_norm": 2.3642752170562744,
        "learning_rate": 5.0698059802616284e-05,
        "epoch": 0.4456,
        "step": 3342
    },
    {
        "loss": 2.5596,
        "grad_norm": 2.3416388034820557,
        "learning_rate": 5.0643313447390815e-05,
        "epoch": 0.4457333333333333,
        "step": 3343
    },
    {
        "loss": 2.6197,
        "grad_norm": 3.505481719970703,
        "learning_rate": 5.0588586642517087e-05,
        "epoch": 0.4458666666666667,
        "step": 3344
    },
    {
        "loss": 2.1964,
        "grad_norm": 3.1126837730407715,
        "learning_rate": 5.053387940967259e-05,
        "epoch": 0.446,
        "step": 3345
    },
    {
        "loss": 2.1329,
        "grad_norm": 4.283589839935303,
        "learning_rate": 5.0479191770527e-05,
        "epoch": 0.4461333333333333,
        "step": 3346
    },
    {
        "loss": 2.5407,
        "grad_norm": 2.417119264602661,
        "learning_rate": 5.042452374674223e-05,
        "epoch": 0.44626666666666664,
        "step": 3347
    },
    {
        "loss": 2.8942,
        "grad_norm": 3.7508323192596436,
        "learning_rate": 5.036987535997258e-05,
        "epoch": 0.4464,
        "step": 3348
    },
    {
        "loss": 2.5876,
        "grad_norm": 2.648726463317871,
        "learning_rate": 5.0315246631864435e-05,
        "epoch": 0.44653333333333334,
        "step": 3349
    },
    {
        "loss": 2.4913,
        "grad_norm": 2.9437978267669678,
        "learning_rate": 5.02606375840563e-05,
        "epoch": 0.44666666666666666,
        "step": 3350
    },
    {
        "loss": 1.748,
        "grad_norm": 3.0019266605377197,
        "learning_rate": 5.0206048238179185e-05,
        "epoch": 0.4468,
        "step": 3351
    },
    {
        "loss": 1.7268,
        "grad_norm": 3.0618960857391357,
        "learning_rate": 5.015147861585603e-05,
        "epoch": 0.44693333333333335,
        "step": 3352
    },
    {
        "loss": 2.8427,
        "grad_norm": 2.319025754928589,
        "learning_rate": 5.009692873870202e-05,
        "epoch": 0.44706666666666667,
        "step": 3353
    },
    {
        "loss": 2.4658,
        "grad_norm": 2.7898144721984863,
        "learning_rate": 5.0042398628324604e-05,
        "epoch": 0.4472,
        "step": 3354
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.7412288188934326,
        "learning_rate": 4.998788830632328e-05,
        "epoch": 0.44733333333333336,
        "step": 3355
    },
    {
        "loss": 1.5578,
        "grad_norm": 3.7026028633117676,
        "learning_rate": 4.9933397794289815e-05,
        "epoch": 0.4474666666666667,
        "step": 3356
    },
    {
        "loss": 1.4906,
        "grad_norm": 2.3026206493377686,
        "learning_rate": 4.987892711380813e-05,
        "epoch": 0.4476,
        "step": 3357
    },
    {
        "loss": 1.5599,
        "grad_norm": 3.8431236743927,
        "learning_rate": 4.982447628645415e-05,
        "epoch": 0.4477333333333333,
        "step": 3358
    },
    {
        "loss": 2.598,
        "grad_norm": 2.351091146469116,
        "learning_rate": 4.977004533379608e-05,
        "epoch": 0.4478666666666667,
        "step": 3359
    },
    {
        "loss": 1.5914,
        "grad_norm": 4.379304885864258,
        "learning_rate": 4.971563427739424e-05,
        "epoch": 0.448,
        "step": 3360
    },
    {
        "loss": 2.7916,
        "grad_norm": 3.0261666774749756,
        "learning_rate": 4.966124313880101e-05,
        "epoch": 0.44813333333333333,
        "step": 3361
    },
    {
        "loss": 2.34,
        "grad_norm": 2.6456198692321777,
        "learning_rate": 4.960687193956086e-05,
        "epoch": 0.44826666666666665,
        "step": 3362
    },
    {
        "loss": 2.8779,
        "grad_norm": 2.2179670333862305,
        "learning_rate": 4.955252070121045e-05,
        "epoch": 0.4484,
        "step": 3363
    },
    {
        "loss": 2.3935,
        "grad_norm": 2.928117036819458,
        "learning_rate": 4.9498189445278474e-05,
        "epoch": 0.44853333333333334,
        "step": 3364
    },
    {
        "loss": 1.8509,
        "grad_norm": 3.047135829925537,
        "learning_rate": 4.9443878193285786e-05,
        "epoch": 0.44866666666666666,
        "step": 3365
    },
    {
        "loss": 1.1776,
        "grad_norm": 5.976503372192383,
        "learning_rate": 4.938958696674517e-05,
        "epoch": 0.4488,
        "step": 3366
    },
    {
        "loss": 2.1927,
        "grad_norm": 2.3215034008026123,
        "learning_rate": 4.9335315787161605e-05,
        "epoch": 0.44893333333333335,
        "step": 3367
    },
    {
        "loss": 1.6274,
        "grad_norm": 4.280954360961914,
        "learning_rate": 4.9281064676032126e-05,
        "epoch": 0.44906666666666667,
        "step": 3368
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.8394615650177,
        "learning_rate": 4.922683365484573e-05,
        "epoch": 0.4492,
        "step": 3369
    },
    {
        "loss": 2.414,
        "grad_norm": 3.837059259414673,
        "learning_rate": 4.917262274508355e-05,
        "epoch": 0.4493333333333333,
        "step": 3370
    },
    {
        "loss": 1.7246,
        "grad_norm": 3.6123132705688477,
        "learning_rate": 4.9118431968218695e-05,
        "epoch": 0.4494666666666667,
        "step": 3371
    },
    {
        "loss": 0.9982,
        "grad_norm": 3.1991522312164307,
        "learning_rate": 4.906426134571624e-05,
        "epoch": 0.4496,
        "step": 3372
    },
    {
        "loss": 2.929,
        "grad_norm": 2.234187602996826,
        "learning_rate": 4.90101108990335e-05,
        "epoch": 0.4497333333333333,
        "step": 3373
    },
    {
        "loss": 1.5583,
        "grad_norm": 4.506634712219238,
        "learning_rate": 4.895598064961957e-05,
        "epoch": 0.4498666666666667,
        "step": 3374
    },
    {
        "loss": 2.607,
        "grad_norm": 2.4277756214141846,
        "learning_rate": 4.890187061891557e-05,
        "epoch": 0.45,
        "step": 3375
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.9598686695098877,
        "learning_rate": 4.8847780828354795e-05,
        "epoch": 0.45013333333333333,
        "step": 3376
    },
    {
        "loss": 2.2255,
        "grad_norm": 3.9680356979370117,
        "learning_rate": 4.879371129936233e-05,
        "epoch": 0.45026666666666665,
        "step": 3377
    },
    {
        "loss": 1.3827,
        "grad_norm": 3.0205016136169434,
        "learning_rate": 4.8739662053355276e-05,
        "epoch": 0.4504,
        "step": 3378
    },
    {
        "loss": 1.8533,
        "grad_norm": 4.64898681640625,
        "learning_rate": 4.8685633111742765e-05,
        "epoch": 0.45053333333333334,
        "step": 3379
    },
    {
        "loss": 2.5377,
        "grad_norm": 2.3608009815216064,
        "learning_rate": 4.8631624495925806e-05,
        "epoch": 0.45066666666666666,
        "step": 3380
    },
    {
        "loss": 1.5552,
        "grad_norm": 3.9032070636749268,
        "learning_rate": 4.857763622729741e-05,
        "epoch": 0.4508,
        "step": 3381
    },
    {
        "loss": 2.4825,
        "grad_norm": 3.1424155235290527,
        "learning_rate": 4.852366832724256e-05,
        "epoch": 0.45093333333333335,
        "step": 3382
    },
    {
        "loss": 2.8637,
        "grad_norm": 2.5322394371032715,
        "learning_rate": 4.8469720817138054e-05,
        "epoch": 0.45106666666666667,
        "step": 3383
    },
    {
        "loss": 0.5123,
        "grad_norm": 4.116481304168701,
        "learning_rate": 4.841579371835272e-05,
        "epoch": 0.4512,
        "step": 3384
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.3276472091674805,
        "learning_rate": 4.8361887052247255e-05,
        "epoch": 0.4513333333333333,
        "step": 3385
    },
    {
        "loss": 2.6316,
        "grad_norm": 3.3796324729919434,
        "learning_rate": 4.830800084017433e-05,
        "epoch": 0.4514666666666667,
        "step": 3386
    },
    {
        "loss": 2.1888,
        "grad_norm": 3.23834490776062,
        "learning_rate": 4.8254135103478395e-05,
        "epoch": 0.4516,
        "step": 3387
    },
    {
        "loss": 2.8044,
        "grad_norm": 2.5167410373687744,
        "learning_rate": 4.820028986349583e-05,
        "epoch": 0.4517333333333333,
        "step": 3388
    },
    {
        "loss": 2.4852,
        "grad_norm": 2.15921688079834,
        "learning_rate": 4.814646514155496e-05,
        "epoch": 0.4518666666666667,
        "step": 3389
    },
    {
        "loss": 1.6916,
        "grad_norm": 1.903868317604065,
        "learning_rate": 4.8092660958975966e-05,
        "epoch": 0.452,
        "step": 3390
    },
    {
        "loss": 2.3341,
        "grad_norm": 2.242788314819336,
        "learning_rate": 4.80388773370708e-05,
        "epoch": 0.45213333333333333,
        "step": 3391
    },
    {
        "loss": 2.1494,
        "grad_norm": 3.6193554401397705,
        "learning_rate": 4.7985114297143373e-05,
        "epoch": 0.45226666666666665,
        "step": 3392
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.482257604598999,
        "learning_rate": 4.793137186048945e-05,
        "epoch": 0.4524,
        "step": 3393
    },
    {
        "loss": 2.2955,
        "grad_norm": 3.3638463020324707,
        "learning_rate": 4.7877650048396516e-05,
        "epoch": 0.45253333333333334,
        "step": 3394
    },
    {
        "loss": 1.942,
        "grad_norm": 4.272364139556885,
        "learning_rate": 4.7823948882144034e-05,
        "epoch": 0.45266666666666666,
        "step": 3395
    },
    {
        "loss": 2.342,
        "grad_norm": 2.708632707595825,
        "learning_rate": 4.777026838300319e-05,
        "epoch": 0.4528,
        "step": 3396
    },
    {
        "loss": 2.5977,
        "grad_norm": 2.5905919075012207,
        "learning_rate": 4.771660857223692e-05,
        "epoch": 0.45293333333333335,
        "step": 3397
    },
    {
        "loss": 2.2438,
        "grad_norm": 3.7162246704101562,
        "learning_rate": 4.766296947110024e-05,
        "epoch": 0.4530666666666667,
        "step": 3398
    },
    {
        "loss": 2.2316,
        "grad_norm": 3.044159173965454,
        "learning_rate": 4.760935110083967e-05,
        "epoch": 0.4532,
        "step": 3399
    },
    {
        "loss": 1.6639,
        "grad_norm": 3.774430751800537,
        "learning_rate": 4.755575348269362e-05,
        "epoch": 0.4533333333333333,
        "step": 3400
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.1560583114624023,
        "learning_rate": 4.75021766378923e-05,
        "epoch": 0.4534666666666667,
        "step": 3401
    },
    {
        "loss": 1.9874,
        "grad_norm": 3.4357542991638184,
        "learning_rate": 4.744862058765776e-05,
        "epoch": 0.4536,
        "step": 3402
    },
    {
        "loss": 2.2444,
        "grad_norm": 2.536273241043091,
        "learning_rate": 4.7395085353203607e-05,
        "epoch": 0.4537333333333333,
        "step": 3403
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.4639503955841064,
        "learning_rate": 4.73415709557354e-05,
        "epoch": 0.45386666666666664,
        "step": 3404
    },
    {
        "loss": 2.7852,
        "grad_norm": 2.97591233253479,
        "learning_rate": 4.7288077416450405e-05,
        "epoch": 0.454,
        "step": 3405
    },
    {
        "loss": 2.4834,
        "grad_norm": 3.008962869644165,
        "learning_rate": 4.7234604756537513e-05,
        "epoch": 0.45413333333333333,
        "step": 3406
    },
    {
        "loss": 2.5149,
        "grad_norm": 2.665076971054077,
        "learning_rate": 4.718115299717748e-05,
        "epoch": 0.45426666666666665,
        "step": 3407
    },
    {
        "loss": 2.62,
        "grad_norm": 2.465426445007324,
        "learning_rate": 4.7127722159542755e-05,
        "epoch": 0.4544,
        "step": 3408
    },
    {
        "loss": 3.0038,
        "grad_norm": 2.7898409366607666,
        "learning_rate": 4.7074312264797404e-05,
        "epoch": 0.45453333333333334,
        "step": 3409
    },
    {
        "loss": 2.2915,
        "grad_norm": 3.752493143081665,
        "learning_rate": 4.70209233340973e-05,
        "epoch": 0.45466666666666666,
        "step": 3410
    },
    {
        "loss": 2.9534,
        "grad_norm": 2.318127155303955,
        "learning_rate": 4.696755538859004e-05,
        "epoch": 0.4548,
        "step": 3411
    },
    {
        "loss": 1.4293,
        "grad_norm": 2.428224563598633,
        "learning_rate": 4.691420844941478e-05,
        "epoch": 0.45493333333333336,
        "step": 3412
    },
    {
        "loss": 1.9869,
        "grad_norm": 2.448399305343628,
        "learning_rate": 4.686088253770239e-05,
        "epoch": 0.4550666666666667,
        "step": 3413
    },
    {
        "loss": 1.6485,
        "grad_norm": 2.7937731742858887,
        "learning_rate": 4.680757767457551e-05,
        "epoch": 0.4552,
        "step": 3414
    },
    {
        "loss": 2.476,
        "grad_norm": 3.0703213214874268,
        "learning_rate": 4.675429388114839e-05,
        "epoch": 0.4553333333333333,
        "step": 3415
    },
    {
        "loss": 2.7521,
        "grad_norm": 2.187121629714966,
        "learning_rate": 4.670103117852687e-05,
        "epoch": 0.4554666666666667,
        "step": 3416
    },
    {
        "loss": 1.3004,
        "grad_norm": 2.2405524253845215,
        "learning_rate": 4.6647789587808496e-05,
        "epoch": 0.4556,
        "step": 3417
    },
    {
        "loss": 2.6615,
        "grad_norm": 2.4219422340393066,
        "learning_rate": 4.6594569130082514e-05,
        "epoch": 0.4557333333333333,
        "step": 3418
    },
    {
        "loss": 1.7276,
        "grad_norm": 5.117930889129639,
        "learning_rate": 4.654136982642961e-05,
        "epoch": 0.45586666666666664,
        "step": 3419
    },
    {
        "loss": 2.4915,
        "grad_norm": 2.2268097400665283,
        "learning_rate": 4.648819169792233e-05,
        "epoch": 0.456,
        "step": 3420
    },
    {
        "loss": 1.9377,
        "grad_norm": 2.3607938289642334,
        "learning_rate": 4.643503476562466e-05,
        "epoch": 0.45613333333333334,
        "step": 3421
    },
    {
        "loss": 2.5967,
        "grad_norm": 2.3237245082855225,
        "learning_rate": 4.638189905059216e-05,
        "epoch": 0.45626666666666665,
        "step": 3422
    },
    {
        "loss": 2.4797,
        "grad_norm": 4.335165500640869,
        "learning_rate": 4.632878457387221e-05,
        "epoch": 0.4564,
        "step": 3423
    },
    {
        "loss": 2.6808,
        "grad_norm": 3.5934360027313232,
        "learning_rate": 4.627569135650355e-05,
        "epoch": 0.45653333333333335,
        "step": 3424
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.611358404159546,
        "learning_rate": 4.622261941951662e-05,
        "epoch": 0.45666666666666667,
        "step": 3425
    },
    {
        "loss": 2.6953,
        "grad_norm": 2.3996737003326416,
        "learning_rate": 4.616956878393333e-05,
        "epoch": 0.4568,
        "step": 3426
    },
    {
        "loss": 2.4607,
        "grad_norm": 2.3753292560577393,
        "learning_rate": 4.611653947076732e-05,
        "epoch": 0.45693333333333336,
        "step": 3427
    },
    {
        "loss": 2.0086,
        "grad_norm": 2.82932448387146,
        "learning_rate": 4.6063531501023636e-05,
        "epoch": 0.4570666666666667,
        "step": 3428
    },
    {
        "loss": 2.3969,
        "grad_norm": 3.032017469406128,
        "learning_rate": 4.601054489569887e-05,
        "epoch": 0.4572,
        "step": 3429
    },
    {
        "loss": 1.486,
        "grad_norm": 3.9883623123168945,
        "learning_rate": 4.595757967578128e-05,
        "epoch": 0.4573333333333333,
        "step": 3430
    },
    {
        "loss": 1.7071,
        "grad_norm": 1.933769941329956,
        "learning_rate": 4.590463586225048e-05,
        "epoch": 0.4574666666666667,
        "step": 3431
    },
    {
        "loss": 2.8224,
        "grad_norm": 3.3933098316192627,
        "learning_rate": 4.585171347607775e-05,
        "epoch": 0.4576,
        "step": 3432
    },
    {
        "loss": 2.1091,
        "grad_norm": 3.594695806503296,
        "learning_rate": 4.579881253822584e-05,
        "epoch": 0.4577333333333333,
        "step": 3433
    },
    {
        "loss": 2.6335,
        "grad_norm": 2.959700345993042,
        "learning_rate": 4.5745933069648946e-05,
        "epoch": 0.45786666666666664,
        "step": 3434
    },
    {
        "loss": 2.185,
        "grad_norm": 3.110684394836426,
        "learning_rate": 4.569307509129283e-05,
        "epoch": 0.458,
        "step": 3435
    },
    {
        "loss": 2.8673,
        "grad_norm": 3.0006158351898193,
        "learning_rate": 4.5640238624094744e-05,
        "epoch": 0.45813333333333334,
        "step": 3436
    },
    {
        "loss": 2.3112,
        "grad_norm": 2.4067976474761963,
        "learning_rate": 4.558742368898337e-05,
        "epoch": 0.45826666666666666,
        "step": 3437
    },
    {
        "loss": 2.5672,
        "grad_norm": 1.8816219568252563,
        "learning_rate": 4.553463030687884e-05,
        "epoch": 0.4584,
        "step": 3438
    },
    {
        "loss": 2.2075,
        "grad_norm": 4.322205066680908,
        "learning_rate": 4.548185849869283e-05,
        "epoch": 0.45853333333333335,
        "step": 3439
    },
    {
        "loss": 2.5697,
        "grad_norm": 2.2593038082122803,
        "learning_rate": 4.542910828532848e-05,
        "epoch": 0.45866666666666667,
        "step": 3440
    },
    {
        "loss": 2.3224,
        "grad_norm": 3.2151167392730713,
        "learning_rate": 4.5376379687680236e-05,
        "epoch": 0.4588,
        "step": 3441
    },
    {
        "loss": 2.7491,
        "grad_norm": 2.910426378250122,
        "learning_rate": 4.532367272663414e-05,
        "epoch": 0.45893333333333336,
        "step": 3442
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.8636006116867065,
        "learning_rate": 4.52709874230676e-05,
        "epoch": 0.4590666666666667,
        "step": 3443
    },
    {
        "loss": 2.4599,
        "grad_norm": 3.5093870162963867,
        "learning_rate": 4.5218323797849407e-05,
        "epoch": 0.4592,
        "step": 3444
    },
    {
        "loss": 3.297,
        "grad_norm": 3.624464750289917,
        "learning_rate": 4.5165681871839806e-05,
        "epoch": 0.4593333333333333,
        "step": 3445
    },
    {
        "loss": 1.1515,
        "grad_norm": 3.3332602977752686,
        "learning_rate": 4.51130616658905e-05,
        "epoch": 0.4594666666666667,
        "step": 3446
    },
    {
        "loss": 2.0927,
        "grad_norm": 4.131920337677002,
        "learning_rate": 4.5060463200844494e-05,
        "epoch": 0.4596,
        "step": 3447
    },
    {
        "loss": 2.4314,
        "grad_norm": 3.5226032733917236,
        "learning_rate": 4.500788649753614e-05,
        "epoch": 0.4597333333333333,
        "step": 3448
    },
    {
        "loss": 2.7383,
        "grad_norm": 2.8772943019866943,
        "learning_rate": 4.4955331576791406e-05,
        "epoch": 0.45986666666666665,
        "step": 3449
    },
    {
        "loss": 1.7253,
        "grad_norm": 3.289668083190918,
        "learning_rate": 4.4902798459427385e-05,
        "epoch": 0.46,
        "step": 3450
    },
    {
        "loss": 2.6889,
        "grad_norm": 2.279222011566162,
        "learning_rate": 4.485028716625257e-05,
        "epoch": 0.46013333333333334,
        "step": 3451
    },
    {
        "loss": 1.919,
        "grad_norm": 3.390394449234009,
        "learning_rate": 4.479779771806699e-05,
        "epoch": 0.46026666666666666,
        "step": 3452
    },
    {
        "loss": 2.7239,
        "grad_norm": 3.1160390377044678,
        "learning_rate": 4.4745330135661836e-05,
        "epoch": 0.4604,
        "step": 3453
    },
    {
        "loss": 2.1054,
        "grad_norm": 3.1773040294647217,
        "learning_rate": 4.469288443981965e-05,
        "epoch": 0.46053333333333335,
        "step": 3454
    },
    {
        "loss": 2.1377,
        "grad_norm": 3.847738265991211,
        "learning_rate": 4.464046065131443e-05,
        "epoch": 0.46066666666666667,
        "step": 3455
    },
    {
        "loss": 2.3582,
        "grad_norm": 2.9536149501800537,
        "learning_rate": 4.458805879091135e-05,
        "epoch": 0.4608,
        "step": 3456
    },
    {
        "loss": 2.5109,
        "grad_norm": 2.857968807220459,
        "learning_rate": 4.453567887936698e-05,
        "epoch": 0.4609333333333333,
        "step": 3457
    },
    {
        "loss": 2.7757,
        "grad_norm": 4.020962715148926,
        "learning_rate": 4.448332093742924e-05,
        "epoch": 0.4610666666666667,
        "step": 3458
    },
    {
        "loss": 0.7593,
        "grad_norm": 2.156315565109253,
        "learning_rate": 4.443098498583721e-05,
        "epoch": 0.4612,
        "step": 3459
    },
    {
        "loss": 2.7169,
        "grad_norm": 3.35394024848938,
        "learning_rate": 4.4378671045321375e-05,
        "epoch": 0.4613333333333333,
        "step": 3460
    },
    {
        "loss": 2.2446,
        "grad_norm": 2.0866541862487793,
        "learning_rate": 4.43263791366035e-05,
        "epoch": 0.4614666666666667,
        "step": 3461
    },
    {
        "loss": 1.35,
        "grad_norm": 3.302661180496216,
        "learning_rate": 4.427410928039655e-05,
        "epoch": 0.4616,
        "step": 3462
    },
    {
        "loss": 2.5729,
        "grad_norm": 2.571714162826538,
        "learning_rate": 4.422186149740476e-05,
        "epoch": 0.46173333333333333,
        "step": 3463
    },
    {
        "loss": 2.0487,
        "grad_norm": 2.710249423980713,
        "learning_rate": 4.4169635808323684e-05,
        "epoch": 0.46186666666666665,
        "step": 3464
    },
    {
        "loss": 2.0066,
        "grad_norm": 3.492269515991211,
        "learning_rate": 4.411743223384011e-05,
        "epoch": 0.462,
        "step": 3465
    },
    {
        "loss": 1.8329,
        "grad_norm": 3.6412386894226074,
        "learning_rate": 4.406525079463208e-05,
        "epoch": 0.46213333333333334,
        "step": 3466
    },
    {
        "loss": 2.5644,
        "grad_norm": 2.242020606994629,
        "learning_rate": 4.401309151136876e-05,
        "epoch": 0.46226666666666666,
        "step": 3467
    },
    {
        "loss": 2.6636,
        "grad_norm": 2.491830348968506,
        "learning_rate": 4.3960954404710664e-05,
        "epoch": 0.4624,
        "step": 3468
    },
    {
        "loss": 2.3117,
        "grad_norm": 2.750779390335083,
        "learning_rate": 4.390883949530952e-05,
        "epoch": 0.46253333333333335,
        "step": 3469
    },
    {
        "loss": 2.4276,
        "grad_norm": 3.244203567504883,
        "learning_rate": 4.385674680380813e-05,
        "epoch": 0.46266666666666667,
        "step": 3470
    },
    {
        "loss": 2.5242,
        "grad_norm": 2.6112687587738037,
        "learning_rate": 4.380467635084069e-05,
        "epoch": 0.4628,
        "step": 3471
    },
    {
        "loss": 2.4495,
        "grad_norm": 4.120702743530273,
        "learning_rate": 4.3752628157032416e-05,
        "epoch": 0.4629333333333333,
        "step": 3472
    },
    {
        "loss": 2.623,
        "grad_norm": 3.5521392822265625,
        "learning_rate": 4.370060224299971e-05,
        "epoch": 0.4630666666666667,
        "step": 3473
    },
    {
        "loss": 2.9211,
        "grad_norm": 2.602121591567993,
        "learning_rate": 4.364859862935037e-05,
        "epoch": 0.4632,
        "step": 3474
    },
    {
        "loss": 2.4101,
        "grad_norm": 3.6189937591552734,
        "learning_rate": 4.359661733668312e-05,
        "epoch": 0.4633333333333333,
        "step": 3475
    },
    {
        "loss": 0.9295,
        "grad_norm": 3.6462666988372803,
        "learning_rate": 4.3544658385587865e-05,
        "epoch": 0.4634666666666667,
        "step": 3476
    },
    {
        "loss": 3.0184,
        "grad_norm": 2.840916395187378,
        "learning_rate": 4.3492721796645866e-05,
        "epoch": 0.4636,
        "step": 3477
    },
    {
        "loss": 3.1775,
        "grad_norm": 4.474728107452393,
        "learning_rate": 4.34408075904293e-05,
        "epoch": 0.46373333333333333,
        "step": 3478
    },
    {
        "loss": 1.7574,
        "grad_norm": 3.189485549926758,
        "learning_rate": 4.3388915787501535e-05,
        "epoch": 0.46386666666666665,
        "step": 3479
    },
    {
        "loss": 2.7,
        "grad_norm": 2.4780821800231934,
        "learning_rate": 4.333704640841716e-05,
        "epoch": 0.464,
        "step": 3480
    },
    {
        "loss": 2.4692,
        "grad_norm": 2.2004380226135254,
        "learning_rate": 4.328519947372175e-05,
        "epoch": 0.46413333333333334,
        "step": 3481
    },
    {
        "loss": 2.2632,
        "grad_norm": 3.647193431854248,
        "learning_rate": 4.323337500395207e-05,
        "epoch": 0.46426666666666666,
        "step": 3482
    },
    {
        "loss": 1.6386,
        "grad_norm": 2.3283872604370117,
        "learning_rate": 4.318157301963601e-05,
        "epoch": 0.4644,
        "step": 3483
    },
    {
        "loss": 2.6487,
        "grad_norm": 2.1655707359313965,
        "learning_rate": 4.312979354129245e-05,
        "epoch": 0.46453333333333335,
        "step": 3484
    },
    {
        "loss": 1.9773,
        "grad_norm": 2.9246644973754883,
        "learning_rate": 4.3078036589431446e-05,
        "epoch": 0.4646666666666667,
        "step": 3485
    },
    {
        "loss": 1.9913,
        "grad_norm": 3.031252384185791,
        "learning_rate": 4.302630218455411e-05,
        "epoch": 0.4648,
        "step": 3486
    },
    {
        "loss": 1.8836,
        "grad_norm": 6.032568454742432,
        "learning_rate": 4.297459034715265e-05,
        "epoch": 0.4649333333333333,
        "step": 3487
    },
    {
        "loss": 1.1339,
        "grad_norm": 3.4655556678771973,
        "learning_rate": 4.292290109771027e-05,
        "epoch": 0.4650666666666667,
        "step": 3488
    },
    {
        "loss": 2.2131,
        "grad_norm": 3.804718255996704,
        "learning_rate": 4.2871234456701195e-05,
        "epoch": 0.4652,
        "step": 3489
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.5376572608947754,
        "learning_rate": 4.281959044459082e-05,
        "epoch": 0.4653333333333333,
        "step": 3490
    },
    {
        "loss": 1.2789,
        "grad_norm": 1.7902815341949463,
        "learning_rate": 4.276796908183555e-05,
        "epoch": 0.46546666666666664,
        "step": 3491
    },
    {
        "loss": 1.6923,
        "grad_norm": 3.3454015254974365,
        "learning_rate": 4.27163703888827e-05,
        "epoch": 0.4656,
        "step": 3492
    },
    {
        "loss": 1.7467,
        "grad_norm": 3.3690052032470703,
        "learning_rate": 4.2664794386170725e-05,
        "epoch": 0.46573333333333333,
        "step": 3493
    },
    {
        "loss": 2.307,
        "grad_norm": 2.7224175930023193,
        "learning_rate": 4.26132410941291e-05,
        "epoch": 0.46586666666666665,
        "step": 3494
    },
    {
        "loss": 2.3493,
        "grad_norm": 2.9644598960876465,
        "learning_rate": 4.256171053317818e-05,
        "epoch": 0.466,
        "step": 3495
    },
    {
        "loss": 2.8575,
        "grad_norm": 3.234194040298462,
        "learning_rate": 4.2510202723729464e-05,
        "epoch": 0.46613333333333334,
        "step": 3496
    },
    {
        "loss": 2.8466,
        "grad_norm": 2.709249496459961,
        "learning_rate": 4.245871768618535e-05,
        "epoch": 0.46626666666666666,
        "step": 3497
    },
    {
        "loss": 2.492,
        "grad_norm": 2.2414865493774414,
        "learning_rate": 4.2407255440939155e-05,
        "epoch": 0.4664,
        "step": 3498
    },
    {
        "loss": 2.3993,
        "grad_norm": 1.6086024045944214,
        "learning_rate": 4.235581600837539e-05,
        "epoch": 0.46653333333333336,
        "step": 3499
    },
    {
        "loss": 1.1045,
        "grad_norm": 3.4848620891571045,
        "learning_rate": 4.230439940886932e-05,
        "epoch": 0.4666666666666667,
        "step": 3500
    },
    {
        "loss": 1.4993,
        "grad_norm": 3.17787504196167,
        "learning_rate": 4.2253005662787205e-05,
        "epoch": 0.4668,
        "step": 3501
    },
    {
        "loss": 2.1815,
        "grad_norm": 4.4515228271484375,
        "learning_rate": 4.2201634790486314e-05,
        "epoch": 0.4669333333333333,
        "step": 3502
    },
    {
        "loss": 2.2624,
        "grad_norm": 4.074491024017334,
        "learning_rate": 4.2150286812314866e-05,
        "epoch": 0.4670666666666667,
        "step": 3503
    },
    {
        "loss": 2.4399,
        "grad_norm": 2.5111441612243652,
        "learning_rate": 4.2098961748611884e-05,
        "epoch": 0.4672,
        "step": 3504
    },
    {
        "loss": 2.1794,
        "grad_norm": 3.6465201377868652,
        "learning_rate": 4.204765961970746e-05,
        "epoch": 0.4673333333333333,
        "step": 3505
    },
    {
        "loss": 2.5694,
        "grad_norm": 4.466720104217529,
        "learning_rate": 4.199638044592256e-05,
        "epoch": 0.46746666666666664,
        "step": 3506
    },
    {
        "loss": 2.324,
        "grad_norm": 3.8853626251220703,
        "learning_rate": 4.194512424756896e-05,
        "epoch": 0.4676,
        "step": 3507
    },
    {
        "loss": 2.8432,
        "grad_norm": 2.4618072509765625,
        "learning_rate": 4.1893891044949475e-05,
        "epoch": 0.46773333333333333,
        "step": 3508
    },
    {
        "loss": 2.7055,
        "grad_norm": 3.091545820236206,
        "learning_rate": 4.184268085835776e-05,
        "epoch": 0.46786666666666665,
        "step": 3509
    },
    {
        "loss": 2.1683,
        "grad_norm": 3.3259007930755615,
        "learning_rate": 4.17914937080783e-05,
        "epoch": 0.468,
        "step": 3510
    },
    {
        "loss": 2.2294,
        "grad_norm": 2.375516414642334,
        "learning_rate": 4.174032961438653e-05,
        "epoch": 0.46813333333333335,
        "step": 3511
    },
    {
        "loss": 2.5893,
        "grad_norm": 3.0073611736297607,
        "learning_rate": 4.168918859754873e-05,
        "epoch": 0.46826666666666666,
        "step": 3512
    },
    {
        "loss": 2.3495,
        "grad_norm": 3.1596899032592773,
        "learning_rate": 4.163807067782204e-05,
        "epoch": 0.4684,
        "step": 3513
    },
    {
        "loss": 2.3121,
        "grad_norm": 2.216688871383667,
        "learning_rate": 4.158697587545436e-05,
        "epoch": 0.46853333333333336,
        "step": 3514
    },
    {
        "loss": 2.2508,
        "grad_norm": 3.23492431640625,
        "learning_rate": 4.1535904210684586e-05,
        "epoch": 0.4686666666666667,
        "step": 3515
    },
    {
        "loss": 2.2528,
        "grad_norm": 3.6964001655578613,
        "learning_rate": 4.14848557037424e-05,
        "epoch": 0.4688,
        "step": 3516
    },
    {
        "loss": 2.0081,
        "grad_norm": 3.210171937942505,
        "learning_rate": 4.1433830374848215e-05,
        "epoch": 0.4689333333333333,
        "step": 3517
    },
    {
        "loss": 2.0996,
        "grad_norm": 2.5021138191223145,
        "learning_rate": 4.138282824421338e-05,
        "epoch": 0.4690666666666667,
        "step": 3518
    },
    {
        "loss": 3.0094,
        "grad_norm": 2.912646532058716,
        "learning_rate": 4.133184933204005e-05,
        "epoch": 0.4692,
        "step": 3519
    },
    {
        "loss": 2.4686,
        "grad_norm": 3.3679065704345703,
        "learning_rate": 4.128089365852107e-05,
        "epoch": 0.4693333333333333,
        "step": 3520
    },
    {
        "loss": 1.764,
        "grad_norm": 3.651599407196045,
        "learning_rate": 4.122996124384022e-05,
        "epoch": 0.46946666666666664,
        "step": 3521
    },
    {
        "loss": 2.1373,
        "grad_norm": 3.228595733642578,
        "learning_rate": 4.117905210817198e-05,
        "epoch": 0.4696,
        "step": 3522
    },
    {
        "loss": 2.1278,
        "grad_norm": 1.9019606113433838,
        "learning_rate": 4.112816627168156e-05,
        "epoch": 0.46973333333333334,
        "step": 3523
    },
    {
        "loss": 2.6988,
        "grad_norm": 3.219052791595459,
        "learning_rate": 4.107730375452511e-05,
        "epoch": 0.46986666666666665,
        "step": 3524
    },
    {
        "loss": 2.2153,
        "grad_norm": 2.5628116130828857,
        "learning_rate": 4.102646457684945e-05,
        "epoch": 0.47,
        "step": 3525
    },
    {
        "loss": 1.6095,
        "grad_norm": 4.549145221710205,
        "learning_rate": 4.097564875879213e-05,
        "epoch": 0.47013333333333335,
        "step": 3526
    },
    {
        "loss": 2.5933,
        "grad_norm": 3.425978660583496,
        "learning_rate": 4.092485632048138e-05,
        "epoch": 0.47026666666666667,
        "step": 3527
    },
    {
        "loss": 2.8209,
        "grad_norm": 3.1142213344573975,
        "learning_rate": 4.08740872820364e-05,
        "epoch": 0.4704,
        "step": 3528
    },
    {
        "loss": 1.0244,
        "grad_norm": 2.549147844314575,
        "learning_rate": 4.082334166356693e-05,
        "epoch": 0.47053333333333336,
        "step": 3529
    },
    {
        "loss": 1.2959,
        "grad_norm": 3.9344749450683594,
        "learning_rate": 4.077261948517341e-05,
        "epoch": 0.4706666666666667,
        "step": 3530
    },
    {
        "loss": 2.316,
        "grad_norm": 2.7761690616607666,
        "learning_rate": 4.072192076694718e-05,
        "epoch": 0.4708,
        "step": 3531
    },
    {
        "loss": 1.5846,
        "grad_norm": 1.6155169010162354,
        "learning_rate": 4.067124552897007e-05,
        "epoch": 0.4709333333333333,
        "step": 3532
    },
    {
        "loss": 2.1991,
        "grad_norm": 2.5050835609436035,
        "learning_rate": 4.062059379131478e-05,
        "epoch": 0.4710666666666667,
        "step": 3533
    },
    {
        "loss": 2.1866,
        "grad_norm": 3.5888831615448,
        "learning_rate": 4.0569965574044634e-05,
        "epoch": 0.4712,
        "step": 3534
    },
    {
        "loss": 2.633,
        "grad_norm": 1.8959219455718994,
        "learning_rate": 4.05193608972136e-05,
        "epoch": 0.4713333333333333,
        "step": 3535
    },
    {
        "loss": 1.5701,
        "grad_norm": 3.079437494277954,
        "learning_rate": 4.0468779780866396e-05,
        "epoch": 0.47146666666666665,
        "step": 3536
    },
    {
        "loss": 2.121,
        "grad_norm": 3.950573205947876,
        "learning_rate": 4.04182222450384e-05,
        "epoch": 0.4716,
        "step": 3537
    },
    {
        "loss": 2.4288,
        "grad_norm": 2.322650909423828,
        "learning_rate": 4.036768830975559e-05,
        "epoch": 0.47173333333333334,
        "step": 3538
    },
    {
        "loss": 2.4582,
        "grad_norm": 3.3567023277282715,
        "learning_rate": 4.031717799503459e-05,
        "epoch": 0.47186666666666666,
        "step": 3539
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.9534847736358643,
        "learning_rate": 4.0266691320882766e-05,
        "epoch": 0.472,
        "step": 3540
    },
    {
        "loss": 2.1399,
        "grad_norm": 3.6440064907073975,
        "learning_rate": 4.021622830729808e-05,
        "epoch": 0.47213333333333335,
        "step": 3541
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.4858007431030273,
        "learning_rate": 4.016578897426903e-05,
        "epoch": 0.47226666666666667,
        "step": 3542
    },
    {
        "loss": 2.9568,
        "grad_norm": 3.3870580196380615,
        "learning_rate": 4.0115373341774845e-05,
        "epoch": 0.4724,
        "step": 3543
    },
    {
        "loss": 1.6224,
        "grad_norm": 3.31561541557312,
        "learning_rate": 4.006498142978536e-05,
        "epoch": 0.47253333333333336,
        "step": 3544
    },
    {
        "loss": 2.6692,
        "grad_norm": 3.054598569869995,
        "learning_rate": 4.001461325826094e-05,
        "epoch": 0.4726666666666667,
        "step": 3545
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.7139573097229004,
        "learning_rate": 3.996426884715259e-05,
        "epoch": 0.4728,
        "step": 3546
    },
    {
        "loss": 2.3918,
        "grad_norm": 4.415544033050537,
        "learning_rate": 3.9913948216401945e-05,
        "epoch": 0.4729333333333333,
        "step": 3547
    },
    {
        "loss": 2.1447,
        "grad_norm": 3.3376665115356445,
        "learning_rate": 3.986365138594116e-05,
        "epoch": 0.4730666666666667,
        "step": 3548
    },
    {
        "loss": 2.9012,
        "grad_norm": 1.9267655611038208,
        "learning_rate": 3.98133783756929e-05,
        "epoch": 0.4732,
        "step": 3549
    },
    {
        "loss": 2.3604,
        "grad_norm": 2.4011173248291016,
        "learning_rate": 3.976312920557061e-05,
        "epoch": 0.47333333333333333,
        "step": 3550
    },
    {
        "loss": 2.5424,
        "grad_norm": 3.556171178817749,
        "learning_rate": 3.9712903895478094e-05,
        "epoch": 0.47346666666666665,
        "step": 3551
    },
    {
        "loss": 3.0493,
        "grad_norm": 2.1220762729644775,
        "learning_rate": 3.966270246530971e-05,
        "epoch": 0.4736,
        "step": 3552
    },
    {
        "loss": 1.9967,
        "grad_norm": 3.375277280807495,
        "learning_rate": 3.9612524934950535e-05,
        "epoch": 0.47373333333333334,
        "step": 3553
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.041475534439087,
        "learning_rate": 3.956237132427599e-05,
        "epoch": 0.47386666666666666,
        "step": 3554
    },
    {
        "loss": 2.7319,
        "grad_norm": 2.929868459701538,
        "learning_rate": 3.951224165315208e-05,
        "epoch": 0.474,
        "step": 3555
    },
    {
        "loss": 2.6203,
        "grad_norm": 2.082359790802002,
        "learning_rate": 3.9462135941435384e-05,
        "epoch": 0.47413333333333335,
        "step": 3556
    },
    {
        "loss": 2.2505,
        "grad_norm": 3.6572163105010986,
        "learning_rate": 3.941205420897289e-05,
        "epoch": 0.47426666666666667,
        "step": 3557
    },
    {
        "loss": 2.6303,
        "grad_norm": 4.015374660491943,
        "learning_rate": 3.936199647560217e-05,
        "epoch": 0.4744,
        "step": 3558
    },
    {
        "loss": 0.9299,
        "grad_norm": 2.9843814373016357,
        "learning_rate": 3.93119627611513e-05,
        "epoch": 0.4745333333333333,
        "step": 3559
    },
    {
        "loss": 2.4824,
        "grad_norm": 4.209056377410889,
        "learning_rate": 3.926195308543873e-05,
        "epoch": 0.4746666666666667,
        "step": 3560
    },
    {
        "loss": 1.4119,
        "grad_norm": 2.8621785640716553,
        "learning_rate": 3.921196746827351e-05,
        "epoch": 0.4748,
        "step": 3561
    },
    {
        "loss": 3.0977,
        "grad_norm": 2.3435800075531006,
        "learning_rate": 3.916200592945514e-05,
        "epoch": 0.4749333333333333,
        "step": 3562
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.7558796405792236,
        "learning_rate": 3.911206848877351e-05,
        "epoch": 0.4750666666666667,
        "step": 3563
    },
    {
        "loss": 2.8469,
        "grad_norm": 2.0527985095977783,
        "learning_rate": 3.906215516600901e-05,
        "epoch": 0.4752,
        "step": 3564
    },
    {
        "loss": 2.3653,
        "grad_norm": 2.5388851165771484,
        "learning_rate": 3.9012265980932475e-05,
        "epoch": 0.47533333333333333,
        "step": 3565
    },
    {
        "loss": 2.6104,
        "grad_norm": 3.449629306793213,
        "learning_rate": 3.896240095330519e-05,
        "epoch": 0.47546666666666665,
        "step": 3566
    },
    {
        "loss": 1.5425,
        "grad_norm": 3.1337151527404785,
        "learning_rate": 3.8912560102878916e-05,
        "epoch": 0.4756,
        "step": 3567
    },
    {
        "loss": 2.571,
        "grad_norm": 2.9066221714019775,
        "learning_rate": 3.886274344939569e-05,
        "epoch": 0.47573333333333334,
        "step": 3568
    },
    {
        "loss": 2.7103,
        "grad_norm": 2.9718973636627197,
        "learning_rate": 3.8812951012588104e-05,
        "epoch": 0.47586666666666666,
        "step": 3569
    },
    {
        "loss": 1.8534,
        "grad_norm": 4.707241535186768,
        "learning_rate": 3.8763182812179153e-05,
        "epoch": 0.476,
        "step": 3570
    },
    {
        "loss": 1.8713,
        "grad_norm": 4.823306083679199,
        "learning_rate": 3.871343886788211e-05,
        "epoch": 0.47613333333333335,
        "step": 3571
    },
    {
        "loss": 1.7409,
        "grad_norm": 2.0926458835601807,
        "learning_rate": 3.866371919940079e-05,
        "epoch": 0.47626666666666667,
        "step": 3572
    },
    {
        "loss": 2.7273,
        "grad_norm": 2.48612117767334,
        "learning_rate": 3.8614023826429314e-05,
        "epoch": 0.4764,
        "step": 3573
    },
    {
        "loss": 2.3352,
        "grad_norm": 3.0188257694244385,
        "learning_rate": 3.856435276865208e-05,
        "epoch": 0.4765333333333333,
        "step": 3574
    },
    {
        "loss": 1.6001,
        "grad_norm": 2.465557098388672,
        "learning_rate": 3.851470604574412e-05,
        "epoch": 0.4766666666666667,
        "step": 3575
    },
    {
        "loss": 1.8685,
        "grad_norm": 3.254437208175659,
        "learning_rate": 3.8465083677370614e-05,
        "epoch": 0.4768,
        "step": 3576
    },
    {
        "loss": 2.4675,
        "grad_norm": 3.373347759246826,
        "learning_rate": 3.841548568318706e-05,
        "epoch": 0.4769333333333333,
        "step": 3577
    },
    {
        "loss": 2.351,
        "grad_norm": 2.3358101844787598,
        "learning_rate": 3.836591208283954e-05,
        "epoch": 0.4770666666666667,
        "step": 3578
    },
    {
        "loss": 2.7356,
        "grad_norm": 2.690460681915283,
        "learning_rate": 3.8316362895964266e-05,
        "epoch": 0.4772,
        "step": 3579
    },
    {
        "loss": 1.1482,
        "grad_norm": 4.2217488288879395,
        "learning_rate": 3.826683814218779e-05,
        "epoch": 0.47733333333333333,
        "step": 3580
    },
    {
        "loss": 1.4358,
        "grad_norm": 2.241509437561035,
        "learning_rate": 3.82173378411271e-05,
        "epoch": 0.47746666666666665,
        "step": 3581
    },
    {
        "loss": 2.5371,
        "grad_norm": 2.394268274307251,
        "learning_rate": 3.816786201238939e-05,
        "epoch": 0.4776,
        "step": 3582
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.6429450511932373,
        "learning_rate": 3.8118410675572227e-05,
        "epoch": 0.47773333333333334,
        "step": 3583
    },
    {
        "loss": 3.0291,
        "grad_norm": 2.0440149307250977,
        "learning_rate": 3.8068983850263474e-05,
        "epoch": 0.47786666666666666,
        "step": 3584
    },
    {
        "loss": 0.9685,
        "grad_norm": 3.3990304470062256,
        "learning_rate": 3.8019581556041215e-05,
        "epoch": 0.478,
        "step": 3585
    },
    {
        "loss": 3.2563,
        "grad_norm": 2.385869264602661,
        "learning_rate": 3.7970203812473894e-05,
        "epoch": 0.47813333333333335,
        "step": 3586
    },
    {
        "loss": 2.5157,
        "grad_norm": 1.941192388534546,
        "learning_rate": 3.792085063912021e-05,
        "epoch": 0.4782666666666667,
        "step": 3587
    },
    {
        "loss": 2.584,
        "grad_norm": 2.124701738357544,
        "learning_rate": 3.787152205552914e-05,
        "epoch": 0.4784,
        "step": 3588
    },
    {
        "loss": 2.4784,
        "grad_norm": 2.8570690155029297,
        "learning_rate": 3.782221808123989e-05,
        "epoch": 0.4785333333333333,
        "step": 3589
    },
    {
        "loss": 2.987,
        "grad_norm": 2.1665353775024414,
        "learning_rate": 3.777293873578188e-05,
        "epoch": 0.4786666666666667,
        "step": 3590
    },
    {
        "loss": 2.5054,
        "grad_norm": 2.2462379932403564,
        "learning_rate": 3.7723684038674876e-05,
        "epoch": 0.4788,
        "step": 3591
    },
    {
        "loss": 2.7846,
        "grad_norm": 2.819871425628662,
        "learning_rate": 3.767445400942886e-05,
        "epoch": 0.4789333333333333,
        "step": 3592
    },
    {
        "loss": 2.2952,
        "grad_norm": 3.2661099433898926,
        "learning_rate": 3.762524866754395e-05,
        "epoch": 0.47906666666666664,
        "step": 3593
    },
    {
        "loss": 1.9055,
        "grad_norm": 2.0087971687316895,
        "learning_rate": 3.757606803251058e-05,
        "epoch": 0.4792,
        "step": 3594
    },
    {
        "loss": 0.9591,
        "grad_norm": 3.639143466949463,
        "learning_rate": 3.75269121238094e-05,
        "epoch": 0.47933333333333333,
        "step": 3595
    },
    {
        "loss": 1.6081,
        "grad_norm": 2.1693780422210693,
        "learning_rate": 3.7477780960911154e-05,
        "epoch": 0.47946666666666665,
        "step": 3596
    },
    {
        "loss": 2.4893,
        "grad_norm": 2.9335036277770996,
        "learning_rate": 3.7428674563276956e-05,
        "epoch": 0.4796,
        "step": 3597
    },
    {
        "loss": 2.8512,
        "grad_norm": 3.288489580154419,
        "learning_rate": 3.7379592950357965e-05,
        "epoch": 0.47973333333333334,
        "step": 3598
    },
    {
        "loss": 2.09,
        "grad_norm": 3.609804630279541,
        "learning_rate": 3.7330536141595506e-05,
        "epoch": 0.47986666666666666,
        "step": 3599
    },
    {
        "loss": 2.6866,
        "grad_norm": 1.9357316493988037,
        "learning_rate": 3.72815041564213e-05,
        "epoch": 0.48,
        "step": 3600
    },
    {
        "loss": 1.188,
        "grad_norm": 4.708593845367432,
        "learning_rate": 3.7232497014257006e-05,
        "epoch": 0.48013333333333336,
        "step": 3601
    },
    {
        "loss": 1.8362,
        "grad_norm": 4.101245403289795,
        "learning_rate": 3.718351473451448e-05,
        "epoch": 0.4802666666666667,
        "step": 3602
    },
    {
        "loss": 2.2159,
        "grad_norm": 2.096989870071411,
        "learning_rate": 3.713455733659581e-05,
        "epoch": 0.4804,
        "step": 3603
    },
    {
        "loss": 2.7478,
        "grad_norm": 2.683467149734497,
        "learning_rate": 3.708562483989323e-05,
        "epoch": 0.4805333333333333,
        "step": 3604
    },
    {
        "loss": 2.1645,
        "grad_norm": 5.443350315093994,
        "learning_rate": 3.703671726378898e-05,
        "epoch": 0.4806666666666667,
        "step": 3605
    },
    {
        "loss": 2.5645,
        "grad_norm": 3.63714337348938,
        "learning_rate": 3.698783462765557e-05,
        "epoch": 0.4808,
        "step": 3606
    },
    {
        "loss": 1.1399,
        "grad_norm": 4.068205833435059,
        "learning_rate": 3.6938976950855606e-05,
        "epoch": 0.4809333333333333,
        "step": 3607
    },
    {
        "loss": 3.0284,
        "grad_norm": 6.499335765838623,
        "learning_rate": 3.689014425274171e-05,
        "epoch": 0.48106666666666664,
        "step": 3608
    },
    {
        "loss": 2.1678,
        "grad_norm": 1.9774821996688843,
        "learning_rate": 3.6841336552656724e-05,
        "epoch": 0.4812,
        "step": 3609
    },
    {
        "loss": 3.004,
        "grad_norm": 2.6179654598236084,
        "learning_rate": 3.679255386993356e-05,
        "epoch": 0.48133333333333334,
        "step": 3610
    },
    {
        "loss": 1.9123,
        "grad_norm": 2.8725149631500244,
        "learning_rate": 3.674379622389516e-05,
        "epoch": 0.48146666666666665,
        "step": 3611
    },
    {
        "loss": 2.064,
        "grad_norm": 3.0016510486602783,
        "learning_rate": 3.669506363385462e-05,
        "epoch": 0.4816,
        "step": 3612
    },
    {
        "loss": 2.2553,
        "grad_norm": 2.9555182456970215,
        "learning_rate": 3.66463561191151e-05,
        "epoch": 0.48173333333333335,
        "step": 3613
    },
    {
        "loss": 1.0025,
        "grad_norm": 3.70196795463562,
        "learning_rate": 3.65976736989698e-05,
        "epoch": 0.48186666666666667,
        "step": 3614
    },
    {
        "loss": 2.3522,
        "grad_norm": 3.0143120288848877,
        "learning_rate": 3.654901639270195e-05,
        "epoch": 0.482,
        "step": 3615
    },
    {
        "loss": 2.3916,
        "grad_norm": 2.819091796875,
        "learning_rate": 3.650038421958491e-05,
        "epoch": 0.48213333333333336,
        "step": 3616
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.9218482971191406,
        "learning_rate": 3.6451777198882095e-05,
        "epoch": 0.4822666666666667,
        "step": 3617
    },
    {
        "loss": 2.6434,
        "grad_norm": 2.251858949661255,
        "learning_rate": 3.640319534984683e-05,
        "epoch": 0.4824,
        "step": 3618
    },
    {
        "loss": 3.1124,
        "grad_norm": 3.9485645294189453,
        "learning_rate": 3.6354638691722575e-05,
        "epoch": 0.4825333333333333,
        "step": 3619
    },
    {
        "loss": 2.0749,
        "grad_norm": 2.541074275970459,
        "learning_rate": 3.630610724374285e-05,
        "epoch": 0.4826666666666667,
        "step": 3620
    },
    {
        "loss": 1.8776,
        "grad_norm": 4.6219072341918945,
        "learning_rate": 3.6257601025131026e-05,
        "epoch": 0.4828,
        "step": 3621
    },
    {
        "loss": 2.2237,
        "grad_norm": 3.4190351963043213,
        "learning_rate": 3.620912005510068e-05,
        "epoch": 0.4829333333333333,
        "step": 3622
    },
    {
        "loss": 2.4815,
        "grad_norm": 2.5839831829071045,
        "learning_rate": 3.6160664352855246e-05,
        "epoch": 0.48306666666666664,
        "step": 3623
    },
    {
        "loss": 2.3732,
        "grad_norm": 3.2676167488098145,
        "learning_rate": 3.61122339375881e-05,
        "epoch": 0.4832,
        "step": 3624
    },
    {
        "loss": 1.7782,
        "grad_norm": 2.97890043258667,
        "learning_rate": 3.606382882848284e-05,
        "epoch": 0.48333333333333334,
        "step": 3625
    },
    {
        "loss": 2.7046,
        "grad_norm": 3.6601014137268066,
        "learning_rate": 3.601544904471286e-05,
        "epoch": 0.48346666666666666,
        "step": 3626
    },
    {
        "loss": 0.8557,
        "grad_norm": 1.6637567281723022,
        "learning_rate": 3.596709460544154e-05,
        "epoch": 0.4836,
        "step": 3627
    },
    {
        "loss": 1.4714,
        "grad_norm": 3.785682201385498,
        "learning_rate": 3.591876552982216e-05,
        "epoch": 0.48373333333333335,
        "step": 3628
    },
    {
        "loss": 2.1748,
        "grad_norm": 3.635953426361084,
        "learning_rate": 3.587046183699818e-05,
        "epoch": 0.48386666666666667,
        "step": 3629
    },
    {
        "loss": 2.7749,
        "grad_norm": 2.5936503410339355,
        "learning_rate": 3.5822183546102786e-05,
        "epoch": 0.484,
        "step": 3630
    },
    {
        "loss": 2.8972,
        "grad_norm": 2.2319350242614746,
        "learning_rate": 3.5773930676259127e-05,
        "epoch": 0.48413333333333336,
        "step": 3631
    },
    {
        "loss": 2.4221,
        "grad_norm": 3.4041907787323,
        "learning_rate": 3.572570324658041e-05,
        "epoch": 0.4842666666666667,
        "step": 3632
    },
    {
        "loss": 0.8585,
        "grad_norm": 2.99568510055542,
        "learning_rate": 3.567750127616961e-05,
        "epoch": 0.4844,
        "step": 3633
    },
    {
        "loss": 1.7553,
        "grad_norm": 3.4426627159118652,
        "learning_rate": 3.562932478411973e-05,
        "epoch": 0.4845333333333333,
        "step": 3634
    },
    {
        "loss": 2.2531,
        "grad_norm": 3.4081037044525146,
        "learning_rate": 3.5581173789513674e-05,
        "epoch": 0.4846666666666667,
        "step": 3635
    },
    {
        "loss": 2.4071,
        "grad_norm": 2.6254444122314453,
        "learning_rate": 3.553304831142414e-05,
        "epoch": 0.4848,
        "step": 3636
    },
    {
        "loss": 2.4118,
        "grad_norm": 3.2845945358276367,
        "learning_rate": 3.548494836891385e-05,
        "epoch": 0.4849333333333333,
        "step": 3637
    },
    {
        "loss": 2.8838,
        "grad_norm": 2.7412099838256836,
        "learning_rate": 3.543687398103537e-05,
        "epoch": 0.48506666666666665,
        "step": 3638
    },
    {
        "loss": 2.6155,
        "grad_norm": 2.9790618419647217,
        "learning_rate": 3.53888251668311e-05,
        "epoch": 0.4852,
        "step": 3639
    },
    {
        "loss": 3.2306,
        "grad_norm": 3.873558521270752,
        "learning_rate": 3.5340801945333304e-05,
        "epoch": 0.48533333333333334,
        "step": 3640
    },
    {
        "loss": 2.6846,
        "grad_norm": 2.255826234817505,
        "learning_rate": 3.529280433556417e-05,
        "epoch": 0.48546666666666666,
        "step": 3641
    },
    {
        "loss": 2.7113,
        "grad_norm": 2.243596315383911,
        "learning_rate": 3.5244832356535764e-05,
        "epoch": 0.4856,
        "step": 3642
    },
    {
        "loss": 2.147,
        "grad_norm": 3.835665464401245,
        "learning_rate": 3.519688602724987e-05,
        "epoch": 0.48573333333333335,
        "step": 3643
    },
    {
        "loss": 2.3324,
        "grad_norm": 3.5837819576263428,
        "learning_rate": 3.514896536669824e-05,
        "epoch": 0.48586666666666667,
        "step": 3644
    },
    {
        "loss": 2.0316,
        "grad_norm": 2.4233930110931396,
        "learning_rate": 3.51010703938624e-05,
        "epoch": 0.486,
        "step": 3645
    },
    {
        "loss": 2.6219,
        "grad_norm": 2.370176315307617,
        "learning_rate": 3.505320112771375e-05,
        "epoch": 0.4861333333333333,
        "step": 3646
    },
    {
        "loss": 1.9263,
        "grad_norm": 2.8493382930755615,
        "learning_rate": 3.5005357587213395e-05,
        "epoch": 0.4862666666666667,
        "step": 3647
    },
    {
        "loss": 2.5396,
        "grad_norm": 2.6331710815429688,
        "learning_rate": 3.49575397913124e-05,
        "epoch": 0.4864,
        "step": 3648
    },
    {
        "loss": 2.907,
        "grad_norm": 2.9173755645751953,
        "learning_rate": 3.490974775895153e-05,
        "epoch": 0.4865333333333333,
        "step": 3649
    },
    {
        "loss": 1.1971,
        "grad_norm": 3.888268232345581,
        "learning_rate": 3.486198150906128e-05,
        "epoch": 0.4866666666666667,
        "step": 3650
    },
    {
        "loss": 1.8566,
        "grad_norm": 3.3127498626708984,
        "learning_rate": 3.481424106056218e-05,
        "epoch": 0.4868,
        "step": 3651
    },
    {
        "loss": 2.3769,
        "grad_norm": 3.151123046875,
        "learning_rate": 3.476652643236431e-05,
        "epoch": 0.48693333333333333,
        "step": 3652
    },
    {
        "loss": 2.622,
        "grad_norm": 2.5809364318847656,
        "learning_rate": 3.471883764336753e-05,
        "epoch": 0.48706666666666665,
        "step": 3653
    },
    {
        "loss": 3.0519,
        "grad_norm": 2.2379353046417236,
        "learning_rate": 3.4671174712461675e-05,
        "epoch": 0.4872,
        "step": 3654
    },
    {
        "loss": 2.5723,
        "grad_norm": 2.814732551574707,
        "learning_rate": 3.46235376585261e-05,
        "epoch": 0.48733333333333334,
        "step": 3655
    },
    {
        "loss": 1.6441,
        "grad_norm": 3.583381175994873,
        "learning_rate": 3.457592650043001e-05,
        "epoch": 0.48746666666666666,
        "step": 3656
    },
    {
        "loss": 2.264,
        "grad_norm": 3.2151248455047607,
        "learning_rate": 3.452834125703238e-05,
        "epoch": 0.4876,
        "step": 3657
    },
    {
        "loss": 2.6645,
        "grad_norm": 2.7082254886627197,
        "learning_rate": 3.448078194718184e-05,
        "epoch": 0.48773333333333335,
        "step": 3658
    },
    {
        "loss": 0.6764,
        "grad_norm": 3.170323371887207,
        "learning_rate": 3.4433248589716825e-05,
        "epoch": 0.48786666666666667,
        "step": 3659
    },
    {
        "loss": 1.2498,
        "grad_norm": 3.6198689937591553,
        "learning_rate": 3.438574120346548e-05,
        "epoch": 0.488,
        "step": 3660
    },
    {
        "loss": 1.9091,
        "grad_norm": 3.984893798828125,
        "learning_rate": 3.43382598072456e-05,
        "epoch": 0.4881333333333333,
        "step": 3661
    },
    {
        "loss": 3.2686,
        "grad_norm": 2.9077210426330566,
        "learning_rate": 3.429080441986474e-05,
        "epoch": 0.4882666666666667,
        "step": 3662
    },
    {
        "loss": 2.0625,
        "grad_norm": 3.478099822998047,
        "learning_rate": 3.424337506012017e-05,
        "epoch": 0.4884,
        "step": 3663
    },
    {
        "loss": 1.8745,
        "grad_norm": 2.5687382221221924,
        "learning_rate": 3.419597174679882e-05,
        "epoch": 0.4885333333333333,
        "step": 3664
    },
    {
        "loss": 2.2334,
        "grad_norm": 2.626556634902954,
        "learning_rate": 3.4148594498677235e-05,
        "epoch": 0.4886666666666667,
        "step": 3665
    },
    {
        "loss": 2.608,
        "grad_norm": 1.327838659286499,
        "learning_rate": 3.410124333452176e-05,
        "epoch": 0.4888,
        "step": 3666
    },
    {
        "loss": 1.8783,
        "grad_norm": 2.9305875301361084,
        "learning_rate": 3.4053918273088334e-05,
        "epoch": 0.48893333333333333,
        "step": 3667
    },
    {
        "loss": 5.0084,
        "grad_norm": 4.209469318389893,
        "learning_rate": 3.4006619333122635e-05,
        "epoch": 0.48906666666666665,
        "step": 3668
    },
    {
        "loss": 2.5332,
        "grad_norm": 2.404916286468506,
        "learning_rate": 3.395934653335984e-05,
        "epoch": 0.4892,
        "step": 3669
    },
    {
        "loss": 0.7282,
        "grad_norm": 2.505382537841797,
        "learning_rate": 3.391209989252492e-05,
        "epoch": 0.48933333333333334,
        "step": 3670
    },
    {
        "loss": 2.4788,
        "grad_norm": 2.468282699584961,
        "learning_rate": 3.386487942933244e-05,
        "epoch": 0.48946666666666666,
        "step": 3671
    },
    {
        "loss": 3.0755,
        "grad_norm": 4.887845039367676,
        "learning_rate": 3.3817685162486546e-05,
        "epoch": 0.4896,
        "step": 3672
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.5183491706848145,
        "learning_rate": 3.37705171106811e-05,
        "epoch": 0.48973333333333335,
        "step": 3673
    },
    {
        "loss": 3.0112,
        "grad_norm": 3.64089298248291,
        "learning_rate": 3.37233752925995e-05,
        "epoch": 0.4898666666666667,
        "step": 3674
    },
    {
        "loss": 2.4219,
        "grad_norm": 3.846903085708618,
        "learning_rate": 3.367625972691471e-05,
        "epoch": 0.49,
        "step": 3675
    },
    {
        "loss": 2.5661,
        "grad_norm": 2.910945415496826,
        "learning_rate": 3.36291704322895e-05,
        "epoch": 0.4901333333333333,
        "step": 3676
    },
    {
        "loss": 2.8274,
        "grad_norm": 2.1847288608551025,
        "learning_rate": 3.358210742737604e-05,
        "epoch": 0.4902666666666667,
        "step": 3677
    },
    {
        "loss": 3.4905,
        "grad_norm": 2.9373443126678467,
        "learning_rate": 3.353507073081608e-05,
        "epoch": 0.4904,
        "step": 3678
    },
    {
        "loss": 2.7399,
        "grad_norm": 2.1707026958465576,
        "learning_rate": 3.348806036124113e-05,
        "epoch": 0.4905333333333333,
        "step": 3679
    },
    {
        "loss": 2.0505,
        "grad_norm": 3.238394260406494,
        "learning_rate": 3.3441076337272116e-05,
        "epoch": 0.49066666666666664,
        "step": 3680
    },
    {
        "loss": 1.5768,
        "grad_norm": 3.5041983127593994,
        "learning_rate": 3.339411867751951e-05,
        "epoch": 0.4908,
        "step": 3681
    },
    {
        "loss": 1.7805,
        "grad_norm": 3.2212016582489014,
        "learning_rate": 3.33471874005835e-05,
        "epoch": 0.49093333333333333,
        "step": 3682
    },
    {
        "loss": 2.3154,
        "grad_norm": 3.414931058883667,
        "learning_rate": 3.330028252505363e-05,
        "epoch": 0.49106666666666665,
        "step": 3683
    },
    {
        "loss": 3.1186,
        "grad_norm": 2.643542766571045,
        "learning_rate": 3.325340406950913e-05,
        "epoch": 0.4912,
        "step": 3684
    },
    {
        "loss": 1.591,
        "grad_norm": 2.797011375427246,
        "learning_rate": 3.320655205251875e-05,
        "epoch": 0.49133333333333334,
        "step": 3685
    },
    {
        "loss": 2.2764,
        "grad_norm": 5.567631721496582,
        "learning_rate": 3.3159726492640655e-05,
        "epoch": 0.49146666666666666,
        "step": 3686
    },
    {
        "loss": 2.9274,
        "grad_norm": 5.431581497192383,
        "learning_rate": 3.3112927408422645e-05,
        "epoch": 0.4916,
        "step": 3687
    },
    {
        "loss": 2.5485,
        "grad_norm": 3.0452568531036377,
        "learning_rate": 3.3066154818402007e-05,
        "epoch": 0.49173333333333336,
        "step": 3688
    },
    {
        "loss": 1.4307,
        "grad_norm": 2.1745166778564453,
        "learning_rate": 3.301940874110555e-05,
        "epoch": 0.4918666666666667,
        "step": 3689
    },
    {
        "loss": 2.2185,
        "grad_norm": 4.576330184936523,
        "learning_rate": 3.297268919504952e-05,
        "epoch": 0.492,
        "step": 3690
    },
    {
        "loss": 1.9328,
        "grad_norm": 2.6487107276916504,
        "learning_rate": 3.2925996198739664e-05,
        "epoch": 0.4921333333333333,
        "step": 3691
    },
    {
        "loss": 2.0355,
        "grad_norm": 3.0382513999938965,
        "learning_rate": 3.2879329770671255e-05,
        "epoch": 0.4922666666666667,
        "step": 3692
    },
    {
        "loss": 2.496,
        "grad_norm": 3.540186882019043,
        "learning_rate": 3.283268992932906e-05,
        "epoch": 0.4924,
        "step": 3693
    },
    {
        "loss": 2.1243,
        "grad_norm": 4.348667144775391,
        "learning_rate": 3.278607669318723e-05,
        "epoch": 0.4925333333333333,
        "step": 3694
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.9685910940170288,
        "learning_rate": 3.273949008070943e-05,
        "epoch": 0.49266666666666664,
        "step": 3695
    },
    {
        "loss": 2.4987,
        "grad_norm": 2.556317090988159,
        "learning_rate": 3.269293011034884e-05,
        "epoch": 0.4928,
        "step": 3696
    },
    {
        "loss": 2.6098,
        "grad_norm": 2.8849663734436035,
        "learning_rate": 3.264639680054795e-05,
        "epoch": 0.49293333333333333,
        "step": 3697
    },
    {
        "loss": 2.4129,
        "grad_norm": 2.4282078742980957,
        "learning_rate": 3.2599890169738815e-05,
        "epoch": 0.49306666666666665,
        "step": 3698
    },
    {
        "loss": 2.8193,
        "grad_norm": 3.339005708694458,
        "learning_rate": 3.255341023634284e-05,
        "epoch": 0.4932,
        "step": 3699
    },
    {
        "loss": 2.9381,
        "grad_norm": 2.6613054275512695,
        "learning_rate": 3.250695701877082e-05,
        "epoch": 0.49333333333333335,
        "step": 3700
    },
    {
        "loss": 1.9012,
        "grad_norm": 2.8454790115356445,
        "learning_rate": 3.246053053542318e-05,
        "epoch": 0.49346666666666666,
        "step": 3701
    },
    {
        "loss": 1.7299,
        "grad_norm": 5.424699306488037,
        "learning_rate": 3.241413080468953e-05,
        "epoch": 0.4936,
        "step": 3702
    },
    {
        "loss": 2.433,
        "grad_norm": 2.6331682205200195,
        "learning_rate": 3.236775784494891e-05,
        "epoch": 0.49373333333333336,
        "step": 3703
    },
    {
        "loss": 2.0572,
        "grad_norm": 3.7075536251068115,
        "learning_rate": 3.232141167456988e-05,
        "epoch": 0.4938666666666667,
        "step": 3704
    },
    {
        "loss": 3.7068,
        "grad_norm": 5.628732204437256,
        "learning_rate": 3.227509231191032e-05,
        "epoch": 0.494,
        "step": 3705
    },
    {
        "loss": 2.7831,
        "grad_norm": 2.023293972015381,
        "learning_rate": 3.222879977531742e-05,
        "epoch": 0.4941333333333333,
        "step": 3706
    },
    {
        "loss": 2.4017,
        "grad_norm": 4.013097763061523,
        "learning_rate": 3.2182534083127866e-05,
        "epoch": 0.4942666666666667,
        "step": 3707
    },
    {
        "loss": 2.6719,
        "grad_norm": 2.143298864364624,
        "learning_rate": 3.213629525366767e-05,
        "epoch": 0.4944,
        "step": 3708
    },
    {
        "loss": 2.9145,
        "grad_norm": 2.0426011085510254,
        "learning_rate": 3.209008330525214e-05,
        "epoch": 0.4945333333333333,
        "step": 3709
    },
    {
        "loss": 2.813,
        "grad_norm": 4.652870178222656,
        "learning_rate": 3.2043898256186e-05,
        "epoch": 0.49466666666666664,
        "step": 3710
    },
    {
        "loss": 2.8321,
        "grad_norm": 2.002689838409424,
        "learning_rate": 3.1997740124763355e-05,
        "epoch": 0.4948,
        "step": 3711
    },
    {
        "loss": 1.3975,
        "grad_norm": 4.870621681213379,
        "learning_rate": 3.1951608929267516e-05,
        "epoch": 0.49493333333333334,
        "step": 3712
    },
    {
        "loss": 2.1304,
        "grad_norm": 3.0406508445739746,
        "learning_rate": 3.190550468797126e-05,
        "epoch": 0.49506666666666665,
        "step": 3713
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.540971279144287,
        "learning_rate": 3.1859427419136645e-05,
        "epoch": 0.4952,
        "step": 3714
    },
    {
        "loss": 2.8185,
        "grad_norm": 3.013062000274658,
        "learning_rate": 3.181337714101502e-05,
        "epoch": 0.49533333333333335,
        "step": 3715
    },
    {
        "loss": 2.2453,
        "grad_norm": 3.0578513145446777,
        "learning_rate": 3.1767353871847016e-05,
        "epoch": 0.49546666666666667,
        "step": 3716
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.6899876594543457,
        "learning_rate": 3.1721357629862644e-05,
        "epoch": 0.4956,
        "step": 3717
    },
    {
        "loss": 2.071,
        "grad_norm": 2.50893235206604,
        "learning_rate": 3.1675388433281204e-05,
        "epoch": 0.49573333333333336,
        "step": 3718
    },
    {
        "loss": 1.0187,
        "grad_norm": 4.420984745025635,
        "learning_rate": 3.162944630031117e-05,
        "epoch": 0.4958666666666667,
        "step": 3719
    },
    {
        "loss": 1.8749,
        "grad_norm": 3.2919273376464844,
        "learning_rate": 3.158353124915043e-05,
        "epoch": 0.496,
        "step": 3720
    },
    {
        "loss": 2.1435,
        "grad_norm": 4.340289115905762,
        "learning_rate": 3.153764329798613e-05,
        "epoch": 0.4961333333333333,
        "step": 3721
    },
    {
        "loss": 2.6173,
        "grad_norm": 3.775786876678467,
        "learning_rate": 3.149178246499456e-05,
        "epoch": 0.4962666666666667,
        "step": 3722
    },
    {
        "loss": 2.0948,
        "grad_norm": 3.2073862552642822,
        "learning_rate": 3.144594876834141e-05,
        "epoch": 0.4964,
        "step": 3723
    },
    {
        "loss": 1.9561,
        "grad_norm": 3.9672951698303223,
        "learning_rate": 3.140014222618156e-05,
        "epoch": 0.4965333333333333,
        "step": 3724
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.8344225883483887,
        "learning_rate": 3.1354362856659045e-05,
        "epoch": 0.49666666666666665,
        "step": 3725
    },
    {
        "loss": 2.6425,
        "grad_norm": 2.3992674350738525,
        "learning_rate": 3.130861067790736e-05,
        "epoch": 0.4968,
        "step": 3726
    },
    {
        "loss": 1.3725,
        "grad_norm": 3.6382110118865967,
        "learning_rate": 3.126288570804906e-05,
        "epoch": 0.49693333333333334,
        "step": 3727
    },
    {
        "loss": 2.7053,
        "grad_norm": 2.6151647567749023,
        "learning_rate": 3.121718796519595e-05,
        "epoch": 0.49706666666666666,
        "step": 3728
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.8603265285491943,
        "learning_rate": 3.1171517467448994e-05,
        "epoch": 0.4972,
        "step": 3729
    },
    {
        "loss": 2.5501,
        "grad_norm": 2.754863739013672,
        "learning_rate": 3.112587423289857e-05,
        "epoch": 0.49733333333333335,
        "step": 3730
    },
    {
        "loss": 1.5834,
        "grad_norm": 3.8525166511535645,
        "learning_rate": 3.108025827962404e-05,
        "epoch": 0.49746666666666667,
        "step": 3731
    },
    {
        "loss": 2.4177,
        "grad_norm": 3.211623430252075,
        "learning_rate": 3.1034669625694e-05,
        "epoch": 0.4976,
        "step": 3732
    },
    {
        "loss": 2.3833,
        "grad_norm": 3.7487599849700928,
        "learning_rate": 3.098910828916637e-05,
        "epoch": 0.49773333333333336,
        "step": 3733
    },
    {
        "loss": 2.4322,
        "grad_norm": 2.244173049926758,
        "learning_rate": 3.094357428808804e-05,
        "epoch": 0.4978666666666667,
        "step": 3734
    },
    {
        "loss": 1.1354,
        "grad_norm": 3.508929491043091,
        "learning_rate": 3.089806764049526e-05,
        "epoch": 0.498,
        "step": 3735
    },
    {
        "loss": 1.5988,
        "grad_norm": 4.2164626121521,
        "learning_rate": 3.085258836441336e-05,
        "epoch": 0.4981333333333333,
        "step": 3736
    },
    {
        "loss": 2.6773,
        "grad_norm": 1.8298332691192627,
        "learning_rate": 3.08071364778568e-05,
        "epoch": 0.4982666666666667,
        "step": 3737
    },
    {
        "loss": 2.3736,
        "grad_norm": 2.6977038383483887,
        "learning_rate": 3.0761711998829245e-05,
        "epoch": 0.4984,
        "step": 3738
    },
    {
        "loss": 2.6602,
        "grad_norm": 3.2835628986358643,
        "learning_rate": 3.071631494532353e-05,
        "epoch": 0.49853333333333333,
        "step": 3739
    },
    {
        "loss": 2.3065,
        "grad_norm": 2.686183214187622,
        "learning_rate": 3.067094533532154e-05,
        "epoch": 0.49866666666666665,
        "step": 3740
    },
    {
        "loss": 1.603,
        "grad_norm": 3.3824551105499268,
        "learning_rate": 3.062560318679431e-05,
        "epoch": 0.4988,
        "step": 3741
    },
    {
        "loss": 2.4588,
        "grad_norm": 3.0214991569519043,
        "learning_rate": 3.058028851770203e-05,
        "epoch": 0.49893333333333334,
        "step": 3742
    },
    {
        "loss": 1.7931,
        "grad_norm": 2.9259121417999268,
        "learning_rate": 3.0535001345994054e-05,
        "epoch": 0.49906666666666666,
        "step": 3743
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.6699059009552,
        "learning_rate": 3.048974168960871e-05,
        "epoch": 0.4992,
        "step": 3744
    },
    {
        "loss": 2.6006,
        "grad_norm": 2.9520187377929688,
        "learning_rate": 3.0444509566473522e-05,
        "epoch": 0.49933333333333335,
        "step": 3745
    },
    {
        "loss": 2.2663,
        "grad_norm": 2.9254231452941895,
        "learning_rate": 3.0399304994505107e-05,
        "epoch": 0.49946666666666667,
        "step": 3746
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.970067024230957,
        "learning_rate": 3.035412799160917e-05,
        "epoch": 0.4996,
        "step": 3747
    },
    {
        "loss": 3.2432,
        "grad_norm": 3.0294642448425293,
        "learning_rate": 3.0308978575680424e-05,
        "epoch": 0.4997333333333333,
        "step": 3748
    },
    {
        "loss": 2.4896,
        "grad_norm": 4.750515460968018,
        "learning_rate": 3.026385676460276e-05,
        "epoch": 0.4998666666666667,
        "step": 3749
    },
    {
        "loss": 2.6044,
        "grad_norm": 1.8619403839111328,
        "learning_rate": 3.021876257624905e-05,
        "epoch": 0.5,
        "step": 3750
    },
    {
        "loss": 2.9237,
        "grad_norm": 1.9233996868133545,
        "learning_rate": 3.017369602848119e-05,
        "epoch": 0.5001333333333333,
        "step": 3751
    },
    {
        "loss": 2.5054,
        "grad_norm": 2.296909809112549,
        "learning_rate": 3.012865713915033e-05,
        "epoch": 0.5002666666666666,
        "step": 3752
    },
    {
        "loss": 2.1591,
        "grad_norm": 3.374016284942627,
        "learning_rate": 3.008364592609645e-05,
        "epoch": 0.5004,
        "step": 3753
    },
    {
        "loss": 2.8212,
        "grad_norm": 3.094137668609619,
        "learning_rate": 3.00386624071486e-05,
        "epoch": 0.5005333333333334,
        "step": 3754
    },
    {
        "loss": 2.0228,
        "grad_norm": 3.5481555461883545,
        "learning_rate": 2.9993706600125015e-05,
        "epoch": 0.5006666666666667,
        "step": 3755
    },
    {
        "loss": 1.7499,
        "grad_norm": 1.8517400026321411,
        "learning_rate": 2.9948778522832788e-05,
        "epoch": 0.5008,
        "step": 3756
    },
    {
        "loss": 2.3523,
        "grad_norm": 3.0407094955444336,
        "learning_rate": 2.9903878193068048e-05,
        "epoch": 0.5009333333333333,
        "step": 3757
    },
    {
        "loss": 2.1401,
        "grad_norm": 2.892833948135376,
        "learning_rate": 2.9859005628616032e-05,
        "epoch": 0.5010666666666667,
        "step": 3758
    },
    {
        "loss": 2.2412,
        "grad_norm": 3.428654432296753,
        "learning_rate": 2.981416084725086e-05,
        "epoch": 0.5012,
        "step": 3759
    },
    {
        "loss": 2.2587,
        "grad_norm": 2.903550386428833,
        "learning_rate": 2.9769343866735734e-05,
        "epoch": 0.5013333333333333,
        "step": 3760
    },
    {
        "loss": 2.0609,
        "grad_norm": 3.619295835494995,
        "learning_rate": 2.9724554704822828e-05,
        "epoch": 0.5014666666666666,
        "step": 3761
    },
    {
        "loss": 1.9615,
        "grad_norm": 3.0062501430511475,
        "learning_rate": 2.967979337925323e-05,
        "epoch": 0.5016,
        "step": 3762
    },
    {
        "loss": 2.307,
        "grad_norm": 2.372021436691284,
        "learning_rate": 2.963505990775709e-05,
        "epoch": 0.5017333333333334,
        "step": 3763
    },
    {
        "loss": 2.6529,
        "grad_norm": 2.791414260864258,
        "learning_rate": 2.9590354308053514e-05,
        "epoch": 0.5018666666666667,
        "step": 3764
    },
    {
        "loss": 2.8969,
        "grad_norm": 2.711599826812744,
        "learning_rate": 2.9545676597850503e-05,
        "epoch": 0.502,
        "step": 3765
    },
    {
        "loss": 2.496,
        "grad_norm": 2.523528814315796,
        "learning_rate": 2.9501026794845044e-05,
        "epoch": 0.5021333333333333,
        "step": 3766
    },
    {
        "loss": 2.5831,
        "grad_norm": 2.970529079437256,
        "learning_rate": 2.945640491672308e-05,
        "epoch": 0.5022666666666666,
        "step": 3767
    },
    {
        "loss": 1.8249,
        "grad_norm": 4.1631269454956055,
        "learning_rate": 2.9411810981159492e-05,
        "epoch": 0.5024,
        "step": 3768
    },
    {
        "loss": 2.4665,
        "grad_norm": 2.045233964920044,
        "learning_rate": 2.9367245005818146e-05,
        "epoch": 0.5025333333333334,
        "step": 3769
    },
    {
        "loss": 1.3665,
        "grad_norm": 3.3190054893493652,
        "learning_rate": 2.932270700835169e-05,
        "epoch": 0.5026666666666667,
        "step": 3770
    },
    {
        "loss": 2.2921,
        "grad_norm": 2.504143238067627,
        "learning_rate": 2.927819700640181e-05,
        "epoch": 0.5028,
        "step": 3771
    },
    {
        "loss": 1.8704,
        "grad_norm": 3.7100977897644043,
        "learning_rate": 2.9233715017599117e-05,
        "epoch": 0.5029333333333333,
        "step": 3772
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.9113069772720337,
        "learning_rate": 2.918926105956299e-05,
        "epoch": 0.5030666666666667,
        "step": 3773
    },
    {
        "loss": 3.9439,
        "grad_norm": 3.1775503158569336,
        "learning_rate": 2.9144835149901916e-05,
        "epoch": 0.5032,
        "step": 3774
    },
    {
        "loss": 2.1669,
        "grad_norm": 2.8800034523010254,
        "learning_rate": 2.9100437306213003e-05,
        "epoch": 0.5033333333333333,
        "step": 3775
    },
    {
        "loss": 2.3075,
        "grad_norm": 2.559467315673828,
        "learning_rate": 2.9056067546082423e-05,
        "epoch": 0.5034666666666666,
        "step": 3776
    },
    {
        "loss": 2.6505,
        "grad_norm": 2.659677028656006,
        "learning_rate": 2.9011725887085283e-05,
        "epoch": 0.5036,
        "step": 3777
    },
    {
        "loss": 2.0838,
        "grad_norm": 3.124828577041626,
        "learning_rate": 2.8967412346785385e-05,
        "epoch": 0.5037333333333334,
        "step": 3778
    },
    {
        "loss": 2.4829,
        "grad_norm": 3.4256746768951416,
        "learning_rate": 2.892312694273549e-05,
        "epoch": 0.5038666666666667,
        "step": 3779
    },
    {
        "loss": 2.4488,
        "grad_norm": 2.68837571144104,
        "learning_rate": 2.8878869692477194e-05,
        "epoch": 0.504,
        "step": 3780
    },
    {
        "loss": 2.5985,
        "grad_norm": 3.055088758468628,
        "learning_rate": 2.883464061354103e-05,
        "epoch": 0.5041333333333333,
        "step": 3781
    },
    {
        "loss": 2.904,
        "grad_norm": 4.616458892822266,
        "learning_rate": 2.879043972344614e-05,
        "epoch": 0.5042666666666666,
        "step": 3782
    },
    {
        "loss": 2.3336,
        "grad_norm": 3.255751848220825,
        "learning_rate": 2.8746267039700735e-05,
        "epoch": 0.5044,
        "step": 3783
    },
    {
        "loss": 2.7434,
        "grad_norm": 2.7746944427490234,
        "learning_rate": 2.8702122579801728e-05,
        "epoch": 0.5045333333333333,
        "step": 3784
    },
    {
        "loss": 2.8713,
        "grad_norm": 2.6393826007843018,
        "learning_rate": 2.8658006361234912e-05,
        "epoch": 0.5046666666666667,
        "step": 3785
    },
    {
        "loss": 2.6705,
        "grad_norm": 3.1151742935180664,
        "learning_rate": 2.861391840147486e-05,
        "epoch": 0.5048,
        "step": 3786
    },
    {
        "loss": 2.6464,
        "grad_norm": 2.694196939468384,
        "learning_rate": 2.856985871798501e-05,
        "epoch": 0.5049333333333333,
        "step": 3787
    },
    {
        "loss": 2.6079,
        "grad_norm": 2.3798365592956543,
        "learning_rate": 2.852582732821747e-05,
        "epoch": 0.5050666666666667,
        "step": 3788
    },
    {
        "loss": 2.0799,
        "grad_norm": 2.747152328491211,
        "learning_rate": 2.848182424961331e-05,
        "epoch": 0.5052,
        "step": 3789
    },
    {
        "loss": 2.1613,
        "grad_norm": 2.120030164718628,
        "learning_rate": 2.843784949960222e-05,
        "epoch": 0.5053333333333333,
        "step": 3790
    },
    {
        "loss": 0.8239,
        "grad_norm": 3.4469456672668457,
        "learning_rate": 2.8393903095602824e-05,
        "epoch": 0.5054666666666666,
        "step": 3791
    },
    {
        "loss": 1.1699,
        "grad_norm": 5.636563777923584,
        "learning_rate": 2.8349985055022366e-05,
        "epoch": 0.5056,
        "step": 3792
    },
    {
        "loss": 2.095,
        "grad_norm": 3.15088152885437,
        "learning_rate": 2.8306095395257004e-05,
        "epoch": 0.5057333333333334,
        "step": 3793
    },
    {
        "loss": 2.7268,
        "grad_norm": 1.8076021671295166,
        "learning_rate": 2.826223413369151e-05,
        "epoch": 0.5058666666666667,
        "step": 3794
    },
    {
        "loss": 3.2703,
        "grad_norm": 2.2487845420837402,
        "learning_rate": 2.8218401287699525e-05,
        "epoch": 0.506,
        "step": 3795
    },
    {
        "loss": 2.249,
        "grad_norm": 3.3523941040039062,
        "learning_rate": 2.8174596874643367e-05,
        "epoch": 0.5061333333333333,
        "step": 3796
    },
    {
        "loss": 2.298,
        "grad_norm": 3.6077635288238525,
        "learning_rate": 2.8130820911874132e-05,
        "epoch": 0.5062666666666666,
        "step": 3797
    },
    {
        "loss": 2.3031,
        "grad_norm": 2.187666177749634,
        "learning_rate": 2.808707341673166e-05,
        "epoch": 0.5064,
        "step": 3798
    },
    {
        "loss": 3.1416,
        "grad_norm": 3.735522508621216,
        "learning_rate": 2.8043354406544407e-05,
        "epoch": 0.5065333333333333,
        "step": 3799
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.5322763919830322,
        "learning_rate": 2.7999663898629723e-05,
        "epoch": 0.5066666666666667,
        "step": 3800
    },
    {
        "loss": 3.1074,
        "grad_norm": 2.819535970687866,
        "learning_rate": 2.7956001910293417e-05,
        "epoch": 0.5068,
        "step": 3801
    },
    {
        "loss": 2.1105,
        "grad_norm": 3.60579252243042,
        "learning_rate": 2.7912368458830297e-05,
        "epoch": 0.5069333333333333,
        "step": 3802
    },
    {
        "loss": 1.9167,
        "grad_norm": 3.633824348449707,
        "learning_rate": 2.7868763561523625e-05,
        "epoch": 0.5070666666666667,
        "step": 3803
    },
    {
        "loss": 1.8915,
        "grad_norm": 3.6427760124206543,
        "learning_rate": 2.782518723564552e-05,
        "epoch": 0.5072,
        "step": 3804
    },
    {
        "loss": 2.5752,
        "grad_norm": 3.485645294189453,
        "learning_rate": 2.7781639498456624e-05,
        "epoch": 0.5073333333333333,
        "step": 3805
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.993293046951294,
        "learning_rate": 2.773812036720649e-05,
        "epoch": 0.5074666666666666,
        "step": 3806
    },
    {
        "loss": 1.5853,
        "grad_norm": 3.86678147315979,
        "learning_rate": 2.7694629859133047e-05,
        "epoch": 0.5076,
        "step": 3807
    },
    {
        "loss": 2.5062,
        "grad_norm": 2.3264338970184326,
        "learning_rate": 2.765116799146311e-05,
        "epoch": 0.5077333333333334,
        "step": 3808
    },
    {
        "loss": 2.4457,
        "grad_norm": 3.0618245601654053,
        "learning_rate": 2.7607734781412044e-05,
        "epoch": 0.5078666666666667,
        "step": 3809
    },
    {
        "loss": 3.3052,
        "grad_norm": 3.0214483737945557,
        "learning_rate": 2.756433024618389e-05,
        "epoch": 0.508,
        "step": 3810
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.5585076808929443,
        "learning_rate": 2.7520954402971344e-05,
        "epoch": 0.5081333333333333,
        "step": 3811
    },
    {
        "loss": 2.0489,
        "grad_norm": 3.4413163661956787,
        "learning_rate": 2.747760726895575e-05,
        "epoch": 0.5082666666666666,
        "step": 3812
    },
    {
        "loss": 2.1714,
        "grad_norm": 2.86279034614563,
        "learning_rate": 2.743428886130701e-05,
        "epoch": 0.5084,
        "step": 3813
    },
    {
        "loss": 2.621,
        "grad_norm": 2.5080485343933105,
        "learning_rate": 2.739099919718374e-05,
        "epoch": 0.5085333333333333,
        "step": 3814
    },
    {
        "loss": 2.2565,
        "grad_norm": 2.2930233478546143,
        "learning_rate": 2.7347738293733062e-05,
        "epoch": 0.5086666666666667,
        "step": 3815
    },
    {
        "loss": 2.3347,
        "grad_norm": 2.386246919631958,
        "learning_rate": 2.7304506168090838e-05,
        "epoch": 0.5088,
        "step": 3816
    },
    {
        "loss": 2.0689,
        "grad_norm": 3.4685044288635254,
        "learning_rate": 2.7261302837381398e-05,
        "epoch": 0.5089333333333333,
        "step": 3817
    },
    {
        "loss": 2.709,
        "grad_norm": 2.900519609451294,
        "learning_rate": 2.7218128318717752e-05,
        "epoch": 0.5090666666666667,
        "step": 3818
    },
    {
        "loss": 2.7816,
        "grad_norm": 2.8419387340545654,
        "learning_rate": 2.7174982629201505e-05,
        "epoch": 0.5092,
        "step": 3819
    },
    {
        "loss": 2.0918,
        "grad_norm": 2.634073257446289,
        "learning_rate": 2.713186578592276e-05,
        "epoch": 0.5093333333333333,
        "step": 3820
    },
    {
        "loss": 1.5409,
        "grad_norm": 2.3672945499420166,
        "learning_rate": 2.7088777805960263e-05,
        "epoch": 0.5094666666666666,
        "step": 3821
    },
    {
        "loss": 2.2408,
        "grad_norm": 3.979495048522949,
        "learning_rate": 2.7045718706381317e-05,
        "epoch": 0.5096,
        "step": 3822
    },
    {
        "loss": 2.3682,
        "grad_norm": 2.3932089805603027,
        "learning_rate": 2.7002688504241802e-05,
        "epoch": 0.5097333333333334,
        "step": 3823
    },
    {
        "loss": 2.0206,
        "grad_norm": 2.9932618141174316,
        "learning_rate": 2.6959687216586084e-05,
        "epoch": 0.5098666666666667,
        "step": 3824
    },
    {
        "loss": 1.6768,
        "grad_norm": 4.541736602783203,
        "learning_rate": 2.691671486044719e-05,
        "epoch": 0.51,
        "step": 3825
    },
    {
        "loss": 2.8222,
        "grad_norm": 2.8940556049346924,
        "learning_rate": 2.6873771452846474e-05,
        "epoch": 0.5101333333333333,
        "step": 3826
    },
    {
        "loss": 2.5009,
        "grad_norm": 3.310746908187866,
        "learning_rate": 2.683085701079412e-05,
        "epoch": 0.5102666666666666,
        "step": 3827
    },
    {
        "loss": 2.3895,
        "grad_norm": 3.331913948059082,
        "learning_rate": 2.6787971551288594e-05,
        "epoch": 0.5104,
        "step": 3828
    },
    {
        "loss": 2.6749,
        "grad_norm": 3.7499287128448486,
        "learning_rate": 2.674511509131703e-05,
        "epoch": 0.5105333333333333,
        "step": 3829
    },
    {
        "loss": 0.8492,
        "grad_norm": 3.280158281326294,
        "learning_rate": 2.670228764785493e-05,
        "epoch": 0.5106666666666667,
        "step": 3830
    },
    {
        "loss": 2.5146,
        "grad_norm": 2.5431113243103027,
        "learning_rate": 2.665948923786653e-05,
        "epoch": 0.5108,
        "step": 3831
    },
    {
        "loss": 2.5214,
        "grad_norm": 3.2129061222076416,
        "learning_rate": 2.6616719878304297e-05,
        "epoch": 0.5109333333333334,
        "step": 3832
    },
    {
        "loss": 1.9777,
        "grad_norm": 4.2509002685546875,
        "learning_rate": 2.6573979586109387e-05,
        "epoch": 0.5110666666666667,
        "step": 3833
    },
    {
        "loss": 2.2227,
        "grad_norm": 3.24198579788208,
        "learning_rate": 2.6531268378211328e-05,
        "epoch": 0.5112,
        "step": 3834
    },
    {
        "loss": 1.7899,
        "grad_norm": 5.196167469024658,
        "learning_rate": 2.6488586271528203e-05,
        "epoch": 0.5113333333333333,
        "step": 3835
    },
    {
        "loss": 2.7766,
        "grad_norm": 3.041414737701416,
        "learning_rate": 2.644593328296655e-05,
        "epoch": 0.5114666666666666,
        "step": 3836
    },
    {
        "loss": 1.2857,
        "grad_norm": 3.933629035949707,
        "learning_rate": 2.640330942942134e-05,
        "epoch": 0.5116,
        "step": 3837
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.6568442583084106,
        "learning_rate": 2.636071472777607e-05,
        "epoch": 0.5117333333333334,
        "step": 3838
    },
    {
        "loss": 2.7543,
        "grad_norm": 3.0041189193725586,
        "learning_rate": 2.63181491949026e-05,
        "epoch": 0.5118666666666667,
        "step": 3839
    },
    {
        "loss": 1.9199,
        "grad_norm": 3.531923532485962,
        "learning_rate": 2.6275612847661313e-05,
        "epoch": 0.512,
        "step": 3840
    },
    {
        "loss": 2.4329,
        "grad_norm": 4.128945350646973,
        "learning_rate": 2.6233105702900962e-05,
        "epoch": 0.5121333333333333,
        "step": 3841
    },
    {
        "loss": 2.3992,
        "grad_norm": 2.9325132369995117,
        "learning_rate": 2.619062777745883e-05,
        "epoch": 0.5122666666666666,
        "step": 3842
    },
    {
        "loss": 2.4321,
        "grad_norm": 2.4156134128570557,
        "learning_rate": 2.6148179088160498e-05,
        "epoch": 0.5124,
        "step": 3843
    },
    {
        "loss": 2.8784,
        "grad_norm": 2.6490671634674072,
        "learning_rate": 2.6105759651820104e-05,
        "epoch": 0.5125333333333333,
        "step": 3844
    },
    {
        "loss": 1.4245,
        "grad_norm": 3.252293348312378,
        "learning_rate": 2.6063369485240073e-05,
        "epoch": 0.5126666666666667,
        "step": 3845
    },
    {
        "loss": 1.4232,
        "grad_norm": 3.463430404663086,
        "learning_rate": 2.6021008605211328e-05,
        "epoch": 0.5128,
        "step": 3846
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.41058349609375,
        "learning_rate": 2.5978677028513144e-05,
        "epoch": 0.5129333333333334,
        "step": 3847
    },
    {
        "loss": 2.3349,
        "grad_norm": 4.52406644821167,
        "learning_rate": 2.593637477191324e-05,
        "epoch": 0.5130666666666667,
        "step": 3848
    },
    {
        "loss": 2.4959,
        "grad_norm": 2.06469464302063,
        "learning_rate": 2.589410185216763e-05,
        "epoch": 0.5132,
        "step": 3849
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.188304901123047,
        "learning_rate": 2.5851858286020835e-05,
        "epoch": 0.5133333333333333,
        "step": 3850
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.915069341659546,
        "learning_rate": 2.5809644090205575e-05,
        "epoch": 0.5134666666666666,
        "step": 3851
    },
    {
        "loss": 0.8465,
        "grad_norm": 3.889852523803711,
        "learning_rate": 2.5767459281443087e-05,
        "epoch": 0.5136,
        "step": 3852
    },
    {
        "loss": 2.347,
        "grad_norm": 2.36427903175354,
        "learning_rate": 2.5725303876442918e-05,
        "epoch": 0.5137333333333334,
        "step": 3853
    },
    {
        "loss": 2.8,
        "grad_norm": 3.4592349529266357,
        "learning_rate": 2.5683177891903e-05,
        "epoch": 0.5138666666666667,
        "step": 3854
    },
    {
        "loss": 2.1778,
        "grad_norm": 2.929473400115967,
        "learning_rate": 2.5641081344509488e-05,
        "epoch": 0.514,
        "step": 3855
    },
    {
        "loss": 1.9298,
        "grad_norm": 3.229334831237793,
        "learning_rate": 2.5599014250937114e-05,
        "epoch": 0.5141333333333333,
        "step": 3856
    },
    {
        "loss": 2.826,
        "grad_norm": 2.099790334701538,
        "learning_rate": 2.555697662784866e-05,
        "epoch": 0.5142666666666666,
        "step": 3857
    },
    {
        "loss": 2.8036,
        "grad_norm": 1.9942229986190796,
        "learning_rate": 2.551496849189541e-05,
        "epoch": 0.5144,
        "step": 3858
    },
    {
        "loss": 2.2445,
        "grad_norm": 2.2375741004943848,
        "learning_rate": 2.547298985971698e-05,
        "epoch": 0.5145333333333333,
        "step": 3859
    },
    {
        "loss": 2.6632,
        "grad_norm": 2.889678716659546,
        "learning_rate": 2.5431040747941172e-05,
        "epoch": 0.5146666666666667,
        "step": 3860
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.62762713432312,
        "learning_rate": 2.5389121173184215e-05,
        "epoch": 0.5148,
        "step": 3861
    },
    {
        "loss": 3.0544,
        "grad_norm": 3.7936644554138184,
        "learning_rate": 2.5347231152050598e-05,
        "epoch": 0.5149333333333334,
        "step": 3862
    },
    {
        "loss": 2.2304,
        "grad_norm": 2.5173354148864746,
        "learning_rate": 2.5305370701133123e-05,
        "epoch": 0.5150666666666667,
        "step": 3863
    },
    {
        "loss": 2.5182,
        "grad_norm": 2.976871967315674,
        "learning_rate": 2.5263539837012794e-05,
        "epoch": 0.5152,
        "step": 3864
    },
    {
        "loss": 1.5271,
        "grad_norm": 3.875749349594116,
        "learning_rate": 2.5221738576259025e-05,
        "epoch": 0.5153333333333333,
        "step": 3865
    },
    {
        "loss": 2.3421,
        "grad_norm": 2.756739854812622,
        "learning_rate": 2.5179966935429377e-05,
        "epoch": 0.5154666666666666,
        "step": 3866
    },
    {
        "loss": 2.6778,
        "grad_norm": 3.017348051071167,
        "learning_rate": 2.5138224931069798e-05,
        "epoch": 0.5156,
        "step": 3867
    },
    {
        "loss": 1.9004,
        "grad_norm": 3.4364702701568604,
        "learning_rate": 2.5096512579714383e-05,
        "epoch": 0.5157333333333334,
        "step": 3868
    },
    {
        "loss": 1.675,
        "grad_norm": 3.535797595977783,
        "learning_rate": 2.505482989788559e-05,
        "epoch": 0.5158666666666667,
        "step": 3869
    },
    {
        "loss": 2.1471,
        "grad_norm": 2.588955879211426,
        "learning_rate": 2.5013176902094016e-05,
        "epoch": 0.516,
        "step": 3870
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.7558295726776123,
        "learning_rate": 2.4971553608838583e-05,
        "epoch": 0.5161333333333333,
        "step": 3871
    },
    {
        "loss": 2.7617,
        "grad_norm": 1.9140735864639282,
        "learning_rate": 2.4929960034606413e-05,
        "epoch": 0.5162666666666667,
        "step": 3872
    },
    {
        "loss": 2.0347,
        "grad_norm": 2.294628381729126,
        "learning_rate": 2.48883961958729e-05,
        "epoch": 0.5164,
        "step": 3873
    },
    {
        "loss": 2.6167,
        "grad_norm": 3.17948317527771,
        "learning_rate": 2.4846862109101564e-05,
        "epoch": 0.5165333333333333,
        "step": 3874
    },
    {
        "loss": 2.4819,
        "grad_norm": 2.827791929244995,
        "learning_rate": 2.4805357790744278e-05,
        "epoch": 0.5166666666666667,
        "step": 3875
    },
    {
        "loss": 3.189,
        "grad_norm": 4.298208236694336,
        "learning_rate": 2.476388325724094e-05,
        "epoch": 0.5168,
        "step": 3876
    },
    {
        "loss": 1.8901,
        "grad_norm": 3.8152191638946533,
        "learning_rate": 2.4722438525019765e-05,
        "epoch": 0.5169333333333334,
        "step": 3877
    },
    {
        "loss": 2.5071,
        "grad_norm": 3.6056127548217773,
        "learning_rate": 2.4681023610497254e-05,
        "epoch": 0.5170666666666667,
        "step": 3878
    },
    {
        "loss": 2.7604,
        "grad_norm": 3.03023624420166,
        "learning_rate": 2.4639638530077912e-05,
        "epoch": 0.5172,
        "step": 3879
    },
    {
        "loss": 0.5517,
        "grad_norm": 2.708411693572998,
        "learning_rate": 2.4598283300154522e-05,
        "epoch": 0.5173333333333333,
        "step": 3880
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.8659517765045166,
        "learning_rate": 2.4556957937108048e-05,
        "epoch": 0.5174666666666666,
        "step": 3881
    },
    {
        "loss": 1.7808,
        "grad_norm": 4.11863374710083,
        "learning_rate": 2.4515662457307663e-05,
        "epoch": 0.5176,
        "step": 3882
    },
    {
        "loss": 2.3652,
        "grad_norm": 1.6145601272583008,
        "learning_rate": 2.4474396877110518e-05,
        "epoch": 0.5177333333333334,
        "step": 3883
    },
    {
        "loss": 2.1696,
        "grad_norm": 2.2228329181671143,
        "learning_rate": 2.4433161212862153e-05,
        "epoch": 0.5178666666666667,
        "step": 3884
    },
    {
        "loss": 2.1481,
        "grad_norm": 2.204336166381836,
        "learning_rate": 2.4391955480896088e-05,
        "epoch": 0.518,
        "step": 3885
    },
    {
        "loss": 2.6462,
        "grad_norm": 2.0653469562530518,
        "learning_rate": 2.435077969753409e-05,
        "epoch": 0.5181333333333333,
        "step": 3886
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.800506830215454,
        "learning_rate": 2.4309633879086014e-05,
        "epoch": 0.5182666666666667,
        "step": 3887
    },
    {
        "loss": 2.5958,
        "grad_norm": 2.3014707565307617,
        "learning_rate": 2.4268518041849898e-05,
        "epoch": 0.5184,
        "step": 3888
    },
    {
        "loss": 2.1193,
        "grad_norm": 4.715787887573242,
        "learning_rate": 2.42274322021118e-05,
        "epoch": 0.5185333333333333,
        "step": 3889
    },
    {
        "loss": 2.7168,
        "grad_norm": 3.8729591369628906,
        "learning_rate": 2.4186376376146034e-05,
        "epoch": 0.5186666666666667,
        "step": 3890
    },
    {
        "loss": 2.3102,
        "grad_norm": 2.664598226547241,
        "learning_rate": 2.4145350580214886e-05,
        "epoch": 0.5188,
        "step": 3891
    },
    {
        "loss": 1.759,
        "grad_norm": 3.402513027191162,
        "learning_rate": 2.4104354830568866e-05,
        "epoch": 0.5189333333333334,
        "step": 3892
    },
    {
        "loss": 2.7279,
        "grad_norm": 2.4695968627929688,
        "learning_rate": 2.406338914344648e-05,
        "epoch": 0.5190666666666667,
        "step": 3893
    },
    {
        "loss": 2.7909,
        "grad_norm": 1.617644190788269,
        "learning_rate": 2.402245353507442e-05,
        "epoch": 0.5192,
        "step": 3894
    },
    {
        "loss": 3.092,
        "grad_norm": 5.356477737426758,
        "learning_rate": 2.398154802166739e-05,
        "epoch": 0.5193333333333333,
        "step": 3895
    },
    {
        "loss": 1.8753,
        "grad_norm": 2.765845537185669,
        "learning_rate": 2.3940672619428194e-05,
        "epoch": 0.5194666666666666,
        "step": 3896
    },
    {
        "loss": 2.4858,
        "grad_norm": 2.3728129863739014,
        "learning_rate": 2.3899827344547753e-05,
        "epoch": 0.5196,
        "step": 3897
    },
    {
        "loss": 2.3378,
        "grad_norm": 2.714970588684082,
        "learning_rate": 2.3859012213205012e-05,
        "epoch": 0.5197333333333334,
        "step": 3898
    },
    {
        "loss": 2.1028,
        "grad_norm": 2.6943986415863037,
        "learning_rate": 2.3818227241566992e-05,
        "epoch": 0.5198666666666667,
        "step": 3899
    },
    {
        "loss": 1.6148,
        "grad_norm": 3.3021857738494873,
        "learning_rate": 2.3777472445788728e-05,
        "epoch": 0.52,
        "step": 3900
    },
    {
        "loss": 1.1043,
        "grad_norm": 2.0828332901000977,
        "learning_rate": 2.3736747842013397e-05,
        "epoch": 0.5201333333333333,
        "step": 3901
    },
    {
        "loss": 2.6872,
        "grad_norm": 2.85052227973938,
        "learning_rate": 2.3696053446372024e-05,
        "epoch": 0.5202666666666667,
        "step": 3902
    },
    {
        "loss": 2.2282,
        "grad_norm": 2.823115348815918,
        "learning_rate": 2.365538927498394e-05,
        "epoch": 0.5204,
        "step": 3903
    },
    {
        "loss": 1.87,
        "grad_norm": 3.192692995071411,
        "learning_rate": 2.3614755343956275e-05,
        "epoch": 0.5205333333333333,
        "step": 3904
    },
    {
        "loss": 3.0535,
        "grad_norm": 3.380053997039795,
        "learning_rate": 2.3574151669384315e-05,
        "epoch": 0.5206666666666667,
        "step": 3905
    },
    {
        "loss": 2.7009,
        "grad_norm": 2.1823172569274902,
        "learning_rate": 2.3533578267351243e-05,
        "epoch": 0.5208,
        "step": 3906
    },
    {
        "loss": 2.4097,
        "grad_norm": 2.5395314693450928,
        "learning_rate": 2.3493035153928454e-05,
        "epoch": 0.5209333333333334,
        "step": 3907
    },
    {
        "loss": 1.742,
        "grad_norm": 2.5048444271087646,
        "learning_rate": 2.3452522345175087e-05,
        "epoch": 0.5210666666666667,
        "step": 3908
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.0751802921295166,
        "learning_rate": 2.341203985713847e-05,
        "epoch": 0.5212,
        "step": 3909
    },
    {
        "loss": 2.8427,
        "grad_norm": 2.9892709255218506,
        "learning_rate": 2.337158770585379e-05,
        "epoch": 0.5213333333333333,
        "step": 3910
    },
    {
        "loss": 1.9338,
        "grad_norm": 1.6795283555984497,
        "learning_rate": 2.3331165907344345e-05,
        "epoch": 0.5214666666666666,
        "step": 3911
    },
    {
        "loss": 2.1915,
        "grad_norm": 2.9346659183502197,
        "learning_rate": 2.3290774477621313e-05,
        "epoch": 0.5216,
        "step": 3912
    },
    {
        "loss": 2.6237,
        "grad_norm": 2.6042280197143555,
        "learning_rate": 2.3250413432683927e-05,
        "epoch": 0.5217333333333334,
        "step": 3913
    },
    {
        "loss": 1.2575,
        "grad_norm": 3.075183391571045,
        "learning_rate": 2.321008278851926e-05,
        "epoch": 0.5218666666666667,
        "step": 3914
    },
    {
        "loss": 1.4111,
        "grad_norm": 3.106717348098755,
        "learning_rate": 2.3169782561102492e-05,
        "epoch": 0.522,
        "step": 3915
    },
    {
        "loss": 1.0041,
        "grad_norm": 3.1695706844329834,
        "learning_rate": 2.312951276639661e-05,
        "epoch": 0.5221333333333333,
        "step": 3916
    },
    {
        "loss": 2.4423,
        "grad_norm": 2.9082767963409424,
        "learning_rate": 2.3089273420352665e-05,
        "epoch": 0.5222666666666667,
        "step": 3917
    },
    {
        "loss": 2.6125,
        "grad_norm": 2.5781426429748535,
        "learning_rate": 2.3049064538909592e-05,
        "epoch": 0.5224,
        "step": 3918
    },
    {
        "loss": 2.0904,
        "grad_norm": 2.722163200378418,
        "learning_rate": 2.3008886137994245e-05,
        "epoch": 0.5225333333333333,
        "step": 3919
    },
    {
        "loss": 2.2822,
        "grad_norm": 2.8325743675231934,
        "learning_rate": 2.296873823352147e-05,
        "epoch": 0.5226666666666666,
        "step": 3920
    },
    {
        "loss": 2.9886,
        "grad_norm": 4.1801557540893555,
        "learning_rate": 2.292862084139392e-05,
        "epoch": 0.5228,
        "step": 3921
    },
    {
        "loss": 2.5967,
        "grad_norm": 3.253241539001465,
        "learning_rate": 2.288853397750228e-05,
        "epoch": 0.5229333333333334,
        "step": 3922
    },
    {
        "loss": 2.7079,
        "grad_norm": 2.8895857334136963,
        "learning_rate": 2.2848477657725075e-05,
        "epoch": 0.5230666666666667,
        "step": 3923
    },
    {
        "loss": 2.5024,
        "grad_norm": 4.187711238861084,
        "learning_rate": 2.2808451897928785e-05,
        "epoch": 0.5232,
        "step": 3924
    },
    {
        "loss": 2.6328,
        "grad_norm": 2.7388803958892822,
        "learning_rate": 2.27684567139677e-05,
        "epoch": 0.5233333333333333,
        "step": 3925
    },
    {
        "loss": 2.0948,
        "grad_norm": 3.4311556816101074,
        "learning_rate": 2.272849212168412e-05,
        "epoch": 0.5234666666666666,
        "step": 3926
    },
    {
        "loss": 1.6297,
        "grad_norm": 3.235002040863037,
        "learning_rate": 2.2688558136908022e-05,
        "epoch": 0.5236,
        "step": 3927
    },
    {
        "loss": 2.5179,
        "grad_norm": 3.9754955768585205,
        "learning_rate": 2.2648654775457535e-05,
        "epoch": 0.5237333333333334,
        "step": 3928
    },
    {
        "loss": 1.1942,
        "grad_norm": 3.206385850906372,
        "learning_rate": 2.2608782053138433e-05,
        "epoch": 0.5238666666666667,
        "step": 3929
    },
    {
        "loss": 1.77,
        "grad_norm": 3.7294845581054688,
        "learning_rate": 2.2568939985744496e-05,
        "epoch": 0.524,
        "step": 3930
    },
    {
        "loss": 2.3903,
        "grad_norm": 2.6663661003112793,
        "learning_rate": 2.2529128589057212e-05,
        "epoch": 0.5241333333333333,
        "step": 3931
    },
    {
        "loss": 3.0007,
        "grad_norm": 1.9049046039581299,
        "learning_rate": 2.2489347878846156e-05,
        "epoch": 0.5242666666666667,
        "step": 3932
    },
    {
        "loss": 2.7838,
        "grad_norm": 2.9830760955810547,
        "learning_rate": 2.2449597870868456e-05,
        "epoch": 0.5244,
        "step": 3933
    },
    {
        "loss": 2.2026,
        "grad_norm": 3.128549337387085,
        "learning_rate": 2.2409878580869313e-05,
        "epoch": 0.5245333333333333,
        "step": 3934
    },
    {
        "loss": 2.3026,
        "grad_norm": 3.579979658126831,
        "learning_rate": 2.2370190024581615e-05,
        "epoch": 0.5246666666666666,
        "step": 3935
    },
    {
        "loss": 1.1008,
        "grad_norm": 3.1924078464508057,
        "learning_rate": 2.2330532217726173e-05,
        "epoch": 0.5248,
        "step": 3936
    },
    {
        "loss": 2.5356,
        "grad_norm": 3.9801065921783447,
        "learning_rate": 2.2290905176011567e-05,
        "epoch": 0.5249333333333334,
        "step": 3937
    },
    {
        "loss": 2.011,
        "grad_norm": 1.4629497528076172,
        "learning_rate": 2.2251308915134217e-05,
        "epoch": 0.5250666666666667,
        "step": 3938
    },
    {
        "loss": 1.8754,
        "grad_norm": 4.5391154289245605,
        "learning_rate": 2.2211743450778345e-05,
        "epoch": 0.5252,
        "step": 3939
    },
    {
        "loss": 2.5126,
        "grad_norm": 2.0065670013427734,
        "learning_rate": 2.2172208798615923e-05,
        "epoch": 0.5253333333333333,
        "step": 3940
    },
    {
        "loss": 2.447,
        "grad_norm": 2.101451873779297,
        "learning_rate": 2.2132704974306817e-05,
        "epoch": 0.5254666666666666,
        "step": 3941
    },
    {
        "loss": 1.4496,
        "grad_norm": 3.9842472076416016,
        "learning_rate": 2.2093231993498576e-05,
        "epoch": 0.5256,
        "step": 3942
    },
    {
        "loss": 1.9769,
        "grad_norm": 3.9127845764160156,
        "learning_rate": 2.2053789871826624e-05,
        "epoch": 0.5257333333333334,
        "step": 3943
    },
    {
        "loss": 0.7742,
        "grad_norm": 2.2654409408569336,
        "learning_rate": 2.2014378624914068e-05,
        "epoch": 0.5258666666666667,
        "step": 3944
    },
    {
        "loss": 0.6617,
        "grad_norm": 3.2188706398010254,
        "learning_rate": 2.1974998268371903e-05,
        "epoch": 0.526,
        "step": 3945
    },
    {
        "loss": 2.5744,
        "grad_norm": 4.574316501617432,
        "learning_rate": 2.193564881779876e-05,
        "epoch": 0.5261333333333333,
        "step": 3946
    },
    {
        "loss": 2.5414,
        "grad_norm": 3.259492874145508,
        "learning_rate": 2.189633028878111e-05,
        "epoch": 0.5262666666666667,
        "step": 3947
    },
    {
        "loss": 0.8213,
        "grad_norm": 2.898667097091675,
        "learning_rate": 2.185704269689317e-05,
        "epoch": 0.5264,
        "step": 3948
    },
    {
        "loss": 1.939,
        "grad_norm": 3.502082109451294,
        "learning_rate": 2.181778605769691e-05,
        "epoch": 0.5265333333333333,
        "step": 3949
    },
    {
        "loss": 2.2105,
        "grad_norm": 2.3207788467407227,
        "learning_rate": 2.1778560386741965e-05,
        "epoch": 0.5266666666666666,
        "step": 3950
    },
    {
        "loss": 1.7512,
        "grad_norm": 4.870973587036133,
        "learning_rate": 2.1739365699565837e-05,
        "epoch": 0.5268,
        "step": 3951
    },
    {
        "loss": 2.8098,
        "grad_norm": 2.48166561126709,
        "learning_rate": 2.1700202011693573e-05,
        "epoch": 0.5269333333333334,
        "step": 3952
    },
    {
        "loss": 2.2511,
        "grad_norm": 3.059591293334961,
        "learning_rate": 2.1661069338638084e-05,
        "epoch": 0.5270666666666667,
        "step": 3953
    },
    {
        "loss": 2.8878,
        "grad_norm": 3.041609764099121,
        "learning_rate": 2.162196769589997e-05,
        "epoch": 0.5272,
        "step": 3954
    },
    {
        "loss": 2.6137,
        "grad_norm": 2.8575987815856934,
        "learning_rate": 2.1582897098967538e-05,
        "epoch": 0.5273333333333333,
        "step": 3955
    },
    {
        "loss": 1.9533,
        "grad_norm": 3.2250027656555176,
        "learning_rate": 2.1543857563316715e-05,
        "epoch": 0.5274666666666666,
        "step": 3956
    },
    {
        "loss": 2.7808,
        "grad_norm": 3.481121301651001,
        "learning_rate": 2.1504849104411327e-05,
        "epoch": 0.5276,
        "step": 3957
    },
    {
        "loss": 3.1199,
        "grad_norm": 2.7253401279449463,
        "learning_rate": 2.1465871737702615e-05,
        "epoch": 0.5277333333333334,
        "step": 3958
    },
    {
        "loss": 2.233,
        "grad_norm": 2.8022100925445557,
        "learning_rate": 2.142692547862971e-05,
        "epoch": 0.5278666666666667,
        "step": 3959
    },
    {
        "loss": 2.5146,
        "grad_norm": 3.4515724182128906,
        "learning_rate": 2.138801034261938e-05,
        "epoch": 0.528,
        "step": 3960
    },
    {
        "loss": 2.5405,
        "grad_norm": 3.5660698413848877,
        "learning_rate": 2.134912634508599e-05,
        "epoch": 0.5281333333333333,
        "step": 3961
    },
    {
        "loss": 2.7533,
        "grad_norm": 2.27032470703125,
        "learning_rate": 2.1310273501431654e-05,
        "epoch": 0.5282666666666667,
        "step": 3962
    },
    {
        "loss": 1.8949,
        "grad_norm": 3.452052116394043,
        "learning_rate": 2.1271451827046108e-05,
        "epoch": 0.5284,
        "step": 3963
    },
    {
        "loss": 2.4458,
        "grad_norm": 4.436798095703125,
        "learning_rate": 2.123266133730678e-05,
        "epoch": 0.5285333333333333,
        "step": 3964
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.723285436630249,
        "learning_rate": 2.1193902047578673e-05,
        "epoch": 0.5286666666666666,
        "step": 3965
    },
    {
        "loss": 2.3652,
        "grad_norm": 3.2781271934509277,
        "learning_rate": 2.1155173973214503e-05,
        "epoch": 0.5288,
        "step": 3966
    },
    {
        "loss": 3.1636,
        "grad_norm": 2.4319472312927246,
        "learning_rate": 2.1116477129554557e-05,
        "epoch": 0.5289333333333334,
        "step": 3967
    },
    {
        "loss": 2.7995,
        "grad_norm": 1.7460262775421143,
        "learning_rate": 2.1077811531926838e-05,
        "epoch": 0.5290666666666667,
        "step": 3968
    },
    {
        "loss": 2.7831,
        "grad_norm": 2.383110284805298,
        "learning_rate": 2.1039177195646874e-05,
        "epoch": 0.5292,
        "step": 3969
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.201054096221924,
        "learning_rate": 2.100057413601789e-05,
        "epoch": 0.5293333333333333,
        "step": 3970
    },
    {
        "loss": 2.134,
        "grad_norm": 4.014152526855469,
        "learning_rate": 2.0962002368330657e-05,
        "epoch": 0.5294666666666666,
        "step": 3971
    },
    {
        "loss": 2.0259,
        "grad_norm": 4.310042858123779,
        "learning_rate": 2.0923461907863595e-05,
        "epoch": 0.5296,
        "step": 3972
    },
    {
        "loss": 2.2352,
        "grad_norm": 1.7881752252578735,
        "learning_rate": 2.0884952769882716e-05,
        "epoch": 0.5297333333333333,
        "step": 3973
    },
    {
        "loss": 3.1541,
        "grad_norm": 3.280827045440674,
        "learning_rate": 2.084647496964165e-05,
        "epoch": 0.5298666666666667,
        "step": 3974
    },
    {
        "loss": 2.9759,
        "grad_norm": 1.5546151399612427,
        "learning_rate": 2.0808028522381527e-05,
        "epoch": 0.53,
        "step": 3975
    },
    {
        "loss": 0.7702,
        "grad_norm": 3.1862335205078125,
        "learning_rate": 2.0769613443331214e-05,
        "epoch": 0.5301333333333333,
        "step": 3976
    },
    {
        "loss": 1.405,
        "grad_norm": 4.193836688995361,
        "learning_rate": 2.0731229747706925e-05,
        "epoch": 0.5302666666666667,
        "step": 3977
    },
    {
        "loss": 1.3449,
        "grad_norm": 3.404215097427368,
        "learning_rate": 2.069287745071261e-05,
        "epoch": 0.5304,
        "step": 3978
    },
    {
        "loss": 1.0082,
        "grad_norm": 3.1658170223236084,
        "learning_rate": 2.0654556567539817e-05,
        "epoch": 0.5305333333333333,
        "step": 3979
    },
    {
        "loss": 2.599,
        "grad_norm": 2.311007261276245,
        "learning_rate": 2.0616267113367504e-05,
        "epoch": 0.5306666666666666,
        "step": 3980
    },
    {
        "loss": 2.0868,
        "grad_norm": 2.554765224456787,
        "learning_rate": 2.0578009103362295e-05,
        "epoch": 0.5308,
        "step": 3981
    },
    {
        "loss": 0.9151,
        "grad_norm": 4.653837203979492,
        "learning_rate": 2.053978255267829e-05,
        "epoch": 0.5309333333333334,
        "step": 3982
    },
    {
        "loss": 1.8781,
        "grad_norm": 3.6200780868530273,
        "learning_rate": 2.050158747645723e-05,
        "epoch": 0.5310666666666667,
        "step": 3983
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.8456811904907227,
        "learning_rate": 2.0463423889828193e-05,
        "epoch": 0.5312,
        "step": 3984
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.5139167308807373,
        "learning_rate": 2.0425291807908e-05,
        "epoch": 0.5313333333333333,
        "step": 3985
    },
    {
        "loss": 2.7738,
        "grad_norm": 1.5292973518371582,
        "learning_rate": 2.0387191245800842e-05,
        "epoch": 0.5314666666666666,
        "step": 3986
    },
    {
        "loss": 0.5563,
        "grad_norm": 2.653592109680176,
        "learning_rate": 2.0349122218598494e-05,
        "epoch": 0.5316,
        "step": 3987
    },
    {
        "loss": 2.6558,
        "grad_norm": 2.1077001094818115,
        "learning_rate": 2.031108474138025e-05,
        "epoch": 0.5317333333333333,
        "step": 3988
    },
    {
        "loss": 2.4008,
        "grad_norm": 2.5988457202911377,
        "learning_rate": 2.02730788292129e-05,
        "epoch": 0.5318666666666667,
        "step": 3989
    },
    {
        "loss": 2.3381,
        "grad_norm": 3.547924518585205,
        "learning_rate": 2.023510449715066e-05,
        "epoch": 0.532,
        "step": 3990
    },
    {
        "loss": 2.4214,
        "grad_norm": 3.9938127994537354,
        "learning_rate": 2.019716176023534e-05,
        "epoch": 0.5321333333333333,
        "step": 3991
    },
    {
        "loss": 1.9045,
        "grad_norm": 2.173872232437134,
        "learning_rate": 2.0159250633496153e-05,
        "epoch": 0.5322666666666667,
        "step": 3992
    },
    {
        "loss": 2.1736,
        "grad_norm": 3.63287091255188,
        "learning_rate": 2.012137113194986e-05,
        "epoch": 0.5324,
        "step": 3993
    },
    {
        "loss": 2.6771,
        "grad_norm": 3.156967878341675,
        "learning_rate": 2.0083523270600624e-05,
        "epoch": 0.5325333333333333,
        "step": 3994
    },
    {
        "loss": 2.4888,
        "grad_norm": 2.856828451156616,
        "learning_rate": 2.0045707064440145e-05,
        "epoch": 0.5326666666666666,
        "step": 3995
    },
    {
        "loss": 2.3774,
        "grad_norm": 3.4584851264953613,
        "learning_rate": 2.000792252844751e-05,
        "epoch": 0.5328,
        "step": 3996
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.0020289421081543,
        "learning_rate": 1.9970169677589335e-05,
        "epoch": 0.5329333333333334,
        "step": 3997
    },
    {
        "loss": 2.6951,
        "grad_norm": 1.9385128021240234,
        "learning_rate": 1.9932448526819637e-05,
        "epoch": 0.5330666666666667,
        "step": 3998
    },
    {
        "loss": 2.6525,
        "grad_norm": 3.795499086380005,
        "learning_rate": 1.9894759091079907e-05,
        "epoch": 0.5332,
        "step": 3999
    },
    {
        "loss": 2.5539,
        "grad_norm": 3.5776846408843994,
        "learning_rate": 1.985710138529908e-05,
        "epoch": 0.5333333333333333,
        "step": 4000
    },
    {
        "loss": 2.6461,
        "grad_norm": 4.354888439178467,
        "learning_rate": 1.981947542439344e-05,
        "epoch": 0.5334666666666666,
        "step": 4001
    },
    {
        "loss": 2.9108,
        "grad_norm": 2.59511399269104,
        "learning_rate": 1.9781881223266852e-05,
        "epoch": 0.5336,
        "step": 4002
    },
    {
        "loss": 1.1709,
        "grad_norm": 3.0992681980133057,
        "learning_rate": 1.9744318796810367e-05,
        "epoch": 0.5337333333333333,
        "step": 4003
    },
    {
        "loss": 2.4567,
        "grad_norm": 2.803331136703491,
        "learning_rate": 1.9706788159902734e-05,
        "epoch": 0.5338666666666667,
        "step": 4004
    },
    {
        "loss": 2.0858,
        "grad_norm": 4.001009941101074,
        "learning_rate": 1.9669289327409867e-05,
        "epoch": 0.534,
        "step": 4005
    },
    {
        "loss": 2.4099,
        "grad_norm": 1.7127841711044312,
        "learning_rate": 1.9631822314185243e-05,
        "epoch": 0.5341333333333333,
        "step": 4006
    },
    {
        "loss": 2.2789,
        "grad_norm": 2.504589557647705,
        "learning_rate": 1.9594387135069603e-05,
        "epoch": 0.5342666666666667,
        "step": 4007
    },
    {
        "loss": 2.054,
        "grad_norm": 4.820371627807617,
        "learning_rate": 1.955698380489126e-05,
        "epoch": 0.5344,
        "step": 4008
    },
    {
        "loss": 2.4225,
        "grad_norm": 3.835200071334839,
        "learning_rate": 1.9519612338465697e-05,
        "epoch": 0.5345333333333333,
        "step": 4009
    },
    {
        "loss": 1.7345,
        "grad_norm": 2.7408876419067383,
        "learning_rate": 1.948227275059593e-05,
        "epoch": 0.5346666666666666,
        "step": 4010
    },
    {
        "loss": 2.6279,
        "grad_norm": 2.5096499919891357,
        "learning_rate": 1.9444965056072262e-05,
        "epoch": 0.5348,
        "step": 4011
    },
    {
        "loss": 2.5796,
        "grad_norm": 2.2458455562591553,
        "learning_rate": 1.9407689269672424e-05,
        "epoch": 0.5349333333333334,
        "step": 4012
    },
    {
        "loss": 2.0979,
        "grad_norm": 4.167016983032227,
        "learning_rate": 1.9370445406161476e-05,
        "epoch": 0.5350666666666667,
        "step": 4013
    },
    {
        "loss": 1.8064,
        "grad_norm": 2.329836130142212,
        "learning_rate": 1.933323348029187e-05,
        "epoch": 0.5352,
        "step": 4014
    },
    {
        "loss": 3.456,
        "grad_norm": 4.876291751861572,
        "learning_rate": 1.9296053506803314e-05,
        "epoch": 0.5353333333333333,
        "step": 4015
    },
    {
        "loss": 2.3507,
        "grad_norm": 3.7837374210357666,
        "learning_rate": 1.9258905500422986e-05,
        "epoch": 0.5354666666666666,
        "step": 4016
    },
    {
        "loss": 1.2682,
        "grad_norm": 3.8569109439849854,
        "learning_rate": 1.922178947586528e-05,
        "epoch": 0.5356,
        "step": 4017
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.764254570007324,
        "learning_rate": 1.9184705447832018e-05,
        "epoch": 0.5357333333333333,
        "step": 4018
    },
    {
        "loss": 2.2684,
        "grad_norm": 3.7530648708343506,
        "learning_rate": 1.9147653431012313e-05,
        "epoch": 0.5358666666666667,
        "step": 4019
    },
    {
        "loss": 2.387,
        "grad_norm": 2.323056697845459,
        "learning_rate": 1.911063344008256e-05,
        "epoch": 0.536,
        "step": 4020
    },
    {
        "loss": 3.0219,
        "grad_norm": 3.3779056072235107,
        "learning_rate": 1.9073645489706548e-05,
        "epoch": 0.5361333333333334,
        "step": 4021
    },
    {
        "loss": 2.1399,
        "grad_norm": 3.2111642360687256,
        "learning_rate": 1.9036689594535285e-05,
        "epoch": 0.5362666666666667,
        "step": 4022
    },
    {
        "loss": 1.9581,
        "grad_norm": 2.8855361938476562,
        "learning_rate": 1.8999765769207155e-05,
        "epoch": 0.5364,
        "step": 4023
    },
    {
        "loss": 1.937,
        "grad_norm": 2.8578274250030518,
        "learning_rate": 1.89628740283478e-05,
        "epoch": 0.5365333333333333,
        "step": 4024
    },
    {
        "loss": 2.1509,
        "grad_norm": 2.767747402191162,
        "learning_rate": 1.892601438657019e-05,
        "epoch": 0.5366666666666666,
        "step": 4025
    },
    {
        "loss": 2.7494,
        "grad_norm": 1.62385094165802,
        "learning_rate": 1.8889186858474516e-05,
        "epoch": 0.5368,
        "step": 4026
    },
    {
        "loss": 2.6414,
        "grad_norm": 2.1027629375457764,
        "learning_rate": 1.8852391458648356e-05,
        "epoch": 0.5369333333333334,
        "step": 4027
    },
    {
        "loss": 2.4983,
        "grad_norm": 3.2456259727478027,
        "learning_rate": 1.8815628201666357e-05,
        "epoch": 0.5370666666666667,
        "step": 4028
    },
    {
        "loss": 2.3455,
        "grad_norm": 3.312544107437134,
        "learning_rate": 1.877889710209072e-05,
        "epoch": 0.5372,
        "step": 4029
    },
    {
        "loss": 2.0685,
        "grad_norm": 2.9561357498168945,
        "learning_rate": 1.874219817447066e-05,
        "epoch": 0.5373333333333333,
        "step": 4030
    },
    {
        "loss": 2.6491,
        "grad_norm": 2.8946666717529297,
        "learning_rate": 1.8705531433342815e-05,
        "epoch": 0.5374666666666666,
        "step": 4031
    },
    {
        "loss": 2.6962,
        "grad_norm": 2.971925973892212,
        "learning_rate": 1.8668896893230913e-05,
        "epoch": 0.5376,
        "step": 4032
    },
    {
        "loss": 1.0626,
        "grad_norm": 2.9896209239959717,
        "learning_rate": 1.863229456864617e-05,
        "epoch": 0.5377333333333333,
        "step": 4033
    },
    {
        "loss": 2.6038,
        "grad_norm": 3.6460835933685303,
        "learning_rate": 1.859572447408674e-05,
        "epoch": 0.5378666666666667,
        "step": 4034
    },
    {
        "loss": 2.4479,
        "grad_norm": 2.029158115386963,
        "learning_rate": 1.855918662403826e-05,
        "epoch": 0.538,
        "step": 4035
    },
    {
        "loss": 1.8542,
        "grad_norm": 2.9708073139190674,
        "learning_rate": 1.852268103297342e-05,
        "epoch": 0.5381333333333334,
        "step": 4036
    },
    {
        "loss": 2.312,
        "grad_norm": 3.8912463188171387,
        "learning_rate": 1.8486207715352265e-05,
        "epoch": 0.5382666666666667,
        "step": 4037
    },
    {
        "loss": 1.8373,
        "grad_norm": 5.094330787658691,
        "learning_rate": 1.8449766685621984e-05,
        "epoch": 0.5384,
        "step": 4038
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.7482619285583496,
        "learning_rate": 1.841335795821698e-05,
        "epoch": 0.5385333333333333,
        "step": 4039
    },
    {
        "loss": 1.9622,
        "grad_norm": 3.328585386276245,
        "learning_rate": 1.8376981547558924e-05,
        "epoch": 0.5386666666666666,
        "step": 4040
    },
    {
        "loss": 2.7747,
        "grad_norm": 4.083789348602295,
        "learning_rate": 1.8340637468056576e-05,
        "epoch": 0.5388,
        "step": 4041
    },
    {
        "loss": 2.7398,
        "grad_norm": 2.0580122470855713,
        "learning_rate": 1.8304325734106e-05,
        "epoch": 0.5389333333333334,
        "step": 4042
    },
    {
        "loss": 2.5175,
        "grad_norm": 3.5677411556243896,
        "learning_rate": 1.8268046360090353e-05,
        "epoch": 0.5390666666666667,
        "step": 4043
    },
    {
        "loss": 2.515,
        "grad_norm": 2.1698968410491943,
        "learning_rate": 1.8231799360380052e-05,
        "epoch": 0.5392,
        "step": 4044
    },
    {
        "loss": 3.1022,
        "grad_norm": 2.333820104598999,
        "learning_rate": 1.819558474933262e-05,
        "epoch": 0.5393333333333333,
        "step": 4045
    },
    {
        "loss": 2.2387,
        "grad_norm": 2.4430878162384033,
        "learning_rate": 1.8159402541292837e-05,
        "epoch": 0.5394666666666666,
        "step": 4046
    },
    {
        "loss": 2.1464,
        "grad_norm": 2.8743789196014404,
        "learning_rate": 1.8123252750592546e-05,
        "epoch": 0.5396,
        "step": 4047
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.646958589553833,
        "learning_rate": 1.8087135391550823e-05,
        "epoch": 0.5397333333333333,
        "step": 4048
    },
    {
        "loss": 1.8993,
        "grad_norm": 2.1804583072662354,
        "learning_rate": 1.8051050478473896e-05,
        "epoch": 0.5398666666666667,
        "step": 4049
    },
    {
        "loss": 1.476,
        "grad_norm": 7.111042499542236,
        "learning_rate": 1.8014998025655128e-05,
        "epoch": 0.54,
        "step": 4050
    },
    {
        "loss": 2.1798,
        "grad_norm": 4.003819942474365,
        "learning_rate": 1.797897804737497e-05,
        "epoch": 0.5401333333333334,
        "step": 4051
    },
    {
        "loss": 1.896,
        "grad_norm": 3.688499927520752,
        "learning_rate": 1.794299055790114e-05,
        "epoch": 0.5402666666666667,
        "step": 4052
    },
    {
        "loss": 1.3941,
        "grad_norm": 3.89754056930542,
        "learning_rate": 1.7907035571488307e-05,
        "epoch": 0.5404,
        "step": 4053
    },
    {
        "loss": 2.4085,
        "grad_norm": 3.717116355895996,
        "learning_rate": 1.78711131023784e-05,
        "epoch": 0.5405333333333333,
        "step": 4054
    },
    {
        "loss": 2.6011,
        "grad_norm": 2.1948039531707764,
        "learning_rate": 1.7835223164800442e-05,
        "epoch": 0.5406666666666666,
        "step": 4055
    },
    {
        "loss": 1.3814,
        "grad_norm": 4.131279945373535,
        "learning_rate": 1.7799365772970587e-05,
        "epoch": 0.5408,
        "step": 4056
    },
    {
        "loss": 1.2072,
        "grad_norm": Infinity,
        "learning_rate": 1.7799365772970587e-05,
        "epoch": 0.5409333333333334,
        "step": 4057
    },
    {
        "loss": 1.4786,
        "grad_norm": 2.350491523742676,
        "learning_rate": 1.7763540941091995e-05,
        "epoch": 0.5410666666666667,
        "step": 4058
    },
    {
        "loss": 3.1229,
        "grad_norm": 3.5068907737731934,
        "learning_rate": 1.772774868335506e-05,
        "epoch": 0.5412,
        "step": 4059
    },
    {
        "loss": 3.606,
        "grad_norm": 9.70588493347168,
        "learning_rate": 1.7691989013937272e-05,
        "epoch": 0.5413333333333333,
        "step": 4060
    },
    {
        "loss": 1.9654,
        "grad_norm": 3.147261142730713,
        "learning_rate": 1.7656261947003037e-05,
        "epoch": 0.5414666666666667,
        "step": 4061
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.816795587539673,
        "learning_rate": 1.7620567496704056e-05,
        "epoch": 0.5416,
        "step": 4062
    },
    {
        "loss": 2.115,
        "grad_norm": 3.327849864959717,
        "learning_rate": 1.7584905677178954e-05,
        "epoch": 0.5417333333333333,
        "step": 4063
    },
    {
        "loss": 2.4475,
        "grad_norm": 2.8401436805725098,
        "learning_rate": 1.7549276502553547e-05,
        "epoch": 0.5418666666666667,
        "step": 4064
    },
    {
        "loss": 1.8221,
        "grad_norm": 4.309818267822266,
        "learning_rate": 1.7513679986940656e-05,
        "epoch": 0.542,
        "step": 4065
    },
    {
        "loss": 2.5392,
        "grad_norm": 2.508186101913452,
        "learning_rate": 1.7478116144440214e-05,
        "epoch": 0.5421333333333334,
        "step": 4066
    },
    {
        "loss": 1.9506,
        "grad_norm": 2.9665346145629883,
        "learning_rate": 1.744258498913912e-05,
        "epoch": 0.5422666666666667,
        "step": 4067
    },
    {
        "loss": 2.378,
        "grad_norm": 2.303858518600464,
        "learning_rate": 1.740708653511146e-05,
        "epoch": 0.5424,
        "step": 4068
    },
    {
        "loss": 1.9267,
        "grad_norm": 3.6934595108032227,
        "learning_rate": 1.737162079641822e-05,
        "epoch": 0.5425333333333333,
        "step": 4069
    },
    {
        "loss": 2.7729,
        "grad_norm": 3.2419328689575195,
        "learning_rate": 1.733618778710755e-05,
        "epoch": 0.5426666666666666,
        "step": 4070
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.7802598476409912,
        "learning_rate": 1.7300787521214545e-05,
        "epoch": 0.5428,
        "step": 4071
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.0630300045013428,
        "learning_rate": 1.7265420012761423e-05,
        "epoch": 0.5429333333333334,
        "step": 4072
    },
    {
        "loss": 2.1411,
        "grad_norm": 3.4700417518615723,
        "learning_rate": 1.7230085275757324e-05,
        "epoch": 0.5430666666666667,
        "step": 4073
    },
    {
        "loss": 2.7152,
        "grad_norm": 3.1594202518463135,
        "learning_rate": 1.7194783324198473e-05,
        "epoch": 0.5432,
        "step": 4074
    },
    {
        "loss": 1.4841,
        "grad_norm": 3.949972629547119,
        "learning_rate": 1.71595141720681e-05,
        "epoch": 0.5433333333333333,
        "step": 4075
    },
    {
        "loss": 0.6385,
        "grad_norm": 3.0721662044525146,
        "learning_rate": 1.7124277833336453e-05,
        "epoch": 0.5434666666666667,
        "step": 4076
    },
    {
        "loss": 2.1426,
        "grad_norm": 2.6278610229492188,
        "learning_rate": 1.708907432196074e-05,
        "epoch": 0.5436,
        "step": 4077
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.3946807384490967,
        "learning_rate": 1.7053903651885238e-05,
        "epoch": 0.5437333333333333,
        "step": 4078
    },
    {
        "loss": 2.7552,
        "grad_norm": 3.1877405643463135,
        "learning_rate": 1.7018765837041084e-05,
        "epoch": 0.5438666666666667,
        "step": 4079
    },
    {
        "loss": 1.3581,
        "grad_norm": 4.584149360656738,
        "learning_rate": 1.6983660891346508e-05,
        "epoch": 0.544,
        "step": 4080
    },
    {
        "loss": 2.36,
        "grad_norm": 3.2468318939208984,
        "learning_rate": 1.694858882870678e-05,
        "epoch": 0.5441333333333334,
        "step": 4081
    },
    {
        "loss": 1.9674,
        "grad_norm": 3.2314462661743164,
        "learning_rate": 1.6913549663013973e-05,
        "epoch": 0.5442666666666667,
        "step": 4082
    },
    {
        "loss": 1.8538,
        "grad_norm": 3.1761152744293213,
        "learning_rate": 1.6878543408147263e-05,
        "epoch": 0.5444,
        "step": 4083
    },
    {
        "loss": 1.9522,
        "grad_norm": 3.0829215049743652,
        "learning_rate": 1.6843570077972727e-05,
        "epoch": 0.5445333333333333,
        "step": 4084
    },
    {
        "loss": 2.4602,
        "grad_norm": 2.3927254676818848,
        "learning_rate": 1.6808629686343492e-05,
        "epoch": 0.5446666666666666,
        "step": 4085
    },
    {
        "loss": 2.8928,
        "grad_norm": 3.036226272583008,
        "learning_rate": 1.6773722247099455e-05,
        "epoch": 0.5448,
        "step": 4086
    },
    {
        "loss": 1.6483,
        "grad_norm": 1.6842741966247559,
        "learning_rate": 1.6738847774067644e-05,
        "epoch": 0.5449333333333334,
        "step": 4087
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.435671091079712,
        "learning_rate": 1.670400628106191e-05,
        "epoch": 0.5450666666666667,
        "step": 4088
    },
    {
        "loss": 2.6432,
        "grad_norm": 3.9449803829193115,
        "learning_rate": 1.6669197781883118e-05,
        "epoch": 0.5452,
        "step": 4089
    },
    {
        "loss": 1.9292,
        "grad_norm": 3.592097520828247,
        "learning_rate": 1.663442229031902e-05,
        "epoch": 0.5453333333333333,
        "step": 4090
    },
    {
        "loss": 3.1394,
        "grad_norm": 2.8014461994171143,
        "learning_rate": 1.6599679820144343e-05,
        "epoch": 0.5454666666666667,
        "step": 4091
    },
    {
        "loss": 2.0303,
        "grad_norm": 4.43798303604126,
        "learning_rate": 1.656497038512065e-05,
        "epoch": 0.5456,
        "step": 4092
    },
    {
        "loss": 2.2369,
        "grad_norm": 2.723564624786377,
        "learning_rate": 1.6530293998996493e-05,
        "epoch": 0.5457333333333333,
        "step": 4093
    },
    {
        "loss": 2.3736,
        "grad_norm": 2.9312868118286133,
        "learning_rate": 1.649565067550729e-05,
        "epoch": 0.5458666666666666,
        "step": 4094
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.2184696197509766,
        "learning_rate": 1.6461040428375407e-05,
        "epoch": 0.546,
        "step": 4095
    },
    {
        "loss": 2.5866,
        "grad_norm": 2.136260509490967,
        "learning_rate": 1.6426463271310045e-05,
        "epoch": 0.5461333333333334,
        "step": 4096
    },
    {
        "loss": 2.685,
        "grad_norm": 2.307898998260498,
        "learning_rate": 1.6391919218007368e-05,
        "epoch": 0.5462666666666667,
        "step": 4097
    },
    {
        "loss": 2.5385,
        "grad_norm": 2.2789597511291504,
        "learning_rate": 1.6357408282150343e-05,
        "epoch": 0.5464,
        "step": 4098
    },
    {
        "loss": 1.456,
        "grad_norm": 3.6324307918548584,
        "learning_rate": 1.6322930477408916e-05,
        "epoch": 0.5465333333333333,
        "step": 4099
    },
    {
        "loss": 1.7016,
        "grad_norm": 2.1022019386291504,
        "learning_rate": 1.6288485817439846e-05,
        "epoch": 0.5466666666666666,
        "step": 4100
    },
    {
        "loss": 2.3341,
        "grad_norm": 2.5280683040618896,
        "learning_rate": 1.6254074315886792e-05,
        "epoch": 0.5468,
        "step": 4101
    },
    {
        "loss": 2.4484,
        "grad_norm": 2.938432455062866,
        "learning_rate": 1.6219695986380268e-05,
        "epoch": 0.5469333333333334,
        "step": 4102
    },
    {
        "loss": 2.4236,
        "grad_norm": 2.679642915725708,
        "learning_rate": 1.6185350842537627e-05,
        "epoch": 0.5470666666666667,
        "step": 4103
    },
    {
        "loss": 2.2866,
        "grad_norm": 3.7160871028900146,
        "learning_rate": 1.6151038897963143e-05,
        "epoch": 0.5472,
        "step": 4104
    },
    {
        "loss": 2.5655,
        "grad_norm": 2.53487491607666,
        "learning_rate": 1.6116760166247802e-05,
        "epoch": 0.5473333333333333,
        "step": 4105
    },
    {
        "loss": 2.4462,
        "grad_norm": 3.4668939113616943,
        "learning_rate": 1.6082514660969627e-05,
        "epoch": 0.5474666666666667,
        "step": 4106
    },
    {
        "loss": 2.3478,
        "grad_norm": 3.3938560485839844,
        "learning_rate": 1.6048302395693304e-05,
        "epoch": 0.5476,
        "step": 4107
    },
    {
        "loss": 2.5321,
        "grad_norm": 2.3679213523864746,
        "learning_rate": 1.6014123383970493e-05,
        "epoch": 0.5477333333333333,
        "step": 4108
    },
    {
        "loss": 2.3769,
        "grad_norm": 3.399710178375244,
        "learning_rate": 1.5979977639339537e-05,
        "epoch": 0.5478666666666666,
        "step": 4109
    },
    {
        "loss": 1.5181,
        "grad_norm": 1.46956467628479,
        "learning_rate": 1.59458651753258e-05,
        "epoch": 0.548,
        "step": 4110
    },
    {
        "loss": 3.7124,
        "grad_norm": 4.697291374206543,
        "learning_rate": 1.5911786005441232e-05,
        "epoch": 0.5481333333333334,
        "step": 4111
    },
    {
        "loss": 2.6836,
        "grad_norm": 1.7308132648468018,
        "learning_rate": 1.587774014318476e-05,
        "epoch": 0.5482666666666667,
        "step": 4112
    },
    {
        "loss": 2.6743,
        "grad_norm": 2.944826126098633,
        "learning_rate": 1.5843727602042046e-05,
        "epoch": 0.5484,
        "step": 4113
    },
    {
        "loss": 1.5294,
        "grad_norm": 3.833587169647217,
        "learning_rate": 1.580974839548558e-05,
        "epoch": 0.5485333333333333,
        "step": 4114
    },
    {
        "loss": 2.3165,
        "grad_norm": 2.6245646476745605,
        "learning_rate": 1.5775802536974647e-05,
        "epoch": 0.5486666666666666,
        "step": 4115
    },
    {
        "loss": 2.6523,
        "grad_norm": 2.0191380977630615,
        "learning_rate": 1.5741890039955332e-05,
        "epoch": 0.5488,
        "step": 4116
    },
    {
        "loss": 2.2288,
        "grad_norm": 3.350825548171997,
        "learning_rate": 1.570801091786046e-05,
        "epoch": 0.5489333333333334,
        "step": 4117
    },
    {
        "loss": 2.0593,
        "grad_norm": 5.83174991607666,
        "learning_rate": 1.56741651841097e-05,
        "epoch": 0.5490666666666667,
        "step": 4118
    },
    {
        "loss": 2.0436,
        "grad_norm": 3.430384397506714,
        "learning_rate": 1.564035285210943e-05,
        "epoch": 0.5492,
        "step": 4119
    },
    {
        "loss": 2.6043,
        "grad_norm": 3.7810230255126953,
        "learning_rate": 1.560657393525283e-05,
        "epoch": 0.5493333333333333,
        "step": 4120
    },
    {
        "loss": 1.7688,
        "grad_norm": 2.287881374359131,
        "learning_rate": 1.5572828446919897e-05,
        "epoch": 0.5494666666666667,
        "step": 4121
    },
    {
        "loss": 2.336,
        "grad_norm": 2.131124973297119,
        "learning_rate": 1.553911640047726e-05,
        "epoch": 0.5496,
        "step": 4122
    },
    {
        "loss": 2.1606,
        "grad_norm": 3.602177619934082,
        "learning_rate": 1.5505437809278432e-05,
        "epoch": 0.5497333333333333,
        "step": 4123
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.8246163129806519,
        "learning_rate": 1.5471792686663577e-05,
        "epoch": 0.5498666666666666,
        "step": 4124
    },
    {
        "loss": 1.125,
        "grad_norm": 3.63089919090271,
        "learning_rate": 1.5438181045959664e-05,
        "epoch": 0.55,
        "step": 4125
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.0190625190734863,
        "learning_rate": 1.540460290048037e-05,
        "epoch": 0.5501333333333334,
        "step": 4126
    },
    {
        "loss": 2.8834,
        "grad_norm": 2.362820863723755,
        "learning_rate": 1.537105826352615e-05,
        "epoch": 0.5502666666666667,
        "step": 4127
    },
    {
        "loss": 2.4795,
        "grad_norm": 2.3771140575408936,
        "learning_rate": 1.533754714838408e-05,
        "epoch": 0.5504,
        "step": 4128
    },
    {
        "loss": 1.9521,
        "grad_norm": 3.6011953353881836,
        "learning_rate": 1.5304069568328126e-05,
        "epoch": 0.5505333333333333,
        "step": 4129
    },
    {
        "loss": 2.7539,
        "grad_norm": 2.6147007942199707,
        "learning_rate": 1.527062553661873e-05,
        "epoch": 0.5506666666666666,
        "step": 4130
    },
    {
        "loss": 2.0886,
        "grad_norm": 3.004990339279175,
        "learning_rate": 1.523721506650332e-05,
        "epoch": 0.5508,
        "step": 4131
    },
    {
        "loss": 2.6903,
        "grad_norm": 4.295003414154053,
        "learning_rate": 1.5203838171215824e-05,
        "epoch": 0.5509333333333334,
        "step": 4132
    },
    {
        "loss": 1.5913,
        "grad_norm": 3.8324267864227295,
        "learning_rate": 1.5170494863976981e-05,
        "epoch": 0.5510666666666667,
        "step": 4133
    },
    {
        "loss": 2.8666,
        "grad_norm": 3.9375009536743164,
        "learning_rate": 1.5137185157994127e-05,
        "epoch": 0.5512,
        "step": 4134
    },
    {
        "loss": 3.3613,
        "grad_norm": 3.0878641605377197,
        "learning_rate": 1.5103909066461475e-05,
        "epoch": 0.5513333333333333,
        "step": 4135
    },
    {
        "loss": 1.674,
        "grad_norm": 4.475473880767822,
        "learning_rate": 1.5070666602559657e-05,
        "epoch": 0.5514666666666667,
        "step": 4136
    },
    {
        "loss": 3.1222,
        "grad_norm": 2.965339183807373,
        "learning_rate": 1.503745777945622e-05,
        "epoch": 0.5516,
        "step": 4137
    },
    {
        "loss": 2.1959,
        "grad_norm": 2.598870277404785,
        "learning_rate": 1.5004282610305232e-05,
        "epoch": 0.5517333333333333,
        "step": 4138
    },
    {
        "loss": 2.1499,
        "grad_norm": 1.804200291633606,
        "learning_rate": 1.4971141108247533e-05,
        "epoch": 0.5518666666666666,
        "step": 4139
    },
    {
        "loss": 2.5499,
        "grad_norm": 1.954294204711914,
        "learning_rate": 1.4938033286410535e-05,
        "epoch": 0.552,
        "step": 4140
    },
    {
        "loss": 2.2777,
        "grad_norm": 2.6706669330596924,
        "learning_rate": 1.4904959157908415e-05,
        "epoch": 0.5521333333333334,
        "step": 4141
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.604750871658325,
        "learning_rate": 1.487191873584195e-05,
        "epoch": 0.5522666666666667,
        "step": 4142
    },
    {
        "loss": 2.8694,
        "grad_norm": 2.7554001808166504,
        "learning_rate": 1.4838912033298512e-05,
        "epoch": 0.5524,
        "step": 4143
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.3337202072143555,
        "learning_rate": 1.4805939063352225e-05,
        "epoch": 0.5525333333333333,
        "step": 4144
    },
    {
        "loss": 2.6983,
        "grad_norm": 3.121983289718628,
        "learning_rate": 1.4772999839063739e-05,
        "epoch": 0.5526666666666666,
        "step": 4145
    },
    {
        "loss": 2.8237,
        "grad_norm": 2.919954299926758,
        "learning_rate": 1.474009437348045e-05,
        "epoch": 0.5528,
        "step": 4146
    },
    {
        "loss": 2.9663,
        "grad_norm": 2.128445863723755,
        "learning_rate": 1.4707222679636268e-05,
        "epoch": 0.5529333333333334,
        "step": 4147
    },
    {
        "loss": 0.8158,
        "grad_norm": 3.6595966815948486,
        "learning_rate": 1.467438477055183e-05,
        "epoch": 0.5530666666666667,
        "step": 4148
    },
    {
        "loss": 1.6236,
        "grad_norm": 4.08696174621582,
        "learning_rate": 1.464158065923431e-05,
        "epoch": 0.5532,
        "step": 4149
    },
    {
        "loss": 1.7308,
        "grad_norm": 2.255570650100708,
        "learning_rate": 1.4608810358677539e-05,
        "epoch": 0.5533333333333333,
        "step": 4150
    },
    {
        "loss": 1.8297,
        "grad_norm": 5.017848014831543,
        "learning_rate": 1.4576073881861951e-05,
        "epoch": 0.5534666666666667,
        "step": 4151
    },
    {
        "loss": 2.1495,
        "grad_norm": 2.12111234664917,
        "learning_rate": 1.4543371241754588e-05,
        "epoch": 0.5536,
        "step": 4152
    },
    {
        "loss": 2.1116,
        "grad_norm": 3.7718467712402344,
        "learning_rate": 1.4510702451309055e-05,
        "epoch": 0.5537333333333333,
        "step": 4153
    },
    {
        "loss": 2.0768,
        "grad_norm": 2.84728741645813,
        "learning_rate": 1.447806752346561e-05,
        "epoch": 0.5538666666666666,
        "step": 4154
    },
    {
        "loss": 1.3797,
        "grad_norm": 3.6997339725494385,
        "learning_rate": 1.4445466471150982e-05,
        "epoch": 0.554,
        "step": 4155
    },
    {
        "loss": 2.416,
        "grad_norm": 2.1351656913757324,
        "learning_rate": 1.4412899307278616e-05,
        "epoch": 0.5541333333333334,
        "step": 4156
    },
    {
        "loss": 2.5948,
        "grad_norm": 2.7762184143066406,
        "learning_rate": 1.438036604474845e-05,
        "epoch": 0.5542666666666667,
        "step": 4157
    },
    {
        "loss": 2.2122,
        "grad_norm": 2.9602766036987305,
        "learning_rate": 1.4347866696447076e-05,
        "epoch": 0.5544,
        "step": 4158
    },
    {
        "loss": 1.4734,
        "grad_norm": 2.948831081390381,
        "learning_rate": 1.4315401275247498e-05,
        "epoch": 0.5545333333333333,
        "step": 4159
    },
    {
        "loss": 2.8913,
        "grad_norm": 2.7672412395477295,
        "learning_rate": 1.428296979400946e-05,
        "epoch": 0.5546666666666666,
        "step": 4160
    },
    {
        "loss": 2.6341,
        "grad_norm": 2.5955660343170166,
        "learning_rate": 1.4250572265579209e-05,
        "epoch": 0.5548,
        "step": 4161
    },
    {
        "loss": 2.7315,
        "grad_norm": 2.9442520141601562,
        "learning_rate": 1.4218208702789425e-05,
        "epoch": 0.5549333333333333,
        "step": 4162
    },
    {
        "loss": 1.9942,
        "grad_norm": 3.1113202571868896,
        "learning_rate": 1.4185879118459489e-05,
        "epoch": 0.5550666666666667,
        "step": 4163
    },
    {
        "loss": 2.9015,
        "grad_norm": 2.774726390838623,
        "learning_rate": 1.415358352539522e-05,
        "epoch": 0.5552,
        "step": 4164
    },
    {
        "loss": 1.9746,
        "grad_norm": 3.5365078449249268,
        "learning_rate": 1.412132193638902e-05,
        "epoch": 0.5553333333333333,
        "step": 4165
    },
    {
        "loss": 3.0274,
        "grad_norm": 2.3491241931915283,
        "learning_rate": 1.4089094364219834e-05,
        "epoch": 0.5554666666666667,
        "step": 4166
    },
    {
        "loss": 1.9555,
        "grad_norm": 1.9662240743637085,
        "learning_rate": 1.4056900821653129e-05,
        "epoch": 0.5556,
        "step": 4167
    },
    {
        "loss": 0.7261,
        "grad_norm": 3.564612627029419,
        "learning_rate": 1.4024741321440826e-05,
        "epoch": 0.5557333333333333,
        "step": 4168
    },
    {
        "loss": 2.6157,
        "grad_norm": 2.9074795246124268,
        "learning_rate": 1.3992615876321458e-05,
        "epoch": 0.5558666666666666,
        "step": 4169
    },
    {
        "loss": 2.4395,
        "grad_norm": 1.9995516538619995,
        "learning_rate": 1.396052449901999e-05,
        "epoch": 0.556,
        "step": 4170
    },
    {
        "loss": 2.1489,
        "grad_norm": 3.0406196117401123,
        "learning_rate": 1.3928467202247953e-05,
        "epoch": 0.5561333333333334,
        "step": 4171
    },
    {
        "loss": 2.6676,
        "grad_norm": 2.523244857788086,
        "learning_rate": 1.3896443998703323e-05,
        "epoch": 0.5562666666666667,
        "step": 4172
    },
    {
        "loss": 1.9454,
        "grad_norm": 3.2289512157440186,
        "learning_rate": 1.3864454901070634e-05,
        "epoch": 0.5564,
        "step": 4173
    },
    {
        "loss": 1.754,
        "grad_norm": 4.089713096618652,
        "learning_rate": 1.3832499922020848e-05,
        "epoch": 0.5565333333333333,
        "step": 4174
    },
    {
        "loss": 3.4507,
        "grad_norm": 4.533400058746338,
        "learning_rate": 1.3800579074211439e-05,
        "epoch": 0.5566666666666666,
        "step": 4175
    },
    {
        "loss": 2.8107,
        "grad_norm": 1.8831144571304321,
        "learning_rate": 1.3768692370286374e-05,
        "epoch": 0.5568,
        "step": 4176
    },
    {
        "loss": 2.0837,
        "grad_norm": 2.9638023376464844,
        "learning_rate": 1.3736839822876124e-05,
        "epoch": 0.5569333333333333,
        "step": 4177
    },
    {
        "loss": 2.0676,
        "grad_norm": 3.3684122562408447,
        "learning_rate": 1.3705021444597522e-05,
        "epoch": 0.5570666666666667,
        "step": 4178
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.405536413192749,
        "learning_rate": 1.3673237248054016e-05,
        "epoch": 0.5572,
        "step": 4179
    },
    {
        "loss": 2.262,
        "grad_norm": 2.6049227714538574,
        "learning_rate": 1.3641487245835339e-05,
        "epoch": 0.5573333333333333,
        "step": 4180
    },
    {
        "loss": 2.3026,
        "grad_norm": 3.1284215450286865,
        "learning_rate": 1.3609771450517806e-05,
        "epoch": 0.5574666666666667,
        "step": 4181
    },
    {
        "loss": 2.3034,
        "grad_norm": 3.114598035812378,
        "learning_rate": 1.3578089874664202e-05,
        "epoch": 0.5576,
        "step": 4182
    },
    {
        "loss": 2.4331,
        "grad_norm": 3.0128397941589355,
        "learning_rate": 1.3546442530823667e-05,
        "epoch": 0.5577333333333333,
        "step": 4183
    },
    {
        "loss": 1.9696,
        "grad_norm": 3.2750022411346436,
        "learning_rate": 1.3514829431531817e-05,
        "epoch": 0.5578666666666666,
        "step": 4184
    },
    {
        "loss": 2.4126,
        "grad_norm": 2.7879116535186768,
        "learning_rate": 1.3483250589310725e-05,
        "epoch": 0.558,
        "step": 4185
    },
    {
        "loss": 2.3994,
        "grad_norm": 3.488767623901367,
        "learning_rate": 1.3451706016668919e-05,
        "epoch": 0.5581333333333334,
        "step": 4186
    },
    {
        "loss": 1.2139,
        "grad_norm": 3.6954052448272705,
        "learning_rate": 1.3420195726101225e-05,
        "epoch": 0.5582666666666667,
        "step": 4187
    },
    {
        "loss": 1.1204,
        "grad_norm": 3.1788406372070312,
        "learning_rate": 1.3388719730089027e-05,
        "epoch": 0.5584,
        "step": 4188
    },
    {
        "loss": 2.499,
        "grad_norm": 3.0259270668029785,
        "learning_rate": 1.3357278041100053e-05,
        "epoch": 0.5585333333333333,
        "step": 4189
    },
    {
        "loss": 2.407,
        "grad_norm": 2.274773120880127,
        "learning_rate": 1.332587067158847e-05,
        "epoch": 0.5586666666666666,
        "step": 4190
    },
    {
        "loss": 2.605,
        "grad_norm": 2.2622828483581543,
        "learning_rate": 1.3294497633994852e-05,
        "epoch": 0.5588,
        "step": 4191
    },
    {
        "loss": 2.4708,
        "grad_norm": 2.6797690391540527,
        "learning_rate": 1.326315894074619e-05,
        "epoch": 0.5589333333333333,
        "step": 4192
    },
    {
        "loss": 2.4032,
        "grad_norm": 2.651747465133667,
        "learning_rate": 1.3231854604255788e-05,
        "epoch": 0.5590666666666667,
        "step": 4193
    },
    {
        "loss": 2.1365,
        "grad_norm": 2.7507781982421875,
        "learning_rate": 1.3200584636923463e-05,
        "epoch": 0.5592,
        "step": 4194
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.5969974994659424,
        "learning_rate": 1.3169349051135293e-05,
        "epoch": 0.5593333333333333,
        "step": 4195
    },
    {
        "loss": 2.6359,
        "grad_norm": 2.3201560974121094,
        "learning_rate": 1.3138147859263861e-05,
        "epoch": 0.5594666666666667,
        "step": 4196
    },
    {
        "loss": 2.8343,
        "grad_norm": 2.6690475940704346,
        "learning_rate": 1.3106981073668012e-05,
        "epoch": 0.5596,
        "step": 4197
    },
    {
        "loss": 2.0954,
        "grad_norm": 2.5021934509277344,
        "learning_rate": 1.3075848706693072e-05,
        "epoch": 0.5597333333333333,
        "step": 4198
    },
    {
        "loss": 2.2203,
        "grad_norm": 3.3859307765960693,
        "learning_rate": 1.304475077067061e-05,
        "epoch": 0.5598666666666666,
        "step": 4199
    },
    {
        "loss": 1.5943,
        "grad_norm": 3.4246773719787598,
        "learning_rate": 1.3013687277918662e-05,
        "epoch": 0.56,
        "step": 4200
    },
    {
        "loss": 2.0746,
        "grad_norm": 4.0563812255859375,
        "learning_rate": 1.2982658240741597e-05,
        "epoch": 0.5601333333333334,
        "step": 4201
    },
    {
        "loss": 1.7273,
        "grad_norm": 2.167402982711792,
        "learning_rate": 1.2951663671430092e-05,
        "epoch": 0.5602666666666667,
        "step": 4202
    },
    {
        "loss": 1.9751,
        "grad_norm": 2.50752854347229,
        "learning_rate": 1.292070358226124e-05,
        "epoch": 0.5604,
        "step": 4203
    },
    {
        "loss": 2.5149,
        "grad_norm": 3.625765562057495,
        "learning_rate": 1.2889777985498397e-05,
        "epoch": 0.5605333333333333,
        "step": 4204
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.435541868209839,
        "learning_rate": 1.2858886893391353e-05,
        "epoch": 0.5606666666666666,
        "step": 4205
    },
    {
        "loss": 2.3246,
        "grad_norm": 3.413235902786255,
        "learning_rate": 1.2828030318176077e-05,
        "epoch": 0.5608,
        "step": 4206
    },
    {
        "loss": 1.3118,
        "grad_norm": 2.8911080360412598,
        "learning_rate": 1.2797208272075067e-05,
        "epoch": 0.5609333333333333,
        "step": 4207
    },
    {
        "loss": 2.9511,
        "grad_norm": 3.162163734436035,
        "learning_rate": 1.2766420767296972e-05,
        "epoch": 0.5610666666666667,
        "step": 4208
    },
    {
        "loss": 2.463,
        "grad_norm": 2.2753610610961914,
        "learning_rate": 1.2735667816036878e-05,
        "epoch": 0.5612,
        "step": 4209
    },
    {
        "loss": 2.149,
        "grad_norm": 3.3737895488739014,
        "learning_rate": 1.2704949430476054e-05,
        "epoch": 0.5613333333333334,
        "step": 4210
    },
    {
        "loss": 2.1118,
        "grad_norm": 3.36383318901062,
        "learning_rate": 1.2674265622782288e-05,
        "epoch": 0.5614666666666667,
        "step": 4211
    },
    {
        "loss": 2.0989,
        "grad_norm": 3.0343334674835205,
        "learning_rate": 1.2643616405109415e-05,
        "epoch": 0.5616,
        "step": 4212
    },
    {
        "loss": 2.222,
        "grad_norm": 3.786449670791626,
        "learning_rate": 1.2613001789597766e-05,
        "epoch": 0.5617333333333333,
        "step": 4213
    },
    {
        "loss": 2.4113,
        "grad_norm": 2.9196970462799072,
        "learning_rate": 1.2582421788373855e-05,
        "epoch": 0.5618666666666666,
        "step": 4214
    },
    {
        "loss": 1.3549,
        "grad_norm": 3.4024906158447266,
        "learning_rate": 1.2551876413550533e-05,
        "epoch": 0.562,
        "step": 4215
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.9891488552093506,
        "learning_rate": 1.2521365677226937e-05,
        "epoch": 0.5621333333333334,
        "step": 4216
    },
    {
        "loss": 2.5707,
        "grad_norm": 2.8263378143310547,
        "learning_rate": 1.2490889591488497e-05,
        "epoch": 0.5622666666666667,
        "step": 4217
    },
    {
        "loss": 2.0976,
        "grad_norm": 3.3663604259490967,
        "learning_rate": 1.2460448168406846e-05,
        "epoch": 0.5624,
        "step": 4218
    },
    {
        "loss": 2.2054,
        "grad_norm": 3.2597904205322266,
        "learning_rate": 1.2430041420039985e-05,
        "epoch": 0.5625333333333333,
        "step": 4219
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.455404758453369,
        "learning_rate": 1.2399669358432086e-05,
        "epoch": 0.5626666666666666,
        "step": 4220
    },
    {
        "loss": 2.1666,
        "grad_norm": 3.7909555435180664,
        "learning_rate": 1.2369331995613643e-05,
        "epoch": 0.5628,
        "step": 4221
    },
    {
        "loss": 2.171,
        "grad_norm": 2.6084165573120117,
        "learning_rate": 1.2339029343601405e-05,
        "epoch": 0.5629333333333333,
        "step": 4222
    },
    {
        "loss": 2.3136,
        "grad_norm": 1.4578661918640137,
        "learning_rate": 1.2308761414398328e-05,
        "epoch": 0.5630666666666667,
        "step": 4223
    },
    {
        "loss": 2.6079,
        "grad_norm": 2.3207573890686035,
        "learning_rate": 1.2278528219993668e-05,
        "epoch": 0.5632,
        "step": 4224
    },
    {
        "loss": 1.4588,
        "grad_norm": 5.183388710021973,
        "learning_rate": 1.2248329772362865e-05,
        "epoch": 0.5633333333333334,
        "step": 4225
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.9562599658966064,
        "learning_rate": 1.2218166083467653e-05,
        "epoch": 0.5634666666666667,
        "step": 4226
    },
    {
        "loss": 1.5393,
        "grad_norm": 5.297942638397217,
        "learning_rate": 1.218803716525596e-05,
        "epoch": 0.5636,
        "step": 4227
    },
    {
        "loss": 2.838,
        "grad_norm": 1.9722981452941895,
        "learning_rate": 1.2157943029661978e-05,
        "epoch": 0.5637333333333333,
        "step": 4228
    },
    {
        "loss": 2.1164,
        "grad_norm": 3.405137300491333,
        "learning_rate": 1.2127883688606056e-05,
        "epoch": 0.5638666666666666,
        "step": 4229
    },
    {
        "loss": 2.1536,
        "grad_norm": 3.2667319774627686,
        "learning_rate": 1.2097859153994862e-05,
        "epoch": 0.564,
        "step": 4230
    },
    {
        "loss": 3.3273,
        "grad_norm": 3.134746551513672,
        "learning_rate": 1.2067869437721113e-05,
        "epoch": 0.5641333333333334,
        "step": 4231
    },
    {
        "loss": 2.1578,
        "grad_norm": 2.959125518798828,
        "learning_rate": 1.2037914551663953e-05,
        "epoch": 0.5642666666666667,
        "step": 4232
    },
    {
        "loss": 2.3127,
        "grad_norm": 2.9578208923339844,
        "learning_rate": 1.2007994507688524e-05,
        "epoch": 0.5644,
        "step": 4233
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.9165756702423096,
        "learning_rate": 1.1978109317646335e-05,
        "epoch": 0.5645333333333333,
        "step": 4234
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.1874775886535645,
        "learning_rate": 1.1948258993374917e-05,
        "epoch": 0.5646666666666667,
        "step": 4235
    },
    {
        "loss": 1.4204,
        "grad_norm": 4.473874568939209,
        "learning_rate": 1.191844354669821e-05,
        "epoch": 0.5648,
        "step": 4236
    },
    {
        "loss": 2.2836,
        "grad_norm": 3.453702688217163,
        "learning_rate": 1.1888662989426102e-05,
        "epoch": 0.5649333333333333,
        "step": 4237
    },
    {
        "loss": 2.2351,
        "grad_norm": 2.8201608657836914,
        "learning_rate": 1.1858917333354835e-05,
        "epoch": 0.5650666666666667,
        "step": 4238
    },
    {
        "loss": 1.3607,
        "grad_norm": 4.4267497062683105,
        "learning_rate": 1.1829206590266729e-05,
        "epoch": 0.5652,
        "step": 4239
    },
    {
        "loss": 2.6449,
        "grad_norm": 3.475435256958008,
        "learning_rate": 1.1799530771930346e-05,
        "epoch": 0.5653333333333334,
        "step": 4240
    },
    {
        "loss": 2.5052,
        "grad_norm": 1.5255790948867798,
        "learning_rate": 1.1769889890100316e-05,
        "epoch": 0.5654666666666667,
        "step": 4241
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.4291539192199707,
        "learning_rate": 1.1740283956517562e-05,
        "epoch": 0.5656,
        "step": 4242
    },
    {
        "loss": 2.6436,
        "grad_norm": 3.664445638656616,
        "learning_rate": 1.171071298290909e-05,
        "epoch": 0.5657333333333333,
        "step": 4243
    },
    {
        "loss": 1.9924,
        "grad_norm": 3.9041943550109863,
        "learning_rate": 1.1681176980988018e-05,
        "epoch": 0.5658666666666666,
        "step": 4244
    },
    {
        "loss": 2.8702,
        "grad_norm": 3.3246421813964844,
        "learning_rate": 1.1651675962453713e-05,
        "epoch": 0.566,
        "step": 4245
    },
    {
        "loss": 2.1707,
        "grad_norm": 2.3922574520111084,
        "learning_rate": 1.1622209938991569e-05,
        "epoch": 0.5661333333333334,
        "step": 4246
    },
    {
        "loss": 2.2375,
        "grad_norm": 2.73500394821167,
        "learning_rate": 1.1592778922273229e-05,
        "epoch": 0.5662666666666667,
        "step": 4247
    },
    {
        "loss": 2.5227,
        "grad_norm": 3.791990041732788,
        "learning_rate": 1.1563382923956379e-05,
        "epoch": 0.5664,
        "step": 4248
    },
    {
        "loss": 1.6821,
        "grad_norm": 4.128780364990234,
        "learning_rate": 1.1534021955684915e-05,
        "epoch": 0.5665333333333333,
        "step": 4249
    },
    {
        "loss": 2.5549,
        "grad_norm": 3.0778324604034424,
        "learning_rate": 1.150469602908878e-05,
        "epoch": 0.5666666666666667,
        "step": 4250
    },
    {
        "loss": 2.2496,
        "grad_norm": 3.012979507446289,
        "learning_rate": 1.1475405155784081e-05,
        "epoch": 0.5668,
        "step": 4251
    },
    {
        "loss": 1.7842,
        "grad_norm": 3.3029165267944336,
        "learning_rate": 1.1446149347373048e-05,
        "epoch": 0.5669333333333333,
        "step": 4252
    },
    {
        "loss": 1.8139,
        "grad_norm": 3.305612087249756,
        "learning_rate": 1.1416928615444011e-05,
        "epoch": 0.5670666666666667,
        "step": 4253
    },
    {
        "loss": 3.2,
        "grad_norm": 2.797081708908081,
        "learning_rate": 1.1387742971571369e-05,
        "epoch": 0.5672,
        "step": 4254
    },
    {
        "loss": 1.3037,
        "grad_norm": 3.0951693058013916,
        "learning_rate": 1.1358592427315718e-05,
        "epoch": 0.5673333333333334,
        "step": 4255
    },
    {
        "loss": 1.7503,
        "grad_norm": 3.1184260845184326,
        "learning_rate": 1.1329476994223597e-05,
        "epoch": 0.5674666666666667,
        "step": 4256
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.7127983570098877,
        "learning_rate": 1.1300396683827786e-05,
        "epoch": 0.5676,
        "step": 4257
    },
    {
        "loss": 1.3471,
        "grad_norm": 3.000255823135376,
        "learning_rate": 1.1271351507647077e-05,
        "epoch": 0.5677333333333333,
        "step": 4258
    },
    {
        "loss": 1.5709,
        "grad_norm": 3.153290033340454,
        "learning_rate": 1.1242341477186402e-05,
        "epoch": 0.5678666666666666,
        "step": 4259
    },
    {
        "loss": 3.799,
        "grad_norm": 5.21708869934082,
        "learning_rate": 1.1213366603936648e-05,
        "epoch": 0.568,
        "step": 4260
    },
    {
        "loss": 2.9525,
        "grad_norm": 2.9999186992645264,
        "learning_rate": 1.1184426899374945e-05,
        "epoch": 0.5681333333333334,
        "step": 4261
    },
    {
        "loss": 2.6317,
        "grad_norm": 2.613374710083008,
        "learning_rate": 1.1155522374964412e-05,
        "epoch": 0.5682666666666667,
        "step": 4262
    },
    {
        "loss": 2.5098,
        "grad_norm": 3.871001720428467,
        "learning_rate": 1.1126653042154168e-05,
        "epoch": 0.5684,
        "step": 4263
    },
    {
        "loss": 2.5657,
        "grad_norm": 2.3787038326263428,
        "learning_rate": 1.1097818912379488e-05,
        "epoch": 0.5685333333333333,
        "step": 4264
    },
    {
        "loss": 2.7948,
        "grad_norm": 2.931607723236084,
        "learning_rate": 1.1069019997061658e-05,
        "epoch": 0.5686666666666667,
        "step": 4265
    },
    {
        "loss": 3.7434,
        "grad_norm": 2.116732358932495,
        "learning_rate": 1.104025630760802e-05,
        "epoch": 0.5688,
        "step": 4266
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.7081730365753174,
        "learning_rate": 1.1011527855411985e-05,
        "epoch": 0.5689333333333333,
        "step": 4267
    },
    {
        "loss": 0.989,
        "grad_norm": 2.961435556411743,
        "learning_rate": 1.0982834651853013e-05,
        "epoch": 0.5690666666666667,
        "step": 4268
    },
    {
        "loss": 1.1565,
        "grad_norm": 4.447704792022705,
        "learning_rate": 1.0954176708296527e-05,
        "epoch": 0.5692,
        "step": 4269
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.532191514968872,
        "learning_rate": 1.0925554036094088e-05,
        "epoch": 0.5693333333333334,
        "step": 4270
    },
    {
        "loss": 1.5684,
        "grad_norm": 4.1628570556640625,
        "learning_rate": 1.0896966646583184e-05,
        "epoch": 0.5694666666666667,
        "step": 4271
    },
    {
        "loss": 1.4078,
        "grad_norm": 4.177940368652344,
        "learning_rate": 1.0868414551087424e-05,
        "epoch": 0.5696,
        "step": 4272
    },
    {
        "loss": 2.1538,
        "grad_norm": 2.6158833503723145,
        "learning_rate": 1.083989776091635e-05,
        "epoch": 0.5697333333333333,
        "step": 4273
    },
    {
        "loss": 2.3625,
        "grad_norm": 2.697453498840332,
        "learning_rate": 1.0811416287365606e-05,
        "epoch": 0.5698666666666666,
        "step": 4274
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.7086358070373535,
        "learning_rate": 1.0782970141716741e-05,
        "epoch": 0.57,
        "step": 4275
    },
    {
        "loss": 0.678,
        "grad_norm": 2.7407453060150146,
        "learning_rate": 1.0754559335237412e-05,
        "epoch": 0.5701333333333334,
        "step": 4276
    },
    {
        "loss": 2.516,
        "grad_norm": 2.7167885303497314,
        "learning_rate": 1.0726183879181218e-05,
        "epoch": 0.5702666666666667,
        "step": 4277
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.7480850219726562,
        "learning_rate": 1.0697843784787808e-05,
        "epoch": 0.5704,
        "step": 4278
    },
    {
        "loss": 2.7024,
        "grad_norm": 2.28869366645813,
        "learning_rate": 1.0669539063282741e-05,
        "epoch": 0.5705333333333333,
        "step": 4279
    },
    {
        "loss": 2.6269,
        "grad_norm": 5.137398719787598,
        "learning_rate": 1.0641269725877679e-05,
        "epoch": 0.5706666666666667,
        "step": 4280
    },
    {
        "loss": 0.8576,
        "grad_norm": 3.4059674739837646,
        "learning_rate": 1.061303578377012e-05,
        "epoch": 0.5708,
        "step": 4281
    },
    {
        "loss": 2.7601,
        "grad_norm": 2.3451015949249268,
        "learning_rate": 1.0584837248143642e-05,
        "epoch": 0.5709333333333333,
        "step": 4282
    },
    {
        "loss": 2.5768,
        "grad_norm": 2.8675975799560547,
        "learning_rate": 1.0556674130167853e-05,
        "epoch": 0.5710666666666666,
        "step": 4283
    },
    {
        "loss": 1.8413,
        "grad_norm": 3.016702651977539,
        "learning_rate": 1.0528546440998188e-05,
        "epoch": 0.5712,
        "step": 4284
    },
    {
        "loss": 1.6023,
        "grad_norm": 7.308816909790039,
        "learning_rate": 1.0500454191776143e-05,
        "epoch": 0.5713333333333334,
        "step": 4285
    },
    {
        "loss": 2.291,
        "grad_norm": 2.1973214149475098,
        "learning_rate": 1.0472397393629163e-05,
        "epoch": 0.5714666666666667,
        "step": 4286
    },
    {
        "loss": 2.2894,
        "grad_norm": 3.4165103435516357,
        "learning_rate": 1.0444376057670657e-05,
        "epoch": 0.5716,
        "step": 4287
    },
    {
        "loss": 0.9832,
        "grad_norm": 3.333313465118408,
        "learning_rate": 1.0416390194999925e-05,
        "epoch": 0.5717333333333333,
        "step": 4288
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.5750086307525635,
        "learning_rate": 1.0388439816702289e-05,
        "epoch": 0.5718666666666666,
        "step": 4289
    },
    {
        "loss": 2.6204,
        "grad_norm": 2.1081936359405518,
        "learning_rate": 1.036052493384898e-05,
        "epoch": 0.572,
        "step": 4290
    },
    {
        "loss": 2.4121,
        "grad_norm": 2.2733116149902344,
        "learning_rate": 1.0332645557497179e-05,
        "epoch": 0.5721333333333334,
        "step": 4291
    },
    {
        "loss": 2.5733,
        "grad_norm": 3.4075844287872314,
        "learning_rate": 1.0304801698690003e-05,
        "epoch": 0.5722666666666667,
        "step": 4292
    },
    {
        "loss": 2.7819,
        "grad_norm": 2.8102099895477295,
        "learning_rate": 1.0276993368456522e-05,
        "epoch": 0.5724,
        "step": 4293
    },
    {
        "loss": 1.6981,
        "grad_norm": 2.713034152984619,
        "learning_rate": 1.024922057781168e-05,
        "epoch": 0.5725333333333333,
        "step": 4294
    },
    {
        "loss": 2.9199,
        "grad_norm": 3.662381172180176,
        "learning_rate": 1.0221483337756399e-05,
        "epoch": 0.5726666666666667,
        "step": 4295
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.4736287593841553,
        "learning_rate": 1.0193781659277458e-05,
        "epoch": 0.5728,
        "step": 4296
    },
    {
        "loss": 1.9059,
        "grad_norm": 2.636002779006958,
        "learning_rate": 1.0166115553347632e-05,
        "epoch": 0.5729333333333333,
        "step": 4297
    },
    {
        "loss": 1.7743,
        "grad_norm": 3.64011549949646,
        "learning_rate": 1.0138485030925527e-05,
        "epoch": 0.5730666666666666,
        "step": 4298
    },
    {
        "loss": 2.8498,
        "grad_norm": 2.65706205368042,
        "learning_rate": 1.0110890102955706e-05,
        "epoch": 0.5732,
        "step": 4299
    },
    {
        "loss": 2.3184,
        "grad_norm": 3.936359167098999,
        "learning_rate": 1.0083330780368582e-05,
        "epoch": 0.5733333333333334,
        "step": 4300
    },
    {
        "loss": 2.3319,
        "grad_norm": 3.1336183547973633,
        "learning_rate": 1.0055807074080504e-05,
        "epoch": 0.5734666666666667,
        "step": 4301
    },
    {
        "loss": 1.1081,
        "grad_norm": Infinity,
        "learning_rate": 1.0055807074080504e-05,
        "epoch": 0.5736,
        "step": 4302
    },
    {
        "loss": 2.5974,
        "grad_norm": 3.935516357421875,
        "learning_rate": 1.0028318994993758e-05,
        "epoch": 0.5737333333333333,
        "step": 4303
    },
    {
        "loss": 1.5379,
        "grad_norm": 3.125537633895874,
        "learning_rate": 1.0000866553996414e-05,
        "epoch": 0.5738666666666666,
        "step": 4304
    },
    {
        "loss": 2.6767,
        "grad_norm": 3.5213356018066406,
        "learning_rate": 9.973449761962517e-06,
        "epoch": 0.574,
        "step": 4305
    },
    {
        "loss": 2.4864,
        "grad_norm": 2.5848302841186523,
        "learning_rate": 9.946068629751892e-06,
        "epoch": 0.5741333333333334,
        "step": 4306
    },
    {
        "loss": 2.5387,
        "grad_norm": 2.482511043548584,
        "learning_rate": 9.918723168210386e-06,
        "epoch": 0.5742666666666667,
        "step": 4307
    },
    {
        "loss": 3.0917,
        "grad_norm": 2.4987146854400635,
        "learning_rate": 9.891413388169513e-06,
        "epoch": 0.5744,
        "step": 4308
    },
    {
        "loss": 2.4175,
        "grad_norm": 2.707603931427002,
        "learning_rate": 9.864139300446873e-06,
        "epoch": 0.5745333333333333,
        "step": 4309
    },
    {
        "loss": 1.9158,
        "grad_norm": 3.082174777984619,
        "learning_rate": 9.83690091584577e-06,
        "epoch": 0.5746666666666667,
        "step": 4310
    },
    {
        "loss": 1.9735,
        "grad_norm": 3.2284960746765137,
        "learning_rate": 9.809698245155463e-06,
        "epoch": 0.5748,
        "step": 4311
    },
    {
        "loss": 2.8347,
        "grad_norm": 2.961470603942871,
        "learning_rate": 9.782531299150954e-06,
        "epoch": 0.5749333333333333,
        "step": 4312
    },
    {
        "loss": 2.4744,
        "grad_norm": 2.9919662475585938,
        "learning_rate": 9.755400088593258e-06,
        "epoch": 0.5750666666666666,
        "step": 4313
    },
    {
        "loss": 1.7669,
        "grad_norm": 4.298333644866943,
        "learning_rate": 9.728304624229056e-06,
        "epoch": 0.5752,
        "step": 4314
    },
    {
        "loss": 2.5812,
        "grad_norm": 2.3452980518341064,
        "learning_rate": 9.701244916790996e-06,
        "epoch": 0.5753333333333334,
        "step": 4315
    },
    {
        "loss": 1.9324,
        "grad_norm": 4.357650279998779,
        "learning_rate": 9.674220976997494e-06,
        "epoch": 0.5754666666666667,
        "step": 4316
    },
    {
        "loss": 2.0525,
        "grad_norm": 3.4135684967041016,
        "learning_rate": 9.647232815552843e-06,
        "epoch": 0.5756,
        "step": 4317
    },
    {
        "loss": 2.147,
        "grad_norm": 1.6771268844604492,
        "learning_rate": 9.620280443147145e-06,
        "epoch": 0.5757333333333333,
        "step": 4318
    },
    {
        "loss": 2.874,
        "grad_norm": 2.4524385929107666,
        "learning_rate": 9.59336387045634e-06,
        "epoch": 0.5758666666666666,
        "step": 4319
    },
    {
        "loss": 1.4479,
        "grad_norm": 3.1146435737609863,
        "learning_rate": 9.566483108142144e-06,
        "epoch": 0.576,
        "step": 4320
    },
    {
        "loss": 2.0535,
        "grad_norm": 3.090951919555664,
        "learning_rate": 9.53963816685215e-06,
        "epoch": 0.5761333333333334,
        "step": 4321
    },
    {
        "loss": 2.4813,
        "grad_norm": 2.272617816925049,
        "learning_rate": 9.512829057219697e-06,
        "epoch": 0.5762666666666667,
        "step": 4322
    },
    {
        "loss": 2.4863,
        "grad_norm": 2.9245851039886475,
        "learning_rate": 9.486055789863979e-06,
        "epoch": 0.5764,
        "step": 4323
    },
    {
        "loss": 2.879,
        "grad_norm": 4.871779918670654,
        "learning_rate": 9.459318375390003e-06,
        "epoch": 0.5765333333333333,
        "step": 4324
    },
    {
        "loss": 2.1939,
        "grad_norm": 2.253877878189087,
        "learning_rate": 9.432616824388508e-06,
        "epoch": 0.5766666666666667,
        "step": 4325
    },
    {
        "loss": 2.8149,
        "grad_norm": 2.5448131561279297,
        "learning_rate": 9.40595114743611e-06,
        "epoch": 0.5768,
        "step": 4326
    },
    {
        "loss": 1.0217,
        "grad_norm": 3.991420030593872,
        "learning_rate": 9.379321355095139e-06,
        "epoch": 0.5769333333333333,
        "step": 4327
    },
    {
        "loss": 2.6603,
        "grad_norm": 1.9446338415145874,
        "learning_rate": 9.352727457913756e-06,
        "epoch": 0.5770666666666666,
        "step": 4328
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.350642442703247,
        "learning_rate": 9.326169466425893e-06,
        "epoch": 0.5772,
        "step": 4329
    },
    {
        "loss": 2.3112,
        "grad_norm": 2.43314790725708,
        "learning_rate": 9.2996473911513e-06,
        "epoch": 0.5773333333333334,
        "step": 4330
    },
    {
        "loss": 3.0473,
        "grad_norm": 2.8371694087982178,
        "learning_rate": 9.273161242595408e-06,
        "epoch": 0.5774666666666667,
        "step": 4331
    },
    {
        "loss": 2.0545,
        "grad_norm": 3.562838077545166,
        "learning_rate": 9.246711031249521e-06,
        "epoch": 0.5776,
        "step": 4332
    },
    {
        "loss": 2.7096,
        "grad_norm": 3.415151834487915,
        "learning_rate": 9.220296767590586e-06,
        "epoch": 0.5777333333333333,
        "step": 4333
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.698071241378784,
        "learning_rate": 9.193918462081453e-06,
        "epoch": 0.5778666666666666,
        "step": 4334
    },
    {
        "loss": 1.4649,
        "grad_norm": 4.1243391036987305,
        "learning_rate": 9.167576125170607e-06,
        "epoch": 0.578,
        "step": 4335
    },
    {
        "loss": 1.7707,
        "grad_norm": 4.955281734466553,
        "learning_rate": 9.141269767292393e-06,
        "epoch": 0.5781333333333334,
        "step": 4336
    },
    {
        "loss": 2.2354,
        "grad_norm": 3.165762424468994,
        "learning_rate": 9.114999398866775e-06,
        "epoch": 0.5782666666666667,
        "step": 4337
    },
    {
        "loss": 1.9284,
        "grad_norm": 4.486746788024902,
        "learning_rate": 9.088765030299628e-06,
        "epoch": 0.5784,
        "step": 4338
    },
    {
        "loss": 2.7023,
        "grad_norm": 2.3316564559936523,
        "learning_rate": 9.062566671982398e-06,
        "epoch": 0.5785333333333333,
        "step": 4339
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.924839973449707,
        "learning_rate": 9.036404334292393e-06,
        "epoch": 0.5786666666666667,
        "step": 4340
    },
    {
        "loss": 1.6159,
        "grad_norm": 3.218230962753296,
        "learning_rate": 9.010278027592556e-06,
        "epoch": 0.5788,
        "step": 4341
    },
    {
        "loss": 2.8619,
        "grad_norm": 2.160327196121216,
        "learning_rate": 8.984187762231645e-06,
        "epoch": 0.5789333333333333,
        "step": 4342
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.807684302330017,
        "learning_rate": 8.958133548544056e-06,
        "epoch": 0.5790666666666666,
        "step": 4343
    },
    {
        "loss": 2.6416,
        "grad_norm": 2.7477638721466064,
        "learning_rate": 8.932115396850016e-06,
        "epoch": 0.5792,
        "step": 4344
    },
    {
        "loss": 2.6987,
        "grad_norm": 3.5603392124176025,
        "learning_rate": 8.906133317455378e-06,
        "epoch": 0.5793333333333334,
        "step": 4345
    },
    {
        "loss": 1.564,
        "grad_norm": 4.041427135467529,
        "learning_rate": 8.880187320651711e-06,
        "epoch": 0.5794666666666667,
        "step": 4346
    },
    {
        "loss": 2.5927,
        "grad_norm": 2.8283162117004395,
        "learning_rate": 8.85427741671635e-06,
        "epoch": 0.5796,
        "step": 4347
    },
    {
        "loss": 2.1522,
        "grad_norm": 2.3359522819519043,
        "learning_rate": 8.828403615912262e-06,
        "epoch": 0.5797333333333333,
        "step": 4348
    },
    {
        "loss": 2.0412,
        "grad_norm": 2.260727643966675,
        "learning_rate": 8.802565928488183e-06,
        "epoch": 0.5798666666666666,
        "step": 4349
    },
    {
        "loss": 1.7441,
        "grad_norm": 2.5417873859405518,
        "learning_rate": 8.776764364678458e-06,
        "epoch": 0.58,
        "step": 4350
    },
    {
        "loss": 1.5784,
        "grad_norm": 6.020577907562256,
        "learning_rate": 8.750998934703225e-06,
        "epoch": 0.5801333333333333,
        "step": 4351
    },
    {
        "loss": 1.1381,
        "grad_norm": 3.746049165725708,
        "learning_rate": 8.72526964876822e-06,
        "epoch": 0.5802666666666667,
        "step": 4352
    },
    {
        "loss": 2.4004,
        "grad_norm": 2.129605770111084,
        "learning_rate": 8.699576517064922e-06,
        "epoch": 0.5804,
        "step": 4353
    },
    {
        "loss": 2.9297,
        "grad_norm": 3.0242021083831787,
        "learning_rate": 8.67391954977046e-06,
        "epoch": 0.5805333333333333,
        "step": 4354
    },
    {
        "loss": 2.6462,
        "grad_norm": 3.3107898235321045,
        "learning_rate": 8.648298757047668e-06,
        "epoch": 0.5806666666666667,
        "step": 4355
    },
    {
        "loss": 3.1484,
        "grad_norm": 2.4043376445770264,
        "learning_rate": 8.622714149044998e-06,
        "epoch": 0.5808,
        "step": 4356
    },
    {
        "loss": 2.3444,
        "grad_norm": 2.8046305179595947,
        "learning_rate": 8.59716573589664e-06,
        "epoch": 0.5809333333333333,
        "step": 4357
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.793294906616211,
        "learning_rate": 8.571653527722356e-06,
        "epoch": 0.5810666666666666,
        "step": 4358
    },
    {
        "loss": 2.6278,
        "grad_norm": 2.4959609508514404,
        "learning_rate": 8.546177534627642e-06,
        "epoch": 0.5812,
        "step": 4359
    },
    {
        "loss": 2.3824,
        "grad_norm": 2.223799467086792,
        "learning_rate": 8.520737766703623e-06,
        "epoch": 0.5813333333333334,
        "step": 4360
    },
    {
        "loss": 1.5339,
        "grad_norm": 3.6176888942718506,
        "learning_rate": 8.495334234027109e-06,
        "epoch": 0.5814666666666667,
        "step": 4361
    },
    {
        "loss": 2.6212,
        "grad_norm": 5.4857001304626465,
        "learning_rate": 8.469966946660457e-06,
        "epoch": 0.5816,
        "step": 4362
    },
    {
        "loss": 2.7644,
        "grad_norm": 2.892378568649292,
        "learning_rate": 8.444635914651799e-06,
        "epoch": 0.5817333333333333,
        "step": 4363
    },
    {
        "loss": 2.4716,
        "grad_norm": 3.333608865737915,
        "learning_rate": 8.419341148034853e-06,
        "epoch": 0.5818666666666666,
        "step": 4364
    },
    {
        "loss": 2.1526,
        "grad_norm": 4.022854804992676,
        "learning_rate": 8.394082656828906e-06,
        "epoch": 0.582,
        "step": 4365
    },
    {
        "loss": 2.9719,
        "grad_norm": 2.346336841583252,
        "learning_rate": 8.368860451038962e-06,
        "epoch": 0.5821333333333333,
        "step": 4366
    },
    {
        "loss": 2.9435,
        "grad_norm": 3.3779261112213135,
        "learning_rate": 8.343674540655611e-06,
        "epoch": 0.5822666666666667,
        "step": 4367
    },
    {
        "loss": 2.5686,
        "grad_norm": 3.766247034072876,
        "learning_rate": 8.318524935655081e-06,
        "epoch": 0.5824,
        "step": 4368
    },
    {
        "loss": 2.2302,
        "grad_norm": 3.6715545654296875,
        "learning_rate": 8.293411645999228e-06,
        "epoch": 0.5825333333333333,
        "step": 4369
    },
    {
        "loss": 2.8588,
        "grad_norm": 3.3619327545166016,
        "learning_rate": 8.268334681635515e-06,
        "epoch": 0.5826666666666667,
        "step": 4370
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.1352968215942383,
        "learning_rate": 8.243294052496986e-06,
        "epoch": 0.5828,
        "step": 4371
    },
    {
        "loss": 2.059,
        "grad_norm": 2.4997236728668213,
        "learning_rate": 8.218289768502363e-06,
        "epoch": 0.5829333333333333,
        "step": 4372
    },
    {
        "loss": 2.966,
        "grad_norm": 3.950871467590332,
        "learning_rate": 8.193321839555868e-06,
        "epoch": 0.5830666666666666,
        "step": 4373
    },
    {
        "loss": 2.937,
        "grad_norm": 3.9485690593719482,
        "learning_rate": 8.168390275547443e-06,
        "epoch": 0.5832,
        "step": 4374
    },
    {
        "loss": 1.6364,
        "grad_norm": 5.604894161224365,
        "learning_rate": 8.143495086352526e-06,
        "epoch": 0.5833333333333334,
        "step": 4375
    },
    {
        "loss": 2.6696,
        "grad_norm": 3.9861085414886475,
        "learning_rate": 8.118636281832204e-06,
        "epoch": 0.5834666666666667,
        "step": 4376
    },
    {
        "loss": 2.1222,
        "grad_norm": 3.0563669204711914,
        "learning_rate": 8.093813871833122e-06,
        "epoch": 0.5836,
        "step": 4377
    },
    {
        "loss": 2.0634,
        "grad_norm": 4.307070255279541,
        "learning_rate": 8.069027866187517e-06,
        "epoch": 0.5837333333333333,
        "step": 4378
    },
    {
        "loss": 0.7194,
        "grad_norm": 2.7481272220611572,
        "learning_rate": 8.044278274713236e-06,
        "epoch": 0.5838666666666666,
        "step": 4379
    },
    {
        "loss": 1.6043,
        "grad_norm": 3.9298057556152344,
        "learning_rate": 8.019565107213667e-06,
        "epoch": 0.584,
        "step": 4380
    },
    {
        "loss": 1.4964,
        "grad_norm": 4.387721061706543,
        "learning_rate": 7.99488837347776e-06,
        "epoch": 0.5841333333333333,
        "step": 4381
    },
    {
        "loss": 2.3193,
        "grad_norm": 3.6398696899414062,
        "learning_rate": 7.970248083280096e-06,
        "epoch": 0.5842666666666667,
        "step": 4382
    },
    {
        "loss": 1.7225,
        "grad_norm": 4.796278476715088,
        "learning_rate": 7.945644246380723e-06,
        "epoch": 0.5844,
        "step": 4383
    },
    {
        "loss": 1.3558,
        "grad_norm": 3.424438238143921,
        "learning_rate": 7.921076872525324e-06,
        "epoch": 0.5845333333333333,
        "step": 4384
    },
    {
        "loss": 1.1848,
        "grad_norm": 3.765237808227539,
        "learning_rate": 7.896545971445146e-06,
        "epoch": 0.5846666666666667,
        "step": 4385
    },
    {
        "loss": 2.3075,
        "grad_norm": 3.3949902057647705,
        "learning_rate": 7.872051552856929e-06,
        "epoch": 0.5848,
        "step": 4386
    },
    {
        "loss": 2.6106,
        "grad_norm": 3.3644111156463623,
        "learning_rate": 7.847593626463012e-06,
        "epoch": 0.5849333333333333,
        "step": 4387
    },
    {
        "loss": 2.147,
        "grad_norm": 3.1031787395477295,
        "learning_rate": 7.823172201951267e-06,
        "epoch": 0.5850666666666666,
        "step": 4388
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.7613627910614014,
        "learning_rate": 7.798787288995124e-06,
        "epoch": 0.5852,
        "step": 4389
    },
    {
        "loss": 2.0201,
        "grad_norm": 2.7381136417388916,
        "learning_rate": 7.774438897253478e-06,
        "epoch": 0.5853333333333334,
        "step": 4390
    },
    {
        "loss": 2.372,
        "grad_norm": 3.121002197265625,
        "learning_rate": 7.750127036370847e-06,
        "epoch": 0.5854666666666667,
        "step": 4391
    },
    {
        "loss": 2.1811,
        "grad_norm": 3.932408571243286,
        "learning_rate": 7.725851715977217e-06,
        "epoch": 0.5856,
        "step": 4392
    },
    {
        "loss": 0.6469,
        "grad_norm": 2.4007532596588135,
        "learning_rate": 7.70161294568813e-06,
        "epoch": 0.5857333333333333,
        "step": 4393
    },
    {
        "loss": 2.4657,
        "grad_norm": 4.546789646148682,
        "learning_rate": 7.677410735104663e-06,
        "epoch": 0.5858666666666666,
        "step": 4394
    },
    {
        "loss": 2.5087,
        "grad_norm": 2.4818062782287598,
        "learning_rate": 7.6532450938134e-06,
        "epoch": 0.586,
        "step": 4395
    },
    {
        "loss": 2.7084,
        "grad_norm": 2.18353533744812,
        "learning_rate": 7.629116031386397e-06,
        "epoch": 0.5861333333333333,
        "step": 4396
    },
    {
        "loss": 2.0562,
        "grad_norm": 2.793462038040161,
        "learning_rate": 7.605023557381308e-06,
        "epoch": 0.5862666666666667,
        "step": 4397
    },
    {
        "loss": 2.4595,
        "grad_norm": 2.832702875137329,
        "learning_rate": 7.580967681341189e-06,
        "epoch": 0.5864,
        "step": 4398
    },
    {
        "loss": 2.6255,
        "grad_norm": 2.482917547225952,
        "learning_rate": 7.556948412794695e-06,
        "epoch": 0.5865333333333334,
        "step": 4399
    },
    {
        "loss": 2.9656,
        "grad_norm": 6.567662715911865,
        "learning_rate": 7.532965761255928e-06,
        "epoch": 0.5866666666666667,
        "step": 4400
    },
    {
        "loss": 1.7412,
        "grad_norm": 1.9406347274780273,
        "learning_rate": 7.5090197362244985e-06,
        "epoch": 0.5868,
        "step": 4401
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.2131857872009277,
        "learning_rate": 7.485110347185498e-06,
        "epoch": 0.5869333333333333,
        "step": 4402
    },
    {
        "loss": 2.5619,
        "grad_norm": 3.370173931121826,
        "learning_rate": 7.461237603609494e-06,
        "epoch": 0.5870666666666666,
        "step": 4403
    },
    {
        "loss": 2.1262,
        "grad_norm": 2.9614651203155518,
        "learning_rate": 7.4374015149526464e-06,
        "epoch": 0.5872,
        "step": 4404
    },
    {
        "loss": 2.0683,
        "grad_norm": 3.2589447498321533,
        "learning_rate": 7.4136020906564245e-06,
        "epoch": 0.5873333333333334,
        "step": 4405
    },
    {
        "loss": 2.2981,
        "grad_norm": 3.781811237335205,
        "learning_rate": 7.389839340147919e-06,
        "epoch": 0.5874666666666667,
        "step": 4406
    },
    {
        "loss": 1.0098,
        "grad_norm": 5.273495197296143,
        "learning_rate": 7.366113272839603e-06,
        "epoch": 0.5876,
        "step": 4407
    },
    {
        "loss": 2.5439,
        "grad_norm": 2.764159917831421,
        "learning_rate": 7.342423898129491e-06,
        "epoch": 0.5877333333333333,
        "step": 4408
    },
    {
        "loss": 2.2753,
        "grad_norm": 2.7812306880950928,
        "learning_rate": 7.318771225400944e-06,
        "epoch": 0.5878666666666666,
        "step": 4409
    },
    {
        "loss": 1.9766,
        "grad_norm": 2.876352548599243,
        "learning_rate": 7.295155264022946e-06,
        "epoch": 0.588,
        "step": 4410
    },
    {
        "loss": 2.7849,
        "grad_norm": 2.12306809425354,
        "learning_rate": 7.2715760233498245e-06,
        "epoch": 0.5881333333333333,
        "step": 4411
    },
    {
        "loss": 2.1911,
        "grad_norm": 3.708085536956787,
        "learning_rate": 7.248033512721408e-06,
        "epoch": 0.5882666666666667,
        "step": 4412
    },
    {
        "loss": 2.7258,
        "grad_norm": 1.9500871896743774,
        "learning_rate": 7.224527741462939e-06,
        "epoch": 0.5884,
        "step": 4413
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.5973737239837646,
        "learning_rate": 7.201058718885212e-06,
        "epoch": 0.5885333333333334,
        "step": 4414
    },
    {
        "loss": 2.4026,
        "grad_norm": 5.392193794250488,
        "learning_rate": 7.1776264542842805e-06,
        "epoch": 0.5886666666666667,
        "step": 4415
    },
    {
        "loss": 1.1648,
        "grad_norm": 2.8382489681243896,
        "learning_rate": 7.154230956941832e-06,
        "epoch": 0.5888,
        "step": 4416
    },
    {
        "loss": 2.4138,
        "grad_norm": 4.417716026306152,
        "learning_rate": 7.13087223612483e-06,
        "epoch": 0.5889333333333333,
        "step": 4417
    },
    {
        "loss": 1.9637,
        "grad_norm": 3.738492488861084,
        "learning_rate": 7.107550301085786e-06,
        "epoch": 0.5890666666666666,
        "step": 4418
    },
    {
        "loss": 1.6292,
        "grad_norm": 5.52825927734375,
        "learning_rate": 7.084265161062598e-06,
        "epoch": 0.5892,
        "step": 4419
    },
    {
        "loss": 2.7681,
        "grad_norm": 2.855923891067505,
        "learning_rate": 7.061016825278588e-06,
        "epoch": 0.5893333333333334,
        "step": 4420
    },
    {
        "loss": 3.2867,
        "grad_norm": 2.1698756217956543,
        "learning_rate": 7.037805302942491e-06,
        "epoch": 0.5894666666666667,
        "step": 4421
    },
    {
        "loss": 1.7996,
        "grad_norm": 3.0267722606658936,
        "learning_rate": 7.014630603248485e-06,
        "epoch": 0.5896,
        "step": 4422
    },
    {
        "loss": 1.9099,
        "grad_norm": 2.9674534797668457,
        "learning_rate": 6.991492735376115e-06,
        "epoch": 0.5897333333333333,
        "step": 4423
    },
    {
        "loss": 2.0553,
        "grad_norm": 2.64367413520813,
        "learning_rate": 6.968391708490396e-06,
        "epoch": 0.5898666666666667,
        "step": 4424
    },
    {
        "loss": 1.9296,
        "grad_norm": 4.266141891479492,
        "learning_rate": 6.94532753174173e-06,
        "epoch": 0.59,
        "step": 4425
    },
    {
        "loss": 2.3203,
        "grad_norm": 2.8038108348846436,
        "learning_rate": 6.922300214265898e-06,
        "epoch": 0.5901333333333333,
        "step": 4426
    },
    {
        "loss": 1.8539,
        "grad_norm": 2.17580509185791,
        "learning_rate": 6.899309765184114e-06,
        "epoch": 0.5902666666666667,
        "step": 4427
    },
    {
        "loss": 2.4596,
        "grad_norm": 3.110260009765625,
        "learning_rate": 6.876356193602951e-06,
        "epoch": 0.5904,
        "step": 4428
    },
    {
        "loss": 2.6679,
        "grad_norm": 3.37589955329895,
        "learning_rate": 6.853439508614412e-06,
        "epoch": 0.5905333333333334,
        "step": 4429
    },
    {
        "loss": 2.1442,
        "grad_norm": 2.8893396854400635,
        "learning_rate": 6.830559719295871e-06,
        "epoch": 0.5906666666666667,
        "step": 4430
    },
    {
        "loss": 1.7598,
        "grad_norm": 4.05450963973999,
        "learning_rate": 6.807716834710109e-06,
        "epoch": 0.5908,
        "step": 4431
    },
    {
        "loss": 0.7625,
        "grad_norm": 2.6037421226501465,
        "learning_rate": 6.784910863905247e-06,
        "epoch": 0.5909333333333333,
        "step": 4432
    },
    {
        "loss": 2.6901,
        "grad_norm": 3.0075430870056152,
        "learning_rate": 6.762141815914835e-06,
        "epoch": 0.5910666666666666,
        "step": 4433
    },
    {
        "loss": 2.7849,
        "grad_norm": 2.1246824264526367,
        "learning_rate": 6.739409699757704e-06,
        "epoch": 0.5912,
        "step": 4434
    },
    {
        "loss": 1.1346,
        "grad_norm": 3.6944427490234375,
        "learning_rate": 6.716714524438206e-06,
        "epoch": 0.5913333333333334,
        "step": 4435
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.6017980575561523,
        "learning_rate": 6.694056298945917e-06,
        "epoch": 0.5914666666666667,
        "step": 4436
    },
    {
        "loss": 2.4827,
        "grad_norm": 3.1512327194213867,
        "learning_rate": 6.6714350322558796e-06,
        "epoch": 0.5916,
        "step": 4437
    },
    {
        "loss": 1.6017,
        "grad_norm": 3.770418167114258,
        "learning_rate": 6.648850733328394e-06,
        "epoch": 0.5917333333333333,
        "step": 4438
    },
    {
        "loss": 2.3288,
        "grad_norm": 3.112030029296875,
        "learning_rate": 6.62630341110928e-06,
        "epoch": 0.5918666666666667,
        "step": 4439
    },
    {
        "loss": 1.9529,
        "grad_norm": 3.403965711593628,
        "learning_rate": 6.603793074529508e-06,
        "epoch": 0.592,
        "step": 4440
    },
    {
        "loss": 1.8147,
        "grad_norm": 6.4810967445373535,
        "learning_rate": 6.581319732505553e-06,
        "epoch": 0.5921333333333333,
        "step": 4441
    },
    {
        "loss": 3.0811,
        "grad_norm": 2.336202621459961,
        "learning_rate": 6.558883393939163e-06,
        "epoch": 0.5922666666666667,
        "step": 4442
    },
    {
        "loss": 2.2627,
        "grad_norm": 2.543919086456299,
        "learning_rate": 6.536484067717452e-06,
        "epoch": 0.5924,
        "step": 4443
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.8766915798187256,
        "learning_rate": 6.514121762712866e-06,
        "epoch": 0.5925333333333334,
        "step": 4444
    },
    {
        "loss": 2.3892,
        "grad_norm": 2.3730599880218506,
        "learning_rate": 6.491796487783186e-06,
        "epoch": 0.5926666666666667,
        "step": 4445
    },
    {
        "loss": 2.1865,
        "grad_norm": 2.4450159072875977,
        "learning_rate": 6.4695082517715725e-06,
        "epoch": 0.5928,
        "step": 4446
    },
    {
        "loss": 1.6924,
        "grad_norm": 5.0807204246521,
        "learning_rate": 6.447257063506407e-06,
        "epoch": 0.5929333333333333,
        "step": 4447
    },
    {
        "loss": 1.454,
        "grad_norm": 6.159759998321533,
        "learning_rate": 6.425042931801517e-06,
        "epoch": 0.5930666666666666,
        "step": 4448
    },
    {
        "loss": 1.3402,
        "grad_norm": 2.990985631942749,
        "learning_rate": 6.402865865455954e-06,
        "epoch": 0.5932,
        "step": 4449
    },
    {
        "loss": 2.4556,
        "grad_norm": 2.1754047870635986,
        "learning_rate": 6.380725873254156e-06,
        "epoch": 0.5933333333333334,
        "step": 4450
    },
    {
        "loss": 1.3317,
        "grad_norm": 2.470033645629883,
        "learning_rate": 6.358622963965833e-06,
        "epoch": 0.5934666666666667,
        "step": 4451
    },
    {
        "loss": 2.5497,
        "grad_norm": 2.3354501724243164,
        "learning_rate": 6.336557146346034e-06,
        "epoch": 0.5936,
        "step": 4452
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.45371675491333,
        "learning_rate": 6.3145284291350915e-06,
        "epoch": 0.5937333333333333,
        "step": 4453
    },
    {
        "loss": 3.0493,
        "grad_norm": 3.2539753913879395,
        "learning_rate": 6.292536821058659e-06,
        "epoch": 0.5938666666666667,
        "step": 4454
    },
    {
        "loss": 2.3862,
        "grad_norm": 3.16595458984375,
        "learning_rate": 6.270582330827701e-06,
        "epoch": 0.594,
        "step": 4455
    },
    {
        "loss": 2.7469,
        "grad_norm": 2.161400318145752,
        "learning_rate": 6.2486649671384624e-06,
        "epoch": 0.5941333333333333,
        "step": 4456
    },
    {
        "loss": 2.4233,
        "grad_norm": 2.5260720252990723,
        "learning_rate": 6.226784738672464e-06,
        "epoch": 0.5942666666666667,
        "step": 4457
    },
    {
        "loss": 1.8762,
        "grad_norm": 3.286806583404541,
        "learning_rate": 6.204941654096597e-06,
        "epoch": 0.5944,
        "step": 4458
    },
    {
        "loss": 2.9132,
        "grad_norm": 2.160102367401123,
        "learning_rate": 6.183135722062894e-06,
        "epoch": 0.5945333333333334,
        "step": 4459
    },
    {
        "loss": 2.4553,
        "grad_norm": 3.454497814178467,
        "learning_rate": 6.161366951208813e-06,
        "epoch": 0.5946666666666667,
        "step": 4460
    },
    {
        "loss": 2.2471,
        "grad_norm": 5.605955600738525,
        "learning_rate": 6.139635350157036e-06,
        "epoch": 0.5948,
        "step": 4461
    },
    {
        "loss": 1.8588,
        "grad_norm": 3.734647274017334,
        "learning_rate": 6.11794092751552e-06,
        "epoch": 0.5949333333333333,
        "step": 4462
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.8800644874572754,
        "learning_rate": 6.09628369187748e-06,
        "epoch": 0.5950666666666666,
        "step": 4463
    },
    {
        "loss": 2.6316,
        "grad_norm": 2.5855157375335693,
        "learning_rate": 6.074663651821444e-06,
        "epoch": 0.5952,
        "step": 4464
    },
    {
        "loss": 2.6329,
        "grad_norm": 2.4797160625457764,
        "learning_rate": 6.053080815911216e-06,
        "epoch": 0.5953333333333334,
        "step": 4465
    },
    {
        "loss": 1.9559,
        "grad_norm": 4.667211532592773,
        "learning_rate": 6.031535192695758e-06,
        "epoch": 0.5954666666666667,
        "step": 4466
    },
    {
        "loss": 2.0979,
        "grad_norm": 4.213087558746338,
        "learning_rate": 6.010026790709422e-06,
        "epoch": 0.5956,
        "step": 4467
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.9715492725372314,
        "learning_rate": 5.988555618471714e-06,
        "epoch": 0.5957333333333333,
        "step": 4468
    },
    {
        "loss": 2.6469,
        "grad_norm": 3.213083267211914,
        "learning_rate": 5.967121684487464e-06,
        "epoch": 0.5958666666666667,
        "step": 4469
    },
    {
        "loss": 1.9981,
        "grad_norm": 3.0881528854370117,
        "learning_rate": 5.945724997246727e-06,
        "epoch": 0.596,
        "step": 4470
    },
    {
        "loss": 2.7983,
        "grad_norm": 3.5614919662475586,
        "learning_rate": 5.92436556522481e-06,
        "epoch": 0.5961333333333333,
        "step": 4471
    },
    {
        "loss": 2.1713,
        "grad_norm": 2.985250234603882,
        "learning_rate": 5.903043396882235e-06,
        "epoch": 0.5962666666666666,
        "step": 4472
    },
    {
        "loss": 2.0275,
        "grad_norm": 2.9702067375183105,
        "learning_rate": 5.881758500664813e-06,
        "epoch": 0.5964,
        "step": 4473
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.9517217874526978,
        "learning_rate": 5.860510885003534e-06,
        "epoch": 0.5965333333333334,
        "step": 4474
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.4141643047332764,
        "learning_rate": 5.839300558314686e-06,
        "epoch": 0.5966666666666667,
        "step": 4475
    },
    {
        "loss": 2.5345,
        "grad_norm": 4.571232795715332,
        "learning_rate": 5.8181275289997066e-06,
        "epoch": 0.5968,
        "step": 4476
    },
    {
        "loss": 2.0327,
        "grad_norm": 4.462876796722412,
        "learning_rate": 5.796991805445351e-06,
        "epoch": 0.5969333333333333,
        "step": 4477
    },
    {
        "loss": 2.9577,
        "grad_norm": 3.423161745071411,
        "learning_rate": 5.775893396023513e-06,
        "epoch": 0.5970666666666666,
        "step": 4478
    },
    {
        "loss": 2.7955,
        "grad_norm": 2.4151060581207275,
        "learning_rate": 5.754832309091362e-06,
        "epoch": 0.5972,
        "step": 4479
    },
    {
        "loss": 1.1294,
        "grad_norm": 3.655263662338257,
        "learning_rate": 5.733808552991271e-06,
        "epoch": 0.5973333333333334,
        "step": 4480
    },
    {
        "loss": 0.9339,
        "grad_norm": 4.271159648895264,
        "learning_rate": 5.7128221360508326e-06,
        "epoch": 0.5974666666666667,
        "step": 4481
    },
    {
        "loss": 0.5203,
        "grad_norm": 2.060920476913452,
        "learning_rate": 5.691873066582798e-06,
        "epoch": 0.5976,
        "step": 4482
    },
    {
        "loss": 1.6607,
        "grad_norm": 3.829472064971924,
        "learning_rate": 5.670961352885218e-06,
        "epoch": 0.5977333333333333,
        "step": 4483
    },
    {
        "loss": 2.4143,
        "grad_norm": 2.2994751930236816,
        "learning_rate": 5.650087003241244e-06,
        "epoch": 0.5978666666666667,
        "step": 4484
    },
    {
        "loss": 1.9359,
        "grad_norm": 2.764758348464966,
        "learning_rate": 5.629250025919275e-06,
        "epoch": 0.598,
        "step": 4485
    },
    {
        "loss": 2.1249,
        "grad_norm": 3.195152521133423,
        "learning_rate": 5.608450429172963e-06,
        "epoch": 0.5981333333333333,
        "step": 4486
    },
    {
        "loss": 2.2227,
        "grad_norm": 2.4473681449890137,
        "learning_rate": 5.58768822124105e-06,
        "epoch": 0.5982666666666666,
        "step": 4487
    },
    {
        "loss": 3.061,
        "grad_norm": 3.231264591217041,
        "learning_rate": 5.566963410347536e-06,
        "epoch": 0.5984,
        "step": 4488
    },
    {
        "loss": 1.9235,
        "grad_norm": 4.211774826049805,
        "learning_rate": 5.546276004701589e-06,
        "epoch": 0.5985333333333334,
        "step": 4489
    },
    {
        "loss": 1.435,
        "grad_norm": 3.7244491577148438,
        "learning_rate": 5.525626012497587e-06,
        "epoch": 0.5986666666666667,
        "step": 4490
    },
    {
        "loss": 2.3587,
        "grad_norm": 3.2643496990203857,
        "learning_rate": 5.505013441915008e-06,
        "epoch": 0.5988,
        "step": 4491
    },
    {
        "loss": 1.4205,
        "grad_norm": 3.1573660373687744,
        "learning_rate": 5.48443830111861e-06,
        "epoch": 0.5989333333333333,
        "step": 4492
    },
    {
        "loss": 2.4146,
        "grad_norm": 1.7658451795578003,
        "learning_rate": 5.463900598258232e-06,
        "epoch": 0.5990666666666666,
        "step": 4493
    },
    {
        "loss": 2.3804,
        "grad_norm": 3.9525582790374756,
        "learning_rate": 5.443400341468952e-06,
        "epoch": 0.5992,
        "step": 4494
    },
    {
        "loss": 2.5878,
        "grad_norm": 3.018054723739624,
        "learning_rate": 5.422937538870987e-06,
        "epoch": 0.5993333333333334,
        "step": 4495
    },
    {
        "loss": 2.4061,
        "grad_norm": 2.387360095977783,
        "learning_rate": 5.402512198569742e-06,
        "epoch": 0.5994666666666667,
        "step": 4496
    },
    {
        "loss": 3.3792,
        "grad_norm": 4.25956916809082,
        "learning_rate": 5.3821243286557355e-06,
        "epoch": 0.5996,
        "step": 4497
    },
    {
        "loss": 2.3667,
        "grad_norm": 2.7190310955047607,
        "learning_rate": 5.361773937204706e-06,
        "epoch": 0.5997333333333333,
        "step": 4498
    },
    {
        "loss": 2.1529,
        "grad_norm": 2.9866154193878174,
        "learning_rate": 5.3414610322774724e-06,
        "epoch": 0.5998666666666667,
        "step": 4499
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.9398117065429688,
        "learning_rate": 5.321185621920077e-06,
        "epoch": 0.6,
        "step": 4500
    },
    {
        "loss": 2.5362,
        "grad_norm": 2.6100730895996094,
        "learning_rate": 5.300947714163651e-06,
        "epoch": 0.6001333333333333,
        "step": 4501
    },
    {
        "loss": 3.0904,
        "grad_norm": 2.5843236446380615,
        "learning_rate": 5.2807473170245365e-06,
        "epoch": 0.6002666666666666,
        "step": 4502
    },
    {
        "loss": 2.5573,
        "grad_norm": 2.550739288330078,
        "learning_rate": 5.260584438504135e-06,
        "epoch": 0.6004,
        "step": 4503
    },
    {
        "loss": 2.3189,
        "grad_norm": 2.839538097381592,
        "learning_rate": 5.240459086589045e-06,
        "epoch": 0.6005333333333334,
        "step": 4504
    },
    {
        "loss": 2.004,
        "grad_norm": 3.554260730743408,
        "learning_rate": 5.220371269251023e-06,
        "epoch": 0.6006666666666667,
        "step": 4505
    },
    {
        "loss": 2.1251,
        "grad_norm": 2.8041722774505615,
        "learning_rate": 5.200320994446883e-06,
        "epoch": 0.6008,
        "step": 4506
    },
    {
        "loss": 1.7053,
        "grad_norm": 2.9693076610565186,
        "learning_rate": 5.180308270118628e-06,
        "epoch": 0.6009333333333333,
        "step": 4507
    },
    {
        "loss": 1.02,
        "grad_norm": 4.998657703399658,
        "learning_rate": 5.160333104193349e-06,
        "epoch": 0.6010666666666666,
        "step": 4508
    },
    {
        "loss": 2.761,
        "grad_norm": 2.878296136856079,
        "learning_rate": 5.1403955045833064e-06,
        "epoch": 0.6012,
        "step": 4509
    },
    {
        "loss": 2.6125,
        "grad_norm": 5.331050872802734,
        "learning_rate": 5.120495479185794e-06,
        "epoch": 0.6013333333333334,
        "step": 4510
    },
    {
        "loss": 2.6653,
        "grad_norm": 3.2507824897766113,
        "learning_rate": 5.10063303588334e-06,
        "epoch": 0.6014666666666667,
        "step": 4511
    },
    {
        "loss": 2.7813,
        "grad_norm": 1.766020655632019,
        "learning_rate": 5.080808182543495e-06,
        "epoch": 0.6016,
        "step": 4512
    },
    {
        "loss": 3.0489,
        "grad_norm": 2.2688350677490234,
        "learning_rate": 5.0610209270189665e-06,
        "epoch": 0.6017333333333333,
        "step": 4513
    },
    {
        "loss": 2.6053,
        "grad_norm": 1.885440468788147,
        "learning_rate": 5.041271277147519e-06,
        "epoch": 0.6018666666666667,
        "step": 4514
    },
    {
        "loss": 2.0387,
        "grad_norm": 2.876302480697632,
        "learning_rate": 5.021559240752127e-06,
        "epoch": 0.602,
        "step": 4515
    },
    {
        "loss": 2.8551,
        "grad_norm": 3.8331429958343506,
        "learning_rate": 5.0018848256407235e-06,
        "epoch": 0.6021333333333333,
        "step": 4516
    },
    {
        "loss": 2.2005,
        "grad_norm": 3.03226375579834,
        "learning_rate": 4.982248039606441e-06,
        "epoch": 0.6022666666666666,
        "step": 4517
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.153033971786499,
        "learning_rate": 4.9626488904274796e-06,
        "epoch": 0.6024,
        "step": 4518
    },
    {
        "loss": 2.6101,
        "grad_norm": 2.677269697189331,
        "learning_rate": 4.943087385867106e-06,
        "epoch": 0.6025333333333334,
        "step": 4519
    },
    {
        "loss": 2.1387,
        "grad_norm": 2.4124627113342285,
        "learning_rate": 4.923563533673736e-06,
        "epoch": 0.6026666666666667,
        "step": 4520
    },
    {
        "loss": 2.3543,
        "grad_norm": 3.334533214569092,
        "learning_rate": 4.904077341580815e-06,
        "epoch": 0.6028,
        "step": 4521
    },
    {
        "loss": 1.779,
        "grad_norm": 2.106677770614624,
        "learning_rate": 4.88462881730688e-06,
        "epoch": 0.6029333333333333,
        "step": 4522
    },
    {
        "loss": 2.6092,
        "grad_norm": 2.6833977699279785,
        "learning_rate": 4.8652179685555795e-06,
        "epoch": 0.6030666666666666,
        "step": 4523
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.1152303218841553,
        "learning_rate": 4.8458448030155975e-06,
        "epoch": 0.6032,
        "step": 4524
    },
    {
        "loss": 2.5518,
        "grad_norm": 1.6232342720031738,
        "learning_rate": 4.826509328360718e-06,
        "epoch": 0.6033333333333334,
        "step": 4525
    },
    {
        "loss": 2.4158,
        "grad_norm": 4.091209888458252,
        "learning_rate": 4.807211552249802e-06,
        "epoch": 0.6034666666666667,
        "step": 4526
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.569795608520508,
        "learning_rate": 4.787951482326736e-06,
        "epoch": 0.6036,
        "step": 4527
    },
    {
        "loss": 0.8475,
        "grad_norm": 4.351698875427246,
        "learning_rate": 4.768729126220528e-06,
        "epoch": 0.6037333333333333,
        "step": 4528
    },
    {
        "loss": 2.4971,
        "grad_norm": 3.822704792022705,
        "learning_rate": 4.749544491545199e-06,
        "epoch": 0.6038666666666667,
        "step": 4529
    },
    {
        "loss": 1.6622,
        "grad_norm": 2.8750107288360596,
        "learning_rate": 4.7303975858998485e-06,
        "epoch": 0.604,
        "step": 4530
    },
    {
        "loss": 1.9293,
        "grad_norm": 4.001433849334717,
        "learning_rate": 4.711288416868642e-06,
        "epoch": 0.6041333333333333,
        "step": 4531
    },
    {
        "loss": 2.9116,
        "grad_norm": 1.8916022777557373,
        "learning_rate": 4.6922169920207925e-06,
        "epoch": 0.6042666666666666,
        "step": 4532
    },
    {
        "loss": 3.0983,
        "grad_norm": 2.2120020389556885,
        "learning_rate": 4.673183318910534e-06,
        "epoch": 0.6044,
        "step": 4533
    },
    {
        "loss": 1.3071,
        "grad_norm": 2.2662086486816406,
        "learning_rate": 4.654187405077204e-06,
        "epoch": 0.6045333333333334,
        "step": 4534
    },
    {
        "loss": 0.9723,
        "grad_norm": 2.7305824756622314,
        "learning_rate": 4.635229258045082e-06,
        "epoch": 0.6046666666666667,
        "step": 4535
    },
    {
        "loss": 2.9236,
        "grad_norm": 2.822535514831543,
        "learning_rate": 4.6163088853236395e-06,
        "epoch": 0.6048,
        "step": 4536
    },
    {
        "loss": 2.2742,
        "grad_norm": 4.223981857299805,
        "learning_rate": 4.597426294407237e-06,
        "epoch": 0.6049333333333333,
        "step": 4537
    },
    {
        "loss": 2.0486,
        "grad_norm": 3.8282692432403564,
        "learning_rate": 4.578581492775369e-06,
        "epoch": 0.6050666666666666,
        "step": 4538
    },
    {
        "loss": 1.5705,
        "grad_norm": 3.1001594066619873,
        "learning_rate": 4.5597744878924765e-06,
        "epoch": 0.6052,
        "step": 4539
    },
    {
        "loss": 2.0919,
        "grad_norm": 3.5817618370056152,
        "learning_rate": 4.5410052872081664e-06,
        "epoch": 0.6053333333333333,
        "step": 4540
    },
    {
        "loss": 2.5821,
        "grad_norm": 1.7613632678985596,
        "learning_rate": 4.522273898156903e-06,
        "epoch": 0.6054666666666667,
        "step": 4541
    },
    {
        "loss": 2.2144,
        "grad_norm": 3.2206575870513916,
        "learning_rate": 4.503580328158297e-06,
        "epoch": 0.6056,
        "step": 4542
    },
    {
        "loss": 2.8813,
        "grad_norm": 2.312556028366089,
        "learning_rate": 4.484924584616901e-06,
        "epoch": 0.6057333333333333,
        "step": 4543
    },
    {
        "loss": 2.8966,
        "grad_norm": 3.114655017852783,
        "learning_rate": 4.466306674922327e-06,
        "epoch": 0.6058666666666667,
        "step": 4544
    },
    {
        "loss": 2.8607,
        "grad_norm": 2.2554733753204346,
        "learning_rate": 4.4477266064492205e-06,
        "epoch": 0.606,
        "step": 4545
    },
    {
        "loss": 2.8383,
        "grad_norm": 3.2338743209838867,
        "learning_rate": 4.429184386557184e-06,
        "epoch": 0.6061333333333333,
        "step": 4546
    },
    {
        "loss": 2.6069,
        "grad_norm": 2.4502954483032227,
        "learning_rate": 4.410680022590863e-06,
        "epoch": 0.6062666666666666,
        "step": 4547
    },
    {
        "loss": 1.9527,
        "grad_norm": 2.999981641769409,
        "learning_rate": 4.392213521879896e-06,
        "epoch": 0.6064,
        "step": 4548
    },
    {
        "loss": 2.5811,
        "grad_norm": 2.8338563442230225,
        "learning_rate": 4.373784891738941e-06,
        "epoch": 0.6065333333333334,
        "step": 4549
    },
    {
        "loss": 2.52,
        "grad_norm": 3.2140822410583496,
        "learning_rate": 4.355394139467617e-06,
        "epoch": 0.6066666666666667,
        "step": 4550
    },
    {
        "loss": 1.8107,
        "grad_norm": 1.9397495985031128,
        "learning_rate": 4.3370412723505835e-06,
        "epoch": 0.6068,
        "step": 4551
    },
    {
        "loss": 2.268,
        "grad_norm": 3.5667624473571777,
        "learning_rate": 4.318726297657461e-06,
        "epoch": 0.6069333333333333,
        "step": 4552
    },
    {
        "loss": 1.9131,
        "grad_norm": 2.9745283126831055,
        "learning_rate": 4.300449222642888e-06,
        "epoch": 0.6070666666666666,
        "step": 4553
    },
    {
        "loss": 2.6209,
        "grad_norm": 1.3848227262496948,
        "learning_rate": 4.282210054546454e-06,
        "epoch": 0.6072,
        "step": 4554
    },
    {
        "loss": 2.7971,
        "grad_norm": 3.0241637229919434,
        "learning_rate": 4.264008800592756e-06,
        "epoch": 0.6073333333333333,
        "step": 4555
    },
    {
        "loss": 2.2083,
        "grad_norm": 3.7614800930023193,
        "learning_rate": 4.2458454679913936e-06,
        "epoch": 0.6074666666666667,
        "step": 4556
    },
    {
        "loss": 2.7838,
        "grad_norm": 1.8810402154922485,
        "learning_rate": 4.227720063936935e-06,
        "epoch": 0.6076,
        "step": 4557
    },
    {
        "loss": 2.6982,
        "grad_norm": 2.215221643447876,
        "learning_rate": 4.20963259560887e-06,
        "epoch": 0.6077333333333333,
        "step": 4558
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.0963265895843506,
        "learning_rate": 4.191583070171768e-06,
        "epoch": 0.6078666666666667,
        "step": 4559
    },
    {
        "loss": 2.16,
        "grad_norm": 2.1760969161987305,
        "learning_rate": 4.17357149477503e-06,
        "epoch": 0.608,
        "step": 4560
    },
    {
        "loss": 2.04,
        "grad_norm": 4.570574760437012,
        "learning_rate": 4.155597876553163e-06,
        "epoch": 0.6081333333333333,
        "step": 4561
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.6061904430389404,
        "learning_rate": 4.137662222625549e-06,
        "epoch": 0.6082666666666666,
        "step": 4562
    },
    {
        "loss": 2.4852,
        "grad_norm": 2.990117073059082,
        "learning_rate": 4.1197645400965826e-06,
        "epoch": 0.6084,
        "step": 4563
    },
    {
        "loss": 1.5256,
        "grad_norm": 5.246287822723389,
        "learning_rate": 4.101904836055559e-06,
        "epoch": 0.6085333333333334,
        "step": 4564
    },
    {
        "loss": 2.203,
        "grad_norm": 3.6114184856414795,
        "learning_rate": 4.084083117576809e-06,
        "epoch": 0.6086666666666667,
        "step": 4565
    },
    {
        "loss": 2.7693,
        "grad_norm": 3.337764263153076,
        "learning_rate": 4.066299391719597e-06,
        "epoch": 0.6088,
        "step": 4566
    },
    {
        "loss": 1.8596,
        "grad_norm": 3.022061586380005,
        "learning_rate": 4.048553665528043e-06,
        "epoch": 0.6089333333333333,
        "step": 4567
    },
    {
        "loss": 1.9154,
        "grad_norm": 3.231290817260742,
        "learning_rate": 4.030845946031348e-06,
        "epoch": 0.6090666666666666,
        "step": 4568
    },
    {
        "loss": 2.5507,
        "grad_norm": 4.058978080749512,
        "learning_rate": 4.013176240243565e-06,
        "epoch": 0.6092,
        "step": 4569
    },
    {
        "loss": 2.6226,
        "grad_norm": 3.8831353187561035,
        "learning_rate": 3.995544555163744e-06,
        "epoch": 0.6093333333333333,
        "step": 4570
    },
    {
        "loss": 1.8417,
        "grad_norm": 4.776546955108643,
        "learning_rate": 3.977950897775851e-06,
        "epoch": 0.6094666666666667,
        "step": 4571
    },
    {
        "loss": 2.2597,
        "grad_norm": 2.647491455078125,
        "learning_rate": 3.9603952750488e-06,
        "epoch": 0.6096,
        "step": 4572
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.7283575534820557,
        "learning_rate": 3.94287769393642e-06,
        "epoch": 0.6097333333333333,
        "step": 4573
    },
    {
        "loss": 2.7617,
        "grad_norm": 2.283464193344116,
        "learning_rate": 3.925398161377502e-06,
        "epoch": 0.6098666666666667,
        "step": 4574
    },
    {
        "loss": 2.8402,
        "grad_norm": 4.177620887756348,
        "learning_rate": 3.907956684295733e-06,
        "epoch": 0.61,
        "step": 4575
    },
    {
        "loss": 2.0144,
        "grad_norm": 3.0132079124450684,
        "learning_rate": 3.8905532695997614e-06,
        "epoch": 0.6101333333333333,
        "step": 4576
    },
    {
        "loss": 2.0787,
        "grad_norm": 3.5339083671569824,
        "learning_rate": 3.8731879241831085e-06,
        "epoch": 0.6102666666666666,
        "step": 4577
    },
    {
        "loss": 2.4742,
        "grad_norm": 3.231079578399658,
        "learning_rate": 3.85586065492427e-06,
        "epoch": 0.6104,
        "step": 4578
    },
    {
        "loss": 2.8675,
        "grad_norm": 2.863307237625122,
        "learning_rate": 3.838571468686614e-06,
        "epoch": 0.6105333333333334,
        "step": 4579
    },
    {
        "loss": 1.9631,
        "grad_norm": 2.6732914447784424,
        "learning_rate": 3.821320372318471e-06,
        "epoch": 0.6106666666666667,
        "step": 4580
    },
    {
        "loss": 2.6852,
        "grad_norm": 2.4920811653137207,
        "learning_rate": 3.804107372653043e-06,
        "epoch": 0.6108,
        "step": 4581
    },
    {
        "loss": 1.3583,
        "grad_norm": 3.6081199645996094,
        "learning_rate": 3.7869324765084868e-06,
        "epoch": 0.6109333333333333,
        "step": 4582
    },
    {
        "loss": 1.7736,
        "grad_norm": 2.5882444381713867,
        "learning_rate": 3.769795690687794e-06,
        "epoch": 0.6110666666666666,
        "step": 4583
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.8732643127441406,
        "learning_rate": 3.7526970219789238e-06,
        "epoch": 0.6112,
        "step": 4584
    },
    {
        "loss": 2.4134,
        "grad_norm": 3.146777868270874,
        "learning_rate": 3.735636477154736e-06,
        "epoch": 0.6113333333333333,
        "step": 4585
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.506669521331787,
        "learning_rate": 3.7186140629729247e-06,
        "epoch": 0.6114666666666667,
        "step": 4586
    },
    {
        "loss": 2.4509,
        "grad_norm": 3.278102159500122,
        "learning_rate": 3.7016297861761864e-06,
        "epoch": 0.6116,
        "step": 4587
    },
    {
        "loss": 2.6753,
        "grad_norm": 3.034555196762085,
        "learning_rate": 3.6846836534919935e-06,
        "epoch": 0.6117333333333334,
        "step": 4588
    },
    {
        "loss": 1.4653,
        "grad_norm": 2.3787107467651367,
        "learning_rate": 3.6677756716327893e-06,
        "epoch": 0.6118666666666667,
        "step": 4589
    },
    {
        "loss": 0.6498,
        "grad_norm": 2.9107565879821777,
        "learning_rate": 3.6509058472958934e-06,
        "epoch": 0.612,
        "step": 4590
    },
    {
        "loss": 1.5195,
        "grad_norm": 4.493719577789307,
        "learning_rate": 3.6340741871635165e-06,
        "epoch": 0.6121333333333333,
        "step": 4591
    },
    {
        "loss": 2.4007,
        "grad_norm": 4.67194938659668,
        "learning_rate": 3.6172806979026917e-06,
        "epoch": 0.6122666666666666,
        "step": 4592
    },
    {
        "loss": 2.0467,
        "grad_norm": 4.80173921585083,
        "learning_rate": 3.6005253861653985e-06,
        "epoch": 0.6124,
        "step": 4593
    },
    {
        "loss": 1.5608,
        "grad_norm": 3.547788619995117,
        "learning_rate": 3.5838082585884723e-06,
        "epoch": 0.6125333333333334,
        "step": 4594
    },
    {
        "loss": 3.2688,
        "grad_norm": 2.278827667236328,
        "learning_rate": 3.567129321793616e-06,
        "epoch": 0.6126666666666667,
        "step": 4595
    },
    {
        "loss": 2.9002,
        "grad_norm": 3.14021635055542,
        "learning_rate": 3.5504885823874346e-06,
        "epoch": 0.6128,
        "step": 4596
    },
    {
        "loss": 2.3788,
        "grad_norm": 2.578401565551758,
        "learning_rate": 3.5338860469613766e-06,
        "epoch": 0.6129333333333333,
        "step": 4597
    },
    {
        "loss": 2.4241,
        "grad_norm": 2.4019365310668945,
        "learning_rate": 3.5173217220917375e-06,
        "epoch": 0.6130666666666666,
        "step": 4598
    },
    {
        "loss": 2.0514,
        "grad_norm": 3.032440423965454,
        "learning_rate": 3.500795614339736e-06,
        "epoch": 0.6132,
        "step": 4599
    },
    {
        "loss": 2.0251,
        "grad_norm": 2.7753536701202393,
        "learning_rate": 3.4843077302513906e-06,
        "epoch": 0.6133333333333333,
        "step": 4600
    },
    {
        "loss": 2.8042,
        "grad_norm": 2.9909884929656982,
        "learning_rate": 3.4678580763576442e-06,
        "epoch": 0.6134666666666667,
        "step": 4601
    },
    {
        "loss": 2.4642,
        "grad_norm": 1.8280322551727295,
        "learning_rate": 3.451446659174218e-06,
        "epoch": 0.6136,
        "step": 4602
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.912186622619629,
        "learning_rate": 3.4350734852017673e-06,
        "epoch": 0.6137333333333334,
        "step": 4603
    },
    {
        "loss": 2.8819,
        "grad_norm": 3.4401497840881348,
        "learning_rate": 3.418738560925727e-06,
        "epoch": 0.6138666666666667,
        "step": 4604
    },
    {
        "loss": 2.644,
        "grad_norm": 2.7278852462768555,
        "learning_rate": 3.4024418928164438e-06,
        "epoch": 0.614,
        "step": 4605
    },
    {
        "loss": 2.9793,
        "grad_norm": 2.8559532165527344,
        "learning_rate": 3.3861834873290755e-06,
        "epoch": 0.6141333333333333,
        "step": 4606
    },
    {
        "loss": 2.0215,
        "grad_norm": 3.6046266555786133,
        "learning_rate": 3.3699633509036268e-06,
        "epoch": 0.6142666666666666,
        "step": 4607
    },
    {
        "loss": 2.7815,
        "grad_norm": 2.879831075668335,
        "learning_rate": 3.3537814899649578e-06,
        "epoch": 0.6144,
        "step": 4608
    },
    {
        "loss": 2.3199,
        "grad_norm": 2.919188976287842,
        "learning_rate": 3.3376379109227527e-06,
        "epoch": 0.6145333333333334,
        "step": 4609
    },
    {
        "loss": 2.3371,
        "grad_norm": 3.0184195041656494,
        "learning_rate": 3.3215326201715636e-06,
        "epoch": 0.6146666666666667,
        "step": 4610
    },
    {
        "loss": 2.275,
        "grad_norm": 3.114048719406128,
        "learning_rate": 3.305465624090687e-06,
        "epoch": 0.6148,
        "step": 4611
    },
    {
        "loss": 2.2673,
        "grad_norm": 2.4718070030212402,
        "learning_rate": 3.2894369290443873e-06,
        "epoch": 0.6149333333333333,
        "step": 4612
    },
    {
        "loss": 2.3903,
        "grad_norm": 4.330550193786621,
        "learning_rate": 3.2734465413816417e-06,
        "epoch": 0.6150666666666667,
        "step": 4613
    },
    {
        "loss": 2.8591,
        "grad_norm": 2.300475835800171,
        "learning_rate": 3.2574944674363063e-06,
        "epoch": 0.6152,
        "step": 4614
    },
    {
        "loss": 2.9256,
        "grad_norm": 2.1664488315582275,
        "learning_rate": 3.2415807135270483e-06,
        "epoch": 0.6153333333333333,
        "step": 4615
    },
    {
        "loss": 1.9706,
        "grad_norm": 2.8096396923065186,
        "learning_rate": 3.2257052859573923e-06,
        "epoch": 0.6154666666666667,
        "step": 4616
    },
    {
        "loss": 2.8082,
        "grad_norm": 3.425551652908325,
        "learning_rate": 3.209868191015597e-06,
        "epoch": 0.6156,
        "step": 4617
    },
    {
        "loss": 1.7116,
        "grad_norm": 4.291553020477295,
        "learning_rate": 3.194069434974822e-06,
        "epoch": 0.6157333333333334,
        "step": 4618
    },
    {
        "loss": 2.363,
        "grad_norm": 4.4492340087890625,
        "learning_rate": 3.178309024093007e-06,
        "epoch": 0.6158666666666667,
        "step": 4619
    },
    {
        "loss": 0.9276,
        "grad_norm": 2.6498568058013916,
        "learning_rate": 3.1625869646128903e-06,
        "epoch": 0.616,
        "step": 4620
    },
    {
        "loss": 2.4228,
        "grad_norm": 2.8470208644866943,
        "learning_rate": 3.1469032627620354e-06,
        "epoch": 0.6161333333333333,
        "step": 4621
    },
    {
        "loss": 1.9256,
        "grad_norm": 2.9047505855560303,
        "learning_rate": 3.1312579247528393e-06,
        "epoch": 0.6162666666666666,
        "step": 4622
    },
    {
        "loss": 1.9183,
        "grad_norm": 3.5858712196350098,
        "learning_rate": 3.1156509567824345e-06,
        "epoch": 0.6164,
        "step": 4623
    },
    {
        "loss": 2.8445,
        "grad_norm": 1.9782649278640747,
        "learning_rate": 3.10008236503283e-06,
        "epoch": 0.6165333333333334,
        "step": 4624
    },
    {
        "loss": 2.1087,
        "grad_norm": 4.628060340881348,
        "learning_rate": 3.0845521556707614e-06,
        "epoch": 0.6166666666666667,
        "step": 4625
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.8460826873779297,
        "learning_rate": 3.0690603348478066e-06,
        "epoch": 0.6168,
        "step": 4626
    },
    {
        "loss": 2.6977,
        "grad_norm": 1.8618813753128052,
        "learning_rate": 3.0536069087003573e-06,
        "epoch": 0.6169333333333333,
        "step": 4627
    },
    {
        "loss": 1.5302,
        "grad_norm": 3.9990787506103516,
        "learning_rate": 3.0381918833495394e-06,
        "epoch": 0.6170666666666667,
        "step": 4628
    },
    {
        "loss": 2.4186,
        "grad_norm": 3.1433656215667725,
        "learning_rate": 3.022815264901313e-06,
        "epoch": 0.6172,
        "step": 4629
    },
    {
        "loss": 1.9855,
        "grad_norm": 2.467043399810791,
        "learning_rate": 3.007477059446395e-06,
        "epoch": 0.6173333333333333,
        "step": 4630
    },
    {
        "loss": 3.319,
        "grad_norm": 3.3087165355682373,
        "learning_rate": 2.9921772730603036e-06,
        "epoch": 0.6174666666666667,
        "step": 4631
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.567981719970703,
        "learning_rate": 2.9769159118033464e-06,
        "epoch": 0.6176,
        "step": 4632
    },
    {
        "loss": 1.085,
        "grad_norm": 6.6933369636535645,
        "learning_rate": 2.9616929817205987e-06,
        "epoch": 0.6177333333333334,
        "step": 4633
    },
    {
        "loss": 2.1152,
        "grad_norm": 1.8253412246704102,
        "learning_rate": 2.9465084888419037e-06,
        "epoch": 0.6178666666666667,
        "step": 4634
    },
    {
        "loss": 1.1916,
        "grad_norm": 3.567870616912842,
        "learning_rate": 2.931362439181917e-06,
        "epoch": 0.618,
        "step": 4635
    },
    {
        "loss": 1.9465,
        "grad_norm": 2.8541488647460938,
        "learning_rate": 2.9162548387399957e-06,
        "epoch": 0.6181333333333333,
        "step": 4636
    },
    {
        "loss": 2.0555,
        "grad_norm": 2.8799872398376465,
        "learning_rate": 2.9011856935003525e-06,
        "epoch": 0.6182666666666666,
        "step": 4637
    },
    {
        "loss": 3.6219,
        "grad_norm": 4.095085144042969,
        "learning_rate": 2.886155009431901e-06,
        "epoch": 0.6184,
        "step": 4638
    },
    {
        "loss": 1.9302,
        "grad_norm": 3.8518638610839844,
        "learning_rate": 2.871162792488369e-06,
        "epoch": 0.6185333333333334,
        "step": 4639
    },
    {
        "loss": 2.1248,
        "grad_norm": 3.676767349243164,
        "learning_rate": 2.856209048608194e-06,
        "epoch": 0.6186666666666667,
        "step": 4640
    },
    {
        "loss": 1.6069,
        "grad_norm": 3.780582904815674,
        "learning_rate": 2.841293783714649e-06,
        "epoch": 0.6188,
        "step": 4641
    },
    {
        "loss": 2.5119,
        "grad_norm": 2.6814615726470947,
        "learning_rate": 2.8264170037156866e-06,
        "epoch": 0.6189333333333333,
        "step": 4642
    },
    {
        "loss": 1.8305,
        "grad_norm": 3.5655875205993652,
        "learning_rate": 2.8115787145040595e-06,
        "epoch": 0.6190666666666667,
        "step": 4643
    },
    {
        "loss": 2.3481,
        "grad_norm": 2.7469000816345215,
        "learning_rate": 2.796778921957266e-06,
        "epoch": 0.6192,
        "step": 4644
    },
    {
        "loss": 2.3575,
        "grad_norm": 3.495345115661621,
        "learning_rate": 2.78201763193755e-06,
        "epoch": 0.6193333333333333,
        "step": 4645
    },
    {
        "loss": 2.233,
        "grad_norm": 3.0796926021575928,
        "learning_rate": 2.7672948502919014e-06,
        "epoch": 0.6194666666666667,
        "step": 4646
    },
    {
        "loss": 1.857,
        "grad_norm": 2.6883647441864014,
        "learning_rate": 2.7526105828520886e-06,
        "epoch": 0.6196,
        "step": 4647
    },
    {
        "loss": 2.584,
        "grad_norm": 2.753547430038452,
        "learning_rate": 2.737964835434592e-06,
        "epoch": 0.6197333333333334,
        "step": 4648
    },
    {
        "loss": 2.7571,
        "grad_norm": 3.397664785385132,
        "learning_rate": 2.7233576138406157e-06,
        "epoch": 0.6198666666666667,
        "step": 4649
    },
    {
        "loss": 0.8974,
        "grad_norm": 5.234347820281982,
        "learning_rate": 2.7087889238561758e-06,
        "epoch": 0.62,
        "step": 4650
    },
    {
        "loss": 1.0115,
        "grad_norm": 3.8742480278015137,
        "learning_rate": 2.694258771251934e-06,
        "epoch": 0.6201333333333333,
        "step": 4651
    },
    {
        "loss": 2.7585,
        "grad_norm": 2.3804931640625,
        "learning_rate": 2.6797671617833754e-06,
        "epoch": 0.6202666666666666,
        "step": 4652
    },
    {
        "loss": 3.2657,
        "grad_norm": 3.392468214035034,
        "learning_rate": 2.665314101190641e-06,
        "epoch": 0.6204,
        "step": 4653
    },
    {
        "loss": 0.558,
        "grad_norm": 2.1514997482299805,
        "learning_rate": 2.6508995951986527e-06,
        "epoch": 0.6205333333333334,
        "step": 4654
    },
    {
        "loss": 2.4534,
        "grad_norm": 2.0704376697540283,
        "learning_rate": 2.636523649517042e-06,
        "epoch": 0.6206666666666667,
        "step": 4655
    },
    {
        "loss": 1.5692,
        "grad_norm": 1.5168583393096924,
        "learning_rate": 2.6221862698401658e-06,
        "epoch": 0.6208,
        "step": 4656
    },
    {
        "loss": 2.2319,
        "grad_norm": 3.573500156402588,
        "learning_rate": 2.607887461847125e-06,
        "epoch": 0.6209333333333333,
        "step": 4657
    },
    {
        "loss": 3.5667,
        "grad_norm": 2.534357786178589,
        "learning_rate": 2.593627231201712e-06,
        "epoch": 0.6210666666666667,
        "step": 4658
    },
    {
        "loss": 2.6529,
        "grad_norm": 3.900390625,
        "learning_rate": 2.579405583552452e-06,
        "epoch": 0.6212,
        "step": 4659
    },
    {
        "loss": 2.3468,
        "grad_norm": 2.547145366668701,
        "learning_rate": 2.5652225245326177e-06,
        "epoch": 0.6213333333333333,
        "step": 4660
    },
    {
        "loss": 1.7728,
        "grad_norm": 2.955862522125244,
        "learning_rate": 2.551078059760126e-06,
        "epoch": 0.6214666666666666,
        "step": 4661
    },
    {
        "loss": 3.3726,
        "grad_norm": 3.93347430229187,
        "learning_rate": 2.53697219483765e-06,
        "epoch": 0.6216,
        "step": 4662
    },
    {
        "loss": 2.1752,
        "grad_norm": 5.9615254402160645,
        "learning_rate": 2.522904935352599e-06,
        "epoch": 0.6217333333333334,
        "step": 4663
    },
    {
        "loss": 2.4077,
        "grad_norm": 2.939216136932373,
        "learning_rate": 2.508876286877049e-06,
        "epoch": 0.6218666666666667,
        "step": 4664
    },
    {
        "loss": 2.5096,
        "grad_norm": 3.468019962310791,
        "learning_rate": 2.4948862549677766e-06,
        "epoch": 0.622,
        "step": 4665
    },
    {
        "loss": 3.1914,
        "grad_norm": 2.7179393768310547,
        "learning_rate": 2.480934845166305e-06,
        "epoch": 0.6221333333333333,
        "step": 4666
    },
    {
        "loss": 2.1194,
        "grad_norm": 2.8676538467407227,
        "learning_rate": 2.467022062998858e-06,
        "epoch": 0.6222666666666666,
        "step": 4667
    },
    {
        "loss": 3.0862,
        "grad_norm": 2.836674690246582,
        "learning_rate": 2.4531479139762837e-06,
        "epoch": 0.6224,
        "step": 4668
    },
    {
        "loss": 3.2959,
        "grad_norm": 3.2537405490875244,
        "learning_rate": 2.439312403594207e-06,
        "epoch": 0.6225333333333334,
        "step": 4669
    },
    {
        "loss": 0.9493,
        "grad_norm": 3.053668737411499,
        "learning_rate": 2.4255155373329117e-06,
        "epoch": 0.6226666666666667,
        "step": 4670
    },
    {
        "loss": 2.2974,
        "grad_norm": 2.9980456829071045,
        "learning_rate": 2.411757320657393e-06,
        "epoch": 0.6228,
        "step": 4671
    },
    {
        "loss": 2.9591,
        "grad_norm": 2.716907262802124,
        "learning_rate": 2.3980377590173244e-06,
        "epoch": 0.6229333333333333,
        "step": 4672
    },
    {
        "loss": 1.1513,
        "grad_norm": 3.5256845951080322,
        "learning_rate": 2.38435685784707e-06,
        "epoch": 0.6230666666666667,
        "step": 4673
    },
    {
        "loss": 1.619,
        "grad_norm": 2.8178439140319824,
        "learning_rate": 2.370714622565684e-06,
        "epoch": 0.6232,
        "step": 4674
    },
    {
        "loss": 2.9239,
        "grad_norm": 3.062363862991333,
        "learning_rate": 2.3571110585768997e-06,
        "epoch": 0.6233333333333333,
        "step": 4675
    },
    {
        "loss": 1.4587,
        "grad_norm": 3.1469638347625732,
        "learning_rate": 2.3435461712691286e-06,
        "epoch": 0.6234666666666666,
        "step": 4676
    },
    {
        "loss": 2.6132,
        "grad_norm": 1.5737709999084473,
        "learning_rate": 2.330019966015484e-06,
        "epoch": 0.6236,
        "step": 4677
    },
    {
        "loss": 2.9079,
        "grad_norm": 1.9788894653320312,
        "learning_rate": 2.316532448173725e-06,
        "epoch": 0.6237333333333334,
        "step": 4678
    },
    {
        "loss": 2.7472,
        "grad_norm": 3.043046474456787,
        "learning_rate": 2.3030836230863216e-06,
        "epoch": 0.6238666666666667,
        "step": 4679
    },
    {
        "loss": 2.6357,
        "grad_norm": 2.5698394775390625,
        "learning_rate": 2.28967349608038e-06,
        "epoch": 0.624,
        "step": 4680
    },
    {
        "loss": 2.4712,
        "grad_norm": 2.55423641204834,
        "learning_rate": 2.276302072467695e-06,
        "epoch": 0.6241333333333333,
        "step": 4681
    },
    {
        "loss": 1.422,
        "grad_norm": 2.5878384113311768,
        "learning_rate": 2.262969357544764e-06,
        "epoch": 0.6242666666666666,
        "step": 4682
    },
    {
        "loss": 2.0658,
        "grad_norm": 1.9305723905563354,
        "learning_rate": 2.2496753565926954e-06,
        "epoch": 0.6244,
        "step": 4683
    },
    {
        "loss": 2.0961,
        "grad_norm": 2.2252299785614014,
        "learning_rate": 2.2364200748772878e-06,
        "epoch": 0.6245333333333334,
        "step": 4684
    },
    {
        "loss": 1.4155,
        "grad_norm": 2.9353723526000977,
        "learning_rate": 2.2232035176490196e-06,
        "epoch": 0.6246666666666667,
        "step": 4685
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.382642984390259,
        "learning_rate": 2.2100256901430137e-06,
        "epoch": 0.6248,
        "step": 4686
    },
    {
        "loss": 1.9243,
        "grad_norm": 2.9338395595550537,
        "learning_rate": 2.1968865975790174e-06,
        "epoch": 0.6249333333333333,
        "step": 4687
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.7763296365737915,
        "learning_rate": 2.183786245161512e-06,
        "epoch": 0.6250666666666667,
        "step": 4688
    },
    {
        "loss": 3.2449,
        "grad_norm": 2.0043818950653076,
        "learning_rate": 2.1707246380795686e-06,
        "epoch": 0.6252,
        "step": 4689
    },
    {
        "loss": 2.7324,
        "grad_norm": 2.2361621856689453,
        "learning_rate": 2.157701781506938e-06,
        "epoch": 0.6253333333333333,
        "step": 4690
    },
    {
        "loss": 2.4494,
        "grad_norm": 2.385434627532959,
        "learning_rate": 2.1447176806020263e-06,
        "epoch": 0.6254666666666666,
        "step": 4691
    },
    {
        "loss": 1.6003,
        "grad_norm": 3.350522518157959,
        "learning_rate": 2.131772340507887e-06,
        "epoch": 0.6256,
        "step": 4692
    },
    {
        "loss": 2.5237,
        "grad_norm": 3.8919739723205566,
        "learning_rate": 2.118865766352196e-06,
        "epoch": 0.6257333333333334,
        "step": 4693
    },
    {
        "loss": 1.1414,
        "grad_norm": 4.885985851287842,
        "learning_rate": 2.105997963247297e-06,
        "epoch": 0.6258666666666667,
        "step": 4694
    },
    {
        "loss": 2.7234,
        "grad_norm": 2.5578267574310303,
        "learning_rate": 2.0931689362901576e-06,
        "epoch": 0.626,
        "step": 4695
    },
    {
        "loss": 1.9584,
        "grad_norm": 2.9392666816711426,
        "learning_rate": 2.080378690562412e-06,
        "epoch": 0.6261333333333333,
        "step": 4696
    },
    {
        "loss": 1.9232,
        "grad_norm": 3.5829808712005615,
        "learning_rate": 2.0676272311303314e-06,
        "epoch": 0.6262666666666666,
        "step": 4697
    },
    {
        "loss": 1.9312,
        "grad_norm": 3.134570360183716,
        "learning_rate": 2.054914563044796e-06,
        "epoch": 0.6264,
        "step": 4698
    },
    {
        "loss": 2.8679,
        "grad_norm": 2.9498915672302246,
        "learning_rate": 2.042240691341335e-06,
        "epoch": 0.6265333333333334,
        "step": 4699
    },
    {
        "loss": 2.5462,
        "grad_norm": 2.7667605876922607,
        "learning_rate": 2.0296056210401315e-06,
        "epoch": 0.6266666666666667,
        "step": 4700
    },
    {
        "loss": 2.3846,
        "grad_norm": 2.594341516494751,
        "learning_rate": 2.01700935714596e-06,
        "epoch": 0.6268,
        "step": 4701
    },
    {
        "loss": 2.983,
        "grad_norm": 3.4728076457977295,
        "learning_rate": 2.0044519046482636e-06,
        "epoch": 0.6269333333333333,
        "step": 4702
    },
    {
        "loss": 2.2829,
        "grad_norm": 2.1452102661132812,
        "learning_rate": 1.9919332685210624e-06,
        "epoch": 0.6270666666666667,
        "step": 4703
    },
    {
        "loss": 3.0403,
        "grad_norm": 2.418400287628174,
        "learning_rate": 1.979453453723068e-06,
        "epoch": 0.6272,
        "step": 4704
    },
    {
        "loss": 2.3576,
        "grad_norm": 2.576336622238159,
        "learning_rate": 1.9670124651975376e-06,
        "epoch": 0.6273333333333333,
        "step": 4705
    },
    {
        "loss": 1.3583,
        "grad_norm": 3.6509828567504883,
        "learning_rate": 1.9546103078724064e-06,
        "epoch": 0.6274666666666666,
        "step": 4706
    },
    {
        "loss": 2.6898,
        "grad_norm": 3.4701595306396484,
        "learning_rate": 1.9422469866602234e-06,
        "epoch": 0.6276,
        "step": 4707
    },
    {
        "loss": 0.8949,
        "grad_norm": 3.770766019821167,
        "learning_rate": 1.9299225064581263e-06,
        "epoch": 0.6277333333333334,
        "step": 4708
    },
    {
        "loss": 0.734,
        "grad_norm": 2.9005396366119385,
        "learning_rate": 1.9176368721478987e-06,
        "epoch": 0.6278666666666667,
        "step": 4709
    },
    {
        "loss": 2.545,
        "grad_norm": 2.384629487991333,
        "learning_rate": 1.9053900885959153e-06,
        "epoch": 0.628,
        "step": 4710
    },
    {
        "loss": 2.8656,
        "grad_norm": 3.066507577896118,
        "learning_rate": 1.893182160653173e-06,
        "epoch": 0.6281333333333333,
        "step": 4711
    },
    {
        "loss": 2.4785,
        "grad_norm": 2.910740375518799,
        "learning_rate": 1.8810130931552483e-06,
        "epoch": 0.6282666666666666,
        "step": 4712
    },
    {
        "loss": 1.5269,
        "grad_norm": 3.090057134628296,
        "learning_rate": 1.8688828909223744e-06,
        "epoch": 0.6284,
        "step": 4713
    },
    {
        "loss": 1.6019,
        "grad_norm": 5.581817150115967,
        "learning_rate": 1.8567915587593632e-06,
        "epoch": 0.6285333333333334,
        "step": 4714
    },
    {
        "loss": 2.492,
        "grad_norm": 3.3321142196655273,
        "learning_rate": 1.8447391014556393e-06,
        "epoch": 0.6286666666666667,
        "step": 4715
    },
    {
        "loss": 1.4228,
        "grad_norm": 3.87556529045105,
        "learning_rate": 1.8327255237851949e-06,
        "epoch": 0.6288,
        "step": 4716
    },
    {
        "loss": 1.8064,
        "grad_norm": 2.8820228576660156,
        "learning_rate": 1.82075083050669e-06,
        "epoch": 0.6289333333333333,
        "step": 4717
    },
    {
        "loss": 1.2042,
        "grad_norm": 3.5650477409362793,
        "learning_rate": 1.8088150263633086e-06,
        "epoch": 0.6290666666666667,
        "step": 4718
    },
    {
        "loss": 3.4543,
        "grad_norm": 2.700772523880005,
        "learning_rate": 1.7969181160828686e-06,
        "epoch": 0.6292,
        "step": 4719
    },
    {
        "loss": 1.2523,
        "grad_norm": 3.0147242546081543,
        "learning_rate": 1.7850601043777892e-06,
        "epoch": 0.6293333333333333,
        "step": 4720
    },
    {
        "loss": 2.5437,
        "grad_norm": 2.4179084300994873,
        "learning_rate": 1.773240995945047e-06,
        "epoch": 0.6294666666666666,
        "step": 4721
    },
    {
        "loss": 2.5806,
        "grad_norm": 5.374069690704346,
        "learning_rate": 1.761460795466252e-06,
        "epoch": 0.6296,
        "step": 4722
    },
    {
        "loss": 2.0154,
        "grad_norm": 3.7822422981262207,
        "learning_rate": 1.7497195076075946e-06,
        "epoch": 0.6297333333333334,
        "step": 4723
    },
    {
        "loss": 1.3252,
        "grad_norm": 3.793325901031494,
        "learning_rate": 1.7380171370197985e-06,
        "epoch": 0.6298666666666667,
        "step": 4724
    },
    {
        "loss": 2.1371,
        "grad_norm": 2.676823377609253,
        "learning_rate": 1.7263536883382446e-06,
        "epoch": 0.63,
        "step": 4725
    },
    {
        "loss": 3.073,
        "grad_norm": 2.609093189239502,
        "learning_rate": 1.714729166182849e-06,
        "epoch": 0.6301333333333333,
        "step": 4726
    },
    {
        "loss": 1.272,
        "grad_norm": 3.0234014987945557,
        "learning_rate": 1.7031435751581282e-06,
        "epoch": 0.6302666666666666,
        "step": 4727
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.911048650741577,
        "learning_rate": 1.6915969198531777e-06,
        "epoch": 0.6304,
        "step": 4728
    },
    {
        "loss": 2.2923,
        "grad_norm": 2.0432164669036865,
        "learning_rate": 1.6800892048416617e-06,
        "epoch": 0.6305333333333333,
        "step": 4729
    },
    {
        "loss": 2.9481,
        "grad_norm": 2.411835193634033,
        "learning_rate": 1.6686204346818336e-06,
        "epoch": 0.6306666666666667,
        "step": 4730
    },
    {
        "loss": 1.0695,
        "grad_norm": 4.1794867515563965,
        "learning_rate": 1.657190613916504e-06,
        "epoch": 0.6308,
        "step": 4731
    },
    {
        "loss": 2.3406,
        "grad_norm": 3.125121831893921,
        "learning_rate": 1.6457997470730623e-06,
        "epoch": 0.6309333333333333,
        "step": 4732
    },
    {
        "loss": 1.7466,
        "grad_norm": 3.220437526702881,
        "learning_rate": 1.6344478386634776e-06,
        "epoch": 0.6310666666666667,
        "step": 4733
    },
    {
        "loss": 2.1751,
        "grad_norm": 3.9504308700561523,
        "learning_rate": 1.623134893184286e-06,
        "epoch": 0.6312,
        "step": 4734
    },
    {
        "loss": 3.0593,
        "grad_norm": 2.2373342514038086,
        "learning_rate": 1.6118609151165697e-06,
        "epoch": 0.6313333333333333,
        "step": 4735
    },
    {
        "loss": 1.5704,
        "grad_norm": 1.935013771057129,
        "learning_rate": 1.6006259089260011e-06,
        "epoch": 0.6314666666666666,
        "step": 4736
    },
    {
        "loss": 2.8676,
        "grad_norm": 2.52301025390625,
        "learning_rate": 1.589429879062776e-06,
        "epoch": 0.6316,
        "step": 4737
    },
    {
        "loss": 3.0427,
        "grad_norm": 3.9892475605010986,
        "learning_rate": 1.5782728299617244e-06,
        "epoch": 0.6317333333333334,
        "step": 4738
    },
    {
        "loss": 2.8644,
        "grad_norm": 2.6959452629089355,
        "learning_rate": 1.567154766042156e-06,
        "epoch": 0.6318666666666667,
        "step": 4739
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.9215087890625,
        "learning_rate": 1.556075691707992e-06,
        "epoch": 0.632,
        "step": 4740
    },
    {
        "loss": 2.1964,
        "grad_norm": 2.972241163253784,
        "learning_rate": 1.5450356113476671e-06,
        "epoch": 0.6321333333333333,
        "step": 4741
    },
    {
        "loss": 2.0046,
        "grad_norm": 3.688859224319458,
        "learning_rate": 1.5340345293342494e-06,
        "epoch": 0.6322666666666666,
        "step": 4742
    },
    {
        "loss": 2.6037,
        "grad_norm": 3.1583504676818848,
        "learning_rate": 1.5230724500252425e-06,
        "epoch": 0.6324,
        "step": 4743
    },
    {
        "loss": 2.7037,
        "grad_norm": 2.3164098262786865,
        "learning_rate": 1.5121493777628059e-06,
        "epoch": 0.6325333333333333,
        "step": 4744
    },
    {
        "loss": 1.9019,
        "grad_norm": 2.567110061645508,
        "learning_rate": 1.501265316873579e-06,
        "epoch": 0.6326666666666667,
        "step": 4745
    },
    {
        "loss": 2.6436,
        "grad_norm": 2.91208815574646,
        "learning_rate": 1.4904202716687798e-06,
        "epoch": 0.6328,
        "step": 4746
    },
    {
        "loss": 3.0719,
        "grad_norm": 2.244567632675171,
        "learning_rate": 1.479614246444183e-06,
        "epoch": 0.6329333333333333,
        "step": 4747
    },
    {
        "loss": 1.8993,
        "grad_norm": 3.584913492202759,
        "learning_rate": 1.468847245480076e-06,
        "epoch": 0.6330666666666667,
        "step": 4748
    },
    {
        "loss": 2.9615,
        "grad_norm": 3.013613700866699,
        "learning_rate": 1.458119273041325e-06,
        "epoch": 0.6332,
        "step": 4749
    },
    {
        "loss": 2.4717,
        "grad_norm": 3.315340042114258,
        "learning_rate": 1.4474303333772976e-06,
        "epoch": 0.6333333333333333,
        "step": 4750
    },
    {
        "loss": 2.4954,
        "grad_norm": 2.5270891189575195,
        "learning_rate": 1.436780430721929e-06,
        "epoch": 0.6334666666666666,
        "step": 4751
    },
    {
        "loss": 1.9435,
        "grad_norm": 3.8541343212127686,
        "learning_rate": 1.4261695692936672e-06,
        "epoch": 0.6336,
        "step": 4752
    },
    {
        "loss": 1.8473,
        "grad_norm": 2.036689519882202,
        "learning_rate": 1.4155977532955277e-06,
        "epoch": 0.6337333333333334,
        "step": 4753
    },
    {
        "loss": 2.8711,
        "grad_norm": 2.9995930194854736,
        "learning_rate": 1.4050649869150278e-06,
        "epoch": 0.6338666666666667,
        "step": 4754
    },
    {
        "loss": 2.7686,
        "grad_norm": 2.2708468437194824,
        "learning_rate": 1.3945712743242522e-06,
        "epoch": 0.634,
        "step": 4755
    },
    {
        "loss": 1.1225,
        "grad_norm": 3.364135265350342,
        "learning_rate": 1.3841166196797872e-06,
        "epoch": 0.6341333333333333,
        "step": 4756
    },
    {
        "loss": 1.8925,
        "grad_norm": 3.2703192234039307,
        "learning_rate": 1.3737010271227424e-06,
        "epoch": 0.6342666666666666,
        "step": 4757
    },
    {
        "loss": 1.0322,
        "grad_norm": 3.6125810146331787,
        "learning_rate": 1.3633245007787842e-06,
        "epoch": 0.6344,
        "step": 4758
    },
    {
        "loss": 2.662,
        "grad_norm": 2.4477601051330566,
        "learning_rate": 1.3529870447580918e-06,
        "epoch": 0.6345333333333333,
        "step": 4759
    },
    {
        "loss": 2.5628,
        "grad_norm": 3.4048171043395996,
        "learning_rate": 1.342688663155356e-06,
        "epoch": 0.6346666666666667,
        "step": 4760
    },
    {
        "loss": 2.5884,
        "grad_norm": 2.720602512359619,
        "learning_rate": 1.332429360049825e-06,
        "epoch": 0.6348,
        "step": 4761
    },
    {
        "loss": 2.8546,
        "grad_norm": 2.524850845336914,
        "learning_rate": 1.322209139505204e-06,
        "epoch": 0.6349333333333333,
        "step": 4762
    },
    {
        "loss": 2.2339,
        "grad_norm": 6.943150043487549,
        "learning_rate": 1.3120280055697876e-06,
        "epoch": 0.6350666666666667,
        "step": 4763
    },
    {
        "loss": 2.6057,
        "grad_norm": 4.396602153778076,
        "learning_rate": 1.30188596227635e-06,
        "epoch": 0.6352,
        "step": 4764
    },
    {
        "loss": 2.5902,
        "grad_norm": 3.899463415145874,
        "learning_rate": 1.2917830136421894e-06,
        "epoch": 0.6353333333333333,
        "step": 4765
    },
    {
        "loss": 2.9516,
        "grad_norm": 3.534872531890869,
        "learning_rate": 1.281719163669104e-06,
        "epoch": 0.6354666666666666,
        "step": 4766
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.967885136604309,
        "learning_rate": 1.2716944163434497e-06,
        "epoch": 0.6356,
        "step": 4767
    },
    {
        "loss": 1.6165,
        "grad_norm": 4.166140079498291,
        "learning_rate": 1.2617087756360503e-06,
        "epoch": 0.6357333333333334,
        "step": 4768
    },
    {
        "loss": 2.6954,
        "grad_norm": 3.270963668823242,
        "learning_rate": 1.2517622455022304e-06,
        "epoch": 0.6358666666666667,
        "step": 4769
    },
    {
        "loss": 2.2244,
        "grad_norm": 3.294367790222168,
        "learning_rate": 1.2418548298818721e-06,
        "epoch": 0.636,
        "step": 4770
    },
    {
        "loss": 3.0578,
        "grad_norm": 2.1825759410858154,
        "learning_rate": 1.231986532699314e-06,
        "epoch": 0.6361333333333333,
        "step": 4771
    },
    {
        "loss": 0.6619,
        "grad_norm": 2.668135643005371,
        "learning_rate": 1.2221573578634405e-06,
        "epoch": 0.6362666666666666,
        "step": 4772
    },
    {
        "loss": 2.4981,
        "grad_norm": 2.693101406097412,
        "learning_rate": 1.2123673092676147e-06,
        "epoch": 0.6364,
        "step": 4773
    },
    {
        "loss": 2.097,
        "grad_norm": 2.580036163330078,
        "learning_rate": 1.2026163907897126e-06,
        "epoch": 0.6365333333333333,
        "step": 4774
    },
    {
        "loss": 2.5511,
        "grad_norm": 3.828453302383423,
        "learning_rate": 1.1929046062921002e-06,
        "epoch": 0.6366666666666667,
        "step": 4775
    },
    {
        "loss": 2.6913,
        "grad_norm": 2.698404312133789,
        "learning_rate": 1.1832319596216556e-06,
        "epoch": 0.6368,
        "step": 4776
    },
    {
        "loss": 2.3137,
        "grad_norm": 2.7344586849212646,
        "learning_rate": 1.1735984546097256e-06,
        "epoch": 0.6369333333333334,
        "step": 4777
    },
    {
        "loss": 1.8861,
        "grad_norm": 3.4694905281066895,
        "learning_rate": 1.164004095072202e-06,
        "epoch": 0.6370666666666667,
        "step": 4778
    },
    {
        "loss": 1.8404,
        "grad_norm": 3.9063003063201904,
        "learning_rate": 1.1544488848094338e-06,
        "epoch": 0.6372,
        "step": 4779
    },
    {
        "loss": 1.7199,
        "grad_norm": 2.243662118911743,
        "learning_rate": 1.144932827606271e-06,
        "epoch": 0.6373333333333333,
        "step": 4780
    },
    {
        "loss": 3.099,
        "grad_norm": 3.5649263858795166,
        "learning_rate": 1.135455927232043e-06,
        "epoch": 0.6374666666666666,
        "step": 4781
    },
    {
        "loss": 1.8254,
        "grad_norm": 3.129080295562744,
        "learning_rate": 1.126018187440603e-06,
        "epoch": 0.6376,
        "step": 4782
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.343161106109619,
        "learning_rate": 1.1166196119702599e-06,
        "epoch": 0.6377333333333334,
        "step": 4783
    },
    {
        "loss": 2.7715,
        "grad_norm": 2.344954013824463,
        "learning_rate": 1.1072602045438253e-06,
        "epoch": 0.6378666666666667,
        "step": 4784
    },
    {
        "loss": 1.6381,
        "grad_norm": 4.460968971252441,
        "learning_rate": 1.0979399688686e-06,
        "epoch": 0.638,
        "step": 4785
    },
    {
        "loss": 2.9502,
        "grad_norm": 3.5172338485717773,
        "learning_rate": 1.0886589086363418e-06,
        "epoch": 0.6381333333333333,
        "step": 4786
    },
    {
        "loss": 0.7813,
        "grad_norm": 4.387738227844238,
        "learning_rate": 1.079417027523344e-06,
        "epoch": 0.6382666666666666,
        "step": 4787
    },
    {
        "loss": 1.8243,
        "grad_norm": 2.3236424922943115,
        "learning_rate": 1.0702143291902999e-06,
        "epoch": 0.6384,
        "step": 4788
    },
    {
        "loss": 2.6872,
        "grad_norm": 2.3728792667388916,
        "learning_rate": 1.0610508172824718e-06,
        "epoch": 0.6385333333333333,
        "step": 4789
    },
    {
        "loss": 1.9453,
        "grad_norm": 3.2624993324279785,
        "learning_rate": 1.0519264954295337e-06,
        "epoch": 0.6386666666666667,
        "step": 4790
    },
    {
        "loss": 2.3367,
        "grad_norm": 4.013227462768555,
        "learning_rate": 1.0428413672456616e-06,
        "epoch": 0.6388,
        "step": 4791
    },
    {
        "loss": 2.1944,
        "grad_norm": 4.327252388000488,
        "learning_rate": 1.033795436329521e-06,
        "epoch": 0.6389333333333334,
        "step": 4792
    },
    {
        "loss": 2.683,
        "grad_norm": 3.128218650817871,
        "learning_rate": 1.0247887062642347e-06,
        "epoch": 0.6390666666666667,
        "step": 4793
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.773977279663086,
        "learning_rate": 1.0158211806173822e-06,
        "epoch": 0.6392,
        "step": 4794
    },
    {
        "loss": 2.9849,
        "grad_norm": 2.0748848915100098,
        "learning_rate": 1.0068928629410445e-06,
        "epoch": 0.6393333333333333,
        "step": 4795
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.796683669090271,
        "learning_rate": 9.980037567717481e-07,
        "epoch": 0.6394666666666666,
        "step": 4796
    },
    {
        "loss": 2.7441,
        "grad_norm": 1.9468743801116943,
        "learning_rate": 9.8915386563051e-07,
        "epoch": 0.6396,
        "step": 4797
    },
    {
        "loss": 2.7725,
        "grad_norm": 1.8579061031341553,
        "learning_rate": 9.803431930227924e-07,
        "epoch": 0.6397333333333334,
        "step": 4798
    },
    {
        "loss": 2.6096,
        "grad_norm": 2.514284133911133,
        "learning_rate": 9.715717424385373e-07,
        "epoch": 0.6398666666666667,
        "step": 4799
    },
    {
        "loss": 2.8223,
        "grad_norm": 5.484684467315674,
        "learning_rate": 9.628395173521432e-07,
        "epoch": 0.64,
        "step": 4800
    },
    {
        "loss": 2.0265,
        "grad_norm": 3.4687678813934326,
        "learning_rate": 9.541465212224876e-07,
        "epoch": 0.6401333333333333,
        "step": 4801
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.8641579151153564,
        "learning_rate": 9.454927574928829e-07,
        "epoch": 0.6402666666666667,
        "step": 4802
    },
    {
        "loss": 1.9585,
        "grad_norm": 2.5298044681549072,
        "learning_rate": 9.368782295911093e-07,
        "epoch": 0.6404,
        "step": 4803
    },
    {
        "loss": 1.4816,
        "grad_norm": 2.847141742706299,
        "learning_rate": 9.283029409294264e-07,
        "epoch": 0.6405333333333333,
        "step": 4804
    },
    {
        "loss": 2.1531,
        "grad_norm": 4.368990898132324,
        "learning_rate": 9.197668949045279e-07,
        "epoch": 0.6406666666666667,
        "step": 4805
    },
    {
        "loss": 3.0088,
        "grad_norm": 2.921980619430542,
        "learning_rate": 9.11270094897565e-07,
        "epoch": 0.6408,
        "step": 4806
    },
    {
        "loss": 2.4848,
        "grad_norm": 2.168853759765625,
        "learning_rate": 9.028125442741453e-07,
        "epoch": 0.6409333333333334,
        "step": 4807
    },
    {
        "loss": 0.7987,
        "grad_norm": 3.0338544845581055,
        "learning_rate": 8.943942463843558e-07,
        "epoch": 0.6410666666666667,
        "step": 4808
    },
    {
        "loss": 2.3558,
        "grad_norm": 3.708371877670288,
        "learning_rate": 8.860152045626957e-07,
        "epoch": 0.6412,
        "step": 4809
    },
    {
        "loss": 2.7333,
        "grad_norm": 2.9914140701293945,
        "learning_rate": 8.776754221281324e-07,
        "epoch": 0.6413333333333333,
        "step": 4810
    },
    {
        "loss": 2.9279,
        "grad_norm": 2.0791354179382324,
        "learning_rate": 8.693749023840903e-07,
        "epoch": 0.6414666666666666,
        "step": 4811
    },
    {
        "loss": 2.2848,
        "grad_norm": 3.87528657913208,
        "learning_rate": 8.61113648618439e-07,
        "epoch": 0.6416,
        "step": 4812
    },
    {
        "loss": 2.0299,
        "grad_norm": 3.1658570766448975,
        "learning_rate": 8.528916641034501e-07,
        "epoch": 0.6417333333333334,
        "step": 4813
    },
    {
        "loss": 2.1803,
        "grad_norm": 3.343726634979248,
        "learning_rate": 8.447089520959295e-07,
        "epoch": 0.6418666666666667,
        "step": 4814
    },
    {
        "loss": 1.8891,
        "grad_norm": 2.099285125732422,
        "learning_rate": 8.365655158370511e-07,
        "epoch": 0.642,
        "step": 4815
    },
    {
        "loss": 0.8083,
        "grad_norm": 2.786527633666992,
        "learning_rate": 8.284613585524681e-07,
        "epoch": 0.6421333333333333,
        "step": 4816
    },
    {
        "loss": 2.081,
        "grad_norm": 3.831296682357788,
        "learning_rate": 8.20396483452246e-07,
        "epoch": 0.6422666666666667,
        "step": 4817
    },
    {
        "loss": 1.7824,
        "grad_norm": 5.470468044281006,
        "learning_rate": 8.123708937309404e-07,
        "epoch": 0.6424,
        "step": 4818
    },
    {
        "loss": 1.7185,
        "grad_norm": 6.197458744049072,
        "learning_rate": 8.043845925674976e-07,
        "epoch": 0.6425333333333333,
        "step": 4819
    },
    {
        "loss": 1.6709,
        "grad_norm": 3.8707661628723145,
        "learning_rate": 7.964375831253091e-07,
        "epoch": 0.6426666666666667,
        "step": 4820
    },
    {
        "loss": 2.0605,
        "grad_norm": 2.3513219356536865,
        "learning_rate": 7.885298685522235e-07,
        "epoch": 0.6428,
        "step": 4821
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.1020658016204834,
        "learning_rate": 7.806614519805022e-07,
        "epoch": 0.6429333333333334,
        "step": 4822
    },
    {
        "loss": 1.6872,
        "grad_norm": 3.7091195583343506,
        "learning_rate": 7.728323365268741e-07,
        "epoch": 0.6430666666666667,
        "step": 4823
    },
    {
        "loss": 2.2616,
        "grad_norm": 3.301234006881714,
        "learning_rate": 7.650425252924586e-07,
        "epoch": 0.6432,
        "step": 4824
    },
    {
        "loss": 2.6121,
        "grad_norm": 2.0329430103302,
        "learning_rate": 7.572920213628432e-07,
        "epoch": 0.6433333333333333,
        "step": 4825
    },
    {
        "loss": 1.0711,
        "grad_norm": 3.8270630836486816,
        "learning_rate": 7.495808278080052e-07,
        "epoch": 0.6434666666666666,
        "step": 4826
    },
    {
        "loss": 2.2165,
        "grad_norm": 4.331695556640625,
        "learning_rate": 7.419089476824015e-07,
        "epoch": 0.6436,
        "step": 4827
    },
    {
        "loss": 2.7054,
        "grad_norm": 2.3477680683135986,
        "learning_rate": 7.342763840248678e-07,
        "epoch": 0.6437333333333334,
        "step": 4828
    },
    {
        "loss": 2.78,
        "grad_norm": 3.1031711101531982,
        "learning_rate": 7.266831398587081e-07,
        "epoch": 0.6438666666666667,
        "step": 4829
    },
    {
        "loss": 0.8973,
        "grad_norm": 5.347402095794678,
        "learning_rate": 7.191292181916165e-07,
        "epoch": 0.644,
        "step": 4830
    },
    {
        "loss": 2.4514,
        "grad_norm": 2.370500326156616,
        "learning_rate": 7.116146220157438e-07,
        "epoch": 0.6441333333333333,
        "step": 4831
    },
    {
        "loss": 2.0197,
        "grad_norm": 4.740547180175781,
        "learning_rate": 7.041393543076202e-07,
        "epoch": 0.6442666666666667,
        "step": 4832
    },
    {
        "loss": 1.6914,
        "grad_norm": 4.2317962646484375,
        "learning_rate": 6.967034180282439e-07,
        "epoch": 0.6444,
        "step": 4833
    },
    {
        "loss": 2.6566,
        "grad_norm": 3.242537021636963,
        "learning_rate": 6.893068161230142e-07,
        "epoch": 0.6445333333333333,
        "step": 4834
    },
    {
        "loss": 1.8997,
        "grad_norm": 3.2253377437591553,
        "learning_rate": 6.819495515217433e-07,
        "epoch": 0.6446666666666667,
        "step": 4835
    },
    {
        "loss": 2.8704,
        "grad_norm": 3.5641963481903076,
        "learning_rate": 6.746316271386776e-07,
        "epoch": 0.6448,
        "step": 4836
    },
    {
        "loss": 2.6233,
        "grad_norm": 2.4306342601776123,
        "learning_rate": 6.673530458724764e-07,
        "epoch": 0.6449333333333334,
        "step": 4837
    },
    {
        "loss": 2.2914,
        "grad_norm": 3.7850050926208496,
        "learning_rate": 6.601138106061888e-07,
        "epoch": 0.6450666666666667,
        "step": 4838
    },
    {
        "loss": 1.8324,
        "grad_norm": 3.0336008071899414,
        "learning_rate": 6.529139242073101e-07,
        "epoch": 0.6452,
        "step": 4839
    },
    {
        "loss": 2.0115,
        "grad_norm": 3.6113674640655518,
        "learning_rate": 6.457533895277479e-07,
        "epoch": 0.6453333333333333,
        "step": 4840
    },
    {
        "loss": 3.2292,
        "grad_norm": 3.303853750228882,
        "learning_rate": 6.386322094038222e-07,
        "epoch": 0.6454666666666666,
        "step": 4841
    },
    {
        "loss": 2.39,
        "grad_norm": 3.170111894607544,
        "learning_rate": 6.315503866562211e-07,
        "epoch": 0.6456,
        "step": 4842
    },
    {
        "loss": 2.1,
        "grad_norm": 3.914618492126465,
        "learning_rate": 6.24507924090123e-07,
        "epoch": 0.6457333333333334,
        "step": 4843
    },
    {
        "loss": 3.0188,
        "grad_norm": 2.324619770050049,
        "learning_rate": 6.175048244950299e-07,
        "epoch": 0.6458666666666667,
        "step": 4844
    },
    {
        "loss": 2.5207,
        "grad_norm": 1.9226880073547363,
        "learning_rate": 6.105410906449338e-07,
        "epoch": 0.646,
        "step": 4845
    },
    {
        "loss": 2.2871,
        "grad_norm": 3.124330759048462,
        "learning_rate": 6.036167252981617e-07,
        "epoch": 0.6461333333333333,
        "step": 4846
    },
    {
        "loss": 2.0941,
        "grad_norm": 1.7041538953781128,
        "learning_rate": 5.967317311974863e-07,
        "epoch": 0.6462666666666667,
        "step": 4847
    },
    {
        "loss": 2.5574,
        "grad_norm": 2.6392478942871094,
        "learning_rate": 5.898861110700815e-07,
        "epoch": 0.6464,
        "step": 4848
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.9514658451080322,
        "learning_rate": 5.830798676275229e-07,
        "epoch": 0.6465333333333333,
        "step": 4849
    },
    {
        "loss": 2.3918,
        "grad_norm": 2.7750985622406006,
        "learning_rate": 5.76313003565776e-07,
        "epoch": 0.6466666666666666,
        "step": 4850
    },
    {
        "loss": 2.1688,
        "grad_norm": 3.154489755630493,
        "learning_rate": 5.695855215652413e-07,
        "epoch": 0.6468,
        "step": 4851
    },
    {
        "loss": 2.0718,
        "grad_norm": 2.2309277057647705,
        "learning_rate": 5.628974242906759e-07,
        "epoch": 0.6469333333333334,
        "step": 4852
    },
    {
        "loss": 2.2941,
        "grad_norm": 2.346904993057251,
        "learning_rate": 5.562487143912609e-07,
        "epoch": 0.6470666666666667,
        "step": 4853
    },
    {
        "loss": 1.4537,
        "grad_norm": 2.6514344215393066,
        "learning_rate": 5.496393945005784e-07,
        "epoch": 0.6472,
        "step": 4854
    },
    {
        "loss": 2.136,
        "grad_norm": 3.9240357875823975,
        "learning_rate": 5.43069467236601e-07,
        "epoch": 0.6473333333333333,
        "step": 4855
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.857387065887451,
        "learning_rate": 5.365389352017025e-07,
        "epoch": 0.6474666666666666,
        "step": 4856
    },
    {
        "loss": 0.8446,
        "grad_norm": 2.410160779953003,
        "learning_rate": 5.300478009826359e-07,
        "epoch": 0.6476,
        "step": 4857
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.883605718612671,
        "learning_rate": 5.23596067150578e-07,
        "epoch": 0.6477333333333334,
        "step": 4858
    },
    {
        "loss": 1.9391,
        "grad_norm": 4.2826738357543945,
        "learning_rate": 5.171837362610621e-07,
        "epoch": 0.6478666666666667,
        "step": 4859
    },
    {
        "loss": 2.7931,
        "grad_norm": 2.9045541286468506,
        "learning_rate": 5.108108108540676e-07,
        "epoch": 0.648,
        "step": 4860
    },
    {
        "loss": 2.8817,
        "grad_norm": 2.097564458847046,
        "learning_rate": 5.044772934538977e-07,
        "epoch": 0.6481333333333333,
        "step": 4861
    },
    {
        "loss": 2.011,
        "grad_norm": 2.010456085205078,
        "learning_rate": 4.981831865693121e-07,
        "epoch": 0.6482666666666667,
        "step": 4862
    },
    {
        "loss": 2.6069,
        "grad_norm": 2.6980204582214355,
        "learning_rate": 4.919284926933942e-07,
        "epoch": 0.6484,
        "step": 4863
    },
    {
        "loss": 2.2566,
        "grad_norm": 3.3286240100860596,
        "learning_rate": 4.857132143036735e-07,
        "epoch": 0.6485333333333333,
        "step": 4864
    },
    {
        "loss": 1.6795,
        "grad_norm": 2.9784555435180664,
        "learning_rate": 4.795373538620251e-07,
        "epoch": 0.6486666666666666,
        "step": 4865
    },
    {
        "loss": 2.3636,
        "grad_norm": 3.777266025543213,
        "learning_rate": 4.734009138147477e-07,
        "epoch": 0.6488,
        "step": 4866
    },
    {
        "loss": 2.4596,
        "grad_norm": 2.224703788757324,
        "learning_rate": 4.6730389659248584e-07,
        "epoch": 0.6489333333333334,
        "step": 4867
    },
    {
        "loss": 2.2214,
        "grad_norm": 1.8735631704330444,
        "learning_rate": 4.612463046103077e-07,
        "epoch": 0.6490666666666667,
        "step": 4868
    },
    {
        "loss": 2.633,
        "grad_norm": 2.338216543197632,
        "learning_rate": 4.552281402676495e-07,
        "epoch": 0.6492,
        "step": 4869
    },
    {
        "loss": 2.838,
        "grad_norm": 3.0954620838165283,
        "learning_rate": 4.4924940594830435e-07,
        "epoch": 0.6493333333333333,
        "step": 4870
    },
    {
        "loss": 1.6389,
        "grad_norm": 1.9366031885147095,
        "learning_rate": 4.433101040204779e-07,
        "epoch": 0.6494666666666666,
        "step": 4871
    },
    {
        "loss": 1.4912,
        "grad_norm": 4.26876974105835,
        "learning_rate": 4.3741023683675495e-07,
        "epoch": 0.6496,
        "step": 4872
    },
    {
        "loss": 2.2826,
        "grad_norm": 2.761301040649414,
        "learning_rate": 4.315498067340884e-07,
        "epoch": 0.6497333333333334,
        "step": 4873
    },
    {
        "loss": 2.3741,
        "grad_norm": 2.6400787830352783,
        "learning_rate": 4.257288160338102e-07,
        "epoch": 0.6498666666666667,
        "step": 4874
    },
    {
        "loss": 2.7518,
        "grad_norm": 2.7061471939086914,
        "learning_rate": 4.1994726704164266e-07,
        "epoch": 0.65,
        "step": 4875
    },
    {
        "loss": 1.8295,
        "grad_norm": 4.293347358703613,
        "learning_rate": 4.142051620476761e-07,
        "epoch": 0.6501333333333333,
        "step": 4876
    },
    {
        "loss": 0.8673,
        "grad_norm": 4.0014777183532715,
        "learning_rate": 4.0850250332636895e-07,
        "epoch": 0.6502666666666667,
        "step": 4877
    },
    {
        "loss": 3.0062,
        "grad_norm": 3.4640755653381348,
        "learning_rate": 4.028392931365699e-07,
        "epoch": 0.6504,
        "step": 4878
    },
    {
        "loss": 2.5621,
        "grad_norm": 2.211299180984497,
        "learning_rate": 3.9721553372150664e-07,
        "epoch": 0.6505333333333333,
        "step": 4879
    },
    {
        "loss": 2.4013,
        "grad_norm": 2.6749210357666016,
        "learning_rate": 3.916312273087419e-07,
        "epoch": 0.6506666666666666,
        "step": 4880
    },
    {
        "loss": 2.1389,
        "grad_norm": 3.00901460647583,
        "learning_rate": 3.8608637611027286e-07,
        "epoch": 0.6508,
        "step": 4881
    },
    {
        "loss": 2.3785,
        "grad_norm": 5.0641279220581055,
        "learning_rate": 3.8058098232239826e-07,
        "epoch": 0.6509333333333334,
        "step": 4882
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.03977108001709,
        "learning_rate": 3.751150481258514e-07,
        "epoch": 0.6510666666666667,
        "step": 4883
    },
    {
        "loss": 2.5784,
        "grad_norm": 3.8717775344848633,
        "learning_rate": 3.696885756857005e-07,
        "epoch": 0.6512,
        "step": 4884
    },
    {
        "loss": 2.2933,
        "grad_norm": 4.7885613441467285,
        "learning_rate": 3.6430156715138164e-07,
        "epoch": 0.6513333333333333,
        "step": 4885
    },
    {
        "loss": 2.2492,
        "grad_norm": 3.3584463596343994,
        "learning_rate": 3.5895402465671026e-07,
        "epoch": 0.6514666666666666,
        "step": 4886
    },
    {
        "loss": 2.3094,
        "grad_norm": 3.589522361755371,
        "learning_rate": 3.536459503198697e-07,
        "epoch": 0.6516,
        "step": 4887
    },
    {
        "loss": 2.4176,
        "grad_norm": 2.082855224609375,
        "learning_rate": 3.4837734624341144e-07,
        "epoch": 0.6517333333333334,
        "step": 4888
    },
    {
        "loss": 1.959,
        "grad_norm": 3.18668794631958,
        "learning_rate": 3.4314821451423283e-07,
        "epoch": 0.6518666666666667,
        "step": 4889
    },
    {
        "loss": 1.8018,
        "grad_norm": 2.930164098739624,
        "learning_rate": 3.379585572036215e-07,
        "epoch": 0.652,
        "step": 4890
    },
    {
        "loss": 2.6418,
        "grad_norm": 2.6863853931427,
        "learning_rate": 3.3280837636722183e-07,
        "epoch": 0.6521333333333333,
        "step": 4891
    },
    {
        "loss": 0.7473,
        "grad_norm": 3.3310256004333496,
        "learning_rate": 3.276976740450244e-07,
        "epoch": 0.6522666666666667,
        "step": 4892
    },
    {
        "loss": 2.1785,
        "grad_norm": 4.144495964050293,
        "learning_rate": 3.226264522613987e-07,
        "epoch": 0.6524,
        "step": 4893
    },
    {
        "loss": 2.3396,
        "grad_norm": 3.7098305225372314,
        "learning_rate": 3.175947130250934e-07,
        "epoch": 0.6525333333333333,
        "step": 4894
    },
    {
        "loss": 2.281,
        "grad_norm": 3.1629788875579834,
        "learning_rate": 3.1260245832916976e-07,
        "epoch": 0.6526666666666666,
        "step": 4895
    },
    {
        "loss": 1.6981,
        "grad_norm": 3.78548002243042,
        "learning_rate": 3.076496901510906e-07,
        "epoch": 0.6528,
        "step": 4896
    },
    {
        "loss": 2.4519,
        "grad_norm": 3.3023083209991455,
        "learning_rate": 3.027364104526642e-07,
        "epoch": 0.6529333333333334,
        "step": 4897
    },
    {
        "loss": 1.8893,
        "grad_norm": 3.6658935546875,
        "learning_rate": 2.9786262118005615e-07,
        "epoch": 0.6530666666666667,
        "step": 4898
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.616799831390381,
        "learning_rate": 2.930283242637888e-07,
        "epoch": 0.6532,
        "step": 4899
    },
    {
        "loss": 2.0525,
        "grad_norm": 3.387458324432373,
        "learning_rate": 2.882335216187526e-07,
        "epoch": 0.6533333333333333,
        "step": 4900
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.0855796337127686,
        "learning_rate": 2.834782151441617e-07,
        "epoch": 0.6534666666666666,
        "step": 4901
    },
    {
        "loss": 0.7717,
        "grad_norm": 2.262847900390625,
        "learning_rate": 2.7876240672364273e-07,
        "epoch": 0.6536,
        "step": 4902
    },
    {
        "loss": 2.6582,
        "grad_norm": 2.3249247074127197,
        "learning_rate": 2.7408609822511253e-07,
        "epoch": 0.6537333333333334,
        "step": 4903
    },
    {
        "loss": 2.3076,
        "grad_norm": 4.231384754180908,
        "learning_rate": 2.694492915009006e-07,
        "epoch": 0.6538666666666667,
        "step": 4904
    },
    {
        "loss": 3.0239,
        "grad_norm": 3.4220333099365234,
        "learning_rate": 2.648519883876377e-07,
        "epoch": 0.654,
        "step": 4905
    },
    {
        "loss": 2.3667,
        "grad_norm": 4.833501815795898,
        "learning_rate": 2.6029419070634497e-07,
        "epoch": 0.6541333333333333,
        "step": 4906
    },
    {
        "loss": 2.4802,
        "grad_norm": 3.2717041969299316,
        "learning_rate": 2.557759002623783e-07,
        "epoch": 0.6542666666666667,
        "step": 4907
    },
    {
        "loss": 2.6073,
        "grad_norm": 3.3807437419891357,
        "learning_rate": 2.5129711884543937e-07,
        "epoch": 0.6544,
        "step": 4908
    },
    {
        "loss": 1.4333,
        "grad_norm": 4.997982025146484,
        "learning_rate": 2.468578482296091e-07,
        "epoch": 0.6545333333333333,
        "step": 4909
    },
    {
        "loss": 1.8973,
        "grad_norm": 2.649909019470215,
        "learning_rate": 2.424580901732698e-07,
        "epoch": 0.6546666666666666,
        "step": 4910
    },
    {
        "loss": 1.7031,
        "grad_norm": 3.5402135848999023,
        "learning_rate": 2.380978464192163e-07,
        "epoch": 0.6548,
        "step": 4911
    },
    {
        "loss": 1.5039,
        "grad_norm": 3.759725332260132,
        "learning_rate": 2.3377711869452257e-07,
        "epoch": 0.6549333333333334,
        "step": 4912
    },
    {
        "loss": 1.515,
        "grad_norm": 3.763608932495117,
        "learning_rate": 2.2949590871066407e-07,
        "epoch": 0.6550666666666667,
        "step": 4913
    },
    {
        "loss": 1.6086,
        "grad_norm": 1.62552011013031,
        "learning_rate": 2.2525421816342874e-07,
        "epoch": 0.6552,
        "step": 4914
    },
    {
        "loss": 2.111,
        "grad_norm": 4.374935150146484,
        "learning_rate": 2.210520487329837e-07,
        "epoch": 0.6553333333333333,
        "step": 4915
    },
    {
        "loss": 2.2471,
        "grad_norm": 2.569598436355591,
        "learning_rate": 2.1688940208379748e-07,
        "epoch": 0.6554666666666666,
        "step": 4916
    },
    {
        "loss": 2.3068,
        "grad_norm": 2.561924457550049,
        "learning_rate": 2.1276627986472897e-07,
        "epoch": 0.6556,
        "step": 4917
    },
    {
        "loss": 0.5654,
        "grad_norm": 2.8584930896759033,
        "learning_rate": 2.086826837089495e-07,
        "epoch": 0.6557333333333333,
        "step": 4918
    },
    {
        "loss": 1.7065,
        "grad_norm": 4.672577381134033,
        "learning_rate": 2.0463861523399853e-07,
        "epoch": 0.6558666666666667,
        "step": 4919
    },
    {
        "loss": 1.792,
        "grad_norm": 1.666105031967163,
        "learning_rate": 2.0063407604172803e-07,
        "epoch": 0.656,
        "step": 4920
    },
    {
        "loss": 2.5172,
        "grad_norm": 2.870440721511841,
        "learning_rate": 1.9666906771835803e-07,
        "epoch": 0.6561333333333333,
        "step": 4921
    },
    {
        "loss": 2.7213,
        "grad_norm": 1.8926032781600952,
        "learning_rate": 1.9274359183444336e-07,
        "epoch": 0.6562666666666667,
        "step": 4922
    },
    {
        "loss": 2.9597,
        "grad_norm": 2.3892688751220703,
        "learning_rate": 1.8885764994487354e-07,
        "epoch": 0.6564,
        "step": 4923
    },
    {
        "loss": 2.5697,
        "grad_norm": 2.4010982513427734,
        "learning_rate": 1.850112435888951e-07,
        "epoch": 0.6565333333333333,
        "step": 4924
    },
    {
        "loss": 2.534,
        "grad_norm": 2.600093126296997,
        "learning_rate": 1.812043742900671e-07,
        "epoch": 0.6566666666666666,
        "step": 4925
    },
    {
        "loss": 2.6057,
        "grad_norm": 1.8314818143844604,
        "learning_rate": 1.7743704355631663e-07,
        "epoch": 0.6568,
        "step": 4926
    },
    {
        "loss": 1.3187,
        "grad_norm": 3.612550973892212,
        "learning_rate": 1.7370925287988337e-07,
        "epoch": 0.6569333333333334,
        "step": 4927
    },
    {
        "loss": 0.8726,
        "grad_norm": 2.9244184494018555,
        "learning_rate": 1.7002100373737505e-07,
        "epoch": 0.6570666666666667,
        "step": 4928
    },
    {
        "loss": 1.6446,
        "grad_norm": 3.453533887863159,
        "learning_rate": 1.6637229758970086e-07,
        "epoch": 0.6572,
        "step": 4929
    },
    {
        "loss": 2.4342,
        "grad_norm": 3.013901472091675,
        "learning_rate": 1.62763135882138e-07,
        "epoch": 0.6573333333333333,
        "step": 4930
    },
    {
        "loss": 1.9238,
        "grad_norm": 3.3744351863861084,
        "learning_rate": 1.5919352004427623e-07,
        "epoch": 0.6574666666666666,
        "step": 4931
    },
    {
        "loss": 2.3948,
        "grad_norm": 3.602822780609131,
        "learning_rate": 1.5566345149006235e-07,
        "epoch": 0.6576,
        "step": 4932
    },
    {
        "loss": 2.8322,
        "grad_norm": 4.307845115661621,
        "learning_rate": 1.521729316177778e-07,
        "epoch": 0.6577333333333333,
        "step": 4933
    },
    {
        "loss": 2.2484,
        "grad_norm": 3.9532856941223145,
        "learning_rate": 1.4872196181000553e-07,
        "epoch": 0.6578666666666667,
        "step": 4934
    },
    {
        "loss": 2.5305,
        "grad_norm": 2.52712345123291,
        "learning_rate": 1.4531054343370764e-07,
        "epoch": 0.658,
        "step": 4935
    },
    {
        "loss": 2.4568,
        "grad_norm": 3.247936964035034,
        "learning_rate": 1.4193867784014769e-07,
        "epoch": 0.6581333333333333,
        "step": 4936
    },
    {
        "loss": 2.389,
        "grad_norm": 3.1460957527160645,
        "learning_rate": 1.3860636636493506e-07,
        "epoch": 0.6582666666666667,
        "step": 4937
    },
    {
        "loss": 2.0032,
        "grad_norm": 2.1354191303253174,
        "learning_rate": 1.3531361032801394e-07,
        "epoch": 0.6584,
        "step": 4938
    },
    {
        "loss": 1.8137,
        "grad_norm": 3.339282274246216,
        "learning_rate": 1.32060411033641e-07,
        "epoch": 0.6585333333333333,
        "step": 4939
    },
    {
        "loss": 2.6513,
        "grad_norm": 3.3562119007110596,
        "learning_rate": 1.2884676977044097e-07,
        "epoch": 0.6586666666666666,
        "step": 4940
    },
    {
        "loss": 2.3794,
        "grad_norm": 3.0974457263946533,
        "learning_rate": 1.2567268781132902e-07,
        "epoch": 0.6588,
        "step": 4941
    },
    {
        "loss": 1.0179,
        "grad_norm": 4.843603610992432,
        "learning_rate": 1.225381664135883e-07,
        "epoch": 0.6589333333333334,
        "step": 4942
    },
    {
        "loss": 2.6402,
        "grad_norm": 2.723051071166992,
        "learning_rate": 1.1944320681879229e-07,
        "epoch": 0.6590666666666667,
        "step": 4943
    },
    {
        "loss": 1.6883,
        "grad_norm": 2.624008893966675,
        "learning_rate": 1.1638781025289369e-07,
        "epoch": 0.6592,
        "step": 4944
    },
    {
        "loss": 2.6734,
        "grad_norm": 1.4389148950576782,
        "learning_rate": 1.1337197792611332e-07,
        "epoch": 0.6593333333333333,
        "step": 4945
    },
    {
        "loss": 2.3598,
        "grad_norm": 2.8736307621002197,
        "learning_rate": 1.1039571103306224e-07,
        "epoch": 0.6594666666666666,
        "step": 4946
    },
    {
        "loss": 2.1169,
        "grad_norm": 3.300100326538086,
        "learning_rate": 1.0745901075261966e-07,
        "epoch": 0.6596,
        "step": 4947
    },
    {
        "loss": 1.6339,
        "grad_norm": 3.2339203357696533,
        "learning_rate": 1.0456187824805508e-07,
        "epoch": 0.6597333333333333,
        "step": 4948
    },
    {
        "loss": 2.4827,
        "grad_norm": 2.605888843536377,
        "learning_rate": 1.017043146669061e-07,
        "epoch": 0.6598666666666667,
        "step": 4949
    },
    {
        "loss": 2.61,
        "grad_norm": 2.391268730163574,
        "learning_rate": 9.888632114106733e-08,
        "epoch": 0.66,
        "step": 4950
    },
    {
        "loss": 2.4263,
        "grad_norm": 3.749882221221924,
        "learning_rate": 9.610789878676807e-08,
        "epoch": 0.6601333333333333,
        "step": 4951
    },
    {
        "loss": 3.0081,
        "grad_norm": 3.3508317470550537,
        "learning_rate": 9.336904870455021e-08,
        "epoch": 0.6602666666666667,
        "step": 4952
    },
    {
        "loss": 2.143,
        "grad_norm": 2.44793963432312,
        "learning_rate": 9.066977197926818e-08,
        "epoch": 0.6604,
        "step": 4953
    },
    {
        "loss": 1.9804,
        "grad_norm": 3.489985466003418,
        "learning_rate": 8.801006968012226e-08,
        "epoch": 0.6605333333333333,
        "step": 4954
    },
    {
        "loss": 2.0436,
        "grad_norm": 2.618480920791626,
        "learning_rate": 8.53899428606364e-08,
        "epoch": 0.6606666666666666,
        "step": 4955
    },
    {
        "loss": 2.2077,
        "grad_norm": 3.9908056259155273,
        "learning_rate": 8.280939255863595e-08,
        "epoch": 0.6608,
        "step": 4956
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.6491501331329346,
        "learning_rate": 8.026841979630328e-08,
        "epoch": 0.6609333333333334,
        "step": 4957
    },
    {
        "loss": 2.698,
        "grad_norm": 2.855300188064575,
        "learning_rate": 7.776702558011106e-08,
        "epoch": 0.6610666666666667,
        "step": 4958
    },
    {
        "loss": 2.9798,
        "grad_norm": 2.5315475463867188,
        "learning_rate": 7.530521090087783e-08,
        "epoch": 0.6612,
        "step": 4959
    },
    {
        "loss": 2.8707,
        "grad_norm": 2.0354397296905518,
        "learning_rate": 7.288297673373468e-08,
        "epoch": 0.6613333333333333,
        "step": 4960
    },
    {
        "loss": 2.5243,
        "grad_norm": 2.786229133605957,
        "learning_rate": 7.050032403813633e-08,
        "epoch": 0.6614666666666666,
        "step": 4961
    },
    {
        "loss": 2.6821,
        "grad_norm": 2.465555429458618,
        "learning_rate": 6.81572537578612e-08,
        "epoch": 0.6616,
        "step": 4962
    },
    {
        "loss": 1.3513,
        "grad_norm": 3.917865753173828,
        "learning_rate": 6.58537668210113e-08,
        "epoch": 0.6617333333333333,
        "step": 4963
    },
    {
        "loss": 2.3933,
        "grad_norm": 2.4789953231811523,
        "learning_rate": 6.358986414000123e-08,
        "epoch": 0.6618666666666667,
        "step": 4964
    },
    {
        "loss": 2.0022,
        "grad_norm": 2.943934440612793,
        "learning_rate": 6.136554661156924e-08,
        "epoch": 0.662,
        "step": 4965
    },
    {
        "loss": 2.1096,
        "grad_norm": 5.68405294418335,
        "learning_rate": 5.91808151167772e-08,
        "epoch": 0.6621333333333334,
        "step": 4966
    },
    {
        "loss": 2.543,
        "grad_norm": 2.6531753540039062,
        "learning_rate": 5.703567052099956e-08,
        "epoch": 0.6622666666666667,
        "step": 4967
    },
    {
        "loss": 1.8675,
        "grad_norm": 4.1809821128845215,
        "learning_rate": 5.493011367394552e-08,
        "epoch": 0.6624,
        "step": 4968
    },
    {
        "loss": 2.7174,
        "grad_norm": 3.488884210586548,
        "learning_rate": 5.2864145409625695e-08,
        "epoch": 0.6625333333333333,
        "step": 4969
    },
    {
        "loss": 1.9535,
        "grad_norm": 4.124392986297607,
        "learning_rate": 5.0837766546385504e-08,
        "epoch": 0.6626666666666666,
        "step": 4970
    },
    {
        "loss": 2.5132,
        "grad_norm": 3.5805389881134033,
        "learning_rate": 4.885097788687176e-08,
        "epoch": 0.6628,
        "step": 4971
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.3782200813293457,
        "learning_rate": 4.690378021805497e-08,
        "epoch": 0.6629333333333334,
        "step": 4972
    },
    {
        "loss": 2.7399,
        "grad_norm": 2.2103195190429688,
        "learning_rate": 4.499617431124037e-08,
        "epoch": 0.6630666666666667,
        "step": 4973
    },
    {
        "loss": 2.2297,
        "grad_norm": 3.133603572845459,
        "learning_rate": 4.312816092202354e-08,
        "epoch": 0.6632,
        "step": 4974
    },
    {
        "loss": 2.1195,
        "grad_norm": 2.5027971267700195,
        "learning_rate": 4.129974079034593e-08,
        "epoch": 0.6633333333333333,
        "step": 4975
    },
    {
        "loss": 2.362,
        "grad_norm": 2.73299241065979,
        "learning_rate": 3.951091464042822e-08,
        "epoch": 0.6634666666666666,
        "step": 4976
    },
    {
        "loss": 1.8034,
        "grad_norm": 3.2948124408721924,
        "learning_rate": 3.776168318084805e-08,
        "epoch": 0.6636,
        "step": 4977
    },
    {
        "loss": 2.3227,
        "grad_norm": 3.766812324523926,
        "learning_rate": 3.605204710448451e-08,
        "epoch": 0.6637333333333333,
        "step": 4978
    },
    {
        "loss": 2.4998,
        "grad_norm": 2.316028118133545,
        "learning_rate": 3.438200708851813e-08,
        "epoch": 0.6638666666666667,
        "step": 4979
    },
    {
        "loss": 2.1489,
        "grad_norm": 3.792008876800537,
        "learning_rate": 3.2751563794475305e-08,
        "epoch": 0.664,
        "step": 4980
    },
    {
        "loss": 1.7866,
        "grad_norm": 2.502702236175537,
        "learning_rate": 3.116071786815056e-08,
        "epoch": 0.6641333333333334,
        "step": 4981
    },
    {
        "loss": 1.9694,
        "grad_norm": 3.313030958175659,
        "learning_rate": 2.9609469939717583e-08,
        "epoch": 0.6642666666666667,
        "step": 4982
    },
    {
        "loss": 2.5956,
        "grad_norm": 2.4885478019714355,
        "learning_rate": 2.8097820623596006e-08,
        "epoch": 0.6644,
        "step": 4983
    },
    {
        "loss": 1.9285,
        "grad_norm": 3.2595791816711426,
        "learning_rate": 2.6625770518584615e-08,
        "epoch": 0.6645333333333333,
        "step": 4984
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.7914687395095825,
        "learning_rate": 2.5193320207761438e-08,
        "epoch": 0.6646666666666666,
        "step": 4985
    },
    {
        "loss": 2.4059,
        "grad_norm": 2.6889028549194336,
        "learning_rate": 2.3800470258505958e-08,
        "epoch": 0.6648,
        "step": 4986
    },
    {
        "loss": 0.7665,
        "grad_norm": 7.115774154663086,
        "learning_rate": 2.2447221222554605e-08,
        "epoch": 0.6649333333333334,
        "step": 4987
    },
    {
        "loss": 2.2579,
        "grad_norm": 4.115840911865234,
        "learning_rate": 2.1133573635923053e-08,
        "epoch": 0.6650666666666667,
        "step": 4988
    },
    {
        "loss": 1.0909,
        "grad_norm": 3.864190101623535,
        "learning_rate": 1.9859528018950634e-08,
        "epoch": 0.6652,
        "step": 4989
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.34240984916687,
        "learning_rate": 1.8625084876289224e-08,
        "epoch": 0.6653333333333333,
        "step": 4990
    },
    {
        "loss": 1.2516,
        "grad_norm": 3.141165256500244,
        "learning_rate": 1.7430244696925447e-08,
        "epoch": 0.6654666666666667,
        "step": 4991
    },
    {
        "loss": 2.0023,
        "grad_norm": 1.998534083366394,
        "learning_rate": 1.627500795410297e-08,
        "epoch": 0.6656,
        "step": 4992
    },
    {
        "loss": 1.9069,
        "grad_norm": 2.103848695755005,
        "learning_rate": 1.5159375105455732e-08,
        "epoch": 0.6657333333333333,
        "step": 4993
    },
    {
        "loss": 2.4381,
        "grad_norm": 3.3465545177459717,
        "learning_rate": 1.4083346592852487e-08,
        "epoch": 0.6658666666666667,
        "step": 4994
    },
    {
        "loss": 1.4848,
        "grad_norm": 3.1581666469573975,
        "learning_rate": 1.3046922842541166e-08,
        "epoch": 0.666,
        "step": 4995
    },
    {
        "loss": 1.9516,
        "grad_norm": 2.957486391067505,
        "learning_rate": 1.2050104265037832e-08,
        "epoch": 0.6661333333333334,
        "step": 4996
    },
    {
        "loss": 2.5162,
        "grad_norm": 2.324937105178833,
        "learning_rate": 1.1092891255182204e-08,
        "epoch": 0.6662666666666667,
        "step": 4997
    },
    {
        "loss": 2.5886,
        "grad_norm": 2.6558353900909424,
        "learning_rate": 1.0175284192148749e-08,
        "epoch": 0.6664,
        "step": 4998
    },
    {
        "loss": 1.9453,
        "grad_norm": 3.270521879196167,
        "learning_rate": 9.297283439380079e-09,
        "epoch": 0.6665333333333333,
        "step": 4999
    },
    {
        "loss": 2.3574,
        "grad_norm": 3.6619491577148438,
        "learning_rate": 8.458889344675758e-09,
        "epoch": 0.6666666666666666,
        "step": 5000
    },
    {
        "loss": 2.401,
        "grad_norm": 2.8943562507629395,
        "learning_rate": 7.660102240114597e-09,
        "epoch": 0.6668,
        "step": 5001
    },
    {
        "loss": 2.7389,
        "grad_norm": 1.8590028285980225,
        "learning_rate": 6.900922442099056e-09,
        "epoch": 0.6669333333333334,
        "step": 5002
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.1187000274658203,
        "learning_rate": 6.1813502513552445e-09,
        "epoch": 0.6670666666666667,
        "step": 5003
    },
    {
        "loss": 2.974,
        "grad_norm": 2.4495768547058105,
        "learning_rate": 5.501385952888516e-09,
        "epoch": 0.6672,
        "step": 5004
    },
    {
        "loss": 2.9164,
        "grad_norm": 1.9181363582611084,
        "learning_rate": 4.86102981606118e-09,
        "epoch": 0.6673333333333333,
        "step": 5005
    },
    {
        "loss": 1.8346,
        "grad_norm": 2.176689863204956,
        "learning_rate": 4.260282094492585e-09,
        "epoch": 0.6674666666666667,
        "step": 5006
    },
    {
        "loss": 2.5526,
        "grad_norm": 2.1602461338043213,
        "learning_rate": 3.6991430261590354e-09,
        "epoch": 0.6676,
        "step": 5007
    },
    {
        "loss": 2.2238,
        "grad_norm": 5.101714611053467,
        "learning_rate": 3.177612833316079e-09,
        "epoch": 0.6677333333333333,
        "step": 5008
    },
    {
        "loss": 2.3046,
        "grad_norm": 3.2909836769104004,
        "learning_rate": 2.695691722554017e-09,
        "epoch": 0.6678666666666667,
        "step": 5009
    },
    {
        "loss": 2.4591,
        "grad_norm": 2.2843751907348633,
        "learning_rate": 2.253379884764595e-09,
        "epoch": 0.668,
        "step": 5010
    },
    {
        "loss": 2.9257,
        "grad_norm": 3.1312332153320312,
        "learning_rate": 1.8506774951410066e-09,
        "epoch": 0.6681333333333334,
        "step": 5011
    },
    {
        "loss": 2.2531,
        "grad_norm": 2.541612386703491,
        "learning_rate": 1.4875847132000963e-09,
        "epoch": 0.6682666666666667,
        "step": 5012
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.4351258277893066,
        "learning_rate": 1.164101682771257e-09,
        "epoch": 0.6684,
        "step": 5013
    },
    {
        "loss": 1.6758,
        "grad_norm": 2.2944185733795166,
        "learning_rate": 8.802285319742254e-10,
        "epoch": 0.6685333333333333,
        "step": 5014
    },
    {
        "loss": 2.5271,
        "grad_norm": 2.3459722995758057,
        "learning_rate": 6.359653732523896e-10,
        "epoch": 0.6686666666666666,
        "step": 5015
    },
    {
        "loss": 2.3505,
        "grad_norm": 3.426440954208374,
        "learning_rate": 4.313123033727884e-10,
        "epoch": 0.6688,
        "step": 5016
    },
    {
        "loss": 1.4002,
        "grad_norm": 3.0228261947631836,
        "learning_rate": 2.662694033817026e-10,
        "epoch": 0.6689333333333334,
        "step": 5017
    },
    {
        "loss": 2.5261,
        "grad_norm": 3.040663957595825,
        "learning_rate": 1.4083673867126833e-10,
        "epoch": 0.6690666666666667,
        "step": 5018
    },
    {
        "loss": 1.7806,
        "grad_norm": 2.9310131072998047,
        "learning_rate": 5.5014358912863775e-11,
        "epoch": 0.6692,
        "step": 5019
    },
    {
        "loss": 2.4665,
        "grad_norm": 3.600069522857666,
        "learning_rate": 8.802298101517892e-12,
        "epoch": 0.6693333333333333,
        "step": 5020
    },
    {
        "loss": 2.5905,
        "grad_norm": 3.5193114280700684,
        "learning_rate": 0.00019999999779942546,
        "epoch": 0.6694666666666667,
        "step": 5021
    },
    {
        "loss": 2.9193,
        "grad_norm": 2.9989306926727295,
        "learning_rate": 0.00019999996479080917,
        "epoch": 0.6696,
        "step": 5022
    },
    {
        "loss": 1.4314,
        "grad_norm": 5.284580707550049,
        "learning_rate": 0.00019999989217186603,
        "epoch": 0.6697333333333333,
        "step": 5023
    },
    {
        "loss": 2.955,
        "grad_norm": 2.3274757862091064,
        "learning_rate": 0.00019999977994262488,
        "epoch": 0.6698666666666667,
        "step": 5024
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.858457088470459,
        "learning_rate": 0.00019999962810313018,
        "epoch": 0.67,
        "step": 5025
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.5598702430725098,
        "learning_rate": 0.00019999943665344202,
        "epoch": 0.6701333333333334,
        "step": 5026
    },
    {
        "loss": 3.176,
        "grad_norm": 3.4367973804473877,
        "learning_rate": 0.00019999920559363627,
        "epoch": 0.6702666666666667,
        "step": 5027
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.326260805130005,
        "learning_rate": 0.00019999893492380445,
        "epoch": 0.6704,
        "step": 5028
    },
    {
        "loss": 1.7155,
        "grad_norm": 3.8835582733154297,
        "learning_rate": 0.00019999862464405377,
        "epoch": 0.6705333333333333,
        "step": 5029
    },
    {
        "loss": 2.3584,
        "grad_norm": 2.67985463142395,
        "learning_rate": 0.00019999827475450713,
        "epoch": 0.6706666666666666,
        "step": 5030
    },
    {
        "loss": 2.348,
        "grad_norm": 2.2540180683135986,
        "learning_rate": 0.0001999978852553031,
        "epoch": 0.6708,
        "step": 5031
    },
    {
        "loss": 2.3993,
        "grad_norm": 2.97107195854187,
        "learning_rate": 0.00019999745614659602,
        "epoch": 0.6709333333333334,
        "step": 5032
    },
    {
        "loss": 2.1399,
        "grad_norm": 4.636279582977295,
        "learning_rate": 0.0001999969874285558,
        "epoch": 0.6710666666666667,
        "step": 5033
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.4133427143096924,
        "learning_rate": 0.00019999647910136817,
        "epoch": 0.6712,
        "step": 5034
    },
    {
        "loss": 2.4015,
        "grad_norm": 2.6256461143493652,
        "learning_rate": 0.0001999959311652344,
        "epoch": 0.6713333333333333,
        "step": 5035
    },
    {
        "loss": 2.7537,
        "grad_norm": 2.800401449203491,
        "learning_rate": 0.0001999953436203716,
        "epoch": 0.6714666666666667,
        "step": 5036
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.270162343978882,
        "learning_rate": 0.00019999471646701242,
        "epoch": 0.6716,
        "step": 5037
    },
    {
        "loss": 1.9078,
        "grad_norm": 4.540157318115234,
        "learning_rate": 0.00019999404970540537,
        "epoch": 0.6717333333333333,
        "step": 5038
    },
    {
        "loss": 1.9112,
        "grad_norm": 3.1501874923706055,
        "learning_rate": 0.00019999334333581449,
        "epoch": 0.6718666666666666,
        "step": 5039
    },
    {
        "loss": 1.4365,
        "grad_norm": 12.148240089416504,
        "learning_rate": 0.00019999259735851958,
        "epoch": 0.672,
        "step": 5040
    },
    {
        "loss": 2.4561,
        "grad_norm": 3.773771047592163,
        "learning_rate": 0.00019999181177381617,
        "epoch": 0.6721333333333334,
        "step": 5041
    },
    {
        "loss": 2.4317,
        "grad_norm": 9.741081237792969,
        "learning_rate": 0.00019999098658201537,
        "epoch": 0.6722666666666667,
        "step": 5042
    },
    {
        "loss": 2.1104,
        "grad_norm": 3.2430174350738525,
        "learning_rate": 0.0001999901217834441,
        "epoch": 0.6724,
        "step": 5043
    },
    {
        "loss": 2.3284,
        "grad_norm": 4.13735818862915,
        "learning_rate": 0.00019998921737844486,
        "epoch": 0.6725333333333333,
        "step": 5044
    },
    {
        "loss": 2.4533,
        "grad_norm": 6.208957195281982,
        "learning_rate": 0.00019998827336737595,
        "epoch": 0.6726666666666666,
        "step": 5045
    },
    {
        "loss": 2.2777,
        "grad_norm": 5.6092448234558105,
        "learning_rate": 0.0001999872897506112,
        "epoch": 0.6728,
        "step": 5046
    },
    {
        "loss": 3.0462,
        "grad_norm": 2.3409900665283203,
        "learning_rate": 0.00019998626652854035,
        "epoch": 0.6729333333333334,
        "step": 5047
    },
    {
        "loss": 2.1165,
        "grad_norm": 3.8762903213500977,
        "learning_rate": 0.0001999852037015686,
        "epoch": 0.6730666666666667,
        "step": 5048
    },
    {
        "loss": 1.9174,
        "grad_norm": 4.441023826599121,
        "learning_rate": 0.00019998410127011694,
        "epoch": 0.6732,
        "step": 5049
    },
    {
        "loss": 2.2508,
        "grad_norm": 3.6763949394226074,
        "learning_rate": 0.00019998295923462212,
        "epoch": 0.6733333333333333,
        "step": 5050
    },
    {
        "loss": 2.7952,
        "grad_norm": 4.122528553009033,
        "learning_rate": 0.00019998177759553647,
        "epoch": 0.6734666666666667,
        "step": 5051
    },
    {
        "loss": 1.0177,
        "grad_norm": 4.117578983306885,
        "learning_rate": 0.00019998055635332798,
        "epoch": 0.6736,
        "step": 5052
    },
    {
        "loss": 1.8223,
        "grad_norm": 4.384770393371582,
        "learning_rate": 0.00019997929550848048,
        "epoch": 0.6737333333333333,
        "step": 5053
    },
    {
        "loss": 1.3777,
        "grad_norm": 5.815518379211426,
        "learning_rate": 0.00019997799506149338,
        "epoch": 0.6738666666666666,
        "step": 5054
    },
    {
        "loss": 2.3766,
        "grad_norm": 2.6181583404541016,
        "learning_rate": 0.00019997665501288175,
        "epoch": 0.674,
        "step": 5055
    },
    {
        "loss": 2.4467,
        "grad_norm": 3.063338279724121,
        "learning_rate": 0.00019997527536317643,
        "epoch": 0.6741333333333334,
        "step": 5056
    },
    {
        "loss": 2.4782,
        "grad_norm": 3.8188459873199463,
        "learning_rate": 0.00019997385611292384,
        "epoch": 0.6742666666666667,
        "step": 5057
    },
    {
        "loss": 2.0485,
        "grad_norm": 4.251352787017822,
        "learning_rate": 0.0001999723972626862,
        "epoch": 0.6744,
        "step": 5058
    },
    {
        "loss": 2.7465,
        "grad_norm": 2.618380308151245,
        "learning_rate": 0.00019997089881304135,
        "epoch": 0.6745333333333333,
        "step": 5059
    },
    {
        "loss": 2.4232,
        "grad_norm": 3.070899724960327,
        "learning_rate": 0.00019996936076458288,
        "epoch": 0.6746666666666666,
        "step": 5060
    },
    {
        "loss": 0.93,
        "grad_norm": 4.052858829498291,
        "learning_rate": 0.00019996778311791994,
        "epoch": 0.6748,
        "step": 5061
    },
    {
        "loss": 3.0248,
        "grad_norm": 1.9439382553100586,
        "learning_rate": 0.0001999661658736775,
        "epoch": 0.6749333333333334,
        "step": 5062
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.1344683170318604,
        "learning_rate": 0.0001999645090324961,
        "epoch": 0.6750666666666667,
        "step": 5063
    },
    {
        "loss": 1.6398,
        "grad_norm": 4.023131370544434,
        "learning_rate": 0.00019996281259503207,
        "epoch": 0.6752,
        "step": 5064
    },
    {
        "loss": 1.9385,
        "grad_norm": 3.389634847640991,
        "learning_rate": 0.00019996107656195734,
        "epoch": 0.6753333333333333,
        "step": 5065
    },
    {
        "loss": 3.0257,
        "grad_norm": 2.906879186630249,
        "learning_rate": 0.00019995930093395957,
        "epoch": 0.6754666666666667,
        "step": 5066
    },
    {
        "loss": 2.3846,
        "grad_norm": 3.0317881107330322,
        "learning_rate": 0.00019995748571174216,
        "epoch": 0.6756,
        "step": 5067
    },
    {
        "loss": 2.135,
        "grad_norm": 3.313169240951538,
        "learning_rate": 0.000199955630896024,
        "epoch": 0.6757333333333333,
        "step": 5068
    },
    {
        "loss": 2.5414,
        "grad_norm": 2.352567672729492,
        "learning_rate": 0.00019995373648753985,
        "epoch": 0.6758666666666666,
        "step": 5069
    },
    {
        "loss": 2.3966,
        "grad_norm": 3.036081552505493,
        "learning_rate": 0.00019995180248704011,
        "epoch": 0.676,
        "step": 5070
    },
    {
        "loss": 3.1202,
        "grad_norm": 2.854706048965454,
        "learning_rate": 0.00019994982889529083,
        "epoch": 0.6761333333333334,
        "step": 5071
    },
    {
        "loss": 2.8995,
        "grad_norm": 2.9319725036621094,
        "learning_rate": 0.00019994781571307376,
        "epoch": 0.6762666666666667,
        "step": 5072
    },
    {
        "loss": 1.8522,
        "grad_norm": 6.125239849090576,
        "learning_rate": 0.0001999457629411863,
        "epoch": 0.6764,
        "step": 5073
    },
    {
        "loss": 2.9969,
        "grad_norm": 2.9358041286468506,
        "learning_rate": 0.0001999436705804416,
        "epoch": 0.6765333333333333,
        "step": 5074
    },
    {
        "loss": 1.1348,
        "grad_norm": 5.6392903327941895,
        "learning_rate": 0.00019994153863166846,
        "epoch": 0.6766666666666666,
        "step": 5075
    },
    {
        "loss": 2.5568,
        "grad_norm": 1.7783942222595215,
        "learning_rate": 0.00019993936709571128,
        "epoch": 0.6768,
        "step": 5076
    },
    {
        "loss": 2.0734,
        "grad_norm": 2.0448741912841797,
        "learning_rate": 0.00019993715597343026,
        "epoch": 0.6769333333333334,
        "step": 5077
    },
    {
        "loss": 1.1963,
        "grad_norm": 2.6738717555999756,
        "learning_rate": 0.00019993490526570127,
        "epoch": 0.6770666666666667,
        "step": 5078
    },
    {
        "loss": 2.5337,
        "grad_norm": 3.9547557830810547,
        "learning_rate": 0.00019993261497341575,
        "epoch": 0.6772,
        "step": 5079
    },
    {
        "loss": 1.8811,
        "grad_norm": 4.909202575683594,
        "learning_rate": 0.00019993028509748093,
        "epoch": 0.6773333333333333,
        "step": 5080
    },
    {
        "loss": 1.6826,
        "grad_norm": 3.2834205627441406,
        "learning_rate": 0.00019992791563881967,
        "epoch": 0.6774666666666667,
        "step": 5081
    },
    {
        "loss": 2.3609,
        "grad_norm": 3.3186888694763184,
        "learning_rate": 0.00019992550659837056,
        "epoch": 0.6776,
        "step": 5082
    },
    {
        "loss": 1.7828,
        "grad_norm": 3.6530306339263916,
        "learning_rate": 0.0001999230579770878,
        "epoch": 0.6777333333333333,
        "step": 5083
    },
    {
        "loss": 1.7596,
        "grad_norm": 4.896794319152832,
        "learning_rate": 0.00019992056977594127,
        "epoch": 0.6778666666666666,
        "step": 5084
    },
    {
        "loss": 2.3853,
        "grad_norm": 3.1779003143310547,
        "learning_rate": 0.00019991804199591658,
        "epoch": 0.678,
        "step": 5085
    },
    {
        "loss": 2.1592,
        "grad_norm": 3.3714020252227783,
        "learning_rate": 0.000199915474638015,
        "epoch": 0.6781333333333334,
        "step": 5086
    },
    {
        "loss": 2.5298,
        "grad_norm": 2.602975606918335,
        "learning_rate": 0.0001999128677032535,
        "epoch": 0.6782666666666667,
        "step": 5087
    },
    {
        "loss": 2.6485,
        "grad_norm": 4.1523237228393555,
        "learning_rate": 0.0001999102211926646,
        "epoch": 0.6784,
        "step": 5088
    },
    {
        "loss": 2.2699,
        "grad_norm": 3.159083366394043,
        "learning_rate": 0.00019990753510729667,
        "epoch": 0.6785333333333333,
        "step": 5089
    },
    {
        "loss": 0.6562,
        "grad_norm": 2.6029438972473145,
        "learning_rate": 0.00019990480944821366,
        "epoch": 0.6786666666666666,
        "step": 5090
    },
    {
        "loss": 2.0243,
        "grad_norm": 3.650880813598633,
        "learning_rate": 0.00019990204421649522,
        "epoch": 0.6788,
        "step": 5091
    },
    {
        "loss": 2.179,
        "grad_norm": 3.0995779037475586,
        "learning_rate": 0.00019989923941323662,
        "epoch": 0.6789333333333334,
        "step": 5092
    },
    {
        "loss": 2.3222,
        "grad_norm": 3.832653284072876,
        "learning_rate": 0.00019989639503954892,
        "epoch": 0.6790666666666667,
        "step": 5093
    },
    {
        "loss": 2.2203,
        "grad_norm": 5.029541969299316,
        "learning_rate": 0.00019989351109655877,
        "epoch": 0.6792,
        "step": 5094
    },
    {
        "loss": 2.6631,
        "grad_norm": 2.763942241668701,
        "learning_rate": 0.00019989058758540848,
        "epoch": 0.6793333333333333,
        "step": 5095
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.8796498775482178,
        "learning_rate": 0.0001998876245072561,
        "epoch": 0.6794666666666667,
        "step": 5096
    },
    {
        "loss": 1.9278,
        "grad_norm": 3.0779738426208496,
        "learning_rate": 0.00019988462186327526,
        "epoch": 0.6796,
        "step": 5097
    },
    {
        "loss": 1.1616,
        "grad_norm": 4.640063762664795,
        "learning_rate": 0.00019988157965465536,
        "epoch": 0.6797333333333333,
        "step": 5098
    },
    {
        "loss": 2.4774,
        "grad_norm": 4.117778778076172,
        "learning_rate": 0.00019987849788260143,
        "epoch": 0.6798666666666666,
        "step": 5099
    },
    {
        "loss": 2.319,
        "grad_norm": 3.052577018737793,
        "learning_rate": 0.00019987537654833415,
        "epoch": 0.68,
        "step": 5100
    },
    {
        "loss": 2.4965,
        "grad_norm": 5.314592361450195,
        "learning_rate": 0.00019987221565308992,
        "epoch": 0.6801333333333334,
        "step": 5101
    },
    {
        "loss": 2.2067,
        "grad_norm": 2.7932016849517822,
        "learning_rate": 0.00019986901519812076,
        "epoch": 0.6802666666666667,
        "step": 5102
    },
    {
        "loss": 2.4287,
        "grad_norm": 3.457991361618042,
        "learning_rate": 0.00019986577518469435,
        "epoch": 0.6804,
        "step": 5103
    },
    {
        "loss": 1.341,
        "grad_norm": 2.5485239028930664,
        "learning_rate": 0.00019986249561409415,
        "epoch": 0.6805333333333333,
        "step": 5104
    },
    {
        "loss": 1.2149,
        "grad_norm": 4.8390212059021,
        "learning_rate": 0.00019985917648761915,
        "epoch": 0.6806666666666666,
        "step": 5105
    },
    {
        "loss": 1.9367,
        "grad_norm": 5.458975791931152,
        "learning_rate": 0.0001998558178065841,
        "epoch": 0.6808,
        "step": 5106
    },
    {
        "loss": 1.6288,
        "grad_norm": 2.1206836700439453,
        "learning_rate": 0.0001998524195723193,
        "epoch": 0.6809333333333333,
        "step": 5107
    },
    {
        "loss": 2.4445,
        "grad_norm": 2.206125259399414,
        "learning_rate": 0.00019984898178617092,
        "epoch": 0.6810666666666667,
        "step": 5108
    },
    {
        "loss": 2.5877,
        "grad_norm": 3.074742078781128,
        "learning_rate": 0.00019984550444950063,
        "epoch": 0.6812,
        "step": 5109
    },
    {
        "loss": 2.413,
        "grad_norm": 4.449140548706055,
        "learning_rate": 0.0001998419875636858,
        "epoch": 0.6813333333333333,
        "step": 5110
    },
    {
        "loss": 2.4521,
        "grad_norm": 2.1829605102539062,
        "learning_rate": 0.00019983843113011949,
        "epoch": 0.6814666666666667,
        "step": 5111
    },
    {
        "loss": 1.9655,
        "grad_norm": 4.7965922355651855,
        "learning_rate": 0.00019983483515021043,
        "epoch": 0.6816,
        "step": 5112
    },
    {
        "loss": 2.3279,
        "grad_norm": 4.521740436553955,
        "learning_rate": 0.000199831199625383,
        "epoch": 0.6817333333333333,
        "step": 5113
    },
    {
        "loss": 2.8027,
        "grad_norm": 3.192328453063965,
        "learning_rate": 0.0001998275245570772,
        "epoch": 0.6818666666666666,
        "step": 5114
    },
    {
        "loss": 2.6765,
        "grad_norm": 2.3559656143188477,
        "learning_rate": 0.0001998238099467488,
        "epoch": 0.682,
        "step": 5115
    },
    {
        "loss": 2.4994,
        "grad_norm": 2.6832094192504883,
        "learning_rate": 0.0001998200557958691,
        "epoch": 0.6821333333333334,
        "step": 5116
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.6278066635131836,
        "learning_rate": 0.0001998162621059252,
        "epoch": 0.6822666666666667,
        "step": 5117
    },
    {
        "loss": 2.6706,
        "grad_norm": 4.094198703765869,
        "learning_rate": 0.00019981242887841976,
        "epoch": 0.6824,
        "step": 5118
    },
    {
        "loss": 2.5095,
        "grad_norm": 2.6296160221099854,
        "learning_rate": 0.00019980855611487113,
        "epoch": 0.6825333333333333,
        "step": 5119
    },
    {
        "loss": 2.4767,
        "grad_norm": 3.801323890686035,
        "learning_rate": 0.00019980464381681335,
        "epoch": 0.6826666666666666,
        "step": 5120
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.629648447036743,
        "learning_rate": 0.00019980069198579606,
        "epoch": 0.6828,
        "step": 5121
    },
    {
        "loss": 2.7035,
        "grad_norm": 3.451470375061035,
        "learning_rate": 0.0001997967006233846,
        "epoch": 0.6829333333333333,
        "step": 5122
    },
    {
        "loss": 2.2857,
        "grad_norm": 3.1538329124450684,
        "learning_rate": 0.00019979266973116003,
        "epoch": 0.6830666666666667,
        "step": 5123
    },
    {
        "loss": 2.4308,
        "grad_norm": 3.1993179321289062,
        "learning_rate": 0.00019978859931071887,
        "epoch": 0.6832,
        "step": 5124
    },
    {
        "loss": 3.0498,
        "grad_norm": 2.5738298892974854,
        "learning_rate": 0.00019978448936367355,
        "epoch": 0.6833333333333333,
        "step": 5125
    },
    {
        "loss": 2.8148,
        "grad_norm": 2.2584755420684814,
        "learning_rate": 0.00019978033989165198,
        "epoch": 0.6834666666666667,
        "step": 5126
    },
    {
        "loss": 2.2597,
        "grad_norm": 2.643348455429077,
        "learning_rate": 0.00019977615089629777,
        "epoch": 0.6836,
        "step": 5127
    },
    {
        "loss": 2.5703,
        "grad_norm": 5.850571632385254,
        "learning_rate": 0.00019977192237927022,
        "epoch": 0.6837333333333333,
        "step": 5128
    },
    {
        "loss": 2.4495,
        "grad_norm": 2.777215003967285,
        "learning_rate": 0.00019976765434224426,
        "epoch": 0.6838666666666666,
        "step": 5129
    },
    {
        "loss": 2.2231,
        "grad_norm": 3.6583175659179688,
        "learning_rate": 0.00019976334678691046,
        "epoch": 0.684,
        "step": 5130
    },
    {
        "loss": 0.7119,
        "grad_norm": 3.0672805309295654,
        "learning_rate": 0.00019975899971497505,
        "epoch": 0.6841333333333334,
        "step": 5131
    },
    {
        "loss": 2.9189,
        "grad_norm": 2.369380235671997,
        "learning_rate": 0.00019975461312815996,
        "epoch": 0.6842666666666667,
        "step": 5132
    },
    {
        "loss": 2.1732,
        "grad_norm": 2.9479026794433594,
        "learning_rate": 0.00019975018702820266,
        "epoch": 0.6844,
        "step": 5133
    },
    {
        "loss": 2.1759,
        "grad_norm": 3.3279006481170654,
        "learning_rate": 0.00019974572141685643,
        "epoch": 0.6845333333333333,
        "step": 5134
    },
    {
        "loss": 2.6845,
        "grad_norm": 2.4375998973846436,
        "learning_rate": 0.00019974121629589008,
        "epoch": 0.6846666666666666,
        "step": 5135
    },
    {
        "loss": 2.2459,
        "grad_norm": 2.5564122200012207,
        "learning_rate": 0.00019973667166708805,
        "epoch": 0.6848,
        "step": 5136
    },
    {
        "loss": 2.0121,
        "grad_norm": 4.545897006988525,
        "learning_rate": 0.00019973208753225054,
        "epoch": 0.6849333333333333,
        "step": 5137
    },
    {
        "loss": 1.7441,
        "grad_norm": 2.810023069381714,
        "learning_rate": 0.00019972746389319336,
        "epoch": 0.6850666666666667,
        "step": 5138
    },
    {
        "loss": 1.9386,
        "grad_norm": 3.2272722721099854,
        "learning_rate": 0.0001997228007517479,
        "epoch": 0.6852,
        "step": 5139
    },
    {
        "loss": 2.7665,
        "grad_norm": 3.6246604919433594,
        "learning_rate": 0.00019971809810976127,
        "epoch": 0.6853333333333333,
        "step": 5140
    },
    {
        "loss": 2.2621,
        "grad_norm": 3.0451128482818604,
        "learning_rate": 0.0001997133559690962,
        "epoch": 0.6854666666666667,
        "step": 5141
    },
    {
        "loss": 2.9525,
        "grad_norm": 4.14906644821167,
        "learning_rate": 0.00019970857433163105,
        "epoch": 0.6856,
        "step": 5142
    },
    {
        "loss": 2.668,
        "grad_norm": 2.9693171977996826,
        "learning_rate": 0.00019970375319925985,
        "epoch": 0.6857333333333333,
        "step": 5143
    },
    {
        "loss": 1.696,
        "grad_norm": 5.298538684844971,
        "learning_rate": 0.0001996988925738923,
        "epoch": 0.6858666666666666,
        "step": 5144
    },
    {
        "loss": 2.867,
        "grad_norm": 2.523198366165161,
        "learning_rate": 0.00019969399245745368,
        "epoch": 0.686,
        "step": 5145
    },
    {
        "loss": 2.2574,
        "grad_norm": 3.3306708335876465,
        "learning_rate": 0.00019968905285188492,
        "epoch": 0.6861333333333334,
        "step": 5146
    },
    {
        "loss": 2.7447,
        "grad_norm": 2.4396770000457764,
        "learning_rate": 0.0001996840737591427,
        "epoch": 0.6862666666666667,
        "step": 5147
    },
    {
        "loss": 2.3368,
        "grad_norm": 2.729968786239624,
        "learning_rate": 0.00019967905518119915,
        "epoch": 0.6864,
        "step": 5148
    },
    {
        "loss": 2.0008,
        "grad_norm": 3.343114137649536,
        "learning_rate": 0.00019967399712004223,
        "epoch": 0.6865333333333333,
        "step": 5149
    },
    {
        "loss": 2.2237,
        "grad_norm": 3.1737310886383057,
        "learning_rate": 0.00019966889957767538,
        "epoch": 0.6866666666666666,
        "step": 5150
    },
    {
        "loss": 2.3266,
        "grad_norm": 4.182234764099121,
        "learning_rate": 0.00019966376255611782,
        "epoch": 0.6868,
        "step": 5151
    },
    {
        "loss": 2.3253,
        "grad_norm": 2.6417148113250732,
        "learning_rate": 0.00019965858605740434,
        "epoch": 0.6869333333333333,
        "step": 5152
    },
    {
        "loss": 2.5848,
        "grad_norm": 2.018012285232544,
        "learning_rate": 0.0001996533700835853,
        "epoch": 0.6870666666666667,
        "step": 5153
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.5189263820648193,
        "learning_rate": 0.00019964811463672684,
        "epoch": 0.6872,
        "step": 5154
    },
    {
        "loss": 1.5256,
        "grad_norm": 3.5102713108062744,
        "learning_rate": 0.0001996428197189106,
        "epoch": 0.6873333333333334,
        "step": 5155
    },
    {
        "loss": 2.9714,
        "grad_norm": 2.7797787189483643,
        "learning_rate": 0.00019963748533223394,
        "epoch": 0.6874666666666667,
        "step": 5156
    },
    {
        "loss": 2.879,
        "grad_norm": 3.8475632667541504,
        "learning_rate": 0.00019963211147880987,
        "epoch": 0.6876,
        "step": 5157
    },
    {
        "loss": 2.8537,
        "grad_norm": 2.226915121078491,
        "learning_rate": 0.00019962669816076694,
        "epoch": 0.6877333333333333,
        "step": 5158
    },
    {
        "loss": 1.8966,
        "grad_norm": 3.2022531032562256,
        "learning_rate": 0.0001996212453802494,
        "epoch": 0.6878666666666666,
        "step": 5159
    },
    {
        "loss": 1.9336,
        "grad_norm": 3.0327348709106445,
        "learning_rate": 0.00019961575313941712,
        "epoch": 0.688,
        "step": 5160
    },
    {
        "loss": 2.815,
        "grad_norm": 2.8988420963287354,
        "learning_rate": 0.00019961022144044556,
        "epoch": 0.6881333333333334,
        "step": 5161
    },
    {
        "loss": 2.4425,
        "grad_norm": 3.139383554458618,
        "learning_rate": 0.0001996046502855259,
        "epoch": 0.6882666666666667,
        "step": 5162
    },
    {
        "loss": 2.5259,
        "grad_norm": 3.091871500015259,
        "learning_rate": 0.00019959903967686486,
        "epoch": 0.6884,
        "step": 5163
    },
    {
        "loss": 2.3282,
        "grad_norm": 5.9149861335754395,
        "learning_rate": 0.00019959338961668483,
        "epoch": 0.6885333333333333,
        "step": 5164
    },
    {
        "loss": 2.2423,
        "grad_norm": 3.1382884979248047,
        "learning_rate": 0.00019958770010722383,
        "epoch": 0.6886666666666666,
        "step": 5165
    },
    {
        "loss": 2.3327,
        "grad_norm": 3.5181541442871094,
        "learning_rate": 0.00019958197115073546,
        "epoch": 0.6888,
        "step": 5166
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.5583837032318115,
        "learning_rate": 0.000199576202749489,
        "epoch": 0.6889333333333333,
        "step": 5167
    },
    {
        "loss": 1.3448,
        "grad_norm": 3.6463634967803955,
        "learning_rate": 0.00019957039490576937,
        "epoch": 0.6890666666666667,
        "step": 5168
    },
    {
        "loss": 2.6914,
        "grad_norm": 4.158394813537598,
        "learning_rate": 0.000199564547621877,
        "epoch": 0.6892,
        "step": 5169
    },
    {
        "loss": 3.0264,
        "grad_norm": 3.200652837753296,
        "learning_rate": 0.00019955866090012807,
        "epoch": 0.6893333333333334,
        "step": 5170
    },
    {
        "loss": 2.3317,
        "grad_norm": 3.3590247631073,
        "learning_rate": 0.0001995527347428543,
        "epoch": 0.6894666666666667,
        "step": 5171
    },
    {
        "loss": 2.7452,
        "grad_norm": 3.0320940017700195,
        "learning_rate": 0.0001995467691524031,
        "epoch": 0.6896,
        "step": 5172
    },
    {
        "loss": 2.0082,
        "grad_norm": 3.5177488327026367,
        "learning_rate": 0.00019954076413113744,
        "epoch": 0.6897333333333333,
        "step": 5173
    },
    {
        "loss": 2.5569,
        "grad_norm": 4.199742794036865,
        "learning_rate": 0.00019953471968143596,
        "epoch": 0.6898666666666666,
        "step": 5174
    },
    {
        "loss": 1.0644,
        "grad_norm": 4.101823806762695,
        "learning_rate": 0.00019952863580569282,
        "epoch": 0.69,
        "step": 5175
    },
    {
        "loss": 1.623,
        "grad_norm": 3.074651002883911,
        "learning_rate": 0.0001995225125063179,
        "epoch": 0.6901333333333334,
        "step": 5176
    },
    {
        "loss": 2.249,
        "grad_norm": 3.8524630069732666,
        "learning_rate": 0.00019951634978573668,
        "epoch": 0.6902666666666667,
        "step": 5177
    },
    {
        "loss": 2.8414,
        "grad_norm": 2.1442854404449463,
        "learning_rate": 0.00019951014764639022,
        "epoch": 0.6904,
        "step": 5178
    },
    {
        "loss": 1.1377,
        "grad_norm": 4.119131088256836,
        "learning_rate": 0.0001995039060907352,
        "epoch": 0.6905333333333333,
        "step": 5179
    },
    {
        "loss": 2.2191,
        "grad_norm": 2.9419772624969482,
        "learning_rate": 0.00019949762512124392,
        "epoch": 0.6906666666666667,
        "step": 5180
    },
    {
        "loss": 2.1107,
        "grad_norm": 3.721905469894409,
        "learning_rate": 0.0001994913047404043,
        "epoch": 0.6908,
        "step": 5181
    },
    {
        "loss": 2.2413,
        "grad_norm": 2.585655927658081,
        "learning_rate": 0.00019948494495071985,
        "epoch": 0.6909333333333333,
        "step": 5182
    },
    {
        "loss": 2.63,
        "grad_norm": 3.534985303878784,
        "learning_rate": 0.00019947854575470974,
        "epoch": 0.6910666666666667,
        "step": 5183
    },
    {
        "loss": 2.9123,
        "grad_norm": 2.8540732860565186,
        "learning_rate": 0.0001994721071549087,
        "epoch": 0.6912,
        "step": 5184
    },
    {
        "loss": 1.8282,
        "grad_norm": 3.5289905071258545,
        "learning_rate": 0.00019946562915386707,
        "epoch": 0.6913333333333334,
        "step": 5185
    },
    {
        "loss": 2.3098,
        "grad_norm": 4.128145217895508,
        "learning_rate": 0.0001994591117541508,
        "epoch": 0.6914666666666667,
        "step": 5186
    },
    {
        "loss": 2.6288,
        "grad_norm": 4.012704849243164,
        "learning_rate": 0.00019945255495834143,
        "epoch": 0.6916,
        "step": 5187
    },
    {
        "loss": 2.9122,
        "grad_norm": 2.2975358963012695,
        "learning_rate": 0.00019944595876903622,
        "epoch": 0.6917333333333333,
        "step": 5188
    },
    {
        "loss": 2.7679,
        "grad_norm": 2.291252851486206,
        "learning_rate": 0.00019943932318884788,
        "epoch": 0.6918666666666666,
        "step": 5189
    },
    {
        "loss": 1.3874,
        "grad_norm": 2.153695583343506,
        "learning_rate": 0.00019943264822040477,
        "epoch": 0.692,
        "step": 5190
    },
    {
        "loss": 2.743,
        "grad_norm": 3.242133378982544,
        "learning_rate": 0.00019942593386635088,
        "epoch": 0.6921333333333334,
        "step": 5191
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.1225340366363525,
        "learning_rate": 0.00019941918012934587,
        "epoch": 0.6922666666666667,
        "step": 5192
    },
    {
        "loss": 2.3099,
        "grad_norm": 2.247140645980835,
        "learning_rate": 0.00019941238701206478,
        "epoch": 0.6924,
        "step": 5193
    },
    {
        "loss": 2.6087,
        "grad_norm": 2.994751453399658,
        "learning_rate": 0.00019940555451719848,
        "epoch": 0.6925333333333333,
        "step": 5194
    },
    {
        "loss": 2.8667,
        "grad_norm": 2.924075126647949,
        "learning_rate": 0.00019939868264745329,
        "epoch": 0.6926666666666667,
        "step": 5195
    },
    {
        "loss": 2.5776,
        "grad_norm": 3.4389755725860596,
        "learning_rate": 0.00019939177140555124,
        "epoch": 0.6928,
        "step": 5196
    },
    {
        "loss": 2.233,
        "grad_norm": 2.531419038772583,
        "learning_rate": 0.00019938482079422988,
        "epoch": 0.6929333333333333,
        "step": 5197
    },
    {
        "loss": 2.013,
        "grad_norm": 2.968759059906006,
        "learning_rate": 0.00019937783081624234,
        "epoch": 0.6930666666666667,
        "step": 5198
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.106437921524048,
        "learning_rate": 0.0001993708014743574,
        "epoch": 0.6932,
        "step": 5199
    },
    {
        "loss": 2.0131,
        "grad_norm": 4.156862258911133,
        "learning_rate": 0.0001993637327713594,
        "epoch": 0.6933333333333334,
        "step": 5200
    },
    {
        "loss": 2.857,
        "grad_norm": 2.494135618209839,
        "learning_rate": 0.00019935662471004827,
        "epoch": 0.6934666666666667,
        "step": 5201
    },
    {
        "loss": 2.2805,
        "grad_norm": 4.301651954650879,
        "learning_rate": 0.00019934947729323952,
        "epoch": 0.6936,
        "step": 5202
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.989419937133789,
        "learning_rate": 0.0001993422905237643,
        "epoch": 0.6937333333333333,
        "step": 5203
    },
    {
        "loss": 1.497,
        "grad_norm": 4.646087169647217,
        "learning_rate": 0.0001993350644044693,
        "epoch": 0.6938666666666666,
        "step": 5204
    },
    {
        "loss": 2.8531,
        "grad_norm": 3.180257558822632,
        "learning_rate": 0.00019932779893821686,
        "epoch": 0.694,
        "step": 5205
    },
    {
        "loss": 2.8075,
        "grad_norm": 3.3684282302856445,
        "learning_rate": 0.00019932049412788478,
        "epoch": 0.6941333333333334,
        "step": 5206
    },
    {
        "loss": 2.6134,
        "grad_norm": 2.4293038845062256,
        "learning_rate": 0.00019931314997636654,
        "epoch": 0.6942666666666667,
        "step": 5207
    },
    {
        "loss": 1.7636,
        "grad_norm": 2.8690903186798096,
        "learning_rate": 0.0001993057664865712,
        "epoch": 0.6944,
        "step": 5208
    },
    {
        "loss": 2.217,
        "grad_norm": 3.3331892490386963,
        "learning_rate": 0.00019929834366142336,
        "epoch": 0.6945333333333333,
        "step": 5209
    },
    {
        "loss": 2.6427,
        "grad_norm": 3.1432013511657715,
        "learning_rate": 0.00019929088150386328,
        "epoch": 0.6946666666666667,
        "step": 5210
    },
    {
        "loss": 2.6382,
        "grad_norm": 3.858555793762207,
        "learning_rate": 0.00019928338001684668,
        "epoch": 0.6948,
        "step": 5211
    },
    {
        "loss": 1.8083,
        "grad_norm": 3.478825092315674,
        "learning_rate": 0.00019927583920334495,
        "epoch": 0.6949333333333333,
        "step": 5212
    },
    {
        "loss": 2.3102,
        "grad_norm": 2.9510483741760254,
        "learning_rate": 0.00019926825906634507,
        "epoch": 0.6950666666666667,
        "step": 5213
    },
    {
        "loss": 1.854,
        "grad_norm": 2.673774242401123,
        "learning_rate": 0.00019926063960884954,
        "epoch": 0.6952,
        "step": 5214
    },
    {
        "loss": 1.7974,
        "grad_norm": 3.104337453842163,
        "learning_rate": 0.00019925298083387638,
        "epoch": 0.6953333333333334,
        "step": 5215
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.6437883377075195,
        "learning_rate": 0.00019924528274445935,
        "epoch": 0.6954666666666667,
        "step": 5216
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.9890365600585938,
        "learning_rate": 0.00019923754534364766,
        "epoch": 0.6956,
        "step": 5217
    },
    {
        "loss": 0.6842,
        "grad_norm": 2.364813804626465,
        "learning_rate": 0.0001992297686345061,
        "epoch": 0.6957333333333333,
        "step": 5218
    },
    {
        "loss": 3.375,
        "grad_norm": 2.9364805221557617,
        "learning_rate": 0.00019922195262011508,
        "epoch": 0.6958666666666666,
        "step": 5219
    },
    {
        "loss": 2.3798,
        "grad_norm": 2.2762563228607178,
        "learning_rate": 0.00019921409730357052,
        "epoch": 0.696,
        "step": 5220
    },
    {
        "loss": 3.0811,
        "grad_norm": 3.532745838165283,
        "learning_rate": 0.00019920620268798397,
        "epoch": 0.6961333333333334,
        "step": 5221
    },
    {
        "loss": 2.616,
        "grad_norm": 3.2825539112091064,
        "learning_rate": 0.0001991982687764825,
        "epoch": 0.6962666666666667,
        "step": 5222
    },
    {
        "loss": 2.1761,
        "grad_norm": 3.768688917160034,
        "learning_rate": 0.00019919029557220874,
        "epoch": 0.6964,
        "step": 5223
    },
    {
        "loss": 2.5541,
        "grad_norm": 3.795048713684082,
        "learning_rate": 0.00019918228307832093,
        "epoch": 0.6965333333333333,
        "step": 5224
    },
    {
        "loss": 1.9549,
        "grad_norm": 4.45460844039917,
        "learning_rate": 0.00019917423129799284,
        "epoch": 0.6966666666666667,
        "step": 5225
    },
    {
        "loss": 2.5087,
        "grad_norm": 2.607822895050049,
        "learning_rate": 0.0001991661402344138,
        "epoch": 0.6968,
        "step": 5226
    },
    {
        "loss": 2.8164,
        "grad_norm": 2.53147292137146,
        "learning_rate": 0.00019915800989078872,
        "epoch": 0.6969333333333333,
        "step": 5227
    },
    {
        "loss": 2.7487,
        "grad_norm": 2.877119779586792,
        "learning_rate": 0.00019914984027033804,
        "epoch": 0.6970666666666666,
        "step": 5228
    },
    {
        "loss": 2.3986,
        "grad_norm": 3.847935676574707,
        "learning_rate": 0.0001991416313762978,
        "epoch": 0.6972,
        "step": 5229
    },
    {
        "loss": 2.1535,
        "grad_norm": 3.6622314453125,
        "learning_rate": 0.00019913338321191955,
        "epoch": 0.6973333333333334,
        "step": 5230
    },
    {
        "loss": 2.6676,
        "grad_norm": 2.9911139011383057,
        "learning_rate": 0.00019912509578047042,
        "epoch": 0.6974666666666667,
        "step": 5231
    },
    {
        "loss": 2.689,
        "grad_norm": 3.457152843475342,
        "learning_rate": 0.00019911676908523308,
        "epoch": 0.6976,
        "step": 5232
    },
    {
        "loss": 2.3243,
        "grad_norm": 4.045319557189941,
        "learning_rate": 0.00019910840312950578,
        "epoch": 0.6977333333333333,
        "step": 5233
    },
    {
        "loss": 2.8073,
        "grad_norm": 2.495729923248291,
        "learning_rate": 0.0001990999979166023,
        "epoch": 0.6978666666666666,
        "step": 5234
    },
    {
        "loss": 1.6357,
        "grad_norm": 4.742962837219238,
        "learning_rate": 0.00019909155344985196,
        "epoch": 0.698,
        "step": 5235
    },
    {
        "loss": 2.4196,
        "grad_norm": 3.4004158973693848,
        "learning_rate": 0.00019908306973259965,
        "epoch": 0.6981333333333334,
        "step": 5236
    },
    {
        "loss": 1.903,
        "grad_norm": 4.875508785247803,
        "learning_rate": 0.0001990745467682058,
        "epoch": 0.6982666666666667,
        "step": 5237
    },
    {
        "loss": 1.7316,
        "grad_norm": 3.5288171768188477,
        "learning_rate": 0.00019906598456004643,
        "epoch": 0.6984,
        "step": 5238
    },
    {
        "loss": 2.54,
        "grad_norm": 3.2672200202941895,
        "learning_rate": 0.00019905738311151296,
        "epoch": 0.6985333333333333,
        "step": 5239
    },
    {
        "loss": 2.643,
        "grad_norm": 3.1208715438842773,
        "learning_rate": 0.00019904874242601255,
        "epoch": 0.6986666666666667,
        "step": 5240
    },
    {
        "loss": 1.7795,
        "grad_norm": 3.328457832336426,
        "learning_rate": 0.00019904006250696777,
        "epoch": 0.6988,
        "step": 5241
    },
    {
        "loss": 1.9637,
        "grad_norm": 3.3594071865081787,
        "learning_rate": 0.00019903134335781672,
        "epoch": 0.6989333333333333,
        "step": 5242
    },
    {
        "loss": 2.4103,
        "grad_norm": 3.3693835735321045,
        "learning_rate": 0.00019902258498201314,
        "epoch": 0.6990666666666666,
        "step": 5243
    },
    {
        "loss": 2.0889,
        "grad_norm": 3.839278221130371,
        "learning_rate": 0.00019901378738302624,
        "epoch": 0.6992,
        "step": 5244
    },
    {
        "loss": 2.3684,
        "grad_norm": 3.1005823612213135,
        "learning_rate": 0.00019900495056434078,
        "epoch": 0.6993333333333334,
        "step": 5245
    },
    {
        "loss": 1.3961,
        "grad_norm": 4.737524509429932,
        "learning_rate": 0.00019899607452945705,
        "epoch": 0.6994666666666667,
        "step": 5246
    },
    {
        "loss": 2.2979,
        "grad_norm": 3.5046727657318115,
        "learning_rate": 0.00019898715928189087,
        "epoch": 0.6996,
        "step": 5247
    },
    {
        "loss": 2.5371,
        "grad_norm": 3.525716543197632,
        "learning_rate": 0.00019897820482517363,
        "epoch": 0.6997333333333333,
        "step": 5248
    },
    {
        "loss": 2.7048,
        "grad_norm": 4.571484088897705,
        "learning_rate": 0.0001989692111628522,
        "epoch": 0.6998666666666666,
        "step": 5249
    },
    {
        "loss": 1.9916,
        "grad_norm": 4.039708137512207,
        "learning_rate": 0.00019896017829848896,
        "epoch": 0.7,
        "step": 5250
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.3916029930114746,
        "learning_rate": 0.00019895110623566192,
        "epoch": 0.7001333333333334,
        "step": 5251
    },
    {
        "loss": 2.5663,
        "grad_norm": 3.183807134628296,
        "learning_rate": 0.00019894199497796453,
        "epoch": 0.7002666666666667,
        "step": 5252
    },
    {
        "loss": 3.456,
        "grad_norm": 2.595806837081909,
        "learning_rate": 0.0001989328445290058,
        "epoch": 0.7004,
        "step": 5253
    },
    {
        "loss": 2.5024,
        "grad_norm": 3.334202289581299,
        "learning_rate": 0.00019892365489241023,
        "epoch": 0.7005333333333333,
        "step": 5254
    },
    {
        "loss": 2.5253,
        "grad_norm": 1.893941879272461,
        "learning_rate": 0.0001989144260718179,
        "epoch": 0.7006666666666667,
        "step": 5255
    },
    {
        "loss": 2.3488,
        "grad_norm": 1.9474782943725586,
        "learning_rate": 0.00019890515807088438,
        "epoch": 0.7008,
        "step": 5256
    },
    {
        "loss": 2.5011,
        "grad_norm": 3.476851224899292,
        "learning_rate": 0.00019889585089328068,
        "epoch": 0.7009333333333333,
        "step": 5257
    },
    {
        "loss": 2.9631,
        "grad_norm": 2.06943678855896,
        "learning_rate": 0.00019888650454269352,
        "epoch": 0.7010666666666666,
        "step": 5258
    },
    {
        "loss": 2.8503,
        "grad_norm": 2.13478684425354,
        "learning_rate": 0.0001988771190228249,
        "epoch": 0.7012,
        "step": 5259
    },
    {
        "loss": 1.7612,
        "grad_norm": 4.094030857086182,
        "learning_rate": 0.00019886769433739257,
        "epoch": 0.7013333333333334,
        "step": 5260
    },
    {
        "loss": 2.2106,
        "grad_norm": 2.7456486225128174,
        "learning_rate": 0.0001988582304901296,
        "epoch": 0.7014666666666667,
        "step": 5261
    },
    {
        "loss": 0.8578,
        "grad_norm": 4.7607645988464355,
        "learning_rate": 0.00019884872748478474,
        "epoch": 0.7016,
        "step": 5262
    },
    {
        "loss": 2.6453,
        "grad_norm": 3.390693426132202,
        "learning_rate": 0.00019883918532512205,
        "epoch": 0.7017333333333333,
        "step": 5263
    },
    {
        "loss": 1.9564,
        "grad_norm": 3.522758960723877,
        "learning_rate": 0.00019882960401492128,
        "epoch": 0.7018666666666666,
        "step": 5264
    },
    {
        "loss": 3.093,
        "grad_norm": 1.230861783027649,
        "learning_rate": 0.00019881998355797758,
        "epoch": 0.702,
        "step": 5265
    },
    {
        "loss": 2.6086,
        "grad_norm": 2.239617347717285,
        "learning_rate": 0.00019881032395810172,
        "epoch": 0.7021333333333334,
        "step": 5266
    },
    {
        "loss": 2.1167,
        "grad_norm": 4.000277996063232,
        "learning_rate": 0.0001988006252191198,
        "epoch": 0.7022666666666667,
        "step": 5267
    },
    {
        "loss": 2.6137,
        "grad_norm": 1.6429193019866943,
        "learning_rate": 0.00019879088734487362,
        "epoch": 0.7024,
        "step": 5268
    },
    {
        "loss": 1.6658,
        "grad_norm": 4.31428861618042,
        "learning_rate": 0.00019878111033922032,
        "epoch": 0.7025333333333333,
        "step": 5269
    },
    {
        "loss": 2.7238,
        "grad_norm": 3.6020045280456543,
        "learning_rate": 0.00019877129420603265,
        "epoch": 0.7026666666666667,
        "step": 5270
    },
    {
        "loss": 3.0715,
        "grad_norm": 2.408177614212036,
        "learning_rate": 0.00019876143894919876,
        "epoch": 0.7028,
        "step": 5271
    },
    {
        "loss": 2.3414,
        "grad_norm": 2.096177816390991,
        "learning_rate": 0.0001987515445726224,
        "epoch": 0.7029333333333333,
        "step": 5272
    },
    {
        "loss": 1.5311,
        "grad_norm": 5.587564945220947,
        "learning_rate": 0.00019874161108022275,
        "epoch": 0.7030666666666666,
        "step": 5273
    },
    {
        "loss": 2.898,
        "grad_norm": 2.7320477962493896,
        "learning_rate": 0.00019873163847593448,
        "epoch": 0.7032,
        "step": 5274
    },
    {
        "loss": 2.1511,
        "grad_norm": 4.166131496429443,
        "learning_rate": 0.0001987216267637078,
        "epoch": 0.7033333333333334,
        "step": 5275
    },
    {
        "loss": 2.0761,
        "grad_norm": 2.9082648754119873,
        "learning_rate": 0.00019871157594750834,
        "epoch": 0.7034666666666667,
        "step": 5276
    },
    {
        "loss": 2.1788,
        "grad_norm": 2.8925704956054688,
        "learning_rate": 0.00019870148603131735,
        "epoch": 0.7036,
        "step": 5277
    },
    {
        "loss": 1.3113,
        "grad_norm": 2.3194615840911865,
        "learning_rate": 0.0001986913570191314,
        "epoch": 0.7037333333333333,
        "step": 5278
    },
    {
        "loss": 2.3833,
        "grad_norm": 2.6848864555358887,
        "learning_rate": 0.00019868118891496267,
        "epoch": 0.7038666666666666,
        "step": 5279
    },
    {
        "loss": 2.6226,
        "grad_norm": 2.9992258548736572,
        "learning_rate": 0.00019867098172283873,
        "epoch": 0.704,
        "step": 5280
    },
    {
        "loss": 2.3726,
        "grad_norm": 2.4298369884490967,
        "learning_rate": 0.00019866073544680274,
        "epoch": 0.7041333333333334,
        "step": 5281
    },
    {
        "loss": 1.9804,
        "grad_norm": 4.098055362701416,
        "learning_rate": 0.00019865045009091325,
        "epoch": 0.7042666666666667,
        "step": 5282
    },
    {
        "loss": 2.6679,
        "grad_norm": 3.1385390758514404,
        "learning_rate": 0.00019864012565924436,
        "epoch": 0.7044,
        "step": 5283
    },
    {
        "loss": 1.6411,
        "grad_norm": 3.5072600841522217,
        "learning_rate": 0.00019862976215588555,
        "epoch": 0.7045333333333333,
        "step": 5284
    },
    {
        "loss": 2.1129,
        "grad_norm": 2.365374803543091,
        "learning_rate": 0.0001986193595849419,
        "epoch": 0.7046666666666667,
        "step": 5285
    },
    {
        "loss": 2.1393,
        "grad_norm": 3.8519575595855713,
        "learning_rate": 0.00019860891795053386,
        "epoch": 0.7048,
        "step": 5286
    },
    {
        "loss": 2.1658,
        "grad_norm": 6.082248210906982,
        "learning_rate": 0.00019859843725679745,
        "epoch": 0.7049333333333333,
        "step": 5287
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.060877561569214,
        "learning_rate": 0.00019858791750788405,
        "epoch": 0.7050666666666666,
        "step": 5288
    },
    {
        "loss": 2.6246,
        "grad_norm": 3.8756582736968994,
        "learning_rate": 0.00019857735870796063,
        "epoch": 0.7052,
        "step": 5289
    },
    {
        "loss": 2.7165,
        "grad_norm": 2.871079206466675,
        "learning_rate": 0.00019856676086120952,
        "epoch": 0.7053333333333334,
        "step": 5290
    },
    {
        "loss": 2.0818,
        "grad_norm": 4.672671318054199,
        "learning_rate": 0.00019855612397182853,
        "epoch": 0.7054666666666667,
        "step": 5291
    },
    {
        "loss": 1.9899,
        "grad_norm": 2.461376667022705,
        "learning_rate": 0.00019854544804403105,
        "epoch": 0.7056,
        "step": 5292
    },
    {
        "loss": 2.1455,
        "grad_norm": 3.099630355834961,
        "learning_rate": 0.00019853473308204583,
        "epoch": 0.7057333333333333,
        "step": 5293
    },
    {
        "loss": 2.1757,
        "grad_norm": 3.064758777618408,
        "learning_rate": 0.0001985239790901171,
        "epoch": 0.7058666666666666,
        "step": 5294
    },
    {
        "loss": 2.2704,
        "grad_norm": 3.1863386631011963,
        "learning_rate": 0.00019851318607250445,
        "epoch": 0.706,
        "step": 5295
    },
    {
        "loss": 2.2887,
        "grad_norm": 2.964045763015747,
        "learning_rate": 0.0001985023540334832,
        "epoch": 0.7061333333333333,
        "step": 5296
    },
    {
        "loss": 2.9041,
        "grad_norm": 2.6648852825164795,
        "learning_rate": 0.00019849148297734385,
        "epoch": 0.7062666666666667,
        "step": 5297
    },
    {
        "loss": 2.5039,
        "grad_norm": 5.198060989379883,
        "learning_rate": 0.0001984805729083925,
        "epoch": 0.7064,
        "step": 5298
    },
    {
        "loss": 1.7759,
        "grad_norm": 2.5885064601898193,
        "learning_rate": 0.00019846962383095066,
        "epoch": 0.7065333333333333,
        "step": 5299
    },
    {
        "loss": 2.1946,
        "grad_norm": 3.9477481842041016,
        "learning_rate": 0.00019845863574935533,
        "epoch": 0.7066666666666667,
        "step": 5300
    },
    {
        "loss": 1.8713,
        "grad_norm": 7.785982608795166,
        "learning_rate": 0.00019844760866795884,
        "epoch": 0.7068,
        "step": 5301
    },
    {
        "loss": 2.8691,
        "grad_norm": 3.684964179992676,
        "learning_rate": 0.00019843654259112909,
        "epoch": 0.7069333333333333,
        "step": 5302
    },
    {
        "loss": 1.6336,
        "grad_norm": 1.7940841913223267,
        "learning_rate": 0.00019842543752324944,
        "epoch": 0.7070666666666666,
        "step": 5303
    },
    {
        "loss": 2.4391,
        "grad_norm": 2.4522688388824463,
        "learning_rate": 0.0001984142934687186,
        "epoch": 0.7072,
        "step": 5304
    },
    {
        "loss": 1.973,
        "grad_norm": 5.254283428192139,
        "learning_rate": 0.00019840311043195077,
        "epoch": 0.7073333333333334,
        "step": 5305
    },
    {
        "loss": 1.8466,
        "grad_norm": 3.029904842376709,
        "learning_rate": 0.00019839188841737563,
        "epoch": 0.7074666666666667,
        "step": 5306
    },
    {
        "loss": 2.4125,
        "grad_norm": 2.586932897567749,
        "learning_rate": 0.0001983806274294382,
        "epoch": 0.7076,
        "step": 5307
    },
    {
        "loss": 2.291,
        "grad_norm": 3.236063003540039,
        "learning_rate": 0.00019836932747259903,
        "epoch": 0.7077333333333333,
        "step": 5308
    },
    {
        "loss": 2.5547,
        "grad_norm": 3.944978713989258,
        "learning_rate": 0.00019835798855133405,
        "epoch": 0.7078666666666666,
        "step": 5309
    },
    {
        "loss": 2.6492,
        "grad_norm": 5.682470321655273,
        "learning_rate": 0.00019834661067013465,
        "epoch": 0.708,
        "step": 5310
    },
    {
        "loss": 2.0624,
        "grad_norm": 3.1721274852752686,
        "learning_rate": 0.00019833519383350768,
        "epoch": 0.7081333333333333,
        "step": 5311
    },
    {
        "loss": 3.0009,
        "grad_norm": 2.1642675399780273,
        "learning_rate": 0.00019832373804597536,
        "epoch": 0.7082666666666667,
        "step": 5312
    },
    {
        "loss": 1.5925,
        "grad_norm": 7.124103546142578,
        "learning_rate": 0.00019831224331207534,
        "epoch": 0.7084,
        "step": 5313
    },
    {
        "loss": 2.8371,
        "grad_norm": 4.657803058624268,
        "learning_rate": 0.0001983007096363608,
        "epoch": 0.7085333333333333,
        "step": 5314
    },
    {
        "loss": 1.3182,
        "grad_norm": 4.095045566558838,
        "learning_rate": 0.0001982891370234002,
        "epoch": 0.7086666666666667,
        "step": 5315
    },
    {
        "loss": 2.2635,
        "grad_norm": 3.3637564182281494,
        "learning_rate": 0.00019827752547777751,
        "epoch": 0.7088,
        "step": 5316
    },
    {
        "loss": 1.3958,
        "grad_norm": 5.064047336578369,
        "learning_rate": 0.00019826587500409212,
        "epoch": 0.7089333333333333,
        "step": 5317
    },
    {
        "loss": 2.4269,
        "grad_norm": 5.6579437255859375,
        "learning_rate": 0.0001982541856069588,
        "epoch": 0.7090666666666666,
        "step": 5318
    },
    {
        "loss": 2.7552,
        "grad_norm": 2.481397867202759,
        "learning_rate": 0.00019824245729100777,
        "epoch": 0.7092,
        "step": 5319
    },
    {
        "loss": 1.2222,
        "grad_norm": 4.830408573150635,
        "learning_rate": 0.0001982306900608846,
        "epoch": 0.7093333333333334,
        "step": 5320
    },
    {
        "loss": 1.2405,
        "grad_norm": 4.024074077606201,
        "learning_rate": 0.00019821888392125046,
        "epoch": 0.7094666666666667,
        "step": 5321
    },
    {
        "loss": 2.8764,
        "grad_norm": 2.277127504348755,
        "learning_rate": 0.0001982070388767817,
        "epoch": 0.7096,
        "step": 5322
    },
    {
        "loss": 2.6523,
        "grad_norm": 2.7883529663085938,
        "learning_rate": 0.0001981951549321702,
        "epoch": 0.7097333333333333,
        "step": 5323
    },
    {
        "loss": 1.7059,
        "grad_norm": 3.092177391052246,
        "learning_rate": 0.00019818323209212327,
        "epoch": 0.7098666666666666,
        "step": 5324
    },
    {
        "loss": 2.5985,
        "grad_norm": 3.060925245285034,
        "learning_rate": 0.00019817127036136353,
        "epoch": 0.71,
        "step": 5325
    },
    {
        "loss": 2.6917,
        "grad_norm": 3.0664236545562744,
        "learning_rate": 0.00019815926974462908,
        "epoch": 0.7101333333333333,
        "step": 5326
    },
    {
        "loss": 2.6966,
        "grad_norm": 3.0266311168670654,
        "learning_rate": 0.00019814723024667342,
        "epoch": 0.7102666666666667,
        "step": 5327
    },
    {
        "loss": 1.2445,
        "grad_norm": 2.6432745456695557,
        "learning_rate": 0.00019813515187226547,
        "epoch": 0.7104,
        "step": 5328
    },
    {
        "loss": 1.0872,
        "grad_norm": 3.9048407077789307,
        "learning_rate": 0.00019812303462618945,
        "epoch": 0.7105333333333334,
        "step": 5329
    },
    {
        "loss": 2.0953,
        "grad_norm": 3.918619394302368,
        "learning_rate": 0.00019811087851324505,
        "epoch": 0.7106666666666667,
        "step": 5330
    },
    {
        "loss": 2.3489,
        "grad_norm": 2.4893484115600586,
        "learning_rate": 0.00019809868353824737,
        "epoch": 0.7108,
        "step": 5331
    },
    {
        "loss": 2.7536,
        "grad_norm": 3.7355775833129883,
        "learning_rate": 0.0001980864497060269,
        "epoch": 0.7109333333333333,
        "step": 5332
    },
    {
        "loss": 2.0765,
        "grad_norm": 3.7730777263641357,
        "learning_rate": 0.00019807417702142946,
        "epoch": 0.7110666666666666,
        "step": 5333
    },
    {
        "loss": 2.3041,
        "grad_norm": 2.571096420288086,
        "learning_rate": 0.00019806186548931634,
        "epoch": 0.7112,
        "step": 5334
    },
    {
        "loss": 2.5,
        "grad_norm": 3.28463077545166,
        "learning_rate": 0.00019804951511456412,
        "epoch": 0.7113333333333334,
        "step": 5335
    },
    {
        "loss": 2.9136,
        "grad_norm": 2.7952520847320557,
        "learning_rate": 0.00019803712590206493,
        "epoch": 0.7114666666666667,
        "step": 5336
    },
    {
        "loss": 2.9877,
        "grad_norm": 2.669844627380371,
        "learning_rate": 0.0001980246978567261,
        "epoch": 0.7116,
        "step": 5337
    },
    {
        "loss": 1.4108,
        "grad_norm": 4.9864654541015625,
        "learning_rate": 0.0001980122309834704,
        "epoch": 0.7117333333333333,
        "step": 5338
    },
    {
        "loss": 2.2569,
        "grad_norm": 3.203577756881714,
        "learning_rate": 0.00019799972528723606,
        "epoch": 0.7118666666666666,
        "step": 5339
    },
    {
        "loss": 1.9871,
        "grad_norm": 3.122407913208008,
        "learning_rate": 0.0001979871807729766,
        "epoch": 0.712,
        "step": 5340
    },
    {
        "loss": 3.124,
        "grad_norm": 2.596383571624756,
        "learning_rate": 0.000197974597445661,
        "epoch": 0.7121333333333333,
        "step": 5341
    },
    {
        "loss": 2.148,
        "grad_norm": 3.9130005836486816,
        "learning_rate": 0.00019796197531027346,
        "epoch": 0.7122666666666667,
        "step": 5342
    },
    {
        "loss": 2.2326,
        "grad_norm": 3.306785821914673,
        "learning_rate": 0.0001979493143718137,
        "epoch": 0.7124,
        "step": 5343
    },
    {
        "loss": 2.6463,
        "grad_norm": 3.327979803085327,
        "learning_rate": 0.0001979366146352968,
        "epoch": 0.7125333333333334,
        "step": 5344
    },
    {
        "loss": 3.4092,
        "grad_norm": 3.7362990379333496,
        "learning_rate": 0.0001979238761057531,
        "epoch": 0.7126666666666667,
        "step": 5345
    },
    {
        "loss": 2.3589,
        "grad_norm": 3.0363733768463135,
        "learning_rate": 0.00019791109878822843,
        "epoch": 0.7128,
        "step": 5346
    },
    {
        "loss": 1.5308,
        "grad_norm": 5.45327091217041,
        "learning_rate": 0.0001978982826877839,
        "epoch": 0.7129333333333333,
        "step": 5347
    },
    {
        "loss": 2.6356,
        "grad_norm": 2.897768259048462,
        "learning_rate": 0.00019788542780949602,
        "epoch": 0.7130666666666666,
        "step": 5348
    },
    {
        "loss": 2.8275,
        "grad_norm": 3.6450846195220947,
        "learning_rate": 0.00019787253415845665,
        "epoch": 0.7132,
        "step": 5349
    },
    {
        "loss": 1.2942,
        "grad_norm": 3.834310293197632,
        "learning_rate": 0.00019785960173977296,
        "epoch": 0.7133333333333334,
        "step": 5350
    },
    {
        "loss": 1.9711,
        "grad_norm": 4.149840831756592,
        "learning_rate": 0.00019784663055856766,
        "epoch": 0.7134666666666667,
        "step": 5351
    },
    {
        "loss": 2.6454,
        "grad_norm": 1.6708189249038696,
        "learning_rate": 0.0001978336206199785,
        "epoch": 0.7136,
        "step": 5352
    },
    {
        "loss": 3.0219,
        "grad_norm": 3.774454355239868,
        "learning_rate": 0.0001978205719291589,
        "epoch": 0.7137333333333333,
        "step": 5353
    },
    {
        "loss": 2.1315,
        "grad_norm": 2.256963014602661,
        "learning_rate": 0.00019780748449127743,
        "epoch": 0.7138666666666666,
        "step": 5354
    },
    {
        "loss": 2.5783,
        "grad_norm": 3.5227460861206055,
        "learning_rate": 0.0001977943583115181,
        "epoch": 0.714,
        "step": 5355
    },
    {
        "loss": 2.5025,
        "grad_norm": 3.7420241832733154,
        "learning_rate": 0.0001977811933950802,
        "epoch": 0.7141333333333333,
        "step": 5356
    },
    {
        "loss": 1.7483,
        "grad_norm": 3.5911102294921875,
        "learning_rate": 0.00019776798974717843,
        "epoch": 0.7142666666666667,
        "step": 5357
    },
    {
        "loss": 2.093,
        "grad_norm": 4.494291305541992,
        "learning_rate": 0.00019775474737304278,
        "epoch": 0.7144,
        "step": 5358
    },
    {
        "loss": 3.2894,
        "grad_norm": 4.432532787322998,
        "learning_rate": 0.00019774146627791862,
        "epoch": 0.7145333333333334,
        "step": 5359
    },
    {
        "loss": 2.323,
        "grad_norm": 3.6220812797546387,
        "learning_rate": 0.0001977281464670666,
        "epoch": 0.7146666666666667,
        "step": 5360
    },
    {
        "loss": 2.1192,
        "grad_norm": 3.1017651557922363,
        "learning_rate": 0.00019771478794576277,
        "epoch": 0.7148,
        "step": 5361
    },
    {
        "loss": 2.0579,
        "grad_norm": 3.364549398422241,
        "learning_rate": 0.00019770139071929846,
        "epoch": 0.7149333333333333,
        "step": 5362
    },
    {
        "loss": 2.1673,
        "grad_norm": 4.6856513023376465,
        "learning_rate": 0.0001976879547929804,
        "epoch": 0.7150666666666666,
        "step": 5363
    },
    {
        "loss": 2.3499,
        "grad_norm": 3.1360762119293213,
        "learning_rate": 0.00019767448017213058,
        "epoch": 0.7152,
        "step": 5364
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.9329324960708618,
        "learning_rate": 0.00019766096686208635,
        "epoch": 0.7153333333333334,
        "step": 5365
    },
    {
        "loss": 1.858,
        "grad_norm": 3.7081215381622314,
        "learning_rate": 0.00019764741486820038,
        "epoch": 0.7154666666666667,
        "step": 5366
    },
    {
        "loss": 2.4338,
        "grad_norm": 4.833047389984131,
        "learning_rate": 0.00019763382419584066,
        "epoch": 0.7156,
        "step": 5367
    },
    {
        "loss": 2.3423,
        "grad_norm": 2.6172423362731934,
        "learning_rate": 0.00019762019485039046,
        "epoch": 0.7157333333333333,
        "step": 5368
    },
    {
        "loss": 2.4964,
        "grad_norm": 3.903904914855957,
        "learning_rate": 0.00019760652683724846,
        "epoch": 0.7158666666666667,
        "step": 5369
    },
    {
        "loss": 2.412,
        "grad_norm": 2.2841684818267822,
        "learning_rate": 0.00019759282016182858,
        "epoch": 0.716,
        "step": 5370
    },
    {
        "loss": 2.7558,
        "grad_norm": 3.207125425338745,
        "learning_rate": 0.00019757907482956014,
        "epoch": 0.7161333333333333,
        "step": 5371
    },
    {
        "loss": 2.7431,
        "grad_norm": 3.3078434467315674,
        "learning_rate": 0.00019756529084588765,
        "epoch": 0.7162666666666667,
        "step": 5372
    },
    {
        "loss": 2.9699,
        "grad_norm": 3.764615535736084,
        "learning_rate": 0.00019755146821627098,
        "epoch": 0.7164,
        "step": 5373
    },
    {
        "loss": 2.8213,
        "grad_norm": 3.7439351081848145,
        "learning_rate": 0.00019753760694618538,
        "epoch": 0.7165333333333334,
        "step": 5374
    },
    {
        "loss": 1.6579,
        "grad_norm": 3.370187282562256,
        "learning_rate": 0.0001975237070411213,
        "epoch": 0.7166666666666667,
        "step": 5375
    },
    {
        "loss": 2.4017,
        "grad_norm": 5.438174724578857,
        "learning_rate": 0.00019750976850658458,
        "epoch": 0.7168,
        "step": 5376
    },
    {
        "loss": 1.9638,
        "grad_norm": 3.8675923347473145,
        "learning_rate": 0.00019749579134809627,
        "epoch": 0.7169333333333333,
        "step": 5377
    },
    {
        "loss": 2.6639,
        "grad_norm": 3.857983112335205,
        "learning_rate": 0.00019748177557119285,
        "epoch": 0.7170666666666666,
        "step": 5378
    },
    {
        "loss": 2.9581,
        "grad_norm": 3.7974050045013428,
        "learning_rate": 0.00019746772118142588,
        "epoch": 0.7172,
        "step": 5379
    },
    {
        "loss": 1.2611,
        "grad_norm": 7.3087687492370605,
        "learning_rate": 0.00019745362818436251,
        "epoch": 0.7173333333333334,
        "step": 5380
    },
    {
        "loss": 3.2075,
        "grad_norm": 2.9742929935455322,
        "learning_rate": 0.00019743949658558493,
        "epoch": 0.7174666666666667,
        "step": 5381
    },
    {
        "loss": 2.6446,
        "grad_norm": 5.2677321434021,
        "learning_rate": 0.00019742532639069075,
        "epoch": 0.7176,
        "step": 5382
    },
    {
        "loss": 2.4559,
        "grad_norm": 2.565732002258301,
        "learning_rate": 0.00019741111760529278,
        "epoch": 0.7177333333333333,
        "step": 5383
    },
    {
        "loss": 2.1988,
        "grad_norm": 5.82450008392334,
        "learning_rate": 0.00019739687023501924,
        "epoch": 0.7178666666666667,
        "step": 5384
    },
    {
        "loss": 2.7355,
        "grad_norm": 6.247633934020996,
        "learning_rate": 0.00019738258428551351,
        "epoch": 0.718,
        "step": 5385
    },
    {
        "loss": 2.2501,
        "grad_norm": 4.11216402053833,
        "learning_rate": 0.0001973682597624343,
        "epoch": 0.7181333333333333,
        "step": 5386
    },
    {
        "loss": 2.4694,
        "grad_norm": 4.269083023071289,
        "learning_rate": 0.0001973538966714557,
        "epoch": 0.7182666666666667,
        "step": 5387
    },
    {
        "loss": 2.812,
        "grad_norm": 2.8779354095458984,
        "learning_rate": 0.00019733949501826684,
        "epoch": 0.7184,
        "step": 5388
    },
    {
        "loss": 2.4318,
        "grad_norm": 2.4673266410827637,
        "learning_rate": 0.00019732505480857233,
        "epoch": 0.7185333333333334,
        "step": 5389
    },
    {
        "loss": 2.106,
        "grad_norm": 4.151945114135742,
        "learning_rate": 0.000197310576048092,
        "epoch": 0.7186666666666667,
        "step": 5390
    },
    {
        "loss": 2.636,
        "grad_norm": 2.2137365341186523,
        "learning_rate": 0.0001972960587425609,
        "epoch": 0.7188,
        "step": 5391
    },
    {
        "loss": 1.2867,
        "grad_norm": 4.347928047180176,
        "learning_rate": 0.00019728150289772942,
        "epoch": 0.7189333333333333,
        "step": 5392
    },
    {
        "loss": 1.5955,
        "grad_norm": 2.154144525527954,
        "learning_rate": 0.00019726690851936318,
        "epoch": 0.7190666666666666,
        "step": 5393
    },
    {
        "loss": 2.1191,
        "grad_norm": 4.3610734939575195,
        "learning_rate": 0.00019725227561324303,
        "epoch": 0.7192,
        "step": 5394
    },
    {
        "loss": 2.3863,
        "grad_norm": 3.6810667514801025,
        "learning_rate": 0.00019723760418516512,
        "epoch": 0.7193333333333334,
        "step": 5395
    },
    {
        "loss": 2.4437,
        "grad_norm": 3.2044949531555176,
        "learning_rate": 0.00019722289424094086,
        "epoch": 0.7194666666666667,
        "step": 5396
    },
    {
        "loss": 2.4097,
        "grad_norm": 2.2996740341186523,
        "learning_rate": 0.00019720814578639694,
        "epoch": 0.7196,
        "step": 5397
    },
    {
        "loss": 2.1625,
        "grad_norm": 2.2720863819122314,
        "learning_rate": 0.00019719335882737525,
        "epoch": 0.7197333333333333,
        "step": 5398
    },
    {
        "loss": 3.0514,
        "grad_norm": 4.090526580810547,
        "learning_rate": 0.0001971785333697329,
        "epoch": 0.7198666666666667,
        "step": 5399
    },
    {
        "loss": 1.666,
        "grad_norm": 3.4061660766601562,
        "learning_rate": 0.0001971636694193424,
        "epoch": 0.72,
        "step": 5400
    },
    {
        "loss": 2.6381,
        "grad_norm": 2.8618383407592773,
        "learning_rate": 0.00019714876698209138,
        "epoch": 0.7201333333333333,
        "step": 5401
    },
    {
        "loss": 2.3597,
        "grad_norm": 5.140539169311523,
        "learning_rate": 0.0001971338260638827,
        "epoch": 0.7202666666666667,
        "step": 5402
    },
    {
        "loss": 2.1032,
        "grad_norm": 2.471841812133789,
        "learning_rate": 0.00019711884667063454,
        "epoch": 0.7204,
        "step": 5403
    },
    {
        "loss": 1.5787,
        "grad_norm": 1.8122920989990234,
        "learning_rate": 0.00019710382880828027,
        "epoch": 0.7205333333333334,
        "step": 5404
    },
    {
        "loss": 2.774,
        "grad_norm": 5.217469215393066,
        "learning_rate": 0.00019708877248276856,
        "epoch": 0.7206666666666667,
        "step": 5405
    },
    {
        "loss": 2.1304,
        "grad_norm": 2.6702702045440674,
        "learning_rate": 0.00019707367770006325,
        "epoch": 0.7208,
        "step": 5406
    },
    {
        "loss": 2.6099,
        "grad_norm": 4.143468379974365,
        "learning_rate": 0.00019705854446614343,
        "epoch": 0.7209333333333333,
        "step": 5407
    },
    {
        "loss": 2.2132,
        "grad_norm": 2.3249759674072266,
        "learning_rate": 0.00019704337278700343,
        "epoch": 0.7210666666666666,
        "step": 5408
    },
    {
        "loss": 2.8629,
        "grad_norm": 3.2763280868530273,
        "learning_rate": 0.0001970281626686528,
        "epoch": 0.7212,
        "step": 5409
    },
    {
        "loss": 2.4325,
        "grad_norm": 2.0581858158111572,
        "learning_rate": 0.00019701291411711633,
        "epoch": 0.7213333333333334,
        "step": 5410
    },
    {
        "loss": 2.6101,
        "grad_norm": 2.825500011444092,
        "learning_rate": 0.00019699762713843398,
        "epoch": 0.7214666666666667,
        "step": 5411
    },
    {
        "loss": 0.832,
        "grad_norm": 3.092528820037842,
        "learning_rate": 0.00019698230173866105,
        "epoch": 0.7216,
        "step": 5412
    },
    {
        "loss": 2.9013,
        "grad_norm": 3.7218029499053955,
        "learning_rate": 0.0001969669379238679,
        "epoch": 0.7217333333333333,
        "step": 5413
    },
    {
        "loss": 1.9625,
        "grad_norm": 3.6183674335479736,
        "learning_rate": 0.00019695153570014026,
        "epoch": 0.7218666666666667,
        "step": 5414
    },
    {
        "loss": 2.7317,
        "grad_norm": 1.6116292476654053,
        "learning_rate": 0.000196936095073579,
        "epoch": 0.722,
        "step": 5415
    },
    {
        "loss": 1.4325,
        "grad_norm": 2.9617533683776855,
        "learning_rate": 0.00019692061605030013,
        "epoch": 0.7221333333333333,
        "step": 5416
    },
    {
        "loss": 1.7719,
        "grad_norm": 3.689309597015381,
        "learning_rate": 0.00019690509863643502,
        "epoch": 0.7222666666666666,
        "step": 5417
    },
    {
        "loss": 2.2841,
        "grad_norm": 2.5187320709228516,
        "learning_rate": 0.00019688954283813014,
        "epoch": 0.7224,
        "step": 5418
    },
    {
        "loss": 2.2602,
        "grad_norm": 3.0085642337799072,
        "learning_rate": 0.00019687394866154724,
        "epoch": 0.7225333333333334,
        "step": 5419
    },
    {
        "loss": 2.5064,
        "grad_norm": 2.571979522705078,
        "learning_rate": 0.0001968583161128631,
        "epoch": 0.7226666666666667,
        "step": 5420
    },
    {
        "loss": 1.6737,
        "grad_norm": 4.545855522155762,
        "learning_rate": 0.00019684264519826997,
        "epoch": 0.7228,
        "step": 5421
    },
    {
        "loss": 2.8251,
        "grad_norm": 3.575870990753174,
        "learning_rate": 0.00019682693592397508,
        "epoch": 0.7229333333333333,
        "step": 5422
    },
    {
        "loss": 2.9117,
        "grad_norm": 2.590747356414795,
        "learning_rate": 0.00019681118829620092,
        "epoch": 0.7230666666666666,
        "step": 5423
    },
    {
        "loss": 1.6723,
        "grad_norm": 4.608334064483643,
        "learning_rate": 0.00019679540232118525,
        "epoch": 0.7232,
        "step": 5424
    },
    {
        "loss": 2.461,
        "grad_norm": 3.1476147174835205,
        "learning_rate": 0.00019677957800518086,
        "epoch": 0.7233333333333334,
        "step": 5425
    },
    {
        "loss": 2.9558,
        "grad_norm": 3.0019543170928955,
        "learning_rate": 0.00019676371535445587,
        "epoch": 0.7234666666666667,
        "step": 5426
    },
    {
        "loss": 1.4098,
        "grad_norm": 3.639925479888916,
        "learning_rate": 0.00019674781437529354,
        "epoch": 0.7236,
        "step": 5427
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.9741063117980957,
        "learning_rate": 0.00019673187507399223,
        "epoch": 0.7237333333333333,
        "step": 5428
    },
    {
        "loss": 0.801,
        "grad_norm": 3.0298991203308105,
        "learning_rate": 0.00019671589745686558,
        "epoch": 0.7238666666666667,
        "step": 5429
    },
    {
        "loss": 2.3081,
        "grad_norm": 2.7564890384674072,
        "learning_rate": 0.00019669988153024243,
        "epoch": 0.724,
        "step": 5430
    },
    {
        "loss": 2.3523,
        "grad_norm": 3.963801860809326,
        "learning_rate": 0.0001966838273004667,
        "epoch": 0.7241333333333333,
        "step": 5431
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.5285747051239014,
        "learning_rate": 0.00019666773477389752,
        "epoch": 0.7242666666666666,
        "step": 5432
    },
    {
        "loss": 2.6793,
        "grad_norm": 3.5925309658050537,
        "learning_rate": 0.00019665160395690925,
        "epoch": 0.7244,
        "step": 5433
    },
    {
        "loss": 2.5088,
        "grad_norm": 4.681370258331299,
        "learning_rate": 0.00019663543485589128,
        "epoch": 0.7245333333333334,
        "step": 5434
    },
    {
        "loss": 2.0272,
        "grad_norm": 3.2638142108917236,
        "learning_rate": 0.00019661922747724834,
        "epoch": 0.7246666666666667,
        "step": 5435
    },
    {
        "loss": 2.3838,
        "grad_norm": 2.8695106506347656,
        "learning_rate": 0.00019660298182740013,
        "epoch": 0.7248,
        "step": 5436
    },
    {
        "loss": 1.6405,
        "grad_norm": 4.062935829162598,
        "learning_rate": 0.0001965866979127817,
        "epoch": 0.7249333333333333,
        "step": 5437
    },
    {
        "loss": 2.3465,
        "grad_norm": 2.768080711364746,
        "learning_rate": 0.00019657037573984302,
        "epoch": 0.7250666666666666,
        "step": 5438
    },
    {
        "loss": 2.3906,
        "grad_norm": 2.366790294647217,
        "learning_rate": 0.00019655401531504954,
        "epoch": 0.7252,
        "step": 5439
    },
    {
        "loss": 1.5263,
        "grad_norm": 5.5016374588012695,
        "learning_rate": 0.00019653761664488152,
        "epoch": 0.7253333333333334,
        "step": 5440
    },
    {
        "loss": 1.868,
        "grad_norm": 3.96761417388916,
        "learning_rate": 0.00019652117973583468,
        "epoch": 0.7254666666666667,
        "step": 5441
    },
    {
        "loss": 1.2189,
        "grad_norm": 2.887432813644409,
        "learning_rate": 0.00019650470459441963,
        "epoch": 0.7256,
        "step": 5442
    },
    {
        "loss": 2.2736,
        "grad_norm": 2.65667462348938,
        "learning_rate": 0.00019648819122716223,
        "epoch": 0.7257333333333333,
        "step": 5443
    },
    {
        "loss": 2.2318,
        "grad_norm": 3.1153059005737305,
        "learning_rate": 0.00019647163964060357,
        "epoch": 0.7258666666666667,
        "step": 5444
    },
    {
        "loss": 1.9127,
        "grad_norm": 3.4488697052001953,
        "learning_rate": 0.00019645504984129968,
        "epoch": 0.726,
        "step": 5445
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.8396248817443848,
        "learning_rate": 0.0001964384218358219,
        "epoch": 0.7261333333333333,
        "step": 5446
    },
    {
        "loss": 1.767,
        "grad_norm": 4.525505065917969,
        "learning_rate": 0.00019642175563075662,
        "epoch": 0.7262666666666666,
        "step": 5447
    },
    {
        "loss": 2.504,
        "grad_norm": 3.652780532836914,
        "learning_rate": 0.0001964050512327054,
        "epoch": 0.7264,
        "step": 5448
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.175337314605713,
        "learning_rate": 0.00019638830864828489,
        "epoch": 0.7265333333333334,
        "step": 5449
    },
    {
        "loss": 1.9229,
        "grad_norm": 3.117809534072876,
        "learning_rate": 0.00019637152788412685,
        "epoch": 0.7266666666666667,
        "step": 5450
    },
    {
        "loss": 2.2385,
        "grad_norm": 2.840562343597412,
        "learning_rate": 0.0001963547089468783,
        "epoch": 0.7268,
        "step": 5451
    },
    {
        "loss": 2.3199,
        "grad_norm": 3.450936794281006,
        "learning_rate": 0.00019633785184320116,
        "epoch": 0.7269333333333333,
        "step": 5452
    },
    {
        "loss": 2.5587,
        "grad_norm": 3.4462296962738037,
        "learning_rate": 0.00019632095657977269,
        "epoch": 0.7270666666666666,
        "step": 5453
    },
    {
        "loss": 2.0583,
        "grad_norm": 2.568852186203003,
        "learning_rate": 0.00019630402316328507,
        "epoch": 0.7272,
        "step": 5454
    },
    {
        "loss": 2.5023,
        "grad_norm": 2.6533350944519043,
        "learning_rate": 0.00019628705160044577,
        "epoch": 0.7273333333333334,
        "step": 5455
    },
    {
        "loss": 2.2098,
        "grad_norm": 3.605881452560425,
        "learning_rate": 0.0001962700418979772,
        "epoch": 0.7274666666666667,
        "step": 5456
    },
    {
        "loss": 1.2027,
        "grad_norm": 4.812460899353027,
        "learning_rate": 0.000196252994062617,
        "epoch": 0.7276,
        "step": 5457
    },
    {
        "loss": 1.5153,
        "grad_norm": 4.738289833068848,
        "learning_rate": 0.00019623590810111794,
        "epoch": 0.7277333333333333,
        "step": 5458
    },
    {
        "loss": 2.5759,
        "grad_norm": 2.3636903762817383,
        "learning_rate": 0.00019621878402024771,
        "epoch": 0.7278666666666667,
        "step": 5459
    },
    {
        "loss": 1.694,
        "grad_norm": 4.333661079406738,
        "learning_rate": 0.0001962016218267893,
        "epoch": 0.728,
        "step": 5460
    },
    {
        "loss": 2.2493,
        "grad_norm": 3.3415849208831787,
        "learning_rate": 0.00019618442152754067,
        "epoch": 0.7281333333333333,
        "step": 5461
    },
    {
        "loss": 1.5744,
        "grad_norm": 3.6383254528045654,
        "learning_rate": 0.00019616718312931496,
        "epoch": 0.7282666666666666,
        "step": 5462
    },
    {
        "loss": 2.5483,
        "grad_norm": 2.8725202083587646,
        "learning_rate": 0.00019614990663894023,
        "epoch": 0.7284,
        "step": 5463
    },
    {
        "loss": 0.8628,
        "grad_norm": 3.0237414836883545,
        "learning_rate": 0.00019613259206325995,
        "epoch": 0.7285333333333334,
        "step": 5464
    },
    {
        "loss": 2.3971,
        "grad_norm": 3.3554441928863525,
        "learning_rate": 0.0001961152394091324,
        "epoch": 0.7286666666666667,
        "step": 5465
    },
    {
        "loss": 2.6247,
        "grad_norm": 3.5867745876312256,
        "learning_rate": 0.00019609784868343096,
        "epoch": 0.7288,
        "step": 5466
    },
    {
        "loss": 2.5404,
        "grad_norm": 3.4355881214141846,
        "learning_rate": 0.00019608041989304425,
        "epoch": 0.7289333333333333,
        "step": 5467
    },
    {
        "loss": 1.7595,
        "grad_norm": 3.879469633102417,
        "learning_rate": 0.00019606295304487584,
        "epoch": 0.7290666666666666,
        "step": 5468
    },
    {
        "loss": 3.1557,
        "grad_norm": 2.940112829208374,
        "learning_rate": 0.00019604544814584438,
        "epoch": 0.7292,
        "step": 5469
    },
    {
        "loss": 2.6991,
        "grad_norm": 3.264972686767578,
        "learning_rate": 0.00019602790520288364,
        "epoch": 0.7293333333333333,
        "step": 5470
    },
    {
        "loss": 2.9899,
        "grad_norm": 4.001091480255127,
        "learning_rate": 0.00019601032422294248,
        "epoch": 0.7294666666666667,
        "step": 5471
    },
    {
        "loss": 3.0726,
        "grad_norm": 2.099210500717163,
        "learning_rate": 0.00019599270521298466,
        "epoch": 0.7296,
        "step": 5472
    },
    {
        "loss": 2.9567,
        "grad_norm": 2.4762518405914307,
        "learning_rate": 0.0001959750481799893,
        "epoch": 0.7297333333333333,
        "step": 5473
    },
    {
        "loss": 1.6322,
        "grad_norm": 5.240304946899414,
        "learning_rate": 0.00019595735313095028,
        "epoch": 0.7298666666666667,
        "step": 5474
    },
    {
        "loss": 2.1289,
        "grad_norm": 4.570229530334473,
        "learning_rate": 0.0001959396200728768,
        "epoch": 0.73,
        "step": 5475
    },
    {
        "loss": 1.7002,
        "grad_norm": 4.374395370483398,
        "learning_rate": 0.00019592184901279282,
        "epoch": 0.7301333333333333,
        "step": 5476
    },
    {
        "loss": 2.6253,
        "grad_norm": 3.0232510566711426,
        "learning_rate": 0.00019590403995773764,
        "epoch": 0.7302666666666666,
        "step": 5477
    },
    {
        "loss": 2.5575,
        "grad_norm": 3.267770528793335,
        "learning_rate": 0.00019588619291476542,
        "epoch": 0.7304,
        "step": 5478
    },
    {
        "loss": 2.6526,
        "grad_norm": 2.8035926818847656,
        "learning_rate": 0.0001958683078909455,
        "epoch": 0.7305333333333334,
        "step": 5479
    },
    {
        "loss": 1.832,
        "grad_norm": 3.944084882736206,
        "learning_rate": 0.00019585038489336213,
        "epoch": 0.7306666666666667,
        "step": 5480
    },
    {
        "loss": 1.5167,
        "grad_norm": 5.12916898727417,
        "learning_rate": 0.00019583242392911472,
        "epoch": 0.7308,
        "step": 5481
    },
    {
        "loss": 2.6621,
        "grad_norm": 2.216265916824341,
        "learning_rate": 0.0001958144250053176,
        "epoch": 0.7309333333333333,
        "step": 5482
    },
    {
        "loss": 2.7665,
        "grad_norm": 2.8268895149230957,
        "learning_rate": 0.00019579638812910032,
        "epoch": 0.7310666666666666,
        "step": 5483
    },
    {
        "loss": 2.1477,
        "grad_norm": 3.4106388092041016,
        "learning_rate": 0.00019577831330760726,
        "epoch": 0.7312,
        "step": 5484
    },
    {
        "loss": 2.0551,
        "grad_norm": 3.0170013904571533,
        "learning_rate": 0.00019576020054799795,
        "epoch": 0.7313333333333333,
        "step": 5485
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.812790632247925,
        "learning_rate": 0.0001957420498574469,
        "epoch": 0.7314666666666667,
        "step": 5486
    },
    {
        "loss": 2.8888,
        "grad_norm": 2.2698140144348145,
        "learning_rate": 0.0001957238612431437,
        "epoch": 0.7316,
        "step": 5487
    },
    {
        "loss": 2.7246,
        "grad_norm": 4.527067184448242,
        "learning_rate": 0.0001957056347122928,
        "epoch": 0.7317333333333333,
        "step": 5488
    },
    {
        "loss": 1.8723,
        "grad_norm": 5.078959941864014,
        "learning_rate": 0.00019568737027211394,
        "epoch": 0.7318666666666667,
        "step": 5489
    },
    {
        "loss": 1.9828,
        "grad_norm": 3.4951741695404053,
        "learning_rate": 0.00019566906792984167,
        "epoch": 0.732,
        "step": 5490
    },
    {
        "loss": 2.8448,
        "grad_norm": 2.7189645767211914,
        "learning_rate": 0.00019565072769272562,
        "epoch": 0.7321333333333333,
        "step": 5491
    },
    {
        "loss": 1.6285,
        "grad_norm": 3.288452386856079,
        "learning_rate": 0.0001956323495680304,
        "epoch": 0.7322666666666666,
        "step": 5492
    },
    {
        "loss": 1.9948,
        "grad_norm": 3.9531424045562744,
        "learning_rate": 0.00019561393356303563,
        "epoch": 0.7324,
        "step": 5493
    },
    {
        "loss": 2.2784,
        "grad_norm": 4.482775688171387,
        "learning_rate": 0.000195595479685036,
        "epoch": 0.7325333333333334,
        "step": 5494
    },
    {
        "loss": 2.4781,
        "grad_norm": 3.7880961894989014,
        "learning_rate": 0.00019557698794134116,
        "epoch": 0.7326666666666667,
        "step": 5495
    },
    {
        "loss": 2.8058,
        "grad_norm": 2.97102952003479,
        "learning_rate": 0.00019555845833927574,
        "epoch": 0.7328,
        "step": 5496
    },
    {
        "loss": 2.0701,
        "grad_norm": 4.01920223236084,
        "learning_rate": 0.00019553989088617928,
        "epoch": 0.7329333333333333,
        "step": 5497
    },
    {
        "loss": 2.9956,
        "grad_norm": 2.801194190979004,
        "learning_rate": 0.00019552128558940655,
        "epoch": 0.7330666666666666,
        "step": 5498
    },
    {
        "loss": 2.1324,
        "grad_norm": 2.998612403869629,
        "learning_rate": 0.0001955026424563271,
        "epoch": 0.7332,
        "step": 5499
    },
    {
        "loss": 1.0229,
        "grad_norm": 3.4678115844726562,
        "learning_rate": 0.00019548396149432558,
        "epoch": 0.7333333333333333,
        "step": 5500
    },
    {
        "loss": 2.353,
        "grad_norm": 2.569169521331787,
        "learning_rate": 0.00019546524271080154,
        "epoch": 0.7334666666666667,
        "step": 5501
    },
    {
        "loss": 2.9476,
        "grad_norm": 2.4154210090637207,
        "learning_rate": 0.0001954464861131696,
        "epoch": 0.7336,
        "step": 5502
    },
    {
        "loss": 2.6984,
        "grad_norm": 4.4415507316589355,
        "learning_rate": 0.00019542769170885924,
        "epoch": 0.7337333333333333,
        "step": 5503
    },
    {
        "loss": 2.4053,
        "grad_norm": 3.728808641433716,
        "learning_rate": 0.00019540885950531503,
        "epoch": 0.7338666666666667,
        "step": 5504
    },
    {
        "loss": 0.8386,
        "grad_norm": 3.4157350063323975,
        "learning_rate": 0.0001953899895099965,
        "epoch": 0.734,
        "step": 5505
    },
    {
        "loss": 1.8419,
        "grad_norm": 4.987338542938232,
        "learning_rate": 0.0001953710817303781,
        "epoch": 0.7341333333333333,
        "step": 5506
    },
    {
        "loss": 2.7526,
        "grad_norm": 2.808279514312744,
        "learning_rate": 0.00019535213617394924,
        "epoch": 0.7342666666666666,
        "step": 5507
    },
    {
        "loss": 2.849,
        "grad_norm": 5.283313751220703,
        "learning_rate": 0.00019533315284821437,
        "epoch": 0.7344,
        "step": 5508
    },
    {
        "loss": 1.4188,
        "grad_norm": 5.130804538726807,
        "learning_rate": 0.00019531413176069275,
        "epoch": 0.7345333333333334,
        "step": 5509
    },
    {
        "loss": 2.4321,
        "grad_norm": 2.472987651824951,
        "learning_rate": 0.00019529507291891882,
        "epoch": 0.7346666666666667,
        "step": 5510
    },
    {
        "loss": 1.6449,
        "grad_norm": 2.920497417449951,
        "learning_rate": 0.0001952759763304418,
        "epoch": 0.7348,
        "step": 5511
    },
    {
        "loss": 2.6651,
        "grad_norm": 5.2854743003845215,
        "learning_rate": 0.00019525684200282588,
        "epoch": 0.7349333333333333,
        "step": 5512
    },
    {
        "loss": 2.8297,
        "grad_norm": 2.7008655071258545,
        "learning_rate": 0.00019523766994365027,
        "epoch": 0.7350666666666666,
        "step": 5513
    },
    {
        "loss": 2.9125,
        "grad_norm": 3.9185197353363037,
        "learning_rate": 0.00019521846016050908,
        "epoch": 0.7352,
        "step": 5514
    },
    {
        "loss": 2.5624,
        "grad_norm": 4.145137310028076,
        "learning_rate": 0.00019519921266101137,
        "epoch": 0.7353333333333333,
        "step": 5515
    },
    {
        "loss": 2.5823,
        "grad_norm": 3.6888720989227295,
        "learning_rate": 0.00019517992745278116,
        "epoch": 0.7354666666666667,
        "step": 5516
    },
    {
        "loss": 2.5933,
        "grad_norm": 4.244812965393066,
        "learning_rate": 0.00019516060454345734,
        "epoch": 0.7356,
        "step": 5517
    },
    {
        "loss": 2.4259,
        "grad_norm": 3.192612648010254,
        "learning_rate": 0.00019514124394069384,
        "epoch": 0.7357333333333334,
        "step": 5518
    },
    {
        "loss": 2.6189,
        "grad_norm": 1.6692520380020142,
        "learning_rate": 0.00019512184565215944,
        "epoch": 0.7358666666666667,
        "step": 5519
    },
    {
        "loss": 2.5557,
        "grad_norm": 3.0805389881134033,
        "learning_rate": 0.0001951024096855378,
        "epoch": 0.736,
        "step": 5520
    },
    {
        "loss": 2.4434,
        "grad_norm": 2.1655356884002686,
        "learning_rate": 0.00019508293604852768,
        "epoch": 0.7361333333333333,
        "step": 5521
    },
    {
        "loss": 2.5638,
        "grad_norm": 2.0532028675079346,
        "learning_rate": 0.00019506342474884253,
        "epoch": 0.7362666666666666,
        "step": 5522
    },
    {
        "loss": 2.0145,
        "grad_norm": 3.0218920707702637,
        "learning_rate": 0.00019504387579421102,
        "epoch": 0.7364,
        "step": 5523
    },
    {
        "loss": 2.1929,
        "grad_norm": 3.323852062225342,
        "learning_rate": 0.0001950242891923764,
        "epoch": 0.7365333333333334,
        "step": 5524
    },
    {
        "loss": 2.563,
        "grad_norm": 2.8910014629364014,
        "learning_rate": 0.00019500466495109704,
        "epoch": 0.7366666666666667,
        "step": 5525
    },
    {
        "loss": 2.7299,
        "grad_norm": 2.6076881885528564,
        "learning_rate": 0.00019498500307814617,
        "epoch": 0.7368,
        "step": 5526
    },
    {
        "loss": 1.6839,
        "grad_norm": 3.666255235671997,
        "learning_rate": 0.00019496530358131199,
        "epoch": 0.7369333333333333,
        "step": 5527
    },
    {
        "loss": 2.3672,
        "grad_norm": 3.864372730255127,
        "learning_rate": 0.0001949455664683974,
        "epoch": 0.7370666666666666,
        "step": 5528
    },
    {
        "loss": 2.5624,
        "grad_norm": 3.196085214614868,
        "learning_rate": 0.00019492579174722042,
        "epoch": 0.7372,
        "step": 5529
    },
    {
        "loss": 2.5296,
        "grad_norm": 3.2066898345947266,
        "learning_rate": 0.0001949059794256139,
        "epoch": 0.7373333333333333,
        "step": 5530
    },
    {
        "loss": 2.216,
        "grad_norm": 4.588150978088379,
        "learning_rate": 0.00019488612951142553,
        "epoch": 0.7374666666666667,
        "step": 5531
    },
    {
        "loss": 1.8069,
        "grad_norm": 2.90456223487854,
        "learning_rate": 0.00019486624201251796,
        "epoch": 0.7376,
        "step": 5532
    },
    {
        "loss": 1.879,
        "grad_norm": 4.498098373413086,
        "learning_rate": 0.00019484631693676864,
        "epoch": 0.7377333333333334,
        "step": 5533
    },
    {
        "loss": 2.4178,
        "grad_norm": 3.976600408554077,
        "learning_rate": 0.00019482635429207007,
        "epoch": 0.7378666666666667,
        "step": 5534
    },
    {
        "loss": 2.5277,
        "grad_norm": 3.131152629852295,
        "learning_rate": 0.0001948063540863294,
        "epoch": 0.738,
        "step": 5535
    },
    {
        "loss": 2.8642,
        "grad_norm": 1.850461483001709,
        "learning_rate": 0.00019478631632746887,
        "epoch": 0.7381333333333333,
        "step": 5536
    },
    {
        "loss": 2.4453,
        "grad_norm": 2.543649911880493,
        "learning_rate": 0.00019476624102342545,
        "epoch": 0.7382666666666666,
        "step": 5537
    },
    {
        "loss": 1.9721,
        "grad_norm": 4.297055244445801,
        "learning_rate": 0.00019474612818215105,
        "epoch": 0.7384,
        "step": 5538
    },
    {
        "loss": 1.2735,
        "grad_norm": 6.235873222351074,
        "learning_rate": 0.00019472597781161242,
        "epoch": 0.7385333333333334,
        "step": 5539
    },
    {
        "loss": 2.0647,
        "grad_norm": 3.1296939849853516,
        "learning_rate": 0.0001947057899197912,
        "epoch": 0.7386666666666667,
        "step": 5540
    },
    {
        "loss": 0.786,
        "grad_norm": 3.0421035289764404,
        "learning_rate": 0.0001946855645146839,
        "epoch": 0.7388,
        "step": 5541
    },
    {
        "loss": 1.714,
        "grad_norm": 3.871549129486084,
        "learning_rate": 0.0001946653016043019,
        "epoch": 0.7389333333333333,
        "step": 5542
    },
    {
        "loss": 2.0157,
        "grad_norm": 3.3037779331207275,
        "learning_rate": 0.00019464500119667136,
        "epoch": 0.7390666666666666,
        "step": 5543
    },
    {
        "loss": 2.0718,
        "grad_norm": 2.5570461750030518,
        "learning_rate": 0.00019462466329983334,
        "epoch": 0.7392,
        "step": 5544
    },
    {
        "loss": 2.488,
        "grad_norm": 2.346597909927368,
        "learning_rate": 0.00019460428792184375,
        "epoch": 0.7393333333333333,
        "step": 5545
    },
    {
        "loss": 2.4951,
        "grad_norm": 3.1927237510681152,
        "learning_rate": 0.00019458387507077335,
        "epoch": 0.7394666666666667,
        "step": 5546
    },
    {
        "loss": 2.63,
        "grad_norm": 2.406550884246826,
        "learning_rate": 0.00019456342475470775,
        "epoch": 0.7396,
        "step": 5547
    },
    {
        "loss": 2.5603,
        "grad_norm": 2.2794394493103027,
        "learning_rate": 0.00019454293698174742,
        "epoch": 0.7397333333333334,
        "step": 5548
    },
    {
        "loss": 2.6513,
        "grad_norm": 4.236966133117676,
        "learning_rate": 0.00019452241176000755,
        "epoch": 0.7398666666666667,
        "step": 5549
    },
    {
        "loss": 2.5578,
        "grad_norm": 3.960163116455078,
        "learning_rate": 0.0001945018490976184,
        "epoch": 0.74,
        "step": 5550
    },
    {
        "loss": 2.7366,
        "grad_norm": 2.893193483352661,
        "learning_rate": 0.00019448124900272473,
        "epoch": 0.7401333333333333,
        "step": 5551
    },
    {
        "loss": 1.7631,
        "grad_norm": 4.884567737579346,
        "learning_rate": 0.0001944606114834864,
        "epoch": 0.7402666666666666,
        "step": 5552
    },
    {
        "loss": 2.476,
        "grad_norm": 2.502041816711426,
        "learning_rate": 0.00019443993654807802,
        "epoch": 0.7404,
        "step": 5553
    },
    {
        "loss": 3.221,
        "grad_norm": 3.9942092895507812,
        "learning_rate": 0.00019441922420468898,
        "epoch": 0.7405333333333334,
        "step": 5554
    },
    {
        "loss": 0.8232,
        "grad_norm": 3.66992449760437,
        "learning_rate": 0.00019439847446152351,
        "epoch": 0.7406666666666667,
        "step": 5555
    },
    {
        "loss": 0.8134,
        "grad_norm": 2.362867593765259,
        "learning_rate": 0.00019437768732680062,
        "epoch": 0.7408,
        "step": 5556
    },
    {
        "loss": 1.1557,
        "grad_norm": 3.205183982849121,
        "learning_rate": 0.0001943568628087542,
        "epoch": 0.7409333333333333,
        "step": 5557
    },
    {
        "loss": 2.118,
        "grad_norm": 3.254591941833496,
        "learning_rate": 0.00019433600091563294,
        "epoch": 0.7410666666666667,
        "step": 5558
    },
    {
        "loss": 3.0829,
        "grad_norm": 4.139183521270752,
        "learning_rate": 0.00019431510165570027,
        "epoch": 0.7412,
        "step": 5559
    },
    {
        "loss": 2.2163,
        "grad_norm": 3.1376218795776367,
        "learning_rate": 0.00019429416503723444,
        "epoch": 0.7413333333333333,
        "step": 5560
    },
    {
        "loss": 3.2536,
        "grad_norm": 5.489628791809082,
        "learning_rate": 0.00019427319106852858,
        "epoch": 0.7414666666666667,
        "step": 5561
    },
    {
        "loss": 2.6091,
        "grad_norm": 2.437805414199829,
        "learning_rate": 0.00019425217975789044,
        "epoch": 0.7416,
        "step": 5562
    },
    {
        "loss": 3.1881,
        "grad_norm": 2.6638314723968506,
        "learning_rate": 0.0001942311311136428,
        "epoch": 0.7417333333333334,
        "step": 5563
    },
    {
        "loss": 2.8041,
        "grad_norm": 3.334757089614868,
        "learning_rate": 0.00019421004514412303,
        "epoch": 0.7418666666666667,
        "step": 5564
    },
    {
        "loss": 1.5264,
        "grad_norm": 3.387258529663086,
        "learning_rate": 0.00019418892185768335,
        "epoch": 0.742,
        "step": 5565
    },
    {
        "loss": 2.6713,
        "grad_norm": 2.895951747894287,
        "learning_rate": 0.0001941677612626908,
        "epoch": 0.7421333333333333,
        "step": 5566
    },
    {
        "loss": 2.2696,
        "grad_norm": 2.1467456817626953,
        "learning_rate": 0.00019414656336752713,
        "epoch": 0.7422666666666666,
        "step": 5567
    },
    {
        "loss": 2.2754,
        "grad_norm": 2.8388350009918213,
        "learning_rate": 0.0001941253281805889,
        "epoch": 0.7424,
        "step": 5568
    },
    {
        "loss": 2.5256,
        "grad_norm": 3.279279947280884,
        "learning_rate": 0.00019410405571028747,
        "epoch": 0.7425333333333334,
        "step": 5569
    },
    {
        "loss": 1.9417,
        "grad_norm": 2.768596887588501,
        "learning_rate": 0.00019408274596504894,
        "epoch": 0.7426666666666667,
        "step": 5570
    },
    {
        "loss": 1.0857,
        "grad_norm": 5.031619548797607,
        "learning_rate": 0.00019406139895331413,
        "epoch": 0.7428,
        "step": 5571
    },
    {
        "loss": 1.9919,
        "grad_norm": 4.321430683135986,
        "learning_rate": 0.00019404001468353868,
        "epoch": 0.7429333333333333,
        "step": 5572
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.603604555130005,
        "learning_rate": 0.000194018593164193,
        "epoch": 0.7430666666666667,
        "step": 5573
    },
    {
        "loss": 2.5059,
        "grad_norm": 3.6549313068389893,
        "learning_rate": 0.0001939971344037622,
        "epoch": 0.7432,
        "step": 5574
    },
    {
        "loss": 2.812,
        "grad_norm": 3.593900680541992,
        "learning_rate": 0.00019397563841074615,
        "epoch": 0.7433333333333333,
        "step": 5575
    },
    {
        "loss": 2.5352,
        "grad_norm": 3.58626389503479,
        "learning_rate": 0.00019395410519365956,
        "epoch": 0.7434666666666667,
        "step": 5576
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.691338062286377,
        "learning_rate": 0.00019393253476103175,
        "epoch": 0.7436,
        "step": 5577
    },
    {
        "loss": 2.8837,
        "grad_norm": 3.0835843086242676,
        "learning_rate": 0.00019391092712140687,
        "epoch": 0.7437333333333334,
        "step": 5578
    },
    {
        "loss": 1.4733,
        "grad_norm": 5.052909851074219,
        "learning_rate": 0.0001938892822833437,
        "epoch": 0.7438666666666667,
        "step": 5579
    },
    {
        "loss": 0.7082,
        "grad_norm": 3.7979962825775146,
        "learning_rate": 0.00019386760025541598,
        "epoch": 0.744,
        "step": 5580
    },
    {
        "loss": 2.8483,
        "grad_norm": 3.3031342029571533,
        "learning_rate": 0.00019384588104621187,
        "epoch": 0.7441333333333333,
        "step": 5581
    },
    {
        "loss": 2.3911,
        "grad_norm": 7.652866840362549,
        "learning_rate": 0.00019382412466433454,
        "epoch": 0.7442666666666666,
        "step": 5582
    },
    {
        "loss": 2.056,
        "grad_norm": 2.99826979637146,
        "learning_rate": 0.00019380233111840177,
        "epoch": 0.7444,
        "step": 5583
    },
    {
        "loss": 2.5899,
        "grad_norm": 3.2211697101593018,
        "learning_rate": 0.00019378050041704597,
        "epoch": 0.7445333333333334,
        "step": 5584
    },
    {
        "loss": 2.0344,
        "grad_norm": 3.7598888874053955,
        "learning_rate": 0.00019375863256891442,
        "epoch": 0.7446666666666667,
        "step": 5585
    },
    {
        "loss": 2.3624,
        "grad_norm": 3.6361303329467773,
        "learning_rate": 0.00019373672758266904,
        "epoch": 0.7448,
        "step": 5586
    },
    {
        "loss": 1.7961,
        "grad_norm": 5.317103385925293,
        "learning_rate": 0.00019371478546698648,
        "epoch": 0.7449333333333333,
        "step": 5587
    },
    {
        "loss": 2.3064,
        "grad_norm": 2.4889416694641113,
        "learning_rate": 0.00019369280623055804,
        "epoch": 0.7450666666666667,
        "step": 5588
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.7530391216278076,
        "learning_rate": 0.00019367078988208984,
        "epoch": 0.7452,
        "step": 5589
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.9157893657684326,
        "learning_rate": 0.00019364873643030255,
        "epoch": 0.7453333333333333,
        "step": 5590
    },
    {
        "loss": 2.653,
        "grad_norm": 2.6784071922302246,
        "learning_rate": 0.00019362664588393167,
        "epoch": 0.7454666666666667,
        "step": 5591
    },
    {
        "loss": 1.8079,
        "grad_norm": 2.6693131923675537,
        "learning_rate": 0.0001936045182517273,
        "epoch": 0.7456,
        "step": 5592
    },
    {
        "loss": 2.4581,
        "grad_norm": 3.280059814453125,
        "learning_rate": 0.00019358235354245438,
        "epoch": 0.7457333333333334,
        "step": 5593
    },
    {
        "loss": 2.3193,
        "grad_norm": 4.717305660247803,
        "learning_rate": 0.0001935601517648923,
        "epoch": 0.7458666666666667,
        "step": 5594
    },
    {
        "loss": 2.7339,
        "grad_norm": 2.476154088973999,
        "learning_rate": 0.00019353791292783533,
        "epoch": 0.746,
        "step": 5595
    },
    {
        "loss": 2.4198,
        "grad_norm": 2.773538112640381,
        "learning_rate": 0.00019351563704009231,
        "epoch": 0.7461333333333333,
        "step": 5596
    },
    {
        "loss": 2.4572,
        "grad_norm": 2.8233885765075684,
        "learning_rate": 0.0001934933241104868,
        "epoch": 0.7462666666666666,
        "step": 5597
    },
    {
        "loss": 1.585,
        "grad_norm": 2.691990852355957,
        "learning_rate": 0.00019347097414785706,
        "epoch": 0.7464,
        "step": 5598
    },
    {
        "loss": 2.8235,
        "grad_norm": 2.6752140522003174,
        "learning_rate": 0.00019344858716105595,
        "epoch": 0.7465333333333334,
        "step": 5599
    },
    {
        "loss": 2.154,
        "grad_norm": 3.4323790073394775,
        "learning_rate": 0.00019342616315895105,
        "epoch": 0.7466666666666667,
        "step": 5600
    },
    {
        "loss": 2.4153,
        "grad_norm": 3.912602186203003,
        "learning_rate": 0.0001934037021504246,
        "epoch": 0.7468,
        "step": 5601
    },
    {
        "loss": 2.4282,
        "grad_norm": 2.4138314723968506,
        "learning_rate": 0.00019338120414437343,
        "epoch": 0.7469333333333333,
        "step": 5602
    },
    {
        "loss": 2.5693,
        "grad_norm": 3.6343984603881836,
        "learning_rate": 0.00019335866914970914,
        "epoch": 0.7470666666666667,
        "step": 5603
    },
    {
        "loss": 1.2936,
        "grad_norm": 4.666545391082764,
        "learning_rate": 0.00019333609717535788,
        "epoch": 0.7472,
        "step": 5604
    },
    {
        "loss": 2.35,
        "grad_norm": 3.476696014404297,
        "learning_rate": 0.00019331348823026053,
        "epoch": 0.7473333333333333,
        "step": 5605
    },
    {
        "loss": 2.4177,
        "grad_norm": 3.91252064704895,
        "learning_rate": 0.00019329084232337244,
        "epoch": 0.7474666666666666,
        "step": 5606
    },
    {
        "loss": 2.6745,
        "grad_norm": 4.407051086425781,
        "learning_rate": 0.00019326815946366387,
        "epoch": 0.7476,
        "step": 5607
    },
    {
        "loss": 3.1581,
        "grad_norm": 2.8362367153167725,
        "learning_rate": 0.00019324543966011955,
        "epoch": 0.7477333333333334,
        "step": 5608
    },
    {
        "loss": 2.7092,
        "grad_norm": 2.014282703399658,
        "learning_rate": 0.00019322268292173885,
        "epoch": 0.7478666666666667,
        "step": 5609
    },
    {
        "loss": 2.6725,
        "grad_norm": 3.0270607471466064,
        "learning_rate": 0.0001931998892575358,
        "epoch": 0.748,
        "step": 5610
    },
    {
        "loss": 1.4195,
        "grad_norm": 4.493118762969971,
        "learning_rate": 0.00019317705867653905,
        "epoch": 0.7481333333333333,
        "step": 5611
    },
    {
        "loss": 2.3168,
        "grad_norm": 3.4674267768859863,
        "learning_rate": 0.00019315419118779182,
        "epoch": 0.7482666666666666,
        "step": 5612
    },
    {
        "loss": 1.3864,
        "grad_norm": 3.077927589416504,
        "learning_rate": 0.00019313128680035208,
        "epoch": 0.7484,
        "step": 5613
    },
    {
        "loss": 3.2115,
        "grad_norm": 2.0740034580230713,
        "learning_rate": 0.00019310834552329235,
        "epoch": 0.7485333333333334,
        "step": 5614
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.8408992290496826,
        "learning_rate": 0.00019308536736569959,
        "epoch": 0.7486666666666667,
        "step": 5615
    },
    {
        "loss": 2.6729,
        "grad_norm": 2.1269748210906982,
        "learning_rate": 0.0001930623523366757,
        "epoch": 0.7488,
        "step": 5616
    },
    {
        "loss": 3.0612,
        "grad_norm": 2.1156275272369385,
        "learning_rate": 0.00019303930044533692,
        "epoch": 0.7489333333333333,
        "step": 5617
    },
    {
        "loss": 2.6892,
        "grad_norm": 4.134737491607666,
        "learning_rate": 0.00019301621170081428,
        "epoch": 0.7490666666666667,
        "step": 5618
    },
    {
        "loss": 3.0308,
        "grad_norm": 2.3021786212921143,
        "learning_rate": 0.00019299308611225318,
        "epoch": 0.7492,
        "step": 5619
    },
    {
        "loss": 2.2892,
        "grad_norm": 3.4716856479644775,
        "learning_rate": 0.00019296992368881383,
        "epoch": 0.7493333333333333,
        "step": 5620
    },
    {
        "loss": 2.4723,
        "grad_norm": 5.004526138305664,
        "learning_rate": 0.0001929467244396709,
        "epoch": 0.7494666666666666,
        "step": 5621
    },
    {
        "loss": 2.2936,
        "grad_norm": 3.891061544418335,
        "learning_rate": 0.0001929234883740137,
        "epoch": 0.7496,
        "step": 5622
    },
    {
        "loss": 3.1486,
        "grad_norm": 2.5170910358428955,
        "learning_rate": 0.00019290021550104615,
        "epoch": 0.7497333333333334,
        "step": 5623
    },
    {
        "loss": 2.573,
        "grad_norm": 2.482015609741211,
        "learning_rate": 0.00019287690582998668,
        "epoch": 0.7498666666666667,
        "step": 5624
    },
    {
        "loss": 2.2406,
        "grad_norm": 2.566101312637329,
        "learning_rate": 0.00019285355937006835,
        "epoch": 0.75,
        "step": 5625
    },
    {
        "loss": 1.9371,
        "grad_norm": 4.096426963806152,
        "learning_rate": 0.00019283017613053876,
        "epoch": 0.7501333333333333,
        "step": 5626
    },
    {
        "loss": 2.6892,
        "grad_norm": 2.5608718395233154,
        "learning_rate": 0.00019280675612066007,
        "epoch": 0.7502666666666666,
        "step": 5627
    },
    {
        "loss": 2.4452,
        "grad_norm": 3.1104085445404053,
        "learning_rate": 0.00019278329934970908,
        "epoch": 0.7504,
        "step": 5628
    },
    {
        "loss": 2.8685,
        "grad_norm": 2.8271257877349854,
        "learning_rate": 0.00019275980582697706,
        "epoch": 0.7505333333333334,
        "step": 5629
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.4812073707580566,
        "learning_rate": 0.0001927362755617699,
        "epoch": 0.7506666666666667,
        "step": 5630
    },
    {
        "loss": 2.6212,
        "grad_norm": 2.2217347621917725,
        "learning_rate": 0.00019271270856340795,
        "epoch": 0.7508,
        "step": 5631
    },
    {
        "loss": 1.9006,
        "grad_norm": 3.683600664138794,
        "learning_rate": 0.00019268910484122626,
        "epoch": 0.7509333333333333,
        "step": 5632
    },
    {
        "loss": 2.3765,
        "grad_norm": 3.550137519836426,
        "learning_rate": 0.0001926654644045743,
        "epoch": 0.7510666666666667,
        "step": 5633
    },
    {
        "loss": 2.1416,
        "grad_norm": 3.4899611473083496,
        "learning_rate": 0.00019264178726281612,
        "epoch": 0.7512,
        "step": 5634
    },
    {
        "loss": 2.1836,
        "grad_norm": 3.3874471187591553,
        "learning_rate": 0.0001926180734253304,
        "epoch": 0.7513333333333333,
        "step": 5635
    },
    {
        "loss": 2.0081,
        "grad_norm": 3.171027660369873,
        "learning_rate": 0.00019259432290151014,
        "epoch": 0.7514666666666666,
        "step": 5636
    },
    {
        "loss": 2.9808,
        "grad_norm": 2.3571879863739014,
        "learning_rate": 0.00019257053570076314,
        "epoch": 0.7516,
        "step": 5637
    },
    {
        "loss": 1.7209,
        "grad_norm": 3.3653910160064697,
        "learning_rate": 0.00019254671183251146,
        "epoch": 0.7517333333333334,
        "step": 5638
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.126397132873535,
        "learning_rate": 0.0001925228513061919,
        "epoch": 0.7518666666666667,
        "step": 5639
    },
    {
        "loss": 2.8951,
        "grad_norm": 2.3131308555603027,
        "learning_rate": 0.00019249895413125565,
        "epoch": 0.752,
        "step": 5640
    },
    {
        "loss": 2.1326,
        "grad_norm": 3.1583473682403564,
        "learning_rate": 0.00019247502031716852,
        "epoch": 0.7521333333333333,
        "step": 5641
    },
    {
        "loss": 2.3026,
        "grad_norm": 4.312743186950684,
        "learning_rate": 0.00019245104987341072,
        "epoch": 0.7522666666666666,
        "step": 5642
    },
    {
        "loss": 2.3174,
        "grad_norm": 3.737156867980957,
        "learning_rate": 0.00019242704280947708,
        "epoch": 0.7524,
        "step": 5643
    },
    {
        "loss": 2.7051,
        "grad_norm": 3.214454174041748,
        "learning_rate": 0.0001924029991348768,
        "epoch": 0.7525333333333334,
        "step": 5644
    },
    {
        "loss": 3.077,
        "grad_norm": 2.5654456615448,
        "learning_rate": 0.00019237891885913374,
        "epoch": 0.7526666666666667,
        "step": 5645
    },
    {
        "loss": 1.5734,
        "grad_norm": 3.2782680988311768,
        "learning_rate": 0.00019235480199178615,
        "epoch": 0.7528,
        "step": 5646
    },
    {
        "loss": 2.2675,
        "grad_norm": 3.6479554176330566,
        "learning_rate": 0.00019233064854238678,
        "epoch": 0.7529333333333333,
        "step": 5647
    },
    {
        "loss": 2.18,
        "grad_norm": 3.5869455337524414,
        "learning_rate": 0.00019230645852050295,
        "epoch": 0.7530666666666667,
        "step": 5648
    },
    {
        "loss": 2.037,
        "grad_norm": 4.702218532562256,
        "learning_rate": 0.00019228223193571632,
        "epoch": 0.7532,
        "step": 5649
    },
    {
        "loss": 2.789,
        "grad_norm": 2.2794384956359863,
        "learning_rate": 0.0001922579687976232,
        "epoch": 0.7533333333333333,
        "step": 5650
    },
    {
        "loss": 1.6503,
        "grad_norm": 3.2121689319610596,
        "learning_rate": 0.00019223366911583426,
        "epoch": 0.7534666666666666,
        "step": 5651
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.896334171295166,
        "learning_rate": 0.00019220933289997475,
        "epoch": 0.7536,
        "step": 5652
    },
    {
        "loss": 2.2717,
        "grad_norm": 3.8730475902557373,
        "learning_rate": 0.00019218496015968424,
        "epoch": 0.7537333333333334,
        "step": 5653
    },
    {
        "loss": 1.6754,
        "grad_norm": 4.417121410369873,
        "learning_rate": 0.00019216055090461693,
        "epoch": 0.7538666666666667,
        "step": 5654
    },
    {
        "loss": 0.9833,
        "grad_norm": 2.594235420227051,
        "learning_rate": 0.00019213610514444134,
        "epoch": 0.754,
        "step": 5655
    },
    {
        "loss": 2.3031,
        "grad_norm": 3.3422458171844482,
        "learning_rate": 0.00019211162288884057,
        "epoch": 0.7541333333333333,
        "step": 5656
    },
    {
        "loss": 2.0957,
        "grad_norm": 3.609687089920044,
        "learning_rate": 0.0001920871041475121,
        "epoch": 0.7542666666666666,
        "step": 5657
    },
    {
        "loss": 2.1173,
        "grad_norm": 3.07185435295105,
        "learning_rate": 0.00019206254893016795,
        "epoch": 0.7544,
        "step": 5658
    },
    {
        "loss": 2.578,
        "grad_norm": 3.0028958320617676,
        "learning_rate": 0.00019203795724653442,
        "epoch": 0.7545333333333333,
        "step": 5659
    },
    {
        "loss": 1.375,
        "grad_norm": 6.968168258666992,
        "learning_rate": 0.00019201332910635243,
        "epoch": 0.7546666666666667,
        "step": 5660
    },
    {
        "loss": 2.3654,
        "grad_norm": 4.229857921600342,
        "learning_rate": 0.00019198866451937727,
        "epoch": 0.7548,
        "step": 5661
    },
    {
        "loss": 2.5321,
        "grad_norm": 3.4691388607025146,
        "learning_rate": 0.00019196396349537866,
        "epoch": 0.7549333333333333,
        "step": 5662
    },
    {
        "loss": 2.9431,
        "grad_norm": 2.6327686309814453,
        "learning_rate": 0.0001919392260441407,
        "epoch": 0.7550666666666667,
        "step": 5663
    },
    {
        "loss": 2.2448,
        "grad_norm": 3.7288637161254883,
        "learning_rate": 0.00019191445217546206,
        "epoch": 0.7552,
        "step": 5664
    },
    {
        "loss": 1.9896,
        "grad_norm": 2.746715545654297,
        "learning_rate": 0.00019188964189915567,
        "epoch": 0.7553333333333333,
        "step": 5665
    },
    {
        "loss": 2.3392,
        "grad_norm": 4.095302104949951,
        "learning_rate": 0.0001918647952250491,
        "epoch": 0.7554666666666666,
        "step": 5666
    },
    {
        "loss": 2.9436,
        "grad_norm": 4.136716842651367,
        "learning_rate": 0.00019183991216298408,
        "epoch": 0.7556,
        "step": 5667
    },
    {
        "loss": 2.0578,
        "grad_norm": 3.1823527812957764,
        "learning_rate": 0.00019181499272281696,
        "epoch": 0.7557333333333334,
        "step": 5668
    },
    {
        "loss": 1.9164,
        "grad_norm": 3.806187868118286,
        "learning_rate": 0.0001917900369144183,
        "epoch": 0.7558666666666667,
        "step": 5669
    },
    {
        "loss": 1.7561,
        "grad_norm": 3.223095417022705,
        "learning_rate": 0.0001917650447476733,
        "epoch": 0.756,
        "step": 5670
    },
    {
        "loss": 2.7217,
        "grad_norm": 3.9648213386535645,
        "learning_rate": 0.00019174001623248135,
        "epoch": 0.7561333333333333,
        "step": 5671
    },
    {
        "loss": 2.7233,
        "grad_norm": 3.224306583404541,
        "learning_rate": 0.0001917149513787564,
        "epoch": 0.7562666666666666,
        "step": 5672
    },
    {
        "loss": 2.5019,
        "grad_norm": 2.4527196884155273,
        "learning_rate": 0.0001916898501964267,
        "epoch": 0.7564,
        "step": 5673
    },
    {
        "loss": 2.0696,
        "grad_norm": 3.9174981117248535,
        "learning_rate": 0.0001916647126954349,
        "epoch": 0.7565333333333333,
        "step": 5674
    },
    {
        "loss": 1.9911,
        "grad_norm": 3.455319404602051,
        "learning_rate": 0.00019163953888573807,
        "epoch": 0.7566666666666667,
        "step": 5675
    },
    {
        "loss": 3.4944,
        "grad_norm": 3.703561305999756,
        "learning_rate": 0.00019161432877730762,
        "epoch": 0.7568,
        "step": 5676
    },
    {
        "loss": 2.5121,
        "grad_norm": 3.379384994506836,
        "learning_rate": 0.0001915890823801294,
        "epoch": 0.7569333333333333,
        "step": 5677
    },
    {
        "loss": 2.6403,
        "grad_norm": 3.1207165718078613,
        "learning_rate": 0.00019156379970420356,
        "epoch": 0.7570666666666667,
        "step": 5678
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.0670502185821533,
        "learning_rate": 0.00019153848075954466,
        "epoch": 0.7572,
        "step": 5679
    },
    {
        "loss": 2.1718,
        "grad_norm": 3.349729299545288,
        "learning_rate": 0.00019151312555618162,
        "epoch": 0.7573333333333333,
        "step": 5680
    },
    {
        "loss": 2.8966,
        "grad_norm": 2.5001959800720215,
        "learning_rate": 0.0001914877341041577,
        "epoch": 0.7574666666666666,
        "step": 5681
    },
    {
        "loss": 1.6701,
        "grad_norm": 3.467092514038086,
        "learning_rate": 0.0001914623064135306,
        "epoch": 0.7576,
        "step": 5682
    },
    {
        "loss": 2.3642,
        "grad_norm": 2.6348538398742676,
        "learning_rate": 0.00019143684249437227,
        "epoch": 0.7577333333333334,
        "step": 5683
    },
    {
        "loss": 2.5984,
        "grad_norm": 3.564988613128662,
        "learning_rate": 0.00019141134235676902,
        "epoch": 0.7578666666666667,
        "step": 5684
    },
    {
        "loss": 2.4446,
        "grad_norm": 3.2837560176849365,
        "learning_rate": 0.00019138580601082167,
        "epoch": 0.758,
        "step": 5685
    },
    {
        "loss": 2.6841,
        "grad_norm": 3.270000457763672,
        "learning_rate": 0.00019136023346664512,
        "epoch": 0.7581333333333333,
        "step": 5686
    },
    {
        "loss": 2.4029,
        "grad_norm": 3.534970998764038,
        "learning_rate": 0.00019133462473436882,
        "epoch": 0.7582666666666666,
        "step": 5687
    },
    {
        "loss": 2.7472,
        "grad_norm": 3.2999794483184814,
        "learning_rate": 0.0001913089798241364,
        "epoch": 0.7584,
        "step": 5688
    },
    {
        "loss": 1.9024,
        "grad_norm": 3.249315023422241,
        "learning_rate": 0.00019128329874610597,
        "epoch": 0.7585333333333333,
        "step": 5689
    },
    {
        "loss": 0.812,
        "grad_norm": 3.156283378601074,
        "learning_rate": 0.00019125758151044983,
        "epoch": 0.7586666666666667,
        "step": 5690
    },
    {
        "loss": 3.1246,
        "grad_norm": 3.239187002182007,
        "learning_rate": 0.00019123182812735476,
        "epoch": 0.7588,
        "step": 5691
    },
    {
        "loss": 1.857,
        "grad_norm": 8.156145095825195,
        "learning_rate": 0.00019120603860702166,
        "epoch": 0.7589333333333333,
        "step": 5692
    },
    {
        "loss": 1.6807,
        "grad_norm": 3.627965211868286,
        "learning_rate": 0.0001911802129596659,
        "epoch": 0.7590666666666667,
        "step": 5693
    },
    {
        "loss": 2.3704,
        "grad_norm": 2.7643818855285645,
        "learning_rate": 0.00019115435119551713,
        "epoch": 0.7592,
        "step": 5694
    },
    {
        "loss": 2.8579,
        "grad_norm": 3.3712961673736572,
        "learning_rate": 0.00019112845332481921,
        "epoch": 0.7593333333333333,
        "step": 5695
    },
    {
        "loss": 2.6612,
        "grad_norm": 3.5772616863250732,
        "learning_rate": 0.00019110251935783047,
        "epoch": 0.7594666666666666,
        "step": 5696
    },
    {
        "loss": 2.4604,
        "grad_norm": 3.8435680866241455,
        "learning_rate": 0.00019107654930482332,
        "epoch": 0.7596,
        "step": 5697
    },
    {
        "loss": 1.0468,
        "grad_norm": 5.499910831451416,
        "learning_rate": 0.0001910505431760847,
        "epoch": 0.7597333333333334,
        "step": 5698
    },
    {
        "loss": 2.9236,
        "grad_norm": 2.953878879547119,
        "learning_rate": 0.00019102450098191565,
        "epoch": 0.7598666666666667,
        "step": 5699
    },
    {
        "loss": 1.1976,
        "grad_norm": 2.6373801231384277,
        "learning_rate": 0.00019099842273263162,
        "epoch": 0.76,
        "step": 5700
    },
    {
        "loss": 2.2355,
        "grad_norm": 3.4289920330047607,
        "learning_rate": 0.00019097230843856226,
        "epoch": 0.7601333333333333,
        "step": 5701
    },
    {
        "loss": 1.6657,
        "grad_norm": 2.690922260284424,
        "learning_rate": 0.00019094615811005158,
        "epoch": 0.7602666666666666,
        "step": 5702
    },
    {
        "loss": 2.2166,
        "grad_norm": 2.2913451194763184,
        "learning_rate": 0.00019091997175745777,
        "epoch": 0.7604,
        "step": 5703
    },
    {
        "loss": 2.3555,
        "grad_norm": 3.5443577766418457,
        "learning_rate": 0.00019089374939115333,
        "epoch": 0.7605333333333333,
        "step": 5704
    },
    {
        "loss": 2.936,
        "grad_norm": 2.0965805053710938,
        "learning_rate": 0.00019086749102152506,
        "epoch": 0.7606666666666667,
        "step": 5705
    },
    {
        "loss": 3.1276,
        "grad_norm": 3.3130762577056885,
        "learning_rate": 0.00019084119665897396,
        "epoch": 0.7608,
        "step": 5706
    },
    {
        "loss": 2.0921,
        "grad_norm": 3.5921785831451416,
        "learning_rate": 0.00019081486631391536,
        "epoch": 0.7609333333333334,
        "step": 5707
    },
    {
        "loss": 2.0456,
        "grad_norm": 4.150162696838379,
        "learning_rate": 0.0001907884999967787,
        "epoch": 0.7610666666666667,
        "step": 5708
    },
    {
        "loss": 2.6392,
        "grad_norm": 2.6679000854492188,
        "learning_rate": 0.0001907620977180079,
        "epoch": 0.7612,
        "step": 5709
    },
    {
        "loss": 1.7302,
        "grad_norm": 3.6417348384857178,
        "learning_rate": 0.0001907356594880609,
        "epoch": 0.7613333333333333,
        "step": 5710
    },
    {
        "loss": 2.7277,
        "grad_norm": 3.2924163341522217,
        "learning_rate": 0.00019070918531741002,
        "epoch": 0.7614666666666666,
        "step": 5711
    },
    {
        "loss": 1.3804,
        "grad_norm": 4.025538921356201,
        "learning_rate": 0.00019068267521654177,
        "epoch": 0.7616,
        "step": 5712
    },
    {
        "loss": 2.3121,
        "grad_norm": 3.494628667831421,
        "learning_rate": 0.00019065612919595684,
        "epoch": 0.7617333333333334,
        "step": 5713
    },
    {
        "loss": 2.0635,
        "grad_norm": 3.44627046585083,
        "learning_rate": 0.00019062954726617023,
        "epoch": 0.7618666666666667,
        "step": 5714
    },
    {
        "loss": 1.4685,
        "grad_norm": 3.6107723712921143,
        "learning_rate": 0.00019060292943771112,
        "epoch": 0.762,
        "step": 5715
    },
    {
        "loss": 2.6601,
        "grad_norm": 2.686958074569702,
        "learning_rate": 0.000190576275721123,
        "epoch": 0.7621333333333333,
        "step": 5716
    },
    {
        "loss": 2.0954,
        "grad_norm": 3.6417598724365234,
        "learning_rate": 0.00019054958612696335,
        "epoch": 0.7622666666666666,
        "step": 5717
    },
    {
        "loss": 2.373,
        "grad_norm": 3.7143607139587402,
        "learning_rate": 0.00019052286066580416,
        "epoch": 0.7624,
        "step": 5718
    },
    {
        "loss": 1.867,
        "grad_norm": 3.0459041595458984,
        "learning_rate": 0.0001904960993482314,
        "epoch": 0.7625333333333333,
        "step": 5719
    },
    {
        "loss": 1.6909,
        "grad_norm": 4.722304821014404,
        "learning_rate": 0.00019046930218484534,
        "epoch": 0.7626666666666667,
        "step": 5720
    },
    {
        "loss": 2.9581,
        "grad_norm": 3.357678174972534,
        "learning_rate": 0.0001904424691862604,
        "epoch": 0.7628,
        "step": 5721
    },
    {
        "loss": 2.7455,
        "grad_norm": 2.653700828552246,
        "learning_rate": 0.00019041560036310524,
        "epoch": 0.7629333333333334,
        "step": 5722
    },
    {
        "loss": 3.839,
        "grad_norm": 3.4855430126190186,
        "learning_rate": 0.0001903886957260227,
        "epoch": 0.7630666666666667,
        "step": 5723
    },
    {
        "loss": 2.4219,
        "grad_norm": 4.458379745483398,
        "learning_rate": 0.00019036175528566976,
        "epoch": 0.7632,
        "step": 5724
    },
    {
        "loss": 2.7703,
        "grad_norm": 5.192643642425537,
        "learning_rate": 0.0001903347790527177,
        "epoch": 0.7633333333333333,
        "step": 5725
    },
    {
        "loss": 2.7762,
        "grad_norm": 2.861234188079834,
        "learning_rate": 0.00019030776703785182,
        "epoch": 0.7634666666666666,
        "step": 5726
    },
    {
        "loss": 2.6184,
        "grad_norm": 2.9061696529388428,
        "learning_rate": 0.00019028071925177172,
        "epoch": 0.7636,
        "step": 5727
    },
    {
        "loss": 2.5261,
        "grad_norm": 3.6776392459869385,
        "learning_rate": 0.00019025363570519108,
        "epoch": 0.7637333333333334,
        "step": 5728
    },
    {
        "loss": 2.321,
        "grad_norm": 3.319697856903076,
        "learning_rate": 0.00019022651640883783,
        "epoch": 0.7638666666666667,
        "step": 5729
    },
    {
        "loss": 2.2345,
        "grad_norm": 3.596945285797119,
        "learning_rate": 0.00019019936137345397,
        "epoch": 0.764,
        "step": 5730
    },
    {
        "loss": 2.1661,
        "grad_norm": 3.4260923862457275,
        "learning_rate": 0.0001901721706097957,
        "epoch": 0.7641333333333333,
        "step": 5731
    },
    {
        "loss": 2.8501,
        "grad_norm": 3.298035144805908,
        "learning_rate": 0.00019014494412863346,
        "epoch": 0.7642666666666666,
        "step": 5732
    },
    {
        "loss": 2.2497,
        "grad_norm": 4.174793720245361,
        "learning_rate": 0.00019011768194075163,
        "epoch": 0.7644,
        "step": 5733
    },
    {
        "loss": 2.8966,
        "grad_norm": 1.845329761505127,
        "learning_rate": 0.0001900903840569489,
        "epoch": 0.7645333333333333,
        "step": 5734
    },
    {
        "loss": 3.3505,
        "grad_norm": 2.8277907371520996,
        "learning_rate": 0.00019006305048803812,
        "epoch": 0.7646666666666667,
        "step": 5735
    },
    {
        "loss": 2.5105,
        "grad_norm": 3.613950490951538,
        "learning_rate": 0.0001900356812448462,
        "epoch": 0.7648,
        "step": 5736
    },
    {
        "loss": 1.1982,
        "grad_norm": 3.7914888858795166,
        "learning_rate": 0.00019000827633821406,
        "epoch": 0.7649333333333334,
        "step": 5737
    },
    {
        "loss": 2.5643,
        "grad_norm": 3.8469746112823486,
        "learning_rate": 0.00018998083577899703,
        "epoch": 0.7650666666666667,
        "step": 5738
    },
    {
        "loss": 1.6716,
        "grad_norm": 3.8460381031036377,
        "learning_rate": 0.0001899533595780643,
        "epoch": 0.7652,
        "step": 5739
    },
    {
        "loss": 2.728,
        "grad_norm": 8.586610794067383,
        "learning_rate": 0.00018992584774629934,
        "epoch": 0.7653333333333333,
        "step": 5740
    },
    {
        "loss": 2.1074,
        "grad_norm": 4.7753753662109375,
        "learning_rate": 0.0001898983002945997,
        "epoch": 0.7654666666666666,
        "step": 5741
    },
    {
        "loss": 2.5162,
        "grad_norm": 3.0587332248687744,
        "learning_rate": 0.00018987071723387697,
        "epoch": 0.7656,
        "step": 5742
    },
    {
        "loss": 2.6914,
        "grad_norm": 2.063884735107422,
        "learning_rate": 0.00018984309857505692,
        "epoch": 0.7657333333333334,
        "step": 5743
    },
    {
        "loss": 2.7014,
        "grad_norm": 1.578616738319397,
        "learning_rate": 0.00018981544432907944,
        "epoch": 0.7658666666666667,
        "step": 5744
    },
    {
        "loss": 2.777,
        "grad_norm": 3.029104471206665,
        "learning_rate": 0.00018978775450689833,
        "epoch": 0.766,
        "step": 5745
    },
    {
        "loss": 2.313,
        "grad_norm": 3.1862118244171143,
        "learning_rate": 0.00018976002911948178,
        "epoch": 0.7661333333333333,
        "step": 5746
    },
    {
        "loss": 2.6826,
        "grad_norm": 2.30753493309021,
        "learning_rate": 0.00018973226817781185,
        "epoch": 0.7662666666666667,
        "step": 5747
    },
    {
        "loss": 2.5425,
        "grad_norm": 2.530148506164551,
        "learning_rate": 0.00018970447169288471,
        "epoch": 0.7664,
        "step": 5748
    },
    {
        "loss": 2.7357,
        "grad_norm": 2.1895711421966553,
        "learning_rate": 0.00018967663967571065,
        "epoch": 0.7665333333333333,
        "step": 5749
    },
    {
        "loss": 2.2197,
        "grad_norm": 4.842834949493408,
        "learning_rate": 0.0001896487721373141,
        "epoch": 0.7666666666666667,
        "step": 5750
    },
    {
        "loss": 2.043,
        "grad_norm": 2.996107339859009,
        "learning_rate": 0.00018962086908873342,
        "epoch": 0.7668,
        "step": 5751
    },
    {
        "loss": 2.5964,
        "grad_norm": 3.1624062061309814,
        "learning_rate": 0.00018959293054102112,
        "epoch": 0.7669333333333334,
        "step": 5752
    },
    {
        "loss": 3.0757,
        "grad_norm": 2.1912620067596436,
        "learning_rate": 0.00018956495650524375,
        "epoch": 0.7670666666666667,
        "step": 5753
    },
    {
        "loss": 1.5052,
        "grad_norm": 3.596134662628174,
        "learning_rate": 0.0001895369469924819,
        "epoch": 0.7672,
        "step": 5754
    },
    {
        "loss": 2.6918,
        "grad_norm": 2.6830170154571533,
        "learning_rate": 0.0001895089020138303,
        "epoch": 0.7673333333333333,
        "step": 5755
    },
    {
        "loss": 2.3322,
        "grad_norm": 4.016264915466309,
        "learning_rate": 0.00018948082158039758,
        "epoch": 0.7674666666666666,
        "step": 5756
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.9611620903015137,
        "learning_rate": 0.00018945270570330656,
        "epoch": 0.7676,
        "step": 5757
    },
    {
        "loss": 2.6465,
        "grad_norm": 2.907320261001587,
        "learning_rate": 0.00018942455439369398,
        "epoch": 0.7677333333333334,
        "step": 5758
    },
    {
        "loss": 2.3887,
        "grad_norm": 4.438436985015869,
        "learning_rate": 0.00018939636766271073,
        "epoch": 0.7678666666666667,
        "step": 5759
    },
    {
        "loss": 2.0717,
        "grad_norm": 2.861990213394165,
        "learning_rate": 0.0001893681455215216,
        "epoch": 0.768,
        "step": 5760
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.297499656677246,
        "learning_rate": 0.00018933988798130555,
        "epoch": 0.7681333333333333,
        "step": 5761
    },
    {
        "loss": 2.7105,
        "grad_norm": 2.4277591705322266,
        "learning_rate": 0.00018931159505325545,
        "epoch": 0.7682666666666667,
        "step": 5762
    },
    {
        "loss": 0.6291,
        "grad_norm": 3.860347032546997,
        "learning_rate": 0.00018928326674857822,
        "epoch": 0.7684,
        "step": 5763
    },
    {
        "loss": 3.0432,
        "grad_norm": 2.444627046585083,
        "learning_rate": 0.00018925490307849477,
        "epoch": 0.7685333333333333,
        "step": 5764
    },
    {
        "loss": 2.7598,
        "grad_norm": 2.8475735187530518,
        "learning_rate": 0.00018922650405424013,
        "epoch": 0.7686666666666667,
        "step": 5765
    },
    {
        "loss": 2.6899,
        "grad_norm": 2.8614399433135986,
        "learning_rate": 0.0001891980696870632,
        "epoch": 0.7688,
        "step": 5766
    },
    {
        "loss": 2.269,
        "grad_norm": 3.0402259826660156,
        "learning_rate": 0.0001891695999882269,
        "epoch": 0.7689333333333334,
        "step": 5767
    },
    {
        "loss": 1.5421,
        "grad_norm": 5.200808048248291,
        "learning_rate": 0.00018914109496900823,
        "epoch": 0.7690666666666667,
        "step": 5768
    },
    {
        "loss": 2.8789,
        "grad_norm": 2.502650499343872,
        "learning_rate": 0.00018911255464069813,
        "epoch": 0.7692,
        "step": 5769
    },
    {
        "loss": 3.0572,
        "grad_norm": 2.5921590328216553,
        "learning_rate": 0.00018908397901460142,
        "epoch": 0.7693333333333333,
        "step": 5770
    },
    {
        "loss": 2.7895,
        "grad_norm": 1.9912233352661133,
        "learning_rate": 0.00018905536810203714,
        "epoch": 0.7694666666666666,
        "step": 5771
    },
    {
        "loss": 2.3509,
        "grad_norm": 3.309969186782837,
        "learning_rate": 0.00018902672191433809,
        "epoch": 0.7696,
        "step": 5772
    },
    {
        "loss": 2.338,
        "grad_norm": 2.755655288696289,
        "learning_rate": 0.00018899804046285115,
        "epoch": 0.7697333333333334,
        "step": 5773
    },
    {
        "loss": 1.0672,
        "grad_norm": 3.945636749267578,
        "learning_rate": 0.00018896932375893704,
        "epoch": 0.7698666666666667,
        "step": 5774
    },
    {
        "loss": 2.5338,
        "grad_norm": 3.5037343502044678,
        "learning_rate": 0.00018894057181397073,
        "epoch": 0.77,
        "step": 5775
    },
    {
        "loss": 1.1173,
        "grad_norm": 6.617015361785889,
        "learning_rate": 0.00018891178463934085,
        "epoch": 0.7701333333333333,
        "step": 5776
    },
    {
        "loss": 2.3702,
        "grad_norm": 2.825469493865967,
        "learning_rate": 0.00018888296224645008,
        "epoch": 0.7702666666666667,
        "step": 5777
    },
    {
        "loss": 2.3125,
        "grad_norm": 4.110517978668213,
        "learning_rate": 0.00018885410464671517,
        "epoch": 0.7704,
        "step": 5778
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.952486515045166,
        "learning_rate": 0.00018882521185156659,
        "epoch": 0.7705333333333333,
        "step": 5779
    },
    {
        "loss": 0.8472,
        "grad_norm": 2.3770909309387207,
        "learning_rate": 0.00018879628387244896,
        "epoch": 0.7706666666666667,
        "step": 5780
    },
    {
        "loss": 2.5925,
        "grad_norm": 2.596802234649658,
        "learning_rate": 0.0001887673207208207,
        "epoch": 0.7708,
        "step": 5781
    },
    {
        "loss": 2.8011,
        "grad_norm": 2.3900563716888428,
        "learning_rate": 0.00018873832240815425,
        "epoch": 0.7709333333333334,
        "step": 5782
    },
    {
        "loss": 2.7289,
        "grad_norm": 2.3686063289642334,
        "learning_rate": 0.0001887092889459359,
        "epoch": 0.7710666666666667,
        "step": 5783
    },
    {
        "loss": 1.8221,
        "grad_norm": 3.6952240467071533,
        "learning_rate": 0.00018868022034566594,
        "epoch": 0.7712,
        "step": 5784
    },
    {
        "loss": 2.9355,
        "grad_norm": 2.3828983306884766,
        "learning_rate": 0.00018865111661885855,
        "epoch": 0.7713333333333333,
        "step": 5785
    },
    {
        "loss": 1.761,
        "grad_norm": 4.049882888793945,
        "learning_rate": 0.0001886219777770418,
        "epoch": 0.7714666666666666,
        "step": 5786
    },
    {
        "loss": 3.04,
        "grad_norm": 3.585958480834961,
        "learning_rate": 0.00018859280383175765,
        "epoch": 0.7716,
        "step": 5787
    },
    {
        "loss": 2.4533,
        "grad_norm": 3.51168155670166,
        "learning_rate": 0.00018856359479456206,
        "epoch": 0.7717333333333334,
        "step": 5788
    },
    {
        "loss": 1.9527,
        "grad_norm": 3.0350968837738037,
        "learning_rate": 0.0001885343506770248,
        "epoch": 0.7718666666666667,
        "step": 5789
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.2782042026519775,
        "learning_rate": 0.00018850507149072954,
        "epoch": 0.772,
        "step": 5790
    },
    {
        "loss": 2.9515,
        "grad_norm": 2.4666855335235596,
        "learning_rate": 0.0001884757572472739,
        "epoch": 0.7721333333333333,
        "step": 5791
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.5436031818389893,
        "learning_rate": 0.00018844640795826934,
        "epoch": 0.7722666666666667,
        "step": 5792
    },
    {
        "loss": 2.1011,
        "grad_norm": 3.4906694889068604,
        "learning_rate": 0.00018841702363534118,
        "epoch": 0.7724,
        "step": 5793
    },
    {
        "loss": 3.2257,
        "grad_norm": 2.140995740890503,
        "learning_rate": 0.00018838760429012872,
        "epoch": 0.7725333333333333,
        "step": 5794
    },
    {
        "loss": 2.5453,
        "grad_norm": 3.731945514678955,
        "learning_rate": 0.00018835814993428503,
        "epoch": 0.7726666666666666,
        "step": 5795
    },
    {
        "loss": 1.7554,
        "grad_norm": 3.2775371074676514,
        "learning_rate": 0.00018832866057947705,
        "epoch": 0.7728,
        "step": 5796
    },
    {
        "loss": 2.6062,
        "grad_norm": 2.9581851959228516,
        "learning_rate": 0.0001882991362373857,
        "epoch": 0.7729333333333334,
        "step": 5797
    },
    {
        "loss": 2.815,
        "grad_norm": 3.0772438049316406,
        "learning_rate": 0.00018826957691970558,
        "epoch": 0.7730666666666667,
        "step": 5798
    },
    {
        "loss": 1.6723,
        "grad_norm": 3.565563440322876,
        "learning_rate": 0.00018823998263814523,
        "epoch": 0.7732,
        "step": 5799
    },
    {
        "loss": 1.6841,
        "grad_norm": 4.7871994972229,
        "learning_rate": 0.00018821035340442713,
        "epoch": 0.7733333333333333,
        "step": 5800
    },
    {
        "loss": 1.8631,
        "grad_norm": 2.778618812561035,
        "learning_rate": 0.00018818068923028748,
        "epoch": 0.7734666666666666,
        "step": 5801
    },
    {
        "loss": 2.5031,
        "grad_norm": 4.29323673248291,
        "learning_rate": 0.00018815099012747634,
        "epoch": 0.7736,
        "step": 5802
    },
    {
        "loss": 1.6966,
        "grad_norm": 6.259536266326904,
        "learning_rate": 0.0001881212561077577,
        "epoch": 0.7737333333333334,
        "step": 5803
    },
    {
        "loss": 2.5772,
        "grad_norm": 3.3242530822753906,
        "learning_rate": 0.00018809148718290918,
        "epoch": 0.7738666666666667,
        "step": 5804
    },
    {
        "loss": 2.4633,
        "grad_norm": 3.7376182079315186,
        "learning_rate": 0.00018806168336472245,
        "epoch": 0.774,
        "step": 5805
    },
    {
        "loss": 2.5013,
        "grad_norm": 3.1118292808532715,
        "learning_rate": 0.0001880318446650029,
        "epoch": 0.7741333333333333,
        "step": 5806
    },
    {
        "loss": 2.0498,
        "grad_norm": 4.041928291320801,
        "learning_rate": 0.00018800197109556968,
        "epoch": 0.7742666666666667,
        "step": 5807
    },
    {
        "loss": 2.1629,
        "grad_norm": 2.3869974613189697,
        "learning_rate": 0.00018797206266825582,
        "epoch": 0.7744,
        "step": 5808
    },
    {
        "loss": 3.0088,
        "grad_norm": 3.627814531326294,
        "learning_rate": 0.00018794211939490824,
        "epoch": 0.7745333333333333,
        "step": 5809
    },
    {
        "loss": 2.6287,
        "grad_norm": 3.035524368286133,
        "learning_rate": 0.00018791214128738748,
        "epoch": 0.7746666666666666,
        "step": 5810
    },
    {
        "loss": 2.947,
        "grad_norm": 5.503178119659424,
        "learning_rate": 0.000187882128357568,
        "epoch": 0.7748,
        "step": 5811
    },
    {
        "loss": 2.1688,
        "grad_norm": 4.769833087921143,
        "learning_rate": 0.00018785208061733804,
        "epoch": 0.7749333333333334,
        "step": 5812
    },
    {
        "loss": 2.2215,
        "grad_norm": 3.418456792831421,
        "learning_rate": 0.00018782199807859963,
        "epoch": 0.7750666666666667,
        "step": 5813
    },
    {
        "loss": 2.4059,
        "grad_norm": 5.208008289337158,
        "learning_rate": 0.0001877918807532685,
        "epoch": 0.7752,
        "step": 5814
    },
    {
        "loss": 2.6246,
        "grad_norm": 2.865858793258667,
        "learning_rate": 0.00018776172865327424,
        "epoch": 0.7753333333333333,
        "step": 5815
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.912808418273926,
        "learning_rate": 0.00018773154179056026,
        "epoch": 0.7754666666666666,
        "step": 5816
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.1067216396331787,
        "learning_rate": 0.0001877013201770836,
        "epoch": 0.7756,
        "step": 5817
    },
    {
        "loss": 2.6071,
        "grad_norm": 2.8775622844696045,
        "learning_rate": 0.00018767106382481524,
        "epoch": 0.7757333333333334,
        "step": 5818
    },
    {
        "loss": 3.0322,
        "grad_norm": 3.0268752574920654,
        "learning_rate": 0.0001876407727457397,
        "epoch": 0.7758666666666667,
        "step": 5819
    },
    {
        "loss": 2.492,
        "grad_norm": 3.939380407333374,
        "learning_rate": 0.0001876104469518555,
        "epoch": 0.776,
        "step": 5820
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.1119515895843506,
        "learning_rate": 0.00018758008645517471,
        "epoch": 0.7761333333333333,
        "step": 5821
    },
    {
        "loss": 2.4259,
        "grad_norm": 2.803560495376587,
        "learning_rate": 0.00018754969126772328,
        "epoch": 0.7762666666666667,
        "step": 5822
    },
    {
        "loss": 2.2247,
        "grad_norm": 3.7265312671661377,
        "learning_rate": 0.00018751926140154077,
        "epoch": 0.7764,
        "step": 5823
    },
    {
        "loss": 2.9275,
        "grad_norm": 3.2018332481384277,
        "learning_rate": 0.00018748879686868064,
        "epoch": 0.7765333333333333,
        "step": 5824
    },
    {
        "loss": 1.3808,
        "grad_norm": 9.768362998962402,
        "learning_rate": 0.00018745829768120997,
        "epoch": 0.7766666666666666,
        "step": 5825
    },
    {
        "loss": 2.3265,
        "grad_norm": 3.7653489112854004,
        "learning_rate": 0.00018742776385120954,
        "epoch": 0.7768,
        "step": 5826
    },
    {
        "loss": 1.6773,
        "grad_norm": 3.6974682807922363,
        "learning_rate": 0.000187397195390774,
        "epoch": 0.7769333333333334,
        "step": 5827
    },
    {
        "loss": 1.4596,
        "grad_norm": 3.6592025756835938,
        "learning_rate": 0.00018736659231201154,
        "epoch": 0.7770666666666667,
        "step": 5828
    },
    {
        "loss": 2.6947,
        "grad_norm": 2.0135793685913086,
        "learning_rate": 0.00018733595462704417,
        "epoch": 0.7772,
        "step": 5829
    },
    {
        "loss": 2.2287,
        "grad_norm": 4.361815929412842,
        "learning_rate": 0.00018730528234800757,
        "epoch": 0.7773333333333333,
        "step": 5830
    },
    {
        "loss": 2.0065,
        "grad_norm": 3.7052927017211914,
        "learning_rate": 0.00018727457548705116,
        "epoch": 0.7774666666666666,
        "step": 5831
    },
    {
        "loss": 2.8725,
        "grad_norm": 2.6605403423309326,
        "learning_rate": 0.00018724383405633804,
        "epoch": 0.7776,
        "step": 5832
    },
    {
        "loss": 1.95,
        "grad_norm": 3.1591620445251465,
        "learning_rate": 0.0001872130580680449,
        "epoch": 0.7777333333333334,
        "step": 5833
    },
    {
        "loss": 2.045,
        "grad_norm": 3.8415908813476562,
        "learning_rate": 0.00018718224753436235,
        "epoch": 0.7778666666666667,
        "step": 5834
    },
    {
        "loss": 2.255,
        "grad_norm": 2.4813404083251953,
        "learning_rate": 0.00018715140246749448,
        "epoch": 0.778,
        "step": 5835
    },
    {
        "loss": 1.8451,
        "grad_norm": 3.0185279846191406,
        "learning_rate": 0.00018712052287965912,
        "epoch": 0.7781333333333333,
        "step": 5836
    },
    {
        "loss": 1.1068,
        "grad_norm": 5.017889499664307,
        "learning_rate": 0.0001870896087830878,
        "epoch": 0.7782666666666667,
        "step": 5837
    },
    {
        "loss": 1.7768,
        "grad_norm": 3.6396570205688477,
        "learning_rate": 0.0001870586601900257,
        "epoch": 0.7784,
        "step": 5838
    },
    {
        "loss": 2.9923,
        "grad_norm": 3.319019317626953,
        "learning_rate": 0.00018702767711273167,
        "epoch": 0.7785333333333333,
        "step": 5839
    },
    {
        "loss": 2.6153,
        "grad_norm": 3.3186511993408203,
        "learning_rate": 0.00018699665956347814,
        "epoch": 0.7786666666666666,
        "step": 5840
    },
    {
        "loss": 2.4523,
        "grad_norm": 5.2924323081970215,
        "learning_rate": 0.00018696560755455138,
        "epoch": 0.7788,
        "step": 5841
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.6488938331604004,
        "learning_rate": 0.00018693452109825107,
        "epoch": 0.7789333333333334,
        "step": 5842
    },
    {
        "loss": 2.8218,
        "grad_norm": 3.582357883453369,
        "learning_rate": 0.00018690340020689078,
        "epoch": 0.7790666666666667,
        "step": 5843
    },
    {
        "loss": 2.7414,
        "grad_norm": 3.8198740482330322,
        "learning_rate": 0.00018687224489279754,
        "epoch": 0.7792,
        "step": 5844
    },
    {
        "loss": 1.516,
        "grad_norm": 8.143916130065918,
        "learning_rate": 0.0001868410551683121,
        "epoch": 0.7793333333333333,
        "step": 5845
    },
    {
        "loss": 2.4965,
        "grad_norm": 4.889052867889404,
        "learning_rate": 0.00018680983104578876,
        "epoch": 0.7794666666666666,
        "step": 5846
    },
    {
        "loss": 2.2932,
        "grad_norm": 3.9713239669799805,
        "learning_rate": 0.00018677857253759564,
        "epoch": 0.7796,
        "step": 5847
    },
    {
        "loss": 2.2976,
        "grad_norm": 5.82031774520874,
        "learning_rate": 0.00018674727965611415,
        "epoch": 0.7797333333333333,
        "step": 5848
    },
    {
        "loss": 1.4848,
        "grad_norm": 2.422539234161377,
        "learning_rate": 0.00018671595241373965,
        "epoch": 0.7798666666666667,
        "step": 5849
    },
    {
        "loss": 0.9055,
        "grad_norm": 3.5218117237091064,
        "learning_rate": 0.0001866845908228809,
        "epoch": 0.78,
        "step": 5850
    },
    {
        "loss": 3.1699,
        "grad_norm": 3.1873159408569336,
        "learning_rate": 0.00018665319489596038,
        "epoch": 0.7801333333333333,
        "step": 5851
    },
    {
        "loss": 2.7876,
        "grad_norm": 4.812132835388184,
        "learning_rate": 0.00018662176464541409,
        "epoch": 0.7802666666666667,
        "step": 5852
    },
    {
        "loss": 1.7353,
        "grad_norm": 20.959875106811523,
        "learning_rate": 0.00018659030008369166,
        "epoch": 0.7804,
        "step": 5853
    },
    {
        "loss": 1.2937,
        "grad_norm": 4.735249042510986,
        "learning_rate": 0.00018655880122325633,
        "epoch": 0.7805333333333333,
        "step": 5854
    },
    {
        "loss": 2.7641,
        "grad_norm": 5.039971828460693,
        "learning_rate": 0.00018652726807658488,
        "epoch": 0.7806666666666666,
        "step": 5855
    },
    {
        "loss": 2.2025,
        "grad_norm": 6.958970069885254,
        "learning_rate": 0.00018649570065616772,
        "epoch": 0.7808,
        "step": 5856
    },
    {
        "loss": 2.3966,
        "grad_norm": 4.3209357261657715,
        "learning_rate": 0.00018646409897450877,
        "epoch": 0.7809333333333334,
        "step": 5857
    },
    {
        "loss": 2.0699,
        "grad_norm": 5.727043628692627,
        "learning_rate": 0.00018643246304412566,
        "epoch": 0.7810666666666667,
        "step": 5858
    },
    {
        "loss": 1.4222,
        "grad_norm": 5.215400218963623,
        "learning_rate": 0.0001864007928775494,
        "epoch": 0.7812,
        "step": 5859
    },
    {
        "loss": 1.9818,
        "grad_norm": 4.289000034332275,
        "learning_rate": 0.00018636908848732465,
        "epoch": 0.7813333333333333,
        "step": 5860
    },
    {
        "loss": 1.3821,
        "grad_norm": 4.845095634460449,
        "learning_rate": 0.00018633734988600967,
        "epoch": 0.7814666666666666,
        "step": 5861
    },
    {
        "loss": 2.7342,
        "grad_norm": 2.111767530441284,
        "learning_rate": 0.0001863055770861762,
        "epoch": 0.7816,
        "step": 5862
    },
    {
        "loss": 2.5352,
        "grad_norm": 3.3479809761047363,
        "learning_rate": 0.0001862737701004096,
        "epoch": 0.7817333333333333,
        "step": 5863
    },
    {
        "loss": 2.2398,
        "grad_norm": 4.700682640075684,
        "learning_rate": 0.0001862419289413087,
        "epoch": 0.7818666666666667,
        "step": 5864
    },
    {
        "loss": 0.6596,
        "grad_norm": 4.1351799964904785,
        "learning_rate": 0.00018621005362148588,
        "epoch": 0.782,
        "step": 5865
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.978459596633911,
        "learning_rate": 0.00018617814415356705,
        "epoch": 0.7821333333333333,
        "step": 5866
    },
    {
        "loss": 0.6744,
        "grad_norm": 2.754716396331787,
        "learning_rate": 0.00018614620055019162,
        "epoch": 0.7822666666666667,
        "step": 5867
    },
    {
        "loss": 2.0161,
        "grad_norm": 3.581739664077759,
        "learning_rate": 0.0001861142228240127,
        "epoch": 0.7824,
        "step": 5868
    },
    {
        "loss": 2.1514,
        "grad_norm": 3.3133046627044678,
        "learning_rate": 0.00018608221098769665,
        "epoch": 0.7825333333333333,
        "step": 5869
    },
    {
        "loss": 3.0623,
        "grad_norm": 2.3781800270080566,
        "learning_rate": 0.00018605016505392354,
        "epoch": 0.7826666666666666,
        "step": 5870
    },
    {
        "loss": 1.9513,
        "grad_norm": 2.8286385536193848,
        "learning_rate": 0.00018601808503538683,
        "epoch": 0.7828,
        "step": 5871
    },
    {
        "loss": 2.7298,
        "grad_norm": 3.7625551223754883,
        "learning_rate": 0.00018598597094479356,
        "epoch": 0.7829333333333334,
        "step": 5872
    },
    {
        "loss": 2.7181,
        "grad_norm": 3.704144239425659,
        "learning_rate": 0.00018595382279486416,
        "epoch": 0.7830666666666667,
        "step": 5873
    },
    {
        "loss": 2.3992,
        "grad_norm": 3.9483227729797363,
        "learning_rate": 0.00018592164059833268,
        "epoch": 0.7832,
        "step": 5874
    },
    {
        "loss": 2.8139,
        "grad_norm": 2.076747417449951,
        "learning_rate": 0.00018588942436794663,
        "epoch": 0.7833333333333333,
        "step": 5875
    },
    {
        "loss": 2.615,
        "grad_norm": 3.1365537643432617,
        "learning_rate": 0.00018585717411646684,
        "epoch": 0.7834666666666666,
        "step": 5876
    },
    {
        "loss": 1.513,
        "grad_norm": 4.481649875640869,
        "learning_rate": 0.0001858248898566679,
        "epoch": 0.7836,
        "step": 5877
    },
    {
        "loss": 2.745,
        "grad_norm": 2.546297550201416,
        "learning_rate": 0.00018579257160133766,
        "epoch": 0.7837333333333333,
        "step": 5878
    },
    {
        "loss": 2.233,
        "grad_norm": 2.8076565265655518,
        "learning_rate": 0.00018576021936327747,
        "epoch": 0.7838666666666667,
        "step": 5879
    },
    {
        "loss": 2.8245,
        "grad_norm": 2.573714017868042,
        "learning_rate": 0.00018572783315530213,
        "epoch": 0.784,
        "step": 5880
    },
    {
        "loss": 2.3419,
        "grad_norm": 3.6290292739868164,
        "learning_rate": 0.00018569541299024,
        "epoch": 0.7841333333333333,
        "step": 5881
    },
    {
        "loss": 1.7017,
        "grad_norm": 3.236398220062256,
        "learning_rate": 0.00018566295888093278,
        "epoch": 0.7842666666666667,
        "step": 5882
    },
    {
        "loss": 1.7492,
        "grad_norm": 4.836870193481445,
        "learning_rate": 0.00018563047084023564,
        "epoch": 0.7844,
        "step": 5883
    },
    {
        "loss": 1.5333,
        "grad_norm": 2.9172284603118896,
        "learning_rate": 0.00018559794888101724,
        "epoch": 0.7845333333333333,
        "step": 5884
    },
    {
        "loss": 2.6595,
        "grad_norm": 2.8502862453460693,
        "learning_rate": 0.0001855653930161596,
        "epoch": 0.7846666666666666,
        "step": 5885
    },
    {
        "loss": 1.2804,
        "grad_norm": 3.125141143798828,
        "learning_rate": 0.00018553280325855825,
        "epoch": 0.7848,
        "step": 5886
    },
    {
        "loss": 2.7347,
        "grad_norm": 2.4269180297851562,
        "learning_rate": 0.0001855001796211221,
        "epoch": 0.7849333333333334,
        "step": 5887
    },
    {
        "loss": 1.8771,
        "grad_norm": 4.518274784088135,
        "learning_rate": 0.00018546752211677343,
        "epoch": 0.7850666666666667,
        "step": 5888
    },
    {
        "loss": 0.9015,
        "grad_norm": 3.731889247894287,
        "learning_rate": 0.00018543483075844806,
        "epoch": 0.7852,
        "step": 5889
    },
    {
        "loss": 2.6155,
        "grad_norm": 5.605831146240234,
        "learning_rate": 0.00018540210555909509,
        "epoch": 0.7853333333333333,
        "step": 5890
    },
    {
        "loss": 2.9925,
        "grad_norm": 3.3463246822357178,
        "learning_rate": 0.0001853693465316771,
        "epoch": 0.7854666666666666,
        "step": 5891
    },
    {
        "loss": 2.1236,
        "grad_norm": 5.213413715362549,
        "learning_rate": 0.00018533655368917006,
        "epoch": 0.7856,
        "step": 5892
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.9963130950927734,
        "learning_rate": 0.00018530372704456336,
        "epoch": 0.7857333333333333,
        "step": 5893
    },
    {
        "loss": 1.8662,
        "grad_norm": 2.394629955291748,
        "learning_rate": 0.00018527086661085967,
        "epoch": 0.7858666666666667,
        "step": 5894
    },
    {
        "loss": 1.1786,
        "grad_norm": 3.0868818759918213,
        "learning_rate": 0.00018523797240107518,
        "epoch": 0.786,
        "step": 5895
    },
    {
        "loss": 2.5601,
        "grad_norm": 2.7972733974456787,
        "learning_rate": 0.0001852050444282394,
        "epoch": 0.7861333333333334,
        "step": 5896
    },
    {
        "loss": 2.6856,
        "grad_norm": 2.563305377960205,
        "learning_rate": 0.00018517208270539518,
        "epoch": 0.7862666666666667,
        "step": 5897
    },
    {
        "loss": 1.333,
        "grad_norm": 3.4549999237060547,
        "learning_rate": 0.00018513908724559878,
        "epoch": 0.7864,
        "step": 5898
    },
    {
        "loss": 2.057,
        "grad_norm": 5.046323776245117,
        "learning_rate": 0.00018510605806191984,
        "epoch": 0.7865333333333333,
        "step": 5899
    },
    {
        "loss": 3.7046,
        "grad_norm": 3.424975872039795,
        "learning_rate": 0.0001850729951674413,
        "epoch": 0.7866666666666666,
        "step": 5900
    },
    {
        "loss": 2.8848,
        "grad_norm": 2.405076503753662,
        "learning_rate": 0.00018503989857525944,
        "epoch": 0.7868,
        "step": 5901
    },
    {
        "loss": 2.4891,
        "grad_norm": 3.75834059715271,
        "learning_rate": 0.00018500676829848403,
        "epoch": 0.7869333333333334,
        "step": 5902
    },
    {
        "loss": 3.0962,
        "grad_norm": 1.8255997896194458,
        "learning_rate": 0.00018497360435023805,
        "epoch": 0.7870666666666667,
        "step": 5903
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.3715505599975586,
        "learning_rate": 0.00018494040674365788,
        "epoch": 0.7872,
        "step": 5904
    },
    {
        "loss": 2.8748,
        "grad_norm": 3.2495250701904297,
        "learning_rate": 0.0001849071754918931,
        "epoch": 0.7873333333333333,
        "step": 5905
    },
    {
        "loss": 1.4406,
        "grad_norm": 4.704560279846191,
        "learning_rate": 0.00018487391060810685,
        "epoch": 0.7874666666666666,
        "step": 5906
    },
    {
        "loss": 1.9784,
        "grad_norm": 4.04236364364624,
        "learning_rate": 0.00018484061210547535,
        "epoch": 0.7876,
        "step": 5907
    },
    {
        "loss": 2.2878,
        "grad_norm": 3.025526285171509,
        "learning_rate": 0.00018480727999718835,
        "epoch": 0.7877333333333333,
        "step": 5908
    },
    {
        "loss": 2.675,
        "grad_norm": 4.974271297454834,
        "learning_rate": 0.0001847739142964488,
        "epoch": 0.7878666666666667,
        "step": 5909
    },
    {
        "loss": 1.9617,
        "grad_norm": 2.4443869590759277,
        "learning_rate": 0.0001847405150164729,
        "epoch": 0.788,
        "step": 5910
    },
    {
        "loss": 2.0493,
        "grad_norm": 3.886247396469116,
        "learning_rate": 0.0001847070821704902,
        "epoch": 0.7881333333333334,
        "step": 5911
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.2971928119659424,
        "learning_rate": 0.00018467361577174373,
        "epoch": 0.7882666666666667,
        "step": 5912
    },
    {
        "loss": 2.9064,
        "grad_norm": 3.493298053741455,
        "learning_rate": 0.00018464011583348947,
        "epoch": 0.7884,
        "step": 5913
    },
    {
        "loss": 1.5021,
        "grad_norm": 4.029465198516846,
        "learning_rate": 0.00018460658236899694,
        "epoch": 0.7885333333333333,
        "step": 5914
    },
    {
        "loss": 2.0604,
        "grad_norm": 3.6721715927124023,
        "learning_rate": 0.00018457301539154884,
        "epoch": 0.7886666666666666,
        "step": 5915
    },
    {
        "loss": 1.0246,
        "grad_norm": 3.02258563041687,
        "learning_rate": 0.00018453941491444113,
        "epoch": 0.7888,
        "step": 5916
    },
    {
        "loss": 2.5512,
        "grad_norm": 3.0232009887695312,
        "learning_rate": 0.00018450578095098314,
        "epoch": 0.7889333333333334,
        "step": 5917
    },
    {
        "loss": 2.9321,
        "grad_norm": 4.753697395324707,
        "learning_rate": 0.0001844721135144974,
        "epoch": 0.7890666666666667,
        "step": 5918
    },
    {
        "loss": 2.541,
        "grad_norm": 4.840531349182129,
        "learning_rate": 0.00018443841261831962,
        "epoch": 0.7892,
        "step": 5919
    },
    {
        "loss": 2.0873,
        "grad_norm": 3.302302360534668,
        "learning_rate": 0.0001844046782757989,
        "epoch": 0.7893333333333333,
        "step": 5920
    },
    {
        "loss": 2.3639,
        "grad_norm": 3.678255081176758,
        "learning_rate": 0.00018437091050029752,
        "epoch": 0.7894666666666666,
        "step": 5921
    },
    {
        "loss": 2.7805,
        "grad_norm": 4.946229934692383,
        "learning_rate": 0.000184337109305191,
        "epoch": 0.7896,
        "step": 5922
    },
    {
        "loss": 2.5102,
        "grad_norm": 2.594676971435547,
        "learning_rate": 0.00018430327470386816,
        "epoch": 0.7897333333333333,
        "step": 5923
    },
    {
        "loss": 1.0719,
        "grad_norm": 4.49614953994751,
        "learning_rate": 0.0001842694067097309,
        "epoch": 0.7898666666666667,
        "step": 5924
    },
    {
        "loss": 2.8671,
        "grad_norm": 4.343084335327148,
        "learning_rate": 0.0001842355053361945,
        "epoch": 0.79,
        "step": 5925
    },
    {
        "loss": 2.6875,
        "grad_norm": 3.7192232608795166,
        "learning_rate": 0.00018420157059668738,
        "epoch": 0.7901333333333334,
        "step": 5926
    },
    {
        "loss": 2.1895,
        "grad_norm": 4.696479797363281,
        "learning_rate": 0.00018416760250465131,
        "epoch": 0.7902666666666667,
        "step": 5927
    },
    {
        "loss": 1.36,
        "grad_norm": 4.181880950927734,
        "learning_rate": 0.00018413360107354104,
        "epoch": 0.7904,
        "step": 5928
    },
    {
        "loss": 2.3916,
        "grad_norm": 2.437638998031616,
        "learning_rate": 0.00018409956631682475,
        "epoch": 0.7905333333333333,
        "step": 5929
    },
    {
        "loss": 1.7741,
        "grad_norm": 2.7088520526885986,
        "learning_rate": 0.00018406549824798364,
        "epoch": 0.7906666666666666,
        "step": 5930
    },
    {
        "loss": 2.909,
        "grad_norm": 2.9703269004821777,
        "learning_rate": 0.0001840313968805123,
        "epoch": 0.7908,
        "step": 5931
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.7623276710510254,
        "learning_rate": 0.00018399726222791823,
        "epoch": 0.7909333333333334,
        "step": 5932
    },
    {
        "loss": 2.6505,
        "grad_norm": 3.739316940307617,
        "learning_rate": 0.0001839630943037224,
        "epoch": 0.7910666666666667,
        "step": 5933
    },
    {
        "loss": 2.8073,
        "grad_norm": 2.4518959522247314,
        "learning_rate": 0.0001839288931214589,
        "epoch": 0.7912,
        "step": 5934
    },
    {
        "loss": 2.8304,
        "grad_norm": 2.5171260833740234,
        "learning_rate": 0.00018389465869467472,
        "epoch": 0.7913333333333333,
        "step": 5935
    },
    {
        "loss": 2.6897,
        "grad_norm": 2.192643642425537,
        "learning_rate": 0.00018386039103693045,
        "epoch": 0.7914666666666667,
        "step": 5936
    },
    {
        "loss": 2.2562,
        "grad_norm": 5.327465057373047,
        "learning_rate": 0.00018382609016179952,
        "epoch": 0.7916,
        "step": 5937
    },
    {
        "loss": 2.8972,
        "grad_norm": 2.853790283203125,
        "learning_rate": 0.0001837917560828687,
        "epoch": 0.7917333333333333,
        "step": 5938
    },
    {
        "loss": 1.6644,
        "grad_norm": 6.244544506072998,
        "learning_rate": 0.0001837573888137377,
        "epoch": 0.7918666666666667,
        "step": 5939
    },
    {
        "loss": 2.8674,
        "grad_norm": 2.742600679397583,
        "learning_rate": 0.00018372298836801968,
        "epoch": 0.792,
        "step": 5940
    },
    {
        "loss": 2.8677,
        "grad_norm": 2.3052773475646973,
        "learning_rate": 0.00018368855475934065,
        "epoch": 0.7921333333333334,
        "step": 5941
    },
    {
        "loss": 2.0468,
        "grad_norm": 3.9021663665771484,
        "learning_rate": 0.00018365408800133989,
        "epoch": 0.7922666666666667,
        "step": 5942
    },
    {
        "loss": 1.3691,
        "grad_norm": 4.8626322746276855,
        "learning_rate": 0.00018361958810766987,
        "epoch": 0.7924,
        "step": 5943
    },
    {
        "loss": 1.9618,
        "grad_norm": 2.7341670989990234,
        "learning_rate": 0.00018358505509199604,
        "epoch": 0.7925333333333333,
        "step": 5944
    },
    {
        "loss": 2.071,
        "grad_norm": 3.3138351440429688,
        "learning_rate": 0.00018355048896799708,
        "epoch": 0.7926666666666666,
        "step": 5945
    },
    {
        "loss": 2.0902,
        "grad_norm": 2.913219451904297,
        "learning_rate": 0.00018351588974936477,
        "epoch": 0.7928,
        "step": 5946
    },
    {
        "loss": 2.908,
        "grad_norm": 2.7012059688568115,
        "learning_rate": 0.00018348125744980392,
        "epoch": 0.7929333333333334,
        "step": 5947
    },
    {
        "loss": 2.0651,
        "grad_norm": 2.7914412021636963,
        "learning_rate": 0.00018344659208303262,
        "epoch": 0.7930666666666667,
        "step": 5948
    },
    {
        "loss": 2.6065,
        "grad_norm": 3.8948421478271484,
        "learning_rate": 0.00018341189366278177,
        "epoch": 0.7932,
        "step": 5949
    },
    {
        "loss": 2.1633,
        "grad_norm": 3.4923219680786133,
        "learning_rate": 0.00018337716220279569,
        "epoch": 0.7933333333333333,
        "step": 5950
    },
    {
        "loss": 1.9275,
        "grad_norm": 8.381949424743652,
        "learning_rate": 0.0001833423977168315,
        "epoch": 0.7934666666666667,
        "step": 5951
    },
    {
        "loss": 2.0815,
        "grad_norm": 3.4357872009277344,
        "learning_rate": 0.00018330760021865962,
        "epoch": 0.7936,
        "step": 5952
    },
    {
        "loss": 2.429,
        "grad_norm": 2.8884401321411133,
        "learning_rate": 0.0001832727697220634,
        "epoch": 0.7937333333333333,
        "step": 5953
    },
    {
        "loss": 3.3216,
        "grad_norm": 2.9085004329681396,
        "learning_rate": 0.00018323790624083942,
        "epoch": 0.7938666666666667,
        "step": 5954
    },
    {
        "loss": 2.6955,
        "grad_norm": 3.366379737854004,
        "learning_rate": 0.0001832030097887971,
        "epoch": 0.794,
        "step": 5955
    },
    {
        "loss": 1.7026,
        "grad_norm": 3.781496524810791,
        "learning_rate": 0.00018316808037975907,
        "epoch": 0.7941333333333334,
        "step": 5956
    },
    {
        "loss": 3.0158,
        "grad_norm": 3.117251396179199,
        "learning_rate": 0.0001831331180275611,
        "epoch": 0.7942666666666667,
        "step": 5957
    },
    {
        "loss": 3.1498,
        "grad_norm": 3.814724922180176,
        "learning_rate": 0.00018309812274605174,
        "epoch": 0.7944,
        "step": 5958
    },
    {
        "loss": 0.9516,
        "grad_norm": 3.0254533290863037,
        "learning_rate": 0.00018306309454909286,
        "epoch": 0.7945333333333333,
        "step": 5959
    },
    {
        "loss": 2.3041,
        "grad_norm": 4.582470893859863,
        "learning_rate": 0.0001830280334505591,
        "epoch": 0.7946666666666666,
        "step": 5960
    },
    {
        "loss": 2.9851,
        "grad_norm": 3.5489649772644043,
        "learning_rate": 0.00018299293946433848,
        "epoch": 0.7948,
        "step": 5961
    },
    {
        "loss": 1.7986,
        "grad_norm": 3.235908269882202,
        "learning_rate": 0.0001829578126043317,
        "epoch": 0.7949333333333334,
        "step": 5962
    },
    {
        "loss": 2.2881,
        "grad_norm": 3.2537038326263428,
        "learning_rate": 0.0001829226528844527,
        "epoch": 0.7950666666666667,
        "step": 5963
    },
    {
        "loss": 2.2344,
        "grad_norm": 3.223026752471924,
        "learning_rate": 0.0001828874603186283,
        "epoch": 0.7952,
        "step": 5964
    },
    {
        "loss": 2.9234,
        "grad_norm": 2.114497423171997,
        "learning_rate": 0.0001828522349207985,
        "epoch": 0.7953333333333333,
        "step": 5965
    },
    {
        "loss": 2.429,
        "grad_norm": 4.018489837646484,
        "learning_rate": 0.00018281697670491612,
        "epoch": 0.7954666666666667,
        "step": 5966
    },
    {
        "loss": 3.1952,
        "grad_norm": 5.154625415802002,
        "learning_rate": 0.000182781685684947,
        "epoch": 0.7956,
        "step": 5967
    },
    {
        "loss": 1.9421,
        "grad_norm": 2.689595937728882,
        "learning_rate": 0.0001827463618748702,
        "epoch": 0.7957333333333333,
        "step": 5968
    },
    {
        "loss": 1.7857,
        "grad_norm": 3.4091060161590576,
        "learning_rate": 0.00018271100528867746,
        "epoch": 0.7958666666666666,
        "step": 5969
    },
    {
        "loss": 2.2271,
        "grad_norm": 2.9898598194122314,
        "learning_rate": 0.0001826756159403737,
        "epoch": 0.796,
        "step": 5970
    },
    {
        "loss": 2.5229,
        "grad_norm": 3.821702241897583,
        "learning_rate": 0.00018264019384397679,
        "epoch": 0.7961333333333334,
        "step": 5971
    },
    {
        "loss": 2.3,
        "grad_norm": 2.9697000980377197,
        "learning_rate": 0.00018260473901351746,
        "epoch": 0.7962666666666667,
        "step": 5972
    },
    {
        "loss": 2.027,
        "grad_norm": 3.4460654258728027,
        "learning_rate": 0.0001825692514630396,
        "epoch": 0.7964,
        "step": 5973
    },
    {
        "loss": 2.5844,
        "grad_norm": 3.249769449234009,
        "learning_rate": 0.00018253373120659982,
        "epoch": 0.7965333333333333,
        "step": 5974
    },
    {
        "loss": 1.908,
        "grad_norm": 3.7320847511291504,
        "learning_rate": 0.0001824981782582679,
        "epoch": 0.7966666666666666,
        "step": 5975
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.548975706100464,
        "learning_rate": 0.0001824625926321265,
        "epoch": 0.7968,
        "step": 5976
    },
    {
        "loss": 3.1997,
        "grad_norm": 3.510160207748413,
        "learning_rate": 0.00018242697434227116,
        "epoch": 0.7969333333333334,
        "step": 5977
    },
    {
        "loss": 2.3182,
        "grad_norm": 2.6821630001068115,
        "learning_rate": 0.00018239132340281037,
        "epoch": 0.7970666666666667,
        "step": 5978
    },
    {
        "loss": 1.7778,
        "grad_norm": 4.066198825836182,
        "learning_rate": 0.00018235563982786567,
        "epoch": 0.7972,
        "step": 5979
    },
    {
        "loss": 2.9308,
        "grad_norm": 3.5330965518951416,
        "learning_rate": 0.00018231992363157144,
        "epoch": 0.7973333333333333,
        "step": 5980
    },
    {
        "loss": 1.6302,
        "grad_norm": 4.09613561630249,
        "learning_rate": 0.00018228417482807493,
        "epoch": 0.7974666666666667,
        "step": 5981
    },
    {
        "loss": 2.0846,
        "grad_norm": 3.6804981231689453,
        "learning_rate": 0.00018224839343153643,
        "epoch": 0.7976,
        "step": 5982
    },
    {
        "loss": 2.1325,
        "grad_norm": 4.332008361816406,
        "learning_rate": 0.00018221257945612903,
        "epoch": 0.7977333333333333,
        "step": 5983
    },
    {
        "loss": 2.9271,
        "grad_norm": 4.144680976867676,
        "learning_rate": 0.00018217673291603877,
        "epoch": 0.7978666666666666,
        "step": 5984
    },
    {
        "loss": 2.0185,
        "grad_norm": 4.979991436004639,
        "learning_rate": 0.00018214085382546453,
        "epoch": 0.798,
        "step": 5985
    },
    {
        "loss": 3.1014,
        "grad_norm": 2.1325368881225586,
        "learning_rate": 0.0001821049421986183,
        "epoch": 0.7981333333333334,
        "step": 5986
    },
    {
        "loss": 2.6166,
        "grad_norm": 3.575479745864868,
        "learning_rate": 0.00018206899804972464,
        "epoch": 0.7982666666666667,
        "step": 5987
    },
    {
        "loss": 1.9783,
        "grad_norm": 3.2424490451812744,
        "learning_rate": 0.00018203302139302124,
        "epoch": 0.7984,
        "step": 5988
    },
    {
        "loss": 2.5195,
        "grad_norm": 2.574420928955078,
        "learning_rate": 0.00018199701224275852,
        "epoch": 0.7985333333333333,
        "step": 5989
    },
    {
        "loss": 2.584,
        "grad_norm": 2.2520909309387207,
        "learning_rate": 0.00018196097061319987,
        "epoch": 0.7986666666666666,
        "step": 5990
    },
    {
        "loss": 3.0589,
        "grad_norm": 3.3638522624969482,
        "learning_rate": 0.00018192489651862147,
        "epoch": 0.7988,
        "step": 5991
    },
    {
        "loss": 2.4999,
        "grad_norm": 3.949943780899048,
        "learning_rate": 0.0001818887899733124,
        "epoch": 0.7989333333333334,
        "step": 5992
    },
    {
        "loss": 2.0797,
        "grad_norm": 6.4883246421813965,
        "learning_rate": 0.00018185265099157465,
        "epoch": 0.7990666666666667,
        "step": 5993
    },
    {
        "loss": 2.9203,
        "grad_norm": 2.289552927017212,
        "learning_rate": 0.00018181647958772284,
        "epoch": 0.7992,
        "step": 5994
    },
    {
        "loss": 2.6814,
        "grad_norm": 3.23455548286438,
        "learning_rate": 0.00018178027577608473,
        "epoch": 0.7993333333333333,
        "step": 5995
    },
    {
        "loss": 2.8707,
        "grad_norm": 3.538904905319214,
        "learning_rate": 0.00018174403957100073,
        "epoch": 0.7994666666666667,
        "step": 5996
    },
    {
        "loss": 2.7538,
        "grad_norm": 3.0358798503875732,
        "learning_rate": 0.00018170777098682413,
        "epoch": 0.7996,
        "step": 5997
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.8290693759918213,
        "learning_rate": 0.00018167147003792102,
        "epoch": 0.7997333333333333,
        "step": 5998
    },
    {
        "loss": 1.6698,
        "grad_norm": 4.843513011932373,
        "learning_rate": 0.00018163513673867033,
        "epoch": 0.7998666666666666,
        "step": 5999
    },
    {
        "loss": 3.0695,
        "grad_norm": 4.0308380126953125,
        "learning_rate": 0.00018159877110346384,
        "epoch": 0.8,
        "step": 6000
    },
    {
        "loss": 2.5509,
        "grad_norm": 4.344172954559326,
        "learning_rate": 0.00018156237314670602,
        "epoch": 0.8001333333333334,
        "step": 6001
    },
    {
        "loss": 2.5524,
        "grad_norm": 4.6900715827941895,
        "learning_rate": 0.00018152594288281434,
        "epoch": 0.8002666666666667,
        "step": 6002
    },
    {
        "loss": 2.538,
        "grad_norm": 2.964740037918091,
        "learning_rate": 0.00018148948032621886,
        "epoch": 0.8004,
        "step": 6003
    },
    {
        "loss": 2.7399,
        "grad_norm": 3.3428311347961426,
        "learning_rate": 0.00018145298549136256,
        "epoch": 0.8005333333333333,
        "step": 6004
    },
    {
        "loss": 2.3036,
        "grad_norm": 5.740912437438965,
        "learning_rate": 0.0001814164583927012,
        "epoch": 0.8006666666666666,
        "step": 6005
    },
    {
        "loss": 2.0799,
        "grad_norm": 4.976456642150879,
        "learning_rate": 0.00018137989904470317,
        "epoch": 0.8008,
        "step": 6006
    },
    {
        "loss": 2.1587,
        "grad_norm": 2.9330894947052,
        "learning_rate": 0.00018134330746184994,
        "epoch": 0.8009333333333334,
        "step": 6007
    },
    {
        "loss": 2.2511,
        "grad_norm": 4.069992542266846,
        "learning_rate": 0.00018130668365863537,
        "epoch": 0.8010666666666667,
        "step": 6008
    },
    {
        "loss": 2.3569,
        "grad_norm": 3.394564628601074,
        "learning_rate": 0.0001812700276495664,
        "epoch": 0.8012,
        "step": 6009
    },
    {
        "loss": 2.6887,
        "grad_norm": 2.084320068359375,
        "learning_rate": 0.0001812333394491625,
        "epoch": 0.8013333333333333,
        "step": 6010
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.9090447425842285,
        "learning_rate": 0.0001811966190719561,
        "epoch": 0.8014666666666667,
        "step": 6011
    },
    {
        "loss": 2.6718,
        "grad_norm": 3.2742443084716797,
        "learning_rate": 0.00018115986653249218,
        "epoch": 0.8016,
        "step": 6012
    },
    {
        "loss": 2.0076,
        "grad_norm": 5.827483654022217,
        "learning_rate": 0.00018112308184532863,
        "epoch": 0.8017333333333333,
        "step": 6013
    },
    {
        "loss": 2.3039,
        "grad_norm": 2.228759527206421,
        "learning_rate": 0.0001810862650250359,
        "epoch": 0.8018666666666666,
        "step": 6014
    },
    {
        "loss": 2.6886,
        "grad_norm": 2.5657503604888916,
        "learning_rate": 0.0001810494160861973,
        "epoch": 0.802,
        "step": 6015
    },
    {
        "loss": 2.2111,
        "grad_norm": 2.973480224609375,
        "learning_rate": 0.00018101253504340884,
        "epoch": 0.8021333333333334,
        "step": 6016
    },
    {
        "loss": 0.837,
        "grad_norm": 2.755046844482422,
        "learning_rate": 0.00018097562191127917,
        "epoch": 0.8022666666666667,
        "step": 6017
    },
    {
        "loss": 1.5176,
        "grad_norm": 2.6348702907562256,
        "learning_rate": 0.0001809386767044298,
        "epoch": 0.8024,
        "step": 6018
    },
    {
        "loss": 2.4125,
        "grad_norm": 2.834914207458496,
        "learning_rate": 0.0001809016994374947,
        "epoch": 0.8025333333333333,
        "step": 6019
    },
    {
        "loss": 2.1716,
        "grad_norm": 3.8327364921569824,
        "learning_rate": 0.00018086469012512088,
        "epoch": 0.8026666666666666,
        "step": 6020
    },
    {
        "loss": 3.1987,
        "grad_norm": 4.256779193878174,
        "learning_rate": 0.00018082764878196773,
        "epoch": 0.8028,
        "step": 6021
    },
    {
        "loss": 2.6968,
        "grad_norm": 2.8188283443450928,
        "learning_rate": 0.0001807905754227075,
        "epoch": 0.8029333333333334,
        "step": 6022
    },
    {
        "loss": 2.3846,
        "grad_norm": 3.312150716781616,
        "learning_rate": 0.00018075347006202505,
        "epoch": 0.8030666666666667,
        "step": 6023
    },
    {
        "loss": 2.9175,
        "grad_norm": 3.4958009719848633,
        "learning_rate": 0.00018071633271461796,
        "epoch": 0.8032,
        "step": 6024
    },
    {
        "loss": 2.0466,
        "grad_norm": 3.069420099258423,
        "learning_rate": 0.00018067916339519643,
        "epoch": 0.8033333333333333,
        "step": 6025
    },
    {
        "loss": 2.4249,
        "grad_norm": 3.666566848754883,
        "learning_rate": 0.00018064196211848335,
        "epoch": 0.8034666666666667,
        "step": 6026
    },
    {
        "loss": 1.4896,
        "grad_norm": 3.828016996383667,
        "learning_rate": 0.00018060472889921435,
        "epoch": 0.8036,
        "step": 6027
    },
    {
        "loss": 1.6176,
        "grad_norm": 3.866624593734741,
        "learning_rate": 0.00018056746375213752,
        "epoch": 0.8037333333333333,
        "step": 6028
    },
    {
        "loss": 0.8202,
        "grad_norm": 3.091175079345703,
        "learning_rate": 0.0001805301666920138,
        "epoch": 0.8038666666666666,
        "step": 6029
    },
    {
        "loss": 3.115,
        "grad_norm": 3.994534969329834,
        "learning_rate": 0.00018049283773361667,
        "epoch": 0.804,
        "step": 6030
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.9386613368988037,
        "learning_rate": 0.00018045547689173218,
        "epoch": 0.8041333333333334,
        "step": 6031
    },
    {
        "loss": 2.4031,
        "grad_norm": 5.148988246917725,
        "learning_rate": 0.00018041808418115923,
        "epoch": 0.8042666666666667,
        "step": 6032
    },
    {
        "loss": 2.0518,
        "grad_norm": 4.211258411407471,
        "learning_rate": 0.00018038065961670908,
        "epoch": 0.8044,
        "step": 6033
    },
    {
        "loss": 2.2987,
        "grad_norm": 2.915757179260254,
        "learning_rate": 0.0001803432032132058,
        "epoch": 0.8045333333333333,
        "step": 6034
    },
    {
        "loss": 2.282,
        "grad_norm": 3.419966220855713,
        "learning_rate": 0.00018030571498548584,
        "epoch": 0.8046666666666666,
        "step": 6035
    },
    {
        "loss": 1.9281,
        "grad_norm": 3.167790412902832,
        "learning_rate": 0.00018026819494839865,
        "epoch": 0.8048,
        "step": 6036
    },
    {
        "loss": 1.4836,
        "grad_norm": 5.239556789398193,
        "learning_rate": 0.0001802306431168059,
        "epoch": 0.8049333333333333,
        "step": 6037
    },
    {
        "loss": 2.6367,
        "grad_norm": 3.212280511856079,
        "learning_rate": 0.00018019305950558204,
        "epoch": 0.8050666666666667,
        "step": 6038
    },
    {
        "loss": 1.3976,
        "grad_norm": 3.353177309036255,
        "learning_rate": 0.0001801554441296141,
        "epoch": 0.8052,
        "step": 6039
    },
    {
        "loss": 2.1396,
        "grad_norm": 3.110149621963501,
        "learning_rate": 0.00018011779700380154,
        "epoch": 0.8053333333333333,
        "step": 6040
    },
    {
        "loss": 1.7066,
        "grad_norm": 2.6565380096435547,
        "learning_rate": 0.00018008011814305664,
        "epoch": 0.8054666666666667,
        "step": 6041
    },
    {
        "loss": 2.1589,
        "grad_norm": 3.9159107208251953,
        "learning_rate": 0.00018004240756230408,
        "epoch": 0.8056,
        "step": 6042
    },
    {
        "loss": 1.3826,
        "grad_norm": 2.7611260414123535,
        "learning_rate": 0.00018000466527648117,
        "epoch": 0.8057333333333333,
        "step": 6043
    },
    {
        "loss": 2.1697,
        "grad_norm": 3.9907708168029785,
        "learning_rate": 0.00017996689130053763,
        "epoch": 0.8058666666666666,
        "step": 6044
    },
    {
        "loss": 2.7545,
        "grad_norm": 2.7079577445983887,
        "learning_rate": 0.00017992908564943603,
        "epoch": 0.806,
        "step": 6045
    },
    {
        "loss": 2.7735,
        "grad_norm": 2.8084070682525635,
        "learning_rate": 0.0001798912483381513,
        "epoch": 0.8061333333333334,
        "step": 6046
    },
    {
        "loss": 2.4685,
        "grad_norm": 6.436400890350342,
        "learning_rate": 0.00017985337938167083,
        "epoch": 0.8062666666666667,
        "step": 6047
    },
    {
        "loss": 1.581,
        "grad_norm": 2.7428598403930664,
        "learning_rate": 0.0001798154787949947,
        "epoch": 0.8064,
        "step": 6048
    },
    {
        "loss": 2.2305,
        "grad_norm": 3.527106285095215,
        "learning_rate": 0.00017977754659313547,
        "epoch": 0.8065333333333333,
        "step": 6049
    },
    {
        "loss": 2.5665,
        "grad_norm": 3.1522350311279297,
        "learning_rate": 0.00017973958279111817,
        "epoch": 0.8066666666666666,
        "step": 6050
    },
    {
        "loss": 2.2714,
        "grad_norm": 3.6657660007476807,
        "learning_rate": 0.00017970158740398043,
        "epoch": 0.8068,
        "step": 6051
    },
    {
        "loss": 2.3572,
        "grad_norm": 2.668463706970215,
        "learning_rate": 0.00017966356044677237,
        "epoch": 0.8069333333333333,
        "step": 6052
    },
    {
        "loss": 2.7612,
        "grad_norm": 4.600321292877197,
        "learning_rate": 0.00017962550193455652,
        "epoch": 0.8070666666666667,
        "step": 6053
    },
    {
        "loss": 2.6357,
        "grad_norm": 2.825446844100952,
        "learning_rate": 0.00017958741188240805,
        "epoch": 0.8072,
        "step": 6054
    },
    {
        "loss": 2.5364,
        "grad_norm": 4.27224063873291,
        "learning_rate": 0.0001795492903054146,
        "epoch": 0.8073333333333333,
        "step": 6055
    },
    {
        "loss": 2.1053,
        "grad_norm": 4.113404750823975,
        "learning_rate": 0.00017951113721867613,
        "epoch": 0.8074666666666667,
        "step": 6056
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.612502336502075,
        "learning_rate": 0.0001794729526373053,
        "epoch": 0.8076,
        "step": 6057
    },
    {
        "loss": 2.6084,
        "grad_norm": 2.450324535369873,
        "learning_rate": 0.00017943473657642717,
        "epoch": 0.8077333333333333,
        "step": 6058
    },
    {
        "loss": 2.4679,
        "grad_norm": 3.3247668743133545,
        "learning_rate": 0.00017939648905117918,
        "epoch": 0.8078666666666666,
        "step": 6059
    },
    {
        "loss": 1.9419,
        "grad_norm": 3.2337560653686523,
        "learning_rate": 0.00017935821007671133,
        "epoch": 0.808,
        "step": 6060
    },
    {
        "loss": 2.5202,
        "grad_norm": 3.613126754760742,
        "learning_rate": 0.0001793198996681861,
        "epoch": 0.8081333333333334,
        "step": 6061
    },
    {
        "loss": 2.0944,
        "grad_norm": 3.8508148193359375,
        "learning_rate": 0.0001792815578407783,
        "epoch": 0.8082666666666667,
        "step": 6062
    },
    {
        "loss": 2.3677,
        "grad_norm": 2.4254889488220215,
        "learning_rate": 0.00017924318460967533,
        "epoch": 0.8084,
        "step": 6063
    },
    {
        "loss": 0.8288,
        "grad_norm": 3.4336183071136475,
        "learning_rate": 0.00017920477999007692,
        "epoch": 0.8085333333333333,
        "step": 6064
    },
    {
        "loss": 1.5454,
        "grad_norm": 3.677081823348999,
        "learning_rate": 0.00017916634399719526,
        "epoch": 0.8086666666666666,
        "step": 6065
    },
    {
        "loss": 2.2961,
        "grad_norm": 2.344757080078125,
        "learning_rate": 0.000179127876646255,
        "epoch": 0.8088,
        "step": 6066
    },
    {
        "loss": 3.3379,
        "grad_norm": 4.43198823928833,
        "learning_rate": 0.00017908937795249315,
        "epoch": 0.8089333333333333,
        "step": 6067
    },
    {
        "loss": 2.7961,
        "grad_norm": 3.4410459995269775,
        "learning_rate": 0.00017905084793115922,
        "epoch": 0.8090666666666667,
        "step": 6068
    },
    {
        "loss": 2.4844,
        "grad_norm": 4.134475231170654,
        "learning_rate": 0.00017901228659751502,
        "epoch": 0.8092,
        "step": 6069
    },
    {
        "loss": 2.1233,
        "grad_norm": 3.3363537788391113,
        "learning_rate": 0.00017897369396683495,
        "epoch": 0.8093333333333333,
        "step": 6070
    },
    {
        "loss": 3.0025,
        "grad_norm": 5.452554702758789,
        "learning_rate": 0.00017893507005440555,
        "epoch": 0.8094666666666667,
        "step": 6071
    },
    {
        "loss": 1.9938,
        "grad_norm": 2.2137937545776367,
        "learning_rate": 0.00017889641487552597,
        "epoch": 0.8096,
        "step": 6072
    },
    {
        "loss": 1.7532,
        "grad_norm": 3.258737087249756,
        "learning_rate": 0.00017885772844550762,
        "epoch": 0.8097333333333333,
        "step": 6073
    },
    {
        "loss": 1.698,
        "grad_norm": 4.5741472244262695,
        "learning_rate": 0.00017881901077967434,
        "epoch": 0.8098666666666666,
        "step": 6074
    },
    {
        "loss": 2.8833,
        "grad_norm": 2.101771593093872,
        "learning_rate": 0.00017878026189336227,
        "epoch": 0.81,
        "step": 6075
    },
    {
        "loss": 2.215,
        "grad_norm": 2.313553810119629,
        "learning_rate": 0.00017874148180192006,
        "epoch": 0.8101333333333334,
        "step": 6076
    },
    {
        "loss": 2.9963,
        "grad_norm": 6.443095684051514,
        "learning_rate": 0.0001787026705207086,
        "epoch": 0.8102666666666667,
        "step": 6077
    },
    {
        "loss": 2.4592,
        "grad_norm": 2.410998582839966,
        "learning_rate": 0.00017866382806510113,
        "epoch": 0.8104,
        "step": 6078
    },
    {
        "loss": 1.9642,
        "grad_norm": 2.9594719409942627,
        "learning_rate": 0.00017862495445048339,
        "epoch": 0.8105333333333333,
        "step": 6079
    },
    {
        "loss": 2.097,
        "grad_norm": 4.5390825271606445,
        "learning_rate": 0.00017858604969225324,
        "epoch": 0.8106666666666666,
        "step": 6080
    },
    {
        "loss": 2.9368,
        "grad_norm": 2.708970785140991,
        "learning_rate": 0.0001785471138058211,
        "epoch": 0.8108,
        "step": 6081
    },
    {
        "loss": 2.5385,
        "grad_norm": 4.016210556030273,
        "learning_rate": 0.00017850814680660948,
        "epoch": 0.8109333333333333,
        "step": 6082
    },
    {
        "loss": 2.0793,
        "grad_norm": 4.8033552169799805,
        "learning_rate": 0.00017846914871005341,
        "epoch": 0.8110666666666667,
        "step": 6083
    },
    {
        "loss": 2.6789,
        "grad_norm": 3.852315664291382,
        "learning_rate": 0.00017843011953160018,
        "epoch": 0.8112,
        "step": 6084
    },
    {
        "loss": 3.0485,
        "grad_norm": 3.6505401134490967,
        "learning_rate": 0.00017839105928670932,
        "epoch": 0.8113333333333334,
        "step": 6085
    },
    {
        "loss": 1.6031,
        "grad_norm": 4.1802263259887695,
        "learning_rate": 0.0001783519679908528,
        "epoch": 0.8114666666666667,
        "step": 6086
    },
    {
        "loss": 2.2115,
        "grad_norm": 4.725165843963623,
        "learning_rate": 0.00017831284565951477,
        "epoch": 0.8116,
        "step": 6087
    },
    {
        "loss": 1.5297,
        "grad_norm": 3.383681297302246,
        "learning_rate": 0.0001782736923081917,
        "epoch": 0.8117333333333333,
        "step": 6088
    },
    {
        "loss": 2.3781,
        "grad_norm": 2.999882698059082,
        "learning_rate": 0.00017823450795239246,
        "epoch": 0.8118666666666666,
        "step": 6089
    },
    {
        "loss": 2.2961,
        "grad_norm": 4.05558967590332,
        "learning_rate": 0.000178195292607638,
        "epoch": 0.812,
        "step": 6090
    },
    {
        "loss": 1.2406,
        "grad_norm": 3.9362199306488037,
        "learning_rate": 0.00017815604628946172,
        "epoch": 0.8121333333333334,
        "step": 6091
    },
    {
        "loss": 1.5307,
        "grad_norm": 4.958061218261719,
        "learning_rate": 0.00017811676901340916,
        "epoch": 0.8122666666666667,
        "step": 6092
    },
    {
        "loss": 1.5652,
        "grad_norm": 3.429100275039673,
        "learning_rate": 0.00017807746079503823,
        "epoch": 0.8124,
        "step": 6093
    },
    {
        "loss": 2.5605,
        "grad_norm": 2.3215270042419434,
        "learning_rate": 0.000178038121649919,
        "epoch": 0.8125333333333333,
        "step": 6094
    },
    {
        "loss": 0.6018,
        "grad_norm": 2.704690933227539,
        "learning_rate": 0.00017799875159363394,
        "epoch": 0.8126666666666666,
        "step": 6095
    },
    {
        "loss": 2.5505,
        "grad_norm": 2.99774169921875,
        "learning_rate": 0.00017795935064177754,
        "epoch": 0.8128,
        "step": 6096
    },
    {
        "loss": 3.1591,
        "grad_norm": 2.656409502029419,
        "learning_rate": 0.00017791991880995676,
        "epoch": 0.8129333333333333,
        "step": 6097
    },
    {
        "loss": 1.0726,
        "grad_norm": 3.2130043506622314,
        "learning_rate": 0.00017788045611379061,
        "epoch": 0.8130666666666667,
        "step": 6098
    },
    {
        "loss": 1.9333,
        "grad_norm": 2.7510921955108643,
        "learning_rate": 0.00017784096256891045,
        "epoch": 0.8132,
        "step": 6099
    },
    {
        "loss": 2.5746,
        "grad_norm": 3.5501654148101807,
        "learning_rate": 0.00017780143819095978,
        "epoch": 0.8133333333333334,
        "step": 6100
    },
    {
        "loss": 1.4617,
        "grad_norm": 4.05613899230957,
        "learning_rate": 0.0001777618829955943,
        "epoch": 0.8134666666666667,
        "step": 6101
    },
    {
        "loss": 1.4206,
        "grad_norm": 3.400388479232788,
        "learning_rate": 0.00017772229699848207,
        "epoch": 0.8136,
        "step": 6102
    },
    {
        "loss": 2.355,
        "grad_norm": 4.095175743103027,
        "learning_rate": 0.00017768268021530303,
        "epoch": 0.8137333333333333,
        "step": 6103
    },
    {
        "loss": 2.7144,
        "grad_norm": 3.408473253250122,
        "learning_rate": 0.0001776430326617498,
        "epoch": 0.8138666666666666,
        "step": 6104
    },
    {
        "loss": 2.8236,
        "grad_norm": 3.6084649562835693,
        "learning_rate": 0.0001776033543535267,
        "epoch": 0.814,
        "step": 6105
    },
    {
        "loss": 1.7546,
        "grad_norm": 3.7637996673583984,
        "learning_rate": 0.00017756364530635056,
        "epoch": 0.8141333333333334,
        "step": 6106
    },
    {
        "loss": 2.7672,
        "grad_norm": 3.2061619758605957,
        "learning_rate": 0.00017752390553595014,
        "epoch": 0.8142666666666667,
        "step": 6107
    },
    {
        "loss": 2.2689,
        "grad_norm": 4.004517078399658,
        "learning_rate": 0.00017748413505806667,
        "epoch": 0.8144,
        "step": 6108
    },
    {
        "loss": 2.0384,
        "grad_norm": 4.316385746002197,
        "learning_rate": 0.00017744433388845323,
        "epoch": 0.8145333333333333,
        "step": 6109
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.850128173828125,
        "learning_rate": 0.00017740450204287527,
        "epoch": 0.8146666666666667,
        "step": 6110
    },
    {
        "loss": 2.9122,
        "grad_norm": 2.9363627433776855,
        "learning_rate": 0.00017736463953711034,
        "epoch": 0.8148,
        "step": 6111
    },
    {
        "loss": 2.8852,
        "grad_norm": 3.663996934890747,
        "learning_rate": 0.00017732474638694801,
        "epoch": 0.8149333333333333,
        "step": 6112
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.343475818634033,
        "learning_rate": 0.00017728482260819016,
        "epoch": 0.8150666666666667,
        "step": 6113
    },
    {
        "loss": 2.691,
        "grad_norm": 3.061613082885742,
        "learning_rate": 0.0001772448682166508,
        "epoch": 0.8152,
        "step": 6114
    },
    {
        "loss": 1.9331,
        "grad_norm": 3.813403367996216,
        "learning_rate": 0.00017720488322815587,
        "epoch": 0.8153333333333334,
        "step": 6115
    },
    {
        "loss": 2.0544,
        "grad_norm": 4.195065021514893,
        "learning_rate": 0.00017716486765854362,
        "epoch": 0.8154666666666667,
        "step": 6116
    },
    {
        "loss": 1.658,
        "grad_norm": 6.50111198425293,
        "learning_rate": 0.0001771248215236644,
        "epoch": 0.8156,
        "step": 6117
    },
    {
        "loss": 2.7068,
        "grad_norm": 2.234867811203003,
        "learning_rate": 0.00017708474483938056,
        "epoch": 0.8157333333333333,
        "step": 6118
    },
    {
        "loss": 1.763,
        "grad_norm": 4.747062683105469,
        "learning_rate": 0.0001770446376215666,
        "epoch": 0.8158666666666666,
        "step": 6119
    },
    {
        "loss": 2.5529,
        "grad_norm": 3.2995243072509766,
        "learning_rate": 0.0001770044998861092,
        "epoch": 0.816,
        "step": 6120
    },
    {
        "loss": 2.7369,
        "grad_norm": 2.9715447425842285,
        "learning_rate": 0.000176964331648907,
        "epoch": 0.8161333333333334,
        "step": 6121
    },
    {
        "loss": 2.0412,
        "grad_norm": 4.4253387451171875,
        "learning_rate": 0.00017692413292587077,
        "epoch": 0.8162666666666667,
        "step": 6122
    },
    {
        "loss": 2.1066,
        "grad_norm": 3.4495084285736084,
        "learning_rate": 0.0001768839037329234,
        "epoch": 0.8164,
        "step": 6123
    },
    {
        "loss": 1.8658,
        "grad_norm": 2.323056936264038,
        "learning_rate": 0.00017684364408599977,
        "epoch": 0.8165333333333333,
        "step": 6124
    },
    {
        "loss": 1.1864,
        "grad_norm": 4.458170413970947,
        "learning_rate": 0.00017680335400104688,
        "epoch": 0.8166666666666667,
        "step": 6125
    },
    {
        "loss": 1.3278,
        "grad_norm": 3.67375111579895,
        "learning_rate": 0.00017676303349402377,
        "epoch": 0.8168,
        "step": 6126
    },
    {
        "loss": 1.5294,
        "grad_norm": 2.6039133071899414,
        "learning_rate": 0.00017672268258090156,
        "epoch": 0.8169333333333333,
        "step": 6127
    },
    {
        "loss": 2.429,
        "grad_norm": 2.718071937561035,
        "learning_rate": 0.00017668230127766324,
        "epoch": 0.8170666666666667,
        "step": 6128
    },
    {
        "loss": 2.9703,
        "grad_norm": 2.9310755729675293,
        "learning_rate": 0.0001766418896003042,
        "epoch": 0.8172,
        "step": 6129
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.550569534301758,
        "learning_rate": 0.00017660144756483152,
        "epoch": 0.8173333333333334,
        "step": 6130
    },
    {
        "loss": 2.8866,
        "grad_norm": 2.667327642440796,
        "learning_rate": 0.00017656097518726447,
        "epoch": 0.8174666666666667,
        "step": 6131
    },
    {
        "loss": 0.9797,
        "grad_norm": 3.2323496341705322,
        "learning_rate": 0.00017652047248363425,
        "epoch": 0.8176,
        "step": 6132
    },
    {
        "loss": 1.8231,
        "grad_norm": 3.488328456878662,
        "learning_rate": 0.00017647993946998412,
        "epoch": 0.8177333333333333,
        "step": 6133
    },
    {
        "loss": 2.5483,
        "grad_norm": 3.0231456756591797,
        "learning_rate": 0.0001764393761623694,
        "epoch": 0.8178666666666666,
        "step": 6134
    },
    {
        "loss": 2.2696,
        "grad_norm": 2.930293321609497,
        "learning_rate": 0.0001763987825768573,
        "epoch": 0.818,
        "step": 6135
    },
    {
        "loss": 2.7174,
        "grad_norm": 3.241007089614868,
        "learning_rate": 0.0001763581587295271,
        "epoch": 0.8181333333333334,
        "step": 6136
    },
    {
        "loss": 2.0664,
        "grad_norm": 5.323823928833008,
        "learning_rate": 0.00017631750463647,
        "epoch": 0.8182666666666667,
        "step": 6137
    },
    {
        "loss": 2.2843,
        "grad_norm": 2.9201107025146484,
        "learning_rate": 0.00017627682031378934,
        "epoch": 0.8184,
        "step": 6138
    },
    {
        "loss": 1.4362,
        "grad_norm": 3.985982656478882,
        "learning_rate": 0.00017623610577760022,
        "epoch": 0.8185333333333333,
        "step": 6139
    },
    {
        "loss": 2.7368,
        "grad_norm": 2.6418678760528564,
        "learning_rate": 0.00017619536104402985,
        "epoch": 0.8186666666666667,
        "step": 6140
    },
    {
        "loss": 2.8564,
        "grad_norm": 2.45914888381958,
        "learning_rate": 0.00017615458612921733,
        "epoch": 0.8188,
        "step": 6141
    },
    {
        "loss": 2.8199,
        "grad_norm": 3.403104066848755,
        "learning_rate": 0.00017611378104931375,
        "epoch": 0.8189333333333333,
        "step": 6142
    },
    {
        "loss": 3.0438,
        "grad_norm": 2.6580708026885986,
        "learning_rate": 0.00017607294582048212,
        "epoch": 0.8190666666666667,
        "step": 6143
    },
    {
        "loss": 2.7184,
        "grad_norm": 3.016204357147217,
        "learning_rate": 0.00017603208045889748,
        "epoch": 0.8192,
        "step": 6144
    },
    {
        "loss": 2.2851,
        "grad_norm": 2.932879686355591,
        "learning_rate": 0.0001759911849807467,
        "epoch": 0.8193333333333334,
        "step": 6145
    },
    {
        "loss": 3.3117,
        "grad_norm": 5.371586322784424,
        "learning_rate": 0.0001759502594022286,
        "epoch": 0.8194666666666667,
        "step": 6146
    },
    {
        "loss": 2.365,
        "grad_norm": 2.8631949424743652,
        "learning_rate": 0.00017590930373955393,
        "epoch": 0.8196,
        "step": 6147
    },
    {
        "loss": 1.9508,
        "grad_norm": 5.082234859466553,
        "learning_rate": 0.00017586831800894543,
        "epoch": 0.8197333333333333,
        "step": 6148
    },
    {
        "loss": 1.9502,
        "grad_norm": 4.206530570983887,
        "learning_rate": 0.0001758273022266376,
        "epoch": 0.8198666666666666,
        "step": 6149
    },
    {
        "loss": 2.7332,
        "grad_norm": 4.437075138092041,
        "learning_rate": 0.00017578625640887708,
        "epoch": 0.82,
        "step": 6150
    },
    {
        "loss": 2.469,
        "grad_norm": 3.038564443588257,
        "learning_rate": 0.0001757451805719221,
        "epoch": 0.8201333333333334,
        "step": 6151
    },
    {
        "loss": 2.6625,
        "grad_norm": 2.495492696762085,
        "learning_rate": 0.000175704074732043,
        "epoch": 0.8202666666666667,
        "step": 6152
    },
    {
        "loss": 1.6951,
        "grad_norm": 4.863834381103516,
        "learning_rate": 0.0001756629389055219,
        "epoch": 0.8204,
        "step": 6153
    },
    {
        "loss": 2.5927,
        "grad_norm": 4.18949031829834,
        "learning_rate": 0.00017562177310865296,
        "epoch": 0.8205333333333333,
        "step": 6154
    },
    {
        "loss": 1.6103,
        "grad_norm": 5.603130340576172,
        "learning_rate": 0.00017558057735774196,
        "epoch": 0.8206666666666667,
        "step": 6155
    },
    {
        "loss": 3.1931,
        "grad_norm": 4.988536357879639,
        "learning_rate": 0.00017553935166910679,
        "epoch": 0.8208,
        "step": 6156
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.8685874938964844,
        "learning_rate": 0.00017549809605907699,
        "epoch": 0.8209333333333333,
        "step": 6157
    },
    {
        "loss": 2.4446,
        "grad_norm": 4.455477237701416,
        "learning_rate": 0.0001754568105439941,
        "epoch": 0.8210666666666666,
        "step": 6158
    },
    {
        "loss": 1.6705,
        "grad_norm": 3.4574975967407227,
        "learning_rate": 0.00017541549514021144,
        "epoch": 0.8212,
        "step": 6159
    },
    {
        "loss": 2.6347,
        "grad_norm": 2.452779531478882,
        "learning_rate": 0.00017537414986409418,
        "epoch": 0.8213333333333334,
        "step": 6160
    },
    {
        "loss": 2.8577,
        "grad_norm": 4.277885913848877,
        "learning_rate": 0.00017533277473201934,
        "epoch": 0.8214666666666667,
        "step": 6161
    },
    {
        "loss": 2.6618,
        "grad_norm": 3.41291880607605,
        "learning_rate": 0.0001752913697603757,
        "epoch": 0.8216,
        "step": 6162
    },
    {
        "loss": 2.223,
        "grad_norm": 3.408531427383423,
        "learning_rate": 0.000175249934965564,
        "epoch": 0.8217333333333333,
        "step": 6163
    },
    {
        "loss": 2.5292,
        "grad_norm": 3.3003814220428467,
        "learning_rate": 0.00017520847036399658,
        "epoch": 0.8218666666666666,
        "step": 6164
    },
    {
        "loss": 2.6511,
        "grad_norm": 3.746271848678589,
        "learning_rate": 0.00017516697597209788,
        "epoch": 0.822,
        "step": 6165
    },
    {
        "loss": 1.7526,
        "grad_norm": 5.813709259033203,
        "learning_rate": 0.0001751254518063038,
        "epoch": 0.8221333333333334,
        "step": 6166
    },
    {
        "loss": 2.0174,
        "grad_norm": 3.4770150184631348,
        "learning_rate": 0.0001750838978830623,
        "epoch": 0.8222666666666667,
        "step": 6167
    },
    {
        "loss": 1.9927,
        "grad_norm": 3.432605504989624,
        "learning_rate": 0.000175042314218833,
        "epoch": 0.8224,
        "step": 6168
    },
    {
        "loss": 2.9614,
        "grad_norm": 2.6633689403533936,
        "learning_rate": 0.00017500070083008735,
        "epoch": 0.8225333333333333,
        "step": 6169
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.57033634185791,
        "learning_rate": 0.0001749590577333086,
        "epoch": 0.8226666666666667,
        "step": 6170
    },
    {
        "loss": 1.7987,
        "grad_norm": 4.895143032073975,
        "learning_rate": 0.00017491738494499157,
        "epoch": 0.8228,
        "step": 6171
    },
    {
        "loss": 1.8107,
        "grad_norm": 3.487257719039917,
        "learning_rate": 0.0001748756824816431,
        "epoch": 0.8229333333333333,
        "step": 6172
    },
    {
        "loss": 2.6168,
        "grad_norm": 2.480166435241699,
        "learning_rate": 0.0001748339503597817,
        "epoch": 0.8230666666666666,
        "step": 6173
    },
    {
        "loss": 1.9844,
        "grad_norm": 3.7682511806488037,
        "learning_rate": 0.00017479218859593756,
        "epoch": 0.8232,
        "step": 6174
    },
    {
        "loss": 1.805,
        "grad_norm": 2.6200101375579834,
        "learning_rate": 0.0001747503972066527,
        "epoch": 0.8233333333333334,
        "step": 6175
    },
    {
        "loss": 2.3706,
        "grad_norm": 2.9835920333862305,
        "learning_rate": 0.00017470857620848073,
        "epoch": 0.8234666666666667,
        "step": 6176
    },
    {
        "loss": 2.476,
        "grad_norm": 2.6907730102539062,
        "learning_rate": 0.00017466672561798722,
        "epoch": 0.8236,
        "step": 6177
    },
    {
        "loss": 2.065,
        "grad_norm": 3.483072519302368,
        "learning_rate": 0.0001746248454517492,
        "epoch": 0.8237333333333333,
        "step": 6178
    },
    {
        "loss": 2.1088,
        "grad_norm": 3.6584815979003906,
        "learning_rate": 0.00017458293572635572,
        "epoch": 0.8238666666666666,
        "step": 6179
    },
    {
        "loss": 2.0967,
        "grad_norm": 4.536016464233398,
        "learning_rate": 0.00017454099645840718,
        "epoch": 0.824,
        "step": 6180
    },
    {
        "loss": 2.3022,
        "grad_norm": 3.5058822631835938,
        "learning_rate": 0.00017449902766451597,
        "epoch": 0.8241333333333334,
        "step": 6181
    },
    {
        "loss": 1.3715,
        "grad_norm": 4.88485860824585,
        "learning_rate": 0.0001744570293613061,
        "epoch": 0.8242666666666667,
        "step": 6182
    },
    {
        "loss": 2.6282,
        "grad_norm": 2.1957004070281982,
        "learning_rate": 0.00017441500156541314,
        "epoch": 0.8244,
        "step": 6183
    },
    {
        "loss": 2.191,
        "grad_norm": 3.8406639099121094,
        "learning_rate": 0.00017437294429348453,
        "epoch": 0.8245333333333333,
        "step": 6184
    },
    {
        "loss": 0.7363,
        "grad_norm": 3.2164628505706787,
        "learning_rate": 0.00017433085756217925,
        "epoch": 0.8246666666666667,
        "step": 6185
    },
    {
        "loss": 2.4238,
        "grad_norm": 2.9023916721343994,
        "learning_rate": 0.00017428874138816807,
        "epoch": 0.8248,
        "step": 6186
    },
    {
        "loss": 2.3478,
        "grad_norm": 2.0951061248779297,
        "learning_rate": 0.00017424659578813316,
        "epoch": 0.8249333333333333,
        "step": 6187
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.559952974319458,
        "learning_rate": 0.00017420442077876878,
        "epoch": 0.8250666666666666,
        "step": 6188
    },
    {
        "loss": 2.1629,
        "grad_norm": 3.4593377113342285,
        "learning_rate": 0.00017416221637678043,
        "epoch": 0.8252,
        "step": 6189
    },
    {
        "loss": 2.3607,
        "grad_norm": 3.612787961959839,
        "learning_rate": 0.0001741199825988855,
        "epoch": 0.8253333333333334,
        "step": 6190
    },
    {
        "loss": 1.8333,
        "grad_norm": 4.6788530349731445,
        "learning_rate": 0.00017407771946181293,
        "epoch": 0.8254666666666667,
        "step": 6191
    },
    {
        "loss": 1.7642,
        "grad_norm": 3.716930389404297,
        "learning_rate": 0.00017403542698230323,
        "epoch": 0.8256,
        "step": 6192
    },
    {
        "loss": 2.1493,
        "grad_norm": 4.302706241607666,
        "learning_rate": 0.00017399310517710864,
        "epoch": 0.8257333333333333,
        "step": 6193
    },
    {
        "loss": 2.599,
        "grad_norm": 3.82389760017395,
        "learning_rate": 0.00017395075406299296,
        "epoch": 0.8258666666666666,
        "step": 6194
    },
    {
        "loss": 2.306,
        "grad_norm": 3.825854778289795,
        "learning_rate": 0.00017390837365673166,
        "epoch": 0.826,
        "step": 6195
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.5178661346435547,
        "learning_rate": 0.00017386596397511164,
        "epoch": 0.8261333333333334,
        "step": 6196
    },
    {
        "loss": 2.4918,
        "grad_norm": 5.965393543243408,
        "learning_rate": 0.0001738235250349316,
        "epoch": 0.8262666666666667,
        "step": 6197
    },
    {
        "loss": 2.5041,
        "grad_norm": 2.5774285793304443,
        "learning_rate": 0.0001737810568530018,
        "epoch": 0.8264,
        "step": 6198
    },
    {
        "loss": 2.1795,
        "grad_norm": 4.198765754699707,
        "learning_rate": 0.00017373855944614397,
        "epoch": 0.8265333333333333,
        "step": 6199
    },
    {
        "loss": 2.0276,
        "grad_norm": 4.212784290313721,
        "learning_rate": 0.00017369603283119145,
        "epoch": 0.8266666666666667,
        "step": 6200
    },
    {
        "loss": 1.9095,
        "grad_norm": 2.882448673248291,
        "learning_rate": 0.00017365347702498926,
        "epoch": 0.8268,
        "step": 6201
    },
    {
        "loss": 2.6864,
        "grad_norm": 2.375659942626953,
        "learning_rate": 0.00017361089204439382,
        "epoch": 0.8269333333333333,
        "step": 6202
    },
    {
        "loss": 2.0489,
        "grad_norm": 3.1098766326904297,
        "learning_rate": 0.00017356827790627322,
        "epoch": 0.8270666666666666,
        "step": 6203
    },
    {
        "loss": 2.1069,
        "grad_norm": 3.746817111968994,
        "learning_rate": 0.0001735256346275071,
        "epoch": 0.8272,
        "step": 6204
    },
    {
        "loss": 2.4366,
        "grad_norm": 3.0642642974853516,
        "learning_rate": 0.00017348296222498651,
        "epoch": 0.8273333333333334,
        "step": 6205
    },
    {
        "loss": 1.791,
        "grad_norm": 2.510329246520996,
        "learning_rate": 0.0001734402607156142,
        "epoch": 0.8274666666666667,
        "step": 6206
    },
    {
        "loss": 2.6401,
        "grad_norm": 3.556321620941162,
        "learning_rate": 0.00017339753011630447,
        "epoch": 0.8276,
        "step": 6207
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.9931294918060303,
        "learning_rate": 0.00017335477044398288,
        "epoch": 0.8277333333333333,
        "step": 6208
    },
    {
        "loss": 1.279,
        "grad_norm": 4.426075458526611,
        "learning_rate": 0.00017331198171558683,
        "epoch": 0.8278666666666666,
        "step": 6209
    },
    {
        "loss": 2.26,
        "grad_norm": 3.7039451599121094,
        "learning_rate": 0.00017326916394806493,
        "epoch": 0.828,
        "step": 6210
    },
    {
        "loss": 1.4146,
        "grad_norm": 2.6277856826782227,
        "learning_rate": 0.00017322631715837762,
        "epoch": 0.8281333333333334,
        "step": 6211
    },
    {
        "loss": 1.8167,
        "grad_norm": 3.5086984634399414,
        "learning_rate": 0.00017318344136349646,
        "epoch": 0.8282666666666667,
        "step": 6212
    },
    {
        "loss": 1.737,
        "grad_norm": 3.308267116546631,
        "learning_rate": 0.00017314053658040493,
        "epoch": 0.8284,
        "step": 6213
    },
    {
        "loss": 2.0056,
        "grad_norm": 3.40976881980896,
        "learning_rate": 0.00017309760282609754,
        "epoch": 0.8285333333333333,
        "step": 6214
    },
    {
        "loss": 2.0565,
        "grad_norm": 3.6687700748443604,
        "learning_rate": 0.00017305464011758067,
        "epoch": 0.8286666666666667,
        "step": 6215
    },
    {
        "loss": 2.6003,
        "grad_norm": 3.7032084465026855,
        "learning_rate": 0.0001730116484718719,
        "epoch": 0.8288,
        "step": 6216
    },
    {
        "loss": 2.0225,
        "grad_norm": 3.135175943374634,
        "learning_rate": 0.00017296862790600038,
        "epoch": 0.8289333333333333,
        "step": 6217
    },
    {
        "loss": 2.7348,
        "grad_norm": 2.729940414428711,
        "learning_rate": 0.00017292557843700675,
        "epoch": 0.8290666666666666,
        "step": 6218
    },
    {
        "loss": 1.5647,
        "grad_norm": 3.2056996822357178,
        "learning_rate": 0.00017288250008194297,
        "epoch": 0.8292,
        "step": 6219
    },
    {
        "loss": 2.9204,
        "grad_norm": 4.033718585968018,
        "learning_rate": 0.0001728393928578726,
        "epoch": 0.8293333333333334,
        "step": 6220
    },
    {
        "loss": 2.2613,
        "grad_norm": 2.854494094848633,
        "learning_rate": 0.00017279625678187044,
        "epoch": 0.8294666666666667,
        "step": 6221
    },
    {
        "loss": 2.1588,
        "grad_norm": 3.741975784301758,
        "learning_rate": 0.000172753091871023,
        "epoch": 0.8296,
        "step": 6222
    },
    {
        "loss": 2.64,
        "grad_norm": 4.208341598510742,
        "learning_rate": 0.00017270989814242795,
        "epoch": 0.8297333333333333,
        "step": 6223
    },
    {
        "loss": 2.401,
        "grad_norm": 4.129392147064209,
        "learning_rate": 0.00017266667561319454,
        "epoch": 0.8298666666666666,
        "step": 6224
    },
    {
        "loss": 2.7352,
        "grad_norm": 3.798672914505005,
        "learning_rate": 0.00017262342430044325,
        "epoch": 0.83,
        "step": 6225
    },
    {
        "loss": 2.9122,
        "grad_norm": 3.783085346221924,
        "learning_rate": 0.0001725801442213062,
        "epoch": 0.8301333333333333,
        "step": 6226
    },
    {
        "loss": 2.4663,
        "grad_norm": 3.08282732963562,
        "learning_rate": 0.00017253683539292664,
        "epoch": 0.8302666666666667,
        "step": 6227
    },
    {
        "loss": 2.5042,
        "grad_norm": 3.5145702362060547,
        "learning_rate": 0.0001724934978324594,
        "epoch": 0.8304,
        "step": 6228
    },
    {
        "loss": 2.5864,
        "grad_norm": 3.3016345500946045,
        "learning_rate": 0.00017245013155707076,
        "epoch": 0.8305333333333333,
        "step": 6229
    },
    {
        "loss": 2.4172,
        "grad_norm": 2.7789905071258545,
        "learning_rate": 0.00017240673658393805,
        "epoch": 0.8306666666666667,
        "step": 6230
    },
    {
        "loss": 3.3087,
        "grad_norm": 2.6792993545532227,
        "learning_rate": 0.0001723633129302503,
        "epoch": 0.8308,
        "step": 6231
    },
    {
        "loss": 1.7559,
        "grad_norm": 2.4063360691070557,
        "learning_rate": 0.00017231986061320775,
        "epoch": 0.8309333333333333,
        "step": 6232
    },
    {
        "loss": 2.4307,
        "grad_norm": 3.643127679824829,
        "learning_rate": 0.0001722763796500219,
        "epoch": 0.8310666666666666,
        "step": 6233
    },
    {
        "loss": 3.0161,
        "grad_norm": 3.7392079830169678,
        "learning_rate": 0.0001722328700579159,
        "epoch": 0.8312,
        "step": 6234
    },
    {
        "loss": 3.1167,
        "grad_norm": 2.9881715774536133,
        "learning_rate": 0.00017218933185412388,
        "epoch": 0.8313333333333334,
        "step": 6235
    },
    {
        "loss": 1.6921,
        "grad_norm": 2.7528088092803955,
        "learning_rate": 0.00017214576505589156,
        "epoch": 0.8314666666666667,
        "step": 6236
    },
    {
        "loss": 2.6054,
        "grad_norm": 3.6026930809020996,
        "learning_rate": 0.00017210216968047585,
        "epoch": 0.8316,
        "step": 6237
    },
    {
        "loss": 1.6921,
        "grad_norm": 2.9764068126678467,
        "learning_rate": 0.00017205854574514506,
        "epoch": 0.8317333333333333,
        "step": 6238
    },
    {
        "loss": 2.0039,
        "grad_norm": 4.506057262420654,
        "learning_rate": 0.00017201489326717874,
        "epoch": 0.8318666666666666,
        "step": 6239
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.138266086578369,
        "learning_rate": 0.00017197121226386784,
        "epoch": 0.832,
        "step": 6240
    },
    {
        "loss": 2.3107,
        "grad_norm": 3.2415943145751953,
        "learning_rate": 0.00017192750275251456,
        "epoch": 0.8321333333333333,
        "step": 6241
    },
    {
        "loss": 0.8795,
        "grad_norm": 2.9701578617095947,
        "learning_rate": 0.00017188376475043226,
        "epoch": 0.8322666666666667,
        "step": 6242
    },
    {
        "loss": 2.4984,
        "grad_norm": 3.0636022090911865,
        "learning_rate": 0.0001718399982749459,
        "epoch": 0.8324,
        "step": 6243
    },
    {
        "loss": 2.2651,
        "grad_norm": 4.723931312561035,
        "learning_rate": 0.00017179620334339138,
        "epoch": 0.8325333333333333,
        "step": 6244
    },
    {
        "loss": 2.5535,
        "grad_norm": 3.6428213119506836,
        "learning_rate": 0.00017175237997311613,
        "epoch": 0.8326666666666667,
        "step": 6245
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.622232675552368,
        "learning_rate": 0.00017170852818147855,
        "epoch": 0.8328,
        "step": 6246
    },
    {
        "loss": 2.4516,
        "grad_norm": 3.448939561843872,
        "learning_rate": 0.00017166464798584876,
        "epoch": 0.8329333333333333,
        "step": 6247
    },
    {
        "loss": 1.051,
        "grad_norm": 4.573660850524902,
        "learning_rate": 0.00017162073940360764,
        "epoch": 0.8330666666666666,
        "step": 6248
    },
    {
        "loss": 2.9152,
        "grad_norm": 2.8132758140563965,
        "learning_rate": 0.00017157680245214764,
        "epoch": 0.8332,
        "step": 6249
    },
    {
        "loss": 2.0563,
        "grad_norm": 2.2818233966827393,
        "learning_rate": 0.00017153283714887226,
        "epoch": 0.8333333333333334,
        "step": 6250
    },
    {
        "loss": 2.4455,
        "grad_norm": 3.211782217025757,
        "learning_rate": 0.0001714888435111964,
        "epoch": 0.8334666666666667,
        "step": 6251
    },
    {
        "loss": 2.6946,
        "grad_norm": 2.1975789070129395,
        "learning_rate": 0.00017144482155654597,
        "epoch": 0.8336,
        "step": 6252
    },
    {
        "loss": 2.3664,
        "grad_norm": 2.37357497215271,
        "learning_rate": 0.0001714007713023583,
        "epoch": 0.8337333333333333,
        "step": 6253
    },
    {
        "loss": 3.1189,
        "grad_norm": 3.00254225730896,
        "learning_rate": 0.00017135669276608184,
        "epoch": 0.8338666666666666,
        "step": 6254
    },
    {
        "loss": 2.3483,
        "grad_norm": 2.98966908454895,
        "learning_rate": 0.00017131258596517622,
        "epoch": 0.834,
        "step": 6255
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.01888108253479,
        "learning_rate": 0.00017126845091711227,
        "epoch": 0.8341333333333333,
        "step": 6256
    },
    {
        "loss": 2.4287,
        "grad_norm": 2.9217231273651123,
        "learning_rate": 0.0001712242876393721,
        "epoch": 0.8342666666666667,
        "step": 6257
    },
    {
        "loss": 3.5168,
        "grad_norm": 2.608015537261963,
        "learning_rate": 0.00017118009614944894,
        "epoch": 0.8344,
        "step": 6258
    },
    {
        "loss": 2.7827,
        "grad_norm": 2.2588369846343994,
        "learning_rate": 0.0001711358764648471,
        "epoch": 0.8345333333333333,
        "step": 6259
    },
    {
        "loss": 2.9074,
        "grad_norm": 1.831385850906372,
        "learning_rate": 0.00017109162860308227,
        "epoch": 0.8346666666666667,
        "step": 6260
    },
    {
        "loss": 2.6508,
        "grad_norm": 3.273682117462158,
        "learning_rate": 0.00017104735258168105,
        "epoch": 0.8348,
        "step": 6261
    },
    {
        "loss": 2.3505,
        "grad_norm": 2.3193068504333496,
        "learning_rate": 0.00017100304841818139,
        "epoch": 0.8349333333333333,
        "step": 6262
    },
    {
        "loss": 2.6474,
        "grad_norm": 3.5061123371124268,
        "learning_rate": 0.00017095871613013238,
        "epoch": 0.8350666666666666,
        "step": 6263
    },
    {
        "loss": 2.2065,
        "grad_norm": 3.3018150329589844,
        "learning_rate": 0.00017091435573509406,
        "epoch": 0.8352,
        "step": 6264
    },
    {
        "loss": 2.6267,
        "grad_norm": 2.7706918716430664,
        "learning_rate": 0.0001708699672506378,
        "epoch": 0.8353333333333334,
        "step": 6265
    },
    {
        "loss": 1.663,
        "grad_norm": 3.6174533367156982,
        "learning_rate": 0.00017082555069434607,
        "epoch": 0.8354666666666667,
        "step": 6266
    },
    {
        "loss": 2.6924,
        "grad_norm": 2.790635108947754,
        "learning_rate": 0.00017078110608381234,
        "epoch": 0.8356,
        "step": 6267
    },
    {
        "loss": 1.4787,
        "grad_norm": 4.431135177612305,
        "learning_rate": 0.00017073663343664136,
        "epoch": 0.8357333333333333,
        "step": 6268
    },
    {
        "loss": 2.5679,
        "grad_norm": 2.5322048664093018,
        "learning_rate": 0.00017069213277044878,
        "epoch": 0.8358666666666666,
        "step": 6269
    },
    {
        "loss": 2.7632,
        "grad_norm": 3.371053457260132,
        "learning_rate": 0.0001706476041028616,
        "epoch": 0.836,
        "step": 6270
    },
    {
        "loss": 2.8075,
        "grad_norm": 2.1554114818573,
        "learning_rate": 0.0001706030474515176,
        "epoch": 0.8361333333333333,
        "step": 6271
    },
    {
        "loss": 2.4926,
        "grad_norm": 3.1745500564575195,
        "learning_rate": 0.000170558462834066,
        "epoch": 0.8362666666666667,
        "step": 6272
    },
    {
        "loss": 2.1963,
        "grad_norm": 2.0025007724761963,
        "learning_rate": 0.00017051385026816687,
        "epoch": 0.8364,
        "step": 6273
    },
    {
        "loss": 1.7515,
        "grad_norm": 5.5874247550964355,
        "learning_rate": 0.00017046920977149138,
        "epoch": 0.8365333333333334,
        "step": 6274
    },
    {
        "loss": 2.5248,
        "grad_norm": 4.000272274017334,
        "learning_rate": 0.00017042454136172172,
        "epoch": 0.8366666666666667,
        "step": 6275
    },
    {
        "loss": 1.0213,
        "grad_norm": 4.077207565307617,
        "learning_rate": 0.00017037984505655134,
        "epoch": 0.8368,
        "step": 6276
    },
    {
        "loss": 1.587,
        "grad_norm": 3.5623018741607666,
        "learning_rate": 0.0001703351208736845,
        "epoch": 0.8369333333333333,
        "step": 6277
    },
    {
        "loss": 2.5913,
        "grad_norm": 3.455518960952759,
        "learning_rate": 0.00017029036883083656,
        "epoch": 0.8370666666666666,
        "step": 6278
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.8088603019714355,
        "learning_rate": 0.00017024558894573408,
        "epoch": 0.8372,
        "step": 6279
    },
    {
        "loss": 2.2786,
        "grad_norm": 3.0925419330596924,
        "learning_rate": 0.0001702007812361144,
        "epoch": 0.8373333333333334,
        "step": 6280
    },
    {
        "loss": 2.6458,
        "grad_norm": 3.792884111404419,
        "learning_rate": 0.0001701559457197261,
        "epoch": 0.8374666666666667,
        "step": 6281
    },
    {
        "loss": 2.7527,
        "grad_norm": 2.6466124057769775,
        "learning_rate": 0.00017011108241432864,
        "epoch": 0.8376,
        "step": 6282
    },
    {
        "loss": 2.5124,
        "grad_norm": 3.964926242828369,
        "learning_rate": 0.00017006619133769262,
        "epoch": 0.8377333333333333,
        "step": 6283
    },
    {
        "loss": 2.1703,
        "grad_norm": 3.5464231967926025,
        "learning_rate": 0.00017002127250759935,
        "epoch": 0.8378666666666666,
        "step": 6284
    },
    {
        "loss": 2.5021,
        "grad_norm": 4.387383937835693,
        "learning_rate": 0.00016997632594184156,
        "epoch": 0.838,
        "step": 6285
    },
    {
        "loss": 1.8506,
        "grad_norm": 3.532383441925049,
        "learning_rate": 0.00016993135165822257,
        "epoch": 0.8381333333333333,
        "step": 6286
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.6250545978546143,
        "learning_rate": 0.00016988634967455687,
        "epoch": 0.8382666666666667,
        "step": 6287
    },
    {
        "loss": 3.2013,
        "grad_norm": 2.9256348609924316,
        "learning_rate": 0.00016984132000866998,
        "epoch": 0.8384,
        "step": 6288
    },
    {
        "loss": 2.9754,
        "grad_norm": 2.0751829147338867,
        "learning_rate": 0.00016979626267839824,
        "epoch": 0.8385333333333334,
        "step": 6289
    },
    {
        "loss": 0.9926,
        "grad_norm": 3.2088425159454346,
        "learning_rate": 0.00016975117770158898,
        "epoch": 0.8386666666666667,
        "step": 6290
    },
    {
        "loss": 2.0029,
        "grad_norm": 3.6332550048828125,
        "learning_rate": 0.00016970606509610065,
        "epoch": 0.8388,
        "step": 6291
    },
    {
        "loss": 2.4129,
        "grad_norm": 2.611262798309326,
        "learning_rate": 0.00016966092487980235,
        "epoch": 0.8389333333333333,
        "step": 6292
    },
    {
        "loss": 1.3657,
        "grad_norm": 3.2062201499938965,
        "learning_rate": 0.0001696157570705744,
        "epoch": 0.8390666666666666,
        "step": 6293
    },
    {
        "loss": 1.6424,
        "grad_norm": 3.5192458629608154,
        "learning_rate": 0.00016957056168630783,
        "epoch": 0.8392,
        "step": 6294
    },
    {
        "loss": 2.9005,
        "grad_norm": 3.139450788497925,
        "learning_rate": 0.00016952533874490478,
        "epoch": 0.8393333333333334,
        "step": 6295
    },
    {
        "loss": 2.765,
        "grad_norm": 2.4771599769592285,
        "learning_rate": 0.000169480088264278,
        "epoch": 0.8394666666666667,
        "step": 6296
    },
    {
        "loss": 2.7948,
        "grad_norm": 2.7459964752197266,
        "learning_rate": 0.00016943481026235164,
        "epoch": 0.8396,
        "step": 6297
    },
    {
        "loss": 2.6618,
        "grad_norm": 3.2862887382507324,
        "learning_rate": 0.0001693895047570603,
        "epoch": 0.8397333333333333,
        "step": 6298
    },
    {
        "loss": 1.4043,
        "grad_norm": 3.9697906970977783,
        "learning_rate": 0.00016934417176634966,
        "epoch": 0.8398666666666667,
        "step": 6299
    },
    {
        "loss": 2.3054,
        "grad_norm": 2.4959232807159424,
        "learning_rate": 0.00016929881130817637,
        "epoch": 0.84,
        "step": 6300
    },
    {
        "loss": 3.3803,
        "grad_norm": 3.963078737258911,
        "learning_rate": 0.00016925342340050773,
        "epoch": 0.8401333333333333,
        "step": 6301
    },
    {
        "loss": 2.0756,
        "grad_norm": 2.498894691467285,
        "learning_rate": 0.00016920800806132218,
        "epoch": 0.8402666666666667,
        "step": 6302
    },
    {
        "loss": 1.715,
        "grad_norm": 3.1272995471954346,
        "learning_rate": 0.00016916256530860873,
        "epoch": 0.8404,
        "step": 6303
    },
    {
        "loss": 2.1916,
        "grad_norm": 3.6780316829681396,
        "learning_rate": 0.00016911709516036756,
        "epoch": 0.8405333333333334,
        "step": 6304
    },
    {
        "loss": 2.472,
        "grad_norm": 5.10329008102417,
        "learning_rate": 0.00016907159763460938,
        "epoch": 0.8406666666666667,
        "step": 6305
    },
    {
        "loss": 1.263,
        "grad_norm": 4.140804290771484,
        "learning_rate": 0.00016902607274935614,
        "epoch": 0.8408,
        "step": 6306
    },
    {
        "loss": 3.3115,
        "grad_norm": 5.20788049697876,
        "learning_rate": 0.00016898052052264023,
        "epoch": 0.8409333333333333,
        "step": 6307
    },
    {
        "loss": 3.0087,
        "grad_norm": 2.9018747806549072,
        "learning_rate": 0.00016893494097250513,
        "epoch": 0.8410666666666666,
        "step": 6308
    },
    {
        "loss": 1.1155,
        "grad_norm": 8.79317855834961,
        "learning_rate": 0.00016888933411700498,
        "epoch": 0.8412,
        "step": 6309
    },
    {
        "loss": 2.675,
        "grad_norm": 3.4646596908569336,
        "learning_rate": 0.0001688436999742049,
        "epoch": 0.8413333333333334,
        "step": 6310
    },
    {
        "loss": 1.8808,
        "grad_norm": 3.836555242538452,
        "learning_rate": 0.00016879803856218066,
        "epoch": 0.8414666666666667,
        "step": 6311
    },
    {
        "loss": 2.6718,
        "grad_norm": 3.5706286430358887,
        "learning_rate": 0.0001687523498990189,
        "epoch": 0.8416,
        "step": 6312
    },
    {
        "loss": 2.0241,
        "grad_norm": 3.9063169956207275,
        "learning_rate": 0.0001687066340028171,
        "epoch": 0.8417333333333333,
        "step": 6313
    },
    {
        "loss": 2.3822,
        "grad_norm": 2.141761541366577,
        "learning_rate": 0.00016866089089168345,
        "epoch": 0.8418666666666667,
        "step": 6314
    },
    {
        "loss": 2.932,
        "grad_norm": 2.9418609142303467,
        "learning_rate": 0.00016861512058373692,
        "epoch": 0.842,
        "step": 6315
    },
    {
        "loss": 2.3719,
        "grad_norm": 4.208820819854736,
        "learning_rate": 0.0001685693230971074,
        "epoch": 0.8421333333333333,
        "step": 6316
    },
    {
        "loss": 2.2347,
        "grad_norm": 3.4475419521331787,
        "learning_rate": 0.0001685234984499353,
        "epoch": 0.8422666666666667,
        "step": 6317
    },
    {
        "loss": 1.5045,
        "grad_norm": 3.2887308597564697,
        "learning_rate": 0.000168477646660372,
        "epoch": 0.8424,
        "step": 6318
    },
    {
        "loss": 3.0634,
        "grad_norm": 2.099867105484009,
        "learning_rate": 0.00016843176774657955,
        "epoch": 0.8425333333333334,
        "step": 6319
    },
    {
        "loss": 2.8552,
        "grad_norm": 4.489395618438721,
        "learning_rate": 0.0001683858617267307,
        "epoch": 0.8426666666666667,
        "step": 6320
    },
    {
        "loss": 2.4301,
        "grad_norm": 2.8658394813537598,
        "learning_rate": 0.00016833992861900896,
        "epoch": 0.8428,
        "step": 6321
    },
    {
        "loss": 2.7704,
        "grad_norm": 2.4641225337982178,
        "learning_rate": 0.00016829396844160872,
        "epoch": 0.8429333333333333,
        "step": 6322
    },
    {
        "loss": 2.119,
        "grad_norm": 2.88071346282959,
        "learning_rate": 0.0001682479812127348,
        "epoch": 0.8430666666666666,
        "step": 6323
    },
    {
        "loss": 2.4883,
        "grad_norm": 3.961846351623535,
        "learning_rate": 0.00016820196695060299,
        "epoch": 0.8432,
        "step": 6324
    },
    {
        "loss": 1.6163,
        "grad_norm": 5.731438159942627,
        "learning_rate": 0.0001681559256734397,
        "epoch": 0.8433333333333334,
        "step": 6325
    },
    {
        "loss": 0.9323,
        "grad_norm": 4.302698135375977,
        "learning_rate": 0.00016810985739948198,
        "epoch": 0.8434666666666667,
        "step": 6326
    },
    {
        "loss": 2.8825,
        "grad_norm": 3.1104860305786133,
        "learning_rate": 0.0001680637621469777,
        "epoch": 0.8436,
        "step": 6327
    },
    {
        "loss": 2.1107,
        "grad_norm": 2.2718887329101562,
        "learning_rate": 0.0001680176399341853,
        "epoch": 0.8437333333333333,
        "step": 6328
    },
    {
        "loss": 1.4282,
        "grad_norm": 3.5100793838500977,
        "learning_rate": 0.00016797149077937398,
        "epoch": 0.8438666666666667,
        "step": 6329
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.8407070636749268,
        "learning_rate": 0.00016792531470082345,
        "epoch": 0.844,
        "step": 6330
    },
    {
        "loss": 2.3088,
        "grad_norm": 3.5153791904449463,
        "learning_rate": 0.00016787911171682444,
        "epoch": 0.8441333333333333,
        "step": 6331
    },
    {
        "loss": 1.8058,
        "grad_norm": 3.9818239212036133,
        "learning_rate": 0.00016783288184567794,
        "epoch": 0.8442666666666667,
        "step": 6332
    },
    {
        "loss": 3.0504,
        "grad_norm": 1.9888089895248413,
        "learning_rate": 0.00016778662510569584,
        "epoch": 0.8444,
        "step": 6333
    },
    {
        "loss": 2.3246,
        "grad_norm": 3.3137950897216797,
        "learning_rate": 0.0001677403415152005,
        "epoch": 0.8445333333333334,
        "step": 6334
    },
    {
        "loss": 1.855,
        "grad_norm": 3.803478240966797,
        "learning_rate": 0.00016769403109252512,
        "epoch": 0.8446666666666667,
        "step": 6335
    },
    {
        "loss": 2.257,
        "grad_norm": 2.4377219676971436,
        "learning_rate": 0.0001676476938560133,
        "epoch": 0.8448,
        "step": 6336
    },
    {
        "loss": 2.2107,
        "grad_norm": 3.993290424346924,
        "learning_rate": 0.00016760132982401947,
        "epoch": 0.8449333333333333,
        "step": 6337
    },
    {
        "loss": 2.1055,
        "grad_norm": 4.469290733337402,
        "learning_rate": 0.00016755493901490857,
        "epoch": 0.8450666666666666,
        "step": 6338
    },
    {
        "loss": 2.2413,
        "grad_norm": 2.4096670150756836,
        "learning_rate": 0.00016750852144705608,
        "epoch": 0.8452,
        "step": 6339
    },
    {
        "loss": 2.6925,
        "grad_norm": 1.5293582677841187,
        "learning_rate": 0.0001674620771388483,
        "epoch": 0.8453333333333334,
        "step": 6340
    },
    {
        "loss": 2.1646,
        "grad_norm": 2.473790168762207,
        "learning_rate": 0.00016741560610868182,
        "epoch": 0.8454666666666667,
        "step": 6341
    },
    {
        "loss": 2.1243,
        "grad_norm": 3.844059705734253,
        "learning_rate": 0.00016736910837496412,
        "epoch": 0.8456,
        "step": 6342
    },
    {
        "loss": 2.237,
        "grad_norm": 3.766019105911255,
        "learning_rate": 0.00016732258395611298,
        "epoch": 0.8457333333333333,
        "step": 6343
    },
    {
        "loss": 2.7922,
        "grad_norm": 2.9896774291992188,
        "learning_rate": 0.00016727603287055697,
        "epoch": 0.8458666666666667,
        "step": 6344
    },
    {
        "loss": 2.3238,
        "grad_norm": 3.266580104827881,
        "learning_rate": 0.0001672294551367351,
        "epoch": 0.846,
        "step": 6345
    },
    {
        "loss": 2.0146,
        "grad_norm": 3.6786208152770996,
        "learning_rate": 0.00016718285077309695,
        "epoch": 0.8461333333333333,
        "step": 6346
    },
    {
        "loss": 1.8749,
        "grad_norm": 4.08075475692749,
        "learning_rate": 0.0001671362197981027,
        "epoch": 0.8462666666666666,
        "step": 6347
    },
    {
        "loss": 2.6966,
        "grad_norm": 3.1816842555999756,
        "learning_rate": 0.00016708956223022303,
        "epoch": 0.8464,
        "step": 6348
    },
    {
        "loss": 2.2761,
        "grad_norm": 3.4019346237182617,
        "learning_rate": 0.00016704287808793912,
        "epoch": 0.8465333333333334,
        "step": 6349
    },
    {
        "loss": 2.3153,
        "grad_norm": 3.5140280723571777,
        "learning_rate": 0.00016699616738974284,
        "epoch": 0.8466666666666667,
        "step": 6350
    },
    {
        "loss": 2.6649,
        "grad_norm": 2.6687309741973877,
        "learning_rate": 0.0001669494301541363,
        "epoch": 0.8468,
        "step": 6351
    },
    {
        "loss": 1.2351,
        "grad_norm": 5.400947570800781,
        "learning_rate": 0.00016690266639963244,
        "epoch": 0.8469333333333333,
        "step": 6352
    },
    {
        "loss": 2.612,
        "grad_norm": 3.539876937866211,
        "learning_rate": 0.00016685587614475436,
        "epoch": 0.8470666666666666,
        "step": 6353
    },
    {
        "loss": 1.844,
        "grad_norm": 4.214788913726807,
        "learning_rate": 0.000166809059408036,
        "epoch": 0.8472,
        "step": 6354
    },
    {
        "loss": 2.6885,
        "grad_norm": 4.0255126953125,
        "learning_rate": 0.00016676221620802144,
        "epoch": 0.8473333333333334,
        "step": 6355
    },
    {
        "loss": 3.0523,
        "grad_norm": 2.2038631439208984,
        "learning_rate": 0.0001667153465632657,
        "epoch": 0.8474666666666667,
        "step": 6356
    },
    {
        "loss": 2.323,
        "grad_norm": 2.1374738216400146,
        "learning_rate": 0.00016666845049233376,
        "epoch": 0.8476,
        "step": 6357
    },
    {
        "loss": 3.9505,
        "grad_norm": 3.223409414291382,
        "learning_rate": 0.00016662152801380145,
        "epoch": 0.8477333333333333,
        "step": 6358
    },
    {
        "loss": 2.1088,
        "grad_norm": 3.9557433128356934,
        "learning_rate": 0.00016657457914625491,
        "epoch": 0.8478666666666667,
        "step": 6359
    },
    {
        "loss": 2.4434,
        "grad_norm": 4.231848239898682,
        "learning_rate": 0.00016652760390829067,
        "epoch": 0.848,
        "step": 6360
    },
    {
        "loss": 0.7123,
        "grad_norm": 2.8322343826293945,
        "learning_rate": 0.0001664806023185159,
        "epoch": 0.8481333333333333,
        "step": 6361
    },
    {
        "loss": 2.3244,
        "grad_norm": 3.3349051475524902,
        "learning_rate": 0.00016643357439554798,
        "epoch": 0.8482666666666666,
        "step": 6362
    },
    {
        "loss": 0.9238,
        "grad_norm": 5.7499589920043945,
        "learning_rate": 0.00016638652015801492,
        "epoch": 0.8484,
        "step": 6363
    },
    {
        "loss": 2.1449,
        "grad_norm": 2.812619209289551,
        "learning_rate": 0.0001663394396245549,
        "epoch": 0.8485333333333334,
        "step": 6364
    },
    {
        "loss": 2.8288,
        "grad_norm": 4.0052571296691895,
        "learning_rate": 0.00016629233281381694,
        "epoch": 0.8486666666666667,
        "step": 6365
    },
    {
        "loss": 2.6899,
        "grad_norm": 4.6595940589904785,
        "learning_rate": 0.00016624519974446,
        "epoch": 0.8488,
        "step": 6366
    },
    {
        "loss": 3.0102,
        "grad_norm": 2.4559781551361084,
        "learning_rate": 0.00016619804043515376,
        "epoch": 0.8489333333333333,
        "step": 6367
    },
    {
        "loss": 1.8404,
        "grad_norm": 2.3164303302764893,
        "learning_rate": 0.00016615085490457806,
        "epoch": 0.8490666666666666,
        "step": 6368
    },
    {
        "loss": 2.375,
        "grad_norm": 3.486268997192383,
        "learning_rate": 0.00016610364317142342,
        "epoch": 0.8492,
        "step": 6369
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.2608602046966553,
        "learning_rate": 0.0001660564052543904,
        "epoch": 0.8493333333333334,
        "step": 6370
    },
    {
        "loss": 2.6624,
        "grad_norm": 5.683837413787842,
        "learning_rate": 0.0001660091411721902,
        "epoch": 0.8494666666666667,
        "step": 6371
    },
    {
        "loss": 2.8484,
        "grad_norm": 5.281884670257568,
        "learning_rate": 0.0001659618509435443,
        "epoch": 0.8496,
        "step": 6372
    },
    {
        "loss": 1.9953,
        "grad_norm": 3.2389633655548096,
        "learning_rate": 0.0001659145345871844,
        "epoch": 0.8497333333333333,
        "step": 6373
    },
    {
        "loss": 2.6634,
        "grad_norm": 3.711103916168213,
        "learning_rate": 0.0001658671921218528,
        "epoch": 0.8498666666666667,
        "step": 6374
    },
    {
        "loss": 0.6336,
        "grad_norm": 2.1647281646728516,
        "learning_rate": 0.00016581982356630203,
        "epoch": 0.85,
        "step": 6375
    },
    {
        "loss": 1.9155,
        "grad_norm": 4.292088985443115,
        "learning_rate": 0.0001657724289392948,
        "epoch": 0.8501333333333333,
        "step": 6376
    },
    {
        "loss": 1.8071,
        "grad_norm": 5.53949499130249,
        "learning_rate": 0.00016572500825960438,
        "epoch": 0.8502666666666666,
        "step": 6377
    },
    {
        "loss": 2.8471,
        "grad_norm": 3.149341583251953,
        "learning_rate": 0.0001656775615460142,
        "epoch": 0.8504,
        "step": 6378
    },
    {
        "loss": 2.7217,
        "grad_norm": 3.3464508056640625,
        "learning_rate": 0.0001656300888173181,
        "epoch": 0.8505333333333334,
        "step": 6379
    },
    {
        "loss": 2.4929,
        "grad_norm": 2.3891196250915527,
        "learning_rate": 0.00016558259009232017,
        "epoch": 0.8506666666666667,
        "step": 6380
    },
    {
        "loss": 2.5172,
        "grad_norm": 2.98616361618042,
        "learning_rate": 0.00016553506538983492,
        "epoch": 0.8508,
        "step": 6381
    },
    {
        "loss": 2.013,
        "grad_norm": 3.499666929244995,
        "learning_rate": 0.00016548751472868692,
        "epoch": 0.8509333333333333,
        "step": 6382
    },
    {
        "loss": 0.9591,
        "grad_norm": 3.356621265411377,
        "learning_rate": 0.00016543993812771115,
        "epoch": 0.8510666666666666,
        "step": 6383
    },
    {
        "loss": 2.6738,
        "grad_norm": 1.9331055879592896,
        "learning_rate": 0.00016539233560575298,
        "epoch": 0.8512,
        "step": 6384
    },
    {
        "loss": 2.1859,
        "grad_norm": 3.1341350078582764,
        "learning_rate": 0.0001653447071816678,
        "epoch": 0.8513333333333334,
        "step": 6385
    },
    {
        "loss": 2.1064,
        "grad_norm": 3.9324731826782227,
        "learning_rate": 0.00016529705287432153,
        "epoch": 0.8514666666666667,
        "step": 6386
    },
    {
        "loss": 2.645,
        "grad_norm": 4.5872344970703125,
        "learning_rate": 0.00016524937270259003,
        "epoch": 0.8516,
        "step": 6387
    },
    {
        "loss": 2.4247,
        "grad_norm": 3.3783419132232666,
        "learning_rate": 0.00016520166668535974,
        "epoch": 0.8517333333333333,
        "step": 6388
    },
    {
        "loss": 2.587,
        "grad_norm": 2.9882655143737793,
        "learning_rate": 0.00016515393484152702,
        "epoch": 0.8518666666666667,
        "step": 6389
    },
    {
        "loss": 2.4223,
        "grad_norm": 3.1042325496673584,
        "learning_rate": 0.00016510617718999877,
        "epoch": 0.852,
        "step": 6390
    },
    {
        "loss": 2.2615,
        "grad_norm": 3.1757616996765137,
        "learning_rate": 0.00016505839374969186,
        "epoch": 0.8521333333333333,
        "step": 6391
    },
    {
        "loss": 2.7633,
        "grad_norm": 2.6179535388946533,
        "learning_rate": 0.00016501058453953355,
        "epoch": 0.8522666666666666,
        "step": 6392
    },
    {
        "loss": 2.2472,
        "grad_norm": 4.341197967529297,
        "learning_rate": 0.00016496274957846115,
        "epoch": 0.8524,
        "step": 6393
    },
    {
        "loss": 2.6534,
        "grad_norm": 3.0251290798187256,
        "learning_rate": 0.0001649148888854223,
        "epoch": 0.8525333333333334,
        "step": 6394
    },
    {
        "loss": 2.1257,
        "grad_norm": 3.1890134811401367,
        "learning_rate": 0.00016486700247937477,
        "epoch": 0.8526666666666667,
        "step": 6395
    },
    {
        "loss": 1.9141,
        "grad_norm": 4.422197341918945,
        "learning_rate": 0.00016481909037928652,
        "epoch": 0.8528,
        "step": 6396
    },
    {
        "loss": 2.7127,
        "grad_norm": 4.353317737579346,
        "learning_rate": 0.00016477115260413573,
        "epoch": 0.8529333333333333,
        "step": 6397
    },
    {
        "loss": 3.0522,
        "grad_norm": 2.9036056995391846,
        "learning_rate": 0.00016472318917291057,
        "epoch": 0.8530666666666666,
        "step": 6398
    },
    {
        "loss": 2.3225,
        "grad_norm": 4.771596908569336,
        "learning_rate": 0.0001646752001046098,
        "epoch": 0.8532,
        "step": 6399
    },
    {
        "loss": 3.0305,
        "grad_norm": 2.239717721939087,
        "learning_rate": 0.00016462718541824183,
        "epoch": 0.8533333333333334,
        "step": 6400
    },
    {
        "loss": 2.453,
        "grad_norm": 1.692041039466858,
        "learning_rate": 0.00016457914513282553,
        "epoch": 0.8534666666666667,
        "step": 6401
    },
    {
        "loss": 2.6408,
        "grad_norm": 3.770580530166626,
        "learning_rate": 0.00016453107926738976,
        "epoch": 0.8536,
        "step": 6402
    },
    {
        "loss": 2.8328,
        "grad_norm": 3.358433246612549,
        "learning_rate": 0.00016448298784097363,
        "epoch": 0.8537333333333333,
        "step": 6403
    },
    {
        "loss": 2.3883,
        "grad_norm": 3.73716402053833,
        "learning_rate": 0.00016443487087262627,
        "epoch": 0.8538666666666667,
        "step": 6404
    },
    {
        "loss": 1.2949,
        "grad_norm": 3.661165714263916,
        "learning_rate": 0.00016438672838140698,
        "epoch": 0.854,
        "step": 6405
    },
    {
        "loss": 1.7891,
        "grad_norm": 2.3988585472106934,
        "learning_rate": 0.00016433856038638526,
        "epoch": 0.8541333333333333,
        "step": 6406
    },
    {
        "loss": 2.259,
        "grad_norm": 2.666968584060669,
        "learning_rate": 0.0001642903669066405,
        "epoch": 0.8542666666666666,
        "step": 6407
    },
    {
        "loss": 2.7069,
        "grad_norm": 2.719972848892212,
        "learning_rate": 0.00016424214796126232,
        "epoch": 0.8544,
        "step": 6408
    },
    {
        "loss": 1.1535,
        "grad_norm": 3.089571237564087,
        "learning_rate": 0.0001641939035693505,
        "epoch": 0.8545333333333334,
        "step": 6409
    },
    {
        "loss": 2.5589,
        "grad_norm": 3.0512874126434326,
        "learning_rate": 0.0001641456337500147,
        "epoch": 0.8546666666666667,
        "step": 6410
    },
    {
        "loss": 2.3971,
        "grad_norm": 4.200301647186279,
        "learning_rate": 0.00016409733852237483,
        "epoch": 0.8548,
        "step": 6411
    },
    {
        "loss": 3.2563,
        "grad_norm": 4.071128845214844,
        "learning_rate": 0.0001640490179055607,
        "epoch": 0.8549333333333333,
        "step": 6412
    },
    {
        "loss": 2.7141,
        "grad_norm": 2.499007225036621,
        "learning_rate": 0.00016400067191871243,
        "epoch": 0.8550666666666666,
        "step": 6413
    },
    {
        "loss": 1.6201,
        "grad_norm": 2.99176287651062,
        "learning_rate": 0.00016395230058097982,
        "epoch": 0.8552,
        "step": 6414
    },
    {
        "loss": 2.2318,
        "grad_norm": 2.144268751144409,
        "learning_rate": 0.00016390390391152312,
        "epoch": 0.8553333333333333,
        "step": 6415
    },
    {
        "loss": 2.6863,
        "grad_norm": 2.4320921897888184,
        "learning_rate": 0.0001638554819295123,
        "epoch": 0.8554666666666667,
        "step": 6416
    },
    {
        "loss": 2.5802,
        "grad_norm": 4.440957546234131,
        "learning_rate": 0.00016380703465412755,
        "epoch": 0.8556,
        "step": 6417
    },
    {
        "loss": 1.505,
        "grad_norm": 3.822343587875366,
        "learning_rate": 0.00016375856210455888,
        "epoch": 0.8557333333333333,
        "step": 6418
    },
    {
        "loss": 1.3999,
        "grad_norm": 3.600796699523926,
        "learning_rate": 0.00016371006430000655,
        "epoch": 0.8558666666666667,
        "step": 6419
    },
    {
        "loss": 1.4535,
        "grad_norm": 4.102974891662598,
        "learning_rate": 0.0001636615412596807,
        "epoch": 0.856,
        "step": 6420
    },
    {
        "loss": 2.3957,
        "grad_norm": 3.394052267074585,
        "learning_rate": 0.00016361299300280137,
        "epoch": 0.8561333333333333,
        "step": 6421
    },
    {
        "loss": 2.2711,
        "grad_norm": 3.485062837600708,
        "learning_rate": 0.00016356441954859885,
        "epoch": 0.8562666666666666,
        "step": 6422
    },
    {
        "loss": 1.8955,
        "grad_norm": 3.4937143325805664,
        "learning_rate": 0.00016351582091631302,
        "epoch": 0.8564,
        "step": 6423
    },
    {
        "loss": 2.5685,
        "grad_norm": 2.8904640674591064,
        "learning_rate": 0.0001634671971251942,
        "epoch": 0.8565333333333334,
        "step": 6424
    },
    {
        "loss": 1.579,
        "grad_norm": 4.298336029052734,
        "learning_rate": 0.0001634185481945023,
        "epoch": 0.8566666666666667,
        "step": 6425
    },
    {
        "loss": 2.9259,
        "grad_norm": 2.869680166244507,
        "learning_rate": 0.00016336987414350743,
        "epoch": 0.8568,
        "step": 6426
    },
    {
        "loss": 3.0791,
        "grad_norm": 2.8998019695281982,
        "learning_rate": 0.0001633211749914894,
        "epoch": 0.8569333333333333,
        "step": 6427
    },
    {
        "loss": 1.5336,
        "grad_norm": 3.107280969619751,
        "learning_rate": 0.00016327245075773824,
        "epoch": 0.8570666666666666,
        "step": 6428
    },
    {
        "loss": 1.4024,
        "grad_norm": 4.034419536590576,
        "learning_rate": 0.0001632237014615537,
        "epoch": 0.8572,
        "step": 6429
    },
    {
        "loss": 2.4536,
        "grad_norm": 4.954023838043213,
        "learning_rate": 0.00016317492712224563,
        "epoch": 0.8573333333333333,
        "step": 6430
    },
    {
        "loss": 1.5021,
        "grad_norm": 4.27783727645874,
        "learning_rate": 0.00016312612775913366,
        "epoch": 0.8574666666666667,
        "step": 6431
    },
    {
        "loss": 3.2177,
        "grad_norm": 4.321399211883545,
        "learning_rate": 0.00016307730339154737,
        "epoch": 0.8576,
        "step": 6432
    },
    {
        "loss": 2.7309,
        "grad_norm": 3.0022921562194824,
        "learning_rate": 0.00016302845403882624,
        "epoch": 0.8577333333333333,
        "step": 6433
    },
    {
        "loss": 0.9806,
        "grad_norm": 4.834471225738525,
        "learning_rate": 0.0001629795797203198,
        "epoch": 0.8578666666666667,
        "step": 6434
    },
    {
        "loss": 2.7175,
        "grad_norm": 6.276225566864014,
        "learning_rate": 0.00016293068045538712,
        "epoch": 0.858,
        "step": 6435
    },
    {
        "loss": 2.04,
        "grad_norm": 6.063230037689209,
        "learning_rate": 0.00016288175626339756,
        "epoch": 0.8581333333333333,
        "step": 6436
    },
    {
        "loss": 1.91,
        "grad_norm": 3.1772773265838623,
        "learning_rate": 0.00016283280716373005,
        "epoch": 0.8582666666666666,
        "step": 6437
    },
    {
        "loss": 2.7194,
        "grad_norm": 2.9351840019226074,
        "learning_rate": 0.00016278383317577358,
        "epoch": 0.8584,
        "step": 6438
    },
    {
        "loss": 1.3995,
        "grad_norm": 4.065642356872559,
        "learning_rate": 0.0001627348343189267,
        "epoch": 0.8585333333333334,
        "step": 6439
    },
    {
        "loss": 3.0501,
        "grad_norm": 2.7658650875091553,
        "learning_rate": 0.00016268581061259835,
        "epoch": 0.8586666666666667,
        "step": 6440
    },
    {
        "loss": 1.5186,
        "grad_norm": 4.3112592697143555,
        "learning_rate": 0.00016263676207620674,
        "epoch": 0.8588,
        "step": 6441
    },
    {
        "loss": 1.6905,
        "grad_norm": 4.152101516723633,
        "learning_rate": 0.00016258768872918022,
        "epoch": 0.8589333333333333,
        "step": 6442
    },
    {
        "loss": 2.3117,
        "grad_norm": 5.081745147705078,
        "learning_rate": 0.00016253859059095702,
        "epoch": 0.8590666666666666,
        "step": 6443
    },
    {
        "loss": 1.9612,
        "grad_norm": 2.7018611431121826,
        "learning_rate": 0.00016248946768098493,
        "epoch": 0.8592,
        "step": 6444
    },
    {
        "loss": 2.167,
        "grad_norm": 5.2405195236206055,
        "learning_rate": 0.0001624403200187218,
        "epoch": 0.8593333333333333,
        "step": 6445
    },
    {
        "loss": 2.0561,
        "grad_norm": 3.3129959106445312,
        "learning_rate": 0.0001623911476236351,
        "epoch": 0.8594666666666667,
        "step": 6446
    },
    {
        "loss": 2.6483,
        "grad_norm": 3.02005672454834,
        "learning_rate": 0.0001623419505152023,
        "epoch": 0.8596,
        "step": 6447
    },
    {
        "loss": 2.0821,
        "grad_norm": 3.877729654312134,
        "learning_rate": 0.00016229272871291038,
        "epoch": 0.8597333333333333,
        "step": 6448
    },
    {
        "loss": 2.8131,
        "grad_norm": 4.536624908447266,
        "learning_rate": 0.00016224348223625645,
        "epoch": 0.8598666666666667,
        "step": 6449
    },
    {
        "loss": 2.9746,
        "grad_norm": 5.079041481018066,
        "learning_rate": 0.00016219421110474707,
        "epoch": 0.86,
        "step": 6450
    },
    {
        "loss": 1.859,
        "grad_norm": 4.003105163574219,
        "learning_rate": 0.0001621449153378988,
        "epoch": 0.8601333333333333,
        "step": 6451
    },
    {
        "loss": 2.9206,
        "grad_norm": 3.4398367404937744,
        "learning_rate": 0.00016209559495523776,
        "epoch": 0.8602666666666666,
        "step": 6452
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.160877227783203,
        "learning_rate": 0.00016204624997630002,
        "epoch": 0.8604,
        "step": 6453
    },
    {
        "loss": 2.5333,
        "grad_norm": 3.019348621368408,
        "learning_rate": 0.0001619968804206312,
        "epoch": 0.8605333333333334,
        "step": 6454
    },
    {
        "loss": 2.9404,
        "grad_norm": 2.7365405559539795,
        "learning_rate": 0.00016194748630778678,
        "epoch": 0.8606666666666667,
        "step": 6455
    },
    {
        "loss": 2.7331,
        "grad_norm": 2.7598910331726074,
        "learning_rate": 0.00016189806765733202,
        "epoch": 0.8608,
        "step": 6456
    },
    {
        "loss": 1.0066,
        "grad_norm": 3.7580859661102295,
        "learning_rate": 0.00016184862448884171,
        "epoch": 0.8609333333333333,
        "step": 6457
    },
    {
        "loss": 2.2709,
        "grad_norm": 3.38252329826355,
        "learning_rate": 0.00016179915682190048,
        "epoch": 0.8610666666666666,
        "step": 6458
    },
    {
        "loss": 3.2199,
        "grad_norm": 3.3598976135253906,
        "learning_rate": 0.00016174966467610264,
        "epoch": 0.8612,
        "step": 6459
    },
    {
        "loss": 3.0467,
        "grad_norm": 4.00152587890625,
        "learning_rate": 0.0001617001480710523,
        "epoch": 0.8613333333333333,
        "step": 6460
    },
    {
        "loss": 2.5216,
        "grad_norm": 3.2146503925323486,
        "learning_rate": 0.00016165060702636299,
        "epoch": 0.8614666666666667,
        "step": 6461
    },
    {
        "loss": 2.5124,
        "grad_norm": 2.8371376991271973,
        "learning_rate": 0.00016160104156165824,
        "epoch": 0.8616,
        "step": 6462
    },
    {
        "loss": 1.5769,
        "grad_norm": 4.724026203155518,
        "learning_rate": 0.00016155145169657095,
        "epoch": 0.8617333333333334,
        "step": 6463
    },
    {
        "loss": 1.3139,
        "grad_norm": 5.915924549102783,
        "learning_rate": 0.00016150183745074393,
        "epoch": 0.8618666666666667,
        "step": 6464
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.848651647567749,
        "learning_rate": 0.0001614521988438296,
        "epoch": 0.862,
        "step": 6465
    },
    {
        "loss": 2.3533,
        "grad_norm": 3.8043417930603027,
        "learning_rate": 0.00016140253589548985,
        "epoch": 0.8621333333333333,
        "step": 6466
    },
    {
        "loss": 1.9936,
        "grad_norm": 3.292330026626587,
        "learning_rate": 0.00016135284862539638,
        "epoch": 0.8622666666666666,
        "step": 6467
    },
    {
        "loss": 1.6794,
        "grad_norm": 4.147143363952637,
        "learning_rate": 0.00016130313705323062,
        "epoch": 0.8624,
        "step": 6468
    },
    {
        "loss": 2.9497,
        "grad_norm": 4.472464561462402,
        "learning_rate": 0.0001612534011986833,
        "epoch": 0.8625333333333334,
        "step": 6469
    },
    {
        "loss": 1.7174,
        "grad_norm": 3.7995293140411377,
        "learning_rate": 0.0001612036410814551,
        "epoch": 0.8626666666666667,
        "step": 6470
    },
    {
        "loss": 1.8626,
        "grad_norm": 3.414983034133911,
        "learning_rate": 0.0001611538567212561,
        "epoch": 0.8628,
        "step": 6471
    },
    {
        "loss": 2.5961,
        "grad_norm": 6.639219760894775,
        "learning_rate": 0.0001611040481378061,
        "epoch": 0.8629333333333333,
        "step": 6472
    },
    {
        "loss": 2.3032,
        "grad_norm": 3.0986952781677246,
        "learning_rate": 0.00016105421535083433,
        "epoch": 0.8630666666666666,
        "step": 6473
    },
    {
        "loss": 2.1739,
        "grad_norm": 6.153705596923828,
        "learning_rate": 0.00016100435838007992,
        "epoch": 0.8632,
        "step": 6474
    },
    {
        "loss": 1.8037,
        "grad_norm": 3.638195514678955,
        "learning_rate": 0.00016095447724529124,
        "epoch": 0.8633333333333333,
        "step": 6475
    },
    {
        "loss": 1.4083,
        "grad_norm": 3.524441719055176,
        "learning_rate": 0.00016090457196622651,
        "epoch": 0.8634666666666667,
        "step": 6476
    },
    {
        "loss": 2.0898,
        "grad_norm": 4.274633884429932,
        "learning_rate": 0.0001608546425626532,
        "epoch": 0.8636,
        "step": 6477
    },
    {
        "loss": 2.125,
        "grad_norm": 3.258746385574341,
        "learning_rate": 0.0001608046890543487,
        "epoch": 0.8637333333333334,
        "step": 6478
    },
    {
        "loss": 2.0,
        "grad_norm": 2.2356197834014893,
        "learning_rate": 0.00016075471146109957,
        "epoch": 0.8638666666666667,
        "step": 6479
    },
    {
        "loss": 2.6051,
        "grad_norm": 2.970965623855591,
        "learning_rate": 0.00016070470980270228,
        "epoch": 0.864,
        "step": 6480
    },
    {
        "loss": 1.8955,
        "grad_norm": 5.150641918182373,
        "learning_rate": 0.00016065468409896257,
        "epoch": 0.8641333333333333,
        "step": 6481
    },
    {
        "loss": 2.9141,
        "grad_norm": 3.7254433631896973,
        "learning_rate": 0.00016060463436969578,
        "epoch": 0.8642666666666666,
        "step": 6482
    },
    {
        "loss": 1.8125,
        "grad_norm": 3.029792308807373,
        "learning_rate": 0.00016055456063472686,
        "epoch": 0.8644,
        "step": 6483
    },
    {
        "loss": 1.9969,
        "grad_norm": 3.6276495456695557,
        "learning_rate": 0.00016050446291389014,
        "epoch": 0.8645333333333334,
        "step": 6484
    },
    {
        "loss": 2.3308,
        "grad_norm": 3.7043638229370117,
        "learning_rate": 0.00016045434122702952,
        "epoch": 0.8646666666666667,
        "step": 6485
    },
    {
        "loss": 2.1599,
        "grad_norm": 3.6330859661102295,
        "learning_rate": 0.00016040419559399828,
        "epoch": 0.8648,
        "step": 6486
    },
    {
        "loss": 2.3396,
        "grad_norm": 3.539482593536377,
        "learning_rate": 0.00016035402603465943,
        "epoch": 0.8649333333333333,
        "step": 6487
    },
    {
        "loss": 2.2121,
        "grad_norm": 4.0406904220581055,
        "learning_rate": 0.00016030383256888513,
        "epoch": 0.8650666666666667,
        "step": 6488
    },
    {
        "loss": 2.0408,
        "grad_norm": 4.201488494873047,
        "learning_rate": 0.0001602536152165573,
        "epoch": 0.8652,
        "step": 6489
    },
    {
        "loss": 2.4811,
        "grad_norm": 3.616528272628784,
        "learning_rate": 0.0001602033739975672,
        "epoch": 0.8653333333333333,
        "step": 6490
    },
    {
        "loss": 1.6555,
        "grad_norm": 4.614626884460449,
        "learning_rate": 0.00016015310893181548,
        "epoch": 0.8654666666666667,
        "step": 6491
    },
    {
        "loss": 0.9805,
        "grad_norm": 3.5717122554779053,
        "learning_rate": 0.00016010282003921234,
        "epoch": 0.8656,
        "step": 6492
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.949042558670044,
        "learning_rate": 0.00016005250733967745,
        "epoch": 0.8657333333333334,
        "step": 6493
    },
    {
        "loss": 3.3651,
        "grad_norm": 2.019747018814087,
        "learning_rate": 0.0001600021708531397,
        "epoch": 0.8658666666666667,
        "step": 6494
    },
    {
        "loss": 2.596,
        "grad_norm": 4.939227104187012,
        "learning_rate": 0.00015995181059953772,
        "epoch": 0.866,
        "step": 6495
    },
    {
        "loss": 1.8549,
        "grad_norm": 5.194525241851807,
        "learning_rate": 0.0001599014265988192,
        "epoch": 0.8661333333333333,
        "step": 6496
    },
    {
        "loss": 1.7806,
        "grad_norm": 3.856945753097534,
        "learning_rate": 0.00015985101887094154,
        "epoch": 0.8662666666666666,
        "step": 6497
    },
    {
        "loss": 2.7209,
        "grad_norm": 2.173314094543457,
        "learning_rate": 0.00015980058743587128,
        "epoch": 0.8664,
        "step": 6498
    },
    {
        "loss": 2.8249,
        "grad_norm": 2.5117745399475098,
        "learning_rate": 0.00015975013231358468,
        "epoch": 0.8665333333333334,
        "step": 6499
    },
    {
        "loss": 1.2146,
        "grad_norm": 3.6746702194213867,
        "learning_rate": 0.000159699653524067,
        "epoch": 0.8666666666666667,
        "step": 6500
    },
    {
        "loss": 1.8107,
        "grad_norm": 2.800452947616577,
        "learning_rate": 0.00015964915108731317,
        "epoch": 0.8668,
        "step": 6501
    },
    {
        "loss": 2.4268,
        "grad_norm": 2.798074722290039,
        "learning_rate": 0.00015959862502332736,
        "epoch": 0.8669333333333333,
        "step": 6502
    },
    {
        "loss": 2.6708,
        "grad_norm": 2.7745420932769775,
        "learning_rate": 0.00015954807535212308,
        "epoch": 0.8670666666666667,
        "step": 6503
    },
    {
        "loss": 3.0366,
        "grad_norm": 3.3325002193450928,
        "learning_rate": 0.00015949750209372327,
        "epoch": 0.8672,
        "step": 6504
    },
    {
        "loss": 2.09,
        "grad_norm": 3.654836416244507,
        "learning_rate": 0.00015944690526816015,
        "epoch": 0.8673333333333333,
        "step": 6505
    },
    {
        "loss": 2.3426,
        "grad_norm": 2.898055076599121,
        "learning_rate": 0.00015939628489547528,
        "epoch": 0.8674666666666667,
        "step": 6506
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.801880121231079,
        "learning_rate": 0.0001593456409957195,
        "epoch": 0.8676,
        "step": 6507
    },
    {
        "loss": 2.5962,
        "grad_norm": 2.3077611923217773,
        "learning_rate": 0.00015929497358895323,
        "epoch": 0.8677333333333334,
        "step": 6508
    },
    {
        "loss": 2.3799,
        "grad_norm": 2.9268338680267334,
        "learning_rate": 0.00015924428269524577,
        "epoch": 0.8678666666666667,
        "step": 6509
    },
    {
        "loss": 2.2675,
        "grad_norm": 3.193406343460083,
        "learning_rate": 0.0001591935683346762,
        "epoch": 0.868,
        "step": 6510
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.6000497341156006,
        "learning_rate": 0.0001591428305273324,
        "epoch": 0.8681333333333333,
        "step": 6511
    },
    {
        "loss": 2.6507,
        "grad_norm": 3.1314666271209717,
        "learning_rate": 0.00015909206929331197,
        "epoch": 0.8682666666666666,
        "step": 6512
    },
    {
        "loss": 2.08,
        "grad_norm": 2.6565423011779785,
        "learning_rate": 0.0001590412846527215,
        "epoch": 0.8684,
        "step": 6513
    },
    {
        "loss": 0.8456,
        "grad_norm": 2.7217934131622314,
        "learning_rate": 0.000158990476625677,
        "epoch": 0.8685333333333334,
        "step": 6514
    },
    {
        "loss": 1.981,
        "grad_norm": 4.275835037231445,
        "learning_rate": 0.00015893964523230373,
        "epoch": 0.8686666666666667,
        "step": 6515
    },
    {
        "loss": 2.34,
        "grad_norm": 3.1214654445648193,
        "learning_rate": 0.00015888879049273612,
        "epoch": 0.8688,
        "step": 6516
    },
    {
        "loss": 0.6569,
        "grad_norm": 2.690711736679077,
        "learning_rate": 0.00015883791242711793,
        "epoch": 0.8689333333333333,
        "step": 6517
    },
    {
        "loss": 2.7804,
        "grad_norm": 2.575526237487793,
        "learning_rate": 0.00015878701105560223,
        "epoch": 0.8690666666666667,
        "step": 6518
    },
    {
        "loss": 2.5112,
        "grad_norm": 3.5240707397460938,
        "learning_rate": 0.00015873608639835104,
        "epoch": 0.8692,
        "step": 6519
    },
    {
        "loss": 2.8738,
        "grad_norm": 2.7923049926757812,
        "learning_rate": 0.0001586851384755359,
        "epoch": 0.8693333333333333,
        "step": 6520
    },
    {
        "loss": 2.7044,
        "grad_norm": 3.6379880905151367,
        "learning_rate": 0.0001586341673073375,
        "epoch": 0.8694666666666667,
        "step": 6521
    },
    {
        "loss": 2.1999,
        "grad_norm": 3.414950370788574,
        "learning_rate": 0.0001585831729139456,
        "epoch": 0.8696,
        "step": 6522
    },
    {
        "loss": 2.7921,
        "grad_norm": 2.9912972450256348,
        "learning_rate": 0.0001585321553155593,
        "epoch": 0.8697333333333334,
        "step": 6523
    },
    {
        "loss": 2.643,
        "grad_norm": 3.850973606109619,
        "learning_rate": 0.00015848111453238686,
        "epoch": 0.8698666666666667,
        "step": 6524
    },
    {
        "loss": 2.4549,
        "grad_norm": 3.1183207035064697,
        "learning_rate": 0.00015843005058464564,
        "epoch": 0.87,
        "step": 6525
    },
    {
        "loss": 2.122,
        "grad_norm": 3.6975607872009277,
        "learning_rate": 0.00015837896349256227,
        "epoch": 0.8701333333333333,
        "step": 6526
    },
    {
        "loss": 2.8307,
        "grad_norm": 3.272150754928589,
        "learning_rate": 0.00015832785327637262,
        "epoch": 0.8702666666666666,
        "step": 6527
    },
    {
        "loss": 2.4201,
        "grad_norm": 4.085704326629639,
        "learning_rate": 0.0001582767199563215,
        "epoch": 0.8704,
        "step": 6528
    },
    {
        "loss": 2.8516,
        "grad_norm": 2.1395838260650635,
        "learning_rate": 0.00015822556355266304,
        "epoch": 0.8705333333333334,
        "step": 6529
    },
    {
        "loss": 3.1245,
        "grad_norm": 6.2624616622924805,
        "learning_rate": 0.00015817438408566043,
        "epoch": 0.8706666666666667,
        "step": 6530
    },
    {
        "loss": 2.7814,
        "grad_norm": 3.068544626235962,
        "learning_rate": 0.00015812318157558614,
        "epoch": 0.8708,
        "step": 6531
    },
    {
        "loss": 1.7261,
        "grad_norm": 3.51210880279541,
        "learning_rate": 0.00015807195604272141,
        "epoch": 0.8709333333333333,
        "step": 6532
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.310758590698242,
        "learning_rate": 0.00015802070750735715,
        "epoch": 0.8710666666666667,
        "step": 6533
    },
    {
        "loss": 2.3742,
        "grad_norm": 3.2218360900878906,
        "learning_rate": 0.00015796943598979293,
        "epoch": 0.8712,
        "step": 6534
    },
    {
        "loss": 2.0003,
        "grad_norm": 4.070403099060059,
        "learning_rate": 0.0001579181415103376,
        "epoch": 0.8713333333333333,
        "step": 6535
    },
    {
        "loss": 0.9551,
        "grad_norm": 5.14825439453125,
        "learning_rate": 0.00015786682408930905,
        "epoch": 0.8714666666666666,
        "step": 6536
    },
    {
        "loss": 2.2368,
        "grad_norm": 4.188813209533691,
        "learning_rate": 0.0001578154837470343,
        "epoch": 0.8716,
        "step": 6537
    },
    {
        "loss": 1.9174,
        "grad_norm": 3.9285097122192383,
        "learning_rate": 0.0001577641205038494,
        "epoch": 0.8717333333333334,
        "step": 6538
    },
    {
        "loss": 2.3669,
        "grad_norm": 2.2082953453063965,
        "learning_rate": 0.00015771273438009958,
        "epoch": 0.8718666666666667,
        "step": 6539
    },
    {
        "loss": 2.6749,
        "grad_norm": 2.4892401695251465,
        "learning_rate": 0.00015766132539613904,
        "epoch": 0.872,
        "step": 6540
    },
    {
        "loss": 2.8191,
        "grad_norm": 2.445443868637085,
        "learning_rate": 0.00015760989357233092,
        "epoch": 0.8721333333333333,
        "step": 6541
    },
    {
        "loss": 1.9079,
        "grad_norm": 2.7464075088500977,
        "learning_rate": 0.00015755843892904778,
        "epoch": 0.8722666666666666,
        "step": 6542
    },
    {
        "loss": 1.78,
        "grad_norm": 3.445371627807617,
        "learning_rate": 0.0001575069614866708,
        "epoch": 0.8724,
        "step": 6543
    },
    {
        "loss": 1.0897,
        "grad_norm": 3.832587957382202,
        "learning_rate": 0.00015745546126559048,
        "epoch": 0.8725333333333334,
        "step": 6544
    },
    {
        "loss": 2.1984,
        "grad_norm": 2.970672369003296,
        "learning_rate": 0.00015740393828620613,
        "epoch": 0.8726666666666667,
        "step": 6545
    },
    {
        "loss": 1.2249,
        "grad_norm": 5.090946674346924,
        "learning_rate": 0.00015735239256892624,
        "epoch": 0.8728,
        "step": 6546
    },
    {
        "loss": 2.6661,
        "grad_norm": 2.207188129425049,
        "learning_rate": 0.00015730082413416822,
        "epoch": 0.8729333333333333,
        "step": 6547
    },
    {
        "loss": 2.4292,
        "grad_norm": 4.254787445068359,
        "learning_rate": 0.0001572492330023585,
        "epoch": 0.8730666666666667,
        "step": 6548
    },
    {
        "loss": 2.6889,
        "grad_norm": 3.826500654220581,
        "learning_rate": 0.00015719761919393257,
        "epoch": 0.8732,
        "step": 6549
    },
    {
        "loss": 2.4093,
        "grad_norm": 4.331879138946533,
        "learning_rate": 0.00015714598272933473,
        "epoch": 0.8733333333333333,
        "step": 6550
    },
    {
        "loss": 2.932,
        "grad_norm": 5.20279598236084,
        "learning_rate": 0.00015709432362901842,
        "epoch": 0.8734666666666666,
        "step": 6551
    },
    {
        "loss": 2.5583,
        "grad_norm": 3.1329452991485596,
        "learning_rate": 0.000157042641913446,
        "epoch": 0.8736,
        "step": 6552
    },
    {
        "loss": 2.744,
        "grad_norm": 2.3538246154785156,
        "learning_rate": 0.00015699093760308873,
        "epoch": 0.8737333333333334,
        "step": 6553
    },
    {
        "loss": 2.1849,
        "grad_norm": 3.164790391921997,
        "learning_rate": 0.00015693921071842693,
        "epoch": 0.8738666666666667,
        "step": 6554
    },
    {
        "loss": 1.4906,
        "grad_norm": 4.1436285972595215,
        "learning_rate": 0.00015688746127994965,
        "epoch": 0.874,
        "step": 6555
    },
    {
        "loss": 2.6983,
        "grad_norm": 3.3637890815734863,
        "learning_rate": 0.00015683568930815523,
        "epoch": 0.8741333333333333,
        "step": 6556
    },
    {
        "loss": 2.44,
        "grad_norm": 2.953676700592041,
        "learning_rate": 0.00015678389482355048,
        "epoch": 0.8742666666666666,
        "step": 6557
    },
    {
        "loss": 2.454,
        "grad_norm": 2.663898468017578,
        "learning_rate": 0.0001567320778466516,
        "epoch": 0.8744,
        "step": 6558
    },
    {
        "loss": 1.7643,
        "grad_norm": 3.3089816570281982,
        "learning_rate": 0.00015668023839798334,
        "epoch": 0.8745333333333334,
        "step": 6559
    },
    {
        "loss": 2.7467,
        "grad_norm": 3.8201260566711426,
        "learning_rate": 0.0001566283764980795,
        "epoch": 0.8746666666666667,
        "step": 6560
    },
    {
        "loss": 2.6949,
        "grad_norm": 2.870245933532715,
        "learning_rate": 0.00015657649216748286,
        "epoch": 0.8748,
        "step": 6561
    },
    {
        "loss": 3.1588,
        "grad_norm": 5.818042755126953,
        "learning_rate": 0.0001565245854267448,
        "epoch": 0.8749333333333333,
        "step": 6562
    },
    {
        "loss": 1.2592,
        "grad_norm": 4.646119117736816,
        "learning_rate": 0.00015647265629642593,
        "epoch": 0.8750666666666667,
        "step": 6563
    },
    {
        "loss": 1.9722,
        "grad_norm": 2.993133783340454,
        "learning_rate": 0.0001564207047970954,
        "epoch": 0.8752,
        "step": 6564
    },
    {
        "loss": 2.7084,
        "grad_norm": 2.6282618045806885,
        "learning_rate": 0.00015636873094933154,
        "epoch": 0.8753333333333333,
        "step": 6565
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.515320301055908,
        "learning_rate": 0.00015631673477372115,
        "epoch": 0.8754666666666666,
        "step": 6566
    },
    {
        "loss": 2.785,
        "grad_norm": 3.9463870525360107,
        "learning_rate": 0.00015626471629086032,
        "epoch": 0.8756,
        "step": 6567
    },
    {
        "loss": 2.722,
        "grad_norm": 3.7429630756378174,
        "learning_rate": 0.0001562126755213536,
        "epoch": 0.8757333333333334,
        "step": 6568
    },
    {
        "loss": 1.32,
        "grad_norm": 3.484280824661255,
        "learning_rate": 0.00015616061248581464,
        "epoch": 0.8758666666666667,
        "step": 6569
    },
    {
        "loss": 3.049,
        "grad_norm": 3.1728153228759766,
        "learning_rate": 0.00015610852720486564,
        "epoch": 0.876,
        "step": 6570
    },
    {
        "loss": 2.1734,
        "grad_norm": 2.7566721439361572,
        "learning_rate": 0.00015605641969913787,
        "epoch": 0.8761333333333333,
        "step": 6571
    },
    {
        "loss": 2.7114,
        "grad_norm": 3.364793062210083,
        "learning_rate": 0.0001560042899892712,
        "epoch": 0.8762666666666666,
        "step": 6572
    },
    {
        "loss": 2.272,
        "grad_norm": 3.5989091396331787,
        "learning_rate": 0.00015595213809591445,
        "epoch": 0.8764,
        "step": 6573
    },
    {
        "loss": 2.2726,
        "grad_norm": 2.9150331020355225,
        "learning_rate": 0.00015589996403972522,
        "epoch": 0.8765333333333334,
        "step": 6574
    },
    {
        "loss": 2.9538,
        "grad_norm": 3.426621913909912,
        "learning_rate": 0.00015584776784136967,
        "epoch": 0.8766666666666667,
        "step": 6575
    },
    {
        "loss": 1.7059,
        "grad_norm": 4.8873443603515625,
        "learning_rate": 0.000155795549521523,
        "epoch": 0.8768,
        "step": 6576
    },
    {
        "loss": 1.3694,
        "grad_norm": 3.997486114501953,
        "learning_rate": 0.0001557433091008691,
        "epoch": 0.8769333333333333,
        "step": 6577
    },
    {
        "loss": 2.6207,
        "grad_norm": 2.7789711952209473,
        "learning_rate": 0.00015569104660010044,
        "epoch": 0.8770666666666667,
        "step": 6578
    },
    {
        "loss": 2.4244,
        "grad_norm": Infinity,
        "learning_rate": 0.00015569104660010044,
        "epoch": 0.8772,
        "step": 6579
    },
    {
        "loss": 2.8695,
        "grad_norm": 3.5363309383392334,
        "learning_rate": 0.00015563876203991857,
        "epoch": 0.8773333333333333,
        "step": 6580
    },
    {
        "loss": 3.1325,
        "grad_norm": 2.3335416316986084,
        "learning_rate": 0.00015558645544103337,
        "epoch": 0.8774666666666666,
        "step": 6581
    },
    {
        "loss": 2.8093,
        "grad_norm": 2.4605066776275635,
        "learning_rate": 0.00015553412682416378,
        "epoch": 0.8776,
        "step": 6582
    },
    {
        "loss": 2.6391,
        "grad_norm": 3.829707384109497,
        "learning_rate": 0.0001554817762100373,
        "epoch": 0.8777333333333334,
        "step": 6583
    },
    {
        "loss": 2.7659,
        "grad_norm": 3.1302080154418945,
        "learning_rate": 0.0001554294036193903,
        "epoch": 0.8778666666666667,
        "step": 6584
    },
    {
        "loss": 2.6846,
        "grad_norm": 3.0773630142211914,
        "learning_rate": 0.0001553770090729676,
        "epoch": 0.878,
        "step": 6585
    },
    {
        "loss": 2.1147,
        "grad_norm": 3.8067774772644043,
        "learning_rate": 0.00015532459259152292,
        "epoch": 0.8781333333333333,
        "step": 6586
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.1435306072235107,
        "learning_rate": 0.00015527215419581863,
        "epoch": 0.8782666666666666,
        "step": 6587
    },
    {
        "loss": 1.9335,
        "grad_norm": 3.4652628898620605,
        "learning_rate": 0.0001552196939066257,
        "epoch": 0.8784,
        "step": 6588
    },
    {
        "loss": 0.8039,
        "grad_norm": 3.244626760482788,
        "learning_rate": 0.00015516721174472391,
        "epoch": 0.8785333333333334,
        "step": 6589
    },
    {
        "loss": 2.2056,
        "grad_norm": 3.0901906490325928,
        "learning_rate": 0.0001551147077309015,
        "epoch": 0.8786666666666667,
        "step": 6590
    },
    {
        "loss": 1.3337,
        "grad_norm": 3.2794792652130127,
        "learning_rate": 0.00015506218188595562,
        "epoch": 0.8788,
        "step": 6591
    },
    {
        "loss": 2.7839,
        "grad_norm": 2.613151788711548,
        "learning_rate": 0.00015500963423069173,
        "epoch": 0.8789333333333333,
        "step": 6592
    },
    {
        "loss": 1.856,
        "grad_norm": 3.3489179611206055,
        "learning_rate": 0.00015495706478592446,
        "epoch": 0.8790666666666667,
        "step": 6593
    },
    {
        "loss": 1.8636,
        "grad_norm": 4.273003578186035,
        "learning_rate": 0.00015490447357247646,
        "epoch": 0.8792,
        "step": 6594
    },
    {
        "loss": 2.246,
        "grad_norm": 2.700732707977295,
        "learning_rate": 0.00015485186061117947,
        "epoch": 0.8793333333333333,
        "step": 6595
    },
    {
        "loss": 2.2515,
        "grad_norm": 3.897245407104492,
        "learning_rate": 0.00015479922592287351,
        "epoch": 0.8794666666666666,
        "step": 6596
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.114504814147949,
        "learning_rate": 0.00015474656952840755,
        "epoch": 0.8796,
        "step": 6597
    },
    {
        "loss": 3.0461,
        "grad_norm": 2.4915075302124023,
        "learning_rate": 0.00015469389144863875,
        "epoch": 0.8797333333333334,
        "step": 6598
    },
    {
        "loss": 2.6443,
        "grad_norm": 3.574108123779297,
        "learning_rate": 0.0001546411917044332,
        "epoch": 0.8798666666666667,
        "step": 6599
    },
    {
        "loss": 2.1022,
        "grad_norm": 4.029551029205322,
        "learning_rate": 0.0001545884703166655,
        "epoch": 0.88,
        "step": 6600
    },
    {
        "loss": 1.9598,
        "grad_norm": 4.550681114196777,
        "learning_rate": 0.00015453572730621856,
        "epoch": 0.8801333333333333,
        "step": 6601
    },
    {
        "loss": 2.0377,
        "grad_norm": 3.5334625244140625,
        "learning_rate": 0.00015448296269398432,
        "epoch": 0.8802666666666666,
        "step": 6602
    },
    {
        "loss": 2.4361,
        "grad_norm": 5.063065052032471,
        "learning_rate": 0.00015443017650086289,
        "epoch": 0.8804,
        "step": 6603
    },
    {
        "loss": 2.4928,
        "grad_norm": 3.6709346771240234,
        "learning_rate": 0.0001543773687477631,
        "epoch": 0.8805333333333333,
        "step": 6604
    },
    {
        "loss": 2.1803,
        "grad_norm": 3.1340830326080322,
        "learning_rate": 0.00015432453945560224,
        "epoch": 0.8806666666666667,
        "step": 6605
    },
    {
        "loss": 2.0591,
        "grad_norm": 2.586369037628174,
        "learning_rate": 0.0001542716886453062,
        "epoch": 0.8808,
        "step": 6606
    },
    {
        "loss": 2.5386,
        "grad_norm": 2.048342704772949,
        "learning_rate": 0.00015421881633780936,
        "epoch": 0.8809333333333333,
        "step": 6607
    },
    {
        "loss": 2.7314,
        "grad_norm": 3.0850942134857178,
        "learning_rate": 0.0001541659225540546,
        "epoch": 0.8810666666666667,
        "step": 6608
    },
    {
        "loss": 2.4456,
        "grad_norm": 5.528629779815674,
        "learning_rate": 0.0001541130073149934,
        "epoch": 0.8812,
        "step": 6609
    },
    {
        "loss": 2.5805,
        "grad_norm": 3.498227119445801,
        "learning_rate": 0.00015406007064158557,
        "epoch": 0.8813333333333333,
        "step": 6610
    },
    {
        "loss": 2.1025,
        "grad_norm": 2.9412760734558105,
        "learning_rate": 0.00015400711255479955,
        "epoch": 0.8814666666666666,
        "step": 6611
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.8283660411834717,
        "learning_rate": 0.00015395413307561228,
        "epoch": 0.8816,
        "step": 6612
    },
    {
        "loss": 2.2599,
        "grad_norm": 2.1489510536193848,
        "learning_rate": 0.00015390113222500898,
        "epoch": 0.8817333333333334,
        "step": 6613
    },
    {
        "loss": 2.3378,
        "grad_norm": 4.621570587158203,
        "learning_rate": 0.00015384811002398363,
        "epoch": 0.8818666666666667,
        "step": 6614
    },
    {
        "loss": 0.8131,
        "grad_norm": 3.3240573406219482,
        "learning_rate": 0.00015379506649353838,
        "epoch": 0.882,
        "step": 6615
    },
    {
        "loss": 2.7497,
        "grad_norm": 3.0451395511627197,
        "learning_rate": 0.000153742001654684,
        "epoch": 0.8821333333333333,
        "step": 6616
    },
    {
        "loss": 0.8688,
        "grad_norm": 3.5728352069854736,
        "learning_rate": 0.00015368891552843951,
        "epoch": 0.8822666666666666,
        "step": 6617
    },
    {
        "loss": 2.8537,
        "grad_norm": 2.4899888038635254,
        "learning_rate": 0.0001536358081358328,
        "epoch": 0.8824,
        "step": 6618
    },
    {
        "loss": 2.2016,
        "grad_norm": 3.993757486343384,
        "learning_rate": 0.00015358267949789966,
        "epoch": 0.8825333333333333,
        "step": 6619
    },
    {
        "loss": 2.479,
        "grad_norm": 3.1819565296173096,
        "learning_rate": 0.00015352952963568463,
        "epoch": 0.8826666666666667,
        "step": 6620
    },
    {
        "loss": 2.6808,
        "grad_norm": 2.8491127490997314,
        "learning_rate": 0.00015347635857024047,
        "epoch": 0.8828,
        "step": 6621
    },
    {
        "loss": 2.4609,
        "grad_norm": 4.163424968719482,
        "learning_rate": 0.00015342316632262847,
        "epoch": 0.8829333333333333,
        "step": 6622
    },
    {
        "loss": 3.3562,
        "grad_norm": 3.7907278537750244,
        "learning_rate": 0.0001533699529139183,
        "epoch": 0.8830666666666667,
        "step": 6623
    },
    {
        "loss": 2.5899,
        "grad_norm": 3.9588091373443604,
        "learning_rate": 0.00015331671836518789,
        "epoch": 0.8832,
        "step": 6624
    },
    {
        "loss": 1.9507,
        "grad_norm": 2.778507947921753,
        "learning_rate": 0.00015326346269752371,
        "epoch": 0.8833333333333333,
        "step": 6625
    },
    {
        "loss": 2.6074,
        "grad_norm": 2.3819544315338135,
        "learning_rate": 0.00015321018593202036,
        "epoch": 0.8834666666666666,
        "step": 6626
    },
    {
        "loss": 2.8435,
        "grad_norm": 2.102658748626709,
        "learning_rate": 0.00015315688808978115,
        "epoch": 0.8836,
        "step": 6627
    },
    {
        "loss": 2.9785,
        "grad_norm": 5.317220211029053,
        "learning_rate": 0.0001531035691919174,
        "epoch": 0.8837333333333334,
        "step": 6628
    },
    {
        "loss": 2.8075,
        "grad_norm": 2.094275951385498,
        "learning_rate": 0.000153050229259549,
        "epoch": 0.8838666666666667,
        "step": 6629
    },
    {
        "loss": 3.1895,
        "grad_norm": 3.2279512882232666,
        "learning_rate": 0.00015299686831380394,
        "epoch": 0.884,
        "step": 6630
    },
    {
        "loss": 2.3097,
        "grad_norm": 3.8874411582946777,
        "learning_rate": 0.00015294348637581883,
        "epoch": 0.8841333333333333,
        "step": 6631
    },
    {
        "loss": 2.7226,
        "grad_norm": 3.5774290561676025,
        "learning_rate": 0.00015289008346673833,
        "epoch": 0.8842666666666666,
        "step": 6632
    },
    {
        "loss": 2.6629,
        "grad_norm": 2.758979082107544,
        "learning_rate": 0.00015283665960771553,
        "epoch": 0.8844,
        "step": 6633
    },
    {
        "loss": 2.9522,
        "grad_norm": 2.636319398880005,
        "learning_rate": 0.0001527832148199119,
        "epoch": 0.8845333333333333,
        "step": 6634
    },
    {
        "loss": 2.3135,
        "grad_norm": 3.6591076850891113,
        "learning_rate": 0.00015272974912449697,
        "epoch": 0.8846666666666667,
        "step": 6635
    },
    {
        "loss": 2.1083,
        "grad_norm": 3.6345670223236084,
        "learning_rate": 0.00015267626254264873,
        "epoch": 0.8848,
        "step": 6636
    },
    {
        "loss": 2.3789,
        "grad_norm": 2.808478355407715,
        "learning_rate": 0.00015262275509555346,
        "epoch": 0.8849333333333333,
        "step": 6637
    },
    {
        "loss": 2.3867,
        "grad_norm": 2.4597878456115723,
        "learning_rate": 0.00015256922680440552,
        "epoch": 0.8850666666666667,
        "step": 6638
    },
    {
        "loss": 3.1285,
        "grad_norm": 3.34804105758667,
        "learning_rate": 0.00015251567769040776,
        "epoch": 0.8852,
        "step": 6639
    },
    {
        "loss": 2.0193,
        "grad_norm": 2.673419237136841,
        "learning_rate": 0.00015246210777477111,
        "epoch": 0.8853333333333333,
        "step": 6640
    },
    {
        "loss": 0.7067,
        "grad_norm": 2.948556423187256,
        "learning_rate": 0.0001524085170787148,
        "epoch": 0.8854666666666666,
        "step": 6641
    },
    {
        "loss": 2.4927,
        "grad_norm": 3.7532951831817627,
        "learning_rate": 0.00015235490562346627,
        "epoch": 0.8856,
        "step": 6642
    },
    {
        "loss": 2.2103,
        "grad_norm": 2.746814489364624,
        "learning_rate": 0.0001523012734302613,
        "epoch": 0.8857333333333334,
        "step": 6643
    },
    {
        "loss": 2.8733,
        "grad_norm": 2.4753787517547607,
        "learning_rate": 0.00015224762052034368,
        "epoch": 0.8858666666666667,
        "step": 6644
    },
    {
        "loss": 2.7429,
        "grad_norm": 1.956113576889038,
        "learning_rate": 0.0001521939469149655,
        "epoch": 0.886,
        "step": 6645
    },
    {
        "loss": 0.9874,
        "grad_norm": 4.01298713684082,
        "learning_rate": 0.0001521402526353872,
        "epoch": 0.8861333333333333,
        "step": 6646
    },
    {
        "loss": 2.7782,
        "grad_norm": 2.6854100227355957,
        "learning_rate": 0.0001520865377028771,
        "epoch": 0.8862666666666666,
        "step": 6647
    },
    {
        "loss": 2.4736,
        "grad_norm": 3.3744020462036133,
        "learning_rate": 0.00015203280213871198,
        "epoch": 0.8864,
        "step": 6648
    },
    {
        "loss": 2.2506,
        "grad_norm": 2.139890193939209,
        "learning_rate": 0.00015197904596417656,
        "epoch": 0.8865333333333333,
        "step": 6649
    },
    {
        "loss": 2.5389,
        "grad_norm": 3.7538037300109863,
        "learning_rate": 0.00015192526920056401,
        "epoch": 0.8866666666666667,
        "step": 6650
    },
    {
        "loss": 2.897,
        "grad_norm": 2.2667765617370605,
        "learning_rate": 0.0001518714718691753,
        "epoch": 0.8868,
        "step": 6651
    },
    {
        "loss": 2.5742,
        "grad_norm": 2.62450909614563,
        "learning_rate": 0.0001518176539913199,
        "epoch": 0.8869333333333334,
        "step": 6652
    },
    {
        "loss": 0.8823,
        "grad_norm": 3.999690532684326,
        "learning_rate": 0.00015176381558831516,
        "epoch": 0.8870666666666667,
        "step": 6653
    },
    {
        "loss": 2.5085,
        "grad_norm": 2.179198980331421,
        "learning_rate": 0.00015170995668148672,
        "epoch": 0.8872,
        "step": 6654
    },
    {
        "loss": 2.116,
        "grad_norm": 3.9516546726226807,
        "learning_rate": 0.00015165607729216823,
        "epoch": 0.8873333333333333,
        "step": 6655
    },
    {
        "loss": 2.1996,
        "grad_norm": 4.031660079956055,
        "learning_rate": 0.0001516021774417015,
        "epoch": 0.8874666666666666,
        "step": 6656
    },
    {
        "loss": 2.6134,
        "grad_norm": 2.6288504600524902,
        "learning_rate": 0.00015154825715143642,
        "epoch": 0.8876,
        "step": 6657
    },
    {
        "loss": 1.9943,
        "grad_norm": 3.487535238265991,
        "learning_rate": 0.00015149431644273108,
        "epoch": 0.8877333333333334,
        "step": 6658
    },
    {
        "loss": 1.8981,
        "grad_norm": 3.741521120071411,
        "learning_rate": 0.00015144035533695157,
        "epoch": 0.8878666666666667,
        "step": 6659
    },
    {
        "loss": 2.7474,
        "grad_norm": 3.1504321098327637,
        "learning_rate": 0.00015138637385547196,
        "epoch": 0.888,
        "step": 6660
    },
    {
        "loss": 2.9789,
        "grad_norm": 3.1621923446655273,
        "learning_rate": 0.0001513323720196746,
        "epoch": 0.8881333333333333,
        "step": 6661
    },
    {
        "loss": 3.4281,
        "grad_norm": 4.5191473960876465,
        "learning_rate": 0.00015127834985094975,
        "epoch": 0.8882666666666666,
        "step": 6662
    },
    {
        "loss": 2.0069,
        "grad_norm": 3.1851794719696045,
        "learning_rate": 0.00015122430737069583,
        "epoch": 0.8884,
        "step": 6663
    },
    {
        "loss": 1.6163,
        "grad_norm": 5.16875696182251,
        "learning_rate": 0.0001511702446003192,
        "epoch": 0.8885333333333333,
        "step": 6664
    },
    {
        "loss": 2.4944,
        "grad_norm": 4.087806224822998,
        "learning_rate": 0.00015111616156123438,
        "epoch": 0.8886666666666667,
        "step": 6665
    },
    {
        "loss": 1.623,
        "grad_norm": 2.754183053970337,
        "learning_rate": 0.00015106205827486373,
        "epoch": 0.8888,
        "step": 6666
    },
    {
        "loss": 2.3855,
        "grad_norm": 2.925292730331421,
        "learning_rate": 0.00015100793476263778,
        "epoch": 0.8889333333333334,
        "step": 6667
    },
    {
        "loss": 3.2163,
        "grad_norm": 3.7751526832580566,
        "learning_rate": 0.00015095379104599515,
        "epoch": 0.8890666666666667,
        "step": 6668
    },
    {
        "loss": 2.8245,
        "grad_norm": 3.939068555831909,
        "learning_rate": 0.00015089962714638218,
        "epoch": 0.8892,
        "step": 6669
    },
    {
        "loss": 1.1057,
        "grad_norm": 2.6916072368621826,
        "learning_rate": 0.0001508454430852535,
        "epoch": 0.8893333333333333,
        "step": 6670
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.987640142440796,
        "learning_rate": 0.0001507912388840716,
        "epoch": 0.8894666666666666,
        "step": 6671
    },
    {
        "loss": 2.291,
        "grad_norm": 4.275875091552734,
        "learning_rate": 0.00015073701456430686,
        "epoch": 0.8896,
        "step": 6672
    },
    {
        "loss": 2.7065,
        "grad_norm": 2.40962815284729,
        "learning_rate": 0.00015068277014743784,
        "epoch": 0.8897333333333334,
        "step": 6673
    },
    {
        "loss": 2.6823,
        "grad_norm": 4.191564083099365,
        "learning_rate": 0.00015062850565495078,
        "epoch": 0.8898666666666667,
        "step": 6674
    },
    {
        "loss": 2.7132,
        "grad_norm": 3.231844186782837,
        "learning_rate": 0.0001505742211083402,
        "epoch": 0.89,
        "step": 6675
    },
    {
        "loss": 1.0896,
        "grad_norm": 3.0936224460601807,
        "learning_rate": 0.0001505199165291082,
        "epoch": 0.8901333333333333,
        "step": 6676
    },
    {
        "loss": 1.6695,
        "grad_norm": 4.477710247039795,
        "learning_rate": 0.0001504655919387652,
        "epoch": 0.8902666666666667,
        "step": 6677
    },
    {
        "loss": 2.6278,
        "grad_norm": 2.9403257369995117,
        "learning_rate": 0.00015041124735882923,
        "epoch": 0.8904,
        "step": 6678
    },
    {
        "loss": 1.8287,
        "grad_norm": 3.568521499633789,
        "learning_rate": 0.00015035688281082648,
        "epoch": 0.8905333333333333,
        "step": 6679
    },
    {
        "loss": 2.9501,
        "grad_norm": 5.706362247467041,
        "learning_rate": 0.0001503024983162908,
        "epoch": 0.8906666666666667,
        "step": 6680
    },
    {
        "loss": 2.3747,
        "grad_norm": 2.504953384399414,
        "learning_rate": 0.00015024809389676419,
        "epoch": 0.8908,
        "step": 6681
    },
    {
        "loss": 2.7328,
        "grad_norm": 2.7095448970794678,
        "learning_rate": 0.00015019366957379633,
        "epoch": 0.8909333333333334,
        "step": 6682
    },
    {
        "loss": 2.3275,
        "grad_norm": 3.702409505844116,
        "learning_rate": 0.00015013922536894492,
        "epoch": 0.8910666666666667,
        "step": 6683
    },
    {
        "loss": 2.2946,
        "grad_norm": 3.773834228515625,
        "learning_rate": 0.00015008476130377554,
        "epoch": 0.8912,
        "step": 6684
    },
    {
        "loss": 1.8907,
        "grad_norm": 2.6424272060394287,
        "learning_rate": 0.0001500302773998614,
        "epoch": 0.8913333333333333,
        "step": 6685
    },
    {
        "loss": 2.5291,
        "grad_norm": 3.6179888248443604,
        "learning_rate": 0.00014997577367878407,
        "epoch": 0.8914666666666666,
        "step": 6686
    },
    {
        "loss": 1.7485,
        "grad_norm": 3.6434521675109863,
        "learning_rate": 0.00014992125016213243,
        "epoch": 0.8916,
        "step": 6687
    },
    {
        "loss": 2.2575,
        "grad_norm": 3.5890583992004395,
        "learning_rate": 0.00014986670687150354,
        "epoch": 0.8917333333333334,
        "step": 6688
    },
    {
        "loss": 2.6073,
        "grad_norm": 3.096245527267456,
        "learning_rate": 0.0001498121438285021,
        "epoch": 0.8918666666666667,
        "step": 6689
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.8080596923828125,
        "learning_rate": 0.00014975756105474082,
        "epoch": 0.892,
        "step": 6690
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.543004035949707,
        "learning_rate": 0.00014970295857184,
        "epoch": 0.8921333333333333,
        "step": 6691
    },
    {
        "loss": 2.7059,
        "grad_norm": 2.0740067958831787,
        "learning_rate": 0.00014964833640142793,
        "epoch": 0.8922666666666667,
        "step": 6692
    },
    {
        "loss": 2.7268,
        "grad_norm": 3.820037603378296,
        "learning_rate": 0.0001495936945651407,
        "epoch": 0.8924,
        "step": 6693
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.3629674911499023,
        "learning_rate": 0.000149539033084622,
        "epoch": 0.8925333333333333,
        "step": 6694
    },
    {
        "loss": 3.0579,
        "grad_norm": 2.221245050430298,
        "learning_rate": 0.00014948435198152348,
        "epoch": 0.8926666666666667,
        "step": 6695
    },
    {
        "loss": 1.9749,
        "grad_norm": 4.245752811431885,
        "learning_rate": 0.0001494296512775046,
        "epoch": 0.8928,
        "step": 6696
    },
    {
        "loss": 2.5473,
        "grad_norm": 4.048515319824219,
        "learning_rate": 0.0001493749309942324,
        "epoch": 0.8929333333333334,
        "step": 6697
    },
    {
        "loss": 2.2687,
        "grad_norm": 4.214646816253662,
        "learning_rate": 0.0001493201911533818,
        "epoch": 0.8930666666666667,
        "step": 6698
    },
    {
        "loss": 1.3694,
        "grad_norm": 3.942601203918457,
        "learning_rate": 0.00014926543177663538,
        "epoch": 0.8932,
        "step": 6699
    },
    {
        "loss": 2.5001,
        "grad_norm": 3.30145263671875,
        "learning_rate": 0.00014921065288568363,
        "epoch": 0.8933333333333333,
        "step": 6700
    },
    {
        "loss": 2.9868,
        "grad_norm": 5.831491470336914,
        "learning_rate": 0.0001491558545022245,
        "epoch": 0.8934666666666666,
        "step": 6701
    },
    {
        "loss": 2.3652,
        "grad_norm": 2.7608139514923096,
        "learning_rate": 0.000149101036647964,
        "epoch": 0.8936,
        "step": 6702
    },
    {
        "loss": 2.4711,
        "grad_norm": 3.379427433013916,
        "learning_rate": 0.0001490461993446155,
        "epoch": 0.8937333333333334,
        "step": 6703
    },
    {
        "loss": 2.0356,
        "grad_norm": 3.2391433715820312,
        "learning_rate": 0.00014899134261390034,
        "epoch": 0.8938666666666667,
        "step": 6704
    },
    {
        "loss": 2.7887,
        "grad_norm": 2.742797613143921,
        "learning_rate": 0.0001489364664775475,
        "epoch": 0.894,
        "step": 6705
    },
    {
        "loss": 2.468,
        "grad_norm": 4.890165328979492,
        "learning_rate": 0.00014888157095729345,
        "epoch": 0.8941333333333333,
        "step": 6706
    },
    {
        "loss": 2.9165,
        "grad_norm": 3.0842301845550537,
        "learning_rate": 0.0001488266560748827,
        "epoch": 0.8942666666666667,
        "step": 6707
    },
    {
        "loss": 1.0225,
        "grad_norm": 2.9827184677124023,
        "learning_rate": 0.00014877172185206701,
        "epoch": 0.8944,
        "step": 6708
    },
    {
        "loss": 2.5013,
        "grad_norm": 3.064131021499634,
        "learning_rate": 0.00014871676831060618,
        "epoch": 0.8945333333333333,
        "step": 6709
    },
    {
        "loss": 2.273,
        "grad_norm": 4.4245147705078125,
        "learning_rate": 0.00014866179547226726,
        "epoch": 0.8946666666666667,
        "step": 6710
    },
    {
        "loss": 2.3377,
        "grad_norm": 3.301424503326416,
        "learning_rate": 0.0001486068033588255,
        "epoch": 0.8948,
        "step": 6711
    },
    {
        "loss": 2.525,
        "grad_norm": 4.038188934326172,
        "learning_rate": 0.00014855179199206326,
        "epoch": 0.8949333333333334,
        "step": 6712
    },
    {
        "loss": 2.6349,
        "grad_norm": 3.117443323135376,
        "learning_rate": 0.00014849676139377084,
        "epoch": 0.8950666666666667,
        "step": 6713
    },
    {
        "loss": 1.656,
        "grad_norm": 4.335290431976318,
        "learning_rate": 0.0001484417115857459,
        "epoch": 0.8952,
        "step": 6714
    },
    {
        "loss": 1.9224,
        "grad_norm": 2.9026567935943604,
        "learning_rate": 0.00014838664258979398,
        "epoch": 0.8953333333333333,
        "step": 6715
    },
    {
        "loss": 2.6451,
        "grad_norm": 2.6917977333068848,
        "learning_rate": 0.00014833155442772804,
        "epoch": 0.8954666666666666,
        "step": 6716
    },
    {
        "loss": 2.6507,
        "grad_norm": 3.2865495681762695,
        "learning_rate": 0.00014827644712136866,
        "epoch": 0.8956,
        "step": 6717
    },
    {
        "loss": 1.5708,
        "grad_norm": 3.543724298477173,
        "learning_rate": 0.00014822132069254416,
        "epoch": 0.8957333333333334,
        "step": 6718
    },
    {
        "loss": 1.8282,
        "grad_norm": 3.960495948791504,
        "learning_rate": 0.0001481661751630902,
        "epoch": 0.8958666666666667,
        "step": 6719
    },
    {
        "loss": 2.5719,
        "grad_norm": 5.069400787353516,
        "learning_rate": 0.00014811101055485008,
        "epoch": 0.896,
        "step": 6720
    },
    {
        "loss": 1.8188,
        "grad_norm": 3.951026678085327,
        "learning_rate": 0.00014805582688967483,
        "epoch": 0.8961333333333333,
        "step": 6721
    },
    {
        "loss": 2.6661,
        "grad_norm": 3.1939287185668945,
        "learning_rate": 0.00014800062418942274,
        "epoch": 0.8962666666666667,
        "step": 6722
    },
    {
        "loss": 2.0035,
        "grad_norm": 3.5566554069519043,
        "learning_rate": 0.00014794540247595986,
        "epoch": 0.8964,
        "step": 6723
    },
    {
        "loss": 2.7649,
        "grad_norm": 2.471153974533081,
        "learning_rate": 0.00014789016177115972,
        "epoch": 0.8965333333333333,
        "step": 6724
    },
    {
        "loss": 2.9229,
        "grad_norm": 3.0054409503936768,
        "learning_rate": 0.0001478349020969033,
        "epoch": 0.8966666666666666,
        "step": 6725
    },
    {
        "loss": 2.7704,
        "grad_norm": 3.3706438541412354,
        "learning_rate": 0.00014777962347507914,
        "epoch": 0.8968,
        "step": 6726
    },
    {
        "loss": 2.863,
        "grad_norm": 2.1571743488311768,
        "learning_rate": 0.00014772432592758338,
        "epoch": 0.8969333333333334,
        "step": 6727
    },
    {
        "loss": 1.3454,
        "grad_norm": 3.1665472984313965,
        "learning_rate": 0.00014766900947631943,
        "epoch": 0.8970666666666667,
        "step": 6728
    },
    {
        "loss": 1.1754,
        "grad_norm": 4.722635269165039,
        "learning_rate": 0.00014761367414319843,
        "epoch": 0.8972,
        "step": 6729
    },
    {
        "loss": 2.805,
        "grad_norm": 3.2774016857147217,
        "learning_rate": 0.00014755831995013892,
        "epoch": 0.8973333333333333,
        "step": 6730
    },
    {
        "loss": 2.3295,
        "grad_norm": 3.1714367866516113,
        "learning_rate": 0.00014750294691906674,
        "epoch": 0.8974666666666666,
        "step": 6731
    },
    {
        "loss": 2.32,
        "grad_norm": 2.919276475906372,
        "learning_rate": 0.0001474475550719155,
        "epoch": 0.8976,
        "step": 6732
    },
    {
        "loss": 3.255,
        "grad_norm": 2.986421585083008,
        "learning_rate": 0.00014739214443062598,
        "epoch": 0.8977333333333334,
        "step": 6733
    },
    {
        "loss": 1.2978,
        "grad_norm": 6.227573394775391,
        "learning_rate": 0.0001473367150171466,
        "epoch": 0.8978666666666667,
        "step": 6734
    },
    {
        "loss": 2.7688,
        "grad_norm": 2.6092545986175537,
        "learning_rate": 0.000147281266853433,
        "epoch": 0.898,
        "step": 6735
    },
    {
        "loss": 1.5067,
        "grad_norm": 4.137331008911133,
        "learning_rate": 0.00014722579996144858,
        "epoch": 0.8981333333333333,
        "step": 6736
    },
    {
        "loss": 2.6575,
        "grad_norm": 3.570871353149414,
        "learning_rate": 0.00014717031436316385,
        "epoch": 0.8982666666666667,
        "step": 6737
    },
    {
        "loss": 2.598,
        "grad_norm": 2.8509597778320312,
        "learning_rate": 0.00014711481008055692,
        "epoch": 0.8984,
        "step": 6738
    },
    {
        "loss": 2.1455,
        "grad_norm": 4.914891719818115,
        "learning_rate": 0.00014705928713561309,
        "epoch": 0.8985333333333333,
        "step": 6739
    },
    {
        "loss": 2.2727,
        "grad_norm": 2.6562998294830322,
        "learning_rate": 0.00014700374555032538,
        "epoch": 0.8986666666666666,
        "step": 6740
    },
    {
        "loss": 2.5499,
        "grad_norm": 2.901456832885742,
        "learning_rate": 0.00014694818534669385,
        "epoch": 0.8988,
        "step": 6741
    },
    {
        "loss": 1.5044,
        "grad_norm": 5.129391193389893,
        "learning_rate": 0.00014689260654672607,
        "epoch": 0.8989333333333334,
        "step": 6742
    },
    {
        "loss": 0.9302,
        "grad_norm": 2.8206140995025635,
        "learning_rate": 0.00014683700917243715,
        "epoch": 0.8990666666666667,
        "step": 6743
    },
    {
        "loss": 2.8265,
        "grad_norm": 3.2540247440338135,
        "learning_rate": 0.00014678139324584918,
        "epoch": 0.8992,
        "step": 6744
    },
    {
        "loss": 2.9935,
        "grad_norm": 3.0140933990478516,
        "learning_rate": 0.0001467257587889921,
        "epoch": 0.8993333333333333,
        "step": 6745
    },
    {
        "loss": 2.0388,
        "grad_norm": 2.9559147357940674,
        "learning_rate": 0.00014667010582390262,
        "epoch": 0.8994666666666666,
        "step": 6746
    },
    {
        "loss": 2.9728,
        "grad_norm": 2.0332837104797363,
        "learning_rate": 0.0001466144343726253,
        "epoch": 0.8996,
        "step": 6747
    },
    {
        "loss": 2.3792,
        "grad_norm": 3.5856199264526367,
        "learning_rate": 0.00014655874445721162,
        "epoch": 0.8997333333333334,
        "step": 6748
    },
    {
        "loss": 2.7144,
        "grad_norm": 2.4406065940856934,
        "learning_rate": 0.00014650303609972068,
        "epoch": 0.8998666666666667,
        "step": 6749
    },
    {
        "loss": 1.9921,
        "grad_norm": 3.6619296073913574,
        "learning_rate": 0.00014644730932221863,
        "epoch": 0.9,
        "step": 6750
    },
    {
        "loss": 2.9849,
        "grad_norm": 2.358191967010498,
        "learning_rate": 0.00014639156414677907,
        "epoch": 0.9001333333333333,
        "step": 6751
    },
    {
        "loss": 2.3727,
        "grad_norm": 3.131215810775757,
        "learning_rate": 0.00014633580059548296,
        "epoch": 0.9002666666666667,
        "step": 6752
    },
    {
        "loss": 2.9085,
        "grad_norm": 2.946758508682251,
        "learning_rate": 0.00014628001869041826,
        "epoch": 0.9004,
        "step": 6753
    },
    {
        "loss": 2.3324,
        "grad_norm": 2.736335039138794,
        "learning_rate": 0.0001462242184536805,
        "epoch": 0.9005333333333333,
        "step": 6754
    },
    {
        "loss": 2.9105,
        "grad_norm": 2.619338035583496,
        "learning_rate": 0.00014616839990737234,
        "epoch": 0.9006666666666666,
        "step": 6755
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.978513240814209,
        "learning_rate": 0.0001461125630736036,
        "epoch": 0.9008,
        "step": 6756
    },
    {
        "loss": 2.4401,
        "grad_norm": 3.949124336242676,
        "learning_rate": 0.00014605670797449157,
        "epoch": 0.9009333333333334,
        "step": 6757
    },
    {
        "loss": 2.5883,
        "grad_norm": 3.8861708641052246,
        "learning_rate": 0.00014600083463216053,
        "epoch": 0.9010666666666667,
        "step": 6758
    },
    {
        "loss": 2.4339,
        "grad_norm": 3.465984344482422,
        "learning_rate": 0.0001459449430687422,
        "epoch": 0.9012,
        "step": 6759
    },
    {
        "loss": 2.3751,
        "grad_norm": 3.073198080062866,
        "learning_rate": 0.00014588903330637522,
        "epoch": 0.9013333333333333,
        "step": 6760
    },
    {
        "loss": 2.7414,
        "grad_norm": 2.7453184127807617,
        "learning_rate": 0.00014583310536720592,
        "epoch": 0.9014666666666666,
        "step": 6761
    },
    {
        "loss": 2.4377,
        "grad_norm": 6.617713451385498,
        "learning_rate": 0.00014577715927338738,
        "epoch": 0.9016,
        "step": 6762
    },
    {
        "loss": 1.787,
        "grad_norm": 3.5948686599731445,
        "learning_rate": 0.00014572119504708005,
        "epoch": 0.9017333333333334,
        "step": 6763
    },
    {
        "loss": 2.823,
        "grad_norm": 3.2678751945495605,
        "learning_rate": 0.0001456652127104516,
        "epoch": 0.9018666666666667,
        "step": 6764
    },
    {
        "loss": 2.4171,
        "grad_norm": 2.071087121963501,
        "learning_rate": 0.0001456092122856768,
        "epoch": 0.902,
        "step": 6765
    },
    {
        "loss": 2.3901,
        "grad_norm": 3.062117576599121,
        "learning_rate": 0.00014555319379493762,
        "epoch": 0.9021333333333333,
        "step": 6766
    },
    {
        "loss": 2.5276,
        "grad_norm": 3.329362630844116,
        "learning_rate": 0.0001454971572604231,
        "epoch": 0.9022666666666667,
        "step": 6767
    },
    {
        "loss": 3.0639,
        "grad_norm": 4.12092924118042,
        "learning_rate": 0.0001454411027043296,
        "epoch": 0.9024,
        "step": 6768
    },
    {
        "loss": 3.0465,
        "grad_norm": 4.202397346496582,
        "learning_rate": 0.00014538503014886042,
        "epoch": 0.9025333333333333,
        "step": 6769
    },
    {
        "loss": 1.8269,
        "grad_norm": 3.3727524280548096,
        "learning_rate": 0.00014532893961622626,
        "epoch": 0.9026666666666666,
        "step": 6770
    },
    {
        "loss": 3.0934,
        "grad_norm": 3.009784698486328,
        "learning_rate": 0.00014527283112864456,
        "epoch": 0.9028,
        "step": 6771
    },
    {
        "loss": 2.0231,
        "grad_norm": 4.01632022857666,
        "learning_rate": 0.00014521670470834028,
        "epoch": 0.9029333333333334,
        "step": 6772
    },
    {
        "loss": 2.1995,
        "grad_norm": 4.138615608215332,
        "learning_rate": 0.00014516056037754512,
        "epoch": 0.9030666666666667,
        "step": 6773
    },
    {
        "loss": 1.8027,
        "grad_norm": 3.736647129058838,
        "learning_rate": 0.00014510439815849818,
        "epoch": 0.9032,
        "step": 6774
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.2392079830169678,
        "learning_rate": 0.00014504821807344537,
        "epoch": 0.9033333333333333,
        "step": 6775
    },
    {
        "loss": 1.7045,
        "grad_norm": 4.597736835479736,
        "learning_rate": 0.0001449920201446399,
        "epoch": 0.9034666666666666,
        "step": 6776
    },
    {
        "loss": 2.0796,
        "grad_norm": 3.7830450534820557,
        "learning_rate": 0.000144935804394342,
        "epoch": 0.9036,
        "step": 6777
    },
    {
        "loss": 1.9588,
        "grad_norm": 4.539167404174805,
        "learning_rate": 0.00014487957084481878,
        "epoch": 0.9037333333333334,
        "step": 6778
    },
    {
        "loss": 0.7677,
        "grad_norm": 4.984569072723389,
        "learning_rate": 0.00014482331951834465,
        "epoch": 0.9038666666666667,
        "step": 6779
    },
    {
        "loss": 1.2307,
        "grad_norm": 3.5693302154541016,
        "learning_rate": 0.000144767050437201,
        "epoch": 0.904,
        "step": 6780
    },
    {
        "loss": 3.1746,
        "grad_norm": 2.7271294593811035,
        "learning_rate": 0.00014471076362367605,
        "epoch": 0.9041333333333333,
        "step": 6781
    },
    {
        "loss": 2.6402,
        "grad_norm": 3.715808153152466,
        "learning_rate": 0.00014465445910006528,
        "epoch": 0.9042666666666667,
        "step": 6782
    },
    {
        "loss": 1.6914,
        "grad_norm": 6.808281898498535,
        "learning_rate": 0.00014459813688867118,
        "epoch": 0.9044,
        "step": 6783
    },
    {
        "loss": 2.529,
        "grad_norm": 3.508100748062134,
        "learning_rate": 0.000144541797011803,
        "epoch": 0.9045333333333333,
        "step": 6784
    },
    {
        "loss": 2.0411,
        "grad_norm": 3.180974006652832,
        "learning_rate": 0.00014448543949177728,
        "epoch": 0.9046666666666666,
        "step": 6785
    },
    {
        "loss": 1.9344,
        "grad_norm": 3.7599945068359375,
        "learning_rate": 0.00014442906435091745,
        "epoch": 0.9048,
        "step": 6786
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.9921422004699707,
        "learning_rate": 0.00014437267161155378,
        "epoch": 0.9049333333333334,
        "step": 6787
    },
    {
        "loss": 1.1608,
        "grad_norm": 4.598356246948242,
        "learning_rate": 0.0001443162612960237,
        "epoch": 0.9050666666666667,
        "step": 6788
    },
    {
        "loss": 1.25,
        "grad_norm": 11.757116317749023,
        "learning_rate": 0.00014425983342667155,
        "epoch": 0.9052,
        "step": 6789
    },
    {
        "loss": 2.6222,
        "grad_norm": 3.6638357639312744,
        "learning_rate": 0.00014420338802584854,
        "epoch": 0.9053333333333333,
        "step": 6790
    },
    {
        "loss": 2.5414,
        "grad_norm": 3.18804669380188,
        "learning_rate": 0.00014414692511591296,
        "epoch": 0.9054666666666666,
        "step": 6791
    },
    {
        "loss": 0.6556,
        "grad_norm": 5.197278022766113,
        "learning_rate": 0.00014409044471922988,
        "epoch": 0.9056,
        "step": 6792
    },
    {
        "loss": 2.9586,
        "grad_norm": 2.5561416149139404,
        "learning_rate": 0.0001440339468581714,
        "epoch": 0.9057333333333333,
        "step": 6793
    },
    {
        "loss": 2.3086,
        "grad_norm": 2.8585433959960938,
        "learning_rate": 0.00014397743155511648,
        "epoch": 0.9058666666666667,
        "step": 6794
    },
    {
        "loss": 2.7837,
        "grad_norm": 2.801421642303467,
        "learning_rate": 0.0001439208988324512,
        "epoch": 0.906,
        "step": 6795
    },
    {
        "loss": 2.0986,
        "grad_norm": 4.719674587249756,
        "learning_rate": 0.0001438643487125681,
        "epoch": 0.9061333333333333,
        "step": 6796
    },
    {
        "loss": 2.3794,
        "grad_norm": 2.6276931762695312,
        "learning_rate": 0.00014380778121786714,
        "epoch": 0.9062666666666667,
        "step": 6797
    },
    {
        "loss": 2.765,
        "grad_norm": 3.465463876724243,
        "learning_rate": 0.00014375119637075466,
        "epoch": 0.9064,
        "step": 6798
    },
    {
        "loss": 2.1871,
        "grad_norm": 3.475010633468628,
        "learning_rate": 0.0001436945941936443,
        "epoch": 0.9065333333333333,
        "step": 6799
    },
    {
        "loss": 2.8016,
        "grad_norm": 3.6382193565368652,
        "learning_rate": 0.00014363797470895624,
        "epoch": 0.9066666666666666,
        "step": 6800
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.025371789932251,
        "learning_rate": 0.0001435813379391177,
        "epoch": 0.9068,
        "step": 6801
    },
    {
        "loss": 2.9147,
        "grad_norm": 3.7144970893859863,
        "learning_rate": 0.00014352468390656275,
        "epoch": 0.9069333333333334,
        "step": 6802
    },
    {
        "loss": 1.9702,
        "grad_norm": 2.822416067123413,
        "learning_rate": 0.0001434680126337321,
        "epoch": 0.9070666666666667,
        "step": 6803
    },
    {
        "loss": 2.773,
        "grad_norm": 3.118215560913086,
        "learning_rate": 0.00014341132414307363,
        "epoch": 0.9072,
        "step": 6804
    },
    {
        "loss": 1.9876,
        "grad_norm": 4.03197717666626,
        "learning_rate": 0.0001433546184570417,
        "epoch": 0.9073333333333333,
        "step": 6805
    },
    {
        "loss": 2.0984,
        "grad_norm": 3.684997081756592,
        "learning_rate": 0.00014329789559809774,
        "epoch": 0.9074666666666666,
        "step": 6806
    },
    {
        "loss": 2.3245,
        "grad_norm": 3.8822457790374756,
        "learning_rate": 0.00014324115558870973,
        "epoch": 0.9076,
        "step": 6807
    },
    {
        "loss": 1.9083,
        "grad_norm": 4.780266761779785,
        "learning_rate": 0.0001431843984513527,
        "epoch": 0.9077333333333333,
        "step": 6808
    },
    {
        "loss": 0.5661,
        "grad_norm": 2.974789619445801,
        "learning_rate": 0.00014312762420850826,
        "epoch": 0.9078666666666667,
        "step": 6809
    },
    {
        "loss": 2.5449,
        "grad_norm": 2.5935864448547363,
        "learning_rate": 0.0001430708328826649,
        "epoch": 0.908,
        "step": 6810
    },
    {
        "loss": 2.6031,
        "grad_norm": 2.8878657817840576,
        "learning_rate": 0.00014301402449631793,
        "epoch": 0.9081333333333333,
        "step": 6811
    },
    {
        "loss": 2.597,
        "grad_norm": 2.508998394012451,
        "learning_rate": 0.00014295719907196926,
        "epoch": 0.9082666666666667,
        "step": 6812
    },
    {
        "loss": 2.623,
        "grad_norm": 3.171215534210205,
        "learning_rate": 0.00014290035663212764,
        "epoch": 0.9084,
        "step": 6813
    },
    {
        "loss": 3.1062,
        "grad_norm": 2.5837178230285645,
        "learning_rate": 0.00014284349719930864,
        "epoch": 0.9085333333333333,
        "step": 6814
    },
    {
        "loss": 1.4676,
        "grad_norm": 4.525455951690674,
        "learning_rate": 0.00014278662079603434,
        "epoch": 0.9086666666666666,
        "step": 6815
    },
    {
        "loss": 3.0191,
        "grad_norm": 4.6057963371276855,
        "learning_rate": 0.00014272972744483383,
        "epoch": 0.9088,
        "step": 6816
    },
    {
        "loss": 3.0614,
        "grad_norm": 1.9865186214447021,
        "learning_rate": 0.00014267281716824262,
        "epoch": 0.9089333333333334,
        "step": 6817
    },
    {
        "loss": 2.4324,
        "grad_norm": 3.9369587898254395,
        "learning_rate": 0.0001426158899888032,
        "epoch": 0.9090666666666667,
        "step": 6818
    },
    {
        "loss": 2.3897,
        "grad_norm": 2.9735958576202393,
        "learning_rate": 0.0001425589459290644,
        "epoch": 0.9092,
        "step": 6819
    },
    {
        "loss": 2.5831,
        "grad_norm": 2.4817020893096924,
        "learning_rate": 0.00014250198501158224,
        "epoch": 0.9093333333333333,
        "step": 6820
    },
    {
        "loss": 3.0967,
        "grad_norm": 3.045011520385742,
        "learning_rate": 0.0001424450072589189,
        "epoch": 0.9094666666666666,
        "step": 6821
    },
    {
        "loss": 1.3691,
        "grad_norm": 3.623279571533203,
        "learning_rate": 0.00014238801269364365,
        "epoch": 0.9096,
        "step": 6822
    },
    {
        "loss": 2.9336,
        "grad_norm": 2.9028446674346924,
        "learning_rate": 0.00014233100133833205,
        "epoch": 0.9097333333333333,
        "step": 6823
    },
    {
        "loss": 2.1191,
        "grad_norm": 3.2720155715942383,
        "learning_rate": 0.00014227397321556665,
        "epoch": 0.9098666666666667,
        "step": 6824
    },
    {
        "loss": 2.9121,
        "grad_norm": 2.8629350662231445,
        "learning_rate": 0.00014221692834793642,
        "epoch": 0.91,
        "step": 6825
    },
    {
        "loss": 1.614,
        "grad_norm": 4.617535591125488,
        "learning_rate": 0.00014215986675803694,
        "epoch": 0.9101333333333333,
        "step": 6826
    },
    {
        "loss": 2.5777,
        "grad_norm": 2.64367938041687,
        "learning_rate": 0.00014210278846847068,
        "epoch": 0.9102666666666667,
        "step": 6827
    },
    {
        "loss": 2.2536,
        "grad_norm": 3.0377614498138428,
        "learning_rate": 0.00014204569350184632,
        "epoch": 0.9104,
        "step": 6828
    },
    {
        "loss": 3.0431,
        "grad_norm": 2.346287250518799,
        "learning_rate": 0.0001419885818807796,
        "epoch": 0.9105333333333333,
        "step": 6829
    },
    {
        "loss": 2.3007,
        "grad_norm": 4.505295753479004,
        "learning_rate": 0.00014193145362789247,
        "epoch": 0.9106666666666666,
        "step": 6830
    },
    {
        "loss": 2.6327,
        "grad_norm": 2.7030208110809326,
        "learning_rate": 0.00014187430876581373,
        "epoch": 0.9108,
        "step": 6831
    },
    {
        "loss": 1.7895,
        "grad_norm": 3.9050183296203613,
        "learning_rate": 0.00014181714731717856,
        "epoch": 0.9109333333333334,
        "step": 6832
    },
    {
        "loss": 1.781,
        "grad_norm": 4.05072546005249,
        "learning_rate": 0.00014175996930462892,
        "epoch": 0.9110666666666667,
        "step": 6833
    },
    {
        "loss": 2.0787,
        "grad_norm": 3.2130563259124756,
        "learning_rate": 0.00014170277475081307,
        "epoch": 0.9112,
        "step": 6834
    },
    {
        "loss": 2.4964,
        "grad_norm": 3.0334770679473877,
        "learning_rate": 0.00014164556367838603,
        "epoch": 0.9113333333333333,
        "step": 6835
    },
    {
        "loss": 2.8992,
        "grad_norm": 2.468003034591675,
        "learning_rate": 0.0001415883361100094,
        "epoch": 0.9114666666666666,
        "step": 6836
    },
    {
        "loss": 2.7301,
        "grad_norm": 2.9095041751861572,
        "learning_rate": 0.00014153109206835107,
        "epoch": 0.9116,
        "step": 6837
    },
    {
        "loss": 0.7244,
        "grad_norm": 2.4924304485321045,
        "learning_rate": 0.00014147383157608564,
        "epoch": 0.9117333333333333,
        "step": 6838
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.4970335960388184,
        "learning_rate": 0.00014141655465589427,
        "epoch": 0.9118666666666667,
        "step": 6839
    },
    {
        "loss": 2.8725,
        "grad_norm": 2.5552594661712646,
        "learning_rate": 0.0001413592613304644,
        "epoch": 0.912,
        "step": 6840
    },
    {
        "loss": 2.6799,
        "grad_norm": 2.099419593811035,
        "learning_rate": 0.00014130195162249026,
        "epoch": 0.9121333333333334,
        "step": 6841
    },
    {
        "loss": 2.909,
        "grad_norm": 3.907405376434326,
        "learning_rate": 0.0001412446255546723,
        "epoch": 0.9122666666666667,
        "step": 6842
    },
    {
        "loss": 2.1346,
        "grad_norm": 2.5118184089660645,
        "learning_rate": 0.0001411872831497176,
        "epoch": 0.9124,
        "step": 6843
    },
    {
        "loss": 2.1031,
        "grad_norm": 3.4201819896698,
        "learning_rate": 0.00014112992443033971,
        "epoch": 0.9125333333333333,
        "step": 6844
    },
    {
        "loss": 2.4685,
        "grad_norm": 2.6526615619659424,
        "learning_rate": 0.0001410725494192587,
        "epoch": 0.9126666666666666,
        "step": 6845
    },
    {
        "loss": 2.3293,
        "grad_norm": 3.312539577484131,
        "learning_rate": 0.00014101515813920082,
        "epoch": 0.9128,
        "step": 6846
    },
    {
        "loss": 2.9966,
        "grad_norm": 2.30277419090271,
        "learning_rate": 0.00014095775061289905,
        "epoch": 0.9129333333333334,
        "step": 6847
    },
    {
        "loss": 3.4841,
        "grad_norm": 2.72790789604187,
        "learning_rate": 0.00014090032686309278,
        "epoch": 0.9130666666666667,
        "step": 6848
    },
    {
        "loss": 1.6637,
        "grad_norm": 3.420118808746338,
        "learning_rate": 0.0001408428869125276,
        "epoch": 0.9132,
        "step": 6849
    },
    {
        "loss": 2.3796,
        "grad_norm": 2.4477481842041016,
        "learning_rate": 0.0001407854307839558,
        "epoch": 0.9133333333333333,
        "step": 6850
    },
    {
        "loss": 1.9161,
        "grad_norm": 2.203219413757324,
        "learning_rate": 0.00014072795850013584,
        "epoch": 0.9134666666666666,
        "step": 6851
    },
    {
        "loss": 2.921,
        "grad_norm": 2.404144287109375,
        "learning_rate": 0.00014067047008383277,
        "epoch": 0.9136,
        "step": 6852
    },
    {
        "loss": 2.6391,
        "grad_norm": 4.385261058807373,
        "learning_rate": 0.00014061296555781784,
        "epoch": 0.9137333333333333,
        "step": 6853
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.473297119140625,
        "learning_rate": 0.00014055544494486898,
        "epoch": 0.9138666666666667,
        "step": 6854
    },
    {
        "loss": 1.8877,
        "grad_norm": 3.9648032188415527,
        "learning_rate": 0.0001404979082677701,
        "epoch": 0.914,
        "step": 6855
    },
    {
        "loss": 2.1775,
        "grad_norm": 4.037871360778809,
        "learning_rate": 0.00014044035554931187,
        "epoch": 0.9141333333333334,
        "step": 6856
    },
    {
        "loss": 2.6027,
        "grad_norm": 4.040201663970947,
        "learning_rate": 0.00014038278681229092,
        "epoch": 0.9142666666666667,
        "step": 6857
    },
    {
        "loss": 2.9098,
        "grad_norm": 2.733234167098999,
        "learning_rate": 0.00014032520207951058,
        "epoch": 0.9144,
        "step": 6858
    },
    {
        "loss": 1.781,
        "grad_norm": 6.62331485748291,
        "learning_rate": 0.00014026760137378025,
        "epoch": 0.9145333333333333,
        "step": 6859
    },
    {
        "loss": 2.9428,
        "grad_norm": 1.5414751768112183,
        "learning_rate": 0.00014020998471791577,
        "epoch": 0.9146666666666666,
        "step": 6860
    },
    {
        "loss": 2.6256,
        "grad_norm": 2.8302907943725586,
        "learning_rate": 0.00014015235213473944,
        "epoch": 0.9148,
        "step": 6861
    },
    {
        "loss": 1.4609,
        "grad_norm": 6.049583435058594,
        "learning_rate": 0.0001400947036470795,
        "epoch": 0.9149333333333334,
        "step": 6862
    },
    {
        "loss": 2.0054,
        "grad_norm": 4.284839153289795,
        "learning_rate": 0.00014003703927777084,
        "epoch": 0.9150666666666667,
        "step": 6863
    },
    {
        "loss": 2.5075,
        "grad_norm": 3.093654155731201,
        "learning_rate": 0.00013997935904965452,
        "epoch": 0.9152,
        "step": 6864
    },
    {
        "loss": 1.9746,
        "grad_norm": 3.7963271141052246,
        "learning_rate": 0.00013992166298557786,
        "epoch": 0.9153333333333333,
        "step": 6865
    },
    {
        "loss": 2.6934,
        "grad_norm": 2.814626455307007,
        "learning_rate": 0.00013986395110839446,
        "epoch": 0.9154666666666667,
        "step": 6866
    },
    {
        "loss": 2.4966,
        "grad_norm": 2.852750778198242,
        "learning_rate": 0.00013980622344096424,
        "epoch": 0.9156,
        "step": 6867
    },
    {
        "loss": 2.3059,
        "grad_norm": 4.959228515625,
        "learning_rate": 0.0001397484800061532,
        "epoch": 0.9157333333333333,
        "step": 6868
    },
    {
        "loss": 1.2015,
        "grad_norm": 3.866548776626587,
        "learning_rate": 0.0001396907208268338,
        "epoch": 0.9158666666666667,
        "step": 6869
    },
    {
        "loss": 1.7592,
        "grad_norm": 3.9387574195861816,
        "learning_rate": 0.0001396329459258847,
        "epoch": 0.916,
        "step": 6870
    },
    {
        "loss": 1.5478,
        "grad_norm": 4.372531890869141,
        "learning_rate": 0.00013957515532619062,
        "epoch": 0.9161333333333334,
        "step": 6871
    },
    {
        "loss": 2.1792,
        "grad_norm": 4.257197856903076,
        "learning_rate": 0.00013951734905064266,
        "epoch": 0.9162666666666667,
        "step": 6872
    },
    {
        "loss": 2.6581,
        "grad_norm": 3.167353868484497,
        "learning_rate": 0.00013945952712213816,
        "epoch": 0.9164,
        "step": 6873
    },
    {
        "loss": 2.5247,
        "grad_norm": 2.8182005882263184,
        "learning_rate": 0.00013940168956358043,
        "epoch": 0.9165333333333333,
        "step": 6874
    },
    {
        "loss": 1.7603,
        "grad_norm": 3.821479320526123,
        "learning_rate": 0.00013934383639787928,
        "epoch": 0.9166666666666666,
        "step": 6875
    },
    {
        "loss": 2.56,
        "grad_norm": 3.028135299682617,
        "learning_rate": 0.0001392859676479504,
        "epoch": 0.9168,
        "step": 6876
    },
    {
        "loss": 1.9511,
        "grad_norm": 2.9096813201904297,
        "learning_rate": 0.00013922808333671595,
        "epoch": 0.9169333333333334,
        "step": 6877
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.7533071041107178,
        "learning_rate": 0.00013917018348710389,
        "epoch": 0.9170666666666667,
        "step": 6878
    },
    {
        "loss": 2.8815,
        "grad_norm": 3.1533570289611816,
        "learning_rate": 0.0001391122681220488,
        "epoch": 0.9172,
        "step": 6879
    },
    {
        "loss": 3.0133,
        "grad_norm": 3.1980843544006348,
        "learning_rate": 0.00013905433726449097,
        "epoch": 0.9173333333333333,
        "step": 6880
    },
    {
        "loss": 0.7942,
        "grad_norm": 2.9729037284851074,
        "learning_rate": 0.00013899639093737718,
        "epoch": 0.9174666666666667,
        "step": 6881
    },
    {
        "loss": 2.6814,
        "grad_norm": 2.855600118637085,
        "learning_rate": 0.00013893842916365995,
        "epoch": 0.9176,
        "step": 6882
    },
    {
        "loss": 2.4353,
        "grad_norm": 3.306938648223877,
        "learning_rate": 0.00013888045196629826,
        "epoch": 0.9177333333333333,
        "step": 6883
    },
    {
        "loss": 1.8369,
        "grad_norm": 3.3400330543518066,
        "learning_rate": 0.00013882245936825712,
        "epoch": 0.9178666666666667,
        "step": 6884
    },
    {
        "loss": 3.4165,
        "grad_norm": 3.4834322929382324,
        "learning_rate": 0.0001387644513925075,
        "epoch": 0.918,
        "step": 6885
    },
    {
        "loss": 2.3848,
        "grad_norm": 2.9311134815216064,
        "learning_rate": 0.00013870642806202669,
        "epoch": 0.9181333333333334,
        "step": 6886
    },
    {
        "loss": 2.2876,
        "grad_norm": 3.466627597808838,
        "learning_rate": 0.00013864838939979762,
        "epoch": 0.9182666666666667,
        "step": 6887
    },
    {
        "loss": 1.0372,
        "grad_norm": 3.6225333213806152,
        "learning_rate": 0.00013859033542881,
        "epoch": 0.9184,
        "step": 6888
    },
    {
        "loss": 2.1544,
        "grad_norm": 3.9404854774475098,
        "learning_rate": 0.000138532266172059,
        "epoch": 0.9185333333333333,
        "step": 6889
    },
    {
        "loss": 2.33,
        "grad_norm": 2.5755369663238525,
        "learning_rate": 0.00013847418165254606,
        "epoch": 0.9186666666666666,
        "step": 6890
    },
    {
        "loss": 1.82,
        "grad_norm": 3.2178235054016113,
        "learning_rate": 0.00013841608189327863,
        "epoch": 0.9188,
        "step": 6891
    },
    {
        "loss": 2.4139,
        "grad_norm": 1.9549031257629395,
        "learning_rate": 0.00013835796691727035,
        "epoch": 0.9189333333333334,
        "step": 6892
    },
    {
        "loss": 2.2098,
        "grad_norm": 3.5651395320892334,
        "learning_rate": 0.0001382998367475406,
        "epoch": 0.9190666666666667,
        "step": 6893
    },
    {
        "loss": 2.1156,
        "grad_norm": 4.558871269226074,
        "learning_rate": 0.00013824169140711499,
        "epoch": 0.9192,
        "step": 6894
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.8414957523345947,
        "learning_rate": 0.0001381835309190252,
        "epoch": 0.9193333333333333,
        "step": 6895
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.020667791366577,
        "learning_rate": 0.0001381253553063086,
        "epoch": 0.9194666666666667,
        "step": 6896
    },
    {
        "loss": 3.0055,
        "grad_norm": 2.502376079559326,
        "learning_rate": 0.00013806716459200888,
        "epoch": 0.9196,
        "step": 6897
    },
    {
        "loss": 2.3119,
        "grad_norm": 2.7536299228668213,
        "learning_rate": 0.00013800895879917562,
        "epoch": 0.9197333333333333,
        "step": 6898
    },
    {
        "loss": 3.1509,
        "grad_norm": 4.774331569671631,
        "learning_rate": 0.00013795073795086423,
        "epoch": 0.9198666666666667,
        "step": 6899
    },
    {
        "loss": 1.9721,
        "grad_norm": 2.507169485092163,
        "learning_rate": 0.00013789250207013625,
        "epoch": 0.92,
        "step": 6900
    },
    {
        "loss": 2.8536,
        "grad_norm": 2.35524582862854,
        "learning_rate": 0.00013783425118005902,
        "epoch": 0.9201333333333334,
        "step": 6901
    },
    {
        "loss": 1.7492,
        "grad_norm": 3.768724203109741,
        "learning_rate": 0.00013777598530370606,
        "epoch": 0.9202666666666667,
        "step": 6902
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.91117000579834,
        "learning_rate": 0.00013771770446415648,
        "epoch": 0.9204,
        "step": 6903
    },
    {
        "loss": 0.7542,
        "grad_norm": 5.898952960968018,
        "learning_rate": 0.00013765940868449577,
        "epoch": 0.9205333333333333,
        "step": 6904
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.681467294692993,
        "learning_rate": 0.00013760109798781486,
        "epoch": 0.9206666666666666,
        "step": 6905
    },
    {
        "loss": 3.146,
        "grad_norm": 2.1767690181732178,
        "learning_rate": 0.00013754277239721093,
        "epoch": 0.9208,
        "step": 6906
    },
    {
        "loss": 2.0508,
        "grad_norm": 2.0164906978607178,
        "learning_rate": 0.00013748443193578702,
        "epoch": 0.9209333333333334,
        "step": 6907
    },
    {
        "loss": 2.4475,
        "grad_norm": 2.1650924682617188,
        "learning_rate": 0.00013742607662665178,
        "epoch": 0.9210666666666667,
        "step": 6908
    },
    {
        "loss": 2.7837,
        "grad_norm": 2.830418586730957,
        "learning_rate": 0.00013736770649292018,
        "epoch": 0.9212,
        "step": 6909
    },
    {
        "loss": 2.822,
        "grad_norm": 2.073758602142334,
        "learning_rate": 0.00013730932155771262,
        "epoch": 0.9213333333333333,
        "step": 6910
    },
    {
        "loss": 2.9703,
        "grad_norm": 2.7309494018554688,
        "learning_rate": 0.00013725092184415575,
        "epoch": 0.9214666666666667,
        "step": 6911
    },
    {
        "loss": 2.6926,
        "grad_norm": 3.541929006576538,
        "learning_rate": 0.00013719250737538166,
        "epoch": 0.9216,
        "step": 6912
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.5259244441986084,
        "learning_rate": 0.0001371340781745288,
        "epoch": 0.9217333333333333,
        "step": 6913
    },
    {
        "loss": 2.6675,
        "grad_norm": 2.713897705078125,
        "learning_rate": 0.00013707563426474097,
        "epoch": 0.9218666666666666,
        "step": 6914
    },
    {
        "loss": 1.3826,
        "grad_norm": 4.012861251831055,
        "learning_rate": 0.00013701717566916817,
        "epoch": 0.922,
        "step": 6915
    },
    {
        "loss": 2.5706,
        "grad_norm": 3.0313920974731445,
        "learning_rate": 0.00013695870241096586,
        "epoch": 0.9221333333333334,
        "step": 6916
    },
    {
        "loss": 2.2942,
        "grad_norm": 4.907463550567627,
        "learning_rate": 0.00013690021451329566,
        "epoch": 0.9222666666666667,
        "step": 6917
    },
    {
        "loss": 1.0069,
        "grad_norm": 3.6616697311401367,
        "learning_rate": 0.0001368417119993247,
        "epoch": 0.9224,
        "step": 6918
    },
    {
        "loss": 2.3366,
        "grad_norm": 4.215383052825928,
        "learning_rate": 0.00013678319489222605,
        "epoch": 0.9225333333333333,
        "step": 6919
    },
    {
        "loss": 1.5826,
        "grad_norm": 2.9957664012908936,
        "learning_rate": 0.0001367246632151787,
        "epoch": 0.9226666666666666,
        "step": 6920
    },
    {
        "loss": 1.1965,
        "grad_norm": 6.440446376800537,
        "learning_rate": 0.000136666116991367,
        "epoch": 0.9228,
        "step": 6921
    },
    {
        "loss": 2.6277,
        "grad_norm": 3.960411310195923,
        "learning_rate": 0.00013660755624398145,
        "epoch": 0.9229333333333334,
        "step": 6922
    },
    {
        "loss": 2.1397,
        "grad_norm": 4.648440837860107,
        "learning_rate": 0.0001365489809962181,
        "epoch": 0.9230666666666667,
        "step": 6923
    },
    {
        "loss": 1.8746,
        "grad_norm": 4.0300116539001465,
        "learning_rate": 0.00013649039127127893,
        "epoch": 0.9232,
        "step": 6924
    },
    {
        "loss": 2.73,
        "grad_norm": 4.004718780517578,
        "learning_rate": 0.00013643178709237138,
        "epoch": 0.9233333333333333,
        "step": 6925
    },
    {
        "loss": 1.7797,
        "grad_norm": 3.574507474899292,
        "learning_rate": 0.00013637316848270887,
        "epoch": 0.9234666666666667,
        "step": 6926
    },
    {
        "loss": 2.691,
        "grad_norm": 3.4239885807037354,
        "learning_rate": 0.00013631453546551032,
        "epoch": 0.9236,
        "step": 6927
    },
    {
        "loss": 2.7205,
        "grad_norm": 3.602952718734741,
        "learning_rate": 0.00013625588806400054,
        "epoch": 0.9237333333333333,
        "step": 6928
    },
    {
        "loss": 2.3444,
        "grad_norm": 3.011021137237549,
        "learning_rate": 0.00013619722630141002,
        "epoch": 0.9238666666666666,
        "step": 6929
    },
    {
        "loss": 2.5535,
        "grad_norm": 3.300572156906128,
        "learning_rate": 0.00013613855020097476,
        "epoch": 0.924,
        "step": 6930
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.390204668045044,
        "learning_rate": 0.00013607985978593664,
        "epoch": 0.9241333333333334,
        "step": 6931
    },
    {
        "loss": 2.5028,
        "grad_norm": 2.796772003173828,
        "learning_rate": 0.00013602115507954318,
        "epoch": 0.9242666666666667,
        "step": 6932
    },
    {
        "loss": 2.3651,
        "grad_norm": 2.354151725769043,
        "learning_rate": 0.00013596243610504736,
        "epoch": 0.9244,
        "step": 6933
    },
    {
        "loss": 2.3176,
        "grad_norm": 4.345939636230469,
        "learning_rate": 0.00013590370288570815,
        "epoch": 0.9245333333333333,
        "step": 6934
    },
    {
        "loss": 2.1162,
        "grad_norm": 3.0522594451904297,
        "learning_rate": 0.00013584495544478984,
        "epoch": 0.9246666666666666,
        "step": 6935
    },
    {
        "loss": 2.6819,
        "grad_norm": 2.6640758514404297,
        "learning_rate": 0.0001357861938055626,
        "epoch": 0.9248,
        "step": 6936
    },
    {
        "loss": 2.2067,
        "grad_norm": 2.372307538986206,
        "learning_rate": 0.00013572741799130187,
        "epoch": 0.9249333333333334,
        "step": 6937
    },
    {
        "loss": 2.7266,
        "grad_norm": 2.6599817276000977,
        "learning_rate": 0.00013566862802528933,
        "epoch": 0.9250666666666667,
        "step": 6938
    },
    {
        "loss": 2.8165,
        "grad_norm": 2.2905571460723877,
        "learning_rate": 0.00013560982393081162,
        "epoch": 0.9252,
        "step": 6939
    },
    {
        "loss": 2.6034,
        "grad_norm": 2.2804758548736572,
        "learning_rate": 0.00013555100573116134,
        "epoch": 0.9253333333333333,
        "step": 6940
    },
    {
        "loss": 2.5476,
        "grad_norm": 2.2549314498901367,
        "learning_rate": 0.0001354921734496365,
        "epoch": 0.9254666666666667,
        "step": 6941
    },
    {
        "loss": 2.1445,
        "grad_norm": 3.8997249603271484,
        "learning_rate": 0.0001354333271095409,
        "epoch": 0.9256,
        "step": 6942
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.469666004180908,
        "learning_rate": 0.00013537446673418365,
        "epoch": 0.9257333333333333,
        "step": 6943
    },
    {
        "loss": 1.8387,
        "grad_norm": 3.1748054027557373,
        "learning_rate": 0.00013531559234687953,
        "epoch": 0.9258666666666666,
        "step": 6944
    },
    {
        "loss": 3.0304,
        "grad_norm": 3.2557437419891357,
        "learning_rate": 0.00013525670397094905,
        "epoch": 0.926,
        "step": 6945
    },
    {
        "loss": 2.0861,
        "grad_norm": 4.035717964172363,
        "learning_rate": 0.00013519780162971786,
        "epoch": 0.9261333333333334,
        "step": 6946
    },
    {
        "loss": 2.7282,
        "grad_norm": 3.6055757999420166,
        "learning_rate": 0.00013513888534651768,
        "epoch": 0.9262666666666667,
        "step": 6947
    },
    {
        "loss": 2.4636,
        "grad_norm": 3.5392234325408936,
        "learning_rate": 0.00013507995514468513,
        "epoch": 0.9264,
        "step": 6948
    },
    {
        "loss": 1.9953,
        "grad_norm": 3.5294692516326904,
        "learning_rate": 0.00013502101104756296,
        "epoch": 0.9265333333333333,
        "step": 6949
    },
    {
        "loss": 2.3511,
        "grad_norm": 3.2037787437438965,
        "learning_rate": 0.00013496205307849887,
        "epoch": 0.9266666666666666,
        "step": 6950
    },
    {
        "loss": 2.2704,
        "grad_norm": 3.0801515579223633,
        "learning_rate": 0.00013490308126084651,
        "epoch": 0.9268,
        "step": 6951
    },
    {
        "loss": 2.8602,
        "grad_norm": 1.3225406408309937,
        "learning_rate": 0.00013484409561796468,
        "epoch": 0.9269333333333334,
        "step": 6952
    },
    {
        "loss": 2.2183,
        "grad_norm": 4.570487976074219,
        "learning_rate": 0.0001347850961732178,
        "epoch": 0.9270666666666667,
        "step": 6953
    },
    {
        "loss": 2.4011,
        "grad_norm": 2.958951473236084,
        "learning_rate": 0.0001347260829499759,
        "epoch": 0.9272,
        "step": 6954
    },
    {
        "loss": 2.354,
        "grad_norm": 4.615775108337402,
        "learning_rate": 0.00013466705597161416,
        "epoch": 0.9273333333333333,
        "step": 6955
    },
    {
        "loss": 2.2371,
        "grad_norm": 3.4783804416656494,
        "learning_rate": 0.00013460801526151342,
        "epoch": 0.9274666666666667,
        "step": 6956
    },
    {
        "loss": 2.2921,
        "grad_norm": 5.091836929321289,
        "learning_rate": 0.00013454896084305994,
        "epoch": 0.9276,
        "step": 6957
    },
    {
        "loss": 1.6605,
        "grad_norm": 4.8729119300842285,
        "learning_rate": 0.00013448989273964534,
        "epoch": 0.9277333333333333,
        "step": 6958
    },
    {
        "loss": 2.7256,
        "grad_norm": 3.410996437072754,
        "learning_rate": 0.00013443081097466673,
        "epoch": 0.9278666666666666,
        "step": 6959
    },
    {
        "loss": 2.2518,
        "grad_norm": 2.9015979766845703,
        "learning_rate": 0.0001343717155715265,
        "epoch": 0.928,
        "step": 6960
    },
    {
        "loss": 2.4357,
        "grad_norm": 2.654588460922241,
        "learning_rate": 0.00013431260655363272,
        "epoch": 0.9281333333333334,
        "step": 6961
    },
    {
        "loss": 1.9218,
        "grad_norm": 2.31984543800354,
        "learning_rate": 0.0001342534839443984,
        "epoch": 0.9282666666666667,
        "step": 6962
    },
    {
        "loss": 2.0589,
        "grad_norm": 5.370497226715088,
        "learning_rate": 0.00013419434776724252,
        "epoch": 0.9284,
        "step": 6963
    },
    {
        "loss": 2.2233,
        "grad_norm": 3.6769981384277344,
        "learning_rate": 0.0001341351980455889,
        "epoch": 0.9285333333333333,
        "step": 6964
    },
    {
        "loss": 2.2437,
        "grad_norm": 4.011181354522705,
        "learning_rate": 0.00013407603480286703,
        "epoch": 0.9286666666666666,
        "step": 6965
    },
    {
        "loss": 2.1531,
        "grad_norm": 3.3178653717041016,
        "learning_rate": 0.0001340168580625117,
        "epoch": 0.9288,
        "step": 6966
    },
    {
        "loss": 2.1778,
        "grad_norm": 4.049461364746094,
        "learning_rate": 0.00013395766784796297,
        "epoch": 0.9289333333333334,
        "step": 6967
    },
    {
        "loss": 1.239,
        "grad_norm": 3.992084264755249,
        "learning_rate": 0.00013389846418266634,
        "epoch": 0.9290666666666667,
        "step": 6968
    },
    {
        "loss": 2.1494,
        "grad_norm": 2.9639222621917725,
        "learning_rate": 0.00013383924709007246,
        "epoch": 0.9292,
        "step": 6969
    },
    {
        "loss": 1.8526,
        "grad_norm": 4.841090679168701,
        "learning_rate": 0.00013378001659363758,
        "epoch": 0.9293333333333333,
        "step": 6970
    },
    {
        "loss": 2.8753,
        "grad_norm": 3.137078285217285,
        "learning_rate": 0.00013372077271682291,
        "epoch": 0.9294666666666667,
        "step": 6971
    },
    {
        "loss": 2.6265,
        "grad_norm": 3.529555082321167,
        "learning_rate": 0.00013366151548309537,
        "epoch": 0.9296,
        "step": 6972
    },
    {
        "loss": 2.8863,
        "grad_norm": 3.2636618614196777,
        "learning_rate": 0.00013360224491592679,
        "epoch": 0.9297333333333333,
        "step": 6973
    },
    {
        "loss": 1.8984,
        "grad_norm": 4.771998405456543,
        "learning_rate": 0.00013354296103879455,
        "epoch": 0.9298666666666666,
        "step": 6974
    },
    {
        "loss": 2.2484,
        "grad_norm": 4.79489278793335,
        "learning_rate": 0.00013348366387518112,
        "epoch": 0.93,
        "step": 6975
    },
    {
        "loss": 1.7528,
        "grad_norm": 2.6346819400787354,
        "learning_rate": 0.0001334243534485744,
        "epoch": 0.9301333333333334,
        "step": 6976
    },
    {
        "loss": 1.5317,
        "grad_norm": 3.5401558876037598,
        "learning_rate": 0.0001333650297824673,
        "epoch": 0.9302666666666667,
        "step": 6977
    },
    {
        "loss": 2.1893,
        "grad_norm": 3.286884307861328,
        "learning_rate": 0.00013330569290035822,
        "epoch": 0.9304,
        "step": 6978
    },
    {
        "loss": 2.6297,
        "grad_norm": 4.268720626831055,
        "learning_rate": 0.00013324634282575076,
        "epoch": 0.9305333333333333,
        "step": 6979
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.245041847229004,
        "learning_rate": 0.00013318697958215357,
        "epoch": 0.9306666666666666,
        "step": 6980
    },
    {
        "loss": 2.7014,
        "grad_norm": 3.7392327785491943,
        "learning_rate": 0.00013312760319308066,
        "epoch": 0.9308,
        "step": 6981
    },
    {
        "loss": 2.7979,
        "grad_norm": 4.587785720825195,
        "learning_rate": 0.00013306821368205132,
        "epoch": 0.9309333333333333,
        "step": 6982
    },
    {
        "loss": 1.8755,
        "grad_norm": 3.180459976196289,
        "learning_rate": 0.0001330088110725898,
        "epoch": 0.9310666666666667,
        "step": 6983
    },
    {
        "loss": 2.5063,
        "grad_norm": 2.6746296882629395,
        "learning_rate": 0.00013294939538822576,
        "epoch": 0.9312,
        "step": 6984
    },
    {
        "loss": 2.7818,
        "grad_norm": 5.386935234069824,
        "learning_rate": 0.000132889966652494,
        "epoch": 0.9313333333333333,
        "step": 6985
    },
    {
        "loss": 1.7474,
        "grad_norm": 2.9382901191711426,
        "learning_rate": 0.00013283052488893428,
        "epoch": 0.9314666666666667,
        "step": 6986
    },
    {
        "loss": 1.5261,
        "grad_norm": 4.255714416503906,
        "learning_rate": 0.0001327710701210918,
        "epoch": 0.9316,
        "step": 6987
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.062331199645996,
        "learning_rate": 0.00013271160237251684,
        "epoch": 0.9317333333333333,
        "step": 6988
    },
    {
        "loss": 2.1861,
        "grad_norm": 3.0521347522735596,
        "learning_rate": 0.0001326521216667647,
        "epoch": 0.9318666666666666,
        "step": 6989
    },
    {
        "loss": 2.8045,
        "grad_norm": 2.8243191242218018,
        "learning_rate": 0.00013259262802739588,
        "epoch": 0.932,
        "step": 6990
    },
    {
        "loss": 1.6832,
        "grad_norm": 2.7900166511535645,
        "learning_rate": 0.00013253312147797606,
        "epoch": 0.9321333333333334,
        "step": 6991
    },
    {
        "loss": 2.6592,
        "grad_norm": 2.6950416564941406,
        "learning_rate": 0.00013247360204207595,
        "epoch": 0.9322666666666667,
        "step": 6992
    },
    {
        "loss": 2.1182,
        "grad_norm": 2.9889142513275146,
        "learning_rate": 0.00013241406974327146,
        "epoch": 0.9324,
        "step": 6993
    },
    {
        "loss": 2.3773,
        "grad_norm": 3.557359218597412,
        "learning_rate": 0.00013235452460514345,
        "epoch": 0.9325333333333333,
        "step": 6994
    },
    {
        "loss": 3.0612,
        "grad_norm": 1.9132112264633179,
        "learning_rate": 0.00013229496665127804,
        "epoch": 0.9326666666666666,
        "step": 6995
    },
    {
        "loss": 2.4259,
        "grad_norm": 3.340452194213867,
        "learning_rate": 0.00013223539590526622,
        "epoch": 0.9328,
        "step": 6996
    },
    {
        "loss": 1.2368,
        "grad_norm": 4.281762599945068,
        "learning_rate": 0.00013217581239070432,
        "epoch": 0.9329333333333333,
        "step": 6997
    },
    {
        "loss": 2.6122,
        "grad_norm": 3.5208680629730225,
        "learning_rate": 0.00013211621613119347,
        "epoch": 0.9330666666666667,
        "step": 6998
    },
    {
        "loss": 2.436,
        "grad_norm": 3.0500590801239014,
        "learning_rate": 0.00013205660715034,
        "epoch": 0.9332,
        "step": 6999
    },
    {
        "loss": 2.7441,
        "grad_norm": 3.9297149181365967,
        "learning_rate": 0.0001319969854717552,
        "epoch": 0.9333333333333333,
        "step": 7000
    },
    {
        "loss": 2.354,
        "grad_norm": 2.473299741744995,
        "learning_rate": 0.00013193735111905546,
        "epoch": 0.9334666666666667,
        "step": 7001
    },
    {
        "loss": 2.9656,
        "grad_norm": 2.4558157920837402,
        "learning_rate": 0.00013187770411586202,
        "epoch": 0.9336,
        "step": 7002
    },
    {
        "loss": 1.1385,
        "grad_norm": 4.092345237731934,
        "learning_rate": 0.0001318180444858014,
        "epoch": 0.9337333333333333,
        "step": 7003
    },
    {
        "loss": 1.7135,
        "grad_norm": 3.3476004600524902,
        "learning_rate": 0.000131758372252505,
        "epoch": 0.9338666666666666,
        "step": 7004
    },
    {
        "loss": 1.1858,
        "grad_norm": 4.183059215545654,
        "learning_rate": 0.00013169868743960898,
        "epoch": 0.934,
        "step": 7005
    },
    {
        "loss": 3.0159,
        "grad_norm": 2.7137367725372314,
        "learning_rate": 0.000131638990070755,
        "epoch": 0.9341333333333334,
        "step": 7006
    },
    {
        "loss": 2.405,
        "grad_norm": 3.5132665634155273,
        "learning_rate": 0.00013157928016958924,
        "epoch": 0.9342666666666667,
        "step": 7007
    },
    {
        "loss": 2.2851,
        "grad_norm": 3.866116523742676,
        "learning_rate": 0.000131519557759763,
        "epoch": 0.9344,
        "step": 7008
    },
    {
        "loss": 1.7158,
        "grad_norm": 3.7872188091278076,
        "learning_rate": 0.00013145982286493245,
        "epoch": 0.9345333333333333,
        "step": 7009
    },
    {
        "loss": 1.5367,
        "grad_norm": 3.0140817165374756,
        "learning_rate": 0.000131400075508759,
        "epoch": 0.9346666666666666,
        "step": 7010
    },
    {
        "loss": 2.6753,
        "grad_norm": 3.2777857780456543,
        "learning_rate": 0.00013134031571490853,
        "epoch": 0.9348,
        "step": 7011
    },
    {
        "loss": 1.379,
        "grad_norm": 3.536402463912964,
        "learning_rate": 0.00013128054350705222,
        "epoch": 0.9349333333333333,
        "step": 7012
    },
    {
        "loss": 2.7697,
        "grad_norm": 3.4629788398742676,
        "learning_rate": 0.00013122075890886612,
        "epoch": 0.9350666666666667,
        "step": 7013
    },
    {
        "loss": 2.463,
        "grad_norm": 2.460691452026367,
        "learning_rate": 0.00013116096194403095,
        "epoch": 0.9352,
        "step": 7014
    },
    {
        "loss": 2.8193,
        "grad_norm": 4.037999153137207,
        "learning_rate": 0.00013110115263623262,
        "epoch": 0.9353333333333333,
        "step": 7015
    },
    {
        "loss": 1.56,
        "grad_norm": 5.785741329193115,
        "learning_rate": 0.0001310413310091618,
        "epoch": 0.9354666666666667,
        "step": 7016
    },
    {
        "loss": 1.5292,
        "grad_norm": 3.534207820892334,
        "learning_rate": 0.00013098149708651392,
        "epoch": 0.9356,
        "step": 7017
    },
    {
        "loss": 2.7125,
        "grad_norm": 3.7586677074432373,
        "learning_rate": 0.00013092165089198955,
        "epoch": 0.9357333333333333,
        "step": 7018
    },
    {
        "loss": 2.5642,
        "grad_norm": 4.630098342895508,
        "learning_rate": 0.0001308617924492938,
        "epoch": 0.9358666666666666,
        "step": 7019
    },
    {
        "loss": 2.5625,
        "grad_norm": 2.7641360759735107,
        "learning_rate": 0.00013080192178213703,
        "epoch": 0.936,
        "step": 7020
    },
    {
        "loss": 2.7368,
        "grad_norm": 2.703585624694824,
        "learning_rate": 0.0001307420389142339,
        "epoch": 0.9361333333333334,
        "step": 7021
    },
    {
        "loss": 2.0741,
        "grad_norm": 3.2419211864471436,
        "learning_rate": 0.00013068214386930457,
        "epoch": 0.9362666666666667,
        "step": 7022
    },
    {
        "loss": 2.4282,
        "grad_norm": 3.4143171310424805,
        "learning_rate": 0.00013062223667107342,
        "epoch": 0.9364,
        "step": 7023
    },
    {
        "loss": 1.7676,
        "grad_norm": 2.886648654937744,
        "learning_rate": 0.00013056231734327,
        "epoch": 0.9365333333333333,
        "step": 7024
    },
    {
        "loss": 2.0133,
        "grad_norm": 3.456835985183716,
        "learning_rate": 0.0001305023859096286,
        "epoch": 0.9366666666666666,
        "step": 7025
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.715430736541748,
        "learning_rate": 0.00013044244239388813,
        "epoch": 0.9368,
        "step": 7026
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.7583870887756348,
        "learning_rate": 0.00013038248681979258,
        "epoch": 0.9369333333333333,
        "step": 7027
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.9757556915283203,
        "learning_rate": 0.0001303225192110904,
        "epoch": 0.9370666666666667,
        "step": 7028
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.123487710952759,
        "learning_rate": 0.0001302625395915351,
        "epoch": 0.9372,
        "step": 7029
    },
    {
        "loss": 2.239,
        "grad_norm": 3.5700724124908447,
        "learning_rate": 0.00013020254798488465,
        "epoch": 0.9373333333333334,
        "step": 7030
    },
    {
        "loss": 1.1416,
        "grad_norm": 5.184057712554932,
        "learning_rate": 0.00013014254441490213,
        "epoch": 0.9374666666666667,
        "step": 7031
    },
    {
        "loss": 1.9144,
        "grad_norm": 2.7931697368621826,
        "learning_rate": 0.000130082528905355,
        "epoch": 0.9376,
        "step": 7032
    },
    {
        "loss": 1.3975,
        "grad_norm": 2.859208106994629,
        "learning_rate": 0.00013002250148001573,
        "epoch": 0.9377333333333333,
        "step": 7033
    },
    {
        "loss": 1.3804,
        "grad_norm": 4.5652689933776855,
        "learning_rate": 0.00012996246216266126,
        "epoch": 0.9378666666666666,
        "step": 7034
    },
    {
        "loss": 2.0999,
        "grad_norm": 2.8488550186157227,
        "learning_rate": 0.00012990241097707352,
        "epoch": 0.938,
        "step": 7035
    },
    {
        "loss": 2.0628,
        "grad_norm": 3.3585121631622314,
        "learning_rate": 0.00012984234794703882,
        "epoch": 0.9381333333333334,
        "step": 7036
    },
    {
        "loss": 2.4024,
        "grad_norm": 3.037053108215332,
        "learning_rate": 0.0001297822730963484,
        "epoch": 0.9382666666666667,
        "step": 7037
    },
    {
        "loss": 2.1907,
        "grad_norm": 4.469365119934082,
        "learning_rate": 0.00012972218644879822,
        "epoch": 0.9384,
        "step": 7038
    },
    {
        "loss": 1.9509,
        "grad_norm": 3.0359485149383545,
        "learning_rate": 0.0001296620880281886,
        "epoch": 0.9385333333333333,
        "step": 7039
    },
    {
        "loss": 2.5113,
        "grad_norm": 3.381577253341675,
        "learning_rate": 0.00012960197785832488,
        "epoch": 0.9386666666666666,
        "step": 7040
    },
    {
        "loss": 2.6475,
        "grad_norm": 2.1767659187316895,
        "learning_rate": 0.00012954185596301696,
        "epoch": 0.9388,
        "step": 7041
    },
    {
        "loss": 2.8353,
        "grad_norm": 2.7787954807281494,
        "learning_rate": 0.0001294817223660791,
        "epoch": 0.9389333333333333,
        "step": 7042
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.5231549739837646,
        "learning_rate": 0.00012942157709133065,
        "epoch": 0.9390666666666667,
        "step": 7043
    },
    {
        "loss": 2.7726,
        "grad_norm": 3.2609219551086426,
        "learning_rate": 0.0001293614201625952,
        "epoch": 0.9392,
        "step": 7044
    },
    {
        "loss": 2.0317,
        "grad_norm": 3.0783205032348633,
        "learning_rate": 0.00012930125160370116,
        "epoch": 0.9393333333333334,
        "step": 7045
    },
    {
        "loss": 2.107,
        "grad_norm": 2.9987316131591797,
        "learning_rate": 0.00012924107143848153,
        "epoch": 0.9394666666666667,
        "step": 7046
    },
    {
        "loss": 2.4602,
        "grad_norm": 3.119309425354004,
        "learning_rate": 0.00012918087969077393,
        "epoch": 0.9396,
        "step": 7047
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.8872690200805664,
        "learning_rate": 0.0001291206763844204,
        "epoch": 0.9397333333333333,
        "step": 7048
    },
    {
        "loss": 2.9861,
        "grad_norm": 2.9944987297058105,
        "learning_rate": 0.0001290604615432677,
        "epoch": 0.9398666666666666,
        "step": 7049
    },
    {
        "loss": 1.8373,
        "grad_norm": 1.7665539979934692,
        "learning_rate": 0.00012900023519116723,
        "epoch": 0.94,
        "step": 7050
    },
    {
        "loss": 2.8531,
        "grad_norm": 2.6658945083618164,
        "learning_rate": 0.00012893999735197475,
        "epoch": 0.9401333333333334,
        "step": 7051
    },
    {
        "loss": 3.6151,
        "grad_norm": 4.120575428009033,
        "learning_rate": 0.00012887974804955075,
        "epoch": 0.9402666666666667,
        "step": 7052
    },
    {
        "loss": 1.976,
        "grad_norm": 4.176344871520996,
        "learning_rate": 0.00012881948730776008,
        "epoch": 0.9404,
        "step": 7053
    },
    {
        "loss": 2.3909,
        "grad_norm": 3.6076502799987793,
        "learning_rate": 0.00012875921515047235,
        "epoch": 0.9405333333333333,
        "step": 7054
    },
    {
        "loss": 2.0773,
        "grad_norm": 3.5569534301757812,
        "learning_rate": 0.0001286989316015614,
        "epoch": 0.9406666666666667,
        "step": 7055
    },
    {
        "loss": 2.76,
        "grad_norm": 2.4165987968444824,
        "learning_rate": 0.00012863863668490595,
        "epoch": 0.9408,
        "step": 7056
    },
    {
        "loss": 2.393,
        "grad_norm": 3.2053792476654053,
        "learning_rate": 0.00012857833042438887,
        "epoch": 0.9409333333333333,
        "step": 7057
    },
    {
        "loss": 2.4499,
        "grad_norm": 2.4561734199523926,
        "learning_rate": 0.0001285180128438978,
        "epoch": 0.9410666666666667,
        "step": 7058
    },
    {
        "loss": 2.4824,
        "grad_norm": 1.7021527290344238,
        "learning_rate": 0.00012845768396732462,
        "epoch": 0.9412,
        "step": 7059
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.761107921600342,
        "learning_rate": 0.0001283973438185659,
        "epoch": 0.9413333333333334,
        "step": 7060
    },
    {
        "loss": 2.8961,
        "grad_norm": 3.462731122970581,
        "learning_rate": 0.00012833699242152246,
        "epoch": 0.9414666666666667,
        "step": 7061
    },
    {
        "loss": 2.1198,
        "grad_norm": 4.453528881072998,
        "learning_rate": 0.00012827662980009977,
        "epoch": 0.9416,
        "step": 7062
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.627668857574463,
        "learning_rate": 0.00012821625597820777,
        "epoch": 0.9417333333333333,
        "step": 7063
    },
    {
        "loss": 2.1869,
        "grad_norm": 3.3558740615844727,
        "learning_rate": 0.0001281558709797605,
        "epoch": 0.9418666666666666,
        "step": 7064
    },
    {
        "loss": 2.2428,
        "grad_norm": 3.517505168914795,
        "learning_rate": 0.00012809547482867686,
        "epoch": 0.942,
        "step": 7065
    },
    {
        "loss": 1.6942,
        "grad_norm": 4.743943691253662,
        "learning_rate": 0.00012803506754887985,
        "epoch": 0.9421333333333334,
        "step": 7066
    },
    {
        "loss": 2.0792,
        "grad_norm": 5.030924320220947,
        "learning_rate": 0.00012797464916429715,
        "epoch": 0.9422666666666667,
        "step": 7067
    },
    {
        "loss": 2.4294,
        "grad_norm": 4.494312286376953,
        "learning_rate": 0.00012791421969886053,
        "epoch": 0.9424,
        "step": 7068
    },
    {
        "loss": 0.8461,
        "grad_norm": 4.202905178070068,
        "learning_rate": 0.00012785377917650645,
        "epoch": 0.9425333333333333,
        "step": 7069
    },
    {
        "loss": 1.1133,
        "grad_norm": 2.1690938472747803,
        "learning_rate": 0.00012779332762117546,
        "epoch": 0.9426666666666667,
        "step": 7070
    },
    {
        "loss": 1.7721,
        "grad_norm": 3.057312250137329,
        "learning_rate": 0.00012773286505681267,
        "epoch": 0.9428,
        "step": 7071
    },
    {
        "loss": 1.0375,
        "grad_norm": 3.304943084716797,
        "learning_rate": 0.0001276723915073676,
        "epoch": 0.9429333333333333,
        "step": 7072
    },
    {
        "loss": 1.0473,
        "grad_norm": 2.998713493347168,
        "learning_rate": 0.00012761190699679394,
        "epoch": 0.9430666666666667,
        "step": 7073
    },
    {
        "loss": 1.1227,
        "grad_norm": 5.219776630401611,
        "learning_rate": 0.0001275514115490498,
        "epoch": 0.9432,
        "step": 7074
    },
    {
        "loss": 2.6399,
        "grad_norm": 2.326479911804199,
        "learning_rate": 0.00012749090518809772,
        "epoch": 0.9433333333333334,
        "step": 7075
    },
    {
        "loss": 3.0595,
        "grad_norm": 3.8493099212646484,
        "learning_rate": 0.0001274303879379044,
        "epoch": 0.9434666666666667,
        "step": 7076
    },
    {
        "loss": 3.0564,
        "grad_norm": 3.1483054161071777,
        "learning_rate": 0.00012736985982244098,
        "epoch": 0.9436,
        "step": 7077
    },
    {
        "loss": 2.7707,
        "grad_norm": 3.2727038860321045,
        "learning_rate": 0.00012730932086568275,
        "epoch": 0.9437333333333333,
        "step": 7078
    },
    {
        "loss": 3.1744,
        "grad_norm": 3.0909690856933594,
        "learning_rate": 0.00012724877109160956,
        "epoch": 0.9438666666666666,
        "step": 7079
    },
    {
        "loss": 1.2638,
        "grad_norm": 5.84576940536499,
        "learning_rate": 0.00012718821052420517,
        "epoch": 0.944,
        "step": 7080
    },
    {
        "loss": 2.2896,
        "grad_norm": 2.94885516166687,
        "learning_rate": 0.00012712763918745808,
        "epoch": 0.9441333333333334,
        "step": 7081
    },
    {
        "loss": 2.8193,
        "grad_norm": 3.260889768600464,
        "learning_rate": 0.00012706705710536058,
        "epoch": 0.9442666666666667,
        "step": 7082
    },
    {
        "loss": 2.0637,
        "grad_norm": 2.973675012588501,
        "learning_rate": 0.0001270064643019096,
        "epoch": 0.9444,
        "step": 7083
    },
    {
        "loss": 2.4916,
        "grad_norm": 4.947357177734375,
        "learning_rate": 0.00012694586080110604,
        "epoch": 0.9445333333333333,
        "step": 7084
    },
    {
        "loss": 2.9572,
        "grad_norm": 2.565255880355835,
        "learning_rate": 0.0001268852466269552,
        "epoch": 0.9446666666666667,
        "step": 7085
    },
    {
        "loss": 2.8456,
        "grad_norm": 2.8511319160461426,
        "learning_rate": 0.0001268246218034666,
        "epoch": 0.9448,
        "step": 7086
    },
    {
        "loss": 2.3708,
        "grad_norm": 2.6420843601226807,
        "learning_rate": 0.00012676398635465388,
        "epoch": 0.9449333333333333,
        "step": 7087
    },
    {
        "loss": 2.5221,
        "grad_norm": 1.9934747219085693,
        "learning_rate": 0.00012670334030453502,
        "epoch": 0.9450666666666667,
        "step": 7088
    },
    {
        "loss": 0.9647,
        "grad_norm": 2.1354594230651855,
        "learning_rate": 0.00012664268367713191,
        "epoch": 0.9452,
        "step": 7089
    },
    {
        "loss": 2.0012,
        "grad_norm": 3.0832479000091553,
        "learning_rate": 0.00012658201649647116,
        "epoch": 0.9453333333333334,
        "step": 7090
    },
    {
        "loss": 2.4218,
        "grad_norm": 5.597230911254883,
        "learning_rate": 0.0001265213387865831,
        "epoch": 0.9454666666666667,
        "step": 7091
    },
    {
        "loss": 1.1215,
        "grad_norm": 3.4505765438079834,
        "learning_rate": 0.00012646065057150233,
        "epoch": 0.9456,
        "step": 7092
    },
    {
        "loss": 2.1375,
        "grad_norm": 3.3906219005584717,
        "learning_rate": 0.0001263999518752677,
        "epoch": 0.9457333333333333,
        "step": 7093
    },
    {
        "loss": 2.463,
        "grad_norm": 3.0120770931243896,
        "learning_rate": 0.00012633924272192226,
        "epoch": 0.9458666666666666,
        "step": 7094
    },
    {
        "loss": 2.692,
        "grad_norm": 4.834059238433838,
        "learning_rate": 0.00012627852313551295,
        "epoch": 0.946,
        "step": 7095
    },
    {
        "loss": 2.6419,
        "grad_norm": 2.7264487743377686,
        "learning_rate": 0.00012621779314009107,
        "epoch": 0.9461333333333334,
        "step": 7096
    },
    {
        "loss": 2.8047,
        "grad_norm": 2.5105133056640625,
        "learning_rate": 0.00012615705275971205,
        "epoch": 0.9462666666666667,
        "step": 7097
    },
    {
        "loss": 2.3245,
        "grad_norm": 2.9769704341888428,
        "learning_rate": 0.00012609630201843527,
        "epoch": 0.9464,
        "step": 7098
    },
    {
        "loss": 2.0876,
        "grad_norm": 2.346785306930542,
        "learning_rate": 0.0001260355409403243,
        "epoch": 0.9465333333333333,
        "step": 7099
    },
    {
        "loss": 2.9025,
        "grad_norm": 3.2435944080352783,
        "learning_rate": 0.00012597476954944695,
        "epoch": 0.9466666666666667,
        "step": 7100
    },
    {
        "loss": 2.1691,
        "grad_norm": 3.0523126125335693,
        "learning_rate": 0.0001259139878698748,
        "epoch": 0.9468,
        "step": 7101
    },
    {
        "loss": 2.6077,
        "grad_norm": 3.97528076171875,
        "learning_rate": 0.00012585319592568384,
        "epoch": 0.9469333333333333,
        "step": 7102
    },
    {
        "loss": 2.7966,
        "grad_norm": 2.7825191020965576,
        "learning_rate": 0.00012579239374095377,
        "epoch": 0.9470666666666666,
        "step": 7103
    },
    {
        "loss": 1.7902,
        "grad_norm": 3.3744513988494873,
        "learning_rate": 0.00012573158133976874,
        "epoch": 0.9472,
        "step": 7104
    },
    {
        "loss": 2.0823,
        "grad_norm": 2.2837162017822266,
        "learning_rate": 0.00012567075874621655,
        "epoch": 0.9473333333333334,
        "step": 7105
    },
    {
        "loss": 1.9443,
        "grad_norm": 3.5767030715942383,
        "learning_rate": 0.00012560992598438948,
        "epoch": 0.9474666666666667,
        "step": 7106
    },
    {
        "loss": 0.8566,
        "grad_norm": 2.4197628498077393,
        "learning_rate": 0.00012554908307838337,
        "epoch": 0.9476,
        "step": 7107
    },
    {
        "loss": 3.0367,
        "grad_norm": 2.1081247329711914,
        "learning_rate": 0.0001254882300522984,
        "epoch": 0.9477333333333333,
        "step": 7108
    },
    {
        "loss": 1.4242,
        "grad_norm": 5.830113410949707,
        "learning_rate": 0.00012542736693023872,
        "epoch": 0.9478666666666666,
        "step": 7109
    },
    {
        "loss": 1.2177,
        "grad_norm": 3.317209243774414,
        "learning_rate": 0.0001253664937363123,
        "epoch": 0.948,
        "step": 7110
    },
    {
        "loss": 2.4438,
        "grad_norm": 2.8148341178894043,
        "learning_rate": 0.00012530561049463136,
        "epoch": 0.9481333333333334,
        "step": 7111
    },
    {
        "loss": 2.1494,
        "grad_norm": 3.0854101181030273,
        "learning_rate": 0.00012524471722931183,
        "epoch": 0.9482666666666667,
        "step": 7112
    },
    {
        "loss": 2.2557,
        "grad_norm": 3.374800205230713,
        "learning_rate": 0.0001251838139644738,
        "epoch": 0.9484,
        "step": 7113
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.3280768394470215,
        "learning_rate": 0.00012512290072424112,
        "epoch": 0.9485333333333333,
        "step": 7114
    },
    {
        "loss": 1.9403,
        "grad_norm": 4.668184280395508,
        "learning_rate": 0.00012506197753274202,
        "epoch": 0.9486666666666667,
        "step": 7115
    },
    {
        "loss": 2.1785,
        "grad_norm": 4.053136825561523,
        "learning_rate": 0.00012500104441410812,
        "epoch": 0.9488,
        "step": 7116
    },
    {
        "loss": 2.5291,
        "grad_norm": 3.143007278442383,
        "learning_rate": 0.00012494010139247548,
        "epoch": 0.9489333333333333,
        "step": 7117
    },
    {
        "loss": 2.607,
        "grad_norm": 4.169038772583008,
        "learning_rate": 0.00012487914849198354,
        "epoch": 0.9490666666666666,
        "step": 7118
    },
    {
        "loss": 2.0827,
        "grad_norm": 2.4919002056121826,
        "learning_rate": 0.00012481818573677623,
        "epoch": 0.9492,
        "step": 7119
    },
    {
        "loss": 2.5646,
        "grad_norm": 3.2539424896240234,
        "learning_rate": 0.00012475721315100092,
        "epoch": 0.9493333333333334,
        "step": 7120
    },
    {
        "loss": 0.9123,
        "grad_norm": 4.143946647644043,
        "learning_rate": 0.00012469623075880907,
        "epoch": 0.9494666666666667,
        "step": 7121
    },
    {
        "loss": 2.3023,
        "grad_norm": 2.6704962253570557,
        "learning_rate": 0.00012463523858435618,
        "epoch": 0.9496,
        "step": 7122
    },
    {
        "loss": 2.3668,
        "grad_norm": 4.631045341491699,
        "learning_rate": 0.00012457423665180125,
        "epoch": 0.9497333333333333,
        "step": 7123
    },
    {
        "loss": 2.5233,
        "grad_norm": 4.151994228363037,
        "learning_rate": 0.0001245132249853075,
        "epoch": 0.9498666666666666,
        "step": 7124
    },
    {
        "loss": 2.4581,
        "grad_norm": 4.20534086227417,
        "learning_rate": 0.00012445220360904173,
        "epoch": 0.95,
        "step": 7125
    },
    {
        "loss": 2.5764,
        "grad_norm": 2.6428844928741455,
        "learning_rate": 0.0001243911725471749,
        "epoch": 0.9501333333333334,
        "step": 7126
    },
    {
        "loss": 2.6756,
        "grad_norm": 3.762187957763672,
        "learning_rate": 0.00012433013182388148,
        "epoch": 0.9502666666666667,
        "step": 7127
    },
    {
        "loss": 2.4579,
        "grad_norm": 2.8360257148742676,
        "learning_rate": 0.00012426908146333997,
        "epoch": 0.9504,
        "step": 7128
    },
    {
        "loss": 1.9923,
        "grad_norm": 4.479281425476074,
        "learning_rate": 0.0001242080214897325,
        "epoch": 0.9505333333333333,
        "step": 7129
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.827319383621216,
        "learning_rate": 0.00012414695192724525,
        "epoch": 0.9506666666666667,
        "step": 7130
    },
    {
        "loss": 2.5496,
        "grad_norm": 1.9767261743545532,
        "learning_rate": 0.00012408587280006816,
        "epoch": 0.9508,
        "step": 7131
    },
    {
        "loss": 2.7605,
        "grad_norm": 2.3331637382507324,
        "learning_rate": 0.00012402478413239468,
        "epoch": 0.9509333333333333,
        "step": 7132
    },
    {
        "loss": 2.2676,
        "grad_norm": 2.4827513694763184,
        "learning_rate": 0.00012396368594842238,
        "epoch": 0.9510666666666666,
        "step": 7133
    },
    {
        "loss": 3.2087,
        "grad_norm": 2.6278140544891357,
        "learning_rate": 0.00012390257827235248,
        "epoch": 0.9512,
        "step": 7134
    },
    {
        "loss": 2.5226,
        "grad_norm": 2.811939239501953,
        "learning_rate": 0.0001238414611283898,
        "epoch": 0.9513333333333334,
        "step": 7135
    },
    {
        "loss": 1.4111,
        "grad_norm": 3.085916042327881,
        "learning_rate": 0.00012378033454074318,
        "epoch": 0.9514666666666667,
        "step": 7136
    },
    {
        "loss": 2.4773,
        "grad_norm": 2.4885005950927734,
        "learning_rate": 0.000123719198533625,
        "epoch": 0.9516,
        "step": 7137
    },
    {
        "loss": 2.4699,
        "grad_norm": 3.693786382675171,
        "learning_rate": 0.0001236580531312515,
        "epoch": 0.9517333333333333,
        "step": 7138
    },
    {
        "loss": 2.7975,
        "grad_norm": 2.1965579986572266,
        "learning_rate": 0.0001235968983578424,
        "epoch": 0.9518666666666666,
        "step": 7139
    },
    {
        "loss": 2.0288,
        "grad_norm": 4.404387950897217,
        "learning_rate": 0.0001235357342376216,
        "epoch": 0.952,
        "step": 7140
    },
    {
        "loss": 2.9851,
        "grad_norm": 4.0913872718811035,
        "learning_rate": 0.00012347456079481617,
        "epoch": 0.9521333333333334,
        "step": 7141
    },
    {
        "loss": 2.1503,
        "grad_norm": 5.22714900970459,
        "learning_rate": 0.00012341337805365737,
        "epoch": 0.9522666666666667,
        "step": 7142
    },
    {
        "loss": 2.1941,
        "grad_norm": 4.096092224121094,
        "learning_rate": 0.0001233521860383796,
        "epoch": 0.9524,
        "step": 7143
    },
    {
        "loss": 2.0063,
        "grad_norm": 2.404080629348755,
        "learning_rate": 0.00012329098477322146,
        "epoch": 0.9525333333333333,
        "step": 7144
    },
    {
        "loss": 2.224,
        "grad_norm": 3.7605412006378174,
        "learning_rate": 0.00012322977428242483,
        "epoch": 0.9526666666666667,
        "step": 7145
    },
    {
        "loss": 1.8178,
        "grad_norm": 3.825869560241699,
        "learning_rate": 0.00012316855459023544,
        "epoch": 0.9528,
        "step": 7146
    },
    {
        "loss": 2.1622,
        "grad_norm": 4.140796661376953,
        "learning_rate": 0.0001231073257209027,
        "epoch": 0.9529333333333333,
        "step": 7147
    },
    {
        "loss": 2.195,
        "grad_norm": 3.194556474685669,
        "learning_rate": 0.0001230460876986794,
        "epoch": 0.9530666666666666,
        "step": 7148
    },
    {
        "loss": 1.6518,
        "grad_norm": 5.128961563110352,
        "learning_rate": 0.00012298484054782234,
        "epoch": 0.9532,
        "step": 7149
    },
    {
        "loss": 2.3333,
        "grad_norm": 3.7386460304260254,
        "learning_rate": 0.00012292358429259157,
        "epoch": 0.9533333333333334,
        "step": 7150
    },
    {
        "loss": 2.451,
        "grad_norm": 3.4089438915252686,
        "learning_rate": 0.000122862318957251,
        "epoch": 0.9534666666666667,
        "step": 7151
    },
    {
        "loss": 2.7724,
        "grad_norm": 2.5256247520446777,
        "learning_rate": 0.00012280104456606792,
        "epoch": 0.9536,
        "step": 7152
    },
    {
        "loss": 0.9413,
        "grad_norm": 3.754584312438965,
        "learning_rate": 0.00012273976114331352,
        "epoch": 0.9537333333333333,
        "step": 7153
    },
    {
        "loss": 2.8414,
        "grad_norm": 3.3483805656433105,
        "learning_rate": 0.00012267846871326208,
        "epoch": 0.9538666666666666,
        "step": 7154
    },
    {
        "loss": 1.8592,
        "grad_norm": 3.2705302238464355,
        "learning_rate": 0.000122617167300192,
        "epoch": 0.954,
        "step": 7155
    },
    {
        "loss": 2.4465,
        "grad_norm": 3.2218997478485107,
        "learning_rate": 0.0001225558569283849,
        "epoch": 0.9541333333333334,
        "step": 7156
    },
    {
        "loss": 2.2775,
        "grad_norm": 2.6252758502960205,
        "learning_rate": 0.00012249453762212597,
        "epoch": 0.9542666666666667,
        "step": 7157
    },
    {
        "loss": 1.669,
        "grad_norm": 2.942412853240967,
        "learning_rate": 0.00012243320940570404,
        "epoch": 0.9544,
        "step": 7158
    },
    {
        "loss": 2.7798,
        "grad_norm": 2.8790242671966553,
        "learning_rate": 0.00012237187230341147,
        "epoch": 0.9545333333333333,
        "step": 7159
    },
    {
        "loss": 2.1915,
        "grad_norm": 3.2564563751220703,
        "learning_rate": 0.000122310526339544,
        "epoch": 0.9546666666666667,
        "step": 7160
    },
    {
        "loss": 1.6986,
        "grad_norm": 2.621084213256836,
        "learning_rate": 0.0001222491715384011,
        "epoch": 0.9548,
        "step": 7161
    },
    {
        "loss": 2.4817,
        "grad_norm": 3.1797027587890625,
        "learning_rate": 0.00012218780792428547,
        "epoch": 0.9549333333333333,
        "step": 7162
    },
    {
        "loss": 2.2865,
        "grad_norm": 2.872915744781494,
        "learning_rate": 0.0001221264355215036,
        "epoch": 0.9550666666666666,
        "step": 7163
    },
    {
        "loss": 2.6496,
        "grad_norm": 3.462130069732666,
        "learning_rate": 0.00012206505435436503,
        "epoch": 0.9552,
        "step": 7164
    },
    {
        "loss": 2.102,
        "grad_norm": 4.313388347625732,
        "learning_rate": 0.00012200366444718344,
        "epoch": 0.9553333333333334,
        "step": 7165
    },
    {
        "loss": 2.0131,
        "grad_norm": 3.554337978363037,
        "learning_rate": 0.0001219422658242753,
        "epoch": 0.9554666666666667,
        "step": 7166
    },
    {
        "loss": 1.0436,
        "grad_norm": 4.305542945861816,
        "learning_rate": 0.00012188085850996092,
        "epoch": 0.9556,
        "step": 7167
    },
    {
        "loss": 2.0279,
        "grad_norm": 2.825943946838379,
        "learning_rate": 0.00012181944252856394,
        "epoch": 0.9557333333333333,
        "step": 7168
    },
    {
        "loss": 2.6404,
        "grad_norm": 3.9841256141662598,
        "learning_rate": 0.00012175801790441142,
        "epoch": 0.9558666666666666,
        "step": 7169
    },
    {
        "loss": 1.8193,
        "grad_norm": 2.8313398361206055,
        "learning_rate": 0.00012169658466183392,
        "epoch": 0.956,
        "step": 7170
    },
    {
        "loss": 3.1891,
        "grad_norm": 2.0729403495788574,
        "learning_rate": 0.00012163514282516521,
        "epoch": 0.9561333333333333,
        "step": 7171
    },
    {
        "loss": 2.0974,
        "grad_norm": 3.280156373977661,
        "learning_rate": 0.0001215736924187428,
        "epoch": 0.9562666666666667,
        "step": 7172
    },
    {
        "loss": 2.1956,
        "grad_norm": 3.3705742359161377,
        "learning_rate": 0.00012151223346690717,
        "epoch": 0.9564,
        "step": 7173
    },
    {
        "loss": 2.4365,
        "grad_norm": 5.1412034034729,
        "learning_rate": 0.00012145076599400271,
        "epoch": 0.9565333333333333,
        "step": 7174
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.6700191497802734,
        "learning_rate": 0.00012138929002437664,
        "epoch": 0.9566666666666667,
        "step": 7175
    },
    {
        "loss": 2.4899,
        "grad_norm": 3.11661434173584,
        "learning_rate": 0.00012132780558238002,
        "epoch": 0.9568,
        "step": 7176
    },
    {
        "loss": 2.6335,
        "grad_norm": 4.1599202156066895,
        "learning_rate": 0.00012126631269236686,
        "epoch": 0.9569333333333333,
        "step": 7177
    },
    {
        "loss": 2.8494,
        "grad_norm": 3.169008255004883,
        "learning_rate": 0.00012120481137869486,
        "epoch": 0.9570666666666666,
        "step": 7178
    },
    {
        "loss": 2.8343,
        "grad_norm": 3.2329697608947754,
        "learning_rate": 0.00012114330166572472,
        "epoch": 0.9572,
        "step": 7179
    },
    {
        "loss": 1.115,
        "grad_norm": 4.085865497589111,
        "learning_rate": 0.00012108178357782077,
        "epoch": 0.9573333333333334,
        "step": 7180
    },
    {
        "loss": 3.096,
        "grad_norm": 3.251824140548706,
        "learning_rate": 0.00012102025713935057,
        "epoch": 0.9574666666666667,
        "step": 7181
    },
    {
        "loss": 2.093,
        "grad_norm": 3.928499937057495,
        "learning_rate": 0.00012095872237468481,
        "epoch": 0.9576,
        "step": 7182
    },
    {
        "loss": 2.2748,
        "grad_norm": 3.3116893768310547,
        "learning_rate": 0.0001208971793081977,
        "epoch": 0.9577333333333333,
        "step": 7183
    },
    {
        "loss": 2.3874,
        "grad_norm": 4.0069499015808105,
        "learning_rate": 0.00012083562796426675,
        "epoch": 0.9578666666666666,
        "step": 7184
    },
    {
        "loss": 2.6514,
        "grad_norm": 2.0678422451019287,
        "learning_rate": 0.00012077406836727252,
        "epoch": 0.958,
        "step": 7185
    },
    {
        "loss": 1.4355,
        "grad_norm": 3.9977006912231445,
        "learning_rate": 0.00012071250054159897,
        "epoch": 0.9581333333333333,
        "step": 7186
    },
    {
        "loss": 1.4691,
        "grad_norm": 5.87991189956665,
        "learning_rate": 0.00012065092451163349,
        "epoch": 0.9582666666666667,
        "step": 7187
    },
    {
        "loss": 2.4351,
        "grad_norm": 2.7185754776000977,
        "learning_rate": 0.00012058934030176636,
        "epoch": 0.9584,
        "step": 7188
    },
    {
        "loss": 2.415,
        "grad_norm": 3.4636387825012207,
        "learning_rate": 0.00012052774793639136,
        "epoch": 0.9585333333333333,
        "step": 7189
    },
    {
        "loss": 1.6171,
        "grad_norm": 3.520075559616089,
        "learning_rate": 0.00012046614743990556,
        "epoch": 0.9586666666666667,
        "step": 7190
    },
    {
        "loss": 2.2598,
        "grad_norm": 4.608343124389648,
        "learning_rate": 0.00012040453883670894,
        "epoch": 0.9588,
        "step": 7191
    },
    {
        "loss": 2.6689,
        "grad_norm": 2.9188804626464844,
        "learning_rate": 0.00012034292215120497,
        "epoch": 0.9589333333333333,
        "step": 7192
    },
    {
        "loss": 2.35,
        "grad_norm": 3.028660535812378,
        "learning_rate": 0.00012028129740780025,
        "epoch": 0.9590666666666666,
        "step": 7193
    },
    {
        "loss": 1.799,
        "grad_norm": 3.263596296310425,
        "learning_rate": 0.0001202196646309045,
        "epoch": 0.9592,
        "step": 7194
    },
    {
        "loss": 2.7633,
        "grad_norm": 3.455061674118042,
        "learning_rate": 0.00012015802384493073,
        "epoch": 0.9593333333333334,
        "step": 7195
    },
    {
        "loss": 2.3894,
        "grad_norm": 3.014400005340576,
        "learning_rate": 0.00012009637507429498,
        "epoch": 0.9594666666666667,
        "step": 7196
    },
    {
        "loss": 2.5256,
        "grad_norm": 3.3700907230377197,
        "learning_rate": 0.00012003471834341665,
        "epoch": 0.9596,
        "step": 7197
    },
    {
        "loss": 2.3652,
        "grad_norm": 3.0567286014556885,
        "learning_rate": 0.00011997305367671799,
        "epoch": 0.9597333333333333,
        "step": 7198
    },
    {
        "loss": 2.7736,
        "grad_norm": 2.5453901290893555,
        "learning_rate": 0.00011991138109862484,
        "epoch": 0.9598666666666666,
        "step": 7199
    },
    {
        "loss": 1.7658,
        "grad_norm": 4.416884899139404,
        "learning_rate": 0.00011984970063356569,
        "epoch": 0.96,
        "step": 7200
    },
    {
        "loss": 2.5541,
        "grad_norm": 2.585447072982788,
        "learning_rate": 0.00011978801230597259,
        "epoch": 0.9601333333333333,
        "step": 7201
    },
    {
        "loss": 2.0202,
        "grad_norm": 3.3824028968811035,
        "learning_rate": 0.00011972631614028032,
        "epoch": 0.9602666666666667,
        "step": 7202
    },
    {
        "loss": 2.2524,
        "grad_norm": 3.5930793285369873,
        "learning_rate": 0.00011966461216092706,
        "epoch": 0.9604,
        "step": 7203
    },
    {
        "loss": 2.3095,
        "grad_norm": 3.1807522773742676,
        "learning_rate": 0.00011960290039235384,
        "epoch": 0.9605333333333334,
        "step": 7204
    },
    {
        "loss": 1.6866,
        "grad_norm": 5.354726314544678,
        "learning_rate": 0.000119541180859005,
        "epoch": 0.9606666666666667,
        "step": 7205
    },
    {
        "loss": 1.8149,
        "grad_norm": 3.622194766998291,
        "learning_rate": 0.0001194794535853279,
        "epoch": 0.9608,
        "step": 7206
    },
    {
        "loss": 1.0394,
        "grad_norm": 3.118924617767334,
        "learning_rate": 0.00011941771859577273,
        "epoch": 0.9609333333333333,
        "step": 7207
    },
    {
        "loss": 2.6422,
        "grad_norm": 3.6274125576019287,
        "learning_rate": 0.00011935597591479319,
        "epoch": 0.9610666666666666,
        "step": 7208
    },
    {
        "loss": 2.5818,
        "grad_norm": 3.295067548751831,
        "learning_rate": 0.00011929422556684557,
        "epoch": 0.9612,
        "step": 7209
    },
    {
        "loss": 2.6712,
        "grad_norm": 3.4263927936553955,
        "learning_rate": 0.00011923246757638953,
        "epoch": 0.9613333333333334,
        "step": 7210
    },
    {
        "loss": 2.7124,
        "grad_norm": 2.6910464763641357,
        "learning_rate": 0.00011917070196788746,
        "epoch": 0.9614666666666667,
        "step": 7211
    },
    {
        "loss": 2.9934,
        "grad_norm": 1.9524792432785034,
        "learning_rate": 0.00011910892876580513,
        "epoch": 0.9616,
        "step": 7212
    },
    {
        "loss": 2.0242,
        "grad_norm": 3.093977451324463,
        "learning_rate": 0.00011904714799461092,
        "epoch": 0.9617333333333333,
        "step": 7213
    },
    {
        "loss": 1.3364,
        "grad_norm": 3.7332167625427246,
        "learning_rate": 0.00011898535967877648,
        "epoch": 0.9618666666666666,
        "step": 7214
    },
    {
        "loss": 3.1365,
        "grad_norm": 2.8304319381713867,
        "learning_rate": 0.00011892356384277644,
        "epoch": 0.962,
        "step": 7215
    },
    {
        "loss": 1.7519,
        "grad_norm": 2.4290926456451416,
        "learning_rate": 0.00011886176051108821,
        "epoch": 0.9621333333333333,
        "step": 7216
    },
    {
        "loss": 2.2454,
        "grad_norm": 3.7174036502838135,
        "learning_rate": 0.00011879994970819239,
        "epoch": 0.9622666666666667,
        "step": 7217
    },
    {
        "loss": 2.1538,
        "grad_norm": 3.4430978298187256,
        "learning_rate": 0.00011873813145857249,
        "epoch": 0.9624,
        "step": 7218
    },
    {
        "loss": 2.3549,
        "grad_norm": 3.7639248371124268,
        "learning_rate": 0.0001186763057867148,
        "epoch": 0.9625333333333334,
        "step": 7219
    },
    {
        "loss": 3.0046,
        "grad_norm": 3.413637161254883,
        "learning_rate": 0.00011861447271710884,
        "epoch": 0.9626666666666667,
        "step": 7220
    },
    {
        "loss": 2.3661,
        "grad_norm": 3.429947853088379,
        "learning_rate": 0.00011855263227424671,
        "epoch": 0.9628,
        "step": 7221
    },
    {
        "loss": 1.2079,
        "grad_norm": 3.803497791290283,
        "learning_rate": 0.00011849078448262381,
        "epoch": 0.9629333333333333,
        "step": 7222
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.9200050830841064,
        "learning_rate": 0.00011842892936673801,
        "epoch": 0.9630666666666666,
        "step": 7223
    },
    {
        "loss": 2.7681,
        "grad_norm": 2.8056015968322754,
        "learning_rate": 0.00011836706695109065,
        "epoch": 0.9632,
        "step": 7224
    },
    {
        "loss": 1.419,
        "grad_norm": 2.9500715732574463,
        "learning_rate": 0.00011830519726018543,
        "epoch": 0.9633333333333334,
        "step": 7225
    },
    {
        "loss": 2.7718,
        "grad_norm": 3.7585296630859375,
        "learning_rate": 0.00011824332031852916,
        "epoch": 0.9634666666666667,
        "step": 7226
    },
    {
        "loss": 2.3816,
        "grad_norm": 2.7501091957092285,
        "learning_rate": 0.00011818143615063167,
        "epoch": 0.9636,
        "step": 7227
    },
    {
        "loss": 1.9648,
        "grad_norm": 3.270352840423584,
        "learning_rate": 0.00011811954478100526,
        "epoch": 0.9637333333333333,
        "step": 7228
    },
    {
        "loss": 1.6045,
        "grad_norm": 3.613454580307007,
        "learning_rate": 0.0001180576462341655,
        "epoch": 0.9638666666666666,
        "step": 7229
    },
    {
        "loss": 2.3796,
        "grad_norm": 2.9349358081817627,
        "learning_rate": 0.00011799574053463047,
        "epoch": 0.964,
        "step": 7230
    },
    {
        "loss": 2.2475,
        "grad_norm": 3.433520793914795,
        "learning_rate": 0.00011793382770692136,
        "epoch": 0.9641333333333333,
        "step": 7231
    },
    {
        "loss": 1.6471,
        "grad_norm": 2.5098702907562256,
        "learning_rate": 0.00011787190777556187,
        "epoch": 0.9642666666666667,
        "step": 7232
    },
    {
        "loss": 2.0235,
        "grad_norm": 4.926990509033203,
        "learning_rate": 0.00011780998076507892,
        "epoch": 0.9644,
        "step": 7233
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.9614155292510986,
        "learning_rate": 0.00011774804670000186,
        "epoch": 0.9645333333333334,
        "step": 7234
    },
    {
        "loss": 1.6484,
        "grad_norm": 6.466494083404541,
        "learning_rate": 0.00011768610560486308,
        "epoch": 0.9646666666666667,
        "step": 7235
    },
    {
        "loss": 2.5027,
        "grad_norm": 2.8797028064727783,
        "learning_rate": 0.00011762415750419755,
        "epoch": 0.9648,
        "step": 7236
    },
    {
        "loss": 0.7029,
        "grad_norm": 3.11381459236145,
        "learning_rate": 0.00011756220242254323,
        "epoch": 0.9649333333333333,
        "step": 7237
    },
    {
        "loss": 2.7943,
        "grad_norm": 3.624448299407959,
        "learning_rate": 0.00011750024038444065,
        "epoch": 0.9650666666666666,
        "step": 7238
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.9146764278411865,
        "learning_rate": 0.0001174382714144332,
        "epoch": 0.9652,
        "step": 7239
    },
    {
        "loss": 2.4604,
        "grad_norm": 3.4997730255126953,
        "learning_rate": 0.00011737629553706708,
        "epoch": 0.9653333333333334,
        "step": 7240
    },
    {
        "loss": 2.8757,
        "grad_norm": 3.737293243408203,
        "learning_rate": 0.00011731431277689104,
        "epoch": 0.9654666666666667,
        "step": 7241
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.648885488510132,
        "learning_rate": 0.00011725232315845671,
        "epoch": 0.9656,
        "step": 7242
    },
    {
        "loss": 2.0278,
        "grad_norm": 3.4169247150421143,
        "learning_rate": 0.00011719032670631849,
        "epoch": 0.9657333333333333,
        "step": 7243
    },
    {
        "loss": 1.7494,
        "grad_norm": 2.537177562713623,
        "learning_rate": 0.00011712832344503315,
        "epoch": 0.9658666666666667,
        "step": 7244
    },
    {
        "loss": 2.3137,
        "grad_norm": 3.707371711730957,
        "learning_rate": 0.00011706631339916065,
        "epoch": 0.966,
        "step": 7245
    },
    {
        "loss": 2.1929,
        "grad_norm": 3.9248549938201904,
        "learning_rate": 0.00011700429659326318,
        "epoch": 0.9661333333333333,
        "step": 7246
    },
    {
        "loss": 2.5582,
        "grad_norm": 4.495736122131348,
        "learning_rate": 0.00011694227305190588,
        "epoch": 0.9662666666666667,
        "step": 7247
    },
    {
        "loss": 2.9339,
        "grad_norm": 2.198322057723999,
        "learning_rate": 0.0001168802427996565,
        "epoch": 0.9664,
        "step": 7248
    },
    {
        "loss": 2.6478,
        "grad_norm": 2.6060867309570312,
        "learning_rate": 0.00011681820586108552,
        "epoch": 0.9665333333333334,
        "step": 7249
    },
    {
        "loss": 1.6002,
        "grad_norm": 2.5043699741363525,
        "learning_rate": 0.0001167561622607658,
        "epoch": 0.9666666666666667,
        "step": 7250
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.2372782230377197,
        "learning_rate": 0.0001166941120232731,
        "epoch": 0.9668,
        "step": 7251
    },
    {
        "loss": 2.0738,
        "grad_norm": 2.516721248626709,
        "learning_rate": 0.00011663205517318582,
        "epoch": 0.9669333333333333,
        "step": 7252
    },
    {
        "loss": 1.4236,
        "grad_norm": 4.618121147155762,
        "learning_rate": 0.00011656999173508478,
        "epoch": 0.9670666666666666,
        "step": 7253
    },
    {
        "loss": 2.7442,
        "grad_norm": 4.341386318206787,
        "learning_rate": 0.00011650792173355358,
        "epoch": 0.9672,
        "step": 7254
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.309617519378662,
        "learning_rate": 0.00011644584519317828,
        "epoch": 0.9673333333333334,
        "step": 7255
    },
    {
        "loss": 2.2563,
        "grad_norm": 4.196689605712891,
        "learning_rate": 0.00011638376213854773,
        "epoch": 0.9674666666666667,
        "step": 7256
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.8464415073394775,
        "learning_rate": 0.00011632167259425304,
        "epoch": 0.9676,
        "step": 7257
    },
    {
        "loss": 2.7404,
        "grad_norm": 4.636543273925781,
        "learning_rate": 0.00011625957658488833,
        "epoch": 0.9677333333333333,
        "step": 7258
    },
    {
        "loss": 2.0927,
        "grad_norm": 5.298929691314697,
        "learning_rate": 0.00011619747413504989,
        "epoch": 0.9678666666666667,
        "step": 7259
    },
    {
        "loss": 2.1084,
        "grad_norm": 4.073262691497803,
        "learning_rate": 0.00011613536526933682,
        "epoch": 0.968,
        "step": 7260
    },
    {
        "loss": 2.034,
        "grad_norm": 5.945117473602295,
        "learning_rate": 0.00011607325001235051,
        "epoch": 0.9681333333333333,
        "step": 7261
    },
    {
        "loss": 1.5001,
        "grad_norm": 3.867286205291748,
        "learning_rate": 0.00011601112838869517,
        "epoch": 0.9682666666666667,
        "step": 7262
    },
    {
        "loss": 2.9144,
        "grad_norm": 2.4700584411621094,
        "learning_rate": 0.00011594900042297726,
        "epoch": 0.9684,
        "step": 7263
    },
    {
        "loss": 0.7762,
        "grad_norm": 2.5561370849609375,
        "learning_rate": 0.00011588686613980592,
        "epoch": 0.9685333333333334,
        "step": 7264
    },
    {
        "loss": 1.9215,
        "grad_norm": 3.671313762664795,
        "learning_rate": 0.00011582472556379285,
        "epoch": 0.9686666666666667,
        "step": 7265
    },
    {
        "loss": 0.907,
        "grad_norm": 3.4677181243896484,
        "learning_rate": 0.00011576257871955192,
        "epoch": 0.9688,
        "step": 7266
    },
    {
        "loss": 2.3954,
        "grad_norm": 3.0704989433288574,
        "learning_rate": 0.00011570042563169998,
        "epoch": 0.9689333333333333,
        "step": 7267
    },
    {
        "loss": 1.9344,
        "grad_norm": 5.881653785705566,
        "learning_rate": 0.0001156382663248559,
        "epoch": 0.9690666666666666,
        "step": 7268
    },
    {
        "loss": 2.0196,
        "grad_norm": 5.646326065063477,
        "learning_rate": 0.0001155761008236413,
        "epoch": 0.9692,
        "step": 7269
    },
    {
        "loss": 1.1599,
        "grad_norm": 3.082770347595215,
        "learning_rate": 0.00011551392915268002,
        "epoch": 0.9693333333333334,
        "step": 7270
    },
    {
        "loss": 1.9705,
        "grad_norm": 4.239052772521973,
        "learning_rate": 0.00011545175133659863,
        "epoch": 0.9694666666666667,
        "step": 7271
    },
    {
        "loss": 2.3281,
        "grad_norm": 3.47407603263855,
        "learning_rate": 0.0001153895674000258,
        "epoch": 0.9696,
        "step": 7272
    },
    {
        "loss": 2.7551,
        "grad_norm": 2.603945255279541,
        "learning_rate": 0.00011532737736759286,
        "epoch": 0.9697333333333333,
        "step": 7273
    },
    {
        "loss": 1.5851,
        "grad_norm": 4.431077003479004,
        "learning_rate": 0.00011526518126393362,
        "epoch": 0.9698666666666667,
        "step": 7274
    },
    {
        "loss": 2.1285,
        "grad_norm": 4.449250221252441,
        "learning_rate": 0.00011520297911368398,
        "epoch": 0.97,
        "step": 7275
    },
    {
        "loss": 2.4023,
        "grad_norm": 2.571080446243286,
        "learning_rate": 0.00011514077094148253,
        "epoch": 0.9701333333333333,
        "step": 7276
    },
    {
        "loss": 2.6744,
        "grad_norm": 2.7902376651763916,
        "learning_rate": 0.00011507855677197017,
        "epoch": 0.9702666666666667,
        "step": 7277
    },
    {
        "loss": 2.8243,
        "grad_norm": 3.3013076782226562,
        "learning_rate": 0.00011501633662979003,
        "epoch": 0.9704,
        "step": 7278
    },
    {
        "loss": 2.9327,
        "grad_norm": 3.0320162773132324,
        "learning_rate": 0.00011495411053958787,
        "epoch": 0.9705333333333334,
        "step": 7279
    },
    {
        "loss": 2.3502,
        "grad_norm": 5.015780925750732,
        "learning_rate": 0.00011489187852601147,
        "epoch": 0.9706666666666667,
        "step": 7280
    },
    {
        "loss": 2.3948,
        "grad_norm": 9.975984573364258,
        "learning_rate": 0.00011482964061371131,
        "epoch": 0.9708,
        "step": 7281
    },
    {
        "loss": 2.5134,
        "grad_norm": 3.498530387878418,
        "learning_rate": 0.00011476739682733988,
        "epoch": 0.9709333333333333,
        "step": 7282
    },
    {
        "loss": 2.4729,
        "grad_norm": 3.1144206523895264,
        "learning_rate": 0.00011470514719155234,
        "epoch": 0.9710666666666666,
        "step": 7283
    },
    {
        "loss": 2.7744,
        "grad_norm": 4.318483352661133,
        "learning_rate": 0.00011464289173100584,
        "epoch": 0.9712,
        "step": 7284
    },
    {
        "loss": 1.5342,
        "grad_norm": 4.253981590270996,
        "learning_rate": 0.0001145806304703601,
        "epoch": 0.9713333333333334,
        "step": 7285
    },
    {
        "loss": 2.7701,
        "grad_norm": 3.569072961807251,
        "learning_rate": 0.00011451836343427689,
        "epoch": 0.9714666666666667,
        "step": 7286
    },
    {
        "loss": 1.8584,
        "grad_norm": 4.473750591278076,
        "learning_rate": 0.00011445609064742043,
        "epoch": 0.9716,
        "step": 7287
    },
    {
        "loss": 0.9476,
        "grad_norm": 4.4064764976501465,
        "learning_rate": 0.00011439381213445727,
        "epoch": 0.9717333333333333,
        "step": 7288
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.7477320432662964,
        "learning_rate": 0.00011433152792005603,
        "epoch": 0.9718666666666667,
        "step": 7289
    },
    {
        "loss": 2.4184,
        "grad_norm": 3.5301382541656494,
        "learning_rate": 0.00011426923802888783,
        "epoch": 0.972,
        "step": 7290
    },
    {
        "loss": 2.5694,
        "grad_norm": 3.6905722618103027,
        "learning_rate": 0.00011420694248562567,
        "epoch": 0.9721333333333333,
        "step": 7291
    },
    {
        "loss": 2.2341,
        "grad_norm": 2.988043785095215,
        "learning_rate": 0.0001141446413149453,
        "epoch": 0.9722666666666666,
        "step": 7292
    },
    {
        "loss": 1.5358,
        "grad_norm": 5.584383487701416,
        "learning_rate": 0.00011408233454152428,
        "epoch": 0.9724,
        "step": 7293
    },
    {
        "loss": 2.2736,
        "grad_norm": 2.7135629653930664,
        "learning_rate": 0.00011402002219004259,
        "epoch": 0.9725333333333334,
        "step": 7294
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.4060792922973633,
        "learning_rate": 0.0001139577042851823,
        "epoch": 0.9726666666666667,
        "step": 7295
    },
    {
        "loss": 2.7333,
        "grad_norm": 2.939096450805664,
        "learning_rate": 0.00011389538085162786,
        "epoch": 0.9728,
        "step": 7296
    },
    {
        "loss": 2.0953,
        "grad_norm": 3.6421759128570557,
        "learning_rate": 0.00011383305191406564,
        "epoch": 0.9729333333333333,
        "step": 7297
    },
    {
        "loss": 2.9319,
        "grad_norm": 2.454294204711914,
        "learning_rate": 0.0001137707174971844,
        "epoch": 0.9730666666666666,
        "step": 7298
    },
    {
        "loss": 2.8131,
        "grad_norm": 3.426304340362549,
        "learning_rate": 0.0001137083776256751,
        "epoch": 0.9732,
        "step": 7299
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.5565826892852783,
        "learning_rate": 0.00011364603232423068,
        "epoch": 0.9733333333333334,
        "step": 7300
    },
    {
        "loss": 2.5354,
        "grad_norm": 2.7568986415863037,
        "learning_rate": 0.00011358368161754633,
        "epoch": 0.9734666666666667,
        "step": 7301
    },
    {
        "loss": 2.3374,
        "grad_norm": 4.16827392578125,
        "learning_rate": 0.00011352132553031948,
        "epoch": 0.9736,
        "step": 7302
    },
    {
        "loss": 2.5228,
        "grad_norm": 4.135947227478027,
        "learning_rate": 0.00011345896408724947,
        "epoch": 0.9737333333333333,
        "step": 7303
    },
    {
        "loss": 2.3619,
        "grad_norm": 2.9099743366241455,
        "learning_rate": 0.00011339659731303798,
        "epoch": 0.9738666666666667,
        "step": 7304
    },
    {
        "loss": 2.3759,
        "grad_norm": 3.2799854278564453,
        "learning_rate": 0.00011333422523238856,
        "epoch": 0.974,
        "step": 7305
    },
    {
        "loss": 2.5307,
        "grad_norm": 3.0602924823760986,
        "learning_rate": 0.00011327184787000711,
        "epoch": 0.9741333333333333,
        "step": 7306
    },
    {
        "loss": 2.5827,
        "grad_norm": 4.179553508758545,
        "learning_rate": 0.00011320946525060149,
        "epoch": 0.9742666666666666,
        "step": 7307
    },
    {
        "loss": 1.7702,
        "grad_norm": 3.9326868057250977,
        "learning_rate": 0.00011314707739888176,
        "epoch": 0.9744,
        "step": 7308
    },
    {
        "loss": 2.632,
        "grad_norm": 3.6825459003448486,
        "learning_rate": 0.00011308468433955977,
        "epoch": 0.9745333333333334,
        "step": 7309
    },
    {
        "loss": 2.1357,
        "grad_norm": 5.720637321472168,
        "learning_rate": 0.00011302228609734975,
        "epoch": 0.9746666666666667,
        "step": 7310
    },
    {
        "loss": 2.0369,
        "grad_norm": 3.9140121936798096,
        "learning_rate": 0.00011295988269696791,
        "epoch": 0.9748,
        "step": 7311
    },
    {
        "loss": 2.8721,
        "grad_norm": 2.487724781036377,
        "learning_rate": 0.00011289747416313227,
        "epoch": 0.9749333333333333,
        "step": 7312
    },
    {
        "loss": 1.5342,
        "grad_norm": 3.140209674835205,
        "learning_rate": 0.00011283506052056328,
        "epoch": 0.9750666666666666,
        "step": 7313
    },
    {
        "loss": 2.4779,
        "grad_norm": 4.115108966827393,
        "learning_rate": 0.00011277264179398299,
        "epoch": 0.9752,
        "step": 7314
    },
    {
        "loss": 2.4717,
        "grad_norm": 2.978792667388916,
        "learning_rate": 0.00011271021800811586,
        "epoch": 0.9753333333333334,
        "step": 7315
    },
    {
        "loss": 2.5286,
        "grad_norm": 3.591465473175049,
        "learning_rate": 0.00011264778918768788,
        "epoch": 0.9754666666666667,
        "step": 7316
    },
    {
        "loss": 2.52,
        "grad_norm": 6.7191619873046875,
        "learning_rate": 0.00011258535535742767,
        "epoch": 0.9756,
        "step": 7317
    },
    {
        "loss": 1.5421,
        "grad_norm": 3.918565273284912,
        "learning_rate": 0.00011252291654206526,
        "epoch": 0.9757333333333333,
        "step": 7318
    },
    {
        "loss": 2.5604,
        "grad_norm": 1.979816198348999,
        "learning_rate": 0.000112460472766333,
        "epoch": 0.9758666666666667,
        "step": 7319
    },
    {
        "loss": 2.121,
        "grad_norm": 2.731689929962158,
        "learning_rate": 0.00011239802405496496,
        "epoch": 0.976,
        "step": 7320
    },
    {
        "loss": 2.2613,
        "grad_norm": 2.287853956222534,
        "learning_rate": 0.0001123355704326974,
        "epoch": 0.9761333333333333,
        "step": 7321
    },
    {
        "loss": 2.1244,
        "grad_norm": 3.794588327407837,
        "learning_rate": 0.0001122731119242683,
        "epoch": 0.9762666666666666,
        "step": 7322
    },
    {
        "loss": 2.0409,
        "grad_norm": 3.4621026515960693,
        "learning_rate": 0.00011221064855441772,
        "epoch": 0.9764,
        "step": 7323
    },
    {
        "loss": 2.7102,
        "grad_norm": 2.158358335494995,
        "learning_rate": 0.00011214818034788774,
        "epoch": 0.9765333333333334,
        "step": 7324
    },
    {
        "loss": 1.0514,
        "grad_norm": 2.318263053894043,
        "learning_rate": 0.00011208570732942205,
        "epoch": 0.9766666666666667,
        "step": 7325
    },
    {
        "loss": 1.0572,
        "grad_norm": 3.8246583938598633,
        "learning_rate": 0.00011202322952376654,
        "epoch": 0.9768,
        "step": 7326
    },
    {
        "loss": 2.909,
        "grad_norm": 5.589982032775879,
        "learning_rate": 0.00011196074695566878,
        "epoch": 0.9769333333333333,
        "step": 7327
    },
    {
        "loss": 1.6572,
        "grad_norm": 3.191619396209717,
        "learning_rate": 0.00011189825964987852,
        "epoch": 0.9770666666666666,
        "step": 7328
    },
    {
        "loss": 3.494,
        "grad_norm": 4.11953592300415,
        "learning_rate": 0.00011183576763114698,
        "epoch": 0.9772,
        "step": 7329
    },
    {
        "loss": 2.4994,
        "grad_norm": 4.8632283210754395,
        "learning_rate": 0.00011177327092422762,
        "epoch": 0.9773333333333334,
        "step": 7330
    },
    {
        "loss": 2.0443,
        "grad_norm": 4.061253547668457,
        "learning_rate": 0.00011171076955387547,
        "epoch": 0.9774666666666667,
        "step": 7331
    },
    {
        "loss": 1.0681,
        "grad_norm": 3.2775208950042725,
        "learning_rate": 0.00011164826354484759,
        "epoch": 0.9776,
        "step": 7332
    },
    {
        "loss": 1.7132,
        "grad_norm": 3.9220001697540283,
        "learning_rate": 0.0001115857529219029,
        "epoch": 0.9777333333333333,
        "step": 7333
    },
    {
        "loss": 1.5887,
        "grad_norm": 4.565184116363525,
        "learning_rate": 0.00011152323770980193,
        "epoch": 0.9778666666666667,
        "step": 7334
    },
    {
        "loss": 3.0015,
        "grad_norm": 3.2067646980285645,
        "learning_rate": 0.00011146071793330724,
        "epoch": 0.978,
        "step": 7335
    },
    {
        "loss": 3.1928,
        "grad_norm": 5.32854700088501,
        "learning_rate": 0.0001113981936171832,
        "epoch": 0.9781333333333333,
        "step": 7336
    },
    {
        "loss": 2.3629,
        "grad_norm": 3.7159481048583984,
        "learning_rate": 0.00011133566478619575,
        "epoch": 0.9782666666666666,
        "step": 7337
    },
    {
        "loss": 2.8532,
        "grad_norm": 2.2935597896575928,
        "learning_rate": 0.00011127313146511291,
        "epoch": 0.9784,
        "step": 7338
    },
    {
        "loss": 2.3454,
        "grad_norm": 2.508399248123169,
        "learning_rate": 0.00011121059367870423,
        "epoch": 0.9785333333333334,
        "step": 7339
    },
    {
        "loss": 2.2825,
        "grad_norm": 2.704010248184204,
        "learning_rate": 0.00011114805145174128,
        "epoch": 0.9786666666666667,
        "step": 7340
    },
    {
        "loss": 1.2608,
        "grad_norm": 3.521845817565918,
        "learning_rate": 0.000111085504808997,
        "epoch": 0.9788,
        "step": 7341
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.466862678527832,
        "learning_rate": 0.00011102295377524666,
        "epoch": 0.9789333333333333,
        "step": 7342
    },
    {
        "loss": 1.9985,
        "grad_norm": 2.958123207092285,
        "learning_rate": 0.00011096039837526669,
        "epoch": 0.9790666666666666,
        "step": 7343
    },
    {
        "loss": 0.4477,
        "grad_norm": 2.264324426651001,
        "learning_rate": 0.00011089783863383563,
        "epoch": 0.9792,
        "step": 7344
    },
    {
        "loss": 2.7089,
        "grad_norm": 3.741672992706299,
        "learning_rate": 0.00011083527457573348,
        "epoch": 0.9793333333333333,
        "step": 7345
    },
    {
        "loss": 2.6699,
        "grad_norm": 4.933826446533203,
        "learning_rate": 0.00011077270622574222,
        "epoch": 0.9794666666666667,
        "step": 7346
    },
    {
        "loss": 1.6762,
        "grad_norm": 2.938753128051758,
        "learning_rate": 0.00011071013360864528,
        "epoch": 0.9796,
        "step": 7347
    },
    {
        "loss": 3.503,
        "grad_norm": 3.4650001525878906,
        "learning_rate": 0.00011064755674922788,
        "epoch": 0.9797333333333333,
        "step": 7348
    },
    {
        "loss": 2.6527,
        "grad_norm": 3.460407257080078,
        "learning_rate": 0.00011058497567227708,
        "epoch": 0.9798666666666667,
        "step": 7349
    },
    {
        "loss": 2.5246,
        "grad_norm": 3.551802396774292,
        "learning_rate": 0.0001105223904025812,
        "epoch": 0.98,
        "step": 7350
    },
    {
        "loss": 2.7785,
        "grad_norm": 2.6037328243255615,
        "learning_rate": 0.00011045980096493075,
        "epoch": 0.9801333333333333,
        "step": 7351
    },
    {
        "loss": 2.1217,
        "grad_norm": 3.4104151725769043,
        "learning_rate": 0.00011039720738411743,
        "epoch": 0.9802666666666666,
        "step": 7352
    },
    {
        "loss": 2.5204,
        "grad_norm": 1.6958802938461304,
        "learning_rate": 0.00011033460968493492,
        "epoch": 0.9804,
        "step": 7353
    },
    {
        "loss": 2.6048,
        "grad_norm": 4.886890411376953,
        "learning_rate": 0.00011027200789217825,
        "epoch": 0.9805333333333334,
        "step": 7354
    },
    {
        "loss": 3.273,
        "grad_norm": 4.153746604919434,
        "learning_rate": 0.00011020940203064429,
        "epoch": 0.9806666666666667,
        "step": 7355
    },
    {
        "loss": 2.3019,
        "grad_norm": 3.2372119426727295,
        "learning_rate": 0.00011014679212513133,
        "epoch": 0.9808,
        "step": 7356
    },
    {
        "loss": 3.2163,
        "grad_norm": 3.89888072013855,
        "learning_rate": 0.00011008417820043944,
        "epoch": 0.9809333333333333,
        "step": 7357
    },
    {
        "loss": 2.4874,
        "grad_norm": 3.5536210536956787,
        "learning_rate": 0.00011002156028137027,
        "epoch": 0.9810666666666666,
        "step": 7358
    },
    {
        "loss": 2.1453,
        "grad_norm": 4.377005577087402,
        "learning_rate": 0.00010995893839272687,
        "epoch": 0.9812,
        "step": 7359
    },
    {
        "loss": 0.7743,
        "grad_norm": 2.4897289276123047,
        "learning_rate": 0.00010989631255931401,
        "epoch": 0.9813333333333333,
        "step": 7360
    },
    {
        "loss": 2.5048,
        "grad_norm": 4.38959264755249,
        "learning_rate": 0.00010983368280593814,
        "epoch": 0.9814666666666667,
        "step": 7361
    },
    {
        "loss": 3.0555,
        "grad_norm": 3.5182344913482666,
        "learning_rate": 0.00010977104915740691,
        "epoch": 0.9816,
        "step": 7362
    },
    {
        "loss": 1.8159,
        "grad_norm": 3.818793773651123,
        "learning_rate": 0.00010970841163852985,
        "epoch": 0.9817333333333333,
        "step": 7363
    },
    {
        "loss": 3.1947,
        "grad_norm": 3.632874011993408,
        "learning_rate": 0.00010964577027411782,
        "epoch": 0.9818666666666667,
        "step": 7364
    },
    {
        "loss": 2.572,
        "grad_norm": 2.682554006576538,
        "learning_rate": 0.00010958312508898339,
        "epoch": 0.982,
        "step": 7365
    },
    {
        "loss": 2.7167,
        "grad_norm": 2.850018262863159,
        "learning_rate": 0.00010952047610794031,
        "epoch": 0.9821333333333333,
        "step": 7366
    },
    {
        "loss": 2.5269,
        "grad_norm": 2.6427152156829834,
        "learning_rate": 0.00010945782335580435,
        "epoch": 0.9822666666666666,
        "step": 7367
    },
    {
        "loss": 2.8281,
        "grad_norm": 4.089602470397949,
        "learning_rate": 0.00010939516685739226,
        "epoch": 0.9824,
        "step": 7368
    },
    {
        "loss": 2.1862,
        "grad_norm": 3.2953052520751953,
        "learning_rate": 0.00010933250663752257,
        "epoch": 0.9825333333333334,
        "step": 7369
    },
    {
        "loss": 3.6501,
        "grad_norm": 4.2906646728515625,
        "learning_rate": 0.00010926984272101526,
        "epoch": 0.9826666666666667,
        "step": 7370
    },
    {
        "loss": 2.8596,
        "grad_norm": 2.781327962875366,
        "learning_rate": 0.0001092071751326916,
        "epoch": 0.9828,
        "step": 7371
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.868220329284668,
        "learning_rate": 0.00010914450389737456,
        "epoch": 0.9829333333333333,
        "step": 7372
    },
    {
        "loss": 2.4274,
        "grad_norm": 2.512437105178833,
        "learning_rate": 0.00010908182903988833,
        "epoch": 0.9830666666666666,
        "step": 7373
    },
    {
        "loss": 2.5652,
        "grad_norm": 4.608838081359863,
        "learning_rate": 0.0001090191505850587,
        "epoch": 0.9832,
        "step": 7374
    },
    {
        "loss": 2.088,
        "grad_norm": 2.800539255142212,
        "learning_rate": 0.00010895646855771267,
        "epoch": 0.9833333333333333,
        "step": 7375
    },
    {
        "loss": 2.4495,
        "grad_norm": 3.0818378925323486,
        "learning_rate": 0.00010889378298267905,
        "epoch": 0.9834666666666667,
        "step": 7376
    },
    {
        "loss": 1.6534,
        "grad_norm": 3.6416194438934326,
        "learning_rate": 0.00010883109388478763,
        "epoch": 0.9836,
        "step": 7377
    },
    {
        "loss": 2.0795,
        "grad_norm": 2.7774133682250977,
        "learning_rate": 0.00010876840128886989,
        "epoch": 0.9837333333333333,
        "step": 7378
    },
    {
        "loss": 2.2615,
        "grad_norm": 3.195833683013916,
        "learning_rate": 0.00010870570521975847,
        "epoch": 0.9838666666666667,
        "step": 7379
    },
    {
        "loss": 2.6279,
        "grad_norm": 3.384174346923828,
        "learning_rate": 0.0001086430057022876,
        "epoch": 0.984,
        "step": 7380
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.629652976989746,
        "learning_rate": 0.00010858030276129265,
        "epoch": 0.9841333333333333,
        "step": 7381
    },
    {
        "loss": 1.7344,
        "grad_norm": 3.814563512802124,
        "learning_rate": 0.00010851759642161056,
        "epoch": 0.9842666666666666,
        "step": 7382
    },
    {
        "loss": 2.3448,
        "grad_norm": 3.3437423706054688,
        "learning_rate": 0.00010845488670807958,
        "epoch": 0.9844,
        "step": 7383
    },
    {
        "loss": 2.236,
        "grad_norm": 3.2150275707244873,
        "learning_rate": 0.00010839217364553908,
        "epoch": 0.9845333333333334,
        "step": 7384
    },
    {
        "loss": 1.9235,
        "grad_norm": 3.6547839641571045,
        "learning_rate": 0.00010832945725882999,
        "epoch": 0.9846666666666667,
        "step": 7385
    },
    {
        "loss": 2.6726,
        "grad_norm": 3.04913330078125,
        "learning_rate": 0.00010826673757279457,
        "epoch": 0.9848,
        "step": 7386
    },
    {
        "loss": 2.8979,
        "grad_norm": 2.6406009197235107,
        "learning_rate": 0.00010820401461227619,
        "epoch": 0.9849333333333333,
        "step": 7387
    },
    {
        "loss": 2.5331,
        "grad_norm": 3.3981432914733887,
        "learning_rate": 0.00010814128840211963,
        "epoch": 0.9850666666666666,
        "step": 7388
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.0728442668914795,
        "learning_rate": 0.00010807855896717107,
        "epoch": 0.9852,
        "step": 7389
    },
    {
        "loss": 1.6072,
        "grad_norm": 3.987159252166748,
        "learning_rate": 0.00010801582633227774,
        "epoch": 0.9853333333333333,
        "step": 7390
    },
    {
        "loss": 2.2964,
        "grad_norm": 3.627113103866577,
        "learning_rate": 0.00010795309052228826,
        "epoch": 0.9854666666666667,
        "step": 7391
    },
    {
        "loss": 2.144,
        "grad_norm": 3.585984468460083,
        "learning_rate": 0.00010789035156205258,
        "epoch": 0.9856,
        "step": 7392
    },
    {
        "loss": 2.0127,
        "grad_norm": 3.9187612533569336,
        "learning_rate": 0.00010782760947642172,
        "epoch": 0.9857333333333334,
        "step": 7393
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.686447858810425,
        "learning_rate": 0.00010776486429024806,
        "epoch": 0.9858666666666667,
        "step": 7394
    },
    {
        "loss": 1.8118,
        "grad_norm": 3.4716782569885254,
        "learning_rate": 0.00010770211602838527,
        "epoch": 0.986,
        "step": 7395
    },
    {
        "loss": 2.3715,
        "grad_norm": 3.506094217300415,
        "learning_rate": 0.000107639364715688,
        "epoch": 0.9861333333333333,
        "step": 7396
    },
    {
        "loss": 2.3143,
        "grad_norm": 2.073751211166382,
        "learning_rate": 0.00010757661037701242,
        "epoch": 0.9862666666666666,
        "step": 7397
    },
    {
        "loss": 2.4479,
        "grad_norm": 3.4769508838653564,
        "learning_rate": 0.00010751385303721557,
        "epoch": 0.9864,
        "step": 7398
    },
    {
        "loss": 2.5001,
        "grad_norm": 4.270974636077881,
        "learning_rate": 0.00010745109272115599,
        "epoch": 0.9865333333333334,
        "step": 7399
    },
    {
        "loss": 2.3674,
        "grad_norm": 3.6621286869049072,
        "learning_rate": 0.00010738832945369308,
        "epoch": 0.9866666666666667,
        "step": 7400
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.1705708503723145,
        "learning_rate": 0.00010732556325968784,
        "epoch": 0.9868,
        "step": 7401
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.3263769149780273,
        "learning_rate": 0.00010726279416400194,
        "epoch": 0.9869333333333333,
        "step": 7402
    },
    {
        "loss": 2.4267,
        "grad_norm": 2.853170156478882,
        "learning_rate": 0.00010720002219149859,
        "epoch": 0.9870666666666666,
        "step": 7403
    },
    {
        "loss": 2.602,
        "grad_norm": 2.4999799728393555,
        "learning_rate": 0.00010713724736704188,
        "epoch": 0.9872,
        "step": 7404
    },
    {
        "loss": 2.9314,
        "grad_norm": 2.242662191390991,
        "learning_rate": 0.00010707446971549722,
        "epoch": 0.9873333333333333,
        "step": 7405
    },
    {
        "loss": 2.7566,
        "grad_norm": 3.0867037773132324,
        "learning_rate": 0.00010701168926173091,
        "epoch": 0.9874666666666667,
        "step": 7406
    },
    {
        "loss": 1.9298,
        "grad_norm": 2.985185384750366,
        "learning_rate": 0.00010694890603061061,
        "epoch": 0.9876,
        "step": 7407
    },
    {
        "loss": 3.0763,
        "grad_norm": 3.21343731880188,
        "learning_rate": 0.00010688612004700502,
        "epoch": 0.9877333333333334,
        "step": 7408
    },
    {
        "loss": 1.7874,
        "grad_norm": 3.1148464679718018,
        "learning_rate": 0.00010682333133578366,
        "epoch": 0.9878666666666667,
        "step": 7409
    },
    {
        "loss": 2.5171,
        "grad_norm": 2.951848268508911,
        "learning_rate": 0.00010676053992181766,
        "epoch": 0.988,
        "step": 7410
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.125349521636963,
        "learning_rate": 0.00010669774582997871,
        "epoch": 0.9881333333333333,
        "step": 7411
    },
    {
        "loss": 2.3341,
        "grad_norm": 2.9203946590423584,
        "learning_rate": 0.00010663494908513989,
        "epoch": 0.9882666666666666,
        "step": 7412
    },
    {
        "loss": 2.1486,
        "grad_norm": 4.78985595703125,
        "learning_rate": 0.00010657214971217501,
        "epoch": 0.9884,
        "step": 7413
    },
    {
        "loss": 2.2636,
        "grad_norm": 4.300450325012207,
        "learning_rate": 0.00010650934773595935,
        "epoch": 0.9885333333333334,
        "step": 7414
    },
    {
        "loss": 2.3621,
        "grad_norm": 2.916459798812866,
        "learning_rate": 0.0001064465431813688,
        "epoch": 0.9886666666666667,
        "step": 7415
    },
    {
        "loss": 1.8667,
        "grad_norm": 4.409735202789307,
        "learning_rate": 0.00010638373607328052,
        "epoch": 0.9888,
        "step": 7416
    },
    {
        "loss": 2.6835,
        "grad_norm": 5.191704750061035,
        "learning_rate": 0.00010632092643657268,
        "epoch": 0.9889333333333333,
        "step": 7417
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.311326026916504,
        "learning_rate": 0.0001062581142961243,
        "epoch": 0.9890666666666666,
        "step": 7418
    },
    {
        "loss": 3.1668,
        "grad_norm": 3.145400285720825,
        "learning_rate": 0.00010619529967681549,
        "epoch": 0.9892,
        "step": 7419
    },
    {
        "loss": 2.5754,
        "grad_norm": 2.960930824279785,
        "learning_rate": 0.00010613248260352749,
        "epoch": 0.9893333333333333,
        "step": 7420
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.6313247680664062,
        "learning_rate": 0.0001060696631011421,
        "epoch": 0.9894666666666667,
        "step": 7421
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.320723295211792,
        "learning_rate": 0.00010600684119454259,
        "epoch": 0.9896,
        "step": 7422
    },
    {
        "loss": 2.6067,
        "grad_norm": 3.4872379302978516,
        "learning_rate": 0.00010594401690861273,
        "epoch": 0.9897333333333334,
        "step": 7423
    },
    {
        "loss": 2.022,
        "grad_norm": 4.2468695640563965,
        "learning_rate": 0.00010588119026823759,
        "epoch": 0.9898666666666667,
        "step": 7424
    },
    {
        "loss": 2.3543,
        "grad_norm": 3.12774395942688,
        "learning_rate": 0.00010581836129830281,
        "epoch": 0.99,
        "step": 7425
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.215017795562744,
        "learning_rate": 0.00010575553002369543,
        "epoch": 0.9901333333333333,
        "step": 7426
    },
    {
        "loss": 1.4855,
        "grad_norm": 2.747589349746704,
        "learning_rate": 0.00010569269646930293,
        "epoch": 0.9902666666666666,
        "step": 7427
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.938624143600464,
        "learning_rate": 0.00010562986066001395,
        "epoch": 0.9904,
        "step": 7428
    },
    {
        "loss": 1.955,
        "grad_norm": 4.07493257522583,
        "learning_rate": 0.00010556702262071804,
        "epoch": 0.9905333333333334,
        "step": 7429
    },
    {
        "loss": 3.1403,
        "grad_norm": 2.8023669719696045,
        "learning_rate": 0.00010550418237630546,
        "epoch": 0.9906666666666667,
        "step": 7430
    },
    {
        "loss": 2.9695,
        "grad_norm": 3.1875457763671875,
        "learning_rate": 0.00010544133995166756,
        "epoch": 0.9908,
        "step": 7431
    },
    {
        "loss": 3.1056,
        "grad_norm": 4.663815975189209,
        "learning_rate": 0.00010537849537169627,
        "epoch": 0.9909333333333333,
        "step": 7432
    },
    {
        "loss": 3.0372,
        "grad_norm": 3.2290656566619873,
        "learning_rate": 0.0001053156486612847,
        "epoch": 0.9910666666666667,
        "step": 7433
    },
    {
        "loss": 2.7951,
        "grad_norm": 2.4440035820007324,
        "learning_rate": 0.00010525279984532647,
        "epoch": 0.9912,
        "step": 7434
    },
    {
        "loss": 2.328,
        "grad_norm": 4.1147541999816895,
        "learning_rate": 0.00010518994894871644,
        "epoch": 0.9913333333333333,
        "step": 7435
    },
    {
        "loss": 1.2688,
        "grad_norm": 3.095867872238159,
        "learning_rate": 0.00010512709599634989,
        "epoch": 0.9914666666666667,
        "step": 7436
    },
    {
        "loss": 2.673,
        "grad_norm": 2.9717886447906494,
        "learning_rate": 0.0001050642410131232,
        "epoch": 0.9916,
        "step": 7437
    },
    {
        "loss": 0.9029,
        "grad_norm": 2.9148147106170654,
        "learning_rate": 0.00010500138402393333,
        "epoch": 0.9917333333333334,
        "step": 7438
    },
    {
        "loss": 2.6062,
        "grad_norm": 4.766849040985107,
        "learning_rate": 0.00010493852505367827,
        "epoch": 0.9918666666666667,
        "step": 7439
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.5590269565582275,
        "learning_rate": 0.00010487566412725655,
        "epoch": 0.992,
        "step": 7440
    },
    {
        "loss": 2.2293,
        "grad_norm": 3.324268102645874,
        "learning_rate": 0.00010481280126956768,
        "epoch": 0.9921333333333333,
        "step": 7441
    },
    {
        "loss": 1.8913,
        "grad_norm": 3.813657522201538,
        "learning_rate": 0.00010474993650551183,
        "epoch": 0.9922666666666666,
        "step": 7442
    },
    {
        "loss": 0.6623,
        "grad_norm": 2.690533399581909,
        "learning_rate": 0.00010468706985998993,
        "epoch": 0.9924,
        "step": 7443
    },
    {
        "loss": 2.2062,
        "grad_norm": 3.436450958251953,
        "learning_rate": 0.00010462420135790368,
        "epoch": 0.9925333333333334,
        "step": 7444
    },
    {
        "loss": 1.7265,
        "grad_norm": 2.005584716796875,
        "learning_rate": 0.00010456133102415557,
        "epoch": 0.9926666666666667,
        "step": 7445
    },
    {
        "loss": 2.255,
        "grad_norm": 3.6675162315368652,
        "learning_rate": 0.00010449845888364862,
        "epoch": 0.9928,
        "step": 7446
    },
    {
        "loss": 2.3867,
        "grad_norm": 3.197800874710083,
        "learning_rate": 0.0001044355849612868,
        "epoch": 0.9929333333333333,
        "step": 7447
    },
    {
        "loss": 2.4909,
        "grad_norm": 3.82192063331604,
        "learning_rate": 0.00010437270928197473,
        "epoch": 0.9930666666666667,
        "step": 7448
    },
    {
        "loss": 2.3498,
        "grad_norm": 2.417111873626709,
        "learning_rate": 0.00010430983187061754,
        "epoch": 0.9932,
        "step": 7449
    },
    {
        "loss": 1.3479,
        "grad_norm": 3.247917652130127,
        "learning_rate": 0.00010424695275212126,
        "epoch": 0.9933333333333333,
        "step": 7450
    },
    {
        "loss": 2.4523,
        "grad_norm": 2.203605890274048,
        "learning_rate": 0.00010418407195139259,
        "epoch": 0.9934666666666667,
        "step": 7451
    },
    {
        "loss": 2.8104,
        "grad_norm": 4.214440822601318,
        "learning_rate": 0.0001041211894933387,
        "epoch": 0.9936,
        "step": 7452
    },
    {
        "loss": 2.847,
        "grad_norm": 2.641204357147217,
        "learning_rate": 0.00010405830540286761,
        "epoch": 0.9937333333333334,
        "step": 7453
    },
    {
        "loss": 2.6675,
        "grad_norm": 2.7547967433929443,
        "learning_rate": 0.000103995419704888,
        "epoch": 0.9938666666666667,
        "step": 7454
    },
    {
        "loss": 2.7674,
        "grad_norm": 2.155299425125122,
        "learning_rate": 0.00010393253242430895,
        "epoch": 0.994,
        "step": 7455
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.662505626678467,
        "learning_rate": 0.00010386964358604048,
        "epoch": 0.9941333333333333,
        "step": 7456
    },
    {
        "loss": 1.3879,
        "grad_norm": 4.424964904785156,
        "learning_rate": 0.00010380675321499292,
        "epoch": 0.9942666666666666,
        "step": 7457
    },
    {
        "loss": 1.8376,
        "grad_norm": 4.51057767868042,
        "learning_rate": 0.00010374386133607752,
        "epoch": 0.9944,
        "step": 7458
    },
    {
        "loss": 2.4964,
        "grad_norm": 9.142826080322266,
        "learning_rate": 0.00010368096797420575,
        "epoch": 0.9945333333333334,
        "step": 7459
    },
    {
        "loss": 1.8992,
        "grad_norm": 2.899571657180786,
        "learning_rate": 0.00010361807315429016,
        "epoch": 0.9946666666666667,
        "step": 7460
    },
    {
        "loss": 2.5718,
        "grad_norm": 3.155677556991577,
        "learning_rate": 0.00010355517690124339,
        "epoch": 0.9948,
        "step": 7461
    },
    {
        "loss": 2.7289,
        "grad_norm": 2.681251287460327,
        "learning_rate": 0.00010349227923997902,
        "epoch": 0.9949333333333333,
        "step": 7462
    },
    {
        "loss": 2.5032,
        "grad_norm": 2.4721062183380127,
        "learning_rate": 0.00010342938019541087,
        "epoch": 0.9950666666666667,
        "step": 7463
    },
    {
        "loss": 2.517,
        "grad_norm": 3.363102912902832,
        "learning_rate": 0.00010336647979245357,
        "epoch": 0.9952,
        "step": 7464
    },
    {
        "loss": 2.5383,
        "grad_norm": 3.471449136734009,
        "learning_rate": 0.00010330357805602213,
        "epoch": 0.9953333333333333,
        "step": 7465
    },
    {
        "loss": 1.8686,
        "grad_norm": 4.169895648956299,
        "learning_rate": 0.00010324067501103214,
        "epoch": 0.9954666666666667,
        "step": 7466
    },
    {
        "loss": 1.4886,
        "grad_norm": 4.014472484588623,
        "learning_rate": 0.00010317777068239982,
        "epoch": 0.9956,
        "step": 7467
    },
    {
        "loss": 1.721,
        "grad_norm": 4.67259407043457,
        "learning_rate": 0.00010311486509504156,
        "epoch": 0.9957333333333334,
        "step": 7468
    },
    {
        "loss": 1.7177,
        "grad_norm": 3.569983959197998,
        "learning_rate": 0.00010305195827387477,
        "epoch": 0.9958666666666667,
        "step": 7469
    },
    {
        "loss": 3.0106,
        "grad_norm": 2.4478137493133545,
        "learning_rate": 0.00010298905024381684,
        "epoch": 0.996,
        "step": 7470
    },
    {
        "loss": 2.5397,
        "grad_norm": 4.658164024353027,
        "learning_rate": 0.00010292614102978595,
        "epoch": 0.9961333333333333,
        "step": 7471
    },
    {
        "loss": 2.3198,
        "grad_norm": 2.97100830078125,
        "learning_rate": 0.0001028632306567006,
        "epoch": 0.9962666666666666,
        "step": 7472
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.5056962966918945,
        "learning_rate": 0.00010280031914947992,
        "epoch": 0.9964,
        "step": 7473
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.5051496028900146,
        "learning_rate": 0.00010273740653304318,
        "epoch": 0.9965333333333334,
        "step": 7474
    },
    {
        "loss": 2.6078,
        "grad_norm": 3.5273752212524414,
        "learning_rate": 0.00010267449283231039,
        "epoch": 0.9966666666666667,
        "step": 7475
    },
    {
        "loss": 2.4198,
        "grad_norm": 3.913484811782837,
        "learning_rate": 0.00010261157807220195,
        "epoch": 0.9968,
        "step": 7476
    },
    {
        "loss": 2.9194,
        "grad_norm": 3.0441131591796875,
        "learning_rate": 0.00010254866227763846,
        "epoch": 0.9969333333333333,
        "step": 7477
    },
    {
        "loss": 2.1693,
        "grad_norm": 3.2711474895477295,
        "learning_rate": 0.00010248574547354115,
        "epoch": 0.9970666666666667,
        "step": 7478
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.7967529296875,
        "learning_rate": 0.00010242282768483167,
        "epoch": 0.9972,
        "step": 7479
    },
    {
        "loss": 2.203,
        "grad_norm": 3.513608932495117,
        "learning_rate": 0.00010235990893643182,
        "epoch": 0.9973333333333333,
        "step": 7480
    },
    {
        "loss": 1.3805,
        "grad_norm": 3.4709553718566895,
        "learning_rate": 0.00010229698925326407,
        "epoch": 0.9974666666666666,
        "step": 7481
    },
    {
        "loss": 2.3639,
        "grad_norm": 2.5620899200439453,
        "learning_rate": 0.00010223406866025096,
        "epoch": 0.9976,
        "step": 7482
    },
    {
        "loss": 2.2685,
        "grad_norm": 3.679576873779297,
        "learning_rate": 0.00010217114718231573,
        "epoch": 0.9977333333333334,
        "step": 7483
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.856240749359131,
        "learning_rate": 0.00010210822484438159,
        "epoch": 0.9978666666666667,
        "step": 7484
    },
    {
        "loss": 2.4575,
        "grad_norm": 2.1548805236816406,
        "learning_rate": 0.00010204530167137252,
        "epoch": 0.998,
        "step": 7485
    },
    {
        "loss": 1.7845,
        "grad_norm": 3.8938119411468506,
        "learning_rate": 0.0001019823776882124,
        "epoch": 0.9981333333333333,
        "step": 7486
    },
    {
        "loss": 2.331,
        "grad_norm": 2.5077266693115234,
        "learning_rate": 0.00010191945291982581,
        "epoch": 0.9982666666666666,
        "step": 7487
    },
    {
        "loss": 2.5432,
        "grad_norm": 8.066080093383789,
        "learning_rate": 0.00010185652739113731,
        "epoch": 0.9984,
        "step": 7488
    },
    {
        "loss": 1.9053,
        "grad_norm": 4.218750476837158,
        "learning_rate": 0.00010179360112707197,
        "epoch": 0.9985333333333334,
        "step": 7489
    },
    {
        "loss": 1.803,
        "grad_norm": 4.401844024658203,
        "learning_rate": 0.0001017306741525552,
        "epoch": 0.9986666666666667,
        "step": 7490
    },
    {
        "loss": 2.3852,
        "grad_norm": 4.631943225860596,
        "learning_rate": 0.00010166774649251243,
        "epoch": 0.9988,
        "step": 7491
    },
    {
        "loss": 2.3805,
        "grad_norm": 2.8864097595214844,
        "learning_rate": 0.00010160481817186965,
        "epoch": 0.9989333333333333,
        "step": 7492
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.704310417175293,
        "learning_rate": 0.00010154188921555276,
        "epoch": 0.9990666666666667,
        "step": 7493
    },
    {
        "loss": 2.4457,
        "grad_norm": 4.137921333312988,
        "learning_rate": 0.00010147895964848848,
        "epoch": 0.9992,
        "step": 7494
    },
    {
        "loss": 3.4304,
        "grad_norm": 3.265207290649414,
        "learning_rate": 0.00010141602949560318,
        "epoch": 0.9993333333333333,
        "step": 7495
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.476818084716797,
        "learning_rate": 0.0001013530987818238,
        "epoch": 0.9994666666666666,
        "step": 7496
    },
    {
        "loss": 2.2272,
        "grad_norm": 3.319401979446411,
        "learning_rate": 0.00010129016753207734,
        "epoch": 0.9996,
        "step": 7497
    },
    {
        "loss": 1.2425,
        "grad_norm": 5.461694717407227,
        "learning_rate": 0.00010122723577129119,
        "epoch": 0.9997333333333334,
        "step": 7498
    },
    {
        "loss": 2.6824,
        "grad_norm": 1.5290755033493042,
        "learning_rate": 0.0001011643035243927,
        "epoch": 0.9998666666666667,
        "step": 7499
    },
    {
        "loss": 2.8497,
        "grad_norm": 2.576533317565918,
        "learning_rate": 0.00010110137081630962,
        "epoch": 1.0,
        "step": 7500
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.035551071166992,
        "learning_rate": 0.00010103843767196985,
        "epoch": 1.0001333333333333,
        "step": 7501
    },
    {
        "loss": 2.6712,
        "grad_norm": 2.5873730182647705,
        "learning_rate": 0.00010097550411630134,
        "epoch": 1.0002666666666666,
        "step": 7502
    },
    {
        "loss": 1.7223,
        "grad_norm": 3.6158559322357178,
        "learning_rate": 0.00010091257017423232,
        "epoch": 1.0004,
        "step": 7503
    },
    {
        "loss": 2.5949,
        "grad_norm": 3.033595561981201,
        "learning_rate": 0.00010084963587069121,
        "epoch": 1.0005333333333333,
        "step": 7504
    },
    {
        "loss": 1.7102,
        "grad_norm": 3.1379075050354004,
        "learning_rate": 0.00010078670123060638,
        "epoch": 1.0006666666666666,
        "step": 7505
    },
    {
        "loss": 2.3659,
        "grad_norm": 2.764655590057373,
        "learning_rate": 0.00010072376627890656,
        "epoch": 1.0008,
        "step": 7506
    },
    {
        "loss": 1.8192,
        "grad_norm": 2.134185791015625,
        "learning_rate": 0.0001006608310405204,
        "epoch": 1.0009333333333332,
        "step": 7507
    },
    {
        "loss": 2.0258,
        "grad_norm": 2.7120585441589355,
        "learning_rate": 0.0001005978955403768,
        "epoch": 1.0010666666666668,
        "step": 7508
    },
    {
        "loss": 2.1271,
        "grad_norm": 3.143770217895508,
        "learning_rate": 0.00010053495980340475,
        "epoch": 1.0012,
        "step": 7509
    },
    {
        "loss": 2.4034,
        "grad_norm": 2.9854886531829834,
        "learning_rate": 0.00010047202385453336,
        "epoch": 1.0013333333333334,
        "step": 7510
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.7061104774475098,
        "learning_rate": 0.00010040908771869164,
        "epoch": 1.0014666666666667,
        "step": 7511
    },
    {
        "loss": 2.3739,
        "grad_norm": 3.7703919410705566,
        "learning_rate": 0.00010034615142080889,
        "epoch": 1.0016,
        "step": 7512
    },
    {
        "loss": 2.3413,
        "grad_norm": 4.085775375366211,
        "learning_rate": 0.00010028321498581446,
        "epoch": 1.0017333333333334,
        "step": 7513
    },
    {
        "loss": 1.317,
        "grad_norm": 3.7348995208740234,
        "learning_rate": 0.0001002202784386375,
        "epoch": 1.0018666666666667,
        "step": 7514
    },
    {
        "loss": 2.2567,
        "grad_norm": 2.968379020690918,
        "learning_rate": 0.00010015734180420761,
        "epoch": 1.002,
        "step": 7515
    },
    {
        "loss": 2.293,
        "grad_norm": 2.685863494873047,
        "learning_rate": 0.00010009440510745402,
        "epoch": 1.0021333333333333,
        "step": 7516
    },
    {
        "loss": 2.0551,
        "grad_norm": 2.9088966846466064,
        "learning_rate": 0.0001000314683733063,
        "epoch": 1.0022666666666666,
        "step": 7517
    },
    {
        "loss": 1.3256,
        "grad_norm": 4.628177165985107,
        "learning_rate": 9.996853162669372e-05,
        "epoch": 1.0024,
        "step": 7518
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.5376744270324707,
        "learning_rate": 9.9905594892546e-05,
        "epoch": 1.0025333333333333,
        "step": 7519
    },
    {
        "loss": 2.5965,
        "grad_norm": 2.8214306831359863,
        "learning_rate": 9.984265819579248e-05,
        "epoch": 1.0026666666666666,
        "step": 7520
    },
    {
        "loss": 2.5662,
        "grad_norm": 3.3502039909362793,
        "learning_rate": 9.977972156136252e-05,
        "epoch": 1.0028,
        "step": 7521
    },
    {
        "loss": 2.4271,
        "grad_norm": 3.7120046615600586,
        "learning_rate": 9.971678501418557e-05,
        "epoch": 1.0029333333333332,
        "step": 7522
    },
    {
        "loss": 2.3725,
        "grad_norm": 3.591094732284546,
        "learning_rate": 9.965384857919106e-05,
        "epoch": 1.0030666666666668,
        "step": 7523
    },
    {
        "loss": 1.7114,
        "grad_norm": 3.557210922241211,
        "learning_rate": 9.959091228130845e-05,
        "epoch": 1.0032,
        "step": 7524
    },
    {
        "loss": 1.8289,
        "grad_norm": 5.638454437255859,
        "learning_rate": 9.952797614546666e-05,
        "epoch": 1.0033333333333334,
        "step": 7525
    },
    {
        "loss": 1.6352,
        "grad_norm": 2.5245697498321533,
        "learning_rate": 9.946504019659528e-05,
        "epoch": 1.0034666666666667,
        "step": 7526
    },
    {
        "loss": 2.0841,
        "grad_norm": 2.2144594192504883,
        "learning_rate": 9.940210445962316e-05,
        "epoch": 1.0036,
        "step": 7527
    },
    {
        "loss": 2.5551,
        "grad_norm": 5.292253017425537,
        "learning_rate": 9.933916895947963e-05,
        "epoch": 1.0037333333333334,
        "step": 7528
    },
    {
        "loss": 2.0812,
        "grad_norm": 3.0622453689575195,
        "learning_rate": 9.927623372109352e-05,
        "epoch": 1.0038666666666667,
        "step": 7529
    },
    {
        "loss": 1.1938,
        "grad_norm": 4.722670555114746,
        "learning_rate": 9.921329876939365e-05,
        "epoch": 1.004,
        "step": 7530
    },
    {
        "loss": 2.2499,
        "grad_norm": 3.9526755809783936,
        "learning_rate": 9.915036412930882e-05,
        "epoch": 1.0041333333333333,
        "step": 7531
    },
    {
        "loss": 2.745,
        "grad_norm": 3.9337329864501953,
        "learning_rate": 9.908742982576762e-05,
        "epoch": 1.0042666666666666,
        "step": 7532
    },
    {
        "loss": 2.0231,
        "grad_norm": 3.1821866035461426,
        "learning_rate": 9.902449588369873e-05,
        "epoch": 1.0044,
        "step": 7533
    },
    {
        "loss": 2.3548,
        "grad_norm": 2.5224030017852783,
        "learning_rate": 9.896156232803016e-05,
        "epoch": 1.0045333333333333,
        "step": 7534
    },
    {
        "loss": 1.8999,
        "grad_norm": 2.634427070617676,
        "learning_rate": 9.889862918369042e-05,
        "epoch": 1.0046666666666666,
        "step": 7535
    },
    {
        "loss": 1.7673,
        "grad_norm": 2.565753221511841,
        "learning_rate": 9.883569647560725e-05,
        "epoch": 1.0048,
        "step": 7536
    },
    {
        "loss": 2.2801,
        "grad_norm": 2.904168128967285,
        "learning_rate": 9.877276422870889e-05,
        "epoch": 1.0049333333333332,
        "step": 7537
    },
    {
        "loss": 2.1356,
        "grad_norm": 3.273644208908081,
        "learning_rate": 9.870983246792269e-05,
        "epoch": 1.0050666666666668,
        "step": 7538
    },
    {
        "loss": 2.365,
        "grad_norm": 2.822005033493042,
        "learning_rate": 9.864690121817622e-05,
        "epoch": 1.0052,
        "step": 7539
    },
    {
        "loss": 2.1324,
        "grad_norm": 4.445984840393066,
        "learning_rate": 9.858397050439678e-05,
        "epoch": 1.0053333333333334,
        "step": 7540
    },
    {
        "loss": 1.9898,
        "grad_norm": 3.0714402198791504,
        "learning_rate": 9.852104035151153e-05,
        "epoch": 1.0054666666666667,
        "step": 7541
    },
    {
        "loss": 0.6135,
        "grad_norm": 3.480323076248169,
        "learning_rate": 9.845811078444725e-05,
        "epoch": 1.0056,
        "step": 7542
    },
    {
        "loss": 2.4169,
        "grad_norm": 2.7417213916778564,
        "learning_rate": 9.839518182813037e-05,
        "epoch": 1.0057333333333334,
        "step": 7543
    },
    {
        "loss": 0.8184,
        "grad_norm": 3.4042773246765137,
        "learning_rate": 9.83322535074876e-05,
        "epoch": 1.0058666666666667,
        "step": 7544
    },
    {
        "loss": 2.5691,
        "grad_norm": 4.594673156738281,
        "learning_rate": 9.826932584744491e-05,
        "epoch": 1.006,
        "step": 7545
    },
    {
        "loss": 2.0244,
        "grad_norm": 3.0206727981567383,
        "learning_rate": 9.820639887292807e-05,
        "epoch": 1.0061333333333333,
        "step": 7546
    },
    {
        "loss": 1.9972,
        "grad_norm": 2.420290470123291,
        "learning_rate": 9.814347260886272e-05,
        "epoch": 1.0062666666666666,
        "step": 7547
    },
    {
        "loss": 1.9139,
        "grad_norm": 4.059513092041016,
        "learning_rate": 9.808054708017422e-05,
        "epoch": 1.0064,
        "step": 7548
    },
    {
        "loss": 2.4338,
        "grad_norm": 3.9269371032714844,
        "learning_rate": 9.801762231178757e-05,
        "epoch": 1.0065333333333333,
        "step": 7549
    },
    {
        "loss": 1.0762,
        "grad_norm": 4.070634841918945,
        "learning_rate": 9.795469832862751e-05,
        "epoch": 1.0066666666666666,
        "step": 7550
    },
    {
        "loss": 2.2546,
        "grad_norm": 2.2831549644470215,
        "learning_rate": 9.789177515561845e-05,
        "epoch": 1.0068,
        "step": 7551
    },
    {
        "loss": 2.2155,
        "grad_norm": 3.1248154640197754,
        "learning_rate": 9.782885281768428e-05,
        "epoch": 1.0069333333333332,
        "step": 7552
    },
    {
        "loss": 2.3482,
        "grad_norm": 4.579779148101807,
        "learning_rate": 9.776593133974907e-05,
        "epoch": 1.0070666666666668,
        "step": 7553
    },
    {
        "loss": 2.0625,
        "grad_norm": 3.8816516399383545,
        "learning_rate": 9.770301074673602e-05,
        "epoch": 1.0072,
        "step": 7554
    },
    {
        "loss": 1.6694,
        "grad_norm": 3.307955265045166,
        "learning_rate": 9.764009106356819e-05,
        "epoch": 1.0073333333333334,
        "step": 7555
    },
    {
        "loss": 2.3509,
        "grad_norm": 3.6371169090270996,
        "learning_rate": 9.757717231516835e-05,
        "epoch": 1.0074666666666667,
        "step": 7556
    },
    {
        "loss": 2.1654,
        "grad_norm": 2.3290414810180664,
        "learning_rate": 9.751425452645879e-05,
        "epoch": 1.0076,
        "step": 7557
    },
    {
        "loss": 1.0573,
        "grad_norm": 3.5287468433380127,
        "learning_rate": 9.745133772236163e-05,
        "epoch": 1.0077333333333334,
        "step": 7558
    },
    {
        "loss": 1.9258,
        "grad_norm": 3.5902321338653564,
        "learning_rate": 9.738842192779809e-05,
        "epoch": 1.0078666666666667,
        "step": 7559
    },
    {
        "loss": 0.5446,
        "grad_norm": 2.5022904872894287,
        "learning_rate": 9.732550716768965e-05,
        "epoch": 1.008,
        "step": 7560
    },
    {
        "loss": 2.577,
        "grad_norm": 2.329484224319458,
        "learning_rate": 9.726259346695678e-05,
        "epoch": 1.0081333333333333,
        "step": 7561
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.6869703531265259,
        "learning_rate": 9.719968085052018e-05,
        "epoch": 1.0082666666666666,
        "step": 7562
    },
    {
        "loss": 2.3648,
        "grad_norm": 4.455097198486328,
        "learning_rate": 9.713676934329942e-05,
        "epoch": 1.0084,
        "step": 7563
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.8873679637908936,
        "learning_rate": 9.707385897021406e-05,
        "epoch": 1.0085333333333333,
        "step": 7564
    },
    {
        "loss": 0.7676,
        "grad_norm": 4.3508172035217285,
        "learning_rate": 9.701094975618312e-05,
        "epoch": 1.0086666666666666,
        "step": 7565
    },
    {
        "loss": 1.353,
        "grad_norm": 4.8137969970703125,
        "learning_rate": 9.694804172612526e-05,
        "epoch": 1.0088,
        "step": 7566
    },
    {
        "loss": 1.4152,
        "grad_norm": 5.412670135498047,
        "learning_rate": 9.688513490495847e-05,
        "epoch": 1.0089333333333332,
        "step": 7567
    },
    {
        "loss": 2.2577,
        "grad_norm": 3.7659056186676025,
        "learning_rate": 9.682222931760019e-05,
        "epoch": 1.0090666666666666,
        "step": 7568
    },
    {
        "loss": 0.9736,
        "grad_norm": 3.531282901763916,
        "learning_rate": 9.675932498896787e-05,
        "epoch": 1.0092,
        "step": 7569
    },
    {
        "loss": 2.9083,
        "grad_norm": 1.8655457496643066,
        "learning_rate": 9.669642194397783e-05,
        "epoch": 1.0093333333333334,
        "step": 7570
    },
    {
        "loss": 2.9819,
        "grad_norm": 2.5768518447875977,
        "learning_rate": 9.663352020754652e-05,
        "epoch": 1.0094666666666667,
        "step": 7571
    },
    {
        "loss": 2.3587,
        "grad_norm": 2.8124585151672363,
        "learning_rate": 9.657061980458917e-05,
        "epoch": 1.0096,
        "step": 7572
    },
    {
        "loss": 1.8335,
        "grad_norm": 3.235297679901123,
        "learning_rate": 9.650772076002102e-05,
        "epoch": 1.0097333333333334,
        "step": 7573
    },
    {
        "loss": 2.4448,
        "grad_norm": 2.2372848987579346,
        "learning_rate": 9.644482309875658e-05,
        "epoch": 1.0098666666666667,
        "step": 7574
    },
    {
        "loss": 0.8865,
        "grad_norm": 3.148303270339966,
        "learning_rate": 9.638192684570988e-05,
        "epoch": 1.01,
        "step": 7575
    },
    {
        "loss": 2.9445,
        "grad_norm": 3.866091251373291,
        "learning_rate": 9.631903202579426e-05,
        "epoch": 1.0101333333333333,
        "step": 7576
    },
    {
        "loss": 2.1376,
        "grad_norm": 4.479506492614746,
        "learning_rate": 9.62561386639225e-05,
        "epoch": 1.0102666666666666,
        "step": 7577
    },
    {
        "loss": 1.1158,
        "grad_norm": 2.9467411041259766,
        "learning_rate": 9.619324678500711e-05,
        "epoch": 1.0104,
        "step": 7578
    },
    {
        "loss": 2.1082,
        "grad_norm": 2.8883302211761475,
        "learning_rate": 9.613035641395963e-05,
        "epoch": 1.0105333333333333,
        "step": 7579
    },
    {
        "loss": 1.7604,
        "grad_norm": 3.420386552810669,
        "learning_rate": 9.606746757569107e-05,
        "epoch": 1.0106666666666666,
        "step": 7580
    },
    {
        "loss": 0.7973,
        "grad_norm": 3.2253429889678955,
        "learning_rate": 9.600458029511202e-05,
        "epoch": 1.0108,
        "step": 7581
    },
    {
        "loss": 2.8717,
        "grad_norm": 3.6725447177886963,
        "learning_rate": 9.594169459713234e-05,
        "epoch": 1.0109333333333332,
        "step": 7582
    },
    {
        "loss": 1.7459,
        "grad_norm": 3.476438045501709,
        "learning_rate": 9.587881050666139e-05,
        "epoch": 1.0110666666666666,
        "step": 7583
    },
    {
        "loss": 0.6209,
        "grad_norm": 3.2867398262023926,
        "learning_rate": 9.581592804860742e-05,
        "epoch": 1.0112,
        "step": 7584
    },
    {
        "loss": 1.6726,
        "grad_norm": 4.139235973358154,
        "learning_rate": 9.575304724787876e-05,
        "epoch": 1.0113333333333334,
        "step": 7585
    },
    {
        "loss": 1.8594,
        "grad_norm": 2.9609932899475098,
        "learning_rate": 9.569016812938239e-05,
        "epoch": 1.0114666666666667,
        "step": 7586
    },
    {
        "loss": 2.7991,
        "grad_norm": 3.690549373626709,
        "learning_rate": 9.562729071802535e-05,
        "epoch": 1.0116,
        "step": 7587
    },
    {
        "loss": 2.318,
        "grad_norm": 2.519270181655884,
        "learning_rate": 9.556441503871322e-05,
        "epoch": 1.0117333333333334,
        "step": 7588
    },
    {
        "loss": 2.1202,
        "grad_norm": 3.407423973083496,
        "learning_rate": 9.55015411163514e-05,
        "epoch": 1.0118666666666667,
        "step": 7589
    },
    {
        "loss": 2.1774,
        "grad_norm": 2.222825288772583,
        "learning_rate": 9.543866897584445e-05,
        "epoch": 1.012,
        "step": 7590
    },
    {
        "loss": 2.7702,
        "grad_norm": 2.831209421157837,
        "learning_rate": 9.537579864209628e-05,
        "epoch": 1.0121333333333333,
        "step": 7591
    },
    {
        "loss": 1.9094,
        "grad_norm": 2.6788551807403564,
        "learning_rate": 9.531293014001016e-05,
        "epoch": 1.0122666666666666,
        "step": 7592
    },
    {
        "loss": 2.2962,
        "grad_norm": 3.308310031890869,
        "learning_rate": 9.525006349448819e-05,
        "epoch": 1.0124,
        "step": 7593
    },
    {
        "loss": 0.6666,
        "grad_norm": 3.570624828338623,
        "learning_rate": 9.518719873043236e-05,
        "epoch": 1.0125333333333333,
        "step": 7594
    },
    {
        "loss": 2.457,
        "grad_norm": 3.4074673652648926,
        "learning_rate": 9.51243358727434e-05,
        "epoch": 1.0126666666666666,
        "step": 7595
    },
    {
        "loss": 0.5627,
        "grad_norm": 2.3821797370910645,
        "learning_rate": 9.506147494632182e-05,
        "epoch": 1.0128,
        "step": 7596
    },
    {
        "loss": 1.9283,
        "grad_norm": 3.8678486347198486,
        "learning_rate": 9.499861597606668e-05,
        "epoch": 1.0129333333333332,
        "step": 7597
    },
    {
        "loss": 2.0279,
        "grad_norm": 2.6375086307525635,
        "learning_rate": 9.49357589868768e-05,
        "epoch": 1.0130666666666666,
        "step": 7598
    },
    {
        "loss": 1.6209,
        "grad_norm": 3.0814566612243652,
        "learning_rate": 9.487290400365008e-05,
        "epoch": 1.0132,
        "step": 7599
    },
    {
        "loss": 0.981,
        "grad_norm": 3.9351518154144287,
        "learning_rate": 9.48100510512836e-05,
        "epoch": 1.0133333333333334,
        "step": 7600
    },
    {
        "loss": 2.3413,
        "grad_norm": 3.221951961517334,
        "learning_rate": 9.474720015467355e-05,
        "epoch": 1.0134666666666667,
        "step": 7601
    },
    {
        "loss": 2.1361,
        "grad_norm": 3.5422568321228027,
        "learning_rate": 9.468435133871531e-05,
        "epoch": 1.0136,
        "step": 7602
    },
    {
        "loss": 2.0384,
        "grad_norm": 2.023237943649292,
        "learning_rate": 9.462150462830377e-05,
        "epoch": 1.0137333333333334,
        "step": 7603
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.633894205093384,
        "learning_rate": 9.455866004833256e-05,
        "epoch": 1.0138666666666667,
        "step": 7604
    },
    {
        "loss": 3.4485,
        "grad_norm": 4.288819789886475,
        "learning_rate": 9.449581762369456e-05,
        "epoch": 1.014,
        "step": 7605
    },
    {
        "loss": 1.4386,
        "grad_norm": 3.090618371963501,
        "learning_rate": 9.443297737928198e-05,
        "epoch": 1.0141333333333333,
        "step": 7606
    },
    {
        "loss": 2.0365,
        "grad_norm": 4.200965404510498,
        "learning_rate": 9.437013933998602e-05,
        "epoch": 1.0142666666666666,
        "step": 7607
    },
    {
        "loss": 1.1172,
        "grad_norm": 5.494082927703857,
        "learning_rate": 9.430730353069718e-05,
        "epoch": 1.0144,
        "step": 7608
    },
    {
        "loss": 2.8487,
        "grad_norm": 3.504976749420166,
        "learning_rate": 9.424446997630459e-05,
        "epoch": 1.0145333333333333,
        "step": 7609
    },
    {
        "loss": 1.9329,
        "grad_norm": 3.603097438812256,
        "learning_rate": 9.418163870169722e-05,
        "epoch": 1.0146666666666666,
        "step": 7610
    },
    {
        "loss": 2.1571,
        "grad_norm": 3.0597360134124756,
        "learning_rate": 9.411880973176243e-05,
        "epoch": 1.0148,
        "step": 7611
    },
    {
        "loss": 2.2598,
        "grad_norm": 2.800478219985962,
        "learning_rate": 9.405598309138729e-05,
        "epoch": 1.0149333333333332,
        "step": 7612
    },
    {
        "loss": 2.4144,
        "grad_norm": 3.0253400802612305,
        "learning_rate": 9.39931588054575e-05,
        "epoch": 1.0150666666666666,
        "step": 7613
    },
    {
        "loss": 1.7732,
        "grad_norm": 5.7946600914001465,
        "learning_rate": 9.393033689885792e-05,
        "epoch": 1.0152,
        "step": 7614
    },
    {
        "loss": 1.1096,
        "grad_norm": 5.769865036010742,
        "learning_rate": 9.386751739647255e-05,
        "epoch": 1.0153333333333334,
        "step": 7615
    },
    {
        "loss": 1.7613,
        "grad_norm": 1.7094401121139526,
        "learning_rate": 9.380470032318445e-05,
        "epoch": 1.0154666666666667,
        "step": 7616
    },
    {
        "loss": 2.2208,
        "grad_norm": 2.887317180633545,
        "learning_rate": 9.37418857038758e-05,
        "epoch": 1.0156,
        "step": 7617
    },
    {
        "loss": 2.6782,
        "grad_norm": 3.118652820587158,
        "learning_rate": 9.367907356342734e-05,
        "epoch": 1.0157333333333334,
        "step": 7618
    },
    {
        "loss": 1.9269,
        "grad_norm": 3.6196722984313965,
        "learning_rate": 9.36162639267195e-05,
        "epoch": 1.0158666666666667,
        "step": 7619
    },
    {
        "loss": 1.7794,
        "grad_norm": 4.068387508392334,
        "learning_rate": 9.355345681863117e-05,
        "epoch": 1.016,
        "step": 7620
    },
    {
        "loss": 1.5943,
        "grad_norm": 2.9781508445739746,
        "learning_rate": 9.349065226404074e-05,
        "epoch": 1.0161333333333333,
        "step": 7621
    },
    {
        "loss": 2.5446,
        "grad_norm": 4.000399112701416,
        "learning_rate": 9.3427850287825e-05,
        "epoch": 1.0162666666666667,
        "step": 7622
    },
    {
        "loss": 2.174,
        "grad_norm": 4.044856071472168,
        "learning_rate": 9.336505091486014e-05,
        "epoch": 1.0164,
        "step": 7623
    },
    {
        "loss": 2.2517,
        "grad_norm": 3.6379551887512207,
        "learning_rate": 9.330225417002125e-05,
        "epoch": 1.0165333333333333,
        "step": 7624
    },
    {
        "loss": 2.1194,
        "grad_norm": 4.2698798179626465,
        "learning_rate": 9.323946007818235e-05,
        "epoch": 1.0166666666666666,
        "step": 7625
    },
    {
        "loss": 2.445,
        "grad_norm": 3.966592311859131,
        "learning_rate": 9.317666866421635e-05,
        "epoch": 1.0168,
        "step": 7626
    },
    {
        "loss": 1.279,
        "grad_norm": 2.3003196716308594,
        "learning_rate": 9.311387995299499e-05,
        "epoch": 1.0169333333333332,
        "step": 7627
    },
    {
        "loss": 1.3072,
        "grad_norm": 3.086078643798828,
        "learning_rate": 9.305109396938941e-05,
        "epoch": 1.0170666666666666,
        "step": 7628
    },
    {
        "loss": 2.7317,
        "grad_norm": 2.4438436031341553,
        "learning_rate": 9.298831073826903e-05,
        "epoch": 1.0172,
        "step": 7629
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.053914785385132,
        "learning_rate": 9.292553028450288e-05,
        "epoch": 1.0173333333333334,
        "step": 7630
    },
    {
        "loss": 2.0647,
        "grad_norm": 4.118453025817871,
        "learning_rate": 9.286275263295816e-05,
        "epoch": 1.0174666666666667,
        "step": 7631
    },
    {
        "loss": 2.7119,
        "grad_norm": 3.3537771701812744,
        "learning_rate": 9.279997780850142e-05,
        "epoch": 1.0176,
        "step": 7632
    },
    {
        "loss": 2.2046,
        "grad_norm": 2.9693284034729004,
        "learning_rate": 9.2737205835998e-05,
        "epoch": 1.0177333333333334,
        "step": 7633
    },
    {
        "loss": 1.8304,
        "grad_norm": 1.8453401327133179,
        "learning_rate": 9.267443674031218e-05,
        "epoch": 1.0178666666666667,
        "step": 7634
    },
    {
        "loss": 2.4659,
        "grad_norm": 3.3377633094787598,
        "learning_rate": 9.261167054630694e-05,
        "epoch": 1.018,
        "step": 7635
    },
    {
        "loss": 0.8961,
        "grad_norm": 3.3105592727661133,
        "learning_rate": 9.254890727884404e-05,
        "epoch": 1.0181333333333333,
        "step": 7636
    },
    {
        "loss": 1.8049,
        "grad_norm": 3.1114978790283203,
        "learning_rate": 9.248614696278445e-05,
        "epoch": 1.0182666666666667,
        "step": 7637
    },
    {
        "loss": 2.5978,
        "grad_norm": 3.689420461654663,
        "learning_rate": 9.242338962298766e-05,
        "epoch": 1.0184,
        "step": 7638
    },
    {
        "loss": 2.8104,
        "grad_norm": 4.109665393829346,
        "learning_rate": 9.236063528431203e-05,
        "epoch": 1.0185333333333333,
        "step": 7639
    },
    {
        "loss": 2.0621,
        "grad_norm": 3.06499981880188,
        "learning_rate": 9.229788397161477e-05,
        "epoch": 1.0186666666666666,
        "step": 7640
    },
    {
        "loss": 1.9064,
        "grad_norm": 6.387450218200684,
        "learning_rate": 9.223513570975188e-05,
        "epoch": 1.0188,
        "step": 7641
    },
    {
        "loss": 2.4134,
        "grad_norm": 4.002386093139648,
        "learning_rate": 9.217239052357837e-05,
        "epoch": 1.0189333333333332,
        "step": 7642
    },
    {
        "loss": 1.6596,
        "grad_norm": 7.491599082946777,
        "learning_rate": 9.210964843794744e-05,
        "epoch": 1.0190666666666666,
        "step": 7643
    },
    {
        "loss": 2.0676,
        "grad_norm": 2.5232784748077393,
        "learning_rate": 9.204690947771176e-05,
        "epoch": 1.0192,
        "step": 7644
    },
    {
        "loss": 1.6062,
        "grad_norm": 3.774783134460449,
        "learning_rate": 9.198417366772222e-05,
        "epoch": 1.0193333333333334,
        "step": 7645
    },
    {
        "loss": 2.9268,
        "grad_norm": 5.278313159942627,
        "learning_rate": 9.192144103282901e-05,
        "epoch": 1.0194666666666667,
        "step": 7646
    },
    {
        "loss": 2.1701,
        "grad_norm": 2.032858371734619,
        "learning_rate": 9.185871159788039e-05,
        "epoch": 1.0196,
        "step": 7647
    },
    {
        "loss": 2.2868,
        "grad_norm": 3.022552967071533,
        "learning_rate": 9.179598538772383e-05,
        "epoch": 1.0197333333333334,
        "step": 7648
    },
    {
        "loss": 2.6758,
        "grad_norm": 3.120283603668213,
        "learning_rate": 9.173326242720545e-05,
        "epoch": 1.0198666666666667,
        "step": 7649
    },
    {
        "loss": 1.3234,
        "grad_norm": 5.1547346115112305,
        "learning_rate": 9.167054274116997e-05,
        "epoch": 1.02,
        "step": 7650
    },
    {
        "loss": 2.0452,
        "grad_norm": 3.7440264225006104,
        "learning_rate": 9.160782635446103e-05,
        "epoch": 1.0201333333333333,
        "step": 7651
    },
    {
        "loss": 1.9387,
        "grad_norm": 3.563354015350342,
        "learning_rate": 9.154511329192045e-05,
        "epoch": 1.0202666666666667,
        "step": 7652
    },
    {
        "loss": 1.8295,
        "grad_norm": 3.263793468475342,
        "learning_rate": 9.148240357838946e-05,
        "epoch": 1.0204,
        "step": 7653
    },
    {
        "loss": 2.1765,
        "grad_norm": 3.236046314239502,
        "learning_rate": 9.141969723870731e-05,
        "epoch": 1.0205333333333333,
        "step": 7654
    },
    {
        "loss": 2.6234,
        "grad_norm": 3.546759605407715,
        "learning_rate": 9.135699429771247e-05,
        "epoch": 1.0206666666666666,
        "step": 7655
    },
    {
        "loss": 2.1705,
        "grad_norm": 2.0005390644073486,
        "learning_rate": 9.129429478024155e-05,
        "epoch": 1.0208,
        "step": 7656
    },
    {
        "loss": 1.9537,
        "grad_norm": 3.1791136264801025,
        "learning_rate": 9.123159871113013e-05,
        "epoch": 1.0209333333333332,
        "step": 7657
    },
    {
        "loss": 1.6832,
        "grad_norm": 3.6542136669158936,
        "learning_rate": 9.116890611521231e-05,
        "epoch": 1.0210666666666666,
        "step": 7658
    },
    {
        "loss": 2.7335,
        "grad_norm": 2.625291109085083,
        "learning_rate": 9.110621701732096e-05,
        "epoch": 1.0212,
        "step": 7659
    },
    {
        "loss": 2.02,
        "grad_norm": 3.3880512714385986,
        "learning_rate": 9.104353144228736e-05,
        "epoch": 1.0213333333333334,
        "step": 7660
    },
    {
        "loss": 2.5929,
        "grad_norm": 3.1557106971740723,
        "learning_rate": 9.098084941494134e-05,
        "epoch": 1.0214666666666667,
        "step": 7661
    },
    {
        "loss": 1.1589,
        "grad_norm": 3.57570219039917,
        "learning_rate": 9.091817096011171e-05,
        "epoch": 1.0216,
        "step": 7662
    },
    {
        "loss": 2.1782,
        "grad_norm": 3.8533005714416504,
        "learning_rate": 9.085549610262552e-05,
        "epoch": 1.0217333333333334,
        "step": 7663
    },
    {
        "loss": 2.8584,
        "grad_norm": 3.7699222564697266,
        "learning_rate": 9.079282486730842e-05,
        "epoch": 1.0218666666666667,
        "step": 7664
    },
    {
        "loss": 2.5519,
        "grad_norm": 2.828584671020508,
        "learning_rate": 9.073015727898476e-05,
        "epoch": 1.022,
        "step": 7665
    },
    {
        "loss": 1.2802,
        "grad_norm": 3.559725284576416,
        "learning_rate": 9.066749336247737e-05,
        "epoch": 1.0221333333333333,
        "step": 7666
    },
    {
        "loss": 1.4843,
        "grad_norm": 2.1405911445617676,
        "learning_rate": 9.060483314260783e-05,
        "epoch": 1.0222666666666667,
        "step": 7667
    },
    {
        "loss": 2.0399,
        "grad_norm": 3.432041645050049,
        "learning_rate": 9.054217664419568e-05,
        "epoch": 1.0224,
        "step": 7668
    },
    {
        "loss": 0.7388,
        "grad_norm": 3.106959581375122,
        "learning_rate": 9.047952389205971e-05,
        "epoch": 1.0225333333333333,
        "step": 7669
    },
    {
        "loss": 2.0939,
        "grad_norm": 3.0362348556518555,
        "learning_rate": 9.041687491101662e-05,
        "epoch": 1.0226666666666666,
        "step": 7670
    },
    {
        "loss": 2.0913,
        "grad_norm": 2.459146738052368,
        "learning_rate": 9.03542297258822e-05,
        "epoch": 1.0228,
        "step": 7671
    },
    {
        "loss": 2.047,
        "grad_norm": 3.144583225250244,
        "learning_rate": 9.029158836147024e-05,
        "epoch": 1.0229333333333333,
        "step": 7672
    },
    {
        "loss": 2.3306,
        "grad_norm": 3.481318950653076,
        "learning_rate": 9.022895084259313e-05,
        "epoch": 1.0230666666666666,
        "step": 7673
    },
    {
        "loss": 2.1385,
        "grad_norm": 3.171001672744751,
        "learning_rate": 9.01663171940619e-05,
        "epoch": 1.0232,
        "step": 7674
    },
    {
        "loss": 1.8271,
        "grad_norm": 5.078902244567871,
        "learning_rate": 9.010368744068593e-05,
        "epoch": 1.0233333333333334,
        "step": 7675
    },
    {
        "loss": 2.7085,
        "grad_norm": 2.8908867835998535,
        "learning_rate": 9.004106160727324e-05,
        "epoch": 1.0234666666666667,
        "step": 7676
    },
    {
        "loss": 1.2404,
        "grad_norm": 4.414392948150635,
        "learning_rate": 8.997843971862975e-05,
        "epoch": 1.0236,
        "step": 7677
    },
    {
        "loss": 1.5224,
        "grad_norm": 3.517730474472046,
        "learning_rate": 8.991582179956058e-05,
        "epoch": 1.0237333333333334,
        "step": 7678
    },
    {
        "loss": 2.5251,
        "grad_norm": 5.837968826293945,
        "learning_rate": 8.985320787486863e-05,
        "epoch": 1.0238666666666667,
        "step": 7679
    },
    {
        "loss": 1.0938,
        "grad_norm": 4.816255569458008,
        "learning_rate": 8.979059796935581e-05,
        "epoch": 1.024,
        "step": 7680
    },
    {
        "loss": 1.4524,
        "grad_norm": 3.1518020629882812,
        "learning_rate": 8.972799210782178e-05,
        "epoch": 1.0241333333333333,
        "step": 7681
    },
    {
        "loss": 2.0033,
        "grad_norm": 3.377824544906616,
        "learning_rate": 8.966539031506509e-05,
        "epoch": 1.0242666666666667,
        "step": 7682
    },
    {
        "loss": 1.947,
        "grad_norm": 4.428462028503418,
        "learning_rate": 8.960279261588251e-05,
        "epoch": 1.0244,
        "step": 7683
    },
    {
        "loss": 3.0398,
        "grad_norm": 4.382453918457031,
        "learning_rate": 8.954019903506926e-05,
        "epoch": 1.0245333333333333,
        "step": 7684
    },
    {
        "loss": 2.0869,
        "grad_norm": 2.0790021419525146,
        "learning_rate": 8.947760959741882e-05,
        "epoch": 1.0246666666666666,
        "step": 7685
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.3210806846618652,
        "learning_rate": 8.941502432772294e-05,
        "epoch": 1.0248,
        "step": 7686
    },
    {
        "loss": 2.0078,
        "grad_norm": 2.6556382179260254,
        "learning_rate": 8.935244325077213e-05,
        "epoch": 1.0249333333333333,
        "step": 7687
    },
    {
        "loss": 1.4256,
        "grad_norm": 3.4841506481170654,
        "learning_rate": 8.928986639135481e-05,
        "epoch": 1.0250666666666666,
        "step": 7688
    },
    {
        "loss": 1.6781,
        "grad_norm": 3.7390241622924805,
        "learning_rate": 8.922729377425784e-05,
        "epoch": 1.0252,
        "step": 7689
    },
    {
        "loss": 2.0927,
        "grad_norm": 3.147395133972168,
        "learning_rate": 8.916472542426654e-05,
        "epoch": 1.0253333333333334,
        "step": 7690
    },
    {
        "loss": 2.2659,
        "grad_norm": 2.326359272003174,
        "learning_rate": 8.91021613661644e-05,
        "epoch": 1.0254666666666667,
        "step": 7691
    },
    {
        "loss": 1.6396,
        "grad_norm": 3.012535333633423,
        "learning_rate": 8.903960162473328e-05,
        "epoch": 1.0256,
        "step": 7692
    },
    {
        "loss": 2.7462,
        "grad_norm": 3.4807236194610596,
        "learning_rate": 8.897704622475338e-05,
        "epoch": 1.0257333333333334,
        "step": 7693
    },
    {
        "loss": 1.6057,
        "grad_norm": 1.7327215671539307,
        "learning_rate": 8.891449519100303e-05,
        "epoch": 1.0258666666666667,
        "step": 7694
    },
    {
        "loss": 3.1166,
        "grad_norm": 3.481131076812744,
        "learning_rate": 8.885194854825873e-05,
        "epoch": 1.026,
        "step": 7695
    },
    {
        "loss": 1.9901,
        "grad_norm": 3.9932308197021484,
        "learning_rate": 8.878940632129578e-05,
        "epoch": 1.0261333333333333,
        "step": 7696
    },
    {
        "loss": 0.4687,
        "grad_norm": 2.7900683879852295,
        "learning_rate": 8.872686853488718e-05,
        "epoch": 1.0262666666666667,
        "step": 7697
    },
    {
        "loss": 2.0939,
        "grad_norm": 4.984866142272949,
        "learning_rate": 8.866433521380425e-05,
        "epoch": 1.0264,
        "step": 7698
    },
    {
        "loss": 1.9896,
        "grad_norm": 4.3495192527771,
        "learning_rate": 8.860180638281683e-05,
        "epoch": 1.0265333333333333,
        "step": 7699
    },
    {
        "loss": 2.2267,
        "grad_norm": 4.842635631561279,
        "learning_rate": 8.853928206669272e-05,
        "epoch": 1.0266666666666666,
        "step": 7700
    },
    {
        "loss": 2.2489,
        "grad_norm": 3.3047497272491455,
        "learning_rate": 8.847676229019817e-05,
        "epoch": 1.0268,
        "step": 7701
    },
    {
        "loss": 1.9578,
        "grad_norm": 3.670945405960083,
        "learning_rate": 8.841424707809712e-05,
        "epoch": 1.0269333333333333,
        "step": 7702
    },
    {
        "loss": 2.537,
        "grad_norm": 4.4927802085876465,
        "learning_rate": 8.835173645515243e-05,
        "epoch": 1.0270666666666666,
        "step": 7703
    },
    {
        "loss": 1.8432,
        "grad_norm": 3.7975118160247803,
        "learning_rate": 8.82892304461245e-05,
        "epoch": 1.0272,
        "step": 7704
    },
    {
        "loss": 2.4777,
        "grad_norm": 3.107011079788208,
        "learning_rate": 8.822672907577247e-05,
        "epoch": 1.0273333333333334,
        "step": 7705
    },
    {
        "loss": 0.5187,
        "grad_norm": 2.49470853805542,
        "learning_rate": 8.816423236885303e-05,
        "epoch": 1.0274666666666668,
        "step": 7706
    },
    {
        "loss": 2.5197,
        "grad_norm": 2.0075490474700928,
        "learning_rate": 8.81017403501215e-05,
        "epoch": 1.0276,
        "step": 7707
    },
    {
        "loss": 2.0443,
        "grad_norm": 2.8089587688446045,
        "learning_rate": 8.803925304433118e-05,
        "epoch": 1.0277333333333334,
        "step": 7708
    },
    {
        "loss": 0.8835,
        "grad_norm": 2.579475164413452,
        "learning_rate": 8.797677047623343e-05,
        "epoch": 1.0278666666666667,
        "step": 7709
    },
    {
        "loss": 0.6275,
        "grad_norm": 2.7348387241363525,
        "learning_rate": 8.791429267057802e-05,
        "epoch": 1.028,
        "step": 7710
    },
    {
        "loss": 2.204,
        "grad_norm": 4.083411693572998,
        "learning_rate": 8.785181965211228e-05,
        "epoch": 1.0281333333333333,
        "step": 7711
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.637059450149536,
        "learning_rate": 8.778935144558231e-05,
        "epoch": 1.0282666666666667,
        "step": 7712
    },
    {
        "loss": 1.4134,
        "grad_norm": 2.016390562057495,
        "learning_rate": 8.772688807573167e-05,
        "epoch": 1.0284,
        "step": 7713
    },
    {
        "loss": 2.934,
        "grad_norm": 2.5557000637054443,
        "learning_rate": 8.76644295673027e-05,
        "epoch": 1.0285333333333333,
        "step": 7714
    },
    {
        "loss": 1.9381,
        "grad_norm": 2.4220097064971924,
        "learning_rate": 8.760197594503508e-05,
        "epoch": 1.0286666666666666,
        "step": 7715
    },
    {
        "loss": 3.7911,
        "grad_norm": 4.456204891204834,
        "learning_rate": 8.7539527233667e-05,
        "epoch": 1.0288,
        "step": 7716
    },
    {
        "loss": 1.8037,
        "grad_norm": 3.270582675933838,
        "learning_rate": 8.747708345793468e-05,
        "epoch": 1.0289333333333333,
        "step": 7717
    },
    {
        "loss": 2.2206,
        "grad_norm": 3.326681613922119,
        "learning_rate": 8.741464464257234e-05,
        "epoch": 1.0290666666666666,
        "step": 7718
    },
    {
        "loss": 1.1584,
        "grad_norm": 2.6746652126312256,
        "learning_rate": 8.735221081231215e-05,
        "epoch": 1.0292,
        "step": 7719
    },
    {
        "loss": 1.3645,
        "grad_norm": 3.0322265625,
        "learning_rate": 8.728978199188418e-05,
        "epoch": 1.0293333333333334,
        "step": 7720
    },
    {
        "loss": 2.5681,
        "grad_norm": 3.539426326751709,
        "learning_rate": 8.722735820601702e-05,
        "epoch": 1.0294666666666668,
        "step": 7721
    },
    {
        "loss": 1.9409,
        "grad_norm": 2.8473565578460693,
        "learning_rate": 8.716493947943683e-05,
        "epoch": 1.0296,
        "step": 7722
    },
    {
        "loss": 1.617,
        "grad_norm": 2.1675167083740234,
        "learning_rate": 8.710252583686775e-05,
        "epoch": 1.0297333333333334,
        "step": 7723
    },
    {
        "loss": 2.1573,
        "grad_norm": 2.721229314804077,
        "learning_rate": 8.704011730303212e-05,
        "epoch": 1.0298666666666667,
        "step": 7724
    },
    {
        "loss": 1.3061,
        "grad_norm": 2.570805311203003,
        "learning_rate": 8.69777139026502e-05,
        "epoch": 1.03,
        "step": 7725
    },
    {
        "loss": 2.4496,
        "grad_norm": 1.9554301500320435,
        "learning_rate": 8.691531566044032e-05,
        "epoch": 1.0301333333333333,
        "step": 7726
    },
    {
        "loss": 2.6874,
        "grad_norm": 4.713325500488281,
        "learning_rate": 8.685292260111827e-05,
        "epoch": 1.0302666666666667,
        "step": 7727
    },
    {
        "loss": 2.4653,
        "grad_norm": 4.4789252281188965,
        "learning_rate": 8.679053474939853e-05,
        "epoch": 1.0304,
        "step": 7728
    },
    {
        "loss": 1.9965,
        "grad_norm": 3.8203084468841553,
        "learning_rate": 8.672815212999285e-05,
        "epoch": 1.0305333333333333,
        "step": 7729
    },
    {
        "loss": 1.8332,
        "grad_norm": 2.2656924724578857,
        "learning_rate": 8.666577476761146e-05,
        "epoch": 1.0306666666666666,
        "step": 7730
    },
    {
        "loss": 2.4303,
        "grad_norm": 3.364616632461548,
        "learning_rate": 8.660340268696211e-05,
        "epoch": 1.0308,
        "step": 7731
    },
    {
        "loss": 2.1442,
        "grad_norm": 3.191911458969116,
        "learning_rate": 8.654103591275055e-05,
        "epoch": 1.0309333333333333,
        "step": 7732
    },
    {
        "loss": 2.2902,
        "grad_norm": 2.9106762409210205,
        "learning_rate": 8.647867446968053e-05,
        "epoch": 1.0310666666666666,
        "step": 7733
    },
    {
        "loss": 1.7134,
        "grad_norm": 6.754865646362305,
        "learning_rate": 8.641631838245361e-05,
        "epoch": 1.0312,
        "step": 7734
    },
    {
        "loss": 2.1447,
        "grad_norm": 3.1220011711120605,
        "learning_rate": 8.635396767576941e-05,
        "epoch": 1.0313333333333334,
        "step": 7735
    },
    {
        "loss": 1.8476,
        "grad_norm": 5.262947082519531,
        "learning_rate": 8.62916223743249e-05,
        "epoch": 1.0314666666666668,
        "step": 7736
    },
    {
        "loss": 2.3129,
        "grad_norm": 4.408276081085205,
        "learning_rate": 8.622928250281563e-05,
        "epoch": 1.0316,
        "step": 7737
    },
    {
        "loss": 1.8153,
        "grad_norm": 5.079676151275635,
        "learning_rate": 8.616694808593433e-05,
        "epoch": 1.0317333333333334,
        "step": 7738
    },
    {
        "loss": 2.1149,
        "grad_norm": 3.084087610244751,
        "learning_rate": 8.610461914837225e-05,
        "epoch": 1.0318666666666667,
        "step": 7739
    },
    {
        "loss": 1.4213,
        "grad_norm": 3.844943046569824,
        "learning_rate": 8.604229571481772e-05,
        "epoch": 1.032,
        "step": 7740
    },
    {
        "loss": 1.5087,
        "grad_norm": 2.9083709716796875,
        "learning_rate": 8.597997780995743e-05,
        "epoch": 1.0321333333333333,
        "step": 7741
    },
    {
        "loss": 2.5503,
        "grad_norm": 3.329784393310547,
        "learning_rate": 8.591766545847569e-05,
        "epoch": 1.0322666666666667,
        "step": 7742
    },
    {
        "loss": 2.2633,
        "grad_norm": 3.4923386573791504,
        "learning_rate": 8.58553586850547e-05,
        "epoch": 1.0324,
        "step": 7743
    },
    {
        "loss": 2.9387,
        "grad_norm": 2.3833811283111572,
        "learning_rate": 8.579305751437436e-05,
        "epoch": 1.0325333333333333,
        "step": 7744
    },
    {
        "loss": 1.0916,
        "grad_norm": 4.3558855056762695,
        "learning_rate": 8.573076197111219e-05,
        "epoch": 1.0326666666666666,
        "step": 7745
    },
    {
        "loss": 2.2073,
        "grad_norm": 4.701603889465332,
        "learning_rate": 8.566847207994398e-05,
        "epoch": 1.0328,
        "step": 7746
    },
    {
        "loss": 0.7563,
        "grad_norm": 4.458503246307373,
        "learning_rate": 8.56061878655428e-05,
        "epoch": 1.0329333333333333,
        "step": 7747
    },
    {
        "loss": 1.2938,
        "grad_norm": 4.482043266296387,
        "learning_rate": 8.554390935257961e-05,
        "epoch": 1.0330666666666666,
        "step": 7748
    },
    {
        "loss": 2.3182,
        "grad_norm": 4.1151838302612305,
        "learning_rate": 8.548163656572314e-05,
        "epoch": 1.0332,
        "step": 7749
    },
    {
        "loss": 2.5678,
        "grad_norm": 3.624995470046997,
        "learning_rate": 8.541936952963992e-05,
        "epoch": 1.0333333333333334,
        "step": 7750
    },
    {
        "loss": 2.0902,
        "grad_norm": 4.383675575256348,
        "learning_rate": 8.535710826899413e-05,
        "epoch": 1.0334666666666668,
        "step": 7751
    },
    {
        "loss": 1.8648,
        "grad_norm": 2.7358598709106445,
        "learning_rate": 8.529485280844767e-05,
        "epoch": 1.0336,
        "step": 7752
    },
    {
        "loss": 0.7321,
        "grad_norm": 3.8486318588256836,
        "learning_rate": 8.523260317266015e-05,
        "epoch": 1.0337333333333334,
        "step": 7753
    },
    {
        "loss": 2.6329,
        "grad_norm": 2.3516571521759033,
        "learning_rate": 8.51703593862887e-05,
        "epoch": 1.0338666666666667,
        "step": 7754
    },
    {
        "loss": 2.1309,
        "grad_norm": 4.391445636749268,
        "learning_rate": 8.510812147398857e-05,
        "epoch": 1.034,
        "step": 7755
    },
    {
        "loss": 2.0455,
        "grad_norm": 2.6036808490753174,
        "learning_rate": 8.504588946041223e-05,
        "epoch": 1.0341333333333333,
        "step": 7756
    },
    {
        "loss": 1.5599,
        "grad_norm": 4.851375579833984,
        "learning_rate": 8.498366337020998e-05,
        "epoch": 1.0342666666666667,
        "step": 7757
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.7439374923706055,
        "learning_rate": 8.492144322802986e-05,
        "epoch": 1.0344,
        "step": 7758
    },
    {
        "loss": 1.3013,
        "grad_norm": 3.3196728229522705,
        "learning_rate": 8.485922905851742e-05,
        "epoch": 1.0345333333333333,
        "step": 7759
    },
    {
        "loss": 2.6807,
        "grad_norm": 3.6134791374206543,
        "learning_rate": 8.47970208863161e-05,
        "epoch": 1.0346666666666666,
        "step": 7760
    },
    {
        "loss": 2.0012,
        "grad_norm": 3.8564844131469727,
        "learning_rate": 8.47348187360664e-05,
        "epoch": 1.0348,
        "step": 7761
    },
    {
        "loss": 2.6731,
        "grad_norm": 3.6828958988189697,
        "learning_rate": 8.467262263240716e-05,
        "epoch": 1.0349333333333333,
        "step": 7762
    },
    {
        "loss": 2.162,
        "grad_norm": 3.083914041519165,
        "learning_rate": 8.461043259997416e-05,
        "epoch": 1.0350666666666666,
        "step": 7763
    },
    {
        "loss": 1.6588,
        "grad_norm": 4.3967366218566895,
        "learning_rate": 8.454824866340147e-05,
        "epoch": 1.0352,
        "step": 7764
    },
    {
        "loss": 2.0594,
        "grad_norm": 6.944418430328369,
        "learning_rate": 8.448607084732e-05,
        "epoch": 1.0353333333333334,
        "step": 7765
    },
    {
        "loss": 2.8168,
        "grad_norm": 5.472807884216309,
        "learning_rate": 8.442389917635872e-05,
        "epoch": 1.0354666666666668,
        "step": 7766
    },
    {
        "loss": 1.6697,
        "grad_norm": 2.863706111907959,
        "learning_rate": 8.436173367514404e-05,
        "epoch": 1.0356,
        "step": 7767
    },
    {
        "loss": 2.5027,
        "grad_norm": 3.548628807067871,
        "learning_rate": 8.429957436830003e-05,
        "epoch": 1.0357333333333334,
        "step": 7768
    },
    {
        "loss": 2.3459,
        "grad_norm": 2.677501678466797,
        "learning_rate": 8.423742128044811e-05,
        "epoch": 1.0358666666666667,
        "step": 7769
    },
    {
        "loss": 1.3397,
        "grad_norm": 3.5743606090545654,
        "learning_rate": 8.417527443620716e-05,
        "epoch": 1.036,
        "step": 7770
    },
    {
        "loss": 1.8785,
        "grad_norm": 4.084017276763916,
        "learning_rate": 8.411313386019409e-05,
        "epoch": 1.0361333333333334,
        "step": 7771
    },
    {
        "loss": 2.4324,
        "grad_norm": 3.942474126815796,
        "learning_rate": 8.40509995770227e-05,
        "epoch": 1.0362666666666667,
        "step": 7772
    },
    {
        "loss": 1.8352,
        "grad_norm": 3.850041389465332,
        "learning_rate": 8.398887161130494e-05,
        "epoch": 1.0364,
        "step": 7773
    },
    {
        "loss": 2.1564,
        "grad_norm": 5.13205623626709,
        "learning_rate": 8.392674998764951e-05,
        "epoch": 1.0365333333333333,
        "step": 7774
    },
    {
        "loss": 2.5334,
        "grad_norm": 3.2359139919281006,
        "learning_rate": 8.386463473066321e-05,
        "epoch": 1.0366666666666666,
        "step": 7775
    },
    {
        "loss": 2.0322,
        "grad_norm": 3.407087802886963,
        "learning_rate": 8.380252586495008e-05,
        "epoch": 1.0368,
        "step": 7776
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.8548388481140137,
        "learning_rate": 8.37404234151117e-05,
        "epoch": 1.0369333333333333,
        "step": 7777
    },
    {
        "loss": 2.0646,
        "grad_norm": 3.4464895725250244,
        "learning_rate": 8.3678327405747e-05,
        "epoch": 1.0370666666666666,
        "step": 7778
    },
    {
        "loss": 2.2759,
        "grad_norm": 4.205596446990967,
        "learning_rate": 8.361623786145229e-05,
        "epoch": 1.0372,
        "step": 7779
    },
    {
        "loss": 2.5214,
        "grad_norm": 2.5299365520477295,
        "learning_rate": 8.355415480682175e-05,
        "epoch": 1.0373333333333334,
        "step": 7780
    },
    {
        "loss": 2.1466,
        "grad_norm": 2.672252655029297,
        "learning_rate": 8.349207826644651e-05,
        "epoch": 1.0374666666666668,
        "step": 7781
    },
    {
        "loss": 2.3821,
        "grad_norm": 3.544926881790161,
        "learning_rate": 8.343000826491525e-05,
        "epoch": 1.0376,
        "step": 7782
    },
    {
        "loss": 1.1577,
        "grad_norm": 3.6482458114624023,
        "learning_rate": 8.336794482681419e-05,
        "epoch": 1.0377333333333334,
        "step": 7783
    },
    {
        "loss": 2.7112,
        "grad_norm": 3.0047049522399902,
        "learning_rate": 8.330588797672686e-05,
        "epoch": 1.0378666666666667,
        "step": 7784
    },
    {
        "loss": 1.7701,
        "grad_norm": 4.629358291625977,
        "learning_rate": 8.324383773923431e-05,
        "epoch": 1.038,
        "step": 7785
    },
    {
        "loss": 2.2747,
        "grad_norm": 2.7538464069366455,
        "learning_rate": 8.318179413891452e-05,
        "epoch": 1.0381333333333334,
        "step": 7786
    },
    {
        "loss": 1.7924,
        "grad_norm": 3.112133502960205,
        "learning_rate": 8.311975720034351e-05,
        "epoch": 1.0382666666666667,
        "step": 7787
    },
    {
        "loss": 2.3128,
        "grad_norm": 2.5285332202911377,
        "learning_rate": 8.305772694809406e-05,
        "epoch": 1.0384,
        "step": 7788
    },
    {
        "loss": 2.1385,
        "grad_norm": 4.1554107666015625,
        "learning_rate": 8.29957034067369e-05,
        "epoch": 1.0385333333333333,
        "step": 7789
    },
    {
        "loss": 2.2259,
        "grad_norm": 3.315678119659424,
        "learning_rate": 8.293368660083944e-05,
        "epoch": 1.0386666666666666,
        "step": 7790
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.0400071144104004,
        "learning_rate": 8.287167655496686e-05,
        "epoch": 1.0388,
        "step": 7791
    },
    {
        "loss": 2.2903,
        "grad_norm": 4.260866165161133,
        "learning_rate": 8.280967329368155e-05,
        "epoch": 1.0389333333333333,
        "step": 7792
    },
    {
        "loss": 1.7148,
        "grad_norm": 3.9254696369171143,
        "learning_rate": 8.274767684154323e-05,
        "epoch": 1.0390666666666666,
        "step": 7793
    },
    {
        "loss": 1.5723,
        "grad_norm": 3.235422372817993,
        "learning_rate": 8.268568722310904e-05,
        "epoch": 1.0392,
        "step": 7794
    },
    {
        "loss": 2.4787,
        "grad_norm": 4.053859710693359,
        "learning_rate": 8.262370446293293e-05,
        "epoch": 1.0393333333333334,
        "step": 7795
    },
    {
        "loss": 0.6819,
        "grad_norm": 2.8315045833587646,
        "learning_rate": 8.256172858556682e-05,
        "epoch": 1.0394666666666668,
        "step": 7796
    },
    {
        "loss": 2.5704,
        "grad_norm": 3.2182016372680664,
        "learning_rate": 8.249975961555931e-05,
        "epoch": 1.0396,
        "step": 7797
    },
    {
        "loss": 1.2289,
        "grad_norm": 2.9037206172943115,
        "learning_rate": 8.243779757745685e-05,
        "epoch": 1.0397333333333334,
        "step": 7798
    },
    {
        "loss": 2.7133,
        "grad_norm": 3.5958964824676514,
        "learning_rate": 8.237584249580246e-05,
        "epoch": 1.0398666666666667,
        "step": 7799
    },
    {
        "loss": 1.0164,
        "grad_norm": 3.353701114654541,
        "learning_rate": 8.231389439513694e-05,
        "epoch": 1.04,
        "step": 7800
    },
    {
        "loss": 1.9651,
        "grad_norm": 5.548664093017578,
        "learning_rate": 8.22519532999981e-05,
        "epoch": 1.0401333333333334,
        "step": 7801
    },
    {
        "loss": 2.5871,
        "grad_norm": 4.211902618408203,
        "learning_rate": 8.21900192349211e-05,
        "epoch": 1.0402666666666667,
        "step": 7802
    },
    {
        "loss": 2.4021,
        "grad_norm": 3.419924259185791,
        "learning_rate": 8.212809222443815e-05,
        "epoch": 1.0404,
        "step": 7803
    },
    {
        "loss": 1.1088,
        "grad_norm": 6.284378528594971,
        "learning_rate": 8.206617229307865e-05,
        "epoch": 1.0405333333333333,
        "step": 7804
    },
    {
        "loss": 2.2647,
        "grad_norm": 2.89044189453125,
        "learning_rate": 8.200425946536956e-05,
        "epoch": 1.0406666666666666,
        "step": 7805
    },
    {
        "loss": 0.6485,
        "grad_norm": 3.361741781234741,
        "learning_rate": 8.19423537658346e-05,
        "epoch": 1.0408,
        "step": 7806
    },
    {
        "loss": 1.7593,
        "grad_norm": 3.3498220443725586,
        "learning_rate": 8.188045521899475e-05,
        "epoch": 1.0409333333333333,
        "step": 7807
    },
    {
        "loss": 1.4294,
        "grad_norm": 4.019435882568359,
        "learning_rate": 8.181856384936837e-05,
        "epoch": 1.0410666666666666,
        "step": 7808
    },
    {
        "loss": 2.0902,
        "grad_norm": 3.4615318775177,
        "learning_rate": 8.175667968147078e-05,
        "epoch": 1.0412,
        "step": 7809
    },
    {
        "loss": 1.7365,
        "grad_norm": 3.8261232376098633,
        "learning_rate": 8.169480273981453e-05,
        "epoch": 1.0413333333333332,
        "step": 7810
    },
    {
        "loss": 0.8942,
        "grad_norm": 4.71165132522583,
        "learning_rate": 8.163293304890937e-05,
        "epoch": 1.0414666666666668,
        "step": 7811
    },
    {
        "loss": 1.478,
        "grad_norm": 5.88477897644043,
        "learning_rate": 8.1571070633262e-05,
        "epoch": 1.0416,
        "step": 7812
    },
    {
        "loss": 2.099,
        "grad_norm": 3.249753952026367,
        "learning_rate": 8.15092155173762e-05,
        "epoch": 1.0417333333333334,
        "step": 7813
    },
    {
        "loss": 2.6972,
        "grad_norm": 3.0451059341430664,
        "learning_rate": 8.14473677257533e-05,
        "epoch": 1.0418666666666667,
        "step": 7814
    },
    {
        "loss": 2.4916,
        "grad_norm": 3.6250722408294678,
        "learning_rate": 8.138552728289127e-05,
        "epoch": 1.042,
        "step": 7815
    },
    {
        "loss": 3.2431,
        "grad_norm": 3.0074422359466553,
        "learning_rate": 8.132369421328523e-05,
        "epoch": 1.0421333333333334,
        "step": 7816
    },
    {
        "loss": 2.3594,
        "grad_norm": 3.5439231395721436,
        "learning_rate": 8.126186854142752e-05,
        "epoch": 1.0422666666666667,
        "step": 7817
    },
    {
        "loss": 2.154,
        "grad_norm": 3.4671974182128906,
        "learning_rate": 8.120005029180757e-05,
        "epoch": 1.0424,
        "step": 7818
    },
    {
        "loss": 1.8286,
        "grad_norm": 3.170438528060913,
        "learning_rate": 8.113823948891188e-05,
        "epoch": 1.0425333333333333,
        "step": 7819
    },
    {
        "loss": 0.8852,
        "grad_norm": 2.266432285308838,
        "learning_rate": 8.10764361572236e-05,
        "epoch": 1.0426666666666666,
        "step": 7820
    },
    {
        "loss": 1.5016,
        "grad_norm": 3.9262940883636475,
        "learning_rate": 8.101464032122354e-05,
        "epoch": 1.0428,
        "step": 7821
    },
    {
        "loss": 1.7654,
        "grad_norm": 2.0689494609832764,
        "learning_rate": 8.095285200538904e-05,
        "epoch": 1.0429333333333333,
        "step": 7822
    },
    {
        "loss": 2.1797,
        "grad_norm": 4.356232166290283,
        "learning_rate": 8.089107123419497e-05,
        "epoch": 1.0430666666666666,
        "step": 7823
    },
    {
        "loss": 2.3211,
        "grad_norm": 3.722529411315918,
        "learning_rate": 8.082929803211255e-05,
        "epoch": 1.0432,
        "step": 7824
    },
    {
        "loss": 1.8724,
        "grad_norm": 5.008166313171387,
        "learning_rate": 8.076753242361048e-05,
        "epoch": 1.0433333333333334,
        "step": 7825
    },
    {
        "loss": 1.9741,
        "grad_norm": 4.9273881912231445,
        "learning_rate": 8.070577443315439e-05,
        "epoch": 1.0434666666666668,
        "step": 7826
    },
    {
        "loss": 1.8231,
        "grad_norm": 3.5439183712005615,
        "learning_rate": 8.064402408520683e-05,
        "epoch": 1.0436,
        "step": 7827
    },
    {
        "loss": 2.4633,
        "grad_norm": 3.8320069313049316,
        "learning_rate": 8.05822814042273e-05,
        "epoch": 1.0437333333333334,
        "step": 7828
    },
    {
        "loss": 2.0584,
        "grad_norm": 2.9142322540283203,
        "learning_rate": 8.052054641467212e-05,
        "epoch": 1.0438666666666667,
        "step": 7829
    },
    {
        "loss": 2.3604,
        "grad_norm": 3.2220561504364014,
        "learning_rate": 8.045881914099502e-05,
        "epoch": 1.044,
        "step": 7830
    },
    {
        "loss": 1.8589,
        "grad_norm": 3.0456137657165527,
        "learning_rate": 8.03970996076461e-05,
        "epoch": 1.0441333333333334,
        "step": 7831
    },
    {
        "loss": 1.9863,
        "grad_norm": 2.9275245666503906,
        "learning_rate": 8.033538783907303e-05,
        "epoch": 1.0442666666666667,
        "step": 7832
    },
    {
        "loss": 1.7321,
        "grad_norm": 3.7345409393310547,
        "learning_rate": 8.027368385971972e-05,
        "epoch": 1.0444,
        "step": 7833
    },
    {
        "loss": 1.8874,
        "grad_norm": 3.70554780960083,
        "learning_rate": 8.021198769402744e-05,
        "epoch": 1.0445333333333333,
        "step": 7834
    },
    {
        "loss": 2.77,
        "grad_norm": 5.174532890319824,
        "learning_rate": 8.015029936643425e-05,
        "epoch": 1.0446666666666666,
        "step": 7835
    },
    {
        "loss": 2.3355,
        "grad_norm": 2.1496188640594482,
        "learning_rate": 8.00886189013752e-05,
        "epoch": 1.0448,
        "step": 7836
    },
    {
        "loss": 2.5039,
        "grad_norm": 3.263732433319092,
        "learning_rate": 8.002694632328205e-05,
        "epoch": 1.0449333333333333,
        "step": 7837
    },
    {
        "loss": 2.6817,
        "grad_norm": 4.93101692199707,
        "learning_rate": 7.996528165658338e-05,
        "epoch": 1.0450666666666666,
        "step": 7838
    },
    {
        "loss": 1.8556,
        "grad_norm": 3.8160996437072754,
        "learning_rate": 7.990362492570503e-05,
        "epoch": 1.0452,
        "step": 7839
    },
    {
        "loss": 2.5776,
        "grad_norm": 2.509920358657837,
        "learning_rate": 7.984197615506934e-05,
        "epoch": 1.0453333333333332,
        "step": 7840
    },
    {
        "loss": 2.4428,
        "grad_norm": 3.2497854232788086,
        "learning_rate": 7.978033536909553e-05,
        "epoch": 1.0454666666666668,
        "step": 7841
    },
    {
        "loss": 0.9603,
        "grad_norm": 3.395231246948242,
        "learning_rate": 7.971870259219976e-05,
        "epoch": 1.0456,
        "step": 7842
    },
    {
        "loss": 2.2984,
        "grad_norm": 3.9843361377716064,
        "learning_rate": 7.965707784879498e-05,
        "epoch": 1.0457333333333334,
        "step": 7843
    },
    {
        "loss": 2.644,
        "grad_norm": 1.6509473323822021,
        "learning_rate": 7.959546116329113e-05,
        "epoch": 1.0458666666666667,
        "step": 7844
    },
    {
        "loss": 2.456,
        "grad_norm": 2.332789182662964,
        "learning_rate": 7.953385256009446e-05,
        "epoch": 1.046,
        "step": 7845
    },
    {
        "loss": 1.8249,
        "grad_norm": 2.984088897705078,
        "learning_rate": 7.947225206360865e-05,
        "epoch": 1.0461333333333334,
        "step": 7846
    },
    {
        "loss": 1.6052,
        "grad_norm": 3.6854701042175293,
        "learning_rate": 7.94106596982336e-05,
        "epoch": 1.0462666666666667,
        "step": 7847
    },
    {
        "loss": 4.202,
        "grad_norm": 5.155655384063721,
        "learning_rate": 7.93490754883666e-05,
        "epoch": 1.0464,
        "step": 7848
    },
    {
        "loss": 2.2192,
        "grad_norm": 3.1130993366241455,
        "learning_rate": 7.928749945840104e-05,
        "epoch": 1.0465333333333333,
        "step": 7849
    },
    {
        "loss": 2.5762,
        "grad_norm": 4.324869155883789,
        "learning_rate": 7.922593163272751e-05,
        "epoch": 1.0466666666666666,
        "step": 7850
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.2235004901885986,
        "learning_rate": 7.916437203573326e-05,
        "epoch": 1.0468,
        "step": 7851
    },
    {
        "loss": 0.9887,
        "grad_norm": 4.2259135246276855,
        "learning_rate": 7.910282069180224e-05,
        "epoch": 1.0469333333333333,
        "step": 7852
    },
    {
        "loss": 2.8653,
        "grad_norm": 3.922499418258667,
        "learning_rate": 7.904127762531528e-05,
        "epoch": 1.0470666666666666,
        "step": 7853
    },
    {
        "loss": 2.0574,
        "grad_norm": 5.104011535644531,
        "learning_rate": 7.897974286064946e-05,
        "epoch": 1.0472,
        "step": 7854
    },
    {
        "loss": 2.2836,
        "grad_norm": 4.3775129318237305,
        "learning_rate": 7.891821642217926e-05,
        "epoch": 1.0473333333333332,
        "step": 7855
    },
    {
        "loss": 1.8969,
        "grad_norm": 2.951681137084961,
        "learning_rate": 7.885669833427525e-05,
        "epoch": 1.0474666666666668,
        "step": 7856
    },
    {
        "loss": 2.0184,
        "grad_norm": 2.929877996444702,
        "learning_rate": 7.879518862130524e-05,
        "epoch": 1.0476,
        "step": 7857
    },
    {
        "loss": 1.6145,
        "grad_norm": 3.055098533630371,
        "learning_rate": 7.873368730763315e-05,
        "epoch": 1.0477333333333334,
        "step": 7858
    },
    {
        "loss": 2.8496,
        "grad_norm": 3.344412088394165,
        "learning_rate": 7.867219441761999e-05,
        "epoch": 1.0478666666666667,
        "step": 7859
    },
    {
        "loss": 1.1955,
        "grad_norm": 3.1301817893981934,
        "learning_rate": 7.861070997562328e-05,
        "epoch": 1.048,
        "step": 7860
    },
    {
        "loss": 1.5382,
        "grad_norm": 6.6031975746154785,
        "learning_rate": 7.854923400599731e-05,
        "epoch": 1.0481333333333334,
        "step": 7861
    },
    {
        "loss": 1.3128,
        "grad_norm": 3.6320736408233643,
        "learning_rate": 7.848776653309284e-05,
        "epoch": 1.0482666666666667,
        "step": 7862
    },
    {
        "loss": 1.6985,
        "grad_norm": 3.9859352111816406,
        "learning_rate": 7.842630758125723e-05,
        "epoch": 1.0484,
        "step": 7863
    },
    {
        "loss": 1.5709,
        "grad_norm": 2.6525256633758545,
        "learning_rate": 7.83648571748348e-05,
        "epoch": 1.0485333333333333,
        "step": 7864
    },
    {
        "loss": 2.8178,
        "grad_norm": 2.1699368953704834,
        "learning_rate": 7.830341533816619e-05,
        "epoch": 1.0486666666666666,
        "step": 7865
    },
    {
        "loss": 1.1287,
        "grad_norm": 1.6790820360183716,
        "learning_rate": 7.824198209558862e-05,
        "epoch": 1.0488,
        "step": 7866
    },
    {
        "loss": 2.516,
        "grad_norm": 2.327960252761841,
        "learning_rate": 7.818055747143608e-05,
        "epoch": 1.0489333333333333,
        "step": 7867
    },
    {
        "loss": 2.2801,
        "grad_norm": 4.356217861175537,
        "learning_rate": 7.811914149003905e-05,
        "epoch": 1.0490666666666666,
        "step": 7868
    },
    {
        "loss": 2.5347,
        "grad_norm": 2.218137741088867,
        "learning_rate": 7.805773417572479e-05,
        "epoch": 1.0492,
        "step": 7869
    },
    {
        "loss": 1.8879,
        "grad_norm": 3.3286638259887695,
        "learning_rate": 7.799633555281659e-05,
        "epoch": 1.0493333333333332,
        "step": 7870
    },
    {
        "loss": 1.8015,
        "grad_norm": 3.6311099529266357,
        "learning_rate": 7.793494564563499e-05,
        "epoch": 1.0494666666666668,
        "step": 7871
    },
    {
        "loss": 2.1193,
        "grad_norm": 2.5224456787109375,
        "learning_rate": 7.787356447849645e-05,
        "epoch": 1.0496,
        "step": 7872
    },
    {
        "loss": 2.4347,
        "grad_norm": 2.6495704650878906,
        "learning_rate": 7.781219207571457e-05,
        "epoch": 1.0497333333333334,
        "step": 7873
    },
    {
        "loss": 1.4475,
        "grad_norm": 2.375977039337158,
        "learning_rate": 7.7750828461599e-05,
        "epoch": 1.0498666666666667,
        "step": 7874
    },
    {
        "loss": 2.0607,
        "grad_norm": 4.536863803863525,
        "learning_rate": 7.7689473660456e-05,
        "epoch": 1.05,
        "step": 7875
    },
    {
        "loss": 2.8423,
        "grad_norm": 5.256189346313477,
        "learning_rate": 7.762812769658854e-05,
        "epoch": 1.0501333333333334,
        "step": 7876
    },
    {
        "loss": 1.4963,
        "grad_norm": 3.478628396987915,
        "learning_rate": 7.756679059429592e-05,
        "epoch": 1.0502666666666667,
        "step": 7877
    },
    {
        "loss": 1.6182,
        "grad_norm": 3.9513802528381348,
        "learning_rate": 7.750546237787413e-05,
        "epoch": 1.0504,
        "step": 7878
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.606926441192627,
        "learning_rate": 7.74441430716151e-05,
        "epoch": 1.0505333333333333,
        "step": 7879
    },
    {
        "loss": 2.6645,
        "grad_norm": 3.0922305583953857,
        "learning_rate": 7.738283269980803e-05,
        "epoch": 1.0506666666666666,
        "step": 7880
    },
    {
        "loss": 0.6299,
        "grad_norm": 3.31292724609375,
        "learning_rate": 7.732153128673784e-05,
        "epoch": 1.0508,
        "step": 7881
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.693389892578125,
        "learning_rate": 7.72602388566866e-05,
        "epoch": 1.0509333333333333,
        "step": 7882
    },
    {
        "loss": 1.0766,
        "grad_norm": 3.176002025604248,
        "learning_rate": 7.719895543393207e-05,
        "epoch": 1.0510666666666666,
        "step": 7883
    },
    {
        "loss": 3.0489,
        "grad_norm": 5.534043788909912,
        "learning_rate": 7.713768104274901e-05,
        "epoch": 1.0512,
        "step": 7884
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.6299023628234863,
        "learning_rate": 7.70764157074084e-05,
        "epoch": 1.0513333333333332,
        "step": 7885
    },
    {
        "loss": 2.3484,
        "grad_norm": 2.573587417602539,
        "learning_rate": 7.701515945217767e-05,
        "epoch": 1.0514666666666668,
        "step": 7886
    },
    {
        "loss": 1.2265,
        "grad_norm": 3.2489089965820312,
        "learning_rate": 7.695391230132062e-05,
        "epoch": 1.0516,
        "step": 7887
    },
    {
        "loss": 2.2928,
        "grad_norm": 2.5466830730438232,
        "learning_rate": 7.689267427909731e-05,
        "epoch": 1.0517333333333334,
        "step": 7888
    },
    {
        "loss": 1.3185,
        "grad_norm": 4.309272766113281,
        "learning_rate": 7.683144540976458e-05,
        "epoch": 1.0518666666666667,
        "step": 7889
    },
    {
        "loss": 2.6887,
        "grad_norm": 3.3747000694274902,
        "learning_rate": 7.677022571757524e-05,
        "epoch": 1.052,
        "step": 7890
    },
    {
        "loss": 2.8787,
        "grad_norm": 3.399414539337158,
        "learning_rate": 7.670901522677862e-05,
        "epoch": 1.0521333333333334,
        "step": 7891
    },
    {
        "loss": 0.9159,
        "grad_norm": 3.3292593955993652,
        "learning_rate": 7.664781396162044e-05,
        "epoch": 1.0522666666666667,
        "step": 7892
    },
    {
        "loss": 1.4695,
        "grad_norm": 4.2113728523254395,
        "learning_rate": 7.658662194634267e-05,
        "epoch": 1.0524,
        "step": 7893
    },
    {
        "loss": 1.516,
        "grad_norm": 2.36857533454895,
        "learning_rate": 7.652543920518375e-05,
        "epoch": 1.0525333333333333,
        "step": 7894
    },
    {
        "loss": 1.6651,
        "grad_norm": 1.8211201429367065,
        "learning_rate": 7.646426576237844e-05,
        "epoch": 1.0526666666666666,
        "step": 7895
    },
    {
        "loss": 2.4963,
        "grad_norm": 2.5773136615753174,
        "learning_rate": 7.640310164215764e-05,
        "epoch": 1.0528,
        "step": 7896
    },
    {
        "loss": 0.7165,
        "grad_norm": 2.813086748123169,
        "learning_rate": 7.634194686874852e-05,
        "epoch": 1.0529333333333333,
        "step": 7897
    },
    {
        "loss": 1.9083,
        "grad_norm": 2.751361608505249,
        "learning_rate": 7.628080146637503e-05,
        "epoch": 1.0530666666666666,
        "step": 7898
    },
    {
        "loss": 2.4461,
        "grad_norm": 4.26284122467041,
        "learning_rate": 7.62196654592569e-05,
        "epoch": 1.0532,
        "step": 7899
    },
    {
        "loss": 1.9495,
        "grad_norm": 4.048759460449219,
        "learning_rate": 7.615853887161021e-05,
        "epoch": 1.0533333333333332,
        "step": 7900
    },
    {
        "loss": 1.8376,
        "grad_norm": 3.6785480976104736,
        "learning_rate": 7.609742172764754e-05,
        "epoch": 1.0534666666666668,
        "step": 7901
    },
    {
        "loss": 2.4683,
        "grad_norm": 4.026406764984131,
        "learning_rate": 7.603631405157758e-05,
        "epoch": 1.0536,
        "step": 7902
    },
    {
        "loss": 1.9658,
        "grad_norm": 3.382721424102783,
        "learning_rate": 7.597521586760538e-05,
        "epoch": 1.0537333333333334,
        "step": 7903
    },
    {
        "loss": 1.7682,
        "grad_norm": 3.8339054584503174,
        "learning_rate": 7.591412719993185e-05,
        "epoch": 1.0538666666666667,
        "step": 7904
    },
    {
        "loss": 1.7739,
        "grad_norm": 3.4815385341644287,
        "learning_rate": 7.585304807275475e-05,
        "epoch": 1.054,
        "step": 7905
    },
    {
        "loss": 1.3719,
        "grad_norm": 2.806345224380493,
        "learning_rate": 7.579197851026746e-05,
        "epoch": 1.0541333333333334,
        "step": 7906
    },
    {
        "loss": 3.8478,
        "grad_norm": 3.9350168704986572,
        "learning_rate": 7.573091853666015e-05,
        "epoch": 1.0542666666666667,
        "step": 7907
    },
    {
        "loss": 1.9554,
        "grad_norm": 5.59946870803833,
        "learning_rate": 7.566986817611856e-05,
        "epoch": 1.0544,
        "step": 7908
    },
    {
        "loss": 1.3041,
        "grad_norm": 3.692026376724243,
        "learning_rate": 7.560882745282512e-05,
        "epoch": 1.0545333333333333,
        "step": 7909
    },
    {
        "loss": 2.2389,
        "grad_norm": 4.134609699249268,
        "learning_rate": 7.554779639095821e-05,
        "epoch": 1.0546666666666666,
        "step": 7910
    },
    {
        "loss": 2.6009,
        "grad_norm": 2.2672903537750244,
        "learning_rate": 7.548677501469248e-05,
        "epoch": 1.0548,
        "step": 7911
    },
    {
        "loss": 2.4467,
        "grad_norm": 3.301736354827881,
        "learning_rate": 7.542576334819882e-05,
        "epoch": 1.0549333333333333,
        "step": 7912
    },
    {
        "loss": 2.0109,
        "grad_norm": 3.0809059143066406,
        "learning_rate": 7.536476141564385e-05,
        "epoch": 1.0550666666666666,
        "step": 7913
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.2665321826934814,
        "learning_rate": 7.530376924119095e-05,
        "epoch": 1.0552,
        "step": 7914
    },
    {
        "loss": 2.05,
        "grad_norm": 2.7825822830200195,
        "learning_rate": 7.524278684899905e-05,
        "epoch": 1.0553333333333332,
        "step": 7915
    },
    {
        "loss": 1.7209,
        "grad_norm": 6.1316609382629395,
        "learning_rate": 7.518181426322386e-05,
        "epoch": 1.0554666666666668,
        "step": 7916
    },
    {
        "loss": 1.1161,
        "grad_norm": 4.514312267303467,
        "learning_rate": 7.512085150801648e-05,
        "epoch": 1.0556,
        "step": 7917
    },
    {
        "loss": 2.0977,
        "grad_norm": 3.9589226245880127,
        "learning_rate": 7.505989860752456e-05,
        "epoch": 1.0557333333333334,
        "step": 7918
    },
    {
        "loss": 0.8911,
        "grad_norm": 4.612318992614746,
        "learning_rate": 7.49989555858918e-05,
        "epoch": 1.0558666666666667,
        "step": 7919
    },
    {
        "loss": 2.2791,
        "grad_norm": 2.8784854412078857,
        "learning_rate": 7.4938022467258e-05,
        "epoch": 1.056,
        "step": 7920
    },
    {
        "loss": 1.1154,
        "grad_norm": 3.7662711143493652,
        "learning_rate": 7.48770992757589e-05,
        "epoch": 1.0561333333333334,
        "step": 7921
    },
    {
        "loss": 1.4802,
        "grad_norm": 3.8273653984069824,
        "learning_rate": 7.481618603552621e-05,
        "epoch": 1.0562666666666667,
        "step": 7922
    },
    {
        "loss": 2.4961,
        "grad_norm": 4.519709587097168,
        "learning_rate": 7.475528277068821e-05,
        "epoch": 1.0564,
        "step": 7923
    },
    {
        "loss": 1.3755,
        "grad_norm": 7.473303318023682,
        "learning_rate": 7.469438950536871e-05,
        "epoch": 1.0565333333333333,
        "step": 7924
    },
    {
        "loss": 1.4216,
        "grad_norm": 3.9773435592651367,
        "learning_rate": 7.46335062636877e-05,
        "epoch": 1.0566666666666666,
        "step": 7925
    },
    {
        "loss": 1.5187,
        "grad_norm": 4.164013862609863,
        "learning_rate": 7.457263306976128e-05,
        "epoch": 1.0568,
        "step": 7926
    },
    {
        "loss": 1.5489,
        "grad_norm": 4.710339546203613,
        "learning_rate": 7.451176994770155e-05,
        "epoch": 1.0569333333333333,
        "step": 7927
    },
    {
        "loss": 1.572,
        "grad_norm": 2.2333014011383057,
        "learning_rate": 7.445091692161673e-05,
        "epoch": 1.0570666666666666,
        "step": 7928
    },
    {
        "loss": 2.2509,
        "grad_norm": 3.6244606971740723,
        "learning_rate": 7.439007401561056e-05,
        "epoch": 1.0572,
        "step": 7929
    },
    {
        "loss": 1.9381,
        "grad_norm": 2.271071195602417,
        "learning_rate": 7.432924125378347e-05,
        "epoch": 1.0573333333333332,
        "step": 7930
    },
    {
        "loss": 2.855,
        "grad_norm": 4.277002811431885,
        "learning_rate": 7.426841866023128e-05,
        "epoch": 1.0574666666666666,
        "step": 7931
    },
    {
        "loss": 2.1523,
        "grad_norm": 2.6460437774658203,
        "learning_rate": 7.420760625904626e-05,
        "epoch": 1.0576,
        "step": 7932
    },
    {
        "loss": 1.802,
        "grad_norm": 4.495572566986084,
        "learning_rate": 7.414680407431626e-05,
        "epoch": 1.0577333333333334,
        "step": 7933
    },
    {
        "loss": 1.9691,
        "grad_norm": 2.850771188735962,
        "learning_rate": 7.408601213012521e-05,
        "epoch": 1.0578666666666667,
        "step": 7934
    },
    {
        "loss": 1.8253,
        "grad_norm": 3.412621259689331,
        "learning_rate": 7.402523045055307e-05,
        "epoch": 1.058,
        "step": 7935
    },
    {
        "loss": 1.9813,
        "grad_norm": 4.673500061035156,
        "learning_rate": 7.396445905967563e-05,
        "epoch": 1.0581333333333334,
        "step": 7936
    },
    {
        "loss": 1.1784,
        "grad_norm": 4.110273838043213,
        "learning_rate": 7.390369798156483e-05,
        "epoch": 1.0582666666666667,
        "step": 7937
    },
    {
        "loss": 1.9653,
        "grad_norm": 4.913242340087891,
        "learning_rate": 7.384294724028796e-05,
        "epoch": 1.0584,
        "step": 7938
    },
    {
        "loss": 0.9884,
        "grad_norm": 4.166382312774658,
        "learning_rate": 7.378220685990897e-05,
        "epoch": 1.0585333333333333,
        "step": 7939
    },
    {
        "loss": 2.0007,
        "grad_norm": 3.6587483882904053,
        "learning_rate": 7.3721476864487e-05,
        "epoch": 1.0586666666666666,
        "step": 7940
    },
    {
        "loss": 2.2498,
        "grad_norm": 3.2777366638183594,
        "learning_rate": 7.366075727807783e-05,
        "epoch": 1.0588,
        "step": 7941
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.979910135269165,
        "learning_rate": 7.360004812473231e-05,
        "epoch": 1.0589333333333333,
        "step": 7942
    },
    {
        "loss": 1.3971,
        "grad_norm": 4.5798773765563965,
        "learning_rate": 7.35393494284977e-05,
        "epoch": 1.0590666666666666,
        "step": 7943
    },
    {
        "loss": 2.3933,
        "grad_norm": 3.740811347961426,
        "learning_rate": 7.34786612134169e-05,
        "epoch": 1.0592,
        "step": 7944
    },
    {
        "loss": 1.8812,
        "grad_norm": 3.2716195583343506,
        "learning_rate": 7.341798350352884e-05,
        "epoch": 1.0593333333333332,
        "step": 7945
    },
    {
        "loss": 2.5681,
        "grad_norm": 3.9428694248199463,
        "learning_rate": 7.335731632286811e-05,
        "epoch": 1.0594666666666668,
        "step": 7946
    },
    {
        "loss": 0.4938,
        "grad_norm": 3.3359642028808594,
        "learning_rate": 7.329665969546501e-05,
        "epoch": 1.0596,
        "step": 7947
    },
    {
        "loss": 1.1545,
        "grad_norm": 2.5608584880828857,
        "learning_rate": 7.323601364534615e-05,
        "epoch": 1.0597333333333334,
        "step": 7948
    },
    {
        "loss": 1.7528,
        "grad_norm": 5.413125991821289,
        "learning_rate": 7.317537819653349e-05,
        "epoch": 1.0598666666666667,
        "step": 7949
    },
    {
        "loss": 1.9224,
        "grad_norm": 4.0377631187438965,
        "learning_rate": 7.311475337304485e-05,
        "epoch": 1.06,
        "step": 7950
    },
    {
        "loss": 2.2623,
        "grad_norm": 3.987826347351074,
        "learning_rate": 7.305413919889398e-05,
        "epoch": 1.0601333333333334,
        "step": 7951
    },
    {
        "loss": 2.461,
        "grad_norm": 5.3574299812316895,
        "learning_rate": 7.299353569809042e-05,
        "epoch": 1.0602666666666667,
        "step": 7952
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.2820804119110107,
        "learning_rate": 7.293294289463939e-05,
        "epoch": 1.0604,
        "step": 7953
    },
    {
        "loss": 1.6697,
        "grad_norm": 2.9373602867126465,
        "learning_rate": 7.287236081254196e-05,
        "epoch": 1.0605333333333333,
        "step": 7954
    },
    {
        "loss": 2.0089,
        "grad_norm": 3.430026054382324,
        "learning_rate": 7.281178947579486e-05,
        "epoch": 1.0606666666666666,
        "step": 7955
    },
    {
        "loss": 1.9323,
        "grad_norm": 4.060389518737793,
        "learning_rate": 7.275122890839046e-05,
        "epoch": 1.0608,
        "step": 7956
    },
    {
        "loss": 1.281,
        "grad_norm": 2.1627752780914307,
        "learning_rate": 7.269067913431727e-05,
        "epoch": 1.0609333333333333,
        "step": 7957
    },
    {
        "loss": 0.9177,
        "grad_norm": 4.003027439117432,
        "learning_rate": 7.263014017755911e-05,
        "epoch": 1.0610666666666666,
        "step": 7958
    },
    {
        "loss": 2.4298,
        "grad_norm": 2.2821555137634277,
        "learning_rate": 7.256961206209564e-05,
        "epoch": 1.0612,
        "step": 7959
    },
    {
        "loss": 1.4355,
        "grad_norm": 4.202019214630127,
        "learning_rate": 7.25090948119023e-05,
        "epoch": 1.0613333333333332,
        "step": 7960
    },
    {
        "loss": 2.0756,
        "grad_norm": 3.390312433242798,
        "learning_rate": 7.244858845095015e-05,
        "epoch": 1.0614666666666666,
        "step": 7961
    },
    {
        "loss": 1.7338,
        "grad_norm": 3.5144541263580322,
        "learning_rate": 7.238809300320614e-05,
        "epoch": 1.0616,
        "step": 7962
    },
    {
        "loss": 2.2574,
        "grad_norm": 3.7610042095184326,
        "learning_rate": 7.232760849263241e-05,
        "epoch": 1.0617333333333334,
        "step": 7963
    },
    {
        "loss": 2.2569,
        "grad_norm": 2.6368956565856934,
        "learning_rate": 7.226713494318735e-05,
        "epoch": 1.0618666666666667,
        "step": 7964
    },
    {
        "loss": 1.5891,
        "grad_norm": 3.8867506980895996,
        "learning_rate": 7.22066723788245e-05,
        "epoch": 1.062,
        "step": 7965
    },
    {
        "loss": 1.8854,
        "grad_norm": 3.1155216693878174,
        "learning_rate": 7.214622082349364e-05,
        "epoch": 1.0621333333333334,
        "step": 7966
    },
    {
        "loss": 1.6405,
        "grad_norm": 3.4994161128997803,
        "learning_rate": 7.208578030113948e-05,
        "epoch": 1.0622666666666667,
        "step": 7967
    },
    {
        "loss": 2.368,
        "grad_norm": 3.4488091468811035,
        "learning_rate": 7.202535083570287e-05,
        "epoch": 1.0624,
        "step": 7968
    },
    {
        "loss": 2.2052,
        "grad_norm": 2.9401657581329346,
        "learning_rate": 7.196493245112008e-05,
        "epoch": 1.0625333333333333,
        "step": 7969
    },
    {
        "loss": 1.515,
        "grad_norm": 4.826699256896973,
        "learning_rate": 7.190452517132317e-05,
        "epoch": 1.0626666666666666,
        "step": 7970
    },
    {
        "loss": 2.2782,
        "grad_norm": 4.001692295074463,
        "learning_rate": 7.184412902023959e-05,
        "epoch": 1.0628,
        "step": 7971
    },
    {
        "loss": 1.9126,
        "grad_norm": 4.520223140716553,
        "learning_rate": 7.178374402179227e-05,
        "epoch": 1.0629333333333333,
        "step": 7972
    },
    {
        "loss": 2.5776,
        "grad_norm": 3.210686206817627,
        "learning_rate": 7.172337019990023e-05,
        "epoch": 1.0630666666666666,
        "step": 7973
    },
    {
        "loss": 2.3986,
        "grad_norm": 4.024057388305664,
        "learning_rate": 7.166300757847749e-05,
        "epoch": 1.0632,
        "step": 7974
    },
    {
        "loss": 0.7595,
        "grad_norm": 3.4926106929779053,
        "learning_rate": 7.160265618143419e-05,
        "epoch": 1.0633333333333332,
        "step": 7975
    },
    {
        "loss": 1.9639,
        "grad_norm": 4.255049228668213,
        "learning_rate": 7.15423160326754e-05,
        "epoch": 1.0634666666666668,
        "step": 7976
    },
    {
        "loss": 2.0374,
        "grad_norm": 5.367473125457764,
        "learning_rate": 7.14819871561022e-05,
        "epoch": 1.0636,
        "step": 7977
    },
    {
        "loss": 1.5102,
        "grad_norm": 4.762202739715576,
        "learning_rate": 7.142166957561107e-05,
        "epoch": 1.0637333333333334,
        "step": 7978
    },
    {
        "loss": 1.8379,
        "grad_norm": 1.8644474744796753,
        "learning_rate": 7.136136331509407e-05,
        "epoch": 1.0638666666666667,
        "step": 7979
    },
    {
        "loss": 1.8299,
        "grad_norm": 3.8737356662750244,
        "learning_rate": 7.130106839843862e-05,
        "epoch": 1.064,
        "step": 7980
    },
    {
        "loss": 0.6764,
        "grad_norm": 2.8662681579589844,
        "learning_rate": 7.124078484952766e-05,
        "epoch": 1.0641333333333334,
        "step": 7981
    },
    {
        "loss": 2.3427,
        "grad_norm": 3.8862948417663574,
        "learning_rate": 7.118051269223993e-05,
        "epoch": 1.0642666666666667,
        "step": 7982
    },
    {
        "loss": 1.8785,
        "grad_norm": 3.6469779014587402,
        "learning_rate": 7.112025195044934e-05,
        "epoch": 1.0644,
        "step": 7983
    },
    {
        "loss": 2.5627,
        "grad_norm": 5.419376850128174,
        "learning_rate": 7.106000264802526e-05,
        "epoch": 1.0645333333333333,
        "step": 7984
    },
    {
        "loss": 1.5019,
        "grad_norm": 3.292768955230713,
        "learning_rate": 7.099976480883276e-05,
        "epoch": 1.0646666666666667,
        "step": 7985
    },
    {
        "loss": 1.8844,
        "grad_norm": 2.6359448432922363,
        "learning_rate": 7.093953845673225e-05,
        "epoch": 1.0648,
        "step": 7986
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.1940054893493652,
        "learning_rate": 7.087932361557971e-05,
        "epoch": 1.0649333333333333,
        "step": 7987
    },
    {
        "loss": 2.3676,
        "grad_norm": 2.8350391387939453,
        "learning_rate": 7.081912030922608e-05,
        "epoch": 1.0650666666666666,
        "step": 7988
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.648674249649048,
        "learning_rate": 7.075892856151849e-05,
        "epoch": 1.0652,
        "step": 7989
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.0719594955444336,
        "learning_rate": 7.069874839629878e-05,
        "epoch": 1.0653333333333332,
        "step": 7990
    },
    {
        "loss": 2.0607,
        "grad_norm": 2.529454231262207,
        "learning_rate": 7.06385798374049e-05,
        "epoch": 1.0654666666666666,
        "step": 7991
    },
    {
        "loss": 2.0073,
        "grad_norm": 3.448359966278076,
        "learning_rate": 7.057842290866944e-05,
        "epoch": 1.0656,
        "step": 7992
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.9172935485839844,
        "learning_rate": 7.051827763392093e-05,
        "epoch": 1.0657333333333334,
        "step": 7993
    },
    {
        "loss": 1.8882,
        "grad_norm": 3.1379456520080566,
        "learning_rate": 7.045814403698307e-05,
        "epoch": 1.0658666666666667,
        "step": 7994
    },
    {
        "loss": 2.9498,
        "grad_norm": 3.754887342453003,
        "learning_rate": 7.039802214167506e-05,
        "epoch": 1.066,
        "step": 7995
    },
    {
        "loss": 2.1381,
        "grad_norm": 2.622401714324951,
        "learning_rate": 7.033791197181146e-05,
        "epoch": 1.0661333333333334,
        "step": 7996
    },
    {
        "loss": 2.5121,
        "grad_norm": 3.5631020069122314,
        "learning_rate": 7.027781355120182e-05,
        "epoch": 1.0662666666666667,
        "step": 7997
    },
    {
        "loss": 1.3565,
        "grad_norm": 1.7553894519805908,
        "learning_rate": 7.02177269036516e-05,
        "epoch": 1.0664,
        "step": 7998
    },
    {
        "loss": 2.6337,
        "grad_norm": 2.8360934257507324,
        "learning_rate": 7.015765205296114e-05,
        "epoch": 1.0665333333333333,
        "step": 7999
    },
    {
        "loss": 1.267,
        "grad_norm": 4.362647533416748,
        "learning_rate": 7.009758902292657e-05,
        "epoch": 1.0666666666666667,
        "step": 8000
    },
    {
        "loss": 1.8496,
        "grad_norm": 3.1717169284820557,
        "learning_rate": 7.003753783733875e-05,
        "epoch": 1.0668,
        "step": 8001
    },
    {
        "loss": 2.4741,
        "grad_norm": 2.6010608673095703,
        "learning_rate": 6.997749851998427e-05,
        "epoch": 1.0669333333333333,
        "step": 8002
    },
    {
        "loss": 0.839,
        "grad_norm": 3.009147882461548,
        "learning_rate": 6.991747109464494e-05,
        "epoch": 1.0670666666666666,
        "step": 8003
    },
    {
        "loss": 1.8542,
        "grad_norm": 4.305183410644531,
        "learning_rate": 6.985745558509789e-05,
        "epoch": 1.0672,
        "step": 8004
    },
    {
        "loss": 2.4214,
        "grad_norm": 3.422079086303711,
        "learning_rate": 6.979745201511536e-05,
        "epoch": 1.0673333333333332,
        "step": 8005
    },
    {
        "loss": 1.529,
        "grad_norm": 2.779374599456787,
        "learning_rate": 6.97374604084649e-05,
        "epoch": 1.0674666666666666,
        "step": 8006
    },
    {
        "loss": 1.7492,
        "grad_norm": 2.513599395751953,
        "learning_rate": 6.967748078890961e-05,
        "epoch": 1.0676,
        "step": 8007
    },
    {
        "loss": 0.8479,
        "grad_norm": 3.2935495376586914,
        "learning_rate": 6.961751318020751e-05,
        "epoch": 1.0677333333333334,
        "step": 8008
    },
    {
        "loss": 1.8713,
        "grad_norm": 2.388305187225342,
        "learning_rate": 6.955755760611186e-05,
        "epoch": 1.0678666666666667,
        "step": 8009
    },
    {
        "loss": 1.7054,
        "grad_norm": 3.7763068675994873,
        "learning_rate": 6.949761409037142e-05,
        "epoch": 1.068,
        "step": 8010
    },
    {
        "loss": 1.4454,
        "grad_norm": 5.0862531661987305,
        "learning_rate": 6.943768265672997e-05,
        "epoch": 1.0681333333333334,
        "step": 8011
    },
    {
        "loss": 2.1274,
        "grad_norm": 3.7289986610412598,
        "learning_rate": 6.937776332892651e-05,
        "epoch": 1.0682666666666667,
        "step": 8012
    },
    {
        "loss": 2.3334,
        "grad_norm": 2.9187638759613037,
        "learning_rate": 6.931785613069546e-05,
        "epoch": 1.0684,
        "step": 8013
    },
    {
        "loss": 1.916,
        "grad_norm": 3.6449761390686035,
        "learning_rate": 6.925796108576612e-05,
        "epoch": 1.0685333333333333,
        "step": 8014
    },
    {
        "loss": 1.6403,
        "grad_norm": 3.680438756942749,
        "learning_rate": 6.9198078217863e-05,
        "epoch": 1.0686666666666667,
        "step": 8015
    },
    {
        "loss": 2.1174,
        "grad_norm": 3.7072243690490723,
        "learning_rate": 6.91382075507062e-05,
        "epoch": 1.0688,
        "step": 8016
    },
    {
        "loss": 1.9215,
        "grad_norm": 3.611359119415283,
        "learning_rate": 6.907834910801054e-05,
        "epoch": 1.0689333333333333,
        "step": 8017
    },
    {
        "loss": 1.9709,
        "grad_norm": 3.3705179691314697,
        "learning_rate": 6.901850291348612e-05,
        "epoch": 1.0690666666666666,
        "step": 8018
    },
    {
        "loss": 0.7955,
        "grad_norm": 6.654722690582275,
        "learning_rate": 6.895866899083823e-05,
        "epoch": 1.0692,
        "step": 8019
    },
    {
        "loss": 1.211,
        "grad_norm": 4.561402797698975,
        "learning_rate": 6.889884736376733e-05,
        "epoch": 1.0693333333333332,
        "step": 8020
    },
    {
        "loss": 1.3544,
        "grad_norm": 3.819857597351074,
        "learning_rate": 6.883903805596913e-05,
        "epoch": 1.0694666666666666,
        "step": 8021
    },
    {
        "loss": 2.1393,
        "grad_norm": 3.6370949745178223,
        "learning_rate": 6.87792410911339e-05,
        "epoch": 1.0695999999999999,
        "step": 8022
    },
    {
        "loss": 1.7159,
        "grad_norm": 3.901237964630127,
        "learning_rate": 6.871945649294778e-05,
        "epoch": 1.0697333333333334,
        "step": 8023
    },
    {
        "loss": 1.6041,
        "grad_norm": 4.274213790893555,
        "learning_rate": 6.865968428509142e-05,
        "epoch": 1.0698666666666667,
        "step": 8024
    },
    {
        "loss": 1.8388,
        "grad_norm": 2.369371175765991,
        "learning_rate": 6.859992449124111e-05,
        "epoch": 1.07,
        "step": 8025
    },
    {
        "loss": 2.3175,
        "grad_norm": 3.179231882095337,
        "learning_rate": 6.854017713506755e-05,
        "epoch": 1.0701333333333334,
        "step": 8026
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.6750144958496094,
        "learning_rate": 6.848044224023702e-05,
        "epoch": 1.0702666666666667,
        "step": 8027
    },
    {
        "loss": 1.4355,
        "grad_norm": 4.94551944732666,
        "learning_rate": 6.842071983041074e-05,
        "epoch": 1.0704,
        "step": 8028
    },
    {
        "loss": 2.2288,
        "grad_norm": 4.192991256713867,
        "learning_rate": 6.836100992924499e-05,
        "epoch": 1.0705333333333333,
        "step": 8029
    },
    {
        "loss": 1.5925,
        "grad_norm": 3.0100157260894775,
        "learning_rate": 6.830131256039103e-05,
        "epoch": 1.0706666666666667,
        "step": 8030
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.26887583732605,
        "learning_rate": 6.824162774749502e-05,
        "epoch": 1.0708,
        "step": 8031
    },
    {
        "loss": 1.7373,
        "grad_norm": 3.863126277923584,
        "learning_rate": 6.818195551419861e-05,
        "epoch": 1.0709333333333333,
        "step": 8032
    },
    {
        "loss": 1.6493,
        "grad_norm": 3.429008960723877,
        "learning_rate": 6.812229588413792e-05,
        "epoch": 1.0710666666666666,
        "step": 8033
    },
    {
        "loss": 1.4395,
        "grad_norm": 6.3935956954956055,
        "learning_rate": 6.806264888094464e-05,
        "epoch": 1.0712,
        "step": 8034
    },
    {
        "loss": 0.9678,
        "grad_norm": 2.6624364852905273,
        "learning_rate": 6.800301452824484e-05,
        "epoch": 1.0713333333333332,
        "step": 8035
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.819892168045044,
        "learning_rate": 6.794339284966e-05,
        "epoch": 1.0714666666666666,
        "step": 8036
    },
    {
        "loss": 2.1312,
        "grad_norm": 3.053248643875122,
        "learning_rate": 6.788378386880647e-05,
        "epoch": 1.0716,
        "step": 8037
    },
    {
        "loss": 2.7881,
        "grad_norm": 3.678586006164551,
        "learning_rate": 6.782418760929567e-05,
        "epoch": 1.0717333333333334,
        "step": 8038
    },
    {
        "loss": 2.1183,
        "grad_norm": 2.9521005153656006,
        "learning_rate": 6.77646040947338e-05,
        "epoch": 1.0718666666666667,
        "step": 8039
    },
    {
        "loss": 2.3787,
        "grad_norm": 2.8024370670318604,
        "learning_rate": 6.770503334872195e-05,
        "epoch": 1.072,
        "step": 8040
    },
    {
        "loss": 2.1735,
        "grad_norm": 4.621862411499023,
        "learning_rate": 6.764547539485654e-05,
        "epoch": 1.0721333333333334,
        "step": 8041
    },
    {
        "loss": 1.963,
        "grad_norm": 2.309554100036621,
        "learning_rate": 6.75859302567286e-05,
        "epoch": 1.0722666666666667,
        "step": 8042
    },
    {
        "loss": 2.7666,
        "grad_norm": 2.706712245941162,
        "learning_rate": 6.752639795792408e-05,
        "epoch": 1.0724,
        "step": 8043
    },
    {
        "loss": 2.4532,
        "grad_norm": 3.016120433807373,
        "learning_rate": 6.746687852202396e-05,
        "epoch": 1.0725333333333333,
        "step": 8044
    },
    {
        "loss": 2.0295,
        "grad_norm": 2.9850118160247803,
        "learning_rate": 6.74073719726041e-05,
        "epoch": 1.0726666666666667,
        "step": 8045
    },
    {
        "loss": 1.6155,
        "grad_norm": 3.418424367904663,
        "learning_rate": 6.73478783332354e-05,
        "epoch": 1.0728,
        "step": 8046
    },
    {
        "loss": 2.1937,
        "grad_norm": 3.1232540607452393,
        "learning_rate": 6.728839762748316e-05,
        "epoch": 1.0729333333333333,
        "step": 8047
    },
    {
        "loss": 1.4393,
        "grad_norm": 2.956691265106201,
        "learning_rate": 6.722892987890822e-05,
        "epoch": 1.0730666666666666,
        "step": 8048
    },
    {
        "loss": 1.4995,
        "grad_norm": 5.223776817321777,
        "learning_rate": 6.716947511106566e-05,
        "epoch": 1.0732,
        "step": 8049
    },
    {
        "loss": 1.9535,
        "grad_norm": 3.1358373165130615,
        "learning_rate": 6.71100333475061e-05,
        "epoch": 1.0733333333333333,
        "step": 8050
    },
    {
        "loss": 2.2843,
        "grad_norm": 3.2957513332366943,
        "learning_rate": 6.705060461177426e-05,
        "epoch": 1.0734666666666666,
        "step": 8051
    },
    {
        "loss": 2.6629,
        "grad_norm": 3.3125481605529785,
        "learning_rate": 6.699118892741018e-05,
        "epoch": 1.0735999999999999,
        "step": 8052
    },
    {
        "loss": 1.5594,
        "grad_norm": 3.3404769897460938,
        "learning_rate": 6.693178631794867e-05,
        "epoch": 1.0737333333333334,
        "step": 8053
    },
    {
        "loss": 2.1105,
        "grad_norm": 3.9460244178771973,
        "learning_rate": 6.687239680691929e-05,
        "epoch": 1.0738666666666667,
        "step": 8054
    },
    {
        "loss": 2.9389,
        "grad_norm": 3.9265451431274414,
        "learning_rate": 6.681302041784653e-05,
        "epoch": 1.074,
        "step": 8055
    },
    {
        "loss": 2.6312,
        "grad_norm": 4.3496856689453125,
        "learning_rate": 6.675365717424925e-05,
        "epoch": 1.0741333333333334,
        "step": 8056
    },
    {
        "loss": 1.8775,
        "grad_norm": 3.1619763374328613,
        "learning_rate": 6.66943070996418e-05,
        "epoch": 1.0742666666666667,
        "step": 8057
    },
    {
        "loss": 1.3854,
        "grad_norm": 3.9584648609161377,
        "learning_rate": 6.663497021753267e-05,
        "epoch": 1.0744,
        "step": 8058
    },
    {
        "loss": 2.0201,
        "grad_norm": 3.6692087650299072,
        "learning_rate": 6.657564655142568e-05,
        "epoch": 1.0745333333333333,
        "step": 8059
    },
    {
        "loss": 2.4881,
        "grad_norm": 2.6403753757476807,
        "learning_rate": 6.65163361248189e-05,
        "epoch": 1.0746666666666667,
        "step": 8060
    },
    {
        "loss": 1.651,
        "grad_norm": 3.1250264644622803,
        "learning_rate": 6.645703896120545e-05,
        "epoch": 1.0748,
        "step": 8061
    },
    {
        "loss": 1.1464,
        "grad_norm": 2.2617461681365967,
        "learning_rate": 6.639775508407316e-05,
        "epoch": 1.0749333333333333,
        "step": 8062
    },
    {
        "loss": 3.2305,
        "grad_norm": 4.68097448348999,
        "learning_rate": 6.633848451690465e-05,
        "epoch": 1.0750666666666666,
        "step": 8063
    },
    {
        "loss": 2.1885,
        "grad_norm": 4.727121829986572,
        "learning_rate": 6.627922728317712e-05,
        "epoch": 1.0752,
        "step": 8064
    },
    {
        "loss": 2.5768,
        "grad_norm": 3.6592514514923096,
        "learning_rate": 6.621998340636244e-05,
        "epoch": 1.0753333333333333,
        "step": 8065
    },
    {
        "loss": 1.5494,
        "grad_norm": 2.84804630279541,
        "learning_rate": 6.616075290992755e-05,
        "epoch": 1.0754666666666666,
        "step": 8066
    },
    {
        "loss": 2.5171,
        "grad_norm": 2.7088029384613037,
        "learning_rate": 6.610153581733375e-05,
        "epoch": 1.0756000000000001,
        "step": 8067
    },
    {
        "loss": 2.0399,
        "grad_norm": 4.229447364807129,
        "learning_rate": 6.604233215203705e-05,
        "epoch": 1.0757333333333334,
        "step": 8068
    },
    {
        "loss": 2.1738,
        "grad_norm": 3.9343597888946533,
        "learning_rate": 6.59831419374883e-05,
        "epoch": 1.0758666666666667,
        "step": 8069
    },
    {
        "loss": 2.5755,
        "grad_norm": 2.289296865463257,
        "learning_rate": 6.592396519713292e-05,
        "epoch": 1.076,
        "step": 8070
    },
    {
        "loss": 2.6649,
        "grad_norm": 3.1893081665039062,
        "learning_rate": 6.586480195441118e-05,
        "epoch": 1.0761333333333334,
        "step": 8071
    },
    {
        "loss": 0.7103,
        "grad_norm": 4.418773174285889,
        "learning_rate": 6.58056522327575e-05,
        "epoch": 1.0762666666666667,
        "step": 8072
    },
    {
        "loss": 1.7402,
        "grad_norm": 3.3873586654663086,
        "learning_rate": 6.574651605560163e-05,
        "epoch": 1.0764,
        "step": 8073
    },
    {
        "loss": 2.0675,
        "grad_norm": 2.4464383125305176,
        "learning_rate": 6.568739344636732e-05,
        "epoch": 1.0765333333333333,
        "step": 8074
    },
    {
        "loss": 2.3187,
        "grad_norm": 3.703578233718872,
        "learning_rate": 6.56282844284735e-05,
        "epoch": 1.0766666666666667,
        "step": 8075
    },
    {
        "loss": 1.4188,
        "grad_norm": 4.000669479370117,
        "learning_rate": 6.556918902533337e-05,
        "epoch": 1.0768,
        "step": 8076
    },
    {
        "loss": 2.5418,
        "grad_norm": 2.668642520904541,
        "learning_rate": 6.551010726035469e-05,
        "epoch": 1.0769333333333333,
        "step": 8077
    },
    {
        "loss": 2.0914,
        "grad_norm": 2.408388137817383,
        "learning_rate": 6.545103915694007e-05,
        "epoch": 1.0770666666666666,
        "step": 8078
    },
    {
        "loss": 1.4923,
        "grad_norm": 3.7451112270355225,
        "learning_rate": 6.539198473848655e-05,
        "epoch": 1.0772,
        "step": 8079
    },
    {
        "loss": 2.1378,
        "grad_norm": 4.04987096786499,
        "learning_rate": 6.533294402838593e-05,
        "epoch": 1.0773333333333333,
        "step": 8080
    },
    {
        "loss": 1.9543,
        "grad_norm": 3.1468300819396973,
        "learning_rate": 6.52739170500241e-05,
        "epoch": 1.0774666666666666,
        "step": 8081
    },
    {
        "loss": 2.4936,
        "grad_norm": 3.3222312927246094,
        "learning_rate": 6.52149038267822e-05,
        "epoch": 1.0776,
        "step": 8082
    },
    {
        "loss": 2.0264,
        "grad_norm": 3.296137571334839,
        "learning_rate": 6.515590438203529e-05,
        "epoch": 1.0777333333333334,
        "step": 8083
    },
    {
        "loss": 2.3517,
        "grad_norm": 3.284684658050537,
        "learning_rate": 6.509691873915358e-05,
        "epoch": 1.0778666666666668,
        "step": 8084
    },
    {
        "loss": 2.0533,
        "grad_norm": 4.408594608306885,
        "learning_rate": 6.503794692150114e-05,
        "epoch": 1.078,
        "step": 8085
    },
    {
        "loss": 2.178,
        "grad_norm": 3.6006507873535156,
        "learning_rate": 6.497898895243709e-05,
        "epoch": 1.0781333333333334,
        "step": 8086
    },
    {
        "loss": 1.6995,
        "grad_norm": 4.099773406982422,
        "learning_rate": 6.492004485531482e-05,
        "epoch": 1.0782666666666667,
        "step": 8087
    },
    {
        "loss": 1.1709,
        "grad_norm": 3.9378294944763184,
        "learning_rate": 6.486111465348236e-05,
        "epoch": 1.0784,
        "step": 8088
    },
    {
        "loss": 2.4173,
        "grad_norm": 4.892111301422119,
        "learning_rate": 6.480219837028214e-05,
        "epoch": 1.0785333333333333,
        "step": 8089
    },
    {
        "loss": 2.1724,
        "grad_norm": 2.8987784385681152,
        "learning_rate": 6.474329602905096e-05,
        "epoch": 1.0786666666666667,
        "step": 8090
    },
    {
        "loss": 1.9737,
        "grad_norm": 3.166335105895996,
        "learning_rate": 6.468440765312048e-05,
        "epoch": 1.0788,
        "step": 8091
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.506481885910034,
        "learning_rate": 6.462553326581632e-05,
        "epoch": 1.0789333333333333,
        "step": 8092
    },
    {
        "loss": 2.508,
        "grad_norm": 3.2945408821105957,
        "learning_rate": 6.456667289045918e-05,
        "epoch": 1.0790666666666666,
        "step": 8093
    },
    {
        "loss": 2.2113,
        "grad_norm": 3.1396830081939697,
        "learning_rate": 6.450782655036351e-05,
        "epoch": 1.0792,
        "step": 8094
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.519792318344116,
        "learning_rate": 6.444899426883866e-05,
        "epoch": 1.0793333333333333,
        "step": 8095
    },
    {
        "loss": 2.522,
        "grad_norm": 3.318847417831421,
        "learning_rate": 6.439017606918836e-05,
        "epoch": 1.0794666666666666,
        "step": 8096
    },
    {
        "loss": 1.5995,
        "grad_norm": 3.9538042545318604,
        "learning_rate": 6.43313719747107e-05,
        "epoch": 1.0796000000000001,
        "step": 8097
    },
    {
        "loss": 1.883,
        "grad_norm": 3.5791282653808594,
        "learning_rate": 6.427258200869814e-05,
        "epoch": 1.0797333333333334,
        "step": 8098
    },
    {
        "loss": 1.5941,
        "grad_norm": 3.5104336738586426,
        "learning_rate": 6.421380619443744e-05,
        "epoch": 1.0798666666666668,
        "step": 8099
    },
    {
        "loss": 1.7,
        "grad_norm": 3.7501091957092285,
        "learning_rate": 6.415504455521019e-05,
        "epoch": 1.08,
        "step": 8100
    },
    {
        "loss": 1.7414,
        "grad_norm": 3.3447349071502686,
        "learning_rate": 6.409629711429193e-05,
        "epoch": 1.0801333333333334,
        "step": 8101
    },
    {
        "loss": 1.2149,
        "grad_norm": 5.359584808349609,
        "learning_rate": 6.403756389495266e-05,
        "epoch": 1.0802666666666667,
        "step": 8102
    },
    {
        "loss": 2.0502,
        "grad_norm": 2.4766135215759277,
        "learning_rate": 6.397884492045684e-05,
        "epoch": 1.0804,
        "step": 8103
    },
    {
        "loss": 2.029,
        "grad_norm": 3.599626302719116,
        "learning_rate": 6.392014021406331e-05,
        "epoch": 1.0805333333333333,
        "step": 8104
    },
    {
        "loss": 2.4592,
        "grad_norm": 2.454576015472412,
        "learning_rate": 6.386144979902531e-05,
        "epoch": 1.0806666666666667,
        "step": 8105
    },
    {
        "loss": 2.1102,
        "grad_norm": 3.2179102897644043,
        "learning_rate": 6.380277369858997e-05,
        "epoch": 1.0808,
        "step": 8106
    },
    {
        "loss": 2.014,
        "grad_norm": 2.2851133346557617,
        "learning_rate": 6.374411193599947e-05,
        "epoch": 1.0809333333333333,
        "step": 8107
    },
    {
        "loss": 2.4536,
        "grad_norm": 5.004411697387695,
        "learning_rate": 6.368546453448965e-05,
        "epoch": 1.0810666666666666,
        "step": 8108
    },
    {
        "loss": 1.606,
        "grad_norm": 3.386915922164917,
        "learning_rate": 6.362683151729122e-05,
        "epoch": 1.0812,
        "step": 8109
    },
    {
        "loss": 2.073,
        "grad_norm": 4.375330924987793,
        "learning_rate": 6.356821290762865e-05,
        "epoch": 1.0813333333333333,
        "step": 8110
    },
    {
        "loss": 1.3701,
        "grad_norm": 3.654320240020752,
        "learning_rate": 6.35096087287211e-05,
        "epoch": 1.0814666666666666,
        "step": 8111
    },
    {
        "loss": 2.0735,
        "grad_norm": 3.465052366256714,
        "learning_rate": 6.345101900378185e-05,
        "epoch": 1.0816,
        "step": 8112
    },
    {
        "loss": 1.6784,
        "grad_norm": 3.2826414108276367,
        "learning_rate": 6.339244375601852e-05,
        "epoch": 1.0817333333333334,
        "step": 8113
    },
    {
        "loss": 2.007,
        "grad_norm": 4.324939727783203,
        "learning_rate": 6.333388300863308e-05,
        "epoch": 1.0818666666666668,
        "step": 8114
    },
    {
        "loss": 1.2525,
        "grad_norm": 2.537343740463257,
        "learning_rate": 6.327533678482134e-05,
        "epoch": 1.082,
        "step": 8115
    },
    {
        "loss": 1.5987,
        "grad_norm": 4.204493999481201,
        "learning_rate": 6.321680510777395e-05,
        "epoch": 1.0821333333333334,
        "step": 8116
    },
    {
        "loss": 2.438,
        "grad_norm": 2.74251651763916,
        "learning_rate": 6.315828800067525e-05,
        "epoch": 1.0822666666666667,
        "step": 8117
    },
    {
        "loss": 2.3694,
        "grad_norm": 6.089107513427734,
        "learning_rate": 6.309978548670442e-05,
        "epoch": 1.0824,
        "step": 8118
    },
    {
        "loss": 1.5886,
        "grad_norm": 2.4382786750793457,
        "learning_rate": 6.304129758903417e-05,
        "epoch": 1.0825333333333333,
        "step": 8119
    },
    {
        "loss": 3.2807,
        "grad_norm": 3.955811023712158,
        "learning_rate": 6.298282433083184e-05,
        "epoch": 1.0826666666666667,
        "step": 8120
    },
    {
        "loss": 2.3986,
        "grad_norm": 3.7173566818237305,
        "learning_rate": 6.292436573525897e-05,
        "epoch": 1.0828,
        "step": 8121
    },
    {
        "loss": 0.9833,
        "grad_norm": 4.953927040100098,
        "learning_rate": 6.286592182547122e-05,
        "epoch": 1.0829333333333333,
        "step": 8122
    },
    {
        "loss": 1.2824,
        "grad_norm": 4.6756134033203125,
        "learning_rate": 6.280749262461837e-05,
        "epoch": 1.0830666666666666,
        "step": 8123
    },
    {
        "loss": 1.9782,
        "grad_norm": 4.184546947479248,
        "learning_rate": 6.274907815584427e-05,
        "epoch": 1.0832,
        "step": 8124
    },
    {
        "loss": 2.1116,
        "grad_norm": 3.8268349170684814,
        "learning_rate": 6.269067844228739e-05,
        "epoch": 1.0833333333333333,
        "step": 8125
    },
    {
        "loss": 2.3223,
        "grad_norm": 2.6461739540100098,
        "learning_rate": 6.26322935070799e-05,
        "epoch": 1.0834666666666666,
        "step": 8126
    },
    {
        "loss": 0.7058,
        "grad_norm": 3.7659029960632324,
        "learning_rate": 6.257392337334821e-05,
        "epoch": 1.0836,
        "step": 8127
    },
    {
        "loss": 2.4973,
        "grad_norm": 3.417506456375122,
        "learning_rate": 6.2515568064213e-05,
        "epoch": 1.0837333333333334,
        "step": 8128
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.990028738975525,
        "learning_rate": 6.245722760278902e-05,
        "epoch": 1.0838666666666668,
        "step": 8129
    },
    {
        "loss": 1.3124,
        "grad_norm": 4.532041549682617,
        "learning_rate": 6.239890201218524e-05,
        "epoch": 1.084,
        "step": 8130
    },
    {
        "loss": 1.6921,
        "grad_norm": 3.277536630630493,
        "learning_rate": 6.234059131550427e-05,
        "epoch": 1.0841333333333334,
        "step": 8131
    },
    {
        "loss": 1.4942,
        "grad_norm": 1.9067562818527222,
        "learning_rate": 6.228229553584354e-05,
        "epoch": 1.0842666666666667,
        "step": 8132
    },
    {
        "loss": 1.2514,
        "grad_norm": 2.8141403198242188,
        "learning_rate": 6.222401469629398e-05,
        "epoch": 1.0844,
        "step": 8133
    },
    {
        "loss": 1.8242,
        "grad_norm": 3.4146995544433594,
        "learning_rate": 6.2165748819941e-05,
        "epoch": 1.0845333333333333,
        "step": 8134
    },
    {
        "loss": 2.2609,
        "grad_norm": 3.552281141281128,
        "learning_rate": 6.210749792986384e-05,
        "epoch": 1.0846666666666667,
        "step": 8135
    },
    {
        "loss": 3.9951,
        "grad_norm": 3.1660964488983154,
        "learning_rate": 6.204926204913581e-05,
        "epoch": 1.0848,
        "step": 8136
    },
    {
        "loss": 1.6105,
        "grad_norm": 4.174849987030029,
        "learning_rate": 6.199104120082439e-05,
        "epoch": 1.0849333333333333,
        "step": 8137
    },
    {
        "loss": 2.0047,
        "grad_norm": 3.7022743225097656,
        "learning_rate": 6.193283540799106e-05,
        "epoch": 1.0850666666666666,
        "step": 8138
    },
    {
        "loss": 2.5956,
        "grad_norm": 2.824925661087036,
        "learning_rate": 6.187464469369147e-05,
        "epoch": 1.0852,
        "step": 8139
    },
    {
        "loss": 1.0667,
        "grad_norm": 4.002964019775391,
        "learning_rate": 6.181646908097485e-05,
        "epoch": 1.0853333333333333,
        "step": 8140
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.3047637939453125,
        "learning_rate": 6.175830859288504e-05,
        "epoch": 1.0854666666666666,
        "step": 8141
    },
    {
        "loss": 1.7224,
        "grad_norm": 2.6813833713531494,
        "learning_rate": 6.170016325245937e-05,
        "epoch": 1.0856,
        "step": 8142
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.148742198944092,
        "learning_rate": 6.164203308272973e-05,
        "epoch": 1.0857333333333332,
        "step": 8143
    },
    {
        "loss": 2.6938,
        "grad_norm": 4.010758399963379,
        "learning_rate": 6.158391810672138e-05,
        "epoch": 1.0858666666666668,
        "step": 8144
    },
    {
        "loss": 1.9664,
        "grad_norm": 4.166799068450928,
        "learning_rate": 6.152581834745396e-05,
        "epoch": 1.086,
        "step": 8145
    },
    {
        "loss": 2.1603,
        "grad_norm": 2.7982468605041504,
        "learning_rate": 6.146773382794097e-05,
        "epoch": 1.0861333333333334,
        "step": 8146
    },
    {
        "loss": 2.8299,
        "grad_norm": 3.1213691234588623,
        "learning_rate": 6.140966457119e-05,
        "epoch": 1.0862666666666667,
        "step": 8147
    },
    {
        "loss": 2.3139,
        "grad_norm": 4.042137622833252,
        "learning_rate": 6.135161060020237e-05,
        "epoch": 1.0864,
        "step": 8148
    },
    {
        "loss": 2.2024,
        "grad_norm": 3.531553268432617,
        "learning_rate": 6.129357193797335e-05,
        "epoch": 1.0865333333333334,
        "step": 8149
    },
    {
        "loss": 2.3665,
        "grad_norm": 4.550263404846191,
        "learning_rate": 6.12355486074925e-05,
        "epoch": 1.0866666666666667,
        "step": 8150
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.8823471069335938,
        "learning_rate": 6.117754063174295e-05,
        "epoch": 1.0868,
        "step": 8151
    },
    {
        "loss": 2.063,
        "grad_norm": 2.4963977336883545,
        "learning_rate": 6.111954803370176e-05,
        "epoch": 1.0869333333333333,
        "step": 8152
    },
    {
        "loss": 1.6277,
        "grad_norm": 3.476595640182495,
        "learning_rate": 6.106157083634006e-05,
        "epoch": 1.0870666666666666,
        "step": 8153
    },
    {
        "loss": 1.744,
        "grad_norm": 3.4489986896514893,
        "learning_rate": 6.100360906262286e-05,
        "epoch": 1.0872,
        "step": 8154
    },
    {
        "loss": 2.0426,
        "grad_norm": 4.196605682373047,
        "learning_rate": 6.094566273550897e-05,
        "epoch": 1.0873333333333333,
        "step": 8155
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.777635097503662,
        "learning_rate": 6.088773187795122e-05,
        "epoch": 1.0874666666666666,
        "step": 8156
    },
    {
        "loss": 2.1477,
        "grad_norm": 3.3735146522521973,
        "learning_rate": 6.082981651289612e-05,
        "epoch": 1.0876,
        "step": 8157
    },
    {
        "loss": 3.0396,
        "grad_norm": 2.89292311668396,
        "learning_rate": 6.077191666328407e-05,
        "epoch": 1.0877333333333334,
        "step": 8158
    },
    {
        "loss": 1.9737,
        "grad_norm": 3.477187156677246,
        "learning_rate": 6.071403235204962e-05,
        "epoch": 1.0878666666666668,
        "step": 8159
    },
    {
        "loss": 1.9956,
        "grad_norm": 3.117947578430176,
        "learning_rate": 6.0656163602120786e-05,
        "epoch": 1.088,
        "step": 8160
    },
    {
        "loss": 1.4074,
        "grad_norm": 2.6554973125457764,
        "learning_rate": 6.0598310436419584e-05,
        "epoch": 1.0881333333333334,
        "step": 8161
    },
    {
        "loss": 2.0418,
        "grad_norm": 3.167522668838501,
        "learning_rate": 6.054047287786187e-05,
        "epoch": 1.0882666666666667,
        "step": 8162
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.545497417449951,
        "learning_rate": 6.048265094935728e-05,
        "epoch": 1.0884,
        "step": 8163
    },
    {
        "loss": 2.5257,
        "grad_norm": 2.3419189453125,
        "learning_rate": 6.042484467380944e-05,
        "epoch": 1.0885333333333334,
        "step": 8164
    },
    {
        "loss": 1.8876,
        "grad_norm": 3.234866142272949,
        "learning_rate": 6.03670540741153e-05,
        "epoch": 1.0886666666666667,
        "step": 8165
    },
    {
        "loss": 2.3695,
        "grad_norm": 3.972275972366333,
        "learning_rate": 6.0309279173166214e-05,
        "epoch": 1.0888,
        "step": 8166
    },
    {
        "loss": 1.7761,
        "grad_norm": 3.136385440826416,
        "learning_rate": 6.025151999384674e-05,
        "epoch": 1.0889333333333333,
        "step": 8167
    },
    {
        "loss": 1.9719,
        "grad_norm": 4.127507209777832,
        "learning_rate": 6.0193776559035844e-05,
        "epoch": 1.0890666666666666,
        "step": 8168
    },
    {
        "loss": 1.7483,
        "grad_norm": 2.5210158824920654,
        "learning_rate": 6.0136048891605555e-05,
        "epoch": 1.0892,
        "step": 8169
    },
    {
        "loss": 2.3948,
        "grad_norm": 3.413756847381592,
        "learning_rate": 6.007833701442215e-05,
        "epoch": 1.0893333333333333,
        "step": 8170
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.625467300415039,
        "learning_rate": 6.002064095034545e-05,
        "epoch": 1.0894666666666666,
        "step": 8171
    },
    {
        "loss": 1.0168,
        "grad_norm": 4.2495903968811035,
        "learning_rate": 5.996296072222918e-05,
        "epoch": 1.0896,
        "step": 8172
    },
    {
        "loss": 1.9823,
        "grad_norm": 3.957942247390747,
        "learning_rate": 5.9905296352920594e-05,
        "epoch": 1.0897333333333332,
        "step": 8173
    },
    {
        "loss": 2.0214,
        "grad_norm": 4.270811557769775,
        "learning_rate": 5.98476478652606e-05,
        "epoch": 1.0898666666666668,
        "step": 8174
    },
    {
        "loss": 2.1315,
        "grad_norm": 3.35807728767395,
        "learning_rate": 5.979001528208424e-05,
        "epoch": 1.09,
        "step": 8175
    },
    {
        "loss": 1.9845,
        "grad_norm": 3.2382352352142334,
        "learning_rate": 5.973239862621973e-05,
        "epoch": 1.0901333333333334,
        "step": 8176
    },
    {
        "loss": 1.535,
        "grad_norm": 3.555521249771118,
        "learning_rate": 5.96747979204895e-05,
        "epoch": 1.0902666666666667,
        "step": 8177
    },
    {
        "loss": 1.9851,
        "grad_norm": 3.2693655490875244,
        "learning_rate": 5.9617213187709086e-05,
        "epoch": 1.0904,
        "step": 8178
    },
    {
        "loss": 1.59,
        "grad_norm": 3.193290948867798,
        "learning_rate": 5.9559644450688144e-05,
        "epoch": 1.0905333333333334,
        "step": 8179
    },
    {
        "loss": 2.2419,
        "grad_norm": 2.410242795944214,
        "learning_rate": 5.950209173222984e-05,
        "epoch": 1.0906666666666667,
        "step": 8180
    },
    {
        "loss": 1.4197,
        "grad_norm": 4.1317925453186035,
        "learning_rate": 5.9444555055131044e-05,
        "epoch": 1.0908,
        "step": 8181
    },
    {
        "loss": 2.6299,
        "grad_norm": 4.031029224395752,
        "learning_rate": 5.9387034442182165e-05,
        "epoch": 1.0909333333333333,
        "step": 8182
    },
    {
        "loss": 2.2459,
        "grad_norm": 3.59767484664917,
        "learning_rate": 5.932952991616723e-05,
        "epoch": 1.0910666666666666,
        "step": 8183
    },
    {
        "loss": 2.1478,
        "grad_norm": 2.975715160369873,
        "learning_rate": 5.9272041499864185e-05,
        "epoch": 1.0912,
        "step": 8184
    },
    {
        "loss": 2.5901,
        "grad_norm": 3.583768844604492,
        "learning_rate": 5.9214569216044294e-05,
        "epoch": 1.0913333333333333,
        "step": 8185
    },
    {
        "loss": 1.8401,
        "grad_norm": 3.0228164196014404,
        "learning_rate": 5.915711308747243e-05,
        "epoch": 1.0914666666666666,
        "step": 8186
    },
    {
        "loss": 2.9301,
        "grad_norm": 2.738940954208374,
        "learning_rate": 5.9099673136907255e-05,
        "epoch": 1.0916,
        "step": 8187
    },
    {
        "loss": 2.1155,
        "grad_norm": 3.2567431926727295,
        "learning_rate": 5.9042249387100904e-05,
        "epoch": 1.0917333333333334,
        "step": 8188
    },
    {
        "loss": 2.7754,
        "grad_norm": 2.8763182163238525,
        "learning_rate": 5.898484186079927e-05,
        "epoch": 1.0918666666666668,
        "step": 8189
    },
    {
        "loss": 2.5385,
        "grad_norm": 4.243220806121826,
        "learning_rate": 5.892745058074132e-05,
        "epoch": 1.092,
        "step": 8190
    },
    {
        "loss": 2.3985,
        "grad_norm": 2.660682201385498,
        "learning_rate": 5.887007556966029e-05,
        "epoch": 1.0921333333333334,
        "step": 8191
    },
    {
        "loss": 2.0822,
        "grad_norm": 3.2725791931152344,
        "learning_rate": 5.881271685028233e-05,
        "epoch": 1.0922666666666667,
        "step": 8192
    },
    {
        "loss": 1.5387,
        "grad_norm": 3.7905373573303223,
        "learning_rate": 5.8755374445327706e-05,
        "epoch": 1.0924,
        "step": 8193
    },
    {
        "loss": 2.6151,
        "grad_norm": 3.9274473190307617,
        "learning_rate": 5.869804837750981e-05,
        "epoch": 1.0925333333333334,
        "step": 8194
    },
    {
        "loss": 1.4699,
        "grad_norm": 5.827526092529297,
        "learning_rate": 5.8640738669535614e-05,
        "epoch": 1.0926666666666667,
        "step": 8195
    },
    {
        "loss": 2.1742,
        "grad_norm": 3.034492015838623,
        "learning_rate": 5.858344534410575e-05,
        "epoch": 1.0928,
        "step": 8196
    },
    {
        "loss": 2.2487,
        "grad_norm": 1.931023359298706,
        "learning_rate": 5.8526168423914316e-05,
        "epoch": 1.0929333333333333,
        "step": 8197
    },
    {
        "loss": 0.7362,
        "grad_norm": 3.191377878189087,
        "learning_rate": 5.846890793164903e-05,
        "epoch": 1.0930666666666666,
        "step": 8198
    },
    {
        "loss": 1.4766,
        "grad_norm": 3.440020799636841,
        "learning_rate": 5.841166388999062e-05,
        "epoch": 1.0932,
        "step": 8199
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.5178232192993164,
        "learning_rate": 5.8354436321613995e-05,
        "epoch": 1.0933333333333333,
        "step": 8200
    },
    {
        "loss": 2.2536,
        "grad_norm": 3.7388949394226074,
        "learning_rate": 5.8297225249186884e-05,
        "epoch": 1.0934666666666666,
        "step": 8201
    },
    {
        "loss": 1.6123,
        "grad_norm": 3.501612663269043,
        "learning_rate": 5.824003069537116e-05,
        "epoch": 1.0936,
        "step": 8202
    },
    {
        "loss": 2.161,
        "grad_norm": 4.123081684112549,
        "learning_rate": 5.8182852682821465e-05,
        "epoch": 1.0937333333333332,
        "step": 8203
    },
    {
        "loss": 1.3818,
        "grad_norm": 4.344903945922852,
        "learning_rate": 5.8125691234186294e-05,
        "epoch": 1.0938666666666668,
        "step": 8204
    },
    {
        "loss": 2.3343,
        "grad_norm": 2.336160182952881,
        "learning_rate": 5.8068546372107476e-05,
        "epoch": 1.094,
        "step": 8205
    },
    {
        "loss": 1.9295,
        "grad_norm": 3.4550814628601074,
        "learning_rate": 5.801141811922044e-05,
        "epoch": 1.0941333333333334,
        "step": 8206
    },
    {
        "loss": 0.6917,
        "grad_norm": 2.9789938926696777,
        "learning_rate": 5.795430649815371e-05,
        "epoch": 1.0942666666666667,
        "step": 8207
    },
    {
        "loss": 2.436,
        "grad_norm": 4.319288730621338,
        "learning_rate": 5.789721153152936e-05,
        "epoch": 1.0944,
        "step": 8208
    },
    {
        "loss": 2.0973,
        "grad_norm": 3.0696568489074707,
        "learning_rate": 5.784013324196308e-05,
        "epoch": 1.0945333333333334,
        "step": 8209
    },
    {
        "loss": 1.8973,
        "grad_norm": 2.6601052284240723,
        "learning_rate": 5.778307165206367e-05,
        "epoch": 1.0946666666666667,
        "step": 8210
    },
    {
        "loss": 2.0784,
        "grad_norm": 3.5480988025665283,
        "learning_rate": 5.77260267844334e-05,
        "epoch": 1.0948,
        "step": 8211
    },
    {
        "loss": 2.4676,
        "grad_norm": 3.577848434448242,
        "learning_rate": 5.766899866166795e-05,
        "epoch": 1.0949333333333333,
        "step": 8212
    },
    {
        "loss": 3.0303,
        "grad_norm": 5.1630659103393555,
        "learning_rate": 5.7611987306356375e-05,
        "epoch": 1.0950666666666666,
        "step": 8213
    },
    {
        "loss": 2.0706,
        "grad_norm": 2.835366725921631,
        "learning_rate": 5.755499274108107e-05,
        "epoch": 1.0952,
        "step": 8214
    },
    {
        "loss": 2.4903,
        "grad_norm": 2.9103615283966064,
        "learning_rate": 5.749801498841779e-05,
        "epoch": 1.0953333333333333,
        "step": 8215
    },
    {
        "loss": 2.5152,
        "grad_norm": 3.2920947074890137,
        "learning_rate": 5.744105407093561e-05,
        "epoch": 1.0954666666666666,
        "step": 8216
    },
    {
        "loss": 2.1697,
        "grad_norm": 6.3568501472473145,
        "learning_rate": 5.7384110011196846e-05,
        "epoch": 1.0956,
        "step": 8217
    },
    {
        "loss": 1.9565,
        "grad_norm": 1.9711204767227173,
        "learning_rate": 5.73271828317574e-05,
        "epoch": 1.0957333333333334,
        "step": 8218
    },
    {
        "loss": 1.0351,
        "grad_norm": 4.078566074371338,
        "learning_rate": 5.727027255516626e-05,
        "epoch": 1.0958666666666668,
        "step": 8219
    },
    {
        "loss": 0.8498,
        "grad_norm": 3.1684770584106445,
        "learning_rate": 5.7213379203965655e-05,
        "epoch": 1.096,
        "step": 8220
    },
    {
        "loss": 1.4292,
        "grad_norm": 4.044189929962158,
        "learning_rate": 5.715650280069138e-05,
        "epoch": 1.0961333333333334,
        "step": 8221
    },
    {
        "loss": 1.6991,
        "grad_norm": 7.075840473175049,
        "learning_rate": 5.709964336787233e-05,
        "epoch": 1.0962666666666667,
        "step": 8222
    },
    {
        "loss": 1.7127,
        "grad_norm": 4.200577735900879,
        "learning_rate": 5.7042800928030834e-05,
        "epoch": 1.0964,
        "step": 8223
    },
    {
        "loss": 2.5392,
        "grad_norm": 1.4134267568588257,
        "learning_rate": 5.6985975503682075e-05,
        "epoch": 1.0965333333333334,
        "step": 8224
    },
    {
        "loss": 2.8967,
        "grad_norm": 3.128453016281128,
        "learning_rate": 5.692916711733514e-05,
        "epoch": 1.0966666666666667,
        "step": 8225
    },
    {
        "loss": 2.3465,
        "grad_norm": 4.62677001953125,
        "learning_rate": 5.687237579149171e-05,
        "epoch": 1.0968,
        "step": 8226
    },
    {
        "loss": 2.636,
        "grad_norm": 3.201765775680542,
        "learning_rate": 5.681560154864738e-05,
        "epoch": 1.0969333333333333,
        "step": 8227
    },
    {
        "loss": 2.4177,
        "grad_norm": 2.4482250213623047,
        "learning_rate": 5.67588444112903e-05,
        "epoch": 1.0970666666666666,
        "step": 8228
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.0457489490509033,
        "learning_rate": 5.670210440190229e-05,
        "epoch": 1.0972,
        "step": 8229
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.9676742553710938,
        "learning_rate": 5.664538154295823e-05,
        "epoch": 1.0973333333333333,
        "step": 8230
    },
    {
        "loss": 0.8672,
        "grad_norm": 2.347179412841797,
        "learning_rate": 5.658867585692639e-05,
        "epoch": 1.0974666666666666,
        "step": 8231
    },
    {
        "loss": 2.8545,
        "grad_norm": 3.9270665645599365,
        "learning_rate": 5.6531987366267913e-05,
        "epoch": 1.0976,
        "step": 8232
    },
    {
        "loss": 2.3855,
        "grad_norm": 3.107048511505127,
        "learning_rate": 5.6475316093437283e-05,
        "epoch": 1.0977333333333332,
        "step": 8233
    },
    {
        "loss": 2.6682,
        "grad_norm": 2.8964548110961914,
        "learning_rate": 5.641866206088232e-05,
        "epoch": 1.0978666666666668,
        "step": 8234
    },
    {
        "loss": 2.413,
        "grad_norm": 3.9032723903656006,
        "learning_rate": 5.636202529104371e-05,
        "epoch": 1.098,
        "step": 8235
    },
    {
        "loss": 2.9385,
        "grad_norm": 4.998908519744873,
        "learning_rate": 5.6305405806355774e-05,
        "epoch": 1.0981333333333334,
        "step": 8236
    },
    {
        "loss": 1.5594,
        "grad_norm": 3.225071907043457,
        "learning_rate": 5.624880362924536e-05,
        "epoch": 1.0982666666666667,
        "step": 8237
    },
    {
        "loss": 2.3792,
        "grad_norm": 3.026651620864868,
        "learning_rate": 5.61922187821329e-05,
        "epoch": 1.0984,
        "step": 8238
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.3989791870117188,
        "learning_rate": 5.613565128743185e-05,
        "epoch": 1.0985333333333334,
        "step": 8239
    },
    {
        "loss": 1.3136,
        "grad_norm": 3.760784149169922,
        "learning_rate": 5.607910116754884e-05,
        "epoch": 1.0986666666666667,
        "step": 8240
    },
    {
        "loss": 2.6196,
        "grad_norm": 1.963854193687439,
        "learning_rate": 5.602256844488356e-05,
        "epoch": 1.0988,
        "step": 8241
    },
    {
        "loss": 0.7586,
        "grad_norm": 2.4909584522247314,
        "learning_rate": 5.596605314182859e-05,
        "epoch": 1.0989333333333333,
        "step": 8242
    },
    {
        "loss": 2.3604,
        "grad_norm": 2.3543999195098877,
        "learning_rate": 5.590955528077015e-05,
        "epoch": 1.0990666666666666,
        "step": 8243
    },
    {
        "loss": 1.7112,
        "grad_norm": 4.017882823944092,
        "learning_rate": 5.585307488408713e-05,
        "epoch": 1.0992,
        "step": 8244
    },
    {
        "loss": 1.0389,
        "grad_norm": 5.079553127288818,
        "learning_rate": 5.579661197415145e-05,
        "epoch": 1.0993333333333333,
        "step": 8245
    },
    {
        "loss": 1.6942,
        "grad_norm": 2.5023252964019775,
        "learning_rate": 5.574016657332843e-05,
        "epoch": 1.0994666666666666,
        "step": 8246
    },
    {
        "loss": 2.2664,
        "grad_norm": 2.1489152908325195,
        "learning_rate": 5.568373870397625e-05,
        "epoch": 1.0996,
        "step": 8247
    },
    {
        "loss": 1.5666,
        "grad_norm": 3.845906972885132,
        "learning_rate": 5.5627328388446286e-05,
        "epoch": 1.0997333333333332,
        "step": 8248
    },
    {
        "loss": 1.8326,
        "grad_norm": 2.2745349407196045,
        "learning_rate": 5.557093564908254e-05,
        "epoch": 1.0998666666666668,
        "step": 8249
    },
    {
        "loss": 1.9502,
        "grad_norm": 4.253905296325684,
        "learning_rate": 5.5514560508222746e-05,
        "epoch": 1.1,
        "step": 8250
    },
    {
        "loss": 2.0514,
        "grad_norm": 3.3037147521972656,
        "learning_rate": 5.545820298819695e-05,
        "epoch": 1.1001333333333334,
        "step": 8251
    },
    {
        "loss": 2.3372,
        "grad_norm": 2.4204955101013184,
        "learning_rate": 5.540186311132891e-05,
        "epoch": 1.1002666666666667,
        "step": 8252
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.3803956508636475,
        "learning_rate": 5.5345540899934746e-05,
        "epoch": 1.1004,
        "step": 8253
    },
    {
        "loss": 1.9661,
        "grad_norm": 2.7871992588043213,
        "learning_rate": 5.528923637632398e-05,
        "epoch": 1.1005333333333334,
        "step": 8254
    },
    {
        "loss": 1.5734,
        "grad_norm": 3.5226478576660156,
        "learning_rate": 5.523294956279905e-05,
        "epoch": 1.1006666666666667,
        "step": 8255
    },
    {
        "loss": 2.1275,
        "grad_norm": 3.893836259841919,
        "learning_rate": 5.5176680481655285e-05,
        "epoch": 1.1008,
        "step": 8256
    },
    {
        "loss": 1.6944,
        "grad_norm": 3.760072708129883,
        "learning_rate": 5.5120429155181275e-05,
        "epoch": 1.1009333333333333,
        "step": 8257
    },
    {
        "loss": 1.9815,
        "grad_norm": 5.6850972175598145,
        "learning_rate": 5.5064195605658034e-05,
        "epoch": 1.1010666666666666,
        "step": 8258
    },
    {
        "loss": 2.4453,
        "grad_norm": 4.735645294189453,
        "learning_rate": 5.500797985536012e-05,
        "epoch": 1.1012,
        "step": 8259
    },
    {
        "loss": 2.1441,
        "grad_norm": 3.7758212089538574,
        "learning_rate": 5.495178192655457e-05,
        "epoch": 1.1013333333333333,
        "step": 8260
    },
    {
        "loss": 2.3615,
        "grad_norm": 3.9282078742980957,
        "learning_rate": 5.489560184150189e-05,
        "epoch": 1.1014666666666666,
        "step": 8261
    },
    {
        "loss": 1.3593,
        "grad_norm": 4.181535720825195,
        "learning_rate": 5.48394396224549e-05,
        "epoch": 1.1016,
        "step": 8262
    },
    {
        "loss": 2.1455,
        "grad_norm": 3.564033269882202,
        "learning_rate": 5.478329529165975e-05,
        "epoch": 1.1017333333333332,
        "step": 8263
    },
    {
        "loss": 1.6589,
        "grad_norm": 3.9953131675720215,
        "learning_rate": 5.47271688713554e-05,
        "epoch": 1.1018666666666665,
        "step": 8264
    },
    {
        "loss": 2.1909,
        "grad_norm": 3.4590156078338623,
        "learning_rate": 5.467106038377378e-05,
        "epoch": 1.102,
        "step": 8265
    },
    {
        "loss": 1.6009,
        "grad_norm": 3.415461778640747,
        "learning_rate": 5.461496985113963e-05,
        "epoch": 1.1021333333333334,
        "step": 8266
    },
    {
        "loss": 1.4771,
        "grad_norm": 4.940643310546875,
        "learning_rate": 5.4558897295670405e-05,
        "epoch": 1.1022666666666667,
        "step": 8267
    },
    {
        "loss": 1.9141,
        "grad_norm": 3.258382797241211,
        "learning_rate": 5.450284273957691e-05,
        "epoch": 1.1024,
        "step": 8268
    },
    {
        "loss": 2.0517,
        "grad_norm": 4.506231307983398,
        "learning_rate": 5.444680620506249e-05,
        "epoch": 1.1025333333333334,
        "step": 8269
    },
    {
        "loss": 2.0915,
        "grad_norm": 2.285536527633667,
        "learning_rate": 5.439078771432322e-05,
        "epoch": 1.1026666666666667,
        "step": 8270
    },
    {
        "loss": 1.5519,
        "grad_norm": 3.5689778327941895,
        "learning_rate": 5.433478728954839e-05,
        "epoch": 1.1028,
        "step": 8271
    },
    {
        "loss": 1.7915,
        "grad_norm": 3.553719997406006,
        "learning_rate": 5.42788049529199e-05,
        "epoch": 1.1029333333333333,
        "step": 8272
    },
    {
        "loss": 2.4304,
        "grad_norm": 1.8685466051101685,
        "learning_rate": 5.42228407266127e-05,
        "epoch": 1.1030666666666666,
        "step": 8273
    },
    {
        "loss": 2.5631,
        "grad_norm": 3.5037660598754883,
        "learning_rate": 5.416689463279407e-05,
        "epoch": 1.1032,
        "step": 8274
    },
    {
        "loss": 2.1077,
        "grad_norm": 3.023787498474121,
        "learning_rate": 5.4110966693624795e-05,
        "epoch": 1.1033333333333333,
        "step": 8275
    },
    {
        "loss": 2.3605,
        "grad_norm": 5.261621475219727,
        "learning_rate": 5.405505693125783e-05,
        "epoch": 1.1034666666666666,
        "step": 8276
    },
    {
        "loss": 2.3973,
        "grad_norm": 2.7875587940216064,
        "learning_rate": 5.3999165367839474e-05,
        "epoch": 1.1036,
        "step": 8277
    },
    {
        "loss": 1.8111,
        "grad_norm": 2.850203275680542,
        "learning_rate": 5.3943292025508506e-05,
        "epoch": 1.1037333333333332,
        "step": 8278
    },
    {
        "loss": 1.9703,
        "grad_norm": 4.146637916564941,
        "learning_rate": 5.388743692639642e-05,
        "epoch": 1.1038666666666668,
        "step": 8279
    },
    {
        "loss": 3.3895,
        "grad_norm": 2.1684508323669434,
        "learning_rate": 5.3831600092627696e-05,
        "epoch": 1.104,
        "step": 8280
    },
    {
        "loss": 2.2925,
        "grad_norm": 3.4490816593170166,
        "learning_rate": 5.377578154631945e-05,
        "epoch": 1.1041333333333334,
        "step": 8281
    },
    {
        "loss": 2.2543,
        "grad_norm": 3.176664113998413,
        "learning_rate": 5.37199813095818e-05,
        "epoch": 1.1042666666666667,
        "step": 8282
    },
    {
        "loss": 2.0693,
        "grad_norm": 3.2009687423706055,
        "learning_rate": 5.366419940451708e-05,
        "epoch": 1.1044,
        "step": 8283
    },
    {
        "loss": 1.3986,
        "grad_norm": 4.688605785369873,
        "learning_rate": 5.360843585322095e-05,
        "epoch": 1.1045333333333334,
        "step": 8284
    },
    {
        "loss": 2.34,
        "grad_norm": 3.9820480346679688,
        "learning_rate": 5.355269067778132e-05,
        "epoch": 1.1046666666666667,
        "step": 8285
    },
    {
        "loss": 2.6955,
        "grad_norm": 4.220211029052734,
        "learning_rate": 5.3496963900279395e-05,
        "epoch": 1.1048,
        "step": 8286
    },
    {
        "loss": 1.3335,
        "grad_norm": 6.0915398597717285,
        "learning_rate": 5.3441255542788406e-05,
        "epoch": 1.1049333333333333,
        "step": 8287
    },
    {
        "loss": 1.9172,
        "grad_norm": 2.938610076904297,
        "learning_rate": 5.338556562737472e-05,
        "epoch": 1.1050666666666666,
        "step": 8288
    },
    {
        "loss": 2.1439,
        "grad_norm": 4.373386859893799,
        "learning_rate": 5.332989417609734e-05,
        "epoch": 1.1052,
        "step": 8289
    },
    {
        "loss": 2.8381,
        "grad_norm": 3.007563352584839,
        "learning_rate": 5.3274241211007946e-05,
        "epoch": 1.1053333333333333,
        "step": 8290
    },
    {
        "loss": 1.5107,
        "grad_norm": 3.4614508152008057,
        "learning_rate": 5.321860675415085e-05,
        "epoch": 1.1054666666666666,
        "step": 8291
    },
    {
        "loss": 1.5513,
        "grad_norm": 3.061148166656494,
        "learning_rate": 5.316299082756286e-05,
        "epoch": 1.1056,
        "step": 8292
    },
    {
        "loss": 2.367,
        "grad_norm": 3.182644844055176,
        "learning_rate": 5.3107393453273934e-05,
        "epoch": 1.1057333333333332,
        "step": 8293
    },
    {
        "loss": 2.1145,
        "grad_norm": 3.5775649547576904,
        "learning_rate": 5.3051814653306156e-05,
        "epoch": 1.1058666666666666,
        "step": 8294
    },
    {
        "loss": 1.0839,
        "grad_norm": 3.3091437816619873,
        "learning_rate": 5.299625444967471e-05,
        "epoch": 1.106,
        "step": 8295
    },
    {
        "loss": 2.7857,
        "grad_norm": 4.306458950042725,
        "learning_rate": 5.294071286438689e-05,
        "epoch": 1.1061333333333334,
        "step": 8296
    },
    {
        "loss": 2.7187,
        "grad_norm": 5.0727128982543945,
        "learning_rate": 5.28851899194431e-05,
        "epoch": 1.1062666666666667,
        "step": 8297
    },
    {
        "loss": 1.9774,
        "grad_norm": 3.1231822967529297,
        "learning_rate": 5.282968563683612e-05,
        "epoch": 1.1064,
        "step": 8298
    },
    {
        "loss": 2.3825,
        "grad_norm": 2.697211265563965,
        "learning_rate": 5.2774200038551424e-05,
        "epoch": 1.1065333333333334,
        "step": 8299
    },
    {
        "loss": 2.3274,
        "grad_norm": 2.910067081451416,
        "learning_rate": 5.2718733146567055e-05,
        "epoch": 1.1066666666666667,
        "step": 8300
    },
    {
        "loss": 2.4303,
        "grad_norm": 4.048678398132324,
        "learning_rate": 5.266328498285343e-05,
        "epoch": 1.1068,
        "step": 8301
    },
    {
        "loss": 2.0485,
        "grad_norm": 3.4584834575653076,
        "learning_rate": 5.260785556937409e-05,
        "epoch": 1.1069333333333333,
        "step": 8302
    },
    {
        "loss": 2.5492,
        "grad_norm": 4.318633556365967,
        "learning_rate": 5.255244492808458e-05,
        "epoch": 1.1070666666666666,
        "step": 8303
    },
    {
        "loss": 2.6442,
        "grad_norm": 2.3160808086395264,
        "learning_rate": 5.249705308093329e-05,
        "epoch": 1.1072,
        "step": 8304
    },
    {
        "loss": 1.7164,
        "grad_norm": 4.766714572906494,
        "learning_rate": 5.244168004986112e-05,
        "epoch": 1.1073333333333333,
        "step": 8305
    },
    {
        "loss": 1.9901,
        "grad_norm": 3.4955503940582275,
        "learning_rate": 5.238632585680151e-05,
        "epoch": 1.1074666666666666,
        "step": 8306
    },
    {
        "loss": 2.4977,
        "grad_norm": 3.682122230529785,
        "learning_rate": 5.2330990523680624e-05,
        "epoch": 1.1076,
        "step": 8307
    },
    {
        "loss": 2.614,
        "grad_norm": 4.330704689025879,
        "learning_rate": 5.227567407241665e-05,
        "epoch": 1.1077333333333332,
        "step": 8308
    },
    {
        "loss": 1.7002,
        "grad_norm": 3.3167407512664795,
        "learning_rate": 5.222037652492088e-05,
        "epoch": 1.1078666666666668,
        "step": 8309
    },
    {
        "loss": 1.4818,
        "grad_norm": 2.206324338912964,
        "learning_rate": 5.216509790309665e-05,
        "epoch": 1.108,
        "step": 8310
    },
    {
        "loss": 1.989,
        "grad_norm": 3.135169744491577,
        "learning_rate": 5.2109838228840334e-05,
        "epoch": 1.1081333333333334,
        "step": 8311
    },
    {
        "loss": 1.621,
        "grad_norm": 2.8510193824768066,
        "learning_rate": 5.205459752404016e-05,
        "epoch": 1.1082666666666667,
        "step": 8312
    },
    {
        "loss": 1.9237,
        "grad_norm": 3.745116710662842,
        "learning_rate": 5.199937581057728e-05,
        "epoch": 1.1084,
        "step": 8313
    },
    {
        "loss": 1.0499,
        "grad_norm": 4.2215423583984375,
        "learning_rate": 5.1944173110325204e-05,
        "epoch": 1.1085333333333334,
        "step": 8314
    },
    {
        "loss": 1.4736,
        "grad_norm": 4.23406982421875,
        "learning_rate": 5.1888989445149885e-05,
        "epoch": 1.1086666666666667,
        "step": 8315
    },
    {
        "loss": 1.456,
        "grad_norm": 3.795452117919922,
        "learning_rate": 5.183382483690992e-05,
        "epoch": 1.1088,
        "step": 8316
    },
    {
        "loss": 2.1115,
        "grad_norm": 3.323373556137085,
        "learning_rate": 5.1778679307455845e-05,
        "epoch": 1.1089333333333333,
        "step": 8317
    },
    {
        "loss": 1.8713,
        "grad_norm": 3.9138729572296143,
        "learning_rate": 5.172355287863133e-05,
        "epoch": 1.1090666666666666,
        "step": 8318
    },
    {
        "loss": 2.8211,
        "grad_norm": 3.6902856826782227,
        "learning_rate": 5.166844557227195e-05,
        "epoch": 1.1092,
        "step": 8319
    },
    {
        "loss": 1.5976,
        "grad_norm": 3.4616193771362305,
        "learning_rate": 5.161335741020612e-05,
        "epoch": 1.1093333333333333,
        "step": 8320
    },
    {
        "loss": 2.6526,
        "grad_norm": 4.090436935424805,
        "learning_rate": 5.155828841425412e-05,
        "epoch": 1.1094666666666666,
        "step": 8321
    },
    {
        "loss": 1.5425,
        "grad_norm": 3.0741825103759766,
        "learning_rate": 5.150323860622919e-05,
        "epoch": 1.1096,
        "step": 8322
    },
    {
        "loss": 1.6795,
        "grad_norm": 4.377195358276367,
        "learning_rate": 5.144820800793671e-05,
        "epoch": 1.1097333333333332,
        "step": 8323
    },
    {
        "loss": 1.9211,
        "grad_norm": 2.57735013961792,
        "learning_rate": 5.139319664117449e-05,
        "epoch": 1.1098666666666666,
        "step": 8324
    },
    {
        "loss": 2.2832,
        "grad_norm": 3.1621246337890625,
        "learning_rate": 5.1338204527732745e-05,
        "epoch": 1.11,
        "step": 8325
    },
    {
        "loss": 0.9667,
        "grad_norm": 3.4021718502044678,
        "learning_rate": 5.128323168939386e-05,
        "epoch": 1.1101333333333334,
        "step": 8326
    },
    {
        "loss": 1.5251,
        "grad_norm": 2.543761968612671,
        "learning_rate": 5.122827814793305e-05,
        "epoch": 1.1102666666666667,
        "step": 8327
    },
    {
        "loss": 1.6825,
        "grad_norm": 4.180540084838867,
        "learning_rate": 5.11733439251174e-05,
        "epoch": 1.1104,
        "step": 8328
    },
    {
        "loss": 2.5946,
        "grad_norm": 3.4211020469665527,
        "learning_rate": 5.111842904270656e-05,
        "epoch": 1.1105333333333334,
        "step": 8329
    },
    {
        "loss": 1.6984,
        "grad_norm": 3.959193468093872,
        "learning_rate": 5.1063533522452544e-05,
        "epoch": 1.1106666666666667,
        "step": 8330
    },
    {
        "loss": 1.9418,
        "grad_norm": 4.87183141708374,
        "learning_rate": 5.1008657386099614e-05,
        "epoch": 1.1108,
        "step": 8331
    },
    {
        "loss": 2.1205,
        "grad_norm": 3.6532599925994873,
        "learning_rate": 5.095380065538457e-05,
        "epoch": 1.1109333333333333,
        "step": 8332
    },
    {
        "loss": 2.4678,
        "grad_norm": 3.79630970954895,
        "learning_rate": 5.089896335203604e-05,
        "epoch": 1.1110666666666666,
        "step": 8333
    },
    {
        "loss": 2.0749,
        "grad_norm": 4.0586018562316895,
        "learning_rate": 5.084414549777553e-05,
        "epoch": 1.1112,
        "step": 8334
    },
    {
        "loss": 2.7051,
        "grad_norm": 3.436108112335205,
        "learning_rate": 5.0789347114316366e-05,
        "epoch": 1.1113333333333333,
        "step": 8335
    },
    {
        "loss": 1.9133,
        "grad_norm": 2.4314632415771484,
        "learning_rate": 5.073456822336464e-05,
        "epoch": 1.1114666666666666,
        "step": 8336
    },
    {
        "loss": 1.9919,
        "grad_norm": 3.152117967605591,
        "learning_rate": 5.067980884661827e-05,
        "epoch": 1.1116,
        "step": 8337
    },
    {
        "loss": 1.4185,
        "grad_norm": 2.5515694618225098,
        "learning_rate": 5.062506900576764e-05,
        "epoch": 1.1117333333333332,
        "step": 8338
    },
    {
        "loss": 0.9012,
        "grad_norm": 3.5359513759613037,
        "learning_rate": 5.057034872249541e-05,
        "epoch": 1.1118666666666668,
        "step": 8339
    },
    {
        "loss": 2.032,
        "grad_norm": 3.695751428604126,
        "learning_rate": 5.051564801847648e-05,
        "epoch": 1.112,
        "step": 8340
    },
    {
        "loss": 1.821,
        "grad_norm": 3.198622703552246,
        "learning_rate": 5.0460966915378095e-05,
        "epoch": 1.1121333333333334,
        "step": 8341
    },
    {
        "loss": 1.4352,
        "grad_norm": 3.9219424724578857,
        "learning_rate": 5.0406305434859335e-05,
        "epoch": 1.1122666666666667,
        "step": 8342
    },
    {
        "loss": 2.3537,
        "grad_norm": 4.511832237243652,
        "learning_rate": 5.035166359857209e-05,
        "epoch": 1.1124,
        "step": 8343
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.0503416061401367,
        "learning_rate": 5.029704142815997e-05,
        "epoch": 1.1125333333333334,
        "step": 8344
    },
    {
        "loss": 1.4466,
        "grad_norm": 2.5207760334014893,
        "learning_rate": 5.0242438945259275e-05,
        "epoch": 1.1126666666666667,
        "step": 8345
    },
    {
        "loss": 2.5697,
        "grad_norm": 4.625073432922363,
        "learning_rate": 5.0187856171497894e-05,
        "epoch": 1.1128,
        "step": 8346
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.4942574501037598,
        "learning_rate": 5.013329312849646e-05,
        "epoch": 1.1129333333333333,
        "step": 8347
    },
    {
        "loss": 2.5652,
        "grad_norm": 4.5032830238342285,
        "learning_rate": 5.0078749837867535e-05,
        "epoch": 1.1130666666666666,
        "step": 8348
    },
    {
        "loss": 1.5469,
        "grad_norm": 5.011101722717285,
        "learning_rate": 5.0024226321215974e-05,
        "epoch": 1.1132,
        "step": 8349
    },
    {
        "loss": 2.2543,
        "grad_norm": 3.1902482509613037,
        "learning_rate": 4.9969722600138615e-05,
        "epoch": 1.1133333333333333,
        "step": 8350
    },
    {
        "loss": 3.294,
        "grad_norm": 3.941828489303589,
        "learning_rate": 4.991523869622449e-05,
        "epoch": 1.1134666666666666,
        "step": 8351
    },
    {
        "loss": 2.1733,
        "grad_norm": 2.7272446155548096,
        "learning_rate": 4.986077463105514e-05,
        "epoch": 1.1136,
        "step": 8352
    },
    {
        "loss": 2.0841,
        "grad_norm": 4.803038597106934,
        "learning_rate": 4.9806330426203763e-05,
        "epoch": 1.1137333333333332,
        "step": 8353
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.92378568649292,
        "learning_rate": 4.975190610323589e-05,
        "epoch": 1.1138666666666666,
        "step": 8354
    },
    {
        "loss": 1.2575,
        "grad_norm": 3.9814722537994385,
        "learning_rate": 4.969750168370923e-05,
        "epoch": 1.114,
        "step": 8355
    },
    {
        "loss": 1.3553,
        "grad_norm": 4.361644268035889,
        "learning_rate": 4.964311718917356e-05,
        "epoch": 1.1141333333333334,
        "step": 8356
    },
    {
        "loss": 2.7548,
        "grad_norm": 3.8790621757507324,
        "learning_rate": 4.958875264117071e-05,
        "epoch": 1.1142666666666667,
        "step": 8357
    },
    {
        "loss": 2.3489,
        "grad_norm": 2.824089765548706,
        "learning_rate": 4.953440806123483e-05,
        "epoch": 1.1144,
        "step": 8358
    },
    {
        "loss": 0.7118,
        "grad_norm": 2.7541122436523438,
        "learning_rate": 4.9480083470891834e-05,
        "epoch": 1.1145333333333334,
        "step": 8359
    },
    {
        "loss": 2.4059,
        "grad_norm": 5.353358268737793,
        "learning_rate": 4.942577889165981e-05,
        "epoch": 1.1146666666666667,
        "step": 8360
    },
    {
        "loss": 2.2491,
        "grad_norm": 3.0797066688537598,
        "learning_rate": 4.937149434504924e-05,
        "epoch": 1.1148,
        "step": 8361
    },
    {
        "loss": 2.8841,
        "grad_norm": 2.9668257236480713,
        "learning_rate": 4.931722985256224e-05,
        "epoch": 1.1149333333333333,
        "step": 8362
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.7717018127441406,
        "learning_rate": 4.926298543569315e-05,
        "epoch": 1.1150666666666667,
        "step": 8363
    },
    {
        "loss": 2.2625,
        "grad_norm": 2.8162922859191895,
        "learning_rate": 4.9208761115928414e-05,
        "epoch": 1.1152,
        "step": 8364
    },
    {
        "loss": 1.6729,
        "grad_norm": 3.636823892593384,
        "learning_rate": 4.915455691474646e-05,
        "epoch": 1.1153333333333333,
        "step": 8365
    },
    {
        "loss": 2.1,
        "grad_norm": 3.485581636428833,
        "learning_rate": 4.9100372853617895e-05,
        "epoch": 1.1154666666666666,
        "step": 8366
    },
    {
        "loss": 1.1448,
        "grad_norm": 3.5615615844726562,
        "learning_rate": 4.904620895400487e-05,
        "epoch": 1.1156,
        "step": 8367
    },
    {
        "loss": 2.064,
        "grad_norm": 2.4463515281677246,
        "learning_rate": 4.899206523736222e-05,
        "epoch": 1.1157333333333332,
        "step": 8368
    },
    {
        "loss": 2.22,
        "grad_norm": 2.821502447128296,
        "learning_rate": 4.893794172513626e-05,
        "epoch": 1.1158666666666668,
        "step": 8369
    },
    {
        "loss": 2.3011,
        "grad_norm": 2.968782901763916,
        "learning_rate": 4.888383843876573e-05,
        "epoch": 1.116,
        "step": 8370
    },
    {
        "loss": 2.4959,
        "grad_norm": 4.0896077156066895,
        "learning_rate": 4.88297553996808e-05,
        "epoch": 1.1161333333333334,
        "step": 8371
    },
    {
        "loss": 1.5761,
        "grad_norm": 3.3884377479553223,
        "learning_rate": 4.877569262930417e-05,
        "epoch": 1.1162666666666667,
        "step": 8372
    },
    {
        "loss": 2.3431,
        "grad_norm": 2.7784125804901123,
        "learning_rate": 4.872165014905023e-05,
        "epoch": 1.1164,
        "step": 8373
    },
    {
        "loss": 1.9444,
        "grad_norm": 3.191984176635742,
        "learning_rate": 4.8667627980325455e-05,
        "epoch": 1.1165333333333334,
        "step": 8374
    },
    {
        "loss": 1.5926,
        "grad_norm": 3.743401527404785,
        "learning_rate": 4.861362614452811e-05,
        "epoch": 1.1166666666666667,
        "step": 8375
    },
    {
        "loss": 0.8425,
        "grad_norm": 3.4345738887786865,
        "learning_rate": 4.855964466304844e-05,
        "epoch": 1.1168,
        "step": 8376
    },
    {
        "loss": 2.0136,
        "grad_norm": 3.2557661533355713,
        "learning_rate": 4.850568355726895e-05,
        "epoch": 1.1169333333333333,
        "step": 8377
    },
    {
        "loss": 1.5707,
        "grad_norm": 4.693911552429199,
        "learning_rate": 4.8451742848563506e-05,
        "epoch": 1.1170666666666667,
        "step": 8378
    },
    {
        "loss": 2.5047,
        "grad_norm": 2.815676212310791,
        "learning_rate": 4.8397822558298565e-05,
        "epoch": 1.1172,
        "step": 8379
    },
    {
        "loss": 0.8471,
        "grad_norm": 3.700901508331299,
        "learning_rate": 4.834392270783182e-05,
        "epoch": 1.1173333333333333,
        "step": 8380
    },
    {
        "loss": 2.192,
        "grad_norm": 2.4747374057769775,
        "learning_rate": 4.8290043318513314e-05,
        "epoch": 1.1174666666666666,
        "step": 8381
    },
    {
        "loss": 2.6727,
        "grad_norm": 2.629213571548462,
        "learning_rate": 4.823618441168479e-05,
        "epoch": 1.1176,
        "step": 8382
    },
    {
        "loss": 1.7421,
        "grad_norm": 2.5904181003570557,
        "learning_rate": 4.8182346008680136e-05,
        "epoch": 1.1177333333333332,
        "step": 8383
    },
    {
        "loss": 1.6755,
        "grad_norm": 4.876260280609131,
        "learning_rate": 4.8128528130824735e-05,
        "epoch": 1.1178666666666666,
        "step": 8384
    },
    {
        "loss": 2.0072,
        "grad_norm": 2.917685031890869,
        "learning_rate": 4.807473079943599e-05,
        "epoch": 1.1179999999999999,
        "step": 8385
    },
    {
        "loss": 0.9194,
        "grad_norm": 3.705955982208252,
        "learning_rate": 4.802095403582345e-05,
        "epoch": 1.1181333333333334,
        "step": 8386
    },
    {
        "loss": 2.162,
        "grad_norm": 7.232960224151611,
        "learning_rate": 4.7967197861288094e-05,
        "epoch": 1.1182666666666667,
        "step": 8387
    },
    {
        "loss": 1.7096,
        "grad_norm": 3.1085205078125,
        "learning_rate": 4.7913462297122926e-05,
        "epoch": 1.1184,
        "step": 8388
    },
    {
        "loss": 1.8159,
        "grad_norm": 3.2584762573242188,
        "learning_rate": 4.7859747364612814e-05,
        "epoch": 1.1185333333333334,
        "step": 8389
    },
    {
        "loss": 1.7477,
        "grad_norm": 3.185340642929077,
        "learning_rate": 4.780605308503445e-05,
        "epoch": 1.1186666666666667,
        "step": 8390
    },
    {
        "loss": 0.9874,
        "grad_norm": 3.97869610786438,
        "learning_rate": 4.7752379479656404e-05,
        "epoch": 1.1188,
        "step": 8391
    },
    {
        "loss": 1.7308,
        "grad_norm": 3.267477035522461,
        "learning_rate": 4.76987265697387e-05,
        "epoch": 1.1189333333333333,
        "step": 8392
    },
    {
        "loss": 2.4783,
        "grad_norm": 3.0535340309143066,
        "learning_rate": 4.764509437653372e-05,
        "epoch": 1.1190666666666667,
        "step": 8393
    },
    {
        "loss": 2.4663,
        "grad_norm": 3.099321126937866,
        "learning_rate": 4.759148292128518e-05,
        "epoch": 1.1192,
        "step": 8394
    },
    {
        "loss": 1.7496,
        "grad_norm": 4.046047687530518,
        "learning_rate": 4.753789222522891e-05,
        "epoch": 1.1193333333333333,
        "step": 8395
    },
    {
        "loss": 1.778,
        "grad_norm": 4.0675950050354,
        "learning_rate": 4.748432230959232e-05,
        "epoch": 1.1194666666666666,
        "step": 8396
    },
    {
        "loss": 2.2169,
        "grad_norm": 3.2474746704101562,
        "learning_rate": 4.743077319559449e-05,
        "epoch": 1.1196,
        "step": 8397
    },
    {
        "loss": 1.4103,
        "grad_norm": 5.138369083404541,
        "learning_rate": 4.737724490444657e-05,
        "epoch": 1.1197333333333332,
        "step": 8398
    },
    {
        "loss": 1.8077,
        "grad_norm": 2.7641239166259766,
        "learning_rate": 4.732373745735125e-05,
        "epoch": 1.1198666666666666,
        "step": 8399
    },
    {
        "loss": 1.6202,
        "grad_norm": 4.264233589172363,
        "learning_rate": 4.727025087550312e-05,
        "epoch": 1.12,
        "step": 8400
    },
    {
        "loss": 1.4131,
        "grad_norm": 3.2288570404052734,
        "learning_rate": 4.7216785180088106e-05,
        "epoch": 1.1201333333333334,
        "step": 8401
    },
    {
        "loss": 1.5905,
        "grad_norm": 4.48111629486084,
        "learning_rate": 4.716334039228449e-05,
        "epoch": 1.1202666666666667,
        "step": 8402
    },
    {
        "loss": 2.2675,
        "grad_norm": 3.3201236724853516,
        "learning_rate": 4.710991653326163e-05,
        "epoch": 1.1204,
        "step": 8403
    },
    {
        "loss": 2.8054,
        "grad_norm": 2.822993755340576,
        "learning_rate": 4.705651362418123e-05,
        "epoch": 1.1205333333333334,
        "step": 8404
    },
    {
        "loss": 1.6301,
        "grad_norm": 4.515330791473389,
        "learning_rate": 4.700313168619608e-05,
        "epoch": 1.1206666666666667,
        "step": 8405
    },
    {
        "loss": 2.2462,
        "grad_norm": 3.8186681270599365,
        "learning_rate": 4.694977074045105e-05,
        "epoch": 1.1208,
        "step": 8406
    },
    {
        "loss": 2.6235,
        "grad_norm": 2.809019088745117,
        "learning_rate": 4.6896430808082546e-05,
        "epoch": 1.1209333333333333,
        "step": 8407
    },
    {
        "loss": 1.9711,
        "grad_norm": 4.349940776824951,
        "learning_rate": 4.684311191021887e-05,
        "epoch": 1.1210666666666667,
        "step": 8408
    },
    {
        "loss": 2.0632,
        "grad_norm": 3.6575052738189697,
        "learning_rate": 4.6789814067979655e-05,
        "epoch": 1.1212,
        "step": 8409
    },
    {
        "loss": 2.4541,
        "grad_norm": 3.8746585845947266,
        "learning_rate": 4.673653730247632e-05,
        "epoch": 1.1213333333333333,
        "step": 8410
    },
    {
        "loss": 2.0625,
        "grad_norm": 3.533949136734009,
        "learning_rate": 4.6683281634812124e-05,
        "epoch": 1.1214666666666666,
        "step": 8411
    },
    {
        "loss": 1.6849,
        "grad_norm": 3.4871346950531006,
        "learning_rate": 4.663004708608175e-05,
        "epoch": 1.1216,
        "step": 8412
    },
    {
        "loss": 2.551,
        "grad_norm": 2.881845474243164,
        "learning_rate": 4.657683367737153e-05,
        "epoch": 1.1217333333333332,
        "step": 8413
    },
    {
        "loss": 2.5148,
        "grad_norm": 2.4328012466430664,
        "learning_rate": 4.6523641429759535e-05,
        "epoch": 1.1218666666666666,
        "step": 8414
    },
    {
        "loss": 2.3357,
        "grad_norm": 5.708963394165039,
        "learning_rate": 4.647047036431539e-05,
        "epoch": 1.1219999999999999,
        "step": 8415
    },
    {
        "loss": 1.5677,
        "grad_norm": 3.78125262260437,
        "learning_rate": 4.6417320502100316e-05,
        "epoch": 1.1221333333333334,
        "step": 8416
    },
    {
        "loss": 2.2505,
        "grad_norm": 2.1240484714508057,
        "learning_rate": 4.636419186416722e-05,
        "epoch": 1.1222666666666667,
        "step": 8417
    },
    {
        "loss": 1.3222,
        "grad_norm": 3.375115394592285,
        "learning_rate": 4.6311084471560475e-05,
        "epoch": 1.1224,
        "step": 8418
    },
    {
        "loss": 1.7291,
        "grad_norm": 3.7096731662750244,
        "learning_rate": 4.6257998345316045e-05,
        "epoch": 1.1225333333333334,
        "step": 8419
    },
    {
        "loss": 0.6937,
        "grad_norm": 3.7610666751861572,
        "learning_rate": 4.6204933506461666e-05,
        "epoch": 1.1226666666666667,
        "step": 8420
    },
    {
        "loss": 3.3042,
        "grad_norm": 4.446540355682373,
        "learning_rate": 4.615188997601646e-05,
        "epoch": 1.1228,
        "step": 8421
    },
    {
        "loss": 0.927,
        "grad_norm": 3.6755383014678955,
        "learning_rate": 4.6098867774991e-05,
        "epoch": 1.1229333333333333,
        "step": 8422
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.9429047107696533,
        "learning_rate": 4.6045866924387736e-05,
        "epoch": 1.1230666666666667,
        "step": 8423
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.8572607040405273,
        "learning_rate": 4.599288744520042e-05,
        "epoch": 1.1232,
        "step": 8424
    },
    {
        "loss": 2.286,
        "grad_norm": 3.910397529602051,
        "learning_rate": 4.5939929358414514e-05,
        "epoch": 1.1233333333333333,
        "step": 8425
    },
    {
        "loss": 0.7517,
        "grad_norm": 4.0463762283325195,
        "learning_rate": 4.588699268500661e-05,
        "epoch": 1.1234666666666666,
        "step": 8426
    },
    {
        "loss": 2.5985,
        "grad_norm": 1.942870020866394,
        "learning_rate": 4.5834077445945445e-05,
        "epoch": 1.1236,
        "step": 8427
    },
    {
        "loss": 1.6421,
        "grad_norm": 5.5642194747924805,
        "learning_rate": 4.578118366219061e-05,
        "epoch": 1.1237333333333333,
        "step": 8428
    },
    {
        "loss": 2.0398,
        "grad_norm": 3.608053684234619,
        "learning_rate": 4.5728311354693865e-05,
        "epoch": 1.1238666666666666,
        "step": 8429
    },
    {
        "loss": 2.3385,
        "grad_norm": 3.130664587020874,
        "learning_rate": 4.567546054439781e-05,
        "epoch": 1.124,
        "step": 8430
    },
    {
        "loss": 2.8727,
        "grad_norm": 3.289594888687134,
        "learning_rate": 4.5622631252236936e-05,
        "epoch": 1.1241333333333334,
        "step": 8431
    },
    {
        "loss": 0.9251,
        "grad_norm": 3.2220427989959717,
        "learning_rate": 4.5569823499137076e-05,
        "epoch": 1.1242666666666667,
        "step": 8432
    },
    {
        "loss": 2.1879,
        "grad_norm": 3.787402391433716,
        "learning_rate": 4.5517037306015697e-05,
        "epoch": 1.1244,
        "step": 8433
    },
    {
        "loss": 1.6987,
        "grad_norm": 2.9186248779296875,
        "learning_rate": 4.546427269378146e-05,
        "epoch": 1.1245333333333334,
        "step": 8434
    },
    {
        "loss": 1.6586,
        "grad_norm": 2.4317362308502197,
        "learning_rate": 4.541152968333455e-05,
        "epoch": 1.1246666666666667,
        "step": 8435
    },
    {
        "loss": 1.8516,
        "grad_norm": 2.4881083965301514,
        "learning_rate": 4.5358808295566826e-05,
        "epoch": 1.1248,
        "step": 8436
    },
    {
        "loss": 1.7998,
        "grad_norm": 3.352728843688965,
        "learning_rate": 4.53061085513612e-05,
        "epoch": 1.1249333333333333,
        "step": 8437
    },
    {
        "loss": 0.5957,
        "grad_norm": 3.2718050479888916,
        "learning_rate": 4.525343047159253e-05,
        "epoch": 1.1250666666666667,
        "step": 8438
    },
    {
        "loss": 2.0742,
        "grad_norm": 3.594621181488037,
        "learning_rate": 4.520077407712648e-05,
        "epoch": 1.1252,
        "step": 8439
    },
    {
        "loss": 2.1294,
        "grad_norm": 3.3318002223968506,
        "learning_rate": 4.514813938882054e-05,
        "epoch": 1.1253333333333333,
        "step": 8440
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.0375149250030518,
        "learning_rate": 4.50955264275235e-05,
        "epoch": 1.1254666666666666,
        "step": 8441
    },
    {
        "loss": 1.9543,
        "grad_norm": 3.094782590866089,
        "learning_rate": 4.504293521407557e-05,
        "epoch": 1.1256,
        "step": 8442
    },
    {
        "loss": 1.4368,
        "grad_norm": 2.9833128452301025,
        "learning_rate": 4.499036576930828e-05,
        "epoch": 1.1257333333333333,
        "step": 8443
    },
    {
        "loss": 0.6766,
        "grad_norm": 2.0993001461029053,
        "learning_rate": 4.4937818114044405e-05,
        "epoch": 1.1258666666666666,
        "step": 8444
    },
    {
        "loss": 2.0273,
        "grad_norm": 4.048853874206543,
        "learning_rate": 4.488529226909851e-05,
        "epoch": 1.126,
        "step": 8445
    },
    {
        "loss": 1.8745,
        "grad_norm": 3.3616526126861572,
        "learning_rate": 4.483278825527619e-05,
        "epoch": 1.1261333333333332,
        "step": 8446
    },
    {
        "loss": 2.1279,
        "grad_norm": 2.766033411026001,
        "learning_rate": 4.47803060933743e-05,
        "epoch": 1.1262666666666667,
        "step": 8447
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.238129138946533,
        "learning_rate": 4.472784580418136e-05,
        "epoch": 1.1264,
        "step": 8448
    },
    {
        "loss": 2.3414,
        "grad_norm": 2.4144434928894043,
        "learning_rate": 4.467540740847703e-05,
        "epoch": 1.1265333333333334,
        "step": 8449
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.821669578552246,
        "learning_rate": 4.462299092703246e-05,
        "epoch": 1.1266666666666667,
        "step": 8450
    },
    {
        "loss": 2.1112,
        "grad_norm": 3.544896125793457,
        "learning_rate": 4.4570596380609686e-05,
        "epoch": 1.1268,
        "step": 8451
    },
    {
        "loss": 2.2454,
        "grad_norm": 3.764376640319824,
        "learning_rate": 4.451822378996271e-05,
        "epoch": 1.1269333333333333,
        "step": 8452
    },
    {
        "loss": 2.7462,
        "grad_norm": 3.0936732292175293,
        "learning_rate": 4.446587317583618e-05,
        "epoch": 1.1270666666666667,
        "step": 8453
    },
    {
        "loss": 2.07,
        "grad_norm": 2.3093278408050537,
        "learning_rate": 4.44135445589667e-05,
        "epoch": 1.1272,
        "step": 8454
    },
    {
        "loss": 2.1914,
        "grad_norm": 4.115506172180176,
        "learning_rate": 4.436123796008153e-05,
        "epoch": 1.1273333333333333,
        "step": 8455
    },
    {
        "loss": 2.8099,
        "grad_norm": 3.386654853820801,
        "learning_rate": 4.430895339989958e-05,
        "epoch": 1.1274666666666666,
        "step": 8456
    },
    {
        "loss": 2.6092,
        "grad_norm": 3.434561252593994,
        "learning_rate": 4.425669089913095e-05,
        "epoch": 1.1276,
        "step": 8457
    },
    {
        "loss": 2.6214,
        "grad_norm": 3.4273622035980225,
        "learning_rate": 4.4204450478476955e-05,
        "epoch": 1.1277333333333333,
        "step": 8458
    },
    {
        "loss": 2.0875,
        "grad_norm": 2.7290565967559814,
        "learning_rate": 4.4152232158630394e-05,
        "epoch": 1.1278666666666666,
        "step": 8459
    },
    {
        "loss": 2.4664,
        "grad_norm": 4.612227439880371,
        "learning_rate": 4.410003596027482e-05,
        "epoch": 1.1280000000000001,
        "step": 8460
    },
    {
        "loss": 2.2357,
        "grad_norm": 3.7369561195373535,
        "learning_rate": 4.4047861904085565e-05,
        "epoch": 1.1281333333333334,
        "step": 8461
    },
    {
        "loss": 1.8715,
        "grad_norm": 3.73641037940979,
        "learning_rate": 4.399571001072874e-05,
        "epoch": 1.1282666666666668,
        "step": 8462
    },
    {
        "loss": 2.9052,
        "grad_norm": 2.580005407333374,
        "learning_rate": 4.3943580300862195e-05,
        "epoch": 1.1284,
        "step": 8463
    },
    {
        "loss": 2.4856,
        "grad_norm": 3.9143781661987305,
        "learning_rate": 4.389147279513439e-05,
        "epoch": 1.1285333333333334,
        "step": 8464
    },
    {
        "loss": 1.863,
        "grad_norm": 3.548938035964966,
        "learning_rate": 4.383938751418539e-05,
        "epoch": 1.1286666666666667,
        "step": 8465
    },
    {
        "loss": 1.1276,
        "grad_norm": 4.150027275085449,
        "learning_rate": 4.378732447864638e-05,
        "epoch": 1.1288,
        "step": 8466
    },
    {
        "loss": 2.6001,
        "grad_norm": 3.1561279296875,
        "learning_rate": 4.37352837091397e-05,
        "epoch": 1.1289333333333333,
        "step": 8467
    },
    {
        "loss": 2.103,
        "grad_norm": 4.160793304443359,
        "learning_rate": 4.36832652262789e-05,
        "epoch": 1.1290666666666667,
        "step": 8468
    },
    {
        "loss": 2.491,
        "grad_norm": 2.042414426803589,
        "learning_rate": 4.3631269050668486e-05,
        "epoch": 1.1292,
        "step": 8469
    },
    {
        "loss": 1.5795,
        "grad_norm": 3.001232862472534,
        "learning_rate": 4.357929520290459e-05,
        "epoch": 1.1293333333333333,
        "step": 8470
    },
    {
        "loss": 2.3089,
        "grad_norm": 4.3711347579956055,
        "learning_rate": 4.352734370357415e-05,
        "epoch": 1.1294666666666666,
        "step": 8471
    },
    {
        "loss": 1.4251,
        "grad_norm": 3.9005439281463623,
        "learning_rate": 4.3475414573255194e-05,
        "epoch": 1.1296,
        "step": 8472
    },
    {
        "loss": 1.1088,
        "grad_norm": 5.908616542816162,
        "learning_rate": 4.3423507832517155e-05,
        "epoch": 1.1297333333333333,
        "step": 8473
    },
    {
        "loss": 2.2933,
        "grad_norm": 4.391979694366455,
        "learning_rate": 4.3371623501920445e-05,
        "epoch": 1.1298666666666666,
        "step": 8474
    },
    {
        "loss": 1.7561,
        "grad_norm": 3.0422585010528564,
        "learning_rate": 4.331976160201662e-05,
        "epoch": 1.13,
        "step": 8475
    },
    {
        "loss": 1.398,
        "grad_norm": 2.993835210800171,
        "learning_rate": 4.3267922153348396e-05,
        "epoch": 1.1301333333333332,
        "step": 8476
    },
    {
        "loss": 2.2905,
        "grad_norm": 4.910112380981445,
        "learning_rate": 4.321610517644955e-05,
        "epoch": 1.1302666666666668,
        "step": 8477
    },
    {
        "loss": 2.3658,
        "grad_norm": 3.365530014038086,
        "learning_rate": 4.316431069184479e-05,
        "epoch": 1.1304,
        "step": 8478
    },
    {
        "loss": 1.7157,
        "grad_norm": 3.5452089309692383,
        "learning_rate": 4.311253872005033e-05,
        "epoch": 1.1305333333333334,
        "step": 8479
    },
    {
        "loss": 2.1875,
        "grad_norm": 3.3212380409240723,
        "learning_rate": 4.306078928157317e-05,
        "epoch": 1.1306666666666667,
        "step": 8480
    },
    {
        "loss": 1.8126,
        "grad_norm": 3.589733123779297,
        "learning_rate": 4.3009062396911315e-05,
        "epoch": 1.1308,
        "step": 8481
    },
    {
        "loss": 3.0822,
        "grad_norm": 2.6237123012542725,
        "learning_rate": 4.2957358086554036e-05,
        "epoch": 1.1309333333333333,
        "step": 8482
    },
    {
        "loss": 1.7965,
        "grad_norm": 4.539637088775635,
        "learning_rate": 4.290567637098154e-05,
        "epoch": 1.1310666666666667,
        "step": 8483
    },
    {
        "loss": 2.0989,
        "grad_norm": 4.6899309158325195,
        "learning_rate": 4.285401727066534e-05,
        "epoch": 1.1312,
        "step": 8484
    },
    {
        "loss": 2.0019,
        "grad_norm": 4.073565483093262,
        "learning_rate": 4.280238080606745e-05,
        "epoch": 1.1313333333333333,
        "step": 8485
    },
    {
        "loss": 2.6594,
        "grad_norm": 2.4382717609405518,
        "learning_rate": 4.275076699764151e-05,
        "epoch": 1.1314666666666666,
        "step": 8486
    },
    {
        "loss": 1.7924,
        "grad_norm": 2.7460877895355225,
        "learning_rate": 4.269917586583173e-05,
        "epoch": 1.1316,
        "step": 8487
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.77681565284729,
        "learning_rate": 4.264760743107381e-05,
        "epoch": 1.1317333333333333,
        "step": 8488
    },
    {
        "loss": 1.4917,
        "grad_norm": 3.644327402114868,
        "learning_rate": 4.25960617137939e-05,
        "epoch": 1.1318666666666666,
        "step": 8489
    },
    {
        "loss": 1.5562,
        "grad_norm": 3.1427559852600098,
        "learning_rate": 4.2544538734409545e-05,
        "epoch": 1.1320000000000001,
        "step": 8490
    },
    {
        "loss": 2.5738,
        "grad_norm": 3.0512402057647705,
        "learning_rate": 4.249303851332918e-05,
        "epoch": 1.1321333333333334,
        "step": 8491
    },
    {
        "loss": 1.36,
        "grad_norm": 1.9476622343063354,
        "learning_rate": 4.244156107095223e-05,
        "epoch": 1.1322666666666668,
        "step": 8492
    },
    {
        "loss": 2.0095,
        "grad_norm": 4.0976128578186035,
        "learning_rate": 4.2390106427669106e-05,
        "epoch": 1.1324,
        "step": 8493
    },
    {
        "loss": 1.8172,
        "grad_norm": 3.310839891433716,
        "learning_rate": 4.233867460386099e-05,
        "epoch": 1.1325333333333334,
        "step": 8494
    },
    {
        "loss": 2.3087,
        "grad_norm": 2.6965105533599854,
        "learning_rate": 4.2287265619900454e-05,
        "epoch": 1.1326666666666667,
        "step": 8495
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.5444509983062744,
        "learning_rate": 4.223587949615057e-05,
        "epoch": 1.1328,
        "step": 8496
    },
    {
        "loss": 2.8958,
        "grad_norm": 2.923435926437378,
        "learning_rate": 4.2184516252965786e-05,
        "epoch": 1.1329333333333333,
        "step": 8497
    },
    {
        "loss": 1.784,
        "grad_norm": 3.18280291557312,
        "learning_rate": 4.213317591069098e-05,
        "epoch": 1.1330666666666667,
        "step": 8498
    },
    {
        "loss": 1.8036,
        "grad_norm": 2.5553085803985596,
        "learning_rate": 4.208185848966242e-05,
        "epoch": 1.1332,
        "step": 8499
    },
    {
        "loss": 1.7858,
        "grad_norm": 4.13197135925293,
        "learning_rate": 4.2030564010207044e-05,
        "epoch": 1.1333333333333333,
        "step": 8500
    },
    {
        "loss": 2.407,
        "grad_norm": 3.9908194541931152,
        "learning_rate": 4.197929249264283e-05,
        "epoch": 1.1334666666666666,
        "step": 8501
    },
    {
        "loss": 2.0149,
        "grad_norm": 2.4621496200561523,
        "learning_rate": 4.192804395727861e-05,
        "epoch": 1.1336,
        "step": 8502
    },
    {
        "loss": 2.1515,
        "grad_norm": 3.739004611968994,
        "learning_rate": 4.187681842441389e-05,
        "epoch": 1.1337333333333333,
        "step": 8503
    },
    {
        "loss": 0.7069,
        "grad_norm": 3.42807936668396,
        "learning_rate": 4.18256159143396e-05,
        "epoch": 1.1338666666666666,
        "step": 8504
    },
    {
        "loss": 2.3246,
        "grad_norm": 3.788820266723633,
        "learning_rate": 4.177443644733703e-05,
        "epoch": 1.134,
        "step": 8505
    },
    {
        "loss": 2.1229,
        "grad_norm": 2.8681507110595703,
        "learning_rate": 4.1723280043678534e-05,
        "epoch": 1.1341333333333332,
        "step": 8506
    },
    {
        "loss": 1.8142,
        "grad_norm": 3.200331449508667,
        "learning_rate": 4.1672146723627405e-05,
        "epoch": 1.1342666666666668,
        "step": 8507
    },
    {
        "loss": 1.5693,
        "grad_norm": 2.502352714538574,
        "learning_rate": 4.162103650743766e-05,
        "epoch": 1.1344,
        "step": 8508
    },
    {
        "loss": 2.306,
        "grad_norm": 2.2725913524627686,
        "learning_rate": 4.156994941535443e-05,
        "epoch": 1.1345333333333334,
        "step": 8509
    },
    {
        "loss": 2.555,
        "grad_norm": 2.865208864212036,
        "learning_rate": 4.151888546761318e-05,
        "epoch": 1.1346666666666667,
        "step": 8510
    },
    {
        "loss": 2.1008,
        "grad_norm": 1.8912293910980225,
        "learning_rate": 4.1467844684440734e-05,
        "epoch": 1.1348,
        "step": 8511
    },
    {
        "loss": 2.7458,
        "grad_norm": 2.6214520931243896,
        "learning_rate": 4.1416827086054346e-05,
        "epoch": 1.1349333333333333,
        "step": 8512
    },
    {
        "loss": 1.9868,
        "grad_norm": 3.1076443195343018,
        "learning_rate": 4.136583269266255e-05,
        "epoch": 1.1350666666666667,
        "step": 8513
    },
    {
        "loss": 2.5026,
        "grad_norm": 2.595649480819702,
        "learning_rate": 4.131486152446411e-05,
        "epoch": 1.1352,
        "step": 8514
    },
    {
        "loss": 2.7334,
        "grad_norm": 2.865612030029297,
        "learning_rate": 4.126391360164897e-05,
        "epoch": 1.1353333333333333,
        "step": 8515
    },
    {
        "loss": 1.1501,
        "grad_norm": 2.763674736022949,
        "learning_rate": 4.121298894439781e-05,
        "epoch": 1.1354666666666666,
        "step": 8516
    },
    {
        "loss": 1.8924,
        "grad_norm": 4.6861114501953125,
        "learning_rate": 4.1162087572882034e-05,
        "epoch": 1.1356,
        "step": 8517
    },
    {
        "loss": 2.5139,
        "grad_norm": 3.4246485233306885,
        "learning_rate": 4.111120950726396e-05,
        "epoch": 1.1357333333333333,
        "step": 8518
    },
    {
        "loss": 1.3466,
        "grad_norm": 4.547789096832275,
        "learning_rate": 4.106035476769628e-05,
        "epoch": 1.1358666666666666,
        "step": 8519
    },
    {
        "loss": 2.5459,
        "grad_norm": 2.267303943634033,
        "learning_rate": 4.100952337432302e-05,
        "epoch": 1.1360000000000001,
        "step": 8520
    },
    {
        "loss": 1.8779,
        "grad_norm": 3.8516595363616943,
        "learning_rate": 4.095871534727847e-05,
        "epoch": 1.1361333333333334,
        "step": 8521
    },
    {
        "loss": 2.5983,
        "grad_norm": 3.6744179725646973,
        "learning_rate": 4.0907930706688116e-05,
        "epoch": 1.1362666666666668,
        "step": 8522
    },
    {
        "loss": 2.4509,
        "grad_norm": 2.5123074054718018,
        "learning_rate": 4.08571694726676e-05,
        "epoch": 1.1364,
        "step": 8523
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.5632054805755615,
        "learning_rate": 4.080643166532383e-05,
        "epoch": 1.1365333333333334,
        "step": 8524
    },
    {
        "loss": 2.4499,
        "grad_norm": 2.6667556762695312,
        "learning_rate": 4.075571730475416e-05,
        "epoch": 1.1366666666666667,
        "step": 8525
    },
    {
        "loss": 1.0714,
        "grad_norm": 3.9354026317596436,
        "learning_rate": 4.0705026411046775e-05,
        "epoch": 1.1368,
        "step": 8526
    },
    {
        "loss": 1.4273,
        "grad_norm": 3.413034200668335,
        "learning_rate": 4.065435900428052e-05,
        "epoch": 1.1369333333333334,
        "step": 8527
    },
    {
        "loss": 1.0549,
        "grad_norm": 3.6262898445129395,
        "learning_rate": 4.060371510452472e-05,
        "epoch": 1.1370666666666667,
        "step": 8528
    },
    {
        "loss": 2.076,
        "grad_norm": 2.7286126613616943,
        "learning_rate": 4.05530947318399e-05,
        "epoch": 1.1372,
        "step": 8529
    },
    {
        "loss": 1.2523,
        "grad_norm": 4.133048057556152,
        "learning_rate": 4.050249790627678e-05,
        "epoch": 1.1373333333333333,
        "step": 8530
    },
    {
        "loss": 2.8772,
        "grad_norm": 2.5695948600769043,
        "learning_rate": 4.045192464787695e-05,
        "epoch": 1.1374666666666666,
        "step": 8531
    },
    {
        "loss": 2.5355,
        "grad_norm": 3.3928418159484863,
        "learning_rate": 4.040137497667266e-05,
        "epoch": 1.1376,
        "step": 8532
    },
    {
        "loss": 0.9736,
        "grad_norm": 4.838212966918945,
        "learning_rate": 4.035084891268679e-05,
        "epoch": 1.1377333333333333,
        "step": 8533
    },
    {
        "loss": 0.7413,
        "grad_norm": 3.178809881210327,
        "learning_rate": 4.030034647593305e-05,
        "epoch": 1.1378666666666666,
        "step": 8534
    },
    {
        "loss": 1.6276,
        "grad_norm": 4.235317230224609,
        "learning_rate": 4.024986768641535e-05,
        "epoch": 1.138,
        "step": 8535
    },
    {
        "loss": 2.4997,
        "grad_norm": 3.815746545791626,
        "learning_rate": 4.019941256412873e-05,
        "epoch": 1.1381333333333332,
        "step": 8536
    },
    {
        "loss": 1.9181,
        "grad_norm": 4.19011116027832,
        "learning_rate": 4.014898112905845e-05,
        "epoch": 1.1382666666666668,
        "step": 8537
    },
    {
        "loss": 1.914,
        "grad_norm": 6.2483811378479,
        "learning_rate": 4.009857340118083e-05,
        "epoch": 1.1384,
        "step": 8538
    },
    {
        "loss": 2.6124,
        "grad_norm": 2.4907402992248535,
        "learning_rate": 4.0048189400462355e-05,
        "epoch": 1.1385333333333334,
        "step": 8539
    },
    {
        "loss": 1.4956,
        "grad_norm": 3.4956679344177246,
        "learning_rate": 3.99978291468603e-05,
        "epoch": 1.1386666666666667,
        "step": 8540
    },
    {
        "loss": 1.2572,
        "grad_norm": 3.8117547035217285,
        "learning_rate": 3.9947492660322574e-05,
        "epoch": 1.1388,
        "step": 8541
    },
    {
        "loss": 2.2644,
        "grad_norm": 3.6749565601348877,
        "learning_rate": 3.989717996078763e-05,
        "epoch": 1.1389333333333334,
        "step": 8542
    },
    {
        "loss": 2.5616,
        "grad_norm": 3.8619704246520996,
        "learning_rate": 3.984689106818461e-05,
        "epoch": 1.1390666666666667,
        "step": 8543
    },
    {
        "loss": 2.1428,
        "grad_norm": 3.504573106765747,
        "learning_rate": 3.979662600243282e-05,
        "epoch": 1.1392,
        "step": 8544
    },
    {
        "loss": 1.6452,
        "grad_norm": 2.212083101272583,
        "learning_rate": 3.9746384783442713e-05,
        "epoch": 1.1393333333333333,
        "step": 8545
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.6219723224639893,
        "learning_rate": 3.969616743111484e-05,
        "epoch": 1.1394666666666666,
        "step": 8546
    },
    {
        "loss": 2.1552,
        "grad_norm": 3.611206293106079,
        "learning_rate": 3.964597396534067e-05,
        "epoch": 1.1396,
        "step": 8547
    },
    {
        "loss": 2.6474,
        "grad_norm": 3.056471586227417,
        "learning_rate": 3.959580440600171e-05,
        "epoch": 1.1397333333333333,
        "step": 8548
    },
    {
        "loss": 2.0568,
        "grad_norm": 3.773540496826172,
        "learning_rate": 3.9545658772970495e-05,
        "epoch": 1.1398666666666666,
        "step": 8549
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.413191795349121,
        "learning_rate": 3.9495537086109826e-05,
        "epoch": 1.1400000000000001,
        "step": 8550
    },
    {
        "loss": 0.7389,
        "grad_norm": 4.841182231903076,
        "learning_rate": 3.944543936527316e-05,
        "epoch": 1.1401333333333334,
        "step": 8551
    },
    {
        "loss": 2.3732,
        "grad_norm": 3.2190096378326416,
        "learning_rate": 3.939536563030425e-05,
        "epoch": 1.1402666666666668,
        "step": 8552
    },
    {
        "loss": 2.1955,
        "grad_norm": 3.1208386421203613,
        "learning_rate": 3.9345315901037415e-05,
        "epoch": 1.1404,
        "step": 8553
    },
    {
        "loss": 2.7733,
        "grad_norm": 4.344605922698975,
        "learning_rate": 3.9295290197297774e-05,
        "epoch": 1.1405333333333334,
        "step": 8554
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.4855592250823975,
        "learning_rate": 3.924528853890049e-05,
        "epoch": 1.1406666666666667,
        "step": 8555
    },
    {
        "loss": 2.078,
        "grad_norm": 3.703057050704956,
        "learning_rate": 3.919531094565141e-05,
        "epoch": 1.1408,
        "step": 8556
    },
    {
        "loss": 1.7602,
        "grad_norm": 4.579957485198975,
        "learning_rate": 3.914535743734684e-05,
        "epoch": 1.1409333333333334,
        "step": 8557
    },
    {
        "loss": 2.1269,
        "grad_norm": 3.3698198795318604,
        "learning_rate": 3.909542803377355e-05,
        "epoch": 1.1410666666666667,
        "step": 8558
    },
    {
        "loss": 2.0556,
        "grad_norm": 3.4996392726898193,
        "learning_rate": 3.9045522754708705e-05,
        "epoch": 1.1412,
        "step": 8559
    },
    {
        "loss": 0.4963,
        "grad_norm": 2.565333127975464,
        "learning_rate": 3.89956416199201e-05,
        "epoch": 1.1413333333333333,
        "step": 8560
    },
    {
        "loss": 2.4305,
        "grad_norm": 3.926467180252075,
        "learning_rate": 3.8945784649165683e-05,
        "epoch": 1.1414666666666666,
        "step": 8561
    },
    {
        "loss": 2.0449,
        "grad_norm": 4.9139204025268555,
        "learning_rate": 3.88959518621939e-05,
        "epoch": 1.1416,
        "step": 8562
    },
    {
        "loss": 2.3103,
        "grad_norm": 3.1498754024505615,
        "learning_rate": 3.884614327874393e-05,
        "epoch": 1.1417333333333333,
        "step": 8563
    },
    {
        "loss": 1.8504,
        "grad_norm": 3.8786377906799316,
        "learning_rate": 3.879635891854495e-05,
        "epoch": 1.1418666666666666,
        "step": 8564
    },
    {
        "loss": 2.1571,
        "grad_norm": 2.7470130920410156,
        "learning_rate": 3.8746598801316716e-05,
        "epoch": 1.142,
        "step": 8565
    },
    {
        "loss": 1.3076,
        "grad_norm": 4.095278739929199,
        "learning_rate": 3.8696862946769405e-05,
        "epoch": 1.1421333333333332,
        "step": 8566
    },
    {
        "loss": 1.7446,
        "grad_norm": 2.7204315662384033,
        "learning_rate": 3.864715137460357e-05,
        "epoch": 1.1422666666666668,
        "step": 8567
    },
    {
        "loss": 1.8946,
        "grad_norm": 2.500218152999878,
        "learning_rate": 3.859746410451024e-05,
        "epoch": 1.1424,
        "step": 8568
    },
    {
        "loss": 2.2354,
        "grad_norm": 3.7784762382507324,
        "learning_rate": 3.854780115617042e-05,
        "epoch": 1.1425333333333334,
        "step": 8569
    },
    {
        "loss": 1.9807,
        "grad_norm": 3.1633408069610596,
        "learning_rate": 3.8498162549256054e-05,
        "epoch": 1.1426666666666667,
        "step": 8570
    },
    {
        "loss": 1.6203,
        "grad_norm": 3.1008291244506836,
        "learning_rate": 3.844854830342901e-05,
        "epoch": 1.1428,
        "step": 8571
    },
    {
        "loss": 2.3718,
        "grad_norm": 3.3839776515960693,
        "learning_rate": 3.839895843834185e-05,
        "epoch": 1.1429333333333334,
        "step": 8572
    },
    {
        "loss": 2.8794,
        "grad_norm": 3.8513312339782715,
        "learning_rate": 3.834939297363701e-05,
        "epoch": 1.1430666666666667,
        "step": 8573
    },
    {
        "loss": 0.8934,
        "grad_norm": 4.5273613929748535,
        "learning_rate": 3.8299851928947716e-05,
        "epoch": 1.1432,
        "step": 8574
    },
    {
        "loss": 2.0795,
        "grad_norm": 3.2712817192077637,
        "learning_rate": 3.825033532389731e-05,
        "epoch": 1.1433333333333333,
        "step": 8575
    },
    {
        "loss": 1.6032,
        "grad_norm": 3.8152012825012207,
        "learning_rate": 3.82008431780995e-05,
        "epoch": 1.1434666666666666,
        "step": 8576
    },
    {
        "loss": 1.755,
        "grad_norm": 1.9231847524642944,
        "learning_rate": 3.815137551115837e-05,
        "epoch": 1.1436,
        "step": 8577
    },
    {
        "loss": 1.9856,
        "grad_norm": 4.846399784088135,
        "learning_rate": 3.810193234266798e-05,
        "epoch": 1.1437333333333333,
        "step": 8578
    },
    {
        "loss": 1.0943,
        "grad_norm": 5.345716953277588,
        "learning_rate": 3.8052513692213245e-05,
        "epoch": 1.1438666666666666,
        "step": 8579
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.8790247440338135,
        "learning_rate": 3.8003119579368775e-05,
        "epoch": 1.144,
        "step": 8580
    },
    {
        "loss": 1.7321,
        "grad_norm": 3.548936605453491,
        "learning_rate": 3.795375002370006e-05,
        "epoch": 1.1441333333333334,
        "step": 8581
    },
    {
        "loss": 2.0685,
        "grad_norm": 4.284091949462891,
        "learning_rate": 3.790440504476227e-05,
        "epoch": 1.1442666666666668,
        "step": 8582
    },
    {
        "loss": 2.0336,
        "grad_norm": 5.895968914031982,
        "learning_rate": 3.785508466210124e-05,
        "epoch": 1.1444,
        "step": 8583
    },
    {
        "loss": 1.1313,
        "grad_norm": 3.644416570663452,
        "learning_rate": 3.780578889525288e-05,
        "epoch": 1.1445333333333334,
        "step": 8584
    },
    {
        "loss": 0.6055,
        "grad_norm": 3.7073886394500732,
        "learning_rate": 3.775651776374357e-05,
        "epoch": 1.1446666666666667,
        "step": 8585
    },
    {
        "loss": 1.3189,
        "grad_norm": 5.500892162322998,
        "learning_rate": 3.770727128708963e-05,
        "epoch": 1.1448,
        "step": 8586
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.6438851356506348,
        "learning_rate": 3.7658049484797686e-05,
        "epoch": 1.1449333333333334,
        "step": 8587
    },
    {
        "loss": 2.5253,
        "grad_norm": 3.847952365875244,
        "learning_rate": 3.7608852376364915e-05,
        "epoch": 1.1450666666666667,
        "step": 8588
    },
    {
        "loss": 1.4576,
        "grad_norm": 2.786581039428711,
        "learning_rate": 3.755967998127825e-05,
        "epoch": 1.1452,
        "step": 8589
    },
    {
        "loss": 2.1365,
        "grad_norm": 3.5271780490875244,
        "learning_rate": 3.751053231901509e-05,
        "epoch": 1.1453333333333333,
        "step": 8590
    },
    {
        "loss": 0.6446,
        "grad_norm": 1.5536863803863525,
        "learning_rate": 3.7461409409043e-05,
        "epoch": 1.1454666666666666,
        "step": 8591
    },
    {
        "loss": 2.6678,
        "grad_norm": 2.205735683441162,
        "learning_rate": 3.741231127081974e-05,
        "epoch": 1.1456,
        "step": 8592
    },
    {
        "loss": 2.4557,
        "grad_norm": 2.8474955558776855,
        "learning_rate": 3.7363237923793346e-05,
        "epoch": 1.1457333333333333,
        "step": 8593
    },
    {
        "loss": 2.1763,
        "grad_norm": 3.793705463409424,
        "learning_rate": 3.731418938740168e-05,
        "epoch": 1.1458666666666666,
        "step": 8594
    },
    {
        "loss": 1.4387,
        "grad_norm": 3.491837739944458,
        "learning_rate": 3.726516568107329e-05,
        "epoch": 1.146,
        "step": 8595
    },
    {
        "loss": 2.5701,
        "grad_norm": 3.673363208770752,
        "learning_rate": 3.721616682422646e-05,
        "epoch": 1.1461333333333332,
        "step": 8596
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.2864887714385986,
        "learning_rate": 3.716719283626997e-05,
        "epoch": 1.1462666666666665,
        "step": 8597
    },
    {
        "loss": 1.8049,
        "grad_norm": 3.8554608821868896,
        "learning_rate": 3.711824373660251e-05,
        "epoch": 1.1464,
        "step": 8598
    },
    {
        "loss": 3.0547,
        "grad_norm": 2.7685229778289795,
        "learning_rate": 3.706931954461288e-05,
        "epoch": 1.1465333333333334,
        "step": 8599
    },
    {
        "loss": 1.3331,
        "grad_norm": 4.236363410949707,
        "learning_rate": 3.7020420279680234e-05,
        "epoch": 1.1466666666666667,
        "step": 8600
    },
    {
        "loss": 2.064,
        "grad_norm": 3.2857964038848877,
        "learning_rate": 3.697154596117373e-05,
        "epoch": 1.1468,
        "step": 8601
    },
    {
        "loss": 1.4641,
        "grad_norm": 10.120918273925781,
        "learning_rate": 3.692269660845272e-05,
        "epoch": 1.1469333333333334,
        "step": 8602
    },
    {
        "loss": 1.6097,
        "grad_norm": 4.6763763427734375,
        "learning_rate": 3.6873872240866344e-05,
        "epoch": 1.1470666666666667,
        "step": 8603
    },
    {
        "loss": 2.0849,
        "grad_norm": 3.2308285236358643,
        "learning_rate": 3.6825072877754416e-05,
        "epoch": 1.1472,
        "step": 8604
    },
    {
        "loss": 2.4704,
        "grad_norm": 3.738058567047119,
        "learning_rate": 3.6776298538446244e-05,
        "epoch": 1.1473333333333333,
        "step": 8605
    },
    {
        "loss": 2.2944,
        "grad_norm": 3.015584945678711,
        "learning_rate": 3.6727549242261815e-05,
        "epoch": 1.1474666666666666,
        "step": 8606
    },
    {
        "loss": 1.0032,
        "grad_norm": 4.289825439453125,
        "learning_rate": 3.667882500851063e-05,
        "epoch": 1.1476,
        "step": 8607
    },
    {
        "loss": 2.3489,
        "grad_norm": 1.4969282150268555,
        "learning_rate": 3.663012585649263e-05,
        "epoch": 1.1477333333333333,
        "step": 8608
    },
    {
        "loss": 2.627,
        "grad_norm": 3.182321548461914,
        "learning_rate": 3.658145180549766e-05,
        "epoch": 1.1478666666666666,
        "step": 8609
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.7285094261169434,
        "learning_rate": 3.6532802874805825e-05,
        "epoch": 1.148,
        "step": 8610
    },
    {
        "loss": 1.9703,
        "grad_norm": 3.3268094062805176,
        "learning_rate": 3.6484179083687e-05,
        "epoch": 1.1481333333333335,
        "step": 8611
    },
    {
        "loss": 1.0975,
        "grad_norm": 4.985671520233154,
        "learning_rate": 3.64355804514012e-05,
        "epoch": 1.1482666666666668,
        "step": 8612
    },
    {
        "loss": 1.7987,
        "grad_norm": 4.574925899505615,
        "learning_rate": 3.638700699719863e-05,
        "epoch": 1.1484,
        "step": 8613
    },
    {
        "loss": 1.7411,
        "grad_norm": 5.263576030731201,
        "learning_rate": 3.633845874031936e-05,
        "epoch": 1.1485333333333334,
        "step": 8614
    },
    {
        "loss": 1.9341,
        "grad_norm": 2.5250415802001953,
        "learning_rate": 3.628993569999346e-05,
        "epoch": 1.1486666666666667,
        "step": 8615
    },
    {
        "loss": 2.6387,
        "grad_norm": 3.0590264797210693,
        "learning_rate": 3.624143789544112e-05,
        "epoch": 1.1488,
        "step": 8616
    },
    {
        "loss": 2.2389,
        "grad_norm": 3.1102705001831055,
        "learning_rate": 3.6192965345872475e-05,
        "epoch": 1.1489333333333334,
        "step": 8617
    },
    {
        "loss": 1.2865,
        "grad_norm": 3.236236572265625,
        "learning_rate": 3.6144518070487685e-05,
        "epoch": 1.1490666666666667,
        "step": 8618
    },
    {
        "loss": 2.323,
        "grad_norm": 1.7687549591064453,
        "learning_rate": 3.609609608847689e-05,
        "epoch": 1.1492,
        "step": 8619
    },
    {
        "loss": 1.7166,
        "grad_norm": 3.0187525749206543,
        "learning_rate": 3.60476994190202e-05,
        "epoch": 1.1493333333333333,
        "step": 8620
    },
    {
        "loss": 2.2481,
        "grad_norm": 2.947866439819336,
        "learning_rate": 3.59993280812876e-05,
        "epoch": 1.1494666666666666,
        "step": 8621
    },
    {
        "loss": 0.8615,
        "grad_norm": 4.308549880981445,
        "learning_rate": 3.595098209443929e-05,
        "epoch": 1.1496,
        "step": 8622
    },
    {
        "loss": 1.3497,
        "grad_norm": 4.23866605758667,
        "learning_rate": 3.5902661477625254e-05,
        "epoch": 1.1497333333333333,
        "step": 8623
    },
    {
        "loss": 1.6086,
        "grad_norm": 3.317657470703125,
        "learning_rate": 3.5854366249985315e-05,
        "epoch": 1.1498666666666666,
        "step": 8624
    },
    {
        "loss": 1.3411,
        "grad_norm": 5.225331783294678,
        "learning_rate": 3.5806096430649506e-05,
        "epoch": 1.15,
        "step": 8625
    },
    {
        "loss": 2.9175,
        "grad_norm": 4.84995174407959,
        "learning_rate": 3.575785203873763e-05,
        "epoch": 1.1501333333333332,
        "step": 8626
    },
    {
        "loss": 1.4171,
        "grad_norm": 3.9304394721984863,
        "learning_rate": 3.570963309335956e-05,
        "epoch": 1.1502666666666665,
        "step": 8627
    },
    {
        "loss": 2.0167,
        "grad_norm": 3.281555414199829,
        "learning_rate": 3.566143961361473e-05,
        "epoch": 1.1504,
        "step": 8628
    },
    {
        "loss": 2.7068,
        "grad_norm": 3.2358310222625732,
        "learning_rate": 3.561327161859303e-05,
        "epoch": 1.1505333333333334,
        "step": 8629
    },
    {
        "loss": 2.1313,
        "grad_norm": 3.2028090953826904,
        "learning_rate": 3.55651291273737e-05,
        "epoch": 1.1506666666666667,
        "step": 8630
    },
    {
        "loss": 2.6946,
        "grad_norm": 2.9091877937316895,
        "learning_rate": 3.551701215902644e-05,
        "epoch": 1.1508,
        "step": 8631
    },
    {
        "loss": 0.6005,
        "grad_norm": 2.7517683506011963,
        "learning_rate": 3.546892073261028e-05,
        "epoch": 1.1509333333333334,
        "step": 8632
    },
    {
        "loss": 1.912,
        "grad_norm": 3.034862518310547,
        "learning_rate": 3.5420854867174516e-05,
        "epoch": 1.1510666666666667,
        "step": 8633
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.8496716022491455,
        "learning_rate": 3.537281458175814e-05,
        "epoch": 1.1512,
        "step": 8634
    },
    {
        "loss": 2.5961,
        "grad_norm": 3.6085169315338135,
        "learning_rate": 3.532479989539023e-05,
        "epoch": 1.1513333333333333,
        "step": 8635
    },
    {
        "loss": 2.363,
        "grad_norm": 3.859813928604126,
        "learning_rate": 3.527681082708942e-05,
        "epoch": 1.1514666666666666,
        "step": 8636
    },
    {
        "loss": 2.2475,
        "grad_norm": 3.167377471923828,
        "learning_rate": 3.52288473958643e-05,
        "epoch": 1.1516,
        "step": 8637
    },
    {
        "loss": 1.5603,
        "grad_norm": 4.368274211883545,
        "learning_rate": 3.518090962071352e-05,
        "epoch": 1.1517333333333333,
        "step": 8638
    },
    {
        "loss": 1.9169,
        "grad_norm": 3.9973304271698,
        "learning_rate": 3.5132997520625185e-05,
        "epoch": 1.1518666666666666,
        "step": 8639
    },
    {
        "loss": 2.567,
        "grad_norm": 1.7205513715744019,
        "learning_rate": 3.508511111457775e-05,
        "epoch": 1.152,
        "step": 8640
    },
    {
        "loss": 2.575,
        "grad_norm": 2.435908794403076,
        "learning_rate": 3.503725042153888e-05,
        "epoch": 1.1521333333333335,
        "step": 8641
    },
    {
        "loss": 2.0346,
        "grad_norm": 3.0895402431488037,
        "learning_rate": 3.4989415460466465e-05,
        "epoch": 1.1522666666666668,
        "step": 8642
    },
    {
        "loss": 2.5006,
        "grad_norm": 2.58428955078125,
        "learning_rate": 3.494160625030812e-05,
        "epoch": 1.1524,
        "step": 8643
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.6655585765838623,
        "learning_rate": 3.4893822810001244e-05,
        "epoch": 1.1525333333333334,
        "step": 8644
    },
    {
        "loss": 2.4871,
        "grad_norm": 2.827909231185913,
        "learning_rate": 3.484606515847304e-05,
        "epoch": 1.1526666666666667,
        "step": 8645
    },
    {
        "loss": 2.9466,
        "grad_norm": 2.065013885498047,
        "learning_rate": 3.479833331464029e-05,
        "epoch": 1.1528,
        "step": 8646
    },
    {
        "loss": 1.7337,
        "grad_norm": 4.187771320343018,
        "learning_rate": 3.475062729740999e-05,
        "epoch": 1.1529333333333334,
        "step": 8647
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.260842800140381,
        "learning_rate": 3.470294712567857e-05,
        "epoch": 1.1530666666666667,
        "step": 8648
    },
    {
        "loss": 2.3132,
        "grad_norm": 2.8699443340301514,
        "learning_rate": 3.4655292818332196e-05,
        "epoch": 1.1532,
        "step": 8649
    },
    {
        "loss": 1.6609,
        "grad_norm": 3.622114896774292,
        "learning_rate": 3.460766439424703e-05,
        "epoch": 1.1533333333333333,
        "step": 8650
    },
    {
        "loss": 1.9319,
        "grad_norm": 3.658853530883789,
        "learning_rate": 3.4560061872288815e-05,
        "epoch": 1.1534666666666666,
        "step": 8651
    },
    {
        "loss": 0.6617,
        "grad_norm": 3.5720531940460205,
        "learning_rate": 3.4512485271313175e-05,
        "epoch": 1.1536,
        "step": 8652
    },
    {
        "loss": 1.0302,
        "grad_norm": 3.811270236968994,
        "learning_rate": 3.446493461016509e-05,
        "epoch": 1.1537333333333333,
        "step": 8653
    },
    {
        "loss": 2.3789,
        "grad_norm": 3.2843198776245117,
        "learning_rate": 3.4417409907679854e-05,
        "epoch": 1.1538666666666666,
        "step": 8654
    },
    {
        "loss": 2.6305,
        "grad_norm": 3.845851421356201,
        "learning_rate": 3.436991118268188e-05,
        "epoch": 1.154,
        "step": 8655
    },
    {
        "loss": 1.6873,
        "grad_norm": 2.2084314823150635,
        "learning_rate": 3.432243845398588e-05,
        "epoch": 1.1541333333333332,
        "step": 8656
    },
    {
        "loss": 2.191,
        "grad_norm": 3.626570701599121,
        "learning_rate": 3.42749917403957e-05,
        "epoch": 1.1542666666666666,
        "step": 8657
    },
    {
        "loss": 1.8045,
        "grad_norm": 3.3030056953430176,
        "learning_rate": 3.422757106070526e-05,
        "epoch": 1.1544,
        "step": 8658
    },
    {
        "loss": 2.1081,
        "grad_norm": 2.2387747764587402,
        "learning_rate": 3.418017643369801e-05,
        "epoch": 1.1545333333333334,
        "step": 8659
    },
    {
        "loss": 1.607,
        "grad_norm": 4.859073638916016,
        "learning_rate": 3.4132807878147136e-05,
        "epoch": 1.1546666666666667,
        "step": 8660
    },
    {
        "loss": 2.2515,
        "grad_norm": 3.093675374984741,
        "learning_rate": 3.4085465412815624e-05,
        "epoch": 1.1548,
        "step": 8661
    },
    {
        "loss": 1.3312,
        "grad_norm": 3.9442834854125977,
        "learning_rate": 3.4038149056455724e-05,
        "epoch": 1.1549333333333334,
        "step": 8662
    },
    {
        "loss": 2.0314,
        "grad_norm": 2.9146196842193604,
        "learning_rate": 3.399085882780981e-05,
        "epoch": 1.1550666666666667,
        "step": 8663
    },
    {
        "loss": 1.3502,
        "grad_norm": 4.062119960784912,
        "learning_rate": 3.394359474560954e-05,
        "epoch": 1.1552,
        "step": 8664
    },
    {
        "loss": 3.1102,
        "grad_norm": 3.590423583984375,
        "learning_rate": 3.389635682857665e-05,
        "epoch": 1.1553333333333333,
        "step": 8665
    },
    {
        "loss": 1.7208,
        "grad_norm": 2.7205073833465576,
        "learning_rate": 3.384914509542194e-05,
        "epoch": 1.1554666666666666,
        "step": 8666
    },
    {
        "loss": 2.2849,
        "grad_norm": 2.9544825553894043,
        "learning_rate": 3.380195956484627e-05,
        "epoch": 1.1556,
        "step": 8667
    },
    {
        "loss": 2.7175,
        "grad_norm": 2.374835252761841,
        "learning_rate": 3.3754800255539985e-05,
        "epoch": 1.1557333333333333,
        "step": 8668
    },
    {
        "loss": 1.7167,
        "grad_norm": 4.6995038986206055,
        "learning_rate": 3.370766718618308e-05,
        "epoch": 1.1558666666666666,
        "step": 8669
    },
    {
        "loss": 2.4747,
        "grad_norm": 3.242227554321289,
        "learning_rate": 3.3660560375445116e-05,
        "epoch": 1.156,
        "step": 8670
    },
    {
        "loss": 1.9447,
        "grad_norm": 4.965532302856445,
        "learning_rate": 3.361347984198511e-05,
        "epoch": 1.1561333333333335,
        "step": 8671
    },
    {
        "loss": 1.9565,
        "grad_norm": 4.270812034606934,
        "learning_rate": 3.3566425604452035e-05,
        "epoch": 1.1562666666666668,
        "step": 8672
    },
    {
        "loss": 1.9118,
        "grad_norm": 4.018158435821533,
        "learning_rate": 3.3519397681484174e-05,
        "epoch": 1.1564,
        "step": 8673
    },
    {
        "loss": 0.8092,
        "grad_norm": 3.4834916591644287,
        "learning_rate": 3.3472396091709325e-05,
        "epoch": 1.1565333333333334,
        "step": 8674
    },
    {
        "loss": 0.6961,
        "grad_norm": 3.054853916168213,
        "learning_rate": 3.34254208537451e-05,
        "epoch": 1.1566666666666667,
        "step": 8675
    },
    {
        "loss": 0.483,
        "grad_norm": 2.3875279426574707,
        "learning_rate": 3.337847198619852e-05,
        "epoch": 1.1568,
        "step": 8676
    },
    {
        "loss": 2.1273,
        "grad_norm": 3.554877519607544,
        "learning_rate": 3.333154950766621e-05,
        "epoch": 1.1569333333333334,
        "step": 8677
    },
    {
        "loss": 2.186,
        "grad_norm": 2.913691520690918,
        "learning_rate": 3.328465343673432e-05,
        "epoch": 1.1570666666666667,
        "step": 8678
    },
    {
        "loss": 1.5513,
        "grad_norm": 2.9845082759857178,
        "learning_rate": 3.323778379197857e-05,
        "epoch": 1.1572,
        "step": 8679
    },
    {
        "loss": 1.2247,
        "grad_norm": 3.174485445022583,
        "learning_rate": 3.319094059196403e-05,
        "epoch": 1.1573333333333333,
        "step": 8680
    },
    {
        "loss": 2.3409,
        "grad_norm": 4.331005096435547,
        "learning_rate": 3.3144123855245646e-05,
        "epoch": 1.1574666666666666,
        "step": 8681
    },
    {
        "loss": 2.9625,
        "grad_norm": 2.1714305877685547,
        "learning_rate": 3.309733360036765e-05,
        "epoch": 1.1576,
        "step": 8682
    },
    {
        "loss": 0.9939,
        "grad_norm": 3.7791597843170166,
        "learning_rate": 3.3050569845863723e-05,
        "epoch": 1.1577333333333333,
        "step": 8683
    },
    {
        "loss": 2.1437,
        "grad_norm": 2.7292966842651367,
        "learning_rate": 3.300383261025719e-05,
        "epoch": 1.1578666666666666,
        "step": 8684
    },
    {
        "loss": 2.3494,
        "grad_norm": 4.049195766448975,
        "learning_rate": 3.295712191206082e-05,
        "epoch": 1.158,
        "step": 8685
    },
    {
        "loss": 2.12,
        "grad_norm": 3.1145236492156982,
        "learning_rate": 3.291043776977703e-05,
        "epoch": 1.1581333333333332,
        "step": 8686
    },
    {
        "loss": 1.5032,
        "grad_norm": 2.9264495372772217,
        "learning_rate": 3.286378020189731e-05,
        "epoch": 1.1582666666666666,
        "step": 8687
    },
    {
        "loss": 1.8184,
        "grad_norm": 3.040733814239502,
        "learning_rate": 3.2817149226903086e-05,
        "epoch": 1.1584,
        "step": 8688
    },
    {
        "loss": 2.3081,
        "grad_norm": 2.705129384994507,
        "learning_rate": 3.2770544863264864e-05,
        "epoch": 1.1585333333333334,
        "step": 8689
    },
    {
        "loss": 2.1883,
        "grad_norm": 4.652054309844971,
        "learning_rate": 3.272396712944307e-05,
        "epoch": 1.1586666666666667,
        "step": 8690
    },
    {
        "loss": 3.3176,
        "grad_norm": 3.6824734210968018,
        "learning_rate": 3.267741604388703e-05,
        "epoch": 1.1588,
        "step": 8691
    },
    {
        "loss": 2.3184,
        "grad_norm": 2.879274368286133,
        "learning_rate": 3.2630891625035907e-05,
        "epoch": 1.1589333333333334,
        "step": 8692
    },
    {
        "loss": 1.4559,
        "grad_norm": 4.434454917907715,
        "learning_rate": 3.2584393891318154e-05,
        "epoch": 1.1590666666666667,
        "step": 8693
    },
    {
        "loss": 2.1624,
        "grad_norm": 2.6171786785125732,
        "learning_rate": 3.2537922861151725e-05,
        "epoch": 1.1592,
        "step": 8694
    },
    {
        "loss": 0.8706,
        "grad_norm": 3.129267454147339,
        "learning_rate": 3.2491478552943954e-05,
        "epoch": 1.1593333333333333,
        "step": 8695
    },
    {
        "loss": 2.1805,
        "grad_norm": 3.8778460025787354,
        "learning_rate": 3.244506098509143e-05,
        "epoch": 1.1594666666666666,
        "step": 8696
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.7715539932250977,
        "learning_rate": 3.239867017598054e-05,
        "epoch": 1.1596,
        "step": 8697
    },
    {
        "loss": 1.8444,
        "grad_norm": 2.9427623748779297,
        "learning_rate": 3.2352306143986665e-05,
        "epoch": 1.1597333333333333,
        "step": 8698
    },
    {
        "loss": 2.1696,
        "grad_norm": 2.2475996017456055,
        "learning_rate": 3.230596890747497e-05,
        "epoch": 1.1598666666666666,
        "step": 8699
    },
    {
        "loss": 2.6173,
        "grad_norm": 3.057300329208374,
        "learning_rate": 3.2259658484799515e-05,
        "epoch": 1.16,
        "step": 8700
    },
    {
        "loss": 2.3657,
        "grad_norm": 3.2641422748565674,
        "learning_rate": 3.221337489430419e-05,
        "epoch": 1.1601333333333332,
        "step": 8701
    },
    {
        "loss": 1.7178,
        "grad_norm": 4.198509216308594,
        "learning_rate": 3.216711815432205e-05,
        "epoch": 1.1602666666666668,
        "step": 8702
    },
    {
        "loss": 1.9138,
        "grad_norm": 5.768178462982178,
        "learning_rate": 3.212088828317556e-05,
        "epoch": 1.1604,
        "step": 8703
    },
    {
        "loss": 2.5165,
        "grad_norm": 4.558216094970703,
        "learning_rate": 3.207468529917657e-05,
        "epoch": 1.1605333333333334,
        "step": 8704
    },
    {
        "loss": 2.0102,
        "grad_norm": 3.3114473819732666,
        "learning_rate": 3.202850922062603e-05,
        "epoch": 1.1606666666666667,
        "step": 8705
    },
    {
        "loss": 2.8665,
        "grad_norm": 2.7868990898132324,
        "learning_rate": 3.198236006581474e-05,
        "epoch": 1.1608,
        "step": 8706
    },
    {
        "loss": 2.5314,
        "grad_norm": 3.6786012649536133,
        "learning_rate": 3.193623785302237e-05,
        "epoch": 1.1609333333333334,
        "step": 8707
    },
    {
        "loss": 1.6957,
        "grad_norm": 1.9857503175735474,
        "learning_rate": 3.189014260051806e-05,
        "epoch": 1.1610666666666667,
        "step": 8708
    },
    {
        "loss": 1.59,
        "grad_norm": 1.5904450416564941,
        "learning_rate": 3.184407432656034e-05,
        "epoch": 1.1612,
        "step": 8709
    },
    {
        "loss": 2.1377,
        "grad_norm": 4.0210041999816895,
        "learning_rate": 3.179803304939699e-05,
        "epoch": 1.1613333333333333,
        "step": 8710
    },
    {
        "loss": 1.367,
        "grad_norm": 3.3444936275482178,
        "learning_rate": 3.175201878726527e-05,
        "epoch": 1.1614666666666666,
        "step": 8711
    },
    {
        "loss": 2.2933,
        "grad_norm": 2.7888355255126953,
        "learning_rate": 3.170603155839133e-05,
        "epoch": 1.1616,
        "step": 8712
    },
    {
        "loss": 1.7581,
        "grad_norm": 2.311133623123169,
        "learning_rate": 3.1660071380991055e-05,
        "epoch": 1.1617333333333333,
        "step": 8713
    },
    {
        "loss": 1.7314,
        "grad_norm": 3.480464458465576,
        "learning_rate": 3.1614138273269265e-05,
        "epoch": 1.1618666666666666,
        "step": 8714
    },
    {
        "loss": 2.5163,
        "grad_norm": 2.279773712158203,
        "learning_rate": 3.15682322534205e-05,
        "epoch": 1.162,
        "step": 8715
    },
    {
        "loss": 2.21,
        "grad_norm": 3.2851293087005615,
        "learning_rate": 3.152235333962802e-05,
        "epoch": 1.1621333333333332,
        "step": 8716
    },
    {
        "loss": 2.1342,
        "grad_norm": 2.4785115718841553,
        "learning_rate": 3.1476501550064706e-05,
        "epoch": 1.1622666666666666,
        "step": 8717
    },
    {
        "loss": 2.6423,
        "grad_norm": 5.273868083953857,
        "learning_rate": 3.143067690289262e-05,
        "epoch": 1.1623999999999999,
        "step": 8718
    },
    {
        "loss": 1.6386,
        "grad_norm": 4.6583147048950195,
        "learning_rate": 3.138487941626305e-05,
        "epoch": 1.1625333333333334,
        "step": 8719
    },
    {
        "loss": 2.3419,
        "grad_norm": 2.7780230045318604,
        "learning_rate": 3.1339109108316646e-05,
        "epoch": 1.1626666666666667,
        "step": 8720
    },
    {
        "loss": 1.9542,
        "grad_norm": 3.5706255435943604,
        "learning_rate": 3.129336599718291e-05,
        "epoch": 1.1628,
        "step": 8721
    },
    {
        "loss": 2.2002,
        "grad_norm": 4.559468746185303,
        "learning_rate": 3.124765010098113e-05,
        "epoch": 1.1629333333333334,
        "step": 8722
    },
    {
        "loss": 1.2049,
        "grad_norm": 4.671762466430664,
        "learning_rate": 3.120196143781934e-05,
        "epoch": 1.1630666666666667,
        "step": 8723
    },
    {
        "loss": 1.6379,
        "grad_norm": 4.223835468292236,
        "learning_rate": 3.1156300025795174e-05,
        "epoch": 1.1632,
        "step": 8724
    },
    {
        "loss": 2.834,
        "grad_norm": 4.0346360206604,
        "learning_rate": 3.1110665882995016e-05,
        "epoch": 1.1633333333333333,
        "step": 8725
    },
    {
        "loss": 1.7372,
        "grad_norm": 3.227675437927246,
        "learning_rate": 3.106505902749487e-05,
        "epoch": 1.1634666666666666,
        "step": 8726
    },
    {
        "loss": 1.97,
        "grad_norm": 2.9163455963134766,
        "learning_rate": 3.1019479477359735e-05,
        "epoch": 1.1636,
        "step": 8727
    },
    {
        "loss": 3.0203,
        "grad_norm": 2.4141178131103516,
        "learning_rate": 3.097392725064385e-05,
        "epoch": 1.1637333333333333,
        "step": 8728
    },
    {
        "loss": 0.9092,
        "grad_norm": 3.740098237991333,
        "learning_rate": 3.092840236539062e-05,
        "epoch": 1.1638666666666666,
        "step": 8729
    },
    {
        "loss": 1.8452,
        "grad_norm": 2.9613535404205322,
        "learning_rate": 3.088290483963244e-05,
        "epoch": 1.164,
        "step": 8730
    },
    {
        "loss": 2.4824,
        "grad_norm": 3.034944534301758,
        "learning_rate": 3.08374346913913e-05,
        "epoch": 1.1641333333333332,
        "step": 8731
    },
    {
        "loss": 0.6548,
        "grad_norm": 3.663848876953125,
        "learning_rate": 3.07919919386779e-05,
        "epoch": 1.1642666666666668,
        "step": 8732
    },
    {
        "loss": 2.2221,
        "grad_norm": 2.918259620666504,
        "learning_rate": 3.074657659949228e-05,
        "epoch": 1.1644,
        "step": 8733
    },
    {
        "loss": 2.2042,
        "grad_norm": 2.9787731170654297,
        "learning_rate": 3.070118869182366e-05,
        "epoch": 1.1645333333333334,
        "step": 8734
    },
    {
        "loss": 2.3591,
        "grad_norm": 3.6980857849121094,
        "learning_rate": 3.0655828233650287e-05,
        "epoch": 1.1646666666666667,
        "step": 8735
    },
    {
        "loss": 2.4284,
        "grad_norm": 2.589900016784668,
        "learning_rate": 3.061049524293976e-05,
        "epoch": 1.1648,
        "step": 8736
    },
    {
        "loss": 2.3572,
        "grad_norm": 2.9136855602264404,
        "learning_rate": 3.056518973764838e-05,
        "epoch": 1.1649333333333334,
        "step": 8737
    },
    {
        "loss": 2.0229,
        "grad_norm": 4.378079891204834,
        "learning_rate": 3.0519911735722005e-05,
        "epoch": 1.1650666666666667,
        "step": 8738
    },
    {
        "loss": 2.2565,
        "grad_norm": 3.3030102252960205,
        "learning_rate": 3.047466125509525e-05,
        "epoch": 1.1652,
        "step": 8739
    },
    {
        "loss": 2.1524,
        "grad_norm": 2.9960036277770996,
        "learning_rate": 3.0429438313692192e-05,
        "epoch": 1.1653333333333333,
        "step": 8740
    },
    {
        "loss": 0.8965,
        "grad_norm": 4.1645588874816895,
        "learning_rate": 3.0384242929425642e-05,
        "epoch": 1.1654666666666667,
        "step": 8741
    },
    {
        "loss": 1.1108,
        "grad_norm": 5.628846645355225,
        "learning_rate": 3.0339075120197645e-05,
        "epoch": 1.1656,
        "step": 8742
    },
    {
        "loss": 2.2443,
        "grad_norm": 2.323345184326172,
        "learning_rate": 3.0293934903899356e-05,
        "epoch": 1.1657333333333333,
        "step": 8743
    },
    {
        "loss": 1.5618,
        "grad_norm": 3.3068687915802,
        "learning_rate": 3.0248822298410972e-05,
        "epoch": 1.1658666666666666,
        "step": 8744
    },
    {
        "loss": 0.9483,
        "grad_norm": 2.7009804248809814,
        "learning_rate": 3.0203737321601844e-05,
        "epoch": 1.166,
        "step": 8745
    },
    {
        "loss": 1.4325,
        "grad_norm": 3.2280473709106445,
        "learning_rate": 3.015867999133003e-05,
        "epoch": 1.1661333333333332,
        "step": 8746
    },
    {
        "loss": 2.4511,
        "grad_norm": 4.8192949295043945,
        "learning_rate": 3.0113650325443143e-05,
        "epoch": 1.1662666666666666,
        "step": 8747
    },
    {
        "loss": 1.7646,
        "grad_norm": 5.563431262969971,
        "learning_rate": 3.006864834177743e-05,
        "epoch": 1.1663999999999999,
        "step": 8748
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.7114977836608887,
        "learning_rate": 3.0023674058158523e-05,
        "epoch": 1.1665333333333334,
        "step": 8749
    },
    {
        "loss": 1.8311,
        "grad_norm": 8.0874605178833,
        "learning_rate": 2.9978727492400627e-05,
        "epoch": 1.1666666666666667,
        "step": 8750
    },
    {
        "loss": 0.9186,
        "grad_norm": 3.677819013595581,
        "learning_rate": 2.9933808662307394e-05,
        "epoch": 1.1668,
        "step": 8751
    },
    {
        "loss": 2.0943,
        "grad_norm": 4.10616397857666,
        "learning_rate": 2.9888917585671307e-05,
        "epoch": 1.1669333333333334,
        "step": 8752
    },
    {
        "loss": 2.2963,
        "grad_norm": 2.826908826828003,
        "learning_rate": 2.9844054280273914e-05,
        "epoch": 1.1670666666666667,
        "step": 8753
    },
    {
        "loss": 2.7071,
        "grad_norm": 2.160243034362793,
        "learning_rate": 2.979921876388564e-05,
        "epoch": 1.1672,
        "step": 8754
    },
    {
        "loss": 1.5679,
        "grad_norm": 2.3113508224487305,
        "learning_rate": 2.975441105426593e-05,
        "epoch": 1.1673333333333333,
        "step": 8755
    },
    {
        "loss": 2.7653,
        "grad_norm": 2.465895891189575,
        "learning_rate": 2.9709631169163477e-05,
        "epoch": 1.1674666666666667,
        "step": 8756
    },
    {
        "loss": 2.3542,
        "grad_norm": 2.855379343032837,
        "learning_rate": 2.966487912631559e-05,
        "epoch": 1.1676,
        "step": 8757
    },
    {
        "loss": 2.1883,
        "grad_norm": 2.8589770793914795,
        "learning_rate": 2.962015494344873e-05,
        "epoch": 1.1677333333333333,
        "step": 8758
    },
    {
        "loss": 2.1716,
        "grad_norm": 2.600416898727417,
        "learning_rate": 2.9575458638278296e-05,
        "epoch": 1.1678666666666666,
        "step": 8759
    },
    {
        "loss": 1.8156,
        "grad_norm": 3.6755239963531494,
        "learning_rate": 2.9530790228508664e-05,
        "epoch": 1.168,
        "step": 8760
    },
    {
        "loss": 1.959,
        "grad_norm": 4.360110759735107,
        "learning_rate": 2.94861497318331e-05,
        "epoch": 1.1681333333333332,
        "step": 8761
    },
    {
        "loss": 2.555,
        "grad_norm": 2.963909387588501,
        "learning_rate": 2.944153716593401e-05,
        "epoch": 1.1682666666666668,
        "step": 8762
    },
    {
        "loss": 2.4027,
        "grad_norm": 3.233592987060547,
        "learning_rate": 2.9396952548482424e-05,
        "epoch": 1.1684,
        "step": 8763
    },
    {
        "loss": 1.785,
        "grad_norm": 3.53755784034729,
        "learning_rate": 2.935239589713842e-05,
        "epoch": 1.1685333333333334,
        "step": 8764
    },
    {
        "loss": 1.7875,
        "grad_norm": 3.584902048110962,
        "learning_rate": 2.9307867229551245e-05,
        "epoch": 1.1686666666666667,
        "step": 8765
    },
    {
        "loss": 1.8213,
        "grad_norm": 3.398123025894165,
        "learning_rate": 2.9263366563358708e-05,
        "epoch": 1.1688,
        "step": 8766
    },
    {
        "loss": 2.1559,
        "grad_norm": 3.8679959774017334,
        "learning_rate": 2.9218893916187685e-05,
        "epoch": 1.1689333333333334,
        "step": 8767
    },
    {
        "loss": 1.6048,
        "grad_norm": 3.720996379852295,
        "learning_rate": 2.917444930565395e-05,
        "epoch": 1.1690666666666667,
        "step": 8768
    },
    {
        "loss": 2.9116,
        "grad_norm": 3.1710550785064697,
        "learning_rate": 2.913003274936218e-05,
        "epoch": 1.1692,
        "step": 8769
    },
    {
        "loss": 2.0179,
        "grad_norm": 3.7758524417877197,
        "learning_rate": 2.908564426490602e-05,
        "epoch": 1.1693333333333333,
        "step": 8770
    },
    {
        "loss": 2.2828,
        "grad_norm": 4.94971227645874,
        "learning_rate": 2.904128386986764e-05,
        "epoch": 1.1694666666666667,
        "step": 8771
    },
    {
        "loss": 2.6018,
        "grad_norm": 4.8567705154418945,
        "learning_rate": 2.89969515818186e-05,
        "epoch": 1.1696,
        "step": 8772
    },
    {
        "loss": 1.5543,
        "grad_norm": 2.279827117919922,
        "learning_rate": 2.8952647418318913e-05,
        "epoch": 1.1697333333333333,
        "step": 8773
    },
    {
        "loss": 2.2542,
        "grad_norm": 2.1508889198303223,
        "learning_rate": 2.89083713969178e-05,
        "epoch": 1.1698666666666666,
        "step": 8774
    },
    {
        "loss": 1.9957,
        "grad_norm": 3.240712881088257,
        "learning_rate": 2.886412353515289e-05,
        "epoch": 1.17,
        "step": 8775
    },
    {
        "loss": 1.8484,
        "grad_norm": 3.3223986625671387,
        "learning_rate": 2.8819903850551068e-05,
        "epoch": 1.1701333333333332,
        "step": 8776
    },
    {
        "loss": 1.5088,
        "grad_norm": 3.5607731342315674,
        "learning_rate": 2.877571236062787e-05,
        "epoch": 1.1702666666666666,
        "step": 8777
    },
    {
        "loss": 2.4214,
        "grad_norm": 3.0871994495391846,
        "learning_rate": 2.8731549082887688e-05,
        "epoch": 1.1703999999999999,
        "step": 8778
    },
    {
        "loss": 1.6409,
        "grad_norm": 3.2615039348602295,
        "learning_rate": 2.8687414034823856e-05,
        "epoch": 1.1705333333333334,
        "step": 8779
    },
    {
        "loss": 2.5157,
        "grad_norm": 4.0256781578063965,
        "learning_rate": 2.864330723391817e-05,
        "epoch": 1.1706666666666667,
        "step": 8780
    },
    {
        "loss": 1.2323,
        "grad_norm": 3.005878210067749,
        "learning_rate": 2.8599228697641745e-05,
        "epoch": 1.1708,
        "step": 8781
    },
    {
        "loss": 2.7376,
        "grad_norm": 3.520591974258423,
        "learning_rate": 2.8555178443454e-05,
        "epoch": 1.1709333333333334,
        "step": 8782
    },
    {
        "loss": 1.6395,
        "grad_norm": 3.0973150730133057,
        "learning_rate": 2.851115648880368e-05,
        "epoch": 1.1710666666666667,
        "step": 8783
    },
    {
        "loss": 2.1851,
        "grad_norm": 3.3140578269958496,
        "learning_rate": 2.846716285112776e-05,
        "epoch": 1.1712,
        "step": 8784
    },
    {
        "loss": 1.0616,
        "grad_norm": 3.6971797943115234,
        "learning_rate": 2.8423197547852387e-05,
        "epoch": 1.1713333333333333,
        "step": 8785
    },
    {
        "loss": 1.8568,
        "grad_norm": 3.342836618423462,
        "learning_rate": 2.8379260596392322e-05,
        "epoch": 1.1714666666666667,
        "step": 8786
    },
    {
        "loss": 2.2329,
        "grad_norm": 3.787961959838867,
        "learning_rate": 2.8335352014151273e-05,
        "epoch": 1.1716,
        "step": 8787
    },
    {
        "loss": 1.6852,
        "grad_norm": 3.130446195602417,
        "learning_rate": 2.8291471818521455e-05,
        "epoch": 1.1717333333333333,
        "step": 8788
    },
    {
        "loss": 2.4206,
        "grad_norm": 3.2380259037017822,
        "learning_rate": 2.8247620026883892e-05,
        "epoch": 1.1718666666666666,
        "step": 8789
    },
    {
        "loss": 2.1443,
        "grad_norm": 3.6548001766204834,
        "learning_rate": 2.820379665660865e-05,
        "epoch": 1.172,
        "step": 8790
    },
    {
        "loss": 1.4319,
        "grad_norm": 2.423145294189453,
        "learning_rate": 2.8160001725054163e-05,
        "epoch": 1.1721333333333332,
        "step": 8791
    },
    {
        "loss": 0.6846,
        "grad_norm": 4.0116167068481445,
        "learning_rate": 2.8116235249567747e-05,
        "epoch": 1.1722666666666668,
        "step": 8792
    },
    {
        "loss": 2.8499,
        "grad_norm": 3.358729600906372,
        "learning_rate": 2.8072497247485485e-05,
        "epoch": 1.1724,
        "step": 8793
    },
    {
        "loss": 2.0793,
        "grad_norm": 2.8983538150787354,
        "learning_rate": 2.8028787736132156e-05,
        "epoch": 1.1725333333333334,
        "step": 8794
    },
    {
        "loss": 0.8769,
        "grad_norm": 3.8808863162994385,
        "learning_rate": 2.7985106732821332e-05,
        "epoch": 1.1726666666666667,
        "step": 8795
    },
    {
        "loss": 1.1861,
        "grad_norm": 2.8966622352600098,
        "learning_rate": 2.794145425485496e-05,
        "epoch": 1.1728,
        "step": 8796
    },
    {
        "loss": 2.185,
        "grad_norm": 2.9226012229919434,
        "learning_rate": 2.7897830319524187e-05,
        "epoch": 1.1729333333333334,
        "step": 8797
    },
    {
        "loss": 1.5242,
        "grad_norm": 5.0874223709106445,
        "learning_rate": 2.7854234944108437e-05,
        "epoch": 1.1730666666666667,
        "step": 8798
    },
    {
        "loss": 1.7475,
        "grad_norm": 2.3534390926361084,
        "learning_rate": 2.781066814587613e-05,
        "epoch": 1.1732,
        "step": 8799
    },
    {
        "loss": 2.0188,
        "grad_norm": 2.730006456375122,
        "learning_rate": 2.776712994208417e-05,
        "epoch": 1.1733333333333333,
        "step": 8800
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.5015792846679688,
        "learning_rate": 2.7723620349978074e-05,
        "epoch": 1.1734666666666667,
        "step": 8801
    },
    {
        "loss": 2.2666,
        "grad_norm": 3.4353039264678955,
        "learning_rate": 2.768013938679227e-05,
        "epoch": 1.1736,
        "step": 8802
    },
    {
        "loss": 1.4826,
        "grad_norm": 2.7447891235351562,
        "learning_rate": 2.7636687069749677e-05,
        "epoch": 1.1737333333333333,
        "step": 8803
    },
    {
        "loss": 0.7095,
        "grad_norm": 2.818486452102661,
        "learning_rate": 2.7593263416062e-05,
        "epoch": 1.1738666666666666,
        "step": 8804
    },
    {
        "loss": 2.1802,
        "grad_norm": 3.4496912956237793,
        "learning_rate": 2.754986844292925e-05,
        "epoch": 1.174,
        "step": 8805
    },
    {
        "loss": 1.9648,
        "grad_norm": 3.3376870155334473,
        "learning_rate": 2.7506502167540603e-05,
        "epoch": 1.1741333333333333,
        "step": 8806
    },
    {
        "loss": 2.4929,
        "grad_norm": 3.265916347503662,
        "learning_rate": 2.7463164607073345e-05,
        "epoch": 1.1742666666666666,
        "step": 8807
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.4748339653015137,
        "learning_rate": 2.7419855778693894e-05,
        "epoch": 1.1743999999999999,
        "step": 8808
    },
    {
        "loss": 2.8403,
        "grad_norm": 4.21298885345459,
        "learning_rate": 2.7376575699556783e-05,
        "epoch": 1.1745333333333334,
        "step": 8809
    },
    {
        "loss": 2.1889,
        "grad_norm": 3.4526045322418213,
        "learning_rate": 2.7333324386805504e-05,
        "epoch": 1.1746666666666667,
        "step": 8810
    },
    {
        "loss": 2.951,
        "grad_norm": 3.313427448272705,
        "learning_rate": 2.7290101857572013e-05,
        "epoch": 1.1748,
        "step": 8811
    },
    {
        "loss": 2.1372,
        "grad_norm": 3.0776584148406982,
        "learning_rate": 2.7246908128977012e-05,
        "epoch": 1.1749333333333334,
        "step": 8812
    },
    {
        "loss": 2.1931,
        "grad_norm": 3.5580010414123535,
        "learning_rate": 2.7203743218129574e-05,
        "epoch": 1.1750666666666667,
        "step": 8813
    },
    {
        "loss": 2.0686,
        "grad_norm": 2.78100323677063,
        "learning_rate": 2.7160607142127437e-05,
        "epoch": 1.1752,
        "step": 8814
    },
    {
        "loss": 3.0869,
        "grad_norm": 3.364189863204956,
        "learning_rate": 2.7117499918057066e-05,
        "epoch": 1.1753333333333333,
        "step": 8815
    },
    {
        "loss": 2.5427,
        "grad_norm": 1.9828509092330933,
        "learning_rate": 2.7074421562993313e-05,
        "epoch": 1.1754666666666667,
        "step": 8816
    },
    {
        "loss": 2.1818,
        "grad_norm": 2.2741317749023438,
        "learning_rate": 2.703137209399964e-05,
        "epoch": 1.1756,
        "step": 8817
    },
    {
        "loss": 0.5908,
        "grad_norm": 2.882214069366455,
        "learning_rate": 2.698835152812812e-05,
        "epoch": 1.1757333333333333,
        "step": 8818
    },
    {
        "loss": 3.57,
        "grad_norm": 4.57853364944458,
        "learning_rate": 2.694535988241933e-05,
        "epoch": 1.1758666666666666,
        "step": 8819
    },
    {
        "loss": 2.3118,
        "grad_norm": 3.3597640991210938,
        "learning_rate": 2.6902397173902427e-05,
        "epoch": 1.176,
        "step": 8820
    },
    {
        "loss": 1.5939,
        "grad_norm": 3.55208420753479,
        "learning_rate": 2.68594634195951e-05,
        "epoch": 1.1761333333333333,
        "step": 8821
    },
    {
        "loss": 2.1403,
        "grad_norm": 3.2726171016693115,
        "learning_rate": 2.681655863650354e-05,
        "epoch": 1.1762666666666666,
        "step": 8822
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.4507081508636475,
        "learning_rate": 2.6773682841622404e-05,
        "epoch": 1.1764000000000001,
        "step": 8823
    },
    {
        "loss": 1.9038,
        "grad_norm": 2.1268396377563477,
        "learning_rate": 2.673083605193507e-05,
        "epoch": 1.1765333333333334,
        "step": 8824
    },
    {
        "loss": 1.5474,
        "grad_norm": 3.1456871032714844,
        "learning_rate": 2.6688018284413262e-05,
        "epoch": 1.1766666666666667,
        "step": 8825
    },
    {
        "loss": 1.7998,
        "grad_norm": 5.092025279998779,
        "learning_rate": 2.6645229556017136e-05,
        "epoch": 1.1768,
        "step": 8826
    },
    {
        "loss": 2.4168,
        "grad_norm": 3.805753469467163,
        "learning_rate": 2.6602469883695545e-05,
        "epoch": 1.1769333333333334,
        "step": 8827
    },
    {
        "loss": 1.3448,
        "grad_norm": 4.212167739868164,
        "learning_rate": 2.6559739284385745e-05,
        "epoch": 1.1770666666666667,
        "step": 8828
    },
    {
        "loss": 2.2449,
        "grad_norm": 3.571669101715088,
        "learning_rate": 2.6517037775013542e-05,
        "epoch": 1.1772,
        "step": 8829
    },
    {
        "loss": 2.2451,
        "grad_norm": 3.8702280521392822,
        "learning_rate": 2.6474365372492916e-05,
        "epoch": 1.1773333333333333,
        "step": 8830
    },
    {
        "loss": 1.3199,
        "grad_norm": 3.1073808670043945,
        "learning_rate": 2.6431722093726808e-05,
        "epoch": 1.1774666666666667,
        "step": 8831
    },
    {
        "loss": 1.5708,
        "grad_norm": 2.7632195949554443,
        "learning_rate": 2.638910795560614e-05,
        "epoch": 1.1776,
        "step": 8832
    },
    {
        "loss": 2.5156,
        "grad_norm": 4.475223064422607,
        "learning_rate": 2.634652297501079e-05,
        "epoch": 1.1777333333333333,
        "step": 8833
    },
    {
        "loss": 1.7265,
        "grad_norm": 2.820763111114502,
        "learning_rate": 2.6303967168808564e-05,
        "epoch": 1.1778666666666666,
        "step": 8834
    },
    {
        "loss": 2.2502,
        "grad_norm": 3.774348497390747,
        "learning_rate": 2.6261440553856055e-05,
        "epoch": 1.178,
        "step": 8835
    },
    {
        "loss": 0.569,
        "grad_norm": 2.5883095264434814,
        "learning_rate": 2.6218943146998175e-05,
        "epoch": 1.1781333333333333,
        "step": 8836
    },
    {
        "loss": 2.1314,
        "grad_norm": 2.938260793685913,
        "learning_rate": 2.617647496506841e-05,
        "epoch": 1.1782666666666666,
        "step": 8837
    },
    {
        "loss": 2.3565,
        "grad_norm": 4.526596546173096,
        "learning_rate": 2.613403602488842e-05,
        "epoch": 1.1784,
        "step": 8838
    },
    {
        "loss": 1.43,
        "grad_norm": 2.405791759490967,
        "learning_rate": 2.6091626343268393e-05,
        "epoch": 1.1785333333333332,
        "step": 8839
    },
    {
        "loss": 2.181,
        "grad_norm": 3.4222939014434814,
        "learning_rate": 2.604924593700707e-05,
        "epoch": 1.1786666666666668,
        "step": 8840
    },
    {
        "loss": 0.8976,
        "grad_norm": 3.5665700435638428,
        "learning_rate": 2.6006894822891326e-05,
        "epoch": 1.1788,
        "step": 8841
    },
    {
        "loss": 1.7588,
        "grad_norm": 2.6814992427825928,
        "learning_rate": 2.5964573017696814e-05,
        "epoch": 1.1789333333333334,
        "step": 8842
    },
    {
        "loss": 0.8624,
        "grad_norm": 4.9406609535217285,
        "learning_rate": 2.5922280538187106e-05,
        "epoch": 1.1790666666666667,
        "step": 8843
    },
    {
        "loss": 2.0403,
        "grad_norm": 2.5800890922546387,
        "learning_rate": 2.5880017401114498e-05,
        "epoch": 1.1792,
        "step": 8844
    },
    {
        "loss": 1.9252,
        "grad_norm": 3.017713785171509,
        "learning_rate": 2.5837783623219546e-05,
        "epoch": 1.1793333333333333,
        "step": 8845
    },
    {
        "loss": 2.1997,
        "grad_norm": 4.675843715667725,
        "learning_rate": 2.5795579221231236e-05,
        "epoch": 1.1794666666666667,
        "step": 8846
    },
    {
        "loss": 2.5588,
        "grad_norm": 3.064819812774658,
        "learning_rate": 2.575340421186688e-05,
        "epoch": 1.1796,
        "step": 8847
    },
    {
        "loss": 2.1816,
        "grad_norm": 2.2738020420074463,
        "learning_rate": 2.571125861183198e-05,
        "epoch": 1.1797333333333333,
        "step": 8848
    },
    {
        "loss": 2.2731,
        "grad_norm": 3.117053985595703,
        "learning_rate": 2.566914243782076e-05,
        "epoch": 1.1798666666666666,
        "step": 8849
    },
    {
        "loss": 2.2165,
        "grad_norm": 1.9497401714324951,
        "learning_rate": 2.5627055706515535e-05,
        "epoch": 1.18,
        "step": 8850
    },
    {
        "loss": 2.3839,
        "grad_norm": 3.5430355072021484,
        "learning_rate": 2.558499843458687e-05,
        "epoch": 1.1801333333333333,
        "step": 8851
    },
    {
        "loss": 1.7205,
        "grad_norm": 6.083010196685791,
        "learning_rate": 2.554297063869392e-05,
        "epoch": 1.1802666666666666,
        "step": 8852
    },
    {
        "loss": 1.9509,
        "grad_norm": 3.094593048095703,
        "learning_rate": 2.5500972335483996e-05,
        "epoch": 1.1804000000000001,
        "step": 8853
    },
    {
        "loss": 1.2184,
        "grad_norm": 3.301544189453125,
        "learning_rate": 2.5459003541592886e-05,
        "epoch": 1.1805333333333334,
        "step": 8854
    },
    {
        "loss": 2.2075,
        "grad_norm": 2.851250171661377,
        "learning_rate": 2.541706427364431e-05,
        "epoch": 1.1806666666666668,
        "step": 8855
    },
    {
        "loss": 1.7918,
        "grad_norm": 5.174218654632568,
        "learning_rate": 2.5375154548250825e-05,
        "epoch": 1.1808,
        "step": 8856
    },
    {
        "loss": 2.6625,
        "grad_norm": 2.311420440673828,
        "learning_rate": 2.5333274382012783e-05,
        "epoch": 1.1809333333333334,
        "step": 8857
    },
    {
        "loss": 2.5279,
        "grad_norm": 3.2945539951324463,
        "learning_rate": 2.5291423791519264e-05,
        "epoch": 1.1810666666666667,
        "step": 8858
    },
    {
        "loss": 2.7651,
        "grad_norm": 3.0588889122009277,
        "learning_rate": 2.524960279334738e-05,
        "epoch": 1.1812,
        "step": 8859
    },
    {
        "loss": 2.2967,
        "grad_norm": 4.16192102432251,
        "learning_rate": 2.5207811404062474e-05,
        "epoch": 1.1813333333333333,
        "step": 8860
    },
    {
        "loss": 2.5781,
        "grad_norm": 3.1060171127319336,
        "learning_rate": 2.5166049640218327e-05,
        "epoch": 1.1814666666666667,
        "step": 8861
    },
    {
        "loss": 0.7836,
        "grad_norm": 4.265508651733398,
        "learning_rate": 2.5124317518356878e-05,
        "epoch": 1.1816,
        "step": 8862
    },
    {
        "loss": 2.563,
        "grad_norm": 2.4310531616210938,
        "learning_rate": 2.5082615055008495e-05,
        "epoch": 1.1817333333333333,
        "step": 8863
    },
    {
        "loss": 0.9723,
        "grad_norm": 5.644680976867676,
        "learning_rate": 2.504094226669146e-05,
        "epoch": 1.1818666666666666,
        "step": 8864
    },
    {
        "loss": 1.8142,
        "grad_norm": 3.1736438274383545,
        "learning_rate": 2.499929916991266e-05,
        "epoch": 1.182,
        "step": 8865
    },
    {
        "loss": 2.1716,
        "grad_norm": 3.1169626712799072,
        "learning_rate": 2.495768578116694e-05,
        "epoch": 1.1821333333333333,
        "step": 8866
    },
    {
        "loss": 1.9532,
        "grad_norm": 2.993812322616577,
        "learning_rate": 2.4916102116937722e-05,
        "epoch": 1.1822666666666666,
        "step": 8867
    },
    {
        "loss": 2.5282,
        "grad_norm": 3.3033246994018555,
        "learning_rate": 2.4874548193696213e-05,
        "epoch": 1.1824,
        "step": 8868
    },
    {
        "loss": 1.5962,
        "grad_norm": 3.967533826828003,
        "learning_rate": 2.483302402790214e-05,
        "epoch": 1.1825333333333332,
        "step": 8869
    },
    {
        "loss": 1.7841,
        "grad_norm": 5.070146560668945,
        "learning_rate": 2.4791529636003387e-05,
        "epoch": 1.1826666666666668,
        "step": 8870
    },
    {
        "loss": 2.9462,
        "grad_norm": 3.3410980701446533,
        "learning_rate": 2.4750065034436033e-05,
        "epoch": 1.1828,
        "step": 8871
    },
    {
        "loss": 0.8485,
        "grad_norm": 3.2112205028533936,
        "learning_rate": 2.470863023962435e-05,
        "epoch": 1.1829333333333334,
        "step": 8872
    },
    {
        "loss": 2.2961,
        "grad_norm": 3.1798131465911865,
        "learning_rate": 2.4667225267980677e-05,
        "epoch": 1.1830666666666667,
        "step": 8873
    },
    {
        "loss": 1.6783,
        "grad_norm": 3.860802173614502,
        "learning_rate": 2.4625850135905837e-05,
        "epoch": 1.1832,
        "step": 8874
    },
    {
        "loss": 2.2068,
        "grad_norm": 4.101646900177002,
        "learning_rate": 2.4584504859788626e-05,
        "epoch": 1.1833333333333333,
        "step": 8875
    },
    {
        "loss": 2.025,
        "grad_norm": 4.144690990447998,
        "learning_rate": 2.4543189456005922e-05,
        "epoch": 1.1834666666666667,
        "step": 8876
    },
    {
        "loss": 1.2953,
        "grad_norm": 5.3048095703125,
        "learning_rate": 2.450190394092302e-05,
        "epoch": 1.1836,
        "step": 8877
    },
    {
        "loss": 2.0305,
        "grad_norm": 5.896093368530273,
        "learning_rate": 2.446064833089322e-05,
        "epoch": 1.1837333333333333,
        "step": 8878
    },
    {
        "loss": 1.8045,
        "grad_norm": 3.115544080734253,
        "learning_rate": 2.4419422642258015e-05,
        "epoch": 1.1838666666666666,
        "step": 8879
    },
    {
        "loss": 1.7835,
        "grad_norm": 4.3201069831848145,
        "learning_rate": 2.4378226891347056e-05,
        "epoch": 1.184,
        "step": 8880
    },
    {
        "loss": 1.7586,
        "grad_norm": 4.0543622970581055,
        "learning_rate": 2.433706109447813e-05,
        "epoch": 1.1841333333333333,
        "step": 8881
    },
    {
        "loss": 2.5288,
        "grad_norm": 2.956282377243042,
        "learning_rate": 2.429592526795701e-05,
        "epoch": 1.1842666666666666,
        "step": 8882
    },
    {
        "loss": 1.8812,
        "grad_norm": 5.590622901916504,
        "learning_rate": 2.4254819428077925e-05,
        "epoch": 1.1844000000000001,
        "step": 8883
    },
    {
        "loss": 1.2924,
        "grad_norm": 3.8926827907562256,
        "learning_rate": 2.4213743591122995e-05,
        "epoch": 1.1845333333333334,
        "step": 8884
    },
    {
        "loss": 1.6043,
        "grad_norm": 4.527365207672119,
        "learning_rate": 2.4172697773362407e-05,
        "epoch": 1.1846666666666668,
        "step": 8885
    },
    {
        "loss": 0.8165,
        "grad_norm": 3.384587049484253,
        "learning_rate": 2.41316819910546e-05,
        "epoch": 1.1848,
        "step": 8886
    },
    {
        "loss": 1.4776,
        "grad_norm": 7.2097978591918945,
        "learning_rate": 2.4090696260446055e-05,
        "epoch": 1.1849333333333334,
        "step": 8887
    },
    {
        "loss": 2.3585,
        "grad_norm": 2.8712961673736572,
        "learning_rate": 2.4049740597771473e-05,
        "epoch": 1.1850666666666667,
        "step": 8888
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.737806797027588,
        "learning_rate": 2.4008815019253337e-05,
        "epoch": 1.1852,
        "step": 8889
    },
    {
        "loss": 2.3097,
        "grad_norm": 4.267200946807861,
        "learning_rate": 2.3967919541102556e-05,
        "epoch": 1.1853333333333333,
        "step": 8890
    },
    {
        "loss": 1.8827,
        "grad_norm": 3.515470027923584,
        "learning_rate": 2.3927054179517826e-05,
        "epoch": 1.1854666666666667,
        "step": 8891
    },
    {
        "loss": 2.6127,
        "grad_norm": 3.3663651943206787,
        "learning_rate": 2.38862189506863e-05,
        "epoch": 1.1856,
        "step": 8892
    },
    {
        "loss": 2.4124,
        "grad_norm": 3.256361484527588,
        "learning_rate": 2.3845413870782707e-05,
        "epoch": 1.1857333333333333,
        "step": 8893
    },
    {
        "loss": 2.1669,
        "grad_norm": 2.4765303134918213,
        "learning_rate": 2.380463895597017e-05,
        "epoch": 1.1858666666666666,
        "step": 8894
    },
    {
        "loss": 1.5583,
        "grad_norm": 2.3708815574645996,
        "learning_rate": 2.3763894222399774e-05,
        "epoch": 1.186,
        "step": 8895
    },
    {
        "loss": 1.323,
        "grad_norm": 5.294396877288818,
        "learning_rate": 2.3723179686210674e-05,
        "epoch": 1.1861333333333333,
        "step": 8896
    },
    {
        "loss": 1.5281,
        "grad_norm": 3.37587833404541,
        "learning_rate": 2.3682495363530033e-05,
        "epoch": 1.1862666666666666,
        "step": 8897
    },
    {
        "loss": 1.5019,
        "grad_norm": 3.4052085876464844,
        "learning_rate": 2.3641841270472907e-05,
        "epoch": 1.1864,
        "step": 8898
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.8247687816619873,
        "learning_rate": 2.360121742314273e-05,
        "epoch": 1.1865333333333332,
        "step": 8899
    },
    {
        "loss": 2.4885,
        "grad_norm": 2.855370283126831,
        "learning_rate": 2.3560623837630603e-05,
        "epoch": 1.1866666666666668,
        "step": 8900
    },
    {
        "loss": 1.0888,
        "grad_norm": 4.328147888183594,
        "learning_rate": 2.352006053001593e-05,
        "epoch": 1.1868,
        "step": 8901
    },
    {
        "loss": 2.7255,
        "grad_norm": 3.3505918979644775,
        "learning_rate": 2.3479527516365784e-05,
        "epoch": 1.1869333333333334,
        "step": 8902
    },
    {
        "loss": 2.4904,
        "grad_norm": 3.3585097789764404,
        "learning_rate": 2.3439024812735542e-05,
        "epoch": 1.1870666666666667,
        "step": 8903
    },
    {
        "loss": 2.8182,
        "grad_norm": 3.0053937435150146,
        "learning_rate": 2.3398552435168454e-05,
        "epoch": 1.1872,
        "step": 8904
    },
    {
        "loss": 2.3132,
        "grad_norm": 4.699826240539551,
        "learning_rate": 2.3358110399695786e-05,
        "epoch": 1.1873333333333334,
        "step": 8905
    },
    {
        "loss": 0.5398,
        "grad_norm": 2.8913590908050537,
        "learning_rate": 2.3317698722336768e-05,
        "epoch": 1.1874666666666667,
        "step": 8906
    },
    {
        "loss": 2.5032,
        "grad_norm": 3.4252400398254395,
        "learning_rate": 2.3277317419098466e-05,
        "epoch": 1.1876,
        "step": 8907
    },
    {
        "loss": 1.2387,
        "grad_norm": 3.990098476409912,
        "learning_rate": 2.3236966505976264e-05,
        "epoch": 1.1877333333333333,
        "step": 8908
    },
    {
        "loss": 1.4025,
        "grad_norm": 3.535823106765747,
        "learning_rate": 2.319664599895317e-05,
        "epoch": 1.1878666666666666,
        "step": 8909
    },
    {
        "loss": 2.0337,
        "grad_norm": 1.9560561180114746,
        "learning_rate": 2.315635591400027e-05,
        "epoch": 1.188,
        "step": 8910
    },
    {
        "loss": 1.7673,
        "grad_norm": 3.1711695194244385,
        "learning_rate": 2.3116096267076627e-05,
        "epoch": 1.1881333333333333,
        "step": 8911
    },
    {
        "loss": 0.7448,
        "grad_norm": 5.0984296798706055,
        "learning_rate": 2.30758670741292e-05,
        "epoch": 1.1882666666666666,
        "step": 8912
    },
    {
        "loss": 2.0284,
        "grad_norm": 4.7522292137146,
        "learning_rate": 2.3035668351093052e-05,
        "epoch": 1.1884000000000001,
        "step": 8913
    },
    {
        "loss": 2.2693,
        "grad_norm": 2.9295434951782227,
        "learning_rate": 2.29955001138908e-05,
        "epoch": 1.1885333333333334,
        "step": 8914
    },
    {
        "loss": 2.52,
        "grad_norm": 3.307451009750366,
        "learning_rate": 2.295536237843341e-05,
        "epoch": 1.1886666666666668,
        "step": 8915
    },
    {
        "loss": 2.5879,
        "grad_norm": 4.207030296325684,
        "learning_rate": 2.291525516061941e-05,
        "epoch": 1.1888,
        "step": 8916
    },
    {
        "loss": 0.9514,
        "grad_norm": 3.365658760070801,
        "learning_rate": 2.2875178476335646e-05,
        "epoch": 1.1889333333333334,
        "step": 8917
    },
    {
        "loss": 2.6781,
        "grad_norm": 2.8066089153289795,
        "learning_rate": 2.2835132341456388e-05,
        "epoch": 1.1890666666666667,
        "step": 8918
    },
    {
        "loss": 1.4897,
        "grad_norm": 4.6060309410095215,
        "learning_rate": 2.2795116771844162e-05,
        "epoch": 1.1892,
        "step": 8919
    },
    {
        "loss": 2.6127,
        "grad_norm": 4.324453830718994,
        "learning_rate": 2.2755131783349236e-05,
        "epoch": 1.1893333333333334,
        "step": 8920
    },
    {
        "loss": 2.8514,
        "grad_norm": 3.5918519496917725,
        "learning_rate": 2.2715177391809827e-05,
        "epoch": 1.1894666666666667,
        "step": 8921
    },
    {
        "loss": 0.8731,
        "grad_norm": 4.146273612976074,
        "learning_rate": 2.2675253613052074e-05,
        "epoch": 1.1896,
        "step": 8922
    },
    {
        "loss": 2.1738,
        "grad_norm": 3.1929173469543457,
        "learning_rate": 2.26353604628897e-05,
        "epoch": 1.1897333333333333,
        "step": 8923
    },
    {
        "loss": 2.5702,
        "grad_norm": 3.831756353378296,
        "learning_rate": 2.2595497957124746e-05,
        "epoch": 1.1898666666666666,
        "step": 8924
    },
    {
        "loss": 1.9918,
        "grad_norm": 3.636035203933716,
        "learning_rate": 2.2555666111546747e-05,
        "epoch": 1.19,
        "step": 8925
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.4558165073394775,
        "learning_rate": 2.2515864941933394e-05,
        "epoch": 1.1901333333333333,
        "step": 8926
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.537997245788574,
        "learning_rate": 2.2476094464049846e-05,
        "epoch": 1.1902666666666666,
        "step": 8927
    },
    {
        "loss": 2.6636,
        "grad_norm": 3.91788387298584,
        "learning_rate": 2.2436354693649465e-05,
        "epoch": 1.1904,
        "step": 8928
    },
    {
        "loss": 2.5298,
        "grad_norm": 3.3790838718414307,
        "learning_rate": 2.239664564647329e-05,
        "epoch": 1.1905333333333332,
        "step": 8929
    },
    {
        "loss": 1.8199,
        "grad_norm": 3.3372905254364014,
        "learning_rate": 2.2356967338250223e-05,
        "epoch": 1.1906666666666668,
        "step": 8930
    },
    {
        "loss": 2.4984,
        "grad_norm": 4.545605182647705,
        "learning_rate": 2.2317319784696976e-05,
        "epoch": 1.1908,
        "step": 8931
    },
    {
        "loss": 1.8093,
        "grad_norm": 3.2676329612731934,
        "learning_rate": 2.2277703001517958e-05,
        "epoch": 1.1909333333333334,
        "step": 8932
    },
    {
        "loss": 1.8406,
        "grad_norm": 3.151057720184326,
        "learning_rate": 2.2238117004405723e-05,
        "epoch": 1.1910666666666667,
        "step": 8933
    },
    {
        "loss": 1.6203,
        "grad_norm": 3.2804651260375977,
        "learning_rate": 2.2198561809040285e-05,
        "epoch": 1.1912,
        "step": 8934
    },
    {
        "loss": 2.3772,
        "grad_norm": 3.0194621086120605,
        "learning_rate": 2.215903743108958e-05,
        "epoch": 1.1913333333333334,
        "step": 8935
    },
    {
        "loss": 2.1263,
        "grad_norm": 5.086939811706543,
        "learning_rate": 2.211954388620939e-05,
        "epoch": 1.1914666666666667,
        "step": 8936
    },
    {
        "loss": 2.1338,
        "grad_norm": 2.1293821334838867,
        "learning_rate": 2.2080081190043213e-05,
        "epoch": 1.1916,
        "step": 8937
    },
    {
        "loss": 1.7117,
        "grad_norm": 3.6667797565460205,
        "learning_rate": 2.2040649358222486e-05,
        "epoch": 1.1917333333333333,
        "step": 8938
    },
    {
        "loss": 1.6212,
        "grad_norm": 4.2311601638793945,
        "learning_rate": 2.2001248406366083e-05,
        "epoch": 1.1918666666666666,
        "step": 8939
    },
    {
        "loss": 2.6682,
        "grad_norm": 2.909270763397217,
        "learning_rate": 2.196187835008101e-05,
        "epoch": 1.192,
        "step": 8940
    },
    {
        "loss": 2.0634,
        "grad_norm": 5.178682804107666,
        "learning_rate": 2.1922539204961755e-05,
        "epoch": 1.1921333333333333,
        "step": 8941
    },
    {
        "loss": 2.0364,
        "grad_norm": 2.743150234222412,
        "learning_rate": 2.1883230986590874e-05,
        "epoch": 1.1922666666666666,
        "step": 8942
    },
    {
        "loss": 1.6333,
        "grad_norm": 5.27600622177124,
        "learning_rate": 2.1843953710538346e-05,
        "epoch": 1.1924,
        "step": 8943
    },
    {
        "loss": 1.8067,
        "grad_norm": 3.039393901824951,
        "learning_rate": 2.1804707392362033e-05,
        "epoch": 1.1925333333333334,
        "step": 8944
    },
    {
        "loss": 2.4716,
        "grad_norm": 4.014865398406982,
        "learning_rate": 2.1765492047607573e-05,
        "epoch": 1.1926666666666668,
        "step": 8945
    },
    {
        "loss": 1.3315,
        "grad_norm": 3.5974948406219482,
        "learning_rate": 2.172630769180828e-05,
        "epoch": 1.1928,
        "step": 8946
    },
    {
        "loss": 2.5369,
        "grad_norm": 3.2003841400146484,
        "learning_rate": 2.1687154340485305e-05,
        "epoch": 1.1929333333333334,
        "step": 8947
    },
    {
        "loss": 2.0018,
        "grad_norm": 3.0780222415924072,
        "learning_rate": 2.164803200914721e-05,
        "epoch": 1.1930666666666667,
        "step": 8948
    },
    {
        "loss": 0.634,
        "grad_norm": 5.738903999328613,
        "learning_rate": 2.1608940713290692e-05,
        "epoch": 1.1932,
        "step": 8949
    },
    {
        "loss": 2.103,
        "grad_norm": 2.8958988189697266,
        "learning_rate": 2.1569880468399818e-05,
        "epoch": 1.1933333333333334,
        "step": 8950
    },
    {
        "loss": 2.4758,
        "grad_norm": 2.389343023300171,
        "learning_rate": 2.1530851289946642e-05,
        "epoch": 1.1934666666666667,
        "step": 8951
    },
    {
        "loss": 1.9521,
        "grad_norm": 2.958256959915161,
        "learning_rate": 2.1491853193390532e-05,
        "epoch": 1.1936,
        "step": 8952
    },
    {
        "loss": 2.3648,
        "grad_norm": 3.634016275405884,
        "learning_rate": 2.1452886194178913e-05,
        "epoch": 1.1937333333333333,
        "step": 8953
    },
    {
        "loss": 1.7377,
        "grad_norm": 4.216363906860352,
        "learning_rate": 2.1413950307746723e-05,
        "epoch": 1.1938666666666666,
        "step": 8954
    },
    {
        "loss": 2.5245,
        "grad_norm": 3.3990542888641357,
        "learning_rate": 2.1375045549516637e-05,
        "epoch": 1.194,
        "step": 8955
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.780803680419922,
        "learning_rate": 2.1336171934898885e-05,
        "epoch": 1.1941333333333333,
        "step": 8956
    },
    {
        "loss": 1.9575,
        "grad_norm": 3.5268642902374268,
        "learning_rate": 2.1297329479291394e-05,
        "epoch": 1.1942666666666666,
        "step": 8957
    },
    {
        "loss": 1.8736,
        "grad_norm": 4.115333557128906,
        "learning_rate": 2.125851819807998e-05,
        "epoch": 1.1944,
        "step": 8958
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.7502336502075195,
        "learning_rate": 2.121973810663771e-05,
        "epoch": 1.1945333333333332,
        "step": 8959
    },
    {
        "loss": 1.7757,
        "grad_norm": 3.3453500270843506,
        "learning_rate": 2.118098922032574e-05,
        "epoch": 1.1946666666666665,
        "step": 8960
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.6806509494781494,
        "learning_rate": 2.1142271554492433e-05,
        "epoch": 1.1948,
        "step": 8961
    },
    {
        "loss": 2.027,
        "grad_norm": 4.559287071228027,
        "learning_rate": 2.1103585124474058e-05,
        "epoch": 1.1949333333333334,
        "step": 8962
    },
    {
        "loss": 2.0685,
        "grad_norm": 4.493790149688721,
        "learning_rate": 2.1064929945594424e-05,
        "epoch": 1.1950666666666667,
        "step": 8963
    },
    {
        "loss": 2.6367,
        "grad_norm": 3.0812926292419434,
        "learning_rate": 2.102630603316508e-05,
        "epoch": 1.1952,
        "step": 8964
    },
    {
        "loss": 2.358,
        "grad_norm": 3.308788776397705,
        "learning_rate": 2.098771340248499e-05,
        "epoch": 1.1953333333333334,
        "step": 8965
    },
    {
        "loss": 2.3305,
        "grad_norm": 3.3527002334594727,
        "learning_rate": 2.094915206884077e-05,
        "epoch": 1.1954666666666667,
        "step": 8966
    },
    {
        "loss": 1.2922,
        "grad_norm": 3.314910411834717,
        "learning_rate": 2.091062204750688e-05,
        "epoch": 1.1956,
        "step": 8967
    },
    {
        "loss": 1.8791,
        "grad_norm": 3.147968053817749,
        "learning_rate": 2.0872123353745054e-05,
        "epoch": 1.1957333333333333,
        "step": 8968
    },
    {
        "loss": 2.3028,
        "grad_norm": 2.2644875049591064,
        "learning_rate": 2.083365600280478e-05,
        "epoch": 1.1958666666666666,
        "step": 8969
    },
    {
        "loss": 2.1727,
        "grad_norm": 3.8861072063446045,
        "learning_rate": 2.07952200099231e-05,
        "epoch": 1.196,
        "step": 8970
    },
    {
        "loss": 2.2724,
        "grad_norm": 4.437888145446777,
        "learning_rate": 2.0756815390324656e-05,
        "epoch": 1.1961333333333333,
        "step": 8971
    },
    {
        "loss": 2.3682,
        "grad_norm": 2.009068250656128,
        "learning_rate": 2.071844215922174e-05,
        "epoch": 1.1962666666666666,
        "step": 8972
    },
    {
        "loss": 0.5327,
        "grad_norm": 2.608633279800415,
        "learning_rate": 2.06801003318139e-05,
        "epoch": 1.1964,
        "step": 8973
    },
    {
        "loss": 2.0761,
        "grad_norm": 3.8866302967071533,
        "learning_rate": 2.0641789923288667e-05,
        "epoch": 1.1965333333333334,
        "step": 8974
    },
    {
        "loss": 2.3848,
        "grad_norm": 2.8650825023651123,
        "learning_rate": 2.0603510948820803e-05,
        "epoch": 1.1966666666666668,
        "step": 8975
    },
    {
        "loss": 1.7136,
        "grad_norm": 3.4860668182373047,
        "learning_rate": 2.056526342357289e-05,
        "epoch": 1.1968,
        "step": 8976
    },
    {
        "loss": 1.6835,
        "grad_norm": 5.294009208679199,
        "learning_rate": 2.0527047362694697e-05,
        "epoch": 1.1969333333333334,
        "step": 8977
    },
    {
        "loss": 1.7177,
        "grad_norm": 3.027973175048828,
        "learning_rate": 2.048886278132387e-05,
        "epoch": 1.1970666666666667,
        "step": 8978
    },
    {
        "loss": 1.5989,
        "grad_norm": 2.0441675186157227,
        "learning_rate": 2.0450709694585423e-05,
        "epoch": 1.1972,
        "step": 8979
    },
    {
        "loss": 1.6829,
        "grad_norm": 2.8035292625427246,
        "learning_rate": 2.0412588117591923e-05,
        "epoch": 1.1973333333333334,
        "step": 8980
    },
    {
        "loss": 1.5521,
        "grad_norm": 2.2500107288360596,
        "learning_rate": 2.0374498065443538e-05,
        "epoch": 1.1974666666666667,
        "step": 8981
    },
    {
        "loss": 1.6593,
        "grad_norm": 5.748926162719727,
        "learning_rate": 2.0336439553227647e-05,
        "epoch": 1.1976,
        "step": 8982
    },
    {
        "loss": 2.2624,
        "grad_norm": 3.839703321456909,
        "learning_rate": 2.029841259601961e-05,
        "epoch": 1.1977333333333333,
        "step": 8983
    },
    {
        "loss": 2.45,
        "grad_norm": 2.2200307846069336,
        "learning_rate": 2.026041720888181e-05,
        "epoch": 1.1978666666666666,
        "step": 8984
    },
    {
        "loss": 2.2485,
        "grad_norm": 3.454493761062622,
        "learning_rate": 2.0222453406864595e-05,
        "epoch": 1.198,
        "step": 8985
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.5915744304656982,
        "learning_rate": 2.0184521205005335e-05,
        "epoch": 1.1981333333333333,
        "step": 8986
    },
    {
        "loss": 1.3341,
        "grad_norm": 2.942964792251587,
        "learning_rate": 2.0146620618329192e-05,
        "epoch": 1.1982666666666666,
        "step": 8987
    },
    {
        "loss": 2.6862,
        "grad_norm": 5.080556392669678,
        "learning_rate": 2.01087516618487e-05,
        "epoch": 1.1984,
        "step": 8988
    },
    {
        "loss": 1.6095,
        "grad_norm": 2.272517442703247,
        "learning_rate": 2.007091435056396e-05,
        "epoch": 1.1985333333333332,
        "step": 8989
    },
    {
        "loss": 2.9312,
        "grad_norm": 2.662353515625,
        "learning_rate": 2.0033108699462388e-05,
        "epoch": 1.1986666666666665,
        "step": 8990
    },
    {
        "loss": 2.4015,
        "grad_norm": 3.1690897941589355,
        "learning_rate": 1.9995334723518855e-05,
        "epoch": 1.1988,
        "step": 8991
    },
    {
        "loss": 1.8892,
        "grad_norm": 5.177964687347412,
        "learning_rate": 1.995759243769595e-05,
        "epoch": 1.1989333333333334,
        "step": 8992
    },
    {
        "loss": 1.3893,
        "grad_norm": 2.42293119430542,
        "learning_rate": 1.991988185694339e-05,
        "epoch": 1.1990666666666667,
        "step": 8993
    },
    {
        "loss": 2.3122,
        "grad_norm": 5.859715461730957,
        "learning_rate": 1.9882202996198474e-05,
        "epoch": 1.1992,
        "step": 8994
    },
    {
        "loss": 2.2127,
        "grad_norm": 2.2965497970581055,
        "learning_rate": 1.984455587038594e-05,
        "epoch": 1.1993333333333334,
        "step": 8995
    },
    {
        "loss": 1.0968,
        "grad_norm": 2.9521632194519043,
        "learning_rate": 1.9806940494417947e-05,
        "epoch": 1.1994666666666667,
        "step": 8996
    },
    {
        "loss": 2.0461,
        "grad_norm": 4.457379341125488,
        "learning_rate": 1.9769356883194156e-05,
        "epoch": 1.1996,
        "step": 8997
    },
    {
        "loss": 1.8653,
        "grad_norm": 3.9009499549865723,
        "learning_rate": 1.973180505160136e-05,
        "epoch": 1.1997333333333333,
        "step": 8998
    },
    {
        "loss": 1.4228,
        "grad_norm": 4.932462215423584,
        "learning_rate": 1.9694285014514158e-05,
        "epoch": 1.1998666666666666,
        "step": 8999
    },
    {
        "loss": 2.8439,
        "grad_norm": 2.334240674972534,
        "learning_rate": 1.965679678679425e-05,
        "epoch": 1.2,
        "step": 9000
    },
    {
        "loss": 1.5326,
        "grad_norm": 4.404105186462402,
        "learning_rate": 1.961934038329094e-05,
        "epoch": 1.2001333333333333,
        "step": 9001
    },
    {
        "loss": 1.1849,
        "grad_norm": 3.1475207805633545,
        "learning_rate": 1.9581915818840835e-05,
        "epoch": 1.2002666666666666,
        "step": 9002
    },
    {
        "loss": 1.5044,
        "grad_norm": 4.669647216796875,
        "learning_rate": 1.954452310826781e-05,
        "epoch": 1.2004,
        "step": 9003
    },
    {
        "loss": 1.6933,
        "grad_norm": 3.0929887294769287,
        "learning_rate": 1.950716226638335e-05,
        "epoch": 1.2005333333333335,
        "step": 9004
    },
    {
        "loss": 2.0245,
        "grad_norm": 3.2589590549468994,
        "learning_rate": 1.9469833307986185e-05,
        "epoch": 1.2006666666666668,
        "step": 9005
    },
    {
        "loss": 0.8953,
        "grad_norm": 3.78764009475708,
        "learning_rate": 1.943253624786253e-05,
        "epoch": 1.2008,
        "step": 9006
    },
    {
        "loss": 3.0005,
        "grad_norm": 2.675262928009033,
        "learning_rate": 1.939527110078566e-05,
        "epoch": 1.2009333333333334,
        "step": 9007
    },
    {
        "loss": 2.1872,
        "grad_norm": 3.3583388328552246,
        "learning_rate": 1.9358037881516666e-05,
        "epoch": 1.2010666666666667,
        "step": 9008
    },
    {
        "loss": 2.3794,
        "grad_norm": 3.4931797981262207,
        "learning_rate": 1.932083660480355e-05,
        "epoch": 1.2012,
        "step": 9009
    },
    {
        "loss": 2.7068,
        "grad_norm": 3.217661142349243,
        "learning_rate": 1.928366728538209e-05,
        "epoch": 1.2013333333333334,
        "step": 9010
    },
    {
        "loss": 1.7323,
        "grad_norm": 3.294726848602295,
        "learning_rate": 1.9246529937974977e-05,
        "epoch": 1.2014666666666667,
        "step": 9011
    },
    {
        "loss": 2.2742,
        "grad_norm": 4.493830680847168,
        "learning_rate": 1.9209424577292512e-05,
        "epoch": 1.2016,
        "step": 9012
    },
    {
        "loss": 1.8932,
        "grad_norm": 3.913292169570923,
        "learning_rate": 1.9172351218032226e-05,
        "epoch": 1.2017333333333333,
        "step": 9013
    },
    {
        "loss": 2.6221,
        "grad_norm": 2.728846311569214,
        "learning_rate": 1.9135309874879126e-05,
        "epoch": 1.2018666666666666,
        "step": 9014
    },
    {
        "loss": 2.2661,
        "grad_norm": 2.184279203414917,
        "learning_rate": 1.909830056250529e-05,
        "epoch": 1.202,
        "step": 9015
    },
    {
        "loss": 2.1377,
        "grad_norm": 4.020258903503418,
        "learning_rate": 1.9061323295570222e-05,
        "epoch": 1.2021333333333333,
        "step": 9016
    },
    {
        "loss": 2.0298,
        "grad_norm": 3.448476791381836,
        "learning_rate": 1.902437808872083e-05,
        "epoch": 1.2022666666666666,
        "step": 9017
    },
    {
        "loss": 1.9849,
        "grad_norm": 3.3381776809692383,
        "learning_rate": 1.8987464956591206e-05,
        "epoch": 1.2024,
        "step": 9018
    },
    {
        "loss": 2.0209,
        "grad_norm": 3.944020986557007,
        "learning_rate": 1.8950583913802723e-05,
        "epoch": 1.2025333333333332,
        "step": 9019
    },
    {
        "loss": 1.834,
        "grad_norm": 3.552785873413086,
        "learning_rate": 1.8913734974964126e-05,
        "epoch": 1.2026666666666666,
        "step": 9020
    },
    {
        "loss": 2.3731,
        "grad_norm": 3.263547658920288,
        "learning_rate": 1.8876918154671396e-05,
        "epoch": 1.2028,
        "step": 9021
    },
    {
        "loss": 0.8754,
        "grad_norm": 3.073589324951172,
        "learning_rate": 1.8840133467507804e-05,
        "epoch": 1.2029333333333334,
        "step": 9022
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.626345634460449,
        "learning_rate": 1.8803380928043923e-05,
        "epoch": 1.2030666666666667,
        "step": 9023
    },
    {
        "loss": 1.6562,
        "grad_norm": 3.9262049198150635,
        "learning_rate": 1.876666055083752e-05,
        "epoch": 1.2032,
        "step": 9024
    },
    {
        "loss": 2.3947,
        "grad_norm": 2.5894229412078857,
        "learning_rate": 1.8729972350433623e-05,
        "epoch": 1.2033333333333334,
        "step": 9025
    },
    {
        "loss": 1.4052,
        "grad_norm": 4.098121166229248,
        "learning_rate": 1.8693316341364642e-05,
        "epoch": 1.2034666666666667,
        "step": 9026
    },
    {
        "loss": 2.1155,
        "grad_norm": 4.26952600479126,
        "learning_rate": 1.8656692538150144e-05,
        "epoch": 1.2036,
        "step": 9027
    },
    {
        "loss": 2.3017,
        "grad_norm": 3.5829930305480957,
        "learning_rate": 1.8620100955296814e-05,
        "epoch": 1.2037333333333333,
        "step": 9028
    },
    {
        "loss": 2.5045,
        "grad_norm": 2.6642277240753174,
        "learning_rate": 1.8583541607298816e-05,
        "epoch": 1.2038666666666666,
        "step": 9029
    },
    {
        "loss": 2.4196,
        "grad_norm": 2.4582414627075195,
        "learning_rate": 1.8547014508637415e-05,
        "epoch": 1.204,
        "step": 9030
    },
    {
        "loss": 2.2541,
        "grad_norm": 5.088522434234619,
        "learning_rate": 1.8510519673781178e-05,
        "epoch": 1.2041333333333333,
        "step": 9031
    },
    {
        "loss": 0.6419,
        "grad_norm": 3.0520682334899902,
        "learning_rate": 1.8474057117185652e-05,
        "epoch": 1.2042666666666666,
        "step": 9032
    },
    {
        "loss": 1.5037,
        "grad_norm": 3.8103973865509033,
        "learning_rate": 1.8437626853293987e-05,
        "epoch": 1.2044,
        "step": 9033
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.630373477935791,
        "learning_rate": 1.8401228896536137e-05,
        "epoch": 1.2045333333333335,
        "step": 9034
    },
    {
        "loss": 2.0803,
        "grad_norm": 2.7173280715942383,
        "learning_rate": 1.83648632613297e-05,
        "epoch": 1.2046666666666668,
        "step": 9035
    },
    {
        "loss": 1.8218,
        "grad_norm": 4.150346279144287,
        "learning_rate": 1.832852996207901e-05,
        "epoch": 1.2048,
        "step": 9036
    },
    {
        "loss": 2.5026,
        "grad_norm": 3.7984511852264404,
        "learning_rate": 1.829222901317589e-05,
        "epoch": 1.2049333333333334,
        "step": 9037
    },
    {
        "loss": 2.0882,
        "grad_norm": 3.22615647315979,
        "learning_rate": 1.825596042899924e-05,
        "epoch": 1.2050666666666667,
        "step": 9038
    },
    {
        "loss": 1.8425,
        "grad_norm": 4.336110591888428,
        "learning_rate": 1.821972422391528e-05,
        "epoch": 1.2052,
        "step": 9039
    },
    {
        "loss": 2.7871,
        "grad_norm": 2.15746808052063,
        "learning_rate": 1.8183520412277188e-05,
        "epoch": 1.2053333333333334,
        "step": 9040
    },
    {
        "loss": 1.8292,
        "grad_norm": 3.68037486076355,
        "learning_rate": 1.8147349008425395e-05,
        "epoch": 1.2054666666666667,
        "step": 9041
    },
    {
        "loss": 2.0111,
        "grad_norm": 3.462595224380493,
        "learning_rate": 1.811121002668762e-05,
        "epoch": 1.2056,
        "step": 9042
    },
    {
        "loss": 0.8681,
        "grad_norm": 5.202419757843018,
        "learning_rate": 1.8075103481378498e-05,
        "epoch": 1.2057333333333333,
        "step": 9043
    },
    {
        "loss": 1.8357,
        "grad_norm": 3.8470897674560547,
        "learning_rate": 1.8039029386800178e-05,
        "epoch": 1.2058666666666666,
        "step": 9044
    },
    {
        "loss": 1.8647,
        "grad_norm": 4.080851078033447,
        "learning_rate": 1.8002987757241507e-05,
        "epoch": 1.206,
        "step": 9045
    },
    {
        "loss": 1.2432,
        "grad_norm": 4.442611217498779,
        "learning_rate": 1.7966978606978778e-05,
        "epoch": 1.2061333333333333,
        "step": 9046
    },
    {
        "loss": 1.9455,
        "grad_norm": 3.6641175746917725,
        "learning_rate": 1.793100195027535e-05,
        "epoch": 1.2062666666666666,
        "step": 9047
    },
    {
        "loss": 2.6131,
        "grad_norm": 3.3322484493255615,
        "learning_rate": 1.789505780138172e-05,
        "epoch": 1.2064,
        "step": 9048
    },
    {
        "loss": 1.561,
        "grad_norm": 2.800635814666748,
        "learning_rate": 1.785914617453549e-05,
        "epoch": 1.2065333333333332,
        "step": 9049
    },
    {
        "loss": 2.393,
        "grad_norm": 2.8468618392944336,
        "learning_rate": 1.782326708396124e-05,
        "epoch": 1.2066666666666666,
        "step": 9050
    },
    {
        "loss": 2.2169,
        "grad_norm": 4.125355243682861,
        "learning_rate": 1.7787420543870992e-05,
        "epoch": 1.2068,
        "step": 9051
    },
    {
        "loss": 0.6776,
        "grad_norm": 3.7555465698242188,
        "learning_rate": 1.775160656846362e-05,
        "epoch": 1.2069333333333334,
        "step": 9052
    },
    {
        "loss": 2.4514,
        "grad_norm": 2.4128987789154053,
        "learning_rate": 1.771582517192506e-05,
        "epoch": 1.2070666666666667,
        "step": 9053
    },
    {
        "loss": 1.8171,
        "grad_norm": 3.3288679122924805,
        "learning_rate": 1.7680076368428556e-05,
        "epoch": 1.2072,
        "step": 9054
    },
    {
        "loss": 1.3494,
        "grad_norm": 4.096227169036865,
        "learning_rate": 1.76443601721343e-05,
        "epoch": 1.2073333333333334,
        "step": 9055
    },
    {
        "loss": 2.6834,
        "grad_norm": 2.601536750793457,
        "learning_rate": 1.7608676597189677e-05,
        "epoch": 1.2074666666666667,
        "step": 9056
    },
    {
        "loss": 0.9903,
        "grad_norm": 2.128185749053955,
        "learning_rate": 1.757302565772887e-05,
        "epoch": 1.2076,
        "step": 9057
    },
    {
        "loss": 2.5423,
        "grad_norm": 4.362541675567627,
        "learning_rate": 1.753740736787356e-05,
        "epoch": 1.2077333333333333,
        "step": 9058
    },
    {
        "loss": 1.7923,
        "grad_norm": 4.830152988433838,
        "learning_rate": 1.750182174173207e-05,
        "epoch": 1.2078666666666666,
        "step": 9059
    },
    {
        "loss": 1.1725,
        "grad_norm": 3.0314269065856934,
        "learning_rate": 1.7466268793400175e-05,
        "epoch": 1.208,
        "step": 9060
    },
    {
        "loss": 2.2338,
        "grad_norm": 3.337415933609009,
        "learning_rate": 1.743074853696046e-05,
        "epoch": 1.2081333333333333,
        "step": 9061
    },
    {
        "loss": 2.6082,
        "grad_norm": 4.661092758178711,
        "learning_rate": 1.7395260986482553e-05,
        "epoch": 1.2082666666666666,
        "step": 9062
    },
    {
        "loss": 2.3751,
        "grad_norm": 2.9687728881835938,
        "learning_rate": 1.735980615602324e-05,
        "epoch": 1.2084,
        "step": 9063
    },
    {
        "loss": 1.9657,
        "grad_norm": 5.418008804321289,
        "learning_rate": 1.7324384059626274e-05,
        "epoch": 1.2085333333333332,
        "step": 9064
    },
    {
        "loss": 0.926,
        "grad_norm": 4.4774065017700195,
        "learning_rate": 1.7288994711322582e-05,
        "epoch": 1.2086666666666668,
        "step": 9065
    },
    {
        "loss": 2.152,
        "grad_norm": 3.1242551803588867,
        "learning_rate": 1.7253638125129823e-05,
        "epoch": 1.2088,
        "step": 9066
    },
    {
        "loss": 2.3255,
        "grad_norm": 3.178633689880371,
        "learning_rate": 1.7218314315052996e-05,
        "epoch": 1.2089333333333334,
        "step": 9067
    },
    {
        "loss": 1.9466,
        "grad_norm": 3.414451837539673,
        "learning_rate": 1.7183023295083877e-05,
        "epoch": 1.2090666666666667,
        "step": 9068
    },
    {
        "loss": 2.4516,
        "grad_norm": 3.410392999649048,
        "learning_rate": 1.7147765079201538e-05,
        "epoch": 1.2092,
        "step": 9069
    },
    {
        "loss": 2.659,
        "grad_norm": 4.222513198852539,
        "learning_rate": 1.7112539681371696e-05,
        "epoch": 1.2093333333333334,
        "step": 9070
    },
    {
        "loss": 2.527,
        "grad_norm": 2.8467090129852295,
        "learning_rate": 1.7077347115547314e-05,
        "epoch": 1.2094666666666667,
        "step": 9071
    },
    {
        "loss": 1.7194,
        "grad_norm": 3.265573024749756,
        "learning_rate": 1.7042187395668296e-05,
        "epoch": 1.2096,
        "step": 9072
    },
    {
        "loss": 2.3521,
        "grad_norm": 4.263228893280029,
        "learning_rate": 1.7007060535661534e-05,
        "epoch": 1.2097333333333333,
        "step": 9073
    },
    {
        "loss": 3.184,
        "grad_norm": 3.7580690383911133,
        "learning_rate": 1.697196654944092e-05,
        "epoch": 1.2098666666666666,
        "step": 9074
    },
    {
        "loss": 1.5688,
        "grad_norm": 3.335230588912964,
        "learning_rate": 1.693690545090717e-05,
        "epoch": 1.21,
        "step": 9075
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.8831214904785156,
        "learning_rate": 1.6901877253948272e-05,
        "epoch": 1.2101333333333333,
        "step": 9076
    },
    {
        "loss": 2.611,
        "grad_norm": 3.6669535636901855,
        "learning_rate": 1.6866881972438974e-05,
        "epoch": 1.2102666666666666,
        "step": 9077
    },
    {
        "loss": 0.8323,
        "grad_norm": 2.8270602226257324,
        "learning_rate": 1.6831919620240922e-05,
        "epoch": 1.2104,
        "step": 9078
    },
    {
        "loss": 1.9769,
        "grad_norm": 3.6045949459075928,
        "learning_rate": 1.6796990211202913e-05,
        "epoch": 1.2105333333333332,
        "step": 9079
    },
    {
        "loss": 2.4173,
        "grad_norm": 3.367776870727539,
        "learning_rate": 1.676209375916059e-05,
        "epoch": 1.2106666666666666,
        "step": 9080
    },
    {
        "loss": 2.5407,
        "grad_norm": 3.8061676025390625,
        "learning_rate": 1.6727230277936555e-05,
        "epoch": 1.2107999999999999,
        "step": 9081
    },
    {
        "loss": 1.086,
        "grad_norm": 3.0731489658355713,
        "learning_rate": 1.6692399781340386e-05,
        "epoch": 1.2109333333333334,
        "step": 9082
    },
    {
        "loss": 2.8849,
        "grad_norm": 4.184908866882324,
        "learning_rate": 1.665760228316854e-05,
        "epoch": 1.2110666666666667,
        "step": 9083
    },
    {
        "loss": 2.0032,
        "grad_norm": 3.4869449138641357,
        "learning_rate": 1.6622837797204317e-05,
        "epoch": 1.2112,
        "step": 9084
    },
    {
        "loss": 1.5038,
        "grad_norm": 2.288861036300659,
        "learning_rate": 1.6588106337218224e-05,
        "epoch": 1.2113333333333334,
        "step": 9085
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.704771041870117,
        "learning_rate": 1.6553407916967446e-05,
        "epoch": 1.2114666666666667,
        "step": 9086
    },
    {
        "loss": 2.6925,
        "grad_norm": 3.5146589279174805,
        "learning_rate": 1.6518742550196075e-05,
        "epoch": 1.2116,
        "step": 9087
    },
    {
        "loss": 1.1938,
        "grad_norm": 1.9211153984069824,
        "learning_rate": 1.6484110250635254e-05,
        "epoch": 1.2117333333333333,
        "step": 9088
    },
    {
        "loss": 1.0915,
        "grad_norm": 2.324232339859009,
        "learning_rate": 1.6449511032002896e-05,
        "epoch": 1.2118666666666666,
        "step": 9089
    },
    {
        "loss": 2.3504,
        "grad_norm": 3.1370089054107666,
        "learning_rate": 1.6414944908004017e-05,
        "epoch": 1.212,
        "step": 9090
    },
    {
        "loss": 2.3849,
        "grad_norm": 3.1243762969970703,
        "learning_rate": 1.6380411892330162e-05,
        "epoch": 1.2121333333333333,
        "step": 9091
    },
    {
        "loss": 2.315,
        "grad_norm": 4.102250099182129,
        "learning_rate": 1.6345911998660134e-05,
        "epoch": 1.2122666666666666,
        "step": 9092
    },
    {
        "loss": 1.893,
        "grad_norm": 3.411525249481201,
        "learning_rate": 1.631144524065934e-05,
        "epoch": 1.2124,
        "step": 9093
    },
    {
        "loss": 2.2253,
        "grad_norm": 3.6286520957946777,
        "learning_rate": 1.6277011631980366e-05,
        "epoch": 1.2125333333333332,
        "step": 9094
    },
    {
        "loss": 1.3191,
        "grad_norm": 4.43856954574585,
        "learning_rate": 1.624261118626229e-05,
        "epoch": 1.2126666666666668,
        "step": 9095
    },
    {
        "loss": 0.9009,
        "grad_norm": 3.871633768081665,
        "learning_rate": 1.6208243917131327e-05,
        "epoch": 1.2128,
        "step": 9096
    },
    {
        "loss": 1.9133,
        "grad_norm": 3.3175432682037354,
        "learning_rate": 1.6173909838200453e-05,
        "epoch": 1.2129333333333334,
        "step": 9097
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.5330166816711426,
        "learning_rate": 1.613960896306955e-05,
        "epoch": 1.2130666666666667,
        "step": 9098
    },
    {
        "loss": 2.4035,
        "grad_norm": 3.510417938232422,
        "learning_rate": 1.6105341305325306e-05,
        "epoch": 1.2132,
        "step": 9099
    },
    {
        "loss": 1.5921,
        "grad_norm": 2.6912410259246826,
        "learning_rate": 1.6071106878541142e-05,
        "epoch": 1.2133333333333334,
        "step": 9100
    },
    {
        "loss": 1.0487,
        "grad_norm": 4.417660713195801,
        "learning_rate": 1.6036905696277592e-05,
        "epoch": 1.2134666666666667,
        "step": 9101
    },
    {
        "loss": 1.1846,
        "grad_norm": 3.8959577083587646,
        "learning_rate": 1.6002737772081742e-05,
        "epoch": 1.2136,
        "step": 9102
    },
    {
        "loss": 1.909,
        "grad_norm": 4.185866355895996,
        "learning_rate": 1.596860311948778e-05,
        "epoch": 1.2137333333333333,
        "step": 9103
    },
    {
        "loss": 1.759,
        "grad_norm": 3.0814599990844727,
        "learning_rate": 1.593450175201635e-05,
        "epoch": 1.2138666666666666,
        "step": 9104
    },
    {
        "loss": 0.7114,
        "grad_norm": 4.963902473449707,
        "learning_rate": 1.5900433683175252e-05,
        "epoch": 1.214,
        "step": 9105
    },
    {
        "loss": 1.0441,
        "grad_norm": 3.038121223449707,
        "learning_rate": 1.5866398926458924e-05,
        "epoch": 1.2141333333333333,
        "step": 9106
    },
    {
        "loss": 0.8412,
        "grad_norm": 3.0282139778137207,
        "learning_rate": 1.5832397495348696e-05,
        "epoch": 1.2142666666666666,
        "step": 9107
    },
    {
        "loss": 0.518,
        "grad_norm": 2.587718963623047,
        "learning_rate": 1.579842940331263e-05,
        "epoch": 1.2144,
        "step": 9108
    },
    {
        "loss": 2.2266,
        "grad_norm": 2.7265994548797607,
        "learning_rate": 1.5764494663805496e-05,
        "epoch": 1.2145333333333332,
        "step": 9109
    },
    {
        "loss": 2.4312,
        "grad_norm": 2.263094186782837,
        "learning_rate": 1.5730593290269147e-05,
        "epoch": 1.2146666666666666,
        "step": 9110
    },
    {
        "loss": 2.2168,
        "grad_norm": 2.240919351577759,
        "learning_rate": 1.569672529613191e-05,
        "epoch": 1.2147999999999999,
        "step": 9111
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.9293060302734375,
        "learning_rate": 1.566289069480901e-05,
        "epoch": 1.2149333333333334,
        "step": 9112
    },
    {
        "loss": 2.4013,
        "grad_norm": 3.6952857971191406,
        "learning_rate": 1.562908949970249e-05,
        "epoch": 1.2150666666666667,
        "step": 9113
    },
    {
        "loss": 1.5992,
        "grad_norm": 3.558056592941284,
        "learning_rate": 1.5595321724201075e-05,
        "epoch": 1.2152,
        "step": 9114
    },
    {
        "loss": 2.11,
        "grad_norm": 3.381974935531616,
        "learning_rate": 1.556158738168042e-05,
        "epoch": 1.2153333333333334,
        "step": 9115
    },
    {
        "loss": 2.4959,
        "grad_norm": 4.0735182762146,
        "learning_rate": 1.552788648550263e-05,
        "epoch": 1.2154666666666667,
        "step": 9116
    },
    {
        "loss": 1.1822,
        "grad_norm": 4.563819408416748,
        "learning_rate": 1.549421904901688e-05,
        "epoch": 1.2156,
        "step": 9117
    },
    {
        "loss": 0.7846,
        "grad_norm": 2.8961243629455566,
        "learning_rate": 1.5460585085558833e-05,
        "epoch": 1.2157333333333333,
        "step": 9118
    },
    {
        "loss": 0.9292,
        "grad_norm": 3.8708155155181885,
        "learning_rate": 1.5426984608451223e-05,
        "epoch": 1.2158666666666667,
        "step": 9119
    },
    {
        "loss": 1.9303,
        "grad_norm": 3.1794259548187256,
        "learning_rate": 1.5393417631003104e-05,
        "epoch": 1.216,
        "step": 9120
    },
    {
        "loss": 2.4062,
        "grad_norm": 3.939302444458008,
        "learning_rate": 1.535988416651056e-05,
        "epoch": 1.2161333333333333,
        "step": 9121
    },
    {
        "loss": 1.728,
        "grad_norm": 5.319077014923096,
        "learning_rate": 1.5326384228256296e-05,
        "epoch": 1.2162666666666666,
        "step": 9122
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.90116286277771,
        "learning_rate": 1.5292917829509767e-05,
        "epoch": 1.2164,
        "step": 9123
    },
    {
        "loss": 1.6777,
        "grad_norm": 4.001235485076904,
        "learning_rate": 1.5259484983527182e-05,
        "epoch": 1.2165333333333332,
        "step": 9124
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.7010462284088135,
        "learning_rate": 1.5226085703551218e-05,
        "epoch": 1.2166666666666668,
        "step": 9125
    },
    {
        "loss": 2.2555,
        "grad_norm": 5.661201000213623,
        "learning_rate": 1.5192720002811645e-05,
        "epoch": 1.2168,
        "step": 9126
    },
    {
        "loss": 1.798,
        "grad_norm": 3.501190423965454,
        "learning_rate": 1.5159387894524613e-05,
        "epoch": 1.2169333333333334,
        "step": 9127
    },
    {
        "loss": 1.4605,
        "grad_norm": 3.6495511531829834,
        "learning_rate": 1.5126089391893206e-05,
        "epoch": 1.2170666666666667,
        "step": 9128
    },
    {
        "loss": 1.9041,
        "grad_norm": 3.6085734367370605,
        "learning_rate": 1.5092824508106896e-05,
        "epoch": 1.2172,
        "step": 9129
    },
    {
        "loss": 0.8613,
        "grad_norm": 3.597759962081909,
        "learning_rate": 1.5059593256342141e-05,
        "epoch": 1.2173333333333334,
        "step": 9130
    },
    {
        "loss": 2.7059,
        "grad_norm": 2.5783185958862305,
        "learning_rate": 1.5026395649761927e-05,
        "epoch": 1.2174666666666667,
        "step": 9131
    },
    {
        "loss": 1.9273,
        "grad_norm": 2.587289810180664,
        "learning_rate": 1.4993231701515954e-05,
        "epoch": 1.2176,
        "step": 9132
    },
    {
        "loss": 0.9792,
        "grad_norm": 3.6667113304138184,
        "learning_rate": 1.496010142474058e-05,
        "epoch": 1.2177333333333333,
        "step": 9133
    },
    {
        "loss": 2.3124,
        "grad_norm": 4.027429580688477,
        "learning_rate": 1.4927004832558712e-05,
        "epoch": 1.2178666666666667,
        "step": 9134
    },
    {
        "loss": 2.0634,
        "grad_norm": 3.57167387008667,
        "learning_rate": 1.4893941938080202e-05,
        "epoch": 1.218,
        "step": 9135
    },
    {
        "loss": 1.7852,
        "grad_norm": 3.1443984508514404,
        "learning_rate": 1.4860912754401246e-05,
        "epoch": 1.2181333333333333,
        "step": 9136
    },
    {
        "loss": 2.3792,
        "grad_norm": 2.6630189418792725,
        "learning_rate": 1.4827917294604843e-05,
        "epoch": 1.2182666666666666,
        "step": 9137
    },
    {
        "loss": 1.8905,
        "grad_norm": 3.4552106857299805,
        "learning_rate": 1.4794955571760616e-05,
        "epoch": 1.2184,
        "step": 9138
    },
    {
        "loss": 1.2649,
        "grad_norm": 3.159268617630005,
        "learning_rate": 1.4762027598924789e-05,
        "epoch": 1.2185333333333332,
        "step": 9139
    },
    {
        "loss": 1.4382,
        "grad_norm": 4.540618896484375,
        "learning_rate": 1.4729133389140293e-05,
        "epoch": 1.2186666666666666,
        "step": 9140
    },
    {
        "loss": 1.5732,
        "grad_norm": 2.588866710662842,
        "learning_rate": 1.4696272955436663e-05,
        "epoch": 1.2187999999999999,
        "step": 9141
    },
    {
        "loss": 1.4489,
        "grad_norm": 3.1872596740722656,
        "learning_rate": 1.4663446310829953e-05,
        "epoch": 1.2189333333333334,
        "step": 9142
    },
    {
        "loss": 2.2132,
        "grad_norm": 4.1512885093688965,
        "learning_rate": 1.4630653468322875e-05,
        "epoch": 1.2190666666666667,
        "step": 9143
    },
    {
        "loss": 2.3397,
        "grad_norm": 3.24406099319458,
        "learning_rate": 1.4597894440904935e-05,
        "epoch": 1.2192,
        "step": 9144
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.752822160720825,
        "learning_rate": 1.456516924155198e-05,
        "epoch": 1.2193333333333334,
        "step": 9145
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.800027847290039,
        "learning_rate": 1.4532477883226581e-05,
        "epoch": 1.2194666666666667,
        "step": 9146
    },
    {
        "loss": 0.801,
        "grad_norm": 3.3841447830200195,
        "learning_rate": 1.4499820378877915e-05,
        "epoch": 1.2196,
        "step": 9147
    },
    {
        "loss": 1.8826,
        "grad_norm": 3.8312933444976807,
        "learning_rate": 1.4467196741441735e-05,
        "epoch": 1.2197333333333333,
        "step": 9148
    },
    {
        "loss": 1.5946,
        "grad_norm": 7.281036853790283,
        "learning_rate": 1.4434606983840437e-05,
        "epoch": 1.2198666666666667,
        "step": 9149
    },
    {
        "loss": 1.6286,
        "grad_norm": 4.409564971923828,
        "learning_rate": 1.4402051118982762e-05,
        "epoch": 1.22,
        "step": 9150
    },
    {
        "loss": 2.2771,
        "grad_norm": 3.0396769046783447,
        "learning_rate": 1.4369529159764373e-05,
        "epoch": 1.2201333333333333,
        "step": 9151
    },
    {
        "loss": 1.985,
        "grad_norm": 3.4035117626190186,
        "learning_rate": 1.4337041119067207e-05,
        "epoch": 1.2202666666666666,
        "step": 9152
    },
    {
        "loss": 1.6456,
        "grad_norm": 1.7691479921340942,
        "learning_rate": 1.430458700976005e-05,
        "epoch": 1.2204,
        "step": 9153
    },
    {
        "loss": 1.7904,
        "grad_norm": 5.983232498168945,
        "learning_rate": 1.4272166844697876e-05,
        "epoch": 1.2205333333333332,
        "step": 9154
    },
    {
        "loss": 2.3353,
        "grad_norm": 2.4794275760650635,
        "learning_rate": 1.4239780636722555e-05,
        "epoch": 1.2206666666666668,
        "step": 9155
    },
    {
        "loss": 2.0663,
        "grad_norm": 3.553408145904541,
        "learning_rate": 1.4207428398662337e-05,
        "epoch": 1.2208,
        "step": 9156
    },
    {
        "loss": 2.6229,
        "grad_norm": 4.269548416137695,
        "learning_rate": 1.4175110143332105e-05,
        "epoch": 1.2209333333333334,
        "step": 9157
    },
    {
        "loss": 2.4971,
        "grad_norm": 3.0581893920898438,
        "learning_rate": 1.4142825883533172e-05,
        "epoch": 1.2210666666666667,
        "step": 9158
    },
    {
        "loss": 1.5721,
        "grad_norm": 3.9180803298950195,
        "learning_rate": 1.4110575632053392e-05,
        "epoch": 1.2212,
        "step": 9159
    },
    {
        "loss": 2.7314,
        "grad_norm": 2.922114610671997,
        "learning_rate": 1.407835940166734e-05,
        "epoch": 1.2213333333333334,
        "step": 9160
    },
    {
        "loss": 2.5461,
        "grad_norm": 3.7245335578918457,
        "learning_rate": 1.4046177205135825e-05,
        "epoch": 1.2214666666666667,
        "step": 9161
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.3125085830688477,
        "learning_rate": 1.4014029055206512e-05,
        "epoch": 1.2216,
        "step": 9162
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.8343145847320557,
        "learning_rate": 1.3981914964613208e-05,
        "epoch": 1.2217333333333333,
        "step": 9163
    },
    {
        "loss": 3.2246,
        "grad_norm": 2.734255313873291,
        "learning_rate": 1.3949834946076479e-05,
        "epoch": 1.2218666666666667,
        "step": 9164
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.8367605209350586,
        "learning_rate": 1.3917789012303317e-05,
        "epoch": 1.222,
        "step": 9165
    },
    {
        "loss": 2.5986,
        "grad_norm": 2.801722764968872,
        "learning_rate": 1.3885777175987324e-05,
        "epoch": 1.2221333333333333,
        "step": 9166
    },
    {
        "loss": 1.4609,
        "grad_norm": 3.9911627769470215,
        "learning_rate": 1.3853799449808392e-05,
        "epoch": 1.2222666666666666,
        "step": 9167
    },
    {
        "loss": 2.9605,
        "grad_norm": 3.358996868133545,
        "learning_rate": 1.3821855846432952e-05,
        "epoch": 1.2224,
        "step": 9168
    },
    {
        "loss": 2.4613,
        "grad_norm": 3.0980727672576904,
        "learning_rate": 1.3789946378514163e-05,
        "epoch": 1.2225333333333332,
        "step": 9169
    },
    {
        "loss": 2.7656,
        "grad_norm": 2.759930372238159,
        "learning_rate": 1.3758071058691335e-05,
        "epoch": 1.2226666666666666,
        "step": 9170
    },
    {
        "loss": 1.7913,
        "grad_norm": 4.102045059204102,
        "learning_rate": 1.3726229899590403e-05,
        "epoch": 1.2227999999999999,
        "step": 9171
    },
    {
        "loss": 1.1258,
        "grad_norm": 4.472929000854492,
        "learning_rate": 1.369442291382378e-05,
        "epoch": 1.2229333333333334,
        "step": 9172
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.2972989082336426,
        "learning_rate": 1.3662650113990316e-05,
        "epoch": 1.2230666666666667,
        "step": 9173
    },
    {
        "loss": 2.5489,
        "grad_norm": 3.8170113563537598,
        "learning_rate": 1.3630911512675404e-05,
        "epoch": 1.2232,
        "step": 9174
    },
    {
        "loss": 1.9581,
        "grad_norm": 2.773944616317749,
        "learning_rate": 1.359920712245062e-05,
        "epoch": 1.2233333333333334,
        "step": 9175
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.6463751792907715,
        "learning_rate": 1.3567536955874372e-05,
        "epoch": 1.2234666666666667,
        "step": 9176
    },
    {
        "loss": 2.4108,
        "grad_norm": 3.405973196029663,
        "learning_rate": 1.3535901025491194e-05,
        "epoch": 1.2236,
        "step": 9177
    },
    {
        "loss": 1.5168,
        "grad_norm": 6.334645748138428,
        "learning_rate": 1.3504299343832328e-05,
        "epoch": 1.2237333333333333,
        "step": 9178
    },
    {
        "loss": 1.9666,
        "grad_norm": 2.902475118637085,
        "learning_rate": 1.3472731923415138e-05,
        "epoch": 1.2238666666666667,
        "step": 9179
    },
    {
        "loss": 0.5882,
        "grad_norm": 3.571336507797241,
        "learning_rate": 1.344119877674368e-05,
        "epoch": 1.224,
        "step": 9180
    },
    {
        "loss": 2.2404,
        "grad_norm": 3.0891706943511963,
        "learning_rate": 1.3409699916308338e-05,
        "epoch": 1.2241333333333333,
        "step": 9181
    },
    {
        "loss": 2.9331,
        "grad_norm": 3.6018266677856445,
        "learning_rate": 1.3378235354585899e-05,
        "epoch": 1.2242666666666666,
        "step": 9182
    },
    {
        "loss": 2.4924,
        "grad_norm": 3.3592450618743896,
        "learning_rate": 1.3346805104039662e-05,
        "epoch": 1.2244,
        "step": 9183
    },
    {
        "loss": 1.5507,
        "grad_norm": 3.2222089767456055,
        "learning_rate": 1.3315409177119076e-05,
        "epoch": 1.2245333333333333,
        "step": 9184
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.749983549118042,
        "learning_rate": 1.3284047586260374e-05,
        "epoch": 1.2246666666666666,
        "step": 9185
    },
    {
        "loss": 2.4098,
        "grad_norm": 4.7231035232543945,
        "learning_rate": 1.3252720343885816e-05,
        "epoch": 1.2248,
        "step": 9186
    },
    {
        "loss": 2.289,
        "grad_norm": 3.2934675216674805,
        "learning_rate": 1.3221427462404423e-05,
        "epoch": 1.2249333333333334,
        "step": 9187
    },
    {
        "loss": 0.7076,
        "grad_norm": 2.315073251724243,
        "learning_rate": 1.3190168954211235e-05,
        "epoch": 1.2250666666666667,
        "step": 9188
    },
    {
        "loss": 2.153,
        "grad_norm": 3.6377787590026855,
        "learning_rate": 1.3158944831687925e-05,
        "epoch": 1.2252,
        "step": 9189
    },
    {
        "loss": 2.4278,
        "grad_norm": 4.211082458496094,
        "learning_rate": 1.3127755107202445e-05,
        "epoch": 1.2253333333333334,
        "step": 9190
    },
    {
        "loss": 1.5494,
        "grad_norm": 5.58310079574585,
        "learning_rate": 1.3096599793109244e-05,
        "epoch": 1.2254666666666667,
        "step": 9191
    },
    {
        "loss": 2.6913,
        "grad_norm": 2.219573974609375,
        "learning_rate": 1.3065478901748952e-05,
        "epoch": 1.2256,
        "step": 9192
    },
    {
        "loss": 2.0804,
        "grad_norm": 3.663980007171631,
        "learning_rate": 1.3034392445448628e-05,
        "epoch": 1.2257333333333333,
        "step": 9193
    },
    {
        "loss": 1.8054,
        "grad_norm": 3.127619743347168,
        "learning_rate": 1.3003340436521883e-05,
        "epoch": 1.2258666666666667,
        "step": 9194
    },
    {
        "loss": 1.8428,
        "grad_norm": 2.6937758922576904,
        "learning_rate": 1.2972322887268384e-05,
        "epoch": 1.226,
        "step": 9195
    },
    {
        "loss": 2.0839,
        "grad_norm": 3.790687084197998,
        "learning_rate": 1.2941339809974318e-05,
        "epoch": 1.2261333333333333,
        "step": 9196
    },
    {
        "loss": 2.1274,
        "grad_norm": 2.4165220260620117,
        "learning_rate": 1.2910391216912198e-05,
        "epoch": 1.2262666666666666,
        "step": 9197
    },
    {
        "loss": 1.7122,
        "grad_norm": 3.9642183780670166,
        "learning_rate": 1.2879477120340866e-05,
        "epoch": 1.2264,
        "step": 9198
    },
    {
        "loss": 1.6134,
        "grad_norm": 6.0640716552734375,
        "learning_rate": 1.2848597532505568e-05,
        "epoch": 1.2265333333333333,
        "step": 9199
    },
    {
        "loss": 1.5924,
        "grad_norm": 3.304453134536743,
        "learning_rate": 1.2817752465637645e-05,
        "epoch": 1.2266666666666666,
        "step": 9200
    },
    {
        "loss": 2.5277,
        "grad_norm": 4.094182968139648,
        "learning_rate": 1.2786941931955098e-05,
        "epoch": 1.2268,
        "step": 9201
    },
    {
        "loss": 2.2868,
        "grad_norm": 3.160146951675415,
        "learning_rate": 1.2756165943661979e-05,
        "epoch": 1.2269333333333332,
        "step": 9202
    },
    {
        "loss": 1.8344,
        "grad_norm": 3.3400421142578125,
        "learning_rate": 1.2725424512948847e-05,
        "epoch": 1.2270666666666667,
        "step": 9203
    },
    {
        "loss": 0.9383,
        "grad_norm": 7.829486846923828,
        "learning_rate": 1.2694717651992471e-05,
        "epoch": 1.2272,
        "step": 9204
    },
    {
        "loss": 1.8261,
        "grad_norm": 4.931262493133545,
        "learning_rate": 1.2664045372955856e-05,
        "epoch": 1.2273333333333334,
        "step": 9205
    },
    {
        "loss": 2.5148,
        "grad_norm": 2.7899553775787354,
        "learning_rate": 1.2633407687988475e-05,
        "epoch": 1.2274666666666667,
        "step": 9206
    },
    {
        "loss": 2.2728,
        "grad_norm": 2.945714235305786,
        "learning_rate": 1.2602804609226004e-05,
        "epoch": 1.2276,
        "step": 9207
    },
    {
        "loss": 2.1553,
        "grad_norm": 3.9177722930908203,
        "learning_rate": 1.2572236148790495e-05,
        "epoch": 1.2277333333333333,
        "step": 9208
    },
    {
        "loss": 1.9353,
        "grad_norm": 2.4839706420898438,
        "learning_rate": 1.2541702318790038e-05,
        "epoch": 1.2278666666666667,
        "step": 9209
    },
    {
        "loss": 2.5477,
        "grad_norm": 2.1306238174438477,
        "learning_rate": 1.2511203131319382e-05,
        "epoch": 1.228,
        "step": 9210
    },
    {
        "loss": 2.4289,
        "grad_norm": 3.704594612121582,
        "learning_rate": 1.2480738598459195e-05,
        "epoch": 1.2281333333333333,
        "step": 9211
    },
    {
        "loss": 2.7996,
        "grad_norm": 2.351360559463501,
        "learning_rate": 1.2450308732276771e-05,
        "epoch": 1.2282666666666666,
        "step": 9212
    },
    {
        "loss": 0.5552,
        "grad_norm": 3.184147834777832,
        "learning_rate": 1.2419913544825312e-05,
        "epoch": 1.2284,
        "step": 9213
    },
    {
        "loss": 2.138,
        "grad_norm": 4.1507792472839355,
        "learning_rate": 1.238955304814452e-05,
        "epoch": 1.2285333333333333,
        "step": 9214
    },
    {
        "loss": 2.7262,
        "grad_norm": 2.851475954055786,
        "learning_rate": 1.2359227254260276e-05,
        "epoch": 1.2286666666666666,
        "step": 9215
    },
    {
        "loss": 2.1725,
        "grad_norm": 4.010155200958252,
        "learning_rate": 1.2328936175184803e-05,
        "epoch": 1.2288000000000001,
        "step": 9216
    },
    {
        "loss": 1.335,
        "grad_norm": 4.184098720550537,
        "learning_rate": 1.2298679822916414e-05,
        "epoch": 1.2289333333333334,
        "step": 9217
    },
    {
        "loss": 1.5915,
        "grad_norm": 2.5449087619781494,
        "learning_rate": 1.2268458209439748e-05,
        "epoch": 1.2290666666666668,
        "step": 9218
    },
    {
        "loss": 2.1783,
        "grad_norm": 2.9161298274993896,
        "learning_rate": 1.2238271346725771e-05,
        "epoch": 1.2292,
        "step": 9219
    },
    {
        "loss": 2.6944,
        "grad_norm": 2.6746609210968018,
        "learning_rate": 1.2208119246731542e-05,
        "epoch": 1.2293333333333334,
        "step": 9220
    },
    {
        "loss": 1.7189,
        "grad_norm": 3.902536392211914,
        "learning_rate": 1.2178001921400406e-05,
        "epoch": 1.2294666666666667,
        "step": 9221
    },
    {
        "loss": 2.0463,
        "grad_norm": 3.267941474914551,
        "learning_rate": 1.2147919382661955e-05,
        "epoch": 1.2296,
        "step": 9222
    },
    {
        "loss": 0.7927,
        "grad_norm": 4.183261871337891,
        "learning_rate": 1.2117871642431988e-05,
        "epoch": 1.2297333333333333,
        "step": 9223
    },
    {
        "loss": 1.7255,
        "grad_norm": 2.984760284423828,
        "learning_rate": 1.208785871261252e-05,
        "epoch": 1.2298666666666667,
        "step": 9224
    },
    {
        "loss": 1.2355,
        "grad_norm": 5.595669746398926,
        "learning_rate": 1.2057880605091776e-05,
        "epoch": 1.23,
        "step": 9225
    },
    {
        "loss": 2.4009,
        "grad_norm": 3.5293967723846436,
        "learning_rate": 1.2027937331744188e-05,
        "epoch": 1.2301333333333333,
        "step": 9226
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.4234120845794678,
        "learning_rate": 1.1998028904430337e-05,
        "epoch": 1.2302666666666666,
        "step": 9227
    },
    {
        "loss": 2.679,
        "grad_norm": 3.0560920238494873,
        "learning_rate": 1.1968155334997145e-05,
        "epoch": 1.2304,
        "step": 9228
    },
    {
        "loss": 2.6637,
        "grad_norm": 2.3939878940582275,
        "learning_rate": 1.1938316635277601e-05,
        "epoch": 1.2305333333333333,
        "step": 9229
    },
    {
        "loss": 2.5401,
        "grad_norm": 3.594752550125122,
        "learning_rate": 1.1908512817090833e-05,
        "epoch": 1.2306666666666666,
        "step": 9230
    },
    {
        "loss": 2.2449,
        "grad_norm": 3.5112388134002686,
        "learning_rate": 1.1878743892242328e-05,
        "epoch": 1.2308,
        "step": 9231
    },
    {
        "loss": 0.5886,
        "grad_norm": 3.0637922286987305,
        "learning_rate": 1.184900987252363e-05,
        "epoch": 1.2309333333333332,
        "step": 9232
    },
    {
        "loss": 2.8509,
        "grad_norm": 2.970533847808838,
        "learning_rate": 1.1819310769712556e-05,
        "epoch": 1.2310666666666668,
        "step": 9233
    },
    {
        "loss": 2.1124,
        "grad_norm": 3.5290565490722656,
        "learning_rate": 1.1789646595572845e-05,
        "epoch": 1.2312,
        "step": 9234
    },
    {
        "loss": 2.1847,
        "grad_norm": 4.051142692565918,
        "learning_rate": 1.176001736185478e-05,
        "epoch": 1.2313333333333334,
        "step": 9235
    },
    {
        "loss": 2.2253,
        "grad_norm": 2.5798678398132324,
        "learning_rate": 1.1730423080294417e-05,
        "epoch": 1.2314666666666667,
        "step": 9236
    },
    {
        "loss": 1.6812,
        "grad_norm": 3.6278891563415527,
        "learning_rate": 1.1700863762614344e-05,
        "epoch": 1.2316,
        "step": 9237
    },
    {
        "loss": 2.1344,
        "grad_norm": 6.922723293304443,
        "learning_rate": 1.1671339420522954e-05,
        "epoch": 1.2317333333333333,
        "step": 9238
    },
    {
        "loss": 1.793,
        "grad_norm": 2.494455099105835,
        "learning_rate": 1.1641850065714987e-05,
        "epoch": 1.2318666666666667,
        "step": 9239
    },
    {
        "loss": 2.6717,
        "grad_norm": 2.3698487281799316,
        "learning_rate": 1.161239570987126e-05,
        "epoch": 1.232,
        "step": 9240
    },
    {
        "loss": 1.6435,
        "grad_norm": 4.618335723876953,
        "learning_rate": 1.1582976364658794e-05,
        "epoch": 1.2321333333333333,
        "step": 9241
    },
    {
        "loss": 1.6385,
        "grad_norm": 2.7622697353363037,
        "learning_rate": 1.155359204173071e-05,
        "epoch": 1.2322666666666666,
        "step": 9242
    },
    {
        "loss": 1.4134,
        "grad_norm": 4.319054126739502,
        "learning_rate": 1.1524242752726122e-05,
        "epoch": 1.2324,
        "step": 9243
    },
    {
        "loss": 1.09,
        "grad_norm": 3.992807388305664,
        "learning_rate": 1.1494928509270486e-05,
        "epoch": 1.2325333333333333,
        "step": 9244
    },
    {
        "loss": 2.0401,
        "grad_norm": 5.0711565017700195,
        "learning_rate": 1.1465649322975192e-05,
        "epoch": 1.2326666666666666,
        "step": 9245
    },
    {
        "loss": 2.2836,
        "grad_norm": 2.475771903991699,
        "learning_rate": 1.1436405205437972e-05,
        "epoch": 1.2328000000000001,
        "step": 9246
    },
    {
        "loss": 1.4633,
        "grad_norm": 5.261738300323486,
        "learning_rate": 1.1407196168242352e-05,
        "epoch": 1.2329333333333334,
        "step": 9247
    },
    {
        "loss": 2.2916,
        "grad_norm": 4.360983848571777,
        "learning_rate": 1.1378022222958218e-05,
        "epoch": 1.2330666666666668,
        "step": 9248
    },
    {
        "loss": 1.5609,
        "grad_norm": 3.3244078159332275,
        "learning_rate": 1.1348883381141439e-05,
        "epoch": 1.2332,
        "step": 9249
    },
    {
        "loss": 1.0604,
        "grad_norm": 3.492518901824951,
        "learning_rate": 1.131977965433405e-05,
        "epoch": 1.2333333333333334,
        "step": 9250
    },
    {
        "loss": 1.477,
        "grad_norm": 5.901626110076904,
        "learning_rate": 1.129071105406413e-05,
        "epoch": 1.2334666666666667,
        "step": 9251
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.229506254196167,
        "learning_rate": 1.1261677591845754e-05,
        "epoch": 1.2336,
        "step": 9252
    },
    {
        "loss": 2.5284,
        "grad_norm": 5.037615776062012,
        "learning_rate": 1.1232679279179314e-05,
        "epoch": 1.2337333333333333,
        "step": 9253
    },
    {
        "loss": 1.5125,
        "grad_norm": 2.417654275894165,
        "learning_rate": 1.120371612755109e-05,
        "epoch": 1.2338666666666667,
        "step": 9254
    },
    {
        "loss": 0.6094,
        "grad_norm": 2.7132670879364014,
        "learning_rate": 1.1174788148433424e-05,
        "epoch": 1.234,
        "step": 9255
    },
    {
        "loss": 1.6887,
        "grad_norm": 2.973604440689087,
        "learning_rate": 1.1145895353284853e-05,
        "epoch": 1.2341333333333333,
        "step": 9256
    },
    {
        "loss": 2.6458,
        "grad_norm": 3.420255184173584,
        "learning_rate": 1.1117037753549887e-05,
        "epoch": 1.2342666666666666,
        "step": 9257
    },
    {
        "loss": 2.4027,
        "grad_norm": 3.5011844635009766,
        "learning_rate": 1.1088215360659205e-05,
        "epoch": 1.2344,
        "step": 9258
    },
    {
        "loss": 1.9807,
        "grad_norm": 4.133405685424805,
        "learning_rate": 1.1059428186029263e-05,
        "epoch": 1.2345333333333333,
        "step": 9259
    },
    {
        "loss": 1.2147,
        "grad_norm": 3.5998995304107666,
        "learning_rate": 1.1030676241062955e-05,
        "epoch": 1.2346666666666666,
        "step": 9260
    },
    {
        "loss": 0.7837,
        "grad_norm": 4.379147052764893,
        "learning_rate": 1.1001959537148876e-05,
        "epoch": 1.2348,
        "step": 9261
    },
    {
        "loss": 2.5233,
        "grad_norm": 3.133495569229126,
        "learning_rate": 1.097327808566192e-05,
        "epoch": 1.2349333333333332,
        "step": 9262
    },
    {
        "loss": 1.6392,
        "grad_norm": 4.229416847229004,
        "learning_rate": 1.0944631897962899e-05,
        "epoch": 1.2350666666666668,
        "step": 9263
    },
    {
        "loss": 2.2708,
        "grad_norm": 5.220009803771973,
        "learning_rate": 1.091602098539859e-05,
        "epoch": 1.2352,
        "step": 9264
    },
    {
        "loss": 1.795,
        "grad_norm": 3.889937162399292,
        "learning_rate": 1.0887445359301918e-05,
        "epoch": 1.2353333333333334,
        "step": 9265
    },
    {
        "loss": 1.8456,
        "grad_norm": 2.5490384101867676,
        "learning_rate": 1.085890503099175e-05,
        "epoch": 1.2354666666666667,
        "step": 9266
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.0831892490386963,
        "learning_rate": 1.0830400011773133e-05,
        "epoch": 1.2356,
        "step": 9267
    },
    {
        "loss": 1.62,
        "grad_norm": 2.865898370742798,
        "learning_rate": 1.0801930312936825e-05,
        "epoch": 1.2357333333333334,
        "step": 9268
    },
    {
        "loss": 2.4082,
        "grad_norm": 2.84299635887146,
        "learning_rate": 1.0773495945759893e-05,
        "epoch": 1.2358666666666667,
        "step": 9269
    },
    {
        "loss": 2.1893,
        "grad_norm": 3.2376625537872314,
        "learning_rate": 1.0745096921505182e-05,
        "epoch": 1.236,
        "step": 9270
    },
    {
        "loss": 3.0235,
        "grad_norm": 4.395519733428955,
        "learning_rate": 1.0716733251421817e-05,
        "epoch": 1.2361333333333333,
        "step": 9271
    },
    {
        "loss": 1.6297,
        "grad_norm": 2.030315637588501,
        "learning_rate": 1.0688404946744578e-05,
        "epoch": 1.2362666666666666,
        "step": 9272
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.5676844120025635,
        "learning_rate": 1.0660112018694457e-05,
        "epoch": 1.2364,
        "step": 9273
    },
    {
        "loss": 2.7999,
        "grad_norm": 3.212221145629883,
        "learning_rate": 1.0631854478478386e-05,
        "epoch": 1.2365333333333333,
        "step": 9274
    },
    {
        "loss": 1.1656,
        "grad_norm": 5.2395806312561035,
        "learning_rate": 1.0603632337289293e-05,
        "epoch": 1.2366666666666666,
        "step": 9275
    },
    {
        "loss": 1.6239,
        "grad_norm": 3.1846742630004883,
        "learning_rate": 1.0575445606306067e-05,
        "epoch": 1.2368000000000001,
        "step": 9276
    },
    {
        "loss": 1.9694,
        "grad_norm": 3.7079477310180664,
        "learning_rate": 1.0547294296693456e-05,
        "epoch": 1.2369333333333334,
        "step": 9277
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.956521987915039,
        "learning_rate": 1.0519178419602427e-05,
        "epoch": 1.2370666666666668,
        "step": 9278
    },
    {
        "loss": 1.5714,
        "grad_norm": 4.396199703216553,
        "learning_rate": 1.0491097986169763e-05,
        "epoch": 1.2372,
        "step": 9279
    },
    {
        "loss": 2.2758,
        "grad_norm": 2.884965419769287,
        "learning_rate": 1.0463053007518108e-05,
        "epoch": 1.2373333333333334,
        "step": 9280
    },
    {
        "loss": 2.5326,
        "grad_norm": 2.910595417022705,
        "learning_rate": 1.043504349475627e-05,
        "epoch": 1.2374666666666667,
        "step": 9281
    },
    {
        "loss": 1.3998,
        "grad_norm": 2.815401792526245,
        "learning_rate": 1.0407069458978891e-05,
        "epoch": 1.2376,
        "step": 9282
    },
    {
        "loss": 1.4126,
        "grad_norm": 2.3944449424743652,
        "learning_rate": 1.0379130911266577e-05,
        "epoch": 1.2377333333333334,
        "step": 9283
    },
    {
        "loss": 1.5932,
        "grad_norm": 4.240428924560547,
        "learning_rate": 1.0351227862685897e-05,
        "epoch": 1.2378666666666667,
        "step": 9284
    },
    {
        "loss": 1.3887,
        "grad_norm": 4.798877239227295,
        "learning_rate": 1.0323360324289367e-05,
        "epoch": 1.238,
        "step": 9285
    },
    {
        "loss": 1.1584,
        "grad_norm": 4.147108554840088,
        "learning_rate": 1.0295528307115288e-05,
        "epoch": 1.2381333333333333,
        "step": 9286
    },
    {
        "loss": 1.9088,
        "grad_norm": 3.240596294403076,
        "learning_rate": 1.0267731822188165e-05,
        "epoch": 1.2382666666666666,
        "step": 9287
    },
    {
        "loss": 1.2349,
        "grad_norm": 4.700280666351318,
        "learning_rate": 1.0239970880518247e-05,
        "epoch": 1.2384,
        "step": 9288
    },
    {
        "loss": 2.5205,
        "grad_norm": 2.579554319381714,
        "learning_rate": 1.021224549310168e-05,
        "epoch": 1.2385333333333333,
        "step": 9289
    },
    {
        "loss": 0.5903,
        "grad_norm": 3.459970235824585,
        "learning_rate": 1.0184555670920604e-05,
        "epoch": 1.2386666666666666,
        "step": 9290
    },
    {
        "loss": 2.1869,
        "grad_norm": 2.935648202896118,
        "learning_rate": 1.0156901424943055e-05,
        "epoch": 1.2388,
        "step": 9291
    },
    {
        "loss": 2.15,
        "grad_norm": 2.859271764755249,
        "learning_rate": 1.0129282766123061e-05,
        "epoch": 1.2389333333333332,
        "step": 9292
    },
    {
        "loss": 1.5326,
        "grad_norm": 2.472593307495117,
        "learning_rate": 1.0101699705400313e-05,
        "epoch": 1.2390666666666668,
        "step": 9293
    },
    {
        "loss": 4.6283,
        "grad_norm": 4.245730876922607,
        "learning_rate": 1.0074152253700663e-05,
        "epoch": 1.2392,
        "step": 9294
    },
    {
        "loss": 2.2123,
        "grad_norm": 3.573928117752075,
        "learning_rate": 1.0046640421935672e-05,
        "epoch": 1.2393333333333334,
        "step": 9295
    },
    {
        "loss": 1.9108,
        "grad_norm": 4.280845642089844,
        "learning_rate": 1.0019164221003019e-05,
        "epoch": 1.2394666666666667,
        "step": 9296
    },
    {
        "loss": 1.2923,
        "grad_norm": 4.070681095123291,
        "learning_rate": 9.991723661785946e-06,
        "epoch": 1.2396,
        "step": 9297
    },
    {
        "loss": 2.7262,
        "grad_norm": 2.9704291820526123,
        "learning_rate": 9.964318755153845e-06,
        "epoch": 1.2397333333333334,
        "step": 9298
    },
    {
        "loss": 2.558,
        "grad_norm": 2.8090946674346924,
        "learning_rate": 9.936949511961857e-06,
        "epoch": 1.2398666666666667,
        "step": 9299
    },
    {
        "loss": 1.8794,
        "grad_norm": 5.1509013175964355,
        "learning_rate": 9.909615943051076e-06,
        "epoch": 1.24,
        "step": 9300
    },
    {
        "loss": 1.5711,
        "grad_norm": 4.521489143371582,
        "learning_rate": 9.88231805924842e-06,
        "epoch": 1.2401333333333333,
        "step": 9301
    },
    {
        "loss": 1.096,
        "grad_norm": 3.4745070934295654,
        "learning_rate": 9.85505587136657e-06,
        "epoch": 1.2402666666666666,
        "step": 9302
    },
    {
        "loss": 2.4114,
        "grad_norm": 2.542544364929199,
        "learning_rate": 9.827829390204301e-06,
        "epoch": 1.2404,
        "step": 9303
    },
    {
        "loss": 2.8144,
        "grad_norm": 2.243727445602417,
        "learning_rate": 9.800638626546032e-06,
        "epoch": 1.2405333333333333,
        "step": 9304
    },
    {
        "loss": 1.8076,
        "grad_norm": 4.3144989013671875,
        "learning_rate": 9.773483591162225e-06,
        "epoch": 1.2406666666666666,
        "step": 9305
    },
    {
        "loss": 2.0038,
        "grad_norm": 3.5081846714019775,
        "learning_rate": 9.74636429480893e-06,
        "epoch": 1.2408,
        "step": 9306
    },
    {
        "loss": 1.4421,
        "grad_norm": 3.154808521270752,
        "learning_rate": 9.719280748228288e-06,
        "epoch": 1.2409333333333334,
        "step": 9307
    },
    {
        "loss": 2.6757,
        "grad_norm": 4.4734063148498535,
        "learning_rate": 9.692232962148173e-06,
        "epoch": 1.2410666666666668,
        "step": 9308
    },
    {
        "loss": 2.7925,
        "grad_norm": 3.4707000255584717,
        "learning_rate": 9.665220947282305e-06,
        "epoch": 1.2412,
        "step": 9309
    },
    {
        "loss": 2.5644,
        "grad_norm": 2.7710487842559814,
        "learning_rate": 9.638244714330258e-06,
        "epoch": 1.2413333333333334,
        "step": 9310
    },
    {
        "loss": 1.8548,
        "grad_norm": 3.243250608444214,
        "learning_rate": 9.611304273977296e-06,
        "epoch": 1.2414666666666667,
        "step": 9311
    },
    {
        "loss": 1.4179,
        "grad_norm": 5.736625671386719,
        "learning_rate": 9.584399636894781e-06,
        "epoch": 1.2416,
        "step": 9312
    },
    {
        "loss": 1.5851,
        "grad_norm": 3.866835355758667,
        "learning_rate": 9.557530813739635e-06,
        "epoch": 1.2417333333333334,
        "step": 9313
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.8054001331329346,
        "learning_rate": 9.530697815154698e-06,
        "epoch": 1.2418666666666667,
        "step": 9314
    },
    {
        "loss": 1.7016,
        "grad_norm": 3.0878632068634033,
        "learning_rate": 9.503900651768616e-06,
        "epoch": 1.242,
        "step": 9315
    },
    {
        "loss": 2.6097,
        "grad_norm": 3.9967870712280273,
        "learning_rate": 9.477139334195817e-06,
        "epoch": 1.2421333333333333,
        "step": 9316
    },
    {
        "loss": 2.3948,
        "grad_norm": 4.660346031188965,
        "learning_rate": 9.450413873036668e-06,
        "epoch": 1.2422666666666666,
        "step": 9317
    },
    {
        "loss": 2.0076,
        "grad_norm": 2.535064935684204,
        "learning_rate": 9.423724278877033e-06,
        "epoch": 1.2424,
        "step": 9318
    },
    {
        "loss": 2.1577,
        "grad_norm": 3.0391085147857666,
        "learning_rate": 9.39707056228889e-06,
        "epoch": 1.2425333333333333,
        "step": 9319
    },
    {
        "loss": 2.3416,
        "grad_norm": 2.8309473991394043,
        "learning_rate": 9.37045273382976e-06,
        "epoch": 1.2426666666666666,
        "step": 9320
    },
    {
        "loss": 2.0692,
        "grad_norm": 2.4949910640716553,
        "learning_rate": 9.343870804043208e-06,
        "epoch": 1.2428,
        "step": 9321
    },
    {
        "loss": 0.7098,
        "grad_norm": 3.175762176513672,
        "learning_rate": 9.317324783458282e-06,
        "epoch": 1.2429333333333332,
        "step": 9322
    },
    {
        "loss": 2.1016,
        "grad_norm": 4.914890766143799,
        "learning_rate": 9.290814682589987e-06,
        "epoch": 1.2430666666666665,
        "step": 9323
    },
    {
        "loss": 2.2461,
        "grad_norm": 4.8911309242248535,
        "learning_rate": 9.264340511939097e-06,
        "epoch": 1.2432,
        "step": 9324
    },
    {
        "loss": 1.6585,
        "grad_norm": 3.122074604034424,
        "learning_rate": 9.237902281992105e-06,
        "epoch": 1.2433333333333334,
        "step": 9325
    },
    {
        "loss": 2.3331,
        "grad_norm": 3.7288105487823486,
        "learning_rate": 9.211500003221329e-06,
        "epoch": 1.2434666666666667,
        "step": 9326
    },
    {
        "loss": 2.9887,
        "grad_norm": 2.7835166454315186,
        "learning_rate": 9.185133686084668e-06,
        "epoch": 1.2436,
        "step": 9327
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.9012632369995117,
        "learning_rate": 9.158803341026057e-06,
        "epoch": 1.2437333333333334,
        "step": 9328
    },
    {
        "loss": 2.1725,
        "grad_norm": 3.0156657695770264,
        "learning_rate": 9.13250897847493e-06,
        "epoch": 1.2438666666666667,
        "step": 9329
    },
    {
        "loss": 1.7854,
        "grad_norm": 2.801722764968872,
        "learning_rate": 9.1062506088467e-06,
        "epoch": 1.244,
        "step": 9330
    },
    {
        "loss": 2.2911,
        "grad_norm": 4.005645275115967,
        "learning_rate": 9.080028242542238e-06,
        "epoch": 1.2441333333333333,
        "step": 9331
    },
    {
        "loss": 2.7925,
        "grad_norm": 4.199463367462158,
        "learning_rate": 9.053841889948422e-06,
        "epoch": 1.2442666666666666,
        "step": 9332
    },
    {
        "loss": 0.8625,
        "grad_norm": 5.1327738761901855,
        "learning_rate": 9.027691561437712e-06,
        "epoch": 1.2444,
        "step": 9333
    },
    {
        "loss": 1.2458,
        "grad_norm": 4.482820987701416,
        "learning_rate": 9.001577267368378e-06,
        "epoch": 1.2445333333333333,
        "step": 9334
    },
    {
        "loss": 2.5167,
        "grad_norm": 4.453835964202881,
        "learning_rate": 8.975499018084377e-06,
        "epoch": 1.2446666666666666,
        "step": 9335
    },
    {
        "loss": 2.7755,
        "grad_norm": 2.359482526779175,
        "learning_rate": 8.949456823915303e-06,
        "epoch": 1.2448,
        "step": 9336
    },
    {
        "loss": 1.3848,
        "grad_norm": 3.1736459732055664,
        "learning_rate": 8.923450695176694e-06,
        "epoch": 1.2449333333333334,
        "step": 9337
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.688311815261841,
        "learning_rate": 8.897480642169587e-06,
        "epoch": 1.2450666666666668,
        "step": 9338
    },
    {
        "loss": 1.8978,
        "grad_norm": 2.346233367919922,
        "learning_rate": 8.871546675180808e-06,
        "epoch": 1.2452,
        "step": 9339
    },
    {
        "loss": 2.2987,
        "grad_norm": 2.6566641330718994,
        "learning_rate": 8.845648804482898e-06,
        "epoch": 1.2453333333333334,
        "step": 9340
    },
    {
        "loss": 0.5112,
        "grad_norm": 2.3198306560516357,
        "learning_rate": 8.819787040334082e-06,
        "epoch": 1.2454666666666667,
        "step": 9341
    },
    {
        "loss": 1.1962,
        "grad_norm": 6.949309825897217,
        "learning_rate": 8.793961392978323e-06,
        "epoch": 1.2456,
        "step": 9342
    },
    {
        "loss": 1.7862,
        "grad_norm": 3.4639265537261963,
        "learning_rate": 8.76817187264527e-06,
        "epoch": 1.2457333333333334,
        "step": 9343
    },
    {
        "loss": 2.5905,
        "grad_norm": 1.110521674156189,
        "learning_rate": 8.742418489550185e-06,
        "epoch": 1.2458666666666667,
        "step": 9344
    },
    {
        "loss": 2.337,
        "grad_norm": 2.9003400802612305,
        "learning_rate": 8.716701253894033e-06,
        "epoch": 1.246,
        "step": 9345
    },
    {
        "loss": 2.4924,
        "grad_norm": 3.891964912414551,
        "learning_rate": 8.691020175863628e-06,
        "epoch": 1.2461333333333333,
        "step": 9346
    },
    {
        "loss": 2.1605,
        "grad_norm": 3.329150438308716,
        "learning_rate": 8.665375265631237e-06,
        "epoch": 1.2462666666666666,
        "step": 9347
    },
    {
        "loss": 2.8892,
        "grad_norm": 2.514704465866089,
        "learning_rate": 8.639766533354909e-06,
        "epoch": 1.2464,
        "step": 9348
    },
    {
        "loss": 1.2567,
        "grad_norm": 3.726710319519043,
        "learning_rate": 8.614193989178354e-06,
        "epoch": 1.2465333333333333,
        "step": 9349
    },
    {
        "loss": 1.7362,
        "grad_norm": 2.878993272781372,
        "learning_rate": 8.588657643230957e-06,
        "epoch": 1.2466666666666666,
        "step": 9350
    },
    {
        "loss": 2.2863,
        "grad_norm": 2.016221046447754,
        "learning_rate": 8.563157505627795e-06,
        "epoch": 1.2468,
        "step": 9351
    },
    {
        "loss": 2.4239,
        "grad_norm": 2.8099894523620605,
        "learning_rate": 8.537693586469408e-06,
        "epoch": 1.2469333333333332,
        "step": 9352
    },
    {
        "loss": 2.9045,
        "grad_norm": 2.6572835445404053,
        "learning_rate": 8.512265895842308e-06,
        "epoch": 1.2470666666666665,
        "step": 9353
    },
    {
        "loss": 1.1943,
        "grad_norm": 4.723595142364502,
        "learning_rate": 8.486874443818382e-06,
        "epoch": 1.2472,
        "step": 9354
    },
    {
        "loss": 1.9477,
        "grad_norm": 3.9542369842529297,
        "learning_rate": 8.461519240455385e-06,
        "epoch": 1.2473333333333334,
        "step": 9355
    },
    {
        "loss": 2.4191,
        "grad_norm": 3.358804225921631,
        "learning_rate": 8.436200295796449e-06,
        "epoch": 1.2474666666666667,
        "step": 9356
    },
    {
        "loss": 1.6498,
        "grad_norm": 3.025632619857788,
        "learning_rate": 8.410917619870596e-06,
        "epoch": 1.2476,
        "step": 9357
    },
    {
        "loss": 1.4539,
        "grad_norm": 3.0439188480377197,
        "learning_rate": 8.385671222692359e-06,
        "epoch": 1.2477333333333334,
        "step": 9358
    },
    {
        "loss": 3.0918,
        "grad_norm": 4.6355156898498535,
        "learning_rate": 8.360461114261953e-06,
        "epoch": 1.2478666666666667,
        "step": 9359
    },
    {
        "loss": 1.2086,
        "grad_norm": 3.8953495025634766,
        "learning_rate": 8.335287304565132e-06,
        "epoch": 1.248,
        "step": 9360
    },
    {
        "loss": 2.9604,
        "grad_norm": 3.13517689704895,
        "learning_rate": 8.310149803573297e-06,
        "epoch": 1.2481333333333333,
        "step": 9361
    },
    {
        "loss": 1.8247,
        "grad_norm": 2.5510189533233643,
        "learning_rate": 8.285048621243619e-06,
        "epoch": 1.2482666666666666,
        "step": 9362
    },
    {
        "loss": 2.2692,
        "grad_norm": 3.90731143951416,
        "learning_rate": 8.259983767518631e-06,
        "epoch": 1.2484,
        "step": 9363
    },
    {
        "loss": 2.4624,
        "grad_norm": 3.0788257122039795,
        "learning_rate": 8.234955252326748e-06,
        "epoch": 1.2485333333333333,
        "step": 9364
    },
    {
        "loss": 2.5316,
        "grad_norm": 4.850666522979736,
        "learning_rate": 8.209963085581718e-06,
        "epoch": 1.2486666666666666,
        "step": 9365
    },
    {
        "loss": 1.1305,
        "grad_norm": 6.03167724609375,
        "learning_rate": 8.185007277183088e-06,
        "epoch": 1.2488,
        "step": 9366
    },
    {
        "loss": 1.9384,
        "grad_norm": 4.48394775390625,
        "learning_rate": 8.160087837015894e-06,
        "epoch": 1.2489333333333335,
        "step": 9367
    },
    {
        "loss": 0.9619,
        "grad_norm": 3.988894462585449,
        "learning_rate": 8.135204774950912e-06,
        "epoch": 1.2490666666666668,
        "step": 9368
    },
    {
        "loss": 2.1941,
        "grad_norm": 2.9912500381469727,
        "learning_rate": 8.110358100844317e-06,
        "epoch": 1.2492,
        "step": 9369
    },
    {
        "loss": 1.5123,
        "grad_norm": 4.2962164878845215,
        "learning_rate": 8.085547824537932e-06,
        "epoch": 1.2493333333333334,
        "step": 9370
    },
    {
        "loss": 1.766,
        "grad_norm": 2.722855806350708,
        "learning_rate": 8.060773955859314e-06,
        "epoch": 1.2494666666666667,
        "step": 9371
    },
    {
        "loss": 1.8293,
        "grad_norm": 2.7856295108795166,
        "learning_rate": 8.036036504621392e-06,
        "epoch": 1.2496,
        "step": 9372
    },
    {
        "loss": 1.9732,
        "grad_norm": 3.547490119934082,
        "learning_rate": 8.01133548062275e-06,
        "epoch": 1.2497333333333334,
        "step": 9373
    },
    {
        "loss": 2.325,
        "grad_norm": 3.2965567111968994,
        "learning_rate": 7.986670893647563e-06,
        "epoch": 1.2498666666666667,
        "step": 9374
    },
    {
        "loss": 2.6757,
        "grad_norm": 1.9999384880065918,
        "learning_rate": 7.96204275346557e-06,
        "epoch": 1.25,
        "step": 9375
    },
    {
        "loss": 1.8431,
        "grad_norm": 5.22396183013916,
        "learning_rate": 7.937451069832102e-06,
        "epoch": 1.2501333333333333,
        "step": 9376
    },
    {
        "loss": 2.8237,
        "grad_norm": 1.7692333459854126,
        "learning_rate": 7.912895852487878e-06,
        "epoch": 1.2502666666666666,
        "step": 9377
    },
    {
        "loss": 2.656,
        "grad_norm": 3.7213833332061768,
        "learning_rate": 7.88837711115944e-06,
        "epoch": 1.2504,
        "step": 9378
    },
    {
        "loss": 3.0976,
        "grad_norm": 4.1689605712890625,
        "learning_rate": 7.863894855558652e-06,
        "epoch": 1.2505333333333333,
        "step": 9379
    },
    {
        "loss": 1.2431,
        "grad_norm": 4.391244411468506,
        "learning_rate": 7.839449095383122e-06,
        "epoch": 1.2506666666666666,
        "step": 9380
    },
    {
        "loss": 2.6218,
        "grad_norm": 2.105872869491577,
        "learning_rate": 7.815039840315775e-06,
        "epoch": 1.2508,
        "step": 9381
    },
    {
        "loss": 1.2681,
        "grad_norm": 2.8288824558258057,
        "learning_rate": 7.790667100025273e-06,
        "epoch": 1.2509333333333332,
        "step": 9382
    },
    {
        "loss": 1.8429,
        "grad_norm": 2.700357675552368,
        "learning_rate": 7.766330884165729e-06,
        "epoch": 1.2510666666666665,
        "step": 9383
    },
    {
        "loss": 2.3683,
        "grad_norm": 3.0548431873321533,
        "learning_rate": 7.742031202376798e-06,
        "epoch": 1.2511999999999999,
        "step": 9384
    },
    {
        "loss": 2.1884,
        "grad_norm": 3.880709409713745,
        "learning_rate": 7.717768064283726e-06,
        "epoch": 1.2513333333333334,
        "step": 9385
    },
    {
        "loss": 1.9228,
        "grad_norm": 4.6905364990234375,
        "learning_rate": 7.69354147949708e-06,
        "epoch": 1.2514666666666667,
        "step": 9386
    },
    {
        "loss": 0.5738,
        "grad_norm": 2.6264588832855225,
        "learning_rate": 7.669351457613239e-06,
        "epoch": 1.2516,
        "step": 9387
    },
    {
        "loss": 2.4996,
        "grad_norm": 3.398972988128662,
        "learning_rate": 7.645198008213838e-06,
        "epoch": 1.2517333333333334,
        "step": 9388
    },
    {
        "loss": 2.6557,
        "grad_norm": 3.3913490772247314,
        "learning_rate": 7.621081140866282e-06,
        "epoch": 1.2518666666666667,
        "step": 9389
    },
    {
        "loss": 0.931,
        "grad_norm": 3.235445022583008,
        "learning_rate": 7.597000865123205e-06,
        "epoch": 1.252,
        "step": 9390
    },
    {
        "loss": 1.8246,
        "grad_norm": 3.01804256439209,
        "learning_rate": 7.572957190522934e-06,
        "epoch": 1.2521333333333333,
        "step": 9391
    },
    {
        "loss": 2.3757,
        "grad_norm": 3.145082473754883,
        "learning_rate": 7.548950126589249e-06,
        "epoch": 1.2522666666666666,
        "step": 9392
    },
    {
        "loss": 1.6094,
        "grad_norm": 2.743351697921753,
        "learning_rate": 7.52497968283149e-06,
        "epoch": 1.2524,
        "step": 9393
    },
    {
        "loss": 2.4922,
        "grad_norm": 3.8364341259002686,
        "learning_rate": 7.5010458687443605e-06,
        "epoch": 1.2525333333333333,
        "step": 9394
    },
    {
        "loss": 2.5011,
        "grad_norm": 2.8594212532043457,
        "learning_rate": 7.477148693808089e-06,
        "epoch": 1.2526666666666666,
        "step": 9395
    },
    {
        "loss": 1.5889,
        "grad_norm": 3.708040952682495,
        "learning_rate": 7.453288167488559e-06,
        "epoch": 1.2528000000000001,
        "step": 9396
    },
    {
        "loss": 1.739,
        "grad_norm": 4.9064812660217285,
        "learning_rate": 7.42946429923691e-06,
        "epoch": 1.2529333333333335,
        "step": 9397
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.762824296951294,
        "learning_rate": 7.40567709848986e-06,
        "epoch": 1.2530666666666668,
        "step": 9398
    },
    {
        "loss": 2.5594,
        "grad_norm": 3.725813150405884,
        "learning_rate": 7.381926574669628e-06,
        "epoch": 1.2532,
        "step": 9399
    },
    {
        "loss": 1.6479,
        "grad_norm": 2.0855486392974854,
        "learning_rate": 7.358212737183856e-06,
        "epoch": 1.2533333333333334,
        "step": 9400
    },
    {
        "loss": 1.5185,
        "grad_norm": 3.562380075454712,
        "learning_rate": 7.334535595425751e-06,
        "epoch": 1.2534666666666667,
        "step": 9401
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.5969631671905518,
        "learning_rate": 7.310895158773756e-06,
        "epoch": 1.2536,
        "step": 9402
    },
    {
        "loss": 2.2719,
        "grad_norm": 3.758077383041382,
        "learning_rate": 7.28729143659207e-06,
        "epoch": 1.2537333333333334,
        "step": 9403
    },
    {
        "loss": 2.1881,
        "grad_norm": 4.4628586769104,
        "learning_rate": 7.263724438230113e-06,
        "epoch": 1.2538666666666667,
        "step": 9404
    },
    {
        "loss": 1.9822,
        "grad_norm": 3.7055611610412598,
        "learning_rate": 7.240194173022941e-06,
        "epoch": 1.254,
        "step": 9405
    },
    {
        "loss": 1.7925,
        "grad_norm": 4.3099236488342285,
        "learning_rate": 7.216700650290942e-06,
        "epoch": 1.2541333333333333,
        "step": 9406
    },
    {
        "loss": 1.7268,
        "grad_norm": 3.1760551929473877,
        "learning_rate": 7.193243879339928e-06,
        "epoch": 1.2542666666666666,
        "step": 9407
    },
    {
        "loss": 1.7413,
        "grad_norm": 3.87709379196167,
        "learning_rate": 7.1698238694612455e-06,
        "epoch": 1.2544,
        "step": 9408
    },
    {
        "loss": 1.5315,
        "grad_norm": 2.4566004276275635,
        "learning_rate": 7.14644062993165e-06,
        "epoch": 1.2545333333333333,
        "step": 9409
    },
    {
        "loss": 2.9973,
        "grad_norm": 3.2501025199890137,
        "learning_rate": 7.1230941700133554e-06,
        "epoch": 1.2546666666666666,
        "step": 9410
    },
    {
        "loss": 1.6447,
        "grad_norm": 3.7109978199005127,
        "learning_rate": 7.0997844989538435e-06,
        "epoch": 1.2548,
        "step": 9411
    },
    {
        "loss": 1.538,
        "grad_norm": 3.809459686279297,
        "learning_rate": 7.076511625986315e-06,
        "epoch": 1.2549333333333332,
        "step": 9412
    },
    {
        "loss": 3.0024,
        "grad_norm": 3.0892386436462402,
        "learning_rate": 7.053275560329098e-06,
        "epoch": 1.2550666666666666,
        "step": 9413
    },
    {
        "loss": 2.3071,
        "grad_norm": 2.986043691635132,
        "learning_rate": 7.030076311186218e-06,
        "epoch": 1.2551999999999999,
        "step": 9414
    },
    {
        "loss": 1.6673,
        "grad_norm": 2.8895347118377686,
        "learning_rate": 7.006913887746847e-06,
        "epoch": 1.2553333333333334,
        "step": 9415
    },
    {
        "loss": 1.3557,
        "grad_norm": 5.325847625732422,
        "learning_rate": 6.983788299185745e-06,
        "epoch": 1.2554666666666667,
        "step": 9416
    },
    {
        "loss": 0.7695,
        "grad_norm": 3.9552764892578125,
        "learning_rate": 6.960699554663041e-06,
        "epoch": 1.2556,
        "step": 9417
    },
    {
        "loss": 2.2741,
        "grad_norm": 2.635434150695801,
        "learning_rate": 6.9376476633243095e-06,
        "epoch": 1.2557333333333334,
        "step": 9418
    },
    {
        "loss": 0.9873,
        "grad_norm": 4.131601333618164,
        "learning_rate": 6.914632634300422e-06,
        "epoch": 1.2558666666666667,
        "step": 9419
    },
    {
        "loss": 1.3513,
        "grad_norm": 4.450596332550049,
        "learning_rate": 6.891654476707699e-06,
        "epoch": 1.256,
        "step": 9420
    },
    {
        "loss": 2.053,
        "grad_norm": 4.5010271072387695,
        "learning_rate": 6.868713199647924e-06,
        "epoch": 1.2561333333333333,
        "step": 9421
    },
    {
        "loss": 1.8145,
        "grad_norm": 3.820998191833496,
        "learning_rate": 6.845808812208188e-06,
        "epoch": 1.2562666666666666,
        "step": 9422
    },
    {
        "loss": 2.1683,
        "grad_norm": 4.279437065124512,
        "learning_rate": 6.822941323460985e-06,
        "epoch": 1.2564,
        "step": 9423
    },
    {
        "loss": 1.3486,
        "grad_norm": 3.295088529586792,
        "learning_rate": 6.800110742464216e-06,
        "epoch": 1.2565333333333333,
        "step": 9424
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.0749499797821045,
        "learning_rate": 6.777317078261148e-06,
        "epoch": 1.2566666666666666,
        "step": 9425
    },
    {
        "loss": 1.9991,
        "grad_norm": 3.6732864379882812,
        "learning_rate": 6.754560339880445e-06,
        "epoch": 1.2568,
        "step": 9426
    },
    {
        "loss": 1.7758,
        "grad_norm": 3.2387115955352783,
        "learning_rate": 6.731840536336121e-06,
        "epoch": 1.2569333333333335,
        "step": 9427
    },
    {
        "loss": 2.3749,
        "grad_norm": 3.783499002456665,
        "learning_rate": 6.7091576766275865e-06,
        "epoch": 1.2570666666666668,
        "step": 9428
    },
    {
        "loss": 2.6063,
        "grad_norm": 3.0716147422790527,
        "learning_rate": 6.6865117697395115e-06,
        "epoch": 1.2572,
        "step": 9429
    },
    {
        "loss": 1.691,
        "grad_norm": 2.609476089477539,
        "learning_rate": 6.663902824642132e-06,
        "epoch": 1.2573333333333334,
        "step": 9430
    },
    {
        "loss": 1.9589,
        "grad_norm": 3.1422154903411865,
        "learning_rate": 6.641330850290894e-06,
        "epoch": 1.2574666666666667,
        "step": 9431
    },
    {
        "loss": 1.9366,
        "grad_norm": 3.5333969593048096,
        "learning_rate": 6.61879585562657e-06,
        "epoch": 1.2576,
        "step": 9432
    },
    {
        "loss": 1.063,
        "grad_norm": 3.431525945663452,
        "learning_rate": 6.5962978495754166e-06,
        "epoch": 1.2577333333333334,
        "step": 9433
    },
    {
        "loss": 2.0382,
        "grad_norm": 3.326183319091797,
        "learning_rate": 6.573836841048941e-06,
        "epoch": 1.2578666666666667,
        "step": 9434
    },
    {
        "loss": 1.8055,
        "grad_norm": 3.816263198852539,
        "learning_rate": 6.5514128389440935e-06,
        "epoch": 1.258,
        "step": 9435
    },
    {
        "loss": 1.4218,
        "grad_norm": 3.130556583404541,
        "learning_rate": 6.529025852142945e-06,
        "epoch": 1.2581333333333333,
        "step": 9436
    },
    {
        "loss": 2.1765,
        "grad_norm": 3.6074776649475098,
        "learning_rate": 6.506675889513225e-06,
        "epoch": 1.2582666666666666,
        "step": 9437
    },
    {
        "loss": 2.3721,
        "grad_norm": 3.160372495651245,
        "learning_rate": 6.484362959907686e-06,
        "epoch": 1.2584,
        "step": 9438
    },
    {
        "loss": 1.6563,
        "grad_norm": 5.16854190826416,
        "learning_rate": 6.462087072164713e-06,
        "epoch": 1.2585333333333333,
        "step": 9439
    },
    {
        "loss": 1.9523,
        "grad_norm": 3.433933734893799,
        "learning_rate": 6.439848235107715e-06,
        "epoch": 1.2586666666666666,
        "step": 9440
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.273153066635132,
        "learning_rate": 6.4176464575456455e-06,
        "epoch": 1.2588,
        "step": 9441
    },
    {
        "loss": 1.4501,
        "grad_norm": 3.0339934825897217,
        "learning_rate": 6.3954817482726585e-06,
        "epoch": 1.2589333333333332,
        "step": 9442
    },
    {
        "loss": 1.9483,
        "grad_norm": 3.880242347717285,
        "learning_rate": 6.37335411606832e-06,
        "epoch": 1.2590666666666666,
        "step": 9443
    },
    {
        "loss": 1.4726,
        "grad_norm": 2.9776413440704346,
        "learning_rate": 6.351263569697485e-06,
        "epoch": 1.2591999999999999,
        "step": 9444
    },
    {
        "loss": 2.1764,
        "grad_norm": 2.6987082958221436,
        "learning_rate": 6.329210117910189e-06,
        "epoch": 1.2593333333333334,
        "step": 9445
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.8947324752807617,
        "learning_rate": 6.307193769441977e-06,
        "epoch": 1.2594666666666667,
        "step": 9446
    },
    {
        "loss": 1.8716,
        "grad_norm": 4.2836737632751465,
        "learning_rate": 6.2852145330135175e-06,
        "epoch": 1.2596,
        "step": 9447
    },
    {
        "loss": 1.7895,
        "grad_norm": 3.7028369903564453,
        "learning_rate": 6.263272417330979e-06,
        "epoch": 1.2597333333333334,
        "step": 9448
    },
    {
        "loss": 1.5526,
        "grad_norm": 4.173379898071289,
        "learning_rate": 6.241367431085588e-06,
        "epoch": 1.2598666666666667,
        "step": 9449
    },
    {
        "loss": 1.7527,
        "grad_norm": 3.512881278991699,
        "learning_rate": 6.219499582954036e-06,
        "epoch": 1.26,
        "step": 9450
    },
    {
        "loss": 2.7904,
        "grad_norm": 3.0988950729370117,
        "learning_rate": 6.197668881598251e-06,
        "epoch": 1.2601333333333333,
        "step": 9451
    },
    {
        "loss": 1.3799,
        "grad_norm": 3.8882172107696533,
        "learning_rate": 6.17587533566546e-06,
        "epoch": 1.2602666666666666,
        "step": 9452
    },
    {
        "loss": 1.9351,
        "grad_norm": 3.0094785690307617,
        "learning_rate": 6.154118953788157e-06,
        "epoch": 1.2604,
        "step": 9453
    },
    {
        "loss": 2.4913,
        "grad_norm": 4.937999248504639,
        "learning_rate": 6.132399744584049e-06,
        "epoch": 1.2605333333333333,
        "step": 9454
    },
    {
        "loss": 2.0184,
        "grad_norm": 2.895538330078125,
        "learning_rate": 6.110717716656289e-06,
        "epoch": 1.2606666666666666,
        "step": 9455
    },
    {
        "loss": 2.3053,
        "grad_norm": 3.4422099590301514,
        "learning_rate": 6.089072878593183e-06,
        "epoch": 1.2608,
        "step": 9456
    },
    {
        "loss": 1.7023,
        "grad_norm": 3.526064395904541,
        "learning_rate": 6.067465238968261e-06,
        "epoch": 1.2609333333333335,
        "step": 9457
    },
    {
        "loss": 1.7161,
        "grad_norm": 4.005649566650391,
        "learning_rate": 6.045894806340435e-06,
        "epoch": 1.2610666666666668,
        "step": 9458
    },
    {
        "loss": 2.408,
        "grad_norm": 3.5754048824310303,
        "learning_rate": 6.024361589253824e-06,
        "epoch": 1.2612,
        "step": 9459
    },
    {
        "loss": 0.9029,
        "grad_norm": 4.129844665527344,
        "learning_rate": 6.002865596237839e-06,
        "epoch": 1.2613333333333334,
        "step": 9460
    },
    {
        "loss": 1.5372,
        "grad_norm": 2.6669323444366455,
        "learning_rate": 5.981406835807002e-06,
        "epoch": 1.2614666666666667,
        "step": 9461
    },
    {
        "loss": 2.3046,
        "grad_norm": 4.019436836242676,
        "learning_rate": 5.959985316461347e-06,
        "epoch": 1.2616,
        "step": 9462
    },
    {
        "loss": 1.547,
        "grad_norm": 3.989206314086914,
        "learning_rate": 5.9386010466858764e-06,
        "epoch": 1.2617333333333334,
        "step": 9463
    },
    {
        "loss": 1.9282,
        "grad_norm": 4.896220684051514,
        "learning_rate": 5.917254034951081e-06,
        "epoch": 1.2618666666666667,
        "step": 9464
    },
    {
        "loss": 2.036,
        "grad_norm": 2.5017757415771484,
        "learning_rate": 5.895944289712552e-06,
        "epoch": 1.262,
        "step": 9465
    },
    {
        "loss": 2.7044,
        "grad_norm": 2.3779637813568115,
        "learning_rate": 5.874671819411126e-06,
        "epoch": 1.2621333333333333,
        "step": 9466
    },
    {
        "loss": 1.4848,
        "grad_norm": 3.226022958755493,
        "learning_rate": 5.853436632472908e-06,
        "epoch": 1.2622666666666666,
        "step": 9467
    },
    {
        "loss": 2.3699,
        "grad_norm": 2.3195576667785645,
        "learning_rate": 5.8322387373092145e-06,
        "epoch": 1.2624,
        "step": 9468
    },
    {
        "loss": 1.4573,
        "grad_norm": 3.9471492767333984,
        "learning_rate": 5.811078142316695e-06,
        "epoch": 1.2625333333333333,
        "step": 9469
    },
    {
        "loss": 2.6282,
        "grad_norm": 2.1671180725097656,
        "learning_rate": 5.789954855877e-06,
        "epoch": 1.2626666666666666,
        "step": 9470
    },
    {
        "loss": 1.4488,
        "grad_norm": 3.5597736835479736,
        "learning_rate": 5.768868886357237e-06,
        "epoch": 1.2628,
        "step": 9471
    },
    {
        "loss": 0.6513,
        "grad_norm": 3.391714096069336,
        "learning_rate": 5.747820242109536e-06,
        "epoch": 1.2629333333333332,
        "step": 9472
    },
    {
        "loss": 2.5103,
        "grad_norm": 3.248643159866333,
        "learning_rate": 5.726808931471472e-06,
        "epoch": 1.2630666666666666,
        "step": 9473
    },
    {
        "loss": 1.1401,
        "grad_norm": 4.55634880065918,
        "learning_rate": 5.705834962765577e-06,
        "epoch": 1.2631999999999999,
        "step": 9474
    },
    {
        "loss": 2.084,
        "grad_norm": 3.4806125164031982,
        "learning_rate": 5.68489834429976e-06,
        "epoch": 1.2633333333333332,
        "step": 9475
    },
    {
        "loss": 1.7695,
        "grad_norm": 3.541078805923462,
        "learning_rate": 5.663999084367067e-06,
        "epoch": 1.2634666666666667,
        "step": 9476
    },
    {
        "loss": 3.1286,
        "grad_norm": 6.139671802520752,
        "learning_rate": 5.6431371912457956e-06,
        "epoch": 1.2636,
        "step": 9477
    },
    {
        "loss": 1.7403,
        "grad_norm": 4.713547229766846,
        "learning_rate": 5.6223126731994145e-06,
        "epoch": 1.2637333333333334,
        "step": 9478
    },
    {
        "loss": 2.3257,
        "grad_norm": 4.863503456115723,
        "learning_rate": 5.601525538476504e-06,
        "epoch": 1.2638666666666667,
        "step": 9479
    },
    {
        "loss": 1.7451,
        "grad_norm": 3.1143877506256104,
        "learning_rate": 5.580775795311033e-06,
        "epoch": 1.264,
        "step": 9480
    },
    {
        "loss": 1.7237,
        "grad_norm": 3.4794535636901855,
        "learning_rate": 5.560063451922004e-06,
        "epoch": 1.2641333333333333,
        "step": 9481
    },
    {
        "loss": 2.0247,
        "grad_norm": 4.494108200073242,
        "learning_rate": 5.53938851651361e-06,
        "epoch": 1.2642666666666666,
        "step": 9482
    },
    {
        "loss": 2.2202,
        "grad_norm": 4.305825233459473,
        "learning_rate": 5.518750997275279e-06,
        "epoch": 1.2644,
        "step": 9483
    },
    {
        "loss": 2.0592,
        "grad_norm": 4.331748008728027,
        "learning_rate": 5.498150902381638e-06,
        "epoch": 1.2645333333333333,
        "step": 9484
    },
    {
        "loss": 0.4373,
        "grad_norm": 2.9223458766937256,
        "learning_rate": 5.477588239992415e-06,
        "epoch": 1.2646666666666666,
        "step": 9485
    },
    {
        "loss": 1.8708,
        "grad_norm": 4.683635711669922,
        "learning_rate": 5.457063018252595e-06,
        "epoch": 1.2648,
        "step": 9486
    },
    {
        "loss": 2.4002,
        "grad_norm": 3.768765926361084,
        "learning_rate": 5.436575245292263e-06,
        "epoch": 1.2649333333333335,
        "step": 9487
    },
    {
        "loss": 1.3393,
        "grad_norm": 2.9704620838165283,
        "learning_rate": 5.41612492922664e-06,
        "epoch": 1.2650666666666668,
        "step": 9488
    },
    {
        "loss": 2.3043,
        "grad_norm": 4.001798629760742,
        "learning_rate": 5.395712078156267e-06,
        "epoch": 1.2652,
        "step": 9489
    },
    {
        "loss": 2.1091,
        "grad_norm": 2.4684391021728516,
        "learning_rate": 5.375336700166711e-06,
        "epoch": 1.2653333333333334,
        "step": 9490
    },
    {
        "loss": 2.627,
        "grad_norm": 3.588718891143799,
        "learning_rate": 5.354998803328681e-06,
        "epoch": 1.2654666666666667,
        "step": 9491
    },
    {
        "loss": 0.673,
        "grad_norm": 3.8570170402526855,
        "learning_rate": 5.334698395698123e-06,
        "epoch": 1.2656,
        "step": 9492
    },
    {
        "loss": 1.63,
        "grad_norm": 2.8347573280334473,
        "learning_rate": 5.314435485316071e-06,
        "epoch": 1.2657333333333334,
        "step": 9493
    },
    {
        "loss": 1.245,
        "grad_norm": 4.80896520614624,
        "learning_rate": 5.294210080208828e-06,
        "epoch": 1.2658666666666667,
        "step": 9494
    },
    {
        "loss": 1.9641,
        "grad_norm": 3.2114169597625732,
        "learning_rate": 5.274022188387606e-06,
        "epoch": 1.266,
        "step": 9495
    },
    {
        "loss": 2.346,
        "grad_norm": 2.5790064334869385,
        "learning_rate": 5.2538718178489984e-06,
        "epoch": 1.2661333333333333,
        "step": 9496
    },
    {
        "loss": 1.917,
        "grad_norm": 3.253525972366333,
        "learning_rate": 5.233758976574554e-06,
        "epoch": 1.2662666666666667,
        "step": 9497
    },
    {
        "loss": 2.0336,
        "grad_norm": 3.803741931915283,
        "learning_rate": 5.2136836725311665e-06,
        "epoch": 1.2664,
        "step": 9498
    },
    {
        "loss": 2.3631,
        "grad_norm": 3.687485694885254,
        "learning_rate": 5.193645913670608e-06,
        "epoch": 1.2665333333333333,
        "step": 9499
    },
    {
        "loss": 1.8834,
        "grad_norm": 2.606210470199585,
        "learning_rate": 5.173645707929953e-06,
        "epoch": 1.2666666666666666,
        "step": 9500
    },
    {
        "loss": 2.4346,
        "grad_norm": 3.2964563369750977,
        "learning_rate": 5.15368306323134e-06,
        "epoch": 1.2668,
        "step": 9501
    },
    {
        "loss": 2.2948,
        "grad_norm": 3.0426547527313232,
        "learning_rate": 5.1337579874820575e-06,
        "epoch": 1.2669333333333332,
        "step": 9502
    },
    {
        "loss": 1.8443,
        "grad_norm": 3.884413480758667,
        "learning_rate": 5.113870488574502e-06,
        "epoch": 1.2670666666666666,
        "step": 9503
    },
    {
        "loss": 0.6701,
        "grad_norm": 2.9793286323547363,
        "learning_rate": 5.094020574386105e-06,
        "epoch": 1.2671999999999999,
        "step": 9504
    },
    {
        "loss": 2.1227,
        "grad_norm": 3.792116165161133,
        "learning_rate": 5.074208252779588e-06,
        "epoch": 1.2673333333333332,
        "step": 9505
    },
    {
        "loss": 2.6234,
        "grad_norm": 2.4514267444610596,
        "learning_rate": 5.054433531602609e-06,
        "epoch": 1.2674666666666667,
        "step": 9506
    },
    {
        "loss": 2.316,
        "grad_norm": 1.5862178802490234,
        "learning_rate": 5.0346964186880785e-06,
        "epoch": 1.2676,
        "step": 9507
    },
    {
        "loss": 2.0794,
        "grad_norm": 3.4016261100769043,
        "learning_rate": 5.014996921853832e-06,
        "epoch": 1.2677333333333334,
        "step": 9508
    },
    {
        "loss": 2.3029,
        "grad_norm": 3.2363059520721436,
        "learning_rate": 4.99533504890296e-06,
        "epoch": 1.2678666666666667,
        "step": 9509
    },
    {
        "loss": 2.2501,
        "grad_norm": 3.339855670928955,
        "learning_rate": 4.975710807623612e-06,
        "epoch": 1.268,
        "step": 9510
    },
    {
        "loss": 1.4371,
        "grad_norm": 3.405673027038574,
        "learning_rate": 4.956124205789003e-06,
        "epoch": 1.2681333333333333,
        "step": 9511
    },
    {
        "loss": 2.0281,
        "grad_norm": 3.5016183853149414,
        "learning_rate": 4.9365752511574605e-06,
        "epoch": 1.2682666666666667,
        "step": 9512
    },
    {
        "loss": 1.2155,
        "grad_norm": 3.594097852706909,
        "learning_rate": 4.917063951472334e-06,
        "epoch": 1.2684,
        "step": 9513
    },
    {
        "loss": 1.7328,
        "grad_norm": 3.807922124862671,
        "learning_rate": 4.89759031446222e-06,
        "epoch": 1.2685333333333333,
        "step": 9514
    },
    {
        "loss": 1.7291,
        "grad_norm": 3.433650016784668,
        "learning_rate": 4.878154347840613e-06,
        "epoch": 1.2686666666666666,
        "step": 9515
    },
    {
        "loss": 2.0334,
        "grad_norm": 3.9199893474578857,
        "learning_rate": 4.858756059306191e-06,
        "epoch": 1.2688,
        "step": 9516
    },
    {
        "loss": 2.4952,
        "grad_norm": 3.3889920711517334,
        "learning_rate": 4.839395456542661e-06,
        "epoch": 1.2689333333333335,
        "step": 9517
    },
    {
        "loss": 1.0434,
        "grad_norm": 3.5464508533477783,
        "learning_rate": 4.820072547218846e-06,
        "epoch": 1.2690666666666668,
        "step": 9518
    },
    {
        "loss": 2.6007,
        "grad_norm": 2.1886587142944336,
        "learning_rate": 4.800787338988655e-06,
        "epoch": 1.2692,
        "step": 9519
    },
    {
        "loss": 2.3256,
        "grad_norm": 3.4119246006011963,
        "learning_rate": 4.781539839490934e-06,
        "epoch": 1.2693333333333334,
        "step": 9520
    },
    {
        "loss": 1.3738,
        "grad_norm": 3.4681153297424316,
        "learning_rate": 4.762330056349751e-06,
        "epoch": 1.2694666666666667,
        "step": 9521
    },
    {
        "loss": 2.3228,
        "grad_norm": 3.277867555618286,
        "learning_rate": 4.743157997174119e-06,
        "epoch": 1.2696,
        "step": 9522
    },
    {
        "loss": 2.0915,
        "grad_norm": 2.5827248096466064,
        "learning_rate": 4.7240236695582305e-06,
        "epoch": 1.2697333333333334,
        "step": 9523
    },
    {
        "loss": 2.2899,
        "grad_norm": 2.3057401180267334,
        "learning_rate": 4.704927081081201e-06,
        "epoch": 1.2698666666666667,
        "step": 9524
    },
    {
        "loss": 1.6527,
        "grad_norm": 4.334000110626221,
        "learning_rate": 4.685868239307245e-06,
        "epoch": 1.27,
        "step": 9525
    },
    {
        "loss": 1.772,
        "grad_norm": 3.7788050174713135,
        "learning_rate": 4.666847151785658e-06,
        "epoch": 1.2701333333333333,
        "step": 9526
    },
    {
        "loss": 2.5151,
        "grad_norm": 3.3857202529907227,
        "learning_rate": 4.6478638260507575e-06,
        "epoch": 1.2702666666666667,
        "step": 9527
    },
    {
        "loss": 2.1325,
        "grad_norm": 3.7946219444274902,
        "learning_rate": 4.628918269621929e-06,
        "epoch": 1.2704,
        "step": 9528
    },
    {
        "loss": 2.416,
        "grad_norm": 3.7147722244262695,
        "learning_rate": 4.610010490003491e-06,
        "epoch": 1.2705333333333333,
        "step": 9529
    },
    {
        "loss": 2.0173,
        "grad_norm": 2.5480520725250244,
        "learning_rate": 4.591140494684965e-06,
        "epoch": 1.2706666666666666,
        "step": 9530
    },
    {
        "loss": 1.6358,
        "grad_norm": 3.5882294178009033,
        "learning_rate": 4.572308291140759e-06,
        "epoch": 1.2708,
        "step": 9531
    },
    {
        "loss": 1.4034,
        "grad_norm": 2.7083218097686768,
        "learning_rate": 4.553513886830452e-06,
        "epoch": 1.2709333333333332,
        "step": 9532
    },
    {
        "loss": 1.6806,
        "grad_norm": 4.6837263107299805,
        "learning_rate": 4.5347572891984655e-06,
        "epoch": 1.2710666666666666,
        "step": 9533
    },
    {
        "loss": 0.8467,
        "grad_norm": 4.056201457977295,
        "learning_rate": 4.5160385056744244e-06,
        "epoch": 1.2711999999999999,
        "step": 9534
    },
    {
        "loss": 2.3164,
        "grad_norm": 2.517329216003418,
        "learning_rate": 4.4973575436728865e-06,
        "epoch": 1.2713333333333332,
        "step": 9535
    },
    {
        "loss": 1.9425,
        "grad_norm": 7.109694480895996,
        "learning_rate": 4.4787144105934545e-06,
        "epoch": 1.2714666666666667,
        "step": 9536
    },
    {
        "loss": 2.3616,
        "grad_norm": 3.9691336154937744,
        "learning_rate": 4.460109113820732e-06,
        "epoch": 1.2716,
        "step": 9537
    },
    {
        "loss": 2.1599,
        "grad_norm": 3.1296398639678955,
        "learning_rate": 4.441541660724291e-06,
        "epoch": 1.2717333333333334,
        "step": 9538
    },
    {
        "loss": 1.4894,
        "grad_norm": 5.686707019805908,
        "learning_rate": 4.423012058658849e-06,
        "epoch": 1.2718666666666667,
        "step": 9539
    },
    {
        "loss": 1.8878,
        "grad_norm": 3.9329540729522705,
        "learning_rate": 4.404520314964e-06,
        "epoch": 1.272,
        "step": 9540
    },
    {
        "loss": 2.3129,
        "grad_norm": 3.67545747756958,
        "learning_rate": 4.386066436964365e-06,
        "epoch": 1.2721333333333333,
        "step": 9541
    },
    {
        "loss": 1.9509,
        "grad_norm": 3.1831181049346924,
        "learning_rate": 4.367650431969616e-06,
        "epoch": 1.2722666666666667,
        "step": 9542
    },
    {
        "loss": 1.1009,
        "grad_norm": 4.350292205810547,
        "learning_rate": 4.3492723072743765e-06,
        "epoch": 1.2724,
        "step": 9543
    },
    {
        "loss": 1.5293,
        "grad_norm": 4.434651851654053,
        "learning_rate": 4.3309320701583e-06,
        "epoch": 1.2725333333333333,
        "step": 9544
    },
    {
        "loss": 2.0784,
        "grad_norm": 3.836721658706665,
        "learning_rate": 4.312629727886053e-06,
        "epoch": 1.2726666666666666,
        "step": 9545
    },
    {
        "loss": 3.0995,
        "grad_norm": 2.9884800910949707,
        "learning_rate": 4.294365287707203e-06,
        "epoch": 1.2728,
        "step": 9546
    },
    {
        "loss": 1.6751,
        "grad_norm": 3.980663299560547,
        "learning_rate": 4.276138756856329e-06,
        "epoch": 1.2729333333333333,
        "step": 9547
    },
    {
        "loss": 2.4666,
        "grad_norm": 4.081807613372803,
        "learning_rate": 4.257950142553125e-06,
        "epoch": 1.2730666666666668,
        "step": 9548
    },
    {
        "loss": 1.5214,
        "grad_norm": 4.423537731170654,
        "learning_rate": 4.23979945200208e-06,
        "epoch": 1.2732,
        "step": 9549
    },
    {
        "loss": 2.1629,
        "grad_norm": 2.6687402725219727,
        "learning_rate": 4.221686692392757e-06,
        "epoch": 1.2733333333333334,
        "step": 9550
    },
    {
        "loss": 0.8545,
        "grad_norm": 4.042566299438477,
        "learning_rate": 4.203611870899693e-06,
        "epoch": 1.2734666666666667,
        "step": 9551
    },
    {
        "loss": 1.8211,
        "grad_norm": 3.1589481830596924,
        "learning_rate": 4.185574994682384e-06,
        "epoch": 1.2736,
        "step": 9552
    },
    {
        "loss": 2.4069,
        "grad_norm": 2.8912441730499268,
        "learning_rate": 4.167576070885337e-06,
        "epoch": 1.2737333333333334,
        "step": 9553
    },
    {
        "loss": 0.7858,
        "grad_norm": 3.2284531593322754,
        "learning_rate": 4.149615106637883e-06,
        "epoch": 1.2738666666666667,
        "step": 9554
    },
    {
        "loss": 1.7557,
        "grad_norm": 3.8416590690612793,
        "learning_rate": 4.13169210905453e-06,
        "epoch": 1.274,
        "step": 9555
    },
    {
        "loss": 1.6681,
        "grad_norm": 2.090325355529785,
        "learning_rate": 4.113807085234567e-06,
        "epoch": 1.2741333333333333,
        "step": 9556
    },
    {
        "loss": 1.5676,
        "grad_norm": 4.183511734008789,
        "learning_rate": 4.095960042262392e-06,
        "epoch": 1.2742666666666667,
        "step": 9557
    },
    {
        "loss": 0.6231,
        "grad_norm": 2.904388427734375,
        "learning_rate": 4.0781509872071855e-06,
        "epoch": 1.2744,
        "step": 9558
    },
    {
        "loss": 0.8208,
        "grad_norm": 3.685981512069702,
        "learning_rate": 4.060379927123225e-06,
        "epoch": 1.2745333333333333,
        "step": 9559
    },
    {
        "loss": 2.5602,
        "grad_norm": 3.4027490615844727,
        "learning_rate": 4.0426468690496824e-06,
        "epoch": 1.2746666666666666,
        "step": 9560
    },
    {
        "loss": 0.7696,
        "grad_norm": 4.840805530548096,
        "learning_rate": 4.024951820010714e-06,
        "epoch": 1.2748,
        "step": 9561
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.0530288219451904,
        "learning_rate": 4.007294787015337e-06,
        "epoch": 1.2749333333333333,
        "step": 9562
    },
    {
        "loss": 2.6426,
        "grad_norm": 2.8113315105438232,
        "learning_rate": 3.989675777057545e-06,
        "epoch": 1.2750666666666666,
        "step": 9563
    },
    {
        "loss": 2.3315,
        "grad_norm": 3.204602003097534,
        "learning_rate": 3.9720947971163765e-06,
        "epoch": 1.2752,
        "step": 9564
    },
    {
        "loss": 2.0995,
        "grad_norm": 3.0315887928009033,
        "learning_rate": 3.954551854155619e-06,
        "epoch": 1.2753333333333332,
        "step": 9565
    },
    {
        "loss": 1.1995,
        "grad_norm": 3.937237024307251,
        "learning_rate": 3.937046955124191e-06,
        "epoch": 1.2754666666666667,
        "step": 9566
    },
    {
        "loss": 1.7013,
        "grad_norm": 4.148221015930176,
        "learning_rate": 3.919580106955756e-06,
        "epoch": 1.2756,
        "step": 9567
    },
    {
        "loss": 2.4881,
        "grad_norm": 4.625699996948242,
        "learning_rate": 3.9021513165690356e-06,
        "epoch": 1.2757333333333334,
        "step": 9568
    },
    {
        "loss": 1.9185,
        "grad_norm": 3.6701481342315674,
        "learning_rate": 3.884760590867609e-06,
        "epoch": 1.2758666666666667,
        "step": 9569
    },
    {
        "loss": 1.9042,
        "grad_norm": 3.093217611312866,
        "learning_rate": 3.867407936740042e-06,
        "epoch": 1.276,
        "step": 9570
    },
    {
        "loss": 1.4137,
        "grad_norm": 2.8360164165496826,
        "learning_rate": 3.850093361059759e-06,
        "epoch": 1.2761333333333333,
        "step": 9571
    },
    {
        "loss": 2.0785,
        "grad_norm": 2.161520481109619,
        "learning_rate": 3.832816870685074e-06,
        "epoch": 1.2762666666666667,
        "step": 9572
    },
    {
        "loss": 1.1917,
        "grad_norm": 3.555065870285034,
        "learning_rate": 3.815578472459358e-06,
        "epoch": 1.2764,
        "step": 9573
    },
    {
        "loss": 2.308,
        "grad_norm": 2.538456439971924,
        "learning_rate": 3.798378173210737e-06,
        "epoch": 1.2765333333333333,
        "step": 9574
    },
    {
        "loss": 2.3956,
        "grad_norm": 3.487030029296875,
        "learning_rate": 3.7812159797523062e-06,
        "epoch": 1.2766666666666666,
        "step": 9575
    },
    {
        "loss": 2.3758,
        "grad_norm": 3.6671299934387207,
        "learning_rate": 3.764091898882083e-06,
        "epoch": 1.2768,
        "step": 9576
    },
    {
        "loss": 2.3984,
        "grad_norm": 5.335535049438477,
        "learning_rate": 3.7470059373829746e-06,
        "epoch": 1.2769333333333333,
        "step": 9577
    },
    {
        "loss": 1.3312,
        "grad_norm": 3.9565372467041016,
        "learning_rate": 3.729958102022835e-06,
        "epoch": 1.2770666666666668,
        "step": 9578
    },
    {
        "loss": 2.7172,
        "grad_norm": 4.027039527893066,
        "learning_rate": 3.71294839955425e-06,
        "epoch": 1.2772000000000001,
        "step": 9579
    },
    {
        "loss": 1.8616,
        "grad_norm": 2.357773780822754,
        "learning_rate": 3.6959768367149315e-06,
        "epoch": 1.2773333333333334,
        "step": 9580
    },
    {
        "loss": 2.3063,
        "grad_norm": 3.811699390411377,
        "learning_rate": 3.679043420227324e-06,
        "epoch": 1.2774666666666668,
        "step": 9581
    },
    {
        "loss": 0.7934,
        "grad_norm": 3.5054359436035156,
        "learning_rate": 3.662148156798861e-06,
        "epoch": 1.2776,
        "step": 9582
    },
    {
        "loss": 2.5259,
        "grad_norm": 3.232813835144043,
        "learning_rate": 3.6452910531217353e-06,
        "epoch": 1.2777333333333334,
        "step": 9583
    },
    {
        "loss": 2.0032,
        "grad_norm": 3.5914902687072754,
        "learning_rate": 3.6284721158731383e-06,
        "epoch": 1.2778666666666667,
        "step": 9584
    },
    {
        "loss": 1.5018,
        "grad_norm": 3.287745952606201,
        "learning_rate": 3.6116913517151297e-06,
        "epoch": 1.278,
        "step": 9585
    },
    {
        "loss": 2.2344,
        "grad_norm": 3.5628182888031006,
        "learning_rate": 3.5949487672946146e-06,
        "epoch": 1.2781333333333333,
        "step": 9586
    },
    {
        "loss": 2.0048,
        "grad_norm": 3.1429896354675293,
        "learning_rate": 3.578244369243411e-06,
        "epoch": 1.2782666666666667,
        "step": 9587
    },
    {
        "loss": 1.5143,
        "grad_norm": 3.530773401260376,
        "learning_rate": 3.5615781641781034e-06,
        "epoch": 1.2784,
        "step": 9588
    },
    {
        "loss": 1.5274,
        "grad_norm": 3.681185483932495,
        "learning_rate": 3.5449501587003444e-06,
        "epoch": 1.2785333333333333,
        "step": 9589
    },
    {
        "loss": 1.442,
        "grad_norm": 3.0893826484680176,
        "learning_rate": 3.528360359396443e-06,
        "epoch": 1.2786666666666666,
        "step": 9590
    },
    {
        "loss": 1.5564,
        "grad_norm": 4.4784698486328125,
        "learning_rate": 3.511808772837777e-06,
        "epoch": 1.2788,
        "step": 9591
    },
    {
        "loss": 2.0803,
        "grad_norm": 3.8792717456817627,
        "learning_rate": 3.49529540558039e-06,
        "epoch": 1.2789333333333333,
        "step": 9592
    },
    {
        "loss": 2.3276,
        "grad_norm": 2.6840178966522217,
        "learning_rate": 3.478820264165339e-06,
        "epoch": 1.2790666666666666,
        "step": 9593
    },
    {
        "loss": 2.3845,
        "grad_norm": 2.5920984745025635,
        "learning_rate": 3.462383355118448e-06,
        "epoch": 1.2792,
        "step": 9594
    },
    {
        "loss": 1.7732,
        "grad_norm": 4.459619522094727,
        "learning_rate": 3.4459846849504875e-06,
        "epoch": 1.2793333333333332,
        "step": 9595
    },
    {
        "loss": 1.7473,
        "grad_norm": 5.276932716369629,
        "learning_rate": 3.429624260156983e-06,
        "epoch": 1.2794666666666665,
        "step": 9596
    },
    {
        "loss": 1.7293,
        "grad_norm": 4.1028571128845215,
        "learning_rate": 3.4133020872183397e-06,
        "epoch": 1.2796,
        "step": 9597
    },
    {
        "loss": 2.2154,
        "grad_norm": 3.154236316680908,
        "learning_rate": 3.397018172599886e-06,
        "epoch": 1.2797333333333334,
        "step": 9598
    },
    {
        "loss": 1.4242,
        "grad_norm": 5.069326400756836,
        "learning_rate": 3.3807725227516964e-06,
        "epoch": 1.2798666666666667,
        "step": 9599
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.8985085487365723,
        "learning_rate": 3.3645651441087113e-06,
        "epoch": 1.28,
        "step": 9600
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.9252407550811768,
        "learning_rate": 3.3483960430907513e-06,
        "epoch": 1.2801333333333333,
        "step": 9601
    },
    {
        "loss": 2.4584,
        "grad_norm": 3.2697665691375732,
        "learning_rate": 3.33226522610246e-06,
        "epoch": 1.2802666666666667,
        "step": 9602
    },
    {
        "loss": 1.7346,
        "grad_norm": 3.682389259338379,
        "learning_rate": 3.3161726995333265e-06,
        "epoch": 1.2804,
        "step": 9603
    },
    {
        "loss": 2.0883,
        "grad_norm": 3.2384164333343506,
        "learning_rate": 3.300118469757574e-06,
        "epoch": 1.2805333333333333,
        "step": 9604
    },
    {
        "loss": 1.0481,
        "grad_norm": 4.547325611114502,
        "learning_rate": 3.2841025431344263e-06,
        "epoch": 1.2806666666666666,
        "step": 9605
    },
    {
        "loss": 0.7014,
        "grad_norm": 4.348245143890381,
        "learning_rate": 3.2681249260077873e-06,
        "epoch": 1.2808,
        "step": 9606
    },
    {
        "loss": 1.4614,
        "grad_norm": 4.503143787384033,
        "learning_rate": 3.2521856247065054e-06,
        "epoch": 1.2809333333333333,
        "step": 9607
    },
    {
        "loss": 1.8162,
        "grad_norm": 3.4488532543182373,
        "learning_rate": 3.236284645544152e-06,
        "epoch": 1.2810666666666668,
        "step": 9608
    },
    {
        "loss": 1.9988,
        "grad_norm": 5.952453136444092,
        "learning_rate": 3.220421994819145e-06,
        "epoch": 1.2812000000000001,
        "step": 9609
    },
    {
        "loss": 2.2472,
        "grad_norm": 4.157306671142578,
        "learning_rate": 3.204597678814758e-06,
        "epoch": 1.2813333333333334,
        "step": 9610
    },
    {
        "loss": 2.2729,
        "grad_norm": 3.6388251781463623,
        "learning_rate": 3.188811703799055e-06,
        "epoch": 1.2814666666666668,
        "step": 9611
    },
    {
        "loss": 2.1403,
        "grad_norm": 4.209617614746094,
        "learning_rate": 3.1730640760249454e-06,
        "epoch": 1.2816,
        "step": 9612
    },
    {
        "loss": 1.9714,
        "grad_norm": 3.229964017868042,
        "learning_rate": 3.157354801730028e-06,
        "epoch": 1.2817333333333334,
        "step": 9613
    },
    {
        "loss": 2.8899,
        "grad_norm": 3.1799087524414062,
        "learning_rate": 3.141683887136904e-06,
        "epoch": 1.2818666666666667,
        "step": 9614
    },
    {
        "loss": 1.4714,
        "grad_norm": 4.107885837554932,
        "learning_rate": 3.1260513384527733e-06,
        "epoch": 1.282,
        "step": 9615
    },
    {
        "loss": 1.5946,
        "grad_norm": 2.222407817840576,
        "learning_rate": 3.110457161869862e-06,
        "epoch": 1.2821333333333333,
        "step": 9616
    },
    {
        "loss": 1.9892,
        "grad_norm": 2.9363937377929688,
        "learning_rate": 3.094901363564984e-06,
        "epoch": 1.2822666666666667,
        "step": 9617
    },
    {
        "loss": 1.3082,
        "grad_norm": 4.285458564758301,
        "learning_rate": 3.0793839496998654e-06,
        "epoch": 1.2824,
        "step": 9618
    },
    {
        "loss": 2.3051,
        "grad_norm": 2.6795177459716797,
        "learning_rate": 3.063904926421002e-06,
        "epoch": 1.2825333333333333,
        "step": 9619
    },
    {
        "loss": 2.1551,
        "grad_norm": 3.76284122467041,
        "learning_rate": 3.0484642998597324e-06,
        "epoch": 1.2826666666666666,
        "step": 9620
    },
    {
        "loss": 1.5174,
        "grad_norm": 4.940459728240967,
        "learning_rate": 3.033062076132098e-06,
        "epoch": 1.2828,
        "step": 9621
    },
    {
        "loss": 2.3901,
        "grad_norm": 4.107420921325684,
        "learning_rate": 3.017698261338964e-06,
        "epoch": 1.2829333333333333,
        "step": 9622
    },
    {
        "loss": 2.2894,
        "grad_norm": 2.8477237224578857,
        "learning_rate": 3.002372861566027e-06,
        "epoch": 1.2830666666666666,
        "step": 9623
    },
    {
        "loss": 2.7238,
        "grad_norm": 3.3501152992248535,
        "learning_rate": 2.9870858828836777e-06,
        "epoch": 1.2832,
        "step": 9624
    },
    {
        "loss": 1.899,
        "grad_norm": 2.8275678157806396,
        "learning_rate": 2.971837331347227e-06,
        "epoch": 1.2833333333333332,
        "step": 9625
    },
    {
        "loss": 0.7401,
        "grad_norm": 4.293814659118652,
        "learning_rate": 2.956627212996588e-06,
        "epoch": 1.2834666666666665,
        "step": 9626
    },
    {
        "loss": 1.7086,
        "grad_norm": 3.496001720428467,
        "learning_rate": 2.9414555338565854e-06,
        "epoch": 1.2836,
        "step": 9627
    },
    {
        "loss": 0.7565,
        "grad_norm": 4.279588222503662,
        "learning_rate": 2.926322299936757e-06,
        "epoch": 1.2837333333333334,
        "step": 9628
    },
    {
        "loss": 1.8616,
        "grad_norm": 3.859703540802002,
        "learning_rate": 2.9112275172314517e-06,
        "epoch": 1.2838666666666667,
        "step": 9629
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.6938512325286865,
        "learning_rate": 2.896171191719754e-06,
        "epoch": 1.284,
        "step": 9630
    },
    {
        "loss": 2.3794,
        "grad_norm": 2.5726494789123535,
        "learning_rate": 2.8811533293654825e-06,
        "epoch": 1.2841333333333333,
        "step": 9631
    },
    {
        "loss": 1.5745,
        "grad_norm": 2.447917938232422,
        "learning_rate": 2.8661739361173336e-06,
        "epoch": 1.2842666666666667,
        "step": 9632
    },
    {
        "loss": 2.5169,
        "grad_norm": 4.188011169433594,
        "learning_rate": 2.851233017908672e-06,
        "epoch": 1.2844,
        "step": 9633
    },
    {
        "loss": 2.4239,
        "grad_norm": 3.106449842453003,
        "learning_rate": 2.836330580657609e-06,
        "epoch": 1.2845333333333333,
        "step": 9634
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.0898518562316895,
        "learning_rate": 2.821466630267089e-06,
        "epoch": 1.2846666666666666,
        "step": 9635
    },
    {
        "loss": 1.6644,
        "grad_norm": 3.5889713764190674,
        "learning_rate": 2.8066411726247688e-06,
        "epoch": 1.2848,
        "step": 9636
    },
    {
        "loss": 1.7127,
        "grad_norm": 3.378986358642578,
        "learning_rate": 2.7918542136030846e-06,
        "epoch": 1.2849333333333333,
        "step": 9637
    },
    {
        "loss": 1.9033,
        "grad_norm": 3.1472129821777344,
        "learning_rate": 2.7771057590591287e-06,
        "epoch": 1.2850666666666668,
        "step": 9638
    },
    {
        "loss": 2.1645,
        "grad_norm": 2.9972195625305176,
        "learning_rate": 2.762395814834895e-06,
        "epoch": 1.2852000000000001,
        "step": 9639
    },
    {
        "loss": 1.9367,
        "grad_norm": 4.0713043212890625,
        "learning_rate": 2.7477243867569668e-06,
        "epoch": 1.2853333333333334,
        "step": 9640
    },
    {
        "loss": 2.1453,
        "grad_norm": 4.186808109283447,
        "learning_rate": 2.73309148063684e-06,
        "epoch": 1.2854666666666668,
        "step": 9641
    },
    {
        "loss": 1.8169,
        "grad_norm": 3.169989585876465,
        "learning_rate": 2.7184971022705785e-06,
        "epoch": 1.2856,
        "step": 9642
    },
    {
        "loss": 2.5333,
        "grad_norm": 2.61482310295105,
        "learning_rate": 2.703941257439102e-06,
        "epoch": 1.2857333333333334,
        "step": 9643
    },
    {
        "loss": 1.9567,
        "grad_norm": 3.512284994125366,
        "learning_rate": 2.6894239519079988e-06,
        "epoch": 1.2858666666666667,
        "step": 9644
    },
    {
        "loss": 2.0484,
        "grad_norm": 3.8322696685791016,
        "learning_rate": 2.674945191427658e-06,
        "epoch": 1.286,
        "step": 9645
    },
    {
        "loss": 2.2653,
        "grad_norm": 4.500271320343018,
        "learning_rate": 2.6605049817331917e-06,
        "epoch": 1.2861333333333334,
        "step": 9646
    },
    {
        "loss": 2.3724,
        "grad_norm": 2.279067277908325,
        "learning_rate": 2.6461033285443247e-06,
        "epoch": 1.2862666666666667,
        "step": 9647
    },
    {
        "loss": 2.6413,
        "grad_norm": 2.9575138092041016,
        "learning_rate": 2.631740237565683e-06,
        "epoch": 1.2864,
        "step": 9648
    },
    {
        "loss": 1.873,
        "grad_norm": 4.78075647354126,
        "learning_rate": 2.617415714486482e-06,
        "epoch": 1.2865333333333333,
        "step": 9649
    },
    {
        "loss": 1.79,
        "grad_norm": 3.4656054973602295,
        "learning_rate": 2.603129764980783e-06,
        "epoch": 1.2866666666666666,
        "step": 9650
    },
    {
        "loss": 1.9205,
        "grad_norm": 3.2167766094207764,
        "learning_rate": 2.588882394707226e-06,
        "epoch": 1.2868,
        "step": 9651
    },
    {
        "loss": 1.7114,
        "grad_norm": 6.443075656890869,
        "learning_rate": 2.574673609309275e-06,
        "epoch": 1.2869333333333333,
        "step": 9652
    },
    {
        "loss": 1.7846,
        "grad_norm": 2.532959461212158,
        "learning_rate": 2.5605034144150717e-06,
        "epoch": 1.2870666666666666,
        "step": 9653
    },
    {
        "loss": 2.0883,
        "grad_norm": 2.758068084716797,
        "learning_rate": 2.5463718156374937e-06,
        "epoch": 1.2872,
        "step": 9654
    },
    {
        "loss": 2.2341,
        "grad_norm": 3.874133586883545,
        "learning_rate": 2.532278818574119e-06,
        "epoch": 1.2873333333333332,
        "step": 9655
    },
    {
        "loss": 2.513,
        "grad_norm": 3.2596242427825928,
        "learning_rate": 2.518224428807181e-06,
        "epoch": 1.2874666666666665,
        "step": 9656
    },
    {
        "loss": 1.908,
        "grad_norm": 2.1717357635498047,
        "learning_rate": 2.5042086519037276e-06,
        "epoch": 1.2876,
        "step": 9657
    },
    {
        "loss": 2.6526,
        "grad_norm": 2.4478681087493896,
        "learning_rate": 2.4902314934154516e-06,
        "epoch": 1.2877333333333334,
        "step": 9658
    },
    {
        "loss": 2.7005,
        "grad_norm": 2.1081202030181885,
        "learning_rate": 2.476292958878701e-06,
        "epoch": 1.2878666666666667,
        "step": 9659
    },
    {
        "loss": 2.1129,
        "grad_norm": 3.5490665435791016,
        "learning_rate": 2.4623930538146378e-06,
        "epoch": 1.288,
        "step": 9660
    },
    {
        "loss": 1.9735,
        "grad_norm": 3.830565929412842,
        "learning_rate": 2.4485317837290245e-06,
        "epoch": 1.2881333333333334,
        "step": 9661
    },
    {
        "loss": 2.2778,
        "grad_norm": 3.7713937759399414,
        "learning_rate": 2.4347091541124022e-06,
        "epoch": 1.2882666666666667,
        "step": 9662
    },
    {
        "loss": 1.4558,
        "grad_norm": 2.701582908630371,
        "learning_rate": 2.4209251704398807e-06,
        "epoch": 1.2884,
        "step": 9663
    },
    {
        "loss": 2.5084,
        "grad_norm": 3.4483163356781006,
        "learning_rate": 2.407179838171436e-06,
        "epoch": 1.2885333333333333,
        "step": 9664
    },
    {
        "loss": 2.1846,
        "grad_norm": 2.112072706222534,
        "learning_rate": 2.3934731627515472e-06,
        "epoch": 1.2886666666666666,
        "step": 9665
    },
    {
        "loss": 1.9066,
        "grad_norm": 3.3191726207733154,
        "learning_rate": 2.3798051496095596e-06,
        "epoch": 1.2888,
        "step": 9666
    },
    {
        "loss": 1.7244,
        "grad_norm": 2.5325968265533447,
        "learning_rate": 2.3661758041593983e-06,
        "epoch": 1.2889333333333333,
        "step": 9667
    },
    {
        "loss": 2.6648,
        "grad_norm": 2.6226956844329834,
        "learning_rate": 2.352585131799656e-06,
        "epoch": 1.2890666666666668,
        "step": 9668
    },
    {
        "loss": 1.9995,
        "grad_norm": 3.1115200519561768,
        "learning_rate": 2.339033137913671e-06,
        "epoch": 1.2892000000000001,
        "step": 9669
    },
    {
        "loss": 1.5508,
        "grad_norm": 4.995954513549805,
        "learning_rate": 2.325519827869416e-06,
        "epoch": 1.2893333333333334,
        "step": 9670
    },
    {
        "loss": 1.8825,
        "grad_norm": 4.021786689758301,
        "learning_rate": 2.3120452070196197e-06,
        "epoch": 1.2894666666666668,
        "step": 9671
    },
    {
        "loss": 2.1885,
        "grad_norm": 3.2893004417419434,
        "learning_rate": 2.2986092807015357e-06,
        "epoch": 1.2896,
        "step": 9672
    },
    {
        "loss": 1.429,
        "grad_norm": 3.4934535026550293,
        "learning_rate": 2.2852120542372514e-06,
        "epoch": 1.2897333333333334,
        "step": 9673
    },
    {
        "loss": 1.7942,
        "grad_norm": 4.497774600982666,
        "learning_rate": 2.271853532933399e-06,
        "epoch": 1.2898666666666667,
        "step": 9674
    },
    {
        "loss": 0.6472,
        "grad_norm": 4.462339401245117,
        "learning_rate": 2.2585337220814128e-06,
        "epoch": 1.29,
        "step": 9675
    },
    {
        "loss": 1.9571,
        "grad_norm": 3.184915065765381,
        "learning_rate": 2.2452526269572283e-06,
        "epoch": 1.2901333333333334,
        "step": 9676
    },
    {
        "loss": 2.5212,
        "grad_norm": 3.5966763496398926,
        "learning_rate": 2.23201025282157e-06,
        "epoch": 1.2902666666666667,
        "step": 9677
    },
    {
        "loss": 1.4026,
        "grad_norm": 4.392528533935547,
        "learning_rate": 2.2188066049197876e-06,
        "epoch": 1.2904,
        "step": 9678
    },
    {
        "loss": 1.6196,
        "grad_norm": 4.450190544128418,
        "learning_rate": 2.2056416884819077e-06,
        "epoch": 1.2905333333333333,
        "step": 9679
    },
    {
        "loss": 1.1423,
        "grad_norm": 2.8439955711364746,
        "learning_rate": 2.1925155087225815e-06,
        "epoch": 1.2906666666666666,
        "step": 9680
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.8394107818603516,
        "learning_rate": 2.179428070841094e-06,
        "epoch": 1.2908,
        "step": 9681
    },
    {
        "loss": 2.5269,
        "grad_norm": 3.4663960933685303,
        "learning_rate": 2.1663793800214993e-06,
        "epoch": 1.2909333333333333,
        "step": 9682
    },
    {
        "loss": 0.8299,
        "grad_norm": 3.5675179958343506,
        "learning_rate": 2.153369441432396e-06,
        "epoch": 1.2910666666666666,
        "step": 9683
    },
    {
        "loss": 2.1553,
        "grad_norm": 3.473860025405884,
        "learning_rate": 2.140398260227039e-06,
        "epoch": 1.2912,
        "step": 9684
    },
    {
        "loss": 1.857,
        "grad_norm": 2.9918689727783203,
        "learning_rate": 2.1274658415433746e-06,
        "epoch": 1.2913333333333332,
        "step": 9685
    },
    {
        "loss": 2.4227,
        "grad_norm": 2.5752687454223633,
        "learning_rate": 2.114572190503994e-06,
        "epoch": 1.2914666666666665,
        "step": 9686
    },
    {
        "loss": 2.3346,
        "grad_norm": 4.442715644836426,
        "learning_rate": 2.1017173122161114e-06,
        "epoch": 1.2916,
        "step": 9687
    },
    {
        "loss": 2.27,
        "grad_norm": 2.5507075786590576,
        "learning_rate": 2.0889012117715766e-06,
        "epoch": 1.2917333333333334,
        "step": 9688
    },
    {
        "loss": 0.8446,
        "grad_norm": 3.735055685043335,
        "learning_rate": 2.0761238942469173e-06,
        "epoch": 1.2918666666666667,
        "step": 9689
    },
    {
        "loss": 2.0605,
        "grad_norm": 3.0398030281066895,
        "learning_rate": 2.0633853647032075e-06,
        "epoch": 1.292,
        "step": 9690
    },
    {
        "loss": 1.3801,
        "grad_norm": 4.292514324188232,
        "learning_rate": 2.0506856281862885e-06,
        "epoch": 1.2921333333333334,
        "step": 9691
    },
    {
        "loss": 1.9147,
        "grad_norm": 3.6261422634124756,
        "learning_rate": 2.03802468972657e-06,
        "epoch": 1.2922666666666667,
        "step": 9692
    },
    {
        "loss": 2.7512,
        "grad_norm": 4.452552318572998,
        "learning_rate": 2.0254025543390396e-06,
        "epoch": 1.2924,
        "step": 9693
    },
    {
        "loss": 1.0193,
        "grad_norm": 3.111480474472046,
        "learning_rate": 2.0128192270233993e-06,
        "epoch": 1.2925333333333333,
        "step": 9694
    },
    {
        "loss": 2.0314,
        "grad_norm": 3.7090206146240234,
        "learning_rate": 2.000274712763939e-06,
        "epoch": 1.2926666666666666,
        "step": 9695
    },
    {
        "loss": 1.5562,
        "grad_norm": 3.5566256046295166,
        "learning_rate": 1.987769016529628e-06,
        "epoch": 1.2928,
        "step": 9696
    },
    {
        "loss": 1.7793,
        "grad_norm": 3.0714011192321777,
        "learning_rate": 1.9753021432739383e-06,
        "epoch": 1.2929333333333333,
        "step": 9697
    },
    {
        "loss": 1.9384,
        "grad_norm": 3.453561782836914,
        "learning_rate": 1.9628740979350967e-06,
        "epoch": 1.2930666666666666,
        "step": 9698
    },
    {
        "loss": 2.5309,
        "grad_norm": 2.95169734954834,
        "learning_rate": 1.9504848854358547e-06,
        "epoch": 1.2932000000000001,
        "step": 9699
    },
    {
        "loss": 2.2434,
        "grad_norm": 4.009751319885254,
        "learning_rate": 1.9381345106836866e-06,
        "epoch": 1.2933333333333334,
        "step": 9700
    },
    {
        "loss": 1.5627,
        "grad_norm": 3.5517728328704834,
        "learning_rate": 1.925822978570557e-06,
        "epoch": 1.2934666666666668,
        "step": 9701
    },
    {
        "loss": 2.101,
        "grad_norm": 3.8399369716644287,
        "learning_rate": 1.9135502939731208e-06,
        "epoch": 1.2936,
        "step": 9702
    },
    {
        "loss": 1.943,
        "grad_norm": 2.3685977458953857,
        "learning_rate": 1.9013164617526335e-06,
        "epoch": 1.2937333333333334,
        "step": 9703
    },
    {
        "loss": 1.483,
        "grad_norm": 3.724665880203247,
        "learning_rate": 1.8891214867549745e-06,
        "epoch": 1.2938666666666667,
        "step": 9704
    },
    {
        "loss": 1.813,
        "grad_norm": 3.5909371376037598,
        "learning_rate": 1.876965373810602e-06,
        "epoch": 1.294,
        "step": 9705
    },
    {
        "loss": 1.8304,
        "grad_norm": 4.5807671546936035,
        "learning_rate": 1.864848127734553e-06,
        "epoch": 1.2941333333333334,
        "step": 9706
    },
    {
        "loss": 1.7805,
        "grad_norm": 2.858614683151245,
        "learning_rate": 1.8527697533265775e-06,
        "epoch": 1.2942666666666667,
        "step": 9707
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.5438547134399414,
        "learning_rate": 1.8407302553709148e-06,
        "epoch": 1.2944,
        "step": 9708
    },
    {
        "loss": 2.2965,
        "grad_norm": 1.9157766103744507,
        "learning_rate": 1.828729638636506e-06,
        "epoch": 1.2945333333333333,
        "step": 9709
    },
    {
        "loss": 0.8496,
        "grad_norm": 3.3789002895355225,
        "learning_rate": 1.8167679078767486e-06,
        "epoch": 1.2946666666666666,
        "step": 9710
    },
    {
        "loss": 1.8079,
        "grad_norm": 4.07616662979126,
        "learning_rate": 1.8048450678297968e-06,
        "epoch": 1.2948,
        "step": 9711
    },
    {
        "loss": 1.7085,
        "grad_norm": 5.235418796539307,
        "learning_rate": 1.7929611232183064e-06,
        "epoch": 1.2949333333333333,
        "step": 9712
    },
    {
        "loss": 2.1921,
        "grad_norm": 3.75557279586792,
        "learning_rate": 1.781116078749545e-06,
        "epoch": 1.2950666666666666,
        "step": 9713
    },
    {
        "loss": 1.6037,
        "grad_norm": 3.0369412899017334,
        "learning_rate": 1.7693099391153933e-06,
        "epoch": 1.2952,
        "step": 9714
    },
    {
        "loss": 2.5424,
        "grad_norm": 2.9794697761535645,
        "learning_rate": 1.7575427089922547e-06,
        "epoch": 1.2953333333333332,
        "step": 9715
    },
    {
        "loss": 1.0838,
        "grad_norm": 4.2843756675720215,
        "learning_rate": 1.745814393041234e-06,
        "epoch": 1.2954666666666665,
        "step": 9716
    },
    {
        "loss": 1.808,
        "grad_norm": 3.206967830657959,
        "learning_rate": 1.7341249959079153e-06,
        "epoch": 1.2955999999999999,
        "step": 9717
    },
    {
        "loss": 1.6826,
        "grad_norm": 3.56947660446167,
        "learning_rate": 1.7224745222225057e-06,
        "epoch": 1.2957333333333334,
        "step": 9718
    },
    {
        "loss": 1.5312,
        "grad_norm": 3.7655279636383057,
        "learning_rate": 1.710862976599814e-06,
        "epoch": 1.2958666666666667,
        "step": 9719
    },
    {
        "loss": 2.7236,
        "grad_norm": 2.7646689414978027,
        "learning_rate": 1.6992903636392054e-06,
        "epoch": 1.296,
        "step": 9720
    },
    {
        "loss": 2.4123,
        "grad_norm": 2.725599765777588,
        "learning_rate": 1.6877566879246687e-06,
        "epoch": 1.2961333333333334,
        "step": 9721
    },
    {
        "loss": 2.3868,
        "grad_norm": 3.453118085861206,
        "learning_rate": 1.6762619540246605e-06,
        "epoch": 1.2962666666666667,
        "step": 9722
    },
    {
        "loss": 2.5858,
        "grad_norm": 3.05072283744812,
        "learning_rate": 1.6648061664923386e-06,
        "epoch": 1.2964,
        "step": 9723
    },
    {
        "loss": 1.246,
        "grad_norm": 5.844072341918945,
        "learning_rate": 1.6533893298653401e-06,
        "epoch": 1.2965333333333333,
        "step": 9724
    },
    {
        "loss": 2.4655,
        "grad_norm": 3.6289947032928467,
        "learning_rate": 1.6420114486659698e-06,
        "epoch": 1.2966666666666666,
        "step": 9725
    },
    {
        "loss": 0.9413,
        "grad_norm": 3.8487472534179688,
        "learning_rate": 1.6306725274010003e-06,
        "epoch": 1.2968,
        "step": 9726
    },
    {
        "loss": 2.0081,
        "grad_norm": 3.737996816635132,
        "learning_rate": 1.6193725705618168e-06,
        "epoch": 1.2969333333333333,
        "step": 9727
    },
    {
        "loss": 1.8007,
        "grad_norm": 1.407361626625061,
        "learning_rate": 1.6081115826243941e-06,
        "epoch": 1.2970666666666666,
        "step": 9728
    },
    {
        "loss": 2.072,
        "grad_norm": 3.622236728668213,
        "learning_rate": 1.5968895680492202e-06,
        "epoch": 1.2972000000000001,
        "step": 9729
    },
    {
        "loss": 2.383,
        "grad_norm": 3.2580106258392334,
        "learning_rate": 1.585706531281428e-06,
        "epoch": 1.2973333333333334,
        "step": 9730
    },
    {
        "loss": 2.163,
        "grad_norm": 3.8253540992736816,
        "learning_rate": 1.5745624767505629e-06,
        "epoch": 1.2974666666666668,
        "step": 9731
    },
    {
        "loss": 1.514,
        "grad_norm": 3.7403619289398193,
        "learning_rate": 1.5634574088709163e-06,
        "epoch": 1.2976,
        "step": 9732
    },
    {
        "loss": 2.3419,
        "grad_norm": 5.101190090179443,
        "learning_rate": 1.552391332041181e-06,
        "epoch": 1.2977333333333334,
        "step": 9733
    },
    {
        "loss": 2.3357,
        "grad_norm": 4.263604640960693,
        "learning_rate": 1.541364250644717e-06,
        "epoch": 1.2978666666666667,
        "step": 9734
    },
    {
        "loss": 0.6722,
        "grad_norm": 4.005423069000244,
        "learning_rate": 1.5303761690493412e-06,
        "epoch": 1.298,
        "step": 9735
    },
    {
        "loss": 1.7094,
        "grad_norm": 3.8178915977478027,
        "learning_rate": 1.519427091607495e-06,
        "epoch": 1.2981333333333334,
        "step": 9736
    },
    {
        "loss": 2.5574,
        "grad_norm": 2.6045897006988525,
        "learning_rate": 1.5085170226561417e-06,
        "epoch": 1.2982666666666667,
        "step": 9737
    },
    {
        "loss": 2.3374,
        "grad_norm": 3.865720748901367,
        "learning_rate": 1.4976459665168141e-06,
        "epoch": 1.2984,
        "step": 9738
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.9367990493774414,
        "learning_rate": 1.486813927495545e-06,
        "epoch": 1.2985333333333333,
        "step": 9739
    },
    {
        "loss": 1.8787,
        "grad_norm": 2.418782949447632,
        "learning_rate": 1.4760209098829358e-06,
        "epoch": 1.2986666666666666,
        "step": 9740
    },
    {
        "loss": 2.5831,
        "grad_norm": 2.9006407260894775,
        "learning_rate": 1.4652669179541889e-06,
        "epoch": 1.2988,
        "step": 9741
    },
    {
        "loss": 1.2201,
        "grad_norm": 3.540351390838623,
        "learning_rate": 1.4545519559689524e-06,
        "epoch": 1.2989333333333333,
        "step": 9742
    },
    {
        "loss": 1.9015,
        "grad_norm": 4.054115295410156,
        "learning_rate": 1.4438760281714648e-06,
        "epoch": 1.2990666666666666,
        "step": 9743
    },
    {
        "loss": 1.55,
        "grad_norm": 4.852797031402588,
        "learning_rate": 1.4332391387905098e-06,
        "epoch": 1.2992,
        "step": 9744
    },
    {
        "loss": 1.4883,
        "grad_norm": 3.9105818271636963,
        "learning_rate": 1.4226412920393729e-06,
        "epoch": 1.2993333333333332,
        "step": 9745
    },
    {
        "loss": 2.3918,
        "grad_norm": 3.8821897506713867,
        "learning_rate": 1.4120824921159292e-06,
        "epoch": 1.2994666666666665,
        "step": 9746
    },
    {
        "loss": 2.4345,
        "grad_norm": 3.3271942138671875,
        "learning_rate": 1.4015627432025558e-06,
        "epoch": 1.2995999999999999,
        "step": 9747
    },
    {
        "loss": 2.4323,
        "grad_norm": 2.033210277557373,
        "learning_rate": 1.3910820494661414e-06,
        "epoch": 1.2997333333333334,
        "step": 9748
    },
    {
        "loss": 1.8282,
        "grad_norm": 2.506103754043579,
        "learning_rate": 1.3806404150580988e-06,
        "epoch": 1.2998666666666667,
        "step": 9749
    },
    {
        "loss": 2.3943,
        "grad_norm": 3.307583808898926,
        "learning_rate": 1.3702378441144524e-06,
        "epoch": 1.3,
        "step": 9750
    },
    {
        "loss": 0.9944,
        "grad_norm": 4.381242752075195,
        "learning_rate": 1.3598743407556736e-06,
        "epoch": 1.3001333333333334,
        "step": 9751
    },
    {
        "loss": 1.8472,
        "grad_norm": 3.0938432216644287,
        "learning_rate": 1.349549909086756e-06,
        "epoch": 1.3002666666666667,
        "step": 9752
    },
    {
        "loss": 1.7577,
        "grad_norm": 4.962100028991699,
        "learning_rate": 1.3392645531972726e-06,
        "epoch": 1.3004,
        "step": 9753
    },
    {
        "loss": 2.0399,
        "grad_norm": 2.8840713500976562,
        "learning_rate": 1.3290182771612757e-06,
        "epoch": 1.3005333333333333,
        "step": 9754
    },
    {
        "loss": 0.8692,
        "grad_norm": 3.7114689350128174,
        "learning_rate": 1.3188110850373635e-06,
        "epoch": 1.3006666666666666,
        "step": 9755
    },
    {
        "loss": 2.6965,
        "grad_norm": 2.435290575027466,
        "learning_rate": 1.308642980868602e-06,
        "epoch": 1.3008,
        "step": 9756
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.1538989543914795,
        "learning_rate": 1.2985139686826575e-06,
        "epoch": 1.3009333333333333,
        "step": 9757
    },
    {
        "loss": 2.0782,
        "grad_norm": 3.6841118335723877,
        "learning_rate": 1.2884240524916323e-06,
        "epoch": 1.3010666666666666,
        "step": 9758
    },
    {
        "loss": 2.0843,
        "grad_norm": 4.090490818023682,
        "learning_rate": 1.2783732362922296e-06,
        "epoch": 1.3012000000000001,
        "step": 9759
    },
    {
        "loss": 1.9541,
        "grad_norm": 3.47599720954895,
        "learning_rate": 1.2683615240655311e-06,
        "epoch": 1.3013333333333335,
        "step": 9760
    },
    {
        "loss": 2.0187,
        "grad_norm": 3.181666612625122,
        "learning_rate": 1.2583889197772648e-06,
        "epoch": 1.3014666666666668,
        "step": 9761
    },
    {
        "loss": 2.0956,
        "grad_norm": 3.8271732330322266,
        "learning_rate": 1.2484554273776039e-06,
        "epoch": 1.3016,
        "step": 9762
    },
    {
        "loss": 2.1924,
        "grad_norm": 3.8843493461608887,
        "learning_rate": 1.2385610508012458e-06,
        "epoch": 1.3017333333333334,
        "step": 9763
    },
    {
        "loss": 0.7649,
        "grad_norm": 2.1049702167510986,
        "learning_rate": 1.2287057939673774e-06,
        "epoch": 1.3018666666666667,
        "step": 9764
    },
    {
        "loss": 0.9094,
        "grad_norm": 3.499965190887451,
        "learning_rate": 1.2188896607796763e-06,
        "epoch": 1.302,
        "step": 9765
    },
    {
        "loss": 2.2979,
        "grad_norm": 4.2813239097595215,
        "learning_rate": 1.2091126551263875e-06,
        "epoch": 1.3021333333333334,
        "step": 9766
    },
    {
        "loss": 2.8936,
        "grad_norm": 3.6669018268585205,
        "learning_rate": 1.1993747808801802e-06,
        "epoch": 1.3022666666666667,
        "step": 9767
    },
    {
        "loss": 2.127,
        "grad_norm": 3.3117849826812744,
        "learning_rate": 1.1896760418983022e-06,
        "epoch": 1.3024,
        "step": 9768
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.2635498046875,
        "learning_rate": 1.1800164420224136e-06,
        "epoch": 1.3025333333333333,
        "step": 9769
    },
    {
        "loss": 2.2257,
        "grad_norm": 2.9736485481262207,
        "learning_rate": 1.1703959850787427e-06,
        "epoch": 1.3026666666666666,
        "step": 9770
    },
    {
        "loss": 1.7261,
        "grad_norm": 3.9701993465423584,
        "learning_rate": 1.1608146748779637e-06,
        "epoch": 1.3028,
        "step": 9771
    },
    {
        "loss": 1.6711,
        "grad_norm": 3.3552258014678955,
        "learning_rate": 1.151272515215296e-06,
        "epoch": 1.3029333333333333,
        "step": 9772
    },
    {
        "loss": 2.6681,
        "grad_norm": 3.69989275932312,
        "learning_rate": 1.141769509870394e-06,
        "epoch": 1.3030666666666666,
        "step": 9773
    },
    {
        "loss": 1.1978,
        "grad_norm": 2.8993258476257324,
        "learning_rate": 1.1323056626074246e-06,
        "epoch": 1.3032,
        "step": 9774
    },
    {
        "loss": 1.8066,
        "grad_norm": 3.325666904449463,
        "learning_rate": 1.1228809771750891e-06,
        "epoch": 1.3033333333333332,
        "step": 9775
    },
    {
        "loss": 2.226,
        "grad_norm": 3.426016092300415,
        "learning_rate": 1.1134954573065126e-06,
        "epoch": 1.3034666666666666,
        "step": 9776
    },
    {
        "loss": 1.4962,
        "grad_norm": 3.9693379402160645,
        "learning_rate": 1.1041491067193211e-06,
        "epoch": 1.3035999999999999,
        "step": 9777
    },
    {
        "loss": 2.2802,
        "grad_norm": 2.3073959350585938,
        "learning_rate": 1.094841929115653e-06,
        "epoch": 1.3037333333333334,
        "step": 9778
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.10863995552063,
        "learning_rate": 1.0855739281821041e-06,
        "epoch": 1.3038666666666667,
        "step": 9779
    },
    {
        "loss": 1.1748,
        "grad_norm": 5.917725563049316,
        "learning_rate": 1.0763451075897935e-06,
        "epoch": 1.304,
        "step": 9780
    },
    {
        "loss": 2.2834,
        "grad_norm": 2.9739983081817627,
        "learning_rate": 1.0671554709942189e-06,
        "epoch": 1.3041333333333334,
        "step": 9781
    },
    {
        "loss": 2.5202,
        "grad_norm": 3.846513509750366,
        "learning_rate": 1.0580050220354798e-06,
        "epoch": 1.3042666666666667,
        "step": 9782
    },
    {
        "loss": 1.0984,
        "grad_norm": 4.416444301605225,
        "learning_rate": 1.048893764338088e-06,
        "epoch": 1.3044,
        "step": 9783
    },
    {
        "loss": 1.6504,
        "grad_norm": 4.977077960968018,
        "learning_rate": 1.0398217015110677e-06,
        "epoch": 1.3045333333333333,
        "step": 9784
    },
    {
        "loss": 1.9425,
        "grad_norm": 3.826233386993408,
        "learning_rate": 1.0307888371478447e-06,
        "epoch": 1.3046666666666666,
        "step": 9785
    },
    {
        "loss": 1.841,
        "grad_norm": 4.240045547485352,
        "learning_rate": 1.0217951748263899e-06,
        "epoch": 1.3048,
        "step": 9786
    },
    {
        "loss": 1.7644,
        "grad_norm": 2.5171942710876465,
        "learning_rate": 1.0128407181091315e-06,
        "epoch": 1.3049333333333333,
        "step": 9787
    },
    {
        "loss": 1.5436,
        "grad_norm": 3.5637409687042236,
        "learning_rate": 1.0039254705429546e-06,
        "epoch": 1.3050666666666666,
        "step": 9788
    },
    {
        "loss": 2.0636,
        "grad_norm": 4.633299827575684,
        "learning_rate": 9.950494356592344e-07,
        "epoch": 1.3052000000000001,
        "step": 9789
    },
    {
        "loss": 1.9251,
        "grad_norm": 2.8539981842041016,
        "learning_rate": 9.862126169737584e-07,
        "epoch": 1.3053333333333335,
        "step": 9790
    },
    {
        "loss": 1.7659,
        "grad_norm": 4.077932357788086,
        "learning_rate": 9.774150179868712e-07,
        "epoch": 1.3054666666666668,
        "step": 9791
    },
    {
        "loss": 1.7045,
        "grad_norm": 5.182852745056152,
        "learning_rate": 9.686566421832854e-07,
        "epoch": 1.3056,
        "step": 9792
    },
    {
        "loss": 2.0623,
        "grad_norm": 8.180014610290527,
        "learning_rate": 9.599374930322701e-07,
        "epoch": 1.3057333333333334,
        "step": 9793
    },
    {
        "loss": 2.1107,
        "grad_norm": 3.697052001953125,
        "learning_rate": 9.51257573987463e-07,
        "epoch": 1.3058666666666667,
        "step": 9794
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.7682254314422607,
        "learning_rate": 9.426168884870356e-07,
        "epoch": 1.306,
        "step": 9795
    },
    {
        "loss": 2.1085,
        "grad_norm": 3.7893011569976807,
        "learning_rate": 9.340154399535838e-07,
        "epoch": 1.3061333333333334,
        "step": 9796
    },
    {
        "loss": 2.2383,
        "grad_norm": 3.4029014110565186,
        "learning_rate": 9.254532317941933e-07,
        "epoch": 1.3062666666666667,
        "step": 9797
    },
    {
        "loss": 2.2231,
        "grad_norm": 2.929973602294922,
        "learning_rate": 9.169302674003621e-07,
        "epoch": 1.3064,
        "step": 9798
    },
    {
        "loss": 1.4338,
        "grad_norm": 4.838291168212891,
        "learning_rate": 9.084465501480455e-07,
        "epoch": 1.3065333333333333,
        "step": 9799
    },
    {
        "loss": 2.7692,
        "grad_norm": 2.8668079376220703,
        "learning_rate": 9.000020833977219e-07,
        "epoch": 1.3066666666666666,
        "step": 9800
    },
    {
        "loss": 1.6171,
        "grad_norm": 4.10960054397583,
        "learning_rate": 8.915968704942379e-07,
        "epoch": 1.3068,
        "step": 9801
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.017925977706909,
        "learning_rate": 8.832309147669415e-07,
        "epoch": 1.3069333333333333,
        "step": 9802
    },
    {
        "loss": 2.0579,
        "grad_norm": 2.3025524616241455,
        "learning_rate": 8.749042195296042e-07,
        "epoch": 1.3070666666666666,
        "step": 9803
    },
    {
        "loss": 1.7334,
        "grad_norm": 3.212285041809082,
        "learning_rate": 8.666167880804654e-07,
        "epoch": 1.3072,
        "step": 9804
    },
    {
        "loss": 1.4783,
        "grad_norm": 3.5521180629730225,
        "learning_rate": 8.583686237022326e-07,
        "epoch": 1.3073333333333332,
        "step": 9805
    },
    {
        "loss": 1.7138,
        "grad_norm": 4.203660488128662,
        "learning_rate": 8.50159729661959e-07,
        "epoch": 1.3074666666666666,
        "step": 9806
    },
    {
        "loss": 2.2466,
        "grad_norm": 4.032647609710693,
        "learning_rate": 8.419901092112881e-07,
        "epoch": 1.3075999999999999,
        "step": 9807
    },
    {
        "loss": 2.363,
        "grad_norm": 5.671744346618652,
        "learning_rate": 8.33859765586198e-07,
        "epoch": 1.3077333333333334,
        "step": 9808
    },
    {
        "loss": 2.3012,
        "grad_norm": 2.7378945350646973,
        "learning_rate": 8.257687020071681e-07,
        "epoch": 1.3078666666666667,
        "step": 9809
    },
    {
        "loss": 2.1791,
        "grad_norm": 4.075369834899902,
        "learning_rate": 8.177169216790903e-07,
        "epoch": 1.308,
        "step": 9810
    },
    {
        "loss": 1.8729,
        "grad_norm": 4.661862373352051,
        "learning_rate": 8.097044277912691e-07,
        "epoch": 1.3081333333333334,
        "step": 9811
    },
    {
        "loss": 2.7021,
        "grad_norm": 7.277341842651367,
        "learning_rate": 8.017312235175212e-07,
        "epoch": 1.3082666666666667,
        "step": 9812
    },
    {
        "loss": 2.0069,
        "grad_norm": 4.262632846832275,
        "learning_rate": 7.937973120160314e-07,
        "epoch": 1.3084,
        "step": 9813
    },
    {
        "loss": 2.1557,
        "grad_norm": 3.414659023284912,
        "learning_rate": 7.859026964294968e-07,
        "epoch": 1.3085333333333333,
        "step": 9814
    },
    {
        "loss": 2.3164,
        "grad_norm": 4.489402770996094,
        "learning_rate": 7.780473798849275e-07,
        "epoch": 1.3086666666666666,
        "step": 9815
    },
    {
        "loss": 1.3598,
        "grad_norm": 5.086723804473877,
        "learning_rate": 7.702313654939119e-07,
        "epoch": 1.3088,
        "step": 9816
    },
    {
        "loss": 2.8728,
        "grad_norm": 2.2969584465026855,
        "learning_rate": 7.624546563523405e-07,
        "epoch": 1.3089333333333333,
        "step": 9817
    },
    {
        "loss": 2.4794,
        "grad_norm": 3.46699857711792,
        "learning_rate": 7.5471725554066e-07,
        "epoch": 1.3090666666666666,
        "step": 9818
    },
    {
        "loss": 2.4895,
        "grad_norm": 3.238074541091919,
        "learning_rate": 7.470191661236192e-07,
        "epoch": 1.3092,
        "step": 9819
    },
    {
        "loss": 2.3797,
        "grad_norm": 3.6603798866271973,
        "learning_rate": 7.393603911504899e-07,
        "epoch": 1.3093333333333335,
        "step": 9820
    },
    {
        "loss": 1.4777,
        "grad_norm": 3.7144808769226074,
        "learning_rate": 7.31740933654923e-07,
        "epoch": 1.3094666666666668,
        "step": 9821
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.9928226470947266,
        "learning_rate": 7.241607966550379e-07,
        "epoch": 1.3096,
        "step": 9822
    },
    {
        "loss": 2.0903,
        "grad_norm": 4.688577175140381,
        "learning_rate": 7.166199831533327e-07,
        "epoch": 1.3097333333333334,
        "step": 9823
    },
    {
        "loss": 1.9675,
        "grad_norm": 3.9476239681243896,
        "learning_rate": 7.091184961367403e-07,
        "epoch": 1.3098666666666667,
        "step": 9824
    },
    {
        "loss": 2.4016,
        "grad_norm": 3.167421579360962,
        "learning_rate": 7.016563385766395e-07,
        "epoch": 1.31,
        "step": 9825
    },
    {
        "loss": 2.1893,
        "grad_norm": 2.623853921890259,
        "learning_rate": 6.942335134288103e-07,
        "epoch": 1.3101333333333334,
        "step": 9826
    },
    {
        "loss": 1.475,
        "grad_norm": 3.3209774494171143,
        "learning_rate": 6.868500236334785e-07,
        "epoch": 1.3102666666666667,
        "step": 9827
    },
    {
        "loss": 1.6234,
        "grad_norm": 3.5397140979766846,
        "learning_rate": 6.795058721152381e-07,
        "epoch": 1.3104,
        "step": 9828
    },
    {
        "loss": 1.9413,
        "grad_norm": 2.9096038341522217,
        "learning_rate": 6.722010617831509e-07,
        "epoch": 1.3105333333333333,
        "step": 9829
    },
    {
        "loss": 2.122,
        "grad_norm": 2.934980869293213,
        "learning_rate": 6.649355955306801e-07,
        "epoch": 1.3106666666666666,
        "step": 9830
    },
    {
        "loss": 1.4159,
        "grad_norm": 2.4931576251983643,
        "learning_rate": 6.577094762356906e-07,
        "epoch": 1.3108,
        "step": 9831
    },
    {
        "loss": 1.7463,
        "grad_norm": 3.909669876098633,
        "learning_rate": 6.505227067604924e-07,
        "epoch": 1.3109333333333333,
        "step": 9832
    },
    {
        "loss": 2.7693,
        "grad_norm": 2.2377400398254395,
        "learning_rate": 6.433752899517531e-07,
        "epoch": 1.3110666666666666,
        "step": 9833
    },
    {
        "loss": 2.2751,
        "grad_norm": 3.529411792755127,
        "learning_rate": 6.36267228640619e-07,
        "epoch": 1.3112,
        "step": 9834
    },
    {
        "loss": 1.8395,
        "grad_norm": 3.4367244243621826,
        "learning_rate": 6.291985256426158e-07,
        "epoch": 1.3113333333333332,
        "step": 9835
    },
    {
        "loss": 2.9152,
        "grad_norm": 3.075126886367798,
        "learning_rate": 6.221691837576704e-07,
        "epoch": 1.3114666666666666,
        "step": 9836
    },
    {
        "loss": 1.6746,
        "grad_norm": 3.5042388439178467,
        "learning_rate": 6.151792057701334e-07,
        "epoch": 1.3115999999999999,
        "step": 9837
    },
    {
        "loss": 1.5588,
        "grad_norm": 4.0031657218933105,
        "learning_rate": 6.082285944487454e-07,
        "epoch": 1.3117333333333332,
        "step": 9838
    },
    {
        "loss": 2.716,
        "grad_norm": 4.038442611694336,
        "learning_rate": 6.013173525467042e-07,
        "epoch": 1.3118666666666667,
        "step": 9839
    },
    {
        "loss": 2.0632,
        "grad_norm": 4.3975725173950195,
        "learning_rate": 5.944454828015311e-07,
        "epoch": 1.312,
        "step": 9840
    },
    {
        "loss": 2.4646,
        "grad_norm": 4.642223834991455,
        "learning_rate": 5.876129879352377e-07,
        "epoch": 1.3121333333333334,
        "step": 9841
    },
    {
        "loss": 2.6785,
        "grad_norm": 2.59629225730896,
        "learning_rate": 5.80819870654159e-07,
        "epoch": 1.3122666666666667,
        "step": 9842
    },
    {
        "loss": 1.9286,
        "grad_norm": 4.06573486328125,
        "learning_rate": 5.740661336491093e-07,
        "epoch": 1.3124,
        "step": 9843
    },
    {
        "loss": 2.6633,
        "grad_norm": 3.276268720626831,
        "learning_rate": 5.673517795952488e-07,
        "epoch": 1.3125333333333333,
        "step": 9844
    },
    {
        "loss": 1.7622,
        "grad_norm": 4.5638885498046875,
        "learning_rate": 5.606768111521499e-07,
        "epoch": 1.3126666666666666,
        "step": 9845
    },
    {
        "loss": 2.1265,
        "grad_norm": 3.8857216835021973,
        "learning_rate": 5.540412309637866e-07,
        "epoch": 1.3128,
        "step": 9846
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.9874041080474854,
        "learning_rate": 5.474450416585563e-07,
        "epoch": 1.3129333333333333,
        "step": 9847
    },
    {
        "loss": 3.156,
        "grad_norm": 3.861449718475342,
        "learning_rate": 5.408882458492359e-07,
        "epoch": 1.3130666666666666,
        "step": 9848
    },
    {
        "loss": 2.4887,
        "grad_norm": 2.9671974182128906,
        "learning_rate": 5.343708461329588e-07,
        "epoch": 1.3132,
        "step": 9849
    },
    {
        "loss": 3.0955,
        "grad_norm": 3.04042911529541,
        "learning_rate": 5.278928450913156e-07,
        "epoch": 1.3133333333333335,
        "step": 9850
    },
    {
        "loss": 1.9861,
        "grad_norm": 2.912051200866699,
        "learning_rate": 5.214542452902538e-07,
        "epoch": 1.3134666666666668,
        "step": 9851
    },
    {
        "loss": 2.2896,
        "grad_norm": 3.470256805419922,
        "learning_rate": 5.150550492801442e-07,
        "epoch": 1.3136,
        "step": 9852
    },
    {
        "loss": 2.0127,
        "grad_norm": 3.7178213596343994,
        "learning_rate": 5.08695259595715e-07,
        "epoch": 1.3137333333333334,
        "step": 9853
    },
    {
        "loss": 2.1786,
        "grad_norm": 3.5961906909942627,
        "learning_rate": 5.023748787560956e-07,
        "epoch": 1.3138666666666667,
        "step": 9854
    },
    {
        "loss": 0.9739,
        "grad_norm": 3.9872140884399414,
        "learning_rate": 4.960939092648054e-07,
        "epoch": 1.314,
        "step": 9855
    },
    {
        "loss": 1.7504,
        "grad_norm": 2.9036617279052734,
        "learning_rate": 4.898523536097876e-07,
        "epoch": 1.3141333333333334,
        "step": 9856
    },
    {
        "loss": 1.2426,
        "grad_norm": 3.227477788925171,
        "learning_rate": 4.836502142633315e-07,
        "epoch": 1.3142666666666667,
        "step": 9857
    },
    {
        "loss": 3.4897,
        "grad_norm": 7.824618339538574,
        "learning_rate": 4.774874936820939e-07,
        "epoch": 1.3144,
        "step": 9858
    },
    {
        "loss": 2.6238,
        "grad_norm": 3.7672388553619385,
        "learning_rate": 4.713641943071889e-07,
        "epoch": 1.3145333333333333,
        "step": 9859
    },
    {
        "loss": 1.3943,
        "grad_norm": 4.011856555938721,
        "learning_rate": 4.6528031856406527e-07,
        "epoch": 1.3146666666666667,
        "step": 9860
    },
    {
        "loss": 1.8285,
        "grad_norm": 4.0281572341918945,
        "learning_rate": 4.592358688625509e-07,
        "epoch": 1.3148,
        "step": 9861
    },
    {
        "loss": 1.95,
        "grad_norm": 3.4529151916503906,
        "learning_rate": 4.532308475968972e-07,
        "epoch": 1.3149333333333333,
        "step": 9862
    },
    {
        "loss": 1.1554,
        "grad_norm": 5.26410436630249,
        "learning_rate": 4.4726525714569036e-07,
        "epoch": 1.3150666666666666,
        "step": 9863
    },
    {
        "loss": 1.9879,
        "grad_norm": 3.3780415058135986,
        "learning_rate": 4.413390998719513e-07,
        "epoch": 1.3152,
        "step": 9864
    },
    {
        "loss": 2.5978,
        "grad_norm": 3.8489468097686768,
        "learning_rate": 4.3545237812301353e-07,
        "epoch": 1.3153333333333332,
        "step": 9865
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.9805623292922974,
        "learning_rate": 4.296050942306562e-07,
        "epoch": 1.3154666666666666,
        "step": 9866
    },
    {
        "loss": 1.91,
        "grad_norm": 2.990115165710449,
        "learning_rate": 4.2379725051099327e-07,
        "epoch": 1.3155999999999999,
        "step": 9867
    },
    {
        "loss": 2.019,
        "grad_norm": 3.3290834426879883,
        "learning_rate": 4.180288492645401e-07,
        "epoch": 1.3157333333333332,
        "step": 9868
    },
    {
        "loss": 2.399,
        "grad_norm": 4.980313301086426,
        "learning_rate": 4.1229989277619117e-07,
        "epoch": 1.3158666666666667,
        "step": 9869
    },
    {
        "loss": 1.7881,
        "grad_norm": 2.710864543914795,
        "learning_rate": 4.0661038331517574e-07,
        "epoch": 1.316,
        "step": 9870
    },
    {
        "loss": 1.2364,
        "grad_norm": 3.700758934020996,
        "learning_rate": 4.0096032313514667e-07,
        "epoch": 1.3161333333333334,
        "step": 9871
    },
    {
        "loss": 1.6958,
        "grad_norm": 3.2907943725585938,
        "learning_rate": 3.953497144741025e-07,
        "epoch": 1.3162666666666667,
        "step": 9872
    },
    {
        "loss": 1.9669,
        "grad_norm": 4.294686794281006,
        "learning_rate": 3.897785595544434e-07,
        "epoch": 1.3164,
        "step": 9873
    },
    {
        "loss": 2.4618,
        "grad_norm": 3.1569738388061523,
        "learning_rate": 3.8424686058290415e-07,
        "epoch": 1.3165333333333333,
        "step": 9874
    },
    {
        "loss": 2.8111,
        "grad_norm": 3.873893976211548,
        "learning_rate": 3.787546197506098e-07,
        "epoch": 1.3166666666666667,
        "step": 9875
    },
    {
        "loss": 1.8012,
        "grad_norm": 3.7544476985931396,
        "learning_rate": 3.733018392330645e-07,
        "epoch": 1.3168,
        "step": 9876
    },
    {
        "loss": 2.8371,
        "grad_norm": 3.5742509365081787,
        "learning_rate": 3.678885211901406e-07,
        "epoch": 1.3169333333333333,
        "step": 9877
    },
    {
        "loss": 2.1303,
        "grad_norm": 2.5698533058166504,
        "learning_rate": 3.6251466776605625e-07,
        "epoch": 1.3170666666666666,
        "step": 9878
    },
    {
        "loss": 2.0888,
        "grad_norm": 3.2898073196411133,
        "learning_rate": 3.571802810894087e-07,
        "epoch": 1.3172,
        "step": 9879
    },
    {
        "loss": 2.2695,
        "grad_norm": 4.501869201660156,
        "learning_rate": 3.5188536327318557e-07,
        "epoch": 1.3173333333333335,
        "step": 9880
    },
    {
        "loss": 2.3861,
        "grad_norm": 3.4800987243652344,
        "learning_rate": 3.466299164147091e-07,
        "epoch": 1.3174666666666668,
        "step": 9881
    },
    {
        "loss": 2.1406,
        "grad_norm": 2.3165977001190186,
        "learning_rate": 3.4141394259569195e-07,
        "epoch": 1.3176,
        "step": 9882
    },
    {
        "loss": 1.9305,
        "grad_norm": 3.3485934734344482,
        "learning_rate": 3.3623744388218136e-07,
        "epoch": 1.3177333333333334,
        "step": 9883
    },
    {
        "loss": 0.9403,
        "grad_norm": 3.306128978729248,
        "learning_rate": 3.3110042232461504e-07,
        "epoch": 1.3178666666666667,
        "step": 9884
    },
    {
        "loss": 1.2745,
        "grad_norm": 4.46085262298584,
        "learning_rate": 3.2600287995779854e-07,
        "epoch": 1.318,
        "step": 9885
    },
    {
        "loss": 2.2822,
        "grad_norm": 3.777359962463379,
        "learning_rate": 3.2094481880086126e-07,
        "epoch": 1.3181333333333334,
        "step": 9886
    },
    {
        "loss": 2.9211,
        "grad_norm": 4.199748516082764,
        "learning_rate": 3.159262408573227e-07,
        "epoch": 1.3182666666666667,
        "step": 9887
    },
    {
        "loss": 2.2355,
        "grad_norm": 3.410048484802246,
        "learning_rate": 3.1094714811507053e-07,
        "epoch": 1.3184,
        "step": 9888
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.676851987838745,
        "learning_rate": 3.060075425463382e-07,
        "epoch": 1.3185333333333333,
        "step": 9889
    },
    {
        "loss": 2.032,
        "grad_norm": 2.6738078594207764,
        "learning_rate": 3.0110742610771623e-07,
        "epoch": 1.3186666666666667,
        "step": 9890
    },
    {
        "loss": 0.6932,
        "grad_norm": 3.2318952083587646,
        "learning_rate": 2.9624680074016307e-07,
        "epoch": 1.3188,
        "step": 9891
    },
    {
        "loss": 2.2291,
        "grad_norm": 3.7988524436950684,
        "learning_rate": 2.91425668368972e-07,
        "epoch": 1.3189333333333333,
        "step": 9892
    },
    {
        "loss": 1.7538,
        "grad_norm": 3.8684191703796387,
        "learning_rate": 2.8664403090382654e-07,
        "epoch": 1.3190666666666666,
        "step": 9893
    },
    {
        "loss": 2.8102,
        "grad_norm": 4.1332244873046875,
        "learning_rate": 2.81901890238756e-07,
        "epoch": 1.3192,
        "step": 9894
    },
    {
        "loss": 1.5758,
        "grad_norm": 3.827342987060547,
        "learning_rate": 2.771992482521135e-07,
        "epoch": 1.3193333333333332,
        "step": 9895
    },
    {
        "loss": 2.2836,
        "grad_norm": 3.5557451248168945,
        "learning_rate": 2.7253610680665344e-07,
        "epoch": 1.3194666666666666,
        "step": 9896
    },
    {
        "loss": 1.3452,
        "grad_norm": 4.2445969581604,
        "learning_rate": 2.6791246774945377e-07,
        "epoch": 1.3195999999999999,
        "step": 9897
    },
    {
        "loss": 2.0556,
        "grad_norm": 3.7070467472076416,
        "learning_rate": 2.6332833291196067e-07,
        "epoch": 1.3197333333333332,
        "step": 9898
    },
    {
        "loss": 1.0364,
        "grad_norm": 4.992415428161621,
        "learning_rate": 2.587837041099439e-07,
        "epoch": 1.3198666666666667,
        "step": 9899
    },
    {
        "loss": 0.6719,
        "grad_norm": 3.395070791244507,
        "learning_rate": 2.5427858314357454e-07,
        "epoch": 1.32,
        "step": 9900
    },
    {
        "loss": 2.294,
        "grad_norm": 3.1921117305755615,
        "learning_rate": 2.498129717973252e-07,
        "epoch": 1.3201333333333334,
        "step": 9901
    },
    {
        "loss": 2.4078,
        "grad_norm": 4.605774402618408,
        "learning_rate": 2.453868718400587e-07,
        "epoch": 1.3202666666666667,
        "step": 9902
    },
    {
        "loss": 2.1631,
        "grad_norm": 3.120910167694092,
        "learning_rate": 2.4100028502495044e-07,
        "epoch": 1.3204,
        "step": 9903
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.70839786529541,
        "learning_rate": 2.366532130895438e-07,
        "epoch": 1.3205333333333333,
        "step": 9904
    },
    {
        "loss": 2.1974,
        "grad_norm": 3.0500149726867676,
        "learning_rate": 2.3234565775575036e-07,
        "epoch": 1.3206666666666667,
        "step": 9905
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.9817523956298828,
        "learning_rate": 2.2807762072977191e-07,
        "epoch": 1.3208,
        "step": 9906
    },
    {
        "loss": 1.9144,
        "grad_norm": 5.2144880294799805,
        "learning_rate": 2.238491037022339e-07,
        "epoch": 1.3209333333333333,
        "step": 9907
    },
    {
        "loss": 2.2848,
        "grad_norm": 3.293691873550415,
        "learning_rate": 2.1966010834802987e-07,
        "epoch": 1.3210666666666666,
        "step": 9908
    },
    {
        "loss": 1.9209,
        "grad_norm": 4.739296913146973,
        "learning_rate": 2.1551063632645474e-07,
        "epoch": 1.3212,
        "step": 9909
    },
    {
        "loss": 2.3506,
        "grad_norm": 2.148540735244751,
        "learning_rate": 2.114006892811271e-07,
        "epoch": 1.3213333333333335,
        "step": 9910
    },
    {
        "loss": 1.9557,
        "grad_norm": 5.569900035858154,
        "learning_rate": 2.073302688400114e-07,
        "epoch": 1.3214666666666668,
        "step": 9911
    },
    {
        "loss": 0.5471,
        "grad_norm": 2.638852596282959,
        "learning_rate": 2.0329937661539565e-07,
        "epoch": 1.3216,
        "step": 9912
    },
    {
        "loss": 1.4156,
        "grad_norm": 4.021390438079834,
        "learning_rate": 1.9930801420395828e-07,
        "epoch": 1.3217333333333334,
        "step": 9913
    },
    {
        "loss": 1.633,
        "grad_norm": 3.938681125640869,
        "learning_rate": 1.9535618318666792e-07,
        "epoch": 1.3218666666666667,
        "step": 9914
    },
    {
        "loss": 1.7409,
        "grad_norm": 4.471812725067139,
        "learning_rate": 1.9144388512888356e-07,
        "epoch": 1.322,
        "step": 9915
    },
    {
        "loss": 1.2114,
        "grad_norm": 4.410165786743164,
        "learning_rate": 1.8757112158026558e-07,
        "epoch": 1.3221333333333334,
        "step": 9916
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.619412660598755,
        "learning_rate": 1.8373789407480912e-07,
        "epoch": 1.3222666666666667,
        "step": 9917
    },
    {
        "loss": 1.7979,
        "grad_norm": 3.421842575073242,
        "learning_rate": 1.7994420413091073e-07,
        "epoch": 1.3224,
        "step": 9918
    },
    {
        "loss": 1.1772,
        "grad_norm": 3.361589193344116,
        "learning_rate": 1.7619005325123505e-07,
        "epoch": 1.3225333333333333,
        "step": 9919
    },
    {
        "loss": 2.1829,
        "grad_norm": 3.1023378372192383,
        "learning_rate": 1.7247544292281482e-07,
        "epoch": 1.3226666666666667,
        "step": 9920
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.127676248550415,
        "learning_rate": 1.6880037461702857e-07,
        "epoch": 1.3228,
        "step": 9921
    },
    {
        "loss": 0.7817,
        "grad_norm": 2.103827953338623,
        "learning_rate": 1.651648497895786e-07,
        "epoch": 1.3229333333333333,
        "step": 9922
    },
    {
        "loss": 2.0665,
        "grad_norm": 4.528426647186279,
        "learning_rate": 1.615688698805129e-07,
        "epoch": 1.3230666666666666,
        "step": 9923
    },
    {
        "loss": 3.1311,
        "grad_norm": 3.784437894821167,
        "learning_rate": 1.5801243631420326e-07,
        "epoch": 1.3232,
        "step": 9924
    },
    {
        "loss": 1.439,
        "grad_norm": 3.6471292972564697,
        "learning_rate": 1.5449555049937835e-07,
        "epoch": 1.3233333333333333,
        "step": 9925
    },
    {
        "loss": 2.238,
        "grad_norm": 2.9726173877716064,
        "learning_rate": 1.5101821382907944e-07,
        "epoch": 1.3234666666666666,
        "step": 9926
    },
    {
        "loss": 1.9751,
        "grad_norm": 3.0163724422454834,
        "learning_rate": 1.4758042768069359e-07,
        "epoch": 1.3235999999999999,
        "step": 9927
    },
    {
        "loss": 0.9561,
        "grad_norm": 4.721684455871582,
        "learning_rate": 1.4418219341594262e-07,
        "epoch": 1.3237333333333332,
        "step": 9928
    },
    {
        "loss": 0.6103,
        "grad_norm": 3.29571795463562,
        "learning_rate": 1.40823512380861e-07,
        "epoch": 1.3238666666666667,
        "step": 9929
    },
    {
        "loss": 0.9076,
        "grad_norm": 3.8943233489990234,
        "learning_rate": 1.3750438590586222e-07,
        "epoch": 1.324,
        "step": 9930
    },
    {
        "loss": 2.454,
        "grad_norm": 1.928878664970398,
        "learning_rate": 1.3422481530563912e-07,
        "epoch": 1.3241333333333334,
        "step": 9931
    },
    {
        "loss": 1.5506,
        "grad_norm": 3.193903684616089,
        "learning_rate": 1.3098480187926366e-07,
        "epoch": 1.3242666666666667,
        "step": 9932
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.8227453231811523,
        "learning_rate": 1.27784346910087e-07,
        "epoch": 1.3244,
        "step": 9933
    },
    {
        "loss": 1.6026,
        "grad_norm": 3.8867502212524414,
        "learning_rate": 1.2462345166585065e-07,
        "epoch": 1.3245333333333333,
        "step": 9934
    },
    {
        "loss": 1.7562,
        "grad_norm": 4.718782424926758,
        "learning_rate": 1.2150211739857532e-07,
        "epoch": 1.3246666666666667,
        "step": 9935
    },
    {
        "loss": 2.8482,
        "grad_norm": 3.4722177982330322,
        "learning_rate": 1.1842034534464974e-07,
        "epoch": 1.3248,
        "step": 9936
    },
    {
        "loss": 0.8102,
        "grad_norm": 2.833005666732788,
        "learning_rate": 1.1537813672475307e-07,
        "epoch": 1.3249333333333333,
        "step": 9937
    },
    {
        "loss": 2.5228,
        "grad_norm": 2.5656301975250244,
        "learning_rate": 1.1237549274392133e-07,
        "epoch": 1.3250666666666666,
        "step": 9938
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.4982635974884033,
        "learning_rate": 1.094124145915254e-07,
        "epoch": 1.3252,
        "step": 9939
    },
    {
        "loss": 2.4237,
        "grad_norm": 2.832465887069702,
        "learning_rate": 1.0648890344123751e-07,
        "epoch": 1.3253333333333333,
        "step": 9940
    },
    {
        "loss": 2.6771,
        "grad_norm": 3.1320102214813232,
        "learning_rate": 1.0360496045107582e-07,
        "epoch": 1.3254666666666668,
        "step": 9941
    },
    {
        "loss": 0.9281,
        "grad_norm": 3.660567283630371,
        "learning_rate": 1.0076058676337097e-07,
        "epoch": 1.3256000000000001,
        "step": 9942
    },
    {
        "loss": 2.3597,
        "grad_norm": 3.3215889930725098,
        "learning_rate": 9.79557835047995e-08,
        "epoch": 1.3257333333333334,
        "step": 9943
    },
    {
        "loss": 1.5095,
        "grad_norm": 3.171595335006714,
        "learning_rate": 9.51905517863505e-08,
        "epoch": 1.3258666666666667,
        "step": 9944
    },
    {
        "loss": 1.8681,
        "grad_norm": 4.043682098388672,
        "learning_rate": 9.246489270333669e-08,
        "epoch": 1.326,
        "step": 9945
    },
    {
        "loss": 1.3759,
        "grad_norm": 5.142865180969238,
        "learning_rate": 8.977880733541666e-08,
        "epoch": 1.3261333333333334,
        "step": 9946
    },
    {
        "loss": 1.5202,
        "grad_norm": 2.9665534496307373,
        "learning_rate": 8.713229674653933e-08,
        "epoch": 1.3262666666666667,
        "step": 9947
    },
    {
        "loss": 1.8412,
        "grad_norm": 3.175746440887451,
        "learning_rate": 8.45253619849995e-08,
        "epoch": 1.3264,
        "step": 9948
    },
    {
        "loss": 1.4623,
        "grad_norm": 5.81734561920166,
        "learning_rate": 8.195800408342668e-08,
        "epoch": 1.3265333333333333,
        "step": 9949
    },
    {
        "loss": 2.105,
        "grad_norm": 4.008460998535156,
        "learning_rate": 7.943022405874079e-08,
        "epoch": 1.3266666666666667,
        "step": 9950
    },
    {
        "loss": 2.0691,
        "grad_norm": 3.957501173019409,
        "learning_rate": 7.694202291221863e-08,
        "epoch": 1.3268,
        "step": 9951
    },
    {
        "loss": 2.2496,
        "grad_norm": 2.040184736251831,
        "learning_rate": 7.449340162944963e-08,
        "epoch": 1.3269333333333333,
        "step": 9952
    },
    {
        "loss": 2.3017,
        "grad_norm": 4.514237880706787,
        "learning_rate": 7.208436118032458e-08,
        "epoch": 1.3270666666666666,
        "step": 9953
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.6156561374664307,
        "learning_rate": 6.971490251908019e-08,
        "epoch": 1.3272,
        "step": 9954
    },
    {
        "loss": 1.6215,
        "grad_norm": 6.984746932983398,
        "learning_rate": 6.73850265842657e-08,
        "epoch": 1.3273333333333333,
        "step": 9955
    },
    {
        "loss": 2.5219,
        "grad_norm": 4.042230606079102,
        "learning_rate": 6.509473429875402e-08,
        "epoch": 1.3274666666666666,
        "step": 9956
    },
    {
        "loss": 1.8871,
        "grad_norm": 4.244530200958252,
        "learning_rate": 6.284402656974164e-08,
        "epoch": 1.3276,
        "step": 9957
    },
    {
        "loss": 1.7693,
        "grad_norm": 3.2174623012542725,
        "learning_rate": 6.063290428872659e-08,
        "epoch": 1.3277333333333332,
        "step": 9958
    },
    {
        "loss": 0.9509,
        "grad_norm": 2.316772222518921,
        "learning_rate": 5.8461368331563796e-08,
        "epoch": 1.3278666666666665,
        "step": 9959
    },
    {
        "loss": 2.2118,
        "grad_norm": 2.0047788619995117,
        "learning_rate": 5.632941955839854e-08,
        "epoch": 1.328,
        "step": 9960
    },
    {
        "loss": 2.3927,
        "grad_norm": 3.6974728107452393,
        "learning_rate": 5.423705881369978e-08,
        "epoch": 1.3281333333333334,
        "step": 9961
    },
    {
        "loss": 1.4719,
        "grad_norm": 2.4381043910980225,
        "learning_rate": 5.2184286926248995e-08,
        "epoch": 1.3282666666666667,
        "step": 9962
    },
    {
        "loss": 2.0297,
        "grad_norm": 3.180495500564575,
        "learning_rate": 5.0171104709173524e-08,
        "epoch": 1.3284,
        "step": 9963
    },
    {
        "loss": 1.2111,
        "grad_norm": 2.652008295059204,
        "learning_rate": 4.819751295989106e-08,
        "epoch": 1.3285333333333333,
        "step": 9964
    },
    {
        "loss": 1.6017,
        "grad_norm": 4.185251712799072,
        "learning_rate": 4.6263512460154037e-08,
        "epoch": 1.3286666666666667,
        "step": 9965
    },
    {
        "loss": 2.6214,
        "grad_norm": 2.711616039276123,
        "learning_rate": 4.4369103976016347e-08,
        "epoch": 1.3288,
        "step": 9966
    },
    {
        "loss": 1.9069,
        "grad_norm": 3.1314995288848877,
        "learning_rate": 4.251428825786663e-08,
        "epoch": 1.3289333333333333,
        "step": 9967
    },
    {
        "loss": 2.8989,
        "grad_norm": 5.239007472991943,
        "learning_rate": 4.069906604041718e-08,
        "epoch": 1.3290666666666666,
        "step": 9968
    },
    {
        "loss": 1.6462,
        "grad_norm": 3.66249418258667,
        "learning_rate": 3.892343804265952e-08,
        "epoch": 1.3292,
        "step": 9969
    },
    {
        "loss": 2.3515,
        "grad_norm": 3.33369779586792,
        "learning_rate": 3.718740496794215e-08,
        "epoch": 1.3293333333333333,
        "step": 9970
    },
    {
        "loss": 1.6704,
        "grad_norm": 4.124721527099609,
        "learning_rate": 3.5490967503903907e-08,
        "epoch": 1.3294666666666668,
        "step": 9971
    },
    {
        "loss": 2.0383,
        "grad_norm": 2.6638286113739014,
        "learning_rate": 3.383412632251837e-08,
        "epoch": 1.3296000000000001,
        "step": 9972
    },
    {
        "loss": 1.1115,
        "grad_norm": 4.614017009735107,
        "learning_rate": 3.2216882080060573e-08,
        "epoch": 1.3297333333333334,
        "step": 9973
    },
    {
        "loss": 2.3788,
        "grad_norm": 2.034977674484253,
        "learning_rate": 3.063923541712921e-08,
        "epoch": 1.3298666666666668,
        "step": 9974
    },
    {
        "loss": 1.7797,
        "grad_norm": 4.485980987548828,
        "learning_rate": 2.9101186958635507e-08,
        "epoch": 1.33,
        "step": 9975
    },
    {
        "loss": 1.5604,
        "grad_norm": 3.8124186992645264,
        "learning_rate": 2.7602737313803252e-08,
        "epoch": 1.3301333333333334,
        "step": 9976
    },
    {
        "loss": 2.2401,
        "grad_norm": 3.617722272872925,
        "learning_rate": 2.6143887076168772e-08,
        "epoch": 1.3302666666666667,
        "step": 9977
    },
    {
        "loss": 2.0545,
        "grad_norm": 2.8662898540496826,
        "learning_rate": 2.4724636823603153e-08,
        "epoch": 1.3304,
        "step": 9978
    },
    {
        "loss": 2.6778,
        "grad_norm": 5.98614501953125,
        "learning_rate": 2.3344987118256722e-08,
        "epoch": 1.3305333333333333,
        "step": 9979
    },
    {
        "loss": 2.5069,
        "grad_norm": 2.542253017425537,
        "learning_rate": 2.200493850662566e-08,
        "epoch": 1.3306666666666667,
        "step": 9980
    },
    {
        "loss": 2.4209,
        "grad_norm": 3.1031484603881836,
        "learning_rate": 2.0704491519507596e-08,
        "epoch": 1.3308,
        "step": 9981
    },
    {
        "loss": 2.4363,
        "grad_norm": 4.4683332443237305,
        "learning_rate": 1.9443646672012705e-08,
        "epoch": 1.3309333333333333,
        "step": 9982
    },
    {
        "loss": 2.8727,
        "grad_norm": 3.2431483268737793,
        "learning_rate": 1.822240446355261e-08,
        "epoch": 1.3310666666666666,
        "step": 9983
    },
    {
        "loss": 1.4508,
        "grad_norm": 2.0384955406188965,
        "learning_rate": 1.7040765377884792e-08,
        "epoch": 1.3312,
        "step": 9984
    },
    {
        "loss": 2.3413,
        "grad_norm": 2.4543967247009277,
        "learning_rate": 1.5898729883057072e-08,
        "epoch": 1.3313333333333333,
        "step": 9985
    },
    {
        "loss": 1.3757,
        "grad_norm": 3.352886438369751,
        "learning_rate": 1.4796298431429822e-08,
        "epoch": 1.3314666666666666,
        "step": 9986
    },
    {
        "loss": 1.3417,
        "grad_norm": 4.2555365562438965,
        "learning_rate": 1.3733471459675962e-08,
        "epoch": 1.3316,
        "step": 9987
    },
    {
        "loss": 2.1083,
        "grad_norm": 5.302672386169434,
        "learning_rate": 1.2710249388792061e-08,
        "epoch": 1.3317333333333332,
        "step": 9988
    },
    {
        "loss": 1.4696,
        "grad_norm": 3.950411796569824,
        "learning_rate": 1.1726632624076139e-08,
        "epoch": 1.3318666666666665,
        "step": 9989
    },
    {
        "loss": 1.9278,
        "grad_norm": 4.545337677001953,
        "learning_rate": 1.0782621555138761e-08,
        "epoch": 1.332,
        "step": 9990
    },
    {
        "loss": 1.5328,
        "grad_norm": 4.454185962677002,
        "learning_rate": 9.878216555925246e-09,
        "epoch": 1.3321333333333334,
        "step": 9991
    },
    {
        "loss": 1.5945,
        "grad_norm": 4.066452980041504,
        "learning_rate": 9.013417984637951e-09,
        "epoch": 1.3322666666666667,
        "step": 9992
    },
    {
        "loss": 1.5023,
        "grad_norm": 2.478602170944214,
        "learning_rate": 8.188226183858394e-09,
        "epoch": 1.3324,
        "step": 9993
    },
    {
        "loss": 1.6771,
        "grad_norm": 3.993818521499634,
        "learning_rate": 7.402641480436234e-09,
        "epoch": 1.3325333333333333,
        "step": 9994
    },
    {
        "loss": 1.363,
        "grad_norm": 3.305900812149048,
        "learning_rate": 6.656664185533678e-09,
        "epoch": 1.3326666666666667,
        "step": 9995
    },
    {
        "loss": 2.0445,
        "grad_norm": 3.7168800830841064,
        "learning_rate": 5.95029459465879e-09,
        "epoch": 1.3328,
        "step": 9996
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.157560348510742,
        "learning_rate": 5.283532987587769e-09,
        "epoch": 1.3329333333333333,
        "step": 9997
    },
    {
        "loss": 2.327,
        "grad_norm": 2.0782485008239746,
        "learning_rate": 4.656379628431573e-09,
        "epoch": 1.3330666666666666,
        "step": 9998
    },
    {
        "loss": 1.6215,
        "grad_norm": 3.2201831340789795,
        "learning_rate": 4.068834765613705e-09,
        "epoch": 1.3332,
        "step": 9999
    },
    {
        "loss": 1.2512,
        "grad_norm": 4.346017837524414,
        "learning_rate": 3.520898631848013e-09,
        "epoch": 1.3333333333333333,
        "step": 10000
    },
    {
        "loss": 1.4214,
        "grad_norm": 2.518862724304199,
        "learning_rate": 3.012571444194201e-09,
        "epoch": 1.3334666666666668,
        "step": 10001
    },
    {
        "loss": 2.1314,
        "grad_norm": 3.054863214492798,
        "learning_rate": 2.543853403991214e-09,
        "epoch": 1.3336000000000001,
        "step": 10002
    },
    {
        "loss": 2.4884,
        "grad_norm": 4.234767436981201,
        "learning_rate": 2.114744696890547e-09,
        "epoch": 1.3337333333333334,
        "step": 10003
    },
    {
        "loss": 2.677,
        "grad_norm": 3.8063318729400635,
        "learning_rate": 1.7252454928895489e-09,
        "epoch": 1.3338666666666668,
        "step": 10004
    },
    {
        "loss": 2.8028,
        "grad_norm": 3.311483383178711,
        "learning_rate": 1.375355946242607e-09,
        "epoch": 1.334,
        "step": 10005
    },
    {
        "loss": 2.5112,
        "grad_norm": 2.7230401039123535,
        "learning_rate": 1.0650761955610656e-09,
        "epoch": 1.3341333333333334,
        "step": 10006
    },
    {
        "loss": 1.4731,
        "grad_norm": 2.4434590339660645,
        "learning_rate": 7.944063637355114e-10,
        "epoch": 1.3342666666666667,
        "step": 10007
    },
    {
        "loss": 2.5887,
        "grad_norm": 3.6711180210113525,
        "learning_rate": 5.633465579912844e-10,
        "epoch": 1.3344,
        "step": 10008
    },
    {
        "loss": 2.1081,
        "grad_norm": 4.113908767700195,
        "learning_rate": 3.718968698329661e-10,
        "epoch": 1.3345333333333333,
        "step": 10009
    },
    {
        "loss": 2.8115,
        "grad_norm": 3.599658250808716,
        "learning_rate": 2.200573751220958e-10,
        "epoch": 1.3346666666666667,
        "step": 10010
    },
    {
        "loss": 1.6546,
        "grad_norm": 3.7577404975891113,
        "learning_rate": 1.0782813397725022e-10,
        "epoch": 1.3348,
        "step": 10011
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.840644598007202,
        "learning_rate": 3.5209190862861563e-11,
        "epoch": 1.3349333333333333,
        "step": 10012
    },
    {
        "loss": 2.525,
        "grad_norm": 3.2356765270233154,
        "learning_rate": 2.2005745448083758e-12,
        "epoch": 1.3350666666666666,
        "step": 10013
    },
    {
        "loss": 1.0238,
        "grad_norm": 5.0395097732543945,
        "learning_rate": 0.00019999999119770193,
        "epoch": 1.3352,
        "step": 10014
    },
    {
        "loss": 1.356,
        "grad_norm": 5.435511112213135,
        "learning_rate": 0.0001999999449856411,
        "epoch": 1.3353333333333333,
        "step": 10015
    },
    {
        "loss": 1.814,
        "grad_norm": 2.819610118865967,
        "learning_rate": 0.00019999985916326134,
        "epoch": 1.3354666666666666,
        "step": 10016
    },
    {
        "loss": 2.6737,
        "grad_norm": 3.210186243057251,
        "learning_rate": 0.00019999973373059665,
        "epoch": 1.3356,
        "step": 10017
    },
    {
        "loss": 1.9842,
        "grad_norm": 3.21164608001709,
        "learning_rate": 0.00019999956868769662,
        "epoch": 1.3357333333333332,
        "step": 10018
    },
    {
        "loss": 2.4011,
        "grad_norm": 3.7954208850860596,
        "learning_rate": 0.00019999936403462676,
        "epoch": 1.3358666666666665,
        "step": 10019
    },
    {
        "loss": 2.7495,
        "grad_norm": 2.3610291481018066,
        "learning_rate": 0.00019999911977146805,
        "epoch": 1.336,
        "step": 10020
    },
    {
        "loss": 2.1379,
        "grad_norm": 3.7519450187683105,
        "learning_rate": 0.00019999883589831723,
        "epoch": 1.3361333333333334,
        "step": 10021
    },
    {
        "loss": 1.4103,
        "grad_norm": 5.984853267669678,
        "learning_rate": 0.0001999985124152868,
        "epoch": 1.3362666666666667,
        "step": 10022
    },
    {
        "loss": 2.1334,
        "grad_norm": 2.945643663406372,
        "learning_rate": 0.00019999814932250486,
        "epoch": 1.3364,
        "step": 10023
    },
    {
        "loss": 1.2316,
        "grad_norm": 3.0015347003936768,
        "learning_rate": 0.00019999774662011522,
        "epoch": 1.3365333333333334,
        "step": 10024
    },
    {
        "loss": 2.4087,
        "grad_norm": 2.3426244258880615,
        "learning_rate": 0.00019999730430827746,
        "epoch": 1.3366666666666667,
        "step": 10025
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.422217845916748,
        "learning_rate": 0.0001999968223871667,
        "epoch": 1.3368,
        "step": 10026
    },
    {
        "loss": 1.5732,
        "grad_norm": 3.970439910888672,
        "learning_rate": 0.00019999630085697386,
        "epoch": 1.3369333333333333,
        "step": 10027
    },
    {
        "loss": 2.5349,
        "grad_norm": 2.979064702987671,
        "learning_rate": 0.00019999573971790554,
        "epoch": 1.3370666666666666,
        "step": 10028
    },
    {
        "loss": 2.019,
        "grad_norm": 3.5338847637176514,
        "learning_rate": 0.00019999513897018395,
        "epoch": 1.3372,
        "step": 10029
    },
    {
        "loss": 2.6603,
        "grad_norm": 4.638438701629639,
        "learning_rate": 0.00019999449861404713,
        "epoch": 1.3373333333333333,
        "step": 10030
    },
    {
        "loss": 2.3613,
        "grad_norm": 2.4269046783447266,
        "learning_rate": 0.00019999381864974867,
        "epoch": 1.3374666666666668,
        "step": 10031
    },
    {
        "loss": 2.4435,
        "grad_norm": 3.775393009185791,
        "learning_rate": 0.0001999930990775579,
        "epoch": 1.3376000000000001,
        "step": 10032
    },
    {
        "loss": 2.7271,
        "grad_norm": 3.3047122955322266,
        "learning_rate": 0.0001999923398977599,
        "epoch": 1.3377333333333334,
        "step": 10033
    },
    {
        "loss": 1.5759,
        "grad_norm": 3.3850960731506348,
        "learning_rate": 0.00019999154111065534,
        "epoch": 1.3378666666666668,
        "step": 10034
    },
    {
        "loss": 2.3446,
        "grad_norm": 2.8109960556030273,
        "learning_rate": 0.00019999070271656063,
        "epoch": 1.338,
        "step": 10035
    },
    {
        "loss": 2.7011,
        "grad_norm": 2.6031553745269775,
        "learning_rate": 0.00019998982471580785,
        "epoch": 1.3381333333333334,
        "step": 10036
    },
    {
        "loss": 1.1419,
        "grad_norm": 5.5214667320251465,
        "learning_rate": 0.00019998890710874482,
        "epoch": 1.3382666666666667,
        "step": 10037
    },
    {
        "loss": 1.4543,
        "grad_norm": 4.085045337677002,
        "learning_rate": 0.00019998794989573498,
        "epoch": 1.3384,
        "step": 10038
    },
    {
        "loss": 0.6296,
        "grad_norm": 3.1235811710357666,
        "learning_rate": 0.00019998695307715746,
        "epoch": 1.3385333333333334,
        "step": 10039
    },
    {
        "loss": 2.0055,
        "grad_norm": 2.258143663406372,
        "learning_rate": 0.00019998591665340717,
        "epoch": 1.3386666666666667,
        "step": 10040
    },
    {
        "loss": 1.3995,
        "grad_norm": 4.04700231552124,
        "learning_rate": 0.00019998484062489456,
        "epoch": 1.3388,
        "step": 10041
    },
    {
        "loss": 2.0827,
        "grad_norm": 4.8906426429748535,
        "learning_rate": 0.0001999837249920459,
        "epoch": 1.3389333333333333,
        "step": 10042
    },
    {
        "loss": 1.8993,
        "grad_norm": 3.590681791305542,
        "learning_rate": 0.0001999825697553031,
        "epoch": 1.3390666666666666,
        "step": 10043
    },
    {
        "loss": 1.4042,
        "grad_norm": 3.72845458984375,
        "learning_rate": 0.00019998137491512372,
        "epoch": 1.3392,
        "step": 10044
    },
    {
        "loss": 2.372,
        "grad_norm": 2.997774362564087,
        "learning_rate": 0.00019998014047198107,
        "epoch": 1.3393333333333333,
        "step": 10045
    },
    {
        "loss": 2.0253,
        "grad_norm": 3.9471778869628906,
        "learning_rate": 0.00019997886642636407,
        "epoch": 1.3394666666666666,
        "step": 10046
    },
    {
        "loss": 2.7672,
        "grad_norm": 3.3331105709075928,
        "learning_rate": 0.00019997755277877745,
        "epoch": 1.3396,
        "step": 10047
    },
    {
        "loss": 2.5179,
        "grad_norm": 4.241156101226807,
        "learning_rate": 0.0001999761995297415,
        "epoch": 1.3397333333333332,
        "step": 10048
    },
    {
        "loss": 2.8508,
        "grad_norm": 3.1667325496673584,
        "learning_rate": 0.00019997480667979225,
        "epoch": 1.3398666666666665,
        "step": 10049
    },
    {
        "loss": 1.8485,
        "grad_norm": 3.4853365421295166,
        "learning_rate": 0.0001999733742294814,
        "epoch": 1.34,
        "step": 10050
    },
    {
        "loss": 2.3374,
        "grad_norm": 3.989544630050659,
        "learning_rate": 0.0001999719021793764,
        "epoch": 1.3401333333333334,
        "step": 10051
    },
    {
        "loss": 2.158,
        "grad_norm": 2.6952695846557617,
        "learning_rate": 0.0001999703905300603,
        "epoch": 1.3402666666666667,
        "step": 10052
    },
    {
        "loss": 2.0193,
        "grad_norm": 3.469252109527588,
        "learning_rate": 0.00019996883928213185,
        "epoch": 1.3404,
        "step": 10053
    },
    {
        "loss": 1.6758,
        "grad_norm": 3.322103977203369,
        "learning_rate": 0.00019996724843620553,
        "epoch": 1.3405333333333334,
        "step": 10054
    },
    {
        "loss": 1.7612,
        "grad_norm": 5.203179836273193,
        "learning_rate": 0.00019996561799291148,
        "epoch": 1.3406666666666667,
        "step": 10055
    },
    {
        "loss": 2.6151,
        "grad_norm": 3.7433879375457764,
        "learning_rate": 0.00019996394795289554,
        "epoch": 1.3408,
        "step": 10056
    },
    {
        "loss": 2.8885,
        "grad_norm": 2.3160243034362793,
        "learning_rate": 0.00019996223831681914,
        "epoch": 1.3409333333333333,
        "step": 10057
    },
    {
        "loss": 1.9253,
        "grad_norm": 2.8580706119537354,
        "learning_rate": 0.0001999604890853596,
        "epoch": 1.3410666666666666,
        "step": 10058
    },
    {
        "loss": 0.8413,
        "grad_norm": 4.2249436378479,
        "learning_rate": 0.0001999587002592097,
        "epoch": 1.3412,
        "step": 10059
    },
    {
        "loss": 2.0604,
        "grad_norm": 2.785095453262329,
        "learning_rate": 0.00019995687183907798,
        "epoch": 1.3413333333333333,
        "step": 10060
    },
    {
        "loss": 2.0196,
        "grad_norm": 2.549219846725464,
        "learning_rate": 0.00019995500382568877,
        "epoch": 1.3414666666666666,
        "step": 10061
    },
    {
        "loss": 1.3477,
        "grad_norm": 3.903918981552124,
        "learning_rate": 0.00019995309621978195,
        "epoch": 1.3416000000000001,
        "step": 10062
    },
    {
        "loss": 2.3264,
        "grad_norm": 3.6452908515930176,
        "learning_rate": 0.00019995114902211314,
        "epoch": 1.3417333333333334,
        "step": 10063
    },
    {
        "loss": 2.3203,
        "grad_norm": 2.1218180656433105,
        "learning_rate": 0.00019994916223345363,
        "epoch": 1.3418666666666668,
        "step": 10064
    },
    {
        "loss": 2.434,
        "grad_norm": 3.039437770843506,
        "learning_rate": 0.0001999471358545904,
        "epoch": 1.342,
        "step": 10065
    },
    {
        "loss": 2.316,
        "grad_norm": 3.4967637062072754,
        "learning_rate": 0.00019994506988632607,
        "epoch": 1.3421333333333334,
        "step": 10066
    },
    {
        "loss": 2.3025,
        "grad_norm": 3.4634931087493896,
        "learning_rate": 0.000199942964329479,
        "epoch": 1.3422666666666667,
        "step": 10067
    },
    {
        "loss": 2.0421,
        "grad_norm": 3.5161237716674805,
        "learning_rate": 0.00019994081918488324,
        "epoch": 1.3424,
        "step": 10068
    },
    {
        "loss": 1.8225,
        "grad_norm": 2.4371402263641357,
        "learning_rate": 0.00019993863445338846,
        "epoch": 1.3425333333333334,
        "step": 10069
    },
    {
        "loss": 1.4357,
        "grad_norm": 5.843520164489746,
        "learning_rate": 0.00019993641013586,
        "epoch": 1.3426666666666667,
        "step": 10070
    },
    {
        "loss": 2.0722,
        "grad_norm": 2.7690489292144775,
        "learning_rate": 0.00019993414623317902,
        "epoch": 1.3428,
        "step": 10071
    },
    {
        "loss": 2.4551,
        "grad_norm": 2.759324073791504,
        "learning_rate": 0.00019993184274624216,
        "epoch": 1.3429333333333333,
        "step": 10072
    },
    {
        "loss": 2.1619,
        "grad_norm": 3.267779588699341,
        "learning_rate": 0.00019992949967596188,
        "epoch": 1.3430666666666666,
        "step": 10073
    },
    {
        "loss": 2.8369,
        "grad_norm": 2.9231436252593994,
        "learning_rate": 0.00019992711702326627,
        "epoch": 1.3432,
        "step": 10074
    },
    {
        "loss": 1.1222,
        "grad_norm": 3.991694927215576,
        "learning_rate": 0.00019992469478909914,
        "epoch": 1.3433333333333333,
        "step": 10075
    },
    {
        "loss": 2.0316,
        "grad_norm": 3.2316362857818604,
        "learning_rate": 0.0001999222329744199,
        "epoch": 1.3434666666666666,
        "step": 10076
    },
    {
        "loss": 1.4387,
        "grad_norm": 4.952078342437744,
        "learning_rate": 0.0001999197315802037,
        "epoch": 1.3436,
        "step": 10077
    },
    {
        "loss": 2.2977,
        "grad_norm": 2.820378541946411,
        "learning_rate": 0.00019991719060744137,
        "epoch": 1.3437333333333332,
        "step": 10078
    },
    {
        "loss": 3.2289,
        "grad_norm": 3.9556381702423096,
        "learning_rate": 0.0001999146100571394,
        "epoch": 1.3438666666666665,
        "step": 10079
    },
    {
        "loss": 2.119,
        "grad_norm": 3.685297727584839,
        "learning_rate": 0.0001999119899303199,
        "epoch": 1.3439999999999999,
        "step": 10080
    },
    {
        "loss": 2.1617,
        "grad_norm": 5.644521236419678,
        "learning_rate": 0.00019990933022802075,
        "epoch": 1.3441333333333334,
        "step": 10081
    },
    {
        "loss": 1.8185,
        "grad_norm": 4.044926166534424,
        "learning_rate": 0.00019990663095129544,
        "epoch": 1.3442666666666667,
        "step": 10082
    },
    {
        "loss": 2.591,
        "grad_norm": 4.127100944519043,
        "learning_rate": 0.00019990389210121324,
        "epoch": 1.3444,
        "step": 10083
    },
    {
        "loss": 2.4069,
        "grad_norm": 2.7202019691467285,
        "learning_rate": 0.00019990111367885894,
        "epoch": 1.3445333333333334,
        "step": 10084
    },
    {
        "loss": 2.4236,
        "grad_norm": 4.588656902313232,
        "learning_rate": 0.00019989829568533313,
        "epoch": 1.3446666666666667,
        "step": 10085
    },
    {
        "loss": 2.0933,
        "grad_norm": 3.9425365924835205,
        "learning_rate": 0.00019989543812175197,
        "epoch": 1.3448,
        "step": 10086
    },
    {
        "loss": 1.0083,
        "grad_norm": 5.854105472564697,
        "learning_rate": 0.0001998925409892474,
        "epoch": 1.3449333333333333,
        "step": 10087
    },
    {
        "loss": 2.1513,
        "grad_norm": 2.6700572967529297,
        "learning_rate": 0.00019988960428896695,
        "epoch": 1.3450666666666666,
        "step": 10088
    },
    {
        "loss": 1.5904,
        "grad_norm": 3.698486804962158,
        "learning_rate": 0.00019988662802207388,
        "epoch": 1.3452,
        "step": 10089
    },
    {
        "loss": 2.0727,
        "grad_norm": 2.834733724594116,
        "learning_rate": 0.00019988361218974712,
        "epoch": 1.3453333333333333,
        "step": 10090
    },
    {
        "loss": 1.4581,
        "grad_norm": 4.302179336547852,
        "learning_rate": 0.0001998805567931812,
        "epoch": 1.3454666666666666,
        "step": 10091
    },
    {
        "loss": 2.4336,
        "grad_norm": 4.450955390930176,
        "learning_rate": 0.00019987746183358643,
        "epoch": 1.3456000000000001,
        "step": 10092
    },
    {
        "loss": 2.3708,
        "grad_norm": 3.578676223754883,
        "learning_rate": 0.00019987432731218867,
        "epoch": 1.3457333333333334,
        "step": 10093
    },
    {
        "loss": 2.2137,
        "grad_norm": 2.907144069671631,
        "learning_rate": 0.00019987115323022957,
        "epoch": 1.3458666666666668,
        "step": 10094
    },
    {
        "loss": 2.407,
        "grad_norm": 3.5594677925109863,
        "learning_rate": 0.00019986793958896637,
        "epoch": 1.346,
        "step": 10095
    },
    {
        "loss": 2.132,
        "grad_norm": 6.188275337219238,
        "learning_rate": 0.000199864686389672,
        "epoch": 1.3461333333333334,
        "step": 10096
    },
    {
        "loss": 1.2675,
        "grad_norm": 4.192275524139404,
        "learning_rate": 0.0001998613936336351,
        "epoch": 1.3462666666666667,
        "step": 10097
    },
    {
        "loss": 2.3275,
        "grad_norm": 3.2089200019836426,
        "learning_rate": 0.00019985806132215986,
        "epoch": 1.3464,
        "step": 10098
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.9926068782806396,
        "learning_rate": 0.00019985468945656632,
        "epoch": 1.3465333333333334,
        "step": 10099
    },
    {
        "loss": 0.9253,
        "grad_norm": 3.605461835861206,
        "learning_rate": 0.00019985127803819,
        "epoch": 1.3466666666666667,
        "step": 10100
    },
    {
        "loss": 2.8414,
        "grad_norm": 3.526211977005005,
        "learning_rate": 0.00019984782706838222,
        "epoch": 1.3468,
        "step": 10101
    },
    {
        "loss": 1.978,
        "grad_norm": 3.7970778942108154,
        "learning_rate": 0.00019984433654850995,
        "epoch": 1.3469333333333333,
        "step": 10102
    },
    {
        "loss": 2.4462,
        "grad_norm": 2.4745888710021973,
        "learning_rate": 0.00019984080647995575,
        "epoch": 1.3470666666666666,
        "step": 10103
    },
    {
        "loss": 0.9656,
        "grad_norm": 4.939951419830322,
        "learning_rate": 0.00019983723686411788,
        "epoch": 1.3472,
        "step": 10104
    },
    {
        "loss": 2.5608,
        "grad_norm": 3.3666722774505615,
        "learning_rate": 0.00019983362770241032,
        "epoch": 1.3473333333333333,
        "step": 10105
    },
    {
        "loss": 1.9461,
        "grad_norm": 5.224881649017334,
        "learning_rate": 0.00019982997899626264,
        "epoch": 1.3474666666666666,
        "step": 10106
    },
    {
        "loss": 1.2397,
        "grad_norm": 2.6111133098602295,
        "learning_rate": 0.00019982629074712013,
        "epoch": 1.3476,
        "step": 10107
    },
    {
        "loss": 2.2156,
        "grad_norm": 3.0654101371765137,
        "learning_rate": 0.0001998225629564437,
        "epoch": 1.3477333333333332,
        "step": 10108
    },
    {
        "loss": 1.4788,
        "grad_norm": 3.0769569873809814,
        "learning_rate": 0.00019981879562570994,
        "epoch": 1.3478666666666665,
        "step": 10109
    },
    {
        "loss": 2.6319,
        "grad_norm": 6.902125835418701,
        "learning_rate": 0.00019981498875641112,
        "epoch": 1.3479999999999999,
        "step": 10110
    },
    {
        "loss": 2.1319,
        "grad_norm": 3.4047393798828125,
        "learning_rate": 0.00019981114235005513,
        "epoch": 1.3481333333333334,
        "step": 10111
    },
    {
        "loss": 2.2384,
        "grad_norm": 4.312123775482178,
        "learning_rate": 0.00019980725640816557,
        "epoch": 1.3482666666666667,
        "step": 10112
    },
    {
        "loss": 2.2403,
        "grad_norm": 3.5778117179870605,
        "learning_rate": 0.00019980333093228166,
        "epoch": 1.3484,
        "step": 10113
    },
    {
        "loss": 2.5729,
        "grad_norm": 2.4475340843200684,
        "learning_rate": 0.00019979936592395828,
        "epoch": 1.3485333333333334,
        "step": 10114
    },
    {
        "loss": 2.1508,
        "grad_norm": 4.914051055908203,
        "learning_rate": 0.00019979536138476603,
        "epoch": 1.3486666666666667,
        "step": 10115
    },
    {
        "loss": 1.8571,
        "grad_norm": 2.812706232070923,
        "learning_rate": 0.00019979131731629105,
        "epoch": 1.3488,
        "step": 10116
    },
    {
        "loss": 0.9772,
        "grad_norm": 4.009664535522461,
        "learning_rate": 0.00019978723372013528,
        "epoch": 1.3489333333333333,
        "step": 10117
    },
    {
        "loss": 2.1283,
        "grad_norm": 3.1567866802215576,
        "learning_rate": 0.00019978311059791622,
        "epoch": 1.3490666666666666,
        "step": 10118
    },
    {
        "loss": 2.6127,
        "grad_norm": 3.860990047454834,
        "learning_rate": 0.00019977894795126703,
        "epoch": 1.3492,
        "step": 10119
    },
    {
        "loss": 2.2617,
        "grad_norm": Infinity,
        "learning_rate": 0.00019977894795126703,
        "epoch": 1.3493333333333333,
        "step": 10120
    },
    {
        "loss": 2.7012,
        "grad_norm": 5.255187034606934,
        "learning_rate": 0.0001997747457818366,
        "epoch": 1.3494666666666666,
        "step": 10121
    },
    {
        "loss": 1.9008,
        "grad_norm": 4.306234359741211,
        "learning_rate": 0.00019977050409128935,
        "epoch": 1.3496000000000001,
        "step": 10122
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.9060864448547363,
        "learning_rate": 0.0001997662228813055,
        "epoch": 1.3497333333333335,
        "step": 10123
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.1091630458831787,
        "learning_rate": 0.0001997619021535808,
        "epoch": 1.3498666666666668,
        "step": 10124
    },
    {
        "loss": 1.6799,
        "grad_norm": 3.455291986465454,
        "learning_rate": 0.00019975754190982675,
        "epoch": 1.35,
        "step": 10125
    },
    {
        "loss": 1.3236,
        "grad_norm": 5.46012544631958,
        "learning_rate": 0.0001997531421517704,
        "epoch": 1.3501333333333334,
        "step": 10126
    },
    {
        "loss": 2.644,
        "grad_norm": 2.6786413192749023,
        "learning_rate": 0.00019974870288115456,
        "epoch": 1.3502666666666667,
        "step": 10127
    },
    {
        "loss": 1.9633,
        "grad_norm": 5.307703495025635,
        "learning_rate": 0.00019974422409973762,
        "epoch": 1.3504,
        "step": 10128
    },
    {
        "loss": 2.2727,
        "grad_norm": 4.265058517456055,
        "learning_rate": 0.00019973970580929365,
        "epoch": 1.3505333333333334,
        "step": 10129
    },
    {
        "loss": 1.9296,
        "grad_norm": 4.392147064208984,
        "learning_rate": 0.00019973514801161236,
        "epoch": 1.3506666666666667,
        "step": 10130
    },
    {
        "loss": 2.5822,
        "grad_norm": 3.3869857788085938,
        "learning_rate": 0.00019973055070849913,
        "epoch": 1.3508,
        "step": 10131
    },
    {
        "loss": 2.4,
        "grad_norm": 4.133147239685059,
        "learning_rate": 0.00019972591390177488,
        "epoch": 1.3509333333333333,
        "step": 10132
    },
    {
        "loss": 1.2691,
        "grad_norm": 3.0669803619384766,
        "learning_rate": 0.00019972123759327636,
        "epoch": 1.3510666666666666,
        "step": 10133
    },
    {
        "loss": 2.4447,
        "grad_norm": 2.6806318759918213,
        "learning_rate": 0.00019971652178485584,
        "epoch": 1.3512,
        "step": 10134
    },
    {
        "loss": 1.7989,
        "grad_norm": 2.2746903896331787,
        "learning_rate": 0.0001997117664783813,
        "epoch": 1.3513333333333333,
        "step": 10135
    },
    {
        "loss": 2.304,
        "grad_norm": 3.243201971054077,
        "learning_rate": 0.00019970697167573623,
        "epoch": 1.3514666666666666,
        "step": 10136
    },
    {
        "loss": 2.9272,
        "grad_norm": 4.2460503578186035,
        "learning_rate": 0.00019970213737881994,
        "epoch": 1.3516,
        "step": 10137
    },
    {
        "loss": 1.5851,
        "grad_norm": 2.7476344108581543,
        "learning_rate": 0.00019969726358954733,
        "epoch": 1.3517333333333332,
        "step": 10138
    },
    {
        "loss": 2.24,
        "grad_norm": 3.0487465858459473,
        "learning_rate": 0.00019969235030984893,
        "epoch": 1.3518666666666665,
        "step": 10139
    },
    {
        "loss": 1.8522,
        "grad_norm": 3.06406307220459,
        "learning_rate": 0.00019968739754167082,
        "epoch": 1.3519999999999999,
        "step": 10140
    },
    {
        "loss": 1.2423,
        "grad_norm": 4.852334499359131,
        "learning_rate": 0.00019968240528697492,
        "epoch": 1.3521333333333334,
        "step": 10141
    },
    {
        "loss": 2.3577,
        "grad_norm": 3.1310877799987793,
        "learning_rate": 0.00019967737354773858,
        "epoch": 1.3522666666666667,
        "step": 10142
    },
    {
        "loss": 2.1918,
        "grad_norm": 3.7898550033569336,
        "learning_rate": 0.00019967230232595498,
        "epoch": 1.3524,
        "step": 10143
    },
    {
        "loss": 2.5178,
        "grad_norm": 3.6112027168273926,
        "learning_rate": 0.00019966719162363278,
        "epoch": 1.3525333333333334,
        "step": 10144
    },
    {
        "loss": 2.1436,
        "grad_norm": 3.8341786861419678,
        "learning_rate": 0.00019966204144279638,
        "epoch": 1.3526666666666667,
        "step": 10145
    },
    {
        "loss": 2.4494,
        "grad_norm": 2.6350021362304688,
        "learning_rate": 0.00019965685178548577,
        "epoch": 1.3528,
        "step": 10146
    },
    {
        "loss": 2.5118,
        "grad_norm": 2.8988325595855713,
        "learning_rate": 0.0001996516226537566,
        "epoch": 1.3529333333333333,
        "step": 10147
    },
    {
        "loss": 1.0904,
        "grad_norm": 2.8820834159851074,
        "learning_rate": 0.00019964635404968016,
        "epoch": 1.3530666666666666,
        "step": 10148
    },
    {
        "loss": 2.222,
        "grad_norm": 3.277886152267456,
        "learning_rate": 0.0001996410459753433,
        "epoch": 1.3532,
        "step": 10149
    },
    {
        "loss": 2.6061,
        "grad_norm": 3.352290630340576,
        "learning_rate": 0.00019963569843284862,
        "epoch": 1.3533333333333333,
        "step": 10150
    },
    {
        "loss": 1.1154,
        "grad_norm": 3.668478488922119,
        "learning_rate": 0.00019963031142431433,
        "epoch": 1.3534666666666666,
        "step": 10151
    },
    {
        "loss": 2.1673,
        "grad_norm": 3.002469062805176,
        "learning_rate": 0.00019962488495187418,
        "epoch": 1.3536000000000001,
        "step": 10152
    },
    {
        "loss": 1.7544,
        "grad_norm": 3.5540623664855957,
        "learning_rate": 0.00019961941901767763,
        "epoch": 1.3537333333333335,
        "step": 10153
    },
    {
        "loss": 2.3519,
        "grad_norm": 4.331006050109863,
        "learning_rate": 0.00019961391362388975,
        "epoch": 1.3538666666666668,
        "step": 10154
    },
    {
        "loss": 2.5934,
        "grad_norm": 3.1008684635162354,
        "learning_rate": 0.00019960836877269127,
        "epoch": 1.354,
        "step": 10155
    },
    {
        "loss": 2.7235,
        "grad_norm": 4.204442977905273,
        "learning_rate": 0.0001996027844662785,
        "epoch": 1.3541333333333334,
        "step": 10156
    },
    {
        "loss": 2.7578,
        "grad_norm": 4.6419572830200195,
        "learning_rate": 0.00019959716070686346,
        "epoch": 1.3542666666666667,
        "step": 10157
    },
    {
        "loss": 1.7543,
        "grad_norm": 6.4565348625183105,
        "learning_rate": 0.00019959149749667363,
        "epoch": 1.3544,
        "step": 10158
    },
    {
        "loss": 2.3024,
        "grad_norm": 4.228745937347412,
        "learning_rate": 0.00019958579483795233,
        "epoch": 1.3545333333333334,
        "step": 10159
    },
    {
        "loss": 1.2372,
        "grad_norm": 3.984126091003418,
        "learning_rate": 0.00019958005273295836,
        "epoch": 1.3546666666666667,
        "step": 10160
    },
    {
        "loss": 1.5382,
        "grad_norm": 3.6216864585876465,
        "learning_rate": 0.00019957427118396618,
        "epoch": 1.3548,
        "step": 10161
    },
    {
        "loss": 2.49,
        "grad_norm": 3.8549745082855225,
        "learning_rate": 0.00019956845019326592,
        "epoch": 1.3549333333333333,
        "step": 10162
    },
    {
        "loss": 2.5042,
        "grad_norm": 3.9242427349090576,
        "learning_rate": 0.00019956258976316327,
        "epoch": 1.3550666666666666,
        "step": 10163
    },
    {
        "loss": 2.3974,
        "grad_norm": 5.238226413726807,
        "learning_rate": 0.00019955668989597952,
        "epoch": 1.3552,
        "step": 10164
    },
    {
        "loss": 1.9675,
        "grad_norm": 4.994564056396484,
        "learning_rate": 0.0001995507505940517,
        "epoch": 1.3553333333333333,
        "step": 10165
    },
    {
        "loss": 2.4185,
        "grad_norm": 3.519118309020996,
        "learning_rate": 0.00019954477185973236,
        "epoch": 1.3554666666666666,
        "step": 10166
    },
    {
        "loss": 2.2819,
        "grad_norm": 2.8453280925750732,
        "learning_rate": 0.0001995387536953897,
        "epoch": 1.3556,
        "step": 10167
    },
    {
        "loss": 2.2268,
        "grad_norm": 3.1946871280670166,
        "learning_rate": 0.00019953269610340751,
        "epoch": 1.3557333333333332,
        "step": 10168
    },
    {
        "loss": 1.6073,
        "grad_norm": 5.264034271240234,
        "learning_rate": 0.00019952659908618528,
        "epoch": 1.3558666666666666,
        "step": 10169
    },
    {
        "loss": 2.226,
        "grad_norm": 3.274747610092163,
        "learning_rate": 0.000199520462646138,
        "epoch": 1.3559999999999999,
        "step": 10170
    },
    {
        "loss": 2.8421,
        "grad_norm": 4.434020042419434,
        "learning_rate": 0.00019951428678569635,
        "epoch": 1.3561333333333334,
        "step": 10171
    },
    {
        "loss": 2.3195,
        "grad_norm": 3.411566734313965,
        "learning_rate": 0.00019950807150730663,
        "epoch": 1.3562666666666667,
        "step": 10172
    },
    {
        "loss": 2.3248,
        "grad_norm": 2.8274996280670166,
        "learning_rate": 0.0001995018168134307,
        "epoch": 1.3564,
        "step": 10173
    },
    {
        "loss": 2.5565,
        "grad_norm": 3.062737464904785,
        "learning_rate": 0.00019949552270654612,
        "epoch": 1.3565333333333334,
        "step": 10174
    },
    {
        "loss": 2.3407,
        "grad_norm": 2.688317060470581,
        "learning_rate": 0.00019948918918914594,
        "epoch": 1.3566666666666667,
        "step": 10175
    },
    {
        "loss": 2.409,
        "grad_norm": 4.5085530281066895,
        "learning_rate": 0.00019948281626373896,
        "epoch": 1.3568,
        "step": 10176
    },
    {
        "loss": 2.1274,
        "grad_norm": 2.4484214782714844,
        "learning_rate": 0.00019947640393284943,
        "epoch": 1.3569333333333333,
        "step": 10177
    },
    {
        "loss": 2.3659,
        "grad_norm": 2.9749605655670166,
        "learning_rate": 0.00019946995219901739,
        "epoch": 1.3570666666666666,
        "step": 10178
    },
    {
        "loss": 1.1807,
        "grad_norm": 4.988271713256836,
        "learning_rate": 0.0001994634610647983,
        "epoch": 1.3572,
        "step": 10179
    },
    {
        "loss": 2.4886,
        "grad_norm": 2.7497928142547607,
        "learning_rate": 0.00019945693053276343,
        "epoch": 1.3573333333333333,
        "step": 10180
    },
    {
        "loss": 2.1624,
        "grad_norm": 3.4368584156036377,
        "learning_rate": 0.00019945036060549945,
        "epoch": 1.3574666666666666,
        "step": 10181
    },
    {
        "loss": 0.6256,
        "grad_norm": 2.678414821624756,
        "learning_rate": 0.00019944375128560875,
        "epoch": 1.3576,
        "step": 10182
    },
    {
        "loss": 2.5499,
        "grad_norm": 4.744976043701172,
        "learning_rate": 0.00019943710257570933,
        "epoch": 1.3577333333333335,
        "step": 10183
    },
    {
        "loss": 2.1191,
        "grad_norm": 3.4161012172698975,
        "learning_rate": 0.0001994304144784348,
        "epoch": 1.3578666666666668,
        "step": 10184
    },
    {
        "loss": 1.4285,
        "grad_norm": 3.08371639251709,
        "learning_rate": 0.00019942368699643425,
        "epoch": 1.358,
        "step": 10185
    },
    {
        "loss": 2.6043,
        "grad_norm": 3.9454729557037354,
        "learning_rate": 0.00019941692013237248,
        "epoch": 1.3581333333333334,
        "step": 10186
    },
    {
        "loss": 2.7887,
        "grad_norm": 2.1961705684661865,
        "learning_rate": 0.00019941011388892993,
        "epoch": 1.3582666666666667,
        "step": 10187
    },
    {
        "loss": 2.1686,
        "grad_norm": 3.773240566253662,
        "learning_rate": 0.0001994032682688025,
        "epoch": 1.3584,
        "step": 10188
    },
    {
        "loss": 1.8918,
        "grad_norm": 4.192315578460693,
        "learning_rate": 0.00019939638327470187,
        "epoch": 1.3585333333333334,
        "step": 10189
    },
    {
        "loss": 1.4853,
        "grad_norm": 4.885788440704346,
        "learning_rate": 0.0001993894589093551,
        "epoch": 1.3586666666666667,
        "step": 10190
    },
    {
        "loss": 2.4458,
        "grad_norm": 3.3913118839263916,
        "learning_rate": 0.00019938249517550497,
        "epoch": 1.3588,
        "step": 10191
    },
    {
        "loss": 2.7143,
        "grad_norm": 3.275750160217285,
        "learning_rate": 0.0001993754920759099,
        "epoch": 1.3589333333333333,
        "step": 10192
    },
    {
        "loss": 1.1339,
        "grad_norm": 5.500826835632324,
        "learning_rate": 0.0001993684496133438,
        "epoch": 1.3590666666666666,
        "step": 10193
    },
    {
        "loss": 2.0901,
        "grad_norm": 3.036503553390503,
        "learning_rate": 0.0001993613677905962,
        "epoch": 1.3592,
        "step": 10194
    },
    {
        "loss": 2.523,
        "grad_norm": 3.5602669715881348,
        "learning_rate": 0.00019935424661047225,
        "epoch": 1.3593333333333333,
        "step": 10195
    },
    {
        "loss": 2.797,
        "grad_norm": 4.4455647468566895,
        "learning_rate": 0.0001993470860757927,
        "epoch": 1.3594666666666666,
        "step": 10196
    },
    {
        "loss": 1.5125,
        "grad_norm": 3.384446144104004,
        "learning_rate": 0.00019933988618939382,
        "epoch": 1.3596,
        "step": 10197
    },
    {
        "loss": 1.4103,
        "grad_norm": 3.5688560009002686,
        "learning_rate": 0.00019933264695412754,
        "epoch": 1.3597333333333332,
        "step": 10198
    },
    {
        "loss": 1.7838,
        "grad_norm": 4.043889999389648,
        "learning_rate": 0.00019932536837286133,
        "epoch": 1.3598666666666666,
        "step": 10199
    },
    {
        "loss": 2.1673,
        "grad_norm": 4.491984844207764,
        "learning_rate": 0.00019931805044847827,
        "epoch": 1.3599999999999999,
        "step": 10200
    },
    {
        "loss": 1.4635,
        "grad_norm": 4.379834175109863,
        "learning_rate": 0.00019931069318387699,
        "epoch": 1.3601333333333334,
        "step": 10201
    },
    {
        "loss": 2.5664,
        "grad_norm": 2.724717617034912,
        "learning_rate": 0.00019930329658197175,
        "epoch": 1.3602666666666667,
        "step": 10202
    },
    {
        "loss": 1.873,
        "grad_norm": 2.728544235229492,
        "learning_rate": 0.0001992958606456924,
        "epoch": 1.3604,
        "step": 10203
    },
    {
        "loss": 1.8885,
        "grad_norm": 2.3799610137939453,
        "learning_rate": 0.00019928838537798426,
        "epoch": 1.3605333333333334,
        "step": 10204
    },
    {
        "loss": 1.8396,
        "grad_norm": 4.578784465789795,
        "learning_rate": 0.0001992808707818084,
        "epoch": 1.3606666666666667,
        "step": 10205
    },
    {
        "loss": 1.7362,
        "grad_norm": 5.849160194396973,
        "learning_rate": 0.0001992733168601413,
        "epoch": 1.3608,
        "step": 10206
    },
    {
        "loss": 1.8767,
        "grad_norm": 3.761768341064453,
        "learning_rate": 0.00019926572361597512,
        "epoch": 1.3609333333333333,
        "step": 10207
    },
    {
        "loss": 2.8328,
        "grad_norm": 5.1102399826049805,
        "learning_rate": 0.0001992580910523176,
        "epoch": 1.3610666666666666,
        "step": 10208
    },
    {
        "loss": 2.4158,
        "grad_norm": 4.162815093994141,
        "learning_rate": 0.00019925041917219198,
        "epoch": 1.3612,
        "step": 10209
    },
    {
        "loss": 2.152,
        "grad_norm": 4.382146835327148,
        "learning_rate": 0.00019924270797863714,
        "epoch": 1.3613333333333333,
        "step": 10210
    },
    {
        "loss": 2.2796,
        "grad_norm": 4.956902503967285,
        "learning_rate": 0.00019923495747470755,
        "epoch": 1.3614666666666666,
        "step": 10211
    },
    {
        "loss": 2.544,
        "grad_norm": 2.564455986022949,
        "learning_rate": 0.00019922716766347315,
        "epoch": 1.3616,
        "step": 10212
    },
    {
        "loss": 1.678,
        "grad_norm": 3.084540367126465,
        "learning_rate": 0.0001992193385480195,
        "epoch": 1.3617333333333335,
        "step": 10213
    },
    {
        "loss": 1.5349,
        "grad_norm": 3.683304786682129,
        "learning_rate": 0.0001992114701314478,
        "epoch": 1.3618666666666668,
        "step": 10214
    },
    {
        "loss": 2.655,
        "grad_norm": 3.551332712173462,
        "learning_rate": 0.0001992035624168747,
        "epoch": 1.362,
        "step": 10215
    },
    {
        "loss": 2.7398,
        "grad_norm": 3.3286118507385254,
        "learning_rate": 0.00019919561540743255,
        "epoch": 1.3621333333333334,
        "step": 10216
    },
    {
        "loss": 2.4288,
        "grad_norm": 2.7544615268707275,
        "learning_rate": 0.00019918762910626906,
        "epoch": 1.3622666666666667,
        "step": 10217
    },
    {
        "loss": 2.494,
        "grad_norm": 3.3028182983398438,
        "learning_rate": 0.00019917960351654773,
        "epoch": 1.3624,
        "step": 10218
    },
    {
        "loss": 2.1197,
        "grad_norm": 3.993072748184204,
        "learning_rate": 0.00019917153864144755,
        "epoch": 1.3625333333333334,
        "step": 10219
    },
    {
        "loss": 2.7741,
        "grad_norm": 3.3127009868621826,
        "learning_rate": 0.00019916343448416295,
        "epoch": 1.3626666666666667,
        "step": 10220
    },
    {
        "loss": 2.09,
        "grad_norm": 3.435275077819824,
        "learning_rate": 0.00019915529104790408,
        "epoch": 1.3628,
        "step": 10221
    },
    {
        "loss": 1.6055,
        "grad_norm": 3.6691718101501465,
        "learning_rate": 0.00019914710833589654,
        "epoch": 1.3629333333333333,
        "step": 10222
    },
    {
        "loss": 2.1046,
        "grad_norm": 3.009153127670288,
        "learning_rate": 0.00019913888635138158,
        "epoch": 1.3630666666666666,
        "step": 10223
    },
    {
        "loss": 1.9199,
        "grad_norm": 5.458744525909424,
        "learning_rate": 0.00019913062509761592,
        "epoch": 1.3632,
        "step": 10224
    },
    {
        "loss": 1.5486,
        "grad_norm": 2.598541021347046,
        "learning_rate": 0.0001991223245778719,
        "epoch": 1.3633333333333333,
        "step": 10225
    },
    {
        "loss": 1.987,
        "grad_norm": 2.888692617416382,
        "learning_rate": 0.0001991139847954373,
        "epoch": 1.3634666666666666,
        "step": 10226
    },
    {
        "loss": 1.6396,
        "grad_norm": 4.628839492797852,
        "learning_rate": 0.00019910560575361566,
        "epoch": 1.3636,
        "step": 10227
    },
    {
        "loss": 2.6173,
        "grad_norm": 3.2284138202667236,
        "learning_rate": 0.00019909718745572588,
        "epoch": 1.3637333333333332,
        "step": 10228
    },
    {
        "loss": 1.8936,
        "grad_norm": 3.8493974208831787,
        "learning_rate": 0.00019908872990510244,
        "epoch": 1.3638666666666666,
        "step": 10229
    },
    {
        "loss": 1.5797,
        "grad_norm": 5.419857978820801,
        "learning_rate": 0.0001990802331050955,
        "epoch": 1.3639999999999999,
        "step": 10230
    },
    {
        "loss": 2.3221,
        "grad_norm": 5.149615287780762,
        "learning_rate": 0.00019907169705907056,
        "epoch": 1.3641333333333332,
        "step": 10231
    },
    {
        "loss": 2.6893,
        "grad_norm": 4.938531875610352,
        "learning_rate": 0.0001990631217704089,
        "epoch": 1.3642666666666667,
        "step": 10232
    },
    {
        "loss": 1.9907,
        "grad_norm": 2.889493703842163,
        "learning_rate": 0.00019905450724250713,
        "epoch": 1.3644,
        "step": 10233
    },
    {
        "loss": 2.6423,
        "grad_norm": 4.629117965698242,
        "learning_rate": 0.00019904585347877752,
        "epoch": 1.3645333333333334,
        "step": 10234
    },
    {
        "loss": 1.3234,
        "grad_norm": 6.370842456817627,
        "learning_rate": 0.00019903716048264784,
        "epoch": 1.3646666666666667,
        "step": 10235
    },
    {
        "loss": 2.3687,
        "grad_norm": 3.8994908332824707,
        "learning_rate": 0.00019902842825756146,
        "epoch": 1.3648,
        "step": 10236
    },
    {
        "loss": 2.0622,
        "grad_norm": 3.5312204360961914,
        "learning_rate": 0.00019901965680697723,
        "epoch": 1.3649333333333333,
        "step": 10237
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.4622933864593506,
        "learning_rate": 0.0001990108461343695,
        "epoch": 1.3650666666666667,
        "step": 10238
    },
    {
        "loss": 1.8075,
        "grad_norm": 4.747612953186035,
        "learning_rate": 0.00019900199624322827,
        "epoch": 1.3652,
        "step": 10239
    },
    {
        "loss": 2.1294,
        "grad_norm": 4.629059314727783,
        "learning_rate": 0.000198993107137059,
        "epoch": 1.3653333333333333,
        "step": 10240
    },
    {
        "loss": 2.1148,
        "grad_norm": 4.256617069244385,
        "learning_rate": 0.00019898417881938262,
        "epoch": 1.3654666666666666,
        "step": 10241
    },
    {
        "loss": 1.272,
        "grad_norm": 4.820775985717773,
        "learning_rate": 0.00019897521129373576,
        "epoch": 1.3656,
        "step": 10242
    },
    {
        "loss": 2.61,
        "grad_norm": 2.4361965656280518,
        "learning_rate": 0.00019896620456367047,
        "epoch": 1.3657333333333335,
        "step": 10243
    },
    {
        "loss": 1.5086,
        "grad_norm": 2.4921507835388184,
        "learning_rate": 0.00019895715863275438,
        "epoch": 1.3658666666666668,
        "step": 10244
    },
    {
        "loss": 1.593,
        "grad_norm": 6.421159744262695,
        "learning_rate": 0.00019894807350457047,
        "epoch": 1.366,
        "step": 10245
    },
    {
        "loss": 2.5706,
        "grad_norm": 4.135919094085693,
        "learning_rate": 0.00019893894918271757,
        "epoch": 1.3661333333333334,
        "step": 10246
    },
    {
        "loss": 2.0925,
        "grad_norm": 6.485594272613525,
        "learning_rate": 0.00019892978567080972,
        "epoch": 1.3662666666666667,
        "step": 10247
    },
    {
        "loss": 2.4319,
        "grad_norm": 6.096234321594238,
        "learning_rate": 0.0001989205829724767,
        "epoch": 1.3664,
        "step": 10248
    },
    {
        "loss": 2.4234,
        "grad_norm": 3.1969451904296875,
        "learning_rate": 0.00019891134109136367,
        "epoch": 1.3665333333333334,
        "step": 10249
    },
    {
        "loss": 0.6076,
        "grad_norm": 3.629154682159424,
        "learning_rate": 0.00019890206003113142,
        "epoch": 1.3666666666666667,
        "step": 10250
    },
    {
        "loss": 1.9453,
        "grad_norm": 3.906722068786621,
        "learning_rate": 0.00019889273979545617,
        "epoch": 1.3668,
        "step": 10251
    },
    {
        "loss": 2.0061,
        "grad_norm": 7.876947402954102,
        "learning_rate": 0.00019888338038802976,
        "epoch": 1.3669333333333333,
        "step": 10252
    },
    {
        "loss": 2.2521,
        "grad_norm": 3.5098965167999268,
        "learning_rate": 0.00019887398181255944,
        "epoch": 1.3670666666666667,
        "step": 10253
    },
    {
        "loss": 2.9143,
        "grad_norm": 3.3806960582733154,
        "learning_rate": 0.00019886454407276797,
        "epoch": 1.3672,
        "step": 10254
    },
    {
        "loss": 1.9115,
        "grad_norm": 4.841450214385986,
        "learning_rate": 0.00019885506717239374,
        "epoch": 1.3673333333333333,
        "step": 10255
    },
    {
        "loss": 1.8946,
        "grad_norm": 3.4753148555755615,
        "learning_rate": 0.00019884555111519058,
        "epoch": 1.3674666666666666,
        "step": 10256
    },
    {
        "loss": 2.607,
        "grad_norm": 5.326353073120117,
        "learning_rate": 0.00019883599590492782,
        "epoch": 1.3676,
        "step": 10257
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.2583794593811035,
        "learning_rate": 0.00019882640154539026,
        "epoch": 1.3677333333333332,
        "step": 10258
    },
    {
        "loss": 1.8021,
        "grad_norm": 5.968078136444092,
        "learning_rate": 0.00019881676804037835,
        "epoch": 1.3678666666666666,
        "step": 10259
    },
    {
        "loss": 1.7051,
        "grad_norm": 4.603637218475342,
        "learning_rate": 0.00019880709539370792,
        "epoch": 1.3679999999999999,
        "step": 10260
    },
    {
        "loss": 2.0339,
        "grad_norm": 3.230104923248291,
        "learning_rate": 0.0001987973836092103,
        "epoch": 1.3681333333333332,
        "step": 10261
    },
    {
        "loss": 2.1314,
        "grad_norm": 4.27816915512085,
        "learning_rate": 0.0001987876326907324,
        "epoch": 1.3682666666666667,
        "step": 10262
    },
    {
        "loss": 1.6892,
        "grad_norm": 4.311403751373291,
        "learning_rate": 0.00019877784264213654,
        "epoch": 1.3684,
        "step": 10263
    },
    {
        "loss": 2.6512,
        "grad_norm": 3.6849255561828613,
        "learning_rate": 0.0001987680134673007,
        "epoch": 1.3685333333333334,
        "step": 10264
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.4928886890411377,
        "learning_rate": 0.00019875814517011814,
        "epoch": 1.3686666666666667,
        "step": 10265
    },
    {
        "loss": 2.4508,
        "grad_norm": 3.2730448246002197,
        "learning_rate": 0.0001987482377544978,
        "epoch": 1.3688,
        "step": 10266
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.702340841293335,
        "learning_rate": 0.00019873829122436397,
        "epoch": 1.3689333333333333,
        "step": 10267
    },
    {
        "loss": 2.7738,
        "grad_norm": 3.1977875232696533,
        "learning_rate": 0.00019872830558365654,
        "epoch": 1.3690666666666667,
        "step": 10268
    },
    {
        "loss": 2.0161,
        "grad_norm": 4.176691055297852,
        "learning_rate": 0.0001987182808363309,
        "epoch": 1.3692,
        "step": 10269
    },
    {
        "loss": 2.3179,
        "grad_norm": 3.4272565841674805,
        "learning_rate": 0.00019870821698635783,
        "epoch": 1.3693333333333333,
        "step": 10270
    },
    {
        "loss": 2.779,
        "grad_norm": 3.6253268718719482,
        "learning_rate": 0.00019869811403772368,
        "epoch": 1.3694666666666666,
        "step": 10271
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.7144761085510254,
        "learning_rate": 0.00019868797199443022,
        "epoch": 1.3696,
        "step": 10272
    },
    {
        "loss": 2.5366,
        "grad_norm": 3.403798818588257,
        "learning_rate": 0.00019867779086049482,
        "epoch": 1.3697333333333335,
        "step": 10273
    },
    {
        "loss": 2.6031,
        "grad_norm": 4.241276741027832,
        "learning_rate": 0.0001986675706399502,
        "epoch": 1.3698666666666668,
        "step": 10274
    },
    {
        "loss": 2.6353,
        "grad_norm": 3.946031332015991,
        "learning_rate": 0.00019865731133684466,
        "epoch": 1.37,
        "step": 10275
    },
    {
        "loss": 2.0068,
        "grad_norm": 2.785614252090454,
        "learning_rate": 0.00019864701295524192,
        "epoch": 1.3701333333333334,
        "step": 10276
    },
    {
        "loss": 2.4286,
        "grad_norm": 3.2784831523895264,
        "learning_rate": 0.0001986366754992212,
        "epoch": 1.3702666666666667,
        "step": 10277
    },
    {
        "loss": 2.2618,
        "grad_norm": 4.294848442077637,
        "learning_rate": 0.00019862629897287728,
        "epoch": 1.3704,
        "step": 10278
    },
    {
        "loss": 2.0217,
        "grad_norm": 3.962570905685425,
        "learning_rate": 0.00019861588338032022,
        "epoch": 1.3705333333333334,
        "step": 10279
    },
    {
        "loss": 2.3646,
        "grad_norm": 3.01218318939209,
        "learning_rate": 0.00019860542872567576,
        "epoch": 1.3706666666666667,
        "step": 10280
    },
    {
        "loss": 2.369,
        "grad_norm": 2.6384801864624023,
        "learning_rate": 0.00019859493501308496,
        "epoch": 1.3708,
        "step": 10281
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.5573890209198,
        "learning_rate": 0.0001985844022467045,
        "epoch": 1.3709333333333333,
        "step": 10282
    },
    {
        "loss": 1.9543,
        "grad_norm": 3.664241075515747,
        "learning_rate": 0.00019857383043070636,
        "epoch": 1.3710666666666667,
        "step": 10283
    },
    {
        "loss": 1.836,
        "grad_norm": 3.198723316192627,
        "learning_rate": 0.0001985632195692781,
        "epoch": 1.3712,
        "step": 10284
    },
    {
        "loss": 1.9335,
        "grad_norm": 3.9091713428497314,
        "learning_rate": 0.0001985525696666227,
        "epoch": 1.3713333333333333,
        "step": 10285
    },
    {
        "loss": 2.3351,
        "grad_norm": 3.2403831481933594,
        "learning_rate": 0.0001985418807269587,
        "epoch": 1.3714666666666666,
        "step": 10286
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.7379086017608643,
        "learning_rate": 0.00019853115275451994,
        "epoch": 1.3716,
        "step": 10287
    },
    {
        "loss": 2.749,
        "grad_norm": 3.42187237739563,
        "learning_rate": 0.00019852038575355582,
        "epoch": 1.3717333333333332,
        "step": 10288
    },
    {
        "loss": 1.7407,
        "grad_norm": 3.786071538925171,
        "learning_rate": 0.0001985095797283312,
        "epoch": 1.3718666666666666,
        "step": 10289
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.422577381134033,
        "learning_rate": 0.00019849873468312642,
        "epoch": 1.3719999999999999,
        "step": 10290
    },
    {
        "loss": 2.2222,
        "grad_norm": 2.644598960876465,
        "learning_rate": 0.0001984878506222372,
        "epoch": 1.3721333333333332,
        "step": 10291
    },
    {
        "loss": 2.0806,
        "grad_norm": 4.2225165367126465,
        "learning_rate": 0.00019847692754997476,
        "epoch": 1.3722666666666667,
        "step": 10292
    },
    {
        "loss": 2.7426,
        "grad_norm": 4.219064712524414,
        "learning_rate": 0.00019846596547066578,
        "epoch": 1.3724,
        "step": 10293
    },
    {
        "loss": 1.8836,
        "grad_norm": 3.455272674560547,
        "learning_rate": 0.00019845496438865233,
        "epoch": 1.3725333333333334,
        "step": 10294
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.994687557220459,
        "learning_rate": 0.000198443924308292,
        "epoch": 1.3726666666666667,
        "step": 10295
    },
    {
        "loss": 2.348,
        "grad_norm": 2.8782737255096436,
        "learning_rate": 0.00019843284523395787,
        "epoch": 1.3728,
        "step": 10296
    },
    {
        "loss": 2.3614,
        "grad_norm": 3.564260959625244,
        "learning_rate": 0.0001984217271700383,
        "epoch": 1.3729333333333333,
        "step": 10297
    },
    {
        "loss": 2.5452,
        "grad_norm": 3.1885955333709717,
        "learning_rate": 0.00019841057012093724,
        "epoch": 1.3730666666666667,
        "step": 10298
    },
    {
        "loss": 1.8763,
        "grad_norm": 3.9539053440093994,
        "learning_rate": 0.00019839937409107402,
        "epoch": 1.3732,
        "step": 10299
    },
    {
        "loss": 2.7733,
        "grad_norm": 4.476062297821045,
        "learning_rate": 0.00019838813908488342,
        "epoch": 1.3733333333333333,
        "step": 10300
    },
    {
        "loss": 2.1056,
        "grad_norm": 2.5919837951660156,
        "learning_rate": 0.00019837686510681572,
        "epoch": 1.3734666666666666,
        "step": 10301
    },
    {
        "loss": 0.64,
        "grad_norm": 2.963809013366699,
        "learning_rate": 0.00019836555216133653,
        "epoch": 1.3736,
        "step": 10302
    },
    {
        "loss": 2.2508,
        "grad_norm": 3.169066905975342,
        "learning_rate": 0.00019835420025292696,
        "epoch": 1.3737333333333333,
        "step": 10303
    },
    {
        "loss": 1.1101,
        "grad_norm": 3.7085001468658447,
        "learning_rate": 0.00019834280938608353,
        "epoch": 1.3738666666666668,
        "step": 10304
    },
    {
        "loss": 2.3267,
        "grad_norm": 3.3102574348449707,
        "learning_rate": 0.0001983313795653182,
        "epoch": 1.374,
        "step": 10305
    },
    {
        "loss": 2.0694,
        "grad_norm": 4.797055244445801,
        "learning_rate": 0.00019831991079515834,
        "epoch": 1.3741333333333334,
        "step": 10306
    },
    {
        "loss": 2.4796,
        "grad_norm": 4.314819812774658,
        "learning_rate": 0.00019830840308014686,
        "epoch": 1.3742666666666667,
        "step": 10307
    },
    {
        "loss": 2.1319,
        "grad_norm": 3.280287504196167,
        "learning_rate": 0.0001982968564248419,
        "epoch": 1.3744,
        "step": 10308
    },
    {
        "loss": 2.2741,
        "grad_norm": 3.5720551013946533,
        "learning_rate": 0.00019828527083381715,
        "epoch": 1.3745333333333334,
        "step": 10309
    },
    {
        "loss": 2.3106,
        "grad_norm": 2.897085428237915,
        "learning_rate": 0.00019827364631166177,
        "epoch": 1.3746666666666667,
        "step": 10310
    },
    {
        "loss": 2.4028,
        "grad_norm": 3.1695380210876465,
        "learning_rate": 0.00019826198286298024,
        "epoch": 1.3748,
        "step": 10311
    },
    {
        "loss": 1.4644,
        "grad_norm": 3.801934242248535,
        "learning_rate": 0.00019825028049239244,
        "epoch": 1.3749333333333333,
        "step": 10312
    },
    {
        "loss": 1.2223,
        "grad_norm": 3.912108898162842,
        "learning_rate": 0.00019823853920453375,
        "epoch": 1.3750666666666667,
        "step": 10313
    },
    {
        "loss": 2.0001,
        "grad_norm": 3.2124645709991455,
        "learning_rate": 0.00019822675900405497,
        "epoch": 1.3752,
        "step": 10314
    },
    {
        "loss": 2.0829,
        "grad_norm": 3.433242082595825,
        "learning_rate": 0.0001982149398956222,
        "epoch": 1.3753333333333333,
        "step": 10315
    },
    {
        "loss": 2.3856,
        "grad_norm": 3.716996908187866,
        "learning_rate": 0.00019820308188391717,
        "epoch": 1.3754666666666666,
        "step": 10316
    },
    {
        "loss": 2.5881,
        "grad_norm": 3.0983240604400635,
        "learning_rate": 0.0001981911849736367,
        "epoch": 1.3756,
        "step": 10317
    },
    {
        "loss": 1.1847,
        "grad_norm": 4.400412559509277,
        "learning_rate": 0.00019817924916949332,
        "epoch": 1.3757333333333333,
        "step": 10318
    },
    {
        "loss": 2.0142,
        "grad_norm": 4.379990577697754,
        "learning_rate": 0.0001981672744762148,
        "epoch": 1.3758666666666666,
        "step": 10319
    },
    {
        "loss": 2.7706,
        "grad_norm": 2.997098207473755,
        "learning_rate": 0.00019815526089854438,
        "epoch": 1.376,
        "step": 10320
    },
    {
        "loss": 1.8763,
        "grad_norm": 5.736509323120117,
        "learning_rate": 0.00019814320844124066,
        "epoch": 1.3761333333333332,
        "step": 10321
    },
    {
        "loss": 2.5013,
        "grad_norm": 2.5491912364959717,
        "learning_rate": 0.0001981311171090776,
        "epoch": 1.3762666666666667,
        "step": 10322
    },
    {
        "loss": 1.3279,
        "grad_norm": 2.2028121948242188,
        "learning_rate": 0.00019811898690684477,
        "epoch": 1.3764,
        "step": 10323
    },
    {
        "loss": 0.7731,
        "grad_norm": 4.240290641784668,
        "learning_rate": 0.00019810681783934687,
        "epoch": 1.3765333333333334,
        "step": 10324
    },
    {
        "loss": 1.4274,
        "grad_norm": 3.2907652854919434,
        "learning_rate": 0.00019809460991140413,
        "epoch": 1.3766666666666667,
        "step": 10325
    },
    {
        "loss": 2.3153,
        "grad_norm": 2.969623565673828,
        "learning_rate": 0.00019808236312785212,
        "epoch": 1.3768,
        "step": 10326
    },
    {
        "loss": 1.2623,
        "grad_norm": 4.078336238861084,
        "learning_rate": 0.00019807007749354186,
        "epoch": 1.3769333333333333,
        "step": 10327
    },
    {
        "loss": 2.2962,
        "grad_norm": 4.455415725708008,
        "learning_rate": 0.0001980577530133398,
        "epoch": 1.3770666666666667,
        "step": 10328
    },
    {
        "loss": 2.7908,
        "grad_norm": 4.1948137283325195,
        "learning_rate": 0.0001980453896921276,
        "epoch": 1.3772,
        "step": 10329
    },
    {
        "loss": 1.9962,
        "grad_norm": 3.450944185256958,
        "learning_rate": 0.00019803298753480247,
        "epoch": 1.3773333333333333,
        "step": 10330
    },
    {
        "loss": 2.0569,
        "grad_norm": 4.146583557128906,
        "learning_rate": 0.00019802054654627696,
        "epoch": 1.3774666666666666,
        "step": 10331
    },
    {
        "loss": 1.8135,
        "grad_norm": 3.6383237838745117,
        "learning_rate": 0.00019800806673147894,
        "epoch": 1.3776,
        "step": 10332
    },
    {
        "loss": 1.9024,
        "grad_norm": 4.036456108093262,
        "learning_rate": 0.00019799554809535176,
        "epoch": 1.3777333333333333,
        "step": 10333
    },
    {
        "loss": 2.3146,
        "grad_norm": 5.403249740600586,
        "learning_rate": 0.00019798299064285405,
        "epoch": 1.3778666666666668,
        "step": 10334
    },
    {
        "loss": 2.5526,
        "grad_norm": 2.8748137950897217,
        "learning_rate": 0.00019797039437895989,
        "epoch": 1.3780000000000001,
        "step": 10335
    },
    {
        "loss": 1.9953,
        "grad_norm": 4.6253886222839355,
        "learning_rate": 0.00019795775930865866,
        "epoch": 1.3781333333333334,
        "step": 10336
    },
    {
        "loss": 1.3309,
        "grad_norm": 3.8013482093811035,
        "learning_rate": 0.00019794508543695524,
        "epoch": 1.3782666666666668,
        "step": 10337
    },
    {
        "loss": 2.248,
        "grad_norm": 3.297759532928467,
        "learning_rate": 0.0001979323727688697,
        "epoch": 1.3784,
        "step": 10338
    },
    {
        "loss": 2.57,
        "grad_norm": 2.35378098487854,
        "learning_rate": 0.0001979196213094376,
        "epoch": 1.3785333333333334,
        "step": 10339
    },
    {
        "loss": 1.5122,
        "grad_norm": 4.737290382385254,
        "learning_rate": 0.00019790683106370985,
        "epoch": 1.3786666666666667,
        "step": 10340
    },
    {
        "loss": 2.8224,
        "grad_norm": 3.8162825107574463,
        "learning_rate": 0.0001978940020367527,
        "epoch": 1.3788,
        "step": 10341
    },
    {
        "loss": 2.3719,
        "grad_norm": 4.1070451736450195,
        "learning_rate": 0.00019788113423364784,
        "epoch": 1.3789333333333333,
        "step": 10342
    },
    {
        "loss": 2.2651,
        "grad_norm": 5.668233871459961,
        "learning_rate": 0.00019786822765949214,
        "epoch": 1.3790666666666667,
        "step": 10343
    },
    {
        "loss": 1.8223,
        "grad_norm": 3.35306715965271,
        "learning_rate": 0.00019785528231939798,
        "epoch": 1.3792,
        "step": 10344
    },
    {
        "loss": 2.3165,
        "grad_norm": 3.1574697494506836,
        "learning_rate": 0.00019784229821849308,
        "epoch": 1.3793333333333333,
        "step": 10345
    },
    {
        "loss": 2.3098,
        "grad_norm": 3.8057360649108887,
        "learning_rate": 0.00019782927536192047,
        "epoch": 1.3794666666666666,
        "step": 10346
    },
    {
        "loss": 2.4052,
        "grad_norm": 4.1944732666015625,
        "learning_rate": 0.0001978162137548385,
        "epoch": 1.3796,
        "step": 10347
    },
    {
        "loss": 0.8715,
        "grad_norm": 5.336772918701172,
        "learning_rate": 0.000197803113402421,
        "epoch": 1.3797333333333333,
        "step": 10348
    },
    {
        "loss": 1.7439,
        "grad_norm": 4.2373270988464355,
        "learning_rate": 0.00019778997430985702,
        "epoch": 1.3798666666666666,
        "step": 10349
    },
    {
        "loss": 2.5734,
        "grad_norm": 2.013211250305176,
        "learning_rate": 0.00019777679648235098,
        "epoch": 1.38,
        "step": 10350
    },
    {
        "loss": 0.9332,
        "grad_norm": 3.7859530448913574,
        "learning_rate": 0.00019776357992512273,
        "epoch": 1.3801333333333332,
        "step": 10351
    },
    {
        "loss": 2.6127,
        "grad_norm": 4.062570095062256,
        "learning_rate": 0.00019775032464340733,
        "epoch": 1.3802666666666665,
        "step": 10352
    },
    {
        "loss": 2.174,
        "grad_norm": 4.130375862121582,
        "learning_rate": 0.00019773703064245523,
        "epoch": 1.3804,
        "step": 10353
    },
    {
        "loss": 1.793,
        "grad_norm": 4.304019927978516,
        "learning_rate": 0.00019772369792753233,
        "epoch": 1.3805333333333334,
        "step": 10354
    },
    {
        "loss": 1.845,
        "grad_norm": 5.5396246910095215,
        "learning_rate": 0.00019771032650391966,
        "epoch": 1.3806666666666667,
        "step": 10355
    },
    {
        "loss": 3.4169,
        "grad_norm": 4.769949913024902,
        "learning_rate": 0.0001976969163769137,
        "epoch": 1.3808,
        "step": 10356
    },
    {
        "loss": 1.4346,
        "grad_norm": 4.11615514755249,
        "learning_rate": 0.0001976834675518263,
        "epoch": 1.3809333333333333,
        "step": 10357
    },
    {
        "loss": 1.7905,
        "grad_norm": 3.5415773391723633,
        "learning_rate": 0.00019766998003398453,
        "epoch": 1.3810666666666667,
        "step": 10358
    },
    {
        "loss": 1.8601,
        "grad_norm": 4.324472427368164,
        "learning_rate": 0.00019765645382873088,
        "epoch": 1.3812,
        "step": 10359
    },
    {
        "loss": 2.72,
        "grad_norm": 3.4969215393066406,
        "learning_rate": 0.00019764288894142312,
        "epoch": 1.3813333333333333,
        "step": 10360
    },
    {
        "loss": 2.5348,
        "grad_norm": 4.03546667098999,
        "learning_rate": 0.00019762928537743433,
        "epoch": 1.3814666666666666,
        "step": 10361
    },
    {
        "loss": 2.1653,
        "grad_norm": 4.181326389312744,
        "learning_rate": 0.00019761564314215298,
        "epoch": 1.3816,
        "step": 10362
    },
    {
        "loss": 0.6022,
        "grad_norm": 5.152568817138672,
        "learning_rate": 0.0001976019622409827,
        "epoch": 1.3817333333333333,
        "step": 10363
    },
    {
        "loss": 1.1845,
        "grad_norm": 7.779303550720215,
        "learning_rate": 0.00019758824267934264,
        "epoch": 1.3818666666666668,
        "step": 10364
    },
    {
        "loss": 2.6663,
        "grad_norm": 3.3363037109375,
        "learning_rate": 0.0001975744844626671,
        "epoch": 1.3820000000000001,
        "step": 10365
    },
    {
        "loss": 2.9759,
        "grad_norm": 2.6225833892822266,
        "learning_rate": 0.00019756068759640584,
        "epoch": 1.3821333333333334,
        "step": 10366
    },
    {
        "loss": 1.6931,
        "grad_norm": 3.302311420440674,
        "learning_rate": 0.00019754685208602375,
        "epoch": 1.3822666666666668,
        "step": 10367
    },
    {
        "loss": 2.7436,
        "grad_norm": 3.79984974861145,
        "learning_rate": 0.00019753297793700115,
        "epoch": 1.3824,
        "step": 10368
    },
    {
        "loss": 1.5006,
        "grad_norm": 4.081418037414551,
        "learning_rate": 0.00019751906515483368,
        "epoch": 1.3825333333333334,
        "step": 10369
    },
    {
        "loss": 2.0251,
        "grad_norm": 6.053056240081787,
        "learning_rate": 0.00019750511374503225,
        "epoch": 1.3826666666666667,
        "step": 10370
    },
    {
        "loss": 2.4844,
        "grad_norm": 2.8039965629577637,
        "learning_rate": 0.000197491123713123,
        "epoch": 1.3828,
        "step": 10371
    },
    {
        "loss": 1.8976,
        "grad_norm": 4.593446731567383,
        "learning_rate": 0.0001974770950646474,
        "epoch": 1.3829333333333333,
        "step": 10372
    },
    {
        "loss": 2.4688,
        "grad_norm": 4.207582473754883,
        "learning_rate": 0.00019746302780516238,
        "epoch": 1.3830666666666667,
        "step": 10373
    },
    {
        "loss": 1.6169,
        "grad_norm": 3.1010639667510986,
        "learning_rate": 0.00019744892194023986,
        "epoch": 1.3832,
        "step": 10374
    },
    {
        "loss": 2.359,
        "grad_norm": 2.8009345531463623,
        "learning_rate": 0.0001974347774754674,
        "epoch": 1.3833333333333333,
        "step": 10375
    },
    {
        "loss": 2.0379,
        "grad_norm": 4.493375301361084,
        "learning_rate": 0.00019742059441644756,
        "epoch": 1.3834666666666666,
        "step": 10376
    },
    {
        "loss": 2.3299,
        "grad_norm": 4.244520664215088,
        "learning_rate": 0.00019740637276879826,
        "epoch": 1.3836,
        "step": 10377
    },
    {
        "loss": 2.4469,
        "grad_norm": 3.18725323677063,
        "learning_rate": 0.00019739211253815285,
        "epoch": 1.3837333333333333,
        "step": 10378
    },
    {
        "loss": 2.2605,
        "grad_norm": 4.704263210296631,
        "learning_rate": 0.00019737781373015985,
        "epoch": 1.3838666666666666,
        "step": 10379
    },
    {
        "loss": 2.2862,
        "grad_norm": 3.601534605026245,
        "learning_rate": 0.00019736347635048297,
        "epoch": 1.384,
        "step": 10380
    },
    {
        "loss": 2.0124,
        "grad_norm": 2.3687362670898438,
        "learning_rate": 0.00019734910040480137,
        "epoch": 1.3841333333333332,
        "step": 10381
    },
    {
        "loss": 2.3927,
        "grad_norm": 5.222123622894287,
        "learning_rate": 0.00019733468589880937,
        "epoch": 1.3842666666666665,
        "step": 10382
    },
    {
        "loss": 1.8132,
        "grad_norm": 5.151547431945801,
        "learning_rate": 0.00019732023283821666,
        "epoch": 1.3844,
        "step": 10383
    },
    {
        "loss": 2.1396,
        "grad_norm": 2.820162057876587,
        "learning_rate": 0.0001973057412287481,
        "epoch": 1.3845333333333334,
        "step": 10384
    },
    {
        "loss": 2.7928,
        "grad_norm": 3.2419865131378174,
        "learning_rate": 0.00019729121107614383,
        "epoch": 1.3846666666666667,
        "step": 10385
    },
    {
        "loss": 2.478,
        "grad_norm": 4.545865058898926,
        "learning_rate": 0.00019727664238615935,
        "epoch": 1.3848,
        "step": 10386
    },
    {
        "loss": 1.9305,
        "grad_norm": 3.909498691558838,
        "learning_rate": 0.00019726203516456546,
        "epoch": 1.3849333333333333,
        "step": 10387
    },
    {
        "loss": 2.5185,
        "grad_norm": 4.820404052734375,
        "learning_rate": 0.00019724738941714792,
        "epoch": 1.3850666666666667,
        "step": 10388
    },
    {
        "loss": 1.8916,
        "grad_norm": 3.9578239917755127,
        "learning_rate": 0.0001972327051497081,
        "epoch": 1.3852,
        "step": 10389
    },
    {
        "loss": 2.2922,
        "grad_norm": 3.968106746673584,
        "learning_rate": 0.00019721798236806246,
        "epoch": 1.3853333333333333,
        "step": 10390
    },
    {
        "loss": 2.3039,
        "grad_norm": 4.511802673339844,
        "learning_rate": 0.00019720322107804274,
        "epoch": 1.3854666666666666,
        "step": 10391
    },
    {
        "loss": 1.7266,
        "grad_norm": 2.663754940032959,
        "learning_rate": 0.00019718842128549596,
        "epoch": 1.3856,
        "step": 10392
    },
    {
        "loss": 2.8302,
        "grad_norm": 3.426509141921997,
        "learning_rate": 0.00019717358299628434,
        "epoch": 1.3857333333333333,
        "step": 10393
    },
    {
        "loss": 0.8188,
        "grad_norm": 3.4216396808624268,
        "learning_rate": 0.00019715870621628536,
        "epoch": 1.3858666666666668,
        "step": 10394
    },
    {
        "loss": 2.3082,
        "grad_norm": 3.5569872856140137,
        "learning_rate": 0.0001971437909513918,
        "epoch": 1.3860000000000001,
        "step": 10395
    },
    {
        "loss": 0.5638,
        "grad_norm": 3.456209182739258,
        "learning_rate": 0.00019712883720751165,
        "epoch": 1.3861333333333334,
        "step": 10396
    },
    {
        "loss": 1.5898,
        "grad_norm": 3.7320637702941895,
        "learning_rate": 0.0001971138449905681,
        "epoch": 1.3862666666666668,
        "step": 10397
    },
    {
        "loss": 2.0216,
        "grad_norm": 3.242856502532959,
        "learning_rate": 0.00019709881430649966,
        "epoch": 1.3864,
        "step": 10398
    },
    {
        "loss": 1.4647,
        "grad_norm": 4.404478549957275,
        "learning_rate": 0.00019708374516126,
        "epoch": 1.3865333333333334,
        "step": 10399
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.3916687965393066,
        "learning_rate": 0.0001970686375608181,
        "epoch": 1.3866666666666667,
        "step": 10400
    },
    {
        "loss": 2.119,
        "grad_norm": 4.4396209716796875,
        "learning_rate": 0.0001970534915111581,
        "epoch": 1.3868,
        "step": 10401
    },
    {
        "loss": 1.7764,
        "grad_norm": 3.919131278991699,
        "learning_rate": 0.00019703830701827944,
        "epoch": 1.3869333333333334,
        "step": 10402
    },
    {
        "loss": 2.2447,
        "grad_norm": 3.765319585800171,
        "learning_rate": 0.00019702308408819667,
        "epoch": 1.3870666666666667,
        "step": 10403
    },
    {
        "loss": 2.0482,
        "grad_norm": 3.241306781768799,
        "learning_rate": 0.0001970078227269397,
        "epoch": 1.3872,
        "step": 10404
    },
    {
        "loss": 2.1168,
        "grad_norm": 3.9330835342407227,
        "learning_rate": 0.00019699252294055363,
        "epoch": 1.3873333333333333,
        "step": 10405
    },
    {
        "loss": 2.2158,
        "grad_norm": 4.555774688720703,
        "learning_rate": 0.0001969771847350987,
        "epoch": 1.3874666666666666,
        "step": 10406
    },
    {
        "loss": 0.7635,
        "grad_norm": 5.034947395324707,
        "learning_rate": 0.00019696180811665047,
        "epoch": 1.3876,
        "step": 10407
    },
    {
        "loss": 2.3145,
        "grad_norm": 3.1283011436462402,
        "learning_rate": 0.00019694639309129967,
        "epoch": 1.3877333333333333,
        "step": 10408
    },
    {
        "loss": 1.3472,
        "grad_norm": 3.5084445476531982,
        "learning_rate": 0.0001969309396651522,
        "epoch": 1.3878666666666666,
        "step": 10409
    },
    {
        "loss": 2.5194,
        "grad_norm": 4.159595489501953,
        "learning_rate": 0.00019691544784432928,
        "epoch": 1.388,
        "step": 10410
    },
    {
        "loss": 2.2957,
        "grad_norm": 3.6707589626312256,
        "learning_rate": 0.0001968999176349672,
        "epoch": 1.3881333333333332,
        "step": 10411
    },
    {
        "loss": 0.5869,
        "grad_norm": 7.59894323348999,
        "learning_rate": 0.00019688434904321754,
        "epoch": 1.3882666666666665,
        "step": 10412
    },
    {
        "loss": 1.8917,
        "grad_norm": 3.8617823123931885,
        "learning_rate": 0.00019686874207524717,
        "epoch": 1.3884,
        "step": 10413
    },
    {
        "loss": 1.5596,
        "grad_norm": 5.503599166870117,
        "learning_rate": 0.00019685309673723796,
        "epoch": 1.3885333333333334,
        "step": 10414
    },
    {
        "loss": 1.8119,
        "grad_norm": 4.000231742858887,
        "learning_rate": 0.00019683741303538711,
        "epoch": 1.3886666666666667,
        "step": 10415
    },
    {
        "loss": 1.8627,
        "grad_norm": 3.4466326236724854,
        "learning_rate": 0.00019682169097590702,
        "epoch": 1.3888,
        "step": 10416
    },
    {
        "loss": 2.8116,
        "grad_norm": 3.414530038833618,
        "learning_rate": 0.00019680593056502522,
        "epoch": 1.3889333333333334,
        "step": 10417
    },
    {
        "loss": 3.3862,
        "grad_norm": 4.484805583953857,
        "learning_rate": 0.0001967901318089844,
        "epoch": 1.3890666666666667,
        "step": 10418
    },
    {
        "loss": 2.6897,
        "grad_norm": 4.64445161819458,
        "learning_rate": 0.00019677429471404262,
        "epoch": 1.3892,
        "step": 10419
    },
    {
        "loss": 1.2937,
        "grad_norm": 3.6708600521087646,
        "learning_rate": 0.00019675841928647295,
        "epoch": 1.3893333333333333,
        "step": 10420
    },
    {
        "loss": 1.9531,
        "grad_norm": 3.166571855545044,
        "learning_rate": 0.00019674250553256372,
        "epoch": 1.3894666666666666,
        "step": 10421
    },
    {
        "loss": 2.7111,
        "grad_norm": 5.108421802520752,
        "learning_rate": 0.00019672655345861836,
        "epoch": 1.3896,
        "step": 10422
    },
    {
        "loss": 2.8463,
        "grad_norm": 5.530219554901123,
        "learning_rate": 0.00019671056307095564,
        "epoch": 1.3897333333333333,
        "step": 10423
    },
    {
        "loss": 1.9004,
        "grad_norm": 3.8776280879974365,
        "learning_rate": 0.0001966945343759093,
        "epoch": 1.3898666666666666,
        "step": 10424
    },
    {
        "loss": 1.6137,
        "grad_norm": 6.09841775894165,
        "learning_rate": 0.0001966784673798285,
        "epoch": 1.3900000000000001,
        "step": 10425
    },
    {
        "loss": 1.248,
        "grad_norm": 4.257434368133545,
        "learning_rate": 0.00019666236208907724,
        "epoch": 1.3901333333333334,
        "step": 10426
    },
    {
        "loss": 2.1358,
        "grad_norm": 3.138878583908081,
        "learning_rate": 0.00019664621851003502,
        "epoch": 1.3902666666666668,
        "step": 10427
    },
    {
        "loss": 1.1527,
        "grad_norm": 4.168941974639893,
        "learning_rate": 0.00019663003664909638,
        "epoch": 1.3904,
        "step": 10428
    },
    {
        "loss": 1.998,
        "grad_norm": 3.8379833698272705,
        "learning_rate": 0.00019661381651267095,
        "epoch": 1.3905333333333334,
        "step": 10429
    },
    {
        "loss": 2.2334,
        "grad_norm": 3.138946771621704,
        "learning_rate": 0.00019659755810718358,
        "epoch": 1.3906666666666667,
        "step": 10430
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.795628070831299,
        "learning_rate": 0.00019658126143907428,
        "epoch": 1.3908,
        "step": 10431
    },
    {
        "loss": 0.8584,
        "grad_norm": 4.297008037567139,
        "learning_rate": 0.00019656492651479827,
        "epoch": 1.3909333333333334,
        "step": 10432
    },
    {
        "loss": 1.9164,
        "grad_norm": 3.554283380508423,
        "learning_rate": 0.00019654855334082577,
        "epoch": 1.3910666666666667,
        "step": 10433
    },
    {
        "loss": 2.183,
        "grad_norm": 3.1953020095825195,
        "learning_rate": 0.0001965321419236424,
        "epoch": 1.3912,
        "step": 10434
    },
    {
        "loss": 1.8727,
        "grad_norm": 5.261538505554199,
        "learning_rate": 0.00019651569226974863,
        "epoch": 1.3913333333333333,
        "step": 10435
    },
    {
        "loss": 1.98,
        "grad_norm": 5.34848690032959,
        "learning_rate": 0.00019649920438566024,
        "epoch": 1.3914666666666666,
        "step": 10436
    },
    {
        "loss": 1.9692,
        "grad_norm": 3.1410582065582275,
        "learning_rate": 0.00019648267827790825,
        "epoch": 1.3916,
        "step": 10437
    },
    {
        "loss": 1.6512,
        "grad_norm": 5.696277618408203,
        "learning_rate": 0.00019646611395303864,
        "epoch": 1.3917333333333333,
        "step": 10438
    },
    {
        "loss": 1.7548,
        "grad_norm": 4.183567047119141,
        "learning_rate": 0.0001964495114176126,
        "epoch": 1.3918666666666666,
        "step": 10439
    },
    {
        "loss": 2.8651,
        "grad_norm": 3.4737300872802734,
        "learning_rate": 0.00019643287067820636,
        "epoch": 1.392,
        "step": 10440
    },
    {
        "loss": 1.4637,
        "grad_norm": 5.011123180389404,
        "learning_rate": 0.00019641619174141155,
        "epoch": 1.3921333333333332,
        "step": 10441
    },
    {
        "loss": 1.4979,
        "grad_norm": 4.512730121612549,
        "learning_rate": 0.00019639947461383463,
        "epoch": 1.3922666666666665,
        "step": 10442
    },
    {
        "loss": 2.1853,
        "grad_norm": 2.4155476093292236,
        "learning_rate": 0.00019638271930209734,
        "epoch": 1.3924,
        "step": 10443
    },
    {
        "loss": 2.0852,
        "grad_norm": 3.5929574966430664,
        "learning_rate": 0.00019636592581283652,
        "epoch": 1.3925333333333334,
        "step": 10444
    },
    {
        "loss": 2.1752,
        "grad_norm": 3.739311695098877,
        "learning_rate": 0.00019634909415270407,
        "epoch": 1.3926666666666667,
        "step": 10445
    },
    {
        "loss": 2.2052,
        "grad_norm": 3.8890221118927,
        "learning_rate": 0.00019633222432836723,
        "epoch": 1.3928,
        "step": 10446
    },
    {
        "loss": 1.8603,
        "grad_norm": 5.075820446014404,
        "learning_rate": 0.000196315316346508,
        "epoch": 1.3929333333333334,
        "step": 10447
    },
    {
        "loss": 1.9778,
        "grad_norm": 3.6997878551483154,
        "learning_rate": 0.00019629837021382384,
        "epoch": 1.3930666666666667,
        "step": 10448
    },
    {
        "loss": 1.7532,
        "grad_norm": 4.873267650604248,
        "learning_rate": 0.00019628138593702707,
        "epoch": 1.3932,
        "step": 10449
    },
    {
        "loss": 2.4184,
        "grad_norm": 4.975508213043213,
        "learning_rate": 0.00019626436352284527,
        "epoch": 1.3933333333333333,
        "step": 10450
    },
    {
        "loss": 2.1213,
        "grad_norm": 5.417428970336914,
        "learning_rate": 0.0001962473029780211,
        "epoch": 1.3934666666666666,
        "step": 10451
    },
    {
        "loss": 1.7322,
        "grad_norm": 6.708042621612549,
        "learning_rate": 0.00019623020430931224,
        "epoch": 1.3936,
        "step": 10452
    },
    {
        "loss": 2.4197,
        "grad_norm": 3.505241870880127,
        "learning_rate": 0.00019621306752349153,
        "epoch": 1.3937333333333333,
        "step": 10453
    },
    {
        "loss": 2.1288,
        "grad_norm": 3.7201569080352783,
        "learning_rate": 0.00019619589262734693,
        "epoch": 1.3938666666666666,
        "step": 10454
    },
    {
        "loss": 2.2018,
        "grad_norm": 2.5478036403656006,
        "learning_rate": 0.00019617867962768158,
        "epoch": 1.3940000000000001,
        "step": 10455
    },
    {
        "loss": 2.5635,
        "grad_norm": 3.7816336154937744,
        "learning_rate": 0.00019616142853131342,
        "epoch": 1.3941333333333334,
        "step": 10456
    },
    {
        "loss": 2.4218,
        "grad_norm": 3.9896240234375,
        "learning_rate": 0.00019614413934507577,
        "epoch": 1.3942666666666668,
        "step": 10457
    },
    {
        "loss": 1.6664,
        "grad_norm": 3.973640203475952,
        "learning_rate": 0.00019612681207581688,
        "epoch": 1.3944,
        "step": 10458
    },
    {
        "loss": 1.3988,
        "grad_norm": 4.111894607543945,
        "learning_rate": 0.00019610944673040023,
        "epoch": 1.3945333333333334,
        "step": 10459
    },
    {
        "loss": 1.2687,
        "grad_norm": 4.227956295013428,
        "learning_rate": 0.00019609204331570427,
        "epoch": 1.3946666666666667,
        "step": 10460
    },
    {
        "loss": 2.8752,
        "grad_norm": 2.5339043140411377,
        "learning_rate": 0.00019607460183862252,
        "epoch": 1.3948,
        "step": 10461
    },
    {
        "loss": 1.3851,
        "grad_norm": 4.038857460021973,
        "learning_rate": 0.00019605712230606358,
        "epoch": 1.3949333333333334,
        "step": 10462
    },
    {
        "loss": 2.2615,
        "grad_norm": 3.290347099304199,
        "learning_rate": 0.00019603960472495122,
        "epoch": 1.3950666666666667,
        "step": 10463
    },
    {
        "loss": 1.9526,
        "grad_norm": 4.189679145812988,
        "learning_rate": 0.00019602204910222417,
        "epoch": 1.3952,
        "step": 10464
    },
    {
        "loss": 2.8448,
        "grad_norm": 2.6246156692504883,
        "learning_rate": 0.00019600445544483625,
        "epoch": 1.3953333333333333,
        "step": 10465
    },
    {
        "loss": 1.7999,
        "grad_norm": 3.5888335704803467,
        "learning_rate": 0.00019598682375975644,
        "epoch": 1.3954666666666666,
        "step": 10466
    },
    {
        "loss": 2.0987,
        "grad_norm": 2.552427291870117,
        "learning_rate": 0.00019596915405396869,
        "epoch": 1.3956,
        "step": 10467
    },
    {
        "loss": 1.9158,
        "grad_norm": 3.98307466506958,
        "learning_rate": 0.00019595144633447197,
        "epoch": 1.3957333333333333,
        "step": 10468
    },
    {
        "loss": 2.7322,
        "grad_norm": 5.135439872741699,
        "learning_rate": 0.00019593370060828043,
        "epoch": 1.3958666666666666,
        "step": 10469
    },
    {
        "loss": 2.1515,
        "grad_norm": 4.00147819519043,
        "learning_rate": 0.0001959159168824232,
        "epoch": 1.396,
        "step": 10470
    },
    {
        "loss": 1.8897,
        "grad_norm": 4.073206901550293,
        "learning_rate": 0.00019589809516394447,
        "epoch": 1.3961333333333332,
        "step": 10471
    },
    {
        "loss": 1.4883,
        "grad_norm": 2.7640280723571777,
        "learning_rate": 0.00019588023545990345,
        "epoch": 1.3962666666666665,
        "step": 10472
    },
    {
        "loss": 2.3928,
        "grad_norm": 3.908263683319092,
        "learning_rate": 0.00019586233777737447,
        "epoch": 1.3963999999999999,
        "step": 10473
    },
    {
        "loss": 1.6586,
        "grad_norm": 4.045062065124512,
        "learning_rate": 0.00019584440212344685,
        "epoch": 1.3965333333333334,
        "step": 10474
    },
    {
        "loss": 1.9675,
        "grad_norm": 3.8082170486450195,
        "learning_rate": 0.00019582642850522497,
        "epoch": 1.3966666666666667,
        "step": 10475
    },
    {
        "loss": 2.3663,
        "grad_norm": 3.866173267364502,
        "learning_rate": 0.0001958084169298283,
        "epoch": 1.3968,
        "step": 10476
    },
    {
        "loss": 1.7902,
        "grad_norm": 5.127920150756836,
        "learning_rate": 0.0001957903674043911,
        "epoch": 1.3969333333333334,
        "step": 10477
    },
    {
        "loss": 2.2197,
        "grad_norm": 2.78920316696167,
        "learning_rate": 0.00019577227993606305,
        "epoch": 1.3970666666666667,
        "step": 10478
    },
    {
        "loss": 1.5563,
        "grad_norm": 3.132396697998047,
        "learning_rate": 0.0001957541545320086,
        "epoch": 1.3972,
        "step": 10479
    },
    {
        "loss": 1.8364,
        "grad_norm": 3.5270426273345947,
        "learning_rate": 0.00019573599119940727,
        "epoch": 1.3973333333333333,
        "step": 10480
    },
    {
        "loss": 2.1226,
        "grad_norm": 4.266479015350342,
        "learning_rate": 0.00019571778994545354,
        "epoch": 1.3974666666666666,
        "step": 10481
    },
    {
        "loss": 2.286,
        "grad_norm": 2.8734092712402344,
        "learning_rate": 0.00019569955077735712,
        "epoch": 1.3976,
        "step": 10482
    },
    {
        "loss": 2.2622,
        "grad_norm": 4.352132320404053,
        "learning_rate": 0.00019568127370234251,
        "epoch": 1.3977333333333333,
        "step": 10483
    },
    {
        "loss": 2.1674,
        "grad_norm": 4.666947841644287,
        "learning_rate": 0.00019566295872764945,
        "epoch": 1.3978666666666666,
        "step": 10484
    },
    {
        "loss": 2.1108,
        "grad_norm": 3.4968841075897217,
        "learning_rate": 0.0001956446058605324,
        "epoch": 1.3980000000000001,
        "step": 10485
    },
    {
        "loss": 1.4189,
        "grad_norm": 3.1897103786468506,
        "learning_rate": 0.00019562621510826105,
        "epoch": 1.3981333333333335,
        "step": 10486
    },
    {
        "loss": 2.5877,
        "grad_norm": 2.5381250381469727,
        "learning_rate": 0.0001956077864781201,
        "epoch": 1.3982666666666668,
        "step": 10487
    },
    {
        "loss": 2.7592,
        "grad_norm": 3.421900987625122,
        "learning_rate": 0.00019558931997740915,
        "epoch": 1.3984,
        "step": 10488
    },
    {
        "loss": 2.146,
        "grad_norm": 3.3835649490356445,
        "learning_rate": 0.00019557081561344286,
        "epoch": 1.3985333333333334,
        "step": 10489
    },
    {
        "loss": 0.7844,
        "grad_norm": 3.936403512954712,
        "learning_rate": 0.0001955522733935508,
        "epoch": 1.3986666666666667,
        "step": 10490
    },
    {
        "loss": 1.6267,
        "grad_norm": 5.257124423980713,
        "learning_rate": 0.00019553369332507767,
        "epoch": 1.3988,
        "step": 10491
    },
    {
        "loss": 0.9571,
        "grad_norm": 5.305405139923096,
        "learning_rate": 0.00019551507541538309,
        "epoch": 1.3989333333333334,
        "step": 10492
    },
    {
        "loss": 2.5671,
        "grad_norm": 2.4702069759368896,
        "learning_rate": 0.00019549641967184177,
        "epoch": 1.3990666666666667,
        "step": 10493
    },
    {
        "loss": 1.3987,
        "grad_norm": 4.4943647384643555,
        "learning_rate": 0.00019547772610184312,
        "epoch": 1.3992,
        "step": 10494
    },
    {
        "loss": 2.626,
        "grad_norm": 3.7493736743927,
        "learning_rate": 0.00019545899471279184,
        "epoch": 1.3993333333333333,
        "step": 10495
    },
    {
        "loss": 1.6641,
        "grad_norm": 3.593797445297241,
        "learning_rate": 0.0001954402255121075,
        "epoch": 1.3994666666666666,
        "step": 10496
    },
    {
        "loss": 1.7333,
        "grad_norm": 5.93208646774292,
        "learning_rate": 0.00019542141850722463,
        "epoch": 1.3996,
        "step": 10497
    },
    {
        "loss": 1.5859,
        "grad_norm": 5.554633140563965,
        "learning_rate": 0.0001954025737055928,
        "epoch": 1.3997333333333333,
        "step": 10498
    },
    {
        "loss": 2.0467,
        "grad_norm": 3.3791708946228027,
        "learning_rate": 0.00019538369111467638,
        "epoch": 1.3998666666666666,
        "step": 10499
    },
    {
        "loss": 1.9733,
        "grad_norm": 4.068948745727539,
        "learning_rate": 0.0001953647707419549,
        "epoch": 1.4,
        "step": 10500
    },
    {
        "loss": 3.0516,
        "grad_norm": 3.927125930786133,
        "learning_rate": 0.00019534581259492284,
        "epoch": 1.4001333333333332,
        "step": 10501
    },
    {
        "loss": 2.6579,
        "grad_norm": 2.7533092498779297,
        "learning_rate": 0.0001953268166810895,
        "epoch": 1.4002666666666665,
        "step": 10502
    },
    {
        "loss": 2.1886,
        "grad_norm": 4.850358963012695,
        "learning_rate": 0.0001953077830079792,
        "epoch": 1.4003999999999999,
        "step": 10503
    },
    {
        "loss": 1.5098,
        "grad_norm": 2.852818489074707,
        "learning_rate": 0.00019528871158313134,
        "epoch": 1.4005333333333334,
        "step": 10504
    },
    {
        "loss": 2.7353,
        "grad_norm": 3.6653594970703125,
        "learning_rate": 0.00019526960241410016,
        "epoch": 1.4006666666666667,
        "step": 10505
    },
    {
        "loss": 0.9608,
        "grad_norm": 3.3635153770446777,
        "learning_rate": 0.0001952504555084548,
        "epoch": 1.4008,
        "step": 10506
    },
    {
        "loss": 2.3757,
        "grad_norm": 2.9660677909851074,
        "learning_rate": 0.0001952312708737795,
        "epoch": 1.4009333333333334,
        "step": 10507
    },
    {
        "loss": 3.2496,
        "grad_norm": 5.267414569854736,
        "learning_rate": 0.00019521204851767325,
        "epoch": 1.4010666666666667,
        "step": 10508
    },
    {
        "loss": 1.9578,
        "grad_norm": 4.442469596862793,
        "learning_rate": 0.0001951927884477502,
        "epoch": 1.4012,
        "step": 10509
    },
    {
        "loss": 2.2546,
        "grad_norm": 4.884840965270996,
        "learning_rate": 0.0001951734906716393,
        "epoch": 1.4013333333333333,
        "step": 10510
    },
    {
        "loss": 2.1592,
        "grad_norm": 3.7426846027374268,
        "learning_rate": 0.00019515415519698442,
        "epoch": 1.4014666666666666,
        "step": 10511
    },
    {
        "loss": 2.4963,
        "grad_norm": 2.7826504707336426,
        "learning_rate": 0.00019513478203144442,
        "epoch": 1.4016,
        "step": 10512
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.8231139183044434,
        "learning_rate": 0.0001951153711826931,
        "epoch": 1.4017333333333333,
        "step": 10513
    },
    {
        "loss": 1.6519,
        "grad_norm": 5.01079797744751,
        "learning_rate": 0.0001950959226584192,
        "epoch": 1.4018666666666666,
        "step": 10514
    },
    {
        "loss": 2.2853,
        "grad_norm": 2.966231107711792,
        "learning_rate": 0.00019507643646632627,
        "epoch": 1.4020000000000001,
        "step": 10515
    },
    {
        "loss": 2.9385,
        "grad_norm": 3.716322183609009,
        "learning_rate": 0.0001950569126141329,
        "epoch": 1.4021333333333335,
        "step": 10516
    },
    {
        "loss": 2.1413,
        "grad_norm": 2.982722520828247,
        "learning_rate": 0.00019503735110957252,
        "epoch": 1.4022666666666668,
        "step": 10517
    },
    {
        "loss": 2.2162,
        "grad_norm": 2.6187422275543213,
        "learning_rate": 0.00019501775196039355,
        "epoch": 1.4024,
        "step": 10518
    },
    {
        "loss": 2.7879,
        "grad_norm": 4.737366199493408,
        "learning_rate": 0.0001949981151743593,
        "epoch": 1.4025333333333334,
        "step": 10519
    },
    {
        "loss": 2.7919,
        "grad_norm": 3.739452838897705,
        "learning_rate": 0.00019497844075924792,
        "epoch": 1.4026666666666667,
        "step": 10520
    },
    {
        "loss": 1.0673,
        "grad_norm": 5.102264881134033,
        "learning_rate": 0.00019495872872285247,
        "epoch": 1.4028,
        "step": 10521
    },
    {
        "loss": 2.1908,
        "grad_norm": 3.5995564460754395,
        "learning_rate": 0.00019493897907298108,
        "epoch": 1.4029333333333334,
        "step": 10522
    },
    {
        "loss": 2.3791,
        "grad_norm": 3.277442216873169,
        "learning_rate": 0.00019491919181745653,
        "epoch": 1.4030666666666667,
        "step": 10523
    },
    {
        "loss": 1.6653,
        "grad_norm": 4.131172180175781,
        "learning_rate": 0.00019489936696411665,
        "epoch": 1.4032,
        "step": 10524
    },
    {
        "loss": 2.1809,
        "grad_norm": 3.522122859954834,
        "learning_rate": 0.00019487950452081422,
        "epoch": 1.4033333333333333,
        "step": 10525
    },
    {
        "loss": 2.3904,
        "grad_norm": 3.5773284435272217,
        "learning_rate": 0.00019485960449541675,
        "epoch": 1.4034666666666666,
        "step": 10526
    },
    {
        "loss": 2.2898,
        "grad_norm": 4.258666038513184,
        "learning_rate": 0.00019483966689580665,
        "epoch": 1.4036,
        "step": 10527
    },
    {
        "loss": 1.1895,
        "grad_norm": 4.051350116729736,
        "learning_rate": 0.00019481969172988138,
        "epoch": 1.4037333333333333,
        "step": 10528
    },
    {
        "loss": 2.6564,
        "grad_norm": 3.3980929851531982,
        "learning_rate": 0.00019479967900555315,
        "epoch": 1.4038666666666666,
        "step": 10529
    },
    {
        "loss": 0.9758,
        "grad_norm": 5.77005672454834,
        "learning_rate": 0.000194779628730749,
        "epoch": 1.404,
        "step": 10530
    },
    {
        "loss": 2.2133,
        "grad_norm": 4.458693027496338,
        "learning_rate": 0.00019475954091341096,
        "epoch": 1.4041333333333332,
        "step": 10531
    },
    {
        "loss": 2.0047,
        "grad_norm": 3.101794481277466,
        "learning_rate": 0.0001947394155614959,
        "epoch": 1.4042666666666666,
        "step": 10532
    },
    {
        "loss": 1.9437,
        "grad_norm": 2.7997324466705322,
        "learning_rate": 0.00019471925268297547,
        "epoch": 1.4043999999999999,
        "step": 10533
    },
    {
        "loss": 1.4564,
        "grad_norm": 4.605929851531982,
        "learning_rate": 0.00019469905228583636,
        "epoch": 1.4045333333333334,
        "step": 10534
    },
    {
        "loss": 1.815,
        "grad_norm": 4.602528095245361,
        "learning_rate": 0.00019467881437807994,
        "epoch": 1.4046666666666667,
        "step": 10535
    },
    {
        "loss": 1.8004,
        "grad_norm": 3.119717597961426,
        "learning_rate": 0.00019465853896772254,
        "epoch": 1.4048,
        "step": 10536
    },
    {
        "loss": 1.4702,
        "grad_norm": 4.524820804595947,
        "learning_rate": 0.0001946382260627953,
        "epoch": 1.4049333333333334,
        "step": 10537
    },
    {
        "loss": 2.1795,
        "grad_norm": 4.219334125518799,
        "learning_rate": 0.00019461787567134428,
        "epoch": 1.4050666666666667,
        "step": 10538
    },
    {
        "loss": 1.7892,
        "grad_norm": 6.125502586364746,
        "learning_rate": 0.00019459748780143026,
        "epoch": 1.4052,
        "step": 10539
    },
    {
        "loss": 2.1895,
        "grad_norm": 3.8225274085998535,
        "learning_rate": 0.00019457706246112902,
        "epoch": 1.4053333333333333,
        "step": 10540
    },
    {
        "loss": 2.5665,
        "grad_norm": 4.225759983062744,
        "learning_rate": 0.00019455659965853105,
        "epoch": 1.4054666666666666,
        "step": 10541
    },
    {
        "loss": 2.7834,
        "grad_norm": 2.6625773906707764,
        "learning_rate": 0.00019453609940174178,
        "epoch": 1.4056,
        "step": 10542
    },
    {
        "loss": 1.746,
        "grad_norm": 5.096012115478516,
        "learning_rate": 0.00019451556169888145,
        "epoch": 1.4057333333333333,
        "step": 10543
    },
    {
        "loss": 0.9411,
        "grad_norm": 5.182247161865234,
        "learning_rate": 0.000194494986558085,
        "epoch": 1.4058666666666666,
        "step": 10544
    },
    {
        "loss": 2.0951,
        "grad_norm": 4.166055202484131,
        "learning_rate": 0.00019447437398750238,
        "epoch": 1.4060000000000001,
        "step": 10545
    },
    {
        "loss": 1.709,
        "grad_norm": 3.5015439987182617,
        "learning_rate": 0.0001944537239952984,
        "epoch": 1.4061333333333335,
        "step": 10546
    },
    {
        "loss": 2.3392,
        "grad_norm": 4.754919052124023,
        "learning_rate": 0.00019443303658965246,
        "epoch": 1.4062666666666668,
        "step": 10547
    },
    {
        "loss": 0.9537,
        "grad_norm": 3.0618739128112793,
        "learning_rate": 0.000194412311778759,
        "epoch": 1.4064,
        "step": 10548
    },
    {
        "loss": 0.9701,
        "grad_norm": 3.9118502140045166,
        "learning_rate": 0.00019439154957082703,
        "epoch": 1.4065333333333334,
        "step": 10549
    },
    {
        "loss": 1.3713,
        "grad_norm": 3.6785857677459717,
        "learning_rate": 0.00019437074997408075,
        "epoch": 1.4066666666666667,
        "step": 10550
    },
    {
        "loss": 2.3194,
        "grad_norm": 3.406033754348755,
        "learning_rate": 0.0001943499129967588,
        "epoch": 1.4068,
        "step": 10551
    },
    {
        "loss": 1.4,
        "grad_norm": 4.426336765289307,
        "learning_rate": 0.00019432903864711483,
        "epoch": 1.4069333333333334,
        "step": 10552
    },
    {
        "loss": 2.7672,
        "grad_norm": 2.6621415615081787,
        "learning_rate": 0.00019430812693341722,
        "epoch": 1.4070666666666667,
        "step": 10553
    },
    {
        "loss": 2.4234,
        "grad_norm": 3.5350821018218994,
        "learning_rate": 0.00019428717786394915,
        "epoch": 1.4072,
        "step": 10554
    },
    {
        "loss": 1.8336,
        "grad_norm": 2.7232367992401123,
        "learning_rate": 0.0001942661914470087,
        "epoch": 1.4073333333333333,
        "step": 10555
    },
    {
        "loss": 0.7655,
        "grad_norm": 4.191483020782471,
        "learning_rate": 0.00019424516769090864,
        "epoch": 1.4074666666666666,
        "step": 10556
    },
    {
        "loss": 1.5578,
        "grad_norm": 5.181034564971924,
        "learning_rate": 0.00019422410660397652,
        "epoch": 1.4076,
        "step": 10557
    },
    {
        "loss": 2.5886,
        "grad_norm": 2.5286519527435303,
        "learning_rate": 0.00019420300819455462,
        "epoch": 1.4077333333333333,
        "step": 10558
    },
    {
        "loss": 2.2829,
        "grad_norm": 4.1444292068481445,
        "learning_rate": 0.00019418187247100032,
        "epoch": 1.4078666666666666,
        "step": 10559
    },
    {
        "loss": 2.3867,
        "grad_norm": 2.8026061058044434,
        "learning_rate": 0.00019416069944168537,
        "epoch": 1.408,
        "step": 10560
    },
    {
        "loss": 2.6145,
        "grad_norm": 2.9632246494293213,
        "learning_rate": 0.0001941394891149965,
        "epoch": 1.4081333333333332,
        "step": 10561
    },
    {
        "loss": 2.4391,
        "grad_norm": 3.6224613189697266,
        "learning_rate": 0.0001941182414993352,
        "epoch": 1.4082666666666666,
        "step": 10562
    },
    {
        "loss": 2.0166,
        "grad_norm": 3.5579769611358643,
        "learning_rate": 0.00019409695660311773,
        "epoch": 1.4083999999999999,
        "step": 10563
    },
    {
        "loss": 2.1663,
        "grad_norm": 3.923009157180786,
        "learning_rate": 0.00019407563443477524,
        "epoch": 1.4085333333333334,
        "step": 10564
    },
    {
        "loss": 0.7713,
        "grad_norm": 3.1039037704467773,
        "learning_rate": 0.00019405427500275327,
        "epoch": 1.4086666666666667,
        "step": 10565
    },
    {
        "loss": 2.1518,
        "grad_norm": 3.639248847961426,
        "learning_rate": 0.00019403287831551257,
        "epoch": 1.4088,
        "step": 10566
    },
    {
        "loss": 2.4549,
        "grad_norm": 2.8113937377929688,
        "learning_rate": 0.00019401144438152828,
        "epoch": 1.4089333333333334,
        "step": 10567
    },
    {
        "loss": 2.6438,
        "grad_norm": 2.7009451389312744,
        "learning_rate": 0.0001939899732092906,
        "epoch": 1.4090666666666667,
        "step": 10568
    },
    {
        "loss": 2.6008,
        "grad_norm": 4.465677261352539,
        "learning_rate": 0.00019396846480730426,
        "epoch": 1.4092,
        "step": 10569
    },
    {
        "loss": 1.3417,
        "grad_norm": 6.30244255065918,
        "learning_rate": 0.00019394691918408883,
        "epoch": 1.4093333333333333,
        "step": 10570
    },
    {
        "loss": 1.5177,
        "grad_norm": 2.7314674854278564,
        "learning_rate": 0.00019392533634817854,
        "epoch": 1.4094666666666666,
        "step": 10571
    },
    {
        "loss": 1.2291,
        "grad_norm": 5.720280647277832,
        "learning_rate": 0.00019390371630812248,
        "epoch": 1.4096,
        "step": 10572
    },
    {
        "loss": 2.3611,
        "grad_norm": 2.8450722694396973,
        "learning_rate": 0.00019388205907248455,
        "epoch": 1.4097333333333333,
        "step": 10573
    },
    {
        "loss": 1.1308,
        "grad_norm": 6.0060014724731445,
        "learning_rate": 0.00019386036464984297,
        "epoch": 1.4098666666666666,
        "step": 10574
    },
    {
        "loss": 1.9215,
        "grad_norm": 3.1383328437805176,
        "learning_rate": 0.00019383863304879123,
        "epoch": 1.41,
        "step": 10575
    },
    {
        "loss": 2.2521,
        "grad_norm": 5.182095050811768,
        "learning_rate": 0.0001938168642779371,
        "epoch": 1.4101333333333335,
        "step": 10576
    },
    {
        "loss": 2.2968,
        "grad_norm": 3.742764949798584,
        "learning_rate": 0.00019379505834590344,
        "epoch": 1.4102666666666668,
        "step": 10577
    },
    {
        "loss": 2.4037,
        "grad_norm": 4.348532199859619,
        "learning_rate": 0.00019377321526132754,
        "epoch": 1.4104,
        "step": 10578
    },
    {
        "loss": 2.4191,
        "grad_norm": 3.5875275135040283,
        "learning_rate": 0.00019375133503286157,
        "epoch": 1.4105333333333334,
        "step": 10579
    },
    {
        "loss": 2.5469,
        "grad_norm": 3.942758798599243,
        "learning_rate": 0.0001937294176691723,
        "epoch": 1.4106666666666667,
        "step": 10580
    },
    {
        "loss": 2.1221,
        "grad_norm": 3.936112403869629,
        "learning_rate": 0.00019370746317894135,
        "epoch": 1.4108,
        "step": 10581
    },
    {
        "loss": 1.3996,
        "grad_norm": 3.8551573753356934,
        "learning_rate": 0.00019368547157086493,
        "epoch": 1.4109333333333334,
        "step": 10582
    },
    {
        "loss": 2.5447,
        "grad_norm": 3.4071145057678223,
        "learning_rate": 0.00019366344285365398,
        "epoch": 1.4110666666666667,
        "step": 10583
    },
    {
        "loss": 2.1355,
        "grad_norm": 4.307185649871826,
        "learning_rate": 0.0001936413770360342,
        "epoch": 1.4112,
        "step": 10584
    },
    {
        "loss": 1.3485,
        "grad_norm": 4.538991928100586,
        "learning_rate": 0.00019361927412674587,
        "epoch": 1.4113333333333333,
        "step": 10585
    },
    {
        "loss": 1.5901,
        "grad_norm": 7.769183158874512,
        "learning_rate": 0.00019359713413454404,
        "epoch": 1.4114666666666666,
        "step": 10586
    },
    {
        "loss": 2.2189,
        "grad_norm": 3.9780678749084473,
        "learning_rate": 0.00019357495706819847,
        "epoch": 1.4116,
        "step": 10587
    },
    {
        "loss": 1.5281,
        "grad_norm": 2.3701071739196777,
        "learning_rate": 0.00019355274293649362,
        "epoch": 1.4117333333333333,
        "step": 10588
    },
    {
        "loss": 0.5593,
        "grad_norm": 2.8335776329040527,
        "learning_rate": 0.00019353049174822848,
        "epoch": 1.4118666666666666,
        "step": 10589
    },
    {
        "loss": 2.6605,
        "grad_norm": 3.980315923690796,
        "learning_rate": 0.00019350820351221682,
        "epoch": 1.412,
        "step": 10590
    },
    {
        "loss": 2.58,
        "grad_norm": 3.6324355602264404,
        "learning_rate": 0.00019348587823728717,
        "epoch": 1.4121333333333332,
        "step": 10591
    },
    {
        "loss": 2.6507,
        "grad_norm": 3.4271962642669678,
        "learning_rate": 0.00019346351593228255,
        "epoch": 1.4122666666666666,
        "step": 10592
    },
    {
        "loss": 2.6033,
        "grad_norm": 3.6835899353027344,
        "learning_rate": 0.00019344111660606086,
        "epoch": 1.4123999999999999,
        "step": 10593
    },
    {
        "loss": 2.3894,
        "grad_norm": 4.544325828552246,
        "learning_rate": 0.00019341868026749449,
        "epoch": 1.4125333333333332,
        "step": 10594
    },
    {
        "loss": 2.5044,
        "grad_norm": 3.767627477645874,
        "learning_rate": 0.0001933962069254705,
        "epoch": 1.4126666666666667,
        "step": 10595
    },
    {
        "loss": 2.6571,
        "grad_norm": 4.403134346008301,
        "learning_rate": 0.00019337369658889073,
        "epoch": 1.4128,
        "step": 10596
    },
    {
        "loss": 2.2874,
        "grad_norm": 4.883232593536377,
        "learning_rate": 0.0001933511492666716,
        "epoch": 1.4129333333333334,
        "step": 10597
    },
    {
        "loss": 2.5305,
        "grad_norm": 3.415616750717163,
        "learning_rate": 0.00019332856496774418,
        "epoch": 1.4130666666666667,
        "step": 10598
    },
    {
        "loss": 2.5852,
        "grad_norm": 5.252291679382324,
        "learning_rate": 0.00019330594370105408,
        "epoch": 1.4132,
        "step": 10599
    },
    {
        "loss": 2.2582,
        "grad_norm": 1.9994696378707886,
        "learning_rate": 0.00019328328547556182,
        "epoch": 1.4133333333333333,
        "step": 10600
    },
    {
        "loss": 2.3787,
        "grad_norm": 4.185726165771484,
        "learning_rate": 0.00019326059030024228,
        "epoch": 1.4134666666666666,
        "step": 10601
    },
    {
        "loss": 2.4929,
        "grad_norm": 3.3428397178649902,
        "learning_rate": 0.00019323785818408523,
        "epoch": 1.4136,
        "step": 10602
    },
    {
        "loss": 2.2172,
        "grad_norm": 2.966338634490967,
        "learning_rate": 0.00019321508913609477,
        "epoch": 1.4137333333333333,
        "step": 10603
    },
    {
        "loss": 2.3764,
        "grad_norm": 3.1589841842651367,
        "learning_rate": 0.00019319228316528987,
        "epoch": 1.4138666666666666,
        "step": 10604
    },
    {
        "loss": 2.3955,
        "grad_norm": 4.946837425231934,
        "learning_rate": 0.00019316944028070413,
        "epoch": 1.414,
        "step": 10605
    },
    {
        "loss": 0.5219,
        "grad_norm": 3.112860679626465,
        "learning_rate": 0.0001931465604913856,
        "epoch": 1.4141333333333335,
        "step": 10606
    },
    {
        "loss": 2.0367,
        "grad_norm": 3.8461544513702393,
        "learning_rate": 0.00019312364380639707,
        "epoch": 1.4142666666666668,
        "step": 10607
    },
    {
        "loss": 2.3968,
        "grad_norm": 7.1180033683776855,
        "learning_rate": 0.0001931006902348159,
        "epoch": 1.4144,
        "step": 10608
    },
    {
        "loss": 1.4703,
        "grad_norm": 4.353155136108398,
        "learning_rate": 0.0001930776997857341,
        "epoch": 1.4145333333333334,
        "step": 10609
    },
    {
        "loss": 1.8659,
        "grad_norm": 3.0816524028778076,
        "learning_rate": 0.0001930546724682583,
        "epoch": 1.4146666666666667,
        "step": 10610
    },
    {
        "loss": 1.0919,
        "grad_norm": 3.846817970275879,
        "learning_rate": 0.00019303160829150967,
        "epoch": 1.4148,
        "step": 10611
    },
    {
        "loss": 2.3549,
        "grad_norm": 2.1972951889038086,
        "learning_rate": 0.0001930085072646239,
        "epoch": 1.4149333333333334,
        "step": 10612
    },
    {
        "loss": 1.9138,
        "grad_norm": 3.4314002990722656,
        "learning_rate": 0.0001929853693967515,
        "epoch": 1.4150666666666667,
        "step": 10613
    },
    {
        "loss": 2.5434,
        "grad_norm": 3.2531936168670654,
        "learning_rate": 0.0001929621946970575,
        "epoch": 1.4152,
        "step": 10614
    },
    {
        "loss": 2.3502,
        "grad_norm": 3.4327054023742676,
        "learning_rate": 0.00019293898317472143,
        "epoch": 1.4153333333333333,
        "step": 10615
    },
    {
        "loss": 2.1291,
        "grad_norm": 4.492797374725342,
        "learning_rate": 0.00019291573483893744,
        "epoch": 1.4154666666666667,
        "step": 10616
    },
    {
        "loss": 2.6901,
        "grad_norm": 3.616018533706665,
        "learning_rate": 0.0001928924496989142,
        "epoch": 1.4156,
        "step": 10617
    },
    {
        "loss": 2.4281,
        "grad_norm": 3.399841547012329,
        "learning_rate": 0.0001928691277638752,
        "epoch": 1.4157333333333333,
        "step": 10618
    },
    {
        "loss": 2.0007,
        "grad_norm": 4.054447174072266,
        "learning_rate": 0.0001928457690430582,
        "epoch": 1.4158666666666666,
        "step": 10619
    },
    {
        "loss": 1.9428,
        "grad_norm": 4.722332954406738,
        "learning_rate": 0.00019282237354571575,
        "epoch": 1.416,
        "step": 10620
    },
    {
        "loss": 1.9284,
        "grad_norm": 4.009755611419678,
        "learning_rate": 0.0001927989412811148,
        "epoch": 1.4161333333333332,
        "step": 10621
    },
    {
        "loss": 2.1052,
        "grad_norm": 4.077970027923584,
        "learning_rate": 0.000192775472258537,
        "epoch": 1.4162666666666666,
        "step": 10622
    },
    {
        "loss": 1.8162,
        "grad_norm": 4.7471537590026855,
        "learning_rate": 0.00019275196648727865,
        "epoch": 1.4163999999999999,
        "step": 10623
    },
    {
        "loss": 2.2692,
        "grad_norm": 3.3842639923095703,
        "learning_rate": 0.00019272842397665018,
        "epoch": 1.4165333333333332,
        "step": 10624
    },
    {
        "loss": 0.6503,
        "grad_norm": 3.923950672149658,
        "learning_rate": 0.00019270484473597708,
        "epoch": 1.4166666666666667,
        "step": 10625
    },
    {
        "loss": 1.7175,
        "grad_norm": 5.1890549659729,
        "learning_rate": 0.00019268122877459903,
        "epoch": 1.4168,
        "step": 10626
    },
    {
        "loss": 1.8102,
        "grad_norm": 3.34582257270813,
        "learning_rate": 0.00019265757610187052,
        "epoch": 1.4169333333333334,
        "step": 10627
    },
    {
        "loss": 1.9936,
        "grad_norm": 6.024624824523926,
        "learning_rate": 0.00019263388672716042,
        "epoch": 1.4170666666666667,
        "step": 10628
    },
    {
        "loss": 2.436,
        "grad_norm": 2.8275089263916016,
        "learning_rate": 0.0001926101606598521,
        "epoch": 1.4172,
        "step": 10629
    },
    {
        "loss": 2.5466,
        "grad_norm": 4.491117000579834,
        "learning_rate": 0.00019258639790934357,
        "epoch": 1.4173333333333333,
        "step": 10630
    },
    {
        "loss": 0.8495,
        "grad_norm": 4.603631973266602,
        "learning_rate": 0.00019256259848504737,
        "epoch": 1.4174666666666667,
        "step": 10631
    },
    {
        "loss": 2.2068,
        "grad_norm": 3.463995933532715,
        "learning_rate": 0.00019253876239639053,
        "epoch": 1.4176,
        "step": 10632
    },
    {
        "loss": 2.1685,
        "grad_norm": 3.3225579261779785,
        "learning_rate": 0.00019251488965281452,
        "epoch": 1.4177333333333333,
        "step": 10633
    },
    {
        "loss": 2.6916,
        "grad_norm": 2.8162195682525635,
        "learning_rate": 0.00019249098026377556,
        "epoch": 1.4178666666666666,
        "step": 10634
    },
    {
        "loss": 1.7231,
        "grad_norm": 5.06231164932251,
        "learning_rate": 0.00019246703423874407,
        "epoch": 1.418,
        "step": 10635
    },
    {
        "loss": 2.3429,
        "grad_norm": 2.2254133224487305,
        "learning_rate": 0.0001924430515872053,
        "epoch": 1.4181333333333335,
        "step": 10636
    },
    {
        "loss": 2.6679,
        "grad_norm": 4.698245525360107,
        "learning_rate": 0.00019241903231865883,
        "epoch": 1.4182666666666668,
        "step": 10637
    },
    {
        "loss": 1.6306,
        "grad_norm": 4.194555759429932,
        "learning_rate": 0.0001923949764426187,
        "epoch": 1.4184,
        "step": 10638
    },
    {
        "loss": 2.4579,
        "grad_norm": 4.261199951171875,
        "learning_rate": 0.0001923708839686136,
        "epoch": 1.4185333333333334,
        "step": 10639
    },
    {
        "loss": 3.0063,
        "grad_norm": 2.6245150566101074,
        "learning_rate": 0.00019234675490618664,
        "epoch": 1.4186666666666667,
        "step": 10640
    },
    {
        "loss": 2.2465,
        "grad_norm": 4.070481300354004,
        "learning_rate": 0.00019232258926489535,
        "epoch": 1.4188,
        "step": 10641
    },
    {
        "loss": 2.0024,
        "grad_norm": 3.491129159927368,
        "learning_rate": 0.00019229838705431184,
        "epoch": 1.4189333333333334,
        "step": 10642
    },
    {
        "loss": 2.31,
        "grad_norm": 4.842794895172119,
        "learning_rate": 0.00019227414828402282,
        "epoch": 1.4190666666666667,
        "step": 10643
    },
    {
        "loss": 2.6377,
        "grad_norm": 4.151209831237793,
        "learning_rate": 0.0001922498729636292,
        "epoch": 1.4192,
        "step": 10644
    },
    {
        "loss": 2.5052,
        "grad_norm": 3.3353421688079834,
        "learning_rate": 0.0001922255611027465,
        "epoch": 1.4193333333333333,
        "step": 10645
    },
    {
        "loss": 2.2433,
        "grad_norm": 3.059235095977783,
        "learning_rate": 0.0001922012127110049,
        "epoch": 1.4194666666666667,
        "step": 10646
    },
    {
        "loss": 1.6815,
        "grad_norm": 2.1951680183410645,
        "learning_rate": 0.00019217682779804873,
        "epoch": 1.4196,
        "step": 10647
    },
    {
        "loss": 2.6943,
        "grad_norm": 2.6084322929382324,
        "learning_rate": 0.00019215240637353702,
        "epoch": 1.4197333333333333,
        "step": 10648
    },
    {
        "loss": 2.5458,
        "grad_norm": 4.716648101806641,
        "learning_rate": 0.0001921279484471431,
        "epoch": 1.4198666666666666,
        "step": 10649
    },
    {
        "loss": 2.3946,
        "grad_norm": 3.6536030769348145,
        "learning_rate": 0.0001921034540285549,
        "epoch": 1.42,
        "step": 10650
    },
    {
        "loss": 2.4422,
        "grad_norm": 2.708521604537964,
        "learning_rate": 0.00019207892312747468,
        "epoch": 1.4201333333333332,
        "step": 10651
    },
    {
        "loss": 2.1231,
        "grad_norm": 3.509018659591675,
        "learning_rate": 0.00019205435575361937,
        "epoch": 1.4202666666666666,
        "step": 10652
    },
    {
        "loss": 2.1736,
        "grad_norm": 3.449082136154175,
        "learning_rate": 0.00019202975191671994,
        "epoch": 1.4203999999999999,
        "step": 10653
    },
    {
        "loss": 2.0138,
        "grad_norm": 3.5854880809783936,
        "learning_rate": 0.00019200511162652224,
        "epoch": 1.4205333333333332,
        "step": 10654
    },
    {
        "loss": 1.6458,
        "grad_norm": 4.663283348083496,
        "learning_rate": 0.00019198043489278635,
        "epoch": 1.4206666666666667,
        "step": 10655
    },
    {
        "loss": 2.7524,
        "grad_norm": 4.230960369110107,
        "learning_rate": 0.0001919557217252868,
        "epoch": 1.4208,
        "step": 10656
    },
    {
        "loss": 2.0182,
        "grad_norm": 3.2305986881256104,
        "learning_rate": 0.0001919309721338125,
        "epoch": 1.4209333333333334,
        "step": 10657
    },
    {
        "loss": 0.6894,
        "grad_norm": 3.4415476322174072,
        "learning_rate": 0.00019190618612816688,
        "epoch": 1.4210666666666667,
        "step": 10658
    },
    {
        "loss": 2.1202,
        "grad_norm": 3.6389174461364746,
        "learning_rate": 0.00019188136371816784,
        "epoch": 1.4212,
        "step": 10659
    },
    {
        "loss": 1.5505,
        "grad_norm": 3.9209604263305664,
        "learning_rate": 0.00019185650491364747,
        "epoch": 1.4213333333333333,
        "step": 10660
    },
    {
        "loss": 2.6212,
        "grad_norm": 3.2926058769226074,
        "learning_rate": 0.00019183160972445262,
        "epoch": 1.4214666666666667,
        "step": 10661
    },
    {
        "loss": 2.882,
        "grad_norm": 6.2693400382995605,
        "learning_rate": 0.00019180667816044413,
        "epoch": 1.4216,
        "step": 10662
    },
    {
        "loss": 2.0112,
        "grad_norm": 4.532606601715088,
        "learning_rate": 0.00019178171023149764,
        "epoch": 1.4217333333333333,
        "step": 10663
    },
    {
        "loss": 2.8526,
        "grad_norm": 4.050668239593506,
        "learning_rate": 0.000191756705947503,
        "epoch": 1.4218666666666666,
        "step": 10664
    },
    {
        "loss": 2.222,
        "grad_norm": 4.233691692352295,
        "learning_rate": 0.0001917316653183645,
        "epoch": 1.422,
        "step": 10665
    },
    {
        "loss": 2.6379,
        "grad_norm": 4.916103363037109,
        "learning_rate": 0.00019170658835400078,
        "epoch": 1.4221333333333335,
        "step": 10666
    },
    {
        "loss": 2.6705,
        "grad_norm": 3.7794950008392334,
        "learning_rate": 0.0001916814750643449,
        "epoch": 1.4222666666666668,
        "step": 10667
    },
    {
        "loss": 1.6044,
        "grad_norm": 4.027751922607422,
        "learning_rate": 0.00019165632545934443,
        "epoch": 1.4224,
        "step": 10668
    },
    {
        "loss": 0.8886,
        "grad_norm": 3.6319029331207275,
        "learning_rate": 0.0001916311395489611,
        "epoch": 1.4225333333333334,
        "step": 10669
    },
    {
        "loss": 1.5082,
        "grad_norm": 4.411289691925049,
        "learning_rate": 0.00019160591734317114,
        "epoch": 1.4226666666666667,
        "step": 10670
    },
    {
        "loss": 2.6689,
        "grad_norm": 3.271218776702881,
        "learning_rate": 0.00019158065885196518,
        "epoch": 1.4228,
        "step": 10671
    },
    {
        "loss": 1.6819,
        "grad_norm": 4.3098320960998535,
        "learning_rate": 0.00019155536408534815,
        "epoch": 1.4229333333333334,
        "step": 10672
    },
    {
        "loss": 1.9922,
        "grad_norm": 3.8755340576171875,
        "learning_rate": 0.00019153003305333953,
        "epoch": 1.4230666666666667,
        "step": 10673
    },
    {
        "loss": 1.8313,
        "grad_norm": 3.9990880489349365,
        "learning_rate": 0.00019150466576597292,
        "epoch": 1.4232,
        "step": 10674
    },
    {
        "loss": 2.4409,
        "grad_norm": 2.445180892944336,
        "learning_rate": 0.0001914792622332964,
        "epoch": 1.4233333333333333,
        "step": 10675
    },
    {
        "loss": 2.4915,
        "grad_norm": 3.011505603790283,
        "learning_rate": 0.00019145382246537235,
        "epoch": 1.4234666666666667,
        "step": 10676
    },
    {
        "loss": 1.0655,
        "grad_norm": 3.8219361305236816,
        "learning_rate": 0.00019142834647227765,
        "epoch": 1.4236,
        "step": 10677
    },
    {
        "loss": 1.9192,
        "grad_norm": 2.5410022735595703,
        "learning_rate": 0.0001914028342641034,
        "epoch": 1.4237333333333333,
        "step": 10678
    },
    {
        "loss": 2.5512,
        "grad_norm": 3.2174313068389893,
        "learning_rate": 0.00019137728585095506,
        "epoch": 1.4238666666666666,
        "step": 10679
    },
    {
        "loss": 2.6303,
        "grad_norm": 3.1431612968444824,
        "learning_rate": 0.00019135170124295233,
        "epoch": 1.424,
        "step": 10680
    },
    {
        "loss": 2.2134,
        "grad_norm": 4.4669270515441895,
        "learning_rate": 0.00019132608045022948,
        "epoch": 1.4241333333333333,
        "step": 10681
    },
    {
        "loss": 2.5017,
        "grad_norm": 3.9583420753479004,
        "learning_rate": 0.00019130042348293512,
        "epoch": 1.4242666666666666,
        "step": 10682
    },
    {
        "loss": 1.9663,
        "grad_norm": 3.2355403900146484,
        "learning_rate": 0.0001912747303512318,
        "epoch": 1.4243999999999999,
        "step": 10683
    },
    {
        "loss": 2.1901,
        "grad_norm": 5.020630836486816,
        "learning_rate": 0.0001912490010652968,
        "epoch": 1.4245333333333332,
        "step": 10684
    },
    {
        "loss": 2.0007,
        "grad_norm": 3.1216025352478027,
        "learning_rate": 0.00019122323563532155,
        "epoch": 1.4246666666666667,
        "step": 10685
    },
    {
        "loss": 2.1094,
        "grad_norm": 4.0395402908325195,
        "learning_rate": 0.00019119743407151183,
        "epoch": 1.4248,
        "step": 10686
    },
    {
        "loss": 0.9533,
        "grad_norm": 3.3860323429107666,
        "learning_rate": 0.00019117159638408775,
        "epoch": 1.4249333333333334,
        "step": 10687
    },
    {
        "loss": 1.7706,
        "grad_norm": 7.259500980377197,
        "learning_rate": 0.0001911457225832837,
        "epoch": 1.4250666666666667,
        "step": 10688
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.1783108711242676,
        "learning_rate": 0.00019111981267934825,
        "epoch": 1.4252,
        "step": 10689
    },
    {
        "loss": 1.3257,
        "grad_norm": 4.112711429595947,
        "learning_rate": 0.00019109386668254464,
        "epoch": 1.4253333333333333,
        "step": 10690
    },
    {
        "loss": 2.4856,
        "grad_norm": 4.098393440246582,
        "learning_rate": 0.00019106788460315001,
        "epoch": 1.4254666666666667,
        "step": 10691
    },
    {
        "loss": 1.0627,
        "grad_norm": 4.217862129211426,
        "learning_rate": 0.00019104186645145594,
        "epoch": 1.4256,
        "step": 10692
    },
    {
        "loss": 2.7714,
        "grad_norm": 2.0976815223693848,
        "learning_rate": 0.0001910158122377684,
        "epoch": 1.4257333333333333,
        "step": 10693
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.428006410598755,
        "learning_rate": 0.00019098972197240742,
        "epoch": 1.4258666666666666,
        "step": 10694
    },
    {
        "loss": 2.6441,
        "grad_norm": 4.578895092010498,
        "learning_rate": 0.00019096359566570762,
        "epoch": 1.426,
        "step": 10695
    },
    {
        "loss": 2.1448,
        "grad_norm": 2.448474884033203,
        "learning_rate": 0.0001909374333280176,
        "epoch": 1.4261333333333333,
        "step": 10696
    },
    {
        "loss": 2.7283,
        "grad_norm": 4.488401889801025,
        "learning_rate": 0.00019091123496970042,
        "epoch": 1.4262666666666668,
        "step": 10697
    },
    {
        "loss": 2.0659,
        "grad_norm": 3.872652530670166,
        "learning_rate": 0.00019088500060113322,
        "epoch": 1.4264000000000001,
        "step": 10698
    },
    {
        "loss": 1.8845,
        "grad_norm": 4.9297380447387695,
        "learning_rate": 0.00019085873023270763,
        "epoch": 1.4265333333333334,
        "step": 10699
    },
    {
        "loss": 1.9908,
        "grad_norm": 3.624995470046997,
        "learning_rate": 0.00019083242387482943,
        "epoch": 1.4266666666666667,
        "step": 10700
    },
    {
        "loss": 1.8028,
        "grad_norm": 4.748695373535156,
        "learning_rate": 0.00019080608153791855,
        "epoch": 1.4268,
        "step": 10701
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.530122995376587,
        "learning_rate": 0.00019077970323240942,
        "epoch": 1.4269333333333334,
        "step": 10702
    },
    {
        "loss": 1.2676,
        "grad_norm": 2.3908872604370117,
        "learning_rate": 0.00019075328896875053,
        "epoch": 1.4270666666666667,
        "step": 10703
    },
    {
        "loss": 2.4628,
        "grad_norm": 3.668358325958252,
        "learning_rate": 0.00019072683875740457,
        "epoch": 1.4272,
        "step": 10704
    },
    {
        "loss": 2.556,
        "grad_norm": 2.400240659713745,
        "learning_rate": 0.00019070035260884872,
        "epoch": 1.4273333333333333,
        "step": 10705
    },
    {
        "loss": 2.6404,
        "grad_norm": 5.087809085845947,
        "learning_rate": 0.0001906738305335741,
        "epoch": 1.4274666666666667,
        "step": 10706
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.6917903423309326,
        "learning_rate": 0.0001906472725420863,
        "epoch": 1.4276,
        "step": 10707
    },
    {
        "loss": 1.9892,
        "grad_norm": 3.4480834007263184,
        "learning_rate": 0.00019062067864490488,
        "epoch": 1.4277333333333333,
        "step": 10708
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.94036865234375,
        "learning_rate": 0.00019059404885256393,
        "epoch": 1.4278666666666666,
        "step": 10709
    },
    {
        "loss": 1.3715,
        "grad_norm": 3.3178632259368896,
        "learning_rate": 0.00019056738317561146,
        "epoch": 1.428,
        "step": 10710
    },
    {
        "loss": 3.0521,
        "grad_norm": 3.1131412982940674,
        "learning_rate": 0.00019054068162461008,
        "epoch": 1.4281333333333333,
        "step": 10711
    },
    {
        "loss": 1.8882,
        "grad_norm": 3.443193197250366,
        "learning_rate": 0.00019051394421013603,
        "epoch": 1.4282666666666666,
        "step": 10712
    },
    {
        "loss": 1.7089,
        "grad_norm": 4.837456703186035,
        "learning_rate": 0.00019048717094278028,
        "epoch": 1.4284,
        "step": 10713
    },
    {
        "loss": 2.5382,
        "grad_norm": 3.3984992504119873,
        "learning_rate": 0.00019046036183314785,
        "epoch": 1.4285333333333332,
        "step": 10714
    },
    {
        "loss": 1.6539,
        "grad_norm": 2.422454595565796,
        "learning_rate": 0.0001904335168918579,
        "epoch": 1.4286666666666665,
        "step": 10715
    },
    {
        "loss": 2.3696,
        "grad_norm": 3.6051158905029297,
        "learning_rate": 0.00019040663612954371,
        "epoch": 1.4288,
        "step": 10716
    },
    {
        "loss": 2.3187,
        "grad_norm": 3.8081891536712646,
        "learning_rate": 0.00019037971955685286,
        "epoch": 1.4289333333333334,
        "step": 10717
    },
    {
        "loss": 2.7846,
        "grad_norm": 2.6274125576019287,
        "learning_rate": 0.00019035276718444718,
        "epoch": 1.4290666666666667,
        "step": 10718
    },
    {
        "loss": 1.358,
        "grad_norm": 3.3712596893310547,
        "learning_rate": 0.0001903257790230025,
        "epoch": 1.4292,
        "step": 10719
    },
    {
        "loss": 1.0685,
        "grad_norm": 4.610400199890137,
        "learning_rate": 0.00019029875508320908,
        "epoch": 1.4293333333333333,
        "step": 10720
    },
    {
        "loss": 2.2649,
        "grad_norm": 7.0521111488342285,
        "learning_rate": 0.00019027169537577096,
        "epoch": 1.4294666666666667,
        "step": 10721
    },
    {
        "loss": 2.2135,
        "grad_norm": 3.35016131401062,
        "learning_rate": 0.00019024459991140674,
        "epoch": 1.4296,
        "step": 10722
    },
    {
        "loss": 1.3259,
        "grad_norm": 2.3355050086975098,
        "learning_rate": 0.00019021746870084903,
        "epoch": 1.4297333333333333,
        "step": 10723
    },
    {
        "loss": 2.1733,
        "grad_norm": 3.152822256088257,
        "learning_rate": 0.00019019030175484458,
        "epoch": 1.4298666666666666,
        "step": 10724
    },
    {
        "loss": 2.94,
        "grad_norm": 4.639236927032471,
        "learning_rate": 0.00019016309908415427,
        "epoch": 1.43,
        "step": 10725
    },
    {
        "loss": 1.4785,
        "grad_norm": 3.8981544971466064,
        "learning_rate": 0.00019013586069955313,
        "epoch": 1.4301333333333333,
        "step": 10726
    },
    {
        "loss": 2.055,
        "grad_norm": 3.206265926361084,
        "learning_rate": 0.00019010858661183048,
        "epoch": 1.4302666666666668,
        "step": 10727
    },
    {
        "loss": 1.7549,
        "grad_norm": 4.591329574584961,
        "learning_rate": 0.00019008127683178965,
        "epoch": 1.4304000000000001,
        "step": 10728
    },
    {
        "loss": 3.0046,
        "grad_norm": 3.6707427501678467,
        "learning_rate": 0.00019005393137024813,
        "epoch": 1.4305333333333334,
        "step": 10729
    },
    {
        "loss": 2.5607,
        "grad_norm": 3.4555740356445312,
        "learning_rate": 0.0001900265502380375,
        "epoch": 1.4306666666666668,
        "step": 10730
    },
    {
        "loss": 2.5745,
        "grad_norm": 3.898238182067871,
        "learning_rate": 0.00018999913344600354,
        "epoch": 1.4308,
        "step": 10731
    },
    {
        "loss": 1.629,
        "grad_norm": 4.50935697555542,
        "learning_rate": 0.00018997168100500628,
        "epoch": 1.4309333333333334,
        "step": 10732
    },
    {
        "loss": 2.6287,
        "grad_norm": 3.588266134262085,
        "learning_rate": 0.00018994419292591949,
        "epoch": 1.4310666666666667,
        "step": 10733
    },
    {
        "loss": 3.2411,
        "grad_norm": 3.920855760574341,
        "learning_rate": 0.00018991666921963144,
        "epoch": 1.4312,
        "step": 10734
    },
    {
        "loss": 2.2355,
        "grad_norm": 3.4880270957946777,
        "learning_rate": 0.0001898891098970443,
        "epoch": 1.4313333333333333,
        "step": 10735
    },
    {
        "loss": 1.8825,
        "grad_norm": 4.544148921966553,
        "learning_rate": 0.00018986151496907448,
        "epoch": 1.4314666666666667,
        "step": 10736
    },
    {
        "loss": 2.9691,
        "grad_norm": 3.5620977878570557,
        "learning_rate": 0.0001898338844466524,
        "epoch": 1.4316,
        "step": 10737
    },
    {
        "loss": 2.5312,
        "grad_norm": 3.3628737926483154,
        "learning_rate": 0.00018980621834072258,
        "epoch": 1.4317333333333333,
        "step": 10738
    },
    {
        "loss": 1.5027,
        "grad_norm": 2.4369618892669678,
        "learning_rate": 0.00018977851666224363,
        "epoch": 1.4318666666666666,
        "step": 10739
    },
    {
        "loss": 1.0563,
        "grad_norm": 5.618182182312012,
        "learning_rate": 0.00018975077942218828,
        "epoch": 1.432,
        "step": 10740
    },
    {
        "loss": 0.7054,
        "grad_norm": 4.283596992492676,
        "learning_rate": 0.00018972300663154353,
        "epoch": 1.4321333333333333,
        "step": 10741
    },
    {
        "loss": 2.4999,
        "grad_norm": 2.8512685298919678,
        "learning_rate": 0.00018969519830131,
        "epoch": 1.4322666666666666,
        "step": 10742
    },
    {
        "loss": 1.6079,
        "grad_norm": 3.8279800415039062,
        "learning_rate": 0.00018966735444250285,
        "epoch": 1.4324,
        "step": 10743
    },
    {
        "loss": 1.6805,
        "grad_norm": 3.225602388381958,
        "learning_rate": 0.00018963947506615101,
        "epoch": 1.4325333333333332,
        "step": 10744
    },
    {
        "loss": 2.449,
        "grad_norm": 4.113253116607666,
        "learning_rate": 0.00018961156018329774,
        "epoch": 1.4326666666666665,
        "step": 10745
    },
    {
        "loss": 2.0139,
        "grad_norm": 3.3851749897003174,
        "learning_rate": 0.00018958360980500012,
        "epoch": 1.4328,
        "step": 10746
    },
    {
        "loss": 1.8716,
        "grad_norm": 5.614554405212402,
        "learning_rate": 0.00018955562394232939,
        "epoch": 1.4329333333333334,
        "step": 10747
    },
    {
        "loss": 1.3689,
        "grad_norm": 6.610302925109863,
        "learning_rate": 0.00018952760260637083,
        "epoch": 1.4330666666666667,
        "step": 10748
    },
    {
        "loss": 1.8064,
        "grad_norm": 3.6446409225463867,
        "learning_rate": 0.00018949954580822388,
        "epoch": 1.4332,
        "step": 10749
    },
    {
        "loss": 2.3437,
        "grad_norm": 2.1753461360931396,
        "learning_rate": 0.00018947145355900185,
        "epoch": 1.4333333333333333,
        "step": 10750
    },
    {
        "loss": 1.8495,
        "grad_norm": 5.016623497009277,
        "learning_rate": 0.00018944332586983214,
        "epoch": 1.4334666666666667,
        "step": 10751
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.7670161724090576,
        "learning_rate": 0.00018941516275185636,
        "epoch": 1.4336,
        "step": 10752
    },
    {
        "loss": 2.88,
        "grad_norm": 4.968822002410889,
        "learning_rate": 0.00018938696421622994,
        "epoch": 1.4337333333333333,
        "step": 10753
    },
    {
        "loss": 2.689,
        "grad_norm": 2.74634051322937,
        "learning_rate": 0.00018935873027412235,
        "epoch": 1.4338666666666666,
        "step": 10754
    },
    {
        "loss": 1.9998,
        "grad_norm": 3.486274480819702,
        "learning_rate": 0.00018933046093671727,
        "epoch": 1.434,
        "step": 10755
    },
    {
        "loss": 2.5836,
        "grad_norm": 4.479734420776367,
        "learning_rate": 0.00018930215621521223,
        "epoch": 1.4341333333333333,
        "step": 10756
    },
    {
        "loss": 2.1857,
        "grad_norm": 4.910617351531982,
        "learning_rate": 0.00018927381612081877,
        "epoch": 1.4342666666666668,
        "step": 10757
    },
    {
        "loss": 2.065,
        "grad_norm": 4.094331741333008,
        "learning_rate": 0.00018924544066476262,
        "epoch": 1.4344000000000001,
        "step": 10758
    },
    {
        "loss": 2.5226,
        "grad_norm": 3.8309102058410645,
        "learning_rate": 0.0001892170298582833,
        "epoch": 1.4345333333333334,
        "step": 10759
    },
    {
        "loss": 1.8854,
        "grad_norm": 5.580044269561768,
        "learning_rate": 0.00018918858371263443,
        "epoch": 1.4346666666666668,
        "step": 10760
    },
    {
        "loss": 2.0642,
        "grad_norm": 3.879375696182251,
        "learning_rate": 0.00018916010223908364,
        "epoch": 1.4348,
        "step": 10761
    },
    {
        "loss": 2.28,
        "grad_norm": 3.068319082260132,
        "learning_rate": 0.00018913158544891262,
        "epoch": 1.4349333333333334,
        "step": 10762
    },
    {
        "loss": 2.2197,
        "grad_norm": 3.5147697925567627,
        "learning_rate": 0.00018910303335341683,
        "epoch": 1.4350666666666667,
        "step": 10763
    },
    {
        "loss": 1.1673,
        "grad_norm": 6.623409271240234,
        "learning_rate": 0.0001890744459639059,
        "epoch": 1.4352,
        "step": 10764
    },
    {
        "loss": 2.0036,
        "grad_norm": 4.429891109466553,
        "learning_rate": 0.0001890458232917035,
        "epoch": 1.4353333333333333,
        "step": 10765
    },
    {
        "loss": 2.4176,
        "grad_norm": 4.32204532623291,
        "learning_rate": 0.00018901716534814705,
        "epoch": 1.4354666666666667,
        "step": 10766
    },
    {
        "loss": 1.9997,
        "grad_norm": 4.142117500305176,
        "learning_rate": 0.00018898847214458798,
        "epoch": 1.4356,
        "step": 10767
    },
    {
        "loss": 2.008,
        "grad_norm": 6.861474514007568,
        "learning_rate": 0.000188959743692392,
        "epoch": 1.4357333333333333,
        "step": 10768
    },
    {
        "loss": 2.2432,
        "grad_norm": 4.3525848388671875,
        "learning_rate": 0.0001889309800029383,
        "epoch": 1.4358666666666666,
        "step": 10769
    },
    {
        "loss": 2.5959,
        "grad_norm": 4.269968032836914,
        "learning_rate": 0.00018890218108762058,
        "epoch": 1.436,
        "step": 10770
    },
    {
        "loss": 2.1139,
        "grad_norm": 3.7971596717834473,
        "learning_rate": 0.00018887334695784586,
        "epoch": 1.4361333333333333,
        "step": 10771
    },
    {
        "loss": 2.8432,
        "grad_norm": 5.794326305389404,
        "learning_rate": 0.00018884447762503557,
        "epoch": 1.4362666666666666,
        "step": 10772
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.2988476753234863,
        "learning_rate": 0.00018881557310062504,
        "epoch": 1.4364,
        "step": 10773
    },
    {
        "loss": 1.9714,
        "grad_norm": 4.0356879234313965,
        "learning_rate": 0.00018878663339606336,
        "epoch": 1.4365333333333332,
        "step": 10774
    },
    {
        "loss": 0.5993,
        "grad_norm": 3.5696589946746826,
        "learning_rate": 0.00018875765852281365,
        "epoch": 1.4366666666666665,
        "step": 10775
    },
    {
        "loss": 1.6239,
        "grad_norm": 7.4120354652404785,
        "learning_rate": 0.00018872864849235293,
        "epoch": 1.4368,
        "step": 10776
    },
    {
        "loss": 2.1219,
        "grad_norm": 3.0754692554473877,
        "learning_rate": 0.00018869960331617224,
        "epoch": 1.4369333333333334,
        "step": 10777
    },
    {
        "loss": 1.8769,
        "grad_norm": 3.867645502090454,
        "learning_rate": 0.00018867052300577638,
        "epoch": 1.4370666666666667,
        "step": 10778
    },
    {
        "loss": 2.0245,
        "grad_norm": 2.9640893936157227,
        "learning_rate": 0.00018864140757268437,
        "epoch": 1.4372,
        "step": 10779
    },
    {
        "loss": 2.1824,
        "grad_norm": 4.310103416442871,
        "learning_rate": 0.00018861225702842863,
        "epoch": 1.4373333333333334,
        "step": 10780
    },
    {
        "loss": 1.9445,
        "grad_norm": 4.427868366241455,
        "learning_rate": 0.00018858307138455596,
        "epoch": 1.4374666666666667,
        "step": 10781
    },
    {
        "loss": 2.7084,
        "grad_norm": 2.8339834213256836,
        "learning_rate": 0.00018855385065262695,
        "epoch": 1.4376,
        "step": 10782
    },
    {
        "loss": 0.7343,
        "grad_norm": 3.0022835731506348,
        "learning_rate": 0.00018852459484421593,
        "epoch": 1.4377333333333333,
        "step": 10783
    },
    {
        "loss": 2.7724,
        "grad_norm": 5.277461528778076,
        "learning_rate": 0.00018849530397091127,
        "epoch": 1.4378666666666666,
        "step": 10784
    },
    {
        "loss": 2.5324,
        "grad_norm": 3.5334479808807373,
        "learning_rate": 0.00018846597804431507,
        "epoch": 1.438,
        "step": 10785
    },
    {
        "loss": 2.1214,
        "grad_norm": 4.943829536437988,
        "learning_rate": 0.00018843661707604363,
        "epoch": 1.4381333333333333,
        "step": 10786
    },
    {
        "loss": 1.2594,
        "grad_norm": 5.326956748962402,
        "learning_rate": 0.00018840722107772682,
        "epoch": 1.4382666666666668,
        "step": 10787
    },
    {
        "loss": 1.6352,
        "grad_norm": 4.506839752197266,
        "learning_rate": 0.0001883777900610085,
        "epoch": 1.4384000000000001,
        "step": 10788
    },
    {
        "loss": 2.024,
        "grad_norm": 3.4285895824432373,
        "learning_rate": 0.00018834832403754633,
        "epoch": 1.4385333333333334,
        "step": 10789
    },
    {
        "loss": 2.2454,
        "grad_norm": 3.572547197341919,
        "learning_rate": 0.00018831882301901195,
        "epoch": 1.4386666666666668,
        "step": 10790
    },
    {
        "loss": 2.7442,
        "grad_norm": 4.149123668670654,
        "learning_rate": 0.00018828928701709096,
        "epoch": 1.4388,
        "step": 10791
    },
    {
        "loss": 1.3517,
        "grad_norm": 5.756343364715576,
        "learning_rate": 0.00018825971604348243,
        "epoch": 1.4389333333333334,
        "step": 10792
    },
    {
        "loss": 2.2243,
        "grad_norm": 5.2870635986328125,
        "learning_rate": 0.00018823011010989972,
        "epoch": 1.4390666666666667,
        "step": 10793
    },
    {
        "loss": 1.5897,
        "grad_norm": 3.7855751514434814,
        "learning_rate": 0.00018820046922806967,
        "epoch": 1.4392,
        "step": 10794
    },
    {
        "loss": 2.5712,
        "grad_norm": 2.985029697418213,
        "learning_rate": 0.0001881707934097333,
        "epoch": 1.4393333333333334,
        "step": 10795
    },
    {
        "loss": 2.0165,
        "grad_norm": 5.878156661987305,
        "learning_rate": 0.00018814108266664522,
        "epoch": 1.4394666666666667,
        "step": 10796
    },
    {
        "loss": 2.4311,
        "grad_norm": 2.9212753772735596,
        "learning_rate": 0.00018811133701057396,
        "epoch": 1.4396,
        "step": 10797
    },
    {
        "loss": 2.1638,
        "grad_norm": 4.757256984710693,
        "learning_rate": 0.0001880815564533018,
        "epoch": 1.4397333333333333,
        "step": 10798
    },
    {
        "loss": 1.6219,
        "grad_norm": 5.137350559234619,
        "learning_rate": 0.00018805174100662502,
        "epoch": 1.4398666666666666,
        "step": 10799
    },
    {
        "loss": 1.9571,
        "grad_norm": 4.165030479431152,
        "learning_rate": 0.00018802189068235374,
        "epoch": 1.44,
        "step": 10800
    },
    {
        "loss": 1.8907,
        "grad_norm": 6.004042625427246,
        "learning_rate": 0.0001879920054923115,
        "epoch": 1.4401333333333333,
        "step": 10801
    },
    {
        "loss": 2.4924,
        "grad_norm": 3.00571870803833,
        "learning_rate": 0.00018796208544833606,
        "epoch": 1.4402666666666666,
        "step": 10802
    },
    {
        "loss": 2.1991,
        "grad_norm": 4.372211456298828,
        "learning_rate": 0.00018793213056227885,
        "epoch": 1.4404,
        "step": 10803
    },
    {
        "loss": 2.2638,
        "grad_norm": 3.601391077041626,
        "learning_rate": 0.00018790214084600515,
        "epoch": 1.4405333333333332,
        "step": 10804
    },
    {
        "loss": 1.9608,
        "grad_norm": 3.8564231395721436,
        "learning_rate": 0.00018787211631139395,
        "epoch": 1.4406666666666665,
        "step": 10805
    },
    {
        "loss": 2.4246,
        "grad_norm": 4.18889045715332,
        "learning_rate": 0.00018784205697033808,
        "epoch": 1.4408,
        "step": 10806
    },
    {
        "loss": 2.2913,
        "grad_norm": 5.277446746826172,
        "learning_rate": 0.00018781196283474403,
        "epoch": 1.4409333333333334,
        "step": 10807
    },
    {
        "loss": 1.3566,
        "grad_norm": 4.7200751304626465,
        "learning_rate": 0.00018778183391653235,
        "epoch": 1.4410666666666667,
        "step": 10808
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.0815510749816895,
        "learning_rate": 0.00018775167022763717,
        "epoch": 1.4412,
        "step": 10809
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.2293190956115723,
        "learning_rate": 0.00018772147178000635,
        "epoch": 1.4413333333333334,
        "step": 10810
    },
    {
        "loss": 1.9479,
        "grad_norm": 3.2291667461395264,
        "learning_rate": 0.0001876912385856017,
        "epoch": 1.4414666666666667,
        "step": 10811
    },
    {
        "loss": 2.8683,
        "grad_norm": 2.808908224105835,
        "learning_rate": 0.00018766097065639863,
        "epoch": 1.4416,
        "step": 10812
    },
    {
        "loss": 2.577,
        "grad_norm": 4.308724880218506,
        "learning_rate": 0.00018763066800438636,
        "epoch": 1.4417333333333333,
        "step": 10813
    },
    {
        "loss": 1.8313,
        "grad_norm": 4.010860919952393,
        "learning_rate": 0.00018760033064156795,
        "epoch": 1.4418666666666666,
        "step": 10814
    },
    {
        "loss": 2.3243,
        "grad_norm": 4.5359978675842285,
        "learning_rate": 0.00018756995857996004,
        "epoch": 1.442,
        "step": 10815
    },
    {
        "loss": 2.9076,
        "grad_norm": 3.907729387283325,
        "learning_rate": 0.00018753955183159314,
        "epoch": 1.4421333333333333,
        "step": 10816
    },
    {
        "loss": 2.515,
        "grad_norm": 3.801933526992798,
        "learning_rate": 0.0001875091104085115,
        "epoch": 1.4422666666666666,
        "step": 10817
    },
    {
        "loss": 2.2487,
        "grad_norm": 3.790691375732422,
        "learning_rate": 0.00018747863432277308,
        "epoch": 1.4424000000000001,
        "step": 10818
    },
    {
        "loss": 2.3164,
        "grad_norm": 3.652540445327759,
        "learning_rate": 0.00018744812358644945,
        "epoch": 1.4425333333333334,
        "step": 10819
    },
    {
        "loss": 1.5693,
        "grad_norm": 5.067070484161377,
        "learning_rate": 0.00018741757821162617,
        "epoch": 1.4426666666666668,
        "step": 10820
    },
    {
        "loss": 1.6512,
        "grad_norm": 3.437424898147583,
        "learning_rate": 0.00018738699821040227,
        "epoch": 1.4428,
        "step": 10821
    },
    {
        "loss": 1.7668,
        "grad_norm": 5.551323890686035,
        "learning_rate": 0.00018735638359489057,
        "epoch": 1.4429333333333334,
        "step": 10822
    },
    {
        "loss": 2.1636,
        "grad_norm": 2.6400632858276367,
        "learning_rate": 0.00018732573437721771,
        "epoch": 1.4430666666666667,
        "step": 10823
    },
    {
        "loss": 2.1785,
        "grad_norm": 3.6246845722198486,
        "learning_rate": 0.00018729505056952394,
        "epoch": 1.4432,
        "step": 10824
    },
    {
        "loss": 2.0891,
        "grad_norm": 2.7147209644317627,
        "learning_rate": 0.0001872643321839632,
        "epoch": 1.4433333333333334,
        "step": 10825
    },
    {
        "loss": 2.305,
        "grad_norm": 3.5466413497924805,
        "learning_rate": 0.00018723357923270304,
        "epoch": 1.4434666666666667,
        "step": 10826
    },
    {
        "loss": 2.0468,
        "grad_norm": 3.787217617034912,
        "learning_rate": 0.00018720279172792497,
        "epoch": 1.4436,
        "step": 10827
    },
    {
        "loss": 2.2898,
        "grad_norm": 2.707399368286133,
        "learning_rate": 0.00018717196968182388,
        "epoch": 1.4437333333333333,
        "step": 10828
    },
    {
        "loss": 2.2019,
        "grad_norm": 3.9084460735321045,
        "learning_rate": 0.00018714111310660873,
        "epoch": 1.4438666666666666,
        "step": 10829
    },
    {
        "loss": 2.7005,
        "grad_norm": 4.077117443084717,
        "learning_rate": 0.0001871102220145016,
        "epoch": 1.444,
        "step": 10830
    },
    {
        "loss": 2.1376,
        "grad_norm": 3.2591490745544434,
        "learning_rate": 0.0001870792964177387,
        "epoch": 1.4441333333333333,
        "step": 10831
    },
    {
        "loss": 1.9743,
        "grad_norm": 3.8287644386291504,
        "learning_rate": 0.0001870483363285699,
        "epoch": 1.4442666666666666,
        "step": 10832
    },
    {
        "loss": 1.9874,
        "grad_norm": 5.259500026702881,
        "learning_rate": 0.00018701734175925841,
        "epoch": 1.4444,
        "step": 10833
    },
    {
        "loss": 1.9335,
        "grad_norm": 3.726346254348755,
        "learning_rate": 0.00018698631272208137,
        "epoch": 1.4445333333333332,
        "step": 10834
    },
    {
        "loss": 2.2125,
        "grad_norm": 3.5814573764801025,
        "learning_rate": 0.00018695524922932938,
        "epoch": 1.4446666666666665,
        "step": 10835
    },
    {
        "loss": 0.8027,
        "grad_norm": 3.2336738109588623,
        "learning_rate": 0.00018692415129330697,
        "epoch": 1.4447999999999999,
        "step": 10836
    },
    {
        "loss": 2.4246,
        "grad_norm": 2.5225911140441895,
        "learning_rate": 0.00018689301892633195,
        "epoch": 1.4449333333333334,
        "step": 10837
    },
    {
        "loss": 2.5335,
        "grad_norm": 4.286986351013184,
        "learning_rate": 0.00018686185214073623,
        "epoch": 1.4450666666666667,
        "step": 10838
    },
    {
        "loss": 2.5032,
        "grad_norm": 4.158437728881836,
        "learning_rate": 0.00018683065094886472,
        "epoch": 1.4452,
        "step": 10839
    },
    {
        "loss": 1.1221,
        "grad_norm": 3.432642936706543,
        "learning_rate": 0.0001867994153630765,
        "epoch": 1.4453333333333334,
        "step": 10840
    },
    {
        "loss": 2.4537,
        "grad_norm": 4.087735652923584,
        "learning_rate": 0.00018676814539574422,
        "epoch": 1.4454666666666667,
        "step": 10841
    },
    {
        "loss": 2.1742,
        "grad_norm": 4.865254878997803,
        "learning_rate": 0.00018673684105925383,
        "epoch": 1.4456,
        "step": 10842
    },
    {
        "loss": 2.3454,
        "grad_norm": 4.719925880432129,
        "learning_rate": 0.00018670550236600517,
        "epoch": 1.4457333333333333,
        "step": 10843
    },
    {
        "loss": 2.2941,
        "grad_norm": 3.5159685611724854,
        "learning_rate": 0.00018667412932841151,
        "epoch": 1.4458666666666666,
        "step": 10844
    },
    {
        "loss": 1.6766,
        "grad_norm": 7.01373291015625,
        "learning_rate": 0.0001866427219589,
        "epoch": 1.446,
        "step": 10845
    },
    {
        "loss": 2.4751,
        "grad_norm": 6.914608001708984,
        "learning_rate": 0.000186611280269911,
        "epoch": 1.4461333333333333,
        "step": 10846
    },
    {
        "loss": 2.418,
        "grad_norm": 5.059474945068359,
        "learning_rate": 0.00018657980427389884,
        "epoch": 1.4462666666666666,
        "step": 10847
    },
    {
        "loss": 2.8309,
        "grad_norm": 2.4964346885681152,
        "learning_rate": 0.0001865482939833311,
        "epoch": 1.4464000000000001,
        "step": 10848
    },
    {
        "loss": 1.6339,
        "grad_norm": 4.460158824920654,
        "learning_rate": 0.0001865167494106892,
        "epoch": 1.4465333333333334,
        "step": 10849
    },
    {
        "loss": 2.2648,
        "grad_norm": 3.3659369945526123,
        "learning_rate": 0.00018648517056846822,
        "epoch": 1.4466666666666668,
        "step": 10850
    },
    {
        "loss": 2.623,
        "grad_norm": 2.6269092559814453,
        "learning_rate": 0.00018645355746917632,
        "epoch": 1.4468,
        "step": 10851
    },
    {
        "loss": 2.9815,
        "grad_norm": 4.015993595123291,
        "learning_rate": 0.00018642191012533584,
        "epoch": 1.4469333333333334,
        "step": 10852
    },
    {
        "loss": 1.7037,
        "grad_norm": 3.477623462677002,
        "learning_rate": 0.00018639022854948216,
        "epoch": 1.4470666666666667,
        "step": 10853
    },
    {
        "loss": 1.9763,
        "grad_norm": 4.492977142333984,
        "learning_rate": 0.0001863585127541647,
        "epoch": 1.4472,
        "step": 10854
    },
    {
        "loss": 2.4177,
        "grad_norm": 2.948697566986084,
        "learning_rate": 0.00018632676275194605,
        "epoch": 1.4473333333333334,
        "step": 10855
    },
    {
        "loss": 2.1652,
        "grad_norm": 3.598430633544922,
        "learning_rate": 0.00018629497855540252,
        "epoch": 1.4474666666666667,
        "step": 10856
    },
    {
        "loss": 0.9895,
        "grad_norm": 5.027272701263428,
        "learning_rate": 0.00018626316017712389,
        "epoch": 1.4476,
        "step": 10857
    },
    {
        "loss": 3.7726,
        "grad_norm": 6.396230220794678,
        "learning_rate": 0.00018623130762971356,
        "epoch": 1.4477333333333333,
        "step": 10858
    },
    {
        "loss": 2.6821,
        "grad_norm": 2.208620309829712,
        "learning_rate": 0.0001861994209257886,
        "epoch": 1.4478666666666666,
        "step": 10859
    },
    {
        "loss": 2.2676,
        "grad_norm": 5.943939208984375,
        "learning_rate": 0.00018616750007797916,
        "epoch": 1.448,
        "step": 10860
    },
    {
        "loss": 1.4921,
        "grad_norm": 2.889002799987793,
        "learning_rate": 0.0001861355450989294,
        "epoch": 1.4481333333333333,
        "step": 10861
    },
    {
        "loss": 1.6063,
        "grad_norm": 4.517246723175049,
        "learning_rate": 0.00018610355600129664,
        "epoch": 1.4482666666666666,
        "step": 10862
    },
    {
        "loss": 2.2051,
        "grad_norm": 4.203526020050049,
        "learning_rate": 0.00018607153279775207,
        "epoch": 1.4484,
        "step": 10863
    },
    {
        "loss": 2.0464,
        "grad_norm": 6.3245768547058105,
        "learning_rate": 0.00018603947550098005,
        "epoch": 1.4485333333333332,
        "step": 10864
    },
    {
        "loss": 2.5244,
        "grad_norm": 2.973789930343628,
        "learning_rate": 0.0001860073841236786,
        "epoch": 1.4486666666666665,
        "step": 10865
    },
    {
        "loss": 2.221,
        "grad_norm": 4.16475772857666,
        "learning_rate": 0.00018597525867855916,
        "epoch": 1.4487999999999999,
        "step": 10866
    },
    {
        "loss": 1.8668,
        "grad_norm": 2.9791934490203857,
        "learning_rate": 0.0001859430991783469,
        "epoch": 1.4489333333333334,
        "step": 10867
    },
    {
        "loss": 2.5539,
        "grad_norm": 3.4817302227020264,
        "learning_rate": 0.00018591090563578017,
        "epoch": 1.4490666666666667,
        "step": 10868
    },
    {
        "loss": 1.2696,
        "grad_norm": 3.050455093383789,
        "learning_rate": 0.00018587867806361095,
        "epoch": 1.4492,
        "step": 10869
    },
    {
        "loss": 1.9283,
        "grad_norm": 6.194765567779541,
        "learning_rate": 0.0001858464164746048,
        "epoch": 1.4493333333333334,
        "step": 10870
    },
    {
        "loss": 2.255,
        "grad_norm": 3.9306533336639404,
        "learning_rate": 0.00018581412088154058,
        "epoch": 1.4494666666666667,
        "step": 10871
    },
    {
        "loss": 0.9447,
        "grad_norm": 4.310296058654785,
        "learning_rate": 0.00018578179129721055,
        "epoch": 1.4496,
        "step": 10872
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.8170855045318604,
        "learning_rate": 0.0001857494277344208,
        "epoch": 1.4497333333333333,
        "step": 10873
    },
    {
        "loss": 1.8702,
        "grad_norm": 5.282038688659668,
        "learning_rate": 0.00018571703020599055,
        "epoch": 1.4498666666666666,
        "step": 10874
    },
    {
        "loss": 1.3532,
        "grad_norm": 4.236130237579346,
        "learning_rate": 0.00018568459872475245,
        "epoch": 1.45,
        "step": 10875
    },
    {
        "loss": 2.3678,
        "grad_norm": 3.155195713043213,
        "learning_rate": 0.00018565213330355295,
        "epoch": 1.4501333333333333,
        "step": 10876
    },
    {
        "loss": 1.4046,
        "grad_norm": 6.529730796813965,
        "learning_rate": 0.00018561963395525156,
        "epoch": 1.4502666666666666,
        "step": 10877
    },
    {
        "loss": 1.9191,
        "grad_norm": 3.77828049659729,
        "learning_rate": 0.00018558710069272138,
        "epoch": 1.4504000000000001,
        "step": 10878
    },
    {
        "loss": 1.3184,
        "grad_norm": 3.429772138595581,
        "learning_rate": 0.00018555453352884905,
        "epoch": 1.4505333333333335,
        "step": 10879
    },
    {
        "loss": 2.4498,
        "grad_norm": 3.4553110599517822,
        "learning_rate": 0.00018552193247653445,
        "epoch": 1.4506666666666668,
        "step": 10880
    },
    {
        "loss": 0.5303,
        "grad_norm": 3.5128555297851562,
        "learning_rate": 0.0001854892975486909,
        "epoch": 1.4508,
        "step": 10881
    },
    {
        "loss": 2.8424,
        "grad_norm": 2.9296514987945557,
        "learning_rate": 0.00018545662875824541,
        "epoch": 1.4509333333333334,
        "step": 10882
    },
    {
        "loss": 1.7577,
        "grad_norm": 4.842856407165527,
        "learning_rate": 0.0001854239261181381,
        "epoch": 1.4510666666666667,
        "step": 10883
    },
    {
        "loss": 2.6669,
        "grad_norm": 3.4056079387664795,
        "learning_rate": 0.0001853911896413225,
        "epoch": 1.4512,
        "step": 10884
    },
    {
        "loss": 2.1055,
        "grad_norm": 4.237600326538086,
        "learning_rate": 0.0001853584193407657,
        "epoch": 1.4513333333333334,
        "step": 10885
    },
    {
        "loss": 2.4387,
        "grad_norm": 3.7220301628112793,
        "learning_rate": 0.0001853256152294482,
        "epoch": 1.4514666666666667,
        "step": 10886
    },
    {
        "loss": 2.1101,
        "grad_norm": 2.2807416915893555,
        "learning_rate": 0.0001852927773203637,
        "epoch": 1.4516,
        "step": 10887
    },
    {
        "loss": 2.4876,
        "grad_norm": 6.108607292175293,
        "learning_rate": 0.00018525990562651961,
        "epoch": 1.4517333333333333,
        "step": 10888
    },
    {
        "loss": 2.8494,
        "grad_norm": 2.8545024394989014,
        "learning_rate": 0.00018522700016093628,
        "epoch": 1.4518666666666666,
        "step": 10889
    },
    {
        "loss": 1.208,
        "grad_norm": 4.308278560638428,
        "learning_rate": 0.00018519406093664778,
        "epoch": 1.452,
        "step": 10890
    },
    {
        "loss": 2.3672,
        "grad_norm": 4.800609111785889,
        "learning_rate": 0.00018516108796670148,
        "epoch": 1.4521333333333333,
        "step": 10891
    },
    {
        "loss": 2.6067,
        "grad_norm": 4.021961688995361,
        "learning_rate": 0.0001851280812641581,
        "epoch": 1.4522666666666666,
        "step": 10892
    },
    {
        "loss": 2.7001,
        "grad_norm": 3.74501895904541,
        "learning_rate": 0.00018509504084209162,
        "epoch": 1.4524,
        "step": 10893
    },
    {
        "loss": 1.8088,
        "grad_norm": 4.026756286621094,
        "learning_rate": 0.00018506196671358945,
        "epoch": 1.4525333333333332,
        "step": 10894
    },
    {
        "loss": 1.8346,
        "grad_norm": 7.2139363288879395,
        "learning_rate": 0.0001850288588917525,
        "epoch": 1.4526666666666666,
        "step": 10895
    },
    {
        "loss": 2.1904,
        "grad_norm": 3.649059534072876,
        "learning_rate": 0.00018499571738969474,
        "epoch": 1.4527999999999999,
        "step": 10896
    },
    {
        "loss": 1.8203,
        "grad_norm": 3.1504061222076416,
        "learning_rate": 0.00018496254222054388,
        "epoch": 1.4529333333333334,
        "step": 10897
    },
    {
        "loss": 2.1075,
        "grad_norm": 2.414004325866699,
        "learning_rate": 0.00018492933339744037,
        "epoch": 1.4530666666666667,
        "step": 10898
    },
    {
        "loss": 2.3142,
        "grad_norm": 3.6006486415863037,
        "learning_rate": 0.00018489609093353852,
        "epoch": 1.4532,
        "step": 10899
    },
    {
        "loss": 2.2623,
        "grad_norm": 4.4788384437561035,
        "learning_rate": 0.00018486281484200582,
        "epoch": 1.4533333333333334,
        "step": 10900
    },
    {
        "loss": 2.2992,
        "grad_norm": 5.321666717529297,
        "learning_rate": 0.00018482950513602303,
        "epoch": 1.4534666666666667,
        "step": 10901
    },
    {
        "loss": 2.3885,
        "grad_norm": 3.997649669647217,
        "learning_rate": 0.00018479616182878422,
        "epoch": 1.4536,
        "step": 10902
    },
    {
        "loss": 1.5269,
        "grad_norm": 3.876516342163086,
        "learning_rate": 0.00018476278493349666,
        "epoch": 1.4537333333333333,
        "step": 10903
    },
    {
        "loss": 0.6256,
        "grad_norm": 2.6535048484802246,
        "learning_rate": 0.00018472937446338124,
        "epoch": 1.4538666666666666,
        "step": 10904
    },
    {
        "loss": 1.781,
        "grad_norm": 4.5526652336120605,
        "learning_rate": 0.00018469593043167194,
        "epoch": 1.454,
        "step": 10905
    },
    {
        "loss": 1.8042,
        "grad_norm": 6.554699897766113,
        "learning_rate": 0.00018466245285161598,
        "epoch": 1.4541333333333333,
        "step": 10906
    },
    {
        "loss": 1.2085,
        "grad_norm": 8.011333465576172,
        "learning_rate": 0.00018462894173647385,
        "epoch": 1.4542666666666666,
        "step": 10907
    },
    {
        "loss": 2.9295,
        "grad_norm": 2.90547513961792,
        "learning_rate": 0.00018459539709951957,
        "epoch": 1.4544000000000001,
        "step": 10908
    },
    {
        "loss": 1.8555,
        "grad_norm": 3.6969707012176514,
        "learning_rate": 0.00018456181895404038,
        "epoch": 1.4545333333333335,
        "step": 10909
    },
    {
        "loss": 2.8955,
        "grad_norm": 3.75398588180542,
        "learning_rate": 0.00018452820731333644,
        "epoch": 1.4546666666666668,
        "step": 10910
    },
    {
        "loss": 1.9323,
        "grad_norm": 4.880461692810059,
        "learning_rate": 0.0001844945621907216,
        "epoch": 1.4548,
        "step": 10911
    },
    {
        "loss": 2.0209,
        "grad_norm": 5.739006042480469,
        "learning_rate": 0.0001844608835995227,
        "epoch": 1.4549333333333334,
        "step": 10912
    },
    {
        "loss": 1.5821,
        "grad_norm": 4.559377670288086,
        "learning_rate": 0.00018442717155308012,
        "epoch": 1.4550666666666667,
        "step": 10913
    },
    {
        "loss": 1.7258,
        "grad_norm": 2.304722309112549,
        "learning_rate": 0.00018439342606474718,
        "epoch": 1.4552,
        "step": 10914
    },
    {
        "loss": 1.7558,
        "grad_norm": 4.337166786193848,
        "learning_rate": 0.00018435964714789063,
        "epoch": 1.4553333333333334,
        "step": 10915
    },
    {
        "loss": 1.3426,
        "grad_norm": 2.680696964263916,
        "learning_rate": 0.00018432583481589032,
        "epoch": 1.4554666666666667,
        "step": 10916
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.737154960632324,
        "learning_rate": 0.00018429198908213946,
        "epoch": 1.4556,
        "step": 10917
    },
    {
        "loss": 1.5449,
        "grad_norm": 4.401240348815918,
        "learning_rate": 0.0001842581099600447,
        "epoch": 1.4557333333333333,
        "step": 10918
    },
    {
        "loss": 1.8386,
        "grad_norm": 4.84341287612915,
        "learning_rate": 0.00018422419746302535,
        "epoch": 1.4558666666666666,
        "step": 10919
    },
    {
        "loss": 2.4106,
        "grad_norm": 4.136438369750977,
        "learning_rate": 0.00018419025160451444,
        "epoch": 1.456,
        "step": 10920
    },
    {
        "loss": 2.3433,
        "grad_norm": 4.666528224945068,
        "learning_rate": 0.00018415627239795794,
        "epoch": 1.4561333333333333,
        "step": 10921
    },
    {
        "loss": 2.2206,
        "grad_norm": 5.378710746765137,
        "learning_rate": 0.00018412225985681526,
        "epoch": 1.4562666666666666,
        "step": 10922
    },
    {
        "loss": 2.6604,
        "grad_norm": 2.843644380569458,
        "learning_rate": 0.0001840882139945588,
        "epoch": 1.4564,
        "step": 10923
    },
    {
        "loss": 0.8792,
        "grad_norm": 5.03328275680542,
        "learning_rate": 0.00018405413482467428,
        "epoch": 1.4565333333333332,
        "step": 10924
    },
    {
        "loss": 2.3231,
        "grad_norm": 3.651118516921997,
        "learning_rate": 0.00018402002236066043,
        "epoch": 1.4566666666666666,
        "step": 10925
    },
    {
        "loss": 1.9649,
        "grad_norm": 5.256961345672607,
        "learning_rate": 0.00018398587661602953,
        "epoch": 1.4567999999999999,
        "step": 10926
    },
    {
        "loss": 2.6965,
        "grad_norm": 4.737443923950195,
        "learning_rate": 0.00018395169760430672,
        "epoch": 1.4569333333333334,
        "step": 10927
    },
    {
        "loss": 1.9451,
        "grad_norm": 4.249260902404785,
        "learning_rate": 0.00018391748533903038,
        "epoch": 1.4570666666666667,
        "step": 10928
    },
    {
        "loss": 2.0306,
        "grad_norm": 3.344515562057495,
        "learning_rate": 0.00018388323983375222,
        "epoch": 1.4572,
        "step": 10929
    },
    {
        "loss": 2.3345,
        "grad_norm": 3.587714195251465,
        "learning_rate": 0.0001838489611020369,
        "epoch": 1.4573333333333334,
        "step": 10930
    },
    {
        "loss": 2.764,
        "grad_norm": 4.288863658905029,
        "learning_rate": 0.00018381464915746235,
        "epoch": 1.4574666666666667,
        "step": 10931
    },
    {
        "loss": 2.3204,
        "grad_norm": 3.329306125640869,
        "learning_rate": 0.00018378030401361973,
        "epoch": 1.4576,
        "step": 10932
    },
    {
        "loss": 1.2031,
        "grad_norm": 6.650922775268555,
        "learning_rate": 0.00018374592568411323,
        "epoch": 1.4577333333333333,
        "step": 10933
    },
    {
        "loss": 2.4329,
        "grad_norm": 4.0885491371154785,
        "learning_rate": 0.0001837115141825602,
        "epoch": 1.4578666666666666,
        "step": 10934
    },
    {
        "loss": 1.3322,
        "grad_norm": 3.8804855346679688,
        "learning_rate": 0.0001836770695225911,
        "epoch": 1.458,
        "step": 10935
    },
    {
        "loss": 1.0276,
        "grad_norm": 5.888739109039307,
        "learning_rate": 0.0001836425917178497,
        "epoch": 1.4581333333333333,
        "step": 10936
    },
    {
        "loss": 3.0984,
        "grad_norm": 3.199418544769287,
        "learning_rate": 0.00018360808078199265,
        "epoch": 1.4582666666666666,
        "step": 10937
    },
    {
        "loss": 1.1973,
        "grad_norm": 2.405656337738037,
        "learning_rate": 0.00018357353672868997,
        "epoch": 1.4584,
        "step": 10938
    },
    {
        "loss": 1.9882,
        "grad_norm": 2.9905431270599365,
        "learning_rate": 0.00018353895957162462,
        "epoch": 1.4585333333333335,
        "step": 10939
    },
    {
        "loss": 2.035,
        "grad_norm": 6.620442867279053,
        "learning_rate": 0.0001835043493244927,
        "epoch": 1.4586666666666668,
        "step": 10940
    },
    {
        "loss": 2.151,
        "grad_norm": 3.7112209796905518,
        "learning_rate": 0.0001834697060010035,
        "epoch": 1.4588,
        "step": 10941
    },
    {
        "loss": 2.8735,
        "grad_norm": 3.522209644317627,
        "learning_rate": 0.00018343502961487936,
        "epoch": 1.4589333333333334,
        "step": 10942
    },
    {
        "loss": 2.1491,
        "grad_norm": 3.260843515396118,
        "learning_rate": 0.0001834003201798557,
        "epoch": 1.4590666666666667,
        "step": 10943
    },
    {
        "loss": 2.0507,
        "grad_norm": 2.997610092163086,
        "learning_rate": 0.00018336557770968096,
        "epoch": 1.4592,
        "step": 10944
    },
    {
        "loss": 1.6378,
        "grad_norm": 5.105613708496094,
        "learning_rate": 0.0001833308022181169,
        "epoch": 1.4593333333333334,
        "step": 10945
    },
    {
        "loss": 2.6781,
        "grad_norm": 3.9981746673583984,
        "learning_rate": 0.00018329599371893807,
        "epoch": 1.4594666666666667,
        "step": 10946
    },
    {
        "loss": 2.0343,
        "grad_norm": 5.524855136871338,
        "learning_rate": 0.00018326115222593245,
        "epoch": 1.4596,
        "step": 10947
    },
    {
        "loss": 3.0753,
        "grad_norm": 5.284116268157959,
        "learning_rate": 0.00018322627775290058,
        "epoch": 1.4597333333333333,
        "step": 10948
    },
    {
        "loss": 2.052,
        "grad_norm": 4.69478702545166,
        "learning_rate": 0.0001831913703136565,
        "epoch": 1.4598666666666666,
        "step": 10949
    },
    {
        "loss": 2.5868,
        "grad_norm": 4.344204902648926,
        "learning_rate": 0.00018315642992202725,
        "epoch": 1.46,
        "step": 10950
    },
    {
        "loss": 2.1776,
        "grad_norm": 3.5718133449554443,
        "learning_rate": 0.00018312145659185274,
        "epoch": 1.4601333333333333,
        "step": 10951
    },
    {
        "loss": 2.5525,
        "grad_norm": 5.239073753356934,
        "learning_rate": 0.00018308645033698605,
        "epoch": 1.4602666666666666,
        "step": 10952
    },
    {
        "loss": 2.0277,
        "grad_norm": 4.67832088470459,
        "learning_rate": 0.00018305141117129324,
        "epoch": 1.4604,
        "step": 10953
    },
    {
        "loss": 2.5703,
        "grad_norm": 3.5719351768493652,
        "learning_rate": 0.0001830163391086535,
        "epoch": 1.4605333333333332,
        "step": 10954
    },
    {
        "loss": 1.1802,
        "grad_norm": 3.6072990894317627,
        "learning_rate": 0.0001829812341629589,
        "epoch": 1.4606666666666666,
        "step": 10955
    },
    {
        "loss": 1.136,
        "grad_norm": 5.130859851837158,
        "learning_rate": 0.0001829460963481149,
        "epoch": 1.4607999999999999,
        "step": 10956
    },
    {
        "loss": 2.3019,
        "grad_norm": 3.5692832469940186,
        "learning_rate": 0.00018291092567803926,
        "epoch": 1.4609333333333332,
        "step": 10957
    },
    {
        "loss": 2.483,
        "grad_norm": 5.276274681091309,
        "learning_rate": 0.0001828757221666635,
        "epoch": 1.4610666666666667,
        "step": 10958
    },
    {
        "loss": 2.0229,
        "grad_norm": 6.262644290924072,
        "learning_rate": 0.00018284048582793187,
        "epoch": 1.4612,
        "step": 10959
    },
    {
        "loss": 1.7344,
        "grad_norm": 4.9530930519104,
        "learning_rate": 0.00018280521667580154,
        "epoch": 1.4613333333333334,
        "step": 10960
    },
    {
        "loss": 2.7673,
        "grad_norm": 3.1773602962493896,
        "learning_rate": 0.00018276991472424272,
        "epoch": 1.4614666666666667,
        "step": 10961
    },
    {
        "loss": 2.5806,
        "grad_norm": 5.7439351081848145,
        "learning_rate": 0.00018273457998723856,
        "epoch": 1.4616,
        "step": 10962
    },
    {
        "loss": 2.6434,
        "grad_norm": 3.963503837585449,
        "learning_rate": 0.00018269921247878544,
        "epoch": 1.4617333333333333,
        "step": 10963
    },
    {
        "loss": 1.929,
        "grad_norm": 3.9229888916015625,
        "learning_rate": 0.0001826638122128925,
        "epoch": 1.4618666666666666,
        "step": 10964
    },
    {
        "loss": 2.6641,
        "grad_norm": 3.8450825214385986,
        "learning_rate": 0.00018262837920358184,
        "epoch": 1.462,
        "step": 10965
    },
    {
        "loss": 1.7427,
        "grad_norm": 3.421142101287842,
        "learning_rate": 0.00018259291346488856,
        "epoch": 1.4621333333333333,
        "step": 10966
    },
    {
        "loss": 2.2356,
        "grad_norm": 2.8877241611480713,
        "learning_rate": 0.00018255741501086082,
        "epoch": 1.4622666666666666,
        "step": 10967
    },
    {
        "loss": 2.3576,
        "grad_norm": 2.4640655517578125,
        "learning_rate": 0.00018252188385555984,
        "epoch": 1.4624,
        "step": 10968
    },
    {
        "loss": 2.1636,
        "grad_norm": 2.8126871585845947,
        "learning_rate": 0.00018248632001305933,
        "epoch": 1.4625333333333335,
        "step": 10969
    },
    {
        "loss": 2.0588,
        "grad_norm": 3.4193050861358643,
        "learning_rate": 0.00018245072349744649,
        "epoch": 1.4626666666666668,
        "step": 10970
    },
    {
        "loss": 2.7363,
        "grad_norm": 2.5988354682922363,
        "learning_rate": 0.00018241509432282102,
        "epoch": 1.4628,
        "step": 10971
    },
    {
        "loss": 1.8058,
        "grad_norm": 3.7686264514923096,
        "learning_rate": 0.00018237943250329596,
        "epoch": 1.4629333333333334,
        "step": 10972
    },
    {
        "loss": 1.5909,
        "grad_norm": 3.719785213470459,
        "learning_rate": 0.000182343738052997,
        "epoch": 1.4630666666666667,
        "step": 10973
    },
    {
        "loss": 2.2866,
        "grad_norm": 4.2921648025512695,
        "learning_rate": 0.00018230801098606278,
        "epoch": 1.4632,
        "step": 10974
    },
    {
        "loss": 2.6675,
        "grad_norm": 3.694800615310669,
        "learning_rate": 0.0001822722513166449,
        "epoch": 1.4633333333333334,
        "step": 10975
    },
    {
        "loss": 2.518,
        "grad_norm": 4.106180667877197,
        "learning_rate": 0.00018223645905890795,
        "epoch": 1.4634666666666667,
        "step": 10976
    },
    {
        "loss": 1.0646,
        "grad_norm": 3.866164445877075,
        "learning_rate": 0.0001822006342270295,
        "epoch": 1.4636,
        "step": 10977
    },
    {
        "loss": 2.1069,
        "grad_norm": 3.190415382385254,
        "learning_rate": 0.00018216477683519953,
        "epoch": 1.4637333333333333,
        "step": 10978
    },
    {
        "loss": 2.2722,
        "grad_norm": 4.125644207000732,
        "learning_rate": 0.00018212888689762163,
        "epoch": 1.4638666666666666,
        "step": 10979
    },
    {
        "loss": 1.9146,
        "grad_norm": 3.9113895893096924,
        "learning_rate": 0.00018209296442851168,
        "epoch": 1.464,
        "step": 10980
    },
    {
        "loss": 2.6759,
        "grad_norm": 3.579378128051758,
        "learning_rate": 0.00018205700944209889,
        "epoch": 1.4641333333333333,
        "step": 10981
    },
    {
        "loss": 2.3859,
        "grad_norm": 3.7264251708984375,
        "learning_rate": 0.00018202102195262506,
        "epoch": 1.4642666666666666,
        "step": 10982
    },
    {
        "loss": 1.9544,
        "grad_norm": 3.641167640686035,
        "learning_rate": 0.00018198500197434494,
        "epoch": 1.4644,
        "step": 10983
    },
    {
        "loss": 2.4227,
        "grad_norm": 3.8994359970092773,
        "learning_rate": 0.00018194894952152608,
        "epoch": 1.4645333333333332,
        "step": 10984
    },
    {
        "loss": 1.9122,
        "grad_norm": 4.083367347717285,
        "learning_rate": 0.00018191286460844918,
        "epoch": 1.4646666666666666,
        "step": 10985
    },
    {
        "loss": 1.8676,
        "grad_norm": 4.806783199310303,
        "learning_rate": 0.0001818767472494075,
        "epoch": 1.4647999999999999,
        "step": 10986
    },
    {
        "loss": 2.4209,
        "grad_norm": 4.996936321258545,
        "learning_rate": 0.0001818405974587072,
        "epoch": 1.4649333333333332,
        "step": 10987
    },
    {
        "loss": 0.6011,
        "grad_norm": 2.84421968460083,
        "learning_rate": 0.0001818044152506674,
        "epoch": 1.4650666666666667,
        "step": 10988
    },
    {
        "loss": 1.869,
        "grad_norm": 3.4620931148529053,
        "learning_rate": 0.00018176820063962,
        "epoch": 1.4652,
        "step": 10989
    },
    {
        "loss": 2.6635,
        "grad_norm": 2.3986568450927734,
        "learning_rate": 0.00018173195363990963,
        "epoch": 1.4653333333333334,
        "step": 10990
    },
    {
        "loss": 1.3607,
        "grad_norm": 6.634371280670166,
        "learning_rate": 0.000181695674265894,
        "epoch": 1.4654666666666667,
        "step": 10991
    },
    {
        "loss": 2.4395,
        "grad_norm": 2.8627636432647705,
        "learning_rate": 0.00018165936253194343,
        "epoch": 1.4656,
        "step": 10992
    },
    {
        "loss": 2.3087,
        "grad_norm": 4.031715393066406,
        "learning_rate": 0.00018162301845244113,
        "epoch": 1.4657333333333333,
        "step": 10993
    },
    {
        "loss": 1.9421,
        "grad_norm": 3.6531684398651123,
        "learning_rate": 0.000181586642041783,
        "epoch": 1.4658666666666667,
        "step": 10994
    },
    {
        "loss": 1.6627,
        "grad_norm": 4.900719165802002,
        "learning_rate": 0.00018155023331437807,
        "epoch": 1.466,
        "step": 10995
    },
    {
        "loss": 0.9648,
        "grad_norm": 3.987077474594116,
        "learning_rate": 0.0001815137922846477,
        "epoch": 1.4661333333333333,
        "step": 10996
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.8218042850494385,
        "learning_rate": 0.0001814773189670266,
        "epoch": 1.4662666666666666,
        "step": 10997
    },
    {
        "loss": 2.8036,
        "grad_norm": 3.5866212844848633,
        "learning_rate": 0.0001814408133759618,
        "epoch": 1.4664,
        "step": 10998
    },
    {
        "loss": 1.88,
        "grad_norm": 4.50319242477417,
        "learning_rate": 0.00018140427552591323,
        "epoch": 1.4665333333333335,
        "step": 10999
    },
    {
        "loss": 1.9056,
        "grad_norm": 7.04489803314209,
        "learning_rate": 0.00018136770543135383,
        "epoch": 1.4666666666666668,
        "step": 11000
    },
    {
        "loss": 2.6071,
        "grad_norm": 3.337258815765381,
        "learning_rate": 0.0001813311031067691,
        "epoch": 1.4668,
        "step": 11001
    },
    {
        "loss": 1.7543,
        "grad_norm": 3.6942434310913086,
        "learning_rate": 0.00018129446856665724,
        "epoch": 1.4669333333333334,
        "step": 11002
    },
    {
        "loss": 2.589,
        "grad_norm": 2.7913882732391357,
        "learning_rate": 0.00018125780182552933,
        "epoch": 1.4670666666666667,
        "step": 11003
    },
    {
        "loss": 2.1022,
        "grad_norm": 4.362461090087891,
        "learning_rate": 0.00018122110289790931,
        "epoch": 1.4672,
        "step": 11004
    },
    {
        "loss": 2.7962,
        "grad_norm": 3.4130074977874756,
        "learning_rate": 0.0001811843717983336,
        "epoch": 1.4673333333333334,
        "step": 11005
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.7984468936920166,
        "learning_rate": 0.00018114760854135174,
        "epoch": 1.4674666666666667,
        "step": 11006
    },
    {
        "loss": 1.2623,
        "grad_norm": 3.74809193611145,
        "learning_rate": 0.0001811108131415255,
        "epoch": 1.4676,
        "step": 11007
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.4940123558044434,
        "learning_rate": 0.00018107398561342976,
        "epoch": 1.4677333333333333,
        "step": 11008
    },
    {
        "loss": 1.4159,
        "grad_norm": 2.9051718711853027,
        "learning_rate": 0.00018103712597165216,
        "epoch": 1.4678666666666667,
        "step": 11009
    },
    {
        "loss": 2.3693,
        "grad_norm": 5.164948463439941,
        "learning_rate": 0.00018100023423079286,
        "epoch": 1.468,
        "step": 11010
    },
    {
        "loss": 2.2455,
        "grad_norm": 2.4860422611236572,
        "learning_rate": 0.00018096331040546475,
        "epoch": 1.4681333333333333,
        "step": 11011
    },
    {
        "loss": 1.5549,
        "grad_norm": 3.379413604736328,
        "learning_rate": 0.00018092635451029344,
        "epoch": 1.4682666666666666,
        "step": 11012
    },
    {
        "loss": 2.7148,
        "grad_norm": 5.096798896789551,
        "learning_rate": 0.00018088936655991745,
        "epoch": 1.4684,
        "step": 11013
    },
    {
        "loss": 1.4736,
        "grad_norm": 4.321845531463623,
        "learning_rate": 0.00018085234656898774,
        "epoch": 1.4685333333333332,
        "step": 11014
    },
    {
        "loss": 2.3807,
        "grad_norm": 3.0204918384552,
        "learning_rate": 0.00018081529455216803,
        "epoch": 1.4686666666666666,
        "step": 11015
    },
    {
        "loss": 2.1459,
        "grad_norm": 3.9853734970092773,
        "learning_rate": 0.00018077821052413473,
        "epoch": 1.4687999999999999,
        "step": 11016
    },
    {
        "loss": 0.7152,
        "grad_norm": 3.105961799621582,
        "learning_rate": 0.00018074109449957697,
        "epoch": 1.4689333333333332,
        "step": 11017
    },
    {
        "loss": 2.4918,
        "grad_norm": 3.6845507621765137,
        "learning_rate": 0.00018070394649319667,
        "epoch": 1.4690666666666667,
        "step": 11018
    },
    {
        "loss": 2.3683,
        "grad_norm": 3.144453525543213,
        "learning_rate": 0.00018066676651970815,
        "epoch": 1.4692,
        "step": 11019
    },
    {
        "loss": 2.3992,
        "grad_norm": 3.161086082458496,
        "learning_rate": 0.00018062955459383857,
        "epoch": 1.4693333333333334,
        "step": 11020
    },
    {
        "loss": 2.4618,
        "grad_norm": 3.8608202934265137,
        "learning_rate": 0.00018059231073032757,
        "epoch": 1.4694666666666667,
        "step": 11021
    },
    {
        "loss": 2.4077,
        "grad_norm": 4.502967357635498,
        "learning_rate": 0.00018055503494392776,
        "epoch": 1.4696,
        "step": 11022
    },
    {
        "loss": 2.3492,
        "grad_norm": 3.1462674140930176,
        "learning_rate": 0.0001805177272494041,
        "epoch": 1.4697333333333333,
        "step": 11023
    },
    {
        "loss": 1.8401,
        "grad_norm": 5.054120063781738,
        "learning_rate": 0.00018048038766153436,
        "epoch": 1.4698666666666667,
        "step": 11024
    },
    {
        "loss": 2.6054,
        "grad_norm": 3.5396695137023926,
        "learning_rate": 0.00018044301619510878,
        "epoch": 1.47,
        "step": 11025
    },
    {
        "loss": 1.5545,
        "grad_norm": 3.422358751296997,
        "learning_rate": 0.00018040561286493035,
        "epoch": 1.4701333333333333,
        "step": 11026
    },
    {
        "loss": 1.6486,
        "grad_norm": 3.828653335571289,
        "learning_rate": 0.00018036817768581482,
        "epoch": 1.4702666666666666,
        "step": 11027
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.807013750076294,
        "learning_rate": 0.00018033071067259014,
        "epoch": 1.4704,
        "step": 11028
    },
    {
        "loss": 2.1585,
        "grad_norm": 3.5233006477355957,
        "learning_rate": 0.00018029321184009732,
        "epoch": 1.4705333333333335,
        "step": 11029
    },
    {
        "loss": 2.9574,
        "grad_norm": 2.28745174407959,
        "learning_rate": 0.00018025568120318957,
        "epoch": 1.4706666666666668,
        "step": 11030
    },
    {
        "loss": 0.7347,
        "grad_norm": 3.658504009246826,
        "learning_rate": 0.00018021811877673316,
        "epoch": 1.4708,
        "step": 11031
    },
    {
        "loss": 2.2518,
        "grad_norm": 2.7916529178619385,
        "learning_rate": 0.00018018052457560656,
        "epoch": 1.4709333333333334,
        "step": 11032
    },
    {
        "loss": 2.6946,
        "grad_norm": 4.01380729675293,
        "learning_rate": 0.00018014289861470099,
        "epoch": 1.4710666666666667,
        "step": 11033
    },
    {
        "loss": 2.5036,
        "grad_norm": 3.8489763736724854,
        "learning_rate": 0.00018010524090892008,
        "epoch": 1.4712,
        "step": 11034
    },
    {
        "loss": 2.103,
        "grad_norm": 3.7590134143829346,
        "learning_rate": 0.00018006755147318036,
        "epoch": 1.4713333333333334,
        "step": 11035
    },
    {
        "loss": 2.9789,
        "grad_norm": 5.22633695602417,
        "learning_rate": 0.00018002983032241072,
        "epoch": 1.4714666666666667,
        "step": 11036
    },
    {
        "loss": 2.7105,
        "grad_norm": 2.817814588546753,
        "learning_rate": 0.0001799920774715525,
        "epoch": 1.4716,
        "step": 11037
    },
    {
        "loss": 1.9461,
        "grad_norm": 4.148972511291504,
        "learning_rate": 0.0001799542929355599,
        "epoch": 1.4717333333333333,
        "step": 11038
    },
    {
        "loss": 3.1849,
        "grad_norm": 4.895856857299805,
        "learning_rate": 0.00017991647672939935,
        "epoch": 1.4718666666666667,
        "step": 11039
    },
    {
        "loss": 2.7066,
        "grad_norm": 3.3009233474731445,
        "learning_rate": 0.00017987862886805015,
        "epoch": 1.472,
        "step": 11040
    },
    {
        "loss": 1.1874,
        "grad_norm": 4.437134265899658,
        "learning_rate": 0.00017984074936650387,
        "epoch": 1.4721333333333333,
        "step": 11041
    },
    {
        "loss": 0.5707,
        "grad_norm": 3.603475332260132,
        "learning_rate": 0.00017980283823976472,
        "epoch": 1.4722666666666666,
        "step": 11042
    },
    {
        "loss": 1.9208,
        "grad_norm": 4.082946300506592,
        "learning_rate": 0.00017976489550284935,
        "epoch": 1.4724,
        "step": 11043
    },
    {
        "loss": 2.764,
        "grad_norm": 4.523796081542969,
        "learning_rate": 0.00017972692117078714,
        "epoch": 1.4725333333333332,
        "step": 11044
    },
    {
        "loss": 2.6507,
        "grad_norm": 3.922783851623535,
        "learning_rate": 0.00017968891525861977,
        "epoch": 1.4726666666666666,
        "step": 11045
    },
    {
        "loss": 2.3698,
        "grad_norm": 2.422842025756836,
        "learning_rate": 0.0001796508777814015,
        "epoch": 1.4727999999999999,
        "step": 11046
    },
    {
        "loss": 2.1458,
        "grad_norm": 4.79912805557251,
        "learning_rate": 0.0001796128087541992,
        "epoch": 1.4729333333333332,
        "step": 11047
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.6179609298706055,
        "learning_rate": 0.00017957470819209204,
        "epoch": 1.4730666666666667,
        "step": 11048
    },
    {
        "loss": 1.6815,
        "grad_norm": 4.122889995574951,
        "learning_rate": 0.0001795365761101718,
        "epoch": 1.4732,
        "step": 11049
    },
    {
        "loss": 1.7701,
        "grad_norm": 4.552428722381592,
        "learning_rate": 0.0001794984125235428,
        "epoch": 1.4733333333333334,
        "step": 11050
    },
    {
        "loss": 2.1429,
        "grad_norm": 3.6331565380096436,
        "learning_rate": 0.0001794602174473217,
        "epoch": 1.4734666666666667,
        "step": 11051
    },
    {
        "loss": 0.6041,
        "grad_norm": 2.7945616245269775,
        "learning_rate": 0.00017942199089663775,
        "epoch": 1.4736,
        "step": 11052
    },
    {
        "loss": 2.375,
        "grad_norm": 3.008601427078247,
        "learning_rate": 0.00017938373288663249,
        "epoch": 1.4737333333333333,
        "step": 11053
    },
    {
        "loss": 1.2779,
        "grad_norm": 5.230085849761963,
        "learning_rate": 0.00017934544343246024,
        "epoch": 1.4738666666666667,
        "step": 11054
    },
    {
        "loss": 2.6853,
        "grad_norm": 3.9277830123901367,
        "learning_rate": 0.00017930712254928736,
        "epoch": 1.474,
        "step": 11055
    },
    {
        "loss": 2.457,
        "grad_norm": 4.995230674743652,
        "learning_rate": 0.0001792687702522931,
        "epoch": 1.4741333333333333,
        "step": 11056
    },
    {
        "loss": 2.545,
        "grad_norm": 3.094165086746216,
        "learning_rate": 0.00017923038655666887,
        "epoch": 1.4742666666666666,
        "step": 11057
    },
    {
        "loss": 2.7215,
        "grad_norm": 5.137253284454346,
        "learning_rate": 0.00017919197147761841,
        "epoch": 1.4744,
        "step": 11058
    },
    {
        "loss": 2.4906,
        "grad_norm": 3.9830737113952637,
        "learning_rate": 0.00017915352503035834,
        "epoch": 1.4745333333333333,
        "step": 11059
    },
    {
        "loss": 1.4272,
        "grad_norm": 3.6750755310058594,
        "learning_rate": 0.0001791150472301173,
        "epoch": 1.4746666666666668,
        "step": 11060
    },
    {
        "loss": 1.9681,
        "grad_norm": 3.5527150630950928,
        "learning_rate": 0.00017907653809213646,
        "epoch": 1.4748,
        "step": 11061
    },
    {
        "loss": 2.3059,
        "grad_norm": 4.404359817504883,
        "learning_rate": 0.00017903799763166936,
        "epoch": 1.4749333333333334,
        "step": 11062
    },
    {
        "loss": 1.6786,
        "grad_norm": 5.2264790534973145,
        "learning_rate": 0.00017899942586398215,
        "epoch": 1.4750666666666667,
        "step": 11063
    },
    {
        "loss": 2.6171,
        "grad_norm": 3.8119957447052,
        "learning_rate": 0.00017896082280435311,
        "epoch": 1.4752,
        "step": 11064
    },
    {
        "loss": 2.7928,
        "grad_norm": 4.514828681945801,
        "learning_rate": 0.00017892218846807324,
        "epoch": 1.4753333333333334,
        "step": 11065
    },
    {
        "loss": 1.9786,
        "grad_norm": 3.320845603942871,
        "learning_rate": 0.00017888352287044544,
        "epoch": 1.4754666666666667,
        "step": 11066
    },
    {
        "loss": 1.0161,
        "grad_norm": 3.7609591484069824,
        "learning_rate": 0.00017884482602678546,
        "epoch": 1.4756,
        "step": 11067
    },
    {
        "loss": 0.4873,
        "grad_norm": 2.595163106918335,
        "learning_rate": 0.00017880609795242132,
        "epoch": 1.4757333333333333,
        "step": 11068
    },
    {
        "loss": 2.5156,
        "grad_norm": 4.332775592803955,
        "learning_rate": 0.00017876733866269324,
        "epoch": 1.4758666666666667,
        "step": 11069
    },
    {
        "loss": 2.7412,
        "grad_norm": 2.8998589515686035,
        "learning_rate": 0.00017872854817295393,
        "epoch": 1.476,
        "step": 11070
    },
    {
        "loss": 2.3447,
        "grad_norm": 3.569281816482544,
        "learning_rate": 0.00017868972649856833,
        "epoch": 1.4761333333333333,
        "step": 11071
    },
    {
        "loss": 3.3338,
        "grad_norm": 2.4221901893615723,
        "learning_rate": 0.00017865087365491404,
        "epoch": 1.4762666666666666,
        "step": 11072
    },
    {
        "loss": 1.5988,
        "grad_norm": 3.0242726802825928,
        "learning_rate": 0.0001786119896573807,
        "epoch": 1.4764,
        "step": 11073
    },
    {
        "loss": 2.4645,
        "grad_norm": 4.053656578063965,
        "learning_rate": 0.00017857307452137037,
        "epoch": 1.4765333333333333,
        "step": 11074
    },
    {
        "loss": 1.5957,
        "grad_norm": 3.891808271408081,
        "learning_rate": 0.0001785341282622974,
        "epoch": 1.4766666666666666,
        "step": 11075
    },
    {
        "loss": 1.7874,
        "grad_norm": 3.4555201530456543,
        "learning_rate": 0.00017849515089558864,
        "epoch": 1.4768,
        "step": 11076
    },
    {
        "loss": 2.3373,
        "grad_norm": 3.9594433307647705,
        "learning_rate": 0.00017845614243668326,
        "epoch": 1.4769333333333332,
        "step": 11077
    },
    {
        "loss": 1.752,
        "grad_norm": 4.481871128082275,
        "learning_rate": 0.0001784171029010325,
        "epoch": 1.4770666666666667,
        "step": 11078
    },
    {
        "loss": 2.6266,
        "grad_norm": 3.897915840148926,
        "learning_rate": 0.00017837803230410007,
        "epoch": 1.4772,
        "step": 11079
    },
    {
        "loss": 1.4363,
        "grad_norm": 6.5339837074279785,
        "learning_rate": 0.0001783389306613619,
        "epoch": 1.4773333333333334,
        "step": 11080
    },
    {
        "loss": 2.4386,
        "grad_norm": 3.7788782119750977,
        "learning_rate": 0.00017829979798830644,
        "epoch": 1.4774666666666667,
        "step": 11081
    },
    {
        "loss": 2.3604,
        "grad_norm": 5.6310133934021,
        "learning_rate": 0.00017826063430043422,
        "epoch": 1.4776,
        "step": 11082
    },
    {
        "loss": 2.8278,
        "grad_norm": 4.46317195892334,
        "learning_rate": 0.00017822143961325807,
        "epoch": 1.4777333333333333,
        "step": 11083
    },
    {
        "loss": 2.3734,
        "grad_norm": 3.224557638168335,
        "learning_rate": 0.0001781822139423031,
        "epoch": 1.4778666666666667,
        "step": 11084
    },
    {
        "loss": 2.2124,
        "grad_norm": 3.4353020191192627,
        "learning_rate": 0.00017814295730310677,
        "epoch": 1.478,
        "step": 11085
    },
    {
        "loss": 1.9842,
        "grad_norm": 3.773838758468628,
        "learning_rate": 0.00017810366971121895,
        "epoch": 1.4781333333333333,
        "step": 11086
    },
    {
        "loss": 1.8694,
        "grad_norm": 3.0454070568084717,
        "learning_rate": 0.00017806435118220123,
        "epoch": 1.4782666666666666,
        "step": 11087
    },
    {
        "loss": 1.0082,
        "grad_norm": 4.068000793457031,
        "learning_rate": 0.00017802500173162814,
        "epoch": 1.4784,
        "step": 11088
    },
    {
        "loss": 2.3353,
        "grad_norm": 4.255163669586182,
        "learning_rate": 0.0001779856213750859,
        "epoch": 1.4785333333333333,
        "step": 11089
    },
    {
        "loss": 1.5967,
        "grad_norm": 4.1376848220825195,
        "learning_rate": 0.00017794621012817339,
        "epoch": 1.4786666666666668,
        "step": 11090
    },
    {
        "loss": 1.4073,
        "grad_norm": 4.893722057342529,
        "learning_rate": 0.00017790676800650147,
        "epoch": 1.4788000000000001,
        "step": 11091
    },
    {
        "loss": 1.4335,
        "grad_norm": 5.543796539306641,
        "learning_rate": 0.00017786729502569323,
        "epoch": 1.4789333333333334,
        "step": 11092
    },
    {
        "loss": 1.972,
        "grad_norm": 3.985131025314331,
        "learning_rate": 0.00017782779120138406,
        "epoch": 1.4790666666666668,
        "step": 11093
    },
    {
        "loss": 2.6464,
        "grad_norm": 3.6073355674743652,
        "learning_rate": 0.00017778825654922166,
        "epoch": 1.4792,
        "step": 11094
    },
    {
        "loss": 1.9313,
        "grad_norm": 4.112011432647705,
        "learning_rate": 0.0001777486910848658,
        "epoch": 1.4793333333333334,
        "step": 11095
    },
    {
        "loss": 2.1477,
        "grad_norm": 4.214001655578613,
        "learning_rate": 0.00017770909482398844,
        "epoch": 1.4794666666666667,
        "step": 11096
    },
    {
        "loss": 1.9356,
        "grad_norm": 3.3320069313049316,
        "learning_rate": 0.00017766946778227384,
        "epoch": 1.4796,
        "step": 11097
    },
    {
        "loss": 2.0047,
        "grad_norm": 5.758961200714111,
        "learning_rate": 0.00017762980997541836,
        "epoch": 1.4797333333333333,
        "step": 11098
    },
    {
        "loss": 1.2918,
        "grad_norm": 4.340188980102539,
        "learning_rate": 0.00017759012141913073,
        "epoch": 1.4798666666666667,
        "step": 11099
    },
    {
        "loss": 1.9005,
        "grad_norm": 2.963637351989746,
        "learning_rate": 0.00017755040212913157,
        "epoch": 1.48,
        "step": 11100
    },
    {
        "loss": 2.5857,
        "grad_norm": 4.124686241149902,
        "learning_rate": 0.0001775106521211539,
        "epoch": 1.4801333333333333,
        "step": 11101
    },
    {
        "loss": 2.6667,
        "grad_norm": 3.6841580867767334,
        "learning_rate": 0.00017747087141094277,
        "epoch": 1.4802666666666666,
        "step": 11102
    },
    {
        "loss": 2.3081,
        "grad_norm": 2.9684934616088867,
        "learning_rate": 0.00017743106001425553,
        "epoch": 1.4804,
        "step": 11103
    },
    {
        "loss": 0.884,
        "grad_norm": 4.232602596282959,
        "learning_rate": 0.0001773912179468616,
        "epoch": 1.4805333333333333,
        "step": 11104
    },
    {
        "loss": 2.362,
        "grad_norm": 4.623129844665527,
        "learning_rate": 0.00017735134522454246,
        "epoch": 1.4806666666666666,
        "step": 11105
    },
    {
        "loss": 1.5211,
        "grad_norm": 2.5640649795532227,
        "learning_rate": 0.000177311441863092,
        "epoch": 1.4808,
        "step": 11106
    },
    {
        "loss": 1.2974,
        "grad_norm": 3.5788142681121826,
        "learning_rate": 0.00017727150787831596,
        "epoch": 1.4809333333333332,
        "step": 11107
    },
    {
        "loss": 2.0089,
        "grad_norm": 3.5867202281951904,
        "learning_rate": 0.00017723154328603226,
        "epoch": 1.4810666666666665,
        "step": 11108
    },
    {
        "loss": 2.1502,
        "grad_norm": 3.267542839050293,
        "learning_rate": 0.0001771915481020712,
        "epoch": 1.4812,
        "step": 11109
    },
    {
        "loss": 1.7299,
        "grad_norm": 3.877268075942993,
        "learning_rate": 0.00017715152234227494,
        "epoch": 1.4813333333333334,
        "step": 11110
    },
    {
        "loss": 1.4318,
        "grad_norm": 4.415822505950928,
        "learning_rate": 0.00017711146602249778,
        "epoch": 1.4814666666666667,
        "step": 11111
    },
    {
        "loss": 1.4214,
        "grad_norm": 3.693983793258667,
        "learning_rate": 0.0001770713791586061,
        "epoch": 1.4816,
        "step": 11112
    },
    {
        "loss": 2.0942,
        "grad_norm": 2.9498462677001953,
        "learning_rate": 0.00017703126176647858,
        "epoch": 1.4817333333333333,
        "step": 11113
    },
    {
        "loss": 2.2629,
        "grad_norm": 3.009728193283081,
        "learning_rate": 0.00017699111386200572,
        "epoch": 1.4818666666666667,
        "step": 11114
    },
    {
        "loss": 2.5891,
        "grad_norm": 3.3874521255493164,
        "learning_rate": 0.0001769509354610905,
        "epoch": 1.482,
        "step": 11115
    },
    {
        "loss": 2.613,
        "grad_norm": 3.7843246459960938,
        "learning_rate": 0.00017691072657964737,
        "epoch": 1.4821333333333333,
        "step": 11116
    },
    {
        "loss": 1.3597,
        "grad_norm": 2.680529832839966,
        "learning_rate": 0.00017687048723360335,
        "epoch": 1.4822666666666666,
        "step": 11117
    },
    {
        "loss": 2.4249,
        "grad_norm": 3.0939881801605225,
        "learning_rate": 0.0001768302174388975,
        "epoch": 1.4824,
        "step": 11118
    },
    {
        "loss": 2.2904,
        "grad_norm": 3.579571008682251,
        "learning_rate": 0.00017678991721148076,
        "epoch": 1.4825333333333333,
        "step": 11119
    },
    {
        "loss": 2.4032,
        "grad_norm": 5.823729038238525,
        "learning_rate": 0.00017674958656731615,
        "epoch": 1.4826666666666668,
        "step": 11120
    },
    {
        "loss": 2.3823,
        "grad_norm": 4.6030755043029785,
        "learning_rate": 0.00017670922552237868,
        "epoch": 1.4828000000000001,
        "step": 11121
    },
    {
        "loss": 1.655,
        "grad_norm": 3.9621591567993164,
        "learning_rate": 0.0001766688340926557,
        "epoch": 1.4829333333333334,
        "step": 11122
    },
    {
        "loss": 2.5162,
        "grad_norm": 4.065162658691406,
        "learning_rate": 0.00017662841229414616,
        "epoch": 1.4830666666666668,
        "step": 11123
    },
    {
        "loss": 1.1513,
        "grad_norm": 5.561079978942871,
        "learning_rate": 0.00017658796014286164,
        "epoch": 1.4832,
        "step": 11124
    },
    {
        "loss": 2.4916,
        "grad_norm": 4.776193141937256,
        "learning_rate": 0.00017654747765482494,
        "epoch": 1.4833333333333334,
        "step": 11125
    },
    {
        "loss": 2.2555,
        "grad_norm": 3.8694207668304443,
        "learning_rate": 0.00017650696484607152,
        "epoch": 1.4834666666666667,
        "step": 11126
    },
    {
        "loss": 3.0498,
        "grad_norm": 3.5691356658935547,
        "learning_rate": 0.0001764664217326487,
        "epoch": 1.4836,
        "step": 11127
    },
    {
        "loss": 2.4885,
        "grad_norm": 3.5362038612365723,
        "learning_rate": 0.00017642584833061574,
        "epoch": 1.4837333333333333,
        "step": 11128
    },
    {
        "loss": 1.9914,
        "grad_norm": 3.574831247329712,
        "learning_rate": 0.00017638524465604376,
        "epoch": 1.4838666666666667,
        "step": 11129
    },
    {
        "loss": 2.2891,
        "grad_norm": 4.229552745819092,
        "learning_rate": 0.00017634461072501604,
        "epoch": 1.484,
        "step": 11130
    },
    {
        "loss": 1.993,
        "grad_norm": 3.888082981109619,
        "learning_rate": 0.00017630394655362798,
        "epoch": 1.4841333333333333,
        "step": 11131
    },
    {
        "loss": 2.1491,
        "grad_norm": 5.322718620300293,
        "learning_rate": 0.0001762632521579867,
        "epoch": 1.4842666666666666,
        "step": 11132
    },
    {
        "loss": 2.1724,
        "grad_norm": 4.264313697814941,
        "learning_rate": 0.00017622252755421133,
        "epoch": 1.4844,
        "step": 11133
    },
    {
        "loss": 1.668,
        "grad_norm": 4.287731647491455,
        "learning_rate": 0.000176181772758433,
        "epoch": 1.4845333333333333,
        "step": 11134
    },
    {
        "loss": 2.311,
        "grad_norm": 3.6182591915130615,
        "learning_rate": 0.0001761409877867949,
        "epoch": 1.4846666666666666,
        "step": 11135
    },
    {
        "loss": 1.6502,
        "grad_norm": 4.827892303466797,
        "learning_rate": 0.00017610017265545232,
        "epoch": 1.4848,
        "step": 11136
    },
    {
        "loss": 2.3086,
        "grad_norm": 2.8432676792144775,
        "learning_rate": 0.00017605932738057182,
        "epoch": 1.4849333333333332,
        "step": 11137
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.252887725830078,
        "learning_rate": 0.00017601845197833267,
        "epoch": 1.4850666666666665,
        "step": 11138
    },
    {
        "loss": 1.4187,
        "grad_norm": 5.28987455368042,
        "learning_rate": 0.0001759775464649256,
        "epoch": 1.4852,
        "step": 11139
    },
    {
        "loss": 2.5579,
        "grad_norm": 4.186004638671875,
        "learning_rate": 0.00017593661085655354,
        "epoch": 1.4853333333333334,
        "step": 11140
    },
    {
        "loss": 2.8061,
        "grad_norm": 4.007123947143555,
        "learning_rate": 0.00017589564516943117,
        "epoch": 1.4854666666666667,
        "step": 11141
    },
    {
        "loss": 2.3535,
        "grad_norm": 3.166195869445801,
        "learning_rate": 0.00017585464941978517,
        "epoch": 1.4856,
        "step": 11142
    },
    {
        "loss": 2.8622,
        "grad_norm": 4.307115077972412,
        "learning_rate": 0.00017581362362385398,
        "epoch": 1.4857333333333334,
        "step": 11143
    },
    {
        "loss": 2.4777,
        "grad_norm": 3.0396673679351807,
        "learning_rate": 0.00017577256779788814,
        "epoch": 1.4858666666666667,
        "step": 11144
    },
    {
        "loss": 1.7037,
        "grad_norm": 4.076451778411865,
        "learning_rate": 0.00017573148195815016,
        "epoch": 1.486,
        "step": 11145
    },
    {
        "loss": 2.8744,
        "grad_norm": 3.387768268585205,
        "learning_rate": 0.000175690366120914,
        "epoch": 1.4861333333333333,
        "step": 11146
    },
    {
        "loss": 1.9376,
        "grad_norm": 2.9475603103637695,
        "learning_rate": 0.00017564922030246597,
        "epoch": 1.4862666666666666,
        "step": 11147
    },
    {
        "loss": 1.3869,
        "grad_norm": 5.494629859924316,
        "learning_rate": 0.0001756080445191039,
        "epoch": 1.4864,
        "step": 11148
    },
    {
        "loss": 2.4582,
        "grad_norm": 4.687304973602295,
        "learning_rate": 0.00017556683878713786,
        "epoch": 1.4865333333333333,
        "step": 11149
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.438976526260376,
        "learning_rate": 0.00017552560312288952,
        "epoch": 1.4866666666666668,
        "step": 11150
    },
    {
        "loss": 1.9764,
        "grad_norm": 3.653303384780884,
        "learning_rate": 0.0001754843375426924,
        "epoch": 1.4868000000000001,
        "step": 11151
    },
    {
        "loss": 2.617,
        "grad_norm": 2.699842929840088,
        "learning_rate": 0.00017544304206289191,
        "epoch": 1.4869333333333334,
        "step": 11152
    },
    {
        "loss": 2.1412,
        "grad_norm": 3.4723165035247803,
        "learning_rate": 0.0001754017166998455,
        "epoch": 1.4870666666666668,
        "step": 11153
    },
    {
        "loss": 2.353,
        "grad_norm": 4.06901216506958,
        "learning_rate": 0.00017536036146992214,
        "epoch": 1.4872,
        "step": 11154
    },
    {
        "loss": 1.7529,
        "grad_norm": 5.105721473693848,
        "learning_rate": 0.00017531897638950274,
        "epoch": 1.4873333333333334,
        "step": 11155
    },
    {
        "loss": 2.4164,
        "grad_norm": 3.8776698112487793,
        "learning_rate": 0.00017527756147498025,
        "epoch": 1.4874666666666667,
        "step": 11156
    },
    {
        "loss": 2.3569,
        "grad_norm": 4.490899085998535,
        "learning_rate": 0.00017523611674275907,
        "epoch": 1.4876,
        "step": 11157
    },
    {
        "loss": 2.3018,
        "grad_norm": 3.1243057250976562,
        "learning_rate": 0.00017519464220925574,
        "epoch": 1.4877333333333334,
        "step": 11158
    },
    {
        "loss": 2.6452,
        "grad_norm": 2.8257906436920166,
        "learning_rate": 0.00017515313789089846,
        "epoch": 1.4878666666666667,
        "step": 11159
    },
    {
        "loss": 1.4476,
        "grad_norm": 4.185365676879883,
        "learning_rate": 0.00017511160380412715,
        "epoch": 1.488,
        "step": 11160
    },
    {
        "loss": 1.6788,
        "grad_norm": 4.3868842124938965,
        "learning_rate": 0.00017507003996539356,
        "epoch": 1.4881333333333333,
        "step": 11161
    },
    {
        "loss": 2.101,
        "grad_norm": 4.887244701385498,
        "learning_rate": 0.00017502844639116143,
        "epoch": 1.4882666666666666,
        "step": 11162
    },
    {
        "loss": 2.2891,
        "grad_norm": 2.7989540100097656,
        "learning_rate": 0.00017498682309790604,
        "epoch": 1.4884,
        "step": 11163
    },
    {
        "loss": 1.7667,
        "grad_norm": 5.8482770919799805,
        "learning_rate": 0.0001749451701021144,
        "epoch": 1.4885333333333333,
        "step": 11164
    },
    {
        "loss": 2.403,
        "grad_norm": 4.770418643951416,
        "learning_rate": 0.00017490348742028563,
        "epoch": 1.4886666666666666,
        "step": 11165
    },
    {
        "loss": 0.6026,
        "grad_norm": 2.922428607940674,
        "learning_rate": 0.00017486177506893027,
        "epoch": 1.4888,
        "step": 11166
    },
    {
        "loss": 2.3488,
        "grad_norm": 3.6863603591918945,
        "learning_rate": 0.0001748200330645706,
        "epoch": 1.4889333333333332,
        "step": 11167
    },
    {
        "loss": 2.3294,
        "grad_norm": 4.125959873199463,
        "learning_rate": 0.000174778261423741,
        "epoch": 1.4890666666666665,
        "step": 11168
    },
    {
        "loss": 1.8109,
        "grad_norm": 3.6523663997650146,
        "learning_rate": 0.00017473646016298724,
        "epoch": 1.4892,
        "step": 11169
    },
    {
        "loss": 1.1862,
        "grad_norm": 3.3930628299713135,
        "learning_rate": 0.00017469462929886695,
        "epoch": 1.4893333333333334,
        "step": 11170
    },
    {
        "loss": 2.5784,
        "grad_norm": 3.3408291339874268,
        "learning_rate": 0.00017465276884794938,
        "epoch": 1.4894666666666667,
        "step": 11171
    },
    {
        "loss": 2.551,
        "grad_norm": 5.078415393829346,
        "learning_rate": 0.00017461087882681583,
        "epoch": 1.4896,
        "step": 11172
    },
    {
        "loss": 2.0955,
        "grad_norm": 5.133977890014648,
        "learning_rate": 0.0001745689592520588,
        "epoch": 1.4897333333333334,
        "step": 11173
    },
    {
        "loss": 1.9193,
        "grad_norm": 4.797346115112305,
        "learning_rate": 0.00017452701014028312,
        "epoch": 1.4898666666666667,
        "step": 11174
    },
    {
        "loss": 2.4844,
        "grad_norm": 3.3131017684936523,
        "learning_rate": 0.00017448503150810463,
        "epoch": 1.49,
        "step": 11175
    },
    {
        "loss": 2.4034,
        "grad_norm": 4.124515056610107,
        "learning_rate": 0.00017444302337215134,
        "epoch": 1.4901333333333333,
        "step": 11176
    },
    {
        "loss": 0.5863,
        "grad_norm": 3.667189121246338,
        "learning_rate": 0.0001744009857490629,
        "epoch": 1.4902666666666666,
        "step": 11177
    },
    {
        "loss": 2.3665,
        "grad_norm": 4.422553539276123,
        "learning_rate": 0.0001743589186554905,
        "epoch": 1.4904,
        "step": 11178
    },
    {
        "loss": 2.2939,
        "grad_norm": 2.890794515609741,
        "learning_rate": 0.00017431682210809708,
        "epoch": 1.4905333333333333,
        "step": 11179
    },
    {
        "loss": 2.0417,
        "grad_norm": 4.868936061859131,
        "learning_rate": 0.00017427469612355708,
        "epoch": 1.4906666666666666,
        "step": 11180
    },
    {
        "loss": 2.4639,
        "grad_norm": 4.532962322235107,
        "learning_rate": 0.00017423254071855695,
        "epoch": 1.4908000000000001,
        "step": 11181
    },
    {
        "loss": 2.6203,
        "grad_norm": 3.1598424911499023,
        "learning_rate": 0.00017419035590979443,
        "epoch": 1.4909333333333334,
        "step": 11182
    },
    {
        "loss": 2.2731,
        "grad_norm": 4.659826755523682,
        "learning_rate": 0.00017414814171397928,
        "epoch": 1.4910666666666668,
        "step": 11183
    },
    {
        "loss": 2.4615,
        "grad_norm": 2.358522891998291,
        "learning_rate": 0.0001741058981478324,
        "epoch": 1.4912,
        "step": 11184
    },
    {
        "loss": 2.7998,
        "grad_norm": 2.7391490936279297,
        "learning_rate": 0.00017406362522808671,
        "epoch": 1.4913333333333334,
        "step": 11185
    },
    {
        "loss": 2.8421,
        "grad_norm": 4.692783355712891,
        "learning_rate": 0.00017402132297148683,
        "epoch": 1.4914666666666667,
        "step": 11186
    },
    {
        "loss": 2.227,
        "grad_norm": 3.0545475482940674,
        "learning_rate": 0.00017397899139478867,
        "epoch": 1.4916,
        "step": 11187
    },
    {
        "loss": 1.0622,
        "grad_norm": 5.4539079666137695,
        "learning_rate": 0.00017393663051475997,
        "epoch": 1.4917333333333334,
        "step": 11188
    },
    {
        "loss": 2.5171,
        "grad_norm": 5.056539058685303,
        "learning_rate": 0.0001738942403481799,
        "epoch": 1.4918666666666667,
        "step": 11189
    },
    {
        "loss": 1.2383,
        "grad_norm": 5.197195529937744,
        "learning_rate": 0.00017385182091183952,
        "epoch": 1.492,
        "step": 11190
    },
    {
        "loss": 2.3624,
        "grad_norm": 3.738063097000122,
        "learning_rate": 0.00017380937222254123,
        "epoch": 1.4921333333333333,
        "step": 11191
    },
    {
        "loss": 2.7901,
        "grad_norm": 4.161841869354248,
        "learning_rate": 0.0001737668942970991,
        "epoch": 1.4922666666666666,
        "step": 11192
    },
    {
        "loss": 2.4,
        "grad_norm": 3.516677141189575,
        "learning_rate": 0.00017372438715233872,
        "epoch": 1.4924,
        "step": 11193
    },
    {
        "loss": 2.1657,
        "grad_norm": 6.425106048583984,
        "learning_rate": 0.00017368185080509735,
        "epoch": 1.4925333333333333,
        "step": 11194
    },
    {
        "loss": 2.2834,
        "grad_norm": 2.8318817615509033,
        "learning_rate": 0.00017363928527222398,
        "epoch": 1.4926666666666666,
        "step": 11195
    },
    {
        "loss": 2.519,
        "grad_norm": 3.669135093688965,
        "learning_rate": 0.00017359669057057862,
        "epoch": 1.4928,
        "step": 11196
    },
    {
        "loss": 2.1236,
        "grad_norm": 3.4026472568511963,
        "learning_rate": 0.00017355406671703348,
        "epoch": 1.4929333333333332,
        "step": 11197
    },
    {
        "loss": 2.3442,
        "grad_norm": 3.3465778827667236,
        "learning_rate": 0.00017351141372847178,
        "epoch": 1.4930666666666665,
        "step": 11198
    },
    {
        "loss": 3.0792,
        "grad_norm": 4.852260589599609,
        "learning_rate": 0.0001734687316217887,
        "epoch": 1.4932,
        "step": 11199
    },
    {
        "loss": 2.1565,
        "grad_norm": 3.921271800994873,
        "learning_rate": 0.00017342602041389067,
        "epoch": 1.4933333333333334,
        "step": 11200
    },
    {
        "loss": 1.8954,
        "grad_norm": 3.951845407485962,
        "learning_rate": 0.00017338328012169577,
        "epoch": 1.4934666666666667,
        "step": 11201
    },
    {
        "loss": 1.7512,
        "grad_norm": 3.879004716873169,
        "learning_rate": 0.00017334051076213348,
        "epoch": 1.4936,
        "step": 11202
    },
    {
        "loss": 2.5478,
        "grad_norm": 2.7666330337524414,
        "learning_rate": 0.00017329771235214498,
        "epoch": 1.4937333333333334,
        "step": 11203
    },
    {
        "loss": 1.5784,
        "grad_norm": 3.901453733444214,
        "learning_rate": 0.00017325488490868305,
        "epoch": 1.4938666666666667,
        "step": 11204
    },
    {
        "loss": 2.5172,
        "grad_norm": 4.963419437408447,
        "learning_rate": 0.0001732120284487114,
        "epoch": 1.494,
        "step": 11205
    },
    {
        "loss": 1.5755,
        "grad_norm": 5.209932327270508,
        "learning_rate": 0.0001731691429892059,
        "epoch": 1.4941333333333333,
        "step": 11206
    },
    {
        "loss": 2.7522,
        "grad_norm": 3.4485890865325928,
        "learning_rate": 0.0001731262285471535,
        "epoch": 1.4942666666666666,
        "step": 11207
    },
    {
        "loss": 1.5423,
        "grad_norm": 4.373403072357178,
        "learning_rate": 0.00017308328513955282,
        "epoch": 1.4944,
        "step": 11208
    },
    {
        "loss": 2.4715,
        "grad_norm": 4.423645496368408,
        "learning_rate": 0.00017304031278341393,
        "epoch": 1.4945333333333333,
        "step": 11209
    },
    {
        "loss": 1.8008,
        "grad_norm": 3.9902665615081787,
        "learning_rate": 0.00017299731149575823,
        "epoch": 1.4946666666666666,
        "step": 11210
    },
    {
        "loss": 1.8642,
        "grad_norm": 3.9620442390441895,
        "learning_rate": 0.00017295428129361867,
        "epoch": 1.4948000000000001,
        "step": 11211
    },
    {
        "loss": 2.389,
        "grad_norm": 4.660595417022705,
        "learning_rate": 0.00017291122219403974,
        "epoch": 1.4949333333333334,
        "step": 11212
    },
    {
        "loss": 1.6694,
        "grad_norm": 4.700321197509766,
        "learning_rate": 0.0001728681342140773,
        "epoch": 1.4950666666666668,
        "step": 11213
    },
    {
        "loss": 1.4224,
        "grad_norm": 4.397701740264893,
        "learning_rate": 0.0001728250173707985,
        "epoch": 1.4952,
        "step": 11214
    },
    {
        "loss": 2.9361,
        "grad_norm": 2.379492998123169,
        "learning_rate": 0.00017278187168128225,
        "epoch": 1.4953333333333334,
        "step": 11215
    },
    {
        "loss": 2.0374,
        "grad_norm": 3.277758836746216,
        "learning_rate": 0.00017273869716261865,
        "epoch": 1.4954666666666667,
        "step": 11216
    },
    {
        "loss": 1.2066,
        "grad_norm": 5.1055588722229,
        "learning_rate": 0.00017269549383190915,
        "epoch": 1.4956,
        "step": 11217
    },
    {
        "loss": 2.1695,
        "grad_norm": 3.842170476913452,
        "learning_rate": 0.00017265226170626695,
        "epoch": 1.4957333333333334,
        "step": 11218
    },
    {
        "loss": 1.783,
        "grad_norm": 2.9520554542541504,
        "learning_rate": 0.0001726090008028163,
        "epoch": 1.4958666666666667,
        "step": 11219
    },
    {
        "loss": 1.5101,
        "grad_norm": 2.9791274070739746,
        "learning_rate": 0.00017256571113869298,
        "epoch": 1.496,
        "step": 11220
    },
    {
        "loss": 2.3834,
        "grad_norm": 4.807342529296875,
        "learning_rate": 0.00017252239273104426,
        "epoch": 1.4961333333333333,
        "step": 11221
    },
    {
        "loss": 2.4088,
        "grad_norm": 4.659562110900879,
        "learning_rate": 0.0001724790455970287,
        "epoch": 1.4962666666666666,
        "step": 11222
    },
    {
        "loss": 2.7273,
        "grad_norm": 4.255544185638428,
        "learning_rate": 0.0001724356697538161,
        "epoch": 1.4964,
        "step": 11223
    },
    {
        "loss": 2.4283,
        "grad_norm": 3.4662320613861084,
        "learning_rate": 0.000172392265218588,
        "epoch": 1.4965333333333333,
        "step": 11224
    },
    {
        "loss": 1.7135,
        "grad_norm": 2.9509315490722656,
        "learning_rate": 0.00017234883200853695,
        "epoch": 1.4966666666666666,
        "step": 11225
    },
    {
        "loss": 3.0647,
        "grad_norm": 2.6379001140594482,
        "learning_rate": 0.00017230537014086694,
        "epoch": 1.4968,
        "step": 11226
    },
    {
        "loss": 2.1081,
        "grad_norm": 4.25499153137207,
        "learning_rate": 0.00017226187963279355,
        "epoch": 1.4969333333333332,
        "step": 11227
    },
    {
        "loss": 2.2651,
        "grad_norm": 3.6204066276550293,
        "learning_rate": 0.00017221836050154338,
        "epoch": 1.4970666666666665,
        "step": 11228
    },
    {
        "loss": 2.078,
        "grad_norm": 2.669198751449585,
        "learning_rate": 0.00017217481276435455,
        "epoch": 1.4971999999999999,
        "step": 11229
    },
    {
        "loss": 1.7013,
        "grad_norm": 2.9710967540740967,
        "learning_rate": 0.00017213123643847636,
        "epoch": 1.4973333333333334,
        "step": 11230
    },
    {
        "loss": 0.5596,
        "grad_norm": 2.4543724060058594,
        "learning_rate": 0.00017208763154116972,
        "epoch": 1.4974666666666667,
        "step": 11231
    },
    {
        "loss": 1.8104,
        "grad_norm": 3.438464879989624,
        "learning_rate": 0.00017204399808970654,
        "epoch": 1.4976,
        "step": 11232
    },
    {
        "loss": 1.7286,
        "grad_norm": 2.7523555755615234,
        "learning_rate": 0.0001720003361013704,
        "epoch": 1.4977333333333334,
        "step": 11233
    },
    {
        "loss": 0.7102,
        "grad_norm": 3.24079966545105,
        "learning_rate": 0.00017195664559345559,
        "epoch": 1.4978666666666667,
        "step": 11234
    },
    {
        "loss": 2.463,
        "grad_norm": 3.4710004329681396,
        "learning_rate": 0.0001719129265832683,
        "epoch": 1.498,
        "step": 11235
    },
    {
        "loss": 1.1876,
        "grad_norm": 4.955880641937256,
        "learning_rate": 0.00017186917908812584,
        "epoch": 1.4981333333333333,
        "step": 11236
    },
    {
        "loss": 1.9822,
        "grad_norm": 4.2018938064575195,
        "learning_rate": 0.00017182540312535665,
        "epoch": 1.4982666666666666,
        "step": 11237
    },
    {
        "loss": 2.4228,
        "grad_norm": 4.272835731506348,
        "learning_rate": 0.00017178159871230055,
        "epoch": 1.4984,
        "step": 11238
    },
    {
        "loss": 1.5198,
        "grad_norm": 4.787167072296143,
        "learning_rate": 0.0001717377658663085,
        "epoch": 1.4985333333333333,
        "step": 11239
    },
    {
        "loss": 2.7489,
        "grad_norm": 3.401942253112793,
        "learning_rate": 0.00017169390460474303,
        "epoch": 1.4986666666666666,
        "step": 11240
    },
    {
        "loss": 2.3822,
        "grad_norm": 3.2808802127838135,
        "learning_rate": 0.0001716500149449776,
        "epoch": 1.4988000000000001,
        "step": 11241
    },
    {
        "loss": 2.5787,
        "grad_norm": 2.8164210319519043,
        "learning_rate": 0.00017160609690439728,
        "epoch": 1.4989333333333335,
        "step": 11242
    },
    {
        "loss": 1.5566,
        "grad_norm": 3.931556224822998,
        "learning_rate": 0.00017156215050039778,
        "epoch": 1.4990666666666668,
        "step": 11243
    },
    {
        "loss": 2.7372,
        "grad_norm": 3.5066981315612793,
        "learning_rate": 0.00017151817575038666,
        "epoch": 1.4992,
        "step": 11244
    },
    {
        "loss": 1.7879,
        "grad_norm": 3.4887218475341797,
        "learning_rate": 0.00017147417267178252,
        "epoch": 1.4993333333333334,
        "step": 11245
    },
    {
        "loss": 1.9998,
        "grad_norm": 4.296799659729004,
        "learning_rate": 0.00017143014128201503,
        "epoch": 1.4994666666666667,
        "step": 11246
    },
    {
        "loss": 2.2159,
        "grad_norm": 5.216710567474365,
        "learning_rate": 0.00017138608159852518,
        "epoch": 1.4996,
        "step": 11247
    },
    {
        "loss": 2.765,
        "grad_norm": 3.283968687057495,
        "learning_rate": 0.00017134199363876506,
        "epoch": 1.4997333333333334,
        "step": 11248
    },
    {
        "loss": 1.789,
        "grad_norm": 4.271053314208984,
        "learning_rate": 0.0001712978774201983,
        "epoch": 1.4998666666666667,
        "step": 11249
    },
    {
        "loss": 2.3727,
        "grad_norm": 4.445423603057861,
        "learning_rate": 0.00017125373296029932,
        "epoch": 1.5,
        "step": 11250
    },
    {
        "loss": 2.4139,
        "grad_norm": 2.627206563949585,
        "learning_rate": 0.00017120956027655396,
        "epoch": 1.5001333333333333,
        "step": 11251
    },
    {
        "loss": 2.936,
        "grad_norm": 4.426635265350342,
        "learning_rate": 0.00017116535938645902,
        "epoch": 1.5002666666666666,
        "step": 11252
    },
    {
        "loss": 1.3917,
        "grad_norm": 4.726857662200928,
        "learning_rate": 0.00017112113030752274,
        "epoch": 1.5004,
        "step": 11253
    },
    {
        "loss": 2.7212,
        "grad_norm": 3.8695528507232666,
        "learning_rate": 0.00017107687305726457,
        "epoch": 1.5005333333333333,
        "step": 11254
    },
    {
        "loss": 2.7588,
        "grad_norm": 5.2832112312316895,
        "learning_rate": 0.0001710325876532146,
        "epoch": 1.5006666666666666,
        "step": 11255
    },
    {
        "loss": 2.3693,
        "grad_norm": 4.294127941131592,
        "learning_rate": 0.00017098827411291475,
        "epoch": 1.5008,
        "step": 11256
    },
    {
        "loss": 2.3642,
        "grad_norm": 4.08454704284668,
        "learning_rate": 0.00017094393245391753,
        "epoch": 1.5009333333333332,
        "step": 11257
    },
    {
        "loss": 1.5777,
        "grad_norm": 3.500685691833496,
        "learning_rate": 0.00017089956269378704,
        "epoch": 1.5010666666666665,
        "step": 11258
    },
    {
        "loss": 1.5757,
        "grad_norm": 3.093308448791504,
        "learning_rate": 0.00017085516485009815,
        "epoch": 1.5011999999999999,
        "step": 11259
    },
    {
        "loss": 3.0277,
        "grad_norm": 2.7601327896118164,
        "learning_rate": 0.00017081073894043708,
        "epoch": 1.5013333333333332,
        "step": 11260
    },
    {
        "loss": 2.755,
        "grad_norm": 3.1381006240844727,
        "learning_rate": 0.00017076628498240092,
        "epoch": 1.5014666666666665,
        "step": 11261
    },
    {
        "loss": 2.6951,
        "grad_norm": 4.548790454864502,
        "learning_rate": 0.0001707218029935981,
        "epoch": 1.5016,
        "step": 11262
    },
    {
        "loss": 2.1928,
        "grad_norm": 4.394650459289551,
        "learning_rate": 0.00017067729299164838,
        "epoch": 1.5017333333333334,
        "step": 11263
    },
    {
        "loss": 0.6319,
        "grad_norm": 3.144371747970581,
        "learning_rate": 0.00017063275499418185,
        "epoch": 1.5018666666666667,
        "step": 11264
    },
    {
        "loss": 1.9715,
        "grad_norm": 3.5573811531066895,
        "learning_rate": 0.0001705881890188405,
        "epoch": 1.502,
        "step": 11265
    },
    {
        "loss": 1.977,
        "grad_norm": 4.621729850769043,
        "learning_rate": 0.0001705435950832769,
        "epoch": 1.5021333333333333,
        "step": 11266
    },
    {
        "loss": 2.5941,
        "grad_norm": 4.745477199554443,
        "learning_rate": 0.00017049897320515497,
        "epoch": 1.5022666666666666,
        "step": 11267
    },
    {
        "loss": 1.5876,
        "grad_norm": 5.203526020050049,
        "learning_rate": 0.00017045432340214952,
        "epoch": 1.5024,
        "step": 11268
    },
    {
        "loss": 2.2222,
        "grad_norm": 3.7259390354156494,
        "learning_rate": 0.00017040964569194655,
        "epoch": 1.5025333333333335,
        "step": 11269
    },
    {
        "loss": 1.8012,
        "grad_norm": 4.48774528503418,
        "learning_rate": 0.0001703649400922429,
        "epoch": 1.5026666666666668,
        "step": 11270
    },
    {
        "loss": 2.3844,
        "grad_norm": 4.741119384765625,
        "learning_rate": 0.00017032020662074678,
        "epoch": 1.5028000000000001,
        "step": 11271
    },
    {
        "loss": 2.734,
        "grad_norm": 3.4504919052124023,
        "learning_rate": 0.00017027544529517723,
        "epoch": 1.5029333333333335,
        "step": 11272
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.370659351348877,
        "learning_rate": 0.00017023065613326426,
        "epoch": 1.5030666666666668,
        "step": 11273
    },
    {
        "loss": 2.7917,
        "grad_norm": 4.061772346496582,
        "learning_rate": 0.00017018583915274916,
        "epoch": 1.5032,
        "step": 11274
    },
    {
        "loss": 1.3227,
        "grad_norm": 4.880396366119385,
        "learning_rate": 0.00017014099437138406,
        "epoch": 1.5033333333333334,
        "step": 11275
    },
    {
        "loss": 1.3678,
        "grad_norm": 5.765927314758301,
        "learning_rate": 0.00017009612180693195,
        "epoch": 1.5034666666666667,
        "step": 11276
    },
    {
        "loss": 2.4002,
        "grad_norm": 2.086820125579834,
        "learning_rate": 0.00017005122147716723,
        "epoch": 1.5036,
        "step": 11277
    },
    {
        "loss": 2.2666,
        "grad_norm": 3.461872100830078,
        "learning_rate": 0.00017000629339987503,
        "epoch": 1.5037333333333334,
        "step": 11278
    },
    {
        "loss": 1.6626,
        "grad_norm": 2.9155099391937256,
        "learning_rate": 0.00016996133759285135,
        "epoch": 1.5038666666666667,
        "step": 11279
    },
    {
        "loss": 1.4158,
        "grad_norm": 4.1619439125061035,
        "learning_rate": 0.00016991635407390357,
        "epoch": 1.504,
        "step": 11280
    },
    {
        "loss": 1.2616,
        "grad_norm": 4.566393852233887,
        "learning_rate": 0.00016987134286084972,
        "epoch": 1.5041333333333333,
        "step": 11281
    },
    {
        "loss": 2.2243,
        "grad_norm": 4.550957202911377,
        "learning_rate": 0.00016982630397151876,
        "epoch": 1.5042666666666666,
        "step": 11282
    },
    {
        "loss": 1.6049,
        "grad_norm": 4.912700176239014,
        "learning_rate": 0.000169781237423751,
        "epoch": 1.5044,
        "step": 11283
    },
    {
        "loss": 2.4374,
        "grad_norm": 3.4315385818481445,
        "learning_rate": 0.00016973614323539731,
        "epoch": 1.5045333333333333,
        "step": 11284
    },
    {
        "loss": 1.6208,
        "grad_norm": 5.2768473625183105,
        "learning_rate": 0.00016969102142431954,
        "epoch": 1.5046666666666666,
        "step": 11285
    },
    {
        "loss": 2.3358,
        "grad_norm": 2.639862298965454,
        "learning_rate": 0.00016964587200839085,
        "epoch": 1.5048,
        "step": 11286
    },
    {
        "loss": 1.524,
        "grad_norm": 3.492648124694824,
        "learning_rate": 0.00016960069500549492,
        "epoch": 1.5049333333333332,
        "step": 11287
    },
    {
        "loss": 1.8839,
        "grad_norm": 5.34867000579834,
        "learning_rate": 0.00016955549043352655,
        "epoch": 1.5050666666666666,
        "step": 11288
    },
    {
        "loss": 1.8884,
        "grad_norm": 3.339923143386841,
        "learning_rate": 0.0001695102583103913,
        "epoch": 1.5051999999999999,
        "step": 11289
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.976234197616577,
        "learning_rate": 0.000169464998654006,
        "epoch": 1.5053333333333332,
        "step": 11290
    },
    {
        "loss": 2.5863,
        "grad_norm": 4.4197540283203125,
        "learning_rate": 0.0001694197114822979,
        "epoch": 1.5054666666666665,
        "step": 11291
    },
    {
        "loss": 2.3929,
        "grad_norm": 5.563991546630859,
        "learning_rate": 0.00016937439681320582,
        "epoch": 1.5056,
        "step": 11292
    },
    {
        "loss": 2.2558,
        "grad_norm": 4.510335922241211,
        "learning_rate": 0.0001693290546646785,
        "epoch": 1.5057333333333334,
        "step": 11293
    },
    {
        "loss": 3.1766,
        "grad_norm": 3.389946222305298,
        "learning_rate": 0.00016928368505467645,
        "epoch": 1.5058666666666667,
        "step": 11294
    },
    {
        "loss": 2.2594,
        "grad_norm": 3.7817864418029785,
        "learning_rate": 0.00016923828800117073,
        "epoch": 1.506,
        "step": 11295
    },
    {
        "loss": 1.7295,
        "grad_norm": 4.577788352966309,
        "learning_rate": 0.00016919286352214322,
        "epoch": 1.5061333333333333,
        "step": 11296
    },
    {
        "loss": 1.8313,
        "grad_norm": 3.246027708053589,
        "learning_rate": 0.00016914741163558673,
        "epoch": 1.5062666666666666,
        "step": 11297
    },
    {
        "loss": 2.8831,
        "grad_norm": 2.789165496826172,
        "learning_rate": 0.00016910193235950474,
        "epoch": 1.5064,
        "step": 11298
    },
    {
        "loss": 2.3113,
        "grad_norm": 3.5830278396606445,
        "learning_rate": 0.00016905642571191199,
        "epoch": 1.5065333333333333,
        "step": 11299
    },
    {
        "loss": 2.6391,
        "grad_norm": 4.3129191398620605,
        "learning_rate": 0.0001690108917108336,
        "epoch": 1.5066666666666668,
        "step": 11300
    },
    {
        "loss": 2.061,
        "grad_norm": 3.2206976413726807,
        "learning_rate": 0.0001689653303743061,
        "epoch": 1.5068000000000001,
        "step": 11301
    },
    {
        "loss": 2.9346,
        "grad_norm": 2.333707332611084,
        "learning_rate": 0.000168919741720376,
        "epoch": 1.5069333333333335,
        "step": 11302
    },
    {
        "loss": 1.134,
        "grad_norm": 4.9454665184021,
        "learning_rate": 0.0001688741257671014,
        "epoch": 1.5070666666666668,
        "step": 11303
    },
    {
        "loss": 1.935,
        "grad_norm": 2.1737899780273438,
        "learning_rate": 0.00016882848253255096,
        "epoch": 1.5072,
        "step": 11304
    },
    {
        "loss": 1.6397,
        "grad_norm": 4.351170063018799,
        "learning_rate": 0.0001687828120348041,
        "epoch": 1.5073333333333334,
        "step": 11305
    },
    {
        "loss": 1.9565,
        "grad_norm": 3.2605984210968018,
        "learning_rate": 0.00016873711429195098,
        "epoch": 1.5074666666666667,
        "step": 11306
    },
    {
        "loss": 1.9193,
        "grad_norm": 5.659816741943359,
        "learning_rate": 0.00016869138932209258,
        "epoch": 1.5076,
        "step": 11307
    },
    {
        "loss": 2.1326,
        "grad_norm": 4.3546905517578125,
        "learning_rate": 0.00016864563714334097,
        "epoch": 1.5077333333333334,
        "step": 11308
    },
    {
        "loss": 1.9604,
        "grad_norm": 3.8416755199432373,
        "learning_rate": 0.00016859985777381852,
        "epoch": 1.5078666666666667,
        "step": 11309
    },
    {
        "loss": 2.2591,
        "grad_norm": 3.5530519485473633,
        "learning_rate": 0.00016855405123165869,
        "epoch": 1.508,
        "step": 11310
    },
    {
        "loss": 2.2741,
        "grad_norm": 3.1988272666931152,
        "learning_rate": 0.00016850821753500546,
        "epoch": 1.5081333333333333,
        "step": 11311
    },
    {
        "loss": 2.8668,
        "grad_norm": 3.4269421100616455,
        "learning_rate": 0.00016846235670201382,
        "epoch": 1.5082666666666666,
        "step": 11312
    },
    {
        "loss": 2.6935,
        "grad_norm": 2.896756172180176,
        "learning_rate": 0.00016841646875084964,
        "epoch": 1.5084,
        "step": 11313
    },
    {
        "loss": 0.6327,
        "grad_norm": 3.4944493770599365,
        "learning_rate": 0.00016837055369968884,
        "epoch": 1.5085333333333333,
        "step": 11314
    },
    {
        "loss": 2.7662,
        "grad_norm": 4.4831390380859375,
        "learning_rate": 0.00016832461156671884,
        "epoch": 1.5086666666666666,
        "step": 11315
    },
    {
        "loss": 1.3879,
        "grad_norm": 2.915959119796753,
        "learning_rate": 0.00016827864237013734,
        "epoch": 1.5088,
        "step": 11316
    },
    {
        "loss": 1.3138,
        "grad_norm": 3.9221975803375244,
        "learning_rate": 0.000168232646128153,
        "epoch": 1.5089333333333332,
        "step": 11317
    },
    {
        "loss": 2.4888,
        "grad_norm": 3.8767454624176025,
        "learning_rate": 0.00016818662285898503,
        "epoch": 1.5090666666666666,
        "step": 11318
    },
    {
        "loss": 2.0494,
        "grad_norm": 3.976036310195923,
        "learning_rate": 0.00016814057258086342,
        "epoch": 1.5091999999999999,
        "step": 11319
    },
    {
        "loss": 1.6623,
        "grad_norm": 5.892748832702637,
        "learning_rate": 0.00016809449531202873,
        "epoch": 1.5093333333333332,
        "step": 11320
    },
    {
        "loss": 1.5493,
        "grad_norm": 6.425638198852539,
        "learning_rate": 0.0001680483910707324,
        "epoch": 1.5094666666666665,
        "step": 11321
    },
    {
        "loss": 2.5659,
        "grad_norm": 3.9614059925079346,
        "learning_rate": 0.00016800225987523672,
        "epoch": 1.5096,
        "step": 11322
    },
    {
        "loss": 2.6351,
        "grad_norm": 3.6154582500457764,
        "learning_rate": 0.000167956101743814,
        "epoch": 1.5097333333333334,
        "step": 11323
    },
    {
        "loss": 2.1751,
        "grad_norm": 4.761599063873291,
        "learning_rate": 0.00016790991669474788,
        "epoch": 1.5098666666666667,
        "step": 11324
    },
    {
        "loss": 2.2598,
        "grad_norm": 2.923039436340332,
        "learning_rate": 0.00016786370474633228,
        "epoch": 1.51,
        "step": 11325
    },
    {
        "loss": 2.1657,
        "grad_norm": 2.7449135780334473,
        "learning_rate": 0.00016781746591687212,
        "epoch": 1.5101333333333333,
        "step": 11326
    },
    {
        "loss": 1.9466,
        "grad_norm": 4.2996320724487305,
        "learning_rate": 0.0001677712002246826,
        "epoch": 1.5102666666666666,
        "step": 11327
    },
    {
        "loss": 0.749,
        "grad_norm": 3.2418415546417236,
        "learning_rate": 0.00016772490768808974,
        "epoch": 1.5104,
        "step": 11328
    },
    {
        "loss": 2.1817,
        "grad_norm": 5.524394512176514,
        "learning_rate": 0.0001676785883254301,
        "epoch": 1.5105333333333333,
        "step": 11329
    },
    {
        "loss": 1.5748,
        "grad_norm": 5.8210129737854,
        "learning_rate": 0.00016763224215505114,
        "epoch": 1.5106666666666668,
        "step": 11330
    },
    {
        "loss": 1.546,
        "grad_norm": 3.9887406826019287,
        "learning_rate": 0.00016758586919531055,
        "epoch": 1.5108000000000001,
        "step": 11331
    },
    {
        "loss": 2.5165,
        "grad_norm": 3.189392328262329,
        "learning_rate": 0.0001675394694645768,
        "epoch": 1.5109333333333335,
        "step": 11332
    },
    {
        "loss": 2.9791,
        "grad_norm": 4.291786193847656,
        "learning_rate": 0.00016749304298122918,
        "epoch": 1.5110666666666668,
        "step": 11333
    },
    {
        "loss": 1.9247,
        "grad_norm": 4.638702392578125,
        "learning_rate": 0.00016744658976365723,
        "epoch": 1.5112,
        "step": 11334
    },
    {
        "loss": 2.4504,
        "grad_norm": 4.691517353057861,
        "learning_rate": 0.0001674001098302612,
        "epoch": 1.5113333333333334,
        "step": 11335
    },
    {
        "loss": 2.3501,
        "grad_norm": 4.2969465255737305,
        "learning_rate": 0.00016735360319945205,
        "epoch": 1.5114666666666667,
        "step": 11336
    },
    {
        "loss": 2.4039,
        "grad_norm": 3.9480535984039307,
        "learning_rate": 0.0001673070698896512,
        "epoch": 1.5116,
        "step": 11337
    },
    {
        "loss": 2.0163,
        "grad_norm": 3.5631306171417236,
        "learning_rate": 0.00016726050991929052,
        "epoch": 1.5117333333333334,
        "step": 11338
    },
    {
        "loss": 2.5023,
        "grad_norm": 4.087244033813477,
        "learning_rate": 0.00016721392330681277,
        "epoch": 1.5118666666666667,
        "step": 11339
    },
    {
        "loss": 2.2692,
        "grad_norm": 2.5819549560546875,
        "learning_rate": 0.00016716731007067098,
        "epoch": 1.512,
        "step": 11340
    },
    {
        "loss": 0.8404,
        "grad_norm": 5.561154365539551,
        "learning_rate": 0.0001671206702293287,
        "epoch": 1.5121333333333333,
        "step": 11341
    },
    {
        "loss": 2.2292,
        "grad_norm": 3.9896416664123535,
        "learning_rate": 0.00016707400380126035,
        "epoch": 1.5122666666666666,
        "step": 11342
    },
    {
        "loss": 2.6494,
        "grad_norm": 2.6456761360168457,
        "learning_rate": 0.00016702731080495053,
        "epoch": 1.5124,
        "step": 11343
    },
    {
        "loss": 2.5317,
        "grad_norm": 2.867225170135498,
        "learning_rate": 0.00016698059125889441,
        "epoch": 1.5125333333333333,
        "step": 11344
    },
    {
        "loss": 2.5078,
        "grad_norm": 3.4131689071655273,
        "learning_rate": 0.000166933845181598,
        "epoch": 1.5126666666666666,
        "step": 11345
    },
    {
        "loss": 2.4675,
        "grad_norm": 3.5471057891845703,
        "learning_rate": 0.0001668870725915774,
        "epoch": 1.5128,
        "step": 11346
    },
    {
        "loss": 2.5845,
        "grad_norm": 3.1767067909240723,
        "learning_rate": 0.00016684027350735942,
        "epoch": 1.5129333333333332,
        "step": 11347
    },
    {
        "loss": 2.1197,
        "grad_norm": 4.865372180938721,
        "learning_rate": 0.00016679344794748126,
        "epoch": 1.5130666666666666,
        "step": 11348
    },
    {
        "loss": 1.4522,
        "grad_norm": 4.326340675354004,
        "learning_rate": 0.0001667465959304909,
        "epoch": 1.5131999999999999,
        "step": 11349
    },
    {
        "loss": 1.0813,
        "grad_norm": 3.9882757663726807,
        "learning_rate": 0.00016669971747494634,
        "epoch": 1.5133333333333332,
        "step": 11350
    },
    {
        "loss": 1.0129,
        "grad_norm": 4.815852642059326,
        "learning_rate": 0.00016665281259941662,
        "epoch": 1.5134666666666665,
        "step": 11351
    },
    {
        "loss": 2.3379,
        "grad_norm": 4.664523601531982,
        "learning_rate": 0.00016660588132248053,
        "epoch": 1.5135999999999998,
        "step": 11352
    },
    {
        "loss": 1.7118,
        "grad_norm": 5.776066780090332,
        "learning_rate": 0.00016655892366272787,
        "epoch": 1.5137333333333334,
        "step": 11353
    },
    {
        "loss": 2.2204,
        "grad_norm": 5.142646312713623,
        "learning_rate": 0.00016651193963875886,
        "epoch": 1.5138666666666667,
        "step": 11354
    },
    {
        "loss": 2.8985,
        "grad_norm": 3.856565475463867,
        "learning_rate": 0.00016646492926918394,
        "epoch": 1.514,
        "step": 11355
    },
    {
        "loss": 2.795,
        "grad_norm": 3.384972095489502,
        "learning_rate": 0.00016641789257262402,
        "epoch": 1.5141333333333333,
        "step": 11356
    },
    {
        "loss": 1.827,
        "grad_norm": 5.937140464782715,
        "learning_rate": 0.00016637082956771047,
        "epoch": 1.5142666666666666,
        "step": 11357
    },
    {
        "loss": 2.2753,
        "grad_norm": 4.480418682098389,
        "learning_rate": 0.00016632374027308528,
        "epoch": 1.5144,
        "step": 11358
    },
    {
        "loss": 2.5428,
        "grad_norm": 2.2345376014709473,
        "learning_rate": 0.00016627662470740048,
        "epoch": 1.5145333333333333,
        "step": 11359
    },
    {
        "loss": 2.4375,
        "grad_norm": 4.664998531341553,
        "learning_rate": 0.000166229482889319,
        "epoch": 1.5146666666666668,
        "step": 11360
    },
    {
        "loss": 2.5327,
        "grad_norm": 6.1594367027282715,
        "learning_rate": 0.00016618231483751344,
        "epoch": 1.5148000000000001,
        "step": 11361
    },
    {
        "loss": 3.0301,
        "grad_norm": 3.258826732635498,
        "learning_rate": 0.00016613512057066753,
        "epoch": 1.5149333333333335,
        "step": 11362
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.326829433441162,
        "learning_rate": 0.00016608790010747506,
        "epoch": 1.5150666666666668,
        "step": 11363
    },
    {
        "loss": 1.9272,
        "grad_norm": 3.813805341720581,
        "learning_rate": 0.00016604065346664018,
        "epoch": 1.5152,
        "step": 11364
    },
    {
        "loss": 2.6583,
        "grad_norm": 4.3644256591796875,
        "learning_rate": 0.0001659933806668774,
        "epoch": 1.5153333333333334,
        "step": 11365
    },
    {
        "loss": 1.7687,
        "grad_norm": 3.9180071353912354,
        "learning_rate": 0.00016594608172691163,
        "epoch": 1.5154666666666667,
        "step": 11366
    },
    {
        "loss": 1.0443,
        "grad_norm": 4.276979446411133,
        "learning_rate": 0.00016589875666547826,
        "epoch": 1.5156,
        "step": 11367
    },
    {
        "loss": 2.1372,
        "grad_norm": 3.4831490516662598,
        "learning_rate": 0.00016585140550132282,
        "epoch": 1.5157333333333334,
        "step": 11368
    },
    {
        "loss": 2.1335,
        "grad_norm": 2.49112606048584,
        "learning_rate": 0.0001658040282532013,
        "epoch": 1.5158666666666667,
        "step": 11369
    },
    {
        "loss": 2.1035,
        "grad_norm": 5.008364677429199,
        "learning_rate": 0.00016575662493987985,
        "epoch": 1.516,
        "step": 11370
    },
    {
        "loss": 2.2541,
        "grad_norm": 4.967123508453369,
        "learning_rate": 0.00016570919558013517,
        "epoch": 1.5161333333333333,
        "step": 11371
    },
    {
        "loss": 2.5471,
        "grad_norm": 4.283451557159424,
        "learning_rate": 0.0001656617401927545,
        "epoch": 1.5162666666666667,
        "step": 11372
    },
    {
        "loss": 2.2243,
        "grad_norm": 4.181335926055908,
        "learning_rate": 0.00016561425879653454,
        "epoch": 1.5164,
        "step": 11373
    },
    {
        "loss": 2.2654,
        "grad_norm": 4.238916397094727,
        "learning_rate": 0.00016556675141028322,
        "epoch": 1.5165333333333333,
        "step": 11374
    },
    {
        "loss": 2.0799,
        "grad_norm": 4.964427471160889,
        "learning_rate": 0.00016551921805281814,
        "epoch": 1.5166666666666666,
        "step": 11375
    },
    {
        "loss": 2.3729,
        "grad_norm": 3.4373674392700195,
        "learning_rate": 0.00016547165874296763,
        "epoch": 1.5168,
        "step": 11376
    },
    {
        "loss": 1.6267,
        "grad_norm": 3.9707562923431396,
        "learning_rate": 0.00016542407349957004,
        "epoch": 1.5169333333333332,
        "step": 11377
    },
    {
        "loss": 2.6359,
        "grad_norm": 6.4839582443237305,
        "learning_rate": 0.00016537646234147398,
        "epoch": 1.5170666666666666,
        "step": 11378
    },
    {
        "loss": 1.7956,
        "grad_norm": 4.129837512969971,
        "learning_rate": 0.00016532882528753833,
        "epoch": 1.5171999999999999,
        "step": 11379
    },
    {
        "loss": 2.7456,
        "grad_norm": 4.279078483581543,
        "learning_rate": 0.00016528116235663238,
        "epoch": 1.5173333333333332,
        "step": 11380
    },
    {
        "loss": 2.1595,
        "grad_norm": 3.34820556640625,
        "learning_rate": 0.0001652334735676358,
        "epoch": 1.5174666666666665,
        "step": 11381
    },
    {
        "loss": 2.3561,
        "grad_norm": 2.8808536529541016,
        "learning_rate": 0.0001651857589394378,
        "epoch": 1.5175999999999998,
        "step": 11382
    },
    {
        "loss": 3.1615,
        "grad_norm": 4.334304332733154,
        "learning_rate": 0.00016513801849093874,
        "epoch": 1.5177333333333334,
        "step": 11383
    },
    {
        "loss": 3.0452,
        "grad_norm": 3.7885138988494873,
        "learning_rate": 0.00016509025224104845,
        "epoch": 1.5178666666666667,
        "step": 11384
    },
    {
        "loss": 1.0407,
        "grad_norm": 4.064329147338867,
        "learning_rate": 0.0001650424602086876,
        "epoch": 1.518,
        "step": 11385
    },
    {
        "loss": 2.4684,
        "grad_norm": 3.9431376457214355,
        "learning_rate": 0.0001649946424127866,
        "epoch": 1.5181333333333333,
        "step": 11386
    },
    {
        "loss": 2.2398,
        "grad_norm": 3.4769883155822754,
        "learning_rate": 0.00016494679887228632,
        "epoch": 1.5182666666666667,
        "step": 11387
    },
    {
        "loss": 2.3039,
        "grad_norm": 3.7867023944854736,
        "learning_rate": 0.00016489892960613758,
        "epoch": 1.5184,
        "step": 11388
    },
    {
        "loss": 2.4375,
        "grad_norm": 4.140745162963867,
        "learning_rate": 0.00016485103463330177,
        "epoch": 1.5185333333333333,
        "step": 11389
    },
    {
        "loss": 2.7389,
        "grad_norm": 3.7023563385009766,
        "learning_rate": 0.0001648031139727502,
        "epoch": 1.5186666666666668,
        "step": 11390
    },
    {
        "loss": 1.9511,
        "grad_norm": 4.552777290344238,
        "learning_rate": 0.00016475516764346423,
        "epoch": 1.5188000000000001,
        "step": 11391
    },
    {
        "loss": 2.4143,
        "grad_norm": 4.004589557647705,
        "learning_rate": 0.00016470719566443585,
        "epoch": 1.5189333333333335,
        "step": 11392
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.9420464038848877,
        "learning_rate": 0.00016465919805466677,
        "epoch": 1.5190666666666668,
        "step": 11393
    },
    {
        "loss": 2.1978,
        "grad_norm": 3.1225860118865967,
        "learning_rate": 0.0001646111748331689,
        "epoch": 1.5192,
        "step": 11394
    },
    {
        "loss": 2.0494,
        "grad_norm": 3.1880719661712646,
        "learning_rate": 0.00016456312601896466,
        "epoch": 1.5193333333333334,
        "step": 11395
    },
    {
        "loss": 2.3854,
        "grad_norm": 3.6395041942596436,
        "learning_rate": 0.00016451505163108619,
        "epoch": 1.5194666666666667,
        "step": 11396
    },
    {
        "loss": 1.2273,
        "grad_norm": 5.795525550842285,
        "learning_rate": 0.00016446695168857592,
        "epoch": 1.5196,
        "step": 11397
    },
    {
        "loss": 1.9982,
        "grad_norm": 4.883420467376709,
        "learning_rate": 0.00016441882621048634,
        "epoch": 1.5197333333333334,
        "step": 11398
    },
    {
        "loss": 2.0785,
        "grad_norm": 4.500346660614014,
        "learning_rate": 0.00016437067521588033,
        "epoch": 1.5198666666666667,
        "step": 11399
    },
    {
        "loss": 2.6556,
        "grad_norm": 2.9617860317230225,
        "learning_rate": 0.00016432249872383036,
        "epoch": 1.52,
        "step": 11400
    },
    {
        "loss": 1.7653,
        "grad_norm": 4.692610740661621,
        "learning_rate": 0.00016427429675341961,
        "epoch": 1.5201333333333333,
        "step": 11401
    },
    {
        "loss": 1.6125,
        "grad_norm": 4.320019245147705,
        "learning_rate": 0.0001642260693237409,
        "epoch": 1.5202666666666667,
        "step": 11402
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.6741931438446045,
        "learning_rate": 0.0001641778164538972,
        "epoch": 1.5204,
        "step": 11403
    },
    {
        "loss": 1.0531,
        "grad_norm": 3.817652463912964,
        "learning_rate": 0.0001641295381630018,
        "epoch": 1.5205333333333333,
        "step": 11404
    },
    {
        "loss": 1.9809,
        "grad_norm": 4.278133392333984,
        "learning_rate": 0.00016408123447017781,
        "epoch": 1.5206666666666666,
        "step": 11405
    },
    {
        "loss": 1.6732,
        "grad_norm": 3.9835760593414307,
        "learning_rate": 0.00016403290539455852,
        "epoch": 1.5208,
        "step": 11406
    },
    {
        "loss": 2.3569,
        "grad_norm": 3.7384676933288574,
        "learning_rate": 0.00016398455095528714,
        "epoch": 1.5209333333333332,
        "step": 11407
    },
    {
        "loss": 1.3511,
        "grad_norm": 4.208891868591309,
        "learning_rate": 0.00016393617117151717,
        "epoch": 1.5210666666666666,
        "step": 11408
    },
    {
        "loss": 2.2021,
        "grad_norm": 4.763233184814453,
        "learning_rate": 0.00016388776606241184,
        "epoch": 1.5211999999999999,
        "step": 11409
    },
    {
        "loss": 2.4033,
        "grad_norm": 3.311725616455078,
        "learning_rate": 0.0001638393356471449,
        "epoch": 1.5213333333333332,
        "step": 11410
    },
    {
        "loss": 1.0329,
        "grad_norm": 3.4385311603546143,
        "learning_rate": 0.00016379087994489936,
        "epoch": 1.5214666666666665,
        "step": 11411
    },
    {
        "loss": 0.9389,
        "grad_norm": 4.636976718902588,
        "learning_rate": 0.0001637423989748689,
        "epoch": 1.5215999999999998,
        "step": 11412
    },
    {
        "loss": 2.4089,
        "grad_norm": 3.9429497718811035,
        "learning_rate": 0.00016369389275625715,
        "epoch": 1.5217333333333334,
        "step": 11413
    },
    {
        "loss": 2.2851,
        "grad_norm": 5.541670799255371,
        "learning_rate": 0.00016364536130827743,
        "epoch": 1.5218666666666667,
        "step": 11414
    },
    {
        "loss": 2.1212,
        "grad_norm": 4.322271823883057,
        "learning_rate": 0.00016359680465015324,
        "epoch": 1.522,
        "step": 11415
    },
    {
        "loss": 2.4748,
        "grad_norm": 2.65706467628479,
        "learning_rate": 0.0001635482228011179,
        "epoch": 1.5221333333333333,
        "step": 11416
    },
    {
        "loss": 2.3588,
        "grad_norm": 2.587463140487671,
        "learning_rate": 0.0001634996157804151,
        "epoch": 1.5222666666666667,
        "step": 11417
    },
    {
        "loss": 1.641,
        "grad_norm": 2.4107539653778076,
        "learning_rate": 0.0001634509836072981,
        "epoch": 1.5224,
        "step": 11418
    },
    {
        "loss": 2.3352,
        "grad_norm": 4.110329627990723,
        "learning_rate": 0.0001634023263010303,
        "epoch": 1.5225333333333333,
        "step": 11419
    },
    {
        "loss": 1.731,
        "grad_norm": 3.854870557785034,
        "learning_rate": 0.00016335364388088492,
        "epoch": 1.5226666666666666,
        "step": 11420
    },
    {
        "loss": 2.769,
        "grad_norm": 3.527839183807373,
        "learning_rate": 0.0001633049363661453,
        "epoch": 1.5228000000000002,
        "step": 11421
    },
    {
        "loss": 2.3988,
        "grad_norm": 6.2977824211120605,
        "learning_rate": 0.00016325620377610482,
        "epoch": 1.5229333333333335,
        "step": 11422
    },
    {
        "loss": 2.5749,
        "grad_norm": 3.247480630874634,
        "learning_rate": 0.00016320744613006647,
        "epoch": 1.5230666666666668,
        "step": 11423
    },
    {
        "loss": 3.0033,
        "grad_norm": 6.2561564445495605,
        "learning_rate": 0.00016315866344734329,
        "epoch": 1.5232,
        "step": 11424
    },
    {
        "loss": 2.8448,
        "grad_norm": 3.701932907104492,
        "learning_rate": 0.00016310985574725825,
        "epoch": 1.5233333333333334,
        "step": 11425
    },
    {
        "loss": 2.5201,
        "grad_norm": 4.985001087188721,
        "learning_rate": 0.00016306102304914442,
        "epoch": 1.5234666666666667,
        "step": 11426
    },
    {
        "loss": 1.3593,
        "grad_norm": 4.550411224365234,
        "learning_rate": 0.00016301216537234448,
        "epoch": 1.5236,
        "step": 11427
    },
    {
        "loss": 2.7485,
        "grad_norm": 3.6379706859588623,
        "learning_rate": 0.00016296328273621112,
        "epoch": 1.5237333333333334,
        "step": 11428
    },
    {
        "loss": 2.4277,
        "grad_norm": 4.2292160987854,
        "learning_rate": 0.00016291437516010682,
        "epoch": 1.5238666666666667,
        "step": 11429
    },
    {
        "loss": 0.9526,
        "grad_norm": 3.206923723220825,
        "learning_rate": 0.0001628654426634041,
        "epoch": 1.524,
        "step": 11430
    },
    {
        "loss": 1.6283,
        "grad_norm": 4.733539581298828,
        "learning_rate": 0.0001628164852654856,
        "epoch": 1.5241333333333333,
        "step": 11431
    },
    {
        "loss": 2.0036,
        "grad_norm": 5.755936145782471,
        "learning_rate": 0.00016276750298574303,
        "epoch": 1.5242666666666667,
        "step": 11432
    },
    {
        "loss": 2.5194,
        "grad_norm": 2.8479535579681396,
        "learning_rate": 0.00016271849584357874,
        "epoch": 1.5244,
        "step": 11433
    },
    {
        "loss": 2.9536,
        "grad_norm": 2.414137363433838,
        "learning_rate": 0.00016266946385840446,
        "epoch": 1.5245333333333333,
        "step": 11434
    },
    {
        "loss": 1.9202,
        "grad_norm": 5.168132781982422,
        "learning_rate": 0.00016262040704964208,
        "epoch": 1.5246666666666666,
        "step": 11435
    },
    {
        "loss": 0.9274,
        "grad_norm": 5.930417060852051,
        "learning_rate": 0.0001625713254367231,
        "epoch": 1.5248,
        "step": 11436
    },
    {
        "loss": 2.1804,
        "grad_norm": 3.3289573192596436,
        "learning_rate": 0.0001625222190390889,
        "epoch": 1.5249333333333333,
        "step": 11437
    },
    {
        "loss": 1.3092,
        "grad_norm": 4.6309919357299805,
        "learning_rate": 0.0001624730878761906,
        "epoch": 1.5250666666666666,
        "step": 11438
    },
    {
        "loss": 2.1971,
        "grad_norm": 3.575540781021118,
        "learning_rate": 0.00016242393196748932,
        "epoch": 1.5252,
        "step": 11439
    },
    {
        "loss": 2.3602,
        "grad_norm": 3.722855567932129,
        "learning_rate": 0.0001623747513324561,
        "epoch": 1.5253333333333332,
        "step": 11440
    },
    {
        "loss": 2.5437,
        "grad_norm": 3.960986375808716,
        "learning_rate": 0.00016232554599057112,
        "epoch": 1.5254666666666665,
        "step": 11441
    },
    {
        "loss": 2.3645,
        "grad_norm": 3.6879494190216064,
        "learning_rate": 0.00016227631596132515,
        "epoch": 1.5255999999999998,
        "step": 11442
    },
    {
        "loss": 1.5638,
        "grad_norm": 2.8948934078216553,
        "learning_rate": 0.00016222706126421807,
        "epoch": 1.5257333333333334,
        "step": 11443
    },
    {
        "loss": 1.1729,
        "grad_norm": 4.254565715789795,
        "learning_rate": 0.00016217778191876013,
        "epoch": 1.5258666666666667,
        "step": 11444
    },
    {
        "loss": 0.8915,
        "grad_norm": 4.845799446105957,
        "learning_rate": 0.00016212847794447088,
        "epoch": 1.526,
        "step": 11445
    },
    {
        "loss": 1.4832,
        "grad_norm": 4.815165042877197,
        "learning_rate": 0.00016207914936087984,
        "epoch": 1.5261333333333333,
        "step": 11446
    },
    {
        "loss": 1.6276,
        "grad_norm": 3.94981050491333,
        "learning_rate": 0.00016202979618752607,
        "epoch": 1.5262666666666667,
        "step": 11447
    },
    {
        "loss": 1.4447,
        "grad_norm": 4.140108585357666,
        "learning_rate": 0.0001619804184439588,
        "epoch": 1.5264,
        "step": 11448
    },
    {
        "loss": 1.8461,
        "grad_norm": 4.562086582183838,
        "learning_rate": 0.00016193101614973658,
        "epoch": 1.5265333333333333,
        "step": 11449
    },
    {
        "loss": 2.4882,
        "grad_norm": 2.9244296550750732,
        "learning_rate": 0.00016188158932442775,
        "epoch": 1.5266666666666666,
        "step": 11450
    },
    {
        "loss": 2.4607,
        "grad_norm": 3.8690848350524902,
        "learning_rate": 0.00016183213798761064,
        "epoch": 1.5268000000000002,
        "step": 11451
    },
    {
        "loss": 1.6602,
        "grad_norm": 3.899911403656006,
        "learning_rate": 0.00016178266215887297,
        "epoch": 1.5269333333333335,
        "step": 11452
    },
    {
        "loss": 2.8324,
        "grad_norm": 3.259132146835327,
        "learning_rate": 0.0001617331618578122,
        "epoch": 1.5270666666666668,
        "step": 11453
    },
    {
        "loss": 1.8312,
        "grad_norm": 4.48469877243042,
        "learning_rate": 0.0001616836371040358,
        "epoch": 1.5272000000000001,
        "step": 11454
    },
    {
        "loss": 2.3853,
        "grad_norm": 3.3953490257263184,
        "learning_rate": 0.0001616340879171605,
        "epoch": 1.5273333333333334,
        "step": 11455
    },
    {
        "loss": 2.0818,
        "grad_norm": 4.015257835388184,
        "learning_rate": 0.000161584514316813,
        "epoch": 1.5274666666666668,
        "step": 11456
    },
    {
        "loss": 1.6066,
        "grad_norm": 4.583137035369873,
        "learning_rate": 0.00016153491632262942,
        "epoch": 1.5276,
        "step": 11457
    },
    {
        "loss": 1.4191,
        "grad_norm": 4.660552978515625,
        "learning_rate": 0.00016148529395425593,
        "epoch": 1.5277333333333334,
        "step": 11458
    },
    {
        "loss": 2.684,
        "grad_norm": 3.131284236907959,
        "learning_rate": 0.00016143564723134786,
        "epoch": 1.5278666666666667,
        "step": 11459
    },
    {
        "loss": 2.5653,
        "grad_norm": 4.215594291687012,
        "learning_rate": 0.00016138597617357072,
        "epoch": 1.528,
        "step": 11460
    },
    {
        "loss": 2.6677,
        "grad_norm": 5.089936256408691,
        "learning_rate": 0.00016133628080059923,
        "epoch": 1.5281333333333333,
        "step": 11461
    },
    {
        "loss": 2.1279,
        "grad_norm": 5.151280879974365,
        "learning_rate": 0.00016128656113211784,
        "epoch": 1.5282666666666667,
        "step": 11462
    },
    {
        "loss": 2.8044,
        "grad_norm": 2.156397819519043,
        "learning_rate": 0.00016123681718782087,
        "epoch": 1.5284,
        "step": 11463
    },
    {
        "loss": 1.7224,
        "grad_norm": 5.033545017242432,
        "learning_rate": 0.00016118704898741192,
        "epoch": 1.5285333333333333,
        "step": 11464
    },
    {
        "loss": 2.5755,
        "grad_norm": 3.642768144607544,
        "learning_rate": 0.00016113725655060435,
        "epoch": 1.5286666666666666,
        "step": 11465
    },
    {
        "loss": 2.0243,
        "grad_norm": 6.000802516937256,
        "learning_rate": 0.0001610874398971211,
        "epoch": 1.5288,
        "step": 11466
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.937612771987915,
        "learning_rate": 0.00016103759904669484,
        "epoch": 1.5289333333333333,
        "step": 11467
    },
    {
        "loss": 1.9549,
        "grad_norm": 2.0891501903533936,
        "learning_rate": 0.00016098773401906747,
        "epoch": 1.5290666666666666,
        "step": 11468
    },
    {
        "loss": 1.8593,
        "grad_norm": 3.4105331897735596,
        "learning_rate": 0.00016093784483399112,
        "epoch": 1.5292,
        "step": 11469
    },
    {
        "loss": 1.4944,
        "grad_norm": 5.543007850646973,
        "learning_rate": 0.00016088793151122655,
        "epoch": 1.5293333333333332,
        "step": 11470
    },
    {
        "loss": 2.362,
        "grad_norm": 3.8081300258636475,
        "learning_rate": 0.0001608379940705448,
        "epoch": 1.5294666666666665,
        "step": 11471
    },
    {
        "loss": 2.4157,
        "grad_norm": 2.961005210876465,
        "learning_rate": 0.00016078803253172645,
        "epoch": 1.5295999999999998,
        "step": 11472
    },
    {
        "loss": 1.5724,
        "grad_norm": 2.553164005279541,
        "learning_rate": 0.0001607380469145613,
        "epoch": 1.5297333333333332,
        "step": 11473
    },
    {
        "loss": 2.9366,
        "grad_norm": 2.7016477584838867,
        "learning_rate": 0.00016068803723884878,
        "epoch": 1.5298666666666667,
        "step": 11474
    },
    {
        "loss": 2.6119,
        "grad_norm": 3.3804197311401367,
        "learning_rate": 0.00016063800352439782,
        "epoch": 1.53,
        "step": 11475
    },
    {
        "loss": 2.5389,
        "grad_norm": 2.7212717533111572,
        "learning_rate": 0.00016058794579102713,
        "epoch": 1.5301333333333333,
        "step": 11476
    },
    {
        "loss": 1.3442,
        "grad_norm": 7.543001174926758,
        "learning_rate": 0.00016053786405856467,
        "epoch": 1.5302666666666667,
        "step": 11477
    },
    {
        "loss": 0.7902,
        "grad_norm": 3.1202237606048584,
        "learning_rate": 0.00016048775834684803,
        "epoch": 1.5304,
        "step": 11478
    },
    {
        "loss": 2.4165,
        "grad_norm": 3.6891305446624756,
        "learning_rate": 0.00016043762867572404,
        "epoch": 1.5305333333333333,
        "step": 11479
    },
    {
        "loss": 1.6146,
        "grad_norm": 4.399562835693359,
        "learning_rate": 0.0001603874750650494,
        "epoch": 1.5306666666666666,
        "step": 11480
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.7586209774017334,
        "learning_rate": 0.00016033729753469025,
        "epoch": 1.5308000000000002,
        "step": 11481
    },
    {
        "loss": 2.1099,
        "grad_norm": 3.9586586952209473,
        "learning_rate": 0.00016028709610452192,
        "epoch": 1.5309333333333335,
        "step": 11482
    },
    {
        "loss": 2.8903,
        "grad_norm": 4.903591156005859,
        "learning_rate": 0.00016023687079442945,
        "epoch": 1.5310666666666668,
        "step": 11483
    },
    {
        "loss": 1.4223,
        "grad_norm": 4.532008647918701,
        "learning_rate": 0.00016018662162430706,
        "epoch": 1.5312000000000001,
        "step": 11484
    },
    {
        "loss": 2.5852,
        "grad_norm": 7.585771083831787,
        "learning_rate": 0.00016013634861405888,
        "epoch": 1.5313333333333334,
        "step": 11485
    },
    {
        "loss": 1.4276,
        "grad_norm": 3.8815927505493164,
        "learning_rate": 0.00016008605178359812,
        "epoch": 1.5314666666666668,
        "step": 11486
    },
    {
        "loss": 1.9815,
        "grad_norm": 3.577732563018799,
        "learning_rate": 0.0001600357311528475,
        "epoch": 1.5316,
        "step": 11487
    },
    {
        "loss": 2.3479,
        "grad_norm": 2.2790212631225586,
        "learning_rate": 0.00015998538674173905,
        "epoch": 1.5317333333333334,
        "step": 11488
    },
    {
        "loss": 2.6539,
        "grad_norm": 3.590785026550293,
        "learning_rate": 0.00015993501857021456,
        "epoch": 1.5318666666666667,
        "step": 11489
    },
    {
        "loss": 2.5742,
        "grad_norm": 3.636375665664673,
        "learning_rate": 0.00015988462665822523,
        "epoch": 1.532,
        "step": 11490
    },
    {
        "loss": 2.505,
        "grad_norm": 4.416572570800781,
        "learning_rate": 0.00015983421102573096,
        "epoch": 1.5321333333333333,
        "step": 11491
    },
    {
        "loss": 2.012,
        "grad_norm": 4.574957370758057,
        "learning_rate": 0.00015978377169270197,
        "epoch": 1.5322666666666667,
        "step": 11492
    },
    {
        "loss": 2.1113,
        "grad_norm": 3.986009359359741,
        "learning_rate": 0.0001597333086791172,
        "epoch": 1.5324,
        "step": 11493
    },
    {
        "loss": 1.2168,
        "grad_norm": 2.3310585021972656,
        "learning_rate": 0.00015968282200496542,
        "epoch": 1.5325333333333333,
        "step": 11494
    },
    {
        "loss": 2.7101,
        "grad_norm": 2.8079752922058105,
        "learning_rate": 0.00015963231169024446,
        "epoch": 1.5326666666666666,
        "step": 11495
    },
    {
        "loss": 2.1226,
        "grad_norm": 4.201146602630615,
        "learning_rate": 0.00015958177775496165,
        "epoch": 1.5328,
        "step": 11496
    },
    {
        "loss": 1.7335,
        "grad_norm": 3.8167531490325928,
        "learning_rate": 0.0001595312202191336,
        "epoch": 1.5329333333333333,
        "step": 11497
    },
    {
        "loss": 2.0379,
        "grad_norm": 3.6753923892974854,
        "learning_rate": 0.0001594806391027864,
        "epoch": 1.5330666666666666,
        "step": 11498
    },
    {
        "loss": 2.6975,
        "grad_norm": 3.094794511795044,
        "learning_rate": 0.00015943003442595542,
        "epoch": 1.5332,
        "step": 11499
    },
    {
        "loss": 2.665,
        "grad_norm": 3.659865617752075,
        "learning_rate": 0.0001593794062086852,
        "epoch": 1.5333333333333332,
        "step": 11500
    },
    {
        "loss": 2.2294,
        "grad_norm": 2.9929745197296143,
        "learning_rate": 0.00015932875447102995,
        "epoch": 1.5334666666666665,
        "step": 11501
    },
    {
        "loss": 2.4912,
        "grad_norm": 3.6169867515563965,
        "learning_rate": 0.0001592780792330528,
        "epoch": 1.5335999999999999,
        "step": 11502
    },
    {
        "loss": 2.5859,
        "grad_norm": 4.363094806671143,
        "learning_rate": 0.00015922738051482656,
        "epoch": 1.5337333333333332,
        "step": 11503
    },
    {
        "loss": 1.67,
        "grad_norm": 3.6199183464050293,
        "learning_rate": 0.0001591766583364331,
        "epoch": 1.5338666666666667,
        "step": 11504
    },
    {
        "loss": 1.7779,
        "grad_norm": 3.6824543476104736,
        "learning_rate": 0.00015912591271796363,
        "epoch": 1.534,
        "step": 11505
    },
    {
        "loss": 2.2014,
        "grad_norm": 3.7416293621063232,
        "learning_rate": 0.00015907514367951856,
        "epoch": 1.5341333333333333,
        "step": 11506
    },
    {
        "loss": 1.9365,
        "grad_norm": 3.5498552322387695,
        "learning_rate": 0.00015902435124120793,
        "epoch": 1.5342666666666667,
        "step": 11507
    },
    {
        "loss": 1.1197,
        "grad_norm": 4.911277770996094,
        "learning_rate": 0.00015897353542315058,
        "epoch": 1.5344,
        "step": 11508
    },
    {
        "loss": 2.6106,
        "grad_norm": 3.8221943378448486,
        "learning_rate": 0.00015892269624547486,
        "epoch": 1.5345333333333333,
        "step": 11509
    },
    {
        "loss": 1.9251,
        "grad_norm": 4.148867130279541,
        "learning_rate": 0.00015887183372831844,
        "epoch": 1.5346666666666666,
        "step": 11510
    },
    {
        "loss": 1.0173,
        "grad_norm": 5.495140552520752,
        "learning_rate": 0.0001588209478918281,
        "epoch": 1.5348000000000002,
        "step": 11511
    },
    {
        "loss": 2.2678,
        "grad_norm": 4.813939094543457,
        "learning_rate": 0.0001587700387561598,
        "epoch": 1.5349333333333335,
        "step": 11512
    },
    {
        "loss": 2.1335,
        "grad_norm": 2.725917100906372,
        "learning_rate": 0.00015871910634147893,
        "epoch": 1.5350666666666668,
        "step": 11513
    },
    {
        "loss": 1.4741,
        "grad_norm": 3.957808256149292,
        "learning_rate": 0.00015866815066796,
        "epoch": 1.5352000000000001,
        "step": 11514
    },
    {
        "loss": 2.4872,
        "grad_norm": 3.939836263656616,
        "learning_rate": 0.0001586171717557867,
        "epoch": 1.5353333333333334,
        "step": 11515
    },
    {
        "loss": 2.0458,
        "grad_norm": 5.37699556350708,
        "learning_rate": 0.00015856616962515177,
        "epoch": 1.5354666666666668,
        "step": 11516
    },
    {
        "loss": 2.5035,
        "grad_norm": 3.9051897525787354,
        "learning_rate": 0.00015851514429625766,
        "epoch": 1.5356,
        "step": 11517
    },
    {
        "loss": 1.7886,
        "grad_norm": 3.1350202560424805,
        "learning_rate": 0.00015846409578931537,
        "epoch": 1.5357333333333334,
        "step": 11518
    },
    {
        "loss": 2.9234,
        "grad_norm": 2.160541534423828,
        "learning_rate": 0.00015841302412454575,
        "epoch": 1.5358666666666667,
        "step": 11519
    },
    {
        "loss": 2.9826,
        "grad_norm": 4.001468658447266,
        "learning_rate": 0.00015836192932217803,
        "epoch": 1.536,
        "step": 11520
    },
    {
        "loss": 2.06,
        "grad_norm": 4.777065753936768,
        "learning_rate": 0.00015831081140245124,
        "epoch": 1.5361333333333334,
        "step": 11521
    },
    {
        "loss": 0.7008,
        "grad_norm": 4.001513957977295,
        "learning_rate": 0.00015825967038561345,
        "epoch": 1.5362666666666667,
        "step": 11522
    },
    {
        "loss": 1.9443,
        "grad_norm": 3.753875732421875,
        "learning_rate": 0.00015820850629192173,
        "epoch": 1.5364,
        "step": 11523
    },
    {
        "loss": 1.354,
        "grad_norm": 5.1214118003845215,
        "learning_rate": 0.0001581573191416423,
        "epoch": 1.5365333333333333,
        "step": 11524
    },
    {
        "loss": 2.3243,
        "grad_norm": 4.083398818969727,
        "learning_rate": 0.0001581061089550505,
        "epoch": 1.5366666666666666,
        "step": 11525
    },
    {
        "loss": 2.8503,
        "grad_norm": 4.3161749839782715,
        "learning_rate": 0.00015805487575243108,
        "epoch": 1.5368,
        "step": 11526
    },
    {
        "loss": 2.7183,
        "grad_norm": 3.079904794692993,
        "learning_rate": 0.0001580036195540774,
        "epoch": 1.5369333333333333,
        "step": 11527
    },
    {
        "loss": 1.4321,
        "grad_norm": 4.773504257202148,
        "learning_rate": 0.00015795234038029263,
        "epoch": 1.5370666666666666,
        "step": 11528
    },
    {
        "loss": 2.8591,
        "grad_norm": 5.003594398498535,
        "learning_rate": 0.00015790103825138815,
        "epoch": 1.5372,
        "step": 11529
    },
    {
        "loss": 2.4874,
        "grad_norm": 3.4952263832092285,
        "learning_rate": 0.00015784971318768512,
        "epoch": 1.5373333333333332,
        "step": 11530
    },
    {
        "loss": 2.6594,
        "grad_norm": 4.737109661102295,
        "learning_rate": 0.00015779836520951368,
        "epoch": 1.5374666666666665,
        "step": 11531
    },
    {
        "loss": 2.5582,
        "grad_norm": 4.335854530334473,
        "learning_rate": 0.00015774699433721283,
        "epoch": 1.5375999999999999,
        "step": 11532
    },
    {
        "loss": 1.2394,
        "grad_norm": 5.406049728393555,
        "learning_rate": 0.00015769560059113074,
        "epoch": 1.5377333333333332,
        "step": 11533
    },
    {
        "loss": 1.7225,
        "grad_norm": 4.459497928619385,
        "learning_rate": 0.00015764418399162459,
        "epoch": 1.5378666666666667,
        "step": 11534
    },
    {
        "loss": 2.2745,
        "grad_norm": 3.9238200187683105,
        "learning_rate": 0.00015759274455906085,
        "epoch": 1.538,
        "step": 11535
    },
    {
        "loss": 2.5981,
        "grad_norm": 3.7453458309173584,
        "learning_rate": 0.00015754128231381474,
        "epoch": 1.5381333333333334,
        "step": 11536
    },
    {
        "loss": 2.4256,
        "grad_norm": 2.6337087154388428,
        "learning_rate": 0.00015748979727627063,
        "epoch": 1.5382666666666667,
        "step": 11537
    },
    {
        "loss": 2.7033,
        "grad_norm": 4.770684242248535,
        "learning_rate": 0.00015743828946682183,
        "epoch": 1.5384,
        "step": 11538
    },
    {
        "loss": 2.3157,
        "grad_norm": 2.632141351699829,
        "learning_rate": 0.00015738675890587084,
        "epoch": 1.5385333333333333,
        "step": 11539
    },
    {
        "loss": 2.0977,
        "grad_norm": 3.814967155456543,
        "learning_rate": 0.00015733520561382922,
        "epoch": 1.5386666666666666,
        "step": 11540
    },
    {
        "loss": 2.2211,
        "grad_norm": 4.491898536682129,
        "learning_rate": 0.00015728362961111732,
        "epoch": 1.5388,
        "step": 11541
    },
    {
        "loss": 2.5853,
        "grad_norm": 4.167994499206543,
        "learning_rate": 0.00015723203091816453,
        "epoch": 1.5389333333333335,
        "step": 11542
    },
    {
        "loss": 1.4253,
        "grad_norm": 4.684764862060547,
        "learning_rate": 0.00015718040955540915,
        "epoch": 1.5390666666666668,
        "step": 11543
    },
    {
        "loss": 1.5052,
        "grad_norm": 5.196969985961914,
        "learning_rate": 0.00015712876554329884,
        "epoch": 1.5392000000000001,
        "step": 11544
    },
    {
        "loss": 2.3386,
        "grad_norm": 3.9751782417297363,
        "learning_rate": 0.0001570770989022898,
        "epoch": 1.5393333333333334,
        "step": 11545
    },
    {
        "loss": 2.3007,
        "grad_norm": 3.9024481773376465,
        "learning_rate": 0.00015702540965284743,
        "epoch": 1.5394666666666668,
        "step": 11546
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.306281805038452,
        "learning_rate": 0.00015697369781544587,
        "epoch": 1.5396,
        "step": 11547
    },
    {
        "loss": 2.3279,
        "grad_norm": 4.187707424163818,
        "learning_rate": 0.0001569219634105685,
        "epoch": 1.5397333333333334,
        "step": 11548
    },
    {
        "loss": 2.6062,
        "grad_norm": 3.1970467567443848,
        "learning_rate": 0.00015687020645870761,
        "epoch": 1.5398666666666667,
        "step": 11549
    },
    {
        "loss": 2.4314,
        "grad_norm": 3.1279261112213135,
        "learning_rate": 0.000156818426980364,
        "epoch": 1.54,
        "step": 11550
    },
    {
        "loss": 0.8863,
        "grad_norm": 3.776789426803589,
        "learning_rate": 0.00015676662499604797,
        "epoch": 1.5401333333333334,
        "step": 11551
    },
    {
        "loss": 1.6115,
        "grad_norm": 4.764410972595215,
        "learning_rate": 0.00015671480052627823,
        "epoch": 1.5402666666666667,
        "step": 11552
    },
    {
        "loss": 2.0511,
        "grad_norm": 4.004251003265381,
        "learning_rate": 0.00015666295359158285,
        "epoch": 1.5404,
        "step": 11553
    },
    {
        "loss": 2.1466,
        "grad_norm": 2.9631080627441406,
        "learning_rate": 0.00015661108421249851,
        "epoch": 1.5405333333333333,
        "step": 11554
    },
    {
        "loss": 2.632,
        "grad_norm": 3.406775712966919,
        "learning_rate": 0.00015655919240957077,
        "epoch": 1.5406666666666666,
        "step": 11555
    },
    {
        "loss": 0.9316,
        "grad_norm": 4.786859035491943,
        "learning_rate": 0.00015650727820335414,
        "epoch": 1.5408,
        "step": 11556
    },
    {
        "loss": 2.5298,
        "grad_norm": 3.0821433067321777,
        "learning_rate": 0.00015645534161441214,
        "epoch": 1.5409333333333333,
        "step": 11557
    },
    {
        "loss": 1.6745,
        "grad_norm": 4.496963024139404,
        "learning_rate": 0.00015640338266331697,
        "epoch": 1.5410666666666666,
        "step": 11558
    },
    {
        "loss": 0.7882,
        "grad_norm": 3.8987510204315186,
        "learning_rate": 0.0001563514013706496,
        "epoch": 1.5412,
        "step": 11559
    },
    {
        "loss": 2.7215,
        "grad_norm": 4.1136860847473145,
        "learning_rate": 0.00015629939775700029,
        "epoch": 1.5413333333333332,
        "step": 11560
    },
    {
        "loss": 1.7386,
        "grad_norm": 5.1026716232299805,
        "learning_rate": 0.00015624737184296756,
        "epoch": 1.5414666666666665,
        "step": 11561
    },
    {
        "loss": 1.9958,
        "grad_norm": 5.014944076538086,
        "learning_rate": 0.0001561953236491593,
        "epoch": 1.5415999999999999,
        "step": 11562
    },
    {
        "loss": 2.0123,
        "grad_norm": 3.432112693786621,
        "learning_rate": 0.00015614325319619188,
        "epoch": 1.5417333333333332,
        "step": 11563
    },
    {
        "loss": 2.8768,
        "grad_norm": 3.3083159923553467,
        "learning_rate": 0.00015609116050469056,
        "epoch": 1.5418666666666667,
        "step": 11564
    },
    {
        "loss": 2.0032,
        "grad_norm": 4.864651203155518,
        "learning_rate": 0.00015603904559528928,
        "epoch": 1.542,
        "step": 11565
    },
    {
        "loss": 2.9964,
        "grad_norm": 6.459137439727783,
        "learning_rate": 0.00015598690848863124,
        "epoch": 1.5421333333333334,
        "step": 11566
    },
    {
        "loss": 2.5251,
        "grad_norm": 3.6252074241638184,
        "learning_rate": 0.00015593474920536798,
        "epoch": 1.5422666666666667,
        "step": 11567
    },
    {
        "loss": 2.0157,
        "grad_norm": 5.363121509552002,
        "learning_rate": 0.00015588256776615986,
        "epoch": 1.5424,
        "step": 11568
    },
    {
        "loss": 2.1566,
        "grad_norm": 4.281491756439209,
        "learning_rate": 0.00015583036419167633,
        "epoch": 1.5425333333333333,
        "step": 11569
    },
    {
        "loss": 2.4759,
        "grad_norm": 4.0578293800354,
        "learning_rate": 0.0001557781385025953,
        "epoch": 1.5426666666666666,
        "step": 11570
    },
    {
        "loss": 2.4143,
        "grad_norm": 3.383812665939331,
        "learning_rate": 0.00015572589071960344,
        "epoch": 1.5428,
        "step": 11571
    },
    {
        "loss": 2.0556,
        "grad_norm": 3.4518401622772217,
        "learning_rate": 0.00015567362086339653,
        "epoch": 1.5429333333333335,
        "step": 11572
    },
    {
        "loss": 2.3852,
        "grad_norm": 3.3588688373565674,
        "learning_rate": 0.00015562132895467868,
        "epoch": 1.5430666666666668,
        "step": 11573
    },
    {
        "loss": 1.9249,
        "grad_norm": 3.626692533493042,
        "learning_rate": 0.0001555690150141629,
        "epoch": 1.5432000000000001,
        "step": 11574
    },
    {
        "loss": 2.0613,
        "grad_norm": 2.871962308883667,
        "learning_rate": 0.0001555166790625708,
        "epoch": 1.5433333333333334,
        "step": 11575
    },
    {
        "loss": 0.7849,
        "grad_norm": 4.458282470703125,
        "learning_rate": 0.00015546432112063308,
        "epoch": 1.5434666666666668,
        "step": 11576
    },
    {
        "loss": 2.0729,
        "grad_norm": 3.1800029277801514,
        "learning_rate": 0.00015541194120908863,
        "epoch": 1.5436,
        "step": 11577
    },
    {
        "loss": 2.0838,
        "grad_norm": 5.545621871948242,
        "learning_rate": 0.0001553595393486857,
        "epoch": 1.5437333333333334,
        "step": 11578
    },
    {
        "loss": 2.5097,
        "grad_norm": 5.350877285003662,
        "learning_rate": 0.0001553071155601804,
        "epoch": 1.5438666666666667,
        "step": 11579
    },
    {
        "loss": 3.1946,
        "grad_norm": 4.7710418701171875,
        "learning_rate": 0.00015525466986433814,
        "epoch": 1.544,
        "step": 11580
    },
    {
        "loss": 2.4594,
        "grad_norm": 2.7538979053497314,
        "learning_rate": 0.000155202202281933,
        "epoch": 1.5441333333333334,
        "step": 11581
    },
    {
        "loss": 1.202,
        "grad_norm": 3.664687156677246,
        "learning_rate": 0.00015514971283374745,
        "epoch": 1.5442666666666667,
        "step": 11582
    },
    {
        "loss": 1.5592,
        "grad_norm": 2.693023443222046,
        "learning_rate": 0.00015509720154057273,
        "epoch": 1.5444,
        "step": 11583
    },
    {
        "loss": 2.5278,
        "grad_norm": 4.794522285461426,
        "learning_rate": 0.00015504466842320856,
        "epoch": 1.5445333333333333,
        "step": 11584
    },
    {
        "loss": 1.4924,
        "grad_norm": 3.793360710144043,
        "learning_rate": 0.00015499211350246387,
        "epoch": 1.5446666666666666,
        "step": 11585
    },
    {
        "loss": 2.2474,
        "grad_norm": 3.5965332984924316,
        "learning_rate": 0.00015493953679915546,
        "epoch": 1.5448,
        "step": 11586
    },
    {
        "loss": 2.0357,
        "grad_norm": 6.728110313415527,
        "learning_rate": 0.0001548869383341096,
        "epoch": 1.5449333333333333,
        "step": 11587
    },
    {
        "loss": 2.3328,
        "grad_norm": 3.2465786933898926,
        "learning_rate": 0.0001548343181281602,
        "epoch": 1.5450666666666666,
        "step": 11588
    },
    {
        "loss": 2.1739,
        "grad_norm": 4.453670978546143,
        "learning_rate": 0.00015478167620215053,
        "epoch": 1.5452,
        "step": 11589
    },
    {
        "loss": 1.7949,
        "grad_norm": 3.494990825653076,
        "learning_rate": 0.0001547290125769324,
        "epoch": 1.5453333333333332,
        "step": 11590
    },
    {
        "loss": 1.4706,
        "grad_norm": 3.853494882583618,
        "learning_rate": 0.00015467632727336587,
        "epoch": 1.5454666666666665,
        "step": 11591
    },
    {
        "loss": 2.1561,
        "grad_norm": 3.7448384761810303,
        "learning_rate": 0.00015462362031231982,
        "epoch": 1.5455999999999999,
        "step": 11592
    },
    {
        "loss": 2.8102,
        "grad_norm": 3.3488316535949707,
        "learning_rate": 0.0001545708917146715,
        "epoch": 1.5457333333333332,
        "step": 11593
    },
    {
        "loss": 2.218,
        "grad_norm": 3.1325201988220215,
        "learning_rate": 0.0001545181415013072,
        "epoch": 1.5458666666666665,
        "step": 11594
    },
    {
        "loss": 2.6083,
        "grad_norm": 2.958386182785034,
        "learning_rate": 0.00015446536969312123,
        "epoch": 1.546,
        "step": 11595
    },
    {
        "loss": 1.5667,
        "grad_norm": 3.9988486766815186,
        "learning_rate": 0.00015441257631101674,
        "epoch": 1.5461333333333334,
        "step": 11596
    },
    {
        "loss": 1.7016,
        "grad_norm": 3.9857232570648193,
        "learning_rate": 0.00015435976137590527,
        "epoch": 1.5462666666666667,
        "step": 11597
    },
    {
        "loss": 1.7641,
        "grad_norm": 2.8327507972717285,
        "learning_rate": 0.0001543069249087071,
        "epoch": 1.5464,
        "step": 11598
    },
    {
        "loss": 1.4913,
        "grad_norm": 4.1506171226501465,
        "learning_rate": 0.00015425406693035114,
        "epoch": 1.5465333333333333,
        "step": 11599
    },
    {
        "loss": 2.1232,
        "grad_norm": 4.071052074432373,
        "learning_rate": 0.00015420118746177418,
        "epoch": 1.5466666666666666,
        "step": 11600
    },
    {
        "loss": 2.7486,
        "grad_norm": 2.7184741497039795,
        "learning_rate": 0.0001541482865239223,
        "epoch": 1.5468,
        "step": 11601
    },
    {
        "loss": 2.152,
        "grad_norm": 4.47882604598999,
        "learning_rate": 0.00015409536413774948,
        "epoch": 1.5469333333333335,
        "step": 11602
    },
    {
        "loss": 1.73,
        "grad_norm": 3.532167673110962,
        "learning_rate": 0.00015404242032421875,
        "epoch": 1.5470666666666668,
        "step": 11603
    },
    {
        "loss": 2.0017,
        "grad_norm": 3.1941943168640137,
        "learning_rate": 0.00015398945510430116,
        "epoch": 1.5472000000000001,
        "step": 11604
    },
    {
        "loss": 2.5952,
        "grad_norm": 3.469799757003784,
        "learning_rate": 0.00015393646849897646,
        "epoch": 1.5473333333333334,
        "step": 11605
    },
    {
        "loss": 2.3269,
        "grad_norm": 2.9752702713012695,
        "learning_rate": 0.00015388346052923267,
        "epoch": 1.5474666666666668,
        "step": 11606
    },
    {
        "loss": 1.3888,
        "grad_norm": 3.66104793548584,
        "learning_rate": 0.00015383043121606658,
        "epoch": 1.5476,
        "step": 11607
    },
    {
        "loss": 1.6055,
        "grad_norm": 5.562140941619873,
        "learning_rate": 0.00015377738058048346,
        "epoch": 1.5477333333333334,
        "step": 11608
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.8098011016845703,
        "learning_rate": 0.00015372430864349647,
        "epoch": 1.5478666666666667,
        "step": 11609
    },
    {
        "loss": 1.3617,
        "grad_norm": 9.457006454467773,
        "learning_rate": 0.0001536712154261278,
        "epoch": 1.548,
        "step": 11610
    },
    {
        "loss": 1.8122,
        "grad_norm": 2.6180293560028076,
        "learning_rate": 0.00015361810094940777,
        "epoch": 1.5481333333333334,
        "step": 11611
    },
    {
        "loss": 1.8657,
        "grad_norm": 3.9596049785614014,
        "learning_rate": 0.0001535649652343754,
        "epoch": 1.5482666666666667,
        "step": 11612
    },
    {
        "loss": 3.0632,
        "grad_norm": 4.431568622589111,
        "learning_rate": 0.00015351180830207772,
        "epoch": 1.5484,
        "step": 11613
    },
    {
        "loss": 1.9926,
        "grad_norm": 3.7488455772399902,
        "learning_rate": 0.00015345863017357045,
        "epoch": 1.5485333333333333,
        "step": 11614
    },
    {
        "loss": 2.4379,
        "grad_norm": 4.5169267654418945,
        "learning_rate": 0.0001534054308699175,
        "epoch": 1.5486666666666666,
        "step": 11615
    },
    {
        "loss": 2.3959,
        "grad_norm": 5.948934078216553,
        "learning_rate": 0.00015335221041219151,
        "epoch": 1.5488,
        "step": 11616
    },
    {
        "loss": 2.2978,
        "grad_norm": 3.0901448726654053,
        "learning_rate": 0.00015329896882147322,
        "epoch": 1.5489333333333333,
        "step": 11617
    },
    {
        "loss": 1.3642,
        "grad_norm": 4.564600467681885,
        "learning_rate": 0.0001532457061188516,
        "epoch": 1.5490666666666666,
        "step": 11618
    },
    {
        "loss": 2.2887,
        "grad_norm": 4.029095649719238,
        "learning_rate": 0.0001531924223254245,
        "epoch": 1.5492,
        "step": 11619
    },
    {
        "loss": 2.1974,
        "grad_norm": 4.198141574859619,
        "learning_rate": 0.00015313911746229754,
        "epoch": 1.5493333333333332,
        "step": 11620
    },
    {
        "loss": 2.3444,
        "grad_norm": 4.017580986022949,
        "learning_rate": 0.0001530857915505852,
        "epoch": 1.5494666666666665,
        "step": 11621
    },
    {
        "loss": 2.2774,
        "grad_norm": 4.164405822753906,
        "learning_rate": 0.00015303244461140997,
        "epoch": 1.5495999999999999,
        "step": 11622
    },
    {
        "loss": 2.326,
        "grad_norm": 3.851679801940918,
        "learning_rate": 0.0001529790766659027,
        "epoch": 1.5497333333333332,
        "step": 11623
    },
    {
        "loss": 2.3889,
        "grad_norm": 3.3237414360046387,
        "learning_rate": 0.00015292568773520254,
        "epoch": 1.5498666666666665,
        "step": 11624
    },
    {
        "loss": 2.4973,
        "grad_norm": 3.681849241256714,
        "learning_rate": 0.00015287227784045726,
        "epoch": 1.55,
        "step": 11625
    },
    {
        "loss": 2.0408,
        "grad_norm": 2.834672212600708,
        "learning_rate": 0.00015281884700282255,
        "epoch": 1.5501333333333334,
        "step": 11626
    },
    {
        "loss": 0.6195,
        "grad_norm": 4.031725883483887,
        "learning_rate": 0.00015276539524346246,
        "epoch": 1.5502666666666667,
        "step": 11627
    },
    {
        "loss": 1.5096,
        "grad_norm": 5.038833141326904,
        "learning_rate": 0.0001527119225835496,
        "epoch": 1.5504,
        "step": 11628
    },
    {
        "loss": 2.3899,
        "grad_norm": 3.0205724239349365,
        "learning_rate": 0.00015265842904426465,
        "epoch": 1.5505333333333333,
        "step": 11629
    },
    {
        "loss": 2.8463,
        "grad_norm": 3.4214987754821777,
        "learning_rate": 0.00015260491464679635,
        "epoch": 1.5506666666666666,
        "step": 11630
    },
    {
        "loss": 1.7368,
        "grad_norm": 5.178180694580078,
        "learning_rate": 0.00015255137941234227,
        "epoch": 1.5508,
        "step": 11631
    },
    {
        "loss": 2.311,
        "grad_norm": 3.795985221862793,
        "learning_rate": 0.00015249782336210771,
        "epoch": 1.5509333333333335,
        "step": 11632
    },
    {
        "loss": 2.2238,
        "grad_norm": 3.2543857097625732,
        "learning_rate": 0.00015244424651730647,
        "epoch": 1.5510666666666668,
        "step": 11633
    },
    {
        "loss": 1.6573,
        "grad_norm": 3.8653757572174072,
        "learning_rate": 0.00015239064889916037,
        "epoch": 1.5512000000000001,
        "step": 11634
    },
    {
        "loss": 2.4438,
        "grad_norm": 5.043446063995361,
        "learning_rate": 0.0001523370305288998,
        "epoch": 1.5513333333333335,
        "step": 11635
    },
    {
        "loss": 1.9289,
        "grad_norm": 5.867278099060059,
        "learning_rate": 0.00015228339142776302,
        "epoch": 1.5514666666666668,
        "step": 11636
    },
    {
        "loss": 2.4254,
        "grad_norm": 3.6551051139831543,
        "learning_rate": 0.00015222973161699697,
        "epoch": 1.5516,
        "step": 11637
    },
    {
        "loss": 2.3771,
        "grad_norm": 4.193419456481934,
        "learning_rate": 0.00015217605111785603,
        "epoch": 1.5517333333333334,
        "step": 11638
    },
    {
        "loss": 1.8415,
        "grad_norm": 2.6246936321258545,
        "learning_rate": 0.00015212234995160343,
        "epoch": 1.5518666666666667,
        "step": 11639
    },
    {
        "loss": 1.9618,
        "grad_norm": 3.8589766025543213,
        "learning_rate": 0.00015206862813951056,
        "epoch": 1.552,
        "step": 11640
    },
    {
        "loss": 1.3422,
        "grad_norm": 3.7506730556488037,
        "learning_rate": 0.00015201488570285664,
        "epoch": 1.5521333333333334,
        "step": 11641
    },
    {
        "loss": 2.4198,
        "grad_norm": 4.817918300628662,
        "learning_rate": 0.00015196112266292928,
        "epoch": 1.5522666666666667,
        "step": 11642
    },
    {
        "loss": 2.0138,
        "grad_norm": 4.070659160614014,
        "learning_rate": 0.00015190733904102402,
        "epoch": 1.5524,
        "step": 11643
    },
    {
        "loss": 1.693,
        "grad_norm": 4.923677444458008,
        "learning_rate": 0.00015185353485844507,
        "epoch": 1.5525333333333333,
        "step": 11644
    },
    {
        "loss": 1.6961,
        "grad_norm": 4.55946159362793,
        "learning_rate": 0.0001517997101365041,
        "epoch": 1.5526666666666666,
        "step": 11645
    },
    {
        "loss": 2.7104,
        "grad_norm": 4.2061567306518555,
        "learning_rate": 0.00015174586489652173,
        "epoch": 1.5528,
        "step": 11646
    },
    {
        "loss": 1.7886,
        "grad_norm": 4.060473918914795,
        "learning_rate": 0.0001516919991598257,
        "epoch": 1.5529333333333333,
        "step": 11647
    },
    {
        "loss": 1.488,
        "grad_norm": 3.2578203678131104,
        "learning_rate": 0.00015163811294775266,
        "epoch": 1.5530666666666666,
        "step": 11648
    },
    {
        "loss": 2.824,
        "grad_norm": 3.3099844455718994,
        "learning_rate": 0.00015158420628164724,
        "epoch": 1.5532,
        "step": 11649
    },
    {
        "loss": 2.413,
        "grad_norm": 3.457289695739746,
        "learning_rate": 0.00015153027918286198,
        "epoch": 1.5533333333333332,
        "step": 11650
    },
    {
        "loss": 2.5464,
        "grad_norm": 2.904447078704834,
        "learning_rate": 0.00015147633167275753,
        "epoch": 1.5534666666666666,
        "step": 11651
    },
    {
        "loss": 1.6702,
        "grad_norm": 2.100400924682617,
        "learning_rate": 0.00015142236377270257,
        "epoch": 1.5535999999999999,
        "step": 11652
    },
    {
        "loss": 0.6662,
        "grad_norm": 3.9645540714263916,
        "learning_rate": 0.0001513683755040742,
        "epoch": 1.5537333333333332,
        "step": 11653
    },
    {
        "loss": 2.1095,
        "grad_norm": 4.060276508331299,
        "learning_rate": 0.00015131436688825732,
        "epoch": 1.5538666666666665,
        "step": 11654
    },
    {
        "loss": 2.3896,
        "grad_norm": 4.068795680999756,
        "learning_rate": 0.00015126033794664483,
        "epoch": 1.554,
        "step": 11655
    },
    {
        "loss": 2.5839,
        "grad_norm": 4.749050140380859,
        "learning_rate": 0.0001512062887006377,
        "epoch": 1.5541333333333334,
        "step": 11656
    },
    {
        "loss": 2.5747,
        "grad_norm": 2.3313965797424316,
        "learning_rate": 0.00015115221917164515,
        "epoch": 1.5542666666666667,
        "step": 11657
    },
    {
        "loss": 1.9944,
        "grad_norm": 4.7595295906066895,
        "learning_rate": 0.0001510981293810845,
        "epoch": 1.5544,
        "step": 11658
    },
    {
        "loss": 2.541,
        "grad_norm": 5.5373945236206055,
        "learning_rate": 0.00015104401935038046,
        "epoch": 1.5545333333333333,
        "step": 11659
    },
    {
        "loss": 1.9933,
        "grad_norm": 4.303831577301025,
        "learning_rate": 0.00015098988910096657,
        "epoch": 1.5546666666666666,
        "step": 11660
    },
    {
        "loss": 2.7436,
        "grad_norm": 3.6026880741119385,
        "learning_rate": 0.0001509357386542837,
        "epoch": 1.5548,
        "step": 11661
    },
    {
        "loss": 2.7487,
        "grad_norm": 3.7548739910125732,
        "learning_rate": 0.00015088156803178134,
        "epoch": 1.5549333333333333,
        "step": 11662
    },
    {
        "loss": 3.0024,
        "grad_norm": 3.7940282821655273,
        "learning_rate": 0.0001508273772549165,
        "epoch": 1.5550666666666668,
        "step": 11663
    },
    {
        "loss": 1.5582,
        "grad_norm": 4.405724048614502,
        "learning_rate": 0.00015077316634515435,
        "epoch": 1.5552000000000001,
        "step": 11664
    },
    {
        "loss": 2.2851,
        "grad_norm": 2.738307476043701,
        "learning_rate": 0.00015071893532396787,
        "epoch": 1.5553333333333335,
        "step": 11665
    },
    {
        "loss": 3.0858,
        "grad_norm": 3.4263248443603516,
        "learning_rate": 0.0001506646842128383,
        "epoch": 1.5554666666666668,
        "step": 11666
    },
    {
        "loss": 2.164,
        "grad_norm": 2.8980600833892822,
        "learning_rate": 0.00015061041303325492,
        "epoch": 1.5556,
        "step": 11667
    },
    {
        "loss": 1.9033,
        "grad_norm": 3.7375071048736572,
        "learning_rate": 0.00015055612180671422,
        "epoch": 1.5557333333333334,
        "step": 11668
    },
    {
        "loss": 1.8749,
        "grad_norm": 2.8571863174438477,
        "learning_rate": 0.00015050181055472156,
        "epoch": 1.5558666666666667,
        "step": 11669
    },
    {
        "loss": 2.7997,
        "grad_norm": 2.7730586528778076,
        "learning_rate": 0.00015044747929878952,
        "epoch": 1.556,
        "step": 11670
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.6261463165283203,
        "learning_rate": 0.00015039312806043915,
        "epoch": 1.5561333333333334,
        "step": 11671
    },
    {
        "loss": 2.0402,
        "grad_norm": 4.071357250213623,
        "learning_rate": 0.00015033875686119904,
        "epoch": 1.5562666666666667,
        "step": 11672
    },
    {
        "loss": 2.488,
        "grad_norm": 3.622194528579712,
        "learning_rate": 0.00015028436572260586,
        "epoch": 1.5564,
        "step": 11673
    },
    {
        "loss": 1.7732,
        "grad_norm": 4.46212911605835,
        "learning_rate": 0.0001502299546662039,
        "epoch": 1.5565333333333333,
        "step": 11674
    },
    {
        "loss": 0.6365,
        "grad_norm": 4.0213303565979,
        "learning_rate": 0.00015017552371354587,
        "epoch": 1.5566666666666666,
        "step": 11675
    },
    {
        "loss": 1.8615,
        "grad_norm": 4.882173538208008,
        "learning_rate": 0.00015012107288619195,
        "epoch": 1.5568,
        "step": 11676
    },
    {
        "loss": 2.3253,
        "grad_norm": 3.9526638984680176,
        "learning_rate": 0.00015006660220571015,
        "epoch": 1.5569333333333333,
        "step": 11677
    },
    {
        "loss": 1.8043,
        "grad_norm": 4.127358436584473,
        "learning_rate": 0.00015001211169367675,
        "epoch": 1.5570666666666666,
        "step": 11678
    },
    {
        "loss": 2.0433,
        "grad_norm": 3.3713831901550293,
        "learning_rate": 0.00014995760137167548,
        "epoch": 1.5572,
        "step": 11679
    },
    {
        "loss": 1.2608,
        "grad_norm": 3.867006778717041,
        "learning_rate": 0.00014990307126129797,
        "epoch": 1.5573333333333332,
        "step": 11680
    },
    {
        "loss": 1.9344,
        "grad_norm": 5.1042022705078125,
        "learning_rate": 0.000149848521384144,
        "epoch": 1.5574666666666666,
        "step": 11681
    },
    {
        "loss": 2.3464,
        "grad_norm": 4.11940860748291,
        "learning_rate": 0.00014979395176182084,
        "epoch": 1.5575999999999999,
        "step": 11682
    },
    {
        "loss": 0.8996,
        "grad_norm": 4.521748065948486,
        "learning_rate": 0.00014973936241594363,
        "epoch": 1.5577333333333332,
        "step": 11683
    },
    {
        "loss": 1.6377,
        "grad_norm": 3.4149866104125977,
        "learning_rate": 0.00014968475336813561,
        "epoch": 1.5578666666666665,
        "step": 11684
    },
    {
        "loss": 2.2263,
        "grad_norm": 3.96539044380188,
        "learning_rate": 0.00014963012464002748,
        "epoch": 1.558,
        "step": 11685
    },
    {
        "loss": 1.9504,
        "grad_norm": 4.31279993057251,
        "learning_rate": 0.00014957547625325771,
        "epoch": 1.5581333333333334,
        "step": 11686
    },
    {
        "loss": 2.0071,
        "grad_norm": 3.590712547302246,
        "learning_rate": 0.00014952080822947303,
        "epoch": 1.5582666666666667,
        "step": 11687
    },
    {
        "loss": 1.5418,
        "grad_norm": 3.7460999488830566,
        "learning_rate": 0.00014946612059032747,
        "epoch": 1.5584,
        "step": 11688
    },
    {
        "loss": 2.7975,
        "grad_norm": 3.889681339263916,
        "learning_rate": 0.00014941141335748286,
        "epoch": 1.5585333333333333,
        "step": 11689
    },
    {
        "loss": 2.5542,
        "grad_norm": 2.689669132232666,
        "learning_rate": 0.00014935668655260918,
        "epoch": 1.5586666666666666,
        "step": 11690
    },
    {
        "loss": 2.4766,
        "grad_norm": 4.254390716552734,
        "learning_rate": 0.00014930194019738377,
        "epoch": 1.5588,
        "step": 11691
    },
    {
        "loss": 1.3441,
        "grad_norm": 4.541999816894531,
        "learning_rate": 0.00014924717431349187,
        "epoch": 1.5589333333333333,
        "step": 11692
    },
    {
        "loss": 1.5441,
        "grad_norm": 2.6759042739868164,
        "learning_rate": 0.00014919238892262629,
        "epoch": 1.5590666666666668,
        "step": 11693
    },
    {
        "loss": 1.2368,
        "grad_norm": 3.588710069656372,
        "learning_rate": 0.00014913758404648796,
        "epoch": 1.5592000000000001,
        "step": 11694
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.5998194217681885,
        "learning_rate": 0.00014908275970678504,
        "epoch": 1.5593333333333335,
        "step": 11695
    },
    {
        "loss": 2.2193,
        "grad_norm": 2.860020399093628,
        "learning_rate": 0.000149027915925234,
        "epoch": 1.5594666666666668,
        "step": 11696
    },
    {
        "loss": 2.9481,
        "grad_norm": 3.909686326980591,
        "learning_rate": 0.00014897305272355813,
        "epoch": 1.5596,
        "step": 11697
    },
    {
        "loss": 2.7756,
        "grad_norm": 2.9228429794311523,
        "learning_rate": 0.00014891817012348924,
        "epoch": 1.5597333333333334,
        "step": 11698
    },
    {
        "loss": 2.4801,
        "grad_norm": 5.5293965339660645,
        "learning_rate": 0.0001488632681467666,
        "epoch": 1.5598666666666667,
        "step": 11699
    },
    {
        "loss": 1.3073,
        "grad_norm": 2.3971481323242188,
        "learning_rate": 0.00014880834681513694,
        "epoch": 1.56,
        "step": 11700
    },
    {
        "loss": 0.7378,
        "grad_norm": 4.815925598144531,
        "learning_rate": 0.00014875340615035478,
        "epoch": 1.5601333333333334,
        "step": 11701
    },
    {
        "loss": 2.1716,
        "grad_norm": 3.3591744899749756,
        "learning_rate": 0.00014869844617418221,
        "epoch": 1.5602666666666667,
        "step": 11702
    },
    {
        "loss": 2.7651,
        "grad_norm": 3.443204164505005,
        "learning_rate": 0.00014864346690838932,
        "epoch": 1.5604,
        "step": 11703
    },
    {
        "loss": 1.4503,
        "grad_norm": 3.4820897579193115,
        "learning_rate": 0.0001485884683747533,
        "epoch": 1.5605333333333333,
        "step": 11704
    },
    {
        "loss": 1.7111,
        "grad_norm": 5.7894287109375,
        "learning_rate": 0.00014853345059505962,
        "epoch": 1.5606666666666666,
        "step": 11705
    },
    {
        "loss": 1.3927,
        "grad_norm": 4.119943618774414,
        "learning_rate": 0.00014847841359110056,
        "epoch": 1.5608,
        "step": 11706
    },
    {
        "loss": 1.8509,
        "grad_norm": 4.72048282623291,
        "learning_rate": 0.0001484233573846767,
        "epoch": 1.5609333333333333,
        "step": 11707
    },
    {
        "loss": 2.8649,
        "grad_norm": 3.86967134475708,
        "learning_rate": 0.0001483682819975961,
        "epoch": 1.5610666666666666,
        "step": 11708
    },
    {
        "loss": 2.6297,
        "grad_norm": 4.429858207702637,
        "learning_rate": 0.00014831318745167418,
        "epoch": 1.5612,
        "step": 11709
    },
    {
        "loss": 1.4101,
        "grad_norm": 4.967990875244141,
        "learning_rate": 0.00014825807376873408,
        "epoch": 1.5613333333333332,
        "step": 11710
    },
    {
        "loss": 1.86,
        "grad_norm": 4.207316875457764,
        "learning_rate": 0.00014820294097060642,
        "epoch": 1.5614666666666666,
        "step": 11711
    },
    {
        "loss": 2.5982,
        "grad_norm": 3.5550789833068848,
        "learning_rate": 0.0001481477890791297,
        "epoch": 1.5615999999999999,
        "step": 11712
    },
    {
        "loss": 2.536,
        "grad_norm": 4.101327419281006,
        "learning_rate": 0.00014809261811614969,
        "epoch": 1.5617333333333332,
        "step": 11713
    },
    {
        "loss": 2.6579,
        "grad_norm": 4.526012420654297,
        "learning_rate": 0.00014803742810351974,
        "epoch": 1.5618666666666665,
        "step": 11714
    },
    {
        "loss": 1.4333,
        "grad_norm": 4.110405921936035,
        "learning_rate": 0.00014798221906310073,
        "epoch": 1.562,
        "step": 11715
    },
    {
        "loss": 2.2049,
        "grad_norm": 4.938425540924072,
        "learning_rate": 0.00014792699101676127,
        "epoch": 1.5621333333333334,
        "step": 11716
    },
    {
        "loss": 1.8424,
        "grad_norm": 2.9170069694519043,
        "learning_rate": 0.00014787174398637757,
        "epoch": 1.5622666666666667,
        "step": 11717
    },
    {
        "loss": 2.7129,
        "grad_norm": 3.7703096866607666,
        "learning_rate": 0.00014781647799383274,
        "epoch": 1.5624,
        "step": 11718
    },
    {
        "loss": 2.3941,
        "grad_norm": 5.508823394775391,
        "learning_rate": 0.00014776119306101818,
        "epoch": 1.5625333333333333,
        "step": 11719
    },
    {
        "loss": 1.1835,
        "grad_norm": 4.799806594848633,
        "learning_rate": 0.00014770588920983216,
        "epoch": 1.5626666666666666,
        "step": 11720
    },
    {
        "loss": 1.3411,
        "grad_norm": 5.204471111297607,
        "learning_rate": 0.000147650566462181,
        "epoch": 1.5628,
        "step": 11721
    },
    {
        "loss": 1.4716,
        "grad_norm": 6.376678943634033,
        "learning_rate": 0.00014759522483997808,
        "epoch": 1.5629333333333333,
        "step": 11722
    },
    {
        "loss": 1.444,
        "grad_norm": 4.3743896484375,
        "learning_rate": 0.00014753986436514445,
        "epoch": 1.5630666666666668,
        "step": 11723
    },
    {
        "loss": 1.7097,
        "grad_norm": 4.972283840179443,
        "learning_rate": 0.00014748448505960845,
        "epoch": 1.5632000000000001,
        "step": 11724
    },
    {
        "loss": 1.8063,
        "grad_norm": 3.537257194519043,
        "learning_rate": 0.00014742908694530607,
        "epoch": 1.5633333333333335,
        "step": 11725
    },
    {
        "loss": 2.1309,
        "grad_norm": 3.5857396125793457,
        "learning_rate": 0.000147373670044181,
        "epoch": 1.5634666666666668,
        "step": 11726
    },
    {
        "loss": 2.495,
        "grad_norm": 4.395607948303223,
        "learning_rate": 0.00014731823437818357,
        "epoch": 1.5636,
        "step": 11727
    },
    {
        "loss": 2.5753,
        "grad_norm": 4.239428997039795,
        "learning_rate": 0.0001472627799692724,
        "epoch": 1.5637333333333334,
        "step": 11728
    },
    {
        "loss": 1.4506,
        "grad_norm": 4.254903793334961,
        "learning_rate": 0.0001472073068394129,
        "epoch": 1.5638666666666667,
        "step": 11729
    },
    {
        "loss": 1.3838,
        "grad_norm": 3.877338409423828,
        "learning_rate": 0.00014715181501057844,
        "epoch": 1.564,
        "step": 11730
    },
    {
        "loss": 1.6068,
        "grad_norm": 4.840207099914551,
        "learning_rate": 0.0001470963045047494,
        "epoch": 1.5641333333333334,
        "step": 11731
    },
    {
        "loss": 2.6599,
        "grad_norm": 3.8747971057891846,
        "learning_rate": 0.00014704077534391364,
        "epoch": 1.5642666666666667,
        "step": 11732
    },
    {
        "loss": 0.9983,
        "grad_norm": 4.407088279724121,
        "learning_rate": 0.00014698522755006636,
        "epoch": 1.5644,
        "step": 11733
    },
    {
        "loss": 2.8707,
        "grad_norm": 3.9491841793060303,
        "learning_rate": 0.0001469296611452105,
        "epoch": 1.5645333333333333,
        "step": 11734
    },
    {
        "loss": 2.7511,
        "grad_norm": 3.970456123352051,
        "learning_rate": 0.00014687407615135594,
        "epoch": 1.5646666666666667,
        "step": 11735
    },
    {
        "loss": 2.1554,
        "grad_norm": 4.2328200340271,
        "learning_rate": 0.00014681847259051992,
        "epoch": 1.5648,
        "step": 11736
    },
    {
        "loss": 2.3282,
        "grad_norm": 4.894954204559326,
        "learning_rate": 0.00014676285048472755,
        "epoch": 1.5649333333333333,
        "step": 11737
    },
    {
        "loss": 3.0465,
        "grad_norm": 2.936124563217163,
        "learning_rate": 0.0001467072098560108,
        "epoch": 1.5650666666666666,
        "step": 11738
    },
    {
        "loss": 2.3492,
        "grad_norm": 4.719810962677002,
        "learning_rate": 0.0001466515507264089,
        "epoch": 1.5652,
        "step": 11739
    },
    {
        "loss": 0.6983,
        "grad_norm": 3.892853021621704,
        "learning_rate": 0.00014659587311796892,
        "epoch": 1.5653333333333332,
        "step": 11740
    },
    {
        "loss": 2.1021,
        "grad_norm": 6.320189952850342,
        "learning_rate": 0.00014654017705274482,
        "epoch": 1.5654666666666666,
        "step": 11741
    },
    {
        "loss": 2.1744,
        "grad_norm": 3.425039291381836,
        "learning_rate": 0.00014648446255279793,
        "epoch": 1.5655999999999999,
        "step": 11742
    },
    {
        "loss": 2.7097,
        "grad_norm": 4.457569122314453,
        "learning_rate": 0.00014642872964019713,
        "epoch": 1.5657333333333332,
        "step": 11743
    },
    {
        "loss": 2.0332,
        "grad_norm": 2.4566004276275635,
        "learning_rate": 0.00014637297833701828,
        "epoch": 1.5658666666666665,
        "step": 11744
    },
    {
        "loss": 1.6996,
        "grad_norm": 3.9470200538635254,
        "learning_rate": 0.00014631720866534464,
        "epoch": 1.5659999999999998,
        "step": 11745
    },
    {
        "loss": 1.5925,
        "grad_norm": 2.753890037536621,
        "learning_rate": 0.0001462614206472669,
        "epoch": 1.5661333333333334,
        "step": 11746
    },
    {
        "loss": 2.2407,
        "grad_norm": 4.894860744476318,
        "learning_rate": 0.00014620561430488281,
        "epoch": 1.5662666666666667,
        "step": 11747
    },
    {
        "loss": 2.1279,
        "grad_norm": 3.224879741668701,
        "learning_rate": 0.00014614978966029737,
        "epoch": 1.5664,
        "step": 11748
    },
    {
        "loss": 1.7519,
        "grad_norm": 4.197251796722412,
        "learning_rate": 0.00014609394673562308,
        "epoch": 1.5665333333333333,
        "step": 11749
    },
    {
        "loss": 1.6259,
        "grad_norm": 6.522130012512207,
        "learning_rate": 0.00014603808555297947,
        "epoch": 1.5666666666666667,
        "step": 11750
    },
    {
        "loss": 0.8632,
        "grad_norm": 4.423676013946533,
        "learning_rate": 0.00014598220613449325,
        "epoch": 1.5668,
        "step": 11751
    },
    {
        "loss": 2.4069,
        "grad_norm": 2.9806690216064453,
        "learning_rate": 0.0001459263085022984,
        "epoch": 1.5669333333333333,
        "step": 11752
    },
    {
        "loss": 2.7348,
        "grad_norm": 4.334686279296875,
        "learning_rate": 0.0001458703926785364,
        "epoch": 1.5670666666666668,
        "step": 11753
    },
    {
        "loss": 2.108,
        "grad_norm": 4.5546722412109375,
        "learning_rate": 0.0001458144586853554,
        "epoch": 1.5672000000000001,
        "step": 11754
    },
    {
        "loss": 2.526,
        "grad_norm": 3.125175714492798,
        "learning_rate": 0.00014575850654491147,
        "epoch": 1.5673333333333335,
        "step": 11755
    },
    {
        "loss": 1.1415,
        "grad_norm": 5.183370113372803,
        "learning_rate": 0.00014570253627936694,
        "epoch": 1.5674666666666668,
        "step": 11756
    },
    {
        "loss": 2.4217,
        "grad_norm": 3.018752336502075,
        "learning_rate": 0.00014564654791089204,
        "epoch": 1.5676,
        "step": 11757
    },
    {
        "loss": 2.3354,
        "grad_norm": 2.8710556030273438,
        "learning_rate": 0.00014559054146166413,
        "epoch": 1.5677333333333334,
        "step": 11758
    },
    {
        "loss": 2.1508,
        "grad_norm": 3.2232248783111572,
        "learning_rate": 0.0001455345169538674,
        "epoch": 1.5678666666666667,
        "step": 11759
    },
    {
        "loss": 2.4345,
        "grad_norm": 4.728703498840332,
        "learning_rate": 0.00014547847440969332,
        "epoch": 1.568,
        "step": 11760
    },
    {
        "loss": 2.766,
        "grad_norm": 3.246284008026123,
        "learning_rate": 0.00014542241385134046,
        "epoch": 1.5681333333333334,
        "step": 11761
    },
    {
        "loss": 2.1373,
        "grad_norm": 4.042884349822998,
        "learning_rate": 0.00014536633530101484,
        "epoch": 1.5682666666666667,
        "step": 11762
    },
    {
        "loss": 2.1565,
        "grad_norm": 3.6705169677734375,
        "learning_rate": 0.0001453102387809291,
        "epoch": 1.5684,
        "step": 11763
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.925168991088867,
        "learning_rate": 0.0001452541243133036,
        "epoch": 1.5685333333333333,
        "step": 11764
    },
    {
        "loss": 2.4747,
        "grad_norm": 2.8469932079315186,
        "learning_rate": 0.000145197991920365,
        "epoch": 1.5686666666666667,
        "step": 11765
    },
    {
        "loss": 2.4506,
        "grad_norm": 4.799139976501465,
        "learning_rate": 0.00014514184162434782,
        "epoch": 1.5688,
        "step": 11766
    },
    {
        "loss": 1.312,
        "grad_norm": 6.489470958709717,
        "learning_rate": 0.0001450856734474935,
        "epoch": 1.5689333333333333,
        "step": 11767
    },
    {
        "loss": 2.2054,
        "grad_norm": 3.4636552333831787,
        "learning_rate": 0.00014502948741205028,
        "epoch": 1.5690666666666666,
        "step": 11768
    },
    {
        "loss": 1.0331,
        "grad_norm": 4.121914386749268,
        "learning_rate": 0.00014497328354027365,
        "epoch": 1.5692,
        "step": 11769
    },
    {
        "loss": 1.1397,
        "grad_norm": 4.0720343589782715,
        "learning_rate": 0.0001449170618544261,
        "epoch": 1.5693333333333332,
        "step": 11770
    },
    {
        "loss": 2.6249,
        "grad_norm": 4.0429253578186035,
        "learning_rate": 0.00014486082237677737,
        "epoch": 1.5694666666666666,
        "step": 11771
    },
    {
        "loss": 2.4604,
        "grad_norm": 5.130497932434082,
        "learning_rate": 0.00014480456512960403,
        "epoch": 1.5695999999999999,
        "step": 11772
    },
    {
        "loss": 2.2044,
        "grad_norm": 4.836048126220703,
        "learning_rate": 0.0001447482901351898,
        "epoch": 1.5697333333333332,
        "step": 11773
    },
    {
        "loss": 2.2034,
        "grad_norm": 3.949474573135376,
        "learning_rate": 0.0001446919974158252,
        "epoch": 1.5698666666666665,
        "step": 11774
    },
    {
        "loss": 1.5134,
        "grad_norm": 5.5521674156188965,
        "learning_rate": 0.0001446356869938082,
        "epoch": 1.5699999999999998,
        "step": 11775
    },
    {
        "loss": 2.565,
        "grad_norm": 4.849133491516113,
        "learning_rate": 0.00014457935889144373,
        "epoch": 1.5701333333333334,
        "step": 11776
    },
    {
        "loss": 3.076,
        "grad_norm": 3.688659429550171,
        "learning_rate": 0.00014452301313104304,
        "epoch": 1.5702666666666667,
        "step": 11777
    },
    {
        "loss": 1.8036,
        "grad_norm": 4.507772922515869,
        "learning_rate": 0.00014446664973492528,
        "epoch": 1.5704,
        "step": 11778
    },
    {
        "loss": 2.2233,
        "grad_norm": 3.433004379272461,
        "learning_rate": 0.00014441026872541587,
        "epoch": 1.5705333333333333,
        "step": 11779
    },
    {
        "loss": 2.6632,
        "grad_norm": 3.998509407043457,
        "learning_rate": 0.00014435387012484786,
        "epoch": 1.5706666666666667,
        "step": 11780
    },
    {
        "loss": 2.5795,
        "grad_norm": 3.8060359954833984,
        "learning_rate": 0.00014429745395556077,
        "epoch": 1.5708,
        "step": 11781
    },
    {
        "loss": 2.113,
        "grad_norm": 4.042315483093262,
        "learning_rate": 0.0001442410202399012,
        "epoch": 1.5709333333333333,
        "step": 11782
    },
    {
        "loss": 2.0505,
        "grad_norm": 4.659472465515137,
        "learning_rate": 0.00014418456900022264,
        "epoch": 1.5710666666666666,
        "step": 11783
    },
    {
        "loss": 0.7771,
        "grad_norm": 3.3990578651428223,
        "learning_rate": 0.00014412810025888576,
        "epoch": 1.5712000000000002,
        "step": 11784
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.2506332397460938,
        "learning_rate": 0.00014407161403825827,
        "epoch": 1.5713333333333335,
        "step": 11785
    },
    {
        "loss": 0.9631,
        "grad_norm": 6.283336639404297,
        "learning_rate": 0.00014401511036071403,
        "epoch": 1.5714666666666668,
        "step": 11786
    },
    {
        "loss": 1.2933,
        "grad_norm": 4.239009857177734,
        "learning_rate": 0.0001439585892486347,
        "epoch": 1.5716,
        "step": 11787
    },
    {
        "loss": 1.5531,
        "grad_norm": 4.804279804229736,
        "learning_rate": 0.00014390205072440827,
        "epoch": 1.5717333333333334,
        "step": 11788
    },
    {
        "loss": 2.7122,
        "grad_norm": 2.8472485542297363,
        "learning_rate": 0.00014384549481043007,
        "epoch": 1.5718666666666667,
        "step": 11789
    },
    {
        "loss": 2.2823,
        "grad_norm": 4.113258361816406,
        "learning_rate": 0.00014378892152910197,
        "epoch": 1.572,
        "step": 11790
    },
    {
        "loss": 1.7178,
        "grad_norm": 4.636908054351807,
        "learning_rate": 0.0001437323309028329,
        "epoch": 1.5721333333333334,
        "step": 11791
    },
    {
        "loss": 2.6032,
        "grad_norm": 4.327267169952393,
        "learning_rate": 0.00014367572295403836,
        "epoch": 1.5722666666666667,
        "step": 11792
    },
    {
        "loss": 2.2777,
        "grad_norm": 4.446136951446533,
        "learning_rate": 0.0001436190977051413,
        "epoch": 1.5724,
        "step": 11793
    },
    {
        "loss": 2.604,
        "grad_norm": 2.645411491394043,
        "learning_rate": 0.000143562455178571,
        "epoch": 1.5725333333333333,
        "step": 11794
    },
    {
        "loss": 1.4798,
        "grad_norm": 4.45376443862915,
        "learning_rate": 0.0001435057953967636,
        "epoch": 1.5726666666666667,
        "step": 11795
    },
    {
        "loss": 1.9199,
        "grad_norm": 3.680980920791626,
        "learning_rate": 0.0001434491183821626,
        "epoch": 1.5728,
        "step": 11796
    },
    {
        "loss": 2.0309,
        "grad_norm": 2.7877399921417236,
        "learning_rate": 0.00014339242415721773,
        "epoch": 1.5729333333333333,
        "step": 11797
    },
    {
        "loss": 2.7055,
        "grad_norm": 2.9994585514068604,
        "learning_rate": 0.00014333571274438564,
        "epoch": 1.5730666666666666,
        "step": 11798
    },
    {
        "loss": 2.6146,
        "grad_norm": 7.2758097648620605,
        "learning_rate": 0.0001432789841661302,
        "epoch": 1.5732,
        "step": 11799
    },
    {
        "loss": 1.57,
        "grad_norm": 3.5806124210357666,
        "learning_rate": 0.00014322223844492172,
        "epoch": 1.5733333333333333,
        "step": 11800
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.0628578662872314,
        "learning_rate": 0.00014316547560323727,
        "epoch": 1.5734666666666666,
        "step": 11801
    },
    {
        "loss": 1.307,
        "grad_norm": 2.97135591506958,
        "learning_rate": 0.00014310869566356072,
        "epoch": 1.5735999999999999,
        "step": 11802
    },
    {
        "loss": 2.7224,
        "grad_norm": 2.9036571979522705,
        "learning_rate": 0.00014305189864838305,
        "epoch": 1.5737333333333332,
        "step": 11803
    },
    {
        "loss": 1.4889,
        "grad_norm": 4.884087562561035,
        "learning_rate": 0.00014299508458020152,
        "epoch": 1.5738666666666665,
        "step": 11804
    },
    {
        "loss": 0.5431,
        "grad_norm": 4.6364970207214355,
        "learning_rate": 0.00014293825348152064,
        "epoch": 1.5739999999999998,
        "step": 11805
    },
    {
        "loss": 1.3172,
        "grad_norm": 6.205986499786377,
        "learning_rate": 0.0001428814053748512,
        "epoch": 1.5741333333333334,
        "step": 11806
    },
    {
        "loss": 1.513,
        "grad_norm": 5.322718143463135,
        "learning_rate": 0.00014282454028271088,
        "epoch": 1.5742666666666667,
        "step": 11807
    },
    {
        "loss": 2.398,
        "grad_norm": 3.888848304748535,
        "learning_rate": 0.0001427676582276243,
        "epoch": 1.5744,
        "step": 11808
    },
    {
        "loss": 2.1021,
        "grad_norm": 4.482628345489502,
        "learning_rate": 0.00014271075923212264,
        "epoch": 1.5745333333333333,
        "step": 11809
    },
    {
        "loss": 1.8751,
        "grad_norm": 5.5811357498168945,
        "learning_rate": 0.00014265384331874369,
        "epoch": 1.5746666666666667,
        "step": 11810
    },
    {
        "loss": 2.4524,
        "grad_norm": 3.469362497329712,
        "learning_rate": 0.00014259691051003197,
        "epoch": 1.5748,
        "step": 11811
    },
    {
        "loss": 1.5669,
        "grad_norm": 3.34861421585083,
        "learning_rate": 0.00014253996082853892,
        "epoch": 1.5749333333333333,
        "step": 11812
    },
    {
        "loss": 2.103,
        "grad_norm": 3.1901695728302,
        "learning_rate": 0.00014248299429682235,
        "epoch": 1.5750666666666666,
        "step": 11813
    },
    {
        "loss": 2.6909,
        "grad_norm": 3.3166849613189697,
        "learning_rate": 0.0001424260109374472,
        "epoch": 1.5752000000000002,
        "step": 11814
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.6916849613189697,
        "learning_rate": 0.00014236901077298436,
        "epoch": 1.5753333333333335,
        "step": 11815
    },
    {
        "loss": 1.1802,
        "grad_norm": 4.45009708404541,
        "learning_rate": 0.00014231199382601194,
        "epoch": 1.5754666666666668,
        "step": 11816
    },
    {
        "loss": 1.6872,
        "grad_norm": 6.096923828125,
        "learning_rate": 0.0001422549601191147,
        "epoch": 1.5756000000000001,
        "step": 11817
    },
    {
        "loss": 2.1559,
        "grad_norm": 4.329078197479248,
        "learning_rate": 0.0001421979096748838,
        "epoch": 1.5757333333333334,
        "step": 11818
    },
    {
        "loss": 3.1841,
        "grad_norm": 3.341789484024048,
        "learning_rate": 0.0001421408425159171,
        "epoch": 1.5758666666666667,
        "step": 11819
    },
    {
        "loss": 1.705,
        "grad_norm": 2.562166213989258,
        "learning_rate": 0.00014208375866481894,
        "epoch": 1.576,
        "step": 11820
    },
    {
        "loss": 1.8723,
        "grad_norm": 5.510876178741455,
        "learning_rate": 0.00014202665814420072,
        "epoch": 1.5761333333333334,
        "step": 11821
    },
    {
        "loss": 1.4163,
        "grad_norm": 4.212207317352295,
        "learning_rate": 0.00014196954097667986,
        "epoch": 1.5762666666666667,
        "step": 11822
    },
    {
        "loss": 2.1129,
        "grad_norm": 3.7101404666900635,
        "learning_rate": 0.00014191240718488111,
        "epoch": 1.5764,
        "step": 11823
    },
    {
        "loss": 2.2684,
        "grad_norm": 3.398841619491577,
        "learning_rate": 0.00014185525679143474,
        "epoch": 1.5765333333333333,
        "step": 11824
    },
    {
        "loss": 2.2028,
        "grad_norm": 3.965625047683716,
        "learning_rate": 0.00014179808981897856,
        "epoch": 1.5766666666666667,
        "step": 11825
    },
    {
        "loss": 2.0729,
        "grad_norm": 4.2649736404418945,
        "learning_rate": 0.00014174090629015673,
        "epoch": 1.5768,
        "step": 11826
    },
    {
        "loss": 1.5753,
        "grad_norm": 3.6764068603515625,
        "learning_rate": 0.00014168370622761968,
        "epoch": 1.5769333333333333,
        "step": 11827
    },
    {
        "loss": 2.8049,
        "grad_norm": 3.6272077560424805,
        "learning_rate": 0.00014162648965402455,
        "epoch": 1.5770666666666666,
        "step": 11828
    },
    {
        "loss": 2.5874,
        "grad_norm": 3.3635029792785645,
        "learning_rate": 0.00014156925659203492,
        "epoch": 1.5772,
        "step": 11829
    },
    {
        "loss": 2.4921,
        "grad_norm": 4.060016632080078,
        "learning_rate": 0.0001415120070643212,
        "epoch": 1.5773333333333333,
        "step": 11830
    },
    {
        "loss": 1.1598,
        "grad_norm": 3.8488550186157227,
        "learning_rate": 0.00014145474109356009,
        "epoch": 1.5774666666666666,
        "step": 11831
    },
    {
        "loss": 2.3843,
        "grad_norm": 2.993394613265991,
        "learning_rate": 0.00014139745870243476,
        "epoch": 1.5776,
        "step": 11832
    },
    {
        "loss": 2.2056,
        "grad_norm": 4.6612162590026855,
        "learning_rate": 0.00014134015991363485,
        "epoch": 1.5777333333333332,
        "step": 11833
    },
    {
        "loss": 1.5189,
        "grad_norm": 5.04913330078125,
        "learning_rate": 0.00014128284474985672,
        "epoch": 1.5778666666666665,
        "step": 11834
    },
    {
        "loss": 2.1552,
        "grad_norm": 3.579634666442871,
        "learning_rate": 0.00014122551323380336,
        "epoch": 1.5779999999999998,
        "step": 11835
    },
    {
        "loss": 2.312,
        "grad_norm": 4.162776947021484,
        "learning_rate": 0.0001411681653881835,
        "epoch": 1.5781333333333334,
        "step": 11836
    },
    {
        "loss": 2.5753,
        "grad_norm": 4.724298000335693,
        "learning_rate": 0.00014111080123571318,
        "epoch": 1.5782666666666667,
        "step": 11837
    },
    {
        "loss": 2.1745,
        "grad_norm": 3.775578498840332,
        "learning_rate": 0.00014105342079911428,
        "epoch": 1.5784,
        "step": 11838
    },
    {
        "loss": 1.5567,
        "grad_norm": 4.784788131713867,
        "learning_rate": 0.00014099602410111566,
        "epoch": 1.5785333333333333,
        "step": 11839
    },
    {
        "loss": 1.4069,
        "grad_norm": 2.710519313812256,
        "learning_rate": 0.00014093861116445217,
        "epoch": 1.5786666666666667,
        "step": 11840
    },
    {
        "loss": 1.6583,
        "grad_norm": 4.6529059410095215,
        "learning_rate": 0.0001408811820118653,
        "epoch": 1.5788,
        "step": 11841
    },
    {
        "loss": 2.91,
        "grad_norm": 2.6681079864501953,
        "learning_rate": 0.00014082373666610284,
        "epoch": 1.5789333333333333,
        "step": 11842
    },
    {
        "loss": 1.8135,
        "grad_norm": 4.278225898742676,
        "learning_rate": 0.0001407662751499192,
        "epoch": 1.5790666666666666,
        "step": 11843
    },
    {
        "loss": 2.3522,
        "grad_norm": 3.1648480892181396,
        "learning_rate": 0.00014070879748607527,
        "epoch": 1.5792000000000002,
        "step": 11844
    },
    {
        "loss": 2.5861,
        "grad_norm": 2.997555732727051,
        "learning_rate": 0.00014065130369733772,
        "epoch": 1.5793333333333335,
        "step": 11845
    },
    {
        "loss": 2.512,
        "grad_norm": 3.8254916667938232,
        "learning_rate": 0.0001405937938064804,
        "epoch": 1.5794666666666668,
        "step": 11846
    },
    {
        "loss": 2.2682,
        "grad_norm": 3.401231288909912,
        "learning_rate": 0.00014053626783628285,
        "epoch": 1.5796000000000001,
        "step": 11847
    },
    {
        "loss": 1.67,
        "grad_norm": 4.497119903564453,
        "learning_rate": 0.00014047872580953165,
        "epoch": 1.5797333333333334,
        "step": 11848
    },
    {
        "loss": 1.6206,
        "grad_norm": 3.285429000854492,
        "learning_rate": 0.00014042116774901924,
        "epoch": 1.5798666666666668,
        "step": 11849
    },
    {
        "loss": 2.1665,
        "grad_norm": 3.851423978805542,
        "learning_rate": 0.00014036359367754453,
        "epoch": 1.58,
        "step": 11850
    },
    {
        "loss": 2.8153,
        "grad_norm": 2.7345399856567383,
        "learning_rate": 0.0001403060036179127,
        "epoch": 1.5801333333333334,
        "step": 11851
    },
    {
        "loss": 2.506,
        "grad_norm": 2.258030414581299,
        "learning_rate": 0.00014024839759293564,
        "epoch": 1.5802666666666667,
        "step": 11852
    },
    {
        "loss": 2.1939,
        "grad_norm": 4.355889797210693,
        "learning_rate": 0.00014019077562543115,
        "epoch": 1.5804,
        "step": 11853
    },
    {
        "loss": 2.675,
        "grad_norm": 2.638154983520508,
        "learning_rate": 0.00014013313773822336,
        "epoch": 1.5805333333333333,
        "step": 11854
    },
    {
        "loss": 0.8267,
        "grad_norm": 3.68355131149292,
        "learning_rate": 0.00014007548395414307,
        "epoch": 1.5806666666666667,
        "step": 11855
    },
    {
        "loss": 2.1974,
        "grad_norm": 2.8658978939056396,
        "learning_rate": 0.00014001781429602705,
        "epoch": 1.5808,
        "step": 11856
    },
    {
        "loss": 1.9336,
        "grad_norm": 4.404527187347412,
        "learning_rate": 0.0001399601287867183,
        "epoch": 1.5809333333333333,
        "step": 11857
    },
    {
        "loss": 2.097,
        "grad_norm": 5.189929962158203,
        "learning_rate": 0.00013990242744906653,
        "epoch": 1.5810666666666666,
        "step": 11858
    },
    {
        "loss": 2.6828,
        "grad_norm": 3.0715794563293457,
        "learning_rate": 0.00013984471030592722,
        "epoch": 1.5812,
        "step": 11859
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.997629404067993,
        "learning_rate": 0.00013978697738016243,
        "epoch": 1.5813333333333333,
        "step": 11860
    },
    {
        "loss": 2.1749,
        "grad_norm": 2.154256582260132,
        "learning_rate": 0.00013972922869464021,
        "epoch": 1.5814666666666666,
        "step": 11861
    },
    {
        "loss": 2.6281,
        "grad_norm": 2.8632946014404297,
        "learning_rate": 0.00013967146427223525,
        "epoch": 1.5816,
        "step": 11862
    },
    {
        "loss": 2.8399,
        "grad_norm": 3.1988842487335205,
        "learning_rate": 0.00013961368413582798,
        "epoch": 1.5817333333333332,
        "step": 11863
    },
    {
        "loss": 2.4099,
        "grad_norm": 4.108260631561279,
        "learning_rate": 0.0001395558883083056,
        "epoch": 1.5818666666666665,
        "step": 11864
    },
    {
        "loss": 1.9711,
        "grad_norm": 3.7155044078826904,
        "learning_rate": 0.00013949807681256103,
        "epoch": 1.5819999999999999,
        "step": 11865
    },
    {
        "loss": 1.9796,
        "grad_norm": 3.6169779300689697,
        "learning_rate": 0.00013944024967149353,
        "epoch": 1.5821333333333332,
        "step": 11866
    },
    {
        "loss": 2.8031,
        "grad_norm": 4.487864971160889,
        "learning_rate": 0.0001393824069080089,
        "epoch": 1.5822666666666667,
        "step": 11867
    },
    {
        "loss": 3.0411,
        "grad_norm": 4.1936845779418945,
        "learning_rate": 0.00013932454854501868,
        "epoch": 1.5824,
        "step": 11868
    },
    {
        "loss": 2.1571,
        "grad_norm": 3.7055678367614746,
        "learning_rate": 0.0001392666746054408,
        "epoch": 1.5825333333333333,
        "step": 11869
    },
    {
        "loss": 2.4168,
        "grad_norm": 3.1297972202301025,
        "learning_rate": 0.00013920878511219917,
        "epoch": 1.5826666666666667,
        "step": 11870
    },
    {
        "loss": 1.3473,
        "grad_norm": 4.250556468963623,
        "learning_rate": 0.0001391508800882243,
        "epoch": 1.5828,
        "step": 11871
    },
    {
        "loss": 3.38,
        "grad_norm": 3.3605668544769287,
        "learning_rate": 0.00013909295955645228,
        "epoch": 1.5829333333333333,
        "step": 11872
    },
    {
        "loss": 2.0784,
        "grad_norm": 2.8543355464935303,
        "learning_rate": 0.00013903502353982604,
        "epoch": 1.5830666666666666,
        "step": 11873
    },
    {
        "loss": 2.2874,
        "grad_norm": 2.643414258956909,
        "learning_rate": 0.0001389770720612937,
        "epoch": 1.5832000000000002,
        "step": 11874
    },
    {
        "loss": 1.9482,
        "grad_norm": 3.2985525131225586,
        "learning_rate": 0.00013891910514381031,
        "epoch": 1.5833333333333335,
        "step": 11875
    },
    {
        "loss": 2.6531,
        "grad_norm": 4.455264568328857,
        "learning_rate": 0.00013886112281033693,
        "epoch": 1.5834666666666668,
        "step": 11876
    },
    {
        "loss": 2.2132,
        "grad_norm": 2.9476802349090576,
        "learning_rate": 0.00013880312508384032,
        "epoch": 1.5836000000000001,
        "step": 11877
    },
    {
        "loss": 2.2605,
        "grad_norm": 4.003291130065918,
        "learning_rate": 0.00013874511198729367,
        "epoch": 1.5837333333333334,
        "step": 11878
    },
    {
        "loss": 2.4194,
        "grad_norm": 3.920480489730835,
        "learning_rate": 0.00013868708354367604,
        "epoch": 1.5838666666666668,
        "step": 11879
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.9197733402252197,
        "learning_rate": 0.0001386290397759729,
        "epoch": 1.584,
        "step": 11880
    },
    {
        "loss": 1.756,
        "grad_norm": 4.456866264343262,
        "learning_rate": 0.00013857098070717546,
        "epoch": 1.5841333333333334,
        "step": 11881
    },
    {
        "loss": 2.2865,
        "grad_norm": 4.111220836639404,
        "learning_rate": 0.00013851290636028116,
        "epoch": 1.5842666666666667,
        "step": 11882
    },
    {
        "loss": 1.7148,
        "grad_norm": 7.784387588500977,
        "learning_rate": 0.0001384548167582933,
        "epoch": 1.5844,
        "step": 11883
    },
    {
        "loss": 1.5916,
        "grad_norm": 5.923384666442871,
        "learning_rate": 0.00013839671192422145,
        "epoch": 1.5845333333333333,
        "step": 11884
    },
    {
        "loss": 1.7756,
        "grad_norm": 4.350714683532715,
        "learning_rate": 0.00013833859188108134,
        "epoch": 1.5846666666666667,
        "step": 11885
    },
    {
        "loss": 2.3755,
        "grad_norm": 3.4467532634735107,
        "learning_rate": 0.00013828045665189434,
        "epoch": 1.5848,
        "step": 11886
    },
    {
        "loss": 2.2219,
        "grad_norm": 4.728496551513672,
        "learning_rate": 0.00013822230625968804,
        "epoch": 1.5849333333333333,
        "step": 11887
    },
    {
        "loss": 1.6998,
        "grad_norm": 3.435089349746704,
        "learning_rate": 0.00013816414072749585,
        "epoch": 1.5850666666666666,
        "step": 11888
    },
    {
        "loss": 1.6236,
        "grad_norm": 6.237676620483398,
        "learning_rate": 0.0001381059600783576,
        "epoch": 1.5852,
        "step": 11889
    },
    {
        "loss": 1.9533,
        "grad_norm": 3.481163740158081,
        "learning_rate": 0.00013804776433531873,
        "epoch": 1.5853333333333333,
        "step": 11890
    },
    {
        "loss": 2.2932,
        "grad_norm": 3.771117687225342,
        "learning_rate": 0.00013798955352143074,
        "epoch": 1.5854666666666666,
        "step": 11891
    },
    {
        "loss": 2.4319,
        "grad_norm": 3.1269161701202393,
        "learning_rate": 0.00013793132765975102,
        "epoch": 1.5856,
        "step": 11892
    },
    {
        "loss": 2.3912,
        "grad_norm": 2.598989725112915,
        "learning_rate": 0.0001378730867733432,
        "epoch": 1.5857333333333332,
        "step": 11893
    },
    {
        "loss": 1.6037,
        "grad_norm": 3.875434398651123,
        "learning_rate": 0.00013781483088527691,
        "epoch": 1.5858666666666665,
        "step": 11894
    },
    {
        "loss": 2.748,
        "grad_norm": 3.036715507507324,
        "learning_rate": 0.00013775656001862699,
        "epoch": 1.5859999999999999,
        "step": 11895
    },
    {
        "loss": 2.2337,
        "grad_norm": 3.6641013622283936,
        "learning_rate": 0.00013769827419647513,
        "epoch": 1.5861333333333332,
        "step": 11896
    },
    {
        "loss": 2.0892,
        "grad_norm": 3.2286839485168457,
        "learning_rate": 0.00013763997344190827,
        "epoch": 1.5862666666666667,
        "step": 11897
    },
    {
        "loss": 2.7582,
        "grad_norm": 2.8213584423065186,
        "learning_rate": 0.00013758165777801978,
        "epoch": 1.5864,
        "step": 11898
    },
    {
        "loss": 1.7605,
        "grad_norm": 4.90670108795166,
        "learning_rate": 0.0001375233272279086,
        "epoch": 1.5865333333333334,
        "step": 11899
    },
    {
        "loss": 2.302,
        "grad_norm": 4.006523609161377,
        "learning_rate": 0.00013746498181467967,
        "epoch": 1.5866666666666667,
        "step": 11900
    },
    {
        "loss": 2.3585,
        "grad_norm": 4.446822166442871,
        "learning_rate": 0.00013740662156144366,
        "epoch": 1.5868,
        "step": 11901
    },
    {
        "loss": 1.8945,
        "grad_norm": 3.0520448684692383,
        "learning_rate": 0.0001373482464913175,
        "epoch": 1.5869333333333333,
        "step": 11902
    },
    {
        "loss": 1.5835,
        "grad_norm": 4.286004066467285,
        "learning_rate": 0.00013728985662742364,
        "epoch": 1.5870666666666666,
        "step": 11903
    },
    {
        "loss": 2.4622,
        "grad_norm": 3.669365406036377,
        "learning_rate": 0.0001372314519928904,
        "epoch": 1.5872000000000002,
        "step": 11904
    },
    {
        "loss": 2.105,
        "grad_norm": 3.1540884971618652,
        "learning_rate": 0.0001371730326108523,
        "epoch": 1.5873333333333335,
        "step": 11905
    },
    {
        "loss": 1.762,
        "grad_norm": 4.32637357711792,
        "learning_rate": 0.00013711459850444916,
        "epoch": 1.5874666666666668,
        "step": 11906
    },
    {
        "loss": 2.5804,
        "grad_norm": 3.992309331893921,
        "learning_rate": 0.0001370561496968272,
        "epoch": 1.5876000000000001,
        "step": 11907
    },
    {
        "loss": 1.2927,
        "grad_norm": 3.5369229316711426,
        "learning_rate": 0.00013699768621113813,
        "epoch": 1.5877333333333334,
        "step": 11908
    },
    {
        "loss": 2.6668,
        "grad_norm": 2.6385815143585205,
        "learning_rate": 0.00013693920807053946,
        "epoch": 1.5878666666666668,
        "step": 11909
    },
    {
        "loss": 1.9693,
        "grad_norm": 2.946809768676758,
        "learning_rate": 0.00013688071529819446,
        "epoch": 1.588,
        "step": 11910
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.888249635696411,
        "learning_rate": 0.0001368222079172726,
        "epoch": 1.5881333333333334,
        "step": 11911
    },
    {
        "loss": 2.8822,
        "grad_norm": 2.208017349243164,
        "learning_rate": 0.00013676368595094872,
        "epoch": 1.5882666666666667,
        "step": 11912
    },
    {
        "loss": 1.685,
        "grad_norm": 3.8748676776885986,
        "learning_rate": 0.00013670514942240345,
        "epoch": 1.5884,
        "step": 11913
    },
    {
        "loss": 2.1651,
        "grad_norm": 4.112832069396973,
        "learning_rate": 0.00013664659835482354,
        "epoch": 1.5885333333333334,
        "step": 11914
    },
    {
        "loss": 0.7518,
        "grad_norm": 6.38564395904541,
        "learning_rate": 0.00013658803277140114,
        "epoch": 1.5886666666666667,
        "step": 11915
    },
    {
        "loss": 2.4642,
        "grad_norm": 4.166836261749268,
        "learning_rate": 0.0001365294526953342,
        "epoch": 1.5888,
        "step": 11916
    },
    {
        "loss": 0.9753,
        "grad_norm": 5.400299549102783,
        "learning_rate": 0.00013647085814982667,
        "epoch": 1.5889333333333333,
        "step": 11917
    },
    {
        "loss": 2.0263,
        "grad_norm": 5.929066181182861,
        "learning_rate": 0.00013641224915808797,
        "epoch": 1.5890666666666666,
        "step": 11918
    },
    {
        "loss": 1.3756,
        "grad_norm": 2.630451202392578,
        "learning_rate": 0.0001363536257433333,
        "epoch": 1.5892,
        "step": 11919
    },
    {
        "loss": 2.376,
        "grad_norm": 3.7458860874176025,
        "learning_rate": 0.00013629498792878345,
        "epoch": 1.5893333333333333,
        "step": 11920
    },
    {
        "loss": 1.4355,
        "grad_norm": 6.421525001525879,
        "learning_rate": 0.0001362363357376654,
        "epoch": 1.5894666666666666,
        "step": 11921
    },
    {
        "loss": 2.3366,
        "grad_norm": 4.986698627471924,
        "learning_rate": 0.00013617766919321112,
        "epoch": 1.5896,
        "step": 11922
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.7167255878448486,
        "learning_rate": 0.00013611898831865891,
        "epoch": 1.5897333333333332,
        "step": 11923
    },
    {
        "loss": 1.7373,
        "grad_norm": 5.048524379730225,
        "learning_rate": 0.00013606029313725237,
        "epoch": 1.5898666666666665,
        "step": 11924
    },
    {
        "loss": 2.1924,
        "grad_norm": 3.2150039672851562,
        "learning_rate": 0.00013600158367224074,
        "epoch": 1.5899999999999999,
        "step": 11925
    },
    {
        "loss": 1.603,
        "grad_norm": 3.271277904510498,
        "learning_rate": 0.00013594285994687928,
        "epoch": 1.5901333333333332,
        "step": 11926
    },
    {
        "loss": 2.158,
        "grad_norm": 3.8347394466400146,
        "learning_rate": 0.00013588412198442851,
        "epoch": 1.5902666666666667,
        "step": 11927
    },
    {
        "loss": 2.2213,
        "grad_norm": 4.108543872833252,
        "learning_rate": 0.0001358253698081547,
        "epoch": 1.5904,
        "step": 11928
    },
    {
        "loss": 2.5445,
        "grad_norm": 2.9027554988861084,
        "learning_rate": 0.00013576660344132976,
        "epoch": 1.5905333333333334,
        "step": 11929
    },
    {
        "loss": 2.0093,
        "grad_norm": 2.685089588165283,
        "learning_rate": 0.00013570782290723145,
        "epoch": 1.5906666666666667,
        "step": 11930
    },
    {
        "loss": 2.8229,
        "grad_norm": 3.4314491748809814,
        "learning_rate": 0.00013564902822914269,
        "epoch": 1.5908,
        "step": 11931
    },
    {
        "loss": 1.879,
        "grad_norm": 5.938746452331543,
        "learning_rate": 0.00013559021943035263,
        "epoch": 1.5909333333333333,
        "step": 11932
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.788848638534546,
        "learning_rate": 0.0001355313965341551,
        "epoch": 1.5910666666666666,
        "step": 11933
    },
    {
        "loss": 1.6983,
        "grad_norm": 4.352898120880127,
        "learning_rate": 0.00013547255956385034,
        "epoch": 1.5912,
        "step": 11934
    },
    {
        "loss": 2.1892,
        "grad_norm": 2.9885709285736084,
        "learning_rate": 0.00013541370854274403,
        "epoch": 1.5913333333333335,
        "step": 11935
    },
    {
        "loss": 2.1937,
        "grad_norm": 3.075558662414551,
        "learning_rate": 0.00013535484349414705,
        "epoch": 1.5914666666666668,
        "step": 11936
    },
    {
        "loss": 2.3858,
        "grad_norm": 5.035975456237793,
        "learning_rate": 0.00013529596444137614,
        "epoch": 1.5916000000000001,
        "step": 11937
    },
    {
        "loss": 1.8016,
        "grad_norm": 3.441572427749634,
        "learning_rate": 0.0001352370714077533,
        "epoch": 1.5917333333333334,
        "step": 11938
    },
    {
        "loss": 1.8793,
        "grad_norm": 5.817654132843018,
        "learning_rate": 0.00013517816441660654,
        "epoch": 1.5918666666666668,
        "step": 11939
    },
    {
        "loss": 2.5472,
        "grad_norm": 3.547555923461914,
        "learning_rate": 0.00013511924349126895,
        "epoch": 1.592,
        "step": 11940
    },
    {
        "loss": 2.1113,
        "grad_norm": 5.235751628875732,
        "learning_rate": 0.0001350603086550794,
        "epoch": 1.5921333333333334,
        "step": 11941
    },
    {
        "loss": 2.2033,
        "grad_norm": 3.646498680114746,
        "learning_rate": 0.00013500135993138197,
        "epoch": 1.5922666666666667,
        "step": 11942
    },
    {
        "loss": 1.8698,
        "grad_norm": 4.969967842102051,
        "learning_rate": 0.00013494239734352664,
        "epoch": 1.5924,
        "step": 11943
    },
    {
        "loss": 2.3631,
        "grad_norm": 4.238678932189941,
        "learning_rate": 0.00013488342091486874,
        "epoch": 1.5925333333333334,
        "step": 11944
    },
    {
        "loss": 2.3694,
        "grad_norm": 3.610790491104126,
        "learning_rate": 0.00013482443066876898,
        "epoch": 1.5926666666666667,
        "step": 11945
    },
    {
        "loss": 1.6961,
        "grad_norm": 4.41463041305542,
        "learning_rate": 0.00013476542662859355,
        "epoch": 1.5928,
        "step": 11946
    },
    {
        "loss": 2.0304,
        "grad_norm": 4.544861793518066,
        "learning_rate": 0.0001347064088177141,
        "epoch": 1.5929333333333333,
        "step": 11947
    },
    {
        "loss": 2.6618,
        "grad_norm": 4.3544392585754395,
        "learning_rate": 0.00013464737725950792,
        "epoch": 1.5930666666666666,
        "step": 11948
    },
    {
        "loss": 0.7596,
        "grad_norm": 4.3555731773376465,
        "learning_rate": 0.00013458833197735757,
        "epoch": 1.5932,
        "step": 11949
    },
    {
        "loss": 1.9296,
        "grad_norm": 3.6796231269836426,
        "learning_rate": 0.00013452927299465107,
        "epoch": 1.5933333333333333,
        "step": 11950
    },
    {
        "loss": 2.1068,
        "grad_norm": 4.884252548217773,
        "learning_rate": 0.00013447020033478178,
        "epoch": 1.5934666666666666,
        "step": 11951
    },
    {
        "loss": 2.9427,
        "grad_norm": 4.3629655838012695,
        "learning_rate": 0.00013441111402114864,
        "epoch": 1.5936,
        "step": 11952
    },
    {
        "loss": 2.8372,
        "grad_norm": 2.946380376815796,
        "learning_rate": 0.00013435201407715626,
        "epoch": 1.5937333333333332,
        "step": 11953
    },
    {
        "loss": 2.3168,
        "grad_norm": 3.4563486576080322,
        "learning_rate": 0.0001342929005262138,
        "epoch": 1.5938666666666665,
        "step": 11954
    },
    {
        "loss": 2.6564,
        "grad_norm": 3.4714770317077637,
        "learning_rate": 0.0001342337733917367,
        "epoch": 1.5939999999999999,
        "step": 11955
    },
    {
        "loss": 1.9663,
        "grad_norm": 2.696253776550293,
        "learning_rate": 0.0001341746326971452,
        "epoch": 1.5941333333333332,
        "step": 11956
    },
    {
        "loss": 2.6619,
        "grad_norm": 5.756495475769043,
        "learning_rate": 0.00013411547846586537,
        "epoch": 1.5942666666666667,
        "step": 11957
    },
    {
        "loss": 2.6251,
        "grad_norm": 2.5150036811828613,
        "learning_rate": 0.00013405631072132827,
        "epoch": 1.5944,
        "step": 11958
    },
    {
        "loss": 2.3351,
        "grad_norm": 4.408604145050049,
        "learning_rate": 0.00013399712948697045,
        "epoch": 1.5945333333333334,
        "step": 11959
    },
    {
        "loss": 2.1335,
        "grad_norm": 4.375376224517822,
        "learning_rate": 0.00013393793478623368,
        "epoch": 1.5946666666666667,
        "step": 11960
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.841810464859009,
        "learning_rate": 0.0001338787266425654,
        "epoch": 1.5948,
        "step": 11961
    },
    {
        "loss": 1.2836,
        "grad_norm": 2.764836072921753,
        "learning_rate": 0.00013381950507941807,
        "epoch": 1.5949333333333333,
        "step": 11962
    },
    {
        "loss": 2.3108,
        "grad_norm": 3.197366952896118,
        "learning_rate": 0.0001337602701202494,
        "epoch": 1.5950666666666666,
        "step": 11963
    },
    {
        "loss": 2.7183,
        "grad_norm": 3.8114192485809326,
        "learning_rate": 0.00013370102178852285,
        "epoch": 1.5952,
        "step": 11964
    },
    {
        "loss": 1.5501,
        "grad_norm": 4.687864303588867,
        "learning_rate": 0.00013364176010770664,
        "epoch": 1.5953333333333335,
        "step": 11965
    },
    {
        "loss": 2.6629,
        "grad_norm": 4.801291465759277,
        "learning_rate": 0.00013358248510127472,
        "epoch": 1.5954666666666668,
        "step": 11966
    },
    {
        "loss": 2.0381,
        "grad_norm": 3.4256231784820557,
        "learning_rate": 0.00013352319679270597,
        "epoch": 1.5956000000000001,
        "step": 11967
    },
    {
        "loss": 2.6954,
        "grad_norm": 2.678173303604126,
        "learning_rate": 0.0001334638952054848,
        "epoch": 1.5957333333333334,
        "step": 11968
    },
    {
        "loss": 2.2104,
        "grad_norm": 2.184434652328491,
        "learning_rate": 0.0001334045803631006,
        "epoch": 1.5958666666666668,
        "step": 11969
    },
    {
        "loss": 2.5785,
        "grad_norm": 3.4168214797973633,
        "learning_rate": 0.00013334525228904837,
        "epoch": 1.596,
        "step": 11970
    },
    {
        "loss": 2.4194,
        "grad_norm": 2.5203771591186523,
        "learning_rate": 0.00013328591100682813,
        "epoch": 1.5961333333333334,
        "step": 11971
    },
    {
        "loss": 2.6853,
        "grad_norm": 2.6817541122436523,
        "learning_rate": 0.00013322655653994495,
        "epoch": 1.5962666666666667,
        "step": 11972
    },
    {
        "loss": 1.6964,
        "grad_norm": 3.8344295024871826,
        "learning_rate": 0.00013316718891190967,
        "epoch": 1.5964,
        "step": 11973
    },
    {
        "loss": 3.1433,
        "grad_norm": 4.732661247253418,
        "learning_rate": 0.00013310780814623785,
        "epoch": 1.5965333333333334,
        "step": 11974
    },
    {
        "loss": 0.8536,
        "grad_norm": 4.520662784576416,
        "learning_rate": 0.00013304841426645028,
        "epoch": 1.5966666666666667,
        "step": 11975
    },
    {
        "loss": 2.0034,
        "grad_norm": 3.28559947013855,
        "learning_rate": 0.00013298900729607333,
        "epoch": 1.5968,
        "step": 11976
    },
    {
        "loss": 1.2123,
        "grad_norm": 3.3314249515533447,
        "learning_rate": 0.00013292958725863818,
        "epoch": 1.5969333333333333,
        "step": 11977
    },
    {
        "loss": 2.7803,
        "grad_norm": 4.032360553741455,
        "learning_rate": 0.0001328701541776813,
        "epoch": 1.5970666666666666,
        "step": 11978
    },
    {
        "loss": 2.0244,
        "grad_norm": 4.057797908782959,
        "learning_rate": 0.00013281070807674424,
        "epoch": 1.5972,
        "step": 11979
    },
    {
        "loss": 0.6459,
        "grad_norm": 3.372070789337158,
        "learning_rate": 0.00013275124897937403,
        "epoch": 1.5973333333333333,
        "step": 11980
    },
    {
        "loss": 2.1648,
        "grad_norm": 3.050718069076538,
        "learning_rate": 0.00013269177690912234,
        "epoch": 1.5974666666666666,
        "step": 11981
    },
    {
        "loss": 1.1789,
        "grad_norm": 4.227225303649902,
        "learning_rate": 0.00013263229188954675,
        "epoch": 1.5976,
        "step": 11982
    },
    {
        "loss": 2.5892,
        "grad_norm": 4.788135051727295,
        "learning_rate": 0.00013257279394420887,
        "epoch": 1.5977333333333332,
        "step": 11983
    },
    {
        "loss": 1.3208,
        "grad_norm": 5.0865478515625,
        "learning_rate": 0.0001325132830966763,
        "epoch": 1.5978666666666665,
        "step": 11984
    },
    {
        "loss": 2.0308,
        "grad_norm": 3.418492078781128,
        "learning_rate": 0.00013245375937052164,
        "epoch": 1.5979999999999999,
        "step": 11985
    },
    {
        "loss": 1.7725,
        "grad_norm": 3.7225916385650635,
        "learning_rate": 0.00013239422278932232,
        "epoch": 1.5981333333333332,
        "step": 11986
    },
    {
        "loss": 3.2906,
        "grad_norm": 4.294040203094482,
        "learning_rate": 0.00013233467337666097,
        "epoch": 1.5982666666666665,
        "step": 11987
    },
    {
        "loss": 1.9451,
        "grad_norm": 3.702258348464966,
        "learning_rate": 0.00013227511115612521,
        "epoch": 1.5984,
        "step": 11988
    },
    {
        "loss": 2.2337,
        "grad_norm": 3.5293314456939697,
        "learning_rate": 0.00013221553615130804,
        "epoch": 1.5985333333333334,
        "step": 11989
    },
    {
        "loss": 2.1475,
        "grad_norm": 3.5887458324432373,
        "learning_rate": 0.00013215594838580713,
        "epoch": 1.5986666666666667,
        "step": 11990
    },
    {
        "loss": 2.6334,
        "grad_norm": 4.130654335021973,
        "learning_rate": 0.00013209634788322573,
        "epoch": 1.5988,
        "step": 11991
    },
    {
        "loss": 2.0817,
        "grad_norm": 3.9674482345581055,
        "learning_rate": 0.0001320367346671713,
        "epoch": 1.5989333333333333,
        "step": 11992
    },
    {
        "loss": 2.4677,
        "grad_norm": 2.9140021800994873,
        "learning_rate": 0.00013197710876125714,
        "epoch": 1.5990666666666666,
        "step": 11993
    },
    {
        "loss": 1.0961,
        "grad_norm": 4.792263984680176,
        "learning_rate": 0.00013191747018910134,
        "epoch": 1.5992,
        "step": 11994
    },
    {
        "loss": 2.4998,
        "grad_norm": 4.331897258758545,
        "learning_rate": 0.00013185781897432687,
        "epoch": 1.5993333333333335,
        "step": 11995
    },
    {
        "loss": 1.0323,
        "grad_norm": 6.618802547454834,
        "learning_rate": 0.00013179815514056172,
        "epoch": 1.5994666666666668,
        "step": 11996
    },
    {
        "loss": 2.39,
        "grad_norm": 3.146088123321533,
        "learning_rate": 0.00013173847871143885,
        "epoch": 1.5996000000000001,
        "step": 11997
    },
    {
        "loss": 2.6572,
        "grad_norm": 5.268372535705566,
        "learning_rate": 0.00013167878971059657,
        "epoch": 1.5997333333333335,
        "step": 11998
    },
    {
        "loss": 2.2469,
        "grad_norm": 4.829124927520752,
        "learning_rate": 0.00013161908816167772,
        "epoch": 1.5998666666666668,
        "step": 11999
    },
    {
        "loss": 1.8422,
        "grad_norm": 5.134047508239746,
        "learning_rate": 0.0001315593740883303,
        "epoch": 1.6,
        "step": 12000
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.5975959300994873,
        "learning_rate": 0.00013149964751420717,
        "epoch": 1.6001333333333334,
        "step": 12001
    },
    {
        "loss": 2.4932,
        "grad_norm": 2.6076600551605225,
        "learning_rate": 0.00013143990846296627,
        "epoch": 1.6002666666666667,
        "step": 12002
    },
    {
        "loss": 2.4531,
        "grad_norm": 3.315176486968994,
        "learning_rate": 0.0001313801569582707,
        "epoch": 1.6004,
        "step": 12003
    },
    {
        "loss": 2.42,
        "grad_norm": 4.404360294342041,
        "learning_rate": 0.00013132039302378798,
        "epoch": 1.6005333333333334,
        "step": 12004
    },
    {
        "loss": 1.8636,
        "grad_norm": 4.711575508117676,
        "learning_rate": 0.0001312606166831909,
        "epoch": 1.6006666666666667,
        "step": 12005
    },
    {
        "loss": 2.9764,
        "grad_norm": 2.1610710620880127,
        "learning_rate": 0.00013120082796015686,
        "epoch": 1.6008,
        "step": 12006
    },
    {
        "loss": 2.0318,
        "grad_norm": 4.029977798461914,
        "learning_rate": 0.00013114102687836874,
        "epoch": 1.6009333333333333,
        "step": 12007
    },
    {
        "loss": 1.7559,
        "grad_norm": 2.7052547931671143,
        "learning_rate": 0.00013108121346151372,
        "epoch": 1.6010666666666666,
        "step": 12008
    },
    {
        "loss": 1.211,
        "grad_norm": 4.331090927124023,
        "learning_rate": 0.00013102138773328416,
        "epoch": 1.6012,
        "step": 12009
    },
    {
        "loss": 0.8021,
        "grad_norm": 4.727712631225586,
        "learning_rate": 0.00013096154971737706,
        "epoch": 1.6013333333333333,
        "step": 12010
    },
    {
        "loss": 1.8404,
        "grad_norm": 3.745577573776245,
        "learning_rate": 0.00013090169943749463,
        "epoch": 1.6014666666666666,
        "step": 12011
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.413271903991699,
        "learning_rate": 0.00013084183691734405,
        "epoch": 1.6016,
        "step": 12012
    },
    {
        "loss": 2.3632,
        "grad_norm": 3.564417600631714,
        "learning_rate": 0.0001307819621806365,
        "epoch": 1.6017333333333332,
        "step": 12013
    },
    {
        "loss": 2.0741,
        "grad_norm": 2.758404016494751,
        "learning_rate": 0.00013072207525108892,
        "epoch": 1.6018666666666665,
        "step": 12014
    },
    {
        "loss": 1.8909,
        "grad_norm": 4.868255138397217,
        "learning_rate": 0.00013066217615242257,
        "epoch": 1.6019999999999999,
        "step": 12015
    },
    {
        "loss": 0.7548,
        "grad_norm": 3.6836771965026855,
        "learning_rate": 0.00013060226490836392,
        "epoch": 1.6021333333333332,
        "step": 12016
    },
    {
        "loss": 1.3878,
        "grad_norm": 3.7656333446502686,
        "learning_rate": 0.00013054234154264384,
        "epoch": 1.6022666666666665,
        "step": 12017
    },
    {
        "loss": 1.1026,
        "grad_norm": 3.906071901321411,
        "learning_rate": 0.0001304824060789982,
        "epoch": 1.6024,
        "step": 12018
    },
    {
        "loss": 2.217,
        "grad_norm": 3.9213972091674805,
        "learning_rate": 0.0001304224585411675,
        "epoch": 1.6025333333333334,
        "step": 12019
    },
    {
        "loss": 1.557,
        "grad_norm": 7.49896764755249,
        "learning_rate": 0.00013036249895289744,
        "epoch": 1.6026666666666667,
        "step": 12020
    },
    {
        "loss": 0.9411,
        "grad_norm": 4.061570167541504,
        "learning_rate": 0.00013030252733793813,
        "epoch": 1.6028,
        "step": 12021
    },
    {
        "loss": 1.435,
        "grad_norm": 3.645437002182007,
        "learning_rate": 0.0001302425437200443,
        "epoch": 1.6029333333333333,
        "step": 12022
    },
    {
        "loss": 2.7421,
        "grad_norm": 2.6339614391326904,
        "learning_rate": 0.00013018254812297602,
        "epoch": 1.6030666666666666,
        "step": 12023
    },
    {
        "loss": 2.2254,
        "grad_norm": 4.224928379058838,
        "learning_rate": 0.00013012254057049748,
        "epoch": 1.6032,
        "step": 12024
    },
    {
        "loss": 2.9081,
        "grad_norm": 3.6978073120117188,
        "learning_rate": 0.00013006252108637806,
        "epoch": 1.6033333333333335,
        "step": 12025
    },
    {
        "loss": 2.5754,
        "grad_norm": 2.549117088317871,
        "learning_rate": 0.0001300024896943916,
        "epoch": 1.6034666666666668,
        "step": 12026
    },
    {
        "loss": 2.8176,
        "grad_norm": 3.496267557144165,
        "learning_rate": 0.00012994244641831678,
        "epoch": 1.6036000000000001,
        "step": 12027
    },
    {
        "loss": 2.0003,
        "grad_norm": 5.563737392425537,
        "learning_rate": 0.00012988239128193676,
        "epoch": 1.6037333333333335,
        "step": 12028
    },
    {
        "loss": 2.97,
        "grad_norm": 3.760446071624756,
        "learning_rate": 0.00012982232430903986,
        "epoch": 1.6038666666666668,
        "step": 12029
    },
    {
        "loss": 2.274,
        "grad_norm": 4.1490373611450195,
        "learning_rate": 0.0001297622455234187,
        "epoch": 1.604,
        "step": 12030
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.8950884342193604,
        "learning_rate": 0.00012970215494887053,
        "epoch": 1.6041333333333334,
        "step": 12031
    },
    {
        "loss": 1.1575,
        "grad_norm": 3.965985059738159,
        "learning_rate": 0.0001296420526091977,
        "epoch": 1.6042666666666667,
        "step": 12032
    },
    {
        "loss": 1.7257,
        "grad_norm": 3.5004994869232178,
        "learning_rate": 0.00012958193852820684,
        "epoch": 1.6044,
        "step": 12033
    },
    {
        "loss": 1.3646,
        "grad_norm": 2.0604803562164307,
        "learning_rate": 0.0001295218127297092,
        "epoch": 1.6045333333333334,
        "step": 12034
    },
    {
        "loss": 2.5564,
        "grad_norm": 4.375117778778076,
        "learning_rate": 0.00012946167523752108,
        "epoch": 1.6046666666666667,
        "step": 12035
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.955888271331787,
        "learning_rate": 0.000129401526075463,
        "epoch": 1.6048,
        "step": 12036
    },
    {
        "loss": 1.7759,
        "grad_norm": 4.657325744628906,
        "learning_rate": 0.00012934136526736024,
        "epoch": 1.6049333333333333,
        "step": 12037
    },
    {
        "loss": 2.7944,
        "grad_norm": 4.808791637420654,
        "learning_rate": 0.00012928119283704264,
        "epoch": 1.6050666666666666,
        "step": 12038
    },
    {
        "loss": 2.0387,
        "grad_norm": 3.4499971866607666,
        "learning_rate": 0.00012922100880834484,
        "epoch": 1.6052,
        "step": 12039
    },
    {
        "loss": 1.963,
        "grad_norm": 3.2295758724212646,
        "learning_rate": 0.00012916081320510584,
        "epoch": 1.6053333333333333,
        "step": 12040
    },
    {
        "loss": 0.7674,
        "grad_norm": 3.8167967796325684,
        "learning_rate": 0.00012910060605116956,
        "epoch": 1.6054666666666666,
        "step": 12041
    },
    {
        "loss": 1.1659,
        "grad_norm": 5.3916730880737305,
        "learning_rate": 0.00012904038737038384,
        "epoch": 1.6056,
        "step": 12042
    },
    {
        "loss": 1.7474,
        "grad_norm": 3.6070032119750977,
        "learning_rate": 0.0001289801571866017,
        "epoch": 1.6057333333333332,
        "step": 12043
    },
    {
        "loss": 2.2783,
        "grad_norm": 5.263450622558594,
        "learning_rate": 0.0001289199155236807,
        "epoch": 1.6058666666666666,
        "step": 12044
    },
    {
        "loss": 1.6607,
        "grad_norm": 11.397439002990723,
        "learning_rate": 0.00012885966240548262,
        "epoch": 1.6059999999999999,
        "step": 12045
    },
    {
        "loss": 1.8207,
        "grad_norm": 3.4828598499298096,
        "learning_rate": 0.00012879939785587395,
        "epoch": 1.6061333333333332,
        "step": 12046
    },
    {
        "loss": 0.9741,
        "grad_norm": 3.793649673461914,
        "learning_rate": 0.0001287391218987255,
        "epoch": 1.6062666666666665,
        "step": 12047
    },
    {
        "loss": 2.6339,
        "grad_norm": 3.6598668098449707,
        "learning_rate": 0.00012867883455791306,
        "epoch": 1.6064,
        "step": 12048
    },
    {
        "loss": 2.5649,
        "grad_norm": 3.1894545555114746,
        "learning_rate": 0.0001286185358573164,
        "epoch": 1.6065333333333334,
        "step": 12049
    },
    {
        "loss": 1.8029,
        "grad_norm": 5.344022274017334,
        "learning_rate": 0.00012855822582082046,
        "epoch": 1.6066666666666667,
        "step": 12050
    },
    {
        "loss": 1.8196,
        "grad_norm": 7.779177188873291,
        "learning_rate": 0.00012849790447231365,
        "epoch": 1.6068,
        "step": 12051
    },
    {
        "loss": 2.1264,
        "grad_norm": 4.692493438720703,
        "learning_rate": 0.00012843757183568982,
        "epoch": 1.6069333333333333,
        "step": 12052
    },
    {
        "loss": 0.8892,
        "grad_norm": 7.061687469482422,
        "learning_rate": 0.00012837722793484702,
        "epoch": 1.6070666666666666,
        "step": 12053
    },
    {
        "loss": 2.1308,
        "grad_norm": 3.9388809204101562,
        "learning_rate": 0.0001283168727936875,
        "epoch": 1.6072,
        "step": 12054
    },
    {
        "loss": 2.9362,
        "grad_norm": 3.918654680252075,
        "learning_rate": 0.0001282565064361182,
        "epoch": 1.6073333333333333,
        "step": 12055
    },
    {
        "loss": 2.4688,
        "grad_norm": 4.275073528289795,
        "learning_rate": 0.00012819612888605031,
        "epoch": 1.6074666666666668,
        "step": 12056
    },
    {
        "loss": 1.6479,
        "grad_norm": 3.4364254474639893,
        "learning_rate": 0.0001281357401673998,
        "epoch": 1.6076000000000001,
        "step": 12057
    },
    {
        "loss": 1.1325,
        "grad_norm": 5.465395927429199,
        "learning_rate": 0.00012807534030408676,
        "epoch": 1.6077333333333335,
        "step": 12058
    },
    {
        "loss": 2.0483,
        "grad_norm": 4.787249565124512,
        "learning_rate": 0.00012801492932003577,
        "epoch": 1.6078666666666668,
        "step": 12059
    },
    {
        "loss": 1.2468,
        "grad_norm": 5.585445404052734,
        "learning_rate": 0.00012795450723917566,
        "epoch": 1.608,
        "step": 12060
    },
    {
        "loss": 2.1998,
        "grad_norm": 4.696710109710693,
        "learning_rate": 0.00012789407408543996,
        "epoch": 1.6081333333333334,
        "step": 12061
    },
    {
        "loss": 2.5228,
        "grad_norm": 2.7074663639068604,
        "learning_rate": 0.00012783362988276676,
        "epoch": 1.6082666666666667,
        "step": 12062
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.8989155292510986,
        "learning_rate": 0.00012777317465509767,
        "epoch": 1.6084,
        "step": 12063
    },
    {
        "loss": 1.6019,
        "grad_norm": 5.3549604415893555,
        "learning_rate": 0.00012771270842637958,
        "epoch": 1.6085333333333334,
        "step": 12064
    },
    {
        "loss": 1.1821,
        "grad_norm": 4.547832012176514,
        "learning_rate": 0.00012765223122056314,
        "epoch": 1.6086666666666667,
        "step": 12065
    },
    {
        "loss": 1.6773,
        "grad_norm": 2.597165584564209,
        "learning_rate": 0.0001275917430616038,
        "epoch": 1.6088,
        "step": 12066
    },
    {
        "loss": 2.1812,
        "grad_norm": 3.341290235519409,
        "learning_rate": 0.00012753124397346104,
        "epoch": 1.6089333333333333,
        "step": 12067
    },
    {
        "loss": 2.4739,
        "grad_norm": 4.73922872543335,
        "learning_rate": 0.00012747073398009873,
        "epoch": 1.6090666666666666,
        "step": 12068
    },
    {
        "loss": 1.875,
        "grad_norm": 4.409732818603516,
        "learning_rate": 0.00012741021310548492,
        "epoch": 1.6092,
        "step": 12069
    },
    {
        "loss": 1.8983,
        "grad_norm": 3.45697021484375,
        "learning_rate": 0.0001273496813735923,
        "epoch": 1.6093333333333333,
        "step": 12070
    },
    {
        "loss": 1.9408,
        "grad_norm": 5.423669338226318,
        "learning_rate": 0.000127289138808398,
        "epoch": 1.6094666666666666,
        "step": 12071
    },
    {
        "loss": 1.5159,
        "grad_norm": 3.941188097000122,
        "learning_rate": 0.00012722858543388253,
        "epoch": 1.6096,
        "step": 12072
    },
    {
        "loss": 2.0591,
        "grad_norm": 3.68281888961792,
        "learning_rate": 0.00012716802127403176,
        "epoch": 1.6097333333333332,
        "step": 12073
    },
    {
        "loss": 2.5002,
        "grad_norm": 4.165986061096191,
        "learning_rate": 0.00012710744635283504,
        "epoch": 1.6098666666666666,
        "step": 12074
    },
    {
        "loss": 2.8012,
        "grad_norm": 4.0171217918396,
        "learning_rate": 0.00012704686069428658,
        "epoch": 1.6099999999999999,
        "step": 12075
    },
    {
        "loss": 1.7223,
        "grad_norm": 4.780428409576416,
        "learning_rate": 0.00012698626432238442,
        "epoch": 1.6101333333333332,
        "step": 12076
    },
    {
        "loss": 2.5104,
        "grad_norm": 3.497321367263794,
        "learning_rate": 0.00012692565726113105,
        "epoch": 1.6102666666666665,
        "step": 12077
    },
    {
        "loss": 1.4691,
        "grad_norm": 5.440497875213623,
        "learning_rate": 0.00012686503953453294,
        "epoch": 1.6104,
        "step": 12078
    },
    {
        "loss": 3.1663,
        "grad_norm": 2.967031240463257,
        "learning_rate": 0.00012680441116660126,
        "epoch": 1.6105333333333334,
        "step": 12079
    },
    {
        "loss": 2.0567,
        "grad_norm": 6.52268648147583,
        "learning_rate": 0.000126743772181351,
        "epoch": 1.6106666666666667,
        "step": 12080
    },
    {
        "loss": 2.1332,
        "grad_norm": 2.879305601119995,
        "learning_rate": 0.0001266831226028013,
        "epoch": 1.6108,
        "step": 12081
    },
    {
        "loss": 2.8704,
        "grad_norm": 2.65682053565979,
        "learning_rate": 0.000126622462454976,
        "epoch": 1.6109333333333333,
        "step": 12082
    },
    {
        "loss": 1.5089,
        "grad_norm": 4.6180315017700195,
        "learning_rate": 0.00012656179176190254,
        "epoch": 1.6110666666666666,
        "step": 12083
    },
    {
        "loss": 2.6009,
        "grad_norm": 2.211488723754883,
        "learning_rate": 0.00012650111054761273,
        "epoch": 1.6112,
        "step": 12084
    },
    {
        "loss": 2.108,
        "grad_norm": 3.542494058609009,
        "learning_rate": 0.0001264404188361429,
        "epoch": 1.6113333333333333,
        "step": 12085
    },
    {
        "loss": 3.3626,
        "grad_norm": 5.3438849449157715,
        "learning_rate": 0.00012637971665153307,
        "epoch": 1.6114666666666668,
        "step": 12086
    },
    {
        "loss": 2.2908,
        "grad_norm": 2.896958351135254,
        "learning_rate": 0.00012631900401782745,
        "epoch": 1.6116000000000001,
        "step": 12087
    },
    {
        "loss": 2.1817,
        "grad_norm": 5.174956798553467,
        "learning_rate": 0.00012625828095907477,
        "epoch": 1.6117333333333335,
        "step": 12088
    },
    {
        "loss": 2.7264,
        "grad_norm": 1.8839302062988281,
        "learning_rate": 0.00012619754749932754,
        "epoch": 1.6118666666666668,
        "step": 12089
    },
    {
        "loss": 2.7945,
        "grad_norm": 2.5004935264587402,
        "learning_rate": 0.00012613680366264232,
        "epoch": 1.612,
        "step": 12090
    },
    {
        "loss": 2.3486,
        "grad_norm": 3.1492221355438232,
        "learning_rate": 0.00012607604947308029,
        "epoch": 1.6121333333333334,
        "step": 12091
    },
    {
        "loss": 1.3029,
        "grad_norm": 5.561349868774414,
        "learning_rate": 0.0001260152849547062,
        "epoch": 1.6122666666666667,
        "step": 12092
    },
    {
        "loss": 0.6136,
        "grad_norm": 3.7624738216400146,
        "learning_rate": 0.00012595451013158897,
        "epoch": 1.6124,
        "step": 12093
    },
    {
        "loss": 2.3874,
        "grad_norm": 3.1105666160583496,
        "learning_rate": 0.00012589372502780195,
        "epoch": 1.6125333333333334,
        "step": 12094
    },
    {
        "loss": 2.6326,
        "grad_norm": 4.159910678863525,
        "learning_rate": 0.00012583292966742224,
        "epoch": 1.6126666666666667,
        "step": 12095
    },
    {
        "loss": 2.2489,
        "grad_norm": 3.7136447429656982,
        "learning_rate": 0.00012577212407453105,
        "epoch": 1.6128,
        "step": 12096
    },
    {
        "loss": 2.1876,
        "grad_norm": 3.1159210205078125,
        "learning_rate": 0.00012571130827321355,
        "epoch": 1.6129333333333333,
        "step": 12097
    },
    {
        "loss": 2.1233,
        "grad_norm": 3.720757246017456,
        "learning_rate": 0.00012565048228755938,
        "epoch": 1.6130666666666666,
        "step": 12098
    },
    {
        "loss": 2.0967,
        "grad_norm": 4.441466808319092,
        "learning_rate": 0.00012558964614166164,
        "epoch": 1.6132,
        "step": 12099
    },
    {
        "loss": 1.7966,
        "grad_norm": 3.3515310287475586,
        "learning_rate": 0.00012552879985961815,
        "epoch": 1.6133333333333333,
        "step": 12100
    },
    {
        "loss": 1.9695,
        "grad_norm": 3.4328665733337402,
        "learning_rate": 0.00012546794346552975,
        "epoch": 1.6134666666666666,
        "step": 12101
    },
    {
        "loss": 2.3992,
        "grad_norm": 3.2798500061035156,
        "learning_rate": 0.00012540707698350222,
        "epoch": 1.6136,
        "step": 12102
    },
    {
        "loss": 2.8788,
        "grad_norm": 4.27874231338501,
        "learning_rate": 0.000125346200437645,
        "epoch": 1.6137333333333332,
        "step": 12103
    },
    {
        "loss": 2.0502,
        "grad_norm": 5.360260486602783,
        "learning_rate": 0.00012528531385207144,
        "epoch": 1.6138666666666666,
        "step": 12104
    },
    {
        "loss": 2.2036,
        "grad_norm": 8.506573677062988,
        "learning_rate": 0.0001252244172508989,
        "epoch": 1.6139999999999999,
        "step": 12105
    },
    {
        "loss": 2.553,
        "grad_norm": 4.634954929351807,
        "learning_rate": 0.00012516351065824862,
        "epoch": 1.6141333333333332,
        "step": 12106
    },
    {
        "loss": 2.6121,
        "grad_norm": 4.158702373504639,
        "learning_rate": 0.00012510259409824617,
        "epoch": 1.6142666666666665,
        "step": 12107
    },
    {
        "loss": 1.8721,
        "grad_norm": 4.758413791656494,
        "learning_rate": 0.00012504166759502047,
        "epoch": 1.6143999999999998,
        "step": 12108
    },
    {
        "loss": 2.6286,
        "grad_norm": 4.049917697906494,
        "learning_rate": 0.00012498073117270524,
        "epoch": 1.6145333333333334,
        "step": 12109
    },
    {
        "loss": 2.5673,
        "grad_norm": 3.356170177459717,
        "learning_rate": 0.000124919784855437,
        "epoch": 1.6146666666666667,
        "step": 12110
    },
    {
        "loss": 2.7142,
        "grad_norm": 3.114349603652954,
        "learning_rate": 0.000124858828667357,
        "epoch": 1.6148,
        "step": 12111
    },
    {
        "loss": 2.2065,
        "grad_norm": 3.627345323562622,
        "learning_rate": 0.00012479786263261048,
        "epoch": 1.6149333333333333,
        "step": 12112
    },
    {
        "loss": 1.2178,
        "grad_norm": 6.640990257263184,
        "learning_rate": 0.00012473688677534602,
        "epoch": 1.6150666666666667,
        "step": 12113
    },
    {
        "loss": 2.4686,
        "grad_norm": 4.700440406799316,
        "learning_rate": 0.00012467590111971642,
        "epoch": 1.6152,
        "step": 12114
    },
    {
        "loss": 1.7369,
        "grad_norm": 4.252650737762451,
        "learning_rate": 0.00012461490568987818,
        "epoch": 1.6153333333333333,
        "step": 12115
    },
    {
        "loss": 0.6817,
        "grad_norm": 3.0693798065185547,
        "learning_rate": 0.00012455390050999208,
        "epoch": 1.6154666666666668,
        "step": 12116
    },
    {
        "loss": 2.0619,
        "grad_norm": 4.061734199523926,
        "learning_rate": 0.0001244928856042223,
        "epoch": 1.6156000000000001,
        "step": 12117
    },
    {
        "loss": 1.3715,
        "grad_norm": 4.055041790008545,
        "learning_rate": 0.00012443186099673708,
        "epoch": 1.6157333333333335,
        "step": 12118
    },
    {
        "loss": 3.0573,
        "grad_norm": 5.468112945556641,
        "learning_rate": 0.00012437082671170834,
        "epoch": 1.6158666666666668,
        "step": 12119
    },
    {
        "loss": 2.3646,
        "grad_norm": 4.017026424407959,
        "learning_rate": 0.00012430978277331212,
        "epoch": 1.616,
        "step": 12120
    },
    {
        "loss": 0.8318,
        "grad_norm": 3.1834421157836914,
        "learning_rate": 0.00012424872920572833,
        "epoch": 1.6161333333333334,
        "step": 12121
    },
    {
        "loss": 2.0438,
        "grad_norm": 3.005359411239624,
        "learning_rate": 0.00012418766603314004,
        "epoch": 1.6162666666666667,
        "step": 12122
    },
    {
        "loss": 2.4512,
        "grad_norm": 3.7472894191741943,
        "learning_rate": 0.00012412659327973492,
        "epoch": 1.6164,
        "step": 12123
    },
    {
        "loss": 2.1005,
        "grad_norm": 4.282540798187256,
        "learning_rate": 0.00012406551096970388,
        "epoch": 1.6165333333333334,
        "step": 12124
    },
    {
        "loss": 2.5027,
        "grad_norm": 3.024564504623413,
        "learning_rate": 0.0001240044191272421,
        "epoch": 1.6166666666666667,
        "step": 12125
    },
    {
        "loss": 2.452,
        "grad_norm": 4.854211807250977,
        "learning_rate": 0.00012394331777654805,
        "epoch": 1.6168,
        "step": 12126
    },
    {
        "loss": 2.6609,
        "grad_norm": 2.45117449760437,
        "learning_rate": 0.00012388220694182432,
        "epoch": 1.6169333333333333,
        "step": 12127
    },
    {
        "loss": 2.6454,
        "grad_norm": 3.9185593128204346,
        "learning_rate": 0.00012382108664727687,
        "epoch": 1.6170666666666667,
        "step": 12128
    },
    {
        "loss": 3.0869,
        "grad_norm": 3.9921586513519287,
        "learning_rate": 0.0001237599569171158,
        "epoch": 1.6172,
        "step": 12129
    },
    {
        "loss": 2.2803,
        "grad_norm": 4.052432537078857,
        "learning_rate": 0.00012369881777555514,
        "epoch": 1.6173333333333333,
        "step": 12130
    },
    {
        "loss": 1.832,
        "grad_norm": 5.0342183113098145,
        "learning_rate": 0.00012363766924681176,
        "epoch": 1.6174666666666666,
        "step": 12131
    },
    {
        "loss": 2.6289,
        "grad_norm": 3.712226152420044,
        "learning_rate": 0.00012357651135510715,
        "epoch": 1.6176,
        "step": 12132
    },
    {
        "loss": 2.6889,
        "grad_norm": 2.7224483489990234,
        "learning_rate": 0.00012351534412466592,
        "epoch": 1.6177333333333332,
        "step": 12133
    },
    {
        "loss": 1.5696,
        "grad_norm": 4.015711307525635,
        "learning_rate": 0.0001234541675797169,
        "epoch": 1.6178666666666666,
        "step": 12134
    },
    {
        "loss": 1.6662,
        "grad_norm": 5.269691467285156,
        "learning_rate": 0.00012339298174449218,
        "epoch": 1.6179999999999999,
        "step": 12135
    },
    {
        "loss": 2.5081,
        "grad_norm": 3.463249921798706,
        "learning_rate": 0.0001233317866432277,
        "epoch": 1.6181333333333332,
        "step": 12136
    },
    {
        "loss": 1.7394,
        "grad_norm": 3.299506187438965,
        "learning_rate": 0.00012327058230016287,
        "epoch": 1.6182666666666665,
        "step": 12137
    },
    {
        "loss": 2.151,
        "grad_norm": 5.23679256439209,
        "learning_rate": 0.00012320936873954125,
        "epoch": 1.6183999999999998,
        "step": 12138
    },
    {
        "loss": 1.756,
        "grad_norm": 4.192372798919678,
        "learning_rate": 0.0001231481459856096,
        "epoch": 1.6185333333333334,
        "step": 12139
    },
    {
        "loss": 2.9351,
        "grad_norm": 3.7525928020477295,
        "learning_rate": 0.00012308691406261827,
        "epoch": 1.6186666666666667,
        "step": 12140
    },
    {
        "loss": 2.227,
        "grad_norm": 3.192661762237549,
        "learning_rate": 0.00012302567299482179,
        "epoch": 1.6188,
        "step": 12141
    },
    {
        "loss": 1.5497,
        "grad_norm": 2.4602489471435547,
        "learning_rate": 0.00012296442280647775,
        "epoch": 1.6189333333333333,
        "step": 12142
    },
    {
        "loss": 1.8116,
        "grad_norm": 3.2096080780029297,
        "learning_rate": 0.00012290316352184748,
        "epoch": 1.6190666666666667,
        "step": 12143
    },
    {
        "loss": 0.5234,
        "grad_norm": 2.79552960395813,
        "learning_rate": 0.0001228418951651962,
        "epoch": 1.6192,
        "step": 12144
    },
    {
        "loss": 2.0522,
        "grad_norm": 3.745108127593994,
        "learning_rate": 0.00012278061776079243,
        "epoch": 1.6193333333333333,
        "step": 12145
    },
    {
        "loss": 2.0269,
        "grad_norm": 2.9031920433044434,
        "learning_rate": 0.00012271933133290821,
        "epoch": 1.6194666666666668,
        "step": 12146
    },
    {
        "loss": 1.9992,
        "grad_norm": 3.5158400535583496,
        "learning_rate": 0.00012265803590581957,
        "epoch": 1.6196000000000002,
        "step": 12147
    },
    {
        "loss": 2.9361,
        "grad_norm": 4.644580841064453,
        "learning_rate": 0.0001225967315038057,
        "epoch": 1.6197333333333335,
        "step": 12148
    },
    {
        "loss": 1.3949,
        "grad_norm": 4.754332542419434,
        "learning_rate": 0.00012253541815114937,
        "epoch": 1.6198666666666668,
        "step": 12149
    },
    {
        "loss": 1.8847,
        "grad_norm": 4.053832530975342,
        "learning_rate": 0.00012247409587213724,
        "epoch": 1.62,
        "step": 12150
    },
    {
        "loss": 2.5134,
        "grad_norm": 3.2719850540161133,
        "learning_rate": 0.00012241276469105914,
        "epoch": 1.6201333333333334,
        "step": 12151
    },
    {
        "loss": 1.2546,
        "grad_norm": 3.5047268867492676,
        "learning_rate": 0.00012235142463220845,
        "epoch": 1.6202666666666667,
        "step": 12152
    },
    {
        "loss": 2.5144,
        "grad_norm": 4.73149299621582,
        "learning_rate": 0.00012229007571988245,
        "epoch": 1.6204,
        "step": 12153
    },
    {
        "loss": 1.8036,
        "grad_norm": 4.377925395965576,
        "learning_rate": 0.00012222871797838152,
        "epoch": 1.6205333333333334,
        "step": 12154
    },
    {
        "loss": 1.2063,
        "grad_norm": 5.369540214538574,
        "learning_rate": 0.00012216735143200962,
        "epoch": 1.6206666666666667,
        "step": 12155
    },
    {
        "loss": 2.2125,
        "grad_norm": 5.067526817321777,
        "learning_rate": 0.00012210597610507414,
        "epoch": 1.6208,
        "step": 12156
    },
    {
        "loss": 2.0988,
        "grad_norm": 4.594137191772461,
        "learning_rate": 0.00012204459202188639,
        "epoch": 1.6209333333333333,
        "step": 12157
    },
    {
        "loss": 2.3609,
        "grad_norm": 3.4563608169555664,
        "learning_rate": 0.00012198319920676045,
        "epoch": 1.6210666666666667,
        "step": 12158
    },
    {
        "loss": 2.2693,
        "grad_norm": 4.050284385681152,
        "learning_rate": 0.00012192179768401465,
        "epoch": 1.6212,
        "step": 12159
    },
    {
        "loss": 2.2543,
        "grad_norm": 2.5806314945220947,
        "learning_rate": 0.00012186038747796988,
        "epoch": 1.6213333333333333,
        "step": 12160
    },
    {
        "loss": 2.1702,
        "grad_norm": 3.2195706367492676,
        "learning_rate": 0.0001217989686129511,
        "epoch": 1.6214666666666666,
        "step": 12161
    },
    {
        "loss": 1.9772,
        "grad_norm": 4.632390022277832,
        "learning_rate": 0.00012173754111328672,
        "epoch": 1.6216,
        "step": 12162
    },
    {
        "loss": 2.7098,
        "grad_norm": 3.0544068813323975,
        "learning_rate": 0.00012167610500330823,
        "epoch": 1.6217333333333332,
        "step": 12163
    },
    {
        "loss": 1.57,
        "grad_norm": 4.41778039932251,
        "learning_rate": 0.00012161466030735071,
        "epoch": 1.6218666666666666,
        "step": 12164
    },
    {
        "loss": 2.1641,
        "grad_norm": 3.4129440784454346,
        "learning_rate": 0.00012155320704975243,
        "epoch": 1.6219999999999999,
        "step": 12165
    },
    {
        "loss": 2.0132,
        "grad_norm": 2.7813913822174072,
        "learning_rate": 0.00012149174525485556,
        "epoch": 1.6221333333333332,
        "step": 12166
    },
    {
        "loss": 1.9543,
        "grad_norm": 4.158891201019287,
        "learning_rate": 0.000121430274947005,
        "epoch": 1.6222666666666665,
        "step": 12167
    },
    {
        "loss": 2.2273,
        "grad_norm": 3.1811249256134033,
        "learning_rate": 0.00012136879615054982,
        "epoch": 1.6223999999999998,
        "step": 12168
    },
    {
        "loss": 2.1609,
        "grad_norm": 6.279452800750732,
        "learning_rate": 0.00012130730888984137,
        "epoch": 1.6225333333333334,
        "step": 12169
    },
    {
        "loss": 2.9536,
        "grad_norm": 5.223134994506836,
        "learning_rate": 0.00012124581318923526,
        "epoch": 1.6226666666666667,
        "step": 12170
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.898585557937622,
        "learning_rate": 0.0001211843090730903,
        "epoch": 1.6228,
        "step": 12171
    },
    {
        "loss": 2.0324,
        "grad_norm": 3.576507568359375,
        "learning_rate": 0.00012112279656576835,
        "epoch": 1.6229333333333333,
        "step": 12172
    },
    {
        "loss": 2.4323,
        "grad_norm": 4.29739236831665,
        "learning_rate": 0.00012106127569163468,
        "epoch": 1.6230666666666667,
        "step": 12173
    },
    {
        "loss": 2.4863,
        "grad_norm": 3.980271577835083,
        "learning_rate": 0.0001209997464750578,
        "epoch": 1.6232,
        "step": 12174
    },
    {
        "loss": 0.7762,
        "grad_norm": 2.9200427532196045,
        "learning_rate": 0.00012093820894040996,
        "epoch": 1.6233333333333333,
        "step": 12175
    },
    {
        "loss": 0.688,
        "grad_norm": 3.426581621170044,
        "learning_rate": 0.0001208766631120662,
        "epoch": 1.6234666666666666,
        "step": 12176
    },
    {
        "loss": 1.277,
        "grad_norm": 3.922887086868286,
        "learning_rate": 0.00012081510901440502,
        "epoch": 1.6236000000000002,
        "step": 12177
    },
    {
        "loss": 2.0103,
        "grad_norm": 4.7829670906066895,
        "learning_rate": 0.00012075354667180811,
        "epoch": 1.6237333333333335,
        "step": 12178
    },
    {
        "loss": 2.1868,
        "grad_norm": 3.8379719257354736,
        "learning_rate": 0.00012069197610866066,
        "epoch": 1.6238666666666668,
        "step": 12179
    },
    {
        "loss": 2.1117,
        "grad_norm": 3.890845775604248,
        "learning_rate": 0.00012063039734935125,
        "epoch": 1.624,
        "step": 12180
    },
    {
        "loss": 0.9892,
        "grad_norm": 4.004106521606445,
        "learning_rate": 0.00012056881041827086,
        "epoch": 1.6241333333333334,
        "step": 12181
    },
    {
        "loss": 1.6596,
        "grad_norm": 4.9627180099487305,
        "learning_rate": 0.00012050721533981471,
        "epoch": 1.6242666666666667,
        "step": 12182
    },
    {
        "loss": 2.3732,
        "grad_norm": 3.0186057090759277,
        "learning_rate": 0.00012044561213838054,
        "epoch": 1.6244,
        "step": 12183
    },
    {
        "loss": 1.8645,
        "grad_norm": 3.901752471923828,
        "learning_rate": 0.00012038400083836992,
        "epoch": 1.6245333333333334,
        "step": 12184
    },
    {
        "loss": 2.1964,
        "grad_norm": 4.1953535079956055,
        "learning_rate": 0.00012032238146418707,
        "epoch": 1.6246666666666667,
        "step": 12185
    },
    {
        "loss": 2.1072,
        "grad_norm": 2.8318355083465576,
        "learning_rate": 0.00012026075404023969,
        "epoch": 1.6248,
        "step": 12186
    },
    {
        "loss": 2.341,
        "grad_norm": 3.672797441482544,
        "learning_rate": 0.00012019911859093844,
        "epoch": 1.6249333333333333,
        "step": 12187
    },
    {
        "loss": 2.6437,
        "grad_norm": 4.070271015167236,
        "learning_rate": 0.00012013747514069753,
        "epoch": 1.6250666666666667,
        "step": 12188
    },
    {
        "loss": 2.375,
        "grad_norm": 3.135019302368164,
        "learning_rate": 0.00012007582371393433,
        "epoch": 1.6252,
        "step": 12189
    },
    {
        "loss": 2.1049,
        "grad_norm": 2.3515281677246094,
        "learning_rate": 0.00012001416433506871,
        "epoch": 1.6253333333333333,
        "step": 12190
    },
    {
        "loss": 2.1947,
        "grad_norm": 4.41792631149292,
        "learning_rate": 0.00011995249702852452,
        "epoch": 1.6254666666666666,
        "step": 12191
    },
    {
        "loss": 2.4283,
        "grad_norm": 4.147940158843994,
        "learning_rate": 0.00011989082181872815,
        "epoch": 1.6256,
        "step": 12192
    },
    {
        "loss": 2.3665,
        "grad_norm": 3.3957953453063965,
        "learning_rate": 0.00011982913873010959,
        "epoch": 1.6257333333333333,
        "step": 12193
    },
    {
        "loss": 3.1034,
        "grad_norm": 3.1502444744110107,
        "learning_rate": 0.00011976744778710162,
        "epoch": 1.6258666666666666,
        "step": 12194
    },
    {
        "loss": 2.3425,
        "grad_norm": 4.24669885635376,
        "learning_rate": 0.00011970574901414024,
        "epoch": 1.626,
        "step": 12195
    },
    {
        "loss": 2.1261,
        "grad_norm": 3.721909999847412,
        "learning_rate": 0.0001196440424356644,
        "epoch": 1.6261333333333332,
        "step": 12196
    },
    {
        "loss": 2.2551,
        "grad_norm": 4.032421588897705,
        "learning_rate": 0.00011958232807611653,
        "epoch": 1.6262666666666665,
        "step": 12197
    },
    {
        "loss": 2.134,
        "grad_norm": 3.1110622882843018,
        "learning_rate": 0.00011952060595994179,
        "epoch": 1.6263999999999998,
        "step": 12198
    },
    {
        "loss": 1.7728,
        "grad_norm": 4.620967388153076,
        "learning_rate": 0.0001194588761115884,
        "epoch": 1.6265333333333334,
        "step": 12199
    },
    {
        "loss": 0.8849,
        "grad_norm": 3.0427913665771484,
        "learning_rate": 0.00011939713855550798,
        "epoch": 1.6266666666666667,
        "step": 12200
    },
    {
        "loss": 2.2215,
        "grad_norm": 3.4573516845703125,
        "learning_rate": 0.00011933539331615491,
        "epoch": 1.6268,
        "step": 12201
    },
    {
        "loss": 2.3978,
        "grad_norm": 3.693532705307007,
        "learning_rate": 0.00011927364041798653,
        "epoch": 1.6269333333333333,
        "step": 12202
    },
    {
        "loss": 2.0214,
        "grad_norm": 3.0096540451049805,
        "learning_rate": 0.00011921187988546361,
        "epoch": 1.6270666666666667,
        "step": 12203
    },
    {
        "loss": 2.8154,
        "grad_norm": 3.4322052001953125,
        "learning_rate": 0.00011915011174304964,
        "epoch": 1.6272,
        "step": 12204
    },
    {
        "loss": 2.678,
        "grad_norm": 3.919504165649414,
        "learning_rate": 0.00011908833601521099,
        "epoch": 1.6273333333333333,
        "step": 12205
    },
    {
        "loss": 2.2753,
        "grad_norm": 3.9084601402282715,
        "learning_rate": 0.00011902655272641757,
        "epoch": 1.6274666666666666,
        "step": 12206
    },
    {
        "loss": 2.3697,
        "grad_norm": 2.6291468143463135,
        "learning_rate": 0.00011896476190114177,
        "epoch": 1.6276000000000002,
        "step": 12207
    },
    {
        "loss": 2.9282,
        "grad_norm": 4.414319038391113,
        "learning_rate": 0.000118902963563859,
        "epoch": 1.6277333333333335,
        "step": 12208
    },
    {
        "loss": 1.7027,
        "grad_norm": 3.082990884780884,
        "learning_rate": 0.00011884115773904811,
        "epoch": 1.6278666666666668,
        "step": 12209
    },
    {
        "loss": 2.4652,
        "grad_norm": 3.5026357173919678,
        "learning_rate": 0.00011877934445119045,
        "epoch": 1.6280000000000001,
        "step": 12210
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.2015135288238525,
        "learning_rate": 0.00011871752372477032,
        "epoch": 1.6281333333333334,
        "step": 12211
    },
    {
        "loss": 2.1257,
        "grad_norm": 5.175387859344482,
        "learning_rate": 0.00011865569558427538,
        "epoch": 1.6282666666666668,
        "step": 12212
    },
    {
        "loss": 2.3324,
        "grad_norm": 3.470799684524536,
        "learning_rate": 0.00011859386005419588,
        "epoch": 1.6284,
        "step": 12213
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.8265669345855713,
        "learning_rate": 0.00011853201715902504,
        "epoch": 1.6285333333333334,
        "step": 12214
    },
    {
        "loss": 2.7951,
        "grad_norm": 2.7307050228118896,
        "learning_rate": 0.00011847016692325894,
        "epoch": 1.6286666666666667,
        "step": 12215
    },
    {
        "loss": 1.1956,
        "grad_norm": 5.657410144805908,
        "learning_rate": 0.00011840830937139689,
        "epoch": 1.6288,
        "step": 12216
    },
    {
        "loss": 2.1671,
        "grad_norm": 2.5520379543304443,
        "learning_rate": 0.00011834644452794066,
        "epoch": 1.6289333333333333,
        "step": 12217
    },
    {
        "loss": 2.3625,
        "grad_norm": 5.371370792388916,
        "learning_rate": 0.0001182845724173955,
        "epoch": 1.6290666666666667,
        "step": 12218
    },
    {
        "loss": 2.162,
        "grad_norm": 3.1116890907287598,
        "learning_rate": 0.00011822269306426865,
        "epoch": 1.6292,
        "step": 12219
    },
    {
        "loss": 1.6136,
        "grad_norm": 3.079694986343384,
        "learning_rate": 0.00011816080649307096,
        "epoch": 1.6293333333333333,
        "step": 12220
    },
    {
        "loss": 2.9548,
        "grad_norm": 3.0141332149505615,
        "learning_rate": 0.00011809891272831608,
        "epoch": 1.6294666666666666,
        "step": 12221
    },
    {
        "loss": 1.9645,
        "grad_norm": 4.093583583831787,
        "learning_rate": 0.00011803701179452023,
        "epoch": 1.6296,
        "step": 12222
    },
    {
        "loss": 2.0826,
        "grad_norm": 4.427276611328125,
        "learning_rate": 0.0001179751037162025,
        "epoch": 1.6297333333333333,
        "step": 12223
    },
    {
        "loss": 2.285,
        "grad_norm": 4.3564839363098145,
        "learning_rate": 0.00011791318851788482,
        "epoch": 1.6298666666666666,
        "step": 12224
    },
    {
        "loss": 2.1884,
        "grad_norm": 4.6183061599731445,
        "learning_rate": 0.00011785126622409228,
        "epoch": 1.63,
        "step": 12225
    },
    {
        "loss": 1.9004,
        "grad_norm": 4.030752182006836,
        "learning_rate": 0.0001177893368593522,
        "epoch": 1.6301333333333332,
        "step": 12226
    },
    {
        "loss": 2.8077,
        "grad_norm": 3.792898416519165,
        "learning_rate": 0.00011772740044819548,
        "epoch": 1.6302666666666665,
        "step": 12227
    },
    {
        "loss": 2.6327,
        "grad_norm": 3.781581401824951,
        "learning_rate": 0.00011766545701515474,
        "epoch": 1.6303999999999998,
        "step": 12228
    },
    {
        "loss": 3.0611,
        "grad_norm": 4.633982181549072,
        "learning_rate": 0.00011760350658476629,
        "epoch": 1.6305333333333332,
        "step": 12229
    },
    {
        "loss": 2.5825,
        "grad_norm": 4.281510353088379,
        "learning_rate": 0.00011754154918156903,
        "epoch": 1.6306666666666667,
        "step": 12230
    },
    {
        "loss": 0.9625,
        "grad_norm": 6.593982696533203,
        "learning_rate": 0.00011747958483010437,
        "epoch": 1.6308,
        "step": 12231
    },
    {
        "loss": 2.9317,
        "grad_norm": 4.121029853820801,
        "learning_rate": 0.00011741761355491661,
        "epoch": 1.6309333333333333,
        "step": 12232
    },
    {
        "loss": 2.1332,
        "grad_norm": 3.2302746772766113,
        "learning_rate": 0.00011735563538055263,
        "epoch": 1.6310666666666667,
        "step": 12233
    },
    {
        "loss": 1.4032,
        "grad_norm": 5.4652862548828125,
        "learning_rate": 0.00011729365033156245,
        "epoch": 1.6312,
        "step": 12234
    },
    {
        "loss": 3.0701,
        "grad_norm": 3.16662335395813,
        "learning_rate": 0.00011723165843249848,
        "epoch": 1.6313333333333333,
        "step": 12235
    },
    {
        "loss": 2.2566,
        "grad_norm": 3.1321659088134766,
        "learning_rate": 0.00011716965970791589,
        "epoch": 1.6314666666666666,
        "step": 12236
    },
    {
        "loss": 2.6486,
        "grad_norm": 2.5710856914520264,
        "learning_rate": 0.00011710765418237245,
        "epoch": 1.6316000000000002,
        "step": 12237
    },
    {
        "loss": 1.6616,
        "grad_norm": 3.6432044506073,
        "learning_rate": 0.00011704564188042886,
        "epoch": 1.6317333333333335,
        "step": 12238
    },
    {
        "loss": 2.0583,
        "grad_norm": 3.0138461589813232,
        "learning_rate": 0.00011698362282664874,
        "epoch": 1.6318666666666668,
        "step": 12239
    },
    {
        "loss": 3.0235,
        "grad_norm": 5.146448135375977,
        "learning_rate": 0.00011692159704559749,
        "epoch": 1.6320000000000001,
        "step": 12240
    },
    {
        "loss": 2.6231,
        "grad_norm": 4.696277618408203,
        "learning_rate": 0.0001168595645618441,
        "epoch": 1.6321333333333334,
        "step": 12241
    },
    {
        "loss": 2.0776,
        "grad_norm": 3.839386463165283,
        "learning_rate": 0.00011679752539995961,
        "epoch": 1.6322666666666668,
        "step": 12242
    },
    {
        "loss": 2.3208,
        "grad_norm": 3.9769949913024902,
        "learning_rate": 0.0001167354795845182,
        "epoch": 1.6324,
        "step": 12243
    },
    {
        "loss": 2.0423,
        "grad_norm": 2.0826127529144287,
        "learning_rate": 0.00011667342714009632,
        "epoch": 1.6325333333333334,
        "step": 12244
    },
    {
        "loss": 2.6604,
        "grad_norm": 3.4808456897735596,
        "learning_rate": 0.0001166113680912731,
        "epoch": 1.6326666666666667,
        "step": 12245
    },
    {
        "loss": 2.5435,
        "grad_norm": 2.998917818069458,
        "learning_rate": 0.0001165493024626303,
        "epoch": 1.6328,
        "step": 12246
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.6795248985290527,
        "learning_rate": 0.00011648723027875236,
        "epoch": 1.6329333333333333,
        "step": 12247
    },
    {
        "loss": 0.6177,
        "grad_norm": 2.6814627647399902,
        "learning_rate": 0.00011642515156422662,
        "epoch": 1.6330666666666667,
        "step": 12248
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.2122812271118164,
        "learning_rate": 0.00011636306634364208,
        "epoch": 1.6332,
        "step": 12249
    },
    {
        "loss": 1.9787,
        "grad_norm": 2.9485361576080322,
        "learning_rate": 0.00011630097464159137,
        "epoch": 1.6333333333333333,
        "step": 12250
    },
    {
        "loss": 1.9563,
        "grad_norm": 2.959019422531128,
        "learning_rate": 0.00011623887648266894,
        "epoch": 1.6334666666666666,
        "step": 12251
    },
    {
        "loss": 0.8892,
        "grad_norm": 3.708307981491089,
        "learning_rate": 0.0001161767718914723,
        "epoch": 1.6336,
        "step": 12252
    },
    {
        "loss": 2.7275,
        "grad_norm": 2.8315370082855225,
        "learning_rate": 0.00011611466089260122,
        "epoch": 1.6337333333333333,
        "step": 12253
    },
    {
        "loss": 2.4683,
        "grad_norm": 3.0384202003479004,
        "learning_rate": 0.00011605254351065802,
        "epoch": 1.6338666666666666,
        "step": 12254
    },
    {
        "loss": 2.6314,
        "grad_norm": 5.468266487121582,
        "learning_rate": 0.00011599041977024751,
        "epoch": 1.634,
        "step": 12255
    },
    {
        "loss": 2.6612,
        "grad_norm": 2.8579728603363037,
        "learning_rate": 0.0001159282896959774,
        "epoch": 1.6341333333333332,
        "step": 12256
    },
    {
        "loss": 2.0158,
        "grad_norm": 4.326852321624756,
        "learning_rate": 0.00011586615331245747,
        "epoch": 1.6342666666666665,
        "step": 12257
    },
    {
        "loss": 2.0872,
        "grad_norm": 4.998522758483887,
        "learning_rate": 0.0001158040106443,
        "epoch": 1.6343999999999999,
        "step": 12258
    },
    {
        "loss": 2.0819,
        "grad_norm": 3.506317377090454,
        "learning_rate": 0.00011574186171612019,
        "epoch": 1.6345333333333332,
        "step": 12259
    },
    {
        "loss": 2.346,
        "grad_norm": 5.017430782318115,
        "learning_rate": 0.00011567970655253533,
        "epoch": 1.6346666666666667,
        "step": 12260
    },
    {
        "loss": 2.7312,
        "grad_norm": 4.407722473144531,
        "learning_rate": 0.00011561754517816515,
        "epoch": 1.6348,
        "step": 12261
    },
    {
        "loss": 2.4894,
        "grad_norm": 2.852391004562378,
        "learning_rate": 0.00011555537761763222,
        "epoch": 1.6349333333333333,
        "step": 12262
    },
    {
        "loss": 2.1574,
        "grad_norm": 2.7999074459075928,
        "learning_rate": 0.0001154932038955612,
        "epoch": 1.6350666666666667,
        "step": 12263
    },
    {
        "loss": 2.5849,
        "grad_norm": 3.16127872467041,
        "learning_rate": 0.00011543102403657935,
        "epoch": 1.6352,
        "step": 12264
    },
    {
        "loss": 1.8141,
        "grad_norm": 5.103236675262451,
        "learning_rate": 0.00011536883806531617,
        "epoch": 1.6353333333333333,
        "step": 12265
    },
    {
        "loss": 2.1461,
        "grad_norm": 4.144482612609863,
        "learning_rate": 0.00011530664600640402,
        "epoch": 1.6354666666666666,
        "step": 12266
    },
    {
        "loss": 1.9045,
        "grad_norm": 6.966902732849121,
        "learning_rate": 0.00011524444788447703,
        "epoch": 1.6356000000000002,
        "step": 12267
    },
    {
        "loss": 2.2991,
        "grad_norm": 3.011570930480957,
        "learning_rate": 0.00011518224372417247,
        "epoch": 1.6357333333333335,
        "step": 12268
    },
    {
        "loss": 1.7113,
        "grad_norm": 3.2029435634613037,
        "learning_rate": 0.00011512003355012944,
        "epoch": 1.6358666666666668,
        "step": 12269
    },
    {
        "loss": 2.5591,
        "grad_norm": 2.904938220977783,
        "learning_rate": 0.00011505781738698944,
        "epoch": 1.6360000000000001,
        "step": 12270
    },
    {
        "loss": 2.0968,
        "grad_norm": 4.841294765472412,
        "learning_rate": 0.00011499559525939682,
        "epoch": 1.6361333333333334,
        "step": 12271
    },
    {
        "loss": 2.5752,
        "grad_norm": 3.0358288288116455,
        "learning_rate": 0.00011493336719199783,
        "epoch": 1.6362666666666668,
        "step": 12272
    },
    {
        "loss": 2.1052,
        "grad_norm": 3.6656157970428467,
        "learning_rate": 0.0001148711332094412,
        "epoch": 1.6364,
        "step": 12273
    },
    {
        "loss": 2.422,
        "grad_norm": 3.3621864318847656,
        "learning_rate": 0.00011480889333637787,
        "epoch": 1.6365333333333334,
        "step": 12274
    },
    {
        "loss": 2.0956,
        "grad_norm": 4.065418243408203,
        "learning_rate": 0.0001147466475974616,
        "epoch": 1.6366666666666667,
        "step": 12275
    },
    {
        "loss": 1.9283,
        "grad_norm": 3.357085943222046,
        "learning_rate": 0.00011468439601734784,
        "epoch": 1.6368,
        "step": 12276
    },
    {
        "loss": 2.3137,
        "grad_norm": 3.0245168209075928,
        "learning_rate": 0.00011462213862069504,
        "epoch": 1.6369333333333334,
        "step": 12277
    },
    {
        "loss": 2.2145,
        "grad_norm": 3.0552215576171875,
        "learning_rate": 0.00011455987543216303,
        "epoch": 1.6370666666666667,
        "step": 12278
    },
    {
        "loss": 2.3906,
        "grad_norm": 4.0495500564575195,
        "learning_rate": 0.00011449760647641474,
        "epoch": 1.6372,
        "step": 12279
    },
    {
        "loss": 1.843,
        "grad_norm": 5.537553787231445,
        "learning_rate": 0.0001144353317781153,
        "epoch": 1.6373333333333333,
        "step": 12280
    },
    {
        "loss": 2.0742,
        "grad_norm": 3.9443776607513428,
        "learning_rate": 0.0001143730513619317,
        "epoch": 1.6374666666666666,
        "step": 12281
    },
    {
        "loss": 1.5749,
        "grad_norm": 4.428731441497803,
        "learning_rate": 0.00011431076525253352,
        "epoch": 1.6376,
        "step": 12282
    },
    {
        "loss": 2.1604,
        "grad_norm": 4.128113746643066,
        "learning_rate": 0.00011424847347459233,
        "epoch": 1.6377333333333333,
        "step": 12283
    },
    {
        "loss": 2.761,
        "grad_norm": 3.425724983215332,
        "learning_rate": 0.00011418617605278237,
        "epoch": 1.6378666666666666,
        "step": 12284
    },
    {
        "loss": 2.394,
        "grad_norm": 4.285565376281738,
        "learning_rate": 0.00011412387301177972,
        "epoch": 1.638,
        "step": 12285
    },
    {
        "loss": 1.4721,
        "grad_norm": 2.8726158142089844,
        "learning_rate": 0.00011406156437626285,
        "epoch": 1.6381333333333332,
        "step": 12286
    },
    {
        "loss": 2.4774,
        "grad_norm": 3.2784805297851562,
        "learning_rate": 0.00011399925017091231,
        "epoch": 1.6382666666666665,
        "step": 12287
    },
    {
        "loss": 2.0957,
        "grad_norm": 3.745250940322876,
        "learning_rate": 0.00011393693042041102,
        "epoch": 1.6383999999999999,
        "step": 12288
    },
    {
        "loss": 2.1014,
        "grad_norm": 3.640058755874634,
        "learning_rate": 0.0001138746051494443,
        "epoch": 1.6385333333333332,
        "step": 12289
    },
    {
        "loss": 2.5039,
        "grad_norm": 3.2416062355041504,
        "learning_rate": 0.00011381227438269917,
        "epoch": 1.6386666666666667,
        "step": 12290
    },
    {
        "loss": 1.2723,
        "grad_norm": 4.700285911560059,
        "learning_rate": 0.00011374993814486512,
        "epoch": 1.6388,
        "step": 12291
    },
    {
        "loss": 2.6348,
        "grad_norm": 5.291651725769043,
        "learning_rate": 0.00011368759646063354,
        "epoch": 1.6389333333333334,
        "step": 12292
    },
    {
        "loss": 2.3011,
        "grad_norm": 3.4223673343658447,
        "learning_rate": 0.00011362524935469853,
        "epoch": 1.6390666666666667,
        "step": 12293
    },
    {
        "loss": 2.9231,
        "grad_norm": 3.7012970447540283,
        "learning_rate": 0.00011356289685175583,
        "epoch": 1.6392,
        "step": 12294
    },
    {
        "loss": 2.0657,
        "grad_norm": 3.529951572418213,
        "learning_rate": 0.00011350053897650345,
        "epoch": 1.6393333333333333,
        "step": 12295
    },
    {
        "loss": 2.5596,
        "grad_norm": 4.4063401222229,
        "learning_rate": 0.00011343817575364144,
        "epoch": 1.6394666666666666,
        "step": 12296
    },
    {
        "loss": 2.2267,
        "grad_norm": 4.546873569488525,
        "learning_rate": 0.00011337580720787224,
        "epoch": 1.6396,
        "step": 12297
    },
    {
        "loss": 2.8224,
        "grad_norm": 3.440638780593872,
        "learning_rate": 0.0001133134333639005,
        "epoch": 1.6397333333333335,
        "step": 12298
    },
    {
        "loss": 1.7064,
        "grad_norm": 4.836916446685791,
        "learning_rate": 0.00011325105424643217,
        "epoch": 1.6398666666666668,
        "step": 12299
    },
    {
        "loss": 2.8675,
        "grad_norm": 3.963062286376953,
        "learning_rate": 0.00011318866988017622,
        "epoch": 1.6400000000000001,
        "step": 12300
    },
    {
        "loss": 2.2854,
        "grad_norm": 3.5809710025787354,
        "learning_rate": 0.0001131262802898431,
        "epoch": 1.6401333333333334,
        "step": 12301
    },
    {
        "loss": 1.771,
        "grad_norm": 3.2531051635742188,
        "learning_rate": 0.0001130638855001457,
        "epoch": 1.6402666666666668,
        "step": 12302
    },
    {
        "loss": 1.9027,
        "grad_norm": 3.5567777156829834,
        "learning_rate": 0.00011300148553579877,
        "epoch": 1.6404,
        "step": 12303
    },
    {
        "loss": 3.0236,
        "grad_norm": 5.5230560302734375,
        "learning_rate": 0.00011293908042151913,
        "epoch": 1.6405333333333334,
        "step": 12304
    },
    {
        "loss": 1.8701,
        "grad_norm": 4.745451927185059,
        "learning_rate": 0.0001128766701820255,
        "epoch": 1.6406666666666667,
        "step": 12305
    },
    {
        "loss": 1.146,
        "grad_norm": 3.672351121902466,
        "learning_rate": 0.00011281425484203893,
        "epoch": 1.6408,
        "step": 12306
    },
    {
        "loss": 2.1511,
        "grad_norm": 4.118398189544678,
        "learning_rate": 0.00011275183442628263,
        "epoch": 1.6409333333333334,
        "step": 12307
    },
    {
        "loss": 1.3652,
        "grad_norm": 2.558868169784546,
        "learning_rate": 0.00011268940895948099,
        "epoch": 1.6410666666666667,
        "step": 12308
    },
    {
        "loss": 2.4468,
        "grad_norm": 1.792904257774353,
        "learning_rate": 0.00011262697846636139,
        "epoch": 1.6412,
        "step": 12309
    },
    {
        "loss": 2.6426,
        "grad_norm": 2.981027126312256,
        "learning_rate": 0.0001125645429716524,
        "epoch": 1.6413333333333333,
        "step": 12310
    },
    {
        "loss": 1.9446,
        "grad_norm": 3.5787155628204346,
        "learning_rate": 0.0001125021025000853,
        "epoch": 1.6414666666666666,
        "step": 12311
    },
    {
        "loss": 1.626,
        "grad_norm": 4.502116680145264,
        "learning_rate": 0.00011243965707639279,
        "epoch": 1.6416,
        "step": 12312
    },
    {
        "loss": 2.243,
        "grad_norm": 3.954742670059204,
        "learning_rate": 0.00011237720672530973,
        "epoch": 1.6417333333333333,
        "step": 12313
    },
    {
        "loss": 3.4587,
        "grad_norm": 3.003143787384033,
        "learning_rate": 0.00011231475147157275,
        "epoch": 1.6418666666666666,
        "step": 12314
    },
    {
        "loss": 1.7198,
        "grad_norm": 3.876873016357422,
        "learning_rate": 0.00011225229133992092,
        "epoch": 1.642,
        "step": 12315
    },
    {
        "loss": 2.7958,
        "grad_norm": 2.9541797637939453,
        "learning_rate": 0.00011218982635509474,
        "epoch": 1.6421333333333332,
        "step": 12316
    },
    {
        "loss": 0.904,
        "grad_norm": 3.868579149246216,
        "learning_rate": 0.00011212735654183664,
        "epoch": 1.6422666666666665,
        "step": 12317
    },
    {
        "loss": 2.4534,
        "grad_norm": 3.0388128757476807,
        "learning_rate": 0.00011206488192489146,
        "epoch": 1.6423999999999999,
        "step": 12318
    },
    {
        "loss": 1.674,
        "grad_norm": 1.9919798374176025,
        "learning_rate": 0.00011200240252900542,
        "epoch": 1.6425333333333332,
        "step": 12319
    },
    {
        "loss": 2.5654,
        "grad_norm": 4.769124507904053,
        "learning_rate": 0.0001119399183789267,
        "epoch": 1.6426666666666667,
        "step": 12320
    },
    {
        "loss": 2.5304,
        "grad_norm": 3.715611696243286,
        "learning_rate": 0.00011187742949940578,
        "epoch": 1.6428,
        "step": 12321
    },
    {
        "loss": 2.1349,
        "grad_norm": 3.383664131164551,
        "learning_rate": 0.00011181493591519456,
        "epoch": 1.6429333333333334,
        "step": 12322
    },
    {
        "loss": 1.8957,
        "grad_norm": 3.8924829959869385,
        "learning_rate": 0.00011175243765104696,
        "epoch": 1.6430666666666667,
        "step": 12323
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.4710726737976074,
        "learning_rate": 0.00011168993473171863,
        "epoch": 1.6432,
        "step": 12324
    },
    {
        "loss": 1.6367,
        "grad_norm": 4.285580635070801,
        "learning_rate": 0.00011162742718196746,
        "epoch": 1.6433333333333333,
        "step": 12325
    },
    {
        "loss": 2.0291,
        "grad_norm": 4.172823905944824,
        "learning_rate": 0.00011156491502655262,
        "epoch": 1.6434666666666666,
        "step": 12326
    },
    {
        "loss": 2.0554,
        "grad_norm": 4.362602710723877,
        "learning_rate": 0.00011150239829023568,
        "epoch": 1.6436,
        "step": 12327
    },
    {
        "loss": 2.1003,
        "grad_norm": 6.969339370727539,
        "learning_rate": 0.00011143987699777956,
        "epoch": 1.6437333333333335,
        "step": 12328
    },
    {
        "loss": 2.2057,
        "grad_norm": 3.8797476291656494,
        "learning_rate": 0.00011137735117394905,
        "epoch": 1.6438666666666668,
        "step": 12329
    },
    {
        "loss": 2.403,
        "grad_norm": 3.1375744342803955,
        "learning_rate": 0.00011131482084351112,
        "epoch": 1.6440000000000001,
        "step": 12330
    },
    {
        "loss": 2.1687,
        "grad_norm": 4.328426837921143,
        "learning_rate": 0.0001112522860312341,
        "epoch": 1.6441333333333334,
        "step": 12331
    },
    {
        "loss": 2.4152,
        "grad_norm": 4.838957786560059,
        "learning_rate": 0.00011118974676188823,
        "epoch": 1.6442666666666668,
        "step": 12332
    },
    {
        "loss": 2.3915,
        "grad_norm": 3.061793088912964,
        "learning_rate": 0.00011112720306024543,
        "epoch": 1.6444,
        "step": 12333
    },
    {
        "loss": 1.5472,
        "grad_norm": 5.890463829040527,
        "learning_rate": 0.00011106465495107969,
        "epoch": 1.6445333333333334,
        "step": 12334
    },
    {
        "loss": 1.8069,
        "grad_norm": 4.01027250289917,
        "learning_rate": 0.00011100210245916627,
        "epoch": 1.6446666666666667,
        "step": 12335
    },
    {
        "loss": 2.6938,
        "grad_norm": 3.5817885398864746,
        "learning_rate": 0.00011093954560928288,
        "epoch": 1.6448,
        "step": 12336
    },
    {
        "loss": 1.4795,
        "grad_norm": 4.032251358032227,
        "learning_rate": 0.00011087698442620794,
        "epoch": 1.6449333333333334,
        "step": 12337
    },
    {
        "loss": 2.1016,
        "grad_norm": 5.0156049728393555,
        "learning_rate": 0.00011081441893472238,
        "epoch": 1.6450666666666667,
        "step": 12338
    },
    {
        "loss": 2.0592,
        "grad_norm": 4.412632942199707,
        "learning_rate": 0.00011075184915960877,
        "epoch": 1.6452,
        "step": 12339
    },
    {
        "loss": 2.0135,
        "grad_norm": 4.599084854125977,
        "learning_rate": 0.00011068927512565109,
        "epoch": 1.6453333333333333,
        "step": 12340
    },
    {
        "loss": 1.9972,
        "grad_norm": 2.937838554382324,
        "learning_rate": 0.0001106266968576351,
        "epoch": 1.6454666666666666,
        "step": 12341
    },
    {
        "loss": 2.2977,
        "grad_norm": 2.7397749423980713,
        "learning_rate": 0.00011056411438034811,
        "epoch": 1.6456,
        "step": 12342
    },
    {
        "loss": 1.631,
        "grad_norm": 3.004410982131958,
        "learning_rate": 0.00011050152771857957,
        "epoch": 1.6457333333333333,
        "step": 12343
    },
    {
        "loss": 1.7071,
        "grad_norm": 3.3301429748535156,
        "learning_rate": 0.00011043893689712015,
        "epoch": 1.6458666666666666,
        "step": 12344
    },
    {
        "loss": 1.4757,
        "grad_norm": 3.510571002960205,
        "learning_rate": 0.00011037634194076223,
        "epoch": 1.646,
        "step": 12345
    },
    {
        "loss": 1.3647,
        "grad_norm": 6.038540840148926,
        "learning_rate": 0.00011031374287429977,
        "epoch": 1.6461333333333332,
        "step": 12346
    },
    {
        "loss": 2.3603,
        "grad_norm": 3.7772436141967773,
        "learning_rate": 0.00011025113972252863,
        "epoch": 1.6462666666666665,
        "step": 12347
    },
    {
        "loss": 2.1496,
        "grad_norm": 4.138237953186035,
        "learning_rate": 0.00011018853251024624,
        "epoch": 1.6463999999999999,
        "step": 12348
    },
    {
        "loss": 2.3554,
        "grad_norm": 2.8055806159973145,
        "learning_rate": 0.00011012592126225139,
        "epoch": 1.6465333333333332,
        "step": 12349
    },
    {
        "loss": 2.5958,
        "grad_norm": 3.2912983894348145,
        "learning_rate": 0.00011006330600334465,
        "epoch": 1.6466666666666665,
        "step": 12350
    },
    {
        "loss": 1.979,
        "grad_norm": 4.8882317543029785,
        "learning_rate": 0.00011000068675832797,
        "epoch": 1.6468,
        "step": 12351
    },
    {
        "loss": 1.0014,
        "grad_norm": 4.063173770904541,
        "learning_rate": 0.00010993806355200534,
        "epoch": 1.6469333333333334,
        "step": 12352
    },
    {
        "loss": 2.7302,
        "grad_norm": 2.3842926025390625,
        "learning_rate": 0.00010987543640918184,
        "epoch": 1.6470666666666667,
        "step": 12353
    },
    {
        "loss": 1.6679,
        "grad_norm": 4.586262226104736,
        "learning_rate": 0.00010981280535466434,
        "epoch": 1.6472,
        "step": 12354
    },
    {
        "loss": 2.1494,
        "grad_norm": 4.381242752075195,
        "learning_rate": 0.00010975017041326108,
        "epoch": 1.6473333333333333,
        "step": 12355
    },
    {
        "loss": 1.9881,
        "grad_norm": 2.9592363834381104,
        "learning_rate": 0.0001096875316097821,
        "epoch": 1.6474666666666666,
        "step": 12356
    },
    {
        "loss": 1.1441,
        "grad_norm": 5.599098205566406,
        "learning_rate": 0.00010962488896903908,
        "epoch": 1.6476,
        "step": 12357
    },
    {
        "loss": 1.6477,
        "grad_norm": 3.1577463150024414,
        "learning_rate": 0.0001095622425158445,
        "epoch": 1.6477333333333335,
        "step": 12358
    },
    {
        "loss": 1.7289,
        "grad_norm": 4.1902947425842285,
        "learning_rate": 0.0001094995922750132,
        "epoch": 1.6478666666666668,
        "step": 12359
    },
    {
        "loss": 2.6915,
        "grad_norm": 4.774152755737305,
        "learning_rate": 0.0001094369382713609,
        "epoch": 1.6480000000000001,
        "step": 12360
    },
    {
        "loss": 1.2472,
        "grad_norm": 5.301043510437012,
        "learning_rate": 0.00010937428052970533,
        "epoch": 1.6481333333333335,
        "step": 12361
    },
    {
        "loss": 1.8679,
        "grad_norm": 2.9396259784698486,
        "learning_rate": 0.00010931161907486532,
        "epoch": 1.6482666666666668,
        "step": 12362
    },
    {
        "loss": 2.9655,
        "grad_norm": 2.8765368461608887,
        "learning_rate": 0.00010924895393166125,
        "epoch": 1.6484,
        "step": 12363
    },
    {
        "loss": 1.8586,
        "grad_norm": 4.147707939147949,
        "learning_rate": 0.00010918628512491487,
        "epoch": 1.6485333333333334,
        "step": 12364
    },
    {
        "loss": 3.0795,
        "grad_norm": 2.8752336502075195,
        "learning_rate": 0.0001091236126794498,
        "epoch": 1.6486666666666667,
        "step": 12365
    },
    {
        "loss": 2.2822,
        "grad_norm": 3.348578929901123,
        "learning_rate": 0.00010906093662009065,
        "epoch": 1.6488,
        "step": 12366
    },
    {
        "loss": 1.5324,
        "grad_norm": 4.371807098388672,
        "learning_rate": 0.00010899825697166349,
        "epoch": 1.6489333333333334,
        "step": 12367
    },
    {
        "loss": 1.4257,
        "grad_norm": 4.227636814117432,
        "learning_rate": 0.0001089355737589962,
        "epoch": 1.6490666666666667,
        "step": 12368
    },
    {
        "loss": 2.2058,
        "grad_norm": 3.4073946475982666,
        "learning_rate": 0.0001088728870069176,
        "epoch": 1.6492,
        "step": 12369
    },
    {
        "loss": 0.9934,
        "grad_norm": 3.8259503841400146,
        "learning_rate": 0.00010881019674025836,
        "epoch": 1.6493333333333333,
        "step": 12370
    },
    {
        "loss": 2.1147,
        "grad_norm": 3.369523525238037,
        "learning_rate": 0.00010874750298385012,
        "epoch": 1.6494666666666666,
        "step": 12371
    },
    {
        "loss": 2.0001,
        "grad_norm": 3.5963282585144043,
        "learning_rate": 0.0001086848057625262,
        "epoch": 1.6496,
        "step": 12372
    },
    {
        "loss": 1.2693,
        "grad_norm": 4.6708502769470215,
        "learning_rate": 0.00010862210510112097,
        "epoch": 1.6497333333333333,
        "step": 12373
    },
    {
        "loss": 2.6747,
        "grad_norm": 3.7181782722473145,
        "learning_rate": 0.00010855940102447066,
        "epoch": 1.6498666666666666,
        "step": 12374
    },
    {
        "loss": 3.0937,
        "grad_norm": 3.0582921504974365,
        "learning_rate": 0.00010849669355741243,
        "epoch": 1.65,
        "step": 12375
    },
    {
        "loss": 2.2565,
        "grad_norm": 3.561863422393799,
        "learning_rate": 0.0001084339827247848,
        "epoch": 1.6501333333333332,
        "step": 12376
    },
    {
        "loss": 2.6638,
        "grad_norm": 3.6335971355438232,
        "learning_rate": 0.00010837126855142803,
        "epoch": 1.6502666666666665,
        "step": 12377
    },
    {
        "loss": 2.8501,
        "grad_norm": 4.585659980773926,
        "learning_rate": 0.00010830855106218325,
        "epoch": 1.6503999999999999,
        "step": 12378
    },
    {
        "loss": 2.5524,
        "grad_norm": 3.8462085723876953,
        "learning_rate": 0.00010824583028189293,
        "epoch": 1.6505333333333332,
        "step": 12379
    },
    {
        "loss": 2.257,
        "grad_norm": 3.410287380218506,
        "learning_rate": 0.00010818310623540129,
        "epoch": 1.6506666666666665,
        "step": 12380
    },
    {
        "loss": 2.3441,
        "grad_norm": 3.3723230361938477,
        "learning_rate": 0.0001081203789475534,
        "epoch": 1.6508,
        "step": 12381
    },
    {
        "loss": 2.501,
        "grad_norm": 3.4250893592834473,
        "learning_rate": 0.00010805764844319573,
        "epoch": 1.6509333333333334,
        "step": 12382
    },
    {
        "loss": 2.1722,
        "grad_norm": 4.581724166870117,
        "learning_rate": 0.00010799491474717588,
        "epoch": 1.6510666666666667,
        "step": 12383
    },
    {
        "loss": 1.3898,
        "grad_norm": 2.3881046772003174,
        "learning_rate": 0.00010793217788434324,
        "epoch": 1.6512,
        "step": 12384
    },
    {
        "loss": 2.7685,
        "grad_norm": 3.0149807929992676,
        "learning_rate": 0.00010786943787954775,
        "epoch": 1.6513333333333333,
        "step": 12385
    },
    {
        "loss": 2.4223,
        "grad_norm": 3.4784023761749268,
        "learning_rate": 0.00010780669475764127,
        "epoch": 1.6514666666666666,
        "step": 12386
    },
    {
        "loss": 2.1332,
        "grad_norm": 3.492668628692627,
        "learning_rate": 0.00010774394854347638,
        "epoch": 1.6516,
        "step": 12387
    },
    {
        "loss": 1.1318,
        "grad_norm": 3.5164406299591064,
        "learning_rate": 0.00010768119926190696,
        "epoch": 1.6517333333333335,
        "step": 12388
    },
    {
        "loss": 1.6058,
        "grad_norm": 4.652278900146484,
        "learning_rate": 0.00010761844693778844,
        "epoch": 1.6518666666666668,
        "step": 12389
    },
    {
        "loss": 1.5923,
        "grad_norm": 3.144943952560425,
        "learning_rate": 0.00010755569159597717,
        "epoch": 1.6520000000000001,
        "step": 12390
    },
    {
        "loss": 2.3145,
        "grad_norm": 3.9761767387390137,
        "learning_rate": 0.00010749293326133068,
        "epoch": 1.6521333333333335,
        "step": 12391
    },
    {
        "loss": 2.1118,
        "grad_norm": 3.2270984649658203,
        "learning_rate": 0.00010743017195870768,
        "epoch": 1.6522666666666668,
        "step": 12392
    },
    {
        "loss": 2.4736,
        "grad_norm": 3.0959200859069824,
        "learning_rate": 0.00010736740771296836,
        "epoch": 1.6524,
        "step": 12393
    },
    {
        "loss": 2.326,
        "grad_norm": 4.150707721710205,
        "learning_rate": 0.00010730464054897362,
        "epoch": 1.6525333333333334,
        "step": 12394
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.1993744373321533,
        "learning_rate": 0.00010724187049158616,
        "epoch": 1.6526666666666667,
        "step": 12395
    },
    {
        "loss": 2.3248,
        "grad_norm": 3.6600730419158936,
        "learning_rate": 0.00010717909756566884,
        "epoch": 1.6528,
        "step": 12396
    },
    {
        "loss": 3.0974,
        "grad_norm": 3.257664918899536,
        "learning_rate": 0.00010711632179608648,
        "epoch": 1.6529333333333334,
        "step": 12397
    },
    {
        "loss": 2.4769,
        "grad_norm": 4.042283058166504,
        "learning_rate": 0.00010705354320770498,
        "epoch": 1.6530666666666667,
        "step": 12398
    },
    {
        "loss": 2.1351,
        "grad_norm": 4.112521648406982,
        "learning_rate": 0.00010699076182539098,
        "epoch": 1.6532,
        "step": 12399
    },
    {
        "loss": 2.026,
        "grad_norm": 3.5440943241119385,
        "learning_rate": 0.00010692797767401237,
        "epoch": 1.6533333333333333,
        "step": 12400
    },
    {
        "loss": 2.0766,
        "grad_norm": 3.1116139888763428,
        "learning_rate": 0.00010686519077843808,
        "epoch": 1.6534666666666666,
        "step": 12401
    },
    {
        "loss": 1.7007,
        "grad_norm": 3.430988073348999,
        "learning_rate": 0.00010680240116353848,
        "epoch": 1.6536,
        "step": 12402
    },
    {
        "loss": 2.5058,
        "grad_norm": 2.276305675506592,
        "learning_rate": 0.0001067396088541846,
        "epoch": 1.6537333333333333,
        "step": 12403
    },
    {
        "loss": 2.6844,
        "grad_norm": 3.137258768081665,
        "learning_rate": 0.00010667681387524866,
        "epoch": 1.6538666666666666,
        "step": 12404
    },
    {
        "loss": 2.6651,
        "grad_norm": 4.517763614654541,
        "learning_rate": 0.00010661401625160388,
        "epoch": 1.654,
        "step": 12405
    },
    {
        "loss": 0.9803,
        "grad_norm": 3.8989477157592773,
        "learning_rate": 0.00010655121600812472,
        "epoch": 1.6541333333333332,
        "step": 12406
    },
    {
        "loss": 2.5011,
        "grad_norm": 3.337662935256958,
        "learning_rate": 0.0001064884131696867,
        "epoch": 1.6542666666666666,
        "step": 12407
    },
    {
        "loss": 1.5768,
        "grad_norm": 4.918634414672852,
        "learning_rate": 0.00010642560776116613,
        "epoch": 1.6543999999999999,
        "step": 12408
    },
    {
        "loss": 2.4064,
        "grad_norm": 4.0146870613098145,
        "learning_rate": 0.00010636279980744043,
        "epoch": 1.6545333333333332,
        "step": 12409
    },
    {
        "loss": 1.6664,
        "grad_norm": 5.690120220184326,
        "learning_rate": 0.0001062999893333879,
        "epoch": 1.6546666666666665,
        "step": 12410
    },
    {
        "loss": 1.6793,
        "grad_norm": 3.2315785884857178,
        "learning_rate": 0.00010623717636388828,
        "epoch": 1.6548,
        "step": 12411
    },
    {
        "loss": 2.2754,
        "grad_norm": 3.8182480335235596,
        "learning_rate": 0.00010617436092382185,
        "epoch": 1.6549333333333334,
        "step": 12412
    },
    {
        "loss": 2.4524,
        "grad_norm": 2.95990252494812,
        "learning_rate": 0.00010611154303807001,
        "epoch": 1.6550666666666667,
        "step": 12413
    },
    {
        "loss": 2.6958,
        "grad_norm": 3.7888011932373047,
        "learning_rate": 0.000106048722731515,
        "epoch": 1.6552,
        "step": 12414
    },
    {
        "loss": 2.5699,
        "grad_norm": 3.132974147796631,
        "learning_rate": 0.00010598590002904032,
        "epoch": 1.6553333333333333,
        "step": 12415
    },
    {
        "loss": 1.9664,
        "grad_norm": 4.4903130531311035,
        "learning_rate": 0.0001059230749555305,
        "epoch": 1.6554666666666666,
        "step": 12416
    },
    {
        "loss": 2.3548,
        "grad_norm": 4.429891586303711,
        "learning_rate": 0.00010586024753587027,
        "epoch": 1.6556,
        "step": 12417
    },
    {
        "loss": 1.6667,
        "grad_norm": 3.191004991531372,
        "learning_rate": 0.00010579741779494612,
        "epoch": 1.6557333333333333,
        "step": 12418
    },
    {
        "loss": 1.9984,
        "grad_norm": 4.600814342498779,
        "learning_rate": 0.00010573458575764492,
        "epoch": 1.6558666666666668,
        "step": 12419
    },
    {
        "loss": 1.9648,
        "grad_norm": 6.412405967712402,
        "learning_rate": 0.00010567175144885488,
        "epoch": 1.6560000000000001,
        "step": 12420
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.1950864791870117,
        "learning_rate": 0.00010560891489346479,
        "epoch": 1.6561333333333335,
        "step": 12421
    },
    {
        "loss": 2.4175,
        "grad_norm": 4.7304840087890625,
        "learning_rate": 0.00010554607611636436,
        "epoch": 1.6562666666666668,
        "step": 12422
    },
    {
        "loss": 1.1111,
        "grad_norm": 3.695797920227051,
        "learning_rate": 0.00010548323514244418,
        "epoch": 1.6564,
        "step": 12423
    },
    {
        "loss": 1.3034,
        "grad_norm": 4.841508865356445,
        "learning_rate": 0.000105420391996596,
        "epoch": 1.6565333333333334,
        "step": 12424
    },
    {
        "loss": 1.2628,
        "grad_norm": 4.251420974731445,
        "learning_rate": 0.00010535754670371207,
        "epoch": 1.6566666666666667,
        "step": 12425
    },
    {
        "loss": 2.7661,
        "grad_norm": 4.925978183746338,
        "learning_rate": 0.00010529469928868552,
        "epoch": 1.6568,
        "step": 12426
    },
    {
        "loss": 2.1477,
        "grad_norm": 3.4435927867889404,
        "learning_rate": 0.00010523184977641065,
        "epoch": 1.6569333333333334,
        "step": 12427
    },
    {
        "loss": 2.4877,
        "grad_norm": 3.657691717147827,
        "learning_rate": 0.00010516899819178211,
        "epoch": 1.6570666666666667,
        "step": 12428
    },
    {
        "loss": 2.1942,
        "grad_norm": 2.8709144592285156,
        "learning_rate": 0.00010510614455969597,
        "epoch": 1.6572,
        "step": 12429
    },
    {
        "loss": 1.8919,
        "grad_norm": 4.338871479034424,
        "learning_rate": 0.00010504328890504853,
        "epoch": 1.6573333333333333,
        "step": 12430
    },
    {
        "loss": 2.7679,
        "grad_norm": 5.042625904083252,
        "learning_rate": 0.00010498043125273718,
        "epoch": 1.6574666666666666,
        "step": 12431
    },
    {
        "loss": 3.2269,
        "grad_norm": 5.05903959274292,
        "learning_rate": 0.00010491757162765989,
        "epoch": 1.6576,
        "step": 12432
    },
    {
        "loss": 2.6026,
        "grad_norm": 2.0868659019470215,
        "learning_rate": 0.00010485471005471588,
        "epoch": 1.6577333333333333,
        "step": 12433
    },
    {
        "loss": 3.0511,
        "grad_norm": 2.167793035507202,
        "learning_rate": 0.00010479184655880466,
        "epoch": 1.6578666666666666,
        "step": 12434
    },
    {
        "loss": 1.6669,
        "grad_norm": 2.560417890548706,
        "learning_rate": 0.00010472898116482657,
        "epoch": 1.658,
        "step": 12435
    },
    {
        "loss": 3.3276,
        "grad_norm": 4.386620044708252,
        "learning_rate": 0.00010466611389768301,
        "epoch": 1.6581333333333332,
        "step": 12436
    },
    {
        "loss": 1.91,
        "grad_norm": 3.8721354007720947,
        "learning_rate": 0.0001046032447822759,
        "epoch": 1.6582666666666666,
        "step": 12437
    },
    {
        "loss": 2.1428,
        "grad_norm": 2.3753974437713623,
        "learning_rate": 0.0001045403738435077,
        "epoch": 1.6583999999999999,
        "step": 12438
    },
    {
        "loss": 2.5514,
        "grad_norm": 3.008301258087158,
        "learning_rate": 0.00010447750110628208,
        "epoch": 1.6585333333333332,
        "step": 12439
    },
    {
        "loss": 2.3986,
        "grad_norm": 4.066123962402344,
        "learning_rate": 0.00010441462659550299,
        "epoch": 1.6586666666666665,
        "step": 12440
    },
    {
        "loss": 2.2275,
        "grad_norm": 4.826091766357422,
        "learning_rate": 0.00010435175033607527,
        "epoch": 1.6588,
        "step": 12441
    },
    {
        "loss": 1.3532,
        "grad_norm": 3.972914218902588,
        "learning_rate": 0.0001042888723529043,
        "epoch": 1.6589333333333334,
        "step": 12442
    },
    {
        "loss": 2.6904,
        "grad_norm": 3.2033534049987793,
        "learning_rate": 0.00010422599267089652,
        "epoch": 1.6590666666666667,
        "step": 12443
    },
    {
        "loss": 2.0972,
        "grad_norm": 4.20939826965332,
        "learning_rate": 0.00010416311131495846,
        "epoch": 1.6592,
        "step": 12444
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.8946781158447266,
        "learning_rate": 0.00010410022830999813,
        "epoch": 1.6593333333333333,
        "step": 12445
    },
    {
        "loss": 3.1238,
        "grad_norm": 2.0790867805480957,
        "learning_rate": 0.00010403734368092312,
        "epoch": 1.6594666666666666,
        "step": 12446
    },
    {
        "loss": 2.4629,
        "grad_norm": 3.6259689331054688,
        "learning_rate": 0.00010397445745264252,
        "epoch": 1.6596,
        "step": 12447
    },
    {
        "loss": 2.4715,
        "grad_norm": 2.5338802337646484,
        "learning_rate": 0.0001039115696500659,
        "epoch": 1.6597333333333333,
        "step": 12448
    },
    {
        "loss": 2.187,
        "grad_norm": 3.4065349102020264,
        "learning_rate": 0.00010384868029810324,
        "epoch": 1.6598666666666668,
        "step": 12449
    },
    {
        "loss": 2.6521,
        "grad_norm": 3.9133245944976807,
        "learning_rate": 0.00010378578942166519,
        "epoch": 1.6600000000000001,
        "step": 12450
    },
    {
        "loss": 2.9179,
        "grad_norm": 2.8380203247070312,
        "learning_rate": 0.0001037228970456629,
        "epoch": 1.6601333333333335,
        "step": 12451
    },
    {
        "loss": 1.9389,
        "grad_norm": 2.1440699100494385,
        "learning_rate": 0.00010366000319500857,
        "epoch": 1.6602666666666668,
        "step": 12452
    },
    {
        "loss": 2.5635,
        "grad_norm": 3.0467162132263184,
        "learning_rate": 0.00010359710789461435,
        "epoch": 1.6604,
        "step": 12453
    },
    {
        "loss": 2.1646,
        "grad_norm": 4.519589424133301,
        "learning_rate": 0.0001035342111693937,
        "epoch": 1.6605333333333334,
        "step": 12454
    },
    {
        "loss": 1.6101,
        "grad_norm": 4.306407928466797,
        "learning_rate": 0.0001034713130442597,
        "epoch": 1.6606666666666667,
        "step": 12455
    },
    {
        "loss": 1.1986,
        "grad_norm": 5.982223033905029,
        "learning_rate": 0.00010340841354412678,
        "epoch": 1.6608,
        "step": 12456
    },
    {
        "loss": 3.2009,
        "grad_norm": 2.552459478378296,
        "learning_rate": 0.00010334551269390975,
        "epoch": 1.6609333333333334,
        "step": 12457
    },
    {
        "loss": 1.302,
        "grad_norm": 5.876638412475586,
        "learning_rate": 0.00010328261051852378,
        "epoch": 1.6610666666666667,
        "step": 12458
    },
    {
        "loss": 1.7775,
        "grad_norm": 6.892972469329834,
        "learning_rate": 0.00010321970704288457,
        "epoch": 1.6612,
        "step": 12459
    },
    {
        "loss": 2.3729,
        "grad_norm": 2.7815816402435303,
        "learning_rate": 0.00010315680229190832,
        "epoch": 1.6613333333333333,
        "step": 12460
    },
    {
        "loss": 2.3043,
        "grad_norm": 3.6707375049591064,
        "learning_rate": 0.00010309389629051202,
        "epoch": 1.6614666666666666,
        "step": 12461
    },
    {
        "loss": 2.1339,
        "grad_norm": 5.926987648010254,
        "learning_rate": 0.0001030309890636129,
        "epoch": 1.6616,
        "step": 12462
    },
    {
        "loss": 2.5068,
        "grad_norm": 3.338884115219116,
        "learning_rate": 0.00010296808063612865,
        "epoch": 1.6617333333333333,
        "step": 12463
    },
    {
        "loss": 2.5008,
        "grad_norm": 4.645654678344727,
        "learning_rate": 0.00010290517103297746,
        "epoch": 1.6618666666666666,
        "step": 12464
    },
    {
        "loss": 1.3694,
        "grad_norm": 4.699068546295166,
        "learning_rate": 0.00010284226027907808,
        "epoch": 1.662,
        "step": 12465
    },
    {
        "loss": 2.0005,
        "grad_norm": 3.119298219680786,
        "learning_rate": 0.00010277934839934999,
        "epoch": 1.6621333333333332,
        "step": 12466
    },
    {
        "loss": 1.5824,
        "grad_norm": 4.406652450561523,
        "learning_rate": 0.0001027164354187123,
        "epoch": 1.6622666666666666,
        "step": 12467
    },
    {
        "loss": 2.6684,
        "grad_norm": 3.0237698554992676,
        "learning_rate": 0.00010265352136208539,
        "epoch": 1.6623999999999999,
        "step": 12468
    },
    {
        "loss": 2.327,
        "grad_norm": 2.2869439125061035,
        "learning_rate": 0.00010259060625438948,
        "epoch": 1.6625333333333332,
        "step": 12469
    },
    {
        "loss": 1.4573,
        "grad_norm": 4.268381595611572,
        "learning_rate": 0.00010252769012054576,
        "epoch": 1.6626666666666665,
        "step": 12470
    },
    {
        "loss": 2.259,
        "grad_norm": 3.3815114498138428,
        "learning_rate": 0.00010246477298547537,
        "epoch": 1.6627999999999998,
        "step": 12471
    },
    {
        "loss": 1.9573,
        "grad_norm": 4.590785980224609,
        "learning_rate": 0.00010240185487409999,
        "epoch": 1.6629333333333334,
        "step": 12472
    },
    {
        "loss": 1.7714,
        "grad_norm": 4.708763122558594,
        "learning_rate": 0.00010233893581134158,
        "epoch": 1.6630666666666667,
        "step": 12473
    },
    {
        "loss": 2.299,
        "grad_norm": 4.247941493988037,
        "learning_rate": 0.00010227601582212271,
        "epoch": 1.6632,
        "step": 12474
    },
    {
        "loss": 1.9861,
        "grad_norm": 3.878481388092041,
        "learning_rate": 0.00010221309493136652,
        "epoch": 1.6633333333333333,
        "step": 12475
    },
    {
        "loss": 2.5176,
        "grad_norm": 4.232171058654785,
        "learning_rate": 0.0001021501731639956,
        "epoch": 1.6634666666666666,
        "step": 12476
    },
    {
        "loss": 1.8105,
        "grad_norm": 3.5574090480804443,
        "learning_rate": 0.0001020872505449339,
        "epoch": 1.6636,
        "step": 12477
    },
    {
        "loss": 1.6981,
        "grad_norm": 3.1822028160095215,
        "learning_rate": 0.00010202432709910502,
        "epoch": 1.6637333333333333,
        "step": 12478
    },
    {
        "loss": 2.4043,
        "grad_norm": 4.403955459594727,
        "learning_rate": 0.00010196140285143339,
        "epoch": 1.6638666666666668,
        "step": 12479
    },
    {
        "loss": 2.4911,
        "grad_norm": 3.582253932952881,
        "learning_rate": 0.0001018984778268434,
        "epoch": 1.6640000000000001,
        "step": 12480
    },
    {
        "loss": 2.378,
        "grad_norm": 2.235572099685669,
        "learning_rate": 0.00010183555205025992,
        "epoch": 1.6641333333333335,
        "step": 12481
    },
    {
        "loss": 2.4227,
        "grad_norm": 2.628476858139038,
        "learning_rate": 0.00010177262554660787,
        "epoch": 1.6642666666666668,
        "step": 12482
    },
    {
        "loss": 2.5625,
        "grad_norm": 3.6240808963775635,
        "learning_rate": 0.0001017096983408129,
        "epoch": 1.6644,
        "step": 12483
    },
    {
        "loss": 1.9906,
        "grad_norm": 2.97843337059021,
        "learning_rate": 0.00010164677045780062,
        "epoch": 1.6645333333333334,
        "step": 12484
    },
    {
        "loss": 0.9693,
        "grad_norm": 4.037018775939941,
        "learning_rate": 0.00010158384192249681,
        "epoch": 1.6646666666666667,
        "step": 12485
    },
    {
        "loss": 2.4579,
        "grad_norm": 4.085378170013428,
        "learning_rate": 0.00010152091275982798,
        "epoch": 1.6648,
        "step": 12486
    },
    {
        "loss": 1.7581,
        "grad_norm": 4.722583293914795,
        "learning_rate": 0.00010145798299472025,
        "epoch": 1.6649333333333334,
        "step": 12487
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.2507190704345703,
        "learning_rate": 0.00010139505265210063,
        "epoch": 1.6650666666666667,
        "step": 12488
    },
    {
        "loss": 2.5735,
        "grad_norm": 3.4950830936431885,
        "learning_rate": 0.00010133212175689593,
        "epoch": 1.6652,
        "step": 12489
    },
    {
        "loss": 2.2055,
        "grad_norm": 3.404850959777832,
        "learning_rate": 0.00010126919033403324,
        "epoch": 1.6653333333333333,
        "step": 12490
    },
    {
        "loss": 1.9839,
        "grad_norm": 4.122715473175049,
        "learning_rate": 0.00010120625840843978,
        "epoch": 1.6654666666666667,
        "step": 12491
    },
    {
        "loss": 3.0344,
        "grad_norm": 5.508575916290283,
        "learning_rate": 0.00010114332600504341,
        "epoch": 1.6656,
        "step": 12492
    },
    {
        "loss": 2.5882,
        "grad_norm": 6.588871955871582,
        "learning_rate": 0.00010108039314877172,
        "epoch": 1.6657333333333333,
        "step": 12493
    },
    {
        "loss": 2.1929,
        "grad_norm": 4.175707817077637,
        "learning_rate": 0.00010101745986455247,
        "epoch": 1.6658666666666666,
        "step": 12494
    },
    {
        "loss": 1.7637,
        "grad_norm": 3.4170308113098145,
        "learning_rate": 0.00010095452617731405,
        "epoch": 1.666,
        "step": 12495
    },
    {
        "loss": 1.2122,
        "grad_norm": 4.790088653564453,
        "learning_rate": 0.00010089159211198456,
        "epoch": 1.6661333333333332,
        "step": 12496
    },
    {
        "loss": 1.8686,
        "grad_norm": 2.632283926010132,
        "learning_rate": 0.00010082865769349223,
        "epoch": 1.6662666666666666,
        "step": 12497
    },
    {
        "loss": 3.1228,
        "grad_norm": 5.880157470703125,
        "learning_rate": 0.00010076572294676592,
        "epoch": 1.6663999999999999,
        "step": 12498
    },
    {
        "loss": 1.7485,
        "grad_norm": 3.663761854171753,
        "learning_rate": 0.00010070278789673416,
        "epoch": 1.6665333333333332,
        "step": 12499
    },
    {
        "loss": 2.718,
        "grad_norm": 2.4353885650634766,
        "learning_rate": 0.00010063985256832571,
        "epoch": 1.6666666666666665,
        "step": 12500
    },
    {
        "loss": 2.8813,
        "grad_norm": 2.526209592819214,
        "learning_rate": 0.00010057691698646932,
        "epoch": 1.6667999999999998,
        "step": 12501
    },
    {
        "loss": 1.6574,
        "grad_norm": 3.5661072731018066,
        "learning_rate": 0.0001005139811760943,
        "epoch": 1.6669333333333334,
        "step": 12502
    },
    {
        "loss": 2.8167,
        "grad_norm": 3.1546623706817627,
        "learning_rate": 0.00010045104516212947,
        "epoch": 1.6670666666666667,
        "step": 12503
    },
    {
        "loss": 1.9652,
        "grad_norm": 4.376914024353027,
        "learning_rate": 0.00010038810896950435,
        "epoch": 1.6672,
        "step": 12504
    },
    {
        "loss": 2.0858,
        "grad_norm": 2.841139078140259,
        "learning_rate": 0.00010032517262314767,
        "epoch": 1.6673333333333333,
        "step": 12505
    },
    {
        "loss": 2.7815,
        "grad_norm": 3.250447988510132,
        "learning_rate": 0.00010026223614798905,
        "epoch": 1.6674666666666667,
        "step": 12506
    },
    {
        "loss": 2.4911,
        "grad_norm": 2.4610159397125244,
        "learning_rate": 0.0001001992995689579,
        "epoch": 1.6676,
        "step": 12507
    },
    {
        "loss": 1.6963,
        "grad_norm": 4.452084064483643,
        "learning_rate": 0.0001001363629109835,
        "epoch": 1.6677333333333333,
        "step": 12508
    },
    {
        "loss": 2.6416,
        "grad_norm": 3.758716106414795,
        "learning_rate": 0.00010007342619899531,
        "epoch": 1.6678666666666668,
        "step": 12509
    },
    {
        "loss": 1.7009,
        "grad_norm": 4.055510997772217,
        "learning_rate": 0.00010001048945792262,
        "epoch": 1.6680000000000001,
        "step": 12510
    },
    {
        "loss": 2.0257,
        "grad_norm": 4.356405258178711,
        "learning_rate": 9.994755271269515e-05,
        "epoch": 1.6681333333333335,
        "step": 12511
    },
    {
        "loss": 2.4839,
        "grad_norm": 3.266371011734009,
        "learning_rate": 9.988461598824208e-05,
        "epoch": 1.6682666666666668,
        "step": 12512
    },
    {
        "loss": 2.202,
        "grad_norm": 3.5318565368652344,
        "learning_rate": 9.982167930949329e-05,
        "epoch": 1.6684,
        "step": 12513
    },
    {
        "loss": 1.5443,
        "grad_norm": 4.535223960876465,
        "learning_rate": 9.975874270137767e-05,
        "epoch": 1.6685333333333334,
        "step": 12514
    },
    {
        "loss": 2.6512,
        "grad_norm": 4.647054672241211,
        "learning_rate": 9.969580618882487e-05,
        "epoch": 1.6686666666666667,
        "step": 12515
    },
    {
        "loss": 2.3583,
        "grad_norm": 4.452592849731445,
        "learning_rate": 9.963286979676442e-05,
        "epoch": 1.6688,
        "step": 12516
    },
    {
        "loss": 1.8147,
        "grad_norm": 5.913488864898682,
        "learning_rate": 9.95699335501255e-05,
        "epoch": 1.6689333333333334,
        "step": 12517
    },
    {
        "loss": 1.9734,
        "grad_norm": 4.112459659576416,
        "learning_rate": 9.950699747383738e-05,
        "epoch": 1.6690666666666667,
        "step": 12518
    },
    {
        "loss": 2.3135,
        "grad_norm": 2.7564070224761963,
        "learning_rate": 9.944406159282907e-05,
        "epoch": 1.6692,
        "step": 12519
    },
    {
        "loss": 2.2818,
        "grad_norm": 3.7799315452575684,
        "learning_rate": 9.938112593203001e-05,
        "epoch": 1.6693333333333333,
        "step": 12520
    },
    {
        "loss": 2.3588,
        "grad_norm": 3.5652506351470947,
        "learning_rate": 9.931819051636909e-05,
        "epoch": 1.6694666666666667,
        "step": 12521
    },
    {
        "loss": 0.7968,
        "grad_norm": 5.539056301116943,
        "learning_rate": 9.925525537077522e-05,
        "epoch": 1.6696,
        "step": 12522
    },
    {
        "loss": 1.2534,
        "grad_norm": 4.495660305023193,
        "learning_rate": 9.919232052017709e-05,
        "epoch": 1.6697333333333333,
        "step": 12523
    },
    {
        "loss": 2.6293,
        "grad_norm": 3.339162826538086,
        "learning_rate": 9.912938598950356e-05,
        "epoch": 1.6698666666666666,
        "step": 12524
    },
    {
        "loss": 1.6633,
        "grad_norm": 3.7368128299713135,
        "learning_rate": 9.906645180368349e-05,
        "epoch": 1.67,
        "step": 12525
    },
    {
        "loss": 2.8978,
        "grad_norm": 3.562307834625244,
        "learning_rate": 9.900351798764475e-05,
        "epoch": 1.6701333333333332,
        "step": 12526
    },
    {
        "loss": 2.7583,
        "grad_norm": 2.701768159866333,
        "learning_rate": 9.894058456631607e-05,
        "epoch": 1.6702666666666666,
        "step": 12527
    },
    {
        "loss": 1.5019,
        "grad_norm": 4.611523628234863,
        "learning_rate": 9.887765156462533e-05,
        "epoch": 1.6703999999999999,
        "step": 12528
    },
    {
        "loss": 1.2872,
        "grad_norm": 3.510533332824707,
        "learning_rate": 9.881471900750076e-05,
        "epoch": 1.6705333333333332,
        "step": 12529
    },
    {
        "loss": 1.811,
        "grad_norm": 4.492881774902344,
        "learning_rate": 9.875178691987001e-05,
        "epoch": 1.6706666666666665,
        "step": 12530
    },
    {
        "loss": 2.4694,
        "grad_norm": 3.443779706954956,
        "learning_rate": 9.868885532666075e-05,
        "epoch": 1.6707999999999998,
        "step": 12531
    },
    {
        "loss": 1.6891,
        "grad_norm": 4.3114495277404785,
        "learning_rate": 9.862592425280021e-05,
        "epoch": 1.6709333333333334,
        "step": 12532
    },
    {
        "loss": 2.9123,
        "grad_norm": 3.997875928878784,
        "learning_rate": 9.856299372321574e-05,
        "epoch": 1.6710666666666667,
        "step": 12533
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.8290064334869385,
        "learning_rate": 9.850006376283465e-05,
        "epoch": 1.6712,
        "step": 12534
    },
    {
        "loss": 2.0244,
        "grad_norm": 3.7426564693450928,
        "learning_rate": 9.843713439658313e-05,
        "epoch": 1.6713333333333333,
        "step": 12535
    },
    {
        "loss": 0.9027,
        "grad_norm": 5.348476409912109,
        "learning_rate": 9.837420564938815e-05,
        "epoch": 1.6714666666666667,
        "step": 12536
    },
    {
        "loss": 2.7091,
        "grad_norm": 3.4208590984344482,
        "learning_rate": 9.831127754617569e-05,
        "epoch": 1.6716,
        "step": 12537
    },
    {
        "loss": 2.0058,
        "grad_norm": 2.7552568912506104,
        "learning_rate": 9.82483501118721e-05,
        "epoch": 1.6717333333333333,
        "step": 12538
    },
    {
        "loss": 1.6034,
        "grad_norm": 4.3725104331970215,
        "learning_rate": 9.818542337140301e-05,
        "epoch": 1.6718666666666666,
        "step": 12539
    },
    {
        "loss": 2.39,
        "grad_norm": 4.385911464691162,
        "learning_rate": 9.812249734969389e-05,
        "epoch": 1.6720000000000002,
        "step": 12540
    },
    {
        "loss": 2.5447,
        "grad_norm": 3.9956700801849365,
        "learning_rate": 9.805957207166982e-05,
        "epoch": 1.6721333333333335,
        "step": 12541
    },
    {
        "loss": 2.8149,
        "grad_norm": 3.1245787143707275,
        "learning_rate": 9.799664756225603e-05,
        "epoch": 1.6722666666666668,
        "step": 12542
    },
    {
        "loss": 1.4475,
        "grad_norm": 4.003378391265869,
        "learning_rate": 9.793372384637695e-05,
        "epoch": 1.6724,
        "step": 12543
    },
    {
        "loss": 2.5169,
        "grad_norm": 3.1972343921661377,
        "learning_rate": 9.787080094895675e-05,
        "epoch": 1.6725333333333334,
        "step": 12544
    },
    {
        "loss": 2.581,
        "grad_norm": 3.698223352432251,
        "learning_rate": 9.78078788949197e-05,
        "epoch": 1.6726666666666667,
        "step": 12545
    },
    {
        "loss": 1.8801,
        "grad_norm": 4.780205249786377,
        "learning_rate": 9.774495770918929e-05,
        "epoch": 1.6728,
        "step": 12546
    },
    {
        "loss": 1.806,
        "grad_norm": 2.888284683227539,
        "learning_rate": 9.768203741668868e-05,
        "epoch": 1.6729333333333334,
        "step": 12547
    },
    {
        "loss": 1.9716,
        "grad_norm": 4.993988513946533,
        "learning_rate": 9.761911804234107e-05,
        "epoch": 1.6730666666666667,
        "step": 12548
    },
    {
        "loss": 1.8696,
        "grad_norm": 3.7564523220062256,
        "learning_rate": 9.755619961106892e-05,
        "epoch": 1.6732,
        "step": 12549
    },
    {
        "loss": 2.3646,
        "grad_norm": 3.794736862182617,
        "learning_rate": 9.74932821477943e-05,
        "epoch": 1.6733333333333333,
        "step": 12550
    },
    {
        "loss": 2.5533,
        "grad_norm": 3.133171558380127,
        "learning_rate": 9.743036567743932e-05,
        "epoch": 1.6734666666666667,
        "step": 12551
    },
    {
        "loss": 2.9715,
        "grad_norm": 3.4712417125701904,
        "learning_rate": 9.73674502249252e-05,
        "epoch": 1.6736,
        "step": 12552
    },
    {
        "loss": 2.8732,
        "grad_norm": 2.4330315589904785,
        "learning_rate": 9.730453581517293e-05,
        "epoch": 1.6737333333333333,
        "step": 12553
    },
    {
        "loss": 2.6339,
        "grad_norm": 3.128281354904175,
        "learning_rate": 9.724162247310332e-05,
        "epoch": 1.6738666666666666,
        "step": 12554
    },
    {
        "loss": 2.7205,
        "grad_norm": 5.382419586181641,
        "learning_rate": 9.717871022363644e-05,
        "epoch": 1.674,
        "step": 12555
    },
    {
        "loss": 1.4475,
        "grad_norm": 2.4816441535949707,
        "learning_rate": 9.711579909169193e-05,
        "epoch": 1.6741333333333333,
        "step": 12556
    },
    {
        "loss": 2.0072,
        "grad_norm": 3.1946771144866943,
        "learning_rate": 9.705288910218937e-05,
        "epoch": 1.6742666666666666,
        "step": 12557
    },
    {
        "loss": 0.9072,
        "grad_norm": 3.574509382247925,
        "learning_rate": 9.698998028004747e-05,
        "epoch": 1.6743999999999999,
        "step": 12558
    },
    {
        "loss": 2.0189,
        "grad_norm": 5.134246826171875,
        "learning_rate": 9.692707265018463e-05,
        "epoch": 1.6745333333333332,
        "step": 12559
    },
    {
        "loss": 2.9624,
        "grad_norm": 4.147365093231201,
        "learning_rate": 9.686416623751866e-05,
        "epoch": 1.6746666666666665,
        "step": 12560
    },
    {
        "loss": 2.972,
        "grad_norm": 3.3064794540405273,
        "learning_rate": 9.680126106696726e-05,
        "epoch": 1.6747999999999998,
        "step": 12561
    },
    {
        "loss": 2.7676,
        "grad_norm": 4.397752285003662,
        "learning_rate": 9.67383571634471e-05,
        "epoch": 1.6749333333333334,
        "step": 12562
    },
    {
        "loss": 0.9693,
        "grad_norm": 4.827741622924805,
        "learning_rate": 9.667545455187508e-05,
        "epoch": 1.6750666666666667,
        "step": 12563
    },
    {
        "loss": 2.5363,
        "grad_norm": 3.439868688583374,
        "learning_rate": 9.661255325716656e-05,
        "epoch": 1.6752,
        "step": 12564
    },
    {
        "loss": 2.5848,
        "grad_norm": 3.401897430419922,
        "learning_rate": 9.654965330423722e-05,
        "epoch": 1.6753333333333333,
        "step": 12565
    },
    {
        "loss": 1.4052,
        "grad_norm": 4.617359161376953,
        "learning_rate": 9.648675471800212e-05,
        "epoch": 1.6754666666666667,
        "step": 12566
    },
    {
        "loss": 1.6982,
        "grad_norm": 4.854872226715088,
        "learning_rate": 9.642385752337544e-05,
        "epoch": 1.6756,
        "step": 12567
    },
    {
        "loss": 2.5478,
        "grad_norm": 2.8233132362365723,
        "learning_rate": 9.636096174527098e-05,
        "epoch": 1.6757333333333333,
        "step": 12568
    },
    {
        "loss": 2.6546,
        "grad_norm": 3.8686470985412598,
        "learning_rate": 9.629806740860183e-05,
        "epoch": 1.6758666666666666,
        "step": 12569
    },
    {
        "loss": 2.1728,
        "grad_norm": 4.062889099121094,
        "learning_rate": 9.623517453828094e-05,
        "epoch": 1.6760000000000002,
        "step": 12570
    },
    {
        "loss": 1.9812,
        "grad_norm": 4.236096382141113,
        "learning_rate": 9.617228315922011e-05,
        "epoch": 1.6761333333333335,
        "step": 12571
    },
    {
        "loss": 2.4982,
        "grad_norm": 2.7005791664123535,
        "learning_rate": 9.610939329633124e-05,
        "epoch": 1.6762666666666668,
        "step": 12572
    },
    {
        "loss": 2.5205,
        "grad_norm": 3.533409595489502,
        "learning_rate": 9.604650497452465e-05,
        "epoch": 1.6764000000000001,
        "step": 12573
    },
    {
        "loss": 2.1207,
        "grad_norm": 4.499603271484375,
        "learning_rate": 9.598361821871093e-05,
        "epoch": 1.6765333333333334,
        "step": 12574
    },
    {
        "loss": 1.1453,
        "grad_norm": 6.164090156555176,
        "learning_rate": 9.592073305379983e-05,
        "epoch": 1.6766666666666667,
        "step": 12575
    },
    {
        "loss": 1.7135,
        "grad_norm": 3.608715534210205,
        "learning_rate": 9.58578495047003e-05,
        "epoch": 1.6768,
        "step": 12576
    },
    {
        "loss": 1.1365,
        "grad_norm": 5.509888648986816,
        "learning_rate": 9.579496759632066e-05,
        "epoch": 1.6769333333333334,
        "step": 12577
    },
    {
        "loss": 1.6587,
        "grad_norm": 2.980720043182373,
        "learning_rate": 9.573208735356856e-05,
        "epoch": 1.6770666666666667,
        "step": 12578
    },
    {
        "loss": 1.919,
        "grad_norm": 5.749986171722412,
        "learning_rate": 9.566920880135137e-05,
        "epoch": 1.6772,
        "step": 12579
    },
    {
        "loss": 2.0472,
        "grad_norm": 3.2178213596343994,
        "learning_rate": 9.56063319645753e-05,
        "epoch": 1.6773333333333333,
        "step": 12580
    },
    {
        "loss": 1.7256,
        "grad_norm": 4.524111270904541,
        "learning_rate": 9.55434568681461e-05,
        "epoch": 1.6774666666666667,
        "step": 12581
    },
    {
        "loss": 1.6266,
        "grad_norm": 2.5962204933166504,
        "learning_rate": 9.54805835369687e-05,
        "epoch": 1.6776,
        "step": 12582
    },
    {
        "loss": 1.577,
        "grad_norm": 4.332353115081787,
        "learning_rate": 9.541771199594755e-05,
        "epoch": 1.6777333333333333,
        "step": 12583
    },
    {
        "loss": 2.1894,
        "grad_norm": 3.716824531555176,
        "learning_rate": 9.53548422699866e-05,
        "epoch": 1.6778666666666666,
        "step": 12584
    },
    {
        "loss": 2.5174,
        "grad_norm": 5.851325988769531,
        "learning_rate": 9.529197438398813e-05,
        "epoch": 1.678,
        "step": 12585
    },
    {
        "loss": 2.4664,
        "grad_norm": 2.763077974319458,
        "learning_rate": 9.522910836285481e-05,
        "epoch": 1.6781333333333333,
        "step": 12586
    },
    {
        "loss": 2.8634,
        "grad_norm": 2.4957544803619385,
        "learning_rate": 9.516624423148778e-05,
        "epoch": 1.6782666666666666,
        "step": 12587
    },
    {
        "loss": 2.0549,
        "grad_norm": 4.525191307067871,
        "learning_rate": 9.510338201478804e-05,
        "epoch": 1.6784,
        "step": 12588
    },
    {
        "loss": 1.7101,
        "grad_norm": 3.8380820751190186,
        "learning_rate": 9.504052173765534e-05,
        "epoch": 1.6785333333333332,
        "step": 12589
    },
    {
        "loss": 1.8571,
        "grad_norm": 5.039369106292725,
        "learning_rate": 9.49776634249889e-05,
        "epoch": 1.6786666666666665,
        "step": 12590
    },
    {
        "loss": 1.7666,
        "grad_norm": 6.1067280769348145,
        "learning_rate": 9.491480710168694e-05,
        "epoch": 1.6787999999999998,
        "step": 12591
    },
    {
        "loss": 1.7636,
        "grad_norm": 5.020098686218262,
        "learning_rate": 9.485195279264722e-05,
        "epoch": 1.6789333333333334,
        "step": 12592
    },
    {
        "loss": 1.8592,
        "grad_norm": 5.269667625427246,
        "learning_rate": 9.478910052276682e-05,
        "epoch": 1.6790666666666667,
        "step": 12593
    },
    {
        "loss": 2.3595,
        "grad_norm": 4.020346164703369,
        "learning_rate": 9.47262503169412e-05,
        "epoch": 1.6792,
        "step": 12594
    },
    {
        "loss": 2.8473,
        "grad_norm": 5.689800262451172,
        "learning_rate": 9.46634022000659e-05,
        "epoch": 1.6793333333333333,
        "step": 12595
    },
    {
        "loss": 1.6511,
        "grad_norm": 7.1648783683776855,
        "learning_rate": 9.460055619703504e-05,
        "epoch": 1.6794666666666667,
        "step": 12596
    },
    {
        "loss": 2.242,
        "grad_norm": 4.63955545425415,
        "learning_rate": 9.453771233274243e-05,
        "epoch": 1.6796,
        "step": 12597
    },
    {
        "loss": 2.6122,
        "grad_norm": 4.025495529174805,
        "learning_rate": 9.447487063208055e-05,
        "epoch": 1.6797333333333333,
        "step": 12598
    },
    {
        "loss": 2.7922,
        "grad_norm": 2.9053454399108887,
        "learning_rate": 9.441203111994125e-05,
        "epoch": 1.6798666666666666,
        "step": 12599
    },
    {
        "loss": 1.4302,
        "grad_norm": 5.01673698425293,
        "learning_rate": 9.434919382121533e-05,
        "epoch": 1.6800000000000002,
        "step": 12600
    },
    {
        "loss": 1.8778,
        "grad_norm": 3.4488091468811035,
        "learning_rate": 9.428635876079314e-05,
        "epoch": 1.6801333333333335,
        "step": 12601
    },
    {
        "loss": 2.6321,
        "grad_norm": 3.344278573989868,
        "learning_rate": 9.422352596356369e-05,
        "epoch": 1.6802666666666668,
        "step": 12602
    },
    {
        "loss": 3.0508,
        "grad_norm": 3.054990530014038,
        "learning_rate": 9.416069545441517e-05,
        "epoch": 1.6804000000000001,
        "step": 12603
    },
    {
        "loss": 2.5372,
        "grad_norm": 3.5348827838897705,
        "learning_rate": 9.40978672582352e-05,
        "epoch": 1.6805333333333334,
        "step": 12604
    },
    {
        "loss": 2.3918,
        "grad_norm": 2.5200858116149902,
        "learning_rate": 9.403504139991014e-05,
        "epoch": 1.6806666666666668,
        "step": 12605
    },
    {
        "loss": 2.1063,
        "grad_norm": 3.082123041152954,
        "learning_rate": 9.397221790432535e-05,
        "epoch": 1.6808,
        "step": 12606
    },
    {
        "loss": 1.1825,
        "grad_norm": 4.018742561340332,
        "learning_rate": 9.390939679636573e-05,
        "epoch": 1.6809333333333334,
        "step": 12607
    },
    {
        "loss": 1.4662,
        "grad_norm": 4.840084552764893,
        "learning_rate": 9.38465781009148e-05,
        "epoch": 1.6810666666666667,
        "step": 12608
    },
    {
        "loss": 1.919,
        "grad_norm": 4.15463924407959,
        "learning_rate": 9.37837618428551e-05,
        "epoch": 1.6812,
        "step": 12609
    },
    {
        "loss": 1.6759,
        "grad_norm": 3.8651697635650635,
        "learning_rate": 9.372094804706867e-05,
        "epoch": 1.6813333333333333,
        "step": 12610
    },
    {
        "loss": 2.3339,
        "grad_norm": 3.637993097305298,
        "learning_rate": 9.36581367384361e-05,
        "epoch": 1.6814666666666667,
        "step": 12611
    },
    {
        "loss": 2.4551,
        "grad_norm": 2.621373176574707,
        "learning_rate": 9.359532794183704e-05,
        "epoch": 1.6816,
        "step": 12612
    },
    {
        "loss": 2.3378,
        "grad_norm": 4.98801851272583,
        "learning_rate": 9.353252168215055e-05,
        "epoch": 1.6817333333333333,
        "step": 12613
    },
    {
        "loss": 2.0914,
        "grad_norm": 4.120043754577637,
        "learning_rate": 9.346971798425425e-05,
        "epoch": 1.6818666666666666,
        "step": 12614
    },
    {
        "loss": 0.8682,
        "grad_norm": 4.412961959838867,
        "learning_rate": 9.34069168730248e-05,
        "epoch": 1.682,
        "step": 12615
    },
    {
        "loss": 2.9852,
        "grad_norm": 2.674330472946167,
        "learning_rate": 9.334411837333814e-05,
        "epoch": 1.6821333333333333,
        "step": 12616
    },
    {
        "loss": 1.9294,
        "grad_norm": 4.0739641189575195,
        "learning_rate": 9.328132251006889e-05,
        "epoch": 1.6822666666666666,
        "step": 12617
    },
    {
        "loss": 1.2737,
        "grad_norm": 3.089449644088745,
        "learning_rate": 9.32185293080907e-05,
        "epoch": 1.6824,
        "step": 12618
    },
    {
        "loss": 2.3884,
        "grad_norm": 5.092121601104736,
        "learning_rate": 9.3155738792276e-05,
        "epoch": 1.6825333333333332,
        "step": 12619
    },
    {
        "loss": 0.7592,
        "grad_norm": 4.0741095542907715,
        "learning_rate": 9.309295098749662e-05,
        "epoch": 1.6826666666666665,
        "step": 12620
    },
    {
        "loss": 2.7409,
        "grad_norm": 3.1313278675079346,
        "learning_rate": 9.303016591862276e-05,
        "epoch": 1.6827999999999999,
        "step": 12621
    },
    {
        "loss": 2.2701,
        "grad_norm": 4.87442684173584,
        "learning_rate": 9.296738361052421e-05,
        "epoch": 1.6829333333333332,
        "step": 12622
    },
    {
        "loss": 2.6765,
        "grad_norm": 3.2939252853393555,
        "learning_rate": 9.290460408806871e-05,
        "epoch": 1.6830666666666667,
        "step": 12623
    },
    {
        "loss": 3.1621,
        "grad_norm": 4.3613972663879395,
        "learning_rate": 9.284182737612368e-05,
        "epoch": 1.6832,
        "step": 12624
    },
    {
        "loss": 2.5956,
        "grad_norm": 3.869420289993286,
        "learning_rate": 9.277905349955537e-05,
        "epoch": 1.6833333333333333,
        "step": 12625
    },
    {
        "loss": 1.7524,
        "grad_norm": 3.895902156829834,
        "learning_rate": 9.271628248322858e-05,
        "epoch": 1.6834666666666667,
        "step": 12626
    },
    {
        "loss": 2.627,
        "grad_norm": 3.0867507457733154,
        "learning_rate": 9.265351435200711e-05,
        "epoch": 1.6836,
        "step": 12627
    },
    {
        "loss": 2.2242,
        "grad_norm": 4.066697597503662,
        "learning_rate": 9.259074913075349e-05,
        "epoch": 1.6837333333333333,
        "step": 12628
    },
    {
        "loss": 2.1236,
        "grad_norm": 2.9144554138183594,
        "learning_rate": 9.252798684432952e-05,
        "epoch": 1.6838666666666666,
        "step": 12629
    },
    {
        "loss": 2.1236,
        "grad_norm": 2.9946346282958984,
        "learning_rate": 9.246522751759529e-05,
        "epoch": 1.6840000000000002,
        "step": 12630
    },
    {
        "loss": 2.438,
        "grad_norm": 3.720341682434082,
        "learning_rate": 9.240247117541042e-05,
        "epoch": 1.6841333333333335,
        "step": 12631
    },
    {
        "loss": 2.6343,
        "grad_norm": 4.2937912940979,
        "learning_rate": 9.233971784263229e-05,
        "epoch": 1.6842666666666668,
        "step": 12632
    },
    {
        "loss": 1.414,
        "grad_norm": 4.626111030578613,
        "learning_rate": 9.227696754411805e-05,
        "epoch": 1.6844000000000001,
        "step": 12633
    },
    {
        "loss": 1.8333,
        "grad_norm": 2.6514997482299805,
        "learning_rate": 9.221422030472339e-05,
        "epoch": 1.6845333333333334,
        "step": 12634
    },
    {
        "loss": 2.7704,
        "grad_norm": 2.9935038089752197,
        "learning_rate": 9.215147614930262e-05,
        "epoch": 1.6846666666666668,
        "step": 12635
    },
    {
        "loss": 2.3205,
        "grad_norm": 3.657310724258423,
        "learning_rate": 9.208873510270889e-05,
        "epoch": 1.6848,
        "step": 12636
    },
    {
        "loss": 1.8844,
        "grad_norm": 3.6435532569885254,
        "learning_rate": 9.202599718979401e-05,
        "epoch": 1.6849333333333334,
        "step": 12637
    },
    {
        "loss": 1.6942,
        "grad_norm": 3.9236440658569336,
        "learning_rate": 9.196326243540892e-05,
        "epoch": 1.6850666666666667,
        "step": 12638
    },
    {
        "loss": 1.6778,
        "grad_norm": 3.782277822494507,
        "learning_rate": 9.190053086440298e-05,
        "epoch": 1.6852,
        "step": 12639
    },
    {
        "loss": 2.497,
        "grad_norm": 2.615818738937378,
        "learning_rate": 9.183780250162438e-05,
        "epoch": 1.6853333333333333,
        "step": 12640
    },
    {
        "loss": 2.1953,
        "grad_norm": 2.4908249378204346,
        "learning_rate": 9.177507737191989e-05,
        "epoch": 1.6854666666666667,
        "step": 12641
    },
    {
        "loss": 1.4592,
        "grad_norm": 6.154447078704834,
        "learning_rate": 9.171235550013525e-05,
        "epoch": 1.6856,
        "step": 12642
    },
    {
        "loss": 1.4319,
        "grad_norm": 4.200144290924072,
        "learning_rate": 9.164963691111512e-05,
        "epoch": 1.6857333333333333,
        "step": 12643
    },
    {
        "loss": 2.0435,
        "grad_norm": 3.861828088760376,
        "learning_rate": 9.1586921629702e-05,
        "epoch": 1.6858666666666666,
        "step": 12644
    },
    {
        "loss": 1.4294,
        "grad_norm": 4.450629234313965,
        "learning_rate": 9.152420968073801e-05,
        "epoch": 1.686,
        "step": 12645
    },
    {
        "loss": 2.9688,
        "grad_norm": 2.6334290504455566,
        "learning_rate": 9.14615010890633e-05,
        "epoch": 1.6861333333333333,
        "step": 12646
    },
    {
        "loss": 1.7947,
        "grad_norm": 2.5142111778259277,
        "learning_rate": 9.139879587951727e-05,
        "epoch": 1.6862666666666666,
        "step": 12647
    },
    {
        "loss": 3.2062,
        "grad_norm": 3.7665319442749023,
        "learning_rate": 9.133609407693748e-05,
        "epoch": 1.6864,
        "step": 12648
    },
    {
        "loss": 3.2183,
        "grad_norm": 4.886509418487549,
        "learning_rate": 9.127339570616035e-05,
        "epoch": 1.6865333333333332,
        "step": 12649
    },
    {
        "loss": 2.0244,
        "grad_norm": 3.316948652267456,
        "learning_rate": 9.121070079202078e-05,
        "epoch": 1.6866666666666665,
        "step": 12650
    },
    {
        "loss": 2.404,
        "grad_norm": 3.4149932861328125,
        "learning_rate": 9.11480093593526e-05,
        "epoch": 1.6867999999999999,
        "step": 12651
    },
    {
        "loss": 1.6257,
        "grad_norm": 2.967289686203003,
        "learning_rate": 9.108532143298831e-05,
        "epoch": 1.6869333333333332,
        "step": 12652
    },
    {
        "loss": 1.9385,
        "grad_norm": 2.961087703704834,
        "learning_rate": 9.10226370377583e-05,
        "epoch": 1.6870666666666667,
        "step": 12653
    },
    {
        "loss": 1.7174,
        "grad_norm": 3.5823075771331787,
        "learning_rate": 9.095995619849252e-05,
        "epoch": 1.6872,
        "step": 12654
    },
    {
        "loss": 2.1521,
        "grad_norm": 5.10051155090332,
        "learning_rate": 9.089727894001875e-05,
        "epoch": 1.6873333333333334,
        "step": 12655
    },
    {
        "loss": 1.9462,
        "grad_norm": 5.818016052246094,
        "learning_rate": 9.083460528716391e-05,
        "epoch": 1.6874666666666667,
        "step": 12656
    },
    {
        "loss": 2.3224,
        "grad_norm": 3.1045174598693848,
        "learning_rate": 9.077193526475318e-05,
        "epoch": 1.6876,
        "step": 12657
    },
    {
        "loss": 2.2901,
        "grad_norm": 3.5417959690093994,
        "learning_rate": 9.070926889761033e-05,
        "epoch": 1.6877333333333333,
        "step": 12658
    },
    {
        "loss": 2.0391,
        "grad_norm": 4.314396858215332,
        "learning_rate": 9.064660621055761e-05,
        "epoch": 1.6878666666666666,
        "step": 12659
    },
    {
        "loss": 2.2149,
        "grad_norm": 4.739702224731445,
        "learning_rate": 9.058394722841613e-05,
        "epoch": 1.688,
        "step": 12660
    },
    {
        "loss": 1.5048,
        "grad_norm": 4.793227195739746,
        "learning_rate": 9.052129197600527e-05,
        "epoch": 1.6881333333333335,
        "step": 12661
    },
    {
        "loss": 1.0241,
        "grad_norm": 2.5241920948028564,
        "learning_rate": 9.045864047814282e-05,
        "epoch": 1.6882666666666668,
        "step": 12662
    },
    {
        "loss": 2.8453,
        "grad_norm": 2.5223357677459717,
        "learning_rate": 9.03959927596455e-05,
        "epoch": 1.6884000000000001,
        "step": 12663
    },
    {
        "loss": 2.0892,
        "grad_norm": 3.018185615539551,
        "learning_rate": 9.03333488453282e-05,
        "epoch": 1.6885333333333334,
        "step": 12664
    },
    {
        "loss": 1.1714,
        "grad_norm": 5.984456539154053,
        "learning_rate": 9.027070876000422e-05,
        "epoch": 1.6886666666666668,
        "step": 12665
    },
    {
        "loss": 1.7861,
        "grad_norm": 5.291379928588867,
        "learning_rate": 9.020807252848578e-05,
        "epoch": 1.6888,
        "step": 12666
    },
    {
        "loss": 1.5775,
        "grad_norm": 2.344902515411377,
        "learning_rate": 9.014544017558316e-05,
        "epoch": 1.6889333333333334,
        "step": 12667
    },
    {
        "loss": 2.0659,
        "grad_norm": 3.1423025131225586,
        "learning_rate": 9.008281172610529e-05,
        "epoch": 1.6890666666666667,
        "step": 12668
    },
    {
        "loss": 1.8084,
        "grad_norm": 5.680599689483643,
        "learning_rate": 9.002018720485936e-05,
        "epoch": 1.6892,
        "step": 12669
    },
    {
        "loss": 2.2519,
        "grad_norm": 2.969778060913086,
        "learning_rate": 8.99575666366514e-05,
        "epoch": 1.6893333333333334,
        "step": 12670
    },
    {
        "loss": 2.5479,
        "grad_norm": 4.0946784019470215,
        "learning_rate": 8.989495004628538e-05,
        "epoch": 1.6894666666666667,
        "step": 12671
    },
    {
        "loss": 2.2782,
        "grad_norm": 4.558204174041748,
        "learning_rate": 8.983233745856422e-05,
        "epoch": 1.6896,
        "step": 12672
    },
    {
        "loss": 1.7861,
        "grad_norm": 5.148409843444824,
        "learning_rate": 8.976972889828883e-05,
        "epoch": 1.6897333333333333,
        "step": 12673
    },
    {
        "loss": 2.5495,
        "grad_norm": 3.5794827938079834,
        "learning_rate": 8.970712439025855e-05,
        "epoch": 1.6898666666666666,
        "step": 12674
    },
    {
        "loss": 2.6426,
        "grad_norm": 3.170011043548584,
        "learning_rate": 8.96445239592715e-05,
        "epoch": 1.69,
        "step": 12675
    },
    {
        "loss": 1.9887,
        "grad_norm": 2.874295711517334,
        "learning_rate": 8.958192763012382e-05,
        "epoch": 1.6901333333333333,
        "step": 12676
    },
    {
        "loss": 2.2553,
        "grad_norm": 6.165950775146484,
        "learning_rate": 8.951933542761008e-05,
        "epoch": 1.6902666666666666,
        "step": 12677
    },
    {
        "loss": 0.8674,
        "grad_norm": 4.232011795043945,
        "learning_rate": 8.945674737652316e-05,
        "epoch": 1.6904,
        "step": 12678
    },
    {
        "loss": 2.68,
        "grad_norm": 3.7914817333221436,
        "learning_rate": 8.939416350165467e-05,
        "epoch": 1.6905333333333332,
        "step": 12679
    },
    {
        "loss": 1.6251,
        "grad_norm": 5.228318691253662,
        "learning_rate": 8.933158382779401e-05,
        "epoch": 1.6906666666666665,
        "step": 12680
    },
    {
        "loss": 1.8446,
        "grad_norm": 4.197133541107178,
        "learning_rate": 8.926900837972964e-05,
        "epoch": 1.6907999999999999,
        "step": 12681
    },
    {
        "loss": 2.809,
        "grad_norm": 4.426909446716309,
        "learning_rate": 8.920643718224731e-05,
        "epoch": 1.6909333333333332,
        "step": 12682
    },
    {
        "loss": 1.3746,
        "grad_norm": 7.8343048095703125,
        "learning_rate": 8.914387026013202e-05,
        "epoch": 1.6910666666666667,
        "step": 12683
    },
    {
        "loss": 2.6843,
        "grad_norm": 3.7401139736175537,
        "learning_rate": 8.908130763816685e-05,
        "epoch": 1.6912,
        "step": 12684
    },
    {
        "loss": 1.381,
        "grad_norm": 3.565208911895752,
        "learning_rate": 8.901874934113289e-05,
        "epoch": 1.6913333333333334,
        "step": 12685
    },
    {
        "loss": 2.4953,
        "grad_norm": 5.61800479888916,
        "learning_rate": 8.895619539380977e-05,
        "epoch": 1.6914666666666667,
        "step": 12686
    },
    {
        "loss": 1.9887,
        "grad_norm": 4.219080448150635,
        "learning_rate": 8.889364582097515e-05,
        "epoch": 1.6916,
        "step": 12687
    },
    {
        "loss": 1.2956,
        "grad_norm": 3.9985105991363525,
        "learning_rate": 8.883110064740539e-05,
        "epoch": 1.6917333333333333,
        "step": 12688
    },
    {
        "loss": 2.136,
        "grad_norm": 4.579727649688721,
        "learning_rate": 8.876855989787457e-05,
        "epoch": 1.6918666666666666,
        "step": 12689
    },
    {
        "loss": 2.4846,
        "grad_norm": 3.220965623855591,
        "learning_rate": 8.870602359715572e-05,
        "epoch": 1.692,
        "step": 12690
    },
    {
        "loss": 2.3616,
        "grad_norm": 4.6843647956848145,
        "learning_rate": 8.864349177001915e-05,
        "epoch": 1.6921333333333335,
        "step": 12691
    },
    {
        "loss": 1.7873,
        "grad_norm": 3.943338394165039,
        "learning_rate": 8.858096444123419e-05,
        "epoch": 1.6922666666666668,
        "step": 12692
    },
    {
        "loss": 1.7539,
        "grad_norm": 5.913479804992676,
        "learning_rate": 8.851844163556825e-05,
        "epoch": 1.6924000000000001,
        "step": 12693
    },
    {
        "loss": 2.583,
        "grad_norm": 3.207470178604126,
        "learning_rate": 8.845592337778671e-05,
        "epoch": 1.6925333333333334,
        "step": 12694
    },
    {
        "loss": 2.7065,
        "grad_norm": 2.978140354156494,
        "learning_rate": 8.839340969265325e-05,
        "epoch": 1.6926666666666668,
        "step": 12695
    },
    {
        "loss": 0.6283,
        "grad_norm": 3.120999574661255,
        "learning_rate": 8.833090060492961e-05,
        "epoch": 1.6928,
        "step": 12696
    },
    {
        "loss": 2.3205,
        "grad_norm": 3.241032123565674,
        "learning_rate": 8.826839613937615e-05,
        "epoch": 1.6929333333333334,
        "step": 12697
    },
    {
        "loss": 2.743,
        "grad_norm": 4.179664134979248,
        "learning_rate": 8.820589632075096e-05,
        "epoch": 1.6930666666666667,
        "step": 12698
    },
    {
        "loss": 2.3953,
        "grad_norm": 4.2747039794921875,
        "learning_rate": 8.814340117381041e-05,
        "epoch": 1.6932,
        "step": 12699
    },
    {
        "loss": 2.6244,
        "grad_norm": 3.851764678955078,
        "learning_rate": 8.808091072330892e-05,
        "epoch": 1.6933333333333334,
        "step": 12700
    },
    {
        "loss": 2.0755,
        "grad_norm": 3.235936403274536,
        "learning_rate": 8.801842499399933e-05,
        "epoch": 1.6934666666666667,
        "step": 12701
    },
    {
        "loss": 2.8329,
        "grad_norm": 4.2153754234313965,
        "learning_rate": 8.795594401063268e-05,
        "epoch": 1.6936,
        "step": 12702
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.885854482650757,
        "learning_rate": 8.789346779795735e-05,
        "epoch": 1.6937333333333333,
        "step": 12703
    },
    {
        "loss": 2.7051,
        "grad_norm": 4.0780439376831055,
        "learning_rate": 8.783099638072085e-05,
        "epoch": 1.6938666666666666,
        "step": 12704
    },
    {
        "loss": 2.4015,
        "grad_norm": 2.8761472702026367,
        "learning_rate": 8.7768529783668e-05,
        "epoch": 1.694,
        "step": 12705
    },
    {
        "loss": 2.2972,
        "grad_norm": 3.3466453552246094,
        "learning_rate": 8.770606803154233e-05,
        "epoch": 1.6941333333333333,
        "step": 12706
    },
    {
        "loss": 0.8151,
        "grad_norm": 3.2322564125061035,
        "learning_rate": 8.764361114908497e-05,
        "epoch": 1.6942666666666666,
        "step": 12707
    },
    {
        "loss": 1.3262,
        "grad_norm": 3.6273646354675293,
        "learning_rate": 8.758115916103536e-05,
        "epoch": 1.6944,
        "step": 12708
    },
    {
        "loss": 2.4538,
        "grad_norm": 4.021390914916992,
        "learning_rate": 8.751871209213081e-05,
        "epoch": 1.6945333333333332,
        "step": 12709
    },
    {
        "loss": 1.8016,
        "grad_norm": 4.051786422729492,
        "learning_rate": 8.745626996710687e-05,
        "epoch": 1.6946666666666665,
        "step": 12710
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.987243175506592,
        "learning_rate": 8.739383281069745e-05,
        "epoch": 1.6947999999999999,
        "step": 12711
    },
    {
        "loss": 2.2679,
        "grad_norm": 2.967237949371338,
        "learning_rate": 8.733140064763354e-05,
        "epoch": 1.6949333333333332,
        "step": 12712
    },
    {
        "loss": 2.6225,
        "grad_norm": 4.28084659576416,
        "learning_rate": 8.726897350264513e-05,
        "epoch": 1.6950666666666667,
        "step": 12713
    },
    {
        "loss": 2.4929,
        "grad_norm": 2.7003414630889893,
        "learning_rate": 8.720655140045954e-05,
        "epoch": 1.6952,
        "step": 12714
    },
    {
        "loss": 2.5997,
        "grad_norm": 3.37296199798584,
        "learning_rate": 8.714413436580268e-05,
        "epoch": 1.6953333333333334,
        "step": 12715
    },
    {
        "loss": 3.0161,
        "grad_norm": 4.259303569793701,
        "learning_rate": 8.708172242339803e-05,
        "epoch": 1.6954666666666667,
        "step": 12716
    },
    {
        "loss": 2.1702,
        "grad_norm": 3.505391836166382,
        "learning_rate": 8.701931559796713e-05,
        "epoch": 1.6956,
        "step": 12717
    },
    {
        "loss": 0.9374,
        "grad_norm": 3.9938650131225586,
        "learning_rate": 8.695691391422938e-05,
        "epoch": 1.6957333333333333,
        "step": 12718
    },
    {
        "loss": 1.9177,
        "grad_norm": 2.598107099533081,
        "learning_rate": 8.689451739690265e-05,
        "epoch": 1.6958666666666666,
        "step": 12719
    },
    {
        "loss": 2.0226,
        "grad_norm": 4.193325042724609,
        "learning_rate": 8.683212607070223e-05,
        "epoch": 1.696,
        "step": 12720
    },
    {
        "loss": 2.7125,
        "grad_norm": 3.8049211502075195,
        "learning_rate": 8.67697399603414e-05,
        "epoch": 1.6961333333333335,
        "step": 12721
    },
    {
        "loss": 2.5612,
        "grad_norm": 2.560420513153076,
        "learning_rate": 8.670735909053179e-05,
        "epoch": 1.6962666666666668,
        "step": 12722
    },
    {
        "loss": 1.7598,
        "grad_norm": 4.556857109069824,
        "learning_rate": 8.664498348598252e-05,
        "epoch": 1.6964000000000001,
        "step": 12723
    },
    {
        "loss": 2.6194,
        "grad_norm": 3.234276294708252,
        "learning_rate": 8.658261317140066e-05,
        "epoch": 1.6965333333333334,
        "step": 12724
    },
    {
        "loss": 1.8482,
        "grad_norm": 3.432464599609375,
        "learning_rate": 8.652024817149161e-05,
        "epoch": 1.6966666666666668,
        "step": 12725
    },
    {
        "loss": 1.6435,
        "grad_norm": 3.064802646636963,
        "learning_rate": 8.645788851095817e-05,
        "epoch": 1.6968,
        "step": 12726
    },
    {
        "loss": 1.3394,
        "grad_norm": 6.981623649597168,
        "learning_rate": 8.639553421450125e-05,
        "epoch": 1.6969333333333334,
        "step": 12727
    },
    {
        "loss": 2.4411,
        "grad_norm": 3.7895731925964355,
        "learning_rate": 8.633318530681942e-05,
        "epoch": 1.6970666666666667,
        "step": 12728
    },
    {
        "loss": 2.5184,
        "grad_norm": 3.063542604446411,
        "learning_rate": 8.627084181260961e-05,
        "epoch": 1.6972,
        "step": 12729
    },
    {
        "loss": 1.0738,
        "grad_norm": 4.917881011962891,
        "learning_rate": 8.620850375656601e-05,
        "epoch": 1.6973333333333334,
        "step": 12730
    },
    {
        "loss": 2.5307,
        "grad_norm": 1.8755450248718262,
        "learning_rate": 8.614617116338121e-05,
        "epoch": 1.6974666666666667,
        "step": 12731
    },
    {
        "loss": 2.6808,
        "grad_norm": 2.7528059482574463,
        "learning_rate": 8.608384405774519e-05,
        "epoch": 1.6976,
        "step": 12732
    },
    {
        "loss": 2.8378,
        "grad_norm": 3.7196643352508545,
        "learning_rate": 8.602152246434584e-05,
        "epoch": 1.6977333333333333,
        "step": 12733
    },
    {
        "loss": 2.5689,
        "grad_norm": 2.1842384338378906,
        "learning_rate": 8.59592064078692e-05,
        "epoch": 1.6978666666666666,
        "step": 12734
    },
    {
        "loss": 2.4106,
        "grad_norm": 2.713385820388794,
        "learning_rate": 8.589689591299876e-05,
        "epoch": 1.698,
        "step": 12735
    },
    {
        "loss": 2.0807,
        "grad_norm": 2.634243965148926,
        "learning_rate": 8.583459100441586e-05,
        "epoch": 1.6981333333333333,
        "step": 12736
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.558302402496338,
        "learning_rate": 8.577229170679963e-05,
        "epoch": 1.6982666666666666,
        "step": 12737
    },
    {
        "loss": 0.7587,
        "grad_norm": 4.461039066314697,
        "learning_rate": 8.570999804482727e-05,
        "epoch": 1.6984,
        "step": 12738
    },
    {
        "loss": 2.1857,
        "grad_norm": 3.2229807376861572,
        "learning_rate": 8.564771004317324e-05,
        "epoch": 1.6985333333333332,
        "step": 12739
    },
    {
        "loss": 2.1556,
        "grad_norm": 3.6926798820495605,
        "learning_rate": 8.558542772651042e-05,
        "epoch": 1.6986666666666665,
        "step": 12740
    },
    {
        "loss": 1.8448,
        "grad_norm": 3.261697769165039,
        "learning_rate": 8.552315111950855e-05,
        "epoch": 1.6987999999999999,
        "step": 12741
    },
    {
        "loss": 1.5684,
        "grad_norm": 3.809023857116699,
        "learning_rate": 8.546088024683583e-05,
        "epoch": 1.6989333333333332,
        "step": 12742
    },
    {
        "loss": 2.455,
        "grad_norm": 3.342629909515381,
        "learning_rate": 8.539861513315813e-05,
        "epoch": 1.6990666666666665,
        "step": 12743
    },
    {
        "loss": 2.3461,
        "grad_norm": 3.042809009552002,
        "learning_rate": 8.533635580313874e-05,
        "epoch": 1.6992,
        "step": 12744
    },
    {
        "loss": 2.5661,
        "grad_norm": 3.6531388759613037,
        "learning_rate": 8.527410228143877e-05,
        "epoch": 1.6993333333333334,
        "step": 12745
    },
    {
        "loss": 2.7347,
        "grad_norm": 2.690600633621216,
        "learning_rate": 8.52118545927169e-05,
        "epoch": 1.6994666666666667,
        "step": 12746
    },
    {
        "loss": 2.3401,
        "grad_norm": 2.2566511631011963,
        "learning_rate": 8.514961276162998e-05,
        "epoch": 1.6996,
        "step": 12747
    },
    {
        "loss": 2.633,
        "grad_norm": 3.708527088165283,
        "learning_rate": 8.508737681283206e-05,
        "epoch": 1.6997333333333333,
        "step": 12748
    },
    {
        "loss": 1.3297,
        "grad_norm": 4.451744079589844,
        "learning_rate": 8.502514677097498e-05,
        "epoch": 1.6998666666666666,
        "step": 12749
    },
    {
        "loss": 1.8377,
        "grad_norm": 3.2271363735198975,
        "learning_rate": 8.496292266070815e-05,
        "epoch": 1.7,
        "step": 12750
    },
    {
        "loss": 2.2968,
        "grad_norm": 4.8795247077941895,
        "learning_rate": 8.49007045066789e-05,
        "epoch": 1.7001333333333335,
        "step": 12751
    },
    {
        "loss": 1.8388,
        "grad_norm": 3.8412842750549316,
        "learning_rate": 8.483849233353218e-05,
        "epoch": 1.7002666666666668,
        "step": 12752
    },
    {
        "loss": 2.7447,
        "grad_norm": 4.84049654006958,
        "learning_rate": 8.477628616591031e-05,
        "epoch": 1.7004000000000001,
        "step": 12753
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.9709651470184326,
        "learning_rate": 8.471408602845334e-05,
        "epoch": 1.7005333333333335,
        "step": 12754
    },
    {
        "loss": 1.717,
        "grad_norm": 6.404522895812988,
        "learning_rate": 8.465189194579888e-05,
        "epoch": 1.7006666666666668,
        "step": 12755
    },
    {
        "loss": 2.4096,
        "grad_norm": 3.4820680618286133,
        "learning_rate": 8.458970394258243e-05,
        "epoch": 1.7008,
        "step": 12756
    },
    {
        "loss": 2.3914,
        "grad_norm": 3.1043386459350586,
        "learning_rate": 8.452752204343674e-05,
        "epoch": 1.7009333333333334,
        "step": 12757
    },
    {
        "loss": 1.9763,
        "grad_norm": 4.884293556213379,
        "learning_rate": 8.446534627299233e-05,
        "epoch": 1.7010666666666667,
        "step": 12758
    },
    {
        "loss": 2.7272,
        "grad_norm": 3.234001874923706,
        "learning_rate": 8.440317665587704e-05,
        "epoch": 1.7012,
        "step": 12759
    },
    {
        "loss": 1.6513,
        "grad_norm": 2.009443759918213,
        "learning_rate": 8.434101321671662e-05,
        "epoch": 1.7013333333333334,
        "step": 12760
    },
    {
        "loss": 2.265,
        "grad_norm": 4.058302402496338,
        "learning_rate": 8.427885598013449e-05,
        "epoch": 1.7014666666666667,
        "step": 12761
    },
    {
        "loss": 2.6081,
        "grad_norm": 3.0819339752197266,
        "learning_rate": 8.421670497075082e-05,
        "epoch": 1.7016,
        "step": 12762
    },
    {
        "loss": 1.9111,
        "grad_norm": 5.331355094909668,
        "learning_rate": 8.415456021318421e-05,
        "epoch": 1.7017333333333333,
        "step": 12763
    },
    {
        "loss": 2.4526,
        "grad_norm": 3.3689894676208496,
        "learning_rate": 8.409242173205017e-05,
        "epoch": 1.7018666666666666,
        "step": 12764
    },
    {
        "loss": 1.9329,
        "grad_norm": 4.6039276123046875,
        "learning_rate": 8.403028955196222e-05,
        "epoch": 1.702,
        "step": 12765
    },
    {
        "loss": 2.424,
        "grad_norm": 3.611459493637085,
        "learning_rate": 8.396816369753099e-05,
        "epoch": 1.7021333333333333,
        "step": 12766
    },
    {
        "loss": 2.5158,
        "grad_norm": 2.6590347290039062,
        "learning_rate": 8.390604419336474e-05,
        "epoch": 1.7022666666666666,
        "step": 12767
    },
    {
        "loss": 1.5976,
        "grad_norm": 2.1484649181365967,
        "learning_rate": 8.384393106406908e-05,
        "epoch": 1.7024,
        "step": 12768
    },
    {
        "loss": 2.4501,
        "grad_norm": 3.546182632446289,
        "learning_rate": 8.378182433424739e-05,
        "epoch": 1.7025333333333332,
        "step": 12769
    },
    {
        "loss": 2.2505,
        "grad_norm": 4.427168846130371,
        "learning_rate": 8.371972402850062e-05,
        "epoch": 1.7026666666666666,
        "step": 12770
    },
    {
        "loss": 2.5406,
        "grad_norm": 3.165877103805542,
        "learning_rate": 8.365763017142636e-05,
        "epoch": 1.7027999999999999,
        "step": 12771
    },
    {
        "loss": 1.5952,
        "grad_norm": 3.6388437747955322,
        "learning_rate": 8.359554278762061e-05,
        "epoch": 1.7029333333333332,
        "step": 12772
    },
    {
        "loss": 2.5239,
        "grad_norm": 3.011258840560913,
        "learning_rate": 8.353346190167613e-05,
        "epoch": 1.7030666666666665,
        "step": 12773
    },
    {
        "loss": 1.6203,
        "grad_norm": 4.58294153213501,
        "learning_rate": 8.347138753818362e-05,
        "epoch": 1.7032,
        "step": 12774
    },
    {
        "loss": 2.6039,
        "grad_norm": 3.1559560298919678,
        "learning_rate": 8.340931972173082e-05,
        "epoch": 1.7033333333333334,
        "step": 12775
    },
    {
        "loss": 2.5715,
        "grad_norm": 4.0462870597839355,
        "learning_rate": 8.334725847690302e-05,
        "epoch": 1.7034666666666667,
        "step": 12776
    },
    {
        "loss": 2.5903,
        "grad_norm": 4.985218524932861,
        "learning_rate": 8.328520382828276e-05,
        "epoch": 1.7036,
        "step": 12777
    },
    {
        "loss": 1.9541,
        "grad_norm": 6.852345943450928,
        "learning_rate": 8.322315580045034e-05,
        "epoch": 1.7037333333333333,
        "step": 12778
    },
    {
        "loss": 2.2342,
        "grad_norm": 3.880005359649658,
        "learning_rate": 8.316111441798305e-05,
        "epoch": 1.7038666666666666,
        "step": 12779
    },
    {
        "loss": 2.4728,
        "grad_norm": 2.8571622371673584,
        "learning_rate": 8.309907970545561e-05,
        "epoch": 1.704,
        "step": 12780
    },
    {
        "loss": 2.0757,
        "grad_norm": 3.164463758468628,
        "learning_rate": 8.303705168744043e-05,
        "epoch": 1.7041333333333335,
        "step": 12781
    },
    {
        "loss": 2.0357,
        "grad_norm": 4.856589317321777,
        "learning_rate": 8.297503038850685e-05,
        "epoch": 1.7042666666666668,
        "step": 12782
    },
    {
        "loss": 2.787,
        "grad_norm": 2.7651286125183105,
        "learning_rate": 8.291301583322161e-05,
        "epoch": 1.7044000000000001,
        "step": 12783
    },
    {
        "loss": 2.667,
        "grad_norm": 4.066735744476318,
        "learning_rate": 8.285100804614918e-05,
        "epoch": 1.7045333333333335,
        "step": 12784
    },
    {
        "loss": 2.1855,
        "grad_norm": 5.32824182510376,
        "learning_rate": 8.278900705185088e-05,
        "epoch": 1.7046666666666668,
        "step": 12785
    },
    {
        "loss": 1.4009,
        "grad_norm": 6.525848865509033,
        "learning_rate": 8.272701287488555e-05,
        "epoch": 1.7048,
        "step": 12786
    },
    {
        "loss": 2.6949,
        "grad_norm": 3.333341598510742,
        "learning_rate": 8.266502553980911e-05,
        "epoch": 1.7049333333333334,
        "step": 12787
    },
    {
        "loss": 2.5426,
        "grad_norm": 2.6709322929382324,
        "learning_rate": 8.260304507117528e-05,
        "epoch": 1.7050666666666667,
        "step": 12788
    },
    {
        "loss": 1.2829,
        "grad_norm": 4.632513523101807,
        "learning_rate": 8.254107149353442e-05,
        "epoch": 1.7052,
        "step": 12789
    },
    {
        "loss": 1.4694,
        "grad_norm": 3.462038278579712,
        "learning_rate": 8.247910483143479e-05,
        "epoch": 1.7053333333333334,
        "step": 12790
    },
    {
        "loss": 1.9319,
        "grad_norm": 4.159701824188232,
        "learning_rate": 8.241714510942142e-05,
        "epoch": 1.7054666666666667,
        "step": 12791
    },
    {
        "loss": 1.262,
        "grad_norm": 3.5559780597686768,
        "learning_rate": 8.235519235203668e-05,
        "epoch": 1.7056,
        "step": 12792
    },
    {
        "loss": 1.9916,
        "grad_norm": 4.051399230957031,
        "learning_rate": 8.229324658382047e-05,
        "epoch": 1.7057333333333333,
        "step": 12793
    },
    {
        "loss": 1.7862,
        "grad_norm": 3.0104846954345703,
        "learning_rate": 8.223130782930963e-05,
        "epoch": 1.7058666666666666,
        "step": 12794
    },
    {
        "loss": 2.6871,
        "grad_norm": 2.8031270503997803,
        "learning_rate": 8.216937611303832e-05,
        "epoch": 1.706,
        "step": 12795
    },
    {
        "loss": 2.1314,
        "grad_norm": 3.7212369441986084,
        "learning_rate": 8.210745145953775e-05,
        "epoch": 1.7061333333333333,
        "step": 12796
    },
    {
        "loss": 2.5469,
        "grad_norm": 4.4144287109375,
        "learning_rate": 8.204553389333673e-05,
        "epoch": 1.7062666666666666,
        "step": 12797
    },
    {
        "loss": 2.1866,
        "grad_norm": 4.419307708740234,
        "learning_rate": 8.198362343896082e-05,
        "epoch": 1.7064,
        "step": 12798
    },
    {
        "loss": 1.3015,
        "grad_norm": 4.029748916625977,
        "learning_rate": 8.19217201209333e-05,
        "epoch": 1.7065333333333332,
        "step": 12799
    },
    {
        "loss": 1.352,
        "grad_norm": 5.744279861450195,
        "learning_rate": 8.185982396377379e-05,
        "epoch": 1.7066666666666666,
        "step": 12800
    },
    {
        "loss": 2.5943,
        "grad_norm": 2.9996535778045654,
        "learning_rate": 8.179793499199979e-05,
        "epoch": 1.7067999999999999,
        "step": 12801
    },
    {
        "loss": 2.4738,
        "grad_norm": 4.114795684814453,
        "learning_rate": 8.173605323012592e-05,
        "epoch": 1.7069333333333332,
        "step": 12802
    },
    {
        "loss": 2.7138,
        "grad_norm": 2.8507258892059326,
        "learning_rate": 8.167417870266359e-05,
        "epoch": 1.7070666666666665,
        "step": 12803
    },
    {
        "loss": 2.2105,
        "grad_norm": 5.395048141479492,
        "learning_rate": 8.161231143412151e-05,
        "epoch": 1.7072,
        "step": 12804
    },
    {
        "loss": 2.0266,
        "grad_norm": 4.79988956451416,
        "learning_rate": 8.15504514490054e-05,
        "epoch": 1.7073333333333334,
        "step": 12805
    },
    {
        "loss": 1.3007,
        "grad_norm": 3.813169479370117,
        "learning_rate": 8.148859877181848e-05,
        "epoch": 1.7074666666666667,
        "step": 12806
    },
    {
        "loss": 2.1863,
        "grad_norm": 4.045409679412842,
        "learning_rate": 8.142675342706067e-05,
        "epoch": 1.7076,
        "step": 12807
    },
    {
        "loss": 2.086,
        "grad_norm": 3.174767255783081,
        "learning_rate": 8.136491543922908e-05,
        "epoch": 1.7077333333333333,
        "step": 12808
    },
    {
        "loss": 1.6808,
        "grad_norm": 4.9458112716674805,
        "learning_rate": 8.130308483281787e-05,
        "epoch": 1.7078666666666666,
        "step": 12809
    },
    {
        "loss": 1.956,
        "grad_norm": 3.8164665699005127,
        "learning_rate": 8.124126163231846e-05,
        "epoch": 1.708,
        "step": 12810
    },
    {
        "loss": 1.808,
        "grad_norm": 2.9277048110961914,
        "learning_rate": 8.117944586221937e-05,
        "epoch": 1.7081333333333333,
        "step": 12811
    },
    {
        "loss": 1.0707,
        "grad_norm": 3.497138023376465,
        "learning_rate": 8.111763754700591e-05,
        "epoch": 1.7082666666666668,
        "step": 12812
    },
    {
        "loss": 2.0486,
        "grad_norm": 3.183744430541992,
        "learning_rate": 8.105583671116056e-05,
        "epoch": 1.7084000000000001,
        "step": 12813
    },
    {
        "loss": 1.9182,
        "grad_norm": 3.1535732746124268,
        "learning_rate": 8.09940433791627e-05,
        "epoch": 1.7085333333333335,
        "step": 12814
    },
    {
        "loss": 0.9127,
        "grad_norm": 4.1080851554870605,
        "learning_rate": 8.093225757548914e-05,
        "epoch": 1.7086666666666668,
        "step": 12815
    },
    {
        "loss": 1.3354,
        "grad_norm": 4.198015213012695,
        "learning_rate": 8.087047932461334e-05,
        "epoch": 1.7088,
        "step": 12816
    },
    {
        "loss": 2.698,
        "grad_norm": 3.730248212814331,
        "learning_rate": 8.080870865100587e-05,
        "epoch": 1.7089333333333334,
        "step": 12817
    },
    {
        "loss": 0.7963,
        "grad_norm": 6.081956386566162,
        "learning_rate": 8.074694557913413e-05,
        "epoch": 1.7090666666666667,
        "step": 12818
    },
    {
        "loss": 2.2074,
        "grad_norm": 3.476222515106201,
        "learning_rate": 8.068519013346288e-05,
        "epoch": 1.7092,
        "step": 12819
    },
    {
        "loss": 2.5616,
        "grad_norm": 2.4868648052215576,
        "learning_rate": 8.06234423384539e-05,
        "epoch": 1.7093333333333334,
        "step": 12820
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.8769491910934448,
        "learning_rate": 8.056170221856516e-05,
        "epoch": 1.7094666666666667,
        "step": 12821
    },
    {
        "loss": 2.2547,
        "grad_norm": 4.334976673126221,
        "learning_rate": 8.049996979825253e-05,
        "epoch": 1.7096,
        "step": 12822
    },
    {
        "loss": 2.5868,
        "grad_norm": 3.68057918548584,
        "learning_rate": 8.043824510196819e-05,
        "epoch": 1.7097333333333333,
        "step": 12823
    },
    {
        "loss": 2.1351,
        "grad_norm": 4.739989280700684,
        "learning_rate": 8.037652815416174e-05,
        "epoch": 1.7098666666666666,
        "step": 12824
    },
    {
        "loss": 2.5467,
        "grad_norm": 2.5723800659179688,
        "learning_rate": 8.031481897927938e-05,
        "epoch": 1.71,
        "step": 12825
    },
    {
        "loss": 2.619,
        "grad_norm": 3.511599063873291,
        "learning_rate": 8.025311760176424e-05,
        "epoch": 1.7101333333333333,
        "step": 12826
    },
    {
        "loss": 2.5322,
        "grad_norm": 2.8412058353424072,
        "learning_rate": 8.019142404605641e-05,
        "epoch": 1.7102666666666666,
        "step": 12827
    },
    {
        "loss": 1.5433,
        "grad_norm": 4.866629123687744,
        "learning_rate": 8.012973833659315e-05,
        "epoch": 1.7104,
        "step": 12828
    },
    {
        "loss": 2.11,
        "grad_norm": 4.8990278244018555,
        "learning_rate": 8.006806049780824e-05,
        "epoch": 1.7105333333333332,
        "step": 12829
    },
    {
        "loss": 1.864,
        "grad_norm": 4.458354473114014,
        "learning_rate": 8.000639055413232e-05,
        "epoch": 1.7106666666666666,
        "step": 12830
    },
    {
        "loss": 2.5242,
        "grad_norm": 3.644519090652466,
        "learning_rate": 7.994472852999339e-05,
        "epoch": 1.7107999999999999,
        "step": 12831
    },
    {
        "loss": 1.2737,
        "grad_norm": 4.451717853546143,
        "learning_rate": 7.988307444981568e-05,
        "epoch": 1.7109333333333332,
        "step": 12832
    },
    {
        "loss": 1.9185,
        "grad_norm": 5.096459865570068,
        "learning_rate": 7.982142833802086e-05,
        "epoch": 1.7110666666666665,
        "step": 12833
    },
    {
        "loss": 1.926,
        "grad_norm": 2.420441150665283,
        "learning_rate": 7.975979021902705e-05,
        "epoch": 1.7112,
        "step": 12834
    },
    {
        "loss": 2.0899,
        "grad_norm": 3.0507733821868896,
        "learning_rate": 7.96981601172493e-05,
        "epoch": 1.7113333333333334,
        "step": 12835
    },
    {
        "loss": 1.4967,
        "grad_norm": 6.59323787689209,
        "learning_rate": 7.963653805709934e-05,
        "epoch": 1.7114666666666667,
        "step": 12836
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.4940600395202637,
        "learning_rate": 7.957492406298621e-05,
        "epoch": 1.7116,
        "step": 12837
    },
    {
        "loss": 1.4511,
        "grad_norm": 5.525198936462402,
        "learning_rate": 7.951331815931524e-05,
        "epoch": 1.7117333333333333,
        "step": 12838
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.9050533771514893,
        "learning_rate": 7.945172037048864e-05,
        "epoch": 1.7118666666666666,
        "step": 12839
    },
    {
        "loss": 1.895,
        "grad_norm": 6.312100410461426,
        "learning_rate": 7.939013072090571e-05,
        "epoch": 1.712,
        "step": 12840
    },
    {
        "loss": 1.9159,
        "grad_norm": 3.281297445297241,
        "learning_rate": 7.932854923496226e-05,
        "epoch": 1.7121333333333333,
        "step": 12841
    },
    {
        "loss": 2.3628,
        "grad_norm": 2.856337308883667,
        "learning_rate": 7.926697593705076e-05,
        "epoch": 1.7122666666666668,
        "step": 12842
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.6952505111694336,
        "learning_rate": 7.920541085156086e-05,
        "epoch": 1.7124000000000001,
        "step": 12843
    },
    {
        "loss": 2.9054,
        "grad_norm": 3.8446574211120605,
        "learning_rate": 7.914385400287856e-05,
        "epoch": 1.7125333333333335,
        "step": 12844
    },
    {
        "loss": 2.1375,
        "grad_norm": 4.157776832580566,
        "learning_rate": 7.908230541538679e-05,
        "epoch": 1.7126666666666668,
        "step": 12845
    },
    {
        "loss": 1.3203,
        "grad_norm": 4.850474834442139,
        "learning_rate": 7.902076511346497e-05,
        "epoch": 1.7128,
        "step": 12846
    },
    {
        "loss": 1.9057,
        "grad_norm": 4.0706987380981445,
        "learning_rate": 7.895923312148972e-05,
        "epoch": 1.7129333333333334,
        "step": 12847
    },
    {
        "loss": 2.3681,
        "grad_norm": 3.9419705867767334,
        "learning_rate": 7.889770946383376e-05,
        "epoch": 1.7130666666666667,
        "step": 12848
    },
    {
        "loss": 2.4462,
        "grad_norm": 1.9849916696548462,
        "learning_rate": 7.883619416486728e-05,
        "epoch": 1.7132,
        "step": 12849
    },
    {
        "loss": 2.1597,
        "grad_norm": 3.3141303062438965,
        "learning_rate": 7.877468724895614e-05,
        "epoch": 1.7133333333333334,
        "step": 12850
    },
    {
        "loss": 2.6259,
        "grad_norm": 3.7570104598999023,
        "learning_rate": 7.871318874046368e-05,
        "epoch": 1.7134666666666667,
        "step": 12851
    },
    {
        "loss": 1.6772,
        "grad_norm": 6.14009428024292,
        "learning_rate": 7.865169866374985e-05,
        "epoch": 1.7136,
        "step": 12852
    },
    {
        "loss": 2.2974,
        "grad_norm": 5.4420905113220215,
        "learning_rate": 7.859021704317092e-05,
        "epoch": 1.7137333333333333,
        "step": 12853
    },
    {
        "loss": 3.675,
        "grad_norm": 3.584027051925659,
        "learning_rate": 7.852874390307997e-05,
        "epoch": 1.7138666666666666,
        "step": 12854
    },
    {
        "loss": 2.487,
        "grad_norm": 2.8981616497039795,
        "learning_rate": 7.846727926782658e-05,
        "epoch": 1.714,
        "step": 12855
    },
    {
        "loss": 2.2504,
        "grad_norm": 2.7224247455596924,
        "learning_rate": 7.840582316175738e-05,
        "epoch": 1.7141333333333333,
        "step": 12856
    },
    {
        "loss": 2.7335,
        "grad_norm": 2.778245210647583,
        "learning_rate": 7.834437560921508e-05,
        "epoch": 1.7142666666666666,
        "step": 12857
    },
    {
        "loss": 1.2582,
        "grad_norm": 4.673701763153076,
        "learning_rate": 7.828293663453966e-05,
        "epoch": 1.7144,
        "step": 12858
    },
    {
        "loss": 2.0325,
        "grad_norm": 4.2953057289123535,
        "learning_rate": 7.822150626206673e-05,
        "epoch": 1.7145333333333332,
        "step": 12859
    },
    {
        "loss": 2.3447,
        "grad_norm": 4.677392959594727,
        "learning_rate": 7.816008451612942e-05,
        "epoch": 1.7146666666666666,
        "step": 12860
    },
    {
        "loss": 2.4233,
        "grad_norm": 4.526821613311768,
        "learning_rate": 7.809867142105718e-05,
        "epoch": 1.7147999999999999,
        "step": 12861
    },
    {
        "loss": 1.88,
        "grad_norm": 2.601719856262207,
        "learning_rate": 7.803726700117579e-05,
        "epoch": 1.7149333333333332,
        "step": 12862
    },
    {
        "loss": 1.7477,
        "grad_norm": 2.8450465202331543,
        "learning_rate": 7.797587128080781e-05,
        "epoch": 1.7150666666666665,
        "step": 12863
    },
    {
        "loss": 2.4674,
        "grad_norm": 2.9708364009857178,
        "learning_rate": 7.791448428427212e-05,
        "epoch": 1.7151999999999998,
        "step": 12864
    },
    {
        "loss": 1.4317,
        "grad_norm": 4.805385589599609,
        "learning_rate": 7.785310603588457e-05,
        "epoch": 1.7153333333333334,
        "step": 12865
    },
    {
        "loss": 1.9352,
        "grad_norm": 2.8290305137634277,
        "learning_rate": 7.77917365599572e-05,
        "epoch": 1.7154666666666667,
        "step": 12866
    },
    {
        "loss": 2.3447,
        "grad_norm": 3.0864038467407227,
        "learning_rate": 7.773037588079867e-05,
        "epoch": 1.7156,
        "step": 12867
    },
    {
        "loss": 2.2017,
        "grad_norm": 3.296086549758911,
        "learning_rate": 7.766902402271398e-05,
        "epoch": 1.7157333333333333,
        "step": 12868
    },
    {
        "loss": 1.6687,
        "grad_norm": 3.2592968940734863,
        "learning_rate": 7.760768101000496e-05,
        "epoch": 1.7158666666666667,
        "step": 12869
    },
    {
        "loss": 2.1112,
        "grad_norm": 3.861003875732422,
        "learning_rate": 7.754634686696994e-05,
        "epoch": 1.716,
        "step": 12870
    },
    {
        "loss": 1.1141,
        "grad_norm": 4.155435562133789,
        "learning_rate": 7.748502161790344e-05,
        "epoch": 1.7161333333333333,
        "step": 12871
    },
    {
        "loss": 2.0107,
        "grad_norm": 4.210467338562012,
        "learning_rate": 7.74237052870966e-05,
        "epoch": 1.7162666666666668,
        "step": 12872
    },
    {
        "loss": 1.8013,
        "grad_norm": 3.3771228790283203,
        "learning_rate": 7.736239789883685e-05,
        "epoch": 1.7164000000000001,
        "step": 12873
    },
    {
        "loss": 2.3176,
        "grad_norm": 2.1071481704711914,
        "learning_rate": 7.73010994774086e-05,
        "epoch": 1.7165333333333335,
        "step": 12874
    },
    {
        "loss": 1.9296,
        "grad_norm": 4.906648635864258,
        "learning_rate": 7.723981004709217e-05,
        "epoch": 1.7166666666666668,
        "step": 12875
    },
    {
        "loss": 2.5103,
        "grad_norm": 2.6095426082611084,
        "learning_rate": 7.717852963216455e-05,
        "epoch": 1.7168,
        "step": 12876
    },
    {
        "loss": 0.9174,
        "grad_norm": 4.146819591522217,
        "learning_rate": 7.711725825689895e-05,
        "epoch": 1.7169333333333334,
        "step": 12877
    },
    {
        "loss": 3.0883,
        "grad_norm": 3.6354353427886963,
        "learning_rate": 7.705599594556536e-05,
        "epoch": 1.7170666666666667,
        "step": 12878
    },
    {
        "loss": 1.6245,
        "grad_norm": 3.7358429431915283,
        "learning_rate": 7.699474272243019e-05,
        "epoch": 1.7172,
        "step": 12879
    },
    {
        "loss": 2.9343,
        "grad_norm": 3.287168502807617,
        "learning_rate": 7.693349861175555e-05,
        "epoch": 1.7173333333333334,
        "step": 12880
    },
    {
        "loss": 1.4017,
        "grad_norm": 4.637509346008301,
        "learning_rate": 7.687226363780086e-05,
        "epoch": 1.7174666666666667,
        "step": 12881
    },
    {
        "loss": 2.4045,
        "grad_norm": 4.410549640655518,
        "learning_rate": 7.68110378248212e-05,
        "epoch": 1.7176,
        "step": 12882
    },
    {
        "loss": 1.1104,
        "grad_norm": 2.9308066368103027,
        "learning_rate": 7.67498211970686e-05,
        "epoch": 1.7177333333333333,
        "step": 12883
    },
    {
        "loss": 2.4314,
        "grad_norm": 3.1234238147735596,
        "learning_rate": 7.668861377879107e-05,
        "epoch": 1.7178666666666667,
        "step": 12884
    },
    {
        "loss": 0.9822,
        "grad_norm": 4.233125686645508,
        "learning_rate": 7.662741559423304e-05,
        "epoch": 1.718,
        "step": 12885
    },
    {
        "loss": 2.3241,
        "grad_norm": 5.133185863494873,
        "learning_rate": 7.656622666763522e-05,
        "epoch": 1.7181333333333333,
        "step": 12886
    },
    {
        "loss": 1.1371,
        "grad_norm": 3.9408512115478516,
        "learning_rate": 7.650504702323496e-05,
        "epoch": 1.7182666666666666,
        "step": 12887
    },
    {
        "loss": 2.5177,
        "grad_norm": 3.853172779083252,
        "learning_rate": 7.644387668526566e-05,
        "epoch": 1.7184,
        "step": 12888
    },
    {
        "loss": 1.8652,
        "grad_norm": 3.5119636058807373,
        "learning_rate": 7.638271567795694e-05,
        "epoch": 1.7185333333333332,
        "step": 12889
    },
    {
        "loss": 1.4394,
        "grad_norm": 3.623314142227173,
        "learning_rate": 7.632156402553514e-05,
        "epoch": 1.7186666666666666,
        "step": 12890
    },
    {
        "loss": 0.4342,
        "grad_norm": 2.189596652984619,
        "learning_rate": 7.626042175222237e-05,
        "epoch": 1.7187999999999999,
        "step": 12891
    },
    {
        "loss": 1.9827,
        "grad_norm": 2.9948322772979736,
        "learning_rate": 7.619928888223757e-05,
        "epoch": 1.7189333333333332,
        "step": 12892
    },
    {
        "loss": 2.3631,
        "grad_norm": 3.6340322494506836,
        "learning_rate": 7.613816543979555e-05,
        "epoch": 1.7190666666666665,
        "step": 12893
    },
    {
        "loss": 1.4402,
        "grad_norm": 3.4426956176757812,
        "learning_rate": 7.60770514491075e-05,
        "epoch": 1.7191999999999998,
        "step": 12894
    },
    {
        "loss": 2.5071,
        "grad_norm": 5.004988193511963,
        "learning_rate": 7.601594693438074e-05,
        "epoch": 1.7193333333333334,
        "step": 12895
    },
    {
        "loss": 1.927,
        "grad_norm": 4.253110408782959,
        "learning_rate": 7.595485191981929e-05,
        "epoch": 1.7194666666666667,
        "step": 12896
    },
    {
        "loss": 2.0592,
        "grad_norm": 5.388397693634033,
        "learning_rate": 7.589376642962292e-05,
        "epoch": 1.7196,
        "step": 12897
    },
    {
        "loss": 1.576,
        "grad_norm": 5.1509270668029785,
        "learning_rate": 7.583269048798763e-05,
        "epoch": 1.7197333333333333,
        "step": 12898
    },
    {
        "loss": 2.7388,
        "grad_norm": 4.0493669509887695,
        "learning_rate": 7.577162411910617e-05,
        "epoch": 1.7198666666666667,
        "step": 12899
    },
    {
        "loss": 2.0073,
        "grad_norm": 5.1456298828125,
        "learning_rate": 7.571056734716693e-05,
        "epoch": 1.72,
        "step": 12900
    },
    {
        "loss": 2.2641,
        "grad_norm": 4.020243167877197,
        "learning_rate": 7.564952019635459e-05,
        "epoch": 1.7201333333333333,
        "step": 12901
    },
    {
        "loss": 1.5563,
        "grad_norm": 3.2050137519836426,
        "learning_rate": 7.558848269085042e-05,
        "epoch": 1.7202666666666668,
        "step": 12902
    },
    {
        "loss": 1.9391,
        "grad_norm": 3.0580081939697266,
        "learning_rate": 7.552745485483144e-05,
        "epoch": 1.7204000000000002,
        "step": 12903
    },
    {
        "loss": 0.5817,
        "grad_norm": 3.55832576751709,
        "learning_rate": 7.546643671247097e-05,
        "epoch": 1.7205333333333335,
        "step": 12904
    },
    {
        "loss": 1.2229,
        "grad_norm": 3.8196709156036377,
        "learning_rate": 7.540542828793836e-05,
        "epoch": 1.7206666666666668,
        "step": 12905
    },
    {
        "loss": 2.7302,
        "grad_norm": 4.533539295196533,
        "learning_rate": 7.534442960539958e-05,
        "epoch": 1.7208,
        "step": 12906
    },
    {
        "loss": 2.2171,
        "grad_norm": 4.767816543579102,
        "learning_rate": 7.52834406890161e-05,
        "epoch": 1.7209333333333334,
        "step": 12907
    },
    {
        "loss": 0.6637,
        "grad_norm": 3.249870777130127,
        "learning_rate": 7.522246156294625e-05,
        "epoch": 1.7210666666666667,
        "step": 12908
    },
    {
        "loss": 1.374,
        "grad_norm": 4.065051555633545,
        "learning_rate": 7.516149225134351e-05,
        "epoch": 1.7212,
        "step": 12909
    },
    {
        "loss": 1.4398,
        "grad_norm": 3.997786283493042,
        "learning_rate": 7.510053277835836e-05,
        "epoch": 1.7213333333333334,
        "step": 12910
    },
    {
        "loss": 2.2898,
        "grad_norm": 3.168316125869751,
        "learning_rate": 7.503958316813714e-05,
        "epoch": 1.7214666666666667,
        "step": 12911
    },
    {
        "loss": 2.7418,
        "grad_norm": 2.7804830074310303,
        "learning_rate": 7.497864344482205e-05,
        "epoch": 1.7216,
        "step": 12912
    },
    {
        "loss": 1.746,
        "grad_norm": 5.108021259307861,
        "learning_rate": 7.491771363255159e-05,
        "epoch": 1.7217333333333333,
        "step": 12913
    },
    {
        "loss": 0.8886,
        "grad_norm": 3.5044913291931152,
        "learning_rate": 7.485679375546009e-05,
        "epoch": 1.7218666666666667,
        "step": 12914
    },
    {
        "loss": 2.0566,
        "grad_norm": 4.124887943267822,
        "learning_rate": 7.479588383767837e-05,
        "epoch": 1.722,
        "step": 12915
    },
    {
        "loss": 2.3756,
        "grad_norm": 4.305298328399658,
        "learning_rate": 7.473498390333281e-05,
        "epoch": 1.7221333333333333,
        "step": 12916
    },
    {
        "loss": 1.3184,
        "grad_norm": 4.052777290344238,
        "learning_rate": 7.467409397654649e-05,
        "epoch": 1.7222666666666666,
        "step": 12917
    },
    {
        "loss": 1.7949,
        "grad_norm": 6.1100687980651855,
        "learning_rate": 7.461321408143758e-05,
        "epoch": 1.7224,
        "step": 12918
    },
    {
        "loss": 2.1343,
        "grad_norm": 3.303504705429077,
        "learning_rate": 7.455234424212106e-05,
        "epoch": 1.7225333333333332,
        "step": 12919
    },
    {
        "loss": 1.9374,
        "grad_norm": 6.128054618835449,
        "learning_rate": 7.449148448270785e-05,
        "epoch": 1.7226666666666666,
        "step": 12920
    },
    {
        "loss": 1.782,
        "grad_norm": 3.4250378608703613,
        "learning_rate": 7.443063482730455e-05,
        "epoch": 1.7227999999999999,
        "step": 12921
    },
    {
        "loss": 1.8623,
        "grad_norm": 4.778023719787598,
        "learning_rate": 7.43697953000139e-05,
        "epoch": 1.7229333333333332,
        "step": 12922
    },
    {
        "loss": 2.2586,
        "grad_norm": 2.9374728202819824,
        "learning_rate": 7.430896592493455e-05,
        "epoch": 1.7230666666666665,
        "step": 12923
    },
    {
        "loss": 2.0336,
        "grad_norm": 3.3088417053222656,
        "learning_rate": 7.424814672616146e-05,
        "epoch": 1.7231999999999998,
        "step": 12924
    },
    {
        "loss": 2.5291,
        "grad_norm": 3.3946950435638428,
        "learning_rate": 7.418733772778522e-05,
        "epoch": 1.7233333333333334,
        "step": 12925
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.2105047702789307,
        "learning_rate": 7.412653895389245e-05,
        "epoch": 1.7234666666666667,
        "step": 12926
    },
    {
        "loss": 2.5107,
        "grad_norm": 3.2276103496551514,
        "learning_rate": 7.40657504285656e-05,
        "epoch": 1.7236,
        "step": 12927
    },
    {
        "loss": 1.4974,
        "grad_norm": 6.137575149536133,
        "learning_rate": 7.400497217588341e-05,
        "epoch": 1.7237333333333333,
        "step": 12928
    },
    {
        "loss": 1.7226,
        "grad_norm": 3.272700548171997,
        "learning_rate": 7.394420421992056e-05,
        "epoch": 1.7238666666666667,
        "step": 12929
    },
    {
        "loss": 1.9947,
        "grad_norm": 3.4259989261627197,
        "learning_rate": 7.388344658474695e-05,
        "epoch": 1.724,
        "step": 12930
    },
    {
        "loss": 1.7029,
        "grad_norm": 2.6864686012268066,
        "learning_rate": 7.382269929442926e-05,
        "epoch": 1.7241333333333333,
        "step": 12931
    },
    {
        "loss": 1.891,
        "grad_norm": 3.865682363510132,
        "learning_rate": 7.376196237302947e-05,
        "epoch": 1.7242666666666666,
        "step": 12932
    },
    {
        "loss": 1.9801,
        "grad_norm": 4.020776748657227,
        "learning_rate": 7.370123584460591e-05,
        "epoch": 1.7244000000000002,
        "step": 12933
    },
    {
        "loss": 1.5748,
        "grad_norm": 5.083964824676514,
        "learning_rate": 7.36405197332124e-05,
        "epoch": 1.7245333333333335,
        "step": 12934
    },
    {
        "loss": 2.5547,
        "grad_norm": 2.79227876663208,
        "learning_rate": 7.357981406289889e-05,
        "epoch": 1.7246666666666668,
        "step": 12935
    },
    {
        "loss": 2.3424,
        "grad_norm": 4.0448317527771,
        "learning_rate": 7.351911885771092e-05,
        "epoch": 1.7248,
        "step": 12936
    },
    {
        "loss": 2.445,
        "grad_norm": 6.362358093261719,
        "learning_rate": 7.345843414169018e-05,
        "epoch": 1.7249333333333334,
        "step": 12937
    },
    {
        "loss": 1.4168,
        "grad_norm": 2.304584503173828,
        "learning_rate": 7.339775993887439e-05,
        "epoch": 1.7250666666666667,
        "step": 12938
    },
    {
        "loss": 2.1181,
        "grad_norm": 4.250462532043457,
        "learning_rate": 7.333709627329627e-05,
        "epoch": 1.7252,
        "step": 12939
    },
    {
        "loss": 1.646,
        "grad_norm": 4.376728534698486,
        "learning_rate": 7.32764431689853e-05,
        "epoch": 1.7253333333333334,
        "step": 12940
    },
    {
        "loss": 1.6418,
        "grad_norm": 6.237851142883301,
        "learning_rate": 7.321580064996611e-05,
        "epoch": 1.7254666666666667,
        "step": 12941
    },
    {
        "loss": 1.7545,
        "grad_norm": 4.737664699554443,
        "learning_rate": 7.315516874025968e-05,
        "epoch": 1.7256,
        "step": 12942
    },
    {
        "loss": 2.4008,
        "grad_norm": 3.3745415210723877,
        "learning_rate": 7.30945474638824e-05,
        "epoch": 1.7257333333333333,
        "step": 12943
    },
    {
        "loss": 1.7725,
        "grad_norm": 3.7718889713287354,
        "learning_rate": 7.303393684484654e-05,
        "epoch": 1.7258666666666667,
        "step": 12944
    },
    {
        "loss": 2.5439,
        "grad_norm": 4.264256000518799,
        "learning_rate": 7.297333690716001e-05,
        "epoch": 1.726,
        "step": 12945
    },
    {
        "loss": 2.036,
        "grad_norm": 3.765774965286255,
        "learning_rate": 7.291274767482701e-05,
        "epoch": 1.7261333333333333,
        "step": 12946
    },
    {
        "loss": 1.7478,
        "grad_norm": 3.97967529296875,
        "learning_rate": 7.285216917184694e-05,
        "epoch": 1.7262666666666666,
        "step": 12947
    },
    {
        "loss": 1.7817,
        "grad_norm": 3.9521257877349854,
        "learning_rate": 7.279160142221501e-05,
        "epoch": 1.7264,
        "step": 12948
    },
    {
        "loss": 1.4485,
        "grad_norm": 3.6312077045440674,
        "learning_rate": 7.27310444499226e-05,
        "epoch": 1.7265333333333333,
        "step": 12949
    },
    {
        "loss": 1.7398,
        "grad_norm": 4.442734241485596,
        "learning_rate": 7.267049827895642e-05,
        "epoch": 1.7266666666666666,
        "step": 12950
    },
    {
        "loss": 1.9179,
        "grad_norm": 4.578732967376709,
        "learning_rate": 7.260996293329888e-05,
        "epoch": 1.7268,
        "step": 12951
    },
    {
        "loss": 2.5455,
        "grad_norm": 3.25492262840271,
        "learning_rate": 7.254943843692843e-05,
        "epoch": 1.7269333333333332,
        "step": 12952
    },
    {
        "loss": 0.5163,
        "grad_norm": 2.716578960418701,
        "learning_rate": 7.248892481381899e-05,
        "epoch": 1.7270666666666665,
        "step": 12953
    },
    {
        "loss": 1.4452,
        "grad_norm": 8.68405532836914,
        "learning_rate": 7.242842208794004e-05,
        "epoch": 1.7271999999999998,
        "step": 12954
    },
    {
        "loss": 2.5862,
        "grad_norm": 4.074240207672119,
        "learning_rate": 7.236793028325717e-05,
        "epoch": 1.7273333333333334,
        "step": 12955
    },
    {
        "loss": 1.7919,
        "grad_norm": 4.097311496734619,
        "learning_rate": 7.230744942373126e-05,
        "epoch": 1.7274666666666667,
        "step": 12956
    },
    {
        "loss": 1.5865,
        "grad_norm": 3.5355517864227295,
        "learning_rate": 7.224697953331888e-05,
        "epoch": 1.7276,
        "step": 12957
    },
    {
        "loss": 2.2235,
        "grad_norm": 4.514149188995361,
        "learning_rate": 7.21865206359726e-05,
        "epoch": 1.7277333333333333,
        "step": 12958
    },
    {
        "loss": 2.2508,
        "grad_norm": 4.089416980743408,
        "learning_rate": 7.212607275564025e-05,
        "epoch": 1.7278666666666667,
        "step": 12959
    },
    {
        "loss": 2.033,
        "grad_norm": 3.3959498405456543,
        "learning_rate": 7.206563591626535e-05,
        "epoch": 1.728,
        "step": 12960
    },
    {
        "loss": 2.4244,
        "grad_norm": 4.201065540313721,
        "learning_rate": 7.200521014178738e-05,
        "epoch": 1.7281333333333333,
        "step": 12961
    },
    {
        "loss": 1.7721,
        "grad_norm": 5.294266223907471,
        "learning_rate": 7.194479545614106e-05,
        "epoch": 1.7282666666666666,
        "step": 12962
    },
    {
        "loss": 1.7718,
        "grad_norm": 3.421236038208008,
        "learning_rate": 7.188439188325683e-05,
        "epoch": 1.7284000000000002,
        "step": 12963
    },
    {
        "loss": 3.4457,
        "grad_norm": 5.56016206741333,
        "learning_rate": 7.182399944706067e-05,
        "epoch": 1.7285333333333335,
        "step": 12964
    },
    {
        "loss": 1.3799,
        "grad_norm": 3.4752037525177,
        "learning_rate": 7.176361817147448e-05,
        "epoch": 1.7286666666666668,
        "step": 12965
    },
    {
        "loss": 1.9266,
        "grad_norm": 4.749780654907227,
        "learning_rate": 7.170324808041518e-05,
        "epoch": 1.7288000000000001,
        "step": 12966
    },
    {
        "loss": 2.363,
        "grad_norm": 3.809260368347168,
        "learning_rate": 7.1642889197796e-05,
        "epoch": 1.7289333333333334,
        "step": 12967
    },
    {
        "loss": 2.0262,
        "grad_norm": 4.4391350746154785,
        "learning_rate": 7.15825415475248e-05,
        "epoch": 1.7290666666666668,
        "step": 12968
    },
    {
        "loss": 1.1507,
        "grad_norm": 3.6363744735717773,
        "learning_rate": 7.152220515350571e-05,
        "epoch": 1.7292,
        "step": 12969
    },
    {
        "loss": 0.8808,
        "grad_norm": 3.2013299465179443,
        "learning_rate": 7.146188003963832e-05,
        "epoch": 1.7293333333333334,
        "step": 12970
    },
    {
        "loss": 1.2884,
        "grad_norm": 2.7604477405548096,
        "learning_rate": 7.140156622981749e-05,
        "epoch": 1.7294666666666667,
        "step": 12971
    },
    {
        "loss": 2.6553,
        "grad_norm": 3.403143882751465,
        "learning_rate": 7.134126374793372e-05,
        "epoch": 1.7296,
        "step": 12972
    },
    {
        "loss": 1.6184,
        "grad_norm": 4.198104381561279,
        "learning_rate": 7.12809726178729e-05,
        "epoch": 1.7297333333333333,
        "step": 12973
    },
    {
        "loss": 2.1218,
        "grad_norm": 2.456024408340454,
        "learning_rate": 7.122069286351683e-05,
        "epoch": 1.7298666666666667,
        "step": 12974
    },
    {
        "loss": 0.9855,
        "grad_norm": 6.237693786621094,
        "learning_rate": 7.116042450874223e-05,
        "epoch": 1.73,
        "step": 12975
    },
    {
        "loss": 2.3988,
        "grad_norm": 2.8987457752227783,
        "learning_rate": 7.110016757742204e-05,
        "epoch": 1.7301333333333333,
        "step": 12976
    },
    {
        "loss": 2.4981,
        "grad_norm": 4.15862512588501,
        "learning_rate": 7.103992209342365e-05,
        "epoch": 1.7302666666666666,
        "step": 12977
    },
    {
        "loss": 2.2539,
        "grad_norm": 4.077193737030029,
        "learning_rate": 7.097968808061081e-05,
        "epoch": 1.7304,
        "step": 12978
    },
    {
        "loss": 2.2922,
        "grad_norm": 3.3477771282196045,
        "learning_rate": 7.091946556284254e-05,
        "epoch": 1.7305333333333333,
        "step": 12979
    },
    {
        "loss": 2.0717,
        "grad_norm": 4.624650478363037,
        "learning_rate": 7.085925456397307e-05,
        "epoch": 1.7306666666666666,
        "step": 12980
    },
    {
        "loss": 1.677,
        "grad_norm": 5.7190446853637695,
        "learning_rate": 7.079905510785212e-05,
        "epoch": 1.7308,
        "step": 12981
    },
    {
        "loss": 2.8937,
        "grad_norm": 2.3357443809509277,
        "learning_rate": 7.073886721832485e-05,
        "epoch": 1.7309333333333332,
        "step": 12982
    },
    {
        "loss": 1.6764,
        "grad_norm": 5.362955570220947,
        "learning_rate": 7.06786909192321e-05,
        "epoch": 1.7310666666666665,
        "step": 12983
    },
    {
        "loss": 2.1032,
        "grad_norm": 3.583178997039795,
        "learning_rate": 7.061852623440977e-05,
        "epoch": 1.7311999999999999,
        "step": 12984
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.3555502891540527,
        "learning_rate": 7.055837318768931e-05,
        "epoch": 1.7313333333333332,
        "step": 12985
    },
    {
        "loss": 2.6902,
        "grad_norm": 2.8732240200042725,
        "learning_rate": 7.049823180289744e-05,
        "epoch": 1.7314666666666667,
        "step": 12986
    },
    {
        "loss": 2.5769,
        "grad_norm": 5.010643482208252,
        "learning_rate": 7.043810210385646e-05,
        "epoch": 1.7316,
        "step": 12987
    },
    {
        "loss": 1.9761,
        "grad_norm": 6.665105819702148,
        "learning_rate": 7.037798411438426e-05,
        "epoch": 1.7317333333333333,
        "step": 12988
    },
    {
        "loss": 1.8957,
        "grad_norm": 3.8805572986602783,
        "learning_rate": 7.031787785829319e-05,
        "epoch": 1.7318666666666667,
        "step": 12989
    },
    {
        "loss": 0.9897,
        "grad_norm": 4.258739948272705,
        "learning_rate": 7.025778335939199e-05,
        "epoch": 1.732,
        "step": 12990
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.7650723457336426,
        "learning_rate": 7.019770064148399e-05,
        "epoch": 1.7321333333333333,
        "step": 12991
    },
    {
        "loss": 2.5846,
        "grad_norm": 2.979749917984009,
        "learning_rate": 7.013762972836847e-05,
        "epoch": 1.7322666666666666,
        "step": 12992
    },
    {
        "loss": 2.1151,
        "grad_norm": 4.811305522918701,
        "learning_rate": 7.007757064383957e-05,
        "epoch": 1.7324000000000002,
        "step": 12993
    },
    {
        "loss": 2.969,
        "grad_norm": 3.223829984664917,
        "learning_rate": 7.00175234116869e-05,
        "epoch": 1.7325333333333335,
        "step": 12994
    },
    {
        "loss": 2.6645,
        "grad_norm": 2.447291612625122,
        "learning_rate": 6.995748805569519e-05,
        "epoch": 1.7326666666666668,
        "step": 12995
    },
    {
        "loss": 1.3308,
        "grad_norm": 4.464100360870361,
        "learning_rate": 6.989746459964489e-05,
        "epoch": 1.7328000000000001,
        "step": 12996
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.4686620235443115,
        "learning_rate": 6.983745306731167e-05,
        "epoch": 1.7329333333333334,
        "step": 12997
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.9362010955810547,
        "learning_rate": 6.977745348246582e-05,
        "epoch": 1.7330666666666668,
        "step": 12998
    },
    {
        "loss": 0.9318,
        "grad_norm": 4.1565327644348145,
        "learning_rate": 6.971746586887372e-05,
        "epoch": 1.7332,
        "step": 12999
    },
    {
        "loss": 2.7059,
        "grad_norm": 4.493592262268066,
        "learning_rate": 6.965749025029648e-05,
        "epoch": 1.7333333333333334,
        "step": 13000
    },
    {
        "loss": 1.6351,
        "grad_norm": 4.208953380584717,
        "learning_rate": 6.959752665049087e-05,
        "epoch": 1.7334666666666667,
        "step": 13001
    },
    {
        "loss": 1.3405,
        "grad_norm": 4.228005886077881,
        "learning_rate": 6.953757509320852e-05,
        "epoch": 1.7336,
        "step": 13002
    },
    {
        "loss": 1.9,
        "grad_norm": 4.151369094848633,
        "learning_rate": 6.947763560219648e-05,
        "epoch": 1.7337333333333333,
        "step": 13003
    },
    {
        "loss": 1.305,
        "grad_norm": 5.393535614013672,
        "learning_rate": 6.941770820119687e-05,
        "epoch": 1.7338666666666667,
        "step": 13004
    },
    {
        "loss": 2.0903,
        "grad_norm": 2.7748031616210938,
        "learning_rate": 6.935779291394737e-05,
        "epoch": 1.734,
        "step": 13005
    },
    {
        "loss": 2.6672,
        "grad_norm": 2.9979026317596436,
        "learning_rate": 6.929788976418052e-05,
        "epoch": 1.7341333333333333,
        "step": 13006
    },
    {
        "loss": 2.079,
        "grad_norm": 2.609955310821533,
        "learning_rate": 6.923799877562399e-05,
        "epoch": 1.7342666666666666,
        "step": 13007
    },
    {
        "loss": 2.0355,
        "grad_norm": 3.5933001041412354,
        "learning_rate": 6.917811997200112e-05,
        "epoch": 1.7344,
        "step": 13008
    },
    {
        "loss": 2.1387,
        "grad_norm": 4.7188825607299805,
        "learning_rate": 6.911825337702994e-05,
        "epoch": 1.7345333333333333,
        "step": 13009
    },
    {
        "loss": 2.4256,
        "grad_norm": 2.936624526977539,
        "learning_rate": 6.905839901442368e-05,
        "epoch": 1.7346666666666666,
        "step": 13010
    },
    {
        "loss": 1.6475,
        "grad_norm": 4.972775459289551,
        "learning_rate": 6.899855690789116e-05,
        "epoch": 1.7348,
        "step": 13011
    },
    {
        "loss": 2.5567,
        "grad_norm": 4.138814449310303,
        "learning_rate": 6.893872708113587e-05,
        "epoch": 1.7349333333333332,
        "step": 13012
    },
    {
        "loss": 2.5406,
        "grad_norm": 3.2022767066955566,
        "learning_rate": 6.887890955785647e-05,
        "epoch": 1.7350666666666665,
        "step": 13013
    },
    {
        "loss": 1.346,
        "grad_norm": 2.033802032470703,
        "learning_rate": 6.88191043617472e-05,
        "epoch": 1.7351999999999999,
        "step": 13014
    },
    {
        "loss": 1.6123,
        "grad_norm": 5.398107528686523,
        "learning_rate": 6.875931151649692e-05,
        "epoch": 1.7353333333333332,
        "step": 13015
    },
    {
        "loss": 2.0347,
        "grad_norm": 3.5143415927886963,
        "learning_rate": 6.86995310457897e-05,
        "epoch": 1.7354666666666667,
        "step": 13016
    },
    {
        "loss": 1.8546,
        "grad_norm": 3.6653690338134766,
        "learning_rate": 6.863976297330498e-05,
        "epoch": 1.7356,
        "step": 13017
    },
    {
        "loss": 2.2802,
        "grad_norm": 2.935532569885254,
        "learning_rate": 6.858000732271702e-05,
        "epoch": 1.7357333333333334,
        "step": 13018
    },
    {
        "loss": 1.9651,
        "grad_norm": 3.202849864959717,
        "learning_rate": 6.85202641176951e-05,
        "epoch": 1.7358666666666667,
        "step": 13019
    },
    {
        "loss": 1.834,
        "grad_norm": 3.6775684356689453,
        "learning_rate": 6.846053338190393e-05,
        "epoch": 1.736,
        "step": 13020
    },
    {
        "loss": 0.787,
        "grad_norm": 2.80490779876709,
        "learning_rate": 6.840081513900296e-05,
        "epoch": 1.7361333333333333,
        "step": 13021
    },
    {
        "loss": 2.295,
        "grad_norm": 5.844114303588867,
        "learning_rate": 6.834110941264679e-05,
        "epoch": 1.7362666666666666,
        "step": 13022
    },
    {
        "loss": 2.577,
        "grad_norm": 3.3672292232513428,
        "learning_rate": 6.828141622648495e-05,
        "epoch": 1.7364000000000002,
        "step": 13023
    },
    {
        "loss": 1.7506,
        "grad_norm": 4.896838188171387,
        "learning_rate": 6.822173560416232e-05,
        "epoch": 1.7365333333333335,
        "step": 13024
    },
    {
        "loss": 2.3792,
        "grad_norm": 4.977553367614746,
        "learning_rate": 6.816206756931839e-05,
        "epoch": 1.7366666666666668,
        "step": 13025
    },
    {
        "loss": 1.6223,
        "grad_norm": 3.039949417114258,
        "learning_rate": 6.810241214558824e-05,
        "epoch": 1.7368000000000001,
        "step": 13026
    },
    {
        "loss": 1.8371,
        "grad_norm": 4.388985633850098,
        "learning_rate": 6.804276935660106e-05,
        "epoch": 1.7369333333333334,
        "step": 13027
    },
    {
        "loss": 1.4612,
        "grad_norm": 4.877370357513428,
        "learning_rate": 6.798313922598182e-05,
        "epoch": 1.7370666666666668,
        "step": 13028
    },
    {
        "loss": 1.8225,
        "grad_norm": 3.05576491355896,
        "learning_rate": 6.792352177735038e-05,
        "epoch": 1.7372,
        "step": 13029
    },
    {
        "loss": 2.3175,
        "grad_norm": 3.0639538764953613,
        "learning_rate": 6.786391703432124e-05,
        "epoch": 1.7373333333333334,
        "step": 13030
    },
    {
        "loss": 3.0866,
        "grad_norm": 3.341524362564087,
        "learning_rate": 6.780432502050407e-05,
        "epoch": 1.7374666666666667,
        "step": 13031
    },
    {
        "loss": 1.6065,
        "grad_norm": 4.113102912902832,
        "learning_rate": 6.774474575950334e-05,
        "epoch": 1.7376,
        "step": 13032
    },
    {
        "loss": 2.1068,
        "grad_norm": 6.263415336608887,
        "learning_rate": 6.768517927491884e-05,
        "epoch": 1.7377333333333334,
        "step": 13033
    },
    {
        "loss": 2.493,
        "grad_norm": 3.1837148666381836,
        "learning_rate": 6.76256255903448e-05,
        "epoch": 1.7378666666666667,
        "step": 13034
    },
    {
        "loss": 1.8942,
        "grad_norm": 2.8704833984375,
        "learning_rate": 6.756608472937109e-05,
        "epoch": 1.738,
        "step": 13035
    },
    {
        "loss": 2.0242,
        "grad_norm": 4.033528804779053,
        "learning_rate": 6.750655671558142e-05,
        "epoch": 1.7381333333333333,
        "step": 13036
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.6010148525238037,
        "learning_rate": 6.744704157255537e-05,
        "epoch": 1.7382666666666666,
        "step": 13037
    },
    {
        "loss": 2.9437,
        "grad_norm": 7.081958293914795,
        "learning_rate": 6.738753932386719e-05,
        "epoch": 1.7384,
        "step": 13038
    },
    {
        "loss": 2.613,
        "grad_norm": 3.6420698165893555,
        "learning_rate": 6.732804999308585e-05,
        "epoch": 1.7385333333333333,
        "step": 13039
    },
    {
        "loss": 2.6509,
        "grad_norm": 2.396895408630371,
        "learning_rate": 6.72685736037752e-05,
        "epoch": 1.7386666666666666,
        "step": 13040
    },
    {
        "loss": 1.8203,
        "grad_norm": 4.284577369689941,
        "learning_rate": 6.720911017949396e-05,
        "epoch": 1.7388,
        "step": 13041
    },
    {
        "loss": 1.6288,
        "grad_norm": 3.853126287460327,
        "learning_rate": 6.714965974379602e-05,
        "epoch": 1.7389333333333332,
        "step": 13042
    },
    {
        "loss": 3.0183,
        "grad_norm": 3.559739828109741,
        "learning_rate": 6.709022232022981e-05,
        "epoch": 1.7390666666666665,
        "step": 13043
    },
    {
        "loss": 2.666,
        "grad_norm": 4.249436378479004,
        "learning_rate": 6.703079793233868e-05,
        "epoch": 1.7391999999999999,
        "step": 13044
    },
    {
        "loss": 2.1149,
        "grad_norm": 3.4662818908691406,
        "learning_rate": 6.697138660366069e-05,
        "epoch": 1.7393333333333332,
        "step": 13045
    },
    {
        "loss": 2.4103,
        "grad_norm": 2.866720199584961,
        "learning_rate": 6.6911988357729e-05,
        "epoch": 1.7394666666666667,
        "step": 13046
    },
    {
        "loss": 2.6564,
        "grad_norm": 2.5307323932647705,
        "learning_rate": 6.685260321807173e-05,
        "epoch": 1.7396,
        "step": 13047
    },
    {
        "loss": 2.4473,
        "grad_norm": 4.15561056137085,
        "learning_rate": 6.679323120821103e-05,
        "epoch": 1.7397333333333334,
        "step": 13048
    },
    {
        "loss": 1.4723,
        "grad_norm": 5.498973369598389,
        "learning_rate": 6.673387235166471e-05,
        "epoch": 1.7398666666666667,
        "step": 13049
    },
    {
        "loss": 1.9488,
        "grad_norm": 3.0920450687408447,
        "learning_rate": 6.667452667194478e-05,
        "epoch": 1.74,
        "step": 13050
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.6479411125183105,
        "learning_rate": 6.66151941925585e-05,
        "epoch": 1.7401333333333333,
        "step": 13051
    },
    {
        "loss": 1.9794,
        "grad_norm": 4.7373480796813965,
        "learning_rate": 6.655587493700759e-05,
        "epoch": 1.7402666666666666,
        "step": 13052
    },
    {
        "loss": 2.4443,
        "grad_norm": 4.026447296142578,
        "learning_rate": 6.649656892878856e-05,
        "epoch": 1.7404,
        "step": 13053
    },
    {
        "loss": 2.2647,
        "grad_norm": 4.41613245010376,
        "learning_rate": 6.643727619139259e-05,
        "epoch": 1.7405333333333335,
        "step": 13054
    },
    {
        "loss": 2.3113,
        "grad_norm": 3.5437464714050293,
        "learning_rate": 6.637799674830591e-05,
        "epoch": 1.7406666666666668,
        "step": 13055
    },
    {
        "loss": 2.0519,
        "grad_norm": 4.723463535308838,
        "learning_rate": 6.631873062300953e-05,
        "epoch": 1.7408000000000001,
        "step": 13056
    },
    {
        "loss": 2.8897,
        "grad_norm": 4.839674472808838,
        "learning_rate": 6.625947783897845e-05,
        "epoch": 1.7409333333333334,
        "step": 13057
    },
    {
        "loss": 2.0572,
        "grad_norm": 3.2261812686920166,
        "learning_rate": 6.62002384196833e-05,
        "epoch": 1.7410666666666668,
        "step": 13058
    },
    {
        "loss": 2.0067,
        "grad_norm": 4.37264347076416,
        "learning_rate": 6.614101238858876e-05,
        "epoch": 1.7412,
        "step": 13059
    },
    {
        "loss": 1.4723,
        "grad_norm": 4.79109525680542,
        "learning_rate": 6.608179976915469e-05,
        "epoch": 1.7413333333333334,
        "step": 13060
    },
    {
        "loss": 2.4692,
        "grad_norm": 5.013881683349609,
        "learning_rate": 6.602260058483534e-05,
        "epoch": 1.7414666666666667,
        "step": 13061
    },
    {
        "loss": 1.3721,
        "grad_norm": 3.3262386322021484,
        "learning_rate": 6.596341485907966e-05,
        "epoch": 1.7416,
        "step": 13062
    },
    {
        "loss": 1.7295,
        "grad_norm": 2.7581934928894043,
        "learning_rate": 6.590424261533121e-05,
        "epoch": 1.7417333333333334,
        "step": 13063
    },
    {
        "loss": 2.2453,
        "grad_norm": 3.547719717025757,
        "learning_rate": 6.584508387702854e-05,
        "epoch": 1.7418666666666667,
        "step": 13064
    },
    {
        "loss": 2.5418,
        "grad_norm": 2.3413851261138916,
        "learning_rate": 6.578593866760454e-05,
        "epoch": 1.742,
        "step": 13065
    },
    {
        "loss": 2.5236,
        "grad_norm": 3.601867437362671,
        "learning_rate": 6.57268070104867e-05,
        "epoch": 1.7421333333333333,
        "step": 13066
    },
    {
        "loss": 1.5975,
        "grad_norm": 2.9606120586395264,
        "learning_rate": 6.566768892909749e-05,
        "epoch": 1.7422666666666666,
        "step": 13067
    },
    {
        "loss": 1.7687,
        "grad_norm": 3.1749141216278076,
        "learning_rate": 6.560858444685369e-05,
        "epoch": 1.7424,
        "step": 13068
    },
    {
        "loss": 1.6525,
        "grad_norm": 4.041463851928711,
        "learning_rate": 6.554949358716667e-05,
        "epoch": 1.7425333333333333,
        "step": 13069
    },
    {
        "loss": 1.4572,
        "grad_norm": 5.179522514343262,
        "learning_rate": 6.549041637344275e-05,
        "epoch": 1.7426666666666666,
        "step": 13070
    },
    {
        "loss": 1.5719,
        "grad_norm": 3.412912607192993,
        "learning_rate": 6.543135282908246e-05,
        "epoch": 1.7428,
        "step": 13071
    },
    {
        "loss": 2.1614,
        "grad_norm": 4.054466724395752,
        "learning_rate": 6.5372302977481e-05,
        "epoch": 1.7429333333333332,
        "step": 13072
    },
    {
        "loss": 1.2878,
        "grad_norm": 6.144757270812988,
        "learning_rate": 6.531326684202846e-05,
        "epoch": 1.7430666666666665,
        "step": 13073
    },
    {
        "loss": 2.1182,
        "grad_norm": 3.232534646987915,
        "learning_rate": 6.52542444461091e-05,
        "epoch": 1.7431999999999999,
        "step": 13074
    },
    {
        "loss": 2.1455,
        "grad_norm": 3.0465598106384277,
        "learning_rate": 6.519523581310177e-05,
        "epoch": 1.7433333333333332,
        "step": 13075
    },
    {
        "loss": 3.0939,
        "grad_norm": 4.476560115814209,
        "learning_rate": 6.513624096638027e-05,
        "epoch": 1.7434666666666667,
        "step": 13076
    },
    {
        "loss": 1.8329,
        "grad_norm": 4.265008449554443,
        "learning_rate": 6.507725992931253e-05,
        "epoch": 1.7436,
        "step": 13077
    },
    {
        "loss": 2.098,
        "grad_norm": 5.725372791290283,
        "learning_rate": 6.501829272526098e-05,
        "epoch": 1.7437333333333334,
        "step": 13078
    },
    {
        "loss": 2.6875,
        "grad_norm": 4.104023456573486,
        "learning_rate": 6.495933937758302e-05,
        "epoch": 1.7438666666666667,
        "step": 13079
    },
    {
        "loss": 2.5544,
        "grad_norm": 2.767984390258789,
        "learning_rate": 6.490039990963013e-05,
        "epoch": 1.744,
        "step": 13080
    },
    {
        "loss": 1.1159,
        "grad_norm": 4.417404651641846,
        "learning_rate": 6.484147434474843e-05,
        "epoch": 1.7441333333333333,
        "step": 13081
    },
    {
        "loss": 2.0765,
        "grad_norm": 4.880882263183594,
        "learning_rate": 6.47825627062784e-05,
        "epoch": 1.7442666666666666,
        "step": 13082
    },
    {
        "loss": 1.9365,
        "grad_norm": 3.9855995178222656,
        "learning_rate": 6.472366501755542e-05,
        "epoch": 1.7444,
        "step": 13083
    },
    {
        "loss": 2.9786,
        "grad_norm": 3.7820727825164795,
        "learning_rate": 6.466478130190883e-05,
        "epoch": 1.7445333333333335,
        "step": 13084
    },
    {
        "loss": 2.0572,
        "grad_norm": 3.2393336296081543,
        "learning_rate": 6.460591158266299e-05,
        "epoch": 1.7446666666666668,
        "step": 13085
    },
    {
        "loss": 2.2216,
        "grad_norm": 2.8313164710998535,
        "learning_rate": 6.454705588313592e-05,
        "epoch": 1.7448000000000001,
        "step": 13086
    },
    {
        "loss": 1.2316,
        "grad_norm": 4.820414066314697,
        "learning_rate": 6.448821422664083e-05,
        "epoch": 1.7449333333333334,
        "step": 13087
    },
    {
        "loss": 1.8642,
        "grad_norm": 2.447495222091675,
        "learning_rate": 6.442938663648521e-05,
        "epoch": 1.7450666666666668,
        "step": 13088
    },
    {
        "loss": 2.3487,
        "grad_norm": 3.9597582817077637,
        "learning_rate": 6.437057313597075e-05,
        "epoch": 1.7452,
        "step": 13089
    },
    {
        "loss": 2.6714,
        "grad_norm": 4.708275318145752,
        "learning_rate": 6.431177374839367e-05,
        "epoch": 1.7453333333333334,
        "step": 13090
    },
    {
        "loss": 2.1886,
        "grad_norm": 3.4988396167755127,
        "learning_rate": 6.425298849704448e-05,
        "epoch": 1.7454666666666667,
        "step": 13091
    },
    {
        "loss": 2.5573,
        "grad_norm": 2.2185425758361816,
        "learning_rate": 6.41942174052085e-05,
        "epoch": 1.7456,
        "step": 13092
    },
    {
        "loss": 2.1427,
        "grad_norm": 2.5062623023986816,
        "learning_rate": 6.413546049616486e-05,
        "epoch": 1.7457333333333334,
        "step": 13093
    },
    {
        "loss": 2.5016,
        "grad_norm": 3.2148821353912354,
        "learning_rate": 6.40767177931878e-05,
        "epoch": 1.7458666666666667,
        "step": 13094
    },
    {
        "loss": 0.8166,
        "grad_norm": 3.2227792739868164,
        "learning_rate": 6.401798931954497e-05,
        "epoch": 1.746,
        "step": 13095
    },
    {
        "loss": 1.5632,
        "grad_norm": 3.9961507320404053,
        "learning_rate": 6.395927509849914e-05,
        "epoch": 1.7461333333333333,
        "step": 13096
    },
    {
        "loss": 1.72,
        "grad_norm": 3.20707106590271,
        "learning_rate": 6.390057515330742e-05,
        "epoch": 1.7462666666666666,
        "step": 13097
    },
    {
        "loss": 2.2005,
        "grad_norm": 3.3466274738311768,
        "learning_rate": 6.384188950722088e-05,
        "epoch": 1.7464,
        "step": 13098
    },
    {
        "loss": 2.4792,
        "grad_norm": 2.9266040325164795,
        "learning_rate": 6.378321818348511e-05,
        "epoch": 1.7465333333333333,
        "step": 13099
    },
    {
        "loss": 2.5619,
        "grad_norm": 3.3784992694854736,
        "learning_rate": 6.372456120533988e-05,
        "epoch": 1.7466666666666666,
        "step": 13100
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.77748441696167,
        "learning_rate": 6.366591859601969e-05,
        "epoch": 1.7468,
        "step": 13101
    },
    {
        "loss": 2.6545,
        "grad_norm": 3.000114679336548,
        "learning_rate": 6.360729037875297e-05,
        "epoch": 1.7469333333333332,
        "step": 13102
    },
    {
        "loss": 2.0539,
        "grad_norm": 3.311831474304199,
        "learning_rate": 6.354867657676253e-05,
        "epoch": 1.7470666666666665,
        "step": 13103
    },
    {
        "loss": 1.9393,
        "grad_norm": 3.2864603996276855,
        "learning_rate": 6.349007721326537e-05,
        "epoch": 1.7471999999999999,
        "step": 13104
    },
    {
        "loss": 2.5273,
        "grad_norm": 3.334665298461914,
        "learning_rate": 6.343149231147302e-05,
        "epoch": 1.7473333333333332,
        "step": 13105
    },
    {
        "loss": 1.7761,
        "grad_norm": 4.466557025909424,
        "learning_rate": 6.337292189459144e-05,
        "epoch": 1.7474666666666665,
        "step": 13106
    },
    {
        "loss": 1.5049,
        "grad_norm": 4.866791725158691,
        "learning_rate": 6.331436598582003e-05,
        "epoch": 1.7476,
        "step": 13107
    },
    {
        "loss": 2.4888,
        "grad_norm": 4.517982482910156,
        "learning_rate": 6.325582460835339e-05,
        "epoch": 1.7477333333333334,
        "step": 13108
    },
    {
        "loss": 2.4259,
        "grad_norm": 3.3673484325408936,
        "learning_rate": 6.319729778537973e-05,
        "epoch": 1.7478666666666667,
        "step": 13109
    },
    {
        "loss": 2.1668,
        "grad_norm": 5.283166885375977,
        "learning_rate": 6.313878554008191e-05,
        "epoch": 1.748,
        "step": 13110
    },
    {
        "loss": 2.484,
        "grad_norm": 3.5039734840393066,
        "learning_rate": 6.308028789563673e-05,
        "epoch": 1.7481333333333333,
        "step": 13111
    },
    {
        "loss": 1.5335,
        "grad_norm": 5.4469428062438965,
        "learning_rate": 6.302180487521534e-05,
        "epoch": 1.7482666666666666,
        "step": 13112
    },
    {
        "loss": 1.882,
        "grad_norm": 3.062809944152832,
        "learning_rate": 6.296333650198292e-05,
        "epoch": 1.7484,
        "step": 13113
    },
    {
        "loss": 1.5902,
        "grad_norm": 4.01491117477417,
        "learning_rate": 6.290488279909909e-05,
        "epoch": 1.7485333333333335,
        "step": 13114
    },
    {
        "loss": 0.6119,
        "grad_norm": 2.124861717224121,
        "learning_rate": 6.284644378971785e-05,
        "epoch": 1.7486666666666668,
        "step": 13115
    },
    {
        "loss": 2.0784,
        "grad_norm": 4.061697959899902,
        "learning_rate": 6.278801949698657e-05,
        "epoch": 1.7488000000000001,
        "step": 13116
    },
    {
        "loss": 3.8082,
        "grad_norm": 2.7955322265625,
        "learning_rate": 6.272960994404768e-05,
        "epoch": 1.7489333333333335,
        "step": 13117
    },
    {
        "loss": 1.8298,
        "grad_norm": 3.2269487380981445,
        "learning_rate": 6.267121515403715e-05,
        "epoch": 1.7490666666666668,
        "step": 13118
    },
    {
        "loss": 1.9137,
        "grad_norm": 4.1713972091674805,
        "learning_rate": 6.261283515008562e-05,
        "epoch": 1.7492,
        "step": 13119
    },
    {
        "loss": 1.9335,
        "grad_norm": 4.427908420562744,
        "learning_rate": 6.255446995531751e-05,
        "epoch": 1.7493333333333334,
        "step": 13120
    },
    {
        "loss": 2.2659,
        "grad_norm": 3.1956191062927246,
        "learning_rate": 6.249611959285142e-05,
        "epoch": 1.7494666666666667,
        "step": 13121
    },
    {
        "loss": 1.8496,
        "grad_norm": 4.318535804748535,
        "learning_rate": 6.243778408580002e-05,
        "epoch": 1.7496,
        "step": 13122
    },
    {
        "loss": 2.2509,
        "grad_norm": 2.6828086376190186,
        "learning_rate": 6.237946345727048e-05,
        "epoch": 1.7497333333333334,
        "step": 13123
    },
    {
        "loss": 2.4438,
        "grad_norm": 3.8520946502685547,
        "learning_rate": 6.232115773036364e-05,
        "epoch": 1.7498666666666667,
        "step": 13124
    },
    {
        "loss": 2.7883,
        "grad_norm": 4.471982479095459,
        "learning_rate": 6.226286692817448e-05,
        "epoch": 1.75,
        "step": 13125
    },
    {
        "loss": 1.3268,
        "grad_norm": 2.8790082931518555,
        "learning_rate": 6.220459107379245e-05,
        "epoch": 1.7501333333333333,
        "step": 13126
    },
    {
        "loss": 2.0738,
        "grad_norm": 3.75044846534729,
        "learning_rate": 6.214633019030069e-05,
        "epoch": 1.7502666666666666,
        "step": 13127
    },
    {
        "loss": 2.5475,
        "grad_norm": 2.723459243774414,
        "learning_rate": 6.208808430077637e-05,
        "epoch": 1.7504,
        "step": 13128
    },
    {
        "loss": 2.3023,
        "grad_norm": 4.073275089263916,
        "learning_rate": 6.20298534282912e-05,
        "epoch": 1.7505333333333333,
        "step": 13129
    },
    {
        "loss": 2.3786,
        "grad_norm": 3.9457364082336426,
        "learning_rate": 6.197163759591044e-05,
        "epoch": 1.7506666666666666,
        "step": 13130
    },
    {
        "loss": 1.5877,
        "grad_norm": 3.5801682472229004,
        "learning_rate": 6.191343682669363e-05,
        "epoch": 1.7508,
        "step": 13131
    },
    {
        "loss": 2.644,
        "grad_norm": 3.7389273643493652,
        "learning_rate": 6.185525114369412e-05,
        "epoch": 1.7509333333333332,
        "step": 13132
    },
    {
        "loss": 2.5069,
        "grad_norm": 3.044074296951294,
        "learning_rate": 6.179708056995974e-05,
        "epoch": 1.7510666666666665,
        "step": 13133
    },
    {
        "loss": 1.8112,
        "grad_norm": 3.44356369972229,
        "learning_rate": 6.173892512853182e-05,
        "epoch": 1.7511999999999999,
        "step": 13134
    },
    {
        "loss": 1.7944,
        "grad_norm": 3.4230434894561768,
        "learning_rate": 6.168078484244613e-05,
        "epoch": 1.7513333333333332,
        "step": 13135
    },
    {
        "loss": 2.6452,
        "grad_norm": 4.5295891761779785,
        "learning_rate": 6.162265973473212e-05,
        "epoch": 1.7514666666666665,
        "step": 13136
    },
    {
        "loss": 1.8456,
        "grad_norm": 2.9024555683135986,
        "learning_rate": 6.156454982841326e-05,
        "epoch": 1.7516,
        "step": 13137
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.4973514080047607,
        "learning_rate": 6.150645514650729e-05,
        "epoch": 1.7517333333333334,
        "step": 13138
    },
    {
        "loss": 0.6263,
        "grad_norm": 2.2942864894866943,
        "learning_rate": 6.14483757120256e-05,
        "epoch": 1.7518666666666667,
        "step": 13139
    },
    {
        "loss": 2.4724,
        "grad_norm": 3.639286994934082,
        "learning_rate": 6.139031154797365e-05,
        "epoch": 1.752,
        "step": 13140
    },
    {
        "loss": 2.4406,
        "grad_norm": 4.609814643859863,
        "learning_rate": 6.133226267735074e-05,
        "epoch": 1.7521333333333333,
        "step": 13141
    },
    {
        "loss": 2.5973,
        "grad_norm": 3.74520206451416,
        "learning_rate": 6.127422912315046e-05,
        "epoch": 1.7522666666666666,
        "step": 13142
    },
    {
        "loss": 2.8298,
        "grad_norm": 4.038679599761963,
        "learning_rate": 6.121621090835983e-05,
        "epoch": 1.7524,
        "step": 13143
    },
    {
        "loss": 2.7985,
        "grad_norm": 5.935153961181641,
        "learning_rate": 6.11582080559605e-05,
        "epoch": 1.7525333333333335,
        "step": 13144
    },
    {
        "loss": 2.5913,
        "grad_norm": 3.4889638423919678,
        "learning_rate": 6.110022058892704e-05,
        "epoch": 1.7526666666666668,
        "step": 13145
    },
    {
        "loss": 2.1092,
        "grad_norm": 4.203833103179932,
        "learning_rate": 6.104224853022875e-05,
        "epoch": 1.7528000000000001,
        "step": 13146
    },
    {
        "loss": 1.1494,
        "grad_norm": 3.776268243789673,
        "learning_rate": 6.09842919028287e-05,
        "epoch": 1.7529333333333335,
        "step": 13147
    },
    {
        "loss": 1.7485,
        "grad_norm": 4.212810039520264,
        "learning_rate": 6.092635072968357e-05,
        "epoch": 1.7530666666666668,
        "step": 13148
    },
    {
        "loss": 0.5771,
        "grad_norm": 2.1723341941833496,
        "learning_rate": 6.086842503374406e-05,
        "epoch": 1.7532,
        "step": 13149
    },
    {
        "loss": 2.1505,
        "grad_norm": 4.45450496673584,
        "learning_rate": 6.081051483795457e-05,
        "epoch": 1.7533333333333334,
        "step": 13150
    },
    {
        "loss": 2.4737,
        "grad_norm": 4.111488342285156,
        "learning_rate": 6.075262016525385e-05,
        "epoch": 1.7534666666666667,
        "step": 13151
    },
    {
        "loss": 1.8038,
        "grad_norm": 3.320807695388794,
        "learning_rate": 6.0694741038573886e-05,
        "epoch": 1.7536,
        "step": 13152
    },
    {
        "loss": 2.0007,
        "grad_norm": 4.090437412261963,
        "learning_rate": 6.063687748084117e-05,
        "epoch": 1.7537333333333334,
        "step": 13153
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.673368453979492,
        "learning_rate": 6.057902951497514e-05,
        "epoch": 1.7538666666666667,
        "step": 13154
    },
    {
        "loss": 2.4942,
        "grad_norm": 3.365793228149414,
        "learning_rate": 6.0521197163889875e-05,
        "epoch": 1.754,
        "step": 13155
    },
    {
        "loss": 1.7747,
        "grad_norm": 7.338737487792969,
        "learning_rate": 6.046338045049304e-05,
        "epoch": 1.7541333333333333,
        "step": 13156
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.5222620964050293,
        "learning_rate": 6.040557939768594e-05,
        "epoch": 1.7542666666666666,
        "step": 13157
    },
    {
        "loss": 1.51,
        "grad_norm": 3.3027236461639404,
        "learning_rate": 6.034779402836375e-05,
        "epoch": 1.7544,
        "step": 13158
    },
    {
        "loss": 2.0111,
        "grad_norm": 4.110459804534912,
        "learning_rate": 6.029002436541533e-05,
        "epoch": 1.7545333333333333,
        "step": 13159
    },
    {
        "loss": 1.2108,
        "grad_norm": 2.8793885707855225,
        "learning_rate": 6.0232270431723695e-05,
        "epoch": 1.7546666666666666,
        "step": 13160
    },
    {
        "loss": 2.3546,
        "grad_norm": 4.739643573760986,
        "learning_rate": 6.017453225016525e-05,
        "epoch": 1.7548,
        "step": 13161
    },
    {
        "loss": 2.6986,
        "grad_norm": 3.846436023712158,
        "learning_rate": 6.0116809843610255e-05,
        "epoch": 1.7549333333333332,
        "step": 13162
    },
    {
        "loss": 2.2342,
        "grad_norm": 2.967108726501465,
        "learning_rate": 6.005910323492269e-05,
        "epoch": 1.7550666666666666,
        "step": 13163
    },
    {
        "loss": 1.6708,
        "grad_norm": 3.7563254833221436,
        "learning_rate": 6.0001412446960405e-05,
        "epoch": 1.7551999999999999,
        "step": 13164
    },
    {
        "loss": 1.6,
        "grad_norm": 4.317990303039551,
        "learning_rate": 5.99437375025752e-05,
        "epoch": 1.7553333333333332,
        "step": 13165
    },
    {
        "loss": 1.8259,
        "grad_norm": 3.294156551361084,
        "learning_rate": 5.9886078424611825e-05,
        "epoch": 1.7554666666666665,
        "step": 13166
    },
    {
        "loss": 2.7656,
        "grad_norm": 3.557137966156006,
        "learning_rate": 5.982843523590957e-05,
        "epoch": 1.7556,
        "step": 13167
    },
    {
        "loss": 2.556,
        "grad_norm": 3.4506964683532715,
        "learning_rate": 5.9770807959300876e-05,
        "epoch": 1.7557333333333334,
        "step": 13168
    },
    {
        "loss": 1.8075,
        "grad_norm": 3.669466257095337,
        "learning_rate": 5.9713196617612364e-05,
        "epoch": 1.7558666666666667,
        "step": 13169
    },
    {
        "loss": 0.8063,
        "grad_norm": 3.1962809562683105,
        "learning_rate": 5.965560123366396e-05,
        "epoch": 1.756,
        "step": 13170
    },
    {
        "loss": 1.9515,
        "grad_norm": 3.5224497318267822,
        "learning_rate": 5.959802183026936e-05,
        "epoch": 1.7561333333333333,
        "step": 13171
    },
    {
        "loss": 2.2113,
        "grad_norm": 4.262594223022461,
        "learning_rate": 5.954045843023583e-05,
        "epoch": 1.7562666666666666,
        "step": 13172
    },
    {
        "loss": 1.8683,
        "grad_norm": 4.670441150665283,
        "learning_rate": 5.948291105636462e-05,
        "epoch": 1.7564,
        "step": 13173
    },
    {
        "loss": 2.6829,
        "grad_norm": 2.299150228500366,
        "learning_rate": 5.942537973145064e-05,
        "epoch": 1.7565333333333333,
        "step": 13174
    },
    {
        "loss": 1.8517,
        "grad_norm": 3.3762683868408203,
        "learning_rate": 5.9367864478281734e-05,
        "epoch": 1.7566666666666668,
        "step": 13175
    },
    {
        "loss": 1.9595,
        "grad_norm": 3.2112436294555664,
        "learning_rate": 5.9310365319640295e-05,
        "epoch": 1.7568000000000001,
        "step": 13176
    },
    {
        "loss": 0.8098,
        "grad_norm": 3.875749111175537,
        "learning_rate": 5.925288227830168e-05,
        "epoch": 1.7569333333333335,
        "step": 13177
    },
    {
        "loss": 2.0115,
        "grad_norm": 3.8939590454101562,
        "learning_rate": 5.919541537703537e-05,
        "epoch": 1.7570666666666668,
        "step": 13178
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.7421669960021973,
        "learning_rate": 5.9137964638604035e-05,
        "epoch": 1.7572,
        "step": 13179
    },
    {
        "loss": 3.2699,
        "grad_norm": 3.339270830154419,
        "learning_rate": 5.9080530085764166e-05,
        "epoch": 1.7573333333333334,
        "step": 13180
    },
    {
        "loss": 2.6449,
        "grad_norm": 3.610518217086792,
        "learning_rate": 5.902311174126562e-05,
        "epoch": 1.7574666666666667,
        "step": 13181
    },
    {
        "loss": 1.99,
        "grad_norm": 5.65167760848999,
        "learning_rate": 5.8965709627852284e-05,
        "epoch": 1.7576,
        "step": 13182
    },
    {
        "loss": 2.0206,
        "grad_norm": 2.933879852294922,
        "learning_rate": 5.890832376826116e-05,
        "epoch": 1.7577333333333334,
        "step": 13183
    },
    {
        "loss": 1.7576,
        "grad_norm": 3.964290142059326,
        "learning_rate": 5.885095418522291e-05,
        "epoch": 1.7578666666666667,
        "step": 13184
    },
    {
        "loss": 1.1275,
        "grad_norm": 4.067594528198242,
        "learning_rate": 5.879360090146203e-05,
        "epoch": 1.758,
        "step": 13185
    },
    {
        "loss": 2.7071,
        "grad_norm": 3.486509084701538,
        "learning_rate": 5.8736263939696245e-05,
        "epoch": 1.7581333333333333,
        "step": 13186
    },
    {
        "loss": 2.255,
        "grad_norm": 3.2069456577301025,
        "learning_rate": 5.867894332263677e-05,
        "epoch": 1.7582666666666666,
        "step": 13187
    },
    {
        "loss": 2.1721,
        "grad_norm": 3.3766369819641113,
        "learning_rate": 5.862163907298879e-05,
        "epoch": 1.7584,
        "step": 13188
    },
    {
        "loss": 2.7266,
        "grad_norm": 3.0909500122070312,
        "learning_rate": 5.8564351213450565e-05,
        "epoch": 1.7585333333333333,
        "step": 13189
    },
    {
        "loss": 1.8944,
        "grad_norm": 3.9841623306274414,
        "learning_rate": 5.8507079766714014e-05,
        "epoch": 1.7586666666666666,
        "step": 13190
    },
    {
        "loss": 2.8217,
        "grad_norm": 3.8841652870178223,
        "learning_rate": 5.844982475546441e-05,
        "epoch": 1.7588,
        "step": 13191
    },
    {
        "loss": 2.193,
        "grad_norm": 2.5424745082855225,
        "learning_rate": 5.839258620238095e-05,
        "epoch": 1.7589333333333332,
        "step": 13192
    },
    {
        "loss": 1.7629,
        "grad_norm": 4.095339775085449,
        "learning_rate": 5.833536413013573e-05,
        "epoch": 1.7590666666666666,
        "step": 13193
    },
    {
        "loss": 2.995,
        "grad_norm": 4.400768280029297,
        "learning_rate": 5.8278158561394856e-05,
        "epoch": 1.7591999999999999,
        "step": 13194
    },
    {
        "loss": 2.1063,
        "grad_norm": 3.8477253913879395,
        "learning_rate": 5.822096951881758e-05,
        "epoch": 1.7593333333333332,
        "step": 13195
    },
    {
        "loss": 1.8984,
        "grad_norm": 3.046971082687378,
        "learning_rate": 5.8163797025056473e-05,
        "epoch": 1.7594666666666665,
        "step": 13196
    },
    {
        "loss": 1.5074,
        "grad_norm": 2.858353853225708,
        "learning_rate": 5.810664110275804e-05,
        "epoch": 1.7596,
        "step": 13197
    },
    {
        "loss": 1.8096,
        "grad_norm": 4.466599941253662,
        "learning_rate": 5.8049501774561855e-05,
        "epoch": 1.7597333333333334,
        "step": 13198
    },
    {
        "loss": 1.53,
        "grad_norm": 2.9753191471099854,
        "learning_rate": 5.799237906310091e-05,
        "epoch": 1.7598666666666667,
        "step": 13199
    },
    {
        "loss": 2.2311,
        "grad_norm": 3.6804299354553223,
        "learning_rate": 5.793527299100166e-05,
        "epoch": 1.76,
        "step": 13200
    },
    {
        "loss": 2.7425,
        "grad_norm": 3.1207358837127686,
        "learning_rate": 5.787818358088425e-05,
        "epoch": 1.7601333333333333,
        "step": 13201
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.1982502937316895,
        "learning_rate": 5.782111085536168e-05,
        "epoch": 1.7602666666666666,
        "step": 13202
    },
    {
        "loss": 2.6878,
        "grad_norm": 3.5990381240844727,
        "learning_rate": 5.776405483704113e-05,
        "epoch": 1.7604,
        "step": 13203
    },
    {
        "loss": 2.0823,
        "grad_norm": 2.351621627807617,
        "learning_rate": 5.770701554852207e-05,
        "epoch": 1.7605333333333333,
        "step": 13204
    },
    {
        "loss": 2.3803,
        "grad_norm": 2.458024501800537,
        "learning_rate": 5.7649993012398285e-05,
        "epoch": 1.7606666666666668,
        "step": 13205
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.9257936477661133,
        "learning_rate": 5.759298725125667e-05,
        "epoch": 1.7608000000000001,
        "step": 13206
    },
    {
        "loss": 1.6465,
        "grad_norm": 3.1561357975006104,
        "learning_rate": 5.753599828767735e-05,
        "epoch": 1.7609333333333335,
        "step": 13207
    },
    {
        "loss": 2.7858,
        "grad_norm": 2.7655062675476074,
        "learning_rate": 5.7479026144233774e-05,
        "epoch": 1.7610666666666668,
        "step": 13208
    },
    {
        "loss": 1.6586,
        "grad_norm": 3.5507383346557617,
        "learning_rate": 5.7422070843492734e-05,
        "epoch": 1.7612,
        "step": 13209
    },
    {
        "loss": 2.3013,
        "grad_norm": 3.444277286529541,
        "learning_rate": 5.7365132408014597e-05,
        "epoch": 1.7613333333333334,
        "step": 13210
    },
    {
        "loss": 2.3414,
        "grad_norm": 3.922652006149292,
        "learning_rate": 5.7308210860352786e-05,
        "epoch": 1.7614666666666667,
        "step": 13211
    },
    {
        "loss": 1.9408,
        "grad_norm": 3.3483822345733643,
        "learning_rate": 5.725130622305417e-05,
        "epoch": 1.7616,
        "step": 13212
    },
    {
        "loss": 1.6172,
        "grad_norm": 4.423121929168701,
        "learning_rate": 5.719441851865864e-05,
        "epoch": 1.7617333333333334,
        "step": 13213
    },
    {
        "loss": 0.5005,
        "grad_norm": 2.2155556678771973,
        "learning_rate": 5.7137547769699817e-05,
        "epoch": 1.7618666666666667,
        "step": 13214
    },
    {
        "loss": 1.5498,
        "grad_norm": 4.598906993865967,
        "learning_rate": 5.7080693998704515e-05,
        "epoch": 1.762,
        "step": 13215
    },
    {
        "loss": 2.2563,
        "grad_norm": 3.3083183765411377,
        "learning_rate": 5.7023857228192536e-05,
        "epoch": 1.7621333333333333,
        "step": 13216
    },
    {
        "loss": 2.9219,
        "grad_norm": 3.345919370651245,
        "learning_rate": 5.696703748067713e-05,
        "epoch": 1.7622666666666666,
        "step": 13217
    },
    {
        "loss": 2.0489,
        "grad_norm": 4.620850563049316,
        "learning_rate": 5.691023477866475e-05,
        "epoch": 1.7624,
        "step": 13218
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.8130252361297607,
        "learning_rate": 5.685344914465527e-05,
        "epoch": 1.7625333333333333,
        "step": 13219
    },
    {
        "loss": 2.4536,
        "grad_norm": 5.962169170379639,
        "learning_rate": 5.679668060114165e-05,
        "epoch": 1.7626666666666666,
        "step": 13220
    },
    {
        "loss": 1.9558,
        "grad_norm": 6.126753807067871,
        "learning_rate": 5.6739929170609994e-05,
        "epoch": 1.7628,
        "step": 13221
    },
    {
        "loss": 1.3098,
        "grad_norm": 4.606744766235352,
        "learning_rate": 5.66831948755397e-05,
        "epoch": 1.7629333333333332,
        "step": 13222
    },
    {
        "loss": 2.4676,
        "grad_norm": 3.5231640338897705,
        "learning_rate": 5.662647773840355e-05,
        "epoch": 1.7630666666666666,
        "step": 13223
    },
    {
        "loss": 2.2712,
        "grad_norm": 4.286745548248291,
        "learning_rate": 5.656977778166754e-05,
        "epoch": 1.7631999999999999,
        "step": 13224
    },
    {
        "loss": 2.0052,
        "grad_norm": 5.1362481117248535,
        "learning_rate": 5.6513095027790317e-05,
        "epoch": 1.7633333333333332,
        "step": 13225
    },
    {
        "loss": 1.8302,
        "grad_norm": 3.573076009750366,
        "learning_rate": 5.645642949922449e-05,
        "epoch": 1.7634666666666665,
        "step": 13226
    },
    {
        "loss": 2.0106,
        "grad_norm": 3.1567351818084717,
        "learning_rate": 5.639978121841514e-05,
        "epoch": 1.7635999999999998,
        "step": 13227
    },
    {
        "loss": 2.8545,
        "grad_norm": 4.100234031677246,
        "learning_rate": 5.6343150207801146e-05,
        "epoch": 1.7637333333333334,
        "step": 13228
    },
    {
        "loss": 2.3376,
        "grad_norm": 4.3459086418151855,
        "learning_rate": 5.6286536489814145e-05,
        "epoch": 1.7638666666666667,
        "step": 13229
    },
    {
        "loss": 1.2457,
        "grad_norm": 4.270704746246338,
        "learning_rate": 5.622994008687894e-05,
        "epoch": 1.764,
        "step": 13230
    },
    {
        "loss": 1.0046,
        "grad_norm": 3.250156879425049,
        "learning_rate": 5.617336102141351e-05,
        "epoch": 1.7641333333333333,
        "step": 13231
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.0898964405059814,
        "learning_rate": 5.611679931582926e-05,
        "epoch": 1.7642666666666666,
        "step": 13232
    },
    {
        "loss": 1.9311,
        "grad_norm": 4.53161096572876,
        "learning_rate": 5.6060254992530294e-05,
        "epoch": 1.7644,
        "step": 13233
    },
    {
        "loss": 1.7495,
        "grad_norm": 3.0610475540161133,
        "learning_rate": 5.600372807391396e-05,
        "epoch": 1.7645333333333333,
        "step": 13234
    },
    {
        "loss": 2.4828,
        "grad_norm": 3.885723114013672,
        "learning_rate": 5.5947218582370945e-05,
        "epoch": 1.7646666666666668,
        "step": 13235
    },
    {
        "loss": 2.4495,
        "grad_norm": 4.665599822998047,
        "learning_rate": 5.589072654028462e-05,
        "epoch": 1.7648000000000001,
        "step": 13236
    },
    {
        "loss": 2.2117,
        "grad_norm": 3.035799741744995,
        "learning_rate": 5.583425197003198e-05,
        "epoch": 1.7649333333333335,
        "step": 13237
    },
    {
        "loss": 1.8018,
        "grad_norm": 2.9982895851135254,
        "learning_rate": 5.5777794893982584e-05,
        "epoch": 1.7650666666666668,
        "step": 13238
    },
    {
        "loss": 2.293,
        "grad_norm": 4.128312110900879,
        "learning_rate": 5.5721355334499316e-05,
        "epoch": 1.7652,
        "step": 13239
    },
    {
        "loss": 1.0696,
        "grad_norm": 4.395695686340332,
        "learning_rate": 5.566493331393798e-05,
        "epoch": 1.7653333333333334,
        "step": 13240
    },
    {
        "loss": 2.4462,
        "grad_norm": 4.525145053863525,
        "learning_rate": 5.560852885464774e-05,
        "epoch": 1.7654666666666667,
        "step": 13241
    },
    {
        "loss": 2.2658,
        "grad_norm": 3.8901209831237793,
        "learning_rate": 5.55521419789705e-05,
        "epoch": 1.7656,
        "step": 13242
    },
    {
        "loss": 1.2217,
        "grad_norm": 6.724936008453369,
        "learning_rate": 5.549577270924111e-05,
        "epoch": 1.7657333333333334,
        "step": 13243
    },
    {
        "loss": 2.5459,
        "grad_norm": 2.225457191467285,
        "learning_rate": 5.5439421067787925e-05,
        "epoch": 1.7658666666666667,
        "step": 13244
    },
    {
        "loss": 2.9443,
        "grad_norm": 1.2183791399002075,
        "learning_rate": 5.5383087076931916e-05,
        "epoch": 1.766,
        "step": 13245
    },
    {
        "loss": 2.5018,
        "grad_norm": 3.1639163494110107,
        "learning_rate": 5.532677075898699e-05,
        "epoch": 1.7661333333333333,
        "step": 13246
    },
    {
        "loss": 1.8693,
        "grad_norm": 4.712474822998047,
        "learning_rate": 5.527047213626051e-05,
        "epoch": 1.7662666666666667,
        "step": 13247
    },
    {
        "loss": 2.7422,
        "grad_norm": 2.5666701793670654,
        "learning_rate": 5.521419123105247e-05,
        "epoch": 1.7664,
        "step": 13248
    },
    {
        "loss": 0.852,
        "grad_norm": 5.361706733703613,
        "learning_rate": 5.5157928065655854e-05,
        "epoch": 1.7665333333333333,
        "step": 13249
    },
    {
        "loss": 2.1283,
        "grad_norm": 4.982020854949951,
        "learning_rate": 5.51016826623566e-05,
        "epoch": 1.7666666666666666,
        "step": 13250
    },
    {
        "loss": 2.3713,
        "grad_norm": 3.3113608360290527,
        "learning_rate": 5.504545504343399e-05,
        "epoch": 1.7668,
        "step": 13251
    },
    {
        "loss": 2.0098,
        "grad_norm": 3.12418270111084,
        "learning_rate": 5.498924523115965e-05,
        "epoch": 1.7669333333333332,
        "step": 13252
    },
    {
        "loss": 1.7802,
        "grad_norm": 2.8923187255859375,
        "learning_rate": 5.493305324779877e-05,
        "epoch": 1.7670666666666666,
        "step": 13253
    },
    {
        "loss": 2.0782,
        "grad_norm": 3.308600425720215,
        "learning_rate": 5.48768791156091e-05,
        "epoch": 1.7671999999999999,
        "step": 13254
    },
    {
        "loss": 2.1911,
        "grad_norm": 2.9883434772491455,
        "learning_rate": 5.482072285684121e-05,
        "epoch": 1.7673333333333332,
        "step": 13255
    },
    {
        "loss": 1.5818,
        "grad_norm": 4.328634738922119,
        "learning_rate": 5.476458449373909e-05,
        "epoch": 1.7674666666666665,
        "step": 13256
    },
    {
        "loss": 2.6458,
        "grad_norm": 3.513678550720215,
        "learning_rate": 5.4708464048539157e-05,
        "epoch": 1.7675999999999998,
        "step": 13257
    },
    {
        "loss": 2.6743,
        "grad_norm": 4.617169380187988,
        "learning_rate": 5.465236154347094e-05,
        "epoch": 1.7677333333333334,
        "step": 13258
    },
    {
        "loss": 2.1008,
        "grad_norm": 3.482139825820923,
        "learning_rate": 5.4596277000756754e-05,
        "epoch": 1.7678666666666667,
        "step": 13259
    },
    {
        "loss": 2.21,
        "grad_norm": 3.222008466720581,
        "learning_rate": 5.4540210442612036e-05,
        "epoch": 1.768,
        "step": 13260
    },
    {
        "loss": 0.8435,
        "grad_norm": 5.5784454345703125,
        "learning_rate": 5.448416189124474e-05,
        "epoch": 1.7681333333333333,
        "step": 13261
    },
    {
        "loss": 1.7017,
        "grad_norm": 3.5771734714508057,
        "learning_rate": 5.442813136885629e-05,
        "epoch": 1.7682666666666667,
        "step": 13262
    },
    {
        "loss": 1.0567,
        "grad_norm": 4.447531223297119,
        "learning_rate": 5.4372118897640025e-05,
        "epoch": 1.7684,
        "step": 13263
    },
    {
        "loss": 1.1119,
        "grad_norm": 3.8940892219543457,
        "learning_rate": 5.431612449978292e-05,
        "epoch": 1.7685333333333333,
        "step": 13264
    },
    {
        "loss": 2.039,
        "grad_norm": 4.216506004333496,
        "learning_rate": 5.4260148197464736e-05,
        "epoch": 1.7686666666666668,
        "step": 13265
    },
    {
        "loss": 1.4715,
        "grad_norm": 5.295482158660889,
        "learning_rate": 5.420419001285766e-05,
        "epoch": 1.7688000000000001,
        "step": 13266
    },
    {
        "loss": 2.3471,
        "grad_norm": 3.522674798965454,
        "learning_rate": 5.4148249968127e-05,
        "epoch": 1.7689333333333335,
        "step": 13267
    },
    {
        "loss": 1.9154,
        "grad_norm": 3.767303466796875,
        "learning_rate": 5.409232808543061e-05,
        "epoch": 1.7690666666666668,
        "step": 13268
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.612053632736206,
        "learning_rate": 5.4036424386919604e-05,
        "epoch": 1.7692,
        "step": 13269
    },
    {
        "loss": 2.5441,
        "grad_norm": 2.1855883598327637,
        "learning_rate": 5.398053889473755e-05,
        "epoch": 1.7693333333333334,
        "step": 13270
    },
    {
        "loss": 1.5793,
        "grad_norm": 5.422306537628174,
        "learning_rate": 5.392467163102082e-05,
        "epoch": 1.7694666666666667,
        "step": 13271
    },
    {
        "loss": 1.1098,
        "grad_norm": 4.064785480499268,
        "learning_rate": 5.386882261789854e-05,
        "epoch": 1.7696,
        "step": 13272
    },
    {
        "loss": 1.0098,
        "grad_norm": 3.9284558296203613,
        "learning_rate": 5.3812991877492845e-05,
        "epoch": 1.7697333333333334,
        "step": 13273
    },
    {
        "loss": 0.8495,
        "grad_norm": 4.756285667419434,
        "learning_rate": 5.3757179431918515e-05,
        "epoch": 1.7698666666666667,
        "step": 13274
    },
    {
        "loss": 2.274,
        "grad_norm": 3.18627667427063,
        "learning_rate": 5.370138530328299e-05,
        "epoch": 1.77,
        "step": 13275
    },
    {
        "loss": 0.7621,
        "grad_norm": 3.6450765132904053,
        "learning_rate": 5.364560951368654e-05,
        "epoch": 1.7701333333333333,
        "step": 13276
    },
    {
        "loss": 1.8978,
        "grad_norm": 3.420720338821411,
        "learning_rate": 5.358985208522196e-05,
        "epoch": 1.7702666666666667,
        "step": 13277
    },
    {
        "loss": 2.267,
        "grad_norm": 3.3561012744903564,
        "learning_rate": 5.353411303997522e-05,
        "epoch": 1.7704,
        "step": 13278
    },
    {
        "loss": 1.4179,
        "grad_norm": 4.766966342926025,
        "learning_rate": 5.3478392400024676e-05,
        "epoch": 1.7705333333333333,
        "step": 13279
    },
    {
        "loss": 2.4572,
        "grad_norm": 4.417152404785156,
        "learning_rate": 5.342269018744137e-05,
        "epoch": 1.7706666666666666,
        "step": 13280
    },
    {
        "loss": 1.4925,
        "grad_norm": 3.265780210494995,
        "learning_rate": 5.3367006424289126e-05,
        "epoch": 1.7708,
        "step": 13281
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.7698471546173096,
        "learning_rate": 5.331134113262448e-05,
        "epoch": 1.7709333333333332,
        "step": 13282
    },
    {
        "loss": 2.0885,
        "grad_norm": 3.7501418590545654,
        "learning_rate": 5.325569433449695e-05,
        "epoch": 1.7710666666666666,
        "step": 13283
    },
    {
        "loss": 1.7054,
        "grad_norm": 4.210559844970703,
        "learning_rate": 5.3200066051947974e-05,
        "epoch": 1.7711999999999999,
        "step": 13284
    },
    {
        "loss": 2.1526,
        "grad_norm": 4.066940784454346,
        "learning_rate": 5.314445630701237e-05,
        "epoch": 1.7713333333333332,
        "step": 13285
    },
    {
        "loss": 2.3613,
        "grad_norm": 2.6362764835357666,
        "learning_rate": 5.308886512171717e-05,
        "epoch": 1.7714666666666665,
        "step": 13286
    },
    {
        "loss": 2.465,
        "grad_norm": 5.545718669891357,
        "learning_rate": 5.303329251808249e-05,
        "epoch": 1.7715999999999998,
        "step": 13287
    },
    {
        "loss": 2.1345,
        "grad_norm": 3.1485958099365234,
        "learning_rate": 5.2977738518120645e-05,
        "epoch": 1.7717333333333334,
        "step": 13288
    },
    {
        "loss": 0.5728,
        "grad_norm": 2.6486928462982178,
        "learning_rate": 5.292220314383685e-05,
        "epoch": 1.7718666666666667,
        "step": 13289
    },
    {
        "loss": 2.3404,
        "grad_norm": 3.99161696434021,
        "learning_rate": 5.286668641722863e-05,
        "epoch": 1.772,
        "step": 13290
    },
    {
        "loss": 2.0916,
        "grad_norm": 3.842205286026001,
        "learning_rate": 5.281118836028664e-05,
        "epoch": 1.7721333333333333,
        "step": 13291
    },
    {
        "loss": 1.1183,
        "grad_norm": 3.798358201980591,
        "learning_rate": 5.275570899499377e-05,
        "epoch": 1.7722666666666667,
        "step": 13292
    },
    {
        "loss": 2.1829,
        "grad_norm": 4.276244640350342,
        "learning_rate": 5.270024834332539e-05,
        "epoch": 1.7724,
        "step": 13293
    },
    {
        "loss": 2.665,
        "grad_norm": 4.389557361602783,
        "learning_rate": 5.2644806427249935e-05,
        "epoch": 1.7725333333333333,
        "step": 13294
    },
    {
        "loss": 1.1204,
        "grad_norm": 6.070012092590332,
        "learning_rate": 5.258938326872793e-05,
        "epoch": 1.7726666666666666,
        "step": 13295
    },
    {
        "loss": 1.366,
        "grad_norm": 3.522305727005005,
        "learning_rate": 5.253397888971285e-05,
        "epoch": 1.7728000000000002,
        "step": 13296
    },
    {
        "loss": 2.1377,
        "grad_norm": 3.285782814025879,
        "learning_rate": 5.247859331215046e-05,
        "epoch": 1.7729333333333335,
        "step": 13297
    },
    {
        "loss": 2.5405,
        "grad_norm": 3.704569101333618,
        "learning_rate": 5.242322655797922e-05,
        "epoch": 1.7730666666666668,
        "step": 13298
    },
    {
        "loss": 1.7294,
        "grad_norm": 3.4292352199554443,
        "learning_rate": 5.236787864912991e-05,
        "epoch": 1.7732,
        "step": 13299
    },
    {
        "loss": 2.2316,
        "grad_norm": 5.041680812835693,
        "learning_rate": 5.2312549607526275e-05,
        "epoch": 1.7733333333333334,
        "step": 13300
    },
    {
        "loss": 2.5729,
        "grad_norm": 3.9811480045318604,
        "learning_rate": 5.225723945508427e-05,
        "epoch": 1.7734666666666667,
        "step": 13301
    },
    {
        "loss": 2.4456,
        "grad_norm": 4.136440753936768,
        "learning_rate": 5.220194821371225e-05,
        "epoch": 1.7736,
        "step": 13302
    },
    {
        "loss": 1.8633,
        "grad_norm": 5.182337284088135,
        "learning_rate": 5.214667590531156e-05,
        "epoch": 1.7737333333333334,
        "step": 13303
    },
    {
        "loss": 2.2666,
        "grad_norm": 2.9115865230560303,
        "learning_rate": 5.209142255177557e-05,
        "epoch": 1.7738666666666667,
        "step": 13304
    },
    {
        "loss": 1.6395,
        "grad_norm": 3.101766586303711,
        "learning_rate": 5.203618817499022e-05,
        "epoch": 1.774,
        "step": 13305
    },
    {
        "loss": 1.8458,
        "grad_norm": 3.2900240421295166,
        "learning_rate": 5.198097279683434e-05,
        "epoch": 1.7741333333333333,
        "step": 13306
    },
    {
        "loss": 2.4131,
        "grad_norm": 4.367088794708252,
        "learning_rate": 5.19257764391787e-05,
        "epoch": 1.7742666666666667,
        "step": 13307
    },
    {
        "loss": 1.3028,
        "grad_norm": 3.656721830368042,
        "learning_rate": 5.187059912388683e-05,
        "epoch": 1.7744,
        "step": 13308
    },
    {
        "loss": 2.168,
        "grad_norm": 3.0569002628326416,
        "learning_rate": 5.181544087281456e-05,
        "epoch": 1.7745333333333333,
        "step": 13309
    },
    {
        "loss": 1.706,
        "grad_norm": 3.741929769515991,
        "learning_rate": 5.1760301707810434e-05,
        "epoch": 1.7746666666666666,
        "step": 13310
    },
    {
        "loss": 1.4628,
        "grad_norm": 3.284250020980835,
        "learning_rate": 5.170518165071504e-05,
        "epoch": 1.7748,
        "step": 13311
    },
    {
        "loss": 2.1882,
        "grad_norm": 3.534731388092041,
        "learning_rate": 5.165008072336204e-05,
        "epoch": 1.7749333333333333,
        "step": 13312
    },
    {
        "loss": 1.6967,
        "grad_norm": 4.663723945617676,
        "learning_rate": 5.1594998947576535e-05,
        "epoch": 1.7750666666666666,
        "step": 13313
    },
    {
        "loss": 2.8504,
        "grad_norm": 5.136022567749023,
        "learning_rate": 5.1539936345176886e-05,
        "epoch": 1.7752,
        "step": 13314
    },
    {
        "loss": 2.4616,
        "grad_norm": 4.852818489074707,
        "learning_rate": 5.1484892937973735e-05,
        "epoch": 1.7753333333333332,
        "step": 13315
    },
    {
        "loss": 2.7645,
        "grad_norm": 4.177610874176025,
        "learning_rate": 5.142986874776976e-05,
        "epoch": 1.7754666666666665,
        "step": 13316
    },
    {
        "loss": 1.9919,
        "grad_norm": 3.746342658996582,
        "learning_rate": 5.137486379636032e-05,
        "epoch": 1.7755999999999998,
        "step": 13317
    },
    {
        "loss": 2.3724,
        "grad_norm": 3.821450710296631,
        "learning_rate": 5.131987810553285e-05,
        "epoch": 1.7757333333333334,
        "step": 13318
    },
    {
        "loss": 2.4426,
        "grad_norm": 4.852906227111816,
        "learning_rate": 5.126491169706764e-05,
        "epoch": 1.7758666666666667,
        "step": 13319
    },
    {
        "loss": 2.1506,
        "grad_norm": 4.4891438484191895,
        "learning_rate": 5.1209964592736906e-05,
        "epoch": 1.776,
        "step": 13320
    },
    {
        "loss": 2.3647,
        "grad_norm": 3.695499897003174,
        "learning_rate": 5.1155036814305646e-05,
        "epoch": 1.7761333333333333,
        "step": 13321
    },
    {
        "loss": 2.8427,
        "grad_norm": 3.4362852573394775,
        "learning_rate": 5.1100128383530486e-05,
        "epoch": 1.7762666666666667,
        "step": 13322
    },
    {
        "loss": 2.9081,
        "grad_norm": 5.411068439483643,
        "learning_rate": 5.1045239322161145e-05,
        "epoch": 1.7764,
        "step": 13323
    },
    {
        "loss": 2.4187,
        "grad_norm": 3.915987014770508,
        "learning_rate": 5.099036965193939e-05,
        "epoch": 1.7765333333333333,
        "step": 13324
    },
    {
        "loss": 1.2146,
        "grad_norm": 5.223258018493652,
        "learning_rate": 5.093551939459923e-05,
        "epoch": 1.7766666666666666,
        "step": 13325
    },
    {
        "loss": 2.5333,
        "grad_norm": 3.44655179977417,
        "learning_rate": 5.088068857186705e-05,
        "epoch": 1.7768000000000002,
        "step": 13326
    },
    {
        "loss": 0.8534,
        "grad_norm": 4.1927080154418945,
        "learning_rate": 5.0825877205461334e-05,
        "epoch": 1.7769333333333335,
        "step": 13327
    },
    {
        "loss": 2.0481,
        "grad_norm": 3.231480836868286,
        "learning_rate": 5.077108531709338e-05,
        "epoch": 1.7770666666666668,
        "step": 13328
    },
    {
        "loss": 2.1679,
        "grad_norm": 2.776298761367798,
        "learning_rate": 5.071631292846623e-05,
        "epoch": 1.7772000000000001,
        "step": 13329
    },
    {
        "loss": 2.8953,
        "grad_norm": 3.610614538192749,
        "learning_rate": 5.066156006127545e-05,
        "epoch": 1.7773333333333334,
        "step": 13330
    },
    {
        "loss": 1.9014,
        "grad_norm": 2.985351324081421,
        "learning_rate": 5.060682673720878e-05,
        "epoch": 1.7774666666666668,
        "step": 13331
    },
    {
        "loss": 1.1203,
        "grad_norm": 4.206031322479248,
        "learning_rate": 5.055211297794631e-05,
        "epoch": 1.7776,
        "step": 13332
    },
    {
        "loss": 2.6191,
        "grad_norm": 2.9992048740386963,
        "learning_rate": 5.049741880516059e-05,
        "epoch": 1.7777333333333334,
        "step": 13333
    },
    {
        "loss": 2.2553,
        "grad_norm": 6.221587657928467,
        "learning_rate": 5.044274424051578e-05,
        "epoch": 1.7778666666666667,
        "step": 13334
    },
    {
        "loss": 1.2397,
        "grad_norm": 4.041728496551514,
        "learning_rate": 5.038808930566892e-05,
        "epoch": 1.778,
        "step": 13335
    },
    {
        "loss": 1.733,
        "grad_norm": 5.343059062957764,
        "learning_rate": 5.033345402226881e-05,
        "epoch": 1.7781333333333333,
        "step": 13336
    },
    {
        "loss": 2.9736,
        "grad_norm": 4.319118022918701,
        "learning_rate": 5.0278838411956955e-05,
        "epoch": 1.7782666666666667,
        "step": 13337
    },
    {
        "loss": 1.6162,
        "grad_norm": 6.548061370849609,
        "learning_rate": 5.022424249636658e-05,
        "epoch": 1.7784,
        "step": 13338
    },
    {
        "loss": 2.0518,
        "grad_norm": 4.6322736740112305,
        "learning_rate": 5.0169666297123405e-05,
        "epoch": 1.7785333333333333,
        "step": 13339
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.577357053756714,
        "learning_rate": 5.011510983584504e-05,
        "epoch": 1.7786666666666666,
        "step": 13340
    },
    {
        "loss": 1.1851,
        "grad_norm": 3.7200326919555664,
        "learning_rate": 5.006057313414165e-05,
        "epoch": 1.7788,
        "step": 13341
    },
    {
        "loss": 2.5804,
        "grad_norm": 3.698975086212158,
        "learning_rate": 5.000605621361568e-05,
        "epoch": 1.7789333333333333,
        "step": 13342
    },
    {
        "loss": 2.595,
        "grad_norm": 3.5122034549713135,
        "learning_rate": 4.995155909586092e-05,
        "epoch": 1.7790666666666666,
        "step": 13343
    },
    {
        "loss": 1.592,
        "grad_norm": 4.692622184753418,
        "learning_rate": 4.9897081802464264e-05,
        "epoch": 1.7792,
        "step": 13344
    },
    {
        "loss": 1.8485,
        "grad_norm": 3.784193754196167,
        "learning_rate": 4.984262435500414e-05,
        "epoch": 1.7793333333333332,
        "step": 13345
    },
    {
        "loss": 1.8182,
        "grad_norm": 4.04149055480957,
        "learning_rate": 4.978818677505149e-05,
        "epoch": 1.7794666666666665,
        "step": 13346
    },
    {
        "loss": 1.8532,
        "grad_norm": 3.460819959640503,
        "learning_rate": 4.973376908416916e-05,
        "epoch": 1.7795999999999998,
        "step": 13347
    },
    {
        "loss": 1.853,
        "grad_norm": 5.087231159210205,
        "learning_rate": 4.9679371303912214e-05,
        "epoch": 1.7797333333333332,
        "step": 13348
    },
    {
        "loss": 2.2539,
        "grad_norm": 3.029001474380493,
        "learning_rate": 4.96249934558276e-05,
        "epoch": 1.7798666666666667,
        "step": 13349
    },
    {
        "loss": 2.5924,
        "grad_norm": 2.6505074501037598,
        "learning_rate": 4.957063556145487e-05,
        "epoch": 1.78,
        "step": 13350
    },
    {
        "loss": 2.0673,
        "grad_norm": 3.0653138160705566,
        "learning_rate": 4.951629764232521e-05,
        "epoch": 1.7801333333333333,
        "step": 13351
    },
    {
        "loss": 2.2461,
        "grad_norm": 3.4253878593444824,
        "learning_rate": 4.9461979719961936e-05,
        "epoch": 1.7802666666666667,
        "step": 13352
    },
    {
        "loss": 2.211,
        "grad_norm": 3.9692537784576416,
        "learning_rate": 4.940768181588087e-05,
        "epoch": 1.7804,
        "step": 13353
    },
    {
        "loss": 1.9349,
        "grad_norm": 3.9347035884857178,
        "learning_rate": 4.935340395158926e-05,
        "epoch": 1.7805333333333333,
        "step": 13354
    },
    {
        "loss": 1.9768,
        "grad_norm": 3.1756997108459473,
        "learning_rate": 4.929914614858704e-05,
        "epoch": 1.7806666666666666,
        "step": 13355
    },
    {
        "loss": 2.9014,
        "grad_norm": 4.644198417663574,
        "learning_rate": 4.9244908428365834e-05,
        "epoch": 1.7808000000000002,
        "step": 13356
    },
    {
        "loss": 2.4589,
        "grad_norm": 3.7065632343292236,
        "learning_rate": 4.919069081240929e-05,
        "epoch": 1.7809333333333335,
        "step": 13357
    },
    {
        "loss": 2.403,
        "grad_norm": 3.839614152908325,
        "learning_rate": 4.9136493322193146e-05,
        "epoch": 1.7810666666666668,
        "step": 13358
    },
    {
        "loss": 1.4234,
        "grad_norm": 3.4588875770568848,
        "learning_rate": 4.9082315979185446e-05,
        "epoch": 1.7812000000000001,
        "step": 13359
    },
    {
        "loss": 2.5794,
        "grad_norm": 2.705415964126587,
        "learning_rate": 4.902815880484586e-05,
        "epoch": 1.7813333333333334,
        "step": 13360
    },
    {
        "loss": 2.2089,
        "grad_norm": 3.9330899715423584,
        "learning_rate": 4.8974021820626126e-05,
        "epoch": 1.7814666666666668,
        "step": 13361
    },
    {
        "loss": 2.7697,
        "grad_norm": 3.302588939666748,
        "learning_rate": 4.891990504797038e-05,
        "epoch": 1.7816,
        "step": 13362
    },
    {
        "loss": 1.2207,
        "grad_norm": 4.861401557922363,
        "learning_rate": 4.886580850831426e-05,
        "epoch": 1.7817333333333334,
        "step": 13363
    },
    {
        "loss": 1.9433,
        "grad_norm": 3.5831003189086914,
        "learning_rate": 4.881173222308554e-05,
        "epoch": 1.7818666666666667,
        "step": 13364
    },
    {
        "loss": 1.51,
        "grad_norm": 4.102545261383057,
        "learning_rate": 4.875767621370419e-05,
        "epoch": 1.782,
        "step": 13365
    },
    {
        "loss": 2.1811,
        "grad_norm": 3.5666356086730957,
        "learning_rate": 4.8703640501581885e-05,
        "epoch": 1.7821333333333333,
        "step": 13366
    },
    {
        "loss": 0.7929,
        "grad_norm": 3.1149656772613525,
        "learning_rate": 4.864962510812241e-05,
        "epoch": 1.7822666666666667,
        "step": 13367
    },
    {
        "loss": 1.516,
        "grad_norm": 6.207836151123047,
        "learning_rate": 4.8595630054721244e-05,
        "epoch": 1.7824,
        "step": 13368
    },
    {
        "loss": 1.1358,
        "grad_norm": 4.4168219566345215,
        "learning_rate": 4.8541655362766295e-05,
        "epoch": 1.7825333333333333,
        "step": 13369
    },
    {
        "loss": 2.2656,
        "grad_norm": 2.601419687271118,
        "learning_rate": 4.84877010536369e-05,
        "epoch": 1.7826666666666666,
        "step": 13370
    },
    {
        "loss": 1.0662,
        "grad_norm": 4.773478031158447,
        "learning_rate": 4.8433767148704854e-05,
        "epoch": 1.7828,
        "step": 13371
    },
    {
        "loss": 2.1648,
        "grad_norm": 2.296163558959961,
        "learning_rate": 4.837985366933311e-05,
        "epoch": 1.7829333333333333,
        "step": 13372
    },
    {
        "loss": 1.5806,
        "grad_norm": 3.524360418319702,
        "learning_rate": 4.832596063687725e-05,
        "epoch": 1.7830666666666666,
        "step": 13373
    },
    {
        "loss": 1.6181,
        "grad_norm": 4.546783447265625,
        "learning_rate": 4.827208807268455e-05,
        "epoch": 1.7832,
        "step": 13374
    },
    {
        "loss": 2.2979,
        "grad_norm": 4.662326812744141,
        "learning_rate": 4.821823599809405e-05,
        "epoch": 1.7833333333333332,
        "step": 13375
    },
    {
        "loss": 2.0272,
        "grad_norm": 2.846198081970215,
        "learning_rate": 4.816440443443667e-05,
        "epoch": 1.7834666666666665,
        "step": 13376
    },
    {
        "loss": 2.2615,
        "grad_norm": 3.5574824810028076,
        "learning_rate": 4.811059340303522e-05,
        "epoch": 1.7835999999999999,
        "step": 13377
    },
    {
        "loss": 2.358,
        "grad_norm": 3.800670623779297,
        "learning_rate": 4.805680292520468e-05,
        "epoch": 1.7837333333333332,
        "step": 13378
    },
    {
        "loss": 1.4152,
        "grad_norm": 4.7561869621276855,
        "learning_rate": 4.800303302225133e-05,
        "epoch": 1.7838666666666667,
        "step": 13379
    },
    {
        "loss": 2.4502,
        "grad_norm": 4.605703830718994,
        "learning_rate": 4.794928371547401e-05,
        "epoch": 1.784,
        "step": 13380
    },
    {
        "loss": 1.803,
        "grad_norm": 4.0766119956970215,
        "learning_rate": 4.7895555026162584e-05,
        "epoch": 1.7841333333333333,
        "step": 13381
    },
    {
        "loss": 1.9109,
        "grad_norm": 4.042793273925781,
        "learning_rate": 4.7841846975599314e-05,
        "epoch": 1.7842666666666667,
        "step": 13382
    },
    {
        "loss": 1.337,
        "grad_norm": 4.573596954345703,
        "learning_rate": 4.7788159585058286e-05,
        "epoch": 1.7844,
        "step": 13383
    },
    {
        "loss": 2.4068,
        "grad_norm": 5.508547782897949,
        "learning_rate": 4.7734492875805206e-05,
        "epoch": 1.7845333333333333,
        "step": 13384
    },
    {
        "loss": 1.5703,
        "grad_norm": 4.0332112312316895,
        "learning_rate": 4.768084686909754e-05,
        "epoch": 1.7846666666666666,
        "step": 13385
    },
    {
        "loss": 2.0358,
        "grad_norm": 3.6245994567871094,
        "learning_rate": 4.762722158618459e-05,
        "epoch": 1.7848000000000002,
        "step": 13386
    },
    {
        "loss": 2.008,
        "grad_norm": 4.349732875823975,
        "learning_rate": 4.7573617048307774e-05,
        "epoch": 1.7849333333333335,
        "step": 13387
    },
    {
        "loss": 2.0644,
        "grad_norm": 3.5594210624694824,
        "learning_rate": 4.752003327669985e-05,
        "epoch": 1.7850666666666668,
        "step": 13388
    },
    {
        "loss": 0.9926,
        "grad_norm": 3.5408127307891846,
        "learning_rate": 4.7466470292585605e-05,
        "epoch": 1.7852000000000001,
        "step": 13389
    },
    {
        "loss": 2.4449,
        "grad_norm": 2.015108585357666,
        "learning_rate": 4.7412928117181334e-05,
        "epoch": 1.7853333333333334,
        "step": 13390
    },
    {
        "loss": 2.576,
        "grad_norm": 3.263824224472046,
        "learning_rate": 4.7359406771695413e-05,
        "epoch": 1.7854666666666668,
        "step": 13391
    },
    {
        "loss": 1.5809,
        "grad_norm": 4.825216770172119,
        "learning_rate": 4.730590627732811e-05,
        "epoch": 1.7856,
        "step": 13392
    },
    {
        "loss": 2.8823,
        "grad_norm": 3.6786112785339355,
        "learning_rate": 4.725242665527061e-05,
        "epoch": 1.7857333333333334,
        "step": 13393
    },
    {
        "loss": 1.744,
        "grad_norm": 3.595456838607788,
        "learning_rate": 4.719896792670676e-05,
        "epoch": 1.7858666666666667,
        "step": 13394
    },
    {
        "loss": 1.3917,
        "grad_norm": 4.655481338500977,
        "learning_rate": 4.7145530112811554e-05,
        "epoch": 1.786,
        "step": 13395
    },
    {
        "loss": 1.3962,
        "grad_norm": 6.562224388122559,
        "learning_rate": 4.709211323475204e-05,
        "epoch": 1.7861333333333334,
        "step": 13396
    },
    {
        "loss": 2.7865,
        "grad_norm": 3.326319932937622,
        "learning_rate": 4.70387173136868e-05,
        "epoch": 1.7862666666666667,
        "step": 13397
    },
    {
        "loss": 2.0907,
        "grad_norm": 2.997417449951172,
        "learning_rate": 4.698534237076605e-05,
        "epoch": 1.7864,
        "step": 13398
    },
    {
        "loss": 2.4596,
        "grad_norm": 3.7779738903045654,
        "learning_rate": 4.693198842713171e-05,
        "epoch": 1.7865333333333333,
        "step": 13399
    },
    {
        "loss": 2.166,
        "grad_norm": 3.848060131072998,
        "learning_rate": 4.687865550391759e-05,
        "epoch": 1.7866666666666666,
        "step": 13400
    },
    {
        "loss": 1.7583,
        "grad_norm": 1.926055908203125,
        "learning_rate": 4.682534362224929e-05,
        "epoch": 1.7868,
        "step": 13401
    },
    {
        "loss": 1.8804,
        "grad_norm": 4.6821722984313965,
        "learning_rate": 4.67720528032433e-05,
        "epoch": 1.7869333333333333,
        "step": 13402
    },
    {
        "loss": 2.5819,
        "grad_norm": 5.543467044830322,
        "learning_rate": 4.6718783068008734e-05,
        "epoch": 1.7870666666666666,
        "step": 13403
    },
    {
        "loss": 0.9689,
        "grad_norm": 3.1901142597198486,
        "learning_rate": 4.6665534437645575e-05,
        "epoch": 1.7872,
        "step": 13404
    },
    {
        "loss": 0.6988,
        "grad_norm": 3.9162557125091553,
        "learning_rate": 4.6612306933246086e-05,
        "epoch": 1.7873333333333332,
        "step": 13405
    },
    {
        "loss": 2.2611,
        "grad_norm": 3.084677219390869,
        "learning_rate": 4.655910057589376e-05,
        "epoch": 1.7874666666666665,
        "step": 13406
    },
    {
        "loss": 1.5673,
        "grad_norm": 4.081221103668213,
        "learning_rate": 4.650591538666377e-05,
        "epoch": 1.7875999999999999,
        "step": 13407
    },
    {
        "loss": 1.5984,
        "grad_norm": 4.3324971199035645,
        "learning_rate": 4.645275138662284e-05,
        "epoch": 1.7877333333333332,
        "step": 13408
    },
    {
        "loss": 1.9585,
        "grad_norm": 4.102733135223389,
        "learning_rate": 4.6399608596829705e-05,
        "epoch": 1.7878666666666667,
        "step": 13409
    },
    {
        "loss": 2.1328,
        "grad_norm": 3.641111373901367,
        "learning_rate": 4.6346487038334196e-05,
        "epoch": 1.788,
        "step": 13410
    },
    {
        "loss": 3.2848,
        "grad_norm": 3.1711483001708984,
        "learning_rate": 4.6293386732177924e-05,
        "epoch": 1.7881333333333334,
        "step": 13411
    },
    {
        "loss": 2.0845,
        "grad_norm": 4.35275936126709,
        "learning_rate": 4.624030769939425e-05,
        "epoch": 1.7882666666666667,
        "step": 13412
    },
    {
        "loss": 2.3438,
        "grad_norm": 3.3406331539154053,
        "learning_rate": 4.618724996100786e-05,
        "epoch": 1.7884,
        "step": 13413
    },
    {
        "loss": 1.9213,
        "grad_norm": 1.6593208312988281,
        "learning_rate": 4.613421353803506e-05,
        "epoch": 1.7885333333333333,
        "step": 13414
    },
    {
        "loss": 2.2793,
        "grad_norm": 4.793193817138672,
        "learning_rate": 4.60811984514839e-05,
        "epoch": 1.7886666666666666,
        "step": 13415
    },
    {
        "loss": 2.9549,
        "grad_norm": 2.1502325534820557,
        "learning_rate": 4.6028204722353755e-05,
        "epoch": 1.7888,
        "step": 13416
    },
    {
        "loss": 1.5857,
        "grad_norm": 2.278865337371826,
        "learning_rate": 4.597523237163554e-05,
        "epoch": 1.7889333333333335,
        "step": 13417
    },
    {
        "loss": 1.3644,
        "grad_norm": 3.843209981918335,
        "learning_rate": 4.592228142031195e-05,
        "epoch": 1.7890666666666668,
        "step": 13418
    },
    {
        "loss": 1.5019,
        "grad_norm": 3.9794678688049316,
        "learning_rate": 4.586935188935695e-05,
        "epoch": 1.7892000000000001,
        "step": 13419
    },
    {
        "loss": 2.3295,
        "grad_norm": 2.7955524921417236,
        "learning_rate": 4.581644379973604e-05,
        "epoch": 1.7893333333333334,
        "step": 13420
    },
    {
        "loss": 1.9865,
        "grad_norm": 3.827186346054077,
        "learning_rate": 4.576355717240645e-05,
        "epoch": 1.7894666666666668,
        "step": 13421
    },
    {
        "loss": 2.6343,
        "grad_norm": 2.984680414199829,
        "learning_rate": 4.571069202831669e-05,
        "epoch": 1.7896,
        "step": 13422
    },
    {
        "loss": 2.7826,
        "grad_norm": 2.9103474617004395,
        "learning_rate": 4.5657848388406666e-05,
        "epoch": 1.7897333333333334,
        "step": 13423
    },
    {
        "loss": 2.678,
        "grad_norm": 3.4900565147399902,
        "learning_rate": 4.560502627360816e-05,
        "epoch": 1.7898666666666667,
        "step": 13424
    },
    {
        "loss": 2.6896,
        "grad_norm": 3.3291923999786377,
        "learning_rate": 4.555222570484415e-05,
        "epoch": 1.79,
        "step": 13425
    },
    {
        "loss": 2.0918,
        "grad_norm": 3.983430862426758,
        "learning_rate": 4.549944670302901e-05,
        "epoch": 1.7901333333333334,
        "step": 13426
    },
    {
        "loss": 1.6338,
        "grad_norm": 4.235113143920898,
        "learning_rate": 4.5446689289068645e-05,
        "epoch": 1.7902666666666667,
        "step": 13427
    },
    {
        "loss": 2.3117,
        "grad_norm": 2.4445040225982666,
        "learning_rate": 4.539395348386068e-05,
        "epoch": 1.7904,
        "step": 13428
    },
    {
        "loss": 2.1692,
        "grad_norm": 2.7527222633361816,
        "learning_rate": 4.5341239308293646e-05,
        "epoch": 1.7905333333333333,
        "step": 13429
    },
    {
        "loss": 1.8314,
        "grad_norm": 3.1235833168029785,
        "learning_rate": 4.528854678324823e-05,
        "epoch": 1.7906666666666666,
        "step": 13430
    },
    {
        "loss": 2.3722,
        "grad_norm": 3.032194137573242,
        "learning_rate": 4.52358759295956e-05,
        "epoch": 1.7908,
        "step": 13431
    },
    {
        "loss": 1.4814,
        "grad_norm": 5.466542720794678,
        "learning_rate": 4.5183226768199126e-05,
        "epoch": 1.7909333333333333,
        "step": 13432
    },
    {
        "loss": 2.3404,
        "grad_norm": 6.733774662017822,
        "learning_rate": 4.513059931991342e-05,
        "epoch": 1.7910666666666666,
        "step": 13433
    },
    {
        "loss": 2.5644,
        "grad_norm": 5.062298774719238,
        "learning_rate": 4.507799360558432e-05,
        "epoch": 1.7912,
        "step": 13434
    },
    {
        "loss": 2.1943,
        "grad_norm": 3.2916996479034424,
        "learning_rate": 4.502540964604907e-05,
        "epoch": 1.7913333333333332,
        "step": 13435
    },
    {
        "loss": 1.8395,
        "grad_norm": 3.620306968688965,
        "learning_rate": 4.497284746213629e-05,
        "epoch": 1.7914666666666665,
        "step": 13436
    },
    {
        "loss": 1.1561,
        "grad_norm": 3.5741286277770996,
        "learning_rate": 4.4920307074666215e-05,
        "epoch": 1.7915999999999999,
        "step": 13437
    },
    {
        "loss": 1.2008,
        "grad_norm": 4.3677215576171875,
        "learning_rate": 4.486778850445007e-05,
        "epoch": 1.7917333333333332,
        "step": 13438
    },
    {
        "loss": 2.7817,
        "grad_norm": 3.7082743644714355,
        "learning_rate": 4.481529177229101e-05,
        "epoch": 1.7918666666666667,
        "step": 13439
    },
    {
        "loss": 2.7471,
        "grad_norm": 3.534487724304199,
        "learning_rate": 4.476281689898267e-05,
        "epoch": 1.792,
        "step": 13440
    },
    {
        "loss": 2.2395,
        "grad_norm": 3.9667274951934814,
        "learning_rate": 4.471036390531077e-05,
        "epoch": 1.7921333333333334,
        "step": 13441
    },
    {
        "loss": 2.1913,
        "grad_norm": 3.5078306198120117,
        "learning_rate": 4.4657932812052274e-05,
        "epoch": 1.7922666666666667,
        "step": 13442
    },
    {
        "loss": 1.2793,
        "grad_norm": 4.641580104827881,
        "learning_rate": 4.460552363997511e-05,
        "epoch": 1.7924,
        "step": 13443
    },
    {
        "loss": 2.3953,
        "grad_norm": 3.5355632305145264,
        "learning_rate": 4.4553136409838833e-05,
        "epoch": 1.7925333333333333,
        "step": 13444
    },
    {
        "loss": 1.8222,
        "grad_norm": 2.4897944927215576,
        "learning_rate": 4.450077114239402e-05,
        "epoch": 1.7926666666666666,
        "step": 13445
    },
    {
        "loss": 2.18,
        "grad_norm": 3.4249579906463623,
        "learning_rate": 4.444842785838296e-05,
        "epoch": 1.7928,
        "step": 13446
    },
    {
        "loss": 1.4592,
        "grad_norm": 4.638746738433838,
        "learning_rate": 4.4396106578538956e-05,
        "epoch": 1.7929333333333335,
        "step": 13447
    },
    {
        "loss": 2.9047,
        "grad_norm": 3.701333522796631,
        "learning_rate": 4.434380732358655e-05,
        "epoch": 1.7930666666666668,
        "step": 13448
    },
    {
        "loss": 1.1422,
        "grad_norm": 3.6709511280059814,
        "learning_rate": 4.429153011424162e-05,
        "epoch": 1.7932000000000001,
        "step": 13449
    },
    {
        "loss": 2.7403,
        "grad_norm": 4.535902500152588,
        "learning_rate": 4.423927497121145e-05,
        "epoch": 1.7933333333333334,
        "step": 13450
    },
    {
        "loss": 1.5467,
        "grad_norm": 4.683111190795898,
        "learning_rate": 4.418704191519466e-05,
        "epoch": 1.7934666666666668,
        "step": 13451
    },
    {
        "loss": 3.2164,
        "grad_norm": 2.496615171432495,
        "learning_rate": 4.4134830966880525e-05,
        "epoch": 1.7936,
        "step": 13452
    },
    {
        "loss": 2.1826,
        "grad_norm": 3.8259520530700684,
        "learning_rate": 4.4082642146950335e-05,
        "epoch": 1.7937333333333334,
        "step": 13453
    },
    {
        "loss": 2.3884,
        "grad_norm": 3.753542184829712,
        "learning_rate": 4.4030475476075996e-05,
        "epoch": 1.7938666666666667,
        "step": 13454
    },
    {
        "loss": 2.6735,
        "grad_norm": 5.136549949645996,
        "learning_rate": 4.3978330974921133e-05,
        "epoch": 1.794,
        "step": 13455
    },
    {
        "loss": 2.5453,
        "grad_norm": 3.905151844024658,
        "learning_rate": 4.392620866414029e-05,
        "epoch": 1.7941333333333334,
        "step": 13456
    },
    {
        "loss": 2.0921,
        "grad_norm": 2.557647943496704,
        "learning_rate": 4.3874108564379234e-05,
        "epoch": 1.7942666666666667,
        "step": 13457
    },
    {
        "loss": 1.0392,
        "grad_norm": 4.8396077156066895,
        "learning_rate": 4.3822030696274944e-05,
        "epoch": 1.7944,
        "step": 13458
    },
    {
        "loss": 2.6632,
        "grad_norm": 3.7361226081848145,
        "learning_rate": 4.376997508045568e-05,
        "epoch": 1.7945333333333333,
        "step": 13459
    },
    {
        "loss": 1.6512,
        "grad_norm": 3.265117883682251,
        "learning_rate": 4.3717941737541114e-05,
        "epoch": 1.7946666666666666,
        "step": 13460
    },
    {
        "loss": 2.1517,
        "grad_norm": 4.648103713989258,
        "learning_rate": 4.36659306881414e-05,
        "epoch": 1.7948,
        "step": 13461
    },
    {
        "loss": 1.7857,
        "grad_norm": 3.58707332611084,
        "learning_rate": 4.3613941952858585e-05,
        "epoch": 1.7949333333333333,
        "step": 13462
    },
    {
        "loss": 2.4995,
        "grad_norm": 2.4846763610839844,
        "learning_rate": 4.3561975552285376e-05,
        "epoch": 1.7950666666666666,
        "step": 13463
    },
    {
        "loss": 3.1243,
        "grad_norm": 5.4008870124816895,
        "learning_rate": 4.35100315070061e-05,
        "epoch": 1.7952,
        "step": 13464
    },
    {
        "loss": 1.9273,
        "grad_norm": 4.395893573760986,
        "learning_rate": 4.345810983759581e-05,
        "epoch": 1.7953333333333332,
        "step": 13465
    },
    {
        "loss": 2.3779,
        "grad_norm": 3.9963483810424805,
        "learning_rate": 4.34062105646209e-05,
        "epoch": 1.7954666666666665,
        "step": 13466
    },
    {
        "loss": 2.7799,
        "grad_norm": 3.0610992908477783,
        "learning_rate": 4.335433370863867e-05,
        "epoch": 1.7955999999999999,
        "step": 13467
    },
    {
        "loss": 2.403,
        "grad_norm": 4.907337665557861,
        "learning_rate": 4.3302479290198e-05,
        "epoch": 1.7957333333333332,
        "step": 13468
    },
    {
        "loss": 2.6351,
        "grad_norm": 3.130744218826294,
        "learning_rate": 4.325064732983849e-05,
        "epoch": 1.7958666666666665,
        "step": 13469
    },
    {
        "loss": 2.1825,
        "grad_norm": 4.270047187805176,
        "learning_rate": 4.319883784809081e-05,
        "epoch": 1.796,
        "step": 13470
    },
    {
        "loss": 1.3191,
        "grad_norm": 3.8592185974121094,
        "learning_rate": 4.314705086547711e-05,
        "epoch": 1.7961333333333334,
        "step": 13471
    },
    {
        "loss": 2.318,
        "grad_norm": 3.4434432983398438,
        "learning_rate": 4.309528640251032e-05,
        "epoch": 1.7962666666666667,
        "step": 13472
    },
    {
        "loss": 2.2552,
        "grad_norm": 3.2628560066223145,
        "learning_rate": 4.3043544479694344e-05,
        "epoch": 1.7964,
        "step": 13473
    },
    {
        "loss": 1.7663,
        "grad_norm": 5.151852130889893,
        "learning_rate": 4.2991825117524574e-05,
        "epoch": 1.7965333333333333,
        "step": 13474
    },
    {
        "loss": 2.3627,
        "grad_norm": 3.5384023189544678,
        "learning_rate": 4.2940128336487175e-05,
        "epoch": 1.7966666666666666,
        "step": 13475
    },
    {
        "loss": 2.4903,
        "grad_norm": 2.69193696975708,
        "learning_rate": 4.288845415705921e-05,
        "epoch": 1.7968,
        "step": 13476
    },
    {
        "loss": 1.7873,
        "grad_norm": 4.143462657928467,
        "learning_rate": 4.283680259970927e-05,
        "epoch": 1.7969333333333335,
        "step": 13477
    },
    {
        "loss": 1.8773,
        "grad_norm": 3.87809681892395,
        "learning_rate": 4.278517368489663e-05,
        "epoch": 1.7970666666666668,
        "step": 13478
    },
    {
        "loss": 2.1481,
        "grad_norm": 3.6318233013153076,
        "learning_rate": 4.273356743307151e-05,
        "epoch": 1.7972000000000001,
        "step": 13479
    },
    {
        "loss": 2.5316,
        "grad_norm": 3.8835701942443848,
        "learning_rate": 4.268198386467557e-05,
        "epoch": 1.7973333333333334,
        "step": 13480
    },
    {
        "loss": 1.9529,
        "grad_norm": 5.261007308959961,
        "learning_rate": 4.2630423000141164e-05,
        "epoch": 1.7974666666666668,
        "step": 13481
    },
    {
        "loss": 2.6917,
        "grad_norm": 1.5171843767166138,
        "learning_rate": 4.2578884859891534e-05,
        "epoch": 1.7976,
        "step": 13482
    },
    {
        "loss": 1.9795,
        "grad_norm": 3.739854335784912,
        "learning_rate": 4.2527369464341426e-05,
        "epoch": 1.7977333333333334,
        "step": 13483
    },
    {
        "loss": 0.7549,
        "grad_norm": 4.546062469482422,
        "learning_rate": 4.247587683389607e-05,
        "epoch": 1.7978666666666667,
        "step": 13484
    },
    {
        "loss": 1.0329,
        "grad_norm": 4.6077680587768555,
        "learning_rate": 4.2424406988951904e-05,
        "epoch": 1.798,
        "step": 13485
    },
    {
        "loss": 1.8073,
        "grad_norm": 1.8330460786819458,
        "learning_rate": 4.237295994989624e-05,
        "epoch": 1.7981333333333334,
        "step": 13486
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.511016368865967,
        "learning_rate": 4.2321535737107566e-05,
        "epoch": 1.7982666666666667,
        "step": 13487
    },
    {
        "loss": 2.385,
        "grad_norm": 3.5213541984558105,
        "learning_rate": 4.227013437095503e-05,
        "epoch": 1.7984,
        "step": 13488
    },
    {
        "loss": 1.0736,
        "grad_norm": 4.738835334777832,
        "learning_rate": 4.221875587179922e-05,
        "epoch": 1.7985333333333333,
        "step": 13489
    },
    {
        "loss": 2.1759,
        "grad_norm": 2.3276288509368896,
        "learning_rate": 4.216740025999084e-05,
        "epoch": 1.7986666666666666,
        "step": 13490
    },
    {
        "loss": 2.294,
        "grad_norm": 3.263681411743164,
        "learning_rate": 4.2116067555872275e-05,
        "epoch": 1.7988,
        "step": 13491
    },
    {
        "loss": 1.4323,
        "grad_norm": 3.9566261768341064,
        "learning_rate": 4.206475777977674e-05,
        "epoch": 1.7989333333333333,
        "step": 13492
    },
    {
        "loss": 2.3695,
        "grad_norm": 3.8091237545013428,
        "learning_rate": 4.201347095202801e-05,
        "epoch": 1.7990666666666666,
        "step": 13493
    },
    {
        "loss": 1.8673,
        "grad_norm": 3.7346079349517822,
        "learning_rate": 4.1962207092941065e-05,
        "epoch": 1.7992,
        "step": 13494
    },
    {
        "loss": 1.3334,
        "grad_norm": 4.6571736335754395,
        "learning_rate": 4.19109662228215e-05,
        "epoch": 1.7993333333333332,
        "step": 13495
    },
    {
        "loss": 1.2469,
        "grad_norm": 3.0896193981170654,
        "learning_rate": 4.185974836196627e-05,
        "epoch": 1.7994666666666665,
        "step": 13496
    },
    {
        "loss": 2.2465,
        "grad_norm": 2.8844597339630127,
        "learning_rate": 4.180855353066273e-05,
        "epoch": 1.7995999999999999,
        "step": 13497
    },
    {
        "loss": 2.5086,
        "grad_norm": 5.623622417449951,
        "learning_rate": 4.175738174918962e-05,
        "epoch": 1.7997333333333332,
        "step": 13498
    },
    {
        "loss": 2.4044,
        "grad_norm": 3.217127799987793,
        "learning_rate": 4.170623303781582e-05,
        "epoch": 1.7998666666666665,
        "step": 13499
    },
    {
        "loss": 2.2863,
        "grad_norm": 3.8338277339935303,
        "learning_rate": 4.1655107416801785e-05,
        "epoch": 1.8,
        "step": 13500
    },
    {
        "loss": 1.3765,
        "grad_norm": 3.890531539916992,
        "learning_rate": 4.160400490639859e-05,
        "epoch": 1.8001333333333334,
        "step": 13501
    },
    {
        "loss": 2.486,
        "grad_norm": 5.028221130371094,
        "learning_rate": 4.155292552684802e-05,
        "epoch": 1.8002666666666667,
        "step": 13502
    },
    {
        "loss": 0.912,
        "grad_norm": 2.0605432987213135,
        "learning_rate": 4.1501869298382845e-05,
        "epoch": 1.8004,
        "step": 13503
    },
    {
        "loss": 0.5322,
        "grad_norm": 2.7522623538970947,
        "learning_rate": 4.145083624122642e-05,
        "epoch": 1.8005333333333333,
        "step": 13504
    },
    {
        "loss": 2.3535,
        "grad_norm": 4.059086799621582,
        "learning_rate": 4.139982637559341e-05,
        "epoch": 1.8006666666666666,
        "step": 13505
    },
    {
        "loss": 2.7294,
        "grad_norm": 2.3676083087921143,
        "learning_rate": 4.1348839721688815e-05,
        "epoch": 1.8008,
        "step": 13506
    },
    {
        "loss": 2.5515,
        "grad_norm": 3.5019993782043457,
        "learning_rate": 4.129787629970865e-05,
        "epoch": 1.8009333333333335,
        "step": 13507
    },
    {
        "loss": 1.2593,
        "grad_norm": 4.1742048263549805,
        "learning_rate": 4.124693612983963e-05,
        "epoch": 1.8010666666666668,
        "step": 13508
    },
    {
        "loss": 1.819,
        "grad_norm": 3.0109689235687256,
        "learning_rate": 4.1196019232259363e-05,
        "epoch": 1.8012000000000001,
        "step": 13509
    },
    {
        "loss": 2.608,
        "grad_norm": 2.990067481994629,
        "learning_rate": 4.114512562713648e-05,
        "epoch": 1.8013333333333335,
        "step": 13510
    },
    {
        "loss": 2.5528,
        "grad_norm": 6.359024524688721,
        "learning_rate": 4.10942553346297e-05,
        "epoch": 1.8014666666666668,
        "step": 13511
    },
    {
        "loss": 2.4447,
        "grad_norm": 5.052590847015381,
        "learning_rate": 4.104340837488915e-05,
        "epoch": 1.8016,
        "step": 13512
    },
    {
        "loss": 1.9638,
        "grad_norm": 4.558530807495117,
        "learning_rate": 4.0992584768055345e-05,
        "epoch": 1.8017333333333334,
        "step": 13513
    },
    {
        "loss": 2.0576,
        "grad_norm": 4.851763725280762,
        "learning_rate": 4.094178453425991e-05,
        "epoch": 1.8018666666666667,
        "step": 13514
    },
    {
        "loss": 1.7643,
        "grad_norm": 3.6354877948760986,
        "learning_rate": 4.0891007693624816e-05,
        "epoch": 1.802,
        "step": 13515
    },
    {
        "loss": 2.5326,
        "grad_norm": 2.758915424346924,
        "learning_rate": 4.084025426626302e-05,
        "epoch": 1.8021333333333334,
        "step": 13516
    },
    {
        "loss": 2.7118,
        "grad_norm": 2.544919013977051,
        "learning_rate": 4.078952427227796e-05,
        "epoch": 1.8022666666666667,
        "step": 13517
    },
    {
        "loss": 2.5502,
        "grad_norm": 3.7295145988464355,
        "learning_rate": 4.073881773176409e-05,
        "epoch": 1.8024,
        "step": 13518
    },
    {
        "loss": 2.1018,
        "grad_norm": 4.559410572052002,
        "learning_rate": 4.0688134664806684e-05,
        "epoch": 1.8025333333333333,
        "step": 13519
    },
    {
        "loss": 2.8439,
        "grad_norm": 3.120330572128296,
        "learning_rate": 4.063747509148099e-05,
        "epoch": 1.8026666666666666,
        "step": 13520
    },
    {
        "loss": 1.7372,
        "grad_norm": 4.060431957244873,
        "learning_rate": 4.0586839031853795e-05,
        "epoch": 1.8028,
        "step": 13521
    },
    {
        "loss": 1.7664,
        "grad_norm": 3.2541213035583496,
        "learning_rate": 4.0536226505982034e-05,
        "epoch": 1.8029333333333333,
        "step": 13522
    },
    {
        "loss": 2.6466,
        "grad_norm": 3.7855734825134277,
        "learning_rate": 4.0485637533913644e-05,
        "epoch": 1.8030666666666666,
        "step": 13523
    },
    {
        "loss": 2.4091,
        "grad_norm": 3.6077425479888916,
        "learning_rate": 4.043507213568701e-05,
        "epoch": 1.8032,
        "step": 13524
    },
    {
        "loss": 3.4817,
        "grad_norm": 5.557518482208252,
        "learning_rate": 4.0384530331331304e-05,
        "epoch": 1.8033333333333332,
        "step": 13525
    },
    {
        "loss": 1.697,
        "grad_norm": 3.2714297771453857,
        "learning_rate": 4.033401214086613e-05,
        "epoch": 1.8034666666666666,
        "step": 13526
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.7330727577209473,
        "learning_rate": 4.028351758430215e-05,
        "epoch": 1.8035999999999999,
        "step": 13527
    },
    {
        "loss": 2.3822,
        "grad_norm": 6.733002185821533,
        "learning_rate": 4.0233046681640384e-05,
        "epoch": 1.8037333333333332,
        "step": 13528
    },
    {
        "loss": 2.6607,
        "grad_norm": 2.5990066528320312,
        "learning_rate": 4.018259945287234e-05,
        "epoch": 1.8038666666666665,
        "step": 13529
    },
    {
        "loss": 1.2923,
        "grad_norm": 5.115961074829102,
        "learning_rate": 4.0132175917980596e-05,
        "epoch": 1.804,
        "step": 13530
    },
    {
        "loss": 0.5311,
        "grad_norm": 2.757721185684204,
        "learning_rate": 4.008177609693795e-05,
        "epoch": 1.8041333333333334,
        "step": 13531
    },
    {
        "loss": 1.9553,
        "grad_norm": 5.49476432800293,
        "learning_rate": 4.003140000970784e-05,
        "epoch": 1.8042666666666667,
        "step": 13532
    },
    {
        "loss": 1.7814,
        "grad_norm": 3.774540662765503,
        "learning_rate": 3.998104767624467e-05,
        "epoch": 1.8044,
        "step": 13533
    },
    {
        "loss": 1.584,
        "grad_norm": 3.984123706817627,
        "learning_rate": 3.993071911649299e-05,
        "epoch": 1.8045333333333333,
        "step": 13534
    },
    {
        "loss": 2.1737,
        "grad_norm": 3.637279510498047,
        "learning_rate": 3.988041435038804e-05,
        "epoch": 1.8046666666666666,
        "step": 13535
    },
    {
        "loss": 2.8476,
        "grad_norm": 3.6364831924438477,
        "learning_rate": 3.983013339785596e-05,
        "epoch": 1.8048,
        "step": 13536
    },
    {
        "loss": 2.6236,
        "grad_norm": 5.10243558883667,
        "learning_rate": 3.9779876278813034e-05,
        "epoch": 1.8049333333333333,
        "step": 13537
    },
    {
        "loss": 1.8488,
        "grad_norm": 3.5680267810821533,
        "learning_rate": 3.9729643013166196e-05,
        "epoch": 1.8050666666666668,
        "step": 13538
    },
    {
        "loss": 1.372,
        "grad_norm": 4.487662315368652,
        "learning_rate": 3.967943362081328e-05,
        "epoch": 1.8052000000000001,
        "step": 13539
    },
    {
        "loss": 2.9008,
        "grad_norm": 2.1938536167144775,
        "learning_rate": 3.962924812164219e-05,
        "epoch": 1.8053333333333335,
        "step": 13540
    },
    {
        "loss": 2.0027,
        "grad_norm": 2.406085252761841,
        "learning_rate": 3.957908653553154e-05,
        "epoch": 1.8054666666666668,
        "step": 13541
    },
    {
        "loss": 2.3298,
        "grad_norm": 4.06493616104126,
        "learning_rate": 3.952894888235072e-05,
        "epoch": 1.8056,
        "step": 13542
    },
    {
        "loss": 2.2901,
        "grad_norm": 2.185715436935425,
        "learning_rate": 3.9478835181959273e-05,
        "epoch": 1.8057333333333334,
        "step": 13543
    },
    {
        "loss": 2.1211,
        "grad_norm": 3.758007287979126,
        "learning_rate": 3.942874545420746e-05,
        "epoch": 1.8058666666666667,
        "step": 13544
    },
    {
        "loss": 2.1067,
        "grad_norm": 2.8339180946350098,
        "learning_rate": 3.937867971893583e-05,
        "epoch": 1.806,
        "step": 13545
    },
    {
        "loss": 2.3497,
        "grad_norm": 3.085721492767334,
        "learning_rate": 3.9328637995975827e-05,
        "epoch": 1.8061333333333334,
        "step": 13546
    },
    {
        "loss": 2.2589,
        "grad_norm": 4.491257190704346,
        "learning_rate": 3.9278620305148974e-05,
        "epoch": 1.8062666666666667,
        "step": 13547
    },
    {
        "loss": 2.2657,
        "grad_norm": 3.3394734859466553,
        "learning_rate": 3.92286266662677e-05,
        "epoch": 1.8064,
        "step": 13548
    },
    {
        "loss": 2.0278,
        "grad_norm": 4.9348297119140625,
        "learning_rate": 3.917865709913426e-05,
        "epoch": 1.8065333333333333,
        "step": 13549
    },
    {
        "loss": 2.0116,
        "grad_norm": 3.804448366165161,
        "learning_rate": 3.9128711623542024e-05,
        "epoch": 1.8066666666666666,
        "step": 13550
    },
    {
        "loss": 2.9762,
        "grad_norm": 2.750885486602783,
        "learning_rate": 3.90787902592746e-05,
        "epoch": 1.8068,
        "step": 13551
    },
    {
        "loss": 2.139,
        "grad_norm": 2.616018772125244,
        "learning_rate": 3.902889302610597e-05,
        "epoch": 1.8069333333333333,
        "step": 13552
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.063230514526367,
        "learning_rate": 3.8979019943800545e-05,
        "epoch": 1.8070666666666666,
        "step": 13553
    },
    {
        "loss": 2.2888,
        "grad_norm": 3.4609458446502686,
        "learning_rate": 3.892917103211312e-05,
        "epoch": 1.8072,
        "step": 13554
    },
    {
        "loss": 1.6375,
        "grad_norm": 4.6486496925354,
        "learning_rate": 3.88793463107893e-05,
        "epoch": 1.8073333333333332,
        "step": 13555
    },
    {
        "loss": 0.6212,
        "grad_norm": 2.8551745414733887,
        "learning_rate": 3.882954579956452e-05,
        "epoch": 1.8074666666666666,
        "step": 13556
    },
    {
        "loss": 2.592,
        "grad_norm": 5.228590488433838,
        "learning_rate": 3.877976951816529e-05,
        "epoch": 1.8075999999999999,
        "step": 13557
    },
    {
        "loss": 2.2999,
        "grad_norm": 5.2210516929626465,
        "learning_rate": 3.873001748630779e-05,
        "epoch": 1.8077333333333332,
        "step": 13558
    },
    {
        "loss": 2.0928,
        "grad_norm": 3.63720440864563,
        "learning_rate": 3.8680289723699095e-05,
        "epoch": 1.8078666666666665,
        "step": 13559
    },
    {
        "loss": 1.7192,
        "grad_norm": 3.3605942726135254,
        "learning_rate": 3.863058625003668e-05,
        "epoch": 1.808,
        "step": 13560
    },
    {
        "loss": 1.7097,
        "grad_norm": 2.9035003185272217,
        "learning_rate": 3.8580907085008214e-05,
        "epoch": 1.8081333333333334,
        "step": 13561
    },
    {
        "loss": 1.5035,
        "grad_norm": 2.929086446762085,
        "learning_rate": 3.853125224829168e-05,
        "epoch": 1.8082666666666667,
        "step": 13562
    },
    {
        "loss": 1.8916,
        "grad_norm": 7.166296482086182,
        "learning_rate": 3.848162175955549e-05,
        "epoch": 1.8084,
        "step": 13563
    },
    {
        "loss": 2.7018,
        "grad_norm": 3.721679925918579,
        "learning_rate": 3.8432015638458675e-05,
        "epoch": 1.8085333333333333,
        "step": 13564
    },
    {
        "loss": 1.976,
        "grad_norm": 3.5604960918426514,
        "learning_rate": 3.838243390465022e-05,
        "epoch": 1.8086666666666666,
        "step": 13565
    },
    {
        "loss": 2.2144,
        "grad_norm": 3.1559996604919434,
        "learning_rate": 3.8332876577769686e-05,
        "epoch": 1.8088,
        "step": 13566
    },
    {
        "loss": 2.5305,
        "grad_norm": 2.6244943141937256,
        "learning_rate": 3.8283343677446717e-05,
        "epoch": 1.8089333333333333,
        "step": 13567
    },
    {
        "loss": 2.1152,
        "grad_norm": 2.979545831680298,
        "learning_rate": 3.823383522330162e-05,
        "epoch": 1.8090666666666668,
        "step": 13568
    },
    {
        "loss": 2.0733,
        "grad_norm": 2.68959379196167,
        "learning_rate": 3.818435123494509e-05,
        "epoch": 1.8092000000000001,
        "step": 13569
    },
    {
        "loss": 2.7621,
        "grad_norm": 3.284783124923706,
        "learning_rate": 3.813489173197744e-05,
        "epoch": 1.8093333333333335,
        "step": 13570
    },
    {
        "loss": 1.4032,
        "grad_norm": 3.5428032875061035,
        "learning_rate": 3.8085456733990064e-05,
        "epoch": 1.8094666666666668,
        "step": 13571
    },
    {
        "loss": 2.6766,
        "grad_norm": 4.3452253341674805,
        "learning_rate": 3.8036046260564186e-05,
        "epoch": 1.8096,
        "step": 13572
    },
    {
        "loss": 1.9168,
        "grad_norm": 4.313160419464111,
        "learning_rate": 3.798666033127158e-05,
        "epoch": 1.8097333333333334,
        "step": 13573
    },
    {
        "loss": 1.0229,
        "grad_norm": 3.646104574203491,
        "learning_rate": 3.793729896567413e-05,
        "epoch": 1.8098666666666667,
        "step": 13574
    },
    {
        "loss": 1.5229,
        "grad_norm": 4.591682434082031,
        "learning_rate": 3.788796218332407e-05,
        "epoch": 1.81,
        "step": 13575
    },
    {
        "loss": 0.8631,
        "grad_norm": 2.925011157989502,
        "learning_rate": 3.7838650003763676e-05,
        "epoch": 1.8101333333333334,
        "step": 13576
    },
    {
        "loss": 2.6944,
        "grad_norm": 4.006637096405029,
        "learning_rate": 3.7789362446525836e-05,
        "epoch": 1.8102666666666667,
        "step": 13577
    },
    {
        "loss": 2.3051,
        "grad_norm": 3.8343453407287598,
        "learning_rate": 3.7740099531133756e-05,
        "epoch": 1.8104,
        "step": 13578
    },
    {
        "loss": 2.7195,
        "grad_norm": 2.36040997505188,
        "learning_rate": 3.7690861277100156e-05,
        "epoch": 1.8105333333333333,
        "step": 13579
    },
    {
        "loss": 2.2522,
        "grad_norm": 3.279759407043457,
        "learning_rate": 3.764164770392884e-05,
        "epoch": 1.8106666666666666,
        "step": 13580
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.601701021194458,
        "learning_rate": 3.759245883111322e-05,
        "epoch": 1.8108,
        "step": 13581
    },
    {
        "loss": 1.8207,
        "grad_norm": 3.9863016605377197,
        "learning_rate": 3.754329467813741e-05,
        "epoch": 1.8109333333333333,
        "step": 13582
    },
    {
        "loss": 2.7956,
        "grad_norm": 3.3482182025909424,
        "learning_rate": 3.749415526447543e-05,
        "epoch": 1.8110666666666666,
        "step": 13583
    },
    {
        "loss": 1.7283,
        "grad_norm": 4.4799885749816895,
        "learning_rate": 3.74450406095915e-05,
        "epoch": 1.8112,
        "step": 13584
    },
    {
        "loss": 2.2494,
        "grad_norm": 3.6026551723480225,
        "learning_rate": 3.7395950732940024e-05,
        "epoch": 1.8113333333333332,
        "step": 13585
    },
    {
        "loss": 2.9565,
        "grad_norm": 2.5274815559387207,
        "learning_rate": 3.734688565396592e-05,
        "epoch": 1.8114666666666666,
        "step": 13586
    },
    {
        "loss": 2.6935,
        "grad_norm": 2.717778205871582,
        "learning_rate": 3.729784539210387e-05,
        "epoch": 1.8115999999999999,
        "step": 13587
    },
    {
        "loss": 2.1983,
        "grad_norm": 3.1296894550323486,
        "learning_rate": 3.7248829966778795e-05,
        "epoch": 1.8117333333333332,
        "step": 13588
    },
    {
        "loss": 2.7241,
        "grad_norm": 3.587352752685547,
        "learning_rate": 3.7199839397406143e-05,
        "epoch": 1.8118666666666665,
        "step": 13589
    },
    {
        "loss": 1.375,
        "grad_norm": 1.9703609943389893,
        "learning_rate": 3.715087370339104e-05,
        "epoch": 1.812,
        "step": 13590
    },
    {
        "loss": 1.9472,
        "grad_norm": 3.1510133743286133,
        "learning_rate": 3.7101932904128964e-05,
        "epoch": 1.8121333333333334,
        "step": 13591
    },
    {
        "loss": 2.7371,
        "grad_norm": 5.155289173126221,
        "learning_rate": 3.705301701900564e-05,
        "epoch": 1.8122666666666667,
        "step": 13592
    },
    {
        "loss": 2.772,
        "grad_norm": 3.2419445514678955,
        "learning_rate": 3.700412606739677e-05,
        "epoch": 1.8124,
        "step": 13593
    },
    {
        "loss": 2.5085,
        "grad_norm": 3.004533529281616,
        "learning_rate": 3.695526006866825e-05,
        "epoch": 1.8125333333333333,
        "step": 13594
    },
    {
        "loss": 2.499,
        "grad_norm": 4.071599960327148,
        "learning_rate": 3.69064190421759e-05,
        "epoch": 1.8126666666666666,
        "step": 13595
    },
    {
        "loss": 1.8127,
        "grad_norm": 2.6504857540130615,
        "learning_rate": 3.685760300726605e-05,
        "epoch": 1.8128,
        "step": 13596
    },
    {
        "loss": 2.1011,
        "grad_norm": 3.161438226699829,
        "learning_rate": 3.680881198327474e-05,
        "epoch": 1.8129333333333333,
        "step": 13597
    },
    {
        "loss": 2.4325,
        "grad_norm": 3.5453202724456787,
        "learning_rate": 3.676004598952839e-05,
        "epoch": 1.8130666666666668,
        "step": 13598
    },
    {
        "loss": 2.027,
        "grad_norm": 3.8947737216949463,
        "learning_rate": 3.6711305045343336e-05,
        "epoch": 1.8132000000000001,
        "step": 13599
    },
    {
        "loss": 2.2683,
        "grad_norm": 3.7537484169006348,
        "learning_rate": 3.666258917002587e-05,
        "epoch": 1.8133333333333335,
        "step": 13600
    },
    {
        "loss": 2.4206,
        "grad_norm": 3.8529021739959717,
        "learning_rate": 3.6613898382872756e-05,
        "epoch": 1.8134666666666668,
        "step": 13601
    },
    {
        "loss": 2.4883,
        "grad_norm": 3.287838935852051,
        "learning_rate": 3.6565232703170514e-05,
        "epoch": 1.8136,
        "step": 13602
    },
    {
        "loss": 2.5153,
        "grad_norm": 3.5564727783203125,
        "learning_rate": 3.6516592150195706e-05,
        "epoch": 1.8137333333333334,
        "step": 13603
    },
    {
        "loss": 2.47,
        "grad_norm": 2.437436103820801,
        "learning_rate": 3.646797674321496e-05,
        "epoch": 1.8138666666666667,
        "step": 13604
    },
    {
        "loss": 2.2952,
        "grad_norm": 4.335256099700928,
        "learning_rate": 3.641938650148524e-05,
        "epoch": 1.814,
        "step": 13605
    },
    {
        "loss": 2.0991,
        "grad_norm": 4.285848140716553,
        "learning_rate": 3.6370821444253045e-05,
        "epoch": 1.8141333333333334,
        "step": 13606
    },
    {
        "loss": 2.5328,
        "grad_norm": 4.283454418182373,
        "learning_rate": 3.6322281590755524e-05,
        "epoch": 1.8142666666666667,
        "step": 13607
    },
    {
        "loss": 1.6387,
        "grad_norm": 4.620820999145508,
        "learning_rate": 3.627376696021899e-05,
        "epoch": 1.8144,
        "step": 13608
    },
    {
        "loss": 2.2044,
        "grad_norm": 4.113778591156006,
        "learning_rate": 3.6225277571860515e-05,
        "epoch": 1.8145333333333333,
        "step": 13609
    },
    {
        "loss": 2.6873,
        "grad_norm": 3.16801381111145,
        "learning_rate": 3.6176813444887004e-05,
        "epoch": 1.8146666666666667,
        "step": 13610
    },
    {
        "loss": 1.4609,
        "grad_norm": 6.106261730194092,
        "learning_rate": 3.61283745984952e-05,
        "epoch": 1.8148,
        "step": 13611
    },
    {
        "loss": 1.2831,
        "grad_norm": 3.347691535949707,
        "learning_rate": 3.607996105187182e-05,
        "epoch": 1.8149333333333333,
        "step": 13612
    },
    {
        "loss": 1.6419,
        "grad_norm": 5.496676921844482,
        "learning_rate": 3.6031572824193584e-05,
        "epoch": 1.8150666666666666,
        "step": 13613
    },
    {
        "loss": 1.8043,
        "grad_norm": 3.2140462398529053,
        "learning_rate": 3.598320993462739e-05,
        "epoch": 1.8152,
        "step": 13614
    },
    {
        "loss": 1.5003,
        "grad_norm": 2.1954729557037354,
        "learning_rate": 3.5934872402329886e-05,
        "epoch": 1.8153333333333332,
        "step": 13615
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.5699098110198975,
        "learning_rate": 3.588656024644775e-05,
        "epoch": 1.8154666666666666,
        "step": 13616
    },
    {
        "loss": 2.5436,
        "grad_norm": 5.742445468902588,
        "learning_rate": 3.5838273486117424e-05,
        "epoch": 1.8155999999999999,
        "step": 13617
    },
    {
        "loss": 2.6616,
        "grad_norm": 2.7451980113983154,
        "learning_rate": 3.579001214046559e-05,
        "epoch": 1.8157333333333332,
        "step": 13618
    },
    {
        "loss": 2.3183,
        "grad_norm": 3.6567440032958984,
        "learning_rate": 3.574177622860888e-05,
        "epoch": 1.8158666666666665,
        "step": 13619
    },
    {
        "loss": 1.6265,
        "grad_norm": 3.9883835315704346,
        "learning_rate": 3.5693565769653524e-05,
        "epoch": 1.8159999999999998,
        "step": 13620
    },
    {
        "loss": 1.5622,
        "grad_norm": 4.397594451904297,
        "learning_rate": 3.5645380782695924e-05,
        "epoch": 1.8161333333333334,
        "step": 13621
    },
    {
        "loss": 1.3569,
        "grad_norm": 3.6048834323883057,
        "learning_rate": 3.559722128682214e-05,
        "epoch": 1.8162666666666667,
        "step": 13622
    },
    {
        "loss": 1.6864,
        "grad_norm": 3.4162590503692627,
        "learning_rate": 3.554908730110855e-05,
        "epoch": 1.8164,
        "step": 13623
    },
    {
        "loss": 2.2117,
        "grad_norm": 4.125058650970459,
        "learning_rate": 3.550097884462114e-05,
        "epoch": 1.8165333333333333,
        "step": 13624
    },
    {
        "loss": 1.5556,
        "grad_norm": 4.975003719329834,
        "learning_rate": 3.5452895936415733e-05,
        "epoch": 1.8166666666666667,
        "step": 13625
    },
    {
        "loss": 2.238,
        "grad_norm": 3.9795751571655273,
        "learning_rate": 3.540483859553808e-05,
        "epoch": 1.8168,
        "step": 13626
    },
    {
        "loss": 2.3039,
        "grad_norm": 3.230961799621582,
        "learning_rate": 3.535680684102402e-05,
        "epoch": 1.8169333333333333,
        "step": 13627
    },
    {
        "loss": 1.9506,
        "grad_norm": 3.8131842613220215,
        "learning_rate": 3.530880069189918e-05,
        "epoch": 1.8170666666666668,
        "step": 13628
    },
    {
        "loss": 1.8818,
        "grad_norm": 4.269589424133301,
        "learning_rate": 3.526082016717862e-05,
        "epoch": 1.8172000000000001,
        "step": 13629
    },
    {
        "loss": 0.8851,
        "grad_norm": 3.6492693424224854,
        "learning_rate": 3.521286528586792e-05,
        "epoch": 1.8173333333333335,
        "step": 13630
    },
    {
        "loss": 2.2668,
        "grad_norm": 3.0123231410980225,
        "learning_rate": 3.5164936066961916e-05,
        "epoch": 1.8174666666666668,
        "step": 13631
    },
    {
        "loss": 1.4179,
        "grad_norm": 3.464062452316284,
        "learning_rate": 3.511703252944575e-05,
        "epoch": 1.8176,
        "step": 13632
    },
    {
        "loss": 2.2141,
        "grad_norm": 3.036720037460327,
        "learning_rate": 3.50691546922941e-05,
        "epoch": 1.8177333333333334,
        "step": 13633
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.603818893432617,
        "learning_rate": 3.50213025744715e-05,
        "epoch": 1.8178666666666667,
        "step": 13634
    },
    {
        "loss": 1.5569,
        "grad_norm": 5.327643871307373,
        "learning_rate": 3.497347619493227e-05,
        "epoch": 1.818,
        "step": 13635
    },
    {
        "loss": 1.9088,
        "grad_norm": 4.020325183868408,
        "learning_rate": 3.492567557262072e-05,
        "epoch": 1.8181333333333334,
        "step": 13636
    },
    {
        "loss": 1.4847,
        "grad_norm": 4.695645809173584,
        "learning_rate": 3.487790072647097e-05,
        "epoch": 1.8182666666666667,
        "step": 13637
    },
    {
        "loss": 1.3411,
        "grad_norm": 4.00390625,
        "learning_rate": 3.483015167540648e-05,
        "epoch": 1.8184,
        "step": 13638
    },
    {
        "loss": 1.7765,
        "grad_norm": 4.140014171600342,
        "learning_rate": 3.478242843834103e-05,
        "epoch": 1.8185333333333333,
        "step": 13639
    },
    {
        "loss": 2.0643,
        "grad_norm": 3.113483190536499,
        "learning_rate": 3.4734731034177816e-05,
        "epoch": 1.8186666666666667,
        "step": 13640
    },
    {
        "loss": 2.4019,
        "grad_norm": 4.526642322540283,
        "learning_rate": 3.4687059481810145e-05,
        "epoch": 1.8188,
        "step": 13641
    },
    {
        "loss": 2.5374,
        "grad_norm": 3.4671547412872314,
        "learning_rate": 3.463941380012073e-05,
        "epoch": 1.8189333333333333,
        "step": 13642
    },
    {
        "loss": 2.4572,
        "grad_norm": 2.451390504837036,
        "learning_rate": 3.4591794007982215e-05,
        "epoch": 1.8190666666666666,
        "step": 13643
    },
    {
        "loss": 2.2436,
        "grad_norm": 3.531184434890747,
        "learning_rate": 3.454420012425691e-05,
        "epoch": 1.8192,
        "step": 13644
    },
    {
        "loss": 1.9286,
        "grad_norm": 3.627154588699341,
        "learning_rate": 3.4496632167797036e-05,
        "epoch": 1.8193333333333332,
        "step": 13645
    },
    {
        "loss": 2.6579,
        "grad_norm": 3.6975557804107666,
        "learning_rate": 3.4449090157444385e-05,
        "epoch": 1.8194666666666666,
        "step": 13646
    },
    {
        "loss": 2.7397,
        "grad_norm": 2.7229902744293213,
        "learning_rate": 3.4401574112030355e-05,
        "epoch": 1.8195999999999999,
        "step": 13647
    },
    {
        "loss": 1.7079,
        "grad_norm": 3.730102062225342,
        "learning_rate": 3.435408405037643e-05,
        "epoch": 1.8197333333333332,
        "step": 13648
    },
    {
        "loss": 2.0895,
        "grad_norm": 3.461338758468628,
        "learning_rate": 3.430661999129354e-05,
        "epoch": 1.8198666666666665,
        "step": 13649
    },
    {
        "loss": 2.621,
        "grad_norm": 2.5839645862579346,
        "learning_rate": 3.425918195358218e-05,
        "epoch": 1.8199999999999998,
        "step": 13650
    },
    {
        "loss": 2.5,
        "grad_norm": 4.295550346374512,
        "learning_rate": 3.421176995603296e-05,
        "epoch": 1.8201333333333334,
        "step": 13651
    },
    {
        "loss": 1.6487,
        "grad_norm": 3.440143585205078,
        "learning_rate": 3.4164384017425843e-05,
        "epoch": 1.8202666666666667,
        "step": 13652
    },
    {
        "loss": 0.9638,
        "grad_norm": 5.3407511711120605,
        "learning_rate": 3.411702415653053e-05,
        "epoch": 1.8204,
        "step": 13653
    },
    {
        "loss": 2.3225,
        "grad_norm": 3.8737168312072754,
        "learning_rate": 3.406969039210633e-05,
        "epoch": 1.8205333333333333,
        "step": 13654
    },
    {
        "loss": 1.9729,
        "grad_norm": 6.477265357971191,
        "learning_rate": 3.402238274290256e-05,
        "epoch": 1.8206666666666667,
        "step": 13655
    },
    {
        "loss": 2.3588,
        "grad_norm": 3.228816270828247,
        "learning_rate": 3.397510122765766e-05,
        "epoch": 1.8208,
        "step": 13656
    },
    {
        "loss": 2.5353,
        "grad_norm": 2.9117729663848877,
        "learning_rate": 3.392784586510024e-05,
        "epoch": 1.8209333333333333,
        "step": 13657
    },
    {
        "loss": 1.6058,
        "grad_norm": 5.17037296295166,
        "learning_rate": 3.388061667394824e-05,
        "epoch": 1.8210666666666666,
        "step": 13658
    },
    {
        "loss": 2.3606,
        "grad_norm": 3.375606060028076,
        "learning_rate": 3.383341367290914e-05,
        "epoch": 1.8212000000000002,
        "step": 13659
    },
    {
        "loss": 2.2688,
        "grad_norm": 4.041412353515625,
        "learning_rate": 3.378623688068047e-05,
        "epoch": 1.8213333333333335,
        "step": 13660
    },
    {
        "loss": 1.7352,
        "grad_norm": 4.724035263061523,
        "learning_rate": 3.3739086315948966e-05,
        "epoch": 1.8214666666666668,
        "step": 13661
    },
    {
        "loss": 2.2715,
        "grad_norm": 3.552659511566162,
        "learning_rate": 3.369196199739114e-05,
        "epoch": 1.8216,
        "step": 13662
    },
    {
        "loss": 2.2219,
        "grad_norm": 4.05854606628418,
        "learning_rate": 3.364486394367304e-05,
        "epoch": 1.8217333333333334,
        "step": 13663
    },
    {
        "loss": 2.5303,
        "grad_norm": 4.674722671508789,
        "learning_rate": 3.3597792173450505e-05,
        "epoch": 1.8218666666666667,
        "step": 13664
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.0836684703826904,
        "learning_rate": 3.355074670536864e-05,
        "epoch": 1.822,
        "step": 13665
    },
    {
        "loss": 1.4508,
        "grad_norm": 3.148160219192505,
        "learning_rate": 3.350372755806267e-05,
        "epoch": 1.8221333333333334,
        "step": 13666
    },
    {
        "loss": 2.4222,
        "grad_norm": 3.5093045234680176,
        "learning_rate": 3.3456734750156557e-05,
        "epoch": 1.8222666666666667,
        "step": 13667
    },
    {
        "loss": 1.5913,
        "grad_norm": 3.0961549282073975,
        "learning_rate": 3.340976830026455e-05,
        "epoch": 1.8224,
        "step": 13668
    },
    {
        "loss": 2.4059,
        "grad_norm": 1.9761661291122437,
        "learning_rate": 3.336282822699035e-05,
        "epoch": 1.8225333333333333,
        "step": 13669
    },
    {
        "loss": 2.3945,
        "grad_norm": 3.527111530303955,
        "learning_rate": 3.331591454892692e-05,
        "epoch": 1.8226666666666667,
        "step": 13670
    },
    {
        "loss": 1.955,
        "grad_norm": 2.8630940914154053,
        "learning_rate": 3.326902728465699e-05,
        "epoch": 1.8228,
        "step": 13671
    },
    {
        "loss": 2.6615,
        "grad_norm": 3.092613935470581,
        "learning_rate": 3.3222166452752604e-05,
        "epoch": 1.8229333333333333,
        "step": 13672
    },
    {
        "loss": 1.2591,
        "grad_norm": 3.6313846111297607,
        "learning_rate": 3.3175332071775724e-05,
        "epoch": 1.8230666666666666,
        "step": 13673
    },
    {
        "loss": 0.8875,
        "grad_norm": 4.969091892242432,
        "learning_rate": 3.312852416027754e-05,
        "epoch": 1.8232,
        "step": 13674
    },
    {
        "loss": 1.9288,
        "grad_norm": 8.17691421508789,
        "learning_rate": 3.308174273679876e-05,
        "epoch": 1.8233333333333333,
        "step": 13675
    },
    {
        "loss": 1.9047,
        "grad_norm": 5.2496113777160645,
        "learning_rate": 3.30349878198696e-05,
        "epoch": 1.8234666666666666,
        "step": 13676
    },
    {
        "loss": 1.4251,
        "grad_norm": 4.602444648742676,
        "learning_rate": 3.298825942800998e-05,
        "epoch": 1.8235999999999999,
        "step": 13677
    },
    {
        "loss": 2.6196,
        "grad_norm": 3.283003091812134,
        "learning_rate": 3.2941557579729196e-05,
        "epoch": 1.8237333333333332,
        "step": 13678
    },
    {
        "loss": 2.2433,
        "grad_norm": 4.939917087554932,
        "learning_rate": 3.2894882293525966e-05,
        "epoch": 1.8238666666666665,
        "step": 13679
    },
    {
        "loss": 1.9736,
        "grad_norm": 3.559091091156006,
        "learning_rate": 3.2848233587888535e-05,
        "epoch": 1.8239999999999998,
        "step": 13680
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.8795502185821533,
        "learning_rate": 3.280161148129447e-05,
        "epoch": 1.8241333333333334,
        "step": 13681
    },
    {
        "loss": 1.9708,
        "grad_norm": 3.5982930660247803,
        "learning_rate": 3.275501599221117e-05,
        "epoch": 1.8242666666666667,
        "step": 13682
    },
    {
        "loss": 2.4771,
        "grad_norm": 2.74703049659729,
        "learning_rate": 3.2708447139095185e-05,
        "epoch": 1.8244,
        "step": 13683
    },
    {
        "loss": 1.7694,
        "grad_norm": 3.413788318634033,
        "learning_rate": 3.2661904940392564e-05,
        "epoch": 1.8245333333333333,
        "step": 13684
    },
    {
        "loss": 2.5351,
        "grad_norm": 3.296663284301758,
        "learning_rate": 3.261538941453877e-05,
        "epoch": 1.8246666666666667,
        "step": 13685
    },
    {
        "loss": 1.6187,
        "grad_norm": 6.638763427734375,
        "learning_rate": 3.25689005799588e-05,
        "epoch": 1.8248,
        "step": 13686
    },
    {
        "loss": 2.2685,
        "grad_norm": 3.4278478622436523,
        "learning_rate": 3.252243845506728e-05,
        "epoch": 1.8249333333333333,
        "step": 13687
    },
    {
        "loss": 1.2671,
        "grad_norm": 4.115441799163818,
        "learning_rate": 3.247600305826765e-05,
        "epoch": 1.8250666666666666,
        "step": 13688
    },
    {
        "loss": 1.769,
        "grad_norm": 4.688576698303223,
        "learning_rate": 3.242959440795334e-05,
        "epoch": 1.8252000000000002,
        "step": 13689
    },
    {
        "loss": 1.8953,
        "grad_norm": 3.077990770339966,
        "learning_rate": 3.238321252250685e-05,
        "epoch": 1.8253333333333335,
        "step": 13690
    },
    {
        "loss": 2.0596,
        "grad_norm": 4.726603031158447,
        "learning_rate": 3.233685742030039e-05,
        "epoch": 1.8254666666666668,
        "step": 13691
    },
    {
        "loss": 2.3405,
        "grad_norm": 2.6769590377807617,
        "learning_rate": 3.229052911969525e-05,
        "epoch": 1.8256000000000001,
        "step": 13692
    },
    {
        "loss": 2.2082,
        "grad_norm": 3.622159242630005,
        "learning_rate": 3.224422763904226e-05,
        "epoch": 1.8257333333333334,
        "step": 13693
    },
    {
        "loss": 2.3015,
        "grad_norm": 3.5695085525512695,
        "learning_rate": 3.219795299668144e-05,
        "epoch": 1.8258666666666667,
        "step": 13694
    },
    {
        "loss": 2.2944,
        "grad_norm": 3.3596718311309814,
        "learning_rate": 3.215170521094257e-05,
        "epoch": 1.826,
        "step": 13695
    },
    {
        "loss": 1.1791,
        "grad_norm": 4.938766956329346,
        "learning_rate": 3.210548430014448e-05,
        "epoch": 1.8261333333333334,
        "step": 13696
    },
    {
        "loss": 2.5098,
        "grad_norm": 2.4858429431915283,
        "learning_rate": 3.205929028259529e-05,
        "epoch": 1.8262666666666667,
        "step": 13697
    },
    {
        "loss": 2.1011,
        "grad_norm": 4.554383754730225,
        "learning_rate": 3.2013123176592807e-05,
        "epoch": 1.8264,
        "step": 13698
    },
    {
        "loss": 2.2299,
        "grad_norm": 3.442699670791626,
        "learning_rate": 3.196698300042382e-05,
        "epoch": 1.8265333333333333,
        "step": 13699
    },
    {
        "loss": 2.078,
        "grad_norm": 3.6816558837890625,
        "learning_rate": 3.192086977236476e-05,
        "epoch": 1.8266666666666667,
        "step": 13700
    },
    {
        "loss": 2.3798,
        "grad_norm": 3.875486135482788,
        "learning_rate": 3.187478351068116e-05,
        "epoch": 1.8268,
        "step": 13701
    },
    {
        "loss": 1.3456,
        "grad_norm": 4.0562639236450195,
        "learning_rate": 3.182872423362797e-05,
        "epoch": 1.8269333333333333,
        "step": 13702
    },
    {
        "loss": 2.1483,
        "grad_norm": 3.8545210361480713,
        "learning_rate": 3.1782691959449275e-05,
        "epoch": 1.8270666666666666,
        "step": 13703
    },
    {
        "loss": 1.8083,
        "grad_norm": 5.431238174438477,
        "learning_rate": 3.173668670637883e-05,
        "epoch": 1.8272,
        "step": 13704
    },
    {
        "loss": 2.6623,
        "grad_norm": 4.314409255981445,
        "learning_rate": 3.169070849263941e-05,
        "epoch": 1.8273333333333333,
        "step": 13705
    },
    {
        "loss": 2.4909,
        "grad_norm": 2.701261520385742,
        "learning_rate": 3.1644757336442986e-05,
        "epoch": 1.8274666666666666,
        "step": 13706
    },
    {
        "loss": 2.5778,
        "grad_norm": 3.4225873947143555,
        "learning_rate": 3.1598833255991236e-05,
        "epoch": 1.8276,
        "step": 13707
    },
    {
        "loss": 2.5977,
        "grad_norm": 3.790897846221924,
        "learning_rate": 3.155293626947466e-05,
        "epoch": 1.8277333333333332,
        "step": 13708
    },
    {
        "loss": 2.5161,
        "grad_norm": 2.148155450820923,
        "learning_rate": 3.150706639507317e-05,
        "epoch": 1.8278666666666665,
        "step": 13709
    },
    {
        "loss": 2.1807,
        "grad_norm": 2.5445048809051514,
        "learning_rate": 3.146122365095618e-05,
        "epoch": 1.8279999999999998,
        "step": 13710
    },
    {
        "loss": 2.1004,
        "grad_norm": 2.586087942123413,
        "learning_rate": 3.1415408055282025e-05,
        "epoch": 1.8281333333333334,
        "step": 13711
    },
    {
        "loss": 2.1393,
        "grad_norm": 3.2975618839263916,
        "learning_rate": 3.136961962619843e-05,
        "epoch": 1.8282666666666667,
        "step": 13712
    },
    {
        "loss": 2.4412,
        "grad_norm": 3.725789785385132,
        "learning_rate": 3.132385838184229e-05,
        "epoch": 1.8284,
        "step": 13713
    },
    {
        "loss": 1.8559,
        "grad_norm": 4.657041072845459,
        "learning_rate": 3.127812434033991e-05,
        "epoch": 1.8285333333333333,
        "step": 13714
    },
    {
        "loss": 0.8501,
        "grad_norm": 7.851393222808838,
        "learning_rate": 3.123241751980656e-05,
        "epoch": 1.8286666666666667,
        "step": 13715
    },
    {
        "loss": 2.7178,
        "grad_norm": 4.285253524780273,
        "learning_rate": 3.118673793834716e-05,
        "epoch": 1.8288,
        "step": 13716
    },
    {
        "loss": 2.5841,
        "grad_norm": 4.182516574859619,
        "learning_rate": 3.1141085614055124e-05,
        "epoch": 1.8289333333333333,
        "step": 13717
    },
    {
        "loss": 2.6807,
        "grad_norm": 2.915602922439575,
        "learning_rate": 3.10954605650137e-05,
        "epoch": 1.8290666666666666,
        "step": 13718
    },
    {
        "loss": 2.482,
        "grad_norm": 2.6586058139801025,
        "learning_rate": 3.104986280929527e-05,
        "epoch": 1.8292000000000002,
        "step": 13719
    },
    {
        "loss": 2.4375,
        "grad_norm": 3.2325172424316406,
        "learning_rate": 3.100429236496109e-05,
        "epoch": 1.8293333333333335,
        "step": 13720
    },
    {
        "loss": 1.9328,
        "grad_norm": 4.462307929992676,
        "learning_rate": 3.0958749250061825e-05,
        "epoch": 1.8294666666666668,
        "step": 13721
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.9839223623275757,
        "learning_rate": 3.091323348263711e-05,
        "epoch": 1.8296000000000001,
        "step": 13722
    },
    {
        "loss": 1.5579,
        "grad_norm": 3.9734699726104736,
        "learning_rate": 3.086774508071611e-05,
        "epoch": 1.8297333333333334,
        "step": 13723
    },
    {
        "loss": 1.7734,
        "grad_norm": 4.046163558959961,
        "learning_rate": 3.0822284062316784e-05,
        "epoch": 1.8298666666666668,
        "step": 13724
    },
    {
        "loss": 2.0762,
        "grad_norm": 3.218601942062378,
        "learning_rate": 3.077685044544663e-05,
        "epoch": 1.83,
        "step": 13725
    },
    {
        "loss": 2.7439,
        "grad_norm": 2.6868844032287598,
        "learning_rate": 3.073144424810168e-05,
        "epoch": 1.8301333333333334,
        "step": 13726
    },
    {
        "loss": 2.5727,
        "grad_norm": 2.5626277923583984,
        "learning_rate": 3.068606548826773e-05,
        "epoch": 1.8302666666666667,
        "step": 13727
    },
    {
        "loss": 1.3644,
        "grad_norm": 3.159658670425415,
        "learning_rate": 3.064071418391948e-05,
        "epoch": 1.8304,
        "step": 13728
    },
    {
        "loss": 2.3796,
        "grad_norm": 3.475635290145874,
        "learning_rate": 3.0595390353020694e-05,
        "epoch": 1.8305333333333333,
        "step": 13729
    },
    {
        "loss": 2.409,
        "grad_norm": 2.808471918106079,
        "learning_rate": 3.05500940135243e-05,
        "epoch": 1.8306666666666667,
        "step": 13730
    },
    {
        "loss": 2.6551,
        "grad_norm": 3.3915960788726807,
        "learning_rate": 3.0504825183372175e-05,
        "epoch": 1.8308,
        "step": 13731
    },
    {
        "loss": 1.8985,
        "grad_norm": 4.5443220138549805,
        "learning_rate": 3.0459583880495735e-05,
        "epoch": 1.8309333333333333,
        "step": 13732
    },
    {
        "loss": 2.3721,
        "grad_norm": 3.25869083404541,
        "learning_rate": 3.041437012281504e-05,
        "epoch": 1.8310666666666666,
        "step": 13733
    },
    {
        "loss": 0.9416,
        "grad_norm": 4.602080821990967,
        "learning_rate": 3.0369183928239443e-05,
        "epoch": 1.8312,
        "step": 13734
    },
    {
        "loss": 1.1915,
        "grad_norm": 3.80741810798645,
        "learning_rate": 3.0324025314667283e-05,
        "epoch": 1.8313333333333333,
        "step": 13735
    },
    {
        "loss": 2.2442,
        "grad_norm": 3.388505458831787,
        "learning_rate": 3.0278894299986082e-05,
        "epoch": 1.8314666666666666,
        "step": 13736
    },
    {
        "loss": 1.0581,
        "grad_norm": 3.4449381828308105,
        "learning_rate": 3.0233790902072522e-05,
        "epoch": 1.8316,
        "step": 13737
    },
    {
        "loss": 2.1923,
        "grad_norm": 3.382159471511841,
        "learning_rate": 3.0188715138792134e-05,
        "epoch": 1.8317333333333332,
        "step": 13738
    },
    {
        "loss": 2.5642,
        "grad_norm": 5.5907087326049805,
        "learning_rate": 3.014366702799952e-05,
        "epoch": 1.8318666666666665,
        "step": 13739
    },
    {
        "loss": 2.6023,
        "grad_norm": 3.680448532104492,
        "learning_rate": 3.009864658753834e-05,
        "epoch": 1.8319999999999999,
        "step": 13740
    },
    {
        "loss": 2.6163,
        "grad_norm": 3.210237741470337,
        "learning_rate": 3.005365383524156e-05,
        "epoch": 1.8321333333333332,
        "step": 13741
    },
    {
        "loss": 0.8763,
        "grad_norm": 4.050689220428467,
        "learning_rate": 3.0008688788930783e-05,
        "epoch": 1.8322666666666667,
        "step": 13742
    },
    {
        "loss": 0.9905,
        "grad_norm": 3.9118409156799316,
        "learning_rate": 2.9963751466416912e-05,
        "epoch": 1.8324,
        "step": 13743
    },
    {
        "loss": 2.0048,
        "grad_norm": 2.8862149715423584,
        "learning_rate": 2.9918841885499594e-05,
        "epoch": 1.8325333333333333,
        "step": 13744
    },
    {
        "loss": 0.5999,
        "grad_norm": 2.590116024017334,
        "learning_rate": 2.98739600639678e-05,
        "epoch": 1.8326666666666667,
        "step": 13745
    },
    {
        "loss": 1.933,
        "grad_norm": 5.238933086395264,
        "learning_rate": 2.9829106019599606e-05,
        "epoch": 1.8328,
        "step": 13746
    },
    {
        "loss": 1.6713,
        "grad_norm": 3.7142269611358643,
        "learning_rate": 2.978427977016138e-05,
        "epoch": 1.8329333333333333,
        "step": 13747
    },
    {
        "loss": 1.6295,
        "grad_norm": 3.2256851196289062,
        "learning_rate": 2.9739481333409302e-05,
        "epoch": 1.8330666666666666,
        "step": 13748
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.8604142665863037,
        "learning_rate": 2.9694710727088003e-05,
        "epoch": 1.8332000000000002,
        "step": 13749
    },
    {
        "loss": 2.379,
        "grad_norm": 2.6866910457611084,
        "learning_rate": 2.9649967968931412e-05,
        "epoch": 1.8333333333333335,
        "step": 13750
    },
    {
        "loss": 1.3784,
        "grad_norm": 4.693917751312256,
        "learning_rate": 2.9605253076662222e-05,
        "epoch": 1.8334666666666668,
        "step": 13751
    },
    {
        "loss": 1.9083,
        "grad_norm": 3.380650043487549,
        "learning_rate": 2.9560566067992178e-05,
        "epoch": 1.8336000000000001,
        "step": 13752
    },
    {
        "loss": 1.5822,
        "grad_norm": 1.934441328048706,
        "learning_rate": 2.951590696062181e-05,
        "epoch": 1.8337333333333334,
        "step": 13753
    },
    {
        "loss": 2.2246,
        "grad_norm": 2.7967989444732666,
        "learning_rate": 2.947127577224098e-05,
        "epoch": 1.8338666666666668,
        "step": 13754
    },
    {
        "loss": 1.9357,
        "grad_norm": 8.22281265258789,
        "learning_rate": 2.942667252052811e-05,
        "epoch": 1.834,
        "step": 13755
    },
    {
        "loss": 1.1831,
        "grad_norm": 3.546759843826294,
        "learning_rate": 2.938209722315064e-05,
        "epoch": 1.8341333333333334,
        "step": 13756
    },
    {
        "loss": 1.877,
        "grad_norm": 3.5394673347473145,
        "learning_rate": 2.9337549897765205e-05,
        "epoch": 1.8342666666666667,
        "step": 13757
    },
    {
        "loss": 2.3782,
        "grad_norm": 3.6789820194244385,
        "learning_rate": 2.929303056201691e-05,
        "epoch": 1.8344,
        "step": 13758
    },
    {
        "loss": 1.9167,
        "grad_norm": 4.104669570922852,
        "learning_rate": 2.9248539233540227e-05,
        "epoch": 1.8345333333333333,
        "step": 13759
    },
    {
        "loss": 2.3748,
        "grad_norm": 3.7568328380584717,
        "learning_rate": 2.9204075929958264e-05,
        "epoch": 1.8346666666666667,
        "step": 13760
    },
    {
        "loss": 1.7874,
        "grad_norm": 4.215263843536377,
        "learning_rate": 2.9159640668883027e-05,
        "epoch": 1.8348,
        "step": 13761
    },
    {
        "loss": 1.7229,
        "grad_norm": 4.674816131591797,
        "learning_rate": 2.9115233467915416e-05,
        "epoch": 1.8349333333333333,
        "step": 13762
    },
    {
        "loss": 2.2785,
        "grad_norm": 3.0543508529663086,
        "learning_rate": 2.9070854344645482e-05,
        "epoch": 1.8350666666666666,
        "step": 13763
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.883382797241211,
        "learning_rate": 2.9026503316651787e-05,
        "epoch": 1.8352,
        "step": 13764
    },
    {
        "loss": 2.0563,
        "grad_norm": 7.438961029052734,
        "learning_rate": 2.898218040150188e-05,
        "epoch": 1.8353333333333333,
        "step": 13765
    },
    {
        "loss": 0.9169,
        "grad_norm": 5.547140598297119,
        "learning_rate": 2.8937885616752426e-05,
        "epoch": 1.8354666666666666,
        "step": 13766
    },
    {
        "loss": 1.0443,
        "grad_norm": 5.159203052520752,
        "learning_rate": 2.8893618979948588e-05,
        "epoch": 1.8356,
        "step": 13767
    },
    {
        "loss": 1.6446,
        "grad_norm": 3.602621555328369,
        "learning_rate": 2.88493805086245e-05,
        "epoch": 1.8357333333333332,
        "step": 13768
    },
    {
        "loss": 2.4497,
        "grad_norm": 2.522110939025879,
        "learning_rate": 2.880517022030329e-05,
        "epoch": 1.8358666666666665,
        "step": 13769
    },
    {
        "loss": 1.2945,
        "grad_norm": 4.166139125823975,
        "learning_rate": 2.8760988132496758e-05,
        "epoch": 1.8359999999999999,
        "step": 13770
    },
    {
        "loss": 1.958,
        "grad_norm": 5.433184623718262,
        "learning_rate": 2.8716834262705584e-05,
        "epoch": 1.8361333333333332,
        "step": 13771
    },
    {
        "loss": 1.4475,
        "grad_norm": 5.116238117218018,
        "learning_rate": 2.867270862841913e-05,
        "epoch": 1.8362666666666667,
        "step": 13772
    },
    {
        "loss": 2.8233,
        "grad_norm": 2.7431130409240723,
        "learning_rate": 2.8628611247115932e-05,
        "epoch": 1.8364,
        "step": 13773
    },
    {
        "loss": 3.3157,
        "grad_norm": 3.020402669906616,
        "learning_rate": 2.8584542136262938e-05,
        "epoch": 1.8365333333333334,
        "step": 13774
    },
    {
        "loss": 2.3337,
        "grad_norm": 2.8657917976379395,
        "learning_rate": 2.8540501313316303e-05,
        "epoch": 1.8366666666666667,
        "step": 13775
    },
    {
        "loss": 2.0497,
        "grad_norm": 3.818533182144165,
        "learning_rate": 2.8496488795720445e-05,
        "epoch": 1.8368,
        "step": 13776
    },
    {
        "loss": 2.4808,
        "grad_norm": 3.9773876667022705,
        "learning_rate": 2.8452504600908968e-05,
        "epoch": 1.8369333333333333,
        "step": 13777
    },
    {
        "loss": 2.6333,
        "grad_norm": 3.129251718521118,
        "learning_rate": 2.840854874630432e-05,
        "epoch": 1.8370666666666666,
        "step": 13778
    },
    {
        "loss": 2.0364,
        "grad_norm": 3.1895811557769775,
        "learning_rate": 2.836462124931748e-05,
        "epoch": 1.8372000000000002,
        "step": 13779
    },
    {
        "loss": 2.2436,
        "grad_norm": 2.895434617996216,
        "learning_rate": 2.832072212734822e-05,
        "epoch": 1.8373333333333335,
        "step": 13780
    },
    {
        "loss": 2.4002,
        "grad_norm": 3.7725582122802734,
        "learning_rate": 2.8276851397785076e-05,
        "epoch": 1.8374666666666668,
        "step": 13781
    },
    {
        "loss": 2.4201,
        "grad_norm": 2.6692371368408203,
        "learning_rate": 2.8233009078005612e-05,
        "epoch": 1.8376000000000001,
        "step": 13782
    },
    {
        "loss": 2.1202,
        "grad_norm": 3.4578914642333984,
        "learning_rate": 2.8189195185375672e-05,
        "epoch": 1.8377333333333334,
        "step": 13783
    },
    {
        "loss": 2.3951,
        "grad_norm": 2.8879151344299316,
        "learning_rate": 2.8145409737250384e-05,
        "epoch": 1.8378666666666668,
        "step": 13784
    },
    {
        "loss": 1.7509,
        "grad_norm": 2.3699567317962646,
        "learning_rate": 2.8101652750972985e-05,
        "epoch": 1.838,
        "step": 13785
    },
    {
        "loss": 1.5376,
        "grad_norm": 5.522393226623535,
        "learning_rate": 2.8057924243875887e-05,
        "epoch": 1.8381333333333334,
        "step": 13786
    },
    {
        "loss": 1.3531,
        "grad_norm": 4.9428582191467285,
        "learning_rate": 2.801422423328023e-05,
        "epoch": 1.8382666666666667,
        "step": 13787
    },
    {
        "loss": 0.9978,
        "grad_norm": 3.6475675106048584,
        "learning_rate": 2.797055273649567e-05,
        "epoch": 1.8384,
        "step": 13788
    },
    {
        "loss": 2.6386,
        "grad_norm": 4.862942218780518,
        "learning_rate": 2.7926909770820575e-05,
        "epoch": 1.8385333333333334,
        "step": 13789
    },
    {
        "loss": 0.7908,
        "grad_norm": 9.121685981750488,
        "learning_rate": 2.7883295353542005e-05,
        "epoch": 1.8386666666666667,
        "step": 13790
    },
    {
        "loss": 0.7622,
        "grad_norm": 3.681879997253418,
        "learning_rate": 2.7839709501936005e-05,
        "epoch": 1.8388,
        "step": 13791
    },
    {
        "loss": 2.3629,
        "grad_norm": 2.8718373775482178,
        "learning_rate": 2.7796152233266926e-05,
        "epoch": 1.8389333333333333,
        "step": 13792
    },
    {
        "loss": 1.4076,
        "grad_norm": 4.478574275970459,
        "learning_rate": 2.7752623564787995e-05,
        "epoch": 1.8390666666666666,
        "step": 13793
    },
    {
        "loss": 2.6406,
        "grad_norm": 3.0970699787139893,
        "learning_rate": 2.770912351374093e-05,
        "epoch": 1.8392,
        "step": 13794
    },
    {
        "loss": 3.296,
        "grad_norm": 4.974465847015381,
        "learning_rate": 2.7665652097356365e-05,
        "epoch": 1.8393333333333333,
        "step": 13795
    },
    {
        "loss": 2.4703,
        "grad_norm": 2.6917457580566406,
        "learning_rate": 2.7622209332853698e-05,
        "epoch": 1.8394666666666666,
        "step": 13796
    },
    {
        "loss": 2.3723,
        "grad_norm": 4.075778484344482,
        "learning_rate": 2.7578795237440314e-05,
        "epoch": 1.8396,
        "step": 13797
    },
    {
        "loss": 0.9831,
        "grad_norm": 3.9418509006500244,
        "learning_rate": 2.7535409828312996e-05,
        "epoch": 1.8397333333333332,
        "step": 13798
    },
    {
        "loss": 1.4694,
        "grad_norm": 3.4779343605041504,
        "learning_rate": 2.7492053122656702e-05,
        "epoch": 1.8398666666666665,
        "step": 13799
    },
    {
        "loss": 2.6574,
        "grad_norm": 2.5010788440704346,
        "learning_rate": 2.7448725137645302e-05,
        "epoch": 1.8399999999999999,
        "step": 13800
    },
    {
        "loss": 1.6556,
        "grad_norm": 5.492433547973633,
        "learning_rate": 2.7405425890441104e-05,
        "epoch": 1.8401333333333332,
        "step": 13801
    },
    {
        "loss": 2.4064,
        "grad_norm": 3.7677109241485596,
        "learning_rate": 2.7362155398195044e-05,
        "epoch": 1.8402666666666667,
        "step": 13802
    },
    {
        "loss": 1.975,
        "grad_norm": 3.9214210510253906,
        "learning_rate": 2.7318913678046655e-05,
        "epoch": 1.8404,
        "step": 13803
    },
    {
        "loss": 2.8823,
        "grad_norm": 3.5837130546569824,
        "learning_rate": 2.7275700747124212e-05,
        "epoch": 1.8405333333333334,
        "step": 13804
    },
    {
        "loss": 2.0888,
        "grad_norm": 4.683788776397705,
        "learning_rate": 2.7232516622544714e-05,
        "epoch": 1.8406666666666667,
        "step": 13805
    },
    {
        "loss": 1.453,
        "grad_norm": 5.429698944091797,
        "learning_rate": 2.718936132141311e-05,
        "epoch": 1.8408,
        "step": 13806
    },
    {
        "loss": 1.0026,
        "grad_norm": 5.531816005706787,
        "learning_rate": 2.7146234860823715e-05,
        "epoch": 1.8409333333333333,
        "step": 13807
    },
    {
        "loss": 1.7257,
        "grad_norm": 3.6024656295776367,
        "learning_rate": 2.71031372578588e-05,
        "epoch": 1.8410666666666666,
        "step": 13808
    },
    {
        "loss": 1.4838,
        "grad_norm": 5.447627544403076,
        "learning_rate": 2.706006852958971e-05,
        "epoch": 1.8412,
        "step": 13809
    },
    {
        "loss": 1.8237,
        "grad_norm": 4.1918439865112305,
        "learning_rate": 2.701702869307604e-05,
        "epoch": 1.8413333333333335,
        "step": 13810
    },
    {
        "loss": 2.6206,
        "grad_norm": 3.268798351287842,
        "learning_rate": 2.6974017765365967e-05,
        "epoch": 1.8414666666666668,
        "step": 13811
    },
    {
        "loss": 1.4092,
        "grad_norm": 4.375943660736084,
        "learning_rate": 2.6931035763496182e-05,
        "epoch": 1.8416000000000001,
        "step": 13812
    },
    {
        "loss": 3.307,
        "grad_norm": 3.6123993396759033,
        "learning_rate": 2.6888082704492235e-05,
        "epoch": 1.8417333333333334,
        "step": 13813
    },
    {
        "loss": 2.0043,
        "grad_norm": 4.451666355133057,
        "learning_rate": 2.6845158605367827e-05,
        "epoch": 1.8418666666666668,
        "step": 13814
    },
    {
        "loss": 2.626,
        "grad_norm": 3.323639154434204,
        "learning_rate": 2.680226348312529e-05,
        "epoch": 1.842,
        "step": 13815
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.5334033966064453,
        "learning_rate": 2.6759397354755666e-05,
        "epoch": 1.8421333333333334,
        "step": 13816
    },
    {
        "loss": 2.5862,
        "grad_norm": 3.074955940246582,
        "learning_rate": 2.671656023723823e-05,
        "epoch": 1.8422666666666667,
        "step": 13817
    },
    {
        "loss": 2.6244,
        "grad_norm": 2.308103322982788,
        "learning_rate": 2.667375214754111e-05,
        "epoch": 1.8424,
        "step": 13818
    },
    {
        "loss": 2.0401,
        "grad_norm": 4.824626445770264,
        "learning_rate": 2.6630973102620582e-05,
        "epoch": 1.8425333333333334,
        "step": 13819
    },
    {
        "loss": 2.6092,
        "grad_norm": 3.373316764831543,
        "learning_rate": 2.6588223119421607e-05,
        "epoch": 1.8426666666666667,
        "step": 13820
    },
    {
        "loss": 1.6358,
        "grad_norm": 3.24222993850708,
        "learning_rate": 2.6545502214877538e-05,
        "epoch": 1.8428,
        "step": 13821
    },
    {
        "loss": 2.2984,
        "grad_norm": 3.017500400543213,
        "learning_rate": 2.6502810405910393e-05,
        "epoch": 1.8429333333333333,
        "step": 13822
    },
    {
        "loss": 2.1849,
        "grad_norm": 3.8261444568634033,
        "learning_rate": 2.6460147709430517e-05,
        "epoch": 1.8430666666666666,
        "step": 13823
    },
    {
        "loss": 1.6399,
        "grad_norm": 3.983595132827759,
        "learning_rate": 2.6417514142336586e-05,
        "epoch": 1.8432,
        "step": 13824
    },
    {
        "loss": 1.9861,
        "grad_norm": 4.568429470062256,
        "learning_rate": 2.6374909721516127e-05,
        "epoch": 1.8433333333333333,
        "step": 13825
    },
    {
        "loss": 1.5792,
        "grad_norm": 4.7065582275390625,
        "learning_rate": 2.6332334463844822e-05,
        "epoch": 1.8434666666666666,
        "step": 13826
    },
    {
        "loss": 1.2867,
        "grad_norm": 3.5661020278930664,
        "learning_rate": 2.6289788386186743e-05,
        "epoch": 1.8436,
        "step": 13827
    },
    {
        "loss": 1.7994,
        "grad_norm": 4.326706409454346,
        "learning_rate": 2.6247271505394723e-05,
        "epoch": 1.8437333333333332,
        "step": 13828
    },
    {
        "loss": 1.1724,
        "grad_norm": 4.933163166046143,
        "learning_rate": 2.62047838383098e-05,
        "epoch": 1.8438666666666665,
        "step": 13829
    },
    {
        "loss": 2.9785,
        "grad_norm": 3.461688756942749,
        "learning_rate": 2.6162325401761422e-05,
        "epoch": 1.8439999999999999,
        "step": 13830
    },
    {
        "loss": 1.2253,
        "grad_norm": 4.2953715324401855,
        "learning_rate": 2.611989621256745e-05,
        "epoch": 1.8441333333333332,
        "step": 13831
    },
    {
        "loss": 2.0464,
        "grad_norm": 5.370014667510986,
        "learning_rate": 2.6077496287534463e-05,
        "epoch": 1.8442666666666667,
        "step": 13832
    },
    {
        "loss": 2.083,
        "grad_norm": 4.272441387176514,
        "learning_rate": 2.6035125643456946e-05,
        "epoch": 1.8444,
        "step": 13833
    },
    {
        "loss": 2.0088,
        "grad_norm": 3.991467237472534,
        "learning_rate": 2.599278429711841e-05,
        "epoch": 1.8445333333333334,
        "step": 13834
    },
    {
        "loss": 2.3629,
        "grad_norm": 4.063496112823486,
        "learning_rate": 2.5950472265289982e-05,
        "epoch": 1.8446666666666667,
        "step": 13835
    },
    {
        "loss": 2.2261,
        "grad_norm": 5.099488735198975,
        "learning_rate": 2.59081895647318e-05,
        "epoch": 1.8448,
        "step": 13836
    },
    {
        "loss": 1.3332,
        "grad_norm": 4.593521595001221,
        "learning_rate": 2.5865936212192278e-05,
        "epoch": 1.8449333333333333,
        "step": 13837
    },
    {
        "loss": 2.63,
        "grad_norm": 3.4520084857940674,
        "learning_rate": 2.582371222440807e-05,
        "epoch": 1.8450666666666666,
        "step": 13838
    },
    {
        "loss": 2.3825,
        "grad_norm": 4.105353832244873,
        "learning_rate": 2.5781517618104157e-05,
        "epoch": 1.8452,
        "step": 13839
    },
    {
        "loss": 2.546,
        "grad_norm": 3.288919448852539,
        "learning_rate": 2.5739352409993957e-05,
        "epoch": 1.8453333333333335,
        "step": 13840
    },
    {
        "loss": 2.2618,
        "grad_norm": 2.2998664379119873,
        "learning_rate": 2.5697216616779362e-05,
        "epoch": 1.8454666666666668,
        "step": 13841
    },
    {
        "loss": 2.6738,
        "grad_norm": 3.941554307937622,
        "learning_rate": 2.565511025515036e-05,
        "epoch": 1.8456000000000001,
        "step": 13842
    },
    {
        "loss": 2.1597,
        "grad_norm": 3.7321290969848633,
        "learning_rate": 2.561303334178571e-05,
        "epoch": 1.8457333333333334,
        "step": 13843
    },
    {
        "loss": 2.6492,
        "grad_norm": 2.005465507507324,
        "learning_rate": 2.5570985893351818e-05,
        "epoch": 1.8458666666666668,
        "step": 13844
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.425208568572998,
        "learning_rate": 2.552896792650402e-05,
        "epoch": 1.846,
        "step": 13845
    },
    {
        "loss": 2.4046,
        "grad_norm": 2.1238160133361816,
        "learning_rate": 2.5486979457885907e-05,
        "epoch": 1.8461333333333334,
        "step": 13846
    },
    {
        "loss": 2.3874,
        "grad_norm": 3.692932605743408,
        "learning_rate": 2.544502050412909e-05,
        "epoch": 1.8462666666666667,
        "step": 13847
    },
    {
        "loss": 2.2247,
        "grad_norm": 4.849466800689697,
        "learning_rate": 2.5403091081853726e-05,
        "epoch": 1.8464,
        "step": 13848
    },
    {
        "loss": 2.3907,
        "grad_norm": 3.318366527557373,
        "learning_rate": 2.5361191207668056e-05,
        "epoch": 1.8465333333333334,
        "step": 13849
    },
    {
        "loss": 2.3306,
        "grad_norm": 3.2708160877227783,
        "learning_rate": 2.531932089816895e-05,
        "epoch": 1.8466666666666667,
        "step": 13850
    },
    {
        "loss": 1.6737,
        "grad_norm": 4.954087257385254,
        "learning_rate": 2.5277480169941337e-05,
        "epoch": 1.8468,
        "step": 13851
    },
    {
        "loss": 1.9704,
        "grad_norm": 3.399172306060791,
        "learning_rate": 2.523566903955841e-05,
        "epoch": 1.8469333333333333,
        "step": 13852
    },
    {
        "loss": 2.4956,
        "grad_norm": 2.8311519622802734,
        "learning_rate": 2.519388752358164e-05,
        "epoch": 1.8470666666666666,
        "step": 13853
    },
    {
        "loss": 1.9517,
        "grad_norm": 3.2509710788726807,
        "learning_rate": 2.5152135638560946e-05,
        "epoch": 1.8472,
        "step": 13854
    },
    {
        "loss": 2.2115,
        "grad_norm": 4.650594234466553,
        "learning_rate": 2.5110413401034517e-05,
        "epoch": 1.8473333333333333,
        "step": 13855
    },
    {
        "loss": 1.7672,
        "grad_norm": 3.3918704986572266,
        "learning_rate": 2.5068720827528337e-05,
        "epoch": 1.8474666666666666,
        "step": 13856
    },
    {
        "loss": 1.9852,
        "grad_norm": 3.5685558319091797,
        "learning_rate": 2.5027057934557284e-05,
        "epoch": 1.8476,
        "step": 13857
    },
    {
        "loss": 2.1913,
        "grad_norm": 2.6518173217773438,
        "learning_rate": 2.498542473862393e-05,
        "epoch": 1.8477333333333332,
        "step": 13858
    },
    {
        "loss": 1.8975,
        "grad_norm": 2.8623015880584717,
        "learning_rate": 2.4943821256219525e-05,
        "epoch": 1.8478666666666665,
        "step": 13859
    },
    {
        "loss": 0.7831,
        "grad_norm": 3.9093284606933594,
        "learning_rate": 2.4902247503823293e-05,
        "epoch": 1.8479999999999999,
        "step": 13860
    },
    {
        "loss": 1.6331,
        "grad_norm": 4.543991565704346,
        "learning_rate": 2.4860703497902692e-05,
        "epoch": 1.8481333333333332,
        "step": 13861
    },
    {
        "loss": 2.7379,
        "grad_norm": 2.4708383083343506,
        "learning_rate": 2.481918925491341e-05,
        "epoch": 1.8482666666666665,
        "step": 13862
    },
    {
        "loss": 1.083,
        "grad_norm": 5.3807573318481445,
        "learning_rate": 2.4777704791299405e-05,
        "epoch": 1.8484,
        "step": 13863
    },
    {
        "loss": 1.9033,
        "grad_norm": 5.318825721740723,
        "learning_rate": 2.473625012349301e-05,
        "epoch": 1.8485333333333334,
        "step": 13864
    },
    {
        "loss": 2.2798,
        "grad_norm": 4.2915802001953125,
        "learning_rate": 2.469482526791427e-05,
        "epoch": 1.8486666666666667,
        "step": 13865
    },
    {
        "loss": 1.836,
        "grad_norm": 4.872453689575195,
        "learning_rate": 2.4653430240971877e-05,
        "epoch": 1.8488,
        "step": 13866
    },
    {
        "loss": 2.7531,
        "grad_norm": 3.2393288612365723,
        "learning_rate": 2.4612065059062427e-05,
        "epoch": 1.8489333333333333,
        "step": 13867
    },
    {
        "loss": 2.4286,
        "grad_norm": 3.649466037750244,
        "learning_rate": 2.4570729738570996e-05,
        "epoch": 1.8490666666666666,
        "step": 13868
    },
    {
        "loss": 2.7627,
        "grad_norm": 3.1998796463012695,
        "learning_rate": 2.4529424295870507e-05,
        "epoch": 1.8492,
        "step": 13869
    },
    {
        "loss": 1.2961,
        "grad_norm": 4.578569412231445,
        "learning_rate": 2.448814874732225e-05,
        "epoch": 1.8493333333333335,
        "step": 13870
    },
    {
        "loss": 2.3038,
        "grad_norm": 4.615584850311279,
        "learning_rate": 2.4446903109275456e-05,
        "epoch": 1.8494666666666668,
        "step": 13871
    },
    {
        "loss": 1.1681,
        "grad_norm": 4.2523016929626465,
        "learning_rate": 2.4405687398067857e-05,
        "epoch": 1.8496000000000001,
        "step": 13872
    },
    {
        "loss": 2.6751,
        "grad_norm": 4.120087623596191,
        "learning_rate": 2.4364501630025105e-05,
        "epoch": 1.8497333333333335,
        "step": 13873
    },
    {
        "loss": 0.7295,
        "grad_norm": 3.388005256652832,
        "learning_rate": 2.432334582146085e-05,
        "epoch": 1.8498666666666668,
        "step": 13874
    },
    {
        "loss": 2.5438,
        "grad_norm": 4.292893409729004,
        "learning_rate": 2.4282219988677268e-05,
        "epoch": 1.85,
        "step": 13875
    },
    {
        "loss": 2.1484,
        "grad_norm": 5.903909683227539,
        "learning_rate": 2.4241124147964377e-05,
        "epoch": 1.8501333333333334,
        "step": 13876
    },
    {
        "loss": 2.5236,
        "grad_norm": 4.150976657867432,
        "learning_rate": 2.420005831560025e-05,
        "epoch": 1.8502666666666667,
        "step": 13877
    },
    {
        "loss": 2.3738,
        "grad_norm": 2.599990129470825,
        "learning_rate": 2.415902250785138e-05,
        "epoch": 1.8504,
        "step": 13878
    },
    {
        "loss": 1.8641,
        "grad_norm": 4.475644111633301,
        "learning_rate": 2.411801674097215e-05,
        "epoch": 1.8505333333333334,
        "step": 13879
    },
    {
        "loss": 1.2462,
        "grad_norm": 4.592168807983398,
        "learning_rate": 2.407704103120494e-05,
        "epoch": 1.8506666666666667,
        "step": 13880
    },
    {
        "loss": 0.9808,
        "grad_norm": 6.06296443939209,
        "learning_rate": 2.4036095394780557e-05,
        "epoch": 1.8508,
        "step": 13881
    },
    {
        "loss": 2.1677,
        "grad_norm": 4.4364213943481445,
        "learning_rate": 2.399517984791766e-05,
        "epoch": 1.8509333333333333,
        "step": 13882
    },
    {
        "loss": 2.5645,
        "grad_norm": 3.9302613735198975,
        "learning_rate": 2.3954294406822886e-05,
        "epoch": 1.8510666666666666,
        "step": 13883
    },
    {
        "loss": 2.1574,
        "grad_norm": 3.5268654823303223,
        "learning_rate": 2.3913439087691312e-05,
        "epoch": 1.8512,
        "step": 13884
    },
    {
        "loss": 2.3323,
        "grad_norm": 4.449306964874268,
        "learning_rate": 2.38726139067058e-05,
        "epoch": 1.8513333333333333,
        "step": 13885
    },
    {
        "loss": 1.9397,
        "grad_norm": 4.734004497528076,
        "learning_rate": 2.3831818880037193e-05,
        "epoch": 1.8514666666666666,
        "step": 13886
    },
    {
        "loss": 1.8623,
        "grad_norm": 3.75799298286438,
        "learning_rate": 2.3791054023844795e-05,
        "epoch": 1.8516,
        "step": 13887
    },
    {
        "loss": 0.8412,
        "grad_norm": 3.3776357173919678,
        "learning_rate": 2.375031935427553e-05,
        "epoch": 1.8517333333333332,
        "step": 13888
    },
    {
        "loss": 1.9782,
        "grad_norm": 3.3549420833587646,
        "learning_rate": 2.3709614887464594e-05,
        "epoch": 1.8518666666666665,
        "step": 13889
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.592698335647583,
        "learning_rate": 2.36689406395351e-05,
        "epoch": 1.8519999999999999,
        "step": 13890
    },
    {
        "loss": 2.0192,
        "grad_norm": 5.491213798522949,
        "learning_rate": 2.3628296626598357e-05,
        "epoch": 1.8521333333333332,
        "step": 13891
    },
    {
        "loss": 1.7394,
        "grad_norm": 3.990612030029297,
        "learning_rate": 2.3587682864753468e-05,
        "epoch": 1.8522666666666665,
        "step": 13892
    },
    {
        "loss": 2.7296,
        "grad_norm": 2.9606034755706787,
        "learning_rate": 2.3547099370087967e-05,
        "epoch": 1.8524,
        "step": 13893
    },
    {
        "loss": 2.317,
        "grad_norm": 3.119915723800659,
        "learning_rate": 2.35065461586767e-05,
        "epoch": 1.8525333333333334,
        "step": 13894
    },
    {
        "loss": 2.3934,
        "grad_norm": 3.1991958618164062,
        "learning_rate": 2.3466023246583158e-05,
        "epoch": 1.8526666666666667,
        "step": 13895
    },
    {
        "loss": 2.0253,
        "grad_norm": 2.3912723064422607,
        "learning_rate": 2.3425530649858707e-05,
        "epoch": 1.8528,
        "step": 13896
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.700439929962158,
        "learning_rate": 2.338506838454245e-05,
        "epoch": 1.8529333333333333,
        "step": 13897
    },
    {
        "loss": 1.8631,
        "grad_norm": 3.657343626022339,
        "learning_rate": 2.3344636466661695e-05,
        "epoch": 1.8530666666666666,
        "step": 13898
    },
    {
        "loss": 2.6905,
        "grad_norm": 3.592947244644165,
        "learning_rate": 2.3304234912231525e-05,
        "epoch": 1.8532,
        "step": 13899
    },
    {
        "loss": 2.1506,
        "grad_norm": 3.669893980026245,
        "learning_rate": 2.326386373725532e-05,
        "epoch": 1.8533333333333335,
        "step": 13900
    },
    {
        "loss": 2.1584,
        "grad_norm": 5.543370246887207,
        "learning_rate": 2.322352295772411e-05,
        "epoch": 1.8534666666666668,
        "step": 13901
    },
    {
        "loss": 2.4323,
        "grad_norm": 1.952925205230713,
        "learning_rate": 2.3183212589617197e-05,
        "epoch": 1.8536000000000001,
        "step": 13902
    },
    {
        "loss": 2.2218,
        "grad_norm": 5.119688034057617,
        "learning_rate": 2.3142932648901362e-05,
        "epoch": 1.8537333333333335,
        "step": 13903
    },
    {
        "loss": 2.8168,
        "grad_norm": 3.0441782474517822,
        "learning_rate": 2.310268315153181e-05,
        "epoch": 1.8538666666666668,
        "step": 13904
    },
    {
        "loss": 1.3135,
        "grad_norm": 3.612872838973999,
        "learning_rate": 2.3062464113451555e-05,
        "epoch": 1.854,
        "step": 13905
    },
    {
        "loss": 2.3379,
        "grad_norm": 3.533006429672241,
        "learning_rate": 2.302227555059141e-05,
        "epoch": 1.8541333333333334,
        "step": 13906
    },
    {
        "loss": 3.1694,
        "grad_norm": 4.162333011627197,
        "learning_rate": 2.2982117478870247e-05,
        "epoch": 1.8542666666666667,
        "step": 13907
    },
    {
        "loss": 2.3756,
        "grad_norm": 4.121949672698975,
        "learning_rate": 2.2941989914194662e-05,
        "epoch": 1.8544,
        "step": 13908
    },
    {
        "loss": 1.8182,
        "grad_norm": 3.256692409515381,
        "learning_rate": 2.290189287245956e-05,
        "epoch": 1.8545333333333334,
        "step": 13909
    },
    {
        "loss": 2.1965,
        "grad_norm": 4.321913242340088,
        "learning_rate": 2.286182636954737e-05,
        "epoch": 1.8546666666666667,
        "step": 13910
    },
    {
        "loss": 1.379,
        "grad_norm": 4.339295387268066,
        "learning_rate": 2.2821790421328593e-05,
        "epoch": 1.8548,
        "step": 13911
    },
    {
        "loss": 2.3911,
        "grad_norm": 3.3212759494781494,
        "learning_rate": 2.278178504366155e-05,
        "epoch": 1.8549333333333333,
        "step": 13912
    },
    {
        "loss": 1.9071,
        "grad_norm": 4.833484649658203,
        "learning_rate": 2.274181025239255e-05,
        "epoch": 1.8550666666666666,
        "step": 13913
    },
    {
        "loss": 1.6267,
        "grad_norm": 3.7290186882019043,
        "learning_rate": 2.2701866063355914e-05,
        "epoch": 1.8552,
        "step": 13914
    },
    {
        "loss": 2.0517,
        "grad_norm": 4.3028435707092285,
        "learning_rate": 2.2661952492373384e-05,
        "epoch": 1.8553333333333333,
        "step": 13915
    },
    {
        "loss": 2.7676,
        "grad_norm": 2.3926544189453125,
        "learning_rate": 2.2622069555255054e-05,
        "epoch": 1.8554666666666666,
        "step": 13916
    },
    {
        "loss": 2.6647,
        "grad_norm": 3.717322826385498,
        "learning_rate": 2.2582217267798545e-05,
        "epoch": 1.8556,
        "step": 13917
    },
    {
        "loss": 2.7042,
        "grad_norm": 3.645325183868408,
        "learning_rate": 2.2542395645789682e-05,
        "epoch": 1.8557333333333332,
        "step": 13918
    },
    {
        "loss": 1.3221,
        "grad_norm": 2.354418992996216,
        "learning_rate": 2.2502604705001807e-05,
        "epoch": 1.8558666666666666,
        "step": 13919
    },
    {
        "loss": 2.0846,
        "grad_norm": 3.4363298416137695,
        "learning_rate": 2.24628444611963e-05,
        "epoch": 1.8559999999999999,
        "step": 13920
    },
    {
        "loss": 1.1415,
        "grad_norm": 5.7166852951049805,
        "learning_rate": 2.242311493012219e-05,
        "epoch": 1.8561333333333332,
        "step": 13921
    },
    {
        "loss": 1.8688,
        "grad_norm": 3.995298147201538,
        "learning_rate": 2.238341612751659e-05,
        "epoch": 1.8562666666666665,
        "step": 13922
    },
    {
        "loss": 1.1508,
        "grad_norm": 3.3818633556365967,
        "learning_rate": 2.234374806910454e-05,
        "epoch": 1.8564,
        "step": 13923
    },
    {
        "loss": 1.9384,
        "grad_norm": 3.4121007919311523,
        "learning_rate": 2.2304110770598286e-05,
        "epoch": 1.8565333333333334,
        "step": 13924
    },
    {
        "loss": 1.5462,
        "grad_norm": 2.488327741622925,
        "learning_rate": 2.226450424769858e-05,
        "epoch": 1.8566666666666667,
        "step": 13925
    },
    {
        "loss": 0.7686,
        "grad_norm": 3.878495693206787,
        "learning_rate": 2.2224928516093568e-05,
        "epoch": 1.8568,
        "step": 13926
    },
    {
        "loss": 1.2891,
        "grad_norm": 3.8786110877990723,
        "learning_rate": 2.218538359145943e-05,
        "epoch": 1.8569333333333333,
        "step": 13927
    },
    {
        "loss": 1.8404,
        "grad_norm": 4.367557048797607,
        "learning_rate": 2.2145869489460004e-05,
        "epoch": 1.8570666666666666,
        "step": 13928
    },
    {
        "loss": 1.9937,
        "grad_norm": 3.7232704162597656,
        "learning_rate": 2.210638622574699e-05,
        "epoch": 1.8572,
        "step": 13929
    },
    {
        "loss": 2.4495,
        "grad_norm": 2.213383436203003,
        "learning_rate": 2.206693381595968e-05,
        "epoch": 1.8573333333333333,
        "step": 13930
    },
    {
        "loss": 1.6749,
        "grad_norm": 3.983398199081421,
        "learning_rate": 2.202751227572556e-05,
        "epoch": 1.8574666666666668,
        "step": 13931
    },
    {
        "loss": 2.8743,
        "grad_norm": 3.5640480518341064,
        "learning_rate": 2.1988121620659473e-05,
        "epoch": 1.8576000000000001,
        "step": 13932
    },
    {
        "loss": 1.8701,
        "grad_norm": 3.671156167984009,
        "learning_rate": 2.1948761866364166e-05,
        "epoch": 1.8577333333333335,
        "step": 13933
    },
    {
        "loss": 2.6383,
        "grad_norm": 3.265552043914795,
        "learning_rate": 2.1909433028430337e-05,
        "epoch": 1.8578666666666668,
        "step": 13934
    },
    {
        "loss": 2.3336,
        "grad_norm": 3.697873592376709,
        "learning_rate": 2.187013512243614e-05,
        "epoch": 1.858,
        "step": 13935
    },
    {
        "loss": 3.0762,
        "grad_norm": 2.268101930618286,
        "learning_rate": 2.183086816394756e-05,
        "epoch": 1.8581333333333334,
        "step": 13936
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.8373208045959473,
        "learning_rate": 2.179163216851857e-05,
        "epoch": 1.8582666666666667,
        "step": 13937
    },
    {
        "loss": 2.2551,
        "grad_norm": 2.8790504932403564,
        "learning_rate": 2.1752427151690546e-05,
        "epoch": 1.8584,
        "step": 13938
    },
    {
        "loss": 2.03,
        "grad_norm": 2.2831850051879883,
        "learning_rate": 2.171325312899265e-05,
        "epoch": 1.8585333333333334,
        "step": 13939
    },
    {
        "loss": 1.4807,
        "grad_norm": 3.4375362396240234,
        "learning_rate": 2.167411011594207e-05,
        "epoch": 1.8586666666666667,
        "step": 13940
    },
    {
        "loss": 2.2508,
        "grad_norm": 3.7098500728607178,
        "learning_rate": 2.1634998128043328e-05,
        "epoch": 1.8588,
        "step": 13941
    },
    {
        "loss": 1.9657,
        "grad_norm": 4.503973484039307,
        "learning_rate": 2.1595917180788772e-05,
        "epoch": 1.8589333333333333,
        "step": 13942
    },
    {
        "loss": 2.543,
        "grad_norm": 3.063286542892456,
        "learning_rate": 2.1556867289658678e-05,
        "epoch": 1.8590666666666666,
        "step": 13943
    },
    {
        "loss": 2.4776,
        "grad_norm": 3.831850528717041,
        "learning_rate": 2.1517848470120715e-05,
        "epoch": 1.8592,
        "step": 13944
    },
    {
        "loss": 2.3444,
        "grad_norm": 4.303174018859863,
        "learning_rate": 2.1478860737630313e-05,
        "epoch": 1.8593333333333333,
        "step": 13945
    },
    {
        "loss": 1.2568,
        "grad_norm": 4.14085578918457,
        "learning_rate": 2.1439904107630836e-05,
        "epoch": 1.8594666666666666,
        "step": 13946
    },
    {
        "loss": 1.6596,
        "grad_norm": 4.151257038116455,
        "learning_rate": 2.1400978595553e-05,
        "epoch": 1.8596,
        "step": 13947
    },
    {
        "loss": 1.4856,
        "grad_norm": 4.290428638458252,
        "learning_rate": 2.1362084216815392e-05,
        "epoch": 1.8597333333333332,
        "step": 13948
    },
    {
        "loss": 2.1727,
        "grad_norm": 4.0491180419921875,
        "learning_rate": 2.1323220986824067e-05,
        "epoch": 1.8598666666666666,
        "step": 13949
    },
    {
        "loss": 2.7822,
        "grad_norm": 3.512024164199829,
        "learning_rate": 2.1284388920973075e-05,
        "epoch": 1.8599999999999999,
        "step": 13950
    },
    {
        "loss": 1.7504,
        "grad_norm": 3.66347599029541,
        "learning_rate": 2.1245588034643792e-05,
        "epoch": 1.8601333333333332,
        "step": 13951
    },
    {
        "loss": 2.1274,
        "grad_norm": 4.495543479919434,
        "learning_rate": 2.120681834320558e-05,
        "epoch": 1.8602666666666665,
        "step": 13952
    },
    {
        "loss": 0.6245,
        "grad_norm": 3.674593925476074,
        "learning_rate": 2.1168079862014935e-05,
        "epoch": 1.8604,
        "step": 13953
    },
    {
        "loss": 1.9986,
        "grad_norm": 3.199281930923462,
        "learning_rate": 2.11293726064165e-05,
        "epoch": 1.8605333333333334,
        "step": 13954
    },
    {
        "loss": 2.2469,
        "grad_norm": 3.0528817176818848,
        "learning_rate": 2.1090696591742377e-05,
        "epoch": 1.8606666666666667,
        "step": 13955
    },
    {
        "loss": 2.248,
        "grad_norm": 2.772968292236328,
        "learning_rate": 2.1052051833312237e-05,
        "epoch": 1.8608,
        "step": 13956
    },
    {
        "loss": 1.6031,
        "grad_norm": 3.5419368743896484,
        "learning_rate": 2.1013438346433347e-05,
        "epoch": 1.8609333333333333,
        "step": 13957
    },
    {
        "loss": 2.1728,
        "grad_norm": 3.043055772781372,
        "learning_rate": 2.097485614640058e-05,
        "epoch": 1.8610666666666666,
        "step": 13958
    },
    {
        "loss": 2.3441,
        "grad_norm": 3.631542205810547,
        "learning_rate": 2.093630524849668e-05,
        "epoch": 1.8612,
        "step": 13959
    },
    {
        "loss": 2.4591,
        "grad_norm": 3.335810422897339,
        "learning_rate": 2.0897785667991555e-05,
        "epoch": 1.8613333333333333,
        "step": 13960
    },
    {
        "loss": 1.444,
        "grad_norm": 3.636214256286621,
        "learning_rate": 2.085929742014323e-05,
        "epoch": 1.8614666666666668,
        "step": 13961
    },
    {
        "loss": 2.4865,
        "grad_norm": 3.3175811767578125,
        "learning_rate": 2.0820840520196738e-05,
        "epoch": 1.8616000000000001,
        "step": 13962
    },
    {
        "loss": 1.6152,
        "grad_norm": 4.7594075202941895,
        "learning_rate": 2.078241498338508e-05,
        "epoch": 1.8617333333333335,
        "step": 13963
    },
    {
        "loss": 2.1973,
        "grad_norm": 3.658627986907959,
        "learning_rate": 2.0744020824928845e-05,
        "epoch": 1.8618666666666668,
        "step": 13964
    },
    {
        "loss": 1.3805,
        "grad_norm": 4.589850902557373,
        "learning_rate": 2.0705658060036058e-05,
        "epoch": 1.862,
        "step": 13965
    },
    {
        "loss": 2.2601,
        "grad_norm": 4.0326828956604,
        "learning_rate": 2.0667326703902267e-05,
        "epoch": 1.8621333333333334,
        "step": 13966
    },
    {
        "loss": 1.8223,
        "grad_norm": 4.061470031738281,
        "learning_rate": 2.0629026771710613e-05,
        "epoch": 1.8622666666666667,
        "step": 13967
    },
    {
        "loss": 1.2316,
        "grad_norm": 4.1794281005859375,
        "learning_rate": 2.0590758278632004e-05,
        "epoch": 1.8624,
        "step": 13968
    },
    {
        "loss": 2.02,
        "grad_norm": 3.4285976886749268,
        "learning_rate": 2.0552521239824607e-05,
        "epoch": 1.8625333333333334,
        "step": 13969
    },
    {
        "loss": 2.3153,
        "grad_norm": 3.810666799545288,
        "learning_rate": 2.051431567043427e-05,
        "epoch": 1.8626666666666667,
        "step": 13970
    },
    {
        "loss": 2.0144,
        "grad_norm": 4.6321282386779785,
        "learning_rate": 2.0476141585594243e-05,
        "epoch": 1.8628,
        "step": 13971
    },
    {
        "loss": 2.3917,
        "grad_norm": 3.315300464630127,
        "learning_rate": 2.04379990004255e-05,
        "epoch": 1.8629333333333333,
        "step": 13972
    },
    {
        "loss": 0.8288,
        "grad_norm": 3.2368698120117188,
        "learning_rate": 2.0399887930036642e-05,
        "epoch": 1.8630666666666666,
        "step": 13973
    },
    {
        "loss": 2.1708,
        "grad_norm": 3.043217897415161,
        "learning_rate": 2.036180838952322e-05,
        "epoch": 1.8632,
        "step": 13974
    },
    {
        "loss": 1.7662,
        "grad_norm": 4.479806900024414,
        "learning_rate": 2.0323760393968926e-05,
        "epoch": 1.8633333333333333,
        "step": 13975
    },
    {
        "loss": 1.6771,
        "grad_norm": 4.284521102905273,
        "learning_rate": 2.0285743958444582e-05,
        "epoch": 1.8634666666666666,
        "step": 13976
    },
    {
        "loss": 1.8108,
        "grad_norm": 2.6928720474243164,
        "learning_rate": 2.0247759098008733e-05,
        "epoch": 1.8636,
        "step": 13977
    },
    {
        "loss": 1.9862,
        "grad_norm": 4.298998832702637,
        "learning_rate": 2.0209805827707272e-05,
        "epoch": 1.8637333333333332,
        "step": 13978
    },
    {
        "loss": 2.5975,
        "grad_norm": 3.2688496112823486,
        "learning_rate": 2.0171884162573583e-05,
        "epoch": 1.8638666666666666,
        "step": 13979
    },
    {
        "loss": 0.9667,
        "grad_norm": 3.3065741062164307,
        "learning_rate": 2.0133994117628506e-05,
        "epoch": 1.8639999999999999,
        "step": 13980
    },
    {
        "loss": 1.2271,
        "grad_norm": 2.4407432079315186,
        "learning_rate": 2.0096135707880492e-05,
        "epoch": 1.8641333333333332,
        "step": 13981
    },
    {
        "loss": 2.1037,
        "grad_norm": 3.776499032974243,
        "learning_rate": 2.0058308948325578e-05,
        "epoch": 1.8642666666666665,
        "step": 13982
    },
    {
        "loss": 2.014,
        "grad_norm": 3.5244860649108887,
        "learning_rate": 2.002051385394669e-05,
        "epoch": 1.8643999999999998,
        "step": 13983
    },
    {
        "loss": 1.0384,
        "grad_norm": 4.125181198120117,
        "learning_rate": 1.9982750439714902e-05,
        "epoch": 1.8645333333333334,
        "step": 13984
    },
    {
        "loss": 1.812,
        "grad_norm": 4.313133239746094,
        "learning_rate": 1.9945018720588203e-05,
        "epoch": 1.8646666666666667,
        "step": 13985
    },
    {
        "loss": 2.6312,
        "grad_norm": 2.884265661239624,
        "learning_rate": 1.9907318711512457e-05,
        "epoch": 1.8648,
        "step": 13986
    },
    {
        "loss": 1.3697,
        "grad_norm": 4.281778812408447,
        "learning_rate": 1.9869650427420706e-05,
        "epoch": 1.8649333333333333,
        "step": 13987
    },
    {
        "loss": 2.4045,
        "grad_norm": 3.5338821411132812,
        "learning_rate": 1.9832013883233436e-05,
        "epoch": 1.8650666666666667,
        "step": 13988
    },
    {
        "loss": 3.059,
        "grad_norm": 2.4899556636810303,
        "learning_rate": 1.9794409093858545e-05,
        "epoch": 1.8652,
        "step": 13989
    },
    {
        "loss": 1.8029,
        "grad_norm": 5.5723347663879395,
        "learning_rate": 1.9756836074191608e-05,
        "epoch": 1.8653333333333333,
        "step": 13990
    },
    {
        "loss": 1.4897,
        "grad_norm": 3.6678860187530518,
        "learning_rate": 1.97192948391153e-05,
        "epoch": 1.8654666666666668,
        "step": 13991
    },
    {
        "loss": 1.9826,
        "grad_norm": 3.413442373275757,
        "learning_rate": 1.9681785403499785e-05,
        "epoch": 1.8656000000000001,
        "step": 13992
    },
    {
        "loss": 2.313,
        "grad_norm": 3.312758207321167,
        "learning_rate": 1.964430778220284e-05,
        "epoch": 1.8657333333333335,
        "step": 13993
    },
    {
        "loss": 0.8786,
        "grad_norm": 3.95232892036438,
        "learning_rate": 1.9606861990069382e-05,
        "epoch": 1.8658666666666668,
        "step": 13994
    },
    {
        "loss": 2.5343,
        "grad_norm": 3.428126811981201,
        "learning_rate": 1.9569448041931747e-05,
        "epoch": 1.866,
        "step": 13995
    },
    {
        "loss": 2.7365,
        "grad_norm": 3.1707346439361572,
        "learning_rate": 1.9532065952609868e-05,
        "epoch": 1.8661333333333334,
        "step": 13996
    },
    {
        "loss": 1.9236,
        "grad_norm": 4.207448959350586,
        "learning_rate": 1.949471573691085e-05,
        "epoch": 1.8662666666666667,
        "step": 13997
    },
    {
        "loss": 2.1763,
        "grad_norm": 3.921917200088501,
        "learning_rate": 1.9457397409629273e-05,
        "epoch": 1.8664,
        "step": 13998
    },
    {
        "loss": 2.0324,
        "grad_norm": 6.916781425476074,
        "learning_rate": 1.9420110985546925e-05,
        "epoch": 1.8665333333333334,
        "step": 13999
    },
    {
        "loss": 1.7943,
        "grad_norm": 3.1014184951782227,
        "learning_rate": 1.9382856479433252e-05,
        "epoch": 1.8666666666666667,
        "step": 14000
    },
    {
        "loss": 2.4224,
        "grad_norm": 3.4958744049072266,
        "learning_rate": 1.934563390604478e-05,
        "epoch": 1.8668,
        "step": 14001
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.6550114154815674,
        "learning_rate": 1.9308443280125587e-05,
        "epoch": 1.8669333333333333,
        "step": 14002
    },
    {
        "loss": 2.1658,
        "grad_norm": 3.661261796951294,
        "learning_rate": 1.9271284616406993e-05,
        "epoch": 1.8670666666666667,
        "step": 14003
    },
    {
        "loss": 2.3554,
        "grad_norm": 3.8384742736816406,
        "learning_rate": 1.923415792960752e-05,
        "epoch": 1.8672,
        "step": 14004
    },
    {
        "loss": 2.6653,
        "grad_norm": 4.081874370574951,
        "learning_rate": 1.919706323443339e-05,
        "epoch": 1.8673333333333333,
        "step": 14005
    },
    {
        "loss": 2.7579,
        "grad_norm": 4.039815425872803,
        "learning_rate": 1.916000054557786e-05,
        "epoch": 1.8674666666666666,
        "step": 14006
    },
    {
        "loss": 1.4887,
        "grad_norm": 3.7505972385406494,
        "learning_rate": 1.9122969877721542e-05,
        "epoch": 1.8676,
        "step": 14007
    },
    {
        "loss": 2.5971,
        "grad_norm": 3.774303674697876,
        "learning_rate": 1.908597124553235e-05,
        "epoch": 1.8677333333333332,
        "step": 14008
    },
    {
        "loss": 2.8057,
        "grad_norm": 1.8371820449829102,
        "learning_rate": 1.9049004663665782e-05,
        "epoch": 1.8678666666666666,
        "step": 14009
    },
    {
        "loss": 2.449,
        "grad_norm": 3.8476216793060303,
        "learning_rate": 1.9012070146764184e-05,
        "epoch": 1.8679999999999999,
        "step": 14010
    },
    {
        "loss": 2.1539,
        "grad_norm": 3.1524600982666016,
        "learning_rate": 1.897516770945774e-05,
        "epoch": 1.8681333333333332,
        "step": 14011
    },
    {
        "loss": 1.9732,
        "grad_norm": 3.674633502960205,
        "learning_rate": 1.8938297366363267e-05,
        "epoch": 1.8682666666666665,
        "step": 14012
    },
    {
        "loss": 2.0241,
        "grad_norm": 3.8321316242218018,
        "learning_rate": 1.890145913208543e-05,
        "epoch": 1.8683999999999998,
        "step": 14013
    },
    {
        "loss": 1.8748,
        "grad_norm": 2.334226608276367,
        "learning_rate": 1.8864653021216038e-05,
        "epoch": 1.8685333333333334,
        "step": 14014
    },
    {
        "loss": 1.3969,
        "grad_norm": 4.3521270751953125,
        "learning_rate": 1.8827879048334074e-05,
        "epoch": 1.8686666666666667,
        "step": 14015
    },
    {
        "loss": 2.3633,
        "grad_norm": 3.275280237197876,
        "learning_rate": 1.8791137228005784e-05,
        "epoch": 1.8688,
        "step": 14016
    },
    {
        "loss": 1.4978,
        "grad_norm": 4.014426231384277,
        "learning_rate": 1.8754427574784695e-05,
        "epoch": 1.8689333333333333,
        "step": 14017
    },
    {
        "loss": 2.1496,
        "grad_norm": 2.8737070560455322,
        "learning_rate": 1.871775010321173e-05,
        "epoch": 1.8690666666666667,
        "step": 14018
    },
    {
        "loss": 2.0196,
        "grad_norm": 3.24484920501709,
        "learning_rate": 1.8681104827814834e-05,
        "epoch": 1.8692,
        "step": 14019
    },
    {
        "loss": 2.7246,
        "grad_norm": 2.9508872032165527,
        "learning_rate": 1.8644491763109583e-05,
        "epoch": 1.8693333333333333,
        "step": 14020
    },
    {
        "loss": 2.0971,
        "grad_norm": 3.4364147186279297,
        "learning_rate": 1.8607910923598182e-05,
        "epoch": 1.8694666666666668,
        "step": 14021
    },
    {
        "loss": 2.6239,
        "grad_norm": 2.7791249752044678,
        "learning_rate": 1.857136232377059e-05,
        "epoch": 1.8696000000000002,
        "step": 14022
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.950277328491211,
        "learning_rate": 1.8534845978103954e-05,
        "epoch": 1.8697333333333335,
        "step": 14023
    },
    {
        "loss": 2.0789,
        "grad_norm": 3.4133987426757812,
        "learning_rate": 1.849836190106239e-05,
        "epoch": 1.8698666666666668,
        "step": 14024
    },
    {
        "loss": 2.3283,
        "grad_norm": 3.0124785900115967,
        "learning_rate": 1.8461910107097414e-05,
        "epoch": 1.87,
        "step": 14025
    },
    {
        "loss": 2.3631,
        "grad_norm": 2.482367753982544,
        "learning_rate": 1.8425490610647567e-05,
        "epoch": 1.8701333333333334,
        "step": 14026
    },
    {
        "loss": 2.188,
        "grad_norm": 3.0902469158172607,
        "learning_rate": 1.8389103426138933e-05,
        "epoch": 1.8702666666666667,
        "step": 14027
    },
    {
        "loss": 2.3103,
        "grad_norm": 4.025542259216309,
        "learning_rate": 1.8352748567984522e-05,
        "epoch": 1.8704,
        "step": 14028
    },
    {
        "loss": 1.9905,
        "grad_norm": 4.068033695220947,
        "learning_rate": 1.8316426050584602e-05,
        "epoch": 1.8705333333333334,
        "step": 14029
    },
    {
        "loss": 2.2996,
        "grad_norm": 4.275308609008789,
        "learning_rate": 1.8280135888326566e-05,
        "epoch": 1.8706666666666667,
        "step": 14030
    },
    {
        "loss": 2.1835,
        "grad_norm": 4.61330509185791,
        "learning_rate": 1.8243878095585198e-05,
        "epoch": 1.8708,
        "step": 14031
    },
    {
        "loss": 2.2454,
        "grad_norm": 4.397092342376709,
        "learning_rate": 1.8207652686722408e-05,
        "epoch": 1.8709333333333333,
        "step": 14032
    },
    {
        "loss": 3.1771,
        "grad_norm": 4.472754001617432,
        "learning_rate": 1.8171459676086945e-05,
        "epoch": 1.8710666666666667,
        "step": 14033
    },
    {
        "loss": 1.7609,
        "grad_norm": 4.699756145477295,
        "learning_rate": 1.813529907801522e-05,
        "epoch": 1.8712,
        "step": 14034
    },
    {
        "loss": 1.9464,
        "grad_norm": 4.446589946746826,
        "learning_rate": 1.8099170906830386e-05,
        "epoch": 1.8713333333333333,
        "step": 14035
    },
    {
        "loss": 2.08,
        "grad_norm": 3.3909902572631836,
        "learning_rate": 1.8063075176843093e-05,
        "epoch": 1.8714666666666666,
        "step": 14036
    },
    {
        "loss": 1.7254,
        "grad_norm": 5.717907905578613,
        "learning_rate": 1.8027011902350943e-05,
        "epoch": 1.8716,
        "step": 14037
    },
    {
        "loss": 2.1154,
        "grad_norm": 2.995328426361084,
        "learning_rate": 1.799098109763867e-05,
        "epoch": 1.8717333333333332,
        "step": 14038
    },
    {
        "loss": 2.1024,
        "grad_norm": 4.499589920043945,
        "learning_rate": 1.7954982776978156e-05,
        "epoch": 1.8718666666666666,
        "step": 14039
    },
    {
        "loss": 2.6518,
        "grad_norm": 4.905511856079102,
        "learning_rate": 1.7919016954628544e-05,
        "epoch": 1.8719999999999999,
        "step": 14040
    },
    {
        "loss": 2.2193,
        "grad_norm": 3.7924249172210693,
        "learning_rate": 1.788308364483613e-05,
        "epoch": 1.8721333333333332,
        "step": 14041
    },
    {
        "loss": 2.1596,
        "grad_norm": 4.092374324798584,
        "learning_rate": 1.7847182861833955e-05,
        "epoch": 1.8722666666666665,
        "step": 14042
    },
    {
        "loss": 1.2523,
        "grad_norm": 5.65610933303833,
        "learning_rate": 1.7811314619842632e-05,
        "epoch": 1.8723999999999998,
        "step": 14043
    },
    {
        "loss": 1.9704,
        "grad_norm": 5.600569725036621,
        "learning_rate": 1.7775478933069566e-05,
        "epoch": 1.8725333333333334,
        "step": 14044
    },
    {
        "loss": 2.6025,
        "grad_norm": 2.5059115886688232,
        "learning_rate": 1.7739675815709568e-05,
        "epoch": 1.8726666666666667,
        "step": 14045
    },
    {
        "loss": 2.6663,
        "grad_norm": 3.789057731628418,
        "learning_rate": 1.7703905281944256e-05,
        "epoch": 1.8728,
        "step": 14046
    },
    {
        "loss": 2.2571,
        "grad_norm": 3.275386333465576,
        "learning_rate": 1.7668167345942467e-05,
        "epoch": 1.8729333333333333,
        "step": 14047
    },
    {
        "loss": 2.1807,
        "grad_norm": 4.258611679077148,
        "learning_rate": 1.7632462021860084e-05,
        "epoch": 1.8730666666666667,
        "step": 14048
    },
    {
        "loss": 2.3558,
        "grad_norm": 3.2658536434173584,
        "learning_rate": 1.7596789323840212e-05,
        "epoch": 1.8732,
        "step": 14049
    },
    {
        "loss": 2.3453,
        "grad_norm": 3.5792629718780518,
        "learning_rate": 1.7561149266012887e-05,
        "epoch": 1.8733333333333333,
        "step": 14050
    },
    {
        "loss": 1.2537,
        "grad_norm": 4.686518669128418,
        "learning_rate": 1.7525541862495143e-05,
        "epoch": 1.8734666666666666,
        "step": 14051
    },
    {
        "loss": 1.7259,
        "grad_norm": 3.8723578453063965,
        "learning_rate": 1.748996712739137e-05,
        "epoch": 1.8736000000000002,
        "step": 14052
    },
    {
        "loss": 1.5163,
        "grad_norm": 3.9241092205047607,
        "learning_rate": 1.745442507479279e-05,
        "epoch": 1.8737333333333335,
        "step": 14053
    },
    {
        "loss": 2.0224,
        "grad_norm": 5.615757942199707,
        "learning_rate": 1.7418915718777605e-05,
        "epoch": 1.8738666666666668,
        "step": 14054
    },
    {
        "loss": 2.1853,
        "grad_norm": 3.6166558265686035,
        "learning_rate": 1.7383439073411357e-05,
        "epoch": 1.874,
        "step": 14055
    },
    {
        "loss": 2.0408,
        "grad_norm": 2.724423408508301,
        "learning_rate": 1.734799515274643e-05,
        "epoch": 1.8741333333333334,
        "step": 14056
    },
    {
        "loss": 2.4211,
        "grad_norm": 2.380520820617676,
        "learning_rate": 1.731258397082224e-05,
        "epoch": 1.8742666666666667,
        "step": 14057
    },
    {
        "loss": 2.7583,
        "grad_norm": 2.7225401401519775,
        "learning_rate": 1.7277205541665208e-05,
        "epoch": 1.8744,
        "step": 14058
    },
    {
        "loss": 3.8617,
        "grad_norm": 3.4351773262023926,
        "learning_rate": 1.7241859879289025e-05,
        "epoch": 1.8745333333333334,
        "step": 14059
    },
    {
        "loss": 0.8572,
        "grad_norm": 4.039904594421387,
        "learning_rate": 1.7206546997694038e-05,
        "epoch": 1.8746666666666667,
        "step": 14060
    },
    {
        "loss": 2.02,
        "grad_norm": 3.096979856491089,
        "learning_rate": 1.717126691086798e-05,
        "epoch": 1.8748,
        "step": 14061
    },
    {
        "loss": 1.2681,
        "grad_norm": 4.3642144203186035,
        "learning_rate": 1.713601963278535e-05,
        "epoch": 1.8749333333333333,
        "step": 14062
    },
    {
        "loss": 1.7432,
        "grad_norm": 3.7756834030151367,
        "learning_rate": 1.7100805177407596e-05,
        "epoch": 1.8750666666666667,
        "step": 14063
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.3162121772766113,
        "learning_rate": 1.7065623558683485e-05,
        "epoch": 1.8752,
        "step": 14064
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.161865234375,
        "learning_rate": 1.7030474790548445e-05,
        "epoch": 1.8753333333333333,
        "step": 14065
    },
    {
        "loss": 2.1461,
        "grad_norm": 5.239987373352051,
        "learning_rate": 1.699535888692505e-05,
        "epoch": 1.8754666666666666,
        "step": 14066
    },
    {
        "loss": 1.372,
        "grad_norm": 2.302769184112549,
        "learning_rate": 1.6960275861722785e-05,
        "epoch": 1.8756,
        "step": 14067
    },
    {
        "loss": 2.6181,
        "grad_norm": 2.6595914363861084,
        "learning_rate": 1.6925225728838258e-05,
        "epoch": 1.8757333333333333,
        "step": 14068
    },
    {
        "loss": 1.5905,
        "grad_norm": 2.7962682247161865,
        "learning_rate": 1.6890208502154814e-05,
        "epoch": 1.8758666666666666,
        "step": 14069
    },
    {
        "loss": 3.2182,
        "grad_norm": 3.8573665618896484,
        "learning_rate": 1.685522419554314e-05,
        "epoch": 1.876,
        "step": 14070
    },
    {
        "loss": 2.6798,
        "grad_norm": 2.9427714347839355,
        "learning_rate": 1.6820272822860296e-05,
        "epoch": 1.8761333333333332,
        "step": 14071
    },
    {
        "loss": 2.2424,
        "grad_norm": 3.4572067260742188,
        "learning_rate": 1.6785354397950815e-05,
        "epoch": 1.8762666666666665,
        "step": 14072
    },
    {
        "loss": 2.2437,
        "grad_norm": 3.425454616546631,
        "learning_rate": 1.6750468934646113e-05,
        "epoch": 1.8763999999999998,
        "step": 14073
    },
    {
        "loss": 1.2011,
        "grad_norm": 3.094651937484741,
        "learning_rate": 1.6715616446764292e-05,
        "epoch": 1.8765333333333334,
        "step": 14074
    },
    {
        "loss": 1.3155,
        "grad_norm": 3.029942512512207,
        "learning_rate": 1.6680796948110622e-05,
        "epoch": 1.8766666666666667,
        "step": 14075
    },
    {
        "loss": 1.9633,
        "grad_norm": 3.3211002349853516,
        "learning_rate": 1.664601045247708e-05,
        "epoch": 1.8768,
        "step": 14076
    },
    {
        "loss": 2.247,
        "grad_norm": 3.635246515274048,
        "learning_rate": 1.6611256973642886e-05,
        "epoch": 1.8769333333333333,
        "step": 14077
    },
    {
        "loss": 2.185,
        "grad_norm": 2.551313877105713,
        "learning_rate": 1.6576536525373976e-05,
        "epoch": 1.8770666666666667,
        "step": 14078
    },
    {
        "loss": 1.7228,
        "grad_norm": 2.519700527191162,
        "learning_rate": 1.654184912142318e-05,
        "epoch": 1.8772,
        "step": 14079
    },
    {
        "loss": 2.8281,
        "grad_norm": 4.363914966583252,
        "learning_rate": 1.6507194775530255e-05,
        "epoch": 1.8773333333333333,
        "step": 14080
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.9854226112365723,
        "learning_rate": 1.6472573501421995e-05,
        "epoch": 1.8774666666666666,
        "step": 14081
    },
    {
        "loss": 2.0231,
        "grad_norm": 3.644413948059082,
        "learning_rate": 1.6437985312812032e-05,
        "epoch": 1.8776000000000002,
        "step": 14082
    },
    {
        "loss": 0.8931,
        "grad_norm": 3.1565284729003906,
        "learning_rate": 1.6403430223400818e-05,
        "epoch": 1.8777333333333335,
        "step": 14083
    },
    {
        "loss": 2.6377,
        "grad_norm": 4.257796764373779,
        "learning_rate": 1.636890824687576e-05,
        "epoch": 1.8778666666666668,
        "step": 14084
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.646453380584717,
        "learning_rate": 1.6334419396911016e-05,
        "epoch": 1.8780000000000001,
        "step": 14085
    },
    {
        "loss": 2.519,
        "grad_norm": 4.5901055335998535,
        "learning_rate": 1.629996368716793e-05,
        "epoch": 1.8781333333333334,
        "step": 14086
    },
    {
        "loss": 2.1565,
        "grad_norm": 3.2435574531555176,
        "learning_rate": 1.6265541131294404e-05,
        "epoch": 1.8782666666666668,
        "step": 14087
    },
    {
        "loss": 2.2885,
        "grad_norm": 2.9844605922698975,
        "learning_rate": 1.6231151742925365e-05,
        "epoch": 1.8784,
        "step": 14088
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.0643413066864014,
        "learning_rate": 1.6196795535682498e-05,
        "epoch": 1.8785333333333334,
        "step": 14089
    },
    {
        "loss": 0.8509,
        "grad_norm": 3.7688190937042236,
        "learning_rate": 1.6162472523174443e-05,
        "epoch": 1.8786666666666667,
        "step": 14090
    },
    {
        "loss": 0.851,
        "grad_norm": 3.579573631286621,
        "learning_rate": 1.6128182718996843e-05,
        "epoch": 1.8788,
        "step": 14091
    },
    {
        "loss": 1.8198,
        "grad_norm": 3.3422157764434814,
        "learning_rate": 1.609392613673173e-05,
        "epoch": 1.8789333333333333,
        "step": 14092
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.9731545448303223,
        "learning_rate": 1.605970278994843e-05,
        "epoch": 1.8790666666666667,
        "step": 14093
    },
    {
        "loss": 1.4651,
        "grad_norm": 4.548812389373779,
        "learning_rate": 1.6025512692202806e-05,
        "epoch": 1.8792,
        "step": 14094
    },
    {
        "loss": 2.228,
        "grad_norm": 3.1112074851989746,
        "learning_rate": 1.5991355857037827e-05,
        "epoch": 1.8793333333333333,
        "step": 14095
    },
    {
        "loss": 2.6055,
        "grad_norm": 3.3347065448760986,
        "learning_rate": 1.5957232297983028e-05,
        "epoch": 1.8794666666666666,
        "step": 14096
    },
    {
        "loss": 2.2683,
        "grad_norm": 3.398345947265625,
        "learning_rate": 1.5923142028554895e-05,
        "epoch": 1.8796,
        "step": 14097
    },
    {
        "loss": 2.2321,
        "grad_norm": 3.3987348079681396,
        "learning_rate": 1.5889085062256602e-05,
        "epoch": 1.8797333333333333,
        "step": 14098
    },
    {
        "loss": 1.8705,
        "grad_norm": 4.204887866973877,
        "learning_rate": 1.58550614125784e-05,
        "epoch": 1.8798666666666666,
        "step": 14099
    },
    {
        "loss": 2.1613,
        "grad_norm": 4.020547866821289,
        "learning_rate": 1.5821071092997096e-05,
        "epoch": 1.88,
        "step": 14100
    },
    {
        "loss": 2.1708,
        "grad_norm": 4.121274948120117,
        "learning_rate": 1.5787114116976275e-05,
        "epoch": 1.8801333333333332,
        "step": 14101
    },
    {
        "loss": 1.8656,
        "grad_norm": 3.978632926940918,
        "learning_rate": 1.5753190497966573e-05,
        "epoch": 1.8802666666666665,
        "step": 14102
    },
    {
        "loss": 1.7629,
        "grad_norm": 3.910815477371216,
        "learning_rate": 1.5719300249405135e-05,
        "epoch": 1.8803999999999998,
        "step": 14103
    },
    {
        "loss": 1.5111,
        "grad_norm": 4.125296592712402,
        "learning_rate": 1.568544338471609e-05,
        "epoch": 1.8805333333333332,
        "step": 14104
    },
    {
        "loss": 1.9776,
        "grad_norm": 5.403811931610107,
        "learning_rate": 1.5651619917310224e-05,
        "epoch": 1.8806666666666667,
        "step": 14105
    },
    {
        "loss": 1.9289,
        "grad_norm": 3.7936949729919434,
        "learning_rate": 1.5617829860585133e-05,
        "epoch": 1.8808,
        "step": 14106
    },
    {
        "loss": 1.1437,
        "grad_norm": Infinity,
        "learning_rate": 1.5617829860585133e-05,
        "epoch": 1.8809333333333333,
        "step": 14107
    },
    {
        "loss": 0.5578,
        "grad_norm": 3.7869691848754883,
        "learning_rate": 1.558407322792507e-05,
        "epoch": 1.8810666666666667,
        "step": 14108
    },
    {
        "loss": 1.9728,
        "grad_norm": 2.859877824783325,
        "learning_rate": 1.5550350032701312e-05,
        "epoch": 1.8812,
        "step": 14109
    },
    {
        "loss": 1.9857,
        "grad_norm": 3.6842358112335205,
        "learning_rate": 1.551666028827168e-05,
        "epoch": 1.8813333333333333,
        "step": 14110
    },
    {
        "loss": 1.7253,
        "grad_norm": 4.425173759460449,
        "learning_rate": 1.5483004007980685e-05,
        "epoch": 1.8814666666666666,
        "step": 14111
    },
    {
        "loss": 1.602,
        "grad_norm": 4.714473247528076,
        "learning_rate": 1.5449381205159884e-05,
        "epoch": 1.8816000000000002,
        "step": 14112
    },
    {
        "loss": 2.1694,
        "grad_norm": 4.849691867828369,
        "learning_rate": 1.541579189312726e-05,
        "epoch": 1.8817333333333335,
        "step": 14113
    },
    {
        "loss": 0.4857,
        "grad_norm": 3.0953316688537598,
        "learning_rate": 1.5382236085187616e-05,
        "epoch": 1.8818666666666668,
        "step": 14114
    },
    {
        "loss": 1.751,
        "grad_norm": 6.291102409362793,
        "learning_rate": 1.5348713794632673e-05,
        "epoch": 1.8820000000000001,
        "step": 14115
    },
    {
        "loss": 2.3148,
        "grad_norm": 3.276139736175537,
        "learning_rate": 1.531522503474062e-05,
        "epoch": 1.8821333333333334,
        "step": 14116
    },
    {
        "loss": 1.8343,
        "grad_norm": 5.224137306213379,
        "learning_rate": 1.5281769818776482e-05,
        "epoch": 1.8822666666666668,
        "step": 14117
    },
    {
        "loss": 0.9337,
        "grad_norm": 5.251552581787109,
        "learning_rate": 1.5248348159991943e-05,
        "epoch": 1.8824,
        "step": 14118
    },
    {
        "loss": 0.767,
        "grad_norm": 3.662846803665161,
        "learning_rate": 1.521496007162554e-05,
        "epoch": 1.8825333333333334,
        "step": 14119
    },
    {
        "loss": 2.6739,
        "grad_norm": 2.808626890182495,
        "learning_rate": 1.5181605566902268e-05,
        "epoch": 1.8826666666666667,
        "step": 14120
    },
    {
        "loss": 2.3163,
        "grad_norm": 6.505638122558594,
        "learning_rate": 1.514828465903414e-05,
        "epoch": 1.8828,
        "step": 14121
    },
    {
        "loss": 2.6527,
        "grad_norm": 2.788511276245117,
        "learning_rate": 1.5114997361219562e-05,
        "epoch": 1.8829333333333333,
        "step": 14122
    },
    {
        "loss": 2.4177,
        "grad_norm": 3.6809794902801514,
        "learning_rate": 1.508174368664369e-05,
        "epoch": 1.8830666666666667,
        "step": 14123
    },
    {
        "loss": 2.4018,
        "grad_norm": 5.825164794921875,
        "learning_rate": 1.5048523648478597e-05,
        "epoch": 1.8832,
        "step": 14124
    },
    {
        "loss": 2.1507,
        "grad_norm": 3.555985450744629,
        "learning_rate": 1.5015337259882733e-05,
        "epoch": 1.8833333333333333,
        "step": 14125
    },
    {
        "loss": 2.2881,
        "grad_norm": 3.9170353412628174,
        "learning_rate": 1.4982184534001376e-05,
        "epoch": 1.8834666666666666,
        "step": 14126
    },
    {
        "loss": 1.7716,
        "grad_norm": 3.7952418327331543,
        "learning_rate": 1.4949065483966318e-05,
        "epoch": 1.8836,
        "step": 14127
    },
    {
        "loss": 2.8965,
        "grad_norm": 3.965247631072998,
        "learning_rate": 1.4915980122896301e-05,
        "epoch": 1.8837333333333333,
        "step": 14128
    },
    {
        "loss": 2.1871,
        "grad_norm": 4.077573299407959,
        "learning_rate": 1.4882928463896428e-05,
        "epoch": 1.8838666666666666,
        "step": 14129
    },
    {
        "loss": 2.4092,
        "grad_norm": 2.0002877712249756,
        "learning_rate": 1.484991052005873e-05,
        "epoch": 1.884,
        "step": 14130
    },
    {
        "loss": 1.8039,
        "grad_norm": 3.527552843093872,
        "learning_rate": 1.4816926304461476e-05,
        "epoch": 1.8841333333333332,
        "step": 14131
    },
    {
        "loss": 1.2922,
        "grad_norm": 4.887115001678467,
        "learning_rate": 1.4783975830170005e-05,
        "epoch": 1.8842666666666665,
        "step": 14132
    },
    {
        "loss": 0.9674,
        "grad_norm": 3.7213592529296875,
        "learning_rate": 1.475105911023611e-05,
        "epoch": 1.8843999999999999,
        "step": 14133
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.851052761077881,
        "learning_rate": 1.4718176157698204e-05,
        "epoch": 1.8845333333333332,
        "step": 14134
    },
    {
        "loss": 1.2172,
        "grad_norm": 4.0354156494140625,
        "learning_rate": 1.4685326985581294e-05,
        "epoch": 1.8846666666666667,
        "step": 14135
    },
    {
        "loss": 1.6522,
        "grad_norm": 3.3766896724700928,
        "learning_rate": 1.465251160689699e-05,
        "epoch": 1.8848,
        "step": 14136
    },
    {
        "loss": 2.9207,
        "grad_norm": 3.1399784088134766,
        "learning_rate": 1.4619730034643753e-05,
        "epoch": 1.8849333333333333,
        "step": 14137
    },
    {
        "loss": 1.6107,
        "grad_norm": 4.389344692230225,
        "learning_rate": 1.4586982281806349e-05,
        "epoch": 1.8850666666666667,
        "step": 14138
    },
    {
        "loss": 1.7174,
        "grad_norm": 2.9616763591766357,
        "learning_rate": 1.4554268361356293e-05,
        "epoch": 1.8852,
        "step": 14139
    },
    {
        "loss": 1.6613,
        "grad_norm": 4.437829971313477,
        "learning_rate": 1.4521588286251653e-05,
        "epoch": 1.8853333333333333,
        "step": 14140
    },
    {
        "loss": 2.9858,
        "grad_norm": 2.645616054534912,
        "learning_rate": 1.4488942069437128e-05,
        "epoch": 1.8854666666666666,
        "step": 14141
    },
    {
        "loss": 1.9912,
        "grad_norm": 3.843127489089966,
        "learning_rate": 1.4456329723844087e-05,
        "epoch": 1.8856000000000002,
        "step": 14142
    },
    {
        "loss": 2.1291,
        "grad_norm": 2.256359100341797,
        "learning_rate": 1.442375126239035e-05,
        "epoch": 1.8857333333333335,
        "step": 14143
    },
    {
        "loss": 1.6444,
        "grad_norm": 4.342496395111084,
        "learning_rate": 1.4391206697980309e-05,
        "epoch": 1.8858666666666668,
        "step": 14144
    },
    {
        "loss": 0.4774,
        "grad_norm": 2.9162042140960693,
        "learning_rate": 1.4358696043504938e-05,
        "epoch": 1.8860000000000001,
        "step": 14145
    },
    {
        "loss": 1.1895,
        "grad_norm": 4.398194789886475,
        "learning_rate": 1.432621931184196e-05,
        "epoch": 1.8861333333333334,
        "step": 14146
    },
    {
        "loss": 2.1448,
        "grad_norm": 3.2780709266662598,
        "learning_rate": 1.4293776515855428e-05,
        "epoch": 1.8862666666666668,
        "step": 14147
    },
    {
        "loss": 2.3853,
        "grad_norm": 4.090729713439941,
        "learning_rate": 1.4261367668396064e-05,
        "epoch": 1.8864,
        "step": 14148
    },
    {
        "loss": 2.1088,
        "grad_norm": 3.351463794708252,
        "learning_rate": 1.4228992782301031e-05,
        "epoch": 1.8865333333333334,
        "step": 14149
    },
    {
        "loss": 2.328,
        "grad_norm": 2.8510079383850098,
        "learning_rate": 1.4196651870394218e-05,
        "epoch": 1.8866666666666667,
        "step": 14150
    },
    {
        "loss": 2.45,
        "grad_norm": 3.326770782470703,
        "learning_rate": 1.4164344945486119e-05,
        "epoch": 1.8868,
        "step": 14151
    },
    {
        "loss": 2.736,
        "grad_norm": 3.8588387966156006,
        "learning_rate": 1.413207202037331e-05,
        "epoch": 1.8869333333333334,
        "step": 14152
    },
    {
        "loss": 1.5889,
        "grad_norm": 4.755591869354248,
        "learning_rate": 1.4099833107839422e-05,
        "epoch": 1.8870666666666667,
        "step": 14153
    },
    {
        "loss": 1.5914,
        "grad_norm": 3.775557041168213,
        "learning_rate": 1.4067628220654294e-05,
        "epoch": 1.8872,
        "step": 14154
    },
    {
        "loss": 2.306,
        "grad_norm": 3.271109104156494,
        "learning_rate": 1.403545737157449e-05,
        "epoch": 1.8873333333333333,
        "step": 14155
    },
    {
        "loss": 2.8919,
        "grad_norm": 1.8951401710510254,
        "learning_rate": 1.4003320573342926e-05,
        "epoch": 1.8874666666666666,
        "step": 14156
    },
    {
        "loss": 2.2439,
        "grad_norm": 4.082305908203125,
        "learning_rate": 1.3971217838689143e-05,
        "epoch": 1.8876,
        "step": 14157
    },
    {
        "loss": 2.0712,
        "grad_norm": 4.694056987762451,
        "learning_rate": 1.3939149180329014e-05,
        "epoch": 1.8877333333333333,
        "step": 14158
    },
    {
        "loss": 2.1214,
        "grad_norm": 3.8781557083129883,
        "learning_rate": 1.3907114610965233e-05,
        "epoch": 1.8878666666666666,
        "step": 14159
    },
    {
        "loss": 3.0609,
        "grad_norm": 3.766054630279541,
        "learning_rate": 1.3875114143286682e-05,
        "epoch": 1.888,
        "step": 14160
    },
    {
        "loss": 2.813,
        "grad_norm": 2.5252795219421387,
        "learning_rate": 1.3843147789968814e-05,
        "epoch": 1.8881333333333332,
        "step": 14161
    },
    {
        "loss": 2.2119,
        "grad_norm": 2.8269758224487305,
        "learning_rate": 1.3811215563673774e-05,
        "epoch": 1.8882666666666665,
        "step": 14162
    },
    {
        "loss": 1.3471,
        "grad_norm": 3.431093454360962,
        "learning_rate": 1.3779317477049835e-05,
        "epoch": 1.8883999999999999,
        "step": 14163
    },
    {
        "loss": 1.6854,
        "grad_norm": 5.0156354904174805,
        "learning_rate": 1.3747453542732114e-05,
        "epoch": 1.8885333333333332,
        "step": 14164
    },
    {
        "loss": 1.1963,
        "grad_norm": 5.020846366882324,
        "learning_rate": 1.3715623773341946e-05,
        "epoch": 1.8886666666666667,
        "step": 14165
    },
    {
        "loss": 3.2446,
        "grad_norm": 4.268388748168945,
        "learning_rate": 1.3683828181487201e-05,
        "epoch": 1.8888,
        "step": 14166
    },
    {
        "loss": 0.8389,
        "grad_norm": 3.2965192794799805,
        "learning_rate": 1.3652066779762151e-05,
        "epoch": 1.8889333333333334,
        "step": 14167
    },
    {
        "loss": 2.507,
        "grad_norm": 4.577913761138916,
        "learning_rate": 1.3620339580747765e-05,
        "epoch": 1.8890666666666667,
        "step": 14168
    },
    {
        "loss": 3.0267,
        "grad_norm": 3.932204008102417,
        "learning_rate": 1.358864659701118e-05,
        "epoch": 1.8892,
        "step": 14169
    },
    {
        "loss": 2.3865,
        "grad_norm": 3.2363977432250977,
        "learning_rate": 1.3556987841106061e-05,
        "epoch": 1.8893333333333333,
        "step": 14170
    },
    {
        "loss": 1.6755,
        "grad_norm": 3.067317008972168,
        "learning_rate": 1.3525363325572681e-05,
        "epoch": 1.8894666666666666,
        "step": 14171
    },
    {
        "loss": 2.3962,
        "grad_norm": 4.212080001831055,
        "learning_rate": 1.349377306293752e-05,
        "epoch": 1.8896,
        "step": 14172
    },
    {
        "loss": 1.3684,
        "grad_norm": 4.988997459411621,
        "learning_rate": 1.3462217065713578e-05,
        "epoch": 1.8897333333333335,
        "step": 14173
    },
    {
        "loss": 2.1737,
        "grad_norm": 4.420346736907959,
        "learning_rate": 1.3430695346400368e-05,
        "epoch": 1.8898666666666668,
        "step": 14174
    },
    {
        "loss": 1.1828,
        "grad_norm": 4.941540241241455,
        "learning_rate": 1.3399207917483703e-05,
        "epoch": 1.8900000000000001,
        "step": 14175
    },
    {
        "loss": 2.5466,
        "grad_norm": 2.6493873596191406,
        "learning_rate": 1.3367754791435893e-05,
        "epoch": 1.8901333333333334,
        "step": 14176
    },
    {
        "loss": 2.2934,
        "grad_norm": 3.6363437175750732,
        "learning_rate": 1.333633598071551e-05,
        "epoch": 1.8902666666666668,
        "step": 14177
    },
    {
        "loss": 1.7697,
        "grad_norm": 4.453592777252197,
        "learning_rate": 1.3304951497767815e-05,
        "epoch": 1.8904,
        "step": 14178
    },
    {
        "loss": 2.2111,
        "grad_norm": 4.476335525512695,
        "learning_rate": 1.327360135502419e-05,
        "epoch": 1.8905333333333334,
        "step": 14179
    },
    {
        "loss": 2.4105,
        "grad_norm": 4.080008029937744,
        "learning_rate": 1.32422855649027e-05,
        "epoch": 1.8906666666666667,
        "step": 14180
    },
    {
        "loss": 2.4291,
        "grad_norm": 5.715554714202881,
        "learning_rate": 1.3211004139807393e-05,
        "epoch": 1.8908,
        "step": 14181
    },
    {
        "loss": 2.1234,
        "grad_norm": 3.2390477657318115,
        "learning_rate": 1.3179757092129064e-05,
        "epoch": 1.8909333333333334,
        "step": 14182
    },
    {
        "loss": 2.5072,
        "grad_norm": 3.256293773651123,
        "learning_rate": 1.3148544434244847e-05,
        "epoch": 1.8910666666666667,
        "step": 14183
    },
    {
        "loss": 1.6679,
        "grad_norm": 3.50053071975708,
        "learning_rate": 1.311736617851813e-05,
        "epoch": 1.8912,
        "step": 14184
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.281874895095825,
        "learning_rate": 1.3086222337298703e-05,
        "epoch": 1.8913333333333333,
        "step": 14185
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.910517454147339,
        "learning_rate": 1.3055112922922696e-05,
        "epoch": 1.8914666666666666,
        "step": 14186
    },
    {
        "loss": 2.7593,
        "grad_norm": 4.185746669769287,
        "learning_rate": 1.3024037947712797e-05,
        "epoch": 1.8916,
        "step": 14187
    },
    {
        "loss": 1.2047,
        "grad_norm": 4.809637069702148,
        "learning_rate": 1.2992997423977748e-05,
        "epoch": 1.8917333333333333,
        "step": 14188
    },
    {
        "loss": 2.4085,
        "grad_norm": 3.4565165042877197,
        "learning_rate": 1.2961991364013015e-05,
        "epoch": 1.8918666666666666,
        "step": 14189
    },
    {
        "loss": 2.4101,
        "grad_norm": 3.722532272338867,
        "learning_rate": 1.2931019780099972e-05,
        "epoch": 1.892,
        "step": 14190
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.752199172973633,
        "learning_rate": 1.2900082684506653e-05,
        "epoch": 1.8921333333333332,
        "step": 14191
    },
    {
        "loss": 2.525,
        "grad_norm": 3.5472116470336914,
        "learning_rate": 1.286918008948743e-05,
        "epoch": 1.8922666666666665,
        "step": 14192
    },
    {
        "loss": 2.4113,
        "grad_norm": 3.319261312484741,
        "learning_rate": 1.2838312007282892e-05,
        "epoch": 1.8923999999999999,
        "step": 14193
    },
    {
        "loss": 2.096,
        "grad_norm": 3.904493808746338,
        "learning_rate": 1.2807478450119936e-05,
        "epoch": 1.8925333333333332,
        "step": 14194
    },
    {
        "loss": 1.5285,
        "grad_norm": 3.4000015258789062,
        "learning_rate": 1.2776679430211813e-05,
        "epoch": 1.8926666666666667,
        "step": 14195
    },
    {
        "loss": 1.9803,
        "grad_norm": 4.071552276611328,
        "learning_rate": 1.2745914959758243e-05,
        "epoch": 1.8928,
        "step": 14196
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.8865604400634766,
        "learning_rate": 1.2715185050945066e-05,
        "epoch": 1.8929333333333334,
        "step": 14197
    },
    {
        "loss": 2.3078,
        "grad_norm": 3.7222516536712646,
        "learning_rate": 1.2684489715944514e-05,
        "epoch": 1.8930666666666667,
        "step": 14198
    },
    {
        "loss": 2.2619,
        "grad_norm": 2.9296703338623047,
        "learning_rate": 1.2653828966915027e-05,
        "epoch": 1.8932,
        "step": 14199
    },
    {
        "loss": 2.5086,
        "grad_norm": 2.995739221572876,
        "learning_rate": 1.26232028160015e-05,
        "epoch": 1.8933333333333333,
        "step": 14200
    },
    {
        "loss": 1.9795,
        "grad_norm": 4.027383804321289,
        "learning_rate": 1.2592611275335165e-05,
        "epoch": 1.8934666666666666,
        "step": 14201
    },
    {
        "loss": 2.2053,
        "grad_norm": 3.0543816089630127,
        "learning_rate": 1.2562054357033315e-05,
        "epoch": 1.8936,
        "step": 14202
    },
    {
        "loss": 2.4117,
        "grad_norm": 3.6582510471343994,
        "learning_rate": 1.253153207319967e-05,
        "epoch": 1.8937333333333335,
        "step": 14203
    },
    {
        "loss": 2.3584,
        "grad_norm": 3.341677188873291,
        "learning_rate": 1.250104443592416e-05,
        "epoch": 1.8938666666666668,
        "step": 14204
    },
    {
        "loss": 3.1579,
        "grad_norm": 3.2112443447113037,
        "learning_rate": 1.2470591457283143e-05,
        "epoch": 1.8940000000000001,
        "step": 14205
    },
    {
        "loss": 1.2469,
        "grad_norm": 4.475287914276123,
        "learning_rate": 1.244017314933913e-05,
        "epoch": 1.8941333333333334,
        "step": 14206
    },
    {
        "loss": 2.3853,
        "grad_norm": 4.4322404861450195,
        "learning_rate": 1.2409789524140846e-05,
        "epoch": 1.8942666666666668,
        "step": 14207
    },
    {
        "loss": 1.8781,
        "grad_norm": 2.667947292327881,
        "learning_rate": 1.2379440593723335e-05,
        "epoch": 1.8944,
        "step": 14208
    },
    {
        "loss": 1.4561,
        "grad_norm": 5.75745153427124,
        "learning_rate": 1.2349126370107966e-05,
        "epoch": 1.8945333333333334,
        "step": 14209
    },
    {
        "loss": 2.2701,
        "grad_norm": 2.621119976043701,
        "learning_rate": 1.2318846865302457e-05,
        "epoch": 1.8946666666666667,
        "step": 14210
    },
    {
        "loss": 1.7969,
        "grad_norm": 6.245023727416992,
        "learning_rate": 1.2288602091300316e-05,
        "epoch": 1.8948,
        "step": 14211
    },
    {
        "loss": 1.5887,
        "grad_norm": 6.608331203460693,
        "learning_rate": 1.2258392060081835e-05,
        "epoch": 1.8949333333333334,
        "step": 14212
    },
    {
        "loss": 2.0195,
        "grad_norm": 3.3366239070892334,
        "learning_rate": 1.2228216783613156e-05,
        "epoch": 1.8950666666666667,
        "step": 14213
    },
    {
        "loss": 2.0644,
        "grad_norm": 3.7448880672454834,
        "learning_rate": 1.2198076273846936e-05,
        "epoch": 1.8952,
        "step": 14214
    },
    {
        "loss": 1.4319,
        "grad_norm": 4.287405014038086,
        "learning_rate": 1.2167970542721918e-05,
        "epoch": 1.8953333333333333,
        "step": 14215
    },
    {
        "loss": 2.7837,
        "grad_norm": 3.811938524246216,
        "learning_rate": 1.2137899602163017e-05,
        "epoch": 1.8954666666666666,
        "step": 14216
    },
    {
        "loss": 2.2041,
        "grad_norm": 3.2643072605133057,
        "learning_rate": 1.2107863464081403e-05,
        "epoch": 1.8956,
        "step": 14217
    },
    {
        "loss": 2.3889,
        "grad_norm": 5.755713939666748,
        "learning_rate": 1.2077862140374651e-05,
        "epoch": 1.8957333333333333,
        "step": 14218
    },
    {
        "loss": 1.7605,
        "grad_norm": 3.5882208347320557,
        "learning_rate": 1.2047895642926254e-05,
        "epoch": 1.8958666666666666,
        "step": 14219
    },
    {
        "loss": 1.6867,
        "grad_norm": 3.218597650527954,
        "learning_rate": 1.201796398360605e-05,
        "epoch": 1.896,
        "step": 14220
    },
    {
        "loss": 2.2016,
        "grad_norm": 2.3547747135162354,
        "learning_rate": 1.1988067174270145e-05,
        "epoch": 1.8961333333333332,
        "step": 14221
    },
    {
        "loss": 2.759,
        "grad_norm": 3.0793099403381348,
        "learning_rate": 1.1958205226760666e-05,
        "epoch": 1.8962666666666665,
        "step": 14222
    },
    {
        "loss": 1.6335,
        "grad_norm": 3.903245687484741,
        "learning_rate": 1.1928378152906171e-05,
        "epoch": 1.8963999999999999,
        "step": 14223
    },
    {
        "loss": 2.8041,
        "grad_norm": 2.549042224884033,
        "learning_rate": 1.189858596452117e-05,
        "epoch": 1.8965333333333332,
        "step": 14224
    },
    {
        "loss": 2.2247,
        "grad_norm": 3.985349655151367,
        "learning_rate": 1.1868828673406462e-05,
        "epoch": 1.8966666666666665,
        "step": 14225
    },
    {
        "loss": 1.2467,
        "grad_norm": 3.3987979888916016,
        "learning_rate": 1.1839106291348977e-05,
        "epoch": 1.8968,
        "step": 14226
    },
    {
        "loss": 2.359,
        "grad_norm": 3.570845365524292,
        "learning_rate": 1.1809418830121933e-05,
        "epoch": 1.8969333333333334,
        "step": 14227
    },
    {
        "loss": 1.8496,
        "grad_norm": 5.991638660430908,
        "learning_rate": 1.1779766301484619e-05,
        "epoch": 1.8970666666666667,
        "step": 14228
    },
    {
        "loss": 1.7767,
        "grad_norm": 3.121894359588623,
        "learning_rate": 1.1750148717182374e-05,
        "epoch": 1.8972,
        "step": 14229
    },
    {
        "loss": 0.5306,
        "grad_norm": 2.688486337661743,
        "learning_rate": 1.1720566088947006e-05,
        "epoch": 1.8973333333333333,
        "step": 14230
    },
    {
        "loss": 2.0781,
        "grad_norm": 3.866746187210083,
        "learning_rate": 1.1691018428496226e-05,
        "epoch": 1.8974666666666666,
        "step": 14231
    },
    {
        "loss": 2.2835,
        "grad_norm": 2.897289514541626,
        "learning_rate": 1.1661505747533875e-05,
        "epoch": 1.8976,
        "step": 14232
    },
    {
        "loss": 2.641,
        "grad_norm": 3.20349383354187,
        "learning_rate": 1.1632028057750166e-05,
        "epoch": 1.8977333333333335,
        "step": 14233
    },
    {
        "loss": 0.9505,
        "grad_norm": 3.4880170822143555,
        "learning_rate": 1.1602585370821262e-05,
        "epoch": 1.8978666666666668,
        "step": 14234
    },
    {
        "loss": 2.745,
        "grad_norm": 3.544243335723877,
        "learning_rate": 1.1573177698409487e-05,
        "epoch": 1.8980000000000001,
        "step": 14235
    },
    {
        "loss": 2.5244,
        "grad_norm": 4.305459499359131,
        "learning_rate": 1.154380505216327e-05,
        "epoch": 1.8981333333333335,
        "step": 14236
    },
    {
        "loss": 2.8928,
        "grad_norm": 5.055683612823486,
        "learning_rate": 1.1514467443717358e-05,
        "epoch": 1.8982666666666668,
        "step": 14237
    },
    {
        "loss": 2.6439,
        "grad_norm": 3.335207462310791,
        "learning_rate": 1.1485164884692323e-05,
        "epoch": 1.8984,
        "step": 14238
    },
    {
        "loss": 1.758,
        "grad_norm": 4.580962657928467,
        "learning_rate": 1.1455897386695225e-05,
        "epoch": 1.8985333333333334,
        "step": 14239
    },
    {
        "loss": 1.6695,
        "grad_norm": 4.615141868591309,
        "learning_rate": 1.1426664961318734e-05,
        "epoch": 1.8986666666666667,
        "step": 14240
    },
    {
        "loss": 2.2793,
        "grad_norm": 3.0071473121643066,
        "learning_rate": 1.1397467620142055e-05,
        "epoch": 1.8988,
        "step": 14241
    },
    {
        "loss": 2.0922,
        "grad_norm": 4.147918224334717,
        "learning_rate": 1.1368305374730437e-05,
        "epoch": 1.8989333333333334,
        "step": 14242
    },
    {
        "loss": 2.0634,
        "grad_norm": 3.972644329071045,
        "learning_rate": 1.133917823663505e-05,
        "epoch": 1.8990666666666667,
        "step": 14243
    },
    {
        "loss": 1.0422,
        "grad_norm": 4.090548992156982,
        "learning_rate": 1.1310086217393268e-05,
        "epoch": 1.8992,
        "step": 14244
    },
    {
        "loss": 1.4273,
        "grad_norm": 4.298293590545654,
        "learning_rate": 1.1281029328528492e-05,
        "epoch": 1.8993333333333333,
        "step": 14245
    },
    {
        "loss": 1.8286,
        "grad_norm": 3.1857011318206787,
        "learning_rate": 1.125200758155035e-05,
        "epoch": 1.8994666666666666,
        "step": 14246
    },
    {
        "loss": 2.6869,
        "grad_norm": 2.7962002754211426,
        "learning_rate": 1.1223020987954346e-05,
        "epoch": 1.8996,
        "step": 14247
    },
    {
        "loss": 2.0902,
        "grad_norm": 4.693515777587891,
        "learning_rate": 1.1194069559222365e-05,
        "epoch": 1.8997333333333333,
        "step": 14248
    },
    {
        "loss": 1.7893,
        "grad_norm": 3.9432902336120605,
        "learning_rate": 1.1165153306821896e-05,
        "epoch": 1.8998666666666666,
        "step": 14249
    },
    {
        "loss": 2.2431,
        "grad_norm": 3.3243134021759033,
        "learning_rate": 1.1136272242206892e-05,
        "epoch": 1.9,
        "step": 14250
    },
    {
        "loss": 1.6311,
        "grad_norm": 3.0488102436065674,
        "learning_rate": 1.1107426376817332e-05,
        "epoch": 1.9001333333333332,
        "step": 14251
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.642120122909546,
        "learning_rate": 1.1078615722079055e-05,
        "epoch": 1.9002666666666665,
        "step": 14252
    },
    {
        "loss": 2.1685,
        "grad_norm": 2.7991058826446533,
        "learning_rate": 1.10498402894041e-05,
        "epoch": 1.9003999999999999,
        "step": 14253
    },
    {
        "loss": 2.6302,
        "grad_norm": 2.999674081802368,
        "learning_rate": 1.1021100090190429e-05,
        "epoch": 1.9005333333333332,
        "step": 14254
    },
    {
        "loss": 2.2459,
        "grad_norm": 5.224806785583496,
        "learning_rate": 1.0992395135822253e-05,
        "epoch": 1.9006666666666665,
        "step": 14255
    },
    {
        "loss": 1.3917,
        "grad_norm": 4.127714157104492,
        "learning_rate": 1.0963725437669669e-05,
        "epoch": 1.9008,
        "step": 14256
    },
    {
        "loss": 2.1359,
        "grad_norm": 3.5034339427948,
        "learning_rate": 1.0935091007088805e-05,
        "epoch": 1.9009333333333334,
        "step": 14257
    },
    {
        "loss": 2.1637,
        "grad_norm": 3.8189170360565186,
        "learning_rate": 1.0906491855421807e-05,
        "epoch": 1.9010666666666667,
        "step": 14258
    },
    {
        "loss": 1.8911,
        "grad_norm": 3.593857526779175,
        "learning_rate": 1.0877927993996994e-05,
        "epoch": 1.9012,
        "step": 14259
    },
    {
        "loss": 2.7306,
        "grad_norm": 4.583299160003662,
        "learning_rate": 1.084939943412867e-05,
        "epoch": 1.9013333333333333,
        "step": 14260
    },
    {
        "loss": 1.8248,
        "grad_norm": 3.2239274978637695,
        "learning_rate": 1.0820906187116898e-05,
        "epoch": 1.9014666666666666,
        "step": 14261
    },
    {
        "loss": 2.6429,
        "grad_norm": 2.642885684967041,
        "learning_rate": 1.0792448264248123e-05,
        "epoch": 1.9016,
        "step": 14262
    },
    {
        "loss": 2.7923,
        "grad_norm": 2.7611801624298096,
        "learning_rate": 1.0764025676794476e-05,
        "epoch": 1.9017333333333335,
        "step": 14263
    },
    {
        "loss": 1.4094,
        "grad_norm": 3.6700937747955322,
        "learning_rate": 1.0735638436014384e-05,
        "epoch": 1.9018666666666668,
        "step": 14264
    },
    {
        "loss": 2.4462,
        "grad_norm": 2.6940624713897705,
        "learning_rate": 1.0707286553152096e-05,
        "epoch": 1.9020000000000001,
        "step": 14265
    },
    {
        "loss": 3.0014,
        "grad_norm": 3.0362391471862793,
        "learning_rate": 1.067897003943784e-05,
        "epoch": 1.9021333333333335,
        "step": 14266
    },
    {
        "loss": 1.3447,
        "grad_norm": 3.9965875148773193,
        "learning_rate": 1.0650688906087857e-05,
        "epoch": 1.9022666666666668,
        "step": 14267
    },
    {
        "loss": 2.3372,
        "grad_norm": 3.5870518684387207,
        "learning_rate": 1.062244316430444e-05,
        "epoch": 1.9024,
        "step": 14268
    },
    {
        "loss": 1.7982,
        "grad_norm": 4.9810285568237305,
        "learning_rate": 1.059423282527594e-05,
        "epoch": 1.9025333333333334,
        "step": 14269
    },
    {
        "loss": 1.9262,
        "grad_norm": 2.847848653793335,
        "learning_rate": 1.0566057900176373e-05,
        "epoch": 1.9026666666666667,
        "step": 14270
    },
    {
        "loss": 2.2715,
        "grad_norm": 3.082571506500244,
        "learning_rate": 1.0537918400166036e-05,
        "epoch": 1.9028,
        "step": 14271
    },
    {
        "loss": 1.1031,
        "grad_norm": 4.44260311126709,
        "learning_rate": 1.050981433639101e-05,
        "epoch": 1.9029333333333334,
        "step": 14272
    },
    {
        "loss": 2.2336,
        "grad_norm": 2.2482032775878906,
        "learning_rate": 1.0481745719983528e-05,
        "epoch": 1.9030666666666667,
        "step": 14273
    },
    {
        "loss": 2.2288,
        "grad_norm": 3.201225996017456,
        "learning_rate": 1.0453712562061569e-05,
        "epoch": 1.9032,
        "step": 14274
    },
    {
        "loss": 2.1515,
        "grad_norm": 4.809305667877197,
        "learning_rate": 1.0425714873729209e-05,
        "epoch": 1.9033333333333333,
        "step": 14275
    },
    {
        "loss": 0.4228,
        "grad_norm": 2.465003490447998,
        "learning_rate": 1.0397752666076333e-05,
        "epoch": 1.9034666666666666,
        "step": 14276
    },
    {
        "loss": 2.258,
        "grad_norm": 5.150854110717773,
        "learning_rate": 1.036982595017899e-05,
        "epoch": 1.9036,
        "step": 14277
    },
    {
        "loss": 2.3877,
        "grad_norm": 3.303799629211426,
        "learning_rate": 1.0341934737099012e-05,
        "epoch": 1.9037333333333333,
        "step": 14278
    },
    {
        "loss": 3.8953,
        "grad_norm": 4.568039417266846,
        "learning_rate": 1.0314079037884117e-05,
        "epoch": 1.9038666666666666,
        "step": 14279
    },
    {
        "loss": 1.9426,
        "grad_norm": 3.3285574913024902,
        "learning_rate": 1.0286258863568166e-05,
        "epoch": 1.904,
        "step": 14280
    },
    {
        "loss": 2.5347,
        "grad_norm": 3.3885204792022705,
        "learning_rate": 1.0258474225170789e-05,
        "epoch": 1.9041333333333332,
        "step": 14281
    },
    {
        "loss": 2.9497,
        "grad_norm": 2.425661087036133,
        "learning_rate": 1.0230725133697483e-05,
        "epoch": 1.9042666666666666,
        "step": 14282
    },
    {
        "loss": 1.9499,
        "grad_norm": 4.193659782409668,
        "learning_rate": 1.020301160013989e-05,
        "epoch": 1.9043999999999999,
        "step": 14283
    },
    {
        "loss": 2.2882,
        "grad_norm": 4.389995098114014,
        "learning_rate": 1.0175333635475392e-05,
        "epoch": 1.9045333333333332,
        "step": 14284
    },
    {
        "loss": 2.3651,
        "grad_norm": 2.7112834453582764,
        "learning_rate": 1.0147691250667235e-05,
        "epoch": 1.9046666666666665,
        "step": 14285
    },
    {
        "loss": 1.3322,
        "grad_norm": 6.95648193359375,
        "learning_rate": 1.0120084456664803e-05,
        "epoch": 1.9048,
        "step": 14286
    },
    {
        "loss": 0.5446,
        "grad_norm": 2.3874447345733643,
        "learning_rate": 1.0092513264403192e-05,
        "epoch": 1.9049333333333334,
        "step": 14287
    },
    {
        "loss": 2.0294,
        "grad_norm": 2.7446556091308594,
        "learning_rate": 1.0064977684803356e-05,
        "epoch": 1.9050666666666667,
        "step": 14288
    },
    {
        "loss": 1.7288,
        "grad_norm": 3.6156864166259766,
        "learning_rate": 1.0037477728772383e-05,
        "epoch": 1.9052,
        "step": 14289
    },
    {
        "loss": 2.0192,
        "grad_norm": 7.7206950187683105,
        "learning_rate": 1.001001340720299e-05,
        "epoch": 1.9053333333333333,
        "step": 14290
    },
    {
        "loss": 1.9857,
        "grad_norm": 4.846325874328613,
        "learning_rate": 9.982584730973876e-06,
        "epoch": 1.9054666666666666,
        "step": 14291
    },
    {
        "loss": 2.749,
        "grad_norm": 2.5491368770599365,
        "learning_rate": 9.955191710949741e-06,
        "epoch": 1.9056,
        "step": 14292
    },
    {
        "loss": 2.1558,
        "grad_norm": 5.301769733428955,
        "learning_rate": 9.92783435798098e-06,
        "epoch": 1.9057333333333333,
        "step": 14293
    },
    {
        "loss": 1.8662,
        "grad_norm": 2.8593590259552,
        "learning_rate": 9.90051268290394e-06,
        "epoch": 1.9058666666666668,
        "step": 14294
    },
    {
        "loss": 1.08,
        "grad_norm": 3.633546829223633,
        "learning_rate": 9.873226696540793e-06,
        "epoch": 1.9060000000000001,
        "step": 14295
    },
    {
        "loss": 1.8004,
        "grad_norm": 3.784938335418701,
        "learning_rate": 9.845976409699687e-06,
        "epoch": 1.9061333333333335,
        "step": 14296
    },
    {
        "loss": 2.3353,
        "grad_norm": 3.680701494216919,
        "learning_rate": 9.818761833174472e-06,
        "epoch": 1.9062666666666668,
        "step": 14297
    },
    {
        "loss": 2.8965,
        "grad_norm": 3.03678822517395,
        "learning_rate": 9.791582977745107e-06,
        "epoch": 1.9064,
        "step": 14298
    },
    {
        "loss": 2.2962,
        "grad_norm": 2.8597328662872314,
        "learning_rate": 9.764439854176965e-06,
        "epoch": 1.9065333333333334,
        "step": 14299
    },
    {
        "loss": 2.3989,
        "grad_norm": 3.334760904312134,
        "learning_rate": 9.737332473221661e-06,
        "epoch": 1.9066666666666667,
        "step": 14300
    },
    {
        "loss": 2.6498,
        "grad_norm": 3.6895620822906494,
        "learning_rate": 9.710260845616615e-06,
        "epoch": 1.9068,
        "step": 14301
    },
    {
        "loss": 2.221,
        "grad_norm": 3.490748405456543,
        "learning_rate": 9.683224982084881e-06,
        "epoch": 1.9069333333333334,
        "step": 14302
    },
    {
        "loss": 0.9812,
        "grad_norm": 3.2396092414855957,
        "learning_rate": 9.656224893335497e-06,
        "epoch": 1.9070666666666667,
        "step": 14303
    },
    {
        "loss": 2.2064,
        "grad_norm": 2.2768800258636475,
        "learning_rate": 9.629260590063194e-06,
        "epoch": 1.9072,
        "step": 14304
    },
    {
        "loss": 0.7021,
        "grad_norm": 4.058343410491943,
        "learning_rate": 9.602332082948761e-06,
        "epoch": 1.9073333333333333,
        "step": 14305
    },
    {
        "loss": 2.5982,
        "grad_norm": 2.013725757598877,
        "learning_rate": 9.575439382658569e-06,
        "epoch": 1.9074666666666666,
        "step": 14306
    },
    {
        "loss": 2.5434,
        "grad_norm": 4.2179155349731445,
        "learning_rate": 9.54858249984505e-06,
        "epoch": 1.9076,
        "step": 14307
    },
    {
        "loss": 2.6781,
        "grad_norm": 4.345681667327881,
        "learning_rate": 9.521761445146093e-06,
        "epoch": 1.9077333333333333,
        "step": 14308
    },
    {
        "loss": 1.7299,
        "grad_norm": 2.3858113288879395,
        "learning_rate": 9.494976229185747e-06,
        "epoch": 1.9078666666666666,
        "step": 14309
    },
    {
        "loss": 1.9465,
        "grad_norm": 3.756335496902466,
        "learning_rate": 9.468226862573748e-06,
        "epoch": 1.908,
        "step": 14310
    },
    {
        "loss": 1.5002,
        "grad_norm": 3.8847622871398926,
        "learning_rate": 9.441513355905562e-06,
        "epoch": 1.9081333333333332,
        "step": 14311
    },
    {
        "loss": 1.9888,
        "grad_norm": 4.026939392089844,
        "learning_rate": 9.414835719762516e-06,
        "epoch": 1.9082666666666666,
        "step": 14312
    },
    {
        "loss": 2.834,
        "grad_norm": 3.191934823989868,
        "learning_rate": 9.388193964711645e-06,
        "epoch": 1.9083999999999999,
        "step": 14313
    },
    {
        "loss": 2.398,
        "grad_norm": 5.273515701293945,
        "learning_rate": 9.361588101305963e-06,
        "epoch": 1.9085333333333332,
        "step": 14314
    },
    {
        "loss": 2.1664,
        "grad_norm": 2.664815664291382,
        "learning_rate": 9.33501814008405e-06,
        "epoch": 1.9086666666666665,
        "step": 14315
    },
    {
        "loss": 2.1964,
        "grad_norm": 4.017107009887695,
        "learning_rate": 9.308484091570401e-06,
        "epoch": 1.9088,
        "step": 14316
    },
    {
        "loss": 2.4384,
        "grad_norm": 3.9178335666656494,
        "learning_rate": 9.281985966275175e-06,
        "epoch": 1.9089333333333334,
        "step": 14317
    },
    {
        "loss": 1.7828,
        "grad_norm": 3.7835469245910645,
        "learning_rate": 9.255523774694419e-06,
        "epoch": 1.9090666666666667,
        "step": 14318
    },
    {
        "loss": 1.7894,
        "grad_norm": 3.7636029720306396,
        "learning_rate": 9.229097527310005e-06,
        "epoch": 1.9092,
        "step": 14319
    },
    {
        "loss": 1.3596,
        "grad_norm": 4.651020050048828,
        "learning_rate": 9.202707234589258e-06,
        "epoch": 1.9093333333333333,
        "step": 14320
    },
    {
        "loss": 2.1201,
        "grad_norm": 4.784125804901123,
        "learning_rate": 9.176352906985597e-06,
        "epoch": 1.9094666666666666,
        "step": 14321
    },
    {
        "loss": 1.5453,
        "grad_norm": 3.598341226577759,
        "learning_rate": 9.150034554937969e-06,
        "epoch": 1.9096,
        "step": 14322
    },
    {
        "loss": 1.948,
        "grad_norm": 3.2185893058776855,
        "learning_rate": 9.123752188871292e-06,
        "epoch": 1.9097333333333333,
        "step": 14323
    },
    {
        "loss": 2.4154,
        "grad_norm": 3.4336259365081787,
        "learning_rate": 9.097505819196007e-06,
        "epoch": 1.9098666666666668,
        "step": 14324
    },
    {
        "loss": 1.3022,
        "grad_norm": 2.124937057495117,
        "learning_rate": 9.071295456308426e-06,
        "epoch": 1.9100000000000001,
        "step": 14325
    },
    {
        "loss": 2.48,
        "grad_norm": 3.2290499210357666,
        "learning_rate": 9.045121110590482e-06,
        "epoch": 1.9101333333333335,
        "step": 14326
    },
    {
        "loss": 2.4327,
        "grad_norm": 2.5172953605651855,
        "learning_rate": 9.01898279240998e-06,
        "epoch": 1.9102666666666668,
        "step": 14327
    },
    {
        "loss": 1.9817,
        "grad_norm": 5.382076740264893,
        "learning_rate": 8.992880512120528e-06,
        "epoch": 1.9104,
        "step": 14328
    },
    {
        "loss": 1.9536,
        "grad_norm": 4.810649394989014,
        "learning_rate": 8.966814280061087e-06,
        "epoch": 1.9105333333333334,
        "step": 14329
    },
    {
        "loss": 2.1155,
        "grad_norm": 5.098601818084717,
        "learning_rate": 8.940784106556743e-06,
        "epoch": 1.9106666666666667,
        "step": 14330
    },
    {
        "loss": 2.2145,
        "grad_norm": 3.3359298706054688,
        "learning_rate": 8.914790001918061e-06,
        "epoch": 1.9108,
        "step": 14331
    },
    {
        "loss": 2.4888,
        "grad_norm": 2.765012741088867,
        "learning_rate": 8.888831976441459e-06,
        "epoch": 1.9109333333333334,
        "step": 14332
    },
    {
        "loss": 2.0155,
        "grad_norm": 5.049325942993164,
        "learning_rate": 8.862910040408967e-06,
        "epoch": 1.9110666666666667,
        "step": 14333
    },
    {
        "loss": 1.6756,
        "grad_norm": 3.4278204441070557,
        "learning_rate": 8.837024204088363e-06,
        "epoch": 1.9112,
        "step": 14334
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.438749074935913,
        "learning_rate": 8.81117447773303e-06,
        "epoch": 1.9113333333333333,
        "step": 14335
    },
    {
        "loss": 2.1823,
        "grad_norm": 6.7351884841918945,
        "learning_rate": 8.785360871582282e-06,
        "epoch": 1.9114666666666666,
        "step": 14336
    },
    {
        "loss": 2.6699,
        "grad_norm": 4.1393561363220215,
        "learning_rate": 8.759583395860882e-06,
        "epoch": 1.9116,
        "step": 14337
    },
    {
        "loss": 2.2511,
        "grad_norm": 3.628948926925659,
        "learning_rate": 8.73384206077933e-06,
        "epoch": 1.9117333333333333,
        "step": 14338
    },
    {
        "loss": 3.4545,
        "grad_norm": 3.801067352294922,
        "learning_rate": 8.708136876534001e-06,
        "epoch": 1.9118666666666666,
        "step": 14339
    },
    {
        "loss": 2.1716,
        "grad_norm": 3.8684093952178955,
        "learning_rate": 8.682467853306697e-06,
        "epoch": 1.912,
        "step": 14340
    },
    {
        "loss": 1.3815,
        "grad_norm": 3.406804323196411,
        "learning_rate": 8.656835001264985e-06,
        "epoch": 1.9121333333333332,
        "step": 14341
    },
    {
        "loss": 2.5899,
        "grad_norm": 3.526784658432007,
        "learning_rate": 8.631238330562241e-06,
        "epoch": 1.9122666666666666,
        "step": 14342
    },
    {
        "loss": 2.6359,
        "grad_norm": 4.362590312957764,
        "learning_rate": 8.605677851337313e-06,
        "epoch": 1.9123999999999999,
        "step": 14343
    },
    {
        "loss": 2.3655,
        "grad_norm": 2.5849454402923584,
        "learning_rate": 8.580153573714756e-06,
        "epoch": 1.9125333333333332,
        "step": 14344
    },
    {
        "loss": 2.3918,
        "grad_norm": 2.7192583084106445,
        "learning_rate": 8.554665507804938e-06,
        "epoch": 1.9126666666666665,
        "step": 14345
    },
    {
        "loss": 1.9273,
        "grad_norm": 2.4500930309295654,
        "learning_rate": 8.529213663703694e-06,
        "epoch": 1.9127999999999998,
        "step": 14346
    },
    {
        "loss": 1.5897,
        "grad_norm": 3.552363157272339,
        "learning_rate": 8.50379805149254e-06,
        "epoch": 1.9129333333333334,
        "step": 14347
    },
    {
        "loss": 2.4236,
        "grad_norm": 3.125624418258667,
        "learning_rate": 8.478418681238809e-06,
        "epoch": 1.9130666666666667,
        "step": 14348
    },
    {
        "loss": 1.5653,
        "grad_norm": 3.8097333908081055,
        "learning_rate": 8.453075562995284e-06,
        "epoch": 1.9132,
        "step": 14349
    },
    {
        "loss": 2.2285,
        "grad_norm": 3.6545658111572266,
        "learning_rate": 8.427768706800398e-06,
        "epoch": 1.9133333333333333,
        "step": 14350
    },
    {
        "loss": 1.78,
        "grad_norm": 1.9834877252578735,
        "learning_rate": 8.402498122678437e-06,
        "epoch": 1.9134666666666666,
        "step": 14351
    },
    {
        "loss": 2.236,
        "grad_norm": 3.499870777130127,
        "learning_rate": 8.377263820639059e-06,
        "epoch": 1.9136,
        "step": 14352
    },
    {
        "loss": 1.158,
        "grad_norm": 8.313777923583984,
        "learning_rate": 8.35206581067769e-06,
        "epoch": 1.9137333333333333,
        "step": 14353
    },
    {
        "loss": 0.6594,
        "grad_norm": 3.1510801315307617,
        "learning_rate": 8.326904102775269e-06,
        "epoch": 1.9138666666666668,
        "step": 14354
    },
    {
        "loss": 1.9244,
        "grad_norm": 3.482015609741211,
        "learning_rate": 8.301778706898555e-06,
        "epoch": 1.9140000000000001,
        "step": 14355
    },
    {
        "loss": 2.5287,
        "grad_norm": 3.2133138179779053,
        "learning_rate": 8.276689632999712e-06,
        "epoch": 1.9141333333333335,
        "step": 14356
    },
    {
        "loss": 1.7974,
        "grad_norm": 5.210549831390381,
        "learning_rate": 8.251636891016745e-06,
        "epoch": 1.9142666666666668,
        "step": 14357
    },
    {
        "loss": 1.6213,
        "grad_norm": 5.654522895812988,
        "learning_rate": 8.226620490872916e-06,
        "epoch": 1.9144,
        "step": 14358
    },
    {
        "loss": 1.7962,
        "grad_norm": 4.244513034820557,
        "learning_rate": 8.201640442477432e-06,
        "epoch": 1.9145333333333334,
        "step": 14359
    },
    {
        "loss": 0.9405,
        "grad_norm": 7.297116756439209,
        "learning_rate": 8.176696755725e-06,
        "epoch": 1.9146666666666667,
        "step": 14360
    },
    {
        "loss": 1.9529,
        "grad_norm": 3.7851357460021973,
        "learning_rate": 8.151789440495893e-06,
        "epoch": 1.9148,
        "step": 14361
    },
    {
        "loss": 2.7463,
        "grad_norm": 2.590393304824829,
        "learning_rate": 8.126918506655923e-06,
        "epoch": 1.9149333333333334,
        "step": 14362
    },
    {
        "loss": 1.8242,
        "grad_norm": 6.194127082824707,
        "learning_rate": 8.10208396405654e-06,
        "epoch": 1.9150666666666667,
        "step": 14363
    },
    {
        "loss": 1.8308,
        "grad_norm": 6.009469985961914,
        "learning_rate": 8.077285822534896e-06,
        "epoch": 1.9152,
        "step": 14364
    },
    {
        "loss": 1.795,
        "grad_norm": 4.019514560699463,
        "learning_rate": 8.052524091913494e-06,
        "epoch": 1.9153333333333333,
        "step": 14365
    },
    {
        "loss": 1.7364,
        "grad_norm": 3.3882038593292236,
        "learning_rate": 8.027798782000694e-06,
        "epoch": 1.9154666666666667,
        "step": 14366
    },
    {
        "loss": 1.9505,
        "grad_norm": 4.885054588317871,
        "learning_rate": 8.003109902590112e-06,
        "epoch": 1.9156,
        "step": 14367
    },
    {
        "loss": 1.5922,
        "grad_norm": 3.546603202819824,
        "learning_rate": 7.978457463461142e-06,
        "epoch": 1.9157333333333333,
        "step": 14368
    },
    {
        "loss": 2.3051,
        "grad_norm": 4.884309768676758,
        "learning_rate": 7.953841474378787e-06,
        "epoch": 1.9158666666666666,
        "step": 14369
    },
    {
        "loss": 2.2722,
        "grad_norm": 3.5194284915924072,
        "learning_rate": 7.92926194509347e-06,
        "epoch": 1.916,
        "step": 14370
    },
    {
        "loss": 2.4028,
        "grad_norm": 2.4521710872650146,
        "learning_rate": 7.904718885341212e-06,
        "epoch": 1.9161333333333332,
        "step": 14371
    },
    {
        "loss": 2.0911,
        "grad_norm": 3.9932210445404053,
        "learning_rate": 7.88021230484356e-06,
        "epoch": 1.9162666666666666,
        "step": 14372
    },
    {
        "loss": 2.0293,
        "grad_norm": 3.6542422771453857,
        "learning_rate": 7.85574221330776e-06,
        "epoch": 1.9163999999999999,
        "step": 14373
    },
    {
        "loss": 2.9441,
        "grad_norm": 3.817171096801758,
        "learning_rate": 7.831308620426436e-06,
        "epoch": 1.9165333333333332,
        "step": 14374
    },
    {
        "loss": 1.1571,
        "grad_norm": 3.279412269592285,
        "learning_rate": 7.806911535877847e-06,
        "epoch": 1.9166666666666665,
        "step": 14375
    },
    {
        "loss": 1.2645,
        "grad_norm": 2.324550151824951,
        "learning_rate": 7.782550969325664e-06,
        "epoch": 1.9167999999999998,
        "step": 14376
    },
    {
        "loss": 1.675,
        "grad_norm": 3.6893303394317627,
        "learning_rate": 7.758226930419266e-06,
        "epoch": 1.9169333333333334,
        "step": 14377
    },
    {
        "loss": 1.8034,
        "grad_norm": 5.374731063842773,
        "learning_rate": 7.733939428793601e-06,
        "epoch": 1.9170666666666667,
        "step": 14378
    },
    {
        "loss": 1.1881,
        "grad_norm": 5.4066948890686035,
        "learning_rate": 7.7096884740688e-06,
        "epoch": 1.9172,
        "step": 14379
    },
    {
        "loss": 2.8405,
        "grad_norm": 3.1043050289154053,
        "learning_rate": 7.685474075850918e-06,
        "epoch": 1.9173333333333333,
        "step": 14380
    },
    {
        "loss": 2.2976,
        "grad_norm": 4.2433271408081055,
        "learning_rate": 7.661296243731276e-06,
        "epoch": 1.9174666666666667,
        "step": 14381
    },
    {
        "loss": 1.5901,
        "grad_norm": 5.08349609375,
        "learning_rate": 7.637154987286866e-06,
        "epoch": 1.9176,
        "step": 14382
    },
    {
        "loss": 2.8309,
        "grad_norm": 3.1838552951812744,
        "learning_rate": 7.613050316080094e-06,
        "epoch": 1.9177333333333333,
        "step": 14383
    },
    {
        "loss": 2.4034,
        "grad_norm": 2.4244792461395264,
        "learning_rate": 7.588982239658882e-06,
        "epoch": 1.9178666666666668,
        "step": 14384
    },
    {
        "loss": 2.5983,
        "grad_norm": 3.4220690727233887,
        "learning_rate": 7.564950767556633e-06,
        "epoch": 1.9180000000000001,
        "step": 14385
    },
    {
        "loss": 1.7959,
        "grad_norm": 4.260988235473633,
        "learning_rate": 7.540955909292346e-06,
        "epoch": 1.9181333333333335,
        "step": 14386
    },
    {
        "loss": 2.4976,
        "grad_norm": 4.230717658996582,
        "learning_rate": 7.516997674370596e-06,
        "epoch": 1.9182666666666668,
        "step": 14387
    },
    {
        "loss": 0.8779,
        "grad_norm": 3.4760801792144775,
        "learning_rate": 7.49307607228108e-06,
        "epoch": 1.9184,
        "step": 14388
    },
    {
        "loss": 0.8915,
        "grad_norm": 2.8758528232574463,
        "learning_rate": 7.469191112499397e-06,
        "epoch": 1.9185333333333334,
        "step": 14389
    },
    {
        "loss": 2.1685,
        "grad_norm": 3.5181832313537598,
        "learning_rate": 7.445342804486344e-06,
        "epoch": 1.9186666666666667,
        "step": 14390
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.5953681468963623,
        "learning_rate": 7.421531157688433e-06,
        "epoch": 1.9188,
        "step": 14391
    },
    {
        "loss": 0.8363,
        "grad_norm": 2.304263114929199,
        "learning_rate": 7.3977561815374965e-06,
        "epoch": 1.9189333333333334,
        "step": 14392
    },
    {
        "loss": 2.1086,
        "grad_norm": 4.188504695892334,
        "learning_rate": 7.374017885450846e-06,
        "epoch": 1.9190666666666667,
        "step": 14393
    },
    {
        "loss": 2.4914,
        "grad_norm": 3.8709352016448975,
        "learning_rate": 7.350316278831293e-06,
        "epoch": 1.9192,
        "step": 14394
    },
    {
        "loss": 2.3267,
        "grad_norm": 3.943073034286499,
        "learning_rate": 7.326651371067206e-06,
        "epoch": 1.9193333333333333,
        "step": 14395
    },
    {
        "loss": 2.1881,
        "grad_norm": 4.817655086517334,
        "learning_rate": 7.303023171532286e-06,
        "epoch": 1.9194666666666667,
        "step": 14396
    },
    {
        "loss": 2.105,
        "grad_norm": 4.356189727783203,
        "learning_rate": 7.279431689585703e-06,
        "epoch": 1.9196,
        "step": 14397
    },
    {
        "loss": 3.1898,
        "grad_norm": 3.676609992980957,
        "learning_rate": 7.255876934572204e-06,
        "epoch": 1.9197333333333333,
        "step": 14398
    },
    {
        "loss": 2.436,
        "grad_norm": 5.221517086029053,
        "learning_rate": 7.232358915821868e-06,
        "epoch": 1.9198666666666666,
        "step": 14399
    },
    {
        "loss": 2.3018,
        "grad_norm": 3.5100364685058594,
        "learning_rate": 7.2088776426502245e-06,
        "epoch": 1.92,
        "step": 14400
    },
    {
        "loss": 0.8285,
        "grad_norm": 3.795034170150757,
        "learning_rate": 7.185433124358365e-06,
        "epoch": 1.9201333333333332,
        "step": 14401
    },
    {
        "loss": 1.9358,
        "grad_norm": 4.395078182220459,
        "learning_rate": 7.162025370232706e-06,
        "epoch": 1.9202666666666666,
        "step": 14402
    },
    {
        "loss": 1.517,
        "grad_norm": 4.309587001800537,
        "learning_rate": 7.138654389545097e-06,
        "epoch": 1.9203999999999999,
        "step": 14403
    },
    {
        "loss": 1.4079,
        "grad_norm": 4.725033760070801,
        "learning_rate": 7.1153201915529325e-06,
        "epoch": 1.9205333333333332,
        "step": 14404
    },
    {
        "loss": 1.7122,
        "grad_norm": 3.5125012397766113,
        "learning_rate": 7.092022785498942e-06,
        "epoch": 1.9206666666666665,
        "step": 14405
    },
    {
        "loss": 2.8331,
        "grad_norm": 3.448760747909546,
        "learning_rate": 7.068762180611266e-06,
        "epoch": 1.9207999999999998,
        "step": 14406
    },
    {
        "loss": 1.9946,
        "grad_norm": 4.297680854797363,
        "learning_rate": 7.045538386103578e-06,
        "epoch": 1.9209333333333334,
        "step": 14407
    },
    {
        "loss": 0.6349,
        "grad_norm": 2.931063413619995,
        "learning_rate": 7.022351411174888e-06,
        "epoch": 1.9210666666666667,
        "step": 14408
    },
    {
        "loss": 1.4771,
        "grad_norm": 3.1485238075256348,
        "learning_rate": 6.999201265009547e-06,
        "epoch": 1.9212,
        "step": 14409
    },
    {
        "loss": 2.4228,
        "grad_norm": 3.6085526943206787,
        "learning_rate": 6.976087956777533e-06,
        "epoch": 1.9213333333333333,
        "step": 14410
    },
    {
        "loss": 1.5458,
        "grad_norm": 4.323941230773926,
        "learning_rate": 6.953011495634065e-06,
        "epoch": 1.9214666666666667,
        "step": 14411
    },
    {
        "loss": 2.0453,
        "grad_norm": 4.853679656982422,
        "learning_rate": 6.929971890719777e-06,
        "epoch": 1.9216,
        "step": 14412
    },
    {
        "loss": 1.998,
        "grad_norm": 3.5980165004730225,
        "learning_rate": 6.906969151160703e-06,
        "epoch": 1.9217333333333333,
        "step": 14413
    },
    {
        "loss": 2.4732,
        "grad_norm": 4.014077186584473,
        "learning_rate": 6.884003286068419e-06,
        "epoch": 1.9218666666666666,
        "step": 14414
    },
    {
        "loss": 2.4092,
        "grad_norm": 3.106520175933838,
        "learning_rate": 6.86107430453965e-06,
        "epoch": 1.9220000000000002,
        "step": 14415
    },
    {
        "loss": 2.8348,
        "grad_norm": 4.151473522186279,
        "learning_rate": 6.8381822156568296e-06,
        "epoch": 1.9221333333333335,
        "step": 14416
    },
    {
        "loss": 2.4496,
        "grad_norm": 4.581161975860596,
        "learning_rate": 6.815327028487362e-06,
        "epoch": 1.9222666666666668,
        "step": 14417
    },
    {
        "loss": 2.6363,
        "grad_norm": 2.9773240089416504,
        "learning_rate": 6.79250875208437e-06,
        "epoch": 1.9224,
        "step": 14418
    },
    {
        "loss": 1.6037,
        "grad_norm": 2.0803089141845703,
        "learning_rate": 6.769727395486314e-06,
        "epoch": 1.9225333333333334,
        "step": 14419
    },
    {
        "loss": 1.9318,
        "grad_norm": 2.3900070190429688,
        "learning_rate": 6.746982967716931e-06,
        "epoch": 1.9226666666666667,
        "step": 14420
    },
    {
        "loss": 0.9409,
        "grad_norm": 3.0310351848602295,
        "learning_rate": 6.724275477785325e-06,
        "epoch": 1.9228,
        "step": 14421
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.337458848953247,
        "learning_rate": 6.701604934686001e-06,
        "epoch": 1.9229333333333334,
        "step": 14422
    },
    {
        "loss": 1.2852,
        "grad_norm": 4.334039688110352,
        "learning_rate": 6.6789713473988994e-06,
        "epoch": 1.9230666666666667,
        "step": 14423
    },
    {
        "loss": 2.6358,
        "grad_norm": 3.918113946914673,
        "learning_rate": 6.65637472488918e-06,
        "epoch": 1.9232,
        "step": 14424
    },
    {
        "loss": 2.0752,
        "grad_norm": 3.364036798477173,
        "learning_rate": 6.633815076107608e-06,
        "epoch": 1.9233333333333333,
        "step": 14425
    },
    {
        "loss": 2.33,
        "grad_norm": 2.9379031658172607,
        "learning_rate": 6.611292409989911e-06,
        "epoch": 1.9234666666666667,
        "step": 14426
    },
    {
        "loss": 0.6797,
        "grad_norm": 4.349255561828613,
        "learning_rate": 6.588806735457498e-06,
        "epoch": 1.9236,
        "step": 14427
    },
    {
        "loss": 1.4453,
        "grad_norm": 3.8122143745422363,
        "learning_rate": 6.566358061417099e-06,
        "epoch": 1.9237333333333333,
        "step": 14428
    },
    {
        "loss": 1.2497,
        "grad_norm": 4.843715190887451,
        "learning_rate": 6.543946396760625e-06,
        "epoch": 1.9238666666666666,
        "step": 14429
    },
    {
        "loss": 2.2749,
        "grad_norm": 2.7911720275878906,
        "learning_rate": 6.521571750365429e-06,
        "epoch": 1.924,
        "step": 14430
    },
    {
        "loss": 2.2771,
        "grad_norm": 4.11342716217041,
        "learning_rate": 6.499234131094134e-06,
        "epoch": 1.9241333333333333,
        "step": 14431
    },
    {
        "loss": 2.7955,
        "grad_norm": 2.231912612915039,
        "learning_rate": 6.476933547794839e-06,
        "epoch": 1.9242666666666666,
        "step": 14432
    },
    {
        "loss": 1.5953,
        "grad_norm": 2.4900400638580322,
        "learning_rate": 6.454670009300856e-06,
        "epoch": 1.9243999999999999,
        "step": 14433
    },
    {
        "loss": 2.2072,
        "grad_norm": 3.250363349914551,
        "learning_rate": 6.432443524430809e-06,
        "epoch": 1.9245333333333332,
        "step": 14434
    },
    {
        "loss": 0.9803,
        "grad_norm": 5.13032341003418,
        "learning_rate": 6.410254101988656e-06,
        "epoch": 1.9246666666666665,
        "step": 14435
    },
    {
        "loss": 1.9623,
        "grad_norm": 4.84279727935791,
        "learning_rate": 6.388101750763775e-06,
        "epoch": 1.9247999999999998,
        "step": 14436
    },
    {
        "loss": 1.7495,
        "grad_norm": 4.3989152908325195,
        "learning_rate": 6.365986479530839e-06,
        "epoch": 1.9249333333333334,
        "step": 14437
    },
    {
        "loss": 2.3834,
        "grad_norm": 3.351034164428711,
        "learning_rate": 6.343908297049617e-06,
        "epoch": 1.9250666666666667,
        "step": 14438
    },
    {
        "loss": 2.6944,
        "grad_norm": 3.5946125984191895,
        "learning_rate": 6.321867212065513e-06,
        "epoch": 1.9252,
        "step": 14439
    },
    {
        "loss": 2.2609,
        "grad_norm": 3.223170757293701,
        "learning_rate": 6.299863233308934e-06,
        "epoch": 1.9253333333333333,
        "step": 14440
    },
    {
        "loss": 2.4246,
        "grad_norm": 2.478146553039551,
        "learning_rate": 6.27789636949585e-06,
        "epoch": 1.9254666666666667,
        "step": 14441
    },
    {
        "loss": 1.7677,
        "grad_norm": 4.052861213684082,
        "learning_rate": 6.2559666293273925e-06,
        "epoch": 1.9256,
        "step": 14442
    },
    {
        "loss": 1.9055,
        "grad_norm": 4.52855110168457,
        "learning_rate": 6.234074021489944e-06,
        "epoch": 1.9257333333333333,
        "step": 14443
    },
    {
        "loss": 4.103,
        "grad_norm": 7.949547290802002,
        "learning_rate": 6.212218554655258e-06,
        "epoch": 1.9258666666666666,
        "step": 14444
    },
    {
        "loss": 1.6593,
        "grad_norm": 4.765027046203613,
        "learning_rate": 6.190400237480354e-06,
        "epoch": 1.9260000000000002,
        "step": 14445
    },
    {
        "loss": 1.7596,
        "grad_norm": 3.310636281967163,
        "learning_rate": 6.168619078607663e-06,
        "epoch": 1.9261333333333335,
        "step": 14446
    },
    {
        "loss": 2.0617,
        "grad_norm": 5.114681720733643,
        "learning_rate": 6.146875086664594e-06,
        "epoch": 1.9262666666666668,
        "step": 14447
    },
    {
        "loss": 2.3704,
        "grad_norm": 2.6699936389923096,
        "learning_rate": 6.125168270264137e-06,
        "epoch": 1.9264000000000001,
        "step": 14448
    },
    {
        "loss": 2.1986,
        "grad_norm": 3.8297369480133057,
        "learning_rate": 6.103498638004357e-06,
        "epoch": 1.9265333333333334,
        "step": 14449
    },
    {
        "loss": 3.5429,
        "grad_norm": 6.179113864898682,
        "learning_rate": 6.081866198468755e-06,
        "epoch": 1.9266666666666667,
        "step": 14450
    },
    {
        "loss": 2.4881,
        "grad_norm": 6.214183330535889,
        "learning_rate": 6.060270960225966e-06,
        "epoch": 1.9268,
        "step": 14451
    },
    {
        "loss": 1.7211,
        "grad_norm": 4.857227325439453,
        "learning_rate": 6.038712931829948e-06,
        "epoch": 1.9269333333333334,
        "step": 14452
    },
    {
        "loss": 0.6964,
        "grad_norm": 3.1732892990112305,
        "learning_rate": 6.017192121819859e-06,
        "epoch": 1.9270666666666667,
        "step": 14453
    },
    {
        "loss": 2.6726,
        "grad_norm": 4.669503211975098,
        "learning_rate": 5.995708538720246e-06,
        "epoch": 1.9272,
        "step": 14454
    },
    {
        "loss": 1.7177,
        "grad_norm": 3.665208339691162,
        "learning_rate": 5.974262191040803e-06,
        "epoch": 1.9273333333333333,
        "step": 14455
    },
    {
        "loss": 1.5318,
        "grad_norm": 4.318024158477783,
        "learning_rate": 5.952853087276444e-06,
        "epoch": 1.9274666666666667,
        "step": 14456
    },
    {
        "loss": 1.8993,
        "grad_norm": 2.7546021938323975,
        "learning_rate": 5.931481235907466e-06,
        "epoch": 1.9276,
        "step": 14457
    },
    {
        "loss": 1.6667,
        "grad_norm": 4.489246845245361,
        "learning_rate": 5.910146645399317e-06,
        "epoch": 1.9277333333333333,
        "step": 14458
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.983752727508545,
        "learning_rate": 5.888849324202628e-06,
        "epoch": 1.9278666666666666,
        "step": 14459
    },
    {
        "loss": 3.0621,
        "grad_norm": 5.829256534576416,
        "learning_rate": 5.867589280753449e-06,
        "epoch": 1.928,
        "step": 14460
    },
    {
        "loss": 3.2324,
        "grad_norm": 3.160735845565796,
        "learning_rate": 5.846366523472912e-06,
        "epoch": 1.9281333333333333,
        "step": 14461
    },
    {
        "loss": 1.9194,
        "grad_norm": 3.586171865463257,
        "learning_rate": 5.825181060767404e-06,
        "epoch": 1.9282666666666666,
        "step": 14462
    },
    {
        "loss": 2.4299,
        "grad_norm": 3.220343828201294,
        "learning_rate": 5.804032901028522e-06,
        "epoch": 1.9284,
        "step": 14463
    },
    {
        "loss": 2.4252,
        "grad_norm": 3.782233238220215,
        "learning_rate": 5.782922052633233e-06,
        "epoch": 1.9285333333333332,
        "step": 14464
    },
    {
        "loss": 2.1795,
        "grad_norm": 3.3118624687194824,
        "learning_rate": 5.761848523943503e-06,
        "epoch": 1.9286666666666665,
        "step": 14465
    },
    {
        "loss": 2.3043,
        "grad_norm": 3.7135112285614014,
        "learning_rate": 5.740812323306721e-06,
        "epoch": 1.9287999999999998,
        "step": 14466
    },
    {
        "loss": 1.9159,
        "grad_norm": 3.984374523162842,
        "learning_rate": 5.719813459055368e-06,
        "epoch": 1.9289333333333334,
        "step": 14467
    },
    {
        "loss": 2.1151,
        "grad_norm": 3.0386390686035156,
        "learning_rate": 5.698851939507099e-06,
        "epoch": 1.9290666666666667,
        "step": 14468
    },
    {
        "loss": 3.099,
        "grad_norm": 3.1610255241394043,
        "learning_rate": 5.6779277729649635e-06,
        "epoch": 1.9292,
        "step": 14469
    },
    {
        "loss": 1.8645,
        "grad_norm": 3.9881370067596436,
        "learning_rate": 5.65704096771702e-06,
        "epoch": 1.9293333333333333,
        "step": 14470
    },
    {
        "loss": 2.4161,
        "grad_norm": 4.405121803283691,
        "learning_rate": 5.636191532036605e-06,
        "epoch": 1.9294666666666667,
        "step": 14471
    },
    {
        "loss": 1.768,
        "grad_norm": 4.1404032707214355,
        "learning_rate": 5.615379474182236e-06,
        "epoch": 1.9296,
        "step": 14472
    },
    {
        "loss": 2.18,
        "grad_norm": 2.9568560123443604,
        "learning_rate": 5.594604802397696e-06,
        "epoch": 1.9297333333333333,
        "step": 14473
    },
    {
        "loss": 1.5099,
        "grad_norm": 4.547136306762695,
        "learning_rate": 5.573867524911825e-06,
        "epoch": 1.9298666666666666,
        "step": 14474
    },
    {
        "loss": 2.75,
        "grad_norm": 3.2766149044036865,
        "learning_rate": 5.553167649938884e-06,
        "epoch": 1.9300000000000002,
        "step": 14475
    },
    {
        "loss": 1.7314,
        "grad_norm": 3.6594138145446777,
        "learning_rate": 5.5325051856779495e-06,
        "epoch": 1.9301333333333335,
        "step": 14476
    },
    {
        "loss": 2.3028,
        "grad_norm": 3.166930675506592,
        "learning_rate": 5.511880140313597e-06,
        "epoch": 1.9302666666666668,
        "step": 14477
    },
    {
        "loss": 2.4118,
        "grad_norm": 4.198437213897705,
        "learning_rate": 5.491292522015546e-06,
        "epoch": 1.9304000000000001,
        "step": 14478
    },
    {
        "loss": 1.8565,
        "grad_norm": 3.823683977127075,
        "learning_rate": 5.47074233893854e-06,
        "epoch": 1.9305333333333334,
        "step": 14479
    },
    {
        "loss": 2.2498,
        "grad_norm": 3.0151355266571045,
        "learning_rate": 5.4502295992226115e-06,
        "epoch": 1.9306666666666668,
        "step": 14480
    },
    {
        "loss": 1.8402,
        "grad_norm": 3.444108009338379,
        "learning_rate": 5.429754310992852e-06,
        "epoch": 1.9308,
        "step": 14481
    },
    {
        "loss": 1.9304,
        "grad_norm": 4.623025417327881,
        "learning_rate": 5.4093164823596946e-06,
        "epoch": 1.9309333333333334,
        "step": 14482
    },
    {
        "loss": 1.4532,
        "grad_norm": 6.14116096496582,
        "learning_rate": 5.3889161214185526e-06,
        "epoch": 1.9310666666666667,
        "step": 14483
    },
    {
        "loss": 1.4669,
        "grad_norm": 2.903634548187256,
        "learning_rate": 5.368553236250196e-06,
        "epoch": 1.9312,
        "step": 14484
    },
    {
        "loss": 1.2377,
        "grad_norm": 3.5134117603302,
        "learning_rate": 5.3482278349202605e-06,
        "epoch": 1.9313333333333333,
        "step": 14485
    },
    {
        "loss": 2.5578,
        "grad_norm": 6.158055305480957,
        "learning_rate": 5.327939925479808e-06,
        "epoch": 1.9314666666666667,
        "step": 14486
    },
    {
        "loss": 1.725,
        "grad_norm": 4.464629650115967,
        "learning_rate": 5.307689515964976e-06,
        "epoch": 1.9316,
        "step": 14487
    },
    {
        "loss": 2.2381,
        "grad_norm": 3.128601551055908,
        "learning_rate": 5.287476614396969e-06,
        "epoch": 1.9317333333333333,
        "step": 14488
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.958028554916382,
        "learning_rate": 5.267301228782229e-06,
        "epoch": 1.9318666666666666,
        "step": 14489
    },
    {
        "loss": 2.2299,
        "grad_norm": 3.33349347114563,
        "learning_rate": 5.2471633671121956e-06,
        "epoch": 1.932,
        "step": 14490
    },
    {
        "loss": 2.3426,
        "grad_norm": 3.7679481506347656,
        "learning_rate": 5.227063037363678e-06,
        "epoch": 1.9321333333333333,
        "step": 14491
    },
    {
        "loss": 3.2407,
        "grad_norm": 4.576475143432617,
        "learning_rate": 5.207000247498406e-06,
        "epoch": 1.9322666666666666,
        "step": 14492
    },
    {
        "loss": 2.007,
        "grad_norm": 3.7639501094818115,
        "learning_rate": 5.186975005463346e-06,
        "epoch": 1.9324,
        "step": 14493
    },
    {
        "loss": 2.1309,
        "grad_norm": 3.0103676319122314,
        "learning_rate": 5.166987319190508e-06,
        "epoch": 1.9325333333333332,
        "step": 14494
    },
    {
        "loss": 1.78,
        "grad_norm": 3.8421080112457275,
        "learning_rate": 5.147037196597127e-06,
        "epoch": 1.9326666666666665,
        "step": 14495
    },
    {
        "loss": 1.8776,
        "grad_norm": 4.73184871673584,
        "learning_rate": 5.127124645585601e-06,
        "epoch": 1.9327999999999999,
        "step": 14496
    },
    {
        "loss": 1.6481,
        "grad_norm": 4.99807071685791,
        "learning_rate": 5.107249674043191e-06,
        "epoch": 1.9329333333333332,
        "step": 14497
    },
    {
        "loss": 2.4731,
        "grad_norm": 3.9828972816467285,
        "learning_rate": 5.087412289842564e-06,
        "epoch": 1.9330666666666667,
        "step": 14498
    },
    {
        "loss": 1.6714,
        "grad_norm": 5.446437835693359,
        "learning_rate": 5.06761250084129e-06,
        "epoch": 1.9332,
        "step": 14499
    },
    {
        "loss": 1.7585,
        "grad_norm": 3.9858360290527344,
        "learning_rate": 5.047850314882241e-06,
        "epoch": 1.9333333333333333,
        "step": 14500
    },
    {
        "loss": 2.0523,
        "grad_norm": 4.837758541107178,
        "learning_rate": 5.028125739793199e-06,
        "epoch": 1.9334666666666667,
        "step": 14501
    },
    {
        "loss": 1.6793,
        "grad_norm": 4.076099872589111,
        "learning_rate": 5.008438783387181e-06,
        "epoch": 1.9336,
        "step": 14502
    },
    {
        "loss": 2.6631,
        "grad_norm": 3.3375308513641357,
        "learning_rate": 4.988789453462184e-06,
        "epoch": 1.9337333333333333,
        "step": 14503
    },
    {
        "loss": 2.0104,
        "grad_norm": 4.837427616119385,
        "learning_rate": 4.969177757801413e-06,
        "epoch": 1.9338666666666666,
        "step": 14504
    },
    {
        "loss": 2.4668,
        "grad_norm": 3.770764112472534,
        "learning_rate": 4.949603704173255e-06,
        "epoch": 1.9340000000000002,
        "step": 14505
    },
    {
        "loss": 2.4587,
        "grad_norm": 3.523453712463379,
        "learning_rate": 4.930067300330843e-06,
        "epoch": 1.9341333333333335,
        "step": 14506
    },
    {
        "loss": 0.711,
        "grad_norm": 3.491406202316284,
        "learning_rate": 4.910568554012751e-06,
        "epoch": 1.9342666666666668,
        "step": 14507
    },
    {
        "loss": 2.7432,
        "grad_norm": 3.302631378173828,
        "learning_rate": 4.891107472942447e-06,
        "epoch": 1.9344000000000001,
        "step": 14508
    },
    {
        "loss": 2.2936,
        "grad_norm": 4.028878688812256,
        "learning_rate": 4.8716840648285745e-06,
        "epoch": 1.9345333333333334,
        "step": 14509
    },
    {
        "loss": 2.0483,
        "grad_norm": 4.755571365356445,
        "learning_rate": 4.85229833736478e-06,
        "epoch": 1.9346666666666668,
        "step": 14510
    },
    {
        "loss": 0.9824,
        "grad_norm": 3.399172306060791,
        "learning_rate": 4.832950298229832e-06,
        "epoch": 1.9348,
        "step": 14511
    },
    {
        "loss": 1.3261,
        "grad_norm": 5.069240570068359,
        "learning_rate": 4.813639955087501e-06,
        "epoch": 1.9349333333333334,
        "step": 14512
    },
    {
        "loss": 2.65,
        "grad_norm": 3.2873973846435547,
        "learning_rate": 4.794367315586756e-06,
        "epoch": 1.9350666666666667,
        "step": 14513
    },
    {
        "loss": 2.6749,
        "grad_norm": 3.5865557193756104,
        "learning_rate": 4.775132387361548e-06,
        "epoch": 1.9352,
        "step": 14514
    },
    {
        "loss": 1.2586,
        "grad_norm": 4.123759746551514,
        "learning_rate": 4.7559351780308015e-06,
        "epoch": 1.9353333333333333,
        "step": 14515
    },
    {
        "loss": 1.676,
        "grad_norm": 4.248552322387695,
        "learning_rate": 4.7367756951987365e-06,
        "epoch": 1.9354666666666667,
        "step": 14516
    },
    {
        "loss": 1.719,
        "grad_norm": 3.1519079208374023,
        "learning_rate": 4.717653946454393e-06,
        "epoch": 1.9356,
        "step": 14517
    },
    {
        "loss": 1.2784,
        "grad_norm": 4.496662616729736,
        "learning_rate": 4.698569939371955e-06,
        "epoch": 1.9357333333333333,
        "step": 14518
    },
    {
        "loss": 1.8997,
        "grad_norm": 3.321115493774414,
        "learning_rate": 4.6795236815107554e-06,
        "epoch": 1.9358666666666666,
        "step": 14519
    },
    {
        "loss": 2.3689,
        "grad_norm": 3.9143502712249756,
        "learning_rate": 4.660515180414993e-06,
        "epoch": 1.936,
        "step": 14520
    },
    {
        "loss": 1.6117,
        "grad_norm": 5.216683864593506,
        "learning_rate": 4.641544443614043e-06,
        "epoch": 1.9361333333333333,
        "step": 14521
    },
    {
        "loss": 2.4726,
        "grad_norm": 2.1596593856811523,
        "learning_rate": 4.6226114786222295e-06,
        "epoch": 1.9362666666666666,
        "step": 14522
    },
    {
        "loss": 2.4979,
        "grad_norm": 2.588090181350708,
        "learning_rate": 4.603716292939031e-06,
        "epoch": 1.9364,
        "step": 14523
    },
    {
        "loss": 2.5765,
        "grad_norm": 2.4796290397644043,
        "learning_rate": 4.5848588940488265e-06,
        "epoch": 1.9365333333333332,
        "step": 14524
    },
    {
        "loss": 1.5951,
        "grad_norm": 3.8340094089508057,
        "learning_rate": 4.566039289421176e-06,
        "epoch": 1.9366666666666665,
        "step": 14525
    },
    {
        "loss": 1.7737,
        "grad_norm": 4.164052486419678,
        "learning_rate": 4.547257486510548e-06,
        "epoch": 1.9367999999999999,
        "step": 14526
    },
    {
        "loss": 2.6554,
        "grad_norm": 3.932363510131836,
        "learning_rate": 4.528513492756425e-06,
        "epoch": 1.9369333333333332,
        "step": 14527
    },
    {
        "loss": 2.6689,
        "grad_norm": 1.9523260593414307,
        "learning_rate": 4.509807315583469e-06,
        "epoch": 1.9370666666666667,
        "step": 14528
    },
    {
        "loss": 2.6724,
        "grad_norm": 2.931580066680908,
        "learning_rate": 4.491138962401209e-06,
        "epoch": 1.9372,
        "step": 14529
    },
    {
        "loss": 1.0076,
        "grad_norm": 4.026939392089844,
        "learning_rate": 4.4725084406042505e-06,
        "epoch": 1.9373333333333334,
        "step": 14530
    },
    {
        "loss": 2.4939,
        "grad_norm": 2.8350234031677246,
        "learning_rate": 4.453915757572147e-06,
        "epoch": 1.9374666666666667,
        "step": 14531
    },
    {
        "loss": 2.6073,
        "grad_norm": 3.722330331802368,
        "learning_rate": 4.435360920669618e-06,
        "epoch": 1.9376,
        "step": 14532
    },
    {
        "loss": 2.4271,
        "grad_norm": 2.629791021347046,
        "learning_rate": 4.41684393724624e-06,
        "epoch": 1.9377333333333333,
        "step": 14533
    },
    {
        "loss": 2.5664,
        "grad_norm": 4.645319938659668,
        "learning_rate": 4.398364814636724e-06,
        "epoch": 1.9378666666666666,
        "step": 14534
    },
    {
        "loss": 1.6856,
        "grad_norm": 2.526005983352661,
        "learning_rate": 4.3799235601605815e-06,
        "epoch": 1.938,
        "step": 14535
    },
    {
        "loss": 2.2974,
        "grad_norm": 3.028430938720703,
        "learning_rate": 4.361520181122547e-06,
        "epoch": 1.9381333333333335,
        "step": 14536
    },
    {
        "loss": 1.4938,
        "grad_norm": 3.6144614219665527,
        "learning_rate": 4.343154684812278e-06,
        "epoch": 1.9382666666666668,
        "step": 14537
    },
    {
        "loss": 2.3204,
        "grad_norm": 3.6196320056915283,
        "learning_rate": 4.324827078504401e-06,
        "epoch": 1.9384000000000001,
        "step": 14538
    },
    {
        "loss": 2.5448,
        "grad_norm": 4.869746208190918,
        "learning_rate": 4.306537369458497e-06,
        "epoch": 1.9385333333333334,
        "step": 14539
    },
    {
        "loss": 2.452,
        "grad_norm": 3.4442567825317383,
        "learning_rate": 4.288285564919181e-06,
        "epoch": 1.9386666666666668,
        "step": 14540
    },
    {
        "loss": 2.5468,
        "grad_norm": 3.2387242317199707,
        "learning_rate": 4.270071672116127e-06,
        "epoch": 1.9388,
        "step": 14541
    },
    {
        "loss": 2.0411,
        "grad_norm": 4.4921159744262695,
        "learning_rate": 4.251895698263864e-06,
        "epoch": 1.9389333333333334,
        "step": 14542
    },
    {
        "loss": 2.4109,
        "grad_norm": 4.736186504364014,
        "learning_rate": 4.233757650561964e-06,
        "epoch": 1.9390666666666667,
        "step": 14543
    },
    {
        "loss": 2.131,
        "grad_norm": 3.167438507080078,
        "learning_rate": 4.215657536194961e-06,
        "epoch": 1.9392,
        "step": 14544
    },
    {
        "loss": 2.4172,
        "grad_norm": 3.36830735206604,
        "learning_rate": 4.197595362332362e-06,
        "epoch": 1.9393333333333334,
        "step": 14545
    },
    {
        "loss": 1.6543,
        "grad_norm": 3.463421106338501,
        "learning_rate": 4.179571136128724e-06,
        "epoch": 1.9394666666666667,
        "step": 14546
    },
    {
        "loss": 1.8345,
        "grad_norm": 4.389009475708008,
        "learning_rate": 4.161584864723466e-06,
        "epoch": 1.9396,
        "step": 14547
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.812628746032715,
        "learning_rate": 4.143636555240993e-06,
        "epoch": 1.9397333333333333,
        "step": 14548
    },
    {
        "loss": 1.5018,
        "grad_norm": 2.234337329864502,
        "learning_rate": 4.125726214790659e-06,
        "epoch": 1.9398666666666666,
        "step": 14549
    },
    {
        "loss": 2.7296,
        "grad_norm": 3.1151106357574463,
        "learning_rate": 4.1078538504669135e-06,
        "epoch": 1.94,
        "step": 14550
    },
    {
        "loss": 2.369,
        "grad_norm": 3.6540427207946777,
        "learning_rate": 4.090019469348994e-06,
        "epoch": 1.9401333333333333,
        "step": 14551
    },
    {
        "loss": 2.2919,
        "grad_norm": 2.633579730987549,
        "learning_rate": 4.07222307850117e-06,
        "epoch": 1.9402666666666666,
        "step": 14552
    },
    {
        "loss": 0.9209,
        "grad_norm": 5.122392654418945,
        "learning_rate": 4.054464684972603e-06,
        "epoch": 1.9404,
        "step": 14553
    },
    {
        "loss": 2.6802,
        "grad_norm": 3.385100841522217,
        "learning_rate": 4.036744295797501e-06,
        "epoch": 1.9405333333333332,
        "step": 14554
    },
    {
        "loss": 1.1868,
        "grad_norm": 2.9357147216796875,
        "learning_rate": 4.01906191799506e-06,
        "epoch": 1.9406666666666665,
        "step": 14555
    },
    {
        "loss": 1.453,
        "grad_norm": 4.3858842849731445,
        "learning_rate": 4.0014175585691535e-06,
        "epoch": 1.9407999999999999,
        "step": 14556
    },
    {
        "loss": 1.849,
        "grad_norm": 3.962177276611328,
        "learning_rate": 3.983811224508893e-06,
        "epoch": 1.9409333333333332,
        "step": 14557
    },
    {
        "loss": 1.9721,
        "grad_norm": 3.786742925643921,
        "learning_rate": 3.966242922788155e-06,
        "epoch": 1.9410666666666667,
        "step": 14558
    },
    {
        "loss": 1.871,
        "grad_norm": 3.91357421875,
        "learning_rate": 3.948712660365838e-06,
        "epoch": 1.9412,
        "step": 14559
    },
    {
        "loss": 2.0818,
        "grad_norm": 2.7174184322357178,
        "learning_rate": 3.931220444185735e-06,
        "epoch": 1.9413333333333334,
        "step": 14560
    },
    {
        "loss": 2.7625,
        "grad_norm": 2.1975693702697754,
        "learning_rate": 3.913766281176556e-06,
        "epoch": 1.9414666666666667,
        "step": 14561
    },
    {
        "loss": 2.6731,
        "grad_norm": 2.854463815689087,
        "learning_rate": 3.896350178251929e-06,
        "epoch": 1.9416,
        "step": 14562
    },
    {
        "loss": 2.3509,
        "grad_norm": 4.149128437042236,
        "learning_rate": 3.8789721423105e-06,
        "epoch": 1.9417333333333333,
        "step": 14563
    },
    {
        "loss": 2.1973,
        "grad_norm": 2.255469560623169,
        "learning_rate": 3.861632180235719e-06,
        "epoch": 1.9418666666666666,
        "step": 14564
    },
    {
        "loss": 2.1242,
        "grad_norm": 3.161595344543457,
        "learning_rate": 3.8443302988959705e-06,
        "epoch": 1.942,
        "step": 14565
    },
    {
        "loss": 1.5714,
        "grad_norm": 2.939558506011963,
        "learning_rate": 3.8270665051446945e-06,
        "epoch": 1.9421333333333335,
        "step": 14566
    },
    {
        "loss": 3.0352,
        "grad_norm": 3.4160521030426025,
        "learning_rate": 3.80984080582002e-06,
        "epoch": 1.9422666666666668,
        "step": 14567
    },
    {
        "loss": 2.3276,
        "grad_norm": 3.2793619632720947,
        "learning_rate": 3.7926532077452004e-06,
        "epoch": 1.9424000000000001,
        "step": 14568
    },
    {
        "loss": 0.7357,
        "grad_norm": 3.1729090213775635,
        "learning_rate": 3.7755037177282793e-06,
        "epoch": 1.9425333333333334,
        "step": 14569
    },
    {
        "loss": 1.6097,
        "grad_norm": 3.8846871852874756,
        "learning_rate": 3.7583923425622e-06,
        "epoch": 1.9426666666666668,
        "step": 14570
    },
    {
        "loss": 2.2107,
        "grad_norm": 2.984685182571411,
        "learning_rate": 3.7413190890248085e-06,
        "epoch": 1.9428,
        "step": 14571
    },
    {
        "loss": 1.8965,
        "grad_norm": 3.7107174396514893,
        "learning_rate": 3.7242839638789716e-06,
        "epoch": 1.9429333333333334,
        "step": 14572
    },
    {
        "loss": 1.611,
        "grad_norm": 3.879023790359497,
        "learning_rate": 3.7072869738722925e-06,
        "epoch": 1.9430666666666667,
        "step": 14573
    },
    {
        "loss": 2.4229,
        "grad_norm": 2.006263256072998,
        "learning_rate": 3.6903281257373192e-06,
        "epoch": 1.9432,
        "step": 14574
    },
    {
        "loss": 1.1406,
        "grad_norm": 3.286275863647461,
        "learning_rate": 3.6734074261915886e-06,
        "epoch": 1.9433333333333334,
        "step": 14575
    },
    {
        "loss": 1.5256,
        "grad_norm": 4.912319660186768,
        "learning_rate": 3.6565248819373843e-06,
        "epoch": 1.9434666666666667,
        "step": 14576
    },
    {
        "loss": 1.0323,
        "grad_norm": 3.265441656112671,
        "learning_rate": 3.639680499661946e-06,
        "epoch": 1.9436,
        "step": 14577
    },
    {
        "loss": 1.6306,
        "grad_norm": 3.720189332962036,
        "learning_rate": 3.6228742860374145e-06,
        "epoch": 1.9437333333333333,
        "step": 14578
    },
    {
        "loss": 2.0355,
        "grad_norm": 2.3300771713256836,
        "learning_rate": 3.606106247720775e-06,
        "epoch": 1.9438666666666666,
        "step": 14579
    },
    {
        "loss": 1.4638,
        "grad_norm": 4.966442108154297,
        "learning_rate": 3.5893763913539267e-06,
        "epoch": 1.944,
        "step": 14580
    },
    {
        "loss": 2.3396,
        "grad_norm": 3.8372836112976074,
        "learning_rate": 3.572684723563546e-06,
        "epoch": 1.9441333333333333,
        "step": 14581
    },
    {
        "loss": 1.8036,
        "grad_norm": 5.497098445892334,
        "learning_rate": 3.5560312509613557e-06,
        "epoch": 1.9442666666666666,
        "step": 14582
    },
    {
        "loss": 2.3345,
        "grad_norm": 4.6586222648620605,
        "learning_rate": 3.5394159801437786e-06,
        "epoch": 1.9444,
        "step": 14583
    },
    {
        "loss": 1.6835,
        "grad_norm": 4.003175258636475,
        "learning_rate": 3.5228389176922394e-06,
        "epoch": 1.9445333333333332,
        "step": 14584
    },
    {
        "loss": 1.9614,
        "grad_norm": 3.230276346206665,
        "learning_rate": 3.506300070172963e-06,
        "epoch": 1.9446666666666665,
        "step": 14585
    },
    {
        "loss": 1.4487,
        "grad_norm": 4.764369487762451,
        "learning_rate": 3.4897994441369654e-06,
        "epoch": 1.9447999999999999,
        "step": 14586
    },
    {
        "loss": 2.447,
        "grad_norm": 2.8716094493865967,
        "learning_rate": 3.4733370461202954e-06,
        "epoch": 1.9449333333333332,
        "step": 14587
    },
    {
        "loss": 2.112,
        "grad_norm": 4.416607856750488,
        "learning_rate": 3.456912882643748e-06,
        "epoch": 1.9450666666666667,
        "step": 14588
    },
    {
        "loss": 1.661,
        "grad_norm": 3.599541664123535,
        "learning_rate": 3.440526960212953e-06,
        "epoch": 1.9452,
        "step": 14589
    },
    {
        "loss": 2.1757,
        "grad_norm": 3.3614537715911865,
        "learning_rate": 3.424179285318407e-06,
        "epoch": 1.9453333333333334,
        "step": 14590
    },
    {
        "loss": 1.9197,
        "grad_norm": 4.136447906494141,
        "learning_rate": 3.4078698644355756e-06,
        "epoch": 1.9454666666666667,
        "step": 14591
    },
    {
        "loss": 1.9713,
        "grad_norm": 3.4875025749206543,
        "learning_rate": 3.391598704024568e-06,
        "epoch": 1.9456,
        "step": 14592
    },
    {
        "loss": 1.9415,
        "grad_norm": 3.2980897426605225,
        "learning_rate": 3.375365810530573e-06,
        "epoch": 1.9457333333333333,
        "step": 14593
    },
    {
        "loss": 1.6638,
        "grad_norm": 4.741749286651611,
        "learning_rate": 3.359171190383359e-06,
        "epoch": 1.9458666666666666,
        "step": 14594
    },
    {
        "loss": 2.3184,
        "grad_norm": 4.665746212005615,
        "learning_rate": 3.3430148499977277e-06,
        "epoch": 1.946,
        "step": 14595
    },
    {
        "loss": 2.3879,
        "grad_norm": 2.306131362915039,
        "learning_rate": 3.3268967957732934e-06,
        "epoch": 1.9461333333333335,
        "step": 14596
    },
    {
        "loss": 1.7259,
        "grad_norm": 3.686027765274048,
        "learning_rate": 3.3108170340944487e-06,
        "epoch": 1.9462666666666668,
        "step": 14597
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.731504201889038,
        "learning_rate": 3.294775571330433e-06,
        "epoch": 1.9464000000000001,
        "step": 14598
    },
    {
        "loss": 2.0641,
        "grad_norm": 4.559146881103516,
        "learning_rate": 3.278772413835307e-06,
        "epoch": 1.9465333333333334,
        "step": 14599
    },
    {
        "loss": 2.295,
        "grad_norm": 2.7529070377349854,
        "learning_rate": 3.262807567948023e-06,
        "epoch": 1.9466666666666668,
        "step": 14600
    },
    {
        "loss": 0.9467,
        "grad_norm": 3.836684226989746,
        "learning_rate": 3.2468810399922776e-06,
        "epoch": 1.9468,
        "step": 14601
    },
    {
        "loss": 1.8659,
        "grad_norm": 4.032766819000244,
        "learning_rate": 3.2309928362766472e-06,
        "epoch": 1.9469333333333334,
        "step": 14602
    },
    {
        "loss": 2.5165,
        "grad_norm": 3.49497389793396,
        "learning_rate": 3.2151429630944307e-06,
        "epoch": 1.9470666666666667,
        "step": 14603
    },
    {
        "loss": 1.9796,
        "grad_norm": 3.3137505054473877,
        "learning_rate": 3.199331426723884e-06,
        "epoch": 1.9472,
        "step": 14604
    },
    {
        "loss": 2.6331,
        "grad_norm": 3.803865432739258,
        "learning_rate": 3.1835582334280412e-06,
        "epoch": 1.9473333333333334,
        "step": 14605
    },
    {
        "loss": 2.3961,
        "grad_norm": 4.357839584350586,
        "learning_rate": 3.1678233894546716e-06,
        "epoch": 1.9474666666666667,
        "step": 14606
    },
    {
        "loss": 1.1122,
        "grad_norm": 4.108084678649902,
        "learning_rate": 3.1521269010364007e-06,
        "epoch": 1.9476,
        "step": 14607
    },
    {
        "loss": 2.3768,
        "grad_norm": 2.876110315322876,
        "learning_rate": 3.136468774390633e-06,
        "epoch": 1.9477333333333333,
        "step": 14608
    },
    {
        "loss": 1.8508,
        "grad_norm": 5.506794452667236,
        "learning_rate": 3.1208490157196734e-06,
        "epoch": 1.9478666666666666,
        "step": 14609
    },
    {
        "loss": 0.5292,
        "grad_norm": 3.753342390060425,
        "learning_rate": 3.105267631210529e-06,
        "epoch": 1.948,
        "step": 14610
    },
    {
        "loss": 2.5484,
        "grad_norm": 4.039473056793213,
        "learning_rate": 3.089724627035029e-06,
        "epoch": 1.9481333333333333,
        "step": 14611
    },
    {
        "loss": 1.9505,
        "grad_norm": 4.031318664550781,
        "learning_rate": 3.074220009349782e-06,
        "epoch": 1.9482666666666666,
        "step": 14612
    },
    {
        "loss": 1.5009,
        "grad_norm": 3.9476609230041504,
        "learning_rate": 3.0587537842962645e-06,
        "epoch": 1.9484,
        "step": 14613
    },
    {
        "loss": 2.4152,
        "grad_norm": 4.371670722961426,
        "learning_rate": 3.0433259580007645e-06,
        "epoch": 1.9485333333333332,
        "step": 14614
    },
    {
        "loss": 1.9299,
        "grad_norm": 2.793241024017334,
        "learning_rate": 3.0279365365741387e-06,
        "epoch": 1.9486666666666665,
        "step": 14615
    },
    {
        "loss": 1.2371,
        "grad_norm": 5.821235656738281,
        "learning_rate": 3.0125855261123326e-06,
        "epoch": 1.9487999999999999,
        "step": 14616
    },
    {
        "loss": 2.3847,
        "grad_norm": 4.869539737701416,
        "learning_rate": 2.997272932695827e-06,
        "epoch": 1.9489333333333332,
        "step": 14617
    },
    {
        "loss": 2.3091,
        "grad_norm": 4.209815502166748,
        "learning_rate": 2.98199876239007e-06,
        "epoch": 1.9490666666666665,
        "step": 14618
    },
    {
        "loss": 1.9611,
        "grad_norm": 5.632624626159668,
        "learning_rate": 2.9667630212452e-06,
        "epoch": 1.9492,
        "step": 14619
    },
    {
        "loss": 1.5935,
        "grad_norm": 3.584476947784424,
        "learning_rate": 2.9515657152961118e-06,
        "epoch": 1.9493333333333334,
        "step": 14620
    },
    {
        "loss": 1.1892,
        "grad_norm": 3.9430787563323975,
        "learning_rate": 2.936406850562501e-06,
        "epoch": 1.9494666666666667,
        "step": 14621
    },
    {
        "loss": 1.2421,
        "grad_norm": 4.724220275878906,
        "learning_rate": 2.9212864330489197e-06,
        "epoch": 1.9496,
        "step": 14622
    },
    {
        "loss": 2.7298,
        "grad_norm": 4.354711532592773,
        "learning_rate": 2.9062044687445554e-06,
        "epoch": 1.9497333333333333,
        "step": 14623
    },
    {
        "loss": 2.542,
        "grad_norm": 1.7199300527572632,
        "learning_rate": 2.8911609636234183e-06,
        "epoch": 1.9498666666666666,
        "step": 14624
    },
    {
        "loss": 3.1057,
        "grad_norm": 3.2764015197753906,
        "learning_rate": 2.8761559236443413e-06,
        "epoch": 1.95,
        "step": 14625
    },
    {
        "loss": 2.0805,
        "grad_norm": 2.8525736331939697,
        "learning_rate": 2.861189354750804e-06,
        "epoch": 1.9501333333333335,
        "step": 14626
    },
    {
        "loss": 1.8492,
        "grad_norm": 5.750330924987793,
        "learning_rate": 2.846261262871197e-06,
        "epoch": 1.9502666666666668,
        "step": 14627
    },
    {
        "loss": 2.0515,
        "grad_norm": 3.253030300140381,
        "learning_rate": 2.8313716539185463e-06,
        "epoch": 1.9504000000000001,
        "step": 14628
    },
    {
        "loss": 2.3781,
        "grad_norm": 3.2781293392181396,
        "learning_rate": 2.8165205337906675e-06,
        "epoch": 1.9505333333333335,
        "step": 14629
    },
    {
        "loss": 2.3357,
        "grad_norm": 3.3429224491119385,
        "learning_rate": 2.8017079083701323e-06,
        "epoch": 1.9506666666666668,
        "step": 14630
    },
    {
        "loss": 0.5248,
        "grad_norm": 2.2172229290008545,
        "learning_rate": 2.786933783524315e-06,
        "epoch": 1.9508,
        "step": 14631
    },
    {
        "loss": 1.6636,
        "grad_norm": 3.3246138095855713,
        "learning_rate": 2.7721981651052776e-06,
        "epoch": 1.9509333333333334,
        "step": 14632
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.9415924549102783,
        "learning_rate": 2.7575010589497963e-06,
        "epoch": 1.9510666666666667,
        "step": 14633
    },
    {
        "loss": 0.684,
        "grad_norm": 3.4225051403045654,
        "learning_rate": 2.7428424708795255e-06,
        "epoch": 1.9512,
        "step": 14634
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.6535961627960205,
        "learning_rate": 2.7282224067007645e-06,
        "epoch": 1.9513333333333334,
        "step": 14635
    },
    {
        "loss": 1.7066,
        "grad_norm": 3.802981376647949,
        "learning_rate": 2.7136408722045036e-06,
        "epoch": 1.9514666666666667,
        "step": 14636
    },
    {
        "loss": 2.1946,
        "grad_norm": 3.177441120147705,
        "learning_rate": 2.6990978731666227e-06,
        "epoch": 1.9516,
        "step": 14637
    },
    {
        "loss": 2.3541,
        "grad_norm": 3.13281512260437,
        "learning_rate": 2.684593415347636e-06,
        "epoch": 1.9517333333333333,
        "step": 14638
    },
    {
        "loss": 2.8787,
        "grad_norm": 3.792154550552368,
        "learning_rate": 2.6701275044927698e-06,
        "epoch": 1.9518666666666666,
        "step": 14639
    },
    {
        "loss": 1.9533,
        "grad_norm": 3.9681286811828613,
        "learning_rate": 2.6557001463320184e-06,
        "epoch": 1.952,
        "step": 14640
    },
    {
        "loss": 2.562,
        "grad_norm": 2.232618808746338,
        "learning_rate": 2.6413113465801666e-06,
        "epoch": 1.9521333333333333,
        "step": 14641
    },
    {
        "loss": 2.4341,
        "grad_norm": 3.7778561115264893,
        "learning_rate": 2.626961110936599e-06,
        "epoch": 1.9522666666666666,
        "step": 14642
    },
    {
        "loss": 1.8561,
        "grad_norm": 2.8991026878356934,
        "learning_rate": 2.612649445085591e-06,
        "epoch": 1.9524,
        "step": 14643
    },
    {
        "loss": 2.3433,
        "grad_norm": 3.85442852973938,
        "learning_rate": 2.5983763546959083e-06,
        "epoch": 1.9525333333333332,
        "step": 14644
    },
    {
        "loss": 1.9082,
        "grad_norm": 4.187197685241699,
        "learning_rate": 2.5841418454212397e-06,
        "epoch": 1.9526666666666666,
        "step": 14645
    },
    {
        "loss": 1.622,
        "grad_norm": 3.988492965698242,
        "learning_rate": 2.5699459228999633e-06,
        "epoch": 1.9527999999999999,
        "step": 14646
    },
    {
        "loss": 2.218,
        "grad_norm": 3.5780766010284424,
        "learning_rate": 2.555788592755082e-06,
        "epoch": 1.9529333333333332,
        "step": 14647
    },
    {
        "loss": 2.7331,
        "grad_norm": 4.426389694213867,
        "learning_rate": 2.541669860594376e-06,
        "epoch": 1.9530666666666665,
        "step": 14648
    },
    {
        "loss": 1.8308,
        "grad_norm": 4.0380473136901855,
        "learning_rate": 2.5275897320102938e-06,
        "epoch": 1.9532,
        "step": 14649
    },
    {
        "loss": 2.4276,
        "grad_norm": 4.860890865325928,
        "learning_rate": 2.5135482125800635e-06,
        "epoch": 1.9533333333333334,
        "step": 14650
    },
    {
        "loss": 2.5907,
        "grad_norm": 3.6913700103759766,
        "learning_rate": 2.4995453078655475e-06,
        "epoch": 1.9534666666666667,
        "step": 14651
    },
    {
        "loss": 1.9781,
        "grad_norm": 3.6378700733184814,
        "learning_rate": 2.4855810234134193e-06,
        "epoch": 1.9536,
        "step": 14652
    },
    {
        "loss": 2.8898,
        "grad_norm": 2.981372833251953,
        "learning_rate": 2.4716553647548434e-06,
        "epoch": 1.9537333333333333,
        "step": 14653
    },
    {
        "loss": 1.3268,
        "grad_norm": 6.28473424911499,
        "learning_rate": 2.457768337405908e-06,
        "epoch": 1.9538666666666666,
        "step": 14654
    },
    {
        "loss": 2.6255,
        "grad_norm": 2.6786086559295654,
        "learning_rate": 2.443919946867346e-06,
        "epoch": 1.954,
        "step": 14655
    },
    {
        "loss": 2.0185,
        "grad_norm": 3.2192609310150146,
        "learning_rate": 2.43011019862448e-06,
        "epoch": 1.9541333333333335,
        "step": 14656
    },
    {
        "loss": 1.7973,
        "grad_norm": 2.962169647216797,
        "learning_rate": 2.4163390981474466e-06,
        "epoch": 1.9542666666666668,
        "step": 14657
    },
    {
        "loss": 1.8808,
        "grad_norm": 3.828345775604248,
        "learning_rate": 2.4026066508909705e-06,
        "epoch": 1.9544000000000001,
        "step": 14658
    },
    {
        "loss": 1.802,
        "grad_norm": 3.0163064002990723,
        "learning_rate": 2.388912862294568e-06,
        "epoch": 1.9545333333333335,
        "step": 14659
    },
    {
        "loss": 2.4337,
        "grad_norm": 3.2623937129974365,
        "learning_rate": 2.3752577377824105e-06,
        "epoch": 1.9546666666666668,
        "step": 14660
    },
    {
        "loss": 1.4353,
        "grad_norm": 2.6305010318756104,
        "learning_rate": 2.3616412827632826e-06,
        "epoch": 1.9548,
        "step": 14661
    },
    {
        "loss": 2.7402,
        "grad_norm": 2.22823166847229,
        "learning_rate": 2.348063502630726e-06,
        "epoch": 1.9549333333333334,
        "step": 14662
    },
    {
        "loss": 1.7883,
        "grad_norm": 3.4525363445281982,
        "learning_rate": 2.3345244027629607e-06,
        "epoch": 1.9550666666666667,
        "step": 14663
    },
    {
        "loss": 2.2587,
        "grad_norm": 3.298377752304077,
        "learning_rate": 2.321023988522908e-06,
        "epoch": 1.9552,
        "step": 14664
    },
    {
        "loss": 1.884,
        "grad_norm": 3.7937612533569336,
        "learning_rate": 2.307562265258034e-06,
        "epoch": 1.9553333333333334,
        "step": 14665
    },
    {
        "loss": 1.8828,
        "grad_norm": 3.398854970932007,
        "learning_rate": 2.2941392383006633e-06,
        "epoch": 1.9554666666666667,
        "step": 14666
    },
    {
        "loss": 1.9219,
        "grad_norm": 2.092210054397583,
        "learning_rate": 2.2807549129676197e-06,
        "epoch": 1.9556,
        "step": 14667
    },
    {
        "loss": 1.8967,
        "grad_norm": 4.472682476043701,
        "learning_rate": 2.267409294560563e-06,
        "epoch": 1.9557333333333333,
        "step": 14668
    },
    {
        "loss": 2.3886,
        "grad_norm": 3.235025405883789,
        "learning_rate": 2.2541023883657085e-06,
        "epoch": 1.9558666666666666,
        "step": 14669
    },
    {
        "loss": 2.3264,
        "grad_norm": 4.384780406951904,
        "learning_rate": 2.2408341996539516e-06,
        "epoch": 1.956,
        "step": 14670
    },
    {
        "loss": 1.8676,
        "grad_norm": 3.874701738357544,
        "learning_rate": 2.2276047336808546e-06,
        "epoch": 1.9561333333333333,
        "step": 14671
    },
    {
        "loss": 2.2435,
        "grad_norm": 3.3610105514526367,
        "learning_rate": 2.2144139956866595e-06,
        "epoch": 1.9562666666666666,
        "step": 14672
    },
    {
        "loss": 1.7672,
        "grad_norm": 2.1462290287017822,
        "learning_rate": 2.2012619908963307e-06,
        "epoch": 1.9564,
        "step": 14673
    },
    {
        "loss": 2.7468,
        "grad_norm": 2.418097496032715,
        "learning_rate": 2.1881487245193345e-06,
        "epoch": 1.9565333333333332,
        "step": 14674
    },
    {
        "loss": 2.2044,
        "grad_norm": 3.475971221923828,
        "learning_rate": 2.175074201749916e-06,
        "epoch": 1.9566666666666666,
        "step": 14675
    },
    {
        "loss": 1.3692,
        "grad_norm": 2.1465213298797607,
        "learning_rate": 2.1620384277669214e-06,
        "epoch": 1.9567999999999999,
        "step": 14676
    },
    {
        "loss": 1.9708,
        "grad_norm": 3.1611626148223877,
        "learning_rate": 2.149041407733909e-06,
        "epoch": 1.9569333333333332,
        "step": 14677
    },
    {
        "loss": 2.3927,
        "grad_norm": 4.076094150543213,
        "learning_rate": 2.136083146799017e-06,
        "epoch": 1.9570666666666665,
        "step": 14678
    },
    {
        "loss": 2.4685,
        "grad_norm": 3.8448190689086914,
        "learning_rate": 2.12316365009505e-06,
        "epoch": 1.9572,
        "step": 14679
    },
    {
        "loss": 2.563,
        "grad_norm": 4.3108744621276855,
        "learning_rate": 2.1102829227394373e-06,
        "epoch": 1.9573333333333334,
        "step": 14680
    },
    {
        "loss": 2.3989,
        "grad_norm": 5.9520087242126465,
        "learning_rate": 2.0974409698343413e-06,
        "epoch": 1.9574666666666667,
        "step": 14681
    },
    {
        "loss": 2.7102,
        "grad_norm": 4.078340530395508,
        "learning_rate": 2.084637796466471e-06,
        "epoch": 1.9576,
        "step": 14682
    },
    {
        "loss": 1.4877,
        "grad_norm": 4.8794355392456055,
        "learning_rate": 2.0718734077071813e-06,
        "epoch": 1.9577333333333333,
        "step": 14683
    },
    {
        "loss": 1.9943,
        "grad_norm": 4.1690754890441895,
        "learning_rate": 2.0591478086125383e-06,
        "epoch": 1.9578666666666666,
        "step": 14684
    },
    {
        "loss": 1.4206,
        "grad_norm": 7.248664379119873,
        "learning_rate": 2.046461004223155e-06,
        "epoch": 1.958,
        "step": 14685
    },
    {
        "loss": 2.1529,
        "grad_norm": 4.45091438293457,
        "learning_rate": 2.033812999564355e-06,
        "epoch": 1.9581333333333333,
        "step": 14686
    },
    {
        "loss": 1.2158,
        "grad_norm": 5.372364044189453,
        "learning_rate": 2.021203799646043e-06,
        "epoch": 1.9582666666666668,
        "step": 14687
    },
    {
        "loss": 2.311,
        "grad_norm": 4.583711624145508,
        "learning_rate": 2.008633409462768e-06,
        "epoch": 1.9584000000000001,
        "step": 14688
    },
    {
        "loss": 2.1773,
        "grad_norm": 3.594348192214966,
        "learning_rate": 1.9961018339936578e-06,
        "epoch": 1.9585333333333335,
        "step": 14689
    },
    {
        "loss": 1.7302,
        "grad_norm": 3.5456860065460205,
        "learning_rate": 1.983609078202575e-06,
        "epoch": 1.9586666666666668,
        "step": 14690
    },
    {
        "loss": 2.3482,
        "grad_norm": 3.2017154693603516,
        "learning_rate": 1.9711551470379288e-06,
        "epoch": 1.9588,
        "step": 14691
    },
    {
        "loss": 1.6518,
        "grad_norm": 3.5172297954559326,
        "learning_rate": 1.9587400454327274e-06,
        "epoch": 1.9589333333333334,
        "step": 14692
    },
    {
        "loss": 2.2674,
        "grad_norm": 2.6769163608551025,
        "learning_rate": 1.946363778304683e-06,
        "epoch": 1.9590666666666667,
        "step": 14693
    },
    {
        "loss": 2.3489,
        "grad_norm": 3.574213743209839,
        "learning_rate": 1.934026350556062e-06,
        "epoch": 1.9592,
        "step": 14694
    },
    {
        "loss": 2.4305,
        "grad_norm": 4.860766410827637,
        "learning_rate": 1.921727767073722e-06,
        "epoch": 1.9593333333333334,
        "step": 14695
    },
    {
        "loss": 2.7084,
        "grad_norm": 3.1981499195098877,
        "learning_rate": 1.9094680327292337e-06,
        "epoch": 1.9594666666666667,
        "step": 14696
    },
    {
        "loss": 2.4746,
        "grad_norm": 3.121460199356079,
        "learning_rate": 1.8972471523787006e-06,
        "epoch": 1.9596,
        "step": 14697
    },
    {
        "loss": 2.4154,
        "grad_norm": 3.9453446865081787,
        "learning_rate": 1.8850651308628287e-06,
        "epoch": 1.9597333333333333,
        "step": 14698
    },
    {
        "loss": 1.5136,
        "grad_norm": 4.479394435882568,
        "learning_rate": 1.872921973006969e-06,
        "epoch": 1.9598666666666666,
        "step": 14699
    },
    {
        "loss": 2.3314,
        "grad_norm": 3.299907922744751,
        "learning_rate": 1.8608176836210966e-06,
        "epoch": 1.96,
        "step": 14700
    },
    {
        "loss": 2.1789,
        "grad_norm": 4.1332597732543945,
        "learning_rate": 1.8487522674997094e-06,
        "epoch": 1.9601333333333333,
        "step": 14701
    },
    {
        "loss": 1.6146,
        "grad_norm": 3.0499649047851562,
        "learning_rate": 1.8367257294220619e-06,
        "epoch": 1.9602666666666666,
        "step": 14702
    },
    {
        "loss": 1.9824,
        "grad_norm": 3.994957208633423,
        "learning_rate": 1.8247380741517662e-06,
        "epoch": 1.9604,
        "step": 14703
    },
    {
        "loss": 1.3545,
        "grad_norm": 6.218601703643799,
        "learning_rate": 1.8127893064372458e-06,
        "epoch": 1.9605333333333332,
        "step": 14704
    },
    {
        "loss": 2.7134,
        "grad_norm": 2.1483616828918457,
        "learning_rate": 1.8008794310114707e-06,
        "epoch": 1.9606666666666666,
        "step": 14705
    },
    {
        "loss": 1.4241,
        "grad_norm": 3.901392936706543,
        "learning_rate": 1.789008452591967e-06,
        "epoch": 1.9607999999999999,
        "step": 14706
    },
    {
        "loss": 2.2908,
        "grad_norm": 3.809081554412842,
        "learning_rate": 1.7771763758808512e-06,
        "epoch": 1.9609333333333332,
        "step": 14707
    },
    {
        "loss": 2.3144,
        "grad_norm": 3.2470479011535645,
        "learning_rate": 1.7653832055648412e-06,
        "epoch": 1.9610666666666665,
        "step": 14708
    },
    {
        "loss": 2.0547,
        "grad_norm": 3.779245615005493,
        "learning_rate": 1.7536289463152887e-06,
        "epoch": 1.9612,
        "step": 14709
    },
    {
        "loss": 2.5008,
        "grad_norm": 2.4097487926483154,
        "learning_rate": 1.741913602788059e-06,
        "epoch": 1.9613333333333334,
        "step": 14710
    },
    {
        "loss": 2.2963,
        "grad_norm": 3.048313856124878,
        "learning_rate": 1.730237179623695e-06,
        "epoch": 1.9614666666666667,
        "step": 14711
    },
    {
        "loss": 1.8283,
        "grad_norm": 3.508601188659668,
        "learning_rate": 1.7185996814471861e-06,
        "epoch": 1.9616,
        "step": 14712
    },
    {
        "loss": 1.9723,
        "grad_norm": 3.388723850250244,
        "learning_rate": 1.707001112868234e-06,
        "epoch": 1.9617333333333333,
        "step": 14713
    },
    {
        "loss": 1.8817,
        "grad_norm": 3.5182693004608154,
        "learning_rate": 1.6954414784810857e-06,
        "epoch": 1.9618666666666666,
        "step": 14714
    },
    {
        "loss": 1.7045,
        "grad_norm": 4.6332502365112305,
        "learning_rate": 1.6839207828645342e-06,
        "epoch": 1.962,
        "step": 14715
    },
    {
        "loss": 2.1339,
        "grad_norm": 5.760105133056641,
        "learning_rate": 1.6724390305819737e-06,
        "epoch": 1.9621333333333333,
        "step": 14716
    },
    {
        "loss": 1.9344,
        "grad_norm": 3.023233652114868,
        "learning_rate": 1.6609962261813217e-06,
        "epoch": 1.9622666666666668,
        "step": 14717
    },
    {
        "loss": 1.9953,
        "grad_norm": 2.990450382232666,
        "learning_rate": 1.6495923741951747e-06,
        "epoch": 1.9624000000000001,
        "step": 14718
    },
    {
        "loss": 2.5068,
        "grad_norm": 3.5987162590026855,
        "learning_rate": 1.6382274791406082e-06,
        "epoch": 1.9625333333333335,
        "step": 14719
    },
    {
        "loss": 2.2456,
        "grad_norm": 2.3564741611480713,
        "learning_rate": 1.6269015455192882e-06,
        "epoch": 1.9626666666666668,
        "step": 14720
    },
    {
        "loss": 1.4557,
        "grad_norm": 4.765798568725586,
        "learning_rate": 1.615614577817448e-06,
        "epoch": 1.9628,
        "step": 14721
    },
    {
        "loss": 2.9928,
        "grad_norm": 3.2301785945892334,
        "learning_rate": 1.6043665805059115e-06,
        "epoch": 1.9629333333333334,
        "step": 14722
    },
    {
        "loss": 0.9633,
        "grad_norm": 3.623363733291626,
        "learning_rate": 1.5931575580400927e-06,
        "epoch": 1.9630666666666667,
        "step": 14723
    },
    {
        "loss": 2.2529,
        "grad_norm": 4.327004432678223,
        "learning_rate": 1.5819875148598285e-06,
        "epoch": 1.9632,
        "step": 14724
    },
    {
        "loss": 2.1522,
        "grad_norm": 3.4849789142608643,
        "learning_rate": 1.5708564553896798e-06,
        "epoch": 1.9633333333333334,
        "step": 14725
    },
    {
        "loss": 2.4007,
        "grad_norm": 2.5359156131744385,
        "learning_rate": 1.5597643840386644e-06,
        "epoch": 1.9634666666666667,
        "step": 14726
    },
    {
        "loss": 1.8705,
        "grad_norm": 5.384810924530029,
        "learning_rate": 1.5487113052004232e-06,
        "epoch": 1.9636,
        "step": 14727
    },
    {
        "loss": 1.49,
        "grad_norm": 2.93940806388855,
        "learning_rate": 1.5376972232530984e-06,
        "epoch": 1.9637333333333333,
        "step": 14728
    },
    {
        "loss": 2.2604,
        "grad_norm": 4.160141468048096,
        "learning_rate": 1.5267221425594115e-06,
        "epoch": 1.9638666666666666,
        "step": 14729
    },
    {
        "loss": 2.6384,
        "grad_norm": 3.0158801078796387,
        "learning_rate": 1.5157860674665959e-06,
        "epoch": 1.964,
        "step": 14730
    },
    {
        "loss": 1.9567,
        "grad_norm": 5.071504592895508,
        "learning_rate": 1.5048890023064865e-06,
        "epoch": 1.9641333333333333,
        "step": 14731
    },
    {
        "loss": 0.9541,
        "grad_norm": 3.0547049045562744,
        "learning_rate": 1.4940309513955197e-06,
        "epoch": 1.9642666666666666,
        "step": 14732
    },
    {
        "loss": 0.8311,
        "grad_norm": 2.1380181312561035,
        "learning_rate": 1.4832119190344884e-06,
        "epoch": 1.9644,
        "step": 14733
    },
    {
        "loss": 2.7206,
        "grad_norm": 2.8833343982696533,
        "learning_rate": 1.4724319095089312e-06,
        "epoch": 1.9645333333333332,
        "step": 14734
    },
    {
        "loss": 1.3538,
        "grad_norm": 3.7517287731170654,
        "learning_rate": 1.461690927088788e-06,
        "epoch": 1.9646666666666666,
        "step": 14735
    },
    {
        "loss": 2.2531,
        "grad_norm": 2.401038646697998,
        "learning_rate": 1.4509889760286555e-06,
        "epoch": 1.9647999999999999,
        "step": 14736
    },
    {
        "loss": 2.5709,
        "grad_norm": 3.392709732055664,
        "learning_rate": 1.4403260605675873e-06,
        "epoch": 1.9649333333333332,
        "step": 14737
    },
    {
        "loss": 1.5994,
        "grad_norm": 4.393020153045654,
        "learning_rate": 1.4297021849292048e-06,
        "epoch": 1.9650666666666665,
        "step": 14738
    },
    {
        "loss": 2.1151,
        "grad_norm": 3.219370126724243,
        "learning_rate": 1.4191173533216307e-06,
        "epoch": 1.9651999999999998,
        "step": 14739
    },
    {
        "loss": 2.4699,
        "grad_norm": 4.222344398498535,
        "learning_rate": 1.408571569937611e-06,
        "epoch": 1.9653333333333334,
        "step": 14740
    },
    {
        "loss": 2.1293,
        "grad_norm": 3.752887487411499,
        "learning_rate": 1.3980648389543272e-06,
        "epoch": 1.9654666666666667,
        "step": 14741
    },
    {
        "loss": 1.2569,
        "grad_norm": 4.315119743347168,
        "learning_rate": 1.3875971645335051e-06,
        "epoch": 1.9656,
        "step": 14742
    },
    {
        "loss": 1.0115,
        "grad_norm": 4.042085647583008,
        "learning_rate": 1.3771685508214949e-06,
        "epoch": 1.9657333333333333,
        "step": 14743
    },
    {
        "loss": 2.2478,
        "grad_norm": 3.925807476043701,
        "learning_rate": 1.36677900194907e-06,
        "epoch": 1.9658666666666667,
        "step": 14744
    },
    {
        "loss": 1.3054,
        "grad_norm": 3.7794814109802246,
        "learning_rate": 1.3564285220315386e-06,
        "epoch": 1.966,
        "step": 14745
    },
    {
        "loss": 1.7681,
        "grad_norm": 4.630925178527832,
        "learning_rate": 1.3461171151688211e-06,
        "epoch": 1.9661333333333333,
        "step": 14746
    },
    {
        "loss": 1.9163,
        "grad_norm": 3.706753730773926,
        "learning_rate": 1.3358447854452616e-06,
        "epoch": 1.9662666666666668,
        "step": 14747
    },
    {
        "loss": 2.1816,
        "grad_norm": 3.9152021408081055,
        "learning_rate": 1.3256115369297495e-06,
        "epoch": 1.9664000000000001,
        "step": 14748
    },
    {
        "loss": 1.6568,
        "grad_norm": 3.046299695968628,
        "learning_rate": 1.3154173736757647e-06,
        "epoch": 1.9665333333333335,
        "step": 14749
    },
    {
        "loss": 2.3578,
        "grad_norm": 3.240149736404419,
        "learning_rate": 1.3052622997211994e-06,
        "epoch": 1.9666666666666668,
        "step": 14750
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.7246596813201904,
        "learning_rate": 1.295146319088525e-06,
        "epoch": 1.9668,
        "step": 14751
    },
    {
        "loss": 2.0533,
        "grad_norm": 3.5218799114227295,
        "learning_rate": 1.285069435784736e-06,
        "epoch": 1.9669333333333334,
        "step": 14752
    },
    {
        "loss": 2.2568,
        "grad_norm": 4.174200057983398,
        "learning_rate": 1.2750316538013063e-06,
        "epoch": 1.9670666666666667,
        "step": 14753
    },
    {
        "loss": 1.5099,
        "grad_norm": 5.064554214477539,
        "learning_rate": 1.2650329771142222e-06,
        "epoch": 1.9672,
        "step": 14754
    },
    {
        "loss": 2.1353,
        "grad_norm": 5.370400428771973,
        "learning_rate": 1.2550734096840266e-06,
        "epoch": 1.9673333333333334,
        "step": 14755
    },
    {
        "loss": 2.3545,
        "grad_norm": 4.009164810180664,
        "learning_rate": 1.2451529554557085e-06,
        "epoch": 1.9674666666666667,
        "step": 14756
    },
    {
        "loss": 2.507,
        "grad_norm": 4.528841018676758,
        "learning_rate": 1.2352716183588131e-06,
        "epoch": 1.9676,
        "step": 14757
    },
    {
        "loss": 2.4646,
        "grad_norm": 3.4650232791900635,
        "learning_rate": 1.2254294023073432e-06,
        "epoch": 1.9677333333333333,
        "step": 14758
    },
    {
        "loss": 2.0281,
        "grad_norm": 2.736523389816284,
        "learning_rate": 1.215626311199869e-06,
        "epoch": 1.9678666666666667,
        "step": 14759
    },
    {
        "loss": 2.164,
        "grad_norm": 3.6521995067596436,
        "learning_rate": 1.2058623489193954e-06,
        "epoch": 1.968,
        "step": 14760
    },
    {
        "loss": 2.275,
        "grad_norm": 3.5962533950805664,
        "learning_rate": 1.1961375193335066e-06,
        "epoch": 1.9681333333333333,
        "step": 14761
    },
    {
        "loss": 1.9179,
        "grad_norm": 4.23663854598999,
        "learning_rate": 1.186451826294177e-06,
        "epoch": 1.9682666666666666,
        "step": 14762
    },
    {
        "loss": 2.677,
        "grad_norm": 3.3520476818084717,
        "learning_rate": 1.1768052736379931e-06,
        "epoch": 1.9684,
        "step": 14763
    },
    {
        "loss": 2.1585,
        "grad_norm": 3.4384071826934814,
        "learning_rate": 1.167197865185976e-06,
        "epoch": 1.9685333333333332,
        "step": 14764
    },
    {
        "loss": 2.0374,
        "grad_norm": 3.744938373565674,
        "learning_rate": 1.1576296047436597e-06,
        "epoch": 1.9686666666666666,
        "step": 14765
    },
    {
        "loss": 2.2429,
        "grad_norm": 4.366753101348877,
        "learning_rate": 1.1481004961010455e-06,
        "epoch": 1.9687999999999999,
        "step": 14766
    },
    {
        "loss": 2.4101,
        "grad_norm": 5.766388893127441,
        "learning_rate": 1.1386105430326366e-06,
        "epoch": 1.9689333333333332,
        "step": 14767
    },
    {
        "loss": 1.6989,
        "grad_norm": 4.118526458740234,
        "learning_rate": 1.1291597492974703e-06,
        "epoch": 1.9690666666666665,
        "step": 14768
    },
    {
        "loss": 2.4116,
        "grad_norm": 3.354400873184204,
        "learning_rate": 1.119748118639008e-06,
        "epoch": 1.9691999999999998,
        "step": 14769
    },
    {
        "loss": 2.3381,
        "grad_norm": 3.4834535121917725,
        "learning_rate": 1.1103756547852783e-06,
        "epoch": 1.9693333333333334,
        "step": 14770
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.7370619773864746,
        "learning_rate": 1.1010423614486676e-06,
        "epoch": 1.9694666666666667,
        "step": 14771
    },
    {
        "loss": 1.6566,
        "grad_norm": 6.113655090332031,
        "learning_rate": 1.0917482423261737e-06,
        "epoch": 1.9696,
        "step": 14772
    },
    {
        "loss": 0.7005,
        "grad_norm": 5.268630504608154,
        "learning_rate": 1.082493301099241e-06,
        "epoch": 1.9697333333333333,
        "step": 14773
    },
    {
        "loss": 2.5834,
        "grad_norm": 3.8600587844848633,
        "learning_rate": 1.0732775414337593e-06,
        "epoch": 1.9698666666666667,
        "step": 14774
    },
    {
        "loss": 0.6452,
        "grad_norm": 3.7668566703796387,
        "learning_rate": 1.0641009669801304e-06,
        "epoch": 1.97,
        "step": 14775
    },
    {
        "loss": 2.2984,
        "grad_norm": 2.3316009044647217,
        "learning_rate": 1.054963581373203e-06,
        "epoch": 1.9701333333333333,
        "step": 14776
    },
    {
        "loss": 2.7596,
        "grad_norm": 2.4625916481018066,
        "learning_rate": 1.045865388232381e-06,
        "epoch": 1.9702666666666668,
        "step": 14777
    },
    {
        "loss": 2.3766,
        "grad_norm": 2.6353750228881836,
        "learning_rate": 1.0368063911614379e-06,
        "epoch": 1.9704000000000002,
        "step": 14778
    },
    {
        "loss": 2.0439,
        "grad_norm": 2.565077304840088,
        "learning_rate": 1.0277865937487031e-06,
        "epoch": 1.9705333333333335,
        "step": 14779
    },
    {
        "loss": 2.0656,
        "grad_norm": 5.355300426483154,
        "learning_rate": 1.0188059995669187e-06,
        "epoch": 1.9706666666666668,
        "step": 14780
    },
    {
        "loss": 2.1359,
        "grad_norm": 4.20304536819458,
        "learning_rate": 1.009864612173339e-06,
        "epoch": 1.9708,
        "step": 14781
    },
    {
        "loss": 1.1454,
        "grad_norm": 3.6651971340179443,
        "learning_rate": 1.0009624351097425e-06,
        "epoch": 1.9709333333333334,
        "step": 14782
    },
    {
        "loss": 2.1737,
        "grad_norm": 2.3233466148376465,
        "learning_rate": 9.920994719022081e-07,
        "epoch": 1.9710666666666667,
        "step": 14783
    },
    {
        "loss": 0.9649,
        "grad_norm": 4.225969314575195,
        "learning_rate": 9.8327572606145e-07,
        "epoch": 1.9712,
        "step": 14784
    },
    {
        "loss": 1.566,
        "grad_norm": 3.7459349632263184,
        "learning_rate": 9.744912010825502e-07,
        "epoch": 1.9713333333333334,
        "step": 14785
    },
    {
        "loss": 2.0045,
        "grad_norm": 2.571676015853882,
        "learning_rate": 9.657459004451253e-07,
        "epoch": 1.9714666666666667,
        "step": 14786
    },
    {
        "loss": 1.9919,
        "grad_norm": 2.537626266479492,
        "learning_rate": 9.570398276132043e-07,
        "epoch": 1.9716,
        "step": 14787
    },
    {
        "loss": 2.5248,
        "grad_norm": 3.8373706340789795,
        "learning_rate": 9.483729860352842e-07,
        "epoch": 1.9717333333333333,
        "step": 14788
    },
    {
        "loss": 2.4071,
        "grad_norm": 2.549377918243408,
        "learning_rate": 9.397453791443189e-07,
        "epoch": 1.9718666666666667,
        "step": 14789
    },
    {
        "loss": 1.8219,
        "grad_norm": 4.204666614532471,
        "learning_rate": 9.311570103577416e-07,
        "epoch": 1.972,
        "step": 14790
    },
    {
        "loss": 1.1373,
        "grad_norm": 4.416178226470947,
        "learning_rate": 9.226078830774864e-07,
        "epoch": 1.9721333333333333,
        "step": 14791
    },
    {
        "loss": 0.6008,
        "grad_norm": 3.8755059242248535,
        "learning_rate": 9.140980006898115e-07,
        "epoch": 1.9722666666666666,
        "step": 14792
    },
    {
        "loss": 1.7303,
        "grad_norm": 3.352821111679077,
        "learning_rate": 9.056273665655646e-07,
        "epoch": 1.9724,
        "step": 14793
    },
    {
        "loss": 2.5581,
        "grad_norm": 3.217745065689087,
        "learning_rate": 8.971959840599398e-07,
        "epoch": 1.9725333333333332,
        "step": 14794
    },
    {
        "loss": 2.9157,
        "grad_norm": 3.7921974658966064,
        "learning_rate": 8.888038565126989e-07,
        "epoch": 1.9726666666666666,
        "step": 14795
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.3455679416656494,
        "learning_rate": 8.804509872479716e-07,
        "epoch": 1.9727999999999999,
        "step": 14796
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.72652268409729,
        "learning_rate": 8.721373795743337e-07,
        "epoch": 1.9729333333333332,
        "step": 14797
    },
    {
        "loss": 1.8203,
        "grad_norm": 3.6494300365448,
        "learning_rate": 8.638630367848399e-07,
        "epoch": 1.9730666666666665,
        "step": 14798
    },
    {
        "loss": 1.5966,
        "grad_norm": 2.09078311920166,
        "learning_rate": 8.55627962157024e-07,
        "epoch": 1.9731999999999998,
        "step": 14799
    },
    {
        "loss": 0.5336,
        "grad_norm": 3.41910719871521,
        "learning_rate": 8.474321589527768e-07,
        "epoch": 1.9733333333333334,
        "step": 14800
    },
    {
        "loss": 2.2467,
        "grad_norm": 4.416956901550293,
        "learning_rate": 8.392756304185013e-07,
        "epoch": 1.9734666666666667,
        "step": 14801
    },
    {
        "loss": 2.6515,
        "grad_norm": 3.8582828044891357,
        "learning_rate": 8.311583797850464e-07,
        "epoch": 1.9736,
        "step": 14802
    },
    {
        "loss": 3.039,
        "grad_norm": 4.50084924697876,
        "learning_rate": 8.230804102676626e-07,
        "epoch": 1.9737333333333333,
        "step": 14803
    },
    {
        "loss": 2.4332,
        "grad_norm": 2.232517719268799,
        "learning_rate": 8.150417250660569e-07,
        "epoch": 1.9738666666666667,
        "step": 14804
    },
    {
        "loss": 1.846,
        "grad_norm": 4.023641109466553,
        "learning_rate": 8.070423273643934e-07,
        "epoch": 1.974,
        "step": 14805
    },
    {
        "loss": 2.1574,
        "grad_norm": 2.8660356998443604,
        "learning_rate": 7.990822203312597e-07,
        "epoch": 1.9741333333333333,
        "step": 14806
    },
    {
        "loss": 2.7875,
        "grad_norm": 4.862863063812256,
        "learning_rate": 7.911614071196671e-07,
        "epoch": 1.9742666666666666,
        "step": 14807
    },
    {
        "loss": 2.4738,
        "grad_norm": 3.6669797897338867,
        "learning_rate": 7.832798908671057e-07,
        "epoch": 1.9744000000000002,
        "step": 14808
    },
    {
        "loss": 1.7695,
        "grad_norm": 3.8066089153289795,
        "learning_rate": 7.754376746954451e-07,
        "epoch": 1.9745333333333335,
        "step": 14809
    },
    {
        "loss": 1.3336,
        "grad_norm": 2.7043588161468506,
        "learning_rate": 7.676347617110003e-07,
        "epoch": 1.9746666666666668,
        "step": 14810
    },
    {
        "loss": 2.1069,
        "grad_norm": 4.320762634277344,
        "learning_rate": 7.598711550045767e-07,
        "epoch": 1.9748,
        "step": 14811
    },
    {
        "loss": 2.1252,
        "grad_norm": 3.407341480255127,
        "learning_rate": 7.521468576513369e-07,
        "epoch": 1.9749333333333334,
        "step": 14812
    },
    {
        "loss": 2.8205,
        "grad_norm": 2.411975860595703,
        "learning_rate": 7.444618727108887e-07,
        "epoch": 1.9750666666666667,
        "step": 14813
    },
    {
        "loss": 1.3897,
        "grad_norm": 3.6694447994232178,
        "learning_rate": 7.368162032273196e-07,
        "epoch": 1.9752,
        "step": 14814
    },
    {
        "loss": 2.3844,
        "grad_norm": 4.153933048248291,
        "learning_rate": 7.292098522290847e-07,
        "epoch": 1.9753333333333334,
        "step": 14815
    },
    {
        "loss": 2.0508,
        "grad_norm": 4.776740074157715,
        "learning_rate": 7.216428227290739e-07,
        "epoch": 1.9754666666666667,
        "step": 14816
    },
    {
        "loss": 2.6099,
        "grad_norm": 3.233412981033325,
        "learning_rate": 7.14115117724612e-07,
        "epoch": 1.9756,
        "step": 14817
    },
    {
        "loss": 1.7495,
        "grad_norm": 3.6314210891723633,
        "learning_rate": 7.066267401974802e-07,
        "epoch": 1.9757333333333333,
        "step": 14818
    },
    {
        "loss": 2.7375,
        "grad_norm": 4.998173236846924,
        "learning_rate": 6.991776931138061e-07,
        "epoch": 1.9758666666666667,
        "step": 14819
    },
    {
        "loss": 2.2418,
        "grad_norm": 2.542062282562256,
        "learning_rate": 6.917679794242516e-07,
        "epoch": 1.976,
        "step": 14820
    },
    {
        "loss": 2.5115,
        "grad_norm": 2.3269290924072266,
        "learning_rate": 6.843976020637466e-07,
        "epoch": 1.9761333333333333,
        "step": 14821
    },
    {
        "loss": 2.5283,
        "grad_norm": 4.79244327545166,
        "learning_rate": 6.77066563951756e-07,
        "epoch": 1.9762666666666666,
        "step": 14822
    },
    {
        "loss": 2.0838,
        "grad_norm": 3.5586020946502686,
        "learning_rate": 6.697748679921567e-07,
        "epoch": 1.9764,
        "step": 14823
    },
    {
        "loss": 0.4988,
        "grad_norm": 2.9090917110443115,
        "learning_rate": 6.625225170731831e-07,
        "epoch": 1.9765333333333333,
        "step": 14824
    },
    {
        "loss": 1.3029,
        "grad_norm": 3.5607776641845703,
        "learning_rate": 6.553095140675369e-07,
        "epoch": 1.9766666666666666,
        "step": 14825
    },
    {
        "loss": 2.584,
        "grad_norm": 4.152447700500488,
        "learning_rate": 6.481358618322775e-07,
        "epoch": 1.9768,
        "step": 14826
    },
    {
        "loss": 0.9666,
        "grad_norm": 4.634001731872559,
        "learning_rate": 6.410015632089539e-07,
        "epoch": 1.9769333333333332,
        "step": 14827
    },
    {
        "loss": 2.1605,
        "grad_norm": 5.167710781097412,
        "learning_rate": 6.339066210234501e-07,
        "epoch": 1.9770666666666665,
        "step": 14828
    },
    {
        "loss": 1.0886,
        "grad_norm": 3.9862279891967773,
        "learning_rate": 6.268510380861514e-07,
        "epoch": 1.9771999999999998,
        "step": 14829
    },
    {
        "loss": 1.7864,
        "grad_norm": 4.685899257659912,
        "learning_rate": 6.198348171917334e-07,
        "epoch": 1.9773333333333334,
        "step": 14830
    },
    {
        "loss": 1.5974,
        "grad_norm": 3.777254343032837,
        "learning_rate": 6.128579611193619e-07,
        "epoch": 1.9774666666666667,
        "step": 14831
    },
    {
        "loss": 2.305,
        "grad_norm": 2.6659910678863525,
        "learning_rate": 6.059204726326373e-07,
        "epoch": 1.9776,
        "step": 14832
    },
    {
        "loss": 2.115,
        "grad_norm": 3.1141912937164307,
        "learning_rate": 5.990223544794726e-07,
        "epoch": 1.9777333333333333,
        "step": 14833
    },
    {
        "loss": 2.3675,
        "grad_norm": 1.784128189086914,
        "learning_rate": 5.921636093922711e-07,
        "epoch": 1.9778666666666667,
        "step": 14834
    },
    {
        "loss": 2.7857,
        "grad_norm": 3.9861505031585693,
        "learning_rate": 5.853442400877596e-07,
        "epoch": 1.978,
        "step": 14835
    },
    {
        "loss": 2.4807,
        "grad_norm": 4.067106246948242,
        "learning_rate": 5.78564249267155e-07,
        "epoch": 1.9781333333333333,
        "step": 14836
    },
    {
        "loss": 2.1583,
        "grad_norm": 3.326843500137329,
        "learning_rate": 5.718236396160315e-07,
        "epoch": 1.9782666666666666,
        "step": 14837
    },
    {
        "loss": 2.0951,
        "grad_norm": 2.822791337966919,
        "learning_rate": 5.651224138043532e-07,
        "epoch": 1.9784000000000002,
        "step": 14838
    },
    {
        "loss": 1.5145,
        "grad_norm": 3.6461682319641113,
        "learning_rate": 5.584605744864857e-07,
        "epoch": 1.9785333333333335,
        "step": 14839
    },
    {
        "loss": 2.2864,
        "grad_norm": 3.2500219345092773,
        "learning_rate": 5.518381243012293e-07,
        "epoch": 1.9786666666666668,
        "step": 14840
    },
    {
        "loss": 2.762,
        "grad_norm": 5.2623186111450195,
        "learning_rate": 5.452550658717858e-07,
        "epoch": 1.9788000000000001,
        "step": 14841
    },
    {
        "loss": 0.705,
        "grad_norm": 3.3349506855010986,
        "learning_rate": 5.387114018056582e-07,
        "epoch": 1.9789333333333334,
        "step": 14842
    },
    {
        "loss": 3.1038,
        "grad_norm": 4.537702560424805,
        "learning_rate": 5.32207134694862e-07,
        "epoch": 1.9790666666666668,
        "step": 14843
    },
    {
        "loss": 0.462,
        "grad_norm": 2.8309977054595947,
        "learning_rate": 5.257422671157364e-07,
        "epoch": 1.9792,
        "step": 14844
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.386101245880127,
        "learning_rate": 5.193168016290662e-07,
        "epoch": 1.9793333333333334,
        "step": 14845
    },
    {
        "loss": 1.0257,
        "grad_norm": 3.5163497924804688,
        "learning_rate": 5.129307407799821e-07,
        "epoch": 1.9794666666666667,
        "step": 14846
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.2850489616394043,
        "learning_rate": 5.065840870980276e-07,
        "epoch": 1.9796,
        "step": 14847
    },
    {
        "loss": 2.4568,
        "grad_norm": 4.228135585784912,
        "learning_rate": 5.002768430971139e-07,
        "epoch": 1.9797333333333333,
        "step": 14848
    },
    {
        "loss": 2.1893,
        "grad_norm": 4.084376335144043,
        "learning_rate": 4.940090112755757e-07,
        "epoch": 1.9798666666666667,
        "step": 14849
    },
    {
        "loss": 2.739,
        "grad_norm": 4.227123260498047,
        "learning_rate": 4.877805941161606e-07,
        "epoch": 1.98,
        "step": 14850
    },
    {
        "loss": 2.196,
        "grad_norm": 2.8061840534210205,
        "learning_rate": 4.815915940859062e-07,
        "epoch": 1.9801333333333333,
        "step": 14851
    },
    {
        "loss": 0.7765,
        "grad_norm": 3.543971538543701,
        "learning_rate": 4.7544201363632913e-07,
        "epoch": 1.9802666666666666,
        "step": 14852
    },
    {
        "loss": 1.5777,
        "grad_norm": 4.033082485198975,
        "learning_rate": 4.6933185520328107e-07,
        "epoch": 1.9804,
        "step": 14853
    },
    {
        "loss": 2.6361,
        "grad_norm": 3.9351563453674316,
        "learning_rate": 4.63261121207037e-07,
        "epoch": 1.9805333333333333,
        "step": 14854
    },
    {
        "loss": 1.582,
        "grad_norm": 5.7210235595703125,
        "learning_rate": 4.572298140522291e-07,
        "epoch": 1.9806666666666666,
        "step": 14855
    },
    {
        "loss": 0.7457,
        "grad_norm": 5.362161636352539,
        "learning_rate": 4.5123793612787957e-07,
        "epoch": 1.9808,
        "step": 14856
    },
    {
        "loss": 2.0627,
        "grad_norm": 3.4051830768585205,
        "learning_rate": 4.452854898073788e-07,
        "epoch": 1.9809333333333332,
        "step": 14857
    },
    {
        "loss": 1.6651,
        "grad_norm": 6.0706610679626465,
        "learning_rate": 4.393724774485186e-07,
        "epoch": 1.9810666666666665,
        "step": 14858
    },
    {
        "loss": 1.4713,
        "grad_norm": 3.9736483097076416,
        "learning_rate": 4.3349890139346983e-07,
        "epoch": 1.9811999999999999,
        "step": 14859
    },
    {
        "loss": 2.0843,
        "grad_norm": 2.8879377841949463,
        "learning_rate": 4.2766476396876034e-07,
        "epoch": 1.9813333333333332,
        "step": 14860
    },
    {
        "loss": 1.4548,
        "grad_norm": 3.456332206726074,
        "learning_rate": 4.2187006748533043e-07,
        "epoch": 1.9814666666666667,
        "step": 14861
    },
    {
        "loss": 1.5354,
        "grad_norm": 4.966158390045166,
        "learning_rate": 4.1611481423847745e-07,
        "epoch": 1.9816,
        "step": 14862
    },
    {
        "loss": 2.5364,
        "grad_norm": 5.092439651489258,
        "learning_rate": 4.1039900650785556e-07,
        "epoch": 1.9817333333333333,
        "step": 14863
    },
    {
        "loss": 2.0759,
        "grad_norm": 4.961788177490234,
        "learning_rate": 4.0472264655754266e-07,
        "epoch": 1.9818666666666667,
        "step": 14864
    },
    {
        "loss": 3.0373,
        "grad_norm": 3.5372085571289062,
        "learning_rate": 3.990857366359513e-07,
        "epoch": 1.982,
        "step": 14865
    },
    {
        "loss": 2.5965,
        "grad_norm": 3.1278765201568604,
        "learning_rate": 3.9348827897587313e-07,
        "epoch": 1.9821333333333333,
        "step": 14866
    },
    {
        "loss": 0.9352,
        "grad_norm": 2.8478994369506836,
        "learning_rate": 3.879302757944903e-07,
        "epoch": 1.9822666666666666,
        "step": 14867
    },
    {
        "loss": 1.9775,
        "grad_norm": 3.5573537349700928,
        "learning_rate": 3.824117292933638e-07,
        "epoch": 1.9824000000000002,
        "step": 14868
    },
    {
        "loss": 2.9947,
        "grad_norm": 4.786499500274658,
        "learning_rate": 3.769326416583563e-07,
        "epoch": 1.9825333333333335,
        "step": 14869
    },
    {
        "loss": 1.9644,
        "grad_norm": 3.6718554496765137,
        "learning_rate": 3.714930150598095e-07,
        "epoch": 1.9826666666666668,
        "step": 14870
    },
    {
        "loss": 2.3309,
        "grad_norm": 4.851568222045898,
        "learning_rate": 3.6609285165235543e-07,
        "epoch": 1.9828000000000001,
        "step": 14871
    },
    {
        "loss": 1.9343,
        "grad_norm": 4.091764450073242,
        "learning_rate": 3.6073215357500525e-07,
        "epoch": 1.9829333333333334,
        "step": 14872
    },
    {
        "loss": 2.0741,
        "grad_norm": 2.6299984455108643,
        "learning_rate": 3.554109229511715e-07,
        "epoch": 1.9830666666666668,
        "step": 14873
    },
    {
        "loss": 0.8382,
        "grad_norm": 4.227154731750488,
        "learning_rate": 3.5012916188860157e-07,
        "epoch": 1.9832,
        "step": 14874
    },
    {
        "loss": 1.4796,
        "grad_norm": 7.211843490600586,
        "learning_rate": 3.4488687247942186e-07,
        "epoch": 1.9833333333333334,
        "step": 14875
    },
    {
        "loss": 1.9909,
        "grad_norm": 4.020299911499023,
        "learning_rate": 3.3968405680010476e-07,
        "epoch": 1.9834666666666667,
        "step": 14876
    },
    {
        "loss": 2.4189,
        "grad_norm": 3.4882118701934814,
        "learning_rate": 3.34520716911535e-07,
        "epoch": 1.9836,
        "step": 14877
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.4395480155944824,
        "learning_rate": 3.293968548588877e-07,
        "epoch": 1.9837333333333333,
        "step": 14878
    },
    {
        "loss": 2.7688,
        "grad_norm": 3.0008621215820312,
        "learning_rate": 3.24312472671795e-07,
        "epoch": 1.9838666666666667,
        "step": 14879
    },
    {
        "loss": 1.8768,
        "grad_norm": 2.9630730152130127,
        "learning_rate": 3.192675723641458e-07,
        "epoch": 1.984,
        "step": 14880
    },
    {
        "loss": 2.2612,
        "grad_norm": 3.3494720458984375,
        "learning_rate": 3.14262155934264e-07,
        "epoch": 1.9841333333333333,
        "step": 14881
    },
    {
        "loss": 1.9407,
        "grad_norm": 3.348299503326416,
        "learning_rate": 3.0929622536481906e-07,
        "epoch": 1.9842666666666666,
        "step": 14882
    },
    {
        "loss": 1.8809,
        "grad_norm": 6.022062301635742,
        "learning_rate": 3.0436978262283754e-07,
        "epoch": 1.9844,
        "step": 14883
    },
    {
        "loss": 1.241,
        "grad_norm": 5.447141170501709,
        "learning_rate": 2.9948282965968077e-07,
        "epoch": 1.9845333333333333,
        "step": 14884
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.47257137298584,
        "learning_rate": 2.946353684110892e-07,
        "epoch": 1.9846666666666666,
        "step": 14885
    },
    {
        "loss": 2.029,
        "grad_norm": 5.119100093841553,
        "learning_rate": 2.898274007971824e-07,
        "epoch": 1.9848,
        "step": 14886
    },
    {
        "loss": 2.3605,
        "grad_norm": 2.9489870071411133,
        "learning_rate": 2.850589287223815e-07,
        "epoch": 1.9849333333333332,
        "step": 14887
    },
    {
        "loss": 2.6959,
        "grad_norm": 4.151632308959961,
        "learning_rate": 2.8032995407552e-07,
        "epoch": 1.9850666666666665,
        "step": 14888
    },
    {
        "loss": 1.5993,
        "grad_norm": 2.7975785732269287,
        "learning_rate": 2.7564047872974396e-07,
        "epoch": 1.9851999999999999,
        "step": 14889
    },
    {
        "loss": 2.1879,
        "grad_norm": 4.101525783538818,
        "learning_rate": 2.709905045425676e-07,
        "epoch": 1.9853333333333332,
        "step": 14890
    },
    {
        "loss": 1.6374,
        "grad_norm": 4.581459999084473,
        "learning_rate": 2.663800333558841e-07,
        "epoch": 1.9854666666666667,
        "step": 14891
    },
    {
        "loss": 0.5857,
        "grad_norm": 2.592226505279541,
        "learning_rate": 2.6180906699589945e-07,
        "epoch": 1.9856,
        "step": 14892
    },
    {
        "loss": 0.9176,
        "grad_norm": 4.390681266784668,
        "learning_rate": 2.572776072731986e-07,
        "epoch": 1.9857333333333334,
        "step": 14893
    },
    {
        "loss": 2.3495,
        "grad_norm": 2.5910511016845703,
        "learning_rate": 2.5278565598269024e-07,
        "epoch": 1.9858666666666667,
        "step": 14894
    },
    {
        "loss": 2.4728,
        "grad_norm": 2.898115634918213,
        "learning_rate": 2.483332149036732e-07,
        "epoch": 1.986,
        "step": 14895
    },
    {
        "loss": 1.2381,
        "grad_norm": 2.639265775680542,
        "learning_rate": 2.439202857997702e-07,
        "epoch": 1.9861333333333333,
        "step": 14896
    },
    {
        "loss": 1.1154,
        "grad_norm": 4.079374313354492,
        "learning_rate": 2.3954687041893855e-07,
        "epoch": 1.9862666666666666,
        "step": 14897
    },
    {
        "loss": 2.5999,
        "grad_norm": 3.914161443710327,
        "learning_rate": 2.3521297049352576e-07,
        "epoch": 1.9864000000000002,
        "step": 14898
    },
    {
        "loss": 2.1345,
        "grad_norm": 4.03402042388916,
        "learning_rate": 2.3091858774018095e-07,
        "epoch": 1.9865333333333335,
        "step": 14899
    },
    {
        "loss": 2.397,
        "grad_norm": 4.5472025871276855,
        "learning_rate": 2.266637238599656e-07,
        "epoch": 1.9866666666666668,
        "step": 14900
    },
    {
        "loss": 2.2107,
        "grad_norm": 3.3362112045288086,
        "learning_rate": 2.2244838053819829e-07,
        "epoch": 1.9868000000000001,
        "step": 14901
    },
    {
        "loss": 1.9107,
        "grad_norm": 2.6717112064361572,
        "learning_rate": 2.182725594446211e-07,
        "epoch": 1.9869333333333334,
        "step": 14902
    },
    {
        "loss": 2.0304,
        "grad_norm": 4.157480716705322,
        "learning_rate": 2.1413626223326655e-07,
        "epoch": 1.9870666666666668,
        "step": 14903
    },
    {
        "loss": 3.0868,
        "grad_norm": 4.35211181640625,
        "learning_rate": 2.1003949054256843e-07,
        "epoch": 1.9872,
        "step": 14904
    },
    {
        "loss": 1.5839,
        "grad_norm": 3.655729293823242,
        "learning_rate": 2.0598224599523985e-07,
        "epoch": 1.9873333333333334,
        "step": 14905
    },
    {
        "loss": 2.2683,
        "grad_norm": 3.4788808822631836,
        "learning_rate": 2.0196453019839522e-07,
        "epoch": 1.9874666666666667,
        "step": 14906
    },
    {
        "loss": 2.4564,
        "grad_norm": 4.65833854675293,
        "learning_rate": 1.9798634474343935e-07,
        "epoch": 1.9876,
        "step": 14907
    },
    {
        "loss": 1.034,
        "grad_norm": 5.019842624664307,
        "learning_rate": 1.9404769120616727e-07,
        "epoch": 1.9877333333333334,
        "step": 14908
    },
    {
        "loss": 1.6245,
        "grad_norm": 5.494483947753906,
        "learning_rate": 1.901485711466977e-07,
        "epoch": 1.9878666666666667,
        "step": 14909
    },
    {
        "loss": 2.495,
        "grad_norm": 3.35379695892334,
        "learning_rate": 1.8628898610945078e-07,
        "epoch": 1.988,
        "step": 14910
    },
    {
        "loss": 2.5154,
        "grad_norm": 3.0091745853424072,
        "learning_rate": 1.8246893762325913e-07,
        "epoch": 1.9881333333333333,
        "step": 14911
    },
    {
        "loss": 2.732,
        "grad_norm": 2.6519808769226074,
        "learning_rate": 1.7868842720122348e-07,
        "epoch": 1.9882666666666666,
        "step": 14912
    },
    {
        "loss": 2.0539,
        "grad_norm": 6.3872270584106445,
        "learning_rate": 1.7494745634085707e-07,
        "epoch": 1.9884,
        "step": 14913
    },
    {
        "loss": 2.4615,
        "grad_norm": 2.490985631942749,
        "learning_rate": 1.712460265239413e-07,
        "epoch": 1.9885333333333333,
        "step": 14914
    },
    {
        "loss": 1.9107,
        "grad_norm": 4.229437351226807,
        "learning_rate": 1.6758413921662553e-07,
        "epoch": 1.9886666666666666,
        "step": 14915
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.83699893951416,
        "learning_rate": 1.6396179586940507e-07,
        "epoch": 1.9888,
        "step": 14916
    },
    {
        "loss": 2.823,
        "grad_norm": 3.953892230987549,
        "learning_rate": 1.6037899791709888e-07,
        "epoch": 1.9889333333333332,
        "step": 14917
    },
    {
        "loss": 2.0533,
        "grad_norm": 4.447792053222656,
        "learning_rate": 1.568357467788717e-07,
        "epoch": 1.9890666666666665,
        "step": 14918
    },
    {
        "loss": 1.426,
        "grad_norm": 5.637057304382324,
        "learning_rate": 1.53332043858212e-07,
        "epoch": 1.9891999999999999,
        "step": 14919
    },
    {
        "loss": 2.0497,
        "grad_norm": 2.7195513248443604,
        "learning_rate": 1.4986789054295402e-07,
        "epoch": 1.9893333333333332,
        "step": 14920
    },
    {
        "loss": 2.2186,
        "grad_norm": 3.1313769817352295,
        "learning_rate": 1.4644328820524465e-07,
        "epoch": 1.9894666666666667,
        "step": 14921
    },
    {
        "loss": 1.3849,
        "grad_norm": 4.447262763977051,
        "learning_rate": 1.430582382015877e-07,
        "epoch": 1.9896,
        "step": 14922
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.6108012199401855,
        "learning_rate": 1.397127418728328e-07,
        "epoch": 1.9897333333333334,
        "step": 14923
    },
    {
        "loss": 2.6461,
        "grad_norm": 2.3951475620269775,
        "learning_rate": 1.364068005441199e-07,
        "epoch": 1.9898666666666667,
        "step": 14924
    },
    {
        "loss": 2.0599,
        "grad_norm": 3.6065735816955566,
        "learning_rate": 1.3314041552494606e-07,
        "epoch": 1.99,
        "step": 14925
    },
    {
        "loss": 2.5703,
        "grad_norm": 5.200157165527344,
        "learning_rate": 1.299135881091429e-07,
        "epoch": 1.9901333333333333,
        "step": 14926
    },
    {
        "loss": 2.2055,
        "grad_norm": 3.714827299118042,
        "learning_rate": 1.2672631957486581e-07,
        "epoch": 1.9902666666666666,
        "step": 14927
    },
    {
        "loss": 1.3819,
        "grad_norm": 4.731049060821533,
        "learning_rate": 1.235786111846049e-07,
        "epoch": 1.9904,
        "step": 14928
    },
    {
        "loss": 1.952,
        "grad_norm": 3.081526756286621,
        "learning_rate": 1.2047046418518505e-07,
        "epoch": 1.9905333333333335,
        "step": 14929
    },
    {
        "loss": 4.2007,
        "grad_norm": 5.143649101257324,
        "learning_rate": 1.1740187980773254e-07,
        "epoch": 1.9906666666666668,
        "step": 14930
    },
    {
        "loss": 2.248,
        "grad_norm": 4.402554512023926,
        "learning_rate": 1.1437285926774177e-07,
        "epoch": 1.9908000000000001,
        "step": 14931
    },
    {
        "loss": 2.4632,
        "grad_norm": 3.5445175170898438,
        "learning_rate": 1.1138340376501966e-07,
        "epoch": 1.9909333333333334,
        "step": 14932
    },
    {
        "loss": 1.9953,
        "grad_norm": 2.5809566974639893,
        "learning_rate": 1.0843351448368566e-07,
        "epoch": 1.9910666666666668,
        "step": 14933
    },
    {
        "loss": 2.227,
        "grad_norm": 4.224668502807617,
        "learning_rate": 1.0552319259221621e-07,
        "epoch": 1.9912,
        "step": 14934
    },
    {
        "loss": 2.7467,
        "grad_norm": 3.7637412548065186,
        "learning_rate": 1.0265243924337809e-07,
        "epoch": 1.9913333333333334,
        "step": 14935
    },
    {
        "loss": 2.5232,
        "grad_norm": 4.748952388763428,
        "learning_rate": 9.98212555743061e-08,
        "epoch": 1.9914666666666667,
        "step": 14936
    },
    {
        "loss": 1.3091,
        "grad_norm": 4.251410961151123,
        "learning_rate": 9.702964270643656e-08,
        "epoch": 1.9916,
        "step": 14937
    },
    {
        "loss": 1.2207,
        "grad_norm": 5.140453815460205,
        "learning_rate": 9.427760174555156e-08,
        "epoch": 1.9917333333333334,
        "step": 14938
    },
    {
        "loss": 2.49,
        "grad_norm": 3.111530065536499,
        "learning_rate": 9.15651337817125e-08,
        "epoch": 1.9918666666666667,
        "step": 14939
    },
    {
        "loss": 2.2975,
        "grad_norm": 4.427798748016357,
        "learning_rate": 8.88922398893488e-08,
        "epoch": 1.992,
        "step": 14940
    },
    {
        "loss": 1.1435,
        "grad_norm": 5.024448871612549,
        "learning_rate": 8.625892112722466e-08,
        "epoch": 1.9921333333333333,
        "step": 14941
    },
    {
        "loss": 1.89,
        "grad_norm": 2.903364896774292,
        "learning_rate": 8.366517853838351e-08,
        "epoch": 1.9922666666666666,
        "step": 14942
    },
    {
        "loss": 1.7441,
        "grad_norm": 4.37040376663208,
        "learning_rate": 8.111101315022574e-08,
        "epoch": 1.9924,
        "step": 14943
    },
    {
        "loss": 1.239,
        "grad_norm": 3.2786314487457275,
        "learning_rate": 7.859642597444206e-08,
        "epoch": 1.9925333333333333,
        "step": 14944
    },
    {
        "loss": 2.0015,
        "grad_norm": 4.5965423583984375,
        "learning_rate": 7.612141800710238e-08,
        "epoch": 1.9926666666666666,
        "step": 14945
    },
    {
        "loss": 1.8339,
        "grad_norm": 3.7639997005462646,
        "learning_rate": 7.368599022855582e-08,
        "epoch": 1.9928,
        "step": 14946
    },
    {
        "loss": 2.6414,
        "grad_norm": 2.435865879058838,
        "learning_rate": 7.129014360347519e-08,
        "epoch": 1.9929333333333332,
        "step": 14947
    },
    {
        "loss": 1.594,
        "grad_norm": 4.685940742492676,
        "learning_rate": 6.893387908085692e-08,
        "epoch": 1.9930666666666665,
        "step": 14948
    },
    {
        "loss": 1.6046,
        "grad_norm": 3.906808614730835,
        "learning_rate": 6.661719759404328e-08,
        "epoch": 1.9931999999999999,
        "step": 14949
    },
    {
        "loss": 1.4024,
        "grad_norm": 3.9762632846832275,
        "learning_rate": 6.434010006067804e-08,
        "epoch": 1.9933333333333332,
        "step": 14950
    },
    {
        "loss": 2.668,
        "grad_norm": 2.7706174850463867,
        "learning_rate": 6.210258738271746e-08,
        "epoch": 1.9934666666666667,
        "step": 14951
    },
    {
        "loss": 2.3599,
        "grad_norm": 6.223598957061768,
        "learning_rate": 5.99046604464526e-08,
        "epoch": 1.9936,
        "step": 14952
    },
    {
        "loss": 2.4073,
        "grad_norm": 3.5546116828918457,
        "learning_rate": 5.774632012248704e-08,
        "epoch": 1.9937333333333334,
        "step": 14953
    },
    {
        "loss": 2.2486,
        "grad_norm": 3.180734395980835,
        "learning_rate": 5.562756726574803e-08,
        "epoch": 1.9938666666666667,
        "step": 14954
    },
    {
        "loss": 2.4323,
        "grad_norm": 2.8038861751556396,
        "learning_rate": 5.354840271548645e-08,
        "epoch": 1.994,
        "step": 14955
    },
    {
        "loss": 2.0284,
        "grad_norm": 3.107771635055542,
        "learning_rate": 5.1508827295265735e-08,
        "epoch": 1.9941333333333333,
        "step": 14956
    },
    {
        "loss": 3.1489,
        "grad_norm": 4.545033931732178,
        "learning_rate": 4.950884181295079e-08,
        "epoch": 1.9942666666666666,
        "step": 14957
    },
    {
        "loss": 2.4922,
        "grad_norm": 3.466904640197754,
        "learning_rate": 4.754844706076345e-08,
        "epoch": 1.9944,
        "step": 14958
    },
    {
        "loss": 1.505,
        "grad_norm": 2.8852524757385254,
        "learning_rate": 4.5627643815215894e-08,
        "epoch": 1.9945333333333335,
        "step": 14959
    },
    {
        "loss": 2.0237,
        "grad_norm": 3.91919207572937,
        "learning_rate": 4.374643283714397e-08,
        "epoch": 1.9946666666666668,
        "step": 14960
    },
    {
        "loss": 1.9933,
        "grad_norm": 4.167683124542236,
        "learning_rate": 4.1904814871707167e-08,
        "epoch": 1.9948000000000001,
        "step": 14961
    },
    {
        "loss": 1.4032,
        "grad_norm": 3.2531821727752686,
        "learning_rate": 4.0102790648366416e-08,
        "epoch": 1.9949333333333334,
        "step": 14962
    },
    {
        "loss": 1.8947,
        "grad_norm": 4.687318325042725,
        "learning_rate": 3.8340360880917415e-08,
        "epoch": 1.9950666666666668,
        "step": 14963
    },
    {
        "loss": 2.9266,
        "grad_norm": 3.8690497875213623,
        "learning_rate": 3.661752626745729e-08,
        "epoch": 1.9952,
        "step": 14964
    },
    {
        "loss": 1.1854,
        "grad_norm": 3.696848154067993,
        "learning_rate": 3.493428749041794e-08,
        "epoch": 1.9953333333333334,
        "step": 14965
    },
    {
        "loss": 2.4654,
        "grad_norm": 3.736311197280884,
        "learning_rate": 3.329064521653269e-08,
        "epoch": 1.9954666666666667,
        "step": 14966
    },
    {
        "loss": 1.701,
        "grad_norm": 3.634411573410034,
        "learning_rate": 3.168660009684743e-08,
        "epoch": 1.9956,
        "step": 14967
    },
    {
        "loss": 1.6513,
        "grad_norm": 5.324605941772461,
        "learning_rate": 3.0122152766731695e-08,
        "epoch": 1.9957333333333334,
        "step": 14968
    },
    {
        "loss": 2.2524,
        "grad_norm": 4.174402236938477,
        "learning_rate": 2.859730384586756e-08,
        "epoch": 1.9958666666666667,
        "step": 14969
    },
    {
        "loss": 2.0205,
        "grad_norm": 4.402285099029541,
        "learning_rate": 2.7112053938260773e-08,
        "epoch": 1.996,
        "step": 14970
    },
    {
        "loss": 1.8485,
        "grad_norm": 3.4734342098236084,
        "learning_rate": 2.5666403632218505e-08,
        "epoch": 1.9961333333333333,
        "step": 14971
    },
    {
        "loss": 2.0863,
        "grad_norm": 5.69310998916626,
        "learning_rate": 2.426035350037159e-08,
        "epoch": 1.9962666666666666,
        "step": 14972
    },
    {
        "loss": 1.5496,
        "grad_norm": 3.1264264583587646,
        "learning_rate": 2.2893904099652307e-08,
        "epoch": 1.9964,
        "step": 14973
    },
    {
        "loss": 2.0569,
        "grad_norm": 3.691153049468994,
        "learning_rate": 2.156705597132769e-08,
        "epoch": 1.9965333333333333,
        "step": 14974
    },
    {
        "loss": 2.5439,
        "grad_norm": 3.2330474853515625,
        "learning_rate": 2.0279809640944003e-08,
        "epoch": 1.9966666666666666,
        "step": 14975
    },
    {
        "loss": 2.6605,
        "grad_norm": 3.6035594940185547,
        "learning_rate": 1.9032165618415586e-08,
        "epoch": 1.9968,
        "step": 14976
    },
    {
        "loss": 1.8578,
        "grad_norm": 4.049517631530762,
        "learning_rate": 1.7824124397924913e-08,
        "epoch": 1.9969333333333332,
        "step": 14977
    },
    {
        "loss": 2.5454,
        "grad_norm": 3.258415937423706,
        "learning_rate": 1.6655686457967e-08,
        "epoch": 1.9970666666666665,
        "step": 14978
    },
    {
        "loss": 2.4501,
        "grad_norm": 3.473482847213745,
        "learning_rate": 1.552685226138273e-08,
        "epoch": 1.9971999999999999,
        "step": 14979
    },
    {
        "loss": 1.9477,
        "grad_norm": 3.0009851455688477,
        "learning_rate": 1.4437622255303317e-08,
        "epoch": 1.9973333333333332,
        "step": 14980
    },
    {
        "loss": 2.0661,
        "grad_norm": 3.39322566986084,
        "learning_rate": 1.3387996871172537e-08,
        "epoch": 1.9974666666666665,
        "step": 14981
    },
    {
        "loss": 2.5925,
        "grad_norm": 3.148549795150757,
        "learning_rate": 1.2377976524746704e-08,
        "epoch": 1.9976,
        "step": 14982
    },
    {
        "loss": 2.197,
        "grad_norm": 3.4772095680236816,
        "learning_rate": 1.140756161611689e-08,
        "epoch": 1.9977333333333334,
        "step": 14983
    },
    {
        "loss": 2.764,
        "grad_norm": 3.9148595333099365,
        "learning_rate": 1.0476752529642309e-08,
        "epoch": 1.9978666666666667,
        "step": 14984
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.2408833503723145,
        "learning_rate": 9.585549634039127e-09,
        "epoch": 1.998,
        "step": 14985
    },
    {
        "loss": 0.7017,
        "grad_norm": 3.4169983863830566,
        "learning_rate": 8.733953282302754e-09,
        "epoch": 1.9981333333333333,
        "step": 14986
    },
    {
        "loss": 2.1218,
        "grad_norm": 3.8991568088531494,
        "learning_rate": 7.921963811763355e-09,
        "epoch": 1.9982666666666666,
        "step": 14987
    },
    {
        "loss": 1.6925,
        "grad_norm": 3.876631021499634,
        "learning_rate": 7.149581544052542e-09,
        "epoch": 1.9984,
        "step": 14988
    },
    {
        "loss": 2.488,
        "grad_norm": 3.3412280082702637,
        "learning_rate": 6.416806785103369e-09,
        "epoch": 1.9985333333333335,
        "step": 14989
    },
    {
        "loss": 1.646,
        "grad_norm": 2.560603618621826,
        "learning_rate": 5.723639825172544e-09,
        "epoch": 1.9986666666666668,
        "step": 14990
    },
    {
        "loss": 1.9037,
        "grad_norm": 3.743283748626709,
        "learning_rate": 5.070080938840427e-09,
        "epoch": 1.9988000000000001,
        "step": 14991
    },
    {
        "loss": 2.1863,
        "grad_norm": 4.329615116119385,
        "learning_rate": 4.456130384966617e-09,
        "epoch": 1.9989333333333335,
        "step": 14992
    },
    {
        "loss": 2.0615,
        "grad_norm": 3.203542947769165,
        "learning_rate": 3.88178840675657e-09,
        "epoch": 1.9990666666666668,
        "step": 14993
    },
    {
        "loss": 1.6036,
        "grad_norm": 3.58029842376709,
        "learning_rate": 3.347055231683882e-09,
        "epoch": 1.9992,
        "step": 14994
    },
    {
        "loss": 2.2894,
        "grad_norm": 4.279943943023682,
        "learning_rate": 2.8519310715791057e-09,
        "epoch": 1.9993333333333334,
        "step": 14995
    },
    {
        "loss": 1.5627,
        "grad_norm": 5.440428733825684,
        "learning_rate": 2.3964161225631386e-09,
        "epoch": 1.9994666666666667,
        "step": 14996
    },
    {
        "loss": 2.4931,
        "grad_norm": 3.143320083618164,
        "learning_rate": 1.9805105650472222e-09,
        "epoch": 1.9996,
        "step": 14997
    },
    {
        "loss": 0.9428,
        "grad_norm": 3.243265151977539,
        "learning_rate": 1.6042145637995553e-09,
        "epoch": 1.9997333333333334,
        "step": 14998
    },
    {
        "loss": 2.7368,
        "grad_norm": 3.7303874492645264,
        "learning_rate": 1.2675282678453748e-09,
        "epoch": 1.9998666666666667,
        "step": 14999
    },
    {
        "loss": 2.3852,
        "grad_norm": 2.436339855194092,
        "learning_rate": 9.704518105668748e-10,
        "epoch": 2.0,
        "step": 15000
    },
    {
        "train_runtime": 8971.9608,
        "train_samples_per_second": 3.344,
        "train_steps_per_second": 1.672,
        "total_flos": 1.0737109232111616e+17,
        "train_loss": 2.1683513993700347,
        "epoch": 2.0,
        "step": 15000
    }
]