[
    {
        "loss": 7.6119,
        "grad_norm": 8.731169700622559,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00013333333333333334,
        "step": 1
    },
    {
        "loss": 8.5097,
        "grad_norm": 9.722492218017578,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.0002666666666666667,
        "step": 2
    },
    {
        "loss": 5.0749,
        "grad_norm": 4.4304070472717285,
        "learning_rate": 2.4e-05,
        "epoch": 0.0004,
        "step": 3
    },
    {
        "loss": 6.509,
        "grad_norm": 7.11168909072876,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0005333333333333334,
        "step": 4
    },
    {
        "loss": 5.0052,
        "grad_norm": 3.8872482776641846,
        "learning_rate": 4e-05,
        "epoch": 0.0006666666666666666,
        "step": 5
    },
    {
        "loss": 4.6313,
        "grad_norm": 3.829613208770752,
        "learning_rate": 4.8e-05,
        "epoch": 0.0008,
        "step": 6
    },
    {
        "loss": 4.7006,
        "grad_norm": 3.934425115585327,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0009333333333333333,
        "step": 7
    },
    {
        "loss": 4.5964,
        "grad_norm": 4.368907451629639,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0010666666666666667,
        "step": 8
    },
    {
        "loss": 6.8158,
        "grad_norm": Infinity,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0012,
        "step": 9
    },
    {
        "loss": 3.9369,
        "grad_norm": 2.3986589908599854,
        "learning_rate": 7.2e-05,
        "epoch": 0.0013333333333333333,
        "step": 10
    },
    {
        "loss": 4.1575,
        "grad_norm": 3.531886339187622,
        "learning_rate": 8e-05,
        "epoch": 0.0014666666666666667,
        "step": 11
    },
    {
        "loss": 8.0507,
        "grad_norm": Infinity,
        "learning_rate": 8e-05,
        "epoch": 0.0016,
        "step": 12
    },
    {
        "loss": 3.9034,
        "grad_norm": 3.3086259365081787,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0017333333333333333,
        "step": 13
    },
    {
        "loss": 5.8252,
        "grad_norm": 10.731626510620117,
        "learning_rate": 9.6e-05,
        "epoch": 0.0018666666666666666,
        "step": 14
    },
    {
        "loss": 4.86,
        "grad_norm": 7.850220203399658,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.002,
        "step": 15
    },
    {
        "loss": 4.1402,
        "grad_norm": 5.941108226776123,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0021333333333333334,
        "step": 16
    },
    {
        "loss": 3.6283,
        "grad_norm": 3.9684665203094482,
        "learning_rate": 0.00012,
        "epoch": 0.002266666666666667,
        "step": 17
    },
    {
        "loss": 4.1062,
        "grad_norm": 6.256809234619141,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0024,
        "step": 18
    },
    {
        "loss": 3.5715,
        "grad_norm": 3.1948976516723633,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.002533333333333333,
        "step": 19
    },
    {
        "loss": 3.1309,
        "grad_norm": 2.9256837368011475,
        "learning_rate": 0.000144,
        "epoch": 0.0026666666666666666,
        "step": 20
    },
    {
        "loss": 3.3576,
        "grad_norm": 3.842763900756836,
        "learning_rate": 0.000152,
        "epoch": 0.0028,
        "step": 21
    },
    {
        "loss": 4.6745,
        "grad_norm": 3.948453903198242,
        "learning_rate": 0.00016,
        "epoch": 0.0029333333333333334,
        "step": 22
    },
    {
        "loss": 3.5731,
        "grad_norm": 3.1097452640533447,
        "learning_rate": 0.000168,
        "epoch": 0.0030666666666666668,
        "step": 23
    },
    {
        "loss": 3.7414,
        "grad_norm": 5.019863128662109,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0032,
        "step": 24
    },
    {
        "loss": 3.5458,
        "grad_norm": 6.7096757888793945,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.0033333333333333335,
        "step": 25
    },
    {
        "loss": 3.1112,
        "grad_norm": 7.033694744110107,
        "learning_rate": 0.000192,
        "epoch": 0.0034666666666666665,
        "step": 26
    },
    {
        "loss": 2.793,
        "grad_norm": 5.445549011230469,
        "learning_rate": 0.0002,
        "epoch": 0.0036,
        "step": 27
    },
    {
        "loss": 3.3412,
        "grad_norm": 2.623781681060791,
        "learning_rate": 0.00019999998019482965,
        "epoch": 0.0037333333333333333,
        "step": 28
    },
    {
        "loss": 3.0865,
        "grad_norm": 3.9232006072998047,
        "learning_rate": 0.00019999992077932637,
        "epoch": 0.0038666666666666667,
        "step": 29
    },
    {
        "loss": 3.1361,
        "grad_norm": 10.359314918518066,
        "learning_rate": 0.00019999982175351373,
        "epoch": 0.004,
        "step": 30
    },
    {
        "loss": 3.1723,
        "grad_norm": 6.590633869171143,
        "learning_rate": 0.00019999968311743097,
        "epoch": 0.0041333333333333335,
        "step": 31
    },
    {
        "loss": 2.9924,
        "grad_norm": 2.7771990299224854,
        "learning_rate": 0.00019999950487113299,
        "epoch": 0.004266666666666667,
        "step": 32
    },
    {
        "loss": 3.0878,
        "grad_norm": 4.6902852058410645,
        "learning_rate": 0.00019999928701469038,
        "epoch": 0.0044,
        "step": 33
    },
    {
        "loss": 3.0057,
        "grad_norm": 6.926594257354736,
        "learning_rate": 0.00019999902954818946,
        "epoch": 0.004533333333333334,
        "step": 34
    },
    {
        "loss": 2.7704,
        "grad_norm": 8.580048561096191,
        "learning_rate": 0.00019999873247173214,
        "epoch": 0.004666666666666667,
        "step": 35
    },
    {
        "loss": 3.2422,
        "grad_norm": 6.5360493659973145,
        "learning_rate": 0.0001999983957854362,
        "epoch": 0.0048,
        "step": 36
    },
    {
        "loss": 3.1767,
        "grad_norm": 2.665585517883301,
        "learning_rate": 0.00019999801948943495,
        "epoch": 0.004933333333333333,
        "step": 37
    },
    {
        "loss": 3.0128,
        "grad_norm": 3.9273715019226074,
        "learning_rate": 0.00019999760358387745,
        "epoch": 0.005066666666666666,
        "step": 38
    },
    {
        "loss": 2.8847,
        "grad_norm": 3.0280561447143555,
        "learning_rate": 0.00019999714806892845,
        "epoch": 0.0052,
        "step": 39
    },
    {
        "loss": 3.1356,
        "grad_norm": 2.227372407913208,
        "learning_rate": 0.00019999665294476832,
        "epoch": 0.005333333333333333,
        "step": 40
    },
    {
        "loss": 2.4199,
        "grad_norm": 6.655343055725098,
        "learning_rate": 0.00019999611821159327,
        "epoch": 0.0054666666666666665,
        "step": 41
    },
    {
        "loss": 3.0015,
        "grad_norm": 2.6355137825012207,
        "learning_rate": 0.00019999554386961505,
        "epoch": 0.0056,
        "step": 42
    },
    {
        "loss": 3.0853,
        "grad_norm": 3.0957250595092773,
        "learning_rate": 0.00019999492991906116,
        "epoch": 0.005733333333333333,
        "step": 43
    },
    {
        "loss": 2.1757,
        "grad_norm": 1.6690129041671753,
        "learning_rate": 0.00019999427636017484,
        "epoch": 0.005866666666666667,
        "step": 44
    },
    {
        "loss": 2.9677,
        "grad_norm": 7.512644290924072,
        "learning_rate": 0.00019999358319321492,
        "epoch": 0.006,
        "step": 45
    },
    {
        "loss": 2.5013,
        "grad_norm": 1.9976882934570312,
        "learning_rate": 0.00019999285041845597,
        "epoch": 0.0061333333333333335,
        "step": 46
    },
    {
        "loss": 2.9269,
        "grad_norm": 3.5741686820983887,
        "learning_rate": 0.00019999207803618823,
        "epoch": 0.006266666666666667,
        "step": 47
    },
    {
        "loss": 3.5192,
        "grad_norm": 3.065322160720825,
        "learning_rate": 0.00019999126604671773,
        "epoch": 0.0064,
        "step": 48
    },
    {
        "loss": 2.3195,
        "grad_norm": 2.6976191997528076,
        "learning_rate": 0.00019999041445036597,
        "epoch": 0.006533333333333334,
        "step": 49
    },
    {
        "loss": 2.567,
        "grad_norm": 3.930345058441162,
        "learning_rate": 0.00019998952324747036,
        "epoch": 0.006666666666666667,
        "step": 50
    },
    {
        "loss": 3.0131,
        "grad_norm": 4.327544689178467,
        "learning_rate": 0.00019998859243838387,
        "epoch": 0.0068,
        "step": 51
    },
    {
        "loss": 1.8817,
        "grad_norm": 1.6701771020889282,
        "learning_rate": 0.00019998762202347528,
        "epoch": 0.006933333333333333,
        "step": 52
    },
    {
        "loss": 2.7406,
        "grad_norm": 2.963376045227051,
        "learning_rate": 0.00019998661200312884,
        "epoch": 0.007066666666666666,
        "step": 53
    },
    {
        "loss": 2.1449,
        "grad_norm": 4.0467634201049805,
        "learning_rate": 0.0001999855623777447,
        "epoch": 0.0072,
        "step": 54
    },
    {
        "loss": 2.1667,
        "grad_norm": 3.554274082183838,
        "learning_rate": 0.00019998447314773863,
        "epoch": 0.007333333333333333,
        "step": 55
    },
    {
        "loss": 2.4248,
        "grad_norm": 3.951615810394287,
        "learning_rate": 0.00019998334431354205,
        "epoch": 0.007466666666666667,
        "step": 56
    },
    {
        "loss": 1.5176,
        "grad_norm": 6.225492000579834,
        "learning_rate": 0.00019998217587560208,
        "epoch": 0.0076,
        "step": 57
    },
    {
        "loss": 2.6877,
        "grad_norm": 3.28908634185791,
        "learning_rate": 0.0001999809678343816,
        "epoch": 0.007733333333333333,
        "step": 58
    },
    {
        "loss": 2.9849,
        "grad_norm": 3.188932180404663,
        "learning_rate": 0.00019997972019035904,
        "epoch": 0.007866666666666666,
        "step": 59
    },
    {
        "loss": 2.2793,
        "grad_norm": 4.065738677978516,
        "learning_rate": 0.00019997843294402868,
        "epoch": 0.008,
        "step": 60
    },
    {
        "loss": 2.6713,
        "grad_norm": 2.176495313644409,
        "learning_rate": 0.00019997710609590038,
        "epoch": 0.008133333333333333,
        "step": 61
    },
    {
        "loss": 2.6813,
        "grad_norm": 3.7095303535461426,
        "learning_rate": 0.00019997573964649962,
        "epoch": 0.008266666666666667,
        "step": 62
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.797288179397583,
        "learning_rate": 0.0001999743335963678,
        "epoch": 0.0084,
        "step": 63
    },
    {
        "loss": 3.2834,
        "grad_norm": 2.413926124572754,
        "learning_rate": 0.00019997288794606176,
        "epoch": 0.008533333333333334,
        "step": 64
    },
    {
        "loss": 1.9358,
        "grad_norm": 5.6341657638549805,
        "learning_rate": 0.00019997140269615416,
        "epoch": 0.008666666666666666,
        "step": 65
    },
    {
        "loss": 2.5892,
        "grad_norm": 3.3001155853271484,
        "learning_rate": 0.00019996987784723328,
        "epoch": 0.0088,
        "step": 66
    },
    {
        "loss": 2.9311,
        "grad_norm": 3.0037498474121094,
        "learning_rate": 0.00019996831339990316,
        "epoch": 0.008933333333333333,
        "step": 67
    },
    {
        "loss": 2.7438,
        "grad_norm": 4.493782043457031,
        "learning_rate": 0.00019996670935478348,
        "epoch": 0.009066666666666667,
        "step": 68
    },
    {
        "loss": 2.145,
        "grad_norm": 3.4331064224243164,
        "learning_rate": 0.0001999650657125096,
        "epoch": 0.0092,
        "step": 69
    },
    {
        "loss": 1.7403,
        "grad_norm": 6.027341365814209,
        "learning_rate": 0.00019996338247373253,
        "epoch": 0.009333333333333334,
        "step": 70
    },
    {
        "loss": 2.6705,
        "grad_norm": 6.518877029418945,
        "learning_rate": 0.0001999616596391191,
        "epoch": 0.009466666666666667,
        "step": 71
    },
    {
        "loss": 2.0535,
        "grad_norm": 3.9579930305480957,
        "learning_rate": 0.00019995989720935164,
        "epoch": 0.0096,
        "step": 72
    },
    {
        "loss": 2.8531,
        "grad_norm": 2.3580477237701416,
        "learning_rate": 0.0001999580951851283,
        "epoch": 0.009733333333333333,
        "step": 73
    },
    {
        "loss": 2.6986,
        "grad_norm": 3.081936836242676,
        "learning_rate": 0.00019995625356716287,
        "epoch": 0.009866666666666666,
        "step": 74
    },
    {
        "loss": 2.0063,
        "grad_norm": 3.7245209217071533,
        "learning_rate": 0.0001999543723561848,
        "epoch": 0.01,
        "step": 75
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.8771965503692627,
        "learning_rate": 0.00019995245155293926,
        "epoch": 0.010133333333333333,
        "step": 76
    },
    {
        "loss": 2.1398,
        "grad_norm": 3.585397958755493,
        "learning_rate": 0.00019995049115818705,
        "epoch": 0.010266666666666667,
        "step": 77
    },
    {
        "loss": 2.6684,
        "grad_norm": 2.7162561416625977,
        "learning_rate": 0.00019994849117270475,
        "epoch": 0.0104,
        "step": 78
    },
    {
        "loss": 2.7344,
        "grad_norm": 2.403534173965454,
        "learning_rate": 0.00019994645159728452,
        "epoch": 0.010533333333333334,
        "step": 79
    },
    {
        "loss": 2.8361,
        "grad_norm": 2.9005837440490723,
        "learning_rate": 0.00019994437243273425,
        "epoch": 0.010666666666666666,
        "step": 80
    },
    {
        "loss": 2.4391,
        "grad_norm": 5.237562656402588,
        "learning_rate": 0.00019994225367987752,
        "epoch": 0.0108,
        "step": 81
    },
    {
        "loss": 1.2637,
        "grad_norm": 4.640646457672119,
        "learning_rate": 0.00019994009533955357,
        "epoch": 0.010933333333333333,
        "step": 82
    },
    {
        "loss": 2.8297,
        "grad_norm": 2.8979432582855225,
        "learning_rate": 0.00019993789741261727,
        "epoch": 0.011066666666666667,
        "step": 83
    },
    {
        "loss": 2.4783,
        "grad_norm": 3.1117281913757324,
        "learning_rate": 0.00019993565989993934,
        "epoch": 0.0112,
        "step": 84
    },
    {
        "loss": 2.2969,
        "grad_norm": 3.283992052078247,
        "learning_rate": 0.00019993338280240597,
        "epoch": 0.011333333333333334,
        "step": 85
    },
    {
        "loss": 3.0184,
        "grad_norm": 3.4825453758239746,
        "learning_rate": 0.00019993106612091913,
        "epoch": 0.011466666666666667,
        "step": 86
    },
    {
        "loss": 2.4861,
        "grad_norm": 4.090800762176514,
        "learning_rate": 0.00019992870985639652,
        "epoch": 0.0116,
        "step": 87
    },
    {
        "loss": 2.3403,
        "grad_norm": 4.587352275848389,
        "learning_rate": 0.00019992631400977146,
        "epoch": 0.011733333333333333,
        "step": 88
    },
    {
        "loss": 2.456,
        "grad_norm": 4.3615288734436035,
        "learning_rate": 0.0001999238785819929,
        "epoch": 0.011866666666666666,
        "step": 89
    },
    {
        "loss": 1.7443,
        "grad_norm": 3.31726336479187,
        "learning_rate": 0.00019992140357402556,
        "epoch": 0.012,
        "step": 90
    },
    {
        "loss": 2.6339,
        "grad_norm": 4.267632961273193,
        "learning_rate": 0.00019991888898684978,
        "epoch": 0.012133333333333333,
        "step": 91
    },
    {
        "loss": 0.8754,
        "grad_norm": 3.4488697052001953,
        "learning_rate": 0.00019991633482146161,
        "epoch": 0.012266666666666667,
        "step": 92
    },
    {
        "loss": 1.6223,
        "grad_norm": 4.622090816497803,
        "learning_rate": 0.00019991374107887277,
        "epoch": 0.0124,
        "step": 93
    },
    {
        "loss": 0.6573,
        "grad_norm": 2.6101672649383545,
        "learning_rate": 0.00019991110776011067,
        "epoch": 0.012533333333333334,
        "step": 94
    },
    {
        "loss": 2.844,
        "grad_norm": 3.6260836124420166,
        "learning_rate": 0.0001999084348662183,
        "epoch": 0.012666666666666666,
        "step": 95
    },
    {
        "loss": 2.879,
        "grad_norm": 2.279507875442505,
        "learning_rate": 0.00019990572239825447,
        "epoch": 0.0128,
        "step": 96
    },
    {
        "loss": 2.325,
        "grad_norm": 6.557432174682617,
        "learning_rate": 0.00019990297035729358,
        "epoch": 0.012933333333333333,
        "step": 97
    },
    {
        "loss": 1.3072,
        "grad_norm": 4.285982131958008,
        "learning_rate": 0.0001999001787444257,
        "epoch": 0.013066666666666667,
        "step": 98
    },
    {
        "loss": 2.9268,
        "grad_norm": 2.236515760421753,
        "learning_rate": 0.0001998973475607566,
        "epoch": 0.0132,
        "step": 99
    },
    {
        "loss": 2.6051,
        "grad_norm": 3.0058326721191406,
        "learning_rate": 0.00019989447680740781,
        "epoch": 0.013333333333333334,
        "step": 100
    },
    {
        "loss": 3.0217,
        "grad_norm": 2.8382256031036377,
        "learning_rate": 0.00019989156648551632,
        "epoch": 0.013466666666666667,
        "step": 101
    },
    {
        "loss": 2.5274,
        "grad_norm": 3.0376055240631104,
        "learning_rate": 0.000199888616596235,
        "epoch": 0.0136,
        "step": 102
    },
    {
        "loss": 3.5549,
        "grad_norm": 2.679272413253784,
        "learning_rate": 0.00019988562714073226,
        "epoch": 0.013733333333333334,
        "step": 103
    },
    {
        "loss": 2.8744,
        "grad_norm": 2.1689844131469727,
        "learning_rate": 0.00019988259812019229,
        "epoch": 0.013866666666666666,
        "step": 104
    },
    {
        "loss": 2.9677,
        "grad_norm": 2.8721067905426025,
        "learning_rate": 0.0001998795295358148,
        "epoch": 0.014,
        "step": 105
    },
    {
        "loss": 2.2348,
        "grad_norm": 3.048251152038574,
        "learning_rate": 0.0001998764213888154,
        "epoch": 0.014133333333333333,
        "step": 106
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.3352315425872803,
        "learning_rate": 0.00019987327368042515,
        "epoch": 0.014266666666666667,
        "step": 107
    },
    {
        "loss": 2.5268,
        "grad_norm": 2.3958871364593506,
        "learning_rate": 0.00019987008641189088,
        "epoch": 0.0144,
        "step": 108
    },
    {
        "loss": 2.6099,
        "grad_norm": 5.918692111968994,
        "learning_rate": 0.00019986685958447506,
        "epoch": 0.014533333333333334,
        "step": 109
    },
    {
        "loss": 2.1298,
        "grad_norm": 2.7433292865753174,
        "learning_rate": 0.00019986359319945589,
        "epoch": 0.014666666666666666,
        "step": 110
    },
    {
        "loss": 1.9195,
        "grad_norm": 2.655024766921997,
        "learning_rate": 0.00019986028725812717,
        "epoch": 0.0148,
        "step": 111
    },
    {
        "loss": 3.1154,
        "grad_norm": 2.4831607341766357,
        "learning_rate": 0.00019985694176179842,
        "epoch": 0.014933333333333333,
        "step": 112
    },
    {
        "loss": 1.9332,
        "grad_norm": 5.346397876739502,
        "learning_rate": 0.00019985355671179477,
        "epoch": 0.015066666666666667,
        "step": 113
    },
    {
        "loss": 2.4602,
        "grad_norm": 3.1185638904571533,
        "learning_rate": 0.00019985013210945706,
        "epoch": 0.0152,
        "step": 114
    },
    {
        "loss": 3.1114,
        "grad_norm": 4.006623268127441,
        "learning_rate": 0.0001998466679561418,
        "epoch": 0.015333333333333332,
        "step": 115
    },
    {
        "loss": 3.0278,
        "grad_norm": 3.2959704399108887,
        "learning_rate": 0.00019984316425322114,
        "epoch": 0.015466666666666667,
        "step": 116
    },
    {
        "loss": 2.0422,
        "grad_norm": 5.918388843536377,
        "learning_rate": 0.00019983962100208292,
        "epoch": 0.0156,
        "step": 117
    },
    {
        "loss": 3.1153,
        "grad_norm": 4.773737907409668,
        "learning_rate": 0.00019983603820413062,
        "epoch": 0.015733333333333332,
        "step": 118
    },
    {
        "loss": 1.7764,
        "grad_norm": 4.469233989715576,
        "learning_rate": 0.00019983241586078338,
        "epoch": 0.015866666666666668,
        "step": 119
    },
    {
        "loss": 3.1708,
        "grad_norm": 2.3045032024383545,
        "learning_rate": 0.00019982875397347606,
        "epoch": 0.016,
        "step": 120
    },
    {
        "loss": 2.5898,
        "grad_norm": 3.8268744945526123,
        "learning_rate": 0.00019982505254365914,
        "epoch": 0.016133333333333333,
        "step": 121
    },
    {
        "loss": 2.1933,
        "grad_norm": 4.386584758758545,
        "learning_rate": 0.00019982131157279877,
        "epoch": 0.016266666666666665,
        "step": 122
    },
    {
        "loss": 1.8753,
        "grad_norm": 4.6653218269348145,
        "learning_rate": 0.00019981753106237675,
        "epoch": 0.0164,
        "step": 123
    },
    {
        "loss": 1.7548,
        "grad_norm": 4.558473110198975,
        "learning_rate": 0.00019981371101389055,
        "epoch": 0.016533333333333334,
        "step": 124
    },
    {
        "loss": 2.5547,
        "grad_norm": 3.6199276447296143,
        "learning_rate": 0.00019980985142885332,
        "epoch": 0.016666666666666666,
        "step": 125
    },
    {
        "loss": 2.6629,
        "grad_norm": 3.3512470722198486,
        "learning_rate": 0.00019980595230879384,
        "epoch": 0.0168,
        "step": 126
    },
    {
        "loss": 2.0538,
        "grad_norm": 3.371354341506958,
        "learning_rate": 0.00019980201365525657,
        "epoch": 0.016933333333333335,
        "step": 127
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.481428384780884,
        "learning_rate": 0.0001997980354698016,
        "epoch": 0.017066666666666667,
        "step": 128
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.8033394813537598,
        "learning_rate": 0.00019979401775400477,
        "epoch": 0.0172,
        "step": 129
    },
    {
        "loss": 2.1968,
        "grad_norm": 3.412588357925415,
        "learning_rate": 0.00019978996050945745,
        "epoch": 0.017333333333333333,
        "step": 130
    },
    {
        "loss": 2.3025,
        "grad_norm": 2.9142441749572754,
        "learning_rate": 0.00019978586373776676,
        "epoch": 0.017466666666666665,
        "step": 131
    },
    {
        "loss": 2.8925,
        "grad_norm": 2.7271828651428223,
        "learning_rate": 0.0001997817274405554,
        "epoch": 0.0176,
        "step": 132
    },
    {
        "loss": 2.3428,
        "grad_norm": 3.9278547763824463,
        "learning_rate": 0.0001997775516194618,
        "epoch": 0.017733333333333334,
        "step": 133
    },
    {
        "loss": 2.8289,
        "grad_norm": 2.077514410018921,
        "learning_rate": 0.00019977333627614006,
        "epoch": 0.017866666666666666,
        "step": 134
    },
    {
        "loss": 2.9886,
        "grad_norm": 2.0180089473724365,
        "learning_rate": 0.00019976908141225983,
        "epoch": 0.018,
        "step": 135
    },
    {
        "loss": 2.605,
        "grad_norm": 3.1958799362182617,
        "learning_rate": 0.00019976478702950646,
        "epoch": 0.018133333333333335,
        "step": 136
    },
    {
        "loss": 1.1229,
        "grad_norm": 6.130755424499512,
        "learning_rate": 0.00019976045312958108,
        "epoch": 0.018266666666666667,
        "step": 137
    },
    {
        "loss": 2.1413,
        "grad_norm": 3.0134849548339844,
        "learning_rate": 0.00019975607971420026,
        "epoch": 0.0184,
        "step": 138
    },
    {
        "loss": 2.4041,
        "grad_norm": 3.724764585494995,
        "learning_rate": 0.00019975166678509634,
        "epoch": 0.018533333333333332,
        "step": 139
    },
    {
        "loss": 3.0622,
        "grad_norm": 2.46858811378479,
        "learning_rate": 0.00019974721434401732,
        "epoch": 0.018666666666666668,
        "step": 140
    },
    {
        "loss": 2.7734,
        "grad_norm": 2.21020245552063,
        "learning_rate": 0.0001997427223927268,
        "epoch": 0.0188,
        "step": 141
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.5945069789886475,
        "learning_rate": 0.0001997381909330041,
        "epoch": 0.018933333333333333,
        "step": 142
    },
    {
        "loss": 2.9429,
        "grad_norm": 2.631063461303711,
        "learning_rate": 0.00019973361996664414,
        "epoch": 0.019066666666666666,
        "step": 143
    },
    {
        "loss": 2.2283,
        "grad_norm": 3.8563284873962402,
        "learning_rate": 0.00019972900949545745,
        "epoch": 0.0192,
        "step": 144
    },
    {
        "loss": 2.8749,
        "grad_norm": 3.3264498710632324,
        "learning_rate": 0.00019972435952127026,
        "epoch": 0.019333333333333334,
        "step": 145
    },
    {
        "loss": 2.6678,
        "grad_norm": 2.529995918273926,
        "learning_rate": 0.0001997196700459245,
        "epoch": 0.019466666666666667,
        "step": 146
    },
    {
        "loss": 2.3646,
        "grad_norm": 3.4271976947784424,
        "learning_rate": 0.0001997149410712776,
        "epoch": 0.0196,
        "step": 147
    },
    {
        "loss": 2.3084,
        "grad_norm": 4.664200782775879,
        "learning_rate": 0.00019971017259920283,
        "epoch": 0.019733333333333332,
        "step": 148
    },
    {
        "loss": 2.1544,
        "grad_norm": 2.334320068359375,
        "learning_rate": 0.00019970536463158892,
        "epoch": 0.019866666666666668,
        "step": 149
    },
    {
        "loss": 2.5892,
        "grad_norm": 2.662583351135254,
        "learning_rate": 0.00019970051717034033,
        "epoch": 0.02,
        "step": 150
    },
    {
        "loss": 2.0273,
        "grad_norm": 3.8822073936462402,
        "learning_rate": 0.00019969563021737718,
        "epoch": 0.020133333333333333,
        "step": 151
    },
    {
        "loss": 2.6714,
        "grad_norm": 2.3360517024993896,
        "learning_rate": 0.00019969070377463517,
        "epoch": 0.020266666666666665,
        "step": 152
    },
    {
        "loss": 2.4922,
        "grad_norm": 2.7722325325012207,
        "learning_rate": 0.00019968573784406575,
        "epoch": 0.0204,
        "step": 153
    },
    {
        "loss": 2.8147,
        "grad_norm": 2.8035218715667725,
        "learning_rate": 0.0001996807324276359,
        "epoch": 0.020533333333333334,
        "step": 154
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.6785805225372314,
        "learning_rate": 0.00019967568752732824,
        "epoch": 0.020666666666666667,
        "step": 155
    },
    {
        "loss": 2.2949,
        "grad_norm": 3.4129443168640137,
        "learning_rate": 0.0001996706031451411,
        "epoch": 0.0208,
        "step": 156
    },
    {
        "loss": 2.6327,
        "grad_norm": 2.355214834213257,
        "learning_rate": 0.0001996654792830885,
        "epoch": 0.020933333333333335,
        "step": 157
    },
    {
        "loss": 2.3693,
        "grad_norm": 3.671967029571533,
        "learning_rate": 0.00019966031594319993,
        "epoch": 0.021066666666666668,
        "step": 158
    },
    {
        "loss": 2.3797,
        "grad_norm": 2.8269922733306885,
        "learning_rate": 0.0001996551131275206,
        "epoch": 0.0212,
        "step": 159
    },
    {
        "loss": 2.6124,
        "grad_norm": 3.7443623542785645,
        "learning_rate": 0.00019964987083811143,
        "epoch": 0.021333333333333333,
        "step": 160
    },
    {
        "loss": 3.1344,
        "grad_norm": 2.9313855171203613,
        "learning_rate": 0.00019964458907704885,
        "epoch": 0.021466666666666665,
        "step": 161
    },
    {
        "loss": 2.4671,
        "grad_norm": 4.269542694091797,
        "learning_rate": 0.000199639267846425,
        "epoch": 0.0216,
        "step": 162
    },
    {
        "loss": 3.0788,
        "grad_norm": 2.6922943592071533,
        "learning_rate": 0.00019963390714834766,
        "epoch": 0.021733333333333334,
        "step": 163
    },
    {
        "loss": 2.4426,
        "grad_norm": 3.1393179893493652,
        "learning_rate": 0.0001996285069849402,
        "epoch": 0.021866666666666666,
        "step": 164
    },
    {
        "loss": 2.5894,
        "grad_norm": 1.5709391832351685,
        "learning_rate": 0.00019962306735834165,
        "epoch": 0.022,
        "step": 165
    },
    {
        "loss": 2.2756,
        "grad_norm": 2.352339267730713,
        "learning_rate": 0.00019961758827070667,
        "epoch": 0.022133333333333335,
        "step": 166
    },
    {
        "loss": 3.0221,
        "grad_norm": 3.1679675579071045,
        "learning_rate": 0.00019961206972420552,
        "epoch": 0.022266666666666667,
        "step": 167
    },
    {
        "loss": 2.0211,
        "grad_norm": 2.269120693206787,
        "learning_rate": 0.00019960651172102413,
        "epoch": 0.0224,
        "step": 168
    },
    {
        "loss": 1.8041,
        "grad_norm": 1.9484304189682007,
        "learning_rate": 0.00019960091426336404,
        "epoch": 0.022533333333333332,
        "step": 169
    },
    {
        "loss": 1.3218,
        "grad_norm": 3.7244954109191895,
        "learning_rate": 0.00019959527735344246,
        "epoch": 0.02266666666666667,
        "step": 170
    },
    {
        "loss": 2.5803,
        "grad_norm": 1.937067985534668,
        "learning_rate": 0.00019958960099349216,
        "epoch": 0.0228,
        "step": 171
    },
    {
        "loss": 2.4237,
        "grad_norm": 3.2841475009918213,
        "learning_rate": 0.00019958388518576153,
        "epoch": 0.022933333333333333,
        "step": 172
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.539823055267334,
        "learning_rate": 0.00019957812993251468,
        "epoch": 0.023066666666666666,
        "step": 173
    },
    {
        "loss": 2.4437,
        "grad_norm": 3.2178187370300293,
        "learning_rate": 0.00019957233523603126,
        "epoch": 0.0232,
        "step": 174
    },
    {
        "loss": 1.2558,
        "grad_norm": 4.812521934509277,
        "learning_rate": 0.00019956650109860652,
        "epoch": 0.023333333333333334,
        "step": 175
    },
    {
        "loss": 2.274,
        "grad_norm": 3.8340883255004883,
        "learning_rate": 0.0001995606275225515,
        "epoch": 0.023466666666666667,
        "step": 176
    },
    {
        "loss": 2.4623,
        "grad_norm": 2.8129608631134033,
        "learning_rate": 0.00019955471451019264,
        "epoch": 0.0236,
        "step": 177
    },
    {
        "loss": 2.1182,
        "grad_norm": 2.3199524879455566,
        "learning_rate": 0.00019954876206387215,
        "epoch": 0.023733333333333332,
        "step": 178
    },
    {
        "loss": 1.7595,
        "grad_norm": 4.790377140045166,
        "learning_rate": 0.00019954277018594777,
        "epoch": 0.023866666666666668,
        "step": 179
    },
    {
        "loss": 1.6558,
        "grad_norm": 3.533679246902466,
        "learning_rate": 0.000199536738878793,
        "epoch": 0.024,
        "step": 180
    },
    {
        "loss": 2.9225,
        "grad_norm": 2.829911231994629,
        "learning_rate": 0.00019953066814479674,
        "epoch": 0.024133333333333333,
        "step": 181
    },
    {
        "loss": 3.3195,
        "grad_norm": 4.498991012573242,
        "learning_rate": 0.00019952455798636368,
        "epoch": 0.024266666666666666,
        "step": 182
    },
    {
        "loss": 2.4409,
        "grad_norm": 2.540734052658081,
        "learning_rate": 0.0001995184084059141,
        "epoch": 0.0244,
        "step": 183
    },
    {
        "loss": 3.058,
        "grad_norm": 1.9804275035858154,
        "learning_rate": 0.00019951221940588387,
        "epoch": 0.024533333333333334,
        "step": 184
    },
    {
        "loss": 2.3208,
        "grad_norm": 2.4842278957366943,
        "learning_rate": 0.00019950599098872443,
        "epoch": 0.024666666666666667,
        "step": 185
    },
    {
        "loss": 3.1166,
        "grad_norm": 3.866223096847534,
        "learning_rate": 0.0001994997231569029,
        "epoch": 0.0248,
        "step": 186
    },
    {
        "loss": 2.9198,
        "grad_norm": 3.207864761352539,
        "learning_rate": 0.00019949341591290201,
        "epoch": 0.02493333333333333,
        "step": 187
    },
    {
        "loss": 2.9622,
        "grad_norm": 3.9371964931488037,
        "learning_rate": 0.00019948706925922003,
        "epoch": 0.025066666666666668,
        "step": 188
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.7197608947753906,
        "learning_rate": 0.00019948068319837093,
        "epoch": 0.0252,
        "step": 189
    },
    {
        "loss": 2.5876,
        "grad_norm": 2.371161937713623,
        "learning_rate": 0.00019947425773288426,
        "epoch": 0.025333333333333333,
        "step": 190
    },
    {
        "loss": 3.0402,
        "grad_norm": 2.3379499912261963,
        "learning_rate": 0.00019946779286530514,
        "epoch": 0.025466666666666665,
        "step": 191
    },
    {
        "loss": 2.8691,
        "grad_norm": 3.71113920211792,
        "learning_rate": 0.00019946128859819434,
        "epoch": 0.0256,
        "step": 192
    },
    {
        "loss": 2.1256,
        "grad_norm": 3.2894058227539062,
        "learning_rate": 0.00019945474493412822,
        "epoch": 0.025733333333333334,
        "step": 193
    },
    {
        "loss": 1.702,
        "grad_norm": 3.211790084838867,
        "learning_rate": 0.00019944816187569876,
        "epoch": 0.025866666666666666,
        "step": 194
    },
    {
        "loss": 2.1061,
        "grad_norm": 3.6875932216644287,
        "learning_rate": 0.00019944153942551352,
        "epoch": 0.026,
        "step": 195
    },
    {
        "loss": 3.0442,
        "grad_norm": 2.6803369522094727,
        "learning_rate": 0.00019943487758619565,
        "epoch": 0.026133333333333335,
        "step": 196
    },
    {
        "loss": 1.6194,
        "grad_norm": 3.726046562194824,
        "learning_rate": 0.00019942817636038398,
        "epoch": 0.026266666666666667,
        "step": 197
    },
    {
        "loss": 2.0124,
        "grad_norm": 2.4952118396759033,
        "learning_rate": 0.00019942143575073286,
        "epoch": 0.0264,
        "step": 198
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.9594149589538574,
        "learning_rate": 0.00019941465575991224,
        "epoch": 0.026533333333333332,
        "step": 199
    },
    {
        "loss": 2.7809,
        "grad_norm": 2.2488532066345215,
        "learning_rate": 0.00019940783639060773,
        "epoch": 0.02666666666666667,
        "step": 200
    },
    {
        "loss": 2.0895,
        "grad_norm": 3.41565203666687,
        "learning_rate": 0.00019940097764552052,
        "epoch": 0.0268,
        "step": 201
    },
    {
        "loss": 2.1061,
        "grad_norm": 3.751131296157837,
        "learning_rate": 0.00019939407952736737,
        "epoch": 0.026933333333333333,
        "step": 202
    },
    {
        "loss": 1.8425,
        "grad_norm": 3.8283371925354004,
        "learning_rate": 0.00019938714203888064,
        "epoch": 0.027066666666666666,
        "step": 203
    },
    {
        "loss": 2.6254,
        "grad_norm": 3.1658596992492676,
        "learning_rate": 0.0001993801651828083,
        "epoch": 0.0272,
        "step": 204
    },
    {
        "loss": 2.4498,
        "grad_norm": 2.7791950702667236,
        "learning_rate": 0.00019937314896191385,
        "epoch": 0.027333333333333334,
        "step": 205
    },
    {
        "loss": 2.3257,
        "grad_norm": 2.8458516597747803,
        "learning_rate": 0.00019936609337897654,
        "epoch": 0.027466666666666667,
        "step": 206
    },
    {
        "loss": 2.5253,
        "grad_norm": 3.2098121643066406,
        "learning_rate": 0.00019935899843679105,
        "epoch": 0.0276,
        "step": 207
    },
    {
        "loss": 2.6141,
        "grad_norm": 3.634317398071289,
        "learning_rate": 0.00019935186413816774,
        "epoch": 0.027733333333333332,
        "step": 208
    },
    {
        "loss": 2.8373,
        "grad_norm": 3.2770180702209473,
        "learning_rate": 0.0001993446904859325,
        "epoch": 0.027866666666666668,
        "step": 209
    },
    {
        "loss": 1.1817,
        "grad_norm": 4.447906494140625,
        "learning_rate": 0.00019933747748292682,
        "epoch": 0.028,
        "step": 210
    },
    {
        "loss": 2.8773,
        "grad_norm": 2.6206130981445312,
        "learning_rate": 0.00019933022513200785,
        "epoch": 0.028133333333333333,
        "step": 211
    },
    {
        "loss": 2.4219,
        "grad_norm": 2.9964561462402344,
        "learning_rate": 0.00019932293343604825,
        "epoch": 0.028266666666666666,
        "step": 212
    },
    {
        "loss": 3.0418,
        "grad_norm": 2.9023211002349854,
        "learning_rate": 0.0001993156023979363,
        "epoch": 0.0284,
        "step": 213
    },
    {
        "loss": 1.9926,
        "grad_norm": 2.9210433959960938,
        "learning_rate": 0.00019930823202057578,
        "epoch": 0.028533333333333334,
        "step": 214
    },
    {
        "loss": 3.0656,
        "grad_norm": 2.4477996826171875,
        "learning_rate": 0.00019930082230688618,
        "epoch": 0.028666666666666667,
        "step": 215
    },
    {
        "loss": 1.3017,
        "grad_norm": 3.333749532699585,
        "learning_rate": 0.00019929337325980253,
        "epoch": 0.0288,
        "step": 216
    },
    {
        "loss": 2.6081,
        "grad_norm": 2.270571231842041,
        "learning_rate": 0.0001992858848822754,
        "epoch": 0.028933333333333332,
        "step": 217
    },
    {
        "loss": 2.7322,
        "grad_norm": 1.983815312385559,
        "learning_rate": 0.00019927835717727094,
        "epoch": 0.029066666666666668,
        "step": 218
    },
    {
        "loss": 2.6422,
        "grad_norm": 3.1078708171844482,
        "learning_rate": 0.00019927079014777092,
        "epoch": 0.0292,
        "step": 219
    },
    {
        "loss": 2.0264,
        "grad_norm": 3.7051477432250977,
        "learning_rate": 0.0001992631837967727,
        "epoch": 0.029333333333333333,
        "step": 220
    },
    {
        "loss": 1.9765,
        "grad_norm": 3.5677549839019775,
        "learning_rate": 0.0001992555381272891,
        "epoch": 0.029466666666666665,
        "step": 221
    },
    {
        "loss": 2.7527,
        "grad_norm": 2.9468650817871094,
        "learning_rate": 0.00019924785314234868,
        "epoch": 0.0296,
        "step": 222
    },
    {
        "loss": 1.8218,
        "grad_norm": 3.1961565017700195,
        "learning_rate": 0.00019924012884499543,
        "epoch": 0.029733333333333334,
        "step": 223
    },
    {
        "loss": 2.9733,
        "grad_norm": 2.4226186275482178,
        "learning_rate": 0.000199232365238289,
        "epoch": 0.029866666666666666,
        "step": 224
    },
    {
        "loss": 1.9627,
        "grad_norm": 2.9433085918426514,
        "learning_rate": 0.00019922456232530458,
        "epoch": 0.03,
        "step": 225
    },
    {
        "loss": 2.1405,
        "grad_norm": 3.575265407562256,
        "learning_rate": 0.0001992167201091329,
        "epoch": 0.030133333333333335,
        "step": 226
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.8588898181915283,
        "learning_rate": 0.00019920883859288034,
        "epoch": 0.030266666666666667,
        "step": 227
    },
    {
        "loss": 1.8717,
        "grad_norm": 3.3986878395080566,
        "learning_rate": 0.00019920091777966877,
        "epoch": 0.0304,
        "step": 228
    },
    {
        "loss": 2.3167,
        "grad_norm": 3.2209696769714355,
        "learning_rate": 0.0001991929576726356,
        "epoch": 0.030533333333333332,
        "step": 229
    },
    {
        "loss": 0.8439,
        "grad_norm": 3.857145071029663,
        "learning_rate": 0.00019918495827493394,
        "epoch": 0.030666666666666665,
        "step": 230
    },
    {
        "loss": 2.9582,
        "grad_norm": 2.6026530265808105,
        "learning_rate": 0.00019917691958973234,
        "epoch": 0.0308,
        "step": 231
    },
    {
        "loss": 2.0641,
        "grad_norm": 3.755699396133423,
        "learning_rate": 0.00019916884162021495,
        "epoch": 0.030933333333333334,
        "step": 232
    },
    {
        "loss": 2.3348,
        "grad_norm": 3.395700693130493,
        "learning_rate": 0.0001991607243695815,
        "epoch": 0.031066666666666666,
        "step": 233
    },
    {
        "loss": 2.8553,
        "grad_norm": 3.4764416217803955,
        "learning_rate": 0.00019915256784104723,
        "epoch": 0.0312,
        "step": 234
    },
    {
        "loss": 2.5233,
        "grad_norm": 3.171721935272217,
        "learning_rate": 0.000199144372037843,
        "epoch": 0.03133333333333333,
        "step": 235
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.7572736740112305,
        "learning_rate": 0.00019913613696321517,
        "epoch": 0.031466666666666664,
        "step": 236
    },
    {
        "loss": 3.1011,
        "grad_norm": 1.8273463249206543,
        "learning_rate": 0.0001991278626204257,
        "epoch": 0.0316,
        "step": 237
    },
    {
        "loss": 2.5257,
        "grad_norm": 5.332932949066162,
        "learning_rate": 0.00019911954901275205,
        "epoch": 0.031733333333333336,
        "step": 238
    },
    {
        "loss": 2.6331,
        "grad_norm": 2.7404301166534424,
        "learning_rate": 0.0001991111961434873,
        "epoch": 0.03186666666666667,
        "step": 239
    },
    {
        "loss": 2.5999,
        "grad_norm": 2.230177640914917,
        "learning_rate": 0.00019910280401594005,
        "epoch": 0.032,
        "step": 240
    },
    {
        "loss": 1.2631,
        "grad_norm": 3.364814281463623,
        "learning_rate": 0.00019909437263343445,
        "epoch": 0.03213333333333333,
        "step": 241
    },
    {
        "loss": 2.6575,
        "grad_norm": 3.1369495391845703,
        "learning_rate": 0.0001990859019993102,
        "epoch": 0.032266666666666666,
        "step": 242
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.7351796627044678,
        "learning_rate": 0.00019907739211692254,
        "epoch": 0.0324,
        "step": 243
    },
    {
        "loss": 2.2248,
        "grad_norm": 3.228902816772461,
        "learning_rate": 0.00019906884298964226,
        "epoch": 0.03253333333333333,
        "step": 244
    },
    {
        "loss": 3.1386,
        "grad_norm": 2.393693208694458,
        "learning_rate": 0.00019906025462085569,
        "epoch": 0.03266666666666666,
        "step": 245
    },
    {
        "loss": 2.8101,
        "grad_norm": 2.6730904579162598,
        "learning_rate": 0.00019905162701396473,
        "epoch": 0.0328,
        "step": 246
    },
    {
        "loss": 2.5041,
        "grad_norm": 3.2532527446746826,
        "learning_rate": 0.0001990429601723868,
        "epoch": 0.032933333333333335,
        "step": 247
    },
    {
        "loss": 1.8497,
        "grad_norm": 5.348456382751465,
        "learning_rate": 0.0001990342540995549,
        "epoch": 0.03306666666666667,
        "step": 248
    },
    {
        "loss": 2.9718,
        "grad_norm": 3.1791553497314453,
        "learning_rate": 0.00019902550879891747,
        "epoch": 0.0332,
        "step": 249
    },
    {
        "loss": 2.4721,
        "grad_norm": 3.642449378967285,
        "learning_rate": 0.00019901672427393856,
        "epoch": 0.03333333333333333,
        "step": 250
    },
    {
        "loss": 2.278,
        "grad_norm": 3.765191078186035,
        "learning_rate": 0.00019900790052809782,
        "epoch": 0.033466666666666665,
        "step": 251
    },
    {
        "loss": 1.8794,
        "grad_norm": 2.697680711746216,
        "learning_rate": 0.0001989990375648903,
        "epoch": 0.0336,
        "step": 252
    },
    {
        "loss": 3.3938,
        "grad_norm": 1.9131579399108887,
        "learning_rate": 0.00019899013538782665,
        "epoch": 0.03373333333333333,
        "step": 253
    },
    {
        "loss": 2.4697,
        "grad_norm": 3.713712453842163,
        "learning_rate": 0.00019898119400043308,
        "epoch": 0.03386666666666667,
        "step": 254
    },
    {
        "loss": 3.2907,
        "grad_norm": 5.568187713623047,
        "learning_rate": 0.0001989722134062513,
        "epoch": 0.034,
        "step": 255
    },
    {
        "loss": 2.3725,
        "grad_norm": 3.050656318664551,
        "learning_rate": 0.00019896319360883857,
        "epoch": 0.034133333333333335,
        "step": 256
    },
    {
        "loss": 1.0881,
        "grad_norm": 3.0037312507629395,
        "learning_rate": 0.00019895413461176763,
        "epoch": 0.03426666666666667,
        "step": 257
    },
    {
        "loss": 2.9017,
        "grad_norm": 3.68183970451355,
        "learning_rate": 0.0001989450364186268,
        "epoch": 0.0344,
        "step": 258
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.497713804244995,
        "learning_rate": 0.00019893589903301987,
        "epoch": 0.03453333333333333,
        "step": 259
    },
    {
        "loss": 1.7575,
        "grad_norm": 4.381228446960449,
        "learning_rate": 0.00019892672245856624,
        "epoch": 0.034666666666666665,
        "step": 260
    },
    {
        "loss": 3.0496,
        "grad_norm": 2.254763126373291,
        "learning_rate": 0.00019891750669890078,
        "epoch": 0.0348,
        "step": 261
    },
    {
        "loss": 1.2445,
        "grad_norm": 3.246922492980957,
        "learning_rate": 0.00019890825175767382,
        "epoch": 0.03493333333333333,
        "step": 262
    },
    {
        "loss": 1.9656,
        "grad_norm": 3.1778438091278076,
        "learning_rate": 0.00019889895763855134,
        "epoch": 0.03506666666666667,
        "step": 263
    },
    {
        "loss": 2.5996,
        "grad_norm": 3.192952871322632,
        "learning_rate": 0.00019888962434521474,
        "epoch": 0.0352,
        "step": 264
    },
    {
        "loss": 2.1602,
        "grad_norm": 5.3266730308532715,
        "learning_rate": 0.000198880251881361,
        "epoch": 0.035333333333333335,
        "step": 265
    },
    {
        "loss": 1.4857,
        "grad_norm": 4.593995094299316,
        "learning_rate": 0.00019887084025070254,
        "epoch": 0.03546666666666667,
        "step": 266
    },
    {
        "loss": 2.4677,
        "grad_norm": 2.465291738510132,
        "learning_rate": 0.00019886138945696737,
        "epoch": 0.0356,
        "step": 267
    },
    {
        "loss": 2.6308,
        "grad_norm": 4.183257102966309,
        "learning_rate": 0.00019885189950389896,
        "epoch": 0.03573333333333333,
        "step": 268
    },
    {
        "loss": 0.9327,
        "grad_norm": 3.8598432540893555,
        "learning_rate": 0.00019884237039525634,
        "epoch": 0.035866666666666665,
        "step": 269
    },
    {
        "loss": 2.6526,
        "grad_norm": 3.127382278442383,
        "learning_rate": 0.00019883280213481403,
        "epoch": 0.036,
        "step": 270
    },
    {
        "loss": 2.9757,
        "grad_norm": 2.698974132537842,
        "learning_rate": 0.000198823194726362,
        "epoch": 0.03613333333333334,
        "step": 271
    },
    {
        "loss": 1.9444,
        "grad_norm": 3.5947792530059814,
        "learning_rate": 0.00019881354817370583,
        "epoch": 0.03626666666666667,
        "step": 272
    },
    {
        "loss": 2.2314,
        "grad_norm": 2.7346014976501465,
        "learning_rate": 0.0001988038624806665,
        "epoch": 0.0364,
        "step": 273
    },
    {
        "loss": 2.8219,
        "grad_norm": 1.7570749521255493,
        "learning_rate": 0.0001987941376510806,
        "epoch": 0.036533333333333334,
        "step": 274
    },
    {
        "loss": 2.396,
        "grad_norm": 4.766597270965576,
        "learning_rate": 0.00019878437368880017,
        "epoch": 0.03666666666666667,
        "step": 275
    },
    {
        "loss": 1.0149,
        "grad_norm": 3.0758516788482666,
        "learning_rate": 0.00019877457059769266,
        "epoch": 0.0368,
        "step": 276
    },
    {
        "loss": 2.5489,
        "grad_norm": 2.47279691696167,
        "learning_rate": 0.00019876472838164123,
        "epoch": 0.03693333333333333,
        "step": 277
    },
    {
        "loss": 2.2918,
        "grad_norm": 2.7669084072113037,
        "learning_rate": 0.0001987548470445443,
        "epoch": 0.037066666666666664,
        "step": 278
    },
    {
        "loss": 2.7413,
        "grad_norm": 2.090632915496826,
        "learning_rate": 0.00019874492659031597,
        "epoch": 0.0372,
        "step": 279
    },
    {
        "loss": 2.9074,
        "grad_norm": 3.1135900020599365,
        "learning_rate": 0.00019873496702288578,
        "epoch": 0.037333333333333336,
        "step": 280
    },
    {
        "loss": 2.9261,
        "grad_norm": 2.8225865364074707,
        "learning_rate": 0.0001987249683461987,
        "epoch": 0.03746666666666667,
        "step": 281
    },
    {
        "loss": 2.4223,
        "grad_norm": 3.9480135440826416,
        "learning_rate": 0.00019871493056421527,
        "epoch": 0.0376,
        "step": 282
    },
    {
        "loss": 2.1097,
        "grad_norm": 4.063195705413818,
        "learning_rate": 0.00019870485368091148,
        "epoch": 0.037733333333333334,
        "step": 283
    },
    {
        "loss": 1.8985,
        "grad_norm": 4.734363079071045,
        "learning_rate": 0.0001986947377002788,
        "epoch": 0.037866666666666667,
        "step": 284
    },
    {
        "loss": 1.9803,
        "grad_norm": 3.616581678390503,
        "learning_rate": 0.00019868458262632424,
        "epoch": 0.038,
        "step": 285
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.883704662322998,
        "learning_rate": 0.00019867438846307025,
        "epoch": 0.03813333333333333,
        "step": 286
    },
    {
        "loss": 1.8302,
        "grad_norm": 3.5123331546783447,
        "learning_rate": 0.00019866415521455476,
        "epoch": 0.038266666666666664,
        "step": 287
    },
    {
        "loss": 2.4513,
        "grad_norm": 2.417264938354492,
        "learning_rate": 0.00019865388288483119,
        "epoch": 0.0384,
        "step": 288
    },
    {
        "loss": 2.6472,
        "grad_norm": 3.0230836868286133,
        "learning_rate": 0.00019864357147796847,
        "epoch": 0.038533333333333336,
        "step": 289
    },
    {
        "loss": 2.3811,
        "grad_norm": 2.6666812896728516,
        "learning_rate": 0.00019863322099805095,
        "epoch": 0.03866666666666667,
        "step": 290
    },
    {
        "loss": 2.2904,
        "grad_norm": 4.406090259552002,
        "learning_rate": 0.00019862283144917852,
        "epoch": 0.0388,
        "step": 291
    },
    {
        "loss": 2.9146,
        "grad_norm": 1.553815484046936,
        "learning_rate": 0.00019861240283546648,
        "epoch": 0.038933333333333334,
        "step": 292
    },
    {
        "loss": 2.3869,
        "grad_norm": 3.1943860054016113,
        "learning_rate": 0.0001986019351610457,
        "epoch": 0.039066666666666666,
        "step": 293
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.5074126720428467,
        "learning_rate": 0.00019859142843006242,
        "epoch": 0.0392,
        "step": 294
    },
    {
        "loss": 2.9348,
        "grad_norm": 2.108140707015991,
        "learning_rate": 0.00019858088264667836,
        "epoch": 0.03933333333333333,
        "step": 295
    },
    {
        "loss": 2.8758,
        "grad_norm": 2.779937744140625,
        "learning_rate": 0.00019857029781507083,
        "epoch": 0.039466666666666664,
        "step": 296
    },
    {
        "loss": 2.7497,
        "grad_norm": 1.8024152517318726,
        "learning_rate": 0.00019855967393943243,
        "epoch": 0.0396,
        "step": 297
    },
    {
        "loss": 2.3536,
        "grad_norm": 3.4145071506500244,
        "learning_rate": 0.00019854901102397135,
        "epoch": 0.039733333333333336,
        "step": 298
    },
    {
        "loss": 3.2677,
        "grad_norm": 2.8296990394592285,
        "learning_rate": 0.0001985383090729112,
        "epoch": 0.03986666666666667,
        "step": 299
    },
    {
        "loss": 1.6528,
        "grad_norm": 2.522285223007202,
        "learning_rate": 0.0001985275680904911,
        "epoch": 0.04,
        "step": 300
    },
    {
        "loss": 1.6934,
        "grad_norm": 4.851395130157471,
        "learning_rate": 0.0001985167880809655,
        "epoch": 0.04013333333333333,
        "step": 301
    },
    {
        "loss": 1.3213,
        "grad_norm": 3.640655040740967,
        "learning_rate": 0.00019850596904860451,
        "epoch": 0.040266666666666666,
        "step": 302
    },
    {
        "loss": 2.3884,
        "grad_norm": 4.4360456466674805,
        "learning_rate": 0.0001984951109976935,
        "epoch": 0.0404,
        "step": 303
    },
    {
        "loss": 2.2204,
        "grad_norm": 3.188222646713257,
        "learning_rate": 0.0001984842139325334,
        "epoch": 0.04053333333333333,
        "step": 304
    },
    {
        "loss": 2.2623,
        "grad_norm": 4.299182415008545,
        "learning_rate": 0.0001984732778574406,
        "epoch": 0.04066666666666666,
        "step": 305
    },
    {
        "loss": 2.7027,
        "grad_norm": 2.932565689086914,
        "learning_rate": 0.00019846230277674692,
        "epoch": 0.0408,
        "step": 306
    },
    {
        "loss": 2.6256,
        "grad_norm": 3.0575344562530518,
        "learning_rate": 0.00019845128869479958,
        "epoch": 0.040933333333333335,
        "step": 307
    },
    {
        "loss": 1.6381,
        "grad_norm": 3.4649529457092285,
        "learning_rate": 0.00019844023561596132,
        "epoch": 0.04106666666666667,
        "step": 308
    },
    {
        "loss": 1.6677,
        "grad_norm": 3.127469539642334,
        "learning_rate": 0.00019842914354461033,
        "epoch": 0.0412,
        "step": 309
    },
    {
        "loss": 2.5995,
        "grad_norm": 3.031148910522461,
        "learning_rate": 0.00019841801248514018,
        "epoch": 0.04133333333333333,
        "step": 310
    },
    {
        "loss": 2.9255,
        "grad_norm": 3.8387386798858643,
        "learning_rate": 0.00019840684244195993,
        "epoch": 0.041466666666666666,
        "step": 311
    },
    {
        "loss": 1.1907,
        "grad_norm": 4.411926746368408,
        "learning_rate": 0.00019839563341949408,
        "epoch": 0.0416,
        "step": 312
    },
    {
        "loss": 2.1977,
        "grad_norm": 2.482475519180298,
        "learning_rate": 0.00019838438542218255,
        "epoch": 0.04173333333333333,
        "step": 313
    },
    {
        "loss": 1.6747,
        "grad_norm": 3.552259683609009,
        "learning_rate": 0.00019837309845448074,
        "epoch": 0.04186666666666667,
        "step": 314
    },
    {
        "loss": 2.5237,
        "grad_norm": 3.0497636795043945,
        "learning_rate": 0.0001983617725208594,
        "epoch": 0.042,
        "step": 315
    },
    {
        "loss": 2.7325,
        "grad_norm": 2.2049713134765625,
        "learning_rate": 0.00019835040762580482,
        "epoch": 0.042133333333333335,
        "step": 316
    },
    {
        "loss": 2.0427,
        "grad_norm": 3.539250612258911,
        "learning_rate": 0.0001983390037738187,
        "epoch": 0.04226666666666667,
        "step": 317
    },
    {
        "loss": 2.8611,
        "grad_norm": 3.755319595336914,
        "learning_rate": 0.00019832756096941804,
        "epoch": 0.0424,
        "step": 318
    },
    {
        "loss": 1.9908,
        "grad_norm": 2.6759092807769775,
        "learning_rate": 0.00019831607921713546,
        "epoch": 0.04253333333333333,
        "step": 319
    },
    {
        "loss": 3.0573,
        "grad_norm": 4.241295337677002,
        "learning_rate": 0.00019830455852151891,
        "epoch": 0.042666666666666665,
        "step": 320
    },
    {
        "loss": 1.9733,
        "grad_norm": 2.471970796585083,
        "learning_rate": 0.0001982929988871318,
        "epoch": 0.0428,
        "step": 321
    },
    {
        "loss": 3.166,
        "grad_norm": 4.225913047790527,
        "learning_rate": 0.00019828140031855282,
        "epoch": 0.04293333333333333,
        "step": 322
    },
    {
        "loss": 1.8111,
        "grad_norm": 4.0035247802734375,
        "learning_rate": 0.00019826976282037633,
        "epoch": 0.04306666666666667,
        "step": 323
    },
    {
        "loss": 1.8351,
        "grad_norm": 3.6929633617401123,
        "learning_rate": 0.00019825808639721193,
        "epoch": 0.0432,
        "step": 324
    },
    {
        "loss": 2.5214,
        "grad_norm": 2.9275741577148438,
        "learning_rate": 0.00019824637105368473,
        "epoch": 0.043333333333333335,
        "step": 325
    },
    {
        "loss": 2.3202,
        "grad_norm": 2.7866029739379883,
        "learning_rate": 0.00019823461679443515,
        "epoch": 0.04346666666666667,
        "step": 326
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.585391044616699,
        "learning_rate": 0.00019822282362411917,
        "epoch": 0.0436,
        "step": 327
    },
    {
        "loss": 2.3773,
        "grad_norm": 2.8939082622528076,
        "learning_rate": 0.00019821099154740805,
        "epoch": 0.04373333333333333,
        "step": 328
    },
    {
        "loss": 2.3658,
        "grad_norm": 2.6514596939086914,
        "learning_rate": 0.00019819912056898853,
        "epoch": 0.043866666666666665,
        "step": 329
    },
    {
        "loss": 2.0013,
        "grad_norm": 3.1449129581451416,
        "learning_rate": 0.00019818721069356275,
        "epoch": 0.044,
        "step": 330
    },
    {
        "loss": 2.628,
        "grad_norm": 2.28059720993042,
        "learning_rate": 0.00019817526192584826,
        "epoch": 0.04413333333333333,
        "step": 331
    },
    {
        "loss": 3.0654,
        "grad_norm": 2.9278757572174072,
        "learning_rate": 0.000198163274270578,
        "epoch": 0.04426666666666667,
        "step": 332
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.3452556133270264,
        "learning_rate": 0.00019815124773250027,
        "epoch": 0.0444,
        "step": 333
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.8960728645324707,
        "learning_rate": 0.0001981391823163789,
        "epoch": 0.044533333333333334,
        "step": 334
    },
    {
        "loss": 2.8206,
        "grad_norm": 3.2456583976745605,
        "learning_rate": 0.00019812707802699305,
        "epoch": 0.04466666666666667,
        "step": 335
    },
    {
        "loss": 2.2777,
        "grad_norm": 2.6303212642669678,
        "learning_rate": 0.0001981149348691372,
        "epoch": 0.0448,
        "step": 336
    },
    {
        "loss": 0.5407,
        "grad_norm": 2.9503650665283203,
        "learning_rate": 0.00019810275284762132,
        "epoch": 0.04493333333333333,
        "step": 337
    },
    {
        "loss": 3.1939,
        "grad_norm": 3.4357340335845947,
        "learning_rate": 0.0001980905319672708,
        "epoch": 0.045066666666666665,
        "step": 338
    },
    {
        "loss": 2.8957,
        "grad_norm": 2.699319362640381,
        "learning_rate": 0.00019807827223292628,
        "epoch": 0.0452,
        "step": 339
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.901029109954834,
        "learning_rate": 0.00019806597364944396,
        "epoch": 0.04533333333333334,
        "step": 340
    },
    {
        "loss": 2.364,
        "grad_norm": 3.1886260509490967,
        "learning_rate": 0.00019805363622169532,
        "epoch": 0.04546666666666667,
        "step": 341
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.5150721073150635,
        "learning_rate": 0.00019804125995456727,
        "epoch": 0.0456,
        "step": 342
    },
    {
        "loss": 2.7392,
        "grad_norm": 2.186493396759033,
        "learning_rate": 0.00019802884485296208,
        "epoch": 0.045733333333333334,
        "step": 343
    },
    {
        "loss": 2.431,
        "grad_norm": 3.4722745418548584,
        "learning_rate": 0.00019801639092179745,
        "epoch": 0.04586666666666667,
        "step": 344
    },
    {
        "loss": 2.4222,
        "grad_norm": 3.54685378074646,
        "learning_rate": 0.00019800389816600634,
        "epoch": 0.046,
        "step": 345
    },
    {
        "loss": 2.713,
        "grad_norm": 3.232438087463379,
        "learning_rate": 0.00019799136659053726,
        "epoch": 0.04613333333333333,
        "step": 346
    },
    {
        "loss": 2.3066,
        "grad_norm": 2.3331611156463623,
        "learning_rate": 0.00019797879620035396,
        "epoch": 0.046266666666666664,
        "step": 347
    },
    {
        "loss": 2.7987,
        "grad_norm": 2.3908705711364746,
        "learning_rate": 0.00019796618700043564,
        "epoch": 0.0464,
        "step": 348
    },
    {
        "loss": 2.3627,
        "grad_norm": 3.5779433250427246,
        "learning_rate": 0.00019795353899577684,
        "epoch": 0.046533333333333336,
        "step": 349
    },
    {
        "loss": 2.547,
        "grad_norm": 3.794996500015259,
        "learning_rate": 0.00019794085219138746,
        "epoch": 0.04666666666666667,
        "step": 350
    },
    {
        "loss": 2.6589,
        "grad_norm": 2.3438994884490967,
        "learning_rate": 0.00019792812659229282,
        "epoch": 0.0468,
        "step": 351
    },
    {
        "loss": 0.9302,
        "grad_norm": 3.5210869312286377,
        "learning_rate": 0.00019791536220353356,
        "epoch": 0.046933333333333334,
        "step": 352
    },
    {
        "loss": 2.8587,
        "grad_norm": 2.858630895614624,
        "learning_rate": 0.00019790255903016568,
        "epoch": 0.047066666666666666,
        "step": 353
    },
    {
        "loss": 2.0422,
        "grad_norm": 3.429009199142456,
        "learning_rate": 0.00019788971707726058,
        "epoch": 0.0472,
        "step": 354
    },
    {
        "loss": 2.7106,
        "grad_norm": 3.3256893157958984,
        "learning_rate": 0.00019787683634990498,
        "epoch": 0.04733333333333333,
        "step": 355
    },
    {
        "loss": 3.1655,
        "grad_norm": 3.898477554321289,
        "learning_rate": 0.000197863916853201,
        "epoch": 0.047466666666666664,
        "step": 356
    },
    {
        "loss": 1.4191,
        "grad_norm": 4.177002906799316,
        "learning_rate": 0.0001978509585922661,
        "epoch": 0.0476,
        "step": 357
    },
    {
        "loss": 2.4119,
        "grad_norm": 2.7918050289154053,
        "learning_rate": 0.00019783796157223308,
        "epoch": 0.047733333333333336,
        "step": 358
    },
    {
        "loss": 1.5603,
        "grad_norm": 2.4622890949249268,
        "learning_rate": 0.00019782492579825008,
        "epoch": 0.04786666666666667,
        "step": 359
    },
    {
        "loss": 2.5923,
        "grad_norm": 3.824068784713745,
        "learning_rate": 0.0001978118512754807,
        "epoch": 0.048,
        "step": 360
    },
    {
        "loss": 2.8984,
        "grad_norm": 2.7737104892730713,
        "learning_rate": 0.00019779873800910368,
        "epoch": 0.048133333333333334,
        "step": 361
    },
    {
        "loss": 2.8866,
        "grad_norm": 3.920114517211914,
        "learning_rate": 0.00019778558600431332,
        "epoch": 0.048266666666666666,
        "step": 362
    },
    {
        "loss": 1.478,
        "grad_norm": 4.249118804931641,
        "learning_rate": 0.00019777239526631916,
        "epoch": 0.0484,
        "step": 363
    },
    {
        "loss": 2.4981,
        "grad_norm": 4.719696521759033,
        "learning_rate": 0.00019775916580034607,
        "epoch": 0.04853333333333333,
        "step": 364
    },
    {
        "loss": 2.931,
        "grad_norm": 2.145660877227783,
        "learning_rate": 0.0001977458976116343,
        "epoch": 0.048666666666666664,
        "step": 365
    },
    {
        "loss": 2.4112,
        "grad_norm": 2.9520180225372314,
        "learning_rate": 0.00019773259070543945,
        "epoch": 0.0488,
        "step": 366
    },
    {
        "loss": 2.8938,
        "grad_norm": 2.382918119430542,
        "learning_rate": 0.00019771924508703238,
        "epoch": 0.048933333333333336,
        "step": 367
    },
    {
        "loss": 1.7793,
        "grad_norm": 3.6527345180511475,
        "learning_rate": 0.00019770586076169936,
        "epoch": 0.04906666666666667,
        "step": 368
    },
    {
        "loss": 2.6956,
        "grad_norm": 4.692768573760986,
        "learning_rate": 0.00019769243773474197,
        "epoch": 0.0492,
        "step": 369
    },
    {
        "loss": 2.7237,
        "grad_norm": 2.460742235183716,
        "learning_rate": 0.00019767897601147714,
        "epoch": 0.04933333333333333,
        "step": 370
    },
    {
        "loss": 1.9348,
        "grad_norm": 3.0909788608551025,
        "learning_rate": 0.00019766547559723705,
        "epoch": 0.049466666666666666,
        "step": 371
    },
    {
        "loss": 2.0272,
        "grad_norm": 3.272979736328125,
        "learning_rate": 0.00019765193649736929,
        "epoch": 0.0496,
        "step": 372
    },
    {
        "loss": 2.4356,
        "grad_norm": 2.0682806968688965,
        "learning_rate": 0.00019763835871723675,
        "epoch": 0.04973333333333333,
        "step": 373
    },
    {
        "loss": 2.8364,
        "grad_norm": 2.4916675090789795,
        "learning_rate": 0.00019762474226221762,
        "epoch": 0.04986666666666666,
        "step": 374
    },
    {
        "loss": 2.0542,
        "grad_norm": 2.8134801387786865,
        "learning_rate": 0.00019761108713770543,
        "epoch": 0.05,
        "step": 375
    },
    {
        "loss": 2.6477,
        "grad_norm": 2.9417083263397217,
        "learning_rate": 0.00019759739334910905,
        "epoch": 0.050133333333333335,
        "step": 376
    },
    {
        "loss": 2.036,
        "grad_norm": 4.690659999847412,
        "learning_rate": 0.00019758366090185257,
        "epoch": 0.05026666666666667,
        "step": 377
    },
    {
        "loss": 0.9637,
        "grad_norm": 3.6891775131225586,
        "learning_rate": 0.00019756988980137554,
        "epoch": 0.0504,
        "step": 378
    },
    {
        "loss": 2.5723,
        "grad_norm": 3.0563442707061768,
        "learning_rate": 0.00019755608005313265,
        "epoch": 0.05053333333333333,
        "step": 379
    },
    {
        "loss": 2.6951,
        "grad_norm": 2.754262685775757,
        "learning_rate": 0.00019754223166259407,
        "epoch": 0.050666666666666665,
        "step": 380
    },
    {
        "loss": 2.5781,
        "grad_norm": 2.2667055130004883,
        "learning_rate": 0.00019752834463524516,
        "epoch": 0.0508,
        "step": 381
    },
    {
        "loss": 2.4616,
        "grad_norm": 2.3850674629211426,
        "learning_rate": 0.0001975144189765866,
        "epoch": 0.05093333333333333,
        "step": 382
    },
    {
        "loss": 2.7061,
        "grad_norm": 3.524641275405884,
        "learning_rate": 0.00019750045469213445,
        "epoch": 0.05106666666666667,
        "step": 383
    },
    {
        "loss": 2.344,
        "grad_norm": 3.201446294784546,
        "learning_rate": 0.00019748645178741995,
        "epoch": 0.0512,
        "step": 384
    },
    {
        "loss": 2.5784,
        "grad_norm": 2.1275720596313477,
        "learning_rate": 0.00019747241026798972,
        "epoch": 0.051333333333333335,
        "step": 385
    },
    {
        "loss": 2.1992,
        "grad_norm": 4.029422283172607,
        "learning_rate": 0.00019745833013940563,
        "epoch": 0.05146666666666667,
        "step": 386
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.029229164123535,
        "learning_rate": 0.00019744421140724492,
        "epoch": 0.0516,
        "step": 387
    },
    {
        "loss": 2.279,
        "grad_norm": 3.7092790603637695,
        "learning_rate": 0.00019743005407710004,
        "epoch": 0.05173333333333333,
        "step": 388
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.718003034591675,
        "learning_rate": 0.00019741585815457875,
        "epoch": 0.051866666666666665,
        "step": 389
    },
    {
        "loss": 2.7916,
        "grad_norm": 2.5666792392730713,
        "learning_rate": 0.0001974016236453041,
        "epoch": 0.052,
        "step": 390
    },
    {
        "loss": 1.5506,
        "grad_norm": 4.570983409881592,
        "learning_rate": 0.00019738735055491446,
        "epoch": 0.05213333333333333,
        "step": 391
    },
    {
        "loss": 2.42,
        "grad_norm": 2.651674747467041,
        "learning_rate": 0.0001973730388890634,
        "epoch": 0.05226666666666667,
        "step": 392
    },
    {
        "loss": 1.7889,
        "grad_norm": 3.267843008041382,
        "learning_rate": 0.00019735868865341987,
        "epoch": 0.0524,
        "step": 393
    },
    {
        "loss": 2.4015,
        "grad_norm": 2.7726917266845703,
        "learning_rate": 0.000197344299853668,
        "epoch": 0.052533333333333335,
        "step": 394
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.5177383422851562,
        "learning_rate": 0.00019732987249550726,
        "epoch": 0.05266666666666667,
        "step": 395
    },
    {
        "loss": 2.1441,
        "grad_norm": 2.7938687801361084,
        "learning_rate": 0.00019731540658465238,
        "epoch": 0.0528,
        "step": 396
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.546494483947754,
        "learning_rate": 0.00019730090212683338,
        "epoch": 0.05293333333333333,
        "step": 397
    },
    {
        "loss": 2.5463,
        "grad_norm": 2.9555654525756836,
        "learning_rate": 0.0001972863591277955,
        "epoch": 0.053066666666666665,
        "step": 398
    },
    {
        "loss": 2.5961,
        "grad_norm": 2.7775914669036865,
        "learning_rate": 0.00019727177759329927,
        "epoch": 0.0532,
        "step": 399
    },
    {
        "loss": 2.6961,
        "grad_norm": 2.533465623855591,
        "learning_rate": 0.0001972571575291205,
        "epoch": 0.05333333333333334,
        "step": 400
    },
    {
        "loss": 2.8661,
        "grad_norm": 3.4119315147399902,
        "learning_rate": 0.0001972424989410502,
        "epoch": 0.05346666666666667,
        "step": 401
    },
    {
        "loss": 2.6488,
        "grad_norm": 4.1511077880859375,
        "learning_rate": 0.00019722780183489475,
        "epoch": 0.0536,
        "step": 402
    },
    {
        "loss": 2.0455,
        "grad_norm": 2.2496347427368164,
        "learning_rate": 0.0001972130662164757,
        "epoch": 0.053733333333333334,
        "step": 403
    },
    {
        "loss": 2.6131,
        "grad_norm": 2.657083749771118,
        "learning_rate": 0.00019719829209162987,
        "epoch": 0.05386666666666667,
        "step": 404
    },
    {
        "loss": 3.4234,
        "grad_norm": 2.824498176574707,
        "learning_rate": 0.00019718347946620933,
        "epoch": 0.054,
        "step": 405
    },
    {
        "loss": 1.0942,
        "grad_norm": 5.071785926818848,
        "learning_rate": 0.00019716862834608147,
        "epoch": 0.05413333333333333,
        "step": 406
    },
    {
        "loss": 2.6837,
        "grad_norm": 2.542792320251465,
        "learning_rate": 0.0001971537387371288,
        "epoch": 0.054266666666666664,
        "step": 407
    },
    {
        "loss": 2.7304,
        "grad_norm": 2.531111001968384,
        "learning_rate": 0.0001971388106452492,
        "epoch": 0.0544,
        "step": 408
    },
    {
        "loss": 2.1597,
        "grad_norm": 3.2661635875701904,
        "learning_rate": 0.0001971238440763557,
        "epoch": 0.054533333333333336,
        "step": 409
    },
    {
        "loss": 1.7434,
        "grad_norm": 4.472848415374756,
        "learning_rate": 0.00019710883903637657,
        "epoch": 0.05466666666666667,
        "step": 410
    },
    {
        "loss": 2.9236,
        "grad_norm": 2.7333178520202637,
        "learning_rate": 0.00019709379553125547,
        "epoch": 0.0548,
        "step": 411
    },
    {
        "loss": 2.6565,
        "grad_norm": 1.9367966651916504,
        "learning_rate": 0.00019707871356695108,
        "epoch": 0.054933333333333334,
        "step": 412
    },
    {
        "loss": 1.5735,
        "grad_norm": 3.1731886863708496,
        "learning_rate": 0.0001970635931494375,
        "epoch": 0.05506666666666667,
        "step": 413
    },
    {
        "loss": 1.0883,
        "grad_norm": 4.251270771026611,
        "learning_rate": 0.00019704843428470392,
        "epoch": 0.0552,
        "step": 414
    },
    {
        "loss": 2.7633,
        "grad_norm": 3.2788290977478027,
        "learning_rate": 0.0001970332369787548,
        "epoch": 0.05533333333333333,
        "step": 415
    },
    {
        "loss": 1.936,
        "grad_norm": 3.746995449066162,
        "learning_rate": 0.00019701800123760995,
        "epoch": 0.055466666666666664,
        "step": 416
    },
    {
        "loss": 1.4951,
        "grad_norm": 3.0980842113494873,
        "learning_rate": 0.00019700272706730417,
        "epoch": 0.0556,
        "step": 417
    },
    {
        "loss": 1.2255,
        "grad_norm": 4.009267807006836,
        "learning_rate": 0.0001969874144738877,
        "epoch": 0.055733333333333336,
        "step": 418
    },
    {
        "loss": 3.0891,
        "grad_norm": 2.4301273822784424,
        "learning_rate": 0.00019697206346342588,
        "epoch": 0.05586666666666667,
        "step": 419
    },
    {
        "loss": 2.1455,
        "grad_norm": 2.2081923484802246,
        "learning_rate": 0.00019695667404199927,
        "epoch": 0.056,
        "step": 420
    },
    {
        "loss": 2.7024,
        "grad_norm": 2.661390542984009,
        "learning_rate": 0.00019694124621570373,
        "epoch": 0.056133333333333334,
        "step": 421
    },
    {
        "loss": 2.2582,
        "grad_norm": 2.5077617168426514,
        "learning_rate": 0.00019692577999065022,
        "epoch": 0.056266666666666666,
        "step": 422
    },
    {
        "loss": 2.6764,
        "grad_norm": 3.224956512451172,
        "learning_rate": 0.000196910275372965,
        "epoch": 0.0564,
        "step": 423
    },
    {
        "loss": 2.909,
        "grad_norm": 2.719959020614624,
        "learning_rate": 0.00019689473236878948,
        "epoch": 0.05653333333333333,
        "step": 424
    },
    {
        "loss": 2.4328,
        "grad_norm": 2.980821132659912,
        "learning_rate": 0.00019687915098428035,
        "epoch": 0.056666666666666664,
        "step": 425
    },
    {
        "loss": 1.8786,
        "grad_norm": 3.8654391765594482,
        "learning_rate": 0.00019686353122560937,
        "epoch": 0.0568,
        "step": 426
    },
    {
        "loss": 2.6784,
        "grad_norm": 2.090989351272583,
        "learning_rate": 0.00019684787309896362,
        "epoch": 0.056933333333333336,
        "step": 427
    },
    {
        "loss": 2.791,
        "grad_norm": 2.7783143520355225,
        "learning_rate": 0.00019683217661054534,
        "epoch": 0.05706666666666667,
        "step": 428
    },
    {
        "loss": 1.9554,
        "grad_norm": 2.701993227005005,
        "learning_rate": 0.00019681644176657197,
        "epoch": 0.0572,
        "step": 429
    },
    {
        "loss": 3.1441,
        "grad_norm": 2.4336609840393066,
        "learning_rate": 0.0001968006685732761,
        "epoch": 0.05733333333333333,
        "step": 430
    },
    {
        "loss": 2.4961,
        "grad_norm": 2.7360408306121826,
        "learning_rate": 0.00019678485703690556,
        "epoch": 0.057466666666666666,
        "step": 431
    },
    {
        "loss": 1.8388,
        "grad_norm": 6.947993278503418,
        "learning_rate": 0.0001967690071637234,
        "epoch": 0.0576,
        "step": 432
    },
    {
        "loss": 2.5838,
        "grad_norm": 2.496999502182007,
        "learning_rate": 0.00019675311896000773,
        "epoch": 0.05773333333333333,
        "step": 433
    },
    {
        "loss": 2.5299,
        "grad_norm": 2.9429547786712646,
        "learning_rate": 0.000196737192432052,
        "epoch": 0.057866666666666663,
        "step": 434
    },
    {
        "loss": 3.0201,
        "grad_norm": 3.2403202056884766,
        "learning_rate": 0.0001967212275861647,
        "epoch": 0.058,
        "step": 435
    },
    {
        "loss": 3.1859,
        "grad_norm": 2.6489510536193848,
        "learning_rate": 0.00019670522442866959,
        "epoch": 0.058133333333333335,
        "step": 436
    },
    {
        "loss": 1.8398,
        "grad_norm": 4.696495056152344,
        "learning_rate": 0.00019668918296590558,
        "epoch": 0.05826666666666667,
        "step": 437
    },
    {
        "loss": 2.326,
        "grad_norm": 2.6768174171447754,
        "learning_rate": 0.0001966731032042267,
        "epoch": 0.0584,
        "step": 438
    },
    {
        "loss": 2.951,
        "grad_norm": 3.4845991134643555,
        "learning_rate": 0.0001966569851500023,
        "epoch": 0.05853333333333333,
        "step": 439
    },
    {
        "loss": 2.5097,
        "grad_norm": 2.9631431102752686,
        "learning_rate": 0.00019664082880961666,
        "epoch": 0.058666666666666666,
        "step": 440
    },
    {
        "loss": 2.4212,
        "grad_norm": 3.456806182861328,
        "learning_rate": 0.00019662463418946947,
        "epoch": 0.0588,
        "step": 441
    },
    {
        "loss": 2.6574,
        "grad_norm": 2.288909435272217,
        "learning_rate": 0.00019660840129597543,
        "epoch": 0.05893333333333333,
        "step": 442
    },
    {
        "loss": 1.1662,
        "grad_norm": 3.017059564590454,
        "learning_rate": 0.00019659213013556442,
        "epoch": 0.05906666666666667,
        "step": 443
    },
    {
        "loss": 2.4391,
        "grad_norm": 2.621053457260132,
        "learning_rate": 0.0001965758207146816,
        "epoch": 0.0592,
        "step": 444
    },
    {
        "loss": 1.7581,
        "grad_norm": 2.7702457904815674,
        "learning_rate": 0.00019655947303978707,
        "epoch": 0.059333333333333335,
        "step": 445
    },
    {
        "loss": 2.4097,
        "grad_norm": 4.787160873413086,
        "learning_rate": 0.00019654308711735628,
        "epoch": 0.05946666666666667,
        "step": 446
    },
    {
        "loss": 2.8166,
        "grad_norm": 3.487694263458252,
        "learning_rate": 0.00019652666295387968,
        "epoch": 0.0596,
        "step": 447
    },
    {
        "loss": 2.5089,
        "grad_norm": 2.826082229614258,
        "learning_rate": 0.00019651020055586304,
        "epoch": 0.05973333333333333,
        "step": 448
    },
    {
        "loss": 2.8033,
        "grad_norm": 3.5816187858581543,
        "learning_rate": 0.00019649369992982707,
        "epoch": 0.059866666666666665,
        "step": 449
    },
    {
        "loss": 2.0655,
        "grad_norm": 3.8978021144866943,
        "learning_rate": 0.00019647716108230778,
        "epoch": 0.06,
        "step": 450
    },
    {
        "loss": 2.5557,
        "grad_norm": 2.5908522605895996,
        "learning_rate": 0.0001964605840198562,
        "epoch": 0.06013333333333333,
        "step": 451
    },
    {
        "loss": 2.0835,
        "grad_norm": 2.4429776668548584,
        "learning_rate": 0.00019644396874903865,
        "epoch": 0.06026666666666667,
        "step": 452
    },
    {
        "loss": 2.337,
        "grad_norm": 2.725182056427002,
        "learning_rate": 0.00019642731527643646,
        "epoch": 0.0604,
        "step": 453
    },
    {
        "loss": 2.6816,
        "grad_norm": 1.9282827377319336,
        "learning_rate": 0.00019641062360864614,
        "epoch": 0.060533333333333335,
        "step": 454
    },
    {
        "loss": 1.4124,
        "grad_norm": 3.411677122116089,
        "learning_rate": 0.00019639389375227925,
        "epoch": 0.06066666666666667,
        "step": 455
    },
    {
        "loss": 2.2102,
        "grad_norm": 3.100426435470581,
        "learning_rate": 0.00019637712571396262,
        "epoch": 0.0608,
        "step": 456
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.7811732292175293,
        "learning_rate": 0.00019636031950033808,
        "epoch": 0.06093333333333333,
        "step": 457
    },
    {
        "loss": 2.3545,
        "grad_norm": 2.485091209411621,
        "learning_rate": 0.00019634347511806262,
        "epoch": 0.061066666666666665,
        "step": 458
    },
    {
        "loss": 3.1983,
        "grad_norm": 1.9690719842910767,
        "learning_rate": 0.00019632659257380844,
        "epoch": 0.0612,
        "step": 459
    },
    {
        "loss": 2.6356,
        "grad_norm": 3.7597968578338623,
        "learning_rate": 0.00019630967187426267,
        "epoch": 0.06133333333333333,
        "step": 460
    },
    {
        "loss": 2.4117,
        "grad_norm": 2.6962287425994873,
        "learning_rate": 0.00019629271302612773,
        "epoch": 0.06146666666666667,
        "step": 461
    },
    {
        "loss": 2.8689,
        "grad_norm": 3.037675142288208,
        "learning_rate": 0.00019627571603612106,
        "epoch": 0.0616,
        "step": 462
    },
    {
        "loss": 2.754,
        "grad_norm": 2.971726179122925,
        "learning_rate": 0.0001962586809109752,
        "epoch": 0.061733333333333335,
        "step": 463
    },
    {
        "loss": 3.3393,
        "grad_norm": 2.9218358993530273,
        "learning_rate": 0.00019624160765743782,
        "epoch": 0.06186666666666667,
        "step": 464
    },
    {
        "loss": 2.1998,
        "grad_norm": 3.2211670875549316,
        "learning_rate": 0.00019622449628227173,
        "epoch": 0.062,
        "step": 465
    },
    {
        "loss": 2.532,
        "grad_norm": 2.678908109664917,
        "learning_rate": 0.0001962073467922548,
        "epoch": 0.06213333333333333,
        "step": 466
    },
    {
        "loss": 2.7196,
        "grad_norm": 2.8821823596954346,
        "learning_rate": 0.00019619015919417998,
        "epoch": 0.062266666666666665,
        "step": 467
    },
    {
        "loss": 1.9773,
        "grad_norm": 2.007739543914795,
        "learning_rate": 0.00019617293349485533,
        "epoch": 0.0624,
        "step": 468
    },
    {
        "loss": 2.3411,
        "grad_norm": 4.0677266120910645,
        "learning_rate": 0.00019615566970110403,
        "epoch": 0.06253333333333333,
        "step": 469
    },
    {
        "loss": 1.4943,
        "grad_norm": 2.9578123092651367,
        "learning_rate": 0.0001961383678197643,
        "epoch": 0.06266666666666666,
        "step": 470
    },
    {
        "loss": 2.8781,
        "grad_norm": 1.6108914613723755,
        "learning_rate": 0.00019612102785768955,
        "epoch": 0.0628,
        "step": 471
    },
    {
        "loss": 2.3775,
        "grad_norm": 2.4457945823669434,
        "learning_rate": 0.00019610364982174808,
        "epoch": 0.06293333333333333,
        "step": 472
    },
    {
        "loss": 2.6907,
        "grad_norm": 3.0153355598449707,
        "learning_rate": 0.00019608623371882348,
        "epoch": 0.06306666666666666,
        "step": 473
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.4849047660827637,
        "learning_rate": 0.00019606877955581428,
        "epoch": 0.0632,
        "step": 474
    },
    {
        "loss": 2.7948,
        "grad_norm": 2.4843106269836426,
        "learning_rate": 0.00019605128733963417,
        "epoch": 0.06333333333333334,
        "step": 475
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.6798558235168457,
        "learning_rate": 0.00019603375707721185,
        "epoch": 0.06346666666666667,
        "step": 476
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.6855099201202393,
        "learning_rate": 0.00019601618877549112,
        "epoch": 0.0636,
        "step": 477
    },
    {
        "loss": 2.6647,
        "grad_norm": 2.8617124557495117,
        "learning_rate": 0.00019599858244143085,
        "epoch": 0.06373333333333334,
        "step": 478
    },
    {
        "loss": 2.8977,
        "grad_norm": 2.852590322494507,
        "learning_rate": 0.00019598093808200498,
        "epoch": 0.06386666666666667,
        "step": 479
    },
    {
        "loss": 2.3639,
        "grad_norm": 2.7651259899139404,
        "learning_rate": 0.00019596325570420248,
        "epoch": 0.064,
        "step": 480
    },
    {
        "loss": 2.377,
        "grad_norm": 2.662625312805176,
        "learning_rate": 0.0001959455353150274,
        "epoch": 0.06413333333333333,
        "step": 481
    },
    {
        "loss": 2.5696,
        "grad_norm": 2.7845051288604736,
        "learning_rate": 0.00019592777692149888,
        "epoch": 0.06426666666666667,
        "step": 482
    },
    {
        "loss": 1.8362,
        "grad_norm": 3.518666982650757,
        "learning_rate": 0.00019590998053065103,
        "epoch": 0.0644,
        "step": 483
    },
    {
        "loss": 2.4145,
        "grad_norm": 2.827171564102173,
        "learning_rate": 0.0001958921461495331,
        "epoch": 0.06453333333333333,
        "step": 484
    },
    {
        "loss": 1.7771,
        "grad_norm": 2.976181745529175,
        "learning_rate": 0.00019587427378520934,
        "epoch": 0.06466666666666666,
        "step": 485
    },
    {
        "loss": 1.0614,
        "grad_norm": 2.7235302925109863,
        "learning_rate": 0.00019585636344475905,
        "epoch": 0.0648,
        "step": 486
    },
    {
        "loss": 2.7113,
        "grad_norm": 1.8825370073318481,
        "learning_rate": 0.00019583841513527657,
        "epoch": 0.06493333333333333,
        "step": 487
    },
    {
        "loss": 2.7448,
        "grad_norm": 2.695662498474121,
        "learning_rate": 0.0001958204288638713,
        "epoch": 0.06506666666666666,
        "step": 488
    },
    {
        "loss": 1.7632,
        "grad_norm": 2.5820491313934326,
        "learning_rate": 0.00019580240463766763,
        "epoch": 0.0652,
        "step": 489
    },
    {
        "loss": 2.4825,
        "grad_norm": 3.9722723960876465,
        "learning_rate": 0.00019578434246380507,
        "epoch": 0.06533333333333333,
        "step": 490
    },
    {
        "loss": 2.3651,
        "grad_norm": 3.605330467224121,
        "learning_rate": 0.00019576624234943807,
        "epoch": 0.06546666666666667,
        "step": 491
    },
    {
        "loss": 2.6742,
        "grad_norm": 3.0642333030700684,
        "learning_rate": 0.00019574810430173618,
        "epoch": 0.0656,
        "step": 492
    },
    {
        "loss": 2.6414,
        "grad_norm": 3.0587692260742188,
        "learning_rate": 0.0001957299283278839,
        "epoch": 0.06573333333333334,
        "step": 493
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.50882887840271,
        "learning_rate": 0.0001957117144350808,
        "epoch": 0.06586666666666667,
        "step": 494
    },
    {
        "loss": 2.1692,
        "grad_norm": 3.511937141418457,
        "learning_rate": 0.00019569346263054153,
        "epoch": 0.066,
        "step": 495
    },
    {
        "loss": 1.9339,
        "grad_norm": 2.269949197769165,
        "learning_rate": 0.00019567517292149562,
        "epoch": 0.06613333333333334,
        "step": 496
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.718715190887451,
        "learning_rate": 0.00019565684531518772,
        "epoch": 0.06626666666666667,
        "step": 497
    },
    {
        "loss": 2.6652,
        "grad_norm": 4.193217754364014,
        "learning_rate": 0.00019563847981887743,
        "epoch": 0.0664,
        "step": 498
    },
    {
        "loss": 2.316,
        "grad_norm": 3.4588499069213867,
        "learning_rate": 0.00019562007643983943,
        "epoch": 0.06653333333333333,
        "step": 499
    },
    {
        "loss": 2.1977,
        "grad_norm": 2.690894842147827,
        "learning_rate": 0.00019560163518536332,
        "epoch": 0.06666666666666667,
        "step": 500
    },
    {
        "loss": 3.015,
        "grad_norm": 2.973276376724243,
        "learning_rate": 0.00019558315606275377,
        "epoch": 0.0668,
        "step": 501
    },
    {
        "loss": 0.9606,
        "grad_norm": 3.833012580871582,
        "learning_rate": 0.00019556463907933038,
        "epoch": 0.06693333333333333,
        "step": 502
    },
    {
        "loss": 2.5753,
        "grad_norm": 3.397230863571167,
        "learning_rate": 0.00019554608424242787,
        "epoch": 0.06706666666666666,
        "step": 503
    },
    {
        "loss": 1.415,
        "grad_norm": 4.5802998542785645,
        "learning_rate": 0.00019552749155939578,
        "epoch": 0.0672,
        "step": 504
    },
    {
        "loss": 2.7585,
        "grad_norm": 2.8504140377044678,
        "learning_rate": 0.0001955088610375988,
        "epoch": 0.06733333333333333,
        "step": 505
    },
    {
        "loss": 3.0237,
        "grad_norm": 2.1764397621154785,
        "learning_rate": 0.00019549019268441656,
        "epoch": 0.06746666666666666,
        "step": 506
    },
    {
        "loss": 0.9142,
        "grad_norm": 3.4551405906677246,
        "learning_rate": 0.00019547148650724358,
        "epoch": 0.0676,
        "step": 507
    },
    {
        "loss": 3.0899,
        "grad_norm": 3.330052137374878,
        "learning_rate": 0.00019545274251348945,
        "epoch": 0.06773333333333334,
        "step": 508
    },
    {
        "loss": 2.2286,
        "grad_norm": 3.797438144683838,
        "learning_rate": 0.00019543396071057884,
        "epoch": 0.06786666666666667,
        "step": 509
    },
    {
        "loss": 1.9622,
        "grad_norm": 3.457294464111328,
        "learning_rate": 0.0001954151411059512,
        "epoch": 0.068,
        "step": 510
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.0267603397369385,
        "learning_rate": 0.000195396283707061,
        "epoch": 0.06813333333333334,
        "step": 511
    },
    {
        "loss": 2.9833,
        "grad_norm": 2.783121347427368,
        "learning_rate": 0.0001953773885213778,
        "epoch": 0.06826666666666667,
        "step": 512
    },
    {
        "loss": 2.424,
        "grad_norm": 3.9260852336883545,
        "learning_rate": 0.00019535845555638599,
        "epoch": 0.0684,
        "step": 513
    },
    {
        "loss": 0.6393,
        "grad_norm": 4.091091632843018,
        "learning_rate": 0.00019533948481958503,
        "epoch": 0.06853333333333333,
        "step": 514
    },
    {
        "loss": 3.1166,
        "grad_norm": 1.9249056577682495,
        "learning_rate": 0.00019532047631848926,
        "epoch": 0.06866666666666667,
        "step": 515
    },
    {
        "loss": 2.7386,
        "grad_norm": 2.8912839889526367,
        "learning_rate": 0.00019530143006062805,
        "epoch": 0.0688,
        "step": 516
    },
    {
        "loss": 1.4275,
        "grad_norm": 5.43263053894043,
        "learning_rate": 0.00019528234605354562,
        "epoch": 0.06893333333333333,
        "step": 517
    },
    {
        "loss": 0.8614,
        "grad_norm": 3.885033130645752,
        "learning_rate": 0.00019526322430480129,
        "epoch": 0.06906666666666667,
        "step": 518
    },
    {
        "loss": 1.8294,
        "grad_norm": 1.3928219079971313,
        "learning_rate": 0.0001952440648219692,
        "epoch": 0.0692,
        "step": 519
    },
    {
        "loss": 3.1199,
        "grad_norm": 3.7836244106292725,
        "learning_rate": 0.00019522486761263847,
        "epoch": 0.06933333333333333,
        "step": 520
    },
    {
        "loss": 1.4682,
        "grad_norm": 4.2686333656311035,
        "learning_rate": 0.00019520563268441324,
        "epoch": 0.06946666666666666,
        "step": 521
    },
    {
        "loss": 2.8993,
        "grad_norm": 2.1270711421966553,
        "learning_rate": 0.0001951863600449125,
        "epoch": 0.0696,
        "step": 522
    },
    {
        "loss": 1.8917,
        "grad_norm": 3.6310112476348877,
        "learning_rate": 0.0001951670497017702,
        "epoch": 0.06973333333333333,
        "step": 523
    },
    {
        "loss": 2.1706,
        "grad_norm": 3.3930466175079346,
        "learning_rate": 0.00019514770166263523,
        "epoch": 0.06986666666666666,
        "step": 524
    },
    {
        "loss": 1.1902,
        "grad_norm": 5.763646602630615,
        "learning_rate": 0.00019512831593517142,
        "epoch": 0.07,
        "step": 525
    },
    {
        "loss": 1.3364,
        "grad_norm": 3.844804525375366,
        "learning_rate": 0.00019510889252705754,
        "epoch": 0.07013333333333334,
        "step": 526
    },
    {
        "loss": 1.9556,
        "grad_norm": 2.1584300994873047,
        "learning_rate": 0.00019508943144598727,
        "epoch": 0.07026666666666667,
        "step": 527
    },
    {
        "loss": 3.3921,
        "grad_norm": 3.5531816482543945,
        "learning_rate": 0.00019506993269966918,
        "epoch": 0.0704,
        "step": 528
    },
    {
        "loss": 2.6734,
        "grad_norm": 2.600665330886841,
        "learning_rate": 0.00019505039629582677,
        "epoch": 0.07053333333333334,
        "step": 529
    },
    {
        "loss": 2.2774,
        "grad_norm": 4.687318801879883,
        "learning_rate": 0.00019503082224219854,
        "epoch": 0.07066666666666667,
        "step": 530
    },
    {
        "loss": 2.7525,
        "grad_norm": 2.0110971927642822,
        "learning_rate": 0.0001950112105465378,
        "epoch": 0.0708,
        "step": 531
    },
    {
        "loss": 2.2627,
        "grad_norm": 1.9555927515029907,
        "learning_rate": 0.00019499156121661286,
        "epoch": 0.07093333333333333,
        "step": 532
    },
    {
        "loss": 1.755,
        "grad_norm": 3.1487669944763184,
        "learning_rate": 0.00019497187426020682,
        "epoch": 0.07106666666666667,
        "step": 533
    },
    {
        "loss": 2.4162,
        "grad_norm": 2.588352918624878,
        "learning_rate": 0.00019495214968511776,
        "epoch": 0.0712,
        "step": 534
    },
    {
        "loss": 2.8213,
        "grad_norm": 2.207134485244751,
        "learning_rate": 0.0001949323874991587,
        "epoch": 0.07133333333333333,
        "step": 535
    },
    {
        "loss": 2.5104,
        "grad_norm": 3.6529557704925537,
        "learning_rate": 0.00019491258771015747,
        "epoch": 0.07146666666666666,
        "step": 536
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.4463367462158203,
        "learning_rate": 0.0001948927503259568,
        "epoch": 0.0716,
        "step": 537
    },
    {
        "loss": 1.9283,
        "grad_norm": 3.866875171661377,
        "learning_rate": 0.00019487287535441443,
        "epoch": 0.07173333333333333,
        "step": 538
    },
    {
        "loss": 2.4456,
        "grad_norm": 2.7162106037139893,
        "learning_rate": 0.00019485296280340287,
        "epoch": 0.07186666666666666,
        "step": 539
    },
    {
        "loss": 2.2101,
        "grad_norm": 2.954291820526123,
        "learning_rate": 0.00019483301268080952,
        "epoch": 0.072,
        "step": 540
    },
    {
        "loss": 1.4434,
        "grad_norm": 5.623537540435791,
        "learning_rate": 0.00019481302499453672,
        "epoch": 0.07213333333333333,
        "step": 541
    },
    {
        "loss": 2.0337,
        "grad_norm": 2.9223928451538086,
        "learning_rate": 0.00019479299975250162,
        "epoch": 0.07226666666666667,
        "step": 542
    },
    {
        "loss": 1.8809,
        "grad_norm": 3.656749725341797,
        "learning_rate": 0.00019477293696263634,
        "epoch": 0.0724,
        "step": 543
    },
    {
        "loss": 2.3183,
        "grad_norm": 3.185265064239502,
        "learning_rate": 0.0001947528366328878,
        "epoch": 0.07253333333333334,
        "step": 544
    },
    {
        "loss": 1.1342,
        "grad_norm": 2.8465685844421387,
        "learning_rate": 0.00019473269877121781,
        "epoch": 0.07266666666666667,
        "step": 545
    },
    {
        "loss": 2.025,
        "grad_norm": 4.035647392272949,
        "learning_rate": 0.00019471252338560305,
        "epoch": 0.0728,
        "step": 546
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.248840570449829,
        "learning_rate": 0.00019469231048403502,
        "epoch": 0.07293333333333334,
        "step": 547
    },
    {
        "loss": 1.4845,
        "grad_norm": 4.276047706604004,
        "learning_rate": 0.00019467206007452016,
        "epoch": 0.07306666666666667,
        "step": 548
    },
    {
        "loss": 2.0159,
        "grad_norm": 3.1934328079223633,
        "learning_rate": 0.00019465177216507974,
        "epoch": 0.0732,
        "step": 549
    },
    {
        "loss": 2.5337,
        "grad_norm": 2.8173699378967285,
        "learning_rate": 0.00019463144676374984,
        "epoch": 0.07333333333333333,
        "step": 550
    },
    {
        "loss": 3.1874,
        "grad_norm": 2.121731996536255,
        "learning_rate": 0.00019461108387858143,
        "epoch": 0.07346666666666667,
        "step": 551
    },
    {
        "loss": 3.6256,
        "grad_norm": 6.274295806884766,
        "learning_rate": 0.00019459068351764032,
        "epoch": 0.0736,
        "step": 552
    },
    {
        "loss": 3.1663,
        "grad_norm": 2.4315783977508545,
        "learning_rate": 0.00019457024568900715,
        "epoch": 0.07373333333333333,
        "step": 553
    },
    {
        "loss": 2.7593,
        "grad_norm": 2.638695478439331,
        "learning_rate": 0.00019454977040077743,
        "epoch": 0.07386666666666666,
        "step": 554
    },
    {
        "loss": 2.0955,
        "grad_norm": 2.712346076965332,
        "learning_rate": 0.00019452925766106147,
        "epoch": 0.074,
        "step": 555
    },
    {
        "loss": 1.9065,
        "grad_norm": 4.934295177459717,
        "learning_rate": 0.00019450870747798446,
        "epoch": 0.07413333333333333,
        "step": 556
    },
    {
        "loss": 1.9936,
        "grad_norm": 2.742825746536255,
        "learning_rate": 0.00019448811985968637,
        "epoch": 0.07426666666666666,
        "step": 557
    },
    {
        "loss": 2.2945,
        "grad_norm": 2.449392318725586,
        "learning_rate": 0.00019446749481432206,
        "epoch": 0.0744,
        "step": 558
    },
    {
        "loss": 2.3838,
        "grad_norm": 3.507721185684204,
        "learning_rate": 0.0001944468323500612,
        "epoch": 0.07453333333333333,
        "step": 559
    },
    {
        "loss": 1.9668,
        "grad_norm": 1.9579771757125854,
        "learning_rate": 0.00019442613247508818,
        "epoch": 0.07466666666666667,
        "step": 560
    },
    {
        "loss": 1.7519,
        "grad_norm": 2.363407850265503,
        "learning_rate": 0.00019440539519760232,
        "epoch": 0.0748,
        "step": 561
    },
    {
        "loss": 2.3572,
        "grad_norm": 2.8140323162078857,
        "learning_rate": 0.0001943846205258178,
        "epoch": 0.07493333333333334,
        "step": 562
    },
    {
        "loss": 3.0802,
        "grad_norm": 3.9381282329559326,
        "learning_rate": 0.00019436380846796342,
        "epoch": 0.07506666666666667,
        "step": 563
    },
    {
        "loss": 2.6575,
        "grad_norm": 3.6347270011901855,
        "learning_rate": 0.00019434295903228302,
        "epoch": 0.0752,
        "step": 564
    },
    {
        "loss": 2.7383,
        "grad_norm": 2.713083028793335,
        "learning_rate": 0.00019432207222703504,
        "epoch": 0.07533333333333334,
        "step": 565
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.798980474472046,
        "learning_rate": 0.0001943011480604929,
        "epoch": 0.07546666666666667,
        "step": 566
    },
    {
        "loss": 3.0824,
        "grad_norm": 2.9309654235839844,
        "learning_rate": 0.00019428018654094465,
        "epoch": 0.0756,
        "step": 567
    },
    {
        "loss": 1.8293,
        "grad_norm": 4.0222392082214355,
        "learning_rate": 0.0001942591876766933,
        "epoch": 0.07573333333333333,
        "step": 568
    },
    {
        "loss": 1.1247,
        "grad_norm": 5.146367073059082,
        "learning_rate": 0.0001942381514760565,
        "epoch": 0.07586666666666667,
        "step": 569
    },
    {
        "loss": 2.7626,
        "grad_norm": 2.4962260723114014,
        "learning_rate": 0.0001942170779473668,
        "epoch": 0.076,
        "step": 570
    },
    {
        "loss": 3.336,
        "grad_norm": 2.7241053581237793,
        "learning_rate": 0.00019419596709897147,
        "epoch": 0.07613333333333333,
        "step": 571
    },
    {
        "loss": 2.104,
        "grad_norm": 3.092360258102417,
        "learning_rate": 0.00019417481893923265,
        "epoch": 0.07626666666666666,
        "step": 572
    },
    {
        "loss": 2.8459,
        "grad_norm": 5.917442321777344,
        "learning_rate": 0.00019415363347652714,
        "epoch": 0.0764,
        "step": 573
    },
    {
        "loss": 2.7279,
        "grad_norm": 3.0156431198120117,
        "learning_rate": 0.00019413241071924655,
        "epoch": 0.07653333333333333,
        "step": 574
    },
    {
        "loss": 2.0829,
        "grad_norm": 3.5002572536468506,
        "learning_rate": 0.00019411115067579737,
        "epoch": 0.07666666666666666,
        "step": 575
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.3842246532440186,
        "learning_rate": 0.00019408985335460073,
        "epoch": 0.0768,
        "step": 576
    },
    {
        "loss": 1.8577,
        "grad_norm": 2.2016470432281494,
        "learning_rate": 0.00019406851876409254,
        "epoch": 0.07693333333333334,
        "step": 577
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.905641555786133,
        "learning_rate": 0.00019404714691272356,
        "epoch": 0.07706666666666667,
        "step": 578
    },
    {
        "loss": 0.9936,
        "grad_norm": 3.1120543479919434,
        "learning_rate": 0.00019402573780895923,
        "epoch": 0.0772,
        "step": 579
    },
    {
        "loss": 0.9477,
        "grad_norm": 3.4375858306884766,
        "learning_rate": 0.00019400429146127976,
        "epoch": 0.07733333333333334,
        "step": 580
    },
    {
        "loss": 1.8717,
        "grad_norm": 4.1570234298706055,
        "learning_rate": 0.00019398280787818014,
        "epoch": 0.07746666666666667,
        "step": 581
    },
    {
        "loss": 2.9416,
        "grad_norm": 2.4753293991088867,
        "learning_rate": 0.0001939612870681701,
        "epoch": 0.0776,
        "step": 582
    },
    {
        "loss": 2.7659,
        "grad_norm": 2.2009780406951904,
        "learning_rate": 0.00019393972903977404,
        "epoch": 0.07773333333333333,
        "step": 583
    },
    {
        "loss": 1.965,
        "grad_norm": 2.955375909805298,
        "learning_rate": 0.00019391813380153122,
        "epoch": 0.07786666666666667,
        "step": 584
    },
    {
        "loss": 2.8466,
        "grad_norm": 2.6832621097564697,
        "learning_rate": 0.00019389650136199564,
        "epoch": 0.078,
        "step": 585
    },
    {
        "loss": 2.8878,
        "grad_norm": 1.844870686531067,
        "learning_rate": 0.00019387483172973589,
        "epoch": 0.07813333333333333,
        "step": 586
    },
    {
        "loss": 2.1941,
        "grad_norm": 3.482966899871826,
        "learning_rate": 0.00019385312491333541,
        "epoch": 0.07826666666666666,
        "step": 587
    },
    {
        "loss": 3.0922,
        "grad_norm": 3.1880943775177,
        "learning_rate": 0.00019383138092139238,
        "epoch": 0.0784,
        "step": 588
    },
    {
        "loss": 2.9589,
        "grad_norm": 2.1498777866363525,
        "learning_rate": 0.00019380959976251962,
        "epoch": 0.07853333333333333,
        "step": 589
    },
    {
        "loss": 2.3023,
        "grad_norm": 3.017929792404175,
        "learning_rate": 0.00019378778144534475,
        "epoch": 0.07866666666666666,
        "step": 590
    },
    {
        "loss": 3.0107,
        "grad_norm": 3.997941493988037,
        "learning_rate": 0.00019376592597851008,
        "epoch": 0.0788,
        "step": 591
    },
    {
        "loss": 1.7342,
        "grad_norm": 3.4159798622131348,
        "learning_rate": 0.00019374403337067263,
        "epoch": 0.07893333333333333,
        "step": 592
    },
    {
        "loss": 1.3341,
        "grad_norm": 3.306399345397949,
        "learning_rate": 0.00019372210363050415,
        "epoch": 0.07906666666666666,
        "step": 593
    },
    {
        "loss": 2.0919,
        "grad_norm": 2.6070363521575928,
        "learning_rate": 0.00019370013676669106,
        "epoch": 0.0792,
        "step": 594
    },
    {
        "loss": 1.7627,
        "grad_norm": 3.375190258026123,
        "learning_rate": 0.0001936781327879345,
        "epoch": 0.07933333333333334,
        "step": 595
    },
    {
        "loss": 1.8461,
        "grad_norm": 3.0640499591827393,
        "learning_rate": 0.00019365609170295039,
        "epoch": 0.07946666666666667,
        "step": 596
    },
    {
        "loss": 2.4521,
        "grad_norm": 3.1057932376861572,
        "learning_rate": 0.0001936340135204692,
        "epoch": 0.0796,
        "step": 597
    },
    {
        "loss": 2.1363,
        "grad_norm": 3.1955950260162354,
        "learning_rate": 0.00019361189824923622,
        "epoch": 0.07973333333333334,
        "step": 598
    },
    {
        "loss": 2.8323,
        "grad_norm": 2.5549044609069824,
        "learning_rate": 0.00019358974589801133,
        "epoch": 0.07986666666666667,
        "step": 599
    },
    {
        "loss": 2.38,
        "grad_norm": 2.710571050643921,
        "learning_rate": 0.00019356755647556922,
        "epoch": 0.08,
        "step": 600
    },
    {
        "loss": 2.8834,
        "grad_norm": 2.534235715866089,
        "learning_rate": 0.0001935453299906992,
        "epoch": 0.08013333333333333,
        "step": 601
    },
    {
        "loss": 2.4971,
        "grad_norm": 2.8471932411193848,
        "learning_rate": 0.00019352306645220517,
        "epoch": 0.08026666666666667,
        "step": 602
    },
    {
        "loss": 1.9381,
        "grad_norm": 3.5631892681121826,
        "learning_rate": 0.00019350076586890585,
        "epoch": 0.0804,
        "step": 603
    },
    {
        "loss": 1.5985,
        "grad_norm": 3.5833795070648193,
        "learning_rate": 0.0001934784282496346,
        "epoch": 0.08053333333333333,
        "step": 604
    },
    {
        "loss": 2.4651,
        "grad_norm": 2.6249446868896484,
        "learning_rate": 0.00019345605360323939,
        "epoch": 0.08066666666666666,
        "step": 605
    },
    {
        "loss": 1.942,
        "grad_norm": 3.116935968399048,
        "learning_rate": 0.0001934336419385829,
        "epoch": 0.0808,
        "step": 606
    },
    {
        "loss": 1.6393,
        "grad_norm": 3.619243860244751,
        "learning_rate": 0.00019341119326454247,
        "epoch": 0.08093333333333333,
        "step": 607
    },
    {
        "loss": 3.4589,
        "grad_norm": 3.02543044090271,
        "learning_rate": 0.00019338870759001012,
        "epoch": 0.08106666666666666,
        "step": 608
    },
    {
        "loss": 2.7379,
        "grad_norm": 3.2092392444610596,
        "learning_rate": 0.00019336618492389243,
        "epoch": 0.0812,
        "step": 609
    },
    {
        "loss": 1.8291,
        "grad_norm": 3.7588179111480713,
        "learning_rate": 0.0001933436252751108,
        "epoch": 0.08133333333333333,
        "step": 610
    },
    {
        "loss": 3.437,
        "grad_norm": 2.5599918365478516,
        "learning_rate": 0.00019332102865260114,
        "epoch": 0.08146666666666667,
        "step": 611
    },
    {
        "loss": 3.2078,
        "grad_norm": 2.1116414070129395,
        "learning_rate": 0.000193298395065314,
        "epoch": 0.0816,
        "step": 612
    },
    {
        "loss": 2.0396,
        "grad_norm": 3.316554069519043,
        "learning_rate": 0.0001932757245222147,
        "epoch": 0.08173333333333334,
        "step": 613
    },
    {
        "loss": 2.9986,
        "grad_norm": 2.8448069095611572,
        "learning_rate": 0.00019325301703228308,
        "epoch": 0.08186666666666667,
        "step": 614
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.156439781188965,
        "learning_rate": 0.00019323027260451366,
        "epoch": 0.082,
        "step": 615
    },
    {
        "loss": 2.5746,
        "grad_norm": 2.7531182765960693,
        "learning_rate": 0.0001932074912479156,
        "epoch": 0.08213333333333334,
        "step": 616
    },
    {
        "loss": 2.4398,
        "grad_norm": 3.4881327152252197,
        "learning_rate": 0.00019318467297151266,
        "epoch": 0.08226666666666667,
        "step": 617
    },
    {
        "loss": 1.5966,
        "grad_norm": 2.503981828689575,
        "learning_rate": 0.00019316181778434324,
        "epoch": 0.0824,
        "step": 618
    },
    {
        "loss": 3.0113,
        "grad_norm": 2.411142587661743,
        "learning_rate": 0.00019313892569546031,
        "epoch": 0.08253333333333333,
        "step": 619
    },
    {
        "loss": 2.3269,
        "grad_norm": 2.7631893157958984,
        "learning_rate": 0.0001931159967139316,
        "epoch": 0.08266666666666667,
        "step": 620
    },
    {
        "loss": 1.9804,
        "grad_norm": 4.581546306610107,
        "learning_rate": 0.00019309303084883933,
        "epoch": 0.0828,
        "step": 621
    },
    {
        "loss": 2.514,
        "grad_norm": 2.185450315475464,
        "learning_rate": 0.00019307002810928028,
        "epoch": 0.08293333333333333,
        "step": 622
    },
    {
        "loss": 2.1895,
        "grad_norm": 3.024418830871582,
        "learning_rate": 0.00019304698850436598,
        "epoch": 0.08306666666666666,
        "step": 623
    },
    {
        "loss": 2.1873,
        "grad_norm": 5.130444526672363,
        "learning_rate": 0.00019302391204322248,
        "epoch": 0.0832,
        "step": 624
    },
    {
        "loss": 2.9747,
        "grad_norm": 2.1994428634643555,
        "learning_rate": 0.00019300079873499045,
        "epoch": 0.08333333333333333,
        "step": 625
    },
    {
        "loss": 2.3488,
        "grad_norm": 2.2609851360321045,
        "learning_rate": 0.00019297764858882514,
        "epoch": 0.08346666666666666,
        "step": 626
    },
    {
        "loss": 2.8871,
        "grad_norm": 3.621126890182495,
        "learning_rate": 0.00019295446161389642,
        "epoch": 0.0836,
        "step": 627
    },
    {
        "loss": 2.62,
        "grad_norm": 2.652846097946167,
        "learning_rate": 0.00019293123781938872,
        "epoch": 0.08373333333333334,
        "step": 628
    },
    {
        "loss": 2.5596,
        "grad_norm": 2.8928422927856445,
        "learning_rate": 0.00019290797721450108,
        "epoch": 0.08386666666666667,
        "step": 629
    },
    {
        "loss": 2.0102,
        "grad_norm": 4.005251407623291,
        "learning_rate": 0.00019288467980844708,
        "epoch": 0.084,
        "step": 630
    },
    {
        "loss": 3.7258,
        "grad_norm": 2.8078503608703613,
        "learning_rate": 0.0001928613456104549,
        "epoch": 0.08413333333333334,
        "step": 631
    },
    {
        "loss": 2.526,
        "grad_norm": 3.717819929122925,
        "learning_rate": 0.0001928379746297673,
        "epoch": 0.08426666666666667,
        "step": 632
    },
    {
        "loss": 2.4988,
        "grad_norm": 3.515507936477661,
        "learning_rate": 0.00019281456687564164,
        "epoch": 0.0844,
        "step": 633
    },
    {
        "loss": 2.3141,
        "grad_norm": 3.733055830001831,
        "learning_rate": 0.00019279112235734977,
        "epoch": 0.08453333333333334,
        "step": 634
    },
    {
        "loss": 2.6995,
        "grad_norm": 2.291529655456543,
        "learning_rate": 0.00019276764108417814,
        "epoch": 0.08466666666666667,
        "step": 635
    },
    {
        "loss": 1.8688,
        "grad_norm": 3.875692844390869,
        "learning_rate": 0.00019274412306542779,
        "epoch": 0.0848,
        "step": 636
    },
    {
        "loss": 2.9527,
        "grad_norm": 2.5555965900421143,
        "learning_rate": 0.00019272056831041428,
        "epoch": 0.08493333333333333,
        "step": 637
    },
    {
        "loss": 2.6974,
        "grad_norm": 2.7427849769592285,
        "learning_rate": 0.00019269697682846774,
        "epoch": 0.08506666666666667,
        "step": 638
    },
    {
        "loss": 3.3321,
        "grad_norm": 2.6955480575561523,
        "learning_rate": 0.0001926733486289328,
        "epoch": 0.0852,
        "step": 639
    },
    {
        "loss": 2.346,
        "grad_norm": 2.140322208404541,
        "learning_rate": 0.0001926496837211687,
        "epoch": 0.08533333333333333,
        "step": 640
    },
    {
        "loss": 3.0444,
        "grad_norm": 2.6223177909851074,
        "learning_rate": 0.0001926259821145492,
        "epoch": 0.08546666666666666,
        "step": 641
    },
    {
        "loss": 0.6289,
        "grad_norm": 4.155755996704102,
        "learning_rate": 0.00019260224381846251,
        "epoch": 0.0856,
        "step": 642
    },
    {
        "loss": 1.8021,
        "grad_norm": 2.3787803649902344,
        "learning_rate": 0.00019257846884231157,
        "epoch": 0.08573333333333333,
        "step": 643
    },
    {
        "loss": 0.8453,
        "grad_norm": 3.2327351570129395,
        "learning_rate": 0.00019255465719551364,
        "epoch": 0.08586666666666666,
        "step": 644
    },
    {
        "loss": 2.1,
        "grad_norm": 4.191812515258789,
        "learning_rate": 0.00019253080888750063,
        "epoch": 0.086,
        "step": 645
    },
    {
        "loss": 3.2173,
        "grad_norm": 2.2137649059295654,
        "learning_rate": 0.00019250692392771892,
        "epoch": 0.08613333333333334,
        "step": 646
    },
    {
        "loss": 1.6141,
        "grad_norm": 4.064820766448975,
        "learning_rate": 0.00019248300232562943,
        "epoch": 0.08626666666666667,
        "step": 647
    },
    {
        "loss": 1.5869,
        "grad_norm": 3.8923537731170654,
        "learning_rate": 0.00019245904409070762,
        "epoch": 0.0864,
        "step": 648
    },
    {
        "loss": 1.0958,
        "grad_norm": 3.943775177001953,
        "learning_rate": 0.00019243504923244337,
        "epoch": 0.08653333333333334,
        "step": 649
    },
    {
        "loss": 2.157,
        "grad_norm": 2.787139654159546,
        "learning_rate": 0.00019241101776034116,
        "epoch": 0.08666666666666667,
        "step": 650
    },
    {
        "loss": 2.2225,
        "grad_norm": 3.1486096382141113,
        "learning_rate": 0.00019238694968391995,
        "epoch": 0.0868,
        "step": 651
    },
    {
        "loss": 2.7392,
        "grad_norm": 4.1019463539123535,
        "learning_rate": 0.00019236284501271316,
        "epoch": 0.08693333333333333,
        "step": 652
    },
    {
        "loss": 0.6255,
        "grad_norm": 3.138092517852783,
        "learning_rate": 0.00019233870375626872,
        "epoch": 0.08706666666666667,
        "step": 653
    },
    {
        "loss": 2.0833,
        "grad_norm": 2.108128070831299,
        "learning_rate": 0.00019231452592414908,
        "epoch": 0.0872,
        "step": 654
    },
    {
        "loss": 2.6263,
        "grad_norm": 2.9731569290161133,
        "learning_rate": 0.0001922903115259312,
        "epoch": 0.08733333333333333,
        "step": 655
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.1598799228668213,
        "learning_rate": 0.00019226606057120646,
        "epoch": 0.08746666666666666,
        "step": 656
    },
    {
        "loss": 3.0021,
        "grad_norm": 4.673302173614502,
        "learning_rate": 0.0001922417730695807,
        "epoch": 0.0876,
        "step": 657
    },
    {
        "loss": 2.0957,
        "grad_norm": 3.081319570541382,
        "learning_rate": 0.00019221744903067436,
        "epoch": 0.08773333333333333,
        "step": 658
    },
    {
        "loss": 2.8726,
        "grad_norm": 2.275301933288574,
        "learning_rate": 0.0001921930884641222,
        "epoch": 0.08786666666666666,
        "step": 659
    },
    {
        "loss": 2.1211,
        "grad_norm": 5.053258895874023,
        "learning_rate": 0.0001921686913795736,
        "epoch": 0.088,
        "step": 660
    },
    {
        "loss": 2.6852,
        "grad_norm": 3.6326804161071777,
        "learning_rate": 0.00019214425778669224,
        "epoch": 0.08813333333333333,
        "step": 661
    },
    {
        "loss": 2.9599,
        "grad_norm": 3.6005618572235107,
        "learning_rate": 0.00019211978769515642,
        "epoch": 0.08826666666666666,
        "step": 662
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.838550329208374,
        "learning_rate": 0.00019209528111465882,
        "epoch": 0.0884,
        "step": 663
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.8958215713500977,
        "learning_rate": 0.00019207073805490655,
        "epoch": 0.08853333333333334,
        "step": 664
    },
    {
        "loss": 2.0114,
        "grad_norm": 3.1026487350463867,
        "learning_rate": 0.0001920461585256212,
        "epoch": 0.08866666666666667,
        "step": 665
    },
    {
        "loss": 2.9001,
        "grad_norm": 3.399226188659668,
        "learning_rate": 0.00019202154253653884,
        "epoch": 0.0888,
        "step": 666
    },
    {
        "loss": 1.6573,
        "grad_norm": 3.19974422454834,
        "learning_rate": 0.00019199689009740992,
        "epoch": 0.08893333333333334,
        "step": 667
    },
    {
        "loss": 2.1044,
        "grad_norm": 3.0187041759490967,
        "learning_rate": 0.00019197220121799935,
        "epoch": 0.08906666666666667,
        "step": 668
    },
    {
        "loss": 2.0921,
        "grad_norm": 2.6863741874694824,
        "learning_rate": 0.0001919474759080865,
        "epoch": 0.0892,
        "step": 669
    },
    {
        "loss": 1.9676,
        "grad_norm": 4.578505992889404,
        "learning_rate": 0.00019192271417746513,
        "epoch": 0.08933333333333333,
        "step": 670
    },
    {
        "loss": 2.2134,
        "grad_norm": 3.171515464782715,
        "learning_rate": 0.00019189791603594345,
        "epoch": 0.08946666666666667,
        "step": 671
    },
    {
        "loss": 2.7083,
        "grad_norm": 2.7507619857788086,
        "learning_rate": 0.00019187308149334412,
        "epoch": 0.0896,
        "step": 672
    },
    {
        "loss": 1.0014,
        "grad_norm": 3.2016942501068115,
        "learning_rate": 0.00019184821055950413,
        "epoch": 0.08973333333333333,
        "step": 673
    },
    {
        "loss": 2.5842,
        "grad_norm": 3.6647489070892334,
        "learning_rate": 0.000191823303244275,
        "epoch": 0.08986666666666666,
        "step": 674
    },
    {
        "loss": 2.933,
        "grad_norm": 3.22878098487854,
        "learning_rate": 0.00019179835955752256,
        "epoch": 0.09,
        "step": 675
    },
    {
        "loss": 3.1945,
        "grad_norm": 3.571146011352539,
        "learning_rate": 0.0001917733795091271,
        "epoch": 0.09013333333333333,
        "step": 676
    },
    {
        "loss": 3.2716,
        "grad_norm": 3.2240984439849854,
        "learning_rate": 0.00019174836310898333,
        "epoch": 0.09026666666666666,
        "step": 677
    },
    {
        "loss": 2.6518,
        "grad_norm": 3.4653377532958984,
        "learning_rate": 0.00019172331036700028,
        "epoch": 0.0904,
        "step": 678
    },
    {
        "loss": 2.8206,
        "grad_norm": 2.816182851791382,
        "learning_rate": 0.00019169822129310146,
        "epoch": 0.09053333333333333,
        "step": 679
    },
    {
        "loss": 2.6898,
        "grad_norm": 1.9054460525512695,
        "learning_rate": 0.00019167309589722474,
        "epoch": 0.09066666666666667,
        "step": 680
    },
    {
        "loss": 1.9822,
        "grad_norm": 3.0184133052825928,
        "learning_rate": 0.00019164793418932234,
        "epoch": 0.0908,
        "step": 681
    },
    {
        "loss": 2.5057,
        "grad_norm": 3.4408514499664307,
        "learning_rate": 0.00019162273617936098,
        "epoch": 0.09093333333333334,
        "step": 682
    },
    {
        "loss": 2.8245,
        "grad_norm": 3.9452784061431885,
        "learning_rate": 0.00019159750187732158,
        "epoch": 0.09106666666666667,
        "step": 683
    },
    {
        "loss": 2.3069,
        "grad_norm": 3.5235238075256348,
        "learning_rate": 0.0001915722312931996,
        "epoch": 0.0912,
        "step": 684
    },
    {
        "loss": 1.1173,
        "grad_norm": 3.8139147758483887,
        "learning_rate": 0.00019154692443700474,
        "epoch": 0.09133333333333334,
        "step": 685
    },
    {
        "loss": 1.3289,
        "grad_norm": 3.9290761947631836,
        "learning_rate": 0.0001915215813187612,
        "epoch": 0.09146666666666667,
        "step": 686
    },
    {
        "loss": 2.5018,
        "grad_norm": 3.3031554222106934,
        "learning_rate": 0.00019149620194850746,
        "epoch": 0.0916,
        "step": 687
    },
    {
        "loss": 2.8696,
        "grad_norm": 2.1674928665161133,
        "learning_rate": 0.00019147078633629632,
        "epoch": 0.09173333333333333,
        "step": 688
    },
    {
        "loss": 2.061,
        "grad_norm": 5.704501628875732,
        "learning_rate": 0.00019144533449219509,
        "epoch": 0.09186666666666667,
        "step": 689
    },
    {
        "loss": 2.1668,
        "grad_norm": 3.614744186401367,
        "learning_rate": 0.00019141984642628523,
        "epoch": 0.092,
        "step": 690
    },
    {
        "loss": 1.64,
        "grad_norm": 3.6571996212005615,
        "learning_rate": 0.0001913943221486627,
        "epoch": 0.09213333333333333,
        "step": 691
    },
    {
        "loss": 2.809,
        "grad_norm": 3.685945510864258,
        "learning_rate": 0.00019136876166943778,
        "epoch": 0.09226666666666666,
        "step": 692
    },
    {
        "loss": 1.494,
        "grad_norm": 2.7801718711853027,
        "learning_rate": 0.00019134316499873499,
        "epoch": 0.0924,
        "step": 693
    },
    {
        "loss": 1.141,
        "grad_norm": 5.134045124053955,
        "learning_rate": 0.00019131753214669334,
        "epoch": 0.09253333333333333,
        "step": 694
    },
    {
        "loss": 3.0419,
        "grad_norm": 2.031024217605591,
        "learning_rate": 0.00019129186312346602,
        "epoch": 0.09266666666666666,
        "step": 695
    },
    {
        "loss": 2.6428,
        "grad_norm": 2.3568036556243896,
        "learning_rate": 0.00019126615793922067,
        "epoch": 0.0928,
        "step": 696
    },
    {
        "loss": 1.506,
        "grad_norm": 4.348597049713135,
        "learning_rate": 0.00019124041660413917,
        "epoch": 0.09293333333333334,
        "step": 697
    },
    {
        "loss": 2.7205,
        "grad_norm": 3.7499139308929443,
        "learning_rate": 0.00019121463912841774,
        "epoch": 0.09306666666666667,
        "step": 698
    },
    {
        "loss": 1.3205,
        "grad_norm": 2.9313430786132812,
        "learning_rate": 0.00019118882552226696,
        "epoch": 0.0932,
        "step": 699
    },
    {
        "loss": 1.1158,
        "grad_norm": 3.7847518920898438,
        "learning_rate": 0.0001911629757959117,
        "epoch": 0.09333333333333334,
        "step": 700
    },
    {
        "loss": 2.7303,
        "grad_norm": 1.7333499193191528,
        "learning_rate": 0.00019113708995959104,
        "epoch": 0.09346666666666667,
        "step": 701
    },
    {
        "loss": 2.2618,
        "grad_norm": 2.2455005645751953,
        "learning_rate": 0.00019111116802355852,
        "epoch": 0.0936,
        "step": 702
    },
    {
        "loss": 2.0393,
        "grad_norm": 3.0887088775634766,
        "learning_rate": 0.00019108520999808196,
        "epoch": 0.09373333333333334,
        "step": 703
    },
    {
        "loss": 1.8287,
        "grad_norm": 3.43802547454834,
        "learning_rate": 0.00019105921589344327,
        "epoch": 0.09386666666666667,
        "step": 704
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.9403069019317627,
        "learning_rate": 0.00019103318571993892,
        "epoch": 0.094,
        "step": 705
    },
    {
        "loss": 2.5073,
        "grad_norm": 2.867692708969116,
        "learning_rate": 0.00019100711948787953,
        "epoch": 0.09413333333333333,
        "step": 706
    },
    {
        "loss": 2.4881,
        "grad_norm": 3.1702303886413574,
        "learning_rate": 0.00019098101720759,
        "epoch": 0.09426666666666667,
        "step": 707
    },
    {
        "loss": 2.6327,
        "grad_norm": 1.9688429832458496,
        "learning_rate": 0.00019095487888940953,
        "epoch": 0.0944,
        "step": 708
    },
    {
        "loss": 2.1688,
        "grad_norm": 3.5270047187805176,
        "learning_rate": 0.00019092870454369163,
        "epoch": 0.09453333333333333,
        "step": 709
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.1733949184417725,
        "learning_rate": 0.000190902494180804,
        "epoch": 0.09466666666666666,
        "step": 710
    },
    {
        "loss": 2.1898,
        "grad_norm": 3.4251084327697754,
        "learning_rate": 0.00019087624781112875,
        "epoch": 0.0948,
        "step": 711
    },
    {
        "loss": 2.3646,
        "grad_norm": 2.609938144683838,
        "learning_rate": 0.00019084996544506202,
        "epoch": 0.09493333333333333,
        "step": 712
    },
    {
        "loss": 1.5909,
        "grad_norm": 2.7086517810821533,
        "learning_rate": 0.00019082364709301444,
        "epoch": 0.09506666666666666,
        "step": 713
    },
    {
        "loss": 3.5283,
        "grad_norm": 2.5861051082611084,
        "learning_rate": 0.00019079729276541077,
        "epoch": 0.0952,
        "step": 714
    },
    {
        "loss": 2.198,
        "grad_norm": 4.685744285583496,
        "learning_rate": 0.00019077090247269002,
        "epoch": 0.09533333333333334,
        "step": 715
    },
    {
        "loss": 2.7331,
        "grad_norm": 3.2899444103240967,
        "learning_rate": 0.00019074447622530557,
        "epoch": 0.09546666666666667,
        "step": 716
    },
    {
        "loss": 2.7589,
        "grad_norm": 2.729318380355835,
        "learning_rate": 0.00019071801403372485,
        "epoch": 0.0956,
        "step": 717
    },
    {
        "loss": 2.7301,
        "grad_norm": 3.8242974281311035,
        "learning_rate": 0.00019069151590842966,
        "epoch": 0.09573333333333334,
        "step": 718
    },
    {
        "loss": 1.829,
        "grad_norm": 1.5601431131362915,
        "learning_rate": 0.000190664981859916,
        "epoch": 0.09586666666666667,
        "step": 719
    },
    {
        "loss": 2.2354,
        "grad_norm": 2.129699230194092,
        "learning_rate": 0.00019063841189869408,
        "epoch": 0.096,
        "step": 720
    },
    {
        "loss": 2.7667,
        "grad_norm": 2.6418771743774414,
        "learning_rate": 0.00019061180603528835,
        "epoch": 0.09613333333333333,
        "step": 721
    },
    {
        "loss": 1.8866,
        "grad_norm": 3.6687333583831787,
        "learning_rate": 0.0001905851642802375,
        "epoch": 0.09626666666666667,
        "step": 722
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.7459917068481445,
        "learning_rate": 0.00019055848664409445,
        "epoch": 0.0964,
        "step": 723
    },
    {
        "loss": 1.5615,
        "grad_norm": 3.456928014755249,
        "learning_rate": 0.00019053177313742625,
        "epoch": 0.09653333333333333,
        "step": 724
    },
    {
        "loss": 1.5913,
        "grad_norm": 3.4790055751800537,
        "learning_rate": 0.00019050502377081422,
        "epoch": 0.09666666666666666,
        "step": 725
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.7960762977600098,
        "learning_rate": 0.0001904782385548539,
        "epoch": 0.0968,
        "step": 726
    },
    {
        "loss": 1.3209,
        "grad_norm": 3.9851648807525635,
        "learning_rate": 0.000190451417500155,
        "epoch": 0.09693333333333333,
        "step": 727
    },
    {
        "loss": 2.6604,
        "grad_norm": 3.0656986236572266,
        "learning_rate": 0.00019042456061734141,
        "epoch": 0.09706666666666666,
        "step": 728
    },
    {
        "loss": 2.5755,
        "grad_norm": 2.0545618534088135,
        "learning_rate": 0.00019039766791705126,
        "epoch": 0.0972,
        "step": 729
    },
    {
        "loss": 2.78,
        "grad_norm": 2.428373336791992,
        "learning_rate": 0.00019037073940993682,
        "epoch": 0.09733333333333333,
        "step": 730
    },
    {
        "loss": 2.5539,
        "grad_norm": 3.164159059524536,
        "learning_rate": 0.00019034377510666458,
        "epoch": 0.09746666666666666,
        "step": 731
    },
    {
        "loss": 2.4601,
        "grad_norm": 2.7961907386779785,
        "learning_rate": 0.00019031677501791513,
        "epoch": 0.0976,
        "step": 732
    },
    {
        "loss": 2.5808,
        "grad_norm": 3.3576223850250244,
        "learning_rate": 0.0001902897391543834,
        "epoch": 0.09773333333333334,
        "step": 733
    },
    {
        "loss": 2.5329,
        "grad_norm": 2.639791965484619,
        "learning_rate": 0.00019026266752677833,
        "epoch": 0.09786666666666667,
        "step": 734
    },
    {
        "loss": 2.545,
        "grad_norm": 2.786980152130127,
        "learning_rate": 0.00019023556014582303,
        "epoch": 0.098,
        "step": 735
    },
    {
        "loss": 2.6421,
        "grad_norm": 3.3093621730804443,
        "learning_rate": 0.00019020841702225495,
        "epoch": 0.09813333333333334,
        "step": 736
    },
    {
        "loss": 2.9435,
        "grad_norm": 2.9309167861938477,
        "learning_rate": 0.00019018123816682552,
        "epoch": 0.09826666666666667,
        "step": 737
    },
    {
        "loss": 0.9872,
        "grad_norm": 2.6317479610443115,
        "learning_rate": 0.00019015402359030034,
        "epoch": 0.0984,
        "step": 738
    },
    {
        "loss": 3.025,
        "grad_norm": 3.0499870777130127,
        "learning_rate": 0.00019012677330345923,
        "epoch": 0.09853333333333333,
        "step": 739
    },
    {
        "loss": 1.5831,
        "grad_norm": 12.76215648651123,
        "learning_rate": 0.00019009948731709607,
        "epoch": 0.09866666666666667,
        "step": 740
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.2323074340820312,
        "learning_rate": 0.00019007216564201907,
        "epoch": 0.0988,
        "step": 741
    },
    {
        "loss": 1.8012,
        "grad_norm": 3.9501070976257324,
        "learning_rate": 0.00019004480828905027,
        "epoch": 0.09893333333333333,
        "step": 742
    },
    {
        "loss": 2.1884,
        "grad_norm": 2.675903081893921,
        "learning_rate": 0.00019001741526902612,
        "epoch": 0.09906666666666666,
        "step": 743
    },
    {
        "loss": 2.252,
        "grad_norm": 4.481441974639893,
        "learning_rate": 0.00018998998659279704,
        "epoch": 0.0992,
        "step": 744
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.7179083824157715,
        "learning_rate": 0.00018996252227122766,
        "epoch": 0.09933333333333333,
        "step": 745
    },
    {
        "loss": 2.7046,
        "grad_norm": 4.514947414398193,
        "learning_rate": 0.00018993502231519664,
        "epoch": 0.09946666666666666,
        "step": 746
    },
    {
        "loss": 2.7277,
        "grad_norm": 3.2867696285247803,
        "learning_rate": 0.00018990748673559685,
        "epoch": 0.0996,
        "step": 747
    },
    {
        "loss": 2.5688,
        "grad_norm": 3.882568359375,
        "learning_rate": 0.00018987991554333519,
        "epoch": 0.09973333333333333,
        "step": 748
    },
    {
        "loss": 1.6661,
        "grad_norm": 3.3623549938201904,
        "learning_rate": 0.00018985230874933275,
        "epoch": 0.09986666666666667,
        "step": 749
    },
    {
        "loss": 0.9991,
        "grad_norm": 3.326411247253418,
        "learning_rate": 0.00018982466636452465,
        "epoch": 0.1,
        "step": 750
    },
    {
        "loss": 2.5669,
        "grad_norm": 2.755234718322754,
        "learning_rate": 0.00018979698839986014,
        "epoch": 0.10013333333333334,
        "step": 751
    },
    {
        "loss": 2.2637,
        "grad_norm": 1.9237711429595947,
        "learning_rate": 0.00018976927486630252,
        "epoch": 0.10026666666666667,
        "step": 752
    },
    {
        "loss": 2.9892,
        "grad_norm": 2.79603910446167,
        "learning_rate": 0.00018974152577482927,
        "epoch": 0.1004,
        "step": 753
    },
    {
        "loss": 2.2156,
        "grad_norm": 3.411431312561035,
        "learning_rate": 0.00018971374113643187,
        "epoch": 0.10053333333333334,
        "step": 754
    },
    {
        "loss": 2.2689,
        "grad_norm": 3.255418062210083,
        "learning_rate": 0.00018968592096211588,
        "epoch": 0.10066666666666667,
        "step": 755
    },
    {
        "loss": 2.5568,
        "grad_norm": 3.9056754112243652,
        "learning_rate": 0.000189658065262901,
        "epoch": 0.1008,
        "step": 756
    },
    {
        "loss": 1.4626,
        "grad_norm": 3.819815158843994,
        "learning_rate": 0.000189630174049821,
        "epoch": 0.10093333333333333,
        "step": 757
    },
    {
        "loss": 2.7153,
        "grad_norm": 3.2173070907592773,
        "learning_rate": 0.00018960224733392366,
        "epoch": 0.10106666666666667,
        "step": 758
    },
    {
        "loss": 2.264,
        "grad_norm": 10.513349533081055,
        "learning_rate": 0.00018957428512627084,
        "epoch": 0.1012,
        "step": 759
    },
    {
        "loss": 2.7642,
        "grad_norm": 2.5222253799438477,
        "learning_rate": 0.00018954628743793846,
        "epoch": 0.10133333333333333,
        "step": 760
    },
    {
        "loss": 1.3168,
        "grad_norm": 1.381760835647583,
        "learning_rate": 0.0001895182542800165,
        "epoch": 0.10146666666666666,
        "step": 761
    },
    {
        "loss": 2.6806,
        "grad_norm": 3.0845940113067627,
        "learning_rate": 0.00018949018566360898,
        "epoch": 0.1016,
        "step": 762
    },
    {
        "loss": 2.2891,
        "grad_norm": 2.9562599658966064,
        "learning_rate": 0.00018946208159983398,
        "epoch": 0.10173333333333333,
        "step": 763
    },
    {
        "loss": 2.502,
        "grad_norm": 2.301685333251953,
        "learning_rate": 0.00018943394209982364,
        "epoch": 0.10186666666666666,
        "step": 764
    },
    {
        "loss": 2.0763,
        "grad_norm": 2.1823172569274902,
        "learning_rate": 0.0001894057671747241,
        "epoch": 0.102,
        "step": 765
    },
    {
        "loss": 3.0919,
        "grad_norm": 2.7236852645874023,
        "learning_rate": 0.00018937755683569554,
        "epoch": 0.10213333333333334,
        "step": 766
    },
    {
        "loss": 2.6864,
        "grad_norm": 2.0801966190338135,
        "learning_rate": 0.00018934931109391217,
        "epoch": 0.10226666666666667,
        "step": 767
    },
    {
        "loss": 1.6586,
        "grad_norm": 3.5821805000305176,
        "learning_rate": 0.00018932102996056221,
        "epoch": 0.1024,
        "step": 768
    },
    {
        "loss": 2.7114,
        "grad_norm": 3.179774522781372,
        "learning_rate": 0.00018929271344684794,
        "epoch": 0.10253333333333334,
        "step": 769
    },
    {
        "loss": 1.4772,
        "grad_norm": 4.470616817474365,
        "learning_rate": 0.00018926436156398562,
        "epoch": 0.10266666666666667,
        "step": 770
    },
    {
        "loss": 1.9669,
        "grad_norm": 2.8920841217041016,
        "learning_rate": 0.0001892359743232055,
        "epoch": 0.1028,
        "step": 771
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.915627956390381,
        "learning_rate": 0.00018920755173575191,
        "epoch": 0.10293333333333334,
        "step": 772
    },
    {
        "loss": 1.1171,
        "grad_norm": 3.338884115219116,
        "learning_rate": 0.0001891790938128831,
        "epoch": 0.10306666666666667,
        "step": 773
    },
    {
        "loss": 2.6465,
        "grad_norm": 3.065310001373291,
        "learning_rate": 0.00018915060056587138,
        "epoch": 0.1032,
        "step": 774
    },
    {
        "loss": 1.783,
        "grad_norm": 3.1004221439361572,
        "learning_rate": 0.00018912207200600298,
        "epoch": 0.10333333333333333,
        "step": 775
    },
    {
        "loss": 2.3788,
        "grad_norm": 2.9901387691497803,
        "learning_rate": 0.00018909350814457821,
        "epoch": 0.10346666666666667,
        "step": 776
    },
    {
        "loss": 1.7686,
        "grad_norm": 3.2247705459594727,
        "learning_rate": 0.00018906490899291126,
        "epoch": 0.1036,
        "step": 777
    },
    {
        "loss": 2.8145,
        "grad_norm": 2.251936912536621,
        "learning_rate": 0.00018903627456233037,
        "epoch": 0.10373333333333333,
        "step": 778
    },
    {
        "loss": 2.0002,
        "grad_norm": 3.1634817123413086,
        "learning_rate": 0.00018900760486417775,
        "epoch": 0.10386666666666666,
        "step": 779
    },
    {
        "loss": 2.4166,
        "grad_norm": 2.5102460384368896,
        "learning_rate": 0.00018897889990980956,
        "epoch": 0.104,
        "step": 780
    },
    {
        "loss": 1.9387,
        "grad_norm": 4.079719543457031,
        "learning_rate": 0.00018895015971059594,
        "epoch": 0.10413333333333333,
        "step": 781
    },
    {
        "loss": 1.5401,
        "grad_norm": 3.5551552772521973,
        "learning_rate": 0.00018892138427792094,
        "epoch": 0.10426666666666666,
        "step": 782
    },
    {
        "loss": 1.3569,
        "grad_norm": 4.028527736663818,
        "learning_rate": 0.00018889257362318267,
        "epoch": 0.1044,
        "step": 783
    },
    {
        "loss": 2.7437,
        "grad_norm": 3.6827244758605957,
        "learning_rate": 0.00018886372775779307,
        "epoch": 0.10453333333333334,
        "step": 784
    },
    {
        "loss": 2.4931,
        "grad_norm": 4.0781731605529785,
        "learning_rate": 0.00018883484669317814,
        "epoch": 0.10466666666666667,
        "step": 785
    },
    {
        "loss": 1.9924,
        "grad_norm": 4.228126525878906,
        "learning_rate": 0.0001888059304407777,
        "epoch": 0.1048,
        "step": 786
    },
    {
        "loss": 1.8581,
        "grad_norm": 3.0183942317962646,
        "learning_rate": 0.00018877697901204562,
        "epoch": 0.10493333333333334,
        "step": 787
    },
    {
        "loss": 1.7946,
        "grad_norm": 2.7436363697052,
        "learning_rate": 0.00018874799241844968,
        "epoch": 0.10506666666666667,
        "step": 788
    },
    {
        "loss": 3.3117,
        "grad_norm": 3.637282133102417,
        "learning_rate": 0.0001887189706714715,
        "epoch": 0.1052,
        "step": 789
    },
    {
        "loss": 3.2749,
        "grad_norm": 3.2408435344696045,
        "learning_rate": 0.00018868991378260676,
        "epoch": 0.10533333333333333,
        "step": 790
    },
    {
        "loss": 2.4542,
        "grad_norm": 3.5514044761657715,
        "learning_rate": 0.00018866082176336498,
        "epoch": 0.10546666666666667,
        "step": 791
    },
    {
        "loss": 2.8719,
        "grad_norm": 2.410230875015259,
        "learning_rate": 0.00018863169462526958,
        "epoch": 0.1056,
        "step": 792
    },
    {
        "loss": 3.1394,
        "grad_norm": 2.1257309913635254,
        "learning_rate": 0.00018860253237985793,
        "epoch": 0.10573333333333333,
        "step": 793
    },
    {
        "loss": 3.2029,
        "grad_norm": 2.7978110313415527,
        "learning_rate": 0.0001885733350386813,
        "epoch": 0.10586666666666666,
        "step": 794
    },
    {
        "loss": 2.6583,
        "grad_norm": 2.2258129119873047,
        "learning_rate": 0.00018854410261330486,
        "epoch": 0.106,
        "step": 795
    },
    {
        "loss": 1.3834,
        "grad_norm": 3.558316469192505,
        "learning_rate": 0.00018851483511530764,
        "epoch": 0.10613333333333333,
        "step": 796
    },
    {
        "loss": 2.0632,
        "grad_norm": 2.6497607231140137,
        "learning_rate": 0.00018848553255628266,
        "epoch": 0.10626666666666666,
        "step": 797
    },
    {
        "loss": 2.3008,
        "grad_norm": 3.8026084899902344,
        "learning_rate": 0.00018845619494783672,
        "epoch": 0.1064,
        "step": 798
    },
    {
        "loss": 2.5507,
        "grad_norm": 2.598412275314331,
        "learning_rate": 0.00018842682230159056,
        "epoch": 0.10653333333333333,
        "step": 799
    },
    {
        "loss": 2.569,
        "grad_norm": 3.583406686782837,
        "learning_rate": 0.00018839741462917876,
        "epoch": 0.10666666666666667,
        "step": 800
    },
    {
        "loss": 2.4905,
        "grad_norm": 2.429903030395508,
        "learning_rate": 0.00018836797194224985,
        "epoch": 0.1068,
        "step": 801
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.5796866416931152,
        "learning_rate": 0.0001883384942524661,
        "epoch": 0.10693333333333334,
        "step": 802
    },
    {
        "loss": 1.9407,
        "grad_norm": 2.5607292652130127,
        "learning_rate": 0.0001883089815715038,
        "epoch": 0.10706666666666667,
        "step": 803
    },
    {
        "loss": 2.1306,
        "grad_norm": 3.573354482650757,
        "learning_rate": 0.000188279433911053,
        "epoch": 0.1072,
        "step": 804
    },
    {
        "loss": 2.8464,
        "grad_norm": 2.7204432487487793,
        "learning_rate": 0.0001882498512828176,
        "epoch": 0.10733333333333334,
        "step": 805
    },
    {
        "loss": 1.944,
        "grad_norm": 2.8719699382781982,
        "learning_rate": 0.00018822023369851544,
        "epoch": 0.10746666666666667,
        "step": 806
    },
    {
        "loss": 2.5512,
        "grad_norm": 2.026522636413574,
        "learning_rate": 0.0001881905811698781,
        "epoch": 0.1076,
        "step": 807
    },
    {
        "loss": 2.0779,
        "grad_norm": 2.7737865447998047,
        "learning_rate": 0.00018816089370865102,
        "epoch": 0.10773333333333333,
        "step": 808
    },
    {
        "loss": 1.5737,
        "grad_norm": 2.6309762001037598,
        "learning_rate": 0.00018813117132659356,
        "epoch": 0.10786666666666667,
        "step": 809
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.042307138442993,
        "learning_rate": 0.00018810141403547884,
        "epoch": 0.108,
        "step": 810
    },
    {
        "loss": 1.7086,
        "grad_norm": 3.9829609394073486,
        "learning_rate": 0.00018807162184709383,
        "epoch": 0.10813333333333333,
        "step": 811
    },
    {
        "loss": 2.5451,
        "grad_norm": 8.03098201751709,
        "learning_rate": 0.0001880417947732393,
        "epoch": 0.10826666666666666,
        "step": 812
    },
    {
        "loss": 1.884,
        "grad_norm": 4.240779399871826,
        "learning_rate": 0.00018801193282572988,
        "epoch": 0.1084,
        "step": 813
    },
    {
        "loss": 2.3907,
        "grad_norm": 3.266596555709839,
        "learning_rate": 0.00018798203601639395,
        "epoch": 0.10853333333333333,
        "step": 814
    },
    {
        "loss": 1.9084,
        "grad_norm": 4.025084972381592,
        "learning_rate": 0.00018795210435707378,
        "epoch": 0.10866666666666666,
        "step": 815
    },
    {
        "loss": 2.8392,
        "grad_norm": 2.714552879333496,
        "learning_rate": 0.0001879221378596254,
        "epoch": 0.1088,
        "step": 816
    },
    {
        "loss": 0.7124,
        "grad_norm": 3.335350275039673,
        "learning_rate": 0.00018789213653591858,
        "epoch": 0.10893333333333333,
        "step": 817
    },
    {
        "loss": 1.4909,
        "grad_norm": 4.839285850524902,
        "learning_rate": 0.00018786210039783703,
        "epoch": 0.10906666666666667,
        "step": 818
    },
    {
        "loss": 0.6647,
        "grad_norm": 3.640371084213257,
        "learning_rate": 0.00018783202945727812,
        "epoch": 0.1092,
        "step": 819
    },
    {
        "loss": 2.4902,
        "grad_norm": 2.5837321281433105,
        "learning_rate": 0.00018780192372615305,
        "epoch": 0.10933333333333334,
        "step": 820
    },
    {
        "loss": 2.4813,
        "grad_norm": 4.848301410675049,
        "learning_rate": 0.0001877717832163868,
        "epoch": 0.10946666666666667,
        "step": 821
    },
    {
        "loss": 2.4812,
        "grad_norm": 5.315032005310059,
        "learning_rate": 0.0001877416079399182,
        "epoch": 0.1096,
        "step": 822
    },
    {
        "loss": 2.6255,
        "grad_norm": 3.52648663520813,
        "learning_rate": 0.0001877113979086997,
        "epoch": 0.10973333333333334,
        "step": 823
    },
    {
        "loss": 1.8028,
        "grad_norm": 3.540700674057007,
        "learning_rate": 0.00018768115313469758,
        "epoch": 0.10986666666666667,
        "step": 824
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.0910236835479736,
        "learning_rate": 0.000187650873629892,
        "epoch": 0.11,
        "step": 825
    },
    {
        "loss": 2.2574,
        "grad_norm": 2.4257571697235107,
        "learning_rate": 0.00018762055940627667,
        "epoch": 0.11013333333333333,
        "step": 826
    },
    {
        "loss": 2.8631,
        "grad_norm": 3.7732248306274414,
        "learning_rate": 0.00018759021047585922,
        "epoch": 0.11026666666666667,
        "step": 827
    },
    {
        "loss": 2.4281,
        "grad_norm": 3.4695231914520264,
        "learning_rate": 0.00018755982685066092,
        "epoch": 0.1104,
        "step": 828
    },
    {
        "loss": 2.4741,
        "grad_norm": 4.908236503601074,
        "learning_rate": 0.00018752940854271688,
        "epoch": 0.11053333333333333,
        "step": 829
    },
    {
        "loss": 3.0987,
        "grad_norm": 2.986879348754883,
        "learning_rate": 0.0001874989555640758,
        "epoch": 0.11066666666666666,
        "step": 830
    },
    {
        "loss": 2.2579,
        "grad_norm": 3.8396081924438477,
        "learning_rate": 0.00018746846792680035,
        "epoch": 0.1108,
        "step": 831
    },
    {
        "loss": 3.1752,
        "grad_norm": 2.832796573638916,
        "learning_rate": 0.00018743794564296667,
        "epoch": 0.11093333333333333,
        "step": 832
    },
    {
        "loss": 2.3237,
        "grad_norm": 3.579425096511841,
        "learning_rate": 0.00018740738872466485,
        "epoch": 0.11106666666666666,
        "step": 833
    },
    {
        "loss": 2.8702,
        "grad_norm": 2.5917577743530273,
        "learning_rate": 0.00018737679718399845,
        "epoch": 0.1112,
        "step": 834
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.0688910484313965,
        "learning_rate": 0.000187346171033085,
        "epoch": 0.11133333333333334,
        "step": 835
    },
    {
        "loss": 2.703,
        "grad_norm": 3.1905179023742676,
        "learning_rate": 0.00018731551028405553,
        "epoch": 0.11146666666666667,
        "step": 836
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.3314385414123535,
        "learning_rate": 0.00018728481494905495,
        "epoch": 0.1116,
        "step": 837
    },
    {
        "loss": 2.4403,
        "grad_norm": 2.1629397869110107,
        "learning_rate": 0.00018725408504024178,
        "epoch": 0.11173333333333334,
        "step": 838
    },
    {
        "loss": 1.258,
        "grad_norm": 4.711872577667236,
        "learning_rate": 0.00018722332056978817,
        "epoch": 0.11186666666666667,
        "step": 839
    },
    {
        "loss": 2.0079,
        "grad_norm": 2.99220871925354,
        "learning_rate": 0.00018719252154988009,
        "epoch": 0.112,
        "step": 840
    },
    {
        "loss": 3.3898,
        "grad_norm": 3.206831216812134,
        "learning_rate": 0.0001871616879927171,
        "epoch": 0.11213333333333333,
        "step": 841
    },
    {
        "loss": 2.5373,
        "grad_norm": 4.405667781829834,
        "learning_rate": 0.00018713081991051256,
        "epoch": 0.11226666666666667,
        "step": 842
    },
    {
        "loss": 2.7211,
        "grad_norm": 2.3269362449645996,
        "learning_rate": 0.00018709991731549332,
        "epoch": 0.1124,
        "step": 843
    },
    {
        "loss": 2.3457,
        "grad_norm": 4.69818639755249,
        "learning_rate": 0.00018706898021990007,
        "epoch": 0.11253333333333333,
        "step": 844
    },
    {
        "loss": 2.9151,
        "grad_norm": 2.4592881202697754,
        "learning_rate": 0.00018703800863598705,
        "epoch": 0.11266666666666666,
        "step": 845
    },
    {
        "loss": 2.3894,
        "grad_norm": 2.3398022651672363,
        "learning_rate": 0.0001870070025760222,
        "epoch": 0.1128,
        "step": 846
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.4237279891967773,
        "learning_rate": 0.00018697596205228726,
        "epoch": 0.11293333333333333,
        "step": 847
    },
    {
        "loss": 2.6531,
        "grad_norm": 2.453709125518799,
        "learning_rate": 0.00018694488707707733,
        "epoch": 0.11306666666666666,
        "step": 848
    },
    {
        "loss": 1.9875,
        "grad_norm": 3.208021640777588,
        "learning_rate": 0.00018691377766270133,
        "epoch": 0.1132,
        "step": 849
    },
    {
        "loss": 2.2274,
        "grad_norm": 2.740974187850952,
        "learning_rate": 0.0001868826338214819,
        "epoch": 0.11333333333333333,
        "step": 850
    },
    {
        "loss": 2.4383,
        "grad_norm": 2.575620412826538,
        "learning_rate": 0.00018685145556575513,
        "epoch": 0.11346666666666666,
        "step": 851
    },
    {
        "loss": 1.587,
        "grad_norm": 3.637504816055298,
        "learning_rate": 0.00018682024290787093,
        "epoch": 0.1136,
        "step": 852
    },
    {
        "loss": 1.1454,
        "grad_norm": 3.452065944671631,
        "learning_rate": 0.00018678899586019264,
        "epoch": 0.11373333333333334,
        "step": 853
    },
    {
        "loss": 1.8627,
        "grad_norm": 3.3185935020446777,
        "learning_rate": 0.0001867577144350974,
        "epoch": 0.11386666666666667,
        "step": 854
    },
    {
        "loss": 2.6781,
        "grad_norm": 2.356534481048584,
        "learning_rate": 0.0001867263986449758,
        "epoch": 0.114,
        "step": 855
    },
    {
        "loss": 2.8326,
        "grad_norm": 2.801797866821289,
        "learning_rate": 0.0001866950485022322,
        "epoch": 0.11413333333333334,
        "step": 856
    },
    {
        "loss": 2.0233,
        "grad_norm": 4.122027397155762,
        "learning_rate": 0.0001866636640192845,
        "epoch": 0.11426666666666667,
        "step": 857
    },
    {
        "loss": 1.0431,
        "grad_norm": 3.0952529907226562,
        "learning_rate": 0.00018663224520856417,
        "epoch": 0.1144,
        "step": 858
    },
    {
        "loss": 1.715,
        "grad_norm": 3.963099479675293,
        "learning_rate": 0.00018660079208251634,
        "epoch": 0.11453333333333333,
        "step": 859
    },
    {
        "loss": 1.5945,
        "grad_norm": 2.9761128425598145,
        "learning_rate": 0.00018656930465359964,
        "epoch": 0.11466666666666667,
        "step": 860
    },
    {
        "loss": 2.7653,
        "grad_norm": 2.0243639945983887,
        "learning_rate": 0.00018653778293428643,
        "epoch": 0.1148,
        "step": 861
    },
    {
        "loss": 2.716,
        "grad_norm": 3.3833887577056885,
        "learning_rate": 0.0001865062269370625,
        "epoch": 0.11493333333333333,
        "step": 862
    },
    {
        "loss": 2.4792,
        "grad_norm": 2.3419878482818604,
        "learning_rate": 0.00018647463667442735,
        "epoch": 0.11506666666666666,
        "step": 863
    },
    {
        "loss": 2.6257,
        "grad_norm": 3.2227697372436523,
        "learning_rate": 0.0001864430121588939,
        "epoch": 0.1152,
        "step": 864
    },
    {
        "loss": 2.861,
        "grad_norm": 3.5515894889831543,
        "learning_rate": 0.00018641135340298885,
        "epoch": 0.11533333333333333,
        "step": 865
    },
    {
        "loss": 3.1331,
        "grad_norm": 3.1174142360687256,
        "learning_rate": 0.00018637966041925224,
        "epoch": 0.11546666666666666,
        "step": 866
    },
    {
        "loss": 2.1447,
        "grad_norm": 3.3861865997314453,
        "learning_rate": 0.00018634793322023786,
        "epoch": 0.1156,
        "step": 867
    },
    {
        "loss": 2.6138,
        "grad_norm": 3.5503194332122803,
        "learning_rate": 0.00018631617181851285,
        "epoch": 0.11573333333333333,
        "step": 868
    },
    {
        "loss": 2.6971,
        "grad_norm": 3.419682502746582,
        "learning_rate": 0.00018628437622665808,
        "epoch": 0.11586666666666667,
        "step": 869
    },
    {
        "loss": 2.4148,
        "grad_norm": 4.255547523498535,
        "learning_rate": 0.0001862525464572679,
        "epoch": 0.116,
        "step": 870
    },
    {
        "loss": 2.7897,
        "grad_norm": 2.828840494155884,
        "learning_rate": 0.00018622068252295012,
        "epoch": 0.11613333333333334,
        "step": 871
    },
    {
        "loss": 2.996,
        "grad_norm": 2.259783983230591,
        "learning_rate": 0.00018618878443632623,
        "epoch": 0.11626666666666667,
        "step": 872
    },
    {
        "loss": 2.312,
        "grad_norm": 3.104048013687134,
        "learning_rate": 0.00018615685221003115,
        "epoch": 0.1164,
        "step": 873
    },
    {
        "loss": 2.6741,
        "grad_norm": 3.7910525798797607,
        "learning_rate": 0.00018612488585671336,
        "epoch": 0.11653333333333334,
        "step": 874
    },
    {
        "loss": 2.8502,
        "grad_norm": 3.118276834487915,
        "learning_rate": 0.0001860928853890348,
        "epoch": 0.11666666666666667,
        "step": 875
    },
    {
        "loss": 2.2646,
        "grad_norm": 3.125847578048706,
        "learning_rate": 0.00018606085081967097,
        "epoch": 0.1168,
        "step": 876
    },
    {
        "loss": 2.775,
        "grad_norm": 2.4594621658325195,
        "learning_rate": 0.00018602878216131093,
        "epoch": 0.11693333333333333,
        "step": 877
    },
    {
        "loss": 1.726,
        "grad_norm": 4.040835857391357,
        "learning_rate": 0.0001859966794266571,
        "epoch": 0.11706666666666667,
        "step": 878
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.5093021392822266,
        "learning_rate": 0.0001859645426284255,
        "epoch": 0.1172,
        "step": 879
    },
    {
        "loss": 2.433,
        "grad_norm": 5.082604885101318,
        "learning_rate": 0.0001859323717793457,
        "epoch": 0.11733333333333333,
        "step": 880
    },
    {
        "loss": 2.3763,
        "grad_norm": 2.983955144882202,
        "learning_rate": 0.0001859001668921606,
        "epoch": 0.11746666666666666,
        "step": 881
    },
    {
        "loss": 2.5394,
        "grad_norm": 3.466933488845825,
        "learning_rate": 0.00018586792797962672,
        "epoch": 0.1176,
        "step": 882
    },
    {
        "loss": 2.2049,
        "grad_norm": 4.12486457824707,
        "learning_rate": 0.00018583565505451394,
        "epoch": 0.11773333333333333,
        "step": 883
    },
    {
        "loss": 2.1138,
        "grad_norm": 4.024240493774414,
        "learning_rate": 0.00018580334812960575,
        "epoch": 0.11786666666666666,
        "step": 884
    },
    {
        "loss": 3.1455,
        "grad_norm": 2.268850088119507,
        "learning_rate": 0.00018577100721769898,
        "epoch": 0.118,
        "step": 885
    },
    {
        "loss": 2.0371,
        "grad_norm": 2.549187183380127,
        "learning_rate": 0.000185738632331604,
        "epoch": 0.11813333333333334,
        "step": 886
    },
    {
        "loss": 2.5888,
        "grad_norm": 2.4084367752075195,
        "learning_rate": 0.0001857062234841446,
        "epoch": 0.11826666666666667,
        "step": 887
    },
    {
        "loss": 0.9438,
        "grad_norm": 5.878178119659424,
        "learning_rate": 0.00018567378068815805,
        "epoch": 0.1184,
        "step": 888
    },
    {
        "loss": 2.1954,
        "grad_norm": 3.7120521068573,
        "learning_rate": 0.00018564130395649504,
        "epoch": 0.11853333333333334,
        "step": 889
    },
    {
        "loss": 1.419,
        "grad_norm": 1.8050525188446045,
        "learning_rate": 0.00018560879330201973,
        "epoch": 0.11866666666666667,
        "step": 890
    },
    {
        "loss": 1.4912,
        "grad_norm": 3.188300848007202,
        "learning_rate": 0.00018557624873760968,
        "epoch": 0.1188,
        "step": 891
    },
    {
        "loss": 0.9157,
        "grad_norm": 5.072048187255859,
        "learning_rate": 0.00018554367027615588,
        "epoch": 0.11893333333333334,
        "step": 892
    },
    {
        "loss": 2.9825,
        "grad_norm": 2.886472702026367,
        "learning_rate": 0.00018551105793056282,
        "epoch": 0.11906666666666667,
        "step": 893
    },
    {
        "loss": 2.2224,
        "grad_norm": 3.1233229637145996,
        "learning_rate": 0.00018547841171374837,
        "epoch": 0.1192,
        "step": 894
    },
    {
        "loss": 2.5923,
        "grad_norm": 3.0861663818359375,
        "learning_rate": 0.00018544573163864375,
        "epoch": 0.11933333333333333,
        "step": 895
    },
    {
        "loss": 2.6791,
        "grad_norm": 4.629072189331055,
        "learning_rate": 0.00018541301771819371,
        "epoch": 0.11946666666666667,
        "step": 896
    },
    {
        "loss": 1.1851,
        "grad_norm": 2.712707042694092,
        "learning_rate": 0.00018538026996535627,
        "epoch": 0.1196,
        "step": 897
    },
    {
        "loss": 2.0265,
        "grad_norm": 2.8503170013427734,
        "learning_rate": 0.00018534748839310302,
        "epoch": 0.11973333333333333,
        "step": 898
    },
    {
        "loss": 1.9137,
        "grad_norm": 2.5535848140716553,
        "learning_rate": 0.00018531467301441875,
        "epoch": 0.11986666666666666,
        "step": 899
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.460930824279785,
        "learning_rate": 0.00018528182384230184,
        "epoch": 0.12,
        "step": 900
    },
    {
        "loss": 2.2922,
        "grad_norm": 3.661098003387451,
        "learning_rate": 0.00018524894088976389,
        "epoch": 0.12013333333333333,
        "step": 901
    },
    {
        "loss": 2.3683,
        "grad_norm": 3.5129523277282715,
        "learning_rate": 0.00018521602416982997,
        "epoch": 0.12026666666666666,
        "step": 902
    },
    {
        "loss": 2.4122,
        "grad_norm": 2.085875988006592,
        "learning_rate": 0.00018518307369553853,
        "epoch": 0.1204,
        "step": 903
    },
    {
        "loss": 1.9981,
        "grad_norm": 2.9865593910217285,
        "learning_rate": 0.00018515008947994133,
        "epoch": 0.12053333333333334,
        "step": 904
    },
    {
        "loss": 1.2889,
        "grad_norm": 4.203243255615234,
        "learning_rate": 0.00018511707153610356,
        "epoch": 0.12066666666666667,
        "step": 905
    },
    {
        "loss": 2.8904,
        "grad_norm": 3.0128231048583984,
        "learning_rate": 0.00018508401987710373,
        "epoch": 0.1208,
        "step": 906
    },
    {
        "loss": 2.1305,
        "grad_norm": 5.857056617736816,
        "learning_rate": 0.0001850509345160337,
        "epoch": 0.12093333333333334,
        "step": 907
    },
    {
        "loss": 2.7213,
        "grad_norm": 2.7032504081726074,
        "learning_rate": 0.00018501781546599868,
        "epoch": 0.12106666666666667,
        "step": 908
    },
    {
        "loss": 2.1932,
        "grad_norm": 4.031980037689209,
        "learning_rate": 0.0001849846627401173,
        "epoch": 0.1212,
        "step": 909
    },
    {
        "loss": 0.7158,
        "grad_norm": 2.9871933460235596,
        "learning_rate": 0.00018495147635152142,
        "epoch": 0.12133333333333333,
        "step": 910
    },
    {
        "loss": 2.1921,
        "grad_norm": 4.125807285308838,
        "learning_rate": 0.0001849182563133563,
        "epoch": 0.12146666666666667,
        "step": 911
    },
    {
        "loss": 2.562,
        "grad_norm": 2.4008328914642334,
        "learning_rate": 0.00018488500263878045,
        "epoch": 0.1216,
        "step": 912
    },
    {
        "loss": 2.6789,
        "grad_norm": 2.3505771160125732,
        "learning_rate": 0.00018485171534096586,
        "epoch": 0.12173333333333333,
        "step": 913
    },
    {
        "loss": 2.9453,
        "grad_norm": 2.6200921535491943,
        "learning_rate": 0.0001848183944330977,
        "epoch": 0.12186666666666666,
        "step": 914
    },
    {
        "loss": 1.3057,
        "grad_norm": 4.278138160705566,
        "learning_rate": 0.0001847850399283745,
        "epoch": 0.122,
        "step": 915
    },
    {
        "loss": 2.8242,
        "grad_norm": 2.04593563079834,
        "learning_rate": 0.00018475165184000806,
        "epoch": 0.12213333333333333,
        "step": 916
    },
    {
        "loss": 2.7963,
        "grad_norm": 2.3010125160217285,
        "learning_rate": 0.00018471823018122356,
        "epoch": 0.12226666666666666,
        "step": 917
    },
    {
        "loss": 2.3939,
        "grad_norm": 2.6670658588409424,
        "learning_rate": 0.00018468477496525943,
        "epoch": 0.1224,
        "step": 918
    },
    {
        "loss": 3.0147,
        "grad_norm": 3.0848116874694824,
        "learning_rate": 0.00018465128620536735,
        "epoch": 0.12253333333333333,
        "step": 919
    },
    {
        "loss": 2.2843,
        "grad_norm": 2.9063215255737305,
        "learning_rate": 0.0001846177639148124,
        "epoch": 0.12266666666666666,
        "step": 920
    },
    {
        "loss": 2.241,
        "grad_norm": 3.8050172328948975,
        "learning_rate": 0.00018458420810687278,
        "epoch": 0.1228,
        "step": 921
    },
    {
        "loss": 2.4859,
        "grad_norm": 2.771880626678467,
        "learning_rate": 0.00018455061879484014,
        "epoch": 0.12293333333333334,
        "step": 922
    },
    {
        "loss": 1.8616,
        "grad_norm": 4.948486804962158,
        "learning_rate": 0.0001845169959920193,
        "epoch": 0.12306666666666667,
        "step": 923
    },
    {
        "loss": 2.5611,
        "grad_norm": 2.8675031661987305,
        "learning_rate": 0.00018448333971172835,
        "epoch": 0.1232,
        "step": 924
    },
    {
        "loss": 1.8451,
        "grad_norm": 4.157590866088867,
        "learning_rate": 0.0001844496499672987,
        "epoch": 0.12333333333333334,
        "step": 925
    },
    {
        "loss": 2.1268,
        "grad_norm": 3.0267891883850098,
        "learning_rate": 0.00018441592677207494,
        "epoch": 0.12346666666666667,
        "step": 926
    },
    {
        "loss": 2.4249,
        "grad_norm": 2.9279863834381104,
        "learning_rate": 0.00018438217013941492,
        "epoch": 0.1236,
        "step": 927
    },
    {
        "loss": 0.9583,
        "grad_norm": 3.2678728103637695,
        "learning_rate": 0.00018434838008268977,
        "epoch": 0.12373333333333333,
        "step": 928
    },
    {
        "loss": 2.5955,
        "grad_norm": 2.5730326175689697,
        "learning_rate": 0.0001843145566152839,
        "epoch": 0.12386666666666667,
        "step": 929
    },
    {
        "loss": 0.9629,
        "grad_norm": 3.405872106552124,
        "learning_rate": 0.00018428069975059486,
        "epoch": 0.124,
        "step": 930
    },
    {
        "loss": 1.971,
        "grad_norm": 3.453289031982422,
        "learning_rate": 0.00018424680950203344,
        "epoch": 0.12413333333333333,
        "step": 931
    },
    {
        "loss": 2.305,
        "grad_norm": 3.799402952194214,
        "learning_rate": 0.00018421288588302373,
        "epoch": 0.12426666666666666,
        "step": 932
    },
    {
        "loss": 2.6373,
        "grad_norm": 2.184274435043335,
        "learning_rate": 0.00018417892890700298,
        "epoch": 0.1244,
        "step": 933
    },
    {
        "loss": 1.9651,
        "grad_norm": 4.678276538848877,
        "learning_rate": 0.00018414493858742161,
        "epoch": 0.12453333333333333,
        "step": 934
    },
    {
        "loss": 2.6347,
        "grad_norm": 2.2267189025878906,
        "learning_rate": 0.0001841109149377434,
        "epoch": 0.12466666666666666,
        "step": 935
    },
    {
        "loss": 1.2107,
        "grad_norm": 5.228926658630371,
        "learning_rate": 0.00018407685797144517,
        "epoch": 0.1248,
        "step": 936
    },
    {
        "loss": 1.2823,
        "grad_norm": 2.9599249362945557,
        "learning_rate": 0.000184042767702017,
        "epoch": 0.12493333333333333,
        "step": 937
    },
    {
        "loss": 2.6316,
        "grad_norm": 3.0891458988189697,
        "learning_rate": 0.00018400864414296216,
        "epoch": 0.12506666666666666,
        "step": 938
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.86625075340271,
        "learning_rate": 0.0001839744873077972,
        "epoch": 0.1252,
        "step": 939
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.79203462600708,
        "learning_rate": 0.0001839402972100516,
        "epoch": 0.12533333333333332,
        "step": 940
    },
    {
        "loss": 1.8375,
        "grad_norm": 3.1802022457122803,
        "learning_rate": 0.00018390607386326827,
        "epoch": 0.12546666666666667,
        "step": 941
    },
    {
        "loss": 2.3682,
        "grad_norm": 3.738990545272827,
        "learning_rate": 0.0001838718172810032,
        "epoch": 0.1256,
        "step": 942
    },
    {
        "loss": 2.3341,
        "grad_norm": 4.992336273193359,
        "learning_rate": 0.0001838375274768255,
        "epoch": 0.12573333333333334,
        "step": 943
    },
    {
        "loss": 0.6664,
        "grad_norm": 4.5528459548950195,
        "learning_rate": 0.0001838032044643175,
        "epoch": 0.12586666666666665,
        "step": 944
    },
    {
        "loss": 3.4401,
        "grad_norm": 3.5149054527282715,
        "learning_rate": 0.0001837688482570747,
        "epoch": 0.126,
        "step": 945
    },
    {
        "loss": 2.3947,
        "grad_norm": 3.170098304748535,
        "learning_rate": 0.00018373445886870563,
        "epoch": 0.12613333333333332,
        "step": 946
    },
    {
        "loss": 2.3059,
        "grad_norm": 3.745264768600464,
        "learning_rate": 0.0001837000363128321,
        "epoch": 0.12626666666666667,
        "step": 947
    },
    {
        "loss": 1.1332,
        "grad_norm": 2.7405760288238525,
        "learning_rate": 0.00018366558060308897,
        "epoch": 0.1264,
        "step": 948
    },
    {
        "loss": 2.6339,
        "grad_norm": 3.0595083236694336,
        "learning_rate": 0.00018363109175312427,
        "epoch": 0.12653333333333333,
        "step": 949
    },
    {
        "loss": 2.3244,
        "grad_norm": 3.8783981800079346,
        "learning_rate": 0.0001835965697765992,
        "epoch": 0.12666666666666668,
        "step": 950
    },
    {
        "loss": 2.1393,
        "grad_norm": 7.110865116119385,
        "learning_rate": 0.00018356201468718798,
        "epoch": 0.1268,
        "step": 951
    },
    {
        "loss": 3.0442,
        "grad_norm": 2.8153600692749023,
        "learning_rate": 0.00018352742649857797,
        "epoch": 0.12693333333333334,
        "step": 952
    },
    {
        "loss": 2.5409,
        "grad_norm": 3.6235108375549316,
        "learning_rate": 0.00018349280522446975,
        "epoch": 0.12706666666666666,
        "step": 953
    },
    {
        "loss": 1.2552,
        "grad_norm": 3.4331142902374268,
        "learning_rate": 0.00018345815087857686,
        "epoch": 0.1272,
        "step": 954
    },
    {
        "loss": 2.3802,
        "grad_norm": 2.142772912979126,
        "learning_rate": 0.00018342346347462608,
        "epoch": 0.12733333333333333,
        "step": 955
    },
    {
        "loss": 3.3161,
        "grad_norm": 3.1155030727386475,
        "learning_rate": 0.00018338874302635714,
        "epoch": 0.12746666666666667,
        "step": 956
    },
    {
        "loss": 2.6821,
        "grad_norm": 2.9291913509368896,
        "learning_rate": 0.00018335398954752292,
        "epoch": 0.1276,
        "step": 957
    },
    {
        "loss": 2.8178,
        "grad_norm": 2.4400672912597656,
        "learning_rate": 0.00018331920305188944,
        "epoch": 0.12773333333333334,
        "step": 958
    },
    {
        "loss": 2.3268,
        "grad_norm": 5.348544597625732,
        "learning_rate": 0.00018328438355323572,
        "epoch": 0.12786666666666666,
        "step": 959
    },
    {
        "loss": 2.7071,
        "grad_norm": 3.4882383346557617,
        "learning_rate": 0.0001832495310653539,
        "epoch": 0.128,
        "step": 960
    },
    {
        "loss": 2.7278,
        "grad_norm": 3.2707836627960205,
        "learning_rate": 0.00018321464560204915,
        "epoch": 0.12813333333333332,
        "step": 961
    },
    {
        "loss": 1.8287,
        "grad_norm": 2.0207297801971436,
        "learning_rate": 0.00018317972717713974,
        "epoch": 0.12826666666666667,
        "step": 962
    },
    {
        "loss": 2.3976,
        "grad_norm": 3.450838804244995,
        "learning_rate": 0.00018314477580445694,
        "epoch": 0.1284,
        "step": 963
    },
    {
        "loss": 2.2165,
        "grad_norm": 4.168943881988525,
        "learning_rate": 0.00018310979149784514,
        "epoch": 0.12853333333333333,
        "step": 964
    },
    {
        "loss": 2.3722,
        "grad_norm": 3.6004233360290527,
        "learning_rate": 0.00018307477427116178,
        "epoch": 0.12866666666666668,
        "step": 965
    },
    {
        "loss": 3.0953,
        "grad_norm": 2.345968246459961,
        "learning_rate": 0.00018303972413827722,
        "epoch": 0.1288,
        "step": 966
    },
    {
        "loss": 2.6142,
        "grad_norm": 3.221920967102051,
        "learning_rate": 0.000183004641113075,
        "epoch": 0.12893333333333334,
        "step": 967
    },
    {
        "loss": 2.7337,
        "grad_norm": 3.395479202270508,
        "learning_rate": 0.0001829695252094516,
        "epoch": 0.12906666666666666,
        "step": 968
    },
    {
        "loss": 2.4034,
        "grad_norm": 2.50993013381958,
        "learning_rate": 0.00018293437644131653,
        "epoch": 0.1292,
        "step": 969
    },
    {
        "loss": 2.8402,
        "grad_norm": 4.390321254730225,
        "learning_rate": 0.0001828991948225924,
        "epoch": 0.12933333333333333,
        "step": 970
    },
    {
        "loss": 2.7438,
        "grad_norm": 3.3643829822540283,
        "learning_rate": 0.00018286398036721467,
        "epoch": 0.12946666666666667,
        "step": 971
    },
    {
        "loss": 2.1703,
        "grad_norm": 4.170001983642578,
        "learning_rate": 0.00018282873308913202,
        "epoch": 0.1296,
        "step": 972
    },
    {
        "loss": 2.4731,
        "grad_norm": 3.192845106124878,
        "learning_rate": 0.00018279345300230594,
        "epoch": 0.12973333333333334,
        "step": 973
    },
    {
        "loss": 2.4376,
        "grad_norm": 4.102617263793945,
        "learning_rate": 0.00018275814012071103,
        "epoch": 0.12986666666666666,
        "step": 974
    },
    {
        "loss": 2.5671,
        "grad_norm": 3.054931879043579,
        "learning_rate": 0.0001827227944583348,
        "epoch": 0.13,
        "step": 975
    },
    {
        "loss": 2.4374,
        "grad_norm": 3.3729240894317627,
        "learning_rate": 0.00018268741602917783,
        "epoch": 0.13013333333333332,
        "step": 976
    },
    {
        "loss": 3.103,
        "grad_norm": 3.2499170303344727,
        "learning_rate": 0.00018265200484725362,
        "epoch": 0.13026666666666667,
        "step": 977
    },
    {
        "loss": 2.4302,
        "grad_norm": 2.4255433082580566,
        "learning_rate": 0.00018261656092658865,
        "epoch": 0.1304,
        "step": 978
    },
    {
        "loss": 3.3017,
        "grad_norm": 3.3280088901519775,
        "learning_rate": 0.00018258108428122237,
        "epoch": 0.13053333333333333,
        "step": 979
    },
    {
        "loss": 1.4221,
        "grad_norm": 2.250514507293701,
        "learning_rate": 0.00018254557492520726,
        "epoch": 0.13066666666666665,
        "step": 980
    },
    {
        "loss": 2.4125,
        "grad_norm": 2.4115941524505615,
        "learning_rate": 0.00018251003287260865,
        "epoch": 0.1308,
        "step": 981
    },
    {
        "loss": 1.9493,
        "grad_norm": 3.298491954803467,
        "learning_rate": 0.00018247445813750486,
        "epoch": 0.13093333333333335,
        "step": 982
    },
    {
        "loss": 1.7629,
        "grad_norm": 3.2104499340057373,
        "learning_rate": 0.00018243885073398718,
        "epoch": 0.13106666666666666,
        "step": 983
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.2091660499572754,
        "learning_rate": 0.0001824032106761598,
        "epoch": 0.1312,
        "step": 984
    },
    {
        "loss": 2.6628,
        "grad_norm": 3.1396238803863525,
        "learning_rate": 0.0001823675379781399,
        "epoch": 0.13133333333333333,
        "step": 985
    },
    {
        "loss": 2.0726,
        "grad_norm": 3.1320574283599854,
        "learning_rate": 0.00018233183265405756,
        "epoch": 0.13146666666666668,
        "step": 986
    },
    {
        "loss": 1.303,
        "grad_norm": 4.350712776184082,
        "learning_rate": 0.00018229609471805578,
        "epoch": 0.1316,
        "step": 987
    },
    {
        "loss": 2.2284,
        "grad_norm": 5.879662036895752,
        "learning_rate": 0.00018226032418429045,
        "epoch": 0.13173333333333334,
        "step": 988
    },
    {
        "loss": 2.5603,
        "grad_norm": 3.6218316555023193,
        "learning_rate": 0.0001822245210669304,
        "epoch": 0.13186666666666666,
        "step": 989
    },
    {
        "loss": 2.7134,
        "grad_norm": 3.225996255874634,
        "learning_rate": 0.0001821886853801574,
        "epoch": 0.132,
        "step": 990
    },
    {
        "loss": 2.8868,
        "grad_norm": 2.2594871520996094,
        "learning_rate": 0.00018215281713816606,
        "epoch": 0.13213333333333332,
        "step": 991
    },
    {
        "loss": 2.2644,
        "grad_norm": 2.4260807037353516,
        "learning_rate": 0.0001821169163551639,
        "epoch": 0.13226666666666667,
        "step": 992
    },
    {
        "loss": 2.1142,
        "grad_norm": 2.758418321609497,
        "learning_rate": 0.0001820809830453714,
        "epoch": 0.1324,
        "step": 993
    },
    {
        "loss": 1.8806,
        "grad_norm": 3.0410139560699463,
        "learning_rate": 0.00018204501722302182,
        "epoch": 0.13253333333333334,
        "step": 994
    },
    {
        "loss": 2.1439,
        "grad_norm": 3.1927947998046875,
        "learning_rate": 0.00018200901890236138,
        "epoch": 0.13266666666666665,
        "step": 995
    },
    {
        "loss": 2.5624,
        "grad_norm": 3.16471791267395,
        "learning_rate": 0.00018197298809764908,
        "epoch": 0.1328,
        "step": 996
    },
    {
        "loss": 1.9576,
        "grad_norm": 3.4775538444519043,
        "learning_rate": 0.0001819369248231569,
        "epoch": 0.13293333333333332,
        "step": 997
    },
    {
        "loss": 1.9088,
        "grad_norm": 3.8367984294891357,
        "learning_rate": 0.00018190082909316958,
        "epoch": 0.13306666666666667,
        "step": 998
    },
    {
        "loss": 2.4063,
        "grad_norm": 3.365278482437134,
        "learning_rate": 0.00018186470092198482,
        "epoch": 0.1332,
        "step": 999
    },
    {
        "loss": 3.0888,
        "grad_norm": 4.548874855041504,
        "learning_rate": 0.00018182854032391306,
        "epoch": 0.13333333333333333,
        "step": 1000
    },
    {
        "loss": 2.1855,
        "grad_norm": 3.0945804119110107,
        "learning_rate": 0.00018179234731327765,
        "epoch": 0.13346666666666668,
        "step": 1001
    },
    {
        "loss": 2.088,
        "grad_norm": 4.528562068939209,
        "learning_rate": 0.00018175612190441477,
        "epoch": 0.1336,
        "step": 1002
    },
    {
        "loss": 2.7182,
        "grad_norm": 2.727321147918701,
        "learning_rate": 0.00018171986411167344,
        "epoch": 0.13373333333333334,
        "step": 1003
    },
    {
        "loss": 1.9922,
        "grad_norm": 2.645350694656372,
        "learning_rate": 0.00018168357394941548,
        "epoch": 0.13386666666666666,
        "step": 1004
    },
    {
        "loss": 2.7096,
        "grad_norm": 2.532198190689087,
        "learning_rate": 0.0001816472514320155,
        "epoch": 0.134,
        "step": 1005
    },
    {
        "loss": 2.7565,
        "grad_norm": 2.299271821975708,
        "learning_rate": 0.00018161089657386107,
        "epoch": 0.13413333333333333,
        "step": 1006
    },
    {
        "loss": 2.2538,
        "grad_norm": 2.8288347721099854,
        "learning_rate": 0.00018157450938935244,
        "epoch": 0.13426666666666667,
        "step": 1007
    },
    {
        "loss": 1.805,
        "grad_norm": 3.9947426319122314,
        "learning_rate": 0.00018153808989290266,
        "epoch": 0.1344,
        "step": 1008
    },
    {
        "loss": 2.3841,
        "grad_norm": 4.534128665924072,
        "learning_rate": 0.0001815016380989376,
        "epoch": 0.13453333333333334,
        "step": 1009
    },
    {
        "loss": 3.2921,
        "grad_norm": 2.913323163986206,
        "learning_rate": 0.00018146515402189602,
        "epoch": 0.13466666666666666,
        "step": 1010
    },
    {
        "loss": 2.3611,
        "grad_norm": 2.7025256156921387,
        "learning_rate": 0.00018142863767622936,
        "epoch": 0.1348,
        "step": 1011
    },
    {
        "loss": 2.736,
        "grad_norm": 2.591341018676758,
        "learning_rate": 0.00018139208907640183,
        "epoch": 0.13493333333333332,
        "step": 1012
    },
    {
        "loss": 2.456,
        "grad_norm": 2.710146188735962,
        "learning_rate": 0.0001813555082368905,
        "epoch": 0.13506666666666667,
        "step": 1013
    },
    {
        "loss": 2.2822,
        "grad_norm": 2.82790207862854,
        "learning_rate": 0.00018131889517218513,
        "epoch": 0.1352,
        "step": 1014
    },
    {
        "loss": 1.1367,
        "grad_norm": 3.47377872467041,
        "learning_rate": 0.00018128224989678827,
        "epoch": 0.13533333333333333,
        "step": 1015
    },
    {
        "loss": 1.8687,
        "grad_norm": 5.445409297943115,
        "learning_rate": 0.00018124557242521535,
        "epoch": 0.13546666666666668,
        "step": 1016
    },
    {
        "loss": 1.844,
        "grad_norm": 4.658004283905029,
        "learning_rate": 0.00018120886277199426,
        "epoch": 0.1356,
        "step": 1017
    },
    {
        "loss": 1.7162,
        "grad_norm": 3.7458925247192383,
        "learning_rate": 0.00018117212095166595,
        "epoch": 0.13573333333333334,
        "step": 1018
    },
    {
        "loss": 2.1876,
        "grad_norm": 2.3151440620422363,
        "learning_rate": 0.00018113534697878394,
        "epoch": 0.13586666666666666,
        "step": 1019
    },
    {
        "loss": 1.9114,
        "grad_norm": 3.7125916481018066,
        "learning_rate": 0.00018109854086791454,
        "epoch": 0.136,
        "step": 1020
    },
    {
        "loss": 2.8387,
        "grad_norm": 2.6469664573669434,
        "learning_rate": 0.00018106170263363676,
        "epoch": 0.13613333333333333,
        "step": 1021
    },
    {
        "loss": 2.2229,
        "grad_norm": 3.0235559940338135,
        "learning_rate": 0.00018102483229054235,
        "epoch": 0.13626666666666667,
        "step": 1022
    },
    {
        "loss": 1.828,
        "grad_norm": 3.171602964401245,
        "learning_rate": 0.0001809879298532358,
        "epoch": 0.1364,
        "step": 1023
    },
    {
        "loss": 0.9183,
        "grad_norm": 2.859971761703491,
        "learning_rate": 0.00018095099533633425,
        "epoch": 0.13653333333333334,
        "step": 1024
    },
    {
        "loss": 2.1509,
        "grad_norm": 2.805845022201538,
        "learning_rate": 0.00018091402875446765,
        "epoch": 0.13666666666666666,
        "step": 1025
    },
    {
        "loss": 2.9361,
        "grad_norm": 2.8542213439941406,
        "learning_rate": 0.0001808770301222785,
        "epoch": 0.1368,
        "step": 1026
    },
    {
        "loss": 2.8369,
        "grad_norm": 2.7656373977661133,
        "learning_rate": 0.00018083999945442219,
        "epoch": 0.13693333333333332,
        "step": 1027
    },
    {
        "loss": 2.2998,
        "grad_norm": 3.2063679695129395,
        "learning_rate": 0.00018080293676556663,
        "epoch": 0.13706666666666667,
        "step": 1028
    },
    {
        "loss": 2.7215,
        "grad_norm": 3.4678025245666504,
        "learning_rate": 0.00018076584207039247,
        "epoch": 0.1372,
        "step": 1029
    },
    {
        "loss": 2.5075,
        "grad_norm": 3.321591854095459,
        "learning_rate": 0.00018072871538359303,
        "epoch": 0.13733333333333334,
        "step": 1030
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.650117039680481,
        "learning_rate": 0.0001806915567198744,
        "epoch": 0.13746666666666665,
        "step": 1031
    },
    {
        "loss": 2.996,
        "grad_norm": 2.9422452449798584,
        "learning_rate": 0.0001806543660939552,
        "epoch": 0.1376,
        "step": 1032
    },
    {
        "loss": 2.9217,
        "grad_norm": 2.224114418029785,
        "learning_rate": 0.00018061714352056676,
        "epoch": 0.13773333333333335,
        "step": 1033
    },
    {
        "loss": 2.0256,
        "grad_norm": 3.9124600887298584,
        "learning_rate": 0.00018057988901445308,
        "epoch": 0.13786666666666667,
        "step": 1034
    },
    {
        "loss": 1.327,
        "grad_norm": 5.117766857147217,
        "learning_rate": 0.00018054260259037078,
        "epoch": 0.138,
        "step": 1035
    },
    {
        "loss": 2.4948,
        "grad_norm": 3.032482147216797,
        "learning_rate": 0.00018050528426308916,
        "epoch": 0.13813333333333333,
        "step": 1036
    },
    {
        "loss": 2.5729,
        "grad_norm": 2.4479143619537354,
        "learning_rate": 0.00018046793404739012,
        "epoch": 0.13826666666666668,
        "step": 1037
    },
    {
        "loss": 2.3352,
        "grad_norm": 2.677497148513794,
        "learning_rate": 0.00018043055195806823,
        "epoch": 0.1384,
        "step": 1038
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.96254563331604,
        "learning_rate": 0.00018039313800993068,
        "epoch": 0.13853333333333334,
        "step": 1039
    },
    {
        "loss": 2.446,
        "grad_norm": 2.6326873302459717,
        "learning_rate": 0.00018035569221779718,
        "epoch": 0.13866666666666666,
        "step": 1040
    },
    {
        "loss": 2.7407,
        "grad_norm": 3.0247676372528076,
        "learning_rate": 0.00018031821459650021,
        "epoch": 0.1388,
        "step": 1041
    },
    {
        "loss": 2.6036,
        "grad_norm": 2.519202470779419,
        "learning_rate": 0.00018028070516088476,
        "epoch": 0.13893333333333333,
        "step": 1042
    },
    {
        "loss": 2.7825,
        "grad_norm": 2.465242385864258,
        "learning_rate": 0.00018024316392580844,
        "epoch": 0.13906666666666667,
        "step": 1043
    },
    {
        "loss": 2.6959,
        "grad_norm": 3.980323314666748,
        "learning_rate": 0.00018020559090614144,
        "epoch": 0.1392,
        "step": 1044
    },
    {
        "loss": 1.1715,
        "grad_norm": 3.79232120513916,
        "learning_rate": 0.00018016798611676662,
        "epoch": 0.13933333333333334,
        "step": 1045
    },
    {
        "loss": 2.2156,
        "grad_norm": 2.7188611030578613,
        "learning_rate": 0.00018013034957257932,
        "epoch": 0.13946666666666666,
        "step": 1046
    },
    {
        "loss": 2.1258,
        "grad_norm": 3.659587860107422,
        "learning_rate": 0.00018009268128848754,
        "epoch": 0.1396,
        "step": 1047
    },
    {
        "loss": 2.6994,
        "grad_norm": 2.37111759185791,
        "learning_rate": 0.00018005498127941175,
        "epoch": 0.13973333333333332,
        "step": 1048
    },
    {
        "loss": 2.9673,
        "grad_norm": 2.531623363494873,
        "learning_rate": 0.00018001724956028515,
        "epoch": 0.13986666666666667,
        "step": 1049
    },
    {
        "loss": 1.9192,
        "grad_norm": 3.6529879570007324,
        "learning_rate": 0.00017997948614605332,
        "epoch": 0.14,
        "step": 1050
    },
    {
        "loss": 2.2199,
        "grad_norm": 3.4090914726257324,
        "learning_rate": 0.0001799416910516745,
        "epoch": 0.14013333333333333,
        "step": 1051
    },
    {
        "loss": 2.7692,
        "grad_norm": 2.219115734100342,
        "learning_rate": 0.00017990386429211946,
        "epoch": 0.14026666666666668,
        "step": 1052
    },
    {
        "loss": 1.6392,
        "grad_norm": 4.501750469207764,
        "learning_rate": 0.00017986600588237148,
        "epoch": 0.1404,
        "step": 1053
    },
    {
        "loss": 2.2069,
        "grad_norm": 6.9213433265686035,
        "learning_rate": 0.00017982811583742647,
        "epoch": 0.14053333333333334,
        "step": 1054
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.9244654178619385,
        "learning_rate": 0.00017979019417229276,
        "epoch": 0.14066666666666666,
        "step": 1055
    },
    {
        "loss": 2.0232,
        "grad_norm": 2.3681681156158447,
        "learning_rate": 0.00017975224090199126,
        "epoch": 0.1408,
        "step": 1056
    },
    {
        "loss": 1.8602,
        "grad_norm": 3.3346402645111084,
        "learning_rate": 0.0001797142560415554,
        "epoch": 0.14093333333333333,
        "step": 1057
    },
    {
        "loss": 2.383,
        "grad_norm": 4.582914352416992,
        "learning_rate": 0.0001796762396060311,
        "epoch": 0.14106666666666667,
        "step": 1058
    },
    {
        "loss": 3.2858,
        "grad_norm": 2.948338747024536,
        "learning_rate": 0.00017963819161047678,
        "epoch": 0.1412,
        "step": 1059
    },
    {
        "loss": 2.3439,
        "grad_norm": 3.057872772216797,
        "learning_rate": 0.00017960011206996345,
        "epoch": 0.14133333333333334,
        "step": 1060
    },
    {
        "loss": 2.7847,
        "grad_norm": 2.958824872970581,
        "learning_rate": 0.00017956200099957445,
        "epoch": 0.14146666666666666,
        "step": 1061
    },
    {
        "loss": 2.8737,
        "grad_norm": 2.7910845279693604,
        "learning_rate": 0.00017952385841440576,
        "epoch": 0.1416,
        "step": 1062
    },
    {
        "loss": 2.5392,
        "grad_norm": 4.390564441680908,
        "learning_rate": 0.0001794856843295658,
        "epoch": 0.14173333333333332,
        "step": 1063
    },
    {
        "loss": 2.9316,
        "grad_norm": 3.5332233905792236,
        "learning_rate": 0.00017944747876017542,
        "epoch": 0.14186666666666667,
        "step": 1064
    },
    {
        "loss": 2.1844,
        "grad_norm": 4.945601940155029,
        "learning_rate": 0.00017940924172136802,
        "epoch": 0.142,
        "step": 1065
    },
    {
        "loss": 2.3911,
        "grad_norm": 2.681037664413452,
        "learning_rate": 0.00017937097322828937,
        "epoch": 0.14213333333333333,
        "step": 1066
    },
    {
        "loss": 2.086,
        "grad_norm": 3.0598273277282715,
        "learning_rate": 0.00017933267329609779,
        "epoch": 0.14226666666666668,
        "step": 1067
    },
    {
        "loss": 1.8216,
        "grad_norm": 4.446377754211426,
        "learning_rate": 0.00017929434193996398,
        "epoch": 0.1424,
        "step": 1068
    },
    {
        "loss": 1.5307,
        "grad_norm": 4.41304349899292,
        "learning_rate": 0.00017925597917507115,
        "epoch": 0.14253333333333335,
        "step": 1069
    },
    {
        "loss": 2.0005,
        "grad_norm": 2.6267013549804688,
        "learning_rate": 0.0001792175850166149,
        "epoch": 0.14266666666666666,
        "step": 1070
    },
    {
        "loss": 2.8349,
        "grad_norm": 3.3689513206481934,
        "learning_rate": 0.0001791791594798033,
        "epoch": 0.1428,
        "step": 1071
    },
    {
        "loss": 3.1102,
        "grad_norm": 2.041536331176758,
        "learning_rate": 0.0001791407025798568,
        "epoch": 0.14293333333333333,
        "step": 1072
    },
    {
        "loss": 1.9603,
        "grad_norm": 3.1280999183654785,
        "learning_rate": 0.00017910221433200843,
        "epoch": 0.14306666666666668,
        "step": 1073
    },
    {
        "loss": 2.0304,
        "grad_norm": 2.419010639190674,
        "learning_rate": 0.00017906369475150335,
        "epoch": 0.1432,
        "step": 1074
    },
    {
        "loss": 3.3701,
        "grad_norm": 2.0846431255340576,
        "learning_rate": 0.00017902514385359942,
        "epoch": 0.14333333333333334,
        "step": 1075
    },
    {
        "loss": 2.7214,
        "grad_norm": 2.1470861434936523,
        "learning_rate": 0.00017898656165356674,
        "epoch": 0.14346666666666666,
        "step": 1076
    },
    {
        "loss": 1.3923,
        "grad_norm": 3.322831153869629,
        "learning_rate": 0.00017894794816668782,
        "epoch": 0.1436,
        "step": 1077
    },
    {
        "loss": 3.187,
        "grad_norm": 3.716503381729126,
        "learning_rate": 0.00017890930340825763,
        "epoch": 0.14373333333333332,
        "step": 1078
    },
    {
        "loss": 2.5634,
        "grad_norm": 2.772718906402588,
        "learning_rate": 0.00017887062739358348,
        "epoch": 0.14386666666666667,
        "step": 1079
    },
    {
        "loss": 2.901,
        "grad_norm": 2.376106023788452,
        "learning_rate": 0.00017883192013798507,
        "epoch": 0.144,
        "step": 1080
    },
    {
        "loss": 2.0843,
        "grad_norm": 3.4657654762268066,
        "learning_rate": 0.00017879318165679449,
        "epoch": 0.14413333333333334,
        "step": 1081
    },
    {
        "loss": 2.4341,
        "grad_norm": 2.928021192550659,
        "learning_rate": 0.0001787544119653562,
        "epoch": 0.14426666666666665,
        "step": 1082
    },
    {
        "loss": 3.0481,
        "grad_norm": 3.150902271270752,
        "learning_rate": 0.00017871561107902695,
        "epoch": 0.1444,
        "step": 1083
    },
    {
        "loss": 1.7295,
        "grad_norm": 2.7896604537963867,
        "learning_rate": 0.00017867677901317594,
        "epoch": 0.14453333333333335,
        "step": 1084
    },
    {
        "loss": 3.042,
        "grad_norm": 3.777111530303955,
        "learning_rate": 0.00017863791578318468,
        "epoch": 0.14466666666666667,
        "step": 1085
    },
    {
        "loss": 2.1037,
        "grad_norm": 4.32567834854126,
        "learning_rate": 0.00017859902140444702,
        "epoch": 0.1448,
        "step": 1086
    },
    {
        "loss": 2.3208,
        "grad_norm": 5.5559282302856445,
        "learning_rate": 0.00017856009589236918,
        "epoch": 0.14493333333333333,
        "step": 1087
    },
    {
        "loss": 2.3246,
        "grad_norm": 2.4885752201080322,
        "learning_rate": 0.00017852113926236966,
        "epoch": 0.14506666666666668,
        "step": 1088
    },
    {
        "loss": 2.5964,
        "grad_norm": 3.5426018238067627,
        "learning_rate": 0.0001784821515298793,
        "epoch": 0.1452,
        "step": 1089
    },
    {
        "loss": 1.9257,
        "grad_norm": 2.988670825958252,
        "learning_rate": 0.00017844313271034133,
        "epoch": 0.14533333333333334,
        "step": 1090
    },
    {
        "loss": 2.016,
        "grad_norm": 2.297370195388794,
        "learning_rate": 0.0001784040828192112,
        "epoch": 0.14546666666666666,
        "step": 1091
    },
    {
        "loss": 2.9753,
        "grad_norm": 3.039797306060791,
        "learning_rate": 0.0001783650018719567,
        "epoch": 0.1456,
        "step": 1092
    },
    {
        "loss": 2.6546,
        "grad_norm": 2.7988317012786865,
        "learning_rate": 0.00017832588988405795,
        "epoch": 0.14573333333333333,
        "step": 1093
    },
    {
        "loss": 0.7672,
        "grad_norm": 2.8487932682037354,
        "learning_rate": 0.00017828674687100733,
        "epoch": 0.14586666666666667,
        "step": 1094
    },
    {
        "loss": 2.9019,
        "grad_norm": 1.5375896692276,
        "learning_rate": 0.0001782475728483095,
        "epoch": 0.146,
        "step": 1095
    },
    {
        "loss": 2.3373,
        "grad_norm": 2.3179574012756348,
        "learning_rate": 0.00017820836783148146,
        "epoch": 0.14613333333333334,
        "step": 1096
    },
    {
        "loss": 3.4073,
        "grad_norm": 2.4761879444122314,
        "learning_rate": 0.0001781691318360524,
        "epoch": 0.14626666666666666,
        "step": 1097
    },
    {
        "loss": 2.7309,
        "grad_norm": 3.8343188762664795,
        "learning_rate": 0.00017812986487756392,
        "epoch": 0.1464,
        "step": 1098
    },
    {
        "loss": 2.556,
        "grad_norm": 5.178828239440918,
        "learning_rate": 0.0001780905669715697,
        "epoch": 0.14653333333333332,
        "step": 1099
    },
    {
        "loss": 1.8941,
        "grad_norm": 2.750558376312256,
        "learning_rate": 0.00017805123813363584,
        "epoch": 0.14666666666666667,
        "step": 1100
    },
    {
        "loss": 2.4486,
        "grad_norm": 2.6626410484313965,
        "learning_rate": 0.00017801187837934056,
        "epoch": 0.1468,
        "step": 1101
    },
    {
        "loss": 1.3883,
        "grad_norm": 4.488876819610596,
        "learning_rate": 0.00017797248772427447,
        "epoch": 0.14693333333333333,
        "step": 1102
    },
    {
        "loss": 2.7548,
        "grad_norm": 4.071834564208984,
        "learning_rate": 0.0001779330661840403,
        "epoch": 0.14706666666666668,
        "step": 1103
    },
    {
        "loss": 2.4686,
        "grad_norm": 3.0601117610931396,
        "learning_rate": 0.00017789361377425306,
        "epoch": 0.1472,
        "step": 1104
    },
    {
        "loss": 1.457,
        "grad_norm": 4.867966175079346,
        "learning_rate": 0.00017785413051054,
        "epoch": 0.14733333333333334,
        "step": 1105
    },
    {
        "loss": 2.5703,
        "grad_norm": 3.694626569747925,
        "learning_rate": 0.00017781461640854057,
        "epoch": 0.14746666666666666,
        "step": 1106
    },
    {
        "loss": 2.9708,
        "grad_norm": 2.2550137042999268,
        "learning_rate": 0.0001777750714839064,
        "epoch": 0.1476,
        "step": 1107
    },
    {
        "loss": 1.832,
        "grad_norm": 3.4893715381622314,
        "learning_rate": 0.00017773549575230145,
        "epoch": 0.14773333333333333,
        "step": 1108
    },
    {
        "loss": 2.553,
        "grad_norm": 3.266202449798584,
        "learning_rate": 0.0001776958892294017,
        "epoch": 0.14786666666666667,
        "step": 1109
    },
    {
        "loss": 2.6737,
        "grad_norm": 3.009777307510376,
        "learning_rate": 0.00017765625193089552,
        "epoch": 0.148,
        "step": 1110
    },
    {
        "loss": 1.7056,
        "grad_norm": 3.960491418838501,
        "learning_rate": 0.00017761658387248334,
        "epoch": 0.14813333333333334,
        "step": 1111
    },
    {
        "loss": 2.4836,
        "grad_norm": 2.2875449657440186,
        "learning_rate": 0.00017757688506987783,
        "epoch": 0.14826666666666666,
        "step": 1112
    },
    {
        "loss": 3.291,
        "grad_norm": 1.9479846954345703,
        "learning_rate": 0.00017753715553880378,
        "epoch": 0.1484,
        "step": 1113
    },
    {
        "loss": 2.4531,
        "grad_norm": 2.4714112281799316,
        "learning_rate": 0.00017749739529499822,
        "epoch": 0.14853333333333332,
        "step": 1114
    },
    {
        "loss": 2.2346,
        "grad_norm": 2.0938034057617188,
        "learning_rate": 0.00017745760435421032,
        "epoch": 0.14866666666666667,
        "step": 1115
    },
    {
        "loss": 2.1107,
        "grad_norm": 2.6455447673797607,
        "learning_rate": 0.00017741778273220145,
        "epoch": 0.1488,
        "step": 1116
    },
    {
        "loss": 2.1174,
        "grad_norm": 3.7399423122406006,
        "learning_rate": 0.00017737793044474498,
        "epoch": 0.14893333333333333,
        "step": 1117
    },
    {
        "loss": 2.5244,
        "grad_norm": 3.712165594100952,
        "learning_rate": 0.00017733804750762663,
        "epoch": 0.14906666666666665,
        "step": 1118
    },
    {
        "loss": 2.4822,
        "grad_norm": 4.016879081726074,
        "learning_rate": 0.00017729813393664413,
        "epoch": 0.1492,
        "step": 1119
    },
    {
        "loss": 0.6687,
        "grad_norm": 4.231316089630127,
        "learning_rate": 0.00017725818974760743,
        "epoch": 0.14933333333333335,
        "step": 1120
    },
    {
        "loss": 2.7268,
        "grad_norm": 3.610379457473755,
        "learning_rate": 0.00017721821495633846,
        "epoch": 0.14946666666666666,
        "step": 1121
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.4918437004089355,
        "learning_rate": 0.00017717820957867145,
        "epoch": 0.1496,
        "step": 1122
    },
    {
        "loss": 1.6353,
        "grad_norm": 4.467937469482422,
        "learning_rate": 0.00017713817363045265,
        "epoch": 0.14973333333333333,
        "step": 1123
    },
    {
        "loss": 2.4184,
        "grad_norm": 2.356236457824707,
        "learning_rate": 0.00017709810712754047,
        "epoch": 0.14986666666666668,
        "step": 1124
    },
    {
        "loss": 2.195,
        "grad_norm": 4.95533561706543,
        "learning_rate": 0.0001770580100858053,
        "epoch": 0.15,
        "step": 1125
    },
    {
        "loss": 2.8058,
        "grad_norm": 3.895402669906616,
        "learning_rate": 0.0001770178825211298,
        "epoch": 0.15013333333333334,
        "step": 1126
    },
    {
        "loss": 2.4725,
        "grad_norm": 1.3674558401107788,
        "learning_rate": 0.0001769777244494086,
        "epoch": 0.15026666666666666,
        "step": 1127
    },
    {
        "loss": 2.2237,
        "grad_norm": 4.111279487609863,
        "learning_rate": 0.00017693753588654844,
        "epoch": 0.1504,
        "step": 1128
    },
    {
        "loss": 2.6291,
        "grad_norm": 2.2505688667297363,
        "learning_rate": 0.00017689731684846817,
        "epoch": 0.15053333333333332,
        "step": 1129
    },
    {
        "loss": 1.6618,
        "grad_norm": 2.111973285675049,
        "learning_rate": 0.00017685706735109865,
        "epoch": 0.15066666666666667,
        "step": 1130
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.21836519241333,
        "learning_rate": 0.00017681678741038288,
        "epoch": 0.1508,
        "step": 1131
    },
    {
        "loss": 2.1848,
        "grad_norm": 3.8112716674804688,
        "learning_rate": 0.00017677647704227587,
        "epoch": 0.15093333333333334,
        "step": 1132
    },
    {
        "loss": 2.0757,
        "grad_norm": 4.833122730255127,
        "learning_rate": 0.0001767361362627447,
        "epoch": 0.15106666666666665,
        "step": 1133
    },
    {
        "loss": 1.8259,
        "grad_norm": 2.50565767288208,
        "learning_rate": 0.0001766957650877685,
        "epoch": 0.1512,
        "step": 1134
    },
    {
        "loss": 1.2977,
        "grad_norm": 3.633654832839966,
        "learning_rate": 0.00017665536353333837,
        "epoch": 0.15133333333333332,
        "step": 1135
    },
    {
        "loss": 1.7023,
        "grad_norm": 5.162441253662109,
        "learning_rate": 0.00017661493161545756,
        "epoch": 0.15146666666666667,
        "step": 1136
    },
    {
        "loss": 1.3918,
        "grad_norm": 3.81831693649292,
        "learning_rate": 0.0001765744693501413,
        "epoch": 0.1516,
        "step": 1137
    },
    {
        "loss": 1.0397,
        "grad_norm": 2.8778393268585205,
        "learning_rate": 0.0001765339767534168,
        "epoch": 0.15173333333333333,
        "step": 1138
    },
    {
        "loss": 3.1193,
        "grad_norm": 2.7718260288238525,
        "learning_rate": 0.00017649345384132333,
        "epoch": 0.15186666666666668,
        "step": 1139
    },
    {
        "loss": 2.2572,
        "grad_norm": 2.933194398880005,
        "learning_rate": 0.00017645290062991213,
        "epoch": 0.152,
        "step": 1140
    },
    {
        "loss": 2.8449,
        "grad_norm": 2.9439542293548584,
        "learning_rate": 0.00017641231713524648,
        "epoch": 0.15213333333333334,
        "step": 1141
    },
    {
        "loss": 1.9073,
        "grad_norm": 2.4462623596191406,
        "learning_rate": 0.0001763717033734017,
        "epoch": 0.15226666666666666,
        "step": 1142
    },
    {
        "loss": 2.2177,
        "grad_norm": 3.399186372756958,
        "learning_rate": 0.00017633105936046492,
        "epoch": 0.1524,
        "step": 1143
    },
    {
        "loss": 2.4889,
        "grad_norm": 2.3745508193969727,
        "learning_rate": 0.00017629038511253546,
        "epoch": 0.15253333333333333,
        "step": 1144
    },
    {
        "loss": 2.5648,
        "grad_norm": 1.6705670356750488,
        "learning_rate": 0.0001762496806457245,
        "epoch": 0.15266666666666667,
        "step": 1145
    },
    {
        "loss": 2.8803,
        "grad_norm": 3.3231656551361084,
        "learning_rate": 0.00017620894597615523,
        "epoch": 0.1528,
        "step": 1146
    },
    {
        "loss": 2.5014,
        "grad_norm": 3.4263956546783447,
        "learning_rate": 0.0001761681811199628,
        "epoch": 0.15293333333333334,
        "step": 1147
    },
    {
        "loss": 3.0032,
        "grad_norm": 2.420464038848877,
        "learning_rate": 0.00017612738609329423,
        "epoch": 0.15306666666666666,
        "step": 1148
    },
    {
        "loss": 1.9059,
        "grad_norm": 3.819711923599243,
        "learning_rate": 0.0001760865609123087,
        "epoch": 0.1532,
        "step": 1149
    },
    {
        "loss": 2.2558,
        "grad_norm": 3.0156564712524414,
        "learning_rate": 0.0001760457055931771,
        "epoch": 0.15333333333333332,
        "step": 1150
    },
    {
        "loss": 2.7239,
        "grad_norm": 2.743175983428955,
        "learning_rate": 0.0001760048201520824,
        "epoch": 0.15346666666666667,
        "step": 1151
    },
    {
        "loss": 1.7146,
        "grad_norm": 1.94727623462677,
        "learning_rate": 0.00017596390460521946,
        "epoch": 0.1536,
        "step": 1152
    },
    {
        "loss": 2.5955,
        "grad_norm": 4.0714111328125,
        "learning_rate": 0.00017592295896879504,
        "epoch": 0.15373333333333333,
        "step": 1153
    },
    {
        "loss": 2.9251,
        "grad_norm": 1.9676564931869507,
        "learning_rate": 0.0001758819832590279,
        "epoch": 0.15386666666666668,
        "step": 1154
    },
    {
        "loss": 1.6002,
        "grad_norm": 3.5763864517211914,
        "learning_rate": 0.00017584097749214864,
        "epoch": 0.154,
        "step": 1155
    },
    {
        "loss": 2.3053,
        "grad_norm": 2.597928524017334,
        "learning_rate": 0.00017579994168439976,
        "epoch": 0.15413333333333334,
        "step": 1156
    },
    {
        "loss": 2.4729,
        "grad_norm": 2.4973254203796387,
        "learning_rate": 0.00017575887585203568,
        "epoch": 0.15426666666666666,
        "step": 1157
    },
    {
        "loss": 2.4339,
        "grad_norm": 2.253455400466919,
        "learning_rate": 0.00017571778001132274,
        "epoch": 0.1544,
        "step": 1158
    },
    {
        "loss": 3.0297,
        "grad_norm": 3.3018219470977783,
        "learning_rate": 0.00017567665417853916,
        "epoch": 0.15453333333333333,
        "step": 1159
    },
    {
        "loss": 3.434,
        "grad_norm": 11.330099105834961,
        "learning_rate": 0.00017563549836997498,
        "epoch": 0.15466666666666667,
        "step": 1160
    },
    {
        "loss": 2.3097,
        "grad_norm": 2.910788059234619,
        "learning_rate": 0.00017559431260193216,
        "epoch": 0.1548,
        "step": 1161
    },
    {
        "loss": 2.5945,
        "grad_norm": 3.1911911964416504,
        "learning_rate": 0.00017555309689072453,
        "epoch": 0.15493333333333334,
        "step": 1162
    },
    {
        "loss": 2.5268,
        "grad_norm": 2.5217909812927246,
        "learning_rate": 0.00017551185125267782,
        "epoch": 0.15506666666666666,
        "step": 1163
    },
    {
        "loss": 2.5018,
        "grad_norm": 2.9529850482940674,
        "learning_rate": 0.0001754705757041295,
        "epoch": 0.1552,
        "step": 1164
    },
    {
        "loss": 2.4511,
        "grad_norm": 2.4876410961151123,
        "learning_rate": 0.000175429270261429,
        "epoch": 0.15533333333333332,
        "step": 1165
    },
    {
        "loss": 2.0419,
        "grad_norm": 2.062513589859009,
        "learning_rate": 0.00017538793494093751,
        "epoch": 0.15546666666666667,
        "step": 1166
    },
    {
        "loss": 1.5838,
        "grad_norm": 2.671351909637451,
        "learning_rate": 0.00017534656975902815,
        "epoch": 0.1556,
        "step": 1167
    },
    {
        "loss": 0.8972,
        "grad_norm": 3.2790677547454834,
        "learning_rate": 0.00017530517473208574,
        "epoch": 0.15573333333333333,
        "step": 1168
    },
    {
        "loss": 2.6549,
        "grad_norm": 2.942859649658203,
        "learning_rate": 0.00017526374987650705,
        "epoch": 0.15586666666666665,
        "step": 1169
    },
    {
        "loss": 2.5807,
        "grad_norm": 4.2528557777404785,
        "learning_rate": 0.00017522229520870055,
        "epoch": 0.156,
        "step": 1170
    },
    {
        "loss": 3.2423,
        "grad_norm": 2.5078489780426025,
        "learning_rate": 0.0001751808107450866,
        "epoch": 0.15613333333333335,
        "step": 1171
    },
    {
        "loss": 2.5807,
        "grad_norm": 3.0806849002838135,
        "learning_rate": 0.00017513929650209736,
        "epoch": 0.15626666666666666,
        "step": 1172
    },
    {
        "loss": 2.7906,
        "grad_norm": 3.3902554512023926,
        "learning_rate": 0.00017509775249617676,
        "epoch": 0.1564,
        "step": 1173
    },
    {
        "loss": 2.4246,
        "grad_norm": 2.5641307830810547,
        "learning_rate": 0.00017505617874378045,
        "epoch": 0.15653333333333333,
        "step": 1174
    },
    {
        "loss": 2.0469,
        "grad_norm": 6.227827548980713,
        "learning_rate": 0.00017501457526137605,
        "epoch": 0.15666666666666668,
        "step": 1175
    },
    {
        "loss": 2.2094,
        "grad_norm": 2.84236478805542,
        "learning_rate": 0.00017497294206544276,
        "epoch": 0.1568,
        "step": 1176
    },
    {
        "loss": 2.9644,
        "grad_norm": 2.390976905822754,
        "learning_rate": 0.00017493127917247168,
        "epoch": 0.15693333333333334,
        "step": 1177
    },
    {
        "loss": 2.1295,
        "grad_norm": 3.455404281616211,
        "learning_rate": 0.00017488958659896554,
        "epoch": 0.15706666666666666,
        "step": 1178
    },
    {
        "loss": 2.5787,
        "grad_norm": 4.152647018432617,
        "learning_rate": 0.00017484786436143903,
        "epoch": 0.1572,
        "step": 1179
    },
    {
        "loss": 2.8953,
        "grad_norm": 2.795591354370117,
        "learning_rate": 0.00017480611247641838,
        "epoch": 0.15733333333333333,
        "step": 1180
    },
    {
        "loss": 1.6796,
        "grad_norm": 4.863767147064209,
        "learning_rate": 0.00017476433096044167,
        "epoch": 0.15746666666666667,
        "step": 1181
    },
    {
        "loss": 2.8001,
        "grad_norm": 1.0964239835739136,
        "learning_rate": 0.00017472251983005873,
        "epoch": 0.1576,
        "step": 1182
    },
    {
        "loss": 2.5586,
        "grad_norm": 5.012556076049805,
        "learning_rate": 0.00017468067910183106,
        "epoch": 0.15773333333333334,
        "step": 1183
    },
    {
        "loss": 1.625,
        "grad_norm": 3.286902904510498,
        "learning_rate": 0.00017463880879233195,
        "epoch": 0.15786666666666666,
        "step": 1184
    },
    {
        "loss": 2.6133,
        "grad_norm": 2.0149660110473633,
        "learning_rate": 0.00017459690891814632,
        "epoch": 0.158,
        "step": 1185
    },
    {
        "loss": 2.3547,
        "grad_norm": 2.717839479446411,
        "learning_rate": 0.0001745549794958709,
        "epoch": 0.15813333333333332,
        "step": 1186
    },
    {
        "loss": 3.1826,
        "grad_norm": 2.8644440174102783,
        "learning_rate": 0.0001745130205421141,
        "epoch": 0.15826666666666667,
        "step": 1187
    },
    {
        "loss": 3.2068,
        "grad_norm": 1.989777684211731,
        "learning_rate": 0.00017447103207349592,
        "epoch": 0.1584,
        "step": 1188
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.6589672565460205,
        "learning_rate": 0.0001744290141066482,
        "epoch": 0.15853333333333333,
        "step": 1189
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.863523244857788,
        "learning_rate": 0.0001743869666582144,
        "epoch": 0.15866666666666668,
        "step": 1190
    },
    {
        "loss": 2.8961,
        "grad_norm": 2.0174036026000977,
        "learning_rate": 0.0001743448897448496,
        "epoch": 0.1588,
        "step": 1191
    },
    {
        "loss": 2.5939,
        "grad_norm": 3.7208242416381836,
        "learning_rate": 0.00017430278338322066,
        "epoch": 0.15893333333333334,
        "step": 1192
    },
    {
        "loss": 1.441,
        "grad_norm": 2.4739601612091064,
        "learning_rate": 0.00017426064759000606,
        "epoch": 0.15906666666666666,
        "step": 1193
    },
    {
        "loss": 2.8277,
        "grad_norm": 2.7983129024505615,
        "learning_rate": 0.0001742184823818959,
        "epoch": 0.1592,
        "step": 1194
    },
    {
        "loss": 2.1839,
        "grad_norm": 2.765179395675659,
        "learning_rate": 0.00017417628777559196,
        "epoch": 0.15933333333333333,
        "step": 1195
    },
    {
        "loss": 2.0459,
        "grad_norm": 3.2932848930358887,
        "learning_rate": 0.00017413406378780771,
        "epoch": 0.15946666666666667,
        "step": 1196
    },
    {
        "loss": 2.1555,
        "grad_norm": 3.3329334259033203,
        "learning_rate": 0.00017409181043526815,
        "epoch": 0.1596,
        "step": 1197
    },
    {
        "loss": 2.9253,
        "grad_norm": 3.6137609481811523,
        "learning_rate": 0.00017404952773471006,
        "epoch": 0.15973333333333334,
        "step": 1198
    },
    {
        "loss": 1.5852,
        "grad_norm": 3.959944248199463,
        "learning_rate": 0.0001740072157028817,
        "epoch": 0.15986666666666666,
        "step": 1199
    },
    {
        "loss": 1.5143,
        "grad_norm": 3.4440886974334717,
        "learning_rate": 0.000173964874356543,
        "epoch": 0.16,
        "step": 1200
    },
    {
        "loss": 2.6347,
        "grad_norm": 2.8507392406463623,
        "learning_rate": 0.00017392250371246558,
        "epoch": 0.16013333333333332,
        "step": 1201
    },
    {
        "loss": 1.9025,
        "grad_norm": 4.597078800201416,
        "learning_rate": 0.00017388010378743254,
        "epoch": 0.16026666666666667,
        "step": 1202
    },
    {
        "loss": 3.0771,
        "grad_norm": 2.4174675941467285,
        "learning_rate": 0.00017383767459823864,
        "epoch": 0.1604,
        "step": 1203
    },
    {
        "loss": 2.6275,
        "grad_norm": 2.759413957595825,
        "learning_rate": 0.00017379521616169024,
        "epoch": 0.16053333333333333,
        "step": 1204
    },
    {
        "loss": 2.3612,
        "grad_norm": 2.302870035171509,
        "learning_rate": 0.00017375272849460527,
        "epoch": 0.16066666666666668,
        "step": 1205
    },
    {
        "loss": 2.4326,
        "grad_norm": 2.7075109481811523,
        "learning_rate": 0.00017371021161381327,
        "epoch": 0.1608,
        "step": 1206
    },
    {
        "loss": 2.3946,
        "grad_norm": 3.705066680908203,
        "learning_rate": 0.00017366766553615523,
        "epoch": 0.16093333333333334,
        "step": 1207
    },
    {
        "loss": 3.0406,
        "grad_norm": 1.9940264225006104,
        "learning_rate": 0.00017362509027848389,
        "epoch": 0.16106666666666666,
        "step": 1208
    },
    {
        "loss": 1.5196,
        "grad_norm": 2.544297933578491,
        "learning_rate": 0.00017358248585766338,
        "epoch": 0.1612,
        "step": 1209
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.866670846939087,
        "learning_rate": 0.00017353985229056956,
        "epoch": 0.16133333333333333,
        "step": 1210
    },
    {
        "loss": 1.9018,
        "grad_norm": 4.861367702484131,
        "learning_rate": 0.00017349718959408964,
        "epoch": 0.16146666666666668,
        "step": 1211
    },
    {
        "loss": 2.6572,
        "grad_norm": 4.990242958068848,
        "learning_rate": 0.00017345449778512247,
        "epoch": 0.1616,
        "step": 1212
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.0792717933654785,
        "learning_rate": 0.00017341177688057845,
        "epoch": 0.16173333333333334,
        "step": 1213
    },
    {
        "loss": 2.8298,
        "grad_norm": 2.1474497318267822,
        "learning_rate": 0.00017336902689737944,
        "epoch": 0.16186666666666666,
        "step": 1214
    },
    {
        "loss": 0.9218,
        "grad_norm": 3.1707804203033447,
        "learning_rate": 0.0001733262478524589,
        "epoch": 0.162,
        "step": 1215
    },
    {
        "loss": 1.9806,
        "grad_norm": 3.5903878211975098,
        "learning_rate": 0.0001732834397627617,
        "epoch": 0.16213333333333332,
        "step": 1216
    },
    {
        "loss": 2.415,
        "grad_norm": 3.452826976776123,
        "learning_rate": 0.00017324060264524435,
        "epoch": 0.16226666666666667,
        "step": 1217
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.696615695953369,
        "learning_rate": 0.0001731977365168747,
        "epoch": 0.1624,
        "step": 1218
    },
    {
        "loss": 3.0847,
        "grad_norm": 3.818423271179199,
        "learning_rate": 0.00017315484139463223,
        "epoch": 0.16253333333333334,
        "step": 1219
    },
    {
        "loss": 2.4286,
        "grad_norm": 4.873373985290527,
        "learning_rate": 0.0001731119172955078,
        "epoch": 0.16266666666666665,
        "step": 1220
    },
    {
        "loss": 2.2607,
        "grad_norm": 3.1609652042388916,
        "learning_rate": 0.0001730689642365038,
        "epoch": 0.1628,
        "step": 1221
    },
    {
        "loss": 2.6627,
        "grad_norm": 4.445690631866455,
        "learning_rate": 0.0001730259822346341,
        "epoch": 0.16293333333333335,
        "step": 1222
    },
    {
        "loss": 2.7447,
        "grad_norm": 2.952794075012207,
        "learning_rate": 0.000172982971306924,
        "epoch": 0.16306666666666667,
        "step": 1223
    },
    {
        "loss": 1.9438,
        "grad_norm": 3.5172438621520996,
        "learning_rate": 0.0001729399314704103,
        "epoch": 0.1632,
        "step": 1224
    },
    {
        "loss": 2.6398,
        "grad_norm": 3.283071279525757,
        "learning_rate": 0.00017289686274214118,
        "epoch": 0.16333333333333333,
        "step": 1225
    },
    {
        "loss": 3.0562,
        "grad_norm": 2.284973382949829,
        "learning_rate": 0.00017285376513917632,
        "epoch": 0.16346666666666668,
        "step": 1226
    },
    {
        "loss": 1.5092,
        "grad_norm": 4.363428592681885,
        "learning_rate": 0.0001728106386785869,
        "epoch": 0.1636,
        "step": 1227
    },
    {
        "loss": 2.1325,
        "grad_norm": 3.187467336654663,
        "learning_rate": 0.00017276748337745535,
        "epoch": 0.16373333333333334,
        "step": 1228
    },
    {
        "loss": 2.7945,
        "grad_norm": 2.421143054962158,
        "learning_rate": 0.00017272429925287573,
        "epoch": 0.16386666666666666,
        "step": 1229
    },
    {
        "loss": 2.7418,
        "grad_norm": 3.6859676837921143,
        "learning_rate": 0.00017268108632195335,
        "epoch": 0.164,
        "step": 1230
    },
    {
        "loss": 1.2395,
        "grad_norm": 4.283824443817139,
        "learning_rate": 0.00017263784460180503,
        "epoch": 0.16413333333333333,
        "step": 1231
    },
    {
        "loss": 2.6628,
        "grad_norm": 3.737959146499634,
        "learning_rate": 0.00017259457410955892,
        "epoch": 0.16426666666666667,
        "step": 1232
    },
    {
        "loss": 2.8487,
        "grad_norm": 3.4388620853424072,
        "learning_rate": 0.0001725512748623547,
        "epoch": 0.1644,
        "step": 1233
    },
    {
        "loss": 2.1632,
        "grad_norm": 3.396574020385742,
        "learning_rate": 0.0001725079468773433,
        "epoch": 0.16453333333333334,
        "step": 1234
    },
    {
        "loss": 2.2439,
        "grad_norm": 2.837824583053589,
        "learning_rate": 0.00017246459017168704,
        "epoch": 0.16466666666666666,
        "step": 1235
    },
    {
        "loss": 1.9814,
        "grad_norm": 2.8872244358062744,
        "learning_rate": 0.00017242120476255969,
        "epoch": 0.1648,
        "step": 1236
    },
    {
        "loss": 2.4377,
        "grad_norm": 3.439973831176758,
        "learning_rate": 0.0001723777906671464,
        "epoch": 0.16493333333333332,
        "step": 1237
    },
    {
        "loss": 1.7923,
        "grad_norm": 3.3474812507629395,
        "learning_rate": 0.00017233434790264358,
        "epoch": 0.16506666666666667,
        "step": 1238
    },
    {
        "loss": 2.3787,
        "grad_norm": 3.6961019039154053,
        "learning_rate": 0.0001722908764862591,
        "epoch": 0.1652,
        "step": 1239
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.843041181564331,
        "learning_rate": 0.0001722473764352121,
        "epoch": 0.16533333333333333,
        "step": 1240
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.667086124420166,
        "learning_rate": 0.0001722038477667331,
        "epoch": 0.16546666666666668,
        "step": 1241
    },
    {
        "loss": 2.3258,
        "grad_norm": 3.07389497756958,
        "learning_rate": 0.000172160290498064,
        "epoch": 0.1656,
        "step": 1242
    },
    {
        "loss": 1.9091,
        "grad_norm": 3.598341703414917,
        "learning_rate": 0.00017211670464645798,
        "epoch": 0.16573333333333334,
        "step": 1243
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.8632218837738037,
        "learning_rate": 0.00017207309022917948,
        "epoch": 0.16586666666666666,
        "step": 1244
    },
    {
        "loss": 2.7849,
        "grad_norm": 2.47002911567688,
        "learning_rate": 0.00017202944726350437,
        "epoch": 0.166,
        "step": 1245
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.9701805114746094,
        "learning_rate": 0.00017198577576671976,
        "epoch": 0.16613333333333333,
        "step": 1246
    },
    {
        "loss": 2.5986,
        "grad_norm": 2.8797850608825684,
        "learning_rate": 0.00017194207575612407,
        "epoch": 0.16626666666666667,
        "step": 1247
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.805433511734009,
        "learning_rate": 0.00017189834724902706,
        "epoch": 0.1664,
        "step": 1248
    },
    {
        "loss": 2.0309,
        "grad_norm": 3.4594712257385254,
        "learning_rate": 0.00017185459026274971,
        "epoch": 0.16653333333333334,
        "step": 1249
    },
    {
        "loss": 2.3954,
        "grad_norm": 3.363314390182495,
        "learning_rate": 0.0001718108048146243,
        "epoch": 0.16666666666666666,
        "step": 1250
    },
    {
        "loss": 2.4103,
        "grad_norm": 5.38358736038208,
        "learning_rate": 0.00017176699092199442,
        "epoch": 0.1668,
        "step": 1251
    },
    {
        "loss": 2.1377,
        "grad_norm": 3.268744707107544,
        "learning_rate": 0.00017172314860221493,
        "epoch": 0.16693333333333332,
        "step": 1252
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.1726953983306885,
        "learning_rate": 0.00017167927787265183,
        "epoch": 0.16706666666666667,
        "step": 1253
    },
    {
        "loss": 2.5172,
        "grad_norm": 3.1456782817840576,
        "learning_rate": 0.00017163537875068257,
        "epoch": 0.1672,
        "step": 1254
    },
    {
        "loss": 2.1784,
        "grad_norm": 3.3149454593658447,
        "learning_rate": 0.00017159145125369567,
        "epoch": 0.16733333333333333,
        "step": 1255
    },
    {
        "loss": 2.168,
        "grad_norm": 3.378872871398926,
        "learning_rate": 0.000171547495399091,
        "epoch": 0.16746666666666668,
        "step": 1256
    },
    {
        "loss": 2.4225,
        "grad_norm": 3.5713183879852295,
        "learning_rate": 0.0001715035112042796,
        "epoch": 0.1676,
        "step": 1257
    },
    {
        "loss": 2.4665,
        "grad_norm": 4.50769567489624,
        "learning_rate": 0.0001714594986866838,
        "epoch": 0.16773333333333335,
        "step": 1258
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.905740976333618,
        "learning_rate": 0.00017141545786373706,
        "epoch": 0.16786666666666666,
        "step": 1259
    },
    {
        "loss": 2.5468,
        "grad_norm": 2.1637821197509766,
        "learning_rate": 0.0001713713887528841,
        "epoch": 0.168,
        "step": 1260
    },
    {
        "loss": 2.4822,
        "grad_norm": 4.050701141357422,
        "learning_rate": 0.00017132729137158087,
        "epoch": 0.16813333333333333,
        "step": 1261
    },
    {
        "loss": 2.5499,
        "grad_norm": 3.791327953338623,
        "learning_rate": 0.0001712831657372945,
        "epoch": 0.16826666666666668,
        "step": 1262
    },
    {
        "loss": 1.9024,
        "grad_norm": 3.4591903686523438,
        "learning_rate": 0.0001712390118675033,
        "epoch": 0.1684,
        "step": 1263
    },
    {
        "loss": 1.6494,
        "grad_norm": 3.480893135070801,
        "learning_rate": 0.00017119482977969672,
        "epoch": 0.16853333333333334,
        "step": 1264
    },
    {
        "loss": 2.8737,
        "grad_norm": 1.7382569313049316,
        "learning_rate": 0.0001711506194913755,
        "epoch": 0.16866666666666666,
        "step": 1265
    },
    {
        "loss": 2.5118,
        "grad_norm": 2.116377353668213,
        "learning_rate": 0.00017110638102005145,
        "epoch": 0.1688,
        "step": 1266
    },
    {
        "loss": 2.445,
        "grad_norm": 1.6337759494781494,
        "learning_rate": 0.0001710621143832476,
        "epoch": 0.16893333333333332,
        "step": 1267
    },
    {
        "loss": 2.9734,
        "grad_norm": 3.273017406463623,
        "learning_rate": 0.00017101781959849807,
        "epoch": 0.16906666666666667,
        "step": 1268
    },
    {
        "loss": 3.1089,
        "grad_norm": 2.4834015369415283,
        "learning_rate": 0.00017097349668334825,
        "epoch": 0.1692,
        "step": 1269
    },
    {
        "loss": 2.8598,
        "grad_norm": 2.8179094791412354,
        "learning_rate": 0.00017092914565535456,
        "epoch": 0.16933333333333334,
        "step": 1270
    },
    {
        "loss": 1.2399,
        "grad_norm": 3.062676429748535,
        "learning_rate": 0.00017088476653208455,
        "epoch": 0.16946666666666665,
        "step": 1271
    },
    {
        "loss": 2.8149,
        "grad_norm": 3.5837466716766357,
        "learning_rate": 0.000170840359331117,
        "epoch": 0.1696,
        "step": 1272
    },
    {
        "loss": 2.6247,
        "grad_norm": 3.160027027130127,
        "learning_rate": 0.00017079592407004177,
        "epoch": 0.16973333333333335,
        "step": 1273
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.0964717864990234,
        "learning_rate": 0.00017075146076645976,
        "epoch": 0.16986666666666667,
        "step": 1274
    },
    {
        "loss": 0.9989,
        "grad_norm": 4.871282577514648,
        "learning_rate": 0.00017070696943798306,
        "epoch": 0.17,
        "step": 1275
    },
    {
        "loss": 2.8158,
        "grad_norm": 2.0132994651794434,
        "learning_rate": 0.00017066245010223483,
        "epoch": 0.17013333333333333,
        "step": 1276
    },
    {
        "loss": 2.3727,
        "grad_norm": 2.1061558723449707,
        "learning_rate": 0.00017061790277684934,
        "epoch": 0.17026666666666668,
        "step": 1277
    },
    {
        "loss": 2.3128,
        "grad_norm": 4.446386337280273,
        "learning_rate": 0.00017057332747947193,
        "epoch": 0.1704,
        "step": 1278
    },
    {
        "loss": 1.8664,
        "grad_norm": 3.432373285293579,
        "learning_rate": 0.00017052872422775907,
        "epoch": 0.17053333333333334,
        "step": 1279
    },
    {
        "loss": 1.6274,
        "grad_norm": 4.489452362060547,
        "learning_rate": 0.00017048409303937817,
        "epoch": 0.17066666666666666,
        "step": 1280
    },
    {
        "loss": 1.7531,
        "grad_norm": 3.1116995811462402,
        "learning_rate": 0.0001704394339320079,
        "epoch": 0.1708,
        "step": 1281
    },
    {
        "loss": 2.1387,
        "grad_norm": 2.363837480545044,
        "learning_rate": 0.00017039474692333778,
        "epoch": 0.17093333333333333,
        "step": 1282
    },
    {
        "loss": 3.3525,
        "grad_norm": 2.3247859477996826,
        "learning_rate": 0.00017035003203106857,
        "epoch": 0.17106666666666667,
        "step": 1283
    },
    {
        "loss": 2.5152,
        "grad_norm": 2.984482765197754,
        "learning_rate": 0.00017030528927291196,
        "epoch": 0.1712,
        "step": 1284
    },
    {
        "loss": 2.1013,
        "grad_norm": 3.0057249069213867,
        "learning_rate": 0.00017026051866659073,
        "epoch": 0.17133333333333334,
        "step": 1285
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.4806082248687744,
        "learning_rate": 0.00017021572022983861,
        "epoch": 0.17146666666666666,
        "step": 1286
    },
    {
        "loss": 2.0792,
        "grad_norm": 3.227093458175659,
        "learning_rate": 0.00017017089398040048,
        "epoch": 0.1716,
        "step": 1287
    },
    {
        "loss": 1.1283,
        "grad_norm": 4.383045673370361,
        "learning_rate": 0.00017012603993603215,
        "epoch": 0.17173333333333332,
        "step": 1288
    },
    {
        "loss": 2.7542,
        "grad_norm": 3.0297741889953613,
        "learning_rate": 0.0001700811581145004,
        "epoch": 0.17186666666666667,
        "step": 1289
    },
    {
        "loss": 1.9532,
        "grad_norm": 2.308807373046875,
        "learning_rate": 0.00017003624853358318,
        "epoch": 0.172,
        "step": 1290
    },
    {
        "loss": 2.3674,
        "grad_norm": 3.159698009490967,
        "learning_rate": 0.00016999131121106924,
        "epoch": 0.17213333333333333,
        "step": 1291
    },
    {
        "loss": 2.7067,
        "grad_norm": 2.4461092948913574,
        "learning_rate": 0.00016994634616475845,
        "epoch": 0.17226666666666668,
        "step": 1292
    },
    {
        "loss": 1.0534,
        "grad_norm": 4.03753662109375,
        "learning_rate": 0.0001699013534124616,
        "epoch": 0.1724,
        "step": 1293
    },
    {
        "loss": 2.4716,
        "grad_norm": 3.8751916885375977,
        "learning_rate": 0.00016985633297200053,
        "epoch": 0.17253333333333334,
        "step": 1294
    },
    {
        "loss": 1.2449,
        "grad_norm": 4.389795303344727,
        "learning_rate": 0.0001698112848612079,
        "epoch": 0.17266666666666666,
        "step": 1295
    },
    {
        "loss": 2.5761,
        "grad_norm": 2.6639785766601562,
        "learning_rate": 0.00016976620909792746,
        "epoch": 0.1728,
        "step": 1296
    },
    {
        "loss": 2.701,
        "grad_norm": 2.2981815338134766,
        "learning_rate": 0.00016972110570001388,
        "epoch": 0.17293333333333333,
        "step": 1297
    },
    {
        "loss": 2.53,
        "grad_norm": 3.926382303237915,
        "learning_rate": 0.00016967597468533273,
        "epoch": 0.17306666666666667,
        "step": 1298
    },
    {
        "loss": 2.7734,
        "grad_norm": 1.7706761360168457,
        "learning_rate": 0.00016963081607176066,
        "epoch": 0.1732,
        "step": 1299
    },
    {
        "loss": 2.6518,
        "grad_norm": 2.8792076110839844,
        "learning_rate": 0.00016958562987718503,
        "epoch": 0.17333333333333334,
        "step": 1300
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.9241130352020264,
        "learning_rate": 0.0001695404161195043,
        "epoch": 0.17346666666666666,
        "step": 1301
    },
    {
        "loss": 2.1755,
        "grad_norm": 3.7704639434814453,
        "learning_rate": 0.0001694951748166278,
        "epoch": 0.1736,
        "step": 1302
    },
    {
        "loss": 2.229,
        "grad_norm": 3.1474246978759766,
        "learning_rate": 0.00016944990598647577,
        "epoch": 0.17373333333333332,
        "step": 1303
    },
    {
        "loss": 2.4703,
        "grad_norm": 2.9462106227874756,
        "learning_rate": 0.00016940460964697934,
        "epoch": 0.17386666666666667,
        "step": 1304
    },
    {
        "loss": 2.5141,
        "grad_norm": 2.985856533050537,
        "learning_rate": 0.0001693592858160805,
        "epoch": 0.174,
        "step": 1305
    },
    {
        "loss": 2.0994,
        "grad_norm": 5.073915004730225,
        "learning_rate": 0.00016931393451173222,
        "epoch": 0.17413333333333333,
        "step": 1306
    },
    {
        "loss": 2.0994,
        "grad_norm": 3.79856538772583,
        "learning_rate": 0.00016926855575189834,
        "epoch": 0.17426666666666665,
        "step": 1307
    },
    {
        "loss": 2.2427,
        "grad_norm": 2.987680196762085,
        "learning_rate": 0.00016922314955455345,
        "epoch": 0.1744,
        "step": 1308
    },
    {
        "loss": 2.5118,
        "grad_norm": 2.4872443675994873,
        "learning_rate": 0.0001691777159376832,
        "epoch": 0.17453333333333335,
        "step": 1309
    },
    {
        "loss": 2.0981,
        "grad_norm": 2.1427395343780518,
        "learning_rate": 0.0001691322549192839,
        "epoch": 0.17466666666666666,
        "step": 1310
    },
    {
        "loss": 0.8484,
        "grad_norm": 3.1534368991851807,
        "learning_rate": 0.00016908676651736292,
        "epoch": 0.1748,
        "step": 1311
    },
    {
        "loss": 2.508,
        "grad_norm": 2.9848108291625977,
        "learning_rate": 0.00016904125074993827,
        "epoch": 0.17493333333333333,
        "step": 1312
    },
    {
        "loss": 1.794,
        "grad_norm": 3.843914031982422,
        "learning_rate": 0.00016899570763503894,
        "epoch": 0.17506666666666668,
        "step": 1313
    },
    {
        "loss": 2.6762,
        "grad_norm": 2.2463417053222656,
        "learning_rate": 0.00016895013719070475,
        "epoch": 0.1752,
        "step": 1314
    },
    {
        "loss": 2.2193,
        "grad_norm": 2.7353286743164062,
        "learning_rate": 0.00016890453943498626,
        "epoch": 0.17533333333333334,
        "step": 1315
    },
    {
        "loss": 1.8982,
        "grad_norm": 3.24082350730896,
        "learning_rate": 0.00016885891438594492,
        "epoch": 0.17546666666666666,
        "step": 1316
    },
    {
        "loss": 2.8882,
        "grad_norm": 2.9376380443573,
        "learning_rate": 0.00016881326206165292,
        "epoch": 0.1756,
        "step": 1317
    },
    {
        "loss": 2.2289,
        "grad_norm": 3.126105785369873,
        "learning_rate": 0.0001687675824801934,
        "epoch": 0.17573333333333332,
        "step": 1318
    },
    {
        "loss": 2.6499,
        "grad_norm": 4.047196388244629,
        "learning_rate": 0.00016872187565966014,
        "epoch": 0.17586666666666667,
        "step": 1319
    },
    {
        "loss": 3.0419,
        "grad_norm": 2.9500677585601807,
        "learning_rate": 0.00016867614161815775,
        "epoch": 0.176,
        "step": 1320
    },
    {
        "loss": 2.8769,
        "grad_norm": 3.7821907997131348,
        "learning_rate": 0.00016863038037380165,
        "epoch": 0.17613333333333334,
        "step": 1321
    },
    {
        "loss": 2.9046,
        "grad_norm": 3.3826420307159424,
        "learning_rate": 0.00016858459194471804,
        "epoch": 0.17626666666666665,
        "step": 1322
    },
    {
        "loss": 1.6812,
        "grad_norm": 3.5766618251800537,
        "learning_rate": 0.0001685387763490438,
        "epoch": 0.1764,
        "step": 1323
    },
    {
        "loss": 2.7564,
        "grad_norm": 2.50126576423645,
        "learning_rate": 0.0001684929336049268,
        "epoch": 0.17653333333333332,
        "step": 1324
    },
    {
        "loss": 2.6394,
        "grad_norm": 2.267540454864502,
        "learning_rate": 0.00016844706373052537,
        "epoch": 0.17666666666666667,
        "step": 1325
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.3480489253997803,
        "learning_rate": 0.0001684011667440088,
        "epoch": 0.1768,
        "step": 1326
    },
    {
        "loss": 2.0095,
        "grad_norm": 3.421722650527954,
        "learning_rate": 0.00016835524266355697,
        "epoch": 0.17693333333333333,
        "step": 1327
    },
    {
        "loss": 2.9481,
        "grad_norm": 2.687236785888672,
        "learning_rate": 0.00016830929150736065,
        "epoch": 0.17706666666666668,
        "step": 1328
    },
    {
        "loss": 2.1315,
        "grad_norm": 3.410773515701294,
        "learning_rate": 0.00016826331329362117,
        "epoch": 0.1772,
        "step": 1329
    },
    {
        "loss": 2.1754,
        "grad_norm": 3.2887744903564453,
        "learning_rate": 0.00016821730804055074,
        "epoch": 0.17733333333333334,
        "step": 1330
    },
    {
        "loss": 2.9842,
        "grad_norm": 3.5484471321105957,
        "learning_rate": 0.00016817127576637209,
        "epoch": 0.17746666666666666,
        "step": 1331
    },
    {
        "loss": 2.1,
        "grad_norm": 4.052454948425293,
        "learning_rate": 0.00016812521648931888,
        "epoch": 0.1776,
        "step": 1332
    },
    {
        "loss": 2.2289,
        "grad_norm": 4.227600574493408,
        "learning_rate": 0.00016807913022763522,
        "epoch": 0.17773333333333333,
        "step": 1333
    },
    {
        "loss": 2.4358,
        "grad_norm": 3.5472769737243652,
        "learning_rate": 0.00016803301699957614,
        "epoch": 0.17786666666666667,
        "step": 1334
    },
    {
        "loss": 2.2403,
        "grad_norm": 2.0860633850097656,
        "learning_rate": 0.00016798687682340722,
        "epoch": 0.178,
        "step": 1335
    },
    {
        "loss": 1.3469,
        "grad_norm": 4.326876163482666,
        "learning_rate": 0.00016794070971740469,
        "epoch": 0.17813333333333334,
        "step": 1336
    },
    {
        "loss": 2.3595,
        "grad_norm": 1.7398152351379395,
        "learning_rate": 0.00016789451569985562,
        "epoch": 0.17826666666666666,
        "step": 1337
    },
    {
        "loss": 2.3106,
        "grad_norm": 2.684154987335205,
        "learning_rate": 0.00016784829478905746,
        "epoch": 0.1784,
        "step": 1338
    },
    {
        "loss": 2.6462,
        "grad_norm": 3.377380847930908,
        "learning_rate": 0.00016780204700331857,
        "epoch": 0.17853333333333332,
        "step": 1339
    },
    {
        "loss": 2.832,
        "grad_norm": 2.701801061630249,
        "learning_rate": 0.00016775577236095783,
        "epoch": 0.17866666666666667,
        "step": 1340
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.766793966293335,
        "learning_rate": 0.0001677094708803048,
        "epoch": 0.1788,
        "step": 1341
    },
    {
        "loss": 2.8258,
        "grad_norm": 2.5049550533294678,
        "learning_rate": 0.00016766314257969964,
        "epoch": 0.17893333333333333,
        "step": 1342
    },
    {
        "loss": 2.7552,
        "grad_norm": 2.51975154876709,
        "learning_rate": 0.00016761678747749311,
        "epoch": 0.17906666666666668,
        "step": 1343
    },
    {
        "loss": 2.3542,
        "grad_norm": 2.2458276748657227,
        "learning_rate": 0.00016757040559204668,
        "epoch": 0.1792,
        "step": 1344
    },
    {
        "loss": 2.8331,
        "grad_norm": 2.6131157875061035,
        "learning_rate": 0.00016752399694173236,
        "epoch": 0.17933333333333334,
        "step": 1345
    },
    {
        "loss": 1.6301,
        "grad_norm": 3.113394021987915,
        "learning_rate": 0.00016747756154493278,
        "epoch": 0.17946666666666666,
        "step": 1346
    },
    {
        "loss": 2.8652,
        "grad_norm": 2.365199089050293,
        "learning_rate": 0.00016743109942004116,
        "epoch": 0.1796,
        "step": 1347
    },
    {
        "loss": 1.9814,
        "grad_norm": 3.499485492706299,
        "learning_rate": 0.00016738461058546126,
        "epoch": 0.17973333333333333,
        "step": 1348
    },
    {
        "loss": 2.4923,
        "grad_norm": 4.04562520980835,
        "learning_rate": 0.00016733809505960751,
        "epoch": 0.17986666666666667,
        "step": 1349
    },
    {
        "loss": 1.6535,
        "grad_norm": 3.8086321353912354,
        "learning_rate": 0.00016729155286090486,
        "epoch": 0.18,
        "step": 1350
    },
    {
        "loss": 2.1197,
        "grad_norm": 2.988206148147583,
        "learning_rate": 0.00016724498400778884,
        "epoch": 0.18013333333333334,
        "step": 1351
    },
    {
        "loss": 2.6745,
        "grad_norm": 3.634007692337036,
        "learning_rate": 0.0001671983885187055,
        "epoch": 0.18026666666666666,
        "step": 1352
    },
    {
        "loss": 2.1532,
        "grad_norm": 2.712859869003296,
        "learning_rate": 0.00016715176641211152,
        "epoch": 0.1804,
        "step": 1353
    },
    {
        "loss": 1.3333,
        "grad_norm": 4.155195713043213,
        "learning_rate": 0.00016710511770647404,
        "epoch": 0.18053333333333332,
        "step": 1354
    },
    {
        "loss": 1.9309,
        "grad_norm": 2.6959760189056396,
        "learning_rate": 0.0001670584424202708,
        "epoch": 0.18066666666666667,
        "step": 1355
    },
    {
        "loss": 2.2571,
        "grad_norm": 3.57440447807312,
        "learning_rate": 0.00016701174057199,
        "epoch": 0.1808,
        "step": 1356
    },
    {
        "loss": 1.82,
        "grad_norm": 3.184171438217163,
        "learning_rate": 0.0001669650121801304,
        "epoch": 0.18093333333333333,
        "step": 1357
    },
    {
        "loss": 3.7637,
        "grad_norm": 4.300503730773926,
        "learning_rate": 0.00016691825726320133,
        "epoch": 0.18106666666666665,
        "step": 1358
    },
    {
        "loss": 3.0016,
        "grad_norm": 2.862366199493408,
        "learning_rate": 0.00016687147583972255,
        "epoch": 0.1812,
        "step": 1359
    },
    {
        "loss": 0.8081,
        "grad_norm": 2.814073324203491,
        "learning_rate": 0.0001668246679282243,
        "epoch": 0.18133333333333335,
        "step": 1360
    },
    {
        "loss": 2.6619,
        "grad_norm": 3.131711006164551,
        "learning_rate": 0.0001667778335472474,
        "epoch": 0.18146666666666667,
        "step": 1361
    },
    {
        "loss": 1.7272,
        "grad_norm": 4.09881591796875,
        "learning_rate": 0.00016673097271534308,
        "epoch": 0.1816,
        "step": 1362
    },
    {
        "loss": 2.5029,
        "grad_norm": 2.458914279937744,
        "learning_rate": 0.0001666840854510731,
        "epoch": 0.18173333333333333,
        "step": 1363
    },
    {
        "loss": 1.0764,
        "grad_norm": 4.136885166168213,
        "learning_rate": 0.00016663717177300966,
        "epoch": 0.18186666666666668,
        "step": 1364
    },
    {
        "loss": 2.7655,
        "grad_norm": 1.6771986484527588,
        "learning_rate": 0.0001665902316997354,
        "epoch": 0.182,
        "step": 1365
    },
    {
        "loss": 1.0243,
        "grad_norm": 1.4453123807907104,
        "learning_rate": 0.00016654326524984347,
        "epoch": 0.18213333333333334,
        "step": 1366
    },
    {
        "loss": 0.9229,
        "grad_norm": 3.7982094287872314,
        "learning_rate": 0.00016649627244193745,
        "epoch": 0.18226666666666666,
        "step": 1367
    },
    {
        "loss": 2.1582,
        "grad_norm": 3.2883503437042236,
        "learning_rate": 0.0001664492532946313,
        "epoch": 0.1824,
        "step": 1368
    },
    {
        "loss": 1.5666,
        "grad_norm": 3.1537983417510986,
        "learning_rate": 0.00016640220782654952,
        "epoch": 0.18253333333333333,
        "step": 1369
    },
    {
        "loss": 2.3906,
        "grad_norm": 2.3911633491516113,
        "learning_rate": 0.000166355136056327,
        "epoch": 0.18266666666666667,
        "step": 1370
    },
    {
        "loss": 1.6903,
        "grad_norm": 2.7592356204986572,
        "learning_rate": 0.00016630803800260892,
        "epoch": 0.1828,
        "step": 1371
    },
    {
        "loss": 2.45,
        "grad_norm": 2.0553441047668457,
        "learning_rate": 0.00016626091368405108,
        "epoch": 0.18293333333333334,
        "step": 1372
    },
    {
        "loss": 2.122,
        "grad_norm": 3.063436985015869,
        "learning_rate": 0.00016621376311931954,
        "epoch": 0.18306666666666666,
        "step": 1373
    },
    {
        "loss": 2.6503,
        "grad_norm": 3.931504964828491,
        "learning_rate": 0.00016616658632709084,
        "epoch": 0.1832,
        "step": 1374
    },
    {
        "loss": 2.837,
        "grad_norm": 2.4539644718170166,
        "learning_rate": 0.0001661193833260518,
        "epoch": 0.18333333333333332,
        "step": 1375
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.528475761413574,
        "learning_rate": 0.00016607215413489977,
        "epoch": 0.18346666666666667,
        "step": 1376
    },
    {
        "loss": 2.795,
        "grad_norm": 3.1276183128356934,
        "learning_rate": 0.0001660248987723423,
        "epoch": 0.1836,
        "step": 1377
    },
    {
        "loss": 1.4369,
        "grad_norm": 3.3586366176605225,
        "learning_rate": 0.00016597761725709747,
        "epoch": 0.18373333333333333,
        "step": 1378
    },
    {
        "loss": 2.8899,
        "grad_norm": 2.2253308296203613,
        "learning_rate": 0.0001659303096078937,
        "epoch": 0.18386666666666668,
        "step": 1379
    },
    {
        "loss": 1.53,
        "grad_norm": 3.7373476028442383,
        "learning_rate": 0.00016588297584346954,
        "epoch": 0.184,
        "step": 1380
    },
    {
        "loss": 2.0293,
        "grad_norm": 6.643592834472656,
        "learning_rate": 0.0001658356159825742,
        "epoch": 0.18413333333333334,
        "step": 1381
    },
    {
        "loss": 2.451,
        "grad_norm": 2.886265993118286,
        "learning_rate": 0.00016578823004396705,
        "epoch": 0.18426666666666666,
        "step": 1382
    },
    {
        "loss": 2.7571,
        "grad_norm": 2.7744264602661133,
        "learning_rate": 0.00016574081804641782,
        "epoch": 0.1844,
        "step": 1383
    },
    {
        "loss": 1.6747,
        "grad_norm": 3.2598109245300293,
        "learning_rate": 0.0001656933800087065,
        "epoch": 0.18453333333333333,
        "step": 1384
    },
    {
        "loss": 1.2167,
        "grad_norm": 4.068180084228516,
        "learning_rate": 0.00016564591594962357,
        "epoch": 0.18466666666666667,
        "step": 1385
    },
    {
        "loss": 2.3226,
        "grad_norm": 2.8473832607269287,
        "learning_rate": 0.00016559842588796962,
        "epoch": 0.1848,
        "step": 1386
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.7310922145843506,
        "learning_rate": 0.0001655509098425557,
        "epoch": 0.18493333333333334,
        "step": 1387
    },
    {
        "loss": 1.6379,
        "grad_norm": 3.83988881111145,
        "learning_rate": 0.00016550336783220296,
        "epoch": 0.18506666666666666,
        "step": 1388
    },
    {
        "loss": 2.4774,
        "grad_norm": 2.9539361000061035,
        "learning_rate": 0.0001654557998757431,
        "epoch": 0.1852,
        "step": 1389
    },
    {
        "loss": 2.1248,
        "grad_norm": 3.99524188041687,
        "learning_rate": 0.00016540820599201782,
        "epoch": 0.18533333333333332,
        "step": 1390
    },
    {
        "loss": 1.5052,
        "grad_norm": 3.639679431915283,
        "learning_rate": 0.0001653605861998793,
        "epoch": 0.18546666666666667,
        "step": 1391
    },
    {
        "loss": 3.2281,
        "grad_norm": 2.851102828979492,
        "learning_rate": 0.00016531294051818986,
        "epoch": 0.1856,
        "step": 1392
    },
    {
        "loss": 2.7001,
        "grad_norm": 2.366729974746704,
        "learning_rate": 0.00016526526896582216,
        "epoch": 0.18573333333333333,
        "step": 1393
    },
    {
        "loss": 2.2101,
        "grad_norm": 2.558372735977173,
        "learning_rate": 0.00016521757156165898,
        "epoch": 0.18586666666666668,
        "step": 1394
    },
    {
        "loss": 2.468,
        "grad_norm": 3.305880308151245,
        "learning_rate": 0.00016516984832459355,
        "epoch": 0.186,
        "step": 1395
    },
    {
        "loss": 2.4331,
        "grad_norm": 3.8100123405456543,
        "learning_rate": 0.0001651220992735291,
        "epoch": 0.18613333333333335,
        "step": 1396
    },
    {
        "loss": 2.1356,
        "grad_norm": 3.7504165172576904,
        "learning_rate": 0.00016507432442737922,
        "epoch": 0.18626666666666666,
        "step": 1397
    },
    {
        "loss": 1.3045,
        "grad_norm": 4.002233028411865,
        "learning_rate": 0.0001650265238050677,
        "epoch": 0.1864,
        "step": 1398
    },
    {
        "loss": 1.2499,
        "grad_norm": 4.345325946807861,
        "learning_rate": 0.00016497869742552855,
        "epoch": 0.18653333333333333,
        "step": 1399
    },
    {
        "loss": 2.3732,
        "grad_norm": 3.507977247238159,
        "learning_rate": 0.00016493084530770595,
        "epoch": 0.18666666666666668,
        "step": 1400
    },
    {
        "loss": 0.6775,
        "grad_norm": 2.7225494384765625,
        "learning_rate": 0.00016488296747055426,
        "epoch": 0.1868,
        "step": 1401
    },
    {
        "loss": 2.7757,
        "grad_norm": 2.2278647422790527,
        "learning_rate": 0.00016483506393303804,
        "epoch": 0.18693333333333334,
        "step": 1402
    },
    {
        "loss": 2.7956,
        "grad_norm": 2.7892115116119385,
        "learning_rate": 0.0001647871347141321,
        "epoch": 0.18706666666666666,
        "step": 1403
    },
    {
        "loss": 2.5467,
        "grad_norm": 1.981838583946228,
        "learning_rate": 0.00016473917983282136,
        "epoch": 0.1872,
        "step": 1404
    },
    {
        "loss": 2.4431,
        "grad_norm": 3.072165012359619,
        "learning_rate": 0.00016469119930810088,
        "epoch": 0.18733333333333332,
        "step": 1405
    },
    {
        "loss": 1.8562,
        "grad_norm": 3.2473106384277344,
        "learning_rate": 0.00016464319315897594,
        "epoch": 0.18746666666666667,
        "step": 1406
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.6631155014038086,
        "learning_rate": 0.0001645951614044619,
        "epoch": 0.1876,
        "step": 1407
    },
    {
        "loss": 1.9572,
        "grad_norm": 3.528496026992798,
        "learning_rate": 0.00016454710406358435,
        "epoch": 0.18773333333333334,
        "step": 1408
    },
    {
        "loss": 2.8994,
        "grad_norm": 5.338257789611816,
        "learning_rate": 0.0001644990211553789,
        "epoch": 0.18786666666666665,
        "step": 1409
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.0663747787475586,
        "learning_rate": 0.00016445091269889147,
        "epoch": 0.188,
        "step": 1410
    },
    {
        "loss": 2.6752,
        "grad_norm": 2.6775898933410645,
        "learning_rate": 0.00016440277871317783,
        "epoch": 0.18813333333333335,
        "step": 1411
    },
    {
        "loss": 3.0216,
        "grad_norm": 3.4046361446380615,
        "learning_rate": 0.00016435461921730416,
        "epoch": 0.18826666666666667,
        "step": 1412
    },
    {
        "loss": 2.1182,
        "grad_norm": 2.6399357318878174,
        "learning_rate": 0.0001643064342303465,
        "epoch": 0.1884,
        "step": 1413
    },
    {
        "loss": 1.9139,
        "grad_norm": 2.8773932456970215,
        "learning_rate": 0.00016425822377139112,
        "epoch": 0.18853333333333333,
        "step": 1414
    },
    {
        "loss": 1.5315,
        "grad_norm": 2.953209638595581,
        "learning_rate": 0.0001642099878595343,
        "epoch": 0.18866666666666668,
        "step": 1415
    },
    {
        "loss": 1.3809,
        "grad_norm": 4.131598472595215,
        "learning_rate": 0.0001641617265138826,
        "epoch": 0.1888,
        "step": 1416
    },
    {
        "loss": 2.7857,
        "grad_norm": 3.4490303993225098,
        "learning_rate": 0.00016411343975355236,
        "epoch": 0.18893333333333334,
        "step": 1417
    },
    {
        "loss": 2.3838,
        "grad_norm": 2.9993014335632324,
        "learning_rate": 0.00016406512759767016,
        "epoch": 0.18906666666666666,
        "step": 1418
    },
    {
        "loss": 2.74,
        "grad_norm": 1.781038761138916,
        "learning_rate": 0.00016401679006537262,
        "epoch": 0.1892,
        "step": 1419
    },
    {
        "loss": 2.5818,
        "grad_norm": 2.586367130279541,
        "learning_rate": 0.00016396842717580643,
        "epoch": 0.18933333333333333,
        "step": 1420
    },
    {
        "loss": 2.403,
        "grad_norm": 2.297039270401001,
        "learning_rate": 0.00016392003894812827,
        "epoch": 0.18946666666666667,
        "step": 1421
    },
    {
        "loss": 1.95,
        "grad_norm": 2.9370522499084473,
        "learning_rate": 0.0001638716254015048,
        "epoch": 0.1896,
        "step": 1422
    },
    {
        "loss": 2.4421,
        "grad_norm": 3.4956047534942627,
        "learning_rate": 0.00016382318655511297,
        "epoch": 0.18973333333333334,
        "step": 1423
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.6556644439697266,
        "learning_rate": 0.00016377472242813944,
        "epoch": 0.18986666666666666,
        "step": 1424
    },
    {
        "loss": 1.4509,
        "grad_norm": 3.6914994716644287,
        "learning_rate": 0.00016372623303978102,
        "epoch": 0.19,
        "step": 1425
    },
    {
        "loss": 1.9028,
        "grad_norm": 3.8811373710632324,
        "learning_rate": 0.0001636777184092446,
        "epoch": 0.19013333333333332,
        "step": 1426
    },
    {
        "loss": 1.5083,
        "grad_norm": 2.360028028488159,
        "learning_rate": 0.00016362917855574694,
        "epoch": 0.19026666666666667,
        "step": 1427
    },
    {
        "loss": 2.3042,
        "grad_norm": 2.341693878173828,
        "learning_rate": 0.00016358061349851482,
        "epoch": 0.1904,
        "step": 1428
    },
    {
        "loss": 1.9457,
        "grad_norm": 3.894007682800293,
        "learning_rate": 0.00016353202325678504,
        "epoch": 0.19053333333333333,
        "step": 1429
    },
    {
        "loss": 0.8999,
        "grad_norm": 3.0986335277557373,
        "learning_rate": 0.00016348340784980435,
        "epoch": 0.19066666666666668,
        "step": 1430
    },
    {
        "loss": 2.8172,
        "grad_norm": 2.443331241607666,
        "learning_rate": 0.00016343476729682954,
        "epoch": 0.1908,
        "step": 1431
    },
    {
        "loss": 2.5065,
        "grad_norm": 3.3953633308410645,
        "learning_rate": 0.00016338610161712724,
        "epoch": 0.19093333333333334,
        "step": 1432
    },
    {
        "loss": 1.977,
        "grad_norm": 3.5500895977020264,
        "learning_rate": 0.00016333741082997412,
        "epoch": 0.19106666666666666,
        "step": 1433
    },
    {
        "loss": 1.0577,
        "grad_norm": 2.678100109100342,
        "learning_rate": 0.00016328869495465674,
        "epoch": 0.1912,
        "step": 1434
    },
    {
        "loss": 2.3902,
        "grad_norm": 2.924997329711914,
        "learning_rate": 0.00016323995401047161,
        "epoch": 0.19133333333333333,
        "step": 1435
    },
    {
        "loss": 4.1594,
        "grad_norm": 3.179278612136841,
        "learning_rate": 0.00016319118801672524,
        "epoch": 0.19146666666666667,
        "step": 1436
    },
    {
        "loss": 2.9729,
        "grad_norm": 4.586575508117676,
        "learning_rate": 0.00016314239699273398,
        "epoch": 0.1916,
        "step": 1437
    },
    {
        "loss": 2.02,
        "grad_norm": 3.701772928237915,
        "learning_rate": 0.00016309358095782412,
        "epoch": 0.19173333333333334,
        "step": 1438
    },
    {
        "loss": 1.9152,
        "grad_norm": 4.171550750732422,
        "learning_rate": 0.00016304473993133186,
        "epoch": 0.19186666666666666,
        "step": 1439
    },
    {
        "loss": 1.5238,
        "grad_norm": 3.447826623916626,
        "learning_rate": 0.00016299587393260327,
        "epoch": 0.192,
        "step": 1440
    },
    {
        "loss": 2.7503,
        "grad_norm": 2.160590648651123,
        "learning_rate": 0.00016294698298099437,
        "epoch": 0.19213333333333332,
        "step": 1441
    },
    {
        "loss": 2.863,
        "grad_norm": 2.327847719192505,
        "learning_rate": 0.00016289806709587107,
        "epoch": 0.19226666666666667,
        "step": 1442
    },
    {
        "loss": 2.9089,
        "grad_norm": 2.5056262016296387,
        "learning_rate": 0.00016284912629660898,
        "epoch": 0.1924,
        "step": 1443
    },
    {
        "loss": 2.3894,
        "grad_norm": 3.541325569152832,
        "learning_rate": 0.00016280016060259386,
        "epoch": 0.19253333333333333,
        "step": 1444
    },
    {
        "loss": 3.1172,
        "grad_norm": 3.688974618911743,
        "learning_rate": 0.00016275117003322116,
        "epoch": 0.19266666666666668,
        "step": 1445
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.9683072566986084,
        "learning_rate": 0.00016270215460789618,
        "epoch": 0.1928,
        "step": 1446
    },
    {
        "loss": 2.4636,
        "grad_norm": 2.6607117652893066,
        "learning_rate": 0.0001626531143460341,
        "epoch": 0.19293333333333335,
        "step": 1447
    },
    {
        "loss": 2.176,
        "grad_norm": 2.410281181335449,
        "learning_rate": 0.00016260404926705997,
        "epoch": 0.19306666666666666,
        "step": 1448
    },
    {
        "loss": 2.8474,
        "grad_norm": 2.805880546569824,
        "learning_rate": 0.00016255495939040854,
        "epoch": 0.1932,
        "step": 1449
    },
    {
        "loss": 2.3289,
        "grad_norm": 4.222158908843994,
        "learning_rate": 0.0001625058447355246,
        "epoch": 0.19333333333333333,
        "step": 1450
    },
    {
        "loss": 1.5061,
        "grad_norm": 5.161406517028809,
        "learning_rate": 0.00016245670532186258,
        "epoch": 0.19346666666666668,
        "step": 1451
    },
    {
        "loss": 2.1172,
        "grad_norm": 3.1356453895568848,
        "learning_rate": 0.00016240754116888674,
        "epoch": 0.1936,
        "step": 1452
    },
    {
        "loss": 2.1254,
        "grad_norm": 3.5824222564697266,
        "learning_rate": 0.00016235835229607117,
        "epoch": 0.19373333333333334,
        "step": 1453
    },
    {
        "loss": 2.0046,
        "grad_norm": 3.6597096920013428,
        "learning_rate": 0.00016230913872289983,
        "epoch": 0.19386666666666666,
        "step": 1454
    },
    {
        "loss": 1.1959,
        "grad_norm": 3.7988178730010986,
        "learning_rate": 0.00016225990046886632,
        "epoch": 0.194,
        "step": 1455
    },
    {
        "loss": 1.6627,
        "grad_norm": 4.545797348022461,
        "learning_rate": 0.00016221063755347408,
        "epoch": 0.19413333333333332,
        "step": 1456
    },
    {
        "loss": 1.8598,
        "grad_norm": 3.011946678161621,
        "learning_rate": 0.0001621613499962363,
        "epoch": 0.19426666666666667,
        "step": 1457
    },
    {
        "loss": 2.5555,
        "grad_norm": 2.952373504638672,
        "learning_rate": 0.00016211203781667602,
        "epoch": 0.1944,
        "step": 1458
    },
    {
        "loss": 1.7971,
        "grad_norm": 1.9705743789672852,
        "learning_rate": 0.0001620627010343259,
        "epoch": 0.19453333333333334,
        "step": 1459
    },
    {
        "loss": 2.3751,
        "grad_norm": 4.4508771896362305,
        "learning_rate": 0.00016201333966872842,
        "epoch": 0.19466666666666665,
        "step": 1460
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.4107847213745117,
        "learning_rate": 0.00016196395373943578,
        "epoch": 0.1948,
        "step": 1461
    },
    {
        "loss": 1.2933,
        "grad_norm": 3.0956733226776123,
        "learning_rate": 0.00016191454326600996,
        "epoch": 0.19493333333333332,
        "step": 1462
    },
    {
        "loss": 2.6435,
        "grad_norm": 3.671760320663452,
        "learning_rate": 0.00016186510826802258,
        "epoch": 0.19506666666666667,
        "step": 1463
    },
    {
        "loss": 2.7819,
        "grad_norm": 3.8166496753692627,
        "learning_rate": 0.00016181564876505496,
        "epoch": 0.1952,
        "step": 1464
    },
    {
        "loss": 2.888,
        "grad_norm": 2.165642261505127,
        "learning_rate": 0.00016176616477669832,
        "epoch": 0.19533333333333333,
        "step": 1465
    },
    {
        "loss": 1.8499,
        "grad_norm": 4.483962059020996,
        "learning_rate": 0.0001617166563225533,
        "epoch": 0.19546666666666668,
        "step": 1466
    },
    {
        "loss": 2.2038,
        "grad_norm": 2.584432363510132,
        "learning_rate": 0.0001616671234222304,
        "epoch": 0.1956,
        "step": 1467
    },
    {
        "loss": 2.3535,
        "grad_norm": 3.4109880924224854,
        "learning_rate": 0.00016161756609534984,
        "epoch": 0.19573333333333334,
        "step": 1468
    },
    {
        "loss": 1.6946,
        "grad_norm": 3.16231632232666,
        "learning_rate": 0.00016156798436154138,
        "epoch": 0.19586666666666666,
        "step": 1469
    },
    {
        "loss": 1.8452,
        "grad_norm": 3.13509202003479,
        "learning_rate": 0.00016151837824044448,
        "epoch": 0.196,
        "step": 1470
    },
    {
        "loss": 2.424,
        "grad_norm": 3.147069215774536,
        "learning_rate": 0.00016146874775170838,
        "epoch": 0.19613333333333333,
        "step": 1471
    },
    {
        "loss": 2.6739,
        "grad_norm": 2.6775128841400146,
        "learning_rate": 0.00016141909291499183,
        "epoch": 0.19626666666666667,
        "step": 1472
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.6320605278015137,
        "learning_rate": 0.0001613694137499633,
        "epoch": 0.1964,
        "step": 1473
    },
    {
        "loss": 2.6734,
        "grad_norm": 3.856973648071289,
        "learning_rate": 0.00016131971027630085,
        "epoch": 0.19653333333333334,
        "step": 1474
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.192148447036743,
        "learning_rate": 0.00016126998251369225,
        "epoch": 0.19666666666666666,
        "step": 1475
    },
    {
        "loss": 1.8523,
        "grad_norm": 2.7156710624694824,
        "learning_rate": 0.0001612202304818348,
        "epoch": 0.1968,
        "step": 1476
    },
    {
        "loss": 2.5146,
        "grad_norm": 2.1693811416625977,
        "learning_rate": 0.00016117045420043543,
        "epoch": 0.19693333333333332,
        "step": 1477
    },
    {
        "loss": 1.7334,
        "grad_norm": 3.0251991748809814,
        "learning_rate": 0.00016112065368921076,
        "epoch": 0.19706666666666667,
        "step": 1478
    },
    {
        "loss": 2.8249,
        "grad_norm": 2.3810880184173584,
        "learning_rate": 0.00016107082896788685,
        "epoch": 0.1972,
        "step": 1479
    },
    {
        "loss": 1.9607,
        "grad_norm": 5.231507301330566,
        "learning_rate": 0.00016102098005619955,
        "epoch": 0.19733333333333333,
        "step": 1480
    },
    {
        "loss": 2.8295,
        "grad_norm": 4.701068878173828,
        "learning_rate": 0.00016097110697389409,
        "epoch": 0.19746666666666668,
        "step": 1481
    },
    {
        "loss": 3.3223,
        "grad_norm": 4.3891215324401855,
        "learning_rate": 0.00016092120974072537,
        "epoch": 0.1976,
        "step": 1482
    },
    {
        "loss": 3.0233,
        "grad_norm": 3.319810628890991,
        "learning_rate": 0.00016087128837645795,
        "epoch": 0.19773333333333334,
        "step": 1483
    },
    {
        "loss": 2.69,
        "grad_norm": 2.629417896270752,
        "learning_rate": 0.00016082134290086578,
        "epoch": 0.19786666666666666,
        "step": 1484
    },
    {
        "loss": 2.6788,
        "grad_norm": 3.6345365047454834,
        "learning_rate": 0.0001607713733337324,
        "epoch": 0.198,
        "step": 1485
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.8550021648406982,
        "learning_rate": 0.000160721379694851,
        "epoch": 0.19813333333333333,
        "step": 1486
    },
    {
        "loss": 2.0385,
        "grad_norm": 3.062981367111206,
        "learning_rate": 0.0001606713620040242,
        "epoch": 0.19826666666666667,
        "step": 1487
    },
    {
        "loss": 2.749,
        "grad_norm": 2.3218681812286377,
        "learning_rate": 0.00016062132028106418,
        "epoch": 0.1984,
        "step": 1488
    },
    {
        "loss": 2.4053,
        "grad_norm": 1.9484448432922363,
        "learning_rate": 0.0001605712545457926,
        "epoch": 0.19853333333333334,
        "step": 1489
    },
    {
        "loss": 2.3674,
        "grad_norm": 2.9053447246551514,
        "learning_rate": 0.00016052116481804076,
        "epoch": 0.19866666666666666,
        "step": 1490
    },
    {
        "loss": 2.5759,
        "grad_norm": 2.2765209674835205,
        "learning_rate": 0.0001604710511176493,
        "epoch": 0.1988,
        "step": 1491
    },
    {
        "loss": 2.9776,
        "grad_norm": 3.0380961894989014,
        "learning_rate": 0.0001604209134644684,
        "epoch": 0.19893333333333332,
        "step": 1492
    },
    {
        "loss": 2.5389,
        "grad_norm": 2.292100667953491,
        "learning_rate": 0.00016037075187835784,
        "epoch": 0.19906666666666667,
        "step": 1493
    },
    {
        "loss": 2.6324,
        "grad_norm": 2.4012763500213623,
        "learning_rate": 0.00016032056637918672,
        "epoch": 0.1992,
        "step": 1494
    },
    {
        "loss": 2.2127,
        "grad_norm": 3.7324066162109375,
        "learning_rate": 0.00016027035698683374,
        "epoch": 0.19933333333333333,
        "step": 1495
    },
    {
        "loss": 2.3956,
        "grad_norm": 3.9383699893951416,
        "learning_rate": 0.00016022012372118703,
        "epoch": 0.19946666666666665,
        "step": 1496
    },
    {
        "loss": 0.8091,
        "grad_norm": 2.4046664237976074,
        "learning_rate": 0.00016016986660214407,
        "epoch": 0.1996,
        "step": 1497
    },
    {
        "loss": 1.9222,
        "grad_norm": 4.244639873504639,
        "learning_rate": 0.0001601195856496119,
        "epoch": 0.19973333333333335,
        "step": 1498
    },
    {
        "loss": 2.677,
        "grad_norm": 3.0381155014038086,
        "learning_rate": 0.00016006928088350706,
        "epoch": 0.19986666666666666,
        "step": 1499
    },
    {
        "loss": 1.7755,
        "grad_norm": 3.562835216522217,
        "learning_rate": 0.00016001895232375536,
        "epoch": 0.2,
        "step": 1500
    },
    {
        "loss": 2.0781,
        "grad_norm": 2.9353859424591064,
        "learning_rate": 0.00015996859999029215,
        "epoch": 0.20013333333333333,
        "step": 1501
    },
    {
        "loss": 3.0669,
        "grad_norm": 2.528841972351074,
        "learning_rate": 0.0001599182239030621,
        "epoch": 0.20026666666666668,
        "step": 1502
    },
    {
        "loss": 3.1923,
        "grad_norm": 3.7963860034942627,
        "learning_rate": 0.00015986782408201943,
        "epoch": 0.2004,
        "step": 1503
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.9255049228668213,
        "learning_rate": 0.00015981740054712764,
        "epoch": 0.20053333333333334,
        "step": 1504
    },
    {
        "loss": 1.8223,
        "grad_norm": 2.430145025253296,
        "learning_rate": 0.00015976695331835968,
        "epoch": 0.20066666666666666,
        "step": 1505
    },
    {
        "loss": 2.7507,
        "grad_norm": 2.2053380012512207,
        "learning_rate": 0.00015971648241569784,
        "epoch": 0.2008,
        "step": 1506
    },
    {
        "loss": 2.3153,
        "grad_norm": 2.672187566757202,
        "learning_rate": 0.00015966598785913387,
        "epoch": 0.20093333333333332,
        "step": 1507
    },
    {
        "loss": 2.8142,
        "grad_norm": 3.9024205207824707,
        "learning_rate": 0.00015961546966866875,
        "epoch": 0.20106666666666667,
        "step": 1508
    },
    {
        "loss": 2.1434,
        "grad_norm": 3.1278598308563232,
        "learning_rate": 0.000159564927864313,
        "epoch": 0.2012,
        "step": 1509
    },
    {
        "loss": 2.4882,
        "grad_norm": 3.2404944896698,
        "learning_rate": 0.00015951436246608637,
        "epoch": 0.20133333333333334,
        "step": 1510
    },
    {
        "loss": 3.0927,
        "grad_norm": 1.9707847833633423,
        "learning_rate": 0.00015946377349401796,
        "epoch": 0.20146666666666666,
        "step": 1511
    },
    {
        "loss": 3.0279,
        "grad_norm": 2.935640335083008,
        "learning_rate": 0.00015941316096814622,
        "epoch": 0.2016,
        "step": 1512
    },
    {
        "loss": 1.8295,
        "grad_norm": 3.440477132797241,
        "learning_rate": 0.00015936252490851902,
        "epoch": 0.20173333333333332,
        "step": 1513
    },
    {
        "loss": 2.0256,
        "grad_norm": 2.6194138526916504,
        "learning_rate": 0.0001593118653351934,
        "epoch": 0.20186666666666667,
        "step": 1514
    },
    {
        "loss": 1.9939,
        "grad_norm": 3.766848564147949,
        "learning_rate": 0.00015926118226823583,
        "epoch": 0.202,
        "step": 1515
    },
    {
        "loss": 2.624,
        "grad_norm": 2.6432676315307617,
        "learning_rate": 0.00015921047572772204,
        "epoch": 0.20213333333333333,
        "step": 1516
    },
    {
        "loss": 2.1734,
        "grad_norm": 2.57100510597229,
        "learning_rate": 0.00015915974573373708,
        "epoch": 0.20226666666666668,
        "step": 1517
    },
    {
        "loss": 2.6321,
        "grad_norm": 2.381718873977661,
        "learning_rate": 0.0001591089923063752,
        "epoch": 0.2024,
        "step": 1518
    },
    {
        "loss": 2.8697,
        "grad_norm": 2.5407848358154297,
        "learning_rate": 0.00015905821546574012,
        "epoch": 0.20253333333333334,
        "step": 1519
    },
    {
        "loss": 1.9739,
        "grad_norm": 3.1364946365356445,
        "learning_rate": 0.0001590074152319446,
        "epoch": 0.20266666666666666,
        "step": 1520
    },
    {
        "loss": 1.8218,
        "grad_norm": 4.052483558654785,
        "learning_rate": 0.0001589565916251109,
        "epoch": 0.2028,
        "step": 1521
    },
    {
        "loss": 1.166,
        "grad_norm": 3.9415273666381836,
        "learning_rate": 0.00015890574466537034,
        "epoch": 0.20293333333333333,
        "step": 1522
    },
    {
        "loss": 2.2076,
        "grad_norm": 3.0862767696380615,
        "learning_rate": 0.0001588548743728636,
        "epoch": 0.20306666666666667,
        "step": 1523
    },
    {
        "loss": 1.518,
        "grad_norm": 4.5492262840271,
        "learning_rate": 0.00015880398076774057,
        "epoch": 0.2032,
        "step": 1524
    },
    {
        "loss": 2.454,
        "grad_norm": 3.261408567428589,
        "learning_rate": 0.00015875306387016038,
        "epoch": 0.20333333333333334,
        "step": 1525
    },
    {
        "loss": 3.1619,
        "grad_norm": 2.6114463806152344,
        "learning_rate": 0.00015870212370029142,
        "epoch": 0.20346666666666666,
        "step": 1526
    },
    {
        "loss": 2.2434,
        "grad_norm": 3.372678756713867,
        "learning_rate": 0.00015865116027831124,
        "epoch": 0.2036,
        "step": 1527
    },
    {
        "loss": 2.2767,
        "grad_norm": 2.9472877979278564,
        "learning_rate": 0.00015860017362440663,
        "epoch": 0.20373333333333332,
        "step": 1528
    },
    {
        "loss": 1.8631,
        "grad_norm": 3.442551374435425,
        "learning_rate": 0.00015854916375877357,
        "epoch": 0.20386666666666667,
        "step": 1529
    },
    {
        "loss": 2.7645,
        "grad_norm": 3.129458427429199,
        "learning_rate": 0.0001584981307016172,
        "epoch": 0.204,
        "step": 1530
    },
    {
        "loss": 2.2731,
        "grad_norm": 3.5272252559661865,
        "learning_rate": 0.00015844707447315197,
        "epoch": 0.20413333333333333,
        "step": 1531
    },
    {
        "loss": 2.708,
        "grad_norm": 3.041167736053467,
        "learning_rate": 0.0001583959950936014,
        "epoch": 0.20426666666666668,
        "step": 1532
    },
    {
        "loss": 2.3207,
        "grad_norm": 3.2050623893737793,
        "learning_rate": 0.00015834489258319818,
        "epoch": 0.2044,
        "step": 1533
    },
    {
        "loss": 2.6652,
        "grad_norm": 3.9655818939208984,
        "learning_rate": 0.0001582937669621842,
        "epoch": 0.20453333333333334,
        "step": 1534
    },
    {
        "loss": 2.419,
        "grad_norm": 3.367738962173462,
        "learning_rate": 0.0001582426182508105,
        "epoch": 0.20466666666666666,
        "step": 1535
    },
    {
        "loss": 2.1599,
        "grad_norm": 3.798259735107422,
        "learning_rate": 0.00015819144646933724,
        "epoch": 0.2048,
        "step": 1536
    },
    {
        "loss": 1.896,
        "grad_norm": 4.245767593383789,
        "learning_rate": 0.00015814025163803373,
        "epoch": 0.20493333333333333,
        "step": 1537
    },
    {
        "loss": 0.8811,
        "grad_norm": 3.402264356613159,
        "learning_rate": 0.0001580890337771785,
        "epoch": 0.20506666666666667,
        "step": 1538
    },
    {
        "loss": 1.83,
        "grad_norm": 2.8853394985198975,
        "learning_rate": 0.00015803779290705903,
        "epoch": 0.2052,
        "step": 1539
    },
    {
        "loss": 3.1397,
        "grad_norm": 3.432844638824463,
        "learning_rate": 0.00015798652904797203,
        "epoch": 0.20533333333333334,
        "step": 1540
    },
    {
        "loss": 2.9864,
        "grad_norm": 2.027339458465576,
        "learning_rate": 0.00015793524222022326,
        "epoch": 0.20546666666666666,
        "step": 1541
    },
    {
        "loss": 2.7354,
        "grad_norm": 2.9836831092834473,
        "learning_rate": 0.00015788393244412768,
        "epoch": 0.2056,
        "step": 1542
    },
    {
        "loss": 2.1945,
        "grad_norm": 3.268913745880127,
        "learning_rate": 0.0001578325997400092,
        "epoch": 0.20573333333333332,
        "step": 1543
    },
    {
        "loss": 2.0329,
        "grad_norm": 3.5260376930236816,
        "learning_rate": 0.0001577812441282009,
        "epoch": 0.20586666666666667,
        "step": 1544
    },
    {
        "loss": 2.6374,
        "grad_norm": 4.364593982696533,
        "learning_rate": 0.00015772986562904493,
        "epoch": 0.206,
        "step": 1545
    },
    {
        "loss": 4.0985,
        "grad_norm": 5.317951202392578,
        "learning_rate": 0.00015767846426289244,
        "epoch": 0.20613333333333334,
        "step": 1546
    },
    {
        "loss": 1.9156,
        "grad_norm": 3.495023250579834,
        "learning_rate": 0.00015762704005010378,
        "epoch": 0.20626666666666665,
        "step": 1547
    },
    {
        "loss": 2.5147,
        "grad_norm": 2.540994882583618,
        "learning_rate": 0.00015757559301104817,
        "epoch": 0.2064,
        "step": 1548
    },
    {
        "loss": 2.0609,
        "grad_norm": 2.5224170684814453,
        "learning_rate": 0.00015752412316610398,
        "epoch": 0.20653333333333335,
        "step": 1549
    },
    {
        "loss": 2.0664,
        "grad_norm": 2.4662413597106934,
        "learning_rate": 0.0001574726305356586,
        "epoch": 0.20666666666666667,
        "step": 1550
    },
    {
        "loss": 1.5622,
        "grad_norm": 3.379136085510254,
        "learning_rate": 0.00015742111514010842,
        "epoch": 0.2068,
        "step": 1551
    },
    {
        "loss": 2.1436,
        "grad_norm": 3.829190969467163,
        "learning_rate": 0.0001573695769998589,
        "epoch": 0.20693333333333333,
        "step": 1552
    },
    {
        "loss": 2.5986,
        "grad_norm": 3.3114051818847656,
        "learning_rate": 0.00015731801613532445,
        "epoch": 0.20706666666666668,
        "step": 1553
    },
    {
        "loss": 3.0084,
        "grad_norm": 3.115811824798584,
        "learning_rate": 0.00015726643256692847,
        "epoch": 0.2072,
        "step": 1554
    },
    {
        "loss": 2.4368,
        "grad_norm": 3.320500135421753,
        "learning_rate": 0.00015721482631510342,
        "epoch": 0.20733333333333334,
        "step": 1555
    },
    {
        "loss": 2.4238,
        "grad_norm": 3.460975408554077,
        "learning_rate": 0.00015716319740029073,
        "epoch": 0.20746666666666666,
        "step": 1556
    },
    {
        "loss": 2.4231,
        "grad_norm": 2.2131383419036865,
        "learning_rate": 0.00015711154584294076,
        "epoch": 0.2076,
        "step": 1557
    },
    {
        "loss": 3.2848,
        "grad_norm": 4.228786468505859,
        "learning_rate": 0.0001570598716635129,
        "epoch": 0.20773333333333333,
        "step": 1558
    },
    {
        "loss": 2.4961,
        "grad_norm": 3.409555435180664,
        "learning_rate": 0.00015700817488247543,
        "epoch": 0.20786666666666667,
        "step": 1559
    },
    {
        "loss": 2.8295,
        "grad_norm": 3.040027379989624,
        "learning_rate": 0.00015695645552030567,
        "epoch": 0.208,
        "step": 1560
    },
    {
        "loss": 2.349,
        "grad_norm": 2.95458984375,
        "learning_rate": 0.00015690471359748975,
        "epoch": 0.20813333333333334,
        "step": 1561
    },
    {
        "loss": 2.2515,
        "grad_norm": 2.2420270442962646,
        "learning_rate": 0.0001568529491345229,
        "epoch": 0.20826666666666666,
        "step": 1562
    },
    {
        "loss": 2.2253,
        "grad_norm": 3.4585442543029785,
        "learning_rate": 0.00015680116215190917,
        "epoch": 0.2084,
        "step": 1563
    },
    {
        "loss": 2.4945,
        "grad_norm": 2.001628875732422,
        "learning_rate": 0.0001567493526701616,
        "epoch": 0.20853333333333332,
        "step": 1564
    },
    {
        "loss": 2.5799,
        "grad_norm": 2.7998907566070557,
        "learning_rate": 0.000156697520709802,
        "epoch": 0.20866666666666667,
        "step": 1565
    },
    {
        "loss": 2.2506,
        "grad_norm": 3.654484748840332,
        "learning_rate": 0.00015664566629136135,
        "epoch": 0.2088,
        "step": 1566
    },
    {
        "loss": 2.0491,
        "grad_norm": 4.406952381134033,
        "learning_rate": 0.0001565937894353792,
        "epoch": 0.20893333333333333,
        "step": 1567
    },
    {
        "loss": 3.3148,
        "grad_norm": 5.077203273773193,
        "learning_rate": 0.00015654189016240424,
        "epoch": 0.20906666666666668,
        "step": 1568
    },
    {
        "loss": 2.0178,
        "grad_norm": 2.8086483478546143,
        "learning_rate": 0.00015648996849299392,
        "epoch": 0.2092,
        "step": 1569
    },
    {
        "loss": 2.8158,
        "grad_norm": 2.5372121334075928,
        "learning_rate": 0.0001564380244477146,
        "epoch": 0.20933333333333334,
        "step": 1570
    },
    {
        "loss": 2.895,
        "grad_norm": 2.964165210723877,
        "learning_rate": 0.00015638605804714145,
        "epoch": 0.20946666666666666,
        "step": 1571
    },
    {
        "loss": 1.9695,
        "grad_norm": 4.021872043609619,
        "learning_rate": 0.0001563340693118586,
        "epoch": 0.2096,
        "step": 1572
    },
    {
        "loss": 2.4638,
        "grad_norm": 2.6894640922546387,
        "learning_rate": 0.00015628205826245898,
        "epoch": 0.20973333333333333,
        "step": 1573
    },
    {
        "loss": 2.663,
        "grad_norm": 2.385270357131958,
        "learning_rate": 0.00015623002491954425,
        "epoch": 0.20986666666666667,
        "step": 1574
    },
    {
        "loss": 1.8733,
        "grad_norm": 3.192821979522705,
        "learning_rate": 0.00015617796930372509,
        "epoch": 0.21,
        "step": 1575
    },
    {
        "loss": 2.9661,
        "grad_norm": 3.431419849395752,
        "learning_rate": 0.0001561258914356208,
        "epoch": 0.21013333333333334,
        "step": 1576
    },
    {
        "loss": 0.9391,
        "grad_norm": 3.768725633621216,
        "learning_rate": 0.00015607379133585978,
        "epoch": 0.21026666666666666,
        "step": 1577
    },
    {
        "loss": 2.2404,
        "grad_norm": 3.4240729808807373,
        "learning_rate": 0.00015602166902507886,
        "epoch": 0.2104,
        "step": 1578
    },
    {
        "loss": 2.6706,
        "grad_norm": 2.333745241165161,
        "learning_rate": 0.00015596952452392397,
        "epoch": 0.21053333333333332,
        "step": 1579
    },
    {
        "loss": 2.9195,
        "grad_norm": 3.2235496044158936,
        "learning_rate": 0.00015591735785304973,
        "epoch": 0.21066666666666667,
        "step": 1580
    },
    {
        "loss": 1.9566,
        "grad_norm": 3.423461675643921,
        "learning_rate": 0.00015586516903311946,
        "epoch": 0.2108,
        "step": 1581
    },
    {
        "loss": 1.1879,
        "grad_norm": 5.239696979522705,
        "learning_rate": 0.00015581295808480544,
        "epoch": 0.21093333333333333,
        "step": 1582
    },
    {
        "loss": 2.4373,
        "grad_norm": 2.792200803756714,
        "learning_rate": 0.00015576072502878848,
        "epoch": 0.21106666666666668,
        "step": 1583
    },
    {
        "loss": 3.0379,
        "grad_norm": 3.7176928520202637,
        "learning_rate": 0.00015570846988575838,
        "epoch": 0.2112,
        "step": 1584
    },
    {
        "loss": 2.6101,
        "grad_norm": 4.2403717041015625,
        "learning_rate": 0.00015565619267641354,
        "epoch": 0.21133333333333335,
        "step": 1585
    },
    {
        "loss": 2.5078,
        "grad_norm": 3.111180305480957,
        "learning_rate": 0.00015560389342146111,
        "epoch": 0.21146666666666666,
        "step": 1586
    },
    {
        "loss": 0.8959,
        "grad_norm": 3.373142957687378,
        "learning_rate": 0.00015555157214161707,
        "epoch": 0.2116,
        "step": 1587
    },
    {
        "loss": 1.9798,
        "grad_norm": 2.611790418624878,
        "learning_rate": 0.00015549922885760599,
        "epoch": 0.21173333333333333,
        "step": 1588
    },
    {
        "loss": 2.1691,
        "grad_norm": 2.984682083129883,
        "learning_rate": 0.00015544686359016122,
        "epoch": 0.21186666666666668,
        "step": 1589
    },
    {
        "loss": 3.2102,
        "grad_norm": 1.9870672225952148,
        "learning_rate": 0.00015539447636002488,
        "epoch": 0.212,
        "step": 1590
    },
    {
        "loss": 2.1082,
        "grad_norm": 3.128725290298462,
        "learning_rate": 0.00015534206718794774,
        "epoch": 0.21213333333333334,
        "step": 1591
    },
    {
        "loss": 2.7401,
        "grad_norm": 3.6701254844665527,
        "learning_rate": 0.00015528963609468915,
        "epoch": 0.21226666666666666,
        "step": 1592
    },
    {
        "loss": 2.5971,
        "grad_norm": 1.9497873783111572,
        "learning_rate": 0.00015523718310101736,
        "epoch": 0.2124,
        "step": 1593
    },
    {
        "loss": 2.2627,
        "grad_norm": 2.1876933574676514,
        "learning_rate": 0.0001551847082277091,
        "epoch": 0.21253333333333332,
        "step": 1594
    },
    {
        "loss": 2.0671,
        "grad_norm": 4.735054969787598,
        "learning_rate": 0.00015513221149554988,
        "epoch": 0.21266666666666667,
        "step": 1595
    },
    {
        "loss": 3.4904,
        "grad_norm": 4.129384517669678,
        "learning_rate": 0.0001550796929253338,
        "epoch": 0.2128,
        "step": 1596
    },
    {
        "loss": 1.4831,
        "grad_norm": 3.4707534313201904,
        "learning_rate": 0.0001550271525378637,
        "epoch": 0.21293333333333334,
        "step": 1597
    },
    {
        "loss": 2.9413,
        "grad_norm": 3.861497402191162,
        "learning_rate": 0.000154974590353951,
        "epoch": 0.21306666666666665,
        "step": 1598
    },
    {
        "loss": 2.6229,
        "grad_norm": 2.6037800312042236,
        "learning_rate": 0.00015492200639441572,
        "epoch": 0.2132,
        "step": 1599
    },
    {
        "loss": 2.4264,
        "grad_norm": 3.002476453781128,
        "learning_rate": 0.00015486940068008655,
        "epoch": 0.21333333333333335,
        "step": 1600
    },
    {
        "loss": 2.8346,
        "grad_norm": 2.3734052181243896,
        "learning_rate": 0.00015481677323180083,
        "epoch": 0.21346666666666667,
        "step": 1601
    },
    {
        "loss": 2.0461,
        "grad_norm": 2.552549123764038,
        "learning_rate": 0.00015476412407040445,
        "epoch": 0.2136,
        "step": 1602
    },
    {
        "loss": 2.0679,
        "grad_norm": 3.8992466926574707,
        "learning_rate": 0.00015471145321675192,
        "epoch": 0.21373333333333333,
        "step": 1603
    },
    {
        "loss": 2.011,
        "grad_norm": 4.299815654754639,
        "learning_rate": 0.0001546587606917063,
        "epoch": 0.21386666666666668,
        "step": 1604
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.5719382762908936,
        "learning_rate": 0.00015460604651613936,
        "epoch": 0.214,
        "step": 1605
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.677051305770874,
        "learning_rate": 0.00015455331071093136,
        "epoch": 0.21413333333333334,
        "step": 1606
    },
    {
        "loss": 2.1077,
        "grad_norm": 3.197685480117798,
        "learning_rate": 0.00015450055329697105,
        "epoch": 0.21426666666666666,
        "step": 1607
    },
    {
        "loss": 3.0297,
        "grad_norm": 2.657418727874756,
        "learning_rate": 0.0001544477742951559,
        "epoch": 0.2144,
        "step": 1608
    },
    {
        "loss": 2.6644,
        "grad_norm": 2.816602945327759,
        "learning_rate": 0.00015439497372639183,
        "epoch": 0.21453333333333333,
        "step": 1609
    },
    {
        "loss": 1.9396,
        "grad_norm": 3.50394606590271,
        "learning_rate": 0.0001543421516115933,
        "epoch": 0.21466666666666667,
        "step": 1610
    },
    {
        "loss": 2.5829,
        "grad_norm": 3.092566728591919,
        "learning_rate": 0.00015428930797168337,
        "epoch": 0.2148,
        "step": 1611
    },
    {
        "loss": 1.6689,
        "grad_norm": 3.9314026832580566,
        "learning_rate": 0.00015423644282759358,
        "epoch": 0.21493333333333334,
        "step": 1612
    },
    {
        "loss": 2.4859,
        "grad_norm": 3.0043890476226807,
        "learning_rate": 0.00015418355620026394,
        "epoch": 0.21506666666666666,
        "step": 1613
    },
    {
        "loss": 2.6414,
        "grad_norm": 4.058751583099365,
        "learning_rate": 0.00015413064811064308,
        "epoch": 0.2152,
        "step": 1614
    },
    {
        "loss": 2.2965,
        "grad_norm": 3.6633670330047607,
        "learning_rate": 0.00015407771857968807,
        "epoch": 0.21533333333333332,
        "step": 1615
    },
    {
        "loss": 2.184,
        "grad_norm": 3.0558643341064453,
        "learning_rate": 0.00015402476762836444,
        "epoch": 0.21546666666666667,
        "step": 1616
    },
    {
        "loss": 1.624,
        "grad_norm": 6.264791011810303,
        "learning_rate": 0.0001539717952776463,
        "epoch": 0.2156,
        "step": 1617
    },
    {
        "loss": 2.7368,
        "grad_norm": 2.8258256912231445,
        "learning_rate": 0.00015391880154851613,
        "epoch": 0.21573333333333333,
        "step": 1618
    },
    {
        "loss": 2.8657,
        "grad_norm": 3.73049259185791,
        "learning_rate": 0.00015386578646196493,
        "epoch": 0.21586666666666668,
        "step": 1619
    },
    {
        "loss": 2.2261,
        "grad_norm": 2.655520439147949,
        "learning_rate": 0.00015381275003899217,
        "epoch": 0.216,
        "step": 1620
    },
    {
        "loss": 2.6724,
        "grad_norm": 2.5063681602478027,
        "learning_rate": 0.00015375969230060576,
        "epoch": 0.21613333333333334,
        "step": 1621
    },
    {
        "loss": 2.326,
        "grad_norm": 3.370255947113037,
        "learning_rate": 0.00015370661326782209,
        "epoch": 0.21626666666666666,
        "step": 1622
    },
    {
        "loss": 2.4198,
        "grad_norm": 2.9891905784606934,
        "learning_rate": 0.0001536535129616659,
        "epoch": 0.2164,
        "step": 1623
    },
    {
        "loss": 1.5452,
        "grad_norm": 3.1409897804260254,
        "learning_rate": 0.00015360039140317034,
        "epoch": 0.21653333333333333,
        "step": 1624
    },
    {
        "loss": 2.8019,
        "grad_norm": 1.9557009935379028,
        "learning_rate": 0.00015354724861337714,
        "epoch": 0.21666666666666667,
        "step": 1625
    },
    {
        "loss": 1.8344,
        "grad_norm": 3.3188889026641846,
        "learning_rate": 0.0001534940846133363,
        "epoch": 0.2168,
        "step": 1626
    },
    {
        "loss": 2.3984,
        "grad_norm": 2.2369985580444336,
        "learning_rate": 0.0001534408994241063,
        "epoch": 0.21693333333333334,
        "step": 1627
    },
    {
        "loss": 3.3837,
        "grad_norm": 3.8293464183807373,
        "learning_rate": 0.0001533876930667539,
        "epoch": 0.21706666666666666,
        "step": 1628
    },
    {
        "loss": 1.2149,
        "grad_norm": 3.3792548179626465,
        "learning_rate": 0.00015333446556235438,
        "epoch": 0.2172,
        "step": 1629
    },
    {
        "loss": 1.3236,
        "grad_norm": 3.0639939308166504,
        "learning_rate": 0.00015328121693199132,
        "epoch": 0.21733333333333332,
        "step": 1630
    },
    {
        "loss": 2.9152,
        "grad_norm": 3.546841621398926,
        "learning_rate": 0.00015322794719675669,
        "epoch": 0.21746666666666667,
        "step": 1631
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.1659796237945557,
        "learning_rate": 0.0001531746563777508,
        "epoch": 0.2176,
        "step": 1632
    },
    {
        "loss": 2.0554,
        "grad_norm": 3.80924129486084,
        "learning_rate": 0.0001531213444960823,
        "epoch": 0.21773333333333333,
        "step": 1633
    },
    {
        "loss": 2.227,
        "grad_norm": 3.4639244079589844,
        "learning_rate": 0.0001530680115728683,
        "epoch": 0.21786666666666665,
        "step": 1634
    },
    {
        "loss": 2.5511,
        "grad_norm": 3.1265344619750977,
        "learning_rate": 0.00015301465762923404,
        "epoch": 0.218,
        "step": 1635
    },
    {
        "loss": 2.0221,
        "grad_norm": 3.123692750930786,
        "learning_rate": 0.00015296128268631328,
        "epoch": 0.21813333333333335,
        "step": 1636
    },
    {
        "loss": 2.5296,
        "grad_norm": 2.0819358825683594,
        "learning_rate": 0.00015290788676524793,
        "epoch": 0.21826666666666666,
        "step": 1637
    },
    {
        "loss": 2.6166,
        "grad_norm": 2.415078639984131,
        "learning_rate": 0.00015285446988718841,
        "epoch": 0.2184,
        "step": 1638
    },
    {
        "loss": 1.1981,
        "grad_norm": 3.3500490188598633,
        "learning_rate": 0.00015280103207329328,
        "epoch": 0.21853333333333333,
        "step": 1639
    },
    {
        "loss": 2.8158,
        "grad_norm": 2.4311630725860596,
        "learning_rate": 0.0001527475733447294,
        "epoch": 0.21866666666666668,
        "step": 1640
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.3292651176452637,
        "learning_rate": 0.000152694093722672,
        "epoch": 0.2188,
        "step": 1641
    },
    {
        "loss": 1.8377,
        "grad_norm": 3.6204864978790283,
        "learning_rate": 0.0001526405932283045,
        "epoch": 0.21893333333333334,
        "step": 1642
    },
    {
        "loss": 1.5382,
        "grad_norm": 4.188083171844482,
        "learning_rate": 0.0001525870718828187,
        "epoch": 0.21906666666666666,
        "step": 1643
    },
    {
        "loss": 2.5082,
        "grad_norm": 2.419813632965088,
        "learning_rate": 0.0001525335297074145,
        "epoch": 0.2192,
        "step": 1644
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.9692370891571045,
        "learning_rate": 0.0001524799667233002,
        "epoch": 0.21933333333333332,
        "step": 1645
    },
    {
        "loss": 2.4189,
        "grad_norm": 2.8171792030334473,
        "learning_rate": 0.00015242638295169224,
        "epoch": 0.21946666666666667,
        "step": 1646
    },
    {
        "loss": 2.1289,
        "grad_norm": 3.7838048934936523,
        "learning_rate": 0.00015237277841381538,
        "epoch": 0.2196,
        "step": 1647
    },
    {
        "loss": 3.2009,
        "grad_norm": 2.503204584121704,
        "learning_rate": 0.00015231915313090253,
        "epoch": 0.21973333333333334,
        "step": 1648
    },
    {
        "loss": 1.7488,
        "grad_norm": 3.939821243286133,
        "learning_rate": 0.00015226550712419483,
        "epoch": 0.21986666666666665,
        "step": 1649
    },
    {
        "loss": 2.3742,
        "grad_norm": 2.621701717376709,
        "learning_rate": 0.0001522118404149417,
        "epoch": 0.22,
        "step": 1650
    },
    {
        "loss": 1.9819,
        "grad_norm": 5.912501335144043,
        "learning_rate": 0.00015215815302440063,
        "epoch": 0.22013333333333332,
        "step": 1651
    },
    {
        "loss": 2.8651,
        "grad_norm": 2.720679759979248,
        "learning_rate": 0.00015210444497383745,
        "epoch": 0.22026666666666667,
        "step": 1652
    },
    {
        "loss": 2.7282,
        "grad_norm": 4.074592113494873,
        "learning_rate": 0.0001520507162845261,
        "epoch": 0.2204,
        "step": 1653
    },
    {
        "loss": 2.456,
        "grad_norm": 3.09169602394104,
        "learning_rate": 0.00015199696697774863,
        "epoch": 0.22053333333333333,
        "step": 1654
    },
    {
        "loss": 2.5657,
        "grad_norm": 2.8540704250335693,
        "learning_rate": 0.00015194319707479537,
        "epoch": 0.22066666666666668,
        "step": 1655
    },
    {
        "loss": 1.0078,
        "grad_norm": 4.570435523986816,
        "learning_rate": 0.00015188940659696475,
        "epoch": 0.2208,
        "step": 1656
    },
    {
        "loss": 2.1759,
        "grad_norm": 2.042849063873291,
        "learning_rate": 0.0001518355955655634,
        "epoch": 0.22093333333333334,
        "step": 1657
    },
    {
        "loss": 2.529,
        "grad_norm": 3.212064504623413,
        "learning_rate": 0.000151781764001906,
        "epoch": 0.22106666666666666,
        "step": 1658
    },
    {
        "loss": 2.0614,
        "grad_norm": 5.383714199066162,
        "learning_rate": 0.00015172791192731545,
        "epoch": 0.2212,
        "step": 1659
    },
    {
        "loss": 0.8739,
        "grad_norm": 5.90691614151001,
        "learning_rate": 0.00015167403936312273,
        "epoch": 0.22133333333333333,
        "step": 1660
    },
    {
        "loss": 3.3062,
        "grad_norm": 1.8684874773025513,
        "learning_rate": 0.00015162014633066692,
        "epoch": 0.22146666666666667,
        "step": 1661
    },
    {
        "loss": 1.4435,
        "grad_norm": 4.204184532165527,
        "learning_rate": 0.00015156623285129527,
        "epoch": 0.2216,
        "step": 1662
    },
    {
        "loss": 2.6575,
        "grad_norm": 2.1377577781677246,
        "learning_rate": 0.00015151229894636305,
        "epoch": 0.22173333333333334,
        "step": 1663
    },
    {
        "loss": 2.4364,
        "grad_norm": 1.8633415699005127,
        "learning_rate": 0.00015145834463723375,
        "epoch": 0.22186666666666666,
        "step": 1664
    },
    {
        "loss": 2.9772,
        "grad_norm": 2.6877245903015137,
        "learning_rate": 0.00015140436994527876,
        "epoch": 0.222,
        "step": 1665
    },
    {
        "loss": 2.8175,
        "grad_norm": 3.0798428058624268,
        "learning_rate": 0.00015135037489187768,
        "epoch": 0.22213333333333332,
        "step": 1666
    },
    {
        "loss": 2.4288,
        "grad_norm": 3.1296474933624268,
        "learning_rate": 0.00015129635949841816,
        "epoch": 0.22226666666666667,
        "step": 1667
    },
    {
        "loss": 2.1975,
        "grad_norm": 3.3042821884155273,
        "learning_rate": 0.00015124232378629584,
        "epoch": 0.2224,
        "step": 1668
    },
    {
        "loss": 3.12,
        "grad_norm": 2.4216742515563965,
        "learning_rate": 0.00015118826777691446,
        "epoch": 0.22253333333333333,
        "step": 1669
    },
    {
        "loss": 3.5874,
        "grad_norm": 6.906096458435059,
        "learning_rate": 0.00015113419149168577,
        "epoch": 0.22266666666666668,
        "step": 1670
    },
    {
        "loss": 2.6359,
        "grad_norm": 2.784843683242798,
        "learning_rate": 0.00015108009495202963,
        "epoch": 0.2228,
        "step": 1671
    },
    {
        "loss": 2.9296,
        "grad_norm": 2.9313361644744873,
        "learning_rate": 0.00015102597817937382,
        "epoch": 0.22293333333333334,
        "step": 1672
    },
    {
        "loss": 2.5781,
        "grad_norm": 3.329030990600586,
        "learning_rate": 0.0001509718411951542,
        "epoch": 0.22306666666666666,
        "step": 1673
    },
    {
        "loss": 2.744,
        "grad_norm": 3.2292733192443848,
        "learning_rate": 0.00015091768402081458,
        "epoch": 0.2232,
        "step": 1674
    },
    {
        "loss": 1.8287,
        "grad_norm": 2.6329891681671143,
        "learning_rate": 0.00015086350667780683,
        "epoch": 0.22333333333333333,
        "step": 1675
    },
    {
        "loss": 1.9933,
        "grad_norm": 3.4641776084899902,
        "learning_rate": 0.00015080930918759074,
        "epoch": 0.22346666666666667,
        "step": 1676
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.8113300800323486,
        "learning_rate": 0.0001507550915716342,
        "epoch": 0.2236,
        "step": 1677
    },
    {
        "loss": 2.0862,
        "grad_norm": 3.146366834640503,
        "learning_rate": 0.00015070085385141292,
        "epoch": 0.22373333333333334,
        "step": 1678
    },
    {
        "loss": 2.6471,
        "grad_norm": 1.9502025842666626,
        "learning_rate": 0.00015064659604841068,
        "epoch": 0.22386666666666666,
        "step": 1679
    },
    {
        "loss": 1.5321,
        "grad_norm": 3.894430637359619,
        "learning_rate": 0.00015059231818411917,
        "epoch": 0.224,
        "step": 1680
    },
    {
        "loss": 2.5358,
        "grad_norm": 5.447006702423096,
        "learning_rate": 0.00015053802028003804,
        "epoch": 0.22413333333333332,
        "step": 1681
    },
    {
        "loss": 2.0859,
        "grad_norm": 5.117911338806152,
        "learning_rate": 0.00015048370235767487,
        "epoch": 0.22426666666666667,
        "step": 1682
    },
    {
        "loss": 2.0045,
        "grad_norm": 3.542219400405884,
        "learning_rate": 0.00015042936443854517,
        "epoch": 0.2244,
        "step": 1683
    },
    {
        "loss": 2.897,
        "grad_norm": 3.035949945449829,
        "learning_rate": 0.00015037500654417239,
        "epoch": 0.22453333333333333,
        "step": 1684
    },
    {
        "loss": 2.0054,
        "grad_norm": 4.563759803771973,
        "learning_rate": 0.00015032062869608788,
        "epoch": 0.22466666666666665,
        "step": 1685
    },
    {
        "loss": 2.4598,
        "grad_norm": 3.96958065032959,
        "learning_rate": 0.00015026623091583086,
        "epoch": 0.2248,
        "step": 1686
    },
    {
        "loss": 2.5727,
        "grad_norm": 2.9176807403564453,
        "learning_rate": 0.00015021181322494848,
        "epoch": 0.22493333333333335,
        "step": 1687
    },
    {
        "loss": 2.6971,
        "grad_norm": 4.683359146118164,
        "learning_rate": 0.00015015737564499584,
        "epoch": 0.22506666666666666,
        "step": 1688
    },
    {
        "loss": 1.4634,
        "grad_norm": 5.135146141052246,
        "learning_rate": 0.00015010291819753574,
        "epoch": 0.2252,
        "step": 1689
    },
    {
        "loss": 1.7331,
        "grad_norm": 4.07649564743042,
        "learning_rate": 0.00015004844090413908,
        "epoch": 0.22533333333333333,
        "step": 1690
    },
    {
        "loss": 2.5899,
        "grad_norm": 3.951146364212036,
        "learning_rate": 0.00014999394378638442,
        "epoch": 0.22546666666666668,
        "step": 1691
    },
    {
        "loss": 1.5684,
        "grad_norm": 4.509937763214111,
        "learning_rate": 0.00014993942686585826,
        "epoch": 0.2256,
        "step": 1692
    },
    {
        "loss": 2.3662,
        "grad_norm": 2.9077181816101074,
        "learning_rate": 0.00014988489016415496,
        "epoch": 0.22573333333333334,
        "step": 1693
    },
    {
        "loss": 3.0574,
        "grad_norm": 2.790311813354492,
        "learning_rate": 0.0001498303337028767,
        "epoch": 0.22586666666666666,
        "step": 1694
    },
    {
        "loss": 2.2924,
        "grad_norm": 3.6953744888305664,
        "learning_rate": 0.00014977575750363346,
        "epoch": 0.226,
        "step": 1695
    },
    {
        "loss": 1.068,
        "grad_norm": 3.493166923522949,
        "learning_rate": 0.0001497211615880431,
        "epoch": 0.22613333333333333,
        "step": 1696
    },
    {
        "loss": 2.0574,
        "grad_norm": 4.121336460113525,
        "learning_rate": 0.00014966654597773114,
        "epoch": 0.22626666666666667,
        "step": 1697
    },
    {
        "loss": 2.3641,
        "grad_norm": 2.6289825439453125,
        "learning_rate": 0.00014961191069433113,
        "epoch": 0.2264,
        "step": 1698
    },
    {
        "loss": 2.4582,
        "grad_norm": 2.9757351875305176,
        "learning_rate": 0.00014955725575948426,
        "epoch": 0.22653333333333334,
        "step": 1699
    },
    {
        "loss": 2.05,
        "grad_norm": 4.8902812004089355,
        "learning_rate": 0.0001495025811948395,
        "epoch": 0.22666666666666666,
        "step": 1700
    },
    {
        "loss": 1.7577,
        "grad_norm": 3.3874101638793945,
        "learning_rate": 0.0001494478870220536,
        "epoch": 0.2268,
        "step": 1701
    },
    {
        "loss": 1.6409,
        "grad_norm": 3.9200222492218018,
        "learning_rate": 0.00014939317326279126,
        "epoch": 0.22693333333333332,
        "step": 1702
    },
    {
        "loss": 1.5286,
        "grad_norm": 4.801708221435547,
        "learning_rate": 0.00014933843993872466,
        "epoch": 0.22706666666666667,
        "step": 1703
    },
    {
        "loss": 1.184,
        "grad_norm": 3.9210622310638428,
        "learning_rate": 0.00014928368707153384,
        "epoch": 0.2272,
        "step": 1704
    },
    {
        "loss": 2.9008,
        "grad_norm": 2.8035624027252197,
        "learning_rate": 0.00014922891468290668,
        "epoch": 0.22733333333333333,
        "step": 1705
    },
    {
        "loss": 1.3912,
        "grad_norm": 3.654473066329956,
        "learning_rate": 0.00014917412279453863,
        "epoch": 0.22746666666666668,
        "step": 1706
    },
    {
        "loss": 2.6421,
        "grad_norm": 2.241915702819824,
        "learning_rate": 0.00014911931142813303,
        "epoch": 0.2276,
        "step": 1707
    },
    {
        "loss": 1.9166,
        "grad_norm": 3.199263572692871,
        "learning_rate": 0.00014906448060540077,
        "epoch": 0.22773333333333334,
        "step": 1708
    },
    {
        "loss": 2.4237,
        "grad_norm": 2.3810465335845947,
        "learning_rate": 0.00014900963034806058,
        "epoch": 0.22786666666666666,
        "step": 1709
    },
    {
        "loss": 1.8681,
        "grad_norm": 3.297461986541748,
        "learning_rate": 0.0001489547606778388,
        "epoch": 0.228,
        "step": 1710
    },
    {
        "loss": 2.6952,
        "grad_norm": 2.368588924407959,
        "learning_rate": 0.0001488998716164695,
        "epoch": 0.22813333333333333,
        "step": 1711
    },
    {
        "loss": 2.5981,
        "grad_norm": 3.90861177444458,
        "learning_rate": 0.00014884496318569445,
        "epoch": 0.22826666666666667,
        "step": 1712
    },
    {
        "loss": 2.8455,
        "grad_norm": 2.337895393371582,
        "learning_rate": 0.00014879003540726305,
        "epoch": 0.2284,
        "step": 1713
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.4636118412017822,
        "learning_rate": 0.00014873508830293237,
        "epoch": 0.22853333333333334,
        "step": 1714
    },
    {
        "loss": 2.6332,
        "grad_norm": 2.739206075668335,
        "learning_rate": 0.00014868012189446717,
        "epoch": 0.22866666666666666,
        "step": 1715
    },
    {
        "loss": 2.5567,
        "grad_norm": 2.1791348457336426,
        "learning_rate": 0.00014862513620363978,
        "epoch": 0.2288,
        "step": 1716
    },
    {
        "loss": 2.4644,
        "grad_norm": 3.761929750442505,
        "learning_rate": 0.00014857013125223024,
        "epoch": 0.22893333333333332,
        "step": 1717
    },
    {
        "loss": 2.823,
        "grad_norm": 2.963804244995117,
        "learning_rate": 0.00014851510706202626,
        "epoch": 0.22906666666666667,
        "step": 1718
    },
    {
        "loss": 1.0205,
        "grad_norm": 3.0294103622436523,
        "learning_rate": 0.00014846006365482306,
        "epoch": 0.2292,
        "step": 1719
    },
    {
        "loss": 2.2515,
        "grad_norm": 2.8611977100372314,
        "learning_rate": 0.0001484050010524235,
        "epoch": 0.22933333333333333,
        "step": 1720
    },
    {
        "loss": 1.4996,
        "grad_norm": 4.102985858917236,
        "learning_rate": 0.00014834991927663811,
        "epoch": 0.22946666666666668,
        "step": 1721
    },
    {
        "loss": 2.399,
        "grad_norm": 2.889512300491333,
        "learning_rate": 0.0001482948183492849,
        "epoch": 0.2296,
        "step": 1722
    },
    {
        "loss": 2.3205,
        "grad_norm": 3.068298816680908,
        "learning_rate": 0.00014823969829218963,
        "epoch": 0.22973333333333334,
        "step": 1723
    },
    {
        "loss": 0.9105,
        "grad_norm": 5.613226413726807,
        "learning_rate": 0.00014818455912718549,
        "epoch": 0.22986666666666666,
        "step": 1724
    },
    {
        "loss": 2.2794,
        "grad_norm": 3.0765626430511475,
        "learning_rate": 0.00014812940087611325,
        "epoch": 0.23,
        "step": 1725
    },
    {
        "loss": 2.6339,
        "grad_norm": 3.1001031398773193,
        "learning_rate": 0.00014807422356082138,
        "epoch": 0.23013333333333333,
        "step": 1726
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.997911214828491,
        "learning_rate": 0.00014801902720316568,
        "epoch": 0.23026666666666668,
        "step": 1727
    },
    {
        "loss": 1.0769,
        "grad_norm": 4.875155925750732,
        "learning_rate": 0.00014796381182500973,
        "epoch": 0.2304,
        "step": 1728
    },
    {
        "loss": 1.1144,
        "grad_norm": 4.978629112243652,
        "learning_rate": 0.0001479085774482245,
        "epoch": 0.23053333333333334,
        "step": 1729
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.9511380195617676,
        "learning_rate": 0.0001478533240946885,
        "epoch": 0.23066666666666666,
        "step": 1730
    },
    {
        "loss": 2.8496,
        "grad_norm": 4.009517192840576,
        "learning_rate": 0.00014779805178628772,
        "epoch": 0.2308,
        "step": 1731
    },
    {
        "loss": 1.7986,
        "grad_norm": 2.6160078048706055,
        "learning_rate": 0.0001477427605449158,
        "epoch": 0.23093333333333332,
        "step": 1732
    },
    {
        "loss": 1.9697,
        "grad_norm": 3.7318968772888184,
        "learning_rate": 0.00014768745039247372,
        "epoch": 0.23106666666666667,
        "step": 1733
    },
    {
        "loss": 2.6256,
        "grad_norm": 4.827157020568848,
        "learning_rate": 0.00014763212135087008,
        "epoch": 0.2312,
        "step": 1734
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.184849977493286,
        "learning_rate": 0.00014757677344202086,
        "epoch": 0.23133333333333334,
        "step": 1735
    },
    {
        "loss": 2.004,
        "grad_norm": 3.015719175338745,
        "learning_rate": 0.00014752140668784956,
        "epoch": 0.23146666666666665,
        "step": 1736
    },
    {
        "loss": 2.9486,
        "grad_norm": 4.349664688110352,
        "learning_rate": 0.00014746602111028714,
        "epoch": 0.2316,
        "step": 1737
    },
    {
        "loss": 3.126,
        "grad_norm": 5.013046741485596,
        "learning_rate": 0.000147410616731272,
        "epoch": 0.23173333333333335,
        "step": 1738
    },
    {
        "loss": 2.3334,
        "grad_norm": 3.267658233642578,
        "learning_rate": 0.00014735519357275006,
        "epoch": 0.23186666666666667,
        "step": 1739
    },
    {
        "loss": 2.2153,
        "grad_norm": 2.692305326461792,
        "learning_rate": 0.0001472997516566746,
        "epoch": 0.232,
        "step": 1740
    },
    {
        "loss": 2.5706,
        "grad_norm": 3.8560431003570557,
        "learning_rate": 0.00014724429100500635,
        "epoch": 0.23213333333333333,
        "step": 1741
    },
    {
        "loss": 2.1303,
        "grad_norm": 2.424804210662842,
        "learning_rate": 0.0001471888116397134,
        "epoch": 0.23226666666666668,
        "step": 1742
    },
    {
        "loss": 2.503,
        "grad_norm": 2.8251049518585205,
        "learning_rate": 0.00014713331358277138,
        "epoch": 0.2324,
        "step": 1743
    },
    {
        "loss": 1.6676,
        "grad_norm": 3.160031318664551,
        "learning_rate": 0.00014707779685616326,
        "epoch": 0.23253333333333334,
        "step": 1744
    },
    {
        "loss": 2.3722,
        "grad_norm": 3.1673057079315186,
        "learning_rate": 0.00014702226148187938,
        "epoch": 0.23266666666666666,
        "step": 1745
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.8733463287353516,
        "learning_rate": 0.0001469667074819175,
        "epoch": 0.2328,
        "step": 1746
    },
    {
        "loss": 1.8452,
        "grad_norm": 3.045271158218384,
        "learning_rate": 0.00014691113487828278,
        "epoch": 0.23293333333333333,
        "step": 1747
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.756844997406006,
        "learning_rate": 0.00014685554369298767,
        "epoch": 0.23306666666666667,
        "step": 1748
    },
    {
        "loss": 2.3969,
        "grad_norm": 3.070301055908203,
        "learning_rate": 0.00014679993394805204,
        "epoch": 0.2332,
        "step": 1749
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.178913116455078,
        "learning_rate": 0.0001467443056655031,
        "epoch": 0.23333333333333334,
        "step": 1750
    },
    {
        "loss": 1.3675,
        "grad_norm": 4.121017932891846,
        "learning_rate": 0.00014668865886737547,
        "epoch": 0.23346666666666666,
        "step": 1751
    },
    {
        "loss": 2.8496,
        "grad_norm": 3.3669466972351074,
        "learning_rate": 0.0001466329935757109,
        "epoch": 0.2336,
        "step": 1752
    },
    {
        "loss": 1.7919,
        "grad_norm": 3.443681001663208,
        "learning_rate": 0.0001465773098125587,
        "epoch": 0.23373333333333332,
        "step": 1753
    },
    {
        "loss": 2.9238,
        "grad_norm": 1.9810645580291748,
        "learning_rate": 0.0001465216075999754,
        "epoch": 0.23386666666666667,
        "step": 1754
    },
    {
        "loss": 2.0661,
        "grad_norm": 3.0674455165863037,
        "learning_rate": 0.0001464658869600248,
        "epoch": 0.234,
        "step": 1755
    },
    {
        "loss": 2.9181,
        "grad_norm": 2.576692581176758,
        "learning_rate": 0.000146410147914778,
        "epoch": 0.23413333333333333,
        "step": 1756
    },
    {
        "loss": 3.2203,
        "grad_norm": 3.002478837966919,
        "learning_rate": 0.00014635439048631352,
        "epoch": 0.23426666666666668,
        "step": 1757
    },
    {
        "loss": 2.6456,
        "grad_norm": 3.633967638015747,
        "learning_rate": 0.00014629861469671703,
        "epoch": 0.2344,
        "step": 1758
    },
    {
        "loss": 1.2237,
        "grad_norm": 3.34061598777771,
        "learning_rate": 0.00014624282056808146,
        "epoch": 0.23453333333333334,
        "step": 1759
    },
    {
        "loss": 2.6725,
        "grad_norm": 2.586233377456665,
        "learning_rate": 0.0001461870081225071,
        "epoch": 0.23466666666666666,
        "step": 1760
    },
    {
        "loss": 2.5947,
        "grad_norm": 2.5658044815063477,
        "learning_rate": 0.00014613117738210147,
        "epoch": 0.2348,
        "step": 1761
    },
    {
        "loss": 2.8367,
        "grad_norm": 3.873323917388916,
        "learning_rate": 0.0001460753283689793,
        "epoch": 0.23493333333333333,
        "step": 1762
    },
    {
        "loss": 2.7417,
        "grad_norm": 2.006551742553711,
        "learning_rate": 0.00014601946110526253,
        "epoch": 0.23506666666666667,
        "step": 1763
    },
    {
        "loss": 2.3899,
        "grad_norm": 2.681583881378174,
        "learning_rate": 0.00014596357561308043,
        "epoch": 0.2352,
        "step": 1764
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.4760091304779053,
        "learning_rate": 0.0001459076719145694,
        "epoch": 0.23533333333333334,
        "step": 1765
    },
    {
        "loss": 2.4599,
        "grad_norm": 3.4528276920318604,
        "learning_rate": 0.00014585175003187307,
        "epoch": 0.23546666666666666,
        "step": 1766
    },
    {
        "loss": 2.0951,
        "grad_norm": 3.4587535858154297,
        "learning_rate": 0.00014579580998714238,
        "epoch": 0.2356,
        "step": 1767
    },
    {
        "loss": 1.1247,
        "grad_norm": 3.3738577365875244,
        "learning_rate": 0.00014573985180253525,
        "epoch": 0.23573333333333332,
        "step": 1768
    },
    {
        "loss": 2.9492,
        "grad_norm": 3.0503857135772705,
        "learning_rate": 0.000145683875500217,
        "epoch": 0.23586666666666667,
        "step": 1769
    },
    {
        "loss": 2.063,
        "grad_norm": 4.232024669647217,
        "learning_rate": 0.00014562788110236,
        "epoch": 0.236,
        "step": 1770
    },
    {
        "loss": 2.2625,
        "grad_norm": 2.3886871337890625,
        "learning_rate": 0.0001455718686311438,
        "epoch": 0.23613333333333333,
        "step": 1771
    },
    {
        "loss": 2.205,
        "grad_norm": 3.156278610229492,
        "learning_rate": 0.0001455158381087552,
        "epoch": 0.23626666666666668,
        "step": 1772
    },
    {
        "loss": 2.5856,
        "grad_norm": 2.7586042881011963,
        "learning_rate": 0.000145459789557388,
        "epoch": 0.2364,
        "step": 1773
    },
    {
        "loss": 2.9867,
        "grad_norm": 2.4850974082946777,
        "learning_rate": 0.00014540372299924327,
        "epoch": 0.23653333333333335,
        "step": 1774
    },
    {
        "loss": 3.0711,
        "grad_norm": 4.880591869354248,
        "learning_rate": 0.00014534763845652912,
        "epoch": 0.23666666666666666,
        "step": 1775
    },
    {
        "loss": 2.7466,
        "grad_norm": 3.5403785705566406,
        "learning_rate": 0.00014529153595146087,
        "epoch": 0.2368,
        "step": 1776
    },
    {
        "loss": 1.6874,
        "grad_norm": 3.6811254024505615,
        "learning_rate": 0.0001452354155062609,
        "epoch": 0.23693333333333333,
        "step": 1777
    },
    {
        "loss": 2.8388,
        "grad_norm": 3.577320098876953,
        "learning_rate": 0.00014517927714315876,
        "epoch": 0.23706666666666668,
        "step": 1778
    },
    {
        "loss": 2.7867,
        "grad_norm": 3.5276801586151123,
        "learning_rate": 0.00014512312088439094,
        "epoch": 0.2372,
        "step": 1779
    },
    {
        "loss": 0.6028,
        "grad_norm": 3.5171046257019043,
        "learning_rate": 0.0001450669467522012,
        "epoch": 0.23733333333333334,
        "step": 1780
    },
    {
        "loss": 1.5554,
        "grad_norm": 3.2817113399505615,
        "learning_rate": 0.0001450107547688403,
        "epoch": 0.23746666666666666,
        "step": 1781
    },
    {
        "loss": 2.1249,
        "grad_norm": 2.189760684967041,
        "learning_rate": 0.00014495454495656607,
        "epoch": 0.2376,
        "step": 1782
    },
    {
        "loss": 1.8267,
        "grad_norm": 3.1432442665100098,
        "learning_rate": 0.0001448983173376434,
        "epoch": 0.23773333333333332,
        "step": 1783
    },
    {
        "loss": 2.5441,
        "grad_norm": 3.1874890327453613,
        "learning_rate": 0.00014484207193434426,
        "epoch": 0.23786666666666667,
        "step": 1784
    },
    {
        "loss": 2.4692,
        "grad_norm": 3.5469937324523926,
        "learning_rate": 0.0001447858087689476,
        "epoch": 0.238,
        "step": 1785
    },
    {
        "loss": 1.4516,
        "grad_norm": 4.320339202880859,
        "learning_rate": 0.0001447295278637395,
        "epoch": 0.23813333333333334,
        "step": 1786
    },
    {
        "loss": 2.5538,
        "grad_norm": 4.29567813873291,
        "learning_rate": 0.00014467322924101297,
        "epoch": 0.23826666666666665,
        "step": 1787
    },
    {
        "loss": 0.9834,
        "grad_norm": 3.5299572944641113,
        "learning_rate": 0.00014461691292306817,
        "epoch": 0.2384,
        "step": 1788
    },
    {
        "loss": 2.485,
        "grad_norm": 3.020512104034424,
        "learning_rate": 0.0001445605789322121,
        "epoch": 0.23853333333333335,
        "step": 1789
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.801036834716797,
        "learning_rate": 0.00014450422729075888,
        "epoch": 0.23866666666666667,
        "step": 1790
    },
    {
        "loss": 1.8563,
        "grad_norm": 4.230000019073486,
        "learning_rate": 0.0001444478580210296,
        "epoch": 0.2388,
        "step": 1791
    },
    {
        "loss": 2.3393,
        "grad_norm": 2.2735862731933594,
        "learning_rate": 0.00014439147114535229,
        "epoch": 0.23893333333333333,
        "step": 1792
    },
    {
        "loss": 0.9416,
        "grad_norm": 2.9023048877716064,
        "learning_rate": 0.000144335066686062,
        "epoch": 0.23906666666666668,
        "step": 1793
    },
    {
        "loss": 2.5041,
        "grad_norm": 2.8464128971099854,
        "learning_rate": 0.0001442786446655007,
        "epoch": 0.2392,
        "step": 1794
    },
    {
        "loss": 2.48,
        "grad_norm": 3.8851287364959717,
        "learning_rate": 0.00014422220510601746,
        "epoch": 0.23933333333333334,
        "step": 1795
    },
    {
        "loss": 2.2671,
        "grad_norm": 4.519399642944336,
        "learning_rate": 0.00014416574802996802,
        "epoch": 0.23946666666666666,
        "step": 1796
    },
    {
        "loss": 1.0061,
        "grad_norm": 4.456841468811035,
        "learning_rate": 0.00014410927345971533,
        "epoch": 0.2396,
        "step": 1797
    },
    {
        "loss": 0.9821,
        "grad_norm": 3.4454944133758545,
        "learning_rate": 0.00014405278141762907,
        "epoch": 0.23973333333333333,
        "step": 1798
    },
    {
        "loss": 2.3298,
        "grad_norm": 4.231146812438965,
        "learning_rate": 0.00014399627192608603,
        "epoch": 0.23986666666666667,
        "step": 1799
    },
    {
        "loss": 2.7717,
        "grad_norm": 3.297224998474121,
        "learning_rate": 0.0001439397450074698,
        "epoch": 0.24,
        "step": 1800
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.656038284301758,
        "learning_rate": 0.00014388320068417078,
        "epoch": 0.24013333333333334,
        "step": 1801
    },
    {
        "loss": 0.778,
        "grad_norm": 2.8861894607543945,
        "learning_rate": 0.00014382663897858646,
        "epoch": 0.24026666666666666,
        "step": 1802
    },
    {
        "loss": 2.2883,
        "grad_norm": 4.566510200500488,
        "learning_rate": 0.00014377005991312114,
        "epoch": 0.2404,
        "step": 1803
    },
    {
        "loss": 2.6741,
        "grad_norm": 3.759068012237549,
        "learning_rate": 0.00014371346351018592,
        "epoch": 0.24053333333333332,
        "step": 1804
    },
    {
        "loss": 2.6006,
        "grad_norm": 3.234970808029175,
        "learning_rate": 0.00014365684979219886,
        "epoch": 0.24066666666666667,
        "step": 1805
    },
    {
        "loss": 2.6801,
        "grad_norm": 4.309480667114258,
        "learning_rate": 0.0001436002187815848,
        "epoch": 0.2408,
        "step": 1806
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.0883100032806396,
        "learning_rate": 0.00014354357050077555,
        "epoch": 0.24093333333333333,
        "step": 1807
    },
    {
        "loss": 2.0002,
        "grad_norm": 2.7803235054016113,
        "learning_rate": 0.00014348690497220968,
        "epoch": 0.24106666666666668,
        "step": 1808
    },
    {
        "loss": 2.1961,
        "grad_norm": 3.3600971698760986,
        "learning_rate": 0.00014343022221833252,
        "epoch": 0.2412,
        "step": 1809
    },
    {
        "loss": 1.8329,
        "grad_norm": 6.644264221191406,
        "learning_rate": 0.00014337352226159638,
        "epoch": 0.24133333333333334,
        "step": 1810
    },
    {
        "loss": 2.374,
        "grad_norm": 3.006643772125244,
        "learning_rate": 0.00014331680512446031,
        "epoch": 0.24146666666666666,
        "step": 1811
    },
    {
        "loss": 3.2472,
        "grad_norm": 4.612018585205078,
        "learning_rate": 0.0001432600708293901,
        "epoch": 0.2416,
        "step": 1812
    },
    {
        "loss": 2.0839,
        "grad_norm": 5.738061904907227,
        "learning_rate": 0.0001432033193988584,
        "epoch": 0.24173333333333333,
        "step": 1813
    },
    {
        "loss": 2.1501,
        "grad_norm": 6.737957000732422,
        "learning_rate": 0.00014314655085534477,
        "epoch": 0.24186666666666667,
        "step": 1814
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.9133905172348022,
        "learning_rate": 0.00014308976522133524,
        "epoch": 0.242,
        "step": 1815
    },
    {
        "loss": 1.7932,
        "grad_norm": 1.343211054801941,
        "learning_rate": 0.0001430329625193229,
        "epoch": 0.24213333333333334,
        "step": 1816
    },
    {
        "loss": 2.0482,
        "grad_norm": 3.7879955768585205,
        "learning_rate": 0.0001429761427718075,
        "epoch": 0.24226666666666666,
        "step": 1817
    },
    {
        "loss": 2.7431,
        "grad_norm": 3.1832361221313477,
        "learning_rate": 0.00014291930600129546,
        "epoch": 0.2424,
        "step": 1818
    },
    {
        "loss": 2.4215,
        "grad_norm": 3.4809677600860596,
        "learning_rate": 0.0001428624522303001,
        "epoch": 0.24253333333333332,
        "step": 1819
    },
    {
        "loss": 2.418,
        "grad_norm": 2.5025081634521484,
        "learning_rate": 0.00014280558148134137,
        "epoch": 0.24266666666666667,
        "step": 1820
    },
    {
        "loss": 2.2043,
        "grad_norm": 4.470690727233887,
        "learning_rate": 0.00014274869377694596,
        "epoch": 0.2428,
        "step": 1821
    },
    {
        "loss": 2.1573,
        "grad_norm": 2.240704298019409,
        "learning_rate": 0.00014269178913964726,
        "epoch": 0.24293333333333333,
        "step": 1822
    },
    {
        "loss": 2.3945,
        "grad_norm": 2.2635159492492676,
        "learning_rate": 0.00014263486759198544,
        "epoch": 0.24306666666666665,
        "step": 1823
    },
    {
        "loss": 2.0324,
        "grad_norm": 4.246941089630127,
        "learning_rate": 0.00014257792915650728,
        "epoch": 0.2432,
        "step": 1824
    },
    {
        "loss": 2.6841,
        "grad_norm": 3.4508447647094727,
        "learning_rate": 0.00014252097385576633,
        "epoch": 0.24333333333333335,
        "step": 1825
    },
    {
        "loss": 2.5617,
        "grad_norm": 3.092684268951416,
        "learning_rate": 0.00014246400171232266,
        "epoch": 0.24346666666666666,
        "step": 1826
    },
    {
        "loss": 2.4473,
        "grad_norm": 2.982774496078491,
        "learning_rate": 0.0001424070127487433,
        "epoch": 0.2436,
        "step": 1827
    },
    {
        "loss": 1.8793,
        "grad_norm": 4.814176082611084,
        "learning_rate": 0.00014235000698760165,
        "epoch": 0.24373333333333333,
        "step": 1828
    },
    {
        "loss": 2.0476,
        "grad_norm": 3.811737060546875,
        "learning_rate": 0.00014229298445147796,
        "epoch": 0.24386666666666668,
        "step": 1829
    },
    {
        "loss": 2.6457,
        "grad_norm": 4.339118957519531,
        "learning_rate": 0.00014223594516295902,
        "epoch": 0.244,
        "step": 1830
    },
    {
        "loss": 2.0767,
        "grad_norm": 3.371333599090576,
        "learning_rate": 0.00014217888914463828,
        "epoch": 0.24413333333333334,
        "step": 1831
    },
    {
        "loss": 1.2005,
        "grad_norm": 4.886441707611084,
        "learning_rate": 0.0001421218164191158,
        "epoch": 0.24426666666666666,
        "step": 1832
    },
    {
        "loss": 2.6354,
        "grad_norm": 3.591231346130371,
        "learning_rate": 0.00014206472700899835,
        "epoch": 0.2444,
        "step": 1833
    },
    {
        "loss": 2.7478,
        "grad_norm": 3.530301094055176,
        "learning_rate": 0.00014200762093689916,
        "epoch": 0.24453333333333332,
        "step": 1834
    },
    {
        "loss": 2.5993,
        "grad_norm": 2.474461555480957,
        "learning_rate": 0.0001419504982254382,
        "epoch": 0.24466666666666667,
        "step": 1835
    },
    {
        "loss": 2.8766,
        "grad_norm": 1.689873456954956,
        "learning_rate": 0.00014189335889724193,
        "epoch": 0.2448,
        "step": 1836
    },
    {
        "loss": 2.1921,
        "grad_norm": 3.375614881515503,
        "learning_rate": 0.0001418362029749435,
        "epoch": 0.24493333333333334,
        "step": 1837
    },
    {
        "loss": 1.2874,
        "grad_norm": 3.8531103134155273,
        "learning_rate": 0.00014177903048118252,
        "epoch": 0.24506666666666665,
        "step": 1838
    },
    {
        "loss": 1.7341,
        "grad_norm": 4.266528129577637,
        "learning_rate": 0.00014172184143860513,
        "epoch": 0.2452,
        "step": 1839
    },
    {
        "loss": 2.3837,
        "grad_norm": 5.449851036071777,
        "learning_rate": 0.00014166463586986423,
        "epoch": 0.24533333333333332,
        "step": 1840
    },
    {
        "loss": 2.8269,
        "grad_norm": 2.9437692165374756,
        "learning_rate": 0.00014160741379761914,
        "epoch": 0.24546666666666667,
        "step": 1841
    },
    {
        "loss": 2.1622,
        "grad_norm": 3.0549681186676025,
        "learning_rate": 0.00014155017524453562,
        "epoch": 0.2456,
        "step": 1842
    },
    {
        "loss": 2.7471,
        "grad_norm": 2.799422025680542,
        "learning_rate": 0.0001414929202332861,
        "epoch": 0.24573333333333333,
        "step": 1843
    },
    {
        "loss": 0.639,
        "grad_norm": 2.794468641281128,
        "learning_rate": 0.0001414356487865495,
        "epoch": 0.24586666666666668,
        "step": 1844
    },
    {
        "loss": 2.9029,
        "grad_norm": 2.988008975982666,
        "learning_rate": 0.00014137836092701123,
        "epoch": 0.246,
        "step": 1845
    },
    {
        "loss": 1.4014,
        "grad_norm": 3.9189536571502686,
        "learning_rate": 0.00014132105667736319,
        "epoch": 0.24613333333333334,
        "step": 1846
    },
    {
        "loss": 2.8609,
        "grad_norm": 2.8107550144195557,
        "learning_rate": 0.0001412637360603038,
        "epoch": 0.24626666666666666,
        "step": 1847
    },
    {
        "loss": 2.891,
        "grad_norm": 3.6302905082702637,
        "learning_rate": 0.000141206399098538,
        "epoch": 0.2464,
        "step": 1848
    },
    {
        "loss": 2.5562,
        "grad_norm": 4.1130595207214355,
        "learning_rate": 0.00014114904581477708,
        "epoch": 0.24653333333333333,
        "step": 1849
    },
    {
        "loss": 2.4465,
        "grad_norm": 2.7543623447418213,
        "learning_rate": 0.0001410916762317389,
        "epoch": 0.24666666666666667,
        "step": 1850
    },
    {
        "loss": 1.679,
        "grad_norm": 6.169337749481201,
        "learning_rate": 0.00014103429037214772,
        "epoch": 0.2468,
        "step": 1851
    },
    {
        "loss": 2.5415,
        "grad_norm": 3.739091157913208,
        "learning_rate": 0.00014097688825873437,
        "epoch": 0.24693333333333334,
        "step": 1852
    },
    {
        "loss": 2.9113,
        "grad_norm": 2.2021372318267822,
        "learning_rate": 0.0001409194699142359,
        "epoch": 0.24706666666666666,
        "step": 1853
    },
    {
        "loss": 2.2377,
        "grad_norm": 4.006032943725586,
        "learning_rate": 0.000140862035361396,
        "epoch": 0.2472,
        "step": 1854
    },
    {
        "loss": 2.5519,
        "grad_norm": 3.3466153144836426,
        "learning_rate": 0.00014080458462296464,
        "epoch": 0.24733333333333332,
        "step": 1855
    },
    {
        "loss": 1.2531,
        "grad_norm": 4.566684246063232,
        "learning_rate": 0.0001407471177216983,
        "epoch": 0.24746666666666667,
        "step": 1856
    },
    {
        "loss": 1.7451,
        "grad_norm": 3.613150119781494,
        "learning_rate": 0.00014068963468035972,
        "epoch": 0.2476,
        "step": 1857
    },
    {
        "loss": 1.5041,
        "grad_norm": 3.260779857635498,
        "learning_rate": 0.00014063213552171827,
        "epoch": 0.24773333333333333,
        "step": 1858
    },
    {
        "loss": 1.9715,
        "grad_norm": 3.4128293991088867,
        "learning_rate": 0.00014057462026854944,
        "epoch": 0.24786666666666668,
        "step": 1859
    },
    {
        "loss": 1.2696,
        "grad_norm": 5.213812351226807,
        "learning_rate": 0.00014051708894363528,
        "epoch": 0.248,
        "step": 1860
    },
    {
        "loss": 2.0664,
        "grad_norm": 2.6691207885742188,
        "learning_rate": 0.00014045954156976414,
        "epoch": 0.24813333333333334,
        "step": 1861
    },
    {
        "loss": 2.7924,
        "grad_norm": 2.4866042137145996,
        "learning_rate": 0.00014040197816973072,
        "epoch": 0.24826666666666666,
        "step": 1862
    },
    {
        "loss": 2.7017,
        "grad_norm": 4.121921062469482,
        "learning_rate": 0.00014034439876633612,
        "epoch": 0.2484,
        "step": 1863
    },
    {
        "loss": 2.0815,
        "grad_norm": 4.655452728271484,
        "learning_rate": 0.00014028680338238765,
        "epoch": 0.24853333333333333,
        "step": 1864
    },
    {
        "loss": 2.2001,
        "grad_norm": 3.8656795024871826,
        "learning_rate": 0.0001402291920406991,
        "epoch": 0.24866666666666667,
        "step": 1865
    },
    {
        "loss": 3.0155,
        "grad_norm": 2.080655336380005,
        "learning_rate": 0.00014017156476409047,
        "epoch": 0.2488,
        "step": 1866
    },
    {
        "loss": 2.4587,
        "grad_norm": 3.2387492656707764,
        "learning_rate": 0.0001401139215753882,
        "epoch": 0.24893333333333334,
        "step": 1867
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.908402442932129,
        "learning_rate": 0.00014005626249742487,
        "epoch": 0.24906666666666666,
        "step": 1868
    },
    {
        "loss": 2.3507,
        "grad_norm": 2.7331924438476562,
        "learning_rate": 0.00013999858755303952,
        "epoch": 0.2492,
        "step": 1869
    },
    {
        "loss": 1.1378,
        "grad_norm": 3.584031581878662,
        "learning_rate": 0.0001399408967650773,
        "epoch": 0.24933333333333332,
        "step": 1870
    },
    {
        "loss": 1.8992,
        "grad_norm": 3.271162271499634,
        "learning_rate": 0.00013988319015638984,
        "epoch": 0.24946666666666667,
        "step": 1871
    },
    {
        "loss": 1.9757,
        "grad_norm": 3.2210702896118164,
        "learning_rate": 0.00013982546774983482,
        "epoch": 0.2496,
        "step": 1872
    },
    {
        "loss": 1.5021,
        "grad_norm": 5.240738868713379,
        "learning_rate": 0.00013976772956827632,
        "epoch": 0.24973333333333333,
        "step": 1873
    },
    {
        "loss": 1.2451,
        "grad_norm": 3.3023109436035156,
        "learning_rate": 0.00013970997563458466,
        "epoch": 0.24986666666666665,
        "step": 1874
    },
    {
        "loss": 2.426,
        "grad_norm": 4.587125778198242,
        "learning_rate": 0.0001396522059716363,
        "epoch": 0.25,
        "step": 1875
    },
    {
        "loss": 2.9286,
        "grad_norm": 2.3117589950561523,
        "learning_rate": 0.00013959442060231406,
        "epoch": 0.2501333333333333,
        "step": 1876
    },
    {
        "loss": 2.3751,
        "grad_norm": 4.039011478424072,
        "learning_rate": 0.0001395366195495069,
        "epoch": 0.2502666666666667,
        "step": 1877
    },
    {
        "loss": 2.001,
        "grad_norm": 4.853797912597656,
        "learning_rate": 0.00013947880283611005,
        "epoch": 0.2504,
        "step": 1878
    },
    {
        "loss": 2.8943,
        "grad_norm": 2.4348325729370117,
        "learning_rate": 0.00013942097048502488,
        "epoch": 0.25053333333333333,
        "step": 1879
    },
    {
        "loss": 1.4224,
        "grad_norm": 3.8832504749298096,
        "learning_rate": 0.00013936312251915895,
        "epoch": 0.25066666666666665,
        "step": 1880
    },
    {
        "loss": 1.504,
        "grad_norm": 3.3814258575439453,
        "learning_rate": 0.00013930525896142606,
        "epoch": 0.2508,
        "step": 1881
    },
    {
        "loss": 2.157,
        "grad_norm": 2.8660545349121094,
        "learning_rate": 0.00013924737983474618,
        "epoch": 0.25093333333333334,
        "step": 1882
    },
    {
        "loss": 2.0558,
        "grad_norm": 3.142153263092041,
        "learning_rate": 0.0001391894851620454,
        "epoch": 0.25106666666666666,
        "step": 1883
    },
    {
        "loss": 2.5782,
        "grad_norm": 1.8159456253051758,
        "learning_rate": 0.00013913157496625602,
        "epoch": 0.2512,
        "step": 1884
    },
    {
        "loss": 1.7155,
        "grad_norm": 3.7397799491882324,
        "learning_rate": 0.00013907364927031647,
        "epoch": 0.25133333333333335,
        "step": 1885
    },
    {
        "loss": 1.1666,
        "grad_norm": 3.409766435623169,
        "learning_rate": 0.0001390157080971713,
        "epoch": 0.25146666666666667,
        "step": 1886
    },
    {
        "loss": 2.4135,
        "grad_norm": 3.4303829669952393,
        "learning_rate": 0.0001389577514697712,
        "epoch": 0.2516,
        "step": 1887
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.7436282634735107,
        "learning_rate": 0.000138899779411073,
        "epoch": 0.2517333333333333,
        "step": 1888
    },
    {
        "loss": 2.6838,
        "grad_norm": 2.5817718505859375,
        "learning_rate": 0.00013884179194403962,
        "epoch": 0.2518666666666667,
        "step": 1889
    },
    {
        "loss": 2.1658,
        "grad_norm": 3.7530157566070557,
        "learning_rate": 0.0001387837890916401,
        "epoch": 0.252,
        "step": 1890
    },
    {
        "loss": 1.8617,
        "grad_norm": 4.644089698791504,
        "learning_rate": 0.0001387257708768496,
        "epoch": 0.2521333333333333,
        "step": 1891
    },
    {
        "loss": 2.5888,
        "grad_norm": 2.485252857208252,
        "learning_rate": 0.00013866773732264926,
        "epoch": 0.25226666666666664,
        "step": 1892
    },
    {
        "loss": 2.4847,
        "grad_norm": 3.3592536449432373,
        "learning_rate": 0.00013860968845202644,
        "epoch": 0.2524,
        "step": 1893
    },
    {
        "loss": 1.785,
        "grad_norm": 3.2936251163482666,
        "learning_rate": 0.00013855162428797445,
        "epoch": 0.25253333333333333,
        "step": 1894
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.414628267288208,
        "learning_rate": 0.0001384935448534927,
        "epoch": 0.25266666666666665,
        "step": 1895
    },
    {
        "loss": 2.8701,
        "grad_norm": 3.0541815757751465,
        "learning_rate": 0.00013843545017158673,
        "epoch": 0.2528,
        "step": 1896
    },
    {
        "loss": 0.7999,
        "grad_norm": 4.077939510345459,
        "learning_rate": 0.00013837734026526791,
        "epoch": 0.25293333333333334,
        "step": 1897
    },
    {
        "loss": 2.7908,
        "grad_norm": 2.2070422172546387,
        "learning_rate": 0.00013831921515755388,
        "epoch": 0.25306666666666666,
        "step": 1898
    },
    {
        "loss": 1.9853,
        "grad_norm": 2.63081431388855,
        "learning_rate": 0.00013826107487146817,
        "epoch": 0.2532,
        "step": 1899
    },
    {
        "loss": 3.029,
        "grad_norm": 4.28003454208374,
        "learning_rate": 0.0001382029194300403,
        "epoch": 0.25333333333333335,
        "step": 1900
    },
    {
        "loss": 2.6472,
        "grad_norm": 3.2501957416534424,
        "learning_rate": 0.00013814474885630592,
        "epoch": 0.2534666666666667,
        "step": 1901
    },
    {
        "loss": 2.3329,
        "grad_norm": 3.616481065750122,
        "learning_rate": 0.00013808656317330646,
        "epoch": 0.2536,
        "step": 1902
    },
    {
        "loss": 2.5662,
        "grad_norm": 5.200764179229736,
        "learning_rate": 0.0001380283624040896,
        "epoch": 0.2537333333333333,
        "step": 1903
    },
    {
        "loss": 3.145,
        "grad_norm": 6.011047840118408,
        "learning_rate": 0.00013797014657170883,
        "epoch": 0.2538666666666667,
        "step": 1904
    },
    {
        "loss": 2.1864,
        "grad_norm": 3.0030484199523926,
        "learning_rate": 0.00013791191569922362,
        "epoch": 0.254,
        "step": 1905
    },
    {
        "loss": 2.3737,
        "grad_norm": 4.304084777832031,
        "learning_rate": 0.0001378536698096994,
        "epoch": 0.2541333333333333,
        "step": 1906
    },
    {
        "loss": 3.0303,
        "grad_norm": 3.26254940032959,
        "learning_rate": 0.00013779540892620756,
        "epoch": 0.25426666666666664,
        "step": 1907
    },
    {
        "loss": 2.1293,
        "grad_norm": 3.083005428314209,
        "learning_rate": 0.0001377371330718255,
        "epoch": 0.2544,
        "step": 1908
    },
    {
        "loss": 2.2876,
        "grad_norm": 2.4442617893218994,
        "learning_rate": 0.00013767884226963643,
        "epoch": 0.25453333333333333,
        "step": 1909
    },
    {
        "loss": 2.1769,
        "grad_norm": 2.816880226135254,
        "learning_rate": 0.00013762053654272957,
        "epoch": 0.25466666666666665,
        "step": 1910
    },
    {
        "loss": 2.4495,
        "grad_norm": 3.536616802215576,
        "learning_rate": 0.00013756221591419997,
        "epoch": 0.2548,
        "step": 1911
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.3728809356689453,
        "learning_rate": 0.00013750388040714863,
        "epoch": 0.25493333333333335,
        "step": 1912
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.962836980819702,
        "learning_rate": 0.0001374455300446825,
        "epoch": 0.25506666666666666,
        "step": 1913
    },
    {
        "loss": 2.1156,
        "grad_norm": 4.12087869644165,
        "learning_rate": 0.00013738716484991436,
        "epoch": 0.2552,
        "step": 1914
    },
    {
        "loss": 0.6276,
        "grad_norm": 2.9494690895080566,
        "learning_rate": 0.00013732878484596278,
        "epoch": 0.25533333333333336,
        "step": 1915
    },
    {
        "loss": 1.5001,
        "grad_norm": 5.569338321685791,
        "learning_rate": 0.00013727039005595235,
        "epoch": 0.2554666666666667,
        "step": 1916
    },
    {
        "loss": 1.7527,
        "grad_norm": 4.832311630249023,
        "learning_rate": 0.00013721198050301346,
        "epoch": 0.2556,
        "step": 1917
    },
    {
        "loss": 2.3644,
        "grad_norm": 3.5913069248199463,
        "learning_rate": 0.00013715355621028225,
        "epoch": 0.2557333333333333,
        "step": 1918
    },
    {
        "loss": 2.223,
        "grad_norm": 2.5339772701263428,
        "learning_rate": 0.00013709511720090081,
        "epoch": 0.2558666666666667,
        "step": 1919
    },
    {
        "loss": 1.914,
        "grad_norm": 2.1157474517822266,
        "learning_rate": 0.0001370366634980171,
        "epoch": 0.256,
        "step": 1920
    },
    {
        "loss": 2.134,
        "grad_norm": 3.3313872814178467,
        "learning_rate": 0.00013697819512478474,
        "epoch": 0.2561333333333333,
        "step": 1921
    },
    {
        "loss": 3.2986,
        "grad_norm": 3.878939390182495,
        "learning_rate": 0.00013691971210436332,
        "epoch": 0.25626666666666664,
        "step": 1922
    },
    {
        "loss": 1.1396,
        "grad_norm": 3.251305103302002,
        "learning_rate": 0.00013686121445991812,
        "epoch": 0.2564,
        "step": 1923
    },
    {
        "loss": 2.4018,
        "grad_norm": 2.3948700428009033,
        "learning_rate": 0.00013680270221462024,
        "epoch": 0.25653333333333334,
        "step": 1924
    },
    {
        "loss": 1.6448,
        "grad_norm": 4.402336120605469,
        "learning_rate": 0.00013674417539164666,
        "epoch": 0.25666666666666665,
        "step": 1925
    },
    {
        "loss": 2.186,
        "grad_norm": 3.788433313369751,
        "learning_rate": 0.00013668563401418,
        "epoch": 0.2568,
        "step": 1926
    },
    {
        "loss": 0.9123,
        "grad_norm": 4.163619041442871,
        "learning_rate": 0.00013662707810540867,
        "epoch": 0.25693333333333335,
        "step": 1927
    },
    {
        "loss": 2.3958,
        "grad_norm": 2.70672607421875,
        "learning_rate": 0.0001365685076885269,
        "epoch": 0.25706666666666667,
        "step": 1928
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.910086989402771,
        "learning_rate": 0.00013650992278673466,
        "epoch": 0.2572,
        "step": 1929
    },
    {
        "loss": 3.0021,
        "grad_norm": 2.016731023788452,
        "learning_rate": 0.0001364513234232376,
        "epoch": 0.25733333333333336,
        "step": 1930
    },
    {
        "loss": 3.4833,
        "grad_norm": 4.557027339935303,
        "learning_rate": 0.0001363927096212471,
        "epoch": 0.2574666666666667,
        "step": 1931
    },
    {
        "loss": 2.596,
        "grad_norm": 2.485881805419922,
        "learning_rate": 0.00013633408140398032,
        "epoch": 0.2576,
        "step": 1932
    },
    {
        "loss": 2.056,
        "grad_norm": 3.0930960178375244,
        "learning_rate": 0.0001362754387946601,
        "epoch": 0.2577333333333333,
        "step": 1933
    },
    {
        "loss": 2.3955,
        "grad_norm": 3.4410951137542725,
        "learning_rate": 0.00013621678181651497,
        "epoch": 0.2578666666666667,
        "step": 1934
    },
    {
        "loss": 2.6498,
        "grad_norm": 2.2132809162139893,
        "learning_rate": 0.00013615811049277914,
        "epoch": 0.258,
        "step": 1935
    },
    {
        "loss": 1.9769,
        "grad_norm": 3.161602020263672,
        "learning_rate": 0.00013609942484669256,
        "epoch": 0.2581333333333333,
        "step": 1936
    },
    {
        "loss": 2.0309,
        "grad_norm": 3.122048854827881,
        "learning_rate": 0.00013604072490150078,
        "epoch": 0.25826666666666664,
        "step": 1937
    },
    {
        "loss": 2.0419,
        "grad_norm": 2.0672619342803955,
        "learning_rate": 0.00013598201068045507,
        "epoch": 0.2584,
        "step": 1938
    },
    {
        "loss": 2.4437,
        "grad_norm": 4.731183052062988,
        "learning_rate": 0.00013592328220681234,
        "epoch": 0.25853333333333334,
        "step": 1939
    },
    {
        "loss": 1.769,
        "grad_norm": 3.7047641277313232,
        "learning_rate": 0.00013586453950383505,
        "epoch": 0.25866666666666666,
        "step": 1940
    },
    {
        "loss": 1.8843,
        "grad_norm": 3.6750166416168213,
        "learning_rate": 0.0001358057825947915,
        "epoch": 0.2588,
        "step": 1941
    },
    {
        "loss": 3.0945,
        "grad_norm": 3.5741055011749268,
        "learning_rate": 0.0001357470115029555,
        "epoch": 0.25893333333333335,
        "step": 1942
    },
    {
        "loss": 2.8193,
        "grad_norm": 2.3745410442352295,
        "learning_rate": 0.00013568822625160642,
        "epoch": 0.25906666666666667,
        "step": 1943
    },
    {
        "loss": 0.9575,
        "grad_norm": 3.762845516204834,
        "learning_rate": 0.0001356294268640293,
        "epoch": 0.2592,
        "step": 1944
    },
    {
        "loss": 2.482,
        "grad_norm": 2.6569032669067383,
        "learning_rate": 0.00013557061336351477,
        "epoch": 0.25933333333333336,
        "step": 1945
    },
    {
        "loss": 2.668,
        "grad_norm": 3.360050916671753,
        "learning_rate": 0.0001355117857733591,
        "epoch": 0.2594666666666667,
        "step": 1946
    },
    {
        "loss": 1.9118,
        "grad_norm": 4.125694274902344,
        "learning_rate": 0.0001354529441168641,
        "epoch": 0.2596,
        "step": 1947
    },
    {
        "loss": 1.462,
        "grad_norm": 3.3043882846832275,
        "learning_rate": 0.00013539408841733717,
        "epoch": 0.2597333333333333,
        "step": 1948
    },
    {
        "loss": 2.9027,
        "grad_norm": 2.2729833126068115,
        "learning_rate": 0.00013533521869809115,
        "epoch": 0.2598666666666667,
        "step": 1949
    },
    {
        "loss": 1.8857,
        "grad_norm": 3.1506435871124268,
        "learning_rate": 0.0001352763349824446,
        "epoch": 0.26,
        "step": 1950
    },
    {
        "loss": 1.6695,
        "grad_norm": 3.376332998275757,
        "learning_rate": 0.00013521743729372164,
        "epoch": 0.2601333333333333,
        "step": 1951
    },
    {
        "loss": 2.6279,
        "grad_norm": 2.6858456134796143,
        "learning_rate": 0.00013515852565525166,
        "epoch": 0.26026666666666665,
        "step": 1952
    },
    {
        "loss": 3.0354,
        "grad_norm": 3.5341272354125977,
        "learning_rate": 0.00013509960009036993,
        "epoch": 0.2604,
        "step": 1953
    },
    {
        "loss": 1.7686,
        "grad_norm": 3.6289453506469727,
        "learning_rate": 0.00013504066062241697,
        "epoch": 0.26053333333333334,
        "step": 1954
    },
    {
        "loss": 2.7039,
        "grad_norm": 3.095154047012329,
        "learning_rate": 0.00013498170727473897,
        "epoch": 0.26066666666666666,
        "step": 1955
    },
    {
        "loss": 2.2702,
        "grad_norm": 2.830129623413086,
        "learning_rate": 0.00013492274007068753,
        "epoch": 0.2608,
        "step": 1956
    },
    {
        "loss": 1.9368,
        "grad_norm": 3.5920400619506836,
        "learning_rate": 0.00013486375903361972,
        "epoch": 0.26093333333333335,
        "step": 1957
    },
    {
        "loss": 2.4975,
        "grad_norm": 3.873500347137451,
        "learning_rate": 0.00013480476418689817,
        "epoch": 0.26106666666666667,
        "step": 1958
    },
    {
        "loss": 1.3755,
        "grad_norm": 4.261908054351807,
        "learning_rate": 0.00013474575555389096,
        "epoch": 0.2612,
        "step": 1959
    },
    {
        "loss": 2.5049,
        "grad_norm": 2.6769280433654785,
        "learning_rate": 0.00013468673315797158,
        "epoch": 0.2613333333333333,
        "step": 1960
    },
    {
        "loss": 2.4365,
        "grad_norm": 4.879457473754883,
        "learning_rate": 0.00013462769702251898,
        "epoch": 0.2614666666666667,
        "step": 1961
    },
    {
        "loss": 2.6299,
        "grad_norm": 3.4456822872161865,
        "learning_rate": 0.0001345686471709176,
        "epoch": 0.2616,
        "step": 1962
    },
    {
        "loss": 2.8288,
        "grad_norm": 2.195643186569214,
        "learning_rate": 0.0001345095836265573,
        "epoch": 0.2617333333333333,
        "step": 1963
    },
    {
        "loss": 1.676,
        "grad_norm": 3.5015523433685303,
        "learning_rate": 0.00013445050641283331,
        "epoch": 0.2618666666666667,
        "step": 1964
    },
    {
        "loss": 2.4335,
        "grad_norm": 3.4104745388031006,
        "learning_rate": 0.00013439141555314637,
        "epoch": 0.262,
        "step": 1965
    },
    {
        "loss": 2.9376,
        "grad_norm": 4.285645961761475,
        "learning_rate": 0.00013433231107090254,
        "epoch": 0.26213333333333333,
        "step": 1966
    },
    {
        "loss": 3.0316,
        "grad_norm": 2.4599199295043945,
        "learning_rate": 0.0001342731929895133,
        "epoch": 0.26226666666666665,
        "step": 1967
    },
    {
        "loss": 1.9736,
        "grad_norm": 3.5313491821289062,
        "learning_rate": 0.00013421406133239553,
        "epoch": 0.2624,
        "step": 1968
    },
    {
        "loss": 2.6745,
        "grad_norm": 2.110365390777588,
        "learning_rate": 0.00013415491612297147,
        "epoch": 0.26253333333333334,
        "step": 1969
    },
    {
        "loss": 2.1969,
        "grad_norm": 4.219215393066406,
        "learning_rate": 0.0001340957573846688,
        "epoch": 0.26266666666666666,
        "step": 1970
    },
    {
        "loss": 2.1371,
        "grad_norm": 2.5380613803863525,
        "learning_rate": 0.00013403658514092045,
        "epoch": 0.2628,
        "step": 1971
    },
    {
        "loss": 2.3309,
        "grad_norm": 3.5989291667938232,
        "learning_rate": 0.0001339773994151647,
        "epoch": 0.26293333333333335,
        "step": 1972
    },
    {
        "loss": 2.9164,
        "grad_norm": 3.013702869415283,
        "learning_rate": 0.00013391820023084527,
        "epoch": 0.26306666666666667,
        "step": 1973
    },
    {
        "loss": 2.8097,
        "grad_norm": 2.538161277770996,
        "learning_rate": 0.00013385898761141118,
        "epoch": 0.2632,
        "step": 1974
    },
    {
        "loss": 2.3158,
        "grad_norm": 1.9709680080413818,
        "learning_rate": 0.00013379976158031673,
        "epoch": 0.2633333333333333,
        "step": 1975
    },
    {
        "loss": 2.204,
        "grad_norm": 2.007452964782715,
        "learning_rate": 0.00013374052216102154,
        "epoch": 0.2634666666666667,
        "step": 1976
    },
    {
        "loss": 1.9806,
        "grad_norm": 3.0966637134552,
        "learning_rate": 0.00013368126937699055,
        "epoch": 0.2636,
        "step": 1977
    },
    {
        "loss": 2.486,
        "grad_norm": 3.235546827316284,
        "learning_rate": 0.00013362200325169401,
        "epoch": 0.2637333333333333,
        "step": 1978
    },
    {
        "loss": 1.899,
        "grad_norm": 3.678703546524048,
        "learning_rate": 0.0001335627238086074,
        "epoch": 0.2638666666666667,
        "step": 1979
    },
    {
        "loss": 2.7652,
        "grad_norm": 3.6839473247528076,
        "learning_rate": 0.00013350343107121156,
        "epoch": 0.264,
        "step": 1980
    },
    {
        "loss": 3.021,
        "grad_norm": 1.4960565567016602,
        "learning_rate": 0.0001334441250629925,
        "epoch": 0.26413333333333333,
        "step": 1981
    },
    {
        "loss": 2.1905,
        "grad_norm": 3.558306932449341,
        "learning_rate": 0.00013338480580744148,
        "epoch": 0.26426666666666665,
        "step": 1982
    },
    {
        "loss": 1.9538,
        "grad_norm": 3.801218032836914,
        "learning_rate": 0.0001333254733280552,
        "epoch": 0.2644,
        "step": 1983
    },
    {
        "loss": 2.336,
        "grad_norm": 1.697875738143921,
        "learning_rate": 0.00013326612764833534,
        "epoch": 0.26453333333333334,
        "step": 1984
    },
    {
        "loss": 2.6472,
        "grad_norm": 2.7121992111206055,
        "learning_rate": 0.000133206768791789,
        "epoch": 0.26466666666666666,
        "step": 1985
    },
    {
        "loss": 2.2733,
        "grad_norm": 1.934156894683838,
        "learning_rate": 0.00013314739678192838,
        "epoch": 0.2648,
        "step": 1986
    },
    {
        "loss": 2.4618,
        "grad_norm": 5.103867530822754,
        "learning_rate": 0.00013308801164227093,
        "epoch": 0.26493333333333335,
        "step": 1987
    },
    {
        "loss": 2.3632,
        "grad_norm": 3.2318408489227295,
        "learning_rate": 0.00013302861339633935,
        "epoch": 0.2650666666666667,
        "step": 1988
    },
    {
        "loss": 2.2759,
        "grad_norm": 2.8478798866271973,
        "learning_rate": 0.00013296920206766147,
        "epoch": 0.2652,
        "step": 1989
    },
    {
        "loss": 2.9253,
        "grad_norm": 2.7565619945526123,
        "learning_rate": 0.00013290977767977026,
        "epoch": 0.2653333333333333,
        "step": 1990
    },
    {
        "loss": 2.8367,
        "grad_norm": 2.8525476455688477,
        "learning_rate": 0.000132850340256204,
        "epoch": 0.2654666666666667,
        "step": 1991
    },
    {
        "loss": 1.5058,
        "grad_norm": 4.714931964874268,
        "learning_rate": 0.00013279088982050605,
        "epoch": 0.2656,
        "step": 1992
    },
    {
        "loss": 2.6967,
        "grad_norm": 2.166499614715576,
        "learning_rate": 0.00013273142639622486,
        "epoch": 0.2657333333333333,
        "step": 1993
    },
    {
        "loss": 2.4487,
        "grad_norm": 2.435270309448242,
        "learning_rate": 0.0001326719500069142,
        "epoch": 0.26586666666666664,
        "step": 1994
    },
    {
        "loss": 2.4129,
        "grad_norm": 2.8515312671661377,
        "learning_rate": 0.00013261246067613276,
        "epoch": 0.266,
        "step": 1995
    },
    {
        "loss": 2.6296,
        "grad_norm": 3.3227269649505615,
        "learning_rate": 0.0001325529584274446,
        "epoch": 0.26613333333333333,
        "step": 1996
    },
    {
        "loss": 1.4604,
        "grad_norm": 2.5361087322235107,
        "learning_rate": 0.0001324934432844186,
        "epoch": 0.26626666666666665,
        "step": 1997
    },
    {
        "loss": 2.0296,
        "grad_norm": 4.172390460968018,
        "learning_rate": 0.00013243391527062906,
        "epoch": 0.2664,
        "step": 1998
    },
    {
        "loss": 3.0056,
        "grad_norm": 3.06298565864563,
        "learning_rate": 0.00013237437440965515,
        "epoch": 0.26653333333333334,
        "step": 1999
    },
    {
        "loss": 2.4369,
        "grad_norm": 3.5970065593719482,
        "learning_rate": 0.00013231482072508118,
        "epoch": 0.26666666666666666,
        "step": 2000
    },
    {
        "loss": 2.6672,
        "grad_norm": 2.760974407196045,
        "learning_rate": 0.00013225525424049664,
        "epoch": 0.2668,
        "step": 2001
    },
    {
        "loss": 2.778,
        "grad_norm": 2.583961009979248,
        "learning_rate": 0.00013219567497949603,
        "epoch": 0.26693333333333336,
        "step": 2002
    },
    {
        "loss": 2.4028,
        "grad_norm": 2.460923194885254,
        "learning_rate": 0.0001321360829656788,
        "epoch": 0.2670666666666667,
        "step": 2003
    },
    {
        "loss": 2.4407,
        "grad_norm": 3.5077013969421387,
        "learning_rate": 0.00013207647822264962,
        "epoch": 0.2672,
        "step": 2004
    },
    {
        "loss": 0.7538,
        "grad_norm": 3.218824863433838,
        "learning_rate": 0.00013201686077401813,
        "epoch": 0.2673333333333333,
        "step": 2005
    },
    {
        "loss": 1.7278,
        "grad_norm": 4.240355491638184,
        "learning_rate": 0.000131957230643399,
        "epoch": 0.2674666666666667,
        "step": 2006
    },
    {
        "loss": 2.2775,
        "grad_norm": 3.216334342956543,
        "learning_rate": 0.0001318975878544119,
        "epoch": 0.2676,
        "step": 2007
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.736699104309082,
        "learning_rate": 0.00013183793243068157,
        "epoch": 0.2677333333333333,
        "step": 2008
    },
    {
        "loss": 2.7377,
        "grad_norm": 2.4248297214508057,
        "learning_rate": 0.00013177826439583773,
        "epoch": 0.26786666666666664,
        "step": 2009
    },
    {
        "loss": 2.6157,
        "grad_norm": 3.2682838439941406,
        "learning_rate": 0.00013171858377351506,
        "epoch": 0.268,
        "step": 2010
    },
    {
        "loss": 2.5024,
        "grad_norm": 2.6031947135925293,
        "learning_rate": 0.0001316588905873533,
        "epoch": 0.26813333333333333,
        "step": 2011
    },
    {
        "loss": 2.7393,
        "grad_norm": 3.4025473594665527,
        "learning_rate": 0.00013159918486099707,
        "epoch": 0.26826666666666665,
        "step": 2012
    },
    {
        "loss": 2.7191,
        "grad_norm": 2.7122881412506104,
        "learning_rate": 0.00013153946661809607,
        "epoch": 0.2684,
        "step": 2013
    },
    {
        "loss": 3.3965,
        "grad_norm": 3.4079864025115967,
        "learning_rate": 0.00013147973588230487,
        "epoch": 0.26853333333333335,
        "step": 2014
    },
    {
        "loss": 2.3289,
        "grad_norm": 3.8408260345458984,
        "learning_rate": 0.000131419992677283,
        "epoch": 0.26866666666666666,
        "step": 2015
    },
    {
        "loss": 2.2465,
        "grad_norm": 2.5029468536376953,
        "learning_rate": 0.00013136023702669503,
        "epoch": 0.2688,
        "step": 2016
    },
    {
        "loss": 3.535,
        "grad_norm": 3.1949379444122314,
        "learning_rate": 0.00013130046895421026,
        "epoch": 0.26893333333333336,
        "step": 2017
    },
    {
        "loss": 2.961,
        "grad_norm": 3.0111820697784424,
        "learning_rate": 0.00013124068848350312,
        "epoch": 0.2690666666666667,
        "step": 2018
    },
    {
        "loss": 2.9648,
        "grad_norm": 3.1275439262390137,
        "learning_rate": 0.00013118089563825283,
        "epoch": 0.2692,
        "step": 2019
    },
    {
        "loss": 2.5855,
        "grad_norm": 3.1771628856658936,
        "learning_rate": 0.00013112109044214346,
        "epoch": 0.2693333333333333,
        "step": 2020
    },
    {
        "loss": 2.5486,
        "grad_norm": 2.8281466960906982,
        "learning_rate": 0.00013106127291886418,
        "epoch": 0.2694666666666667,
        "step": 2021
    },
    {
        "loss": 2.6236,
        "grad_norm": 2.42791485786438,
        "learning_rate": 0.00013100144309210888,
        "epoch": 0.2696,
        "step": 2022
    },
    {
        "loss": 2.2582,
        "grad_norm": 3.111234664916992,
        "learning_rate": 0.0001309416009855763,
        "epoch": 0.2697333333333333,
        "step": 2023
    },
    {
        "loss": 2.8408,
        "grad_norm": 1.9339085817337036,
        "learning_rate": 0.00013088174662297014,
        "epoch": 0.26986666666666664,
        "step": 2024
    },
    {
        "loss": 2.2468,
        "grad_norm": 1.580370545387268,
        "learning_rate": 0.00013082188002799892,
        "epoch": 0.27,
        "step": 2025
    },
    {
        "loss": 1.774,
        "grad_norm": 3.1198477745056152,
        "learning_rate": 0.000130762001224376,
        "epoch": 0.27013333333333334,
        "step": 2026
    },
    {
        "loss": 1.578,
        "grad_norm": 3.062042236328125,
        "learning_rate": 0.0001307021102358196,
        "epoch": 0.27026666666666666,
        "step": 2027
    },
    {
        "loss": 0.684,
        "grad_norm": 3.0423154830932617,
        "learning_rate": 0.00013064220708605267,
        "epoch": 0.2704,
        "step": 2028
    },
    {
        "loss": 2.295,
        "grad_norm": 2.948906898498535,
        "learning_rate": 0.00013058229179880313,
        "epoch": 0.27053333333333335,
        "step": 2029
    },
    {
        "loss": 2.818,
        "grad_norm": 2.6640143394470215,
        "learning_rate": 0.00013052236439780362,
        "epoch": 0.27066666666666667,
        "step": 2030
    },
    {
        "loss": 2.2672,
        "grad_norm": 3.166853427886963,
        "learning_rate": 0.00013046242490679153,
        "epoch": 0.2708,
        "step": 2031
    },
    {
        "loss": 0.6591,
        "grad_norm": 3.3821117877960205,
        "learning_rate": 0.00013040247334950914,
        "epoch": 0.27093333333333336,
        "step": 2032
    },
    {
        "loss": 2.0137,
        "grad_norm": 2.2671799659729004,
        "learning_rate": 0.00013034250974970345,
        "epoch": 0.2710666666666667,
        "step": 2033
    },
    {
        "loss": 2.2122,
        "grad_norm": 3.2224600315093994,
        "learning_rate": 0.0001302825341311263,
        "epoch": 0.2712,
        "step": 2034
    },
    {
        "loss": 1.8236,
        "grad_norm": 3.6113779544830322,
        "learning_rate": 0.00013022254651753418,
        "epoch": 0.2713333333333333,
        "step": 2035
    },
    {
        "loss": 1.4175,
        "grad_norm": 2.806157350540161,
        "learning_rate": 0.00013016254693268842,
        "epoch": 0.2714666666666667,
        "step": 2036
    },
    {
        "loss": 2.478,
        "grad_norm": 2.6449649333953857,
        "learning_rate": 0.00013010253540035501,
        "epoch": 0.2716,
        "step": 2037
    },
    {
        "loss": 1.0451,
        "grad_norm": 3.4046857357025146,
        "learning_rate": 0.00013004251194430476,
        "epoch": 0.2717333333333333,
        "step": 2038
    },
    {
        "loss": 2.6086,
        "grad_norm": 2.736532688140869,
        "learning_rate": 0.0001299824765883132,
        "epoch": 0.27186666666666665,
        "step": 2039
    },
    {
        "loss": 2.7916,
        "grad_norm": 2.4635791778564453,
        "learning_rate": 0.0001299224293561605,
        "epoch": 0.272,
        "step": 2040
    },
    {
        "loss": 1.7914,
        "grad_norm": 3.5294933319091797,
        "learning_rate": 0.00012986237027163154,
        "epoch": 0.27213333333333334,
        "step": 2041
    },
    {
        "loss": 1.8532,
        "grad_norm": 3.0618655681610107,
        "learning_rate": 0.00012980229935851596,
        "epoch": 0.27226666666666666,
        "step": 2042
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.862086534500122,
        "learning_rate": 0.0001297422166406081,
        "epoch": 0.2724,
        "step": 2043
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.1468260288238525,
        "learning_rate": 0.00012968212214170682,
        "epoch": 0.27253333333333335,
        "step": 2044
    },
    {
        "loss": 2.3301,
        "grad_norm": 2.9623446464538574,
        "learning_rate": 0.00012962201588561585,
        "epoch": 0.27266666666666667,
        "step": 2045
    },
    {
        "loss": 2.9168,
        "grad_norm": 2.7498159408569336,
        "learning_rate": 0.00012956189789614347,
        "epoch": 0.2728,
        "step": 2046
    },
    {
        "loss": 2.2618,
        "grad_norm": 3.167391777038574,
        "learning_rate": 0.00012950176819710257,
        "epoch": 0.2729333333333333,
        "step": 2047
    },
    {
        "loss": 2.1279,
        "grad_norm": 2.5093958377838135,
        "learning_rate": 0.0001294416268123108,
        "epoch": 0.2730666666666667,
        "step": 2048
    },
    {
        "loss": 2.8869,
        "grad_norm": 3.0947864055633545,
        "learning_rate": 0.00012938147376559032,
        "epoch": 0.2732,
        "step": 2049
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.035881757736206,
        "learning_rate": 0.00012932130908076793,
        "epoch": 0.2733333333333333,
        "step": 2050
    },
    {
        "loss": 2.0533,
        "grad_norm": 3.7414438724517822,
        "learning_rate": 0.00012926113278167512,
        "epoch": 0.2734666666666667,
        "step": 2051
    },
    {
        "loss": 2.1859,
        "grad_norm": 4.216104030609131,
        "learning_rate": 0.00012920094489214795,
        "epoch": 0.2736,
        "step": 2052
    },
    {
        "loss": 2.8096,
        "grad_norm": 3.6681201457977295,
        "learning_rate": 0.00012914074543602695,
        "epoch": 0.27373333333333333,
        "step": 2053
    },
    {
        "loss": 1.7472,
        "grad_norm": 3.413723945617676,
        "learning_rate": 0.00012908053443715743,
        "epoch": 0.27386666666666665,
        "step": 2054
    },
    {
        "loss": 2.0553,
        "grad_norm": 3.502897262573242,
        "learning_rate": 0.00012902031191938912,
        "epoch": 0.274,
        "step": 2055
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.554717779159546,
        "learning_rate": 0.00012896007790657638,
        "epoch": 0.27413333333333334,
        "step": 2056
    },
    {
        "loss": 2.2318,
        "grad_norm": 2.455490827560425,
        "learning_rate": 0.00012889983242257808,
        "epoch": 0.27426666666666666,
        "step": 2057
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.7920422554016113,
        "learning_rate": 0.00012883957549125768,
        "epoch": 0.2744,
        "step": 2058
    },
    {
        "loss": 1.7757,
        "grad_norm": 3.393749237060547,
        "learning_rate": 0.00012877930713648322,
        "epoch": 0.27453333333333335,
        "step": 2059
    },
    {
        "loss": 2.3985,
        "grad_norm": 2.3975515365600586,
        "learning_rate": 0.0001287190273821271,
        "epoch": 0.27466666666666667,
        "step": 2060
    },
    {
        "loss": 2.2799,
        "grad_norm": 4.017116069793701,
        "learning_rate": 0.00012865873625206634,
        "epoch": 0.2748,
        "step": 2061
    },
    {
        "loss": 2.0724,
        "grad_norm": 2.99359393119812,
        "learning_rate": 0.00012859843377018254,
        "epoch": 0.2749333333333333,
        "step": 2062
    },
    {
        "loss": 2.7595,
        "grad_norm": 3.6032471656799316,
        "learning_rate": 0.00012853811996036167,
        "epoch": 0.2750666666666667,
        "step": 2063
    },
    {
        "loss": 2.4892,
        "grad_norm": 1.9033669233322144,
        "learning_rate": 0.00012847779484649423,
        "epoch": 0.2752,
        "step": 2064
    },
    {
        "loss": 2.178,
        "grad_norm": 3.4264471530914307,
        "learning_rate": 0.00012841745845247526,
        "epoch": 0.2753333333333333,
        "step": 2065
    },
    {
        "loss": 2.6399,
        "grad_norm": 1.7244751453399658,
        "learning_rate": 0.0001283571108022041,
        "epoch": 0.2754666666666667,
        "step": 2066
    },
    {
        "loss": 2.2247,
        "grad_norm": 3.6463167667388916,
        "learning_rate": 0.0001282967519195848,
        "epoch": 0.2756,
        "step": 2067
    },
    {
        "loss": 2.794,
        "grad_norm": 2.8054099082946777,
        "learning_rate": 0.0001282363818285256,
        "epoch": 0.27573333333333333,
        "step": 2068
    },
    {
        "loss": 2.8274,
        "grad_norm": 2.1450037956237793,
        "learning_rate": 0.00012817600055293934,
        "epoch": 0.27586666666666665,
        "step": 2069
    },
    {
        "loss": 1.9636,
        "grad_norm": 4.109917163848877,
        "learning_rate": 0.00012811560811674326,
        "epoch": 0.276,
        "step": 2070
    },
    {
        "loss": 2.6386,
        "grad_norm": 3.078446865081787,
        "learning_rate": 0.000128055204543859,
        "epoch": 0.27613333333333334,
        "step": 2071
    },
    {
        "loss": 3.3606,
        "grad_norm": 3.069373607635498,
        "learning_rate": 0.00012799478985821264,
        "epoch": 0.27626666666666666,
        "step": 2072
    },
    {
        "loss": 2.1621,
        "grad_norm": 4.060051918029785,
        "learning_rate": 0.0001279343640837346,
        "epoch": 0.2764,
        "step": 2073
    },
    {
        "loss": 2.3688,
        "grad_norm": 2.070358991622925,
        "learning_rate": 0.00012787392724435977,
        "epoch": 0.27653333333333335,
        "step": 2074
    },
    {
        "loss": 2.3218,
        "grad_norm": 3.0109097957611084,
        "learning_rate": 0.0001278134793640274,
        "epoch": 0.27666666666666667,
        "step": 2075
    },
    {
        "loss": 2.6871,
        "grad_norm": 2.308565378189087,
        "learning_rate": 0.00012775302046668107,
        "epoch": 0.2768,
        "step": 2076
    },
    {
        "loss": 2.3997,
        "grad_norm": 2.7860732078552246,
        "learning_rate": 0.0001276925505762688,
        "epoch": 0.2769333333333333,
        "step": 2077
    },
    {
        "loss": 2.7712,
        "grad_norm": 3.155226707458496,
        "learning_rate": 0.00012763206971674285,
        "epoch": 0.2770666666666667,
        "step": 2078
    },
    {
        "loss": 2.1825,
        "grad_norm": 3.5105528831481934,
        "learning_rate": 0.00012757157791205994,
        "epoch": 0.2772,
        "step": 2079
    },
    {
        "loss": 2.5475,
        "grad_norm": 2.9343488216400146,
        "learning_rate": 0.00012751107518618103,
        "epoch": 0.2773333333333333,
        "step": 2080
    },
    {
        "loss": 3.424,
        "grad_norm": 2.6945910453796387,
        "learning_rate": 0.00012745056156307158,
        "epoch": 0.27746666666666664,
        "step": 2081
    },
    {
        "loss": 2.0674,
        "grad_norm": 4.791855335235596,
        "learning_rate": 0.00012739003706670112,
        "epoch": 0.2776,
        "step": 2082
    },
    {
        "loss": 2.5337,
        "grad_norm": 4.157748222351074,
        "learning_rate": 0.00012732950172104364,
        "epoch": 0.27773333333333333,
        "step": 2083
    },
    {
        "loss": 1.0402,
        "grad_norm": 4.402752876281738,
        "learning_rate": 0.0001272689555500774,
        "epoch": 0.27786666666666665,
        "step": 2084
    },
    {
        "loss": 2.7214,
        "grad_norm": 3.251725196838379,
        "learning_rate": 0.00012720839857778497,
        "epoch": 0.278,
        "step": 2085
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.368299961090088,
        "learning_rate": 0.00012714783082815316,
        "epoch": 0.27813333333333334,
        "step": 2086
    },
    {
        "loss": 2.9539,
        "grad_norm": 2.400651454925537,
        "learning_rate": 0.00012708725232517303,
        "epoch": 0.27826666666666666,
        "step": 2087
    },
    {
        "loss": 1.8358,
        "grad_norm": 1.71659255027771,
        "learning_rate": 0.00012702666309283994,
        "epoch": 0.2784,
        "step": 2088
    },
    {
        "loss": 2.4202,
        "grad_norm": 2.1629598140716553,
        "learning_rate": 0.00012696606315515356,
        "epoch": 0.27853333333333335,
        "step": 2089
    },
    {
        "loss": 1.079,
        "grad_norm": 3.966646671295166,
        "learning_rate": 0.00012690545253611767,
        "epoch": 0.2786666666666667,
        "step": 2090
    },
    {
        "loss": 2.214,
        "grad_norm": 2.6648902893066406,
        "learning_rate": 0.0001268448312597403,
        "epoch": 0.2788,
        "step": 2091
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.876164436340332,
        "learning_rate": 0.00012678419935003386,
        "epoch": 0.2789333333333333,
        "step": 2092
    },
    {
        "loss": 2.3802,
        "grad_norm": 3.5794081687927246,
        "learning_rate": 0.0001267235568310147,
        "epoch": 0.2790666666666667,
        "step": 2093
    },
    {
        "loss": 3.036,
        "grad_norm": 2.0617990493774414,
        "learning_rate": 0.00012666290372670373,
        "epoch": 0.2792,
        "step": 2094
    },
    {
        "loss": 4.5516,
        "grad_norm": 4.180675506591797,
        "learning_rate": 0.0001266022400611257,
        "epoch": 0.2793333333333333,
        "step": 2095
    },
    {
        "loss": 2.2793,
        "grad_norm": 2.7822821140289307,
        "learning_rate": 0.0001265415658583097,
        "epoch": 0.27946666666666664,
        "step": 2096
    },
    {
        "loss": 2.5275,
        "grad_norm": 3.3890340328216553,
        "learning_rate": 0.0001264808811422891,
        "epoch": 0.2796,
        "step": 2097
    },
    {
        "loss": 1.6794,
        "grad_norm": 3.742597818374634,
        "learning_rate": 0.0001264201859371012,
        "epoch": 0.27973333333333333,
        "step": 2098
    },
    {
        "loss": 2.5967,
        "grad_norm": 2.5164244174957275,
        "learning_rate": 0.00012635948026678764,
        "epoch": 0.27986666666666665,
        "step": 2099
    },
    {
        "loss": 2.3819,
        "grad_norm": 3.253293991088867,
        "learning_rate": 0.00012629876415539413,
        "epoch": 0.28,
        "step": 2100
    },
    {
        "loss": 2.2367,
        "grad_norm": 4.684417724609375,
        "learning_rate": 0.00012623803762697048,
        "epoch": 0.28013333333333335,
        "step": 2101
    },
    {
        "loss": 2.4801,
        "grad_norm": 4.2842864990234375,
        "learning_rate": 0.0001261773007055708,
        "epoch": 0.28026666666666666,
        "step": 2102
    },
    {
        "loss": 2.8733,
        "grad_norm": 2.3294525146484375,
        "learning_rate": 0.0001261165534152531,
        "epoch": 0.2804,
        "step": 2103
    },
    {
        "loss": 0.6698,
        "grad_norm": 4.714259624481201,
        "learning_rate": 0.00012605579578007953,
        "epoch": 0.28053333333333336,
        "step": 2104
    },
    {
        "loss": 2.0205,
        "grad_norm": 3.2171707153320312,
        "learning_rate": 0.0001259950278241165,
        "epoch": 0.2806666666666667,
        "step": 2105
    },
    {
        "loss": 2.4216,
        "grad_norm": 2.07501482963562,
        "learning_rate": 0.00012593424957143442,
        "epoch": 0.2808,
        "step": 2106
    },
    {
        "loss": 1.9718,
        "grad_norm": 2.7276206016540527,
        "learning_rate": 0.00012587346104610768,
        "epoch": 0.2809333333333333,
        "step": 2107
    },
    {
        "loss": 2.7141,
        "grad_norm": 3.2166566848754883,
        "learning_rate": 0.00012581266227221485,
        "epoch": 0.2810666666666667,
        "step": 2108
    },
    {
        "loss": 2.4261,
        "grad_norm": 2.7274010181427,
        "learning_rate": 0.00012575185327383856,
        "epoch": 0.2812,
        "step": 2109
    },
    {
        "loss": 2.2401,
        "grad_norm": 3.68682599067688,
        "learning_rate": 0.00012569103407506543,
        "epoch": 0.2813333333333333,
        "step": 2110
    },
    {
        "loss": 1.9647,
        "grad_norm": 3.465380907058716,
        "learning_rate": 0.00012563020469998616,
        "epoch": 0.28146666666666664,
        "step": 2111
    },
    {
        "loss": 2.868,
        "grad_norm": 2.7693254947662354,
        "learning_rate": 0.0001255693651726955,
        "epoch": 0.2816,
        "step": 2112
    },
    {
        "loss": 2.2115,
        "grad_norm": 2.2259931564331055,
        "learning_rate": 0.00012550851551729214,
        "epoch": 0.28173333333333334,
        "step": 2113
    },
    {
        "loss": 2.7721,
        "grad_norm": 2.3830339908599854,
        "learning_rate": 0.00012544765575787885,
        "epoch": 0.28186666666666665,
        "step": 2114
    },
    {
        "loss": 2.2846,
        "grad_norm": 4.094210147857666,
        "learning_rate": 0.00012538678591856244,
        "epoch": 0.282,
        "step": 2115
    },
    {
        "loss": 2.7183,
        "grad_norm": 2.489785671234131,
        "learning_rate": 0.00012532590602345363,
        "epoch": 0.28213333333333335,
        "step": 2116
    },
    {
        "loss": 1.3527,
        "grad_norm": 5.480679512023926,
        "learning_rate": 0.00012526501609666713,
        "epoch": 0.28226666666666667,
        "step": 2117
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.707476854324341,
        "learning_rate": 0.00012520411616232164,
        "epoch": 0.2824,
        "step": 2118
    },
    {
        "loss": 1.7694,
        "grad_norm": 3.360229015350342,
        "learning_rate": 0.00012514320624453992,
        "epoch": 0.28253333333333336,
        "step": 2119
    },
    {
        "loss": 3.3606,
        "grad_norm": 2.765563488006592,
        "learning_rate": 0.0001250822863674485,
        "epoch": 0.2826666666666667,
        "step": 2120
    },
    {
        "loss": 3.3077,
        "grad_norm": 3.049699068069458,
        "learning_rate": 0.00012502135655517797,
        "epoch": 0.2828,
        "step": 2121
    },
    {
        "loss": 1.6743,
        "grad_norm": 3.4207992553710938,
        "learning_rate": 0.00012496041683186286,
        "epoch": 0.2829333333333333,
        "step": 2122
    },
    {
        "loss": 1.37,
        "grad_norm": 2.4572319984436035,
        "learning_rate": 0.0001248994672216416,
        "epoch": 0.2830666666666667,
        "step": 2123
    },
    {
        "loss": 2.1663,
        "grad_norm": 4.334720611572266,
        "learning_rate": 0.00012483850774865654,
        "epoch": 0.2832,
        "step": 2124
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.0180485248565674,
        "learning_rate": 0.0001247775384370539,
        "epoch": 0.2833333333333333,
        "step": 2125
    },
    {
        "loss": 2.686,
        "grad_norm": 2.4111292362213135,
        "learning_rate": 0.00012471655931098384,
        "epoch": 0.28346666666666664,
        "step": 2126
    },
    {
        "loss": 3.1596,
        "grad_norm": 2.603910207748413,
        "learning_rate": 0.00012465557039460047,
        "epoch": 0.2836,
        "step": 2127
    },
    {
        "loss": 1.9826,
        "grad_norm": 3.1830506324768066,
        "learning_rate": 0.00012459457171206165,
        "epoch": 0.28373333333333334,
        "step": 2128
    },
    {
        "loss": 1.0342,
        "grad_norm": 3.6955251693725586,
        "learning_rate": 0.00012453356328752914,
        "epoch": 0.28386666666666666,
        "step": 2129
    },
    {
        "loss": 1.8939,
        "grad_norm": 3.1209659576416016,
        "learning_rate": 0.00012447254514516863,
        "epoch": 0.284,
        "step": 2130
    },
    {
        "loss": 2.6894,
        "grad_norm": 3.967721462249756,
        "learning_rate": 0.00012441151730914962,
        "epoch": 0.28413333333333335,
        "step": 2131
    },
    {
        "loss": 2.5167,
        "grad_norm": 2.425253391265869,
        "learning_rate": 0.0001243504798036454,
        "epoch": 0.28426666666666667,
        "step": 2132
    },
    {
        "loss": 1.9702,
        "grad_norm": 3.5454564094543457,
        "learning_rate": 0.00012428943265283316,
        "epoch": 0.2844,
        "step": 2133
    },
    {
        "loss": 1.4928,
        "grad_norm": 5.752358436584473,
        "learning_rate": 0.00012422837588089384,
        "epoch": 0.28453333333333336,
        "step": 2134
    },
    {
        "loss": 2.6092,
        "grad_norm": 2.3626129627227783,
        "learning_rate": 0.00012416730951201233,
        "epoch": 0.2846666666666667,
        "step": 2135
    },
    {
        "loss": 2.2526,
        "grad_norm": 3.2525405883789062,
        "learning_rate": 0.00012410623357037717,
        "epoch": 0.2848,
        "step": 2136
    },
    {
        "loss": 2.0324,
        "grad_norm": 3.511779308319092,
        "learning_rate": 0.00012404514808018072,
        "epoch": 0.2849333333333333,
        "step": 2137
    },
    {
        "loss": 2.5675,
        "grad_norm": 2.3890717029571533,
        "learning_rate": 0.00012398405306561922,
        "epoch": 0.2850666666666667,
        "step": 2138
    },
    {
        "loss": 2.5747,
        "grad_norm": 2.7219221591949463,
        "learning_rate": 0.00012392294855089256,
        "epoch": 0.2852,
        "step": 2139
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.728592872619629,
        "learning_rate": 0.00012386183456020448,
        "epoch": 0.2853333333333333,
        "step": 2140
    },
    {
        "loss": 2.4592,
        "grad_norm": 3.6029605865478516,
        "learning_rate": 0.00012380071111776244,
        "epoch": 0.28546666666666665,
        "step": 2141
    },
    {
        "loss": 2.4097,
        "grad_norm": 2.339242696762085,
        "learning_rate": 0.00012373957824777758,
        "epoch": 0.2856,
        "step": 2142
    },
    {
        "loss": 1.8124,
        "grad_norm": 3.279106378555298,
        "learning_rate": 0.0001236784359744649,
        "epoch": 0.28573333333333334,
        "step": 2143
    },
    {
        "loss": 2.2873,
        "grad_norm": 3.655362367630005,
        "learning_rate": 0.00012361728432204306,
        "epoch": 0.28586666666666666,
        "step": 2144
    },
    {
        "loss": 2.6114,
        "grad_norm": 4.159220218658447,
        "learning_rate": 0.00012355612331473444,
        "epoch": 0.286,
        "step": 2145
    },
    {
        "loss": 2.4252,
        "grad_norm": 2.6996073722839355,
        "learning_rate": 0.00012349495297676505,
        "epoch": 0.28613333333333335,
        "step": 2146
    },
    {
        "loss": 2.5033,
        "grad_norm": 2.8646774291992188,
        "learning_rate": 0.00012343377333236477,
        "epoch": 0.28626666666666667,
        "step": 2147
    },
    {
        "loss": 2.3856,
        "grad_norm": 4.046027183532715,
        "learning_rate": 0.00012337258440576704,
        "epoch": 0.2864,
        "step": 2148
    },
    {
        "loss": 1.4437,
        "grad_norm": 4.882054805755615,
        "learning_rate": 0.00012331138622120896,
        "epoch": 0.2865333333333333,
        "step": 2149
    },
    {
        "loss": 1.5174,
        "grad_norm": 3.348853588104248,
        "learning_rate": 0.0001232501788029314,
        "epoch": 0.2866666666666667,
        "step": 2150
    },
    {
        "loss": 2.5284,
        "grad_norm": 2.8530023097991943,
        "learning_rate": 0.00012318896217517874,
        "epoch": 0.2868,
        "step": 2151
    },
    {
        "loss": 2.6482,
        "grad_norm": 3.4559738636016846,
        "learning_rate": 0.0001231277363621992,
        "epoch": 0.2869333333333333,
        "step": 2152
    },
    {
        "loss": 1.5331,
        "grad_norm": 3.828608512878418,
        "learning_rate": 0.00012306650138824447,
        "epoch": 0.2870666666666667,
        "step": 2153
    },
    {
        "loss": 2.0206,
        "grad_norm": 3.5228633880615234,
        "learning_rate": 0.0001230052572775699,
        "epoch": 0.2872,
        "step": 2154
    },
    {
        "loss": 1.4103,
        "grad_norm": 3.7526516914367676,
        "learning_rate": 0.00012294400405443457,
        "epoch": 0.28733333333333333,
        "step": 2155
    },
    {
        "loss": 2.6466,
        "grad_norm": 3.1506688594818115,
        "learning_rate": 0.00012288274174310102,
        "epoch": 0.28746666666666665,
        "step": 2156
    },
    {
        "loss": 2.8427,
        "grad_norm": 3.0097713470458984,
        "learning_rate": 0.00012282147036783556,
        "epoch": 0.2876,
        "step": 2157
    },
    {
        "loss": 2.5032,
        "grad_norm": 1.6923176050186157,
        "learning_rate": 0.00012276018995290787,
        "epoch": 0.28773333333333334,
        "step": 2158
    },
    {
        "loss": 2.9791,
        "grad_norm": 3.541811466217041,
        "learning_rate": 0.00012269890052259142,
        "epoch": 0.28786666666666666,
        "step": 2159
    },
    {
        "loss": 2.6987,
        "grad_norm": 3.7257955074310303,
        "learning_rate": 0.0001226376021011631,
        "epoch": 0.288,
        "step": 2160
    },
    {
        "loss": 2.2211,
        "grad_norm": 2.936683416366577,
        "learning_rate": 0.0001225762947129035,
        "epoch": 0.28813333333333335,
        "step": 2161
    },
    {
        "loss": 2.8575,
        "grad_norm": 2.297813892364502,
        "learning_rate": 0.00012251497838209658,
        "epoch": 0.28826666666666667,
        "step": 2162
    },
    {
        "loss": 2.0899,
        "grad_norm": 4.12659215927124,
        "learning_rate": 0.00012245365313303004,
        "epoch": 0.2884,
        "step": 2163
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.8839216232299805,
        "learning_rate": 0.00012239231898999496,
        "epoch": 0.2885333333333333,
        "step": 2164
    },
    {
        "loss": 2.374,
        "grad_norm": 3.7965803146362305,
        "learning_rate": 0.00012233097597728602,
        "epoch": 0.2886666666666667,
        "step": 2165
    },
    {
        "loss": 1.2839,
        "grad_norm": 2.861356735229492,
        "learning_rate": 0.00012226962411920146,
        "epoch": 0.2888,
        "step": 2166
    },
    {
        "loss": 1.574,
        "grad_norm": 3.439992904663086,
        "learning_rate": 0.00012220826344004287,
        "epoch": 0.2889333333333333,
        "step": 2167
    },
    {
        "loss": 1.797,
        "grad_norm": 3.0297772884368896,
        "learning_rate": 0.00012214689396411544,
        "epoch": 0.2890666666666667,
        "step": 2168
    },
    {
        "loss": 2.6373,
        "grad_norm": 2.650768518447876,
        "learning_rate": 0.00012208551571572787,
        "epoch": 0.2892,
        "step": 2169
    },
    {
        "loss": 1.7658,
        "grad_norm": 3.530910015106201,
        "learning_rate": 0.00012202412871919228,
        "epoch": 0.28933333333333333,
        "step": 2170
    },
    {
        "loss": 1.5786,
        "grad_norm": 3.8017473220825195,
        "learning_rate": 0.00012196273299882422,
        "epoch": 0.28946666666666665,
        "step": 2171
    },
    {
        "loss": 2.2876,
        "grad_norm": 4.056345462799072,
        "learning_rate": 0.0001219013285789428,
        "epoch": 0.2896,
        "step": 2172
    },
    {
        "loss": 2.3878,
        "grad_norm": 3.166729211807251,
        "learning_rate": 0.00012183991548387053,
        "epoch": 0.28973333333333334,
        "step": 2173
    },
    {
        "loss": 2.3128,
        "grad_norm": 2.330413818359375,
        "learning_rate": 0.00012177849373793328,
        "epoch": 0.28986666666666666,
        "step": 2174
    },
    {
        "loss": 2.4682,
        "grad_norm": 2.8679544925689697,
        "learning_rate": 0.00012171706336546048,
        "epoch": 0.29,
        "step": 2175
    },
    {
        "loss": 1.813,
        "grad_norm": 3.7280635833740234,
        "learning_rate": 0.00012165562439078488,
        "epoch": 0.29013333333333335,
        "step": 2176
    },
    {
        "loss": 1.5646,
        "grad_norm": 5.790420055389404,
        "learning_rate": 0.00012159417683824266,
        "epoch": 0.2902666666666667,
        "step": 2177
    },
    {
        "loss": 2.438,
        "grad_norm": 4.2541632652282715,
        "learning_rate": 0.0001215327207321734,
        "epoch": 0.2904,
        "step": 2178
    },
    {
        "loss": 2.2385,
        "grad_norm": 2.318485975265503,
        "learning_rate": 0.00012147125609692012,
        "epoch": 0.2905333333333333,
        "step": 2179
    },
    {
        "loss": 1.8928,
        "grad_norm": 5.366332054138184,
        "learning_rate": 0.00012140978295682912,
        "epoch": 0.2906666666666667,
        "step": 2180
    },
    {
        "loss": 1.4292,
        "grad_norm": 4.175282001495361,
        "learning_rate": 0.00012134830133625012,
        "epoch": 0.2908,
        "step": 2181
    },
    {
        "loss": 1.1531,
        "grad_norm": 4.761017322540283,
        "learning_rate": 0.00012128681125953626,
        "epoch": 0.2909333333333333,
        "step": 2182
    },
    {
        "loss": 2.7192,
        "grad_norm": 3.2432544231414795,
        "learning_rate": 0.00012122531275104391,
        "epoch": 0.29106666666666664,
        "step": 2183
    },
    {
        "loss": 2.4818,
        "grad_norm": 3.0381968021392822,
        "learning_rate": 0.00012116380583513285,
        "epoch": 0.2912,
        "step": 2184
    },
    {
        "loss": 1.872,
        "grad_norm": 3.1155762672424316,
        "learning_rate": 0.00012110229053616619,
        "epoch": 0.29133333333333333,
        "step": 2185
    },
    {
        "loss": 2.7408,
        "grad_norm": 4.3051533699035645,
        "learning_rate": 0.00012104076687851034,
        "epoch": 0.29146666666666665,
        "step": 2186
    },
    {
        "loss": 2.5283,
        "grad_norm": 3.6974034309387207,
        "learning_rate": 0.00012097923488653505,
        "epoch": 0.2916,
        "step": 2187
    },
    {
        "loss": 2.8761,
        "grad_norm": 2.5784592628479004,
        "learning_rate": 0.00012091769458461333,
        "epoch": 0.29173333333333334,
        "step": 2188
    },
    {
        "loss": 3.0093,
        "grad_norm": 4.497734069824219,
        "learning_rate": 0.00012085614599712147,
        "epoch": 0.29186666666666666,
        "step": 2189
    },
    {
        "loss": 1.9736,
        "grad_norm": 5.227110385894775,
        "learning_rate": 0.00012079458914843915,
        "epoch": 0.292,
        "step": 2190
    },
    {
        "loss": 2.7697,
        "grad_norm": 3.5138919353485107,
        "learning_rate": 0.00012073302406294924,
        "epoch": 0.29213333333333336,
        "step": 2191
    },
    {
        "loss": 2.4801,
        "grad_norm": 4.410989284515381,
        "learning_rate": 0.0001206714507650378,
        "epoch": 0.2922666666666667,
        "step": 2192
    },
    {
        "loss": 3.1117,
        "grad_norm": 4.495401382446289,
        "learning_rate": 0.00012060986927909433,
        "epoch": 0.2924,
        "step": 2193
    },
    {
        "loss": 2.154,
        "grad_norm": 3.9464011192321777,
        "learning_rate": 0.00012054827962951132,
        "epoch": 0.2925333333333333,
        "step": 2194
    },
    {
        "loss": 2.3224,
        "grad_norm": 3.1954026222229004,
        "learning_rate": 0.00012048668184068481,
        "epoch": 0.2926666666666667,
        "step": 2195
    },
    {
        "loss": 1.8523,
        "grad_norm": 2.9551992416381836,
        "learning_rate": 0.00012042507593701382,
        "epoch": 0.2928,
        "step": 2196
    },
    {
        "loss": 2.7544,
        "grad_norm": 3.259014129638672,
        "learning_rate": 0.0001203634619429006,
        "epoch": 0.2929333333333333,
        "step": 2197
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.228447437286377,
        "learning_rate": 0.00012030183988275076,
        "epoch": 0.29306666666666664,
        "step": 2198
    },
    {
        "loss": 2.3611,
        "grad_norm": 5.189020156860352,
        "learning_rate": 0.00012024020978097296,
        "epoch": 0.2932,
        "step": 2199
    },
    {
        "loss": 2.5528,
        "grad_norm": 3.0797297954559326,
        "learning_rate": 0.00012017857166197911,
        "epoch": 0.29333333333333333,
        "step": 2200
    },
    {
        "loss": 0.7205,
        "grad_norm": 3.4334545135498047,
        "learning_rate": 0.00012011692555018425,
        "epoch": 0.29346666666666665,
        "step": 2201
    },
    {
        "loss": 2.4071,
        "grad_norm": 3.5505502223968506,
        "learning_rate": 0.00012005527147000663,
        "epoch": 0.2936,
        "step": 2202
    },
    {
        "loss": 1.6714,
        "grad_norm": 3.5253710746765137,
        "learning_rate": 0.00011999360944586766,
        "epoch": 0.29373333333333335,
        "step": 2203
    },
    {
        "loss": 2.0852,
        "grad_norm": 3.560011148452759,
        "learning_rate": 0.00011993193950219186,
        "epoch": 0.29386666666666666,
        "step": 2204
    },
    {
        "loss": 3.0082,
        "grad_norm": 2.351320505142212,
        "learning_rate": 0.00011987026166340689,
        "epoch": 0.294,
        "step": 2205
    },
    {
        "loss": 2.2192,
        "grad_norm": 2.0356500148773193,
        "learning_rate": 0.00011980857595394357,
        "epoch": 0.29413333333333336,
        "step": 2206
    },
    {
        "loss": 1.2121,
        "grad_norm": 4.329596042633057,
        "learning_rate": 0.00011974688239823583,
        "epoch": 0.2942666666666667,
        "step": 2207
    },
    {
        "loss": 1.1553,
        "grad_norm": 3.9301364421844482,
        "learning_rate": 0.0001196851810207207,
        "epoch": 0.2944,
        "step": 2208
    },
    {
        "loss": 2.6102,
        "grad_norm": 2.8736629486083984,
        "learning_rate": 0.00011962347184583827,
        "epoch": 0.2945333333333333,
        "step": 2209
    },
    {
        "loss": 2.9916,
        "grad_norm": 2.05233097076416,
        "learning_rate": 0.00011956175489803177,
        "epoch": 0.2946666666666667,
        "step": 2210
    },
    {
        "loss": 2.418,
        "grad_norm": 3.10917592048645,
        "learning_rate": 0.00011950003020174752,
        "epoch": 0.2948,
        "step": 2211
    },
    {
        "loss": 1.8397,
        "grad_norm": 3.7443864345550537,
        "learning_rate": 0.00011943829778143485,
        "epoch": 0.2949333333333333,
        "step": 2212
    },
    {
        "loss": 2.0157,
        "grad_norm": 3.232903003692627,
        "learning_rate": 0.0001193765576615462,
        "epoch": 0.29506666666666664,
        "step": 2213
    },
    {
        "loss": 2.0182,
        "grad_norm": 3.17413330078125,
        "learning_rate": 0.00011931480986653701,
        "epoch": 0.2952,
        "step": 2214
    },
    {
        "loss": 2.4696,
        "grad_norm": 4.212464809417725,
        "learning_rate": 0.00011925305442086582,
        "epoch": 0.29533333333333334,
        "step": 2215
    },
    {
        "loss": 2.3486,
        "grad_norm": 2.3040122985839844,
        "learning_rate": 0.00011919129134899422,
        "epoch": 0.29546666666666666,
        "step": 2216
    },
    {
        "loss": 2.7915,
        "grad_norm": 4.159972667694092,
        "learning_rate": 0.0001191295206753867,
        "epoch": 0.2956,
        "step": 2217
    },
    {
        "loss": 2.2004,
        "grad_norm": 2.210848093032837,
        "learning_rate": 0.00011906774242451087,
        "epoch": 0.29573333333333335,
        "step": 2218
    },
    {
        "loss": 2.6562,
        "grad_norm": 3.0134260654449463,
        "learning_rate": 0.00011900595662083725,
        "epoch": 0.29586666666666667,
        "step": 2219
    },
    {
        "loss": 1.9786,
        "grad_norm": 3.4075827598571777,
        "learning_rate": 0.0001189441632888395,
        "epoch": 0.296,
        "step": 2220
    },
    {
        "loss": 2.1527,
        "grad_norm": 6.204354286193848,
        "learning_rate": 0.00011888236245299411,
        "epoch": 0.29613333333333336,
        "step": 2221
    },
    {
        "loss": 2.4552,
        "grad_norm": 2.4280755519866943,
        "learning_rate": 0.00011882055413778059,
        "epoch": 0.2962666666666667,
        "step": 2222
    },
    {
        "loss": 2.365,
        "grad_norm": 2.71852707862854,
        "learning_rate": 0.00011875873836768145,
        "epoch": 0.2964,
        "step": 2223
    },
    {
        "loss": 2.7599,
        "grad_norm": 2.1894891262054443,
        "learning_rate": 0.00011869691516718215,
        "epoch": 0.2965333333333333,
        "step": 2224
    },
    {
        "loss": 2.5236,
        "grad_norm": 2.4637956619262695,
        "learning_rate": 0.00011863508456077104,
        "epoch": 0.2966666666666667,
        "step": 2225
    },
    {
        "loss": 2.473,
        "grad_norm": 2.6337618827819824,
        "learning_rate": 0.00011857324657293943,
        "epoch": 0.2968,
        "step": 2226
    },
    {
        "loss": 2.6949,
        "grad_norm": 2.9151713848114014,
        "learning_rate": 0.00011851140122818154,
        "epoch": 0.2969333333333333,
        "step": 2227
    },
    {
        "loss": 2.5699,
        "grad_norm": 2.5393548011779785,
        "learning_rate": 0.00011844954855099458,
        "epoch": 0.29706666666666665,
        "step": 2228
    },
    {
        "loss": 2.5969,
        "grad_norm": 2.2086503505706787,
        "learning_rate": 0.00011838768856587858,
        "epoch": 0.2972,
        "step": 2229
    },
    {
        "loss": 1.8815,
        "grad_norm": 4.717004776000977,
        "learning_rate": 0.00011832582129733644,
        "epoch": 0.29733333333333334,
        "step": 2230
    },
    {
        "loss": 2.0314,
        "grad_norm": 3.4542644023895264,
        "learning_rate": 0.00011826394676987409,
        "epoch": 0.29746666666666666,
        "step": 2231
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.4493491649627686,
        "learning_rate": 0.00011820206500800017,
        "epoch": 0.2976,
        "step": 2232
    },
    {
        "loss": 2.9127,
        "grad_norm": 2.875438928604126,
        "learning_rate": 0.00011814017603622626,
        "epoch": 0.29773333333333335,
        "step": 2233
    },
    {
        "loss": 0.9486,
        "grad_norm": 6.712531566619873,
        "learning_rate": 0.00011807827987906684,
        "epoch": 0.29786666666666667,
        "step": 2234
    },
    {
        "loss": 2.4078,
        "grad_norm": 3.603745222091675,
        "learning_rate": 0.00011801637656103912,
        "epoch": 0.298,
        "step": 2235
    },
    {
        "loss": 2.7579,
        "grad_norm": 3.591435432434082,
        "learning_rate": 0.00011795446610666329,
        "epoch": 0.2981333333333333,
        "step": 2236
    },
    {
        "loss": 2.8423,
        "grad_norm": 3.9262118339538574,
        "learning_rate": 0.00011789254854046225,
        "epoch": 0.2982666666666667,
        "step": 2237
    },
    {
        "loss": 2.396,
        "grad_norm": 2.7005865573883057,
        "learning_rate": 0.00011783062388696177,
        "epoch": 0.2984,
        "step": 2238
    },
    {
        "loss": 2.7749,
        "grad_norm": 3.331901788711548,
        "learning_rate": 0.00011776869217069042,
        "epoch": 0.2985333333333333,
        "step": 2239
    },
    {
        "loss": 2.4001,
        "grad_norm": 2.9047019481658936,
        "learning_rate": 0.00011770675341617952,
        "epoch": 0.2986666666666667,
        "step": 2240
    },
    {
        "loss": 2.0605,
        "grad_norm": 3.857616662979126,
        "learning_rate": 0.00011764480764796328,
        "epoch": 0.2988,
        "step": 2241
    },
    {
        "loss": 2.2971,
        "grad_norm": 4.187235355377197,
        "learning_rate": 0.00011758285489057863,
        "epoch": 0.29893333333333333,
        "step": 2242
    },
    {
        "loss": 1.081,
        "grad_norm": 3.5885696411132812,
        "learning_rate": 0.00011752089516856521,
        "epoch": 0.29906666666666665,
        "step": 2243
    },
    {
        "loss": 2.1176,
        "grad_norm": 3.5631392002105713,
        "learning_rate": 0.00011745892850646552,
        "epoch": 0.2992,
        "step": 2244
    },
    {
        "loss": 2.7336,
        "grad_norm": 3.4617693424224854,
        "learning_rate": 0.00011739695492882479,
        "epoch": 0.29933333333333334,
        "step": 2245
    },
    {
        "loss": 2.4591,
        "grad_norm": 3.0416295528411865,
        "learning_rate": 0.00011733497446019091,
        "epoch": 0.29946666666666666,
        "step": 2246
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.8694207668304443,
        "learning_rate": 0.00011727298712511457,
        "epoch": 0.2996,
        "step": 2247
    },
    {
        "loss": 2.5001,
        "grad_norm": 1.8096996545791626,
        "learning_rate": 0.00011721099294814917,
        "epoch": 0.29973333333333335,
        "step": 2248
    },
    {
        "loss": 2.0481,
        "grad_norm": 3.63324236869812,
        "learning_rate": 0.00011714899195385086,
        "epoch": 0.29986666666666667,
        "step": 2249
    },
    {
        "loss": 1.3017,
        "grad_norm": 2.8082773685455322,
        "learning_rate": 0.00011708698416677837,
        "epoch": 0.3,
        "step": 2250
    },
    {
        "loss": 2.5794,
        "grad_norm": 5.74021053314209,
        "learning_rate": 0.00011702496961149322,
        "epoch": 0.3001333333333333,
        "step": 2251
    },
    {
        "loss": 2.7409,
        "grad_norm": 2.8033339977264404,
        "learning_rate": 0.0001169629483125596,
        "epoch": 0.3002666666666667,
        "step": 2252
    },
    {
        "loss": 2.5185,
        "grad_norm": 2.836212158203125,
        "learning_rate": 0.00011690092029454436,
        "epoch": 0.3004,
        "step": 2253
    },
    {
        "loss": 2.3978,
        "grad_norm": 3.1075832843780518,
        "learning_rate": 0.00011683888558201703,
        "epoch": 0.3005333333333333,
        "step": 2254
    },
    {
        "loss": 2.2131,
        "grad_norm": 3.4171392917633057,
        "learning_rate": 0.00011677684419954968,
        "epoch": 0.3006666666666667,
        "step": 2255
    },
    {
        "loss": 2.602,
        "grad_norm": 2.3961846828460693,
        "learning_rate": 0.00011671479617171721,
        "epoch": 0.3008,
        "step": 2256
    },
    {
        "loss": 1.8566,
        "grad_norm": 2.595667839050293,
        "learning_rate": 0.00011665274152309702,
        "epoch": 0.30093333333333333,
        "step": 2257
    },
    {
        "loss": 2.7949,
        "grad_norm": 2.24348783493042,
        "learning_rate": 0.0001165906802782692,
        "epoch": 0.30106666666666665,
        "step": 2258
    },
    {
        "loss": 3.0558,
        "grad_norm": 3.080622673034668,
        "learning_rate": 0.00011652861246181639,
        "epoch": 0.3012,
        "step": 2259
    },
    {
        "loss": 2.369,
        "grad_norm": 1.7179477214813232,
        "learning_rate": 0.00011646653809832381,
        "epoch": 0.30133333333333334,
        "step": 2260
    },
    {
        "loss": 1.9878,
        "grad_norm": 2.9169321060180664,
        "learning_rate": 0.00011640445721237943,
        "epoch": 0.30146666666666666,
        "step": 2261
    },
    {
        "loss": 1.0192,
        "grad_norm": 2.1875343322753906,
        "learning_rate": 0.00011634236982857363,
        "epoch": 0.3016,
        "step": 2262
    },
    {
        "loss": 2.0227,
        "grad_norm": 2.7231571674346924,
        "learning_rate": 0.00011628027597149947,
        "epoch": 0.30173333333333335,
        "step": 2263
    },
    {
        "loss": 1.5269,
        "grad_norm": 3.9235353469848633,
        "learning_rate": 0.00011621817566575251,
        "epoch": 0.30186666666666667,
        "step": 2264
    },
    {
        "loss": 2.3938,
        "grad_norm": 2.748485803604126,
        "learning_rate": 0.0001161560689359309,
        "epoch": 0.302,
        "step": 2265
    },
    {
        "loss": 2.9494,
        "grad_norm": 4.00535249710083,
        "learning_rate": 0.00011609395580663534,
        "epoch": 0.3021333333333333,
        "step": 2266
    },
    {
        "loss": 1.6607,
        "grad_norm": 4.815486907958984,
        "learning_rate": 0.00011603183630246908,
        "epoch": 0.3022666666666667,
        "step": 2267
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.3218228816986084,
        "learning_rate": 0.00011596971044803779,
        "epoch": 0.3024,
        "step": 2268
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.8315367698669434,
        "learning_rate": 0.0001159075782679498,
        "epoch": 0.3025333333333333,
        "step": 2269
    },
    {
        "loss": 2.9647,
        "grad_norm": 2.756411075592041,
        "learning_rate": 0.00011584543978681584,
        "epoch": 0.30266666666666664,
        "step": 2270
    },
    {
        "loss": 2.2989,
        "grad_norm": 3.9585328102111816,
        "learning_rate": 0.0001157832950292492,
        "epoch": 0.3028,
        "step": 2271
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.9324731826782227,
        "learning_rate": 0.0001157211440198656,
        "epoch": 0.30293333333333333,
        "step": 2272
    },
    {
        "loss": 2.5474,
        "grad_norm": 3.1995434761047363,
        "learning_rate": 0.00011565898678328328,
        "epoch": 0.30306666666666665,
        "step": 2273
    },
    {
        "loss": 2.6465,
        "grad_norm": 2.1724162101745605,
        "learning_rate": 0.00011559682334412296,
        "epoch": 0.3032,
        "step": 2274
    },
    {
        "loss": 2.9397,
        "grad_norm": 3.814934015274048,
        "learning_rate": 0.00011553465372700776,
        "epoch": 0.30333333333333334,
        "step": 2275
    },
    {
        "loss": 2.264,
        "grad_norm": 3.0687344074249268,
        "learning_rate": 0.00011547247795656328,
        "epoch": 0.30346666666666666,
        "step": 2276
    },
    {
        "loss": 2.5697,
        "grad_norm": 2.0083892345428467,
        "learning_rate": 0.00011541029605741758,
        "epoch": 0.3036,
        "step": 2277
    },
    {
        "loss": 2.3771,
        "grad_norm": 2.7048521041870117,
        "learning_rate": 0.00011534810805420109,
        "epoch": 0.30373333333333336,
        "step": 2278
    },
    {
        "loss": 3.1646,
        "grad_norm": 3.7682414054870605,
        "learning_rate": 0.00011528591397154671,
        "epoch": 0.3038666666666667,
        "step": 2279
    },
    {
        "loss": 1.5818,
        "grad_norm": 3.8765432834625244,
        "learning_rate": 0.00011522371383408972,
        "epoch": 0.304,
        "step": 2280
    },
    {
        "loss": 2.5285,
        "grad_norm": 2.727179765701294,
        "learning_rate": 0.0001151615076664678,
        "epoch": 0.3041333333333333,
        "step": 2281
    },
    {
        "loss": 1.9362,
        "grad_norm": 3.6924424171447754,
        "learning_rate": 0.00011509929549332102,
        "epoch": 0.3042666666666667,
        "step": 2282
    },
    {
        "loss": 1.5617,
        "grad_norm": 3.728236198425293,
        "learning_rate": 0.00011503707733929188,
        "epoch": 0.3044,
        "step": 2283
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.9528591632843018,
        "learning_rate": 0.00011497485322902515,
        "epoch": 0.3045333333333333,
        "step": 2284
    },
    {
        "loss": 1.2315,
        "grad_norm": 2.646860122680664,
        "learning_rate": 0.000114912623187168,
        "epoch": 0.30466666666666664,
        "step": 2285
    },
    {
        "loss": 2.5046,
        "grad_norm": 3.605970859527588,
        "learning_rate": 0.00011485038723837003,
        "epoch": 0.3048,
        "step": 2286
    },
    {
        "loss": 1.539,
        "grad_norm": 3.1057674884796143,
        "learning_rate": 0.00011478814540728307,
        "epoch": 0.30493333333333333,
        "step": 2287
    },
    {
        "loss": 1.5221,
        "grad_norm": 3.959027051925659,
        "learning_rate": 0.00011472589771856132,
        "epoch": 0.30506666666666665,
        "step": 2288
    },
    {
        "loss": 2.4724,
        "grad_norm": 2.1837196350097656,
        "learning_rate": 0.00011466364419686131,
        "epoch": 0.3052,
        "step": 2289
    },
    {
        "loss": 2.8367,
        "grad_norm": 2.6492090225219727,
        "learning_rate": 0.00011460138486684184,
        "epoch": 0.30533333333333335,
        "step": 2290
    },
    {
        "loss": 2.4668,
        "grad_norm": 4.154592990875244,
        "learning_rate": 0.00011453911975316411,
        "epoch": 0.30546666666666666,
        "step": 2291
    },
    {
        "loss": 1.9964,
        "grad_norm": 6.800713539123535,
        "learning_rate": 0.0001144768488804915,
        "epoch": 0.3056,
        "step": 2292
    },
    {
        "loss": 2.915,
        "grad_norm": 2.297997236251831,
        "learning_rate": 0.00011441457227348971,
        "epoch": 0.30573333333333336,
        "step": 2293
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.3411104679107666,
        "learning_rate": 0.00011435228995682672,
        "epoch": 0.3058666666666667,
        "step": 2294
    },
    {
        "loss": 2.9956,
        "grad_norm": 3.073439121246338,
        "learning_rate": 0.00011429000195517274,
        "epoch": 0.306,
        "step": 2295
    },
    {
        "loss": 3.0348,
        "grad_norm": 2.824920892715454,
        "learning_rate": 0.00011422770829320035,
        "epoch": 0.3061333333333333,
        "step": 2296
    },
    {
        "loss": 2.8064,
        "grad_norm": 2.0251760482788086,
        "learning_rate": 0.00011416540899558423,
        "epoch": 0.3062666666666667,
        "step": 2297
    },
    {
        "loss": 2.2181,
        "grad_norm": 4.568803310394287,
        "learning_rate": 0.0001141031040870013,
        "epoch": 0.3064,
        "step": 2298
    },
    {
        "loss": 2.0728,
        "grad_norm": 4.025503635406494,
        "learning_rate": 0.0001140407935921308,
        "epoch": 0.3065333333333333,
        "step": 2299
    },
    {
        "loss": 2.6122,
        "grad_norm": 3.0916497707366943,
        "learning_rate": 0.00011397847753565411,
        "epoch": 0.30666666666666664,
        "step": 2300
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.119849681854248,
        "learning_rate": 0.00011391615594225486,
        "epoch": 0.3068,
        "step": 2301
    },
    {
        "loss": 1.0679,
        "grad_norm": 2.93285870552063,
        "learning_rate": 0.00011385382883661881,
        "epoch": 0.30693333333333334,
        "step": 2302
    },
    {
        "loss": 1.9066,
        "grad_norm": 4.276412010192871,
        "learning_rate": 0.00011379149624343395,
        "epoch": 0.30706666666666665,
        "step": 2303
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.782282590866089,
        "learning_rate": 0.00011372915818739043,
        "epoch": 0.3072,
        "step": 2304
    },
    {
        "loss": 2.1844,
        "grad_norm": 3.823303461074829,
        "learning_rate": 0.00011366681469318061,
        "epoch": 0.30733333333333335,
        "step": 2305
    },
    {
        "loss": 2.8071,
        "grad_norm": 2.556309461593628,
        "learning_rate": 0.00011360446578549885,
        "epoch": 0.30746666666666667,
        "step": 2306
    },
    {
        "loss": 2.4049,
        "grad_norm": 3.7869479656219482,
        "learning_rate": 0.00011354211148904185,
        "epoch": 0.3076,
        "step": 2307
    },
    {
        "loss": 2.3925,
        "grad_norm": 1.9571301937103271,
        "learning_rate": 0.0001134797518285084,
        "epoch": 0.30773333333333336,
        "step": 2308
    },
    {
        "loss": 2.7824,
        "grad_norm": 3.623137950897217,
        "learning_rate": 0.0001134173868285993,
        "epoch": 0.3078666666666667,
        "step": 2309
    },
    {
        "loss": 1.5897,
        "grad_norm": 3.3059823513031006,
        "learning_rate": 0.00011335501651401754,
        "epoch": 0.308,
        "step": 2310
    },
    {
        "loss": 2.4048,
        "grad_norm": 3.150970935821533,
        "learning_rate": 0.00011329264090946825,
        "epoch": 0.3081333333333333,
        "step": 2311
    },
    {
        "loss": 2.8135,
        "grad_norm": 2.107131242752075,
        "learning_rate": 0.0001132302600396586,
        "epoch": 0.3082666666666667,
        "step": 2312
    },
    {
        "loss": 2.3632,
        "grad_norm": 3.317012310028076,
        "learning_rate": 0.00011316787392929786,
        "epoch": 0.3084,
        "step": 2313
    },
    {
        "loss": 2.432,
        "grad_norm": 3.3282153606414795,
        "learning_rate": 0.00011310548260309739,
        "epoch": 0.3085333333333333,
        "step": 2314
    },
    {
        "loss": 2.6269,
        "grad_norm": 6.45154333114624,
        "learning_rate": 0.00011304308608577059,
        "epoch": 0.30866666666666664,
        "step": 2315
    },
    {
        "loss": 2.102,
        "grad_norm": 2.987521171569824,
        "learning_rate": 0.00011298068440203294,
        "epoch": 0.3088,
        "step": 2316
    },
    {
        "loss": 2.2323,
        "grad_norm": 5.112251281738281,
        "learning_rate": 0.00011291827757660201,
        "epoch": 0.30893333333333334,
        "step": 2317
    },
    {
        "loss": 2.5864,
        "grad_norm": 2.779350996017456,
        "learning_rate": 0.00011285586563419727,
        "epoch": 0.30906666666666666,
        "step": 2318
    },
    {
        "loss": 1.935,
        "grad_norm": 3.3312737941741943,
        "learning_rate": 0.0001127934485995404,
        "epoch": 0.3092,
        "step": 2319
    },
    {
        "loss": 2.3712,
        "grad_norm": 4.56546688079834,
        "learning_rate": 0.0001127310264973549,
        "epoch": 0.30933333333333335,
        "step": 2320
    },
    {
        "loss": 2.4051,
        "grad_norm": 3.805163860321045,
        "learning_rate": 0.00011266859935236647,
        "epoch": 0.30946666666666667,
        "step": 2321
    },
    {
        "loss": 1.6186,
        "grad_norm": 3.3361096382141113,
        "learning_rate": 0.00011260616718930263,
        "epoch": 0.3096,
        "step": 2322
    },
    {
        "loss": 2.4128,
        "grad_norm": 2.8130884170532227,
        "learning_rate": 0.000112543730032893,
        "epoch": 0.30973333333333336,
        "step": 2323
    },
    {
        "loss": 1.8808,
        "grad_norm": 4.027099609375,
        "learning_rate": 0.0001124812879078692,
        "epoch": 0.3098666666666667,
        "step": 2324
    },
    {
        "loss": 2.5243,
        "grad_norm": 3.476032018661499,
        "learning_rate": 0.00011241884083896472,
        "epoch": 0.31,
        "step": 2325
    },
    {
        "loss": 1.609,
        "grad_norm": 2.392639398574829,
        "learning_rate": 0.00011235638885091506,
        "epoch": 0.3101333333333333,
        "step": 2326
    },
    {
        "loss": 2.539,
        "grad_norm": 5.353792667388916,
        "learning_rate": 0.0001122939319684577,
        "epoch": 0.3102666666666667,
        "step": 2327
    },
    {
        "loss": 2.3397,
        "grad_norm": 2.3594272136688232,
        "learning_rate": 0.00011223147021633194,
        "epoch": 0.3104,
        "step": 2328
    },
    {
        "loss": 2.7501,
        "grad_norm": 3.3655261993408203,
        "learning_rate": 0.0001121690036192792,
        "epoch": 0.31053333333333333,
        "step": 2329
    },
    {
        "loss": 3.0188,
        "grad_norm": 2.836488723754883,
        "learning_rate": 0.00011210653220204266,
        "epoch": 0.31066666666666665,
        "step": 2330
    },
    {
        "loss": 1.7633,
        "grad_norm": 5.385884761810303,
        "learning_rate": 0.00011204405598936743,
        "epoch": 0.3108,
        "step": 2331
    },
    {
        "loss": 2.7519,
        "grad_norm": 2.1367671489715576,
        "learning_rate": 0.00011198157500600062,
        "epoch": 0.31093333333333334,
        "step": 2332
    },
    {
        "loss": 2.1427,
        "grad_norm": 3.534360408782959,
        "learning_rate": 0.0001119190892766911,
        "epoch": 0.31106666666666666,
        "step": 2333
    },
    {
        "loss": 2.8028,
        "grad_norm": 2.4989805221557617,
        "learning_rate": 0.0001118565988261897,
        "epoch": 0.3112,
        "step": 2334
    },
    {
        "loss": 2.4639,
        "grad_norm": 3.119638681411743,
        "learning_rate": 0.00011179410367924911,
        "epoch": 0.31133333333333335,
        "step": 2335
    },
    {
        "loss": 3.0717,
        "grad_norm": 6.600479602813721,
        "learning_rate": 0.00011173160386062385,
        "epoch": 0.31146666666666667,
        "step": 2336
    },
    {
        "loss": 2.2406,
        "grad_norm": 4.312217712402344,
        "learning_rate": 0.00011166909939507036,
        "epoch": 0.3116,
        "step": 2337
    },
    {
        "loss": 2.798,
        "grad_norm": 2.5659756660461426,
        "learning_rate": 0.00011160659030734682,
        "epoch": 0.3117333333333333,
        "step": 2338
    },
    {
        "loss": 2.7571,
        "grad_norm": 3.1699047088623047,
        "learning_rate": 0.00011154407662221331,
        "epoch": 0.3118666666666667,
        "step": 2339
    },
    {
        "loss": 2.1068,
        "grad_norm": 3.2247865200042725,
        "learning_rate": 0.00011148155836443173,
        "epoch": 0.312,
        "step": 2340
    },
    {
        "loss": 2.1114,
        "grad_norm": 3.4920971393585205,
        "learning_rate": 0.00011141903555876574,
        "epoch": 0.3121333333333333,
        "step": 2341
    },
    {
        "loss": 2.2696,
        "grad_norm": 3.0060434341430664,
        "learning_rate": 0.00011135650822998086,
        "epoch": 0.3122666666666667,
        "step": 2342
    },
    {
        "loss": 2.374,
        "grad_norm": 2.534881830215454,
        "learning_rate": 0.00011129397640284441,
        "epoch": 0.3124,
        "step": 2343
    },
    {
        "loss": 1.3603,
        "grad_norm": 3.481275796890259,
        "learning_rate": 0.00011123144010212536,
        "epoch": 0.31253333333333333,
        "step": 2344
    },
    {
        "loss": 0.9602,
        "grad_norm": 3.5593671798706055,
        "learning_rate": 0.00011116889935259462,
        "epoch": 0.31266666666666665,
        "step": 2345
    },
    {
        "loss": 2.0632,
        "grad_norm": 4.157405376434326,
        "learning_rate": 0.00011110635417902486,
        "epoch": 0.3128,
        "step": 2346
    },
    {
        "loss": 2.7287,
        "grad_norm": 3.5689635276794434,
        "learning_rate": 0.00011104380460619032,
        "epoch": 0.31293333333333334,
        "step": 2347
    },
    {
        "loss": 1.8712,
        "grad_norm": 3.2467386722564697,
        "learning_rate": 0.00011098125065886713,
        "epoch": 0.31306666666666666,
        "step": 2348
    },
    {
        "loss": 1.0546,
        "grad_norm": 3.5105795860290527,
        "learning_rate": 0.00011091869236183316,
        "epoch": 0.3132,
        "step": 2349
    },
    {
        "loss": 1.9925,
        "grad_norm": 4.152871131896973,
        "learning_rate": 0.00011085612973986795,
        "epoch": 0.31333333333333335,
        "step": 2350
    },
    {
        "loss": 1.7393,
        "grad_norm": 4.611542701721191,
        "learning_rate": 0.00011079356281775272,
        "epoch": 0.31346666666666667,
        "step": 2351
    },
    {
        "loss": 2.0469,
        "grad_norm": 3.702789306640625,
        "learning_rate": 0.00011073099162027052,
        "epoch": 0.3136,
        "step": 2352
    },
    {
        "loss": 2.5966,
        "grad_norm": 3.8684606552124023,
        "learning_rate": 0.00011066841617220594,
        "epoch": 0.3137333333333333,
        "step": 2353
    },
    {
        "loss": 2.3098,
        "grad_norm": 3.3441579341888428,
        "learning_rate": 0.00011060583649834537,
        "epoch": 0.3138666666666667,
        "step": 2354
    },
    {
        "loss": 2.5015,
        "grad_norm": 2.914283514022827,
        "learning_rate": 0.00011054325262347683,
        "epoch": 0.314,
        "step": 2355
    },
    {
        "loss": 2.8038,
        "grad_norm": 3.190676212310791,
        "learning_rate": 0.00011048066457239,
        "epoch": 0.3141333333333333,
        "step": 2356
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.45039701461792,
        "learning_rate": 0.00011041807236987623,
        "epoch": 0.3142666666666667,
        "step": 2357
    },
    {
        "loss": 1.0492,
        "grad_norm": 3.691190242767334,
        "learning_rate": 0.00011035547604072847,
        "epoch": 0.3144,
        "step": 2358
    },
    {
        "loss": 2.8283,
        "grad_norm": 3.504547119140625,
        "learning_rate": 0.00011029287560974142,
        "epoch": 0.31453333333333333,
        "step": 2359
    },
    {
        "loss": 2.9906,
        "grad_norm": 2.4094226360321045,
        "learning_rate": 0.00011023027110171122,
        "epoch": 0.31466666666666665,
        "step": 2360
    },
    {
        "loss": 2.8317,
        "grad_norm": 3.2703495025634766,
        "learning_rate": 0.00011016766254143578,
        "epoch": 0.3148,
        "step": 2361
    },
    {
        "loss": 2.582,
        "grad_norm": 2.1819546222686768,
        "learning_rate": 0.00011010504995371458,
        "epoch": 0.31493333333333334,
        "step": 2362
    },
    {
        "loss": 2.6256,
        "grad_norm": 3.1187024116516113,
        "learning_rate": 0.00011004243336334865,
        "epoch": 0.31506666666666666,
        "step": 2363
    },
    {
        "loss": 2.5638,
        "grad_norm": 2.829648494720459,
        "learning_rate": 0.00010997981279514067,
        "epoch": 0.3152,
        "step": 2364
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.6277718544006348,
        "learning_rate": 0.00010991718827389482,
        "epoch": 0.31533333333333335,
        "step": 2365
    },
    {
        "loss": 1.0626,
        "grad_norm": 3.4543263912200928,
        "learning_rate": 0.00010985455982441688,
        "epoch": 0.3154666666666667,
        "step": 2366
    },
    {
        "loss": 2.7418,
        "grad_norm": 2.446211576461792,
        "learning_rate": 0.00010979192747151423,
        "epoch": 0.3156,
        "step": 2367
    },
    {
        "loss": 2.4603,
        "grad_norm": 3.5380327701568604,
        "learning_rate": 0.00010972929123999578,
        "epoch": 0.3157333333333333,
        "step": 2368
    },
    {
        "loss": 1.7646,
        "grad_norm": 3.3842661380767822,
        "learning_rate": 0.00010966665115467188,
        "epoch": 0.3158666666666667,
        "step": 2369
    },
    {
        "loss": 1.7892,
        "grad_norm": 4.474334239959717,
        "learning_rate": 0.00010960400724035453,
        "epoch": 0.316,
        "step": 2370
    },
    {
        "loss": 3.0339,
        "grad_norm": 2.0613808631896973,
        "learning_rate": 0.00010954135952185718,
        "epoch": 0.3161333333333333,
        "step": 2371
    },
    {
        "loss": 1.5456,
        "grad_norm": 4.120320796966553,
        "learning_rate": 0.00010947870802399482,
        "epoch": 0.31626666666666664,
        "step": 2372
    },
    {
        "loss": 2.3604,
        "grad_norm": 1.6086626052856445,
        "learning_rate": 0.0001094160527715839,
        "epoch": 0.3164,
        "step": 2373
    },
    {
        "loss": 2.673,
        "grad_norm": 2.1000239849090576,
        "learning_rate": 0.0001093533937894424,
        "epoch": 0.31653333333333333,
        "step": 2374
    },
    {
        "loss": 2.1957,
        "grad_norm": 3.809424877166748,
        "learning_rate": 0.00010929073110238975,
        "epoch": 0.31666666666666665,
        "step": 2375
    },
    {
        "loss": 2.2238,
        "grad_norm": 3.6512649059295654,
        "learning_rate": 0.00010922806473524683,
        "epoch": 0.3168,
        "step": 2376
    },
    {
        "loss": 2.9735,
        "grad_norm": 3.0955305099487305,
        "learning_rate": 0.00010916539471283607,
        "epoch": 0.31693333333333334,
        "step": 2377
    },
    {
        "loss": 3.0144,
        "grad_norm": 2.3154547214508057,
        "learning_rate": 0.0001091027210599812,
        "epoch": 0.31706666666666666,
        "step": 2378
    },
    {
        "loss": 3.0073,
        "grad_norm": 2.6561572551727295,
        "learning_rate": 0.00010904004380150749,
        "epoch": 0.3172,
        "step": 2379
    },
    {
        "loss": 2.7863,
        "grad_norm": 2.59799861907959,
        "learning_rate": 0.00010897736296224165,
        "epoch": 0.31733333333333336,
        "step": 2380
    },
    {
        "loss": 1.4107,
        "grad_norm": 2.2744295597076416,
        "learning_rate": 0.00010891467856701177,
        "epoch": 0.3174666666666667,
        "step": 2381
    },
    {
        "loss": 2.4956,
        "grad_norm": 2.5531225204467773,
        "learning_rate": 0.00010885199064064733,
        "epoch": 0.3176,
        "step": 2382
    },
    {
        "loss": 2.5895,
        "grad_norm": 4.5090107917785645,
        "learning_rate": 0.0001087892992079792,
        "epoch": 0.3177333333333333,
        "step": 2383
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.977854013442993,
        "learning_rate": 0.00010872660429383974,
        "epoch": 0.3178666666666667,
        "step": 2384
    },
    {
        "loss": 2.79,
        "grad_norm": 2.183624505996704,
        "learning_rate": 0.00010866390592306257,
        "epoch": 0.318,
        "step": 2385
    },
    {
        "loss": 2.4095,
        "grad_norm": 2.0377485752105713,
        "learning_rate": 0.00010860120412048274,
        "epoch": 0.3181333333333333,
        "step": 2386
    },
    {
        "loss": 2.4367,
        "grad_norm": 3.1870431900024414,
        "learning_rate": 0.00010853849891093664,
        "epoch": 0.31826666666666664,
        "step": 2387
    },
    {
        "loss": 2.32,
        "grad_norm": 3.0948102474212646,
        "learning_rate": 0.00010847579031926205,
        "epoch": 0.3184,
        "step": 2388
    },
    {
        "loss": 1.8993,
        "grad_norm": 3.8566482067108154,
        "learning_rate": 0.00010841307837029801,
        "epoch": 0.31853333333333333,
        "step": 2389
    },
    {
        "loss": 2.764,
        "grad_norm": 2.241583824157715,
        "learning_rate": 0.000108350363088885,
        "epoch": 0.31866666666666665,
        "step": 2390
    },
    {
        "loss": 2.5558,
        "grad_norm": 2.6660523414611816,
        "learning_rate": 0.00010828764449986467,
        "epoch": 0.3188,
        "step": 2391
    },
    {
        "loss": 1.1479,
        "grad_norm": 4.086541652679443,
        "learning_rate": 0.00010822492262808012,
        "epoch": 0.31893333333333335,
        "step": 2392
    },
    {
        "loss": 1.4349,
        "grad_norm": 2.411606550216675,
        "learning_rate": 0.00010816219749837575,
        "epoch": 0.31906666666666667,
        "step": 2393
    },
    {
        "loss": 2.4667,
        "grad_norm": 3.942181348800659,
        "learning_rate": 0.00010809946913559708,
        "epoch": 0.3192,
        "step": 2394
    },
    {
        "loss": 1.3778,
        "grad_norm": 3.5600132942199707,
        "learning_rate": 0.0001080367375645911,
        "epoch": 0.31933333333333336,
        "step": 2395
    },
    {
        "loss": 1.8481,
        "grad_norm": 3.0994811058044434,
        "learning_rate": 0.00010797400281020596,
        "epoch": 0.3194666666666667,
        "step": 2396
    },
    {
        "loss": 2.4069,
        "grad_norm": 3.2132740020751953,
        "learning_rate": 0.00010791126489729117,
        "epoch": 0.3196,
        "step": 2397
    },
    {
        "loss": 1.6366,
        "grad_norm": 1.7570276260375977,
        "learning_rate": 0.00010784852385069739,
        "epoch": 0.3197333333333333,
        "step": 2398
    },
    {
        "loss": 2.4222,
        "grad_norm": 4.126562118530273,
        "learning_rate": 0.00010778577969527656,
        "epoch": 0.3198666666666667,
        "step": 2399
    },
    {
        "loss": 1.0658,
        "grad_norm": 5.540383815765381,
        "learning_rate": 0.0001077230324558819,
        "epoch": 0.32,
        "step": 2400
    },
    {
        "loss": 1.929,
        "grad_norm": 4.300139427185059,
        "learning_rate": 0.00010766028215736774,
        "epoch": 0.3201333333333333,
        "step": 2401
    },
    {
        "loss": 2.2424,
        "grad_norm": 3.49066162109375,
        "learning_rate": 0.00010759752882458972,
        "epoch": 0.32026666666666664,
        "step": 2402
    },
    {
        "loss": 2.8851,
        "grad_norm": 3.710341453552246,
        "learning_rate": 0.00010753477248240464,
        "epoch": 0.3204,
        "step": 2403
    },
    {
        "loss": 1.9954,
        "grad_norm": 2.4208478927612305,
        "learning_rate": 0.00010747201315567049,
        "epoch": 0.32053333333333334,
        "step": 2404
    },
    {
        "loss": 2.9997,
        "grad_norm": 3.258422613143921,
        "learning_rate": 0.00010740925086924649,
        "epoch": 0.32066666666666666,
        "step": 2405
    },
    {
        "loss": 2.4757,
        "grad_norm": 3.872756004333496,
        "learning_rate": 0.00010734648564799301,
        "epoch": 0.3208,
        "step": 2406
    },
    {
        "loss": 1.8107,
        "grad_norm": 4.58758544921875,
        "learning_rate": 0.00010728371751677147,
        "epoch": 0.32093333333333335,
        "step": 2407
    },
    {
        "loss": 2.3988,
        "grad_norm": 3.471980094909668,
        "learning_rate": 0.00010722094650044461,
        "epoch": 0.32106666666666667,
        "step": 2408
    },
    {
        "loss": 2.7952,
        "grad_norm": 3.2738428115844727,
        "learning_rate": 0.00010715817262387629,
        "epoch": 0.3212,
        "step": 2409
    },
    {
        "loss": 1.2121,
        "grad_norm": 4.368852615356445,
        "learning_rate": 0.00010709539591193136,
        "epoch": 0.32133333333333336,
        "step": 2410
    },
    {
        "loss": 2.0248,
        "grad_norm": 3.5902111530303955,
        "learning_rate": 0.00010703261638947592,
        "epoch": 0.3214666666666667,
        "step": 2411
    },
    {
        "loss": 0.851,
        "grad_norm": 3.6252634525299072,
        "learning_rate": 0.00010696983408137719,
        "epoch": 0.3216,
        "step": 2412
    },
    {
        "loss": 1.2679,
        "grad_norm": 3.757941961288452,
        "learning_rate": 0.00010690704901250343,
        "epoch": 0.3217333333333333,
        "step": 2413
    },
    {
        "loss": 1.5763,
        "grad_norm": 3.601911783218384,
        "learning_rate": 0.00010684426120772401,
        "epoch": 0.3218666666666667,
        "step": 2414
    },
    {
        "loss": 2.3208,
        "grad_norm": 3.267932653427124,
        "learning_rate": 0.00010678147069190942,
        "epoch": 0.322,
        "step": 2415
    },
    {
        "loss": 2.344,
        "grad_norm": 2.039884328842163,
        "learning_rate": 0.00010671867748993116,
        "epoch": 0.3221333333333333,
        "step": 2416
    },
    {
        "loss": 2.6589,
        "grad_norm": 3.0268006324768066,
        "learning_rate": 0.00010665588162666184,
        "epoch": 0.32226666666666665,
        "step": 2417
    },
    {
        "loss": 2.5572,
        "grad_norm": 2.49985933303833,
        "learning_rate": 0.00010659308312697515,
        "epoch": 0.3224,
        "step": 2418
    },
    {
        "loss": 1.8718,
        "grad_norm": 3.192863702774048,
        "learning_rate": 0.00010653028201574577,
        "epoch": 0.32253333333333334,
        "step": 2419
    },
    {
        "loss": 2.5079,
        "grad_norm": 3.389158010482788,
        "learning_rate": 0.00010646747831784945,
        "epoch": 0.32266666666666666,
        "step": 2420
    },
    {
        "loss": 2.6,
        "grad_norm": 3.796123743057251,
        "learning_rate": 0.0001064046720581629,
        "epoch": 0.3228,
        "step": 2421
    },
    {
        "loss": 2.637,
        "grad_norm": 3.3540937900543213,
        "learning_rate": 0.00010634186326156396,
        "epoch": 0.32293333333333335,
        "step": 2422
    },
    {
        "loss": 2.2393,
        "grad_norm": 3.1534433364868164,
        "learning_rate": 0.00010627905195293135,
        "epoch": 0.32306666666666667,
        "step": 2423
    },
    {
        "loss": 2.6139,
        "grad_norm": 1.9003578424453735,
        "learning_rate": 0.00010621623815714486,
        "epoch": 0.3232,
        "step": 2424
    },
    {
        "loss": 2.6343,
        "grad_norm": 2.2627413272857666,
        "learning_rate": 0.00010615342189908526,
        "epoch": 0.3233333333333333,
        "step": 2425
    },
    {
        "loss": 2.1674,
        "grad_norm": 7.579405307769775,
        "learning_rate": 0.00010609060320363428,
        "epoch": 0.3234666666666667,
        "step": 2426
    },
    {
        "loss": 1.9817,
        "grad_norm": 3.518289804458618,
        "learning_rate": 0.00010602778209567463,
        "epoch": 0.3236,
        "step": 2427
    },
    {
        "loss": 1.9633,
        "grad_norm": 3.3399815559387207,
        "learning_rate": 0.00010596495860008995,
        "epoch": 0.3237333333333333,
        "step": 2428
    },
    {
        "loss": 2.4681,
        "grad_norm": 2.5665862560272217,
        "learning_rate": 0.00010590213274176481,
        "epoch": 0.3238666666666667,
        "step": 2429
    },
    {
        "loss": 2.6178,
        "grad_norm": 2.3493340015411377,
        "learning_rate": 0.00010583930454558481,
        "epoch": 0.324,
        "step": 2430
    },
    {
        "loss": 2.6298,
        "grad_norm": 2.4174587726593018,
        "learning_rate": 0.0001057764740364364,
        "epoch": 0.32413333333333333,
        "step": 2431
    },
    {
        "loss": 1.9587,
        "grad_norm": 3.8448197841644287,
        "learning_rate": 0.00010571364123920691,
        "epoch": 0.32426666666666665,
        "step": 2432
    },
    {
        "loss": 2.336,
        "grad_norm": 2.874372720718384,
        "learning_rate": 0.00010565080617878467,
        "epoch": 0.3244,
        "step": 2433
    },
    {
        "loss": 2.858,
        "grad_norm": 3.2295970916748047,
        "learning_rate": 0.00010558796888005884,
        "epoch": 0.32453333333333334,
        "step": 2434
    },
    {
        "loss": 2.1824,
        "grad_norm": 2.6312053203582764,
        "learning_rate": 0.0001055251293679195,
        "epoch": 0.32466666666666666,
        "step": 2435
    },
    {
        "loss": 2.4545,
        "grad_norm": 2.610989570617676,
        "learning_rate": 0.00010546228766725757,
        "epoch": 0.3248,
        "step": 2436
    },
    {
        "loss": 1.7147,
        "grad_norm": 2.836580991744995,
        "learning_rate": 0.00010539944380296489,
        "epoch": 0.32493333333333335,
        "step": 2437
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.86411190032959,
        "learning_rate": 0.00010533659779993414,
        "epoch": 0.32506666666666667,
        "step": 2438
    },
    {
        "loss": 2.4217,
        "grad_norm": 2.7900502681732178,
        "learning_rate": 0.0001052737496830588,
        "epoch": 0.3252,
        "step": 2439
    },
    {
        "loss": 1.7125,
        "grad_norm": 2.9546725749969482,
        "learning_rate": 0.00010521089947723326,
        "epoch": 0.3253333333333333,
        "step": 2440
    },
    {
        "loss": 1.2494,
        "grad_norm": 3.4005939960479736,
        "learning_rate": 0.0001051480472073527,
        "epoch": 0.3254666666666667,
        "step": 2441
    },
    {
        "loss": 1.1121,
        "grad_norm": 3.7190349102020264,
        "learning_rate": 0.00010508519289831305,
        "epoch": 0.3256,
        "step": 2442
    },
    {
        "loss": 1.9931,
        "grad_norm": 3.173560380935669,
        "learning_rate": 0.00010502233657501119,
        "epoch": 0.3257333333333333,
        "step": 2443
    },
    {
        "loss": 1.8014,
        "grad_norm": 3.073201894760132,
        "learning_rate": 0.00010495947826234471,
        "epoch": 0.3258666666666667,
        "step": 2444
    },
    {
        "loss": 2.5278,
        "grad_norm": 1.7477049827575684,
        "learning_rate": 0.00010489661798521197,
        "epoch": 0.326,
        "step": 2445
    },
    {
        "loss": 1.806,
        "grad_norm": 3.0699899196624756,
        "learning_rate": 0.00010483375576851215,
        "epoch": 0.32613333333333333,
        "step": 2446
    },
    {
        "loss": 2.4092,
        "grad_norm": 2.8389077186584473,
        "learning_rate": 0.00010477089163714524,
        "epoch": 0.32626666666666665,
        "step": 2447
    },
    {
        "loss": 2.5278,
        "grad_norm": 3.430189371109009,
        "learning_rate": 0.00010470802561601189,
        "epoch": 0.3264,
        "step": 2448
    },
    {
        "loss": 2.6053,
        "grad_norm": 2.601025104522705,
        "learning_rate": 0.00010464515773001351,
        "epoch": 0.32653333333333334,
        "step": 2449
    },
    {
        "loss": 2.4644,
        "grad_norm": 2.7523598670959473,
        "learning_rate": 0.00010458228800405237,
        "epoch": 0.32666666666666666,
        "step": 2450
    },
    {
        "loss": 2.4877,
        "grad_norm": 3.079646587371826,
        "learning_rate": 0.00010451941646303132,
        "epoch": 0.3268,
        "step": 2451
    },
    {
        "loss": 1.6766,
        "grad_norm": 3.1418673992156982,
        "learning_rate": 0.00010445654313185402,
        "epoch": 0.32693333333333335,
        "step": 2452
    },
    {
        "loss": 2.2869,
        "grad_norm": 3.834109306335449,
        "learning_rate": 0.00010439366803542479,
        "epoch": 0.32706666666666667,
        "step": 2453
    },
    {
        "loss": 2.1039,
        "grad_norm": 3.355985403060913,
        "learning_rate": 0.00010433079119864866,
        "epoch": 0.3272,
        "step": 2454
    },
    {
        "loss": 2.185,
        "grad_norm": 3.4724721908569336,
        "learning_rate": 0.0001042679126464314,
        "epoch": 0.3273333333333333,
        "step": 2455
    },
    {
        "loss": 2.447,
        "grad_norm": 2.882542610168457,
        "learning_rate": 0.00010420503240367939,
        "epoch": 0.3274666666666667,
        "step": 2456
    },
    {
        "loss": 2.3919,
        "grad_norm": 2.6489953994750977,
        "learning_rate": 0.00010414215049529971,
        "epoch": 0.3276,
        "step": 2457
    },
    {
        "loss": 1.9779,
        "grad_norm": 4.941983699798584,
        "learning_rate": 0.00010407926694620013,
        "epoch": 0.3277333333333333,
        "step": 2458
    },
    {
        "loss": 2.1925,
        "grad_norm": 2.0333194732666016,
        "learning_rate": 0.00010401638178128898,
        "epoch": 0.32786666666666664,
        "step": 2459
    },
    {
        "loss": 2.4599,
        "grad_norm": 3.0521931648254395,
        "learning_rate": 0.00010395349502547538,
        "epoch": 0.328,
        "step": 2460
    },
    {
        "loss": 2.7145,
        "grad_norm": 3.0494015216827393,
        "learning_rate": 0.00010389060670366891,
        "epoch": 0.32813333333333333,
        "step": 2461
    },
    {
        "loss": 2.6034,
        "grad_norm": 3.122619867324829,
        "learning_rate": 0.0001038277168407798,
        "epoch": 0.32826666666666665,
        "step": 2462
    },
    {
        "loss": 3.0268,
        "grad_norm": 2.1533079147338867,
        "learning_rate": 0.00010376482546171908,
        "epoch": 0.3284,
        "step": 2463
    },
    {
        "loss": 1.8114,
        "grad_norm": 3.531468629837036,
        "learning_rate": 0.00010370193259139815,
        "epoch": 0.32853333333333334,
        "step": 2464
    },
    {
        "loss": 2.0435,
        "grad_norm": 3.1485002040863037,
        "learning_rate": 0.0001036390382547291,
        "epoch": 0.32866666666666666,
        "step": 2465
    },
    {
        "loss": 2.1124,
        "grad_norm": 3.164400577545166,
        "learning_rate": 0.0001035761424766246,
        "epoch": 0.3288,
        "step": 2466
    },
    {
        "loss": 2.6837,
        "grad_norm": 3.72290301322937,
        "learning_rate": 0.00010351324528199786,
        "epoch": 0.32893333333333336,
        "step": 2467
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.6609723567962646,
        "learning_rate": 0.00010345034669576272,
        "epoch": 0.3290666666666667,
        "step": 2468
    },
    {
        "loss": 2.6217,
        "grad_norm": 2.83483624458313,
        "learning_rate": 0.00010338744674283351,
        "epoch": 0.3292,
        "step": 2469
    },
    {
        "loss": 2.7312,
        "grad_norm": 2.859226942062378,
        "learning_rate": 0.00010332454544812506,
        "epoch": 0.3293333333333333,
        "step": 2470
    },
    {
        "loss": 2.3868,
        "grad_norm": 2.208704710006714,
        "learning_rate": 0.00010326164283655284,
        "epoch": 0.3294666666666667,
        "step": 2471
    },
    {
        "loss": 2.6193,
        "grad_norm": 2.37394118309021,
        "learning_rate": 0.0001031987389330328,
        "epoch": 0.3296,
        "step": 2472
    },
    {
        "loss": 2.0284,
        "grad_norm": 3.7148337364196777,
        "learning_rate": 0.00010313583376248137,
        "epoch": 0.3297333333333333,
        "step": 2473
    },
    {
        "loss": 1.9397,
        "grad_norm": 3.635744571685791,
        "learning_rate": 0.00010307292734981546,
        "epoch": 0.32986666666666664,
        "step": 2474
    },
    {
        "loss": 2.5081,
        "grad_norm": 4.073071479797363,
        "learning_rate": 0.00010301001971995259,
        "epoch": 0.33,
        "step": 2475
    },
    {
        "loss": 3.5345,
        "grad_norm": 3.685626745223999,
        "learning_rate": 0.00010294711089781064,
        "epoch": 0.33013333333333333,
        "step": 2476
    },
    {
        "loss": 1.8467,
        "grad_norm": 3.8694422245025635,
        "learning_rate": 0.00010288420090830802,
        "epoch": 0.33026666666666665,
        "step": 2477
    },
    {
        "loss": 1.7021,
        "grad_norm": 4.116323471069336,
        "learning_rate": 0.0001028212897763636,
        "epoch": 0.3304,
        "step": 2478
    },
    {
        "loss": 1.7368,
        "grad_norm": 3.02484393119812,
        "learning_rate": 0.0001027583775268967,
        "epoch": 0.33053333333333335,
        "step": 2479
    },
    {
        "loss": 1.6159,
        "grad_norm": 4.1314778327941895,
        "learning_rate": 0.00010269546418482703,
        "epoch": 0.33066666666666666,
        "step": 2480
    },
    {
        "loss": 2.7007,
        "grad_norm": 2.887057065963745,
        "learning_rate": 0.00010263254977507483,
        "epoch": 0.3308,
        "step": 2481
    },
    {
        "loss": 2.6233,
        "grad_norm": 2.484342336654663,
        "learning_rate": 0.0001025696343225607,
        "epoch": 0.33093333333333336,
        "step": 2482
    },
    {
        "loss": 1.9173,
        "grad_norm": 3.9435343742370605,
        "learning_rate": 0.00010250671785220567,
        "epoch": 0.3310666666666667,
        "step": 2483
    },
    {
        "loss": 2.7327,
        "grad_norm": 2.5401878356933594,
        "learning_rate": 0.00010244380038893114,
        "epoch": 0.3312,
        "step": 2484
    },
    {
        "loss": 1.6269,
        "grad_norm": 1.8925142288208008,
        "learning_rate": 0.00010238088195765896,
        "epoch": 0.3313333333333333,
        "step": 2485
    },
    {
        "loss": 2.2476,
        "grad_norm": 3.482988119125366,
        "learning_rate": 0.00010231796258331132,
        "epoch": 0.3314666666666667,
        "step": 2486
    },
    {
        "loss": 1.9533,
        "grad_norm": 3.4982759952545166,
        "learning_rate": 0.00010225504229081078,
        "epoch": 0.3316,
        "step": 2487
    },
    {
        "loss": 2.3131,
        "grad_norm": 3.333009958267212,
        "learning_rate": 0.00010219212110508032,
        "epoch": 0.3317333333333333,
        "step": 2488
    },
    {
        "loss": 1.0928,
        "grad_norm": 3.029050350189209,
        "learning_rate": 0.00010212919905104324,
        "epoch": 0.33186666666666664,
        "step": 2489
    },
    {
        "loss": 2.4737,
        "grad_norm": 2.2372875213623047,
        "learning_rate": 0.00010206627615362314,
        "epoch": 0.332,
        "step": 2490
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.1433210372924805,
        "learning_rate": 0.00010200335243774402,
        "epoch": 0.33213333333333334,
        "step": 2491
    },
    {
        "loss": 2.3679,
        "grad_norm": 3.6147348880767822,
        "learning_rate": 0.00010194042792833016,
        "epoch": 0.33226666666666665,
        "step": 2492
    },
    {
        "loss": 1.7512,
        "grad_norm": 3.9177160263061523,
        "learning_rate": 0.0001018775026503062,
        "epoch": 0.3324,
        "step": 2493
    },
    {
        "loss": 2.3219,
        "grad_norm": 3.282503128051758,
        "learning_rate": 0.00010181457662859706,
        "epoch": 0.33253333333333335,
        "step": 2494
    },
    {
        "loss": 2.7961,
        "grad_norm": 2.672788619995117,
        "learning_rate": 0.00010175164988812791,
        "epoch": 0.33266666666666667,
        "step": 2495
    },
    {
        "loss": 2.3299,
        "grad_norm": 3.033512592315674,
        "learning_rate": 0.00010168872245382424,
        "epoch": 0.3328,
        "step": 2496
    },
    {
        "loss": 2.0294,
        "grad_norm": 4.172811031341553,
        "learning_rate": 0.00010162579435061188,
        "epoch": 0.33293333333333336,
        "step": 2497
    },
    {
        "loss": 2.3385,
        "grad_norm": 2.875807762145996,
        "learning_rate": 0.00010156286560341685,
        "epoch": 0.3330666666666667,
        "step": 2498
    },
    {
        "loss": 3.0247,
        "grad_norm": 2.102264404296875,
        "learning_rate": 0.00010149993623716544,
        "epoch": 0.3332,
        "step": 2499
    },
    {
        "loss": 2.7759,
        "grad_norm": 2.345097541809082,
        "learning_rate": 0.00010143700627678414,
        "epoch": 0.3333333333333333,
        "step": 2500
    },
    {
        "loss": 1.8569,
        "grad_norm": 3.996307849884033,
        "learning_rate": 0.0001013740757471998,
        "epoch": 0.3334666666666667,
        "step": 2501
    },
    {
        "loss": 2.1042,
        "grad_norm": 3.459702491760254,
        "learning_rate": 0.00010131114467333935,
        "epoch": 0.3336,
        "step": 2502
    },
    {
        "loss": 3.0301,
        "grad_norm": 3.1274962425231934,
        "learning_rate": 0.00010124821308013001,
        "epoch": 0.3337333333333333,
        "step": 2503
    },
    {
        "loss": 1.9589,
        "grad_norm": 3.3986616134643555,
        "learning_rate": 0.00010118528099249926,
        "epoch": 0.33386666666666664,
        "step": 2504
    },
    {
        "loss": 2.946,
        "grad_norm": 3.109893798828125,
        "learning_rate": 0.00010112234843537463,
        "epoch": 0.334,
        "step": 2505
    },
    {
        "loss": 0.5364,
        "grad_norm": 2.369062662124634,
        "learning_rate": 0.00010105941543368398,
        "epoch": 0.33413333333333334,
        "step": 2506
    },
    {
        "loss": 2.2034,
        "grad_norm": 3.603832960128784,
        "learning_rate": 0.00010099648201235528,
        "epoch": 0.33426666666666666,
        "step": 2507
    },
    {
        "loss": 3.0089,
        "grad_norm": 3.8345515727996826,
        "learning_rate": 0.00010093354819631662,
        "epoch": 0.3344,
        "step": 2508
    },
    {
        "loss": 2.6638,
        "grad_norm": 2.790445327758789,
        "learning_rate": 0.00010087061401049635,
        "epoch": 0.33453333333333335,
        "step": 2509
    },
    {
        "loss": 1.4446,
        "grad_norm": 3.6111836433410645,
        "learning_rate": 0.00010080767947982292,
        "epoch": 0.33466666666666667,
        "step": 2510
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.6850520372390747,
        "learning_rate": 0.00010074474462922492,
        "epoch": 0.3348,
        "step": 2511
    },
    {
        "loss": 2.7653,
        "grad_norm": 3.0359721183776855,
        "learning_rate": 0.00010068180948363097,
        "epoch": 0.33493333333333336,
        "step": 2512
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.9842348098754883,
        "learning_rate": 0.00010061887406797001,
        "epoch": 0.3350666666666667,
        "step": 2513
    },
    {
        "loss": 2.9744,
        "grad_norm": 4.841479778289795,
        "learning_rate": 0.0001005559384071709,
        "epoch": 0.3352,
        "step": 2514
    },
    {
        "loss": 2.7541,
        "grad_norm": 2.290001630783081,
        "learning_rate": 0.00010049300252616271,
        "epoch": 0.3353333333333333,
        "step": 2515
    },
    {
        "loss": 2.2706,
        "grad_norm": 3.045172691345215,
        "learning_rate": 0.00010043006644987453,
        "epoch": 0.3354666666666667,
        "step": 2516
    },
    {
        "loss": 1.1972,
        "grad_norm": 3.312268018722534,
        "learning_rate": 0.00010036713020323556,
        "epoch": 0.3356,
        "step": 2517
    },
    {
        "loss": 1.6733,
        "grad_norm": 3.9009203910827637,
        "learning_rate": 0.00010030419381117504,
        "epoch": 0.33573333333333333,
        "step": 2518
    },
    {
        "loss": 2.3382,
        "grad_norm": 3.0091609954833984,
        "learning_rate": 0.00010024125729862235,
        "epoch": 0.33586666666666665,
        "step": 2519
    },
    {
        "loss": 1.911,
        "grad_norm": 3.2522640228271484,
        "learning_rate": 0.00010017832069050681,
        "epoch": 0.336,
        "step": 2520
    },
    {
        "loss": 2.0641,
        "grad_norm": 3.046132802963257,
        "learning_rate": 0.00010011538401175786,
        "epoch": 0.33613333333333334,
        "step": 2521
    },
    {
        "loss": 2.6575,
        "grad_norm": 3.4428765773773193,
        "learning_rate": 0.00010005244728730487,
        "epoch": 0.33626666666666666,
        "step": 2522
    },
    {
        "loss": 2.0337,
        "grad_norm": 2.069612979888916,
        "learning_rate": 9.998951054207739e-05,
        "epoch": 0.3364,
        "step": 2523
    },
    {
        "loss": 2.1879,
        "grad_norm": 2.663968563079834,
        "learning_rate": 9.992657380100478e-05,
        "epoch": 0.33653333333333335,
        "step": 2524
    },
    {
        "loss": 2.4273,
        "grad_norm": 3.704458475112915,
        "learning_rate": 9.986363708901653e-05,
        "epoch": 0.33666666666666667,
        "step": 2525
    },
    {
        "loss": 2.5089,
        "grad_norm": 4.17243766784668,
        "learning_rate": 9.98007004310421e-05,
        "epoch": 0.3368,
        "step": 2526
    },
    {
        "loss": 1.6999,
        "grad_norm": 3.3273940086364746,
        "learning_rate": 9.973776385201092e-05,
        "epoch": 0.3369333333333333,
        "step": 2527
    },
    {
        "loss": 2.7829,
        "grad_norm": 3.1792728900909424,
        "learning_rate": 9.967482737685237e-05,
        "epoch": 0.3370666666666667,
        "step": 2528
    },
    {
        "loss": 2.4208,
        "grad_norm": 3.2574353218078613,
        "learning_rate": 9.961189103049582e-05,
        "epoch": 0.3372,
        "step": 2529
    },
    {
        "loss": 2.6719,
        "grad_norm": 3.35829758644104,
        "learning_rate": 9.95489548378705e-05,
        "epoch": 0.3373333333333333,
        "step": 2530
    },
    {
        "loss": 2.8048,
        "grad_norm": 3.6864051818847656,
        "learning_rate": 9.948601882390575e-05,
        "epoch": 0.3374666666666667,
        "step": 2531
    },
    {
        "loss": 1.881,
        "grad_norm": 2.349339723587036,
        "learning_rate": 9.94230830135307e-05,
        "epoch": 0.3376,
        "step": 2532
    },
    {
        "loss": 2.8411,
        "grad_norm": 3.329423666000366,
        "learning_rate": 9.936014743167438e-05,
        "epoch": 0.33773333333333333,
        "step": 2533
    },
    {
        "loss": 2.9343,
        "grad_norm": 2.860297918319702,
        "learning_rate": 9.929721210326591e-05,
        "epoch": 0.33786666666666665,
        "step": 2534
    },
    {
        "loss": 2.4751,
        "grad_norm": 3.154116630554199,
        "learning_rate": 9.923427705323409e-05,
        "epoch": 0.338,
        "step": 2535
    },
    {
        "loss": 2.1305,
        "grad_norm": 3.9461750984191895,
        "learning_rate": 9.917134230650775e-05,
        "epoch": 0.33813333333333334,
        "step": 2536
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.1236155033111572,
        "learning_rate": 9.910840788801552e-05,
        "epoch": 0.33826666666666666,
        "step": 2537
    },
    {
        "loss": 1.8165,
        "grad_norm": 3.736556053161621,
        "learning_rate": 9.904547382268599e-05,
        "epoch": 0.3384,
        "step": 2538
    },
    {
        "loss": 3.0321,
        "grad_norm": 3.382791042327881,
        "learning_rate": 9.89825401354475e-05,
        "epoch": 0.33853333333333335,
        "step": 2539
    },
    {
        "loss": 2.9309,
        "grad_norm": 2.5304572582244873,
        "learning_rate": 9.891960685122835e-05,
        "epoch": 0.33866666666666667,
        "step": 2540
    },
    {
        "loss": 2.4646,
        "grad_norm": 3.938711643218994,
        "learning_rate": 9.885667399495661e-05,
        "epoch": 0.3388,
        "step": 2541
    },
    {
        "loss": 2.6053,
        "grad_norm": 4.526046276092529,
        "learning_rate": 9.879374159156019e-05,
        "epoch": 0.3389333333333333,
        "step": 2542
    },
    {
        "loss": 2.6258,
        "grad_norm": 2.913898468017578,
        "learning_rate": 9.873080966596684e-05,
        "epoch": 0.3390666666666667,
        "step": 2543
    },
    {
        "loss": 1.569,
        "grad_norm": 2.97847056388855,
        "learning_rate": 9.86678782431041e-05,
        "epoch": 0.3392,
        "step": 2544
    },
    {
        "loss": 2.2473,
        "grad_norm": 3.1015028953552246,
        "learning_rate": 9.860494734789937e-05,
        "epoch": 0.3393333333333333,
        "step": 2545
    },
    {
        "loss": 2.7764,
        "grad_norm": 2.6784276962280273,
        "learning_rate": 9.854201700527972e-05,
        "epoch": 0.3394666666666667,
        "step": 2546
    },
    {
        "loss": 2.0311,
        "grad_norm": 3.7739927768707275,
        "learning_rate": 9.847908724017205e-05,
        "epoch": 0.3396,
        "step": 2547
    },
    {
        "loss": 1.8697,
        "grad_norm": 3.682910442352295,
        "learning_rate": 9.841615807750317e-05,
        "epoch": 0.33973333333333333,
        "step": 2548
    },
    {
        "loss": 1.8746,
        "grad_norm": 4.227255821228027,
        "learning_rate": 9.835322954219947e-05,
        "epoch": 0.33986666666666665,
        "step": 2549
    },
    {
        "loss": 2.8131,
        "grad_norm": 2.111569881439209,
        "learning_rate": 9.829030165918713e-05,
        "epoch": 0.34,
        "step": 2550
    },
    {
        "loss": 2.4423,
        "grad_norm": 5.085254192352295,
        "learning_rate": 9.822737445339213e-05,
        "epoch": 0.34013333333333334,
        "step": 2551
    },
    {
        "loss": 2.5806,
        "grad_norm": 3.6966958045959473,
        "learning_rate": 9.816444794974017e-05,
        "epoch": 0.34026666666666666,
        "step": 2552
    },
    {
        "loss": 2.7661,
        "grad_norm": 2.958522081375122,
        "learning_rate": 9.810152217315662e-05,
        "epoch": 0.3404,
        "step": 2553
    },
    {
        "loss": 2.5315,
        "grad_norm": 2.3598439693450928,
        "learning_rate": 9.803859714856664e-05,
        "epoch": 0.34053333333333335,
        "step": 2554
    },
    {
        "loss": 2.6001,
        "grad_norm": 3.242093086242676,
        "learning_rate": 9.797567290089494e-05,
        "epoch": 0.3406666666666667,
        "step": 2555
    },
    {
        "loss": 3.2082,
        "grad_norm": 3.307957649230957,
        "learning_rate": 9.791274945506614e-05,
        "epoch": 0.3408,
        "step": 2556
    },
    {
        "loss": 2.0986,
        "grad_norm": 2.5303890705108643,
        "learning_rate": 9.784982683600439e-05,
        "epoch": 0.3409333333333333,
        "step": 2557
    },
    {
        "loss": 2.0017,
        "grad_norm": 2.7862937450408936,
        "learning_rate": 9.778690506863357e-05,
        "epoch": 0.3410666666666667,
        "step": 2558
    },
    {
        "loss": 2.8606,
        "grad_norm": 3.423386812210083,
        "learning_rate": 9.772398417787716e-05,
        "epoch": 0.3412,
        "step": 2559
    },
    {
        "loss": 2.5222,
        "grad_norm": 3.0524909496307373,
        "learning_rate": 9.76610641886584e-05,
        "epoch": 0.3413333333333333,
        "step": 2560
    },
    {
        "loss": 2.9873,
        "grad_norm": 2.481170177459717,
        "learning_rate": 9.75981451259001e-05,
        "epoch": 0.34146666666666664,
        "step": 2561
    },
    {
        "loss": 1.6858,
        "grad_norm": 2.1008830070495605,
        "learning_rate": 9.75352270145247e-05,
        "epoch": 0.3416,
        "step": 2562
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.576258897781372,
        "learning_rate": 9.747230987945425e-05,
        "epoch": 0.34173333333333333,
        "step": 2563
    },
    {
        "loss": 2.5151,
        "grad_norm": 2.593247413635254,
        "learning_rate": 9.740939374561047e-05,
        "epoch": 0.34186666666666665,
        "step": 2564
    },
    {
        "loss": 2.705,
        "grad_norm": 2.5345730781555176,
        "learning_rate": 9.734647863791468e-05,
        "epoch": 0.342,
        "step": 2565
    },
    {
        "loss": 2.5089,
        "grad_norm": 2.886573076248169,
        "learning_rate": 9.728356458128774e-05,
        "epoch": 0.34213333333333334,
        "step": 2566
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.8012319803237915,
        "learning_rate": 9.722065160065009e-05,
        "epoch": 0.34226666666666666,
        "step": 2567
    },
    {
        "loss": 2.6103,
        "grad_norm": 2.5982565879821777,
        "learning_rate": 9.715773972092184e-05,
        "epoch": 0.3424,
        "step": 2568
    },
    {
        "loss": 2.2795,
        "grad_norm": 2.9510011672973633,
        "learning_rate": 9.709482896702256e-05,
        "epoch": 0.34253333333333336,
        "step": 2569
    },
    {
        "loss": 1.9406,
        "grad_norm": 4.695241451263428,
        "learning_rate": 9.703191936387146e-05,
        "epoch": 0.3426666666666667,
        "step": 2570
    },
    {
        "loss": 0.9686,
        "grad_norm": 3.149022340774536,
        "learning_rate": 9.69690109363872e-05,
        "epoch": 0.3428,
        "step": 2571
    },
    {
        "loss": 2.7501,
        "grad_norm": 2.7585837841033936,
        "learning_rate": 9.690610370948799e-05,
        "epoch": 0.3429333333333333,
        "step": 2572
    },
    {
        "loss": 2.5096,
        "grad_norm": 3.9214251041412354,
        "learning_rate": 9.684319770809169e-05,
        "epoch": 0.3430666666666667,
        "step": 2573
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.728602409362793,
        "learning_rate": 9.678029295711552e-05,
        "epoch": 0.3432,
        "step": 2574
    },
    {
        "loss": 2.0417,
        "grad_norm": 2.7528717517852783,
        "learning_rate": 9.671738948147625e-05,
        "epoch": 0.3433333333333333,
        "step": 2575
    },
    {
        "loss": 2.8347,
        "grad_norm": 2.776632070541382,
        "learning_rate": 9.665448730609023e-05,
        "epoch": 0.34346666666666664,
        "step": 2576
    },
    {
        "loss": 2.1483,
        "grad_norm": 3.128883123397827,
        "learning_rate": 9.659158645587316e-05,
        "epoch": 0.3436,
        "step": 2577
    },
    {
        "loss": 1.8204,
        "grad_norm": 4.194844722747803,
        "learning_rate": 9.652868695574033e-05,
        "epoch": 0.34373333333333334,
        "step": 2578
    },
    {
        "loss": 3.2557,
        "grad_norm": 7.498651027679443,
        "learning_rate": 9.646578883060643e-05,
        "epoch": 0.34386666666666665,
        "step": 2579
    },
    {
        "loss": 2.1713,
        "grad_norm": 3.297619581222534,
        "learning_rate": 9.64028921053856e-05,
        "epoch": 0.344,
        "step": 2580
    },
    {
        "loss": 2.2729,
        "grad_norm": 3.802356004714966,
        "learning_rate": 9.633999680499147e-05,
        "epoch": 0.34413333333333335,
        "step": 2581
    },
    {
        "loss": 2.6789,
        "grad_norm": 2.0435585975646973,
        "learning_rate": 9.627710295433707e-05,
        "epoch": 0.34426666666666667,
        "step": 2582
    },
    {
        "loss": 2.0984,
        "grad_norm": 2.551107883453369,
        "learning_rate": 9.621421057833489e-05,
        "epoch": 0.3444,
        "step": 2583
    },
    {
        "loss": 2.0064,
        "grad_norm": 3.604037284851074,
        "learning_rate": 9.615131970189679e-05,
        "epoch": 0.34453333333333336,
        "step": 2584
    },
    {
        "loss": 2.1984,
        "grad_norm": 3.7033777236938477,
        "learning_rate": 9.608843034993407e-05,
        "epoch": 0.3446666666666667,
        "step": 2585
    },
    {
        "loss": 2.5643,
        "grad_norm": 3.2581496238708496,
        "learning_rate": 9.602554254735745e-05,
        "epoch": 0.3448,
        "step": 2586
    },
    {
        "loss": 0.7436,
        "grad_norm": 2.7134511470794678,
        "learning_rate": 9.596265631907694e-05,
        "epoch": 0.3449333333333333,
        "step": 2587
    },
    {
        "loss": 2.5414,
        "grad_norm": 2.1762609481811523,
        "learning_rate": 9.589977169000203e-05,
        "epoch": 0.3450666666666667,
        "step": 2588
    },
    {
        "loss": 2.9731,
        "grad_norm": 2.790752410888672,
        "learning_rate": 9.58368886850415e-05,
        "epoch": 0.3452,
        "step": 2589
    },
    {
        "loss": 2.927,
        "grad_norm": 2.9646878242492676,
        "learning_rate": 9.577400732910356e-05,
        "epoch": 0.3453333333333333,
        "step": 2590
    },
    {
        "loss": 2.8639,
        "grad_norm": 2.1603634357452393,
        "learning_rate": 9.571112764709573e-05,
        "epoch": 0.34546666666666664,
        "step": 2591
    },
    {
        "loss": 2.4734,
        "grad_norm": 3.319767475128174,
        "learning_rate": 9.564824966392482e-05,
        "epoch": 0.3456,
        "step": 2592
    },
    {
        "loss": 1.8276,
        "grad_norm": 2.8459248542785645,
        "learning_rate": 9.558537340449706e-05,
        "epoch": 0.34573333333333334,
        "step": 2593
    },
    {
        "loss": 2.2738,
        "grad_norm": 2.872138738632202,
        "learning_rate": 9.552249889371795e-05,
        "epoch": 0.34586666666666666,
        "step": 2594
    },
    {
        "loss": 2.0934,
        "grad_norm": 3.9218931198120117,
        "learning_rate": 9.54596261564923e-05,
        "epoch": 0.346,
        "step": 2595
    },
    {
        "loss": 2.3141,
        "grad_norm": 2.9139230251312256,
        "learning_rate": 9.539675521772419e-05,
        "epoch": 0.34613333333333335,
        "step": 2596
    },
    {
        "loss": 2.6006,
        "grad_norm": 4.691288471221924,
        "learning_rate": 9.5333886102317e-05,
        "epoch": 0.34626666666666667,
        "step": 2597
    },
    {
        "loss": 2.7233,
        "grad_norm": 2.2758328914642334,
        "learning_rate": 9.52710188351734e-05,
        "epoch": 0.3464,
        "step": 2598
    },
    {
        "loss": 1.4882,
        "grad_norm": 2.5793447494506836,
        "learning_rate": 9.520815344119542e-05,
        "epoch": 0.34653333333333336,
        "step": 2599
    },
    {
        "loss": 1.7073,
        "grad_norm": 3.479790687561035,
        "learning_rate": 9.514528994528416e-05,
        "epoch": 0.3466666666666667,
        "step": 2600
    },
    {
        "loss": 0.8624,
        "grad_norm": 4.196011543273926,
        "learning_rate": 9.508242837234005e-05,
        "epoch": 0.3468,
        "step": 2601
    },
    {
        "loss": 2.6866,
        "grad_norm": 1.927805781364441,
        "learning_rate": 9.501956874726289e-05,
        "epoch": 0.3469333333333333,
        "step": 2602
    },
    {
        "loss": 2.4136,
        "grad_norm": 4.207065582275391,
        "learning_rate": 9.49567110949515e-05,
        "epoch": 0.3470666666666667,
        "step": 2603
    },
    {
        "loss": 2.2347,
        "grad_norm": 3.371098756790161,
        "learning_rate": 9.489385544030403e-05,
        "epoch": 0.3472,
        "step": 2604
    },
    {
        "loss": 2.3202,
        "grad_norm": 2.8951756954193115,
        "learning_rate": 9.48310018082178e-05,
        "epoch": 0.3473333333333333,
        "step": 2605
    },
    {
        "loss": 2.0969,
        "grad_norm": 3.3939578533172607,
        "learning_rate": 9.476815022358937e-05,
        "epoch": 0.34746666666666665,
        "step": 2606
    },
    {
        "loss": 2.9379,
        "grad_norm": 3.102051019668579,
        "learning_rate": 9.470530071131447e-05,
        "epoch": 0.3476,
        "step": 2607
    },
    {
        "loss": 2.5925,
        "grad_norm": 2.5156009197235107,
        "learning_rate": 9.464245329628803e-05,
        "epoch": 0.34773333333333334,
        "step": 2608
    },
    {
        "loss": 2.139,
        "grad_norm": 2.7136471271514893,
        "learning_rate": 9.457960800340402e-05,
        "epoch": 0.34786666666666666,
        "step": 2609
    },
    {
        "loss": 2.0843,
        "grad_norm": 5.054444313049316,
        "learning_rate": 9.45167648575558e-05,
        "epoch": 0.348,
        "step": 2610
    },
    {
        "loss": 2.2979,
        "grad_norm": 3.23410701751709,
        "learning_rate": 9.445392388363573e-05,
        "epoch": 0.34813333333333335,
        "step": 2611
    },
    {
        "loss": 1.5121,
        "grad_norm": 3.5296835899353027,
        "learning_rate": 9.439108510653529e-05,
        "epoch": 0.34826666666666667,
        "step": 2612
    },
    {
        "loss": 1.5616,
        "grad_norm": 4.040867805480957,
        "learning_rate": 9.432824855114513e-05,
        "epoch": 0.3484,
        "step": 2613
    },
    {
        "loss": 2.6641,
        "grad_norm": 3.8564202785491943,
        "learning_rate": 9.426541424235504e-05,
        "epoch": 0.3485333333333333,
        "step": 2614
    },
    {
        "loss": 2.8523,
        "grad_norm": 3.241482973098755,
        "learning_rate": 9.420258220505392e-05,
        "epoch": 0.3486666666666667,
        "step": 2615
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.9553039073944092,
        "learning_rate": 9.413975246412974e-05,
        "epoch": 0.3488,
        "step": 2616
    },
    {
        "loss": 2.1163,
        "grad_norm": 2.841953992843628,
        "learning_rate": 9.407692504446957e-05,
        "epoch": 0.3489333333333333,
        "step": 2617
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.4727768898010254,
        "learning_rate": 9.401409997095957e-05,
        "epoch": 0.3490666666666667,
        "step": 2618
    },
    {
        "loss": 1.7322,
        "grad_norm": 2.823622941970825,
        "learning_rate": 9.395127726848498e-05,
        "epoch": 0.3492,
        "step": 2619
    },
    {
        "loss": 2.3842,
        "grad_norm": 3.010077714920044,
        "learning_rate": 9.388845696193008e-05,
        "epoch": 0.34933333333333333,
        "step": 2620
    },
    {
        "loss": 2.103,
        "grad_norm": 3.604989767074585,
        "learning_rate": 9.38256390761782e-05,
        "epoch": 0.34946666666666665,
        "step": 2621
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.9129624366760254,
        "learning_rate": 9.376282363611173e-05,
        "epoch": 0.3496,
        "step": 2622
    },
    {
        "loss": 0.7086,
        "grad_norm": 2.6489737033843994,
        "learning_rate": 9.370001066661203e-05,
        "epoch": 0.34973333333333334,
        "step": 2623
    },
    {
        "loss": 2.5428,
        "grad_norm": 1.9077914953231812,
        "learning_rate": 9.363720019255963e-05,
        "epoch": 0.34986666666666666,
        "step": 2624
    },
    {
        "loss": 1.5912,
        "grad_norm": 3.0444583892822266,
        "learning_rate": 9.357439223883388e-05,
        "epoch": 0.35,
        "step": 2625
    },
    {
        "loss": 2.6747,
        "grad_norm": 2.4196996688842773,
        "learning_rate": 9.351158683031326e-05,
        "epoch": 0.35013333333333335,
        "step": 2626
    },
    {
        "loss": 1.4767,
        "grad_norm": 1.6302138566970825,
        "learning_rate": 9.344878399187521e-05,
        "epoch": 0.35026666666666667,
        "step": 2627
    },
    {
        "loss": 2.8256,
        "grad_norm": 2.4242498874664307,
        "learning_rate": 9.338598374839614e-05,
        "epoch": 0.3504,
        "step": 2628
    },
    {
        "loss": 1.145,
        "grad_norm": 3.5043785572052,
        "learning_rate": 9.332318612475145e-05,
        "epoch": 0.3505333333333333,
        "step": 2629
    },
    {
        "loss": 2.6158,
        "grad_norm": 2.039139986038208,
        "learning_rate": 9.326039114581551e-05,
        "epoch": 0.3506666666666667,
        "step": 2630
    },
    {
        "loss": 2.4685,
        "grad_norm": 2.8921701908111572,
        "learning_rate": 9.319759883646155e-05,
        "epoch": 0.3508,
        "step": 2631
    },
    {
        "loss": 3.0066,
        "grad_norm": 2.474163293838501,
        "learning_rate": 9.313480922156188e-05,
        "epoch": 0.3509333333333333,
        "step": 2632
    },
    {
        "loss": 3.3483,
        "grad_norm": 5.152552127838135,
        "learning_rate": 9.307202232598772e-05,
        "epoch": 0.3510666666666667,
        "step": 2633
    },
    {
        "loss": 2.887,
        "grad_norm": 2.590885639190674,
        "learning_rate": 9.300923817460905e-05,
        "epoch": 0.3512,
        "step": 2634
    },
    {
        "loss": 2.4579,
        "grad_norm": 3.3863885402679443,
        "learning_rate": 9.294645679229502e-05,
        "epoch": 0.35133333333333333,
        "step": 2635
    },
    {
        "loss": 3.0107,
        "grad_norm": 2.3636441230773926,
        "learning_rate": 9.288367820391346e-05,
        "epoch": 0.35146666666666665,
        "step": 2636
    },
    {
        "loss": 2.345,
        "grad_norm": 2.2199294567108154,
        "learning_rate": 9.282090243433122e-05,
        "epoch": 0.3516,
        "step": 2637
    },
    {
        "loss": 2.7082,
        "grad_norm": 3.2339916229248047,
        "learning_rate": 9.275812950841397e-05,
        "epoch": 0.35173333333333334,
        "step": 2638
    },
    {
        "loss": 1.7389,
        "grad_norm": 2.8964741230010986,
        "learning_rate": 9.269535945102632e-05,
        "epoch": 0.35186666666666666,
        "step": 2639
    },
    {
        "loss": 2.6873,
        "grad_norm": 2.557058572769165,
        "learning_rate": 9.263259228703167e-05,
        "epoch": 0.352,
        "step": 2640
    },
    {
        "loss": 2.4319,
        "grad_norm": 2.0604326725006104,
        "learning_rate": 9.256982804129231e-05,
        "epoch": 0.35213333333333335,
        "step": 2641
    },
    {
        "loss": 3.1723,
        "grad_norm": 2.911832332611084,
        "learning_rate": 9.25070667386694e-05,
        "epoch": 0.3522666666666667,
        "step": 2642
    },
    {
        "loss": 2.1071,
        "grad_norm": 3.186241626739502,
        "learning_rate": 9.244430840402287e-05,
        "epoch": 0.3524,
        "step": 2643
    },
    {
        "loss": 2.9026,
        "grad_norm": 3.8095014095306396,
        "learning_rate": 9.238155306221153e-05,
        "epoch": 0.3525333333333333,
        "step": 2644
    },
    {
        "loss": 2.3347,
        "grad_norm": 2.35528826713562,
        "learning_rate": 9.2318800738093e-05,
        "epoch": 0.3526666666666667,
        "step": 2645
    },
    {
        "loss": 2.2485,
        "grad_norm": 3.4162726402282715,
        "learning_rate": 9.225605145652369e-05,
        "epoch": 0.3528,
        "step": 2646
    },
    {
        "loss": 0.747,
        "grad_norm": 3.4983174800872803,
        "learning_rate": 9.219330524235875e-05,
        "epoch": 0.3529333333333333,
        "step": 2647
    },
    {
        "loss": 2.298,
        "grad_norm": 3.2677807807922363,
        "learning_rate": 9.213056212045217e-05,
        "epoch": 0.35306666666666664,
        "step": 2648
    },
    {
        "loss": 2.7471,
        "grad_norm": 2.860520362854004,
        "learning_rate": 9.206782211565681e-05,
        "epoch": 0.3532,
        "step": 2649
    },
    {
        "loss": 1.6948,
        "grad_norm": 5.5457940101623535,
        "learning_rate": 9.200508525282413e-05,
        "epoch": 0.35333333333333333,
        "step": 2650
    },
    {
        "loss": 2.1419,
        "grad_norm": 2.371455192565918,
        "learning_rate": 9.194235155680439e-05,
        "epoch": 0.35346666666666665,
        "step": 2651
    },
    {
        "loss": 2.7929,
        "grad_norm": 2.583981513977051,
        "learning_rate": 9.187962105244667e-05,
        "epoch": 0.3536,
        "step": 2652
    },
    {
        "loss": 2.7188,
        "grad_norm": 3.958927631378174,
        "learning_rate": 9.181689376459872e-05,
        "epoch": 0.35373333333333334,
        "step": 2653
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.221625328063965,
        "learning_rate": 9.175416971810704e-05,
        "epoch": 0.35386666666666666,
        "step": 2654
    },
    {
        "loss": 1.809,
        "grad_norm": 4.080897331237793,
        "learning_rate": 9.169144893781685e-05,
        "epoch": 0.354,
        "step": 2655
    },
    {
        "loss": 2.3842,
        "grad_norm": 2.6555163860321045,
        "learning_rate": 9.1628731448572e-05,
        "epoch": 0.35413333333333336,
        "step": 2656
    },
    {
        "loss": 0.9996,
        "grad_norm": 3.598876953125,
        "learning_rate": 9.156601727521519e-05,
        "epoch": 0.3542666666666667,
        "step": 2657
    },
    {
        "loss": 0.5184,
        "grad_norm": 1.7709829807281494,
        "learning_rate": 9.150330644258762e-05,
        "epoch": 0.3544,
        "step": 2658
    },
    {
        "loss": 1.8185,
        "grad_norm": 2.7625248432159424,
        "learning_rate": 9.144059897552935e-05,
        "epoch": 0.3545333333333333,
        "step": 2659
    },
    {
        "loss": 1.6375,
        "grad_norm": 3.1825079917907715,
        "learning_rate": 9.137789489887899e-05,
        "epoch": 0.3546666666666667,
        "step": 2660
    },
    {
        "loss": 2.0224,
        "grad_norm": 1.850244402885437,
        "learning_rate": 9.131519423747386e-05,
        "epoch": 0.3548,
        "step": 2661
    },
    {
        "loss": 2.4114,
        "grad_norm": 3.3140792846679688,
        "learning_rate": 9.125249701614989e-05,
        "epoch": 0.3549333333333333,
        "step": 2662
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.305124282836914,
        "learning_rate": 9.118980325974165e-05,
        "epoch": 0.35506666666666664,
        "step": 2663
    },
    {
        "loss": 2.3747,
        "grad_norm": 4.110213279724121,
        "learning_rate": 9.112711299308235e-05,
        "epoch": 0.3552,
        "step": 2664
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.969630002975464,
        "learning_rate": 9.10644262410038e-05,
        "epoch": 0.35533333333333333,
        "step": 2665
    },
    {
        "loss": 1.6142,
        "grad_norm": 3.4004621505737305,
        "learning_rate": 9.100174302833651e-05,
        "epoch": 0.35546666666666665,
        "step": 2666
    },
    {
        "loss": 3.045,
        "grad_norm": 2.482849359512329,
        "learning_rate": 9.093906337990945e-05,
        "epoch": 0.3556,
        "step": 2667
    },
    {
        "loss": 1.86,
        "grad_norm": 3.3555898666381836,
        "learning_rate": 9.087638732055024e-05,
        "epoch": 0.35573333333333335,
        "step": 2668
    },
    {
        "loss": 1.5612,
        "grad_norm": 5.3830671310424805,
        "learning_rate": 9.081371487508513e-05,
        "epoch": 0.35586666666666666,
        "step": 2669
    },
    {
        "loss": 1.7648,
        "grad_norm": 5.602005481719971,
        "learning_rate": 9.075104606833884e-05,
        "epoch": 0.356,
        "step": 2670
    },
    {
        "loss": 2.758,
        "grad_norm": 2.7329931259155273,
        "learning_rate": 9.068838092513475e-05,
        "epoch": 0.35613333333333336,
        "step": 2671
    },
    {
        "loss": 2.9439,
        "grad_norm": 2.8980469703674316,
        "learning_rate": 9.062571947029468e-05,
        "epoch": 0.3562666666666667,
        "step": 2672
    },
    {
        "loss": 2.9842,
        "grad_norm": 3.598771572113037,
        "learning_rate": 9.056306172863905e-05,
        "epoch": 0.3564,
        "step": 2673
    },
    {
        "loss": 1.5943,
        "grad_norm": 4.983419895172119,
        "learning_rate": 9.050040772498685e-05,
        "epoch": 0.3565333333333333,
        "step": 2674
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.532116413116455,
        "learning_rate": 9.043775748415552e-05,
        "epoch": 0.3566666666666667,
        "step": 2675
    },
    {
        "loss": 2.5635,
        "grad_norm": 2.5425374507904053,
        "learning_rate": 9.0375111030961e-05,
        "epoch": 0.3568,
        "step": 2676
    },
    {
        "loss": 2.5598,
        "grad_norm": 2.582240104675293,
        "learning_rate": 9.031246839021783e-05,
        "epoch": 0.3569333333333333,
        "step": 2677
    },
    {
        "loss": 2.2346,
        "grad_norm": 3.4057326316833496,
        "learning_rate": 9.024982958673891e-05,
        "epoch": 0.35706666666666664,
        "step": 2678
    },
    {
        "loss": 1.0131,
        "grad_norm": 3.5327939987182617,
        "learning_rate": 9.018719464533573e-05,
        "epoch": 0.3572,
        "step": 2679
    },
    {
        "loss": 2.207,
        "grad_norm": 3.019697666168213,
        "learning_rate": 9.012456359081821e-05,
        "epoch": 0.35733333333333334,
        "step": 2680
    },
    {
        "loss": 2.4559,
        "grad_norm": 2.491217851638794,
        "learning_rate": 9.006193644799469e-05,
        "epoch": 0.35746666666666665,
        "step": 2681
    },
    {
        "loss": 1.8338,
        "grad_norm": 3.1852405071258545,
        "learning_rate": 8.999931324167198e-05,
        "epoch": 0.3576,
        "step": 2682
    },
    {
        "loss": 1.9062,
        "grad_norm": 3.180253744125366,
        "learning_rate": 8.993669399665539e-05,
        "epoch": 0.35773333333333335,
        "step": 2683
    },
    {
        "loss": 1.0127,
        "grad_norm": 3.2214853763580322,
        "learning_rate": 8.987407873774863e-05,
        "epoch": 0.35786666666666667,
        "step": 2684
    },
    {
        "loss": 2.5168,
        "grad_norm": 2.8785717487335205,
        "learning_rate": 8.981146748975372e-05,
        "epoch": 0.358,
        "step": 2685
    },
    {
        "loss": 1.2226,
        "grad_norm": 2.42695951461792,
        "learning_rate": 8.97488602774713e-05,
        "epoch": 0.35813333333333336,
        "step": 2686
    },
    {
        "loss": 2.7078,
        "grad_norm": 2.264636754989624,
        "learning_rate": 8.968625712570026e-05,
        "epoch": 0.3582666666666667,
        "step": 2687
    },
    {
        "loss": 2.6969,
        "grad_norm": 2.947502851486206,
        "learning_rate": 8.96236580592379e-05,
        "epoch": 0.3584,
        "step": 2688
    },
    {
        "loss": 0.9139,
        "grad_norm": 3.529059648513794,
        "learning_rate": 8.956106310287996e-05,
        "epoch": 0.3585333333333333,
        "step": 2689
    },
    {
        "loss": 1.3397,
        "grad_norm": 4.846205711364746,
        "learning_rate": 8.949847228142044e-05,
        "epoch": 0.3586666666666667,
        "step": 2690
    },
    {
        "loss": 1.6331,
        "grad_norm": 2.2626254558563232,
        "learning_rate": 8.943588561965187e-05,
        "epoch": 0.3588,
        "step": 2691
    },
    {
        "loss": 2.0506,
        "grad_norm": 3.887760639190674,
        "learning_rate": 8.937330314236501e-05,
        "epoch": 0.3589333333333333,
        "step": 2692
    },
    {
        "loss": 2.6672,
        "grad_norm": 3.9266858100891113,
        "learning_rate": 8.931072487434895e-05,
        "epoch": 0.35906666666666665,
        "step": 2693
    },
    {
        "loss": 1.9809,
        "grad_norm": 4.278654098510742,
        "learning_rate": 8.924815084039121e-05,
        "epoch": 0.3592,
        "step": 2694
    },
    {
        "loss": 2.8038,
        "grad_norm": 2.1391372680664062,
        "learning_rate": 8.918558106527757e-05,
        "epoch": 0.35933333333333334,
        "step": 2695
    },
    {
        "loss": 1.875,
        "grad_norm": 2.6589250564575195,
        "learning_rate": 8.912301557379214e-05,
        "epoch": 0.35946666666666666,
        "step": 2696
    },
    {
        "loss": 2.4622,
        "grad_norm": 2.0659749507904053,
        "learning_rate": 8.906045439071728e-05,
        "epoch": 0.3596,
        "step": 2697
    },
    {
        "loss": 1.9851,
        "grad_norm": 3.7342846393585205,
        "learning_rate": 8.899789754083368e-05,
        "epoch": 0.35973333333333335,
        "step": 2698
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.437924385070801,
        "learning_rate": 8.893534504892033e-05,
        "epoch": 0.35986666666666667,
        "step": 2699
    },
    {
        "loss": 1.4453,
        "grad_norm": 2.661564350128174,
        "learning_rate": 8.887279693975458e-05,
        "epoch": 0.36,
        "step": 2700
    },
    {
        "loss": 1.3847,
        "grad_norm": 2.613266706466675,
        "learning_rate": 8.881025323811186e-05,
        "epoch": 0.36013333333333336,
        "step": 2701
    },
    {
        "loss": 3.0297,
        "grad_norm": 3.466400623321533,
        "learning_rate": 8.874771396876593e-05,
        "epoch": 0.3602666666666667,
        "step": 2702
    },
    {
        "loss": 1.6342,
        "grad_norm": 5.1289262771606445,
        "learning_rate": 8.868517915648887e-05,
        "epoch": 0.3604,
        "step": 2703
    },
    {
        "loss": 1.538,
        "grad_norm": 5.917500019073486,
        "learning_rate": 8.86226488260509e-05,
        "epoch": 0.3605333333333333,
        "step": 2704
    },
    {
        "loss": 1.8242,
        "grad_norm": 3.8950204849243164,
        "learning_rate": 8.85601230022205e-05,
        "epoch": 0.3606666666666667,
        "step": 2705
    },
    {
        "loss": 1.5491,
        "grad_norm": 3.247969388961792,
        "learning_rate": 8.849760170976435e-05,
        "epoch": 0.3608,
        "step": 2706
    },
    {
        "loss": 2.7152,
        "grad_norm": 4.989951133728027,
        "learning_rate": 8.843508497344734e-05,
        "epoch": 0.36093333333333333,
        "step": 2707
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.6583478450775146,
        "learning_rate": 8.837257281803259e-05,
        "epoch": 0.36106666666666665,
        "step": 2708
    },
    {
        "loss": 2.1467,
        "grad_norm": 3.09684157371521,
        "learning_rate": 8.83100652682814e-05,
        "epoch": 0.3612,
        "step": 2709
    },
    {
        "loss": 2.3179,
        "grad_norm": 2.3432061672210693,
        "learning_rate": 8.824756234895311e-05,
        "epoch": 0.36133333333333334,
        "step": 2710
    },
    {
        "loss": 2.1941,
        "grad_norm": 3.9684290885925293,
        "learning_rate": 8.818506408480548e-05,
        "epoch": 0.36146666666666666,
        "step": 2711
    },
    {
        "loss": 0.9448,
        "grad_norm": 5.72704553604126,
        "learning_rate": 8.812257050059424e-05,
        "epoch": 0.3616,
        "step": 2712
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.7652037143707275,
        "learning_rate": 8.806008162107327e-05,
        "epoch": 0.36173333333333335,
        "step": 2713
    },
    {
        "loss": 2.2632,
        "grad_norm": 2.766247034072876,
        "learning_rate": 8.799759747099467e-05,
        "epoch": 0.36186666666666667,
        "step": 2714
    },
    {
        "loss": 0.8607,
        "grad_norm": 2.8217504024505615,
        "learning_rate": 8.793511807510857e-05,
        "epoch": 0.362,
        "step": 2715
    },
    {
        "loss": 3.0985,
        "grad_norm": 2.9056172370910645,
        "learning_rate": 8.787264345816333e-05,
        "epoch": 0.3621333333333333,
        "step": 2716
    },
    {
        "loss": 1.5203,
        "grad_norm": 5.037911891937256,
        "learning_rate": 8.781017364490534e-05,
        "epoch": 0.3622666666666667,
        "step": 2717
    },
    {
        "loss": 1.8562,
        "grad_norm": 4.483699321746826,
        "learning_rate": 8.77477086600791e-05,
        "epoch": 0.3624,
        "step": 2718
    },
    {
        "loss": 1.7995,
        "grad_norm": 3.433283805847168,
        "learning_rate": 8.768524852842719e-05,
        "epoch": 0.3625333333333333,
        "step": 2719
    },
    {
        "loss": 2.4821,
        "grad_norm": 2.9819178581237793,
        "learning_rate": 8.762279327469033e-05,
        "epoch": 0.3626666666666667,
        "step": 2720
    },
    {
        "loss": 1.8592,
        "grad_norm": 3.358185052871704,
        "learning_rate": 8.756034292360723e-05,
        "epoch": 0.3628,
        "step": 2721
    },
    {
        "loss": 1.6873,
        "grad_norm": 3.671517848968506,
        "learning_rate": 8.749789749991468e-05,
        "epoch": 0.36293333333333333,
        "step": 2722
    },
    {
        "loss": 3.3069,
        "grad_norm": 3.302936315536499,
        "learning_rate": 8.743545702834756e-05,
        "epoch": 0.36306666666666665,
        "step": 2723
    },
    {
        "loss": 2.2582,
        "grad_norm": 3.1935534477233887,
        "learning_rate": 8.737302153363865e-05,
        "epoch": 0.3632,
        "step": 2724
    },
    {
        "loss": 1.0072,
        "grad_norm": 3.4351401329040527,
        "learning_rate": 8.731059104051899e-05,
        "epoch": 0.36333333333333334,
        "step": 2725
    },
    {
        "loss": 2.4085,
        "grad_norm": 1.9759925603866577,
        "learning_rate": 8.724816557371746e-05,
        "epoch": 0.36346666666666666,
        "step": 2726
    },
    {
        "loss": 2.3191,
        "grad_norm": 3.158735752105713,
        "learning_rate": 8.718574515796097e-05,
        "epoch": 0.3636,
        "step": 2727
    },
    {
        "loss": 1.9831,
        "grad_norm": 4.841362476348877,
        "learning_rate": 8.71233298179745e-05,
        "epoch": 0.36373333333333335,
        "step": 2728
    },
    {
        "loss": 2.5012,
        "grad_norm": 2.3819103240966797,
        "learning_rate": 8.706091957848096e-05,
        "epoch": 0.36386666666666667,
        "step": 2729
    },
    {
        "loss": 2.5441,
        "grad_norm": 3.4034650325775146,
        "learning_rate": 8.699851446420127e-05,
        "epoch": 0.364,
        "step": 2730
    },
    {
        "loss": 2.3358,
        "grad_norm": 3.118154525756836,
        "learning_rate": 8.693611449985432e-05,
        "epoch": 0.3641333333333333,
        "step": 2731
    },
    {
        "loss": 2.106,
        "grad_norm": 2.697200059890747,
        "learning_rate": 8.687371971015687e-05,
        "epoch": 0.3642666666666667,
        "step": 2732
    },
    {
        "loss": 2.0509,
        "grad_norm": 3.8915226459503174,
        "learning_rate": 8.681133011982382e-05,
        "epoch": 0.3644,
        "step": 2733
    },
    {
        "loss": 2.7727,
        "grad_norm": 3.097093343734741,
        "learning_rate": 8.674894575356787e-05,
        "epoch": 0.3645333333333333,
        "step": 2734
    },
    {
        "loss": 2.6825,
        "grad_norm": 2.326479434967041,
        "learning_rate": 8.668656663609959e-05,
        "epoch": 0.36466666666666664,
        "step": 2735
    },
    {
        "loss": 2.618,
        "grad_norm": 2.7630233764648438,
        "learning_rate": 8.662419279212768e-05,
        "epoch": 0.3648,
        "step": 2736
    },
    {
        "loss": 2.5416,
        "grad_norm": 3.1396946907043457,
        "learning_rate": 8.656182424635858e-05,
        "epoch": 0.36493333333333333,
        "step": 2737
    },
    {
        "loss": 2.5726,
        "grad_norm": 2.679426908493042,
        "learning_rate": 8.649946102349663e-05,
        "epoch": 0.36506666666666665,
        "step": 2738
    },
    {
        "loss": 2.4694,
        "grad_norm": 3.51529598236084,
        "learning_rate": 8.643710314824424e-05,
        "epoch": 0.3652,
        "step": 2739
    },
    {
        "loss": 1.8755,
        "grad_norm": 4.039224624633789,
        "learning_rate": 8.637475064530149e-05,
        "epoch": 0.36533333333333334,
        "step": 2740
    },
    {
        "loss": 2.1389,
        "grad_norm": 3.3539485931396484,
        "learning_rate": 8.631240353936642e-05,
        "epoch": 0.36546666666666666,
        "step": 2741
    },
    {
        "loss": 2.595,
        "grad_norm": 3.3444671630859375,
        "learning_rate": 8.625006185513496e-05,
        "epoch": 0.3656,
        "step": 2742
    },
    {
        "loss": 2.7677,
        "grad_norm": 3.004243850708008,
        "learning_rate": 8.618772561730084e-05,
        "epoch": 0.36573333333333335,
        "step": 2743
    },
    {
        "loss": 2.3335,
        "grad_norm": 2.31481671333313,
        "learning_rate": 8.612539485055566e-05,
        "epoch": 0.3658666666666667,
        "step": 2744
    },
    {
        "loss": 2.1767,
        "grad_norm": 3.022656202316284,
        "learning_rate": 8.606306957958887e-05,
        "epoch": 0.366,
        "step": 2745
    },
    {
        "loss": 2.377,
        "grad_norm": 2.6559360027313232,
        "learning_rate": 8.600074982908773e-05,
        "epoch": 0.3661333333333333,
        "step": 2746
    },
    {
        "loss": 2.9185,
        "grad_norm": 3.2094640731811523,
        "learning_rate": 8.593843562373728e-05,
        "epoch": 0.3662666666666667,
        "step": 2747
    },
    {
        "loss": 2.8238,
        "grad_norm": 2.9063258171081543,
        "learning_rate": 8.587612698822037e-05,
        "epoch": 0.3664,
        "step": 2748
    },
    {
        "loss": 2.6631,
        "grad_norm": 3.019653797149658,
        "learning_rate": 8.581382394721766e-05,
        "epoch": 0.3665333333333333,
        "step": 2749
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.062140941619873,
        "learning_rate": 8.575152652540768e-05,
        "epoch": 0.36666666666666664,
        "step": 2750
    },
    {
        "loss": 2.7202,
        "grad_norm": 2.1116011142730713,
        "learning_rate": 8.568923474746657e-05,
        "epoch": 0.3668,
        "step": 2751
    },
    {
        "loss": 2.7385,
        "grad_norm": 2.8075039386749268,
        "learning_rate": 8.562694863806832e-05,
        "epoch": 0.36693333333333333,
        "step": 2752
    },
    {
        "loss": 2.9546,
        "grad_norm": 3.8232369422912598,
        "learning_rate": 8.556466822188471e-05,
        "epoch": 0.36706666666666665,
        "step": 2753
    },
    {
        "loss": 2.6228,
        "grad_norm": 2.1142172813415527,
        "learning_rate": 8.550239352358521e-05,
        "epoch": 0.3672,
        "step": 2754
    },
    {
        "loss": 2.542,
        "grad_norm": 3.618281841278076,
        "learning_rate": 8.544012456783704e-05,
        "epoch": 0.36733333333333335,
        "step": 2755
    },
    {
        "loss": 1.9876,
        "grad_norm": 3.7126922607421875,
        "learning_rate": 8.537786137930514e-05,
        "epoch": 0.36746666666666666,
        "step": 2756
    },
    {
        "loss": 2.8928,
        "grad_norm": 3.0277695655822754,
        "learning_rate": 8.531560398265212e-05,
        "epoch": 0.3676,
        "step": 2757
    },
    {
        "loss": 2.9172,
        "grad_norm": 2.2985610961914062,
        "learning_rate": 8.525335240253842e-05,
        "epoch": 0.36773333333333336,
        "step": 2758
    },
    {
        "loss": 1.3181,
        "grad_norm": 3.302870273590088,
        "learning_rate": 8.51911066636221e-05,
        "epoch": 0.3678666666666667,
        "step": 2759
    },
    {
        "loss": 2.262,
        "grad_norm": 2.981797695159912,
        "learning_rate": 8.512886679055889e-05,
        "epoch": 0.368,
        "step": 2760
    },
    {
        "loss": 2.7752,
        "grad_norm": 2.347835063934326,
        "learning_rate": 8.50666328080022e-05,
        "epoch": 0.3681333333333333,
        "step": 2761
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.2944252490997314,
        "learning_rate": 8.500440474060316e-05,
        "epoch": 0.3682666666666667,
        "step": 2762
    },
    {
        "loss": 1.9186,
        "grad_norm": 1.7106449604034424,
        "learning_rate": 8.494218261301052e-05,
        "epoch": 0.3684,
        "step": 2763
    },
    {
        "loss": 2.4292,
        "grad_norm": 4.372190475463867,
        "learning_rate": 8.487996644987062e-05,
        "epoch": 0.3685333333333333,
        "step": 2764
    },
    {
        "loss": 2.5182,
        "grad_norm": 4.972283840179443,
        "learning_rate": 8.481775627582754e-05,
        "epoch": 0.36866666666666664,
        "step": 2765
    },
    {
        "loss": 2.8198,
        "grad_norm": 2.3575851917266846,
        "learning_rate": 8.475555211552292e-05,
        "epoch": 0.3688,
        "step": 2766
    },
    {
        "loss": 2.1409,
        "grad_norm": 3.7063283920288086,
        "learning_rate": 8.469335399359606e-05,
        "epoch": 0.36893333333333334,
        "step": 2767
    },
    {
        "loss": 3.0368,
        "grad_norm": 2.49967622756958,
        "learning_rate": 8.463116193468384e-05,
        "epoch": 0.36906666666666665,
        "step": 2768
    },
    {
        "loss": 2.7884,
        "grad_norm": 2.701446294784546,
        "learning_rate": 8.456897596342072e-05,
        "epoch": 0.3692,
        "step": 2769
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.9860265254974365,
        "learning_rate": 8.450679610443883e-05,
        "epoch": 0.36933333333333335,
        "step": 2770
    },
    {
        "loss": 1.6815,
        "grad_norm": 3.6774744987487793,
        "learning_rate": 8.44446223823678e-05,
        "epoch": 0.36946666666666667,
        "step": 2771
    },
    {
        "loss": 3.0821,
        "grad_norm": 2.5842063426971436,
        "learning_rate": 8.438245482183486e-05,
        "epoch": 0.3696,
        "step": 2772
    },
    {
        "loss": 3.496,
        "grad_norm": 5.596450328826904,
        "learning_rate": 8.432029344746477e-05,
        "epoch": 0.36973333333333336,
        "step": 2773
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.762087821960449,
        "learning_rate": 8.425813828387984e-05,
        "epoch": 0.3698666666666667,
        "step": 2774
    },
    {
        "loss": 2.3623,
        "grad_norm": 2.754441499710083,
        "learning_rate": 8.41959893557e-05,
        "epoch": 0.37,
        "step": 2775
    },
    {
        "loss": 2.6867,
        "grad_norm": 4.402784824371338,
        "learning_rate": 8.413384668754263e-05,
        "epoch": 0.3701333333333333,
        "step": 2776
    },
    {
        "loss": 1.4767,
        "grad_norm": 3.797243118286133,
        "learning_rate": 8.407171030402262e-05,
        "epoch": 0.3702666666666667,
        "step": 2777
    },
    {
        "loss": 2.9384,
        "grad_norm": 2.5079827308654785,
        "learning_rate": 8.400958022975246e-05,
        "epoch": 0.3704,
        "step": 2778
    },
    {
        "loss": 1.863,
        "grad_norm": 3.6371970176696777,
        "learning_rate": 8.394745648934204e-05,
        "epoch": 0.3705333333333333,
        "step": 2779
    },
    {
        "loss": 1.3631,
        "grad_norm": 4.4658660888671875,
        "learning_rate": 8.388533910739882e-05,
        "epoch": 0.37066666666666664,
        "step": 2780
    },
    {
        "loss": 2.4638,
        "grad_norm": 2.341832399368286,
        "learning_rate": 8.382322810852769e-05,
        "epoch": 0.3708,
        "step": 2781
    },
    {
        "loss": 1.1324,
        "grad_norm": 4.21499490737915,
        "learning_rate": 8.376112351733102e-05,
        "epoch": 0.37093333333333334,
        "step": 2782
    },
    {
        "loss": 2.7634,
        "grad_norm": 2.9464504718780518,
        "learning_rate": 8.369902535840864e-05,
        "epoch": 0.37106666666666666,
        "step": 2783
    },
    {
        "loss": 2.3967,
        "grad_norm": 2.962151527404785,
        "learning_rate": 8.363693365635788e-05,
        "epoch": 0.3712,
        "step": 2784
    },
    {
        "loss": 2.1853,
        "grad_norm": 3.3563010692596436,
        "learning_rate": 8.357484843577348e-05,
        "epoch": 0.37133333333333335,
        "step": 2785
    },
    {
        "loss": 2.8591,
        "grad_norm": 2.0849578380584717,
        "learning_rate": 8.351276972124752e-05,
        "epoch": 0.37146666666666667,
        "step": 2786
    },
    {
        "loss": 2.2408,
        "grad_norm": 3.5125632286071777,
        "learning_rate": 8.34506975373697e-05,
        "epoch": 0.3716,
        "step": 2787
    },
    {
        "loss": 1.6662,
        "grad_norm": 3.2513341903686523,
        "learning_rate": 8.338863190872697e-05,
        "epoch": 0.37173333333333336,
        "step": 2788
    },
    {
        "loss": 1.4727,
        "grad_norm": 3.7148096561431885,
        "learning_rate": 8.332657285990375e-05,
        "epoch": 0.3718666666666667,
        "step": 2789
    },
    {
        "loss": 2.8086,
        "grad_norm": 3.211327075958252,
        "learning_rate": 8.326452041548182e-05,
        "epoch": 0.372,
        "step": 2790
    },
    {
        "loss": 2.8025,
        "grad_norm": 2.639841079711914,
        "learning_rate": 8.320247460004036e-05,
        "epoch": 0.3721333333333333,
        "step": 2791
    },
    {
        "loss": 2.4547,
        "grad_norm": 2.9980733394622803,
        "learning_rate": 8.314043543815596e-05,
        "epoch": 0.3722666666666667,
        "step": 2792
    },
    {
        "loss": 2.3386,
        "grad_norm": 3.4300339221954346,
        "learning_rate": 8.307840295440254e-05,
        "epoch": 0.3724,
        "step": 2793
    },
    {
        "loss": 2.5431,
        "grad_norm": 3.3517982959747314,
        "learning_rate": 8.301637717335135e-05,
        "epoch": 0.3725333333333333,
        "step": 2794
    },
    {
        "loss": 2.7209,
        "grad_norm": 5.533868789672852,
        "learning_rate": 8.295435811957105e-05,
        "epoch": 0.37266666666666665,
        "step": 2795
    },
    {
        "loss": 1.7793,
        "grad_norm": 3.714773416519165,
        "learning_rate": 8.289234581762758e-05,
        "epoch": 0.3728,
        "step": 2796
    },
    {
        "loss": 2.2475,
        "grad_norm": 2.885223150253296,
        "learning_rate": 8.283034029208426e-05,
        "epoch": 0.37293333333333334,
        "step": 2797
    },
    {
        "loss": 1.972,
        "grad_norm": 4.109581470489502,
        "learning_rate": 8.276834156750161e-05,
        "epoch": 0.37306666666666666,
        "step": 2798
    },
    {
        "loss": 2.6951,
        "grad_norm": 3.497985363006592,
        "learning_rate": 8.270634966843756e-05,
        "epoch": 0.3732,
        "step": 2799
    },
    {
        "loss": 1.6171,
        "grad_norm": 2.958583354949951,
        "learning_rate": 8.264436461944732e-05,
        "epoch": 0.37333333333333335,
        "step": 2800
    },
    {
        "loss": 2.0227,
        "grad_norm": 3.293638229370117,
        "learning_rate": 8.258238644508346e-05,
        "epoch": 0.37346666666666667,
        "step": 2801
    },
    {
        "loss": 2.7764,
        "grad_norm": 3.3381011486053467,
        "learning_rate": 8.252041516989564e-05,
        "epoch": 0.3736,
        "step": 2802
    },
    {
        "loss": 2.3129,
        "grad_norm": 2.724156618118286,
        "learning_rate": 8.245845081843091e-05,
        "epoch": 0.3737333333333333,
        "step": 2803
    },
    {
        "loss": 1.88,
        "grad_norm": 2.8298630714416504,
        "learning_rate": 8.239649341523363e-05,
        "epoch": 0.3738666666666667,
        "step": 2804
    },
    {
        "loss": 3.0955,
        "grad_norm": 1.7553138732910156,
        "learning_rate": 8.23345429848453e-05,
        "epoch": 0.374,
        "step": 2805
    },
    {
        "loss": 1.8198,
        "grad_norm": 3.456102132797241,
        "learning_rate": 8.227259955180465e-05,
        "epoch": 0.3741333333333333,
        "step": 2806
    },
    {
        "loss": 2.4923,
        "grad_norm": 3.0283498764038086,
        "learning_rate": 8.221066314064774e-05,
        "epoch": 0.3742666666666667,
        "step": 2807
    },
    {
        "loss": 1.9469,
        "grad_norm": 4.081887245178223,
        "learning_rate": 8.214873377590775e-05,
        "epoch": 0.3744,
        "step": 2808
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.9550939798355103,
        "learning_rate": 8.208681148211516e-05,
        "epoch": 0.37453333333333333,
        "step": 2809
    },
    {
        "loss": 0.6048,
        "grad_norm": 2.3572535514831543,
        "learning_rate": 8.202489628379759e-05,
        "epoch": 0.37466666666666665,
        "step": 2810
    },
    {
        "loss": 1.2801,
        "grad_norm": 3.7499499320983887,
        "learning_rate": 8.19629882054798e-05,
        "epoch": 0.3748,
        "step": 2811
    },
    {
        "loss": 2.4569,
        "grad_norm": 3.9961514472961426,
        "learning_rate": 8.190108727168391e-05,
        "epoch": 0.37493333333333334,
        "step": 2812
    },
    {
        "loss": 2.7015,
        "grad_norm": 2.4600324630737305,
        "learning_rate": 8.183919350692899e-05,
        "epoch": 0.37506666666666666,
        "step": 2813
    },
    {
        "loss": 0.9562,
        "grad_norm": 2.1969118118286133,
        "learning_rate": 8.177730693573142e-05,
        "epoch": 0.3752,
        "step": 2814
    },
    {
        "loss": 2.4936,
        "grad_norm": 3.2954981327056885,
        "learning_rate": 8.171542758260464e-05,
        "epoch": 0.37533333333333335,
        "step": 2815
    },
    {
        "loss": 2.4561,
        "grad_norm": 2.562742233276367,
        "learning_rate": 8.165355547205929e-05,
        "epoch": 0.37546666666666667,
        "step": 2816
    },
    {
        "loss": 1.6946,
        "grad_norm": 3.8787503242492676,
        "learning_rate": 8.159169062860315e-05,
        "epoch": 0.3756,
        "step": 2817
    },
    {
        "loss": 2.2559,
        "grad_norm": 3.1488261222839355,
        "learning_rate": 8.152983307674108e-05,
        "epoch": 0.3757333333333333,
        "step": 2818
    },
    {
        "loss": 1.6973,
        "grad_norm": 4.421343803405762,
        "learning_rate": 8.146798284097504e-05,
        "epoch": 0.3758666666666667,
        "step": 2819
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.4723403453826904,
        "learning_rate": 8.140613994580414e-05,
        "epoch": 0.376,
        "step": 2820
    },
    {
        "loss": 0.6965,
        "grad_norm": 3.6708619594573975,
        "learning_rate": 8.134430441572459e-05,
        "epoch": 0.3761333333333333,
        "step": 2821
    },
    {
        "loss": 2.2758,
        "grad_norm": 2.460519313812256,
        "learning_rate": 8.128247627522964e-05,
        "epoch": 0.3762666666666667,
        "step": 2822
    },
    {
        "loss": 1.8143,
        "grad_norm": 4.450709819793701,
        "learning_rate": 8.12206555488096e-05,
        "epoch": 0.3764,
        "step": 2823
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.988729238510132,
        "learning_rate": 8.115884226095191e-05,
        "epoch": 0.37653333333333333,
        "step": 2824
    },
    {
        "loss": 1.68,
        "grad_norm": 3.32574200630188,
        "learning_rate": 8.109703643614094e-05,
        "epoch": 0.37666666666666665,
        "step": 2825
    },
    {
        "loss": 2.7153,
        "grad_norm": 1.6457939147949219,
        "learning_rate": 8.10352380988583e-05,
        "epoch": 0.3768,
        "step": 2826
    },
    {
        "loss": 1.5619,
        "grad_norm": 5.461518287658691,
        "learning_rate": 8.097344727358247e-05,
        "epoch": 0.37693333333333334,
        "step": 2827
    },
    {
        "loss": 2.4207,
        "grad_norm": 2.892232894897461,
        "learning_rate": 8.091166398478897e-05,
        "epoch": 0.37706666666666666,
        "step": 2828
    },
    {
        "loss": 1.4944,
        "grad_norm": 3.212425708770752,
        "learning_rate": 8.084988825695044e-05,
        "epoch": 0.3772,
        "step": 2829
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.40217924118042,
        "learning_rate": 8.07881201145364e-05,
        "epoch": 0.37733333333333335,
        "step": 2830
    },
    {
        "loss": 1.5295,
        "grad_norm": 3.2634482383728027,
        "learning_rate": 8.072635958201347e-05,
        "epoch": 0.3774666666666667,
        "step": 2831
    },
    {
        "loss": 2.3585,
        "grad_norm": 2.897582530975342,
        "learning_rate": 8.06646066838452e-05,
        "epoch": 0.3776,
        "step": 2832
    },
    {
        "loss": 3.3004,
        "grad_norm": 2.8916373252868652,
        "learning_rate": 8.060286144449203e-05,
        "epoch": 0.3777333333333333,
        "step": 2833
    },
    {
        "loss": 2.618,
        "grad_norm": 2.4015512466430664,
        "learning_rate": 8.054112388841159e-05,
        "epoch": 0.3778666666666667,
        "step": 2834
    },
    {
        "loss": 2.821,
        "grad_norm": 1.8930721282958984,
        "learning_rate": 8.04793940400583e-05,
        "epoch": 0.378,
        "step": 2835
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.0901925563812256,
        "learning_rate": 8.041767192388348e-05,
        "epoch": 0.3781333333333333,
        "step": 2836
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.835273265838623,
        "learning_rate": 8.03559575643356e-05,
        "epoch": 0.37826666666666664,
        "step": 2837
    },
    {
        "loss": 2.809,
        "grad_norm": 2.8515818119049072,
        "learning_rate": 8.029425098585984e-05,
        "epoch": 0.3784,
        "step": 2838
    },
    {
        "loss": 2.5762,
        "grad_norm": 2.5399577617645264,
        "learning_rate": 8.02325522128984e-05,
        "epoch": 0.37853333333333333,
        "step": 2839
    },
    {
        "loss": 2.449,
        "grad_norm": 2.985029458999634,
        "learning_rate": 8.01708612698904e-05,
        "epoch": 0.37866666666666665,
        "step": 2840
    },
    {
        "loss": 0.632,
        "grad_norm": 2.026581048965454,
        "learning_rate": 8.010917818127182e-05,
        "epoch": 0.3788,
        "step": 2841
    },
    {
        "loss": 1.7654,
        "grad_norm": 3.1145641803741455,
        "learning_rate": 8.00475029714755e-05,
        "epoch": 0.37893333333333334,
        "step": 2842
    },
    {
        "loss": 2.3847,
        "grad_norm": 3.3547489643096924,
        "learning_rate": 7.998583566493127e-05,
        "epoch": 0.37906666666666666,
        "step": 2843
    },
    {
        "loss": 2.0149,
        "grad_norm": 3.1974947452545166,
        "learning_rate": 7.992417628606574e-05,
        "epoch": 0.3792,
        "step": 2844
    },
    {
        "loss": 2.785,
        "grad_norm": 1.792594313621521,
        "learning_rate": 7.986252485930237e-05,
        "epoch": 0.37933333333333336,
        "step": 2845
    },
    {
        "loss": 2.3009,
        "grad_norm": 2.94539213180542,
        "learning_rate": 7.980088140906153e-05,
        "epoch": 0.3794666666666667,
        "step": 2846
    },
    {
        "loss": 2.131,
        "grad_norm": 4.453068733215332,
        "learning_rate": 7.97392459597604e-05,
        "epoch": 0.3796,
        "step": 2847
    },
    {
        "loss": 2.5318,
        "grad_norm": 2.577122926712036,
        "learning_rate": 7.9677618535813e-05,
        "epoch": 0.3797333333333333,
        "step": 2848
    },
    {
        "loss": 2.3437,
        "grad_norm": 2.072415590286255,
        "learning_rate": 7.961599916163012e-05,
        "epoch": 0.3798666666666667,
        "step": 2849
    },
    {
        "loss": 1.9093,
        "grad_norm": 6.008394718170166,
        "learning_rate": 7.955438786161939e-05,
        "epoch": 0.38,
        "step": 2850
    },
    {
        "loss": 2.0179,
        "grad_norm": 3.686429023742676,
        "learning_rate": 7.949278466018535e-05,
        "epoch": 0.3801333333333333,
        "step": 2851
    },
    {
        "loss": 2.3454,
        "grad_norm": 4.036721706390381,
        "learning_rate": 7.943118958172917e-05,
        "epoch": 0.38026666666666664,
        "step": 2852
    },
    {
        "loss": 2.2565,
        "grad_norm": 3.0943140983581543,
        "learning_rate": 7.936960265064886e-05,
        "epoch": 0.3804,
        "step": 2853
    },
    {
        "loss": 3.1332,
        "grad_norm": 2.4233696460723877,
        "learning_rate": 7.930802389133926e-05,
        "epoch": 0.38053333333333333,
        "step": 2854
    },
    {
        "loss": 1.9961,
        "grad_norm": 2.7577261924743652,
        "learning_rate": 7.92464533281919e-05,
        "epoch": 0.38066666666666665,
        "step": 2855
    },
    {
        "loss": 4.1678,
        "grad_norm": 4.9908318519592285,
        "learning_rate": 7.91848909855951e-05,
        "epoch": 0.3808,
        "step": 2856
    },
    {
        "loss": 2.7887,
        "grad_norm": 2.3933067321777344,
        "learning_rate": 7.912333688793391e-05,
        "epoch": 0.38093333333333335,
        "step": 2857
    },
    {
        "loss": 1.9329,
        "grad_norm": 3.862739086151123,
        "learning_rate": 7.906179105959006e-05,
        "epoch": 0.38106666666666666,
        "step": 2858
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.0363481044769287,
        "learning_rate": 7.900025352494214e-05,
        "epoch": 0.3812,
        "step": 2859
    },
    {
        "loss": 2.5056,
        "grad_norm": 3.0211260318756104,
        "learning_rate": 7.893872430836537e-05,
        "epoch": 0.38133333333333336,
        "step": 2860
    },
    {
        "loss": 2.8069,
        "grad_norm": 2.505181312561035,
        "learning_rate": 7.887720343423167e-05,
        "epoch": 0.3814666666666667,
        "step": 2861
    },
    {
        "loss": 2.5797,
        "grad_norm": 2.542079448699951,
        "learning_rate": 7.881569092690965e-05,
        "epoch": 0.3816,
        "step": 2862
    },
    {
        "loss": 2.3409,
        "grad_norm": 2.5753908157348633,
        "learning_rate": 7.875418681076465e-05,
        "epoch": 0.3817333333333333,
        "step": 2863
    },
    {
        "loss": 2.8044,
        "grad_norm": 1.5361576080322266,
        "learning_rate": 7.869269111015867e-05,
        "epoch": 0.3818666666666667,
        "step": 2864
    },
    {
        "loss": 2.7602,
        "grad_norm": 2.808549404144287,
        "learning_rate": 7.863120384945031e-05,
        "epoch": 0.382,
        "step": 2865
    },
    {
        "loss": 1.6415,
        "grad_norm": 3.259671926498413,
        "learning_rate": 7.856972505299494e-05,
        "epoch": 0.3821333333333333,
        "step": 2866
    },
    {
        "loss": 2.8547,
        "grad_norm": 3.67022705078125,
        "learning_rate": 7.850825474514447e-05,
        "epoch": 0.38226666666666664,
        "step": 2867
    },
    {
        "loss": 2.0592,
        "grad_norm": 3.9035842418670654,
        "learning_rate": 7.844679295024755e-05,
        "epoch": 0.3824,
        "step": 2868
    },
    {
        "loss": 2.8647,
        "grad_norm": 1.9510525465011597,
        "learning_rate": 7.838533969264938e-05,
        "epoch": 0.38253333333333334,
        "step": 2869
    },
    {
        "loss": 2.4212,
        "grad_norm": 2.529388189315796,
        "learning_rate": 7.832389499669178e-05,
        "epoch": 0.38266666666666665,
        "step": 2870
    },
    {
        "loss": 1.2027,
        "grad_norm": 4.189245223999023,
        "learning_rate": 7.826245888671325e-05,
        "epoch": 0.3828,
        "step": 2871
    },
    {
        "loss": 1.8138,
        "grad_norm": 4.195537567138672,
        "learning_rate": 7.820103138704884e-05,
        "epoch": 0.38293333333333335,
        "step": 2872
    },
    {
        "loss": 2.4611,
        "grad_norm": 3.267190456390381,
        "learning_rate": 7.813961252203019e-05,
        "epoch": 0.38306666666666667,
        "step": 2873
    },
    {
        "loss": 2.2576,
        "grad_norm": 4.851100921630859,
        "learning_rate": 7.807820231598548e-05,
        "epoch": 0.3832,
        "step": 2874
    },
    {
        "loss": 2.5535,
        "grad_norm": 2.745582342147827,
        "learning_rate": 7.801680079323952e-05,
        "epoch": 0.38333333333333336,
        "step": 2875
    },
    {
        "loss": 1.9564,
        "grad_norm": 3.319542407989502,
        "learning_rate": 7.795540797811368e-05,
        "epoch": 0.3834666666666667,
        "step": 2876
    },
    {
        "loss": 2.2826,
        "grad_norm": 3.3726017475128174,
        "learning_rate": 7.789402389492586e-05,
        "epoch": 0.3836,
        "step": 2877
    },
    {
        "loss": 2.3009,
        "grad_norm": 3.124782085418701,
        "learning_rate": 7.783264856799049e-05,
        "epoch": 0.3837333333333333,
        "step": 2878
    },
    {
        "loss": 2.3215,
        "grad_norm": 3.4677560329437256,
        "learning_rate": 7.777128202161851e-05,
        "epoch": 0.3838666666666667,
        "step": 2879
    },
    {
        "loss": 2.4088,
        "grad_norm": 2.777064800262451,
        "learning_rate": 7.770992428011754e-05,
        "epoch": 0.384,
        "step": 2880
    },
    {
        "loss": 2.7831,
        "grad_norm": 2.7846109867095947,
        "learning_rate": 7.76485753677915e-05,
        "epoch": 0.3841333333333333,
        "step": 2881
    },
    {
        "loss": 1.9067,
        "grad_norm": 3.281991958618164,
        "learning_rate": 7.758723530894093e-05,
        "epoch": 0.38426666666666665,
        "step": 2882
    },
    {
        "loss": 1.7789,
        "grad_norm": 3.7053732872009277,
        "learning_rate": 7.752590412786279e-05,
        "epoch": 0.3844,
        "step": 2883
    },
    {
        "loss": 2.8238,
        "grad_norm": 2.082227945327759,
        "learning_rate": 7.74645818488506e-05,
        "epoch": 0.38453333333333334,
        "step": 2884
    },
    {
        "loss": 2.6027,
        "grad_norm": 3.2998955249786377,
        "learning_rate": 7.740326849619434e-05,
        "epoch": 0.38466666666666666,
        "step": 2885
    },
    {
        "loss": 2.6269,
        "grad_norm": 3.3386447429656982,
        "learning_rate": 7.734196409418046e-05,
        "epoch": 0.3848,
        "step": 2886
    },
    {
        "loss": 2.3317,
        "grad_norm": 3.4018566608428955,
        "learning_rate": 7.728066866709174e-05,
        "epoch": 0.38493333333333335,
        "step": 2887
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.764784574508667,
        "learning_rate": 7.72193822392076e-05,
        "epoch": 0.38506666666666667,
        "step": 2888
    },
    {
        "loss": 2.2707,
        "grad_norm": 2.647629499435425,
        "learning_rate": 7.715810483480381e-05,
        "epoch": 0.3852,
        "step": 2889
    },
    {
        "loss": 1.8135,
        "grad_norm": 4.391317844390869,
        "learning_rate": 7.709683647815251e-05,
        "epoch": 0.38533333333333336,
        "step": 2890
    },
    {
        "loss": 0.8801,
        "grad_norm": 2.7835946083068848,
        "learning_rate": 7.703557719352232e-05,
        "epoch": 0.3854666666666667,
        "step": 2891
    },
    {
        "loss": 2.6349,
        "grad_norm": 2.7419071197509766,
        "learning_rate": 7.697432700517824e-05,
        "epoch": 0.3856,
        "step": 2892
    },
    {
        "loss": 2.1912,
        "grad_norm": 3.178065776824951,
        "learning_rate": 7.691308593738171e-05,
        "epoch": 0.3857333333333333,
        "step": 2893
    },
    {
        "loss": 1.4209,
        "grad_norm": 4.133828163146973,
        "learning_rate": 7.68518540143905e-05,
        "epoch": 0.3858666666666667,
        "step": 2894
    },
    {
        "loss": 2.3826,
        "grad_norm": 3.317924737930298,
        "learning_rate": 7.679063126045877e-05,
        "epoch": 0.386,
        "step": 2895
    },
    {
        "loss": 1.8965,
        "grad_norm": 3.753662109375,
        "learning_rate": 7.67294176998371e-05,
        "epoch": 0.38613333333333333,
        "step": 2896
    },
    {
        "loss": 2.8805,
        "grad_norm": 2.128908395767212,
        "learning_rate": 7.666821335677237e-05,
        "epoch": 0.38626666666666665,
        "step": 2897
    },
    {
        "loss": 1.842,
        "grad_norm": 3.722294330596924,
        "learning_rate": 7.660701825550787e-05,
        "epoch": 0.3864,
        "step": 2898
    },
    {
        "loss": 1.7131,
        "grad_norm": 4.7173590660095215,
        "learning_rate": 7.654583242028311e-05,
        "epoch": 0.38653333333333334,
        "step": 2899
    },
    {
        "loss": 2.3374,
        "grad_norm": 1.9337561130523682,
        "learning_rate": 7.648465587533403e-05,
        "epoch": 0.38666666666666666,
        "step": 2900
    },
    {
        "loss": 2.1808,
        "grad_norm": 3.939528226852417,
        "learning_rate": 7.642348864489286e-05,
        "epoch": 0.3868,
        "step": 2901
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.1525423526763916,
        "learning_rate": 7.636233075318824e-05,
        "epoch": 0.38693333333333335,
        "step": 2902
    },
    {
        "loss": 1.6072,
        "grad_norm": 2.7851808071136475,
        "learning_rate": 7.630118222444494e-05,
        "epoch": 0.38706666666666667,
        "step": 2903
    },
    {
        "loss": 2.5674,
        "grad_norm": 3.4062435626983643,
        "learning_rate": 7.624004308288405e-05,
        "epoch": 0.3872,
        "step": 2904
    },
    {
        "loss": 2.3477,
        "grad_norm": 2.796217441558838,
        "learning_rate": 7.617891335272314e-05,
        "epoch": 0.3873333333333333,
        "step": 2905
    },
    {
        "loss": 1.9472,
        "grad_norm": 3.0034987926483154,
        "learning_rate": 7.611779305817577e-05,
        "epoch": 0.3874666666666667,
        "step": 2906
    },
    {
        "loss": 1.7251,
        "grad_norm": 3.51029372215271,
        "learning_rate": 7.605668222345197e-05,
        "epoch": 0.3876,
        "step": 2907
    },
    {
        "loss": 2.6411,
        "grad_norm": 3.874943971633911,
        "learning_rate": 7.599558087275793e-05,
        "epoch": 0.3877333333333333,
        "step": 2908
    },
    {
        "loss": 2.8693,
        "grad_norm": 2.5688562393188477,
        "learning_rate": 7.593448903029606e-05,
        "epoch": 0.3878666666666667,
        "step": 2909
    },
    {
        "loss": 2.0341,
        "grad_norm": 2.0406525135040283,
        "learning_rate": 7.587340672026512e-05,
        "epoch": 0.388,
        "step": 2910
    },
    {
        "loss": 2.6731,
        "grad_norm": 2.868860960006714,
        "learning_rate": 7.581233396686001e-05,
        "epoch": 0.38813333333333333,
        "step": 2911
    },
    {
        "loss": 2.8299,
        "grad_norm": 2.195979356765747,
        "learning_rate": 7.575127079427176e-05,
        "epoch": 0.38826666666666665,
        "step": 2912
    },
    {
        "loss": 1.9719,
        "grad_norm": 3.601736545562744,
        "learning_rate": 7.569021722668783e-05,
        "epoch": 0.3884,
        "step": 2913
    },
    {
        "loss": 2.3444,
        "grad_norm": 2.5457420349121094,
        "learning_rate": 7.562917328829169e-05,
        "epoch": 0.38853333333333334,
        "step": 2914
    },
    {
        "loss": 1.6768,
        "grad_norm": 4.037677764892578,
        "learning_rate": 7.556813900326303e-05,
        "epoch": 0.38866666666666666,
        "step": 2915
    },
    {
        "loss": 2.1192,
        "grad_norm": 6.539840221405029,
        "learning_rate": 7.550711439577778e-05,
        "epoch": 0.3888,
        "step": 2916
    },
    {
        "loss": 2.4117,
        "grad_norm": 2.9629740715026855,
        "learning_rate": 7.544609949000794e-05,
        "epoch": 0.38893333333333335,
        "step": 2917
    },
    {
        "loss": 2.8196,
        "grad_norm": 2.5341312885284424,
        "learning_rate": 7.538509431012179e-05,
        "epoch": 0.38906666666666667,
        "step": 2918
    },
    {
        "loss": 3.1786,
        "grad_norm": 1.7278978824615479,
        "learning_rate": 7.532409888028364e-05,
        "epoch": 0.3892,
        "step": 2919
    },
    {
        "loss": 2.204,
        "grad_norm": 3.42583966255188,
        "learning_rate": 7.5263113224654e-05,
        "epoch": 0.3893333333333333,
        "step": 2920
    },
    {
        "loss": 1.9593,
        "grad_norm": 4.100193023681641,
        "learning_rate": 7.52021373673895e-05,
        "epoch": 0.3894666666666667,
        "step": 2921
    },
    {
        "loss": 1.2027,
        "grad_norm": 4.104150295257568,
        "learning_rate": 7.51411713326429e-05,
        "epoch": 0.3896,
        "step": 2922
    },
    {
        "loss": 2.1883,
        "grad_norm": 3.992666482925415,
        "learning_rate": 7.508021514456305e-05,
        "epoch": 0.3897333333333333,
        "step": 2923
    },
    {
        "loss": 1.5097,
        "grad_norm": 4.019627571105957,
        "learning_rate": 7.501926882729489e-05,
        "epoch": 0.38986666666666664,
        "step": 2924
    },
    {
        "loss": 1.914,
        "grad_norm": 2.0551598072052,
        "learning_rate": 7.495833240497947e-05,
        "epoch": 0.39,
        "step": 2925
    },
    {
        "loss": 1.4293,
        "grad_norm": 3.535029411315918,
        "learning_rate": 7.489740590175387e-05,
        "epoch": 0.39013333333333333,
        "step": 2926
    },
    {
        "loss": 2.0459,
        "grad_norm": 3.2540600299835205,
        "learning_rate": 7.483648934175138e-05,
        "epoch": 0.39026666666666665,
        "step": 2927
    },
    {
        "loss": 1.5788,
        "grad_norm": 2.8259809017181396,
        "learning_rate": 7.477558274910119e-05,
        "epoch": 0.3904,
        "step": 2928
    },
    {
        "loss": 2.654,
        "grad_norm": 3.83451247215271,
        "learning_rate": 7.47146861479286e-05,
        "epoch": 0.39053333333333334,
        "step": 2929
    },
    {
        "loss": 2.8106,
        "grad_norm": 2.3525643348693848,
        "learning_rate": 7.4653799562355e-05,
        "epoch": 0.39066666666666666,
        "step": 2930
    },
    {
        "loss": 2.7934,
        "grad_norm": 4.126173973083496,
        "learning_rate": 7.459292301649777e-05,
        "epoch": 0.3908,
        "step": 2931
    },
    {
        "loss": 2.5323,
        "grad_norm": 1.7112942934036255,
        "learning_rate": 7.453205653447029e-05,
        "epoch": 0.39093333333333335,
        "step": 2932
    },
    {
        "loss": 3.0276,
        "grad_norm": 2.068842887878418,
        "learning_rate": 7.447120014038202e-05,
        "epoch": 0.3910666666666667,
        "step": 2933
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.9685285091400146,
        "learning_rate": 7.44103538583383e-05,
        "epoch": 0.3912,
        "step": 2934
    },
    {
        "loss": 2.072,
        "grad_norm": 4.028964519500732,
        "learning_rate": 7.434951771244067e-05,
        "epoch": 0.3913333333333333,
        "step": 2935
    },
    {
        "loss": 2.2998,
        "grad_norm": 2.424694776535034,
        "learning_rate": 7.428869172678648e-05,
        "epoch": 0.3914666666666667,
        "step": 2936
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.2079427242279053,
        "learning_rate": 7.422787592546904e-05,
        "epoch": 0.3916,
        "step": 2937
    },
    {
        "loss": 2.5981,
        "grad_norm": 2.944831371307373,
        "learning_rate": 7.416707033257784e-05,
        "epoch": 0.3917333333333333,
        "step": 2938
    },
    {
        "loss": 2.3911,
        "grad_norm": 3.2524662017822266,
        "learning_rate": 7.410627497219807e-05,
        "epoch": 0.39186666666666664,
        "step": 2939
    },
    {
        "loss": 2.8109,
        "grad_norm": 4.6935954093933105,
        "learning_rate": 7.4045489868411e-05,
        "epoch": 0.392,
        "step": 2940
    },
    {
        "loss": 2.4597,
        "grad_norm": 2.1672303676605225,
        "learning_rate": 7.398471504529385e-05,
        "epoch": 0.39213333333333333,
        "step": 2941
    },
    {
        "loss": 2.8901,
        "grad_norm": 2.5235660076141357,
        "learning_rate": 7.392395052691971e-05,
        "epoch": 0.39226666666666665,
        "step": 2942
    },
    {
        "loss": 2.6397,
        "grad_norm": 2.3031604290008545,
        "learning_rate": 7.386319633735761e-05,
        "epoch": 0.3924,
        "step": 2943
    },
    {
        "loss": 0.9851,
        "grad_norm": 3.5831549167633057,
        "learning_rate": 7.380245250067252e-05,
        "epoch": 0.39253333333333335,
        "step": 2944
    },
    {
        "loss": 2.1651,
        "grad_norm": 3.8747496604919434,
        "learning_rate": 7.374171904092526e-05,
        "epoch": 0.39266666666666666,
        "step": 2945
    },
    {
        "loss": 0.9787,
        "grad_norm": 2.520432233810425,
        "learning_rate": 7.368099598217254e-05,
        "epoch": 0.3928,
        "step": 2946
    },
    {
        "loss": 3.0253,
        "grad_norm": 3.0054879188537598,
        "learning_rate": 7.362028334846701e-05,
        "epoch": 0.39293333333333336,
        "step": 2947
    },
    {
        "loss": 2.8866,
        "grad_norm": 1.9932880401611328,
        "learning_rate": 7.355958116385712e-05,
        "epoch": 0.3930666666666667,
        "step": 2948
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.22003173828125,
        "learning_rate": 7.349888945238725e-05,
        "epoch": 0.3932,
        "step": 2949
    },
    {
        "loss": 2.018,
        "grad_norm": 4.601108074188232,
        "learning_rate": 7.343820823809755e-05,
        "epoch": 0.3933333333333333,
        "step": 2950
    },
    {
        "loss": 1.6126,
        "grad_norm": 2.132706642150879,
        "learning_rate": 7.337753754502404e-05,
        "epoch": 0.3934666666666667,
        "step": 2951
    },
    {
        "loss": 2.5451,
        "grad_norm": 2.9068593978881836,
        "learning_rate": 7.331687739719868e-05,
        "epoch": 0.3936,
        "step": 2952
    },
    {
        "loss": 2.3961,
        "grad_norm": 2.7927193641662598,
        "learning_rate": 7.325622781864908e-05,
        "epoch": 0.3937333333333333,
        "step": 2953
    },
    {
        "loss": 1.3816,
        "grad_norm": 2.2625510692596436,
        "learning_rate": 7.319558883339875e-05,
        "epoch": 0.39386666666666664,
        "step": 2954
    },
    {
        "loss": 2.683,
        "grad_norm": 2.3814380168914795,
        "learning_rate": 7.313496046546704e-05,
        "epoch": 0.394,
        "step": 2955
    },
    {
        "loss": 1.4359,
        "grad_norm": 3.913609504699707,
        "learning_rate": 7.307434273886902e-05,
        "epoch": 0.39413333333333334,
        "step": 2956
    },
    {
        "loss": 2.4026,
        "grad_norm": 2.206122636795044,
        "learning_rate": 7.30137356776156e-05,
        "epoch": 0.39426666666666665,
        "step": 2957
    },
    {
        "loss": 2.4846,
        "grad_norm": 3.0012729167938232,
        "learning_rate": 7.295313930571345e-05,
        "epoch": 0.3944,
        "step": 2958
    },
    {
        "loss": 2.319,
        "grad_norm": 3.268893241882324,
        "learning_rate": 7.289255364716492e-05,
        "epoch": 0.39453333333333335,
        "step": 2959
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.9696152210235596,
        "learning_rate": 7.283197872596828e-05,
        "epoch": 0.39466666666666667,
        "step": 2960
    },
    {
        "loss": 2.4891,
        "grad_norm": 2.716264486312866,
        "learning_rate": 7.277141456611745e-05,
        "epoch": 0.3948,
        "step": 2961
    },
    {
        "loss": 2.5034,
        "grad_norm": 3.3716089725494385,
        "learning_rate": 7.271086119160208e-05,
        "epoch": 0.39493333333333336,
        "step": 2962
    },
    {
        "loss": 2.6887,
        "grad_norm": 2.8371341228485107,
        "learning_rate": 7.265031862640757e-05,
        "epoch": 0.3950666666666667,
        "step": 2963
    },
    {
        "loss": 1.8472,
        "grad_norm": 3.4745237827301025,
        "learning_rate": 7.258978689451508e-05,
        "epoch": 0.3952,
        "step": 2964
    },
    {
        "loss": 1.8182,
        "grad_norm": 2.843885660171509,
        "learning_rate": 7.25292660199014e-05,
        "epoch": 0.3953333333333333,
        "step": 2965
    },
    {
        "loss": 2.8547,
        "grad_norm": 3.27022123336792,
        "learning_rate": 7.246875602653905e-05,
        "epoch": 0.3954666666666667,
        "step": 2966
    },
    {
        "loss": 2.6197,
        "grad_norm": 3.8315606117248535,
        "learning_rate": 7.240825693839621e-05,
        "epoch": 0.3956,
        "step": 2967
    },
    {
        "loss": 3.1725,
        "grad_norm": 3.677983045578003,
        "learning_rate": 7.234776877943683e-05,
        "epoch": 0.3957333333333333,
        "step": 2968
    },
    {
        "loss": 1.9227,
        "grad_norm": 3.7286581993103027,
        "learning_rate": 7.228729157362047e-05,
        "epoch": 0.39586666666666664,
        "step": 2969
    },
    {
        "loss": 1.306,
        "grad_norm": 3.115370750427246,
        "learning_rate": 7.222682534490235e-05,
        "epoch": 0.396,
        "step": 2970
    },
    {
        "loss": 2.1532,
        "grad_norm": 3.1863913536071777,
        "learning_rate": 7.21663701172333e-05,
        "epoch": 0.39613333333333334,
        "step": 2971
    },
    {
        "loss": 1.7649,
        "grad_norm": 3.206587791442871,
        "learning_rate": 7.210592591455994e-05,
        "epoch": 0.39626666666666666,
        "step": 2972
    },
    {
        "loss": 3.051,
        "grad_norm": 3.5070066452026367,
        "learning_rate": 7.204549276082435e-05,
        "epoch": 0.3964,
        "step": 2973
    },
    {
        "loss": 2.1547,
        "grad_norm": 2.3209316730499268,
        "learning_rate": 7.198507067996436e-05,
        "epoch": 0.39653333333333335,
        "step": 2974
    },
    {
        "loss": 2.316,
        "grad_norm": 2.955763816833496,
        "learning_rate": 7.192465969591332e-05,
        "epoch": 0.39666666666666667,
        "step": 2975
    },
    {
        "loss": 1.952,
        "grad_norm": 6.563129901885986,
        "learning_rate": 7.186425983260021e-05,
        "epoch": 0.3968,
        "step": 2976
    },
    {
        "loss": 2.658,
        "grad_norm": 3.9325356483459473,
        "learning_rate": 7.180387111394967e-05,
        "epoch": 0.39693333333333336,
        "step": 2977
    },
    {
        "loss": 3.317,
        "grad_norm": 4.535315990447998,
        "learning_rate": 7.174349356388186e-05,
        "epoch": 0.3970666666666667,
        "step": 2978
    },
    {
        "loss": 2.8347,
        "grad_norm": 1.816529393196106,
        "learning_rate": 7.168312720631251e-05,
        "epoch": 0.3972,
        "step": 2979
    },
    {
        "loss": 2.5464,
        "grad_norm": 4.073720932006836,
        "learning_rate": 7.162277206515293e-05,
        "epoch": 0.3973333333333333,
        "step": 2980
    },
    {
        "loss": 1.914,
        "grad_norm": 2.3506901264190674,
        "learning_rate": 7.156242816431011e-05,
        "epoch": 0.3974666666666667,
        "step": 2981
    },
    {
        "loss": 2.4989,
        "grad_norm": 2.8748416900634766,
        "learning_rate": 7.150209552768637e-05,
        "epoch": 0.3976,
        "step": 2982
    },
    {
        "loss": 2.0336,
        "grad_norm": 3.6945371627807617,
        "learning_rate": 7.144177417917968e-05,
        "epoch": 0.3977333333333333,
        "step": 2983
    },
    {
        "loss": 2.5499,
        "grad_norm": 2.056619644165039,
        "learning_rate": 7.138146414268356e-05,
        "epoch": 0.39786666666666665,
        "step": 2984
    },
    {
        "loss": 2.1013,
        "grad_norm": 4.263545036315918,
        "learning_rate": 7.132116544208698e-05,
        "epoch": 0.398,
        "step": 2985
    },
    {
        "loss": 2.381,
        "grad_norm": 3.9742462635040283,
        "learning_rate": 7.12608781012745e-05,
        "epoch": 0.39813333333333334,
        "step": 2986
    },
    {
        "loss": 2.5887,
        "grad_norm": 3.7310683727264404,
        "learning_rate": 7.120060214412617e-05,
        "epoch": 0.39826666666666666,
        "step": 2987
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.018934488296509,
        "learning_rate": 7.114033759451739e-05,
        "epoch": 0.3984,
        "step": 2988
    },
    {
        "loss": 2.6845,
        "grad_norm": 2.714980125427246,
        "learning_rate": 7.108008447631928e-05,
        "epoch": 0.39853333333333335,
        "step": 2989
    },
    {
        "loss": 2.7842,
        "grad_norm": 2.754990577697754,
        "learning_rate": 7.101984281339826e-05,
        "epoch": 0.39866666666666667,
        "step": 2990
    },
    {
        "loss": 1.8816,
        "grad_norm": 3.0847389698028564,
        "learning_rate": 7.095961262961624e-05,
        "epoch": 0.3988,
        "step": 2991
    },
    {
        "loss": 3.5144,
        "grad_norm": 4.36604118347168,
        "learning_rate": 7.089939394883059e-05,
        "epoch": 0.3989333333333333,
        "step": 2992
    },
    {
        "loss": 2.3218,
        "grad_norm": 3.164151430130005,
        "learning_rate": 7.083918679489415e-05,
        "epoch": 0.3990666666666667,
        "step": 2993
    },
    {
        "loss": 2.3898,
        "grad_norm": 2.9805891513824463,
        "learning_rate": 7.077899119165518e-05,
        "epoch": 0.3992,
        "step": 2994
    },
    {
        "loss": 0.7698,
        "grad_norm": 3.060776472091675,
        "learning_rate": 7.07188071629574e-05,
        "epoch": 0.3993333333333333,
        "step": 2995
    },
    {
        "loss": 2.1074,
        "grad_norm": 3.9403204917907715,
        "learning_rate": 7.065863473263983e-05,
        "epoch": 0.3994666666666667,
        "step": 2996
    },
    {
        "loss": 2.5581,
        "grad_norm": 1.745590090751648,
        "learning_rate": 7.059847392453707e-05,
        "epoch": 0.3996,
        "step": 2997
    },
    {
        "loss": 2.3605,
        "grad_norm": 3.5338079929351807,
        "learning_rate": 7.053832476247895e-05,
        "epoch": 0.39973333333333333,
        "step": 2998
    },
    {
        "loss": 1.6399,
        "grad_norm": 3.7475016117095947,
        "learning_rate": 7.047818727029081e-05,
        "epoch": 0.39986666666666665,
        "step": 2999
    },
    {
        "loss": 1.2564,
        "grad_norm": 3.4042298793792725,
        "learning_rate": 7.041806147179322e-05,
        "epoch": 0.4,
        "step": 3000
    },
    {
        "loss": 2.6007,
        "grad_norm": 2.961754322052002,
        "learning_rate": 7.035794739080233e-05,
        "epoch": 0.40013333333333334,
        "step": 3001
    },
    {
        "loss": 2.636,
        "grad_norm": 2.240752696990967,
        "learning_rate": 7.029784505112944e-05,
        "epoch": 0.40026666666666666,
        "step": 3002
    },
    {
        "loss": 2.585,
        "grad_norm": 2.485581398010254,
        "learning_rate": 7.023775447658137e-05,
        "epoch": 0.4004,
        "step": 3003
    },
    {
        "loss": 2.1164,
        "grad_norm": 3.507237672805786,
        "learning_rate": 7.017767569096016e-05,
        "epoch": 0.40053333333333335,
        "step": 3004
    },
    {
        "loss": 1.1834,
        "grad_norm": 4.108234882354736,
        "learning_rate": 7.011760871806319e-05,
        "epoch": 0.40066666666666667,
        "step": 3005
    },
    {
        "loss": 2.6755,
        "grad_norm": 3.871633291244507,
        "learning_rate": 7.005755358168327e-05,
        "epoch": 0.4008,
        "step": 3006
    },
    {
        "loss": 2.9106,
        "grad_norm": 2.5272059440612793,
        "learning_rate": 6.99975103056084e-05,
        "epoch": 0.4009333333333333,
        "step": 3007
    },
    {
        "loss": 2.7692,
        "grad_norm": 3.317521333694458,
        "learning_rate": 6.993747891362193e-05,
        "epoch": 0.4010666666666667,
        "step": 3008
    },
    {
        "loss": 2.0922,
        "grad_norm": 3.5603432655334473,
        "learning_rate": 6.987745942950248e-05,
        "epoch": 0.4012,
        "step": 3009
    },
    {
        "loss": 1.622,
        "grad_norm": 5.24045991897583,
        "learning_rate": 6.981745187702397e-05,
        "epoch": 0.4013333333333333,
        "step": 3010
    },
    {
        "loss": 2.7049,
        "grad_norm": 2.8811228275299072,
        "learning_rate": 6.975745627995566e-05,
        "epoch": 0.4014666666666667,
        "step": 3011
    },
    {
        "loss": 3.2245,
        "grad_norm": 2.281284809112549,
        "learning_rate": 6.969747266206196e-05,
        "epoch": 0.4016,
        "step": 3012
    },
    {
        "loss": 2.1279,
        "grad_norm": 2.5493083000183105,
        "learning_rate": 6.963750104710255e-05,
        "epoch": 0.40173333333333333,
        "step": 3013
    },
    {
        "loss": 2.4857,
        "grad_norm": 2.5706636905670166,
        "learning_rate": 6.95775414588325e-05,
        "epoch": 0.40186666666666665,
        "step": 3014
    },
    {
        "loss": 2.1477,
        "grad_norm": 2.113128423690796,
        "learning_rate": 6.95175939210019e-05,
        "epoch": 0.402,
        "step": 3015
    },
    {
        "loss": 2.4838,
        "grad_norm": 2.657625198364258,
        "learning_rate": 6.945765845735624e-05,
        "epoch": 0.40213333333333334,
        "step": 3016
    },
    {
        "loss": 1.0869,
        "grad_norm": 3.0839695930480957,
        "learning_rate": 6.939773509163611e-05,
        "epoch": 0.40226666666666666,
        "step": 3017
    },
    {
        "loss": 1.9028,
        "grad_norm": 5.62191915512085,
        "learning_rate": 6.933782384757736e-05,
        "epoch": 0.4024,
        "step": 3018
    },
    {
        "loss": 2.5533,
        "grad_norm": 3.1241183280944824,
        "learning_rate": 6.927792474891111e-05,
        "epoch": 0.40253333333333335,
        "step": 3019
    },
    {
        "loss": 2.8982,
        "grad_norm": 2.274998188018799,
        "learning_rate": 6.921803781936351e-05,
        "epoch": 0.4026666666666667,
        "step": 3020
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.9428348541259766,
        "learning_rate": 6.915816308265604e-05,
        "epoch": 0.4028,
        "step": 3021
    },
    {
        "loss": 2.5269,
        "grad_norm": 2.970729112625122,
        "learning_rate": 6.909830056250524e-05,
        "epoch": 0.4029333333333333,
        "step": 3022
    },
    {
        "loss": 2.8652,
        "grad_norm": 3.0381784439086914,
        "learning_rate": 6.903845028262291e-05,
        "epoch": 0.4030666666666667,
        "step": 3023
    },
    {
        "loss": 2.9202,
        "grad_norm": 3.1501829624176025,
        "learning_rate": 6.897861226671593e-05,
        "epoch": 0.4032,
        "step": 3024
    },
    {
        "loss": 1.867,
        "grad_norm": 5.137748718261719,
        "learning_rate": 6.891878653848631e-05,
        "epoch": 0.4033333333333333,
        "step": 3025
    },
    {
        "loss": 1.8919,
        "grad_norm": 4.138191223144531,
        "learning_rate": 6.88589731216313e-05,
        "epoch": 0.40346666666666664,
        "step": 3026
    },
    {
        "loss": 1.8466,
        "grad_norm": 4.741801738739014,
        "learning_rate": 6.879917203984307e-05,
        "epoch": 0.4036,
        "step": 3027
    },
    {
        "loss": 2.9548,
        "grad_norm": 5.039188861846924,
        "learning_rate": 6.873938331680915e-05,
        "epoch": 0.40373333333333333,
        "step": 3028
    },
    {
        "loss": 1.9204,
        "grad_norm": 4.118569850921631,
        "learning_rate": 6.867960697621203e-05,
        "epoch": 0.40386666666666665,
        "step": 3029
    },
    {
        "loss": 1.6932,
        "grad_norm": 4.153598308563232,
        "learning_rate": 6.861984304172927e-05,
        "epoch": 0.404,
        "step": 3030
    },
    {
        "loss": 1.9593,
        "grad_norm": 3.0808041095733643,
        "learning_rate": 6.856009153703363e-05,
        "epoch": 0.40413333333333334,
        "step": 3031
    },
    {
        "loss": 2.5275,
        "grad_norm": 3.9533371925354004,
        "learning_rate": 6.850035248579286e-05,
        "epoch": 0.40426666666666666,
        "step": 3032
    },
    {
        "loss": 2.633,
        "grad_norm": 2.730959177017212,
        "learning_rate": 6.844062591166981e-05,
        "epoch": 0.4044,
        "step": 3033
    },
    {
        "loss": 1.1332,
        "grad_norm": 2.459298849105835,
        "learning_rate": 6.838091183832238e-05,
        "epoch": 0.40453333333333336,
        "step": 3034
    },
    {
        "loss": 1.154,
        "grad_norm": 4.0207109451293945,
        "learning_rate": 6.832121028940346e-05,
        "epoch": 0.4046666666666667,
        "step": 3035
    },
    {
        "loss": 2.0296,
        "grad_norm": 4.504434108734131,
        "learning_rate": 6.826152128856112e-05,
        "epoch": 0.4048,
        "step": 3036
    },
    {
        "loss": 2.3727,
        "grad_norm": 4.021528720855713,
        "learning_rate": 6.820184485943837e-05,
        "epoch": 0.4049333333333333,
        "step": 3037
    },
    {
        "loss": 2.6741,
        "grad_norm": 3.4344449043273926,
        "learning_rate": 6.814218102567316e-05,
        "epoch": 0.4050666666666667,
        "step": 3038
    },
    {
        "loss": 2.649,
        "grad_norm": 5.1853156089782715,
        "learning_rate": 6.808252981089866e-05,
        "epoch": 0.4052,
        "step": 3039
    },
    {
        "loss": 2.2567,
        "grad_norm": 3.0894675254821777,
        "learning_rate": 6.802289123874283e-05,
        "epoch": 0.4053333333333333,
        "step": 3040
    },
    {
        "loss": 2.278,
        "grad_norm": 3.7934629917144775,
        "learning_rate": 6.796326533282873e-05,
        "epoch": 0.40546666666666664,
        "step": 3041
    },
    {
        "loss": 2.3913,
        "grad_norm": 1.4648823738098145,
        "learning_rate": 6.79036521167744e-05,
        "epoch": 0.4056,
        "step": 3042
    },
    {
        "loss": 3.0483,
        "grad_norm": 3.037475824356079,
        "learning_rate": 6.784405161419286e-05,
        "epoch": 0.40573333333333333,
        "step": 3043
    },
    {
        "loss": 2.3222,
        "grad_norm": 3.41487455368042,
        "learning_rate": 6.778446384869199e-05,
        "epoch": 0.40586666666666665,
        "step": 3044
    },
    {
        "loss": 1.0483,
        "grad_norm": 5.137465000152588,
        "learning_rate": 6.77248888438748e-05,
        "epoch": 0.406,
        "step": 3045
    },
    {
        "loss": 2.695,
        "grad_norm": 2.4574081897735596,
        "learning_rate": 6.76653266233391e-05,
        "epoch": 0.40613333333333335,
        "step": 3046
    },
    {
        "loss": 1.864,
        "grad_norm": 4.6303791999816895,
        "learning_rate": 6.76057772106777e-05,
        "epoch": 0.40626666666666666,
        "step": 3047
    },
    {
        "loss": 3.2308,
        "grad_norm": 3.3297278881073,
        "learning_rate": 6.754624062947835e-05,
        "epoch": 0.4064,
        "step": 3048
    },
    {
        "loss": 2.6316,
        "grad_norm": 2.562330722808838,
        "learning_rate": 6.748671690332365e-05,
        "epoch": 0.40653333333333336,
        "step": 3049
    },
    {
        "loss": 2.5738,
        "grad_norm": 2.870922327041626,
        "learning_rate": 6.742720605579121e-05,
        "epoch": 0.4066666666666667,
        "step": 3050
    },
    {
        "loss": 1.3193,
        "grad_norm": 4.978755950927734,
        "learning_rate": 6.736770811045339e-05,
        "epoch": 0.4068,
        "step": 3051
    },
    {
        "loss": 1.8122,
        "grad_norm": 2.435243606567383,
        "learning_rate": 6.730822309087756e-05,
        "epoch": 0.4069333333333333,
        "step": 3052
    },
    {
        "loss": 2.9364,
        "grad_norm": 3.1605892181396484,
        "learning_rate": 6.724875102062602e-05,
        "epoch": 0.4070666666666667,
        "step": 3053
    },
    {
        "loss": 1.5847,
        "grad_norm": 3.7647414207458496,
        "learning_rate": 6.718929192325579e-05,
        "epoch": 0.4072,
        "step": 3054
    },
    {
        "loss": 3.0443,
        "grad_norm": 1.8149133920669556,
        "learning_rate": 6.712984582231879e-05,
        "epoch": 0.4073333333333333,
        "step": 3055
    },
    {
        "loss": 1.8118,
        "grad_norm": 3.107717752456665,
        "learning_rate": 6.707041274136189e-05,
        "epoch": 0.40746666666666664,
        "step": 3056
    },
    {
        "loss": 2.3416,
        "grad_norm": 2.9571292400360107,
        "learning_rate": 6.70109927039267e-05,
        "epoch": 0.4076,
        "step": 3057
    },
    {
        "loss": 1.9339,
        "grad_norm": 3.5377988815307617,
        "learning_rate": 6.695158573354972e-05,
        "epoch": 0.40773333333333334,
        "step": 3058
    },
    {
        "loss": 2.0198,
        "grad_norm": 2.6303272247314453,
        "learning_rate": 6.689219185376226e-05,
        "epoch": 0.40786666666666666,
        "step": 3059
    },
    {
        "loss": 2.3548,
        "grad_norm": 2.1542513370513916,
        "learning_rate": 6.683281108809034e-05,
        "epoch": 0.408,
        "step": 3060
    },
    {
        "loss": 1.493,
        "grad_norm": 4.3164472579956055,
        "learning_rate": 6.677344346005498e-05,
        "epoch": 0.40813333333333335,
        "step": 3061
    },
    {
        "loss": 2.4067,
        "grad_norm": 3.047297954559326,
        "learning_rate": 6.671408899317193e-05,
        "epoch": 0.40826666666666667,
        "step": 3062
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.8252344131469727,
        "learning_rate": 6.665474771095164e-05,
        "epoch": 0.4084,
        "step": 3063
    },
    {
        "loss": 2.6197,
        "grad_norm": 2.9639463424682617,
        "learning_rate": 6.659541963689938e-05,
        "epoch": 0.40853333333333336,
        "step": 3064
    },
    {
        "loss": 2.7424,
        "grad_norm": 4.418647766113281,
        "learning_rate": 6.653610479451524e-05,
        "epoch": 0.4086666666666667,
        "step": 3065
    },
    {
        "loss": 2.5563,
        "grad_norm": 2.0229697227478027,
        "learning_rate": 6.647680320729403e-05,
        "epoch": 0.4088,
        "step": 3066
    },
    {
        "loss": 2.1824,
        "grad_norm": 3.4416191577911377,
        "learning_rate": 6.64175148987253e-05,
        "epoch": 0.4089333333333333,
        "step": 3067
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.701562523841858,
        "learning_rate": 6.635823989229333e-05,
        "epoch": 0.4090666666666667,
        "step": 3068
    },
    {
        "loss": 2.6697,
        "grad_norm": 3.0195271968841553,
        "learning_rate": 6.629897821147715e-05,
        "epoch": 0.4092,
        "step": 3069
    },
    {
        "loss": 2.8117,
        "grad_norm": 2.340787172317505,
        "learning_rate": 6.623972987975055e-05,
        "epoch": 0.4093333333333333,
        "step": 3070
    },
    {
        "loss": 0.8708,
        "grad_norm": 2.477529764175415,
        "learning_rate": 6.618049492058201e-05,
        "epoch": 0.40946666666666665,
        "step": 3071
    },
    {
        "loss": 2.4658,
        "grad_norm": 3.0505082607269287,
        "learning_rate": 6.612127335743461e-05,
        "epoch": 0.4096,
        "step": 3072
    },
    {
        "loss": 2.6443,
        "grad_norm": 2.0473315715789795,
        "learning_rate": 6.606206521376632e-05,
        "epoch": 0.40973333333333334,
        "step": 3073
    },
    {
        "loss": 2.1469,
        "grad_norm": 3.941566228866577,
        "learning_rate": 6.600287051302963e-05,
        "epoch": 0.40986666666666666,
        "step": 3074
    },
    {
        "loss": 3.0062,
        "grad_norm": 3.0631208419799805,
        "learning_rate": 6.59436892786718e-05,
        "epoch": 0.41,
        "step": 3075
    },
    {
        "loss": 2.4809,
        "grad_norm": 2.9965124130249023,
        "learning_rate": 6.588452153413465e-05,
        "epoch": 0.41013333333333335,
        "step": 3076
    },
    {
        "loss": 1.7662,
        "grad_norm": 1.577174425125122,
        "learning_rate": 6.582536730285476e-05,
        "epoch": 0.41026666666666667,
        "step": 3077
    },
    {
        "loss": 2.193,
        "grad_norm": 2.76407790184021,
        "learning_rate": 6.576622660826334e-05,
        "epoch": 0.4104,
        "step": 3078
    },
    {
        "loss": 2.5214,
        "grad_norm": 4.0215959548950195,
        "learning_rate": 6.570709947378622e-05,
        "epoch": 0.4105333333333333,
        "step": 3079
    },
    {
        "loss": 2.7056,
        "grad_norm": 3.877748966217041,
        "learning_rate": 6.564798592284383e-05,
        "epoch": 0.4106666666666667,
        "step": 3080
    },
    {
        "loss": 2.4925,
        "grad_norm": 3.950162172317505,
        "learning_rate": 6.558888597885125e-05,
        "epoch": 0.4108,
        "step": 3081
    },
    {
        "loss": 1.6884,
        "grad_norm": 1.9939881563186646,
        "learning_rate": 6.552979966521824e-05,
        "epoch": 0.4109333333333333,
        "step": 3082
    },
    {
        "loss": 2.8292,
        "grad_norm": 2.4107987880706787,
        "learning_rate": 6.547072700534901e-05,
        "epoch": 0.4110666666666667,
        "step": 3083
    },
    {
        "loss": 1.7876,
        "grad_norm": 4.037640571594238,
        "learning_rate": 6.541166802264247e-05,
        "epoch": 0.4112,
        "step": 3084
    },
    {
        "loss": 2.1085,
        "grad_norm": 2.957371473312378,
        "learning_rate": 6.53526227404921e-05,
        "epoch": 0.41133333333333333,
        "step": 3085
    },
    {
        "loss": 2.7175,
        "grad_norm": 3.726518154144287,
        "learning_rate": 6.529359118228589e-05,
        "epoch": 0.41146666666666665,
        "step": 3086
    },
    {
        "loss": 2.1918,
        "grad_norm": 3.393479585647583,
        "learning_rate": 6.523457337140648e-05,
        "epoch": 0.4116,
        "step": 3087
    },
    {
        "loss": 1.7453,
        "grad_norm": 3.363650321960449,
        "learning_rate": 6.517556933123106e-05,
        "epoch": 0.41173333333333334,
        "step": 3088
    },
    {
        "loss": 2.4221,
        "grad_norm": 2.988154649734497,
        "learning_rate": 6.51165790851312e-05,
        "epoch": 0.41186666666666666,
        "step": 3089
    },
    {
        "loss": 2.5348,
        "grad_norm": 2.729541540145874,
        "learning_rate": 6.505760265647329e-05,
        "epoch": 0.412,
        "step": 3090
    },
    {
        "loss": 1.2529,
        "grad_norm": 4.20455265045166,
        "learning_rate": 6.499864006861804e-05,
        "epoch": 0.41213333333333335,
        "step": 3091
    },
    {
        "loss": 2.1309,
        "grad_norm": 2.8335700035095215,
        "learning_rate": 6.493969134492072e-05,
        "epoch": 0.41226666666666667,
        "step": 3092
    },
    {
        "loss": 2.082,
        "grad_norm": 3.5502495765686035,
        "learning_rate": 6.488075650873111e-05,
        "epoch": 0.4124,
        "step": 3093
    },
    {
        "loss": 1.6727,
        "grad_norm": 3.85469126701355,
        "learning_rate": 6.48218355833935e-05,
        "epoch": 0.4125333333333333,
        "step": 3094
    },
    {
        "loss": 1.9375,
        "grad_norm": 4.457569599151611,
        "learning_rate": 6.476292859224669e-05,
        "epoch": 0.4126666666666667,
        "step": 3095
    },
    {
        "loss": 2.0858,
        "grad_norm": 4.748974800109863,
        "learning_rate": 6.470403555862394e-05,
        "epoch": 0.4128,
        "step": 3096
    },
    {
        "loss": 2.5348,
        "grad_norm": 4.052663326263428,
        "learning_rate": 6.464515650585294e-05,
        "epoch": 0.4129333333333333,
        "step": 3097
    },
    {
        "loss": 2.8309,
        "grad_norm": 2.5047643184661865,
        "learning_rate": 6.458629145725596e-05,
        "epoch": 0.4130666666666667,
        "step": 3098
    },
    {
        "loss": 2.2088,
        "grad_norm": 2.4513468742370605,
        "learning_rate": 6.452744043614959e-05,
        "epoch": 0.4132,
        "step": 3099
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.62691330909729,
        "learning_rate": 6.446860346584496e-05,
        "epoch": 0.41333333333333333,
        "step": 3100
    },
    {
        "loss": 2.7301,
        "grad_norm": 3.1352651119232178,
        "learning_rate": 6.440978056964749e-05,
        "epoch": 0.41346666666666665,
        "step": 3101
    },
    {
        "loss": 1.9205,
        "grad_norm": 4.12005615234375,
        "learning_rate": 6.435097177085728e-05,
        "epoch": 0.4136,
        "step": 3102
    },
    {
        "loss": 2.5205,
        "grad_norm": 2.199402332305908,
        "learning_rate": 6.429217709276856e-05,
        "epoch": 0.41373333333333334,
        "step": 3103
    },
    {
        "loss": 1.6652,
        "grad_norm": 3.7636568546295166,
        "learning_rate": 6.423339655867024e-05,
        "epoch": 0.41386666666666666,
        "step": 3104
    },
    {
        "loss": 0.6959,
        "grad_norm": 3.255523204803467,
        "learning_rate": 6.417463019184538e-05,
        "epoch": 0.414,
        "step": 3105
    },
    {
        "loss": 1.3828,
        "grad_norm": 3.087453842163086,
        "learning_rate": 6.411587801557154e-05,
        "epoch": 0.41413333333333335,
        "step": 3106
    },
    {
        "loss": 1.8759,
        "grad_norm": 5.398471832275391,
        "learning_rate": 6.405714005312074e-05,
        "epoch": 0.41426666666666667,
        "step": 3107
    },
    {
        "loss": 2.3027,
        "grad_norm": 2.297297954559326,
        "learning_rate": 6.399841632775922e-05,
        "epoch": 0.4144,
        "step": 3108
    },
    {
        "loss": 2.7005,
        "grad_norm": 2.1920642852783203,
        "learning_rate": 6.393970686274765e-05,
        "epoch": 0.4145333333333333,
        "step": 3109
    },
    {
        "loss": 2.1944,
        "grad_norm": 2.7170841693878174,
        "learning_rate": 6.38810116813411e-05,
        "epoch": 0.4146666666666667,
        "step": 3110
    },
    {
        "loss": 2.2863,
        "grad_norm": 3.260071039199829,
        "learning_rate": 6.382233080678885e-05,
        "epoch": 0.4148,
        "step": 3111
    },
    {
        "loss": 2.7395,
        "grad_norm": 3.0953562259674072,
        "learning_rate": 6.376366426233466e-05,
        "epoch": 0.4149333333333333,
        "step": 3112
    },
    {
        "loss": 1.7549,
        "grad_norm": 2.6878442764282227,
        "learning_rate": 6.370501207121656e-05,
        "epoch": 0.41506666666666664,
        "step": 3113
    },
    {
        "loss": 1.2772,
        "grad_norm": 3.6367363929748535,
        "learning_rate": 6.364637425666679e-05,
        "epoch": 0.4152,
        "step": 3114
    },
    {
        "loss": 2.6035,
        "grad_norm": 2.527559518814087,
        "learning_rate": 6.35877508419121e-05,
        "epoch": 0.41533333333333333,
        "step": 3115
    },
    {
        "loss": 2.1608,
        "grad_norm": 4.252775192260742,
        "learning_rate": 6.352914185017335e-05,
        "epoch": 0.41546666666666665,
        "step": 3116
    },
    {
        "loss": 2.7137,
        "grad_norm": 2.548269271850586,
        "learning_rate": 6.34705473046658e-05,
        "epoch": 0.4156,
        "step": 3117
    },
    {
        "loss": 2.5627,
        "grad_norm": 3.534287929534912,
        "learning_rate": 6.341196722859892e-05,
        "epoch": 0.41573333333333334,
        "step": 3118
    },
    {
        "loss": 2.4543,
        "grad_norm": 3.61687970161438,
        "learning_rate": 6.335340164517648e-05,
        "epoch": 0.41586666666666666,
        "step": 3119
    },
    {
        "loss": 2.7115,
        "grad_norm": 1.9489104747772217,
        "learning_rate": 6.329485057759654e-05,
        "epoch": 0.416,
        "step": 3120
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.6990373134613037,
        "learning_rate": 6.323631404905132e-05,
        "epoch": 0.41613333333333336,
        "step": 3121
    },
    {
        "loss": 1.5948,
        "grad_norm": 3.8295938968658447,
        "learning_rate": 6.31777920827274e-05,
        "epoch": 0.4162666666666667,
        "step": 3122
    },
    {
        "loss": 1.0975,
        "grad_norm": 3.5689520835876465,
        "learning_rate": 6.311928470180551e-05,
        "epoch": 0.4164,
        "step": 3123
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.9143147468566895,
        "learning_rate": 6.306079192946062e-05,
        "epoch": 0.4165333333333333,
        "step": 3124
    },
    {
        "loss": 2.6646,
        "grad_norm": 3.732520341873169,
        "learning_rate": 6.30023137888619e-05,
        "epoch": 0.4166666666666667,
        "step": 3125
    },
    {
        "loss": 2.9183,
        "grad_norm": 3.815129041671753,
        "learning_rate": 6.294385030317277e-05,
        "epoch": 0.4168,
        "step": 3126
    },
    {
        "loss": 1.6414,
        "grad_norm": 3.9745843410491943,
        "learning_rate": 6.288540149555083e-05,
        "epoch": 0.4169333333333333,
        "step": 3127
    },
    {
        "loss": 2.3764,
        "grad_norm": 3.03178334236145,
        "learning_rate": 6.282696738914772e-05,
        "epoch": 0.41706666666666664,
        "step": 3128
    },
    {
        "loss": 2.5794,
        "grad_norm": 2.654420852661133,
        "learning_rate": 6.27685480071096e-05,
        "epoch": 0.4172,
        "step": 3129
    },
    {
        "loss": 2.3159,
        "grad_norm": 2.7812044620513916,
        "learning_rate": 6.271014337257644e-05,
        "epoch": 0.41733333333333333,
        "step": 3130
    },
    {
        "loss": 3.0439,
        "grad_norm": 3.025214195251465,
        "learning_rate": 6.265175350868253e-05,
        "epoch": 0.41746666666666665,
        "step": 3131
    },
    {
        "loss": 1.07,
        "grad_norm": 3.1789183616638184,
        "learning_rate": 6.259337843855633e-05,
        "epoch": 0.4176,
        "step": 3132
    },
    {
        "loss": 2.7591,
        "grad_norm": 2.086355209350586,
        "learning_rate": 6.253501818532039e-05,
        "epoch": 0.41773333333333335,
        "step": 3133
    },
    {
        "loss": 2.5826,
        "grad_norm": 3.1685879230499268,
        "learning_rate": 6.247667277209142e-05,
        "epoch": 0.41786666666666666,
        "step": 3134
    },
    {
        "loss": 0.9234,
        "grad_norm": 2.968416929244995,
        "learning_rate": 6.241834222198024e-05,
        "epoch": 0.418,
        "step": 3135
    },
    {
        "loss": 2.5481,
        "grad_norm": 3.7531683444976807,
        "learning_rate": 6.23600265580917e-05,
        "epoch": 0.41813333333333336,
        "step": 3136
    },
    {
        "loss": 3.0378,
        "grad_norm": 4.223178863525391,
        "learning_rate": 6.230172580352493e-05,
        "epoch": 0.4182666666666667,
        "step": 3137
    },
    {
        "loss": 2.8146,
        "grad_norm": 3.591304063796997,
        "learning_rate": 6.224343998137305e-05,
        "epoch": 0.4184,
        "step": 3138
    },
    {
        "loss": 2.6806,
        "grad_norm": 3.350504159927368,
        "learning_rate": 6.218516911472318e-05,
        "epoch": 0.4185333333333333,
        "step": 3139
    },
    {
        "loss": 2.7028,
        "grad_norm": 3.766709566116333,
        "learning_rate": 6.212691322665674e-05,
        "epoch": 0.4186666666666667,
        "step": 3140
    },
    {
        "loss": 2.5705,
        "grad_norm": 3.7782750129699707,
        "learning_rate": 6.206867234024895e-05,
        "epoch": 0.4188,
        "step": 3141
    },
    {
        "loss": 2.5083,
        "grad_norm": 2.855534791946411,
        "learning_rate": 6.201044647856935e-05,
        "epoch": 0.4189333333333333,
        "step": 3142
    },
    {
        "loss": 1.9919,
        "grad_norm": 2.336642026901245,
        "learning_rate": 6.195223566468135e-05,
        "epoch": 0.41906666666666664,
        "step": 3143
    },
    {
        "loss": 2.3899,
        "grad_norm": 3.1782472133636475,
        "learning_rate": 6.189403992164242e-05,
        "epoch": 0.4192,
        "step": 3144
    },
    {
        "loss": 2.7621,
        "grad_norm": 2.874328851699829,
        "learning_rate": 6.183585927250412e-05,
        "epoch": 0.41933333333333334,
        "step": 3145
    },
    {
        "loss": 2.7616,
        "grad_norm": 3.9492673873901367,
        "learning_rate": 6.177769374031201e-05,
        "epoch": 0.41946666666666665,
        "step": 3146
    },
    {
        "loss": 1.6226,
        "grad_norm": 2.868513822555542,
        "learning_rate": 6.171954334810567e-05,
        "epoch": 0.4196,
        "step": 3147
    },
    {
        "loss": 1.9874,
        "grad_norm": 3.002619981765747,
        "learning_rate": 6.16614081189186e-05,
        "epoch": 0.41973333333333335,
        "step": 3148
    },
    {
        "loss": 2.1509,
        "grad_norm": 3.9378716945648193,
        "learning_rate": 6.160328807577845e-05,
        "epoch": 0.41986666666666667,
        "step": 3149
    },
    {
        "loss": 2.9132,
        "grad_norm": 2.7299633026123047,
        "learning_rate": 6.154518324170673e-05,
        "epoch": 0.42,
        "step": 3150
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.162367105484009,
        "learning_rate": 6.148709363971896e-05,
        "epoch": 0.42013333333333336,
        "step": 3151
    },
    {
        "loss": 2.6944,
        "grad_norm": 3.104367971420288,
        "learning_rate": 6.142901929282462e-05,
        "epoch": 0.4202666666666667,
        "step": 3152
    },
    {
        "loss": 2.4174,
        "grad_norm": 2.5289769172668457,
        "learning_rate": 6.137096022402713e-05,
        "epoch": 0.4204,
        "step": 3153
    },
    {
        "loss": 2.3552,
        "grad_norm": 3.6355910301208496,
        "learning_rate": 6.131291645632398e-05,
        "epoch": 0.4205333333333333,
        "step": 3154
    },
    {
        "loss": 2.5279,
        "grad_norm": 2.937490224838257,
        "learning_rate": 6.12548880127064e-05,
        "epoch": 0.4206666666666667,
        "step": 3155
    },
    {
        "loss": 2.2259,
        "grad_norm": 3.8828837871551514,
        "learning_rate": 6.11968749161597e-05,
        "epoch": 0.4208,
        "step": 3156
    },
    {
        "loss": 2.1871,
        "grad_norm": 4.401808738708496,
        "learning_rate": 6.113887718966309e-05,
        "epoch": 0.4209333333333333,
        "step": 3157
    },
    {
        "loss": 1.189,
        "grad_norm": 3.9922080039978027,
        "learning_rate": 6.108089485618963e-05,
        "epoch": 0.42106666666666664,
        "step": 3158
    },
    {
        "loss": 3.2692,
        "grad_norm": 3.8336832523345947,
        "learning_rate": 6.102292793870634e-05,
        "epoch": 0.4212,
        "step": 3159
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.6801397800445557,
        "learning_rate": 6.096497646017412e-05,
        "epoch": 0.42133333333333334,
        "step": 3160
    },
    {
        "loss": 1.5628,
        "grad_norm": 3.729881525039673,
        "learning_rate": 6.090704044354767e-05,
        "epoch": 0.42146666666666666,
        "step": 3161
    },
    {
        "loss": 2.0685,
        "grad_norm": 3.915729522705078,
        "learning_rate": 6.084911991177572e-05,
        "epoch": 0.4216,
        "step": 3162
    },
    {
        "loss": 2.3262,
        "grad_norm": 3.9349637031555176,
        "learning_rate": 6.0791214887800806e-05,
        "epoch": 0.42173333333333335,
        "step": 3163
    },
    {
        "loss": 1.8423,
        "grad_norm": 3.4872567653656006,
        "learning_rate": 6.073332539455927e-05,
        "epoch": 0.42186666666666667,
        "step": 3164
    },
    {
        "loss": 2.1553,
        "grad_norm": 2.867379665374756,
        "learning_rate": 6.0675451454981326e-05,
        "epoch": 0.422,
        "step": 3165
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.6312460899353027,
        "learning_rate": 6.0617593091991086e-05,
        "epoch": 0.42213333333333336,
        "step": 3166
    },
    {
        "loss": 2.6477,
        "grad_norm": 2.6967523097991943,
        "learning_rate": 6.055975032850643e-05,
        "epoch": 0.4222666666666667,
        "step": 3167
    },
    {
        "loss": 1.9522,
        "grad_norm": 2.8563740253448486,
        "learning_rate": 6.050192318743903e-05,
        "epoch": 0.4224,
        "step": 3168
    },
    {
        "loss": 2.3406,
        "grad_norm": 3.4366161823272705,
        "learning_rate": 6.044411169169444e-05,
        "epoch": 0.4225333333333333,
        "step": 3169
    },
    {
        "loss": 0.9057,
        "grad_norm": 3.5667014122009277,
        "learning_rate": 6.0386315864171984e-05,
        "epoch": 0.4226666666666667,
        "step": 3170
    },
    {
        "loss": 2.4751,
        "grad_norm": 3.5303776264190674,
        "learning_rate": 6.03285357277648e-05,
        "epoch": 0.4228,
        "step": 3171
    },
    {
        "loss": 2.5763,
        "grad_norm": 3.1384873390197754,
        "learning_rate": 6.0270771305359795e-05,
        "epoch": 0.42293333333333333,
        "step": 3172
    },
    {
        "loss": 1.9508,
        "grad_norm": 3.5390779972076416,
        "learning_rate": 6.021302261983763e-05,
        "epoch": 0.42306666666666665,
        "step": 3173
    },
    {
        "loss": 2.5189,
        "grad_norm": 2.9449963569641113,
        "learning_rate": 6.01552896940728e-05,
        "epoch": 0.4232,
        "step": 3174
    },
    {
        "loss": 2.7261,
        "grad_norm": 4.599256992340088,
        "learning_rate": 6.00975725509335e-05,
        "epoch": 0.42333333333333334,
        "step": 3175
    },
    {
        "loss": 1.888,
        "grad_norm": 2.925201892852783,
        "learning_rate": 6.0039871213281685e-05,
        "epoch": 0.42346666666666666,
        "step": 3176
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.5963127613067627,
        "learning_rate": 5.998218570397303e-05,
        "epoch": 0.4236,
        "step": 3177
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.522200345993042,
        "learning_rate": 5.992451604585695e-05,
        "epoch": 0.42373333333333335,
        "step": 3178
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.160433292388916,
        "learning_rate": 5.9866862261776634e-05,
        "epoch": 0.42386666666666667,
        "step": 3179
    },
    {
        "loss": 2.7216,
        "grad_norm": 3.2661690711975098,
        "learning_rate": 5.980922437456893e-05,
        "epoch": 0.424,
        "step": 3180
    },
    {
        "loss": 2.7183,
        "grad_norm": 3.482825517654419,
        "learning_rate": 5.9751602407064374e-05,
        "epoch": 0.4241333333333333,
        "step": 3181
    },
    {
        "loss": 2.0929,
        "grad_norm": 2.5694143772125244,
        "learning_rate": 5.9693996382087235e-05,
        "epoch": 0.4242666666666667,
        "step": 3182
    },
    {
        "loss": 2.5378,
        "grad_norm": 2.287003517150879,
        "learning_rate": 5.963640632245553e-05,
        "epoch": 0.4244,
        "step": 3183
    },
    {
        "loss": 2.9438,
        "grad_norm": 3.590017318725586,
        "learning_rate": 5.9578832250980796e-05,
        "epoch": 0.4245333333333333,
        "step": 3184
    },
    {
        "loss": 2.409,
        "grad_norm": 2.7148873805999756,
        "learning_rate": 5.952127419046833e-05,
        "epoch": 0.4246666666666667,
        "step": 3185
    },
    {
        "loss": 0.8719,
        "grad_norm": 3.659452199935913,
        "learning_rate": 5.946373216371709e-05,
        "epoch": 0.4248,
        "step": 3186
    },
    {
        "loss": 2.1227,
        "grad_norm": 3.2160725593566895,
        "learning_rate": 5.940620619351964e-05,
        "epoch": 0.42493333333333333,
        "step": 3187
    },
    {
        "loss": 2.0282,
        "grad_norm": 2.5755157470703125,
        "learning_rate": 5.9348696302662266e-05,
        "epoch": 0.42506666666666665,
        "step": 3188
    },
    {
        "loss": 1.6323,
        "grad_norm": 4.34370756149292,
        "learning_rate": 5.929120251392482e-05,
        "epoch": 0.4252,
        "step": 3189
    },
    {
        "loss": 2.2124,
        "grad_norm": 3.6865482330322266,
        "learning_rate": 5.9233724850080694e-05,
        "epoch": 0.42533333333333334,
        "step": 3190
    },
    {
        "loss": 1.3421,
        "grad_norm": 3.861135959625244,
        "learning_rate": 5.917626333389712e-05,
        "epoch": 0.42546666666666666,
        "step": 3191
    },
    {
        "loss": 2.382,
        "grad_norm": 3.203829288482666,
        "learning_rate": 5.9118817988134766e-05,
        "epoch": 0.4256,
        "step": 3192
    },
    {
        "loss": 2.6893,
        "grad_norm": 2.007317543029785,
        "learning_rate": 5.906138883554788e-05,
        "epoch": 0.42573333333333335,
        "step": 3193
    },
    {
        "loss": 2.3109,
        "grad_norm": 3.71612286567688,
        "learning_rate": 5.9003975898884365e-05,
        "epoch": 0.42586666666666667,
        "step": 3194
    },
    {
        "loss": 0.7888,
        "grad_norm": 3.046825647354126,
        "learning_rate": 5.8946579200885666e-05,
        "epoch": 0.426,
        "step": 3195
    },
    {
        "loss": 2.9564,
        "grad_norm": 3.852069139480591,
        "learning_rate": 5.888919876428687e-05,
        "epoch": 0.4261333333333333,
        "step": 3196
    },
    {
        "loss": 2.6774,
        "grad_norm": 1.9050037860870361,
        "learning_rate": 5.883183461181651e-05,
        "epoch": 0.4262666666666667,
        "step": 3197
    },
    {
        "loss": 0.8788,
        "grad_norm": 2.896667003631592,
        "learning_rate": 5.877448676619672e-05,
        "epoch": 0.4264,
        "step": 3198
    },
    {
        "loss": 2.5709,
        "grad_norm": 1.8040753602981567,
        "learning_rate": 5.8717155250143206e-05,
        "epoch": 0.4265333333333333,
        "step": 3199
    },
    {
        "loss": 2.6295,
        "grad_norm": 2.9032809734344482,
        "learning_rate": 5.865984008636518e-05,
        "epoch": 0.4266666666666667,
        "step": 3200
    },
    {
        "loss": 2.3439,
        "grad_norm": 2.5024359226226807,
        "learning_rate": 5.860254129756537e-05,
        "epoch": 0.4268,
        "step": 3201
    },
    {
        "loss": 2.7381,
        "grad_norm": 2.474003553390503,
        "learning_rate": 5.854525890643996e-05,
        "epoch": 0.42693333333333333,
        "step": 3202
    },
    {
        "loss": 2.4166,
        "grad_norm": 2.4368913173675537,
        "learning_rate": 5.848799293567879e-05,
        "epoch": 0.42706666666666665,
        "step": 3203
    },
    {
        "loss": 2.2724,
        "grad_norm": 2.8163514137268066,
        "learning_rate": 5.843074340796503e-05,
        "epoch": 0.4272,
        "step": 3204
    },
    {
        "loss": 2.5528,
        "grad_norm": 2.805696725845337,
        "learning_rate": 5.837351034597548e-05,
        "epoch": 0.42733333333333334,
        "step": 3205
    },
    {
        "loss": 2.1246,
        "grad_norm": 3.7732632160186768,
        "learning_rate": 5.831629377238034e-05,
        "epoch": 0.42746666666666666,
        "step": 3206
    },
    {
        "loss": 1.2718,
        "grad_norm": 3.382222890853882,
        "learning_rate": 5.8259093709843236e-05,
        "epoch": 0.4276,
        "step": 3207
    },
    {
        "loss": 2.6854,
        "grad_norm": 1.9981633424758911,
        "learning_rate": 5.820191018102133e-05,
        "epoch": 0.42773333333333335,
        "step": 3208
    },
    {
        "loss": 1.7997,
        "grad_norm": 4.850613594055176,
        "learning_rate": 5.814474320856528e-05,
        "epoch": 0.4278666666666667,
        "step": 3209
    },
    {
        "loss": 2.6193,
        "grad_norm": 1.7136050462722778,
        "learning_rate": 5.808759281511905e-05,
        "epoch": 0.428,
        "step": 3210
    },
    {
        "loss": 1.9901,
        "grad_norm": 3.3735997676849365,
        "learning_rate": 5.803045902332009e-05,
        "epoch": 0.4281333333333333,
        "step": 3211
    },
    {
        "loss": 1.9108,
        "grad_norm": 3.9528210163116455,
        "learning_rate": 5.797334185579931e-05,
        "epoch": 0.4282666666666667,
        "step": 3212
    },
    {
        "loss": 2.3177,
        "grad_norm": 3.7280168533325195,
        "learning_rate": 5.7916241335181066e-05,
        "epoch": 0.4284,
        "step": 3213
    },
    {
        "loss": 2.543,
        "grad_norm": 2.608048677444458,
        "learning_rate": 5.7859157484082994e-05,
        "epoch": 0.4285333333333333,
        "step": 3214
    },
    {
        "loss": 1.8763,
        "grad_norm": 2.8497695922851562,
        "learning_rate": 5.780209032511622e-05,
        "epoch": 0.42866666666666664,
        "step": 3215
    },
    {
        "loss": 2.7741,
        "grad_norm": 3.4590535163879395,
        "learning_rate": 5.77450398808853e-05,
        "epoch": 0.4288,
        "step": 3216
    },
    {
        "loss": 3.3077,
        "grad_norm": 2.9620842933654785,
        "learning_rate": 5.768800617398802e-05,
        "epoch": 0.42893333333333333,
        "step": 3217
    },
    {
        "loss": 1.7726,
        "grad_norm": 2.3309268951416016,
        "learning_rate": 5.763098922701572e-05,
        "epoch": 0.42906666666666665,
        "step": 3218
    },
    {
        "loss": 2.8272,
        "grad_norm": 3.218492031097412,
        "learning_rate": 5.757398906255295e-05,
        "epoch": 0.4292,
        "step": 3219
    },
    {
        "loss": 2.2566,
        "grad_norm": 3.569678783416748,
        "learning_rate": 5.751700570317759e-05,
        "epoch": 0.42933333333333334,
        "step": 3220
    },
    {
        "loss": 0.987,
        "grad_norm": 3.400562286376953,
        "learning_rate": 5.746003917146112e-05,
        "epoch": 0.42946666666666666,
        "step": 3221
    },
    {
        "loss": 2.7537,
        "grad_norm": 2.3321468830108643,
        "learning_rate": 5.740308948996804e-05,
        "epoch": 0.4296,
        "step": 3222
    },
    {
        "loss": 1.6456,
        "grad_norm": 3.7060248851776123,
        "learning_rate": 5.734615668125639e-05,
        "epoch": 0.42973333333333336,
        "step": 3223
    },
    {
        "loss": 2.6774,
        "grad_norm": 3.1615118980407715,
        "learning_rate": 5.728924076787734e-05,
        "epoch": 0.4298666666666667,
        "step": 3224
    },
    {
        "loss": 2.5592,
        "grad_norm": 2.8146398067474365,
        "learning_rate": 5.723234177237564e-05,
        "epoch": 0.43,
        "step": 3225
    },
    {
        "loss": 2.099,
        "grad_norm": 6.994894504547119,
        "learning_rate": 5.7175459717289105e-05,
        "epoch": 0.4301333333333333,
        "step": 3226
    },
    {
        "loss": 2.8422,
        "grad_norm": 3.057126998901367,
        "learning_rate": 5.7118594625148834e-05,
        "epoch": 0.4302666666666667,
        "step": 3227
    },
    {
        "loss": 1.6218,
        "grad_norm": 3.052022933959961,
        "learning_rate": 5.70617465184794e-05,
        "epoch": 0.4304,
        "step": 3228
    },
    {
        "loss": 2.7702,
        "grad_norm": 2.85341477394104,
        "learning_rate": 5.7004915419798434e-05,
        "epoch": 0.4305333333333333,
        "step": 3229
    },
    {
        "loss": 2.3385,
        "grad_norm": 3.3844246864318848,
        "learning_rate": 5.694810135161697e-05,
        "epoch": 0.43066666666666664,
        "step": 3230
    },
    {
        "loss": 3.1296,
        "grad_norm": 2.041952133178711,
        "learning_rate": 5.68913043364393e-05,
        "epoch": 0.4308,
        "step": 3231
    },
    {
        "loss": 2.6853,
        "grad_norm": 3.1047005653381348,
        "learning_rate": 5.6834524396762835e-05,
        "epoch": 0.43093333333333333,
        "step": 3232
    },
    {
        "loss": 1.248,
        "grad_norm": 3.233227491378784,
        "learning_rate": 5.677776155507833e-05,
        "epoch": 0.43106666666666665,
        "step": 3233
    },
    {
        "loss": 2.806,
        "grad_norm": 2.5226473808288574,
        "learning_rate": 5.672101583386979e-05,
        "epoch": 0.4312,
        "step": 3234
    },
    {
        "loss": 2.7985,
        "grad_norm": 3.4981658458709717,
        "learning_rate": 5.6664287255614354e-05,
        "epoch": 0.43133333333333335,
        "step": 3235
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.8036627769470215,
        "learning_rate": 5.6607575842782355e-05,
        "epoch": 0.43146666666666667,
        "step": 3236
    },
    {
        "loss": 2.4119,
        "grad_norm": 2.04840087890625,
        "learning_rate": 5.6550881617837435e-05,
        "epoch": 0.4316,
        "step": 3237
    },
    {
        "loss": 2.309,
        "grad_norm": 3.0894484519958496,
        "learning_rate": 5.6494204603236377e-05,
        "epoch": 0.43173333333333336,
        "step": 3238
    },
    {
        "loss": 2.2839,
        "grad_norm": 3.6249170303344727,
        "learning_rate": 5.643754482142909e-05,
        "epoch": 0.4318666666666667,
        "step": 3239
    },
    {
        "loss": 2.7586,
        "grad_norm": 2.039680242538452,
        "learning_rate": 5.638090229485873e-05,
        "epoch": 0.432,
        "step": 3240
    },
    {
        "loss": 2.6038,
        "grad_norm": 3.2777256965637207,
        "learning_rate": 5.6324277045961637e-05,
        "epoch": 0.4321333333333333,
        "step": 3241
    },
    {
        "loss": 1.3652,
        "grad_norm": 2.8243346214294434,
        "learning_rate": 5.6267669097167187e-05,
        "epoch": 0.4322666666666667,
        "step": 3242
    },
    {
        "loss": 3.4158,
        "grad_norm": 3.375864028930664,
        "learning_rate": 5.6211078470898014e-05,
        "epoch": 0.4324,
        "step": 3243
    },
    {
        "loss": 2.3599,
        "grad_norm": 4.08286190032959,
        "learning_rate": 5.615450518956993e-05,
        "epoch": 0.4325333333333333,
        "step": 3244
    },
    {
        "loss": 1.6816,
        "grad_norm": 3.9164321422576904,
        "learning_rate": 5.609794927559171e-05,
        "epoch": 0.43266666666666664,
        "step": 3245
    },
    {
        "loss": 1.8616,
        "grad_norm": 3.5200908184051514,
        "learning_rate": 5.604141075136532e-05,
        "epoch": 0.4328,
        "step": 3246
    },
    {
        "loss": 3.077,
        "grad_norm": 2.209644317626953,
        "learning_rate": 5.5984889639285984e-05,
        "epoch": 0.43293333333333334,
        "step": 3247
    },
    {
        "loss": 2.6895,
        "grad_norm": 3.596153497695923,
        "learning_rate": 5.592838596174184e-05,
        "epoch": 0.43306666666666666,
        "step": 3248
    },
    {
        "loss": 2.4848,
        "grad_norm": 2.521786689758301,
        "learning_rate": 5.5871899741114106e-05,
        "epoch": 0.4332,
        "step": 3249
    },
    {
        "loss": 2.215,
        "grad_norm": 2.416823387145996,
        "learning_rate": 5.581543099977733e-05,
        "epoch": 0.43333333333333335,
        "step": 3250
    },
    {
        "loss": 1.6071,
        "grad_norm": 2.758805990219116,
        "learning_rate": 5.5758979760098895e-05,
        "epoch": 0.43346666666666667,
        "step": 3251
    },
    {
        "loss": 1.8436,
        "grad_norm": 2.946779489517212,
        "learning_rate": 5.570254604443929e-05,
        "epoch": 0.4336,
        "step": 3252
    },
    {
        "loss": 2.3975,
        "grad_norm": 3.4069159030914307,
        "learning_rate": 5.5646129875152166e-05,
        "epoch": 0.43373333333333336,
        "step": 3253
    },
    {
        "loss": 2.7966,
        "grad_norm": 2.805617094039917,
        "learning_rate": 5.558973127458409e-05,
        "epoch": 0.4338666666666667,
        "step": 3254
    },
    {
        "loss": 2.972,
        "grad_norm": 2.96777606010437,
        "learning_rate": 5.553335026507478e-05,
        "epoch": 0.434,
        "step": 3255
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.0037853717803955,
        "learning_rate": 5.547698686895699e-05,
        "epoch": 0.4341333333333333,
        "step": 3256
    },
    {
        "loss": 2.591,
        "grad_norm": 2.4432406425476074,
        "learning_rate": 5.542064110855636e-05,
        "epoch": 0.4342666666666667,
        "step": 3257
    },
    {
        "loss": 1.7745,
        "grad_norm": 3.192131757736206,
        "learning_rate": 5.5364313006191706e-05,
        "epoch": 0.4344,
        "step": 3258
    },
    {
        "loss": 2.1353,
        "grad_norm": 3.510315179824829,
        "learning_rate": 5.530800258417479e-05,
        "epoch": 0.4345333333333333,
        "step": 3259
    },
    {
        "loss": 1.9427,
        "grad_norm": 4.427607536315918,
        "learning_rate": 5.525170986481034e-05,
        "epoch": 0.43466666666666665,
        "step": 3260
    },
    {
        "loss": 2.1063,
        "grad_norm": 2.106743097305298,
        "learning_rate": 5.5195434870396044e-05,
        "epoch": 0.4348,
        "step": 3261
    },
    {
        "loss": 2.1777,
        "grad_norm": 2.1882855892181396,
        "learning_rate": 5.513917762322266e-05,
        "epoch": 0.43493333333333334,
        "step": 3262
    },
    {
        "loss": 2.6267,
        "grad_norm": 2.623300552368164,
        "learning_rate": 5.508293814557388e-05,
        "epoch": 0.43506666666666666,
        "step": 3263
    },
    {
        "loss": 2.5525,
        "grad_norm": 2.524021625518799,
        "learning_rate": 5.5026716459726404e-05,
        "epoch": 0.4352,
        "step": 3264
    },
    {
        "loss": 2.1703,
        "grad_norm": 2.3131392002105713,
        "learning_rate": 5.497051258794973e-05,
        "epoch": 0.43533333333333335,
        "step": 3265
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.7819008827209473,
        "learning_rate": 5.4914326552506454e-05,
        "epoch": 0.43546666666666667,
        "step": 3266
    },
    {
        "loss": 1.602,
        "grad_norm": 4.565459728240967,
        "learning_rate": 5.48581583756521e-05,
        "epoch": 0.4356,
        "step": 3267
    },
    {
        "loss": 2.2467,
        "grad_norm": 3.6629369258880615,
        "learning_rate": 5.480200807963499e-05,
        "epoch": 0.4357333333333333,
        "step": 3268
    },
    {
        "loss": 2.92,
        "grad_norm": 4.930934906005859,
        "learning_rate": 5.4745875686696534e-05,
        "epoch": 0.4358666666666667,
        "step": 3269
    },
    {
        "loss": 1.802,
        "grad_norm": 3.236698627471924,
        "learning_rate": 5.46897612190709e-05,
        "epoch": 0.436,
        "step": 3270
    },
    {
        "loss": 1.8326,
        "grad_norm": 3.1543123722076416,
        "learning_rate": 5.4633664698985174e-05,
        "epoch": 0.4361333333333333,
        "step": 3271
    },
    {
        "loss": 2.5253,
        "grad_norm": 1.965295672416687,
        "learning_rate": 5.457758614865952e-05,
        "epoch": 0.4362666666666667,
        "step": 3272
    },
    {
        "loss": 1.2829,
        "grad_norm": 4.458360195159912,
        "learning_rate": 5.4521525590306766e-05,
        "epoch": 0.4364,
        "step": 3273
    },
    {
        "loss": 1.7018,
        "grad_norm": 2.2407493591308594,
        "learning_rate": 5.44654830461326e-05,
        "epoch": 0.43653333333333333,
        "step": 3274
    },
    {
        "loss": 2.3943,
        "grad_norm": 2.694838285446167,
        "learning_rate": 5.4409458538335855e-05,
        "epoch": 0.43666666666666665,
        "step": 3275
    },
    {
        "loss": 1.3431,
        "grad_norm": 3.7864553928375244,
        "learning_rate": 5.4353452089107916e-05,
        "epoch": 0.4368,
        "step": 3276
    },
    {
        "loss": 2.554,
        "grad_norm": 2.9908623695373535,
        "learning_rate": 5.429746372063309e-05,
        "epoch": 0.43693333333333334,
        "step": 3277
    },
    {
        "loss": 2.8583,
        "grad_norm": 3.1145403385162354,
        "learning_rate": 5.4241493455088664e-05,
        "epoch": 0.43706666666666666,
        "step": 3278
    },
    {
        "loss": 1.0418,
        "grad_norm": 4.534510612487793,
        "learning_rate": 5.418554131464456e-05,
        "epoch": 0.4372,
        "step": 3279
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.531390905380249,
        "learning_rate": 5.412960732146365e-05,
        "epoch": 0.43733333333333335,
        "step": 3280
    },
    {
        "loss": 2.4571,
        "grad_norm": 3.200361728668213,
        "learning_rate": 5.407369149770162e-05,
        "epoch": 0.43746666666666667,
        "step": 3281
    },
    {
        "loss": 2.7164,
        "grad_norm": 1.79659903049469,
        "learning_rate": 5.401779386550683e-05,
        "epoch": 0.4376,
        "step": 3282
    },
    {
        "loss": 2.3294,
        "grad_norm": 4.3229241371154785,
        "learning_rate": 5.396191444702058e-05,
        "epoch": 0.4377333333333333,
        "step": 3283
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.5034263134002686,
        "learning_rate": 5.390605326437688e-05,
        "epoch": 0.4378666666666667,
        "step": 3284
    },
    {
        "loss": 2.8838,
        "grad_norm": 2.5326147079467773,
        "learning_rate": 5.3850210339702586e-05,
        "epoch": 0.438,
        "step": 3285
    },
    {
        "loss": 2.7918,
        "grad_norm": 1.6567860841751099,
        "learning_rate": 5.3794385695117236e-05,
        "epoch": 0.4381333333333333,
        "step": 3286
    },
    {
        "loss": 1.4092,
        "grad_norm": 3.8652026653289795,
        "learning_rate": 5.373857935273311e-05,
        "epoch": 0.4382666666666667,
        "step": 3287
    },
    {
        "loss": 2.2753,
        "grad_norm": 3.096815586090088,
        "learning_rate": 5.368279133465533e-05,
        "epoch": 0.4384,
        "step": 3288
    },
    {
        "loss": 0.8118,
        "grad_norm": 3.7457144260406494,
        "learning_rate": 5.362702166298177e-05,
        "epoch": 0.43853333333333333,
        "step": 3289
    },
    {
        "loss": 1.1234,
        "grad_norm": 3.236633539199829,
        "learning_rate": 5.35712703598029e-05,
        "epoch": 0.43866666666666665,
        "step": 3290
    },
    {
        "loss": 2.649,
        "grad_norm": 3.5015058517456055,
        "learning_rate": 5.3515537447202036e-05,
        "epoch": 0.4388,
        "step": 3291
    },
    {
        "loss": 2.3206,
        "grad_norm": 2.9958441257476807,
        "learning_rate": 5.345982294725522e-05,
        "epoch": 0.43893333333333334,
        "step": 3292
    },
    {
        "loss": 2.3422,
        "grad_norm": 2.4594202041625977,
        "learning_rate": 5.340412688203109e-05,
        "epoch": 0.43906666666666666,
        "step": 3293
    },
    {
        "loss": 2.4054,
        "grad_norm": 4.065860271453857,
        "learning_rate": 5.3348449273591106e-05,
        "epoch": 0.4392,
        "step": 3294
    },
    {
        "loss": 1.7433,
        "grad_norm": 5.74672794342041,
        "learning_rate": 5.329279014398931e-05,
        "epoch": 0.43933333333333335,
        "step": 3295
    },
    {
        "loss": 1.9009,
        "grad_norm": 4.703622817993164,
        "learning_rate": 5.3237149515272435e-05,
        "epoch": 0.43946666666666667,
        "step": 3296
    },
    {
        "loss": 2.2963,
        "grad_norm": 2.990664005279541,
        "learning_rate": 5.318152740948004e-05,
        "epoch": 0.4396,
        "step": 3297
    },
    {
        "loss": 2.2118,
        "grad_norm": 3.832970142364502,
        "learning_rate": 5.3125923848644175e-05,
        "epoch": 0.4397333333333333,
        "step": 3298
    },
    {
        "loss": 2.3086,
        "grad_norm": 3.156337022781372,
        "learning_rate": 5.3070338854789516e-05,
        "epoch": 0.4398666666666667,
        "step": 3299
    },
    {
        "loss": 2.6291,
        "grad_norm": 3.855581045150757,
        "learning_rate": 5.3014772449933625e-05,
        "epoch": 0.44,
        "step": 3300
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.6883225440979004,
        "learning_rate": 5.295922465608646e-05,
        "epoch": 0.4401333333333333,
        "step": 3301
    },
    {
        "loss": 2.189,
        "grad_norm": 3.5003790855407715,
        "learning_rate": 5.290369549525066e-05,
        "epoch": 0.44026666666666664,
        "step": 3302
    },
    {
        "loss": 1.2836,
        "grad_norm": 3.4933524131774902,
        "learning_rate": 5.284818498942155e-05,
        "epoch": 0.4404,
        "step": 3303
    },
    {
        "loss": 2.1225,
        "grad_norm": 3.0055160522460938,
        "learning_rate": 5.279269316058707e-05,
        "epoch": 0.44053333333333333,
        "step": 3304
    },
    {
        "loss": 0.8918,
        "grad_norm": 2.707122802734375,
        "learning_rate": 5.273722003072764e-05,
        "epoch": 0.44066666666666665,
        "step": 3305
    },
    {
        "loss": 2.0482,
        "grad_norm": 3.238722562789917,
        "learning_rate": 5.268176562181641e-05,
        "epoch": 0.4408,
        "step": 3306
    },
    {
        "loss": 1.8343,
        "grad_norm": 5.055643558502197,
        "learning_rate": 5.262632995581909e-05,
        "epoch": 0.44093333333333334,
        "step": 3307
    },
    {
        "loss": 2.7431,
        "grad_norm": 2.7315258979797363,
        "learning_rate": 5.2570913054693836e-05,
        "epoch": 0.44106666666666666,
        "step": 3308
    },
    {
        "loss": 2.6963,
        "grad_norm": 4.359859466552734,
        "learning_rate": 5.251551494039155e-05,
        "epoch": 0.4412,
        "step": 3309
    },
    {
        "loss": 2.7085,
        "grad_norm": 3.4725799560546875,
        "learning_rate": 5.246013563485563e-05,
        "epoch": 0.44133333333333336,
        "step": 3310
    },
    {
        "loss": 2.0021,
        "grad_norm": 1.813523769378662,
        "learning_rate": 5.240477516002198e-05,
        "epoch": 0.4414666666666667,
        "step": 3311
    },
    {
        "loss": 1.0622,
        "grad_norm": 3.4725003242492676,
        "learning_rate": 5.2349433537819024e-05,
        "epoch": 0.4416,
        "step": 3312
    },
    {
        "loss": 2.7914,
        "grad_norm": 2.5358574390411377,
        "learning_rate": 5.22941107901678e-05,
        "epoch": 0.4417333333333333,
        "step": 3313
    },
    {
        "loss": 2.7174,
        "grad_norm": 1.9199377298355103,
        "learning_rate": 5.22388069389819e-05,
        "epoch": 0.4418666666666667,
        "step": 3314
    },
    {
        "loss": 2.7084,
        "grad_norm": 3.5743260383605957,
        "learning_rate": 5.2183522006167274e-05,
        "epoch": 0.442,
        "step": 3315
    },
    {
        "loss": 3.0744,
        "grad_norm": 1.7437859773635864,
        "learning_rate": 5.21282560136225e-05,
        "epoch": 0.4421333333333333,
        "step": 3316
    },
    {
        "loss": 2.1635,
        "grad_norm": 3.4363105297088623,
        "learning_rate": 5.207300898323868e-05,
        "epoch": 0.44226666666666664,
        "step": 3317
    },
    {
        "loss": 2.6509,
        "grad_norm": 2.9589884281158447,
        "learning_rate": 5.201778093689926e-05,
        "epoch": 0.4424,
        "step": 3318
    },
    {
        "loss": 2.3118,
        "grad_norm": 3.008948564529419,
        "learning_rate": 5.196257189648037e-05,
        "epoch": 0.44253333333333333,
        "step": 3319
    },
    {
        "loss": 3.0785,
        "grad_norm": 1.9004930257797241,
        "learning_rate": 5.1907381883850425e-05,
        "epoch": 0.44266666666666665,
        "step": 3320
    },
    {
        "loss": 1.5679,
        "grad_norm": 1.9950929880142212,
        "learning_rate": 5.185221092087029e-05,
        "epoch": 0.4428,
        "step": 3321
    },
    {
        "loss": 2.2423,
        "grad_norm": 4.0620012283325195,
        "learning_rate": 5.179705902939358e-05,
        "epoch": 0.44293333333333335,
        "step": 3322
    },
    {
        "loss": 2.702,
        "grad_norm": 1.8945131301879883,
        "learning_rate": 5.1741926231265966e-05,
        "epoch": 0.44306666666666666,
        "step": 3323
    },
    {
        "loss": 1.7061,
        "grad_norm": 1.9295295476913452,
        "learning_rate": 5.168681254832586e-05,
        "epoch": 0.4432,
        "step": 3324
    },
    {
        "loss": 2.4706,
        "grad_norm": 2.309379816055298,
        "learning_rate": 5.163171800240385e-05,
        "epoch": 0.44333333333333336,
        "step": 3325
    },
    {
        "loss": 2.1747,
        "grad_norm": 2.655916690826416,
        "learning_rate": 5.1576642615323234e-05,
        "epoch": 0.4434666666666667,
        "step": 3326
    },
    {
        "loss": 2.5296,
        "grad_norm": 2.7604522705078125,
        "learning_rate": 5.152158640889947e-05,
        "epoch": 0.4436,
        "step": 3327
    },
    {
        "loss": 2.1402,
        "grad_norm": 3.6378602981567383,
        "learning_rate": 5.1466549404940465e-05,
        "epoch": 0.4437333333333333,
        "step": 3328
    },
    {
        "loss": 1.3778,
        "grad_norm": 3.602189302444458,
        "learning_rate": 5.141153162524668e-05,
        "epoch": 0.4438666666666667,
        "step": 3329
    },
    {
        "loss": 2.0588,
        "grad_norm": 3.609888792037964,
        "learning_rate": 5.1356533091610725e-05,
        "epoch": 0.444,
        "step": 3330
    },
    {
        "loss": 1.9069,
        "grad_norm": 2.2734928131103516,
        "learning_rate": 5.130155382581776e-05,
        "epoch": 0.4441333333333333,
        "step": 3331
    },
    {
        "loss": 2.6544,
        "grad_norm": 2.9999172687530518,
        "learning_rate": 5.12465938496453e-05,
        "epoch": 0.44426666666666664,
        "step": 3332
    },
    {
        "loss": 1.8181,
        "grad_norm": 2.701439380645752,
        "learning_rate": 5.1191653184863095e-05,
        "epoch": 0.4444,
        "step": 3333
    },
    {
        "loss": 3.1862,
        "grad_norm": 3.493952512741089,
        "learning_rate": 5.113673185323338e-05,
        "epoch": 0.44453333333333334,
        "step": 3334
    },
    {
        "loss": 2.5928,
        "grad_norm": 1.9673314094543457,
        "learning_rate": 5.108182987651071e-05,
        "epoch": 0.44466666666666665,
        "step": 3335
    },
    {
        "loss": 3.0503,
        "grad_norm": 2.9695723056793213,
        "learning_rate": 5.102694727644193e-05,
        "epoch": 0.4448,
        "step": 3336
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.493791341781616,
        "learning_rate": 5.0972084074766144e-05,
        "epoch": 0.44493333333333335,
        "step": 3337
    },
    {
        "loss": 1.7424,
        "grad_norm": 3.1129255294799805,
        "learning_rate": 5.091724029321492e-05,
        "epoch": 0.44506666666666667,
        "step": 3338
    },
    {
        "loss": 0.9262,
        "grad_norm": 3.5146872997283936,
        "learning_rate": 5.08624159535121e-05,
        "epoch": 0.4452,
        "step": 3339
    },
    {
        "loss": 1.8834,
        "grad_norm": 2.0671467781066895,
        "learning_rate": 5.080761107737372e-05,
        "epoch": 0.44533333333333336,
        "step": 3340
    },
    {
        "loss": 1.4271,
        "grad_norm": 4.758121490478516,
        "learning_rate": 5.075282568650821e-05,
        "epoch": 0.4454666666666667,
        "step": 3341
    },
    {
        "loss": 2.7175,
        "grad_norm": 2.424668312072754,
        "learning_rate": 5.0698059802616284e-05,
        "epoch": 0.4456,
        "step": 3342
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.4037113189697266,
        "learning_rate": 5.0643313447390815e-05,
        "epoch": 0.4457333333333333,
        "step": 3343
    },
    {
        "loss": 2.5423,
        "grad_norm": 3.4719951152801514,
        "learning_rate": 5.0588586642517087e-05,
        "epoch": 0.4458666666666667,
        "step": 3344
    },
    {
        "loss": 2.2074,
        "grad_norm": 3.376070737838745,
        "learning_rate": 5.053387940967259e-05,
        "epoch": 0.446,
        "step": 3345
    },
    {
        "loss": 2.1346,
        "grad_norm": 4.379795551300049,
        "learning_rate": 5.0479191770527e-05,
        "epoch": 0.4461333333333333,
        "step": 3346
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.5212154388427734,
        "learning_rate": 5.042452374674223e-05,
        "epoch": 0.44626666666666664,
        "step": 3347
    },
    {
        "loss": 2.765,
        "grad_norm": 3.808413505554199,
        "learning_rate": 5.036987535997258e-05,
        "epoch": 0.4464,
        "step": 3348
    },
    {
        "loss": 2.5929,
        "grad_norm": 2.730242967605591,
        "learning_rate": 5.0315246631864435e-05,
        "epoch": 0.44653333333333334,
        "step": 3349
    },
    {
        "loss": 2.5068,
        "grad_norm": 2.7980785369873047,
        "learning_rate": 5.02606375840563e-05,
        "epoch": 0.44666666666666666,
        "step": 3350
    },
    {
        "loss": 1.7433,
        "grad_norm": 3.08127498626709,
        "learning_rate": 5.0206048238179185e-05,
        "epoch": 0.4468,
        "step": 3351
    },
    {
        "loss": 1.662,
        "grad_norm": 2.9033966064453125,
        "learning_rate": 5.015147861585603e-05,
        "epoch": 0.44693333333333335,
        "step": 3352
    },
    {
        "loss": 2.8527,
        "grad_norm": 2.4850189685821533,
        "learning_rate": 5.009692873870202e-05,
        "epoch": 0.44706666666666667,
        "step": 3353
    },
    {
        "loss": 2.4412,
        "grad_norm": 2.904114246368408,
        "learning_rate": 5.0042398628324604e-05,
        "epoch": 0.4472,
        "step": 3354
    },
    {
        "loss": 2.446,
        "grad_norm": 2.990142583847046,
        "learning_rate": 4.998788830632328e-05,
        "epoch": 0.44733333333333336,
        "step": 3355
    },
    {
        "loss": 1.6274,
        "grad_norm": 4.402720928192139,
        "learning_rate": 4.9933397794289815e-05,
        "epoch": 0.4474666666666667,
        "step": 3356
    },
    {
        "loss": 1.469,
        "grad_norm": 2.481234550476074,
        "learning_rate": 4.987892711380813e-05,
        "epoch": 0.4476,
        "step": 3357
    },
    {
        "loss": 1.4773,
        "grad_norm": 3.4151833057403564,
        "learning_rate": 4.982447628645415e-05,
        "epoch": 0.4477333333333333,
        "step": 3358
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.4044299125671387,
        "learning_rate": 4.977004533379608e-05,
        "epoch": 0.4478666666666667,
        "step": 3359
    },
    {
        "loss": 1.6065,
        "grad_norm": 4.692849636077881,
        "learning_rate": 4.971563427739424e-05,
        "epoch": 0.448,
        "step": 3360
    },
    {
        "loss": 2.798,
        "grad_norm": 3.072887897491455,
        "learning_rate": 4.966124313880101e-05,
        "epoch": 0.44813333333333333,
        "step": 3361
    },
    {
        "loss": 2.298,
        "grad_norm": 2.913698673248291,
        "learning_rate": 4.960687193956086e-05,
        "epoch": 0.44826666666666665,
        "step": 3362
    },
    {
        "loss": 2.8404,
        "grad_norm": 2.182968854904175,
        "learning_rate": 4.955252070121045e-05,
        "epoch": 0.4484,
        "step": 3363
    },
    {
        "loss": 2.374,
        "grad_norm": 2.974459409713745,
        "learning_rate": 4.9498189445278474e-05,
        "epoch": 0.44853333333333334,
        "step": 3364
    },
    {
        "loss": 1.8736,
        "grad_norm": 3.04258394241333,
        "learning_rate": 4.9443878193285786e-05,
        "epoch": 0.44866666666666666,
        "step": 3365
    },
    {
        "loss": 1.1514,
        "grad_norm": 5.704634666442871,
        "learning_rate": 4.938958696674517e-05,
        "epoch": 0.4488,
        "step": 3366
    },
    {
        "loss": 2.1595,
        "grad_norm": 2.3443026542663574,
        "learning_rate": 4.9335315787161605e-05,
        "epoch": 0.44893333333333335,
        "step": 3367
    },
    {
        "loss": 1.6938,
        "grad_norm": 4.930117130279541,
        "learning_rate": 4.9281064676032126e-05,
        "epoch": 0.44906666666666667,
        "step": 3368
    },
    {
        "loss": 1.7269,
        "grad_norm": 2.9155633449554443,
        "learning_rate": 4.922683365484573e-05,
        "epoch": 0.4492,
        "step": 3369
    },
    {
        "loss": 2.3983,
        "grad_norm": 3.515254497528076,
        "learning_rate": 4.917262274508355e-05,
        "epoch": 0.4493333333333333,
        "step": 3370
    },
    {
        "loss": 1.4439,
        "grad_norm": Infinity,
        "learning_rate": 4.917262274508355e-05,
        "epoch": 0.4494666666666667,
        "step": 3371
    },
    {
        "loss": 0.9606,
        "grad_norm": 3.57607102394104,
        "learning_rate": 4.9118431968218695e-05,
        "epoch": 0.4496,
        "step": 3372
    },
    {
        "loss": 2.9332,
        "grad_norm": 2.188748359680176,
        "learning_rate": 4.906426134571624e-05,
        "epoch": 0.4497333333333333,
        "step": 3373
    },
    {
        "loss": 1.5437,
        "grad_norm": 4.945751667022705,
        "learning_rate": 4.90101108990335e-05,
        "epoch": 0.4498666666666667,
        "step": 3374
    },
    {
        "loss": 2.6184,
        "grad_norm": 3.4957997798919678,
        "learning_rate": 4.895598064961957e-05,
        "epoch": 0.45,
        "step": 3375
    },
    {
        "loss": 2.4399,
        "grad_norm": 2.8816819190979004,
        "learning_rate": 4.890187061891557e-05,
        "epoch": 0.45013333333333333,
        "step": 3376
    },
    {
        "loss": 2.2192,
        "grad_norm": 3.71260666847229,
        "learning_rate": 4.8847780828354795e-05,
        "epoch": 0.45026666666666665,
        "step": 3377
    },
    {
        "loss": 1.4288,
        "grad_norm": 3.185359239578247,
        "learning_rate": 4.879371129936233e-05,
        "epoch": 0.4504,
        "step": 3378
    },
    {
        "loss": 1.8292,
        "grad_norm": 5.003437519073486,
        "learning_rate": 4.8739662053355276e-05,
        "epoch": 0.45053333333333334,
        "step": 3379
    },
    {
        "loss": 2.5206,
        "grad_norm": 2.263467788696289,
        "learning_rate": 4.8685633111742765e-05,
        "epoch": 0.45066666666666666,
        "step": 3380
    },
    {
        "loss": 1.5655,
        "grad_norm": 4.6318488121032715,
        "learning_rate": 4.8631624495925806e-05,
        "epoch": 0.4508,
        "step": 3381
    },
    {
        "loss": 2.4575,
        "grad_norm": 3.4274842739105225,
        "learning_rate": 4.857763622729741e-05,
        "epoch": 0.45093333333333335,
        "step": 3382
    },
    {
        "loss": 2.8289,
        "grad_norm": 2.874101161956787,
        "learning_rate": 4.852366832724256e-05,
        "epoch": 0.45106666666666667,
        "step": 3383
    },
    {
        "loss": 0.5231,
        "grad_norm": 3.992429256439209,
        "learning_rate": 4.8469720817138054e-05,
        "epoch": 0.4512,
        "step": 3384
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.289255142211914,
        "learning_rate": 4.841579371835272e-05,
        "epoch": 0.4513333333333333,
        "step": 3385
    },
    {
        "loss": 2.5898,
        "grad_norm": 3.3594043254852295,
        "learning_rate": 4.8361887052247255e-05,
        "epoch": 0.4514666666666667,
        "step": 3386
    },
    {
        "loss": 2.121,
        "grad_norm": 3.2669665813446045,
        "learning_rate": 4.830800084017433e-05,
        "epoch": 0.4516,
        "step": 3387
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.582369327545166,
        "learning_rate": 4.8254135103478395e-05,
        "epoch": 0.4517333333333333,
        "step": 3388
    },
    {
        "loss": 2.4949,
        "grad_norm": 2.2024316787719727,
        "learning_rate": 4.820028986349583e-05,
        "epoch": 0.4518666666666667,
        "step": 3389
    },
    {
        "loss": 1.6624,
        "grad_norm": 1.904914379119873,
        "learning_rate": 4.814646514155496e-05,
        "epoch": 0.452,
        "step": 3390
    },
    {
        "loss": 2.2954,
        "grad_norm": 2.4132986068725586,
        "learning_rate": 4.8092660958975966e-05,
        "epoch": 0.45213333333333333,
        "step": 3391
    },
    {
        "loss": 2.1345,
        "grad_norm": 3.5290536880493164,
        "learning_rate": 4.80388773370708e-05,
        "epoch": 0.45226666666666665,
        "step": 3392
    },
    {
        "loss": 2.1733,
        "grad_norm": 3.335028648376465,
        "learning_rate": 4.7985114297143373e-05,
        "epoch": 0.4524,
        "step": 3393
    },
    {
        "loss": 2.2362,
        "grad_norm": 3.7122275829315186,
        "learning_rate": 4.793137186048945e-05,
        "epoch": 0.45253333333333334,
        "step": 3394
    },
    {
        "loss": 1.9422,
        "grad_norm": 4.500114917755127,
        "learning_rate": 4.7877650048396516e-05,
        "epoch": 0.45266666666666666,
        "step": 3395
    },
    {
        "loss": 2.3084,
        "grad_norm": 2.680565118789673,
        "learning_rate": 4.7823948882144034e-05,
        "epoch": 0.4528,
        "step": 3396
    },
    {
        "loss": 2.5241,
        "grad_norm": 2.615154266357422,
        "learning_rate": 4.777026838300319e-05,
        "epoch": 0.45293333333333335,
        "step": 3397
    },
    {
        "loss": 2.2343,
        "grad_norm": 3.759591579437256,
        "learning_rate": 4.771660857223692e-05,
        "epoch": 0.4530666666666667,
        "step": 3398
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.0552315711975098,
        "learning_rate": 4.766296947110024e-05,
        "epoch": 0.4532,
        "step": 3399
    },
    {
        "loss": 1.7359,
        "grad_norm": 3.9166574478149414,
        "learning_rate": 4.760935110083967e-05,
        "epoch": 0.4533333333333333,
        "step": 3400
    },
    {
        "loss": 2.2558,
        "grad_norm": 3.043189525604248,
        "learning_rate": 4.755575348269362e-05,
        "epoch": 0.4534666666666667,
        "step": 3401
    },
    {
        "loss": 1.9776,
        "grad_norm": 3.753923177719116,
        "learning_rate": 4.75021766378923e-05,
        "epoch": 0.4536,
        "step": 3402
    },
    {
        "loss": 2.2086,
        "grad_norm": 2.503652572631836,
        "learning_rate": 4.744862058765776e-05,
        "epoch": 0.4537333333333333,
        "step": 3403
    },
    {
        "loss": 1.8726,
        "grad_norm": 3.414111852645874,
        "learning_rate": 4.7395085353203607e-05,
        "epoch": 0.45386666666666664,
        "step": 3404
    },
    {
        "loss": 2.7936,
        "grad_norm": 3.7283267974853516,
        "learning_rate": 4.73415709557354e-05,
        "epoch": 0.454,
        "step": 3405
    },
    {
        "loss": 2.5411,
        "grad_norm": 3.187960624694824,
        "learning_rate": 4.7288077416450405e-05,
        "epoch": 0.45413333333333333,
        "step": 3406
    },
    {
        "loss": 2.5466,
        "grad_norm": 2.7991554737091064,
        "learning_rate": 4.7234604756537513e-05,
        "epoch": 0.45426666666666665,
        "step": 3407
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.4064443111419678,
        "learning_rate": 4.718115299717748e-05,
        "epoch": 0.4544,
        "step": 3408
    },
    {
        "loss": 2.9655,
        "grad_norm": 2.6574761867523193,
        "learning_rate": 4.7127722159542755e-05,
        "epoch": 0.45453333333333334,
        "step": 3409
    },
    {
        "loss": 2.2748,
        "grad_norm": 3.541818141937256,
        "learning_rate": 4.7074312264797404e-05,
        "epoch": 0.45466666666666666,
        "step": 3410
    },
    {
        "loss": 2.9475,
        "grad_norm": 2.2903687953948975,
        "learning_rate": 4.70209233340973e-05,
        "epoch": 0.4548,
        "step": 3411
    },
    {
        "loss": 1.3924,
        "grad_norm": 2.3341293334960938,
        "learning_rate": 4.696755538859004e-05,
        "epoch": 0.45493333333333336,
        "step": 3412
    },
    {
        "loss": 1.9476,
        "grad_norm": 2.315185785293579,
        "learning_rate": 4.691420844941478e-05,
        "epoch": 0.4550666666666667,
        "step": 3413
    },
    {
        "loss": 1.5,
        "grad_norm": 2.8841140270233154,
        "learning_rate": 4.686088253770239e-05,
        "epoch": 0.4552,
        "step": 3414
    },
    {
        "loss": 2.4992,
        "grad_norm": 3.100562334060669,
        "learning_rate": 4.680757767457551e-05,
        "epoch": 0.4553333333333333,
        "step": 3415
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.5746285915374756,
        "learning_rate": 4.675429388114839e-05,
        "epoch": 0.4554666666666667,
        "step": 3416
    },
    {
        "loss": 1.3032,
        "grad_norm": 2.205512762069702,
        "learning_rate": 4.670103117852687e-05,
        "epoch": 0.4556,
        "step": 3417
    },
    {
        "loss": 2.6454,
        "grad_norm": 2.4730703830718994,
        "learning_rate": 4.6647789587808496e-05,
        "epoch": 0.4557333333333333,
        "step": 3418
    },
    {
        "loss": 1.7748,
        "grad_norm": 5.237359046936035,
        "learning_rate": 4.6594569130082514e-05,
        "epoch": 0.45586666666666664,
        "step": 3419
    },
    {
        "loss": 2.4519,
        "grad_norm": 2.197585344314575,
        "learning_rate": 4.654136982642961e-05,
        "epoch": 0.456,
        "step": 3420
    },
    {
        "loss": 1.902,
        "grad_norm": 2.3460614681243896,
        "learning_rate": 4.648819169792233e-05,
        "epoch": 0.45613333333333334,
        "step": 3421
    },
    {
        "loss": 2.5554,
        "grad_norm": 2.3145973682403564,
        "learning_rate": 4.643503476562466e-05,
        "epoch": 0.45626666666666665,
        "step": 3422
    },
    {
        "loss": 2.3446,
        "grad_norm": 4.027488708496094,
        "learning_rate": 4.638189905059216e-05,
        "epoch": 0.4564,
        "step": 3423
    },
    {
        "loss": 2.6934,
        "grad_norm": 3.352579355239868,
        "learning_rate": 4.632878457387221e-05,
        "epoch": 0.45653333333333335,
        "step": 3424
    },
    {
        "loss": 2.1418,
        "grad_norm": 3.184242010116577,
        "learning_rate": 4.627569135650355e-05,
        "epoch": 0.45666666666666667,
        "step": 3425
    },
    {
        "loss": 2.7122,
        "grad_norm": 2.3626461029052734,
        "learning_rate": 4.622261941951662e-05,
        "epoch": 0.4568,
        "step": 3426
    },
    {
        "loss": 2.4566,
        "grad_norm": 2.627804756164551,
        "learning_rate": 4.616956878393333e-05,
        "epoch": 0.45693333333333336,
        "step": 3427
    },
    {
        "loss": 2.0259,
        "grad_norm": 3.9210548400878906,
        "learning_rate": 4.611653947076732e-05,
        "epoch": 0.4570666666666667,
        "step": 3428
    },
    {
        "loss": 2.3291,
        "grad_norm": 3.1441736221313477,
        "learning_rate": 4.6063531501023636e-05,
        "epoch": 0.4572,
        "step": 3429
    },
    {
        "loss": 1.3967,
        "grad_norm": 4.3378143310546875,
        "learning_rate": 4.601054489569887e-05,
        "epoch": 0.4573333333333333,
        "step": 3430
    },
    {
        "loss": 1.732,
        "grad_norm": 1.9431781768798828,
        "learning_rate": 4.595757967578128e-05,
        "epoch": 0.4574666666666667,
        "step": 3431
    },
    {
        "loss": 2.8388,
        "grad_norm": 3.2101473808288574,
        "learning_rate": 4.590463586225048e-05,
        "epoch": 0.4576,
        "step": 3432
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.871971607208252,
        "learning_rate": 4.585171347607775e-05,
        "epoch": 0.4577333333333333,
        "step": 3433
    },
    {
        "loss": 2.6429,
        "grad_norm": 2.5892395973205566,
        "learning_rate": 4.579881253822584e-05,
        "epoch": 0.45786666666666664,
        "step": 3434
    },
    {
        "loss": 2.1457,
        "grad_norm": 3.1804771423339844,
        "learning_rate": 4.5745933069648946e-05,
        "epoch": 0.458,
        "step": 3435
    },
    {
        "loss": 2.8359,
        "grad_norm": 2.812594175338745,
        "learning_rate": 4.569307509129283e-05,
        "epoch": 0.45813333333333334,
        "step": 3436
    },
    {
        "loss": 2.2923,
        "grad_norm": 2.7374277114868164,
        "learning_rate": 4.5640238624094744e-05,
        "epoch": 0.45826666666666666,
        "step": 3437
    },
    {
        "loss": 2.5421,
        "grad_norm": 1.8115209341049194,
        "learning_rate": 4.558742368898337e-05,
        "epoch": 0.4584,
        "step": 3438
    },
    {
        "loss": 2.1558,
        "grad_norm": 4.090463161468506,
        "learning_rate": 4.553463030687884e-05,
        "epoch": 0.45853333333333335,
        "step": 3439
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.2974772453308105,
        "learning_rate": 4.548185849869283e-05,
        "epoch": 0.45866666666666667,
        "step": 3440
    },
    {
        "loss": 2.3012,
        "grad_norm": 3.51633882522583,
        "learning_rate": 4.542910828532848e-05,
        "epoch": 0.4588,
        "step": 3441
    },
    {
        "loss": 2.7701,
        "grad_norm": 3.303342819213867,
        "learning_rate": 4.5376379687680236e-05,
        "epoch": 0.45893333333333336,
        "step": 3442
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.9534424543380737,
        "learning_rate": 4.532367272663414e-05,
        "epoch": 0.4590666666666667,
        "step": 3443
    },
    {
        "loss": 2.4176,
        "grad_norm": 3.1375656127929688,
        "learning_rate": 4.52709874230676e-05,
        "epoch": 0.4592,
        "step": 3444
    },
    {
        "loss": 3.2016,
        "grad_norm": 3.305790662765503,
        "learning_rate": 4.5218323797849407e-05,
        "epoch": 0.4593333333333333,
        "step": 3445
    },
    {
        "loss": 1.1122,
        "grad_norm": 3.261629819869995,
        "learning_rate": 4.5165681871839806e-05,
        "epoch": 0.4594666666666667,
        "step": 3446
    },
    {
        "loss": 2.0589,
        "grad_norm": 3.726177930831909,
        "learning_rate": 4.51130616658905e-05,
        "epoch": 0.4596,
        "step": 3447
    },
    {
        "loss": 2.3859,
        "grad_norm": 3.1193463802337646,
        "learning_rate": 4.5060463200844494e-05,
        "epoch": 0.4597333333333333,
        "step": 3448
    },
    {
        "loss": 2.6707,
        "grad_norm": 2.7116305828094482,
        "learning_rate": 4.500788649753614e-05,
        "epoch": 0.45986666666666665,
        "step": 3449
    },
    {
        "loss": 1.7212,
        "grad_norm": 3.2026453018188477,
        "learning_rate": 4.4955331576791406e-05,
        "epoch": 0.46,
        "step": 3450
    },
    {
        "loss": 2.6849,
        "grad_norm": 2.3018343448638916,
        "learning_rate": 4.4902798459427385e-05,
        "epoch": 0.46013333333333334,
        "step": 3451
    },
    {
        "loss": 1.972,
        "grad_norm": 3.8581438064575195,
        "learning_rate": 4.485028716625257e-05,
        "epoch": 0.46026666666666666,
        "step": 3452
    },
    {
        "loss": 2.7158,
        "grad_norm": 3.0580103397369385,
        "learning_rate": 4.479779771806699e-05,
        "epoch": 0.4604,
        "step": 3453
    },
    {
        "loss": 2.0297,
        "grad_norm": 3.175107002258301,
        "learning_rate": 4.4745330135661836e-05,
        "epoch": 0.46053333333333335,
        "step": 3454
    },
    {
        "loss": 1.967,
        "grad_norm": 3.9709296226501465,
        "learning_rate": 4.469288443981965e-05,
        "epoch": 0.46066666666666667,
        "step": 3455
    },
    {
        "loss": 2.3158,
        "grad_norm": 3.0295302867889404,
        "learning_rate": 4.464046065131443e-05,
        "epoch": 0.4608,
        "step": 3456
    },
    {
        "loss": 2.488,
        "grad_norm": 2.7759227752685547,
        "learning_rate": 4.458805879091135e-05,
        "epoch": 0.4609333333333333,
        "step": 3457
    },
    {
        "loss": 2.761,
        "grad_norm": 3.2728288173675537,
        "learning_rate": 4.453567887936698e-05,
        "epoch": 0.4610666666666667,
        "step": 3458
    },
    {
        "loss": 0.7382,
        "grad_norm": 2.0510199069976807,
        "learning_rate": 4.448332093742924e-05,
        "epoch": 0.4612,
        "step": 3459
    },
    {
        "loss": 2.7373,
        "grad_norm": 3.2908029556274414,
        "learning_rate": 4.443098498583721e-05,
        "epoch": 0.4613333333333333,
        "step": 3460
    },
    {
        "loss": 2.3057,
        "grad_norm": 2.1937203407287598,
        "learning_rate": 4.4378671045321375e-05,
        "epoch": 0.4614666666666667,
        "step": 3461
    },
    {
        "loss": 1.2991,
        "grad_norm": 3.1587512493133545,
        "learning_rate": 4.43263791366035e-05,
        "epoch": 0.4616,
        "step": 3462
    },
    {
        "loss": 2.557,
        "grad_norm": 2.690807580947876,
        "learning_rate": 4.427410928039655e-05,
        "epoch": 0.46173333333333333,
        "step": 3463
    },
    {
        "loss": 2.034,
        "grad_norm": 2.6259677410125732,
        "learning_rate": 4.422186149740476e-05,
        "epoch": 0.46186666666666665,
        "step": 3464
    },
    {
        "loss": 2.0158,
        "grad_norm": 4.492316722869873,
        "learning_rate": 4.4169635808323684e-05,
        "epoch": 0.462,
        "step": 3465
    },
    {
        "loss": 1.7425,
        "grad_norm": 3.5268044471740723,
        "learning_rate": 4.411743223384011e-05,
        "epoch": 0.46213333333333334,
        "step": 3466
    },
    {
        "loss": 2.6236,
        "grad_norm": 2.2659013271331787,
        "learning_rate": 4.406525079463208e-05,
        "epoch": 0.46226666666666666,
        "step": 3467
    },
    {
        "loss": 2.6152,
        "grad_norm": 2.3832156658172607,
        "learning_rate": 4.401309151136876e-05,
        "epoch": 0.4624,
        "step": 3468
    },
    {
        "loss": 2.2685,
        "grad_norm": 2.6923890113830566,
        "learning_rate": 4.3960954404710664e-05,
        "epoch": 0.46253333333333335,
        "step": 3469
    },
    {
        "loss": 2.4805,
        "grad_norm": 3.653905153274536,
        "learning_rate": 4.390883949530952e-05,
        "epoch": 0.46266666666666667,
        "step": 3470
    },
    {
        "loss": 2.4963,
        "grad_norm": 3.6335768699645996,
        "learning_rate": 4.385674680380813e-05,
        "epoch": 0.4628,
        "step": 3471
    },
    {
        "loss": 2.4717,
        "grad_norm": 4.531276702880859,
        "learning_rate": 4.380467635084069e-05,
        "epoch": 0.4629333333333333,
        "step": 3472
    },
    {
        "loss": 2.6404,
        "grad_norm": 3.6657724380493164,
        "learning_rate": 4.3752628157032416e-05,
        "epoch": 0.4630666666666667,
        "step": 3473
    },
    {
        "loss": 2.9284,
        "grad_norm": 2.6910574436187744,
        "learning_rate": 4.370060224299971e-05,
        "epoch": 0.4632,
        "step": 3474
    },
    {
        "loss": 2.4887,
        "grad_norm": 4.439059257507324,
        "learning_rate": 4.364859862935037e-05,
        "epoch": 0.4633333333333333,
        "step": 3475
    },
    {
        "loss": 1.0021,
        "grad_norm": 3.5838394165039062,
        "learning_rate": 4.359661733668312e-05,
        "epoch": 0.4634666666666667,
        "step": 3476
    },
    {
        "loss": 3.0472,
        "grad_norm": 3.053035259246826,
        "learning_rate": 4.3544658385587865e-05,
        "epoch": 0.4636,
        "step": 3477
    },
    {
        "loss": 3.2117,
        "grad_norm": 5.717822074890137,
        "learning_rate": 4.3492721796645866e-05,
        "epoch": 0.46373333333333333,
        "step": 3478
    },
    {
        "loss": 1.8431,
        "grad_norm": 3.3951547145843506,
        "learning_rate": 4.34408075904293e-05,
        "epoch": 0.46386666666666665,
        "step": 3479
    },
    {
        "loss": 2.7028,
        "grad_norm": 2.720961570739746,
        "learning_rate": 4.3388915787501535e-05,
        "epoch": 0.464,
        "step": 3480
    },
    {
        "loss": 2.3858,
        "grad_norm": 2.2098822593688965,
        "learning_rate": 4.333704640841716e-05,
        "epoch": 0.46413333333333334,
        "step": 3481
    },
    {
        "loss": 2.2233,
        "grad_norm": 3.4970500469207764,
        "learning_rate": 4.328519947372175e-05,
        "epoch": 0.46426666666666666,
        "step": 3482
    },
    {
        "loss": 1.6984,
        "grad_norm": 2.4472720623016357,
        "learning_rate": 4.323337500395207e-05,
        "epoch": 0.4644,
        "step": 3483
    },
    {
        "loss": 2.6288,
        "grad_norm": 2.3895132541656494,
        "learning_rate": 4.318157301963601e-05,
        "epoch": 0.46453333333333335,
        "step": 3484
    },
    {
        "loss": 1.9062,
        "grad_norm": 2.7971882820129395,
        "learning_rate": 4.312979354129245e-05,
        "epoch": 0.4646666666666667,
        "step": 3485
    },
    {
        "loss": 1.9809,
        "grad_norm": 3.0785882472991943,
        "learning_rate": 4.3078036589431446e-05,
        "epoch": 0.4648,
        "step": 3486
    },
    {
        "loss": 1.7529,
        "grad_norm": 6.060249328613281,
        "learning_rate": 4.302630218455411e-05,
        "epoch": 0.4649333333333333,
        "step": 3487
    },
    {
        "loss": 1.2204,
        "grad_norm": 4.471144676208496,
        "learning_rate": 4.297459034715265e-05,
        "epoch": 0.4650666666666667,
        "step": 3488
    },
    {
        "loss": 2.1725,
        "grad_norm": 3.7395362854003906,
        "learning_rate": 4.292290109771027e-05,
        "epoch": 0.4652,
        "step": 3489
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.4553794860839844,
        "learning_rate": 4.2871234456701195e-05,
        "epoch": 0.4653333333333333,
        "step": 3490
    },
    {
        "loss": 1.3423,
        "grad_norm": 1.8998053073883057,
        "learning_rate": 4.281959044459082e-05,
        "epoch": 0.46546666666666664,
        "step": 3491
    },
    {
        "loss": 1.5266,
        "grad_norm": 3.298062324523926,
        "learning_rate": 4.276796908183555e-05,
        "epoch": 0.4656,
        "step": 3492
    },
    {
        "loss": 1.6707,
        "grad_norm": 3.4127700328826904,
        "learning_rate": 4.27163703888827e-05,
        "epoch": 0.46573333333333333,
        "step": 3493
    },
    {
        "loss": 2.2193,
        "grad_norm": 2.6379592418670654,
        "learning_rate": 4.2664794386170725e-05,
        "epoch": 0.46586666666666665,
        "step": 3494
    },
    {
        "loss": 2.2825,
        "grad_norm": 2.9413795471191406,
        "learning_rate": 4.26132410941291e-05,
        "epoch": 0.466,
        "step": 3495
    },
    {
        "loss": 2.822,
        "grad_norm": 3.604891538619995,
        "learning_rate": 4.256171053317818e-05,
        "epoch": 0.46613333333333334,
        "step": 3496
    },
    {
        "loss": 2.8489,
        "grad_norm": 2.973536968231201,
        "learning_rate": 4.2510202723729464e-05,
        "epoch": 0.46626666666666666,
        "step": 3497
    },
    {
        "loss": 2.4855,
        "grad_norm": 2.3297152519226074,
        "learning_rate": 4.245871768618535e-05,
        "epoch": 0.4664,
        "step": 3498
    },
    {
        "loss": 2.4084,
        "grad_norm": 1.5613428354263306,
        "learning_rate": 4.2407255440939155e-05,
        "epoch": 0.46653333333333336,
        "step": 3499
    },
    {
        "loss": 1.0825,
        "grad_norm": 3.9676690101623535,
        "learning_rate": 4.235581600837539e-05,
        "epoch": 0.4666666666666667,
        "step": 3500
    },
    {
        "loss": 1.4574,
        "grad_norm": 3.3069849014282227,
        "learning_rate": 4.230439940886932e-05,
        "epoch": 0.4668,
        "step": 3501
    },
    {
        "loss": 2.1542,
        "grad_norm": 3.9628827571868896,
        "learning_rate": 4.2253005662787205e-05,
        "epoch": 0.4669333333333333,
        "step": 3502
    },
    {
        "loss": 2.2866,
        "grad_norm": 3.833124876022339,
        "learning_rate": 4.2201634790486314e-05,
        "epoch": 0.4670666666666667,
        "step": 3503
    },
    {
        "loss": 2.4243,
        "grad_norm": 2.503087282180786,
        "learning_rate": 4.2150286812314866e-05,
        "epoch": 0.4672,
        "step": 3504
    },
    {
        "loss": 2.1594,
        "grad_norm": 3.5697906017303467,
        "learning_rate": 4.2098961748611884e-05,
        "epoch": 0.4673333333333333,
        "step": 3505
    },
    {
        "loss": 2.4714,
        "grad_norm": 4.328397274017334,
        "learning_rate": 4.204765961970746e-05,
        "epoch": 0.46746666666666664,
        "step": 3506
    },
    {
        "loss": 2.3089,
        "grad_norm": 3.277798891067505,
        "learning_rate": 4.199638044592256e-05,
        "epoch": 0.4676,
        "step": 3507
    },
    {
        "loss": 2.8147,
        "grad_norm": 2.296182632446289,
        "learning_rate": 4.194512424756896e-05,
        "epoch": 0.46773333333333333,
        "step": 3508
    },
    {
        "loss": 2.725,
        "grad_norm": 3.3881068229675293,
        "learning_rate": 4.1893891044949475e-05,
        "epoch": 0.46786666666666665,
        "step": 3509
    },
    {
        "loss": 2.2627,
        "grad_norm": 3.427957773208618,
        "learning_rate": 4.184268085835776e-05,
        "epoch": 0.468,
        "step": 3510
    },
    {
        "loss": 2.2374,
        "grad_norm": 2.6155614852905273,
        "learning_rate": 4.17914937080783e-05,
        "epoch": 0.46813333333333335,
        "step": 3511
    },
    {
        "loss": 2.514,
        "grad_norm": 3.322444200515747,
        "learning_rate": 4.174032961438653e-05,
        "epoch": 0.46826666666666666,
        "step": 3512
    },
    {
        "loss": 2.346,
        "grad_norm": 3.152029275894165,
        "learning_rate": 4.168918859754873e-05,
        "epoch": 0.4684,
        "step": 3513
    },
    {
        "loss": 2.3208,
        "grad_norm": 2.3426554203033447,
        "learning_rate": 4.163807067782204e-05,
        "epoch": 0.46853333333333336,
        "step": 3514
    },
    {
        "loss": 2.1687,
        "grad_norm": 3.0965185165405273,
        "learning_rate": 4.158697587545436e-05,
        "epoch": 0.4686666666666667,
        "step": 3515
    },
    {
        "loss": 2.2155,
        "grad_norm": 3.4302144050598145,
        "learning_rate": 4.1535904210684586e-05,
        "epoch": 0.4688,
        "step": 3516
    },
    {
        "loss": 1.9863,
        "grad_norm": 3.9185633659362793,
        "learning_rate": 4.14848557037424e-05,
        "epoch": 0.4689333333333333,
        "step": 3517
    },
    {
        "loss": 2.0768,
        "grad_norm": 2.6113979816436768,
        "learning_rate": 4.1433830374848215e-05,
        "epoch": 0.4690666666666667,
        "step": 3518
    },
    {
        "loss": 3.016,
        "grad_norm": 2.7064788341522217,
        "learning_rate": 4.138282824421338e-05,
        "epoch": 0.4692,
        "step": 3519
    },
    {
        "loss": 2.4177,
        "grad_norm": 3.239514112472534,
        "learning_rate": 4.133184933204005e-05,
        "epoch": 0.4693333333333333,
        "step": 3520
    },
    {
        "loss": 1.7097,
        "grad_norm": 4.245858192443848,
        "learning_rate": 4.128089365852107e-05,
        "epoch": 0.46946666666666664,
        "step": 3521
    },
    {
        "loss": 2.217,
        "grad_norm": 4.366470813751221,
        "learning_rate": 4.122996124384022e-05,
        "epoch": 0.4696,
        "step": 3522
    },
    {
        "loss": 2.0979,
        "grad_norm": 1.9740068912506104,
        "learning_rate": 4.117905210817198e-05,
        "epoch": 0.46973333333333334,
        "step": 3523
    },
    {
        "loss": 2.775,
        "grad_norm": 3.3338747024536133,
        "learning_rate": 4.112816627168156e-05,
        "epoch": 0.46986666666666665,
        "step": 3524
    },
    {
        "loss": 2.2491,
        "grad_norm": 2.63529372215271,
        "learning_rate": 4.107730375452511e-05,
        "epoch": 0.47,
        "step": 3525
    },
    {
        "loss": 1.5944,
        "grad_norm": 5.913146018981934,
        "learning_rate": 4.102646457684945e-05,
        "epoch": 0.47013333333333335,
        "step": 3526
    },
    {
        "loss": 2.5859,
        "grad_norm": 2.924013614654541,
        "learning_rate": 4.097564875879213e-05,
        "epoch": 0.47026666666666667,
        "step": 3527
    },
    {
        "loss": 2.7708,
        "grad_norm": 3.123865842819214,
        "learning_rate": 4.092485632048138e-05,
        "epoch": 0.4704,
        "step": 3528
    },
    {
        "loss": 1.0803,
        "grad_norm": 2.6310462951660156,
        "learning_rate": 4.08740872820364e-05,
        "epoch": 0.47053333333333336,
        "step": 3529
    },
    {
        "loss": 1.3896,
        "grad_norm": 4.09127950668335,
        "learning_rate": 4.082334166356693e-05,
        "epoch": 0.4706666666666667,
        "step": 3530
    },
    {
        "loss": 2.2776,
        "grad_norm": 3.1330156326293945,
        "learning_rate": 4.077261948517341e-05,
        "epoch": 0.4708,
        "step": 3531
    },
    {
        "loss": 1.5733,
        "grad_norm": 1.7437450885772705,
        "learning_rate": 4.072192076694718e-05,
        "epoch": 0.4709333333333333,
        "step": 3532
    },
    {
        "loss": 2.1398,
        "grad_norm": 2.474508047103882,
        "learning_rate": 4.067124552897007e-05,
        "epoch": 0.4710666666666667,
        "step": 3533
    },
    {
        "loss": 2.1604,
        "grad_norm": 3.827223539352417,
        "learning_rate": 4.062059379131478e-05,
        "epoch": 0.4712,
        "step": 3534
    },
    {
        "loss": 2.6394,
        "grad_norm": 1.9588143825531006,
        "learning_rate": 4.0569965574044634e-05,
        "epoch": 0.4713333333333333,
        "step": 3535
    },
    {
        "loss": 1.5274,
        "grad_norm": 2.4158694744110107,
        "learning_rate": 4.05193608972136e-05,
        "epoch": 0.47146666666666665,
        "step": 3536
    },
    {
        "loss": 2.1244,
        "grad_norm": 4.001989841461182,
        "learning_rate": 4.0468779780866396e-05,
        "epoch": 0.4716,
        "step": 3537
    },
    {
        "loss": 2.4351,
        "grad_norm": 2.216562271118164,
        "learning_rate": 4.04182222450384e-05,
        "epoch": 0.47173333333333334,
        "step": 3538
    },
    {
        "loss": 2.4244,
        "grad_norm": 3.375850200653076,
        "learning_rate": 4.036768830975559e-05,
        "epoch": 0.47186666666666666,
        "step": 3539
    },
    {
        "loss": 2.3822,
        "grad_norm": 2.0838353633880615,
        "learning_rate": 4.031717799503459e-05,
        "epoch": 0.472,
        "step": 3540
    },
    {
        "loss": 2.0531,
        "grad_norm": 3.7685465812683105,
        "learning_rate": 4.0266691320882766e-05,
        "epoch": 0.47213333333333335,
        "step": 3541
    },
    {
        "loss": 2.2011,
        "grad_norm": 2.525304079055786,
        "learning_rate": 4.021622830729808e-05,
        "epoch": 0.47226666666666667,
        "step": 3542
    },
    {
        "loss": 2.9806,
        "grad_norm": 3.3561182022094727,
        "learning_rate": 4.016578897426903e-05,
        "epoch": 0.4724,
        "step": 3543
    },
    {
        "loss": 1.6117,
        "grad_norm": 3.415018081665039,
        "learning_rate": 4.0115373341774845e-05,
        "epoch": 0.47253333333333336,
        "step": 3544
    },
    {
        "loss": 2.6929,
        "grad_norm": 3.075648307800293,
        "learning_rate": 4.006498142978536e-05,
        "epoch": 0.4726666666666667,
        "step": 3545
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.6802632808685303,
        "learning_rate": 4.001461325826094e-05,
        "epoch": 0.4728,
        "step": 3546
    },
    {
        "loss": 2.5534,
        "grad_norm": 3.8724119663238525,
        "learning_rate": 3.996426884715259e-05,
        "epoch": 0.4729333333333333,
        "step": 3547
    },
    {
        "loss": 2.1563,
        "grad_norm": 3.1243393421173096,
        "learning_rate": 3.9913948216401945e-05,
        "epoch": 0.4730666666666667,
        "step": 3548
    },
    {
        "loss": 2.8737,
        "grad_norm": 1.9324899911880493,
        "learning_rate": 3.986365138594116e-05,
        "epoch": 0.4732,
        "step": 3549
    },
    {
        "loss": 2.382,
        "grad_norm": 2.690063238143921,
        "learning_rate": 3.98133783756929e-05,
        "epoch": 0.47333333333333333,
        "step": 3550
    },
    {
        "loss": 2.4991,
        "grad_norm": 3.6476850509643555,
        "learning_rate": 3.976312920557061e-05,
        "epoch": 0.47346666666666665,
        "step": 3551
    },
    {
        "loss": 3.0693,
        "grad_norm": 2.229853630065918,
        "learning_rate": 3.9712903895478094e-05,
        "epoch": 0.4736,
        "step": 3552
    },
    {
        "loss": 2.008,
        "grad_norm": 3.2257230281829834,
        "learning_rate": 3.966270246530971e-05,
        "epoch": 0.47373333333333334,
        "step": 3553
    },
    {
        "loss": 2.3482,
        "grad_norm": 2.1113440990448,
        "learning_rate": 3.9612524934950535e-05,
        "epoch": 0.47386666666666666,
        "step": 3554
    },
    {
        "loss": 2.7666,
        "grad_norm": 3.0294978618621826,
        "learning_rate": 3.956237132427599e-05,
        "epoch": 0.474,
        "step": 3555
    },
    {
        "loss": 2.5942,
        "grad_norm": 1.9587050676345825,
        "learning_rate": 3.951224165315208e-05,
        "epoch": 0.47413333333333335,
        "step": 3556
    },
    {
        "loss": 2.2305,
        "grad_norm": 4.134889125823975,
        "learning_rate": 3.9462135941435384e-05,
        "epoch": 0.47426666666666667,
        "step": 3557
    },
    {
        "loss": 2.5328,
        "grad_norm": 3.1678102016448975,
        "learning_rate": 3.941205420897289e-05,
        "epoch": 0.4744,
        "step": 3558
    },
    {
        "loss": 0.9272,
        "grad_norm": 3.068629026412964,
        "learning_rate": 3.936199647560217e-05,
        "epoch": 0.4745333333333333,
        "step": 3559
    },
    {
        "loss": 2.4532,
        "grad_norm": 4.091790676116943,
        "learning_rate": 3.93119627611513e-05,
        "epoch": 0.4746666666666667,
        "step": 3560
    },
    {
        "loss": 1.3821,
        "grad_norm": 3.0222644805908203,
        "learning_rate": 3.926195308543873e-05,
        "epoch": 0.4748,
        "step": 3561
    },
    {
        "loss": 3.0722,
        "grad_norm": 2.333589553833008,
        "learning_rate": 3.921196746827351e-05,
        "epoch": 0.4749333333333333,
        "step": 3562
    },
    {
        "loss": 2.4645,
        "grad_norm": 2.9170408248901367,
        "learning_rate": 3.916200592945514e-05,
        "epoch": 0.4750666666666667,
        "step": 3563
    },
    {
        "loss": 2.8377,
        "grad_norm": 2.1272711753845215,
        "learning_rate": 3.911206848877351e-05,
        "epoch": 0.4752,
        "step": 3564
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.304342269897461,
        "learning_rate": 3.906215516600901e-05,
        "epoch": 0.47533333333333333,
        "step": 3565
    },
    {
        "loss": 2.6052,
        "grad_norm": 4.086484432220459,
        "learning_rate": 3.9012265980932475e-05,
        "epoch": 0.47546666666666665,
        "step": 3566
    },
    {
        "loss": 1.519,
        "grad_norm": 3.5057449340820312,
        "learning_rate": 3.896240095330519e-05,
        "epoch": 0.4756,
        "step": 3567
    },
    {
        "loss": 2.5869,
        "grad_norm": 3.057257890701294,
        "learning_rate": 3.8912560102878916e-05,
        "epoch": 0.47573333333333334,
        "step": 3568
    },
    {
        "loss": 2.6832,
        "grad_norm": 3.0342061519622803,
        "learning_rate": 3.886274344939569e-05,
        "epoch": 0.47586666666666666,
        "step": 3569
    },
    {
        "loss": 1.7778,
        "grad_norm": 4.304206371307373,
        "learning_rate": 3.8812951012588104e-05,
        "epoch": 0.476,
        "step": 3570
    },
    {
        "loss": 1.8301,
        "grad_norm": 5.535304069519043,
        "learning_rate": 3.8763182812179153e-05,
        "epoch": 0.47613333333333335,
        "step": 3571
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.102226495742798,
        "learning_rate": 3.871343886788211e-05,
        "epoch": 0.47626666666666667,
        "step": 3572
    },
    {
        "loss": 2.719,
        "grad_norm": 2.5981998443603516,
        "learning_rate": 3.866371919940079e-05,
        "epoch": 0.4764,
        "step": 3573
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.8344645500183105,
        "learning_rate": 3.8614023826429314e-05,
        "epoch": 0.4765333333333333,
        "step": 3574
    },
    {
        "loss": 1.5525,
        "grad_norm": 2.6009573936462402,
        "learning_rate": 3.856435276865208e-05,
        "epoch": 0.4766666666666667,
        "step": 3575
    },
    {
        "loss": 1.8996,
        "grad_norm": 3.659663200378418,
        "learning_rate": 3.851470604574412e-05,
        "epoch": 0.4768,
        "step": 3576
    },
    {
        "loss": 2.4013,
        "grad_norm": 3.34189510345459,
        "learning_rate": 3.8465083677370614e-05,
        "epoch": 0.4769333333333333,
        "step": 3577
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.4236292839050293,
        "learning_rate": 3.841548568318706e-05,
        "epoch": 0.4770666666666667,
        "step": 3578
    },
    {
        "loss": 2.6685,
        "grad_norm": 2.6606552600860596,
        "learning_rate": 3.836591208283954e-05,
        "epoch": 0.4772,
        "step": 3579
    },
    {
        "loss": 1.0844,
        "grad_norm": 4.812734603881836,
        "learning_rate": 3.8316362895964266e-05,
        "epoch": 0.47733333333333333,
        "step": 3580
    },
    {
        "loss": 1.4431,
        "grad_norm": 2.322953939437866,
        "learning_rate": 3.826683814218779e-05,
        "epoch": 0.47746666666666665,
        "step": 3581
    },
    {
        "loss": 2.5507,
        "grad_norm": 2.4857137203216553,
        "learning_rate": 3.82173378411271e-05,
        "epoch": 0.4776,
        "step": 3582
    },
    {
        "loss": 2.6838,
        "grad_norm": 2.6666207313537598,
        "learning_rate": 3.816786201238939e-05,
        "epoch": 0.47773333333333334,
        "step": 3583
    },
    {
        "loss": 2.968,
        "grad_norm": 1.9659850597381592,
        "learning_rate": 3.8118410675572227e-05,
        "epoch": 0.47786666666666666,
        "step": 3584
    },
    {
        "loss": 0.9845,
        "grad_norm": 3.7696876525878906,
        "learning_rate": 3.8068983850263474e-05,
        "epoch": 0.478,
        "step": 3585
    },
    {
        "loss": 3.2173,
        "grad_norm": 2.8713722229003906,
        "learning_rate": 3.8019581556041215e-05,
        "epoch": 0.47813333333333335,
        "step": 3586
    },
    {
        "loss": 2.4839,
        "grad_norm": 1.9762427806854248,
        "learning_rate": 3.7970203812473894e-05,
        "epoch": 0.4782666666666667,
        "step": 3587
    },
    {
        "loss": 2.5764,
        "grad_norm": 2.1451754570007324,
        "learning_rate": 3.792085063912021e-05,
        "epoch": 0.4784,
        "step": 3588
    },
    {
        "loss": 2.3788,
        "grad_norm": 2.833394765853882,
        "learning_rate": 3.787152205552914e-05,
        "epoch": 0.4785333333333333,
        "step": 3589
    },
    {
        "loss": 2.96,
        "grad_norm": 2.224151372909546,
        "learning_rate": 3.782221808123989e-05,
        "epoch": 0.4786666666666667,
        "step": 3590
    },
    {
        "loss": 2.4924,
        "grad_norm": 2.385195016860962,
        "learning_rate": 3.777293873578188e-05,
        "epoch": 0.4788,
        "step": 3591
    },
    {
        "loss": 2.7242,
        "grad_norm": 2.919238328933716,
        "learning_rate": 3.7723684038674876e-05,
        "epoch": 0.4789333333333333,
        "step": 3592
    },
    {
        "loss": 2.3199,
        "grad_norm": 3.3029844760894775,
        "learning_rate": 3.767445400942886e-05,
        "epoch": 0.47906666666666664,
        "step": 3593
    },
    {
        "loss": 1.8772,
        "grad_norm": 2.0474560260772705,
        "learning_rate": 3.762524866754395e-05,
        "epoch": 0.4792,
        "step": 3594
    },
    {
        "loss": 0.9868,
        "grad_norm": 3.640735626220703,
        "learning_rate": 3.757606803251058e-05,
        "epoch": 0.47933333333333333,
        "step": 3595
    },
    {
        "loss": 1.6065,
        "grad_norm": 2.4818642139434814,
        "learning_rate": 3.75269121238094e-05,
        "epoch": 0.47946666666666665,
        "step": 3596
    },
    {
        "loss": 2.5076,
        "grad_norm": 3.0532121658325195,
        "learning_rate": 3.7477780960911154e-05,
        "epoch": 0.4796,
        "step": 3597
    },
    {
        "loss": 2.8807,
        "grad_norm": 3.2974295616149902,
        "learning_rate": 3.7428674563276956e-05,
        "epoch": 0.47973333333333334,
        "step": 3598
    },
    {
        "loss": 2.1331,
        "grad_norm": 3.6704764366149902,
        "learning_rate": 3.7379592950357965e-05,
        "epoch": 0.47986666666666666,
        "step": 3599
    },
    {
        "loss": 2.689,
        "grad_norm": 1.9483637809753418,
        "learning_rate": 3.7330536141595506e-05,
        "epoch": 0.48,
        "step": 3600
    },
    {
        "loss": 1.1626,
        "grad_norm": 4.55615234375,
        "learning_rate": 3.72815041564213e-05,
        "epoch": 0.48013333333333336,
        "step": 3601
    },
    {
        "loss": 1.7969,
        "grad_norm": 4.203695297241211,
        "learning_rate": 3.7232497014257006e-05,
        "epoch": 0.4802666666666667,
        "step": 3602
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.2532260417938232,
        "learning_rate": 3.718351473451448e-05,
        "epoch": 0.4804,
        "step": 3603
    },
    {
        "loss": 2.6752,
        "grad_norm": 2.698209762573242,
        "learning_rate": 3.713455733659581e-05,
        "epoch": 0.4805333333333333,
        "step": 3604
    },
    {
        "loss": 2.0776,
        "grad_norm": 4.842402458190918,
        "learning_rate": 3.708562483989323e-05,
        "epoch": 0.4806666666666667,
        "step": 3605
    },
    {
        "loss": 2.3995,
        "grad_norm": 3.4012935161590576,
        "learning_rate": 3.703671726378898e-05,
        "epoch": 0.4808,
        "step": 3606
    },
    {
        "loss": 1.1462,
        "grad_norm": 4.027487277984619,
        "learning_rate": 3.698783462765557e-05,
        "epoch": 0.4809333333333333,
        "step": 3607
    },
    {
        "loss": 2.9999,
        "grad_norm": 4.576110363006592,
        "learning_rate": 3.6938976950855606e-05,
        "epoch": 0.48106666666666664,
        "step": 3608
    },
    {
        "loss": 2.1624,
        "grad_norm": 2.2821133136749268,
        "learning_rate": 3.689014425274171e-05,
        "epoch": 0.4812,
        "step": 3609
    },
    {
        "loss": 3.0059,
        "grad_norm": 2.791452169418335,
        "learning_rate": 3.6841336552656724e-05,
        "epoch": 0.48133333333333334,
        "step": 3610
    },
    {
        "loss": 1.9383,
        "grad_norm": 3.0781941413879395,
        "learning_rate": 3.679255386993356e-05,
        "epoch": 0.48146666666666665,
        "step": 3611
    },
    {
        "loss": 2.0532,
        "grad_norm": 3.0602171421051025,
        "learning_rate": 3.674379622389516e-05,
        "epoch": 0.4816,
        "step": 3612
    },
    {
        "loss": 2.2583,
        "grad_norm": 2.8230741024017334,
        "learning_rate": 3.669506363385462e-05,
        "epoch": 0.48173333333333335,
        "step": 3613
    },
    {
        "loss": 0.9389,
        "grad_norm": 3.269303798675537,
        "learning_rate": 3.66463561191151e-05,
        "epoch": 0.48186666666666667,
        "step": 3614
    },
    {
        "loss": 2.3199,
        "grad_norm": 2.9663448333740234,
        "learning_rate": 3.65976736989698e-05,
        "epoch": 0.482,
        "step": 3615
    },
    {
        "loss": 2.3503,
        "grad_norm": 3.1778433322906494,
        "learning_rate": 3.654901639270195e-05,
        "epoch": 0.48213333333333336,
        "step": 3616
    },
    {
        "loss": 2.4328,
        "grad_norm": 2.7547590732574463,
        "learning_rate": 3.650038421958491e-05,
        "epoch": 0.4822666666666667,
        "step": 3617
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.2172818183898926,
        "learning_rate": 3.6451777198882095e-05,
        "epoch": 0.4824,
        "step": 3618
    },
    {
        "loss": 3.1115,
        "grad_norm": 3.8303377628326416,
        "learning_rate": 3.640319534984683e-05,
        "epoch": 0.4825333333333333,
        "step": 3619
    },
    {
        "loss": 2.0353,
        "grad_norm": 2.503549814224243,
        "learning_rate": 3.6354638691722575e-05,
        "epoch": 0.4826666666666667,
        "step": 3620
    },
    {
        "loss": 1.8545,
        "grad_norm": 5.067757606506348,
        "learning_rate": 3.630610724374285e-05,
        "epoch": 0.4828,
        "step": 3621
    },
    {
        "loss": 2.2395,
        "grad_norm": 3.3608720302581787,
        "learning_rate": 3.6257601025131026e-05,
        "epoch": 0.4829333333333333,
        "step": 3622
    },
    {
        "loss": 2.4527,
        "grad_norm": 2.6440305709838867,
        "learning_rate": 3.620912005510068e-05,
        "epoch": 0.48306666666666664,
        "step": 3623
    },
    {
        "loss": 2.27,
        "grad_norm": 2.9793708324432373,
        "learning_rate": 3.6160664352855246e-05,
        "epoch": 0.4832,
        "step": 3624
    },
    {
        "loss": 1.7936,
        "grad_norm": 3.0756521224975586,
        "learning_rate": 3.61122339375881e-05,
        "epoch": 0.48333333333333334,
        "step": 3625
    },
    {
        "loss": 2.6119,
        "grad_norm": 2.704195499420166,
        "learning_rate": 3.606382882848284e-05,
        "epoch": 0.48346666666666666,
        "step": 3626
    },
    {
        "loss": 0.8623,
        "grad_norm": 1.7610167264938354,
        "learning_rate": 3.601544904471286e-05,
        "epoch": 0.4836,
        "step": 3627
    },
    {
        "loss": 1.4332,
        "grad_norm": 4.349512577056885,
        "learning_rate": 3.596709460544154e-05,
        "epoch": 0.48373333333333335,
        "step": 3628
    },
    {
        "loss": 2.1261,
        "grad_norm": 3.823241710662842,
        "learning_rate": 3.591876552982216e-05,
        "epoch": 0.48386666666666667,
        "step": 3629
    },
    {
        "loss": 2.7894,
        "grad_norm": 2.8562920093536377,
        "learning_rate": 3.587046183699818e-05,
        "epoch": 0.484,
        "step": 3630
    },
    {
        "loss": 2.858,
        "grad_norm": 2.160114049911499,
        "learning_rate": 3.5822183546102786e-05,
        "epoch": 0.48413333333333336,
        "step": 3631
    },
    {
        "loss": 2.3821,
        "grad_norm": 4.0492048263549805,
        "learning_rate": 3.5773930676259127e-05,
        "epoch": 0.4842666666666667,
        "step": 3632
    },
    {
        "loss": 0.8492,
        "grad_norm": 2.6528878211975098,
        "learning_rate": 3.572570324658041e-05,
        "epoch": 0.4844,
        "step": 3633
    },
    {
        "loss": 1.671,
        "grad_norm": 3.5607974529266357,
        "learning_rate": 3.567750127616961e-05,
        "epoch": 0.4845333333333333,
        "step": 3634
    },
    {
        "loss": 2.1811,
        "grad_norm": 3.260646104812622,
        "learning_rate": 3.562932478411973e-05,
        "epoch": 0.4846666666666667,
        "step": 3635
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.72452449798584,
        "learning_rate": 3.5581173789513674e-05,
        "epoch": 0.4848,
        "step": 3636
    },
    {
        "loss": 2.3774,
        "grad_norm": 2.5789973735809326,
        "learning_rate": 3.553304831142414e-05,
        "epoch": 0.4849333333333333,
        "step": 3637
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.647135019302368,
        "learning_rate": 3.548494836891385e-05,
        "epoch": 0.48506666666666665,
        "step": 3638
    },
    {
        "loss": 2.5715,
        "grad_norm": 2.9103267192840576,
        "learning_rate": 3.543687398103537e-05,
        "epoch": 0.4852,
        "step": 3639
    },
    {
        "loss": 3.2654,
        "grad_norm": 4.022517204284668,
        "learning_rate": 3.53888251668311e-05,
        "epoch": 0.48533333333333334,
        "step": 3640
    },
    {
        "loss": 2.7143,
        "grad_norm": 2.2760560512542725,
        "learning_rate": 3.5340801945333304e-05,
        "epoch": 0.48546666666666666,
        "step": 3641
    },
    {
        "loss": 2.6614,
        "grad_norm": 2.3113067150115967,
        "learning_rate": 3.529280433556417e-05,
        "epoch": 0.4856,
        "step": 3642
    },
    {
        "loss": 2.1134,
        "grad_norm": 3.3776895999908447,
        "learning_rate": 3.5244832356535764e-05,
        "epoch": 0.48573333333333335,
        "step": 3643
    },
    {
        "loss": 2.3859,
        "grad_norm": 3.5950632095336914,
        "learning_rate": 3.519688602724987e-05,
        "epoch": 0.48586666666666667,
        "step": 3644
    },
    {
        "loss": 1.9732,
        "grad_norm": 2.52107834815979,
        "learning_rate": 3.514896536669824e-05,
        "epoch": 0.486,
        "step": 3645
    },
    {
        "loss": 2.6323,
        "grad_norm": 2.277034044265747,
        "learning_rate": 3.51010703938624e-05,
        "epoch": 0.4861333333333333,
        "step": 3646
    },
    {
        "loss": 1.9119,
        "grad_norm": 2.948245048522949,
        "learning_rate": 3.505320112771375e-05,
        "epoch": 0.4862666666666667,
        "step": 3647
    },
    {
        "loss": 2.5325,
        "grad_norm": 2.64363694190979,
        "learning_rate": 3.5005357587213395e-05,
        "epoch": 0.4864,
        "step": 3648
    },
    {
        "loss": 2.8756,
        "grad_norm": 3.2183022499084473,
        "learning_rate": 3.49575397913124e-05,
        "epoch": 0.4865333333333333,
        "step": 3649
    },
    {
        "loss": 1.176,
        "grad_norm": 3.479381561279297,
        "learning_rate": 3.490974775895153e-05,
        "epoch": 0.4866666666666667,
        "step": 3650
    },
    {
        "loss": 1.7639,
        "grad_norm": 3.1373984813690186,
        "learning_rate": 3.486198150906128e-05,
        "epoch": 0.4868,
        "step": 3651
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.9506754875183105,
        "learning_rate": 3.481424106056218e-05,
        "epoch": 0.48693333333333333,
        "step": 3652
    },
    {
        "loss": 2.5161,
        "grad_norm": 2.504546642303467,
        "learning_rate": 3.476652643236431e-05,
        "epoch": 0.48706666666666665,
        "step": 3653
    },
    {
        "loss": 3.0272,
        "grad_norm": 2.244999647140503,
        "learning_rate": 3.471883764336753e-05,
        "epoch": 0.4872,
        "step": 3654
    },
    {
        "loss": 2.6121,
        "grad_norm": 2.865875720977783,
        "learning_rate": 3.4671174712461675e-05,
        "epoch": 0.48733333333333334,
        "step": 3655
    },
    {
        "loss": 1.6492,
        "grad_norm": 3.997519016265869,
        "learning_rate": 3.46235376585261e-05,
        "epoch": 0.48746666666666666,
        "step": 3656
    },
    {
        "loss": 2.2447,
        "grad_norm": 3.085559844970703,
        "learning_rate": 3.457592650043001e-05,
        "epoch": 0.4876,
        "step": 3657
    },
    {
        "loss": 2.7561,
        "grad_norm": 3.079256534576416,
        "learning_rate": 3.452834125703238e-05,
        "epoch": 0.48773333333333335,
        "step": 3658
    },
    {
        "loss": 0.6527,
        "grad_norm": 3.375277519226074,
        "learning_rate": 3.448078194718184e-05,
        "epoch": 0.48786666666666667,
        "step": 3659
    },
    {
        "loss": 1.2414,
        "grad_norm": 3.483665943145752,
        "learning_rate": 3.4433248589716825e-05,
        "epoch": 0.488,
        "step": 3660
    },
    {
        "loss": 1.832,
        "grad_norm": 3.766812801361084,
        "learning_rate": 3.438574120346548e-05,
        "epoch": 0.4881333333333333,
        "step": 3661
    },
    {
        "loss": 3.2049,
        "grad_norm": 2.792790651321411,
        "learning_rate": 3.43382598072456e-05,
        "epoch": 0.4882666666666667,
        "step": 3662
    },
    {
        "loss": 2.0216,
        "grad_norm": 3.44738507270813,
        "learning_rate": 3.429080441986474e-05,
        "epoch": 0.4884,
        "step": 3663
    },
    {
        "loss": 1.8809,
        "grad_norm": 2.6789872646331787,
        "learning_rate": 3.424337506012017e-05,
        "epoch": 0.4885333333333333,
        "step": 3664
    },
    {
        "loss": 2.1956,
        "grad_norm": 2.5275540351867676,
        "learning_rate": 3.419597174679882e-05,
        "epoch": 0.4886666666666667,
        "step": 3665
    },
    {
        "loss": 2.5992,
        "grad_norm": 1.2453272342681885,
        "learning_rate": 3.4148594498677235e-05,
        "epoch": 0.4888,
        "step": 3666
    },
    {
        "loss": 1.9015,
        "grad_norm": 3.5757575035095215,
        "learning_rate": 3.410124333452176e-05,
        "epoch": 0.48893333333333333,
        "step": 3667
    },
    {
        "loss": 4.9985,
        "grad_norm": 4.337711334228516,
        "learning_rate": 3.4053918273088334e-05,
        "epoch": 0.48906666666666665,
        "step": 3668
    },
    {
        "loss": 2.4671,
        "grad_norm": 2.284514904022217,
        "learning_rate": 3.4006619333122635e-05,
        "epoch": 0.4892,
        "step": 3669
    },
    {
        "loss": 0.6885,
        "grad_norm": 2.6063520908355713,
        "learning_rate": 3.395934653335984e-05,
        "epoch": 0.48933333333333334,
        "step": 3670
    },
    {
        "loss": 2.4787,
        "grad_norm": 2.3398609161376953,
        "learning_rate": 3.391209989252492e-05,
        "epoch": 0.48946666666666666,
        "step": 3671
    },
    {
        "loss": 2.9974,
        "grad_norm": 4.531599998474121,
        "learning_rate": 3.386487942933244e-05,
        "epoch": 0.4896,
        "step": 3672
    },
    {
        "loss": 2.0587,
        "grad_norm": 2.3654441833496094,
        "learning_rate": 3.3817685162486546e-05,
        "epoch": 0.48973333333333335,
        "step": 3673
    },
    {
        "loss": 3.0261,
        "grad_norm": 3.481360912322998,
        "learning_rate": 3.37705171106811e-05,
        "epoch": 0.4898666666666667,
        "step": 3674
    },
    {
        "loss": 2.4095,
        "grad_norm": 3.7242941856384277,
        "learning_rate": 3.37233752925995e-05,
        "epoch": 0.49,
        "step": 3675
    },
    {
        "loss": 2.6316,
        "grad_norm": 3.0414977073669434,
        "learning_rate": 3.367625972691471e-05,
        "epoch": 0.4901333333333333,
        "step": 3676
    },
    {
        "loss": 2.8189,
        "grad_norm": 2.1597139835357666,
        "learning_rate": 3.36291704322895e-05,
        "epoch": 0.4902666666666667,
        "step": 3677
    },
    {
        "loss": 3.5204,
        "grad_norm": 4.549247741699219,
        "learning_rate": 3.358210742737604e-05,
        "epoch": 0.4904,
        "step": 3678
    },
    {
        "loss": 2.729,
        "grad_norm": 2.273958206176758,
        "learning_rate": 3.353507073081608e-05,
        "epoch": 0.4905333333333333,
        "step": 3679
    },
    {
        "loss": 2.032,
        "grad_norm": 3.232045888900757,
        "learning_rate": 3.348806036124113e-05,
        "epoch": 0.49066666666666664,
        "step": 3680
    },
    {
        "loss": 1.6227,
        "grad_norm": 3.686103105545044,
        "learning_rate": 3.3441076337272116e-05,
        "epoch": 0.4908,
        "step": 3681
    },
    {
        "loss": 1.7373,
        "grad_norm": 3.1623716354370117,
        "learning_rate": 3.339411867751951e-05,
        "epoch": 0.49093333333333333,
        "step": 3682
    },
    {
        "loss": 2.2679,
        "grad_norm": 3.2890985012054443,
        "learning_rate": 3.33471874005835e-05,
        "epoch": 0.49106666666666665,
        "step": 3683
    },
    {
        "loss": 3.1433,
        "grad_norm": 2.567477226257324,
        "learning_rate": 3.330028252505363e-05,
        "epoch": 0.4912,
        "step": 3684
    },
    {
        "loss": 1.5097,
        "grad_norm": 2.805325746536255,
        "learning_rate": 3.325340406950913e-05,
        "epoch": 0.49133333333333334,
        "step": 3685
    },
    {
        "loss": 2.2484,
        "grad_norm": 5.046591281890869,
        "learning_rate": 3.320655205251875e-05,
        "epoch": 0.49146666666666666,
        "step": 3686
    },
    {
        "loss": 2.8335,
        "grad_norm": 4.941308498382568,
        "learning_rate": 3.3159726492640655e-05,
        "epoch": 0.4916,
        "step": 3687
    },
    {
        "loss": 2.4958,
        "grad_norm": 2.9315590858459473,
        "learning_rate": 3.3112927408422645e-05,
        "epoch": 0.49173333333333336,
        "step": 3688
    },
    {
        "loss": 1.4184,
        "grad_norm": 2.20363450050354,
        "learning_rate": 3.3066154818402007e-05,
        "epoch": 0.4918666666666667,
        "step": 3689
    },
    {
        "loss": 2.2292,
        "grad_norm": 4.9719953536987305,
        "learning_rate": 3.301940874110555e-05,
        "epoch": 0.492,
        "step": 3690
    },
    {
        "loss": 1.9087,
        "grad_norm": 2.6569740772247314,
        "learning_rate": 3.297268919504952e-05,
        "epoch": 0.4921333333333333,
        "step": 3691
    },
    {
        "loss": 1.9863,
        "grad_norm": 2.9335949420928955,
        "learning_rate": 3.2925996198739664e-05,
        "epoch": 0.4922666666666667,
        "step": 3692
    },
    {
        "loss": 2.3837,
        "grad_norm": 3.4421145915985107,
        "learning_rate": 3.2879329770671255e-05,
        "epoch": 0.4924,
        "step": 3693
    },
    {
        "loss": 2.0705,
        "grad_norm": 3.559441566467285,
        "learning_rate": 3.283268992932906e-05,
        "epoch": 0.4925333333333333,
        "step": 3694
    },
    {
        "loss": 2.0936,
        "grad_norm": 2.068183422088623,
        "learning_rate": 3.278607669318723e-05,
        "epoch": 0.49266666666666664,
        "step": 3695
    },
    {
        "loss": 2.5243,
        "grad_norm": 2.2089626789093018,
        "learning_rate": 3.273949008070943e-05,
        "epoch": 0.4928,
        "step": 3696
    },
    {
        "loss": 2.5813,
        "grad_norm": 2.752009153366089,
        "learning_rate": 3.269293011034884e-05,
        "epoch": 0.49293333333333333,
        "step": 3697
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.4134979248046875,
        "learning_rate": 3.264639680054795e-05,
        "epoch": 0.49306666666666665,
        "step": 3698
    },
    {
        "loss": 2.8674,
        "grad_norm": 3.6612894535064697,
        "learning_rate": 3.2599890169738815e-05,
        "epoch": 0.4932,
        "step": 3699
    },
    {
        "loss": 2.9323,
        "grad_norm": 2.695946216583252,
        "learning_rate": 3.255341023634284e-05,
        "epoch": 0.49333333333333335,
        "step": 3700
    },
    {
        "loss": 1.9607,
        "grad_norm": 2.997854709625244,
        "learning_rate": 3.250695701877082e-05,
        "epoch": 0.49346666666666666,
        "step": 3701
    },
    {
        "loss": 1.7434,
        "grad_norm": 7.172852993011475,
        "learning_rate": 3.246053053542318e-05,
        "epoch": 0.4936,
        "step": 3702
    },
    {
        "loss": 2.3857,
        "grad_norm": 2.785000801086426,
        "learning_rate": 3.241413080468953e-05,
        "epoch": 0.49373333333333336,
        "step": 3703
    },
    {
        "loss": 2.0575,
        "grad_norm": 4.43856143951416,
        "learning_rate": 3.236775784494891e-05,
        "epoch": 0.4938666666666667,
        "step": 3704
    },
    {
        "loss": 3.6906,
        "grad_norm": 6.005660057067871,
        "learning_rate": 3.232141167456988e-05,
        "epoch": 0.494,
        "step": 3705
    },
    {
        "loss": 2.7729,
        "grad_norm": 1.7834792137145996,
        "learning_rate": 3.227509231191032e-05,
        "epoch": 0.4941333333333333,
        "step": 3706
    },
    {
        "loss": 2.4476,
        "grad_norm": 4.207943439483643,
        "learning_rate": 3.222879977531742e-05,
        "epoch": 0.4942666666666667,
        "step": 3707
    },
    {
        "loss": 2.6501,
        "grad_norm": 2.171950101852417,
        "learning_rate": 3.2182534083127866e-05,
        "epoch": 0.4944,
        "step": 3708
    },
    {
        "loss": 2.8772,
        "grad_norm": 2.0725419521331787,
        "learning_rate": 3.213629525366767e-05,
        "epoch": 0.4945333333333333,
        "step": 3709
    },
    {
        "loss": 2.8319,
        "grad_norm": 4.877912998199463,
        "learning_rate": 3.209008330525214e-05,
        "epoch": 0.49466666666666664,
        "step": 3710
    },
    {
        "loss": 2.8451,
        "grad_norm": 2.1701667308807373,
        "learning_rate": 3.2043898256186e-05,
        "epoch": 0.4948,
        "step": 3711
    },
    {
        "loss": 1.3049,
        "grad_norm": 5.518343925476074,
        "learning_rate": 3.1997740124763355e-05,
        "epoch": 0.49493333333333334,
        "step": 3712
    },
    {
        "loss": 2.1401,
        "grad_norm": 3.034156322479248,
        "learning_rate": 3.1951608929267516e-05,
        "epoch": 0.49506666666666665,
        "step": 3713
    },
    {
        "loss": 2.1006,
        "grad_norm": 3.6665287017822266,
        "learning_rate": 3.190550468797126e-05,
        "epoch": 0.4952,
        "step": 3714
    },
    {
        "loss": 2.7925,
        "grad_norm": 3.421891450881958,
        "learning_rate": 3.1859427419136645e-05,
        "epoch": 0.49533333333333335,
        "step": 3715
    },
    {
        "loss": 2.2874,
        "grad_norm": 3.6426563262939453,
        "learning_rate": 3.181337714101502e-05,
        "epoch": 0.49546666666666667,
        "step": 3716
    },
    {
        "loss": 1.7303,
        "grad_norm": 2.6491928100585938,
        "learning_rate": 3.1767353871847016e-05,
        "epoch": 0.4956,
        "step": 3717
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.8083858489990234,
        "learning_rate": 3.1721357629862644e-05,
        "epoch": 0.49573333333333336,
        "step": 3718
    },
    {
        "loss": 1.0322,
        "grad_norm": 3.793370485305786,
        "learning_rate": 3.1675388433281204e-05,
        "epoch": 0.4958666666666667,
        "step": 3719
    },
    {
        "loss": 1.8805,
        "grad_norm": 3.110504388809204,
        "learning_rate": 3.162944630031117e-05,
        "epoch": 0.496,
        "step": 3720
    },
    {
        "loss": 2.1557,
        "grad_norm": 5.1747727394104,
        "learning_rate": 3.158353124915043e-05,
        "epoch": 0.4961333333333333,
        "step": 3721
    },
    {
        "loss": 2.5905,
        "grad_norm": 3.6455328464508057,
        "learning_rate": 3.153764329798613e-05,
        "epoch": 0.4962666666666667,
        "step": 3722
    },
    {
        "loss": 2.1155,
        "grad_norm": 3.1919777393341064,
        "learning_rate": 3.149178246499456e-05,
        "epoch": 0.4964,
        "step": 3723
    },
    {
        "loss": 1.9401,
        "grad_norm": 4.477617263793945,
        "learning_rate": 3.144594876834141e-05,
        "epoch": 0.4965333333333333,
        "step": 3724
    },
    {
        "loss": 1.8636,
        "grad_norm": 2.8488998413085938,
        "learning_rate": 3.140014222618156e-05,
        "epoch": 0.49666666666666665,
        "step": 3725
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.3183109760284424,
        "learning_rate": 3.1354362856659045e-05,
        "epoch": 0.4968,
        "step": 3726
    },
    {
        "loss": 1.3778,
        "grad_norm": 3.5759835243225098,
        "learning_rate": 3.130861067790736e-05,
        "epoch": 0.49693333333333334,
        "step": 3727
    },
    {
        "loss": 2.655,
        "grad_norm": 2.6337127685546875,
        "learning_rate": 3.126288570804906e-05,
        "epoch": 0.49706666666666666,
        "step": 3728
    },
    {
        "loss": 2.2356,
        "grad_norm": 2.840334415435791,
        "learning_rate": 3.121718796519595e-05,
        "epoch": 0.4972,
        "step": 3729
    },
    {
        "loss": 2.5032,
        "grad_norm": 2.7016983032226562,
        "learning_rate": 3.1171517467448994e-05,
        "epoch": 0.49733333333333335,
        "step": 3730
    },
    {
        "loss": 1.5511,
        "grad_norm": 3.4322445392608643,
        "learning_rate": 3.112587423289857e-05,
        "epoch": 0.49746666666666667,
        "step": 3731
    },
    {
        "loss": 2.5135,
        "grad_norm": 3.453206777572632,
        "learning_rate": 3.108025827962404e-05,
        "epoch": 0.4976,
        "step": 3732
    },
    {
        "loss": 2.3581,
        "grad_norm": 3.295609712600708,
        "learning_rate": 3.1034669625694e-05,
        "epoch": 0.49773333333333336,
        "step": 3733
    },
    {
        "loss": 2.4563,
        "grad_norm": 2.5002949237823486,
        "learning_rate": 3.098910828916637e-05,
        "epoch": 0.4978666666666667,
        "step": 3734
    },
    {
        "loss": 1.1602,
        "grad_norm": 3.660510778427124,
        "learning_rate": 3.094357428808804e-05,
        "epoch": 0.498,
        "step": 3735
    },
    {
        "loss": 1.6579,
        "grad_norm": 4.349880695343018,
        "learning_rate": 3.089806764049526e-05,
        "epoch": 0.4981333333333333,
        "step": 3736
    },
    {
        "loss": 2.6673,
        "grad_norm": 1.857792854309082,
        "learning_rate": 3.085258836441336e-05,
        "epoch": 0.4982666666666667,
        "step": 3737
    },
    {
        "loss": 2.357,
        "grad_norm": 2.7085320949554443,
        "learning_rate": 3.08071364778568e-05,
        "epoch": 0.4984,
        "step": 3738
    },
    {
        "loss": 2.6285,
        "grad_norm": 3.0937986373901367,
        "learning_rate": 3.0761711998829245e-05,
        "epoch": 0.49853333333333333,
        "step": 3739
    },
    {
        "loss": 2.2825,
        "grad_norm": 2.5607991218566895,
        "learning_rate": 3.071631494532353e-05,
        "epoch": 0.49866666666666665,
        "step": 3740
    },
    {
        "loss": 1.5535,
        "grad_norm": 3.3509788513183594,
        "learning_rate": 3.067094533532154e-05,
        "epoch": 0.4988,
        "step": 3741
    },
    {
        "loss": 2.4187,
        "grad_norm": 2.98689866065979,
        "learning_rate": 3.062560318679431e-05,
        "epoch": 0.49893333333333334,
        "step": 3742
    },
    {
        "loss": 1.8127,
        "grad_norm": 3.0052530765533447,
        "learning_rate": 3.058028851770203e-05,
        "epoch": 0.49906666666666666,
        "step": 3743
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.593625068664551,
        "learning_rate": 3.0535001345994054e-05,
        "epoch": 0.4992,
        "step": 3744
    },
    {
        "loss": 2.5716,
        "grad_norm": 3.2942497730255127,
        "learning_rate": 3.048974168960871e-05,
        "epoch": 0.49933333333333335,
        "step": 3745
    },
    {
        "loss": 2.208,
        "grad_norm": 2.9057655334472656,
        "learning_rate": 3.0444509566473522e-05,
        "epoch": 0.49946666666666667,
        "step": 3746
    },
    {
        "loss": 2.5141,
        "grad_norm": 3.069185495376587,
        "learning_rate": 3.0399304994505107e-05,
        "epoch": 0.4996,
        "step": 3747
    },
    {
        "loss": 3.2293,
        "grad_norm": 3.1440794467926025,
        "learning_rate": 3.035412799160917e-05,
        "epoch": 0.4997333333333333,
        "step": 3748
    },
    {
        "loss": 2.47,
        "grad_norm": 4.668178558349609,
        "learning_rate": 3.0308978575680424e-05,
        "epoch": 0.4998666666666667,
        "step": 3749
    },
    {
        "loss": 2.5997,
        "grad_norm": 2.0578362941741943,
        "learning_rate": 3.026385676460276e-05,
        "epoch": 0.5,
        "step": 3750
    },
    {
        "loss": 2.891,
        "grad_norm": 1.7942208051681519,
        "learning_rate": 3.021876257624905e-05,
        "epoch": 0.5001333333333333,
        "step": 3751
    },
    {
        "loss": 2.4952,
        "grad_norm": 2.4493865966796875,
        "learning_rate": 3.017369602848119e-05,
        "epoch": 0.5002666666666666,
        "step": 3752
    },
    {
        "loss": 2.186,
        "grad_norm": 3.307455539703369,
        "learning_rate": 3.012865713915033e-05,
        "epoch": 0.5004,
        "step": 3753
    },
    {
        "loss": 2.8737,
        "grad_norm": 3.196338176727295,
        "learning_rate": 3.008364592609645e-05,
        "epoch": 0.5005333333333334,
        "step": 3754
    },
    {
        "loss": 2.0281,
        "grad_norm": 3.5759575366973877,
        "learning_rate": 3.00386624071486e-05,
        "epoch": 0.5006666666666667,
        "step": 3755
    },
    {
        "loss": 1.7735,
        "grad_norm": 1.8759829998016357,
        "learning_rate": 2.9993706600125015e-05,
        "epoch": 0.5008,
        "step": 3756
    },
    {
        "loss": 2.3796,
        "grad_norm": 3.132192373275757,
        "learning_rate": 2.9948778522832788e-05,
        "epoch": 0.5009333333333333,
        "step": 3757
    },
    {
        "loss": 2.1547,
        "grad_norm": 4.007081031799316,
        "learning_rate": 2.9903878193068048e-05,
        "epoch": 0.5010666666666667,
        "step": 3758
    },
    {
        "loss": 2.2645,
        "grad_norm": 3.267347812652588,
        "learning_rate": 2.9859005628616032e-05,
        "epoch": 0.5012,
        "step": 3759
    },
    {
        "loss": 2.2748,
        "grad_norm": 2.8079819679260254,
        "learning_rate": 2.981416084725086e-05,
        "epoch": 0.5013333333333333,
        "step": 3760
    },
    {
        "loss": 1.983,
        "grad_norm": 3.6859705448150635,
        "learning_rate": 2.9769343866735734e-05,
        "epoch": 0.5014666666666666,
        "step": 3761
    },
    {
        "loss": 1.9388,
        "grad_norm": 2.9720096588134766,
        "learning_rate": 2.9724554704822828e-05,
        "epoch": 0.5016,
        "step": 3762
    },
    {
        "loss": 2.3269,
        "grad_norm": 2.351595163345337,
        "learning_rate": 2.967979337925323e-05,
        "epoch": 0.5017333333333334,
        "step": 3763
    },
    {
        "loss": 2.6979,
        "grad_norm": 3.265328884124756,
        "learning_rate": 2.963505990775709e-05,
        "epoch": 0.5018666666666667,
        "step": 3764
    },
    {
        "loss": 2.8441,
        "grad_norm": 2.88724946975708,
        "learning_rate": 2.9590354308053514e-05,
        "epoch": 0.502,
        "step": 3765
    },
    {
        "loss": 2.5043,
        "grad_norm": 2.5279581546783447,
        "learning_rate": 2.9545676597850503e-05,
        "epoch": 0.5021333333333333,
        "step": 3766
    },
    {
        "loss": 2.5543,
        "grad_norm": 3.047253370285034,
        "learning_rate": 2.9501026794845044e-05,
        "epoch": 0.5022666666666666,
        "step": 3767
    },
    {
        "loss": 1.798,
        "grad_norm": 3.3059520721435547,
        "learning_rate": 2.945640491672308e-05,
        "epoch": 0.5024,
        "step": 3768
    },
    {
        "loss": 2.4537,
        "grad_norm": 2.099957227706909,
        "learning_rate": 2.9411810981159492e-05,
        "epoch": 0.5025333333333334,
        "step": 3769
    },
    {
        "loss": 1.0462,
        "grad_norm": 3.6949517726898193,
        "learning_rate": 2.9367245005818146e-05,
        "epoch": 0.5026666666666667,
        "step": 3770
    },
    {
        "loss": 2.299,
        "grad_norm": 2.555689573287964,
        "learning_rate": 2.932270700835169e-05,
        "epoch": 0.5028,
        "step": 3771
    },
    {
        "loss": 1.8853,
        "grad_norm": 3.6583681106567383,
        "learning_rate": 2.927819700640181e-05,
        "epoch": 0.5029333333333333,
        "step": 3772
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.83151376247406,
        "learning_rate": 2.9233715017599117e-05,
        "epoch": 0.5030666666666667,
        "step": 3773
    },
    {
        "loss": 3.8827,
        "grad_norm": 2.735037326812744,
        "learning_rate": 2.918926105956299e-05,
        "epoch": 0.5032,
        "step": 3774
    },
    {
        "loss": 2.134,
        "grad_norm": 3.0494930744171143,
        "learning_rate": 2.9144835149901916e-05,
        "epoch": 0.5033333333333333,
        "step": 3775
    },
    {
        "loss": 2.367,
        "grad_norm": 2.6433229446411133,
        "learning_rate": 2.9100437306213003e-05,
        "epoch": 0.5034666666666666,
        "step": 3776
    },
    {
        "loss": 2.6253,
        "grad_norm": 2.987652063369751,
        "learning_rate": 2.9056067546082423e-05,
        "epoch": 0.5036,
        "step": 3777
    },
    {
        "loss": 2.0332,
        "grad_norm": 3.1614327430725098,
        "learning_rate": 2.9011725887085283e-05,
        "epoch": 0.5037333333333334,
        "step": 3778
    },
    {
        "loss": 2.465,
        "grad_norm": 3.4318466186523438,
        "learning_rate": 2.8967412346785385e-05,
        "epoch": 0.5038666666666667,
        "step": 3779
    },
    {
        "loss": 2.4557,
        "grad_norm": 2.8763229846954346,
        "learning_rate": 2.892312694273549e-05,
        "epoch": 0.504,
        "step": 3780
    },
    {
        "loss": 2.5606,
        "grad_norm": 3.37015700340271,
        "learning_rate": 2.8878869692477194e-05,
        "epoch": 0.5041333333333333,
        "step": 3781
    },
    {
        "loss": 2.9992,
        "grad_norm": 3.892404079437256,
        "learning_rate": 2.883464061354103e-05,
        "epoch": 0.5042666666666666,
        "step": 3782
    },
    {
        "loss": 2.3075,
        "grad_norm": 3.544318199157715,
        "learning_rate": 2.879043972344614e-05,
        "epoch": 0.5044,
        "step": 3783
    },
    {
        "loss": 2.6903,
        "grad_norm": 2.6030871868133545,
        "learning_rate": 2.8746267039700735e-05,
        "epoch": 0.5045333333333333,
        "step": 3784
    },
    {
        "loss": 2.9247,
        "grad_norm": 2.5843591690063477,
        "learning_rate": 2.8702122579801728e-05,
        "epoch": 0.5046666666666667,
        "step": 3785
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.0334811210632324,
        "learning_rate": 2.8658006361234912e-05,
        "epoch": 0.5048,
        "step": 3786
    },
    {
        "loss": 2.6926,
        "grad_norm": 2.3728253841400146,
        "learning_rate": 2.861391840147486e-05,
        "epoch": 0.5049333333333333,
        "step": 3787
    },
    {
        "loss": 2.5917,
        "grad_norm": 2.187227487564087,
        "learning_rate": 2.856985871798501e-05,
        "epoch": 0.5050666666666667,
        "step": 3788
    },
    {
        "loss": 2.1058,
        "grad_norm": 2.568230152130127,
        "learning_rate": 2.852582732821747e-05,
        "epoch": 0.5052,
        "step": 3789
    },
    {
        "loss": 2.1607,
        "grad_norm": 2.178919792175293,
        "learning_rate": 2.848182424961331e-05,
        "epoch": 0.5053333333333333,
        "step": 3790
    },
    {
        "loss": 0.8424,
        "grad_norm": 3.716444969177246,
        "learning_rate": 2.843784949960222e-05,
        "epoch": 0.5054666666666666,
        "step": 3791
    },
    {
        "loss": 1.1944,
        "grad_norm": 7.090459823608398,
        "learning_rate": 2.8393903095602824e-05,
        "epoch": 0.5056,
        "step": 3792
    },
    {
        "loss": 2.123,
        "grad_norm": 3.191624402999878,
        "learning_rate": 2.8349985055022366e-05,
        "epoch": 0.5057333333333334,
        "step": 3793
    },
    {
        "loss": 2.7072,
        "grad_norm": 1.7611008882522583,
        "learning_rate": 2.8306095395257004e-05,
        "epoch": 0.5058666666666667,
        "step": 3794
    },
    {
        "loss": 3.2479,
        "grad_norm": 2.363389015197754,
        "learning_rate": 2.826223413369151e-05,
        "epoch": 0.506,
        "step": 3795
    },
    {
        "loss": 2.1396,
        "grad_norm": 3.812371015548706,
        "learning_rate": 2.8218401287699525e-05,
        "epoch": 0.5061333333333333,
        "step": 3796
    },
    {
        "loss": 2.3166,
        "grad_norm": 3.4981255531311035,
        "learning_rate": 2.8174596874643367e-05,
        "epoch": 0.5062666666666666,
        "step": 3797
    },
    {
        "loss": 2.3272,
        "grad_norm": 2.261953592300415,
        "learning_rate": 2.8130820911874132e-05,
        "epoch": 0.5064,
        "step": 3798
    },
    {
        "loss": 3.2079,
        "grad_norm": 3.8247523307800293,
        "learning_rate": 2.808707341673166e-05,
        "epoch": 0.5065333333333333,
        "step": 3799
    },
    {
        "loss": 2.3574,
        "grad_norm": 2.8698365688323975,
        "learning_rate": 2.8043354406544407e-05,
        "epoch": 0.5066666666666667,
        "step": 3800
    },
    {
        "loss": 3.0965,
        "grad_norm": 2.6568734645843506,
        "learning_rate": 2.7999663898629723e-05,
        "epoch": 0.5068,
        "step": 3801
    },
    {
        "loss": 2.1219,
        "grad_norm": 4.167327404022217,
        "learning_rate": 2.7956001910293417e-05,
        "epoch": 0.5069333333333333,
        "step": 3802
    },
    {
        "loss": 1.9727,
        "grad_norm": 3.905815362930298,
        "learning_rate": 2.7912368458830297e-05,
        "epoch": 0.5070666666666667,
        "step": 3803
    },
    {
        "loss": 1.9063,
        "grad_norm": 3.642162322998047,
        "learning_rate": 2.7868763561523625e-05,
        "epoch": 0.5072,
        "step": 3804
    },
    {
        "loss": 2.5264,
        "grad_norm": 3.7977564334869385,
        "learning_rate": 2.782518723564552e-05,
        "epoch": 0.5073333333333333,
        "step": 3805
    },
    {
        "loss": 2.3958,
        "grad_norm": 3.6782891750335693,
        "learning_rate": 2.7781639498456624e-05,
        "epoch": 0.5074666666666666,
        "step": 3806
    },
    {
        "loss": 1.5723,
        "grad_norm": 5.478456020355225,
        "learning_rate": 2.773812036720649e-05,
        "epoch": 0.5076,
        "step": 3807
    },
    {
        "loss": 2.479,
        "grad_norm": 2.0084855556488037,
        "learning_rate": 2.7694629859133047e-05,
        "epoch": 0.5077333333333334,
        "step": 3808
    },
    {
        "loss": 2.5096,
        "grad_norm": 3.059523820877075,
        "learning_rate": 2.765116799146311e-05,
        "epoch": 0.5078666666666667,
        "step": 3809
    },
    {
        "loss": 3.2927,
        "grad_norm": 3.0029635429382324,
        "learning_rate": 2.7607734781412044e-05,
        "epoch": 0.508,
        "step": 3810
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.473902463912964,
        "learning_rate": 2.756433024618389e-05,
        "epoch": 0.5081333333333333,
        "step": 3811
    },
    {
        "loss": 1.9926,
        "grad_norm": 3.5302295684814453,
        "learning_rate": 2.7520954402971344e-05,
        "epoch": 0.5082666666666666,
        "step": 3812
    },
    {
        "loss": 2.1685,
        "grad_norm": 2.99800968170166,
        "learning_rate": 2.747760726895575e-05,
        "epoch": 0.5084,
        "step": 3813
    },
    {
        "loss": 2.5511,
        "grad_norm": 2.3530101776123047,
        "learning_rate": 2.743428886130701e-05,
        "epoch": 0.5085333333333333,
        "step": 3814
    },
    {
        "loss": 2.2904,
        "grad_norm": 2.275994062423706,
        "learning_rate": 2.739099919718374e-05,
        "epoch": 0.5086666666666667,
        "step": 3815
    },
    {
        "loss": 2.3134,
        "grad_norm": 2.230041980743408,
        "learning_rate": 2.7347738293733062e-05,
        "epoch": 0.5088,
        "step": 3816
    },
    {
        "loss": 2.0432,
        "grad_norm": 3.533447027206421,
        "learning_rate": 2.7304506168090838e-05,
        "epoch": 0.5089333333333333,
        "step": 3817
    },
    {
        "loss": 2.7199,
        "grad_norm": 2.8627820014953613,
        "learning_rate": 2.7261302837381398e-05,
        "epoch": 0.5090666666666667,
        "step": 3818
    },
    {
        "loss": 2.7305,
        "grad_norm": 2.8799707889556885,
        "learning_rate": 2.7218128318717752e-05,
        "epoch": 0.5092,
        "step": 3819
    },
    {
        "loss": 2.0964,
        "grad_norm": 2.7536826133728027,
        "learning_rate": 2.7174982629201505e-05,
        "epoch": 0.5093333333333333,
        "step": 3820
    },
    {
        "loss": 1.4741,
        "grad_norm": 2.2730226516723633,
        "learning_rate": 2.713186578592276e-05,
        "epoch": 0.5094666666666666,
        "step": 3821
    },
    {
        "loss": 2.2205,
        "grad_norm": 4.206430912017822,
        "learning_rate": 2.7088777805960263e-05,
        "epoch": 0.5096,
        "step": 3822
    },
    {
        "loss": 2.3545,
        "grad_norm": 2.5144922733306885,
        "learning_rate": 2.7045718706381317e-05,
        "epoch": 0.5097333333333334,
        "step": 3823
    },
    {
        "loss": 2.1067,
        "grad_norm": 3.87921404838562,
        "learning_rate": 2.7002688504241802e-05,
        "epoch": 0.5098666666666667,
        "step": 3824
    },
    {
        "loss": 1.5823,
        "grad_norm": 4.507132053375244,
        "learning_rate": 2.6959687216586084e-05,
        "epoch": 0.51,
        "step": 3825
    },
    {
        "loss": 2.7135,
        "grad_norm": 3.2555830478668213,
        "learning_rate": 2.691671486044719e-05,
        "epoch": 0.5101333333333333,
        "step": 3826
    },
    {
        "loss": 2.4757,
        "grad_norm": 3.4792394638061523,
        "learning_rate": 2.6873771452846474e-05,
        "epoch": 0.5102666666666666,
        "step": 3827
    },
    {
        "loss": 2.407,
        "grad_norm": 2.867988348007202,
        "learning_rate": 2.683085701079412e-05,
        "epoch": 0.5104,
        "step": 3828
    },
    {
        "loss": 2.6275,
        "grad_norm": 3.9190497398376465,
        "learning_rate": 2.6787971551288594e-05,
        "epoch": 0.5105333333333333,
        "step": 3829
    },
    {
        "loss": 0.8373,
        "grad_norm": 3.2133758068084717,
        "learning_rate": 2.674511509131703e-05,
        "epoch": 0.5106666666666667,
        "step": 3830
    },
    {
        "loss": 2.5617,
        "grad_norm": 2.704582691192627,
        "learning_rate": 2.670228764785493e-05,
        "epoch": 0.5108,
        "step": 3831
    },
    {
        "loss": 2.5433,
        "grad_norm": 3.458411693572998,
        "learning_rate": 2.665948923786653e-05,
        "epoch": 0.5109333333333334,
        "step": 3832
    },
    {
        "loss": 1.9583,
        "grad_norm": 4.324947357177734,
        "learning_rate": 2.6616719878304297e-05,
        "epoch": 0.5110666666666667,
        "step": 3833
    },
    {
        "loss": 2.2076,
        "grad_norm": 3.536123275756836,
        "learning_rate": 2.6573979586109387e-05,
        "epoch": 0.5112,
        "step": 3834
    },
    {
        "loss": 1.6332,
        "grad_norm": 3.6902077198028564,
        "learning_rate": 2.6531268378211328e-05,
        "epoch": 0.5113333333333333,
        "step": 3835
    },
    {
        "loss": 2.7688,
        "grad_norm": 2.981901168823242,
        "learning_rate": 2.6488586271528203e-05,
        "epoch": 0.5114666666666666,
        "step": 3836
    },
    {
        "loss": 1.2282,
        "grad_norm": 3.6237106323242188,
        "learning_rate": 2.644593328296655e-05,
        "epoch": 0.5116,
        "step": 3837
    },
    {
        "loss": 2.1421,
        "grad_norm": 2.2214627265930176,
        "learning_rate": 2.640330942942134e-05,
        "epoch": 0.5117333333333334,
        "step": 3838
    },
    {
        "loss": 2.7115,
        "grad_norm": 2.8192150592803955,
        "learning_rate": 2.636071472777607e-05,
        "epoch": 0.5118666666666667,
        "step": 3839
    },
    {
        "loss": 1.9531,
        "grad_norm": 3.5541701316833496,
        "learning_rate": 2.63181491949026e-05,
        "epoch": 0.512,
        "step": 3840
    },
    {
        "loss": 2.4268,
        "grad_norm": 4.670954704284668,
        "learning_rate": 2.6275612847661313e-05,
        "epoch": 0.5121333333333333,
        "step": 3841
    },
    {
        "loss": 2.4049,
        "grad_norm": 2.866504669189453,
        "learning_rate": 2.6233105702900962e-05,
        "epoch": 0.5122666666666666,
        "step": 3842
    },
    {
        "loss": 2.3569,
        "grad_norm": 2.393187999725342,
        "learning_rate": 2.619062777745883e-05,
        "epoch": 0.5124,
        "step": 3843
    },
    {
        "loss": 2.9094,
        "grad_norm": 2.561382532119751,
        "learning_rate": 2.6148179088160498e-05,
        "epoch": 0.5125333333333333,
        "step": 3844
    },
    {
        "loss": 1.4525,
        "grad_norm": 3.4247989654541016,
        "learning_rate": 2.6105759651820104e-05,
        "epoch": 0.5126666666666667,
        "step": 3845
    },
    {
        "loss": 1.4729,
        "grad_norm": 3.504004955291748,
        "learning_rate": 2.6063369485240073e-05,
        "epoch": 0.5128,
        "step": 3846
    },
    {
        "loss": 2.2386,
        "grad_norm": 2.518073558807373,
        "learning_rate": 2.6021008605211328e-05,
        "epoch": 0.5129333333333334,
        "step": 3847
    },
    {
        "loss": 2.3588,
        "grad_norm": 4.359366416931152,
        "learning_rate": 2.5978677028513144e-05,
        "epoch": 0.5130666666666667,
        "step": 3848
    },
    {
        "loss": 2.4546,
        "grad_norm": 2.0706396102905273,
        "learning_rate": 2.593637477191324e-05,
        "epoch": 0.5132,
        "step": 3849
    },
    {
        "loss": 2.3982,
        "grad_norm": 2.3596115112304688,
        "learning_rate": 2.589410185216763e-05,
        "epoch": 0.5133333333333333,
        "step": 3850
    },
    {
        "loss": 2.6687,
        "grad_norm": 2.9165425300598145,
        "learning_rate": 2.5851858286020835e-05,
        "epoch": 0.5134666666666666,
        "step": 3851
    },
    {
        "loss": 0.8283,
        "grad_norm": 3.2086033821105957,
        "learning_rate": 2.5809644090205575e-05,
        "epoch": 0.5136,
        "step": 3852
    },
    {
        "loss": 2.3212,
        "grad_norm": 2.2248239517211914,
        "learning_rate": 2.5767459281443087e-05,
        "epoch": 0.5137333333333334,
        "step": 3853
    },
    {
        "loss": 2.7677,
        "grad_norm": 3.0341248512268066,
        "learning_rate": 2.5725303876442918e-05,
        "epoch": 0.5138666666666667,
        "step": 3854
    },
    {
        "loss": 2.1923,
        "grad_norm": 2.885977029800415,
        "learning_rate": 2.5683177891903e-05,
        "epoch": 0.514,
        "step": 3855
    },
    {
        "loss": 1.9275,
        "grad_norm": 3.2654542922973633,
        "learning_rate": 2.5641081344509488e-05,
        "epoch": 0.5141333333333333,
        "step": 3856
    },
    {
        "loss": 2.7924,
        "grad_norm": 2.197878837585449,
        "learning_rate": 2.5599014250937114e-05,
        "epoch": 0.5142666666666666,
        "step": 3857
    },
    {
        "loss": 2.8047,
        "grad_norm": 2.0808041095733643,
        "learning_rate": 2.555697662784866e-05,
        "epoch": 0.5144,
        "step": 3858
    },
    {
        "loss": 2.2368,
        "grad_norm": 2.4222817420959473,
        "learning_rate": 2.551496849189541e-05,
        "epoch": 0.5145333333333333,
        "step": 3859
    },
    {
        "loss": 2.6903,
        "grad_norm": 2.693307399749756,
        "learning_rate": 2.547298985971698e-05,
        "epoch": 0.5146666666666667,
        "step": 3860
    },
    {
        "loss": 2.2701,
        "grad_norm": 2.381359815597534,
        "learning_rate": 2.5431040747941172e-05,
        "epoch": 0.5148,
        "step": 3861
    },
    {
        "loss": 3.0167,
        "grad_norm": 3.2839720249176025,
        "learning_rate": 2.5389121173184215e-05,
        "epoch": 0.5149333333333334,
        "step": 3862
    },
    {
        "loss": 2.2034,
        "grad_norm": 2.511608600616455,
        "learning_rate": 2.5347231152050598e-05,
        "epoch": 0.5150666666666667,
        "step": 3863
    },
    {
        "loss": 2.484,
        "grad_norm": 2.6226882934570312,
        "learning_rate": 2.5305370701133123e-05,
        "epoch": 0.5152,
        "step": 3864
    },
    {
        "loss": 1.539,
        "grad_norm": 4.297103404998779,
        "learning_rate": 2.5263539837012794e-05,
        "epoch": 0.5153333333333333,
        "step": 3865
    },
    {
        "loss": 2.3658,
        "grad_norm": 2.9091246128082275,
        "learning_rate": 2.5221738576259025e-05,
        "epoch": 0.5154666666666666,
        "step": 3866
    },
    {
        "loss": 2.596,
        "grad_norm": 2.6779913902282715,
        "learning_rate": 2.5179966935429377e-05,
        "epoch": 0.5156,
        "step": 3867
    },
    {
        "loss": 1.9223,
        "grad_norm": 3.8510406017303467,
        "learning_rate": 2.5138224931069798e-05,
        "epoch": 0.5157333333333334,
        "step": 3868
    },
    {
        "loss": 1.7332,
        "grad_norm": 3.75811767578125,
        "learning_rate": 2.5096512579714383e-05,
        "epoch": 0.5158666666666667,
        "step": 3869
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.830630302429199,
        "learning_rate": 2.505482989788559e-05,
        "epoch": 0.516,
        "step": 3870
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.8986949920654297,
        "learning_rate": 2.5013176902094016e-05,
        "epoch": 0.5161333333333333,
        "step": 3871
    },
    {
        "loss": 2.7795,
        "grad_norm": 1.9637410640716553,
        "learning_rate": 2.4971553608838583e-05,
        "epoch": 0.5162666666666667,
        "step": 3872
    },
    {
        "loss": 2.0893,
        "grad_norm": 2.6570842266082764,
        "learning_rate": 2.4929960034606413e-05,
        "epoch": 0.5164,
        "step": 3873
    },
    {
        "loss": 2.6404,
        "grad_norm": 3.154360294342041,
        "learning_rate": 2.48883961958729e-05,
        "epoch": 0.5165333333333333,
        "step": 3874
    },
    {
        "loss": 2.4669,
        "grad_norm": 2.70028018951416,
        "learning_rate": 2.4846862109101564e-05,
        "epoch": 0.5166666666666667,
        "step": 3875
    },
    {
        "loss": 3.1938,
        "grad_norm": 4.608383655548096,
        "learning_rate": 2.4805357790744278e-05,
        "epoch": 0.5168,
        "step": 3876
    },
    {
        "loss": 1.9599,
        "grad_norm": 4.207776069641113,
        "learning_rate": 2.476388325724094e-05,
        "epoch": 0.5169333333333334,
        "step": 3877
    },
    {
        "loss": 2.5444,
        "grad_norm": 4.419205665588379,
        "learning_rate": 2.4722438525019765e-05,
        "epoch": 0.5170666666666667,
        "step": 3878
    },
    {
        "loss": 2.6776,
        "grad_norm": 3.0030367374420166,
        "learning_rate": 2.4681023610497254e-05,
        "epoch": 0.5172,
        "step": 3879
    },
    {
        "loss": 0.5947,
        "grad_norm": 2.898000478744507,
        "learning_rate": 2.4639638530077912e-05,
        "epoch": 0.5173333333333333,
        "step": 3880
    },
    {
        "loss": 2.1159,
        "grad_norm": 2.7711620330810547,
        "learning_rate": 2.4598283300154522e-05,
        "epoch": 0.5174666666666666,
        "step": 3881
    },
    {
        "loss": 1.7804,
        "grad_norm": 4.443507671356201,
        "learning_rate": 2.4556957937108048e-05,
        "epoch": 0.5176,
        "step": 3882
    },
    {
        "loss": 2.3645,
        "grad_norm": 1.5862370729446411,
        "learning_rate": 2.4515662457307663e-05,
        "epoch": 0.5177333333333334,
        "step": 3883
    },
    {
        "loss": 2.1175,
        "grad_norm": 2.209667921066284,
        "learning_rate": 2.4474396877110518e-05,
        "epoch": 0.5178666666666667,
        "step": 3884
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.1899521350860596,
        "learning_rate": 2.4433161212862153e-05,
        "epoch": 0.518,
        "step": 3885
    },
    {
        "loss": 2.6415,
        "grad_norm": 2.155191421508789,
        "learning_rate": 2.4391955480896088e-05,
        "epoch": 0.5181333333333333,
        "step": 3886
    },
    {
        "loss": 2.3492,
        "grad_norm": 2.794466257095337,
        "learning_rate": 2.435077969753409e-05,
        "epoch": 0.5182666666666667,
        "step": 3887
    },
    {
        "loss": 2.503,
        "grad_norm": 2.436957359313965,
        "learning_rate": 2.4309633879086014e-05,
        "epoch": 0.5184,
        "step": 3888
    },
    {
        "loss": 2.1152,
        "grad_norm": 5.614144802093506,
        "learning_rate": 2.4268518041849898e-05,
        "epoch": 0.5185333333333333,
        "step": 3889
    },
    {
        "loss": 2.7123,
        "grad_norm": 3.6223788261413574,
        "learning_rate": 2.42274322021118e-05,
        "epoch": 0.5186666666666667,
        "step": 3890
    },
    {
        "loss": 2.3562,
        "grad_norm": 2.7236831188201904,
        "learning_rate": 2.4186376376146034e-05,
        "epoch": 0.5188,
        "step": 3891
    },
    {
        "loss": 1.7124,
        "grad_norm": 3.3745510578155518,
        "learning_rate": 2.4145350580214886e-05,
        "epoch": 0.5189333333333334,
        "step": 3892
    },
    {
        "loss": 2.7247,
        "grad_norm": 2.6297686100006104,
        "learning_rate": 2.4104354830568866e-05,
        "epoch": 0.5190666666666667,
        "step": 3893
    },
    {
        "loss": 2.7726,
        "grad_norm": 1.622947335243225,
        "learning_rate": 2.406338914344648e-05,
        "epoch": 0.5192,
        "step": 3894
    },
    {
        "loss": 3.083,
        "grad_norm": 5.189888954162598,
        "learning_rate": 2.402245353507442e-05,
        "epoch": 0.5193333333333333,
        "step": 3895
    },
    {
        "loss": 1.8443,
        "grad_norm": 2.9875235557556152,
        "learning_rate": 2.398154802166739e-05,
        "epoch": 0.5194666666666666,
        "step": 3896
    },
    {
        "loss": 2.5014,
        "grad_norm": 2.4205379486083984,
        "learning_rate": 2.3940672619428194e-05,
        "epoch": 0.5196,
        "step": 3897
    },
    {
        "loss": 2.3128,
        "grad_norm": 2.4342474937438965,
        "learning_rate": 2.3899827344547753e-05,
        "epoch": 0.5197333333333334,
        "step": 3898
    },
    {
        "loss": 2.0904,
        "grad_norm": 2.8327510356903076,
        "learning_rate": 2.3859012213205012e-05,
        "epoch": 0.5198666666666667,
        "step": 3899
    },
    {
        "loss": 1.5971,
        "grad_norm": 4.045875549316406,
        "learning_rate": 2.3818227241566992e-05,
        "epoch": 0.52,
        "step": 3900
    },
    {
        "loss": 1.0278,
        "grad_norm": 2.0572621822357178,
        "learning_rate": 2.3777472445788728e-05,
        "epoch": 0.5201333333333333,
        "step": 3901
    },
    {
        "loss": 2.6766,
        "grad_norm": 2.8967278003692627,
        "learning_rate": 2.3736747842013397e-05,
        "epoch": 0.5202666666666667,
        "step": 3902
    },
    {
        "loss": 2.1616,
        "grad_norm": 2.558366060256958,
        "learning_rate": 2.3696053446372024e-05,
        "epoch": 0.5204,
        "step": 3903
    },
    {
        "loss": 1.9042,
        "grad_norm": 3.146639823913574,
        "learning_rate": 2.365538927498394e-05,
        "epoch": 0.5205333333333333,
        "step": 3904
    },
    {
        "loss": 3.0451,
        "grad_norm": 3.4864912033081055,
        "learning_rate": 2.3614755343956275e-05,
        "epoch": 0.5206666666666667,
        "step": 3905
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.101287364959717,
        "learning_rate": 2.3574151669384315e-05,
        "epoch": 0.5208,
        "step": 3906
    },
    {
        "loss": 2.3614,
        "grad_norm": 2.417675495147705,
        "learning_rate": 2.3533578267351243e-05,
        "epoch": 0.5209333333333334,
        "step": 3907
    },
    {
        "loss": 1.7578,
        "grad_norm": 2.9328901767730713,
        "learning_rate": 2.3493035153928454e-05,
        "epoch": 0.5210666666666667,
        "step": 3908
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.9978463649749756,
        "learning_rate": 2.3452522345175087e-05,
        "epoch": 0.5212,
        "step": 3909
    },
    {
        "loss": 2.7706,
        "grad_norm": 2.9476606845855713,
        "learning_rate": 2.341203985713847e-05,
        "epoch": 0.5213333333333333,
        "step": 3910
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.7200233936309814,
        "learning_rate": 2.337158770585379e-05,
        "epoch": 0.5214666666666666,
        "step": 3911
    },
    {
        "loss": 2.138,
        "grad_norm": 2.940704822540283,
        "learning_rate": 2.3331165907344345e-05,
        "epoch": 0.5216,
        "step": 3912
    },
    {
        "loss": 2.6042,
        "grad_norm": 2.5079517364501953,
        "learning_rate": 2.3290774477621313e-05,
        "epoch": 0.5217333333333334,
        "step": 3913
    },
    {
        "loss": 1.3313,
        "grad_norm": 3.288738965988159,
        "learning_rate": 2.3250413432683927e-05,
        "epoch": 0.5218666666666667,
        "step": 3914
    },
    {
        "loss": 1.4806,
        "grad_norm": 3.519012212753296,
        "learning_rate": 2.321008278851926e-05,
        "epoch": 0.522,
        "step": 3915
    },
    {
        "loss": 0.9895,
        "grad_norm": 3.0872533321380615,
        "learning_rate": 2.3169782561102492e-05,
        "epoch": 0.5221333333333333,
        "step": 3916
    },
    {
        "loss": 2.4241,
        "grad_norm": 2.9296534061431885,
        "learning_rate": 2.312951276639661e-05,
        "epoch": 0.5222666666666667,
        "step": 3917
    },
    {
        "loss": 2.6489,
        "grad_norm": 2.832805633544922,
        "learning_rate": 2.3089273420352665e-05,
        "epoch": 0.5224,
        "step": 3918
    },
    {
        "loss": 2.0304,
        "grad_norm": 2.724071502685547,
        "learning_rate": 2.3049064538909592e-05,
        "epoch": 0.5225333333333333,
        "step": 3919
    },
    {
        "loss": 2.2827,
        "grad_norm": 2.978391170501709,
        "learning_rate": 2.3008886137994245e-05,
        "epoch": 0.5226666666666666,
        "step": 3920
    },
    {
        "loss": 3.0008,
        "grad_norm": 4.052328109741211,
        "learning_rate": 2.296873823352147e-05,
        "epoch": 0.5228,
        "step": 3921
    },
    {
        "loss": 2.569,
        "grad_norm": 3.1037042140960693,
        "learning_rate": 2.292862084139392e-05,
        "epoch": 0.5229333333333334,
        "step": 3922
    },
    {
        "loss": 2.6924,
        "grad_norm": 3.065600633621216,
        "learning_rate": 2.288853397750228e-05,
        "epoch": 0.5230666666666667,
        "step": 3923
    },
    {
        "loss": 2.4633,
        "grad_norm": 4.05072021484375,
        "learning_rate": 2.2848477657725075e-05,
        "epoch": 0.5232,
        "step": 3924
    },
    {
        "loss": 2.6196,
        "grad_norm": 2.8002214431762695,
        "learning_rate": 2.2808451897928785e-05,
        "epoch": 0.5233333333333333,
        "step": 3925
    },
    {
        "loss": 2.0512,
        "grad_norm": 3.5691146850585938,
        "learning_rate": 2.27684567139677e-05,
        "epoch": 0.5234666666666666,
        "step": 3926
    },
    {
        "loss": 1.6173,
        "grad_norm": 3.0761866569519043,
        "learning_rate": 2.272849212168412e-05,
        "epoch": 0.5236,
        "step": 3927
    },
    {
        "loss": 2.5417,
        "grad_norm": 4.009677410125732,
        "learning_rate": 2.2688558136908022e-05,
        "epoch": 0.5237333333333334,
        "step": 3928
    },
    {
        "loss": 1.2152,
        "grad_norm": 3.874870777130127,
        "learning_rate": 2.2648654775457535e-05,
        "epoch": 0.5238666666666667,
        "step": 3929
    },
    {
        "loss": 1.6179,
        "grad_norm": 3.1107451915740967,
        "learning_rate": 2.2608782053138433e-05,
        "epoch": 0.524,
        "step": 3930
    },
    {
        "loss": 2.332,
        "grad_norm": 2.649606227874756,
        "learning_rate": 2.2568939985744496e-05,
        "epoch": 0.5241333333333333,
        "step": 3931
    },
    {
        "loss": 2.9744,
        "grad_norm": 1.9968551397323608,
        "learning_rate": 2.2529128589057212e-05,
        "epoch": 0.5242666666666667,
        "step": 3932
    },
    {
        "loss": 2.7565,
        "grad_norm": 2.980346441268921,
        "learning_rate": 2.2489347878846156e-05,
        "epoch": 0.5244,
        "step": 3933
    },
    {
        "loss": 2.1743,
        "grad_norm": 3.208111047744751,
        "learning_rate": 2.2449597870868456e-05,
        "epoch": 0.5245333333333333,
        "step": 3934
    },
    {
        "loss": 2.2652,
        "grad_norm": 3.537442684173584,
        "learning_rate": 2.2409878580869313e-05,
        "epoch": 0.5246666666666666,
        "step": 3935
    },
    {
        "loss": 1.0176,
        "grad_norm": 3.4757676124572754,
        "learning_rate": 2.2370190024581615e-05,
        "epoch": 0.5248,
        "step": 3936
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.675386667251587,
        "learning_rate": 2.2330532217726173e-05,
        "epoch": 0.5249333333333334,
        "step": 3937
    },
    {
        "loss": 2.012,
        "grad_norm": 1.4793423414230347,
        "learning_rate": 2.2290905176011567e-05,
        "epoch": 0.5250666666666667,
        "step": 3938
    },
    {
        "loss": 1.7562,
        "grad_norm": 4.023040771484375,
        "learning_rate": 2.2251308915134217e-05,
        "epoch": 0.5252,
        "step": 3939
    },
    {
        "loss": 2.4897,
        "grad_norm": 2.067314386367798,
        "learning_rate": 2.2211743450778345e-05,
        "epoch": 0.5253333333333333,
        "step": 3940
    },
    {
        "loss": 2.4587,
        "grad_norm": 2.1944828033447266,
        "learning_rate": 2.2172208798615923e-05,
        "epoch": 0.5254666666666666,
        "step": 3941
    },
    {
        "loss": 1.1483,
        "grad_norm": 3.857787609100342,
        "learning_rate": 2.2132704974306817e-05,
        "epoch": 0.5256,
        "step": 3942
    },
    {
        "loss": 2.0907,
        "grad_norm": 3.96187424659729,
        "learning_rate": 2.2093231993498576e-05,
        "epoch": 0.5257333333333334,
        "step": 3943
    },
    {
        "loss": 0.7803,
        "grad_norm": 2.079665422439575,
        "learning_rate": 2.2053789871826624e-05,
        "epoch": 0.5258666666666667,
        "step": 3944
    },
    {
        "loss": 0.6617,
        "grad_norm": 3.44016432762146,
        "learning_rate": 2.2014378624914068e-05,
        "epoch": 0.526,
        "step": 3945
    },
    {
        "loss": 2.5482,
        "grad_norm": 4.540586948394775,
        "learning_rate": 2.1974998268371903e-05,
        "epoch": 0.5261333333333333,
        "step": 3946
    },
    {
        "loss": 2.588,
        "grad_norm": 3.6781418323516846,
        "learning_rate": 2.193564881779876e-05,
        "epoch": 0.5262666666666667,
        "step": 3947
    },
    {
        "loss": 0.8457,
        "grad_norm": 3.4448928833007812,
        "learning_rate": 2.189633028878111e-05,
        "epoch": 0.5264,
        "step": 3948
    },
    {
        "loss": 1.8907,
        "grad_norm": 3.290038585662842,
        "learning_rate": 2.185704269689317e-05,
        "epoch": 0.5265333333333333,
        "step": 3949
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.5061557292938232,
        "learning_rate": 2.181778605769691e-05,
        "epoch": 0.5266666666666666,
        "step": 3950
    },
    {
        "loss": 1.7555,
        "grad_norm": 5.194380283355713,
        "learning_rate": 2.1778560386741965e-05,
        "epoch": 0.5268,
        "step": 3951
    },
    {
        "loss": 2.8148,
        "grad_norm": 2.569359302520752,
        "learning_rate": 2.1739365699565837e-05,
        "epoch": 0.5269333333333334,
        "step": 3952
    },
    {
        "loss": 2.2254,
        "grad_norm": 3.0929720401763916,
        "learning_rate": 2.1700202011693573e-05,
        "epoch": 0.5270666666666667,
        "step": 3953
    },
    {
        "loss": 2.815,
        "grad_norm": 2.7241222858428955,
        "learning_rate": 2.1661069338638084e-05,
        "epoch": 0.5272,
        "step": 3954
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.9607491493225098,
        "learning_rate": 2.162196769589997e-05,
        "epoch": 0.5273333333333333,
        "step": 3955
    },
    {
        "loss": 1.8261,
        "grad_norm": 3.4620094299316406,
        "learning_rate": 2.1582897098967538e-05,
        "epoch": 0.5274666666666666,
        "step": 3956
    },
    {
        "loss": 2.7747,
        "grad_norm": 3.7925143241882324,
        "learning_rate": 2.1543857563316715e-05,
        "epoch": 0.5276,
        "step": 3957
    },
    {
        "loss": 3.1334,
        "grad_norm": 2.70920467376709,
        "learning_rate": 2.1504849104411327e-05,
        "epoch": 0.5277333333333334,
        "step": 3958
    },
    {
        "loss": 2.233,
        "grad_norm": 3.5104317665100098,
        "learning_rate": 2.1465871737702615e-05,
        "epoch": 0.5278666666666667,
        "step": 3959
    },
    {
        "loss": 2.4807,
        "grad_norm": 2.9280784130096436,
        "learning_rate": 2.142692547862971e-05,
        "epoch": 0.528,
        "step": 3960
    },
    {
        "loss": 2.558,
        "grad_norm": 3.3311383724212646,
        "learning_rate": 2.138801034261938e-05,
        "epoch": 0.5281333333333333,
        "step": 3961
    },
    {
        "loss": 2.6726,
        "grad_norm": 2.3671724796295166,
        "learning_rate": 2.134912634508599e-05,
        "epoch": 0.5282666666666667,
        "step": 3962
    },
    {
        "loss": 1.9342,
        "grad_norm": 3.4274518489837646,
        "learning_rate": 2.1310273501431654e-05,
        "epoch": 0.5284,
        "step": 3963
    },
    {
        "loss": 2.4376,
        "grad_norm": 4.5298542976379395,
        "learning_rate": 2.1271451827046108e-05,
        "epoch": 0.5285333333333333,
        "step": 3964
    },
    {
        "loss": 2.5711,
        "grad_norm": 2.83542799949646,
        "learning_rate": 2.123266133730678e-05,
        "epoch": 0.5286666666666666,
        "step": 3965
    },
    {
        "loss": 2.3199,
        "grad_norm": 2.967186689376831,
        "learning_rate": 2.1193902047578673e-05,
        "epoch": 0.5288,
        "step": 3966
    },
    {
        "loss": 3.1506,
        "grad_norm": 2.4026710987091064,
        "learning_rate": 2.1155173973214503e-05,
        "epoch": 0.5289333333333334,
        "step": 3967
    },
    {
        "loss": 2.8072,
        "grad_norm": 1.8401834964752197,
        "learning_rate": 2.1116477129554557e-05,
        "epoch": 0.5290666666666667,
        "step": 3968
    },
    {
        "loss": 2.778,
        "grad_norm": 2.5412635803222656,
        "learning_rate": 2.1077811531926838e-05,
        "epoch": 0.5292,
        "step": 3969
    },
    {
        "loss": 2.2185,
        "grad_norm": 4.2757744789123535,
        "learning_rate": 2.1039177195646874e-05,
        "epoch": 0.5293333333333333,
        "step": 3970
    },
    {
        "loss": 2.1204,
        "grad_norm": 4.178956508636475,
        "learning_rate": 2.100057413601789e-05,
        "epoch": 0.5294666666666666,
        "step": 3971
    },
    {
        "loss": 1.9749,
        "grad_norm": 3.073716163635254,
        "learning_rate": 2.0962002368330657e-05,
        "epoch": 0.5296,
        "step": 3972
    },
    {
        "loss": 2.2433,
        "grad_norm": 1.8673102855682373,
        "learning_rate": 2.0923461907863595e-05,
        "epoch": 0.5297333333333333,
        "step": 3973
    },
    {
        "loss": 3.0426,
        "grad_norm": 2.8832743167877197,
        "learning_rate": 2.0884952769882716e-05,
        "epoch": 0.5298666666666667,
        "step": 3974
    },
    {
        "loss": 2.9685,
        "grad_norm": 1.5657060146331787,
        "learning_rate": 2.084647496964165e-05,
        "epoch": 0.53,
        "step": 3975
    },
    {
        "loss": 0.7846,
        "grad_norm": 3.0476927757263184,
        "learning_rate": 2.0808028522381527e-05,
        "epoch": 0.5301333333333333,
        "step": 3976
    },
    {
        "loss": 1.4457,
        "grad_norm": 4.654890537261963,
        "learning_rate": 2.0769613443331214e-05,
        "epoch": 0.5302666666666667,
        "step": 3977
    },
    {
        "loss": 1.3813,
        "grad_norm": 4.044745445251465,
        "learning_rate": 2.0731229747706925e-05,
        "epoch": 0.5304,
        "step": 3978
    },
    {
        "loss": 1.0148,
        "grad_norm": 4.626986026763916,
        "learning_rate": 2.069287745071261e-05,
        "epoch": 0.5305333333333333,
        "step": 3979
    },
    {
        "loss": 2.6027,
        "grad_norm": 2.2376270294189453,
        "learning_rate": 2.0654556567539817e-05,
        "epoch": 0.5306666666666666,
        "step": 3980
    },
    {
        "loss": 2.1373,
        "grad_norm": 2.435879945755005,
        "learning_rate": 2.0616267113367504e-05,
        "epoch": 0.5308,
        "step": 3981
    },
    {
        "loss": 0.9011,
        "grad_norm": 4.4493088722229,
        "learning_rate": 2.0578009103362295e-05,
        "epoch": 0.5309333333333334,
        "step": 3982
    },
    {
        "loss": 1.8689,
        "grad_norm": 3.3278214931488037,
        "learning_rate": 2.053978255267829e-05,
        "epoch": 0.5310666666666667,
        "step": 3983
    },
    {
        "loss": 2.0791,
        "grad_norm": 2.887653112411499,
        "learning_rate": 2.050158747645723e-05,
        "epoch": 0.5312,
        "step": 3984
    },
    {
        "loss": 2.6838,
        "grad_norm": 2.6183950901031494,
        "learning_rate": 2.0463423889828193e-05,
        "epoch": 0.5313333333333333,
        "step": 3985
    },
    {
        "loss": 2.8012,
        "grad_norm": 1.614138126373291,
        "learning_rate": 2.0425291807908e-05,
        "epoch": 0.5314666666666666,
        "step": 3986
    },
    {
        "loss": 0.5348,
        "grad_norm": 2.512660264968872,
        "learning_rate": 2.0387191245800842e-05,
        "epoch": 0.5316,
        "step": 3987
    },
    {
        "loss": 2.6286,
        "grad_norm": 2.0355803966522217,
        "learning_rate": 2.0349122218598494e-05,
        "epoch": 0.5317333333333333,
        "step": 3988
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.869934320449829,
        "learning_rate": 2.031108474138025e-05,
        "epoch": 0.5318666666666667,
        "step": 3989
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.2400968074798584,
        "learning_rate": 2.02730788292129e-05,
        "epoch": 0.532,
        "step": 3990
    },
    {
        "loss": 2.3802,
        "grad_norm": 3.588930130004883,
        "learning_rate": 2.023510449715066e-05,
        "epoch": 0.5321333333333333,
        "step": 3991
    },
    {
        "loss": 1.8788,
        "grad_norm": 1.9125720262527466,
        "learning_rate": 2.019716176023534e-05,
        "epoch": 0.5322666666666667,
        "step": 3992
    },
    {
        "loss": 2.1306,
        "grad_norm": 3.480764389038086,
        "learning_rate": 2.0159250633496153e-05,
        "epoch": 0.5324,
        "step": 3993
    },
    {
        "loss": 2.6262,
        "grad_norm": 7.443347930908203,
        "learning_rate": 2.012137113194986e-05,
        "epoch": 0.5325333333333333,
        "step": 3994
    },
    {
        "loss": 2.4139,
        "grad_norm": 2.898817539215088,
        "learning_rate": 2.0083523270600624e-05,
        "epoch": 0.5326666666666666,
        "step": 3995
    },
    {
        "loss": 2.4013,
        "grad_norm": 3.4890177249908447,
        "learning_rate": 2.0045707064440145e-05,
        "epoch": 0.5328,
        "step": 3996
    },
    {
        "loss": 2.3271,
        "grad_norm": 2.7884879112243652,
        "learning_rate": 2.000792252844751e-05,
        "epoch": 0.5329333333333334,
        "step": 3997
    },
    {
        "loss": 2.6839,
        "grad_norm": 1.9733113050460815,
        "learning_rate": 1.9970169677589335e-05,
        "epoch": 0.5330666666666667,
        "step": 3998
    },
    {
        "loss": 2.7699,
        "grad_norm": 4.205593585968018,
        "learning_rate": 1.9932448526819637e-05,
        "epoch": 0.5332,
        "step": 3999
    },
    {
        "loss": 2.5662,
        "grad_norm": 3.4594483375549316,
        "learning_rate": 1.9894759091079907e-05,
        "epoch": 0.5333333333333333,
        "step": 4000
    },
    {
        "loss": 2.505,
        "grad_norm": 4.0091376304626465,
        "learning_rate": 1.985710138529908e-05,
        "epoch": 0.5334666666666666,
        "step": 4001
    },
    {
        "loss": 2.8797,
        "grad_norm": 2.602072238922119,
        "learning_rate": 1.981947542439344e-05,
        "epoch": 0.5336,
        "step": 4002
    },
    {
        "loss": 1.1494,
        "grad_norm": 3.272892475128174,
        "learning_rate": 1.9781881223266852e-05,
        "epoch": 0.5337333333333333,
        "step": 4003
    },
    {
        "loss": 2.4286,
        "grad_norm": 2.8463566303253174,
        "learning_rate": 1.9744318796810367e-05,
        "epoch": 0.5338666666666667,
        "step": 4004
    },
    {
        "loss": 2.0066,
        "grad_norm": 3.617866039276123,
        "learning_rate": 1.9706788159902734e-05,
        "epoch": 0.534,
        "step": 4005
    },
    {
        "loss": 2.4111,
        "grad_norm": 1.702135682106018,
        "learning_rate": 1.9669289327409867e-05,
        "epoch": 0.5341333333333333,
        "step": 4006
    },
    {
        "loss": 2.2978,
        "grad_norm": 2.5603058338165283,
        "learning_rate": 1.9631822314185243e-05,
        "epoch": 0.5342666666666667,
        "step": 4007
    },
    {
        "loss": 1.9295,
        "grad_norm": 3.682539224624634,
        "learning_rate": 1.9594387135069603e-05,
        "epoch": 0.5344,
        "step": 4008
    },
    {
        "loss": 2.4553,
        "grad_norm": 3.3796563148498535,
        "learning_rate": 1.955698380489126e-05,
        "epoch": 0.5345333333333333,
        "step": 4009
    },
    {
        "loss": 1.729,
        "grad_norm": 2.429354190826416,
        "learning_rate": 1.9519612338465697e-05,
        "epoch": 0.5346666666666666,
        "step": 4010
    },
    {
        "loss": 2.6022,
        "grad_norm": 2.433680295944214,
        "learning_rate": 1.948227275059593e-05,
        "epoch": 0.5348,
        "step": 4011
    },
    {
        "loss": 2.561,
        "grad_norm": 2.1910688877105713,
        "learning_rate": 1.9444965056072262e-05,
        "epoch": 0.5349333333333334,
        "step": 4012
    },
    {
        "loss": 2.1094,
        "grad_norm": 4.408474445343018,
        "learning_rate": 1.9407689269672424e-05,
        "epoch": 0.5350666666666667,
        "step": 4013
    },
    {
        "loss": 1.7554,
        "grad_norm": 2.5249953269958496,
        "learning_rate": 1.9370445406161476e-05,
        "epoch": 0.5352,
        "step": 4014
    },
    {
        "loss": 3.4322,
        "grad_norm": 4.482268810272217,
        "learning_rate": 1.933323348029187e-05,
        "epoch": 0.5353333333333333,
        "step": 4015
    },
    {
        "loss": 2.2957,
        "grad_norm": 3.553187370300293,
        "learning_rate": 1.9296053506803314e-05,
        "epoch": 0.5354666666666666,
        "step": 4016
    },
    {
        "loss": 1.1983,
        "grad_norm": 3.6246185302734375,
        "learning_rate": 1.9258905500422986e-05,
        "epoch": 0.5356,
        "step": 4017
    },
    {
        "loss": 2.2704,
        "grad_norm": 2.825300455093384,
        "learning_rate": 1.922178947586528e-05,
        "epoch": 0.5357333333333333,
        "step": 4018
    },
    {
        "loss": 2.2965,
        "grad_norm": 3.699601411819458,
        "learning_rate": 1.9184705447832018e-05,
        "epoch": 0.5358666666666667,
        "step": 4019
    },
    {
        "loss": 2.3279,
        "grad_norm": 2.4104862213134766,
        "learning_rate": 1.9147653431012313e-05,
        "epoch": 0.536,
        "step": 4020
    },
    {
        "loss": 3.0129,
        "grad_norm": 3.471794843673706,
        "learning_rate": 1.911063344008256e-05,
        "epoch": 0.5361333333333334,
        "step": 4021
    },
    {
        "loss": 2.0187,
        "grad_norm": 3.505762815475464,
        "learning_rate": 1.9073645489706548e-05,
        "epoch": 0.5362666666666667,
        "step": 4022
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.7021889686584473,
        "learning_rate": 1.9036689594535285e-05,
        "epoch": 0.5364,
        "step": 4023
    },
    {
        "loss": 1.9594,
        "grad_norm": 3.058964967727661,
        "learning_rate": 1.8999765769207155e-05,
        "epoch": 0.5365333333333333,
        "step": 4024
    },
    {
        "loss": 2.1598,
        "grad_norm": 2.6207165718078613,
        "learning_rate": 1.89628740283478e-05,
        "epoch": 0.5366666666666666,
        "step": 4025
    },
    {
        "loss": 2.714,
        "grad_norm": 1.6847360134124756,
        "learning_rate": 1.892601438657019e-05,
        "epoch": 0.5368,
        "step": 4026
    },
    {
        "loss": 2.6287,
        "grad_norm": 2.029085636138916,
        "learning_rate": 1.8889186858474516e-05,
        "epoch": 0.5369333333333334,
        "step": 4027
    },
    {
        "loss": 2.3786,
        "grad_norm": 3.1241514682769775,
        "learning_rate": 1.8852391458648356e-05,
        "epoch": 0.5370666666666667,
        "step": 4028
    },
    {
        "loss": 2.3768,
        "grad_norm": 3.3520255088806152,
        "learning_rate": 1.8815628201666357e-05,
        "epoch": 0.5372,
        "step": 4029
    },
    {
        "loss": 2.0411,
        "grad_norm": 2.840481996536255,
        "learning_rate": 1.877889710209072e-05,
        "epoch": 0.5373333333333333,
        "step": 4030
    },
    {
        "loss": 2.6544,
        "grad_norm": 2.790123462677002,
        "learning_rate": 1.874219817447066e-05,
        "epoch": 0.5374666666666666,
        "step": 4031
    },
    {
        "loss": 2.6977,
        "grad_norm": 2.8699848651885986,
        "learning_rate": 1.8705531433342815e-05,
        "epoch": 0.5376,
        "step": 4032
    },
    {
        "loss": 1.0572,
        "grad_norm": 3.022226333618164,
        "learning_rate": 1.8668896893230913e-05,
        "epoch": 0.5377333333333333,
        "step": 4033
    },
    {
        "loss": 2.627,
        "grad_norm": 3.7876505851745605,
        "learning_rate": 1.863229456864617e-05,
        "epoch": 0.5378666666666667,
        "step": 4034
    },
    {
        "loss": 2.4366,
        "grad_norm": 2.0502452850341797,
        "learning_rate": 1.859572447408674e-05,
        "epoch": 0.538,
        "step": 4035
    },
    {
        "loss": 1.8041,
        "grad_norm": 2.8358356952667236,
        "learning_rate": 1.855918662403826e-05,
        "epoch": 0.5381333333333334,
        "step": 4036
    },
    {
        "loss": 2.289,
        "grad_norm": 3.943326711654663,
        "learning_rate": 1.852268103297342e-05,
        "epoch": 0.5382666666666667,
        "step": 4037
    },
    {
        "loss": 1.7884,
        "grad_norm": 5.992759704589844,
        "learning_rate": 1.8486207715352265e-05,
        "epoch": 0.5384,
        "step": 4038
    },
    {
        "loss": 2.8427,
        "grad_norm": 2.8024816513061523,
        "learning_rate": 1.8449766685621984e-05,
        "epoch": 0.5385333333333333,
        "step": 4039
    },
    {
        "loss": 1.8573,
        "grad_norm": 3.1965689659118652,
        "learning_rate": 1.841335795821698e-05,
        "epoch": 0.5386666666666666,
        "step": 4040
    },
    {
        "loss": 2.752,
        "grad_norm": 4.259651184082031,
        "learning_rate": 1.8376981547558924e-05,
        "epoch": 0.5388,
        "step": 4041
    },
    {
        "loss": 2.7257,
        "grad_norm": 2.0834779739379883,
        "learning_rate": 1.8340637468056576e-05,
        "epoch": 0.5389333333333334,
        "step": 4042
    },
    {
        "loss": 2.5508,
        "grad_norm": 3.279236316680908,
        "learning_rate": 1.8304325734106e-05,
        "epoch": 0.5390666666666667,
        "step": 4043
    },
    {
        "loss": 2.5402,
        "grad_norm": 2.225165367126465,
        "learning_rate": 1.8268046360090353e-05,
        "epoch": 0.5392,
        "step": 4044
    },
    {
        "loss": 3.0721,
        "grad_norm": 2.4315760135650635,
        "learning_rate": 1.8231799360380052e-05,
        "epoch": 0.5393333333333333,
        "step": 4045
    },
    {
        "loss": 2.2541,
        "grad_norm": 2.499129056930542,
        "learning_rate": 1.819558474933262e-05,
        "epoch": 0.5394666666666666,
        "step": 4046
    },
    {
        "loss": 2.1646,
        "grad_norm": 2.6685633659362793,
        "learning_rate": 1.8159402541292837e-05,
        "epoch": 0.5396,
        "step": 4047
    },
    {
        "loss": 1.8464,
        "grad_norm": 2.5685312747955322,
        "learning_rate": 1.8123252750592546e-05,
        "epoch": 0.5397333333333333,
        "step": 4048
    },
    {
        "loss": 1.8902,
        "grad_norm": 2.249255657196045,
        "learning_rate": 1.8087135391550823e-05,
        "epoch": 0.5398666666666667,
        "step": 4049
    },
    {
        "loss": 1.4994,
        "grad_norm": 8.967574119567871,
        "learning_rate": 1.8051050478473896e-05,
        "epoch": 0.54,
        "step": 4050
    },
    {
        "loss": 2.2542,
        "grad_norm": 4.032552242279053,
        "learning_rate": 1.8014998025655128e-05,
        "epoch": 0.5401333333333334,
        "step": 4051
    },
    {
        "loss": 1.925,
        "grad_norm": 3.756293773651123,
        "learning_rate": 1.797897804737497e-05,
        "epoch": 0.5402666666666667,
        "step": 4052
    },
    {
        "loss": 1.2336,
        "grad_norm": 3.3776965141296387,
        "learning_rate": 1.794299055790114e-05,
        "epoch": 0.5404,
        "step": 4053
    },
    {
        "loss": 2.3719,
        "grad_norm": 3.442497491836548,
        "learning_rate": 1.7907035571488307e-05,
        "epoch": 0.5405333333333333,
        "step": 4054
    },
    {
        "loss": 2.5991,
        "grad_norm": 2.0088908672332764,
        "learning_rate": 1.78711131023784e-05,
        "epoch": 0.5406666666666666,
        "step": 4055
    },
    {
        "loss": 1.5429,
        "grad_norm": 5.989844799041748,
        "learning_rate": 1.7835223164800442e-05,
        "epoch": 0.5408,
        "step": 4056
    },
    {
        "loss": 1.175,
        "grad_norm": 5.433992862701416,
        "learning_rate": 1.7799365772970587e-05,
        "epoch": 0.5409333333333334,
        "step": 4057
    },
    {
        "loss": 1.4775,
        "grad_norm": 2.29620361328125,
        "learning_rate": 1.7763540941091995e-05,
        "epoch": 0.5410666666666667,
        "step": 4058
    },
    {
        "loss": 3.1072,
        "grad_norm": 3.5490636825561523,
        "learning_rate": 1.772774868335506e-05,
        "epoch": 0.5412,
        "step": 4059
    },
    {
        "loss": 3.5507,
        "grad_norm": 9.018028259277344,
        "learning_rate": 1.7691989013937272e-05,
        "epoch": 0.5413333333333333,
        "step": 4060
    },
    {
        "loss": 1.9086,
        "grad_norm": 3.3215043544769287,
        "learning_rate": 1.7656261947003037e-05,
        "epoch": 0.5414666666666667,
        "step": 4061
    },
    {
        "loss": 2.2101,
        "grad_norm": 2.8924665451049805,
        "learning_rate": 1.7620567496704056e-05,
        "epoch": 0.5416,
        "step": 4062
    },
    {
        "loss": 1.9057,
        "grad_norm": 3.033489227294922,
        "learning_rate": 1.7584905677178954e-05,
        "epoch": 0.5417333333333333,
        "step": 4063
    },
    {
        "loss": 2.4481,
        "grad_norm": 2.8399572372436523,
        "learning_rate": 1.7549276502553547e-05,
        "epoch": 0.5418666666666667,
        "step": 4064
    },
    {
        "loss": 1.7898,
        "grad_norm": 3.62483811378479,
        "learning_rate": 1.7513679986940656e-05,
        "epoch": 0.542,
        "step": 4065
    },
    {
        "loss": 2.5174,
        "grad_norm": 2.479128837585449,
        "learning_rate": 1.7478116144440214e-05,
        "epoch": 0.5421333333333334,
        "step": 4066
    },
    {
        "loss": 1.9869,
        "grad_norm": 3.033029794692993,
        "learning_rate": 1.744258498913912e-05,
        "epoch": 0.5422666666666667,
        "step": 4067
    },
    {
        "loss": 2.3531,
        "grad_norm": 2.484668731689453,
        "learning_rate": 1.740708653511146e-05,
        "epoch": 0.5424,
        "step": 4068
    },
    {
        "loss": 1.9728,
        "grad_norm": 3.89082670211792,
        "learning_rate": 1.737162079641822e-05,
        "epoch": 0.5425333333333333,
        "step": 4069
    },
    {
        "loss": 2.7825,
        "grad_norm": 3.403791666030884,
        "learning_rate": 1.733618778710755e-05,
        "epoch": 0.5426666666666666,
        "step": 4070
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.7578791379928589,
        "learning_rate": 1.7300787521214545e-05,
        "epoch": 0.5428,
        "step": 4071
    },
    {
        "loss": 2.5956,
        "grad_norm": 2.0358142852783203,
        "learning_rate": 1.7265420012761423e-05,
        "epoch": 0.5429333333333334,
        "step": 4072
    },
    {
        "loss": 2.1034,
        "grad_norm": 4.140567302703857,
        "learning_rate": 1.7230085275757324e-05,
        "epoch": 0.5430666666666667,
        "step": 4073
    },
    {
        "loss": 2.6702,
        "grad_norm": 3.010425329208374,
        "learning_rate": 1.7194783324198473e-05,
        "epoch": 0.5432,
        "step": 4074
    },
    {
        "loss": 1.5298,
        "grad_norm": 4.267834663391113,
        "learning_rate": 1.71595141720681e-05,
        "epoch": 0.5433333333333333,
        "step": 4075
    },
    {
        "loss": 0.6644,
        "grad_norm": 3.2374751567840576,
        "learning_rate": 1.7124277833336453e-05,
        "epoch": 0.5434666666666667,
        "step": 4076
    },
    {
        "loss": 2.1197,
        "grad_norm": 2.5748257637023926,
        "learning_rate": 1.708907432196074e-05,
        "epoch": 0.5436,
        "step": 4077
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.3916950225830078,
        "learning_rate": 1.7053903651885238e-05,
        "epoch": 0.5437333333333333,
        "step": 4078
    },
    {
        "loss": 2.7631,
        "grad_norm": 3.271007537841797,
        "learning_rate": 1.7018765837041084e-05,
        "epoch": 0.5438666666666667,
        "step": 4079
    },
    {
        "loss": 1.3803,
        "grad_norm": 4.571915626525879,
        "learning_rate": 1.6983660891346508e-05,
        "epoch": 0.544,
        "step": 4080
    },
    {
        "loss": 2.2807,
        "grad_norm": 3.183655023574829,
        "learning_rate": 1.694858882870678e-05,
        "epoch": 0.5441333333333334,
        "step": 4081
    },
    {
        "loss": 1.8801,
        "grad_norm": 3.262810230255127,
        "learning_rate": 1.6913549663013973e-05,
        "epoch": 0.5442666666666667,
        "step": 4082
    },
    {
        "loss": 1.8222,
        "grad_norm": 3.1308107376098633,
        "learning_rate": 1.6878543408147263e-05,
        "epoch": 0.5444,
        "step": 4083
    },
    {
        "loss": 1.9696,
        "grad_norm": 3.5763285160064697,
        "learning_rate": 1.6843570077972727e-05,
        "epoch": 0.5445333333333333,
        "step": 4084
    },
    {
        "loss": 2.4743,
        "grad_norm": 2.3453104496002197,
        "learning_rate": 1.6808629686343492e-05,
        "epoch": 0.5446666666666666,
        "step": 4085
    },
    {
        "loss": 2.8328,
        "grad_norm": 2.9845404624938965,
        "learning_rate": 1.6773722247099455e-05,
        "epoch": 0.5448,
        "step": 4086
    },
    {
        "loss": 1.6434,
        "grad_norm": 1.5715150833129883,
        "learning_rate": 1.6738847774067644e-05,
        "epoch": 0.5449333333333334,
        "step": 4087
    },
    {
        "loss": 2.439,
        "grad_norm": 1.4585776329040527,
        "learning_rate": 1.670400628106191e-05,
        "epoch": 0.5450666666666667,
        "step": 4088
    },
    {
        "loss": 2.7334,
        "grad_norm": 4.340430736541748,
        "learning_rate": 1.6669197781883118e-05,
        "epoch": 0.5452,
        "step": 4089
    },
    {
        "loss": 1.9055,
        "grad_norm": 3.3353946208953857,
        "learning_rate": 1.663442229031902e-05,
        "epoch": 0.5453333333333333,
        "step": 4090
    },
    {
        "loss": 3.1013,
        "grad_norm": 2.8494796752929688,
        "learning_rate": 1.6599679820144343e-05,
        "epoch": 0.5454666666666667,
        "step": 4091
    },
    {
        "loss": 1.9315,
        "grad_norm": 4.342791557312012,
        "learning_rate": 1.656497038512065e-05,
        "epoch": 0.5456,
        "step": 4092
    },
    {
        "loss": 2.2711,
        "grad_norm": 2.7667720317840576,
        "learning_rate": 1.6530293998996493e-05,
        "epoch": 0.5457333333333333,
        "step": 4093
    },
    {
        "loss": 2.2699,
        "grad_norm": 2.897128105163574,
        "learning_rate": 1.649565067550729e-05,
        "epoch": 0.5458666666666666,
        "step": 4094
    },
    {
        "loss": 2.444,
        "grad_norm": 2.137662410736084,
        "learning_rate": 1.6461040428375407e-05,
        "epoch": 0.546,
        "step": 4095
    },
    {
        "loss": 2.569,
        "grad_norm": 2.139617681503296,
        "learning_rate": 1.6426463271310045e-05,
        "epoch": 0.5461333333333334,
        "step": 4096
    },
    {
        "loss": 2.6578,
        "grad_norm": 2.2963454723358154,
        "learning_rate": 1.6391919218007368e-05,
        "epoch": 0.5462666666666667,
        "step": 4097
    },
    {
        "loss": 2.4966,
        "grad_norm": 2.2692790031433105,
        "learning_rate": 1.6357408282150343e-05,
        "epoch": 0.5464,
        "step": 4098
    },
    {
        "loss": 1.2982,
        "grad_norm": 3.369823694229126,
        "learning_rate": 1.6322930477408916e-05,
        "epoch": 0.5465333333333333,
        "step": 4099
    },
    {
        "loss": 1.6972,
        "grad_norm": 2.0949928760528564,
        "learning_rate": 1.6288485817439846e-05,
        "epoch": 0.5466666666666666,
        "step": 4100
    },
    {
        "loss": 2.2577,
        "grad_norm": 1.999711513519287,
        "learning_rate": 1.6254074315886792e-05,
        "epoch": 0.5468,
        "step": 4101
    },
    {
        "loss": 2.4096,
        "grad_norm": 3.2498135566711426,
        "learning_rate": 1.6219695986380268e-05,
        "epoch": 0.5469333333333334,
        "step": 4102
    },
    {
        "loss": 2.3462,
        "grad_norm": 2.842130184173584,
        "learning_rate": 1.6185350842537627e-05,
        "epoch": 0.5470666666666667,
        "step": 4103
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.5123369693756104,
        "learning_rate": 1.6151038897963143e-05,
        "epoch": 0.5472,
        "step": 4104
    },
    {
        "loss": 2.527,
        "grad_norm": 2.528679370880127,
        "learning_rate": 1.6116760166247802e-05,
        "epoch": 0.5473333333333333,
        "step": 4105
    },
    {
        "loss": 2.4322,
        "grad_norm": 3.5568394660949707,
        "learning_rate": 1.6082514660969627e-05,
        "epoch": 0.5474666666666667,
        "step": 4106
    },
    {
        "loss": 2.455,
        "grad_norm": 3.2016358375549316,
        "learning_rate": 1.6048302395693304e-05,
        "epoch": 0.5476,
        "step": 4107
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.3316781520843506,
        "learning_rate": 1.6014123383970493e-05,
        "epoch": 0.5477333333333333,
        "step": 4108
    },
    {
        "loss": 2.3499,
        "grad_norm": 3.4904892444610596,
        "learning_rate": 1.5979977639339537e-05,
        "epoch": 0.5478666666666666,
        "step": 4109
    },
    {
        "loss": 1.5205,
        "grad_norm": 1.7705271244049072,
        "learning_rate": 1.59458651753258e-05,
        "epoch": 0.548,
        "step": 4110
    },
    {
        "loss": 3.5041,
        "grad_norm": 4.1794209480285645,
        "learning_rate": 1.5911786005441232e-05,
        "epoch": 0.5481333333333334,
        "step": 4111
    },
    {
        "loss": 2.6853,
        "grad_norm": 1.7592145204544067,
        "learning_rate": 1.587774014318476e-05,
        "epoch": 0.5482666666666667,
        "step": 4112
    },
    {
        "loss": 2.6187,
        "grad_norm": 2.9086086750030518,
        "learning_rate": 1.5843727602042046e-05,
        "epoch": 0.5484,
        "step": 4113
    },
    {
        "loss": 1.5012,
        "grad_norm": 4.02551794052124,
        "learning_rate": 1.580974839548558e-05,
        "epoch": 0.5485333333333333,
        "step": 4114
    },
    {
        "loss": 2.2761,
        "grad_norm": 2.511043071746826,
        "learning_rate": 1.5775802536974647e-05,
        "epoch": 0.5486666666666666,
        "step": 4115
    },
    {
        "loss": 2.6316,
        "grad_norm": 2.072636127471924,
        "learning_rate": 1.5741890039955332e-05,
        "epoch": 0.5488,
        "step": 4116
    },
    {
        "loss": 2.2382,
        "grad_norm": 3.267129421234131,
        "learning_rate": 1.570801091786046e-05,
        "epoch": 0.5489333333333334,
        "step": 4117
    },
    {
        "loss": 2.0318,
        "grad_norm": 5.907043933868408,
        "learning_rate": 1.56741651841097e-05,
        "epoch": 0.5490666666666667,
        "step": 4118
    },
    {
        "loss": 2.087,
        "grad_norm": 3.5668113231658936,
        "learning_rate": 1.564035285210943e-05,
        "epoch": 0.5492,
        "step": 4119
    },
    {
        "loss": 2.6188,
        "grad_norm": 3.579416275024414,
        "learning_rate": 1.560657393525283e-05,
        "epoch": 0.5493333333333333,
        "step": 4120
    },
    {
        "loss": 1.782,
        "grad_norm": 2.468337297439575,
        "learning_rate": 1.5572828446919897e-05,
        "epoch": 0.5494666666666667,
        "step": 4121
    },
    {
        "loss": 2.3504,
        "grad_norm": 2.190763235092163,
        "learning_rate": 1.553911640047726e-05,
        "epoch": 0.5496,
        "step": 4122
    },
    {
        "loss": 2.0694,
        "grad_norm": 3.442107677459717,
        "learning_rate": 1.5505437809278432e-05,
        "epoch": 0.5497333333333333,
        "step": 4123
    },
    {
        "loss": 2.3059,
        "grad_norm": 1.8548896312713623,
        "learning_rate": 1.5471792686663577e-05,
        "epoch": 0.5498666666666666,
        "step": 4124
    },
    {
        "loss": 1.1184,
        "grad_norm": 3.809433698654175,
        "learning_rate": 1.5438181045959664e-05,
        "epoch": 0.55,
        "step": 4125
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.6853816509246826,
        "learning_rate": 1.540460290048037e-05,
        "epoch": 0.5501333333333334,
        "step": 4126
    },
    {
        "loss": 2.8645,
        "grad_norm": 2.427605390548706,
        "learning_rate": 1.537105826352615e-05,
        "epoch": 0.5502666666666667,
        "step": 4127
    },
    {
        "loss": 2.4438,
        "grad_norm": 2.3612377643585205,
        "learning_rate": 1.533754714838408e-05,
        "epoch": 0.5504,
        "step": 4128
    },
    {
        "loss": 1.9038,
        "grad_norm": 3.9281225204467773,
        "learning_rate": 1.5304069568328126e-05,
        "epoch": 0.5505333333333333,
        "step": 4129
    },
    {
        "loss": 2.7605,
        "grad_norm": 2.5944738388061523,
        "learning_rate": 1.527062553661873e-05,
        "epoch": 0.5506666666666666,
        "step": 4130
    },
    {
        "loss": 2.0777,
        "grad_norm": 3.2819664478302,
        "learning_rate": 1.523721506650332e-05,
        "epoch": 0.5508,
        "step": 4131
    },
    {
        "loss": 2.6776,
        "grad_norm": 3.3294434547424316,
        "learning_rate": 1.5203838171215824e-05,
        "epoch": 0.5509333333333334,
        "step": 4132
    },
    {
        "loss": 1.6179,
        "grad_norm": 4.267106056213379,
        "learning_rate": 1.5170494863976981e-05,
        "epoch": 0.5510666666666667,
        "step": 4133
    },
    {
        "loss": 2.8522,
        "grad_norm": 4.287001609802246,
        "learning_rate": 1.5137185157994127e-05,
        "epoch": 0.5512,
        "step": 4134
    },
    {
        "loss": 3.3622,
        "grad_norm": 3.2553789615631104,
        "learning_rate": 1.5103909066461475e-05,
        "epoch": 0.5513333333333333,
        "step": 4135
    },
    {
        "loss": 1.6739,
        "grad_norm": 4.32397985458374,
        "learning_rate": 1.5070666602559657e-05,
        "epoch": 0.5514666666666667,
        "step": 4136
    },
    {
        "loss": 3.1455,
        "grad_norm": 3.1227779388427734,
        "learning_rate": 1.503745777945622e-05,
        "epoch": 0.5516,
        "step": 4137
    },
    {
        "loss": 2.2371,
        "grad_norm": 2.6197097301483154,
        "learning_rate": 1.5004282610305232e-05,
        "epoch": 0.5517333333333333,
        "step": 4138
    },
    {
        "loss": 2.1139,
        "grad_norm": 1.5260186195373535,
        "learning_rate": 1.4971141108247533e-05,
        "epoch": 0.5518666666666666,
        "step": 4139
    },
    {
        "loss": 2.5573,
        "grad_norm": 1.934646725654602,
        "learning_rate": 1.4938033286410535e-05,
        "epoch": 0.552,
        "step": 4140
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.843170166015625,
        "learning_rate": 1.4904959157908415e-05,
        "epoch": 0.5521333333333334,
        "step": 4141
    },
    {
        "loss": 1.9817,
        "grad_norm": 2.838831901550293,
        "learning_rate": 1.487191873584195e-05,
        "epoch": 0.5522666666666667,
        "step": 4142
    },
    {
        "loss": 2.8533,
        "grad_norm": 2.7936387062072754,
        "learning_rate": 1.4838912033298512e-05,
        "epoch": 0.5524,
        "step": 4143
    },
    {
        "loss": 2.065,
        "grad_norm": 3.32607364654541,
        "learning_rate": 1.4805939063352225e-05,
        "epoch": 0.5525333333333333,
        "step": 4144
    },
    {
        "loss": 2.6519,
        "grad_norm": 3.2288646697998047,
        "learning_rate": 1.4772999839063739e-05,
        "epoch": 0.5526666666666666,
        "step": 4145
    },
    {
        "loss": 2.7961,
        "grad_norm": 2.912229537963867,
        "learning_rate": 1.474009437348045e-05,
        "epoch": 0.5528,
        "step": 4146
    },
    {
        "loss": 2.9496,
        "grad_norm": 2.111499547958374,
        "learning_rate": 1.4707222679636268e-05,
        "epoch": 0.5529333333333334,
        "step": 4147
    },
    {
        "loss": 0.8354,
        "grad_norm": 4.219476699829102,
        "learning_rate": 1.467438477055183e-05,
        "epoch": 0.5530666666666667,
        "step": 4148
    },
    {
        "loss": 1.615,
        "grad_norm": 4.6579813957214355,
        "learning_rate": 1.464158065923431e-05,
        "epoch": 0.5532,
        "step": 4149
    },
    {
        "loss": 1.7366,
        "grad_norm": 2.3128013610839844,
        "learning_rate": 1.4608810358677539e-05,
        "epoch": 0.5533333333333333,
        "step": 4150
    },
    {
        "loss": 1.8556,
        "grad_norm": 4.893612384796143,
        "learning_rate": 1.4576073881861951e-05,
        "epoch": 0.5534666666666667,
        "step": 4151
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.024939775466919,
        "learning_rate": 1.4543371241754588e-05,
        "epoch": 0.5536,
        "step": 4152
    },
    {
        "loss": 2.0626,
        "grad_norm": 3.988579273223877,
        "learning_rate": 1.4510702451309055e-05,
        "epoch": 0.5537333333333333,
        "step": 4153
    },
    {
        "loss": 2.0986,
        "grad_norm": 2.875549793243408,
        "learning_rate": 1.447806752346561e-05,
        "epoch": 0.5538666666666666,
        "step": 4154
    },
    {
        "loss": 1.3416,
        "grad_norm": 3.5716965198516846,
        "learning_rate": 1.4445466471150982e-05,
        "epoch": 0.554,
        "step": 4155
    },
    {
        "loss": 2.4672,
        "grad_norm": 2.1298093795776367,
        "learning_rate": 1.4412899307278616e-05,
        "epoch": 0.5541333333333334,
        "step": 4156
    },
    {
        "loss": 2.5827,
        "grad_norm": 2.619638442993164,
        "learning_rate": 1.438036604474845e-05,
        "epoch": 0.5542666666666667,
        "step": 4157
    },
    {
        "loss": 2.2165,
        "grad_norm": 2.4188220500946045,
        "learning_rate": 1.4347866696447076e-05,
        "epoch": 0.5544,
        "step": 4158
    },
    {
        "loss": 1.4871,
        "grad_norm": 3.067864418029785,
        "learning_rate": 1.4315401275247498e-05,
        "epoch": 0.5545333333333333,
        "step": 4159
    },
    {
        "loss": 2.8822,
        "grad_norm": 3.579824447631836,
        "learning_rate": 1.428296979400946e-05,
        "epoch": 0.5546666666666666,
        "step": 4160
    },
    {
        "loss": 2.6184,
        "grad_norm": 2.2421586513519287,
        "learning_rate": 1.4250572265579209e-05,
        "epoch": 0.5548,
        "step": 4161
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.8437225818634033,
        "learning_rate": 1.4218208702789425e-05,
        "epoch": 0.5549333333333333,
        "step": 4162
    },
    {
        "loss": 1.9999,
        "grad_norm": 3.2185394763946533,
        "learning_rate": 1.4185879118459489e-05,
        "epoch": 0.5550666666666667,
        "step": 4163
    },
    {
        "loss": 2.8789,
        "grad_norm": 2.95086669921875,
        "learning_rate": 1.415358352539522e-05,
        "epoch": 0.5552,
        "step": 4164
    },
    {
        "loss": 1.915,
        "grad_norm": 3.876220464706421,
        "learning_rate": 1.412132193638902e-05,
        "epoch": 0.5553333333333333,
        "step": 4165
    },
    {
        "loss": 2.9957,
        "grad_norm": 2.4671003818511963,
        "learning_rate": 1.4089094364219834e-05,
        "epoch": 0.5554666666666667,
        "step": 4166
    },
    {
        "loss": 1.9686,
        "grad_norm": 2.0540270805358887,
        "learning_rate": 1.4056900821653129e-05,
        "epoch": 0.5556,
        "step": 4167
    },
    {
        "loss": 0.7004,
        "grad_norm": 3.233459234237671,
        "learning_rate": 1.4024741321440826e-05,
        "epoch": 0.5557333333333333,
        "step": 4168
    },
    {
        "loss": 2.5826,
        "grad_norm": 2.550154447555542,
        "learning_rate": 1.3992615876321458e-05,
        "epoch": 0.5558666666666666,
        "step": 4169
    },
    {
        "loss": 2.4464,
        "grad_norm": 2.1299397945404053,
        "learning_rate": 1.396052449901999e-05,
        "epoch": 0.556,
        "step": 4170
    },
    {
        "loss": 2.0532,
        "grad_norm": 2.9603500366210938,
        "learning_rate": 1.3928467202247953e-05,
        "epoch": 0.5561333333333334,
        "step": 4171
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.6836624145507812,
        "learning_rate": 1.3896443998703323e-05,
        "epoch": 0.5562666666666667,
        "step": 4172
    },
    {
        "loss": 1.9613,
        "grad_norm": 3.179867744445801,
        "learning_rate": 1.3864454901070634e-05,
        "epoch": 0.5564,
        "step": 4173
    },
    {
        "loss": 1.5934,
        "grad_norm": 3.619154930114746,
        "learning_rate": 1.3832499922020848e-05,
        "epoch": 0.5565333333333333,
        "step": 4174
    },
    {
        "loss": 3.4253,
        "grad_norm": 3.7321853637695312,
        "learning_rate": 1.3800579074211439e-05,
        "epoch": 0.5566666666666666,
        "step": 4175
    },
    {
        "loss": 2.8062,
        "grad_norm": 1.9809447526931763,
        "learning_rate": 1.3768692370286374e-05,
        "epoch": 0.5568,
        "step": 4176
    },
    {
        "loss": 2.0486,
        "grad_norm": 2.986330986022949,
        "learning_rate": 1.3736839822876124e-05,
        "epoch": 0.5569333333333333,
        "step": 4177
    },
    {
        "loss": 1.9603,
        "grad_norm": 3.2491254806518555,
        "learning_rate": 1.3705021444597522e-05,
        "epoch": 0.5570666666666667,
        "step": 4178
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.4057724475860596,
        "learning_rate": 1.3673237248054016e-05,
        "epoch": 0.5572,
        "step": 4179
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.5276527404785156,
        "learning_rate": 1.3641487245835339e-05,
        "epoch": 0.5573333333333333,
        "step": 4180
    },
    {
        "loss": 2.2544,
        "grad_norm": 2.8570339679718018,
        "learning_rate": 1.3609771450517806e-05,
        "epoch": 0.5574666666666667,
        "step": 4181
    },
    {
        "loss": 2.3235,
        "grad_norm": 2.9839818477630615,
        "learning_rate": 1.3578089874664202e-05,
        "epoch": 0.5576,
        "step": 4182
    },
    {
        "loss": 2.4185,
        "grad_norm": 2.985197067260742,
        "learning_rate": 1.3546442530823667e-05,
        "epoch": 0.5577333333333333,
        "step": 4183
    },
    {
        "loss": 1.9918,
        "grad_norm": 3.200727701187134,
        "learning_rate": 1.3514829431531817e-05,
        "epoch": 0.5578666666666666,
        "step": 4184
    },
    {
        "loss": 2.3724,
        "grad_norm": 2.8448126316070557,
        "learning_rate": 1.3483250589310725e-05,
        "epoch": 0.558,
        "step": 4185
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.8845744132995605,
        "learning_rate": 1.3451706016668919e-05,
        "epoch": 0.5581333333333334,
        "step": 4186
    },
    {
        "loss": 1.338,
        "grad_norm": 5.209765911102295,
        "learning_rate": 1.3420195726101225e-05,
        "epoch": 0.5582666666666667,
        "step": 4187
    },
    {
        "loss": 1.1083,
        "grad_norm": 3.1403684616088867,
        "learning_rate": 1.3388719730089027e-05,
        "epoch": 0.5584,
        "step": 4188
    },
    {
        "loss": 2.4618,
        "grad_norm": 2.916330337524414,
        "learning_rate": 1.3357278041100053e-05,
        "epoch": 0.5585333333333333,
        "step": 4189
    },
    {
        "loss": 2.4486,
        "grad_norm": 2.344165802001953,
        "learning_rate": 1.332587067158847e-05,
        "epoch": 0.5586666666666666,
        "step": 4190
    },
    {
        "loss": 2.582,
        "grad_norm": 2.2114646434783936,
        "learning_rate": 1.3294497633994852e-05,
        "epoch": 0.5588,
        "step": 4191
    },
    {
        "loss": 2.4826,
        "grad_norm": 2.5528128147125244,
        "learning_rate": 1.326315894074619e-05,
        "epoch": 0.5589333333333333,
        "step": 4192
    },
    {
        "loss": 2.3872,
        "grad_norm": 2.589108943939209,
        "learning_rate": 1.3231854604255788e-05,
        "epoch": 0.5590666666666667,
        "step": 4193
    },
    {
        "loss": 2.099,
        "grad_norm": 3.427131175994873,
        "learning_rate": 1.3200584636923463e-05,
        "epoch": 0.5592,
        "step": 4194
    },
    {
        "loss": 2.0879,
        "grad_norm": 2.3640124797821045,
        "learning_rate": 1.3169349051135293e-05,
        "epoch": 0.5593333333333333,
        "step": 4195
    },
    {
        "loss": 2.6532,
        "grad_norm": 2.3423659801483154,
        "learning_rate": 1.3138147859263861e-05,
        "epoch": 0.5594666666666667,
        "step": 4196
    },
    {
        "loss": 2.8458,
        "grad_norm": 2.647270441055298,
        "learning_rate": 1.3106981073668012e-05,
        "epoch": 0.5596,
        "step": 4197
    },
    {
        "loss": 2.0805,
        "grad_norm": 2.7815663814544678,
        "learning_rate": 1.3075848706693072e-05,
        "epoch": 0.5597333333333333,
        "step": 4198
    },
    {
        "loss": 2.1938,
        "grad_norm": 3.332923173904419,
        "learning_rate": 1.304475077067061e-05,
        "epoch": 0.5598666666666666,
        "step": 4199
    },
    {
        "loss": 1.6594,
        "grad_norm": 2.9738049507141113,
        "learning_rate": 1.3013687277918662e-05,
        "epoch": 0.56,
        "step": 4200
    },
    {
        "loss": 2.0368,
        "grad_norm": 3.9721381664276123,
        "learning_rate": 1.2982658240741597e-05,
        "epoch": 0.5601333333333334,
        "step": 4201
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.1080048084259033,
        "learning_rate": 1.2951663671430092e-05,
        "epoch": 0.5602666666666667,
        "step": 4202
    },
    {
        "loss": 1.9898,
        "grad_norm": 2.406857967376709,
        "learning_rate": 1.292070358226124e-05,
        "epoch": 0.5604,
        "step": 4203
    },
    {
        "loss": 2.491,
        "grad_norm": 3.547567129135132,
        "learning_rate": 1.2889777985498397e-05,
        "epoch": 0.5605333333333333,
        "step": 4204
    },
    {
        "loss": 2.1633,
        "grad_norm": 2.245702028274536,
        "learning_rate": 1.2858886893391353e-05,
        "epoch": 0.5606666666666666,
        "step": 4205
    },
    {
        "loss": 2.2447,
        "grad_norm": 3.2686800956726074,
        "learning_rate": 1.2828030318176077e-05,
        "epoch": 0.5608,
        "step": 4206
    },
    {
        "loss": 1.2441,
        "grad_norm": 2.8089890480041504,
        "learning_rate": 1.2797208272075067e-05,
        "epoch": 0.5609333333333333,
        "step": 4207
    },
    {
        "loss": 2.9593,
        "grad_norm": 3.1657721996307373,
        "learning_rate": 1.2766420767296972e-05,
        "epoch": 0.5610666666666667,
        "step": 4208
    },
    {
        "loss": 2.4766,
        "grad_norm": 2.193387508392334,
        "learning_rate": 1.2735667816036878e-05,
        "epoch": 0.5612,
        "step": 4209
    },
    {
        "loss": 2.1296,
        "grad_norm": 3.7832705974578857,
        "learning_rate": 1.2704949430476054e-05,
        "epoch": 0.5613333333333334,
        "step": 4210
    },
    {
        "loss": 2.1236,
        "grad_norm": 3.3026514053344727,
        "learning_rate": 1.2674265622782288e-05,
        "epoch": 0.5614666666666667,
        "step": 4211
    },
    {
        "loss": 2.0526,
        "grad_norm": 3.2891595363616943,
        "learning_rate": 1.2643616405109415e-05,
        "epoch": 0.5616,
        "step": 4212
    },
    {
        "loss": 2.1783,
        "grad_norm": 4.289033889770508,
        "learning_rate": 1.2613001789597766e-05,
        "epoch": 0.5617333333333333,
        "step": 4213
    },
    {
        "loss": 2.4107,
        "grad_norm": 2.954697370529175,
        "learning_rate": 1.2582421788373855e-05,
        "epoch": 0.5618666666666666,
        "step": 4214
    },
    {
        "loss": 1.3624,
        "grad_norm": 3.5088539123535156,
        "learning_rate": 1.2551876413550533e-05,
        "epoch": 0.562,
        "step": 4215
    },
    {
        "loss": 1.92,
        "grad_norm": 2.9631266593933105,
        "learning_rate": 1.2521365677226937e-05,
        "epoch": 0.5621333333333334,
        "step": 4216
    },
    {
        "loss": 2.6005,
        "grad_norm": 3.12556791305542,
        "learning_rate": 1.2490889591488497e-05,
        "epoch": 0.5622666666666667,
        "step": 4217
    },
    {
        "loss": 2.0295,
        "grad_norm": 3.2460858821868896,
        "learning_rate": 1.2460448168406846e-05,
        "epoch": 0.5624,
        "step": 4218
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.3668713569641113,
        "learning_rate": 1.2430041420039985e-05,
        "epoch": 0.5625333333333333,
        "step": 4219
    },
    {
        "loss": 1.9486,
        "grad_norm": 3.470266103744507,
        "learning_rate": 1.2399669358432086e-05,
        "epoch": 0.5626666666666666,
        "step": 4220
    },
    {
        "loss": 2.1299,
        "grad_norm": 4.295306205749512,
        "learning_rate": 1.2369331995613643e-05,
        "epoch": 0.5628,
        "step": 4221
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.582489252090454,
        "learning_rate": 1.2339029343601405e-05,
        "epoch": 0.5629333333333333,
        "step": 4222
    },
    {
        "loss": 2.3104,
        "grad_norm": 1.407835602760315,
        "learning_rate": 1.2308761414398328e-05,
        "epoch": 0.5630666666666667,
        "step": 4223
    },
    {
        "loss": 2.5048,
        "grad_norm": 2.5328149795532227,
        "learning_rate": 1.2278528219993668e-05,
        "epoch": 0.5632,
        "step": 4224
    },
    {
        "loss": 1.3762,
        "grad_norm": 5.438392162322998,
        "learning_rate": 1.2248329772362865e-05,
        "epoch": 0.5633333333333334,
        "step": 4225
    },
    {
        "loss": 1.582,
        "grad_norm": 2.6784353256225586,
        "learning_rate": 1.2218166083467653e-05,
        "epoch": 0.5634666666666667,
        "step": 4226
    },
    {
        "loss": 1.5828,
        "grad_norm": 5.384899616241455,
        "learning_rate": 1.218803716525596e-05,
        "epoch": 0.5636,
        "step": 4227
    },
    {
        "loss": 2.8289,
        "grad_norm": 1.9597735404968262,
        "learning_rate": 1.2157943029661978e-05,
        "epoch": 0.5637333333333333,
        "step": 4228
    },
    {
        "loss": 2.0988,
        "grad_norm": 3.4598660469055176,
        "learning_rate": 1.2127883688606056e-05,
        "epoch": 0.5638666666666666,
        "step": 4229
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.070385217666626,
        "learning_rate": 1.2097859153994862e-05,
        "epoch": 0.564,
        "step": 4230
    },
    {
        "loss": 3.3272,
        "grad_norm": 2.9407713413238525,
        "learning_rate": 1.2067869437721113e-05,
        "epoch": 0.5641333333333334,
        "step": 4231
    },
    {
        "loss": 2.144,
        "grad_norm": 2.6700968742370605,
        "learning_rate": 1.2037914551663953e-05,
        "epoch": 0.5642666666666667,
        "step": 4232
    },
    {
        "loss": 2.3,
        "grad_norm": 3.065472364425659,
        "learning_rate": 1.2007994507688524e-05,
        "epoch": 0.5644,
        "step": 4233
    },
    {
        "loss": 2.7871,
        "grad_norm": 3.0023860931396484,
        "learning_rate": 1.1978109317646335e-05,
        "epoch": 0.5645333333333333,
        "step": 4234
    },
    {
        "loss": 2.4961,
        "grad_norm": 2.1886446475982666,
        "learning_rate": 1.1948258993374917e-05,
        "epoch": 0.5646666666666667,
        "step": 4235
    },
    {
        "loss": 1.3509,
        "grad_norm": 5.415783405303955,
        "learning_rate": 1.191844354669821e-05,
        "epoch": 0.5648,
        "step": 4236
    },
    {
        "loss": 2.2682,
        "grad_norm": 3.4106338024139404,
        "learning_rate": 1.1888662989426102e-05,
        "epoch": 0.5649333333333333,
        "step": 4237
    },
    {
        "loss": 2.2089,
        "grad_norm": 2.895242929458618,
        "learning_rate": 1.1858917333354835e-05,
        "epoch": 0.5650666666666667,
        "step": 4238
    },
    {
        "loss": 1.2955,
        "grad_norm": 3.9868853092193604,
        "learning_rate": 1.1829206590266729e-05,
        "epoch": 0.5652,
        "step": 4239
    },
    {
        "loss": 2.651,
        "grad_norm": 3.913869857788086,
        "learning_rate": 1.1799530771930346e-05,
        "epoch": 0.5653333333333334,
        "step": 4240
    },
    {
        "loss": 2.5173,
        "grad_norm": 1.5359470844268799,
        "learning_rate": 1.1769889890100316e-05,
        "epoch": 0.5654666666666667,
        "step": 4241
    },
    {
        "loss": 1.906,
        "grad_norm": 2.4416184425354004,
        "learning_rate": 1.1740283956517562e-05,
        "epoch": 0.5656,
        "step": 4242
    },
    {
        "loss": 2.6342,
        "grad_norm": 3.364565134048462,
        "learning_rate": 1.171071298290909e-05,
        "epoch": 0.5657333333333333,
        "step": 4243
    },
    {
        "loss": 2.0237,
        "grad_norm": 3.903717517852783,
        "learning_rate": 1.1681176980988018e-05,
        "epoch": 0.5658666666666666,
        "step": 4244
    },
    {
        "loss": 2.91,
        "grad_norm": 3.288597583770752,
        "learning_rate": 1.1651675962453713e-05,
        "epoch": 0.566,
        "step": 4245
    },
    {
        "loss": 2.183,
        "grad_norm": 2.5636544227600098,
        "learning_rate": 1.1622209938991569e-05,
        "epoch": 0.5661333333333334,
        "step": 4246
    },
    {
        "loss": 2.222,
        "grad_norm": 2.6540722846984863,
        "learning_rate": 1.1592778922273229e-05,
        "epoch": 0.5662666666666667,
        "step": 4247
    },
    {
        "loss": 2.5111,
        "grad_norm": 3.9814138412475586,
        "learning_rate": 1.1563382923956379e-05,
        "epoch": 0.5664,
        "step": 4248
    },
    {
        "loss": 1.6387,
        "grad_norm": 4.305610656738281,
        "learning_rate": 1.1534021955684915e-05,
        "epoch": 0.5665333333333333,
        "step": 4249
    },
    {
        "loss": 2.5453,
        "grad_norm": 3.157925605773926,
        "learning_rate": 1.150469602908878e-05,
        "epoch": 0.5666666666666667,
        "step": 4250
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.8091559410095215,
        "learning_rate": 1.1475405155784081e-05,
        "epoch": 0.5668,
        "step": 4251
    },
    {
        "loss": 1.7759,
        "grad_norm": 3.2606828212738037,
        "learning_rate": 1.1446149347373048e-05,
        "epoch": 0.5669333333333333,
        "step": 4252
    },
    {
        "loss": 1.8541,
        "grad_norm": 3.4488885402679443,
        "learning_rate": 1.1416928615444011e-05,
        "epoch": 0.5670666666666667,
        "step": 4253
    },
    {
        "loss": 3.2295,
        "grad_norm": 2.7723708152770996,
        "learning_rate": 1.1387742971571369e-05,
        "epoch": 0.5672,
        "step": 4254
    },
    {
        "loss": 1.3474,
        "grad_norm": 3.3373591899871826,
        "learning_rate": 1.1358592427315718e-05,
        "epoch": 0.5673333333333334,
        "step": 4255
    },
    {
        "loss": 1.6997,
        "grad_norm": 3.1148128509521484,
        "learning_rate": 1.1329476994223597e-05,
        "epoch": 0.5674666666666667,
        "step": 4256
    },
    {
        "loss": 2.8089,
        "grad_norm": 2.711979627609253,
        "learning_rate": 1.1300396683827786e-05,
        "epoch": 0.5676,
        "step": 4257
    },
    {
        "loss": 1.3299,
        "grad_norm": 3.082848072052002,
        "learning_rate": 1.1271351507647077e-05,
        "epoch": 0.5677333333333333,
        "step": 4258
    },
    {
        "loss": 1.631,
        "grad_norm": 3.1563479900360107,
        "learning_rate": 1.1242341477186402e-05,
        "epoch": 0.5678666666666666,
        "step": 4259
    },
    {
        "loss": 3.5822,
        "grad_norm": 4.979446887969971,
        "learning_rate": 1.1213366603936648e-05,
        "epoch": 0.568,
        "step": 4260
    },
    {
        "loss": 2.9374,
        "grad_norm": 2.744493246078491,
        "learning_rate": 1.1184426899374945e-05,
        "epoch": 0.5681333333333334,
        "step": 4261
    },
    {
        "loss": 2.6651,
        "grad_norm": 2.725084066390991,
        "learning_rate": 1.1155522374964412e-05,
        "epoch": 0.5682666666666667,
        "step": 4262
    },
    {
        "loss": 2.5377,
        "grad_norm": 4.003847122192383,
        "learning_rate": 1.1126653042154168e-05,
        "epoch": 0.5684,
        "step": 4263
    },
    {
        "loss": 2.5368,
        "grad_norm": 2.3768482208251953,
        "learning_rate": 1.1097818912379488e-05,
        "epoch": 0.5685333333333333,
        "step": 4264
    },
    {
        "loss": 2.7542,
        "grad_norm": 2.8144044876098633,
        "learning_rate": 1.1069019997061658e-05,
        "epoch": 0.5686666666666667,
        "step": 4265
    },
    {
        "loss": 3.7436,
        "grad_norm": 2.112499237060547,
        "learning_rate": 1.104025630760802e-05,
        "epoch": 0.5688,
        "step": 4266
    },
    {
        "loss": 2.5399,
        "grad_norm": 2.5321266651153564,
        "learning_rate": 1.1011527855411985e-05,
        "epoch": 0.5689333333333333,
        "step": 4267
    },
    {
        "loss": 1.0048,
        "grad_norm": 3.463388442993164,
        "learning_rate": 1.0982834651853013e-05,
        "epoch": 0.5690666666666667,
        "step": 4268
    },
    {
        "loss": 1.085,
        "grad_norm": 3.878730535507202,
        "learning_rate": 1.0954176708296527e-05,
        "epoch": 0.5692,
        "step": 4269
    },
    {
        "loss": 2.0379,
        "grad_norm": 2.572530508041382,
        "learning_rate": 1.0925554036094088e-05,
        "epoch": 0.5693333333333334,
        "step": 4270
    },
    {
        "loss": 1.6564,
        "grad_norm": 4.132355213165283,
        "learning_rate": 1.0896966646583184e-05,
        "epoch": 0.5694666666666667,
        "step": 4271
    },
    {
        "loss": 1.4175,
        "grad_norm": 4.36212682723999,
        "learning_rate": 1.0868414551087424e-05,
        "epoch": 0.5696,
        "step": 4272
    },
    {
        "loss": 2.1655,
        "grad_norm": 2.6662728786468506,
        "learning_rate": 1.083989776091635e-05,
        "epoch": 0.5697333333333333,
        "step": 4273
    },
    {
        "loss": 2.3467,
        "grad_norm": 2.745138645172119,
        "learning_rate": 1.0811416287365606e-05,
        "epoch": 0.5698666666666666,
        "step": 4274
    },
    {
        "loss": 2.0796,
        "grad_norm": 2.6886818408966064,
        "learning_rate": 1.0782970141716741e-05,
        "epoch": 0.57,
        "step": 4275
    },
    {
        "loss": 0.6712,
        "grad_norm": 2.606847047805786,
        "learning_rate": 1.0754559335237412e-05,
        "epoch": 0.5701333333333334,
        "step": 4276
    },
    {
        "loss": 2.4747,
        "grad_norm": 2.6627142429351807,
        "learning_rate": 1.0726183879181218e-05,
        "epoch": 0.5702666666666667,
        "step": 4277
    },
    {
        "loss": 2.0231,
        "grad_norm": 2.5966429710388184,
        "learning_rate": 1.0697843784787808e-05,
        "epoch": 0.5704,
        "step": 4278
    },
    {
        "loss": 2.6884,
        "grad_norm": 2.3193957805633545,
        "learning_rate": 1.0669539063282741e-05,
        "epoch": 0.5705333333333333,
        "step": 4279
    },
    {
        "loss": 2.5791,
        "grad_norm": 4.468437194824219,
        "learning_rate": 1.0641269725877679e-05,
        "epoch": 0.5706666666666667,
        "step": 4280
    },
    {
        "loss": 0.7888,
        "grad_norm": 3.351520299911499,
        "learning_rate": 1.061303578377012e-05,
        "epoch": 0.5708,
        "step": 4281
    },
    {
        "loss": 2.7382,
        "grad_norm": 2.4423739910125732,
        "learning_rate": 1.0584837248143642e-05,
        "epoch": 0.5709333333333333,
        "step": 4282
    },
    {
        "loss": 2.5677,
        "grad_norm": 2.870969772338867,
        "learning_rate": 1.0556674130167853e-05,
        "epoch": 0.5710666666666666,
        "step": 4283
    },
    {
        "loss": 1.8681,
        "grad_norm": 2.9406044483184814,
        "learning_rate": 1.0528546440998188e-05,
        "epoch": 0.5712,
        "step": 4284
    },
    {
        "loss": 1.5389,
        "grad_norm": 4.647624969482422,
        "learning_rate": 1.0500454191776143e-05,
        "epoch": 0.5713333333333334,
        "step": 4285
    },
    {
        "loss": 2.286,
        "grad_norm": 2.1911306381225586,
        "learning_rate": 1.0472397393629163e-05,
        "epoch": 0.5714666666666667,
        "step": 4286
    },
    {
        "loss": 2.2186,
        "grad_norm": 3.4271433353424072,
        "learning_rate": 1.0444376057670657e-05,
        "epoch": 0.5716,
        "step": 4287
    },
    {
        "loss": 0.9647,
        "grad_norm": 3.077465057373047,
        "learning_rate": 1.0416390194999925e-05,
        "epoch": 0.5717333333333333,
        "step": 4288
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.411092519760132,
        "learning_rate": 1.0388439816702289e-05,
        "epoch": 0.5718666666666666,
        "step": 4289
    },
    {
        "loss": 2.5986,
        "grad_norm": 2.1951968669891357,
        "learning_rate": 1.036052493384898e-05,
        "epoch": 0.572,
        "step": 4290
    },
    {
        "loss": 2.4275,
        "grad_norm": 2.510586977005005,
        "learning_rate": 1.0332645557497179e-05,
        "epoch": 0.5721333333333334,
        "step": 4291
    },
    {
        "loss": 2.44,
        "grad_norm": 2.712113857269287,
        "learning_rate": 1.0304801698690003e-05,
        "epoch": 0.5722666666666667,
        "step": 4292
    },
    {
        "loss": 2.6988,
        "grad_norm": 3.1224312782287598,
        "learning_rate": 1.0276993368456522e-05,
        "epoch": 0.5724,
        "step": 4293
    },
    {
        "loss": 1.7301,
        "grad_norm": 2.8173255920410156,
        "learning_rate": 1.024922057781168e-05,
        "epoch": 0.5725333333333333,
        "step": 4294
    },
    {
        "loss": 2.9064,
        "grad_norm": 3.240844488143921,
        "learning_rate": 1.0221483337756399e-05,
        "epoch": 0.5726666666666667,
        "step": 4295
    },
    {
        "loss": 2.6201,
        "grad_norm": 2.905102252960205,
        "learning_rate": 1.0193781659277458e-05,
        "epoch": 0.5728,
        "step": 4296
    },
    {
        "loss": 1.9018,
        "grad_norm": 2.8298709392547607,
        "learning_rate": 1.0166115553347632e-05,
        "epoch": 0.5729333333333333,
        "step": 4297
    },
    {
        "loss": 1.7521,
        "grad_norm": 3.825850486755371,
        "learning_rate": 1.0138485030925527e-05,
        "epoch": 0.5730666666666666,
        "step": 4298
    },
    {
        "loss": 2.8262,
        "grad_norm": 2.6870667934417725,
        "learning_rate": 1.0110890102955706e-05,
        "epoch": 0.5732,
        "step": 4299
    },
    {
        "loss": 2.3587,
        "grad_norm": 3.710095167160034,
        "learning_rate": 1.0083330780368582e-05,
        "epoch": 0.5733333333333334,
        "step": 4300
    },
    {
        "loss": 2.3624,
        "grad_norm": 3.993379592895508,
        "learning_rate": 1.0055807074080504e-05,
        "epoch": 0.5734666666666667,
        "step": 4301
    },
    {
        "loss": 1.1436,
        "grad_norm": Infinity,
        "learning_rate": 1.0055807074080504e-05,
        "epoch": 0.5736,
        "step": 4302
    },
    {
        "loss": 2.5719,
        "grad_norm": 3.6279103755950928,
        "learning_rate": 1.0028318994993758e-05,
        "epoch": 0.5737333333333333,
        "step": 4303
    },
    {
        "loss": 1.441,
        "grad_norm": 2.7724437713623047,
        "learning_rate": 1.0000866553996414e-05,
        "epoch": 0.5738666666666666,
        "step": 4304
    },
    {
        "loss": 2.6233,
        "grad_norm": 3.3348922729492188,
        "learning_rate": 9.973449761962517e-06,
        "epoch": 0.574,
        "step": 4305
    },
    {
        "loss": 2.4442,
        "grad_norm": 2.437039852142334,
        "learning_rate": 9.946068629751892e-06,
        "epoch": 0.5741333333333334,
        "step": 4306
    },
    {
        "loss": 2.5073,
        "grad_norm": 2.325784206390381,
        "learning_rate": 9.918723168210386e-06,
        "epoch": 0.5742666666666667,
        "step": 4307
    },
    {
        "loss": 3.0511,
        "grad_norm": 2.289233922958374,
        "learning_rate": 9.891413388169513e-06,
        "epoch": 0.5744,
        "step": 4308
    },
    {
        "loss": 2.4348,
        "grad_norm": 2.945659637451172,
        "learning_rate": 9.864139300446873e-06,
        "epoch": 0.5745333333333333,
        "step": 4309
    },
    {
        "loss": 1.923,
        "grad_norm": 2.9802019596099854,
        "learning_rate": 9.83690091584577e-06,
        "epoch": 0.5746666666666667,
        "step": 4310
    },
    {
        "loss": 2.0041,
        "grad_norm": 3.380303382873535,
        "learning_rate": 9.809698245155463e-06,
        "epoch": 0.5748,
        "step": 4311
    },
    {
        "loss": 2.8402,
        "grad_norm": 2.704091787338257,
        "learning_rate": 9.782531299150954e-06,
        "epoch": 0.5749333333333333,
        "step": 4312
    },
    {
        "loss": 2.4404,
        "grad_norm": 3.3829236030578613,
        "learning_rate": 9.755400088593258e-06,
        "epoch": 0.5750666666666666,
        "step": 4313
    },
    {
        "loss": 1.7482,
        "grad_norm": 3.9603452682495117,
        "learning_rate": 9.728304624229056e-06,
        "epoch": 0.5752,
        "step": 4314
    },
    {
        "loss": 2.5669,
        "grad_norm": 2.0863401889801025,
        "learning_rate": 9.701244916790996e-06,
        "epoch": 0.5753333333333334,
        "step": 4315
    },
    {
        "loss": 1.8945,
        "grad_norm": 4.441375255584717,
        "learning_rate": 9.674220976997494e-06,
        "epoch": 0.5754666666666667,
        "step": 4316
    },
    {
        "loss": 1.989,
        "grad_norm": 3.1176750659942627,
        "learning_rate": 9.647232815552843e-06,
        "epoch": 0.5756,
        "step": 4317
    },
    {
        "loss": 2.086,
        "grad_norm": 1.61318838596344,
        "learning_rate": 9.620280443147145e-06,
        "epoch": 0.5757333333333333,
        "step": 4318
    },
    {
        "loss": 2.8649,
        "grad_norm": 2.2003791332244873,
        "learning_rate": 9.59336387045634e-06,
        "epoch": 0.5758666666666666,
        "step": 4319
    },
    {
        "loss": 1.3774,
        "grad_norm": 3.120314598083496,
        "learning_rate": 9.566483108142144e-06,
        "epoch": 0.576,
        "step": 4320
    },
    {
        "loss": 2.0619,
        "grad_norm": 3.105125665664673,
        "learning_rate": 9.53963816685215e-06,
        "epoch": 0.5761333333333334,
        "step": 4321
    },
    {
        "loss": 2.4382,
        "grad_norm": 2.2740490436553955,
        "learning_rate": 9.512829057219697e-06,
        "epoch": 0.5762666666666667,
        "step": 4322
    },
    {
        "loss": 2.4584,
        "grad_norm": 3.003108263015747,
        "learning_rate": 9.486055789863979e-06,
        "epoch": 0.5764,
        "step": 4323
    },
    {
        "loss": 2.8193,
        "grad_norm": 4.151266574859619,
        "learning_rate": 9.459318375390003e-06,
        "epoch": 0.5765333333333333,
        "step": 4324
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.2718346118927,
        "learning_rate": 9.432616824388508e-06,
        "epoch": 0.5766666666666667,
        "step": 4325
    },
    {
        "loss": 2.8219,
        "grad_norm": 2.869654893875122,
        "learning_rate": 9.40595114743611e-06,
        "epoch": 0.5768,
        "step": 4326
    },
    {
        "loss": 0.981,
        "grad_norm": 3.1948647499084473,
        "learning_rate": 9.379321355095139e-06,
        "epoch": 0.5769333333333333,
        "step": 4327
    },
    {
        "loss": 2.6352,
        "grad_norm": 1.8412737846374512,
        "learning_rate": 9.352727457913756e-06,
        "epoch": 0.5770666666666666,
        "step": 4328
    },
    {
        "loss": 2.5676,
        "grad_norm": 2.4289259910583496,
        "learning_rate": 9.326169466425893e-06,
        "epoch": 0.5772,
        "step": 4329
    },
    {
        "loss": 2.2203,
        "grad_norm": 2.5026674270629883,
        "learning_rate": 9.2996473911513e-06,
        "epoch": 0.5773333333333334,
        "step": 4330
    },
    {
        "loss": 2.9733,
        "grad_norm": 2.6356897354125977,
        "learning_rate": 9.273161242595408e-06,
        "epoch": 0.5774666666666667,
        "step": 4331
    },
    {
        "loss": 2.0877,
        "grad_norm": 3.676447629928589,
        "learning_rate": 9.246711031249521e-06,
        "epoch": 0.5776,
        "step": 4332
    },
    {
        "loss": 2.6361,
        "grad_norm": 3.3398332595825195,
        "learning_rate": 9.220296767590586e-06,
        "epoch": 0.5777333333333333,
        "step": 4333
    },
    {
        "loss": 2.644,
        "grad_norm": 2.6482114791870117,
        "learning_rate": 9.193918462081453e-06,
        "epoch": 0.5778666666666666,
        "step": 4334
    },
    {
        "loss": 1.5189,
        "grad_norm": 3.922133207321167,
        "learning_rate": 9.167576125170607e-06,
        "epoch": 0.578,
        "step": 4335
    },
    {
        "loss": 1.7704,
        "grad_norm": 7.4087419509887695,
        "learning_rate": 9.141269767292393e-06,
        "epoch": 0.5781333333333334,
        "step": 4336
    },
    {
        "loss": 2.167,
        "grad_norm": 3.2094364166259766,
        "learning_rate": 9.114999398866775e-06,
        "epoch": 0.5782666666666667,
        "step": 4337
    },
    {
        "loss": 1.8956,
        "grad_norm": 4.468094348907471,
        "learning_rate": 9.088765030299628e-06,
        "epoch": 0.5784,
        "step": 4338
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.266134023666382,
        "learning_rate": 9.062566671982398e-06,
        "epoch": 0.5785333333333333,
        "step": 4339
    },
    {
        "loss": 1.7974,
        "grad_norm": 2.8719735145568848,
        "learning_rate": 9.036404334292393e-06,
        "epoch": 0.5786666666666667,
        "step": 4340
    },
    {
        "loss": 1.6173,
        "grad_norm": 3.564606189727783,
        "learning_rate": 9.010278027592556e-06,
        "epoch": 0.5788,
        "step": 4341
    },
    {
        "loss": 2.8487,
        "grad_norm": 2.0670886039733887,
        "learning_rate": 8.984187762231645e-06,
        "epoch": 0.5789333333333333,
        "step": 4342
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.7251757383346558,
        "learning_rate": 8.958133548544056e-06,
        "epoch": 0.5790666666666666,
        "step": 4343
    },
    {
        "loss": 2.6754,
        "grad_norm": 2.7620856761932373,
        "learning_rate": 8.932115396850016e-06,
        "epoch": 0.5792,
        "step": 4344
    },
    {
        "loss": 2.6373,
        "grad_norm": 3.572298765182495,
        "learning_rate": 8.906133317455378e-06,
        "epoch": 0.5793333333333334,
        "step": 4345
    },
    {
        "loss": 1.6174,
        "grad_norm": 3.6226425170898438,
        "learning_rate": 8.880187320651711e-06,
        "epoch": 0.5794666666666667,
        "step": 4346
    },
    {
        "loss": 2.5555,
        "grad_norm": 3.02909255027771,
        "learning_rate": 8.85427741671635e-06,
        "epoch": 0.5796,
        "step": 4347
    },
    {
        "loss": 2.1604,
        "grad_norm": 2.6041271686553955,
        "learning_rate": 8.828403615912262e-06,
        "epoch": 0.5797333333333333,
        "step": 4348
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.326606273651123,
        "learning_rate": 8.802565928488183e-06,
        "epoch": 0.5798666666666666,
        "step": 4349
    },
    {
        "loss": 1.7479,
        "grad_norm": 2.4007973670959473,
        "learning_rate": 8.776764364678458e-06,
        "epoch": 0.58,
        "step": 4350
    },
    {
        "loss": 1.5303,
        "grad_norm": 5.164399147033691,
        "learning_rate": 8.750998934703225e-06,
        "epoch": 0.5801333333333333,
        "step": 4351
    },
    {
        "loss": 1.1418,
        "grad_norm": 3.927560567855835,
        "learning_rate": 8.72526964876822e-06,
        "epoch": 0.5802666666666667,
        "step": 4352
    },
    {
        "loss": 2.3979,
        "grad_norm": 2.1605000495910645,
        "learning_rate": 8.699576517064922e-06,
        "epoch": 0.5804,
        "step": 4353
    },
    {
        "loss": 2.8225,
        "grad_norm": 2.9397006034851074,
        "learning_rate": 8.67391954977046e-06,
        "epoch": 0.5805333333333333,
        "step": 4354
    },
    {
        "loss": 2.6341,
        "grad_norm": 3.3974955081939697,
        "learning_rate": 8.648298757047668e-06,
        "epoch": 0.5806666666666667,
        "step": 4355
    },
    {
        "loss": 3.1959,
        "grad_norm": 2.467939615249634,
        "learning_rate": 8.622714149044998e-06,
        "epoch": 0.5808,
        "step": 4356
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.797185182571411,
        "learning_rate": 8.59716573589664e-06,
        "epoch": 0.5809333333333333,
        "step": 4357
    },
    {
        "loss": 2.118,
        "grad_norm": 2.91534161567688,
        "learning_rate": 8.571653527722356e-06,
        "epoch": 0.5810666666666666,
        "step": 4358
    },
    {
        "loss": 2.6058,
        "grad_norm": 2.678745746612549,
        "learning_rate": 8.546177534627642e-06,
        "epoch": 0.5812,
        "step": 4359
    },
    {
        "loss": 2.2848,
        "grad_norm": 2.8382081985473633,
        "learning_rate": 8.520737766703623e-06,
        "epoch": 0.5813333333333334,
        "step": 4360
    },
    {
        "loss": 1.613,
        "grad_norm": 4.538588047027588,
        "learning_rate": 8.495334234027109e-06,
        "epoch": 0.5814666666666667,
        "step": 4361
    },
    {
        "loss": 2.5448,
        "grad_norm": 5.485261917114258,
        "learning_rate": 8.469966946660457e-06,
        "epoch": 0.5816,
        "step": 4362
    },
    {
        "loss": 2.7038,
        "grad_norm": 2.7426705360412598,
        "learning_rate": 8.444635914651799e-06,
        "epoch": 0.5817333333333333,
        "step": 4363
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.1355440616607666,
        "learning_rate": 8.419341148034853e-06,
        "epoch": 0.5818666666666666,
        "step": 4364
    },
    {
        "loss": 2.1415,
        "grad_norm": 4.46265172958374,
        "learning_rate": 8.394082656828906e-06,
        "epoch": 0.582,
        "step": 4365
    },
    {
        "loss": 2.9393,
        "grad_norm": 2.211256265640259,
        "learning_rate": 8.368860451038962e-06,
        "epoch": 0.5821333333333333,
        "step": 4366
    },
    {
        "loss": 2.8468,
        "grad_norm": 2.900963068008423,
        "learning_rate": 8.343674540655611e-06,
        "epoch": 0.5822666666666667,
        "step": 4367
    },
    {
        "loss": 2.5038,
        "grad_norm": 3.98282527923584,
        "learning_rate": 8.318524935655081e-06,
        "epoch": 0.5824,
        "step": 4368
    },
    {
        "loss": 2.1251,
        "grad_norm": 3.9017715454101562,
        "learning_rate": 8.293411645999228e-06,
        "epoch": 0.5825333333333333,
        "step": 4369
    },
    {
        "loss": 2.7866,
        "grad_norm": 3.155613660812378,
        "learning_rate": 8.268334681635515e-06,
        "epoch": 0.5826666666666667,
        "step": 4370
    },
    {
        "loss": 2.4651,
        "grad_norm": 3.7053537368774414,
        "learning_rate": 8.243294052496986e-06,
        "epoch": 0.5828,
        "step": 4371
    },
    {
        "loss": 2.0647,
        "grad_norm": 2.557971715927124,
        "learning_rate": 8.218289768502363e-06,
        "epoch": 0.5829333333333333,
        "step": 4372
    },
    {
        "loss": 2.8268,
        "grad_norm": 3.985236167907715,
        "learning_rate": 8.193321839555868e-06,
        "epoch": 0.5830666666666666,
        "step": 4373
    },
    {
        "loss": 2.9676,
        "grad_norm": 3.9883906841278076,
        "learning_rate": 8.168390275547443e-06,
        "epoch": 0.5832,
        "step": 4374
    },
    {
        "loss": 1.6629,
        "grad_norm": 5.820765018463135,
        "learning_rate": 8.143495086352526e-06,
        "epoch": 0.5833333333333334,
        "step": 4375
    },
    {
        "loss": 2.6374,
        "grad_norm": 4.188417911529541,
        "learning_rate": 8.118636281832204e-06,
        "epoch": 0.5834666666666667,
        "step": 4376
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.0750856399536133,
        "learning_rate": 8.093813871833122e-06,
        "epoch": 0.5836,
        "step": 4377
    },
    {
        "loss": 1.9876,
        "grad_norm": 4.3537373542785645,
        "learning_rate": 8.069027866187517e-06,
        "epoch": 0.5837333333333333,
        "step": 4378
    },
    {
        "loss": 0.7629,
        "grad_norm": 2.4051589965820312,
        "learning_rate": 8.044278274713236e-06,
        "epoch": 0.5838666666666666,
        "step": 4379
    },
    {
        "loss": 1.7068,
        "grad_norm": 6.015425682067871,
        "learning_rate": 8.019565107213667e-06,
        "epoch": 0.584,
        "step": 4380
    },
    {
        "loss": 1.461,
        "grad_norm": 4.422887802124023,
        "learning_rate": 7.99488837347776e-06,
        "epoch": 0.5841333333333333,
        "step": 4381
    },
    {
        "loss": 2.3588,
        "grad_norm": 3.684255361557007,
        "learning_rate": 7.970248083280096e-06,
        "epoch": 0.5842666666666667,
        "step": 4382
    },
    {
        "loss": 1.6673,
        "grad_norm": 4.347966194152832,
        "learning_rate": 7.945644246380723e-06,
        "epoch": 0.5844,
        "step": 4383
    },
    {
        "loss": 1.3345,
        "grad_norm": 3.761932611465454,
        "learning_rate": 7.921076872525324e-06,
        "epoch": 0.5845333333333333,
        "step": 4384
    },
    {
        "loss": 1.1709,
        "grad_norm": 3.554347276687622,
        "learning_rate": 7.896545971445146e-06,
        "epoch": 0.5846666666666667,
        "step": 4385
    },
    {
        "loss": 2.3227,
        "grad_norm": 3.2107884883880615,
        "learning_rate": 7.872051552856929e-06,
        "epoch": 0.5848,
        "step": 4386
    },
    {
        "loss": 2.5773,
        "grad_norm": 3.183832883834839,
        "learning_rate": 7.847593626463012e-06,
        "epoch": 0.5849333333333333,
        "step": 4387
    },
    {
        "loss": 2.2047,
        "grad_norm": 3.454993963241577,
        "learning_rate": 7.823172201951267e-06,
        "epoch": 0.5850666666666666,
        "step": 4388
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.6555867195129395,
        "learning_rate": 7.798787288995124e-06,
        "epoch": 0.5852,
        "step": 4389
    },
    {
        "loss": 1.9917,
        "grad_norm": 2.6896233558654785,
        "learning_rate": 7.774438897253478e-06,
        "epoch": 0.5853333333333334,
        "step": 4390
    },
    {
        "loss": 2.3343,
        "grad_norm": 3.3091506958007812,
        "learning_rate": 7.750127036370847e-06,
        "epoch": 0.5854666666666667,
        "step": 4391
    },
    {
        "loss": 2.1897,
        "grad_norm": 3.6959378719329834,
        "learning_rate": 7.725851715977217e-06,
        "epoch": 0.5856,
        "step": 4392
    },
    {
        "loss": 0.6479,
        "grad_norm": 2.752835512161255,
        "learning_rate": 7.70161294568813e-06,
        "epoch": 0.5857333333333333,
        "step": 4393
    },
    {
        "loss": 2.4058,
        "grad_norm": 4.6665730476379395,
        "learning_rate": 7.677410735104663e-06,
        "epoch": 0.5858666666666666,
        "step": 4394
    },
    {
        "loss": 2.4292,
        "grad_norm": 2.562063694000244,
        "learning_rate": 7.6532450938134e-06,
        "epoch": 0.586,
        "step": 4395
    },
    {
        "loss": 2.6756,
        "grad_norm": 2.2140862941741943,
        "learning_rate": 7.629116031386397e-06,
        "epoch": 0.5861333333333333,
        "step": 4396
    },
    {
        "loss": 2.0094,
        "grad_norm": 2.5329599380493164,
        "learning_rate": 7.605023557381308e-06,
        "epoch": 0.5862666666666667,
        "step": 4397
    },
    {
        "loss": 2.4354,
        "grad_norm": 2.784205913543701,
        "learning_rate": 7.580967681341189e-06,
        "epoch": 0.5864,
        "step": 4398
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.6097781658172607,
        "learning_rate": 7.556948412794695e-06,
        "epoch": 0.5865333333333334,
        "step": 4399
    },
    {
        "loss": 2.7828,
        "grad_norm": 5.33450174331665,
        "learning_rate": 7.532965761255928e-06,
        "epoch": 0.5866666666666667,
        "step": 4400
    },
    {
        "loss": 1.7606,
        "grad_norm": 2.0585854053497314,
        "learning_rate": 7.5090197362244985e-06,
        "epoch": 0.5868,
        "step": 4401
    },
    {
        "loss": 2.0392,
        "grad_norm": 2.236098051071167,
        "learning_rate": 7.485110347185498e-06,
        "epoch": 0.5869333333333333,
        "step": 4402
    },
    {
        "loss": 2.6099,
        "grad_norm": 4.149422645568848,
        "learning_rate": 7.461237603609494e-06,
        "epoch": 0.5870666666666666,
        "step": 4403
    },
    {
        "loss": 2.1308,
        "grad_norm": 2.7879066467285156,
        "learning_rate": 7.4374015149526464e-06,
        "epoch": 0.5872,
        "step": 4404
    },
    {
        "loss": 2.1208,
        "grad_norm": 3.4472410678863525,
        "learning_rate": 7.4136020906564245e-06,
        "epoch": 0.5873333333333334,
        "step": 4405
    },
    {
        "loss": 2.2649,
        "grad_norm": 3.8137898445129395,
        "learning_rate": 7.389839340147919e-06,
        "epoch": 0.5874666666666667,
        "step": 4406
    },
    {
        "loss": 1.012,
        "grad_norm": 6.185749053955078,
        "learning_rate": 7.366113272839603e-06,
        "epoch": 0.5876,
        "step": 4407
    },
    {
        "loss": 2.5285,
        "grad_norm": 2.7415313720703125,
        "learning_rate": 7.342423898129491e-06,
        "epoch": 0.5877333333333333,
        "step": 4408
    },
    {
        "loss": 2.312,
        "grad_norm": 2.883305788040161,
        "learning_rate": 7.318771225400944e-06,
        "epoch": 0.5878666666666666,
        "step": 4409
    },
    {
        "loss": 2.0225,
        "grad_norm": 3.031733989715576,
        "learning_rate": 7.295155264022946e-06,
        "epoch": 0.588,
        "step": 4410
    },
    {
        "loss": 2.7707,
        "grad_norm": 2.820411205291748,
        "learning_rate": 7.2715760233498245e-06,
        "epoch": 0.5881333333333333,
        "step": 4411
    },
    {
        "loss": 2.2487,
        "grad_norm": 4.2213358879089355,
        "learning_rate": 7.248033512721408e-06,
        "epoch": 0.5882666666666667,
        "step": 4412
    },
    {
        "loss": 2.7363,
        "grad_norm": 1.8625043630599976,
        "learning_rate": 7.224527741462939e-06,
        "epoch": 0.5884,
        "step": 4413
    },
    {
        "loss": 2.2382,
        "grad_norm": 2.603942632675171,
        "learning_rate": 7.201058718885212e-06,
        "epoch": 0.5885333333333334,
        "step": 4414
    },
    {
        "loss": 2.4912,
        "grad_norm": 5.868310928344727,
        "learning_rate": 7.1776264542842805e-06,
        "epoch": 0.5886666666666667,
        "step": 4415
    },
    {
        "loss": 1.1659,
        "grad_norm": 3.233049154281616,
        "learning_rate": 7.154230956941832e-06,
        "epoch": 0.5888,
        "step": 4416
    },
    {
        "loss": 2.4144,
        "grad_norm": 3.7787413597106934,
        "learning_rate": 7.13087223612483e-06,
        "epoch": 0.5889333333333333,
        "step": 4417
    },
    {
        "loss": 1.9279,
        "grad_norm": 3.850599765777588,
        "learning_rate": 7.107550301085786e-06,
        "epoch": 0.5890666666666666,
        "step": 4418
    },
    {
        "loss": 1.5967,
        "grad_norm": 5.055149078369141,
        "learning_rate": 7.084265161062598e-06,
        "epoch": 0.5892,
        "step": 4419
    },
    {
        "loss": 2.7733,
        "grad_norm": 2.945497751235962,
        "learning_rate": 7.061016825278588e-06,
        "epoch": 0.5893333333333334,
        "step": 4420
    },
    {
        "loss": 3.2576,
        "grad_norm": 2.1703150272369385,
        "learning_rate": 7.037805302942491e-06,
        "epoch": 0.5894666666666667,
        "step": 4421
    },
    {
        "loss": 1.8059,
        "grad_norm": 3.0510122776031494,
        "learning_rate": 7.014630603248485e-06,
        "epoch": 0.5896,
        "step": 4422
    },
    {
        "loss": 1.859,
        "grad_norm": 2.985015869140625,
        "learning_rate": 6.991492735376115e-06,
        "epoch": 0.5897333333333333,
        "step": 4423
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.565289258956909,
        "learning_rate": 6.968391708490396e-06,
        "epoch": 0.5898666666666667,
        "step": 4424
    },
    {
        "loss": 1.9153,
        "grad_norm": 4.334309101104736,
        "learning_rate": 6.94532753174173e-06,
        "epoch": 0.59,
        "step": 4425
    },
    {
        "loss": 2.2794,
        "grad_norm": 3.0963010787963867,
        "learning_rate": 6.922300214265898e-06,
        "epoch": 0.5901333333333333,
        "step": 4426
    },
    {
        "loss": 1.8376,
        "grad_norm": 2.389275312423706,
        "learning_rate": 6.899309765184114e-06,
        "epoch": 0.5902666666666667,
        "step": 4427
    },
    {
        "loss": 2.4429,
        "grad_norm": 3.0478599071502686,
        "learning_rate": 6.876356193602951e-06,
        "epoch": 0.5904,
        "step": 4428
    },
    {
        "loss": 2.7016,
        "grad_norm": 3.2045750617980957,
        "learning_rate": 6.853439508614412e-06,
        "epoch": 0.5905333333333334,
        "step": 4429
    },
    {
        "loss": 2.0958,
        "grad_norm": 2.9293265342712402,
        "learning_rate": 6.830559719295871e-06,
        "epoch": 0.5906666666666667,
        "step": 4430
    },
    {
        "loss": 1.8049,
        "grad_norm": 4.029805660247803,
        "learning_rate": 6.807716834710109e-06,
        "epoch": 0.5908,
        "step": 4431
    },
    {
        "loss": 0.7436,
        "grad_norm": 2.5662567615509033,
        "learning_rate": 6.784910863905247e-06,
        "epoch": 0.5909333333333333,
        "step": 4432
    },
    {
        "loss": 2.6816,
        "grad_norm": 3.032578945159912,
        "learning_rate": 6.762141815914835e-06,
        "epoch": 0.5910666666666666,
        "step": 4433
    },
    {
        "loss": 2.7995,
        "grad_norm": 2.175732374191284,
        "learning_rate": 6.739409699757704e-06,
        "epoch": 0.5912,
        "step": 4434
    },
    {
        "loss": 1.1125,
        "grad_norm": 3.9619638919830322,
        "learning_rate": 6.716714524438206e-06,
        "epoch": 0.5913333333333334,
        "step": 4435
    },
    {
        "loss": 2.3829,
        "grad_norm": 2.5616185665130615,
        "learning_rate": 6.694056298945917e-06,
        "epoch": 0.5914666666666667,
        "step": 4436
    },
    {
        "loss": 2.4431,
        "grad_norm": 2.9464468955993652,
        "learning_rate": 6.6714350322558796e-06,
        "epoch": 0.5916,
        "step": 4437
    },
    {
        "loss": 1.5839,
        "grad_norm": 3.7691071033477783,
        "learning_rate": 6.648850733328394e-06,
        "epoch": 0.5917333333333333,
        "step": 4438
    },
    {
        "loss": 2.2789,
        "grad_norm": 3.0592756271362305,
        "learning_rate": 6.62630341110928e-06,
        "epoch": 0.5918666666666667,
        "step": 4439
    },
    {
        "loss": 1.9107,
        "grad_norm": 3.6590960025787354,
        "learning_rate": 6.603793074529508e-06,
        "epoch": 0.592,
        "step": 4440
    },
    {
        "loss": 1.7654,
        "grad_norm": 5.36260461807251,
        "learning_rate": 6.581319732505553e-06,
        "epoch": 0.5921333333333333,
        "step": 4441
    },
    {
        "loss": 3.0899,
        "grad_norm": 2.452059268951416,
        "learning_rate": 6.558883393939163e-06,
        "epoch": 0.5922666666666667,
        "step": 4442
    },
    {
        "loss": 2.286,
        "grad_norm": 2.430445909500122,
        "learning_rate": 6.536484067717452e-06,
        "epoch": 0.5924,
        "step": 4443
    },
    {
        "loss": 2.219,
        "grad_norm": 3.5310170650482178,
        "learning_rate": 6.514121762712866e-06,
        "epoch": 0.5925333333333334,
        "step": 4444
    },
    {
        "loss": 2.3608,
        "grad_norm": 2.416652202606201,
        "learning_rate": 6.491796487783186e-06,
        "epoch": 0.5926666666666667,
        "step": 4445
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.461273670196533,
        "learning_rate": 6.4695082517715725e-06,
        "epoch": 0.5928,
        "step": 4446
    },
    {
        "loss": 1.6958,
        "grad_norm": 5.479047775268555,
        "learning_rate": 6.447257063506407e-06,
        "epoch": 0.5929333333333333,
        "step": 4447
    },
    {
        "loss": 1.4045,
        "grad_norm": 5.7178544998168945,
        "learning_rate": 6.425042931801517e-06,
        "epoch": 0.5930666666666666,
        "step": 4448
    },
    {
        "loss": 1.3193,
        "grad_norm": 2.687100887298584,
        "learning_rate": 6.402865865455954e-06,
        "epoch": 0.5932,
        "step": 4449
    },
    {
        "loss": 2.4259,
        "grad_norm": 2.260139226913452,
        "learning_rate": 6.380725873254156e-06,
        "epoch": 0.5933333333333334,
        "step": 4450
    },
    {
        "loss": 1.3614,
        "grad_norm": 2.4897537231445312,
        "learning_rate": 6.358622963965833e-06,
        "epoch": 0.5934666666666667,
        "step": 4451
    },
    {
        "loss": 2.4721,
        "grad_norm": 2.243842124938965,
        "learning_rate": 6.336557146346034e-06,
        "epoch": 0.5936,
        "step": 4452
    },
    {
        "loss": 2.3202,
        "grad_norm": 3.7504544258117676,
        "learning_rate": 6.3145284291350915e-06,
        "epoch": 0.5937333333333333,
        "step": 4453
    },
    {
        "loss": 3.0852,
        "grad_norm": 3.367802381515503,
        "learning_rate": 6.292536821058659e-06,
        "epoch": 0.5938666666666667,
        "step": 4454
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.999177932739258,
        "learning_rate": 6.270582330827701e-06,
        "epoch": 0.594,
        "step": 4455
    },
    {
        "loss": 2.7138,
        "grad_norm": 2.0870909690856934,
        "learning_rate": 6.2486649671384624e-06,
        "epoch": 0.5941333333333333,
        "step": 4456
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.5516114234924316,
        "learning_rate": 6.226784738672464e-06,
        "epoch": 0.5942666666666667,
        "step": 4457
    },
    {
        "loss": 1.7835,
        "grad_norm": 3.957648992538452,
        "learning_rate": 6.204941654096597e-06,
        "epoch": 0.5944,
        "step": 4458
    },
    {
        "loss": 2.9,
        "grad_norm": 1.9354232549667358,
        "learning_rate": 6.183135722062894e-06,
        "epoch": 0.5945333333333334,
        "step": 4459
    },
    {
        "loss": 2.502,
        "grad_norm": 3.7342655658721924,
        "learning_rate": 6.161366951208813e-06,
        "epoch": 0.5946666666666667,
        "step": 4460
    },
    {
        "loss": 2.2307,
        "grad_norm": 6.503857135772705,
        "learning_rate": 6.139635350157036e-06,
        "epoch": 0.5948,
        "step": 4461
    },
    {
        "loss": 1.861,
        "grad_norm": 3.4194016456604004,
        "learning_rate": 6.11794092751552e-06,
        "epoch": 0.5949333333333333,
        "step": 4462
    },
    {
        "loss": 2.1142,
        "grad_norm": 2.7186238765716553,
        "learning_rate": 6.09628369187748e-06,
        "epoch": 0.5950666666666666,
        "step": 4463
    },
    {
        "loss": 2.6144,
        "grad_norm": 2.463013172149658,
        "learning_rate": 6.074663651821444e-06,
        "epoch": 0.5952,
        "step": 4464
    },
    {
        "loss": 2.6102,
        "grad_norm": 2.5667941570281982,
        "learning_rate": 6.053080815911216e-06,
        "epoch": 0.5953333333333334,
        "step": 4465
    },
    {
        "loss": 2.0207,
        "grad_norm": 4.636929988861084,
        "learning_rate": 6.031535192695758e-06,
        "epoch": 0.5954666666666667,
        "step": 4466
    },
    {
        "loss": 2.0449,
        "grad_norm": 4.097538948059082,
        "learning_rate": 6.010026790709422e-06,
        "epoch": 0.5956,
        "step": 4467
    },
    {
        "loss": 2.2146,
        "grad_norm": 4.027776718139648,
        "learning_rate": 5.988555618471714e-06,
        "epoch": 0.5957333333333333,
        "step": 4468
    },
    {
        "loss": 2.5758,
        "grad_norm": 2.8918495178222656,
        "learning_rate": 5.967121684487464e-06,
        "epoch": 0.5958666666666667,
        "step": 4469
    },
    {
        "loss": 1.9511,
        "grad_norm": 2.747835159301758,
        "learning_rate": 5.945724997246727e-06,
        "epoch": 0.596,
        "step": 4470
    },
    {
        "loss": 2.7728,
        "grad_norm": 2.4480481147766113,
        "learning_rate": 5.92436556522481e-06,
        "epoch": 0.5961333333333333,
        "step": 4471
    },
    {
        "loss": 2.1766,
        "grad_norm": 3.076507091522217,
        "learning_rate": 5.903043396882235e-06,
        "epoch": 0.5962666666666666,
        "step": 4472
    },
    {
        "loss": 2.0679,
        "grad_norm": 3.2197258472442627,
        "learning_rate": 5.881758500664813e-06,
        "epoch": 0.5964,
        "step": 4473
    },
    {
        "loss": 2.2529,
        "grad_norm": 2.0242979526519775,
        "learning_rate": 5.860510885003534e-06,
        "epoch": 0.5965333333333334,
        "step": 4474
    },
    {
        "loss": 2.2718,
        "grad_norm": 2.5085902214050293,
        "learning_rate": 5.839300558314686e-06,
        "epoch": 0.5966666666666667,
        "step": 4475
    },
    {
        "loss": 2.5181,
        "grad_norm": 4.4145097732543945,
        "learning_rate": 5.8181275289997066e-06,
        "epoch": 0.5968,
        "step": 4476
    },
    {
        "loss": 2.123,
        "grad_norm": 4.888359069824219,
        "learning_rate": 5.796991805445351e-06,
        "epoch": 0.5969333333333333,
        "step": 4477
    },
    {
        "loss": 2.9738,
        "grad_norm": 3.0233206748962402,
        "learning_rate": 5.775893396023513e-06,
        "epoch": 0.5970666666666666,
        "step": 4478
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.5915684700012207,
        "learning_rate": 5.754832309091362e-06,
        "epoch": 0.5972,
        "step": 4479
    },
    {
        "loss": 1.125,
        "grad_norm": 5.592819690704346,
        "learning_rate": 5.733808552991271e-06,
        "epoch": 0.5973333333333334,
        "step": 4480
    },
    {
        "loss": 0.8714,
        "grad_norm": 3.9881064891815186,
        "learning_rate": 5.7128221360508326e-06,
        "epoch": 0.5974666666666667,
        "step": 4481
    },
    {
        "loss": 0.5061,
        "grad_norm": 2.413605213165283,
        "learning_rate": 5.691873066582798e-06,
        "epoch": 0.5976,
        "step": 4482
    },
    {
        "loss": 1.8019,
        "grad_norm": 7.342085361480713,
        "learning_rate": 5.670961352885218e-06,
        "epoch": 0.5977333333333333,
        "step": 4483
    },
    {
        "loss": 2.4006,
        "grad_norm": 2.207977533340454,
        "learning_rate": 5.650087003241244e-06,
        "epoch": 0.5978666666666667,
        "step": 4484
    },
    {
        "loss": 1.9269,
        "grad_norm": 2.8096492290496826,
        "learning_rate": 5.629250025919275e-06,
        "epoch": 0.598,
        "step": 4485
    },
    {
        "loss": 2.2512,
        "grad_norm": 3.762345314025879,
        "learning_rate": 5.608450429172963e-06,
        "epoch": 0.5981333333333333,
        "step": 4486
    },
    {
        "loss": 2.2689,
        "grad_norm": 2.4953343868255615,
        "learning_rate": 5.58768822124105e-06,
        "epoch": 0.5982666666666666,
        "step": 4487
    },
    {
        "loss": 3.0413,
        "grad_norm": 3.32749605178833,
        "learning_rate": 5.566963410347536e-06,
        "epoch": 0.5984,
        "step": 4488
    },
    {
        "loss": 2.0244,
        "grad_norm": 5.058942794799805,
        "learning_rate": 5.546276004701589e-06,
        "epoch": 0.5985333333333334,
        "step": 4489
    },
    {
        "loss": 1.3933,
        "grad_norm": 3.6876256465911865,
        "learning_rate": 5.525626012497587e-06,
        "epoch": 0.5986666666666667,
        "step": 4490
    },
    {
        "loss": 2.3211,
        "grad_norm": 3.10844087600708,
        "learning_rate": 5.505013441915008e-06,
        "epoch": 0.5988,
        "step": 4491
    },
    {
        "loss": 1.4021,
        "grad_norm": 3.260403871536255,
        "learning_rate": 5.48443830111861e-06,
        "epoch": 0.5989333333333333,
        "step": 4492
    },
    {
        "loss": 2.3834,
        "grad_norm": 1.7871519327163696,
        "learning_rate": 5.463900598258232e-06,
        "epoch": 0.5990666666666666,
        "step": 4493
    },
    {
        "loss": 2.4269,
        "grad_norm": 4.139883995056152,
        "learning_rate": 5.443400341468952e-06,
        "epoch": 0.5992,
        "step": 4494
    },
    {
        "loss": 2.5863,
        "grad_norm": 3.0631041526794434,
        "learning_rate": 5.422937538870987e-06,
        "epoch": 0.5993333333333334,
        "step": 4495
    },
    {
        "loss": 2.37,
        "grad_norm": 2.5145022869110107,
        "learning_rate": 5.402512198569742e-06,
        "epoch": 0.5994666666666667,
        "step": 4496
    },
    {
        "loss": 3.3961,
        "grad_norm": 3.305124044418335,
        "learning_rate": 5.3821243286557355e-06,
        "epoch": 0.5996,
        "step": 4497
    },
    {
        "loss": 2.2837,
        "grad_norm": 2.6553092002868652,
        "learning_rate": 5.361773937204706e-06,
        "epoch": 0.5997333333333333,
        "step": 4498
    },
    {
        "loss": 2.1454,
        "grad_norm": 2.91094708442688,
        "learning_rate": 5.3414610322774724e-06,
        "epoch": 0.5998666666666667,
        "step": 4499
    },
    {
        "loss": 2.4083,
        "grad_norm": 2.214848518371582,
        "learning_rate": 5.321185621920077e-06,
        "epoch": 0.6,
        "step": 4500
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.714719772338867,
        "learning_rate": 5.300947714163651e-06,
        "epoch": 0.6001333333333333,
        "step": 4501
    },
    {
        "loss": 3.0727,
        "grad_norm": 2.630321979522705,
        "learning_rate": 5.2807473170245365e-06,
        "epoch": 0.6002666666666666,
        "step": 4502
    },
    {
        "loss": 2.5583,
        "grad_norm": 2.541933298110962,
        "learning_rate": 5.260584438504135e-06,
        "epoch": 0.6004,
        "step": 4503
    },
    {
        "loss": 2.3227,
        "grad_norm": 2.8182830810546875,
        "learning_rate": 5.240459086589045e-06,
        "epoch": 0.6005333333333334,
        "step": 4504
    },
    {
        "loss": 1.9599,
        "grad_norm": 3.758256196975708,
        "learning_rate": 5.220371269251023e-06,
        "epoch": 0.6006666666666667,
        "step": 4505
    },
    {
        "loss": 2.1298,
        "grad_norm": 2.765840768814087,
        "learning_rate": 5.200320994446883e-06,
        "epoch": 0.6008,
        "step": 4506
    },
    {
        "loss": 1.6962,
        "grad_norm": 2.45340633392334,
        "learning_rate": 5.180308270118628e-06,
        "epoch": 0.6009333333333333,
        "step": 4507
    },
    {
        "loss": 1.0724,
        "grad_norm": 4.474898815155029,
        "learning_rate": 5.160333104193349e-06,
        "epoch": 0.6010666666666666,
        "step": 4508
    },
    {
        "loss": 2.7639,
        "grad_norm": 3.02666974067688,
        "learning_rate": 5.1403955045833064e-06,
        "epoch": 0.6012,
        "step": 4509
    },
    {
        "loss": 2.6222,
        "grad_norm": 5.142009258270264,
        "learning_rate": 5.120495479185794e-06,
        "epoch": 0.6013333333333334,
        "step": 4510
    },
    {
        "loss": 2.638,
        "grad_norm": 3.107609510421753,
        "learning_rate": 5.10063303588334e-06,
        "epoch": 0.6014666666666667,
        "step": 4511
    },
    {
        "loss": 2.7982,
        "grad_norm": 1.8040746450424194,
        "learning_rate": 5.080808182543495e-06,
        "epoch": 0.6016,
        "step": 4512
    },
    {
        "loss": 3.0195,
        "grad_norm": 2.2157983779907227,
        "learning_rate": 5.0610209270189665e-06,
        "epoch": 0.6017333333333333,
        "step": 4513
    },
    {
        "loss": 2.5608,
        "grad_norm": 1.8394367694854736,
        "learning_rate": 5.041271277147519e-06,
        "epoch": 0.6018666666666667,
        "step": 4514
    },
    {
        "loss": 2.0042,
        "grad_norm": 2.9648594856262207,
        "learning_rate": 5.021559240752127e-06,
        "epoch": 0.602,
        "step": 4515
    },
    {
        "loss": 2.8129,
        "grad_norm": 3.9645156860351562,
        "learning_rate": 5.0018848256407235e-06,
        "epoch": 0.6021333333333333,
        "step": 4516
    },
    {
        "loss": 2.1708,
        "grad_norm": 3.2497878074645996,
        "learning_rate": 4.982248039606441e-06,
        "epoch": 0.6022666666666666,
        "step": 4517
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.6684157848358154,
        "learning_rate": 4.9626488904274796e-06,
        "epoch": 0.6024,
        "step": 4518
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.7340450286865234,
        "learning_rate": 4.943087385867106e-06,
        "epoch": 0.6025333333333334,
        "step": 4519
    },
    {
        "loss": 2.1387,
        "grad_norm": 2.8243355751037598,
        "learning_rate": 4.923563533673736e-06,
        "epoch": 0.6026666666666667,
        "step": 4520
    },
    {
        "loss": 2.3011,
        "grad_norm": 3.2466907501220703,
        "learning_rate": 4.904077341580815e-06,
        "epoch": 0.6028,
        "step": 4521
    },
    {
        "loss": 1.7877,
        "grad_norm": 2.1708428859710693,
        "learning_rate": 4.88462881730688e-06,
        "epoch": 0.6029333333333333,
        "step": 4522
    },
    {
        "loss": 2.5969,
        "grad_norm": 2.6005895137786865,
        "learning_rate": 4.8652179685555795e-06,
        "epoch": 0.6030666666666666,
        "step": 4523
    },
    {
        "loss": 2.3684,
        "grad_norm": 2.058816432952881,
        "learning_rate": 4.8458448030155975e-06,
        "epoch": 0.6032,
        "step": 4524
    },
    {
        "loss": 2.5515,
        "grad_norm": 1.652170181274414,
        "learning_rate": 4.826509328360718e-06,
        "epoch": 0.6033333333333334,
        "step": 4525
    },
    {
        "loss": 2.2958,
        "grad_norm": 4.52593994140625,
        "learning_rate": 4.807211552249802e-06,
        "epoch": 0.6034666666666667,
        "step": 4526
    },
    {
        "loss": 2.5488,
        "grad_norm": 2.6588642597198486,
        "learning_rate": 4.787951482326736e-06,
        "epoch": 0.6036,
        "step": 4527
    },
    {
        "loss": 0.8352,
        "grad_norm": 2.9625813961029053,
        "learning_rate": 4.768729126220528e-06,
        "epoch": 0.6037333333333333,
        "step": 4528
    },
    {
        "loss": 2.5414,
        "grad_norm": 3.9206676483154297,
        "learning_rate": 4.749544491545199e-06,
        "epoch": 0.6038666666666667,
        "step": 4529
    },
    {
        "loss": 1.6861,
        "grad_norm": 3.394592761993408,
        "learning_rate": 4.7303975858998485e-06,
        "epoch": 0.604,
        "step": 4530
    },
    {
        "loss": 1.7964,
        "grad_norm": 3.9951722621917725,
        "learning_rate": 4.711288416868642e-06,
        "epoch": 0.6041333333333333,
        "step": 4531
    },
    {
        "loss": 2.8612,
        "grad_norm": 1.6981046199798584,
        "learning_rate": 4.6922169920207925e-06,
        "epoch": 0.6042666666666666,
        "step": 4532
    },
    {
        "loss": 3.0873,
        "grad_norm": 2.151782751083374,
        "learning_rate": 4.673183318910534e-06,
        "epoch": 0.6044,
        "step": 4533
    },
    {
        "loss": 1.2826,
        "grad_norm": 2.050060510635376,
        "learning_rate": 4.654187405077204e-06,
        "epoch": 0.6045333333333334,
        "step": 4534
    },
    {
        "loss": 0.9836,
        "grad_norm": 2.882603883743286,
        "learning_rate": 4.635229258045082e-06,
        "epoch": 0.6046666666666667,
        "step": 4535
    },
    {
        "loss": 2.9216,
        "grad_norm": 2.708301067352295,
        "learning_rate": 4.6163088853236395e-06,
        "epoch": 0.6048,
        "step": 4536
    },
    {
        "loss": 2.2723,
        "grad_norm": 4.836886405944824,
        "learning_rate": 4.597426294407237e-06,
        "epoch": 0.6049333333333333,
        "step": 4537
    },
    {
        "loss": 1.9848,
        "grad_norm": 3.8687586784362793,
        "learning_rate": 4.578581492775369e-06,
        "epoch": 0.6050666666666666,
        "step": 4538
    },
    {
        "loss": 1.5597,
        "grad_norm": 3.0839691162109375,
        "learning_rate": 4.5597744878924765e-06,
        "epoch": 0.6052,
        "step": 4539
    },
    {
        "loss": 1.9938,
        "grad_norm": 3.5926151275634766,
        "learning_rate": 4.5410052872081664e-06,
        "epoch": 0.6053333333333333,
        "step": 4540
    },
    {
        "loss": 2.577,
        "grad_norm": 1.7286949157714844,
        "learning_rate": 4.522273898156903e-06,
        "epoch": 0.6054666666666667,
        "step": 4541
    },
    {
        "loss": 2.2764,
        "grad_norm": 3.2485179901123047,
        "learning_rate": 4.503580328158297e-06,
        "epoch": 0.6056,
        "step": 4542
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.29011607170105,
        "learning_rate": 4.484924584616901e-06,
        "epoch": 0.6057333333333333,
        "step": 4543
    },
    {
        "loss": 2.8787,
        "grad_norm": 3.2037034034729004,
        "learning_rate": 4.466306674922327e-06,
        "epoch": 0.6058666666666667,
        "step": 4544
    },
    {
        "loss": 2.8738,
        "grad_norm": 2.210757255554199,
        "learning_rate": 4.4477266064492205e-06,
        "epoch": 0.606,
        "step": 4545
    },
    {
        "loss": 2.863,
        "grad_norm": 2.8878378868103027,
        "learning_rate": 4.429184386557184e-06,
        "epoch": 0.6061333333333333,
        "step": 4546
    },
    {
        "loss": 2.605,
        "grad_norm": 2.3177905082702637,
        "learning_rate": 4.410680022590863e-06,
        "epoch": 0.6062666666666666,
        "step": 4547
    },
    {
        "loss": 1.9229,
        "grad_norm": 3.2221250534057617,
        "learning_rate": 4.392213521879896e-06,
        "epoch": 0.6064,
        "step": 4548
    },
    {
        "loss": 2.5784,
        "grad_norm": 2.8522369861602783,
        "learning_rate": 4.373784891738941e-06,
        "epoch": 0.6065333333333334,
        "step": 4549
    },
    {
        "loss": 2.4632,
        "grad_norm": 3.3694443702697754,
        "learning_rate": 4.355394139467617e-06,
        "epoch": 0.6066666666666667,
        "step": 4550
    },
    {
        "loss": 1.8539,
        "grad_norm": 2.1674230098724365,
        "learning_rate": 4.3370412723505835e-06,
        "epoch": 0.6068,
        "step": 4551
    },
    {
        "loss": 2.2832,
        "grad_norm": 3.2948648929595947,
        "learning_rate": 4.318726297657461e-06,
        "epoch": 0.6069333333333333,
        "step": 4552
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.910548210144043,
        "learning_rate": 4.300449222642888e-06,
        "epoch": 0.6070666666666666,
        "step": 4553
    },
    {
        "loss": 2.5982,
        "grad_norm": 1.4085602760314941,
        "learning_rate": 4.282210054546454e-06,
        "epoch": 0.6072,
        "step": 4554
    },
    {
        "loss": 2.7707,
        "grad_norm": 3.2412192821502686,
        "learning_rate": 4.264008800592756e-06,
        "epoch": 0.6073333333333333,
        "step": 4555
    },
    {
        "loss": 2.2376,
        "grad_norm": 3.769953727722168,
        "learning_rate": 4.2458454679913936e-06,
        "epoch": 0.6074666666666667,
        "step": 4556
    },
    {
        "loss": 2.8054,
        "grad_norm": 1.8725357055664062,
        "learning_rate": 4.227720063936935e-06,
        "epoch": 0.6076,
        "step": 4557
    },
    {
        "loss": 2.7178,
        "grad_norm": 2.2980027198791504,
        "learning_rate": 4.20963259560887e-06,
        "epoch": 0.6077333333333333,
        "step": 4558
    },
    {
        "loss": 2.4704,
        "grad_norm": 1.996601939201355,
        "learning_rate": 4.191583070171768e-06,
        "epoch": 0.6078666666666667,
        "step": 4559
    },
    {
        "loss": 2.1486,
        "grad_norm": 2.274261713027954,
        "learning_rate": 4.17357149477503e-06,
        "epoch": 0.608,
        "step": 4560
    },
    {
        "loss": 1.9883,
        "grad_norm": 5.348391532897949,
        "learning_rate": 4.155597876553163e-06,
        "epoch": 0.6081333333333333,
        "step": 4561
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.5988714694976807,
        "learning_rate": 4.137662222625549e-06,
        "epoch": 0.6082666666666666,
        "step": 4562
    },
    {
        "loss": 2.516,
        "grad_norm": 3.075650215148926,
        "learning_rate": 4.1197645400965826e-06,
        "epoch": 0.6084,
        "step": 4563
    },
    {
        "loss": 1.5269,
        "grad_norm": 7.147579193115234,
        "learning_rate": 4.101904836055559e-06,
        "epoch": 0.6085333333333334,
        "step": 4564
    },
    {
        "loss": 2.0904,
        "grad_norm": 2.857830286026001,
        "learning_rate": 4.084083117576809e-06,
        "epoch": 0.6086666666666667,
        "step": 4565
    },
    {
        "loss": 2.7149,
        "grad_norm": 3.0775277614593506,
        "learning_rate": 4.066299391719597e-06,
        "epoch": 0.6088,
        "step": 4566
    },
    {
        "loss": 1.8051,
        "grad_norm": 2.7270658016204834,
        "learning_rate": 4.048553665528043e-06,
        "epoch": 0.6089333333333333,
        "step": 4567
    },
    {
        "loss": 1.8653,
        "grad_norm": 3.4622044563293457,
        "learning_rate": 4.030845946031348e-06,
        "epoch": 0.6090666666666666,
        "step": 4568
    },
    {
        "loss": 2.4858,
        "grad_norm": 3.833897590637207,
        "learning_rate": 4.013176240243565e-06,
        "epoch": 0.6092,
        "step": 4569
    },
    {
        "loss": 2.6015,
        "grad_norm": 3.835893392562866,
        "learning_rate": 3.995544555163744e-06,
        "epoch": 0.6093333333333333,
        "step": 4570
    },
    {
        "loss": 1.7767,
        "grad_norm": 4.879337787628174,
        "learning_rate": 3.977950897775851e-06,
        "epoch": 0.6094666666666667,
        "step": 4571
    },
    {
        "loss": 2.2312,
        "grad_norm": 2.6129908561706543,
        "learning_rate": 3.9603952750488e-06,
        "epoch": 0.6096,
        "step": 4572
    },
    {
        "loss": 2.0786,
        "grad_norm": 2.9790902137756348,
        "learning_rate": 3.94287769393642e-06,
        "epoch": 0.6097333333333333,
        "step": 4573
    },
    {
        "loss": 2.7876,
        "grad_norm": 2.360746383666992,
        "learning_rate": 3.925398161377502e-06,
        "epoch": 0.6098666666666667,
        "step": 4574
    },
    {
        "loss": 2.8001,
        "grad_norm": 3.893976926803589,
        "learning_rate": 3.907956684295733e-06,
        "epoch": 0.61,
        "step": 4575
    },
    {
        "loss": 1.9756,
        "grad_norm": 2.8441617488861084,
        "learning_rate": 3.8905532695997614e-06,
        "epoch": 0.6101333333333333,
        "step": 4576
    },
    {
        "loss": 1.9197,
        "grad_norm": 3.595452308654785,
        "learning_rate": 3.8731879241831085e-06,
        "epoch": 0.6102666666666666,
        "step": 4577
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.707864761352539,
        "learning_rate": 3.85586065492427e-06,
        "epoch": 0.6104,
        "step": 4578
    },
    {
        "loss": 2.8492,
        "grad_norm": 2.6701083183288574,
        "learning_rate": 3.838571468686614e-06,
        "epoch": 0.6105333333333334,
        "step": 4579
    },
    {
        "loss": 1.9401,
        "grad_norm": 2.4646222591400146,
        "learning_rate": 3.821320372318471e-06,
        "epoch": 0.6106666666666667,
        "step": 4580
    },
    {
        "loss": 2.6664,
        "grad_norm": 2.38175106048584,
        "learning_rate": 3.804107372653043e-06,
        "epoch": 0.6108,
        "step": 4581
    },
    {
        "loss": 1.2929,
        "grad_norm": 3.7419376373291016,
        "learning_rate": 3.7869324765084868e-06,
        "epoch": 0.6109333333333333,
        "step": 4582
    },
    {
        "loss": 1.7643,
        "grad_norm": 3.1033222675323486,
        "learning_rate": 3.769795690687794e-06,
        "epoch": 0.6110666666666666,
        "step": 4583
    },
    {
        "loss": 2.3156,
        "grad_norm": 4.527652740478516,
        "learning_rate": 3.7526970219789238e-06,
        "epoch": 0.6112,
        "step": 4584
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.2269721031188965,
        "learning_rate": 3.735636477154736e-06,
        "epoch": 0.6113333333333333,
        "step": 4585
    },
    {
        "loss": 2.186,
        "grad_norm": 2.5076777935028076,
        "learning_rate": 3.7186140629729247e-06,
        "epoch": 0.6114666666666667,
        "step": 4586
    },
    {
        "loss": 2.3934,
        "grad_norm": 3.287997007369995,
        "learning_rate": 3.7016297861761864e-06,
        "epoch": 0.6116,
        "step": 4587
    },
    {
        "loss": 2.5826,
        "grad_norm": 2.9921045303344727,
        "learning_rate": 3.6846836534919935e-06,
        "epoch": 0.6117333333333334,
        "step": 4588
    },
    {
        "loss": 1.4051,
        "grad_norm": 2.318603515625,
        "learning_rate": 3.6677756716327893e-06,
        "epoch": 0.6118666666666667,
        "step": 4589
    },
    {
        "loss": 0.6335,
        "grad_norm": 3.178433656692505,
        "learning_rate": 3.6509058472958934e-06,
        "epoch": 0.612,
        "step": 4590
    },
    {
        "loss": 1.4338,
        "grad_norm": 4.13812255859375,
        "learning_rate": 3.6340741871635165e-06,
        "epoch": 0.6121333333333333,
        "step": 4591
    },
    {
        "loss": 2.3593,
        "grad_norm": 4.712899684906006,
        "learning_rate": 3.6172806979026917e-06,
        "epoch": 0.6122666666666666,
        "step": 4592
    },
    {
        "loss": 1.8803,
        "grad_norm": 4.594113826751709,
        "learning_rate": 3.6005253861653985e-06,
        "epoch": 0.6124,
        "step": 4593
    },
    {
        "loss": 1.6235,
        "grad_norm": 3.435662269592285,
        "learning_rate": 3.5838082585884723e-06,
        "epoch": 0.6125333333333334,
        "step": 4594
    },
    {
        "loss": 3.2625,
        "grad_norm": 2.3105247020721436,
        "learning_rate": 3.567129321793616e-06,
        "epoch": 0.6126666666666667,
        "step": 4595
    },
    {
        "loss": 2.9139,
        "grad_norm": 3.0017313957214355,
        "learning_rate": 3.5504885823874346e-06,
        "epoch": 0.6128,
        "step": 4596
    },
    {
        "loss": 2.469,
        "grad_norm": 2.6601316928863525,
        "learning_rate": 3.5338860469613766e-06,
        "epoch": 0.6129333333333333,
        "step": 4597
    },
    {
        "loss": 2.4415,
        "grad_norm": 2.0872035026550293,
        "learning_rate": 3.5173217220917375e-06,
        "epoch": 0.6130666666666666,
        "step": 4598
    },
    {
        "loss": 2.0331,
        "grad_norm": 3.341686725616455,
        "learning_rate": 3.500795614339736e-06,
        "epoch": 0.6132,
        "step": 4599
    },
    {
        "loss": 2.0955,
        "grad_norm": 2.7445671558380127,
        "learning_rate": 3.4843077302513906e-06,
        "epoch": 0.6133333333333333,
        "step": 4600
    },
    {
        "loss": 2.7571,
        "grad_norm": 2.924346685409546,
        "learning_rate": 3.4678580763576442e-06,
        "epoch": 0.6134666666666667,
        "step": 4601
    },
    {
        "loss": 2.4378,
        "grad_norm": 1.8572620153427124,
        "learning_rate": 3.451446659174218e-06,
        "epoch": 0.6136,
        "step": 4602
    },
    {
        "loss": 2.0264,
        "grad_norm": 3.100315570831299,
        "learning_rate": 3.4350734852017673e-06,
        "epoch": 0.6137333333333334,
        "step": 4603
    },
    {
        "loss": 2.8635,
        "grad_norm": 2.9958925247192383,
        "learning_rate": 3.418738560925727e-06,
        "epoch": 0.6138666666666667,
        "step": 4604
    },
    {
        "loss": 2.6822,
        "grad_norm": 2.722414016723633,
        "learning_rate": 3.4024418928164438e-06,
        "epoch": 0.614,
        "step": 4605
    },
    {
        "loss": 2.9081,
        "grad_norm": 2.7650837898254395,
        "learning_rate": 3.3861834873290755e-06,
        "epoch": 0.6141333333333333,
        "step": 4606
    },
    {
        "loss": 2.1009,
        "grad_norm": 4.337416172027588,
        "learning_rate": 3.3699633509036268e-06,
        "epoch": 0.6142666666666666,
        "step": 4607
    },
    {
        "loss": 2.7512,
        "grad_norm": 2.965736150741577,
        "learning_rate": 3.3537814899649578e-06,
        "epoch": 0.6144,
        "step": 4608
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.9526617527008057,
        "learning_rate": 3.3376379109227527e-06,
        "epoch": 0.6145333333333334,
        "step": 4609
    },
    {
        "loss": 2.3078,
        "grad_norm": 2.963252067565918,
        "learning_rate": 3.3215326201715636e-06,
        "epoch": 0.6146666666666667,
        "step": 4610
    },
    {
        "loss": 2.2388,
        "grad_norm": 3.018972158432007,
        "learning_rate": 3.305465624090687e-06,
        "epoch": 0.6148,
        "step": 4611
    },
    {
        "loss": 2.2997,
        "grad_norm": 2.4772069454193115,
        "learning_rate": 3.2894369290443873e-06,
        "epoch": 0.6149333333333333,
        "step": 4612
    },
    {
        "loss": 2.3664,
        "grad_norm": 4.90451192855835,
        "learning_rate": 3.2734465413816417e-06,
        "epoch": 0.6150666666666667,
        "step": 4613
    },
    {
        "loss": 2.8657,
        "grad_norm": 2.3373618125915527,
        "learning_rate": 3.2574944674363063e-06,
        "epoch": 0.6152,
        "step": 4614
    },
    {
        "loss": 2.9465,
        "grad_norm": 2.1032063961029053,
        "learning_rate": 3.2415807135270483e-06,
        "epoch": 0.6153333333333333,
        "step": 4615
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.810392141342163,
        "learning_rate": 3.2257052859573923e-06,
        "epoch": 0.6154666666666667,
        "step": 4616
    },
    {
        "loss": 2.806,
        "grad_norm": 4.247824668884277,
        "learning_rate": 3.209868191015597e-06,
        "epoch": 0.6156,
        "step": 4617
    },
    {
        "loss": 1.6594,
        "grad_norm": 3.8682668209075928,
        "learning_rate": 3.194069434974822e-06,
        "epoch": 0.6157333333333334,
        "step": 4618
    },
    {
        "loss": 2.3343,
        "grad_norm": 4.407042980194092,
        "learning_rate": 3.178309024093007e-06,
        "epoch": 0.6158666666666667,
        "step": 4619
    },
    {
        "loss": 0.9019,
        "grad_norm": 2.8821945190429688,
        "learning_rate": 3.1625869646128903e-06,
        "epoch": 0.616,
        "step": 4620
    },
    {
        "loss": 2.3905,
        "grad_norm": 2.68109393119812,
        "learning_rate": 3.1469032627620354e-06,
        "epoch": 0.6161333333333333,
        "step": 4621
    },
    {
        "loss": 1.8847,
        "grad_norm": 2.8749403953552246,
        "learning_rate": 3.1312579247528393e-06,
        "epoch": 0.6162666666666666,
        "step": 4622
    },
    {
        "loss": 1.917,
        "grad_norm": 3.5030791759490967,
        "learning_rate": 3.1156509567824345e-06,
        "epoch": 0.6164,
        "step": 4623
    },
    {
        "loss": 2.8649,
        "grad_norm": 2.0350258350372314,
        "learning_rate": 3.10008236503283e-06,
        "epoch": 0.6165333333333334,
        "step": 4624
    },
    {
        "loss": 2.2994,
        "grad_norm": 4.591888904571533,
        "learning_rate": 3.0845521556707614e-06,
        "epoch": 0.6166666666666667,
        "step": 4625
    },
    {
        "loss": 2.7458,
        "grad_norm": 2.8977322578430176,
        "learning_rate": 3.0690603348478066e-06,
        "epoch": 0.6168,
        "step": 4626
    },
    {
        "loss": 2.6881,
        "grad_norm": 1.9155936241149902,
        "learning_rate": 3.0536069087003573e-06,
        "epoch": 0.6169333333333333,
        "step": 4627
    },
    {
        "loss": 1.3574,
        "grad_norm": 3.3418545722961426,
        "learning_rate": 3.0381918833495394e-06,
        "epoch": 0.6170666666666667,
        "step": 4628
    },
    {
        "loss": 2.4374,
        "grad_norm": 3.3824682235717773,
        "learning_rate": 3.022815264901313e-06,
        "epoch": 0.6172,
        "step": 4629
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.6121582984924316,
        "learning_rate": 3.007477059446395e-06,
        "epoch": 0.6173333333333333,
        "step": 4630
    },
    {
        "loss": 3.3099,
        "grad_norm": 3.1795167922973633,
        "learning_rate": 2.9921772730603036e-06,
        "epoch": 0.6174666666666667,
        "step": 4631
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.415865421295166,
        "learning_rate": 2.9769159118033464e-06,
        "epoch": 0.6176,
        "step": 4632
    },
    {
        "loss": 1.1668,
        "grad_norm": 8.935633659362793,
        "learning_rate": 2.9616929817205987e-06,
        "epoch": 0.6177333333333334,
        "step": 4633
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.8752235174179077,
        "learning_rate": 2.9465084888419037e-06,
        "epoch": 0.6178666666666667,
        "step": 4634
    },
    {
        "loss": 1.1938,
        "grad_norm": 3.6164956092834473,
        "learning_rate": 2.931362439181917e-06,
        "epoch": 0.618,
        "step": 4635
    },
    {
        "loss": 1.7834,
        "grad_norm": 2.952044725418091,
        "learning_rate": 2.9162548387399957e-06,
        "epoch": 0.6181333333333333,
        "step": 4636
    },
    {
        "loss": 1.9953,
        "grad_norm": 2.8880131244659424,
        "learning_rate": 2.9011856935003525e-06,
        "epoch": 0.6182666666666666,
        "step": 4637
    },
    {
        "loss": 3.634,
        "grad_norm": 4.454171657562256,
        "learning_rate": 2.886155009431901e-06,
        "epoch": 0.6184,
        "step": 4638
    },
    {
        "loss": 1.8976,
        "grad_norm": 3.782716751098633,
        "learning_rate": 2.871162792488369e-06,
        "epoch": 0.6185333333333334,
        "step": 4639
    },
    {
        "loss": 2.02,
        "grad_norm": 4.2102742195129395,
        "learning_rate": 2.856209048608194e-06,
        "epoch": 0.6186666666666667,
        "step": 4640
    },
    {
        "loss": 1.6528,
        "grad_norm": 4.146664142608643,
        "learning_rate": 2.841293783714649e-06,
        "epoch": 0.6188,
        "step": 4641
    },
    {
        "loss": 2.5062,
        "grad_norm": 2.81573748588562,
        "learning_rate": 2.8264170037156866e-06,
        "epoch": 0.6189333333333333,
        "step": 4642
    },
    {
        "loss": 1.8346,
        "grad_norm": 3.4615838527679443,
        "learning_rate": 2.8115787145040595e-06,
        "epoch": 0.6190666666666667,
        "step": 4643
    },
    {
        "loss": 2.3428,
        "grad_norm": 2.7761640548706055,
        "learning_rate": 2.796778921957266e-06,
        "epoch": 0.6192,
        "step": 4644
    },
    {
        "loss": 2.3663,
        "grad_norm": 3.699481725692749,
        "learning_rate": 2.78201763193755e-06,
        "epoch": 0.6193333333333333,
        "step": 4645
    },
    {
        "loss": 2.1686,
        "grad_norm": 3.2173378467559814,
        "learning_rate": 2.7672948502919014e-06,
        "epoch": 0.6194666666666667,
        "step": 4646
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.658038377761841,
        "learning_rate": 2.7526105828520886e-06,
        "epoch": 0.6196,
        "step": 4647
    },
    {
        "loss": 2.5488,
        "grad_norm": 2.6486475467681885,
        "learning_rate": 2.737964835434592e-06,
        "epoch": 0.6197333333333334,
        "step": 4648
    },
    {
        "loss": 2.6231,
        "grad_norm": 2.781644105911255,
        "learning_rate": 2.7233576138406157e-06,
        "epoch": 0.6198666666666667,
        "step": 4649
    },
    {
        "loss": 0.8692,
        "grad_norm": 5.457399845123291,
        "learning_rate": 2.7087889238561758e-06,
        "epoch": 0.62,
        "step": 4650
    },
    {
        "loss": 0.8693,
        "grad_norm": 3.701364755630493,
        "learning_rate": 2.694258771251934e-06,
        "epoch": 0.6201333333333333,
        "step": 4651
    },
    {
        "loss": 2.7333,
        "grad_norm": 2.4092295169830322,
        "learning_rate": 2.6797671617833754e-06,
        "epoch": 0.6202666666666666,
        "step": 4652
    },
    {
        "loss": 3.1723,
        "grad_norm": 3.5414066314697266,
        "learning_rate": 2.665314101190641e-06,
        "epoch": 0.6204,
        "step": 4653
    },
    {
        "loss": 0.5805,
        "grad_norm": 2.065791130065918,
        "learning_rate": 2.6508995951986527e-06,
        "epoch": 0.6205333333333334,
        "step": 4654
    },
    {
        "loss": 2.4009,
        "grad_norm": 1.97477388381958,
        "learning_rate": 2.636523649517042e-06,
        "epoch": 0.6206666666666667,
        "step": 4655
    },
    {
        "loss": 1.556,
        "grad_norm": 1.6519873142242432,
        "learning_rate": 2.6221862698401658e-06,
        "epoch": 0.6208,
        "step": 4656
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.9063923358917236,
        "learning_rate": 2.607887461847125e-06,
        "epoch": 0.6209333333333333,
        "step": 4657
    },
    {
        "loss": 3.5653,
        "grad_norm": 2.479649066925049,
        "learning_rate": 2.593627231201712e-06,
        "epoch": 0.6210666666666667,
        "step": 4658
    },
    {
        "loss": 2.7067,
        "grad_norm": 4.162788391113281,
        "learning_rate": 2.579405583552452e-06,
        "epoch": 0.6212,
        "step": 4659
    },
    {
        "loss": 2.35,
        "grad_norm": 2.560380220413208,
        "learning_rate": 2.5652225245326177e-06,
        "epoch": 0.6213333333333333,
        "step": 4660
    },
    {
        "loss": 1.755,
        "grad_norm": 2.9884679317474365,
        "learning_rate": 2.551078059760126e-06,
        "epoch": 0.6214666666666666,
        "step": 4661
    },
    {
        "loss": 3.3407,
        "grad_norm": 4.263473987579346,
        "learning_rate": 2.53697219483765e-06,
        "epoch": 0.6216,
        "step": 4662
    },
    {
        "loss": 2.2135,
        "grad_norm": 4.685197353363037,
        "learning_rate": 2.522904935352599e-06,
        "epoch": 0.6217333333333334,
        "step": 4663
    },
    {
        "loss": 2.4193,
        "grad_norm": 2.9584922790527344,
        "learning_rate": 2.508876286877049e-06,
        "epoch": 0.6218666666666667,
        "step": 4664
    },
    {
        "loss": 2.4974,
        "grad_norm": 3.5898215770721436,
        "learning_rate": 2.4948862549677766e-06,
        "epoch": 0.622,
        "step": 4665
    },
    {
        "loss": 3.159,
        "grad_norm": 2.6363680362701416,
        "learning_rate": 2.480934845166305e-06,
        "epoch": 0.6221333333333333,
        "step": 4666
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.8503732681274414,
        "learning_rate": 2.467022062998858e-06,
        "epoch": 0.6222666666666666,
        "step": 4667
    },
    {
        "loss": 3.0822,
        "grad_norm": 2.954983949661255,
        "learning_rate": 2.4531479139762837e-06,
        "epoch": 0.6224,
        "step": 4668
    },
    {
        "loss": 3.274,
        "grad_norm": 3.4890353679656982,
        "learning_rate": 2.439312403594207e-06,
        "epoch": 0.6225333333333334,
        "step": 4669
    },
    {
        "loss": 0.9501,
        "grad_norm": 3.096679925918579,
        "learning_rate": 2.4255155373329117e-06,
        "epoch": 0.6226666666666667,
        "step": 4670
    },
    {
        "loss": 2.213,
        "grad_norm": 3.432187557220459,
        "learning_rate": 2.411757320657393e-06,
        "epoch": 0.6228,
        "step": 4671
    },
    {
        "loss": 2.7916,
        "grad_norm": 2.559743881225586,
        "learning_rate": 2.3980377590173244e-06,
        "epoch": 0.6229333333333333,
        "step": 4672
    },
    {
        "loss": 1.1874,
        "grad_norm": 3.714495897293091,
        "learning_rate": 2.38435685784707e-06,
        "epoch": 0.6230666666666667,
        "step": 4673
    },
    {
        "loss": 1.6503,
        "grad_norm": 2.568540096282959,
        "learning_rate": 2.370714622565684e-06,
        "epoch": 0.6232,
        "step": 4674
    },
    {
        "loss": 2.8734,
        "grad_norm": 2.8254287242889404,
        "learning_rate": 2.3571110585768997e-06,
        "epoch": 0.6233333333333333,
        "step": 4675
    },
    {
        "loss": 1.4564,
        "grad_norm": 3.22792387008667,
        "learning_rate": 2.3435461712691286e-06,
        "epoch": 0.6234666666666666,
        "step": 4676
    },
    {
        "loss": 2.6032,
        "grad_norm": 1.591093897819519,
        "learning_rate": 2.330019966015484e-06,
        "epoch": 0.6236,
        "step": 4677
    },
    {
        "loss": 2.8692,
        "grad_norm": 2.0809953212738037,
        "learning_rate": 2.316532448173725e-06,
        "epoch": 0.6237333333333334,
        "step": 4678
    },
    {
        "loss": 2.7146,
        "grad_norm": 3.5320138931274414,
        "learning_rate": 2.3030836230863216e-06,
        "epoch": 0.6238666666666667,
        "step": 4679
    },
    {
        "loss": 2.621,
        "grad_norm": 2.6227385997772217,
        "learning_rate": 2.28967349608038e-06,
        "epoch": 0.624,
        "step": 4680
    },
    {
        "loss": 2.4402,
        "grad_norm": 2.554286003112793,
        "learning_rate": 2.276302072467695e-06,
        "epoch": 0.6241333333333333,
        "step": 4681
    },
    {
        "loss": 1.4245,
        "grad_norm": 2.5276854038238525,
        "learning_rate": 2.262969357544764e-06,
        "epoch": 0.6242666666666666,
        "step": 4682
    },
    {
        "loss": 2.083,
        "grad_norm": 1.8027762174606323,
        "learning_rate": 2.2496753565926954e-06,
        "epoch": 0.6244,
        "step": 4683
    },
    {
        "loss": 2.0888,
        "grad_norm": 2.302525520324707,
        "learning_rate": 2.2364200748772878e-06,
        "epoch": 0.6245333333333334,
        "step": 4684
    },
    {
        "loss": 1.4049,
        "grad_norm": 2.7389638423919678,
        "learning_rate": 2.2232035176490196e-06,
        "epoch": 0.6246666666666667,
        "step": 4685
    },
    {
        "loss": 2.5765,
        "grad_norm": 2.4559249877929688,
        "learning_rate": 2.2100256901430137e-06,
        "epoch": 0.6248,
        "step": 4686
    },
    {
        "loss": 1.8916,
        "grad_norm": 2.858433246612549,
        "learning_rate": 2.1968865975790174e-06,
        "epoch": 0.6249333333333333,
        "step": 4687
    },
    {
        "loss": 2.2001,
        "grad_norm": 1.8375179767608643,
        "learning_rate": 2.183786245161512e-06,
        "epoch": 0.6250666666666667,
        "step": 4688
    },
    {
        "loss": 3.2259,
        "grad_norm": 1.9207892417907715,
        "learning_rate": 2.1707246380795686e-06,
        "epoch": 0.6252,
        "step": 4689
    },
    {
        "loss": 2.7202,
        "grad_norm": 2.2934465408325195,
        "learning_rate": 2.157701781506938e-06,
        "epoch": 0.6253333333333333,
        "step": 4690
    },
    {
        "loss": 2.4346,
        "grad_norm": 2.462925672531128,
        "learning_rate": 2.1447176806020263e-06,
        "epoch": 0.6254666666666666,
        "step": 4691
    },
    {
        "loss": 1.6413,
        "grad_norm": 3.0392115116119385,
        "learning_rate": 2.131772340507887e-06,
        "epoch": 0.6256,
        "step": 4692
    },
    {
        "loss": 2.5149,
        "grad_norm": 3.898639440536499,
        "learning_rate": 2.118865766352196e-06,
        "epoch": 0.6257333333333334,
        "step": 4693
    },
    {
        "loss": 1.0752,
        "grad_norm": 5.146416664123535,
        "learning_rate": 2.105997963247297e-06,
        "epoch": 0.6258666666666667,
        "step": 4694
    },
    {
        "loss": 2.6723,
        "grad_norm": 2.4769904613494873,
        "learning_rate": 2.0931689362901576e-06,
        "epoch": 0.626,
        "step": 4695
    },
    {
        "loss": 2.016,
        "grad_norm": 3.157083749771118,
        "learning_rate": 2.080378690562412e-06,
        "epoch": 0.6261333333333333,
        "step": 4696
    },
    {
        "loss": 1.9404,
        "grad_norm": 4.1359405517578125,
        "learning_rate": 2.0676272311303314e-06,
        "epoch": 0.6262666666666666,
        "step": 4697
    },
    {
        "loss": 1.8044,
        "grad_norm": 2.970304012298584,
        "learning_rate": 2.054914563044796e-06,
        "epoch": 0.6264,
        "step": 4698
    },
    {
        "loss": 2.8842,
        "grad_norm": 2.8327670097351074,
        "learning_rate": 2.042240691341335e-06,
        "epoch": 0.6265333333333334,
        "step": 4699
    },
    {
        "loss": 2.5225,
        "grad_norm": 3.6739614009857178,
        "learning_rate": 2.0296056210401315e-06,
        "epoch": 0.6266666666666667,
        "step": 4700
    },
    {
        "loss": 2.3329,
        "grad_norm": 2.9159066677093506,
        "learning_rate": 2.01700935714596e-06,
        "epoch": 0.6268,
        "step": 4701
    },
    {
        "loss": 2.9189,
        "grad_norm": 4.0683770179748535,
        "learning_rate": 2.0044519046482636e-06,
        "epoch": 0.6269333333333333,
        "step": 4702
    },
    {
        "loss": 2.29,
        "grad_norm": 2.208810567855835,
        "learning_rate": 1.9919332685210624e-06,
        "epoch": 0.6270666666666667,
        "step": 4703
    },
    {
        "loss": 3.0219,
        "grad_norm": 2.382643461227417,
        "learning_rate": 1.979453453723068e-06,
        "epoch": 0.6272,
        "step": 4704
    },
    {
        "loss": 2.4163,
        "grad_norm": 2.629591941833496,
        "learning_rate": 1.9670124651975376e-06,
        "epoch": 0.6273333333333333,
        "step": 4705
    },
    {
        "loss": 1.4134,
        "grad_norm": 4.324747085571289,
        "learning_rate": 1.9546103078724064e-06,
        "epoch": 0.6274666666666666,
        "step": 4706
    },
    {
        "loss": 2.6876,
        "grad_norm": 3.2289745807647705,
        "learning_rate": 1.9422469866602234e-06,
        "epoch": 0.6276,
        "step": 4707
    },
    {
        "loss": 0.8818,
        "grad_norm": 3.5901970863342285,
        "learning_rate": 1.9299225064581263e-06,
        "epoch": 0.6277333333333334,
        "step": 4708
    },
    {
        "loss": 0.7157,
        "grad_norm": 2.799544334411621,
        "learning_rate": 1.9176368721478987e-06,
        "epoch": 0.6278666666666667,
        "step": 4709
    },
    {
        "loss": 2.5284,
        "grad_norm": 2.286309003829956,
        "learning_rate": 1.9053900885959153e-06,
        "epoch": 0.628,
        "step": 4710
    },
    {
        "loss": 2.9232,
        "grad_norm": 3.3359763622283936,
        "learning_rate": 1.893182160653173e-06,
        "epoch": 0.6281333333333333,
        "step": 4711
    },
    {
        "loss": 2.4089,
        "grad_norm": 2.7254884243011475,
        "learning_rate": 1.8810130931552483e-06,
        "epoch": 0.6282666666666666,
        "step": 4712
    },
    {
        "loss": 1.5375,
        "grad_norm": 3.096912145614624,
        "learning_rate": 1.8688828909223744e-06,
        "epoch": 0.6284,
        "step": 4713
    },
    {
        "loss": 1.2207,
        "grad_norm": 4.826245307922363,
        "learning_rate": 1.8567915587593632e-06,
        "epoch": 0.6285333333333334,
        "step": 4714
    },
    {
        "loss": 2.4419,
        "grad_norm": 3.2756285667419434,
        "learning_rate": 1.8447391014556393e-06,
        "epoch": 0.6286666666666667,
        "step": 4715
    },
    {
        "loss": 1.4442,
        "grad_norm": 3.7785837650299072,
        "learning_rate": 1.8327255237851949e-06,
        "epoch": 0.6288,
        "step": 4716
    },
    {
        "loss": 1.8277,
        "grad_norm": 3.1792147159576416,
        "learning_rate": 1.82075083050669e-06,
        "epoch": 0.6289333333333333,
        "step": 4717
    },
    {
        "loss": 1.2503,
        "grad_norm": 3.8162925243377686,
        "learning_rate": 1.8088150263633086e-06,
        "epoch": 0.6290666666666667,
        "step": 4718
    },
    {
        "loss": 3.4503,
        "grad_norm": 2.8512160778045654,
        "learning_rate": 1.7969181160828686e-06,
        "epoch": 0.6292,
        "step": 4719
    },
    {
        "loss": 1.2386,
        "grad_norm": 3.968517303466797,
        "learning_rate": 1.7850601043777892e-06,
        "epoch": 0.6293333333333333,
        "step": 4720
    },
    {
        "loss": 2.5404,
        "grad_norm": 3.0487256050109863,
        "learning_rate": 1.773240995945047e-06,
        "epoch": 0.6294666666666666,
        "step": 4721
    },
    {
        "loss": 2.5639,
        "grad_norm": 4.860729694366455,
        "learning_rate": 1.761460795466252e-06,
        "epoch": 0.6296,
        "step": 4722
    },
    {
        "loss": 1.9405,
        "grad_norm": 3.950932741165161,
        "learning_rate": 1.7497195076075946e-06,
        "epoch": 0.6297333333333334,
        "step": 4723
    },
    {
        "loss": 1.3077,
        "grad_norm": 4.515059471130371,
        "learning_rate": 1.7380171370197985e-06,
        "epoch": 0.6298666666666667,
        "step": 4724
    },
    {
        "loss": 2.1395,
        "grad_norm": 2.713498830795288,
        "learning_rate": 1.7263536883382446e-06,
        "epoch": 0.63,
        "step": 4725
    },
    {
        "loss": 3.0588,
        "grad_norm": 2.5478780269622803,
        "learning_rate": 1.714729166182849e-06,
        "epoch": 0.6301333333333333,
        "step": 4726
    },
    {
        "loss": 1.2063,
        "grad_norm": 3.570312023162842,
        "learning_rate": 1.7031435751581282e-06,
        "epoch": 0.6302666666666666,
        "step": 4727
    },
    {
        "loss": 2.3809,
        "grad_norm": 2.6473169326782227,
        "learning_rate": 1.6915969198531777e-06,
        "epoch": 0.6304,
        "step": 4728
    },
    {
        "loss": 2.271,
        "grad_norm": 1.9753538370132446,
        "learning_rate": 1.6800892048416617e-06,
        "epoch": 0.6305333333333333,
        "step": 4729
    },
    {
        "loss": 2.9457,
        "grad_norm": 2.612433910369873,
        "learning_rate": 1.6686204346818336e-06,
        "epoch": 0.6306666666666667,
        "step": 4730
    },
    {
        "loss": 1.0072,
        "grad_norm": 3.3432881832122803,
        "learning_rate": 1.657190613916504e-06,
        "epoch": 0.6308,
        "step": 4731
    },
    {
        "loss": 2.325,
        "grad_norm": 3.3530256748199463,
        "learning_rate": 1.6457997470730623e-06,
        "epoch": 0.6309333333333333,
        "step": 4732
    },
    {
        "loss": 1.7086,
        "grad_norm": 3.1575634479522705,
        "learning_rate": 1.6344478386634776e-06,
        "epoch": 0.6310666666666667,
        "step": 4733
    },
    {
        "loss": 2.1897,
        "grad_norm": 3.9448578357696533,
        "learning_rate": 1.623134893184286e-06,
        "epoch": 0.6312,
        "step": 4734
    },
    {
        "loss": 3.0491,
        "grad_norm": 2.2401859760284424,
        "learning_rate": 1.6118609151165697e-06,
        "epoch": 0.6313333333333333,
        "step": 4735
    },
    {
        "loss": 1.578,
        "grad_norm": 2.0570521354675293,
        "learning_rate": 1.6006259089260011e-06,
        "epoch": 0.6314666666666666,
        "step": 4736
    },
    {
        "loss": 2.7734,
        "grad_norm": 2.4683830738067627,
        "learning_rate": 1.589429879062776e-06,
        "epoch": 0.6316,
        "step": 4737
    },
    {
        "loss": 3.0261,
        "grad_norm": 3.565953016281128,
        "learning_rate": 1.5782728299617244e-06,
        "epoch": 0.6317333333333334,
        "step": 4738
    },
    {
        "loss": 2.8038,
        "grad_norm": 4.413910388946533,
        "learning_rate": 1.567154766042156e-06,
        "epoch": 0.6318666666666667,
        "step": 4739
    },
    {
        "loss": 2.3669,
        "grad_norm": 2.9107367992401123,
        "learning_rate": 1.556075691707992e-06,
        "epoch": 0.632,
        "step": 4740
    },
    {
        "loss": 2.1666,
        "grad_norm": 2.8550875186920166,
        "learning_rate": 1.5450356113476671e-06,
        "epoch": 0.6321333333333333,
        "step": 4741
    },
    {
        "loss": 1.9602,
        "grad_norm": 3.4968044757843018,
        "learning_rate": 1.5340345293342494e-06,
        "epoch": 0.6322666666666666,
        "step": 4742
    },
    {
        "loss": 2.5377,
        "grad_norm": 3.186187505722046,
        "learning_rate": 1.5230724500252425e-06,
        "epoch": 0.6324,
        "step": 4743
    },
    {
        "loss": 2.6866,
        "grad_norm": 2.3748128414154053,
        "learning_rate": 1.5121493777628059e-06,
        "epoch": 0.6325333333333333,
        "step": 4744
    },
    {
        "loss": 1.9193,
        "grad_norm": 2.4012513160705566,
        "learning_rate": 1.501265316873579e-06,
        "epoch": 0.6326666666666667,
        "step": 4745
    },
    {
        "loss": 2.5824,
        "grad_norm": 2.947197914123535,
        "learning_rate": 1.4904202716687798e-06,
        "epoch": 0.6328,
        "step": 4746
    },
    {
        "loss": 3.0596,
        "grad_norm": 2.308542013168335,
        "learning_rate": 1.479614246444183e-06,
        "epoch": 0.6329333333333333,
        "step": 4747
    },
    {
        "loss": 1.9303,
        "grad_norm": 3.9758925437927246,
        "learning_rate": 1.468847245480076e-06,
        "epoch": 0.6330666666666667,
        "step": 4748
    },
    {
        "loss": 2.9508,
        "grad_norm": 2.8427836894989014,
        "learning_rate": 1.458119273041325e-06,
        "epoch": 0.6332,
        "step": 4749
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.2202115058898926,
        "learning_rate": 1.4474303333772976e-06,
        "epoch": 0.6333333333333333,
        "step": 4750
    },
    {
        "loss": 2.487,
        "grad_norm": 2.6548995971679688,
        "learning_rate": 1.436780430721929e-06,
        "epoch": 0.6334666666666666,
        "step": 4751
    },
    {
        "loss": 1.9298,
        "grad_norm": 3.5142581462860107,
        "learning_rate": 1.4261695692936672e-06,
        "epoch": 0.6336,
        "step": 4752
    },
    {
        "loss": 1.8304,
        "grad_norm": 2.1660327911376953,
        "learning_rate": 1.4155977532955277e-06,
        "epoch": 0.6337333333333334,
        "step": 4753
    },
    {
        "loss": 2.8275,
        "grad_norm": 2.8657009601593018,
        "learning_rate": 1.4050649869150278e-06,
        "epoch": 0.6338666666666667,
        "step": 4754
    },
    {
        "loss": 2.7557,
        "grad_norm": 2.354262113571167,
        "learning_rate": 1.3945712743242522e-06,
        "epoch": 0.634,
        "step": 4755
    },
    {
        "loss": 1.0661,
        "grad_norm": 3.5001187324523926,
        "learning_rate": 1.3841166196797872e-06,
        "epoch": 0.6341333333333333,
        "step": 4756
    },
    {
        "loss": 1.8636,
        "grad_norm": 3.219970226287842,
        "learning_rate": 1.3737010271227424e-06,
        "epoch": 0.6342666666666666,
        "step": 4757
    },
    {
        "loss": 1.1841,
        "grad_norm": 8.280014038085938,
        "learning_rate": 1.3633245007787842e-06,
        "epoch": 0.6344,
        "step": 4758
    },
    {
        "loss": 2.6306,
        "grad_norm": 2.419445037841797,
        "learning_rate": 1.3529870447580918e-06,
        "epoch": 0.6345333333333333,
        "step": 4759
    },
    {
        "loss": 2.6032,
        "grad_norm": 3.594069719314575,
        "learning_rate": 1.342688663155356e-06,
        "epoch": 0.6346666666666667,
        "step": 4760
    },
    {
        "loss": 2.5478,
        "grad_norm": 2.5669124126434326,
        "learning_rate": 1.332429360049825e-06,
        "epoch": 0.6348,
        "step": 4761
    },
    {
        "loss": 2.8689,
        "grad_norm": 2.4954912662506104,
        "learning_rate": 1.322209139505204e-06,
        "epoch": 0.6349333333333333,
        "step": 4762
    },
    {
        "loss": 2.013,
        "grad_norm": 4.274824142456055,
        "learning_rate": 1.3120280055697876e-06,
        "epoch": 0.6350666666666667,
        "step": 4763
    },
    {
        "loss": 2.5589,
        "grad_norm": 3.402498722076416,
        "learning_rate": 1.30188596227635e-06,
        "epoch": 0.6352,
        "step": 4764
    },
    {
        "loss": 2.6988,
        "grad_norm": 4.617854595184326,
        "learning_rate": 1.2917830136421894e-06,
        "epoch": 0.6353333333333333,
        "step": 4765
    },
    {
        "loss": 3.0755,
        "grad_norm": 3.4525344371795654,
        "learning_rate": 1.281719163669104e-06,
        "epoch": 0.6354666666666666,
        "step": 4766
    },
    {
        "loss": 2.339,
        "grad_norm": 2.0124666690826416,
        "learning_rate": 1.2716944163434497e-06,
        "epoch": 0.6356,
        "step": 4767
    },
    {
        "loss": 1.5274,
        "grad_norm": 4.320809841156006,
        "learning_rate": 1.2617087756360503e-06,
        "epoch": 0.6357333333333334,
        "step": 4768
    },
    {
        "loss": 2.6519,
        "grad_norm": 3.293722629547119,
        "learning_rate": 1.2517622455022304e-06,
        "epoch": 0.6358666666666667,
        "step": 4769
    },
    {
        "loss": 2.1078,
        "grad_norm": 3.5570030212402344,
        "learning_rate": 1.2418548298818721e-06,
        "epoch": 0.636,
        "step": 4770
    },
    {
        "loss": 3.0772,
        "grad_norm": 2.250900983810425,
        "learning_rate": 1.231986532699314e-06,
        "epoch": 0.6361333333333333,
        "step": 4771
    },
    {
        "loss": 0.624,
        "grad_norm": 2.226942300796509,
        "learning_rate": 1.2221573578634405e-06,
        "epoch": 0.6362666666666666,
        "step": 4772
    },
    {
        "loss": 2.4929,
        "grad_norm": 3.345675230026245,
        "learning_rate": 1.2123673092676147e-06,
        "epoch": 0.6364,
        "step": 4773
    },
    {
        "loss": 2.1,
        "grad_norm": 2.5155556201934814,
        "learning_rate": 1.2026163907897126e-06,
        "epoch": 0.6365333333333333,
        "step": 4774
    },
    {
        "loss": 2.5949,
        "grad_norm": 6.361470699310303,
        "learning_rate": 1.1929046062921002e-06,
        "epoch": 0.6366666666666667,
        "step": 4775
    },
    {
        "loss": 2.6436,
        "grad_norm": 2.678250551223755,
        "learning_rate": 1.1832319596216556e-06,
        "epoch": 0.6368,
        "step": 4776
    },
    {
        "loss": 2.3088,
        "grad_norm": 2.9284796714782715,
        "learning_rate": 1.1735984546097256e-06,
        "epoch": 0.6369333333333334,
        "step": 4777
    },
    {
        "loss": 1.913,
        "grad_norm": 4.4953999519348145,
        "learning_rate": 1.164004095072202e-06,
        "epoch": 0.6370666666666667,
        "step": 4778
    },
    {
        "loss": 1.9187,
        "grad_norm": 4.929314136505127,
        "learning_rate": 1.1544488848094338e-06,
        "epoch": 0.6372,
        "step": 4779
    },
    {
        "loss": 1.7082,
        "grad_norm": 2.7980055809020996,
        "learning_rate": 1.144932827606271e-06,
        "epoch": 0.6373333333333333,
        "step": 4780
    },
    {
        "loss": 3.069,
        "grad_norm": 3.598193883895874,
        "learning_rate": 1.135455927232043e-06,
        "epoch": 0.6374666666666666,
        "step": 4781
    },
    {
        "loss": 1.8562,
        "grad_norm": 3.623020648956299,
        "learning_rate": 1.126018187440603e-06,
        "epoch": 0.6376,
        "step": 4782
    },
    {
        "loss": 2.1781,
        "grad_norm": 2.6613221168518066,
        "learning_rate": 1.1166196119702599e-06,
        "epoch": 0.6377333333333334,
        "step": 4783
    },
    {
        "loss": 2.6898,
        "grad_norm": 2.306835174560547,
        "learning_rate": 1.1072602045438253e-06,
        "epoch": 0.6378666666666667,
        "step": 4784
    },
    {
        "loss": 1.6701,
        "grad_norm": 4.324480056762695,
        "learning_rate": 1.0979399688686e-06,
        "epoch": 0.638,
        "step": 4785
    },
    {
        "loss": 2.8823,
        "grad_norm": 3.1883485317230225,
        "learning_rate": 1.0886589086363418e-06,
        "epoch": 0.6381333333333333,
        "step": 4786
    },
    {
        "loss": 0.7815,
        "grad_norm": 3.7754411697387695,
        "learning_rate": 1.079417027523344e-06,
        "epoch": 0.6382666666666666,
        "step": 4787
    },
    {
        "loss": 1.7745,
        "grad_norm": 2.6364171504974365,
        "learning_rate": 1.0702143291902999e-06,
        "epoch": 0.6384,
        "step": 4788
    },
    {
        "loss": 2.6813,
        "grad_norm": 2.314589738845825,
        "learning_rate": 1.0610508172824718e-06,
        "epoch": 0.6385333333333333,
        "step": 4789
    },
    {
        "loss": 1.9071,
        "grad_norm": 3.2886738777160645,
        "learning_rate": 1.0519264954295337e-06,
        "epoch": 0.6386666666666667,
        "step": 4790
    },
    {
        "loss": 2.2388,
        "grad_norm": 5.982436656951904,
        "learning_rate": 1.0428413672456616e-06,
        "epoch": 0.6388,
        "step": 4791
    },
    {
        "loss": 2.1767,
        "grad_norm": 4.096830368041992,
        "learning_rate": 1.033795436329521e-06,
        "epoch": 0.6389333333333334,
        "step": 4792
    },
    {
        "loss": 2.6775,
        "grad_norm": 3.5522139072418213,
        "learning_rate": 1.0247887062642347e-06,
        "epoch": 0.6390666666666667,
        "step": 4793
    },
    {
        "loss": 2.2781,
        "grad_norm": 3.4612932205200195,
        "learning_rate": 1.0158211806173822e-06,
        "epoch": 0.6392,
        "step": 4794
    },
    {
        "loss": 2.9532,
        "grad_norm": 2.0015804767608643,
        "learning_rate": 1.0068928629410445e-06,
        "epoch": 0.6393333333333333,
        "step": 4795
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.700384497642517,
        "learning_rate": 9.980037567717481e-07,
        "epoch": 0.6394666666666666,
        "step": 4796
    },
    {
        "loss": 2.7456,
        "grad_norm": 2.1340231895446777,
        "learning_rate": 9.8915386563051e-07,
        "epoch": 0.6396,
        "step": 4797
    },
    {
        "loss": 2.7524,
        "grad_norm": 1.859405279159546,
        "learning_rate": 9.803431930227924e-07,
        "epoch": 0.6397333333333334,
        "step": 4798
    },
    {
        "loss": 2.5693,
        "grad_norm": 2.56628155708313,
        "learning_rate": 9.715717424385373e-07,
        "epoch": 0.6398666666666667,
        "step": 4799
    },
    {
        "loss": 2.7735,
        "grad_norm": 4.5439910888671875,
        "learning_rate": 9.628395173521432e-07,
        "epoch": 0.64,
        "step": 4800
    },
    {
        "loss": 2.0347,
        "grad_norm": 3.6330456733703613,
        "learning_rate": 9.541465212224876e-07,
        "epoch": 0.6401333333333333,
        "step": 4801
    },
    {
        "loss": 2.0807,
        "grad_norm": 3.3243634700775146,
        "learning_rate": 9.454927574928829e-07,
        "epoch": 0.6402666666666667,
        "step": 4802
    },
    {
        "loss": 1.9112,
        "grad_norm": 2.305621862411499,
        "learning_rate": 9.368782295911093e-07,
        "epoch": 0.6404,
        "step": 4803
    },
    {
        "loss": 1.4637,
        "grad_norm": 2.8676414489746094,
        "learning_rate": 9.283029409294264e-07,
        "epoch": 0.6405333333333333,
        "step": 4804
    },
    {
        "loss": 2.1256,
        "grad_norm": 4.359527111053467,
        "learning_rate": 9.197668949045279e-07,
        "epoch": 0.6406666666666667,
        "step": 4805
    },
    {
        "loss": 3.0204,
        "grad_norm": 2.9630892276763916,
        "learning_rate": 9.11270094897565e-07,
        "epoch": 0.6408,
        "step": 4806
    },
    {
        "loss": 2.4577,
        "grad_norm": 2.337428331375122,
        "learning_rate": 9.028125442741453e-07,
        "epoch": 0.6409333333333334,
        "step": 4807
    },
    {
        "loss": 0.805,
        "grad_norm": 3.5031654834747314,
        "learning_rate": 8.943942463843558e-07,
        "epoch": 0.6410666666666667,
        "step": 4808
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.5412535667419434,
        "learning_rate": 8.860152045626957e-07,
        "epoch": 0.6412,
        "step": 4809
    },
    {
        "loss": 2.6971,
        "grad_norm": 3.1292295455932617,
        "learning_rate": 8.776754221281324e-07,
        "epoch": 0.6413333333333333,
        "step": 4810
    },
    {
        "loss": 2.929,
        "grad_norm": 2.1246259212493896,
        "learning_rate": 8.693749023840903e-07,
        "epoch": 0.6414666666666666,
        "step": 4811
    },
    {
        "loss": 2.2655,
        "grad_norm": 3.449836015701294,
        "learning_rate": 8.61113648618439e-07,
        "epoch": 0.6416,
        "step": 4812
    },
    {
        "loss": 2.0593,
        "grad_norm": 3.525817394256592,
        "learning_rate": 8.528916641034501e-07,
        "epoch": 0.6417333333333334,
        "step": 4813
    },
    {
        "loss": 2.1145,
        "grad_norm": 3.26800537109375,
        "learning_rate": 8.447089520959295e-07,
        "epoch": 0.6418666666666667,
        "step": 4814
    },
    {
        "loss": 1.8771,
        "grad_norm": 2.0626003742218018,
        "learning_rate": 8.365655158370511e-07,
        "epoch": 0.642,
        "step": 4815
    },
    {
        "loss": 0.7667,
        "grad_norm": 2.742262601852417,
        "learning_rate": 8.284613585524681e-07,
        "epoch": 0.6421333333333333,
        "step": 4816
    },
    {
        "loss": 2.1314,
        "grad_norm": 4.352752685546875,
        "learning_rate": 8.20396483452246e-07,
        "epoch": 0.6422666666666667,
        "step": 4817
    },
    {
        "loss": 1.7562,
        "grad_norm": 6.106717109680176,
        "learning_rate": 8.123708937309404e-07,
        "epoch": 0.6424,
        "step": 4818
    },
    {
        "loss": 1.6648,
        "grad_norm": 5.1653852462768555,
        "learning_rate": 8.043845925674976e-07,
        "epoch": 0.6425333333333333,
        "step": 4819
    },
    {
        "loss": 1.6625,
        "grad_norm": 4.190820693969727,
        "learning_rate": 7.964375831253091e-07,
        "epoch": 0.6426666666666667,
        "step": 4820
    },
    {
        "loss": 2.03,
        "grad_norm": 2.4419071674346924,
        "learning_rate": 7.885298685522235e-07,
        "epoch": 0.6428,
        "step": 4821
    },
    {
        "loss": 2.0965,
        "grad_norm": 2.1000571250915527,
        "learning_rate": 7.806614519805022e-07,
        "epoch": 0.6429333333333334,
        "step": 4822
    },
    {
        "loss": 1.6569,
        "grad_norm": 3.481548547744751,
        "learning_rate": 7.728323365268741e-07,
        "epoch": 0.6430666666666667,
        "step": 4823
    },
    {
        "loss": 2.2633,
        "grad_norm": 3.410029411315918,
        "learning_rate": 7.650425252924586e-07,
        "epoch": 0.6432,
        "step": 4824
    },
    {
        "loss": 2.6335,
        "grad_norm": 2.1395018100738525,
        "learning_rate": 7.572920213628432e-07,
        "epoch": 0.6433333333333333,
        "step": 4825
    },
    {
        "loss": 1.0607,
        "grad_norm": 3.380295753479004,
        "learning_rate": 7.495808278080052e-07,
        "epoch": 0.6434666666666666,
        "step": 4826
    },
    {
        "loss": 2.1695,
        "grad_norm": 4.080939769744873,
        "learning_rate": 7.419089476824015e-07,
        "epoch": 0.6436,
        "step": 4827
    },
    {
        "loss": 2.7005,
        "grad_norm": 2.3051488399505615,
        "learning_rate": 7.342763840248678e-07,
        "epoch": 0.6437333333333334,
        "step": 4828
    },
    {
        "loss": 2.7454,
        "grad_norm": 3.017636299133301,
        "learning_rate": 7.266831398587081e-07,
        "epoch": 0.6438666666666667,
        "step": 4829
    },
    {
        "loss": 0.8801,
        "grad_norm": 3.5670034885406494,
        "learning_rate": 7.191292181916165e-07,
        "epoch": 0.644,
        "step": 4830
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.387852430343628,
        "learning_rate": 7.116146220157438e-07,
        "epoch": 0.6441333333333333,
        "step": 4831
    },
    {
        "loss": 2.0524,
        "grad_norm": 5.040825843811035,
        "learning_rate": 7.041393543076202e-07,
        "epoch": 0.6442666666666667,
        "step": 4832
    },
    {
        "loss": 1.6413,
        "grad_norm": 4.0481743812561035,
        "learning_rate": 6.967034180282439e-07,
        "epoch": 0.6444,
        "step": 4833
    },
    {
        "loss": 2.5914,
        "grad_norm": 3.2449228763580322,
        "learning_rate": 6.893068161230142e-07,
        "epoch": 0.6445333333333333,
        "step": 4834
    },
    {
        "loss": 1.9017,
        "grad_norm": 3.6057498455047607,
        "learning_rate": 6.819495515217433e-07,
        "epoch": 0.6446666666666667,
        "step": 4835
    },
    {
        "loss": 2.8314,
        "grad_norm": 3.215325117111206,
        "learning_rate": 6.746316271386776e-07,
        "epoch": 0.6448,
        "step": 4836
    },
    {
        "loss": 2.6056,
        "grad_norm": 2.436148166656494,
        "learning_rate": 6.673530458724764e-07,
        "epoch": 0.6449333333333334,
        "step": 4837
    },
    {
        "loss": 2.268,
        "grad_norm": 3.553950548171997,
        "learning_rate": 6.601138106061888e-07,
        "epoch": 0.6450666666666667,
        "step": 4838
    },
    {
        "loss": 1.8346,
        "grad_norm": 3.0957157611846924,
        "learning_rate": 6.529139242073101e-07,
        "epoch": 0.6452,
        "step": 4839
    },
    {
        "loss": 2.0181,
        "grad_norm": 3.3848178386688232,
        "learning_rate": 6.457533895277479e-07,
        "epoch": 0.6453333333333333,
        "step": 4840
    },
    {
        "loss": 3.1674,
        "grad_norm": 3.311771869659424,
        "learning_rate": 6.386322094038222e-07,
        "epoch": 0.6454666666666666,
        "step": 4841
    },
    {
        "loss": 2.3583,
        "grad_norm": 3.3003382682800293,
        "learning_rate": 6.315503866562211e-07,
        "epoch": 0.6456,
        "step": 4842
    },
    {
        "loss": 2.0696,
        "grad_norm": 3.684699058532715,
        "learning_rate": 6.24507924090123e-07,
        "epoch": 0.6457333333333334,
        "step": 4843
    },
    {
        "loss": 2.9884,
        "grad_norm": 2.3419339656829834,
        "learning_rate": 6.175048244950299e-07,
        "epoch": 0.6458666666666667,
        "step": 4844
    },
    {
        "loss": 2.4848,
        "grad_norm": 1.8521491289138794,
        "learning_rate": 6.105410906449338e-07,
        "epoch": 0.646,
        "step": 4845
    },
    {
        "loss": 2.3223,
        "grad_norm": 3.2663681507110596,
        "learning_rate": 6.036167252981617e-07,
        "epoch": 0.6461333333333333,
        "step": 4846
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.7597230672836304,
        "learning_rate": 5.967317311974863e-07,
        "epoch": 0.6462666666666667,
        "step": 4847
    },
    {
        "loss": 2.537,
        "grad_norm": 2.6041347980499268,
        "learning_rate": 5.898861110700815e-07,
        "epoch": 0.6464,
        "step": 4848
    },
    {
        "loss": 1.7223,
        "grad_norm": 3.0825319290161133,
        "learning_rate": 5.830798676275229e-07,
        "epoch": 0.6465333333333333,
        "step": 4849
    },
    {
        "loss": 2.2979,
        "grad_norm": 2.832388162612915,
        "learning_rate": 5.76313003565776e-07,
        "epoch": 0.6466666666666666,
        "step": 4850
    },
    {
        "loss": 2.1433,
        "grad_norm": 3.233809232711792,
        "learning_rate": 5.695855215652413e-07,
        "epoch": 0.6468,
        "step": 4851
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.2050623893737793,
        "learning_rate": 5.628974242906759e-07,
        "epoch": 0.6469333333333334,
        "step": 4852
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.513392925262451,
        "learning_rate": 5.562487143912609e-07,
        "epoch": 0.6470666666666667,
        "step": 4853
    },
    {
        "loss": 1.4505,
        "grad_norm": 2.6866965293884277,
        "learning_rate": 5.496393945005784e-07,
        "epoch": 0.6472,
        "step": 4854
    },
    {
        "loss": 2.1011,
        "grad_norm": 3.5909717082977295,
        "learning_rate": 5.43069467236601e-07,
        "epoch": 0.6473333333333333,
        "step": 4855
    },
    {
        "loss": 2.1959,
        "grad_norm": 3.3375165462493896,
        "learning_rate": 5.365389352017025e-07,
        "epoch": 0.6474666666666666,
        "step": 4856
    },
    {
        "loss": 0.8989,
        "grad_norm": 2.491461753845215,
        "learning_rate": 5.300478009826359e-07,
        "epoch": 0.6476,
        "step": 4857
    },
    {
        "loss": 2.842,
        "grad_norm": 2.7828497886657715,
        "learning_rate": 5.23596067150578e-07,
        "epoch": 0.6477333333333334,
        "step": 4858
    },
    {
        "loss": 1.8981,
        "grad_norm": 4.685093402862549,
        "learning_rate": 5.171837362610621e-07,
        "epoch": 0.6478666666666667,
        "step": 4859
    },
    {
        "loss": 2.7539,
        "grad_norm": 3.0478193759918213,
        "learning_rate": 5.108108108540676e-07,
        "epoch": 0.648,
        "step": 4860
    },
    {
        "loss": 2.8653,
        "grad_norm": 1.981018304824829,
        "learning_rate": 5.044772934538977e-07,
        "epoch": 0.6481333333333333,
        "step": 4861
    },
    {
        "loss": 1.9735,
        "grad_norm": 1.8097361326217651,
        "learning_rate": 4.981831865693121e-07,
        "epoch": 0.6482666666666667,
        "step": 4862
    },
    {
        "loss": 2.5903,
        "grad_norm": 2.709977388381958,
        "learning_rate": 4.919284926933942e-07,
        "epoch": 0.6484,
        "step": 4863
    },
    {
        "loss": 2.2338,
        "grad_norm": 3.450340509414673,
        "learning_rate": 4.857132143036735e-07,
        "epoch": 0.6485333333333333,
        "step": 4864
    },
    {
        "loss": 1.7078,
        "grad_norm": 2.899561882019043,
        "learning_rate": 4.795373538620251e-07,
        "epoch": 0.6486666666666666,
        "step": 4865
    },
    {
        "loss": 2.3471,
        "grad_norm": 3.8174338340759277,
        "learning_rate": 4.734009138147477e-07,
        "epoch": 0.6488,
        "step": 4866
    },
    {
        "loss": 2.4095,
        "grad_norm": 2.180704116821289,
        "learning_rate": 4.6730389659248584e-07,
        "epoch": 0.6489333333333334,
        "step": 4867
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.8501694202423096,
        "learning_rate": 4.612463046103077e-07,
        "epoch": 0.6490666666666667,
        "step": 4868
    },
    {
        "loss": 2.6392,
        "grad_norm": 2.41861891746521,
        "learning_rate": 4.552281402676495e-07,
        "epoch": 0.6492,
        "step": 4869
    },
    {
        "loss": 2.8273,
        "grad_norm": 3.2504453659057617,
        "learning_rate": 4.4924940594830435e-07,
        "epoch": 0.6493333333333333,
        "step": 4870
    },
    {
        "loss": 1.6078,
        "grad_norm": 1.9141862392425537,
        "learning_rate": 4.433101040204779e-07,
        "epoch": 0.6494666666666666,
        "step": 4871
    },
    {
        "loss": 1.4882,
        "grad_norm": 3.815700054168701,
        "learning_rate": 4.3741023683675495e-07,
        "epoch": 0.6496,
        "step": 4872
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.8280787467956543,
        "learning_rate": 4.315498067340884e-07,
        "epoch": 0.6497333333333334,
        "step": 4873
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.7080001831054688,
        "learning_rate": 4.257288160338102e-07,
        "epoch": 0.6498666666666667,
        "step": 4874
    },
    {
        "loss": 2.762,
        "grad_norm": 2.6610758304595947,
        "learning_rate": 4.1994726704164266e-07,
        "epoch": 0.65,
        "step": 4875
    },
    {
        "loss": 1.8007,
        "grad_norm": 3.31294846534729,
        "learning_rate": 4.142051620476761e-07,
        "epoch": 0.6501333333333333,
        "step": 4876
    },
    {
        "loss": 0.7953,
        "grad_norm": 4.049205303192139,
        "learning_rate": 4.0850250332636895e-07,
        "epoch": 0.6502666666666667,
        "step": 4877
    },
    {
        "loss": 2.9701,
        "grad_norm": 3.276503086090088,
        "learning_rate": 4.028392931365699e-07,
        "epoch": 0.6504,
        "step": 4878
    },
    {
        "loss": 2.5423,
        "grad_norm": 2.186277389526367,
        "learning_rate": 3.9721553372150664e-07,
        "epoch": 0.6505333333333333,
        "step": 4879
    },
    {
        "loss": 2.4286,
        "grad_norm": 2.79874324798584,
        "learning_rate": 3.916312273087419e-07,
        "epoch": 0.6506666666666666,
        "step": 4880
    },
    {
        "loss": 2.1572,
        "grad_norm": 3.059265613555908,
        "learning_rate": 3.8608637611027286e-07,
        "epoch": 0.6508,
        "step": 4881
    },
    {
        "loss": 2.3607,
        "grad_norm": 5.147287368774414,
        "learning_rate": 3.8058098232239826e-07,
        "epoch": 0.6509333333333334,
        "step": 4882
    },
    {
        "loss": 2.2143,
        "grad_norm": 5.577723979949951,
        "learning_rate": 3.751150481258514e-07,
        "epoch": 0.6510666666666667,
        "step": 4883
    },
    {
        "loss": 2.5612,
        "grad_norm": 4.151181221008301,
        "learning_rate": 3.696885756857005e-07,
        "epoch": 0.6512,
        "step": 4884
    },
    {
        "loss": 2.258,
        "grad_norm": 4.366161346435547,
        "learning_rate": 3.6430156715138164e-07,
        "epoch": 0.6513333333333333,
        "step": 4885
    },
    {
        "loss": 2.2281,
        "grad_norm": 4.826387882232666,
        "learning_rate": 3.5895402465671026e-07,
        "epoch": 0.6514666666666666,
        "step": 4886
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.9607834815979004,
        "learning_rate": 3.536459503198697e-07,
        "epoch": 0.6516,
        "step": 4887
    },
    {
        "loss": 2.3849,
        "grad_norm": 2.1452038288116455,
        "learning_rate": 3.4837734624341144e-07,
        "epoch": 0.6517333333333334,
        "step": 4888
    },
    {
        "loss": 2.0222,
        "grad_norm": 3.4729690551757812,
        "learning_rate": 3.4314821451423283e-07,
        "epoch": 0.6518666666666667,
        "step": 4889
    },
    {
        "loss": 1.7748,
        "grad_norm": 3.0781009197235107,
        "learning_rate": 3.379585572036215e-07,
        "epoch": 0.652,
        "step": 4890
    },
    {
        "loss": 2.6501,
        "grad_norm": 2.8432745933532715,
        "learning_rate": 3.3280837636722183e-07,
        "epoch": 0.6521333333333333,
        "step": 4891
    },
    {
        "loss": 0.7411,
        "grad_norm": 3.293315887451172,
        "learning_rate": 3.276976740450244e-07,
        "epoch": 0.6522666666666667,
        "step": 4892
    },
    {
        "loss": 2.1995,
        "grad_norm": 3.985100269317627,
        "learning_rate": 3.226264522613987e-07,
        "epoch": 0.6524,
        "step": 4893
    },
    {
        "loss": 2.3863,
        "grad_norm": 3.2926185131073,
        "learning_rate": 3.175947130250934e-07,
        "epoch": 0.6525333333333333,
        "step": 4894
    },
    {
        "loss": 2.2091,
        "grad_norm": 3.0969228744506836,
        "learning_rate": 3.1260245832916976e-07,
        "epoch": 0.6526666666666666,
        "step": 4895
    },
    {
        "loss": 1.6237,
        "grad_norm": 4.018519878387451,
        "learning_rate": 3.076496901510906e-07,
        "epoch": 0.6528,
        "step": 4896
    },
    {
        "loss": 2.4465,
        "grad_norm": 3.2576866149902344,
        "learning_rate": 3.027364104526642e-07,
        "epoch": 0.6529333333333334,
        "step": 4897
    },
    {
        "loss": 1.8512,
        "grad_norm": 3.987783908843994,
        "learning_rate": 2.9786262118005615e-07,
        "epoch": 0.6530666666666667,
        "step": 4898
    },
    {
        "loss": 2.3462,
        "grad_norm": 3.926980495452881,
        "learning_rate": 2.930283242637888e-07,
        "epoch": 0.6532,
        "step": 4899
    },
    {
        "loss": 2.0151,
        "grad_norm": 3.828103542327881,
        "learning_rate": 2.882335216187526e-07,
        "epoch": 0.6533333333333333,
        "step": 4900
    },
    {
        "loss": 2.2298,
        "grad_norm": 2.160125494003296,
        "learning_rate": 2.834782151441617e-07,
        "epoch": 0.6534666666666666,
        "step": 4901
    },
    {
        "loss": 0.7374,
        "grad_norm": 2.332491397857666,
        "learning_rate": 2.7876240672364273e-07,
        "epoch": 0.6536,
        "step": 4902
    },
    {
        "loss": 2.6755,
        "grad_norm": 2.1270718574523926,
        "learning_rate": 2.7408609822511253e-07,
        "epoch": 0.6537333333333334,
        "step": 4903
    },
    {
        "loss": 2.3468,
        "grad_norm": 3.8210432529449463,
        "learning_rate": 2.694492915009006e-07,
        "epoch": 0.6538666666666667,
        "step": 4904
    },
    {
        "loss": 3.0285,
        "grad_norm": 3.579160451889038,
        "learning_rate": 2.648519883876377e-07,
        "epoch": 0.654,
        "step": 4905
    },
    {
        "loss": 2.3604,
        "grad_norm": 4.056262493133545,
        "learning_rate": 2.6029419070634497e-07,
        "epoch": 0.6541333333333333,
        "step": 4906
    },
    {
        "loss": 2.4658,
        "grad_norm": 3.380819082260132,
        "learning_rate": 2.557759002623783e-07,
        "epoch": 0.6542666666666667,
        "step": 4907
    },
    {
        "loss": 2.5536,
        "grad_norm": 2.728531837463379,
        "learning_rate": 2.5129711884543937e-07,
        "epoch": 0.6544,
        "step": 4908
    },
    {
        "loss": 1.4726,
        "grad_norm": 4.744922637939453,
        "learning_rate": 2.468578482296091e-07,
        "epoch": 0.6545333333333333,
        "step": 4909
    },
    {
        "loss": 1.9069,
        "grad_norm": 2.8771307468414307,
        "learning_rate": 2.424580901732698e-07,
        "epoch": 0.6546666666666666,
        "step": 4910
    },
    {
        "loss": 1.7149,
        "grad_norm": 3.692415237426758,
        "learning_rate": 2.380978464192163e-07,
        "epoch": 0.6548,
        "step": 4911
    },
    {
        "loss": 1.5694,
        "grad_norm": 4.149473667144775,
        "learning_rate": 2.3377711869452257e-07,
        "epoch": 0.6549333333333334,
        "step": 4912
    },
    {
        "loss": 1.3918,
        "grad_norm": 3.6574695110321045,
        "learning_rate": 2.2949590871066407e-07,
        "epoch": 0.6550666666666667,
        "step": 4913
    },
    {
        "loss": 1.6065,
        "grad_norm": 1.6026334762573242,
        "learning_rate": 2.2525421816342874e-07,
        "epoch": 0.6552,
        "step": 4914
    },
    {
        "loss": 2.1007,
        "grad_norm": 4.6111321449279785,
        "learning_rate": 2.210520487329837e-07,
        "epoch": 0.6553333333333333,
        "step": 4915
    },
    {
        "loss": 2.2501,
        "grad_norm": 2.6199207305908203,
        "learning_rate": 2.1688940208379748e-07,
        "epoch": 0.6554666666666666,
        "step": 4916
    },
    {
        "loss": 2.2828,
        "grad_norm": 2.509395122528076,
        "learning_rate": 2.1276627986472897e-07,
        "epoch": 0.6556,
        "step": 4917
    },
    {
        "loss": 0.5494,
        "grad_norm": 2.8208847045898438,
        "learning_rate": 2.086826837089495e-07,
        "epoch": 0.6557333333333333,
        "step": 4918
    },
    {
        "loss": 1.7387,
        "grad_norm": 5.107287883758545,
        "learning_rate": 2.0463861523399853e-07,
        "epoch": 0.6558666666666667,
        "step": 4919
    },
    {
        "loss": 1.773,
        "grad_norm": 1.5803802013397217,
        "learning_rate": 2.0063407604172803e-07,
        "epoch": 0.656,
        "step": 4920
    },
    {
        "loss": 2.4982,
        "grad_norm": 3.513103485107422,
        "learning_rate": 1.9666906771835803e-07,
        "epoch": 0.6561333333333333,
        "step": 4921
    },
    {
        "loss": 2.7177,
        "grad_norm": 1.883018136024475,
        "learning_rate": 1.9274359183444336e-07,
        "epoch": 0.6562666666666667,
        "step": 4922
    },
    {
        "loss": 2.9851,
        "grad_norm": 2.467740535736084,
        "learning_rate": 1.8885764994487354e-07,
        "epoch": 0.6564,
        "step": 4923
    },
    {
        "loss": 2.5881,
        "grad_norm": 2.3246138095855713,
        "learning_rate": 1.850112435888951e-07,
        "epoch": 0.6565333333333333,
        "step": 4924
    },
    {
        "loss": 2.5851,
        "grad_norm": 2.586744785308838,
        "learning_rate": 1.812043742900671e-07,
        "epoch": 0.6566666666666666,
        "step": 4925
    },
    {
        "loss": 2.5934,
        "grad_norm": 1.7971175909042358,
        "learning_rate": 1.7743704355631663e-07,
        "epoch": 0.6568,
        "step": 4926
    },
    {
        "loss": 1.3033,
        "grad_norm": 4.724555969238281,
        "learning_rate": 1.7370925287988337e-07,
        "epoch": 0.6569333333333334,
        "step": 4927
    },
    {
        "loss": 0.8709,
        "grad_norm": 3.0292136669158936,
        "learning_rate": 1.7002100373737505e-07,
        "epoch": 0.6570666666666667,
        "step": 4928
    },
    {
        "loss": 1.6517,
        "grad_norm": 4.430515289306641,
        "learning_rate": 1.6637229758970086e-07,
        "epoch": 0.6572,
        "step": 4929
    },
    {
        "loss": 2.4398,
        "grad_norm": 2.965208053588867,
        "learning_rate": 1.62763135882138e-07,
        "epoch": 0.6573333333333333,
        "step": 4930
    },
    {
        "loss": 1.9261,
        "grad_norm": 3.3861846923828125,
        "learning_rate": 1.5919352004427623e-07,
        "epoch": 0.6574666666666666,
        "step": 4931
    },
    {
        "loss": 2.37,
        "grad_norm": 3.6399097442626953,
        "learning_rate": 1.5566345149006235e-07,
        "epoch": 0.6576,
        "step": 4932
    },
    {
        "loss": 2.7262,
        "grad_norm": 3.9676382541656494,
        "learning_rate": 1.521729316177778e-07,
        "epoch": 0.6577333333333333,
        "step": 4933
    },
    {
        "loss": 2.2961,
        "grad_norm": 4.004085063934326,
        "learning_rate": 1.4872196181000553e-07,
        "epoch": 0.6578666666666667,
        "step": 4934
    },
    {
        "loss": 2.5144,
        "grad_norm": 2.5303938388824463,
        "learning_rate": 1.4531054343370764e-07,
        "epoch": 0.658,
        "step": 4935
    },
    {
        "loss": 2.4278,
        "grad_norm": 3.3396553993225098,
        "learning_rate": 1.4193867784014769e-07,
        "epoch": 0.6581333333333333,
        "step": 4936
    },
    {
        "loss": 2.3523,
        "grad_norm": 3.318849802017212,
        "learning_rate": 1.3860636636493506e-07,
        "epoch": 0.6582666666666667,
        "step": 4937
    },
    {
        "loss": 1.9577,
        "grad_norm": 2.27022123336792,
        "learning_rate": 1.3531361032801394e-07,
        "epoch": 0.6584,
        "step": 4938
    },
    {
        "loss": 1.7566,
        "grad_norm": 3.2870800495147705,
        "learning_rate": 1.32060411033641e-07,
        "epoch": 0.6585333333333333,
        "step": 4939
    },
    {
        "loss": 2.595,
        "grad_norm": 3.494150400161743,
        "learning_rate": 1.2884676977044097e-07,
        "epoch": 0.6586666666666666,
        "step": 4940
    },
    {
        "loss": 2.3903,
        "grad_norm": 3.120995044708252,
        "learning_rate": 1.2567268781132902e-07,
        "epoch": 0.6588,
        "step": 4941
    },
    {
        "loss": 0.983,
        "grad_norm": 4.234354496002197,
        "learning_rate": 1.225381664135883e-07,
        "epoch": 0.6589333333333334,
        "step": 4942
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.6965434551239014,
        "learning_rate": 1.1944320681879229e-07,
        "epoch": 0.6590666666666667,
        "step": 4943
    },
    {
        "loss": 1.6484,
        "grad_norm": 2.814915895462036,
        "learning_rate": 1.1638781025289369e-07,
        "epoch": 0.6592,
        "step": 4944
    },
    {
        "loss": 2.6663,
        "grad_norm": 1.5076093673706055,
        "learning_rate": 1.1337197792611332e-07,
        "epoch": 0.6593333333333333,
        "step": 4945
    },
    {
        "loss": 2.3387,
        "grad_norm": 3.242765426635742,
        "learning_rate": 1.1039571103306224e-07,
        "epoch": 0.6594666666666666,
        "step": 4946
    },
    {
        "loss": 2.1386,
        "grad_norm": 4.41837215423584,
        "learning_rate": 1.0745901075261966e-07,
        "epoch": 0.6596,
        "step": 4947
    },
    {
        "loss": 1.6379,
        "grad_norm": 3.326800584793091,
        "learning_rate": 1.0456187824805508e-07,
        "epoch": 0.6597333333333333,
        "step": 4948
    },
    {
        "loss": 2.4992,
        "grad_norm": 2.391362190246582,
        "learning_rate": 1.017043146669061e-07,
        "epoch": 0.6598666666666667,
        "step": 4949
    },
    {
        "loss": 2.5916,
        "grad_norm": 2.494050979614258,
        "learning_rate": 9.888632114106733e-08,
        "epoch": 0.66,
        "step": 4950
    },
    {
        "loss": 2.3988,
        "grad_norm": 3.6270573139190674,
        "learning_rate": 9.610789878676807e-08,
        "epoch": 0.6601333333333333,
        "step": 4951
    },
    {
        "loss": 2.9949,
        "grad_norm": 3.5484116077423096,
        "learning_rate": 9.336904870455021e-08,
        "epoch": 0.6602666666666667,
        "step": 4952
    },
    {
        "loss": 2.1572,
        "grad_norm": 2.4023945331573486,
        "learning_rate": 9.066977197926818e-08,
        "epoch": 0.6604,
        "step": 4953
    },
    {
        "loss": 1.943,
        "grad_norm": 4.277249336242676,
        "learning_rate": 8.801006968012226e-08,
        "epoch": 0.6605333333333333,
        "step": 4954
    },
    {
        "loss": 2.0161,
        "grad_norm": 2.7751638889312744,
        "learning_rate": 8.53899428606364e-08,
        "epoch": 0.6606666666666666,
        "step": 4955
    },
    {
        "loss": 2.1739,
        "grad_norm": 3.9398353099823,
        "learning_rate": 8.280939255863595e-08,
        "epoch": 0.6608,
        "step": 4956
    },
    {
        "loss": 1.8929,
        "grad_norm": 2.7432024478912354,
        "learning_rate": 8.026841979630328e-08,
        "epoch": 0.6609333333333334,
        "step": 4957
    },
    {
        "loss": 2.6695,
        "grad_norm": 2.800156831741333,
        "learning_rate": 7.776702558011106e-08,
        "epoch": 0.6610666666666667,
        "step": 4958
    },
    {
        "loss": 3.0055,
        "grad_norm": 2.548562526702881,
        "learning_rate": 7.530521090087783e-08,
        "epoch": 0.6612,
        "step": 4959
    },
    {
        "loss": 2.888,
        "grad_norm": 1.9528484344482422,
        "learning_rate": 7.288297673373468e-08,
        "epoch": 0.6613333333333333,
        "step": 4960
    },
    {
        "loss": 2.5015,
        "grad_norm": 2.87827205657959,
        "learning_rate": 7.050032403813633e-08,
        "epoch": 0.6614666666666666,
        "step": 4961
    },
    {
        "loss": 2.6662,
        "grad_norm": 2.4714105129241943,
        "learning_rate": 6.81572537578612e-08,
        "epoch": 0.6616,
        "step": 4962
    },
    {
        "loss": 1.6514,
        "grad_norm": 4.935948371887207,
        "learning_rate": 6.58537668210113e-08,
        "epoch": 0.6617333333333333,
        "step": 4963
    },
    {
        "loss": 2.3871,
        "grad_norm": 2.659750461578369,
        "learning_rate": 6.358986414000123e-08,
        "epoch": 0.6618666666666667,
        "step": 4964
    },
    {
        "loss": 1.9994,
        "grad_norm": 2.714877128601074,
        "learning_rate": 6.136554661156924e-08,
        "epoch": 0.662,
        "step": 4965
    },
    {
        "loss": 1.9337,
        "grad_norm": 5.159824848175049,
        "learning_rate": 5.91808151167772e-08,
        "epoch": 0.6621333333333334,
        "step": 4966
    },
    {
        "loss": 2.5592,
        "grad_norm": 2.5758321285247803,
        "learning_rate": 5.703567052099956e-08,
        "epoch": 0.6622666666666667,
        "step": 4967
    },
    {
        "loss": 1.7914,
        "grad_norm": 4.133598804473877,
        "learning_rate": 5.493011367394552e-08,
        "epoch": 0.6624,
        "step": 4968
    },
    {
        "loss": 2.7386,
        "grad_norm": 3.399959087371826,
        "learning_rate": 5.2864145409625695e-08,
        "epoch": 0.6625333333333333,
        "step": 4969
    },
    {
        "loss": 1.9668,
        "grad_norm": 5.9603471755981445,
        "learning_rate": 5.0837766546385504e-08,
        "epoch": 0.6626666666666666,
        "step": 4970
    },
    {
        "loss": 2.5101,
        "grad_norm": 3.243905544281006,
        "learning_rate": 4.885097788687176e-08,
        "epoch": 0.6628,
        "step": 4971
    },
    {
        "loss": 2.4212,
        "grad_norm": 3.6858975887298584,
        "learning_rate": 4.690378021805497e-08,
        "epoch": 0.6629333333333334,
        "step": 4972
    },
    {
        "loss": 2.7368,
        "grad_norm": 2.3219895362854004,
        "learning_rate": 4.499617431124037e-08,
        "epoch": 0.6630666666666667,
        "step": 4973
    },
    {
        "loss": 2.1462,
        "grad_norm": 3.125758171081543,
        "learning_rate": 4.312816092202354e-08,
        "epoch": 0.6632,
        "step": 4974
    },
    {
        "loss": 2.0756,
        "grad_norm": 2.384484052658081,
        "learning_rate": 4.129974079034593e-08,
        "epoch": 0.6633333333333333,
        "step": 4975
    },
    {
        "loss": 2.366,
        "grad_norm": 2.8365724086761475,
        "learning_rate": 3.951091464042822e-08,
        "epoch": 0.6634666666666666,
        "step": 4976
    },
    {
        "loss": 1.8127,
        "grad_norm": 3.4807159900665283,
        "learning_rate": 3.776168318084805e-08,
        "epoch": 0.6636,
        "step": 4977
    },
    {
        "loss": 2.2931,
        "grad_norm": 3.758244276046753,
        "learning_rate": 3.605204710448451e-08,
        "epoch": 0.6637333333333333,
        "step": 4978
    },
    {
        "loss": 2.4613,
        "grad_norm": 2.2095446586608887,
        "learning_rate": 3.438200708851813e-08,
        "epoch": 0.6638666666666667,
        "step": 4979
    },
    {
        "loss": 2.1749,
        "grad_norm": 3.970883846282959,
        "learning_rate": 3.2751563794475305e-08,
        "epoch": 0.664,
        "step": 4980
    },
    {
        "loss": 1.7681,
        "grad_norm": 2.7241756916046143,
        "learning_rate": 3.116071786815056e-08,
        "epoch": 0.6641333333333334,
        "step": 4981
    },
    {
        "loss": 1.9561,
        "grad_norm": 3.6888186931610107,
        "learning_rate": 2.9609469939717583e-08,
        "epoch": 0.6642666666666667,
        "step": 4982
    },
    {
        "loss": 2.5485,
        "grad_norm": 2.441361665725708,
        "learning_rate": 2.8097820623596006e-08,
        "epoch": 0.6644,
        "step": 4983
    },
    {
        "loss": 1.9247,
        "grad_norm": 3.314260244369507,
        "learning_rate": 2.6625770518584615e-08,
        "epoch": 0.6645333333333333,
        "step": 4984
    },
    {
        "loss": 2.2921,
        "grad_norm": 1.7899043560028076,
        "learning_rate": 2.5193320207761438e-08,
        "epoch": 0.6646666666666666,
        "step": 4985
    },
    {
        "loss": 2.3941,
        "grad_norm": 2.580822706222534,
        "learning_rate": 2.3800470258505958e-08,
        "epoch": 0.6648,
        "step": 4986
    },
    {
        "loss": 0.8523,
        "grad_norm": 8.853744506835938,
        "learning_rate": 2.2447221222554605e-08,
        "epoch": 0.6649333333333334,
        "step": 4987
    },
    {
        "loss": 2.2489,
        "grad_norm": 4.01249361038208,
        "learning_rate": 2.1133573635923053e-08,
        "epoch": 0.6650666666666667,
        "step": 4988
    },
    {
        "loss": 1.0807,
        "grad_norm": 3.974172592163086,
        "learning_rate": 1.9859528018950634e-08,
        "epoch": 0.6652,
        "step": 4989
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.284904718399048,
        "learning_rate": 1.8625084876289224e-08,
        "epoch": 0.6653333333333333,
        "step": 4990
    },
    {
        "loss": 1.2659,
        "grad_norm": 2.6349782943725586,
        "learning_rate": 1.7430244696925447e-08,
        "epoch": 0.6654666666666667,
        "step": 4991
    },
    {
        "loss": 2.0703,
        "grad_norm": 2.091524124145508,
        "learning_rate": 1.627500795410297e-08,
        "epoch": 0.6656,
        "step": 4992
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.088627815246582,
        "learning_rate": 1.5159375105455732e-08,
        "epoch": 0.6657333333333333,
        "step": 4993
    },
    {
        "loss": 2.4045,
        "grad_norm": 3.4377827644348145,
        "learning_rate": 1.4083346592852487e-08,
        "epoch": 0.6658666666666667,
        "step": 4994
    },
    {
        "loss": 1.4821,
        "grad_norm": 4.702727794647217,
        "learning_rate": 1.3046922842541166e-08,
        "epoch": 0.666,
        "step": 4995
    },
    {
        "loss": 1.8392,
        "grad_norm": 2.9968338012695312,
        "learning_rate": 1.2050104265037832e-08,
        "epoch": 0.6661333333333334,
        "step": 4996
    },
    {
        "loss": 2.4932,
        "grad_norm": 2.098097562789917,
        "learning_rate": 1.1092891255182204e-08,
        "epoch": 0.6662666666666667,
        "step": 4997
    },
    {
        "loss": 2.6275,
        "grad_norm": 2.748577833175659,
        "learning_rate": 1.0175284192148749e-08,
        "epoch": 0.6664,
        "step": 4998
    },
    {
        "loss": 1.8575,
        "grad_norm": 3.5369954109191895,
        "learning_rate": 9.297283439380079e-09,
        "epoch": 0.6665333333333333,
        "step": 4999
    },
    {
        "loss": 2.3686,
        "grad_norm": 3.3598573207855225,
        "learning_rate": 8.458889344675758e-09,
        "epoch": 0.6666666666666666,
        "step": 5000
    },
    {
        "loss": 2.3916,
        "grad_norm": 2.813690662384033,
        "learning_rate": 7.660102240114597e-09,
        "epoch": 0.6668,
        "step": 5001
    },
    {
        "loss": 2.7407,
        "grad_norm": 1.864205241203308,
        "learning_rate": 6.900922442099056e-09,
        "epoch": 0.6669333333333334,
        "step": 5002
    },
    {
        "loss": 2.6093,
        "grad_norm": 2.2324540615081787,
        "learning_rate": 6.1813502513552445e-09,
        "epoch": 0.6670666666666667,
        "step": 5003
    },
    {
        "loss": 2.9104,
        "grad_norm": 2.4279086589813232,
        "learning_rate": 5.501385952888516e-09,
        "epoch": 0.6672,
        "step": 5004
    },
    {
        "loss": 2.8776,
        "grad_norm": 1.7309550046920776,
        "learning_rate": 4.86102981606118e-09,
        "epoch": 0.6673333333333333,
        "step": 5005
    },
    {
        "loss": 1.8076,
        "grad_norm": 2.3475682735443115,
        "learning_rate": 4.260282094492585e-09,
        "epoch": 0.6674666666666667,
        "step": 5006
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.1280808448791504,
        "learning_rate": 3.6991430261590354e-09,
        "epoch": 0.6676,
        "step": 5007
    },
    {
        "loss": 2.2703,
        "grad_norm": 4.48672342300415,
        "learning_rate": 3.177612833316079e-09,
        "epoch": 0.6677333333333333,
        "step": 5008
    },
    {
        "loss": 2.2786,
        "grad_norm": 3.7757484912872314,
        "learning_rate": 2.695691722554017e-09,
        "epoch": 0.6678666666666667,
        "step": 5009
    },
    {
        "loss": 2.4414,
        "grad_norm": 2.3133747577667236,
        "learning_rate": 2.253379884764595e-09,
        "epoch": 0.668,
        "step": 5010
    },
    {
        "loss": 2.9122,
        "grad_norm": 3.2122137546539307,
        "learning_rate": 1.8506774951410066e-09,
        "epoch": 0.6681333333333334,
        "step": 5011
    },
    {
        "loss": 2.2695,
        "grad_norm": 2.811488389968872,
        "learning_rate": 1.4875847132000963e-09,
        "epoch": 0.6682666666666667,
        "step": 5012
    },
    {
        "loss": 2.2424,
        "grad_norm": 3.7818663120269775,
        "learning_rate": 1.164101682771257e-09,
        "epoch": 0.6684,
        "step": 5013
    },
    {
        "loss": 1.6759,
        "grad_norm": 2.380152463912964,
        "learning_rate": 8.802285319742254e-10,
        "epoch": 0.6685333333333333,
        "step": 5014
    },
    {
        "loss": 2.5247,
        "grad_norm": 2.351957082748413,
        "learning_rate": 6.359653732523896e-10,
        "epoch": 0.6686666666666666,
        "step": 5015
    },
    {
        "loss": 2.4178,
        "grad_norm": 3.7030704021453857,
        "learning_rate": 4.313123033727884e-10,
        "epoch": 0.6688,
        "step": 5016
    },
    {
        "loss": 1.3899,
        "grad_norm": 2.9400851726531982,
        "learning_rate": 2.662694033817026e-10,
        "epoch": 0.6689333333333334,
        "step": 5017
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.5509490966796875,
        "learning_rate": 1.4083673867126833e-10,
        "epoch": 0.6690666666666667,
        "step": 5018
    },
    {
        "loss": 1.7721,
        "grad_norm": 2.935905933380127,
        "learning_rate": 5.5014358912863775e-11,
        "epoch": 0.6692,
        "step": 5019
    },
    {
        "loss": 2.4416,
        "grad_norm": 3.5125949382781982,
        "learning_rate": 8.802298101517892e-12,
        "epoch": 0.6693333333333333,
        "step": 5020
    },
    {
        "loss": 2.5754,
        "grad_norm": 3.647231340408325,
        "learning_rate": 0.00019999999779942546,
        "epoch": 0.6694666666666667,
        "step": 5021
    },
    {
        "loss": 2.9245,
        "grad_norm": 3.280933141708374,
        "learning_rate": 0.00019999996479080917,
        "epoch": 0.6696,
        "step": 5022
    },
    {
        "loss": 1.3693,
        "grad_norm": 5.497812271118164,
        "learning_rate": 0.00019999989217186603,
        "epoch": 0.6697333333333333,
        "step": 5023
    },
    {
        "loss": 2.9471,
        "grad_norm": 2.367631673812866,
        "learning_rate": 0.00019999977994262488,
        "epoch": 0.6698666666666667,
        "step": 5024
    },
    {
        "loss": 2.075,
        "grad_norm": 3.5711750984191895,
        "learning_rate": 0.00019999962810313018,
        "epoch": 0.67,
        "step": 5025
    },
    {
        "loss": 1.8648,
        "grad_norm": 2.5380635261535645,
        "learning_rate": 0.00019999943665344202,
        "epoch": 0.6701333333333334,
        "step": 5026
    },
    {
        "loss": 3.1401,
        "grad_norm": 3.299031972885132,
        "learning_rate": 0.00019999920559363627,
        "epoch": 0.6702666666666667,
        "step": 5027
    },
    {
        "loss": 2.3323,
        "grad_norm": 2.3779103755950928,
        "learning_rate": 0.00019999893492380445,
        "epoch": 0.6704,
        "step": 5028
    },
    {
        "loss": 1.7202,
        "grad_norm": 4.004828453063965,
        "learning_rate": 0.00019999862464405377,
        "epoch": 0.6705333333333333,
        "step": 5029
    },
    {
        "loss": 2.3268,
        "grad_norm": 2.624906301498413,
        "learning_rate": 0.00019999827475450713,
        "epoch": 0.6706666666666666,
        "step": 5030
    },
    {
        "loss": 2.3309,
        "grad_norm": 2.5221214294433594,
        "learning_rate": 0.0001999978852553031,
        "epoch": 0.6708,
        "step": 5031
    },
    {
        "loss": 2.3751,
        "grad_norm": 2.8544533252716064,
        "learning_rate": 0.00019999745614659602,
        "epoch": 0.6709333333333334,
        "step": 5032
    },
    {
        "loss": 2.1271,
        "grad_norm": 4.637521743774414,
        "learning_rate": 0.0001999969874285558,
        "epoch": 0.6710666666666667,
        "step": 5033
    },
    {
        "loss": 2.262,
        "grad_norm": 2.483602285385132,
        "learning_rate": 0.00019999647910136817,
        "epoch": 0.6712,
        "step": 5034
    },
    {
        "loss": 2.3961,
        "grad_norm": 3.744605302810669,
        "learning_rate": 0.0001999959311652344,
        "epoch": 0.6713333333333333,
        "step": 5035
    },
    {
        "loss": 2.7465,
        "grad_norm": 2.872897148132324,
        "learning_rate": 0.0001999953436203716,
        "epoch": 0.6714666666666667,
        "step": 5036
    },
    {
        "loss": 2.0043,
        "grad_norm": 2.450619697570801,
        "learning_rate": 0.00019999471646701242,
        "epoch": 0.6716,
        "step": 5037
    },
    {
        "loss": 1.9402,
        "grad_norm": 4.946652412414551,
        "learning_rate": 0.00019999404970540537,
        "epoch": 0.6717333333333333,
        "step": 5038
    },
    {
        "loss": 1.9655,
        "grad_norm": 3.3862271308898926,
        "learning_rate": 0.00019999334333581449,
        "epoch": 0.6718666666666666,
        "step": 5039
    },
    {
        "loss": 1.4293,
        "grad_norm": 5.051753044128418,
        "learning_rate": 0.00019999259735851958,
        "epoch": 0.672,
        "step": 5040
    },
    {
        "loss": 2.3734,
        "grad_norm": 3.796679735183716,
        "learning_rate": 0.00019999181177381617,
        "epoch": 0.6721333333333334,
        "step": 5041
    },
    {
        "loss": 2.3525,
        "grad_norm": 5.261869430541992,
        "learning_rate": 0.00019999098658201537,
        "epoch": 0.6722666666666667,
        "step": 5042
    },
    {
        "loss": 2.142,
        "grad_norm": 3.489867687225342,
        "learning_rate": 0.0001999901217834441,
        "epoch": 0.6724,
        "step": 5043
    },
    {
        "loss": 2.383,
        "grad_norm": 4.590339183807373,
        "learning_rate": 0.00019998921737844486,
        "epoch": 0.6725333333333333,
        "step": 5044
    },
    {
        "loss": 2.4513,
        "grad_norm": 4.066099166870117,
        "learning_rate": 0.00019998827336737595,
        "epoch": 0.6726666666666666,
        "step": 5045
    },
    {
        "loss": 2.0288,
        "grad_norm": 7.9607343673706055,
        "learning_rate": 0.0001999872897506112,
        "epoch": 0.6728,
        "step": 5046
    },
    {
        "loss": 2.9693,
        "grad_norm": 2.2838711738586426,
        "learning_rate": 0.00019998626652854035,
        "epoch": 0.6729333333333334,
        "step": 5047
    },
    {
        "loss": 2.2253,
        "grad_norm": 4.705495357513428,
        "learning_rate": 0.0001999852037015686,
        "epoch": 0.6730666666666667,
        "step": 5048
    },
    {
        "loss": 1.847,
        "grad_norm": 5.055049896240234,
        "learning_rate": 0.00019998410127011694,
        "epoch": 0.6732,
        "step": 5049
    },
    {
        "loss": 2.1037,
        "grad_norm": 2.9393413066864014,
        "learning_rate": 0.00019998295923462212,
        "epoch": 0.6733333333333333,
        "step": 5050
    },
    {
        "loss": 2.7578,
        "grad_norm": 3.6690168380737305,
        "learning_rate": 0.00019998177759553647,
        "epoch": 0.6734666666666667,
        "step": 5051
    },
    {
        "loss": 1.0539,
        "grad_norm": 6.519392490386963,
        "learning_rate": 0.00019998055635332798,
        "epoch": 0.6736,
        "step": 5052
    },
    {
        "loss": 1.8011,
        "grad_norm": 3.915255308151245,
        "learning_rate": 0.00019997929550848048,
        "epoch": 0.6737333333333333,
        "step": 5053
    },
    {
        "loss": 1.193,
        "grad_norm": 4.147524833679199,
        "learning_rate": 0.00019997799506149338,
        "epoch": 0.6738666666666666,
        "step": 5054
    },
    {
        "loss": 2.3319,
        "grad_norm": 2.5676920413970947,
        "learning_rate": 0.00019997665501288175,
        "epoch": 0.674,
        "step": 5055
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.1580305099487305,
        "learning_rate": 0.00019997527536317643,
        "epoch": 0.6741333333333334,
        "step": 5056
    },
    {
        "loss": 2.4329,
        "grad_norm": 3.4837422370910645,
        "learning_rate": 0.00019997385611292384,
        "epoch": 0.6742666666666667,
        "step": 5057
    },
    {
        "loss": 2.0443,
        "grad_norm": 4.323540687561035,
        "learning_rate": 0.0001999723972626862,
        "epoch": 0.6744,
        "step": 5058
    },
    {
        "loss": 2.7195,
        "grad_norm": 2.464067220687866,
        "learning_rate": 0.00019997089881304135,
        "epoch": 0.6745333333333333,
        "step": 5059
    },
    {
        "loss": 2.4707,
        "grad_norm": 3.382611036300659,
        "learning_rate": 0.00019996936076458288,
        "epoch": 0.6746666666666666,
        "step": 5060
    },
    {
        "loss": 0.8348,
        "grad_norm": 9.00316047668457,
        "learning_rate": 0.00019996778311791994,
        "epoch": 0.6748,
        "step": 5061
    },
    {
        "loss": 3.0152,
        "grad_norm": 1.9803388118743896,
        "learning_rate": 0.0001999661658736775,
        "epoch": 0.6749333333333334,
        "step": 5062
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.649571418762207,
        "learning_rate": 0.0001999645090324961,
        "epoch": 0.6750666666666667,
        "step": 5063
    },
    {
        "loss": 1.6915,
        "grad_norm": 8.363900184631348,
        "learning_rate": 0.00019996281259503207,
        "epoch": 0.6752,
        "step": 5064
    },
    {
        "loss": 1.9337,
        "grad_norm": 3.215294361114502,
        "learning_rate": 0.00019996107656195734,
        "epoch": 0.6753333333333333,
        "step": 5065
    },
    {
        "loss": 3.0226,
        "grad_norm": 2.783924102783203,
        "learning_rate": 0.00019995930093395957,
        "epoch": 0.6754666666666667,
        "step": 5066
    },
    {
        "loss": 2.3734,
        "grad_norm": 8.230790138244629,
        "learning_rate": 0.00019995748571174216,
        "epoch": 0.6756,
        "step": 5067
    },
    {
        "loss": 2.071,
        "grad_norm": 3.3954708576202393,
        "learning_rate": 0.000199955630896024,
        "epoch": 0.6757333333333333,
        "step": 5068
    },
    {
        "loss": 2.5718,
        "grad_norm": 2.500704526901245,
        "learning_rate": 0.00019995373648753985,
        "epoch": 0.6758666666666666,
        "step": 5069
    },
    {
        "loss": 2.3689,
        "grad_norm": 2.8564531803131104,
        "learning_rate": 0.00019995180248704011,
        "epoch": 0.676,
        "step": 5070
    },
    {
        "loss": 3.1531,
        "grad_norm": 3.012805700302124,
        "learning_rate": 0.00019994982889529083,
        "epoch": 0.6761333333333334,
        "step": 5071
    },
    {
        "loss": 2.929,
        "grad_norm": 3.1180579662323,
        "learning_rate": 0.00019994781571307376,
        "epoch": 0.6762666666666667,
        "step": 5072
    },
    {
        "loss": 1.7945,
        "grad_norm": 5.879926681518555,
        "learning_rate": 0.0001999457629411863,
        "epoch": 0.6764,
        "step": 5073
    },
    {
        "loss": 2.9589,
        "grad_norm": 2.959617853164673,
        "learning_rate": 0.0001999436705804416,
        "epoch": 0.6765333333333333,
        "step": 5074
    },
    {
        "loss": 1.1126,
        "grad_norm": 5.013241767883301,
        "learning_rate": 0.00019994153863166846,
        "epoch": 0.6766666666666666,
        "step": 5075
    },
    {
        "loss": 2.5678,
        "grad_norm": 1.8726729154586792,
        "learning_rate": 0.00019993936709571128,
        "epoch": 0.6768,
        "step": 5076
    },
    {
        "loss": 2.0668,
        "grad_norm": 2.1475820541381836,
        "learning_rate": 0.00019993715597343026,
        "epoch": 0.6769333333333334,
        "step": 5077
    },
    {
        "loss": 1.2172,
        "grad_norm": 3.4676287174224854,
        "learning_rate": 0.00019993490526570127,
        "epoch": 0.6770666666666667,
        "step": 5078
    },
    {
        "loss": 2.5074,
        "grad_norm": 4.150631427764893,
        "learning_rate": 0.00019993261497341575,
        "epoch": 0.6772,
        "step": 5079
    },
    {
        "loss": 1.8906,
        "grad_norm": 5.338557243347168,
        "learning_rate": 0.00019993028509748093,
        "epoch": 0.6773333333333333,
        "step": 5080
    },
    {
        "loss": 1.6728,
        "grad_norm": 3.348977565765381,
        "learning_rate": 0.00019992791563881967,
        "epoch": 0.6774666666666667,
        "step": 5081
    },
    {
        "loss": 2.3561,
        "grad_norm": 3.449631690979004,
        "learning_rate": 0.00019992550659837056,
        "epoch": 0.6776,
        "step": 5082
    },
    {
        "loss": 1.8079,
        "grad_norm": 4.227015972137451,
        "learning_rate": 0.0001999230579770878,
        "epoch": 0.6777333333333333,
        "step": 5083
    },
    {
        "loss": 1.7993,
        "grad_norm": 7.705837249755859,
        "learning_rate": 0.00019992056977594127,
        "epoch": 0.6778666666666666,
        "step": 5084
    },
    {
        "loss": 2.2838,
        "grad_norm": 3.097630023956299,
        "learning_rate": 0.00019991804199591658,
        "epoch": 0.678,
        "step": 5085
    },
    {
        "loss": 2.1355,
        "grad_norm": 3.832303524017334,
        "learning_rate": 0.000199915474638015,
        "epoch": 0.6781333333333334,
        "step": 5086
    },
    {
        "loss": 2.5467,
        "grad_norm": 3.025588035583496,
        "learning_rate": 0.0001999128677032535,
        "epoch": 0.6782666666666667,
        "step": 5087
    },
    {
        "loss": 2.6063,
        "grad_norm": 4.4437971115112305,
        "learning_rate": 0.0001999102211926646,
        "epoch": 0.6784,
        "step": 5088
    },
    {
        "loss": 2.2417,
        "grad_norm": 3.1206893920898438,
        "learning_rate": 0.00019990753510729667,
        "epoch": 0.6785333333333333,
        "step": 5089
    },
    {
        "loss": 0.759,
        "grad_norm": 3.1612138748168945,
        "learning_rate": 0.00019990480944821366,
        "epoch": 0.6786666666666666,
        "step": 5090
    },
    {
        "loss": 1.927,
        "grad_norm": 3.1769561767578125,
        "learning_rate": 0.00019990204421649522,
        "epoch": 0.6788,
        "step": 5091
    },
    {
        "loss": 2.1926,
        "grad_norm": 3.5301513671875,
        "learning_rate": 0.00019989923941323662,
        "epoch": 0.6789333333333334,
        "step": 5092
    },
    {
        "loss": 2.2671,
        "grad_norm": 3.736082077026367,
        "learning_rate": 0.00019989639503954892,
        "epoch": 0.6790666666666667,
        "step": 5093
    },
    {
        "loss": 2.2276,
        "grad_norm": 5.115593910217285,
        "learning_rate": 0.00019989351109655877,
        "epoch": 0.6792,
        "step": 5094
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.873720407485962,
        "learning_rate": 0.00019989058758540848,
        "epoch": 0.6793333333333333,
        "step": 5095
    },
    {
        "loss": 2.123,
        "grad_norm": 2.954777956008911,
        "learning_rate": 0.0001998876245072561,
        "epoch": 0.6794666666666667,
        "step": 5096
    },
    {
        "loss": 1.8922,
        "grad_norm": 3.4540984630584717,
        "learning_rate": 0.00019988462186327526,
        "epoch": 0.6796,
        "step": 5097
    },
    {
        "loss": 1.2221,
        "grad_norm": 4.182561874389648,
        "learning_rate": 0.00019988157965465536,
        "epoch": 0.6797333333333333,
        "step": 5098
    },
    {
        "loss": 2.4025,
        "grad_norm": 3.8028182983398438,
        "learning_rate": 0.00019987849788260143,
        "epoch": 0.6798666666666666,
        "step": 5099
    },
    {
        "loss": 2.258,
        "grad_norm": 3.0589797496795654,
        "learning_rate": 0.00019987537654833415,
        "epoch": 0.68,
        "step": 5100
    },
    {
        "loss": 2.3295,
        "grad_norm": 4.46747350692749,
        "learning_rate": 0.00019987221565308992,
        "epoch": 0.6801333333333334,
        "step": 5101
    },
    {
        "loss": 2.2107,
        "grad_norm": 3.122236967086792,
        "learning_rate": 0.00019986901519812076,
        "epoch": 0.6802666666666667,
        "step": 5102
    },
    {
        "loss": 2.4129,
        "grad_norm": 3.3785481452941895,
        "learning_rate": 0.00019986577518469435,
        "epoch": 0.6804,
        "step": 5103
    },
    {
        "loss": 1.3963,
        "grad_norm": 2.829695463180542,
        "learning_rate": 0.00019986249561409415,
        "epoch": 0.6805333333333333,
        "step": 5104
    },
    {
        "loss": 1.2203,
        "grad_norm": 6.427985191345215,
        "learning_rate": 0.00019985917648761915,
        "epoch": 0.6806666666666666,
        "step": 5105
    },
    {
        "loss": 2.0422,
        "grad_norm": 5.268685817718506,
        "learning_rate": 0.0001998558178065841,
        "epoch": 0.6808,
        "step": 5106
    },
    {
        "loss": 1.6186,
        "grad_norm": 2.0195538997650146,
        "learning_rate": 0.0001998524195723193,
        "epoch": 0.6809333333333333,
        "step": 5107
    },
    {
        "loss": 2.4479,
        "grad_norm": 2.147067070007324,
        "learning_rate": 0.00019984898178617092,
        "epoch": 0.6810666666666667,
        "step": 5108
    },
    {
        "loss": 2.5329,
        "grad_norm": 3.1341865062713623,
        "learning_rate": 0.00019984550444950063,
        "epoch": 0.6812,
        "step": 5109
    },
    {
        "loss": 2.4417,
        "grad_norm": 4.670008659362793,
        "learning_rate": 0.0001998419875636858,
        "epoch": 0.6813333333333333,
        "step": 5110
    },
    {
        "loss": 2.4733,
        "grad_norm": 2.2449169158935547,
        "learning_rate": 0.00019983843113011949,
        "epoch": 0.6814666666666667,
        "step": 5111
    },
    {
        "loss": 1.928,
        "grad_norm": 4.363729953765869,
        "learning_rate": 0.00019983483515021043,
        "epoch": 0.6816,
        "step": 5112
    },
    {
        "loss": 2.3618,
        "grad_norm": 4.165435791015625,
        "learning_rate": 0.000199831199625383,
        "epoch": 0.6817333333333333,
        "step": 5113
    },
    {
        "loss": 2.7825,
        "grad_norm": 3.4303672313690186,
        "learning_rate": 0.0001998275245570772,
        "epoch": 0.6818666666666666,
        "step": 5114
    },
    {
        "loss": 2.6599,
        "grad_norm": 2.0857396125793457,
        "learning_rate": 0.0001998238099467488,
        "epoch": 0.682,
        "step": 5115
    },
    {
        "loss": 2.4905,
        "grad_norm": 2.550015687942505,
        "learning_rate": 0.0001998200557958691,
        "epoch": 0.6821333333333334,
        "step": 5116
    },
    {
        "loss": 1.9968,
        "grad_norm": 3.409499168395996,
        "learning_rate": 0.0001998162621059252,
        "epoch": 0.6822666666666667,
        "step": 5117
    },
    {
        "loss": 2.6723,
        "grad_norm": 4.153104305267334,
        "learning_rate": 0.00019981242887841976,
        "epoch": 0.6824,
        "step": 5118
    },
    {
        "loss": 2.4528,
        "grad_norm": 2.4683964252471924,
        "learning_rate": 0.00019980855611487113,
        "epoch": 0.6825333333333333,
        "step": 5119
    },
    {
        "loss": 2.526,
        "grad_norm": 3.9286835193634033,
        "learning_rate": 0.00019980464381681335,
        "epoch": 0.6826666666666666,
        "step": 5120
    },
    {
        "loss": 2.3724,
        "grad_norm": 2.5285961627960205,
        "learning_rate": 0.00019980069198579606,
        "epoch": 0.6828,
        "step": 5121
    },
    {
        "loss": 2.6866,
        "grad_norm": 3.235280990600586,
        "learning_rate": 0.0001997967006233846,
        "epoch": 0.6829333333333333,
        "step": 5122
    },
    {
        "loss": 2.2654,
        "grad_norm": 3.0180304050445557,
        "learning_rate": 0.00019979266973116003,
        "epoch": 0.6830666666666667,
        "step": 5123
    },
    {
        "loss": 2.5192,
        "grad_norm": 3.6342337131500244,
        "learning_rate": 0.00019978859931071887,
        "epoch": 0.6832,
        "step": 5124
    },
    {
        "loss": 3.0125,
        "grad_norm": 2.5328304767608643,
        "learning_rate": 0.00019978448936367355,
        "epoch": 0.6833333333333333,
        "step": 5125
    },
    {
        "loss": 2.7747,
        "grad_norm": 2.3158810138702393,
        "learning_rate": 0.00019978033989165198,
        "epoch": 0.6834666666666667,
        "step": 5126
    },
    {
        "loss": 2.2435,
        "grad_norm": 2.7333240509033203,
        "learning_rate": 0.00019977615089629777,
        "epoch": 0.6836,
        "step": 5127
    },
    {
        "loss": 2.5165,
        "grad_norm": 6.493738651275635,
        "learning_rate": 0.00019977192237927022,
        "epoch": 0.6837333333333333,
        "step": 5128
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.3205275535583496,
        "learning_rate": 0.00019976765434224426,
        "epoch": 0.6838666666666666,
        "step": 5129
    },
    {
        "loss": 2.131,
        "grad_norm": 3.5880162715911865,
        "learning_rate": 0.00019976334678691046,
        "epoch": 0.684,
        "step": 5130
    },
    {
        "loss": 0.721,
        "grad_norm": 2.688253164291382,
        "learning_rate": 0.00019975899971497505,
        "epoch": 0.6841333333333334,
        "step": 5131
    },
    {
        "loss": 2.8871,
        "grad_norm": 2.5437300205230713,
        "learning_rate": 0.00019975461312815996,
        "epoch": 0.6842666666666667,
        "step": 5132
    },
    {
        "loss": 2.0263,
        "grad_norm": 3.00101375579834,
        "learning_rate": 0.00019975018702820266,
        "epoch": 0.6844,
        "step": 5133
    },
    {
        "loss": 2.2108,
        "grad_norm": 4.006961345672607,
        "learning_rate": 0.00019974572141685643,
        "epoch": 0.6845333333333333,
        "step": 5134
    },
    {
        "loss": 2.6748,
        "grad_norm": 2.6036109924316406,
        "learning_rate": 0.00019974121629589008,
        "epoch": 0.6846666666666666,
        "step": 5135
    },
    {
        "loss": 2.169,
        "grad_norm": 2.372368812561035,
        "learning_rate": 0.00019973667166708805,
        "epoch": 0.6848,
        "step": 5136
    },
    {
        "loss": 2.0615,
        "grad_norm": 4.776371955871582,
        "learning_rate": 0.00019973208753225054,
        "epoch": 0.6849333333333333,
        "step": 5137
    },
    {
        "loss": 1.7242,
        "grad_norm": 2.789808750152588,
        "learning_rate": 0.00019972746389319336,
        "epoch": 0.6850666666666667,
        "step": 5138
    },
    {
        "loss": 1.8725,
        "grad_norm": 3.153038263320923,
        "learning_rate": 0.0001997228007517479,
        "epoch": 0.6852,
        "step": 5139
    },
    {
        "loss": 2.7515,
        "grad_norm": 3.6542720794677734,
        "learning_rate": 0.00019971809810976127,
        "epoch": 0.6853333333333333,
        "step": 5140
    },
    {
        "loss": 2.1975,
        "grad_norm": 3.042686939239502,
        "learning_rate": 0.0001997133559690962,
        "epoch": 0.6854666666666667,
        "step": 5141
    },
    {
        "loss": 3.0082,
        "grad_norm": 4.228085041046143,
        "learning_rate": 0.00019970857433163105,
        "epoch": 0.6856,
        "step": 5142
    },
    {
        "loss": 2.696,
        "grad_norm": 3.1150097846984863,
        "learning_rate": 0.00019970375319925985,
        "epoch": 0.6857333333333333,
        "step": 5143
    },
    {
        "loss": 1.5907,
        "grad_norm": 5.122781753540039,
        "learning_rate": 0.0001996988925738923,
        "epoch": 0.6858666666666666,
        "step": 5144
    },
    {
        "loss": 2.8507,
        "grad_norm": 2.632802963256836,
        "learning_rate": 0.00019969399245745368,
        "epoch": 0.686,
        "step": 5145
    },
    {
        "loss": 2.1927,
        "grad_norm": 3.4727871417999268,
        "learning_rate": 0.00019968905285188492,
        "epoch": 0.6861333333333334,
        "step": 5146
    },
    {
        "loss": 2.7442,
        "grad_norm": 2.5403892993927,
        "learning_rate": 0.0001996840737591427,
        "epoch": 0.6862666666666667,
        "step": 5147
    },
    {
        "loss": 2.304,
        "grad_norm": 2.895620584487915,
        "learning_rate": 0.00019967905518119915,
        "epoch": 0.6864,
        "step": 5148
    },
    {
        "loss": 2.0258,
        "grad_norm": 3.698701858520508,
        "learning_rate": 0.00019967399712004223,
        "epoch": 0.6865333333333333,
        "step": 5149
    },
    {
        "loss": 2.1931,
        "grad_norm": 3.2660434246063232,
        "learning_rate": 0.00019966889957767538,
        "epoch": 0.6866666666666666,
        "step": 5150
    },
    {
        "loss": 2.368,
        "grad_norm": 4.346702575683594,
        "learning_rate": 0.00019966376255611782,
        "epoch": 0.6868,
        "step": 5151
    },
    {
        "loss": 2.299,
        "grad_norm": 2.8365366458892822,
        "learning_rate": 0.00019965858605740434,
        "epoch": 0.6869333333333333,
        "step": 5152
    },
    {
        "loss": 2.5715,
        "grad_norm": 1.9604699611663818,
        "learning_rate": 0.0001996533700835853,
        "epoch": 0.6870666666666667,
        "step": 5153
    },
    {
        "loss": 2.2636,
        "grad_norm": 3.7285847663879395,
        "learning_rate": 0.00019964811463672684,
        "epoch": 0.6872,
        "step": 5154
    },
    {
        "loss": 1.5792,
        "grad_norm": 3.6618480682373047,
        "learning_rate": 0.0001996428197189106,
        "epoch": 0.6873333333333334,
        "step": 5155
    },
    {
        "loss": 2.9513,
        "grad_norm": 2.8726532459259033,
        "learning_rate": 0.00019963748533223394,
        "epoch": 0.6874666666666667,
        "step": 5156
    },
    {
        "loss": 2.8579,
        "grad_norm": 3.9593722820281982,
        "learning_rate": 0.00019963211147880987,
        "epoch": 0.6876,
        "step": 5157
    },
    {
        "loss": 2.8507,
        "grad_norm": 2.160349130630493,
        "learning_rate": 0.00019962669816076694,
        "epoch": 0.6877333333333333,
        "step": 5158
    },
    {
        "loss": 1.8189,
        "grad_norm": 3.4295437335968018,
        "learning_rate": 0.0001996212453802494,
        "epoch": 0.6878666666666666,
        "step": 5159
    },
    {
        "loss": 1.8423,
        "grad_norm": 3.490335464477539,
        "learning_rate": 0.00019961575313941712,
        "epoch": 0.688,
        "step": 5160
    },
    {
        "loss": 2.8057,
        "grad_norm": 2.946864604949951,
        "learning_rate": 0.00019961022144044556,
        "epoch": 0.6881333333333334,
        "step": 5161
    },
    {
        "loss": 2.4351,
        "grad_norm": 3.2280609607696533,
        "learning_rate": 0.0001996046502855259,
        "epoch": 0.6882666666666667,
        "step": 5162
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.7003402709960938,
        "learning_rate": 0.00019959903967686486,
        "epoch": 0.6884,
        "step": 5163
    },
    {
        "loss": 2.2546,
        "grad_norm": 6.040924549102783,
        "learning_rate": 0.00019959338961668483,
        "epoch": 0.6885333333333333,
        "step": 5164
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.378507614135742,
        "learning_rate": 0.00019958770010722383,
        "epoch": 0.6886666666666666,
        "step": 5165
    },
    {
        "loss": 2.3885,
        "grad_norm": 3.702354669570923,
        "learning_rate": 0.00019958197115073546,
        "epoch": 0.6888,
        "step": 5166
    },
    {
        "loss": 2.0842,
        "grad_norm": 2.3621740341186523,
        "learning_rate": 0.000199576202749489,
        "epoch": 0.6889333333333333,
        "step": 5167
    },
    {
        "loss": 1.3545,
        "grad_norm": 3.6794049739837646,
        "learning_rate": 0.00019957039490576937,
        "epoch": 0.6890666666666667,
        "step": 5168
    },
    {
        "loss": 2.7172,
        "grad_norm": 4.888355255126953,
        "learning_rate": 0.000199564547621877,
        "epoch": 0.6892,
        "step": 5169
    },
    {
        "loss": 2.9951,
        "grad_norm": 2.5390965938568115,
        "learning_rate": 0.00019955866090012807,
        "epoch": 0.6893333333333334,
        "step": 5170
    },
    {
        "loss": 2.3939,
        "grad_norm": 3.6574227809906006,
        "learning_rate": 0.0001995527347428543,
        "epoch": 0.6894666666666667,
        "step": 5171
    },
    {
        "loss": 2.76,
        "grad_norm": 3.0853676795959473,
        "learning_rate": 0.0001995467691524031,
        "epoch": 0.6896,
        "step": 5172
    },
    {
        "loss": 1.8675,
        "grad_norm": 3.8748843669891357,
        "learning_rate": 0.00019954076413113744,
        "epoch": 0.6897333333333333,
        "step": 5173
    },
    {
        "loss": 2.5162,
        "grad_norm": 4.631681442260742,
        "learning_rate": 0.00019953471968143596,
        "epoch": 0.6898666666666666,
        "step": 5174
    },
    {
        "loss": 1.0573,
        "grad_norm": 4.115206241607666,
        "learning_rate": 0.00019952863580569282,
        "epoch": 0.69,
        "step": 5175
    },
    {
        "loss": 1.5946,
        "grad_norm": 3.0968120098114014,
        "learning_rate": 0.0001995225125063179,
        "epoch": 0.6901333333333334,
        "step": 5176
    },
    {
        "loss": 2.2728,
        "grad_norm": 4.209404945373535,
        "learning_rate": 0.00019951634978573668,
        "epoch": 0.6902666666666667,
        "step": 5177
    },
    {
        "loss": 2.8382,
        "grad_norm": 2.1843724250793457,
        "learning_rate": 0.00019951014764639022,
        "epoch": 0.6904,
        "step": 5178
    },
    {
        "loss": 1.1007,
        "grad_norm": 3.7108540534973145,
        "learning_rate": 0.0001995039060907352,
        "epoch": 0.6905333333333333,
        "step": 5179
    },
    {
        "loss": 2.1949,
        "grad_norm": 2.691779613494873,
        "learning_rate": 0.00019949762512124392,
        "epoch": 0.6906666666666667,
        "step": 5180
    },
    {
        "loss": 2.1115,
        "grad_norm": 3.6716644763946533,
        "learning_rate": 0.0001994913047404043,
        "epoch": 0.6908,
        "step": 5181
    },
    {
        "loss": 2.2467,
        "grad_norm": 2.2046985626220703,
        "learning_rate": 0.00019948494495071985,
        "epoch": 0.6909333333333333,
        "step": 5182
    },
    {
        "loss": 2.5778,
        "grad_norm": 3.38246488571167,
        "learning_rate": 0.00019947854575470974,
        "epoch": 0.6910666666666667,
        "step": 5183
    },
    {
        "loss": 2.8607,
        "grad_norm": 2.7372372150421143,
        "learning_rate": 0.0001994721071549087,
        "epoch": 0.6912,
        "step": 5184
    },
    {
        "loss": 1.8501,
        "grad_norm": 3.237823963165283,
        "learning_rate": 0.00019946562915386707,
        "epoch": 0.6913333333333334,
        "step": 5185
    },
    {
        "loss": 2.2738,
        "grad_norm": 3.7412922382354736,
        "learning_rate": 0.0001994591117541508,
        "epoch": 0.6914666666666667,
        "step": 5186
    },
    {
        "loss": 2.6381,
        "grad_norm": 4.121053218841553,
        "learning_rate": 0.00019945255495834143,
        "epoch": 0.6916,
        "step": 5187
    },
    {
        "loss": 2.9076,
        "grad_norm": 2.3967604637145996,
        "learning_rate": 0.00019944595876903622,
        "epoch": 0.6917333333333333,
        "step": 5188
    },
    {
        "loss": 2.7331,
        "grad_norm": 2.219205856323242,
        "learning_rate": 0.00019943932318884788,
        "epoch": 0.6918666666666666,
        "step": 5189
    },
    {
        "loss": 1.4049,
        "grad_norm": 2.2571399211883545,
        "learning_rate": 0.00019943264822040477,
        "epoch": 0.692,
        "step": 5190
    },
    {
        "loss": 2.7225,
        "grad_norm": 2.9729957580566406,
        "learning_rate": 0.00019942593386635088,
        "epoch": 0.6921333333333334,
        "step": 5191
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.075157880783081,
        "learning_rate": 0.00019941918012934587,
        "epoch": 0.6922666666666667,
        "step": 5192
    },
    {
        "loss": 2.287,
        "grad_norm": 2.126401662826538,
        "learning_rate": 0.00019941238701206478,
        "epoch": 0.6924,
        "step": 5193
    },
    {
        "loss": 2.6243,
        "grad_norm": 3.028175115585327,
        "learning_rate": 0.00019940555451719848,
        "epoch": 0.6925333333333333,
        "step": 5194
    },
    {
        "loss": 2.8938,
        "grad_norm": 3.047257661819458,
        "learning_rate": 0.00019939868264745329,
        "epoch": 0.6926666666666667,
        "step": 5195
    },
    {
        "loss": 2.5834,
        "grad_norm": 3.446831703186035,
        "learning_rate": 0.00019939177140555124,
        "epoch": 0.6928,
        "step": 5196
    },
    {
        "loss": 2.1388,
        "grad_norm": 3.0633902549743652,
        "learning_rate": 0.00019938482079422988,
        "epoch": 0.6929333333333333,
        "step": 5197
    },
    {
        "loss": 1.9938,
        "grad_norm": 3.2241508960723877,
        "learning_rate": 0.00019937783081624234,
        "epoch": 0.6930666666666667,
        "step": 5198
    },
    {
        "loss": 2.6605,
        "grad_norm": 2.1701889038085938,
        "learning_rate": 0.0001993708014743574,
        "epoch": 0.6932,
        "step": 5199
    },
    {
        "loss": 1.9774,
        "grad_norm": 4.32805061340332,
        "learning_rate": 0.0001993637327713594,
        "epoch": 0.6933333333333334,
        "step": 5200
    },
    {
        "loss": 2.8774,
        "grad_norm": 2.5287225246429443,
        "learning_rate": 0.00019935662471004827,
        "epoch": 0.6934666666666667,
        "step": 5201
    },
    {
        "loss": 2.2218,
        "grad_norm": 4.139075756072998,
        "learning_rate": 0.00019934947729323952,
        "epoch": 0.6936,
        "step": 5202
    },
    {
        "loss": 2.582,
        "grad_norm": 3.010287284851074,
        "learning_rate": 0.0001993422905237643,
        "epoch": 0.6937333333333333,
        "step": 5203
    },
    {
        "loss": 1.5365,
        "grad_norm": 5.261138916015625,
        "learning_rate": 0.0001993350644044693,
        "epoch": 0.6938666666666666,
        "step": 5204
    },
    {
        "loss": 2.8365,
        "grad_norm": 3.337237596511841,
        "learning_rate": 0.00019932779893821686,
        "epoch": 0.694,
        "step": 5205
    },
    {
        "loss": 2.79,
        "grad_norm": 3.3041112422943115,
        "learning_rate": 0.00019932049412788478,
        "epoch": 0.6941333333333334,
        "step": 5206
    },
    {
        "loss": 2.6134,
        "grad_norm": 2.4227561950683594,
        "learning_rate": 0.00019931314997636654,
        "epoch": 0.6942666666666667,
        "step": 5207
    },
    {
        "loss": 1.8167,
        "grad_norm": 2.9232490062713623,
        "learning_rate": 0.0001993057664865712,
        "epoch": 0.6944,
        "step": 5208
    },
    {
        "loss": 2.2361,
        "grad_norm": 3.086049795150757,
        "learning_rate": 0.00019929834366142336,
        "epoch": 0.6945333333333333,
        "step": 5209
    },
    {
        "loss": 2.6426,
        "grad_norm": 3.0164108276367188,
        "learning_rate": 0.00019929088150386328,
        "epoch": 0.6946666666666667,
        "step": 5210
    },
    {
        "loss": 2.7181,
        "grad_norm": 3.7156291007995605,
        "learning_rate": 0.00019928338001684668,
        "epoch": 0.6948,
        "step": 5211
    },
    {
        "loss": 1.8313,
        "grad_norm": 3.2504053115844727,
        "learning_rate": 0.00019927583920334495,
        "epoch": 0.6949333333333333,
        "step": 5212
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.970384359359741,
        "learning_rate": 0.00019926825906634507,
        "epoch": 0.6950666666666667,
        "step": 5213
    },
    {
        "loss": 1.8551,
        "grad_norm": 2.379263162612915,
        "learning_rate": 0.00019926063960884954,
        "epoch": 0.6952,
        "step": 5214
    },
    {
        "loss": 1.7431,
        "grad_norm": 3.173304557800293,
        "learning_rate": 0.00019925298083387638,
        "epoch": 0.6953333333333334,
        "step": 5215
    },
    {
        "loss": 2.4577,
        "grad_norm": 2.5341529846191406,
        "learning_rate": 0.00019924528274445935,
        "epoch": 0.6954666666666667,
        "step": 5216
    },
    {
        "loss": 2.521,
        "grad_norm": 2.752594232559204,
        "learning_rate": 0.00019923754534364766,
        "epoch": 0.6956,
        "step": 5217
    },
    {
        "loss": 0.7112,
        "grad_norm": 2.3201076984405518,
        "learning_rate": 0.0001992297686345061,
        "epoch": 0.6957333333333333,
        "step": 5218
    },
    {
        "loss": 3.3768,
        "grad_norm": 3.0209391117095947,
        "learning_rate": 0.00019922195262011508,
        "epoch": 0.6958666666666666,
        "step": 5219
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.1625959873199463,
        "learning_rate": 0.00019921409730357052,
        "epoch": 0.696,
        "step": 5220
    },
    {
        "loss": 3.0704,
        "grad_norm": 3.434864044189453,
        "learning_rate": 0.00019920620268798397,
        "epoch": 0.6961333333333334,
        "step": 5221
    },
    {
        "loss": 2.571,
        "grad_norm": 3.2015058994293213,
        "learning_rate": 0.0001991982687764825,
        "epoch": 0.6962666666666667,
        "step": 5222
    },
    {
        "loss": 2.1359,
        "grad_norm": 3.125173807144165,
        "learning_rate": 0.00019919029557220874,
        "epoch": 0.6964,
        "step": 5223
    },
    {
        "loss": 2.5163,
        "grad_norm": 3.1746346950531006,
        "learning_rate": 0.00019918228307832093,
        "epoch": 0.6965333333333333,
        "step": 5224
    },
    {
        "loss": 1.9487,
        "grad_norm": 3.976811647415161,
        "learning_rate": 0.00019917423129799284,
        "epoch": 0.6966666666666667,
        "step": 5225
    },
    {
        "loss": 2.5093,
        "grad_norm": 2.605468988418579,
        "learning_rate": 0.0001991661402344138,
        "epoch": 0.6968,
        "step": 5226
    },
    {
        "loss": 2.7433,
        "grad_norm": 2.6370294094085693,
        "learning_rate": 0.00019915800989078872,
        "epoch": 0.6969333333333333,
        "step": 5227
    },
    {
        "loss": 2.6821,
        "grad_norm": 2.967829942703247,
        "learning_rate": 0.00019914984027033804,
        "epoch": 0.6970666666666666,
        "step": 5228
    },
    {
        "loss": 2.346,
        "grad_norm": 3.2300126552581787,
        "learning_rate": 0.0001991416313762978,
        "epoch": 0.6972,
        "step": 5229
    },
    {
        "loss": 2.1141,
        "grad_norm": 4.29328727722168,
        "learning_rate": 0.00019913338321191955,
        "epoch": 0.6973333333333334,
        "step": 5230
    },
    {
        "loss": 2.6879,
        "grad_norm": 2.9852912425994873,
        "learning_rate": 0.00019912509578047042,
        "epoch": 0.6974666666666667,
        "step": 5231
    },
    {
        "loss": 2.6867,
        "grad_norm": 3.2534518241882324,
        "learning_rate": 0.00019911676908523308,
        "epoch": 0.6976,
        "step": 5232
    },
    {
        "loss": 2.3288,
        "grad_norm": 3.7980635166168213,
        "learning_rate": 0.00019910840312950578,
        "epoch": 0.6977333333333333,
        "step": 5233
    },
    {
        "loss": 2.7924,
        "grad_norm": 2.7274162769317627,
        "learning_rate": 0.0001990999979166023,
        "epoch": 0.6978666666666666,
        "step": 5234
    },
    {
        "loss": 1.6901,
        "grad_norm": 4.882199764251709,
        "learning_rate": 0.00019909155344985196,
        "epoch": 0.698,
        "step": 5235
    },
    {
        "loss": 2.2722,
        "grad_norm": 3.1773324012756348,
        "learning_rate": 0.00019908306973259965,
        "epoch": 0.6981333333333334,
        "step": 5236
    },
    {
        "loss": 1.7749,
        "grad_norm": 4.255519390106201,
        "learning_rate": 0.0001990745467682058,
        "epoch": 0.6982666666666667,
        "step": 5237
    },
    {
        "loss": 1.6832,
        "grad_norm": 4.017831325531006,
        "learning_rate": 0.00019906598456004643,
        "epoch": 0.6984,
        "step": 5238
    },
    {
        "loss": 2.5705,
        "grad_norm": 3.3912055492401123,
        "learning_rate": 0.00019905738311151296,
        "epoch": 0.6985333333333333,
        "step": 5239
    },
    {
        "loss": 2.6766,
        "grad_norm": 3.2757046222686768,
        "learning_rate": 0.00019904874242601255,
        "epoch": 0.6986666666666667,
        "step": 5240
    },
    {
        "loss": 1.6635,
        "grad_norm": 3.44834303855896,
        "learning_rate": 0.00019904006250696777,
        "epoch": 0.6988,
        "step": 5241
    },
    {
        "loss": 1.9202,
        "grad_norm": 3.3060765266418457,
        "learning_rate": 0.00019903134335781672,
        "epoch": 0.6989333333333333,
        "step": 5242
    },
    {
        "loss": 2.4112,
        "grad_norm": 3.01057505607605,
        "learning_rate": 0.00019902258498201314,
        "epoch": 0.6990666666666666,
        "step": 5243
    },
    {
        "loss": 2.0623,
        "grad_norm": 3.6868138313293457,
        "learning_rate": 0.00019901378738302624,
        "epoch": 0.6992,
        "step": 5244
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.931518316268921,
        "learning_rate": 0.00019900495056434078,
        "epoch": 0.6993333333333334,
        "step": 5245
    },
    {
        "loss": 1.4166,
        "grad_norm": 4.855060577392578,
        "learning_rate": 0.00019899607452945705,
        "epoch": 0.6994666666666667,
        "step": 5246
    },
    {
        "loss": 2.2996,
        "grad_norm": 3.5510451793670654,
        "learning_rate": 0.00019898715928189087,
        "epoch": 0.6996,
        "step": 5247
    },
    {
        "loss": 2.4719,
        "grad_norm": 3.188114881515503,
        "learning_rate": 0.00019897820482517363,
        "epoch": 0.6997333333333333,
        "step": 5248
    },
    {
        "loss": 2.7637,
        "grad_norm": 4.604680061340332,
        "learning_rate": 0.0001989692111628522,
        "epoch": 0.6998666666666666,
        "step": 5249
    },
    {
        "loss": 2.031,
        "grad_norm": 4.074921131134033,
        "learning_rate": 0.00019896017829848896,
        "epoch": 0.7,
        "step": 5250
    },
    {
        "loss": 2.2956,
        "grad_norm": 3.4018542766571045,
        "learning_rate": 0.00019895110623566192,
        "epoch": 0.7001333333333334,
        "step": 5251
    },
    {
        "loss": 2.6754,
        "grad_norm": 3.4561829566955566,
        "learning_rate": 0.00019894199497796453,
        "epoch": 0.7002666666666667,
        "step": 5252
    },
    {
        "loss": 3.4788,
        "grad_norm": 2.5970449447631836,
        "learning_rate": 0.0001989328445290058,
        "epoch": 0.7004,
        "step": 5253
    },
    {
        "loss": 2.5634,
        "grad_norm": 3.58842396736145,
        "learning_rate": 0.00019892365489241023,
        "epoch": 0.7005333333333333,
        "step": 5254
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.0729124546051025,
        "learning_rate": 0.0001989144260718179,
        "epoch": 0.7006666666666667,
        "step": 5255
    },
    {
        "loss": 2.3515,
        "grad_norm": 1.825929045677185,
        "learning_rate": 0.00019890515807088438,
        "epoch": 0.7008,
        "step": 5256
    },
    {
        "loss": 2.5082,
        "grad_norm": 3.5884881019592285,
        "learning_rate": 0.00019889585089328068,
        "epoch": 0.7009333333333333,
        "step": 5257
    },
    {
        "loss": 2.9873,
        "grad_norm": 2.0567786693573,
        "learning_rate": 0.00019888650454269352,
        "epoch": 0.7010666666666666,
        "step": 5258
    },
    {
        "loss": 2.8073,
        "grad_norm": 2.214416027069092,
        "learning_rate": 0.0001988771190228249,
        "epoch": 0.7012,
        "step": 5259
    },
    {
        "loss": 1.7361,
        "grad_norm": 3.5626914501190186,
        "learning_rate": 0.00019886769433739257,
        "epoch": 0.7013333333333334,
        "step": 5260
    },
    {
        "loss": 2.186,
        "grad_norm": 2.7592430114746094,
        "learning_rate": 0.0001988582304901296,
        "epoch": 0.7014666666666667,
        "step": 5261
    },
    {
        "loss": 0.8939,
        "grad_norm": 4.55682373046875,
        "learning_rate": 0.00019884872748478474,
        "epoch": 0.7016,
        "step": 5262
    },
    {
        "loss": 2.6043,
        "grad_norm": 2.806900978088379,
        "learning_rate": 0.00019883918532512205,
        "epoch": 0.7017333333333333,
        "step": 5263
    },
    {
        "loss": 1.9632,
        "grad_norm": 4.484847545623779,
        "learning_rate": 0.00019882960401492128,
        "epoch": 0.7018666666666666,
        "step": 5264
    },
    {
        "loss": 3.1025,
        "grad_norm": 1.147779107093811,
        "learning_rate": 0.00019881998355797758,
        "epoch": 0.702,
        "step": 5265
    },
    {
        "loss": 2.5668,
        "grad_norm": 2.3095314502716064,
        "learning_rate": 0.00019881032395810172,
        "epoch": 0.7021333333333334,
        "step": 5266
    },
    {
        "loss": 2.0708,
        "grad_norm": 3.467026472091675,
        "learning_rate": 0.0001988006252191198,
        "epoch": 0.7022666666666667,
        "step": 5267
    },
    {
        "loss": 2.6453,
        "grad_norm": 1.6883366107940674,
        "learning_rate": 0.00019879088734487362,
        "epoch": 0.7024,
        "step": 5268
    },
    {
        "loss": 1.5665,
        "grad_norm": 3.850372791290283,
        "learning_rate": 0.00019878111033922032,
        "epoch": 0.7025333333333333,
        "step": 5269
    },
    {
        "loss": 2.752,
        "grad_norm": 3.7215635776519775,
        "learning_rate": 0.00019877129420603265,
        "epoch": 0.7026666666666667,
        "step": 5270
    },
    {
        "loss": 3.0476,
        "grad_norm": 2.278388500213623,
        "learning_rate": 0.00019876143894919876,
        "epoch": 0.7028,
        "step": 5271
    },
    {
        "loss": 2.3351,
        "grad_norm": 2.1342504024505615,
        "learning_rate": 0.0001987515445726224,
        "epoch": 0.7029333333333333,
        "step": 5272
    },
    {
        "loss": 1.4411,
        "grad_norm": 5.858157634735107,
        "learning_rate": 0.00019874161108022275,
        "epoch": 0.7030666666666666,
        "step": 5273
    },
    {
        "loss": 2.8605,
        "grad_norm": 2.9118080139160156,
        "learning_rate": 0.00019873163847593448,
        "epoch": 0.7032,
        "step": 5274
    },
    {
        "loss": 2.0915,
        "grad_norm": 3.730146646499634,
        "learning_rate": 0.0001987216267637078,
        "epoch": 0.7033333333333334,
        "step": 5275
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.6753242015838623,
        "learning_rate": 0.00019871157594750834,
        "epoch": 0.7034666666666667,
        "step": 5276
    },
    {
        "loss": 2.2079,
        "grad_norm": 2.9611940383911133,
        "learning_rate": 0.00019870148603131735,
        "epoch": 0.7036,
        "step": 5277
    },
    {
        "loss": 1.3408,
        "grad_norm": 2.5237741470336914,
        "learning_rate": 0.0001986913570191314,
        "epoch": 0.7037333333333333,
        "step": 5278
    },
    {
        "loss": 2.3424,
        "grad_norm": 2.432420492172241,
        "learning_rate": 0.00019868118891496267,
        "epoch": 0.7038666666666666,
        "step": 5279
    },
    {
        "loss": 2.6012,
        "grad_norm": 2.684026002883911,
        "learning_rate": 0.00019867098172283873,
        "epoch": 0.704,
        "step": 5280
    },
    {
        "loss": 2.3889,
        "grad_norm": 2.349144697189331,
        "learning_rate": 0.00019866073544680274,
        "epoch": 0.7041333333333334,
        "step": 5281
    },
    {
        "loss": 1.9999,
        "grad_norm": 4.106145858764648,
        "learning_rate": 0.00019865045009091325,
        "epoch": 0.7042666666666667,
        "step": 5282
    },
    {
        "loss": 2.6181,
        "grad_norm": 3.0488572120666504,
        "learning_rate": 0.00019864012565924436,
        "epoch": 0.7044,
        "step": 5283
    },
    {
        "loss": 1.6331,
        "grad_norm": 3.379303455352783,
        "learning_rate": 0.00019862976215588555,
        "epoch": 0.7045333333333333,
        "step": 5284
    },
    {
        "loss": 2.1181,
        "grad_norm": 2.3136398792266846,
        "learning_rate": 0.0001986193595849419,
        "epoch": 0.7046666666666667,
        "step": 5285
    },
    {
        "loss": 2.11,
        "grad_norm": 3.9540536403656006,
        "learning_rate": 0.00019860891795053386,
        "epoch": 0.7048,
        "step": 5286
    },
    {
        "loss": 2.0434,
        "grad_norm": 6.057440757751465,
        "learning_rate": 0.00019859843725679745,
        "epoch": 0.7049333333333333,
        "step": 5287
    },
    {
        "loss": 2.5874,
        "grad_norm": 3.237733840942383,
        "learning_rate": 0.00019858791750788405,
        "epoch": 0.7050666666666666,
        "step": 5288
    },
    {
        "loss": 2.5449,
        "grad_norm": 3.680819511413574,
        "learning_rate": 0.00019857735870796063,
        "epoch": 0.7052,
        "step": 5289
    },
    {
        "loss": 2.7224,
        "grad_norm": 2.7525453567504883,
        "learning_rate": 0.00019856676086120952,
        "epoch": 0.7053333333333334,
        "step": 5290
    },
    {
        "loss": 2.0686,
        "grad_norm": 4.666413307189941,
        "learning_rate": 0.00019855612397182853,
        "epoch": 0.7054666666666667,
        "step": 5291
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.470790386199951,
        "learning_rate": 0.00019854544804403105,
        "epoch": 0.7056,
        "step": 5292
    },
    {
        "loss": 2.1385,
        "grad_norm": 3.0939714908599854,
        "learning_rate": 0.00019853473308204583,
        "epoch": 0.7057333333333333,
        "step": 5293
    },
    {
        "loss": 2.1615,
        "grad_norm": 2.8248071670532227,
        "learning_rate": 0.0001985239790901171,
        "epoch": 0.7058666666666666,
        "step": 5294
    },
    {
        "loss": 2.2471,
        "grad_norm": 3.09447979927063,
        "learning_rate": 0.00019851318607250445,
        "epoch": 0.706,
        "step": 5295
    },
    {
        "loss": 2.2684,
        "grad_norm": 3.0405099391937256,
        "learning_rate": 0.0001985023540334832,
        "epoch": 0.7061333333333333,
        "step": 5296
    },
    {
        "loss": 2.8627,
        "grad_norm": 2.63440203666687,
        "learning_rate": 0.00019849148297734385,
        "epoch": 0.7062666666666667,
        "step": 5297
    },
    {
        "loss": 2.4628,
        "grad_norm": 3.063755750656128,
        "learning_rate": 0.0001984805729083925,
        "epoch": 0.7064,
        "step": 5298
    },
    {
        "loss": 1.7325,
        "grad_norm": 2.3858554363250732,
        "learning_rate": 0.00019846962383095066,
        "epoch": 0.7065333333333333,
        "step": 5299
    },
    {
        "loss": 2.0451,
        "grad_norm": 2.8956832885742188,
        "learning_rate": 0.00019845863574935533,
        "epoch": 0.7066666666666667,
        "step": 5300
    },
    {
        "loss": 1.7846,
        "grad_norm": 5.407737731933594,
        "learning_rate": 0.00019844760866795884,
        "epoch": 0.7068,
        "step": 5301
    },
    {
        "loss": 2.8586,
        "grad_norm": 4.315145492553711,
        "learning_rate": 0.00019843654259112909,
        "epoch": 0.7069333333333333,
        "step": 5302
    },
    {
        "loss": 1.6393,
        "grad_norm": 1.8583312034606934,
        "learning_rate": 0.00019842543752324944,
        "epoch": 0.7070666666666666,
        "step": 5303
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.9752660989761353,
        "learning_rate": 0.0001984142934687186,
        "epoch": 0.7072,
        "step": 5304
    },
    {
        "loss": 1.8838,
        "grad_norm": 3.4446628093719482,
        "learning_rate": 0.00019840311043195077,
        "epoch": 0.7073333333333334,
        "step": 5305
    },
    {
        "loss": 1.8191,
        "grad_norm": 3.0446057319641113,
        "learning_rate": 0.00019839188841737563,
        "epoch": 0.7074666666666667,
        "step": 5306
    },
    {
        "loss": 2.364,
        "grad_norm": 2.734506607055664,
        "learning_rate": 0.0001983806274294382,
        "epoch": 0.7076,
        "step": 5307
    },
    {
        "loss": 2.3116,
        "grad_norm": 2.7572085857391357,
        "learning_rate": 0.00019836932747259903,
        "epoch": 0.7077333333333333,
        "step": 5308
    },
    {
        "loss": 2.5766,
        "grad_norm": 3.678379774093628,
        "learning_rate": 0.00019835798855133405,
        "epoch": 0.7078666666666666,
        "step": 5309
    },
    {
        "loss": 2.7121,
        "grad_norm": 5.684007167816162,
        "learning_rate": 0.00019834661067013465,
        "epoch": 0.708,
        "step": 5310
    },
    {
        "loss": 2.1534,
        "grad_norm": 3.4830081462860107,
        "learning_rate": 0.00019833519383350768,
        "epoch": 0.7081333333333333,
        "step": 5311
    },
    {
        "loss": 2.9795,
        "grad_norm": 2.3335554599761963,
        "learning_rate": 0.00019832373804597536,
        "epoch": 0.7082666666666667,
        "step": 5312
    },
    {
        "loss": 1.6352,
        "grad_norm": 6.723128795623779,
        "learning_rate": 0.00019831224331207534,
        "epoch": 0.7084,
        "step": 5313
    },
    {
        "loss": 2.8808,
        "grad_norm": 3.8715193271636963,
        "learning_rate": 0.0001983007096363608,
        "epoch": 0.7085333333333333,
        "step": 5314
    },
    {
        "loss": 1.2507,
        "grad_norm": 4.0381388664245605,
        "learning_rate": 0.0001982891370234002,
        "epoch": 0.7086666666666667,
        "step": 5315
    },
    {
        "loss": 2.2354,
        "grad_norm": 3.8983423709869385,
        "learning_rate": 0.00019827752547777751,
        "epoch": 0.7088,
        "step": 5316
    },
    {
        "loss": 1.4528,
        "grad_norm": 4.942281723022461,
        "learning_rate": 0.00019826587500409212,
        "epoch": 0.7089333333333333,
        "step": 5317
    },
    {
        "loss": 2.3422,
        "grad_norm": 3.207416534423828,
        "learning_rate": 0.0001982541856069588,
        "epoch": 0.7090666666666666,
        "step": 5318
    },
    {
        "loss": 2.71,
        "grad_norm": 2.471163272857666,
        "learning_rate": 0.00019824245729100777,
        "epoch": 0.7092,
        "step": 5319
    },
    {
        "loss": 1.2194,
        "grad_norm": 4.22675085067749,
        "learning_rate": 0.0001982306900608846,
        "epoch": 0.7093333333333334,
        "step": 5320
    },
    {
        "loss": 1.1855,
        "grad_norm": 3.7742767333984375,
        "learning_rate": 0.00019821888392125046,
        "epoch": 0.7094666666666667,
        "step": 5321
    },
    {
        "loss": 2.849,
        "grad_norm": 2.243940591812134,
        "learning_rate": 0.0001982070388767817,
        "epoch": 0.7096,
        "step": 5322
    },
    {
        "loss": 2.7476,
        "grad_norm": 2.7994537353515625,
        "learning_rate": 0.0001981951549321702,
        "epoch": 0.7097333333333333,
        "step": 5323
    },
    {
        "loss": 1.6418,
        "grad_norm": 2.8402044773101807,
        "learning_rate": 0.00019818323209212327,
        "epoch": 0.7098666666666666,
        "step": 5324
    },
    {
        "loss": 2.6003,
        "grad_norm": 3.0408101081848145,
        "learning_rate": 0.00019817127036136353,
        "epoch": 0.71,
        "step": 5325
    },
    {
        "loss": 2.7184,
        "grad_norm": 3.2696778774261475,
        "learning_rate": 0.00019815926974462908,
        "epoch": 0.7101333333333333,
        "step": 5326
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.9286162853240967,
        "learning_rate": 0.00019814723024667342,
        "epoch": 0.7102666666666667,
        "step": 5327
    },
    {
        "loss": 1.2147,
        "grad_norm": 2.674863576889038,
        "learning_rate": 0.00019813515187226547,
        "epoch": 0.7104,
        "step": 5328
    },
    {
        "loss": 1.065,
        "grad_norm": 3.1223931312561035,
        "learning_rate": 0.00019812303462618945,
        "epoch": 0.7105333333333334,
        "step": 5329
    },
    {
        "loss": 2.1036,
        "grad_norm": 3.7791855335235596,
        "learning_rate": 0.00019811087851324505,
        "epoch": 0.7106666666666667,
        "step": 5330
    },
    {
        "loss": 2.3728,
        "grad_norm": 2.5326507091522217,
        "learning_rate": 0.00019809868353824737,
        "epoch": 0.7108,
        "step": 5331
    },
    {
        "loss": 2.8444,
        "grad_norm": 3.7760777473449707,
        "learning_rate": 0.0001980864497060269,
        "epoch": 0.7109333333333333,
        "step": 5332
    },
    {
        "loss": 2.0264,
        "grad_norm": 3.925767660140991,
        "learning_rate": 0.00019807417702142946,
        "epoch": 0.7110666666666666,
        "step": 5333
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.5113117694854736,
        "learning_rate": 0.00019806186548931634,
        "epoch": 0.7112,
        "step": 5334
    },
    {
        "loss": 2.5024,
        "grad_norm": 3.410524606704712,
        "learning_rate": 0.00019804951511456412,
        "epoch": 0.7113333333333334,
        "step": 5335
    },
    {
        "loss": 2.9572,
        "grad_norm": 2.6958494186401367,
        "learning_rate": 0.00019803712590206493,
        "epoch": 0.7114666666666667,
        "step": 5336
    },
    {
        "loss": 2.9996,
        "grad_norm": 2.684644937515259,
        "learning_rate": 0.0001980246978567261,
        "epoch": 0.7116,
        "step": 5337
    },
    {
        "loss": 1.3721,
        "grad_norm": 3.9176056385040283,
        "learning_rate": 0.0001980122309834704,
        "epoch": 0.7117333333333333,
        "step": 5338
    },
    {
        "loss": 2.2012,
        "grad_norm": 3.2249016761779785,
        "learning_rate": 0.00019799972528723606,
        "epoch": 0.7118666666666666,
        "step": 5339
    },
    {
        "loss": 1.9601,
        "grad_norm": 3.56681489944458,
        "learning_rate": 0.0001979871807729766,
        "epoch": 0.712,
        "step": 5340
    },
    {
        "loss": 3.0938,
        "grad_norm": 2.830387592315674,
        "learning_rate": 0.000197974597445661,
        "epoch": 0.7121333333333333,
        "step": 5341
    },
    {
        "loss": 2.1545,
        "grad_norm": 3.334550619125366,
        "learning_rate": 0.00019796197531027346,
        "epoch": 0.7122666666666667,
        "step": 5342
    },
    {
        "loss": 2.2097,
        "grad_norm": 3.25795841217041,
        "learning_rate": 0.0001979493143718137,
        "epoch": 0.7124,
        "step": 5343
    },
    {
        "loss": 2.6218,
        "grad_norm": 3.2924511432647705,
        "learning_rate": 0.0001979366146352968,
        "epoch": 0.7125333333333334,
        "step": 5344
    },
    {
        "loss": 3.415,
        "grad_norm": 3.415813684463501,
        "learning_rate": 0.0001979238761057531,
        "epoch": 0.7126666666666667,
        "step": 5345
    },
    {
        "loss": 2.3582,
        "grad_norm": 3.124591112136841,
        "learning_rate": 0.00019791109878822843,
        "epoch": 0.7128,
        "step": 5346
    },
    {
        "loss": 1.5703,
        "grad_norm": 4.736402988433838,
        "learning_rate": 0.0001978982826877839,
        "epoch": 0.7129333333333333,
        "step": 5347
    },
    {
        "loss": 2.5921,
        "grad_norm": 2.7015926837921143,
        "learning_rate": 0.00019788542780949602,
        "epoch": 0.7130666666666666,
        "step": 5348
    },
    {
        "loss": 2.8438,
        "grad_norm": 4.4196367263793945,
        "learning_rate": 0.00019787253415845665,
        "epoch": 0.7132,
        "step": 5349
    },
    {
        "loss": 1.313,
        "grad_norm": 3.768338680267334,
        "learning_rate": 0.00019785960173977296,
        "epoch": 0.7133333333333334,
        "step": 5350
    },
    {
        "loss": 2.0155,
        "grad_norm": 4.23786735534668,
        "learning_rate": 0.00019784663055856766,
        "epoch": 0.7134666666666667,
        "step": 5351
    },
    {
        "loss": 2.6829,
        "grad_norm": 1.723668098449707,
        "learning_rate": 0.0001978336206199785,
        "epoch": 0.7136,
        "step": 5352
    },
    {
        "loss": 3.0608,
        "grad_norm": 4.368020057678223,
        "learning_rate": 0.0001978205719291589,
        "epoch": 0.7137333333333333,
        "step": 5353
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.4096202850341797,
        "learning_rate": 0.00019780748449127743,
        "epoch": 0.7138666666666666,
        "step": 5354
    },
    {
        "loss": 2.4177,
        "grad_norm": 3.1036577224731445,
        "learning_rate": 0.0001977943583115181,
        "epoch": 0.714,
        "step": 5355
    },
    {
        "loss": 2.4292,
        "grad_norm": 2.796013832092285,
        "learning_rate": 0.0001977811933950802,
        "epoch": 0.7141333333333333,
        "step": 5356
    },
    {
        "loss": 1.7002,
        "grad_norm": 3.4130442142486572,
        "learning_rate": 0.00019776798974717843,
        "epoch": 0.7142666666666667,
        "step": 5357
    },
    {
        "loss": 2.1054,
        "grad_norm": 4.563450336456299,
        "learning_rate": 0.00019775474737304278,
        "epoch": 0.7144,
        "step": 5358
    },
    {
        "loss": 3.2286,
        "grad_norm": 4.015263557434082,
        "learning_rate": 0.00019774146627791862,
        "epoch": 0.7145333333333334,
        "step": 5359
    },
    {
        "loss": 2.208,
        "grad_norm": 3.485684633255005,
        "learning_rate": 0.0001977281464670666,
        "epoch": 0.7146666666666667,
        "step": 5360
    },
    {
        "loss": 2.1277,
        "grad_norm": 3.4906256198883057,
        "learning_rate": 0.00019771478794576277,
        "epoch": 0.7148,
        "step": 5361
    },
    {
        "loss": 1.9839,
        "grad_norm": 3.131438970565796,
        "learning_rate": 0.00019770139071929846,
        "epoch": 0.7149333333333333,
        "step": 5362
    },
    {
        "loss": 2.1889,
        "grad_norm": 3.474365472793579,
        "learning_rate": 0.0001976879547929804,
        "epoch": 0.7150666666666666,
        "step": 5363
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.0376405715942383,
        "learning_rate": 0.00019767448017213058,
        "epoch": 0.7152,
        "step": 5364
    },
    {
        "loss": 2.5817,
        "grad_norm": 1.8660922050476074,
        "learning_rate": 0.00019766096686208635,
        "epoch": 0.7153333333333334,
        "step": 5365
    },
    {
        "loss": 1.8623,
        "grad_norm": 3.487706184387207,
        "learning_rate": 0.00019764741486820038,
        "epoch": 0.7154666666666667,
        "step": 5366
    },
    {
        "loss": 2.4418,
        "grad_norm": 3.9761624336242676,
        "learning_rate": 0.00019763382419584066,
        "epoch": 0.7156,
        "step": 5367
    },
    {
        "loss": 2.3116,
        "grad_norm": 2.98930287361145,
        "learning_rate": 0.00019762019485039046,
        "epoch": 0.7157333333333333,
        "step": 5368
    },
    {
        "loss": 2.4703,
        "grad_norm": 3.905871868133545,
        "learning_rate": 0.00019760652683724846,
        "epoch": 0.7158666666666667,
        "step": 5369
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.280128002166748,
        "learning_rate": 0.00019759282016182858,
        "epoch": 0.716,
        "step": 5370
    },
    {
        "loss": 2.739,
        "grad_norm": 3.157874584197998,
        "learning_rate": 0.00019757907482956014,
        "epoch": 0.7161333333333333,
        "step": 5371
    },
    {
        "loss": 2.7494,
        "grad_norm": 3.039740562438965,
        "learning_rate": 0.00019756529084588765,
        "epoch": 0.7162666666666667,
        "step": 5372
    },
    {
        "loss": 2.9929,
        "grad_norm": 3.9327540397644043,
        "learning_rate": 0.00019755146821627098,
        "epoch": 0.7164,
        "step": 5373
    },
    {
        "loss": 2.8673,
        "grad_norm": 3.5780208110809326,
        "learning_rate": 0.00019753760694618538,
        "epoch": 0.7165333333333334,
        "step": 5374
    },
    {
        "loss": 1.6318,
        "grad_norm": 3.4153411388397217,
        "learning_rate": 0.0001975237070411213,
        "epoch": 0.7166666666666667,
        "step": 5375
    },
    {
        "loss": 2.2981,
        "grad_norm": 4.740988731384277,
        "learning_rate": 0.00019750976850658458,
        "epoch": 0.7168,
        "step": 5376
    },
    {
        "loss": 1.9363,
        "grad_norm": 3.442272663116455,
        "learning_rate": 0.00019749579134809627,
        "epoch": 0.7169333333333333,
        "step": 5377
    },
    {
        "loss": 2.6224,
        "grad_norm": 3.3154804706573486,
        "learning_rate": 0.00019748177557119285,
        "epoch": 0.7170666666666666,
        "step": 5378
    },
    {
        "loss": 2.93,
        "grad_norm": 3.483497142791748,
        "learning_rate": 0.00019746772118142588,
        "epoch": 0.7172,
        "step": 5379
    },
    {
        "loss": 1.103,
        "grad_norm": 4.248104572296143,
        "learning_rate": 0.00019745362818436251,
        "epoch": 0.7173333333333334,
        "step": 5380
    },
    {
        "loss": 3.193,
        "grad_norm": 3.0291733741760254,
        "learning_rate": 0.00019743949658558493,
        "epoch": 0.7174666666666667,
        "step": 5381
    },
    {
        "loss": 2.6456,
        "grad_norm": 4.138942241668701,
        "learning_rate": 0.00019742532639069075,
        "epoch": 0.7176,
        "step": 5382
    },
    {
        "loss": 2.413,
        "grad_norm": 2.3437981605529785,
        "learning_rate": 0.00019741111760529278,
        "epoch": 0.7177333333333333,
        "step": 5383
    },
    {
        "loss": 2.127,
        "grad_norm": 4.5422539710998535,
        "learning_rate": 0.00019739687023501924,
        "epoch": 0.7178666666666667,
        "step": 5384
    },
    {
        "loss": 2.6717,
        "grad_norm": 6.084017753601074,
        "learning_rate": 0.00019738258428551351,
        "epoch": 0.718,
        "step": 5385
    },
    {
        "loss": 2.1859,
        "grad_norm": 4.042247772216797,
        "learning_rate": 0.0001973682597624343,
        "epoch": 0.7181333333333333,
        "step": 5386
    },
    {
        "loss": 2.5491,
        "grad_norm": 4.407663822174072,
        "learning_rate": 0.0001973538966714557,
        "epoch": 0.7182666666666667,
        "step": 5387
    },
    {
        "loss": 2.7654,
        "grad_norm": 3.0669901371002197,
        "learning_rate": 0.00019733949501826684,
        "epoch": 0.7184,
        "step": 5388
    },
    {
        "loss": 2.4864,
        "grad_norm": 2.7525625228881836,
        "learning_rate": 0.00019732505480857233,
        "epoch": 0.7185333333333334,
        "step": 5389
    },
    {
        "loss": 2.1658,
        "grad_norm": 4.443499565124512,
        "learning_rate": 0.000197310576048092,
        "epoch": 0.7186666666666667,
        "step": 5390
    },
    {
        "loss": 2.606,
        "grad_norm": 1.9713772535324097,
        "learning_rate": 0.0001972960587425609,
        "epoch": 0.7188,
        "step": 5391
    },
    {
        "loss": 1.276,
        "grad_norm": 4.432734966278076,
        "learning_rate": 0.00019728150289772942,
        "epoch": 0.7189333333333333,
        "step": 5392
    },
    {
        "loss": 1.5793,
        "grad_norm": 1.9719566106796265,
        "learning_rate": 0.00019726690851936318,
        "epoch": 0.7190666666666666,
        "step": 5393
    },
    {
        "loss": 2.0626,
        "grad_norm": 4.251842021942139,
        "learning_rate": 0.00019725227561324303,
        "epoch": 0.7192,
        "step": 5394
    },
    {
        "loss": 2.2974,
        "grad_norm": 4.213525772094727,
        "learning_rate": 0.00019723760418516512,
        "epoch": 0.7193333333333334,
        "step": 5395
    },
    {
        "loss": 2.4935,
        "grad_norm": 5.344780445098877,
        "learning_rate": 0.00019722289424094086,
        "epoch": 0.7194666666666667,
        "step": 5396
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.358396530151367,
        "learning_rate": 0.00019720814578639694,
        "epoch": 0.7196,
        "step": 5397
    },
    {
        "loss": 2.148,
        "grad_norm": 2.2387049198150635,
        "learning_rate": 0.00019719335882737525,
        "epoch": 0.7197333333333333,
        "step": 5398
    },
    {
        "loss": 3.0397,
        "grad_norm": 4.306294918060303,
        "learning_rate": 0.0001971785333697329,
        "epoch": 0.7198666666666667,
        "step": 5399
    },
    {
        "loss": 1.5479,
        "grad_norm": 3.461293935775757,
        "learning_rate": 0.0001971636694193424,
        "epoch": 0.72,
        "step": 5400
    },
    {
        "loss": 2.6202,
        "grad_norm": 3.1306145191192627,
        "learning_rate": 0.00019714876698209138,
        "epoch": 0.7201333333333333,
        "step": 5401
    },
    {
        "loss": 2.1723,
        "grad_norm": 4.247923374176025,
        "learning_rate": 0.0001971338260638827,
        "epoch": 0.7202666666666667,
        "step": 5402
    },
    {
        "loss": 2.0379,
        "grad_norm": 2.388394355773926,
        "learning_rate": 0.00019711884667063454,
        "epoch": 0.7204,
        "step": 5403
    },
    {
        "loss": 1.5781,
        "grad_norm": 1.9058467149734497,
        "learning_rate": 0.00019710382880828027,
        "epoch": 0.7205333333333334,
        "step": 5404
    },
    {
        "loss": 2.7136,
        "grad_norm": 4.4561920166015625,
        "learning_rate": 0.00019708877248276856,
        "epoch": 0.7206666666666667,
        "step": 5405
    },
    {
        "loss": 2.1725,
        "grad_norm": 2.7810449600219727,
        "learning_rate": 0.00019707367770006325,
        "epoch": 0.7208,
        "step": 5406
    },
    {
        "loss": 2.6025,
        "grad_norm": 4.499794960021973,
        "learning_rate": 0.00019705854446614343,
        "epoch": 0.7209333333333333,
        "step": 5407
    },
    {
        "loss": 2.208,
        "grad_norm": 2.3497631549835205,
        "learning_rate": 0.00019704337278700343,
        "epoch": 0.7210666666666666,
        "step": 5408
    },
    {
        "loss": 2.8487,
        "grad_norm": 3.3458828926086426,
        "learning_rate": 0.0001970281626686528,
        "epoch": 0.7212,
        "step": 5409
    },
    {
        "loss": 2.4203,
        "grad_norm": 1.9308010339736938,
        "learning_rate": 0.00019701291411711633,
        "epoch": 0.7213333333333334,
        "step": 5410
    },
    {
        "loss": 2.6361,
        "grad_norm": 2.824659824371338,
        "learning_rate": 0.00019699762713843398,
        "epoch": 0.7214666666666667,
        "step": 5411
    },
    {
        "loss": 0.8745,
        "grad_norm": 3.2819201946258545,
        "learning_rate": 0.00019698230173866105,
        "epoch": 0.7216,
        "step": 5412
    },
    {
        "loss": 2.8527,
        "grad_norm": 3.462538719177246,
        "learning_rate": 0.0001969669379238679,
        "epoch": 0.7217333333333333,
        "step": 5413
    },
    {
        "loss": 1.962,
        "grad_norm": 3.415984869003296,
        "learning_rate": 0.00019695153570014026,
        "epoch": 0.7218666666666667,
        "step": 5414
    },
    {
        "loss": 2.7093,
        "grad_norm": 1.5850766897201538,
        "learning_rate": 0.000196936095073579,
        "epoch": 0.722,
        "step": 5415
    },
    {
        "loss": 1.3539,
        "grad_norm": 2.935955286026001,
        "learning_rate": 0.00019692061605030013,
        "epoch": 0.7221333333333333,
        "step": 5416
    },
    {
        "loss": 1.6851,
        "grad_norm": 3.272480010986328,
        "learning_rate": 0.00019690509863643502,
        "epoch": 0.7222666666666666,
        "step": 5417
    },
    {
        "loss": 2.281,
        "grad_norm": 2.713942527770996,
        "learning_rate": 0.00019688954283813014,
        "epoch": 0.7224,
        "step": 5418
    },
    {
        "loss": 2.2475,
        "grad_norm": 3.0669610500335693,
        "learning_rate": 0.00019687394866154724,
        "epoch": 0.7225333333333334,
        "step": 5419
    },
    {
        "loss": 2.5286,
        "grad_norm": 3.110415458679199,
        "learning_rate": 0.0001968583161128631,
        "epoch": 0.7226666666666667,
        "step": 5420
    },
    {
        "loss": 1.6692,
        "grad_norm": 4.372063636779785,
        "learning_rate": 0.00019684264519826997,
        "epoch": 0.7228,
        "step": 5421
    },
    {
        "loss": 2.845,
        "grad_norm": 3.2146661281585693,
        "learning_rate": 0.00019682693592397508,
        "epoch": 0.7229333333333333,
        "step": 5422
    },
    {
        "loss": 2.8582,
        "grad_norm": 2.606452703475952,
        "learning_rate": 0.00019681118829620092,
        "epoch": 0.7230666666666666,
        "step": 5423
    },
    {
        "loss": 1.709,
        "grad_norm": 4.761568546295166,
        "learning_rate": 0.00019679540232118525,
        "epoch": 0.7232,
        "step": 5424
    },
    {
        "loss": 2.4896,
        "grad_norm": 2.5904014110565186,
        "learning_rate": 0.00019677957800518086,
        "epoch": 0.7233333333333334,
        "step": 5425
    },
    {
        "loss": 2.9334,
        "grad_norm": 2.9227688312530518,
        "learning_rate": 0.00019676371535445587,
        "epoch": 0.7234666666666667,
        "step": 5426
    },
    {
        "loss": 1.3661,
        "grad_norm": 3.6448183059692383,
        "learning_rate": 0.00019674781437529354,
        "epoch": 0.7236,
        "step": 5427
    },
    {
        "loss": 2.7141,
        "grad_norm": 2.827308177947998,
        "learning_rate": 0.00019673187507399223,
        "epoch": 0.7237333333333333,
        "step": 5428
    },
    {
        "loss": 0.8808,
        "grad_norm": 3.453977346420288,
        "learning_rate": 0.00019671589745686558,
        "epoch": 0.7238666666666667,
        "step": 5429
    },
    {
        "loss": 2.3233,
        "grad_norm": 3.1900575160980225,
        "learning_rate": 0.00019669988153024243,
        "epoch": 0.724,
        "step": 5430
    },
    {
        "loss": 2.2675,
        "grad_norm": 4.128564834594727,
        "learning_rate": 0.0001966838273004667,
        "epoch": 0.7241333333333333,
        "step": 5431
    },
    {
        "loss": 2.0712,
        "grad_norm": 1.4227421283721924,
        "learning_rate": 0.00019666773477389752,
        "epoch": 0.7242666666666666,
        "step": 5432
    },
    {
        "loss": 2.6328,
        "grad_norm": 3.7934179306030273,
        "learning_rate": 0.00019665160395690925,
        "epoch": 0.7244,
        "step": 5433
    },
    {
        "loss": 2.3917,
        "grad_norm": 4.525143146514893,
        "learning_rate": 0.00019663543485589128,
        "epoch": 0.7245333333333334,
        "step": 5434
    },
    {
        "loss": 1.9941,
        "grad_norm": 3.4381959438323975,
        "learning_rate": 0.00019661922747724834,
        "epoch": 0.7246666666666667,
        "step": 5435
    },
    {
        "loss": 2.3794,
        "grad_norm": 2.8400092124938965,
        "learning_rate": 0.00019660298182740013,
        "epoch": 0.7248,
        "step": 5436
    },
    {
        "loss": 1.6653,
        "grad_norm": 3.659590005874634,
        "learning_rate": 0.0001965866979127817,
        "epoch": 0.7249333333333333,
        "step": 5437
    },
    {
        "loss": 2.3853,
        "grad_norm": 2.8617963790893555,
        "learning_rate": 0.00019657037573984302,
        "epoch": 0.7250666666666666,
        "step": 5438
    },
    {
        "loss": 2.3487,
        "grad_norm": 2.460902690887451,
        "learning_rate": 0.00019655401531504954,
        "epoch": 0.7252,
        "step": 5439
    },
    {
        "loss": 1.5363,
        "grad_norm": 5.807404518127441,
        "learning_rate": 0.00019653761664488152,
        "epoch": 0.7253333333333334,
        "step": 5440
    },
    {
        "loss": 1.914,
        "grad_norm": 4.46648645401001,
        "learning_rate": 0.00019652117973583468,
        "epoch": 0.7254666666666667,
        "step": 5441
    },
    {
        "loss": 1.16,
        "grad_norm": 2.9349842071533203,
        "learning_rate": 0.00019650470459441963,
        "epoch": 0.7256,
        "step": 5442
    },
    {
        "loss": 2.2697,
        "grad_norm": 2.9083383083343506,
        "learning_rate": 0.00019648819122716223,
        "epoch": 0.7257333333333333,
        "step": 5443
    },
    {
        "loss": 2.0717,
        "grad_norm": 3.1221368312835693,
        "learning_rate": 0.00019647163964060357,
        "epoch": 0.7258666666666667,
        "step": 5444
    },
    {
        "loss": 1.9965,
        "grad_norm": 3.9375357627868652,
        "learning_rate": 0.00019645504984129968,
        "epoch": 0.726,
        "step": 5445
    },
    {
        "loss": 2.5821,
        "grad_norm": 2.7966248989105225,
        "learning_rate": 0.0001964384218358219,
        "epoch": 0.7261333333333333,
        "step": 5446
    },
    {
        "loss": 1.718,
        "grad_norm": 4.256903648376465,
        "learning_rate": 0.00019642175563075662,
        "epoch": 0.7262666666666666,
        "step": 5447
    },
    {
        "loss": 2.4405,
        "grad_norm": 3.4566805362701416,
        "learning_rate": 0.0001964050512327054,
        "epoch": 0.7264,
        "step": 5448
    },
    {
        "loss": 2.5283,
        "grad_norm": 2.9984350204467773,
        "learning_rate": 0.00019638830864828489,
        "epoch": 0.7265333333333334,
        "step": 5449
    },
    {
        "loss": 1.9211,
        "grad_norm": 3.058133840560913,
        "learning_rate": 0.00019637152788412685,
        "epoch": 0.7266666666666667,
        "step": 5450
    },
    {
        "loss": 2.2673,
        "grad_norm": 2.7234389781951904,
        "learning_rate": 0.0001963547089468783,
        "epoch": 0.7268,
        "step": 5451
    },
    {
        "loss": 2.3276,
        "grad_norm": 3.2438318729400635,
        "learning_rate": 0.00019633785184320116,
        "epoch": 0.7269333333333333,
        "step": 5452
    },
    {
        "loss": 2.5436,
        "grad_norm": 3.033917188644409,
        "learning_rate": 0.00019632095657977269,
        "epoch": 0.7270666666666666,
        "step": 5453
    },
    {
        "loss": 2.0693,
        "grad_norm": 2.6035993099212646,
        "learning_rate": 0.00019630402316328507,
        "epoch": 0.7272,
        "step": 5454
    },
    {
        "loss": 2.4789,
        "grad_norm": 2.5776054859161377,
        "learning_rate": 0.00019628705160044577,
        "epoch": 0.7273333333333334,
        "step": 5455
    },
    {
        "loss": 2.2193,
        "grad_norm": 3.42775297164917,
        "learning_rate": 0.0001962700418979772,
        "epoch": 0.7274666666666667,
        "step": 5456
    },
    {
        "loss": 1.0963,
        "grad_norm": 6.302575588226318,
        "learning_rate": 0.000196252994062617,
        "epoch": 0.7276,
        "step": 5457
    },
    {
        "loss": 1.4378,
        "grad_norm": 4.79200553894043,
        "learning_rate": 0.00019623590810111794,
        "epoch": 0.7277333333333333,
        "step": 5458
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.4247264862060547,
        "learning_rate": 0.00019621878402024771,
        "epoch": 0.7278666666666667,
        "step": 5459
    },
    {
        "loss": 1.646,
        "grad_norm": 3.333428144454956,
        "learning_rate": 0.0001962016218267893,
        "epoch": 0.728,
        "step": 5460
    },
    {
        "loss": 2.2846,
        "grad_norm": 3.4244229793548584,
        "learning_rate": 0.00019618442152754067,
        "epoch": 0.7281333333333333,
        "step": 5461
    },
    {
        "loss": 1.6381,
        "grad_norm": 3.725095272064209,
        "learning_rate": 0.00019616718312931496,
        "epoch": 0.7282666666666666,
        "step": 5462
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.6434147357940674,
        "learning_rate": 0.00019614990663894023,
        "epoch": 0.7284,
        "step": 5463
    },
    {
        "loss": 0.8065,
        "grad_norm": 3.017139434814453,
        "learning_rate": 0.00019613259206325995,
        "epoch": 0.7285333333333334,
        "step": 5464
    },
    {
        "loss": 2.4597,
        "grad_norm": 3.345172882080078,
        "learning_rate": 0.0001961152394091324,
        "epoch": 0.7286666666666667,
        "step": 5465
    },
    {
        "loss": 2.5836,
        "grad_norm": 3.939298391342163,
        "learning_rate": 0.00019609784868343096,
        "epoch": 0.7288,
        "step": 5466
    },
    {
        "loss": 2.4105,
        "grad_norm": 3.4797415733337402,
        "learning_rate": 0.00019608041989304425,
        "epoch": 0.7289333333333333,
        "step": 5467
    },
    {
        "loss": 1.7101,
        "grad_norm": 3.988924741744995,
        "learning_rate": 0.00019606295304487584,
        "epoch": 0.7290666666666666,
        "step": 5468
    },
    {
        "loss": 3.1472,
        "grad_norm": 2.9812519550323486,
        "learning_rate": 0.00019604544814584438,
        "epoch": 0.7292,
        "step": 5469
    },
    {
        "loss": 2.6077,
        "grad_norm": 3.4247324466705322,
        "learning_rate": 0.00019602790520288364,
        "epoch": 0.7293333333333333,
        "step": 5470
    },
    {
        "loss": 3.0038,
        "grad_norm": 4.059844017028809,
        "learning_rate": 0.00019601032422294248,
        "epoch": 0.7294666666666667,
        "step": 5471
    },
    {
        "loss": 3.0834,
        "grad_norm": 2.2805216312408447,
        "learning_rate": 0.00019599270521298466,
        "epoch": 0.7296,
        "step": 5472
    },
    {
        "loss": 2.9565,
        "grad_norm": 2.516181230545044,
        "learning_rate": 0.0001959750481799893,
        "epoch": 0.7297333333333333,
        "step": 5473
    },
    {
        "loss": 1.6195,
        "grad_norm": 4.5391082763671875,
        "learning_rate": 0.00019595735313095028,
        "epoch": 0.7298666666666667,
        "step": 5474
    },
    {
        "loss": 2.0718,
        "grad_norm": 4.7110981941223145,
        "learning_rate": 0.0001959396200728768,
        "epoch": 0.73,
        "step": 5475
    },
    {
        "loss": 1.7244,
        "grad_norm": 4.078737258911133,
        "learning_rate": 0.00019592184901279282,
        "epoch": 0.7301333333333333,
        "step": 5476
    },
    {
        "loss": 2.6277,
        "grad_norm": 3.458430051803589,
        "learning_rate": 0.00019590403995773764,
        "epoch": 0.7302666666666666,
        "step": 5477
    },
    {
        "loss": 2.5317,
        "grad_norm": 3.594647169113159,
        "learning_rate": 0.00019588619291476542,
        "epoch": 0.7304,
        "step": 5478
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.496457099914551,
        "learning_rate": 0.0001958683078909455,
        "epoch": 0.7305333333333334,
        "step": 5479
    },
    {
        "loss": 1.8443,
        "grad_norm": 4.188097953796387,
        "learning_rate": 0.00019585038489336213,
        "epoch": 0.7306666666666667,
        "step": 5480
    },
    {
        "loss": 1.5146,
        "grad_norm": 4.837912082672119,
        "learning_rate": 0.00019583242392911472,
        "epoch": 0.7308,
        "step": 5481
    },
    {
        "loss": 2.6665,
        "grad_norm": 2.363192081451416,
        "learning_rate": 0.0001958144250053176,
        "epoch": 0.7309333333333333,
        "step": 5482
    },
    {
        "loss": 2.7483,
        "grad_norm": 2.8195993900299072,
        "learning_rate": 0.00019579638812910032,
        "epoch": 0.7310666666666666,
        "step": 5483
    },
    {
        "loss": 2.1771,
        "grad_norm": 3.0204758644104004,
        "learning_rate": 0.00019577831330760726,
        "epoch": 0.7312,
        "step": 5484
    },
    {
        "loss": 2.0181,
        "grad_norm": 3.037527084350586,
        "learning_rate": 0.00019576020054799795,
        "epoch": 0.7313333333333333,
        "step": 5485
    },
    {
        "loss": 2.7105,
        "grad_norm": 2.843055486679077,
        "learning_rate": 0.0001957420498574469,
        "epoch": 0.7314666666666667,
        "step": 5486
    },
    {
        "loss": 2.9133,
        "grad_norm": 2.303821563720703,
        "learning_rate": 0.0001957238612431437,
        "epoch": 0.7316,
        "step": 5487
    },
    {
        "loss": 2.7751,
        "grad_norm": 4.439110279083252,
        "learning_rate": 0.0001957056347122928,
        "epoch": 0.7317333333333333,
        "step": 5488
    },
    {
        "loss": 2.1415,
        "grad_norm": 5.212821960449219,
        "learning_rate": 0.00019568737027211394,
        "epoch": 0.7318666666666667,
        "step": 5489
    },
    {
        "loss": 1.9295,
        "grad_norm": 3.265521287918091,
        "learning_rate": 0.00019566906792984167,
        "epoch": 0.732,
        "step": 5490
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.793010711669922,
        "learning_rate": 0.00019565072769272562,
        "epoch": 0.7321333333333333,
        "step": 5491
    },
    {
        "loss": 1.6025,
        "grad_norm": 3.371959924697876,
        "learning_rate": 0.0001956323495680304,
        "epoch": 0.7322666666666666,
        "step": 5492
    },
    {
        "loss": 2.0408,
        "grad_norm": 3.7177927494049072,
        "learning_rate": 0.00019561393356303563,
        "epoch": 0.7324,
        "step": 5493
    },
    {
        "loss": 2.3175,
        "grad_norm": 3.7333195209503174,
        "learning_rate": 0.000195595479685036,
        "epoch": 0.7325333333333334,
        "step": 5494
    },
    {
        "loss": 2.347,
        "grad_norm": 3.6508948802948,
        "learning_rate": 0.00019557698794134116,
        "epoch": 0.7326666666666667,
        "step": 5495
    },
    {
        "loss": 2.7981,
        "grad_norm": 3.0568623542785645,
        "learning_rate": 0.00019555845833927574,
        "epoch": 0.7328,
        "step": 5496
    },
    {
        "loss": 2.0683,
        "grad_norm": 4.033321380615234,
        "learning_rate": 0.00019553989088617928,
        "epoch": 0.7329333333333333,
        "step": 5497
    },
    {
        "loss": 2.9625,
        "grad_norm": 2.688739061355591,
        "learning_rate": 0.00019552128558940655,
        "epoch": 0.7330666666666666,
        "step": 5498
    },
    {
        "loss": 2.133,
        "grad_norm": 3.6433098316192627,
        "learning_rate": 0.0001955026424563271,
        "epoch": 0.7332,
        "step": 5499
    },
    {
        "loss": 1.0287,
        "grad_norm": 3.718510389328003,
        "learning_rate": 0.00019548396149432558,
        "epoch": 0.7333333333333333,
        "step": 5500
    },
    {
        "loss": 2.3907,
        "grad_norm": 2.772773504257202,
        "learning_rate": 0.00019546524271080154,
        "epoch": 0.7334666666666667,
        "step": 5501
    },
    {
        "loss": 2.9137,
        "grad_norm": 2.309107780456543,
        "learning_rate": 0.0001954464861131696,
        "epoch": 0.7336,
        "step": 5502
    },
    {
        "loss": 2.7826,
        "grad_norm": 4.339632511138916,
        "learning_rate": 0.00019542769170885924,
        "epoch": 0.7337333333333333,
        "step": 5503
    },
    {
        "loss": 2.3709,
        "grad_norm": 3.3798913955688477,
        "learning_rate": 0.00019540885950531503,
        "epoch": 0.7338666666666667,
        "step": 5504
    },
    {
        "loss": 0.8197,
        "grad_norm": 2.6542646884918213,
        "learning_rate": 0.0001953899895099965,
        "epoch": 0.734,
        "step": 5505
    },
    {
        "loss": 1.9056,
        "grad_norm": 5.395632266998291,
        "learning_rate": 0.0001953710817303781,
        "epoch": 0.7341333333333333,
        "step": 5506
    },
    {
        "loss": 2.8121,
        "grad_norm": 2.910675525665283,
        "learning_rate": 0.00019535213617394924,
        "epoch": 0.7342666666666666,
        "step": 5507
    },
    {
        "loss": 2.8784,
        "grad_norm": 4.446147918701172,
        "learning_rate": 0.00019533315284821437,
        "epoch": 0.7344,
        "step": 5508
    },
    {
        "loss": 1.4379,
        "grad_norm": 5.4075093269348145,
        "learning_rate": 0.00019531413176069275,
        "epoch": 0.7345333333333334,
        "step": 5509
    },
    {
        "loss": 2.3923,
        "grad_norm": 2.436523914337158,
        "learning_rate": 0.00019529507291891882,
        "epoch": 0.7346666666666667,
        "step": 5510
    },
    {
        "loss": 1.6479,
        "grad_norm": 2.484865665435791,
        "learning_rate": 0.0001952759763304418,
        "epoch": 0.7348,
        "step": 5511
    },
    {
        "loss": 2.706,
        "grad_norm": 5.135366439819336,
        "learning_rate": 0.00019525684200282588,
        "epoch": 0.7349333333333333,
        "step": 5512
    },
    {
        "loss": 2.842,
        "grad_norm": 2.7319557666778564,
        "learning_rate": 0.00019523766994365027,
        "epoch": 0.7350666666666666,
        "step": 5513
    },
    {
        "loss": 2.8631,
        "grad_norm": 2.970228672027588,
        "learning_rate": 0.00019521846016050908,
        "epoch": 0.7352,
        "step": 5514
    },
    {
        "loss": 2.5159,
        "grad_norm": 4.303011417388916,
        "learning_rate": 0.00019519921266101137,
        "epoch": 0.7353333333333333,
        "step": 5515
    },
    {
        "loss": 2.6149,
        "grad_norm": 3.776590347290039,
        "learning_rate": 0.00019517992745278116,
        "epoch": 0.7354666666666667,
        "step": 5516
    },
    {
        "loss": 2.6413,
        "grad_norm": 4.381814956665039,
        "learning_rate": 0.00019516060454345734,
        "epoch": 0.7356,
        "step": 5517
    },
    {
        "loss": 2.4168,
        "grad_norm": 3.1536967754364014,
        "learning_rate": 0.00019514124394069384,
        "epoch": 0.7357333333333334,
        "step": 5518
    },
    {
        "loss": 2.6217,
        "grad_norm": 1.6548727750778198,
        "learning_rate": 0.00019512184565215944,
        "epoch": 0.7358666666666667,
        "step": 5519
    },
    {
        "loss": 2.5135,
        "grad_norm": 3.100615978240967,
        "learning_rate": 0.0001951024096855378,
        "epoch": 0.736,
        "step": 5520
    },
    {
        "loss": 2.4347,
        "grad_norm": 2.1855177879333496,
        "learning_rate": 0.00019508293604852768,
        "epoch": 0.7361333333333333,
        "step": 5521
    },
    {
        "loss": 2.5304,
        "grad_norm": 2.082814931869507,
        "learning_rate": 0.00019506342474884253,
        "epoch": 0.7362666666666666,
        "step": 5522
    },
    {
        "loss": 2.0455,
        "grad_norm": 3.074295997619629,
        "learning_rate": 0.00019504387579421102,
        "epoch": 0.7364,
        "step": 5523
    },
    {
        "loss": 2.2338,
        "grad_norm": 3.056016445159912,
        "learning_rate": 0.0001950242891923764,
        "epoch": 0.7365333333333334,
        "step": 5524
    },
    {
        "loss": 2.552,
        "grad_norm": 2.926541328430176,
        "learning_rate": 0.00019500466495109704,
        "epoch": 0.7366666666666667,
        "step": 5525
    },
    {
        "loss": 2.7395,
        "grad_norm": 2.548089027404785,
        "learning_rate": 0.00019498500307814617,
        "epoch": 0.7368,
        "step": 5526
    },
    {
        "loss": 1.6614,
        "grad_norm": 3.920701742172241,
        "learning_rate": 0.00019496530358131199,
        "epoch": 0.7369333333333333,
        "step": 5527
    },
    {
        "loss": 2.3209,
        "grad_norm": 3.498521566390991,
        "learning_rate": 0.0001949455664683974,
        "epoch": 0.7370666666666666,
        "step": 5528
    },
    {
        "loss": 2.5303,
        "grad_norm": 3.4289653301239014,
        "learning_rate": 0.00019492579174722042,
        "epoch": 0.7372,
        "step": 5529
    },
    {
        "loss": 2.5097,
        "grad_norm": 3.3204503059387207,
        "learning_rate": 0.0001949059794256139,
        "epoch": 0.7373333333333333,
        "step": 5530
    },
    {
        "loss": 2.314,
        "grad_norm": 4.8608880043029785,
        "learning_rate": 0.00019488612951142553,
        "epoch": 0.7374666666666667,
        "step": 5531
    },
    {
        "loss": 1.7439,
        "grad_norm": 2.525477647781372,
        "learning_rate": 0.00019486624201251796,
        "epoch": 0.7376,
        "step": 5532
    },
    {
        "loss": 1.7955,
        "grad_norm": 5.080546855926514,
        "learning_rate": 0.00019484631693676864,
        "epoch": 0.7377333333333334,
        "step": 5533
    },
    {
        "loss": 2.3851,
        "grad_norm": 3.9530787467956543,
        "learning_rate": 0.00019482635429207007,
        "epoch": 0.7378666666666667,
        "step": 5534
    },
    {
        "loss": 2.5247,
        "grad_norm": 3.166604518890381,
        "learning_rate": 0.0001948063540863294,
        "epoch": 0.738,
        "step": 5535
    },
    {
        "loss": 2.8769,
        "grad_norm": 1.8734720945358276,
        "learning_rate": 0.00019478631632746887,
        "epoch": 0.7381333333333333,
        "step": 5536
    },
    {
        "loss": 2.4428,
        "grad_norm": 2.5035970211029053,
        "learning_rate": 0.00019476624102342545,
        "epoch": 0.7382666666666666,
        "step": 5537
    },
    {
        "loss": 1.9971,
        "grad_norm": 4.2498459815979,
        "learning_rate": 0.00019474612818215105,
        "epoch": 0.7384,
        "step": 5538
    },
    {
        "loss": 1.2697,
        "grad_norm": 5.581680774688721,
        "learning_rate": 0.00019472597781161242,
        "epoch": 0.7385333333333334,
        "step": 5539
    },
    {
        "loss": 2.06,
        "grad_norm": 2.934354305267334,
        "learning_rate": 0.0001947057899197912,
        "epoch": 0.7386666666666667,
        "step": 5540
    },
    {
        "loss": 0.781,
        "grad_norm": 2.946125030517578,
        "learning_rate": 0.0001946855645146839,
        "epoch": 0.7388,
        "step": 5541
    },
    {
        "loss": 1.5818,
        "grad_norm": 3.1967766284942627,
        "learning_rate": 0.0001946653016043019,
        "epoch": 0.7389333333333333,
        "step": 5542
    },
    {
        "loss": 2.0035,
        "grad_norm": 3.318995237350464,
        "learning_rate": 0.00019464500119667136,
        "epoch": 0.7390666666666666,
        "step": 5543
    },
    {
        "loss": 2.0865,
        "grad_norm": 2.5916860103607178,
        "learning_rate": 0.00019462466329983334,
        "epoch": 0.7392,
        "step": 5544
    },
    {
        "loss": 2.4787,
        "grad_norm": 2.3561360836029053,
        "learning_rate": 0.00019460428792184375,
        "epoch": 0.7393333333333333,
        "step": 5545
    },
    {
        "loss": 2.5479,
        "grad_norm": 3.5643222332000732,
        "learning_rate": 0.00019458387507077335,
        "epoch": 0.7394666666666667,
        "step": 5546
    },
    {
        "loss": 2.661,
        "grad_norm": 2.520803213119507,
        "learning_rate": 0.00019456342475470775,
        "epoch": 0.7396,
        "step": 5547
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.4703030586242676,
        "learning_rate": 0.00019454293698174742,
        "epoch": 0.7397333333333334,
        "step": 5548
    },
    {
        "loss": 2.5563,
        "grad_norm": 4.740079879760742,
        "learning_rate": 0.00019452241176000755,
        "epoch": 0.7398666666666667,
        "step": 5549
    },
    {
        "loss": 2.5341,
        "grad_norm": 3.774442434310913,
        "learning_rate": 0.0001945018490976184,
        "epoch": 0.74,
        "step": 5550
    },
    {
        "loss": 2.7047,
        "grad_norm": 2.9814870357513428,
        "learning_rate": 0.00019448124900272473,
        "epoch": 0.7401333333333333,
        "step": 5551
    },
    {
        "loss": 1.7636,
        "grad_norm": 4.521519184112549,
        "learning_rate": 0.0001944606114834864,
        "epoch": 0.7402666666666666,
        "step": 5552
    },
    {
        "loss": 2.4301,
        "grad_norm": 2.508880615234375,
        "learning_rate": 0.00019443993654807802,
        "epoch": 0.7404,
        "step": 5553
    },
    {
        "loss": 3.0938,
        "grad_norm": 4.250849723815918,
        "learning_rate": 0.00019441922420468898,
        "epoch": 0.7405333333333334,
        "step": 5554
    },
    {
        "loss": 0.7434,
        "grad_norm": 5.896584510803223,
        "learning_rate": 0.00019439847446152351,
        "epoch": 0.7406666666666667,
        "step": 5555
    },
    {
        "loss": 0.8341,
        "grad_norm": 2.6381044387817383,
        "learning_rate": 0.00019437768732680062,
        "epoch": 0.7408,
        "step": 5556
    },
    {
        "loss": 1.1927,
        "grad_norm": 5.007302284240723,
        "learning_rate": 0.0001943568628087542,
        "epoch": 0.7409333333333333,
        "step": 5557
    },
    {
        "loss": 2.1282,
        "grad_norm": 3.272144317626953,
        "learning_rate": 0.00019433600091563294,
        "epoch": 0.7410666666666667,
        "step": 5558
    },
    {
        "loss": 3.0011,
        "grad_norm": 4.127776622772217,
        "learning_rate": 0.00019431510165570027,
        "epoch": 0.7412,
        "step": 5559
    },
    {
        "loss": 2.1244,
        "grad_norm": 3.951906204223633,
        "learning_rate": 0.00019429416503723444,
        "epoch": 0.7413333333333333,
        "step": 5560
    },
    {
        "loss": 3.2273,
        "grad_norm": 5.093678951263428,
        "learning_rate": 0.00019427319106852858,
        "epoch": 0.7414666666666667,
        "step": 5561
    },
    {
        "loss": 2.5674,
        "grad_norm": 2.443230390548706,
        "learning_rate": 0.00019425217975789044,
        "epoch": 0.7416,
        "step": 5562
    },
    {
        "loss": 3.1407,
        "grad_norm": 2.4294281005859375,
        "learning_rate": 0.0001942311311136428,
        "epoch": 0.7417333333333334,
        "step": 5563
    },
    {
        "loss": 2.7075,
        "grad_norm": 3.16489577293396,
        "learning_rate": 0.00019421004514412303,
        "epoch": 0.7418666666666667,
        "step": 5564
    },
    {
        "loss": 1.4962,
        "grad_norm": 3.6869616508483887,
        "learning_rate": 0.00019418892185768335,
        "epoch": 0.742,
        "step": 5565
    },
    {
        "loss": 2.6182,
        "grad_norm": 3.19946551322937,
        "learning_rate": 0.0001941677612626908,
        "epoch": 0.7421333333333333,
        "step": 5566
    },
    {
        "loss": 2.2877,
        "grad_norm": 2.237833023071289,
        "learning_rate": 0.00019414656336752713,
        "epoch": 0.7422666666666666,
        "step": 5567
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.751466989517212,
        "learning_rate": 0.0001941253281805889,
        "epoch": 0.7424,
        "step": 5568
    },
    {
        "loss": 2.6622,
        "grad_norm": 3.7136919498443604,
        "learning_rate": 0.00019410405571028747,
        "epoch": 0.7425333333333334,
        "step": 5569
    },
    {
        "loss": 1.9517,
        "grad_norm": 2.7435989379882812,
        "learning_rate": 0.00019408274596504894,
        "epoch": 0.7426666666666667,
        "step": 5570
    },
    {
        "loss": 1.0875,
        "grad_norm": 8.270101547241211,
        "learning_rate": 0.00019406139895331413,
        "epoch": 0.7428,
        "step": 5571
    },
    {
        "loss": 2.0417,
        "grad_norm": 3.700000762939453,
        "learning_rate": 0.00019404001468353868,
        "epoch": 0.7429333333333333,
        "step": 5572
    },
    {
        "loss": 1.8381,
        "grad_norm": 2.8000869750976562,
        "learning_rate": 0.000194018593164193,
        "epoch": 0.7430666666666667,
        "step": 5573
    },
    {
        "loss": 2.5236,
        "grad_norm": 3.65619158744812,
        "learning_rate": 0.0001939971344037622,
        "epoch": 0.7432,
        "step": 5574
    },
    {
        "loss": 2.7881,
        "grad_norm": 3.687312364578247,
        "learning_rate": 0.00019397563841074615,
        "epoch": 0.7433333333333333,
        "step": 5575
    },
    {
        "loss": 2.4984,
        "grad_norm": 3.5234434604644775,
        "learning_rate": 0.00019395410519365956,
        "epoch": 0.7434666666666667,
        "step": 5576
    },
    {
        "loss": 2.4707,
        "grad_norm": 2.729497194290161,
        "learning_rate": 0.00019393253476103175,
        "epoch": 0.7436,
        "step": 5577
    },
    {
        "loss": 2.8128,
        "grad_norm": 2.7906785011291504,
        "learning_rate": 0.00019391092712140687,
        "epoch": 0.7437333333333334,
        "step": 5578
    },
    {
        "loss": 1.5173,
        "grad_norm": 5.911507606506348,
        "learning_rate": 0.0001938892822833437,
        "epoch": 0.7438666666666667,
        "step": 5579
    },
    {
        "loss": 0.7775,
        "grad_norm": 4.362612247467041,
        "learning_rate": 0.00019386760025541598,
        "epoch": 0.744,
        "step": 5580
    },
    {
        "loss": 2.7888,
        "grad_norm": 2.9323360919952393,
        "learning_rate": 0.00019384588104621187,
        "epoch": 0.7441333333333333,
        "step": 5581
    },
    {
        "loss": 2.2639,
        "grad_norm": 4.496501922607422,
        "learning_rate": 0.00019382412466433454,
        "epoch": 0.7442666666666666,
        "step": 5582
    },
    {
        "loss": 2.1521,
        "grad_norm": 3.378910541534424,
        "learning_rate": 0.00019380233111840177,
        "epoch": 0.7444,
        "step": 5583
    },
    {
        "loss": 2.571,
        "grad_norm": 3.307377576828003,
        "learning_rate": 0.00019378050041704597,
        "epoch": 0.7445333333333334,
        "step": 5584
    },
    {
        "loss": 2.0492,
        "grad_norm": 3.5385818481445312,
        "learning_rate": 0.00019375863256891442,
        "epoch": 0.7446666666666667,
        "step": 5585
    },
    {
        "loss": 2.4275,
        "grad_norm": 3.6271331310272217,
        "learning_rate": 0.00019373672758266904,
        "epoch": 0.7448,
        "step": 5586
    },
    {
        "loss": 1.7944,
        "grad_norm": 4.278644561767578,
        "learning_rate": 0.00019371478546698648,
        "epoch": 0.7449333333333333,
        "step": 5587
    },
    {
        "loss": 2.2642,
        "grad_norm": 2.795984983444214,
        "learning_rate": 0.00019369280623055804,
        "epoch": 0.7450666666666667,
        "step": 5588
    },
    {
        "loss": 2.1307,
        "grad_norm": 3.4715781211853027,
        "learning_rate": 0.00019367078988208984,
        "epoch": 0.7452,
        "step": 5589
    },
    {
        "loss": 2.4458,
        "grad_norm": 3.194850206375122,
        "learning_rate": 0.00019364873643030255,
        "epoch": 0.7453333333333333,
        "step": 5590
    },
    {
        "loss": 2.6671,
        "grad_norm": 2.822002410888672,
        "learning_rate": 0.00019362664588393167,
        "epoch": 0.7454666666666667,
        "step": 5591
    },
    {
        "loss": 1.8261,
        "grad_norm": 2.6661264896392822,
        "learning_rate": 0.0001936045182517273,
        "epoch": 0.7456,
        "step": 5592
    },
    {
        "loss": 2.461,
        "grad_norm": 3.2208781242370605,
        "learning_rate": 0.00019358235354245438,
        "epoch": 0.7457333333333334,
        "step": 5593
    },
    {
        "loss": 2.2248,
        "grad_norm": 4.2086992263793945,
        "learning_rate": 0.0001935601517648923,
        "epoch": 0.7458666666666667,
        "step": 5594
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.6140520572662354,
        "learning_rate": 0.00019353791292783533,
        "epoch": 0.746,
        "step": 5595
    },
    {
        "loss": 2.401,
        "grad_norm": 3.0549192428588867,
        "learning_rate": 0.00019351563704009231,
        "epoch": 0.7461333333333333,
        "step": 5596
    },
    {
        "loss": 2.416,
        "grad_norm": 3.0158746242523193,
        "learning_rate": 0.0001934933241104868,
        "epoch": 0.7462666666666666,
        "step": 5597
    },
    {
        "loss": 1.6031,
        "grad_norm": 2.8210976123809814,
        "learning_rate": 0.00019347097414785706,
        "epoch": 0.7464,
        "step": 5598
    },
    {
        "loss": 2.7692,
        "grad_norm": 2.679064989089966,
        "learning_rate": 0.00019344858716105595,
        "epoch": 0.7465333333333334,
        "step": 5599
    },
    {
        "loss": 2.115,
        "grad_norm": 3.8067400455474854,
        "learning_rate": 0.00019342616315895105,
        "epoch": 0.7466666666666667,
        "step": 5600
    },
    {
        "loss": 2.4523,
        "grad_norm": 4.755658149719238,
        "learning_rate": 0.0001934037021504246,
        "epoch": 0.7468,
        "step": 5601
    },
    {
        "loss": 2.4394,
        "grad_norm": 2.640638828277588,
        "learning_rate": 0.00019338120414437343,
        "epoch": 0.7469333333333333,
        "step": 5602
    },
    {
        "loss": 2.5583,
        "grad_norm": 3.189136505126953,
        "learning_rate": 0.00019335866914970914,
        "epoch": 0.7470666666666667,
        "step": 5603
    },
    {
        "loss": 1.3425,
        "grad_norm": 5.242618083953857,
        "learning_rate": 0.00019333609717535788,
        "epoch": 0.7472,
        "step": 5604
    },
    {
        "loss": 2.3218,
        "grad_norm": 4.167300224304199,
        "learning_rate": 0.00019331348823026053,
        "epoch": 0.7473333333333333,
        "step": 5605
    },
    {
        "loss": 2.2976,
        "grad_norm": 3.449693441390991,
        "learning_rate": 0.00019329084232337244,
        "epoch": 0.7474666666666666,
        "step": 5606
    },
    {
        "loss": 2.6421,
        "grad_norm": 4.0855021476745605,
        "learning_rate": 0.00019326815946366387,
        "epoch": 0.7476,
        "step": 5607
    },
    {
        "loss": 3.1124,
        "grad_norm": 3.944532632827759,
        "learning_rate": 0.00019324543966011955,
        "epoch": 0.7477333333333334,
        "step": 5608
    },
    {
        "loss": 2.6881,
        "grad_norm": 1.9836283922195435,
        "learning_rate": 0.00019322268292173885,
        "epoch": 0.7478666666666667,
        "step": 5609
    },
    {
        "loss": 2.6114,
        "grad_norm": 4.193796157836914,
        "learning_rate": 0.0001931998892575358,
        "epoch": 0.748,
        "step": 5610
    },
    {
        "loss": 1.4436,
        "grad_norm": 4.668746471405029,
        "learning_rate": 0.00019317705867653905,
        "epoch": 0.7481333333333333,
        "step": 5611
    },
    {
        "loss": 2.3211,
        "grad_norm": 3.606451988220215,
        "learning_rate": 0.00019315419118779182,
        "epoch": 0.7482666666666666,
        "step": 5612
    },
    {
        "loss": 1.3162,
        "grad_norm": 3.2841243743896484,
        "learning_rate": 0.00019313128680035208,
        "epoch": 0.7484,
        "step": 5613
    },
    {
        "loss": 3.2086,
        "grad_norm": 2.102080821990967,
        "learning_rate": 0.00019310834552329235,
        "epoch": 0.7485333333333334,
        "step": 5614
    },
    {
        "loss": 1.9432,
        "grad_norm": 3.032954692840576,
        "learning_rate": 0.00019308536736569959,
        "epoch": 0.7486666666666667,
        "step": 5615
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.0292770862579346,
        "learning_rate": 0.0001930623523366757,
        "epoch": 0.7488,
        "step": 5616
    },
    {
        "loss": 3.0199,
        "grad_norm": 2.171419620513916,
        "learning_rate": 0.00019303930044533692,
        "epoch": 0.7489333333333333,
        "step": 5617
    },
    {
        "loss": 2.643,
        "grad_norm": 4.670053958892822,
        "learning_rate": 0.00019301621170081428,
        "epoch": 0.7490666666666667,
        "step": 5618
    },
    {
        "loss": 3.0126,
        "grad_norm": 2.203648567199707,
        "learning_rate": 0.00019299308611225318,
        "epoch": 0.7492,
        "step": 5619
    },
    {
        "loss": 2.1883,
        "grad_norm": 3.682067632675171,
        "learning_rate": 0.00019296992368881383,
        "epoch": 0.7493333333333333,
        "step": 5620
    },
    {
        "loss": 2.4812,
        "grad_norm": 5.637845039367676,
        "learning_rate": 0.0001929467244396709,
        "epoch": 0.7494666666666666,
        "step": 5621
    },
    {
        "loss": 2.3046,
        "grad_norm": 3.110074281692505,
        "learning_rate": 0.0001929234883740137,
        "epoch": 0.7496,
        "step": 5622
    },
    {
        "loss": 3.1699,
        "grad_norm": 2.4926209449768066,
        "learning_rate": 0.00019290021550104615,
        "epoch": 0.7497333333333334,
        "step": 5623
    },
    {
        "loss": 2.5669,
        "grad_norm": 2.509364604949951,
        "learning_rate": 0.00019287690582998668,
        "epoch": 0.7498666666666667,
        "step": 5624
    },
    {
        "loss": 2.2161,
        "grad_norm": 2.850156307220459,
        "learning_rate": 0.00019285355937006835,
        "epoch": 0.75,
        "step": 5625
    },
    {
        "loss": 1.9052,
        "grad_norm": 3.862368106842041,
        "learning_rate": 0.00019283017613053876,
        "epoch": 0.7501333333333333,
        "step": 5626
    },
    {
        "loss": 2.6926,
        "grad_norm": 2.581462860107422,
        "learning_rate": 0.00019280675612066007,
        "epoch": 0.7502666666666666,
        "step": 5627
    },
    {
        "loss": 2.4063,
        "grad_norm": 2.921851873397827,
        "learning_rate": 0.00019278329934970908,
        "epoch": 0.7504,
        "step": 5628
    },
    {
        "loss": 2.923,
        "grad_norm": 2.9236278533935547,
        "learning_rate": 0.00019275980582697706,
        "epoch": 0.7505333333333334,
        "step": 5629
    },
    {
        "loss": 2.1635,
        "grad_norm": 2.0975379943847656,
        "learning_rate": 0.0001927362755617699,
        "epoch": 0.7506666666666667,
        "step": 5630
    },
    {
        "loss": 2.6168,
        "grad_norm": 2.222864866256714,
        "learning_rate": 0.00019271270856340795,
        "epoch": 0.7508,
        "step": 5631
    },
    {
        "loss": 1.9211,
        "grad_norm": 3.766707181930542,
        "learning_rate": 0.00019268910484122626,
        "epoch": 0.7509333333333333,
        "step": 5632
    },
    {
        "loss": 2.2779,
        "grad_norm": 3.6443350315093994,
        "learning_rate": 0.0001926654644045743,
        "epoch": 0.7510666666666667,
        "step": 5633
    },
    {
        "loss": 2.0973,
        "grad_norm": 3.528867244720459,
        "learning_rate": 0.00019264178726281612,
        "epoch": 0.7512,
        "step": 5634
    },
    {
        "loss": 2.1622,
        "grad_norm": 3.5140323638916016,
        "learning_rate": 0.0001926180734253304,
        "epoch": 0.7513333333333333,
        "step": 5635
    },
    {
        "loss": 2.0195,
        "grad_norm": 3.2575135231018066,
        "learning_rate": 0.00019259432290151014,
        "epoch": 0.7514666666666666,
        "step": 5636
    },
    {
        "loss": 2.9728,
        "grad_norm": 2.3697924613952637,
        "learning_rate": 0.00019257053570076314,
        "epoch": 0.7516,
        "step": 5637
    },
    {
        "loss": 1.8257,
        "grad_norm": 3.8879692554473877,
        "learning_rate": 0.00019254671183251146,
        "epoch": 0.7517333333333334,
        "step": 5638
    },
    {
        "loss": 2.4643,
        "grad_norm": 3.0683393478393555,
        "learning_rate": 0.0001925228513061919,
        "epoch": 0.7518666666666667,
        "step": 5639
    },
    {
        "loss": 2.882,
        "grad_norm": 2.186546802520752,
        "learning_rate": 0.00019249895413125565,
        "epoch": 0.752,
        "step": 5640
    },
    {
        "loss": 1.9807,
        "grad_norm": 2.7231826782226562,
        "learning_rate": 0.00019247502031716852,
        "epoch": 0.7521333333333333,
        "step": 5641
    },
    {
        "loss": 2.3885,
        "grad_norm": 4.295309066772461,
        "learning_rate": 0.00019245104987341072,
        "epoch": 0.7522666666666666,
        "step": 5642
    },
    {
        "loss": 2.317,
        "grad_norm": 3.763984441757202,
        "learning_rate": 0.00019242704280947708,
        "epoch": 0.7524,
        "step": 5643
    },
    {
        "loss": 2.6869,
        "grad_norm": 3.064770221710205,
        "learning_rate": 0.0001924029991348768,
        "epoch": 0.7525333333333334,
        "step": 5644
    },
    {
        "loss": 3.1236,
        "grad_norm": 2.5238165855407715,
        "learning_rate": 0.00019237891885913374,
        "epoch": 0.7526666666666667,
        "step": 5645
    },
    {
        "loss": 1.5772,
        "grad_norm": 3.6703736782073975,
        "learning_rate": 0.00019235480199178615,
        "epoch": 0.7528,
        "step": 5646
    },
    {
        "loss": 2.1815,
        "grad_norm": 3.5493433475494385,
        "learning_rate": 0.00019233064854238678,
        "epoch": 0.7529333333333333,
        "step": 5647
    },
    {
        "loss": 2.1081,
        "grad_norm": 3.3918497562408447,
        "learning_rate": 0.00019230645852050295,
        "epoch": 0.7530666666666667,
        "step": 5648
    },
    {
        "loss": 1.9816,
        "grad_norm": 4.029748439788818,
        "learning_rate": 0.00019228223193571632,
        "epoch": 0.7532,
        "step": 5649
    },
    {
        "loss": 2.7686,
        "grad_norm": 2.2477643489837646,
        "learning_rate": 0.0001922579687976232,
        "epoch": 0.7533333333333333,
        "step": 5650
    },
    {
        "loss": 1.6116,
        "grad_norm": 3.0701441764831543,
        "learning_rate": 0.00019223366911583426,
        "epoch": 0.7534666666666666,
        "step": 5651
    },
    {
        "loss": 2.4468,
        "grad_norm": 2.9864892959594727,
        "learning_rate": 0.00019220933289997475,
        "epoch": 0.7536,
        "step": 5652
    },
    {
        "loss": 2.2095,
        "grad_norm": 3.703507423400879,
        "learning_rate": 0.00019218496015968424,
        "epoch": 0.7537333333333334,
        "step": 5653
    },
    {
        "loss": 1.6248,
        "grad_norm": 5.0021209716796875,
        "learning_rate": 0.00019216055090461693,
        "epoch": 0.7538666666666667,
        "step": 5654
    },
    {
        "loss": 0.9631,
        "grad_norm": 2.607477903366089,
        "learning_rate": 0.00019213610514444134,
        "epoch": 0.754,
        "step": 5655
    },
    {
        "loss": 2.2731,
        "grad_norm": 3.2521657943725586,
        "learning_rate": 0.00019211162288884057,
        "epoch": 0.7541333333333333,
        "step": 5656
    },
    {
        "loss": 2.0731,
        "grad_norm": 3.621701717376709,
        "learning_rate": 0.0001920871041475121,
        "epoch": 0.7542666666666666,
        "step": 5657
    },
    {
        "loss": 2.0777,
        "grad_norm": 2.848792314529419,
        "learning_rate": 0.00019206254893016795,
        "epoch": 0.7544,
        "step": 5658
    },
    {
        "loss": 2.5743,
        "grad_norm": 3.0024752616882324,
        "learning_rate": 0.00019203795724653442,
        "epoch": 0.7545333333333333,
        "step": 5659
    },
    {
        "loss": 1.4723,
        "grad_norm": 6.965159893035889,
        "learning_rate": 0.00019201332910635243,
        "epoch": 0.7546666666666667,
        "step": 5660
    },
    {
        "loss": 2.2844,
        "grad_norm": 4.630101680755615,
        "learning_rate": 0.00019198866451937727,
        "epoch": 0.7548,
        "step": 5661
    },
    {
        "loss": 2.4672,
        "grad_norm": 2.733964681625366,
        "learning_rate": 0.00019196396349537866,
        "epoch": 0.7549333333333333,
        "step": 5662
    },
    {
        "loss": 2.8941,
        "grad_norm": 2.438903570175171,
        "learning_rate": 0.0001919392260441407,
        "epoch": 0.7550666666666667,
        "step": 5663
    },
    {
        "loss": 2.2279,
        "grad_norm": 3.8724300861358643,
        "learning_rate": 0.00019191445217546206,
        "epoch": 0.7552,
        "step": 5664
    },
    {
        "loss": 2.0111,
        "grad_norm": 2.9551665782928467,
        "learning_rate": 0.00019188964189915567,
        "epoch": 0.7553333333333333,
        "step": 5665
    },
    {
        "loss": 2.3934,
        "grad_norm": 4.308999061584473,
        "learning_rate": 0.0001918647952250491,
        "epoch": 0.7554666666666666,
        "step": 5666
    },
    {
        "loss": 3.0352,
        "grad_norm": 4.418224334716797,
        "learning_rate": 0.00019183991216298408,
        "epoch": 0.7556,
        "step": 5667
    },
    {
        "loss": 2.0222,
        "grad_norm": 3.4095568656921387,
        "learning_rate": 0.00019181499272281696,
        "epoch": 0.7557333333333334,
        "step": 5668
    },
    {
        "loss": 1.9393,
        "grad_norm": 4.098886489868164,
        "learning_rate": 0.0001917900369144183,
        "epoch": 0.7558666666666667,
        "step": 5669
    },
    {
        "loss": 1.8141,
        "grad_norm": 3.41084623336792,
        "learning_rate": 0.0001917650447476733,
        "epoch": 0.756,
        "step": 5670
    },
    {
        "loss": 2.7703,
        "grad_norm": 4.253940105438232,
        "learning_rate": 0.00019174001623248135,
        "epoch": 0.7561333333333333,
        "step": 5671
    },
    {
        "loss": 2.7602,
        "grad_norm": 3.3366010189056396,
        "learning_rate": 0.0001917149513787564,
        "epoch": 0.7562666666666666,
        "step": 5672
    },
    {
        "loss": 2.4898,
        "grad_norm": 2.8108882904052734,
        "learning_rate": 0.0001916898501964267,
        "epoch": 0.7564,
        "step": 5673
    },
    {
        "loss": 2.0079,
        "grad_norm": 3.524968147277832,
        "learning_rate": 0.0001916647126954349,
        "epoch": 0.7565333333333333,
        "step": 5674
    },
    {
        "loss": 2.0378,
        "grad_norm": 3.728825569152832,
        "learning_rate": 0.00019163953888573807,
        "epoch": 0.7566666666666667,
        "step": 5675
    },
    {
        "loss": 3.4541,
        "grad_norm": 3.561807870864868,
        "learning_rate": 0.00019161432877730762,
        "epoch": 0.7568,
        "step": 5676
    },
    {
        "loss": 2.5595,
        "grad_norm": 3.9671144485473633,
        "learning_rate": 0.0001915890823801294,
        "epoch": 0.7569333333333333,
        "step": 5677
    },
    {
        "loss": 2.5486,
        "grad_norm": 2.990326166152954,
        "learning_rate": 0.00019156379970420356,
        "epoch": 0.7570666666666667,
        "step": 5678
    },
    {
        "loss": 2.4602,
        "grad_norm": 4.664349555969238,
        "learning_rate": 0.00019153848075954466,
        "epoch": 0.7572,
        "step": 5679
    },
    {
        "loss": 2.1415,
        "grad_norm": 3.229279041290283,
        "learning_rate": 0.00019151312555618162,
        "epoch": 0.7573333333333333,
        "step": 5680
    },
    {
        "loss": 2.9068,
        "grad_norm": 2.8627803325653076,
        "learning_rate": 0.0001914877341041577,
        "epoch": 0.7574666666666666,
        "step": 5681
    },
    {
        "loss": 1.6287,
        "grad_norm": 3.626293897628784,
        "learning_rate": 0.0001914623064135306,
        "epoch": 0.7576,
        "step": 5682
    },
    {
        "loss": 2.391,
        "grad_norm": 2.723966360092163,
        "learning_rate": 0.00019143684249437227,
        "epoch": 0.7577333333333334,
        "step": 5683
    },
    {
        "loss": 2.5758,
        "grad_norm": 3.420426845550537,
        "learning_rate": 0.00019141134235676902,
        "epoch": 0.7578666666666667,
        "step": 5684
    },
    {
        "loss": 2.469,
        "grad_norm": 3.464747667312622,
        "learning_rate": 0.00019138580601082167,
        "epoch": 0.758,
        "step": 5685
    },
    {
        "loss": 2.6786,
        "grad_norm": 3.08197021484375,
        "learning_rate": 0.00019136023346664512,
        "epoch": 0.7581333333333333,
        "step": 5686
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.5269222259521484,
        "learning_rate": 0.00019133462473436882,
        "epoch": 0.7582666666666666,
        "step": 5687
    },
    {
        "loss": 2.741,
        "grad_norm": 2.963503837585449,
        "learning_rate": 0.0001913089798241364,
        "epoch": 0.7584,
        "step": 5688
    },
    {
        "loss": 1.911,
        "grad_norm": 3.0238234996795654,
        "learning_rate": 0.00019128329874610597,
        "epoch": 0.7585333333333333,
        "step": 5689
    },
    {
        "loss": 0.781,
        "grad_norm": 3.1804885864257812,
        "learning_rate": 0.00019125758151044983,
        "epoch": 0.7586666666666667,
        "step": 5690
    },
    {
        "loss": 3.1306,
        "grad_norm": 3.178133726119995,
        "learning_rate": 0.00019123182812735476,
        "epoch": 0.7588,
        "step": 5691
    },
    {
        "loss": 1.8009,
        "grad_norm": 3.90263295173645,
        "learning_rate": 0.00019120603860702166,
        "epoch": 0.7589333333333333,
        "step": 5692
    },
    {
        "loss": 1.7188,
        "grad_norm": 3.758775472640991,
        "learning_rate": 0.0001911802129596659,
        "epoch": 0.7590666666666667,
        "step": 5693
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.8065059185028076,
        "learning_rate": 0.00019115435119551713,
        "epoch": 0.7592,
        "step": 5694
    },
    {
        "loss": 2.8277,
        "grad_norm": 3.327650785446167,
        "learning_rate": 0.00019112845332481921,
        "epoch": 0.7593333333333333,
        "step": 5695
    },
    {
        "loss": 2.7002,
        "grad_norm": 3.6756582260131836,
        "learning_rate": 0.00019110251935783047,
        "epoch": 0.7594666666666666,
        "step": 5696
    },
    {
        "loss": 2.3868,
        "grad_norm": 4.378846645355225,
        "learning_rate": 0.00019107654930482332,
        "epoch": 0.7596,
        "step": 5697
    },
    {
        "loss": 1.086,
        "grad_norm": 4.859785079956055,
        "learning_rate": 0.0001910505431760847,
        "epoch": 0.7597333333333334,
        "step": 5698
    },
    {
        "loss": 2.941,
        "grad_norm": 3.021437883377075,
        "learning_rate": 0.00019102450098191565,
        "epoch": 0.7598666666666667,
        "step": 5699
    },
    {
        "loss": 1.299,
        "grad_norm": 3.0352156162261963,
        "learning_rate": 0.00019099842273263162,
        "epoch": 0.76,
        "step": 5700
    },
    {
        "loss": 2.2485,
        "grad_norm": 3.426754951477051,
        "learning_rate": 0.00019097230843856226,
        "epoch": 0.7601333333333333,
        "step": 5701
    },
    {
        "loss": 1.6134,
        "grad_norm": 2.6631007194519043,
        "learning_rate": 0.00019094615811005158,
        "epoch": 0.7602666666666666,
        "step": 5702
    },
    {
        "loss": 2.1514,
        "grad_norm": 2.3633787631988525,
        "learning_rate": 0.00019091997175745777,
        "epoch": 0.7604,
        "step": 5703
    },
    {
        "loss": 2.3661,
        "grad_norm": 3.840895652770996,
        "learning_rate": 0.00019089374939115333,
        "epoch": 0.7605333333333333,
        "step": 5704
    },
    {
        "loss": 2.9242,
        "grad_norm": 2.111050844192505,
        "learning_rate": 0.00019086749102152506,
        "epoch": 0.7606666666666667,
        "step": 5705
    },
    {
        "loss": 3.1111,
        "grad_norm": 3.3974952697753906,
        "learning_rate": 0.00019084119665897396,
        "epoch": 0.7608,
        "step": 5706
    },
    {
        "loss": 2.0655,
        "grad_norm": 3.3697731494903564,
        "learning_rate": 0.00019081486631391536,
        "epoch": 0.7609333333333334,
        "step": 5707
    },
    {
        "loss": 1.8705,
        "grad_norm": 4.116804122924805,
        "learning_rate": 0.0001907884999967787,
        "epoch": 0.7610666666666667,
        "step": 5708
    },
    {
        "loss": 2.6477,
        "grad_norm": 2.6488773822784424,
        "learning_rate": 0.0001907620977180079,
        "epoch": 0.7612,
        "step": 5709
    },
    {
        "loss": 1.7435,
        "grad_norm": 3.7013700008392334,
        "learning_rate": 0.0001907356594880609,
        "epoch": 0.7613333333333333,
        "step": 5710
    },
    {
        "loss": 2.8152,
        "grad_norm": 3.704986810684204,
        "learning_rate": 0.00019070918531741002,
        "epoch": 0.7614666666666666,
        "step": 5711
    },
    {
        "loss": 1.3683,
        "grad_norm": 4.422033786773682,
        "learning_rate": 0.00019068267521654177,
        "epoch": 0.7616,
        "step": 5712
    },
    {
        "loss": 2.4012,
        "grad_norm": 3.5779600143432617,
        "learning_rate": 0.00019065612919595684,
        "epoch": 0.7617333333333334,
        "step": 5713
    },
    {
        "loss": 2.0728,
        "grad_norm": 3.276190757751465,
        "learning_rate": 0.00019062954726617023,
        "epoch": 0.7618666666666667,
        "step": 5714
    },
    {
        "loss": 1.4582,
        "grad_norm": 3.9404525756835938,
        "learning_rate": 0.00019060292943771112,
        "epoch": 0.762,
        "step": 5715
    },
    {
        "loss": 2.6332,
        "grad_norm": 2.7161800861358643,
        "learning_rate": 0.000190576275721123,
        "epoch": 0.7621333333333333,
        "step": 5716
    },
    {
        "loss": 2.2194,
        "grad_norm": 3.4349708557128906,
        "learning_rate": 0.00019054958612696335,
        "epoch": 0.7622666666666666,
        "step": 5717
    },
    {
        "loss": 2.3707,
        "grad_norm": 3.8929123878479004,
        "learning_rate": 0.00019052286066580416,
        "epoch": 0.7624,
        "step": 5718
    },
    {
        "loss": 1.8147,
        "grad_norm": 2.9936389923095703,
        "learning_rate": 0.0001904960993482314,
        "epoch": 0.7625333333333333,
        "step": 5719
    },
    {
        "loss": 1.6941,
        "grad_norm": 5.5305399894714355,
        "learning_rate": 0.00019046930218484534,
        "epoch": 0.7626666666666667,
        "step": 5720
    },
    {
        "loss": 2.9443,
        "grad_norm": 3.2546777725219727,
        "learning_rate": 0.0001904424691862604,
        "epoch": 0.7628,
        "step": 5721
    },
    {
        "loss": 2.6683,
        "grad_norm": 2.6448211669921875,
        "learning_rate": 0.00019041560036310524,
        "epoch": 0.7629333333333334,
        "step": 5722
    },
    {
        "loss": 3.8741,
        "grad_norm": 3.915989398956299,
        "learning_rate": 0.0001903886957260227,
        "epoch": 0.7630666666666667,
        "step": 5723
    },
    {
        "loss": 2.406,
        "grad_norm": 3.7416272163391113,
        "learning_rate": 0.00019036175528566976,
        "epoch": 0.7632,
        "step": 5724
    },
    {
        "loss": 2.6986,
        "grad_norm": 5.39388370513916,
        "learning_rate": 0.0001903347790527177,
        "epoch": 0.7633333333333333,
        "step": 5725
    },
    {
        "loss": 2.758,
        "grad_norm": 3.0056204795837402,
        "learning_rate": 0.00019030776703785182,
        "epoch": 0.7634666666666666,
        "step": 5726
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.556713104248047,
        "learning_rate": 0.00019028071925177172,
        "epoch": 0.7636,
        "step": 5727
    },
    {
        "loss": 2.5131,
        "grad_norm": 3.3983774185180664,
        "learning_rate": 0.00019025363570519108,
        "epoch": 0.7637333333333334,
        "step": 5728
    },
    {
        "loss": 2.266,
        "grad_norm": 3.1544182300567627,
        "learning_rate": 0.00019022651640883783,
        "epoch": 0.7638666666666667,
        "step": 5729
    },
    {
        "loss": 2.0706,
        "grad_norm": 3.7219631671905518,
        "learning_rate": 0.00019019936137345397,
        "epoch": 0.764,
        "step": 5730
    },
    {
        "loss": 2.2355,
        "grad_norm": 3.335686683654785,
        "learning_rate": 0.0001901721706097957,
        "epoch": 0.7641333333333333,
        "step": 5731
    },
    {
        "loss": 2.7837,
        "grad_norm": 3.1666955947875977,
        "learning_rate": 0.00019014494412863346,
        "epoch": 0.7642666666666666,
        "step": 5732
    },
    {
        "loss": 2.2217,
        "grad_norm": 4.0232439041137695,
        "learning_rate": 0.00019011768194075163,
        "epoch": 0.7644,
        "step": 5733
    },
    {
        "loss": 2.9124,
        "grad_norm": 1.7781555652618408,
        "learning_rate": 0.0001900903840569489,
        "epoch": 0.7645333333333333,
        "step": 5734
    },
    {
        "loss": 3.3723,
        "grad_norm": 3.0713677406311035,
        "learning_rate": 0.00019006305048803812,
        "epoch": 0.7646666666666667,
        "step": 5735
    },
    {
        "loss": 2.5025,
        "grad_norm": 3.3835227489471436,
        "learning_rate": 0.0001900356812448462,
        "epoch": 0.7648,
        "step": 5736
    },
    {
        "loss": 1.2854,
        "grad_norm": 3.9025425910949707,
        "learning_rate": 0.00019000827633821406,
        "epoch": 0.7649333333333334,
        "step": 5737
    },
    {
        "loss": 2.5235,
        "grad_norm": 4.318626403808594,
        "learning_rate": 0.00018998083577899703,
        "epoch": 0.7650666666666667,
        "step": 5738
    },
    {
        "loss": 1.6215,
        "grad_norm": 3.892550230026245,
        "learning_rate": 0.0001899533595780643,
        "epoch": 0.7652,
        "step": 5739
    },
    {
        "loss": 2.7639,
        "grad_norm": 5.088179111480713,
        "learning_rate": 0.00018992584774629934,
        "epoch": 0.7653333333333333,
        "step": 5740
    },
    {
        "loss": 2.1503,
        "grad_norm": 4.875383377075195,
        "learning_rate": 0.0001898983002945997,
        "epoch": 0.7654666666666666,
        "step": 5741
    },
    {
        "loss": 2.5098,
        "grad_norm": 2.9352550506591797,
        "learning_rate": 0.00018987071723387697,
        "epoch": 0.7656,
        "step": 5742
    },
    {
        "loss": 2.6861,
        "grad_norm": 2.0900213718414307,
        "learning_rate": 0.00018984309857505692,
        "epoch": 0.7657333333333334,
        "step": 5743
    },
    {
        "loss": 2.6626,
        "grad_norm": 1.6490027904510498,
        "learning_rate": 0.00018981544432907944,
        "epoch": 0.7658666666666667,
        "step": 5744
    },
    {
        "loss": 2.7507,
        "grad_norm": 2.735224723815918,
        "learning_rate": 0.00018978775450689833,
        "epoch": 0.766,
        "step": 5745
    },
    {
        "loss": 2.2912,
        "grad_norm": 3.028022527694702,
        "learning_rate": 0.00018976002911948178,
        "epoch": 0.7661333333333333,
        "step": 5746
    },
    {
        "loss": 2.6792,
        "grad_norm": 2.3051249980926514,
        "learning_rate": 0.00018973226817781185,
        "epoch": 0.7662666666666667,
        "step": 5747
    },
    {
        "loss": 2.5174,
        "grad_norm": 2.385772705078125,
        "learning_rate": 0.00018970447169288471,
        "epoch": 0.7664,
        "step": 5748
    },
    {
        "loss": 2.744,
        "grad_norm": 2.2414138317108154,
        "learning_rate": 0.00018967663967571065,
        "epoch": 0.7665333333333333,
        "step": 5749
    },
    {
        "loss": 2.2393,
        "grad_norm": 4.6859002113342285,
        "learning_rate": 0.0001896487721373141,
        "epoch": 0.7666666666666667,
        "step": 5750
    },
    {
        "loss": 2.0136,
        "grad_norm": 3.3113393783569336,
        "learning_rate": 0.00018962086908873342,
        "epoch": 0.7668,
        "step": 5751
    },
    {
        "loss": 2.5829,
        "grad_norm": 3.3046040534973145,
        "learning_rate": 0.00018959293054102112,
        "epoch": 0.7669333333333334,
        "step": 5752
    },
    {
        "loss": 3.0527,
        "grad_norm": 2.2298431396484375,
        "learning_rate": 0.00018956495650524375,
        "epoch": 0.7670666666666667,
        "step": 5753
    },
    {
        "loss": 1.4925,
        "grad_norm": 3.7031643390655518,
        "learning_rate": 0.0001895369469924819,
        "epoch": 0.7672,
        "step": 5754
    },
    {
        "loss": 2.6513,
        "grad_norm": 2.6949007511138916,
        "learning_rate": 0.0001895089020138303,
        "epoch": 0.7673333333333333,
        "step": 5755
    },
    {
        "loss": 2.1597,
        "grad_norm": 3.5412840843200684,
        "learning_rate": 0.00018948082158039758,
        "epoch": 0.7674666666666666,
        "step": 5756
    },
    {
        "loss": 2.4165,
        "grad_norm": 3.341820240020752,
        "learning_rate": 0.00018945270570330656,
        "epoch": 0.7676,
        "step": 5757
    },
    {
        "loss": 2.5776,
        "grad_norm": 2.844533681869507,
        "learning_rate": 0.00018942455439369398,
        "epoch": 0.7677333333333334,
        "step": 5758
    },
    {
        "loss": 2.3964,
        "grad_norm": 4.438293933868408,
        "learning_rate": 0.00018939636766271073,
        "epoch": 0.7678666666666667,
        "step": 5759
    },
    {
        "loss": 2.1044,
        "grad_norm": 3.1096351146698,
        "learning_rate": 0.0001893681455215216,
        "epoch": 0.768,
        "step": 5760
    },
    {
        "loss": 2.7736,
        "grad_norm": 3.489293336868286,
        "learning_rate": 0.00018933988798130555,
        "epoch": 0.7681333333333333,
        "step": 5761
    },
    {
        "loss": 2.7043,
        "grad_norm": 2.249117136001587,
        "learning_rate": 0.00018931159505325545,
        "epoch": 0.7682666666666667,
        "step": 5762
    },
    {
        "loss": 0.6504,
        "grad_norm": 3.142507314682007,
        "learning_rate": 0.00018928326674857822,
        "epoch": 0.7684,
        "step": 5763
    },
    {
        "loss": 3.0516,
        "grad_norm": 2.571720600128174,
        "learning_rate": 0.00018925490307849477,
        "epoch": 0.7685333333333333,
        "step": 5764
    },
    {
        "loss": 2.7818,
        "grad_norm": 2.7988622188568115,
        "learning_rate": 0.00018922650405424013,
        "epoch": 0.7686666666666667,
        "step": 5765
    },
    {
        "loss": 2.681,
        "grad_norm": 2.8518102169036865,
        "learning_rate": 0.0001891980696870632,
        "epoch": 0.7688,
        "step": 5766
    },
    {
        "loss": 2.2128,
        "grad_norm": 3.143730401992798,
        "learning_rate": 0.0001891695999882269,
        "epoch": 0.7689333333333334,
        "step": 5767
    },
    {
        "loss": 1.5387,
        "grad_norm": 5.265578746795654,
        "learning_rate": 0.00018914109496900823,
        "epoch": 0.7690666666666667,
        "step": 5768
    },
    {
        "loss": 2.8991,
        "grad_norm": 2.448850631713867,
        "learning_rate": 0.00018911255464069813,
        "epoch": 0.7692,
        "step": 5769
    },
    {
        "loss": 2.9907,
        "grad_norm": 2.4183778762817383,
        "learning_rate": 0.00018908397901460142,
        "epoch": 0.7693333333333333,
        "step": 5770
    },
    {
        "loss": 2.783,
        "grad_norm": 1.9654279947280884,
        "learning_rate": 0.00018905536810203714,
        "epoch": 0.7694666666666666,
        "step": 5771
    },
    {
        "loss": 2.353,
        "grad_norm": 3.4162790775299072,
        "learning_rate": 0.00018902672191433809,
        "epoch": 0.7696,
        "step": 5772
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.770934581756592,
        "learning_rate": 0.00018899804046285115,
        "epoch": 0.7697333333333334,
        "step": 5773
    },
    {
        "loss": 1.0314,
        "grad_norm": 4.5352582931518555,
        "learning_rate": 0.00018896932375893704,
        "epoch": 0.7698666666666667,
        "step": 5774
    },
    {
        "loss": 2.4954,
        "grad_norm": 2.9651527404785156,
        "learning_rate": 0.00018894057181397073,
        "epoch": 0.77,
        "step": 5775
    },
    {
        "loss": 1.1429,
        "grad_norm": 5.028962135314941,
        "learning_rate": 0.00018891178463934085,
        "epoch": 0.7701333333333333,
        "step": 5776
    },
    {
        "loss": 2.3471,
        "grad_norm": 2.88901424407959,
        "learning_rate": 0.00018888296224645008,
        "epoch": 0.7702666666666667,
        "step": 5777
    },
    {
        "loss": 2.3411,
        "grad_norm": 4.735653877258301,
        "learning_rate": 0.00018885410464671517,
        "epoch": 0.7704,
        "step": 5778
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.6777236461639404,
        "learning_rate": 0.00018882521185156659,
        "epoch": 0.7705333333333333,
        "step": 5779
    },
    {
        "loss": 0.9391,
        "grad_norm": 4.075439929962158,
        "learning_rate": 0.00018879628387244896,
        "epoch": 0.7706666666666667,
        "step": 5780
    },
    {
        "loss": 2.6005,
        "grad_norm": 2.6933789253234863,
        "learning_rate": 0.0001887673207208207,
        "epoch": 0.7708,
        "step": 5781
    },
    {
        "loss": 2.7967,
        "grad_norm": 2.1645476818084717,
        "learning_rate": 0.00018873832240815425,
        "epoch": 0.7709333333333334,
        "step": 5782
    },
    {
        "loss": 2.7125,
        "grad_norm": 2.3530280590057373,
        "learning_rate": 0.0001887092889459359,
        "epoch": 0.7710666666666667,
        "step": 5783
    },
    {
        "loss": 1.9047,
        "grad_norm": 4.017409801483154,
        "learning_rate": 0.00018868022034566594,
        "epoch": 0.7712,
        "step": 5784
    },
    {
        "loss": 2.9045,
        "grad_norm": 2.470750093460083,
        "learning_rate": 0.00018865111661885855,
        "epoch": 0.7713333333333333,
        "step": 5785
    },
    {
        "loss": 1.8116,
        "grad_norm": 4.289995193481445,
        "learning_rate": 0.0001886219777770418,
        "epoch": 0.7714666666666666,
        "step": 5786
    },
    {
        "loss": 2.9713,
        "grad_norm": 4.459629058837891,
        "learning_rate": 0.00018859280383175765,
        "epoch": 0.7716,
        "step": 5787
    },
    {
        "loss": 2.5185,
        "grad_norm": 3.4324209690093994,
        "learning_rate": 0.00018856359479456206,
        "epoch": 0.7717333333333334,
        "step": 5788
    },
    {
        "loss": 1.9542,
        "grad_norm": 3.3392446041107178,
        "learning_rate": 0.0001885343506770248,
        "epoch": 0.7718666666666667,
        "step": 5789
    },
    {
        "loss": 2.4835,
        "grad_norm": 3.095187187194824,
        "learning_rate": 0.00018850507149072954,
        "epoch": 0.772,
        "step": 5790
    },
    {
        "loss": 2.9379,
        "grad_norm": 2.463897228240967,
        "learning_rate": 0.0001884757572472739,
        "epoch": 0.7721333333333333,
        "step": 5791
    },
    {
        "loss": 2.1193,
        "grad_norm": 2.5619187355041504,
        "learning_rate": 0.00018844640795826934,
        "epoch": 0.7722666666666667,
        "step": 5792
    },
    {
        "loss": 2.1321,
        "grad_norm": 3.7683587074279785,
        "learning_rate": 0.00018841702363534118,
        "epoch": 0.7724,
        "step": 5793
    },
    {
        "loss": 3.2506,
        "grad_norm": 2.1030752658843994,
        "learning_rate": 0.00018838760429012872,
        "epoch": 0.7725333333333333,
        "step": 5794
    },
    {
        "loss": 2.4708,
        "grad_norm": 3.4993081092834473,
        "learning_rate": 0.00018835814993428503,
        "epoch": 0.7726666666666666,
        "step": 5795
    },
    {
        "loss": 1.7718,
        "grad_norm": 3.7067787647247314,
        "learning_rate": 0.00018832866057947705,
        "epoch": 0.7728,
        "step": 5796
    },
    {
        "loss": 2.539,
        "grad_norm": 2.796645402908325,
        "learning_rate": 0.0001882991362373857,
        "epoch": 0.7729333333333334,
        "step": 5797
    },
    {
        "loss": 2.7677,
        "grad_norm": 2.9991722106933594,
        "learning_rate": 0.00018826957691970558,
        "epoch": 0.7730666666666667,
        "step": 5798
    },
    {
        "loss": 1.6332,
        "grad_norm": 3.4983913898468018,
        "learning_rate": 0.00018823998263814523,
        "epoch": 0.7732,
        "step": 5799
    },
    {
        "loss": 1.589,
        "grad_norm": 4.347616672515869,
        "learning_rate": 0.00018821035340442713,
        "epoch": 0.7733333333333333,
        "step": 5800
    },
    {
        "loss": 1.8238,
        "grad_norm": 2.690011739730835,
        "learning_rate": 0.00018818068923028748,
        "epoch": 0.7734666666666666,
        "step": 5801
    },
    {
        "loss": 2.5017,
        "grad_norm": 4.140560150146484,
        "learning_rate": 0.00018815099012747634,
        "epoch": 0.7736,
        "step": 5802
    },
    {
        "loss": 1.6077,
        "grad_norm": 5.1395263671875,
        "learning_rate": 0.0001881212561077577,
        "epoch": 0.7737333333333334,
        "step": 5803
    },
    {
        "loss": 2.594,
        "grad_norm": 3.2707526683807373,
        "learning_rate": 0.00018809148718290918,
        "epoch": 0.7738666666666667,
        "step": 5804
    },
    {
        "loss": 2.4503,
        "grad_norm": 4.103116989135742,
        "learning_rate": 0.00018806168336472245,
        "epoch": 0.774,
        "step": 5805
    },
    {
        "loss": 2.5374,
        "grad_norm": 3.1118850708007812,
        "learning_rate": 0.0001880318446650029,
        "epoch": 0.7741333333333333,
        "step": 5806
    },
    {
        "loss": 2.1094,
        "grad_norm": 3.977653980255127,
        "learning_rate": 0.00018800197109556968,
        "epoch": 0.7742666666666667,
        "step": 5807
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.12020206451416,
        "learning_rate": 0.00018797206266825582,
        "epoch": 0.7744,
        "step": 5808
    },
    {
        "loss": 2.9522,
        "grad_norm": 3.438900947570801,
        "learning_rate": 0.00018794211939490824,
        "epoch": 0.7745333333333333,
        "step": 5809
    },
    {
        "loss": 2.6134,
        "grad_norm": 2.8727049827575684,
        "learning_rate": 0.00018791214128738748,
        "epoch": 0.7746666666666666,
        "step": 5810
    },
    {
        "loss": 2.8988,
        "grad_norm": 5.339721202850342,
        "learning_rate": 0.000187882128357568,
        "epoch": 0.7748,
        "step": 5811
    },
    {
        "loss": 2.0557,
        "grad_norm": 4.22201681137085,
        "learning_rate": 0.00018785208061733804,
        "epoch": 0.7749333333333334,
        "step": 5812
    },
    {
        "loss": 2.254,
        "grad_norm": 3.3933496475219727,
        "learning_rate": 0.00018782199807859963,
        "epoch": 0.7750666666666667,
        "step": 5813
    },
    {
        "loss": 2.3242,
        "grad_norm": 5.131373405456543,
        "learning_rate": 0.0001877918807532685,
        "epoch": 0.7752,
        "step": 5814
    },
    {
        "loss": 2.6054,
        "grad_norm": 2.8107290267944336,
        "learning_rate": 0.00018776172865327424,
        "epoch": 0.7753333333333333,
        "step": 5815
    },
    {
        "loss": 2.8063,
        "grad_norm": 2.8345746994018555,
        "learning_rate": 0.00018773154179056026,
        "epoch": 0.7754666666666666,
        "step": 5816
    },
    {
        "loss": 2.5159,
        "grad_norm": 3.48262095451355,
        "learning_rate": 0.0001877013201770836,
        "epoch": 0.7756,
        "step": 5817
    },
    {
        "loss": 2.6486,
        "grad_norm": 2.890601873397827,
        "learning_rate": 0.00018767106382481524,
        "epoch": 0.7757333333333334,
        "step": 5818
    },
    {
        "loss": 3.0651,
        "grad_norm": 2.994906187057495,
        "learning_rate": 0.0001876407727457397,
        "epoch": 0.7758666666666667,
        "step": 5819
    },
    {
        "loss": 2.4673,
        "grad_norm": 3.9743144512176514,
        "learning_rate": 0.0001876104469518555,
        "epoch": 0.776,
        "step": 5820
    },
    {
        "loss": 1.8031,
        "grad_norm": 3.3552334308624268,
        "learning_rate": 0.00018758008645517471,
        "epoch": 0.7761333333333333,
        "step": 5821
    },
    {
        "loss": 2.3895,
        "grad_norm": 3.0138461589813232,
        "learning_rate": 0.00018754969126772328,
        "epoch": 0.7762666666666667,
        "step": 5822
    },
    {
        "loss": 2.2088,
        "grad_norm": 3.775876045227051,
        "learning_rate": 0.00018751926140154077,
        "epoch": 0.7764,
        "step": 5823
    },
    {
        "loss": 2.8907,
        "grad_norm": 2.8167333602905273,
        "learning_rate": 0.00018748879686868064,
        "epoch": 0.7765333333333333,
        "step": 5824
    },
    {
        "loss": 1.3624,
        "grad_norm": 7.975244522094727,
        "learning_rate": 0.00018745829768120997,
        "epoch": 0.7766666666666666,
        "step": 5825
    },
    {
        "loss": 2.3766,
        "grad_norm": 4.570970058441162,
        "learning_rate": 0.00018742776385120954,
        "epoch": 0.7768,
        "step": 5826
    },
    {
        "loss": 1.6247,
        "grad_norm": 3.573340892791748,
        "learning_rate": 0.000187397195390774,
        "epoch": 0.7769333333333334,
        "step": 5827
    },
    {
        "loss": 1.4106,
        "grad_norm": 3.751685619354248,
        "learning_rate": 0.00018736659231201154,
        "epoch": 0.7770666666666667,
        "step": 5828
    },
    {
        "loss": 2.7262,
        "grad_norm": 2.1220128536224365,
        "learning_rate": 0.00018733595462704417,
        "epoch": 0.7772,
        "step": 5829
    },
    {
        "loss": 2.1996,
        "grad_norm": 5.137322425842285,
        "learning_rate": 0.00018730528234800757,
        "epoch": 0.7773333333333333,
        "step": 5830
    },
    {
        "loss": 2.0945,
        "grad_norm": 3.992860794067383,
        "learning_rate": 0.00018727457548705116,
        "epoch": 0.7774666666666666,
        "step": 5831
    },
    {
        "loss": 2.8397,
        "grad_norm": 2.591757297515869,
        "learning_rate": 0.00018724383405633804,
        "epoch": 0.7776,
        "step": 5832
    },
    {
        "loss": 1.9579,
        "grad_norm": 2.944849729537964,
        "learning_rate": 0.0001872130580680449,
        "epoch": 0.7777333333333334,
        "step": 5833
    },
    {
        "loss": 2.0255,
        "grad_norm": 4.018446445465088,
        "learning_rate": 0.00018718224753436235,
        "epoch": 0.7778666666666667,
        "step": 5834
    },
    {
        "loss": 2.2908,
        "grad_norm": 2.6581549644470215,
        "learning_rate": 0.00018715140246749448,
        "epoch": 0.778,
        "step": 5835
    },
    {
        "loss": 1.8072,
        "grad_norm": 2.8461391925811768,
        "learning_rate": 0.00018712052287965912,
        "epoch": 0.7781333333333333,
        "step": 5836
    },
    {
        "loss": 1.2027,
        "grad_norm": 7.899081707000732,
        "learning_rate": 0.0001870896087830878,
        "epoch": 0.7782666666666667,
        "step": 5837
    },
    {
        "loss": 1.6608,
        "grad_norm": 3.3065555095672607,
        "learning_rate": 0.0001870586601900257,
        "epoch": 0.7784,
        "step": 5838
    },
    {
        "loss": 2.9215,
        "grad_norm": 3.179652452468872,
        "learning_rate": 0.00018702767711273167,
        "epoch": 0.7785333333333333,
        "step": 5839
    },
    {
        "loss": 2.6109,
        "grad_norm": 3.911766290664673,
        "learning_rate": 0.00018699665956347814,
        "epoch": 0.7786666666666666,
        "step": 5840
    },
    {
        "loss": 2.4025,
        "grad_norm": 4.5548505783081055,
        "learning_rate": 0.00018696560755455138,
        "epoch": 0.7788,
        "step": 5841
    },
    {
        "loss": 1.961,
        "grad_norm": 4.473326206207275,
        "learning_rate": 0.00018693452109825107,
        "epoch": 0.7789333333333334,
        "step": 5842
    },
    {
        "loss": 2.7716,
        "grad_norm": 3.6203384399414062,
        "learning_rate": 0.00018690340020689078,
        "epoch": 0.7790666666666667,
        "step": 5843
    },
    {
        "loss": 2.7055,
        "grad_norm": 3.6572225093841553,
        "learning_rate": 0.00018687224489279754,
        "epoch": 0.7792,
        "step": 5844
    },
    {
        "loss": 1.5334,
        "grad_norm": 4.317816734313965,
        "learning_rate": 0.0001868410551683121,
        "epoch": 0.7793333333333333,
        "step": 5845
    },
    {
        "loss": 2.4792,
        "grad_norm": 3.8923661708831787,
        "learning_rate": 0.00018680983104578876,
        "epoch": 0.7794666666666666,
        "step": 5846
    },
    {
        "loss": 2.257,
        "grad_norm": 3.547685146331787,
        "learning_rate": 0.00018677857253759564,
        "epoch": 0.7796,
        "step": 5847
    },
    {
        "loss": 2.3469,
        "grad_norm": 4.731903076171875,
        "learning_rate": 0.00018674727965611415,
        "epoch": 0.7797333333333333,
        "step": 5848
    },
    {
        "loss": 1.5,
        "grad_norm": 2.073810577392578,
        "learning_rate": 0.00018671595241373965,
        "epoch": 0.7798666666666667,
        "step": 5849
    },
    {
        "loss": 0.9736,
        "grad_norm": 3.9524941444396973,
        "learning_rate": 0.0001866845908228809,
        "epoch": 0.78,
        "step": 5850
    },
    {
        "loss": 3.1703,
        "grad_norm": 3.1755073070526123,
        "learning_rate": 0.00018665319489596038,
        "epoch": 0.7801333333333333,
        "step": 5851
    },
    {
        "loss": 2.8094,
        "grad_norm": 4.53809118270874,
        "learning_rate": 0.00018662176464541409,
        "epoch": 0.7802666666666667,
        "step": 5852
    },
    {
        "loss": 1.5741,
        "grad_norm": 3.426583766937256,
        "learning_rate": 0.00018659030008369166,
        "epoch": 0.7804,
        "step": 5853
    },
    {
        "loss": 1.3382,
        "grad_norm": 5.590670585632324,
        "learning_rate": 0.00018655880122325633,
        "epoch": 0.7805333333333333,
        "step": 5854
    },
    {
        "loss": 2.769,
        "grad_norm": 6.799851417541504,
        "learning_rate": 0.00018652726807658488,
        "epoch": 0.7806666666666666,
        "step": 5855
    },
    {
        "loss": 2.152,
        "grad_norm": 5.490047931671143,
        "learning_rate": 0.00018649570065616772,
        "epoch": 0.7808,
        "step": 5856
    },
    {
        "loss": 2.3854,
        "grad_norm": 3.3957715034484863,
        "learning_rate": 0.00018646409897450877,
        "epoch": 0.7809333333333334,
        "step": 5857
    },
    {
        "loss": 2.0375,
        "grad_norm": 6.039125919342041,
        "learning_rate": 0.00018643246304412566,
        "epoch": 0.7810666666666667,
        "step": 5858
    },
    {
        "loss": 1.442,
        "grad_norm": 6.1275858879089355,
        "learning_rate": 0.0001864007928775494,
        "epoch": 0.7812,
        "step": 5859
    },
    {
        "loss": 1.9778,
        "grad_norm": 4.49287748336792,
        "learning_rate": 0.00018636908848732465,
        "epoch": 0.7813333333333333,
        "step": 5860
    },
    {
        "loss": 1.364,
        "grad_norm": 4.299440383911133,
        "learning_rate": 0.00018633734988600967,
        "epoch": 0.7814666666666666,
        "step": 5861
    },
    {
        "loss": 2.7747,
        "grad_norm": 2.0724010467529297,
        "learning_rate": 0.0001863055770861762,
        "epoch": 0.7816,
        "step": 5862
    },
    {
        "loss": 2.588,
        "grad_norm": 3.1900136470794678,
        "learning_rate": 0.0001862737701004096,
        "epoch": 0.7817333333333333,
        "step": 5863
    },
    {
        "loss": 2.2225,
        "grad_norm": 4.186107635498047,
        "learning_rate": 0.0001862419289413087,
        "epoch": 0.7818666666666667,
        "step": 5864
    },
    {
        "loss": 0.594,
        "grad_norm": 3.0438733100891113,
        "learning_rate": 0.00018621005362148588,
        "epoch": 0.782,
        "step": 5865
    },
    {
        "loss": 2.4783,
        "grad_norm": 2.784081220626831,
        "learning_rate": 0.00018617814415356705,
        "epoch": 0.7821333333333333,
        "step": 5866
    },
    {
        "loss": 0.6622,
        "grad_norm": 2.682541608810425,
        "learning_rate": 0.00018614620055019162,
        "epoch": 0.7822666666666667,
        "step": 5867
    },
    {
        "loss": 1.9233,
        "grad_norm": 3.725940704345703,
        "learning_rate": 0.0001861142228240127,
        "epoch": 0.7824,
        "step": 5868
    },
    {
        "loss": 2.1738,
        "grad_norm": 3.461935520172119,
        "learning_rate": 0.00018608221098769665,
        "epoch": 0.7825333333333333,
        "step": 5869
    },
    {
        "loss": 3.0345,
        "grad_norm": 2.218172788619995,
        "learning_rate": 0.00018605016505392354,
        "epoch": 0.7826666666666666,
        "step": 5870
    },
    {
        "loss": 1.9724,
        "grad_norm": 2.9112257957458496,
        "learning_rate": 0.00018601808503538683,
        "epoch": 0.7828,
        "step": 5871
    },
    {
        "loss": 2.7437,
        "grad_norm": 3.158390998840332,
        "learning_rate": 0.00018598597094479356,
        "epoch": 0.7829333333333334,
        "step": 5872
    },
    {
        "loss": 2.7504,
        "grad_norm": 3.678330183029175,
        "learning_rate": 0.00018595382279486416,
        "epoch": 0.7830666666666667,
        "step": 5873
    },
    {
        "loss": 2.4431,
        "grad_norm": 4.031252384185791,
        "learning_rate": 0.00018592164059833268,
        "epoch": 0.7832,
        "step": 5874
    },
    {
        "loss": 2.7587,
        "grad_norm": 2.001253128051758,
        "learning_rate": 0.00018588942436794663,
        "epoch": 0.7833333333333333,
        "step": 5875
    },
    {
        "loss": 2.5129,
        "grad_norm": 3.054182529449463,
        "learning_rate": 0.00018585717411646684,
        "epoch": 0.7834666666666666,
        "step": 5876
    },
    {
        "loss": 1.4572,
        "grad_norm": 4.379716396331787,
        "learning_rate": 0.0001858248898566679,
        "epoch": 0.7836,
        "step": 5877
    },
    {
        "loss": 2.71,
        "grad_norm": 2.5532619953155518,
        "learning_rate": 0.00018579257160133766,
        "epoch": 0.7837333333333333,
        "step": 5878
    },
    {
        "loss": 2.223,
        "grad_norm": 2.7324297428131104,
        "learning_rate": 0.00018576021936327747,
        "epoch": 0.7838666666666667,
        "step": 5879
    },
    {
        "loss": 2.8156,
        "grad_norm": 2.619335174560547,
        "learning_rate": 0.00018572783315530213,
        "epoch": 0.784,
        "step": 5880
    },
    {
        "loss": 2.3406,
        "grad_norm": 3.2807083129882812,
        "learning_rate": 0.00018569541299024,
        "epoch": 0.7841333333333333,
        "step": 5881
    },
    {
        "loss": 1.6751,
        "grad_norm": 3.334277391433716,
        "learning_rate": 0.00018566295888093278,
        "epoch": 0.7842666666666667,
        "step": 5882
    },
    {
        "loss": 1.8103,
        "grad_norm": 6.495969772338867,
        "learning_rate": 0.00018563047084023564,
        "epoch": 0.7844,
        "step": 5883
    },
    {
        "loss": 1.5175,
        "grad_norm": 2.8416574001312256,
        "learning_rate": 0.00018559794888101724,
        "epoch": 0.7845333333333333,
        "step": 5884
    },
    {
        "loss": 2.6751,
        "grad_norm": 2.880469560623169,
        "learning_rate": 0.0001855653930161596,
        "epoch": 0.7846666666666666,
        "step": 5885
    },
    {
        "loss": 1.2318,
        "grad_norm": 3.30607533454895,
        "learning_rate": 0.00018553280325855825,
        "epoch": 0.7848,
        "step": 5886
    },
    {
        "loss": 2.7077,
        "grad_norm": 2.4799644947052,
        "learning_rate": 0.0001855001796211221,
        "epoch": 0.7849333333333334,
        "step": 5887
    },
    {
        "loss": 1.8524,
        "grad_norm": 4.454911231994629,
        "learning_rate": 0.00018546752211677343,
        "epoch": 0.7850666666666667,
        "step": 5888
    },
    {
        "loss": 0.8507,
        "grad_norm": 3.682957649230957,
        "learning_rate": 0.00018543483075844806,
        "epoch": 0.7852,
        "step": 5889
    },
    {
        "loss": 2.6427,
        "grad_norm": 4.871057510375977,
        "learning_rate": 0.00018540210555909509,
        "epoch": 0.7853333333333333,
        "step": 5890
    },
    {
        "loss": 3.0505,
        "grad_norm": 3.1712658405303955,
        "learning_rate": 0.0001853693465316771,
        "epoch": 0.7854666666666666,
        "step": 5891
    },
    {
        "loss": 1.9915,
        "grad_norm": 4.717117786407471,
        "learning_rate": 0.00018533655368917006,
        "epoch": 0.7856,
        "step": 5892
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.9284355640411377,
        "learning_rate": 0.00018530372704456336,
        "epoch": 0.7857333333333333,
        "step": 5893
    },
    {
        "loss": 1.8804,
        "grad_norm": 2.444394826889038,
        "learning_rate": 0.00018527086661085967,
        "epoch": 0.7858666666666667,
        "step": 5894
    },
    {
        "loss": 1.2005,
        "grad_norm": 3.3587357997894287,
        "learning_rate": 0.00018523797240107518,
        "epoch": 0.786,
        "step": 5895
    },
    {
        "loss": 2.6052,
        "grad_norm": 3.366659641265869,
        "learning_rate": 0.0001852050444282394,
        "epoch": 0.7861333333333334,
        "step": 5896
    },
    {
        "loss": 2.6589,
        "grad_norm": 2.3388073444366455,
        "learning_rate": 0.00018517208270539518,
        "epoch": 0.7862666666666667,
        "step": 5897
    },
    {
        "loss": 1.3321,
        "grad_norm": 3.845775842666626,
        "learning_rate": 0.00018513908724559878,
        "epoch": 0.7864,
        "step": 5898
    },
    {
        "loss": 2.2045,
        "grad_norm": 7.73765230178833,
        "learning_rate": 0.00018510605806191984,
        "epoch": 0.7865333333333333,
        "step": 5899
    },
    {
        "loss": 3.6768,
        "grad_norm": 4.112346172332764,
        "learning_rate": 0.0001850729951674413,
        "epoch": 0.7866666666666666,
        "step": 5900
    },
    {
        "loss": 2.861,
        "grad_norm": 2.313028573989868,
        "learning_rate": 0.00018503989857525944,
        "epoch": 0.7868,
        "step": 5901
    },
    {
        "loss": 2.381,
        "grad_norm": 3.8505985736846924,
        "learning_rate": 0.00018500676829848403,
        "epoch": 0.7869333333333334,
        "step": 5902
    },
    {
        "loss": 3.0741,
        "grad_norm": 1.7926021814346313,
        "learning_rate": 0.00018497360435023805,
        "epoch": 0.7870666666666667,
        "step": 5903
    },
    {
        "loss": 2.3991,
        "grad_norm": 2.4147891998291016,
        "learning_rate": 0.00018494040674365788,
        "epoch": 0.7872,
        "step": 5904
    },
    {
        "loss": 2.903,
        "grad_norm": 3.2641708850860596,
        "learning_rate": 0.0001849071754918931,
        "epoch": 0.7873333333333333,
        "step": 5905
    },
    {
        "loss": 1.4747,
        "grad_norm": 4.245410919189453,
        "learning_rate": 0.00018487391060810685,
        "epoch": 0.7874666666666666,
        "step": 5906
    },
    {
        "loss": 1.9526,
        "grad_norm": 4.721649646759033,
        "learning_rate": 0.00018484061210547535,
        "epoch": 0.7876,
        "step": 5907
    },
    {
        "loss": 2.2858,
        "grad_norm": 3.085035800933838,
        "learning_rate": 0.00018480727999718835,
        "epoch": 0.7877333333333333,
        "step": 5908
    },
    {
        "loss": 2.6069,
        "grad_norm": 4.685792922973633,
        "learning_rate": 0.0001847739142964488,
        "epoch": 0.7878666666666667,
        "step": 5909
    },
    {
        "loss": 1.9247,
        "grad_norm": 2.3744077682495117,
        "learning_rate": 0.0001847405150164729,
        "epoch": 0.788,
        "step": 5910
    },
    {
        "loss": 2.0725,
        "grad_norm": 3.8530361652374268,
        "learning_rate": 0.0001847070821704902,
        "epoch": 0.7881333333333334,
        "step": 5911
    },
    {
        "loss": 2.5643,
        "grad_norm": 3.2362334728240967,
        "learning_rate": 0.00018467361577174373,
        "epoch": 0.7882666666666667,
        "step": 5912
    },
    {
        "loss": 2.8567,
        "grad_norm": 2.9449172019958496,
        "learning_rate": 0.00018464011583348947,
        "epoch": 0.7884,
        "step": 5913
    },
    {
        "loss": 1.3751,
        "grad_norm": 3.6230740547180176,
        "learning_rate": 0.00018460658236899694,
        "epoch": 0.7885333333333333,
        "step": 5914
    },
    {
        "loss": 1.9493,
        "grad_norm": 4.115216255187988,
        "learning_rate": 0.00018457301539154884,
        "epoch": 0.7886666666666666,
        "step": 5915
    },
    {
        "loss": 1.0304,
        "grad_norm": 3.005309581756592,
        "learning_rate": 0.00018453941491444113,
        "epoch": 0.7888,
        "step": 5916
    },
    {
        "loss": 2.5269,
        "grad_norm": 3.636688709259033,
        "learning_rate": 0.00018450578095098314,
        "epoch": 0.7889333333333334,
        "step": 5917
    },
    {
        "loss": 2.9262,
        "grad_norm": 4.405484199523926,
        "learning_rate": 0.0001844721135144974,
        "epoch": 0.7890666666666667,
        "step": 5918
    },
    {
        "loss": 2.5415,
        "grad_norm": 3.1982359886169434,
        "learning_rate": 0.00018443841261831962,
        "epoch": 0.7892,
        "step": 5919
    },
    {
        "loss": 1.9723,
        "grad_norm": 3.1826064586639404,
        "learning_rate": 0.0001844046782757989,
        "epoch": 0.7893333333333333,
        "step": 5920
    },
    {
        "loss": 2.3603,
        "grad_norm": 3.765108823776245,
        "learning_rate": 0.00018437091050029752,
        "epoch": 0.7894666666666666,
        "step": 5921
    },
    {
        "loss": 2.7827,
        "grad_norm": 4.64442777633667,
        "learning_rate": 0.000184337109305191,
        "epoch": 0.7896,
        "step": 5922
    },
    {
        "loss": 2.4403,
        "grad_norm": 2.770892381668091,
        "learning_rate": 0.00018430327470386816,
        "epoch": 0.7897333333333333,
        "step": 5923
    },
    {
        "loss": 1.0644,
        "grad_norm": 3.8907697200775146,
        "learning_rate": 0.0001842694067097309,
        "epoch": 0.7898666666666667,
        "step": 5924
    },
    {
        "loss": 2.9109,
        "grad_norm": 5.176295280456543,
        "learning_rate": 0.0001842355053361945,
        "epoch": 0.79,
        "step": 5925
    },
    {
        "loss": 2.6607,
        "grad_norm": 3.4232921600341797,
        "learning_rate": 0.00018420157059668738,
        "epoch": 0.7901333333333334,
        "step": 5926
    },
    {
        "loss": 2.2634,
        "grad_norm": 6.470586776733398,
        "learning_rate": 0.00018416760250465131,
        "epoch": 0.7902666666666667,
        "step": 5927
    },
    {
        "loss": 1.3551,
        "grad_norm": 4.11019229888916,
        "learning_rate": 0.00018413360107354104,
        "epoch": 0.7904,
        "step": 5928
    },
    {
        "loss": 2.446,
        "grad_norm": 2.499643087387085,
        "learning_rate": 0.00018409956631682475,
        "epoch": 0.7905333333333333,
        "step": 5929
    },
    {
        "loss": 1.595,
        "grad_norm": 2.8823752403259277,
        "learning_rate": 0.00018406549824798364,
        "epoch": 0.7906666666666666,
        "step": 5930
    },
    {
        "loss": 2.8362,
        "grad_norm": 3.0682151317596436,
        "learning_rate": 0.0001840313968805123,
        "epoch": 0.7908,
        "step": 5931
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.9865996837615967,
        "learning_rate": 0.00018399726222791823,
        "epoch": 0.7909333333333334,
        "step": 5932
    },
    {
        "loss": 2.6727,
        "grad_norm": 3.778378963470459,
        "learning_rate": 0.0001839630943037224,
        "epoch": 0.7910666666666667,
        "step": 5933
    },
    {
        "loss": 2.7984,
        "grad_norm": 2.4516689777374268,
        "learning_rate": 0.0001839288931214589,
        "epoch": 0.7912,
        "step": 5934
    },
    {
        "loss": 2.7921,
        "grad_norm": 2.6289451122283936,
        "learning_rate": 0.00018389465869467472,
        "epoch": 0.7913333333333333,
        "step": 5935
    },
    {
        "loss": 2.6466,
        "grad_norm": 2.373356342315674,
        "learning_rate": 0.00018386039103693045,
        "epoch": 0.7914666666666667,
        "step": 5936
    },
    {
        "loss": 2.0923,
        "grad_norm": 4.709779262542725,
        "learning_rate": 0.00018382609016179952,
        "epoch": 0.7916,
        "step": 5937
    },
    {
        "loss": 2.8883,
        "grad_norm": 2.8684165477752686,
        "learning_rate": 0.0001837917560828687,
        "epoch": 0.7917333333333333,
        "step": 5938
    },
    {
        "loss": 1.5821,
        "grad_norm": 5.812488079071045,
        "learning_rate": 0.0001837573888137377,
        "epoch": 0.7918666666666667,
        "step": 5939
    },
    {
        "loss": 2.8331,
        "grad_norm": 2.8061373233795166,
        "learning_rate": 0.00018372298836801968,
        "epoch": 0.792,
        "step": 5940
    },
    {
        "loss": 2.8326,
        "grad_norm": 2.204563617706299,
        "learning_rate": 0.00018368855475934065,
        "epoch": 0.7921333333333334,
        "step": 5941
    },
    {
        "loss": 1.9975,
        "grad_norm": 4.0921502113342285,
        "learning_rate": 0.00018365408800133989,
        "epoch": 0.7922666666666667,
        "step": 5942
    },
    {
        "loss": 1.293,
        "grad_norm": 5.11022424697876,
        "learning_rate": 0.00018361958810766987,
        "epoch": 0.7924,
        "step": 5943
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.915048599243164,
        "learning_rate": 0.00018358505509199604,
        "epoch": 0.7925333333333333,
        "step": 5944
    },
    {
        "loss": 2.1104,
        "grad_norm": 3.6916868686676025,
        "learning_rate": 0.00018355048896799708,
        "epoch": 0.7926666666666666,
        "step": 5945
    },
    {
        "loss": 2.0702,
        "grad_norm": 2.6787662506103516,
        "learning_rate": 0.00018351588974936477,
        "epoch": 0.7928,
        "step": 5946
    },
    {
        "loss": 2.8429,
        "grad_norm": 2.7484891414642334,
        "learning_rate": 0.00018348125744980392,
        "epoch": 0.7929333333333334,
        "step": 5947
    },
    {
        "loss": 2.0616,
        "grad_norm": 2.790069818496704,
        "learning_rate": 0.00018344659208303262,
        "epoch": 0.7930666666666667,
        "step": 5948
    },
    {
        "loss": 2.5776,
        "grad_norm": 4.8539838790893555,
        "learning_rate": 0.00018341189366278177,
        "epoch": 0.7932,
        "step": 5949
    },
    {
        "loss": 2.1535,
        "grad_norm": 3.5281927585601807,
        "learning_rate": 0.00018337716220279569,
        "epoch": 0.7933333333333333,
        "step": 5950
    },
    {
        "loss": 1.8295,
        "grad_norm": 7.482046127319336,
        "learning_rate": 0.0001833423977168315,
        "epoch": 0.7934666666666667,
        "step": 5951
    },
    {
        "loss": 2.0935,
        "grad_norm": 3.2115635871887207,
        "learning_rate": 0.00018330760021865962,
        "epoch": 0.7936,
        "step": 5952
    },
    {
        "loss": 2.4371,
        "grad_norm": 2.9058451652526855,
        "learning_rate": 0.0001832727697220634,
        "epoch": 0.7937333333333333,
        "step": 5953
    },
    {
        "loss": 3.3082,
        "grad_norm": 2.7361748218536377,
        "learning_rate": 0.00018323790624083942,
        "epoch": 0.7938666666666667,
        "step": 5954
    },
    {
        "loss": 2.6985,
        "grad_norm": 3.586744546890259,
        "learning_rate": 0.0001832030097887971,
        "epoch": 0.794,
        "step": 5955
    },
    {
        "loss": 1.623,
        "grad_norm": 4.0673017501831055,
        "learning_rate": 0.00018316808037975907,
        "epoch": 0.7941333333333334,
        "step": 5956
    },
    {
        "loss": 2.9966,
        "grad_norm": 3.0176167488098145,
        "learning_rate": 0.0001831331180275611,
        "epoch": 0.7942666666666667,
        "step": 5957
    },
    {
        "loss": 3.0827,
        "grad_norm": 3.462752103805542,
        "learning_rate": 0.00018309812274605174,
        "epoch": 0.7944,
        "step": 5958
    },
    {
        "loss": 1.0048,
        "grad_norm": 4.3214287757873535,
        "learning_rate": 0.00018306309454909286,
        "epoch": 0.7945333333333333,
        "step": 5959
    },
    {
        "loss": 2.322,
        "grad_norm": 4.512637615203857,
        "learning_rate": 0.0001830280334505591,
        "epoch": 0.7946666666666666,
        "step": 5960
    },
    {
        "loss": 2.9182,
        "grad_norm": 3.6197073459625244,
        "learning_rate": 0.00018299293946433848,
        "epoch": 0.7948,
        "step": 5961
    },
    {
        "loss": 1.7945,
        "grad_norm": 3.179051399230957,
        "learning_rate": 0.0001829578126043317,
        "epoch": 0.7949333333333334,
        "step": 5962
    },
    {
        "loss": 2.2875,
        "grad_norm": 3.234050989151001,
        "learning_rate": 0.0001829226528844527,
        "epoch": 0.7950666666666667,
        "step": 5963
    },
    {
        "loss": 2.2893,
        "grad_norm": 3.555300712585449,
        "learning_rate": 0.0001828874603186283,
        "epoch": 0.7952,
        "step": 5964
    },
    {
        "loss": 2.875,
        "grad_norm": 2.2089879512786865,
        "learning_rate": 0.0001828522349207985,
        "epoch": 0.7953333333333333,
        "step": 5965
    },
    {
        "loss": 2.3952,
        "grad_norm": 4.362952709197998,
        "learning_rate": 0.00018281697670491612,
        "epoch": 0.7954666666666667,
        "step": 5966
    },
    {
        "loss": 2.9552,
        "grad_norm": 5.222261905670166,
        "learning_rate": 0.000182781685684947,
        "epoch": 0.7956,
        "step": 5967
    },
    {
        "loss": 1.9705,
        "grad_norm": 3.0128085613250732,
        "learning_rate": 0.0001827463618748702,
        "epoch": 0.7957333333333333,
        "step": 5968
    },
    {
        "loss": 1.8009,
        "grad_norm": 3.9242663383483887,
        "learning_rate": 0.00018271100528867746,
        "epoch": 0.7958666666666666,
        "step": 5969
    },
    {
        "loss": 2.2251,
        "grad_norm": 2.8220763206481934,
        "learning_rate": 0.0001826756159403737,
        "epoch": 0.796,
        "step": 5970
    },
    {
        "loss": 2.4887,
        "grad_norm": 3.696532964706421,
        "learning_rate": 0.00018264019384397679,
        "epoch": 0.7961333333333334,
        "step": 5971
    },
    {
        "loss": 2.331,
        "grad_norm": 3.244309902191162,
        "learning_rate": 0.00018260473901351746,
        "epoch": 0.7962666666666667,
        "step": 5972
    },
    {
        "loss": 1.9559,
        "grad_norm": 3.654182195663452,
        "learning_rate": 0.0001825692514630396,
        "epoch": 0.7964,
        "step": 5973
    },
    {
        "loss": 2.6076,
        "grad_norm": 3.0972936153411865,
        "learning_rate": 0.00018253373120659982,
        "epoch": 0.7965333333333333,
        "step": 5974
    },
    {
        "loss": 1.9957,
        "grad_norm": 3.630260944366455,
        "learning_rate": 0.0001824981782582679,
        "epoch": 0.7966666666666666,
        "step": 5975
    },
    {
        "loss": 2.5949,
        "grad_norm": 2.411266326904297,
        "learning_rate": 0.0001824625926321265,
        "epoch": 0.7968,
        "step": 5976
    },
    {
        "loss": 3.1486,
        "grad_norm": 3.273198366165161,
        "learning_rate": 0.00018242697434227116,
        "epoch": 0.7969333333333334,
        "step": 5977
    },
    {
        "loss": 2.2806,
        "grad_norm": 2.8187685012817383,
        "learning_rate": 0.00018239132340281037,
        "epoch": 0.7970666666666667,
        "step": 5978
    },
    {
        "loss": 1.6972,
        "grad_norm": 5.096837520599365,
        "learning_rate": 0.00018235563982786567,
        "epoch": 0.7972,
        "step": 5979
    },
    {
        "loss": 2.9435,
        "grad_norm": 3.4756720066070557,
        "learning_rate": 0.00018231992363157144,
        "epoch": 0.7973333333333333,
        "step": 5980
    },
    {
        "loss": 1.5908,
        "grad_norm": 3.639415979385376,
        "learning_rate": 0.00018228417482807493,
        "epoch": 0.7974666666666667,
        "step": 5981
    },
    {
        "loss": 2.0022,
        "grad_norm": 3.2156271934509277,
        "learning_rate": 0.00018224839343153643,
        "epoch": 0.7976,
        "step": 5982
    },
    {
        "loss": 2.1426,
        "grad_norm": 3.728342056274414,
        "learning_rate": 0.00018221257945612903,
        "epoch": 0.7977333333333333,
        "step": 5983
    },
    {
        "loss": 2.8056,
        "grad_norm": 4.087533473968506,
        "learning_rate": 0.00018217673291603877,
        "epoch": 0.7978666666666666,
        "step": 5984
    },
    {
        "loss": 2.0233,
        "grad_norm": 4.702190399169922,
        "learning_rate": 0.00018214085382546453,
        "epoch": 0.798,
        "step": 5985
    },
    {
        "loss": 3.0755,
        "grad_norm": 2.256364583969116,
        "learning_rate": 0.0001821049421986183,
        "epoch": 0.7981333333333334,
        "step": 5986
    },
    {
        "loss": 2.6225,
        "grad_norm": 4.010948657989502,
        "learning_rate": 0.00018206899804972464,
        "epoch": 0.7982666666666667,
        "step": 5987
    },
    {
        "loss": 2.0749,
        "grad_norm": 3.593688488006592,
        "learning_rate": 0.00018203302139302124,
        "epoch": 0.7984,
        "step": 5988
    },
    {
        "loss": 2.5482,
        "grad_norm": 2.6519792079925537,
        "learning_rate": 0.00018199701224275852,
        "epoch": 0.7985333333333333,
        "step": 5989
    },
    {
        "loss": 2.5932,
        "grad_norm": 2.1631369590759277,
        "learning_rate": 0.00018196097061319987,
        "epoch": 0.7986666666666666,
        "step": 5990
    },
    {
        "loss": 3.0711,
        "grad_norm": 3.583508014678955,
        "learning_rate": 0.00018192489651862147,
        "epoch": 0.7988,
        "step": 5991
    },
    {
        "loss": 2.5153,
        "grad_norm": 3.809267520904541,
        "learning_rate": 0.0001818887899733124,
        "epoch": 0.7989333333333334,
        "step": 5992
    },
    {
        "loss": 1.9787,
        "grad_norm": 5.115533828735352,
        "learning_rate": 0.00018185265099157465,
        "epoch": 0.7990666666666667,
        "step": 5993
    },
    {
        "loss": 2.9107,
        "grad_norm": 2.338846206665039,
        "learning_rate": 0.00018181647958772284,
        "epoch": 0.7992,
        "step": 5994
    },
    {
        "loss": 2.6987,
        "grad_norm": 3.248404026031494,
        "learning_rate": 0.00018178027577608473,
        "epoch": 0.7993333333333333,
        "step": 5995
    },
    {
        "loss": 2.8839,
        "grad_norm": 4.167245864868164,
        "learning_rate": 0.00018174403957100073,
        "epoch": 0.7994666666666667,
        "step": 5996
    },
    {
        "loss": 2.7148,
        "grad_norm": 2.9810853004455566,
        "learning_rate": 0.00018170777098682413,
        "epoch": 0.7996,
        "step": 5997
    },
    {
        "loss": 1.9655,
        "grad_norm": 3.157210350036621,
        "learning_rate": 0.00018167147003792102,
        "epoch": 0.7997333333333333,
        "step": 5998
    },
    {
        "loss": 1.6509,
        "grad_norm": 4.126523971557617,
        "learning_rate": 0.00018163513673867033,
        "epoch": 0.7998666666666666,
        "step": 5999
    },
    {
        "loss": 3.0725,
        "grad_norm": 4.298118591308594,
        "learning_rate": 0.00018159877110346384,
        "epoch": 0.8,
        "step": 6000
    },
    {
        "loss": 2.5488,
        "grad_norm": 4.372774600982666,
        "learning_rate": 0.00018156237314670602,
        "epoch": 0.8001333333333334,
        "step": 6001
    },
    {
        "loss": 2.4868,
        "grad_norm": 4.37952995300293,
        "learning_rate": 0.00018152594288281434,
        "epoch": 0.8002666666666667,
        "step": 6002
    },
    {
        "loss": 2.577,
        "grad_norm": 2.885977268218994,
        "learning_rate": 0.00018148948032621886,
        "epoch": 0.8004,
        "step": 6003
    },
    {
        "loss": 2.7513,
        "grad_norm": 3.7099738121032715,
        "learning_rate": 0.00018145298549136256,
        "epoch": 0.8005333333333333,
        "step": 6004
    },
    {
        "loss": 2.4374,
        "grad_norm": 7.351400852203369,
        "learning_rate": 0.0001814164583927012,
        "epoch": 0.8006666666666666,
        "step": 6005
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.7219526767730713,
        "learning_rate": 0.00018137989904470317,
        "epoch": 0.8008,
        "step": 6006
    },
    {
        "loss": 2.107,
        "grad_norm": 2.9405810832977295,
        "learning_rate": 0.00018134330746184994,
        "epoch": 0.8009333333333334,
        "step": 6007
    },
    {
        "loss": 2.2484,
        "grad_norm": 4.100776195526123,
        "learning_rate": 0.00018130668365863537,
        "epoch": 0.8010666666666667,
        "step": 6008
    },
    {
        "loss": 2.3781,
        "grad_norm": 3.420604705810547,
        "learning_rate": 0.0001812700276495664,
        "epoch": 0.8012,
        "step": 6009
    },
    {
        "loss": 2.6905,
        "grad_norm": 2.1210925579071045,
        "learning_rate": 0.0001812333394491625,
        "epoch": 0.8013333333333333,
        "step": 6010
    },
    {
        "loss": 2.3095,
        "grad_norm": 1.9117579460144043,
        "learning_rate": 0.0001811966190719561,
        "epoch": 0.8014666666666667,
        "step": 6011
    },
    {
        "loss": 2.689,
        "grad_norm": 2.5878775119781494,
        "learning_rate": 0.00018115986653249218,
        "epoch": 0.8016,
        "step": 6012
    },
    {
        "loss": 2.0515,
        "grad_norm": 6.342752456665039,
        "learning_rate": 0.00018112308184532863,
        "epoch": 0.8017333333333333,
        "step": 6013
    },
    {
        "loss": 2.293,
        "grad_norm": 2.527897357940674,
        "learning_rate": 0.0001810862650250359,
        "epoch": 0.8018666666666666,
        "step": 6014
    },
    {
        "loss": 2.6965,
        "grad_norm": 2.621814727783203,
        "learning_rate": 0.0001810494160861973,
        "epoch": 0.802,
        "step": 6015
    },
    {
        "loss": 2.2122,
        "grad_norm": 3.025883674621582,
        "learning_rate": 0.00018101253504340884,
        "epoch": 0.8021333333333334,
        "step": 6016
    },
    {
        "loss": 0.8762,
        "grad_norm": 3.819230794906616,
        "learning_rate": 0.00018097562191127917,
        "epoch": 0.8022666666666667,
        "step": 6017
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.9820027351379395,
        "learning_rate": 0.0001809386767044298,
        "epoch": 0.8024,
        "step": 6018
    },
    {
        "loss": 2.3355,
        "grad_norm": 2.8724212646484375,
        "learning_rate": 0.0001809016994374947,
        "epoch": 0.8025333333333333,
        "step": 6019
    },
    {
        "loss": 2.1951,
        "grad_norm": 9.463772773742676,
        "learning_rate": 0.00018086469012512088,
        "epoch": 0.8026666666666666,
        "step": 6020
    },
    {
        "loss": 3.2207,
        "grad_norm": 4.349174499511719,
        "learning_rate": 0.00018082764878196773,
        "epoch": 0.8028,
        "step": 6021
    },
    {
        "loss": 2.6723,
        "grad_norm": 2.695042371749878,
        "learning_rate": 0.0001807905754227075,
        "epoch": 0.8029333333333334,
        "step": 6022
    },
    {
        "loss": 2.3555,
        "grad_norm": 3.5737149715423584,
        "learning_rate": 0.00018075347006202505,
        "epoch": 0.8030666666666667,
        "step": 6023
    },
    {
        "loss": 2.7654,
        "grad_norm": 3.5584137439727783,
        "learning_rate": 0.00018071633271461796,
        "epoch": 0.8032,
        "step": 6024
    },
    {
        "loss": 1.9956,
        "grad_norm": 3.1991994380950928,
        "learning_rate": 0.00018067916339519643,
        "epoch": 0.8033333333333333,
        "step": 6025
    },
    {
        "loss": 2.3796,
        "grad_norm": 3.3702611923217773,
        "learning_rate": 0.00018064196211848335,
        "epoch": 0.8034666666666667,
        "step": 6026
    },
    {
        "loss": 1.4734,
        "grad_norm": 4.234908103942871,
        "learning_rate": 0.00018060472889921435,
        "epoch": 0.8036,
        "step": 6027
    },
    {
        "loss": 1.6904,
        "grad_norm": 4.623164653778076,
        "learning_rate": 0.00018056746375213752,
        "epoch": 0.8037333333333333,
        "step": 6028
    },
    {
        "loss": 0.8162,
        "grad_norm": 3.290297269821167,
        "learning_rate": 0.0001805301666920138,
        "epoch": 0.8038666666666666,
        "step": 6029
    },
    {
        "loss": 3.0501,
        "grad_norm": 4.069940090179443,
        "learning_rate": 0.00018049283773361667,
        "epoch": 0.804,
        "step": 6030
    },
    {
        "loss": 2.3483,
        "grad_norm": 2.9223039150238037,
        "learning_rate": 0.00018045547689173218,
        "epoch": 0.8041333333333334,
        "step": 6031
    },
    {
        "loss": 2.3638,
        "grad_norm": 4.418356418609619,
        "learning_rate": 0.00018041808418115923,
        "epoch": 0.8042666666666667,
        "step": 6032
    },
    {
        "loss": 2.1132,
        "grad_norm": 4.125722408294678,
        "learning_rate": 0.00018038065961670908,
        "epoch": 0.8044,
        "step": 6033
    },
    {
        "loss": 2.1982,
        "grad_norm": 3.379826545715332,
        "learning_rate": 0.0001803432032132058,
        "epoch": 0.8045333333333333,
        "step": 6034
    },
    {
        "loss": 2.2913,
        "grad_norm": 3.7041943073272705,
        "learning_rate": 0.00018030571498548584,
        "epoch": 0.8046666666666666,
        "step": 6035
    },
    {
        "loss": 1.9251,
        "grad_norm": 3.266993284225464,
        "learning_rate": 0.00018026819494839865,
        "epoch": 0.8048,
        "step": 6036
    },
    {
        "loss": 1.4756,
        "grad_norm": 5.995697975158691,
        "learning_rate": 0.0001802306431168059,
        "epoch": 0.8049333333333333,
        "step": 6037
    },
    {
        "loss": 2.6743,
        "grad_norm": 3.2629756927490234,
        "learning_rate": 0.00018019305950558204,
        "epoch": 0.8050666666666667,
        "step": 6038
    },
    {
        "loss": 1.3484,
        "grad_norm": 3.7455954551696777,
        "learning_rate": 0.0001801554441296141,
        "epoch": 0.8052,
        "step": 6039
    },
    {
        "loss": 2.0662,
        "grad_norm": 2.731516122817993,
        "learning_rate": 0.00018011779700380154,
        "epoch": 0.8053333333333333,
        "step": 6040
    },
    {
        "loss": 1.7451,
        "grad_norm": 2.4677774906158447,
        "learning_rate": 0.00018008011814305664,
        "epoch": 0.8054666666666667,
        "step": 6041
    },
    {
        "loss": 2.0821,
        "grad_norm": 4.210177421569824,
        "learning_rate": 0.00018004240756230408,
        "epoch": 0.8056,
        "step": 6042
    },
    {
        "loss": 1.4409,
        "grad_norm": 2.646097183227539,
        "learning_rate": 0.00018000466527648117,
        "epoch": 0.8057333333333333,
        "step": 6043
    },
    {
        "loss": 2.2196,
        "grad_norm": 3.6681134700775146,
        "learning_rate": 0.00017996689130053763,
        "epoch": 0.8058666666666666,
        "step": 6044
    },
    {
        "loss": 2.8023,
        "grad_norm": 2.696136713027954,
        "learning_rate": 0.00017992908564943603,
        "epoch": 0.806,
        "step": 6045
    },
    {
        "loss": 2.7304,
        "grad_norm": 2.832181215286255,
        "learning_rate": 0.0001798912483381513,
        "epoch": 0.8061333333333334,
        "step": 6046
    },
    {
        "loss": 2.5131,
        "grad_norm": 6.989231586456299,
        "learning_rate": 0.00017985337938167083,
        "epoch": 0.8062666666666667,
        "step": 6047
    },
    {
        "loss": 1.5873,
        "grad_norm": 2.4326133728027344,
        "learning_rate": 0.0001798154787949947,
        "epoch": 0.8064,
        "step": 6048
    },
    {
        "loss": 2.2414,
        "grad_norm": 3.5543062686920166,
        "learning_rate": 0.00017977754659313547,
        "epoch": 0.8065333333333333,
        "step": 6049
    },
    {
        "loss": 2.5805,
        "grad_norm": 3.326063871383667,
        "learning_rate": 0.00017973958279111817,
        "epoch": 0.8066666666666666,
        "step": 6050
    },
    {
        "loss": 2.1776,
        "grad_norm": 3.715068817138672,
        "learning_rate": 0.00017970158740398043,
        "epoch": 0.8068,
        "step": 6051
    },
    {
        "loss": 2.3892,
        "grad_norm": 2.749267101287842,
        "learning_rate": 0.00017966356044677237,
        "epoch": 0.8069333333333333,
        "step": 6052
    },
    {
        "loss": 2.8571,
        "grad_norm": 4.369410514831543,
        "learning_rate": 0.00017962550193455652,
        "epoch": 0.8070666666666667,
        "step": 6053
    },
    {
        "loss": 2.6471,
        "grad_norm": 2.854203701019287,
        "learning_rate": 0.00017958741188240805,
        "epoch": 0.8072,
        "step": 6054
    },
    {
        "loss": 2.535,
        "grad_norm": 3.8244667053222656,
        "learning_rate": 0.0001795492903054146,
        "epoch": 0.8073333333333333,
        "step": 6055
    },
    {
        "loss": 2.1551,
        "grad_norm": 4.72605562210083,
        "learning_rate": 0.00017951113721867613,
        "epoch": 0.8074666666666667,
        "step": 6056
    },
    {
        "loss": 2.4117,
        "grad_norm": 3.125210762023926,
        "learning_rate": 0.0001794729526373053,
        "epoch": 0.8076,
        "step": 6057
    },
    {
        "loss": 2.6457,
        "grad_norm": 2.6652185916900635,
        "learning_rate": 0.00017943473657642717,
        "epoch": 0.8077333333333333,
        "step": 6058
    },
    {
        "loss": 2.3291,
        "grad_norm": 3.5580005645751953,
        "learning_rate": 0.00017939648905117918,
        "epoch": 0.8078666666666666,
        "step": 6059
    },
    {
        "loss": 1.892,
        "grad_norm": 3.586117744445801,
        "learning_rate": 0.00017935821007671133,
        "epoch": 0.808,
        "step": 6060
    },
    {
        "loss": 2.5272,
        "grad_norm": 3.7491707801818848,
        "learning_rate": 0.0001793198996681861,
        "epoch": 0.8081333333333334,
        "step": 6061
    },
    {
        "loss": 2.1165,
        "grad_norm": 3.7261781692504883,
        "learning_rate": 0.0001792815578407783,
        "epoch": 0.8082666666666667,
        "step": 6062
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.384666919708252,
        "learning_rate": 0.00017924318460967533,
        "epoch": 0.8084,
        "step": 6063
    },
    {
        "loss": 0.8428,
        "grad_norm": 3.442178726196289,
        "learning_rate": 0.00017920477999007692,
        "epoch": 0.8085333333333333,
        "step": 6064
    },
    {
        "loss": 1.596,
        "grad_norm": 3.7857580184936523,
        "learning_rate": 0.00017916634399719526,
        "epoch": 0.8086666666666666,
        "step": 6065
    },
    {
        "loss": 2.2817,
        "grad_norm": 2.3248775005340576,
        "learning_rate": 0.000179127876646255,
        "epoch": 0.8088,
        "step": 6066
    },
    {
        "loss": 3.229,
        "grad_norm": 3.8551316261291504,
        "learning_rate": 0.00017908937795249315,
        "epoch": 0.8089333333333333,
        "step": 6067
    },
    {
        "loss": 2.8092,
        "grad_norm": 3.4100544452667236,
        "learning_rate": 0.00017905084793115922,
        "epoch": 0.8090666666666667,
        "step": 6068
    },
    {
        "loss": 2.4851,
        "grad_norm": 3.646867513656616,
        "learning_rate": 0.00017901228659751502,
        "epoch": 0.8092,
        "step": 6069
    },
    {
        "loss": 2.0694,
        "grad_norm": 3.163666009902954,
        "learning_rate": 0.00017897369396683495,
        "epoch": 0.8093333333333333,
        "step": 6070
    },
    {
        "loss": 2.9462,
        "grad_norm": 5.13017463684082,
        "learning_rate": 0.00017893507005440555,
        "epoch": 0.8094666666666667,
        "step": 6071
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.344360113143921,
        "learning_rate": 0.00017889641487552597,
        "epoch": 0.8096,
        "step": 6072
    },
    {
        "loss": 1.6343,
        "grad_norm": 3.308518886566162,
        "learning_rate": 0.00017885772844550762,
        "epoch": 0.8097333333333333,
        "step": 6073
    },
    {
        "loss": 1.6089,
        "grad_norm": 3.7571892738342285,
        "learning_rate": 0.00017881901077967434,
        "epoch": 0.8098666666666666,
        "step": 6074
    },
    {
        "loss": 2.8248,
        "grad_norm": 1.9409056901931763,
        "learning_rate": 0.00017878026189336227,
        "epoch": 0.81,
        "step": 6075
    },
    {
        "loss": 2.2279,
        "grad_norm": 2.1727681159973145,
        "learning_rate": 0.00017874148180192006,
        "epoch": 0.8101333333333334,
        "step": 6076
    },
    {
        "loss": 2.9757,
        "grad_norm": 5.500877857208252,
        "learning_rate": 0.0001787026705207086,
        "epoch": 0.8102666666666667,
        "step": 6077
    },
    {
        "loss": 2.413,
        "grad_norm": 2.516803503036499,
        "learning_rate": 0.00017866382806510113,
        "epoch": 0.8104,
        "step": 6078
    },
    {
        "loss": 1.8673,
        "grad_norm": 2.9260904788970947,
        "learning_rate": 0.00017862495445048339,
        "epoch": 0.8105333333333333,
        "step": 6079
    },
    {
        "loss": 2.1094,
        "grad_norm": 4.56876277923584,
        "learning_rate": 0.00017858604969225324,
        "epoch": 0.8106666666666666,
        "step": 6080
    },
    {
        "loss": 2.9235,
        "grad_norm": 2.895207643508911,
        "learning_rate": 0.0001785471138058211,
        "epoch": 0.8108,
        "step": 6081
    },
    {
        "loss": 2.4769,
        "grad_norm": 3.9282314777374268,
        "learning_rate": 0.00017850814680660948,
        "epoch": 0.8109333333333333,
        "step": 6082
    },
    {
        "loss": 1.9087,
        "grad_norm": 4.359689712524414,
        "learning_rate": 0.00017846914871005341,
        "epoch": 0.8110666666666667,
        "step": 6083
    },
    {
        "loss": 2.6984,
        "grad_norm": 4.511240005493164,
        "learning_rate": 0.00017843011953160018,
        "epoch": 0.8112,
        "step": 6084
    },
    {
        "loss": 2.9921,
        "grad_norm": 3.372697591781616,
        "learning_rate": 0.00017839105928670932,
        "epoch": 0.8113333333333334,
        "step": 6085
    },
    {
        "loss": 1.6477,
        "grad_norm": 4.257706165313721,
        "learning_rate": 0.0001783519679908528,
        "epoch": 0.8114666666666667,
        "step": 6086
    },
    {
        "loss": 2.146,
        "grad_norm": 4.424006462097168,
        "learning_rate": 0.00017831284565951477,
        "epoch": 0.8116,
        "step": 6087
    },
    {
        "loss": 1.5188,
        "grad_norm": 3.690309524536133,
        "learning_rate": 0.0001782736923081917,
        "epoch": 0.8117333333333333,
        "step": 6088
    },
    {
        "loss": 2.3899,
        "grad_norm": 3.085954427719116,
        "learning_rate": 0.00017823450795239246,
        "epoch": 0.8118666666666666,
        "step": 6089
    },
    {
        "loss": 2.2131,
        "grad_norm": 3.764195442199707,
        "learning_rate": 0.000178195292607638,
        "epoch": 0.812,
        "step": 6090
    },
    {
        "loss": 1.1917,
        "grad_norm": 3.8095173835754395,
        "learning_rate": 0.00017815604628946172,
        "epoch": 0.8121333333333334,
        "step": 6091
    },
    {
        "loss": 1.4724,
        "grad_norm": 3.993086338043213,
        "learning_rate": 0.00017811676901340916,
        "epoch": 0.8122666666666667,
        "step": 6092
    },
    {
        "loss": 1.5271,
        "grad_norm": 3.5846879482269287,
        "learning_rate": 0.00017807746079503823,
        "epoch": 0.8124,
        "step": 6093
    },
    {
        "loss": 2.535,
        "grad_norm": 2.6106786727905273,
        "learning_rate": 0.000178038121649919,
        "epoch": 0.8125333333333333,
        "step": 6094
    },
    {
        "loss": 0.6475,
        "grad_norm": 3.0377542972564697,
        "learning_rate": 0.00017799875159363394,
        "epoch": 0.8126666666666666,
        "step": 6095
    },
    {
        "loss": 2.5205,
        "grad_norm": 3.348327875137329,
        "learning_rate": 0.00017795935064177754,
        "epoch": 0.8128,
        "step": 6096
    },
    {
        "loss": 3.1907,
        "grad_norm": 2.9443671703338623,
        "learning_rate": 0.00017791991880995676,
        "epoch": 0.8129333333333333,
        "step": 6097
    },
    {
        "loss": 1.0619,
        "grad_norm": 3.157466173171997,
        "learning_rate": 0.00017788045611379061,
        "epoch": 0.8130666666666667,
        "step": 6098
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.7837650775909424,
        "learning_rate": 0.00017784096256891045,
        "epoch": 0.8132,
        "step": 6099
    },
    {
        "loss": 2.6489,
        "grad_norm": 3.9560043811798096,
        "learning_rate": 0.00017780143819095978,
        "epoch": 0.8133333333333334,
        "step": 6100
    },
    {
        "loss": 1.404,
        "grad_norm": 3.712402820587158,
        "learning_rate": 0.0001777618829955943,
        "epoch": 0.8134666666666667,
        "step": 6101
    },
    {
        "loss": 1.4714,
        "grad_norm": 3.672623872756958,
        "learning_rate": 0.00017772229699848207,
        "epoch": 0.8136,
        "step": 6102
    },
    {
        "loss": 2.3325,
        "grad_norm": 4.3712897300720215,
        "learning_rate": 0.00017768268021530303,
        "epoch": 0.8137333333333333,
        "step": 6103
    },
    {
        "loss": 2.7383,
        "grad_norm": 3.6088266372680664,
        "learning_rate": 0.0001776430326617498,
        "epoch": 0.8138666666666666,
        "step": 6104
    },
    {
        "loss": 2.9222,
        "grad_norm": 3.6181108951568604,
        "learning_rate": 0.0001776033543535267,
        "epoch": 0.814,
        "step": 6105
    },
    {
        "loss": 1.7662,
        "grad_norm": 4.032167911529541,
        "learning_rate": 0.00017756364530635056,
        "epoch": 0.8141333333333334,
        "step": 6106
    },
    {
        "loss": 2.8396,
        "grad_norm": 3.730750560760498,
        "learning_rate": 0.00017752390553595014,
        "epoch": 0.8142666666666667,
        "step": 6107
    },
    {
        "loss": 2.2295,
        "grad_norm": 3.997788190841675,
        "learning_rate": 0.00017748413505806667,
        "epoch": 0.8144,
        "step": 6108
    },
    {
        "loss": 2.0613,
        "grad_norm": 4.921108722686768,
        "learning_rate": 0.00017744433388845323,
        "epoch": 0.8145333333333333,
        "step": 6109
    },
    {
        "loss": 1.9511,
        "grad_norm": 1.8158371448516846,
        "learning_rate": 0.00017740450204287527,
        "epoch": 0.8146666666666667,
        "step": 6110
    },
    {
        "loss": 2.9,
        "grad_norm": 2.898122549057007,
        "learning_rate": 0.00017736463953711034,
        "epoch": 0.8148,
        "step": 6111
    },
    {
        "loss": 2.8426,
        "grad_norm": 3.6577072143554688,
        "learning_rate": 0.00017732474638694801,
        "epoch": 0.8149333333333333,
        "step": 6112
    },
    {
        "loss": 2.5976,
        "grad_norm": 2.2456977367401123,
        "learning_rate": 0.00017728482260819016,
        "epoch": 0.8150666666666667,
        "step": 6113
    },
    {
        "loss": 2.694,
        "grad_norm": 3.0134522914886475,
        "learning_rate": 0.0001772448682166508,
        "epoch": 0.8152,
        "step": 6114
    },
    {
        "loss": 1.9652,
        "grad_norm": 3.7982590198516846,
        "learning_rate": 0.00017720488322815587,
        "epoch": 0.8153333333333334,
        "step": 6115
    },
    {
        "loss": 2.0632,
        "grad_norm": 3.991218090057373,
        "learning_rate": 0.00017716486765854362,
        "epoch": 0.8154666666666667,
        "step": 6116
    },
    {
        "loss": 1.7463,
        "grad_norm": 5.441512584686279,
        "learning_rate": 0.0001771248215236644,
        "epoch": 0.8156,
        "step": 6117
    },
    {
        "loss": 2.6865,
        "grad_norm": 2.255159854888916,
        "learning_rate": 0.00017708474483938056,
        "epoch": 0.8157333333333333,
        "step": 6118
    },
    {
        "loss": 1.7483,
        "grad_norm": 5.28131103515625,
        "learning_rate": 0.0001770446376215666,
        "epoch": 0.8158666666666666,
        "step": 6119
    },
    {
        "loss": 2.5418,
        "grad_norm": 3.4115138053894043,
        "learning_rate": 0.0001770044998861092,
        "epoch": 0.816,
        "step": 6120
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.613323450088501,
        "learning_rate": 0.000176964331648907,
        "epoch": 0.8161333333333334,
        "step": 6121
    },
    {
        "loss": 2.0996,
        "grad_norm": 4.8007941246032715,
        "learning_rate": 0.00017692413292587077,
        "epoch": 0.8162666666666667,
        "step": 6122
    },
    {
        "loss": 2.0735,
        "grad_norm": 3.245880603790283,
        "learning_rate": 0.0001768839037329234,
        "epoch": 0.8164,
        "step": 6123
    },
    {
        "loss": 1.8762,
        "grad_norm": 2.3101775646209717,
        "learning_rate": 0.00017684364408599977,
        "epoch": 0.8165333333333333,
        "step": 6124
    },
    {
        "loss": 1.1092,
        "grad_norm": 3.7243409156799316,
        "learning_rate": 0.00017680335400104688,
        "epoch": 0.8166666666666667,
        "step": 6125
    },
    {
        "loss": 1.2685,
        "grad_norm": 3.4066498279571533,
        "learning_rate": 0.00017676303349402377,
        "epoch": 0.8168,
        "step": 6126
    },
    {
        "loss": 1.5745,
        "grad_norm": 2.4747402667999268,
        "learning_rate": 0.00017672268258090156,
        "epoch": 0.8169333333333333,
        "step": 6127
    },
    {
        "loss": 2.4463,
        "grad_norm": 2.639218807220459,
        "learning_rate": 0.00017668230127766324,
        "epoch": 0.8170666666666667,
        "step": 6128
    },
    {
        "loss": 3.0006,
        "grad_norm": 2.9868686199188232,
        "learning_rate": 0.0001766418896003042,
        "epoch": 0.8172,
        "step": 6129
    },
    {
        "loss": 2.5488,
        "grad_norm": 2.390963554382324,
        "learning_rate": 0.00017660144756483152,
        "epoch": 0.8173333333333334,
        "step": 6130
    },
    {
        "loss": 2.8727,
        "grad_norm": 2.6858303546905518,
        "learning_rate": 0.00017656097518726447,
        "epoch": 0.8174666666666667,
        "step": 6131
    },
    {
        "loss": 0.9767,
        "grad_norm": 3.8825745582580566,
        "learning_rate": 0.00017652047248363425,
        "epoch": 0.8176,
        "step": 6132
    },
    {
        "loss": 1.9125,
        "grad_norm": 3.678938150405884,
        "learning_rate": 0.00017647993946998412,
        "epoch": 0.8177333333333333,
        "step": 6133
    },
    {
        "loss": 2.4656,
        "grad_norm": 2.870997190475464,
        "learning_rate": 0.0001764393761623694,
        "epoch": 0.8178666666666666,
        "step": 6134
    },
    {
        "loss": 2.2332,
        "grad_norm": 3.1907544136047363,
        "learning_rate": 0.0001763987825768573,
        "epoch": 0.818,
        "step": 6135
    },
    {
        "loss": 2.6919,
        "grad_norm": 3.1207456588745117,
        "learning_rate": 0.0001763581587295271,
        "epoch": 0.8181333333333334,
        "step": 6136
    },
    {
        "loss": 2.0974,
        "grad_norm": 5.12822151184082,
        "learning_rate": 0.00017631750463647,
        "epoch": 0.8182666666666667,
        "step": 6137
    },
    {
        "loss": 2.2843,
        "grad_norm": 2.798600435256958,
        "learning_rate": 0.00017627682031378934,
        "epoch": 0.8184,
        "step": 6138
    },
    {
        "loss": 1.4272,
        "grad_norm": 3.46226167678833,
        "learning_rate": 0.00017623610577760022,
        "epoch": 0.8185333333333333,
        "step": 6139
    },
    {
        "loss": 2.7669,
        "grad_norm": 2.624488592147827,
        "learning_rate": 0.00017619536104402985,
        "epoch": 0.8186666666666667,
        "step": 6140
    },
    {
        "loss": 2.855,
        "grad_norm": 2.591665267944336,
        "learning_rate": 0.00017615458612921733,
        "epoch": 0.8188,
        "step": 6141
    },
    {
        "loss": 2.8219,
        "grad_norm": 3.539537191390991,
        "learning_rate": 0.00017611378104931375,
        "epoch": 0.8189333333333333,
        "step": 6142
    },
    {
        "loss": 3.0342,
        "grad_norm": 2.553169012069702,
        "learning_rate": 0.00017607294582048212,
        "epoch": 0.8190666666666667,
        "step": 6143
    },
    {
        "loss": 2.6942,
        "grad_norm": 3.296966075897217,
        "learning_rate": 0.00017603208045889748,
        "epoch": 0.8192,
        "step": 6144
    },
    {
        "loss": 2.3444,
        "grad_norm": 3.147336483001709,
        "learning_rate": 0.0001759911849807467,
        "epoch": 0.8193333333333334,
        "step": 6145
    },
    {
        "loss": 3.3448,
        "grad_norm": 4.248150825500488,
        "learning_rate": 0.0001759502594022286,
        "epoch": 0.8194666666666667,
        "step": 6146
    },
    {
        "loss": 2.3839,
        "grad_norm": 3.6831653118133545,
        "learning_rate": 0.00017590930373955393,
        "epoch": 0.8196,
        "step": 6147
    },
    {
        "loss": 1.8948,
        "grad_norm": 4.600056171417236,
        "learning_rate": 0.00017586831800894543,
        "epoch": 0.8197333333333333,
        "step": 6148
    },
    {
        "loss": 1.8599,
        "grad_norm": 3.8864552974700928,
        "learning_rate": 0.0001758273022266376,
        "epoch": 0.8198666666666666,
        "step": 6149
    },
    {
        "loss": 2.753,
        "grad_norm": 3.694658041000366,
        "learning_rate": 0.00017578625640887708,
        "epoch": 0.82,
        "step": 6150
    },
    {
        "loss": 2.4613,
        "grad_norm": 2.7345051765441895,
        "learning_rate": 0.0001757451805719221,
        "epoch": 0.8201333333333334,
        "step": 6151
    },
    {
        "loss": 2.6218,
        "grad_norm": 2.6222033500671387,
        "learning_rate": 0.000175704074732043,
        "epoch": 0.8202666666666667,
        "step": 6152
    },
    {
        "loss": 1.737,
        "grad_norm": 4.391173362731934,
        "learning_rate": 0.0001756629389055219,
        "epoch": 0.8204,
        "step": 6153
    },
    {
        "loss": 2.4968,
        "grad_norm": 4.2231855392456055,
        "learning_rate": 0.00017562177310865296,
        "epoch": 0.8205333333333333,
        "step": 6154
    },
    {
        "loss": 1.6686,
        "grad_norm": 5.891149997711182,
        "learning_rate": 0.00017558057735774196,
        "epoch": 0.8206666666666667,
        "step": 6155
    },
    {
        "loss": 3.2264,
        "grad_norm": 4.164207935333252,
        "learning_rate": 0.00017553935166910679,
        "epoch": 0.8208,
        "step": 6156
    },
    {
        "loss": 2.0878,
        "grad_norm": 3.736283302307129,
        "learning_rate": 0.00017549809605907699,
        "epoch": 0.8209333333333333,
        "step": 6157
    },
    {
        "loss": 2.42,
        "grad_norm": 4.242289066314697,
        "learning_rate": 0.0001754568105439941,
        "epoch": 0.8210666666666666,
        "step": 6158
    },
    {
        "loss": 1.612,
        "grad_norm": 3.5921945571899414,
        "learning_rate": 0.00017541549514021144,
        "epoch": 0.8212,
        "step": 6159
    },
    {
        "loss": 2.6475,
        "grad_norm": 2.5764992237091064,
        "learning_rate": 0.00017537414986409418,
        "epoch": 0.8213333333333334,
        "step": 6160
    },
    {
        "loss": 2.8953,
        "grad_norm": 4.454237937927246,
        "learning_rate": 0.00017533277473201934,
        "epoch": 0.8214666666666667,
        "step": 6161
    },
    {
        "loss": 2.7443,
        "grad_norm": 3.840686798095703,
        "learning_rate": 0.0001752913697603757,
        "epoch": 0.8216,
        "step": 6162
    },
    {
        "loss": 2.2134,
        "grad_norm": 2.6198770999908447,
        "learning_rate": 0.000175249934965564,
        "epoch": 0.8217333333333333,
        "step": 6163
    },
    {
        "loss": 2.5142,
        "grad_norm": 3.448309898376465,
        "learning_rate": 0.00017520847036399658,
        "epoch": 0.8218666666666666,
        "step": 6164
    },
    {
        "loss": 2.6935,
        "grad_norm": 4.025124549865723,
        "learning_rate": 0.00017516697597209788,
        "epoch": 0.822,
        "step": 6165
    },
    {
        "loss": 1.7102,
        "grad_norm": 6.203983783721924,
        "learning_rate": 0.0001751254518063038,
        "epoch": 0.8221333333333334,
        "step": 6166
    },
    {
        "loss": 1.9047,
        "grad_norm": 3.0437164306640625,
        "learning_rate": 0.0001750838978830623,
        "epoch": 0.8222666666666667,
        "step": 6167
    },
    {
        "loss": 1.9087,
        "grad_norm": 3.0610098838806152,
        "learning_rate": 0.000175042314218833,
        "epoch": 0.8224,
        "step": 6168
    },
    {
        "loss": 2.9691,
        "grad_norm": 2.54996919631958,
        "learning_rate": 0.00017500070083008735,
        "epoch": 0.8225333333333333,
        "step": 6169
    },
    {
        "loss": 2.2529,
        "grad_norm": 3.8779890537261963,
        "learning_rate": 0.0001749590577333086,
        "epoch": 0.8226666666666667,
        "step": 6170
    },
    {
        "loss": 1.7285,
        "grad_norm": 4.376204490661621,
        "learning_rate": 0.00017491738494499157,
        "epoch": 0.8228,
        "step": 6171
    },
    {
        "loss": 1.8079,
        "grad_norm": 3.487419605255127,
        "learning_rate": 0.0001748756824816431,
        "epoch": 0.8229333333333333,
        "step": 6172
    },
    {
        "loss": 2.6254,
        "grad_norm": 2.402411699295044,
        "learning_rate": 0.0001748339503597817,
        "epoch": 0.8230666666666666,
        "step": 6173
    },
    {
        "loss": 2.0007,
        "grad_norm": 3.466684579849243,
        "learning_rate": 0.00017479218859593756,
        "epoch": 0.8232,
        "step": 6174
    },
    {
        "loss": 1.8307,
        "grad_norm": 4.089010238647461,
        "learning_rate": 0.0001747503972066527,
        "epoch": 0.8233333333333334,
        "step": 6175
    },
    {
        "loss": 2.3942,
        "grad_norm": 3.106217384338379,
        "learning_rate": 0.00017470857620848073,
        "epoch": 0.8234666666666667,
        "step": 6176
    },
    {
        "loss": 2.4669,
        "grad_norm": 2.76094388961792,
        "learning_rate": 0.00017466672561798722,
        "epoch": 0.8236,
        "step": 6177
    },
    {
        "loss": 1.9467,
        "grad_norm": 3.2332706451416016,
        "learning_rate": 0.0001746248454517492,
        "epoch": 0.8237333333333333,
        "step": 6178
    },
    {
        "loss": 2.0712,
        "grad_norm": 4.152267932891846,
        "learning_rate": 0.00017458293572635572,
        "epoch": 0.8238666666666666,
        "step": 6179
    },
    {
        "loss": 2.0906,
        "grad_norm": 4.128783226013184,
        "learning_rate": 0.00017454099645840718,
        "epoch": 0.824,
        "step": 6180
    },
    {
        "loss": 2.2726,
        "grad_norm": 3.6717066764831543,
        "learning_rate": 0.00017449902766451597,
        "epoch": 0.8241333333333334,
        "step": 6181
    },
    {
        "loss": 1.2663,
        "grad_norm": 3.8235361576080322,
        "learning_rate": 0.0001744570293613061,
        "epoch": 0.8242666666666667,
        "step": 6182
    },
    {
        "loss": 2.6119,
        "grad_norm": 2.197252035140991,
        "learning_rate": 0.00017441500156541314,
        "epoch": 0.8244,
        "step": 6183
    },
    {
        "loss": 2.1362,
        "grad_norm": 3.4738948345184326,
        "learning_rate": 0.00017437294429348453,
        "epoch": 0.8245333333333333,
        "step": 6184
    },
    {
        "loss": 0.7735,
        "grad_norm": 3.286893129348755,
        "learning_rate": 0.00017433085756217925,
        "epoch": 0.8246666666666667,
        "step": 6185
    },
    {
        "loss": 2.4159,
        "grad_norm": 3.013777732849121,
        "learning_rate": 0.00017428874138816807,
        "epoch": 0.8248,
        "step": 6186
    },
    {
        "loss": 2.3024,
        "grad_norm": 1.995382308959961,
        "learning_rate": 0.00017424659578813316,
        "epoch": 0.8249333333333333,
        "step": 6187
    },
    {
        "loss": 2.3227,
        "grad_norm": 2.5955727100372314,
        "learning_rate": 0.00017420442077876878,
        "epoch": 0.8250666666666666,
        "step": 6188
    },
    {
        "loss": 2.1296,
        "grad_norm": 3.580483913421631,
        "learning_rate": 0.00017416221637678043,
        "epoch": 0.8252,
        "step": 6189
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.3556268215179443,
        "learning_rate": 0.0001741199825988855,
        "epoch": 0.8253333333333334,
        "step": 6190
    },
    {
        "loss": 1.922,
        "grad_norm": 4.515384674072266,
        "learning_rate": 0.00017407771946181293,
        "epoch": 0.8254666666666667,
        "step": 6191
    },
    {
        "loss": 1.7093,
        "grad_norm": 3.8928682804107666,
        "learning_rate": 0.00017403542698230323,
        "epoch": 0.8256,
        "step": 6192
    },
    {
        "loss": 2.044,
        "grad_norm": 3.5163557529449463,
        "learning_rate": 0.00017399310517710864,
        "epoch": 0.8257333333333333,
        "step": 6193
    },
    {
        "loss": 2.5227,
        "grad_norm": 3.591308116912842,
        "learning_rate": 0.00017395075406299296,
        "epoch": 0.8258666666666666,
        "step": 6194
    },
    {
        "loss": 2.3108,
        "grad_norm": 4.0658369064331055,
        "learning_rate": 0.00017390837365673166,
        "epoch": 0.826,
        "step": 6195
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.783660411834717,
        "learning_rate": 0.00017386596397511164,
        "epoch": 0.8261333333333334,
        "step": 6196
    },
    {
        "loss": 2.6778,
        "grad_norm": 5.291041374206543,
        "learning_rate": 0.0001738235250349316,
        "epoch": 0.8262666666666667,
        "step": 6197
    },
    {
        "loss": 2.5101,
        "grad_norm": 2.6317994594573975,
        "learning_rate": 0.0001737810568530018,
        "epoch": 0.8264,
        "step": 6198
    },
    {
        "loss": 2.1501,
        "grad_norm": 3.6622314453125,
        "learning_rate": 0.00017373855944614397,
        "epoch": 0.8265333333333333,
        "step": 6199
    },
    {
        "loss": 2.0348,
        "grad_norm": 3.823181390762329,
        "learning_rate": 0.00017369603283119145,
        "epoch": 0.8266666666666667,
        "step": 6200
    },
    {
        "loss": 1.9814,
        "grad_norm": 2.9360859394073486,
        "learning_rate": 0.00017365347702498926,
        "epoch": 0.8268,
        "step": 6201
    },
    {
        "loss": 2.7039,
        "grad_norm": 2.237478494644165,
        "learning_rate": 0.00017361089204439382,
        "epoch": 0.8269333333333333,
        "step": 6202
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.97918963432312,
        "learning_rate": 0.00017356827790627322,
        "epoch": 0.8270666666666666,
        "step": 6203
    },
    {
        "loss": 2.0936,
        "grad_norm": 3.608834981918335,
        "learning_rate": 0.0001735256346275071,
        "epoch": 0.8272,
        "step": 6204
    },
    {
        "loss": 2.4386,
        "grad_norm": 3.2838807106018066,
        "learning_rate": 0.00017348296222498651,
        "epoch": 0.8273333333333334,
        "step": 6205
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.7535276412963867,
        "learning_rate": 0.0001734402607156142,
        "epoch": 0.8274666666666667,
        "step": 6206
    },
    {
        "loss": 2.5892,
        "grad_norm": 3.5634541511535645,
        "learning_rate": 0.00017339753011630447,
        "epoch": 0.8276,
        "step": 6207
    },
    {
        "loss": 2.3742,
        "grad_norm": 3.4208784103393555,
        "learning_rate": 0.00017335477044398288,
        "epoch": 0.8277333333333333,
        "step": 6208
    },
    {
        "loss": 1.3072,
        "grad_norm": 4.691075801849365,
        "learning_rate": 0.00017331198171558683,
        "epoch": 0.8278666666666666,
        "step": 6209
    },
    {
        "loss": 2.2681,
        "grad_norm": 4.002610206604004,
        "learning_rate": 0.00017326916394806493,
        "epoch": 0.828,
        "step": 6210
    },
    {
        "loss": 1.4414,
        "grad_norm": 2.726426839828491,
        "learning_rate": 0.00017322631715837762,
        "epoch": 0.8281333333333334,
        "step": 6211
    },
    {
        "loss": 1.8296,
        "grad_norm": 3.2810428142547607,
        "learning_rate": 0.00017318344136349646,
        "epoch": 0.8282666666666667,
        "step": 6212
    },
    {
        "loss": 1.791,
        "grad_norm": 3.440034866333008,
        "learning_rate": 0.00017314053658040493,
        "epoch": 0.8284,
        "step": 6213
    },
    {
        "loss": 1.9759,
        "grad_norm": 3.7308921813964844,
        "learning_rate": 0.00017309760282609754,
        "epoch": 0.8285333333333333,
        "step": 6214
    },
    {
        "loss": 2.1049,
        "grad_norm": 3.641634702682495,
        "learning_rate": 0.00017305464011758067,
        "epoch": 0.8286666666666667,
        "step": 6215
    },
    {
        "loss": 2.613,
        "grad_norm": 3.9502644538879395,
        "learning_rate": 0.0001730116484718719,
        "epoch": 0.8288,
        "step": 6216
    },
    {
        "loss": 2.0092,
        "grad_norm": 3.1406314373016357,
        "learning_rate": 0.00017296862790600038,
        "epoch": 0.8289333333333333,
        "step": 6217
    },
    {
        "loss": 2.8301,
        "grad_norm": 2.81864333152771,
        "learning_rate": 0.00017292557843700675,
        "epoch": 0.8290666666666666,
        "step": 6218
    },
    {
        "loss": 1.653,
        "grad_norm": 3.5894062519073486,
        "learning_rate": 0.00017288250008194297,
        "epoch": 0.8292,
        "step": 6219
    },
    {
        "loss": 2.893,
        "grad_norm": 4.114327430725098,
        "learning_rate": 0.0001728393928578726,
        "epoch": 0.8293333333333334,
        "step": 6220
    },
    {
        "loss": 2.2219,
        "grad_norm": 2.375197649002075,
        "learning_rate": 0.00017279625678187044,
        "epoch": 0.8294666666666667,
        "step": 6221
    },
    {
        "loss": 2.164,
        "grad_norm": 3.2021427154541016,
        "learning_rate": 0.000172753091871023,
        "epoch": 0.8296,
        "step": 6222
    },
    {
        "loss": 2.6255,
        "grad_norm": 3.5483691692352295,
        "learning_rate": 0.00017270989814242795,
        "epoch": 0.8297333333333333,
        "step": 6223
    },
    {
        "loss": 2.6876,
        "grad_norm": 4.99651575088501,
        "learning_rate": 0.00017266667561319454,
        "epoch": 0.8298666666666666,
        "step": 6224
    },
    {
        "loss": 2.6975,
        "grad_norm": 3.0364186763763428,
        "learning_rate": 0.00017262342430044325,
        "epoch": 0.83,
        "step": 6225
    },
    {
        "loss": 2.8651,
        "grad_norm": 3.6881184577941895,
        "learning_rate": 0.0001725801442213062,
        "epoch": 0.8301333333333333,
        "step": 6226
    },
    {
        "loss": 2.497,
        "grad_norm": 3.1492459774017334,
        "learning_rate": 0.00017253683539292664,
        "epoch": 0.8302666666666667,
        "step": 6227
    },
    {
        "loss": 2.5152,
        "grad_norm": 3.733410120010376,
        "learning_rate": 0.0001724934978324594,
        "epoch": 0.8304,
        "step": 6228
    },
    {
        "loss": 2.6131,
        "grad_norm": 2.693089008331299,
        "learning_rate": 0.00017245013155707076,
        "epoch": 0.8305333333333333,
        "step": 6229
    },
    {
        "loss": 2.3714,
        "grad_norm": 3.4063971042633057,
        "learning_rate": 0.00017240673658393805,
        "epoch": 0.8306666666666667,
        "step": 6230
    },
    {
        "loss": 3.3001,
        "grad_norm": 2.919037342071533,
        "learning_rate": 0.0001723633129302503,
        "epoch": 0.8308,
        "step": 6231
    },
    {
        "loss": 1.7914,
        "grad_norm": 2.3436686992645264,
        "learning_rate": 0.00017231986061320775,
        "epoch": 0.8309333333333333,
        "step": 6232
    },
    {
        "loss": 2.4262,
        "grad_norm": 3.72904896736145,
        "learning_rate": 0.0001722763796500219,
        "epoch": 0.8310666666666666,
        "step": 6233
    },
    {
        "loss": 2.9764,
        "grad_norm": 3.8254079818725586,
        "learning_rate": 0.0001722328700579159,
        "epoch": 0.8312,
        "step": 6234
    },
    {
        "loss": 3.0706,
        "grad_norm": 2.990086793899536,
        "learning_rate": 0.00017218933185412388,
        "epoch": 0.8313333333333334,
        "step": 6235
    },
    {
        "loss": 1.7616,
        "grad_norm": 2.896618366241455,
        "learning_rate": 0.00017214576505589156,
        "epoch": 0.8314666666666667,
        "step": 6236
    },
    {
        "loss": 2.6361,
        "grad_norm": 3.706139326095581,
        "learning_rate": 0.00017210216968047585,
        "epoch": 0.8316,
        "step": 6237
    },
    {
        "loss": 1.6784,
        "grad_norm": 3.1412055492401123,
        "learning_rate": 0.00017205854574514506,
        "epoch": 0.8317333333333333,
        "step": 6238
    },
    {
        "loss": 1.9656,
        "grad_norm": 4.695096015930176,
        "learning_rate": 0.00017201489326717874,
        "epoch": 0.8318666666666666,
        "step": 6239
    },
    {
        "loss": 2.4884,
        "grad_norm": 2.1691250801086426,
        "learning_rate": 0.00017197121226386784,
        "epoch": 0.832,
        "step": 6240
    },
    {
        "loss": 2.238,
        "grad_norm": 3.206569194793701,
        "learning_rate": 0.00017192750275251456,
        "epoch": 0.8321333333333333,
        "step": 6241
    },
    {
        "loss": 0.8741,
        "grad_norm": 3.0190529823303223,
        "learning_rate": 0.00017188376475043226,
        "epoch": 0.8322666666666667,
        "step": 6242
    },
    {
        "loss": 2.5175,
        "grad_norm": 3.1296074390411377,
        "learning_rate": 0.0001718399982749459,
        "epoch": 0.8324,
        "step": 6243
    },
    {
        "loss": 2.1834,
        "grad_norm": 4.477163791656494,
        "learning_rate": 0.00017179620334339138,
        "epoch": 0.8325333333333333,
        "step": 6244
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.527362108230591,
        "learning_rate": 0.00017175237997311613,
        "epoch": 0.8326666666666667,
        "step": 6245
    },
    {
        "loss": 2.7097,
        "grad_norm": 2.508061170578003,
        "learning_rate": 0.00017170852818147855,
        "epoch": 0.8328,
        "step": 6246
    },
    {
        "loss": 2.3893,
        "grad_norm": 3.365243673324585,
        "learning_rate": 0.00017166464798584876,
        "epoch": 0.8329333333333333,
        "step": 6247
    },
    {
        "loss": 1.0841,
        "grad_norm": 5.351667404174805,
        "learning_rate": 0.00017162073940360764,
        "epoch": 0.8330666666666666,
        "step": 6248
    },
    {
        "loss": 2.9239,
        "grad_norm": 2.7586042881011963,
        "learning_rate": 0.00017157680245214764,
        "epoch": 0.8332,
        "step": 6249
    },
    {
        "loss": 2.0448,
        "grad_norm": 2.2534594535827637,
        "learning_rate": 0.00017153283714887226,
        "epoch": 0.8333333333333334,
        "step": 6250
    },
    {
        "loss": 2.3024,
        "grad_norm": 3.384795665740967,
        "learning_rate": 0.0001714888435111964,
        "epoch": 0.8334666666666667,
        "step": 6251
    },
    {
        "loss": 2.6815,
        "grad_norm": 2.260420322418213,
        "learning_rate": 0.00017144482155654597,
        "epoch": 0.8336,
        "step": 6252
    },
    {
        "loss": 2.3968,
        "grad_norm": 2.556246280670166,
        "learning_rate": 0.0001714007713023583,
        "epoch": 0.8337333333333333,
        "step": 6253
    },
    {
        "loss": 3.1513,
        "grad_norm": 3.472832202911377,
        "learning_rate": 0.00017135669276608184,
        "epoch": 0.8338666666666666,
        "step": 6254
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.972191095352173,
        "learning_rate": 0.00017131258596517622,
        "epoch": 0.834,
        "step": 6255
    },
    {
        "loss": 2.1253,
        "grad_norm": 1.882163405418396,
        "learning_rate": 0.00017126845091711227,
        "epoch": 0.8341333333333333,
        "step": 6256
    },
    {
        "loss": 2.4024,
        "grad_norm": 3.1052019596099854,
        "learning_rate": 0.0001712242876393721,
        "epoch": 0.8342666666666667,
        "step": 6257
    },
    {
        "loss": 3.4623,
        "grad_norm": 2.712932825088501,
        "learning_rate": 0.00017118009614944894,
        "epoch": 0.8344,
        "step": 6258
    },
    {
        "loss": 2.7737,
        "grad_norm": 2.1168508529663086,
        "learning_rate": 0.0001711358764648471,
        "epoch": 0.8345333333333333,
        "step": 6259
    },
    {
        "loss": 2.8852,
        "grad_norm": 1.8473070859909058,
        "learning_rate": 0.00017109162860308227,
        "epoch": 0.8346666666666667,
        "step": 6260
    },
    {
        "loss": 2.6842,
        "grad_norm": 3.2566287517547607,
        "learning_rate": 0.00017104735258168105,
        "epoch": 0.8348,
        "step": 6261
    },
    {
        "loss": 2.3319,
        "grad_norm": 2.4029393196105957,
        "learning_rate": 0.00017100304841818139,
        "epoch": 0.8349333333333333,
        "step": 6262
    },
    {
        "loss": 2.6391,
        "grad_norm": 3.8214352130889893,
        "learning_rate": 0.00017095871613013238,
        "epoch": 0.8350666666666666,
        "step": 6263
    },
    {
        "loss": 2.1154,
        "grad_norm": 3.155977964401245,
        "learning_rate": 0.00017091435573509406,
        "epoch": 0.8352,
        "step": 6264
    },
    {
        "loss": 2.6413,
        "grad_norm": 2.9419164657592773,
        "learning_rate": 0.0001708699672506378,
        "epoch": 0.8353333333333334,
        "step": 6265
    },
    {
        "loss": 1.6132,
        "grad_norm": 4.543820381164551,
        "learning_rate": 0.00017082555069434607,
        "epoch": 0.8354666666666667,
        "step": 6266
    },
    {
        "loss": 2.7378,
        "grad_norm": 2.9351205825805664,
        "learning_rate": 0.00017078110608381234,
        "epoch": 0.8356,
        "step": 6267
    },
    {
        "loss": 1.5264,
        "grad_norm": 4.817713260650635,
        "learning_rate": 0.00017073663343664136,
        "epoch": 0.8357333333333333,
        "step": 6268
    },
    {
        "loss": 2.5679,
        "grad_norm": 2.5282535552978516,
        "learning_rate": 0.00017069213277044878,
        "epoch": 0.8358666666666666,
        "step": 6269
    },
    {
        "loss": 2.7064,
        "grad_norm": 3.3726110458374023,
        "learning_rate": 0.0001706476041028616,
        "epoch": 0.836,
        "step": 6270
    },
    {
        "loss": 2.8241,
        "grad_norm": 2.1832330226898193,
        "learning_rate": 0.0001706030474515176,
        "epoch": 0.8361333333333333,
        "step": 6271
    },
    {
        "loss": 2.4553,
        "grad_norm": 3.185472249984741,
        "learning_rate": 0.000170558462834066,
        "epoch": 0.8362666666666667,
        "step": 6272
    },
    {
        "loss": 2.178,
        "grad_norm": 2.0733728408813477,
        "learning_rate": 0.00017051385026816687,
        "epoch": 0.8364,
        "step": 6273
    },
    {
        "loss": 1.8042,
        "grad_norm": 4.3111443519592285,
        "learning_rate": 0.00017046920977149138,
        "epoch": 0.8365333333333334,
        "step": 6274
    },
    {
        "loss": 2.5422,
        "grad_norm": 3.6539223194122314,
        "learning_rate": 0.00017042454136172172,
        "epoch": 0.8366666666666667,
        "step": 6275
    },
    {
        "loss": 0.8941,
        "grad_norm": 3.24068284034729,
        "learning_rate": 0.00017037984505655134,
        "epoch": 0.8368,
        "step": 6276
    },
    {
        "loss": 1.6276,
        "grad_norm": 3.886306047439575,
        "learning_rate": 0.0001703351208736845,
        "epoch": 0.8369333333333333,
        "step": 6277
    },
    {
        "loss": 2.6347,
        "grad_norm": 3.541612148284912,
        "learning_rate": 0.00017029036883083656,
        "epoch": 0.8370666666666666,
        "step": 6278
    },
    {
        "loss": 2.1745,
        "grad_norm": 3.132876396179199,
        "learning_rate": 0.00017024558894573408,
        "epoch": 0.8372,
        "step": 6279
    },
    {
        "loss": 2.2052,
        "grad_norm": 2.9681220054626465,
        "learning_rate": 0.0001702007812361144,
        "epoch": 0.8373333333333334,
        "step": 6280
    },
    {
        "loss": 2.7305,
        "grad_norm": 4.009664058685303,
        "learning_rate": 0.0001701559457197261,
        "epoch": 0.8374666666666667,
        "step": 6281
    },
    {
        "loss": 2.7389,
        "grad_norm": 2.4968273639678955,
        "learning_rate": 0.00017011108241432864,
        "epoch": 0.8376,
        "step": 6282
    },
    {
        "loss": 2.519,
        "grad_norm": 4.055601119995117,
        "learning_rate": 0.00017006619133769262,
        "epoch": 0.8377333333333333,
        "step": 6283
    },
    {
        "loss": 2.1113,
        "grad_norm": 3.6543235778808594,
        "learning_rate": 0.00017002127250759935,
        "epoch": 0.8378666666666666,
        "step": 6284
    },
    {
        "loss": 2.5548,
        "grad_norm": 5.146503925323486,
        "learning_rate": 0.00016997632594184156,
        "epoch": 0.838,
        "step": 6285
    },
    {
        "loss": 1.8645,
        "grad_norm": 3.550489664077759,
        "learning_rate": 0.00016993135165822257,
        "epoch": 0.8381333333333333,
        "step": 6286
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.1529552936553955,
        "learning_rate": 0.00016988634967455687,
        "epoch": 0.8382666666666667,
        "step": 6287
    },
    {
        "loss": 3.1837,
        "grad_norm": 2.7347934246063232,
        "learning_rate": 0.00016984132000866998,
        "epoch": 0.8384,
        "step": 6288
    },
    {
        "loss": 2.963,
        "grad_norm": 2.035079002380371,
        "learning_rate": 0.00016979626267839824,
        "epoch": 0.8385333333333334,
        "step": 6289
    },
    {
        "loss": 0.9068,
        "grad_norm": 2.9579927921295166,
        "learning_rate": 0.00016975117770158898,
        "epoch": 0.8386666666666667,
        "step": 6290
    },
    {
        "loss": 2.035,
        "grad_norm": 4.263534069061279,
        "learning_rate": 0.00016970606509610065,
        "epoch": 0.8388,
        "step": 6291
    },
    {
        "loss": 2.3759,
        "grad_norm": 2.5435993671417236,
        "learning_rate": 0.00016966092487980235,
        "epoch": 0.8389333333333333,
        "step": 6292
    },
    {
        "loss": 1.3966,
        "grad_norm": 3.244594097137451,
        "learning_rate": 0.0001696157570705744,
        "epoch": 0.8390666666666666,
        "step": 6293
    },
    {
        "loss": 1.6488,
        "grad_norm": 3.3542630672454834,
        "learning_rate": 0.00016957056168630783,
        "epoch": 0.8392,
        "step": 6294
    },
    {
        "loss": 2.9115,
        "grad_norm": 3.2515666484832764,
        "learning_rate": 0.00016952533874490478,
        "epoch": 0.8393333333333334,
        "step": 6295
    },
    {
        "loss": 2.7162,
        "grad_norm": 2.480031967163086,
        "learning_rate": 0.000169480088264278,
        "epoch": 0.8394666666666667,
        "step": 6296
    },
    {
        "loss": 2.8201,
        "grad_norm": 3.0909934043884277,
        "learning_rate": 0.00016943481026235164,
        "epoch": 0.8396,
        "step": 6297
    },
    {
        "loss": 2.6491,
        "grad_norm": 3.0952436923980713,
        "learning_rate": 0.0001693895047570603,
        "epoch": 0.8397333333333333,
        "step": 6298
    },
    {
        "loss": 1.408,
        "grad_norm": 7.015831470489502,
        "learning_rate": 0.00016934417176634966,
        "epoch": 0.8398666666666667,
        "step": 6299
    },
    {
        "loss": 2.2581,
        "grad_norm": 2.3848989009857178,
        "learning_rate": 0.00016929881130817637,
        "epoch": 0.84,
        "step": 6300
    },
    {
        "loss": 3.2987,
        "grad_norm": 3.6191704273223877,
        "learning_rate": 0.00016925342340050773,
        "epoch": 0.8401333333333333,
        "step": 6301
    },
    {
        "loss": 2.0645,
        "grad_norm": 2.583664894104004,
        "learning_rate": 0.00016920800806132218,
        "epoch": 0.8402666666666667,
        "step": 6302
    },
    {
        "loss": 1.7189,
        "grad_norm": 3.1594507694244385,
        "learning_rate": 0.00016916256530860873,
        "epoch": 0.8404,
        "step": 6303
    },
    {
        "loss": 2.1476,
        "grad_norm": 3.79813814163208,
        "learning_rate": 0.00016911709516036756,
        "epoch": 0.8405333333333334,
        "step": 6304
    },
    {
        "loss": 2.4534,
        "grad_norm": 5.344865798950195,
        "learning_rate": 0.00016907159763460938,
        "epoch": 0.8406666666666667,
        "step": 6305
    },
    {
        "loss": 1.258,
        "grad_norm": 4.0333027839660645,
        "learning_rate": 0.00016902607274935614,
        "epoch": 0.8408,
        "step": 6306
    },
    {
        "loss": 3.2789,
        "grad_norm": 5.262148857116699,
        "learning_rate": 0.00016898052052264023,
        "epoch": 0.8409333333333333,
        "step": 6307
    },
    {
        "loss": 3.0549,
        "grad_norm": 3.117762804031372,
        "learning_rate": 0.00016893494097250513,
        "epoch": 0.8410666666666666,
        "step": 6308
    },
    {
        "loss": 1.039,
        "grad_norm": 5.165889263153076,
        "learning_rate": 0.00016888933411700498,
        "epoch": 0.8412,
        "step": 6309
    },
    {
        "loss": 2.6529,
        "grad_norm": 3.1205928325653076,
        "learning_rate": 0.0001688436999742049,
        "epoch": 0.8413333333333334,
        "step": 6310
    },
    {
        "loss": 1.812,
        "grad_norm": 3.4061005115509033,
        "learning_rate": 0.00016879803856218066,
        "epoch": 0.8414666666666667,
        "step": 6311
    },
    {
        "loss": 2.6841,
        "grad_norm": 3.3821847438812256,
        "learning_rate": 0.0001687523498990189,
        "epoch": 0.8416,
        "step": 6312
    },
    {
        "loss": 2.0437,
        "grad_norm": 3.932129383087158,
        "learning_rate": 0.0001687066340028171,
        "epoch": 0.8417333333333333,
        "step": 6313
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.2172627449035645,
        "learning_rate": 0.00016866089089168345,
        "epoch": 0.8418666666666667,
        "step": 6314
    },
    {
        "loss": 2.9401,
        "grad_norm": 3.3573660850524902,
        "learning_rate": 0.00016861512058373692,
        "epoch": 0.842,
        "step": 6315
    },
    {
        "loss": 2.3118,
        "grad_norm": 4.390988826751709,
        "learning_rate": 0.0001685693230971074,
        "epoch": 0.8421333333333333,
        "step": 6316
    },
    {
        "loss": 2.2379,
        "grad_norm": 3.9374895095825195,
        "learning_rate": 0.0001685234984499353,
        "epoch": 0.8422666666666667,
        "step": 6317
    },
    {
        "loss": 1.4893,
        "grad_norm": 3.3084030151367188,
        "learning_rate": 0.000168477646660372,
        "epoch": 0.8424,
        "step": 6318
    },
    {
        "loss": 3.0634,
        "grad_norm": 2.0550918579101562,
        "learning_rate": 0.00016843176774657955,
        "epoch": 0.8425333333333334,
        "step": 6319
    },
    {
        "loss": 2.9039,
        "grad_norm": 3.9927260875701904,
        "learning_rate": 0.0001683858617267307,
        "epoch": 0.8426666666666667,
        "step": 6320
    },
    {
        "loss": 2.4396,
        "grad_norm": 2.8771378993988037,
        "learning_rate": 0.00016833992861900896,
        "epoch": 0.8428,
        "step": 6321
    },
    {
        "loss": 2.7879,
        "grad_norm": 3.358344554901123,
        "learning_rate": 0.00016829396844160872,
        "epoch": 0.8429333333333333,
        "step": 6322
    },
    {
        "loss": 2.1071,
        "grad_norm": 2.973482370376587,
        "learning_rate": 0.0001682479812127348,
        "epoch": 0.8430666666666666,
        "step": 6323
    },
    {
        "loss": 2.3995,
        "grad_norm": 3.6550028324127197,
        "learning_rate": 0.00016820196695060299,
        "epoch": 0.8432,
        "step": 6324
    },
    {
        "loss": 1.559,
        "grad_norm": 4.7642035484313965,
        "learning_rate": 0.0001681559256734397,
        "epoch": 0.8433333333333334,
        "step": 6325
    },
    {
        "loss": 0.9157,
        "grad_norm": 4.449399948120117,
        "learning_rate": 0.00016810985739948198,
        "epoch": 0.8434666666666667,
        "step": 6326
    },
    {
        "loss": 2.9132,
        "grad_norm": 3.2230441570281982,
        "learning_rate": 0.0001680637621469777,
        "epoch": 0.8436,
        "step": 6327
    },
    {
        "loss": 2.1563,
        "grad_norm": 2.297203779220581,
        "learning_rate": 0.0001680176399341853,
        "epoch": 0.8437333333333333,
        "step": 6328
    },
    {
        "loss": 1.3624,
        "grad_norm": 3.4376113414764404,
        "learning_rate": 0.00016797149077937398,
        "epoch": 0.8438666666666667,
        "step": 6329
    },
    {
        "loss": 2.072,
        "grad_norm": 3.9930167198181152,
        "learning_rate": 0.00016792531470082345,
        "epoch": 0.844,
        "step": 6330
    },
    {
        "loss": 2.2879,
        "grad_norm": 3.4150731563568115,
        "learning_rate": 0.00016787911171682444,
        "epoch": 0.8441333333333333,
        "step": 6331
    },
    {
        "loss": 1.8061,
        "grad_norm": 4.142464637756348,
        "learning_rate": 0.00016783288184567794,
        "epoch": 0.8442666666666667,
        "step": 6332
    },
    {
        "loss": 3.0674,
        "grad_norm": 2.0102131366729736,
        "learning_rate": 0.00016778662510569584,
        "epoch": 0.8444,
        "step": 6333
    },
    {
        "loss": 2.2738,
        "grad_norm": 3.385193109512329,
        "learning_rate": 0.0001677403415152005,
        "epoch": 0.8445333333333334,
        "step": 6334
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.514871597290039,
        "learning_rate": 0.00016769403109252512,
        "epoch": 0.8446666666666667,
        "step": 6335
    },
    {
        "loss": 2.3024,
        "grad_norm": 2.605973482131958,
        "learning_rate": 0.0001676476938560133,
        "epoch": 0.8448,
        "step": 6336
    },
    {
        "loss": 2.2551,
        "grad_norm": 4.20568323135376,
        "learning_rate": 0.00016760132982401947,
        "epoch": 0.8449333333333333,
        "step": 6337
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.755190372467041,
        "learning_rate": 0.00016755493901490857,
        "epoch": 0.8450666666666666,
        "step": 6338
    },
    {
        "loss": 2.2236,
        "grad_norm": 2.2682533264160156,
        "learning_rate": 0.00016750852144705608,
        "epoch": 0.8452,
        "step": 6339
    },
    {
        "loss": 2.6958,
        "grad_norm": 1.459526777267456,
        "learning_rate": 0.0001674620771388483,
        "epoch": 0.8453333333333334,
        "step": 6340
    },
    {
        "loss": 2.199,
        "grad_norm": 2.554706573486328,
        "learning_rate": 0.00016741560610868182,
        "epoch": 0.8454666666666667,
        "step": 6341
    },
    {
        "loss": 2.0815,
        "grad_norm": 4.333641529083252,
        "learning_rate": 0.00016736910837496412,
        "epoch": 0.8456,
        "step": 6342
    },
    {
        "loss": 2.2459,
        "grad_norm": 3.47965145111084,
        "learning_rate": 0.00016732258395611298,
        "epoch": 0.8457333333333333,
        "step": 6343
    },
    {
        "loss": 2.7874,
        "grad_norm": 3.058042049407959,
        "learning_rate": 0.00016727603287055697,
        "epoch": 0.8458666666666667,
        "step": 6344
    },
    {
        "loss": 2.3801,
        "grad_norm": 3.340939998626709,
        "learning_rate": 0.0001672294551367351,
        "epoch": 0.846,
        "step": 6345
    },
    {
        "loss": 2.04,
        "grad_norm": 3.8129186630249023,
        "learning_rate": 0.00016718285077309695,
        "epoch": 0.8461333333333333,
        "step": 6346
    },
    {
        "loss": 1.8685,
        "grad_norm": 4.269987106323242,
        "learning_rate": 0.0001671362197981027,
        "epoch": 0.8462666666666666,
        "step": 6347
    },
    {
        "loss": 2.7052,
        "grad_norm": 3.0593602657318115,
        "learning_rate": 0.00016708956223022303,
        "epoch": 0.8464,
        "step": 6348
    },
    {
        "loss": 2.2916,
        "grad_norm": 3.398604154586792,
        "learning_rate": 0.00016704287808793912,
        "epoch": 0.8465333333333334,
        "step": 6349
    },
    {
        "loss": 2.1857,
        "grad_norm": 3.2096073627471924,
        "learning_rate": 0.00016699616738974284,
        "epoch": 0.8466666666666667,
        "step": 6350
    },
    {
        "loss": 2.6528,
        "grad_norm": 2.5657737255096436,
        "learning_rate": 0.0001669494301541363,
        "epoch": 0.8468,
        "step": 6351
    },
    {
        "loss": 1.201,
        "grad_norm": 7.16999626159668,
        "learning_rate": 0.00016690266639963244,
        "epoch": 0.8469333333333333,
        "step": 6352
    },
    {
        "loss": 2.5301,
        "grad_norm": 4.1054463386535645,
        "learning_rate": 0.00016685587614475436,
        "epoch": 0.8470666666666666,
        "step": 6353
    },
    {
        "loss": 1.7714,
        "grad_norm": 4.527070045471191,
        "learning_rate": 0.000166809059408036,
        "epoch": 0.8472,
        "step": 6354
    },
    {
        "loss": 2.644,
        "grad_norm": 3.733330011367798,
        "learning_rate": 0.00016676221620802144,
        "epoch": 0.8473333333333334,
        "step": 6355
    },
    {
        "loss": 3.0133,
        "grad_norm": 2.1210830211639404,
        "learning_rate": 0.0001667153465632657,
        "epoch": 0.8474666666666667,
        "step": 6356
    },
    {
        "loss": 2.288,
        "grad_norm": 2.039687156677246,
        "learning_rate": 0.00016666845049233376,
        "epoch": 0.8476,
        "step": 6357
    },
    {
        "loss": 4.0042,
        "grad_norm": 3.4827098846435547,
        "learning_rate": 0.00016662152801380145,
        "epoch": 0.8477333333333333,
        "step": 6358
    },
    {
        "loss": 2.1832,
        "grad_norm": 4.346560478210449,
        "learning_rate": 0.00016657457914625491,
        "epoch": 0.8478666666666667,
        "step": 6359
    },
    {
        "loss": 2.3633,
        "grad_norm": 3.6902964115142822,
        "learning_rate": 0.00016652760390829067,
        "epoch": 0.848,
        "step": 6360
    },
    {
        "loss": 0.7066,
        "grad_norm": 2.933133125305176,
        "learning_rate": 0.0001664806023185159,
        "epoch": 0.8481333333333333,
        "step": 6361
    },
    {
        "loss": 2.3216,
        "grad_norm": 3.2659709453582764,
        "learning_rate": 0.00016643357439554798,
        "epoch": 0.8482666666666666,
        "step": 6362
    },
    {
        "loss": 0.773,
        "grad_norm": 6.492038726806641,
        "learning_rate": 0.00016638652015801492,
        "epoch": 0.8484,
        "step": 6363
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.653341054916382,
        "learning_rate": 0.0001663394396245549,
        "epoch": 0.8485333333333334,
        "step": 6364
    },
    {
        "loss": 2.914,
        "grad_norm": 3.984790325164795,
        "learning_rate": 0.00016629233281381694,
        "epoch": 0.8486666666666667,
        "step": 6365
    },
    {
        "loss": 2.659,
        "grad_norm": 4.372334957122803,
        "learning_rate": 0.00016624519974446,
        "epoch": 0.8488,
        "step": 6366
    },
    {
        "loss": 2.9408,
        "grad_norm": 2.348494052886963,
        "learning_rate": 0.00016619804043515376,
        "epoch": 0.8489333333333333,
        "step": 6367
    },
    {
        "loss": 1.8495,
        "grad_norm": 2.3176395893096924,
        "learning_rate": 0.00016615085490457806,
        "epoch": 0.8490666666666666,
        "step": 6368
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.153088092803955,
        "learning_rate": 0.00016610364317142342,
        "epoch": 0.8492,
        "step": 6369
    },
    {
        "loss": 2.2321,
        "grad_norm": 2.3307101726531982,
        "learning_rate": 0.0001660564052543904,
        "epoch": 0.8493333333333334,
        "step": 6370
    },
    {
        "loss": 2.6083,
        "grad_norm": 4.9851508140563965,
        "learning_rate": 0.0001660091411721902,
        "epoch": 0.8494666666666667,
        "step": 6371
    },
    {
        "loss": 2.7423,
        "grad_norm": 4.638516902923584,
        "learning_rate": 0.0001659618509435443,
        "epoch": 0.8496,
        "step": 6372
    },
    {
        "loss": 2.0311,
        "grad_norm": 3.4680192470550537,
        "learning_rate": 0.0001659145345871844,
        "epoch": 0.8497333333333333,
        "step": 6373
    },
    {
        "loss": 2.7253,
        "grad_norm": 3.3212854862213135,
        "learning_rate": 0.0001658671921218528,
        "epoch": 0.8498666666666667,
        "step": 6374
    },
    {
        "loss": 0.6488,
        "grad_norm": 2.91828989982605,
        "learning_rate": 0.00016581982356630203,
        "epoch": 0.85,
        "step": 6375
    },
    {
        "loss": 1.9399,
        "grad_norm": 4.4483561515808105,
        "learning_rate": 0.0001657724289392948,
        "epoch": 0.8501333333333333,
        "step": 6376
    },
    {
        "loss": 1.7689,
        "grad_norm": 5.130101680755615,
        "learning_rate": 0.00016572500825960438,
        "epoch": 0.8502666666666666,
        "step": 6377
    },
    {
        "loss": 2.7614,
        "grad_norm": 3.190540075302124,
        "learning_rate": 0.0001656775615460142,
        "epoch": 0.8504,
        "step": 6378
    },
    {
        "loss": 2.7006,
        "grad_norm": 3.8911306858062744,
        "learning_rate": 0.0001656300888173181,
        "epoch": 0.8505333333333334,
        "step": 6379
    },
    {
        "loss": 2.4581,
        "grad_norm": 2.735200881958008,
        "learning_rate": 0.00016558259009232017,
        "epoch": 0.8506666666666667,
        "step": 6380
    },
    {
        "loss": 2.4887,
        "grad_norm": 2.8875722885131836,
        "learning_rate": 0.00016553506538983492,
        "epoch": 0.8508,
        "step": 6381
    },
    {
        "loss": 1.9362,
        "grad_norm": 3.911572217941284,
        "learning_rate": 0.00016548751472868692,
        "epoch": 0.8509333333333333,
        "step": 6382
    },
    {
        "loss": 1.012,
        "grad_norm": 3.820622444152832,
        "learning_rate": 0.00016543993812771115,
        "epoch": 0.8510666666666666,
        "step": 6383
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.1459591388702393,
        "learning_rate": 0.00016539233560575298,
        "epoch": 0.8512,
        "step": 6384
    },
    {
        "loss": 2.1943,
        "grad_norm": 3.3165555000305176,
        "learning_rate": 0.0001653447071816678,
        "epoch": 0.8513333333333334,
        "step": 6385
    },
    {
        "loss": 2.1215,
        "grad_norm": 3.7972846031188965,
        "learning_rate": 0.00016529705287432153,
        "epoch": 0.8514666666666667,
        "step": 6386
    },
    {
        "loss": 2.6544,
        "grad_norm": 5.0844340324401855,
        "learning_rate": 0.00016524937270259003,
        "epoch": 0.8516,
        "step": 6387
    },
    {
        "loss": 2.4,
        "grad_norm": 3.3974132537841797,
        "learning_rate": 0.00016520166668535974,
        "epoch": 0.8517333333333333,
        "step": 6388
    },
    {
        "loss": 2.557,
        "grad_norm": 2.953322410583496,
        "learning_rate": 0.00016515393484152702,
        "epoch": 0.8518666666666667,
        "step": 6389
    },
    {
        "loss": 2.4203,
        "grad_norm": 3.2389676570892334,
        "learning_rate": 0.00016510617718999877,
        "epoch": 0.852,
        "step": 6390
    },
    {
        "loss": 2.211,
        "grad_norm": 3.265277862548828,
        "learning_rate": 0.00016505839374969186,
        "epoch": 0.8521333333333333,
        "step": 6391
    },
    {
        "loss": 2.6638,
        "grad_norm": 2.4681904315948486,
        "learning_rate": 0.00016501058453953355,
        "epoch": 0.8522666666666666,
        "step": 6392
    },
    {
        "loss": 2.2782,
        "grad_norm": 3.841325521469116,
        "learning_rate": 0.00016496274957846115,
        "epoch": 0.8524,
        "step": 6393
    },
    {
        "loss": 2.6433,
        "grad_norm": 2.8153607845306396,
        "learning_rate": 0.0001649148888854223,
        "epoch": 0.8525333333333334,
        "step": 6394
    },
    {
        "loss": 2.0305,
        "grad_norm": 3.590662717819214,
        "learning_rate": 0.00016486700247937477,
        "epoch": 0.8526666666666667,
        "step": 6395
    },
    {
        "loss": 1.8425,
        "grad_norm": 4.12310266494751,
        "learning_rate": 0.00016481909037928652,
        "epoch": 0.8528,
        "step": 6396
    },
    {
        "loss": 2.6468,
        "grad_norm": 4.341484069824219,
        "learning_rate": 0.00016477115260413573,
        "epoch": 0.8529333333333333,
        "step": 6397
    },
    {
        "loss": 3.0434,
        "grad_norm": 3.1158130168914795,
        "learning_rate": 0.00016472318917291057,
        "epoch": 0.8530666666666666,
        "step": 6398
    },
    {
        "loss": 2.2622,
        "grad_norm": 4.098012447357178,
        "learning_rate": 0.0001646752001046098,
        "epoch": 0.8532,
        "step": 6399
    },
    {
        "loss": 3.0425,
        "grad_norm": 2.2923717498779297,
        "learning_rate": 0.00016462718541824183,
        "epoch": 0.8533333333333334,
        "step": 6400
    },
    {
        "loss": 2.437,
        "grad_norm": 1.7498723268508911,
        "learning_rate": 0.00016457914513282553,
        "epoch": 0.8534666666666667,
        "step": 6401
    },
    {
        "loss": 2.6721,
        "grad_norm": 4.478693008422852,
        "learning_rate": 0.00016453107926738976,
        "epoch": 0.8536,
        "step": 6402
    },
    {
        "loss": 2.8518,
        "grad_norm": 3.6552913188934326,
        "learning_rate": 0.00016448298784097363,
        "epoch": 0.8537333333333333,
        "step": 6403
    },
    {
        "loss": 2.3085,
        "grad_norm": 3.307154417037964,
        "learning_rate": 0.00016443487087262627,
        "epoch": 0.8538666666666667,
        "step": 6404
    },
    {
        "loss": 1.2094,
        "grad_norm": 3.5516886711120605,
        "learning_rate": 0.00016438672838140698,
        "epoch": 0.854,
        "step": 6405
    },
    {
        "loss": 1.7886,
        "grad_norm": 2.346325635910034,
        "learning_rate": 0.00016433856038638526,
        "epoch": 0.8541333333333333,
        "step": 6406
    },
    {
        "loss": 2.2402,
        "grad_norm": 2.677302598953247,
        "learning_rate": 0.0001642903669066405,
        "epoch": 0.8542666666666666,
        "step": 6407
    },
    {
        "loss": 2.6622,
        "grad_norm": 2.832489252090454,
        "learning_rate": 0.00016424214796126232,
        "epoch": 0.8544,
        "step": 6408
    },
    {
        "loss": 1.2003,
        "grad_norm": 4.340499401092529,
        "learning_rate": 0.0001641939035693505,
        "epoch": 0.8545333333333334,
        "step": 6409
    },
    {
        "loss": 2.549,
        "grad_norm": 3.340449810028076,
        "learning_rate": 0.0001641456337500147,
        "epoch": 0.8546666666666667,
        "step": 6410
    },
    {
        "loss": 2.4008,
        "grad_norm": 4.757845878601074,
        "learning_rate": 0.00016409733852237483,
        "epoch": 0.8548,
        "step": 6411
    },
    {
        "loss": 3.2989,
        "grad_norm": 4.331232070922852,
        "learning_rate": 0.0001640490179055607,
        "epoch": 0.8549333333333333,
        "step": 6412
    },
    {
        "loss": 2.674,
        "grad_norm": 2.3574821949005127,
        "learning_rate": 0.00016400067191871243,
        "epoch": 0.8550666666666666,
        "step": 6413
    },
    {
        "loss": 1.6426,
        "grad_norm": 3.3563804626464844,
        "learning_rate": 0.00016395230058097982,
        "epoch": 0.8552,
        "step": 6414
    },
    {
        "loss": 2.2446,
        "grad_norm": 2.151566982269287,
        "learning_rate": 0.00016390390391152312,
        "epoch": 0.8553333333333333,
        "step": 6415
    },
    {
        "loss": 2.677,
        "grad_norm": 2.475492477416992,
        "learning_rate": 0.0001638554819295123,
        "epoch": 0.8554666666666667,
        "step": 6416
    },
    {
        "loss": 2.5598,
        "grad_norm": 3.5774900913238525,
        "learning_rate": 0.00016380703465412755,
        "epoch": 0.8556,
        "step": 6417
    },
    {
        "loss": 1.4723,
        "grad_norm": 3.735262155532837,
        "learning_rate": 0.00016375856210455888,
        "epoch": 0.8557333333333333,
        "step": 6418
    },
    {
        "loss": 1.4316,
        "grad_norm": 3.9393961429595947,
        "learning_rate": 0.00016371006430000655,
        "epoch": 0.8558666666666667,
        "step": 6419
    },
    {
        "loss": 1.4474,
        "grad_norm": 3.864377975463867,
        "learning_rate": 0.0001636615412596807,
        "epoch": 0.856,
        "step": 6420
    },
    {
        "loss": 2.3691,
        "grad_norm": 3.3784968852996826,
        "learning_rate": 0.00016361299300280137,
        "epoch": 0.8561333333333333,
        "step": 6421
    },
    {
        "loss": 2.2532,
        "grad_norm": 3.499199390411377,
        "learning_rate": 0.00016356441954859885,
        "epoch": 0.8562666666666666,
        "step": 6422
    },
    {
        "loss": 1.9375,
        "grad_norm": 3.767773389816284,
        "learning_rate": 0.00016351582091631302,
        "epoch": 0.8564,
        "step": 6423
    },
    {
        "loss": 2.571,
        "grad_norm": 2.763601064682007,
        "learning_rate": 0.0001634671971251942,
        "epoch": 0.8565333333333334,
        "step": 6424
    },
    {
        "loss": 1.6561,
        "grad_norm": 4.511539459228516,
        "learning_rate": 0.0001634185481945023,
        "epoch": 0.8566666666666667,
        "step": 6425
    },
    {
        "loss": 2.8944,
        "grad_norm": 2.92264723777771,
        "learning_rate": 0.00016336987414350743,
        "epoch": 0.8568,
        "step": 6426
    },
    {
        "loss": 3.0237,
        "grad_norm": 2.8533902168273926,
        "learning_rate": 0.0001633211749914894,
        "epoch": 0.8569333333333333,
        "step": 6427
    },
    {
        "loss": 1.487,
        "grad_norm": 2.75114369392395,
        "learning_rate": 0.00016327245075773824,
        "epoch": 0.8570666666666666,
        "step": 6428
    },
    {
        "loss": 1.3666,
        "grad_norm": 3.57513427734375,
        "learning_rate": 0.0001632237014615537,
        "epoch": 0.8572,
        "step": 6429
    },
    {
        "loss": 2.4322,
        "grad_norm": 3.2139456272125244,
        "learning_rate": 0.00016317492712224563,
        "epoch": 0.8573333333333333,
        "step": 6430
    },
    {
        "loss": 1.3747,
        "grad_norm": 3.7467188835144043,
        "learning_rate": 0.00016312612775913366,
        "epoch": 0.8574666666666667,
        "step": 6431
    },
    {
        "loss": 3.2229,
        "grad_norm": 5.467941761016846,
        "learning_rate": 0.00016307730339154737,
        "epoch": 0.8576,
        "step": 6432
    },
    {
        "loss": 2.7444,
        "grad_norm": 2.97118878364563,
        "learning_rate": 0.00016302845403882624,
        "epoch": 0.8577333333333333,
        "step": 6433
    },
    {
        "loss": 0.8905,
        "grad_norm": 3.91726016998291,
        "learning_rate": 0.0001629795797203198,
        "epoch": 0.8578666666666667,
        "step": 6434
    },
    {
        "loss": 2.6858,
        "grad_norm": 4.580388069152832,
        "learning_rate": 0.00016293068045538712,
        "epoch": 0.858,
        "step": 6435
    },
    {
        "loss": 2.018,
        "grad_norm": 5.682910919189453,
        "learning_rate": 0.00016288175626339756,
        "epoch": 0.8581333333333333,
        "step": 6436
    },
    {
        "loss": 1.9979,
        "grad_norm": 3.956127882003784,
        "learning_rate": 0.00016283280716373005,
        "epoch": 0.8582666666666666,
        "step": 6437
    },
    {
        "loss": 2.6857,
        "grad_norm": 2.6316449642181396,
        "learning_rate": 0.00016278383317577358,
        "epoch": 0.8584,
        "step": 6438
    },
    {
        "loss": 1.3491,
        "grad_norm": 4.024316310882568,
        "learning_rate": 0.0001627348343189267,
        "epoch": 0.8585333333333334,
        "step": 6439
    },
    {
        "loss": 2.9999,
        "grad_norm": 2.4722540378570557,
        "learning_rate": 0.00016268581061259835,
        "epoch": 0.8586666666666667,
        "step": 6440
    },
    {
        "loss": 1.5851,
        "grad_norm": 5.796048641204834,
        "learning_rate": 0.00016263676207620674,
        "epoch": 0.8588,
        "step": 6441
    },
    {
        "loss": 1.6962,
        "grad_norm": 4.484273433685303,
        "learning_rate": 0.00016258768872918022,
        "epoch": 0.8589333333333333,
        "step": 6442
    },
    {
        "loss": 2.3178,
        "grad_norm": 5.403782367706299,
        "learning_rate": 0.00016253859059095702,
        "epoch": 0.8590666666666666,
        "step": 6443
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.764596700668335,
        "learning_rate": 0.00016248946768098493,
        "epoch": 0.8592,
        "step": 6444
    },
    {
        "loss": 2.108,
        "grad_norm": 7.186789512634277,
        "learning_rate": 0.0001624403200187218,
        "epoch": 0.8593333333333333,
        "step": 6445
    },
    {
        "loss": 1.9223,
        "grad_norm": 3.1525135040283203,
        "learning_rate": 0.0001623911476236351,
        "epoch": 0.8594666666666667,
        "step": 6446
    },
    {
        "loss": 2.6801,
        "grad_norm": 3.115438938140869,
        "learning_rate": 0.0001623419505152023,
        "epoch": 0.8596,
        "step": 6447
    },
    {
        "loss": 2.1857,
        "grad_norm": 5.530394554138184,
        "learning_rate": 0.00016229272871291038,
        "epoch": 0.8597333333333333,
        "step": 6448
    },
    {
        "loss": 2.8032,
        "grad_norm": 4.854606628417969,
        "learning_rate": 0.00016224348223625645,
        "epoch": 0.8598666666666667,
        "step": 6449
    },
    {
        "loss": 2.9213,
        "grad_norm": 4.767380237579346,
        "learning_rate": 0.00016219421110474707,
        "epoch": 0.86,
        "step": 6450
    },
    {
        "loss": 1.8473,
        "grad_norm": 3.777069091796875,
        "learning_rate": 0.0001621449153378988,
        "epoch": 0.8601333333333333,
        "step": 6451
    },
    {
        "loss": 2.9495,
        "grad_norm": 3.5912668704986572,
        "learning_rate": 0.00016209559495523776,
        "epoch": 0.8602666666666666,
        "step": 6452
    },
    {
        "loss": 2.4635,
        "grad_norm": 1.921772837638855,
        "learning_rate": 0.00016204624997630002,
        "epoch": 0.8604,
        "step": 6453
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.8190011978149414,
        "learning_rate": 0.0001619968804206312,
        "epoch": 0.8605333333333334,
        "step": 6454
    },
    {
        "loss": 2.9766,
        "grad_norm": 2.514191150665283,
        "learning_rate": 0.00016194748630778678,
        "epoch": 0.8606666666666667,
        "step": 6455
    },
    {
        "loss": 2.7641,
        "grad_norm": 2.8264083862304688,
        "learning_rate": 0.00016189806765733202,
        "epoch": 0.8608,
        "step": 6456
    },
    {
        "loss": 1.0036,
        "grad_norm": 3.583552122116089,
        "learning_rate": 0.00016184862448884171,
        "epoch": 0.8609333333333333,
        "step": 6457
    },
    {
        "loss": 2.2538,
        "grad_norm": 4.138901233673096,
        "learning_rate": 0.00016179915682190048,
        "epoch": 0.8610666666666666,
        "step": 6458
    },
    {
        "loss": 3.1593,
        "grad_norm": 3.211811065673828,
        "learning_rate": 0.00016174966467610264,
        "epoch": 0.8612,
        "step": 6459
    },
    {
        "loss": 2.9749,
        "grad_norm": 3.79714298248291,
        "learning_rate": 0.0001617001480710523,
        "epoch": 0.8613333333333333,
        "step": 6460
    },
    {
        "loss": 2.5924,
        "grad_norm": 3.761779546737671,
        "learning_rate": 0.00016165060702636299,
        "epoch": 0.8614666666666667,
        "step": 6461
    },
    {
        "loss": 2.4635,
        "grad_norm": 2.9315829277038574,
        "learning_rate": 0.00016160104156165824,
        "epoch": 0.8616,
        "step": 6462
    },
    {
        "loss": 1.4171,
        "grad_norm": 5.6296234130859375,
        "learning_rate": 0.00016155145169657095,
        "epoch": 0.8617333333333334,
        "step": 6463
    },
    {
        "loss": 1.22,
        "grad_norm": 4.848447799682617,
        "learning_rate": 0.00016150183745074393,
        "epoch": 0.8618666666666667,
        "step": 6464
    },
    {
        "loss": 2.4289,
        "grad_norm": 2.5502378940582275,
        "learning_rate": 0.0001614521988438296,
        "epoch": 0.862,
        "step": 6465
    },
    {
        "loss": 2.4732,
        "grad_norm": 4.3913798332214355,
        "learning_rate": 0.00016140253589548985,
        "epoch": 0.8621333333333333,
        "step": 6466
    },
    {
        "loss": 2.0555,
        "grad_norm": 3.8615543842315674,
        "learning_rate": 0.00016135284862539638,
        "epoch": 0.8622666666666666,
        "step": 6467
    },
    {
        "loss": 1.8049,
        "grad_norm": 4.65857458114624,
        "learning_rate": 0.00016130313705323062,
        "epoch": 0.8624,
        "step": 6468
    },
    {
        "loss": 2.8806,
        "grad_norm": 3.6809470653533936,
        "learning_rate": 0.0001612534011986833,
        "epoch": 0.8625333333333334,
        "step": 6469
    },
    {
        "loss": 1.6985,
        "grad_norm": 3.369284152984619,
        "learning_rate": 0.0001612036410814551,
        "epoch": 0.8626666666666667,
        "step": 6470
    },
    {
        "loss": 1.6987,
        "grad_norm": 3.2302637100219727,
        "learning_rate": 0.0001611538567212561,
        "epoch": 0.8628,
        "step": 6471
    },
    {
        "loss": 2.5897,
        "grad_norm": 6.0643181800842285,
        "learning_rate": 0.0001611040481378061,
        "epoch": 0.8629333333333333,
        "step": 6472
    },
    {
        "loss": 2.2821,
        "grad_norm": 3.1650331020355225,
        "learning_rate": 0.00016105421535083433,
        "epoch": 0.8630666666666666,
        "step": 6473
    },
    {
        "loss": 2.128,
        "grad_norm": 4.81982421875,
        "learning_rate": 0.00016100435838007992,
        "epoch": 0.8632,
        "step": 6474
    },
    {
        "loss": 1.8338,
        "grad_norm": 3.761415719985962,
        "learning_rate": 0.00016095447724529124,
        "epoch": 0.8633333333333333,
        "step": 6475
    },
    {
        "loss": 1.3484,
        "grad_norm": 3.5322210788726807,
        "learning_rate": 0.00016090457196622651,
        "epoch": 0.8634666666666667,
        "step": 6476
    },
    {
        "loss": 2.074,
        "grad_norm": 3.671415328979492,
        "learning_rate": 0.0001608546425626532,
        "epoch": 0.8636,
        "step": 6477
    },
    {
        "loss": 2.1347,
        "grad_norm": 3.19041109085083,
        "learning_rate": 0.0001608046890543487,
        "epoch": 0.8637333333333334,
        "step": 6478
    },
    {
        "loss": 1.937,
        "grad_norm": 1.6913321018218994,
        "learning_rate": 0.00016075471146109957,
        "epoch": 0.8638666666666667,
        "step": 6479
    },
    {
        "loss": 2.5964,
        "grad_norm": 2.7706058025360107,
        "learning_rate": 0.00016070470980270228,
        "epoch": 0.864,
        "step": 6480
    },
    {
        "loss": 2.034,
        "grad_norm": 4.890678405761719,
        "learning_rate": 0.00016065468409896257,
        "epoch": 0.8641333333333333,
        "step": 6481
    },
    {
        "loss": 2.8724,
        "grad_norm": 4.730910778045654,
        "learning_rate": 0.00016060463436969578,
        "epoch": 0.8642666666666666,
        "step": 6482
    },
    {
        "loss": 1.8305,
        "grad_norm": 3.009101629257202,
        "learning_rate": 0.00016055456063472686,
        "epoch": 0.8644,
        "step": 6483
    },
    {
        "loss": 1.985,
        "grad_norm": 3.8818511962890625,
        "learning_rate": 0.00016050446291389014,
        "epoch": 0.8645333333333334,
        "step": 6484
    },
    {
        "loss": 2.447,
        "grad_norm": 3.7851827144622803,
        "learning_rate": 0.00016045434122702952,
        "epoch": 0.8646666666666667,
        "step": 6485
    },
    {
        "loss": 2.1285,
        "grad_norm": 3.38956356048584,
        "learning_rate": 0.00016040419559399828,
        "epoch": 0.8648,
        "step": 6486
    },
    {
        "loss": 2.4191,
        "grad_norm": 3.6753509044647217,
        "learning_rate": 0.00016035402603465943,
        "epoch": 0.8649333333333333,
        "step": 6487
    },
    {
        "loss": 2.1922,
        "grad_norm": 3.679861068725586,
        "learning_rate": 0.00016030383256888513,
        "epoch": 0.8650666666666667,
        "step": 6488
    },
    {
        "loss": 1.9962,
        "grad_norm": 4.096120834350586,
        "learning_rate": 0.0001602536152165573,
        "epoch": 0.8652,
        "step": 6489
    },
    {
        "loss": 2.5181,
        "grad_norm": 4.313405990600586,
        "learning_rate": 0.0001602033739975672,
        "epoch": 0.8653333333333333,
        "step": 6490
    },
    {
        "loss": 1.5548,
        "grad_norm": 4.018128395080566,
        "learning_rate": 0.00016015310893181548,
        "epoch": 0.8654666666666667,
        "step": 6491
    },
    {
        "loss": 1.06,
        "grad_norm": 3.5781710147857666,
        "learning_rate": 0.00016010282003921234,
        "epoch": 0.8656,
        "step": 6492
    },
    {
        "loss": 2.4334,
        "grad_norm": 3.2715072631835938,
        "learning_rate": 0.00016005250733967745,
        "epoch": 0.8657333333333334,
        "step": 6493
    },
    {
        "loss": 3.3809,
        "grad_norm": 2.055821180343628,
        "learning_rate": 0.0001600021708531397,
        "epoch": 0.8658666666666667,
        "step": 6494
    },
    {
        "loss": 2.6492,
        "grad_norm": 4.709364891052246,
        "learning_rate": 0.00015995181059953772,
        "epoch": 0.866,
        "step": 6495
    },
    {
        "loss": 2.097,
        "grad_norm": 5.466166019439697,
        "learning_rate": 0.0001599014265988192,
        "epoch": 0.8661333333333333,
        "step": 6496
    },
    {
        "loss": 1.8592,
        "grad_norm": 4.12018346786499,
        "learning_rate": 0.00015985101887094154,
        "epoch": 0.8662666666666666,
        "step": 6497
    },
    {
        "loss": 2.7557,
        "grad_norm": 2.176196813583374,
        "learning_rate": 0.00015980058743587128,
        "epoch": 0.8664,
        "step": 6498
    },
    {
        "loss": 2.861,
        "grad_norm": 2.7231597900390625,
        "learning_rate": 0.00015975013231358468,
        "epoch": 0.8665333333333334,
        "step": 6499
    },
    {
        "loss": 1.184,
        "grad_norm": 3.5725672245025635,
        "learning_rate": 0.000159699653524067,
        "epoch": 0.8666666666666667,
        "step": 6500
    },
    {
        "loss": 1.8626,
        "grad_norm": 3.019618034362793,
        "learning_rate": 0.00015964915108731317,
        "epoch": 0.8668,
        "step": 6501
    },
    {
        "loss": 2.4297,
        "grad_norm": 2.8801703453063965,
        "learning_rate": 0.00015959862502332736,
        "epoch": 0.8669333333333333,
        "step": 6502
    },
    {
        "loss": 2.6727,
        "grad_norm": 2.786346673965454,
        "learning_rate": 0.00015954807535212308,
        "epoch": 0.8670666666666667,
        "step": 6503
    },
    {
        "loss": 3.0215,
        "grad_norm": 3.1258926391601562,
        "learning_rate": 0.00015949750209372327,
        "epoch": 0.8672,
        "step": 6504
    },
    {
        "loss": 2.0789,
        "grad_norm": 3.7718987464904785,
        "learning_rate": 0.00015944690526816015,
        "epoch": 0.8673333333333333,
        "step": 6505
    },
    {
        "loss": 2.3837,
        "grad_norm": 3.0887198448181152,
        "learning_rate": 0.00015939628489547528,
        "epoch": 0.8674666666666667,
        "step": 6506
    },
    {
        "loss": 2.7739,
        "grad_norm": 3.3943607807159424,
        "learning_rate": 0.0001593456409957195,
        "epoch": 0.8676,
        "step": 6507
    },
    {
        "loss": 2.6266,
        "grad_norm": 2.3869006633758545,
        "learning_rate": 0.00015929497358895323,
        "epoch": 0.8677333333333334,
        "step": 6508
    },
    {
        "loss": 2.3814,
        "grad_norm": 2.858067512512207,
        "learning_rate": 0.00015924428269524577,
        "epoch": 0.8678666666666667,
        "step": 6509
    },
    {
        "loss": 2.2758,
        "grad_norm": 3.1944594383239746,
        "learning_rate": 0.0001591935683346762,
        "epoch": 0.868,
        "step": 6510
    },
    {
        "loss": 2.3926,
        "grad_norm": 2.678720474243164,
        "learning_rate": 0.0001591428305273324,
        "epoch": 0.8681333333333333,
        "step": 6511
    },
    {
        "loss": 2.5748,
        "grad_norm": 3.3390464782714844,
        "learning_rate": 0.00015909206929331197,
        "epoch": 0.8682666666666666,
        "step": 6512
    },
    {
        "loss": 2.0581,
        "grad_norm": 2.6332666873931885,
        "learning_rate": 0.0001590412846527215,
        "epoch": 0.8684,
        "step": 6513
    },
    {
        "loss": 0.8499,
        "grad_norm": 2.759345293045044,
        "learning_rate": 0.000158990476625677,
        "epoch": 0.8685333333333334,
        "step": 6514
    },
    {
        "loss": 2.0805,
        "grad_norm": 4.780750751495361,
        "learning_rate": 0.00015893964523230373,
        "epoch": 0.8686666666666667,
        "step": 6515
    },
    {
        "loss": 2.3716,
        "grad_norm": 3.1352410316467285,
        "learning_rate": 0.00015888879049273612,
        "epoch": 0.8688,
        "step": 6516
    },
    {
        "loss": 0.6332,
        "grad_norm": 2.291057825088501,
        "learning_rate": 0.00015883791242711793,
        "epoch": 0.8689333333333333,
        "step": 6517
    },
    {
        "loss": 2.7045,
        "grad_norm": 2.460399866104126,
        "learning_rate": 0.00015878701105560223,
        "epoch": 0.8690666666666667,
        "step": 6518
    },
    {
        "loss": 2.5827,
        "grad_norm": 3.644831895828247,
        "learning_rate": 0.00015873608639835104,
        "epoch": 0.8692,
        "step": 6519
    },
    {
        "loss": 2.8735,
        "grad_norm": 2.814517021179199,
        "learning_rate": 0.0001586851384755359,
        "epoch": 0.8693333333333333,
        "step": 6520
    },
    {
        "loss": 2.6772,
        "grad_norm": 3.5479483604431152,
        "learning_rate": 0.0001586341673073375,
        "epoch": 0.8694666666666667,
        "step": 6521
    },
    {
        "loss": 2.2034,
        "grad_norm": 3.553205966949463,
        "learning_rate": 0.0001585831729139456,
        "epoch": 0.8696,
        "step": 6522
    },
    {
        "loss": 2.7574,
        "grad_norm": 2.9740359783172607,
        "learning_rate": 0.0001585321553155593,
        "epoch": 0.8697333333333334,
        "step": 6523
    },
    {
        "loss": 2.6709,
        "grad_norm": 3.299330711364746,
        "learning_rate": 0.00015848111453238686,
        "epoch": 0.8698666666666667,
        "step": 6524
    },
    {
        "loss": 2.4262,
        "grad_norm": 3.2903714179992676,
        "learning_rate": 0.00015843005058464564,
        "epoch": 0.87,
        "step": 6525
    },
    {
        "loss": 2.0189,
        "grad_norm": 3.6514430046081543,
        "learning_rate": 0.00015837896349256227,
        "epoch": 0.8701333333333333,
        "step": 6526
    },
    {
        "loss": 2.8124,
        "grad_norm": 3.3734047412872314,
        "learning_rate": 0.00015832785327637262,
        "epoch": 0.8702666666666666,
        "step": 6527
    },
    {
        "loss": 2.4321,
        "grad_norm": 4.671318531036377,
        "learning_rate": 0.0001582767199563215,
        "epoch": 0.8704,
        "step": 6528
    },
    {
        "loss": 2.8294,
        "grad_norm": 2.15277361869812,
        "learning_rate": 0.00015822556355266304,
        "epoch": 0.8705333333333334,
        "step": 6529
    },
    {
        "loss": 3.0347,
        "grad_norm": 6.529243469238281,
        "learning_rate": 0.00015817438408566043,
        "epoch": 0.8706666666666667,
        "step": 6530
    },
    {
        "loss": 2.7626,
        "grad_norm": 3.282518148422241,
        "learning_rate": 0.00015812318157558614,
        "epoch": 0.8708,
        "step": 6531
    },
    {
        "loss": 1.6662,
        "grad_norm": 3.1205029487609863,
        "learning_rate": 0.00015807195604272141,
        "epoch": 0.8709333333333333,
        "step": 6532
    },
    {
        "loss": 2.7344,
        "grad_norm": 2.4081156253814697,
        "learning_rate": 0.00015802070750735715,
        "epoch": 0.8710666666666667,
        "step": 6533
    },
    {
        "loss": 2.4454,
        "grad_norm": 3.7245254516601562,
        "learning_rate": 0.00015796943598979293,
        "epoch": 0.8712,
        "step": 6534
    },
    {
        "loss": 2.0427,
        "grad_norm": 4.220196723937988,
        "learning_rate": 0.0001579181415103376,
        "epoch": 0.8713333333333333,
        "step": 6535
    },
    {
        "loss": 0.8523,
        "grad_norm": 3.754953622817993,
        "learning_rate": 0.00015786682408930905,
        "epoch": 0.8714666666666666,
        "step": 6536
    },
    {
        "loss": 2.0892,
        "grad_norm": 3.748577833175659,
        "learning_rate": 0.0001578154837470343,
        "epoch": 0.8716,
        "step": 6537
    },
    {
        "loss": 1.9261,
        "grad_norm": 4.152286052703857,
        "learning_rate": 0.0001577641205038494,
        "epoch": 0.8717333333333334,
        "step": 6538
    },
    {
        "loss": 2.3388,
        "grad_norm": 2.393118381500244,
        "learning_rate": 0.00015771273438009958,
        "epoch": 0.8718666666666667,
        "step": 6539
    },
    {
        "loss": 2.6813,
        "grad_norm": 2.64473295211792,
        "learning_rate": 0.00015766132539613904,
        "epoch": 0.872,
        "step": 6540
    },
    {
        "loss": 2.837,
        "grad_norm": 2.6173713207244873,
        "learning_rate": 0.00015760989357233092,
        "epoch": 0.8721333333333333,
        "step": 6541
    },
    {
        "loss": 1.899,
        "grad_norm": 2.858259439468384,
        "learning_rate": 0.00015755843892904778,
        "epoch": 0.8722666666666666,
        "step": 6542
    },
    {
        "loss": 1.7996,
        "grad_norm": 4.260402202606201,
        "learning_rate": 0.0001575069614866708,
        "epoch": 0.8724,
        "step": 6543
    },
    {
        "loss": 1.0755,
        "grad_norm": 3.3859448432922363,
        "learning_rate": 0.00015745546126559048,
        "epoch": 0.8725333333333334,
        "step": 6544
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.9427971839904785,
        "learning_rate": 0.00015740393828620613,
        "epoch": 0.8726666666666667,
        "step": 6545
    },
    {
        "loss": 1.2486,
        "grad_norm": 5.073292255401611,
        "learning_rate": 0.00015735239256892624,
        "epoch": 0.8728,
        "step": 6546
    },
    {
        "loss": 2.6743,
        "grad_norm": 2.4534032344818115,
        "learning_rate": 0.00015730082413416822,
        "epoch": 0.8729333333333333,
        "step": 6547
    },
    {
        "loss": 2.4639,
        "grad_norm": 3.6849143505096436,
        "learning_rate": 0.0001572492330023585,
        "epoch": 0.8730666666666667,
        "step": 6548
    },
    {
        "loss": 2.7001,
        "grad_norm": 4.189270496368408,
        "learning_rate": 0.00015719761919393257,
        "epoch": 0.8732,
        "step": 6549
    },
    {
        "loss": 2.3682,
        "grad_norm": 4.544865131378174,
        "learning_rate": 0.00015714598272933473,
        "epoch": 0.8733333333333333,
        "step": 6550
    },
    {
        "loss": 2.9671,
        "grad_norm": 4.862882614135742,
        "learning_rate": 0.00015709432362901842,
        "epoch": 0.8734666666666666,
        "step": 6551
    },
    {
        "loss": 2.5246,
        "grad_norm": 3.16050386428833,
        "learning_rate": 0.000157042641913446,
        "epoch": 0.8736,
        "step": 6552
    },
    {
        "loss": 2.7827,
        "grad_norm": 2.255601644515991,
        "learning_rate": 0.00015699093760308873,
        "epoch": 0.8737333333333334,
        "step": 6553
    },
    {
        "loss": 2.1318,
        "grad_norm": 2.912766218185425,
        "learning_rate": 0.00015693921071842693,
        "epoch": 0.8738666666666667,
        "step": 6554
    },
    {
        "loss": 1.5201,
        "grad_norm": 4.051262378692627,
        "learning_rate": 0.00015688746127994965,
        "epoch": 0.874,
        "step": 6555
    },
    {
        "loss": 2.6549,
        "grad_norm": 3.3558871746063232,
        "learning_rate": 0.00015683568930815523,
        "epoch": 0.8741333333333333,
        "step": 6556
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.751546621322632,
        "learning_rate": 0.00015678389482355048,
        "epoch": 0.8742666666666666,
        "step": 6557
    },
    {
        "loss": 2.455,
        "grad_norm": 2.798265218734741,
        "learning_rate": 0.0001567320778466516,
        "epoch": 0.8744,
        "step": 6558
    },
    {
        "loss": 1.7885,
        "grad_norm": 3.3634095191955566,
        "learning_rate": 0.00015668023839798334,
        "epoch": 0.8745333333333334,
        "step": 6559
    },
    {
        "loss": 2.7063,
        "grad_norm": 3.835378408432007,
        "learning_rate": 0.0001566283764980795,
        "epoch": 0.8746666666666667,
        "step": 6560
    },
    {
        "loss": 2.7176,
        "grad_norm": 2.835214138031006,
        "learning_rate": 0.00015657649216748286,
        "epoch": 0.8748,
        "step": 6561
    },
    {
        "loss": 3.1537,
        "grad_norm": 6.684776306152344,
        "learning_rate": 0.0001565245854267448,
        "epoch": 0.8749333333333333,
        "step": 6562
    },
    {
        "loss": 1.3152,
        "grad_norm": 3.7478432655334473,
        "learning_rate": 0.00015647265629642593,
        "epoch": 0.8750666666666667,
        "step": 6563
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.644270658493042,
        "learning_rate": 0.0001564207047970954,
        "epoch": 0.8752,
        "step": 6564
    },
    {
        "loss": 2.7184,
        "grad_norm": 3.045250177383423,
        "learning_rate": 0.00015636873094933154,
        "epoch": 0.8753333333333333,
        "step": 6565
    },
    {
        "loss": 2.2942,
        "grad_norm": 2.84342098236084,
        "learning_rate": 0.00015631673477372115,
        "epoch": 0.8754666666666666,
        "step": 6566
    },
    {
        "loss": 2.8179,
        "grad_norm": 3.792066812515259,
        "learning_rate": 0.00015626471629086032,
        "epoch": 0.8756,
        "step": 6567
    },
    {
        "loss": 2.6176,
        "grad_norm": 3.4575717449188232,
        "learning_rate": 0.0001562126755213536,
        "epoch": 0.8757333333333334,
        "step": 6568
    },
    {
        "loss": 1.2968,
        "grad_norm": 3.403712749481201,
        "learning_rate": 0.00015616061248581464,
        "epoch": 0.8758666666666667,
        "step": 6569
    },
    {
        "loss": 3.05,
        "grad_norm": 3.2872965335845947,
        "learning_rate": 0.00015610852720486564,
        "epoch": 0.876,
        "step": 6570
    },
    {
        "loss": 2.123,
        "grad_norm": 2.577678918838501,
        "learning_rate": 0.00015605641969913787,
        "epoch": 0.8761333333333333,
        "step": 6571
    },
    {
        "loss": 2.6762,
        "grad_norm": 3.453240394592285,
        "learning_rate": 0.0001560042899892712,
        "epoch": 0.8762666666666666,
        "step": 6572
    },
    {
        "loss": 2.1785,
        "grad_norm": 3.76149845123291,
        "learning_rate": 0.00015595213809591445,
        "epoch": 0.8764,
        "step": 6573
    },
    {
        "loss": 2.3132,
        "grad_norm": 2.877610445022583,
        "learning_rate": 0.00015589996403972522,
        "epoch": 0.8765333333333334,
        "step": 6574
    },
    {
        "loss": 3.0043,
        "grad_norm": 3.4902234077453613,
        "learning_rate": 0.00015584776784136967,
        "epoch": 0.8766666666666667,
        "step": 6575
    },
    {
        "loss": 1.5364,
        "grad_norm": 4.045229434967041,
        "learning_rate": 0.000155795549521523,
        "epoch": 0.8768,
        "step": 6576
    },
    {
        "loss": 1.358,
        "grad_norm": 4.206567764282227,
        "learning_rate": 0.0001557433091008691,
        "epoch": 0.8769333333333333,
        "step": 6577
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.987210273742676,
        "learning_rate": 0.00015569104660010044,
        "epoch": 0.8770666666666667,
        "step": 6578
    },
    {
        "loss": 2.3087,
        "grad_norm": 4.238413333892822,
        "learning_rate": 0.00015563876203991857,
        "epoch": 0.8772,
        "step": 6579
    },
    {
        "loss": 2.7183,
        "grad_norm": 3.8141419887542725,
        "learning_rate": 0.00015558645544103337,
        "epoch": 0.8773333333333333,
        "step": 6580
    },
    {
        "loss": 3.0828,
        "grad_norm": 2.4954922199249268,
        "learning_rate": 0.00015553412682416378,
        "epoch": 0.8774666666666666,
        "step": 6581
    },
    {
        "loss": 2.7435,
        "grad_norm": 2.440063953399658,
        "learning_rate": 0.0001554817762100373,
        "epoch": 0.8776,
        "step": 6582
    },
    {
        "loss": 2.5874,
        "grad_norm": 3.5281646251678467,
        "learning_rate": 0.0001554294036193903,
        "epoch": 0.8777333333333334,
        "step": 6583
    },
    {
        "loss": 2.6649,
        "grad_norm": 3.1251938343048096,
        "learning_rate": 0.0001553770090729676,
        "epoch": 0.8778666666666667,
        "step": 6584
    },
    {
        "loss": 2.7274,
        "grad_norm": 3.300389289855957,
        "learning_rate": 0.00015532459259152292,
        "epoch": 0.878,
        "step": 6585
    },
    {
        "loss": 2.0528,
        "grad_norm": 3.3685364723205566,
        "learning_rate": 0.00015527215419581863,
        "epoch": 0.8781333333333333,
        "step": 6586
    },
    {
        "loss": 2.5048,
        "grad_norm": 2.100421190261841,
        "learning_rate": 0.0001552196939066257,
        "epoch": 0.8782666666666666,
        "step": 6587
    },
    {
        "loss": 1.8813,
        "grad_norm": 3.9452877044677734,
        "learning_rate": 0.00015516721174472391,
        "epoch": 0.8784,
        "step": 6588
    },
    {
        "loss": 0.8474,
        "grad_norm": 3.445465326309204,
        "learning_rate": 0.0001551147077309015,
        "epoch": 0.8785333333333334,
        "step": 6589
    },
    {
        "loss": 2.25,
        "grad_norm": 3.127795934677124,
        "learning_rate": 0.00015506218188595562,
        "epoch": 0.8786666666666667,
        "step": 6590
    },
    {
        "loss": 1.343,
        "grad_norm": 3.1575183868408203,
        "learning_rate": 0.00015500963423069173,
        "epoch": 0.8788,
        "step": 6591
    },
    {
        "loss": 2.7578,
        "grad_norm": 2.855093479156494,
        "learning_rate": 0.00015495706478592446,
        "epoch": 0.8789333333333333,
        "step": 6592
    },
    {
        "loss": 1.8565,
        "grad_norm": 3.189427137374878,
        "learning_rate": 0.00015490447357247646,
        "epoch": 0.8790666666666667,
        "step": 6593
    },
    {
        "loss": 1.8031,
        "grad_norm": 4.003894329071045,
        "learning_rate": 0.00015485186061117947,
        "epoch": 0.8792,
        "step": 6594
    },
    {
        "loss": 2.3619,
        "grad_norm": 3.1550133228302,
        "learning_rate": 0.00015479922592287351,
        "epoch": 0.8793333333333333,
        "step": 6595
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.4768247604370117,
        "learning_rate": 0.00015474656952840755,
        "epoch": 0.8794666666666666,
        "step": 6596
    },
    {
        "loss": 1.886,
        "grad_norm": 3.2034175395965576,
        "learning_rate": 0.00015469389144863875,
        "epoch": 0.8796,
        "step": 6597
    },
    {
        "loss": 3.0406,
        "grad_norm": 2.4896271228790283,
        "learning_rate": 0.0001546411917044332,
        "epoch": 0.8797333333333334,
        "step": 6598
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.325869560241699,
        "learning_rate": 0.0001545884703166655,
        "epoch": 0.8798666666666667,
        "step": 6599
    },
    {
        "loss": 2.1304,
        "grad_norm": 3.8809101581573486,
        "learning_rate": 0.00015453572730621856,
        "epoch": 0.88,
        "step": 6600
    },
    {
        "loss": 2.0801,
        "grad_norm": 5.777408599853516,
        "learning_rate": 0.00015448296269398432,
        "epoch": 0.8801333333333333,
        "step": 6601
    },
    {
        "loss": 2.0252,
        "grad_norm": 3.5912818908691406,
        "learning_rate": 0.00015443017650086289,
        "epoch": 0.8802666666666666,
        "step": 6602
    },
    {
        "loss": 2.3357,
        "grad_norm": 3.548910617828369,
        "learning_rate": 0.0001543773687477631,
        "epoch": 0.8804,
        "step": 6603
    },
    {
        "loss": 2.4624,
        "grad_norm": 4.137007236480713,
        "learning_rate": 0.00015432453945560224,
        "epoch": 0.8805333333333333,
        "step": 6604
    },
    {
        "loss": 2.1988,
        "grad_norm": 3.4499926567077637,
        "learning_rate": 0.0001542716886453062,
        "epoch": 0.8806666666666667,
        "step": 6605
    },
    {
        "loss": 2.0654,
        "grad_norm": 2.937549591064453,
        "learning_rate": 0.00015421881633780936,
        "epoch": 0.8808,
        "step": 6606
    },
    {
        "loss": 2.5743,
        "grad_norm": 2.0485846996307373,
        "learning_rate": 0.0001541659225540546,
        "epoch": 0.8809333333333333,
        "step": 6607
    },
    {
        "loss": 2.6944,
        "grad_norm": 3.0724828243255615,
        "learning_rate": 0.0001541130073149934,
        "epoch": 0.8810666666666667,
        "step": 6608
    },
    {
        "loss": 2.3891,
        "grad_norm": 3.720404863357544,
        "learning_rate": 0.00015406007064158557,
        "epoch": 0.8812,
        "step": 6609
    },
    {
        "loss": 2.5582,
        "grad_norm": 3.2331418991088867,
        "learning_rate": 0.00015400711255479955,
        "epoch": 0.8813333333333333,
        "step": 6610
    },
    {
        "loss": 2.0698,
        "grad_norm": 2.7421188354492188,
        "learning_rate": 0.00015395413307561228,
        "epoch": 0.8814666666666666,
        "step": 6611
    },
    {
        "loss": 2.6734,
        "grad_norm": 2.896233558654785,
        "learning_rate": 0.00015390113222500898,
        "epoch": 0.8816,
        "step": 6612
    },
    {
        "loss": 2.2161,
        "grad_norm": 2.130009412765503,
        "learning_rate": 0.00015384811002398363,
        "epoch": 0.8817333333333334,
        "step": 6613
    },
    {
        "loss": 2.3816,
        "grad_norm": 3.644282817840576,
        "learning_rate": 0.00015379506649353838,
        "epoch": 0.8818666666666667,
        "step": 6614
    },
    {
        "loss": 0.7239,
        "grad_norm": 2.9820003509521484,
        "learning_rate": 0.000153742001654684,
        "epoch": 0.882,
        "step": 6615
    },
    {
        "loss": 2.7505,
        "grad_norm": 3.243507146835327,
        "learning_rate": 0.00015368891552843951,
        "epoch": 0.8821333333333333,
        "step": 6616
    },
    {
        "loss": 1.0608,
        "grad_norm": 4.327914237976074,
        "learning_rate": 0.0001536358081358328,
        "epoch": 0.8822666666666666,
        "step": 6617
    },
    {
        "loss": 2.8038,
        "grad_norm": 2.4583561420440674,
        "learning_rate": 0.00015358267949789966,
        "epoch": 0.8824,
        "step": 6618
    },
    {
        "loss": 2.026,
        "grad_norm": 4.661050319671631,
        "learning_rate": 0.00015352952963568463,
        "epoch": 0.8825333333333333,
        "step": 6619
    },
    {
        "loss": 2.4595,
        "grad_norm": 3.031139850616455,
        "learning_rate": 0.00015347635857024047,
        "epoch": 0.8826666666666667,
        "step": 6620
    },
    {
        "loss": 2.6985,
        "grad_norm": 2.8900623321533203,
        "learning_rate": 0.00015342316632262847,
        "epoch": 0.8828,
        "step": 6621
    },
    {
        "loss": 2.4355,
        "grad_norm": 3.86301326751709,
        "learning_rate": 0.0001533699529139183,
        "epoch": 0.8829333333333333,
        "step": 6622
    },
    {
        "loss": 3.2919,
        "grad_norm": 3.622218608856201,
        "learning_rate": 0.00015331671836518789,
        "epoch": 0.8830666666666667,
        "step": 6623
    },
    {
        "loss": 2.5158,
        "grad_norm": 4.04263973236084,
        "learning_rate": 0.00015326346269752371,
        "epoch": 0.8832,
        "step": 6624
    },
    {
        "loss": 1.9302,
        "grad_norm": 2.8527729511260986,
        "learning_rate": 0.00015321018593202036,
        "epoch": 0.8833333333333333,
        "step": 6625
    },
    {
        "loss": 2.6102,
        "grad_norm": 2.501713752746582,
        "learning_rate": 0.00015315688808978115,
        "epoch": 0.8834666666666666,
        "step": 6626
    },
    {
        "loss": 2.8343,
        "grad_norm": 2.1131064891815186,
        "learning_rate": 0.0001531035691919174,
        "epoch": 0.8836,
        "step": 6627
    },
    {
        "loss": 3.015,
        "grad_norm": 4.569222450256348,
        "learning_rate": 0.000153050229259549,
        "epoch": 0.8837333333333334,
        "step": 6628
    },
    {
        "loss": 2.8331,
        "grad_norm": 2.131113052368164,
        "learning_rate": 0.00015299686831380394,
        "epoch": 0.8838666666666667,
        "step": 6629
    },
    {
        "loss": 3.2141,
        "grad_norm": 2.9440622329711914,
        "learning_rate": 0.00015294348637581883,
        "epoch": 0.884,
        "step": 6630
    },
    {
        "loss": 2.2837,
        "grad_norm": 3.4521498680114746,
        "learning_rate": 0.00015289008346673833,
        "epoch": 0.8841333333333333,
        "step": 6631
    },
    {
        "loss": 2.6881,
        "grad_norm": 3.676439046859741,
        "learning_rate": 0.00015283665960771553,
        "epoch": 0.8842666666666666,
        "step": 6632
    },
    {
        "loss": 2.6538,
        "grad_norm": 3.139615058898926,
        "learning_rate": 0.0001527832148199119,
        "epoch": 0.8844,
        "step": 6633
    },
    {
        "loss": 2.9926,
        "grad_norm": 2.844294786453247,
        "learning_rate": 0.00015272974912449697,
        "epoch": 0.8845333333333333,
        "step": 6634
    },
    {
        "loss": 2.1728,
        "grad_norm": 3.515847682952881,
        "learning_rate": 0.00015267626254264873,
        "epoch": 0.8846666666666667,
        "step": 6635
    },
    {
        "loss": 2.0256,
        "grad_norm": 3.8999834060668945,
        "learning_rate": 0.00015262275509555346,
        "epoch": 0.8848,
        "step": 6636
    },
    {
        "loss": 2.3784,
        "grad_norm": 2.7712666988372803,
        "learning_rate": 0.00015256922680440552,
        "epoch": 0.8849333333333333,
        "step": 6637
    },
    {
        "loss": 2.3652,
        "grad_norm": 2.395129680633545,
        "learning_rate": 0.00015251567769040776,
        "epoch": 0.8850666666666667,
        "step": 6638
    },
    {
        "loss": 3.071,
        "grad_norm": 3.1365935802459717,
        "learning_rate": 0.00015246210777477111,
        "epoch": 0.8852,
        "step": 6639
    },
    {
        "loss": 2.0088,
        "grad_norm": 2.544402837753296,
        "learning_rate": 0.0001524085170787148,
        "epoch": 0.8853333333333333,
        "step": 6640
    },
    {
        "loss": 0.63,
        "grad_norm": 2.6742324829101562,
        "learning_rate": 0.00015235490562346627,
        "epoch": 0.8854666666666666,
        "step": 6641
    },
    {
        "loss": 2.5226,
        "grad_norm": 3.7015137672424316,
        "learning_rate": 0.0001523012734302613,
        "epoch": 0.8856,
        "step": 6642
    },
    {
        "loss": 2.1221,
        "grad_norm": 2.93001651763916,
        "learning_rate": 0.00015224762052034368,
        "epoch": 0.8857333333333334,
        "step": 6643
    },
    {
        "loss": 2.8416,
        "grad_norm": 2.2090389728546143,
        "learning_rate": 0.0001521939469149655,
        "epoch": 0.8858666666666667,
        "step": 6644
    },
    {
        "loss": 2.7009,
        "grad_norm": 2.214815616607666,
        "learning_rate": 0.0001521402526353872,
        "epoch": 0.886,
        "step": 6645
    },
    {
        "loss": 1.0659,
        "grad_norm": 3.7054262161254883,
        "learning_rate": 0.0001520865377028771,
        "epoch": 0.8861333333333333,
        "step": 6646
    },
    {
        "loss": 2.7843,
        "grad_norm": 2.7592291831970215,
        "learning_rate": 0.00015203280213871198,
        "epoch": 0.8862666666666666,
        "step": 6647
    },
    {
        "loss": 2.2837,
        "grad_norm": 3.396451473236084,
        "learning_rate": 0.00015197904596417656,
        "epoch": 0.8864,
        "step": 6648
    },
    {
        "loss": 2.1823,
        "grad_norm": 2.15533185005188,
        "learning_rate": 0.00015192526920056401,
        "epoch": 0.8865333333333333,
        "step": 6649
    },
    {
        "loss": 2.5218,
        "grad_norm": 3.769158363342285,
        "learning_rate": 0.0001518714718691753,
        "epoch": 0.8866666666666667,
        "step": 6650
    },
    {
        "loss": 2.9182,
        "grad_norm": 2.3002963066101074,
        "learning_rate": 0.0001518176539913199,
        "epoch": 0.8868,
        "step": 6651
    },
    {
        "loss": 2.6038,
        "grad_norm": 3.447030782699585,
        "learning_rate": 0.00015176381558831516,
        "epoch": 0.8869333333333334,
        "step": 6652
    },
    {
        "loss": 0.9109,
        "grad_norm": 3.966543197631836,
        "learning_rate": 0.00015170995668148672,
        "epoch": 0.8870666666666667,
        "step": 6653
    },
    {
        "loss": 2.4837,
        "grad_norm": 2.122511148452759,
        "learning_rate": 0.00015165607729216823,
        "epoch": 0.8872,
        "step": 6654
    },
    {
        "loss": 2.042,
        "grad_norm": 3.8831839561462402,
        "learning_rate": 0.0001516021774417015,
        "epoch": 0.8873333333333333,
        "step": 6655
    },
    {
        "loss": 2.14,
        "grad_norm": 3.6227598190307617,
        "learning_rate": 0.00015154825715143642,
        "epoch": 0.8874666666666666,
        "step": 6656
    },
    {
        "loss": 2.6443,
        "grad_norm": 2.730635643005371,
        "learning_rate": 0.00015149431644273108,
        "epoch": 0.8876,
        "step": 6657
    },
    {
        "loss": 2.0832,
        "grad_norm": 3.970973491668701,
        "learning_rate": 0.00015144035533695157,
        "epoch": 0.8877333333333334,
        "step": 6658
    },
    {
        "loss": 1.9247,
        "grad_norm": 3.3514976501464844,
        "learning_rate": 0.00015138637385547196,
        "epoch": 0.8878666666666667,
        "step": 6659
    },
    {
        "loss": 2.8094,
        "grad_norm": 3.208482265472412,
        "learning_rate": 0.0001513323720196746,
        "epoch": 0.888,
        "step": 6660
    },
    {
        "loss": 2.9915,
        "grad_norm": 3.074462890625,
        "learning_rate": 0.00015127834985094975,
        "epoch": 0.8881333333333333,
        "step": 6661
    },
    {
        "loss": 3.2651,
        "grad_norm": 4.14262580871582,
        "learning_rate": 0.00015122430737069583,
        "epoch": 0.8882666666666666,
        "step": 6662
    },
    {
        "loss": 2.0408,
        "grad_norm": 3.2204062938690186,
        "learning_rate": 0.0001511702446003192,
        "epoch": 0.8884,
        "step": 6663
    },
    {
        "loss": 1.5835,
        "grad_norm": 4.451171875,
        "learning_rate": 0.00015111616156123438,
        "epoch": 0.8885333333333333,
        "step": 6664
    },
    {
        "loss": 2.4642,
        "grad_norm": 4.325648307800293,
        "learning_rate": 0.00015106205827486373,
        "epoch": 0.8886666666666667,
        "step": 6665
    },
    {
        "loss": 1.5933,
        "grad_norm": 2.9651854038238525,
        "learning_rate": 0.00015100793476263778,
        "epoch": 0.8888,
        "step": 6666
    },
    {
        "loss": 2.3868,
        "grad_norm": 3.2015888690948486,
        "learning_rate": 0.00015095379104599515,
        "epoch": 0.8889333333333334,
        "step": 6667
    },
    {
        "loss": 3.1584,
        "grad_norm": 3.5912067890167236,
        "learning_rate": 0.00015089962714638218,
        "epoch": 0.8890666666666667,
        "step": 6668
    },
    {
        "loss": 2.7513,
        "grad_norm": 3.583519458770752,
        "learning_rate": 0.0001508454430852535,
        "epoch": 0.8892,
        "step": 6669
    },
    {
        "loss": 1.1179,
        "grad_norm": 3.360651969909668,
        "learning_rate": 0.0001507912388840716,
        "epoch": 0.8893333333333333,
        "step": 6670
    },
    {
        "loss": 2.8075,
        "grad_norm": 2.9789865016937256,
        "learning_rate": 0.00015073701456430686,
        "epoch": 0.8894666666666666,
        "step": 6671
    },
    {
        "loss": 2.2812,
        "grad_norm": 3.989546775817871,
        "learning_rate": 0.00015068277014743784,
        "epoch": 0.8896,
        "step": 6672
    },
    {
        "loss": 2.7223,
        "grad_norm": 2.5558981895446777,
        "learning_rate": 0.00015062850565495078,
        "epoch": 0.8897333333333334,
        "step": 6673
    },
    {
        "loss": 2.6226,
        "grad_norm": 3.87166690826416,
        "learning_rate": 0.0001505742211083402,
        "epoch": 0.8898666666666667,
        "step": 6674
    },
    {
        "loss": 2.6381,
        "grad_norm": 3.7200541496276855,
        "learning_rate": 0.0001505199165291082,
        "epoch": 0.89,
        "step": 6675
    },
    {
        "loss": 1.2915,
        "grad_norm": 3.4525225162506104,
        "learning_rate": 0.0001504655919387652,
        "epoch": 0.8901333333333333,
        "step": 6676
    },
    {
        "loss": 1.6751,
        "grad_norm": 3.504612684249878,
        "learning_rate": 0.00015041124735882923,
        "epoch": 0.8902666666666667,
        "step": 6677
    },
    {
        "loss": 2.5907,
        "grad_norm": 3.0634002685546875,
        "learning_rate": 0.00015035688281082648,
        "epoch": 0.8904,
        "step": 6678
    },
    {
        "loss": 1.825,
        "grad_norm": 3.2609095573425293,
        "learning_rate": 0.0001503024983162908,
        "epoch": 0.8905333333333333,
        "step": 6679
    },
    {
        "loss": 3.0155,
        "grad_norm": 6.150534152984619,
        "learning_rate": 0.00015024809389676419,
        "epoch": 0.8906666666666667,
        "step": 6680
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.4708213806152344,
        "learning_rate": 0.00015019366957379633,
        "epoch": 0.8908,
        "step": 6681
    },
    {
        "loss": 2.723,
        "grad_norm": 2.922281265258789,
        "learning_rate": 0.00015013922536894492,
        "epoch": 0.8909333333333334,
        "step": 6682
    },
    {
        "loss": 2.2067,
        "grad_norm": 3.7017123699188232,
        "learning_rate": 0.00015008476130377554,
        "epoch": 0.8910666666666667,
        "step": 6683
    },
    {
        "loss": 2.2854,
        "grad_norm": 3.5862293243408203,
        "learning_rate": 0.0001500302773998614,
        "epoch": 0.8912,
        "step": 6684
    },
    {
        "loss": 1.9159,
        "grad_norm": 2.4216418266296387,
        "learning_rate": 0.00014997577367878407,
        "epoch": 0.8913333333333333,
        "step": 6685
    },
    {
        "loss": 2.5114,
        "grad_norm": 4.179990768432617,
        "learning_rate": 0.00014992125016213243,
        "epoch": 0.8914666666666666,
        "step": 6686
    },
    {
        "loss": 1.7502,
        "grad_norm": 3.487819194793701,
        "learning_rate": 0.00014986670687150354,
        "epoch": 0.8916,
        "step": 6687
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.4409844875335693,
        "learning_rate": 0.0001498121438285021,
        "epoch": 0.8917333333333334,
        "step": 6688
    },
    {
        "loss": 2.5693,
        "grad_norm": 3.025956869125366,
        "learning_rate": 0.00014975756105474082,
        "epoch": 0.8918666666666667,
        "step": 6689
    },
    {
        "loss": 2.5528,
        "grad_norm": 2.8696129322052,
        "learning_rate": 0.00014970295857184,
        "epoch": 0.892,
        "step": 6690
    },
    {
        "loss": 2.3757,
        "grad_norm": 3.4157838821411133,
        "learning_rate": 0.00014964833640142793,
        "epoch": 0.8921333333333333,
        "step": 6691
    },
    {
        "loss": 2.6981,
        "grad_norm": 2.476071834564209,
        "learning_rate": 0.0001495936945651407,
        "epoch": 0.8922666666666667,
        "step": 6692
    },
    {
        "loss": 2.6812,
        "grad_norm": 3.325653076171875,
        "learning_rate": 0.000149539033084622,
        "epoch": 0.8924,
        "step": 6693
    },
    {
        "loss": 2.6027,
        "grad_norm": 2.0654945373535156,
        "learning_rate": 0.00014948435198152348,
        "epoch": 0.8925333333333333,
        "step": 6694
    },
    {
        "loss": 3.0365,
        "grad_norm": 2.1437172889709473,
        "learning_rate": 0.0001494296512775046,
        "epoch": 0.8926666666666667,
        "step": 6695
    },
    {
        "loss": 1.9973,
        "grad_norm": 4.539544582366943,
        "learning_rate": 0.0001493749309942324,
        "epoch": 0.8928,
        "step": 6696
    },
    {
        "loss": 2.5561,
        "grad_norm": 4.021173477172852,
        "learning_rate": 0.0001493201911533818,
        "epoch": 0.8929333333333334,
        "step": 6697
    },
    {
        "loss": 2.2408,
        "grad_norm": 3.9309165477752686,
        "learning_rate": 0.00014926543177663538,
        "epoch": 0.8930666666666667,
        "step": 6698
    },
    {
        "loss": 1.3289,
        "grad_norm": 3.613190174102783,
        "learning_rate": 0.00014921065288568363,
        "epoch": 0.8932,
        "step": 6699
    },
    {
        "loss": 2.5456,
        "grad_norm": 3.6223580837249756,
        "learning_rate": 0.0001491558545022245,
        "epoch": 0.8933333333333333,
        "step": 6700
    },
    {
        "loss": 2.9266,
        "grad_norm": 4.602858543395996,
        "learning_rate": 0.000149101036647964,
        "epoch": 0.8934666666666666,
        "step": 6701
    },
    {
        "loss": 2.3976,
        "grad_norm": 2.6348512172698975,
        "learning_rate": 0.0001490461993446155,
        "epoch": 0.8936,
        "step": 6702
    },
    {
        "loss": 2.467,
        "grad_norm": 3.4071264266967773,
        "learning_rate": 0.00014899134261390034,
        "epoch": 0.8937333333333334,
        "step": 6703
    },
    {
        "loss": 2.0034,
        "grad_norm": 3.2972211837768555,
        "learning_rate": 0.0001489364664775475,
        "epoch": 0.8938666666666667,
        "step": 6704
    },
    {
        "loss": 2.8196,
        "grad_norm": 2.8040366172790527,
        "learning_rate": 0.00014888157095729345,
        "epoch": 0.894,
        "step": 6705
    },
    {
        "loss": 2.3836,
        "grad_norm": 5.281595230102539,
        "learning_rate": 0.0001488266560748827,
        "epoch": 0.8941333333333333,
        "step": 6706
    },
    {
        "loss": 2.9351,
        "grad_norm": 3.1200690269470215,
        "learning_rate": 0.00014877172185206701,
        "epoch": 0.8942666666666667,
        "step": 6707
    },
    {
        "loss": 1.0775,
        "grad_norm": 3.2089076042175293,
        "learning_rate": 0.00014871676831060618,
        "epoch": 0.8944,
        "step": 6708
    },
    {
        "loss": 2.4241,
        "grad_norm": 3.0377800464630127,
        "learning_rate": 0.00014866179547226726,
        "epoch": 0.8945333333333333,
        "step": 6709
    },
    {
        "loss": 2.1141,
        "grad_norm": 4.540796279907227,
        "learning_rate": 0.0001486068033588255,
        "epoch": 0.8946666666666667,
        "step": 6710
    },
    {
        "loss": 2.2757,
        "grad_norm": 3.1317636966705322,
        "learning_rate": 0.00014855179199206326,
        "epoch": 0.8948,
        "step": 6711
    },
    {
        "loss": 2.5728,
        "grad_norm": 4.244328022003174,
        "learning_rate": 0.00014849676139377084,
        "epoch": 0.8949333333333334,
        "step": 6712
    },
    {
        "loss": 2.6304,
        "grad_norm": 3.0455334186553955,
        "learning_rate": 0.0001484417115857459,
        "epoch": 0.8950666666666667,
        "step": 6713
    },
    {
        "loss": 1.6178,
        "grad_norm": 4.0352349281311035,
        "learning_rate": 0.00014838664258979398,
        "epoch": 0.8952,
        "step": 6714
    },
    {
        "loss": 1.9462,
        "grad_norm": 3.0351738929748535,
        "learning_rate": 0.00014833155442772804,
        "epoch": 0.8953333333333333,
        "step": 6715
    },
    {
        "loss": 2.6333,
        "grad_norm": 2.8218679428100586,
        "learning_rate": 0.00014827644712136866,
        "epoch": 0.8954666666666666,
        "step": 6716
    },
    {
        "loss": 2.6097,
        "grad_norm": 3.3568286895751953,
        "learning_rate": 0.00014822132069254416,
        "epoch": 0.8956,
        "step": 6717
    },
    {
        "loss": 1.5179,
        "grad_norm": 3.7911009788513184,
        "learning_rate": 0.0001481661751630902,
        "epoch": 0.8957333333333334,
        "step": 6718
    },
    {
        "loss": 1.8143,
        "grad_norm": 4.029361724853516,
        "learning_rate": 0.00014811101055485008,
        "epoch": 0.8958666666666667,
        "step": 6719
    },
    {
        "loss": 2.5519,
        "grad_norm": 5.229605674743652,
        "learning_rate": 0.00014805582688967483,
        "epoch": 0.896,
        "step": 6720
    },
    {
        "loss": 1.8234,
        "grad_norm": 3.9109749794006348,
        "learning_rate": 0.00014800062418942274,
        "epoch": 0.8961333333333333,
        "step": 6721
    },
    {
        "loss": 2.5878,
        "grad_norm": 2.9760093688964844,
        "learning_rate": 0.00014794540247595986,
        "epoch": 0.8962666666666667,
        "step": 6722
    },
    {
        "loss": 1.9542,
        "grad_norm": 4.9076337814331055,
        "learning_rate": 0.00014789016177115972,
        "epoch": 0.8964,
        "step": 6723
    },
    {
        "loss": 2.7335,
        "grad_norm": 2.325601577758789,
        "learning_rate": 0.0001478349020969033,
        "epoch": 0.8965333333333333,
        "step": 6724
    },
    {
        "loss": 2.9378,
        "grad_norm": 3.0999209880828857,
        "learning_rate": 0.00014777962347507914,
        "epoch": 0.8966666666666666,
        "step": 6725
    },
    {
        "loss": 2.7088,
        "grad_norm": 3.2324202060699463,
        "learning_rate": 0.00014772432592758338,
        "epoch": 0.8968,
        "step": 6726
    },
    {
        "loss": 2.8376,
        "grad_norm": 2.294919013977051,
        "learning_rate": 0.00014766900947631943,
        "epoch": 0.8969333333333334,
        "step": 6727
    },
    {
        "loss": 1.3203,
        "grad_norm": 3.31435489654541,
        "learning_rate": 0.00014761367414319843,
        "epoch": 0.8970666666666667,
        "step": 6728
    },
    {
        "loss": 1.079,
        "grad_norm": 3.93060302734375,
        "learning_rate": 0.00014755831995013892,
        "epoch": 0.8972,
        "step": 6729
    },
    {
        "loss": 2.7805,
        "grad_norm": 3.169971466064453,
        "learning_rate": 0.00014750294691906674,
        "epoch": 0.8973333333333333,
        "step": 6730
    },
    {
        "loss": 2.3256,
        "grad_norm": 3.4507105350494385,
        "learning_rate": 0.0001474475550719155,
        "epoch": 0.8974666666666666,
        "step": 6731
    },
    {
        "loss": 2.2745,
        "grad_norm": 2.8257832527160645,
        "learning_rate": 0.00014739214443062598,
        "epoch": 0.8976,
        "step": 6732
    },
    {
        "loss": 3.2443,
        "grad_norm": 3.1761629581451416,
        "learning_rate": 0.0001473367150171466,
        "epoch": 0.8977333333333334,
        "step": 6733
    },
    {
        "loss": 1.1771,
        "grad_norm": 6.432065010070801,
        "learning_rate": 0.000147281266853433,
        "epoch": 0.8978666666666667,
        "step": 6734
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.5048913955688477,
        "learning_rate": 0.00014722579996144858,
        "epoch": 0.898,
        "step": 6735
    },
    {
        "loss": 1.4587,
        "grad_norm": 3.8399603366851807,
        "learning_rate": 0.00014717031436316385,
        "epoch": 0.8981333333333333,
        "step": 6736
    },
    {
        "loss": 2.622,
        "grad_norm": 3.412001371383667,
        "learning_rate": 0.00014711481008055692,
        "epoch": 0.8982666666666667,
        "step": 6737
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.7754790782928467,
        "learning_rate": 0.00014705928713561309,
        "epoch": 0.8984,
        "step": 6738
    },
    {
        "loss": 2.2325,
        "grad_norm": 5.335594177246094,
        "learning_rate": 0.00014700374555032538,
        "epoch": 0.8985333333333333,
        "step": 6739
    },
    {
        "loss": 2.2636,
        "grad_norm": 2.8193914890289307,
        "learning_rate": 0.00014694818534669385,
        "epoch": 0.8986666666666666,
        "step": 6740
    },
    {
        "loss": 2.5749,
        "grad_norm": 3.407294273376465,
        "learning_rate": 0.00014689260654672607,
        "epoch": 0.8988,
        "step": 6741
    },
    {
        "loss": 1.6723,
        "grad_norm": 5.518924236297607,
        "learning_rate": 0.00014683700917243715,
        "epoch": 0.8989333333333334,
        "step": 6742
    },
    {
        "loss": 0.915,
        "grad_norm": 3.0369439125061035,
        "learning_rate": 0.00014678139324584918,
        "epoch": 0.8990666666666667,
        "step": 6743
    },
    {
        "loss": 2.7875,
        "grad_norm": 3.254056930541992,
        "learning_rate": 0.0001467257587889921,
        "epoch": 0.8992,
        "step": 6744
    },
    {
        "loss": 2.9303,
        "grad_norm": 2.8922922611236572,
        "learning_rate": 0.00014667010582390262,
        "epoch": 0.8993333333333333,
        "step": 6745
    },
    {
        "loss": 2.0409,
        "grad_norm": 2.741006374359131,
        "learning_rate": 0.0001466144343726253,
        "epoch": 0.8994666666666666,
        "step": 6746
    },
    {
        "loss": 2.9896,
        "grad_norm": 2.279425621032715,
        "learning_rate": 0.00014655874445721162,
        "epoch": 0.8996,
        "step": 6747
    },
    {
        "loss": 2.3095,
        "grad_norm": 4.264328479766846,
        "learning_rate": 0.00014650303609972068,
        "epoch": 0.8997333333333334,
        "step": 6748
    },
    {
        "loss": 2.676,
        "grad_norm": 2.6635873317718506,
        "learning_rate": 0.00014644730932221863,
        "epoch": 0.8998666666666667,
        "step": 6749
    },
    {
        "loss": 1.9462,
        "grad_norm": 3.419243812561035,
        "learning_rate": 0.00014639156414677907,
        "epoch": 0.9,
        "step": 6750
    },
    {
        "loss": 2.9333,
        "grad_norm": 2.4173593521118164,
        "learning_rate": 0.00014633580059548296,
        "epoch": 0.9001333333333333,
        "step": 6751
    },
    {
        "loss": 2.3386,
        "grad_norm": 3.155515670776367,
        "learning_rate": 0.00014628001869041826,
        "epoch": 0.9002666666666667,
        "step": 6752
    },
    {
        "loss": 2.8879,
        "grad_norm": 3.2893877029418945,
        "learning_rate": 0.0001462242184536805,
        "epoch": 0.9004,
        "step": 6753
    },
    {
        "loss": 2.273,
        "grad_norm": 2.8705050945281982,
        "learning_rate": 0.00014616839990737234,
        "epoch": 0.9005333333333333,
        "step": 6754
    },
    {
        "loss": 2.9464,
        "grad_norm": 2.7531797885894775,
        "learning_rate": 0.0001461125630736036,
        "epoch": 0.9006666666666666,
        "step": 6755
    },
    {
        "loss": 1.984,
        "grad_norm": 3.0793027877807617,
        "learning_rate": 0.00014605670797449157,
        "epoch": 0.9008,
        "step": 6756
    },
    {
        "loss": 2.3572,
        "grad_norm": 3.830658435821533,
        "learning_rate": 0.00014600083463216053,
        "epoch": 0.9009333333333334,
        "step": 6757
    },
    {
        "loss": 2.582,
        "grad_norm": 3.5802955627441406,
        "learning_rate": 0.0001459449430687422,
        "epoch": 0.9010666666666667,
        "step": 6758
    },
    {
        "loss": 2.451,
        "grad_norm": 4.432977676391602,
        "learning_rate": 0.00014588903330637522,
        "epoch": 0.9012,
        "step": 6759
    },
    {
        "loss": 2.3084,
        "grad_norm": 3.399876594543457,
        "learning_rate": 0.00014583310536720592,
        "epoch": 0.9013333333333333,
        "step": 6760
    },
    {
        "loss": 2.7098,
        "grad_norm": 2.7645726203918457,
        "learning_rate": 0.00014577715927338738,
        "epoch": 0.9014666666666666,
        "step": 6761
    },
    {
        "loss": 2.4082,
        "grad_norm": 6.4109320640563965,
        "learning_rate": 0.00014572119504708005,
        "epoch": 0.9016,
        "step": 6762
    },
    {
        "loss": 1.6782,
        "grad_norm": 4.252449035644531,
        "learning_rate": 0.0001456652127104516,
        "epoch": 0.9017333333333334,
        "step": 6763
    },
    {
        "loss": 2.7807,
        "grad_norm": 3.259962320327759,
        "learning_rate": 0.0001456092122856768,
        "epoch": 0.9018666666666667,
        "step": 6764
    },
    {
        "loss": 2.3741,
        "grad_norm": 2.0943450927734375,
        "learning_rate": 0.00014555319379493762,
        "epoch": 0.902,
        "step": 6765
    },
    {
        "loss": 2.3885,
        "grad_norm": 3.12988018989563,
        "learning_rate": 0.0001454971572604231,
        "epoch": 0.9021333333333333,
        "step": 6766
    },
    {
        "loss": 2.5317,
        "grad_norm": 5.340823650360107,
        "learning_rate": 0.0001454411027043296,
        "epoch": 0.9022666666666667,
        "step": 6767
    },
    {
        "loss": 3.0616,
        "grad_norm": 4.550991058349609,
        "learning_rate": 0.00014538503014886042,
        "epoch": 0.9024,
        "step": 6768
    },
    {
        "loss": 3.0174,
        "grad_norm": 3.9506337642669678,
        "learning_rate": 0.00014532893961622626,
        "epoch": 0.9025333333333333,
        "step": 6769
    },
    {
        "loss": 1.7187,
        "grad_norm": 3.3654067516326904,
        "learning_rate": 0.00014527283112864456,
        "epoch": 0.9026666666666666,
        "step": 6770
    },
    {
        "loss": 3.1665,
        "grad_norm": 3.196342945098877,
        "learning_rate": 0.00014521670470834028,
        "epoch": 0.9028,
        "step": 6771
    },
    {
        "loss": 1.9975,
        "grad_norm": 3.727858543395996,
        "learning_rate": 0.00014516056037754512,
        "epoch": 0.9029333333333334,
        "step": 6772
    },
    {
        "loss": 2.1536,
        "grad_norm": 3.735287666320801,
        "learning_rate": 0.00014510439815849818,
        "epoch": 0.9030666666666667,
        "step": 6773
    },
    {
        "loss": 1.8149,
        "grad_norm": 3.790724039077759,
        "learning_rate": 0.00014504821807344537,
        "epoch": 0.9032,
        "step": 6774
    },
    {
        "loss": 2.2575,
        "grad_norm": 2.2744593620300293,
        "learning_rate": 0.0001449920201446399,
        "epoch": 0.9033333333333333,
        "step": 6775
    },
    {
        "loss": 1.711,
        "grad_norm": 5.245968341827393,
        "learning_rate": 0.000144935804394342,
        "epoch": 0.9034666666666666,
        "step": 6776
    },
    {
        "loss": 2.0505,
        "grad_norm": 3.8046488761901855,
        "learning_rate": 0.00014487957084481878,
        "epoch": 0.9036,
        "step": 6777
    },
    {
        "loss": 1.9552,
        "grad_norm": 4.7635979652404785,
        "learning_rate": 0.00014482331951834465,
        "epoch": 0.9037333333333334,
        "step": 6778
    },
    {
        "loss": 0.7247,
        "grad_norm": 5.3252034187316895,
        "learning_rate": 0.000144767050437201,
        "epoch": 0.9038666666666667,
        "step": 6779
    },
    {
        "loss": 1.2675,
        "grad_norm": 4.361046314239502,
        "learning_rate": 0.00014471076362367605,
        "epoch": 0.904,
        "step": 6780
    },
    {
        "loss": 3.1631,
        "grad_norm": 2.7242140769958496,
        "learning_rate": 0.00014465445910006528,
        "epoch": 0.9041333333333333,
        "step": 6781
    },
    {
        "loss": 2.628,
        "grad_norm": 3.5754239559173584,
        "learning_rate": 0.00014459813688867118,
        "epoch": 0.9042666666666667,
        "step": 6782
    },
    {
        "loss": 1.5402,
        "grad_norm": 5.967261791229248,
        "learning_rate": 0.000144541797011803,
        "epoch": 0.9044,
        "step": 6783
    },
    {
        "loss": 2.4804,
        "grad_norm": 3.273322343826294,
        "learning_rate": 0.00014448543949177728,
        "epoch": 0.9045333333333333,
        "step": 6784
    },
    {
        "loss": 1.9486,
        "grad_norm": 3.1937711238861084,
        "learning_rate": 0.00014442906435091745,
        "epoch": 0.9046666666666666,
        "step": 6785
    },
    {
        "loss": 1.9289,
        "grad_norm": 3.8915419578552246,
        "learning_rate": 0.00014437267161155378,
        "epoch": 0.9048,
        "step": 6786
    },
    {
        "loss": 2.3366,
        "grad_norm": 3.523876667022705,
        "learning_rate": 0.0001443162612960237,
        "epoch": 0.9049333333333334,
        "step": 6787
    },
    {
        "loss": 1.2034,
        "grad_norm": 6.112388610839844,
        "learning_rate": 0.00014425983342667155,
        "epoch": 0.9050666666666667,
        "step": 6788
    },
    {
        "loss": 1.1456,
        "grad_norm": 3.6242523193359375,
        "learning_rate": 0.00014420338802584854,
        "epoch": 0.9052,
        "step": 6789
    },
    {
        "loss": 2.6095,
        "grad_norm": 3.80497145652771,
        "learning_rate": 0.00014414692511591296,
        "epoch": 0.9053333333333333,
        "step": 6790
    },
    {
        "loss": 2.5447,
        "grad_norm": 3.2870731353759766,
        "learning_rate": 0.00014409044471922988,
        "epoch": 0.9054666666666666,
        "step": 6791
    },
    {
        "loss": 0.6516,
        "grad_norm": 4.501660346984863,
        "learning_rate": 0.0001440339468581714,
        "epoch": 0.9056,
        "step": 6792
    },
    {
        "loss": 2.9107,
        "grad_norm": 2.6348354816436768,
        "learning_rate": 0.00014397743155511648,
        "epoch": 0.9057333333333333,
        "step": 6793
    },
    {
        "loss": 2.2995,
        "grad_norm": 2.9033665657043457,
        "learning_rate": 0.0001439208988324512,
        "epoch": 0.9058666666666667,
        "step": 6794
    },
    {
        "loss": 2.7824,
        "grad_norm": 2.7053260803222656,
        "learning_rate": 0.0001438643487125681,
        "epoch": 0.906,
        "step": 6795
    },
    {
        "loss": 2.1929,
        "grad_norm": 6.746020317077637,
        "learning_rate": 0.00014380778121786714,
        "epoch": 0.9061333333333333,
        "step": 6796
    },
    {
        "loss": 2.3619,
        "grad_norm": 2.694441795349121,
        "learning_rate": 0.00014375119637075466,
        "epoch": 0.9062666666666667,
        "step": 6797
    },
    {
        "loss": 2.7367,
        "grad_norm": 3.1042091846466064,
        "learning_rate": 0.0001436945941936443,
        "epoch": 0.9064,
        "step": 6798
    },
    {
        "loss": 2.1812,
        "grad_norm": 3.654449701309204,
        "learning_rate": 0.00014363797470895624,
        "epoch": 0.9065333333333333,
        "step": 6799
    },
    {
        "loss": 2.755,
        "grad_norm": 3.5102715492248535,
        "learning_rate": 0.0001435813379391177,
        "epoch": 0.9066666666666666,
        "step": 6800
    },
    {
        "loss": 1.7951,
        "grad_norm": 2.049105644226074,
        "learning_rate": 0.00014352468390656275,
        "epoch": 0.9068,
        "step": 6801
    },
    {
        "loss": 2.8485,
        "grad_norm": 3.7571539878845215,
        "learning_rate": 0.0001434680126337321,
        "epoch": 0.9069333333333334,
        "step": 6802
    },
    {
        "loss": 1.9951,
        "grad_norm": 3.030334711074829,
        "learning_rate": 0.00014341132414307363,
        "epoch": 0.9070666666666667,
        "step": 6803
    },
    {
        "loss": 2.8051,
        "grad_norm": 3.2102468013763428,
        "learning_rate": 0.0001433546184570417,
        "epoch": 0.9072,
        "step": 6804
    },
    {
        "loss": 2.0273,
        "grad_norm": 4.658668041229248,
        "learning_rate": 0.00014329789559809774,
        "epoch": 0.9073333333333333,
        "step": 6805
    },
    {
        "loss": 2.064,
        "grad_norm": 3.9688472747802734,
        "learning_rate": 0.00014324115558870973,
        "epoch": 0.9074666666666666,
        "step": 6806
    },
    {
        "loss": 2.2368,
        "grad_norm": 4.603912353515625,
        "learning_rate": 0.0001431843984513527,
        "epoch": 0.9076,
        "step": 6807
    },
    {
        "loss": 1.9404,
        "grad_norm": 4.877270221710205,
        "learning_rate": 0.00014312762420850826,
        "epoch": 0.9077333333333333,
        "step": 6808
    },
    {
        "loss": 0.5834,
        "grad_norm": 3.3879497051239014,
        "learning_rate": 0.0001430708328826649,
        "epoch": 0.9078666666666667,
        "step": 6809
    },
    {
        "loss": 2.5427,
        "grad_norm": 2.807809352874756,
        "learning_rate": 0.00014301402449631793,
        "epoch": 0.908,
        "step": 6810
    },
    {
        "loss": 2.5784,
        "grad_norm": 2.622304677963257,
        "learning_rate": 0.00014295719907196926,
        "epoch": 0.9081333333333333,
        "step": 6811
    },
    {
        "loss": 2.5229,
        "grad_norm": 2.4999053478240967,
        "learning_rate": 0.00014290035663212764,
        "epoch": 0.9082666666666667,
        "step": 6812
    },
    {
        "loss": 2.655,
        "grad_norm": 3.4170901775360107,
        "learning_rate": 0.00014284349719930864,
        "epoch": 0.9084,
        "step": 6813
    },
    {
        "loss": 3.0957,
        "grad_norm": 2.6352899074554443,
        "learning_rate": 0.00014278662079603434,
        "epoch": 0.9085333333333333,
        "step": 6814
    },
    {
        "loss": 1.5536,
        "grad_norm": 4.090671539306641,
        "learning_rate": 0.00014272972744483383,
        "epoch": 0.9086666666666666,
        "step": 6815
    },
    {
        "loss": 3.0679,
        "grad_norm": 4.640148639678955,
        "learning_rate": 0.00014267281716824262,
        "epoch": 0.9088,
        "step": 6816
    },
    {
        "loss": 3.0549,
        "grad_norm": 1.8743478059768677,
        "learning_rate": 0.0001426158899888032,
        "epoch": 0.9089333333333334,
        "step": 6817
    },
    {
        "loss": 2.3659,
        "grad_norm": 3.6830332279205322,
        "learning_rate": 0.0001425589459290644,
        "epoch": 0.9090666666666667,
        "step": 6818
    },
    {
        "loss": 2.3578,
        "grad_norm": 2.906128168106079,
        "learning_rate": 0.00014250198501158224,
        "epoch": 0.9092,
        "step": 6819
    },
    {
        "loss": 2.5604,
        "grad_norm": 2.26395583152771,
        "learning_rate": 0.0001424450072589189,
        "epoch": 0.9093333333333333,
        "step": 6820
    },
    {
        "loss": 3.1112,
        "grad_norm": 3.065389633178711,
        "learning_rate": 0.00014238801269364365,
        "epoch": 0.9094666666666666,
        "step": 6821
    },
    {
        "loss": 1.4783,
        "grad_norm": 3.0236098766326904,
        "learning_rate": 0.00014233100133833205,
        "epoch": 0.9096,
        "step": 6822
    },
    {
        "loss": 2.8755,
        "grad_norm": 3.037076234817505,
        "learning_rate": 0.00014227397321556665,
        "epoch": 0.9097333333333333,
        "step": 6823
    },
    {
        "loss": 2.0666,
        "grad_norm": 3.320368528366089,
        "learning_rate": 0.00014221692834793642,
        "epoch": 0.9098666666666667,
        "step": 6824
    },
    {
        "loss": 2.8995,
        "grad_norm": 2.6599528789520264,
        "learning_rate": 0.00014215986675803694,
        "epoch": 0.91,
        "step": 6825
    },
    {
        "loss": 1.5531,
        "grad_norm": 5.013123512268066,
        "learning_rate": 0.00014210278846847068,
        "epoch": 0.9101333333333333,
        "step": 6826
    },
    {
        "loss": 2.5223,
        "grad_norm": 2.6391329765319824,
        "learning_rate": 0.00014204569350184632,
        "epoch": 0.9102666666666667,
        "step": 6827
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.545551061630249,
        "learning_rate": 0.0001419885818807796,
        "epoch": 0.9104,
        "step": 6828
    },
    {
        "loss": 3.0141,
        "grad_norm": 1.9911643266677856,
        "learning_rate": 0.00014193145362789247,
        "epoch": 0.9105333333333333,
        "step": 6829
    },
    {
        "loss": 2.2504,
        "grad_norm": 4.4145283699035645,
        "learning_rate": 0.00014187430876581373,
        "epoch": 0.9106666666666666,
        "step": 6830
    },
    {
        "loss": 2.6487,
        "grad_norm": 2.5788819789886475,
        "learning_rate": 0.00014181714731717856,
        "epoch": 0.9108,
        "step": 6831
    },
    {
        "loss": 1.811,
        "grad_norm": 3.7120110988616943,
        "learning_rate": 0.00014175996930462892,
        "epoch": 0.9109333333333334,
        "step": 6832
    },
    {
        "loss": 1.7966,
        "grad_norm": 3.871682643890381,
        "learning_rate": 0.00014170277475081307,
        "epoch": 0.9110666666666667,
        "step": 6833
    },
    {
        "loss": 2.0644,
        "grad_norm": 3.001408338546753,
        "learning_rate": 0.00014164556367838603,
        "epoch": 0.9112,
        "step": 6834
    },
    {
        "loss": 2.4604,
        "grad_norm": 3.0364179611206055,
        "learning_rate": 0.0001415883361100094,
        "epoch": 0.9113333333333333,
        "step": 6835
    },
    {
        "loss": 2.8842,
        "grad_norm": 2.3557000160217285,
        "learning_rate": 0.00014153109206835107,
        "epoch": 0.9114666666666666,
        "step": 6836
    },
    {
        "loss": 2.6858,
        "grad_norm": 2.5330142974853516,
        "learning_rate": 0.00014147383157608564,
        "epoch": 0.9116,
        "step": 6837
    },
    {
        "loss": 0.7407,
        "grad_norm": 2.4840593338012695,
        "learning_rate": 0.00014141655465589427,
        "epoch": 0.9117333333333333,
        "step": 6838
    },
    {
        "loss": 2.4633,
        "grad_norm": 3.981929302215576,
        "learning_rate": 0.0001413592613304644,
        "epoch": 0.9118666666666667,
        "step": 6839
    },
    {
        "loss": 2.825,
        "grad_norm": 2.5149524211883545,
        "learning_rate": 0.00014130195162249026,
        "epoch": 0.912,
        "step": 6840
    },
    {
        "loss": 2.6982,
        "grad_norm": 2.1279702186584473,
        "learning_rate": 0.0001412446255546723,
        "epoch": 0.9121333333333334,
        "step": 6841
    },
    {
        "loss": 2.9156,
        "grad_norm": 4.0878472328186035,
        "learning_rate": 0.0001411872831497176,
        "epoch": 0.9122666666666667,
        "step": 6842
    },
    {
        "loss": 2.1396,
        "grad_norm": 2.494650363922119,
        "learning_rate": 0.00014112992443033971,
        "epoch": 0.9124,
        "step": 6843
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.46522855758667,
        "learning_rate": 0.0001410725494192587,
        "epoch": 0.9125333333333333,
        "step": 6844
    },
    {
        "loss": 2.443,
        "grad_norm": 2.6919968128204346,
        "learning_rate": 0.00014101515813920082,
        "epoch": 0.9126666666666666,
        "step": 6845
    },
    {
        "loss": 2.3144,
        "grad_norm": 3.399878978729248,
        "learning_rate": 0.00014095775061289905,
        "epoch": 0.9128,
        "step": 6846
    },
    {
        "loss": 2.9669,
        "grad_norm": 2.2795183658599854,
        "learning_rate": 0.00014090032686309278,
        "epoch": 0.9129333333333334,
        "step": 6847
    },
    {
        "loss": 3.4757,
        "grad_norm": 2.7113919258117676,
        "learning_rate": 0.0001408428869125276,
        "epoch": 0.9130666666666667,
        "step": 6848
    },
    {
        "loss": 1.6582,
        "grad_norm": 3.3552379608154297,
        "learning_rate": 0.0001407854307839558,
        "epoch": 0.9132,
        "step": 6849
    },
    {
        "loss": 2.3632,
        "grad_norm": 2.6182022094726562,
        "learning_rate": 0.00014072795850013584,
        "epoch": 0.9133333333333333,
        "step": 6850
    },
    {
        "loss": 1.9438,
        "grad_norm": 2.3182499408721924,
        "learning_rate": 0.00014067047008383277,
        "epoch": 0.9134666666666666,
        "step": 6851
    },
    {
        "loss": 2.8852,
        "grad_norm": 2.384927749633789,
        "learning_rate": 0.00014061296555781784,
        "epoch": 0.9136,
        "step": 6852
    },
    {
        "loss": 2.5803,
        "grad_norm": 6.213330268859863,
        "learning_rate": 0.00014055544494486898,
        "epoch": 0.9137333333333333,
        "step": 6853
    },
    {
        "loss": 2.2149,
        "grad_norm": 3.696753978729248,
        "learning_rate": 0.0001404979082677701,
        "epoch": 0.9138666666666667,
        "step": 6854
    },
    {
        "loss": 1.9564,
        "grad_norm": 3.8014934062957764,
        "learning_rate": 0.00014044035554931187,
        "epoch": 0.914,
        "step": 6855
    },
    {
        "loss": 2.0279,
        "grad_norm": 4.0331010818481445,
        "learning_rate": 0.00014038278681229092,
        "epoch": 0.9141333333333334,
        "step": 6856
    },
    {
        "loss": 2.5404,
        "grad_norm": 4.11067008972168,
        "learning_rate": 0.00014032520207951058,
        "epoch": 0.9142666666666667,
        "step": 6857
    },
    {
        "loss": 2.9559,
        "grad_norm": 2.8672995567321777,
        "learning_rate": 0.00014026760137378025,
        "epoch": 0.9144,
        "step": 6858
    },
    {
        "loss": 1.8395,
        "grad_norm": 7.239304065704346,
        "learning_rate": 0.00014020998471791577,
        "epoch": 0.9145333333333333,
        "step": 6859
    },
    {
        "loss": 2.9428,
        "grad_norm": 1.45074462890625,
        "learning_rate": 0.00014015235213473944,
        "epoch": 0.9146666666666666,
        "step": 6860
    },
    {
        "loss": 2.6081,
        "grad_norm": 2.6586318016052246,
        "learning_rate": 0.0001400947036470795,
        "epoch": 0.9148,
        "step": 6861
    },
    {
        "loss": 1.3691,
        "grad_norm": 4.980400085449219,
        "learning_rate": 0.00014003703927777084,
        "epoch": 0.9149333333333334,
        "step": 6862
    },
    {
        "loss": 2.1161,
        "grad_norm": 4.223984241485596,
        "learning_rate": 0.00013997935904965452,
        "epoch": 0.9150666666666667,
        "step": 6863
    },
    {
        "loss": 2.4506,
        "grad_norm": 2.6896276473999023,
        "learning_rate": 0.00013992166298557786,
        "epoch": 0.9152,
        "step": 6864
    },
    {
        "loss": 1.9457,
        "grad_norm": 4.107416152954102,
        "learning_rate": 0.00013986395110839446,
        "epoch": 0.9153333333333333,
        "step": 6865
    },
    {
        "loss": 2.6649,
        "grad_norm": 2.7911713123321533,
        "learning_rate": 0.00013980622344096424,
        "epoch": 0.9154666666666667,
        "step": 6866
    },
    {
        "loss": 2.5457,
        "grad_norm": 2.822525978088379,
        "learning_rate": 0.0001397484800061532,
        "epoch": 0.9156,
        "step": 6867
    },
    {
        "loss": 2.3089,
        "grad_norm": 4.790544509887695,
        "learning_rate": 0.0001396907208268338,
        "epoch": 0.9157333333333333,
        "step": 6868
    },
    {
        "loss": 1.1601,
        "grad_norm": 4.385001182556152,
        "learning_rate": 0.0001396329459258847,
        "epoch": 0.9158666666666667,
        "step": 6869
    },
    {
        "loss": 1.7844,
        "grad_norm": 4.369983673095703,
        "learning_rate": 0.00013957515532619062,
        "epoch": 0.916,
        "step": 6870
    },
    {
        "loss": 1.5692,
        "grad_norm": 4.322240352630615,
        "learning_rate": 0.00013951734905064266,
        "epoch": 0.9161333333333334,
        "step": 6871
    },
    {
        "loss": 2.1844,
        "grad_norm": 4.437355041503906,
        "learning_rate": 0.00013945952712213816,
        "epoch": 0.9162666666666667,
        "step": 6872
    },
    {
        "loss": 2.6627,
        "grad_norm": 3.149169445037842,
        "learning_rate": 0.00013940168956358043,
        "epoch": 0.9164,
        "step": 6873
    },
    {
        "loss": 2.4847,
        "grad_norm": 3.143326997756958,
        "learning_rate": 0.00013934383639787928,
        "epoch": 0.9165333333333333,
        "step": 6874
    },
    {
        "loss": 1.7239,
        "grad_norm": 3.5291056632995605,
        "learning_rate": 0.0001392859676479504,
        "epoch": 0.9166666666666666,
        "step": 6875
    },
    {
        "loss": 2.527,
        "grad_norm": 2.9344637393951416,
        "learning_rate": 0.00013922808333671595,
        "epoch": 0.9168,
        "step": 6876
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.831022262573242,
        "learning_rate": 0.00013917018348710389,
        "epoch": 0.9169333333333334,
        "step": 6877
    },
    {
        "loss": 2.4734,
        "grad_norm": 3.05364727973938,
        "learning_rate": 0.0001391122681220488,
        "epoch": 0.9170666666666667,
        "step": 6878
    },
    {
        "loss": 2.8704,
        "grad_norm": 2.920267343521118,
        "learning_rate": 0.00013905433726449097,
        "epoch": 0.9172,
        "step": 6879
    },
    {
        "loss": 2.934,
        "grad_norm": 3.487729787826538,
        "learning_rate": 0.00013899639093737718,
        "epoch": 0.9173333333333333,
        "step": 6880
    },
    {
        "loss": 0.8352,
        "grad_norm": 3.1473655700683594,
        "learning_rate": 0.00013893842916365995,
        "epoch": 0.9174666666666667,
        "step": 6881
    },
    {
        "loss": 2.619,
        "grad_norm": 2.533482313156128,
        "learning_rate": 0.00013888045196629826,
        "epoch": 0.9176,
        "step": 6882
    },
    {
        "loss": 2.454,
        "grad_norm": 3.289064645767212,
        "learning_rate": 0.00013882245936825712,
        "epoch": 0.9177333333333333,
        "step": 6883
    },
    {
        "loss": 1.8604,
        "grad_norm": 3.469149589538574,
        "learning_rate": 0.0001387644513925075,
        "epoch": 0.9178666666666667,
        "step": 6884
    },
    {
        "loss": 3.318,
        "grad_norm": 3.6106033325195312,
        "learning_rate": 0.00013870642806202669,
        "epoch": 0.918,
        "step": 6885
    },
    {
        "loss": 2.3699,
        "grad_norm": 2.8535196781158447,
        "learning_rate": 0.00013864838939979762,
        "epoch": 0.9181333333333334,
        "step": 6886
    },
    {
        "loss": 2.2478,
        "grad_norm": 3.251673698425293,
        "learning_rate": 0.00013859033542881,
        "epoch": 0.9182666666666667,
        "step": 6887
    },
    {
        "loss": 1.0823,
        "grad_norm": 4.1219868659973145,
        "learning_rate": 0.000138532266172059,
        "epoch": 0.9184,
        "step": 6888
    },
    {
        "loss": 2.0898,
        "grad_norm": 3.3787155151367188,
        "learning_rate": 0.00013847418165254606,
        "epoch": 0.9185333333333333,
        "step": 6889
    },
    {
        "loss": 2.3358,
        "grad_norm": 2.605592966079712,
        "learning_rate": 0.00013841608189327863,
        "epoch": 0.9186666666666666,
        "step": 6890
    },
    {
        "loss": 1.7557,
        "grad_norm": 5.398059844970703,
        "learning_rate": 0.00013835796691727035,
        "epoch": 0.9188,
        "step": 6891
    },
    {
        "loss": 2.3966,
        "grad_norm": 2.088855266571045,
        "learning_rate": 0.0001382998367475406,
        "epoch": 0.9189333333333334,
        "step": 6892
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.374995231628418,
        "learning_rate": 0.00013824169140711499,
        "epoch": 0.9190666666666667,
        "step": 6893
    },
    {
        "loss": 1.9495,
        "grad_norm": 4.094488143920898,
        "learning_rate": 0.0001381835309190252,
        "epoch": 0.9192,
        "step": 6894
    },
    {
        "loss": 2.2249,
        "grad_norm": 3.785907506942749,
        "learning_rate": 0.0001381253553063086,
        "epoch": 0.9193333333333333,
        "step": 6895
    },
    {
        "loss": 2.2689,
        "grad_norm": 3.0189270973205566,
        "learning_rate": 0.00013806716459200888,
        "epoch": 0.9194666666666667,
        "step": 6896
    },
    {
        "loss": 2.9737,
        "grad_norm": 2.5809097290039062,
        "learning_rate": 0.00013800895879917562,
        "epoch": 0.9196,
        "step": 6897
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.0911011695861816,
        "learning_rate": 0.00013795073795086423,
        "epoch": 0.9197333333333333,
        "step": 6898
    },
    {
        "loss": 3.0419,
        "grad_norm": 4.724791049957275,
        "learning_rate": 0.00013789250207013625,
        "epoch": 0.9198666666666667,
        "step": 6899
    },
    {
        "loss": 1.9462,
        "grad_norm": 2.298551082611084,
        "learning_rate": 0.00013783425118005902,
        "epoch": 0.92,
        "step": 6900
    },
    {
        "loss": 2.8391,
        "grad_norm": 2.4911928176879883,
        "learning_rate": 0.00013777598530370606,
        "epoch": 0.9201333333333334,
        "step": 6901
    },
    {
        "loss": 1.7515,
        "grad_norm": 4.486868381500244,
        "learning_rate": 0.00013771770446415648,
        "epoch": 0.9202666666666667,
        "step": 6902
    },
    {
        "loss": 2.6721,
        "grad_norm": 3.2715721130371094,
        "learning_rate": 0.00013765940868449577,
        "epoch": 0.9204,
        "step": 6903
    },
    {
        "loss": 0.7586,
        "grad_norm": 5.9912428855896,
        "learning_rate": 0.00013760109798781486,
        "epoch": 0.9205333333333333,
        "step": 6904
    },
    {
        "loss": 2.1781,
        "grad_norm": 2.8886842727661133,
        "learning_rate": 0.00013754277239721093,
        "epoch": 0.9206666666666666,
        "step": 6905
    },
    {
        "loss": 3.1413,
        "grad_norm": 2.238412618637085,
        "learning_rate": 0.00013748443193578702,
        "epoch": 0.9208,
        "step": 6906
    },
    {
        "loss": 2.0369,
        "grad_norm": 2.3601179122924805,
        "learning_rate": 0.00013742607662665178,
        "epoch": 0.9209333333333334,
        "step": 6907
    },
    {
        "loss": 2.4827,
        "grad_norm": 2.3400967121124268,
        "learning_rate": 0.00013736770649292018,
        "epoch": 0.9210666666666667,
        "step": 6908
    },
    {
        "loss": 2.8028,
        "grad_norm": 2.541639804840088,
        "learning_rate": 0.00013730932155771262,
        "epoch": 0.9212,
        "step": 6909
    },
    {
        "loss": 2.771,
        "grad_norm": 2.2544825077056885,
        "learning_rate": 0.00013725092184415575,
        "epoch": 0.9213333333333333,
        "step": 6910
    },
    {
        "loss": 2.9238,
        "grad_norm": 3.113490343093872,
        "learning_rate": 0.00013719250737538166,
        "epoch": 0.9214666666666667,
        "step": 6911
    },
    {
        "loss": 2.6817,
        "grad_norm": 3.281090497970581,
        "learning_rate": 0.0001371340781745288,
        "epoch": 0.9216,
        "step": 6912
    },
    {
        "loss": 1.6799,
        "grad_norm": 2.5480830669403076,
        "learning_rate": 0.00013707563426474097,
        "epoch": 0.9217333333333333,
        "step": 6913
    },
    {
        "loss": 2.6809,
        "grad_norm": 2.7439768314361572,
        "learning_rate": 0.00013701717566916817,
        "epoch": 0.9218666666666666,
        "step": 6914
    },
    {
        "loss": 1.3909,
        "grad_norm": 4.3599724769592285,
        "learning_rate": 0.00013695870241096586,
        "epoch": 0.922,
        "step": 6915
    },
    {
        "loss": 2.5485,
        "grad_norm": 2.718369722366333,
        "learning_rate": 0.00013690021451329566,
        "epoch": 0.9221333333333334,
        "step": 6916
    },
    {
        "loss": 2.2742,
        "grad_norm": 5.458272933959961,
        "learning_rate": 0.0001368417119993247,
        "epoch": 0.9222666666666667,
        "step": 6917
    },
    {
        "loss": 1.0077,
        "grad_norm": 3.264904737472534,
        "learning_rate": 0.00013678319489222605,
        "epoch": 0.9224,
        "step": 6918
    },
    {
        "loss": 2.2254,
        "grad_norm": 3.691370964050293,
        "learning_rate": 0.0001367246632151787,
        "epoch": 0.9225333333333333,
        "step": 6919
    },
    {
        "loss": 1.5874,
        "grad_norm": 3.3368258476257324,
        "learning_rate": 0.000136666116991367,
        "epoch": 0.9226666666666666,
        "step": 6920
    },
    {
        "loss": 1.0904,
        "grad_norm": 5.412068843841553,
        "learning_rate": 0.00013660755624398145,
        "epoch": 0.9228,
        "step": 6921
    },
    {
        "loss": 2.5817,
        "grad_norm": 3.739708423614502,
        "learning_rate": 0.0001365489809962181,
        "epoch": 0.9229333333333334,
        "step": 6922
    },
    {
        "loss": 2.1056,
        "grad_norm": 4.484325408935547,
        "learning_rate": 0.00013649039127127893,
        "epoch": 0.9230666666666667,
        "step": 6923
    },
    {
        "loss": 1.8783,
        "grad_norm": 4.202855587005615,
        "learning_rate": 0.00013643178709237138,
        "epoch": 0.9232,
        "step": 6924
    },
    {
        "loss": 2.5945,
        "grad_norm": 6.229893684387207,
        "learning_rate": 0.00013637316848270887,
        "epoch": 0.9233333333333333,
        "step": 6925
    },
    {
        "loss": 1.7997,
        "grad_norm": 3.567084550857544,
        "learning_rate": 0.00013631453546551032,
        "epoch": 0.9234666666666667,
        "step": 6926
    },
    {
        "loss": 2.6827,
        "grad_norm": 3.1151797771453857,
        "learning_rate": 0.00013625588806400054,
        "epoch": 0.9236,
        "step": 6927
    },
    {
        "loss": 2.6983,
        "grad_norm": 3.4002268314361572,
        "learning_rate": 0.00013619722630141002,
        "epoch": 0.9237333333333333,
        "step": 6928
    },
    {
        "loss": 2.3398,
        "grad_norm": 3.328686475753784,
        "learning_rate": 0.00013613855020097476,
        "epoch": 0.9238666666666666,
        "step": 6929
    },
    {
        "loss": 2.5894,
        "grad_norm": 3.090944290161133,
        "learning_rate": 0.00013607985978593664,
        "epoch": 0.924,
        "step": 6930
    },
    {
        "loss": 2.322,
        "grad_norm": 3.5964972972869873,
        "learning_rate": 0.00013602115507954318,
        "epoch": 0.9241333333333334,
        "step": 6931
    },
    {
        "loss": 2.5424,
        "grad_norm": 2.7669296264648438,
        "learning_rate": 0.00013596243610504736,
        "epoch": 0.9242666666666667,
        "step": 6932
    },
    {
        "loss": 2.363,
        "grad_norm": 2.4983572959899902,
        "learning_rate": 0.00013590370288570815,
        "epoch": 0.9244,
        "step": 6933
    },
    {
        "loss": 2.2755,
        "grad_norm": 6.3347649574279785,
        "learning_rate": 0.00013584495544478984,
        "epoch": 0.9245333333333333,
        "step": 6934
    },
    {
        "loss": 2.1026,
        "grad_norm": 3.073282480239868,
        "learning_rate": 0.0001357861938055626,
        "epoch": 0.9246666666666666,
        "step": 6935
    },
    {
        "loss": 2.6897,
        "grad_norm": 2.550774574279785,
        "learning_rate": 0.00013572741799130187,
        "epoch": 0.9248,
        "step": 6936
    },
    {
        "loss": 2.2111,
        "grad_norm": 2.497765302658081,
        "learning_rate": 0.00013566862802528933,
        "epoch": 0.9249333333333334,
        "step": 6937
    },
    {
        "loss": 2.7134,
        "grad_norm": 2.5153939723968506,
        "learning_rate": 0.00013560982393081162,
        "epoch": 0.9250666666666667,
        "step": 6938
    },
    {
        "loss": 2.8345,
        "grad_norm": 2.4135444164276123,
        "learning_rate": 0.00013555100573116134,
        "epoch": 0.9252,
        "step": 6939
    },
    {
        "loss": 2.644,
        "grad_norm": 2.3135852813720703,
        "learning_rate": 0.0001354921734496365,
        "epoch": 0.9253333333333333,
        "step": 6940
    },
    {
        "loss": 2.5623,
        "grad_norm": 2.2990071773529053,
        "learning_rate": 0.0001354333271095409,
        "epoch": 0.9254666666666667,
        "step": 6941
    },
    {
        "loss": 2.1134,
        "grad_norm": 4.140134811401367,
        "learning_rate": 0.00013537446673418365,
        "epoch": 0.9256,
        "step": 6942
    },
    {
        "loss": 2.5263,
        "grad_norm": 2.564335823059082,
        "learning_rate": 0.00013531559234687953,
        "epoch": 0.9257333333333333,
        "step": 6943
    },
    {
        "loss": 1.8328,
        "grad_norm": 3.254481792449951,
        "learning_rate": 0.00013525670397094905,
        "epoch": 0.9258666666666666,
        "step": 6944
    },
    {
        "loss": 3.0265,
        "grad_norm": 3.1747825145721436,
        "learning_rate": 0.00013519780162971786,
        "epoch": 0.926,
        "step": 6945
    },
    {
        "loss": 1.9961,
        "grad_norm": 3.868647336959839,
        "learning_rate": 0.00013513888534651768,
        "epoch": 0.9261333333333334,
        "step": 6946
    },
    {
        "loss": 2.7326,
        "grad_norm": 3.7530245780944824,
        "learning_rate": 0.00013507995514468513,
        "epoch": 0.9262666666666667,
        "step": 6947
    },
    {
        "loss": 2.4225,
        "grad_norm": 3.748166084289551,
        "learning_rate": 0.00013502101104756296,
        "epoch": 0.9264,
        "step": 6948
    },
    {
        "loss": 1.989,
        "grad_norm": 4.047210693359375,
        "learning_rate": 0.00013496205307849887,
        "epoch": 0.9265333333333333,
        "step": 6949
    },
    {
        "loss": 2.342,
        "grad_norm": 3.1577744483947754,
        "learning_rate": 0.00013490308126084651,
        "epoch": 0.9266666666666666,
        "step": 6950
    },
    {
        "loss": 2.2498,
        "grad_norm": 3.021568536758423,
        "learning_rate": 0.00013484409561796468,
        "epoch": 0.9268,
        "step": 6951
    },
    {
        "loss": 2.8349,
        "grad_norm": 1.2422369718551636,
        "learning_rate": 0.0001347850961732178,
        "epoch": 0.9269333333333334,
        "step": 6952
    },
    {
        "loss": 2.1492,
        "grad_norm": 3.8662006855010986,
        "learning_rate": 0.0001347260829499759,
        "epoch": 0.9270666666666667,
        "step": 6953
    },
    {
        "loss": 2.3714,
        "grad_norm": 2.908278226852417,
        "learning_rate": 0.00013466705597161416,
        "epoch": 0.9272,
        "step": 6954
    },
    {
        "loss": 2.3512,
        "grad_norm": 4.271855354309082,
        "learning_rate": 0.00013460801526151342,
        "epoch": 0.9273333333333333,
        "step": 6955
    },
    {
        "loss": 2.2452,
        "grad_norm": 3.113553762435913,
        "learning_rate": 0.00013454896084305994,
        "epoch": 0.9274666666666667,
        "step": 6956
    },
    {
        "loss": 2.3039,
        "grad_norm": 5.164333820343018,
        "learning_rate": 0.00013448989273964534,
        "epoch": 0.9276,
        "step": 6957
    },
    {
        "loss": 1.6559,
        "grad_norm": 5.456834316253662,
        "learning_rate": 0.00013443081097466673,
        "epoch": 0.9277333333333333,
        "step": 6958
    },
    {
        "loss": 2.6238,
        "grad_norm": 3.1829705238342285,
        "learning_rate": 0.0001343717155715265,
        "epoch": 0.9278666666666666,
        "step": 6959
    },
    {
        "loss": 2.2781,
        "grad_norm": 3.3433945178985596,
        "learning_rate": 0.00013431260655363272,
        "epoch": 0.928,
        "step": 6960
    },
    {
        "loss": 2.3416,
        "grad_norm": 2.746525287628174,
        "learning_rate": 0.0001342534839443984,
        "epoch": 0.9281333333333334,
        "step": 6961
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.477908134460449,
        "learning_rate": 0.00013419434776724252,
        "epoch": 0.9282666666666667,
        "step": 6962
    },
    {
        "loss": 2.0263,
        "grad_norm": 4.44633674621582,
        "learning_rate": 0.0001341351980455889,
        "epoch": 0.9284,
        "step": 6963
    },
    {
        "loss": 2.1939,
        "grad_norm": 3.2839553356170654,
        "learning_rate": 0.00013407603480286703,
        "epoch": 0.9285333333333333,
        "step": 6964
    },
    {
        "loss": 2.2365,
        "grad_norm": 4.296245574951172,
        "learning_rate": 0.0001340168580625117,
        "epoch": 0.9286666666666666,
        "step": 6965
    },
    {
        "loss": 2.202,
        "grad_norm": 3.4689018726348877,
        "learning_rate": 0.00013395766784796297,
        "epoch": 0.9288,
        "step": 6966
    },
    {
        "loss": 2.1364,
        "grad_norm": 3.8183224201202393,
        "learning_rate": 0.00013389846418266634,
        "epoch": 0.9289333333333334,
        "step": 6967
    },
    {
        "loss": 1.2335,
        "grad_norm": 3.791958808898926,
        "learning_rate": 0.00013383924709007246,
        "epoch": 0.9290666666666667,
        "step": 6968
    },
    {
        "loss": 2.1633,
        "grad_norm": 3.2267749309539795,
        "learning_rate": 0.00013378001659363758,
        "epoch": 0.9292,
        "step": 6969
    },
    {
        "loss": 1.8063,
        "grad_norm": 6.103996753692627,
        "learning_rate": 0.00013372077271682291,
        "epoch": 0.9293333333333333,
        "step": 6970
    },
    {
        "loss": 2.8786,
        "grad_norm": 2.9091641902923584,
        "learning_rate": 0.00013366151548309537,
        "epoch": 0.9294666666666667,
        "step": 6971
    },
    {
        "loss": 2.5682,
        "grad_norm": 3.589327335357666,
        "learning_rate": 0.00013360224491592679,
        "epoch": 0.9296,
        "step": 6972
    },
    {
        "loss": 2.8405,
        "grad_norm": 3.2308967113494873,
        "learning_rate": 0.00013354296103879455,
        "epoch": 0.9297333333333333,
        "step": 6973
    },
    {
        "loss": 1.9114,
        "grad_norm": 4.794670104980469,
        "learning_rate": 0.00013348366387518112,
        "epoch": 0.9298666666666666,
        "step": 6974
    },
    {
        "loss": 2.1821,
        "grad_norm": 4.438851833343506,
        "learning_rate": 0.0001334243534485744,
        "epoch": 0.93,
        "step": 6975
    },
    {
        "loss": 1.7505,
        "grad_norm": 2.4681241512298584,
        "learning_rate": 0.0001333650297824673,
        "epoch": 0.9301333333333334,
        "step": 6976
    },
    {
        "loss": 1.4573,
        "grad_norm": 3.4703011512756348,
        "learning_rate": 0.00013330569290035822,
        "epoch": 0.9302666666666667,
        "step": 6977
    },
    {
        "loss": 2.2361,
        "grad_norm": 3.2250404357910156,
        "learning_rate": 0.00013324634282575076,
        "epoch": 0.9304,
        "step": 6978
    },
    {
        "loss": 2.5786,
        "grad_norm": 3.6131386756896973,
        "learning_rate": 0.00013318697958215357,
        "epoch": 0.9305333333333333,
        "step": 6979
    },
    {
        "loss": 2.4895,
        "grad_norm": 2.2615556716918945,
        "learning_rate": 0.00013312760319308066,
        "epoch": 0.9306666666666666,
        "step": 6980
    },
    {
        "loss": 2.634,
        "grad_norm": 3.3462705612182617,
        "learning_rate": 0.00013306821368205132,
        "epoch": 0.9308,
        "step": 6981
    },
    {
        "loss": 2.7463,
        "grad_norm": 4.737680435180664,
        "learning_rate": 0.0001330088110725898,
        "epoch": 0.9309333333333333,
        "step": 6982
    },
    {
        "loss": 1.8671,
        "grad_norm": 3.373267650604248,
        "learning_rate": 0.00013294939538822576,
        "epoch": 0.9310666666666667,
        "step": 6983
    },
    {
        "loss": 2.5103,
        "grad_norm": 2.5774576663970947,
        "learning_rate": 0.000132889966652494,
        "epoch": 0.9312,
        "step": 6984
    },
    {
        "loss": 2.8357,
        "grad_norm": 5.431771278381348,
        "learning_rate": 0.00013283052488893428,
        "epoch": 0.9313333333333333,
        "step": 6985
    },
    {
        "loss": 1.8015,
        "grad_norm": 3.0586934089660645,
        "learning_rate": 0.0001327710701210918,
        "epoch": 0.9314666666666667,
        "step": 6986
    },
    {
        "loss": 1.5423,
        "grad_norm": 4.7034430503845215,
        "learning_rate": 0.00013271160237251684,
        "epoch": 0.9316,
        "step": 6987
    },
    {
        "loss": 2.0997,
        "grad_norm": 3.180121421813965,
        "learning_rate": 0.0001326521216667647,
        "epoch": 0.9317333333333333,
        "step": 6988
    },
    {
        "loss": 2.2257,
        "grad_norm": 3.226562261581421,
        "learning_rate": 0.00013259262802739588,
        "epoch": 0.9318666666666666,
        "step": 6989
    },
    {
        "loss": 2.7522,
        "grad_norm": 2.9241676330566406,
        "learning_rate": 0.00013253312147797606,
        "epoch": 0.932,
        "step": 6990
    },
    {
        "loss": 1.6935,
        "grad_norm": 2.814209222793579,
        "learning_rate": 0.00013247360204207595,
        "epoch": 0.9321333333333334,
        "step": 6991
    },
    {
        "loss": 2.6387,
        "grad_norm": 2.997131824493408,
        "learning_rate": 0.00013241406974327146,
        "epoch": 0.9322666666666667,
        "step": 6992
    },
    {
        "loss": 2.1417,
        "grad_norm": 2.924123525619507,
        "learning_rate": 0.00013235452460514345,
        "epoch": 0.9324,
        "step": 6993
    },
    {
        "loss": 2.4321,
        "grad_norm": 3.482701539993286,
        "learning_rate": 0.00013229496665127804,
        "epoch": 0.9325333333333333,
        "step": 6994
    },
    {
        "loss": 3.0619,
        "grad_norm": 1.8630162477493286,
        "learning_rate": 0.00013223539590526622,
        "epoch": 0.9326666666666666,
        "step": 6995
    },
    {
        "loss": 2.4165,
        "grad_norm": 3.630825996398926,
        "learning_rate": 0.00013217581239070432,
        "epoch": 0.9328,
        "step": 6996
    },
    {
        "loss": 1.1648,
        "grad_norm": 3.527435064315796,
        "learning_rate": 0.00013211621613119347,
        "epoch": 0.9329333333333333,
        "step": 6997
    },
    {
        "loss": 2.5564,
        "grad_norm": 2.7949719429016113,
        "learning_rate": 0.00013205660715034,
        "epoch": 0.9330666666666667,
        "step": 6998
    },
    {
        "loss": 2.4137,
        "grad_norm": 3.056166648864746,
        "learning_rate": 0.0001319969854717552,
        "epoch": 0.9332,
        "step": 6999
    },
    {
        "loss": 2.7347,
        "grad_norm": 4.027506351470947,
        "learning_rate": 0.00013193735111905546,
        "epoch": 0.9333333333333333,
        "step": 7000
    },
    {
        "loss": 2.4171,
        "grad_norm": 2.394360303878784,
        "learning_rate": 0.00013187770411586202,
        "epoch": 0.9334666666666667,
        "step": 7001
    },
    {
        "loss": 2.96,
        "grad_norm": 2.5037834644317627,
        "learning_rate": 0.0001318180444858014,
        "epoch": 0.9336,
        "step": 7002
    },
    {
        "loss": 1.2074,
        "grad_norm": 4.204812526702881,
        "learning_rate": 0.000131758372252505,
        "epoch": 0.9337333333333333,
        "step": 7003
    },
    {
        "loss": 1.6312,
        "grad_norm": 3.192565441131592,
        "learning_rate": 0.00013169868743960898,
        "epoch": 0.9338666666666666,
        "step": 7004
    },
    {
        "loss": 1.1663,
        "grad_norm": 4.163022518157959,
        "learning_rate": 0.000131638990070755,
        "epoch": 0.934,
        "step": 7005
    },
    {
        "loss": 2.9573,
        "grad_norm": 2.693061590194702,
        "learning_rate": 0.00013157928016958924,
        "epoch": 0.9341333333333334,
        "step": 7006
    },
    {
        "loss": 2.3491,
        "grad_norm": 4.500746726989746,
        "learning_rate": 0.000131519557759763,
        "epoch": 0.9342666666666667,
        "step": 7007
    },
    {
        "loss": 2.2499,
        "grad_norm": 3.3877036571502686,
        "learning_rate": 0.00013145982286493245,
        "epoch": 0.9344,
        "step": 7008
    },
    {
        "loss": 1.7422,
        "grad_norm": 3.8832859992980957,
        "learning_rate": 0.000131400075508759,
        "epoch": 0.9345333333333333,
        "step": 7009
    },
    {
        "loss": 1.4895,
        "grad_norm": 3.0844507217407227,
        "learning_rate": 0.00013134031571490853,
        "epoch": 0.9346666666666666,
        "step": 7010
    },
    {
        "loss": 2.6743,
        "grad_norm": 3.698387384414673,
        "learning_rate": 0.00013128054350705222,
        "epoch": 0.9348,
        "step": 7011
    },
    {
        "loss": 1.4681,
        "grad_norm": 3.680175542831421,
        "learning_rate": 0.00013122075890886612,
        "epoch": 0.9349333333333333,
        "step": 7012
    },
    {
        "loss": 2.7554,
        "grad_norm": 3.0979180335998535,
        "learning_rate": 0.00013116096194403095,
        "epoch": 0.9350666666666667,
        "step": 7013
    },
    {
        "loss": 2.4742,
        "grad_norm": 2.599095582962036,
        "learning_rate": 0.00013110115263623262,
        "epoch": 0.9352,
        "step": 7014
    },
    {
        "loss": 2.7452,
        "grad_norm": 3.6616158485412598,
        "learning_rate": 0.0001310413310091618,
        "epoch": 0.9353333333333333,
        "step": 7015
    },
    {
        "loss": 1.5467,
        "grad_norm": 5.175409317016602,
        "learning_rate": 0.00013098149708651392,
        "epoch": 0.9354666666666667,
        "step": 7016
    },
    {
        "loss": 1.4921,
        "grad_norm": 3.250918388366699,
        "learning_rate": 0.00013092165089198955,
        "epoch": 0.9356,
        "step": 7017
    },
    {
        "loss": 2.6971,
        "grad_norm": 3.867825984954834,
        "learning_rate": 0.0001308617924492938,
        "epoch": 0.9357333333333333,
        "step": 7018
    },
    {
        "loss": 2.5608,
        "grad_norm": 4.380634784698486,
        "learning_rate": 0.00013080192178213703,
        "epoch": 0.9358666666666666,
        "step": 7019
    },
    {
        "loss": 2.5697,
        "grad_norm": 2.6274571418762207,
        "learning_rate": 0.0001307420389142339,
        "epoch": 0.936,
        "step": 7020
    },
    {
        "loss": 2.7654,
        "grad_norm": 2.863025426864624,
        "learning_rate": 0.00013068214386930457,
        "epoch": 0.9361333333333334,
        "step": 7021
    },
    {
        "loss": 2.0484,
        "grad_norm": 3.8523762226104736,
        "learning_rate": 0.00013062223667107342,
        "epoch": 0.9362666666666667,
        "step": 7022
    },
    {
        "loss": 2.3932,
        "grad_norm": 4.3190507888793945,
        "learning_rate": 0.00013056231734327,
        "epoch": 0.9364,
        "step": 7023
    },
    {
        "loss": 1.7583,
        "grad_norm": 2.834994077682495,
        "learning_rate": 0.0001305023859096286,
        "epoch": 0.9365333333333333,
        "step": 7024
    },
    {
        "loss": 2.0393,
        "grad_norm": 3.566368818283081,
        "learning_rate": 0.00013044244239388813,
        "epoch": 0.9366666666666666,
        "step": 7025
    },
    {
        "loss": 2.4908,
        "grad_norm": 2.6717066764831543,
        "learning_rate": 0.00013038248681979258,
        "epoch": 0.9368,
        "step": 7026
    },
    {
        "loss": 2.5309,
        "grad_norm": 2.695953845977783,
        "learning_rate": 0.0001303225192110904,
        "epoch": 0.9369333333333333,
        "step": 7027
    },
    {
        "loss": 1.9093,
        "grad_norm": 1.9462804794311523,
        "learning_rate": 0.0001302625395915351,
        "epoch": 0.9370666666666667,
        "step": 7028
    },
    {
        "loss": 2.3319,
        "grad_norm": 2.0049397945404053,
        "learning_rate": 0.00013020254798488465,
        "epoch": 0.9372,
        "step": 7029
    },
    {
        "loss": 2.261,
        "grad_norm": 3.4094183444976807,
        "learning_rate": 0.00013014254441490213,
        "epoch": 0.9373333333333334,
        "step": 7030
    },
    {
        "loss": 1.129,
        "grad_norm": 4.313348770141602,
        "learning_rate": 0.000130082528905355,
        "epoch": 0.9374666666666667,
        "step": 7031
    },
    {
        "loss": 1.9495,
        "grad_norm": 3.3926281929016113,
        "learning_rate": 0.00013002250148001573,
        "epoch": 0.9376,
        "step": 7032
    },
    {
        "loss": 1.4201,
        "grad_norm": 2.692988634109497,
        "learning_rate": 0.00012996246216266126,
        "epoch": 0.9377333333333333,
        "step": 7033
    },
    {
        "loss": 1.384,
        "grad_norm": 6.201064109802246,
        "learning_rate": 0.00012990241097707352,
        "epoch": 0.9378666666666666,
        "step": 7034
    },
    {
        "loss": 2.1174,
        "grad_norm": 2.826702833175659,
        "learning_rate": 0.00012984234794703882,
        "epoch": 0.938,
        "step": 7035
    },
    {
        "loss": 2.0799,
        "grad_norm": 3.515639305114746,
        "learning_rate": 0.0001297822730963484,
        "epoch": 0.9381333333333334,
        "step": 7036
    },
    {
        "loss": 2.4519,
        "grad_norm": 3.143510341644287,
        "learning_rate": 0.00012972218644879822,
        "epoch": 0.9382666666666667,
        "step": 7037
    },
    {
        "loss": 2.1669,
        "grad_norm": 4.646274089813232,
        "learning_rate": 0.0001296620880281886,
        "epoch": 0.9384,
        "step": 7038
    },
    {
        "loss": 1.926,
        "grad_norm": 3.0826733112335205,
        "learning_rate": 0.00012960197785832488,
        "epoch": 0.9385333333333333,
        "step": 7039
    },
    {
        "loss": 2.4755,
        "grad_norm": 3.2294445037841797,
        "learning_rate": 0.00012954185596301696,
        "epoch": 0.9386666666666666,
        "step": 7040
    },
    {
        "loss": 2.6045,
        "grad_norm": 2.2067060470581055,
        "learning_rate": 0.0001294817223660791,
        "epoch": 0.9388,
        "step": 7041
    },
    {
        "loss": 2.8407,
        "grad_norm": 2.6637375354766846,
        "learning_rate": 0.00012942157709133065,
        "epoch": 0.9389333333333333,
        "step": 7042
    },
    {
        "loss": 2.0,
        "grad_norm": 3.5701866149902344,
        "learning_rate": 0.0001293614201625952,
        "epoch": 0.9390666666666667,
        "step": 7043
    },
    {
        "loss": 2.777,
        "grad_norm": 3.21355938911438,
        "learning_rate": 0.00012930125160370116,
        "epoch": 0.9392,
        "step": 7044
    },
    {
        "loss": 2.012,
        "grad_norm": 3.456474781036377,
        "learning_rate": 0.00012924107143848153,
        "epoch": 0.9393333333333334,
        "step": 7045
    },
    {
        "loss": 2.028,
        "grad_norm": 2.9841740131378174,
        "learning_rate": 0.00012918087969077393,
        "epoch": 0.9394666666666667,
        "step": 7046
    },
    {
        "loss": 2.517,
        "grad_norm": 3.669260025024414,
        "learning_rate": 0.0001291206763844204,
        "epoch": 0.9396,
        "step": 7047
    },
    {
        "loss": 1.8305,
        "grad_norm": 3.091419219970703,
        "learning_rate": 0.0001290604615432677,
        "epoch": 0.9397333333333333,
        "step": 7048
    },
    {
        "loss": 2.9951,
        "grad_norm": 3.0274498462677,
        "learning_rate": 0.00012900023519116723,
        "epoch": 0.9398666666666666,
        "step": 7049
    },
    {
        "loss": 1.8002,
        "grad_norm": 1.708919882774353,
        "learning_rate": 0.00012893999735197475,
        "epoch": 0.94,
        "step": 7050
    },
    {
        "loss": 2.8502,
        "grad_norm": 2.7411835193634033,
        "learning_rate": 0.00012887974804955075,
        "epoch": 0.9401333333333334,
        "step": 7051
    },
    {
        "loss": 3.6349,
        "grad_norm": 4.232717037200928,
        "learning_rate": 0.00012881948730776008,
        "epoch": 0.9402666666666667,
        "step": 7052
    },
    {
        "loss": 2.0337,
        "grad_norm": 3.910996675491333,
        "learning_rate": 0.00012875921515047235,
        "epoch": 0.9404,
        "step": 7053
    },
    {
        "loss": 2.3043,
        "grad_norm": 3.5842204093933105,
        "learning_rate": 0.0001286989316015614,
        "epoch": 0.9405333333333333,
        "step": 7054
    },
    {
        "loss": 2.0336,
        "grad_norm": 3.994239568710327,
        "learning_rate": 0.00012863863668490595,
        "epoch": 0.9406666666666667,
        "step": 7055
    },
    {
        "loss": 2.7399,
        "grad_norm": 2.4943528175354004,
        "learning_rate": 0.00012857833042438887,
        "epoch": 0.9408,
        "step": 7056
    },
    {
        "loss": 2.2942,
        "grad_norm": 3.37864351272583,
        "learning_rate": 0.0001285180128438978,
        "epoch": 0.9409333333333333,
        "step": 7057
    },
    {
        "loss": 2.469,
        "grad_norm": 2.6384856700897217,
        "learning_rate": 0.00012845768396732462,
        "epoch": 0.9410666666666667,
        "step": 7058
    },
    {
        "loss": 2.5072,
        "grad_norm": 1.9146010875701904,
        "learning_rate": 0.0001283973438185659,
        "epoch": 0.9412,
        "step": 7059
    },
    {
        "loss": 1.6633,
        "grad_norm": 2.9552741050720215,
        "learning_rate": 0.00012833699242152246,
        "epoch": 0.9413333333333334,
        "step": 7060
    },
    {
        "loss": 2.8722,
        "grad_norm": 3.3938581943511963,
        "learning_rate": 0.00012827662980009977,
        "epoch": 0.9414666666666667,
        "step": 7061
    },
    {
        "loss": 2.1194,
        "grad_norm": 5.480082988739014,
        "learning_rate": 0.00012821625597820777,
        "epoch": 0.9416,
        "step": 7062
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.7559142112731934,
        "learning_rate": 0.0001281558709797605,
        "epoch": 0.9417333333333333,
        "step": 7063
    },
    {
        "loss": 2.2014,
        "grad_norm": 3.283048629760742,
        "learning_rate": 0.00012809547482867686,
        "epoch": 0.9418666666666666,
        "step": 7064
    },
    {
        "loss": 2.1905,
        "grad_norm": 3.6972131729125977,
        "learning_rate": 0.00012803506754887985,
        "epoch": 0.942,
        "step": 7065
    },
    {
        "loss": 1.6635,
        "grad_norm": 3.798499822616577,
        "learning_rate": 0.00012797464916429715,
        "epoch": 0.9421333333333334,
        "step": 7066
    },
    {
        "loss": 2.0894,
        "grad_norm": 5.360044956207275,
        "learning_rate": 0.00012791421969886053,
        "epoch": 0.9422666666666667,
        "step": 7067
    },
    {
        "loss": 2.438,
        "grad_norm": 5.077503204345703,
        "learning_rate": 0.00012785377917650645,
        "epoch": 0.9424,
        "step": 7068
    },
    {
        "loss": 0.8227,
        "grad_norm": 3.8696610927581787,
        "learning_rate": 0.00012779332762117546,
        "epoch": 0.9425333333333333,
        "step": 7069
    },
    {
        "loss": 1.0608,
        "grad_norm": 2.847806930541992,
        "learning_rate": 0.00012773286505681267,
        "epoch": 0.9426666666666667,
        "step": 7070
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.9268906116485596,
        "learning_rate": 0.0001276723915073676,
        "epoch": 0.9428,
        "step": 7071
    },
    {
        "loss": 1.0363,
        "grad_norm": 3.259155035018921,
        "learning_rate": 0.00012761190699679394,
        "epoch": 0.9429333333333333,
        "step": 7072
    },
    {
        "loss": 1.0826,
        "grad_norm": 3.1296560764312744,
        "learning_rate": 0.0001275514115490498,
        "epoch": 0.9430666666666667,
        "step": 7073
    },
    {
        "loss": 1.1141,
        "grad_norm": 5.520702362060547,
        "learning_rate": 0.00012749090518809772,
        "epoch": 0.9432,
        "step": 7074
    },
    {
        "loss": 2.653,
        "grad_norm": 2.453275680541992,
        "learning_rate": 0.0001274303879379044,
        "epoch": 0.9433333333333334,
        "step": 7075
    },
    {
        "loss": 3.1173,
        "grad_norm": 4.309927940368652,
        "learning_rate": 0.00012736985982244098,
        "epoch": 0.9434666666666667,
        "step": 7076
    },
    {
        "loss": 3.0306,
        "grad_norm": 3.27709698677063,
        "learning_rate": 0.00012730932086568275,
        "epoch": 0.9436,
        "step": 7077
    },
    {
        "loss": 2.7356,
        "grad_norm": 3.1781351566314697,
        "learning_rate": 0.00012724877109160956,
        "epoch": 0.9437333333333333,
        "step": 7078
    },
    {
        "loss": 3.155,
        "grad_norm": 3.1365721225738525,
        "learning_rate": 0.00012718821052420517,
        "epoch": 0.9438666666666666,
        "step": 7079
    },
    {
        "loss": 1.2961,
        "grad_norm": 5.782041549682617,
        "learning_rate": 0.00012712763918745808,
        "epoch": 0.944,
        "step": 7080
    },
    {
        "loss": 2.229,
        "grad_norm": 3.5388424396514893,
        "learning_rate": 0.00012706705710536058,
        "epoch": 0.9441333333333334,
        "step": 7081
    },
    {
        "loss": 2.8139,
        "grad_norm": 3.819319248199463,
        "learning_rate": 0.0001270064643019096,
        "epoch": 0.9442666666666667,
        "step": 7082
    },
    {
        "loss": 2.0287,
        "grad_norm": 3.006075382232666,
        "learning_rate": 0.00012694586080110604,
        "epoch": 0.9444,
        "step": 7083
    },
    {
        "loss": 2.5381,
        "grad_norm": 4.5497517585754395,
        "learning_rate": 0.0001268852466269552,
        "epoch": 0.9445333333333333,
        "step": 7084
    },
    {
        "loss": 2.9218,
        "grad_norm": 2.673372983932495,
        "learning_rate": 0.0001268246218034666,
        "epoch": 0.9446666666666667,
        "step": 7085
    },
    {
        "loss": 2.7523,
        "grad_norm": 2.8441243171691895,
        "learning_rate": 0.00012676398635465388,
        "epoch": 0.9448,
        "step": 7086
    },
    {
        "loss": 2.3636,
        "grad_norm": 2.8633861541748047,
        "learning_rate": 0.00012670334030453502,
        "epoch": 0.9449333333333333,
        "step": 7087
    },
    {
        "loss": 2.5118,
        "grad_norm": 1.9511114358901978,
        "learning_rate": 0.00012664268367713191,
        "epoch": 0.9450666666666667,
        "step": 7088
    },
    {
        "loss": 0.9647,
        "grad_norm": 1.927965760231018,
        "learning_rate": 0.00012658201649647116,
        "epoch": 0.9452,
        "step": 7089
    },
    {
        "loss": 2.0091,
        "grad_norm": 3.2872562408447266,
        "learning_rate": 0.0001265213387865831,
        "epoch": 0.9453333333333334,
        "step": 7090
    },
    {
        "loss": 2.4582,
        "grad_norm": 5.760671138763428,
        "learning_rate": 0.00012646065057150233,
        "epoch": 0.9454666666666667,
        "step": 7091
    },
    {
        "loss": 1.0201,
        "grad_norm": 3.187824249267578,
        "learning_rate": 0.0001263999518752677,
        "epoch": 0.9456,
        "step": 7092
    },
    {
        "loss": 2.0999,
        "grad_norm": 3.3241279125213623,
        "learning_rate": 0.00012633924272192226,
        "epoch": 0.9457333333333333,
        "step": 7093
    },
    {
        "loss": 2.5087,
        "grad_norm": 2.9348745346069336,
        "learning_rate": 0.00012627852313551295,
        "epoch": 0.9458666666666666,
        "step": 7094
    },
    {
        "loss": 2.7154,
        "grad_norm": 4.915219306945801,
        "learning_rate": 0.00012621779314009107,
        "epoch": 0.946,
        "step": 7095
    },
    {
        "loss": 2.5989,
        "grad_norm": 2.6754517555236816,
        "learning_rate": 0.00012615705275971205,
        "epoch": 0.9461333333333334,
        "step": 7096
    },
    {
        "loss": 2.7968,
        "grad_norm": 2.5497419834136963,
        "learning_rate": 0.00012609630201843527,
        "epoch": 0.9462666666666667,
        "step": 7097
    },
    {
        "loss": 2.301,
        "grad_norm": 3.098125457763672,
        "learning_rate": 0.0001260355409403243,
        "epoch": 0.9464,
        "step": 7098
    },
    {
        "loss": 2.096,
        "grad_norm": 2.5794918537139893,
        "learning_rate": 0.00012597476954944695,
        "epoch": 0.9465333333333333,
        "step": 7099
    },
    {
        "loss": 2.9346,
        "grad_norm": 3.7415480613708496,
        "learning_rate": 0.0001259139878698748,
        "epoch": 0.9466666666666667,
        "step": 7100
    },
    {
        "loss": 2.1555,
        "grad_norm": 3.226607084274292,
        "learning_rate": 0.00012585319592568384,
        "epoch": 0.9468,
        "step": 7101
    },
    {
        "loss": 2.7283,
        "grad_norm": 4.678950786590576,
        "learning_rate": 0.00012579239374095377,
        "epoch": 0.9469333333333333,
        "step": 7102
    },
    {
        "loss": 2.7649,
        "grad_norm": 2.965409755706787,
        "learning_rate": 0.00012573158133976874,
        "epoch": 0.9470666666666666,
        "step": 7103
    },
    {
        "loss": 1.7599,
        "grad_norm": 3.4771370887756348,
        "learning_rate": 0.00012567075874621655,
        "epoch": 0.9472,
        "step": 7104
    },
    {
        "loss": 2.106,
        "grad_norm": 2.5023136138916016,
        "learning_rate": 0.00012560992598438948,
        "epoch": 0.9473333333333334,
        "step": 7105
    },
    {
        "loss": 1.8234,
        "grad_norm": 3.6942191123962402,
        "learning_rate": 0.00012554908307838337,
        "epoch": 0.9474666666666667,
        "step": 7106
    },
    {
        "loss": 0.8307,
        "grad_norm": 2.7794535160064697,
        "learning_rate": 0.0001254882300522984,
        "epoch": 0.9476,
        "step": 7107
    },
    {
        "loss": 3.0164,
        "grad_norm": 2.240863800048828,
        "learning_rate": 0.00012542736693023872,
        "epoch": 0.9477333333333333,
        "step": 7108
    },
    {
        "loss": 1.4942,
        "grad_norm": 5.99458122253418,
        "learning_rate": 0.0001253664937363123,
        "epoch": 0.9478666666666666,
        "step": 7109
    },
    {
        "loss": 1.2571,
        "grad_norm": 3.372600555419922,
        "learning_rate": 0.00012530561049463136,
        "epoch": 0.948,
        "step": 7110
    },
    {
        "loss": 2.4788,
        "grad_norm": 2.9069206714630127,
        "learning_rate": 0.00012524471722931183,
        "epoch": 0.9481333333333334,
        "step": 7111
    },
    {
        "loss": 2.2675,
        "grad_norm": 3.4459640979766846,
        "learning_rate": 0.0001251838139644738,
        "epoch": 0.9482666666666667,
        "step": 7112
    },
    {
        "loss": 2.2434,
        "grad_norm": 3.677744150161743,
        "learning_rate": 0.00012512290072424112,
        "epoch": 0.9484,
        "step": 7113
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.3585093021392822,
        "learning_rate": 0.00012506197753274202,
        "epoch": 0.9485333333333333,
        "step": 7114
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.41426420211792,
        "learning_rate": 0.00012500104441410812,
        "epoch": 0.9486666666666667,
        "step": 7115
    },
    {
        "loss": 2.1838,
        "grad_norm": 3.9757981300354004,
        "learning_rate": 0.00012494010139247548,
        "epoch": 0.9488,
        "step": 7116
    },
    {
        "loss": 2.4694,
        "grad_norm": 3.054128885269165,
        "learning_rate": 0.00012487914849198354,
        "epoch": 0.9489333333333333,
        "step": 7117
    },
    {
        "loss": 2.5561,
        "grad_norm": 3.356214761734009,
        "learning_rate": 0.00012481818573677623,
        "epoch": 0.9490666666666666,
        "step": 7118
    },
    {
        "loss": 2.0956,
        "grad_norm": 2.532696485519409,
        "learning_rate": 0.00012475721315100092,
        "epoch": 0.9492,
        "step": 7119
    },
    {
        "loss": 2.4954,
        "grad_norm": 3.2231531143188477,
        "learning_rate": 0.00012469623075880907,
        "epoch": 0.9493333333333334,
        "step": 7120
    },
    {
        "loss": 0.8889,
        "grad_norm": 4.110384464263916,
        "learning_rate": 0.00012463523858435618,
        "epoch": 0.9494666666666667,
        "step": 7121
    },
    {
        "loss": 2.3391,
        "grad_norm": 2.811845541000366,
        "learning_rate": 0.00012457423665180125,
        "epoch": 0.9496,
        "step": 7122
    },
    {
        "loss": 2.3413,
        "grad_norm": 4.3919453620910645,
        "learning_rate": 0.0001245132249853075,
        "epoch": 0.9497333333333333,
        "step": 7123
    },
    {
        "loss": 2.5188,
        "grad_norm": 4.095239162445068,
        "learning_rate": 0.00012445220360904173,
        "epoch": 0.9498666666666666,
        "step": 7124
    },
    {
        "loss": 2.3477,
        "grad_norm": 3.5417253971099854,
        "learning_rate": 0.0001243911725471749,
        "epoch": 0.95,
        "step": 7125
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.5958597660064697,
        "learning_rate": 0.00012433013182388148,
        "epoch": 0.9501333333333334,
        "step": 7126
    },
    {
        "loss": 2.6838,
        "grad_norm": 3.737924098968506,
        "learning_rate": 0.00012426908146333997,
        "epoch": 0.9502666666666667,
        "step": 7127
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.8416714668273926,
        "learning_rate": 0.0001242080214897325,
        "epoch": 0.9504,
        "step": 7128
    },
    {
        "loss": 1.9988,
        "grad_norm": 4.306347846984863,
        "learning_rate": 0.00012414695192724525,
        "epoch": 0.9505333333333333,
        "step": 7129
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.907590627670288,
        "learning_rate": 0.00012408587280006816,
        "epoch": 0.9506666666666667,
        "step": 7130
    },
    {
        "loss": 2.5291,
        "grad_norm": 2.061661958694458,
        "learning_rate": 0.00012402478413239468,
        "epoch": 0.9508,
        "step": 7131
    },
    {
        "loss": 2.7476,
        "grad_norm": 2.4205873012542725,
        "learning_rate": 0.00012396368594842238,
        "epoch": 0.9509333333333333,
        "step": 7132
    },
    {
        "loss": 2.2422,
        "grad_norm": 2.342594623565674,
        "learning_rate": 0.00012390257827235248,
        "epoch": 0.9510666666666666,
        "step": 7133
    },
    {
        "loss": 3.1937,
        "grad_norm": 2.5112361907958984,
        "learning_rate": 0.0001238414611283898,
        "epoch": 0.9512,
        "step": 7134
    },
    {
        "loss": 2.5212,
        "grad_norm": 2.6683366298675537,
        "learning_rate": 0.00012378033454074318,
        "epoch": 0.9513333333333334,
        "step": 7135
    },
    {
        "loss": 1.4172,
        "grad_norm": 3.0459656715393066,
        "learning_rate": 0.000123719198533625,
        "epoch": 0.9514666666666667,
        "step": 7136
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.3836355209350586,
        "learning_rate": 0.0001236580531312515,
        "epoch": 0.9516,
        "step": 7137
    },
    {
        "loss": 2.4437,
        "grad_norm": 3.5807557106018066,
        "learning_rate": 0.0001235968983578424,
        "epoch": 0.9517333333333333,
        "step": 7138
    },
    {
        "loss": 2.7782,
        "grad_norm": 2.1262221336364746,
        "learning_rate": 0.0001235357342376216,
        "epoch": 0.9518666666666666,
        "step": 7139
    },
    {
        "loss": 2.1255,
        "grad_norm": 4.096174240112305,
        "learning_rate": 0.00012347456079481617,
        "epoch": 0.952,
        "step": 7140
    },
    {
        "loss": 2.89,
        "grad_norm": 3.7985880374908447,
        "learning_rate": 0.00012341337805365737,
        "epoch": 0.9521333333333334,
        "step": 7141
    },
    {
        "loss": 2.1321,
        "grad_norm": 6.412561893463135,
        "learning_rate": 0.0001233521860383796,
        "epoch": 0.9522666666666667,
        "step": 7142
    },
    {
        "loss": 2.2291,
        "grad_norm": 4.0272417068481445,
        "learning_rate": 0.00012329098477322146,
        "epoch": 0.9524,
        "step": 7143
    },
    {
        "loss": 1.9888,
        "grad_norm": 2.2132906913757324,
        "learning_rate": 0.00012322977428242483,
        "epoch": 0.9525333333333333,
        "step": 7144
    },
    {
        "loss": 2.2557,
        "grad_norm": 3.6417553424835205,
        "learning_rate": 0.00012316855459023544,
        "epoch": 0.9526666666666667,
        "step": 7145
    },
    {
        "loss": 1.8227,
        "grad_norm": 3.888087511062622,
        "learning_rate": 0.0001231073257209027,
        "epoch": 0.9528,
        "step": 7146
    },
    {
        "loss": 2.1613,
        "grad_norm": 4.7147088050842285,
        "learning_rate": 0.0001230460876986794,
        "epoch": 0.9529333333333333,
        "step": 7147
    },
    {
        "loss": 2.118,
        "grad_norm": 3.3025708198547363,
        "learning_rate": 0.00012298484054782234,
        "epoch": 0.9530666666666666,
        "step": 7148
    },
    {
        "loss": 1.7175,
        "grad_norm": 4.778859615325928,
        "learning_rate": 0.00012292358429259157,
        "epoch": 0.9532,
        "step": 7149
    },
    {
        "loss": 2.363,
        "grad_norm": 3.98207950592041,
        "learning_rate": 0.000122862318957251,
        "epoch": 0.9533333333333334,
        "step": 7150
    },
    {
        "loss": 2.45,
        "grad_norm": 4.56386137008667,
        "learning_rate": 0.00012280104456606792,
        "epoch": 0.9534666666666667,
        "step": 7151
    },
    {
        "loss": 2.7522,
        "grad_norm": 2.2834815979003906,
        "learning_rate": 0.00012273976114331352,
        "epoch": 0.9536,
        "step": 7152
    },
    {
        "loss": 0.8768,
        "grad_norm": 3.9801619052886963,
        "learning_rate": 0.00012267846871326208,
        "epoch": 0.9537333333333333,
        "step": 7153
    },
    {
        "loss": 2.8408,
        "grad_norm": 3.538210391998291,
        "learning_rate": 0.000122617167300192,
        "epoch": 0.9538666666666666,
        "step": 7154
    },
    {
        "loss": 1.8854,
        "grad_norm": 3.6537046432495117,
        "learning_rate": 0.0001225558569283849,
        "epoch": 0.954,
        "step": 7155
    },
    {
        "loss": 2.3805,
        "grad_norm": 3.157848834991455,
        "learning_rate": 0.00012249453762212597,
        "epoch": 0.9541333333333334,
        "step": 7156
    },
    {
        "loss": 2.2506,
        "grad_norm": 2.7813315391540527,
        "learning_rate": 0.00012243320940570404,
        "epoch": 0.9542666666666667,
        "step": 7157
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.8476951122283936,
        "learning_rate": 0.00012237187230341147,
        "epoch": 0.9544,
        "step": 7158
    },
    {
        "loss": 2.8167,
        "grad_norm": 3.0607080459594727,
        "learning_rate": 0.000122310526339544,
        "epoch": 0.9545333333333333,
        "step": 7159
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.865335464477539,
        "learning_rate": 0.0001222491715384011,
        "epoch": 0.9546666666666667,
        "step": 7160
    },
    {
        "loss": 1.7098,
        "grad_norm": 2.572144031524658,
        "learning_rate": 0.00012218780792428547,
        "epoch": 0.9548,
        "step": 7161
    },
    {
        "loss": 2.505,
        "grad_norm": 3.0427989959716797,
        "learning_rate": 0.0001221264355215036,
        "epoch": 0.9549333333333333,
        "step": 7162
    },
    {
        "loss": 2.2569,
        "grad_norm": 3.0704972743988037,
        "learning_rate": 0.00012206505435436503,
        "epoch": 0.9550666666666666,
        "step": 7163
    },
    {
        "loss": 2.5993,
        "grad_norm": 3.4193577766418457,
        "learning_rate": 0.00012200366444718344,
        "epoch": 0.9552,
        "step": 7164
    },
    {
        "loss": 2.1294,
        "grad_norm": 4.345877170562744,
        "learning_rate": 0.0001219422658242753,
        "epoch": 0.9553333333333334,
        "step": 7165
    },
    {
        "loss": 1.9921,
        "grad_norm": 3.1975812911987305,
        "learning_rate": 0.00012188085850996092,
        "epoch": 0.9554666666666667,
        "step": 7166
    },
    {
        "loss": 1.0534,
        "grad_norm": 3.5983047485351562,
        "learning_rate": 0.00012181944252856394,
        "epoch": 0.9556,
        "step": 7167
    },
    {
        "loss": 2.0185,
        "grad_norm": 2.6109015941619873,
        "learning_rate": 0.00012175801790441142,
        "epoch": 0.9557333333333333,
        "step": 7168
    },
    {
        "loss": 2.6287,
        "grad_norm": 3.5919084548950195,
        "learning_rate": 0.00012169658466183392,
        "epoch": 0.9558666666666666,
        "step": 7169
    },
    {
        "loss": 1.7506,
        "grad_norm": 2.993818521499634,
        "learning_rate": 0.00012163514282516521,
        "epoch": 0.956,
        "step": 7170
    },
    {
        "loss": 3.1851,
        "grad_norm": 2.3956105709075928,
        "learning_rate": 0.0001215736924187428,
        "epoch": 0.9561333333333333,
        "step": 7171
    },
    {
        "loss": 2.1173,
        "grad_norm": 3.2864437103271484,
        "learning_rate": 0.00012151223346690717,
        "epoch": 0.9562666666666667,
        "step": 7172
    },
    {
        "loss": 2.2206,
        "grad_norm": 3.7061243057250977,
        "learning_rate": 0.00012145076599400271,
        "epoch": 0.9564,
        "step": 7173
    },
    {
        "loss": 2.2762,
        "grad_norm": 5.381947994232178,
        "learning_rate": 0.00012138929002437664,
        "epoch": 0.9565333333333333,
        "step": 7174
    },
    {
        "loss": 2.1391,
        "grad_norm": 4.110231399536133,
        "learning_rate": 0.00012132780558238002,
        "epoch": 0.9566666666666667,
        "step": 7175
    },
    {
        "loss": 2.441,
        "grad_norm": 3.0838124752044678,
        "learning_rate": 0.00012126631269236686,
        "epoch": 0.9568,
        "step": 7176
    },
    {
        "loss": 2.6841,
        "grad_norm": 4.54297399520874,
        "learning_rate": 0.00012120481137869486,
        "epoch": 0.9569333333333333,
        "step": 7177
    },
    {
        "loss": 2.8726,
        "grad_norm": 3.003767251968384,
        "learning_rate": 0.00012114330166572472,
        "epoch": 0.9570666666666666,
        "step": 7178
    },
    {
        "loss": 2.8348,
        "grad_norm": 3.03554105758667,
        "learning_rate": 0.00012108178357782077,
        "epoch": 0.9572,
        "step": 7179
    },
    {
        "loss": 1.0633,
        "grad_norm": 3.7550041675567627,
        "learning_rate": 0.00012102025713935057,
        "epoch": 0.9573333333333334,
        "step": 7180
    },
    {
        "loss": 3.099,
        "grad_norm": 3.316098690032959,
        "learning_rate": 0.00012095872237468481,
        "epoch": 0.9574666666666667,
        "step": 7181
    },
    {
        "loss": 1.9076,
        "grad_norm": 3.4005889892578125,
        "learning_rate": 0.0001208971793081977,
        "epoch": 0.9576,
        "step": 7182
    },
    {
        "loss": 2.2571,
        "grad_norm": 3.5149543285369873,
        "learning_rate": 0.00012083562796426675,
        "epoch": 0.9577333333333333,
        "step": 7183
    },
    {
        "loss": 2.4184,
        "grad_norm": 4.128891468048096,
        "learning_rate": 0.00012077406836727252,
        "epoch": 0.9578666666666666,
        "step": 7184
    },
    {
        "loss": 2.6293,
        "grad_norm": 2.0842947959899902,
        "learning_rate": 0.00012071250054159897,
        "epoch": 0.958,
        "step": 7185
    },
    {
        "loss": 1.4499,
        "grad_norm": 4.365720272064209,
        "learning_rate": 0.00012065092451163349,
        "epoch": 0.9581333333333333,
        "step": 7186
    },
    {
        "loss": 1.4961,
        "grad_norm": 5.74432373046875,
        "learning_rate": 0.00012058934030176636,
        "epoch": 0.9582666666666667,
        "step": 7187
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.9981861114501953,
        "learning_rate": 0.00012052774793639136,
        "epoch": 0.9584,
        "step": 7188
    },
    {
        "loss": 2.3901,
        "grad_norm": 3.392263889312744,
        "learning_rate": 0.00012046614743990556,
        "epoch": 0.9585333333333333,
        "step": 7189
    },
    {
        "loss": 1.509,
        "grad_norm": 3.67141056060791,
        "learning_rate": 0.00012040453883670894,
        "epoch": 0.9586666666666667,
        "step": 7190
    },
    {
        "loss": 2.1862,
        "grad_norm": 4.70503044128418,
        "learning_rate": 0.00012034292215120497,
        "epoch": 0.9588,
        "step": 7191
    },
    {
        "loss": 2.6865,
        "grad_norm": 2.8555915355682373,
        "learning_rate": 0.00012028129740780025,
        "epoch": 0.9589333333333333,
        "step": 7192
    },
    {
        "loss": 2.3817,
        "grad_norm": 3.016366481781006,
        "learning_rate": 0.0001202196646309045,
        "epoch": 0.9590666666666666,
        "step": 7193
    },
    {
        "loss": 1.7504,
        "grad_norm": 3.406581163406372,
        "learning_rate": 0.00012015802384493073,
        "epoch": 0.9592,
        "step": 7194
    },
    {
        "loss": 2.7112,
        "grad_norm": 3.3908066749572754,
        "learning_rate": 0.00012009637507429498,
        "epoch": 0.9593333333333334,
        "step": 7195
    },
    {
        "loss": 2.3774,
        "grad_norm": 3.040288209915161,
        "learning_rate": 0.00012003471834341665,
        "epoch": 0.9594666666666667,
        "step": 7196
    },
    {
        "loss": 2.4686,
        "grad_norm": 3.5202789306640625,
        "learning_rate": 0.00011997305367671799,
        "epoch": 0.9596,
        "step": 7197
    },
    {
        "loss": 2.3741,
        "grad_norm": 3.134456157684326,
        "learning_rate": 0.00011991138109862484,
        "epoch": 0.9597333333333333,
        "step": 7198
    },
    {
        "loss": 2.7655,
        "grad_norm": 2.333850622177124,
        "learning_rate": 0.00011984970063356569,
        "epoch": 0.9598666666666666,
        "step": 7199
    },
    {
        "loss": 1.7563,
        "grad_norm": 4.517764091491699,
        "learning_rate": 0.00011978801230597259,
        "epoch": 0.96,
        "step": 7200
    },
    {
        "loss": 2.5904,
        "grad_norm": 2.599085569381714,
        "learning_rate": 0.00011972631614028032,
        "epoch": 0.9601333333333333,
        "step": 7201
    },
    {
        "loss": 1.9502,
        "grad_norm": 3.3316268920898438,
        "learning_rate": 0.00011966461216092706,
        "epoch": 0.9602666666666667,
        "step": 7202
    },
    {
        "loss": 2.2725,
        "grad_norm": 3.369032859802246,
        "learning_rate": 0.00011960290039235384,
        "epoch": 0.9604,
        "step": 7203
    },
    {
        "loss": 2.1595,
        "grad_norm": 2.925873041152954,
        "learning_rate": 0.000119541180859005,
        "epoch": 0.9605333333333334,
        "step": 7204
    },
    {
        "loss": 1.6266,
        "grad_norm": 5.1123151779174805,
        "learning_rate": 0.0001194794535853279,
        "epoch": 0.9606666666666667,
        "step": 7205
    },
    {
        "loss": 1.839,
        "grad_norm": 3.4692118167877197,
        "learning_rate": 0.00011941771859577273,
        "epoch": 0.9608,
        "step": 7206
    },
    {
        "loss": 0.9899,
        "grad_norm": 3.624819040298462,
        "learning_rate": 0.00011935597591479319,
        "epoch": 0.9609333333333333,
        "step": 7207
    },
    {
        "loss": 2.6034,
        "grad_norm": 3.2034976482391357,
        "learning_rate": 0.00011929422556684557,
        "epoch": 0.9610666666666666,
        "step": 7208
    },
    {
        "loss": 2.6335,
        "grad_norm": 3.474010944366455,
        "learning_rate": 0.00011923246757638953,
        "epoch": 0.9612,
        "step": 7209
    },
    {
        "loss": 2.6723,
        "grad_norm": 3.5024821758270264,
        "learning_rate": 0.00011917070196788746,
        "epoch": 0.9613333333333334,
        "step": 7210
    },
    {
        "loss": 2.7135,
        "grad_norm": 2.5272278785705566,
        "learning_rate": 0.00011910892876580513,
        "epoch": 0.9614666666666667,
        "step": 7211
    },
    {
        "loss": 2.9601,
        "grad_norm": 1.946734070777893,
        "learning_rate": 0.00011904714799461092,
        "epoch": 0.9616,
        "step": 7212
    },
    {
        "loss": 2.0867,
        "grad_norm": 3.339056968688965,
        "learning_rate": 0.00011898535967877648,
        "epoch": 0.9617333333333333,
        "step": 7213
    },
    {
        "loss": 1.2729,
        "grad_norm": 5.229920387268066,
        "learning_rate": 0.00011892356384277644,
        "epoch": 0.9618666666666666,
        "step": 7214
    },
    {
        "loss": 3.1502,
        "grad_norm": 2.8693439960479736,
        "learning_rate": 0.00011886176051108821,
        "epoch": 0.962,
        "step": 7215
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.3545055389404297,
        "learning_rate": 0.00011879994970819239,
        "epoch": 0.9621333333333333,
        "step": 7216
    },
    {
        "loss": 2.2069,
        "grad_norm": 3.2901289463043213,
        "learning_rate": 0.00011873813145857249,
        "epoch": 0.9622666666666667,
        "step": 7217
    },
    {
        "loss": 2.1353,
        "grad_norm": 3.358527183532715,
        "learning_rate": 0.0001186763057867148,
        "epoch": 0.9624,
        "step": 7218
    },
    {
        "loss": 2.3471,
        "grad_norm": 3.63968563079834,
        "learning_rate": 0.00011861447271710884,
        "epoch": 0.9625333333333334,
        "step": 7219
    },
    {
        "loss": 3.0088,
        "grad_norm": 3.137000560760498,
        "learning_rate": 0.00011855263227424671,
        "epoch": 0.9626666666666667,
        "step": 7220
    },
    {
        "loss": 2.325,
        "grad_norm": 3.1012704372406006,
        "learning_rate": 0.00011849078448262381,
        "epoch": 0.9628,
        "step": 7221
    },
    {
        "loss": 1.2566,
        "grad_norm": 3.6748082637786865,
        "learning_rate": 0.00011842892936673801,
        "epoch": 0.9629333333333333,
        "step": 7222
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.8420400619506836,
        "learning_rate": 0.00011836706695109065,
        "epoch": 0.9630666666666666,
        "step": 7223
    },
    {
        "loss": 2.7439,
        "grad_norm": 2.8212742805480957,
        "learning_rate": 0.00011830519726018543,
        "epoch": 0.9632,
        "step": 7224
    },
    {
        "loss": 1.4257,
        "grad_norm": 2.9540114402770996,
        "learning_rate": 0.00011824332031852916,
        "epoch": 0.9633333333333334,
        "step": 7225
    },
    {
        "loss": 2.8129,
        "grad_norm": 4.20261812210083,
        "learning_rate": 0.00011818143615063167,
        "epoch": 0.9634666666666667,
        "step": 7226
    },
    {
        "loss": 2.3523,
        "grad_norm": 2.9694912433624268,
        "learning_rate": 0.00011811954478100526,
        "epoch": 0.9636,
        "step": 7227
    },
    {
        "loss": 1.8034,
        "grad_norm": 3.7234046459198,
        "learning_rate": 0.0001180576462341655,
        "epoch": 0.9637333333333333,
        "step": 7228
    },
    {
        "loss": 1.5941,
        "grad_norm": 3.9937005043029785,
        "learning_rate": 0.00011799574053463047,
        "epoch": 0.9638666666666666,
        "step": 7229
    },
    {
        "loss": 2.3269,
        "grad_norm": 2.875946521759033,
        "learning_rate": 0.00011793382770692136,
        "epoch": 0.964,
        "step": 7230
    },
    {
        "loss": 2.2509,
        "grad_norm": 3.1011886596679688,
        "learning_rate": 0.00011787190777556187,
        "epoch": 0.9641333333333333,
        "step": 7231
    },
    {
        "loss": 1.6136,
        "grad_norm": 2.5189051628112793,
        "learning_rate": 0.00011780998076507892,
        "epoch": 0.9642666666666667,
        "step": 7232
    },
    {
        "loss": 2.0516,
        "grad_norm": 4.879512786865234,
        "learning_rate": 0.00011774804670000186,
        "epoch": 0.9644,
        "step": 7233
    },
    {
        "loss": 2.2277,
        "grad_norm": 3.0431697368621826,
        "learning_rate": 0.00011768610560486308,
        "epoch": 0.9645333333333334,
        "step": 7234
    },
    {
        "loss": 1.5906,
        "grad_norm": 6.895092487335205,
        "learning_rate": 0.00011762415750419755,
        "epoch": 0.9646666666666667,
        "step": 7235
    },
    {
        "loss": 2.5013,
        "grad_norm": 2.8332507610321045,
        "learning_rate": 0.00011756220242254323,
        "epoch": 0.9648,
        "step": 7236
    },
    {
        "loss": 0.6826,
        "grad_norm": 3.231496572494507,
        "learning_rate": 0.00011750024038444065,
        "epoch": 0.9649333333333333,
        "step": 7237
    },
    {
        "loss": 2.8647,
        "grad_norm": 3.8028745651245117,
        "learning_rate": 0.0001174382714144332,
        "epoch": 0.9650666666666666,
        "step": 7238
    },
    {
        "loss": 2.1803,
        "grad_norm": 4.395902156829834,
        "learning_rate": 0.00011737629553706708,
        "epoch": 0.9652,
        "step": 7239
    },
    {
        "loss": 2.4169,
        "grad_norm": 3.3191072940826416,
        "learning_rate": 0.00011731431277689104,
        "epoch": 0.9653333333333334,
        "step": 7240
    },
    {
        "loss": 2.7445,
        "grad_norm": 3.1265530586242676,
        "learning_rate": 0.00011725232315845671,
        "epoch": 0.9654666666666667,
        "step": 7241
    },
    {
        "loss": 2.2907,
        "grad_norm": 2.5359411239624023,
        "learning_rate": 0.00011719032670631849,
        "epoch": 0.9656,
        "step": 7242
    },
    {
        "loss": 2.008,
        "grad_norm": 3.09248685836792,
        "learning_rate": 0.00011712832344503315,
        "epoch": 0.9657333333333333,
        "step": 7243
    },
    {
        "loss": 1.7344,
        "grad_norm": 2.4790618419647217,
        "learning_rate": 0.00011706631339916065,
        "epoch": 0.9658666666666667,
        "step": 7244
    },
    {
        "loss": 2.306,
        "grad_norm": 3.70786714553833,
        "learning_rate": 0.00011700429659326318,
        "epoch": 0.966,
        "step": 7245
    },
    {
        "loss": 2.2332,
        "grad_norm": 3.928283929824829,
        "learning_rate": 0.00011694227305190588,
        "epoch": 0.9661333333333333,
        "step": 7246
    },
    {
        "loss": 2.5886,
        "grad_norm": 4.580767631530762,
        "learning_rate": 0.0001168802427996565,
        "epoch": 0.9662666666666667,
        "step": 7247
    },
    {
        "loss": 2.9061,
        "grad_norm": 1.9701529741287231,
        "learning_rate": 0.00011681820586108552,
        "epoch": 0.9664,
        "step": 7248
    },
    {
        "loss": 2.6534,
        "grad_norm": 2.702171564102173,
        "learning_rate": 0.0001167561622607658,
        "epoch": 0.9665333333333334,
        "step": 7249
    },
    {
        "loss": 1.5797,
        "grad_norm": 2.3807883262634277,
        "learning_rate": 0.0001166941120232731,
        "epoch": 0.9666666666666667,
        "step": 7250
    },
    {
        "loss": 2.4409,
        "grad_norm": 2.2820827960968018,
        "learning_rate": 0.00011663205517318582,
        "epoch": 0.9668,
        "step": 7251
    },
    {
        "loss": 2.0624,
        "grad_norm": 2.658552646636963,
        "learning_rate": 0.00011656999173508478,
        "epoch": 0.9669333333333333,
        "step": 7252
    },
    {
        "loss": 1.3084,
        "grad_norm": 3.9395596981048584,
        "learning_rate": 0.00011650792173355358,
        "epoch": 0.9670666666666666,
        "step": 7253
    },
    {
        "loss": 2.7639,
        "grad_norm": 4.124065399169922,
        "learning_rate": 0.00011644584519317828,
        "epoch": 0.9672,
        "step": 7254
    },
    {
        "loss": 2.1035,
        "grad_norm": 2.241609811782837,
        "learning_rate": 0.00011638376213854773,
        "epoch": 0.9673333333333334,
        "step": 7255
    },
    {
        "loss": 2.2698,
        "grad_norm": 4.018316268920898,
        "learning_rate": 0.00011632167259425304,
        "epoch": 0.9674666666666667,
        "step": 7256
    },
    {
        "loss": 2.3458,
        "grad_norm": 3.115635395050049,
        "learning_rate": 0.00011625957658488833,
        "epoch": 0.9676,
        "step": 7257
    },
    {
        "loss": 2.7648,
        "grad_norm": 5.063719272613525,
        "learning_rate": 0.00011619747413504989,
        "epoch": 0.9677333333333333,
        "step": 7258
    },
    {
        "loss": 2.1533,
        "grad_norm": 4.796276569366455,
        "learning_rate": 0.00011613536526933682,
        "epoch": 0.9678666666666667,
        "step": 7259
    },
    {
        "loss": 2.0672,
        "grad_norm": 3.328723907470703,
        "learning_rate": 0.00011607325001235051,
        "epoch": 0.968,
        "step": 7260
    },
    {
        "loss": 2.266,
        "grad_norm": 7.786264896392822,
        "learning_rate": 0.00011601112838869517,
        "epoch": 0.9681333333333333,
        "step": 7261
    },
    {
        "loss": 1.5837,
        "grad_norm": 4.160078048706055,
        "learning_rate": 0.00011594900042297726,
        "epoch": 0.9682666666666667,
        "step": 7262
    },
    {
        "loss": 2.9278,
        "grad_norm": 2.651554584503174,
        "learning_rate": 0.00011588686613980592,
        "epoch": 0.9684,
        "step": 7263
    },
    {
        "loss": 0.7574,
        "grad_norm": 2.756045341491699,
        "learning_rate": 0.00011582472556379285,
        "epoch": 0.9685333333333334,
        "step": 7264
    },
    {
        "loss": 1.8007,
        "grad_norm": 3.621962547302246,
        "learning_rate": 0.00011576257871955192,
        "epoch": 0.9686666666666667,
        "step": 7265
    },
    {
        "loss": 0.8804,
        "grad_norm": 3.601952075958252,
        "learning_rate": 0.00011570042563169998,
        "epoch": 0.9688,
        "step": 7266
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.969324827194214,
        "learning_rate": 0.0001156382663248559,
        "epoch": 0.9689333333333333,
        "step": 7267
    },
    {
        "loss": 1.8808,
        "grad_norm": 4.852372169494629,
        "learning_rate": 0.0001155761008236413,
        "epoch": 0.9690666666666666,
        "step": 7268
    },
    {
        "loss": 1.9258,
        "grad_norm": 4.574278831481934,
        "learning_rate": 0.00011551392915268002,
        "epoch": 0.9692,
        "step": 7269
    },
    {
        "loss": 1.221,
        "grad_norm": 3.213869094848633,
        "learning_rate": 0.00011545175133659863,
        "epoch": 0.9693333333333334,
        "step": 7270
    },
    {
        "loss": 1.7706,
        "grad_norm": 4.322087287902832,
        "learning_rate": 0.0001153895674000258,
        "epoch": 0.9694666666666667,
        "step": 7271
    },
    {
        "loss": 2.3414,
        "grad_norm": 3.424795150756836,
        "learning_rate": 0.00011532737736759286,
        "epoch": 0.9696,
        "step": 7272
    },
    {
        "loss": 2.6835,
        "grad_norm": 2.611379861831665,
        "learning_rate": 0.00011526518126393362,
        "epoch": 0.9697333333333333,
        "step": 7273
    },
    {
        "loss": 1.5339,
        "grad_norm": 5.084744930267334,
        "learning_rate": 0.00011520297911368398,
        "epoch": 0.9698666666666667,
        "step": 7274
    },
    {
        "loss": 2.1684,
        "grad_norm": 5.559584140777588,
        "learning_rate": 0.00011514077094148253,
        "epoch": 0.97,
        "step": 7275
    },
    {
        "loss": 2.4061,
        "grad_norm": 2.5948166847229004,
        "learning_rate": 0.00011507855677197017,
        "epoch": 0.9701333333333333,
        "step": 7276
    },
    {
        "loss": 2.6067,
        "grad_norm": 2.918177843093872,
        "learning_rate": 0.00011501633662979003,
        "epoch": 0.9702666666666667,
        "step": 7277
    },
    {
        "loss": 2.7859,
        "grad_norm": 3.4389007091522217,
        "learning_rate": 0.00011495411053958787,
        "epoch": 0.9704,
        "step": 7278
    },
    {
        "loss": 2.9398,
        "grad_norm": 3.625793933868408,
        "learning_rate": 0.00011489187852601147,
        "epoch": 0.9705333333333334,
        "step": 7279
    },
    {
        "loss": 2.4185,
        "grad_norm": 5.528350353240967,
        "learning_rate": 0.00011482964061371131,
        "epoch": 0.9706666666666667,
        "step": 7280
    },
    {
        "loss": 2.2382,
        "grad_norm": 6.745075702667236,
        "learning_rate": 0.00011476739682733988,
        "epoch": 0.9708,
        "step": 7281
    },
    {
        "loss": 2.4932,
        "grad_norm": 3.5925779342651367,
        "learning_rate": 0.00011470514719155234,
        "epoch": 0.9709333333333333,
        "step": 7282
    },
    {
        "loss": 2.476,
        "grad_norm": 3.3471479415893555,
        "learning_rate": 0.00011464289173100584,
        "epoch": 0.9710666666666666,
        "step": 7283
    },
    {
        "loss": 2.7617,
        "grad_norm": 4.587810039520264,
        "learning_rate": 0.0001145806304703601,
        "epoch": 0.9712,
        "step": 7284
    },
    {
        "loss": 1.4664,
        "grad_norm": 4.283210754394531,
        "learning_rate": 0.00011451836343427689,
        "epoch": 0.9713333333333334,
        "step": 7285
    },
    {
        "loss": 2.6657,
        "grad_norm": 3.4581944942474365,
        "learning_rate": 0.00011445609064742043,
        "epoch": 0.9714666666666667,
        "step": 7286
    },
    {
        "loss": 1.8093,
        "grad_norm": 2.9190011024475098,
        "learning_rate": 0.00011439381213445727,
        "epoch": 0.9716,
        "step": 7287
    },
    {
        "loss": 0.9107,
        "grad_norm": 4.66030216217041,
        "learning_rate": 0.00011433152792005603,
        "epoch": 0.9717333333333333,
        "step": 7288
    },
    {
        "loss": 2.1472,
        "grad_norm": 1.7004951238632202,
        "learning_rate": 0.00011426923802888783,
        "epoch": 0.9718666666666667,
        "step": 7289
    },
    {
        "loss": 2.3241,
        "grad_norm": 3.364828586578369,
        "learning_rate": 0.00011420694248562567,
        "epoch": 0.972,
        "step": 7290
    },
    {
        "loss": 2.5762,
        "grad_norm": 3.639554262161255,
        "learning_rate": 0.0001141446413149453,
        "epoch": 0.9721333333333333,
        "step": 7291
    },
    {
        "loss": 2.2353,
        "grad_norm": 2.9278626441955566,
        "learning_rate": 0.00011408233454152428,
        "epoch": 0.9722666666666666,
        "step": 7292
    },
    {
        "loss": 1.5119,
        "grad_norm": 4.552659034729004,
        "learning_rate": 0.00011402002219004259,
        "epoch": 0.9724,
        "step": 7293
    },
    {
        "loss": 2.2597,
        "grad_norm": 2.7580084800720215,
        "learning_rate": 0.0001139577042851823,
        "epoch": 0.9725333333333334,
        "step": 7294
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.4776296615600586,
        "learning_rate": 0.00011389538085162786,
        "epoch": 0.9726666666666667,
        "step": 7295
    },
    {
        "loss": 2.7117,
        "grad_norm": 3.2157626152038574,
        "learning_rate": 0.00011383305191406564,
        "epoch": 0.9728,
        "step": 7296
    },
    {
        "loss": 2.0915,
        "grad_norm": 3.3903303146362305,
        "learning_rate": 0.0001137707174971844,
        "epoch": 0.9729333333333333,
        "step": 7297
    },
    {
        "loss": 2.9179,
        "grad_norm": 2.747232675552368,
        "learning_rate": 0.0001137083776256751,
        "epoch": 0.9730666666666666,
        "step": 7298
    },
    {
        "loss": 2.7887,
        "grad_norm": 3.4199697971343994,
        "learning_rate": 0.00011364603232423068,
        "epoch": 0.9732,
        "step": 7299
    },
    {
        "loss": 2.3336,
        "grad_norm": 4.000112056732178,
        "learning_rate": 0.00011358368161754633,
        "epoch": 0.9733333333333334,
        "step": 7300
    },
    {
        "loss": 2.4931,
        "grad_norm": 3.119957685470581,
        "learning_rate": 0.00011352132553031948,
        "epoch": 0.9734666666666667,
        "step": 7301
    },
    {
        "loss": 2.4237,
        "grad_norm": 4.819343566894531,
        "learning_rate": 0.00011345896408724947,
        "epoch": 0.9736,
        "step": 7302
    },
    {
        "loss": 2.4595,
        "grad_norm": 4.1146135330200195,
        "learning_rate": 0.00011339659731303798,
        "epoch": 0.9737333333333333,
        "step": 7303
    },
    {
        "loss": 2.3729,
        "grad_norm": 2.913498640060425,
        "learning_rate": 0.00011333422523238856,
        "epoch": 0.9738666666666667,
        "step": 7304
    },
    {
        "loss": 2.381,
        "grad_norm": 2.9429874420166016,
        "learning_rate": 0.00011327184787000711,
        "epoch": 0.974,
        "step": 7305
    },
    {
        "loss": 2.5216,
        "grad_norm": 2.998657464981079,
        "learning_rate": 0.00011320946525060149,
        "epoch": 0.9741333333333333,
        "step": 7306
    },
    {
        "loss": 2.6171,
        "grad_norm": 4.126806735992432,
        "learning_rate": 0.00011314707739888176,
        "epoch": 0.9742666666666666,
        "step": 7307
    },
    {
        "loss": 1.8021,
        "grad_norm": 3.694153070449829,
        "learning_rate": 0.00011308468433955977,
        "epoch": 0.9744,
        "step": 7308
    },
    {
        "loss": 2.6338,
        "grad_norm": 3.469269275665283,
        "learning_rate": 0.00011302228609734975,
        "epoch": 0.9745333333333334,
        "step": 7309
    },
    {
        "loss": 1.9181,
        "grad_norm": 4.458870887756348,
        "learning_rate": 0.00011295988269696791,
        "epoch": 0.9746666666666667,
        "step": 7310
    },
    {
        "loss": 2.0569,
        "grad_norm": 4.091801166534424,
        "learning_rate": 0.00011289747416313227,
        "epoch": 0.9748,
        "step": 7311
    },
    {
        "loss": 2.8647,
        "grad_norm": 2.3587069511413574,
        "learning_rate": 0.00011283506052056328,
        "epoch": 0.9749333333333333,
        "step": 7312
    },
    {
        "loss": 1.5319,
        "grad_norm": 3.2598111629486084,
        "learning_rate": 0.00011277264179398299,
        "epoch": 0.9750666666666666,
        "step": 7313
    },
    {
        "loss": 2.3793,
        "grad_norm": 4.103546142578125,
        "learning_rate": 0.00011271021800811586,
        "epoch": 0.9752,
        "step": 7314
    },
    {
        "loss": 2.4449,
        "grad_norm": 2.9793150424957275,
        "learning_rate": 0.00011264778918768788,
        "epoch": 0.9753333333333334,
        "step": 7315
    },
    {
        "loss": 2.3353,
        "grad_norm": 3.5129058361053467,
        "learning_rate": 0.00011258535535742767,
        "epoch": 0.9754666666666667,
        "step": 7316
    },
    {
        "loss": 2.5021,
        "grad_norm": 4.973959445953369,
        "learning_rate": 0.00011252291654206526,
        "epoch": 0.9756,
        "step": 7317
    },
    {
        "loss": 1.5081,
        "grad_norm": 4.442878246307373,
        "learning_rate": 0.000112460472766333,
        "epoch": 0.9757333333333333,
        "step": 7318
    },
    {
        "loss": 2.5248,
        "grad_norm": 2.0585107803344727,
        "learning_rate": 0.00011239802405496496,
        "epoch": 0.9758666666666667,
        "step": 7319
    },
    {
        "loss": 2.1616,
        "grad_norm": 2.8177757263183594,
        "learning_rate": 0.0001123355704326974,
        "epoch": 0.976,
        "step": 7320
    },
    {
        "loss": 2.2831,
        "grad_norm": 2.368067502975464,
        "learning_rate": 0.0001122731119242683,
        "epoch": 0.9761333333333333,
        "step": 7321
    },
    {
        "loss": 2.1067,
        "grad_norm": 3.6044111251831055,
        "learning_rate": 0.00011221064855441772,
        "epoch": 0.9762666666666666,
        "step": 7322
    },
    {
        "loss": 2.0404,
        "grad_norm": 3.3961753845214844,
        "learning_rate": 0.00011214818034788774,
        "epoch": 0.9764,
        "step": 7323
    },
    {
        "loss": 2.7211,
        "grad_norm": 2.204113483428955,
        "learning_rate": 0.00011208570732942205,
        "epoch": 0.9765333333333334,
        "step": 7324
    },
    {
        "loss": 1.0673,
        "grad_norm": 2.176499128341675,
        "learning_rate": 0.00011202322952376654,
        "epoch": 0.9766666666666667,
        "step": 7325
    },
    {
        "loss": 1.1189,
        "grad_norm": 3.969064474105835,
        "learning_rate": 0.00011196074695566878,
        "epoch": 0.9768,
        "step": 7326
    },
    {
        "loss": 2.8519,
        "grad_norm": 5.779416561126709,
        "learning_rate": 0.00011189825964987852,
        "epoch": 0.9769333333333333,
        "step": 7327
    },
    {
        "loss": 1.5758,
        "grad_norm": 3.0524778366088867,
        "learning_rate": 0.00011183576763114698,
        "epoch": 0.9770666666666666,
        "step": 7328
    },
    {
        "loss": 3.4886,
        "grad_norm": 4.004661560058594,
        "learning_rate": 0.00011177327092422762,
        "epoch": 0.9772,
        "step": 7329
    },
    {
        "loss": 2.4109,
        "grad_norm": 4.556246280670166,
        "learning_rate": 0.00011171076955387547,
        "epoch": 0.9773333333333334,
        "step": 7330
    },
    {
        "loss": 1.9392,
        "grad_norm": 3.3576390743255615,
        "learning_rate": 0.00011164826354484759,
        "epoch": 0.9774666666666667,
        "step": 7331
    },
    {
        "loss": 1.1382,
        "grad_norm": 3.416386842727661,
        "learning_rate": 0.0001115857529219029,
        "epoch": 0.9776,
        "step": 7332
    },
    {
        "loss": 1.7072,
        "grad_norm": 3.8691000938415527,
        "learning_rate": 0.00011152323770980193,
        "epoch": 0.9777333333333333,
        "step": 7333
    },
    {
        "loss": 1.5631,
        "grad_norm": 4.188314914703369,
        "learning_rate": 0.00011146071793330724,
        "epoch": 0.9778666666666667,
        "step": 7334
    },
    {
        "loss": 2.9519,
        "grad_norm": 3.4269354343414307,
        "learning_rate": 0.0001113981936171832,
        "epoch": 0.978,
        "step": 7335
    },
    {
        "loss": 3.0271,
        "grad_norm": 5.022115230560303,
        "learning_rate": 0.00011133566478619575,
        "epoch": 0.9781333333333333,
        "step": 7336
    },
    {
        "loss": 2.3114,
        "grad_norm": 3.420435667037964,
        "learning_rate": 0.00011127313146511291,
        "epoch": 0.9782666666666666,
        "step": 7337
    },
    {
        "loss": 2.7745,
        "grad_norm": 2.217656135559082,
        "learning_rate": 0.00011121059367870423,
        "epoch": 0.9784,
        "step": 7338
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.64973521232605,
        "learning_rate": 0.00011114805145174128,
        "epoch": 0.9785333333333334,
        "step": 7339
    },
    {
        "loss": 2.2484,
        "grad_norm": 2.8217177391052246,
        "learning_rate": 0.000111085504808997,
        "epoch": 0.9786666666666667,
        "step": 7340
    },
    {
        "loss": 1.2744,
        "grad_norm": 2.9367592334747314,
        "learning_rate": 0.00011102295377524666,
        "epoch": 0.9788,
        "step": 7341
    },
    {
        "loss": 2.4783,
        "grad_norm": 3.494466543197632,
        "learning_rate": 0.00011096039837526669,
        "epoch": 0.9789333333333333,
        "step": 7342
    },
    {
        "loss": 1.9695,
        "grad_norm": 3.158689260482788,
        "learning_rate": 0.00011089783863383563,
        "epoch": 0.9790666666666666,
        "step": 7343
    },
    {
        "loss": 0.4664,
        "grad_norm": 2.385706663131714,
        "learning_rate": 0.00011083527457573348,
        "epoch": 0.9792,
        "step": 7344
    },
    {
        "loss": 2.6019,
        "grad_norm": 4.147548198699951,
        "learning_rate": 0.00011077270622574222,
        "epoch": 0.9793333333333333,
        "step": 7345
    },
    {
        "loss": 2.7659,
        "grad_norm": 5.268376350402832,
        "learning_rate": 0.00011071013360864528,
        "epoch": 0.9794666666666667,
        "step": 7346
    },
    {
        "loss": 1.6433,
        "grad_norm": 3.1035242080688477,
        "learning_rate": 0.00011064755674922788,
        "epoch": 0.9796,
        "step": 7347
    },
    {
        "loss": 3.5221,
        "grad_norm": 3.422520160675049,
        "learning_rate": 0.00011058497567227708,
        "epoch": 0.9797333333333333,
        "step": 7348
    },
    {
        "loss": 2.575,
        "grad_norm": 3.4145140647888184,
        "learning_rate": 0.0001105223904025812,
        "epoch": 0.9798666666666667,
        "step": 7349
    },
    {
        "loss": 2.5134,
        "grad_norm": 3.0865540504455566,
        "learning_rate": 0.00011045980096493075,
        "epoch": 0.98,
        "step": 7350
    },
    {
        "loss": 2.7879,
        "grad_norm": 2.495061159133911,
        "learning_rate": 0.00011039720738411743,
        "epoch": 0.9801333333333333,
        "step": 7351
    },
    {
        "loss": 2.2071,
        "grad_norm": 7.268308162689209,
        "learning_rate": 0.00011033460968493492,
        "epoch": 0.9802666666666666,
        "step": 7352
    },
    {
        "loss": 2.5352,
        "grad_norm": 1.7646244764328003,
        "learning_rate": 0.00011027200789217825,
        "epoch": 0.9804,
        "step": 7353
    },
    {
        "loss": 2.5163,
        "grad_norm": 5.041997909545898,
        "learning_rate": 0.00011020940203064429,
        "epoch": 0.9805333333333334,
        "step": 7354
    },
    {
        "loss": 3.2313,
        "grad_norm": 3.834289312362671,
        "learning_rate": 0.00011014679212513133,
        "epoch": 0.9806666666666667,
        "step": 7355
    },
    {
        "loss": 2.3012,
        "grad_norm": 3.137639045715332,
        "learning_rate": 0.00011008417820043944,
        "epoch": 0.9808,
        "step": 7356
    },
    {
        "loss": 3.2483,
        "grad_norm": 3.607649564743042,
        "learning_rate": 0.00011002156028137027,
        "epoch": 0.9809333333333333,
        "step": 7357
    },
    {
        "loss": 2.513,
        "grad_norm": 3.8010408878326416,
        "learning_rate": 0.00010995893839272687,
        "epoch": 0.9810666666666666,
        "step": 7358
    },
    {
        "loss": 2.1395,
        "grad_norm": 4.275083065032959,
        "learning_rate": 0.00010989631255931401,
        "epoch": 0.9812,
        "step": 7359
    },
    {
        "loss": 0.7206,
        "grad_norm": 2.2562313079833984,
        "learning_rate": 0.00010983368280593814,
        "epoch": 0.9813333333333333,
        "step": 7360
    },
    {
        "loss": 2.4853,
        "grad_norm": 3.824406385421753,
        "learning_rate": 0.00010977104915740691,
        "epoch": 0.9814666666666667,
        "step": 7361
    },
    {
        "loss": 3.0584,
        "grad_norm": 3.404262065887451,
        "learning_rate": 0.00010970841163852985,
        "epoch": 0.9816,
        "step": 7362
    },
    {
        "loss": 1.7784,
        "grad_norm": 4.297672748565674,
        "learning_rate": 0.00010964577027411782,
        "epoch": 0.9817333333333333,
        "step": 7363
    },
    {
        "loss": 3.1757,
        "grad_norm": 3.2362139225006104,
        "learning_rate": 0.00010958312508898339,
        "epoch": 0.9818666666666667,
        "step": 7364
    },
    {
        "loss": 2.5852,
        "grad_norm": 2.8570311069488525,
        "learning_rate": 0.00010952047610794031,
        "epoch": 0.982,
        "step": 7365
    },
    {
        "loss": 2.7768,
        "grad_norm": 2.9169201850891113,
        "learning_rate": 0.00010945782335580435,
        "epoch": 0.9821333333333333,
        "step": 7366
    },
    {
        "loss": 2.5272,
        "grad_norm": 2.725567579269409,
        "learning_rate": 0.00010939516685739226,
        "epoch": 0.9822666666666666,
        "step": 7367
    },
    {
        "loss": 2.8655,
        "grad_norm": 3.8494315147399902,
        "learning_rate": 0.00010933250663752257,
        "epoch": 0.9824,
        "step": 7368
    },
    {
        "loss": 2.1842,
        "grad_norm": 3.150996685028076,
        "learning_rate": 0.00010926984272101526,
        "epoch": 0.9825333333333334,
        "step": 7369
    },
    {
        "loss": 3.629,
        "grad_norm": 4.417293548583984,
        "learning_rate": 0.0001092071751326916,
        "epoch": 0.9826666666666667,
        "step": 7370
    },
    {
        "loss": 2.8559,
        "grad_norm": 2.7489240169525146,
        "learning_rate": 0.00010914450389737456,
        "epoch": 0.9828,
        "step": 7371
    },
    {
        "loss": 2.6013,
        "grad_norm": 2.8716537952423096,
        "learning_rate": 0.00010908182903988833,
        "epoch": 0.9829333333333333,
        "step": 7372
    },
    {
        "loss": 2.4055,
        "grad_norm": 2.6280367374420166,
        "learning_rate": 0.0001090191505850587,
        "epoch": 0.9830666666666666,
        "step": 7373
    },
    {
        "loss": 2.4981,
        "grad_norm": 4.554450988769531,
        "learning_rate": 0.00010895646855771267,
        "epoch": 0.9832,
        "step": 7374
    },
    {
        "loss": 2.0764,
        "grad_norm": 2.8353943824768066,
        "learning_rate": 0.00010889378298267905,
        "epoch": 0.9833333333333333,
        "step": 7375
    },
    {
        "loss": 2.4503,
        "grad_norm": 2.9581215381622314,
        "learning_rate": 0.00010883109388478763,
        "epoch": 0.9834666666666667,
        "step": 7376
    },
    {
        "loss": 1.6403,
        "grad_norm": 3.810382843017578,
        "learning_rate": 0.00010876840128886989,
        "epoch": 0.9836,
        "step": 7377
    },
    {
        "loss": 2.063,
        "grad_norm": 3.1502392292022705,
        "learning_rate": 0.00010870570521975847,
        "epoch": 0.9837333333333333,
        "step": 7378
    },
    {
        "loss": 2.2581,
        "grad_norm": 3.126683473587036,
        "learning_rate": 0.0001086430057022876,
        "epoch": 0.9838666666666667,
        "step": 7379
    },
    {
        "loss": 2.5568,
        "grad_norm": 3.371845245361328,
        "learning_rate": 0.00010858030276129265,
        "epoch": 0.984,
        "step": 7380
    },
    {
        "loss": 2.1533,
        "grad_norm": 2.595851182937622,
        "learning_rate": 0.00010851759642161056,
        "epoch": 0.9841333333333333,
        "step": 7381
    },
    {
        "loss": 1.67,
        "grad_norm": 3.7791588306427,
        "learning_rate": 0.00010845488670807958,
        "epoch": 0.9842666666666666,
        "step": 7382
    },
    {
        "loss": 2.2719,
        "grad_norm": 2.8489787578582764,
        "learning_rate": 0.00010839217364553908,
        "epoch": 0.9844,
        "step": 7383
    },
    {
        "loss": 2.2386,
        "grad_norm": 3.1131649017333984,
        "learning_rate": 0.00010832945725882999,
        "epoch": 0.9845333333333334,
        "step": 7384
    },
    {
        "loss": 1.9165,
        "grad_norm": 4.150122165679932,
        "learning_rate": 0.00010826673757279457,
        "epoch": 0.9846666666666667,
        "step": 7385
    },
    {
        "loss": 2.6664,
        "grad_norm": 2.9924252033233643,
        "learning_rate": 0.00010820401461227619,
        "epoch": 0.9848,
        "step": 7386
    },
    {
        "loss": 2.8534,
        "grad_norm": 2.589956045150757,
        "learning_rate": 0.00010814128840211963,
        "epoch": 0.9849333333333333,
        "step": 7387
    },
    {
        "loss": 2.463,
        "grad_norm": 3.1563801765441895,
        "learning_rate": 0.00010807855896717107,
        "epoch": 0.9850666666666666,
        "step": 7388
    },
    {
        "loss": 2.1973,
        "grad_norm": 2.9597089290618896,
        "learning_rate": 0.00010801582633227774,
        "epoch": 0.9852,
        "step": 7389
    },
    {
        "loss": 1.5738,
        "grad_norm": 3.741360664367676,
        "learning_rate": 0.00010795309052228826,
        "epoch": 0.9853333333333333,
        "step": 7390
    },
    {
        "loss": 2.2616,
        "grad_norm": 3.5356054306030273,
        "learning_rate": 0.00010789035156205258,
        "epoch": 0.9854666666666667,
        "step": 7391
    },
    {
        "loss": 2.2147,
        "grad_norm": 3.461294651031494,
        "learning_rate": 0.00010782760947642172,
        "epoch": 0.9856,
        "step": 7392
    },
    {
        "loss": 1.9883,
        "grad_norm": 3.5206804275512695,
        "learning_rate": 0.00010776486429024806,
        "epoch": 0.9857333333333334,
        "step": 7393
    },
    {
        "loss": 2.2444,
        "grad_norm": 3.6581578254699707,
        "learning_rate": 0.00010770211602838527,
        "epoch": 0.9858666666666667,
        "step": 7394
    },
    {
        "loss": 1.7587,
        "grad_norm": 3.5065512657165527,
        "learning_rate": 0.000107639364715688,
        "epoch": 0.986,
        "step": 7395
    },
    {
        "loss": 2.3378,
        "grad_norm": 4.926368236541748,
        "learning_rate": 0.00010757661037701242,
        "epoch": 0.9861333333333333,
        "step": 7396
    },
    {
        "loss": 2.2767,
        "grad_norm": 2.128180503845215,
        "learning_rate": 0.00010751385303721557,
        "epoch": 0.9862666666666666,
        "step": 7397
    },
    {
        "loss": 2.4741,
        "grad_norm": 3.462099552154541,
        "learning_rate": 0.00010745109272115599,
        "epoch": 0.9864,
        "step": 7398
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.902859687805176,
        "learning_rate": 0.00010738832945369308,
        "epoch": 0.9865333333333334,
        "step": 7399
    },
    {
        "loss": 2.2992,
        "grad_norm": 3.701561450958252,
        "learning_rate": 0.00010732556325968784,
        "epoch": 0.9866666666666667,
        "step": 7400
    },
    {
        "loss": 1.7268,
        "grad_norm": 3.4661550521850586,
        "learning_rate": 0.00010726279416400194,
        "epoch": 0.9868,
        "step": 7401
    },
    {
        "loss": 2.5391,
        "grad_norm": 2.190971612930298,
        "learning_rate": 0.00010720002219149859,
        "epoch": 0.9869333333333333,
        "step": 7402
    },
    {
        "loss": 2.37,
        "grad_norm": 2.973081588745117,
        "learning_rate": 0.00010713724736704188,
        "epoch": 0.9870666666666666,
        "step": 7403
    },
    {
        "loss": 2.6023,
        "grad_norm": 2.5324363708496094,
        "learning_rate": 0.00010707446971549722,
        "epoch": 0.9872,
        "step": 7404
    },
    {
        "loss": 2.8849,
        "grad_norm": 2.243618965148926,
        "learning_rate": 0.00010701168926173091,
        "epoch": 0.9873333333333333,
        "step": 7405
    },
    {
        "loss": 2.8211,
        "grad_norm": 3.4223616123199463,
        "learning_rate": 0.00010694890603061061,
        "epoch": 0.9874666666666667,
        "step": 7406
    },
    {
        "loss": 1.9539,
        "grad_norm": 3.1939268112182617,
        "learning_rate": 0.00010688612004700502,
        "epoch": 0.9876,
        "step": 7407
    },
    {
        "loss": 3.0448,
        "grad_norm": 3.199209213256836,
        "learning_rate": 0.00010682333133578366,
        "epoch": 0.9877333333333334,
        "step": 7408
    },
    {
        "loss": 1.8091,
        "grad_norm": 3.623755693435669,
        "learning_rate": 0.00010676053992181766,
        "epoch": 0.9878666666666667,
        "step": 7409
    },
    {
        "loss": 2.4391,
        "grad_norm": 2.730828046798706,
        "learning_rate": 0.00010669774582997871,
        "epoch": 0.988,
        "step": 7410
    },
    {
        "loss": 2.2606,
        "grad_norm": 3.082199811935425,
        "learning_rate": 0.00010663494908513989,
        "epoch": 0.9881333333333333,
        "step": 7411
    },
    {
        "loss": 2.4045,
        "grad_norm": 2.843306303024292,
        "learning_rate": 0.00010657214971217501,
        "epoch": 0.9882666666666666,
        "step": 7412
    },
    {
        "loss": 2.0897,
        "grad_norm": 3.68485426902771,
        "learning_rate": 0.00010650934773595935,
        "epoch": 0.9884,
        "step": 7413
    },
    {
        "loss": 2.1715,
        "grad_norm": 4.127926826477051,
        "learning_rate": 0.0001064465431813688,
        "epoch": 0.9885333333333334,
        "step": 7414
    },
    {
        "loss": 2.3981,
        "grad_norm": 3.1880855560302734,
        "learning_rate": 0.00010638373607328052,
        "epoch": 0.9886666666666667,
        "step": 7415
    },
    {
        "loss": 1.8516,
        "grad_norm": 4.214897155761719,
        "learning_rate": 0.00010632092643657268,
        "epoch": 0.9888,
        "step": 7416
    },
    {
        "loss": 2.6406,
        "grad_norm": 4.716947078704834,
        "learning_rate": 0.0001062581142961243,
        "epoch": 0.9889333333333333,
        "step": 7417
    },
    {
        "loss": 2.8567,
        "grad_norm": 2.32470965385437,
        "learning_rate": 0.00010619529967681549,
        "epoch": 0.9890666666666666,
        "step": 7418
    },
    {
        "loss": 3.1016,
        "grad_norm": 3.2322912216186523,
        "learning_rate": 0.00010613248260352749,
        "epoch": 0.9892,
        "step": 7419
    },
    {
        "loss": 2.5671,
        "grad_norm": 3.1473991870880127,
        "learning_rate": 0.0001060696631011421,
        "epoch": 0.9893333333333333,
        "step": 7420
    },
    {
        "loss": 2.3705,
        "grad_norm": 2.9555881023406982,
        "learning_rate": 0.00010600684119454259,
        "epoch": 0.9894666666666667,
        "step": 7421
    },
    {
        "loss": 2.3114,
        "grad_norm": 3.563661575317383,
        "learning_rate": 0.00010594401690861273,
        "epoch": 0.9896,
        "step": 7422
    },
    {
        "loss": 2.5788,
        "grad_norm": 3.1020021438598633,
        "learning_rate": 0.00010588119026823759,
        "epoch": 0.9897333333333334,
        "step": 7423
    },
    {
        "loss": 2.0296,
        "grad_norm": 3.6975302696228027,
        "learning_rate": 0.00010581836129830281,
        "epoch": 0.9898666666666667,
        "step": 7424
    },
    {
        "loss": 2.3703,
        "grad_norm": 3.3274683952331543,
        "learning_rate": 0.00010575553002369543,
        "epoch": 0.99,
        "step": 7425
    },
    {
        "loss": 1.6889,
        "grad_norm": 2.130647897720337,
        "learning_rate": 0.00010569269646930293,
        "epoch": 0.9901333333333333,
        "step": 7426
    },
    {
        "loss": 1.5131,
        "grad_norm": 2.9279725551605225,
        "learning_rate": 0.00010562986066001395,
        "epoch": 0.9902666666666666,
        "step": 7427
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.8468244075775146,
        "learning_rate": 0.00010556702262071804,
        "epoch": 0.9904,
        "step": 7428
    },
    {
        "loss": 1.8676,
        "grad_norm": 3.9007632732391357,
        "learning_rate": 0.00010550418237630546,
        "epoch": 0.9905333333333334,
        "step": 7429
    },
    {
        "loss": 3.1488,
        "grad_norm": 2.6533145904541016,
        "learning_rate": 0.00010544133995166756,
        "epoch": 0.9906666666666667,
        "step": 7430
    },
    {
        "loss": 2.9413,
        "grad_norm": 3.1010265350341797,
        "learning_rate": 0.00010537849537169627,
        "epoch": 0.9908,
        "step": 7431
    },
    {
        "loss": 3.1122,
        "grad_norm": 4.332786560058594,
        "learning_rate": 0.0001053156486612847,
        "epoch": 0.9909333333333333,
        "step": 7432
    },
    {
        "loss": 2.9983,
        "grad_norm": 3.1739330291748047,
        "learning_rate": 0.00010525279984532647,
        "epoch": 0.9910666666666667,
        "step": 7433
    },
    {
        "loss": 2.7343,
        "grad_norm": 2.2964119911193848,
        "learning_rate": 0.00010518994894871644,
        "epoch": 0.9912,
        "step": 7434
    },
    {
        "loss": 2.3349,
        "grad_norm": 3.936356782913208,
        "learning_rate": 0.00010512709599634989,
        "epoch": 0.9913333333333333,
        "step": 7435
    },
    {
        "loss": 1.2777,
        "grad_norm": 3.484064817428589,
        "learning_rate": 0.0001050642410131232,
        "epoch": 0.9914666666666667,
        "step": 7436
    },
    {
        "loss": 2.597,
        "grad_norm": 2.864607334136963,
        "learning_rate": 0.00010500138402393333,
        "epoch": 0.9916,
        "step": 7437
    },
    {
        "loss": 0.9108,
        "grad_norm": 2.8999719619750977,
        "learning_rate": 0.00010493852505367827,
        "epoch": 0.9917333333333334,
        "step": 7438
    },
    {
        "loss": 2.546,
        "grad_norm": 4.111749649047852,
        "learning_rate": 0.00010487566412725655,
        "epoch": 0.9918666666666667,
        "step": 7439
    },
    {
        "loss": 2.6822,
        "grad_norm": 2.814119815826416,
        "learning_rate": 0.00010481280126956768,
        "epoch": 0.992,
        "step": 7440
    },
    {
        "loss": 2.1576,
        "grad_norm": 3.203103542327881,
        "learning_rate": 0.00010474993650551183,
        "epoch": 0.9921333333333333,
        "step": 7441
    },
    {
        "loss": 1.8239,
        "grad_norm": 3.7156732082366943,
        "learning_rate": 0.00010468706985998993,
        "epoch": 0.9922666666666666,
        "step": 7442
    },
    {
        "loss": 0.6496,
        "grad_norm": 2.817640542984009,
        "learning_rate": 0.00010462420135790368,
        "epoch": 0.9924,
        "step": 7443
    },
    {
        "loss": 2.2246,
        "grad_norm": 3.2147905826568604,
        "learning_rate": 0.00010456133102415557,
        "epoch": 0.9925333333333334,
        "step": 7444
    },
    {
        "loss": 1.7396,
        "grad_norm": 2.097607374191284,
        "learning_rate": 0.00010449845888364862,
        "epoch": 0.9926666666666667,
        "step": 7445
    },
    {
        "loss": 2.2975,
        "grad_norm": 3.6463088989257812,
        "learning_rate": 0.0001044355849612868,
        "epoch": 0.9928,
        "step": 7446
    },
    {
        "loss": 2.3761,
        "grad_norm": 3.4119114875793457,
        "learning_rate": 0.00010437270928197473,
        "epoch": 0.9929333333333333,
        "step": 7447
    },
    {
        "loss": 2.4695,
        "grad_norm": 4.363394260406494,
        "learning_rate": 0.00010430983187061754,
        "epoch": 0.9930666666666667,
        "step": 7448
    },
    {
        "loss": 2.3615,
        "grad_norm": 2.4308087825775146,
        "learning_rate": 0.00010424695275212126,
        "epoch": 0.9932,
        "step": 7449
    },
    {
        "loss": 1.4018,
        "grad_norm": 4.030932903289795,
        "learning_rate": 0.00010418407195139259,
        "epoch": 0.9933333333333333,
        "step": 7450
    },
    {
        "loss": 2.4571,
        "grad_norm": 2.5836853981018066,
        "learning_rate": 0.0001041211894933387,
        "epoch": 0.9934666666666667,
        "step": 7451
    },
    {
        "loss": 2.7811,
        "grad_norm": 3.5330090522766113,
        "learning_rate": 0.00010405830540286761,
        "epoch": 0.9936,
        "step": 7452
    },
    {
        "loss": 2.7881,
        "grad_norm": 2.455789566040039,
        "learning_rate": 0.000103995419704888,
        "epoch": 0.9937333333333334,
        "step": 7453
    },
    {
        "loss": 2.6962,
        "grad_norm": 3.035297393798828,
        "learning_rate": 0.00010393253242430895,
        "epoch": 0.9938666666666667,
        "step": 7454
    },
    {
        "loss": 2.8067,
        "grad_norm": 2.6570146083831787,
        "learning_rate": 0.00010386964358604048,
        "epoch": 0.994,
        "step": 7455
    },
    {
        "loss": 1.7122,
        "grad_norm": 2.863924264907837,
        "learning_rate": 0.00010380675321499292,
        "epoch": 0.9941333333333333,
        "step": 7456
    },
    {
        "loss": 1.325,
        "grad_norm": 4.530834674835205,
        "learning_rate": 0.00010374386133607752,
        "epoch": 0.9942666666666666,
        "step": 7457
    },
    {
        "loss": 1.8263,
        "grad_norm": 4.8327250480651855,
        "learning_rate": 0.00010368096797420575,
        "epoch": 0.9944,
        "step": 7458
    },
    {
        "loss": 2.4279,
        "grad_norm": 8.412210464477539,
        "learning_rate": 0.00010361807315429016,
        "epoch": 0.9945333333333334,
        "step": 7459
    },
    {
        "loss": 1.9656,
        "grad_norm": 3.2547411918640137,
        "learning_rate": 0.00010355517690124339,
        "epoch": 0.9946666666666667,
        "step": 7460
    },
    {
        "loss": 2.5336,
        "grad_norm": 3.05603289604187,
        "learning_rate": 0.00010349227923997902,
        "epoch": 0.9948,
        "step": 7461
    },
    {
        "loss": 2.7769,
        "grad_norm": 2.655940294265747,
        "learning_rate": 0.00010342938019541087,
        "epoch": 0.9949333333333333,
        "step": 7462
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.395390272140503,
        "learning_rate": 0.00010336647979245357,
        "epoch": 0.9950666666666667,
        "step": 7463
    },
    {
        "loss": 2.4768,
        "grad_norm": 3.0150399208068848,
        "learning_rate": 0.00010330357805602213,
        "epoch": 0.9952,
        "step": 7464
    },
    {
        "loss": 2.6047,
        "grad_norm": 3.588282823562622,
        "learning_rate": 0.00010324067501103214,
        "epoch": 0.9953333333333333,
        "step": 7465
    },
    {
        "loss": 1.8598,
        "grad_norm": 3.604459762573242,
        "learning_rate": 0.00010317777068239982,
        "epoch": 0.9954666666666667,
        "step": 7466
    },
    {
        "loss": 1.4672,
        "grad_norm": 4.408471584320068,
        "learning_rate": 0.00010311486509504156,
        "epoch": 0.9956,
        "step": 7467
    },
    {
        "loss": 1.8085,
        "grad_norm": 5.092672824859619,
        "learning_rate": 0.00010305195827387477,
        "epoch": 0.9957333333333334,
        "step": 7468
    },
    {
        "loss": 1.6824,
        "grad_norm": 3.740734338760376,
        "learning_rate": 0.00010298905024381684,
        "epoch": 0.9958666666666667,
        "step": 7469
    },
    {
        "loss": 2.9968,
        "grad_norm": 2.4888463020324707,
        "learning_rate": 0.00010292614102978595,
        "epoch": 0.996,
        "step": 7470
    },
    {
        "loss": 2.5834,
        "grad_norm": 4.492859363555908,
        "learning_rate": 0.0001028632306567006,
        "epoch": 0.9961333333333333,
        "step": 7471
    },
    {
        "loss": 2.3099,
        "grad_norm": 2.9665143489837646,
        "learning_rate": 0.00010280031914947992,
        "epoch": 0.9962666666666666,
        "step": 7472
    },
    {
        "loss": 1.7766,
        "grad_norm": 2.3056180477142334,
        "learning_rate": 0.00010273740653304318,
        "epoch": 0.9964,
        "step": 7473
    },
    {
        "loss": 2.2137,
        "grad_norm": 2.5993564128875732,
        "learning_rate": 0.00010267449283231039,
        "epoch": 0.9965333333333334,
        "step": 7474
    },
    {
        "loss": 2.6494,
        "grad_norm": 3.932624578475952,
        "learning_rate": 0.00010261157807220195,
        "epoch": 0.9966666666666667,
        "step": 7475
    },
    {
        "loss": 2.3336,
        "grad_norm": 3.994328737258911,
        "learning_rate": 0.00010254866227763846,
        "epoch": 0.9968,
        "step": 7476
    },
    {
        "loss": 2.9369,
        "grad_norm": 3.0219171047210693,
        "learning_rate": 0.00010248574547354115,
        "epoch": 0.9969333333333333,
        "step": 7477
    },
    {
        "loss": 2.225,
        "grad_norm": 3.4677417278289795,
        "learning_rate": 0.00010242282768483167,
        "epoch": 0.9970666666666667,
        "step": 7478
    },
    {
        "loss": 2.6026,
        "grad_norm": 2.771566867828369,
        "learning_rate": 0.00010235990893643182,
        "epoch": 0.9972,
        "step": 7479
    },
    {
        "loss": 2.1315,
        "grad_norm": 3.210031032562256,
        "learning_rate": 0.00010229698925326407,
        "epoch": 0.9973333333333333,
        "step": 7480
    },
    {
        "loss": 1.3328,
        "grad_norm": 6.012540340423584,
        "learning_rate": 0.00010223406866025096,
        "epoch": 0.9974666666666666,
        "step": 7481
    },
    {
        "loss": 2.381,
        "grad_norm": 2.65460205078125,
        "learning_rate": 0.00010217114718231573,
        "epoch": 0.9976,
        "step": 7482
    },
    {
        "loss": 2.4104,
        "grad_norm": 4.176846981048584,
        "learning_rate": 0.00010210822484438159,
        "epoch": 0.9977333333333334,
        "step": 7483
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.671848773956299,
        "learning_rate": 0.00010204530167137252,
        "epoch": 0.9978666666666667,
        "step": 7484
    },
    {
        "loss": 2.3906,
        "grad_norm": 2.1683905124664307,
        "learning_rate": 0.0001019823776882124,
        "epoch": 0.998,
        "step": 7485
    },
    {
        "loss": 1.7065,
        "grad_norm": 3.9274098873138428,
        "learning_rate": 0.00010191945291982581,
        "epoch": 0.9981333333333333,
        "step": 7486
    },
    {
        "loss": 2.3526,
        "grad_norm": 2.6284244060516357,
        "learning_rate": 0.00010185652739113731,
        "epoch": 0.9982666666666666,
        "step": 7487
    },
    {
        "loss": 2.5342,
        "grad_norm": 4.077656269073486,
        "learning_rate": 0.00010179360112707197,
        "epoch": 0.9984,
        "step": 7488
    },
    {
        "loss": 1.8362,
        "grad_norm": 3.674797534942627,
        "learning_rate": 0.0001017306741525552,
        "epoch": 0.9985333333333334,
        "step": 7489
    },
    {
        "loss": 1.8133,
        "grad_norm": 4.172108173370361,
        "learning_rate": 0.00010166774649251243,
        "epoch": 0.9986666666666667,
        "step": 7490
    },
    {
        "loss": 2.2873,
        "grad_norm": 4.135685920715332,
        "learning_rate": 0.00010160481817186965,
        "epoch": 0.9988,
        "step": 7491
    },
    {
        "loss": 2.435,
        "grad_norm": 3.2443578243255615,
        "learning_rate": 0.00010154188921555276,
        "epoch": 0.9989333333333333,
        "step": 7492
    },
    {
        "loss": 2.6981,
        "grad_norm": 2.8378448486328125,
        "learning_rate": 0.00010147895964848848,
        "epoch": 0.9990666666666667,
        "step": 7493
    },
    {
        "loss": 2.4077,
        "grad_norm": 4.864168167114258,
        "learning_rate": 0.00010141602949560318,
        "epoch": 0.9992,
        "step": 7494
    },
    {
        "loss": 3.3217,
        "grad_norm": 2.8809471130371094,
        "learning_rate": 0.0001013530987818238,
        "epoch": 0.9993333333333333,
        "step": 7495
    },
    {
        "loss": 2.3523,
        "grad_norm": 3.554252862930298,
        "learning_rate": 0.00010129016753207734,
        "epoch": 0.9994666666666666,
        "step": 7496
    },
    {
        "loss": 2.2115,
        "grad_norm": 2.767873525619507,
        "learning_rate": 0.00010122723577129119,
        "epoch": 0.9996,
        "step": 7497
    },
    {
        "loss": 1.1955,
        "grad_norm": 6.377252578735352,
        "learning_rate": 0.0001011643035243927,
        "epoch": 0.9997333333333334,
        "step": 7498
    },
    {
        "loss": 2.6731,
        "grad_norm": 1.5273932218551636,
        "learning_rate": 0.00010110137081630962,
        "epoch": 0.9998666666666667,
        "step": 7499
    },
    {
        "loss": 2.866,
        "grad_norm": 2.5420331954956055,
        "learning_rate": 0.00010103843767196985,
        "epoch": 1.0,
        "step": 7500
    },
    {
        "loss": 2.2778,
        "grad_norm": 2.0511021614074707,
        "learning_rate": 0.00010097550411630134,
        "epoch": 1.0001333333333333,
        "step": 7501
    },
    {
        "loss": 2.6511,
        "grad_norm": 2.5250372886657715,
        "learning_rate": 0.00010091257017423232,
        "epoch": 1.0002666666666666,
        "step": 7502
    },
    {
        "loss": 1.7433,
        "grad_norm": 2.984710454940796,
        "learning_rate": 0.00010084963587069121,
        "epoch": 1.0004,
        "step": 7503
    },
    {
        "loss": 2.6191,
        "grad_norm": 2.998856782913208,
        "learning_rate": 0.00010078670123060638,
        "epoch": 1.0005333333333333,
        "step": 7504
    },
    {
        "loss": 1.7221,
        "grad_norm": 3.3684639930725098,
        "learning_rate": 0.00010072376627890656,
        "epoch": 1.0006666666666666,
        "step": 7505
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.847731351852417,
        "learning_rate": 0.0001006608310405204,
        "epoch": 1.0008,
        "step": 7506
    },
    {
        "loss": 1.8688,
        "grad_norm": 2.0938472747802734,
        "learning_rate": 0.0001005978955403768,
        "epoch": 1.0009333333333332,
        "step": 7507
    },
    {
        "loss": 1.9996,
        "grad_norm": 2.444213628768921,
        "learning_rate": 0.00010053495980340475,
        "epoch": 1.0010666666666668,
        "step": 7508
    },
    {
        "loss": 2.0629,
        "grad_norm": 2.6700398921966553,
        "learning_rate": 0.00010047202385453336,
        "epoch": 1.0012,
        "step": 7509
    },
    {
        "loss": 2.4667,
        "grad_norm": 3.0941152572631836,
        "learning_rate": 0.00010040908771869164,
        "epoch": 1.0013333333333334,
        "step": 7510
    },
    {
        "loss": 1.7518,
        "grad_norm": 2.804109811782837,
        "learning_rate": 0.00010034615142080889,
        "epoch": 1.0014666666666667,
        "step": 7511
    },
    {
        "loss": 2.3286,
        "grad_norm": 3.7192540168762207,
        "learning_rate": 0.00010028321498581446,
        "epoch": 1.0016,
        "step": 7512
    },
    {
        "loss": 2.2726,
        "grad_norm": 4.823527812957764,
        "learning_rate": 0.0001002202784386375,
        "epoch": 1.0017333333333334,
        "step": 7513
    },
    {
        "loss": 1.3812,
        "grad_norm": 3.88265323638916,
        "learning_rate": 0.00010015734180420761,
        "epoch": 1.0018666666666667,
        "step": 7514
    },
    {
        "loss": 2.1553,
        "grad_norm": 3.1886935234069824,
        "learning_rate": 0.00010009440510745402,
        "epoch": 1.002,
        "step": 7515
    },
    {
        "loss": 2.3377,
        "grad_norm": 2.742310047149658,
        "learning_rate": 0.0001000314683733063,
        "epoch": 1.0021333333333333,
        "step": 7516
    },
    {
        "loss": 2.0749,
        "grad_norm": 2.9055144786834717,
        "learning_rate": 9.996853162669372e-05,
        "epoch": 1.0022666666666666,
        "step": 7517
    },
    {
        "loss": 1.4217,
        "grad_norm": 4.75640869140625,
        "learning_rate": 9.9905594892546e-05,
        "epoch": 1.0024,
        "step": 7518
    },
    {
        "loss": 2.1994,
        "grad_norm": 3.6273181438446045,
        "learning_rate": 9.984265819579248e-05,
        "epoch": 1.0025333333333333,
        "step": 7519
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.7689709663391113,
        "learning_rate": 9.977972156136252e-05,
        "epoch": 1.0026666666666666,
        "step": 7520
    },
    {
        "loss": 2.5054,
        "grad_norm": 3.3378446102142334,
        "learning_rate": 9.971678501418557e-05,
        "epoch": 1.0028,
        "step": 7521
    },
    {
        "loss": 2.3916,
        "grad_norm": 4.395091533660889,
        "learning_rate": 9.965384857919106e-05,
        "epoch": 1.0029333333333332,
        "step": 7522
    },
    {
        "loss": 2.4319,
        "grad_norm": 3.7106552124023438,
        "learning_rate": 9.959091228130845e-05,
        "epoch": 1.0030666666666668,
        "step": 7523
    },
    {
        "loss": 1.5831,
        "grad_norm": 3.038952112197876,
        "learning_rate": 9.952797614546666e-05,
        "epoch": 1.0032,
        "step": 7524
    },
    {
        "loss": 1.6878,
        "grad_norm": 3.7373290061950684,
        "learning_rate": 9.946504019659528e-05,
        "epoch": 1.0033333333333334,
        "step": 7525
    },
    {
        "loss": 1.661,
        "grad_norm": 2.574831485748291,
        "learning_rate": 9.940210445962316e-05,
        "epoch": 1.0034666666666667,
        "step": 7526
    },
    {
        "loss": 2.0691,
        "grad_norm": 2.0601658821105957,
        "learning_rate": 9.933916895947963e-05,
        "epoch": 1.0036,
        "step": 7527
    },
    {
        "loss": 2.3774,
        "grad_norm": 4.778260231018066,
        "learning_rate": 9.927623372109352e-05,
        "epoch": 1.0037333333333334,
        "step": 7528
    },
    {
        "loss": 2.0241,
        "grad_norm": 3.0926859378814697,
        "learning_rate": 9.921329876939365e-05,
        "epoch": 1.0038666666666667,
        "step": 7529
    },
    {
        "loss": 1.112,
        "grad_norm": 4.893411159515381,
        "learning_rate": 9.915036412930882e-05,
        "epoch": 1.004,
        "step": 7530
    },
    {
        "loss": 2.3041,
        "grad_norm": 4.392735958099365,
        "learning_rate": 9.908742982576762e-05,
        "epoch": 1.0041333333333333,
        "step": 7531
    },
    {
        "loss": 2.7789,
        "grad_norm": 4.005792617797852,
        "learning_rate": 9.902449588369873e-05,
        "epoch": 1.0042666666666666,
        "step": 7532
    },
    {
        "loss": 2.0185,
        "grad_norm": 3.334256172180176,
        "learning_rate": 9.896156232803016e-05,
        "epoch": 1.0044,
        "step": 7533
    },
    {
        "loss": 2.3684,
        "grad_norm": 2.604006052017212,
        "learning_rate": 9.889862918369042e-05,
        "epoch": 1.0045333333333333,
        "step": 7534
    },
    {
        "loss": 1.9743,
        "grad_norm": 2.617403507232666,
        "learning_rate": 9.883569647560725e-05,
        "epoch": 1.0046666666666666,
        "step": 7535
    },
    {
        "loss": 1.5952,
        "grad_norm": 2.4142727851867676,
        "learning_rate": 9.877276422870889e-05,
        "epoch": 1.0048,
        "step": 7536
    },
    {
        "loss": 2.1813,
        "grad_norm": 2.8224751949310303,
        "learning_rate": 9.870983246792269e-05,
        "epoch": 1.0049333333333332,
        "step": 7537
    },
    {
        "loss": 2.128,
        "grad_norm": 3.3370285034179688,
        "learning_rate": 9.864690121817622e-05,
        "epoch": 1.0050666666666668,
        "step": 7538
    },
    {
        "loss": 2.3946,
        "grad_norm": 2.8851096630096436,
        "learning_rate": 9.858397050439678e-05,
        "epoch": 1.0052,
        "step": 7539
    },
    {
        "loss": 2.1093,
        "grad_norm": 4.3397016525268555,
        "learning_rate": 9.852104035151153e-05,
        "epoch": 1.0053333333333334,
        "step": 7540
    },
    {
        "loss": 2.0457,
        "grad_norm": 3.3012378215789795,
        "learning_rate": 9.845811078444725e-05,
        "epoch": 1.0054666666666667,
        "step": 7541
    },
    {
        "loss": 0.5979,
        "grad_norm": 3.237553358078003,
        "learning_rate": 9.839518182813037e-05,
        "epoch": 1.0056,
        "step": 7542
    },
    {
        "loss": 2.4435,
        "grad_norm": 2.7188949584960938,
        "learning_rate": 9.83322535074876e-05,
        "epoch": 1.0057333333333334,
        "step": 7543
    },
    {
        "loss": 0.7795,
        "grad_norm": 3.110701322555542,
        "learning_rate": 9.826932584744491e-05,
        "epoch": 1.0058666666666667,
        "step": 7544
    },
    {
        "loss": 2.6389,
        "grad_norm": 4.250019073486328,
        "learning_rate": 9.820639887292807e-05,
        "epoch": 1.006,
        "step": 7545
    },
    {
        "loss": 2.0378,
        "grad_norm": 3.1210074424743652,
        "learning_rate": 9.814347260886272e-05,
        "epoch": 1.0061333333333333,
        "step": 7546
    },
    {
        "loss": 2.0001,
        "grad_norm": 2.382486343383789,
        "learning_rate": 9.808054708017422e-05,
        "epoch": 1.0062666666666666,
        "step": 7547
    },
    {
        "loss": 1.9341,
        "grad_norm": 3.9219141006469727,
        "learning_rate": 9.801762231178757e-05,
        "epoch": 1.0064,
        "step": 7548
    },
    {
        "loss": 2.3989,
        "grad_norm": 3.778099775314331,
        "learning_rate": 9.795469832862751e-05,
        "epoch": 1.0065333333333333,
        "step": 7549
    },
    {
        "loss": 0.9306,
        "grad_norm": 3.1851911544799805,
        "learning_rate": 9.789177515561845e-05,
        "epoch": 1.0066666666666666,
        "step": 7550
    },
    {
        "loss": 2.2625,
        "grad_norm": 2.2479805946350098,
        "learning_rate": 9.782885281768428e-05,
        "epoch": 1.0068,
        "step": 7551
    },
    {
        "loss": 2.1808,
        "grad_norm": 3.2178590297698975,
        "learning_rate": 9.776593133974907e-05,
        "epoch": 1.0069333333333332,
        "step": 7552
    },
    {
        "loss": 2.4007,
        "grad_norm": 4.537384033203125,
        "learning_rate": 9.770301074673602e-05,
        "epoch": 1.0070666666666668,
        "step": 7553
    },
    {
        "loss": 2.0364,
        "grad_norm": 3.5459439754486084,
        "learning_rate": 9.764009106356819e-05,
        "epoch": 1.0072,
        "step": 7554
    },
    {
        "loss": 1.6489,
        "grad_norm": 3.361530303955078,
        "learning_rate": 9.757717231516835e-05,
        "epoch": 1.0073333333333334,
        "step": 7555
    },
    {
        "loss": 2.3623,
        "grad_norm": 3.4879043102264404,
        "learning_rate": 9.751425452645879e-05,
        "epoch": 1.0074666666666667,
        "step": 7556
    },
    {
        "loss": 2.1834,
        "grad_norm": 2.558631420135498,
        "learning_rate": 9.745133772236163e-05,
        "epoch": 1.0076,
        "step": 7557
    },
    {
        "loss": 1.0304,
        "grad_norm": 3.7627532482147217,
        "learning_rate": 9.738842192779809e-05,
        "epoch": 1.0077333333333334,
        "step": 7558
    },
    {
        "loss": 2.0037,
        "grad_norm": 3.8612220287323,
        "learning_rate": 9.732550716768965e-05,
        "epoch": 1.0078666666666667,
        "step": 7559
    },
    {
        "loss": 0.5519,
        "grad_norm": 2.7292535305023193,
        "learning_rate": 9.726259346695678e-05,
        "epoch": 1.008,
        "step": 7560
    },
    {
        "loss": 2.6026,
        "grad_norm": 2.190349578857422,
        "learning_rate": 9.719968085052018e-05,
        "epoch": 1.0081333333333333,
        "step": 7561
    },
    {
        "loss": 2.243,
        "grad_norm": 1.6424098014831543,
        "learning_rate": 9.713676934329942e-05,
        "epoch": 1.0082666666666666,
        "step": 7562
    },
    {
        "loss": 2.3017,
        "grad_norm": 4.410459518432617,
        "learning_rate": 9.707385897021406e-05,
        "epoch": 1.0084,
        "step": 7563
    },
    {
        "loss": 2.1077,
        "grad_norm": 2.872788906097412,
        "learning_rate": 9.701094975618312e-05,
        "epoch": 1.0085333333333333,
        "step": 7564
    },
    {
        "loss": 0.6443,
        "grad_norm": 3.4941413402557373,
        "learning_rate": 9.694804172612526e-05,
        "epoch": 1.0086666666666666,
        "step": 7565
    },
    {
        "loss": 1.2849,
        "grad_norm": 4.5201616287231445,
        "learning_rate": 9.688513490495847e-05,
        "epoch": 1.0088,
        "step": 7566
    },
    {
        "loss": 1.402,
        "grad_norm": 4.937035083770752,
        "learning_rate": 9.682222931760019e-05,
        "epoch": 1.0089333333333332,
        "step": 7567
    },
    {
        "loss": 2.1955,
        "grad_norm": 3.39326548576355,
        "learning_rate": 9.675932498896787e-05,
        "epoch": 1.0090666666666666,
        "step": 7568
    },
    {
        "loss": 1.003,
        "grad_norm": 3.6007370948791504,
        "learning_rate": 9.669642194397783e-05,
        "epoch": 1.0092,
        "step": 7569
    },
    {
        "loss": 2.9052,
        "grad_norm": 1.7099709510803223,
        "learning_rate": 9.663352020754652e-05,
        "epoch": 1.0093333333333334,
        "step": 7570
    },
    {
        "loss": 2.9932,
        "grad_norm": 2.5493109226226807,
        "learning_rate": 9.657061980458917e-05,
        "epoch": 1.0094666666666667,
        "step": 7571
    },
    {
        "loss": 2.3687,
        "grad_norm": 2.9903154373168945,
        "learning_rate": 9.650772076002102e-05,
        "epoch": 1.0096,
        "step": 7572
    },
    {
        "loss": 1.8503,
        "grad_norm": 3.2398524284362793,
        "learning_rate": 9.644482309875658e-05,
        "epoch": 1.0097333333333334,
        "step": 7573
    },
    {
        "loss": 2.4589,
        "grad_norm": 2.2013535499572754,
        "learning_rate": 9.638192684570988e-05,
        "epoch": 1.0098666666666667,
        "step": 7574
    },
    {
        "loss": 0.8753,
        "grad_norm": 3.9795689582824707,
        "learning_rate": 9.631903202579426e-05,
        "epoch": 1.01,
        "step": 7575
    },
    {
        "loss": 2.9331,
        "grad_norm": 3.842700719833374,
        "learning_rate": 9.62561386639225e-05,
        "epoch": 1.0101333333333333,
        "step": 7576
    },
    {
        "loss": 2.1792,
        "grad_norm": 4.229940891265869,
        "learning_rate": 9.619324678500711e-05,
        "epoch": 1.0102666666666666,
        "step": 7577
    },
    {
        "loss": 1.156,
        "grad_norm": 3.04091215133667,
        "learning_rate": 9.613035641395963e-05,
        "epoch": 1.0104,
        "step": 7578
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.956106424331665,
        "learning_rate": 9.606746757569107e-05,
        "epoch": 1.0105333333333333,
        "step": 7579
    },
    {
        "loss": 1.6947,
        "grad_norm": 3.245149850845337,
        "learning_rate": 9.600458029511202e-05,
        "epoch": 1.0106666666666666,
        "step": 7580
    },
    {
        "loss": 0.7631,
        "grad_norm": 2.9072766304016113,
        "learning_rate": 9.594169459713234e-05,
        "epoch": 1.0108,
        "step": 7581
    },
    {
        "loss": 2.9326,
        "grad_norm": 3.8551268577575684,
        "learning_rate": 9.587881050666139e-05,
        "epoch": 1.0109333333333332,
        "step": 7582
    },
    {
        "loss": 1.7613,
        "grad_norm": 3.723421812057495,
        "learning_rate": 9.581592804860742e-05,
        "epoch": 1.0110666666666666,
        "step": 7583
    },
    {
        "loss": 0.7258,
        "grad_norm": 3.868922233581543,
        "learning_rate": 9.575304724787876e-05,
        "epoch": 1.0112,
        "step": 7584
    },
    {
        "loss": 1.7544,
        "grad_norm": 3.7916767597198486,
        "learning_rate": 9.569016812938239e-05,
        "epoch": 1.0113333333333334,
        "step": 7585
    },
    {
        "loss": 1.8892,
        "grad_norm": 2.958305597305298,
        "learning_rate": 9.562729071802535e-05,
        "epoch": 1.0114666666666667,
        "step": 7586
    },
    {
        "loss": 2.7012,
        "grad_norm": 3.1301188468933105,
        "learning_rate": 9.556441503871322e-05,
        "epoch": 1.0116,
        "step": 7587
    },
    {
        "loss": 2.293,
        "grad_norm": 2.5101940631866455,
        "learning_rate": 9.55015411163514e-05,
        "epoch": 1.0117333333333334,
        "step": 7588
    },
    {
        "loss": 2.2437,
        "grad_norm": 4.0604987144470215,
        "learning_rate": 9.543866897584445e-05,
        "epoch": 1.0118666666666667,
        "step": 7589
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.240541934967041,
        "learning_rate": 9.537579864209628e-05,
        "epoch": 1.012,
        "step": 7590
    },
    {
        "loss": 2.7156,
        "grad_norm": 2.4917654991149902,
        "learning_rate": 9.531293014001016e-05,
        "epoch": 1.0121333333333333,
        "step": 7591
    },
    {
        "loss": 1.8623,
        "grad_norm": 2.792699098587036,
        "learning_rate": 9.525006349448819e-05,
        "epoch": 1.0122666666666666,
        "step": 7592
    },
    {
        "loss": 2.2853,
        "grad_norm": 3.253629446029663,
        "learning_rate": 9.518719873043236e-05,
        "epoch": 1.0124,
        "step": 7593
    },
    {
        "loss": 0.5983,
        "grad_norm": 2.999595880508423,
        "learning_rate": 9.51243358727434e-05,
        "epoch": 1.0125333333333333,
        "step": 7594
    },
    {
        "loss": 2.439,
        "grad_norm": 3.449838876724243,
        "learning_rate": 9.506147494632182e-05,
        "epoch": 1.0126666666666666,
        "step": 7595
    },
    {
        "loss": 0.5835,
        "grad_norm": 2.9797775745391846,
        "learning_rate": 9.499861597606668e-05,
        "epoch": 1.0128,
        "step": 7596
    },
    {
        "loss": 1.8467,
        "grad_norm": 4.377370357513428,
        "learning_rate": 9.49357589868768e-05,
        "epoch": 1.0129333333333332,
        "step": 7597
    },
    {
        "loss": 2.045,
        "grad_norm": 2.703165292739868,
        "learning_rate": 9.487290400365008e-05,
        "epoch": 1.0130666666666666,
        "step": 7598
    },
    {
        "loss": 1.6153,
        "grad_norm": 2.993835687637329,
        "learning_rate": 9.48100510512836e-05,
        "epoch": 1.0132,
        "step": 7599
    },
    {
        "loss": 1.0032,
        "grad_norm": 3.8544435501098633,
        "learning_rate": 9.474720015467355e-05,
        "epoch": 1.0133333333333334,
        "step": 7600
    },
    {
        "loss": 2.2965,
        "grad_norm": 3.2522919178009033,
        "learning_rate": 9.468435133871531e-05,
        "epoch": 1.0134666666666667,
        "step": 7601
    },
    {
        "loss": 2.0214,
        "grad_norm": 3.765230178833008,
        "learning_rate": 9.462150462830377e-05,
        "epoch": 1.0136,
        "step": 7602
    },
    {
        "loss": 2.0215,
        "grad_norm": 2.063730478286743,
        "learning_rate": 9.455866004833256e-05,
        "epoch": 1.0137333333333334,
        "step": 7603
    },
    {
        "loss": 2.2394,
        "grad_norm": 3.094318151473999,
        "learning_rate": 9.449581762369456e-05,
        "epoch": 1.0138666666666667,
        "step": 7604
    },
    {
        "loss": 3.4908,
        "grad_norm": 4.565776348114014,
        "learning_rate": 9.443297737928198e-05,
        "epoch": 1.014,
        "step": 7605
    },
    {
        "loss": 1.4052,
        "grad_norm": 3.0220983028411865,
        "learning_rate": 9.437013933998602e-05,
        "epoch": 1.0141333333333333,
        "step": 7606
    },
    {
        "loss": 2.1637,
        "grad_norm": 4.502399444580078,
        "learning_rate": 9.430730353069718e-05,
        "epoch": 1.0142666666666666,
        "step": 7607
    },
    {
        "loss": 1.1955,
        "grad_norm": 5.48780632019043,
        "learning_rate": 9.424446997630459e-05,
        "epoch": 1.0144,
        "step": 7608
    },
    {
        "loss": 2.8207,
        "grad_norm": 3.4682693481445312,
        "learning_rate": 9.418163870169722e-05,
        "epoch": 1.0145333333333333,
        "step": 7609
    },
    {
        "loss": 1.9226,
        "grad_norm": 3.4769742488861084,
        "learning_rate": 9.411880973176243e-05,
        "epoch": 1.0146666666666666,
        "step": 7610
    },
    {
        "loss": 2.1999,
        "grad_norm": 3.275275945663452,
        "learning_rate": 9.405598309138729e-05,
        "epoch": 1.0148,
        "step": 7611
    },
    {
        "loss": 2.2096,
        "grad_norm": 2.8007798194885254,
        "learning_rate": 9.39931588054575e-05,
        "epoch": 1.0149333333333332,
        "step": 7612
    },
    {
        "loss": 2.4106,
        "grad_norm": 3.0473194122314453,
        "learning_rate": 9.393033689885792e-05,
        "epoch": 1.0150666666666666,
        "step": 7613
    },
    {
        "loss": 1.6974,
        "grad_norm": 4.888355255126953,
        "learning_rate": 9.386751739647255e-05,
        "epoch": 1.0152,
        "step": 7614
    },
    {
        "loss": 1.0489,
        "grad_norm": 5.069857120513916,
        "learning_rate": 9.380470032318445e-05,
        "epoch": 1.0153333333333334,
        "step": 7615
    },
    {
        "loss": 1.781,
        "grad_norm": 2.062561273574829,
        "learning_rate": 9.37418857038758e-05,
        "epoch": 1.0154666666666667,
        "step": 7616
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.991377115249634,
        "learning_rate": 9.367907356342734e-05,
        "epoch": 1.0156,
        "step": 7617
    },
    {
        "loss": 2.755,
        "grad_norm": 3.2957587242126465,
        "learning_rate": 9.36162639267195e-05,
        "epoch": 1.0157333333333334,
        "step": 7618
    },
    {
        "loss": 1.9356,
        "grad_norm": 3.461160659790039,
        "learning_rate": 9.355345681863117e-05,
        "epoch": 1.0158666666666667,
        "step": 7619
    },
    {
        "loss": 1.7231,
        "grad_norm": 3.7281196117401123,
        "learning_rate": 9.349065226404074e-05,
        "epoch": 1.016,
        "step": 7620
    },
    {
        "loss": 1.5867,
        "grad_norm": 3.1036298274993896,
        "learning_rate": 9.3427850287825e-05,
        "epoch": 1.0161333333333333,
        "step": 7621
    },
    {
        "loss": 2.5054,
        "grad_norm": 3.787071704864502,
        "learning_rate": 9.336505091486014e-05,
        "epoch": 1.0162666666666667,
        "step": 7622
    },
    {
        "loss": 2.0521,
        "grad_norm": 4.782291889190674,
        "learning_rate": 9.330225417002125e-05,
        "epoch": 1.0164,
        "step": 7623
    },
    {
        "loss": 2.215,
        "grad_norm": 3.6246278285980225,
        "learning_rate": 9.323946007818235e-05,
        "epoch": 1.0165333333333333,
        "step": 7624
    },
    {
        "loss": 2.0802,
        "grad_norm": 3.8359103202819824,
        "learning_rate": 9.317666866421635e-05,
        "epoch": 1.0166666666666666,
        "step": 7625
    },
    {
        "loss": 2.3282,
        "grad_norm": 3.2686030864715576,
        "learning_rate": 9.311387995299499e-05,
        "epoch": 1.0168,
        "step": 7626
    },
    {
        "loss": 1.279,
        "grad_norm": 3.171851873397827,
        "learning_rate": 9.305109396938941e-05,
        "epoch": 1.0169333333333332,
        "step": 7627
    },
    {
        "loss": 1.2209,
        "grad_norm": 3.0780961513519287,
        "learning_rate": 9.298831073826903e-05,
        "epoch": 1.0170666666666666,
        "step": 7628
    },
    {
        "loss": 2.6884,
        "grad_norm": 2.467160701751709,
        "learning_rate": 9.292553028450288e-05,
        "epoch": 1.0172,
        "step": 7629
    },
    {
        "loss": 1.7765,
        "grad_norm": 2.0845324993133545,
        "learning_rate": 9.286275263295816e-05,
        "epoch": 1.0173333333333334,
        "step": 7630
    },
    {
        "loss": 1.9435,
        "grad_norm": 3.7147581577301025,
        "learning_rate": 9.279997780850142e-05,
        "epoch": 1.0174666666666667,
        "step": 7631
    },
    {
        "loss": 2.6795,
        "grad_norm": 3.4560487270355225,
        "learning_rate": 9.2737205835998e-05,
        "epoch": 1.0176,
        "step": 7632
    },
    {
        "loss": 2.2099,
        "grad_norm": 2.776480197906494,
        "learning_rate": 9.267443674031218e-05,
        "epoch": 1.0177333333333334,
        "step": 7633
    },
    {
        "loss": 1.7941,
        "grad_norm": 1.8165405988693237,
        "learning_rate": 9.261167054630694e-05,
        "epoch": 1.0178666666666667,
        "step": 7634
    },
    {
        "loss": 2.2998,
        "grad_norm": 3.1888844966888428,
        "learning_rate": 9.254890727884404e-05,
        "epoch": 1.018,
        "step": 7635
    },
    {
        "loss": 0.882,
        "grad_norm": 3.1681649684906006,
        "learning_rate": 9.248614696278445e-05,
        "epoch": 1.0181333333333333,
        "step": 7636
    },
    {
        "loss": 1.7214,
        "grad_norm": 3.157585620880127,
        "learning_rate": 9.242338962298766e-05,
        "epoch": 1.0182666666666667,
        "step": 7637
    },
    {
        "loss": 2.5403,
        "grad_norm": 3.986353874206543,
        "learning_rate": 9.236063528431203e-05,
        "epoch": 1.0184,
        "step": 7638
    },
    {
        "loss": 2.7969,
        "grad_norm": 4.740060329437256,
        "learning_rate": 9.229788397161477e-05,
        "epoch": 1.0185333333333333,
        "step": 7639
    },
    {
        "loss": 1.9426,
        "grad_norm": 3.011338710784912,
        "learning_rate": 9.223513570975188e-05,
        "epoch": 1.0186666666666666,
        "step": 7640
    },
    {
        "loss": 1.9306,
        "grad_norm": 6.690124988555908,
        "learning_rate": 9.217239052357837e-05,
        "epoch": 1.0188,
        "step": 7641
    },
    {
        "loss": 2.3388,
        "grad_norm": 3.8094637393951416,
        "learning_rate": 9.210964843794744e-05,
        "epoch": 1.0189333333333332,
        "step": 7642
    },
    {
        "loss": 1.7254,
        "grad_norm": 8.30497932434082,
        "learning_rate": 9.204690947771176e-05,
        "epoch": 1.0190666666666666,
        "step": 7643
    },
    {
        "loss": 1.968,
        "grad_norm": 2.4191129207611084,
        "learning_rate": 9.198417366772222e-05,
        "epoch": 1.0192,
        "step": 7644
    },
    {
        "loss": 1.5794,
        "grad_norm": 3.748934745788574,
        "learning_rate": 9.192144103282901e-05,
        "epoch": 1.0193333333333334,
        "step": 7645
    },
    {
        "loss": 2.8138,
        "grad_norm": 4.949172019958496,
        "learning_rate": 9.185871159788039e-05,
        "epoch": 1.0194666666666667,
        "step": 7646
    },
    {
        "loss": 2.1473,
        "grad_norm": 2.1155412197113037,
        "learning_rate": 9.179598538772383e-05,
        "epoch": 1.0196,
        "step": 7647
    },
    {
        "loss": 2.3628,
        "grad_norm": 3.512669086456299,
        "learning_rate": 9.173326242720545e-05,
        "epoch": 1.0197333333333334,
        "step": 7648
    },
    {
        "loss": 2.5931,
        "grad_norm": 3.3926990032196045,
        "learning_rate": 9.167054274116997e-05,
        "epoch": 1.0198666666666667,
        "step": 7649
    },
    {
        "loss": 1.2684,
        "grad_norm": 6.096060276031494,
        "learning_rate": 9.160782635446103e-05,
        "epoch": 1.02,
        "step": 7650
    },
    {
        "loss": 2.0469,
        "grad_norm": 5.367183685302734,
        "learning_rate": 9.154511329192045e-05,
        "epoch": 1.0201333333333333,
        "step": 7651
    },
    {
        "loss": 1.8472,
        "grad_norm": 3.8380367755889893,
        "learning_rate": 9.148240357838946e-05,
        "epoch": 1.0202666666666667,
        "step": 7652
    },
    {
        "loss": 1.8118,
        "grad_norm": 3.299013376235962,
        "learning_rate": 9.141969723870731e-05,
        "epoch": 1.0204,
        "step": 7653
    },
    {
        "loss": 2.1847,
        "grad_norm": 3.143068790435791,
        "learning_rate": 9.135699429771247e-05,
        "epoch": 1.0205333333333333,
        "step": 7654
    },
    {
        "loss": 2.6386,
        "grad_norm": 3.7698843479156494,
        "learning_rate": 9.129429478024155e-05,
        "epoch": 1.0206666666666666,
        "step": 7655
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.0995874404907227,
        "learning_rate": 9.123159871113013e-05,
        "epoch": 1.0208,
        "step": 7656
    },
    {
        "loss": 1.9479,
        "grad_norm": 3.015866279602051,
        "learning_rate": 9.116890611521231e-05,
        "epoch": 1.0209333333333332,
        "step": 7657
    },
    {
        "loss": 1.5079,
        "grad_norm": 3.931997299194336,
        "learning_rate": 9.110621701732096e-05,
        "epoch": 1.0210666666666666,
        "step": 7658
    },
    {
        "loss": 2.7689,
        "grad_norm": 2.8747408390045166,
        "learning_rate": 9.104353144228736e-05,
        "epoch": 1.0212,
        "step": 7659
    },
    {
        "loss": 2.0279,
        "grad_norm": 3.286454677581787,
        "learning_rate": 9.098084941494134e-05,
        "epoch": 1.0213333333333334,
        "step": 7660
    },
    {
        "loss": 2.5387,
        "grad_norm": 3.187626838684082,
        "learning_rate": 9.091817096011171e-05,
        "epoch": 1.0214666666666667,
        "step": 7661
    },
    {
        "loss": 1.1876,
        "grad_norm": 3.835935592651367,
        "learning_rate": 9.085549610262552e-05,
        "epoch": 1.0216,
        "step": 7662
    },
    {
        "loss": 2.1394,
        "grad_norm": 4.250739097595215,
        "learning_rate": 9.079282486730842e-05,
        "epoch": 1.0217333333333334,
        "step": 7663
    },
    {
        "loss": 2.7351,
        "grad_norm": 3.404318332672119,
        "learning_rate": 9.073015727898476e-05,
        "epoch": 1.0218666666666667,
        "step": 7664
    },
    {
        "loss": 2.5277,
        "grad_norm": 2.9642093181610107,
        "learning_rate": 9.066749336247737e-05,
        "epoch": 1.022,
        "step": 7665
    },
    {
        "loss": 1.2272,
        "grad_norm": 3.3754055500030518,
        "learning_rate": 9.060483314260783e-05,
        "epoch": 1.0221333333333333,
        "step": 7666
    },
    {
        "loss": 1.4872,
        "grad_norm": 2.5624308586120605,
        "learning_rate": 9.054217664419568e-05,
        "epoch": 1.0222666666666667,
        "step": 7667
    },
    {
        "loss": 2.0527,
        "grad_norm": 3.450753688812256,
        "learning_rate": 9.047952389205971e-05,
        "epoch": 1.0224,
        "step": 7668
    },
    {
        "loss": 0.7758,
        "grad_norm": 4.934140205383301,
        "learning_rate": 9.041687491101662e-05,
        "epoch": 1.0225333333333333,
        "step": 7669
    },
    {
        "loss": 2.0867,
        "grad_norm": 3.0968315601348877,
        "learning_rate": 9.03542297258822e-05,
        "epoch": 1.0226666666666666,
        "step": 7670
    },
    {
        "loss": 2.0449,
        "grad_norm": 2.4988088607788086,
        "learning_rate": 9.029158836147024e-05,
        "epoch": 1.0228,
        "step": 7671
    },
    {
        "loss": 2.0265,
        "grad_norm": 3.2993195056915283,
        "learning_rate": 9.022895084259313e-05,
        "epoch": 1.0229333333333333,
        "step": 7672
    },
    {
        "loss": 2.2433,
        "grad_norm": 3.2086949348449707,
        "learning_rate": 9.01663171940619e-05,
        "epoch": 1.0230666666666666,
        "step": 7673
    },
    {
        "loss": 2.1409,
        "grad_norm": 2.897901773452759,
        "learning_rate": 9.010368744068593e-05,
        "epoch": 1.0232,
        "step": 7674
    },
    {
        "loss": 1.8274,
        "grad_norm": 4.727563381195068,
        "learning_rate": 9.004106160727324e-05,
        "epoch": 1.0233333333333334,
        "step": 7675
    },
    {
        "loss": 2.6838,
        "grad_norm": 2.8938403129577637,
        "learning_rate": 8.997843971862975e-05,
        "epoch": 1.0234666666666667,
        "step": 7676
    },
    {
        "loss": 1.3003,
        "grad_norm": 3.876926898956299,
        "learning_rate": 8.991582179956058e-05,
        "epoch": 1.0236,
        "step": 7677
    },
    {
        "loss": 1.4088,
        "grad_norm": 3.411324977874756,
        "learning_rate": 8.985320787486863e-05,
        "epoch": 1.0237333333333334,
        "step": 7678
    },
    {
        "loss": 2.5743,
        "grad_norm": 5.023256778717041,
        "learning_rate": 8.979059796935581e-05,
        "epoch": 1.0238666666666667,
        "step": 7679
    },
    {
        "loss": 1.1419,
        "grad_norm": 4.531010150909424,
        "learning_rate": 8.972799210782178e-05,
        "epoch": 1.024,
        "step": 7680
    },
    {
        "loss": 1.4777,
        "grad_norm": 2.9542243480682373,
        "learning_rate": 8.966539031506509e-05,
        "epoch": 1.0241333333333333,
        "step": 7681
    },
    {
        "loss": 1.9936,
        "grad_norm": 3.484099864959717,
        "learning_rate": 8.960279261588251e-05,
        "epoch": 1.0242666666666667,
        "step": 7682
    },
    {
        "loss": 1.968,
        "grad_norm": 4.426999092102051,
        "learning_rate": 8.954019903506926e-05,
        "epoch": 1.0244,
        "step": 7683
    },
    {
        "loss": 3.021,
        "grad_norm": 4.326518535614014,
        "learning_rate": 8.947760959741882e-05,
        "epoch": 1.0245333333333333,
        "step": 7684
    },
    {
        "loss": 2.064,
        "grad_norm": 2.1208677291870117,
        "learning_rate": 8.941502432772294e-05,
        "epoch": 1.0246666666666666,
        "step": 7685
    },
    {
        "loss": 1.871,
        "grad_norm": 2.077850818634033,
        "learning_rate": 8.935244325077213e-05,
        "epoch": 1.0248,
        "step": 7686
    },
    {
        "loss": 1.9785,
        "grad_norm": 2.6518824100494385,
        "learning_rate": 8.928986639135481e-05,
        "epoch": 1.0249333333333333,
        "step": 7687
    },
    {
        "loss": 1.4376,
        "grad_norm": 3.6208996772766113,
        "learning_rate": 8.922729377425784e-05,
        "epoch": 1.0250666666666666,
        "step": 7688
    },
    {
        "loss": 1.6754,
        "grad_norm": 3.567979335784912,
        "learning_rate": 8.916472542426654e-05,
        "epoch": 1.0252,
        "step": 7689
    },
    {
        "loss": 2.1522,
        "grad_norm": 3.184729814529419,
        "learning_rate": 8.91021613661644e-05,
        "epoch": 1.0253333333333334,
        "step": 7690
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.474285840988159,
        "learning_rate": 8.903960162473328e-05,
        "epoch": 1.0254666666666667,
        "step": 7691
    },
    {
        "loss": 1.6504,
        "grad_norm": 3.185628652572632,
        "learning_rate": 8.897704622475338e-05,
        "epoch": 1.0256,
        "step": 7692
    },
    {
        "loss": 2.7617,
        "grad_norm": 4.253541469573975,
        "learning_rate": 8.891449519100303e-05,
        "epoch": 1.0257333333333334,
        "step": 7693
    },
    {
        "loss": 1.5965,
        "grad_norm": 1.706068992614746,
        "learning_rate": 8.885194854825873e-05,
        "epoch": 1.0258666666666667,
        "step": 7694
    },
    {
        "loss": 3.1848,
        "grad_norm": 3.7328226566314697,
        "learning_rate": 8.878940632129578e-05,
        "epoch": 1.026,
        "step": 7695
    },
    {
        "loss": 1.9693,
        "grad_norm": 4.00184440612793,
        "learning_rate": 8.872686853488718e-05,
        "epoch": 1.0261333333333333,
        "step": 7696
    },
    {
        "loss": 0.4733,
        "grad_norm": 2.520336866378784,
        "learning_rate": 8.866433521380425e-05,
        "epoch": 1.0262666666666667,
        "step": 7697
    },
    {
        "loss": 2.2181,
        "grad_norm": 5.974293231964111,
        "learning_rate": 8.860180638281683e-05,
        "epoch": 1.0264,
        "step": 7698
    },
    {
        "loss": 1.8481,
        "grad_norm": 4.334869861602783,
        "learning_rate": 8.853928206669272e-05,
        "epoch": 1.0265333333333333,
        "step": 7699
    },
    {
        "loss": 2.3392,
        "grad_norm": 4.806255340576172,
        "learning_rate": 8.847676229019817e-05,
        "epoch": 1.0266666666666666,
        "step": 7700
    },
    {
        "loss": 2.2722,
        "grad_norm": 3.395704746246338,
        "learning_rate": 8.841424707809712e-05,
        "epoch": 1.0268,
        "step": 7701
    },
    {
        "loss": 1.7829,
        "grad_norm": 3.841463804244995,
        "learning_rate": 8.835173645515243e-05,
        "epoch": 1.0269333333333333,
        "step": 7702
    },
    {
        "loss": 2.5152,
        "grad_norm": 4.491596221923828,
        "learning_rate": 8.82892304461245e-05,
        "epoch": 1.0270666666666666,
        "step": 7703
    },
    {
        "loss": 1.8245,
        "grad_norm": 3.7612931728363037,
        "learning_rate": 8.822672907577247e-05,
        "epoch": 1.0272,
        "step": 7704
    },
    {
        "loss": 2.3835,
        "grad_norm": 3.033308744430542,
        "learning_rate": 8.816423236885303e-05,
        "epoch": 1.0273333333333334,
        "step": 7705
    },
    {
        "loss": 0.5163,
        "grad_norm": 2.767989158630371,
        "learning_rate": 8.81017403501215e-05,
        "epoch": 1.0274666666666668,
        "step": 7706
    },
    {
        "loss": 2.5364,
        "grad_norm": 2.0978167057037354,
        "learning_rate": 8.803925304433118e-05,
        "epoch": 1.0276,
        "step": 7707
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.760007381439209,
        "learning_rate": 8.797677047623343e-05,
        "epoch": 1.0277333333333334,
        "step": 7708
    },
    {
        "loss": 0.8814,
        "grad_norm": 2.669987678527832,
        "learning_rate": 8.791429267057802e-05,
        "epoch": 1.0278666666666667,
        "step": 7709
    },
    {
        "loss": 0.6461,
        "grad_norm": 2.6485214233398438,
        "learning_rate": 8.785181965211228e-05,
        "epoch": 1.028,
        "step": 7710
    },
    {
        "loss": 2.1412,
        "grad_norm": 4.235666751861572,
        "learning_rate": 8.778935144558231e-05,
        "epoch": 1.0281333333333333,
        "step": 7711
    },
    {
        "loss": 2.3433,
        "grad_norm": 2.8791446685791016,
        "learning_rate": 8.772688807573167e-05,
        "epoch": 1.0282666666666667,
        "step": 7712
    },
    {
        "loss": 1.4057,
        "grad_norm": 2.5664870738983154,
        "learning_rate": 8.76644295673027e-05,
        "epoch": 1.0284,
        "step": 7713
    },
    {
        "loss": 2.896,
        "grad_norm": 2.5884037017822266,
        "learning_rate": 8.760197594503508e-05,
        "epoch": 1.0285333333333333,
        "step": 7714
    },
    {
        "loss": 1.915,
        "grad_norm": 2.578199625015259,
        "learning_rate": 8.7539527233667e-05,
        "epoch": 1.0286666666666666,
        "step": 7715
    },
    {
        "loss": 3.7451,
        "grad_norm": 4.573753356933594,
        "learning_rate": 8.747708345793468e-05,
        "epoch": 1.0288,
        "step": 7716
    },
    {
        "loss": 1.7723,
        "grad_norm": 3.7282872200012207,
        "learning_rate": 8.741464464257234e-05,
        "epoch": 1.0289333333333333,
        "step": 7717
    },
    {
        "loss": 2.1721,
        "grad_norm": 3.3544962406158447,
        "learning_rate": 8.735221081231215e-05,
        "epoch": 1.0290666666666666,
        "step": 7718
    },
    {
        "loss": 1.0862,
        "grad_norm": 2.5997838973999023,
        "learning_rate": 8.728978199188418e-05,
        "epoch": 1.0292,
        "step": 7719
    },
    {
        "loss": 1.3392,
        "grad_norm": 2.9552865028381348,
        "learning_rate": 8.722735820601702e-05,
        "epoch": 1.0293333333333334,
        "step": 7720
    },
    {
        "loss": 2.592,
        "grad_norm": 3.539644479751587,
        "learning_rate": 8.716493947943683e-05,
        "epoch": 1.0294666666666668,
        "step": 7721
    },
    {
        "loss": 1.8677,
        "grad_norm": 2.8620493412017822,
        "learning_rate": 8.710252583686775e-05,
        "epoch": 1.0296,
        "step": 7722
    },
    {
        "loss": 1.6616,
        "grad_norm": 2.3057005405426025,
        "learning_rate": 8.704011730303212e-05,
        "epoch": 1.0297333333333334,
        "step": 7723
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.7010324001312256,
        "learning_rate": 8.69777139026502e-05,
        "epoch": 1.0298666666666667,
        "step": 7724
    },
    {
        "loss": 1.3486,
        "grad_norm": 2.3559515476226807,
        "learning_rate": 8.691531566044032e-05,
        "epoch": 1.03,
        "step": 7725
    },
    {
        "loss": 2.4498,
        "grad_norm": 1.9755141735076904,
        "learning_rate": 8.685292260111827e-05,
        "epoch": 1.0301333333333333,
        "step": 7726
    },
    {
        "loss": 2.7142,
        "grad_norm": 4.9694013595581055,
        "learning_rate": 8.679053474939853e-05,
        "epoch": 1.0302666666666667,
        "step": 7727
    },
    {
        "loss": 2.441,
        "grad_norm": 4.364562034606934,
        "learning_rate": 8.672815212999285e-05,
        "epoch": 1.0304,
        "step": 7728
    },
    {
        "loss": 1.9419,
        "grad_norm": 3.599461078643799,
        "learning_rate": 8.666577476761146e-05,
        "epoch": 1.0305333333333333,
        "step": 7729
    },
    {
        "loss": 1.7694,
        "grad_norm": 2.1874430179595947,
        "learning_rate": 8.660340268696211e-05,
        "epoch": 1.0306666666666666,
        "step": 7730
    },
    {
        "loss": 2.4587,
        "grad_norm": 3.2761504650115967,
        "learning_rate": 8.654103591275055e-05,
        "epoch": 1.0308,
        "step": 7731
    },
    {
        "loss": 2.0592,
        "grad_norm": 3.0956363677978516,
        "learning_rate": 8.647867446968053e-05,
        "epoch": 1.0309333333333333,
        "step": 7732
    },
    {
        "loss": 2.3029,
        "grad_norm": 3.1265456676483154,
        "learning_rate": 8.641631838245361e-05,
        "epoch": 1.0310666666666666,
        "step": 7733
    },
    {
        "loss": 1.6722,
        "grad_norm": 6.424766540527344,
        "learning_rate": 8.635396767576941e-05,
        "epoch": 1.0312,
        "step": 7734
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.3985915184020996,
        "learning_rate": 8.62916223743249e-05,
        "epoch": 1.0313333333333334,
        "step": 7735
    },
    {
        "loss": 1.8478,
        "grad_norm": 5.461617469787598,
        "learning_rate": 8.622928250281563e-05,
        "epoch": 1.0314666666666668,
        "step": 7736
    },
    {
        "loss": 2.2945,
        "grad_norm": 4.217168807983398,
        "learning_rate": 8.616694808593433e-05,
        "epoch": 1.0316,
        "step": 7737
    },
    {
        "loss": 1.8514,
        "grad_norm": 5.9687018394470215,
        "learning_rate": 8.610461914837225e-05,
        "epoch": 1.0317333333333334,
        "step": 7738
    },
    {
        "loss": 2.1512,
        "grad_norm": 4.1554975509643555,
        "learning_rate": 8.604229571481772e-05,
        "epoch": 1.0318666666666667,
        "step": 7739
    },
    {
        "loss": 1.4424,
        "grad_norm": 4.226306438446045,
        "learning_rate": 8.597997780995743e-05,
        "epoch": 1.032,
        "step": 7740
    },
    {
        "loss": 1.5324,
        "grad_norm": 2.9442811012268066,
        "learning_rate": 8.591766545847569e-05,
        "epoch": 1.0321333333333333,
        "step": 7741
    },
    {
        "loss": 2.5215,
        "grad_norm": 3.111285924911499,
        "learning_rate": 8.58553586850547e-05,
        "epoch": 1.0322666666666667,
        "step": 7742
    },
    {
        "loss": 2.2642,
        "grad_norm": 3.231013298034668,
        "learning_rate": 8.579305751437436e-05,
        "epoch": 1.0324,
        "step": 7743
    },
    {
        "loss": 2.9511,
        "grad_norm": 2.3405847549438477,
        "learning_rate": 8.573076197111219e-05,
        "epoch": 1.0325333333333333,
        "step": 7744
    },
    {
        "loss": 1.0967,
        "grad_norm": 4.56007719039917,
        "learning_rate": 8.566847207994398e-05,
        "epoch": 1.0326666666666666,
        "step": 7745
    },
    {
        "loss": 2.2632,
        "grad_norm": 6.05367374420166,
        "learning_rate": 8.56061878655428e-05,
        "epoch": 1.0328,
        "step": 7746
    },
    {
        "loss": 0.788,
        "grad_norm": 4.530277729034424,
        "learning_rate": 8.554390935257961e-05,
        "epoch": 1.0329333333333333,
        "step": 7747
    },
    {
        "loss": 1.2354,
        "grad_norm": 4.88562536239624,
        "learning_rate": 8.548163656572314e-05,
        "epoch": 1.0330666666666666,
        "step": 7748
    },
    {
        "loss": 2.217,
        "grad_norm": 3.9071130752563477,
        "learning_rate": 8.541936952963992e-05,
        "epoch": 1.0332,
        "step": 7749
    },
    {
        "loss": 2.4779,
        "grad_norm": 3.4717342853546143,
        "learning_rate": 8.535710826899413e-05,
        "epoch": 1.0333333333333334,
        "step": 7750
    },
    {
        "loss": 1.9545,
        "grad_norm": 4.25089693069458,
        "learning_rate": 8.529485280844767e-05,
        "epoch": 1.0334666666666668,
        "step": 7751
    },
    {
        "loss": 1.9134,
        "grad_norm": 2.7150912284851074,
        "learning_rate": 8.523260317266015e-05,
        "epoch": 1.0336,
        "step": 7752
    },
    {
        "loss": 0.6803,
        "grad_norm": 2.706423759460449,
        "learning_rate": 8.51703593862887e-05,
        "epoch": 1.0337333333333334,
        "step": 7753
    },
    {
        "loss": 2.6306,
        "grad_norm": 2.2094786167144775,
        "learning_rate": 8.510812147398857e-05,
        "epoch": 1.0338666666666667,
        "step": 7754
    },
    {
        "loss": 2.1024,
        "grad_norm": 4.019716739654541,
        "learning_rate": 8.504588946041223e-05,
        "epoch": 1.034,
        "step": 7755
    },
    {
        "loss": 2.0051,
        "grad_norm": 2.511807441711426,
        "learning_rate": 8.498366337020998e-05,
        "epoch": 1.0341333333333333,
        "step": 7756
    },
    {
        "loss": 1.526,
        "grad_norm": 4.39459753036499,
        "learning_rate": 8.492144322802986e-05,
        "epoch": 1.0342666666666667,
        "step": 7757
    },
    {
        "loss": 2.304,
        "grad_norm": 2.649533748626709,
        "learning_rate": 8.485922905851742e-05,
        "epoch": 1.0344,
        "step": 7758
    },
    {
        "loss": 1.2597,
        "grad_norm": 3.5223488807678223,
        "learning_rate": 8.47970208863161e-05,
        "epoch": 1.0345333333333333,
        "step": 7759
    },
    {
        "loss": 2.6779,
        "grad_norm": 3.7855870723724365,
        "learning_rate": 8.47348187360664e-05,
        "epoch": 1.0346666666666666,
        "step": 7760
    },
    {
        "loss": 2.0028,
        "grad_norm": 3.1119017601013184,
        "learning_rate": 8.467262263240716e-05,
        "epoch": 1.0348,
        "step": 7761
    },
    {
        "loss": 2.6074,
        "grad_norm": 3.4669647216796875,
        "learning_rate": 8.461043259997416e-05,
        "epoch": 1.0349333333333333,
        "step": 7762
    },
    {
        "loss": 2.2412,
        "grad_norm": 3.315951108932495,
        "learning_rate": 8.454824866340147e-05,
        "epoch": 1.0350666666666666,
        "step": 7763
    },
    {
        "loss": 1.6206,
        "grad_norm": 4.502058982849121,
        "learning_rate": 8.448607084732e-05,
        "epoch": 1.0352,
        "step": 7764
    },
    {
        "loss": 2.0971,
        "grad_norm": 5.534605026245117,
        "learning_rate": 8.442389917635872e-05,
        "epoch": 1.0353333333333334,
        "step": 7765
    },
    {
        "loss": 2.743,
        "grad_norm": 4.780779838562012,
        "learning_rate": 8.436173367514404e-05,
        "epoch": 1.0354666666666668,
        "step": 7766
    },
    {
        "loss": 1.6556,
        "grad_norm": 2.782057285308838,
        "learning_rate": 8.429957436830003e-05,
        "epoch": 1.0356,
        "step": 7767
    },
    {
        "loss": 2.4549,
        "grad_norm": 3.488373041152954,
        "learning_rate": 8.423742128044811e-05,
        "epoch": 1.0357333333333334,
        "step": 7768
    },
    {
        "loss": 2.2785,
        "grad_norm": 2.552299737930298,
        "learning_rate": 8.417527443620716e-05,
        "epoch": 1.0358666666666667,
        "step": 7769
    },
    {
        "loss": 1.3642,
        "grad_norm": 3.679445743560791,
        "learning_rate": 8.411313386019409e-05,
        "epoch": 1.036,
        "step": 7770
    },
    {
        "loss": 1.7676,
        "grad_norm": 4.472105979919434,
        "learning_rate": 8.40509995770227e-05,
        "epoch": 1.0361333333333334,
        "step": 7771
    },
    {
        "loss": 2.4468,
        "grad_norm": 3.938934803009033,
        "learning_rate": 8.398887161130494e-05,
        "epoch": 1.0362666666666667,
        "step": 7772
    },
    {
        "loss": 1.9518,
        "grad_norm": 4.414523124694824,
        "learning_rate": 8.392674998764951e-05,
        "epoch": 1.0364,
        "step": 7773
    },
    {
        "loss": 2.047,
        "grad_norm": 4.445125102996826,
        "learning_rate": 8.386463473066321e-05,
        "epoch": 1.0365333333333333,
        "step": 7774
    },
    {
        "loss": 2.4609,
        "grad_norm": 3.5200541019439697,
        "learning_rate": 8.380252586495008e-05,
        "epoch": 1.0366666666666666,
        "step": 7775
    },
    {
        "loss": 1.9852,
        "grad_norm": 3.169600248336792,
        "learning_rate": 8.37404234151117e-05,
        "epoch": 1.0368,
        "step": 7776
    },
    {
        "loss": 2.5575,
        "grad_norm": 2.7989494800567627,
        "learning_rate": 8.3678327405747e-05,
        "epoch": 1.0369333333333333,
        "step": 7777
    },
    {
        "loss": 2.0634,
        "grad_norm": 3.444664716720581,
        "learning_rate": 8.361623786145229e-05,
        "epoch": 1.0370666666666666,
        "step": 7778
    },
    {
        "loss": 2.2408,
        "grad_norm": 4.165883541107178,
        "learning_rate": 8.355415480682175e-05,
        "epoch": 1.0372,
        "step": 7779
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.3123233318328857,
        "learning_rate": 8.349207826644651e-05,
        "epoch": 1.0373333333333334,
        "step": 7780
    },
    {
        "loss": 2.1681,
        "grad_norm": 2.6805508136749268,
        "learning_rate": 8.343000826491525e-05,
        "epoch": 1.0374666666666668,
        "step": 7781
    },
    {
        "loss": 2.384,
        "grad_norm": 4.487368583679199,
        "learning_rate": 8.336794482681419e-05,
        "epoch": 1.0376,
        "step": 7782
    },
    {
        "loss": 1.1244,
        "grad_norm": 3.975233793258667,
        "learning_rate": 8.330588797672686e-05,
        "epoch": 1.0377333333333334,
        "step": 7783
    },
    {
        "loss": 2.6333,
        "grad_norm": 2.928725481033325,
        "learning_rate": 8.324383773923431e-05,
        "epoch": 1.0378666666666667,
        "step": 7784
    },
    {
        "loss": 1.6778,
        "grad_norm": 4.358541965484619,
        "learning_rate": 8.318179413891452e-05,
        "epoch": 1.038,
        "step": 7785
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.8775782585144043,
        "learning_rate": 8.311975720034351e-05,
        "epoch": 1.0381333333333334,
        "step": 7786
    },
    {
        "loss": 1.6903,
        "grad_norm": 3.4700257778167725,
        "learning_rate": 8.305772694809406e-05,
        "epoch": 1.0382666666666667,
        "step": 7787
    },
    {
        "loss": 2.3288,
        "grad_norm": 3.120713949203491,
        "learning_rate": 8.29957034067369e-05,
        "epoch": 1.0384,
        "step": 7788
    },
    {
        "loss": 2.0263,
        "grad_norm": 4.120080947875977,
        "learning_rate": 8.293368660083944e-05,
        "epoch": 1.0385333333333333,
        "step": 7789
    },
    {
        "loss": 2.205,
        "grad_norm": 3.3421757221221924,
        "learning_rate": 8.287167655496686e-05,
        "epoch": 1.0386666666666666,
        "step": 7790
    },
    {
        "loss": 2.3584,
        "grad_norm": 3.1932575702667236,
        "learning_rate": 8.280967329368155e-05,
        "epoch": 1.0388,
        "step": 7791
    },
    {
        "loss": 2.2697,
        "grad_norm": 3.584627628326416,
        "learning_rate": 8.274767684154323e-05,
        "epoch": 1.0389333333333333,
        "step": 7792
    },
    {
        "loss": 1.7447,
        "grad_norm": 4.67594051361084,
        "learning_rate": 8.268568722310904e-05,
        "epoch": 1.0390666666666666,
        "step": 7793
    },
    {
        "loss": 1.4586,
        "grad_norm": 3.1944305896759033,
        "learning_rate": 8.262370446293293e-05,
        "epoch": 1.0392,
        "step": 7794
    },
    {
        "loss": 2.3906,
        "grad_norm": 3.6074483394622803,
        "learning_rate": 8.256172858556682e-05,
        "epoch": 1.0393333333333334,
        "step": 7795
    },
    {
        "loss": 0.6932,
        "grad_norm": 3.3301455974578857,
        "learning_rate": 8.249975961555931e-05,
        "epoch": 1.0394666666666668,
        "step": 7796
    },
    {
        "loss": 2.5366,
        "grad_norm": 3.209578514099121,
        "learning_rate": 8.243779757745685e-05,
        "epoch": 1.0396,
        "step": 7797
    },
    {
        "loss": 1.2006,
        "grad_norm": 2.8364951610565186,
        "learning_rate": 8.237584249580246e-05,
        "epoch": 1.0397333333333334,
        "step": 7798
    },
    {
        "loss": 2.7083,
        "grad_norm": 3.7855584621429443,
        "learning_rate": 8.231389439513694e-05,
        "epoch": 1.0398666666666667,
        "step": 7799
    },
    {
        "loss": 1.0919,
        "grad_norm": 4.860838413238525,
        "learning_rate": 8.22519532999981e-05,
        "epoch": 1.04,
        "step": 7800
    },
    {
        "loss": 1.9774,
        "grad_norm": 5.301713466644287,
        "learning_rate": 8.21900192349211e-05,
        "epoch": 1.0401333333333334,
        "step": 7801
    },
    {
        "loss": 2.6866,
        "grad_norm": 4.466081619262695,
        "learning_rate": 8.212809222443815e-05,
        "epoch": 1.0402666666666667,
        "step": 7802
    },
    {
        "loss": 2.3333,
        "grad_norm": 3.2679860591888428,
        "learning_rate": 8.206617229307865e-05,
        "epoch": 1.0404,
        "step": 7803
    },
    {
        "loss": 1.0782,
        "grad_norm": 6.70484733581543,
        "learning_rate": 8.200425946536956e-05,
        "epoch": 1.0405333333333333,
        "step": 7804
    },
    {
        "loss": 2.2271,
        "grad_norm": 2.751465082168579,
        "learning_rate": 8.19423537658346e-05,
        "epoch": 1.0406666666666666,
        "step": 7805
    },
    {
        "loss": 0.6557,
        "grad_norm": 3.146672010421753,
        "learning_rate": 8.188045521899475e-05,
        "epoch": 1.0408,
        "step": 7806
    },
    {
        "loss": 1.7241,
        "grad_norm": 3.243330717086792,
        "learning_rate": 8.181856384936837e-05,
        "epoch": 1.0409333333333333,
        "step": 7807
    },
    {
        "loss": 1.4201,
        "grad_norm": 3.617314100265503,
        "learning_rate": 8.175667968147078e-05,
        "epoch": 1.0410666666666666,
        "step": 7808
    },
    {
        "loss": 2.1084,
        "grad_norm": 3.336648941040039,
        "learning_rate": 8.169480273981453e-05,
        "epoch": 1.0412,
        "step": 7809
    },
    {
        "loss": 1.7009,
        "grad_norm": 3.7530336380004883,
        "learning_rate": 8.163293304890937e-05,
        "epoch": 1.0413333333333332,
        "step": 7810
    },
    {
        "loss": 0.9399,
        "grad_norm": 4.9201340675354,
        "learning_rate": 8.1571070633262e-05,
        "epoch": 1.0414666666666668,
        "step": 7811
    },
    {
        "loss": 1.4906,
        "grad_norm": 4.782465934753418,
        "learning_rate": 8.15092155173762e-05,
        "epoch": 1.0416,
        "step": 7812
    },
    {
        "loss": 2.0782,
        "grad_norm": 3.6809093952178955,
        "learning_rate": 8.14473677257533e-05,
        "epoch": 1.0417333333333334,
        "step": 7813
    },
    {
        "loss": 2.6358,
        "grad_norm": 3.1706628799438477,
        "learning_rate": 8.138552728289127e-05,
        "epoch": 1.0418666666666667,
        "step": 7814
    },
    {
        "loss": 2.4372,
        "grad_norm": 3.6208908557891846,
        "learning_rate": 8.132369421328523e-05,
        "epoch": 1.042,
        "step": 7815
    },
    {
        "loss": 3.1977,
        "grad_norm": 2.839174747467041,
        "learning_rate": 8.126186854142752e-05,
        "epoch": 1.0421333333333334,
        "step": 7816
    },
    {
        "loss": 2.4098,
        "grad_norm": 3.6114816665649414,
        "learning_rate": 8.120005029180757e-05,
        "epoch": 1.0422666666666667,
        "step": 7817
    },
    {
        "loss": 2.2456,
        "grad_norm": 3.482848882675171,
        "learning_rate": 8.113823948891188e-05,
        "epoch": 1.0424,
        "step": 7818
    },
    {
        "loss": 1.8047,
        "grad_norm": 3.1060986518859863,
        "learning_rate": 8.10764361572236e-05,
        "epoch": 1.0425333333333333,
        "step": 7819
    },
    {
        "loss": 0.8714,
        "grad_norm": 2.1543004512786865,
        "learning_rate": 8.101464032122354e-05,
        "epoch": 1.0426666666666666,
        "step": 7820
    },
    {
        "loss": 1.5307,
        "grad_norm": 4.19429874420166,
        "learning_rate": 8.095285200538904e-05,
        "epoch": 1.0428,
        "step": 7821
    },
    {
        "loss": 1.7747,
        "grad_norm": 2.0271151065826416,
        "learning_rate": 8.089107123419497e-05,
        "epoch": 1.0429333333333333,
        "step": 7822
    },
    {
        "loss": 2.1835,
        "grad_norm": 4.251524925231934,
        "learning_rate": 8.082929803211255e-05,
        "epoch": 1.0430666666666666,
        "step": 7823
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.0194361209869385,
        "learning_rate": 8.076753242361048e-05,
        "epoch": 1.0432,
        "step": 7824
    },
    {
        "loss": 1.7727,
        "grad_norm": 5.022224426269531,
        "learning_rate": 8.070577443315439e-05,
        "epoch": 1.0433333333333334,
        "step": 7825
    },
    {
        "loss": 1.8529,
        "grad_norm": 4.49639368057251,
        "learning_rate": 8.064402408520683e-05,
        "epoch": 1.0434666666666668,
        "step": 7826
    },
    {
        "loss": 1.7727,
        "grad_norm": 3.3270106315612793,
        "learning_rate": 8.05822814042273e-05,
        "epoch": 1.0436,
        "step": 7827
    },
    {
        "loss": 2.309,
        "grad_norm": 3.737260341644287,
        "learning_rate": 8.052054641467212e-05,
        "epoch": 1.0437333333333334,
        "step": 7828
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.9664413928985596,
        "learning_rate": 8.045881914099502e-05,
        "epoch": 1.0438666666666667,
        "step": 7829
    },
    {
        "loss": 2.336,
        "grad_norm": 3.1601855754852295,
        "learning_rate": 8.03970996076461e-05,
        "epoch": 1.044,
        "step": 7830
    },
    {
        "loss": 1.911,
        "grad_norm": 3.3374741077423096,
        "learning_rate": 8.033538783907303e-05,
        "epoch": 1.0441333333333334,
        "step": 7831
    },
    {
        "loss": 1.9543,
        "grad_norm": 2.9846320152282715,
        "learning_rate": 8.027368385971972e-05,
        "epoch": 1.0442666666666667,
        "step": 7832
    },
    {
        "loss": 1.5555,
        "grad_norm": 3.623474597930908,
        "learning_rate": 8.021198769402744e-05,
        "epoch": 1.0444,
        "step": 7833
    },
    {
        "loss": 1.8191,
        "grad_norm": 3.4895131587982178,
        "learning_rate": 8.015029936643425e-05,
        "epoch": 1.0445333333333333,
        "step": 7834
    },
    {
        "loss": 2.6244,
        "grad_norm": 5.862481117248535,
        "learning_rate": 8.00886189013752e-05,
        "epoch": 1.0446666666666666,
        "step": 7835
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.090071201324463,
        "learning_rate": 8.002694632328205e-05,
        "epoch": 1.0448,
        "step": 7836
    },
    {
        "loss": 2.5292,
        "grad_norm": 3.2599472999572754,
        "learning_rate": 7.996528165658338e-05,
        "epoch": 1.0449333333333333,
        "step": 7837
    },
    {
        "loss": 2.6052,
        "grad_norm": 4.1317291259765625,
        "learning_rate": 7.990362492570503e-05,
        "epoch": 1.0450666666666666,
        "step": 7838
    },
    {
        "loss": 1.8617,
        "grad_norm": 3.8394312858581543,
        "learning_rate": 7.984197615506934e-05,
        "epoch": 1.0452,
        "step": 7839
    },
    {
        "loss": 2.5667,
        "grad_norm": 2.5362091064453125,
        "learning_rate": 7.978033536909553e-05,
        "epoch": 1.0453333333333332,
        "step": 7840
    },
    {
        "loss": 2.4157,
        "grad_norm": 3.095590591430664,
        "learning_rate": 7.971870259219976e-05,
        "epoch": 1.0454666666666668,
        "step": 7841
    },
    {
        "loss": 0.9805,
        "grad_norm": 3.8354859352111816,
        "learning_rate": 7.965707784879498e-05,
        "epoch": 1.0456,
        "step": 7842
    },
    {
        "loss": 2.3707,
        "grad_norm": 4.056961536407471,
        "learning_rate": 7.959546116329113e-05,
        "epoch": 1.0457333333333334,
        "step": 7843
    },
    {
        "loss": 2.6097,
        "grad_norm": 1.6509699821472168,
        "learning_rate": 7.953385256009446e-05,
        "epoch": 1.0458666666666667,
        "step": 7844
    },
    {
        "loss": 2.4547,
        "grad_norm": 2.2907395362854004,
        "learning_rate": 7.947225206360865e-05,
        "epoch": 1.046,
        "step": 7845
    },
    {
        "loss": 1.8884,
        "grad_norm": 3.057359457015991,
        "learning_rate": 7.94106596982336e-05,
        "epoch": 1.0461333333333334,
        "step": 7846
    },
    {
        "loss": 1.642,
        "grad_norm": 5.07285737991333,
        "learning_rate": 7.93490754883666e-05,
        "epoch": 1.0462666666666667,
        "step": 7847
    },
    {
        "loss": 4.1238,
        "grad_norm": 5.026001930236816,
        "learning_rate": 7.928749945840104e-05,
        "epoch": 1.0464,
        "step": 7848
    },
    {
        "loss": 2.2274,
        "grad_norm": 3.395630359649658,
        "learning_rate": 7.922593163272751e-05,
        "epoch": 1.0465333333333333,
        "step": 7849
    },
    {
        "loss": 2.5389,
        "grad_norm": 4.0842814445495605,
        "learning_rate": 7.916437203573326e-05,
        "epoch": 1.0466666666666666,
        "step": 7850
    },
    {
        "loss": 2.4644,
        "grad_norm": 2.299135446548462,
        "learning_rate": 7.910282069180224e-05,
        "epoch": 1.0468,
        "step": 7851
    },
    {
        "loss": 0.9793,
        "grad_norm": 4.268161773681641,
        "learning_rate": 7.904127762531528e-05,
        "epoch": 1.0469333333333333,
        "step": 7852
    },
    {
        "loss": 2.9686,
        "grad_norm": 4.368167877197266,
        "learning_rate": 7.897974286064946e-05,
        "epoch": 1.0470666666666666,
        "step": 7853
    },
    {
        "loss": 1.998,
        "grad_norm": 4.925609111785889,
        "learning_rate": 7.891821642217926e-05,
        "epoch": 1.0472,
        "step": 7854
    },
    {
        "loss": 2.1667,
        "grad_norm": 4.342293739318848,
        "learning_rate": 7.885669833427525e-05,
        "epoch": 1.0473333333333332,
        "step": 7855
    },
    {
        "loss": 1.9113,
        "grad_norm": 3.0974016189575195,
        "learning_rate": 7.879518862130524e-05,
        "epoch": 1.0474666666666668,
        "step": 7856
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.873642921447754,
        "learning_rate": 7.873368730763315e-05,
        "epoch": 1.0476,
        "step": 7857
    },
    {
        "loss": 1.5858,
        "grad_norm": 3.306014060974121,
        "learning_rate": 7.867219441761999e-05,
        "epoch": 1.0477333333333334,
        "step": 7858
    },
    {
        "loss": 2.8561,
        "grad_norm": 3.2946465015411377,
        "learning_rate": 7.861070997562328e-05,
        "epoch": 1.0478666666666667,
        "step": 7859
    },
    {
        "loss": 1.2401,
        "grad_norm": 2.1972944736480713,
        "learning_rate": 7.854923400599731e-05,
        "epoch": 1.048,
        "step": 7860
    },
    {
        "loss": 1.4334,
        "grad_norm": 5.4178786277771,
        "learning_rate": 7.848776653309284e-05,
        "epoch": 1.0481333333333334,
        "step": 7861
    },
    {
        "loss": 1.3194,
        "grad_norm": 3.776291608810425,
        "learning_rate": 7.842630758125723e-05,
        "epoch": 1.0482666666666667,
        "step": 7862
    },
    {
        "loss": 1.6217,
        "grad_norm": 3.8241629600524902,
        "learning_rate": 7.83648571748348e-05,
        "epoch": 1.0484,
        "step": 7863
    },
    {
        "loss": 1.5842,
        "grad_norm": 2.6820688247680664,
        "learning_rate": 7.830341533816619e-05,
        "epoch": 1.0485333333333333,
        "step": 7864
    },
    {
        "loss": 2.8648,
        "grad_norm": 2.257430076599121,
        "learning_rate": 7.824198209558862e-05,
        "epoch": 1.0486666666666666,
        "step": 7865
    },
    {
        "loss": 1.1326,
        "grad_norm": 1.6654378175735474,
        "learning_rate": 7.818055747143608e-05,
        "epoch": 1.0488,
        "step": 7866
    },
    {
        "loss": 2.5066,
        "grad_norm": 2.291741132736206,
        "learning_rate": 7.811914149003905e-05,
        "epoch": 1.0489333333333333,
        "step": 7867
    },
    {
        "loss": 2.309,
        "grad_norm": 4.576670169830322,
        "learning_rate": 7.805773417572479e-05,
        "epoch": 1.0490666666666666,
        "step": 7868
    },
    {
        "loss": 2.5211,
        "grad_norm": 2.359379768371582,
        "learning_rate": 7.799633555281659e-05,
        "epoch": 1.0492,
        "step": 7869
    },
    {
        "loss": 1.7659,
        "grad_norm": 3.3953542709350586,
        "learning_rate": 7.793494564563499e-05,
        "epoch": 1.0493333333333332,
        "step": 7870
    },
    {
        "loss": 1.6934,
        "grad_norm": 3.359337091445923,
        "learning_rate": 7.787356447849645e-05,
        "epoch": 1.0494666666666668,
        "step": 7871
    },
    {
        "loss": 2.0602,
        "grad_norm": 2.4362707138061523,
        "learning_rate": 7.781219207571457e-05,
        "epoch": 1.0496,
        "step": 7872
    },
    {
        "loss": 2.4045,
        "grad_norm": 2.459787368774414,
        "learning_rate": 7.7750828461599e-05,
        "epoch": 1.0497333333333334,
        "step": 7873
    },
    {
        "loss": 1.3964,
        "grad_norm": 2.5142829418182373,
        "learning_rate": 7.7689473660456e-05,
        "epoch": 1.0498666666666667,
        "step": 7874
    },
    {
        "loss": 1.925,
        "grad_norm": 3.8281939029693604,
        "learning_rate": 7.762812769658854e-05,
        "epoch": 1.05,
        "step": 7875
    },
    {
        "loss": 2.9082,
        "grad_norm": 5.502169132232666,
        "learning_rate": 7.756679059429592e-05,
        "epoch": 1.0501333333333334,
        "step": 7876
    },
    {
        "loss": 1.5823,
        "grad_norm": 4.014740943908691,
        "learning_rate": 7.750546237787413e-05,
        "epoch": 1.0502666666666667,
        "step": 7877
    },
    {
        "loss": 1.5391,
        "grad_norm": 3.5980749130249023,
        "learning_rate": 7.74441430716151e-05,
        "epoch": 1.0504,
        "step": 7878
    },
    {
        "loss": 2.1803,
        "grad_norm": 2.7196953296661377,
        "learning_rate": 7.738283269980803e-05,
        "epoch": 1.0505333333333333,
        "step": 7879
    },
    {
        "loss": 2.609,
        "grad_norm": 2.975245475769043,
        "learning_rate": 7.732153128673784e-05,
        "epoch": 1.0506666666666666,
        "step": 7880
    },
    {
        "loss": 0.6589,
        "grad_norm": 3.523090124130249,
        "learning_rate": 7.72602388566866e-05,
        "epoch": 1.0508,
        "step": 7881
    },
    {
        "loss": 2.0139,
        "grad_norm": 3.632268190383911,
        "learning_rate": 7.719895543393207e-05,
        "epoch": 1.0509333333333333,
        "step": 7882
    },
    {
        "loss": 1.0879,
        "grad_norm": 3.786158323287964,
        "learning_rate": 7.713768104274901e-05,
        "epoch": 1.0510666666666666,
        "step": 7883
    },
    {
        "loss": 3.0798,
        "grad_norm": 5.195422649383545,
        "learning_rate": 7.70764157074084e-05,
        "epoch": 1.0512,
        "step": 7884
    },
    {
        "loss": 2.5535,
        "grad_norm": 2.786863088607788,
        "learning_rate": 7.701515945217767e-05,
        "epoch": 1.0513333333333332,
        "step": 7885
    },
    {
        "loss": 2.3359,
        "grad_norm": 2.7300267219543457,
        "learning_rate": 7.695391230132062e-05,
        "epoch": 1.0514666666666668,
        "step": 7886
    },
    {
        "loss": 1.2287,
        "grad_norm": 3.264099597930908,
        "learning_rate": 7.689267427909731e-05,
        "epoch": 1.0516,
        "step": 7887
    },
    {
        "loss": 2.2674,
        "grad_norm": 2.6953365802764893,
        "learning_rate": 7.683144540976458e-05,
        "epoch": 1.0517333333333334,
        "step": 7888
    },
    {
        "loss": 1.3051,
        "grad_norm": 4.268332481384277,
        "learning_rate": 7.677022571757524e-05,
        "epoch": 1.0518666666666667,
        "step": 7889
    },
    {
        "loss": 2.6558,
        "grad_norm": 3.153479814529419,
        "learning_rate": 7.670901522677862e-05,
        "epoch": 1.052,
        "step": 7890
    },
    {
        "loss": 2.8559,
        "grad_norm": 3.348304033279419,
        "learning_rate": 7.664781396162044e-05,
        "epoch": 1.0521333333333334,
        "step": 7891
    },
    {
        "loss": 0.9157,
        "grad_norm": 3.191039800643921,
        "learning_rate": 7.658662194634267e-05,
        "epoch": 1.0522666666666667,
        "step": 7892
    },
    {
        "loss": 1.5746,
        "grad_norm": 4.327073097229004,
        "learning_rate": 7.652543920518375e-05,
        "epoch": 1.0524,
        "step": 7893
    },
    {
        "loss": 1.5286,
        "grad_norm": 2.3979921340942383,
        "learning_rate": 7.646426576237844e-05,
        "epoch": 1.0525333333333333,
        "step": 7894
    },
    {
        "loss": 1.6664,
        "grad_norm": 1.9553128480911255,
        "learning_rate": 7.640310164215764e-05,
        "epoch": 1.0526666666666666,
        "step": 7895
    },
    {
        "loss": 2.4901,
        "grad_norm": 2.5363852977752686,
        "learning_rate": 7.634194686874852e-05,
        "epoch": 1.0528,
        "step": 7896
    },
    {
        "loss": 0.68,
        "grad_norm": 2.7059531211853027,
        "learning_rate": 7.628080146637503e-05,
        "epoch": 1.0529333333333333,
        "step": 7897
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.726006507873535,
        "learning_rate": 7.62196654592569e-05,
        "epoch": 1.0530666666666666,
        "step": 7898
    },
    {
        "loss": 2.3428,
        "grad_norm": 4.129497528076172,
        "learning_rate": 7.615853887161021e-05,
        "epoch": 1.0532,
        "step": 7899
    },
    {
        "loss": 1.9949,
        "grad_norm": 4.02144718170166,
        "learning_rate": 7.609742172764754e-05,
        "epoch": 1.0533333333333332,
        "step": 7900
    },
    {
        "loss": 1.8763,
        "grad_norm": 3.5032646656036377,
        "learning_rate": 7.603631405157758e-05,
        "epoch": 1.0534666666666668,
        "step": 7901
    },
    {
        "loss": 2.3071,
        "grad_norm": 4.127269744873047,
        "learning_rate": 7.597521586760538e-05,
        "epoch": 1.0536,
        "step": 7902
    },
    {
        "loss": 1.9649,
        "grad_norm": 3.5045082569122314,
        "learning_rate": 7.591412719993185e-05,
        "epoch": 1.0537333333333334,
        "step": 7903
    },
    {
        "loss": 1.7877,
        "grad_norm": 4.553220748901367,
        "learning_rate": 7.585304807275475e-05,
        "epoch": 1.0538666666666667,
        "step": 7904
    },
    {
        "loss": 1.7274,
        "grad_norm": 3.2174313068389893,
        "learning_rate": 7.579197851026746e-05,
        "epoch": 1.054,
        "step": 7905
    },
    {
        "loss": 1.416,
        "grad_norm": 2.8740692138671875,
        "learning_rate": 7.573091853666015e-05,
        "epoch": 1.0541333333333334,
        "step": 7906
    },
    {
        "loss": 3.8109,
        "grad_norm": 3.251840114593506,
        "learning_rate": 7.566986817611856e-05,
        "epoch": 1.0542666666666667,
        "step": 7907
    },
    {
        "loss": 1.9644,
        "grad_norm": 4.471078872680664,
        "learning_rate": 7.560882745282512e-05,
        "epoch": 1.0544,
        "step": 7908
    },
    {
        "loss": 1.3815,
        "grad_norm": 3.600834846496582,
        "learning_rate": 7.554779639095821e-05,
        "epoch": 1.0545333333333333,
        "step": 7909
    },
    {
        "loss": 2.1935,
        "grad_norm": 4.077672004699707,
        "learning_rate": 7.548677501469248e-05,
        "epoch": 1.0546666666666666,
        "step": 7910
    },
    {
        "loss": 2.5666,
        "grad_norm": 2.186675548553467,
        "learning_rate": 7.542576334819882e-05,
        "epoch": 1.0548,
        "step": 7911
    },
    {
        "loss": 2.4866,
        "grad_norm": 3.5956482887268066,
        "learning_rate": 7.536476141564385e-05,
        "epoch": 1.0549333333333333,
        "step": 7912
    },
    {
        "loss": 2.0747,
        "grad_norm": 3.504739284515381,
        "learning_rate": 7.530376924119095e-05,
        "epoch": 1.0550666666666666,
        "step": 7913
    },
    {
        "loss": 2.2268,
        "grad_norm": 2.072531223297119,
        "learning_rate": 7.524278684899905e-05,
        "epoch": 1.0552,
        "step": 7914
    },
    {
        "loss": 2.0334,
        "grad_norm": 2.711364269256592,
        "learning_rate": 7.518181426322386e-05,
        "epoch": 1.0553333333333332,
        "step": 7915
    },
    {
        "loss": 1.6023,
        "grad_norm": 6.713644504547119,
        "learning_rate": 7.512085150801648e-05,
        "epoch": 1.0554666666666668,
        "step": 7916
    },
    {
        "loss": 1.1365,
        "grad_norm": 4.576148509979248,
        "learning_rate": 7.505989860752456e-05,
        "epoch": 1.0556,
        "step": 7917
    },
    {
        "loss": 2.0633,
        "grad_norm": 3.7245092391967773,
        "learning_rate": 7.49989555858918e-05,
        "epoch": 1.0557333333333334,
        "step": 7918
    },
    {
        "loss": 0.8536,
        "grad_norm": 4.882761478424072,
        "learning_rate": 7.4938022467258e-05,
        "epoch": 1.0558666666666667,
        "step": 7919
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.7381536960601807,
        "learning_rate": 7.48770992757589e-05,
        "epoch": 1.056,
        "step": 7920
    },
    {
        "loss": 1.182,
        "grad_norm": 4.43385124206543,
        "learning_rate": 7.481618603552621e-05,
        "epoch": 1.0561333333333334,
        "step": 7921
    },
    {
        "loss": 1.4789,
        "grad_norm": 3.4497599601745605,
        "learning_rate": 7.475528277068821e-05,
        "epoch": 1.0562666666666667,
        "step": 7922
    },
    {
        "loss": 2.5334,
        "grad_norm": 4.117837429046631,
        "learning_rate": 7.469438950536871e-05,
        "epoch": 1.0564,
        "step": 7923
    },
    {
        "loss": 1.3604,
        "grad_norm": 5.856576919555664,
        "learning_rate": 7.46335062636877e-05,
        "epoch": 1.0565333333333333,
        "step": 7924
    },
    {
        "loss": 1.3725,
        "grad_norm": 4.269599437713623,
        "learning_rate": 7.457263306976128e-05,
        "epoch": 1.0566666666666666,
        "step": 7925
    },
    {
        "loss": 1.5573,
        "grad_norm": 4.576650619506836,
        "learning_rate": 7.451176994770155e-05,
        "epoch": 1.0568,
        "step": 7926
    },
    {
        "loss": 1.6245,
        "grad_norm": 5.862251281738281,
        "learning_rate": 7.445091692161673e-05,
        "epoch": 1.0569333333333333,
        "step": 7927
    },
    {
        "loss": 1.6089,
        "grad_norm": 2.010218858718872,
        "learning_rate": 7.439007401561056e-05,
        "epoch": 1.0570666666666666,
        "step": 7928
    },
    {
        "loss": 2.2179,
        "grad_norm": 3.4167253971099854,
        "learning_rate": 7.432924125378347e-05,
        "epoch": 1.0572,
        "step": 7929
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.453824996948242,
        "learning_rate": 7.426841866023128e-05,
        "epoch": 1.0573333333333332,
        "step": 7930
    },
    {
        "loss": 2.7462,
        "grad_norm": 4.077243328094482,
        "learning_rate": 7.420760625904626e-05,
        "epoch": 1.0574666666666666,
        "step": 7931
    },
    {
        "loss": 2.1551,
        "grad_norm": 2.5719125270843506,
        "learning_rate": 7.414680407431626e-05,
        "epoch": 1.0576,
        "step": 7932
    },
    {
        "loss": 1.8968,
        "grad_norm": 4.494877338409424,
        "learning_rate": 7.408601213012521e-05,
        "epoch": 1.0577333333333334,
        "step": 7933
    },
    {
        "loss": 1.9507,
        "grad_norm": 3.057969570159912,
        "learning_rate": 7.402523045055307e-05,
        "epoch": 1.0578666666666667,
        "step": 7934
    },
    {
        "loss": 1.7823,
        "grad_norm": 3.6641411781311035,
        "learning_rate": 7.396445905967563e-05,
        "epoch": 1.058,
        "step": 7935
    },
    {
        "loss": 2.1562,
        "grad_norm": 4.885272026062012,
        "learning_rate": 7.390369798156483e-05,
        "epoch": 1.0581333333333334,
        "step": 7936
    },
    {
        "loss": 1.3169,
        "grad_norm": 7.423478603363037,
        "learning_rate": 7.384294724028796e-05,
        "epoch": 1.0582666666666667,
        "step": 7937
    },
    {
        "loss": 1.8792,
        "grad_norm": 5.125577926635742,
        "learning_rate": 7.378220685990897e-05,
        "epoch": 1.0584,
        "step": 7938
    },
    {
        "loss": 1.0813,
        "grad_norm": 5.769942760467529,
        "learning_rate": 7.3721476864487e-05,
        "epoch": 1.0585333333333333,
        "step": 7939
    },
    {
        "loss": 1.9898,
        "grad_norm": 3.5023443698883057,
        "learning_rate": 7.366075727807783e-05,
        "epoch": 1.0586666666666666,
        "step": 7940
    },
    {
        "loss": 2.2091,
        "grad_norm": 3.103003978729248,
        "learning_rate": 7.360004812473231e-05,
        "epoch": 1.0588,
        "step": 7941
    },
    {
        "loss": 1.4604,
        "grad_norm": 3.4518086910247803,
        "learning_rate": 7.35393494284977e-05,
        "epoch": 1.0589333333333333,
        "step": 7942
    },
    {
        "loss": 1.4967,
        "grad_norm": 4.775640964508057,
        "learning_rate": 7.34786612134169e-05,
        "epoch": 1.0590666666666666,
        "step": 7943
    },
    {
        "loss": 2.396,
        "grad_norm": 3.9720194339752197,
        "learning_rate": 7.341798350352884e-05,
        "epoch": 1.0592,
        "step": 7944
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.002300977706909,
        "learning_rate": 7.335731632286811e-05,
        "epoch": 1.0593333333333332,
        "step": 7945
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.642357349395752,
        "learning_rate": 7.329665969546501e-05,
        "epoch": 1.0594666666666668,
        "step": 7946
    },
    {
        "loss": 0.5859,
        "grad_norm": 4.0216546058654785,
        "learning_rate": 7.323601364534615e-05,
        "epoch": 1.0596,
        "step": 7947
    },
    {
        "loss": 1.0953,
        "grad_norm": 2.14255428314209,
        "learning_rate": 7.317537819653349e-05,
        "epoch": 1.0597333333333334,
        "step": 7948
    },
    {
        "loss": 1.7223,
        "grad_norm": 9.25039291381836,
        "learning_rate": 7.311475337304485e-05,
        "epoch": 1.0598666666666667,
        "step": 7949
    },
    {
        "loss": 1.883,
        "grad_norm": 4.627714157104492,
        "learning_rate": 7.305413919889398e-05,
        "epoch": 1.06,
        "step": 7950
    },
    {
        "loss": 2.2733,
        "grad_norm": 3.2711150646209717,
        "learning_rate": 7.299353569809042e-05,
        "epoch": 1.0601333333333334,
        "step": 7951
    },
    {
        "loss": 2.441,
        "grad_norm": 4.998482704162598,
        "learning_rate": 7.293294289463939e-05,
        "epoch": 1.0602666666666667,
        "step": 7952
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.624676465988159,
        "learning_rate": 7.287236081254196e-05,
        "epoch": 1.0604,
        "step": 7953
    },
    {
        "loss": 1.7288,
        "grad_norm": 2.8792765140533447,
        "learning_rate": 7.281178947579486e-05,
        "epoch": 1.0605333333333333,
        "step": 7954
    },
    {
        "loss": 2.0321,
        "grad_norm": 3.3507912158966064,
        "learning_rate": 7.275122890839046e-05,
        "epoch": 1.0606666666666666,
        "step": 7955
    },
    {
        "loss": 2.0014,
        "grad_norm": 4.469291687011719,
        "learning_rate": 7.269067913431727e-05,
        "epoch": 1.0608,
        "step": 7956
    },
    {
        "loss": 1.2566,
        "grad_norm": 2.1280534267425537,
        "learning_rate": 7.263014017755911e-05,
        "epoch": 1.0609333333333333,
        "step": 7957
    },
    {
        "loss": 0.8845,
        "grad_norm": 3.093546152114868,
        "learning_rate": 7.256961206209564e-05,
        "epoch": 1.0610666666666666,
        "step": 7958
    },
    {
        "loss": 2.398,
        "grad_norm": 2.323655366897583,
        "learning_rate": 7.25090948119023e-05,
        "epoch": 1.0612,
        "step": 7959
    },
    {
        "loss": 1.5483,
        "grad_norm": 4.462220668792725,
        "learning_rate": 7.244858845095015e-05,
        "epoch": 1.0613333333333332,
        "step": 7960
    },
    {
        "loss": 2.0941,
        "grad_norm": 3.1197211742401123,
        "learning_rate": 7.238809300320614e-05,
        "epoch": 1.0614666666666666,
        "step": 7961
    },
    {
        "loss": 1.6178,
        "grad_norm": 3.9398670196533203,
        "learning_rate": 7.232760849263241e-05,
        "epoch": 1.0616,
        "step": 7962
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.8093008995056152,
        "learning_rate": 7.226713494318735e-05,
        "epoch": 1.0617333333333334,
        "step": 7963
    },
    {
        "loss": 2.2413,
        "grad_norm": 2.5786538124084473,
        "learning_rate": 7.22066723788245e-05,
        "epoch": 1.0618666666666667,
        "step": 7964
    },
    {
        "loss": 1.5964,
        "grad_norm": 3.914376735687256,
        "learning_rate": 7.214622082349364e-05,
        "epoch": 1.062,
        "step": 7965
    },
    {
        "loss": 1.859,
        "grad_norm": 3.1696417331695557,
        "learning_rate": 7.208578030113948e-05,
        "epoch": 1.0621333333333334,
        "step": 7966
    },
    {
        "loss": 1.6177,
        "grad_norm": 3.4985761642456055,
        "learning_rate": 7.202535083570287e-05,
        "epoch": 1.0622666666666667,
        "step": 7967
    },
    {
        "loss": 2.3296,
        "grad_norm": 3.5226619243621826,
        "learning_rate": 7.196493245112008e-05,
        "epoch": 1.0624,
        "step": 7968
    },
    {
        "loss": 2.1528,
        "grad_norm": 3.0479180812835693,
        "learning_rate": 7.190452517132317e-05,
        "epoch": 1.0625333333333333,
        "step": 7969
    },
    {
        "loss": 1.4991,
        "grad_norm": 3.899824857711792,
        "learning_rate": 7.184412902023959e-05,
        "epoch": 1.0626666666666666,
        "step": 7970
    },
    {
        "loss": 2.1476,
        "grad_norm": 4.334081649780273,
        "learning_rate": 7.178374402179227e-05,
        "epoch": 1.0628,
        "step": 7971
    },
    {
        "loss": 1.9927,
        "grad_norm": 4.700148105621338,
        "learning_rate": 7.172337019990023e-05,
        "epoch": 1.0629333333333333,
        "step": 7972
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.979985475540161,
        "learning_rate": 7.166300757847749e-05,
        "epoch": 1.0630666666666666,
        "step": 7973
    },
    {
        "loss": 2.3764,
        "grad_norm": 4.211705207824707,
        "learning_rate": 7.160265618143419e-05,
        "epoch": 1.0632,
        "step": 7974
    },
    {
        "loss": 0.7088,
        "grad_norm": 4.115138053894043,
        "learning_rate": 7.15423160326754e-05,
        "epoch": 1.0633333333333332,
        "step": 7975
    },
    {
        "loss": 1.8846,
        "grad_norm": 4.223634719848633,
        "learning_rate": 7.14819871561022e-05,
        "epoch": 1.0634666666666668,
        "step": 7976
    },
    {
        "loss": 1.9948,
        "grad_norm": 4.602072715759277,
        "learning_rate": 7.142166957561107e-05,
        "epoch": 1.0636,
        "step": 7977
    },
    {
        "loss": 1.4876,
        "grad_norm": 4.838068962097168,
        "learning_rate": 7.136136331509407e-05,
        "epoch": 1.0637333333333334,
        "step": 7978
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.8838962316513062,
        "learning_rate": 7.130106839843862e-05,
        "epoch": 1.0638666666666667,
        "step": 7979
    },
    {
        "loss": 1.7784,
        "grad_norm": 4.9530720710754395,
        "learning_rate": 7.124078484952766e-05,
        "epoch": 1.064,
        "step": 7980
    },
    {
        "loss": 0.6465,
        "grad_norm": 2.7119741439819336,
        "learning_rate": 7.118051269223993e-05,
        "epoch": 1.0641333333333334,
        "step": 7981
    },
    {
        "loss": 2.2266,
        "grad_norm": 3.5782694816589355,
        "learning_rate": 7.112025195044934e-05,
        "epoch": 1.0642666666666667,
        "step": 7982
    },
    {
        "loss": 1.8682,
        "grad_norm": 3.7433383464813232,
        "learning_rate": 7.106000264802526e-05,
        "epoch": 1.0644,
        "step": 7983
    },
    {
        "loss": 2.6454,
        "grad_norm": 5.820067882537842,
        "learning_rate": 7.099976480883276e-05,
        "epoch": 1.0645333333333333,
        "step": 7984
    },
    {
        "loss": 1.3939,
        "grad_norm": 3.682434320449829,
        "learning_rate": 7.093953845673225e-05,
        "epoch": 1.0646666666666667,
        "step": 7985
    },
    {
        "loss": 1.8262,
        "grad_norm": 2.6771786212921143,
        "learning_rate": 7.087932361557971e-05,
        "epoch": 1.0648,
        "step": 7986
    },
    {
        "loss": 2.269,
        "grad_norm": 2.4234368801116943,
        "learning_rate": 7.081912030922608e-05,
        "epoch": 1.0649333333333333,
        "step": 7987
    },
    {
        "loss": 2.3718,
        "grad_norm": 2.7459936141967773,
        "learning_rate": 7.075892856151849e-05,
        "epoch": 1.0650666666666666,
        "step": 7988
    },
    {
        "loss": 2.1085,
        "grad_norm": 3.561959743499756,
        "learning_rate": 7.069874839629878e-05,
        "epoch": 1.0652,
        "step": 7989
    },
    {
        "loss": 2.1678,
        "grad_norm": 2.082125425338745,
        "learning_rate": 7.06385798374049e-05,
        "epoch": 1.0653333333333332,
        "step": 7990
    },
    {
        "loss": 2.0271,
        "grad_norm": 2.560338020324707,
        "learning_rate": 7.057842290866944e-05,
        "epoch": 1.0654666666666666,
        "step": 7991
    },
    {
        "loss": 1.9453,
        "grad_norm": 3.624846935272217,
        "learning_rate": 7.051827763392093e-05,
        "epoch": 1.0656,
        "step": 7992
    },
    {
        "loss": 1.9035,
        "grad_norm": 2.8002254962921143,
        "learning_rate": 7.045814403698307e-05,
        "epoch": 1.0657333333333334,
        "step": 7993
    },
    {
        "loss": 1.7786,
        "grad_norm": 3.32572078704834,
        "learning_rate": 7.039802214167506e-05,
        "epoch": 1.0658666666666667,
        "step": 7994
    },
    {
        "loss": 2.8689,
        "grad_norm": 3.739264965057373,
        "learning_rate": 7.033791197181146e-05,
        "epoch": 1.066,
        "step": 7995
    },
    {
        "loss": 2.134,
        "grad_norm": 2.767627477645874,
        "learning_rate": 7.027781355120182e-05,
        "epoch": 1.0661333333333334,
        "step": 7996
    },
    {
        "loss": 2.5438,
        "grad_norm": 3.663916826248169,
        "learning_rate": 7.02177269036516e-05,
        "epoch": 1.0662666666666667,
        "step": 7997
    },
    {
        "loss": 1.3196,
        "grad_norm": 1.894575834274292,
        "learning_rate": 7.015765205296114e-05,
        "epoch": 1.0664,
        "step": 7998
    },
    {
        "loss": 2.6475,
        "grad_norm": 2.7394516468048096,
        "learning_rate": 7.009758902292657e-05,
        "epoch": 1.0665333333333333,
        "step": 7999
    },
    {
        "loss": 1.2954,
        "grad_norm": 4.2360100746154785,
        "learning_rate": 7.003753783733875e-05,
        "epoch": 1.0666666666666667,
        "step": 8000
    },
    {
        "loss": 1.8665,
        "grad_norm": 3.262988805770874,
        "learning_rate": 6.997749851998427e-05,
        "epoch": 1.0668,
        "step": 8001
    },
    {
        "loss": 2.4281,
        "grad_norm": 2.5235652923583984,
        "learning_rate": 6.991747109464494e-05,
        "epoch": 1.0669333333333333,
        "step": 8002
    },
    {
        "loss": 0.8518,
        "grad_norm": 3.1092782020568848,
        "learning_rate": 6.985745558509789e-05,
        "epoch": 1.0670666666666666,
        "step": 8003
    },
    {
        "loss": 1.8899,
        "grad_norm": 4.824554920196533,
        "learning_rate": 6.979745201511536e-05,
        "epoch": 1.0672,
        "step": 8004
    },
    {
        "loss": 2.3874,
        "grad_norm": 3.2731504440307617,
        "learning_rate": 6.97374604084649e-05,
        "epoch": 1.0673333333333332,
        "step": 8005
    },
    {
        "loss": 1.5816,
        "grad_norm": 2.6278910636901855,
        "learning_rate": 6.967748078890961e-05,
        "epoch": 1.0674666666666666,
        "step": 8006
    },
    {
        "loss": 1.714,
        "grad_norm": 2.519619941711426,
        "learning_rate": 6.961751318020751e-05,
        "epoch": 1.0676,
        "step": 8007
    },
    {
        "loss": 0.8512,
        "grad_norm": 3.464700937271118,
        "learning_rate": 6.955755760611186e-05,
        "epoch": 1.0677333333333334,
        "step": 8008
    },
    {
        "loss": 1.8475,
        "grad_norm": 2.341968297958374,
        "learning_rate": 6.949761409037142e-05,
        "epoch": 1.0678666666666667,
        "step": 8009
    },
    {
        "loss": 1.6166,
        "grad_norm": 4.20020055770874,
        "learning_rate": 6.943768265672997e-05,
        "epoch": 1.068,
        "step": 8010
    },
    {
        "loss": 1.4076,
        "grad_norm": 4.799115180969238,
        "learning_rate": 6.937776332892651e-05,
        "epoch": 1.0681333333333334,
        "step": 8011
    },
    {
        "loss": 2.0554,
        "grad_norm": 3.1484622955322266,
        "learning_rate": 6.931785613069546e-05,
        "epoch": 1.0682666666666667,
        "step": 8012
    },
    {
        "loss": 2.3185,
        "grad_norm": 2.8275115489959717,
        "learning_rate": 6.925796108576612e-05,
        "epoch": 1.0684,
        "step": 8013
    },
    {
        "loss": 1.8955,
        "grad_norm": 3.82783842086792,
        "learning_rate": 6.9198078217863e-05,
        "epoch": 1.0685333333333333,
        "step": 8014
    },
    {
        "loss": 1.6791,
        "grad_norm": 3.7887027263641357,
        "learning_rate": 6.91382075507062e-05,
        "epoch": 1.0686666666666667,
        "step": 8015
    },
    {
        "loss": 2.125,
        "grad_norm": 3.5755105018615723,
        "learning_rate": 6.907834910801054e-05,
        "epoch": 1.0688,
        "step": 8016
    },
    {
        "loss": 1.7834,
        "grad_norm": 3.6614108085632324,
        "learning_rate": 6.901850291348612e-05,
        "epoch": 1.0689333333333333,
        "step": 8017
    },
    {
        "loss": 1.998,
        "grad_norm": 3.103584051132202,
        "learning_rate": 6.895866899083823e-05,
        "epoch": 1.0690666666666666,
        "step": 8018
    },
    {
        "loss": 0.7596,
        "grad_norm": 3.265469551086426,
        "learning_rate": 6.889884736376733e-05,
        "epoch": 1.0692,
        "step": 8019
    },
    {
        "loss": 1.252,
        "grad_norm": 4.62448787689209,
        "learning_rate": 6.883903805596913e-05,
        "epoch": 1.0693333333333332,
        "step": 8020
    },
    {
        "loss": 1.2913,
        "grad_norm": 3.9156241416931152,
        "learning_rate": 6.87792410911339e-05,
        "epoch": 1.0694666666666666,
        "step": 8021
    },
    {
        "loss": 2.1127,
        "grad_norm": 3.6605143547058105,
        "learning_rate": 6.871945649294778e-05,
        "epoch": 1.0695999999999999,
        "step": 8022
    },
    {
        "loss": 1.7536,
        "grad_norm": 4.330150127410889,
        "learning_rate": 6.865968428509142e-05,
        "epoch": 1.0697333333333334,
        "step": 8023
    },
    {
        "loss": 1.6783,
        "grad_norm": 4.5112624168396,
        "learning_rate": 6.859992449124111e-05,
        "epoch": 1.0698666666666667,
        "step": 8024
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.4907493591308594,
        "learning_rate": 6.854017713506755e-05,
        "epoch": 1.07,
        "step": 8025
    },
    {
        "loss": 2.3654,
        "grad_norm": 3.0804898738861084,
        "learning_rate": 6.848044224023702e-05,
        "epoch": 1.0701333333333334,
        "step": 8026
    },
    {
        "loss": 2.1547,
        "grad_norm": 2.7885828018188477,
        "learning_rate": 6.842071983041074e-05,
        "epoch": 1.0702666666666667,
        "step": 8027
    },
    {
        "loss": 1.3575,
        "grad_norm": 3.995859146118164,
        "learning_rate": 6.836100992924499e-05,
        "epoch": 1.0704,
        "step": 8028
    },
    {
        "loss": 2.2576,
        "grad_norm": 3.8195106983184814,
        "learning_rate": 6.830131256039103e-05,
        "epoch": 1.0705333333333333,
        "step": 8029
    },
    {
        "loss": 1.5393,
        "grad_norm": 4.04768705368042,
        "learning_rate": 6.824162774749502e-05,
        "epoch": 1.0706666666666667,
        "step": 8030
    },
    {
        "loss": 1.8999,
        "grad_norm": 2.2747654914855957,
        "learning_rate": 6.818195551419861e-05,
        "epoch": 1.0708,
        "step": 8031
    },
    {
        "loss": 1.697,
        "grad_norm": 3.905566453933716,
        "learning_rate": 6.812229588413792e-05,
        "epoch": 1.0709333333333333,
        "step": 8032
    },
    {
        "loss": 1.627,
        "grad_norm": 3.187741994857788,
        "learning_rate": 6.806264888094464e-05,
        "epoch": 1.0710666666666666,
        "step": 8033
    },
    {
        "loss": 1.3356,
        "grad_norm": 6.638497352600098,
        "learning_rate": 6.800301452824484e-05,
        "epoch": 1.0712,
        "step": 8034
    },
    {
        "loss": 0.9595,
        "grad_norm": 2.311889410018921,
        "learning_rate": 6.794339284966e-05,
        "epoch": 1.0713333333333332,
        "step": 8035
    },
    {
        "loss": 1.8612,
        "grad_norm": 2.871506690979004,
        "learning_rate": 6.788378386880647e-05,
        "epoch": 1.0714666666666666,
        "step": 8036
    },
    {
        "loss": 2.0637,
        "grad_norm": 3.061918258666992,
        "learning_rate": 6.782418760929567e-05,
        "epoch": 1.0716,
        "step": 8037
    },
    {
        "loss": 2.715,
        "grad_norm": 3.4250876903533936,
        "learning_rate": 6.77646040947338e-05,
        "epoch": 1.0717333333333334,
        "step": 8038
    },
    {
        "loss": 2.0685,
        "grad_norm": 2.8638031482696533,
        "learning_rate": 6.770503334872195e-05,
        "epoch": 1.0718666666666667,
        "step": 8039
    },
    {
        "loss": 2.3666,
        "grad_norm": 2.921095371246338,
        "learning_rate": 6.764547539485654e-05,
        "epoch": 1.072,
        "step": 8040
    },
    {
        "loss": 2.0615,
        "grad_norm": 4.9983930587768555,
        "learning_rate": 6.75859302567286e-05,
        "epoch": 1.0721333333333334,
        "step": 8041
    },
    {
        "loss": 1.9579,
        "grad_norm": 2.2309319972991943,
        "learning_rate": 6.752639795792408e-05,
        "epoch": 1.0722666666666667,
        "step": 8042
    },
    {
        "loss": 2.7757,
        "grad_norm": 2.697662830352783,
        "learning_rate": 6.746687852202396e-05,
        "epoch": 1.0724,
        "step": 8043
    },
    {
        "loss": 2.4705,
        "grad_norm": 3.037466287612915,
        "learning_rate": 6.74073719726041e-05,
        "epoch": 1.0725333333333333,
        "step": 8044
    },
    {
        "loss": 1.8601,
        "grad_norm": 2.76250958442688,
        "learning_rate": 6.73478783332354e-05,
        "epoch": 1.0726666666666667,
        "step": 8045
    },
    {
        "loss": 1.5718,
        "grad_norm": 3.512730360031128,
        "learning_rate": 6.728839762748316e-05,
        "epoch": 1.0728,
        "step": 8046
    },
    {
        "loss": 2.1582,
        "grad_norm": 3.017690658569336,
        "learning_rate": 6.722892987890822e-05,
        "epoch": 1.0729333333333333,
        "step": 8047
    },
    {
        "loss": 1.4235,
        "grad_norm": 2.958861827850342,
        "learning_rate": 6.716947511106566e-05,
        "epoch": 1.0730666666666666,
        "step": 8048
    },
    {
        "loss": 1.5169,
        "grad_norm": 5.400737285614014,
        "learning_rate": 6.71100333475061e-05,
        "epoch": 1.0732,
        "step": 8049
    },
    {
        "loss": 1.9026,
        "grad_norm": 3.272581100463867,
        "learning_rate": 6.705060461177426e-05,
        "epoch": 1.0733333333333333,
        "step": 8050
    },
    {
        "loss": 2.37,
        "grad_norm": 3.1417086124420166,
        "learning_rate": 6.699118892741018e-05,
        "epoch": 1.0734666666666666,
        "step": 8051
    },
    {
        "loss": 2.6622,
        "grad_norm": 3.2109673023223877,
        "learning_rate": 6.693178631794867e-05,
        "epoch": 1.0735999999999999,
        "step": 8052
    },
    {
        "loss": 1.4776,
        "grad_norm": 3.3984262943267822,
        "learning_rate": 6.687239680691929e-05,
        "epoch": 1.0737333333333334,
        "step": 8053
    },
    {
        "loss": 2.141,
        "grad_norm": 3.7384023666381836,
        "learning_rate": 6.681302041784653e-05,
        "epoch": 1.0738666666666667,
        "step": 8054
    },
    {
        "loss": 2.8755,
        "grad_norm": 4.040749549865723,
        "learning_rate": 6.675365717424925e-05,
        "epoch": 1.074,
        "step": 8055
    },
    {
        "loss": 2.6807,
        "grad_norm": 4.313693523406982,
        "learning_rate": 6.66943070996418e-05,
        "epoch": 1.0741333333333334,
        "step": 8056
    },
    {
        "loss": 1.8226,
        "grad_norm": 3.1370675563812256,
        "learning_rate": 6.663497021753267e-05,
        "epoch": 1.0742666666666667,
        "step": 8057
    },
    {
        "loss": 1.4158,
        "grad_norm": 4.251107692718506,
        "learning_rate": 6.657564655142568e-05,
        "epoch": 1.0744,
        "step": 8058
    },
    {
        "loss": 1.9126,
        "grad_norm": 3.7204835414886475,
        "learning_rate": 6.65163361248189e-05,
        "epoch": 1.0745333333333333,
        "step": 8059
    },
    {
        "loss": 2.5119,
        "grad_norm": 2.751401662826538,
        "learning_rate": 6.645703896120545e-05,
        "epoch": 1.0746666666666667,
        "step": 8060
    },
    {
        "loss": 1.6805,
        "grad_norm": 3.545595169067383,
        "learning_rate": 6.639775508407316e-05,
        "epoch": 1.0748,
        "step": 8061
    },
    {
        "loss": 1.1324,
        "grad_norm": 1.953586220741272,
        "learning_rate": 6.633848451690465e-05,
        "epoch": 1.0749333333333333,
        "step": 8062
    },
    {
        "loss": 3.3328,
        "grad_norm": 4.696336269378662,
        "learning_rate": 6.627922728317712e-05,
        "epoch": 1.0750666666666666,
        "step": 8063
    },
    {
        "loss": 2.1124,
        "grad_norm": 5.504935264587402,
        "learning_rate": 6.621998340636244e-05,
        "epoch": 1.0752,
        "step": 8064
    },
    {
        "loss": 2.5929,
        "grad_norm": 3.287299871444702,
        "learning_rate": 6.616075290992755e-05,
        "epoch": 1.0753333333333333,
        "step": 8065
    },
    {
        "loss": 1.5004,
        "grad_norm": 2.778106689453125,
        "learning_rate": 6.610153581733375e-05,
        "epoch": 1.0754666666666666,
        "step": 8066
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.7775990962982178,
        "learning_rate": 6.604233215203705e-05,
        "epoch": 1.0756000000000001,
        "step": 8067
    },
    {
        "loss": 2.0441,
        "grad_norm": 4.508565425872803,
        "learning_rate": 6.59831419374883e-05,
        "epoch": 1.0757333333333334,
        "step": 8068
    },
    {
        "loss": 2.1381,
        "grad_norm": 3.6780357360839844,
        "learning_rate": 6.592396519713292e-05,
        "epoch": 1.0758666666666667,
        "step": 8069
    },
    {
        "loss": 2.54,
        "grad_norm": 2.3186984062194824,
        "learning_rate": 6.586480195441118e-05,
        "epoch": 1.076,
        "step": 8070
    },
    {
        "loss": 2.6477,
        "grad_norm": 3.407898426055908,
        "learning_rate": 6.58056522327575e-05,
        "epoch": 1.0761333333333334,
        "step": 8071
    },
    {
        "loss": 0.7858,
        "grad_norm": 5.503343105316162,
        "learning_rate": 6.574651605560163e-05,
        "epoch": 1.0762666666666667,
        "step": 8072
    },
    {
        "loss": 1.6867,
        "grad_norm": 3.3833565711975098,
        "learning_rate": 6.568739344636732e-05,
        "epoch": 1.0764,
        "step": 8073
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.673060417175293,
        "learning_rate": 6.56282844284735e-05,
        "epoch": 1.0765333333333333,
        "step": 8074
    },
    {
        "loss": 2.2749,
        "grad_norm": 3.745548963546753,
        "learning_rate": 6.556918902533337e-05,
        "epoch": 1.0766666666666667,
        "step": 8075
    },
    {
        "loss": 1.3199,
        "grad_norm": 3.8170299530029297,
        "learning_rate": 6.551010726035469e-05,
        "epoch": 1.0768,
        "step": 8076
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.6940627098083496,
        "learning_rate": 6.545103915694007e-05,
        "epoch": 1.0769333333333333,
        "step": 8077
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.459496021270752,
        "learning_rate": 6.539198473848655e-05,
        "epoch": 1.0770666666666666,
        "step": 8078
    },
    {
        "loss": 1.4426,
        "grad_norm": 3.805633306503296,
        "learning_rate": 6.533294402838593e-05,
        "epoch": 1.0772,
        "step": 8079
    },
    {
        "loss": 2.1136,
        "grad_norm": 3.974229335784912,
        "learning_rate": 6.52739170500241e-05,
        "epoch": 1.0773333333333333,
        "step": 8080
    },
    {
        "loss": 1.9623,
        "grad_norm": 3.544053554534912,
        "learning_rate": 6.52149038267822e-05,
        "epoch": 1.0774666666666666,
        "step": 8081
    },
    {
        "loss": 2.4564,
        "grad_norm": 3.411724805831909,
        "learning_rate": 6.515590438203529e-05,
        "epoch": 1.0776,
        "step": 8082
    },
    {
        "loss": 2.0049,
        "grad_norm": 3.2875354290008545,
        "learning_rate": 6.509691873915358e-05,
        "epoch": 1.0777333333333334,
        "step": 8083
    },
    {
        "loss": 2.3775,
        "grad_norm": 3.0653762817382812,
        "learning_rate": 6.503794692150114e-05,
        "epoch": 1.0778666666666668,
        "step": 8084
    },
    {
        "loss": 1.9885,
        "grad_norm": 4.138088226318359,
        "learning_rate": 6.497898895243709e-05,
        "epoch": 1.078,
        "step": 8085
    },
    {
        "loss": 2.1067,
        "grad_norm": 3.8676540851593018,
        "learning_rate": 6.492004485531482e-05,
        "epoch": 1.0781333333333334,
        "step": 8086
    },
    {
        "loss": 1.5177,
        "grad_norm": 4.571548938751221,
        "learning_rate": 6.486111465348236e-05,
        "epoch": 1.0782666666666667,
        "step": 8087
    },
    {
        "loss": 1.1898,
        "grad_norm": 3.89079213142395,
        "learning_rate": 6.480219837028214e-05,
        "epoch": 1.0784,
        "step": 8088
    },
    {
        "loss": 2.3392,
        "grad_norm": 3.8640286922454834,
        "learning_rate": 6.474329602905096e-05,
        "epoch": 1.0785333333333333,
        "step": 8089
    },
    {
        "loss": 2.113,
        "grad_norm": 2.8593978881835938,
        "learning_rate": 6.468440765312048e-05,
        "epoch": 1.0786666666666667,
        "step": 8090
    },
    {
        "loss": 1.9488,
        "grad_norm": 3.3209826946258545,
        "learning_rate": 6.462553326581632e-05,
        "epoch": 1.0788,
        "step": 8091
    },
    {
        "loss": 2.5416,
        "grad_norm": 2.4802780151367188,
        "learning_rate": 6.456667289045918e-05,
        "epoch": 1.0789333333333333,
        "step": 8092
    },
    {
        "loss": 2.4162,
        "grad_norm": 3.0254368782043457,
        "learning_rate": 6.450782655036351e-05,
        "epoch": 1.0790666666666666,
        "step": 8093
    },
    {
        "loss": 2.1557,
        "grad_norm": 3.153860330581665,
        "learning_rate": 6.444899426883866e-05,
        "epoch": 1.0792,
        "step": 8094
    },
    {
        "loss": 1.6614,
        "grad_norm": 2.573662519454956,
        "learning_rate": 6.439017606918836e-05,
        "epoch": 1.0793333333333333,
        "step": 8095
    },
    {
        "loss": 2.4996,
        "grad_norm": 3.641590118408203,
        "learning_rate": 6.43313719747107e-05,
        "epoch": 1.0794666666666666,
        "step": 8096
    },
    {
        "loss": 1.5394,
        "grad_norm": 3.8709635734558105,
        "learning_rate": 6.427258200869814e-05,
        "epoch": 1.0796000000000001,
        "step": 8097
    },
    {
        "loss": 1.9635,
        "grad_norm": 3.537947177886963,
        "learning_rate": 6.421380619443744e-05,
        "epoch": 1.0797333333333334,
        "step": 8098
    },
    {
        "loss": 1.6042,
        "grad_norm": 3.757901906967163,
        "learning_rate": 6.415504455521019e-05,
        "epoch": 1.0798666666666668,
        "step": 8099
    },
    {
        "loss": 1.6708,
        "grad_norm": 3.8450160026550293,
        "learning_rate": 6.409629711429193e-05,
        "epoch": 1.08,
        "step": 8100
    },
    {
        "loss": 1.7046,
        "grad_norm": 3.3207921981811523,
        "learning_rate": 6.403756389495266e-05,
        "epoch": 1.0801333333333334,
        "step": 8101
    },
    {
        "loss": 1.2923,
        "grad_norm": 4.723984718322754,
        "learning_rate": 6.397884492045684e-05,
        "epoch": 1.0802666666666667,
        "step": 8102
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.583611488342285,
        "learning_rate": 6.392014021406331e-05,
        "epoch": 1.0804,
        "step": 8103
    },
    {
        "loss": 2.0353,
        "grad_norm": 3.751631498336792,
        "learning_rate": 6.386144979902531e-05,
        "epoch": 1.0805333333333333,
        "step": 8104
    },
    {
        "loss": 2.4906,
        "grad_norm": 2.447305679321289,
        "learning_rate": 6.380277369858997e-05,
        "epoch": 1.0806666666666667,
        "step": 8105
    },
    {
        "loss": 2.1016,
        "grad_norm": 3.404583692550659,
        "learning_rate": 6.374411193599947e-05,
        "epoch": 1.0808,
        "step": 8106
    },
    {
        "loss": 2.0996,
        "grad_norm": 2.452340602874756,
        "learning_rate": 6.368546453448965e-05,
        "epoch": 1.0809333333333333,
        "step": 8107
    },
    {
        "loss": 2.32,
        "grad_norm": 5.379379749298096,
        "learning_rate": 6.362683151729122e-05,
        "epoch": 1.0810666666666666,
        "step": 8108
    },
    {
        "loss": 1.5493,
        "grad_norm": 3.364889144897461,
        "learning_rate": 6.356821290762865e-05,
        "epoch": 1.0812,
        "step": 8109
    },
    {
        "loss": 2.0895,
        "grad_norm": 4.864697456359863,
        "learning_rate": 6.35096087287211e-05,
        "epoch": 1.0813333333333333,
        "step": 8110
    },
    {
        "loss": 1.199,
        "grad_norm": 3.4021072387695312,
        "learning_rate": 6.345101900378185e-05,
        "epoch": 1.0814666666666666,
        "step": 8111
    },
    {
        "loss": 2.0525,
        "grad_norm": 4.01030969619751,
        "learning_rate": 6.339244375601852e-05,
        "epoch": 1.0816,
        "step": 8112
    },
    {
        "loss": 1.721,
        "grad_norm": 3.2188594341278076,
        "learning_rate": 6.333388300863308e-05,
        "epoch": 1.0817333333333334,
        "step": 8113
    },
    {
        "loss": 2.1327,
        "grad_norm": 4.093344688415527,
        "learning_rate": 6.327533678482134e-05,
        "epoch": 1.0818666666666668,
        "step": 8114
    },
    {
        "loss": 1.2405,
        "grad_norm": 2.77085280418396,
        "learning_rate": 6.321680510777395e-05,
        "epoch": 1.082,
        "step": 8115
    },
    {
        "loss": 1.5914,
        "grad_norm": 3.9845688343048096,
        "learning_rate": 6.315828800067525e-05,
        "epoch": 1.0821333333333334,
        "step": 8116
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.9946765899658203,
        "learning_rate": 6.309978548670442e-05,
        "epoch": 1.0822666666666667,
        "step": 8117
    },
    {
        "loss": 2.2952,
        "grad_norm": 5.316367149353027,
        "learning_rate": 6.304129758903417e-05,
        "epoch": 1.0824,
        "step": 8118
    },
    {
        "loss": 1.5525,
        "grad_norm": 2.3828046321868896,
        "learning_rate": 6.298282433083184e-05,
        "epoch": 1.0825333333333333,
        "step": 8119
    },
    {
        "loss": 3.1233,
        "grad_norm": 4.044369697570801,
        "learning_rate": 6.292436573525897e-05,
        "epoch": 1.0826666666666667,
        "step": 8120
    },
    {
        "loss": 2.3873,
        "grad_norm": 3.4776315689086914,
        "learning_rate": 6.286592182547122e-05,
        "epoch": 1.0828,
        "step": 8121
    },
    {
        "loss": 0.8996,
        "grad_norm": 3.95285964012146,
        "learning_rate": 6.280749262461837e-05,
        "epoch": 1.0829333333333333,
        "step": 8122
    },
    {
        "loss": 1.3138,
        "grad_norm": 4.158994197845459,
        "learning_rate": 6.274907815584427e-05,
        "epoch": 1.0830666666666666,
        "step": 8123
    },
    {
        "loss": 2.0337,
        "grad_norm": 4.411362171173096,
        "learning_rate": 6.269067844228739e-05,
        "epoch": 1.0832,
        "step": 8124
    },
    {
        "loss": 2.1072,
        "grad_norm": 3.8043975830078125,
        "learning_rate": 6.26322935070799e-05,
        "epoch": 1.0833333333333333,
        "step": 8125
    },
    {
        "loss": 2.2875,
        "grad_norm": 2.5141751766204834,
        "learning_rate": 6.257392337334821e-05,
        "epoch": 1.0834666666666666,
        "step": 8126
    },
    {
        "loss": 0.7032,
        "grad_norm": 4.198105812072754,
        "learning_rate": 6.2515568064213e-05,
        "epoch": 1.0836,
        "step": 8127
    },
    {
        "loss": 2.4625,
        "grad_norm": 3.4374279975891113,
        "learning_rate": 6.245722760278902e-05,
        "epoch": 1.0837333333333334,
        "step": 8128
    },
    {
        "loss": 2.3345,
        "grad_norm": 2.013436794281006,
        "learning_rate": 6.239890201218524e-05,
        "epoch": 1.0838666666666668,
        "step": 8129
    },
    {
        "loss": 1.2265,
        "grad_norm": 4.681577205657959,
        "learning_rate": 6.234059131550427e-05,
        "epoch": 1.084,
        "step": 8130
    },
    {
        "loss": 1.681,
        "grad_norm": 3.2448976039886475,
        "learning_rate": 6.228229553584354e-05,
        "epoch": 1.0841333333333334,
        "step": 8131
    },
    {
        "loss": 1.4722,
        "grad_norm": 1.783278465270996,
        "learning_rate": 6.222401469629398e-05,
        "epoch": 1.0842666666666667,
        "step": 8132
    },
    {
        "loss": 1.2427,
        "grad_norm": 2.8537425994873047,
        "learning_rate": 6.2165748819941e-05,
        "epoch": 1.0844,
        "step": 8133
    },
    {
        "loss": 1.7598,
        "grad_norm": 3.2345404624938965,
        "learning_rate": 6.210749792986384e-05,
        "epoch": 1.0845333333333333,
        "step": 8134
    },
    {
        "loss": 2.2038,
        "grad_norm": 3.36680006980896,
        "learning_rate": 6.204926204913581e-05,
        "epoch": 1.0846666666666667,
        "step": 8135
    },
    {
        "loss": 3.9741,
        "grad_norm": 2.9403443336486816,
        "learning_rate": 6.199104120082439e-05,
        "epoch": 1.0848,
        "step": 8136
    },
    {
        "loss": 1.616,
        "grad_norm": 4.186427593231201,
        "learning_rate": 6.193283540799106e-05,
        "epoch": 1.0849333333333333,
        "step": 8137
    },
    {
        "loss": 1.9865,
        "grad_norm": 3.7374794483184814,
        "learning_rate": 6.187464469369147e-05,
        "epoch": 1.0850666666666666,
        "step": 8138
    },
    {
        "loss": 2.6187,
        "grad_norm": 2.831298828125,
        "learning_rate": 6.181646908097485e-05,
        "epoch": 1.0852,
        "step": 8139
    },
    {
        "loss": 1.1137,
        "grad_norm": 3.980903148651123,
        "learning_rate": 6.175830859288504e-05,
        "epoch": 1.0853333333333333,
        "step": 8140
    },
    {
        "loss": 2.0249,
        "grad_norm": 2.4387919902801514,
        "learning_rate": 6.170016325245937e-05,
        "epoch": 1.0854666666666666,
        "step": 8141
    },
    {
        "loss": 1.7807,
        "grad_norm": 2.983255386352539,
        "learning_rate": 6.164203308272973e-05,
        "epoch": 1.0856,
        "step": 8142
    },
    {
        "loss": 2.3015,
        "grad_norm": 3.1021130084991455,
        "learning_rate": 6.158391810672138e-05,
        "epoch": 1.0857333333333332,
        "step": 8143
    },
    {
        "loss": 2.7758,
        "grad_norm": 4.264956951141357,
        "learning_rate": 6.152581834745396e-05,
        "epoch": 1.0858666666666668,
        "step": 8144
    },
    {
        "loss": 1.9363,
        "grad_norm": 3.757966995239258,
        "learning_rate": 6.146773382794097e-05,
        "epoch": 1.086,
        "step": 8145
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.995931625366211,
        "learning_rate": 6.140966457119e-05,
        "epoch": 1.0861333333333334,
        "step": 8146
    },
    {
        "loss": 2.8291,
        "grad_norm": 3.0430359840393066,
        "learning_rate": 6.135161060020237e-05,
        "epoch": 1.0862666666666667,
        "step": 8147
    },
    {
        "loss": 2.367,
        "grad_norm": 4.696852684020996,
        "learning_rate": 6.129357193797335e-05,
        "epoch": 1.0864,
        "step": 8148
    },
    {
        "loss": 2.2123,
        "grad_norm": 4.267204284667969,
        "learning_rate": 6.12355486074925e-05,
        "epoch": 1.0865333333333334,
        "step": 8149
    },
    {
        "loss": 2.354,
        "grad_norm": 4.407780170440674,
        "learning_rate": 6.117754063174295e-05,
        "epoch": 1.0866666666666667,
        "step": 8150
    },
    {
        "loss": 1.9303,
        "grad_norm": 3.195158004760742,
        "learning_rate": 6.111954803370176e-05,
        "epoch": 1.0868,
        "step": 8151
    },
    {
        "loss": 2.0497,
        "grad_norm": 2.6625478267669678,
        "learning_rate": 6.106157083634006e-05,
        "epoch": 1.0869333333333333,
        "step": 8152
    },
    {
        "loss": 1.5755,
        "grad_norm": 3.3621277809143066,
        "learning_rate": 6.100360906262286e-05,
        "epoch": 1.0870666666666666,
        "step": 8153
    },
    {
        "loss": 1.7968,
        "grad_norm": 3.5443735122680664,
        "learning_rate": 6.094566273550897e-05,
        "epoch": 1.0872,
        "step": 8154
    },
    {
        "loss": 2.011,
        "grad_norm": 4.9113688468933105,
        "learning_rate": 6.088773187795122e-05,
        "epoch": 1.0873333333333333,
        "step": 8155
    },
    {
        "loss": 2.2687,
        "grad_norm": 2.8479859828948975,
        "learning_rate": 6.082981651289612e-05,
        "epoch": 1.0874666666666666,
        "step": 8156
    },
    {
        "loss": 2.1918,
        "grad_norm": 3.340808391571045,
        "learning_rate": 6.077191666328407e-05,
        "epoch": 1.0876,
        "step": 8157
    },
    {
        "loss": 3.0631,
        "grad_norm": 2.8549416065216064,
        "learning_rate": 6.071403235204962e-05,
        "epoch": 1.0877333333333334,
        "step": 8158
    },
    {
        "loss": 1.9962,
        "grad_norm": 3.318528413772583,
        "learning_rate": 6.0656163602120786e-05,
        "epoch": 1.0878666666666668,
        "step": 8159
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.7942261695861816,
        "learning_rate": 6.0598310436419584e-05,
        "epoch": 1.088,
        "step": 8160
    },
    {
        "loss": 1.3407,
        "grad_norm": 2.588416337966919,
        "learning_rate": 6.054047287786187e-05,
        "epoch": 1.0881333333333334,
        "step": 8161
    },
    {
        "loss": 2.0226,
        "grad_norm": 2.9760985374450684,
        "learning_rate": 6.048265094935728e-05,
        "epoch": 1.0882666666666667,
        "step": 8162
    },
    {
        "loss": 2.0728,
        "grad_norm": 2.587087392807007,
        "learning_rate": 6.042484467380944e-05,
        "epoch": 1.0884,
        "step": 8163
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.2816219329833984,
        "learning_rate": 6.03670540741153e-05,
        "epoch": 1.0885333333333334,
        "step": 8164
    },
    {
        "loss": 1.9526,
        "grad_norm": 3.0734565258026123,
        "learning_rate": 6.0309279173166214e-05,
        "epoch": 1.0886666666666667,
        "step": 8165
    },
    {
        "loss": 2.372,
        "grad_norm": 3.513270139694214,
        "learning_rate": 6.025151999384674e-05,
        "epoch": 1.0888,
        "step": 8166
    },
    {
        "loss": 1.7087,
        "grad_norm": 3.026827812194824,
        "learning_rate": 6.0193776559035844e-05,
        "epoch": 1.0889333333333333,
        "step": 8167
    },
    {
        "loss": 1.988,
        "grad_norm": 4.520153522491455,
        "learning_rate": 6.0136048891605555e-05,
        "epoch": 1.0890666666666666,
        "step": 8168
    },
    {
        "loss": 1.6599,
        "grad_norm": 2.690584659576416,
        "learning_rate": 6.007833701442215e-05,
        "epoch": 1.0892,
        "step": 8169
    },
    {
        "loss": 2.4278,
        "grad_norm": 3.698438882827759,
        "learning_rate": 6.002064095034545e-05,
        "epoch": 1.0893333333333333,
        "step": 8170
    },
    {
        "loss": 2.2405,
        "grad_norm": 3.5017266273498535,
        "learning_rate": 5.996296072222918e-05,
        "epoch": 1.0894666666666666,
        "step": 8171
    },
    {
        "loss": 1.033,
        "grad_norm": 4.104023456573486,
        "learning_rate": 5.9905296352920594e-05,
        "epoch": 1.0896,
        "step": 8172
    },
    {
        "loss": 2.1019,
        "grad_norm": 4.660562038421631,
        "learning_rate": 5.98476478652606e-05,
        "epoch": 1.0897333333333332,
        "step": 8173
    },
    {
        "loss": 2.0199,
        "grad_norm": 4.51046895980835,
        "learning_rate": 5.979001528208424e-05,
        "epoch": 1.0898666666666668,
        "step": 8174
    },
    {
        "loss": 2.0076,
        "grad_norm": 3.2158732414245605,
        "learning_rate": 5.973239862621973e-05,
        "epoch": 1.09,
        "step": 8175
    },
    {
        "loss": 2.0104,
        "grad_norm": 3.6106367111206055,
        "learning_rate": 5.96747979204895e-05,
        "epoch": 1.0901333333333334,
        "step": 8176
    },
    {
        "loss": 1.5432,
        "grad_norm": 3.862337112426758,
        "learning_rate": 5.9617213187709086e-05,
        "epoch": 1.0902666666666667,
        "step": 8177
    },
    {
        "loss": 1.9501,
        "grad_norm": 3.4973387718200684,
        "learning_rate": 5.9559644450688144e-05,
        "epoch": 1.0904,
        "step": 8178
    },
    {
        "loss": 1.6938,
        "grad_norm": 3.929658889770508,
        "learning_rate": 5.950209173222984e-05,
        "epoch": 1.0905333333333334,
        "step": 8179
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.401726722717285,
        "learning_rate": 5.9444555055131044e-05,
        "epoch": 1.0906666666666667,
        "step": 8180
    },
    {
        "loss": 1.4696,
        "grad_norm": 4.443095684051514,
        "learning_rate": 5.9387034442182165e-05,
        "epoch": 1.0908,
        "step": 8181
    },
    {
        "loss": 2.6762,
        "grad_norm": 3.8173506259918213,
        "learning_rate": 5.932952991616723e-05,
        "epoch": 1.0909333333333333,
        "step": 8182
    },
    {
        "loss": 2.2793,
        "grad_norm": 3.5288407802581787,
        "learning_rate": 5.9272041499864185e-05,
        "epoch": 1.0910666666666666,
        "step": 8183
    },
    {
        "loss": 2.1638,
        "grad_norm": 3.040369749069214,
        "learning_rate": 5.9214569216044294e-05,
        "epoch": 1.0912,
        "step": 8184
    },
    {
        "loss": 2.6332,
        "grad_norm": 3.520273447036743,
        "learning_rate": 5.915711308747243e-05,
        "epoch": 1.0913333333333333,
        "step": 8185
    },
    {
        "loss": 1.8698,
        "grad_norm": 2.9589433670043945,
        "learning_rate": 5.9099673136907255e-05,
        "epoch": 1.0914666666666666,
        "step": 8186
    },
    {
        "loss": 2.889,
        "grad_norm": 2.8592801094055176,
        "learning_rate": 5.9042249387100904e-05,
        "epoch": 1.0916,
        "step": 8187
    },
    {
        "loss": 2.1502,
        "grad_norm": 3.550739288330078,
        "learning_rate": 5.898484186079927e-05,
        "epoch": 1.0917333333333334,
        "step": 8188
    },
    {
        "loss": 2.7637,
        "grad_norm": 2.751898765563965,
        "learning_rate": 5.892745058074132e-05,
        "epoch": 1.0918666666666668,
        "step": 8189
    },
    {
        "loss": 2.5337,
        "grad_norm": 3.8771541118621826,
        "learning_rate": 5.887007556966029e-05,
        "epoch": 1.092,
        "step": 8190
    },
    {
        "loss": 2.4594,
        "grad_norm": 2.804765224456787,
        "learning_rate": 5.881271685028233e-05,
        "epoch": 1.0921333333333334,
        "step": 8191
    },
    {
        "loss": 2.0161,
        "grad_norm": 3.2054545879364014,
        "learning_rate": 5.8755374445327706e-05,
        "epoch": 1.0922666666666667,
        "step": 8192
    },
    {
        "loss": 1.4556,
        "grad_norm": 4.406190872192383,
        "learning_rate": 5.869804837750981e-05,
        "epoch": 1.0924,
        "step": 8193
    },
    {
        "loss": 2.5663,
        "grad_norm": 3.703054904937744,
        "learning_rate": 5.8640738669535614e-05,
        "epoch": 1.0925333333333334,
        "step": 8194
    },
    {
        "loss": 1.4756,
        "grad_norm": 5.445284843444824,
        "learning_rate": 5.858344534410575e-05,
        "epoch": 1.0926666666666667,
        "step": 8195
    },
    {
        "loss": 2.1089,
        "grad_norm": 2.784550666809082,
        "learning_rate": 5.8526168423914316e-05,
        "epoch": 1.0928,
        "step": 8196
    },
    {
        "loss": 2.3037,
        "grad_norm": 1.928868055343628,
        "learning_rate": 5.846890793164903e-05,
        "epoch": 1.0929333333333333,
        "step": 8197
    },
    {
        "loss": 0.7575,
        "grad_norm": 5.734272003173828,
        "learning_rate": 5.841166388999062e-05,
        "epoch": 1.0930666666666666,
        "step": 8198
    },
    {
        "loss": 1.4377,
        "grad_norm": 3.207737922668457,
        "learning_rate": 5.8354436321613995e-05,
        "epoch": 1.0932,
        "step": 8199
    },
    {
        "loss": 2.5688,
        "grad_norm": 2.331615924835205,
        "learning_rate": 5.8297225249186884e-05,
        "epoch": 1.0933333333333333,
        "step": 8200
    },
    {
        "loss": 2.1995,
        "grad_norm": 3.15548038482666,
        "learning_rate": 5.824003069537116e-05,
        "epoch": 1.0934666666666666,
        "step": 8201
    },
    {
        "loss": 1.5789,
        "grad_norm": 3.76391863822937,
        "learning_rate": 5.8182852682821465e-05,
        "epoch": 1.0936,
        "step": 8202
    },
    {
        "loss": 2.116,
        "grad_norm": 3.875129461288452,
        "learning_rate": 5.8125691234186294e-05,
        "epoch": 1.0937333333333332,
        "step": 8203
    },
    {
        "loss": 1.5227,
        "grad_norm": 4.482499122619629,
        "learning_rate": 5.8068546372107476e-05,
        "epoch": 1.0938666666666668,
        "step": 8204
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.4300971031188965,
        "learning_rate": 5.801141811922044e-05,
        "epoch": 1.094,
        "step": 8205
    },
    {
        "loss": 1.7671,
        "grad_norm": 3.4380834102630615,
        "learning_rate": 5.795430649815371e-05,
        "epoch": 1.0941333333333334,
        "step": 8206
    },
    {
        "loss": 0.6793,
        "grad_norm": 2.8554890155792236,
        "learning_rate": 5.789721153152936e-05,
        "epoch": 1.0942666666666667,
        "step": 8207
    },
    {
        "loss": 2.4187,
        "grad_norm": 4.078381061553955,
        "learning_rate": 5.784013324196308e-05,
        "epoch": 1.0944,
        "step": 8208
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.8045918941497803,
        "learning_rate": 5.778307165206367e-05,
        "epoch": 1.0945333333333334,
        "step": 8209
    },
    {
        "loss": 1.8322,
        "grad_norm": 2.523552179336548,
        "learning_rate": 5.77260267844334e-05,
        "epoch": 1.0946666666666667,
        "step": 8210
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.4135050773620605,
        "learning_rate": 5.766899866166795e-05,
        "epoch": 1.0948,
        "step": 8211
    },
    {
        "loss": 2.5155,
        "grad_norm": 3.3382694721221924,
        "learning_rate": 5.7611987306356375e-05,
        "epoch": 1.0949333333333333,
        "step": 8212
    },
    {
        "loss": 2.9058,
        "grad_norm": 5.788570404052734,
        "learning_rate": 5.755499274108107e-05,
        "epoch": 1.0950666666666666,
        "step": 8213
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.406414270401001,
        "learning_rate": 5.749801498841779e-05,
        "epoch": 1.0952,
        "step": 8214
    },
    {
        "loss": 2.4365,
        "grad_norm": 2.8902297019958496,
        "learning_rate": 5.744105407093561e-05,
        "epoch": 1.0953333333333333,
        "step": 8215
    },
    {
        "loss": 2.4671,
        "grad_norm": 3.3967297077178955,
        "learning_rate": 5.7384110011196846e-05,
        "epoch": 1.0954666666666666,
        "step": 8216
    },
    {
        "loss": 2.1122,
        "grad_norm": 5.8565592765808105,
        "learning_rate": 5.73271828317574e-05,
        "epoch": 1.0956,
        "step": 8217
    },
    {
        "loss": 1.917,
        "grad_norm": 1.9196445941925049,
        "learning_rate": 5.727027255516626e-05,
        "epoch": 1.0957333333333334,
        "step": 8218
    },
    {
        "loss": 0.9608,
        "grad_norm": 3.641455888748169,
        "learning_rate": 5.7213379203965655e-05,
        "epoch": 1.0958666666666668,
        "step": 8219
    },
    {
        "loss": 0.872,
        "grad_norm": 3.461763620376587,
        "learning_rate": 5.715650280069138e-05,
        "epoch": 1.096,
        "step": 8220
    },
    {
        "loss": 1.4055,
        "grad_norm": 4.12291145324707,
        "learning_rate": 5.709964336787233e-05,
        "epoch": 1.0961333333333334,
        "step": 8221
    },
    {
        "loss": 1.5027,
        "grad_norm": 5.426497459411621,
        "learning_rate": 5.7042800928030834e-05,
        "epoch": 1.0962666666666667,
        "step": 8222
    },
    {
        "loss": 1.6061,
        "grad_norm": 3.868223190307617,
        "learning_rate": 5.6985975503682075e-05,
        "epoch": 1.0964,
        "step": 8223
    },
    {
        "loss": 2.5375,
        "grad_norm": 1.3878817558288574,
        "learning_rate": 5.692916711733514e-05,
        "epoch": 1.0965333333333334,
        "step": 8224
    },
    {
        "loss": 2.9367,
        "grad_norm": 3.2738125324249268,
        "learning_rate": 5.687237579149171e-05,
        "epoch": 1.0966666666666667,
        "step": 8225
    },
    {
        "loss": 2.3544,
        "grad_norm": 4.21290397644043,
        "learning_rate": 5.681560154864738e-05,
        "epoch": 1.0968,
        "step": 8226
    },
    {
        "loss": 2.5759,
        "grad_norm": 3.14267635345459,
        "learning_rate": 5.67588444112903e-05,
        "epoch": 1.0969333333333333,
        "step": 8227
    },
    {
        "loss": 2.4357,
        "grad_norm": 2.4992854595184326,
        "learning_rate": 5.670210440190229e-05,
        "epoch": 1.0970666666666666,
        "step": 8228
    },
    {
        "loss": 1.7648,
        "grad_norm": 1.870011329650879,
        "learning_rate": 5.664538154295823e-05,
        "epoch": 1.0972,
        "step": 8229
    },
    {
        "loss": 2.5535,
        "grad_norm": 2.884152889251709,
        "learning_rate": 5.658867585692639e-05,
        "epoch": 1.0973333333333333,
        "step": 8230
    },
    {
        "loss": 0.8392,
        "grad_norm": 2.485391139984131,
        "learning_rate": 5.6531987366267913e-05,
        "epoch": 1.0974666666666666,
        "step": 8231
    },
    {
        "loss": 2.8664,
        "grad_norm": 4.3945393562316895,
        "learning_rate": 5.6475316093437283e-05,
        "epoch": 1.0976,
        "step": 8232
    },
    {
        "loss": 2.4576,
        "grad_norm": 3.346036911010742,
        "learning_rate": 5.641866206088232e-05,
        "epoch": 1.0977333333333332,
        "step": 8233
    },
    {
        "loss": 2.6912,
        "grad_norm": 2.8691794872283936,
        "learning_rate": 5.636202529104371e-05,
        "epoch": 1.0978666666666668,
        "step": 8234
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.640798330307007,
        "learning_rate": 5.6305405806355774e-05,
        "epoch": 1.098,
        "step": 8235
    },
    {
        "loss": 2.907,
        "grad_norm": 4.546786308288574,
        "learning_rate": 5.624880362924536e-05,
        "epoch": 1.0981333333333334,
        "step": 8236
    },
    {
        "loss": 1.5324,
        "grad_norm": 3.2118256092071533,
        "learning_rate": 5.61922187821329e-05,
        "epoch": 1.0982666666666667,
        "step": 8237
    },
    {
        "loss": 2.3528,
        "grad_norm": 3.2477619647979736,
        "learning_rate": 5.613565128743185e-05,
        "epoch": 1.0984,
        "step": 8238
    },
    {
        "loss": 1.903,
        "grad_norm": 2.364417552947998,
        "learning_rate": 5.607910116754884e-05,
        "epoch": 1.0985333333333334,
        "step": 8239
    },
    {
        "loss": 1.3274,
        "grad_norm": 4.009993076324463,
        "learning_rate": 5.602256844488356e-05,
        "epoch": 1.0986666666666667,
        "step": 8240
    },
    {
        "loss": 2.6235,
        "grad_norm": 1.981166124343872,
        "learning_rate": 5.596605314182859e-05,
        "epoch": 1.0988,
        "step": 8241
    },
    {
        "loss": 0.7893,
        "grad_norm": 3.07010817527771,
        "learning_rate": 5.590955528077015e-05,
        "epoch": 1.0989333333333333,
        "step": 8242
    },
    {
        "loss": 2.359,
        "grad_norm": 2.1982080936431885,
        "learning_rate": 5.585307488408713e-05,
        "epoch": 1.0990666666666666,
        "step": 8243
    },
    {
        "loss": 1.6605,
        "grad_norm": 3.1045713424682617,
        "learning_rate": 5.579661197415145e-05,
        "epoch": 1.0992,
        "step": 8244
    },
    {
        "loss": 1.0589,
        "grad_norm": 5.533361434936523,
        "learning_rate": 5.574016657332843e-05,
        "epoch": 1.0993333333333333,
        "step": 8245
    },
    {
        "loss": 1.6625,
        "grad_norm": 2.563417673110962,
        "learning_rate": 5.568373870397625e-05,
        "epoch": 1.0994666666666666,
        "step": 8246
    },
    {
        "loss": 2.2619,
        "grad_norm": 2.1425020694732666,
        "learning_rate": 5.5627328388446286e-05,
        "epoch": 1.0996,
        "step": 8247
    },
    {
        "loss": 1.6758,
        "grad_norm": 4.075693607330322,
        "learning_rate": 5.557093564908254e-05,
        "epoch": 1.0997333333333332,
        "step": 8248
    },
    {
        "loss": 1.8308,
        "grad_norm": 2.201253890991211,
        "learning_rate": 5.5514560508222746e-05,
        "epoch": 1.0998666666666668,
        "step": 8249
    },
    {
        "loss": 1.8955,
        "grad_norm": 3.5566599369049072,
        "learning_rate": 5.545820298819695e-05,
        "epoch": 1.1,
        "step": 8250
    },
    {
        "loss": 2.0376,
        "grad_norm": 3.4666178226470947,
        "learning_rate": 5.540186311132891e-05,
        "epoch": 1.1001333333333334,
        "step": 8251
    },
    {
        "loss": 2.3549,
        "grad_norm": 2.784656524658203,
        "learning_rate": 5.5345540899934746e-05,
        "epoch": 1.1002666666666667,
        "step": 8252
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.133765697479248,
        "learning_rate": 5.528923637632398e-05,
        "epoch": 1.1004,
        "step": 8253
    },
    {
        "loss": 2.0302,
        "grad_norm": 2.995448350906372,
        "learning_rate": 5.523294956279905e-05,
        "epoch": 1.1005333333333334,
        "step": 8254
    },
    {
        "loss": 1.4389,
        "grad_norm": 3.13132905960083,
        "learning_rate": 5.5176680481655285e-05,
        "epoch": 1.1006666666666667,
        "step": 8255
    },
    {
        "loss": 2.1339,
        "grad_norm": 3.947063446044922,
        "learning_rate": 5.5120429155181275e-05,
        "epoch": 1.1008,
        "step": 8256
    },
    {
        "loss": 1.5722,
        "grad_norm": 4.01631498336792,
        "learning_rate": 5.5064195605658034e-05,
        "epoch": 1.1009333333333333,
        "step": 8257
    },
    {
        "loss": 2.0033,
        "grad_norm": 4.671634197235107,
        "learning_rate": 5.500797985536012e-05,
        "epoch": 1.1010666666666666,
        "step": 8258
    },
    {
        "loss": 2.481,
        "grad_norm": 4.468249320983887,
        "learning_rate": 5.495178192655457e-05,
        "epoch": 1.1012,
        "step": 8259
    },
    {
        "loss": 2.2322,
        "grad_norm": 4.078102111816406,
        "learning_rate": 5.489560184150189e-05,
        "epoch": 1.1013333333333333,
        "step": 8260
    },
    {
        "loss": 2.3294,
        "grad_norm": 4.121151447296143,
        "learning_rate": 5.48394396224549e-05,
        "epoch": 1.1014666666666666,
        "step": 8261
    },
    {
        "loss": 1.4336,
        "grad_norm": 4.360627174377441,
        "learning_rate": 5.478329529165975e-05,
        "epoch": 1.1016,
        "step": 8262
    },
    {
        "loss": 2.0702,
        "grad_norm": 4.238863468170166,
        "learning_rate": 5.47271688713554e-05,
        "epoch": 1.1017333333333332,
        "step": 8263
    },
    {
        "loss": 1.6749,
        "grad_norm": 4.165792465209961,
        "learning_rate": 5.467106038377378e-05,
        "epoch": 1.1018666666666665,
        "step": 8264
    },
    {
        "loss": 2.2243,
        "grad_norm": 3.666212320327759,
        "learning_rate": 5.461496985113963e-05,
        "epoch": 1.102,
        "step": 8265
    },
    {
        "loss": 1.5625,
        "grad_norm": 3.5314629077911377,
        "learning_rate": 5.4558897295670405e-05,
        "epoch": 1.1021333333333334,
        "step": 8266
    },
    {
        "loss": 1.5055,
        "grad_norm": 5.029800891876221,
        "learning_rate": 5.450284273957691e-05,
        "epoch": 1.1022666666666667,
        "step": 8267
    },
    {
        "loss": 1.9572,
        "grad_norm": 3.7560205459594727,
        "learning_rate": 5.444680620506249e-05,
        "epoch": 1.1024,
        "step": 8268
    },
    {
        "loss": 1.9373,
        "grad_norm": 4.29051399230957,
        "learning_rate": 5.439078771432322e-05,
        "epoch": 1.1025333333333334,
        "step": 8269
    },
    {
        "loss": 2.1115,
        "grad_norm": 2.299809455871582,
        "learning_rate": 5.433478728954839e-05,
        "epoch": 1.1026666666666667,
        "step": 8270
    },
    {
        "loss": 1.6238,
        "grad_norm": 3.5722315311431885,
        "learning_rate": 5.42788049529199e-05,
        "epoch": 1.1028,
        "step": 8271
    },
    {
        "loss": 1.7392,
        "grad_norm": 3.440601110458374,
        "learning_rate": 5.42228407266127e-05,
        "epoch": 1.1029333333333333,
        "step": 8272
    },
    {
        "loss": 2.4024,
        "grad_norm": 1.7965155839920044,
        "learning_rate": 5.416689463279407e-05,
        "epoch": 1.1030666666666666,
        "step": 8273
    },
    {
        "loss": 2.4804,
        "grad_norm": 3.4048471450805664,
        "learning_rate": 5.4110966693624795e-05,
        "epoch": 1.1032,
        "step": 8274
    },
    {
        "loss": 2.0724,
        "grad_norm": 3.432720184326172,
        "learning_rate": 5.405505693125783e-05,
        "epoch": 1.1033333333333333,
        "step": 8275
    },
    {
        "loss": 2.388,
        "grad_norm": 4.998866558074951,
        "learning_rate": 5.3999165367839474e-05,
        "epoch": 1.1034666666666666,
        "step": 8276
    },
    {
        "loss": 2.3525,
        "grad_norm": 2.631415605545044,
        "learning_rate": 5.3943292025508506e-05,
        "epoch": 1.1036,
        "step": 8277
    },
    {
        "loss": 1.7438,
        "grad_norm": 2.6366846561431885,
        "learning_rate": 5.388743692639642e-05,
        "epoch": 1.1037333333333332,
        "step": 8278
    },
    {
        "loss": 1.9494,
        "grad_norm": 4.563628673553467,
        "learning_rate": 5.3831600092627696e-05,
        "epoch": 1.1038666666666668,
        "step": 8279
    },
    {
        "loss": 3.3637,
        "grad_norm": 2.4144785404205322,
        "learning_rate": 5.377578154631945e-05,
        "epoch": 1.104,
        "step": 8280
    },
    {
        "loss": 2.2652,
        "grad_norm": 3.4456558227539062,
        "learning_rate": 5.37199813095818e-05,
        "epoch": 1.1041333333333334,
        "step": 8281
    },
    {
        "loss": 2.213,
        "grad_norm": 3.1506423950195312,
        "learning_rate": 5.366419940451708e-05,
        "epoch": 1.1042666666666667,
        "step": 8282
    },
    {
        "loss": 2.0555,
        "grad_norm": 3.0511810779571533,
        "learning_rate": 5.360843585322095e-05,
        "epoch": 1.1044,
        "step": 8283
    },
    {
        "loss": 1.5279,
        "grad_norm": 5.200328350067139,
        "learning_rate": 5.355269067778132e-05,
        "epoch": 1.1045333333333334,
        "step": 8284
    },
    {
        "loss": 2.323,
        "grad_norm": 4.541146278381348,
        "learning_rate": 5.3496963900279395e-05,
        "epoch": 1.1046666666666667,
        "step": 8285
    },
    {
        "loss": 2.7207,
        "grad_norm": 4.179866313934326,
        "learning_rate": 5.3441255542788406e-05,
        "epoch": 1.1048,
        "step": 8286
    },
    {
        "loss": 1.3285,
        "grad_norm": 4.269518852233887,
        "learning_rate": 5.338556562737472e-05,
        "epoch": 1.1049333333333333,
        "step": 8287
    },
    {
        "loss": 1.8928,
        "grad_norm": 2.9604203701019287,
        "learning_rate": 5.332989417609734e-05,
        "epoch": 1.1050666666666666,
        "step": 8288
    },
    {
        "loss": 2.1908,
        "grad_norm": 4.855635643005371,
        "learning_rate": 5.3274241211007946e-05,
        "epoch": 1.1052,
        "step": 8289
    },
    {
        "loss": 2.8428,
        "grad_norm": 2.976943016052246,
        "learning_rate": 5.321860675415085e-05,
        "epoch": 1.1053333333333333,
        "step": 8290
    },
    {
        "loss": 1.5108,
        "grad_norm": 3.49550199508667,
        "learning_rate": 5.316299082756286e-05,
        "epoch": 1.1054666666666666,
        "step": 8291
    },
    {
        "loss": 1.522,
        "grad_norm": 3.197321891784668,
        "learning_rate": 5.3107393453273934e-05,
        "epoch": 1.1056,
        "step": 8292
    },
    {
        "loss": 2.3111,
        "grad_norm": 3.094679355621338,
        "learning_rate": 5.3051814653306156e-05,
        "epoch": 1.1057333333333332,
        "step": 8293
    },
    {
        "loss": 1.9691,
        "grad_norm": 3.8514139652252197,
        "learning_rate": 5.299625444967471e-05,
        "epoch": 1.1058666666666666,
        "step": 8294
    },
    {
        "loss": 1.0501,
        "grad_norm": 3.281491279602051,
        "learning_rate": 5.294071286438689e-05,
        "epoch": 1.106,
        "step": 8295
    },
    {
        "loss": 2.6881,
        "grad_norm": 4.073616027832031,
        "learning_rate": 5.28851899194431e-05,
        "epoch": 1.1061333333333334,
        "step": 8296
    },
    {
        "loss": 2.7095,
        "grad_norm": 4.8142242431640625,
        "learning_rate": 5.282968563683612e-05,
        "epoch": 1.1062666666666667,
        "step": 8297
    },
    {
        "loss": 1.9356,
        "grad_norm": 3.3111917972564697,
        "learning_rate": 5.2774200038551424e-05,
        "epoch": 1.1064,
        "step": 8298
    },
    {
        "loss": 2.379,
        "grad_norm": 2.545827627182007,
        "learning_rate": 5.2718733146567055e-05,
        "epoch": 1.1065333333333334,
        "step": 8299
    },
    {
        "loss": 2.2865,
        "grad_norm": 2.851392984390259,
        "learning_rate": 5.266328498285343e-05,
        "epoch": 1.1066666666666667,
        "step": 8300
    },
    {
        "loss": 2.4371,
        "grad_norm": 4.3863654136657715,
        "learning_rate": 5.260785556937409e-05,
        "epoch": 1.1068,
        "step": 8301
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.7879488468170166,
        "learning_rate": 5.255244492808458e-05,
        "epoch": 1.1069333333333333,
        "step": 8302
    },
    {
        "loss": 2.4886,
        "grad_norm": 4.114445686340332,
        "learning_rate": 5.249705308093329e-05,
        "epoch": 1.1070666666666666,
        "step": 8303
    },
    {
        "loss": 2.6457,
        "grad_norm": 2.455963134765625,
        "learning_rate": 5.244168004986112e-05,
        "epoch": 1.1072,
        "step": 8304
    },
    {
        "loss": 1.443,
        "grad_norm": 3.7697384357452393,
        "learning_rate": 5.238632585680151e-05,
        "epoch": 1.1073333333333333,
        "step": 8305
    },
    {
        "loss": 1.9566,
        "grad_norm": 3.2521777153015137,
        "learning_rate": 5.2330990523680624e-05,
        "epoch": 1.1074666666666666,
        "step": 8306
    },
    {
        "loss": 2.5151,
        "grad_norm": 3.630650520324707,
        "learning_rate": 5.227567407241665e-05,
        "epoch": 1.1076,
        "step": 8307
    },
    {
        "loss": 2.475,
        "grad_norm": 4.498743057250977,
        "learning_rate": 5.222037652492088e-05,
        "epoch": 1.1077333333333332,
        "step": 8308
    },
    {
        "loss": 1.7189,
        "grad_norm": 3.42294979095459,
        "learning_rate": 5.216509790309665e-05,
        "epoch": 1.1078666666666668,
        "step": 8309
    },
    {
        "loss": 1.4686,
        "grad_norm": 2.184443473815918,
        "learning_rate": 5.2109838228840334e-05,
        "epoch": 1.108,
        "step": 8310
    },
    {
        "loss": 1.8314,
        "grad_norm": 3.270040273666382,
        "learning_rate": 5.205459752404016e-05,
        "epoch": 1.1081333333333334,
        "step": 8311
    },
    {
        "loss": 1.648,
        "grad_norm": 2.926396608352661,
        "learning_rate": 5.199937581057728e-05,
        "epoch": 1.1082666666666667,
        "step": 8312
    },
    {
        "loss": 2.0107,
        "grad_norm": 4.656962871551514,
        "learning_rate": 5.1944173110325204e-05,
        "epoch": 1.1084,
        "step": 8313
    },
    {
        "loss": 1.0982,
        "grad_norm": 4.04237699508667,
        "learning_rate": 5.1888989445149885e-05,
        "epoch": 1.1085333333333334,
        "step": 8314
    },
    {
        "loss": 1.4202,
        "grad_norm": 2.811094045639038,
        "learning_rate": 5.183382483690992e-05,
        "epoch": 1.1086666666666667,
        "step": 8315
    },
    {
        "loss": 1.4436,
        "grad_norm": 3.721956253051758,
        "learning_rate": 5.1778679307455845e-05,
        "epoch": 1.1088,
        "step": 8316
    },
    {
        "loss": 2.1369,
        "grad_norm": 3.5400893688201904,
        "learning_rate": 5.172355287863133e-05,
        "epoch": 1.1089333333333333,
        "step": 8317
    },
    {
        "loss": 1.8318,
        "grad_norm": 4.022298336029053,
        "learning_rate": 5.166844557227195e-05,
        "epoch": 1.1090666666666666,
        "step": 8318
    },
    {
        "loss": 2.6845,
        "grad_norm": 3.5795884132385254,
        "learning_rate": 5.161335741020612e-05,
        "epoch": 1.1092,
        "step": 8319
    },
    {
        "loss": 1.6106,
        "grad_norm": 3.6275064945220947,
        "learning_rate": 5.155828841425412e-05,
        "epoch": 1.1093333333333333,
        "step": 8320
    },
    {
        "loss": 2.6525,
        "grad_norm": 4.801610946655273,
        "learning_rate": 5.150323860622919e-05,
        "epoch": 1.1094666666666666,
        "step": 8321
    },
    {
        "loss": 1.5653,
        "grad_norm": 3.3797218799591064,
        "learning_rate": 5.144820800793671e-05,
        "epoch": 1.1096,
        "step": 8322
    },
    {
        "loss": 1.8043,
        "grad_norm": 4.512572288513184,
        "learning_rate": 5.139319664117449e-05,
        "epoch": 1.1097333333333332,
        "step": 8323
    },
    {
        "loss": 1.9052,
        "grad_norm": 2.599604845046997,
        "learning_rate": 5.1338204527732745e-05,
        "epoch": 1.1098666666666666,
        "step": 8324
    },
    {
        "loss": 2.2708,
        "grad_norm": 3.1481194496154785,
        "learning_rate": 5.128323168939386e-05,
        "epoch": 1.11,
        "step": 8325
    },
    {
        "loss": 1.0471,
        "grad_norm": 5.448206424713135,
        "learning_rate": 5.122827814793305e-05,
        "epoch": 1.1101333333333334,
        "step": 8326
    },
    {
        "loss": 1.5339,
        "grad_norm": 2.5814321041107178,
        "learning_rate": 5.11733439251174e-05,
        "epoch": 1.1102666666666667,
        "step": 8327
    },
    {
        "loss": 1.7225,
        "grad_norm": 4.10035514831543,
        "learning_rate": 5.111842904270656e-05,
        "epoch": 1.1104,
        "step": 8328
    },
    {
        "loss": 2.6178,
        "grad_norm": 3.5281779766082764,
        "learning_rate": 5.1063533522452544e-05,
        "epoch": 1.1105333333333334,
        "step": 8329
    },
    {
        "loss": 1.579,
        "grad_norm": 4.0758161544799805,
        "learning_rate": 5.1008657386099614e-05,
        "epoch": 1.1106666666666667,
        "step": 8330
    },
    {
        "loss": 1.9932,
        "grad_norm": 4.935085773468018,
        "learning_rate": 5.095380065538457e-05,
        "epoch": 1.1108,
        "step": 8331
    },
    {
        "loss": 2.0618,
        "grad_norm": 3.863672971725464,
        "learning_rate": 5.089896335203604e-05,
        "epoch": 1.1109333333333333,
        "step": 8332
    },
    {
        "loss": 2.456,
        "grad_norm": 3.709669351577759,
        "learning_rate": 5.084414549777553e-05,
        "epoch": 1.1110666666666666,
        "step": 8333
    },
    {
        "loss": 2.1332,
        "grad_norm": 3.8356571197509766,
        "learning_rate": 5.0789347114316366e-05,
        "epoch": 1.1112,
        "step": 8334
    },
    {
        "loss": 2.7148,
        "grad_norm": 3.093278408050537,
        "learning_rate": 5.073456822336464e-05,
        "epoch": 1.1113333333333333,
        "step": 8335
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.3095080852508545,
        "learning_rate": 5.067980884661827e-05,
        "epoch": 1.1114666666666666,
        "step": 8336
    },
    {
        "loss": 2.0056,
        "grad_norm": 3.2574193477630615,
        "learning_rate": 5.062506900576764e-05,
        "epoch": 1.1116,
        "step": 8337
    },
    {
        "loss": 1.3511,
        "grad_norm": 2.5494558811187744,
        "learning_rate": 5.057034872249541e-05,
        "epoch": 1.1117333333333332,
        "step": 8338
    },
    {
        "loss": 0.8535,
        "grad_norm": 4.004459381103516,
        "learning_rate": 5.051564801847648e-05,
        "epoch": 1.1118666666666668,
        "step": 8339
    },
    {
        "loss": 2.0762,
        "grad_norm": 3.5344016551971436,
        "learning_rate": 5.0460966915378095e-05,
        "epoch": 1.112,
        "step": 8340
    },
    {
        "loss": 1.8712,
        "grad_norm": 3.2588348388671875,
        "learning_rate": 5.0406305434859335e-05,
        "epoch": 1.1121333333333334,
        "step": 8341
    },
    {
        "loss": 1.3835,
        "grad_norm": 3.630474090576172,
        "learning_rate": 5.035166359857209e-05,
        "epoch": 1.1122666666666667,
        "step": 8342
    },
    {
        "loss": 2.3994,
        "grad_norm": 4.642164707183838,
        "learning_rate": 5.029704142815997e-05,
        "epoch": 1.1124,
        "step": 8343
    },
    {
        "loss": 2.4033,
        "grad_norm": 3.120265483856201,
        "learning_rate": 5.0242438945259275e-05,
        "epoch": 1.1125333333333334,
        "step": 8344
    },
    {
        "loss": 1.3918,
        "grad_norm": 2.2943458557128906,
        "learning_rate": 5.0187856171497894e-05,
        "epoch": 1.1126666666666667,
        "step": 8345
    },
    {
        "loss": 2.5768,
        "grad_norm": 4.706954479217529,
        "learning_rate": 5.013329312849646e-05,
        "epoch": 1.1128,
        "step": 8346
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.4430220127105713,
        "learning_rate": 5.0078749837867535e-05,
        "epoch": 1.1129333333333333,
        "step": 8347
    },
    {
        "loss": 2.4399,
        "grad_norm": 4.307407379150391,
        "learning_rate": 5.0024226321215974e-05,
        "epoch": 1.1130666666666666,
        "step": 8348
    },
    {
        "loss": 1.7545,
        "grad_norm": 5.824600696563721,
        "learning_rate": 4.9969722600138615e-05,
        "epoch": 1.1132,
        "step": 8349
    },
    {
        "loss": 2.24,
        "grad_norm": 3.1871252059936523,
        "learning_rate": 4.991523869622449e-05,
        "epoch": 1.1133333333333333,
        "step": 8350
    },
    {
        "loss": 3.2459,
        "grad_norm": 3.773359537124634,
        "learning_rate": 4.986077463105514e-05,
        "epoch": 1.1134666666666666,
        "step": 8351
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.6780035495758057,
        "learning_rate": 4.9806330426203763e-05,
        "epoch": 1.1136,
        "step": 8352
    },
    {
        "loss": 1.9592,
        "grad_norm": 4.7763824462890625,
        "learning_rate": 4.975190610323589e-05,
        "epoch": 1.1137333333333332,
        "step": 8353
    },
    {
        "loss": 1.9822,
        "grad_norm": 2.8398234844207764,
        "learning_rate": 4.969750168370923e-05,
        "epoch": 1.1138666666666666,
        "step": 8354
    },
    {
        "loss": 1.2258,
        "grad_norm": 4.289913177490234,
        "learning_rate": 4.964311718917356e-05,
        "epoch": 1.114,
        "step": 8355
    },
    {
        "loss": 1.3068,
        "grad_norm": 4.469503402709961,
        "learning_rate": 4.958875264117071e-05,
        "epoch": 1.1141333333333334,
        "step": 8356
    },
    {
        "loss": 2.7918,
        "grad_norm": 4.285444736480713,
        "learning_rate": 4.953440806123483e-05,
        "epoch": 1.1142666666666667,
        "step": 8357
    },
    {
        "loss": 2.3719,
        "grad_norm": 2.9826552867889404,
        "learning_rate": 4.9480083470891834e-05,
        "epoch": 1.1144,
        "step": 8358
    },
    {
        "loss": 0.7262,
        "grad_norm": 3.209352493286133,
        "learning_rate": 4.942577889165981e-05,
        "epoch": 1.1145333333333334,
        "step": 8359
    },
    {
        "loss": 2.3703,
        "grad_norm": 5.332024574279785,
        "learning_rate": 4.937149434504924e-05,
        "epoch": 1.1146666666666667,
        "step": 8360
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.9841957092285156,
        "learning_rate": 4.931722985256224e-05,
        "epoch": 1.1148,
        "step": 8361
    },
    {
        "loss": 2.7677,
        "grad_norm": 3.0646069049835205,
        "learning_rate": 4.926298543569315e-05,
        "epoch": 1.1149333333333333,
        "step": 8362
    },
    {
        "loss": 2.3598,
        "grad_norm": 1.7976006269454956,
        "learning_rate": 4.9208761115928414e-05,
        "epoch": 1.1150666666666667,
        "step": 8363
    },
    {
        "loss": 2.2117,
        "grad_norm": 2.7245945930480957,
        "learning_rate": 4.915455691474646e-05,
        "epoch": 1.1152,
        "step": 8364
    },
    {
        "loss": 1.5487,
        "grad_norm": 3.5533454418182373,
        "learning_rate": 4.9100372853617895e-05,
        "epoch": 1.1153333333333333,
        "step": 8365
    },
    {
        "loss": 2.0027,
        "grad_norm": 3.3958957195281982,
        "learning_rate": 4.904620895400487e-05,
        "epoch": 1.1154666666666666,
        "step": 8366
    },
    {
        "loss": 1.1999,
        "grad_norm": 3.4684507846832275,
        "learning_rate": 4.899206523736222e-05,
        "epoch": 1.1156,
        "step": 8367
    },
    {
        "loss": 2.1257,
        "grad_norm": 2.413361072540283,
        "learning_rate": 4.893794172513626e-05,
        "epoch": 1.1157333333333332,
        "step": 8368
    },
    {
        "loss": 2.2417,
        "grad_norm": 2.86529541015625,
        "learning_rate": 4.888383843876573e-05,
        "epoch": 1.1158666666666668,
        "step": 8369
    },
    {
        "loss": 2.3187,
        "grad_norm": 3.3200016021728516,
        "learning_rate": 4.88297553996808e-05,
        "epoch": 1.116,
        "step": 8370
    },
    {
        "loss": 2.467,
        "grad_norm": 3.612076997756958,
        "learning_rate": 4.877569262930417e-05,
        "epoch": 1.1161333333333334,
        "step": 8371
    },
    {
        "loss": 1.5964,
        "grad_norm": 3.2440185546875,
        "learning_rate": 4.872165014905023e-05,
        "epoch": 1.1162666666666667,
        "step": 8372
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.8514294624328613,
        "learning_rate": 4.8667627980325455e-05,
        "epoch": 1.1164,
        "step": 8373
    },
    {
        "loss": 1.9562,
        "grad_norm": 3.140174627304077,
        "learning_rate": 4.861362614452811e-05,
        "epoch": 1.1165333333333334,
        "step": 8374
    },
    {
        "loss": 1.586,
        "grad_norm": 3.979647159576416,
        "learning_rate": 4.855964466304844e-05,
        "epoch": 1.1166666666666667,
        "step": 8375
    },
    {
        "loss": 0.8658,
        "grad_norm": 3.834533214569092,
        "learning_rate": 4.850568355726895e-05,
        "epoch": 1.1168,
        "step": 8376
    },
    {
        "loss": 1.9519,
        "grad_norm": 3.3139662742614746,
        "learning_rate": 4.8451742848563506e-05,
        "epoch": 1.1169333333333333,
        "step": 8377
    },
    {
        "loss": 1.5474,
        "grad_norm": 7.101921081542969,
        "learning_rate": 4.8397822558298565e-05,
        "epoch": 1.1170666666666667,
        "step": 8378
    },
    {
        "loss": 2.4316,
        "grad_norm": 2.681339740753174,
        "learning_rate": 4.834392270783182e-05,
        "epoch": 1.1172,
        "step": 8379
    },
    {
        "loss": 0.86,
        "grad_norm": 3.867504596710205,
        "learning_rate": 4.8290043318513314e-05,
        "epoch": 1.1173333333333333,
        "step": 8380
    },
    {
        "loss": 2.1753,
        "grad_norm": 2.486618757247925,
        "learning_rate": 4.823618441168479e-05,
        "epoch": 1.1174666666666666,
        "step": 8381
    },
    {
        "loss": 2.5756,
        "grad_norm": 2.4235646724700928,
        "learning_rate": 4.8182346008680136e-05,
        "epoch": 1.1176,
        "step": 8382
    },
    {
        "loss": 1.738,
        "grad_norm": 2.6847617626190186,
        "learning_rate": 4.8128528130824735e-05,
        "epoch": 1.1177333333333332,
        "step": 8383
    },
    {
        "loss": 1.6615,
        "grad_norm": 4.823326587677002,
        "learning_rate": 4.807473079943599e-05,
        "epoch": 1.1178666666666666,
        "step": 8384
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.9139039516448975,
        "learning_rate": 4.802095403582345e-05,
        "epoch": 1.1179999999999999,
        "step": 8385
    },
    {
        "loss": 0.958,
        "grad_norm": 4.95540714263916,
        "learning_rate": 4.7967197861288094e-05,
        "epoch": 1.1181333333333334,
        "step": 8386
    },
    {
        "loss": 2.1527,
        "grad_norm": 5.7038960456848145,
        "learning_rate": 4.7913462297122926e-05,
        "epoch": 1.1182666666666667,
        "step": 8387
    },
    {
        "loss": 1.7149,
        "grad_norm": 3.2084410190582275,
        "learning_rate": 4.7859747364612814e-05,
        "epoch": 1.1184,
        "step": 8388
    },
    {
        "loss": 1.9243,
        "grad_norm": 3.7916758060455322,
        "learning_rate": 4.780605308503445e-05,
        "epoch": 1.1185333333333334,
        "step": 8389
    },
    {
        "loss": 1.7432,
        "grad_norm": 3.047632932662964,
        "learning_rate": 4.7752379479656404e-05,
        "epoch": 1.1186666666666667,
        "step": 8390
    },
    {
        "loss": 0.9344,
        "grad_norm": 3.823154926300049,
        "learning_rate": 4.76987265697387e-05,
        "epoch": 1.1188,
        "step": 8391
    },
    {
        "loss": 1.6957,
        "grad_norm": 3.334690809249878,
        "learning_rate": 4.764509437653372e-05,
        "epoch": 1.1189333333333333,
        "step": 8392
    },
    {
        "loss": 2.4562,
        "grad_norm": 3.2549667358398438,
        "learning_rate": 4.759148292128518e-05,
        "epoch": 1.1190666666666667,
        "step": 8393
    },
    {
        "loss": 2.4387,
        "grad_norm": 3.1864840984344482,
        "learning_rate": 4.753789222522891e-05,
        "epoch": 1.1192,
        "step": 8394
    },
    {
        "loss": 1.7779,
        "grad_norm": 4.389395236968994,
        "learning_rate": 4.748432230959232e-05,
        "epoch": 1.1193333333333333,
        "step": 8395
    },
    {
        "loss": 1.782,
        "grad_norm": 3.7348270416259766,
        "learning_rate": 4.743077319559449e-05,
        "epoch": 1.1194666666666666,
        "step": 8396
    },
    {
        "loss": 2.1761,
        "grad_norm": 3.1009953022003174,
        "learning_rate": 4.737724490444657e-05,
        "epoch": 1.1196,
        "step": 8397
    },
    {
        "loss": 1.3854,
        "grad_norm": 4.4814772605896,
        "learning_rate": 4.732373745735125e-05,
        "epoch": 1.1197333333333332,
        "step": 8398
    },
    {
        "loss": 1.8351,
        "grad_norm": 3.065048933029175,
        "learning_rate": 4.727025087550312e-05,
        "epoch": 1.1198666666666666,
        "step": 8399
    },
    {
        "loss": 1.6714,
        "grad_norm": 4.749800682067871,
        "learning_rate": 4.7216785180088106e-05,
        "epoch": 1.12,
        "step": 8400
    },
    {
        "loss": 1.4679,
        "grad_norm": 3.3034818172454834,
        "learning_rate": 4.716334039228449e-05,
        "epoch": 1.1201333333333334,
        "step": 8401
    },
    {
        "loss": 1.4633,
        "grad_norm": 3.8673207759857178,
        "learning_rate": 4.710991653326163e-05,
        "epoch": 1.1202666666666667,
        "step": 8402
    },
    {
        "loss": 2.2127,
        "grad_norm": 3.1207988262176514,
        "learning_rate": 4.705651362418123e-05,
        "epoch": 1.1204,
        "step": 8403
    },
    {
        "loss": 2.7497,
        "grad_norm": 2.6839423179626465,
        "learning_rate": 4.700313168619608e-05,
        "epoch": 1.1205333333333334,
        "step": 8404
    },
    {
        "loss": 1.4794,
        "grad_norm": 3.980238676071167,
        "learning_rate": 4.694977074045105e-05,
        "epoch": 1.1206666666666667,
        "step": 8405
    },
    {
        "loss": 2.2509,
        "grad_norm": 3.89078426361084,
        "learning_rate": 4.6896430808082546e-05,
        "epoch": 1.1208,
        "step": 8406
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.6456196308135986,
        "learning_rate": 4.684311191021887e-05,
        "epoch": 1.1209333333333333,
        "step": 8407
    },
    {
        "loss": 1.9534,
        "grad_norm": 4.788331985473633,
        "learning_rate": 4.6789814067979655e-05,
        "epoch": 1.1210666666666667,
        "step": 8408
    },
    {
        "loss": 2.0763,
        "grad_norm": 3.6310815811157227,
        "learning_rate": 4.673653730247632e-05,
        "epoch": 1.1212,
        "step": 8409
    },
    {
        "loss": 2.452,
        "grad_norm": 4.311887741088867,
        "learning_rate": 4.6683281634812124e-05,
        "epoch": 1.1213333333333333,
        "step": 8410
    },
    {
        "loss": 2.0135,
        "grad_norm": 3.7363369464874268,
        "learning_rate": 4.663004708608175e-05,
        "epoch": 1.1214666666666666,
        "step": 8411
    },
    {
        "loss": 1.7362,
        "grad_norm": 3.706508159637451,
        "learning_rate": 4.657683367737153e-05,
        "epoch": 1.1216,
        "step": 8412
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.2122573852539062,
        "learning_rate": 4.6523641429759535e-05,
        "epoch": 1.1217333333333332,
        "step": 8413
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.477602005004883,
        "learning_rate": 4.647047036431539e-05,
        "epoch": 1.1218666666666666,
        "step": 8414
    },
    {
        "loss": 2.3584,
        "grad_norm": 5.752352714538574,
        "learning_rate": 4.6417320502100316e-05,
        "epoch": 1.1219999999999999,
        "step": 8415
    },
    {
        "loss": 1.5757,
        "grad_norm": 3.6623833179473877,
        "learning_rate": 4.636419186416722e-05,
        "epoch": 1.1221333333333334,
        "step": 8416
    },
    {
        "loss": 2.2353,
        "grad_norm": 2.092374563217163,
        "learning_rate": 4.6311084471560475e-05,
        "epoch": 1.1222666666666667,
        "step": 8417
    },
    {
        "loss": 1.255,
        "grad_norm": 3.5827999114990234,
        "learning_rate": 4.6257998345316045e-05,
        "epoch": 1.1224,
        "step": 8418
    },
    {
        "loss": 1.7738,
        "grad_norm": 3.8707854747772217,
        "learning_rate": 4.6204933506461666e-05,
        "epoch": 1.1225333333333334,
        "step": 8419
    },
    {
        "loss": 0.7139,
        "grad_norm": 4.495896816253662,
        "learning_rate": 4.615188997601646e-05,
        "epoch": 1.1226666666666667,
        "step": 8420
    },
    {
        "loss": 3.2955,
        "grad_norm": 4.422026634216309,
        "learning_rate": 4.6098867774991e-05,
        "epoch": 1.1228,
        "step": 8421
    },
    {
        "loss": 0.9332,
        "grad_norm": 3.445479154586792,
        "learning_rate": 4.6045866924387736e-05,
        "epoch": 1.1229333333333333,
        "step": 8422
    },
    {
        "loss": 2.3949,
        "grad_norm": 3.1132659912109375,
        "learning_rate": 4.599288744520042e-05,
        "epoch": 1.1230666666666667,
        "step": 8423
    },
    {
        "loss": 2.3426,
        "grad_norm": 2.767496347427368,
        "learning_rate": 4.5939929358414514e-05,
        "epoch": 1.1232,
        "step": 8424
    },
    {
        "loss": 2.0745,
        "grad_norm": 3.5694336891174316,
        "learning_rate": 4.588699268500661e-05,
        "epoch": 1.1233333333333333,
        "step": 8425
    },
    {
        "loss": 0.7846,
        "grad_norm": 2.4226434230804443,
        "learning_rate": 4.5834077445945445e-05,
        "epoch": 1.1234666666666666,
        "step": 8426
    },
    {
        "loss": 2.5409,
        "grad_norm": 1.930802345275879,
        "learning_rate": 4.578118366219061e-05,
        "epoch": 1.1236,
        "step": 8427
    },
    {
        "loss": 1.4354,
        "grad_norm": 5.436556816101074,
        "learning_rate": 4.5728311354693865e-05,
        "epoch": 1.1237333333333333,
        "step": 8428
    },
    {
        "loss": 1.9648,
        "grad_norm": 3.569730520248413,
        "learning_rate": 4.567546054439781e-05,
        "epoch": 1.1238666666666666,
        "step": 8429
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.8331849575042725,
        "learning_rate": 4.5622631252236936e-05,
        "epoch": 1.124,
        "step": 8430
    },
    {
        "loss": 2.7544,
        "grad_norm": 3.2319695949554443,
        "learning_rate": 4.5569823499137076e-05,
        "epoch": 1.1241333333333334,
        "step": 8431
    },
    {
        "loss": 0.9638,
        "grad_norm": 3.3367371559143066,
        "learning_rate": 4.5517037306015697e-05,
        "epoch": 1.1242666666666667,
        "step": 8432
    },
    {
        "loss": 2.1992,
        "grad_norm": 3.372015953063965,
        "learning_rate": 4.546427269378146e-05,
        "epoch": 1.1244,
        "step": 8433
    },
    {
        "loss": 1.746,
        "grad_norm": 3.12640643119812,
        "learning_rate": 4.541152968333455e-05,
        "epoch": 1.1245333333333334,
        "step": 8434
    },
    {
        "loss": 1.6736,
        "grad_norm": 2.5451693534851074,
        "learning_rate": 4.5358808295566826e-05,
        "epoch": 1.1246666666666667,
        "step": 8435
    },
    {
        "loss": 1.8367,
        "grad_norm": 2.5107622146606445,
        "learning_rate": 4.53061085513612e-05,
        "epoch": 1.1248,
        "step": 8436
    },
    {
        "loss": 1.6904,
        "grad_norm": 3.2529008388519287,
        "learning_rate": 4.525343047159253e-05,
        "epoch": 1.1249333333333333,
        "step": 8437
    },
    {
        "loss": 0.6847,
        "grad_norm": 3.3986687660217285,
        "learning_rate": 4.520077407712648e-05,
        "epoch": 1.1250666666666667,
        "step": 8438
    },
    {
        "loss": 2.0498,
        "grad_norm": 3.4658641815185547,
        "learning_rate": 4.514813938882054e-05,
        "epoch": 1.1252,
        "step": 8439
    },
    {
        "loss": 2.1378,
        "grad_norm": 3.085728406906128,
        "learning_rate": 4.50955264275235e-05,
        "epoch": 1.1253333333333333,
        "step": 8440
    },
    {
        "loss": 2.2042,
        "grad_norm": 2.070267677307129,
        "learning_rate": 4.504293521407557e-05,
        "epoch": 1.1254666666666666,
        "step": 8441
    },
    {
        "loss": 1.9565,
        "grad_norm": 3.154303550720215,
        "learning_rate": 4.499036576930828e-05,
        "epoch": 1.1256,
        "step": 8442
    },
    {
        "loss": 1.3892,
        "grad_norm": 3.2913010120391846,
        "learning_rate": 4.4937818114044405e-05,
        "epoch": 1.1257333333333333,
        "step": 8443
    },
    {
        "loss": 0.6972,
        "grad_norm": 2.092869281768799,
        "learning_rate": 4.488529226909851e-05,
        "epoch": 1.1258666666666666,
        "step": 8444
    },
    {
        "loss": 1.9745,
        "grad_norm": 4.527721405029297,
        "learning_rate": 4.483278825527619e-05,
        "epoch": 1.126,
        "step": 8445
    },
    {
        "loss": 1.8866,
        "grad_norm": 3.682983160018921,
        "learning_rate": 4.47803060933743e-05,
        "epoch": 1.1261333333333332,
        "step": 8446
    },
    {
        "loss": 2.1421,
        "grad_norm": 2.8908591270446777,
        "learning_rate": 4.472784580418136e-05,
        "epoch": 1.1262666666666667,
        "step": 8447
    },
    {
        "loss": 2.3819,
        "grad_norm": 2.0406806468963623,
        "learning_rate": 4.467540740847703e-05,
        "epoch": 1.1264,
        "step": 8448
    },
    {
        "loss": 2.3615,
        "grad_norm": 2.460862874984741,
        "learning_rate": 4.462299092703246e-05,
        "epoch": 1.1265333333333334,
        "step": 8449
    },
    {
        "loss": 1.8275,
        "grad_norm": 2.8907456398010254,
        "learning_rate": 4.4570596380609686e-05,
        "epoch": 1.1266666666666667,
        "step": 8450
    },
    {
        "loss": 1.9882,
        "grad_norm": 3.68624210357666,
        "learning_rate": 4.451822378996271e-05,
        "epoch": 1.1268,
        "step": 8451
    },
    {
        "loss": 2.1664,
        "grad_norm": 4.229861259460449,
        "learning_rate": 4.446587317583618e-05,
        "epoch": 1.1269333333333333,
        "step": 8452
    },
    {
        "loss": 2.813,
        "grad_norm": 3.352764129638672,
        "learning_rate": 4.44135445589667e-05,
        "epoch": 1.1270666666666667,
        "step": 8453
    },
    {
        "loss": 2.0679,
        "grad_norm": 2.2585976123809814,
        "learning_rate": 4.436123796008153e-05,
        "epoch": 1.1272,
        "step": 8454
    },
    {
        "loss": 2.0657,
        "grad_norm": 4.010651111602783,
        "learning_rate": 4.430895339989958e-05,
        "epoch": 1.1273333333333333,
        "step": 8455
    },
    {
        "loss": 2.848,
        "grad_norm": 3.6200592517852783,
        "learning_rate": 4.425669089913095e-05,
        "epoch": 1.1274666666666666,
        "step": 8456
    },
    {
        "loss": 2.6328,
        "grad_norm": 3.5546631813049316,
        "learning_rate": 4.4204450478476955e-05,
        "epoch": 1.1276,
        "step": 8457
    },
    {
        "loss": 2.5693,
        "grad_norm": 3.3472862243652344,
        "learning_rate": 4.4152232158630394e-05,
        "epoch": 1.1277333333333333,
        "step": 8458
    },
    {
        "loss": 2.0882,
        "grad_norm": 2.883397102355957,
        "learning_rate": 4.410003596027482e-05,
        "epoch": 1.1278666666666666,
        "step": 8459
    },
    {
        "loss": 2.4504,
        "grad_norm": 4.010952949523926,
        "learning_rate": 4.4047861904085565e-05,
        "epoch": 1.1280000000000001,
        "step": 8460
    },
    {
        "loss": 2.144,
        "grad_norm": 4.091937065124512,
        "learning_rate": 4.399571001072874e-05,
        "epoch": 1.1281333333333334,
        "step": 8461
    },
    {
        "loss": 1.8822,
        "grad_norm": 3.536332607269287,
        "learning_rate": 4.3943580300862195e-05,
        "epoch": 1.1282666666666668,
        "step": 8462
    },
    {
        "loss": 2.9004,
        "grad_norm": 2.578615665435791,
        "learning_rate": 4.389147279513439e-05,
        "epoch": 1.1284,
        "step": 8463
    },
    {
        "loss": 2.4972,
        "grad_norm": 4.076439380645752,
        "learning_rate": 4.383938751418539e-05,
        "epoch": 1.1285333333333334,
        "step": 8464
    },
    {
        "loss": 1.9494,
        "grad_norm": 4.08143949508667,
        "learning_rate": 4.378732447864638e-05,
        "epoch": 1.1286666666666667,
        "step": 8465
    },
    {
        "loss": 1.1612,
        "grad_norm": 3.7551558017730713,
        "learning_rate": 4.37352837091397e-05,
        "epoch": 1.1288,
        "step": 8466
    },
    {
        "loss": 2.5296,
        "grad_norm": 3.241364002227783,
        "learning_rate": 4.36832652262789e-05,
        "epoch": 1.1289333333333333,
        "step": 8467
    },
    {
        "loss": 2.103,
        "grad_norm": 3.9609909057617188,
        "learning_rate": 4.3631269050668486e-05,
        "epoch": 1.1290666666666667,
        "step": 8468
    },
    {
        "loss": 2.4289,
        "grad_norm": 2.0118496417999268,
        "learning_rate": 4.357929520290459e-05,
        "epoch": 1.1292,
        "step": 8469
    },
    {
        "loss": 1.6189,
        "grad_norm": 3.176748037338257,
        "learning_rate": 4.352734370357415e-05,
        "epoch": 1.1293333333333333,
        "step": 8470
    },
    {
        "loss": 2.2201,
        "grad_norm": 3.9950833320617676,
        "learning_rate": 4.3475414573255194e-05,
        "epoch": 1.1294666666666666,
        "step": 8471
    },
    {
        "loss": 1.4346,
        "grad_norm": 3.8836517333984375,
        "learning_rate": 4.3423507832517155e-05,
        "epoch": 1.1296,
        "step": 8472
    },
    {
        "loss": 1.0249,
        "grad_norm": 5.5764312744140625,
        "learning_rate": 4.3371623501920445e-05,
        "epoch": 1.1297333333333333,
        "step": 8473
    },
    {
        "loss": 2.194,
        "grad_norm": 4.511763095855713,
        "learning_rate": 4.331976160201662e-05,
        "epoch": 1.1298666666666666,
        "step": 8474
    },
    {
        "loss": 1.7173,
        "grad_norm": 3.0351803302764893,
        "learning_rate": 4.3267922153348396e-05,
        "epoch": 1.13,
        "step": 8475
    },
    {
        "loss": 1.386,
        "grad_norm": 3.1692535877227783,
        "learning_rate": 4.321610517644955e-05,
        "epoch": 1.1301333333333332,
        "step": 8476
    },
    {
        "loss": 2.2458,
        "grad_norm": 5.1740851402282715,
        "learning_rate": 4.316431069184479e-05,
        "epoch": 1.1302666666666668,
        "step": 8477
    },
    {
        "loss": 2.3895,
        "grad_norm": 3.160811185836792,
        "learning_rate": 4.311253872005033e-05,
        "epoch": 1.1304,
        "step": 8478
    },
    {
        "loss": 1.6594,
        "grad_norm": 3.6852965354919434,
        "learning_rate": 4.306078928157317e-05,
        "epoch": 1.1305333333333334,
        "step": 8479
    },
    {
        "loss": 2.1881,
        "grad_norm": 3.3552610874176025,
        "learning_rate": 4.3009062396911315e-05,
        "epoch": 1.1306666666666667,
        "step": 8480
    },
    {
        "loss": 1.7218,
        "grad_norm": 3.4373271465301514,
        "learning_rate": 4.2957358086554036e-05,
        "epoch": 1.1308,
        "step": 8481
    },
    {
        "loss": 3.0718,
        "grad_norm": 2.4735066890716553,
        "learning_rate": 4.290567637098154e-05,
        "epoch": 1.1309333333333333,
        "step": 8482
    },
    {
        "loss": 1.8337,
        "grad_norm": 5.058947563171387,
        "learning_rate": 4.285401727066534e-05,
        "epoch": 1.1310666666666667,
        "step": 8483
    },
    {
        "loss": 2.1018,
        "grad_norm": 4.881653308868408,
        "learning_rate": 4.280238080606745e-05,
        "epoch": 1.1312,
        "step": 8484
    },
    {
        "loss": 1.8921,
        "grad_norm": 3.9832077026367188,
        "learning_rate": 4.275076699764151e-05,
        "epoch": 1.1313333333333333,
        "step": 8485
    },
    {
        "loss": 2.6355,
        "grad_norm": 2.474531412124634,
        "learning_rate": 4.269917586583173e-05,
        "epoch": 1.1314666666666666,
        "step": 8486
    },
    {
        "loss": 1.8428,
        "grad_norm": 2.588658332824707,
        "learning_rate": 4.264760743107381e-05,
        "epoch": 1.1316,
        "step": 8487
    },
    {
        "loss": 2.3584,
        "grad_norm": 3.1341731548309326,
        "learning_rate": 4.25960617137939e-05,
        "epoch": 1.1317333333333333,
        "step": 8488
    },
    {
        "loss": 1.5594,
        "grad_norm": 3.4609415531158447,
        "learning_rate": 4.2544538734409545e-05,
        "epoch": 1.1318666666666666,
        "step": 8489
    },
    {
        "loss": 1.4085,
        "grad_norm": 2.4541640281677246,
        "learning_rate": 4.249303851332918e-05,
        "epoch": 1.1320000000000001,
        "step": 8490
    },
    {
        "loss": 2.5681,
        "grad_norm": 2.843794822692871,
        "learning_rate": 4.244156107095223e-05,
        "epoch": 1.1321333333333334,
        "step": 8491
    },
    {
        "loss": 1.358,
        "grad_norm": 2.1825954914093018,
        "learning_rate": 4.2390106427669106e-05,
        "epoch": 1.1322666666666668,
        "step": 8492
    },
    {
        "loss": 1.8385,
        "grad_norm": 4.034238815307617,
        "learning_rate": 4.233867460386099e-05,
        "epoch": 1.1324,
        "step": 8493
    },
    {
        "loss": 1.8246,
        "grad_norm": 3.32477068901062,
        "learning_rate": 4.2287265619900454e-05,
        "epoch": 1.1325333333333334,
        "step": 8494
    },
    {
        "loss": 2.3002,
        "grad_norm": 2.663438320159912,
        "learning_rate": 4.223587949615057e-05,
        "epoch": 1.1326666666666667,
        "step": 8495
    },
    {
        "loss": 1.9994,
        "grad_norm": 2.5898892879486084,
        "learning_rate": 4.2184516252965786e-05,
        "epoch": 1.1328,
        "step": 8496
    },
    {
        "loss": 2.8977,
        "grad_norm": 2.649251699447632,
        "learning_rate": 4.213317591069098e-05,
        "epoch": 1.1329333333333333,
        "step": 8497
    },
    {
        "loss": 1.8047,
        "grad_norm": 2.683778762817383,
        "learning_rate": 4.208185848966242e-05,
        "epoch": 1.1330666666666667,
        "step": 8498
    },
    {
        "loss": 1.7947,
        "grad_norm": 2.6163787841796875,
        "learning_rate": 4.2030564010207044e-05,
        "epoch": 1.1332,
        "step": 8499
    },
    {
        "loss": 1.8346,
        "grad_norm": 5.30266809463501,
        "learning_rate": 4.197929249264283e-05,
        "epoch": 1.1333333333333333,
        "step": 8500
    },
    {
        "loss": 2.4241,
        "grad_norm": 4.311131477355957,
        "learning_rate": 4.192804395727861e-05,
        "epoch": 1.1334666666666666,
        "step": 8501
    },
    {
        "loss": 1.9237,
        "grad_norm": 2.3411943912506104,
        "learning_rate": 4.187681842441389e-05,
        "epoch": 1.1336,
        "step": 8502
    },
    {
        "loss": 2.1977,
        "grad_norm": 3.8198094367980957,
        "learning_rate": 4.18256159143396e-05,
        "epoch": 1.1337333333333333,
        "step": 8503
    },
    {
        "loss": 0.6591,
        "grad_norm": 3.5528509616851807,
        "learning_rate": 4.177443644733703e-05,
        "epoch": 1.1338666666666666,
        "step": 8504
    },
    {
        "loss": 2.236,
        "grad_norm": 3.642674684524536,
        "learning_rate": 4.1723280043678534e-05,
        "epoch": 1.134,
        "step": 8505
    },
    {
        "loss": 2.0656,
        "grad_norm": 2.880828380584717,
        "learning_rate": 4.1672146723627405e-05,
        "epoch": 1.1341333333333332,
        "step": 8506
    },
    {
        "loss": 1.7777,
        "grad_norm": 2.530003070831299,
        "learning_rate": 4.162103650743766e-05,
        "epoch": 1.1342666666666668,
        "step": 8507
    },
    {
        "loss": 1.464,
        "grad_norm": 2.5956990718841553,
        "learning_rate": 4.156994941535443e-05,
        "epoch": 1.1344,
        "step": 8508
    },
    {
        "loss": 2.2897,
        "grad_norm": 2.377448558807373,
        "learning_rate": 4.151888546761318e-05,
        "epoch": 1.1345333333333334,
        "step": 8509
    },
    {
        "loss": 2.5423,
        "grad_norm": 2.904273509979248,
        "learning_rate": 4.1467844684440734e-05,
        "epoch": 1.1346666666666667,
        "step": 8510
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.9461357593536377,
        "learning_rate": 4.1416827086054346e-05,
        "epoch": 1.1348,
        "step": 8511
    },
    {
        "loss": 2.7543,
        "grad_norm": 2.5182814598083496,
        "learning_rate": 4.136583269266255e-05,
        "epoch": 1.1349333333333333,
        "step": 8512
    },
    {
        "loss": 1.9565,
        "grad_norm": 3.2762715816497803,
        "learning_rate": 4.131486152446411e-05,
        "epoch": 1.1350666666666667,
        "step": 8513
    },
    {
        "loss": 2.4917,
        "grad_norm": 2.5987539291381836,
        "learning_rate": 4.126391360164897e-05,
        "epoch": 1.1352,
        "step": 8514
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.8998637199401855,
        "learning_rate": 4.121298894439781e-05,
        "epoch": 1.1353333333333333,
        "step": 8515
    },
    {
        "loss": 1.1986,
        "grad_norm": 2.9516685009002686,
        "learning_rate": 4.1162087572882034e-05,
        "epoch": 1.1354666666666666,
        "step": 8516
    },
    {
        "loss": 1.79,
        "grad_norm": 4.628814220428467,
        "learning_rate": 4.111120950726396e-05,
        "epoch": 1.1356,
        "step": 8517
    },
    {
        "loss": 2.4455,
        "grad_norm": 3.155266523361206,
        "learning_rate": 4.106035476769628e-05,
        "epoch": 1.1357333333333333,
        "step": 8518
    },
    {
        "loss": 1.2595,
        "grad_norm": 4.282103061676025,
        "learning_rate": 4.100952337432302e-05,
        "epoch": 1.1358666666666666,
        "step": 8519
    },
    {
        "loss": 2.513,
        "grad_norm": 2.10150408744812,
        "learning_rate": 4.095871534727847e-05,
        "epoch": 1.1360000000000001,
        "step": 8520
    },
    {
        "loss": 1.885,
        "grad_norm": 4.119752407073975,
        "learning_rate": 4.0907930706688116e-05,
        "epoch": 1.1361333333333334,
        "step": 8521
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.7048051357269287,
        "learning_rate": 4.08571694726676e-05,
        "epoch": 1.1362666666666668,
        "step": 8522
    },
    {
        "loss": 2.395,
        "grad_norm": 2.41219425201416,
        "learning_rate": 4.080643166532383e-05,
        "epoch": 1.1364,
        "step": 8523
    },
    {
        "loss": 2.4843,
        "grad_norm": 2.599621534347534,
        "learning_rate": 4.075571730475416e-05,
        "epoch": 1.1365333333333334,
        "step": 8524
    },
    {
        "loss": 2.3748,
        "grad_norm": 2.6766815185546875,
        "learning_rate": 4.0705026411046775e-05,
        "epoch": 1.1366666666666667,
        "step": 8525
    },
    {
        "loss": 1.0258,
        "grad_norm": 3.694531202316284,
        "learning_rate": 4.065435900428052e-05,
        "epoch": 1.1368,
        "step": 8526
    },
    {
        "loss": 1.5005,
        "grad_norm": 4.112959861755371,
        "learning_rate": 4.060371510452472e-05,
        "epoch": 1.1369333333333334,
        "step": 8527
    },
    {
        "loss": 1.0761,
        "grad_norm": 3.2527894973754883,
        "learning_rate": 4.05530947318399e-05,
        "epoch": 1.1370666666666667,
        "step": 8528
    },
    {
        "loss": 2.059,
        "grad_norm": 2.7253808975219727,
        "learning_rate": 4.050249790627678e-05,
        "epoch": 1.1372,
        "step": 8529
    },
    {
        "loss": 1.1484,
        "grad_norm": 3.330512285232544,
        "learning_rate": 4.045192464787695e-05,
        "epoch": 1.1373333333333333,
        "step": 8530
    },
    {
        "loss": 2.9094,
        "grad_norm": 2.7640223503112793,
        "learning_rate": 4.040137497667266e-05,
        "epoch": 1.1374666666666666,
        "step": 8531
    },
    {
        "loss": 2.5539,
        "grad_norm": 3.5896236896514893,
        "learning_rate": 4.035084891268679e-05,
        "epoch": 1.1376,
        "step": 8532
    },
    {
        "loss": 0.875,
        "grad_norm": 4.164577007293701,
        "learning_rate": 4.030034647593305e-05,
        "epoch": 1.1377333333333333,
        "step": 8533
    },
    {
        "loss": 0.7146,
        "grad_norm": 2.8784353733062744,
        "learning_rate": 4.024986768641535e-05,
        "epoch": 1.1378666666666666,
        "step": 8534
    },
    {
        "loss": 1.6097,
        "grad_norm": 4.667786598205566,
        "learning_rate": 4.019941256412873e-05,
        "epoch": 1.138,
        "step": 8535
    },
    {
        "loss": 2.4532,
        "grad_norm": 3.9342892169952393,
        "learning_rate": 4.014898112905845e-05,
        "epoch": 1.1381333333333332,
        "step": 8536
    },
    {
        "loss": 1.9546,
        "grad_norm": 4.5476603507995605,
        "learning_rate": 4.009857340118083e-05,
        "epoch": 1.1382666666666668,
        "step": 8537
    },
    {
        "loss": 1.9537,
        "grad_norm": 6.58375358581543,
        "learning_rate": 4.0048189400462355e-05,
        "epoch": 1.1384,
        "step": 8538
    },
    {
        "loss": 2.5513,
        "grad_norm": 2.428147077560425,
        "learning_rate": 3.99978291468603e-05,
        "epoch": 1.1385333333333334,
        "step": 8539
    },
    {
        "loss": 1.5337,
        "grad_norm": 3.4631683826446533,
        "learning_rate": 3.9947492660322574e-05,
        "epoch": 1.1386666666666667,
        "step": 8540
    },
    {
        "loss": 1.3334,
        "grad_norm": 3.6846842765808105,
        "learning_rate": 3.989717996078763e-05,
        "epoch": 1.1388,
        "step": 8541
    },
    {
        "loss": 2.2198,
        "grad_norm": 3.5876705646514893,
        "learning_rate": 3.984689106818461e-05,
        "epoch": 1.1389333333333334,
        "step": 8542
    },
    {
        "loss": 2.4296,
        "grad_norm": 3.6784164905548096,
        "learning_rate": 3.979662600243282e-05,
        "epoch": 1.1390666666666667,
        "step": 8543
    },
    {
        "loss": 2.0285,
        "grad_norm": 3.490709066390991,
        "learning_rate": 3.9746384783442713e-05,
        "epoch": 1.1392,
        "step": 8544
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.1470446586608887,
        "learning_rate": 3.969616743111484e-05,
        "epoch": 1.1393333333333333,
        "step": 8545
    },
    {
        "loss": 1.8487,
        "grad_norm": 3.5618770122528076,
        "learning_rate": 3.964597396534067e-05,
        "epoch": 1.1394666666666666,
        "step": 8546
    },
    {
        "loss": 2.1313,
        "grad_norm": 3.8138632774353027,
        "learning_rate": 3.959580440600171e-05,
        "epoch": 1.1396,
        "step": 8547
    },
    {
        "loss": 2.6505,
        "grad_norm": 2.8074429035186768,
        "learning_rate": 3.9545658772970495e-05,
        "epoch": 1.1397333333333333,
        "step": 8548
    },
    {
        "loss": 1.9976,
        "grad_norm": 3.880028247833252,
        "learning_rate": 3.9495537086109826e-05,
        "epoch": 1.1398666666666666,
        "step": 8549
    },
    {
        "loss": 2.0149,
        "grad_norm": 2.3877835273742676,
        "learning_rate": 3.944543936527316e-05,
        "epoch": 1.1400000000000001,
        "step": 8550
    },
    {
        "loss": 0.6494,
        "grad_norm": 3.309767723083496,
        "learning_rate": 3.939536563030425e-05,
        "epoch": 1.1401333333333334,
        "step": 8551
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.381455898284912,
        "learning_rate": 3.9345315901037415e-05,
        "epoch": 1.1402666666666668,
        "step": 8552
    },
    {
        "loss": 2.2012,
        "grad_norm": 3.0756876468658447,
        "learning_rate": 3.9295290197297774e-05,
        "epoch": 1.1404,
        "step": 8553
    },
    {
        "loss": 2.7173,
        "grad_norm": 4.607471466064453,
        "learning_rate": 3.924528853890049e-05,
        "epoch": 1.1405333333333334,
        "step": 8554
    },
    {
        "loss": 2.4167,
        "grad_norm": 2.456305503845215,
        "learning_rate": 3.919531094565141e-05,
        "epoch": 1.1406666666666667,
        "step": 8555
    },
    {
        "loss": 2.0838,
        "grad_norm": 3.620896816253662,
        "learning_rate": 3.914535743734684e-05,
        "epoch": 1.1408,
        "step": 8556
    },
    {
        "loss": 1.7837,
        "grad_norm": 4.239401340484619,
        "learning_rate": 3.909542803377355e-05,
        "epoch": 1.1409333333333334,
        "step": 8557
    },
    {
        "loss": 2.0329,
        "grad_norm": 3.4843249320983887,
        "learning_rate": 3.9045522754708705e-05,
        "epoch": 1.1410666666666667,
        "step": 8558
    },
    {
        "loss": 2.0406,
        "grad_norm": 3.1915457248687744,
        "learning_rate": 3.89956416199201e-05,
        "epoch": 1.1412,
        "step": 8559
    },
    {
        "loss": 0.5258,
        "grad_norm": 2.5493428707122803,
        "learning_rate": 3.8945784649165683e-05,
        "epoch": 1.1413333333333333,
        "step": 8560
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.9066946506500244,
        "learning_rate": 3.88959518621939e-05,
        "epoch": 1.1414666666666666,
        "step": 8561
    },
    {
        "loss": 2.013,
        "grad_norm": 4.686307907104492,
        "learning_rate": 3.884614327874393e-05,
        "epoch": 1.1416,
        "step": 8562
    },
    {
        "loss": 2.2977,
        "grad_norm": 3.1618199348449707,
        "learning_rate": 3.879635891854495e-05,
        "epoch": 1.1417333333333333,
        "step": 8563
    },
    {
        "loss": 1.7333,
        "grad_norm": 3.5763306617736816,
        "learning_rate": 3.8746598801316716e-05,
        "epoch": 1.1418666666666666,
        "step": 8564
    },
    {
        "loss": 2.1564,
        "grad_norm": 2.953188180923462,
        "learning_rate": 3.8696862946769405e-05,
        "epoch": 1.142,
        "step": 8565
    },
    {
        "loss": 1.2686,
        "grad_norm": 4.415237903594971,
        "learning_rate": 3.864715137460357e-05,
        "epoch": 1.1421333333333332,
        "step": 8566
    },
    {
        "loss": 1.7007,
        "grad_norm": 2.565390110015869,
        "learning_rate": 3.859746410451024e-05,
        "epoch": 1.1422666666666668,
        "step": 8567
    },
    {
        "loss": 1.9278,
        "grad_norm": 2.5413007736206055,
        "learning_rate": 3.854780115617042e-05,
        "epoch": 1.1424,
        "step": 8568
    },
    {
        "loss": 2.1767,
        "grad_norm": 3.742044448852539,
        "learning_rate": 3.8498162549256054e-05,
        "epoch": 1.1425333333333334,
        "step": 8569
    },
    {
        "loss": 2.0526,
        "grad_norm": 3.083651304244995,
        "learning_rate": 3.844854830342901e-05,
        "epoch": 1.1426666666666667,
        "step": 8570
    },
    {
        "loss": 1.5772,
        "grad_norm": 2.9289283752441406,
        "learning_rate": 3.839895843834185e-05,
        "epoch": 1.1428,
        "step": 8571
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.534302234649658,
        "learning_rate": 3.834939297363701e-05,
        "epoch": 1.1429333333333334,
        "step": 8572
    },
    {
        "loss": 2.8562,
        "grad_norm": 3.7345991134643555,
        "learning_rate": 3.8299851928947716e-05,
        "epoch": 1.1430666666666667,
        "step": 8573
    },
    {
        "loss": 0.8676,
        "grad_norm": 3.9154880046844482,
        "learning_rate": 3.825033532389731e-05,
        "epoch": 1.1432,
        "step": 8574
    },
    {
        "loss": 2.1534,
        "grad_norm": 3.535304546356201,
        "learning_rate": 3.82008431780995e-05,
        "epoch": 1.1433333333333333,
        "step": 8575
    },
    {
        "loss": 1.6113,
        "grad_norm": 4.632418632507324,
        "learning_rate": 3.815137551115837e-05,
        "epoch": 1.1434666666666666,
        "step": 8576
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.0806610584259033,
        "learning_rate": 3.810193234266798e-05,
        "epoch": 1.1436,
        "step": 8577
    },
    {
        "loss": 2.1693,
        "grad_norm": 5.621415615081787,
        "learning_rate": 3.8052513692213245e-05,
        "epoch": 1.1437333333333333,
        "step": 8578
    },
    {
        "loss": 1.033,
        "grad_norm": 4.3919243812561035,
        "learning_rate": 3.8003119579368775e-05,
        "epoch": 1.1438666666666666,
        "step": 8579
    },
    {
        "loss": 1.9876,
        "grad_norm": 3.7959682941436768,
        "learning_rate": 3.795375002370006e-05,
        "epoch": 1.144,
        "step": 8580
    },
    {
        "loss": 1.7356,
        "grad_norm": 3.5567221641540527,
        "learning_rate": 3.790440504476227e-05,
        "epoch": 1.1441333333333334,
        "step": 8581
    },
    {
        "loss": 2.0853,
        "grad_norm": 5.849961280822754,
        "learning_rate": 3.785508466210124e-05,
        "epoch": 1.1442666666666668,
        "step": 8582
    },
    {
        "loss": 2.0238,
        "grad_norm": 5.757904052734375,
        "learning_rate": 3.780578889525288e-05,
        "epoch": 1.1444,
        "step": 8583
    },
    {
        "loss": 1.2124,
        "grad_norm": 5.894914150238037,
        "learning_rate": 3.775651776374357e-05,
        "epoch": 1.1445333333333334,
        "step": 8584
    },
    {
        "loss": 0.5468,
        "grad_norm": 2.367699384689331,
        "learning_rate": 3.770727128708963e-05,
        "epoch": 1.1446666666666667,
        "step": 8585
    },
    {
        "loss": 1.161,
        "grad_norm": 4.807657718658447,
        "learning_rate": 3.7658049484797686e-05,
        "epoch": 1.1448,
        "step": 8586
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.580798387527466,
        "learning_rate": 3.7608852376364915e-05,
        "epoch": 1.1449333333333334,
        "step": 8587
    },
    {
        "loss": 2.5119,
        "grad_norm": 4.443053722381592,
        "learning_rate": 3.755967998127825e-05,
        "epoch": 1.1450666666666667,
        "step": 8588
    },
    {
        "loss": 1.5495,
        "grad_norm": 2.939932346343994,
        "learning_rate": 3.751053231901509e-05,
        "epoch": 1.1452,
        "step": 8589
    },
    {
        "loss": 2.1677,
        "grad_norm": 3.5954530239105225,
        "learning_rate": 3.7461409409043e-05,
        "epoch": 1.1453333333333333,
        "step": 8590
    },
    {
        "loss": 0.6778,
        "grad_norm": 1.8857027292251587,
        "learning_rate": 3.741231127081974e-05,
        "epoch": 1.1454666666666666,
        "step": 8591
    },
    {
        "loss": 2.6618,
        "grad_norm": 2.158513069152832,
        "learning_rate": 3.7363237923793346e-05,
        "epoch": 1.1456,
        "step": 8592
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.453386068344116,
        "learning_rate": 3.731418938740168e-05,
        "epoch": 1.1457333333333333,
        "step": 8593
    },
    {
        "loss": 2.1766,
        "grad_norm": 3.801144599914551,
        "learning_rate": 3.726516568107329e-05,
        "epoch": 1.1458666666666666,
        "step": 8594
    },
    {
        "loss": 1.4153,
        "grad_norm": 3.4472498893737793,
        "learning_rate": 3.721616682422646e-05,
        "epoch": 1.146,
        "step": 8595
    },
    {
        "loss": 2.601,
        "grad_norm": 3.628304958343506,
        "learning_rate": 3.716719283626997e-05,
        "epoch": 1.1461333333333332,
        "step": 8596
    },
    {
        "loss": 1.7523,
        "grad_norm": 2.0979738235473633,
        "learning_rate": 3.711824373660251e-05,
        "epoch": 1.1462666666666665,
        "step": 8597
    },
    {
        "loss": 1.8064,
        "grad_norm": 3.7017815113067627,
        "learning_rate": 3.706931954461288e-05,
        "epoch": 1.1464,
        "step": 8598
    },
    {
        "loss": 3.0216,
        "grad_norm": 2.658327341079712,
        "learning_rate": 3.7020420279680234e-05,
        "epoch": 1.1465333333333334,
        "step": 8599
    },
    {
        "loss": 1.2632,
        "grad_norm": 3.665921688079834,
        "learning_rate": 3.697154596117373e-05,
        "epoch": 1.1466666666666667,
        "step": 8600
    },
    {
        "loss": 2.0778,
        "grad_norm": 3.2943673133850098,
        "learning_rate": 3.692269660845272e-05,
        "epoch": 1.1468,
        "step": 8601
    },
    {
        "loss": 1.3694,
        "grad_norm": 4.111741065979004,
        "learning_rate": 3.6873872240866344e-05,
        "epoch": 1.1469333333333334,
        "step": 8602
    },
    {
        "loss": 1.7545,
        "grad_norm": 5.7921648025512695,
        "learning_rate": 3.6825072877754416e-05,
        "epoch": 1.1470666666666667,
        "step": 8603
    },
    {
        "loss": 2.065,
        "grad_norm": 3.0447258949279785,
        "learning_rate": 3.6776298538446244e-05,
        "epoch": 1.1472,
        "step": 8604
    },
    {
        "loss": 2.4477,
        "grad_norm": 3.633272647857666,
        "learning_rate": 3.6727549242261815e-05,
        "epoch": 1.1473333333333333,
        "step": 8605
    },
    {
        "loss": 2.3462,
        "grad_norm": 3.3916175365448,
        "learning_rate": 3.667882500851063e-05,
        "epoch": 1.1474666666666666,
        "step": 8606
    },
    {
        "loss": 0.9888,
        "grad_norm": 4.131463527679443,
        "learning_rate": 3.663012585649263e-05,
        "epoch": 1.1476,
        "step": 8607
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.7040563821792603,
        "learning_rate": 3.658145180549766e-05,
        "epoch": 1.1477333333333333,
        "step": 8608
    },
    {
        "loss": 2.5695,
        "grad_norm": 2.950897455215454,
        "learning_rate": 3.6532802874805825e-05,
        "epoch": 1.1478666666666666,
        "step": 8609
    },
    {
        "loss": 2.3538,
        "grad_norm": 2.740105152130127,
        "learning_rate": 3.6484179083687e-05,
        "epoch": 1.148,
        "step": 8610
    },
    {
        "loss": 1.9001,
        "grad_norm": 3.3404695987701416,
        "learning_rate": 3.64355804514012e-05,
        "epoch": 1.1481333333333335,
        "step": 8611
    },
    {
        "loss": 1.06,
        "grad_norm": 5.248393535614014,
        "learning_rate": 3.638700699719863e-05,
        "epoch": 1.1482666666666668,
        "step": 8612
    },
    {
        "loss": 1.7737,
        "grad_norm": 4.514676570892334,
        "learning_rate": 3.633845874031936e-05,
        "epoch": 1.1484,
        "step": 8613
    },
    {
        "loss": 1.7311,
        "grad_norm": 6.4712138175964355,
        "learning_rate": 3.628993569999346e-05,
        "epoch": 1.1485333333333334,
        "step": 8614
    },
    {
        "loss": 1.9346,
        "grad_norm": 2.5043907165527344,
        "learning_rate": 3.624143789544112e-05,
        "epoch": 1.1486666666666667,
        "step": 8615
    },
    {
        "loss": 2.6464,
        "grad_norm": 3.019223213195801,
        "learning_rate": 3.6192965345872475e-05,
        "epoch": 1.1488,
        "step": 8616
    },
    {
        "loss": 2.2108,
        "grad_norm": 2.9801509380340576,
        "learning_rate": 3.6144518070487685e-05,
        "epoch": 1.1489333333333334,
        "step": 8617
    },
    {
        "loss": 1.2371,
        "grad_norm": 3.06693434715271,
        "learning_rate": 3.609609608847689e-05,
        "epoch": 1.1490666666666667,
        "step": 8618
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.7156128883361816,
        "learning_rate": 3.60476994190202e-05,
        "epoch": 1.1492,
        "step": 8619
    },
    {
        "loss": 1.742,
        "grad_norm": 3.1314682960510254,
        "learning_rate": 3.59993280812876e-05,
        "epoch": 1.1493333333333333,
        "step": 8620
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.8904829025268555,
        "learning_rate": 3.595098209443929e-05,
        "epoch": 1.1494666666666666,
        "step": 8621
    },
    {
        "loss": 0.8138,
        "grad_norm": 4.228653907775879,
        "learning_rate": 3.5902661477625254e-05,
        "epoch": 1.1496,
        "step": 8622
    },
    {
        "loss": 1.4632,
        "grad_norm": 4.883490562438965,
        "learning_rate": 3.5854366249985315e-05,
        "epoch": 1.1497333333333333,
        "step": 8623
    },
    {
        "loss": 1.5443,
        "grad_norm": 3.097527503967285,
        "learning_rate": 3.5806096430649506e-05,
        "epoch": 1.1498666666666666,
        "step": 8624
    },
    {
        "loss": 1.2532,
        "grad_norm": 5.475090980529785,
        "learning_rate": 3.575785203873763e-05,
        "epoch": 1.15,
        "step": 8625
    },
    {
        "loss": 2.9156,
        "grad_norm": 4.482868671417236,
        "learning_rate": 3.570963309335956e-05,
        "epoch": 1.1501333333333332,
        "step": 8626
    },
    {
        "loss": 1.3949,
        "grad_norm": 3.784336805343628,
        "learning_rate": 3.566143961361473e-05,
        "epoch": 1.1502666666666665,
        "step": 8627
    },
    {
        "loss": 2.0198,
        "grad_norm": 3.844024181365967,
        "learning_rate": 3.561327161859303e-05,
        "epoch": 1.1504,
        "step": 8628
    },
    {
        "loss": 2.7676,
        "grad_norm": 3.4964590072631836,
        "learning_rate": 3.55651291273737e-05,
        "epoch": 1.1505333333333334,
        "step": 8629
    },
    {
        "loss": 2.1908,
        "grad_norm": 3.0988235473632812,
        "learning_rate": 3.551701215902644e-05,
        "epoch": 1.1506666666666667,
        "step": 8630
    },
    {
        "loss": 2.6862,
        "grad_norm": 2.917879581451416,
        "learning_rate": 3.546892073261028e-05,
        "epoch": 1.1508,
        "step": 8631
    },
    {
        "loss": 0.5951,
        "grad_norm": 2.510500192642212,
        "learning_rate": 3.5420854867174516e-05,
        "epoch": 1.1509333333333334,
        "step": 8632
    },
    {
        "loss": 1.905,
        "grad_norm": 2.9639272689819336,
        "learning_rate": 3.537281458175814e-05,
        "epoch": 1.1510666666666667,
        "step": 8633
    },
    {
        "loss": 2.6114,
        "grad_norm": 2.7331461906433105,
        "learning_rate": 3.532479989539023e-05,
        "epoch": 1.1512,
        "step": 8634
    },
    {
        "loss": 2.5465,
        "grad_norm": 3.5042924880981445,
        "learning_rate": 3.527681082708942e-05,
        "epoch": 1.1513333333333333,
        "step": 8635
    },
    {
        "loss": 2.33,
        "grad_norm": 3.572056770324707,
        "learning_rate": 3.52288473958643e-05,
        "epoch": 1.1514666666666666,
        "step": 8636
    },
    {
        "loss": 2.2702,
        "grad_norm": 3.775343894958496,
        "learning_rate": 3.518090962071352e-05,
        "epoch": 1.1516,
        "step": 8637
    },
    {
        "loss": 1.5462,
        "grad_norm": 3.698932409286499,
        "learning_rate": 3.5132997520625185e-05,
        "epoch": 1.1517333333333333,
        "step": 8638
    },
    {
        "loss": 1.9272,
        "grad_norm": 3.7831625938415527,
        "learning_rate": 3.508511111457775e-05,
        "epoch": 1.1518666666666666,
        "step": 8639
    },
    {
        "loss": 2.5555,
        "grad_norm": 1.8083876371383667,
        "learning_rate": 3.503725042153888e-05,
        "epoch": 1.152,
        "step": 8640
    },
    {
        "loss": 2.6293,
        "grad_norm": 2.495790958404541,
        "learning_rate": 3.4989415460466465e-05,
        "epoch": 1.1521333333333335,
        "step": 8641
    },
    {
        "loss": 2.0743,
        "grad_norm": 2.935563325881958,
        "learning_rate": 3.494160625030812e-05,
        "epoch": 1.1522666666666668,
        "step": 8642
    },
    {
        "loss": 2.5092,
        "grad_norm": 2.570582151412964,
        "learning_rate": 3.4893822810001244e-05,
        "epoch": 1.1524,
        "step": 8643
    },
    {
        "loss": 2.3039,
        "grad_norm": 2.483414649963379,
        "learning_rate": 3.484606515847304e-05,
        "epoch": 1.1525333333333334,
        "step": 8644
    },
    {
        "loss": 2.4478,
        "grad_norm": 2.644538164138794,
        "learning_rate": 3.479833331464029e-05,
        "epoch": 1.1526666666666667,
        "step": 8645
    },
    {
        "loss": 2.952,
        "grad_norm": 2.0403504371643066,
        "learning_rate": 3.475062729740999e-05,
        "epoch": 1.1528,
        "step": 8646
    },
    {
        "loss": 1.6792,
        "grad_norm": 3.833890438079834,
        "learning_rate": 3.470294712567857e-05,
        "epoch": 1.1529333333333334,
        "step": 8647
    },
    {
        "loss": 2.5927,
        "grad_norm": 2.3653059005737305,
        "learning_rate": 3.4655292818332196e-05,
        "epoch": 1.1530666666666667,
        "step": 8648
    },
    {
        "loss": 2.2396,
        "grad_norm": 2.796071767807007,
        "learning_rate": 3.460766439424703e-05,
        "epoch": 1.1532,
        "step": 8649
    },
    {
        "loss": 1.7073,
        "grad_norm": 3.595086097717285,
        "learning_rate": 3.4560061872288815e-05,
        "epoch": 1.1533333333333333,
        "step": 8650
    },
    {
        "loss": 1.9264,
        "grad_norm": 3.814983606338501,
        "learning_rate": 3.4512485271313175e-05,
        "epoch": 1.1534666666666666,
        "step": 8651
    },
    {
        "loss": 0.6712,
        "grad_norm": 3.739166021347046,
        "learning_rate": 3.446493461016509e-05,
        "epoch": 1.1536,
        "step": 8652
    },
    {
        "loss": 0.9936,
        "grad_norm": 3.8454957008361816,
        "learning_rate": 3.4417409907679854e-05,
        "epoch": 1.1537333333333333,
        "step": 8653
    },
    {
        "loss": 2.3758,
        "grad_norm": 3.2099406719207764,
        "learning_rate": 3.436991118268188e-05,
        "epoch": 1.1538666666666666,
        "step": 8654
    },
    {
        "loss": 2.5847,
        "grad_norm": 3.91455340385437,
        "learning_rate": 3.432243845398588e-05,
        "epoch": 1.154,
        "step": 8655
    },
    {
        "loss": 1.6889,
        "grad_norm": 2.34305477142334,
        "learning_rate": 3.42749917403957e-05,
        "epoch": 1.1541333333333332,
        "step": 8656
    },
    {
        "loss": 2.2065,
        "grad_norm": 3.5715200901031494,
        "learning_rate": 3.422757106070526e-05,
        "epoch": 1.1542666666666666,
        "step": 8657
    },
    {
        "loss": 1.8974,
        "grad_norm": 3.3460466861724854,
        "learning_rate": 3.418017643369801e-05,
        "epoch": 1.1544,
        "step": 8658
    },
    {
        "loss": 2.1155,
        "grad_norm": 2.2484776973724365,
        "learning_rate": 3.4132807878147136e-05,
        "epoch": 1.1545333333333334,
        "step": 8659
    },
    {
        "loss": 1.5454,
        "grad_norm": 5.236197471618652,
        "learning_rate": 3.4085465412815624e-05,
        "epoch": 1.1546666666666667,
        "step": 8660
    },
    {
        "loss": 2.187,
        "grad_norm": 3.0392343997955322,
        "learning_rate": 3.4038149056455724e-05,
        "epoch": 1.1548,
        "step": 8661
    },
    {
        "loss": 1.3359,
        "grad_norm": 3.7266156673431396,
        "learning_rate": 3.399085882780981e-05,
        "epoch": 1.1549333333333334,
        "step": 8662
    },
    {
        "loss": 2.0651,
        "grad_norm": 3.07249116897583,
        "learning_rate": 3.394359474560954e-05,
        "epoch": 1.1550666666666667,
        "step": 8663
    },
    {
        "loss": 1.3728,
        "grad_norm": 3.905848741531372,
        "learning_rate": 3.389635682857665e-05,
        "epoch": 1.1552,
        "step": 8664
    },
    {
        "loss": 3.0583,
        "grad_norm": 3.341336727142334,
        "learning_rate": 3.384914509542194e-05,
        "epoch": 1.1553333333333333,
        "step": 8665
    },
    {
        "loss": 1.7018,
        "grad_norm": 2.7965590953826904,
        "learning_rate": 3.380195956484627e-05,
        "epoch": 1.1554666666666666,
        "step": 8666
    },
    {
        "loss": 2.3118,
        "grad_norm": 3.0755653381347656,
        "learning_rate": 3.3754800255539985e-05,
        "epoch": 1.1556,
        "step": 8667
    },
    {
        "loss": 2.7082,
        "grad_norm": 2.384904384613037,
        "learning_rate": 3.370766718618308e-05,
        "epoch": 1.1557333333333333,
        "step": 8668
    },
    {
        "loss": 1.7321,
        "grad_norm": 4.52751350402832,
        "learning_rate": 3.3660560375445116e-05,
        "epoch": 1.1558666666666666,
        "step": 8669
    },
    {
        "loss": 2.4602,
        "grad_norm": 3.2086665630340576,
        "learning_rate": 3.361347984198511e-05,
        "epoch": 1.156,
        "step": 8670
    },
    {
        "loss": 1.8711,
        "grad_norm": 4.736686706542969,
        "learning_rate": 3.3566425604452035e-05,
        "epoch": 1.1561333333333335,
        "step": 8671
    },
    {
        "loss": 1.9519,
        "grad_norm": 4.254754543304443,
        "learning_rate": 3.3519397681484174e-05,
        "epoch": 1.1562666666666668,
        "step": 8672
    },
    {
        "loss": 1.8723,
        "grad_norm": 4.097896575927734,
        "learning_rate": 3.3472396091709325e-05,
        "epoch": 1.1564,
        "step": 8673
    },
    {
        "loss": 0.8339,
        "grad_norm": 3.4122183322906494,
        "learning_rate": 3.34254208537451e-05,
        "epoch": 1.1565333333333334,
        "step": 8674
    },
    {
        "loss": 0.709,
        "grad_norm": 3.4449236392974854,
        "learning_rate": 3.337847198619852e-05,
        "epoch": 1.1566666666666667,
        "step": 8675
    },
    {
        "loss": 0.5149,
        "grad_norm": 2.455420970916748,
        "learning_rate": 3.333154950766621e-05,
        "epoch": 1.1568,
        "step": 8676
    },
    {
        "loss": 2.0709,
        "grad_norm": 3.4875481128692627,
        "learning_rate": 3.328465343673432e-05,
        "epoch": 1.1569333333333334,
        "step": 8677
    },
    {
        "loss": 2.1408,
        "grad_norm": 2.7885935306549072,
        "learning_rate": 3.323778379197857e-05,
        "epoch": 1.1570666666666667,
        "step": 8678
    },
    {
        "loss": 1.6103,
        "grad_norm": 2.984391212463379,
        "learning_rate": 3.319094059196403e-05,
        "epoch": 1.1572,
        "step": 8679
    },
    {
        "loss": 1.1382,
        "grad_norm": 2.972867250442505,
        "learning_rate": 3.3144123855245646e-05,
        "epoch": 1.1573333333333333,
        "step": 8680
    },
    {
        "loss": 2.4032,
        "grad_norm": 4.455969333648682,
        "learning_rate": 3.309733360036765e-05,
        "epoch": 1.1574666666666666,
        "step": 8681
    },
    {
        "loss": 2.9906,
        "grad_norm": 2.068385601043701,
        "learning_rate": 3.3050569845863723e-05,
        "epoch": 1.1576,
        "step": 8682
    },
    {
        "loss": 0.941,
        "grad_norm": 4.335609436035156,
        "learning_rate": 3.300383261025719e-05,
        "epoch": 1.1577333333333333,
        "step": 8683
    },
    {
        "loss": 2.1314,
        "grad_norm": 2.7159552574157715,
        "learning_rate": 3.295712191206082e-05,
        "epoch": 1.1578666666666666,
        "step": 8684
    },
    {
        "loss": 2.3684,
        "grad_norm": 4.2786383628845215,
        "learning_rate": 3.291043776977703e-05,
        "epoch": 1.158,
        "step": 8685
    },
    {
        "loss": 2.1918,
        "grad_norm": 3.213900327682495,
        "learning_rate": 3.286378020189731e-05,
        "epoch": 1.1581333333333332,
        "step": 8686
    },
    {
        "loss": 1.4088,
        "grad_norm": 2.7190003395080566,
        "learning_rate": 3.2817149226903086e-05,
        "epoch": 1.1582666666666666,
        "step": 8687
    },
    {
        "loss": 1.8212,
        "grad_norm": 3.1581943035125732,
        "learning_rate": 3.2770544863264864e-05,
        "epoch": 1.1584,
        "step": 8688
    },
    {
        "loss": 2.2602,
        "grad_norm": 2.562849521636963,
        "learning_rate": 3.272396712944307e-05,
        "epoch": 1.1585333333333334,
        "step": 8689
    },
    {
        "loss": 2.1617,
        "grad_norm": 4.959146976470947,
        "learning_rate": 3.267741604388703e-05,
        "epoch": 1.1586666666666667,
        "step": 8690
    },
    {
        "loss": 3.3463,
        "grad_norm": 3.485428810119629,
        "learning_rate": 3.2630891625035907e-05,
        "epoch": 1.1588,
        "step": 8691
    },
    {
        "loss": 2.3237,
        "grad_norm": 2.971108913421631,
        "learning_rate": 3.2584393891318154e-05,
        "epoch": 1.1589333333333334,
        "step": 8692
    },
    {
        "loss": 1.5353,
        "grad_norm": 4.367353916168213,
        "learning_rate": 3.2537922861151725e-05,
        "epoch": 1.1590666666666667,
        "step": 8693
    },
    {
        "loss": 2.11,
        "grad_norm": 2.5483062267303467,
        "learning_rate": 3.2491478552943954e-05,
        "epoch": 1.1592,
        "step": 8694
    },
    {
        "loss": 0.8156,
        "grad_norm": 3.190666675567627,
        "learning_rate": 3.244506098509143e-05,
        "epoch": 1.1593333333333333,
        "step": 8695
    },
    {
        "loss": 2.1727,
        "grad_norm": 3.608109712600708,
        "learning_rate": 3.239867017598054e-05,
        "epoch": 1.1594666666666666,
        "step": 8696
    },
    {
        "loss": 2.2704,
        "grad_norm": 2.884638786315918,
        "learning_rate": 3.2352306143986665e-05,
        "epoch": 1.1596,
        "step": 8697
    },
    {
        "loss": 1.8732,
        "grad_norm": 3.154874086380005,
        "learning_rate": 3.230596890747497e-05,
        "epoch": 1.1597333333333333,
        "step": 8698
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.196178674697876,
        "learning_rate": 3.2259658484799515e-05,
        "epoch": 1.1598666666666666,
        "step": 8699
    },
    {
        "loss": 2.5959,
        "grad_norm": 3.1826274394989014,
        "learning_rate": 3.221337489430419e-05,
        "epoch": 1.16,
        "step": 8700
    },
    {
        "loss": 2.4116,
        "grad_norm": 3.32407808303833,
        "learning_rate": 3.216711815432205e-05,
        "epoch": 1.1601333333333332,
        "step": 8701
    },
    {
        "loss": 1.603,
        "grad_norm": 3.8977584838867188,
        "learning_rate": 3.212088828317556e-05,
        "epoch": 1.1602666666666668,
        "step": 8702
    },
    {
        "loss": 1.8877,
        "grad_norm": 4.498214244842529,
        "learning_rate": 3.207468529917657e-05,
        "epoch": 1.1604,
        "step": 8703
    },
    {
        "loss": 2.3973,
        "grad_norm": 5.245824337005615,
        "learning_rate": 3.202850922062603e-05,
        "epoch": 1.1605333333333334,
        "step": 8704
    },
    {
        "loss": 2.0415,
        "grad_norm": 3.4626710414886475,
        "learning_rate": 3.198236006581474e-05,
        "epoch": 1.1606666666666667,
        "step": 8705
    },
    {
        "loss": 2.821,
        "grad_norm": 2.367248058319092,
        "learning_rate": 3.193623785302237e-05,
        "epoch": 1.1608,
        "step": 8706
    },
    {
        "loss": 2.5016,
        "grad_norm": 3.851269006729126,
        "learning_rate": 3.189014260051806e-05,
        "epoch": 1.1609333333333334,
        "step": 8707
    },
    {
        "loss": 1.7129,
        "grad_norm": 2.0576212406158447,
        "learning_rate": 3.184407432656034e-05,
        "epoch": 1.1610666666666667,
        "step": 8708
    },
    {
        "loss": 1.5294,
        "grad_norm": 1.6851527690887451,
        "learning_rate": 3.179803304939699e-05,
        "epoch": 1.1612,
        "step": 8709
    },
    {
        "loss": 2.159,
        "grad_norm": 4.10228967666626,
        "learning_rate": 3.175201878726527e-05,
        "epoch": 1.1613333333333333,
        "step": 8710
    },
    {
        "loss": 1.2762,
        "grad_norm": 3.5410513877868652,
        "learning_rate": 3.170603155839133e-05,
        "epoch": 1.1614666666666666,
        "step": 8711
    },
    {
        "loss": 2.2941,
        "grad_norm": 3.040450096130371,
        "learning_rate": 3.1660071380991055e-05,
        "epoch": 1.1616,
        "step": 8712
    },
    {
        "loss": 1.7615,
        "grad_norm": 2.43677020072937,
        "learning_rate": 3.1614138273269265e-05,
        "epoch": 1.1617333333333333,
        "step": 8713
    },
    {
        "loss": 1.7639,
        "grad_norm": 3.8526575565338135,
        "learning_rate": 3.15682322534205e-05,
        "epoch": 1.1618666666666666,
        "step": 8714
    },
    {
        "loss": 2.4773,
        "grad_norm": 2.186168670654297,
        "learning_rate": 3.152235333962802e-05,
        "epoch": 1.162,
        "step": 8715
    },
    {
        "loss": 2.1881,
        "grad_norm": 2.9484989643096924,
        "learning_rate": 3.1476501550064706e-05,
        "epoch": 1.1621333333333332,
        "step": 8716
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.4699337482452393,
        "learning_rate": 3.143067690289262e-05,
        "epoch": 1.1622666666666666,
        "step": 8717
    },
    {
        "loss": 2.6186,
        "grad_norm": 6.974758148193359,
        "learning_rate": 3.138487941626305e-05,
        "epoch": 1.1623999999999999,
        "step": 8718
    },
    {
        "loss": 1.6042,
        "grad_norm": 4.395586013793945,
        "learning_rate": 3.1339109108316646e-05,
        "epoch": 1.1625333333333334,
        "step": 8719
    },
    {
        "loss": 2.3851,
        "grad_norm": 2.6678011417388916,
        "learning_rate": 3.129336599718291e-05,
        "epoch": 1.1626666666666667,
        "step": 8720
    },
    {
        "loss": 1.9984,
        "grad_norm": 3.5303823947906494,
        "learning_rate": 3.124765010098113e-05,
        "epoch": 1.1628,
        "step": 8721
    },
    {
        "loss": 2.1444,
        "grad_norm": 5.022760391235352,
        "learning_rate": 3.120196143781934e-05,
        "epoch": 1.1629333333333334,
        "step": 8722
    },
    {
        "loss": 1.1977,
        "grad_norm": 4.577395915985107,
        "learning_rate": 3.1156300025795174e-05,
        "epoch": 1.1630666666666667,
        "step": 8723
    },
    {
        "loss": 1.5728,
        "grad_norm": 4.169539928436279,
        "learning_rate": 3.1110665882995016e-05,
        "epoch": 1.1632,
        "step": 8724
    },
    {
        "loss": 2.7427,
        "grad_norm": 4.1392011642456055,
        "learning_rate": 3.106505902749487e-05,
        "epoch": 1.1633333333333333,
        "step": 8725
    },
    {
        "loss": 1.7345,
        "grad_norm": 3.2409305572509766,
        "learning_rate": 3.1019479477359735e-05,
        "epoch": 1.1634666666666666,
        "step": 8726
    },
    {
        "loss": 1.9464,
        "grad_norm": 3.0563008785247803,
        "learning_rate": 3.097392725064385e-05,
        "epoch": 1.1636,
        "step": 8727
    },
    {
        "loss": 2.9876,
        "grad_norm": 2.369046211242676,
        "learning_rate": 3.092840236539062e-05,
        "epoch": 1.1637333333333333,
        "step": 8728
    },
    {
        "loss": 0.891,
        "grad_norm": 3.611070394515991,
        "learning_rate": 3.088290483963244e-05,
        "epoch": 1.1638666666666666,
        "step": 8729
    },
    {
        "loss": 1.7983,
        "grad_norm": 3.0633633136749268,
        "learning_rate": 3.08374346913913e-05,
        "epoch": 1.164,
        "step": 8730
    },
    {
        "loss": 2.4819,
        "grad_norm": 3.1893503665924072,
        "learning_rate": 3.07919919386779e-05,
        "epoch": 1.1641333333333332,
        "step": 8731
    },
    {
        "loss": 0.7123,
        "grad_norm": 7.953676700592041,
        "learning_rate": 3.074657659949228e-05,
        "epoch": 1.1642666666666668,
        "step": 8732
    },
    {
        "loss": 2.2197,
        "grad_norm": 2.8563175201416016,
        "learning_rate": 3.070118869182366e-05,
        "epoch": 1.1644,
        "step": 8733
    },
    {
        "loss": 2.212,
        "grad_norm": 2.67706036567688,
        "learning_rate": 3.0655828233650287e-05,
        "epoch": 1.1645333333333334,
        "step": 8734
    },
    {
        "loss": 2.2825,
        "grad_norm": 3.295006513595581,
        "learning_rate": 3.061049524293976e-05,
        "epoch": 1.1646666666666667,
        "step": 8735
    },
    {
        "loss": 2.4166,
        "grad_norm": 2.682420492172241,
        "learning_rate": 3.056518973764838e-05,
        "epoch": 1.1648,
        "step": 8736
    },
    {
        "loss": 2.3744,
        "grad_norm": 3.065645456314087,
        "learning_rate": 3.0519911735722005e-05,
        "epoch": 1.1649333333333334,
        "step": 8737
    },
    {
        "loss": 1.9777,
        "grad_norm": 4.402763366699219,
        "learning_rate": 3.047466125509525e-05,
        "epoch": 1.1650666666666667,
        "step": 8738
    },
    {
        "loss": 2.2386,
        "grad_norm": 3.3870511054992676,
        "learning_rate": 3.0429438313692192e-05,
        "epoch": 1.1652,
        "step": 8739
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.9707868099212646,
        "learning_rate": 3.0384242929425642e-05,
        "epoch": 1.1653333333333333,
        "step": 8740
    },
    {
        "loss": 0.9485,
        "grad_norm": 3.911499500274658,
        "learning_rate": 3.0339075120197645e-05,
        "epoch": 1.1654666666666667,
        "step": 8741
    },
    {
        "loss": 1.055,
        "grad_norm": 5.8377790451049805,
        "learning_rate": 3.0293934903899356e-05,
        "epoch": 1.1656,
        "step": 8742
    },
    {
        "loss": 2.2555,
        "grad_norm": 2.406170129776001,
        "learning_rate": 3.0248822298410972e-05,
        "epoch": 1.1657333333333333,
        "step": 8743
    },
    {
        "loss": 1.5312,
        "grad_norm": 3.360194444656372,
        "learning_rate": 3.0203737321601844e-05,
        "epoch": 1.1658666666666666,
        "step": 8744
    },
    {
        "loss": 0.9109,
        "grad_norm": 2.947472333908081,
        "learning_rate": 3.015867999133003e-05,
        "epoch": 1.166,
        "step": 8745
    },
    {
        "loss": 1.3937,
        "grad_norm": 3.386744976043701,
        "learning_rate": 3.0113650325443143e-05,
        "epoch": 1.1661333333333332,
        "step": 8746
    },
    {
        "loss": 2.5733,
        "grad_norm": 5.07934045791626,
        "learning_rate": 3.006864834177743e-05,
        "epoch": 1.1662666666666666,
        "step": 8747
    },
    {
        "loss": 1.831,
        "grad_norm": 6.77891206741333,
        "learning_rate": 3.0023674058158523e-05,
        "epoch": 1.1663999999999999,
        "step": 8748
    },
    {
        "loss": 2.4459,
        "grad_norm": 2.6203866004943848,
        "learning_rate": 2.9978727492400627e-05,
        "epoch": 1.1665333333333334,
        "step": 8749
    },
    {
        "loss": 1.7595,
        "grad_norm": 4.552544593811035,
        "learning_rate": 2.9933808662307394e-05,
        "epoch": 1.1666666666666667,
        "step": 8750
    },
    {
        "loss": 0.8758,
        "grad_norm": 3.9378490447998047,
        "learning_rate": 2.9888917585671307e-05,
        "epoch": 1.1668,
        "step": 8751
    },
    {
        "loss": 2.0893,
        "grad_norm": 4.24724817276001,
        "learning_rate": 2.9844054280273914e-05,
        "epoch": 1.1669333333333334,
        "step": 8752
    },
    {
        "loss": 2.3033,
        "grad_norm": 2.8958547115325928,
        "learning_rate": 2.979921876388564e-05,
        "epoch": 1.1670666666666667,
        "step": 8753
    },
    {
        "loss": 2.7082,
        "grad_norm": 2.167694330215454,
        "learning_rate": 2.975441105426593e-05,
        "epoch": 1.1672,
        "step": 8754
    },
    {
        "loss": 1.5708,
        "grad_norm": 2.3586931228637695,
        "learning_rate": 2.9709631169163477e-05,
        "epoch": 1.1673333333333333,
        "step": 8755
    },
    {
        "loss": 2.7185,
        "grad_norm": 2.450760841369629,
        "learning_rate": 2.966487912631559e-05,
        "epoch": 1.1674666666666667,
        "step": 8756
    },
    {
        "loss": 2.3974,
        "grad_norm": 3.2925937175750732,
        "learning_rate": 2.962015494344873e-05,
        "epoch": 1.1676,
        "step": 8757
    },
    {
        "loss": 2.1617,
        "grad_norm": 2.649343490600586,
        "learning_rate": 2.9575458638278296e-05,
        "epoch": 1.1677333333333333,
        "step": 8758
    },
    {
        "loss": 2.1288,
        "grad_norm": 2.6027133464813232,
        "learning_rate": 2.9530790228508664e-05,
        "epoch": 1.1678666666666666,
        "step": 8759
    },
    {
        "loss": 1.9022,
        "grad_norm": 3.99946665763855,
        "learning_rate": 2.94861497318331e-05,
        "epoch": 1.168,
        "step": 8760
    },
    {
        "loss": 1.9433,
        "grad_norm": 4.951265811920166,
        "learning_rate": 2.944153716593401e-05,
        "epoch": 1.1681333333333332,
        "step": 8761
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.9070017337799072,
        "learning_rate": 2.9396952548482424e-05,
        "epoch": 1.1682666666666668,
        "step": 8762
    },
    {
        "loss": 2.4056,
        "grad_norm": 3.6570944786071777,
        "learning_rate": 2.935239589713842e-05,
        "epoch": 1.1684,
        "step": 8763
    },
    {
        "loss": 1.6717,
        "grad_norm": 3.29463791847229,
        "learning_rate": 2.9307867229551245e-05,
        "epoch": 1.1685333333333334,
        "step": 8764
    },
    {
        "loss": 1.7346,
        "grad_norm": 3.6649892330169678,
        "learning_rate": 2.9263366563358708e-05,
        "epoch": 1.1686666666666667,
        "step": 8765
    },
    {
        "loss": 1.8104,
        "grad_norm": 3.5478827953338623,
        "learning_rate": 2.9218893916187685e-05,
        "epoch": 1.1688,
        "step": 8766
    },
    {
        "loss": 2.1133,
        "grad_norm": 3.9444594383239746,
        "learning_rate": 2.917444930565395e-05,
        "epoch": 1.1689333333333334,
        "step": 8767
    },
    {
        "loss": 1.6083,
        "grad_norm": 3.4991159439086914,
        "learning_rate": 2.913003274936218e-05,
        "epoch": 1.1690666666666667,
        "step": 8768
    },
    {
        "loss": 2.8653,
        "grad_norm": 2.9315943717956543,
        "learning_rate": 2.908564426490602e-05,
        "epoch": 1.1692,
        "step": 8769
    },
    {
        "loss": 1.9494,
        "grad_norm": 3.4388551712036133,
        "learning_rate": 2.904128386986764e-05,
        "epoch": 1.1693333333333333,
        "step": 8770
    },
    {
        "loss": 2.2536,
        "grad_norm": 5.593690395355225,
        "learning_rate": 2.89969515818186e-05,
        "epoch": 1.1694666666666667,
        "step": 8771
    },
    {
        "loss": 2.608,
        "grad_norm": 4.884703636169434,
        "learning_rate": 2.8952647418318913e-05,
        "epoch": 1.1696,
        "step": 8772
    },
    {
        "loss": 1.5485,
        "grad_norm": 2.3739590644836426,
        "learning_rate": 2.89083713969178e-05,
        "epoch": 1.1697333333333333,
        "step": 8773
    },
    {
        "loss": 2.2479,
        "grad_norm": 2.133800983428955,
        "learning_rate": 2.886412353515289e-05,
        "epoch": 1.1698666666666666,
        "step": 8774
    },
    {
        "loss": 1.9398,
        "grad_norm": 2.853334903717041,
        "learning_rate": 2.8819903850551068e-05,
        "epoch": 1.17,
        "step": 8775
    },
    {
        "loss": 1.8415,
        "grad_norm": 3.0806732177734375,
        "learning_rate": 2.877571236062787e-05,
        "epoch": 1.1701333333333332,
        "step": 8776
    },
    {
        "loss": 1.6195,
        "grad_norm": 3.862536668777466,
        "learning_rate": 2.8731549082887688e-05,
        "epoch": 1.1702666666666666,
        "step": 8777
    },
    {
        "loss": 2.4046,
        "grad_norm": 3.2478713989257812,
        "learning_rate": 2.8687414034823856e-05,
        "epoch": 1.1703999999999999,
        "step": 8778
    },
    {
        "loss": 1.6186,
        "grad_norm": 3.157102346420288,
        "learning_rate": 2.864330723391817e-05,
        "epoch": 1.1705333333333334,
        "step": 8779
    },
    {
        "loss": 2.5406,
        "grad_norm": 3.6428866386413574,
        "learning_rate": 2.8599228697641745e-05,
        "epoch": 1.1706666666666667,
        "step": 8780
    },
    {
        "loss": 1.2568,
        "grad_norm": 3.130195379257202,
        "learning_rate": 2.8555178443454e-05,
        "epoch": 1.1708,
        "step": 8781
    },
    {
        "loss": 2.5734,
        "grad_norm": 3.4662394523620605,
        "learning_rate": 2.851115648880368e-05,
        "epoch": 1.1709333333333334,
        "step": 8782
    },
    {
        "loss": 1.5652,
        "grad_norm": 3.141044855117798,
        "learning_rate": 2.846716285112776e-05,
        "epoch": 1.1710666666666667,
        "step": 8783
    },
    {
        "loss": 2.2031,
        "grad_norm": 3.551435947418213,
        "learning_rate": 2.8423197547852387e-05,
        "epoch": 1.1712,
        "step": 8784
    },
    {
        "loss": 1.0968,
        "grad_norm": 3.606778860092163,
        "learning_rate": 2.8379260596392322e-05,
        "epoch": 1.1713333333333333,
        "step": 8785
    },
    {
        "loss": 1.88,
        "grad_norm": 3.583446502685547,
        "learning_rate": 2.8335352014151273e-05,
        "epoch": 1.1714666666666667,
        "step": 8786
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.761770248413086,
        "learning_rate": 2.8291471818521455e-05,
        "epoch": 1.1716,
        "step": 8787
    },
    {
        "loss": 1.6358,
        "grad_norm": 3.065809726715088,
        "learning_rate": 2.8247620026883892e-05,
        "epoch": 1.1717333333333333,
        "step": 8788
    },
    {
        "loss": 2.3993,
        "grad_norm": 3.2752838134765625,
        "learning_rate": 2.820379665660865e-05,
        "epoch": 1.1718666666666666,
        "step": 8789
    },
    {
        "loss": 2.1098,
        "grad_norm": 3.9350814819335938,
        "learning_rate": 2.8160001725054163e-05,
        "epoch": 1.172,
        "step": 8790
    },
    {
        "loss": 1.3574,
        "grad_norm": 2.4783926010131836,
        "learning_rate": 2.8116235249567747e-05,
        "epoch": 1.1721333333333332,
        "step": 8791
    },
    {
        "loss": 0.6509,
        "grad_norm": 3.299473762512207,
        "learning_rate": 2.8072497247485485e-05,
        "epoch": 1.1722666666666668,
        "step": 8792
    },
    {
        "loss": 2.8469,
        "grad_norm": 3.3560383319854736,
        "learning_rate": 2.8028787736132156e-05,
        "epoch": 1.1724,
        "step": 8793
    },
    {
        "loss": 2.0612,
        "grad_norm": 2.9506125450134277,
        "learning_rate": 2.7985106732821332e-05,
        "epoch": 1.1725333333333334,
        "step": 8794
    },
    {
        "loss": 0.8654,
        "grad_norm": 4.825831413269043,
        "learning_rate": 2.794145425485496e-05,
        "epoch": 1.1726666666666667,
        "step": 8795
    },
    {
        "loss": 1.2009,
        "grad_norm": 2.804807424545288,
        "learning_rate": 2.7897830319524187e-05,
        "epoch": 1.1728,
        "step": 8796
    },
    {
        "loss": 2.1594,
        "grad_norm": 2.933990240097046,
        "learning_rate": 2.7854234944108437e-05,
        "epoch": 1.1729333333333334,
        "step": 8797
    },
    {
        "loss": 1.4956,
        "grad_norm": 6.922976970672607,
        "learning_rate": 2.781066814587613e-05,
        "epoch": 1.1730666666666667,
        "step": 8798
    },
    {
        "loss": 1.7416,
        "grad_norm": 2.4005286693573,
        "learning_rate": 2.776712994208417e-05,
        "epoch": 1.1732,
        "step": 8799
    },
    {
        "loss": 2.0315,
        "grad_norm": 2.925417900085449,
        "learning_rate": 2.7723620349978074e-05,
        "epoch": 1.1733333333333333,
        "step": 8800
    },
    {
        "loss": 1.9039,
        "grad_norm": 3.4715416431427,
        "learning_rate": 2.768013938679227e-05,
        "epoch": 1.1734666666666667,
        "step": 8801
    },
    {
        "loss": 2.2026,
        "grad_norm": 3.4363350868225098,
        "learning_rate": 2.7636687069749677e-05,
        "epoch": 1.1736,
        "step": 8802
    },
    {
        "loss": 1.4434,
        "grad_norm": 2.929696559906006,
        "learning_rate": 2.7593263416062e-05,
        "epoch": 1.1737333333333333,
        "step": 8803
    },
    {
        "loss": 0.7305,
        "grad_norm": 3.454455614089966,
        "learning_rate": 2.754986844292925e-05,
        "epoch": 1.1738666666666666,
        "step": 8804
    },
    {
        "loss": 2.1869,
        "grad_norm": 3.6014211177825928,
        "learning_rate": 2.7506502167540603e-05,
        "epoch": 1.174,
        "step": 8805
    },
    {
        "loss": 1.9343,
        "grad_norm": 3.465632200241089,
        "learning_rate": 2.7463164607073345e-05,
        "epoch": 1.1741333333333333,
        "step": 8806
    },
    {
        "loss": 2.4893,
        "grad_norm": 3.700404167175293,
        "learning_rate": 2.7419855778693894e-05,
        "epoch": 1.1742666666666666,
        "step": 8807
    },
    {
        "loss": 1.778,
        "grad_norm": 2.6194727420806885,
        "learning_rate": 2.7376575699556783e-05,
        "epoch": 1.1743999999999999,
        "step": 8808
    },
    {
        "loss": 2.8017,
        "grad_norm": 4.149969100952148,
        "learning_rate": 2.7333324386805504e-05,
        "epoch": 1.1745333333333334,
        "step": 8809
    },
    {
        "loss": 2.0514,
        "grad_norm": 3.496234893798828,
        "learning_rate": 2.7290101857572013e-05,
        "epoch": 1.1746666666666667,
        "step": 8810
    },
    {
        "loss": 2.9738,
        "grad_norm": 3.249433755874634,
        "learning_rate": 2.7246908128977012e-05,
        "epoch": 1.1748,
        "step": 8811
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.179868459701538,
        "learning_rate": 2.7203743218129574e-05,
        "epoch": 1.1749333333333334,
        "step": 8812
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.6836743354797363,
        "learning_rate": 2.7160607142127437e-05,
        "epoch": 1.1750666666666667,
        "step": 8813
    },
    {
        "loss": 2.0399,
        "grad_norm": 2.922149419784546,
        "learning_rate": 2.7117499918057066e-05,
        "epoch": 1.1752,
        "step": 8814
    },
    {
        "loss": 3.0238,
        "grad_norm": 3.319213390350342,
        "learning_rate": 2.7074421562993313e-05,
        "epoch": 1.1753333333333333,
        "step": 8815
    },
    {
        "loss": 2.4852,
        "grad_norm": 1.964084506034851,
        "learning_rate": 2.703137209399964e-05,
        "epoch": 1.1754666666666667,
        "step": 8816
    },
    {
        "loss": 2.1958,
        "grad_norm": 2.3772685527801514,
        "learning_rate": 2.698835152812812e-05,
        "epoch": 1.1756,
        "step": 8817
    },
    {
        "loss": 0.6619,
        "grad_norm": 3.4864661693573,
        "learning_rate": 2.694535988241933e-05,
        "epoch": 1.1757333333333333,
        "step": 8818
    },
    {
        "loss": 3.5441,
        "grad_norm": 5.092090129852295,
        "learning_rate": 2.6902397173902427e-05,
        "epoch": 1.1758666666666666,
        "step": 8819
    },
    {
        "loss": 2.1691,
        "grad_norm": 3.3496971130371094,
        "learning_rate": 2.68594634195951e-05,
        "epoch": 1.176,
        "step": 8820
    },
    {
        "loss": 1.4941,
        "grad_norm": 3.3183043003082275,
        "learning_rate": 2.681655863650354e-05,
        "epoch": 1.1761333333333333,
        "step": 8821
    },
    {
        "loss": 2.1144,
        "grad_norm": 3.110076665878296,
        "learning_rate": 2.6773682841622404e-05,
        "epoch": 1.1762666666666666,
        "step": 8822
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.4924228191375732,
        "learning_rate": 2.673083605193507e-05,
        "epoch": 1.1764000000000001,
        "step": 8823
    },
    {
        "loss": 1.8895,
        "grad_norm": 2.1455130577087402,
        "learning_rate": 2.6688018284413262e-05,
        "epoch": 1.1765333333333334,
        "step": 8824
    },
    {
        "loss": 1.5495,
        "grad_norm": 3.229579210281372,
        "learning_rate": 2.6645229556017136e-05,
        "epoch": 1.1766666666666667,
        "step": 8825
    },
    {
        "loss": 1.6498,
        "grad_norm": 5.237555027008057,
        "learning_rate": 2.6602469883695545e-05,
        "epoch": 1.1768,
        "step": 8826
    },
    {
        "loss": 2.4357,
        "grad_norm": 3.7907865047454834,
        "learning_rate": 2.6559739284385745e-05,
        "epoch": 1.1769333333333334,
        "step": 8827
    },
    {
        "loss": 1.2895,
        "grad_norm": 4.526031017303467,
        "learning_rate": 2.6517037775013542e-05,
        "epoch": 1.1770666666666667,
        "step": 8828
    },
    {
        "loss": 2.2205,
        "grad_norm": 3.761620283126831,
        "learning_rate": 2.6474365372492916e-05,
        "epoch": 1.1772,
        "step": 8829
    },
    {
        "loss": 2.2584,
        "grad_norm": 3.941009283065796,
        "learning_rate": 2.6431722093726808e-05,
        "epoch": 1.1773333333333333,
        "step": 8830
    },
    {
        "loss": 1.2522,
        "grad_norm": 3.080340623855591,
        "learning_rate": 2.638910795560614e-05,
        "epoch": 1.1774666666666667,
        "step": 8831
    },
    {
        "loss": 1.5197,
        "grad_norm": 2.838059902191162,
        "learning_rate": 2.634652297501079e-05,
        "epoch": 1.1776,
        "step": 8832
    },
    {
        "loss": 2.5451,
        "grad_norm": 4.694859027862549,
        "learning_rate": 2.6303967168808564e-05,
        "epoch": 1.1777333333333333,
        "step": 8833
    },
    {
        "loss": 1.711,
        "grad_norm": 2.865626096725464,
        "learning_rate": 2.6261440553856055e-05,
        "epoch": 1.1778666666666666,
        "step": 8834
    },
    {
        "loss": 2.248,
        "grad_norm": 3.7486886978149414,
        "learning_rate": 2.6218943146998175e-05,
        "epoch": 1.178,
        "step": 8835
    },
    {
        "loss": 0.5659,
        "grad_norm": 2.6872167587280273,
        "learning_rate": 2.617647496506841e-05,
        "epoch": 1.1781333333333333,
        "step": 8836
    },
    {
        "loss": 2.1318,
        "grad_norm": 2.9475269317626953,
        "learning_rate": 2.613403602488842e-05,
        "epoch": 1.1782666666666666,
        "step": 8837
    },
    {
        "loss": 2.2981,
        "grad_norm": 4.399754047393799,
        "learning_rate": 2.6091626343268393e-05,
        "epoch": 1.1784,
        "step": 8838
    },
    {
        "loss": 1.4422,
        "grad_norm": 2.559467315673828,
        "learning_rate": 2.604924593700707e-05,
        "epoch": 1.1785333333333332,
        "step": 8839
    },
    {
        "loss": 2.1645,
        "grad_norm": 3.393806219100952,
        "learning_rate": 2.6006894822891326e-05,
        "epoch": 1.1786666666666668,
        "step": 8840
    },
    {
        "loss": 0.7829,
        "grad_norm": 3.154805898666382,
        "learning_rate": 2.5964573017696814e-05,
        "epoch": 1.1788,
        "step": 8841
    },
    {
        "loss": 1.7186,
        "grad_norm": 2.701030969619751,
        "learning_rate": 2.5922280538187106e-05,
        "epoch": 1.1789333333333334,
        "step": 8842
    },
    {
        "loss": 0.7654,
        "grad_norm": 4.116509914398193,
        "learning_rate": 2.5880017401114498e-05,
        "epoch": 1.1790666666666667,
        "step": 8843
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.6688358783721924,
        "learning_rate": 2.5837783623219546e-05,
        "epoch": 1.1792,
        "step": 8844
    },
    {
        "loss": 1.9802,
        "grad_norm": 2.9952449798583984,
        "learning_rate": 2.5795579221231236e-05,
        "epoch": 1.1793333333333333,
        "step": 8845
    },
    {
        "loss": 2.1191,
        "grad_norm": 4.157721042633057,
        "learning_rate": 2.575340421186688e-05,
        "epoch": 1.1794666666666667,
        "step": 8846
    },
    {
        "loss": 2.5704,
        "grad_norm": 3.3505699634552,
        "learning_rate": 2.571125861183198e-05,
        "epoch": 1.1796,
        "step": 8847
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.352302074432373,
        "learning_rate": 2.566914243782076e-05,
        "epoch": 1.1797333333333333,
        "step": 8848
    },
    {
        "loss": 2.3,
        "grad_norm": 3.408012866973877,
        "learning_rate": 2.5627055706515535e-05,
        "epoch": 1.1798666666666666,
        "step": 8849
    },
    {
        "loss": 2.1772,
        "grad_norm": 2.0649144649505615,
        "learning_rate": 2.558499843458687e-05,
        "epoch": 1.18,
        "step": 8850
    },
    {
        "loss": 2.3462,
        "grad_norm": 3.3691978454589844,
        "learning_rate": 2.554297063869392e-05,
        "epoch": 1.1801333333333333,
        "step": 8851
    },
    {
        "loss": 1.7246,
        "grad_norm": 6.556563377380371,
        "learning_rate": 2.5500972335483996e-05,
        "epoch": 1.1802666666666666,
        "step": 8852
    },
    {
        "loss": 2.033,
        "grad_norm": 3.094616174697876,
        "learning_rate": 2.5459003541592886e-05,
        "epoch": 1.1804000000000001,
        "step": 8853
    },
    {
        "loss": 1.0883,
        "grad_norm": 3.000880718231201,
        "learning_rate": 2.541706427364431e-05,
        "epoch": 1.1805333333333334,
        "step": 8854
    },
    {
        "loss": 2.1852,
        "grad_norm": 2.842026472091675,
        "learning_rate": 2.5375154548250825e-05,
        "epoch": 1.1806666666666668,
        "step": 8855
    },
    {
        "loss": 1.6648,
        "grad_norm": 4.9128265380859375,
        "learning_rate": 2.5333274382012783e-05,
        "epoch": 1.1808,
        "step": 8856
    },
    {
        "loss": 2.6609,
        "grad_norm": 2.401885986328125,
        "learning_rate": 2.5291423791519264e-05,
        "epoch": 1.1809333333333334,
        "step": 8857
    },
    {
        "loss": 2.5517,
        "grad_norm": 3.746666431427002,
        "learning_rate": 2.524960279334738e-05,
        "epoch": 1.1810666666666667,
        "step": 8858
    },
    {
        "loss": 2.7217,
        "grad_norm": 3.023250102996826,
        "learning_rate": 2.5207811404062474e-05,
        "epoch": 1.1812,
        "step": 8859
    },
    {
        "loss": 2.34,
        "grad_norm": 4.413723945617676,
        "learning_rate": 2.5166049640218327e-05,
        "epoch": 1.1813333333333333,
        "step": 8860
    },
    {
        "loss": 2.5004,
        "grad_norm": 3.089308023452759,
        "learning_rate": 2.5124317518356878e-05,
        "epoch": 1.1814666666666667,
        "step": 8861
    },
    {
        "loss": 0.8204,
        "grad_norm": 4.315261363983154,
        "learning_rate": 2.5082615055008495e-05,
        "epoch": 1.1816,
        "step": 8862
    },
    {
        "loss": 2.5351,
        "grad_norm": 2.4610188007354736,
        "learning_rate": 2.504094226669146e-05,
        "epoch": 1.1817333333333333,
        "step": 8863
    },
    {
        "loss": 0.9348,
        "grad_norm": 4.91064453125,
        "learning_rate": 2.499929916991266e-05,
        "epoch": 1.1818666666666666,
        "step": 8864
    },
    {
        "loss": 1.8773,
        "grad_norm": 3.3273890018463135,
        "learning_rate": 2.495768578116694e-05,
        "epoch": 1.182,
        "step": 8865
    },
    {
        "loss": 2.1471,
        "grad_norm": 2.9657235145568848,
        "learning_rate": 2.4916102116937722e-05,
        "epoch": 1.1821333333333333,
        "step": 8866
    },
    {
        "loss": 1.8899,
        "grad_norm": 3.03682804107666,
        "learning_rate": 2.4874548193696213e-05,
        "epoch": 1.1822666666666666,
        "step": 8867
    },
    {
        "loss": 2.4273,
        "grad_norm": 3.1494810581207275,
        "learning_rate": 2.483302402790214e-05,
        "epoch": 1.1824,
        "step": 8868
    },
    {
        "loss": 1.5669,
        "grad_norm": 4.29394006729126,
        "learning_rate": 2.4791529636003387e-05,
        "epoch": 1.1825333333333332,
        "step": 8869
    },
    {
        "loss": 1.8401,
        "grad_norm": 5.064544677734375,
        "learning_rate": 2.4750065034436033e-05,
        "epoch": 1.1826666666666668,
        "step": 8870
    },
    {
        "loss": 2.9663,
        "grad_norm": 3.4037117958068848,
        "learning_rate": 2.470863023962435e-05,
        "epoch": 1.1828,
        "step": 8871
    },
    {
        "loss": 0.8881,
        "grad_norm": 3.3655548095703125,
        "learning_rate": 2.4667225267980677e-05,
        "epoch": 1.1829333333333334,
        "step": 8872
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.1374588012695312,
        "learning_rate": 2.4625850135905837e-05,
        "epoch": 1.1830666666666667,
        "step": 8873
    },
    {
        "loss": 1.6455,
        "grad_norm": 3.670201301574707,
        "learning_rate": 2.4584504859788626e-05,
        "epoch": 1.1832,
        "step": 8874
    },
    {
        "loss": 2.2245,
        "grad_norm": 4.443226337432861,
        "learning_rate": 2.4543189456005922e-05,
        "epoch": 1.1833333333333333,
        "step": 8875
    },
    {
        "loss": 2.048,
        "grad_norm": 4.0181450843811035,
        "learning_rate": 2.450190394092302e-05,
        "epoch": 1.1834666666666667,
        "step": 8876
    },
    {
        "loss": 1.2452,
        "grad_norm": 3.5558857917785645,
        "learning_rate": 2.446064833089322e-05,
        "epoch": 1.1836,
        "step": 8877
    },
    {
        "loss": 1.9789,
        "grad_norm": 5.855016708374023,
        "learning_rate": 2.4419422642258015e-05,
        "epoch": 1.1837333333333333,
        "step": 8878
    },
    {
        "loss": 1.7288,
        "grad_norm": 3.080451011657715,
        "learning_rate": 2.4378226891347056e-05,
        "epoch": 1.1838666666666666,
        "step": 8879
    },
    {
        "loss": 1.879,
        "grad_norm": 4.57529878616333,
        "learning_rate": 2.433706109447813e-05,
        "epoch": 1.184,
        "step": 8880
    },
    {
        "loss": 1.717,
        "grad_norm": 4.499853610992432,
        "learning_rate": 2.429592526795701e-05,
        "epoch": 1.1841333333333333,
        "step": 8881
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.936387538909912,
        "learning_rate": 2.4254819428077925e-05,
        "epoch": 1.1842666666666666,
        "step": 8882
    },
    {
        "loss": 1.8319,
        "grad_norm": 4.802548408508301,
        "learning_rate": 2.4213743591122995e-05,
        "epoch": 1.1844000000000001,
        "step": 8883
    },
    {
        "loss": 1.2283,
        "grad_norm": 4.039027690887451,
        "learning_rate": 2.4172697773362407e-05,
        "epoch": 1.1845333333333334,
        "step": 8884
    },
    {
        "loss": 1.7196,
        "grad_norm": 4.8842973709106445,
        "learning_rate": 2.41316819910546e-05,
        "epoch": 1.1846666666666668,
        "step": 8885
    },
    {
        "loss": 0.9123,
        "grad_norm": 4.069790363311768,
        "learning_rate": 2.4090696260446055e-05,
        "epoch": 1.1848,
        "step": 8886
    },
    {
        "loss": 1.4165,
        "grad_norm": 7.267007350921631,
        "learning_rate": 2.4049740597771473e-05,
        "epoch": 1.1849333333333334,
        "step": 8887
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.967390775680542,
        "learning_rate": 2.4008815019253337e-05,
        "epoch": 1.1850666666666667,
        "step": 8888
    },
    {
        "loss": 2.1056,
        "grad_norm": 2.803784132003784,
        "learning_rate": 2.3967919541102556e-05,
        "epoch": 1.1852,
        "step": 8889
    },
    {
        "loss": 2.2932,
        "grad_norm": 4.13126802444458,
        "learning_rate": 2.3927054179517826e-05,
        "epoch": 1.1853333333333333,
        "step": 8890
    },
    {
        "loss": 1.8802,
        "grad_norm": 3.8695578575134277,
        "learning_rate": 2.38862189506863e-05,
        "epoch": 1.1854666666666667,
        "step": 8891
    },
    {
        "loss": 2.5924,
        "grad_norm": 3.3571574687957764,
        "learning_rate": 2.3845413870782707e-05,
        "epoch": 1.1856,
        "step": 8892
    },
    {
        "loss": 2.4259,
        "grad_norm": 3.4008755683898926,
        "learning_rate": 2.380463895597017e-05,
        "epoch": 1.1857333333333333,
        "step": 8893
    },
    {
        "loss": 2.1415,
        "grad_norm": 2.4934051036834717,
        "learning_rate": 2.3763894222399774e-05,
        "epoch": 1.1858666666666666,
        "step": 8894
    },
    {
        "loss": 1.6296,
        "grad_norm": 2.2589080333709717,
        "learning_rate": 2.3723179686210674e-05,
        "epoch": 1.186,
        "step": 8895
    },
    {
        "loss": 1.2903,
        "grad_norm": 5.07000732421875,
        "learning_rate": 2.3682495363530033e-05,
        "epoch": 1.1861333333333333,
        "step": 8896
    },
    {
        "loss": 1.559,
        "grad_norm": 3.465489625930786,
        "learning_rate": 2.3641841270472907e-05,
        "epoch": 1.1862666666666666,
        "step": 8897
    },
    {
        "loss": 1.493,
        "grad_norm": 3.678766965866089,
        "learning_rate": 2.360121742314273e-05,
        "epoch": 1.1864,
        "step": 8898
    },
    {
        "loss": 2.4139,
        "grad_norm": 1.8884117603302002,
        "learning_rate": 2.3560623837630603e-05,
        "epoch": 1.1865333333333332,
        "step": 8899
    },
    {
        "loss": 2.505,
        "grad_norm": 3.1221041679382324,
        "learning_rate": 2.352006053001593e-05,
        "epoch": 1.1866666666666668,
        "step": 8900
    },
    {
        "loss": 1.2064,
        "grad_norm": 5.20829963684082,
        "learning_rate": 2.3479527516365784e-05,
        "epoch": 1.1868,
        "step": 8901
    },
    {
        "loss": 2.6728,
        "grad_norm": 3.246823787689209,
        "learning_rate": 2.3439024812735542e-05,
        "epoch": 1.1869333333333334,
        "step": 8902
    },
    {
        "loss": 2.4519,
        "grad_norm": 3.7820184230804443,
        "learning_rate": 2.3398552435168454e-05,
        "epoch": 1.1870666666666667,
        "step": 8903
    },
    {
        "loss": 2.8014,
        "grad_norm": 2.9194133281707764,
        "learning_rate": 2.3358110399695786e-05,
        "epoch": 1.1872,
        "step": 8904
    },
    {
        "loss": 2.297,
        "grad_norm": 4.17896032333374,
        "learning_rate": 2.3317698722336768e-05,
        "epoch": 1.1873333333333334,
        "step": 8905
    },
    {
        "loss": 0.5215,
        "grad_norm": 2.9459142684936523,
        "learning_rate": 2.3277317419098466e-05,
        "epoch": 1.1874666666666667,
        "step": 8906
    },
    {
        "loss": 2.4515,
        "grad_norm": 3.322847604751587,
        "learning_rate": 2.3236966505976264e-05,
        "epoch": 1.1876,
        "step": 8907
    },
    {
        "loss": 1.2803,
        "grad_norm": 5.2101521492004395,
        "learning_rate": 2.319664599895317e-05,
        "epoch": 1.1877333333333333,
        "step": 8908
    },
    {
        "loss": 1.4116,
        "grad_norm": 3.4350550174713135,
        "learning_rate": 2.315635591400027e-05,
        "epoch": 1.1878666666666666,
        "step": 8909
    },
    {
        "loss": 2.0355,
        "grad_norm": 1.9935559034347534,
        "learning_rate": 2.3116096267076627e-05,
        "epoch": 1.188,
        "step": 8910
    },
    {
        "loss": 1.6817,
        "grad_norm": 3.194859504699707,
        "learning_rate": 2.30758670741292e-05,
        "epoch": 1.1881333333333333,
        "step": 8911
    },
    {
        "loss": 0.6791,
        "grad_norm": 4.303707599639893,
        "learning_rate": 2.3035668351093052e-05,
        "epoch": 1.1882666666666666,
        "step": 8912
    },
    {
        "loss": 1.9935,
        "grad_norm": 4.59904670715332,
        "learning_rate": 2.29955001138908e-05,
        "epoch": 1.1884000000000001,
        "step": 8913
    },
    {
        "loss": 2.3049,
        "grad_norm": 2.8961079120635986,
        "learning_rate": 2.295536237843341e-05,
        "epoch": 1.1885333333333334,
        "step": 8914
    },
    {
        "loss": 2.5036,
        "grad_norm": 3.095734119415283,
        "learning_rate": 2.291525516061941e-05,
        "epoch": 1.1886666666666668,
        "step": 8915
    },
    {
        "loss": 2.6044,
        "grad_norm": 4.125211715698242,
        "learning_rate": 2.2875178476335646e-05,
        "epoch": 1.1888,
        "step": 8916
    },
    {
        "loss": 0.8706,
        "grad_norm": 3.4108352661132812,
        "learning_rate": 2.2835132341456388e-05,
        "epoch": 1.1889333333333334,
        "step": 8917
    },
    {
        "loss": 2.6089,
        "grad_norm": 2.623929023742676,
        "learning_rate": 2.2795116771844162e-05,
        "epoch": 1.1890666666666667,
        "step": 8918
    },
    {
        "loss": 1.4719,
        "grad_norm": 4.324475288391113,
        "learning_rate": 2.2755131783349236e-05,
        "epoch": 1.1892,
        "step": 8919
    },
    {
        "loss": 2.5649,
        "grad_norm": 3.9072210788726807,
        "learning_rate": 2.2715177391809827e-05,
        "epoch": 1.1893333333333334,
        "step": 8920
    },
    {
        "loss": 2.8457,
        "grad_norm": 3.577455997467041,
        "learning_rate": 2.2675253613052074e-05,
        "epoch": 1.1894666666666667,
        "step": 8921
    },
    {
        "loss": 0.8586,
        "grad_norm": 5.1803765296936035,
        "learning_rate": 2.26353604628897e-05,
        "epoch": 1.1896,
        "step": 8922
    },
    {
        "loss": 2.1674,
        "grad_norm": 3.3790478706359863,
        "learning_rate": 2.2595497957124746e-05,
        "epoch": 1.1897333333333333,
        "step": 8923
    },
    {
        "loss": 2.525,
        "grad_norm": 3.835596799850464,
        "learning_rate": 2.2555666111546747e-05,
        "epoch": 1.1898666666666666,
        "step": 8924
    },
    {
        "loss": 1.8221,
        "grad_norm": 3.7708752155303955,
        "learning_rate": 2.2515864941933394e-05,
        "epoch": 1.19,
        "step": 8925
    },
    {
        "loss": 2.3087,
        "grad_norm": 3.549710750579834,
        "learning_rate": 2.2476094464049846e-05,
        "epoch": 1.1901333333333333,
        "step": 8926
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.878060817718506,
        "learning_rate": 2.2436354693649465e-05,
        "epoch": 1.1902666666666666,
        "step": 8927
    },
    {
        "loss": 2.6072,
        "grad_norm": 3.987736463546753,
        "learning_rate": 2.239664564647329e-05,
        "epoch": 1.1904,
        "step": 8928
    },
    {
        "loss": 2.5083,
        "grad_norm": 3.7601447105407715,
        "learning_rate": 2.2356967338250223e-05,
        "epoch": 1.1905333333333332,
        "step": 8929
    },
    {
        "loss": 1.7905,
        "grad_norm": 3.223217248916626,
        "learning_rate": 2.2317319784696976e-05,
        "epoch": 1.1906666666666668,
        "step": 8930
    },
    {
        "loss": 2.507,
        "grad_norm": 4.342796325683594,
        "learning_rate": 2.2277703001517958e-05,
        "epoch": 1.1908,
        "step": 8931
    },
    {
        "loss": 1.711,
        "grad_norm": 3.1360414028167725,
        "learning_rate": 2.2238117004405723e-05,
        "epoch": 1.1909333333333334,
        "step": 8932
    },
    {
        "loss": 1.787,
        "grad_norm": 3.2707526683807373,
        "learning_rate": 2.2198561809040285e-05,
        "epoch": 1.1910666666666667,
        "step": 8933
    },
    {
        "loss": 1.678,
        "grad_norm": 3.320030927658081,
        "learning_rate": 2.215903743108958e-05,
        "epoch": 1.1912,
        "step": 8934
    },
    {
        "loss": 2.3113,
        "grad_norm": 2.9506585597991943,
        "learning_rate": 2.211954388620939e-05,
        "epoch": 1.1913333333333334,
        "step": 8935
    },
    {
        "loss": 2.1349,
        "grad_norm": 6.60067892074585,
        "learning_rate": 2.2080081190043213e-05,
        "epoch": 1.1914666666666667,
        "step": 8936
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.1657657623291016,
        "learning_rate": 2.2040649358222486e-05,
        "epoch": 1.1916,
        "step": 8937
    },
    {
        "loss": 1.7677,
        "grad_norm": 3.3810267448425293,
        "learning_rate": 2.2001248406366083e-05,
        "epoch": 1.1917333333333333,
        "step": 8938
    },
    {
        "loss": 1.6771,
        "grad_norm": 4.692161560058594,
        "learning_rate": 2.196187835008101e-05,
        "epoch": 1.1918666666666666,
        "step": 8939
    },
    {
        "loss": 2.6796,
        "grad_norm": 2.8856089115142822,
        "learning_rate": 2.1922539204961755e-05,
        "epoch": 1.192,
        "step": 8940
    },
    {
        "loss": 2.0157,
        "grad_norm": 4.353328704833984,
        "learning_rate": 2.1883230986590874e-05,
        "epoch": 1.1921333333333333,
        "step": 8941
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.7576777935028076,
        "learning_rate": 2.1843953710538346e-05,
        "epoch": 1.1922666666666666,
        "step": 8942
    },
    {
        "loss": 1.6235,
        "grad_norm": 5.945992469787598,
        "learning_rate": 2.1804707392362033e-05,
        "epoch": 1.1924,
        "step": 8943
    },
    {
        "loss": 1.828,
        "grad_norm": 2.884188413619995,
        "learning_rate": 2.1765492047607573e-05,
        "epoch": 1.1925333333333334,
        "step": 8944
    },
    {
        "loss": 2.5272,
        "grad_norm": 3.862147331237793,
        "learning_rate": 2.172630769180828e-05,
        "epoch": 1.1926666666666668,
        "step": 8945
    },
    {
        "loss": 1.3187,
        "grad_norm": 3.496483325958252,
        "learning_rate": 2.1687154340485305e-05,
        "epoch": 1.1928,
        "step": 8946
    },
    {
        "loss": 2.502,
        "grad_norm": 3.1852405071258545,
        "learning_rate": 2.164803200914721e-05,
        "epoch": 1.1929333333333334,
        "step": 8947
    },
    {
        "loss": 1.9688,
        "grad_norm": 3.267083168029785,
        "learning_rate": 2.1608940713290692e-05,
        "epoch": 1.1930666666666667,
        "step": 8948
    },
    {
        "loss": 0.6782,
        "grad_norm": 3.8238234519958496,
        "learning_rate": 2.1569880468399818e-05,
        "epoch": 1.1932,
        "step": 8949
    },
    {
        "loss": 2.0819,
        "grad_norm": 2.9408228397369385,
        "learning_rate": 2.1530851289946642e-05,
        "epoch": 1.1933333333333334,
        "step": 8950
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.3317201137542725,
        "learning_rate": 2.1491853193390532e-05,
        "epoch": 1.1934666666666667,
        "step": 8951
    },
    {
        "loss": 1.938,
        "grad_norm": 3.0592198371887207,
        "learning_rate": 2.1452886194178913e-05,
        "epoch": 1.1936,
        "step": 8952
    },
    {
        "loss": 2.3932,
        "grad_norm": 3.8772382736206055,
        "learning_rate": 2.1413950307746723e-05,
        "epoch": 1.1937333333333333,
        "step": 8953
    },
    {
        "loss": 1.7733,
        "grad_norm": 4.053914546966553,
        "learning_rate": 2.1375045549516637e-05,
        "epoch": 1.1938666666666666,
        "step": 8954
    },
    {
        "loss": 2.5105,
        "grad_norm": 3.829695701599121,
        "learning_rate": 2.1336171934898885e-05,
        "epoch": 1.194,
        "step": 8955
    },
    {
        "loss": 2.3563,
        "grad_norm": 2.754495620727539,
        "learning_rate": 2.1297329479291394e-05,
        "epoch": 1.1941333333333333,
        "step": 8956
    },
    {
        "loss": 1.9271,
        "grad_norm": 3.3803839683532715,
        "learning_rate": 2.125851819807998e-05,
        "epoch": 1.1942666666666666,
        "step": 8957
    },
    {
        "loss": 1.883,
        "grad_norm": 4.0972113609313965,
        "learning_rate": 2.121973810663771e-05,
        "epoch": 1.1944,
        "step": 8958
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.7495667934417725,
        "learning_rate": 2.118098922032574e-05,
        "epoch": 1.1945333333333332,
        "step": 8959
    },
    {
        "loss": 1.665,
        "grad_norm": 3.097022771835327,
        "learning_rate": 2.1142271554492433e-05,
        "epoch": 1.1946666666666665,
        "step": 8960
    },
    {
        "loss": 2.2697,
        "grad_norm": 2.8646128177642822,
        "learning_rate": 2.1103585124474058e-05,
        "epoch": 1.1948,
        "step": 8961
    },
    {
        "loss": 2.1135,
        "grad_norm": 5.126436710357666,
        "learning_rate": 2.1064929945594424e-05,
        "epoch": 1.1949333333333334,
        "step": 8962
    },
    {
        "loss": 2.0577,
        "grad_norm": 4.104191303253174,
        "learning_rate": 2.102630603316508e-05,
        "epoch": 1.1950666666666667,
        "step": 8963
    },
    {
        "loss": 2.6083,
        "grad_norm": 3.07045316696167,
        "learning_rate": 2.098771340248499e-05,
        "epoch": 1.1952,
        "step": 8964
    },
    {
        "loss": 2.4149,
        "grad_norm": 3.1477890014648438,
        "learning_rate": 2.094915206884077e-05,
        "epoch": 1.1953333333333334,
        "step": 8965
    },
    {
        "loss": 2.2413,
        "grad_norm": 2.8812785148620605,
        "learning_rate": 2.091062204750688e-05,
        "epoch": 1.1954666666666667,
        "step": 8966
    },
    {
        "loss": 1.2205,
        "grad_norm": 3.480785846710205,
        "learning_rate": 2.0872123353745054e-05,
        "epoch": 1.1956,
        "step": 8967
    },
    {
        "loss": 1.8292,
        "grad_norm": 3.3034679889678955,
        "learning_rate": 2.083365600280478e-05,
        "epoch": 1.1957333333333333,
        "step": 8968
    },
    {
        "loss": 2.2356,
        "grad_norm": 2.2986860275268555,
        "learning_rate": 2.07952200099231e-05,
        "epoch": 1.1958666666666666,
        "step": 8969
    },
    {
        "loss": 2.2397,
        "grad_norm": 3.6825687885284424,
        "learning_rate": 2.0756815390324656e-05,
        "epoch": 1.196,
        "step": 8970
    },
    {
        "loss": 2.2079,
        "grad_norm": 4.029965400695801,
        "learning_rate": 2.071844215922174e-05,
        "epoch": 1.1961333333333333,
        "step": 8971
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.932942271232605,
        "learning_rate": 2.06801003318139e-05,
        "epoch": 1.1962666666666666,
        "step": 8972
    },
    {
        "loss": 0.6092,
        "grad_norm": 2.707514762878418,
        "learning_rate": 2.0641789923288667e-05,
        "epoch": 1.1964,
        "step": 8973
    },
    {
        "loss": 1.903,
        "grad_norm": 4.993588924407959,
        "learning_rate": 2.0603510948820803e-05,
        "epoch": 1.1965333333333334,
        "step": 8974
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.751842975616455,
        "learning_rate": 2.056526342357289e-05,
        "epoch": 1.1966666666666668,
        "step": 8975
    },
    {
        "loss": 1.7346,
        "grad_norm": 3.432788848876953,
        "learning_rate": 2.0527047362694697e-05,
        "epoch": 1.1968,
        "step": 8976
    },
    {
        "loss": 1.7442,
        "grad_norm": 5.02681827545166,
        "learning_rate": 2.048886278132387e-05,
        "epoch": 1.1969333333333334,
        "step": 8977
    },
    {
        "loss": 1.7105,
        "grad_norm": 3.0510807037353516,
        "learning_rate": 2.0450709694585423e-05,
        "epoch": 1.1970666666666667,
        "step": 8978
    },
    {
        "loss": 1.5699,
        "grad_norm": 1.9451029300689697,
        "learning_rate": 2.0412588117591923e-05,
        "epoch": 1.1972,
        "step": 8979
    },
    {
        "loss": 1.661,
        "grad_norm": 2.642709732055664,
        "learning_rate": 2.0374498065443538e-05,
        "epoch": 1.1973333333333334,
        "step": 8980
    },
    {
        "loss": 1.5484,
        "grad_norm": 2.4162161350250244,
        "learning_rate": 2.0336439553227647e-05,
        "epoch": 1.1974666666666667,
        "step": 8981
    },
    {
        "loss": 1.6806,
        "grad_norm": 5.362443447113037,
        "learning_rate": 2.029841259601961e-05,
        "epoch": 1.1976,
        "step": 8982
    },
    {
        "loss": 2.1105,
        "grad_norm": 3.990633964538574,
        "learning_rate": 2.026041720888181e-05,
        "epoch": 1.1977333333333333,
        "step": 8983
    },
    {
        "loss": 2.4237,
        "grad_norm": 2.180431365966797,
        "learning_rate": 2.0222453406864595e-05,
        "epoch": 1.1978666666666666,
        "step": 8984
    },
    {
        "loss": 2.1355,
        "grad_norm": 3.4558210372924805,
        "learning_rate": 2.0184521205005335e-05,
        "epoch": 1.198,
        "step": 8985
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.4643807411193848,
        "learning_rate": 2.0146620618329192e-05,
        "epoch": 1.1981333333333333,
        "step": 8986
    },
    {
        "loss": 1.3951,
        "grad_norm": 3.150888204574585,
        "learning_rate": 2.01087516618487e-05,
        "epoch": 1.1982666666666666,
        "step": 8987
    },
    {
        "loss": 2.6922,
        "grad_norm": 4.560753345489502,
        "learning_rate": 2.007091435056396e-05,
        "epoch": 1.1984,
        "step": 8988
    },
    {
        "loss": 1.5784,
        "grad_norm": 2.3736603260040283,
        "learning_rate": 2.0033108699462388e-05,
        "epoch": 1.1985333333333332,
        "step": 8989
    },
    {
        "loss": 2.9189,
        "grad_norm": 2.7207529544830322,
        "learning_rate": 1.9995334723518855e-05,
        "epoch": 1.1986666666666665,
        "step": 8990
    },
    {
        "loss": 2.3549,
        "grad_norm": 3.315916061401367,
        "learning_rate": 1.995759243769595e-05,
        "epoch": 1.1988,
        "step": 8991
    },
    {
        "loss": 1.8206,
        "grad_norm": 6.570125102996826,
        "learning_rate": 1.991988185694339e-05,
        "epoch": 1.1989333333333334,
        "step": 8992
    },
    {
        "loss": 1.4063,
        "grad_norm": 2.771000862121582,
        "learning_rate": 1.9882202996198474e-05,
        "epoch": 1.1990666666666667,
        "step": 8993
    },
    {
        "loss": 2.2521,
        "grad_norm": 5.647513389587402,
        "learning_rate": 1.984455587038594e-05,
        "epoch": 1.1992,
        "step": 8994
    },
    {
        "loss": 2.1565,
        "grad_norm": 2.2682504653930664,
        "learning_rate": 1.9806940494417947e-05,
        "epoch": 1.1993333333333334,
        "step": 8995
    },
    {
        "loss": 1.0858,
        "grad_norm": 3.1165008544921875,
        "learning_rate": 1.9769356883194156e-05,
        "epoch": 1.1994666666666667,
        "step": 8996
    },
    {
        "loss": 2.0468,
        "grad_norm": 4.165132522583008,
        "learning_rate": 1.973180505160136e-05,
        "epoch": 1.1996,
        "step": 8997
    },
    {
        "loss": 1.8248,
        "grad_norm": 4.309103488922119,
        "learning_rate": 1.9694285014514158e-05,
        "epoch": 1.1997333333333333,
        "step": 8998
    },
    {
        "loss": 1.4136,
        "grad_norm": 5.240101337432861,
        "learning_rate": 1.965679678679425e-05,
        "epoch": 1.1998666666666666,
        "step": 8999
    },
    {
        "loss": 2.7629,
        "grad_norm": 2.148700475692749,
        "learning_rate": 1.961934038329094e-05,
        "epoch": 1.2,
        "step": 9000
    },
    {
        "loss": 1.4809,
        "grad_norm": 4.174622058868408,
        "learning_rate": 1.9581915818840835e-05,
        "epoch": 1.2001333333333333,
        "step": 9001
    },
    {
        "loss": 1.1325,
        "grad_norm": 3.2605338096618652,
        "learning_rate": 1.954452310826781e-05,
        "epoch": 1.2002666666666666,
        "step": 9002
    },
    {
        "loss": 1.4884,
        "grad_norm": 4.181483745574951,
        "learning_rate": 1.950716226638335e-05,
        "epoch": 1.2004,
        "step": 9003
    },
    {
        "loss": 1.7501,
        "grad_norm": 3.2666285037994385,
        "learning_rate": 1.9469833307986185e-05,
        "epoch": 1.2005333333333335,
        "step": 9004
    },
    {
        "loss": 2.0283,
        "grad_norm": 3.0839221477508545,
        "learning_rate": 1.943253624786253e-05,
        "epoch": 1.2006666666666668,
        "step": 9005
    },
    {
        "loss": 0.9436,
        "grad_norm": 3.897706985473633,
        "learning_rate": 1.939527110078566e-05,
        "epoch": 1.2008,
        "step": 9006
    },
    {
        "loss": 2.9641,
        "grad_norm": 2.711660385131836,
        "learning_rate": 1.9358037881516666e-05,
        "epoch": 1.2009333333333334,
        "step": 9007
    },
    {
        "loss": 2.1715,
        "grad_norm": 3.591989040374756,
        "learning_rate": 1.932083660480355e-05,
        "epoch": 1.2010666666666667,
        "step": 9008
    },
    {
        "loss": 2.2863,
        "grad_norm": 4.1577887535095215,
        "learning_rate": 1.928366728538209e-05,
        "epoch": 1.2012,
        "step": 9009
    },
    {
        "loss": 2.7141,
        "grad_norm": 3.163529634475708,
        "learning_rate": 1.9246529937974977e-05,
        "epoch": 1.2013333333333334,
        "step": 9010
    },
    {
        "loss": 1.6844,
        "grad_norm": 3.244614839553833,
        "learning_rate": 1.9209424577292512e-05,
        "epoch": 1.2014666666666667,
        "step": 9011
    },
    {
        "loss": 2.2953,
        "grad_norm": 4.4493632316589355,
        "learning_rate": 1.9172351218032226e-05,
        "epoch": 1.2016,
        "step": 9012
    },
    {
        "loss": 1.8807,
        "grad_norm": 4.443678379058838,
        "learning_rate": 1.9135309874879126e-05,
        "epoch": 1.2017333333333333,
        "step": 9013
    },
    {
        "loss": 2.5329,
        "grad_norm": 2.6970767974853516,
        "learning_rate": 1.909830056250529e-05,
        "epoch": 1.2018666666666666,
        "step": 9014
    },
    {
        "loss": 2.2665,
        "grad_norm": 2.200650215148926,
        "learning_rate": 1.9061323295570222e-05,
        "epoch": 1.202,
        "step": 9015
    },
    {
        "loss": 2.2302,
        "grad_norm": 4.140755653381348,
        "learning_rate": 1.902437808872083e-05,
        "epoch": 1.2021333333333333,
        "step": 9016
    },
    {
        "loss": 2.0031,
        "grad_norm": 3.431818962097168,
        "learning_rate": 1.8987464956591206e-05,
        "epoch": 1.2022666666666666,
        "step": 9017
    },
    {
        "loss": 2.0862,
        "grad_norm": 3.505263328552246,
        "learning_rate": 1.8950583913802723e-05,
        "epoch": 1.2024,
        "step": 9018
    },
    {
        "loss": 1.9214,
        "grad_norm": 4.844741344451904,
        "learning_rate": 1.8913734974964126e-05,
        "epoch": 1.2025333333333332,
        "step": 9019
    },
    {
        "loss": 1.9032,
        "grad_norm": 3.7706079483032227,
        "learning_rate": 1.8876918154671396e-05,
        "epoch": 1.2026666666666666,
        "step": 9020
    },
    {
        "loss": 2.3441,
        "grad_norm": 3.417091131210327,
        "learning_rate": 1.8840133467507804e-05,
        "epoch": 1.2028,
        "step": 9021
    },
    {
        "loss": 0.8885,
        "grad_norm": 3.718005895614624,
        "learning_rate": 1.8803380928043923e-05,
        "epoch": 1.2029333333333334,
        "step": 9022
    },
    {
        "loss": 2.589,
        "grad_norm": 2.6948697566986084,
        "learning_rate": 1.876666055083752e-05,
        "epoch": 1.2030666666666667,
        "step": 9023
    },
    {
        "loss": 1.6194,
        "grad_norm": 3.972371816635132,
        "learning_rate": 1.8729972350433623e-05,
        "epoch": 1.2032,
        "step": 9024
    },
    {
        "loss": 2.329,
        "grad_norm": 2.463006019592285,
        "learning_rate": 1.8693316341364642e-05,
        "epoch": 1.2033333333333334,
        "step": 9025
    },
    {
        "loss": 1.4783,
        "grad_norm": 4.432944297790527,
        "learning_rate": 1.8656692538150144e-05,
        "epoch": 1.2034666666666667,
        "step": 9026
    },
    {
        "loss": 2.1166,
        "grad_norm": 4.274014949798584,
        "learning_rate": 1.8620100955296814e-05,
        "epoch": 1.2036,
        "step": 9027
    },
    {
        "loss": 2.1027,
        "grad_norm": 4.345235347747803,
        "learning_rate": 1.8583541607298816e-05,
        "epoch": 1.2037333333333333,
        "step": 9028
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.539808750152588,
        "learning_rate": 1.8547014508637415e-05,
        "epoch": 1.2038666666666666,
        "step": 9029
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.4335973262786865,
        "learning_rate": 1.8510519673781178e-05,
        "epoch": 1.204,
        "step": 9030
    },
    {
        "loss": 2.1078,
        "grad_norm": 5.106287956237793,
        "learning_rate": 1.8474057117185652e-05,
        "epoch": 1.2041333333333333,
        "step": 9031
    },
    {
        "loss": 0.6701,
        "grad_norm": 3.309070348739624,
        "learning_rate": 1.8437626853293987e-05,
        "epoch": 1.2042666666666666,
        "step": 9032
    },
    {
        "loss": 1.424,
        "grad_norm": 3.4220032691955566,
        "learning_rate": 1.8401228896536137e-05,
        "epoch": 1.2044,
        "step": 9033
    },
    {
        "loss": 2.0465,
        "grad_norm": 3.001509666442871,
        "learning_rate": 1.83648632613297e-05,
        "epoch": 1.2045333333333335,
        "step": 9034
    },
    {
        "loss": 2.1186,
        "grad_norm": 2.6220366954803467,
        "learning_rate": 1.832852996207901e-05,
        "epoch": 1.2046666666666668,
        "step": 9035
    },
    {
        "loss": 1.8725,
        "grad_norm": 4.1621904373168945,
        "learning_rate": 1.829222901317589e-05,
        "epoch": 1.2048,
        "step": 9036
    },
    {
        "loss": 2.6124,
        "grad_norm": 4.22625207901001,
        "learning_rate": 1.825596042899924e-05,
        "epoch": 1.2049333333333334,
        "step": 9037
    },
    {
        "loss": 2.0931,
        "grad_norm": 3.003612756729126,
        "learning_rate": 1.821972422391528e-05,
        "epoch": 1.2050666666666667,
        "step": 9038
    },
    {
        "loss": 1.758,
        "grad_norm": 4.378701686859131,
        "learning_rate": 1.8183520412277188e-05,
        "epoch": 1.2052,
        "step": 9039
    },
    {
        "loss": 2.7393,
        "grad_norm": 2.1584856510162354,
        "learning_rate": 1.8147349008425395e-05,
        "epoch": 1.2053333333333334,
        "step": 9040
    },
    {
        "loss": 1.8043,
        "grad_norm": 3.6034185886383057,
        "learning_rate": 1.811121002668762e-05,
        "epoch": 1.2054666666666667,
        "step": 9041
    },
    {
        "loss": 1.9929,
        "grad_norm": 3.649772882461548,
        "learning_rate": 1.8075103481378498e-05,
        "epoch": 1.2056,
        "step": 9042
    },
    {
        "loss": 0.8423,
        "grad_norm": 4.716919422149658,
        "learning_rate": 1.8039029386800178e-05,
        "epoch": 1.2057333333333333,
        "step": 9043
    },
    {
        "loss": 1.8079,
        "grad_norm": 3.9292409420013428,
        "learning_rate": 1.8002987757241507e-05,
        "epoch": 1.2058666666666666,
        "step": 9044
    },
    {
        "loss": 1.8586,
        "grad_norm": 3.5046870708465576,
        "learning_rate": 1.7966978606978778e-05,
        "epoch": 1.206,
        "step": 9045
    },
    {
        "loss": 1.2691,
        "grad_norm": 4.120582580566406,
        "learning_rate": 1.793100195027535e-05,
        "epoch": 1.2061333333333333,
        "step": 9046
    },
    {
        "loss": 1.8807,
        "grad_norm": 3.822451114654541,
        "learning_rate": 1.789505780138172e-05,
        "epoch": 1.2062666666666666,
        "step": 9047
    },
    {
        "loss": 2.5551,
        "grad_norm": 3.2525503635406494,
        "learning_rate": 1.785914617453549e-05,
        "epoch": 1.2064,
        "step": 9048
    },
    {
        "loss": 1.5628,
        "grad_norm": 2.646010160446167,
        "learning_rate": 1.782326708396124e-05,
        "epoch": 1.2065333333333332,
        "step": 9049
    },
    {
        "loss": 2.3594,
        "grad_norm": 3.022109031677246,
        "learning_rate": 1.7787420543870992e-05,
        "epoch": 1.2066666666666666,
        "step": 9050
    },
    {
        "loss": 2.1493,
        "grad_norm": 4.073533058166504,
        "learning_rate": 1.775160656846362e-05,
        "epoch": 1.2068,
        "step": 9051
    },
    {
        "loss": 0.7194,
        "grad_norm": 4.1533660888671875,
        "learning_rate": 1.771582517192506e-05,
        "epoch": 1.2069333333333334,
        "step": 9052
    },
    {
        "loss": 2.4373,
        "grad_norm": 2.4082140922546387,
        "learning_rate": 1.7680076368428556e-05,
        "epoch": 1.2070666666666667,
        "step": 9053
    },
    {
        "loss": 1.8308,
        "grad_norm": 3.468604564666748,
        "learning_rate": 1.76443601721343e-05,
        "epoch": 1.2072,
        "step": 9054
    },
    {
        "loss": 1.3754,
        "grad_norm": 4.610985279083252,
        "learning_rate": 1.7608676597189677e-05,
        "epoch": 1.2073333333333334,
        "step": 9055
    },
    {
        "loss": 2.7116,
        "grad_norm": 2.841033935546875,
        "learning_rate": 1.757302565772887e-05,
        "epoch": 1.2074666666666667,
        "step": 9056
    },
    {
        "loss": 0.9839,
        "grad_norm": 2.230681896209717,
        "learning_rate": 1.753740736787356e-05,
        "epoch": 1.2076,
        "step": 9057
    },
    {
        "loss": 2.4712,
        "grad_norm": 4.01461935043335,
        "learning_rate": 1.750182174173207e-05,
        "epoch": 1.2077333333333333,
        "step": 9058
    },
    {
        "loss": 1.7095,
        "grad_norm": 4.652004718780518,
        "learning_rate": 1.7466268793400175e-05,
        "epoch": 1.2078666666666666,
        "step": 9059
    },
    {
        "loss": 1.2603,
        "grad_norm": 3.607414960861206,
        "learning_rate": 1.743074853696046e-05,
        "epoch": 1.208,
        "step": 9060
    },
    {
        "loss": 2.1848,
        "grad_norm": 3.2422893047332764,
        "learning_rate": 1.7395260986482553e-05,
        "epoch": 1.2081333333333333,
        "step": 9061
    },
    {
        "loss": 2.6322,
        "grad_norm": 4.836031913757324,
        "learning_rate": 1.735980615602324e-05,
        "epoch": 1.2082666666666666,
        "step": 9062
    },
    {
        "loss": 2.3714,
        "grad_norm": 2.9404172897338867,
        "learning_rate": 1.7324384059626274e-05,
        "epoch": 1.2084,
        "step": 9063
    },
    {
        "loss": 2.0845,
        "grad_norm": 6.070480823516846,
        "learning_rate": 1.7288994711322582e-05,
        "epoch": 1.2085333333333332,
        "step": 9064
    },
    {
        "loss": 1.0259,
        "grad_norm": 5.298154830932617,
        "learning_rate": 1.7253638125129823e-05,
        "epoch": 1.2086666666666668,
        "step": 9065
    },
    {
        "loss": 2.1503,
        "grad_norm": 3.098508834838867,
        "learning_rate": 1.7218314315052996e-05,
        "epoch": 1.2088,
        "step": 9066
    },
    {
        "loss": 2.2347,
        "grad_norm": 3.1530373096466064,
        "learning_rate": 1.7183023295083877e-05,
        "epoch": 1.2089333333333334,
        "step": 9067
    },
    {
        "loss": 1.9102,
        "grad_norm": 3.334667921066284,
        "learning_rate": 1.7147765079201538e-05,
        "epoch": 1.2090666666666667,
        "step": 9068
    },
    {
        "loss": 2.4617,
        "grad_norm": 3.2813541889190674,
        "learning_rate": 1.7112539681371696e-05,
        "epoch": 1.2092,
        "step": 9069
    },
    {
        "loss": 2.6576,
        "grad_norm": 4.635376930236816,
        "learning_rate": 1.7077347115547314e-05,
        "epoch": 1.2093333333333334,
        "step": 9070
    },
    {
        "loss": 2.4485,
        "grad_norm": 2.878526210784912,
        "learning_rate": 1.7042187395668296e-05,
        "epoch": 1.2094666666666667,
        "step": 9071
    },
    {
        "loss": 1.7131,
        "grad_norm": 3.265223503112793,
        "learning_rate": 1.7007060535661534e-05,
        "epoch": 1.2096,
        "step": 9072
    },
    {
        "loss": 2.4004,
        "grad_norm": 4.303488731384277,
        "learning_rate": 1.697196654944092e-05,
        "epoch": 1.2097333333333333,
        "step": 9073
    },
    {
        "loss": 3.1571,
        "grad_norm": 3.765594005584717,
        "learning_rate": 1.693690545090717e-05,
        "epoch": 1.2098666666666666,
        "step": 9074
    },
    {
        "loss": 1.5522,
        "grad_norm": 3.3821938037872314,
        "learning_rate": 1.6901877253948272e-05,
        "epoch": 1.21,
        "step": 9075
    },
    {
        "loss": 2.422,
        "grad_norm": 2.773294448852539,
        "learning_rate": 1.6866881972438974e-05,
        "epoch": 1.2101333333333333,
        "step": 9076
    },
    {
        "loss": 2.6792,
        "grad_norm": 3.7006921768188477,
        "learning_rate": 1.6831919620240922e-05,
        "epoch": 1.2102666666666666,
        "step": 9077
    },
    {
        "loss": 0.809,
        "grad_norm": 3.0520782470703125,
        "learning_rate": 1.6796990211202913e-05,
        "epoch": 1.2104,
        "step": 9078
    },
    {
        "loss": 1.9171,
        "grad_norm": 4.204701900482178,
        "learning_rate": 1.676209375916059e-05,
        "epoch": 1.2105333333333332,
        "step": 9079
    },
    {
        "loss": 2.416,
        "grad_norm": 3.398287296295166,
        "learning_rate": 1.6727230277936555e-05,
        "epoch": 1.2106666666666666,
        "step": 9080
    },
    {
        "loss": 2.5833,
        "grad_norm": 3.2245144844055176,
        "learning_rate": 1.6692399781340386e-05,
        "epoch": 1.2107999999999999,
        "step": 9081
    },
    {
        "loss": 1.1747,
        "grad_norm": 3.1969845294952393,
        "learning_rate": 1.665760228316854e-05,
        "epoch": 1.2109333333333334,
        "step": 9082
    },
    {
        "loss": 2.8899,
        "grad_norm": 4.02418851852417,
        "learning_rate": 1.6622837797204317e-05,
        "epoch": 1.2110666666666667,
        "step": 9083
    },
    {
        "loss": 1.8608,
        "grad_norm": 3.221977472305298,
        "learning_rate": 1.6588106337218224e-05,
        "epoch": 1.2112,
        "step": 9084
    },
    {
        "loss": 1.4843,
        "grad_norm": 2.3988537788391113,
        "learning_rate": 1.6553407916967446e-05,
        "epoch": 1.2113333333333334,
        "step": 9085
    },
    {
        "loss": 2.1109,
        "grad_norm": 2.673863410949707,
        "learning_rate": 1.6518742550196075e-05,
        "epoch": 1.2114666666666667,
        "step": 9086
    },
    {
        "loss": 2.6761,
        "grad_norm": 3.613477945327759,
        "learning_rate": 1.6484110250635254e-05,
        "epoch": 1.2116,
        "step": 9087
    },
    {
        "loss": 1.1651,
        "grad_norm": 1.6273653507232666,
        "learning_rate": 1.6449511032002896e-05,
        "epoch": 1.2117333333333333,
        "step": 9088
    },
    {
        "loss": 1.099,
        "grad_norm": 2.33414626121521,
        "learning_rate": 1.6414944908004017e-05,
        "epoch": 1.2118666666666666,
        "step": 9089
    },
    {
        "loss": 2.3082,
        "grad_norm": 3.092712163925171,
        "learning_rate": 1.6380411892330162e-05,
        "epoch": 1.212,
        "step": 9090
    },
    {
        "loss": 2.4017,
        "grad_norm": 3.0722098350524902,
        "learning_rate": 1.6345911998660134e-05,
        "epoch": 1.2121333333333333,
        "step": 9091
    },
    {
        "loss": 2.2096,
        "grad_norm": 3.918630599975586,
        "learning_rate": 1.631144524065934e-05,
        "epoch": 1.2122666666666666,
        "step": 9092
    },
    {
        "loss": 1.9181,
        "grad_norm": 3.661989688873291,
        "learning_rate": 1.6277011631980366e-05,
        "epoch": 1.2124,
        "step": 9093
    },
    {
        "loss": 2.1686,
        "grad_norm": 3.5112011432647705,
        "learning_rate": 1.624261118626229e-05,
        "epoch": 1.2125333333333332,
        "step": 9094
    },
    {
        "loss": 1.3238,
        "grad_norm": 4.986893653869629,
        "learning_rate": 1.6208243917131327e-05,
        "epoch": 1.2126666666666668,
        "step": 9095
    },
    {
        "loss": 0.7353,
        "grad_norm": 3.5779545307159424,
        "learning_rate": 1.6173909838200453e-05,
        "epoch": 1.2128,
        "step": 9096
    },
    {
        "loss": 1.881,
        "grad_norm": 3.1676461696624756,
        "learning_rate": 1.613960896306955e-05,
        "epoch": 1.2129333333333334,
        "step": 9097
    },
    {
        "loss": 2.4974,
        "grad_norm": 3.3735952377319336,
        "learning_rate": 1.6105341305325306e-05,
        "epoch": 1.2130666666666667,
        "step": 9098
    },
    {
        "loss": 2.3904,
        "grad_norm": 3.647163152694702,
        "learning_rate": 1.6071106878541142e-05,
        "epoch": 1.2132,
        "step": 9099
    },
    {
        "loss": 1.5723,
        "grad_norm": 2.4943714141845703,
        "learning_rate": 1.6036905696277592e-05,
        "epoch": 1.2133333333333334,
        "step": 9100
    },
    {
        "loss": 1.016,
        "grad_norm": 4.226067066192627,
        "learning_rate": 1.6002737772081742e-05,
        "epoch": 1.2134666666666667,
        "step": 9101
    },
    {
        "loss": 1.0928,
        "grad_norm": 4.542415142059326,
        "learning_rate": 1.596860311948778e-05,
        "epoch": 1.2136,
        "step": 9102
    },
    {
        "loss": 1.8684,
        "grad_norm": 4.213065147399902,
        "learning_rate": 1.593450175201635e-05,
        "epoch": 1.2137333333333333,
        "step": 9103
    },
    {
        "loss": 1.7633,
        "grad_norm": 3.3344461917877197,
        "learning_rate": 1.5900433683175252e-05,
        "epoch": 1.2138666666666666,
        "step": 9104
    },
    {
        "loss": 0.6966,
        "grad_norm": 5.368227958679199,
        "learning_rate": 1.5866398926458924e-05,
        "epoch": 1.214,
        "step": 9105
    },
    {
        "loss": 1.1215,
        "grad_norm": 3.2606263160705566,
        "learning_rate": 1.5832397495348696e-05,
        "epoch": 1.2141333333333333,
        "step": 9106
    },
    {
        "loss": 0.8973,
        "grad_norm": 3.150543689727783,
        "learning_rate": 1.579842940331263e-05,
        "epoch": 1.2142666666666666,
        "step": 9107
    },
    {
        "loss": 0.5258,
        "grad_norm": 2.412221908569336,
        "learning_rate": 1.5764494663805496e-05,
        "epoch": 1.2144,
        "step": 9108
    },
    {
        "loss": 2.216,
        "grad_norm": 2.709590435028076,
        "learning_rate": 1.5730593290269147e-05,
        "epoch": 1.2145333333333332,
        "step": 9109
    },
    {
        "loss": 2.4423,
        "grad_norm": 2.345964193344116,
        "learning_rate": 1.569672529613191e-05,
        "epoch": 1.2146666666666666,
        "step": 9110
    },
    {
        "loss": 2.2004,
        "grad_norm": 2.189455986022949,
        "learning_rate": 1.566289069480901e-05,
        "epoch": 1.2147999999999999,
        "step": 9111
    },
    {
        "loss": 2.3195,
        "grad_norm": 3.1651604175567627,
        "learning_rate": 1.562908949970249e-05,
        "epoch": 1.2149333333333334,
        "step": 9112
    },
    {
        "loss": 2.3883,
        "grad_norm": 3.481680393218994,
        "learning_rate": 1.5595321724201075e-05,
        "epoch": 1.2150666666666667,
        "step": 9113
    },
    {
        "loss": 1.6171,
        "grad_norm": 3.4149675369262695,
        "learning_rate": 1.556158738168042e-05,
        "epoch": 1.2152,
        "step": 9114
    },
    {
        "loss": 2.1338,
        "grad_norm": 3.1642158031463623,
        "learning_rate": 1.552788648550263e-05,
        "epoch": 1.2153333333333334,
        "step": 9115
    },
    {
        "loss": 2.4573,
        "grad_norm": 3.9755027294158936,
        "learning_rate": 1.549421904901688e-05,
        "epoch": 1.2154666666666667,
        "step": 9116
    },
    {
        "loss": 1.1257,
        "grad_norm": 4.170896530151367,
        "learning_rate": 1.5460585085558833e-05,
        "epoch": 1.2156,
        "step": 9117
    },
    {
        "loss": 0.8164,
        "grad_norm": 2.7578155994415283,
        "learning_rate": 1.5426984608451223e-05,
        "epoch": 1.2157333333333333,
        "step": 9118
    },
    {
        "loss": 0.9043,
        "grad_norm": 3.612205743789673,
        "learning_rate": 1.5393417631003104e-05,
        "epoch": 1.2158666666666667,
        "step": 9119
    },
    {
        "loss": 1.9578,
        "grad_norm": 3.096013069152832,
        "learning_rate": 1.535988416651056e-05,
        "epoch": 1.216,
        "step": 9120
    },
    {
        "loss": 2.456,
        "grad_norm": 5.909262180328369,
        "learning_rate": 1.5326384228256296e-05,
        "epoch": 1.2161333333333333,
        "step": 9121
    },
    {
        "loss": 1.6907,
        "grad_norm": 3.985600233078003,
        "learning_rate": 1.5292917829509767e-05,
        "epoch": 1.2162666666666666,
        "step": 9122
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.786468267440796,
        "learning_rate": 1.5259484983527182e-05,
        "epoch": 1.2164,
        "step": 9123
    },
    {
        "loss": 1.7268,
        "grad_norm": 4.110486030578613,
        "learning_rate": 1.5226085703551218e-05,
        "epoch": 1.2165333333333332,
        "step": 9124
    },
    {
        "loss": 2.4203,
        "grad_norm": 2.830092668533325,
        "learning_rate": 1.5192720002811645e-05,
        "epoch": 1.2166666666666668,
        "step": 9125
    },
    {
        "loss": 2.0924,
        "grad_norm": 5.504336357116699,
        "learning_rate": 1.5159387894524613e-05,
        "epoch": 1.2168,
        "step": 9126
    },
    {
        "loss": 1.7862,
        "grad_norm": 3.38824200630188,
        "learning_rate": 1.5126089391893206e-05,
        "epoch": 1.2169333333333334,
        "step": 9127
    },
    {
        "loss": 1.4847,
        "grad_norm": 4.0504536628723145,
        "learning_rate": 1.5092824508106896e-05,
        "epoch": 1.2170666666666667,
        "step": 9128
    },
    {
        "loss": 1.8471,
        "grad_norm": 3.9733564853668213,
        "learning_rate": 1.5059593256342141e-05,
        "epoch": 1.2172,
        "step": 9129
    },
    {
        "loss": 0.9007,
        "grad_norm": 4.030404090881348,
        "learning_rate": 1.5026395649761927e-05,
        "epoch": 1.2173333333333334,
        "step": 9130
    },
    {
        "loss": 2.6572,
        "grad_norm": 2.4856929779052734,
        "learning_rate": 1.4993231701515954e-05,
        "epoch": 1.2174666666666667,
        "step": 9131
    },
    {
        "loss": 1.9258,
        "grad_norm": 2.573507785797119,
        "learning_rate": 1.496010142474058e-05,
        "epoch": 1.2176,
        "step": 9132
    },
    {
        "loss": 0.946,
        "grad_norm": 3.485004186630249,
        "learning_rate": 1.4927004832558712e-05,
        "epoch": 1.2177333333333333,
        "step": 9133
    },
    {
        "loss": 2.301,
        "grad_norm": 3.9418132305145264,
        "learning_rate": 1.4893941938080202e-05,
        "epoch": 1.2178666666666667,
        "step": 9134
    },
    {
        "loss": 2.1188,
        "grad_norm": 3.6959497928619385,
        "learning_rate": 1.4860912754401246e-05,
        "epoch": 1.218,
        "step": 9135
    },
    {
        "loss": 1.8427,
        "grad_norm": 3.158977508544922,
        "learning_rate": 1.4827917294604843e-05,
        "epoch": 1.2181333333333333,
        "step": 9136
    },
    {
        "loss": 2.3919,
        "grad_norm": 2.6891674995422363,
        "learning_rate": 1.4794955571760616e-05,
        "epoch": 1.2182666666666666,
        "step": 9137
    },
    {
        "loss": 1.9243,
        "grad_norm": 3.6234142780303955,
        "learning_rate": 1.4762027598924789e-05,
        "epoch": 1.2184,
        "step": 9138
    },
    {
        "loss": 1.2956,
        "grad_norm": 3.389096260070801,
        "learning_rate": 1.4729133389140293e-05,
        "epoch": 1.2185333333333332,
        "step": 9139
    },
    {
        "loss": 1.3103,
        "grad_norm": 4.975551128387451,
        "learning_rate": 1.4696272955436663e-05,
        "epoch": 1.2186666666666666,
        "step": 9140
    },
    {
        "loss": 1.7016,
        "grad_norm": 2.7386269569396973,
        "learning_rate": 1.4663446310829953e-05,
        "epoch": 1.2187999999999999,
        "step": 9141
    },
    {
        "loss": 1.5114,
        "grad_norm": 3.6111953258514404,
        "learning_rate": 1.4630653468322875e-05,
        "epoch": 1.2189333333333334,
        "step": 9142
    },
    {
        "loss": 2.1958,
        "grad_norm": 4.405519008636475,
        "learning_rate": 1.4597894440904935e-05,
        "epoch": 1.2190666666666667,
        "step": 9143
    },
    {
        "loss": 2.3171,
        "grad_norm": 3.2574076652526855,
        "learning_rate": 1.456516924155198e-05,
        "epoch": 1.2192,
        "step": 9144
    },
    {
        "loss": 2.4355,
        "grad_norm": 2.6050219535827637,
        "learning_rate": 1.4532477883226581e-05,
        "epoch": 1.2193333333333334,
        "step": 9145
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.826810836791992,
        "learning_rate": 1.4499820378877915e-05,
        "epoch": 1.2194666666666667,
        "step": 9146
    },
    {
        "loss": 0.8137,
        "grad_norm": 3.505811929702759,
        "learning_rate": 1.4467196741441735e-05,
        "epoch": 1.2196,
        "step": 9147
    },
    {
        "loss": 1.7732,
        "grad_norm": 4.291532039642334,
        "learning_rate": 1.4434606983840437e-05,
        "epoch": 1.2197333333333333,
        "step": 9148
    },
    {
        "loss": 1.5541,
        "grad_norm": 7.124781608581543,
        "learning_rate": 1.4402051118982762e-05,
        "epoch": 1.2198666666666667,
        "step": 9149
    },
    {
        "loss": 1.677,
        "grad_norm": 5.125168800354004,
        "learning_rate": 1.4369529159764373e-05,
        "epoch": 1.22,
        "step": 9150
    },
    {
        "loss": 2.2997,
        "grad_norm": 3.0298261642456055,
        "learning_rate": 1.4337041119067207e-05,
        "epoch": 1.2201333333333333,
        "step": 9151
    },
    {
        "loss": 1.9188,
        "grad_norm": 3.4633395671844482,
        "learning_rate": 1.430458700976005e-05,
        "epoch": 1.2202666666666666,
        "step": 9152
    },
    {
        "loss": 1.6444,
        "grad_norm": 1.879765272140503,
        "learning_rate": 1.4272166844697876e-05,
        "epoch": 1.2204,
        "step": 9153
    },
    {
        "loss": 1.7682,
        "grad_norm": 5.826723575592041,
        "learning_rate": 1.4239780636722555e-05,
        "epoch": 1.2205333333333332,
        "step": 9154
    },
    {
        "loss": 2.3917,
        "grad_norm": 2.5788116455078125,
        "learning_rate": 1.4207428398662337e-05,
        "epoch": 1.2206666666666668,
        "step": 9155
    },
    {
        "loss": 2.0868,
        "grad_norm": 3.639047384262085,
        "learning_rate": 1.4175110143332105e-05,
        "epoch": 1.2208,
        "step": 9156
    },
    {
        "loss": 2.5837,
        "grad_norm": 4.092231273651123,
        "learning_rate": 1.4142825883533172e-05,
        "epoch": 1.2209333333333334,
        "step": 9157
    },
    {
        "loss": 2.4423,
        "grad_norm": 2.825239658355713,
        "learning_rate": 1.4110575632053392e-05,
        "epoch": 1.2210666666666667,
        "step": 9158
    },
    {
        "loss": 1.4133,
        "grad_norm": 3.7040464878082275,
        "learning_rate": 1.407835940166734e-05,
        "epoch": 1.2212,
        "step": 9159
    },
    {
        "loss": 2.7162,
        "grad_norm": 3.0521678924560547,
        "learning_rate": 1.4046177205135825e-05,
        "epoch": 1.2213333333333334,
        "step": 9160
    },
    {
        "loss": 2.476,
        "grad_norm": 3.779940128326416,
        "learning_rate": 1.4014029055206512e-05,
        "epoch": 1.2214666666666667,
        "step": 9161
    },
    {
        "loss": 2.8214,
        "grad_norm": 2.24155855178833,
        "learning_rate": 1.3981914964613208e-05,
        "epoch": 1.2216,
        "step": 9162
    },
    {
        "loss": 1.8217,
        "grad_norm": 3.124873638153076,
        "learning_rate": 1.3949834946076479e-05,
        "epoch": 1.2217333333333333,
        "step": 9163
    },
    {
        "loss": 3.2244,
        "grad_norm": 2.450084686279297,
        "learning_rate": 1.3917789012303317e-05,
        "epoch": 1.2218666666666667,
        "step": 9164
    },
    {
        "loss": 2.1039,
        "grad_norm": 2.686077833175659,
        "learning_rate": 1.3885777175987324e-05,
        "epoch": 1.222,
        "step": 9165
    },
    {
        "loss": 2.5566,
        "grad_norm": 2.845750331878662,
        "learning_rate": 1.3853799449808392e-05,
        "epoch": 1.2221333333333333,
        "step": 9166
    },
    {
        "loss": 1.4591,
        "grad_norm": 2.998603582382202,
        "learning_rate": 1.3821855846432952e-05,
        "epoch": 1.2222666666666666,
        "step": 9167
    },
    {
        "loss": 2.9488,
        "grad_norm": 3.3481802940368652,
        "learning_rate": 1.3789946378514163e-05,
        "epoch": 1.2224,
        "step": 9168
    },
    {
        "loss": 2.3767,
        "grad_norm": 3.737257242202759,
        "learning_rate": 1.3758071058691335e-05,
        "epoch": 1.2225333333333332,
        "step": 9169
    },
    {
        "loss": 2.7301,
        "grad_norm": 2.597996234893799,
        "learning_rate": 1.3726229899590403e-05,
        "epoch": 1.2226666666666666,
        "step": 9170
    },
    {
        "loss": 1.7927,
        "grad_norm": 3.9190518856048584,
        "learning_rate": 1.369442291382378e-05,
        "epoch": 1.2227999999999999,
        "step": 9171
    },
    {
        "loss": 1.0685,
        "grad_norm": 4.738558292388916,
        "learning_rate": 1.3662650113990316e-05,
        "epoch": 1.2229333333333334,
        "step": 9172
    },
    {
        "loss": 2.1402,
        "grad_norm": 2.2127721309661865,
        "learning_rate": 1.3630911512675404e-05,
        "epoch": 1.2230666666666667,
        "step": 9173
    },
    {
        "loss": 2.4824,
        "grad_norm": 3.6656494140625,
        "learning_rate": 1.359920712245062e-05,
        "epoch": 1.2232,
        "step": 9174
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.743027687072754,
        "learning_rate": 1.3567536955874372e-05,
        "epoch": 1.2233333333333334,
        "step": 9175
    },
    {
        "loss": 2.0039,
        "grad_norm": 2.654444456100464,
        "learning_rate": 1.3535901025491194e-05,
        "epoch": 1.2234666666666667,
        "step": 9176
    },
    {
        "loss": 2.3372,
        "grad_norm": 3.361574649810791,
        "learning_rate": 1.3504299343832328e-05,
        "epoch": 1.2236,
        "step": 9177
    },
    {
        "loss": 1.5104,
        "grad_norm": 5.401726722717285,
        "learning_rate": 1.3472731923415138e-05,
        "epoch": 1.2237333333333333,
        "step": 9178
    },
    {
        "loss": 1.8913,
        "grad_norm": 2.6926965713500977,
        "learning_rate": 1.344119877674368e-05,
        "epoch": 1.2238666666666667,
        "step": 9179
    },
    {
        "loss": 0.5868,
        "grad_norm": 3.43035888671875,
        "learning_rate": 1.3409699916308338e-05,
        "epoch": 1.224,
        "step": 9180
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.997109889984131,
        "learning_rate": 1.3378235354585899e-05,
        "epoch": 1.2241333333333333,
        "step": 9181
    },
    {
        "loss": 2.95,
        "grad_norm": 3.4470903873443604,
        "learning_rate": 1.3346805104039662e-05,
        "epoch": 1.2242666666666666,
        "step": 9182
    },
    {
        "loss": 2.5245,
        "grad_norm": 3.302503824234009,
        "learning_rate": 1.3315409177119076e-05,
        "epoch": 1.2244,
        "step": 9183
    },
    {
        "loss": 1.5335,
        "grad_norm": 3.3843472003936768,
        "learning_rate": 1.3284047586260374e-05,
        "epoch": 1.2245333333333333,
        "step": 9184
    },
    {
        "loss": 1.8814,
        "grad_norm": 2.772963285446167,
        "learning_rate": 1.3252720343885816e-05,
        "epoch": 1.2246666666666666,
        "step": 9185
    },
    {
        "loss": 2.2533,
        "grad_norm": 4.629335403442383,
        "learning_rate": 1.3221427462404423e-05,
        "epoch": 1.2248,
        "step": 9186
    },
    {
        "loss": 2.2871,
        "grad_norm": 3.1637051105499268,
        "learning_rate": 1.3190168954211235e-05,
        "epoch": 1.2249333333333334,
        "step": 9187
    },
    {
        "loss": 0.6926,
        "grad_norm": 2.0809693336486816,
        "learning_rate": 1.3158944831687925e-05,
        "epoch": 1.2250666666666667,
        "step": 9188
    },
    {
        "loss": 2.139,
        "grad_norm": 3.648465394973755,
        "learning_rate": 1.3127755107202445e-05,
        "epoch": 1.2252,
        "step": 9189
    },
    {
        "loss": 2.3404,
        "grad_norm": 4.729356288909912,
        "learning_rate": 1.3096599793109244e-05,
        "epoch": 1.2253333333333334,
        "step": 9190
    },
    {
        "loss": 1.6959,
        "grad_norm": 5.388803958892822,
        "learning_rate": 1.3065478901748952e-05,
        "epoch": 1.2254666666666667,
        "step": 9191
    },
    {
        "loss": 2.7111,
        "grad_norm": 2.2682416439056396,
        "learning_rate": 1.3034392445448628e-05,
        "epoch": 1.2256,
        "step": 9192
    },
    {
        "loss": 2.1527,
        "grad_norm": 3.6187171936035156,
        "learning_rate": 1.3003340436521883e-05,
        "epoch": 1.2257333333333333,
        "step": 9193
    },
    {
        "loss": 1.8422,
        "grad_norm": 3.060861349105835,
        "learning_rate": 1.2972322887268384e-05,
        "epoch": 1.2258666666666667,
        "step": 9194
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.8914623260498047,
        "learning_rate": 1.2941339809974318e-05,
        "epoch": 1.226,
        "step": 9195
    },
    {
        "loss": 2.0739,
        "grad_norm": 3.8092041015625,
        "learning_rate": 1.2910391216912198e-05,
        "epoch": 1.2261333333333333,
        "step": 9196
    },
    {
        "loss": 2.133,
        "grad_norm": 2.38198184967041,
        "learning_rate": 1.2879477120340866e-05,
        "epoch": 1.2262666666666666,
        "step": 9197
    },
    {
        "loss": 1.5841,
        "grad_norm": 4.052044868469238,
        "learning_rate": 1.2848597532505568e-05,
        "epoch": 1.2264,
        "step": 9198
    },
    {
        "loss": 1.6909,
        "grad_norm": 6.085883617401123,
        "learning_rate": 1.2817752465637645e-05,
        "epoch": 1.2265333333333333,
        "step": 9199
    },
    {
        "loss": 1.5409,
        "grad_norm": 3.398348569869995,
        "learning_rate": 1.2786941931955098e-05,
        "epoch": 1.2266666666666666,
        "step": 9200
    },
    {
        "loss": 2.5098,
        "grad_norm": 3.7466020584106445,
        "learning_rate": 1.2756165943661979e-05,
        "epoch": 1.2268,
        "step": 9201
    },
    {
        "loss": 2.3057,
        "grad_norm": 3.4846267700195312,
        "learning_rate": 1.2725424512948847e-05,
        "epoch": 1.2269333333333332,
        "step": 9202
    },
    {
        "loss": 1.8507,
        "grad_norm": 3.233973264694214,
        "learning_rate": 1.2694717651992471e-05,
        "epoch": 1.2270666666666667,
        "step": 9203
    },
    {
        "loss": 0.8413,
        "grad_norm": 7.409337043762207,
        "learning_rate": 1.2664045372955856e-05,
        "epoch": 1.2272,
        "step": 9204
    },
    {
        "loss": 1.9025,
        "grad_norm": 4.945676326751709,
        "learning_rate": 1.2633407687988475e-05,
        "epoch": 1.2273333333333334,
        "step": 9205
    },
    {
        "loss": 2.5272,
        "grad_norm": 2.6820640563964844,
        "learning_rate": 1.2602804609226004e-05,
        "epoch": 1.2274666666666667,
        "step": 9206
    },
    {
        "loss": 2.306,
        "grad_norm": 3.083595037460327,
        "learning_rate": 1.2572236148790495e-05,
        "epoch": 1.2276,
        "step": 9207
    },
    {
        "loss": 2.1705,
        "grad_norm": 3.7303836345672607,
        "learning_rate": 1.2541702318790038e-05,
        "epoch": 1.2277333333333333,
        "step": 9208
    },
    {
        "loss": 1.9098,
        "grad_norm": 2.4690001010894775,
        "learning_rate": 1.2511203131319382e-05,
        "epoch": 1.2278666666666667,
        "step": 9209
    },
    {
        "loss": 2.5509,
        "grad_norm": 2.1464717388153076,
        "learning_rate": 1.2480738598459195e-05,
        "epoch": 1.228,
        "step": 9210
    },
    {
        "loss": 2.4474,
        "grad_norm": 3.6263418197631836,
        "learning_rate": 1.2450308732276771e-05,
        "epoch": 1.2281333333333333,
        "step": 9211
    },
    {
        "loss": 2.7943,
        "grad_norm": 2.299929618835449,
        "learning_rate": 1.2419913544825312e-05,
        "epoch": 1.2282666666666666,
        "step": 9212
    },
    {
        "loss": 0.6009,
        "grad_norm": 3.184321403503418,
        "learning_rate": 1.238955304814452e-05,
        "epoch": 1.2284,
        "step": 9213
    },
    {
        "loss": 2.1385,
        "grad_norm": 4.312041759490967,
        "learning_rate": 1.2359227254260276e-05,
        "epoch": 1.2285333333333333,
        "step": 9214
    },
    {
        "loss": 2.7575,
        "grad_norm": 2.864760398864746,
        "learning_rate": 1.2328936175184803e-05,
        "epoch": 1.2286666666666666,
        "step": 9215
    },
    {
        "loss": 2.1913,
        "grad_norm": 3.982778787612915,
        "learning_rate": 1.2298679822916414e-05,
        "epoch": 1.2288000000000001,
        "step": 9216
    },
    {
        "loss": 1.3749,
        "grad_norm": 5.49556827545166,
        "learning_rate": 1.2268458209439748e-05,
        "epoch": 1.2289333333333334,
        "step": 9217
    },
    {
        "loss": 1.5777,
        "grad_norm": 2.689065933227539,
        "learning_rate": 1.2238271346725771e-05,
        "epoch": 1.2290666666666668,
        "step": 9218
    },
    {
        "loss": 2.145,
        "grad_norm": 2.8086013793945312,
        "learning_rate": 1.2208119246731542e-05,
        "epoch": 1.2292,
        "step": 9219
    },
    {
        "loss": 2.635,
        "grad_norm": 2.632551431655884,
        "learning_rate": 1.2178001921400406e-05,
        "epoch": 1.2293333333333334,
        "step": 9220
    },
    {
        "loss": 1.7182,
        "grad_norm": 4.455750465393066,
        "learning_rate": 1.2147919382661955e-05,
        "epoch": 1.2294666666666667,
        "step": 9221
    },
    {
        "loss": 2.0528,
        "grad_norm": 3.0632870197296143,
        "learning_rate": 1.2117871642431988e-05,
        "epoch": 1.2296,
        "step": 9222
    },
    {
        "loss": 0.8191,
        "grad_norm": 3.8045692443847656,
        "learning_rate": 1.208785871261252e-05,
        "epoch": 1.2297333333333333,
        "step": 9223
    },
    {
        "loss": 1.7282,
        "grad_norm": 2.819758892059326,
        "learning_rate": 1.2057880605091776e-05,
        "epoch": 1.2298666666666667,
        "step": 9224
    },
    {
        "loss": 1.3289,
        "grad_norm": 5.810372352600098,
        "learning_rate": 1.2027937331744188e-05,
        "epoch": 1.23,
        "step": 9225
    },
    {
        "loss": 2.283,
        "grad_norm": 3.223670244216919,
        "learning_rate": 1.1998028904430337e-05,
        "epoch": 1.2301333333333333,
        "step": 9226
    },
    {
        "loss": 1.9713,
        "grad_norm": 2.428518533706665,
        "learning_rate": 1.1968155334997145e-05,
        "epoch": 1.2302666666666666,
        "step": 9227
    },
    {
        "loss": 2.6765,
        "grad_norm": 2.7905635833740234,
        "learning_rate": 1.1938316635277601e-05,
        "epoch": 1.2304,
        "step": 9228
    },
    {
        "loss": 2.6738,
        "grad_norm": 2.338499069213867,
        "learning_rate": 1.1908512817090833e-05,
        "epoch": 1.2305333333333333,
        "step": 9229
    },
    {
        "loss": 2.4758,
        "grad_norm": 3.55220103263855,
        "learning_rate": 1.1878743892242328e-05,
        "epoch": 1.2306666666666666,
        "step": 9230
    },
    {
        "loss": 2.1741,
        "grad_norm": 3.0018861293792725,
        "learning_rate": 1.184900987252363e-05,
        "epoch": 1.2308,
        "step": 9231
    },
    {
        "loss": 0.6748,
        "grad_norm": 3.3508176803588867,
        "learning_rate": 1.1819310769712556e-05,
        "epoch": 1.2309333333333332,
        "step": 9232
    },
    {
        "loss": 2.884,
        "grad_norm": 3.1181716918945312,
        "learning_rate": 1.1789646595572845e-05,
        "epoch": 1.2310666666666668,
        "step": 9233
    },
    {
        "loss": 2.184,
        "grad_norm": 3.6157891750335693,
        "learning_rate": 1.176001736185478e-05,
        "epoch": 1.2312,
        "step": 9234
    },
    {
        "loss": 2.2362,
        "grad_norm": 3.9554688930511475,
        "learning_rate": 1.1730423080294417e-05,
        "epoch": 1.2313333333333334,
        "step": 9235
    },
    {
        "loss": 2.2093,
        "grad_norm": 2.5742242336273193,
        "learning_rate": 1.1700863762614344e-05,
        "epoch": 1.2314666666666667,
        "step": 9236
    },
    {
        "loss": 1.6633,
        "grad_norm": 3.9037954807281494,
        "learning_rate": 1.1671339420522954e-05,
        "epoch": 1.2316,
        "step": 9237
    },
    {
        "loss": 1.9355,
        "grad_norm": 6.281238555908203,
        "learning_rate": 1.1641850065714987e-05,
        "epoch": 1.2317333333333333,
        "step": 9238
    },
    {
        "loss": 1.8392,
        "grad_norm": 2.555628538131714,
        "learning_rate": 1.161239570987126e-05,
        "epoch": 1.2318666666666667,
        "step": 9239
    },
    {
        "loss": 2.7895,
        "grad_norm": 2.888648271560669,
        "learning_rate": 1.1582976364658794e-05,
        "epoch": 1.232,
        "step": 9240
    },
    {
        "loss": 1.6298,
        "grad_norm": 4.881625175476074,
        "learning_rate": 1.155359204173071e-05,
        "epoch": 1.2321333333333333,
        "step": 9241
    },
    {
        "loss": 1.4758,
        "grad_norm": 2.735483407974243,
        "learning_rate": 1.1524242752726122e-05,
        "epoch": 1.2322666666666666,
        "step": 9242
    },
    {
        "loss": 1.3504,
        "grad_norm": 4.338563919067383,
        "learning_rate": 1.1494928509270486e-05,
        "epoch": 1.2324,
        "step": 9243
    },
    {
        "loss": 1.0277,
        "grad_norm": 3.989027976989746,
        "learning_rate": 1.1465649322975192e-05,
        "epoch": 1.2325333333333333,
        "step": 9244
    },
    {
        "loss": 1.872,
        "grad_norm": 4.209613800048828,
        "learning_rate": 1.1436405205437972e-05,
        "epoch": 1.2326666666666666,
        "step": 9245
    },
    {
        "loss": 2.2798,
        "grad_norm": 2.563229560852051,
        "learning_rate": 1.1407196168242352e-05,
        "epoch": 1.2328000000000001,
        "step": 9246
    },
    {
        "loss": 1.4548,
        "grad_norm": 4.070615768432617,
        "learning_rate": 1.1378022222958218e-05,
        "epoch": 1.2329333333333334,
        "step": 9247
    },
    {
        "loss": 2.2702,
        "grad_norm": 4.5037641525268555,
        "learning_rate": 1.1348883381141439e-05,
        "epoch": 1.2330666666666668,
        "step": 9248
    },
    {
        "loss": 1.6256,
        "grad_norm": 3.3828461170196533,
        "learning_rate": 1.131977965433405e-05,
        "epoch": 1.2332,
        "step": 9249
    },
    {
        "loss": 0.9481,
        "grad_norm": 3.785539388656616,
        "learning_rate": 1.129071105406413e-05,
        "epoch": 1.2333333333333334,
        "step": 9250
    },
    {
        "loss": 1.3767,
        "grad_norm": 4.091448783874512,
        "learning_rate": 1.1261677591845754e-05,
        "epoch": 1.2334666666666667,
        "step": 9251
    },
    {
        "loss": 2.3205,
        "grad_norm": 3.2709290981292725,
        "learning_rate": 1.1232679279179314e-05,
        "epoch": 1.2336,
        "step": 9252
    },
    {
        "loss": 2.4862,
        "grad_norm": 4.575706481933594,
        "learning_rate": 1.120371612755109e-05,
        "epoch": 1.2337333333333333,
        "step": 9253
    },
    {
        "loss": 1.5461,
        "grad_norm": 2.4171483516693115,
        "learning_rate": 1.1174788148433424e-05,
        "epoch": 1.2338666666666667,
        "step": 9254
    },
    {
        "loss": 0.6242,
        "grad_norm": 3.2817976474761963,
        "learning_rate": 1.1145895353284853e-05,
        "epoch": 1.234,
        "step": 9255
    },
    {
        "loss": 1.6561,
        "grad_norm": 3.33093523979187,
        "learning_rate": 1.1117037753549887e-05,
        "epoch": 1.2341333333333333,
        "step": 9256
    },
    {
        "loss": 2.5947,
        "grad_norm": 3.1214518547058105,
        "learning_rate": 1.1088215360659205e-05,
        "epoch": 1.2342666666666666,
        "step": 9257
    },
    {
        "loss": 2.2596,
        "grad_norm": 3.325831413269043,
        "learning_rate": 1.1059428186029263e-05,
        "epoch": 1.2344,
        "step": 9258
    },
    {
        "loss": 1.9556,
        "grad_norm": 4.061574459075928,
        "learning_rate": 1.1030676241062955e-05,
        "epoch": 1.2345333333333333,
        "step": 9259
    },
    {
        "loss": 1.2301,
        "grad_norm": 4.613046169281006,
        "learning_rate": 1.1001959537148876e-05,
        "epoch": 1.2346666666666666,
        "step": 9260
    },
    {
        "loss": 0.7588,
        "grad_norm": 3.4081063270568848,
        "learning_rate": 1.097327808566192e-05,
        "epoch": 1.2348,
        "step": 9261
    },
    {
        "loss": 2.5527,
        "grad_norm": 2.917086601257324,
        "learning_rate": 1.0944631897962899e-05,
        "epoch": 1.2349333333333332,
        "step": 9262
    },
    {
        "loss": 1.4632,
        "grad_norm": 4.381740093231201,
        "learning_rate": 1.091602098539859e-05,
        "epoch": 1.2350666666666668,
        "step": 9263
    },
    {
        "loss": 2.3684,
        "grad_norm": 5.812283515930176,
        "learning_rate": 1.0887445359301918e-05,
        "epoch": 1.2352,
        "step": 9264
    },
    {
        "loss": 1.7939,
        "grad_norm": 3.6921474933624268,
        "learning_rate": 1.085890503099175e-05,
        "epoch": 1.2353333333333334,
        "step": 9265
    },
    {
        "loss": 1.8607,
        "grad_norm": 2.6220102310180664,
        "learning_rate": 1.0830400011773133e-05,
        "epoch": 1.2354666666666667,
        "step": 9266
    },
    {
        "loss": 2.1506,
        "grad_norm": 2.883678674697876,
        "learning_rate": 1.0801930312936825e-05,
        "epoch": 1.2356,
        "step": 9267
    },
    {
        "loss": 1.6167,
        "grad_norm": 3.019360303878784,
        "learning_rate": 1.0773495945759893e-05,
        "epoch": 1.2357333333333334,
        "step": 9268
    },
    {
        "loss": 2.4361,
        "grad_norm": 2.8688716888427734,
        "learning_rate": 1.0745096921505182e-05,
        "epoch": 1.2358666666666667,
        "step": 9269
    },
    {
        "loss": 2.1612,
        "grad_norm": 3.11452579498291,
        "learning_rate": 1.0716733251421817e-05,
        "epoch": 1.236,
        "step": 9270
    },
    {
        "loss": 3.0004,
        "grad_norm": 4.002218723297119,
        "learning_rate": 1.0688404946744578e-05,
        "epoch": 1.2361333333333333,
        "step": 9271
    },
    {
        "loss": 1.6198,
        "grad_norm": 2.03167986869812,
        "learning_rate": 1.0660112018694457e-05,
        "epoch": 1.2362666666666666,
        "step": 9272
    },
    {
        "loss": 2.013,
        "grad_norm": 2.530935764312744,
        "learning_rate": 1.0631854478478386e-05,
        "epoch": 1.2364,
        "step": 9273
    },
    {
        "loss": 2.8115,
        "grad_norm": 3.296590805053711,
        "learning_rate": 1.0603632337289293e-05,
        "epoch": 1.2365333333333333,
        "step": 9274
    },
    {
        "loss": 1.1279,
        "grad_norm": 5.058025360107422,
        "learning_rate": 1.0575445606306067e-05,
        "epoch": 1.2366666666666666,
        "step": 9275
    },
    {
        "loss": 1.6713,
        "grad_norm": 3.0616862773895264,
        "learning_rate": 1.0547294296693456e-05,
        "epoch": 1.2368000000000001,
        "step": 9276
    },
    {
        "loss": 2.0072,
        "grad_norm": 3.8296334743499756,
        "learning_rate": 1.0519178419602427e-05,
        "epoch": 1.2369333333333334,
        "step": 9277
    },
    {
        "loss": 2.5025,
        "grad_norm": 4.281551837921143,
        "learning_rate": 1.0491097986169763e-05,
        "epoch": 1.2370666666666668,
        "step": 9278
    },
    {
        "loss": 1.5314,
        "grad_norm": 4.686666488647461,
        "learning_rate": 1.0463053007518108e-05,
        "epoch": 1.2372,
        "step": 9279
    },
    {
        "loss": 2.296,
        "grad_norm": 2.60927677154541,
        "learning_rate": 1.043504349475627e-05,
        "epoch": 1.2373333333333334,
        "step": 9280
    },
    {
        "loss": 2.5435,
        "grad_norm": 2.740201950073242,
        "learning_rate": 1.0407069458978891e-05,
        "epoch": 1.2374666666666667,
        "step": 9281
    },
    {
        "loss": 1.3116,
        "grad_norm": 2.7497451305389404,
        "learning_rate": 1.0379130911266577e-05,
        "epoch": 1.2376,
        "step": 9282
    },
    {
        "loss": 1.4487,
        "grad_norm": 2.4916694164276123,
        "learning_rate": 1.0351227862685897e-05,
        "epoch": 1.2377333333333334,
        "step": 9283
    },
    {
        "loss": 1.5996,
        "grad_norm": 3.7598421573638916,
        "learning_rate": 1.0323360324289367e-05,
        "epoch": 1.2378666666666667,
        "step": 9284
    },
    {
        "loss": 1.4351,
        "grad_norm": 4.406814098358154,
        "learning_rate": 1.0295528307115288e-05,
        "epoch": 1.238,
        "step": 9285
    },
    {
        "loss": 1.1911,
        "grad_norm": 5.115373611450195,
        "learning_rate": 1.0267731822188165e-05,
        "epoch": 1.2381333333333333,
        "step": 9286
    },
    {
        "loss": 1.996,
        "grad_norm": 3.5418524742126465,
        "learning_rate": 1.0239970880518247e-05,
        "epoch": 1.2382666666666666,
        "step": 9287
    },
    {
        "loss": 1.2093,
        "grad_norm": 5.134047508239746,
        "learning_rate": 1.021224549310168e-05,
        "epoch": 1.2384,
        "step": 9288
    },
    {
        "loss": 2.4994,
        "grad_norm": 3.403582811355591,
        "learning_rate": 1.0184555670920604e-05,
        "epoch": 1.2385333333333333,
        "step": 9289
    },
    {
        "loss": 0.64,
        "grad_norm": 3.718400716781616,
        "learning_rate": 1.0156901424943055e-05,
        "epoch": 1.2386666666666666,
        "step": 9290
    },
    {
        "loss": 2.1006,
        "grad_norm": 2.8480384349823,
        "learning_rate": 1.0129282766123061e-05,
        "epoch": 1.2388,
        "step": 9291
    },
    {
        "loss": 2.1942,
        "grad_norm": 2.928377628326416,
        "learning_rate": 1.0101699705400313e-05,
        "epoch": 1.2389333333333332,
        "step": 9292
    },
    {
        "loss": 1.5274,
        "grad_norm": 2.5491645336151123,
        "learning_rate": 1.0074152253700663e-05,
        "epoch": 1.2390666666666668,
        "step": 9293
    },
    {
        "loss": 4.6299,
        "grad_norm": 4.332298278808594,
        "learning_rate": 1.0046640421935672e-05,
        "epoch": 1.2392,
        "step": 9294
    },
    {
        "loss": 2.2397,
        "grad_norm": 3.520397901535034,
        "learning_rate": 1.0019164221003019e-05,
        "epoch": 1.2393333333333334,
        "step": 9295
    },
    {
        "loss": 1.9169,
        "grad_norm": 4.773158550262451,
        "learning_rate": 9.991723661785946e-06,
        "epoch": 1.2394666666666667,
        "step": 9296
    },
    {
        "loss": 1.304,
        "grad_norm": 5.174716949462891,
        "learning_rate": 9.964318755153845e-06,
        "epoch": 1.2396,
        "step": 9297
    },
    {
        "loss": 2.7065,
        "grad_norm": 2.8958847522735596,
        "learning_rate": 9.936949511961857e-06,
        "epoch": 1.2397333333333334,
        "step": 9298
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.79300594329834,
        "learning_rate": 9.909615943051076e-06,
        "epoch": 1.2398666666666667,
        "step": 9299
    },
    {
        "loss": 1.8727,
        "grad_norm": 4.793961524963379,
        "learning_rate": 9.88231805924842e-06,
        "epoch": 1.24,
        "step": 9300
    },
    {
        "loss": 1.6082,
        "grad_norm": 4.72315788269043,
        "learning_rate": 9.85505587136657e-06,
        "epoch": 1.2401333333333333,
        "step": 9301
    },
    {
        "loss": 1.1339,
        "grad_norm": 4.129166603088379,
        "learning_rate": 9.827829390204301e-06,
        "epoch": 1.2402666666666666,
        "step": 9302
    },
    {
        "loss": 2.3653,
        "grad_norm": 2.4866504669189453,
        "learning_rate": 9.800638626546032e-06,
        "epoch": 1.2404,
        "step": 9303
    },
    {
        "loss": 2.7861,
        "grad_norm": 2.7159430980682373,
        "learning_rate": 9.773483591162225e-06,
        "epoch": 1.2405333333333333,
        "step": 9304
    },
    {
        "loss": 1.6755,
        "grad_norm": 4.8198065757751465,
        "learning_rate": 9.74636429480893e-06,
        "epoch": 1.2406666666666666,
        "step": 9305
    },
    {
        "loss": 2.0393,
        "grad_norm": 3.39993953704834,
        "learning_rate": 9.719280748228288e-06,
        "epoch": 1.2408,
        "step": 9306
    },
    {
        "loss": 1.4214,
        "grad_norm": 3.112044095993042,
        "learning_rate": 9.692232962148173e-06,
        "epoch": 1.2409333333333334,
        "step": 9307
    },
    {
        "loss": 2.7094,
        "grad_norm": 4.82173490524292,
        "learning_rate": 9.665220947282305e-06,
        "epoch": 1.2410666666666668,
        "step": 9308
    },
    {
        "loss": 2.8254,
        "grad_norm": 3.3756346702575684,
        "learning_rate": 9.638244714330258e-06,
        "epoch": 1.2412,
        "step": 9309
    },
    {
        "loss": 2.4868,
        "grad_norm": 2.922102212905884,
        "learning_rate": 9.611304273977296e-06,
        "epoch": 1.2413333333333334,
        "step": 9310
    },
    {
        "loss": 1.8698,
        "grad_norm": 3.024660110473633,
        "learning_rate": 9.584399636894781e-06,
        "epoch": 1.2414666666666667,
        "step": 9311
    },
    {
        "loss": 1.3618,
        "grad_norm": 5.173595905303955,
        "learning_rate": 9.557530813739635e-06,
        "epoch": 1.2416,
        "step": 9312
    },
    {
        "loss": 1.4722,
        "grad_norm": 3.6345207691192627,
        "learning_rate": 9.530697815154698e-06,
        "epoch": 1.2417333333333334,
        "step": 9313
    },
    {
        "loss": 1.9309,
        "grad_norm": 2.79958438873291,
        "learning_rate": 9.503900651768616e-06,
        "epoch": 1.2418666666666667,
        "step": 9314
    },
    {
        "loss": 1.6932,
        "grad_norm": 3.028402328491211,
        "learning_rate": 9.477139334195817e-06,
        "epoch": 1.242,
        "step": 9315
    },
    {
        "loss": 2.6095,
        "grad_norm": 3.7538716793060303,
        "learning_rate": 9.450413873036668e-06,
        "epoch": 1.2421333333333333,
        "step": 9316
    },
    {
        "loss": 2.2479,
        "grad_norm": 4.362249374389648,
        "learning_rate": 9.423724278877033e-06,
        "epoch": 1.2422666666666666,
        "step": 9317
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.9865896701812744,
        "learning_rate": 9.39707056228889e-06,
        "epoch": 1.2424,
        "step": 9318
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.9347827434539795,
        "learning_rate": 9.37045273382976e-06,
        "epoch": 1.2425333333333333,
        "step": 9319
    },
    {
        "loss": 2.3082,
        "grad_norm": 3.0069539546966553,
        "learning_rate": 9.343870804043208e-06,
        "epoch": 1.2426666666666666,
        "step": 9320
    },
    {
        "loss": 1.9373,
        "grad_norm": 2.288738250732422,
        "learning_rate": 9.317324783458282e-06,
        "epoch": 1.2428,
        "step": 9321
    },
    {
        "loss": 0.7205,
        "grad_norm": 4.145144462585449,
        "learning_rate": 9.290814682589987e-06,
        "epoch": 1.2429333333333332,
        "step": 9322
    },
    {
        "loss": 2.0498,
        "grad_norm": 5.963926792144775,
        "learning_rate": 9.264340511939097e-06,
        "epoch": 1.2430666666666665,
        "step": 9323
    },
    {
        "loss": 2.1204,
        "grad_norm": 4.599106788635254,
        "learning_rate": 9.237902281992105e-06,
        "epoch": 1.2432,
        "step": 9324
    },
    {
        "loss": 1.6503,
        "grad_norm": 3.627276659011841,
        "learning_rate": 9.211500003221329e-06,
        "epoch": 1.2433333333333334,
        "step": 9325
    },
    {
        "loss": 2.3628,
        "grad_norm": 3.815903425216675,
        "learning_rate": 9.185133686084668e-06,
        "epoch": 1.2434666666666667,
        "step": 9326
    },
    {
        "loss": 2.9538,
        "grad_norm": 2.807845115661621,
        "learning_rate": 9.158803341026057e-06,
        "epoch": 1.2436,
        "step": 9327
    },
    {
        "loss": 1.6164,
        "grad_norm": 2.956465005874634,
        "learning_rate": 9.13250897847493e-06,
        "epoch": 1.2437333333333334,
        "step": 9328
    },
    {
        "loss": 2.1812,
        "grad_norm": 3.1672232151031494,
        "learning_rate": 9.1062506088467e-06,
        "epoch": 1.2438666666666667,
        "step": 9329
    },
    {
        "loss": 1.8005,
        "grad_norm": 2.820997476577759,
        "learning_rate": 9.080028242542238e-06,
        "epoch": 1.244,
        "step": 9330
    },
    {
        "loss": 2.333,
        "grad_norm": 4.505549430847168,
        "learning_rate": 9.053841889948422e-06,
        "epoch": 1.2441333333333333,
        "step": 9331
    },
    {
        "loss": 2.7527,
        "grad_norm": 4.137471675872803,
        "learning_rate": 9.027691561437712e-06,
        "epoch": 1.2442666666666666,
        "step": 9332
    },
    {
        "loss": 0.8396,
        "grad_norm": 3.878626823425293,
        "learning_rate": 9.001577267368378e-06,
        "epoch": 1.2444,
        "step": 9333
    },
    {
        "loss": 1.2144,
        "grad_norm": 4.5497212409973145,
        "learning_rate": 8.975499018084377e-06,
        "epoch": 1.2445333333333333,
        "step": 9334
    },
    {
        "loss": 2.4476,
        "grad_norm": 4.262983322143555,
        "learning_rate": 8.949456823915303e-06,
        "epoch": 1.2446666666666666,
        "step": 9335
    },
    {
        "loss": 2.774,
        "grad_norm": 2.346179962158203,
        "learning_rate": 8.923450695176694e-06,
        "epoch": 1.2448,
        "step": 9336
    },
    {
        "loss": 1.3345,
        "grad_norm": 3.1121814250946045,
        "learning_rate": 8.897480642169587e-06,
        "epoch": 1.2449333333333334,
        "step": 9337
    },
    {
        "loss": 2.3551,
        "grad_norm": 2.7942216396331787,
        "learning_rate": 8.871546675180808e-06,
        "epoch": 1.2450666666666668,
        "step": 9338
    },
    {
        "loss": 1.9376,
        "grad_norm": 2.5849623680114746,
        "learning_rate": 8.845648804482898e-06,
        "epoch": 1.2452,
        "step": 9339
    },
    {
        "loss": 2.279,
        "grad_norm": 2.7134995460510254,
        "learning_rate": 8.819787040334082e-06,
        "epoch": 1.2453333333333334,
        "step": 9340
    },
    {
        "loss": 0.5514,
        "grad_norm": 3.3713433742523193,
        "learning_rate": 8.793961392978323e-06,
        "epoch": 1.2454666666666667,
        "step": 9341
    },
    {
        "loss": 1.144,
        "grad_norm": 4.828198432922363,
        "learning_rate": 8.76817187264527e-06,
        "epoch": 1.2456,
        "step": 9342
    },
    {
        "loss": 1.7975,
        "grad_norm": 3.3576576709747314,
        "learning_rate": 8.742418489550185e-06,
        "epoch": 1.2457333333333334,
        "step": 9343
    },
    {
        "loss": 2.5752,
        "grad_norm": 1.085949182510376,
        "learning_rate": 8.716701253894033e-06,
        "epoch": 1.2458666666666667,
        "step": 9344
    },
    {
        "loss": 2.2981,
        "grad_norm": 3.2110543251037598,
        "learning_rate": 8.691020175863628e-06,
        "epoch": 1.246,
        "step": 9345
    },
    {
        "loss": 2.419,
        "grad_norm": 3.152812957763672,
        "learning_rate": 8.665375265631237e-06,
        "epoch": 1.2461333333333333,
        "step": 9346
    },
    {
        "loss": 2.1758,
        "grad_norm": 3.6027944087982178,
        "learning_rate": 8.639766533354909e-06,
        "epoch": 1.2462666666666666,
        "step": 9347
    },
    {
        "loss": 2.8361,
        "grad_norm": 2.473860263824463,
        "learning_rate": 8.614193989178354e-06,
        "epoch": 1.2464,
        "step": 9348
    },
    {
        "loss": 1.234,
        "grad_norm": 3.600562810897827,
        "learning_rate": 8.588657643230957e-06,
        "epoch": 1.2465333333333333,
        "step": 9349
    },
    {
        "loss": 1.7161,
        "grad_norm": 2.7525784969329834,
        "learning_rate": 8.563157505627795e-06,
        "epoch": 1.2466666666666666,
        "step": 9350
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.920942783355713,
        "learning_rate": 8.537693586469408e-06,
        "epoch": 1.2468,
        "step": 9351
    },
    {
        "loss": 2.3757,
        "grad_norm": 2.666240930557251,
        "learning_rate": 8.512265895842308e-06,
        "epoch": 1.2469333333333332,
        "step": 9352
    },
    {
        "loss": 2.8894,
        "grad_norm": 2.639383554458618,
        "learning_rate": 8.486874443818382e-06,
        "epoch": 1.2470666666666665,
        "step": 9353
    },
    {
        "loss": 1.1792,
        "grad_norm": 5.063004016876221,
        "learning_rate": 8.461519240455385e-06,
        "epoch": 1.2472,
        "step": 9354
    },
    {
        "loss": 1.9976,
        "grad_norm": 4.272223949432373,
        "learning_rate": 8.436200295796449e-06,
        "epoch": 1.2473333333333334,
        "step": 9355
    },
    {
        "loss": 2.2731,
        "grad_norm": 3.146115303039551,
        "learning_rate": 8.410917619870596e-06,
        "epoch": 1.2474666666666667,
        "step": 9356
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.902022361755371,
        "learning_rate": 8.385671222692359e-06,
        "epoch": 1.2476,
        "step": 9357
    },
    {
        "loss": 1.5124,
        "grad_norm": 3.05867338180542,
        "learning_rate": 8.360461114261953e-06,
        "epoch": 1.2477333333333334,
        "step": 9358
    },
    {
        "loss": 2.8852,
        "grad_norm": 4.535285472869873,
        "learning_rate": 8.335287304565132e-06,
        "epoch": 1.2478666666666667,
        "step": 9359
    },
    {
        "loss": 1.2098,
        "grad_norm": 4.166245937347412,
        "learning_rate": 8.310149803573297e-06,
        "epoch": 1.248,
        "step": 9360
    },
    {
        "loss": 3.0372,
        "grad_norm": 4.026748180389404,
        "learning_rate": 8.285048621243619e-06,
        "epoch": 1.2481333333333333,
        "step": 9361
    },
    {
        "loss": 1.8334,
        "grad_norm": 2.3391408920288086,
        "learning_rate": 8.259983767518631e-06,
        "epoch": 1.2482666666666666,
        "step": 9362
    },
    {
        "loss": 2.2459,
        "grad_norm": 3.510127067565918,
        "learning_rate": 8.234955252326748e-06,
        "epoch": 1.2484,
        "step": 9363
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.8371565341949463,
        "learning_rate": 8.209963085581718e-06,
        "epoch": 1.2485333333333333,
        "step": 9364
    },
    {
        "loss": 2.579,
        "grad_norm": 5.141340255737305,
        "learning_rate": 8.185007277183088e-06,
        "epoch": 1.2486666666666666,
        "step": 9365
    },
    {
        "loss": 1.1423,
        "grad_norm": 4.863449573516846,
        "learning_rate": 8.160087837015894e-06,
        "epoch": 1.2488,
        "step": 9366
    },
    {
        "loss": 1.9372,
        "grad_norm": 4.9589691162109375,
        "learning_rate": 8.135204774950912e-06,
        "epoch": 1.2489333333333335,
        "step": 9367
    },
    {
        "loss": 0.9166,
        "grad_norm": 3.9402809143066406,
        "learning_rate": 8.110358100844317e-06,
        "epoch": 1.2490666666666668,
        "step": 9368
    },
    {
        "loss": 2.1582,
        "grad_norm": 3.1292104721069336,
        "learning_rate": 8.085547824537932e-06,
        "epoch": 1.2492,
        "step": 9369
    },
    {
        "loss": 1.4805,
        "grad_norm": 4.452057361602783,
        "learning_rate": 8.060773955859314e-06,
        "epoch": 1.2493333333333334,
        "step": 9370
    },
    {
        "loss": 1.7639,
        "grad_norm": 2.764845609664917,
        "learning_rate": 8.036036504621392e-06,
        "epoch": 1.2494666666666667,
        "step": 9371
    },
    {
        "loss": 1.7556,
        "grad_norm": 2.5040030479431152,
        "learning_rate": 8.01133548062275e-06,
        "epoch": 1.2496,
        "step": 9372
    },
    {
        "loss": 1.9676,
        "grad_norm": 3.7199230194091797,
        "learning_rate": 7.986670893647563e-06,
        "epoch": 1.2497333333333334,
        "step": 9373
    },
    {
        "loss": 2.375,
        "grad_norm": 3.6591618061065674,
        "learning_rate": 7.96204275346557e-06,
        "epoch": 1.2498666666666667,
        "step": 9374
    },
    {
        "loss": 2.6973,
        "grad_norm": 2.1195218563079834,
        "learning_rate": 7.937451069832102e-06,
        "epoch": 1.25,
        "step": 9375
    },
    {
        "loss": 1.7631,
        "grad_norm": 4.750382423400879,
        "learning_rate": 7.912895852487878e-06,
        "epoch": 1.2501333333333333,
        "step": 9376
    },
    {
        "loss": 2.8149,
        "grad_norm": 1.794213891029358,
        "learning_rate": 7.88837711115944e-06,
        "epoch": 1.2502666666666666,
        "step": 9377
    },
    {
        "loss": 2.6068,
        "grad_norm": 3.4712257385253906,
        "learning_rate": 7.863894855558652e-06,
        "epoch": 1.2504,
        "step": 9378
    },
    {
        "loss": 3.0819,
        "grad_norm": 3.9519519805908203,
        "learning_rate": 7.839449095383122e-06,
        "epoch": 1.2505333333333333,
        "step": 9379
    },
    {
        "loss": 1.2313,
        "grad_norm": 4.017632007598877,
        "learning_rate": 7.815039840315775e-06,
        "epoch": 1.2506666666666666,
        "step": 9380
    },
    {
        "loss": 2.643,
        "grad_norm": 2.157015085220337,
        "learning_rate": 7.790667100025273e-06,
        "epoch": 1.2508,
        "step": 9381
    },
    {
        "loss": 1.2319,
        "grad_norm": 2.7770462036132812,
        "learning_rate": 7.766330884165729e-06,
        "epoch": 1.2509333333333332,
        "step": 9382
    },
    {
        "loss": 1.8792,
        "grad_norm": 2.8472845554351807,
        "learning_rate": 7.742031202376798e-06,
        "epoch": 1.2510666666666665,
        "step": 9383
    },
    {
        "loss": 2.3496,
        "grad_norm": 2.9004268646240234,
        "learning_rate": 7.717768064283726e-06,
        "epoch": 1.2511999999999999,
        "step": 9384
    },
    {
        "loss": 2.1313,
        "grad_norm": 3.9161217212677,
        "learning_rate": 7.69354147949708e-06,
        "epoch": 1.2513333333333334,
        "step": 9385
    },
    {
        "loss": 1.7413,
        "grad_norm": 3.534623861312866,
        "learning_rate": 7.669351457613239e-06,
        "epoch": 1.2514666666666667,
        "step": 9386
    },
    {
        "loss": 0.6232,
        "grad_norm": 3.033015012741089,
        "learning_rate": 7.645198008213838e-06,
        "epoch": 1.2516,
        "step": 9387
    },
    {
        "loss": 2.4613,
        "grad_norm": 3.251512050628662,
        "learning_rate": 7.621081140866282e-06,
        "epoch": 1.2517333333333334,
        "step": 9388
    },
    {
        "loss": 2.6499,
        "grad_norm": 3.6293785572052,
        "learning_rate": 7.597000865123205e-06,
        "epoch": 1.2518666666666667,
        "step": 9389
    },
    {
        "loss": 0.9642,
        "grad_norm": 3.6877472400665283,
        "learning_rate": 7.572957190522934e-06,
        "epoch": 1.252,
        "step": 9390
    },
    {
        "loss": 1.8009,
        "grad_norm": 2.655653953552246,
        "learning_rate": 7.548950126589249e-06,
        "epoch": 1.2521333333333333,
        "step": 9391
    },
    {
        "loss": 2.4412,
        "grad_norm": 3.160224437713623,
        "learning_rate": 7.52497968283149e-06,
        "epoch": 1.2522666666666666,
        "step": 9392
    },
    {
        "loss": 1.6023,
        "grad_norm": 2.395294427871704,
        "learning_rate": 7.5010458687443605e-06,
        "epoch": 1.2524,
        "step": 9393
    },
    {
        "loss": 2.4302,
        "grad_norm": 3.7428927421569824,
        "learning_rate": 7.477148693808089e-06,
        "epoch": 1.2525333333333333,
        "step": 9394
    },
    {
        "loss": 2.4198,
        "grad_norm": 2.782733917236328,
        "learning_rate": 7.453288167488559e-06,
        "epoch": 1.2526666666666666,
        "step": 9395
    },
    {
        "loss": 1.6208,
        "grad_norm": 3.7333972454071045,
        "learning_rate": 7.42946429923691e-06,
        "epoch": 1.2528000000000001,
        "step": 9396
    },
    {
        "loss": 1.7298,
        "grad_norm": 4.8035688400268555,
        "learning_rate": 7.40567709848986e-06,
        "epoch": 1.2529333333333335,
        "step": 9397
    },
    {
        "loss": 2.1268,
        "grad_norm": 3.321668863296509,
        "learning_rate": 7.381926574669628e-06,
        "epoch": 1.2530666666666668,
        "step": 9398
    },
    {
        "loss": 2.5376,
        "grad_norm": 4.693068981170654,
        "learning_rate": 7.358212737183856e-06,
        "epoch": 1.2532,
        "step": 9399
    },
    {
        "loss": 1.6191,
        "grad_norm": 2.1911301612854004,
        "learning_rate": 7.334535595425751e-06,
        "epoch": 1.2533333333333334,
        "step": 9400
    },
    {
        "loss": 1.5418,
        "grad_norm": 3.837984323501587,
        "learning_rate": 7.310895158773756e-06,
        "epoch": 1.2534666666666667,
        "step": 9401
    },
    {
        "loss": 2.1672,
        "grad_norm": 3.5281624794006348,
        "learning_rate": 7.28729143659207e-06,
        "epoch": 1.2536,
        "step": 9402
    },
    {
        "loss": 2.2401,
        "grad_norm": 3.7791965007781982,
        "learning_rate": 7.263724438230113e-06,
        "epoch": 1.2537333333333334,
        "step": 9403
    },
    {
        "loss": 2.1472,
        "grad_norm": 3.703202486038208,
        "learning_rate": 7.240194173022941e-06,
        "epoch": 1.2538666666666667,
        "step": 9404
    },
    {
        "loss": 1.8369,
        "grad_norm": 3.61545991897583,
        "learning_rate": 7.216700650290942e-06,
        "epoch": 1.254,
        "step": 9405
    },
    {
        "loss": 1.808,
        "grad_norm": 4.6008992195129395,
        "learning_rate": 7.193243879339928e-06,
        "epoch": 1.2541333333333333,
        "step": 9406
    },
    {
        "loss": 1.7256,
        "grad_norm": 3.3055810928344727,
        "learning_rate": 7.1698238694612455e-06,
        "epoch": 1.2542666666666666,
        "step": 9407
    },
    {
        "loss": 1.6949,
        "grad_norm": 4.727843761444092,
        "learning_rate": 7.14644062993165e-06,
        "epoch": 1.2544,
        "step": 9408
    },
    {
        "loss": 1.5146,
        "grad_norm": 2.599674940109253,
        "learning_rate": 7.1230941700133554e-06,
        "epoch": 1.2545333333333333,
        "step": 9409
    },
    {
        "loss": 2.9929,
        "grad_norm": 3.119335651397705,
        "learning_rate": 7.0997844989538435e-06,
        "epoch": 1.2546666666666666,
        "step": 9410
    },
    {
        "loss": 1.705,
        "grad_norm": 4.236064434051514,
        "learning_rate": 7.076511625986315e-06,
        "epoch": 1.2548,
        "step": 9411
    },
    {
        "loss": 1.5627,
        "grad_norm": 4.093939304351807,
        "learning_rate": 7.053275560329098e-06,
        "epoch": 1.2549333333333332,
        "step": 9412
    },
    {
        "loss": 2.9451,
        "grad_norm": 3.202967405319214,
        "learning_rate": 7.030076311186218e-06,
        "epoch": 1.2550666666666666,
        "step": 9413
    },
    {
        "loss": 2.3343,
        "grad_norm": 3.155512809753418,
        "learning_rate": 7.006913887746847e-06,
        "epoch": 1.2551999999999999,
        "step": 9414
    },
    {
        "loss": 1.7147,
        "grad_norm": 3.0439581871032715,
        "learning_rate": 6.983788299185745e-06,
        "epoch": 1.2553333333333334,
        "step": 9415
    },
    {
        "loss": 1.4049,
        "grad_norm": 5.152209758758545,
        "learning_rate": 6.960699554663041e-06,
        "epoch": 1.2554666666666667,
        "step": 9416
    },
    {
        "loss": 0.8285,
        "grad_norm": 2.955230951309204,
        "learning_rate": 6.9376476633243095e-06,
        "epoch": 1.2556,
        "step": 9417
    },
    {
        "loss": 2.248,
        "grad_norm": 2.643038034439087,
        "learning_rate": 6.914632634300422e-06,
        "epoch": 1.2557333333333334,
        "step": 9418
    },
    {
        "loss": 1.1077,
        "grad_norm": 4.449662685394287,
        "learning_rate": 6.891654476707699e-06,
        "epoch": 1.2558666666666667,
        "step": 9419
    },
    {
        "loss": 1.1919,
        "grad_norm": 5.394664764404297,
        "learning_rate": 6.868713199647924e-06,
        "epoch": 1.256,
        "step": 9420
    },
    {
        "loss": 2.0257,
        "grad_norm": 4.880651473999023,
        "learning_rate": 6.845808812208188e-06,
        "epoch": 1.2561333333333333,
        "step": 9421
    },
    {
        "loss": 1.8042,
        "grad_norm": 3.619157314300537,
        "learning_rate": 6.822941323460985e-06,
        "epoch": 1.2562666666666666,
        "step": 9422
    },
    {
        "loss": 2.1037,
        "grad_norm": 3.717571258544922,
        "learning_rate": 6.800110742464216e-06,
        "epoch": 1.2564,
        "step": 9423
    },
    {
        "loss": 1.3535,
        "grad_norm": 3.3079535961151123,
        "learning_rate": 6.777317078261148e-06,
        "epoch": 1.2565333333333333,
        "step": 9424
    },
    {
        "loss": 2.271,
        "grad_norm": 3.1469225883483887,
        "learning_rate": 6.754560339880445e-06,
        "epoch": 1.2566666666666666,
        "step": 9425
    },
    {
        "loss": 1.9512,
        "grad_norm": 3.3894307613372803,
        "learning_rate": 6.731840536336121e-06,
        "epoch": 1.2568,
        "step": 9426
    },
    {
        "loss": 1.7652,
        "grad_norm": 3.4964170455932617,
        "learning_rate": 6.7091576766275865e-06,
        "epoch": 1.2569333333333335,
        "step": 9427
    },
    {
        "loss": 2.3109,
        "grad_norm": 3.9267070293426514,
        "learning_rate": 6.6865117697395115e-06,
        "epoch": 1.2570666666666668,
        "step": 9428
    },
    {
        "loss": 2.5559,
        "grad_norm": 2.9136979579925537,
        "learning_rate": 6.663902824642132e-06,
        "epoch": 1.2572,
        "step": 9429
    },
    {
        "loss": 1.6756,
        "grad_norm": 2.8355367183685303,
        "learning_rate": 6.641330850290894e-06,
        "epoch": 1.2573333333333334,
        "step": 9430
    },
    {
        "loss": 1.9412,
        "grad_norm": 3.5424835681915283,
        "learning_rate": 6.61879585562657e-06,
        "epoch": 1.2574666666666667,
        "step": 9431
    },
    {
        "loss": 1.8903,
        "grad_norm": 3.3718745708465576,
        "learning_rate": 6.5962978495754166e-06,
        "epoch": 1.2576,
        "step": 9432
    },
    {
        "loss": 1.0865,
        "grad_norm": 3.752274990081787,
        "learning_rate": 6.573836841048941e-06,
        "epoch": 1.2577333333333334,
        "step": 9433
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.4787464141845703,
        "learning_rate": 6.5514128389440935e-06,
        "epoch": 1.2578666666666667,
        "step": 9434
    },
    {
        "loss": 1.851,
        "grad_norm": 3.628099203109741,
        "learning_rate": 6.529025852142945e-06,
        "epoch": 1.258,
        "step": 9435
    },
    {
        "loss": 1.4014,
        "grad_norm": 3.023369312286377,
        "learning_rate": 6.506675889513225e-06,
        "epoch": 1.2581333333333333,
        "step": 9436
    },
    {
        "loss": 2.2047,
        "grad_norm": 3.819705009460449,
        "learning_rate": 6.484362959907686e-06,
        "epoch": 1.2582666666666666,
        "step": 9437
    },
    {
        "loss": 2.2842,
        "grad_norm": 3.3976023197174072,
        "learning_rate": 6.462087072164713e-06,
        "epoch": 1.2584,
        "step": 9438
    },
    {
        "loss": 1.6138,
        "grad_norm": 4.7680511474609375,
        "learning_rate": 6.439848235107715e-06,
        "epoch": 1.2585333333333333,
        "step": 9439
    },
    {
        "loss": 1.8978,
        "grad_norm": 3.392683267593384,
        "learning_rate": 6.4176464575456455e-06,
        "epoch": 1.2586666666666666,
        "step": 9440
    },
    {
        "loss": 1.8369,
        "grad_norm": 1.9569171667099,
        "learning_rate": 6.3954817482726585e-06,
        "epoch": 1.2588,
        "step": 9441
    },
    {
        "loss": 1.405,
        "grad_norm": 2.9885103702545166,
        "learning_rate": 6.37335411606832e-06,
        "epoch": 1.2589333333333332,
        "step": 9442
    },
    {
        "loss": 1.9469,
        "grad_norm": 3.7628891468048096,
        "learning_rate": 6.351263569697485e-06,
        "epoch": 1.2590666666666666,
        "step": 9443
    },
    {
        "loss": 1.4931,
        "grad_norm": 3.010664701461792,
        "learning_rate": 6.329210117910189e-06,
        "epoch": 1.2591999999999999,
        "step": 9444
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.7467074394226074,
        "learning_rate": 6.307193769441977e-06,
        "epoch": 1.2593333333333334,
        "step": 9445
    },
    {
        "loss": 2.5205,
        "grad_norm": 3.101705551147461,
        "learning_rate": 6.2852145330135175e-06,
        "epoch": 1.2594666666666667,
        "step": 9446
    },
    {
        "loss": 1.8758,
        "grad_norm": 4.19790506362915,
        "learning_rate": 6.263272417330979e-06,
        "epoch": 1.2596,
        "step": 9447
    },
    {
        "loss": 1.7029,
        "grad_norm": 3.437854051589966,
        "learning_rate": 6.241367431085588e-06,
        "epoch": 1.2597333333333334,
        "step": 9448
    },
    {
        "loss": 1.5844,
        "grad_norm": 3.9702279567718506,
        "learning_rate": 6.219499582954036e-06,
        "epoch": 1.2598666666666667,
        "step": 9449
    },
    {
        "loss": 1.6489,
        "grad_norm": 3.201129674911499,
        "learning_rate": 6.197668881598251e-06,
        "epoch": 1.26,
        "step": 9450
    },
    {
        "loss": 2.7993,
        "grad_norm": 3.0756473541259766,
        "learning_rate": 6.17587533566546e-06,
        "epoch": 1.2601333333333333,
        "step": 9451
    },
    {
        "loss": 1.3877,
        "grad_norm": 3.889261484146118,
        "learning_rate": 6.154118953788157e-06,
        "epoch": 1.2602666666666666,
        "step": 9452
    },
    {
        "loss": 1.9618,
        "grad_norm": 3.067441463470459,
        "learning_rate": 6.132399744584049e-06,
        "epoch": 1.2604,
        "step": 9453
    },
    {
        "loss": 2.5487,
        "grad_norm": 4.855341911315918,
        "learning_rate": 6.110717716656289e-06,
        "epoch": 1.2605333333333333,
        "step": 9454
    },
    {
        "loss": 2.0114,
        "grad_norm": 2.8818519115448,
        "learning_rate": 6.089072878593183e-06,
        "epoch": 1.2606666666666666,
        "step": 9455
    },
    {
        "loss": 2.3184,
        "grad_norm": 3.437737464904785,
        "learning_rate": 6.067465238968261e-06,
        "epoch": 1.2608,
        "step": 9456
    },
    {
        "loss": 1.6849,
        "grad_norm": 3.60103178024292,
        "learning_rate": 6.045894806340435e-06,
        "epoch": 1.2609333333333335,
        "step": 9457
    },
    {
        "loss": 1.71,
        "grad_norm": 3.808851718902588,
        "learning_rate": 6.024361589253824e-06,
        "epoch": 1.2610666666666668,
        "step": 9458
    },
    {
        "loss": 2.4838,
        "grad_norm": 3.6749346256256104,
        "learning_rate": 6.002865596237839e-06,
        "epoch": 1.2612,
        "step": 9459
    },
    {
        "loss": 0.9096,
        "grad_norm": 3.8259060382843018,
        "learning_rate": 5.981406835807002e-06,
        "epoch": 1.2613333333333334,
        "step": 9460
    },
    {
        "loss": 1.5651,
        "grad_norm": 2.927823066711426,
        "learning_rate": 5.959985316461347e-06,
        "epoch": 1.2614666666666667,
        "step": 9461
    },
    {
        "loss": 2.257,
        "grad_norm": 5.2281389236450195,
        "learning_rate": 5.9386010466858764e-06,
        "epoch": 1.2616,
        "step": 9462
    },
    {
        "loss": 1.4931,
        "grad_norm": 3.761753559112549,
        "learning_rate": 5.917254034951081e-06,
        "epoch": 1.2617333333333334,
        "step": 9463
    },
    {
        "loss": 1.9767,
        "grad_norm": 4.926163196563721,
        "learning_rate": 5.895944289712552e-06,
        "epoch": 1.2618666666666667,
        "step": 9464
    },
    {
        "loss": 1.9973,
        "grad_norm": 2.353194236755371,
        "learning_rate": 5.874671819411126e-06,
        "epoch": 1.262,
        "step": 9465
    },
    {
        "loss": 2.6932,
        "grad_norm": 2.224167823791504,
        "learning_rate": 5.853436632472908e-06,
        "epoch": 1.2621333333333333,
        "step": 9466
    },
    {
        "loss": 1.506,
        "grad_norm": 3.1893203258514404,
        "learning_rate": 5.8322387373092145e-06,
        "epoch": 1.2622666666666666,
        "step": 9467
    },
    {
        "loss": 2.3394,
        "grad_norm": 2.3350441455841064,
        "learning_rate": 5.811078142316695e-06,
        "epoch": 1.2624,
        "step": 9468
    },
    {
        "loss": 1.3923,
        "grad_norm": 3.274693489074707,
        "learning_rate": 5.789954855877e-06,
        "epoch": 1.2625333333333333,
        "step": 9469
    },
    {
        "loss": 2.6331,
        "grad_norm": 2.182328939437866,
        "learning_rate": 5.768868886357237e-06,
        "epoch": 1.2626666666666666,
        "step": 9470
    },
    {
        "loss": 1.4865,
        "grad_norm": 3.5471556186676025,
        "learning_rate": 5.747820242109536e-06,
        "epoch": 1.2628,
        "step": 9471
    },
    {
        "loss": 0.6633,
        "grad_norm": 3.248183488845825,
        "learning_rate": 5.726808931471472e-06,
        "epoch": 1.2629333333333332,
        "step": 9472
    },
    {
        "loss": 2.5085,
        "grad_norm": 3.32524037361145,
        "learning_rate": 5.705834962765577e-06,
        "epoch": 1.2630666666666666,
        "step": 9473
    },
    {
        "loss": 1.1201,
        "grad_norm": 4.412572383880615,
        "learning_rate": 5.68489834429976e-06,
        "epoch": 1.2631999999999999,
        "step": 9474
    },
    {
        "loss": 2.0682,
        "grad_norm": 3.624579668045044,
        "learning_rate": 5.663999084367067e-06,
        "epoch": 1.2633333333333332,
        "step": 9475
    },
    {
        "loss": 1.803,
        "grad_norm": 3.4032230377197266,
        "learning_rate": 5.6431371912457956e-06,
        "epoch": 1.2634666666666667,
        "step": 9476
    },
    {
        "loss": 3.0237,
        "grad_norm": 5.6512532234191895,
        "learning_rate": 5.6223126731994145e-06,
        "epoch": 1.2636,
        "step": 9477
    },
    {
        "loss": 1.8162,
        "grad_norm": 4.515493392944336,
        "learning_rate": 5.601525538476504e-06,
        "epoch": 1.2637333333333334,
        "step": 9478
    },
    {
        "loss": 2.2712,
        "grad_norm": 5.051154613494873,
        "learning_rate": 5.580775795311033e-06,
        "epoch": 1.2638666666666667,
        "step": 9479
    },
    {
        "loss": 1.7268,
        "grad_norm": 3.144052267074585,
        "learning_rate": 5.560063451922004e-06,
        "epoch": 1.264,
        "step": 9480
    },
    {
        "loss": 1.7536,
        "grad_norm": 3.3766322135925293,
        "learning_rate": 5.53938851651361e-06,
        "epoch": 1.2641333333333333,
        "step": 9481
    },
    {
        "loss": 1.8834,
        "grad_norm": 4.395660877227783,
        "learning_rate": 5.518750997275279e-06,
        "epoch": 1.2642666666666666,
        "step": 9482
    },
    {
        "loss": 2.2725,
        "grad_norm": 4.57855224609375,
        "learning_rate": 5.498150902381638e-06,
        "epoch": 1.2644,
        "step": 9483
    },
    {
        "loss": 2.0008,
        "grad_norm": 4.319489002227783,
        "learning_rate": 5.477588239992415e-06,
        "epoch": 1.2645333333333333,
        "step": 9484
    },
    {
        "loss": 0.433,
        "grad_norm": 3.1750447750091553,
        "learning_rate": 5.457063018252595e-06,
        "epoch": 1.2646666666666666,
        "step": 9485
    },
    {
        "loss": 1.9339,
        "grad_norm": 4.608950614929199,
        "learning_rate": 5.436575245292263e-06,
        "epoch": 1.2648,
        "step": 9486
    },
    {
        "loss": 2.3449,
        "grad_norm": 3.7464730739593506,
        "learning_rate": 5.41612492922664e-06,
        "epoch": 1.2649333333333335,
        "step": 9487
    },
    {
        "loss": 1.3606,
        "grad_norm": 3.427999973297119,
        "learning_rate": 5.395712078156267e-06,
        "epoch": 1.2650666666666668,
        "step": 9488
    },
    {
        "loss": 2.3736,
        "grad_norm": 3.9887783527374268,
        "learning_rate": 5.375336700166711e-06,
        "epoch": 1.2652,
        "step": 9489
    },
    {
        "loss": 2.1528,
        "grad_norm": 2.5515646934509277,
        "learning_rate": 5.354998803328681e-06,
        "epoch": 1.2653333333333334,
        "step": 9490
    },
    {
        "loss": 2.6526,
        "grad_norm": 3.7931017875671387,
        "learning_rate": 5.334698395698123e-06,
        "epoch": 1.2654666666666667,
        "step": 9491
    },
    {
        "loss": 0.6084,
        "grad_norm": 3.5035152435302734,
        "learning_rate": 5.314435485316071e-06,
        "epoch": 1.2656,
        "step": 9492
    },
    {
        "loss": 1.5784,
        "grad_norm": 3.274529457092285,
        "learning_rate": 5.294210080208828e-06,
        "epoch": 1.2657333333333334,
        "step": 9493
    },
    {
        "loss": 1.1893,
        "grad_norm": 4.343080997467041,
        "learning_rate": 5.274022188387606e-06,
        "epoch": 1.2658666666666667,
        "step": 9494
    },
    {
        "loss": 1.9555,
        "grad_norm": 3.2683682441711426,
        "learning_rate": 5.2538718178489984e-06,
        "epoch": 1.266,
        "step": 9495
    },
    {
        "loss": 2.3554,
        "grad_norm": 2.8888051509857178,
        "learning_rate": 5.233758976574554e-06,
        "epoch": 1.2661333333333333,
        "step": 9496
    },
    {
        "loss": 1.9434,
        "grad_norm": 3.161745548248291,
        "learning_rate": 5.2136836725311665e-06,
        "epoch": 1.2662666666666667,
        "step": 9497
    },
    {
        "loss": 2.0361,
        "grad_norm": 4.1122822761535645,
        "learning_rate": 5.193645913670608e-06,
        "epoch": 1.2664,
        "step": 9498
    },
    {
        "loss": 2.3425,
        "grad_norm": 3.625098466873169,
        "learning_rate": 5.173645707929953e-06,
        "epoch": 1.2665333333333333,
        "step": 9499
    },
    {
        "loss": 1.844,
        "grad_norm": 2.7381205558776855,
        "learning_rate": 5.15368306323134e-06,
        "epoch": 1.2666666666666666,
        "step": 9500
    },
    {
        "loss": 2.4723,
        "grad_norm": 3.271941661834717,
        "learning_rate": 5.1337579874820575e-06,
        "epoch": 1.2668,
        "step": 9501
    },
    {
        "loss": 2.3025,
        "grad_norm": 3.0093178749084473,
        "learning_rate": 5.113870488574502e-06,
        "epoch": 1.2669333333333332,
        "step": 9502
    },
    {
        "loss": 1.7783,
        "grad_norm": 3.7749416828155518,
        "learning_rate": 5.094020574386105e-06,
        "epoch": 1.2670666666666666,
        "step": 9503
    },
    {
        "loss": 0.6851,
        "grad_norm": 2.9126343727111816,
        "learning_rate": 5.074208252779588e-06,
        "epoch": 1.2671999999999999,
        "step": 9504
    },
    {
        "loss": 2.0787,
        "grad_norm": 3.257521152496338,
        "learning_rate": 5.054433531602609e-06,
        "epoch": 1.2673333333333332,
        "step": 9505
    },
    {
        "loss": 2.6553,
        "grad_norm": 2.507068634033203,
        "learning_rate": 5.0346964186880785e-06,
        "epoch": 1.2674666666666667,
        "step": 9506
    },
    {
        "loss": 2.3121,
        "grad_norm": 1.5948494672775269,
        "learning_rate": 5.014996921853832e-06,
        "epoch": 1.2676,
        "step": 9507
    },
    {
        "loss": 2.0385,
        "grad_norm": 3.8238909244537354,
        "learning_rate": 4.99533504890296e-06,
        "epoch": 1.2677333333333334,
        "step": 9508
    },
    {
        "loss": 2.3281,
        "grad_norm": 3.5248327255249023,
        "learning_rate": 4.975710807623612e-06,
        "epoch": 1.2678666666666667,
        "step": 9509
    },
    {
        "loss": 2.2364,
        "grad_norm": 3.3894107341766357,
        "learning_rate": 4.956124205789003e-06,
        "epoch": 1.268,
        "step": 9510
    },
    {
        "loss": 1.43,
        "grad_norm": 3.5258567333221436,
        "learning_rate": 4.9365752511574605e-06,
        "epoch": 1.2681333333333333,
        "step": 9511
    },
    {
        "loss": 2.0363,
        "grad_norm": 3.679849147796631,
        "learning_rate": 4.917063951472334e-06,
        "epoch": 1.2682666666666667,
        "step": 9512
    },
    {
        "loss": 1.2092,
        "grad_norm": 3.6219120025634766,
        "learning_rate": 4.89759031446222e-06,
        "epoch": 1.2684,
        "step": 9513
    },
    {
        "loss": 1.7673,
        "grad_norm": 3.9162068367004395,
        "learning_rate": 4.878154347840613e-06,
        "epoch": 1.2685333333333333,
        "step": 9514
    },
    {
        "loss": 1.79,
        "grad_norm": 3.5445141792297363,
        "learning_rate": 4.858756059306191e-06,
        "epoch": 1.2686666666666666,
        "step": 9515
    },
    {
        "loss": 1.9478,
        "grad_norm": 3.431042432785034,
        "learning_rate": 4.839395456542661e-06,
        "epoch": 1.2688,
        "step": 9516
    },
    {
        "loss": 2.4463,
        "grad_norm": 3.341243028640747,
        "learning_rate": 4.820072547218846e-06,
        "epoch": 1.2689333333333335,
        "step": 9517
    },
    {
        "loss": 1.0125,
        "grad_norm": 4.012331485748291,
        "learning_rate": 4.800787338988655e-06,
        "epoch": 1.2690666666666668,
        "step": 9518
    },
    {
        "loss": 2.5984,
        "grad_norm": 2.0291590690612793,
        "learning_rate": 4.781539839490934e-06,
        "epoch": 1.2692,
        "step": 9519
    },
    {
        "loss": 2.225,
        "grad_norm": 2.9354088306427,
        "learning_rate": 4.762330056349751e-06,
        "epoch": 1.2693333333333334,
        "step": 9520
    },
    {
        "loss": 1.3028,
        "grad_norm": 3.2150919437408447,
        "learning_rate": 4.743157997174119e-06,
        "epoch": 1.2694666666666667,
        "step": 9521
    },
    {
        "loss": 2.2883,
        "grad_norm": 3.276843786239624,
        "learning_rate": 4.7240236695582305e-06,
        "epoch": 1.2696,
        "step": 9522
    },
    {
        "loss": 1.9983,
        "grad_norm": 2.425717830657959,
        "learning_rate": 4.704927081081201e-06,
        "epoch": 1.2697333333333334,
        "step": 9523
    },
    {
        "loss": 2.327,
        "grad_norm": 2.4310545921325684,
        "learning_rate": 4.685868239307245e-06,
        "epoch": 1.2698666666666667,
        "step": 9524
    },
    {
        "loss": 1.6116,
        "grad_norm": 4.467895984649658,
        "learning_rate": 4.666847151785658e-06,
        "epoch": 1.27,
        "step": 9525
    },
    {
        "loss": 1.7574,
        "grad_norm": 3.8444602489471436,
        "learning_rate": 4.6478638260507575e-06,
        "epoch": 1.2701333333333333,
        "step": 9526
    },
    {
        "loss": 2.5286,
        "grad_norm": 3.2342987060546875,
        "learning_rate": 4.628918269621929e-06,
        "epoch": 1.2702666666666667,
        "step": 9527
    },
    {
        "loss": 2.0498,
        "grad_norm": 3.829983949661255,
        "learning_rate": 4.610010490003491e-06,
        "epoch": 1.2704,
        "step": 9528
    },
    {
        "loss": 2.3825,
        "grad_norm": 3.5185675621032715,
        "learning_rate": 4.591140494684965e-06,
        "epoch": 1.2705333333333333,
        "step": 9529
    },
    {
        "loss": 2.0187,
        "grad_norm": 2.43813419342041,
        "learning_rate": 4.572308291140759e-06,
        "epoch": 1.2706666666666666,
        "step": 9530
    },
    {
        "loss": 1.6655,
        "grad_norm": 3.641855001449585,
        "learning_rate": 4.553513886830452e-06,
        "epoch": 1.2708,
        "step": 9531
    },
    {
        "loss": 1.3716,
        "grad_norm": 2.767014741897583,
        "learning_rate": 4.5347572891984655e-06,
        "epoch": 1.2709333333333332,
        "step": 9532
    },
    {
        "loss": 1.6705,
        "grad_norm": 4.820069313049316,
        "learning_rate": 4.5160385056744244e-06,
        "epoch": 1.2710666666666666,
        "step": 9533
    },
    {
        "loss": 0.854,
        "grad_norm": 3.9984376430511475,
        "learning_rate": 4.4973575436728865e-06,
        "epoch": 1.2711999999999999,
        "step": 9534
    },
    {
        "loss": 2.2985,
        "grad_norm": 2.6083672046661377,
        "learning_rate": 4.4787144105934545e-06,
        "epoch": 1.2713333333333332,
        "step": 9535
    },
    {
        "loss": 1.809,
        "grad_norm": 10.715883255004883,
        "learning_rate": 4.460109113820732e-06,
        "epoch": 1.2714666666666667,
        "step": 9536
    },
    {
        "loss": 2.3654,
        "grad_norm": 3.805098533630371,
        "learning_rate": 4.441541660724291e-06,
        "epoch": 1.2716,
        "step": 9537
    },
    {
        "loss": 2.1672,
        "grad_norm": 3.1088361740112305,
        "learning_rate": 4.423012058658849e-06,
        "epoch": 1.2717333333333334,
        "step": 9538
    },
    {
        "loss": 1.5184,
        "grad_norm": 4.96180534362793,
        "learning_rate": 4.404520314964e-06,
        "epoch": 1.2718666666666667,
        "step": 9539
    },
    {
        "loss": 1.8365,
        "grad_norm": 4.031659126281738,
        "learning_rate": 4.386066436964365e-06,
        "epoch": 1.272,
        "step": 9540
    },
    {
        "loss": 2.312,
        "grad_norm": 3.5694689750671387,
        "learning_rate": 4.367650431969616e-06,
        "epoch": 1.2721333333333333,
        "step": 9541
    },
    {
        "loss": 1.9608,
        "grad_norm": 3.108242988586426,
        "learning_rate": 4.3492723072743765e-06,
        "epoch": 1.2722666666666667,
        "step": 9542
    },
    {
        "loss": 1.0574,
        "grad_norm": 3.9589717388153076,
        "learning_rate": 4.3309320701583e-06,
        "epoch": 1.2724,
        "step": 9543
    },
    {
        "loss": 1.5961,
        "grad_norm": 4.686847686767578,
        "learning_rate": 4.312629727886053e-06,
        "epoch": 1.2725333333333333,
        "step": 9544
    },
    {
        "loss": 2.031,
        "grad_norm": 4.13862419128418,
        "learning_rate": 4.294365287707203e-06,
        "epoch": 1.2726666666666666,
        "step": 9545
    },
    {
        "loss": 3.0665,
        "grad_norm": 2.9608359336853027,
        "learning_rate": 4.276138756856329e-06,
        "epoch": 1.2728,
        "step": 9546
    },
    {
        "loss": 1.7161,
        "grad_norm": 3.63533878326416,
        "learning_rate": 4.257950142553125e-06,
        "epoch": 1.2729333333333333,
        "step": 9547
    },
    {
        "loss": 2.4742,
        "grad_norm": 4.063655376434326,
        "learning_rate": 4.23979945200208e-06,
        "epoch": 1.2730666666666668,
        "step": 9548
    },
    {
        "loss": 1.5514,
        "grad_norm": 4.323744297027588,
        "learning_rate": 4.221686692392757e-06,
        "epoch": 1.2732,
        "step": 9549
    },
    {
        "loss": 2.1521,
        "grad_norm": 2.7591300010681152,
        "learning_rate": 4.203611870899693e-06,
        "epoch": 1.2733333333333334,
        "step": 9550
    },
    {
        "loss": 0.829,
        "grad_norm": 5.551082134246826,
        "learning_rate": 4.185574994682384e-06,
        "epoch": 1.2734666666666667,
        "step": 9551
    },
    {
        "loss": 1.8059,
        "grad_norm": 3.257544994354248,
        "learning_rate": 4.167576070885337e-06,
        "epoch": 1.2736,
        "step": 9552
    },
    {
        "loss": 2.3353,
        "grad_norm": 2.8864729404449463,
        "learning_rate": 4.149615106637883e-06,
        "epoch": 1.2737333333333334,
        "step": 9553
    },
    {
        "loss": 0.8138,
        "grad_norm": 2.900132417678833,
        "learning_rate": 4.13169210905453e-06,
        "epoch": 1.2738666666666667,
        "step": 9554
    },
    {
        "loss": 1.8727,
        "grad_norm": 4.054073333740234,
        "learning_rate": 4.113807085234567e-06,
        "epoch": 1.274,
        "step": 9555
    },
    {
        "loss": 1.5874,
        "grad_norm": 2.2101292610168457,
        "learning_rate": 4.095960042262392e-06,
        "epoch": 1.2741333333333333,
        "step": 9556
    },
    {
        "loss": 1.607,
        "grad_norm": 4.153722763061523,
        "learning_rate": 4.0781509872071855e-06,
        "epoch": 1.2742666666666667,
        "step": 9557
    },
    {
        "loss": 0.6541,
        "grad_norm": 2.83674955368042,
        "learning_rate": 4.060379927123225e-06,
        "epoch": 1.2744,
        "step": 9558
    },
    {
        "loss": 0.881,
        "grad_norm": 3.6627466678619385,
        "learning_rate": 4.0426468690496824e-06,
        "epoch": 1.2745333333333333,
        "step": 9559
    },
    {
        "loss": 2.5433,
        "grad_norm": 3.604548931121826,
        "learning_rate": 4.024951820010714e-06,
        "epoch": 1.2746666666666666,
        "step": 9560
    },
    {
        "loss": 0.7879,
        "grad_norm": 4.09249210357666,
        "learning_rate": 4.007294787015337e-06,
        "epoch": 1.2748,
        "step": 9561
    },
    {
        "loss": 2.3969,
        "grad_norm": 2.055013418197632,
        "learning_rate": 3.989675777057545e-06,
        "epoch": 1.2749333333333333,
        "step": 9562
    },
    {
        "loss": 2.5945,
        "grad_norm": 2.6663129329681396,
        "learning_rate": 3.9720947971163765e-06,
        "epoch": 1.2750666666666666,
        "step": 9563
    },
    {
        "loss": 2.2409,
        "grad_norm": 3.2015233039855957,
        "learning_rate": 3.954551854155619e-06,
        "epoch": 1.2752,
        "step": 9564
    },
    {
        "loss": 2.056,
        "grad_norm": 2.6796765327453613,
        "learning_rate": 3.937046955124191e-06,
        "epoch": 1.2753333333333332,
        "step": 9565
    },
    {
        "loss": 1.2956,
        "grad_norm": 4.340332984924316,
        "learning_rate": 3.919580106955756e-06,
        "epoch": 1.2754666666666667,
        "step": 9566
    },
    {
        "loss": 1.627,
        "grad_norm": 4.050205707550049,
        "learning_rate": 3.9021513165690356e-06,
        "epoch": 1.2756,
        "step": 9567
    },
    {
        "loss": 2.4263,
        "grad_norm": 4.575494289398193,
        "learning_rate": 3.884760590867609e-06,
        "epoch": 1.2757333333333334,
        "step": 9568
    },
    {
        "loss": 1.8254,
        "grad_norm": 3.716266393661499,
        "learning_rate": 3.867407936740042e-06,
        "epoch": 1.2758666666666667,
        "step": 9569
    },
    {
        "loss": 1.815,
        "grad_norm": 3.2408077716827393,
        "learning_rate": 3.850093361059759e-06,
        "epoch": 1.276,
        "step": 9570
    },
    {
        "loss": 1.422,
        "grad_norm": 2.993974447250366,
        "learning_rate": 3.832816870685074e-06,
        "epoch": 1.2761333333333333,
        "step": 9571
    },
    {
        "loss": 2.0922,
        "grad_norm": 2.212249279022217,
        "learning_rate": 3.815578472459358e-06,
        "epoch": 1.2762666666666667,
        "step": 9572
    },
    {
        "loss": 1.18,
        "grad_norm": 3.2736709117889404,
        "learning_rate": 3.798378173210737e-06,
        "epoch": 1.2764,
        "step": 9573
    },
    {
        "loss": 2.3126,
        "grad_norm": 2.489802837371826,
        "learning_rate": 3.7812159797523062e-06,
        "epoch": 1.2765333333333333,
        "step": 9574
    },
    {
        "loss": 2.3919,
        "grad_norm": 3.4399020671844482,
        "learning_rate": 3.764091898882083e-06,
        "epoch": 1.2766666666666666,
        "step": 9575
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.30007266998291,
        "learning_rate": 3.7470059373829746e-06,
        "epoch": 1.2768,
        "step": 9576
    },
    {
        "loss": 2.4326,
        "grad_norm": 4.840537071228027,
        "learning_rate": 3.729958102022835e-06,
        "epoch": 1.2769333333333333,
        "step": 9577
    },
    {
        "loss": 1.3939,
        "grad_norm": 4.066471576690674,
        "learning_rate": 3.71294839955425e-06,
        "epoch": 1.2770666666666668,
        "step": 9578
    },
    {
        "loss": 2.7261,
        "grad_norm": 4.127338886260986,
        "learning_rate": 3.6959768367149315e-06,
        "epoch": 1.2772000000000001,
        "step": 9579
    },
    {
        "loss": 1.8682,
        "grad_norm": 2.424082040786743,
        "learning_rate": 3.679043420227324e-06,
        "epoch": 1.2773333333333334,
        "step": 9580
    },
    {
        "loss": 2.2924,
        "grad_norm": 3.89428973197937,
        "learning_rate": 3.662148156798861e-06,
        "epoch": 1.2774666666666668,
        "step": 9581
    },
    {
        "loss": 0.8669,
        "grad_norm": 4.839283466339111,
        "learning_rate": 3.6452910531217353e-06,
        "epoch": 1.2776,
        "step": 9582
    },
    {
        "loss": 2.4364,
        "grad_norm": 2.916916847229004,
        "learning_rate": 3.6284721158731383e-06,
        "epoch": 1.2777333333333334,
        "step": 9583
    },
    {
        "loss": 1.9589,
        "grad_norm": 3.7238550186157227,
        "learning_rate": 3.6116913517151297e-06,
        "epoch": 1.2778666666666667,
        "step": 9584
    },
    {
        "loss": 1.5485,
        "grad_norm": 3.1877782344818115,
        "learning_rate": 3.5949487672946146e-06,
        "epoch": 1.278,
        "step": 9585
    },
    {
        "loss": 2.1872,
        "grad_norm": 3.7114923000335693,
        "learning_rate": 3.578244369243411e-06,
        "epoch": 1.2781333333333333,
        "step": 9586
    },
    {
        "loss": 1.9801,
        "grad_norm": 2.9386866092681885,
        "learning_rate": 3.5615781641781034e-06,
        "epoch": 1.2782666666666667,
        "step": 9587
    },
    {
        "loss": 1.4645,
        "grad_norm": 3.902895927429199,
        "learning_rate": 3.5449501587003444e-06,
        "epoch": 1.2784,
        "step": 9588
    },
    {
        "loss": 1.5503,
        "grad_norm": 3.693969488143921,
        "learning_rate": 3.528360359396443e-06,
        "epoch": 1.2785333333333333,
        "step": 9589
    },
    {
        "loss": 1.3392,
        "grad_norm": 2.9053263664245605,
        "learning_rate": 3.511808772837777e-06,
        "epoch": 1.2786666666666666,
        "step": 9590
    },
    {
        "loss": 1.5989,
        "grad_norm": 4.069253444671631,
        "learning_rate": 3.49529540558039e-06,
        "epoch": 1.2788,
        "step": 9591
    },
    {
        "loss": 2.0959,
        "grad_norm": 3.77919864654541,
        "learning_rate": 3.478820264165339e-06,
        "epoch": 1.2789333333333333,
        "step": 9592
    },
    {
        "loss": 2.3019,
        "grad_norm": 2.449732780456543,
        "learning_rate": 3.462383355118448e-06,
        "epoch": 1.2790666666666666,
        "step": 9593
    },
    {
        "loss": 2.3838,
        "grad_norm": 2.8012030124664307,
        "learning_rate": 3.4459846849504875e-06,
        "epoch": 1.2792,
        "step": 9594
    },
    {
        "loss": 1.7564,
        "grad_norm": 4.597896099090576,
        "learning_rate": 3.429624260156983e-06,
        "epoch": 1.2793333333333332,
        "step": 9595
    },
    {
        "loss": 1.7458,
        "grad_norm": 4.883760452270508,
        "learning_rate": 3.4133020872183397e-06,
        "epoch": 1.2794666666666665,
        "step": 9596
    },
    {
        "loss": 1.6946,
        "grad_norm": 4.450483798980713,
        "learning_rate": 3.397018172599886e-06,
        "epoch": 1.2796,
        "step": 9597
    },
    {
        "loss": 2.0791,
        "grad_norm": 3.11411714553833,
        "learning_rate": 3.3807725227516964e-06,
        "epoch": 1.2797333333333334,
        "step": 9598
    },
    {
        "loss": 1.426,
        "grad_norm": 5.686974048614502,
        "learning_rate": 3.3645651441087113e-06,
        "epoch": 1.2798666666666667,
        "step": 9599
    },
    {
        "loss": 2.1971,
        "grad_norm": 3.7260894775390625,
        "learning_rate": 3.3483960430907513e-06,
        "epoch": 1.28,
        "step": 9600
    },
    {
        "loss": 1.627,
        "grad_norm": 2.8221988677978516,
        "learning_rate": 3.33226522610246e-06,
        "epoch": 1.2801333333333333,
        "step": 9601
    },
    {
        "loss": 2.49,
        "grad_norm": 3.2342731952667236,
        "learning_rate": 3.3161726995333265e-06,
        "epoch": 1.2802666666666667,
        "step": 9602
    },
    {
        "loss": 1.8344,
        "grad_norm": 3.7137036323547363,
        "learning_rate": 3.300118469757574e-06,
        "epoch": 1.2804,
        "step": 9603
    },
    {
        "loss": 2.0767,
        "grad_norm": 3.185307025909424,
        "learning_rate": 3.2841025431344263e-06,
        "epoch": 1.2805333333333333,
        "step": 9604
    },
    {
        "loss": 1.0477,
        "grad_norm": 6.288714408874512,
        "learning_rate": 3.2681249260077873e-06,
        "epoch": 1.2806666666666666,
        "step": 9605
    },
    {
        "loss": 0.6263,
        "grad_norm": 3.15177583694458,
        "learning_rate": 3.2521856247065054e-06,
        "epoch": 1.2808,
        "step": 9606
    },
    {
        "loss": 1.5803,
        "grad_norm": 4.523971080780029,
        "learning_rate": 3.236284645544152e-06,
        "epoch": 1.2809333333333333,
        "step": 9607
    },
    {
        "loss": 1.8062,
        "grad_norm": 3.2908177375793457,
        "learning_rate": 3.220421994819145e-06,
        "epoch": 1.2810666666666668,
        "step": 9608
    },
    {
        "loss": 1.9584,
        "grad_norm": 5.793513774871826,
        "learning_rate": 3.204597678814758e-06,
        "epoch": 1.2812000000000001,
        "step": 9609
    },
    {
        "loss": 2.3229,
        "grad_norm": 4.176908493041992,
        "learning_rate": 3.188811703799055e-06,
        "epoch": 1.2813333333333334,
        "step": 9610
    },
    {
        "loss": 2.3265,
        "grad_norm": 3.592592716217041,
        "learning_rate": 3.1730640760249454e-06,
        "epoch": 1.2814666666666668,
        "step": 9611
    },
    {
        "loss": 2.0591,
        "grad_norm": 3.874337911605835,
        "learning_rate": 3.157354801730028e-06,
        "epoch": 1.2816,
        "step": 9612
    },
    {
        "loss": 1.9647,
        "grad_norm": 3.4315378665924072,
        "learning_rate": 3.141683887136904e-06,
        "epoch": 1.2817333333333334,
        "step": 9613
    },
    {
        "loss": 2.8815,
        "grad_norm": 3.2977077960968018,
        "learning_rate": 3.1260513384527733e-06,
        "epoch": 1.2818666666666667,
        "step": 9614
    },
    {
        "loss": 1.5121,
        "grad_norm": 4.631007671356201,
        "learning_rate": 3.110457161869862e-06,
        "epoch": 1.282,
        "step": 9615
    },
    {
        "loss": 1.5447,
        "grad_norm": 2.183699607849121,
        "learning_rate": 3.094901363564984e-06,
        "epoch": 1.2821333333333333,
        "step": 9616
    },
    {
        "loss": 1.9546,
        "grad_norm": 2.916729688644409,
        "learning_rate": 3.0793839496998654e-06,
        "epoch": 1.2822666666666667,
        "step": 9617
    },
    {
        "loss": 1.278,
        "grad_norm": 5.2695536613464355,
        "learning_rate": 3.063904926421002e-06,
        "epoch": 1.2824,
        "step": 9618
    },
    {
        "loss": 2.2927,
        "grad_norm": 2.7633156776428223,
        "learning_rate": 3.0484642998597324e-06,
        "epoch": 1.2825333333333333,
        "step": 9619
    },
    {
        "loss": 2.1791,
        "grad_norm": 3.772282600402832,
        "learning_rate": 3.033062076132098e-06,
        "epoch": 1.2826666666666666,
        "step": 9620
    },
    {
        "loss": 1.643,
        "grad_norm": 5.9250640869140625,
        "learning_rate": 3.017698261338964e-06,
        "epoch": 1.2828,
        "step": 9621
    },
    {
        "loss": 2.3504,
        "grad_norm": 3.6290109157562256,
        "learning_rate": 3.002372861566027e-06,
        "epoch": 1.2829333333333333,
        "step": 9622
    },
    {
        "loss": 2.3014,
        "grad_norm": 2.9869649410247803,
        "learning_rate": 2.9870858828836777e-06,
        "epoch": 1.2830666666666666,
        "step": 9623
    },
    {
        "loss": 2.7671,
        "grad_norm": 3.2677359580993652,
        "learning_rate": 2.971837331347227e-06,
        "epoch": 1.2832,
        "step": 9624
    },
    {
        "loss": 1.9007,
        "grad_norm": 2.800929069519043,
        "learning_rate": 2.956627212996588e-06,
        "epoch": 1.2833333333333332,
        "step": 9625
    },
    {
        "loss": 0.7321,
        "grad_norm": 4.323947906494141,
        "learning_rate": 2.9414555338565854e-06,
        "epoch": 1.2834666666666665,
        "step": 9626
    },
    {
        "loss": 1.7214,
        "grad_norm": 3.5948445796966553,
        "learning_rate": 2.926322299936757e-06,
        "epoch": 1.2836,
        "step": 9627
    },
    {
        "loss": 0.7194,
        "grad_norm": 3.7892699241638184,
        "learning_rate": 2.9112275172314517e-06,
        "epoch": 1.2837333333333334,
        "step": 9628
    },
    {
        "loss": 1.9019,
        "grad_norm": 4.078481197357178,
        "learning_rate": 2.896171191719754e-06,
        "epoch": 1.2838666666666667,
        "step": 9629
    },
    {
        "loss": 1.7364,
        "grad_norm": 2.6056907176971436,
        "learning_rate": 2.8811533293654825e-06,
        "epoch": 1.284,
        "step": 9630
    },
    {
        "loss": 2.3557,
        "grad_norm": 2.511209487915039,
        "learning_rate": 2.8661739361173336e-06,
        "epoch": 1.2841333333333333,
        "step": 9631
    },
    {
        "loss": 1.5945,
        "grad_norm": 2.518686532974243,
        "learning_rate": 2.851233017908672e-06,
        "epoch": 1.2842666666666667,
        "step": 9632
    },
    {
        "loss": 2.4614,
        "grad_norm": 3.851754665374756,
        "learning_rate": 2.836330580657609e-06,
        "epoch": 1.2844,
        "step": 9633
    },
    {
        "loss": 2.4361,
        "grad_norm": 2.9952518939971924,
        "learning_rate": 2.821466630267089e-06,
        "epoch": 1.2845333333333333,
        "step": 9634
    },
    {
        "loss": 1.9767,
        "grad_norm": 3.129229784011841,
        "learning_rate": 2.8066411726247688e-06,
        "epoch": 1.2846666666666666,
        "step": 9635
    },
    {
        "loss": 1.5164,
        "grad_norm": 3.4384422302246094,
        "learning_rate": 2.7918542136030846e-06,
        "epoch": 1.2848,
        "step": 9636
    },
    {
        "loss": 1.7213,
        "grad_norm": 3.3404650688171387,
        "learning_rate": 2.7771057590591287e-06,
        "epoch": 1.2849333333333333,
        "step": 9637
    },
    {
        "loss": 1.8424,
        "grad_norm": 3.0862650871276855,
        "learning_rate": 2.762395814834895e-06,
        "epoch": 1.2850666666666668,
        "step": 9638
    },
    {
        "loss": 2.137,
        "grad_norm": 2.900648832321167,
        "learning_rate": 2.7477243867569668e-06,
        "epoch": 1.2852000000000001,
        "step": 9639
    },
    {
        "loss": 1.8418,
        "grad_norm": 4.059480667114258,
        "learning_rate": 2.73309148063684e-06,
        "epoch": 1.2853333333333334,
        "step": 9640
    },
    {
        "loss": 2.1943,
        "grad_norm": 4.155796527862549,
        "learning_rate": 2.7184971022705785e-06,
        "epoch": 1.2854666666666668,
        "step": 9641
    },
    {
        "loss": 1.8162,
        "grad_norm": 3.2714550495147705,
        "learning_rate": 2.703941257439102e-06,
        "epoch": 1.2856,
        "step": 9642
    },
    {
        "loss": 2.4808,
        "grad_norm": 2.5918397903442383,
        "learning_rate": 2.6894239519079988e-06,
        "epoch": 1.2857333333333334,
        "step": 9643
    },
    {
        "loss": 1.9809,
        "grad_norm": 3.5150833129882812,
        "learning_rate": 2.674945191427658e-06,
        "epoch": 1.2858666666666667,
        "step": 9644
    },
    {
        "loss": 2.0371,
        "grad_norm": 3.852971315383911,
        "learning_rate": 2.6605049817331917e-06,
        "epoch": 1.286,
        "step": 9645
    },
    {
        "loss": 2.2572,
        "grad_norm": 4.19414758682251,
        "learning_rate": 2.6461033285443247e-06,
        "epoch": 1.2861333333333334,
        "step": 9646
    },
    {
        "loss": 2.3231,
        "grad_norm": 2.272718906402588,
        "learning_rate": 2.631740237565683e-06,
        "epoch": 1.2862666666666667,
        "step": 9647
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.8171944618225098,
        "learning_rate": 2.617415714486482e-06,
        "epoch": 1.2864,
        "step": 9648
    },
    {
        "loss": 1.894,
        "grad_norm": 4.947988510131836,
        "learning_rate": 2.603129764980783e-06,
        "epoch": 1.2865333333333333,
        "step": 9649
    },
    {
        "loss": 1.82,
        "grad_norm": 3.2320706844329834,
        "learning_rate": 2.588882394707226e-06,
        "epoch": 1.2866666666666666,
        "step": 9650
    },
    {
        "loss": 1.8873,
        "grad_norm": 3.1495683193206787,
        "learning_rate": 2.574673609309275e-06,
        "epoch": 1.2868,
        "step": 9651
    },
    {
        "loss": 1.6978,
        "grad_norm": 6.135366439819336,
        "learning_rate": 2.5605034144150717e-06,
        "epoch": 1.2869333333333333,
        "step": 9652
    },
    {
        "loss": 1.7316,
        "grad_norm": 2.5440876483917236,
        "learning_rate": 2.5463718156374937e-06,
        "epoch": 1.2870666666666666,
        "step": 9653
    },
    {
        "loss": 2.0452,
        "grad_norm": 2.690322160720825,
        "learning_rate": 2.532278818574119e-06,
        "epoch": 1.2872,
        "step": 9654
    },
    {
        "loss": 2.3141,
        "grad_norm": 4.42015266418457,
        "learning_rate": 2.518224428807181e-06,
        "epoch": 1.2873333333333332,
        "step": 9655
    },
    {
        "loss": 2.4898,
        "grad_norm": 3.587587594985962,
        "learning_rate": 2.5042086519037276e-06,
        "epoch": 1.2874666666666665,
        "step": 9656
    },
    {
        "loss": 1.9459,
        "grad_norm": 2.066349744796753,
        "learning_rate": 2.4902314934154516e-06,
        "epoch": 1.2876,
        "step": 9657
    },
    {
        "loss": 2.5934,
        "grad_norm": 2.392310619354248,
        "learning_rate": 2.476292958878701e-06,
        "epoch": 1.2877333333333334,
        "step": 9658
    },
    {
        "loss": 2.7147,
        "grad_norm": 2.124666452407837,
        "learning_rate": 2.4623930538146378e-06,
        "epoch": 1.2878666666666667,
        "step": 9659
    },
    {
        "loss": 2.0506,
        "grad_norm": 3.6234707832336426,
        "learning_rate": 2.4485317837290245e-06,
        "epoch": 1.288,
        "step": 9660
    },
    {
        "loss": 2.0041,
        "grad_norm": 3.551640033721924,
        "learning_rate": 2.4347091541124022e-06,
        "epoch": 1.2881333333333334,
        "step": 9661
    },
    {
        "loss": 2.2664,
        "grad_norm": 3.6212046146392822,
        "learning_rate": 2.4209251704398807e-06,
        "epoch": 1.2882666666666667,
        "step": 9662
    },
    {
        "loss": 1.4275,
        "grad_norm": 2.612086772918701,
        "learning_rate": 2.407179838171436e-06,
        "epoch": 1.2884,
        "step": 9663
    },
    {
        "loss": 2.418,
        "grad_norm": 3.5279700756073,
        "learning_rate": 2.3934731627515472e-06,
        "epoch": 1.2885333333333333,
        "step": 9664
    },
    {
        "loss": 2.1965,
        "grad_norm": 2.213545799255371,
        "learning_rate": 2.3798051496095596e-06,
        "epoch": 1.2886666666666666,
        "step": 9665
    },
    {
        "loss": 1.9268,
        "grad_norm": 3.2729432582855225,
        "learning_rate": 2.3661758041593983e-06,
        "epoch": 1.2888,
        "step": 9666
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.5718376636505127,
        "learning_rate": 2.352585131799656e-06,
        "epoch": 1.2889333333333333,
        "step": 9667
    },
    {
        "loss": 2.6269,
        "grad_norm": 2.663837432861328,
        "learning_rate": 2.339033137913671e-06,
        "epoch": 1.2890666666666668,
        "step": 9668
    },
    {
        "loss": 1.9612,
        "grad_norm": 3.0709609985351562,
        "learning_rate": 2.325519827869416e-06,
        "epoch": 1.2892000000000001,
        "step": 9669
    },
    {
        "loss": 1.4455,
        "grad_norm": 3.924060344696045,
        "learning_rate": 2.3120452070196197e-06,
        "epoch": 1.2893333333333334,
        "step": 9670
    },
    {
        "loss": 2.0142,
        "grad_norm": 4.284356594085693,
        "learning_rate": 2.2986092807015357e-06,
        "epoch": 1.2894666666666668,
        "step": 9671
    },
    {
        "loss": 2.2432,
        "grad_norm": 3.3677544593811035,
        "learning_rate": 2.2852120542372514e-06,
        "epoch": 1.2896,
        "step": 9672
    },
    {
        "loss": 1.4511,
        "grad_norm": 3.5975143909454346,
        "learning_rate": 2.271853532933399e-06,
        "epoch": 1.2897333333333334,
        "step": 9673
    },
    {
        "loss": 1.8555,
        "grad_norm": 5.3641791343688965,
        "learning_rate": 2.2585337220814128e-06,
        "epoch": 1.2898666666666667,
        "step": 9674
    },
    {
        "loss": 0.6971,
        "grad_norm": 5.5541157722473145,
        "learning_rate": 2.2452526269572283e-06,
        "epoch": 1.29,
        "step": 9675
    },
    {
        "loss": 1.9021,
        "grad_norm": 3.3618006706237793,
        "learning_rate": 2.23201025282157e-06,
        "epoch": 1.2901333333333334,
        "step": 9676
    },
    {
        "loss": 2.4301,
        "grad_norm": 3.3506412506103516,
        "learning_rate": 2.2188066049197876e-06,
        "epoch": 1.2902666666666667,
        "step": 9677
    },
    {
        "loss": 1.3848,
        "grad_norm": 4.727966785430908,
        "learning_rate": 2.2056416884819077e-06,
        "epoch": 1.2904,
        "step": 9678
    },
    {
        "loss": 1.634,
        "grad_norm": 4.558682918548584,
        "learning_rate": 2.1925155087225815e-06,
        "epoch": 1.2905333333333333,
        "step": 9679
    },
    {
        "loss": 1.157,
        "grad_norm": 2.7964093685150146,
        "learning_rate": 2.179428070841094e-06,
        "epoch": 1.2906666666666666,
        "step": 9680
    },
    {
        "loss": 1.7169,
        "grad_norm": 2.8195457458496094,
        "learning_rate": 2.1663793800214993e-06,
        "epoch": 1.2908,
        "step": 9681
    },
    {
        "loss": 2.4468,
        "grad_norm": 3.2094151973724365,
        "learning_rate": 2.153369441432396e-06,
        "epoch": 1.2909333333333333,
        "step": 9682
    },
    {
        "loss": 0.7876,
        "grad_norm": 3.4013288021087646,
        "learning_rate": 2.140398260227039e-06,
        "epoch": 1.2910666666666666,
        "step": 9683
    },
    {
        "loss": 2.0676,
        "grad_norm": 3.620960235595703,
        "learning_rate": 2.1274658415433746e-06,
        "epoch": 1.2912,
        "step": 9684
    },
    {
        "loss": 1.8447,
        "grad_norm": 3.251192331314087,
        "learning_rate": 2.114572190503994e-06,
        "epoch": 1.2913333333333332,
        "step": 9685
    },
    {
        "loss": 2.4254,
        "grad_norm": 2.7693843841552734,
        "learning_rate": 2.1017173122161114e-06,
        "epoch": 1.2914666666666665,
        "step": 9686
    },
    {
        "loss": 2.2826,
        "grad_norm": 3.659414529800415,
        "learning_rate": 2.0889012117715766e-06,
        "epoch": 1.2916,
        "step": 9687
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.633079767227173,
        "learning_rate": 2.0761238942469173e-06,
        "epoch": 1.2917333333333334,
        "step": 9688
    },
    {
        "loss": 0.7881,
        "grad_norm": 3.8757266998291016,
        "learning_rate": 2.0633853647032075e-06,
        "epoch": 1.2918666666666667,
        "step": 9689
    },
    {
        "loss": 2.022,
        "grad_norm": 3.167163610458374,
        "learning_rate": 2.0506856281862885e-06,
        "epoch": 1.292,
        "step": 9690
    },
    {
        "loss": 1.3667,
        "grad_norm": 4.308653354644775,
        "learning_rate": 2.03802468972657e-06,
        "epoch": 1.2921333333333334,
        "step": 9691
    },
    {
        "loss": 1.8364,
        "grad_norm": 3.702315330505371,
        "learning_rate": 2.0254025543390396e-06,
        "epoch": 1.2922666666666667,
        "step": 9692
    },
    {
        "loss": 2.7714,
        "grad_norm": 3.9790618419647217,
        "learning_rate": 2.0128192270233993e-06,
        "epoch": 1.2924,
        "step": 9693
    },
    {
        "loss": 1.0475,
        "grad_norm": 3.2729203701019287,
        "learning_rate": 2.000274712763939e-06,
        "epoch": 1.2925333333333333,
        "step": 9694
    },
    {
        "loss": 2.0144,
        "grad_norm": 3.6969969272613525,
        "learning_rate": 1.987769016529628e-06,
        "epoch": 1.2926666666666666,
        "step": 9695
    },
    {
        "loss": 1.6016,
        "grad_norm": 3.606898546218872,
        "learning_rate": 1.9753021432739383e-06,
        "epoch": 1.2928,
        "step": 9696
    },
    {
        "loss": 1.7979,
        "grad_norm": 3.041431188583374,
        "learning_rate": 1.9628740979350967e-06,
        "epoch": 1.2929333333333333,
        "step": 9697
    },
    {
        "loss": 1.9814,
        "grad_norm": 3.414518117904663,
        "learning_rate": 1.9504848854358547e-06,
        "epoch": 1.2930666666666666,
        "step": 9698
    },
    {
        "loss": 2.48,
        "grad_norm": 2.9784820079803467,
        "learning_rate": 1.9381345106836866e-06,
        "epoch": 1.2932000000000001,
        "step": 9699
    },
    {
        "loss": 2.256,
        "grad_norm": 3.9058799743652344,
        "learning_rate": 1.925822978570557e-06,
        "epoch": 1.2933333333333334,
        "step": 9700
    },
    {
        "loss": 1.4911,
        "grad_norm": 3.193239688873291,
        "learning_rate": 1.9135502939731208e-06,
        "epoch": 1.2934666666666668,
        "step": 9701
    },
    {
        "loss": 1.9699,
        "grad_norm": 3.8670241832733154,
        "learning_rate": 1.9013164617526335e-06,
        "epoch": 1.2936,
        "step": 9702
    },
    {
        "loss": 1.8917,
        "grad_norm": 2.239227056503296,
        "learning_rate": 1.8891214867549745e-06,
        "epoch": 1.2937333333333334,
        "step": 9703
    },
    {
        "loss": 1.4333,
        "grad_norm": 4.31625509262085,
        "learning_rate": 1.876965373810602e-06,
        "epoch": 1.2938666666666667,
        "step": 9704
    },
    {
        "loss": 1.7133,
        "grad_norm": 3.3128533363342285,
        "learning_rate": 1.864848127734553e-06,
        "epoch": 1.294,
        "step": 9705
    },
    {
        "loss": 1.9226,
        "grad_norm": 5.2146782875061035,
        "learning_rate": 1.8527697533265775e-06,
        "epoch": 1.2941333333333334,
        "step": 9706
    },
    {
        "loss": 1.7617,
        "grad_norm": 2.760669469833374,
        "learning_rate": 1.8407302553709148e-06,
        "epoch": 1.2942666666666667,
        "step": 9707
    },
    {
        "loss": 1.7404,
        "grad_norm": 2.438863754272461,
        "learning_rate": 1.828729638636506e-06,
        "epoch": 1.2944,
        "step": 9708
    },
    {
        "loss": 2.2815,
        "grad_norm": 1.8388885259628296,
        "learning_rate": 1.8167679078767486e-06,
        "epoch": 1.2945333333333333,
        "step": 9709
    },
    {
        "loss": 0.8279,
        "grad_norm": 3.6743457317352295,
        "learning_rate": 1.8048450678297968e-06,
        "epoch": 1.2946666666666666,
        "step": 9710
    },
    {
        "loss": 1.9254,
        "grad_norm": 4.164300441741943,
        "learning_rate": 1.7929611232183064e-06,
        "epoch": 1.2948,
        "step": 9711
    },
    {
        "loss": 1.7199,
        "grad_norm": 5.257482528686523,
        "learning_rate": 1.781116078749545e-06,
        "epoch": 1.2949333333333333,
        "step": 9712
    },
    {
        "loss": 2.1819,
        "grad_norm": 3.743635416030884,
        "learning_rate": 1.7693099391153933e-06,
        "epoch": 1.2950666666666666,
        "step": 9713
    },
    {
        "loss": 1.5762,
        "grad_norm": 3.4715845584869385,
        "learning_rate": 1.7575427089922547e-06,
        "epoch": 1.2952,
        "step": 9714
    },
    {
        "loss": 2.5667,
        "grad_norm": 3.037155866622925,
        "learning_rate": 1.745814393041234e-06,
        "epoch": 1.2953333333333332,
        "step": 9715
    },
    {
        "loss": 1.0476,
        "grad_norm": 3.6498517990112305,
        "learning_rate": 1.7341249959079153e-06,
        "epoch": 1.2954666666666665,
        "step": 9716
    },
    {
        "loss": 1.8834,
        "grad_norm": 3.267056465148926,
        "learning_rate": 1.7224745222225057e-06,
        "epoch": 1.2955999999999999,
        "step": 9717
    },
    {
        "loss": 1.6127,
        "grad_norm": 3.8929619789123535,
        "learning_rate": 1.710862976599814e-06,
        "epoch": 1.2957333333333334,
        "step": 9718
    },
    {
        "loss": 1.491,
        "grad_norm": 3.6724374294281006,
        "learning_rate": 1.6992903636392054e-06,
        "epoch": 1.2958666666666667,
        "step": 9719
    },
    {
        "loss": 2.7554,
        "grad_norm": 3.077803373336792,
        "learning_rate": 1.6877566879246687e-06,
        "epoch": 1.296,
        "step": 9720
    },
    {
        "loss": 2.4115,
        "grad_norm": 2.627667188644409,
        "learning_rate": 1.6762619540246605e-06,
        "epoch": 1.2961333333333334,
        "step": 9721
    },
    {
        "loss": 2.322,
        "grad_norm": 3.178647518157959,
        "learning_rate": 1.6648061664923386e-06,
        "epoch": 1.2962666666666667,
        "step": 9722
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.7396128177642822,
        "learning_rate": 1.6533893298653401e-06,
        "epoch": 1.2964,
        "step": 9723
    },
    {
        "loss": 1.1846,
        "grad_norm": 5.922867298126221,
        "learning_rate": 1.6420114486659698e-06,
        "epoch": 1.2965333333333333,
        "step": 9724
    },
    {
        "loss": 2.4401,
        "grad_norm": 3.279733896255493,
        "learning_rate": 1.6306725274010003e-06,
        "epoch": 1.2966666666666666,
        "step": 9725
    },
    {
        "loss": 0.9333,
        "grad_norm": 3.311570882797241,
        "learning_rate": 1.6193725705618168e-06,
        "epoch": 1.2968,
        "step": 9726
    },
    {
        "loss": 2.079,
        "grad_norm": 3.632567882537842,
        "learning_rate": 1.6081115826243941e-06,
        "epoch": 1.2969333333333333,
        "step": 9727
    },
    {
        "loss": 1.814,
        "grad_norm": 1.3118568658828735,
        "learning_rate": 1.5968895680492202e-06,
        "epoch": 1.2970666666666666,
        "step": 9728
    },
    {
        "loss": 2.1297,
        "grad_norm": 4.016841411590576,
        "learning_rate": 1.585706531281428e-06,
        "epoch": 1.2972000000000001,
        "step": 9729
    },
    {
        "loss": 2.3007,
        "grad_norm": 3.213771343231201,
        "learning_rate": 1.5745624767505629e-06,
        "epoch": 1.2973333333333334,
        "step": 9730
    },
    {
        "loss": 2.1892,
        "grad_norm": 4.009477138519287,
        "learning_rate": 1.5634574088709163e-06,
        "epoch": 1.2974666666666668,
        "step": 9731
    },
    {
        "loss": 1.4366,
        "grad_norm": 3.5584352016448975,
        "learning_rate": 1.552391332041181e-06,
        "epoch": 1.2976,
        "step": 9732
    },
    {
        "loss": 2.3452,
        "grad_norm": 4.457314491271973,
        "learning_rate": 1.541364250644717e-06,
        "epoch": 1.2977333333333334,
        "step": 9733
    },
    {
        "loss": 2.2717,
        "grad_norm": 4.310549736022949,
        "learning_rate": 1.5303761690493412e-06,
        "epoch": 1.2978666666666667,
        "step": 9734
    },
    {
        "loss": 0.688,
        "grad_norm": 3.8729348182678223,
        "learning_rate": 1.519427091607495e-06,
        "epoch": 1.298,
        "step": 9735
    },
    {
        "loss": 1.5901,
        "grad_norm": 3.715111017227173,
        "learning_rate": 1.5085170226561417e-06,
        "epoch": 1.2981333333333334,
        "step": 9736
    },
    {
        "loss": 2.5495,
        "grad_norm": 2.4589970111846924,
        "learning_rate": 1.4976459665168141e-06,
        "epoch": 1.2982666666666667,
        "step": 9737
    },
    {
        "loss": 2.3311,
        "grad_norm": 3.725313425064087,
        "learning_rate": 1.486813927495545e-06,
        "epoch": 1.2984,
        "step": 9738
    },
    {
        "loss": 2.5223,
        "grad_norm": 2.929136276245117,
        "learning_rate": 1.4760209098829358e-06,
        "epoch": 1.2985333333333333,
        "step": 9739
    },
    {
        "loss": 1.9204,
        "grad_norm": 2.4675333499908447,
        "learning_rate": 1.4652669179541889e-06,
        "epoch": 1.2986666666666666,
        "step": 9740
    },
    {
        "loss": 2.5138,
        "grad_norm": 2.9203217029571533,
        "learning_rate": 1.4545519559689524e-06,
        "epoch": 1.2988,
        "step": 9741
    },
    {
        "loss": 1.1651,
        "grad_norm": 3.450787305831909,
        "learning_rate": 1.4438760281714648e-06,
        "epoch": 1.2989333333333333,
        "step": 9742
    },
    {
        "loss": 1.8379,
        "grad_norm": 4.061018943786621,
        "learning_rate": 1.4332391387905098e-06,
        "epoch": 1.2990666666666666,
        "step": 9743
    },
    {
        "loss": 1.515,
        "grad_norm": 4.834847450256348,
        "learning_rate": 1.4226412920393729e-06,
        "epoch": 1.2992,
        "step": 9744
    },
    {
        "loss": 1.4743,
        "grad_norm": 4.3456950187683105,
        "learning_rate": 1.4120824921159292e-06,
        "epoch": 1.2993333333333332,
        "step": 9745
    },
    {
        "loss": 2.3612,
        "grad_norm": 3.7275466918945312,
        "learning_rate": 1.4015627432025558e-06,
        "epoch": 1.2994666666666665,
        "step": 9746
    },
    {
        "loss": 2.4354,
        "grad_norm": 3.1031901836395264,
        "learning_rate": 1.3910820494661414e-06,
        "epoch": 1.2995999999999999,
        "step": 9747
    },
    {
        "loss": 2.4319,
        "grad_norm": 2.0276665687561035,
        "learning_rate": 1.3806404150580988e-06,
        "epoch": 1.2997333333333334,
        "step": 9748
    },
    {
        "loss": 1.8115,
        "grad_norm": 2.3504860401153564,
        "learning_rate": 1.3702378441144524e-06,
        "epoch": 1.2998666666666667,
        "step": 9749
    },
    {
        "loss": 2.3742,
        "grad_norm": 3.050326108932495,
        "learning_rate": 1.3598743407556736e-06,
        "epoch": 1.3,
        "step": 9750
    },
    {
        "loss": 0.9277,
        "grad_norm": 3.7669742107391357,
        "learning_rate": 1.349549909086756e-06,
        "epoch": 1.3001333333333334,
        "step": 9751
    },
    {
        "loss": 1.8481,
        "grad_norm": 3.418731689453125,
        "learning_rate": 1.3392645531972726e-06,
        "epoch": 1.3002666666666667,
        "step": 9752
    },
    {
        "loss": 1.7699,
        "grad_norm": 5.012729644775391,
        "learning_rate": 1.3290182771612757e-06,
        "epoch": 1.3004,
        "step": 9753
    },
    {
        "loss": 1.9555,
        "grad_norm": 2.8010036945343018,
        "learning_rate": 1.3188110850373635e-06,
        "epoch": 1.3005333333333333,
        "step": 9754
    },
    {
        "loss": 0.9402,
        "grad_norm": 3.7979660034179688,
        "learning_rate": 1.308642980868602e-06,
        "epoch": 1.3006666666666666,
        "step": 9755
    },
    {
        "loss": 2.6684,
        "grad_norm": 2.476450204849243,
        "learning_rate": 1.2985139686826575e-06,
        "epoch": 1.3008,
        "step": 9756
    },
    {
        "loss": 2.1213,
        "grad_norm": 3.0021142959594727,
        "learning_rate": 1.2884240524916323e-06,
        "epoch": 1.3009333333333333,
        "step": 9757
    },
    {
        "loss": 2.0136,
        "grad_norm": 3.3196263313293457,
        "learning_rate": 1.2783732362922296e-06,
        "epoch": 1.3010666666666666,
        "step": 9758
    },
    {
        "loss": 2.0801,
        "grad_norm": 4.443370342254639,
        "learning_rate": 1.2683615240655311e-06,
        "epoch": 1.3012000000000001,
        "step": 9759
    },
    {
        "loss": 2.0178,
        "grad_norm": 3.618760824203491,
        "learning_rate": 1.2583889197772648e-06,
        "epoch": 1.3013333333333335,
        "step": 9760
    },
    {
        "loss": 1.9468,
        "grad_norm": 3.317019462585449,
        "learning_rate": 1.2484554273776039e-06,
        "epoch": 1.3014666666666668,
        "step": 9761
    },
    {
        "loss": 2.097,
        "grad_norm": 3.8166708946228027,
        "learning_rate": 1.2385610508012458e-06,
        "epoch": 1.3016,
        "step": 9762
    },
    {
        "loss": 2.0613,
        "grad_norm": 3.8669002056121826,
        "learning_rate": 1.2287057939673774e-06,
        "epoch": 1.3017333333333334,
        "step": 9763
    },
    {
        "loss": 0.7499,
        "grad_norm": 2.2484960556030273,
        "learning_rate": 1.2188896607796763e-06,
        "epoch": 1.3018666666666667,
        "step": 9764
    },
    {
        "loss": 0.9692,
        "grad_norm": 3.7013280391693115,
        "learning_rate": 1.2091126551263875e-06,
        "epoch": 1.302,
        "step": 9765
    },
    {
        "loss": 2.3143,
        "grad_norm": 4.2335004806518555,
        "learning_rate": 1.1993747808801802e-06,
        "epoch": 1.3021333333333334,
        "step": 9766
    },
    {
        "loss": 2.8899,
        "grad_norm": 3.7882072925567627,
        "learning_rate": 1.1896760418983022e-06,
        "epoch": 1.3022666666666667,
        "step": 9767
    },
    {
        "loss": 2.0902,
        "grad_norm": 3.3910748958587646,
        "learning_rate": 1.1800164420224136e-06,
        "epoch": 1.3024,
        "step": 9768
    },
    {
        "loss": 2.3332,
        "grad_norm": 2.1726675033569336,
        "learning_rate": 1.1703959850787427e-06,
        "epoch": 1.3025333333333333,
        "step": 9769
    },
    {
        "loss": 2.212,
        "grad_norm": 2.9554693698883057,
        "learning_rate": 1.1608146748779637e-06,
        "epoch": 1.3026666666666666,
        "step": 9770
    },
    {
        "loss": 1.7391,
        "grad_norm": 3.8183131217956543,
        "learning_rate": 1.151272515215296e-06,
        "epoch": 1.3028,
        "step": 9771
    },
    {
        "loss": 1.6912,
        "grad_norm": 3.5827479362487793,
        "learning_rate": 1.141769509870394e-06,
        "epoch": 1.3029333333333333,
        "step": 9772
    },
    {
        "loss": 2.6469,
        "grad_norm": 3.6453473567962646,
        "learning_rate": 1.1323056626074246e-06,
        "epoch": 1.3030666666666666,
        "step": 9773
    },
    {
        "loss": 1.1621,
        "grad_norm": 2.867150068283081,
        "learning_rate": 1.1228809771750891e-06,
        "epoch": 1.3032,
        "step": 9774
    },
    {
        "loss": 1.812,
        "grad_norm": 3.376469612121582,
        "learning_rate": 1.1134954573065126e-06,
        "epoch": 1.3033333333333332,
        "step": 9775
    },
    {
        "loss": 2.2017,
        "grad_norm": 3.498358726501465,
        "learning_rate": 1.1041491067193211e-06,
        "epoch": 1.3034666666666666,
        "step": 9776
    },
    {
        "loss": 1.6031,
        "grad_norm": 3.994019031524658,
        "learning_rate": 1.094841929115653e-06,
        "epoch": 1.3035999999999999,
        "step": 9777
    },
    {
        "loss": 2.3027,
        "grad_norm": 2.5047171115875244,
        "learning_rate": 1.0855739281821041e-06,
        "epoch": 1.3037333333333334,
        "step": 9778
    },
    {
        "loss": 2.3433,
        "grad_norm": 1.9870332479476929,
        "learning_rate": 1.0763451075897935e-06,
        "epoch": 1.3038666666666667,
        "step": 9779
    },
    {
        "loss": 1.0935,
        "grad_norm": 5.820231914520264,
        "learning_rate": 1.0671554709942189e-06,
        "epoch": 1.304,
        "step": 9780
    },
    {
        "loss": 2.1446,
        "grad_norm": 3.4055159091949463,
        "learning_rate": 1.0580050220354798e-06,
        "epoch": 1.3041333333333334,
        "step": 9781
    },
    {
        "loss": 2.4819,
        "grad_norm": 3.0663187503814697,
        "learning_rate": 1.048893764338088e-06,
        "epoch": 1.3042666666666667,
        "step": 9782
    },
    {
        "loss": 1.0046,
        "grad_norm": 5.116135120391846,
        "learning_rate": 1.0398217015110677e-06,
        "epoch": 1.3044,
        "step": 9783
    },
    {
        "loss": 1.6042,
        "grad_norm": 5.179095268249512,
        "learning_rate": 1.0307888371478447e-06,
        "epoch": 1.3045333333333333,
        "step": 9784
    },
    {
        "loss": 1.9237,
        "grad_norm": 3.7116174697875977,
        "learning_rate": 1.0217951748263899e-06,
        "epoch": 1.3046666666666666,
        "step": 9785
    },
    {
        "loss": 1.7695,
        "grad_norm": 3.506922721862793,
        "learning_rate": 1.0128407181091315e-06,
        "epoch": 1.3048,
        "step": 9786
    },
    {
        "loss": 1.7716,
        "grad_norm": 2.4991846084594727,
        "learning_rate": 1.0039254705429546e-06,
        "epoch": 1.3049333333333333,
        "step": 9787
    },
    {
        "loss": 1.585,
        "grad_norm": 3.686098575592041,
        "learning_rate": 9.950494356592344e-07,
        "epoch": 1.3050666666666666,
        "step": 9788
    },
    {
        "loss": 2.0799,
        "grad_norm": 4.173479080200195,
        "learning_rate": 9.862126169737584e-07,
        "epoch": 1.3052000000000001,
        "step": 9789
    },
    {
        "loss": 1.9183,
        "grad_norm": 2.7669944763183594,
        "learning_rate": 9.774150179868712e-07,
        "epoch": 1.3053333333333335,
        "step": 9790
    },
    {
        "loss": 1.7226,
        "grad_norm": 3.633563280105591,
        "learning_rate": 9.686566421832854e-07,
        "epoch": 1.3054666666666668,
        "step": 9791
    },
    {
        "loss": 1.6805,
        "grad_norm": 5.014254093170166,
        "learning_rate": 9.599374930322701e-07,
        "epoch": 1.3056,
        "step": 9792
    },
    {
        "loss": 2.001,
        "grad_norm": 5.0442094802856445,
        "learning_rate": 9.51257573987463e-07,
        "epoch": 1.3057333333333334,
        "step": 9793
    },
    {
        "loss": 2.043,
        "grad_norm": 3.9990384578704834,
        "learning_rate": 9.426168884870356e-07,
        "epoch": 1.3058666666666667,
        "step": 9794
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.9235072135925293,
        "learning_rate": 9.340154399535838e-07,
        "epoch": 1.306,
        "step": 9795
    },
    {
        "loss": 2.0714,
        "grad_norm": 3.32248854637146,
        "learning_rate": 9.254532317941933e-07,
        "epoch": 1.3061333333333334,
        "step": 9796
    },
    {
        "loss": 2.2425,
        "grad_norm": 3.5263264179229736,
        "learning_rate": 9.169302674003621e-07,
        "epoch": 1.3062666666666667,
        "step": 9797
    },
    {
        "loss": 2.2004,
        "grad_norm": 2.8295187950134277,
        "learning_rate": 9.084465501480455e-07,
        "epoch": 1.3064,
        "step": 9798
    },
    {
        "loss": 1.3575,
        "grad_norm": 4.478970050811768,
        "learning_rate": 9.000020833977219e-07,
        "epoch": 1.3065333333333333,
        "step": 9799
    },
    {
        "loss": 2.7224,
        "grad_norm": 3.0964303016662598,
        "learning_rate": 8.915968704942379e-07,
        "epoch": 1.3066666666666666,
        "step": 9800
    },
    {
        "loss": 1.5708,
        "grad_norm": 3.875685453414917,
        "learning_rate": 8.832309147669415e-07,
        "epoch": 1.3068,
        "step": 9801
    },
    {
        "loss": 2.1067,
        "grad_norm": 2.9414100646972656,
        "learning_rate": 8.749042195296042e-07,
        "epoch": 1.3069333333333333,
        "step": 9802
    },
    {
        "loss": 2.0387,
        "grad_norm": 2.4260077476501465,
        "learning_rate": 8.666167880804654e-07,
        "epoch": 1.3070666666666666,
        "step": 9803
    },
    {
        "loss": 1.7705,
        "grad_norm": 3.183682441711426,
        "learning_rate": 8.583686237022326e-07,
        "epoch": 1.3072,
        "step": 9804
    },
    {
        "loss": 1.3911,
        "grad_norm": 3.2572522163391113,
        "learning_rate": 8.50159729661959e-07,
        "epoch": 1.3073333333333332,
        "step": 9805
    },
    {
        "loss": 1.6355,
        "grad_norm": 4.102065563201904,
        "learning_rate": 8.419901092112881e-07,
        "epoch": 1.3074666666666666,
        "step": 9806
    },
    {
        "loss": 2.2892,
        "grad_norm": 4.3668532371521,
        "learning_rate": 8.33859765586198e-07,
        "epoch": 1.3075999999999999,
        "step": 9807
    },
    {
        "loss": 2.3491,
        "grad_norm": 5.398359775543213,
        "learning_rate": 8.257687020071681e-07,
        "epoch": 1.3077333333333334,
        "step": 9808
    },
    {
        "loss": 2.3504,
        "grad_norm": 2.917180299758911,
        "learning_rate": 8.177169216790903e-07,
        "epoch": 1.3078666666666667,
        "step": 9809
    },
    {
        "loss": 2.1107,
        "grad_norm": 4.070512294769287,
        "learning_rate": 8.097044277912691e-07,
        "epoch": 1.308,
        "step": 9810
    },
    {
        "loss": 1.8608,
        "grad_norm": 5.055069923400879,
        "learning_rate": 8.017312235175212e-07,
        "epoch": 1.3081333333333334,
        "step": 9811
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.896521806716919,
        "learning_rate": 7.937973120160314e-07,
        "epoch": 1.3082666666666667,
        "step": 9812
    },
    {
        "loss": 2.0085,
        "grad_norm": 4.198263645172119,
        "learning_rate": 7.859026964294968e-07,
        "epoch": 1.3084,
        "step": 9813
    },
    {
        "loss": 2.0688,
        "grad_norm": 3.3739545345306396,
        "learning_rate": 7.780473798849275e-07,
        "epoch": 1.3085333333333333,
        "step": 9814
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.9304182529449463,
        "learning_rate": 7.702313654939119e-07,
        "epoch": 1.3086666666666666,
        "step": 9815
    },
    {
        "loss": 1.2886,
        "grad_norm": 4.801833152770996,
        "learning_rate": 7.624546563523405e-07,
        "epoch": 1.3088,
        "step": 9816
    },
    {
        "loss": 2.8672,
        "grad_norm": 2.2871017456054688,
        "learning_rate": 7.5471725554066e-07,
        "epoch": 1.3089333333333333,
        "step": 9817
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.91241717338562,
        "learning_rate": 7.470191661236192e-07,
        "epoch": 1.3090666666666666,
        "step": 9818
    },
    {
        "loss": 2.5798,
        "grad_norm": 3.2918813228607178,
        "learning_rate": 7.393603911504899e-07,
        "epoch": 1.3092,
        "step": 9819
    },
    {
        "loss": 2.3867,
        "grad_norm": 4.006103038787842,
        "learning_rate": 7.31740933654923e-07,
        "epoch": 1.3093333333333335,
        "step": 9820
    },
    {
        "loss": 1.4593,
        "grad_norm": 3.633204221725464,
        "learning_rate": 7.241607966550379e-07,
        "epoch": 1.3094666666666668,
        "step": 9821
    },
    {
        "loss": 2.2766,
        "grad_norm": 3.0893149375915527,
        "learning_rate": 7.166199831533327e-07,
        "epoch": 1.3096,
        "step": 9822
    },
    {
        "loss": 2.0866,
        "grad_norm": 4.985596656799316,
        "learning_rate": 7.091184961367403e-07,
        "epoch": 1.3097333333333334,
        "step": 9823
    },
    {
        "loss": 1.818,
        "grad_norm": 3.8934779167175293,
        "learning_rate": 7.016563385766395e-07,
        "epoch": 1.3098666666666667,
        "step": 9824
    },
    {
        "loss": 2.4074,
        "grad_norm": 3.1675918102264404,
        "learning_rate": 6.942335134288103e-07,
        "epoch": 1.31,
        "step": 9825
    },
    {
        "loss": 2.1433,
        "grad_norm": 2.6135239601135254,
        "learning_rate": 6.868500236334785e-07,
        "epoch": 1.3101333333333334,
        "step": 9826
    },
    {
        "loss": 1.5624,
        "grad_norm": 3.1663668155670166,
        "learning_rate": 6.795058721152381e-07,
        "epoch": 1.3102666666666667,
        "step": 9827
    },
    {
        "loss": 1.6159,
        "grad_norm": 3.7644031047821045,
        "learning_rate": 6.722010617831509e-07,
        "epoch": 1.3104,
        "step": 9828
    },
    {
        "loss": 1.9784,
        "grad_norm": 2.845515727996826,
        "learning_rate": 6.649355955306801e-07,
        "epoch": 1.3105333333333333,
        "step": 9829
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.9760940074920654,
        "learning_rate": 6.577094762356906e-07,
        "epoch": 1.3106666666666666,
        "step": 9830
    },
    {
        "loss": 1.4186,
        "grad_norm": 2.6735148429870605,
        "learning_rate": 6.505227067604924e-07,
        "epoch": 1.3108,
        "step": 9831
    },
    {
        "loss": 1.5534,
        "grad_norm": 3.8811612129211426,
        "learning_rate": 6.433752899517531e-07,
        "epoch": 1.3109333333333333,
        "step": 9832
    },
    {
        "loss": 2.7701,
        "grad_norm": 2.3364686965942383,
        "learning_rate": 6.36267228640619e-07,
        "epoch": 1.3110666666666666,
        "step": 9833
    },
    {
        "loss": 2.2162,
        "grad_norm": 3.5393292903900146,
        "learning_rate": 6.291985256426158e-07,
        "epoch": 1.3112,
        "step": 9834
    },
    {
        "loss": 1.8097,
        "grad_norm": 3.4904837608337402,
        "learning_rate": 6.221691837576704e-07,
        "epoch": 1.3113333333333332,
        "step": 9835
    },
    {
        "loss": 2.9394,
        "grad_norm": 3.425138235092163,
        "learning_rate": 6.151792057701334e-07,
        "epoch": 1.3114666666666666,
        "step": 9836
    },
    {
        "loss": 1.6406,
        "grad_norm": 3.8014919757843018,
        "learning_rate": 6.082285944487454e-07,
        "epoch": 1.3115999999999999,
        "step": 9837
    },
    {
        "loss": 1.6071,
        "grad_norm": 3.9453084468841553,
        "learning_rate": 6.013173525467042e-07,
        "epoch": 1.3117333333333332,
        "step": 9838
    },
    {
        "loss": 2.6872,
        "grad_norm": 3.7243599891662598,
        "learning_rate": 5.944454828015311e-07,
        "epoch": 1.3118666666666667,
        "step": 9839
    },
    {
        "loss": 2.0071,
        "grad_norm": 4.203530788421631,
        "learning_rate": 5.876129879352377e-07,
        "epoch": 1.312,
        "step": 9840
    },
    {
        "loss": 2.2795,
        "grad_norm": 4.70871639251709,
        "learning_rate": 5.80819870654159e-07,
        "epoch": 1.3121333333333334,
        "step": 9841
    },
    {
        "loss": 2.7201,
        "grad_norm": 2.6233787536621094,
        "learning_rate": 5.740661336491093e-07,
        "epoch": 1.3122666666666667,
        "step": 9842
    },
    {
        "loss": 1.9271,
        "grad_norm": 3.6638660430908203,
        "learning_rate": 5.673517795952488e-07,
        "epoch": 1.3124,
        "step": 9843
    },
    {
        "loss": 2.6606,
        "grad_norm": 3.589648485183716,
        "learning_rate": 5.606768111521499e-07,
        "epoch": 1.3125333333333333,
        "step": 9844
    },
    {
        "loss": 1.919,
        "grad_norm": 5.775468349456787,
        "learning_rate": 5.540412309637866e-07,
        "epoch": 1.3126666666666666,
        "step": 9845
    },
    {
        "loss": 2.1425,
        "grad_norm": 3.9036972522735596,
        "learning_rate": 5.474450416585563e-07,
        "epoch": 1.3128,
        "step": 9846
    },
    {
        "loss": 2.7644,
        "grad_norm": 3.021648406982422,
        "learning_rate": 5.408882458492359e-07,
        "epoch": 1.3129333333333333,
        "step": 9847
    },
    {
        "loss": 3.1404,
        "grad_norm": 3.7999589443206787,
        "learning_rate": 5.343708461329588e-07,
        "epoch": 1.3130666666666666,
        "step": 9848
    },
    {
        "loss": 2.4388,
        "grad_norm": 2.996906280517578,
        "learning_rate": 5.278928450913156e-07,
        "epoch": 1.3132,
        "step": 9849
    },
    {
        "loss": 3.0811,
        "grad_norm": 3.031707525253296,
        "learning_rate": 5.214542452902538e-07,
        "epoch": 1.3133333333333335,
        "step": 9850
    },
    {
        "loss": 1.9286,
        "grad_norm": 3.1239590644836426,
        "learning_rate": 5.150550492801442e-07,
        "epoch": 1.3134666666666668,
        "step": 9851
    },
    {
        "loss": 2.2242,
        "grad_norm": 3.3353772163391113,
        "learning_rate": 5.08695259595715e-07,
        "epoch": 1.3136,
        "step": 9852
    },
    {
        "loss": 2.0253,
        "grad_norm": 3.798542022705078,
        "learning_rate": 5.023748787560956e-07,
        "epoch": 1.3137333333333334,
        "step": 9853
    },
    {
        "loss": 2.1538,
        "grad_norm": 3.6079866886138916,
        "learning_rate": 4.960939092648054e-07,
        "epoch": 1.3138666666666667,
        "step": 9854
    },
    {
        "loss": 0.9778,
        "grad_norm": 3.922344446182251,
        "learning_rate": 4.898523536097876e-07,
        "epoch": 1.314,
        "step": 9855
    },
    {
        "loss": 1.7475,
        "grad_norm": 2.740111827850342,
        "learning_rate": 4.836502142633315e-07,
        "epoch": 1.3141333333333334,
        "step": 9856
    },
    {
        "loss": 1.2161,
        "grad_norm": 3.2236931324005127,
        "learning_rate": 4.774874936820939e-07,
        "epoch": 1.3142666666666667,
        "step": 9857
    },
    {
        "loss": 3.4791,
        "grad_norm": 9.687119483947754,
        "learning_rate": 4.713641943071889e-07,
        "epoch": 1.3144,
        "step": 9858
    },
    {
        "loss": 2.6018,
        "grad_norm": 3.925783395767212,
        "learning_rate": 4.6528031856406527e-07,
        "epoch": 1.3145333333333333,
        "step": 9859
    },
    {
        "loss": 1.3899,
        "grad_norm": 4.015607833862305,
        "learning_rate": 4.592358688625509e-07,
        "epoch": 1.3146666666666667,
        "step": 9860
    },
    {
        "loss": 1.8395,
        "grad_norm": 3.8099093437194824,
        "learning_rate": 4.532308475968972e-07,
        "epoch": 1.3148,
        "step": 9861
    },
    {
        "loss": 1.8962,
        "grad_norm": 3.511660099029541,
        "learning_rate": 4.4726525714569036e-07,
        "epoch": 1.3149333333333333,
        "step": 9862
    },
    {
        "loss": 1.1815,
        "grad_norm": 4.346688747406006,
        "learning_rate": 4.413390998719513e-07,
        "epoch": 1.3150666666666666,
        "step": 9863
    },
    {
        "loss": 1.9837,
        "grad_norm": 3.436781644821167,
        "learning_rate": 4.3545237812301353e-07,
        "epoch": 1.3152,
        "step": 9864
    },
    {
        "loss": 2.5339,
        "grad_norm": 3.767770290374756,
        "learning_rate": 4.296050942306562e-07,
        "epoch": 1.3153333333333332,
        "step": 9865
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.8458876609802246,
        "learning_rate": 4.2379725051099327e-07,
        "epoch": 1.3154666666666666,
        "step": 9866
    },
    {
        "loss": 1.9793,
        "grad_norm": 3.0794873237609863,
        "learning_rate": 4.180288492645401e-07,
        "epoch": 1.3155999999999999,
        "step": 9867
    },
    {
        "loss": 2.0344,
        "grad_norm": 3.287381887435913,
        "learning_rate": 4.1229989277619117e-07,
        "epoch": 1.3157333333333332,
        "step": 9868
    },
    {
        "loss": 2.377,
        "grad_norm": 4.353209495544434,
        "learning_rate": 4.0661038331517574e-07,
        "epoch": 1.3158666666666667,
        "step": 9869
    },
    {
        "loss": 1.8193,
        "grad_norm": 2.73504638671875,
        "learning_rate": 4.0096032313514667e-07,
        "epoch": 1.316,
        "step": 9870
    },
    {
        "loss": 1.1728,
        "grad_norm": 3.6915764808654785,
        "learning_rate": 3.953497144741025e-07,
        "epoch": 1.3161333333333334,
        "step": 9871
    },
    {
        "loss": 1.6077,
        "grad_norm": 3.225961446762085,
        "learning_rate": 3.897785595544434e-07,
        "epoch": 1.3162666666666667,
        "step": 9872
    },
    {
        "loss": 1.7933,
        "grad_norm": 3.849578857421875,
        "learning_rate": 3.8424686058290415e-07,
        "epoch": 1.3164,
        "step": 9873
    },
    {
        "loss": 2.4251,
        "grad_norm": 2.9484426975250244,
        "learning_rate": 3.787546197506098e-07,
        "epoch": 1.3165333333333333,
        "step": 9874
    },
    {
        "loss": 2.7386,
        "grad_norm": 3.7502760887145996,
        "learning_rate": 3.733018392330645e-07,
        "epoch": 1.3166666666666667,
        "step": 9875
    },
    {
        "loss": 1.7818,
        "grad_norm": 3.62744140625,
        "learning_rate": 3.678885211901406e-07,
        "epoch": 1.3168,
        "step": 9876
    },
    {
        "loss": 2.9037,
        "grad_norm": 3.8822526931762695,
        "learning_rate": 3.6251466776605625e-07,
        "epoch": 1.3169333333333333,
        "step": 9877
    },
    {
        "loss": 2.2169,
        "grad_norm": 2.897890567779541,
        "learning_rate": 3.571802810894087e-07,
        "epoch": 1.3170666666666666,
        "step": 9878
    },
    {
        "loss": 2.1196,
        "grad_norm": 3.388643264770508,
        "learning_rate": 3.5188536327318557e-07,
        "epoch": 1.3172,
        "step": 9879
    },
    {
        "loss": 2.2755,
        "grad_norm": 4.339017868041992,
        "learning_rate": 3.466299164147091e-07,
        "epoch": 1.3173333333333335,
        "step": 9880
    },
    {
        "loss": 2.3687,
        "grad_norm": 3.4023666381835938,
        "learning_rate": 3.4141394259569195e-07,
        "epoch": 1.3174666666666668,
        "step": 9881
    },
    {
        "loss": 2.1453,
        "grad_norm": 2.545253276824951,
        "learning_rate": 3.3623744388218136e-07,
        "epoch": 1.3176,
        "step": 9882
    },
    {
        "loss": 1.9756,
        "grad_norm": 3.5383472442626953,
        "learning_rate": 3.3110042232461504e-07,
        "epoch": 1.3177333333333334,
        "step": 9883
    },
    {
        "loss": 0.9003,
        "grad_norm": 3.6547765731811523,
        "learning_rate": 3.2600287995779854e-07,
        "epoch": 1.3178666666666667,
        "step": 9884
    },
    {
        "loss": 1.2705,
        "grad_norm": 4.16243314743042,
        "learning_rate": 3.2094481880086126e-07,
        "epoch": 1.318,
        "step": 9885
    },
    {
        "loss": 2.1444,
        "grad_norm": 3.7353148460388184,
        "learning_rate": 3.159262408573227e-07,
        "epoch": 1.3181333333333334,
        "step": 9886
    },
    {
        "loss": 2.9157,
        "grad_norm": 4.630495071411133,
        "learning_rate": 3.1094714811507053e-07,
        "epoch": 1.3182666666666667,
        "step": 9887
    },
    {
        "loss": 2.279,
        "grad_norm": 3.7673656940460205,
        "learning_rate": 3.060075425463382e-07,
        "epoch": 1.3184,
        "step": 9888
    },
    {
        "loss": 1.9761,
        "grad_norm": 2.6977007389068604,
        "learning_rate": 3.0110742610771623e-07,
        "epoch": 1.3185333333333333,
        "step": 9889
    },
    {
        "loss": 2.0077,
        "grad_norm": 2.6059298515319824,
        "learning_rate": 2.9624680074016307e-07,
        "epoch": 1.3186666666666667,
        "step": 9890
    },
    {
        "loss": 0.7233,
        "grad_norm": 3.2417149543762207,
        "learning_rate": 2.91425668368972e-07,
        "epoch": 1.3188,
        "step": 9891
    },
    {
        "loss": 2.1835,
        "grad_norm": 3.727036714553833,
        "learning_rate": 2.8664403090382654e-07,
        "epoch": 1.3189333333333333,
        "step": 9892
    },
    {
        "loss": 1.7459,
        "grad_norm": 3.5601894855499268,
        "learning_rate": 2.81901890238756e-07,
        "epoch": 1.3190666666666666,
        "step": 9893
    },
    {
        "loss": 2.8119,
        "grad_norm": 3.989304780960083,
        "learning_rate": 2.771992482521135e-07,
        "epoch": 1.3192,
        "step": 9894
    },
    {
        "loss": 1.5366,
        "grad_norm": 4.471860885620117,
        "learning_rate": 2.7253610680665344e-07,
        "epoch": 1.3193333333333332,
        "step": 9895
    },
    {
        "loss": 2.2932,
        "grad_norm": 3.5479321479797363,
        "learning_rate": 2.6791246774945377e-07,
        "epoch": 1.3194666666666666,
        "step": 9896
    },
    {
        "loss": 1.252,
        "grad_norm": 4.037176132202148,
        "learning_rate": 2.6332833291196067e-07,
        "epoch": 1.3195999999999999,
        "step": 9897
    },
    {
        "loss": 2.0678,
        "grad_norm": 4.310827732086182,
        "learning_rate": 2.587837041099439e-07,
        "epoch": 1.3197333333333332,
        "step": 9898
    },
    {
        "loss": 1.0618,
        "grad_norm": 5.091570854187012,
        "learning_rate": 2.5427858314357454e-07,
        "epoch": 1.3198666666666667,
        "step": 9899
    },
    {
        "loss": 0.6523,
        "grad_norm": 3.376919984817505,
        "learning_rate": 2.498129717973252e-07,
        "epoch": 1.32,
        "step": 9900
    },
    {
        "loss": 2.2986,
        "grad_norm": 3.1373980045318604,
        "learning_rate": 2.453868718400587e-07,
        "epoch": 1.3201333333333334,
        "step": 9901
    },
    {
        "loss": 2.3112,
        "grad_norm": 4.129183292388916,
        "learning_rate": 2.4100028502495044e-07,
        "epoch": 1.3202666666666667,
        "step": 9902
    },
    {
        "loss": 2.1778,
        "grad_norm": 3.0044896602630615,
        "learning_rate": 2.366532130895438e-07,
        "epoch": 1.3204,
        "step": 9903
    },
    {
        "loss": 2.705,
        "grad_norm": 2.6957218647003174,
        "learning_rate": 2.3234565775575036e-07,
        "epoch": 1.3205333333333333,
        "step": 9904
    },
    {
        "loss": 2.1671,
        "grad_norm": 3.0146937370300293,
        "learning_rate": 2.2807762072977191e-07,
        "epoch": 1.3206666666666667,
        "step": 9905
    },
    {
        "loss": 2.4192,
        "grad_norm": 2.0199995040893555,
        "learning_rate": 2.238491037022339e-07,
        "epoch": 1.3208,
        "step": 9906
    },
    {
        "loss": 1.9009,
        "grad_norm": 6.2062482833862305,
        "learning_rate": 2.1966010834802987e-07,
        "epoch": 1.3209333333333333,
        "step": 9907
    },
    {
        "loss": 2.3239,
        "grad_norm": 3.3968069553375244,
        "learning_rate": 2.1551063632645474e-07,
        "epoch": 1.3210666666666666,
        "step": 9908
    },
    {
        "loss": 1.8764,
        "grad_norm": 4.407013416290283,
        "learning_rate": 2.114006892811271e-07,
        "epoch": 1.3212,
        "step": 9909
    },
    {
        "loss": 2.354,
        "grad_norm": 2.146624803543091,
        "learning_rate": 2.073302688400114e-07,
        "epoch": 1.3213333333333335,
        "step": 9910
    },
    {
        "loss": 2.0034,
        "grad_norm": 6.015108585357666,
        "learning_rate": 2.0329937661539565e-07,
        "epoch": 1.3214666666666668,
        "step": 9911
    },
    {
        "loss": 0.5282,
        "grad_norm": 2.559297800064087,
        "learning_rate": 1.9930801420395828e-07,
        "epoch": 1.3216,
        "step": 9912
    },
    {
        "loss": 1.4797,
        "grad_norm": 3.922142744064331,
        "learning_rate": 1.9535618318666792e-07,
        "epoch": 1.3217333333333334,
        "step": 9913
    },
    {
        "loss": 1.6655,
        "grad_norm": 4.319127082824707,
        "learning_rate": 1.9144388512888356e-07,
        "epoch": 1.3218666666666667,
        "step": 9914
    },
    {
        "loss": 1.5481,
        "grad_norm": 4.160261631011963,
        "learning_rate": 1.8757112158026558e-07,
        "epoch": 1.322,
        "step": 9915
    },
    {
        "loss": 1.2594,
        "grad_norm": 4.510994911193848,
        "learning_rate": 1.8373789407480912e-07,
        "epoch": 1.3221333333333334,
        "step": 9916
    },
    {
        "loss": 2.6629,
        "grad_norm": 4.040741920471191,
        "learning_rate": 1.7994420413091073e-07,
        "epoch": 1.3222666666666667,
        "step": 9917
    },
    {
        "loss": 1.8347,
        "grad_norm": 3.499619245529175,
        "learning_rate": 1.7619005325123505e-07,
        "epoch": 1.3224,
        "step": 9918
    },
    {
        "loss": 1.1206,
        "grad_norm": 4.036463260650635,
        "learning_rate": 1.7247544292281482e-07,
        "epoch": 1.3225333333333333,
        "step": 9919
    },
    {
        "loss": 2.1758,
        "grad_norm": 3.1688296794891357,
        "learning_rate": 1.6880037461702857e-07,
        "epoch": 1.3226666666666667,
        "step": 9920
    },
    {
        "loss": 2.3214,
        "grad_norm": 2.92185115814209,
        "learning_rate": 1.651648497895786e-07,
        "epoch": 1.3228,
        "step": 9921
    },
    {
        "loss": 0.7546,
        "grad_norm": 1.8860220909118652,
        "learning_rate": 1.615688698805129e-07,
        "epoch": 1.3229333333333333,
        "step": 9922
    },
    {
        "loss": 2.0153,
        "grad_norm": 4.19914436340332,
        "learning_rate": 1.5801243631420326e-07,
        "epoch": 1.3230666666666666,
        "step": 9923
    },
    {
        "loss": 3.055,
        "grad_norm": 3.4123964309692383,
        "learning_rate": 1.5449555049937835e-07,
        "epoch": 1.3232,
        "step": 9924
    },
    {
        "loss": 1.4484,
        "grad_norm": 3.9845850467681885,
        "learning_rate": 1.5101821382907944e-07,
        "epoch": 1.3233333333333333,
        "step": 9925
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.9761385917663574,
        "learning_rate": 1.4758042768069359e-07,
        "epoch": 1.3234666666666666,
        "step": 9926
    },
    {
        "loss": 1.9292,
        "grad_norm": 2.8678386211395264,
        "learning_rate": 1.4418219341594262e-07,
        "epoch": 1.3235999999999999,
        "step": 9927
    },
    {
        "loss": 0.9408,
        "grad_norm": 4.661907196044922,
        "learning_rate": 1.40823512380861e-07,
        "epoch": 1.3237333333333332,
        "step": 9928
    },
    {
        "loss": 0.5639,
        "grad_norm": 3.071824550628662,
        "learning_rate": 1.3750438590586222e-07,
        "epoch": 1.3238666666666667,
        "step": 9929
    },
    {
        "loss": 0.8494,
        "grad_norm": 3.0997838973999023,
        "learning_rate": 1.3422481530563912e-07,
        "epoch": 1.324,
        "step": 9930
    },
    {
        "loss": 2.4429,
        "grad_norm": 1.9930367469787598,
        "learning_rate": 1.3098480187926366e-07,
        "epoch": 1.3241333333333334,
        "step": 9931
    },
    {
        "loss": 1.5866,
        "grad_norm": 3.2119123935699463,
        "learning_rate": 1.27784346910087e-07,
        "epoch": 1.3242666666666667,
        "step": 9932
    },
    {
        "loss": 1.78,
        "grad_norm": 2.827955484390259,
        "learning_rate": 1.2462345166585065e-07,
        "epoch": 1.3244,
        "step": 9933
    },
    {
        "loss": 1.6349,
        "grad_norm": 3.770994186401367,
        "learning_rate": 1.2150211739857532e-07,
        "epoch": 1.3245333333333333,
        "step": 9934
    },
    {
        "loss": 1.6899,
        "grad_norm": 4.5975470542907715,
        "learning_rate": 1.1842034534464974e-07,
        "epoch": 1.3246666666666667,
        "step": 9935
    },
    {
        "loss": 2.7831,
        "grad_norm": 3.6251306533813477,
        "learning_rate": 1.1537813672475307e-07,
        "epoch": 1.3248,
        "step": 9936
    },
    {
        "loss": 0.8734,
        "grad_norm": 2.507272958755493,
        "learning_rate": 1.1237549274392133e-07,
        "epoch": 1.3249333333333333,
        "step": 9937
    },
    {
        "loss": 2.5508,
        "grad_norm": 2.5888519287109375,
        "learning_rate": 1.094124145915254e-07,
        "epoch": 1.3250666666666666,
        "step": 9938
    },
    {
        "loss": 2.4796,
        "grad_norm": 2.5457608699798584,
        "learning_rate": 1.0648890344123751e-07,
        "epoch": 1.3252,
        "step": 9939
    },
    {
        "loss": 2.3939,
        "grad_norm": 2.921809434890747,
        "learning_rate": 1.0360496045107582e-07,
        "epoch": 1.3253333333333333,
        "step": 9940
    },
    {
        "loss": 2.6823,
        "grad_norm": 3.3567426204681396,
        "learning_rate": 1.0076058676337097e-07,
        "epoch": 1.3254666666666668,
        "step": 9941
    },
    {
        "loss": 0.8543,
        "grad_norm": 3.4537839889526367,
        "learning_rate": 9.79557835047995e-08,
        "epoch": 1.3256000000000001,
        "step": 9942
    },
    {
        "loss": 2.3388,
        "grad_norm": 3.120090961456299,
        "learning_rate": 9.51905517863505e-08,
        "epoch": 1.3257333333333334,
        "step": 9943
    },
    {
        "loss": 1.5178,
        "grad_norm": 3.312520980834961,
        "learning_rate": 9.246489270333669e-08,
        "epoch": 1.3258666666666667,
        "step": 9944
    },
    {
        "loss": 1.8593,
        "grad_norm": 3.9980084896087646,
        "learning_rate": 8.977880733541666e-08,
        "epoch": 1.326,
        "step": 9945
    },
    {
        "loss": 1.286,
        "grad_norm": 4.120863914489746,
        "learning_rate": 8.713229674653933e-08,
        "epoch": 1.3261333333333334,
        "step": 9946
    },
    {
        "loss": 1.4686,
        "grad_norm": 2.644699811935425,
        "learning_rate": 8.45253619849995e-08,
        "epoch": 1.3262666666666667,
        "step": 9947
    },
    {
        "loss": 1.8579,
        "grad_norm": 3.1072230339050293,
        "learning_rate": 8.195800408342668e-08,
        "epoch": 1.3264,
        "step": 9948
    },
    {
        "loss": 1.4356,
        "grad_norm": 5.034914016723633,
        "learning_rate": 7.943022405874079e-08,
        "epoch": 1.3265333333333333,
        "step": 9949
    },
    {
        "loss": 2.0467,
        "grad_norm": 3.6994152069091797,
        "learning_rate": 7.694202291221863e-08,
        "epoch": 1.3266666666666667,
        "step": 9950
    },
    {
        "loss": 2.0311,
        "grad_norm": 3.7135086059570312,
        "learning_rate": 7.449340162944963e-08,
        "epoch": 1.3268,
        "step": 9951
    },
    {
        "loss": 2.212,
        "grad_norm": 2.193467140197754,
        "learning_rate": 7.208436118032458e-08,
        "epoch": 1.3269333333333333,
        "step": 9952
    },
    {
        "loss": 2.3815,
        "grad_norm": 5.140255928039551,
        "learning_rate": 6.971490251908019e-08,
        "epoch": 1.3270666666666666,
        "step": 9953
    },
    {
        "loss": 2.262,
        "grad_norm": 2.624474048614502,
        "learning_rate": 6.73850265842657e-08,
        "epoch": 1.3272,
        "step": 9954
    },
    {
        "loss": 1.54,
        "grad_norm": 6.360062599182129,
        "learning_rate": 6.509473429875402e-08,
        "epoch": 1.3273333333333333,
        "step": 9955
    },
    {
        "loss": 2.5537,
        "grad_norm": 4.169875144958496,
        "learning_rate": 6.284402656974164e-08,
        "epoch": 1.3274666666666666,
        "step": 9956
    },
    {
        "loss": 1.852,
        "grad_norm": 4.245274066925049,
        "learning_rate": 6.063290428872659e-08,
        "epoch": 1.3276,
        "step": 9957
    },
    {
        "loss": 1.7044,
        "grad_norm": 2.981074094772339,
        "learning_rate": 5.8461368331563796e-08,
        "epoch": 1.3277333333333332,
        "step": 9958
    },
    {
        "loss": 0.9623,
        "grad_norm": 2.494162082672119,
        "learning_rate": 5.632941955839854e-08,
        "epoch": 1.3278666666666665,
        "step": 9959
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.119500160217285,
        "learning_rate": 5.423705881369978e-08,
        "epoch": 1.328,
        "step": 9960
    },
    {
        "loss": 2.3381,
        "grad_norm": 3.669663190841675,
        "learning_rate": 5.2184286926248995e-08,
        "epoch": 1.3281333333333334,
        "step": 9961
    },
    {
        "loss": 1.4483,
        "grad_norm": 2.4244911670684814,
        "learning_rate": 5.0171104709173524e-08,
        "epoch": 1.3282666666666667,
        "step": 9962
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.1303255558013916,
        "learning_rate": 4.819751295989106e-08,
        "epoch": 1.3284,
        "step": 9963
    },
    {
        "loss": 1.195,
        "grad_norm": 2.7043886184692383,
        "learning_rate": 4.6263512460154037e-08,
        "epoch": 1.3285333333333333,
        "step": 9964
    },
    {
        "loss": 1.575,
        "grad_norm": 4.598391532897949,
        "learning_rate": 4.4369103976016347e-08,
        "epoch": 1.3286666666666667,
        "step": 9965
    },
    {
        "loss": 2.632,
        "grad_norm": 2.5306172370910645,
        "learning_rate": 4.251428825786663e-08,
        "epoch": 1.3288,
        "step": 9966
    },
    {
        "loss": 1.8536,
        "grad_norm": 3.22552752494812,
        "learning_rate": 4.069906604041718e-08,
        "epoch": 1.3289333333333333,
        "step": 9967
    },
    {
        "loss": 2.8807,
        "grad_norm": 5.384221076965332,
        "learning_rate": 3.892343804265952e-08,
        "epoch": 1.3290666666666666,
        "step": 9968
    },
    {
        "loss": 1.4482,
        "grad_norm": 4.281760215759277,
        "learning_rate": 3.718740496794215e-08,
        "epoch": 1.3292,
        "step": 9969
    },
    {
        "loss": 2.307,
        "grad_norm": 3.3134443759918213,
        "learning_rate": 3.5490967503903907e-08,
        "epoch": 1.3293333333333333,
        "step": 9970
    },
    {
        "loss": 1.7724,
        "grad_norm": 4.4020676612854,
        "learning_rate": 3.383412632251837e-08,
        "epoch": 1.3294666666666668,
        "step": 9971
    },
    {
        "loss": 2.0487,
        "grad_norm": 2.7640697956085205,
        "learning_rate": 3.2216882080060573e-08,
        "epoch": 1.3296000000000001,
        "step": 9972
    },
    {
        "loss": 1.1342,
        "grad_norm": 4.708860397338867,
        "learning_rate": 3.063923541712921e-08,
        "epoch": 1.3297333333333334,
        "step": 9973
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.0007169246673584,
        "learning_rate": 2.9101186958635507e-08,
        "epoch": 1.3298666666666668,
        "step": 9974
    },
    {
        "loss": 1.75,
        "grad_norm": 4.566537380218506,
        "learning_rate": 2.7602737313803252e-08,
        "epoch": 1.33,
        "step": 9975
    },
    {
        "loss": 1.4695,
        "grad_norm": 3.7161617279052734,
        "learning_rate": 2.6143887076168772e-08,
        "epoch": 1.3301333333333334,
        "step": 9976
    },
    {
        "loss": 2.2228,
        "grad_norm": 3.3689472675323486,
        "learning_rate": 2.4724636823603153e-08,
        "epoch": 1.3302666666666667,
        "step": 9977
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.951777935028076,
        "learning_rate": 2.3344987118256722e-08,
        "epoch": 1.3304,
        "step": 9978
    },
    {
        "loss": 2.6244,
        "grad_norm": 5.090376377105713,
        "learning_rate": 2.200493850662566e-08,
        "epoch": 1.3305333333333333,
        "step": 9979
    },
    {
        "loss": 2.456,
        "grad_norm": 2.3953371047973633,
        "learning_rate": 2.0704491519507596e-08,
        "epoch": 1.3306666666666667,
        "step": 9980
    },
    {
        "loss": 2.389,
        "grad_norm": 3.1349852085113525,
        "learning_rate": 1.9443646672012705e-08,
        "epoch": 1.3308,
        "step": 9981
    },
    {
        "loss": 2.3693,
        "grad_norm": 4.107052803039551,
        "learning_rate": 1.822240446355261e-08,
        "epoch": 1.3309333333333333,
        "step": 9982
    },
    {
        "loss": 2.7913,
        "grad_norm": 3.1126277446746826,
        "learning_rate": 1.7040765377884792e-08,
        "epoch": 1.3310666666666666,
        "step": 9983
    },
    {
        "loss": 1.4402,
        "grad_norm": 2.069441318511963,
        "learning_rate": 1.5898729883057072e-08,
        "epoch": 1.3312,
        "step": 9984
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.2970378398895264,
        "learning_rate": 1.4796298431429822e-08,
        "epoch": 1.3313333333333333,
        "step": 9985
    },
    {
        "loss": 1.2539,
        "grad_norm": 3.3493220806121826,
        "learning_rate": 1.3733471459675962e-08,
        "epoch": 1.3314666666666666,
        "step": 9986
    },
    {
        "loss": 1.3409,
        "grad_norm": 3.924391984939575,
        "learning_rate": 1.2710249388792061e-08,
        "epoch": 1.3316,
        "step": 9987
    },
    {
        "loss": 2.0849,
        "grad_norm": 5.026620864868164,
        "learning_rate": 1.1726632624076139e-08,
        "epoch": 1.3317333333333332,
        "step": 9988
    },
    {
        "loss": 1.4381,
        "grad_norm": 3.5543768405914307,
        "learning_rate": 1.0782621555138761e-08,
        "epoch": 1.3318666666666665,
        "step": 9989
    },
    {
        "loss": 1.8397,
        "grad_norm": 4.516401290893555,
        "learning_rate": 9.878216555925246e-09,
        "epoch": 1.332,
        "step": 9990
    },
    {
        "loss": 1.5256,
        "grad_norm": 4.8640289306640625,
        "learning_rate": 9.013417984637951e-09,
        "epoch": 1.3321333333333334,
        "step": 9991
    },
    {
        "loss": 1.5153,
        "grad_norm": 4.09491491317749,
        "learning_rate": 8.188226183858394e-09,
        "epoch": 1.3322666666666667,
        "step": 9992
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.722673177719116,
        "learning_rate": 7.402641480436234e-09,
        "epoch": 1.3324,
        "step": 9993
    },
    {
        "loss": 1.7484,
        "grad_norm": 4.1168107986450195,
        "learning_rate": 6.656664185533678e-09,
        "epoch": 1.3325333333333333,
        "step": 9994
    },
    {
        "loss": 1.3411,
        "grad_norm": 3.198726177215576,
        "learning_rate": 5.95029459465879e-09,
        "epoch": 1.3326666666666667,
        "step": 9995
    },
    {
        "loss": 2.2244,
        "grad_norm": 4.56636905670166,
        "learning_rate": 5.283532987587769e-09,
        "epoch": 1.3328,
        "step": 9996
    },
    {
        "loss": 2.0068,
        "grad_norm": 2.2249414920806885,
        "learning_rate": 4.656379628431573e-09,
        "epoch": 1.3329333333333333,
        "step": 9997
    },
    {
        "loss": 2.3273,
        "grad_norm": 2.0022194385528564,
        "learning_rate": 4.068834765613705e-09,
        "epoch": 1.3330666666666666,
        "step": 9998
    },
    {
        "loss": 1.6021,
        "grad_norm": 3.2079269886016846,
        "learning_rate": 3.520898631848013e-09,
        "epoch": 1.3332,
        "step": 9999
    },
    {
        "loss": 1.2341,
        "grad_norm": 4.012781620025635,
        "learning_rate": 3.012571444194201e-09,
        "epoch": 1.3333333333333333,
        "step": 10000
    },
    {
        "loss": 1.4457,
        "grad_norm": 2.487262487411499,
        "learning_rate": 2.543853403991214e-09,
        "epoch": 1.3334666666666668,
        "step": 10001
    },
    {
        "loss": 2.1578,
        "grad_norm": 3.0163822174072266,
        "learning_rate": 2.114744696890547e-09,
        "epoch": 1.3336000000000001,
        "step": 10002
    },
    {
        "loss": 2.4141,
        "grad_norm": 4.237850189208984,
        "learning_rate": 1.7252454928895489e-09,
        "epoch": 1.3337333333333334,
        "step": 10003
    },
    {
        "loss": 2.6939,
        "grad_norm": 3.52417254447937,
        "learning_rate": 1.375355946242607e-09,
        "epoch": 1.3338666666666668,
        "step": 10004
    },
    {
        "loss": 2.785,
        "grad_norm": 3.177117347717285,
        "learning_rate": 1.0650761955610656e-09,
        "epoch": 1.334,
        "step": 10005
    },
    {
        "loss": 2.4811,
        "grad_norm": 2.334681272506714,
        "learning_rate": 7.944063637355114e-10,
        "epoch": 1.3341333333333334,
        "step": 10006
    },
    {
        "loss": 1.4018,
        "grad_norm": 2.440581798553467,
        "learning_rate": 5.633465579912844e-10,
        "epoch": 1.3342666666666667,
        "step": 10007
    },
    {
        "loss": 2.6154,
        "grad_norm": 3.764064073562622,
        "learning_rate": 3.718968698329661e-10,
        "epoch": 1.3344,
        "step": 10008
    },
    {
        "loss": 1.8689,
        "grad_norm": 4.311528205871582,
        "learning_rate": 2.200573751220958e-10,
        "epoch": 1.3345333333333333,
        "step": 10009
    },
    {
        "loss": 2.8142,
        "grad_norm": 3.515967845916748,
        "learning_rate": 1.0782813397725022e-10,
        "epoch": 1.3346666666666667,
        "step": 10010
    },
    {
        "loss": 1.5936,
        "grad_norm": 4.080203533172607,
        "learning_rate": 3.5209190862861563e-11,
        "epoch": 1.3348,
        "step": 10011
    },
    {
        "loss": 1.9169,
        "grad_norm": 3.4647958278656006,
        "learning_rate": 2.2005745448083758e-12,
        "epoch": 1.3349333333333333,
        "step": 10012
    },
    {
        "loss": 2.5162,
        "grad_norm": 3.301579475402832,
        "learning_rate": 0.00019999999119770193,
        "epoch": 1.3350666666666666,
        "step": 10013
    },
    {
        "loss": 0.9428,
        "grad_norm": 3.748490810394287,
        "learning_rate": 0.0001999999449856411,
        "epoch": 1.3352,
        "step": 10014
    },
    {
        "loss": 1.3129,
        "grad_norm": 4.913283824920654,
        "learning_rate": 0.00019999985916326134,
        "epoch": 1.3353333333333333,
        "step": 10015
    },
    {
        "loss": 1.6279,
        "grad_norm": 2.695967197418213,
        "learning_rate": 0.00019999973373059665,
        "epoch": 1.3354666666666666,
        "step": 10016
    },
    {
        "loss": 2.7187,
        "grad_norm": 3.1239163875579834,
        "learning_rate": 0.00019999956868769662,
        "epoch": 1.3356,
        "step": 10017
    },
    {
        "loss": 2.0519,
        "grad_norm": 3.25351619720459,
        "learning_rate": 0.00019999936403462676,
        "epoch": 1.3357333333333332,
        "step": 10018
    },
    {
        "loss": 2.4389,
        "grad_norm": 4.267697334289551,
        "learning_rate": 0.00019999911977146805,
        "epoch": 1.3358666666666665,
        "step": 10019
    },
    {
        "loss": 2.7263,
        "grad_norm": 2.2543067932128906,
        "learning_rate": 0.00019999883589831723,
        "epoch": 1.336,
        "step": 10020
    },
    {
        "loss": 1.9754,
        "grad_norm": 3.5095696449279785,
        "learning_rate": 0.0001999985124152868,
        "epoch": 1.3361333333333334,
        "step": 10021
    },
    {
        "loss": 1.5897,
        "grad_norm": 6.0709052085876465,
        "learning_rate": 0.00019999814932250486,
        "epoch": 1.3362666666666667,
        "step": 10022
    },
    {
        "loss": 2.162,
        "grad_norm": 3.2293193340301514,
        "learning_rate": 0.00019999774662011522,
        "epoch": 1.3364,
        "step": 10023
    },
    {
        "loss": 1.1656,
        "grad_norm": 3.013169050216675,
        "learning_rate": 0.00019999730430827746,
        "epoch": 1.3365333333333334,
        "step": 10024
    },
    {
        "loss": 2.3697,
        "grad_norm": 2.250535011291504,
        "learning_rate": 0.0001999968223871667,
        "epoch": 1.3366666666666667,
        "step": 10025
    },
    {
        "loss": 2.4323,
        "grad_norm": 2.4049932956695557,
        "learning_rate": 0.00019999630085697386,
        "epoch": 1.3368,
        "step": 10026
    },
    {
        "loss": 1.6431,
        "grad_norm": 5.719805717468262,
        "learning_rate": 0.00019999573971790554,
        "epoch": 1.3369333333333333,
        "step": 10027
    },
    {
        "loss": 2.5206,
        "grad_norm": 2.7046024799346924,
        "learning_rate": 0.00019999513897018395,
        "epoch": 1.3370666666666666,
        "step": 10028
    },
    {
        "loss": 2.094,
        "grad_norm": 4.8079915046691895,
        "learning_rate": 0.00019999449861404713,
        "epoch": 1.3372,
        "step": 10029
    },
    {
        "loss": 2.662,
        "grad_norm": 4.391678810119629,
        "learning_rate": 0.00019999381864974867,
        "epoch": 1.3373333333333333,
        "step": 10030
    },
    {
        "loss": 2.3891,
        "grad_norm": 2.525197744369507,
        "learning_rate": 0.0001999930990775579,
        "epoch": 1.3374666666666668,
        "step": 10031
    },
    {
        "loss": 2.4794,
        "grad_norm": 3.7854552268981934,
        "learning_rate": 0.0001999923398977599,
        "epoch": 1.3376000000000001,
        "step": 10032
    },
    {
        "loss": 2.6994,
        "grad_norm": 3.9677693843841553,
        "learning_rate": 0.00019999154111065534,
        "epoch": 1.3377333333333334,
        "step": 10033
    },
    {
        "loss": 1.5707,
        "grad_norm": 2.972644329071045,
        "learning_rate": 0.00019999070271656063,
        "epoch": 1.3378666666666668,
        "step": 10034
    },
    {
        "loss": 2.3486,
        "grad_norm": 3.030089855194092,
        "learning_rate": 0.00019998982471580785,
        "epoch": 1.338,
        "step": 10035
    },
    {
        "loss": 2.6287,
        "grad_norm": 2.5571682453155518,
        "learning_rate": 0.00019998890710874482,
        "epoch": 1.3381333333333334,
        "step": 10036
    },
    {
        "loss": 1.1052,
        "grad_norm": 7.150775909423828,
        "learning_rate": 0.00019998794989573498,
        "epoch": 1.3382666666666667,
        "step": 10037
    },
    {
        "loss": 1.4264,
        "grad_norm": 3.591071844100952,
        "learning_rate": 0.00019998695307715746,
        "epoch": 1.3384,
        "step": 10038
    },
    {
        "loss": 0.6168,
        "grad_norm": 3.205371379852295,
        "learning_rate": 0.00019998591665340717,
        "epoch": 1.3385333333333334,
        "step": 10039
    },
    {
        "loss": 2.0081,
        "grad_norm": 2.468745708465576,
        "learning_rate": 0.00019998484062489456,
        "epoch": 1.3386666666666667,
        "step": 10040
    },
    {
        "loss": 1.314,
        "grad_norm": 4.325215816497803,
        "learning_rate": 0.0001999837249920459,
        "epoch": 1.3388,
        "step": 10041
    },
    {
        "loss": 2.0496,
        "grad_norm": 4.632659912109375,
        "learning_rate": 0.0001999825697553031,
        "epoch": 1.3389333333333333,
        "step": 10042
    },
    {
        "loss": 1.9895,
        "grad_norm": 3.6390442848205566,
        "learning_rate": 0.00019998137491512372,
        "epoch": 1.3390666666666666,
        "step": 10043
    },
    {
        "loss": 1.3317,
        "grad_norm": 3.923194646835327,
        "learning_rate": 0.00019998014047198107,
        "epoch": 1.3392,
        "step": 10044
    },
    {
        "loss": 2.3263,
        "grad_norm": 2.9622445106506348,
        "learning_rate": 0.00019997886642636407,
        "epoch": 1.3393333333333333,
        "step": 10045
    },
    {
        "loss": 1.9453,
        "grad_norm": 3.4821910858154297,
        "learning_rate": 0.00019997755277877745,
        "epoch": 1.3394666666666666,
        "step": 10046
    },
    {
        "loss": 2.7226,
        "grad_norm": 3.3320271968841553,
        "learning_rate": 0.0001999761995297415,
        "epoch": 1.3396,
        "step": 10047
    },
    {
        "loss": 2.5085,
        "grad_norm": 3.515327215194702,
        "learning_rate": 0.00019997480667979225,
        "epoch": 1.3397333333333332,
        "step": 10048
    },
    {
        "loss": 2.7973,
        "grad_norm": 3.456136703491211,
        "learning_rate": 0.0001999733742294814,
        "epoch": 1.3398666666666665,
        "step": 10049
    },
    {
        "loss": 1.8081,
        "grad_norm": 3.6749167442321777,
        "learning_rate": 0.0001999719021793764,
        "epoch": 1.34,
        "step": 10050
    },
    {
        "loss": 2.3307,
        "grad_norm": 3.918726921081543,
        "learning_rate": 0.0001999703905300603,
        "epoch": 1.3401333333333334,
        "step": 10051
    },
    {
        "loss": 2.1704,
        "grad_norm": 2.3699212074279785,
        "learning_rate": 0.00019996883928213185,
        "epoch": 1.3402666666666667,
        "step": 10052
    },
    {
        "loss": 1.975,
        "grad_norm": 3.3175814151763916,
        "learning_rate": 0.00019996724843620553,
        "epoch": 1.3404,
        "step": 10053
    },
    {
        "loss": 1.7161,
        "grad_norm": 3.52494478225708,
        "learning_rate": 0.00019996561799291148,
        "epoch": 1.3405333333333334,
        "step": 10054
    },
    {
        "loss": 1.6553,
        "grad_norm": 4.1723246574401855,
        "learning_rate": 0.00019996394795289554,
        "epoch": 1.3406666666666667,
        "step": 10055
    },
    {
        "loss": 2.519,
        "grad_norm": 3.6687841415405273,
        "learning_rate": 0.00019996223831681914,
        "epoch": 1.3408,
        "step": 10056
    },
    {
        "loss": 2.9151,
        "grad_norm": 2.3418447971343994,
        "learning_rate": 0.0001999604890853596,
        "epoch": 1.3409333333333333,
        "step": 10057
    },
    {
        "loss": 1.8331,
        "grad_norm": 2.7711730003356934,
        "learning_rate": 0.0001999587002592097,
        "epoch": 1.3410666666666666,
        "step": 10058
    },
    {
        "loss": 0.934,
        "grad_norm": 4.596682548522949,
        "learning_rate": 0.00019995687183907798,
        "epoch": 1.3412,
        "step": 10059
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.709439516067505,
        "learning_rate": 0.00019995500382568877,
        "epoch": 1.3413333333333333,
        "step": 10060
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.8306751251220703,
        "learning_rate": 0.00019995309621978195,
        "epoch": 1.3414666666666666,
        "step": 10061
    },
    {
        "loss": 1.3935,
        "grad_norm": 4.072604656219482,
        "learning_rate": 0.00019995114902211314,
        "epoch": 1.3416000000000001,
        "step": 10062
    },
    {
        "loss": 2.284,
        "grad_norm": 3.666475296020508,
        "learning_rate": 0.00019994916223345363,
        "epoch": 1.3417333333333334,
        "step": 10063
    },
    {
        "loss": 2.3142,
        "grad_norm": 2.3849663734436035,
        "learning_rate": 0.0001999471358545904,
        "epoch": 1.3418666666666668,
        "step": 10064
    },
    {
        "loss": 2.3802,
        "grad_norm": 3.1295053958892822,
        "learning_rate": 0.00019994506988632607,
        "epoch": 1.342,
        "step": 10065
    },
    {
        "loss": 2.3672,
        "grad_norm": 4.2063307762146,
        "learning_rate": 0.000199942964329479,
        "epoch": 1.3421333333333334,
        "step": 10066
    },
    {
        "loss": 2.3066,
        "grad_norm": 3.6023082733154297,
        "learning_rate": 0.00019994081918488324,
        "epoch": 1.3422666666666667,
        "step": 10067
    },
    {
        "loss": 2.0622,
        "grad_norm": 4.05867338180542,
        "learning_rate": 0.00019993863445338846,
        "epoch": 1.3424,
        "step": 10068
    },
    {
        "loss": 1.8461,
        "grad_norm": 2.607597589492798,
        "learning_rate": 0.00019993641013586,
        "epoch": 1.3425333333333334,
        "step": 10069
    },
    {
        "loss": 1.3506,
        "grad_norm": 6.341832160949707,
        "learning_rate": 0.00019993414623317902,
        "epoch": 1.3426666666666667,
        "step": 10070
    },
    {
        "loss": 2.0915,
        "grad_norm": 2.8298118114471436,
        "learning_rate": 0.00019993184274624216,
        "epoch": 1.3428,
        "step": 10071
    },
    {
        "loss": 2.4768,
        "grad_norm": 2.9456491470336914,
        "learning_rate": 0.00019992949967596188,
        "epoch": 1.3429333333333333,
        "step": 10072
    },
    {
        "loss": 2.2228,
        "grad_norm": 3.4194607734680176,
        "learning_rate": 0.00019992711702326627,
        "epoch": 1.3430666666666666,
        "step": 10073
    },
    {
        "loss": 2.8086,
        "grad_norm": 3.010169267654419,
        "learning_rate": 0.00019992469478909914,
        "epoch": 1.3432,
        "step": 10074
    },
    {
        "loss": 1.1344,
        "grad_norm": 5.307085990905762,
        "learning_rate": 0.0001999222329744199,
        "epoch": 1.3433333333333333,
        "step": 10075
    },
    {
        "loss": 2.0911,
        "grad_norm": 4.821370601654053,
        "learning_rate": 0.0001999197315802037,
        "epoch": 1.3434666666666666,
        "step": 10076
    },
    {
        "loss": 1.4974,
        "grad_norm": 4.5602126121521,
        "learning_rate": 0.00019991719060744137,
        "epoch": 1.3436,
        "step": 10077
    },
    {
        "loss": 2.2156,
        "grad_norm": 2.740669012069702,
        "learning_rate": 0.0001999146100571394,
        "epoch": 1.3437333333333332,
        "step": 10078
    },
    {
        "loss": 3.2165,
        "grad_norm": 3.7859697341918945,
        "learning_rate": 0.0001999119899303199,
        "epoch": 1.3438666666666665,
        "step": 10079
    },
    {
        "loss": 2.0059,
        "grad_norm": 3.755039691925049,
        "learning_rate": 0.00019990933022802075,
        "epoch": 1.3439999999999999,
        "step": 10080
    },
    {
        "loss": 2.1212,
        "grad_norm": 4.967247486114502,
        "learning_rate": 0.00019990663095129544,
        "epoch": 1.3441333333333334,
        "step": 10081
    },
    {
        "loss": 1.7428,
        "grad_norm": 3.4386308193206787,
        "learning_rate": 0.00019990389210121324,
        "epoch": 1.3442666666666667,
        "step": 10082
    },
    {
        "loss": 2.4603,
        "grad_norm": 3.5224733352661133,
        "learning_rate": 0.00019990111367885894,
        "epoch": 1.3444,
        "step": 10083
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.7234716415405273,
        "learning_rate": 0.00019989829568533313,
        "epoch": 1.3445333333333334,
        "step": 10084
    },
    {
        "loss": 2.4357,
        "grad_norm": 4.755870342254639,
        "learning_rate": 0.00019989543812175197,
        "epoch": 1.3446666666666667,
        "step": 10085
    },
    {
        "loss": 2.1325,
        "grad_norm": 3.8872196674346924,
        "learning_rate": 0.0001998925409892474,
        "epoch": 1.3448,
        "step": 10086
    },
    {
        "loss": 0.9844,
        "grad_norm": 4.180300712585449,
        "learning_rate": 0.00019988960428896695,
        "epoch": 1.3449333333333333,
        "step": 10087
    },
    {
        "loss": 2.1138,
        "grad_norm": 2.7797060012817383,
        "learning_rate": 0.00019988662802207388,
        "epoch": 1.3450666666666666,
        "step": 10088
    },
    {
        "loss": 1.565,
        "grad_norm": 3.0905117988586426,
        "learning_rate": 0.00019988361218974712,
        "epoch": 1.3452,
        "step": 10089
    },
    {
        "loss": 2.0543,
        "grad_norm": 2.8730459213256836,
        "learning_rate": 0.0001998805567931812,
        "epoch": 1.3453333333333333,
        "step": 10090
    },
    {
        "loss": 1.5011,
        "grad_norm": 4.655550956726074,
        "learning_rate": 0.00019987746183358643,
        "epoch": 1.3454666666666666,
        "step": 10091
    },
    {
        "loss": 2.4004,
        "grad_norm": 4.597330570220947,
        "learning_rate": 0.00019987432731218867,
        "epoch": 1.3456000000000001,
        "step": 10092
    },
    {
        "loss": 2.3731,
        "grad_norm": 3.6075706481933594,
        "learning_rate": 0.00019987115323022957,
        "epoch": 1.3457333333333334,
        "step": 10093
    },
    {
        "loss": 2.1585,
        "grad_norm": 2.5820014476776123,
        "learning_rate": 0.00019986793958896637,
        "epoch": 1.3458666666666668,
        "step": 10094
    },
    {
        "loss": 2.4003,
        "grad_norm": 3.6508800983428955,
        "learning_rate": 0.000199864686389672,
        "epoch": 1.346,
        "step": 10095
    },
    {
        "loss": 2.139,
        "grad_norm": 5.5795817375183105,
        "learning_rate": 0.0001998613936336351,
        "epoch": 1.3461333333333334,
        "step": 10096
    },
    {
        "loss": 1.2663,
        "grad_norm": 4.507910251617432,
        "learning_rate": 0.00019985806132215986,
        "epoch": 1.3462666666666667,
        "step": 10097
    },
    {
        "loss": 2.3036,
        "grad_norm": 4.045675754547119,
        "learning_rate": 0.00019985468945656632,
        "epoch": 1.3464,
        "step": 10098
    },
    {
        "loss": 1.9077,
        "grad_norm": 2.6778223514556885,
        "learning_rate": 0.00019985127803819,
        "epoch": 1.3465333333333334,
        "step": 10099
    },
    {
        "loss": 0.9257,
        "grad_norm": 4.179795265197754,
        "learning_rate": 0.00019984782706838222,
        "epoch": 1.3466666666666667,
        "step": 10100
    },
    {
        "loss": 2.6832,
        "grad_norm": 3.3885035514831543,
        "learning_rate": 0.00019984433654850995,
        "epoch": 1.3468,
        "step": 10101
    },
    {
        "loss": 1.8636,
        "grad_norm": 3.5428199768066406,
        "learning_rate": 0.00019984080647995575,
        "epoch": 1.3469333333333333,
        "step": 10102
    },
    {
        "loss": 2.4049,
        "grad_norm": 3.010495185852051,
        "learning_rate": 0.00019983723686411788,
        "epoch": 1.3470666666666666,
        "step": 10103
    },
    {
        "loss": 1.1153,
        "grad_norm": 5.778826713562012,
        "learning_rate": 0.00019983362770241032,
        "epoch": 1.3472,
        "step": 10104
    },
    {
        "loss": 2.4514,
        "grad_norm": 3.407670021057129,
        "learning_rate": 0.00019982997899626264,
        "epoch": 1.3473333333333333,
        "step": 10105
    },
    {
        "loss": 1.8206,
        "grad_norm": 4.754961967468262,
        "learning_rate": 0.00019982629074712013,
        "epoch": 1.3474666666666666,
        "step": 10106
    },
    {
        "loss": 1.2235,
        "grad_norm": 2.670194387435913,
        "learning_rate": 0.0001998225629564437,
        "epoch": 1.3476,
        "step": 10107
    },
    {
        "loss": 2.2459,
        "grad_norm": 3.070919990539551,
        "learning_rate": 0.00019981879562570994,
        "epoch": 1.3477333333333332,
        "step": 10108
    },
    {
        "loss": 1.4491,
        "grad_norm": 3.0614445209503174,
        "learning_rate": 0.00019981498875641112,
        "epoch": 1.3478666666666665,
        "step": 10109
    },
    {
        "loss": 2.6417,
        "grad_norm": 7.2020392417907715,
        "learning_rate": 0.00019981114235005513,
        "epoch": 1.3479999999999999,
        "step": 10110
    },
    {
        "loss": 2.1023,
        "grad_norm": 3.312652349472046,
        "learning_rate": 0.00019980725640816557,
        "epoch": 1.3481333333333334,
        "step": 10111
    },
    {
        "loss": 1.9689,
        "grad_norm": 4.041101455688477,
        "learning_rate": 0.00019980333093228166,
        "epoch": 1.3482666666666667,
        "step": 10112
    },
    {
        "loss": 2.3159,
        "grad_norm": 3.8739912509918213,
        "learning_rate": 0.00019979936592395828,
        "epoch": 1.3484,
        "step": 10113
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.4371087551116943,
        "learning_rate": 0.00019979536138476603,
        "epoch": 1.3485333333333334,
        "step": 10114
    },
    {
        "loss": 2.1436,
        "grad_norm": 4.63742208480835,
        "learning_rate": 0.00019979131731629105,
        "epoch": 1.3486666666666667,
        "step": 10115
    },
    {
        "loss": 1.8918,
        "grad_norm": 2.898517608642578,
        "learning_rate": 0.00019978723372013528,
        "epoch": 1.3488,
        "step": 10116
    },
    {
        "loss": 0.9917,
        "grad_norm": 4.101002216339111,
        "learning_rate": 0.00019978311059791622,
        "epoch": 1.3489333333333333,
        "step": 10117
    },
    {
        "loss": 2.0681,
        "grad_norm": 3.1499242782592773,
        "learning_rate": 0.00019977894795126703,
        "epoch": 1.3490666666666666,
        "step": 10118
    },
    {
        "loss": 2.6206,
        "grad_norm": 3.8696491718292236,
        "learning_rate": 0.0001997747457818366,
        "epoch": 1.3492,
        "step": 10119
    },
    {
        "loss": 2.1541,
        "grad_norm": 5.845138072967529,
        "learning_rate": 0.00019977050409128935,
        "epoch": 1.3493333333333333,
        "step": 10120
    },
    {
        "loss": 2.7018,
        "grad_norm": 4.7777862548828125,
        "learning_rate": 0.0001997662228813055,
        "epoch": 1.3494666666666666,
        "step": 10121
    },
    {
        "loss": 1.998,
        "grad_norm": 4.264904975891113,
        "learning_rate": 0.0001997619021535808,
        "epoch": 1.3496000000000001,
        "step": 10122
    },
    {
        "loss": 2.0886,
        "grad_norm": 2.8356704711914062,
        "learning_rate": 0.00019975754190982675,
        "epoch": 1.3497333333333335,
        "step": 10123
    },
    {
        "loss": 1.8736,
        "grad_norm": 2.0634679794311523,
        "learning_rate": 0.0001997531421517704,
        "epoch": 1.3498666666666668,
        "step": 10124
    },
    {
        "loss": 1.5837,
        "grad_norm": 3.307461738586426,
        "learning_rate": 0.00019974870288115456,
        "epoch": 1.35,
        "step": 10125
    },
    {
        "loss": 1.2563,
        "grad_norm": 4.741753101348877,
        "learning_rate": 0.00019974422409973762,
        "epoch": 1.3501333333333334,
        "step": 10126
    },
    {
        "loss": 2.6541,
        "grad_norm": 2.585078716278076,
        "learning_rate": 0.00019973970580929365,
        "epoch": 1.3502666666666667,
        "step": 10127
    },
    {
        "loss": 1.96,
        "grad_norm": 5.870701313018799,
        "learning_rate": 0.00019973514801161236,
        "epoch": 1.3504,
        "step": 10128
    },
    {
        "loss": 2.2167,
        "grad_norm": 3.812385320663452,
        "learning_rate": 0.00019973055070849913,
        "epoch": 1.3505333333333334,
        "step": 10129
    },
    {
        "loss": 1.9505,
        "grad_norm": 4.568587303161621,
        "learning_rate": 0.00019972591390177488,
        "epoch": 1.3506666666666667,
        "step": 10130
    },
    {
        "loss": 2.5735,
        "grad_norm": 3.3759357929229736,
        "learning_rate": 0.00019972123759327636,
        "epoch": 1.3508,
        "step": 10131
    },
    {
        "loss": 2.4376,
        "grad_norm": 4.030972480773926,
        "learning_rate": 0.00019971652178485584,
        "epoch": 1.3509333333333333,
        "step": 10132
    },
    {
        "loss": 1.2653,
        "grad_norm": 3.3286616802215576,
        "learning_rate": 0.0001997117664783813,
        "epoch": 1.3510666666666666,
        "step": 10133
    },
    {
        "loss": 2.4002,
        "grad_norm": 2.700195789337158,
        "learning_rate": 0.00019970697167573623,
        "epoch": 1.3512,
        "step": 10134
    },
    {
        "loss": 1.7774,
        "grad_norm": 2.251042366027832,
        "learning_rate": 0.00019970213737881994,
        "epoch": 1.3513333333333333,
        "step": 10135
    },
    {
        "loss": 2.2684,
        "grad_norm": 3.136944055557251,
        "learning_rate": 0.00019969726358954733,
        "epoch": 1.3514666666666666,
        "step": 10136
    },
    {
        "loss": 2.8315,
        "grad_norm": 4.336969375610352,
        "learning_rate": 0.00019969235030984893,
        "epoch": 1.3516,
        "step": 10137
    },
    {
        "loss": 1.6591,
        "grad_norm": 2.8395135402679443,
        "learning_rate": 0.00019968739754167082,
        "epoch": 1.3517333333333332,
        "step": 10138
    },
    {
        "loss": 2.1469,
        "grad_norm": 2.9246435165405273,
        "learning_rate": 0.00019968240528697492,
        "epoch": 1.3518666666666665,
        "step": 10139
    },
    {
        "loss": 1.8768,
        "grad_norm": 2.9594950675964355,
        "learning_rate": 0.00019967737354773858,
        "epoch": 1.3519999999999999,
        "step": 10140
    },
    {
        "loss": 1.2858,
        "grad_norm": 4.891892433166504,
        "learning_rate": 0.00019967230232595498,
        "epoch": 1.3521333333333334,
        "step": 10141
    },
    {
        "loss": 2.3444,
        "grad_norm": 3.0402727127075195,
        "learning_rate": 0.00019966719162363278,
        "epoch": 1.3522666666666667,
        "step": 10142
    },
    {
        "loss": 2.1554,
        "grad_norm": 3.3091018199920654,
        "learning_rate": 0.00019966204144279638,
        "epoch": 1.3524,
        "step": 10143
    },
    {
        "loss": 2.5857,
        "grad_norm": 3.426882028579712,
        "learning_rate": 0.00019965685178548577,
        "epoch": 1.3525333333333334,
        "step": 10144
    },
    {
        "loss": 2.1162,
        "grad_norm": 4.149169445037842,
        "learning_rate": 0.0001996516226537566,
        "epoch": 1.3526666666666667,
        "step": 10145
    },
    {
        "loss": 2.4668,
        "grad_norm": 2.613595962524414,
        "learning_rate": 0.00019964635404968016,
        "epoch": 1.3528,
        "step": 10146
    },
    {
        "loss": 2.433,
        "grad_norm": 2.7737932205200195,
        "learning_rate": 0.0001996410459753433,
        "epoch": 1.3529333333333333,
        "step": 10147
    },
    {
        "loss": 1.1381,
        "grad_norm": 3.200742244720459,
        "learning_rate": 0.00019963569843284862,
        "epoch": 1.3530666666666666,
        "step": 10148
    },
    {
        "loss": 2.2323,
        "grad_norm": 3.5204036235809326,
        "learning_rate": 0.00019963031142431433,
        "epoch": 1.3532,
        "step": 10149
    },
    {
        "loss": 2.6885,
        "grad_norm": 3.4786527156829834,
        "learning_rate": 0.00019962488495187418,
        "epoch": 1.3533333333333333,
        "step": 10150
    },
    {
        "loss": 1.1603,
        "grad_norm": 4.158075332641602,
        "learning_rate": 0.00019961941901767763,
        "epoch": 1.3534666666666666,
        "step": 10151
    },
    {
        "loss": 2.1884,
        "grad_norm": 2.9729998111724854,
        "learning_rate": 0.00019961391362388975,
        "epoch": 1.3536000000000001,
        "step": 10152
    },
    {
        "loss": 1.7389,
        "grad_norm": 3.489103078842163,
        "learning_rate": 0.00019960836877269127,
        "epoch": 1.3537333333333335,
        "step": 10153
    },
    {
        "loss": 2.3104,
        "grad_norm": 3.8362622261047363,
        "learning_rate": 0.0001996027844662785,
        "epoch": 1.3538666666666668,
        "step": 10154
    },
    {
        "loss": 2.6517,
        "grad_norm": 3.1202988624572754,
        "learning_rate": 0.00019959716070686346,
        "epoch": 1.354,
        "step": 10155
    },
    {
        "loss": 2.6989,
        "grad_norm": 3.988572597503662,
        "learning_rate": 0.00019959149749667363,
        "epoch": 1.3541333333333334,
        "step": 10156
    },
    {
        "loss": 2.7713,
        "grad_norm": 3.7481861114501953,
        "learning_rate": 0.00019958579483795233,
        "epoch": 1.3542666666666667,
        "step": 10157
    },
    {
        "loss": 1.8082,
        "grad_norm": 6.238361835479736,
        "learning_rate": 0.00019958005273295836,
        "epoch": 1.3544,
        "step": 10158
    },
    {
        "loss": 2.4541,
        "grad_norm": 4.005072593688965,
        "learning_rate": 0.00019957427118396618,
        "epoch": 1.3545333333333334,
        "step": 10159
    },
    {
        "loss": 1.077,
        "grad_norm": 3.920938491821289,
        "learning_rate": 0.00019956845019326592,
        "epoch": 1.3546666666666667,
        "step": 10160
    },
    {
        "loss": 1.5833,
        "grad_norm": 3.9728622436523438,
        "learning_rate": 0.00019956258976316327,
        "epoch": 1.3548,
        "step": 10161
    },
    {
        "loss": 2.4185,
        "grad_norm": 3.2697103023529053,
        "learning_rate": 0.00019955668989597952,
        "epoch": 1.3549333333333333,
        "step": 10162
    },
    {
        "loss": 2.405,
        "grad_norm": 3.6981241703033447,
        "learning_rate": 0.0001995507505940517,
        "epoch": 1.3550666666666666,
        "step": 10163
    },
    {
        "loss": 2.3211,
        "grad_norm": 4.991148948669434,
        "learning_rate": 0.00019954477185973236,
        "epoch": 1.3552,
        "step": 10164
    },
    {
        "loss": 1.9787,
        "grad_norm": 4.641674995422363,
        "learning_rate": 0.0001995387536953897,
        "epoch": 1.3553333333333333,
        "step": 10165
    },
    {
        "loss": 2.3937,
        "grad_norm": 3.6538267135620117,
        "learning_rate": 0.00019953269610340751,
        "epoch": 1.3554666666666666,
        "step": 10166
    },
    {
        "loss": 2.2476,
        "grad_norm": 2.649353504180908,
        "learning_rate": 0.00019952659908618528,
        "epoch": 1.3556,
        "step": 10167
    },
    {
        "loss": 2.17,
        "grad_norm": 3.0348403453826904,
        "learning_rate": 0.000199520462646138,
        "epoch": 1.3557333333333332,
        "step": 10168
    },
    {
        "loss": 1.6953,
        "grad_norm": 4.932246208190918,
        "learning_rate": 0.00019951428678569635,
        "epoch": 1.3558666666666666,
        "step": 10169
    },
    {
        "loss": 2.2221,
        "grad_norm": 3.374121904373169,
        "learning_rate": 0.00019950807150730663,
        "epoch": 1.3559999999999999,
        "step": 10170
    },
    {
        "loss": 2.8938,
        "grad_norm": 4.490250110626221,
        "learning_rate": 0.0001995018168134307,
        "epoch": 1.3561333333333334,
        "step": 10171
    },
    {
        "loss": 2.3355,
        "grad_norm": 3.2823266983032227,
        "learning_rate": 0.00019949552270654612,
        "epoch": 1.3562666666666667,
        "step": 10172
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.769134283065796,
        "learning_rate": 0.00019948918918914594,
        "epoch": 1.3564,
        "step": 10173
    },
    {
        "loss": 2.5235,
        "grad_norm": 3.2038497924804688,
        "learning_rate": 0.00019948281626373896,
        "epoch": 1.3565333333333334,
        "step": 10174
    },
    {
        "loss": 2.3587,
        "grad_norm": 2.7147765159606934,
        "learning_rate": 0.00019947640393284943,
        "epoch": 1.3566666666666667,
        "step": 10175
    },
    {
        "loss": 2.3239,
        "grad_norm": 4.484771728515625,
        "learning_rate": 0.00019946995219901739,
        "epoch": 1.3568,
        "step": 10176
    },
    {
        "loss": 2.1313,
        "grad_norm": 2.4367687702178955,
        "learning_rate": 0.0001994634610647983,
        "epoch": 1.3569333333333333,
        "step": 10177
    },
    {
        "loss": 2.3725,
        "grad_norm": 2.7693862915039062,
        "learning_rate": 0.00019945693053276343,
        "epoch": 1.3570666666666666,
        "step": 10178
    },
    {
        "loss": 1.1654,
        "grad_norm": 4.859179496765137,
        "learning_rate": 0.00019945036060549945,
        "epoch": 1.3572,
        "step": 10179
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.994384527206421,
        "learning_rate": 0.00019944375128560875,
        "epoch": 1.3573333333333333,
        "step": 10180
    },
    {
        "loss": 2.1493,
        "grad_norm": 3.283304452896118,
        "learning_rate": 0.00019943710257570933,
        "epoch": 1.3574666666666666,
        "step": 10181
    },
    {
        "loss": 0.6148,
        "grad_norm": 2.699228048324585,
        "learning_rate": 0.0001994304144784348,
        "epoch": 1.3576,
        "step": 10182
    },
    {
        "loss": 2.5773,
        "grad_norm": 4.893128871917725,
        "learning_rate": 0.00019942368699643425,
        "epoch": 1.3577333333333335,
        "step": 10183
    },
    {
        "loss": 2.1158,
        "grad_norm": 3.292813301086426,
        "learning_rate": 0.00019941692013237248,
        "epoch": 1.3578666666666668,
        "step": 10184
    },
    {
        "loss": 1.4482,
        "grad_norm": 3.234656810760498,
        "learning_rate": 0.00019941011388892993,
        "epoch": 1.358,
        "step": 10185
    },
    {
        "loss": 2.617,
        "grad_norm": 4.134181976318359,
        "learning_rate": 0.0001994032682688025,
        "epoch": 1.3581333333333334,
        "step": 10186
    },
    {
        "loss": 2.7472,
        "grad_norm": 2.3020732402801514,
        "learning_rate": 0.00019939638327470187,
        "epoch": 1.3582666666666667,
        "step": 10187
    },
    {
        "loss": 2.1741,
        "grad_norm": 3.6827666759490967,
        "learning_rate": 0.0001993894589093551,
        "epoch": 1.3584,
        "step": 10188
    },
    {
        "loss": 1.8691,
        "grad_norm": 3.9619297981262207,
        "learning_rate": 0.00019938249517550497,
        "epoch": 1.3585333333333334,
        "step": 10189
    },
    {
        "loss": 1.4225,
        "grad_norm": 3.53948974609375,
        "learning_rate": 0.0001993754920759099,
        "epoch": 1.3586666666666667,
        "step": 10190
    },
    {
        "loss": 2.4358,
        "grad_norm": 3.5166842937469482,
        "learning_rate": 0.0001993684496133438,
        "epoch": 1.3588,
        "step": 10191
    },
    {
        "loss": 2.6706,
        "grad_norm": 2.9414734840393066,
        "learning_rate": 0.0001993613677905962,
        "epoch": 1.3589333333333333,
        "step": 10192
    },
    {
        "loss": 1.1172,
        "grad_norm": 6.078719139099121,
        "learning_rate": 0.00019935424661047225,
        "epoch": 1.3590666666666666,
        "step": 10193
    },
    {
        "loss": 2.1753,
        "grad_norm": 3.404559373855591,
        "learning_rate": 0.0001993470860757927,
        "epoch": 1.3592,
        "step": 10194
    },
    {
        "loss": 2.4581,
        "grad_norm": 3.613323926925659,
        "learning_rate": 0.00019933988618939382,
        "epoch": 1.3593333333333333,
        "step": 10195
    },
    {
        "loss": 2.7065,
        "grad_norm": 3.937426805496216,
        "learning_rate": 0.00019933264695412754,
        "epoch": 1.3594666666666666,
        "step": 10196
    },
    {
        "loss": 1.4546,
        "grad_norm": 3.4266045093536377,
        "learning_rate": 0.00019932536837286133,
        "epoch": 1.3596,
        "step": 10197
    },
    {
        "loss": 1.4172,
        "grad_norm": 3.6517629623413086,
        "learning_rate": 0.00019931805044847827,
        "epoch": 1.3597333333333332,
        "step": 10198
    },
    {
        "loss": 1.7305,
        "grad_norm": 4.077159404754639,
        "learning_rate": 0.00019931069318387699,
        "epoch": 1.3598666666666666,
        "step": 10199
    },
    {
        "loss": 2.1855,
        "grad_norm": 3.3984529972076416,
        "learning_rate": 0.00019930329658197175,
        "epoch": 1.3599999999999999,
        "step": 10200
    },
    {
        "loss": 1.4996,
        "grad_norm": 4.977419376373291,
        "learning_rate": 0.0001992958606456924,
        "epoch": 1.3601333333333334,
        "step": 10201
    },
    {
        "loss": 2.5512,
        "grad_norm": 2.8696603775024414,
        "learning_rate": 0.00019928838537798426,
        "epoch": 1.3602666666666667,
        "step": 10202
    },
    {
        "loss": 1.9177,
        "grad_norm": 2.823880672454834,
        "learning_rate": 0.0001992808707818084,
        "epoch": 1.3604,
        "step": 10203
    },
    {
        "loss": 1.9833,
        "grad_norm": 2.6738226413726807,
        "learning_rate": 0.0001992733168601413,
        "epoch": 1.3605333333333334,
        "step": 10204
    },
    {
        "loss": 1.8747,
        "grad_norm": 4.696424961090088,
        "learning_rate": 0.00019926572361597512,
        "epoch": 1.3606666666666667,
        "step": 10205
    },
    {
        "loss": 1.796,
        "grad_norm": 4.84770393371582,
        "learning_rate": 0.0001992580910523176,
        "epoch": 1.3608,
        "step": 10206
    },
    {
        "loss": 1.8413,
        "grad_norm": 3.1778039932250977,
        "learning_rate": 0.00019925041917219198,
        "epoch": 1.3609333333333333,
        "step": 10207
    },
    {
        "loss": 2.7323,
        "grad_norm": 4.966287612915039,
        "learning_rate": 0.00019924270797863714,
        "epoch": 1.3610666666666666,
        "step": 10208
    },
    {
        "loss": 2.2905,
        "grad_norm": 3.614962577819824,
        "learning_rate": 0.00019923495747470755,
        "epoch": 1.3612,
        "step": 10209
    },
    {
        "loss": 2.226,
        "grad_norm": 4.694832801818848,
        "learning_rate": 0.00019922716766347315,
        "epoch": 1.3613333333333333,
        "step": 10210
    },
    {
        "loss": 2.2758,
        "grad_norm": 4.587307929992676,
        "learning_rate": 0.0001992193385480195,
        "epoch": 1.3614666666666666,
        "step": 10211
    },
    {
        "loss": 2.5255,
        "grad_norm": 2.510565996170044,
        "learning_rate": 0.0001992114701314478,
        "epoch": 1.3616,
        "step": 10212
    },
    {
        "loss": 1.5944,
        "grad_norm": 3.0146398544311523,
        "learning_rate": 0.0001992035624168747,
        "epoch": 1.3617333333333335,
        "step": 10213
    },
    {
        "loss": 1.5714,
        "grad_norm": 4.132260322570801,
        "learning_rate": 0.00019919561540743255,
        "epoch": 1.3618666666666668,
        "step": 10214
    },
    {
        "loss": 2.6548,
        "grad_norm": 3.3709497451782227,
        "learning_rate": 0.00019918762910626906,
        "epoch": 1.362,
        "step": 10215
    },
    {
        "loss": 2.7493,
        "grad_norm": 3.238865852355957,
        "learning_rate": 0.00019917960351654773,
        "epoch": 1.3621333333333334,
        "step": 10216
    },
    {
        "loss": 2.4213,
        "grad_norm": 2.787752628326416,
        "learning_rate": 0.00019917153864144755,
        "epoch": 1.3622666666666667,
        "step": 10217
    },
    {
        "loss": 2.4639,
        "grad_norm": 3.408946990966797,
        "learning_rate": 0.00019916343448416295,
        "epoch": 1.3624,
        "step": 10218
    },
    {
        "loss": 2.0992,
        "grad_norm": 4.129779815673828,
        "learning_rate": 0.00019915529104790408,
        "epoch": 1.3625333333333334,
        "step": 10219
    },
    {
        "loss": 2.7973,
        "grad_norm": 3.2957561016082764,
        "learning_rate": 0.00019914710833589654,
        "epoch": 1.3626666666666667,
        "step": 10220
    },
    {
        "loss": 2.0011,
        "grad_norm": 3.4891586303710938,
        "learning_rate": 0.00019913888635138158,
        "epoch": 1.3628,
        "step": 10221
    },
    {
        "loss": 1.4941,
        "grad_norm": 4.090476036071777,
        "learning_rate": 0.00019913062509761592,
        "epoch": 1.3629333333333333,
        "step": 10222
    },
    {
        "loss": 2.0725,
        "grad_norm": 3.086059331893921,
        "learning_rate": 0.0001991223245778719,
        "epoch": 1.3630666666666666,
        "step": 10223
    },
    {
        "loss": 1.8494,
        "grad_norm": 5.206122398376465,
        "learning_rate": 0.0001991139847954373,
        "epoch": 1.3632,
        "step": 10224
    },
    {
        "loss": 1.5291,
        "grad_norm": 2.623542308807373,
        "learning_rate": 0.00019910560575361566,
        "epoch": 1.3633333333333333,
        "step": 10225
    },
    {
        "loss": 1.9741,
        "grad_norm": 2.9532968997955322,
        "learning_rate": 0.00019909718745572588,
        "epoch": 1.3634666666666666,
        "step": 10226
    },
    {
        "loss": 1.6082,
        "grad_norm": 4.651123046875,
        "learning_rate": 0.00019908872990510244,
        "epoch": 1.3636,
        "step": 10227
    },
    {
        "loss": 2.6415,
        "grad_norm": 2.994680166244507,
        "learning_rate": 0.0001990802331050955,
        "epoch": 1.3637333333333332,
        "step": 10228
    },
    {
        "loss": 1.8565,
        "grad_norm": 3.2441272735595703,
        "learning_rate": 0.00019907169705907056,
        "epoch": 1.3638666666666666,
        "step": 10229
    },
    {
        "loss": 1.5612,
        "grad_norm": 6.850859642028809,
        "learning_rate": 0.0001990631217704089,
        "epoch": 1.3639999999999999,
        "step": 10230
    },
    {
        "loss": 2.2852,
        "grad_norm": 4.985398292541504,
        "learning_rate": 0.00019905450724250713,
        "epoch": 1.3641333333333332,
        "step": 10231
    },
    {
        "loss": 2.7237,
        "grad_norm": 5.706811904907227,
        "learning_rate": 0.00019904585347877752,
        "epoch": 1.3642666666666667,
        "step": 10232
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.809950351715088,
        "learning_rate": 0.00019903716048264784,
        "epoch": 1.3644,
        "step": 10233
    },
    {
        "loss": 2.6606,
        "grad_norm": 5.089761734008789,
        "learning_rate": 0.00019902842825756146,
        "epoch": 1.3645333333333334,
        "step": 10234
    },
    {
        "loss": 1.3736,
        "grad_norm": 4.850235939025879,
        "learning_rate": 0.00019901965680697723,
        "epoch": 1.3646666666666667,
        "step": 10235
    },
    {
        "loss": 2.4112,
        "grad_norm": 5.708682537078857,
        "learning_rate": 0.0001990108461343695,
        "epoch": 1.3648,
        "step": 10236
    },
    {
        "loss": 2.0648,
        "grad_norm": 3.36376690864563,
        "learning_rate": 0.00019900199624322827,
        "epoch": 1.3649333333333333,
        "step": 10237
    },
    {
        "loss": 2.0341,
        "grad_norm": 2.4930410385131836,
        "learning_rate": 0.000198993107137059,
        "epoch": 1.3650666666666667,
        "step": 10238
    },
    {
        "loss": 1.806,
        "grad_norm": 4.920017242431641,
        "learning_rate": 0.00019898417881938262,
        "epoch": 1.3652,
        "step": 10239
    },
    {
        "loss": 2.2011,
        "grad_norm": 5.296244144439697,
        "learning_rate": 0.00019897521129373576,
        "epoch": 1.3653333333333333,
        "step": 10240
    },
    {
        "loss": 2.2557,
        "grad_norm": 5.652552127838135,
        "learning_rate": 0.00019896620456367047,
        "epoch": 1.3654666666666666,
        "step": 10241
    },
    {
        "loss": 1.3376,
        "grad_norm": 4.732697486877441,
        "learning_rate": 0.00019895715863275438,
        "epoch": 1.3656,
        "step": 10242
    },
    {
        "loss": 2.6081,
        "grad_norm": 2.3695766925811768,
        "learning_rate": 0.00019894807350457047,
        "epoch": 1.3657333333333335,
        "step": 10243
    },
    {
        "loss": 1.4874,
        "grad_norm": 2.3666579723358154,
        "learning_rate": 0.00019893894918271757,
        "epoch": 1.3658666666666668,
        "step": 10244
    },
    {
        "loss": 1.5594,
        "grad_norm": 4.747851848602295,
        "learning_rate": 0.00019892978567080972,
        "epoch": 1.366,
        "step": 10245
    },
    {
        "loss": 2.5537,
        "grad_norm": 4.039852142333984,
        "learning_rate": 0.0001989205829724767,
        "epoch": 1.3661333333333334,
        "step": 10246
    },
    {
        "loss": 2.0246,
        "grad_norm": 6.456639766693115,
        "learning_rate": 0.00019891134109136367,
        "epoch": 1.3662666666666667,
        "step": 10247
    },
    {
        "loss": 2.2894,
        "grad_norm": 5.9000678062438965,
        "learning_rate": 0.00019890206003113142,
        "epoch": 1.3664,
        "step": 10248
    },
    {
        "loss": 2.3976,
        "grad_norm": 2.759315252304077,
        "learning_rate": 0.00019889273979545617,
        "epoch": 1.3665333333333334,
        "step": 10249
    },
    {
        "loss": 0.5956,
        "grad_norm": 2.9453864097595215,
        "learning_rate": 0.00019888338038802976,
        "epoch": 1.3666666666666667,
        "step": 10250
    },
    {
        "loss": 1.8833,
        "grad_norm": 3.727323532104492,
        "learning_rate": 0.00019887398181255944,
        "epoch": 1.3668,
        "step": 10251
    },
    {
        "loss": 1.9026,
        "grad_norm": 3.8614912033081055,
        "learning_rate": 0.00019886454407276797,
        "epoch": 1.3669333333333333,
        "step": 10252
    },
    {
        "loss": 2.1902,
        "grad_norm": 3.168663740158081,
        "learning_rate": 0.00019885506717239374,
        "epoch": 1.3670666666666667,
        "step": 10253
    },
    {
        "loss": 2.8934,
        "grad_norm": 4.037447452545166,
        "learning_rate": 0.00019884555111519058,
        "epoch": 1.3672,
        "step": 10254
    },
    {
        "loss": 1.7676,
        "grad_norm": 4.275069236755371,
        "learning_rate": 0.00019883599590492782,
        "epoch": 1.3673333333333333,
        "step": 10255
    },
    {
        "loss": 1.943,
        "grad_norm": 3.2979185581207275,
        "learning_rate": 0.00019882640154539026,
        "epoch": 1.3674666666666666,
        "step": 10256
    },
    {
        "loss": 2.6561,
        "grad_norm": 5.655029773712158,
        "learning_rate": 0.00019881676804037835,
        "epoch": 1.3676,
        "step": 10257
    },
    {
        "loss": 2.0201,
        "grad_norm": 3.1808760166168213,
        "learning_rate": 0.00019880709539370792,
        "epoch": 1.3677333333333332,
        "step": 10258
    },
    {
        "loss": 1.6242,
        "grad_norm": 4.041362285614014,
        "learning_rate": 0.0001987973836092103,
        "epoch": 1.3678666666666666,
        "step": 10259
    },
    {
        "loss": 1.6648,
        "grad_norm": 4.558941841125488,
        "learning_rate": 0.0001987876326907324,
        "epoch": 1.3679999999999999,
        "step": 10260
    },
    {
        "loss": 1.9359,
        "grad_norm": 3.20112681388855,
        "learning_rate": 0.00019877784264213654,
        "epoch": 1.3681333333333332,
        "step": 10261
    },
    {
        "loss": 1.9709,
        "grad_norm": 3.5279741287231445,
        "learning_rate": 0.0001987680134673007,
        "epoch": 1.3682666666666667,
        "step": 10262
    },
    {
        "loss": 1.6667,
        "grad_norm": 4.190960884094238,
        "learning_rate": 0.00019875814517011814,
        "epoch": 1.3684,
        "step": 10263
    },
    {
        "loss": 2.7265,
        "grad_norm": 3.905785083770752,
        "learning_rate": 0.0001987482377544978,
        "epoch": 1.3685333333333334,
        "step": 10264
    },
    {
        "loss": 2.3307,
        "grad_norm": 2.5907697677612305,
        "learning_rate": 0.00019873829122436397,
        "epoch": 1.3686666666666667,
        "step": 10265
    },
    {
        "loss": 2.4624,
        "grad_norm": 3.545060634613037,
        "learning_rate": 0.00019872830558365654,
        "epoch": 1.3688,
        "step": 10266
    },
    {
        "loss": 2.3434,
        "grad_norm": 1.707632303237915,
        "learning_rate": 0.0001987182808363309,
        "epoch": 1.3689333333333333,
        "step": 10267
    },
    {
        "loss": 2.7237,
        "grad_norm": 3.160261631011963,
        "learning_rate": 0.00019870821698635783,
        "epoch": 1.3690666666666667,
        "step": 10268
    },
    {
        "loss": 2.0279,
        "grad_norm": 4.335415840148926,
        "learning_rate": 0.00019869811403772368,
        "epoch": 1.3692,
        "step": 10269
    },
    {
        "loss": 2.3379,
        "grad_norm": 3.2083966732025146,
        "learning_rate": 0.00019868797199443022,
        "epoch": 1.3693333333333333,
        "step": 10270
    },
    {
        "loss": 2.7097,
        "grad_norm": 3.365619659423828,
        "learning_rate": 0.00019867779086049482,
        "epoch": 1.3694666666666666,
        "step": 10271
    },
    {
        "loss": 2.5984,
        "grad_norm": 2.5581724643707275,
        "learning_rate": 0.0001986675706399502,
        "epoch": 1.3696,
        "step": 10272
    },
    {
        "loss": 2.5572,
        "grad_norm": 3.4006400108337402,
        "learning_rate": 0.00019865731133684466,
        "epoch": 1.3697333333333335,
        "step": 10273
    },
    {
        "loss": 2.5466,
        "grad_norm": 4.174454689025879,
        "learning_rate": 0.00019864701295524192,
        "epoch": 1.3698666666666668,
        "step": 10274
    },
    {
        "loss": 2.5533,
        "grad_norm": 4.252429485321045,
        "learning_rate": 0.0001986366754992212,
        "epoch": 1.37,
        "step": 10275
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.77905535697937,
        "learning_rate": 0.00019862629897287728,
        "epoch": 1.3701333333333334,
        "step": 10276
    },
    {
        "loss": 2.4363,
        "grad_norm": 3.334094285964966,
        "learning_rate": 0.00019861588338032022,
        "epoch": 1.3702666666666667,
        "step": 10277
    },
    {
        "loss": 2.3343,
        "grad_norm": 4.345553398132324,
        "learning_rate": 0.00019860542872567576,
        "epoch": 1.3704,
        "step": 10278
    },
    {
        "loss": 1.9815,
        "grad_norm": 3.4806876182556152,
        "learning_rate": 0.00019859493501308496,
        "epoch": 1.3705333333333334,
        "step": 10279
    },
    {
        "loss": 2.3442,
        "grad_norm": 3.00455641746521,
        "learning_rate": 0.0001985844022467045,
        "epoch": 1.3706666666666667,
        "step": 10280
    },
    {
        "loss": 2.4252,
        "grad_norm": 2.5661888122558594,
        "learning_rate": 0.00019857383043070636,
        "epoch": 1.3708,
        "step": 10281
    },
    {
        "loss": 2.37,
        "grad_norm": 3.5794992446899414,
        "learning_rate": 0.0001985632195692781,
        "epoch": 1.3709333333333333,
        "step": 10282
    },
    {
        "loss": 1.8797,
        "grad_norm": 3.882617950439453,
        "learning_rate": 0.0001985525696666227,
        "epoch": 1.3710666666666667,
        "step": 10283
    },
    {
        "loss": 1.6727,
        "grad_norm": 3.4497222900390625,
        "learning_rate": 0.0001985418807269587,
        "epoch": 1.3712,
        "step": 10284
    },
    {
        "loss": 2.0187,
        "grad_norm": 3.7540817260742188,
        "learning_rate": 0.00019853115275451994,
        "epoch": 1.3713333333333333,
        "step": 10285
    },
    {
        "loss": 2.3685,
        "grad_norm": 3.381504774093628,
        "learning_rate": 0.00019852038575355582,
        "epoch": 1.3714666666666666,
        "step": 10286
    },
    {
        "loss": 2.445,
        "grad_norm": 2.7438230514526367,
        "learning_rate": 0.0001985095797283312,
        "epoch": 1.3716,
        "step": 10287
    },
    {
        "loss": 2.7543,
        "grad_norm": 3.613574504852295,
        "learning_rate": 0.00019849873468312642,
        "epoch": 1.3717333333333332,
        "step": 10288
    },
    {
        "loss": 1.7118,
        "grad_norm": 3.9482789039611816,
        "learning_rate": 0.0001984878506222372,
        "epoch": 1.3718666666666666,
        "step": 10289
    },
    {
        "loss": 2.0661,
        "grad_norm": 2.3739845752716064,
        "learning_rate": 0.00019847692754997476,
        "epoch": 1.3719999999999999,
        "step": 10290
    },
    {
        "loss": 2.217,
        "grad_norm": 2.756544828414917,
        "learning_rate": 0.00019846596547066578,
        "epoch": 1.3721333333333332,
        "step": 10291
    },
    {
        "loss": 2.1139,
        "grad_norm": 4.518158435821533,
        "learning_rate": 0.00019845496438865233,
        "epoch": 1.3722666666666667,
        "step": 10292
    },
    {
        "loss": 2.8612,
        "grad_norm": 4.264512538909912,
        "learning_rate": 0.000198443924308292,
        "epoch": 1.3724,
        "step": 10293
    },
    {
        "loss": 1.8778,
        "grad_norm": 3.650348424911499,
        "learning_rate": 0.00019843284523395787,
        "epoch": 1.3725333333333334,
        "step": 10294
    },
    {
        "loss": 2.1198,
        "grad_norm": 3.001399278640747,
        "learning_rate": 0.0001984217271700383,
        "epoch": 1.3726666666666667,
        "step": 10295
    },
    {
        "loss": 2.2723,
        "grad_norm": 2.782900333404541,
        "learning_rate": 0.00019841057012093724,
        "epoch": 1.3728,
        "step": 10296
    },
    {
        "loss": 2.345,
        "grad_norm": 4.263515949249268,
        "learning_rate": 0.00019839937409107402,
        "epoch": 1.3729333333333333,
        "step": 10297
    },
    {
        "loss": 2.5463,
        "grad_norm": 3.268141746520996,
        "learning_rate": 0.00019838813908488342,
        "epoch": 1.3730666666666667,
        "step": 10298
    },
    {
        "loss": 1.7681,
        "grad_norm": 4.207996368408203,
        "learning_rate": 0.00019837686510681572,
        "epoch": 1.3732,
        "step": 10299
    },
    {
        "loss": 2.6967,
        "grad_norm": 4.473196029663086,
        "learning_rate": 0.00019836555216133653,
        "epoch": 1.3733333333333333,
        "step": 10300
    },
    {
        "loss": 2.0804,
        "grad_norm": 2.559633731842041,
        "learning_rate": 0.00019835420025292696,
        "epoch": 1.3734666666666666,
        "step": 10301
    },
    {
        "loss": 0.657,
        "grad_norm": 3.60503888130188,
        "learning_rate": 0.00019834280938608353,
        "epoch": 1.3736,
        "step": 10302
    },
    {
        "loss": 2.2327,
        "grad_norm": 3.1715409755706787,
        "learning_rate": 0.0001983313795653182,
        "epoch": 1.3737333333333333,
        "step": 10303
    },
    {
        "loss": 1.0375,
        "grad_norm": 3.4218695163726807,
        "learning_rate": 0.00019831991079515834,
        "epoch": 1.3738666666666668,
        "step": 10304
    },
    {
        "loss": 2.3529,
        "grad_norm": 3.4778506755828857,
        "learning_rate": 0.00019830840308014686,
        "epoch": 1.374,
        "step": 10305
    },
    {
        "loss": 2.1336,
        "grad_norm": 5.632978916168213,
        "learning_rate": 0.0001982968564248419,
        "epoch": 1.3741333333333334,
        "step": 10306
    },
    {
        "loss": 2.4527,
        "grad_norm": 4.010079860687256,
        "learning_rate": 0.00019828527083381715,
        "epoch": 1.3742666666666667,
        "step": 10307
    },
    {
        "loss": 2.1581,
        "grad_norm": 3.5340731143951416,
        "learning_rate": 0.00019827364631166177,
        "epoch": 1.3744,
        "step": 10308
    },
    {
        "loss": 2.2187,
        "grad_norm": 3.374843120574951,
        "learning_rate": 0.00019826198286298024,
        "epoch": 1.3745333333333334,
        "step": 10309
    },
    {
        "loss": 2.3178,
        "grad_norm": 3.1297454833984375,
        "learning_rate": 0.00019825028049239244,
        "epoch": 1.3746666666666667,
        "step": 10310
    },
    {
        "loss": 2.3165,
        "grad_norm": 3.04211688041687,
        "learning_rate": 0.00019823853920453375,
        "epoch": 1.3748,
        "step": 10311
    },
    {
        "loss": 1.3217,
        "grad_norm": 5.265425682067871,
        "learning_rate": 0.00019822675900405497,
        "epoch": 1.3749333333333333,
        "step": 10312
    },
    {
        "loss": 1.2328,
        "grad_norm": 3.7841145992279053,
        "learning_rate": 0.0001982149398956222,
        "epoch": 1.3750666666666667,
        "step": 10313
    },
    {
        "loss": 1.9481,
        "grad_norm": 3.2851266860961914,
        "learning_rate": 0.00019820308188391717,
        "epoch": 1.3752,
        "step": 10314
    },
    {
        "loss": 2.0697,
        "grad_norm": 3.751098155975342,
        "learning_rate": 0.0001981911849736367,
        "epoch": 1.3753333333333333,
        "step": 10315
    },
    {
        "loss": 2.3699,
        "grad_norm": 3.7988462448120117,
        "learning_rate": 0.00019817924916949332,
        "epoch": 1.3754666666666666,
        "step": 10316
    },
    {
        "loss": 2.5359,
        "grad_norm": 3.335387706756592,
        "learning_rate": 0.0001981672744762148,
        "epoch": 1.3756,
        "step": 10317
    },
    {
        "loss": 1.1867,
        "grad_norm": 4.400182247161865,
        "learning_rate": 0.00019815526089854438,
        "epoch": 1.3757333333333333,
        "step": 10318
    },
    {
        "loss": 2.0079,
        "grad_norm": 4.227903366088867,
        "learning_rate": 0.00019814320844124066,
        "epoch": 1.3758666666666666,
        "step": 10319
    },
    {
        "loss": 2.7881,
        "grad_norm": 2.9836158752441406,
        "learning_rate": 0.0001981311171090776,
        "epoch": 1.376,
        "step": 10320
    },
    {
        "loss": 1.7577,
        "grad_norm": 4.7905731201171875,
        "learning_rate": 0.00019811898690684477,
        "epoch": 1.3761333333333332,
        "step": 10321
    },
    {
        "loss": 2.445,
        "grad_norm": 2.688270092010498,
        "learning_rate": 0.00019810681783934687,
        "epoch": 1.3762666666666667,
        "step": 10322
    },
    {
        "loss": 1.3413,
        "grad_norm": 2.3218677043914795,
        "learning_rate": 0.00019809460991140413,
        "epoch": 1.3764,
        "step": 10323
    },
    {
        "loss": 0.737,
        "grad_norm": 4.464430809020996,
        "learning_rate": 0.00019808236312785212,
        "epoch": 1.3765333333333334,
        "step": 10324
    },
    {
        "loss": 1.4151,
        "grad_norm": 3.4290342330932617,
        "learning_rate": 0.00019807007749354186,
        "epoch": 1.3766666666666667,
        "step": 10325
    },
    {
        "loss": 2.2719,
        "grad_norm": 3.0859947204589844,
        "learning_rate": 0.0001980577530133398,
        "epoch": 1.3768,
        "step": 10326
    },
    {
        "loss": 1.1754,
        "grad_norm": 3.3461127281188965,
        "learning_rate": 0.0001980453896921276,
        "epoch": 1.3769333333333333,
        "step": 10327
    },
    {
        "loss": 2.1942,
        "grad_norm": 4.381616115570068,
        "learning_rate": 0.00019803298753480247,
        "epoch": 1.3770666666666667,
        "step": 10328
    },
    {
        "loss": 2.8349,
        "grad_norm": 4.149267196655273,
        "learning_rate": 0.00019802054654627696,
        "epoch": 1.3772,
        "step": 10329
    },
    {
        "loss": 2.0123,
        "grad_norm": 3.4106476306915283,
        "learning_rate": 0.00019800806673147894,
        "epoch": 1.3773333333333333,
        "step": 10330
    },
    {
        "loss": 2.007,
        "grad_norm": 3.8536510467529297,
        "learning_rate": 0.00019799554809535176,
        "epoch": 1.3774666666666666,
        "step": 10331
    },
    {
        "loss": 1.7965,
        "grad_norm": 3.7142066955566406,
        "learning_rate": 0.00019798299064285405,
        "epoch": 1.3776,
        "step": 10332
    },
    {
        "loss": 1.8488,
        "grad_norm": 3.9012985229492188,
        "learning_rate": 0.00019797039437895989,
        "epoch": 1.3777333333333333,
        "step": 10333
    },
    {
        "loss": 2.1914,
        "grad_norm": 4.3583478927612305,
        "learning_rate": 0.00019795775930865866,
        "epoch": 1.3778666666666668,
        "step": 10334
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.9823670387268066,
        "learning_rate": 0.00019794508543695524,
        "epoch": 1.3780000000000001,
        "step": 10335
    },
    {
        "loss": 2.0163,
        "grad_norm": 4.465219497680664,
        "learning_rate": 0.0001979323727688697,
        "epoch": 1.3781333333333334,
        "step": 10336
    },
    {
        "loss": 1.2895,
        "grad_norm": 3.627871036529541,
        "learning_rate": 0.0001979196213094376,
        "epoch": 1.3782666666666668,
        "step": 10337
    },
    {
        "loss": 2.1785,
        "grad_norm": 3.4753646850585938,
        "learning_rate": 0.00019790683106370985,
        "epoch": 1.3784,
        "step": 10338
    },
    {
        "loss": 2.556,
        "grad_norm": 2.263828992843628,
        "learning_rate": 0.0001978940020367527,
        "epoch": 1.3785333333333334,
        "step": 10339
    },
    {
        "loss": 1.5275,
        "grad_norm": 9.4113187789917,
        "learning_rate": 0.00019788113423364784,
        "epoch": 1.3786666666666667,
        "step": 10340
    },
    {
        "loss": 2.6616,
        "grad_norm": 3.9002296924591064,
        "learning_rate": 0.00019786822765949214,
        "epoch": 1.3788,
        "step": 10341
    },
    {
        "loss": 2.3432,
        "grad_norm": 3.856051445007324,
        "learning_rate": 0.00019785528231939798,
        "epoch": 1.3789333333333333,
        "step": 10342
    },
    {
        "loss": 2.1984,
        "grad_norm": 4.723376750946045,
        "learning_rate": 0.00019784229821849308,
        "epoch": 1.3790666666666667,
        "step": 10343
    },
    {
        "loss": 1.8507,
        "grad_norm": 3.6646509170532227,
        "learning_rate": 0.00019782927536192047,
        "epoch": 1.3792,
        "step": 10344
    },
    {
        "loss": 2.3191,
        "grad_norm": 3.335632562637329,
        "learning_rate": 0.0001978162137548385,
        "epoch": 1.3793333333333333,
        "step": 10345
    },
    {
        "loss": 2.2365,
        "grad_norm": 3.62654972076416,
        "learning_rate": 0.000197803113402421,
        "epoch": 1.3794666666666666,
        "step": 10346
    },
    {
        "loss": 2.3317,
        "grad_norm": 4.736607551574707,
        "learning_rate": 0.00019778997430985702,
        "epoch": 1.3796,
        "step": 10347
    },
    {
        "loss": 0.806,
        "grad_norm": 4.377904891967773,
        "learning_rate": 0.00019777679648235098,
        "epoch": 1.3797333333333333,
        "step": 10348
    },
    {
        "loss": 1.6489,
        "grad_norm": 4.383890628814697,
        "learning_rate": 0.00019776357992512273,
        "epoch": 1.3798666666666666,
        "step": 10349
    },
    {
        "loss": 2.5583,
        "grad_norm": 2.1110408306121826,
        "learning_rate": 0.00019775032464340733,
        "epoch": 1.38,
        "step": 10350
    },
    {
        "loss": 0.9335,
        "grad_norm": 5.126727104187012,
        "learning_rate": 0.00019773703064245523,
        "epoch": 1.3801333333333332,
        "step": 10351
    },
    {
        "loss": 2.5698,
        "grad_norm": 4.030224800109863,
        "learning_rate": 0.00019772369792753233,
        "epoch": 1.3802666666666665,
        "step": 10352
    },
    {
        "loss": 2.1051,
        "grad_norm": 4.217705249786377,
        "learning_rate": 0.00019771032650391966,
        "epoch": 1.3804,
        "step": 10353
    },
    {
        "loss": 1.7476,
        "grad_norm": 4.169561862945557,
        "learning_rate": 0.0001976969163769137,
        "epoch": 1.3805333333333334,
        "step": 10354
    },
    {
        "loss": 1.7748,
        "grad_norm": 7.7158660888671875,
        "learning_rate": 0.0001976834675518263,
        "epoch": 1.3806666666666667,
        "step": 10355
    },
    {
        "loss": 3.3608,
        "grad_norm": 4.770479679107666,
        "learning_rate": 0.00019766998003398453,
        "epoch": 1.3808,
        "step": 10356
    },
    {
        "loss": 1.4184,
        "grad_norm": 4.94309663772583,
        "learning_rate": 0.00019765645382873088,
        "epoch": 1.3809333333333333,
        "step": 10357
    },
    {
        "loss": 1.7798,
        "grad_norm": 3.8181512355804443,
        "learning_rate": 0.00019764288894142312,
        "epoch": 1.3810666666666667,
        "step": 10358
    },
    {
        "loss": 1.9136,
        "grad_norm": 4.487368583679199,
        "learning_rate": 0.00019762928537743433,
        "epoch": 1.3812,
        "step": 10359
    },
    {
        "loss": 2.6999,
        "grad_norm": 4.2510762214660645,
        "learning_rate": 0.00019761564314215298,
        "epoch": 1.3813333333333333,
        "step": 10360
    },
    {
        "loss": 2.4943,
        "grad_norm": 4.094138145446777,
        "learning_rate": 0.0001976019622409827,
        "epoch": 1.3814666666666666,
        "step": 10361
    },
    {
        "loss": 2.1575,
        "grad_norm": 4.466047286987305,
        "learning_rate": 0.00019758824267934264,
        "epoch": 1.3816,
        "step": 10362
    },
    {
        "loss": 0.5463,
        "grad_norm": 3.556591510772705,
        "learning_rate": 0.0001975744844626671,
        "epoch": 1.3817333333333333,
        "step": 10363
    },
    {
        "loss": 1.0126,
        "grad_norm": 5.529711723327637,
        "learning_rate": 0.00019756068759640584,
        "epoch": 1.3818666666666668,
        "step": 10364
    },
    {
        "loss": 2.6389,
        "grad_norm": 3.1276516914367676,
        "learning_rate": 0.00019754685208602375,
        "epoch": 1.3820000000000001,
        "step": 10365
    },
    {
        "loss": 2.9773,
        "grad_norm": 3.4411025047302246,
        "learning_rate": 0.00019753297793700115,
        "epoch": 1.3821333333333334,
        "step": 10366
    },
    {
        "loss": 1.7379,
        "grad_norm": 3.1613588333129883,
        "learning_rate": 0.00019751906515483368,
        "epoch": 1.3822666666666668,
        "step": 10367
    },
    {
        "loss": 2.7929,
        "grad_norm": 3.9501240253448486,
        "learning_rate": 0.00019750511374503225,
        "epoch": 1.3824,
        "step": 10368
    },
    {
        "loss": 1.521,
        "grad_norm": 4.445329189300537,
        "learning_rate": 0.000197491123713123,
        "epoch": 1.3825333333333334,
        "step": 10369
    },
    {
        "loss": 1.9807,
        "grad_norm": 5.476879596710205,
        "learning_rate": 0.0001974770950646474,
        "epoch": 1.3826666666666667,
        "step": 10370
    },
    {
        "loss": 2.4688,
        "grad_norm": 2.7263479232788086,
        "learning_rate": 0.00019746302780516238,
        "epoch": 1.3828,
        "step": 10371
    },
    {
        "loss": 1.7778,
        "grad_norm": 4.550168514251709,
        "learning_rate": 0.00019744892194023986,
        "epoch": 1.3829333333333333,
        "step": 10372
    },
    {
        "loss": 2.4744,
        "grad_norm": 4.248111248016357,
        "learning_rate": 0.0001974347774754674,
        "epoch": 1.3830666666666667,
        "step": 10373
    },
    {
        "loss": 1.6073,
        "grad_norm": 2.904285430908203,
        "learning_rate": 0.00019742059441644756,
        "epoch": 1.3832,
        "step": 10374
    },
    {
        "loss": 2.3686,
        "grad_norm": 3.046804189682007,
        "learning_rate": 0.00019740637276879826,
        "epoch": 1.3833333333333333,
        "step": 10375
    },
    {
        "loss": 2.1219,
        "grad_norm": 4.910180568695068,
        "learning_rate": 0.00019739211253815285,
        "epoch": 1.3834666666666666,
        "step": 10376
    },
    {
        "loss": 2.3232,
        "grad_norm": 4.119325637817383,
        "learning_rate": 0.00019737781373015985,
        "epoch": 1.3836,
        "step": 10377
    },
    {
        "loss": 2.4054,
        "grad_norm": 3.4184513092041016,
        "learning_rate": 0.00019736347635048297,
        "epoch": 1.3837333333333333,
        "step": 10378
    },
    {
        "loss": 2.2176,
        "grad_norm": 4.858490467071533,
        "learning_rate": 0.00019734910040480137,
        "epoch": 1.3838666666666666,
        "step": 10379
    },
    {
        "loss": 2.2316,
        "grad_norm": 3.8680026531219482,
        "learning_rate": 0.00019733468589880937,
        "epoch": 1.384,
        "step": 10380
    },
    {
        "loss": 2.0559,
        "grad_norm": 2.7436890602111816,
        "learning_rate": 0.00019732023283821666,
        "epoch": 1.3841333333333332,
        "step": 10381
    },
    {
        "loss": 2.2802,
        "grad_norm": 4.3064799308776855,
        "learning_rate": 0.0001973057412287481,
        "epoch": 1.3842666666666665,
        "step": 10382
    },
    {
        "loss": 1.9349,
        "grad_norm": 5.707396984100342,
        "learning_rate": 0.00019729121107614383,
        "epoch": 1.3844,
        "step": 10383
    },
    {
        "loss": 2.1593,
        "grad_norm": 3.0565357208251953,
        "learning_rate": 0.00019727664238615935,
        "epoch": 1.3845333333333334,
        "step": 10384
    },
    {
        "loss": 2.718,
        "grad_norm": 3.069979429244995,
        "learning_rate": 0.00019726203516456546,
        "epoch": 1.3846666666666667,
        "step": 10385
    },
    {
        "loss": 2.4782,
        "grad_norm": 5.043585777282715,
        "learning_rate": 0.00019724738941714792,
        "epoch": 1.3848,
        "step": 10386
    },
    {
        "loss": 2.0009,
        "grad_norm": 4.144999980926514,
        "learning_rate": 0.0001972327051497081,
        "epoch": 1.3849333333333333,
        "step": 10387
    },
    {
        "loss": 2.4154,
        "grad_norm": 4.262664318084717,
        "learning_rate": 0.00019721798236806246,
        "epoch": 1.3850666666666667,
        "step": 10388
    },
    {
        "loss": 1.9075,
        "grad_norm": 3.9022605419158936,
        "learning_rate": 0.00019720322107804274,
        "epoch": 1.3852,
        "step": 10389
    },
    {
        "loss": 2.1861,
        "grad_norm": 3.8309710025787354,
        "learning_rate": 0.00019718842128549596,
        "epoch": 1.3853333333333333,
        "step": 10390
    },
    {
        "loss": 2.2274,
        "grad_norm": 4.294388294219971,
        "learning_rate": 0.00019717358299628434,
        "epoch": 1.3854666666666666,
        "step": 10391
    },
    {
        "loss": 1.6821,
        "grad_norm": 2.5336129665374756,
        "learning_rate": 0.00019715870621628536,
        "epoch": 1.3856,
        "step": 10392
    },
    {
        "loss": 2.8262,
        "grad_norm": 3.420976400375366,
        "learning_rate": 0.0001971437909513918,
        "epoch": 1.3857333333333333,
        "step": 10393
    },
    {
        "loss": 0.8419,
        "grad_norm": 3.478154182434082,
        "learning_rate": 0.00019712883720751165,
        "epoch": 1.3858666666666668,
        "step": 10394
    },
    {
        "loss": 2.3045,
        "grad_norm": 3.683619499206543,
        "learning_rate": 0.0001971138449905681,
        "epoch": 1.3860000000000001,
        "step": 10395
    },
    {
        "loss": 0.5667,
        "grad_norm": 3.9695708751678467,
        "learning_rate": 0.00019709881430649966,
        "epoch": 1.3861333333333334,
        "step": 10396
    },
    {
        "loss": 1.4859,
        "grad_norm": 3.9472525119781494,
        "learning_rate": 0.00019708374516126,
        "epoch": 1.3862666666666668,
        "step": 10397
    },
    {
        "loss": 2.0841,
        "grad_norm": 3.3839869499206543,
        "learning_rate": 0.0001970686375608181,
        "epoch": 1.3864,
        "step": 10398
    },
    {
        "loss": 1.5238,
        "grad_norm": 5.1772074699401855,
        "learning_rate": 0.0001970534915111581,
        "epoch": 1.3865333333333334,
        "step": 10399
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.435991048812866,
        "learning_rate": 0.00019703830701827944,
        "epoch": 1.3866666666666667,
        "step": 10400
    },
    {
        "loss": 2.1627,
        "grad_norm": 4.713760852813721,
        "learning_rate": 0.00019702308408819667,
        "epoch": 1.3868,
        "step": 10401
    },
    {
        "loss": 1.8378,
        "grad_norm": 4.269262313842773,
        "learning_rate": 0.0001970078227269397,
        "epoch": 1.3869333333333334,
        "step": 10402
    },
    {
        "loss": 2.2326,
        "grad_norm": 3.7722227573394775,
        "learning_rate": 0.00019699252294055363,
        "epoch": 1.3870666666666667,
        "step": 10403
    },
    {
        "loss": 2.0885,
        "grad_norm": 3.3281168937683105,
        "learning_rate": 0.0001969771847350987,
        "epoch": 1.3872,
        "step": 10404
    },
    {
        "loss": 2.1225,
        "grad_norm": 4.358511447906494,
        "learning_rate": 0.00019696180811665047,
        "epoch": 1.3873333333333333,
        "step": 10405
    },
    {
        "loss": 2.2199,
        "grad_norm": 4.998700141906738,
        "learning_rate": 0.00019694639309129967,
        "epoch": 1.3874666666666666,
        "step": 10406
    },
    {
        "loss": 0.6868,
        "grad_norm": 4.291116237640381,
        "learning_rate": 0.0001969309396651522,
        "epoch": 1.3876,
        "step": 10407
    },
    {
        "loss": 2.307,
        "grad_norm": 3.80403470993042,
        "learning_rate": 0.00019691544784432928,
        "epoch": 1.3877333333333333,
        "step": 10408
    },
    {
        "loss": 1.3731,
        "grad_norm": 4.016175746917725,
        "learning_rate": 0.0001968999176349672,
        "epoch": 1.3878666666666666,
        "step": 10409
    },
    {
        "loss": 2.4521,
        "grad_norm": 3.9407503604888916,
        "learning_rate": 0.00019688434904321754,
        "epoch": 1.388,
        "step": 10410
    },
    {
        "loss": 2.3445,
        "grad_norm": 3.7165791988372803,
        "learning_rate": 0.00019686874207524717,
        "epoch": 1.3881333333333332,
        "step": 10411
    },
    {
        "loss": 0.5354,
        "grad_norm": 4.169235706329346,
        "learning_rate": 0.00019685309673723796,
        "epoch": 1.3882666666666665,
        "step": 10412
    },
    {
        "loss": 1.8997,
        "grad_norm": 3.93512225151062,
        "learning_rate": 0.00019683741303538711,
        "epoch": 1.3884,
        "step": 10413
    },
    {
        "loss": 1.5699,
        "grad_norm": 6.996798515319824,
        "learning_rate": 0.00019682169097590702,
        "epoch": 1.3885333333333334,
        "step": 10414
    },
    {
        "loss": 1.7,
        "grad_norm": 3.865051746368408,
        "learning_rate": 0.00019680593056502522,
        "epoch": 1.3886666666666667,
        "step": 10415
    },
    {
        "loss": 1.7488,
        "grad_norm": 4.013769626617432,
        "learning_rate": 0.0001967901318089844,
        "epoch": 1.3888,
        "step": 10416
    },
    {
        "loss": 2.8069,
        "grad_norm": 3.152562379837036,
        "learning_rate": 0.00019677429471404262,
        "epoch": 1.3889333333333334,
        "step": 10417
    },
    {
        "loss": 3.3935,
        "grad_norm": 4.554482936859131,
        "learning_rate": 0.00019675841928647295,
        "epoch": 1.3890666666666667,
        "step": 10418
    },
    {
        "loss": 2.4921,
        "grad_norm": 4.237398624420166,
        "learning_rate": 0.00019674250553256372,
        "epoch": 1.3892,
        "step": 10419
    },
    {
        "loss": 1.2706,
        "grad_norm": 4.197184085845947,
        "learning_rate": 0.00019672655345861836,
        "epoch": 1.3893333333333333,
        "step": 10420
    },
    {
        "loss": 1.874,
        "grad_norm": 3.3433477878570557,
        "learning_rate": 0.00019671056307095564,
        "epoch": 1.3894666666666666,
        "step": 10421
    },
    {
        "loss": 2.647,
        "grad_norm": 5.176511287689209,
        "learning_rate": 0.0001966945343759093,
        "epoch": 1.3896,
        "step": 10422
    },
    {
        "loss": 2.8143,
        "grad_norm": 5.865704536437988,
        "learning_rate": 0.0001966784673798285,
        "epoch": 1.3897333333333333,
        "step": 10423
    },
    {
        "loss": 1.8867,
        "grad_norm": 3.6467723846435547,
        "learning_rate": 0.00019666236208907724,
        "epoch": 1.3898666666666666,
        "step": 10424
    },
    {
        "loss": 1.6712,
        "grad_norm": 5.907751560211182,
        "learning_rate": 0.00019664621851003502,
        "epoch": 1.3900000000000001,
        "step": 10425
    },
    {
        "loss": 1.2361,
        "grad_norm": 4.802934169769287,
        "learning_rate": 0.00019663003664909638,
        "epoch": 1.3901333333333334,
        "step": 10426
    },
    {
        "loss": 2.1375,
        "grad_norm": 3.3680951595306396,
        "learning_rate": 0.00019661381651267095,
        "epoch": 1.3902666666666668,
        "step": 10427
    },
    {
        "loss": 1.0883,
        "grad_norm": 4.098921775817871,
        "learning_rate": 0.00019659755810718358,
        "epoch": 1.3904,
        "step": 10428
    },
    {
        "loss": 2.0052,
        "grad_norm": 4.031652927398682,
        "learning_rate": 0.00019658126143907428,
        "epoch": 1.3905333333333334,
        "step": 10429
    },
    {
        "loss": 2.2488,
        "grad_norm": 3.1185731887817383,
        "learning_rate": 0.00019656492651479827,
        "epoch": 1.3906666666666667,
        "step": 10430
    },
    {
        "loss": 2.5627,
        "grad_norm": 2.619903326034546,
        "learning_rate": 0.00019654855334082577,
        "epoch": 1.3908,
        "step": 10431
    },
    {
        "loss": 0.9314,
        "grad_norm": 5.423710823059082,
        "learning_rate": 0.0001965321419236424,
        "epoch": 1.3909333333333334,
        "step": 10432
    },
    {
        "loss": 1.8884,
        "grad_norm": 3.3690590858459473,
        "learning_rate": 0.00019651569226974863,
        "epoch": 1.3910666666666667,
        "step": 10433
    },
    {
        "loss": 2.1407,
        "grad_norm": 2.835540771484375,
        "learning_rate": 0.00019649920438566024,
        "epoch": 1.3912,
        "step": 10434
    },
    {
        "loss": 1.8556,
        "grad_norm": 5.033370018005371,
        "learning_rate": 0.00019648267827790825,
        "epoch": 1.3913333333333333,
        "step": 10435
    },
    {
        "loss": 1.6338,
        "grad_norm": 4.566844463348389,
        "learning_rate": 0.00019646611395303864,
        "epoch": 1.3914666666666666,
        "step": 10436
    },
    {
        "loss": 1.9847,
        "grad_norm": 3.4008071422576904,
        "learning_rate": 0.0001964495114176126,
        "epoch": 1.3916,
        "step": 10437
    },
    {
        "loss": 1.5739,
        "grad_norm": 4.792620658874512,
        "learning_rate": 0.00019643287067820636,
        "epoch": 1.3917333333333333,
        "step": 10438
    },
    {
        "loss": 1.6775,
        "grad_norm": 4.49910831451416,
        "learning_rate": 0.00019641619174141155,
        "epoch": 1.3918666666666666,
        "step": 10439
    },
    {
        "loss": 2.8658,
        "grad_norm": 3.605729103088379,
        "learning_rate": 0.00019639947461383463,
        "epoch": 1.392,
        "step": 10440
    },
    {
        "loss": 1.4441,
        "grad_norm": 6.270993232727051,
        "learning_rate": 0.00019638271930209734,
        "epoch": 1.3921333333333332,
        "step": 10441
    },
    {
        "loss": 1.4467,
        "grad_norm": 4.760524749755859,
        "learning_rate": 0.00019636592581283652,
        "epoch": 1.3922666666666665,
        "step": 10442
    },
    {
        "loss": 2.1932,
        "grad_norm": 2.4503164291381836,
        "learning_rate": 0.00019634909415270407,
        "epoch": 1.3924,
        "step": 10443
    },
    {
        "loss": 2.0732,
        "grad_norm": 4.124917984008789,
        "learning_rate": 0.00019633222432836723,
        "epoch": 1.3925333333333334,
        "step": 10444
    },
    {
        "loss": 2.1282,
        "grad_norm": 3.247323989868164,
        "learning_rate": 0.000196315316346508,
        "epoch": 1.3926666666666667,
        "step": 10445
    },
    {
        "loss": 2.1427,
        "grad_norm": 3.842541217803955,
        "learning_rate": 0.00019629837021382384,
        "epoch": 1.3928,
        "step": 10446
    },
    {
        "loss": 1.8598,
        "grad_norm": 5.091560363769531,
        "learning_rate": 0.00019628138593702707,
        "epoch": 1.3929333333333334,
        "step": 10447
    },
    {
        "loss": 1.9669,
        "grad_norm": 3.7966883182525635,
        "learning_rate": 0.00019626436352284527,
        "epoch": 1.3930666666666667,
        "step": 10448
    },
    {
        "loss": 1.7808,
        "grad_norm": 5.644678115844727,
        "learning_rate": 0.0001962473029780211,
        "epoch": 1.3932,
        "step": 10449
    },
    {
        "loss": 2.3983,
        "grad_norm": 4.528669357299805,
        "learning_rate": 0.00019623020430931224,
        "epoch": 1.3933333333333333,
        "step": 10450
    },
    {
        "loss": 2.1019,
        "grad_norm": 4.981797218322754,
        "learning_rate": 0.00019621306752349153,
        "epoch": 1.3934666666666666,
        "step": 10451
    },
    {
        "loss": 1.7941,
        "grad_norm": 7.1493611335754395,
        "learning_rate": 0.00019619589262734693,
        "epoch": 1.3936,
        "step": 10452
    },
    {
        "loss": 2.4917,
        "grad_norm": 3.523693561553955,
        "learning_rate": 0.00019617867962768158,
        "epoch": 1.3937333333333333,
        "step": 10453
    },
    {
        "loss": 2.0377,
        "grad_norm": 3.440269947052002,
        "learning_rate": 0.00019616142853131342,
        "epoch": 1.3938666666666666,
        "step": 10454
    },
    {
        "loss": 2.2062,
        "grad_norm": 2.697230339050293,
        "learning_rate": 0.00019614413934507577,
        "epoch": 1.3940000000000001,
        "step": 10455
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.6817123889923096,
        "learning_rate": 0.00019612681207581688,
        "epoch": 1.3941333333333334,
        "step": 10456
    },
    {
        "loss": 2.373,
        "grad_norm": 4.034350872039795,
        "learning_rate": 0.00019610944673040023,
        "epoch": 1.3942666666666668,
        "step": 10457
    },
    {
        "loss": 1.6212,
        "grad_norm": 4.949039459228516,
        "learning_rate": 0.00019609204331570427,
        "epoch": 1.3944,
        "step": 10458
    },
    {
        "loss": 1.6135,
        "grad_norm": 4.771577835083008,
        "learning_rate": 0.00019607460183862252,
        "epoch": 1.3945333333333334,
        "step": 10459
    },
    {
        "loss": 1.18,
        "grad_norm": 4.465654373168945,
        "learning_rate": 0.00019605712230606358,
        "epoch": 1.3946666666666667,
        "step": 10460
    },
    {
        "loss": 2.8572,
        "grad_norm": 2.3614118099212646,
        "learning_rate": 0.00019603960472495122,
        "epoch": 1.3948,
        "step": 10461
    },
    {
        "loss": 1.3599,
        "grad_norm": 4.093189716339111,
        "learning_rate": 0.00019602204910222417,
        "epoch": 1.3949333333333334,
        "step": 10462
    },
    {
        "loss": 2.2522,
        "grad_norm": 3.1773746013641357,
        "learning_rate": 0.00019600445544483625,
        "epoch": 1.3950666666666667,
        "step": 10463
    },
    {
        "loss": 1.8566,
        "grad_norm": 4.3125834465026855,
        "learning_rate": 0.00019598682375975644,
        "epoch": 1.3952,
        "step": 10464
    },
    {
        "loss": 2.838,
        "grad_norm": 2.5856525897979736,
        "learning_rate": 0.00019596915405396869,
        "epoch": 1.3953333333333333,
        "step": 10465
    },
    {
        "loss": 1.7693,
        "grad_norm": 3.44132399559021,
        "learning_rate": 0.00019595144633447197,
        "epoch": 1.3954666666666666,
        "step": 10466
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.6230010986328125,
        "learning_rate": 0.00019593370060828043,
        "epoch": 1.3956,
        "step": 10467
    },
    {
        "loss": 1.9837,
        "grad_norm": 4.031505107879639,
        "learning_rate": 0.0001959159168824232,
        "epoch": 1.3957333333333333,
        "step": 10468
    },
    {
        "loss": 2.6308,
        "grad_norm": 5.255810737609863,
        "learning_rate": 0.00019589809516394447,
        "epoch": 1.3958666666666666,
        "step": 10469
    },
    {
        "loss": 2.1653,
        "grad_norm": 4.05375337600708,
        "learning_rate": 0.00019588023545990345,
        "epoch": 1.396,
        "step": 10470
    },
    {
        "loss": 1.7264,
        "grad_norm": 3.5145087242126465,
        "learning_rate": 0.00019586233777737447,
        "epoch": 1.3961333333333332,
        "step": 10471
    },
    {
        "loss": 1.4946,
        "grad_norm": 3.035834789276123,
        "learning_rate": 0.00019584440212344685,
        "epoch": 1.3962666666666665,
        "step": 10472
    },
    {
        "loss": 2.3057,
        "grad_norm": 3.592451572418213,
        "learning_rate": 0.00019582642850522497,
        "epoch": 1.3963999999999999,
        "step": 10473
    },
    {
        "loss": 1.6123,
        "grad_norm": 4.404475688934326,
        "learning_rate": 0.0001958084169298283,
        "epoch": 1.3965333333333334,
        "step": 10474
    },
    {
        "loss": 1.8207,
        "grad_norm": 3.8415069580078125,
        "learning_rate": 0.0001957903674043911,
        "epoch": 1.3966666666666667,
        "step": 10475
    },
    {
        "loss": 2.3968,
        "grad_norm": 4.023626327514648,
        "learning_rate": 0.00019577227993606305,
        "epoch": 1.3968,
        "step": 10476
    },
    {
        "loss": 1.6068,
        "grad_norm": 4.793468952178955,
        "learning_rate": 0.0001957541545320086,
        "epoch": 1.3969333333333334,
        "step": 10477
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.820338249206543,
        "learning_rate": 0.00019573599119940727,
        "epoch": 1.3970666666666667,
        "step": 10478
    },
    {
        "loss": 1.5505,
        "grad_norm": 3.2397897243499756,
        "learning_rate": 0.00019571778994545354,
        "epoch": 1.3972,
        "step": 10479
    },
    {
        "loss": 1.8531,
        "grad_norm": 3.439661979675293,
        "learning_rate": 0.00019569955077735712,
        "epoch": 1.3973333333333333,
        "step": 10480
    },
    {
        "loss": 2.0767,
        "grad_norm": 4.343430995941162,
        "learning_rate": 0.00019568127370234251,
        "epoch": 1.3974666666666666,
        "step": 10481
    },
    {
        "loss": 2.2821,
        "grad_norm": 3.0927305221557617,
        "learning_rate": 0.00019566295872764945,
        "epoch": 1.3976,
        "step": 10482
    },
    {
        "loss": 2.2138,
        "grad_norm": 3.9318807125091553,
        "learning_rate": 0.0001956446058605324,
        "epoch": 1.3977333333333333,
        "step": 10483
    },
    {
        "loss": 2.1972,
        "grad_norm": 5.008049011230469,
        "learning_rate": 0.00019562621510826105,
        "epoch": 1.3978666666666666,
        "step": 10484
    },
    {
        "loss": 2.1128,
        "grad_norm": 3.8531429767608643,
        "learning_rate": 0.0001956077864781201,
        "epoch": 1.3980000000000001,
        "step": 10485
    },
    {
        "loss": 1.4587,
        "grad_norm": 4.15330696105957,
        "learning_rate": 0.00019558931997740915,
        "epoch": 1.3981333333333335,
        "step": 10486
    },
    {
        "loss": 2.5693,
        "grad_norm": 2.6905434131622314,
        "learning_rate": 0.00019557081561344286,
        "epoch": 1.3982666666666668,
        "step": 10487
    },
    {
        "loss": 2.7167,
        "grad_norm": 3.7343711853027344,
        "learning_rate": 0.0001955522733935508,
        "epoch": 1.3984,
        "step": 10488
    },
    {
        "loss": 2.1421,
        "grad_norm": 3.486048460006714,
        "learning_rate": 0.00019553369332507767,
        "epoch": 1.3985333333333334,
        "step": 10489
    },
    {
        "loss": 0.8221,
        "grad_norm": 3.5773062705993652,
        "learning_rate": 0.00019551507541538309,
        "epoch": 1.3986666666666667,
        "step": 10490
    },
    {
        "loss": 1.5466,
        "grad_norm": 4.767180442810059,
        "learning_rate": 0.00019549641967184177,
        "epoch": 1.3988,
        "step": 10491
    },
    {
        "loss": 0.9214,
        "grad_norm": 4.731567859649658,
        "learning_rate": 0.00019547772610184312,
        "epoch": 1.3989333333333334,
        "step": 10492
    },
    {
        "loss": 2.6129,
        "grad_norm": 2.59173321723938,
        "learning_rate": 0.00019545899471279184,
        "epoch": 1.3990666666666667,
        "step": 10493
    },
    {
        "loss": 1.3697,
        "grad_norm": 4.583515644073486,
        "learning_rate": 0.0001954402255121075,
        "epoch": 1.3992,
        "step": 10494
    },
    {
        "loss": 2.5374,
        "grad_norm": 4.121838569641113,
        "learning_rate": 0.00019542141850722463,
        "epoch": 1.3993333333333333,
        "step": 10495
    },
    {
        "loss": 1.5926,
        "grad_norm": 3.477074146270752,
        "learning_rate": 0.0001954025737055928,
        "epoch": 1.3994666666666666,
        "step": 10496
    },
    {
        "loss": 1.7412,
        "grad_norm": 7.237824440002441,
        "learning_rate": 0.00019538369111467638,
        "epoch": 1.3996,
        "step": 10497
    },
    {
        "loss": 1.5238,
        "grad_norm": 4.652573108673096,
        "learning_rate": 0.0001953647707419549,
        "epoch": 1.3997333333333333,
        "step": 10498
    },
    {
        "loss": 2.0604,
        "grad_norm": 3.2794220447540283,
        "learning_rate": 0.00019534581259492284,
        "epoch": 1.3998666666666666,
        "step": 10499
    },
    {
        "loss": 2.1124,
        "grad_norm": 4.599672794342041,
        "learning_rate": 0.0001953268166810895,
        "epoch": 1.4,
        "step": 10500
    },
    {
        "loss": 3.0396,
        "grad_norm": 4.372261047363281,
        "learning_rate": 0.0001953077830079792,
        "epoch": 1.4001333333333332,
        "step": 10501
    },
    {
        "loss": 2.6636,
        "grad_norm": 2.7409651279449463,
        "learning_rate": 0.00019528871158313134,
        "epoch": 1.4002666666666665,
        "step": 10502
    },
    {
        "loss": 2.1715,
        "grad_norm": 5.153870105743408,
        "learning_rate": 0.00019526960241410016,
        "epoch": 1.4003999999999999,
        "step": 10503
    },
    {
        "loss": 1.5281,
        "grad_norm": 3.4159724712371826,
        "learning_rate": 0.0001952504555084548,
        "epoch": 1.4005333333333334,
        "step": 10504
    },
    {
        "loss": 2.7218,
        "grad_norm": 3.271937847137451,
        "learning_rate": 0.0001952312708737795,
        "epoch": 1.4006666666666667,
        "step": 10505
    },
    {
        "loss": 0.9645,
        "grad_norm": 3.571319341659546,
        "learning_rate": 0.00019521204851767325,
        "epoch": 1.4008,
        "step": 10506
    },
    {
        "loss": 2.3875,
        "grad_norm": 2.905292272567749,
        "learning_rate": 0.0001951927884477502,
        "epoch": 1.4009333333333334,
        "step": 10507
    },
    {
        "loss": 3.3433,
        "grad_norm": 5.285591125488281,
        "learning_rate": 0.0001951734906716393,
        "epoch": 1.4010666666666667,
        "step": 10508
    },
    {
        "loss": 1.9634,
        "grad_norm": 4.616928577423096,
        "learning_rate": 0.00019515415519698442,
        "epoch": 1.4012,
        "step": 10509
    },
    {
        "loss": 2.3296,
        "grad_norm": 5.86137580871582,
        "learning_rate": 0.00019513478203144442,
        "epoch": 1.4013333333333333,
        "step": 10510
    },
    {
        "loss": 2.1138,
        "grad_norm": 3.367896795272827,
        "learning_rate": 0.0001951153711826931,
        "epoch": 1.4014666666666666,
        "step": 10511
    },
    {
        "loss": 2.4777,
        "grad_norm": 2.825249433517456,
        "learning_rate": 0.0001950959226584192,
        "epoch": 1.4016,
        "step": 10512
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.7115211486816406,
        "learning_rate": 0.00019507643646632627,
        "epoch": 1.4017333333333333,
        "step": 10513
    },
    {
        "loss": 1.5614,
        "grad_norm": 6.336430549621582,
        "learning_rate": 0.0001950569126141329,
        "epoch": 1.4018666666666666,
        "step": 10514
    },
    {
        "loss": 2.2003,
        "grad_norm": 2.8880109786987305,
        "learning_rate": 0.00019503735110957252,
        "epoch": 1.4020000000000001,
        "step": 10515
    },
    {
        "loss": 2.8945,
        "grad_norm": 3.5133891105651855,
        "learning_rate": 0.00019501775196039355,
        "epoch": 1.4021333333333335,
        "step": 10516
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.384735584259033,
        "learning_rate": 0.0001949981151743593,
        "epoch": 1.4022666666666668,
        "step": 10517
    },
    {
        "loss": 2.2983,
        "grad_norm": 2.8048336505889893,
        "learning_rate": 0.00019497844075924792,
        "epoch": 1.4024,
        "step": 10518
    },
    {
        "loss": 2.7618,
        "grad_norm": 4.162662982940674,
        "learning_rate": 0.00019495872872285247,
        "epoch": 1.4025333333333334,
        "step": 10519
    },
    {
        "loss": 2.7122,
        "grad_norm": 3.5842368602752686,
        "learning_rate": 0.00019493897907298108,
        "epoch": 1.4026666666666667,
        "step": 10520
    },
    {
        "loss": 1.0366,
        "grad_norm": 4.720765590667725,
        "learning_rate": 0.00019491919181745653,
        "epoch": 1.4028,
        "step": 10521
    },
    {
        "loss": 2.2262,
        "grad_norm": 3.629791259765625,
        "learning_rate": 0.00019489936696411665,
        "epoch": 1.4029333333333334,
        "step": 10522
    },
    {
        "loss": 2.2896,
        "grad_norm": 3.1432816982269287,
        "learning_rate": 0.00019487950452081422,
        "epoch": 1.4030666666666667,
        "step": 10523
    },
    {
        "loss": 1.6854,
        "grad_norm": 4.089302062988281,
        "learning_rate": 0.00019485960449541675,
        "epoch": 1.4032,
        "step": 10524
    },
    {
        "loss": 2.1975,
        "grad_norm": 3.4290974140167236,
        "learning_rate": 0.00019483966689580665,
        "epoch": 1.4033333333333333,
        "step": 10525
    },
    {
        "loss": 2.3105,
        "grad_norm": 3.158529043197632,
        "learning_rate": 0.00019481969172988138,
        "epoch": 1.4034666666666666,
        "step": 10526
    },
    {
        "loss": 2.2615,
        "grad_norm": 4.054708480834961,
        "learning_rate": 0.00019479967900555315,
        "epoch": 1.4036,
        "step": 10527
    },
    {
        "loss": 1.2419,
        "grad_norm": 4.129424095153809,
        "learning_rate": 0.000194779628730749,
        "epoch": 1.4037333333333333,
        "step": 10528
    },
    {
        "loss": 2.61,
        "grad_norm": 3.329456090927124,
        "learning_rate": 0.00019475954091341096,
        "epoch": 1.4038666666666666,
        "step": 10529
    },
    {
        "loss": 0.9081,
        "grad_norm": 6.233116626739502,
        "learning_rate": 0.0001947394155614959,
        "epoch": 1.404,
        "step": 10530
    },
    {
        "loss": 2.1811,
        "grad_norm": 5.189091205596924,
        "learning_rate": 0.00019471925268297547,
        "epoch": 1.4041333333333332,
        "step": 10531
    },
    {
        "loss": 1.9837,
        "grad_norm": 3.2552382946014404,
        "learning_rate": 0.00019469905228583636,
        "epoch": 1.4042666666666666,
        "step": 10532
    },
    {
        "loss": 1.9394,
        "grad_norm": 2.8775205612182617,
        "learning_rate": 0.00019467881437807994,
        "epoch": 1.4043999999999999,
        "step": 10533
    },
    {
        "loss": 1.4657,
        "grad_norm": 4.912595272064209,
        "learning_rate": 0.00019465853896772254,
        "epoch": 1.4045333333333334,
        "step": 10534
    },
    {
        "loss": 1.8496,
        "grad_norm": 4.596810817718506,
        "learning_rate": 0.0001946382260627953,
        "epoch": 1.4046666666666667,
        "step": 10535
    },
    {
        "loss": 1.773,
        "grad_norm": 3.2643966674804688,
        "learning_rate": 0.00019461787567134428,
        "epoch": 1.4048,
        "step": 10536
    },
    {
        "loss": 1.4707,
        "grad_norm": Infinity,
        "learning_rate": 0.00019461787567134428,
        "epoch": 1.4049333333333334,
        "step": 10537
    },
    {
        "loss": 2.174,
        "grad_norm": 5.468409538269043,
        "learning_rate": 0.00019459748780143026,
        "epoch": 1.4050666666666667,
        "step": 10538
    },
    {
        "loss": 1.6445,
        "grad_norm": 4.862173080444336,
        "learning_rate": 0.00019457706246112902,
        "epoch": 1.4052,
        "step": 10539
    },
    {
        "loss": 2.1578,
        "grad_norm": 3.4898767471313477,
        "learning_rate": 0.00019455659965853105,
        "epoch": 1.4053333333333333,
        "step": 10540
    },
    {
        "loss": 2.5661,
        "grad_norm": 4.205708980560303,
        "learning_rate": 0.00019453609940174178,
        "epoch": 1.4054666666666666,
        "step": 10541
    },
    {
        "loss": 2.7915,
        "grad_norm": 2.7710163593292236,
        "learning_rate": 0.00019451556169888145,
        "epoch": 1.4056,
        "step": 10542
    },
    {
        "loss": 1.7651,
        "grad_norm": 5.933995723724365,
        "learning_rate": 0.000194494986558085,
        "epoch": 1.4057333333333333,
        "step": 10543
    },
    {
        "loss": 1.0014,
        "grad_norm": 5.23982048034668,
        "learning_rate": 0.00019447437398750238,
        "epoch": 1.4058666666666666,
        "step": 10544
    },
    {
        "loss": 2.0566,
        "grad_norm": 4.205584526062012,
        "learning_rate": 0.0001944537239952984,
        "epoch": 1.4060000000000001,
        "step": 10545
    },
    {
        "loss": 1.6874,
        "grad_norm": 3.0934336185455322,
        "learning_rate": 0.00019443303658965246,
        "epoch": 1.4061333333333335,
        "step": 10546
    },
    {
        "loss": 2.3707,
        "grad_norm": 4.807957649230957,
        "learning_rate": 0.000194412311778759,
        "epoch": 1.4062666666666668,
        "step": 10547
    },
    {
        "loss": 0.9765,
        "grad_norm": 4.073879718780518,
        "learning_rate": 0.00019439154957082703,
        "epoch": 1.4064,
        "step": 10548
    },
    {
        "loss": 1.0082,
        "grad_norm": 4.6257429122924805,
        "learning_rate": 0.00019437074997408075,
        "epoch": 1.4065333333333334,
        "step": 10549
    },
    {
        "loss": 1.2958,
        "grad_norm": 3.977205276489258,
        "learning_rate": 0.0001943499129967588,
        "epoch": 1.4066666666666667,
        "step": 10550
    },
    {
        "loss": 2.3104,
        "grad_norm": 3.4031453132629395,
        "learning_rate": 0.00019432903864711483,
        "epoch": 1.4068,
        "step": 10551
    },
    {
        "loss": 1.5151,
        "grad_norm": 4.441183567047119,
        "learning_rate": 0.00019430812693341722,
        "epoch": 1.4069333333333334,
        "step": 10552
    },
    {
        "loss": 2.7525,
        "grad_norm": 3.2254645824432373,
        "learning_rate": 0.00019428717786394915,
        "epoch": 1.4070666666666667,
        "step": 10553
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.6370670795440674,
        "learning_rate": 0.0001942661914470087,
        "epoch": 1.4072,
        "step": 10554
    },
    {
        "loss": 1.7754,
        "grad_norm": 2.711550235748291,
        "learning_rate": 0.00019424516769090864,
        "epoch": 1.4073333333333333,
        "step": 10555
    },
    {
        "loss": 0.7984,
        "grad_norm": 4.853646755218506,
        "learning_rate": 0.00019422410660397652,
        "epoch": 1.4074666666666666,
        "step": 10556
    },
    {
        "loss": 1.4608,
        "grad_norm": 4.902603626251221,
        "learning_rate": 0.00019420300819455462,
        "epoch": 1.4076,
        "step": 10557
    },
    {
        "loss": 2.6133,
        "grad_norm": 2.8667097091674805,
        "learning_rate": 0.00019418187247100032,
        "epoch": 1.4077333333333333,
        "step": 10558
    },
    {
        "loss": 2.2783,
        "grad_norm": 4.856099605560303,
        "learning_rate": 0.00019416069944168537,
        "epoch": 1.4078666666666666,
        "step": 10559
    },
    {
        "loss": 2.3739,
        "grad_norm": 2.947777271270752,
        "learning_rate": 0.0001941394891149965,
        "epoch": 1.408,
        "step": 10560
    },
    {
        "loss": 2.5942,
        "grad_norm": 3.0921199321746826,
        "learning_rate": 0.0001941182414993352,
        "epoch": 1.4081333333333332,
        "step": 10561
    },
    {
        "loss": 2.3472,
        "grad_norm": 3.5914857387542725,
        "learning_rate": 0.00019409695660311773,
        "epoch": 1.4082666666666666,
        "step": 10562
    },
    {
        "loss": 1.9864,
        "grad_norm": 3.6301324367523193,
        "learning_rate": 0.00019407563443477524,
        "epoch": 1.4083999999999999,
        "step": 10563
    },
    {
        "loss": 2.2449,
        "grad_norm": 4.385258197784424,
        "learning_rate": 0.00019405427500275327,
        "epoch": 1.4085333333333334,
        "step": 10564
    },
    {
        "loss": 0.783,
        "grad_norm": 3.4909229278564453,
        "learning_rate": 0.00019403287831551257,
        "epoch": 1.4086666666666667,
        "step": 10565
    },
    {
        "loss": 2.1555,
        "grad_norm": 3.852181911468506,
        "learning_rate": 0.00019401144438152828,
        "epoch": 1.4088,
        "step": 10566
    },
    {
        "loss": 2.4683,
        "grad_norm": 2.8998289108276367,
        "learning_rate": 0.0001939899732092906,
        "epoch": 1.4089333333333334,
        "step": 10567
    },
    {
        "loss": 2.6035,
        "grad_norm": 2.8081984519958496,
        "learning_rate": 0.00019396846480730426,
        "epoch": 1.4090666666666667,
        "step": 10568
    },
    {
        "loss": 2.4726,
        "grad_norm": 5.858536243438721,
        "learning_rate": 0.00019394691918408883,
        "epoch": 1.4092,
        "step": 10569
    },
    {
        "loss": 1.3399,
        "grad_norm": 6.96235466003418,
        "learning_rate": 0.00019392533634817854,
        "epoch": 1.4093333333333333,
        "step": 10570
    },
    {
        "loss": 1.5438,
        "grad_norm": 3.0414860248565674,
        "learning_rate": 0.00019390371630812248,
        "epoch": 1.4094666666666666,
        "step": 10571
    },
    {
        "loss": 1.1573,
        "grad_norm": 6.626624584197998,
        "learning_rate": 0.00019388205907248455,
        "epoch": 1.4096,
        "step": 10572
    },
    {
        "loss": 2.3417,
        "grad_norm": 3.851107597351074,
        "learning_rate": 0.00019386036464984297,
        "epoch": 1.4097333333333333,
        "step": 10573
    },
    {
        "loss": 1.0375,
        "grad_norm": 5.618655681610107,
        "learning_rate": 0.00019383863304879123,
        "epoch": 1.4098666666666666,
        "step": 10574
    },
    {
        "loss": 1.892,
        "grad_norm": 3.0318074226379395,
        "learning_rate": 0.0001938168642779371,
        "epoch": 1.41,
        "step": 10575
    },
    {
        "loss": 2.168,
        "grad_norm": 5.369265556335449,
        "learning_rate": 0.00019379505834590344,
        "epoch": 1.4101333333333335,
        "step": 10576
    },
    {
        "loss": 2.2382,
        "grad_norm": 3.2460994720458984,
        "learning_rate": 0.00019377321526132754,
        "epoch": 1.4102666666666668,
        "step": 10577
    },
    {
        "loss": 2.3635,
        "grad_norm": 4.6896748542785645,
        "learning_rate": 0.00019375133503286157,
        "epoch": 1.4104,
        "step": 10578
    },
    {
        "loss": 2.3812,
        "grad_norm": 3.974893093109131,
        "learning_rate": 0.0001937294176691723,
        "epoch": 1.4105333333333334,
        "step": 10579
    },
    {
        "loss": 2.5899,
        "grad_norm": 4.474954128265381,
        "learning_rate": 0.00019370746317894135,
        "epoch": 1.4106666666666667,
        "step": 10580
    },
    {
        "loss": 2.2109,
        "grad_norm": 4.537818431854248,
        "learning_rate": 0.00019368547157086493,
        "epoch": 1.4108,
        "step": 10581
    },
    {
        "loss": 1.4378,
        "grad_norm": 4.42191219329834,
        "learning_rate": 0.00019366344285365398,
        "epoch": 1.4109333333333334,
        "step": 10582
    },
    {
        "loss": 2.5994,
        "grad_norm": 4.182415962219238,
        "learning_rate": 0.0001936413770360342,
        "epoch": 1.4110666666666667,
        "step": 10583
    },
    {
        "loss": 2.0927,
        "grad_norm": 4.216601848602295,
        "learning_rate": 0.00019361927412674587,
        "epoch": 1.4112,
        "step": 10584
    },
    {
        "loss": 1.3259,
        "grad_norm": 3.7858290672302246,
        "learning_rate": 0.00019359713413454404,
        "epoch": 1.4113333333333333,
        "step": 10585
    },
    {
        "loss": 1.5422,
        "grad_norm": 7.5409746170043945,
        "learning_rate": 0.00019357495706819847,
        "epoch": 1.4114666666666666,
        "step": 10586
    },
    {
        "loss": 2.0686,
        "grad_norm": 3.8199336528778076,
        "learning_rate": 0.00019355274293649362,
        "epoch": 1.4116,
        "step": 10587
    },
    {
        "loss": 1.5466,
        "grad_norm": 2.6390697956085205,
        "learning_rate": 0.00019353049174822848,
        "epoch": 1.4117333333333333,
        "step": 10588
    },
    {
        "loss": 0.5298,
        "grad_norm": 2.511263132095337,
        "learning_rate": 0.00019350820351221682,
        "epoch": 1.4118666666666666,
        "step": 10589
    },
    {
        "loss": 2.5463,
        "grad_norm": 3.9393022060394287,
        "learning_rate": 0.00019348587823728717,
        "epoch": 1.412,
        "step": 10590
    },
    {
        "loss": 2.6087,
        "grad_norm": 3.4255521297454834,
        "learning_rate": 0.00019346351593228255,
        "epoch": 1.4121333333333332,
        "step": 10591
    },
    {
        "loss": 2.7339,
        "grad_norm": 4.188288688659668,
        "learning_rate": 0.00019344111660606086,
        "epoch": 1.4122666666666666,
        "step": 10592
    },
    {
        "loss": 2.6424,
        "grad_norm": 3.8306686878204346,
        "learning_rate": 0.00019341868026749449,
        "epoch": 1.4123999999999999,
        "step": 10593
    },
    {
        "loss": 2.3838,
        "grad_norm": 4.520009517669678,
        "learning_rate": 0.0001933962069254705,
        "epoch": 1.4125333333333332,
        "step": 10594
    },
    {
        "loss": 2.4141,
        "grad_norm": 4.135351181030273,
        "learning_rate": 0.00019337369658889073,
        "epoch": 1.4126666666666667,
        "step": 10595
    },
    {
        "loss": 2.5333,
        "grad_norm": 4.630098819732666,
        "learning_rate": 0.0001933511492666716,
        "epoch": 1.4128,
        "step": 10596
    },
    {
        "loss": 2.3173,
        "grad_norm": 5.0909342765808105,
        "learning_rate": 0.00019332856496774418,
        "epoch": 1.4129333333333334,
        "step": 10597
    },
    {
        "loss": 2.5287,
        "grad_norm": 2.943427801132202,
        "learning_rate": 0.00019330594370105408,
        "epoch": 1.4130666666666667,
        "step": 10598
    },
    {
        "loss": 2.4335,
        "grad_norm": 7.190823554992676,
        "learning_rate": 0.00019328328547556182,
        "epoch": 1.4132,
        "step": 10599
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.035731077194214,
        "learning_rate": 0.00019326059030024228,
        "epoch": 1.4133333333333333,
        "step": 10600
    },
    {
        "loss": 2.3477,
        "grad_norm": 4.250035762786865,
        "learning_rate": 0.00019323785818408523,
        "epoch": 1.4134666666666666,
        "step": 10601
    },
    {
        "loss": 2.4128,
        "grad_norm": 3.428022623062134,
        "learning_rate": 0.00019321508913609477,
        "epoch": 1.4136,
        "step": 10602
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.944727659225464,
        "learning_rate": 0.00019319228316528987,
        "epoch": 1.4137333333333333,
        "step": 10603
    },
    {
        "loss": 2.3371,
        "grad_norm": 3.5312490463256836,
        "learning_rate": 0.00019316944028070413,
        "epoch": 1.4138666666666666,
        "step": 10604
    },
    {
        "loss": 2.3837,
        "grad_norm": 5.7201247215271,
        "learning_rate": 0.0001931465604913856,
        "epoch": 1.414,
        "step": 10605
    },
    {
        "loss": 0.5666,
        "grad_norm": 3.5919017791748047,
        "learning_rate": 0.00019312364380639707,
        "epoch": 1.4141333333333335,
        "step": 10606
    },
    {
        "loss": 1.9924,
        "grad_norm": 3.5454506874084473,
        "learning_rate": 0.0001931006902348159,
        "epoch": 1.4142666666666668,
        "step": 10607
    },
    {
        "loss": 2.2652,
        "grad_norm": 9.550427436828613,
        "learning_rate": 0.0001930776997857341,
        "epoch": 1.4144,
        "step": 10608
    },
    {
        "loss": 1.4418,
        "grad_norm": 4.517014980316162,
        "learning_rate": 0.0001930546724682583,
        "epoch": 1.4145333333333334,
        "step": 10609
    },
    {
        "loss": 1.8869,
        "grad_norm": 2.9584100246429443,
        "learning_rate": 0.00019303160829150967,
        "epoch": 1.4146666666666667,
        "step": 10610
    },
    {
        "loss": 1.0427,
        "grad_norm": 4.126423358917236,
        "learning_rate": 0.0001930085072646239,
        "epoch": 1.4148,
        "step": 10611
    },
    {
        "loss": 2.3793,
        "grad_norm": 2.0931472778320312,
        "learning_rate": 0.0001929853693967515,
        "epoch": 1.4149333333333334,
        "step": 10612
    },
    {
        "loss": 1.9991,
        "grad_norm": 3.6993441581726074,
        "learning_rate": 0.0001929621946970575,
        "epoch": 1.4150666666666667,
        "step": 10613
    },
    {
        "loss": 2.4737,
        "grad_norm": 3.16092848777771,
        "learning_rate": 0.00019293898317472143,
        "epoch": 1.4152,
        "step": 10614
    },
    {
        "loss": 2.3653,
        "grad_norm": 3.4455084800720215,
        "learning_rate": 0.00019291573483893744,
        "epoch": 1.4153333333333333,
        "step": 10615
    },
    {
        "loss": 2.1378,
        "grad_norm": 4.459077835083008,
        "learning_rate": 0.0001928924496989142,
        "epoch": 1.4154666666666667,
        "step": 10616
    },
    {
        "loss": 2.6463,
        "grad_norm": 3.7737085819244385,
        "learning_rate": 0.0001928691277638752,
        "epoch": 1.4156,
        "step": 10617
    },
    {
        "loss": 2.3058,
        "grad_norm": 3.5040459632873535,
        "learning_rate": 0.0001928457690430582,
        "epoch": 1.4157333333333333,
        "step": 10618
    },
    {
        "loss": 2.0006,
        "grad_norm": 4.428775787353516,
        "learning_rate": 0.00019282237354571575,
        "epoch": 1.4158666666666666,
        "step": 10619
    },
    {
        "loss": 1.9267,
        "grad_norm": 4.492334365844727,
        "learning_rate": 0.0001927989412811148,
        "epoch": 1.416,
        "step": 10620
    },
    {
        "loss": 1.7902,
        "grad_norm": 3.861264228820801,
        "learning_rate": 0.000192775472258537,
        "epoch": 1.4161333333333332,
        "step": 10621
    },
    {
        "loss": 2.0858,
        "grad_norm": 4.2749176025390625,
        "learning_rate": 0.00019275196648727865,
        "epoch": 1.4162666666666666,
        "step": 10622
    },
    {
        "loss": 1.8053,
        "grad_norm": 5.264442443847656,
        "learning_rate": 0.00019272842397665018,
        "epoch": 1.4163999999999999,
        "step": 10623
    },
    {
        "loss": 2.1863,
        "grad_norm": 3.396242141723633,
        "learning_rate": 0.00019270484473597708,
        "epoch": 1.4165333333333332,
        "step": 10624
    },
    {
        "loss": 0.652,
        "grad_norm": 3.3672068119049072,
        "learning_rate": 0.00019268122877459903,
        "epoch": 1.4166666666666667,
        "step": 10625
    },
    {
        "loss": 1.6988,
        "grad_norm": 5.274233818054199,
        "learning_rate": 0.00019265757610187052,
        "epoch": 1.4168,
        "step": 10626
    },
    {
        "loss": 1.847,
        "grad_norm": 3.815831184387207,
        "learning_rate": 0.00019263388672716042,
        "epoch": 1.4169333333333334,
        "step": 10627
    },
    {
        "loss": 1.7931,
        "grad_norm": 4.6974005699157715,
        "learning_rate": 0.0001926101606598521,
        "epoch": 1.4170666666666667,
        "step": 10628
    },
    {
        "loss": 2.379,
        "grad_norm": 2.844620704650879,
        "learning_rate": 0.00019258639790934357,
        "epoch": 1.4172,
        "step": 10629
    },
    {
        "loss": 2.5655,
        "grad_norm": 4.968373775482178,
        "learning_rate": 0.00019256259848504737,
        "epoch": 1.4173333333333333,
        "step": 10630
    },
    {
        "loss": 0.8328,
        "grad_norm": 4.158462047576904,
        "learning_rate": 0.00019253876239639053,
        "epoch": 1.4174666666666667,
        "step": 10631
    },
    {
        "loss": 2.2391,
        "grad_norm": 3.682081937789917,
        "learning_rate": 0.00019251488965281452,
        "epoch": 1.4176,
        "step": 10632
    },
    {
        "loss": 2.1909,
        "grad_norm": 3.751288890838623,
        "learning_rate": 0.00019249098026377556,
        "epoch": 1.4177333333333333,
        "step": 10633
    },
    {
        "loss": 2.6618,
        "grad_norm": 3.076030731201172,
        "learning_rate": 0.00019246703423874407,
        "epoch": 1.4178666666666666,
        "step": 10634
    },
    {
        "loss": 1.8399,
        "grad_norm": 5.183881759643555,
        "learning_rate": 0.0001924430515872053,
        "epoch": 1.418,
        "step": 10635
    },
    {
        "loss": 2.3088,
        "grad_norm": 2.173929214477539,
        "learning_rate": 0.00019241903231865883,
        "epoch": 1.4181333333333335,
        "step": 10636
    },
    {
        "loss": 2.6904,
        "grad_norm": 4.753981590270996,
        "learning_rate": 0.0001923949764426187,
        "epoch": 1.4182666666666668,
        "step": 10637
    },
    {
        "loss": 1.6171,
        "grad_norm": 4.014896869659424,
        "learning_rate": 0.0001923708839686136,
        "epoch": 1.4184,
        "step": 10638
    },
    {
        "loss": 2.4782,
        "grad_norm": 4.52608585357666,
        "learning_rate": 0.00019234675490618664,
        "epoch": 1.4185333333333334,
        "step": 10639
    },
    {
        "loss": 3.0331,
        "grad_norm": 2.825093984603882,
        "learning_rate": 0.00019232258926489535,
        "epoch": 1.4186666666666667,
        "step": 10640
    },
    {
        "loss": 2.276,
        "grad_norm": 4.022964954376221,
        "learning_rate": 0.00019229838705431184,
        "epoch": 1.4188,
        "step": 10641
    },
    {
        "loss": 2.0194,
        "grad_norm": 3.3407511711120605,
        "learning_rate": 0.00019227414828402282,
        "epoch": 1.4189333333333334,
        "step": 10642
    },
    {
        "loss": 2.1898,
        "grad_norm": 4.8688459396362305,
        "learning_rate": 0.0001922498729636292,
        "epoch": 1.4190666666666667,
        "step": 10643
    },
    {
        "loss": 2.6636,
        "grad_norm": 5.183602333068848,
        "learning_rate": 0.0001922255611027465,
        "epoch": 1.4192,
        "step": 10644
    },
    {
        "loss": 2.5011,
        "grad_norm": 3.205751895904541,
        "learning_rate": 0.0001922012127110049,
        "epoch": 1.4193333333333333,
        "step": 10645
    },
    {
        "loss": 2.2786,
        "grad_norm": 3.0181968212127686,
        "learning_rate": 0.00019217682779804873,
        "epoch": 1.4194666666666667,
        "step": 10646
    },
    {
        "loss": 1.6538,
        "grad_norm": 2.5238020420074463,
        "learning_rate": 0.00019215240637353702,
        "epoch": 1.4196,
        "step": 10647
    },
    {
        "loss": 2.6903,
        "grad_norm": 2.4383456707000732,
        "learning_rate": 0.0001921279484471431,
        "epoch": 1.4197333333333333,
        "step": 10648
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.726682186126709,
        "learning_rate": 0.0001921034540285549,
        "epoch": 1.4198666666666666,
        "step": 10649
    },
    {
        "loss": 2.3493,
        "grad_norm": 3.2203564643859863,
        "learning_rate": 0.00019207892312747468,
        "epoch": 1.42,
        "step": 10650
    },
    {
        "loss": 2.468,
        "grad_norm": 2.902235984802246,
        "learning_rate": 0.00019205435575361937,
        "epoch": 1.4201333333333332,
        "step": 10651
    },
    {
        "loss": 2.0753,
        "grad_norm": 3.400062084197998,
        "learning_rate": 0.00019202975191671994,
        "epoch": 1.4202666666666666,
        "step": 10652
    },
    {
        "loss": 2.1791,
        "grad_norm": 3.557438850402832,
        "learning_rate": 0.00019200511162652224,
        "epoch": 1.4203999999999999,
        "step": 10653
    },
    {
        "loss": 2.0325,
        "grad_norm": 3.8033602237701416,
        "learning_rate": 0.00019198043489278635,
        "epoch": 1.4205333333333332,
        "step": 10654
    },
    {
        "loss": 1.5632,
        "grad_norm": 5.369497299194336,
        "learning_rate": 0.0001919557217252868,
        "epoch": 1.4206666666666667,
        "step": 10655
    },
    {
        "loss": 2.6819,
        "grad_norm": 3.8125205039978027,
        "learning_rate": 0.0001919309721338125,
        "epoch": 1.4208,
        "step": 10656
    },
    {
        "loss": 2.0001,
        "grad_norm": 3.9086620807647705,
        "learning_rate": 0.00019190618612816688,
        "epoch": 1.4209333333333334,
        "step": 10657
    },
    {
        "loss": 0.6865,
        "grad_norm": 3.518946647644043,
        "learning_rate": 0.00019188136371816784,
        "epoch": 1.4210666666666667,
        "step": 10658
    },
    {
        "loss": 2.0423,
        "grad_norm": 3.858283042907715,
        "learning_rate": 0.00019185650491364747,
        "epoch": 1.4212,
        "step": 10659
    },
    {
        "loss": 1.5684,
        "grad_norm": 4.717224597930908,
        "learning_rate": 0.00019183160972445262,
        "epoch": 1.4213333333333333,
        "step": 10660
    },
    {
        "loss": 2.5946,
        "grad_norm": 3.1607306003570557,
        "learning_rate": 0.00019180667816044413,
        "epoch": 1.4214666666666667,
        "step": 10661
    },
    {
        "loss": 3.0194,
        "grad_norm": 6.984025478363037,
        "learning_rate": 0.00019178171023149764,
        "epoch": 1.4216,
        "step": 10662
    },
    {
        "loss": 1.9582,
        "grad_norm": 4.125038146972656,
        "learning_rate": 0.000191756705947503,
        "epoch": 1.4217333333333333,
        "step": 10663
    },
    {
        "loss": 2.8206,
        "grad_norm": 3.5317540168762207,
        "learning_rate": 0.0001917316653183645,
        "epoch": 1.4218666666666666,
        "step": 10664
    },
    {
        "loss": 2.1915,
        "grad_norm": 4.103639125823975,
        "learning_rate": 0.00019170658835400078,
        "epoch": 1.422,
        "step": 10665
    },
    {
        "loss": 2.6568,
        "grad_norm": 4.727334022521973,
        "learning_rate": 0.0001916814750643449,
        "epoch": 1.4221333333333335,
        "step": 10666
    },
    {
        "loss": 2.6767,
        "grad_norm": 3.7111430168151855,
        "learning_rate": 0.00019165632545934443,
        "epoch": 1.4222666666666668,
        "step": 10667
    },
    {
        "loss": 1.6275,
        "grad_norm": 3.6718127727508545,
        "learning_rate": 0.0001916311395489611,
        "epoch": 1.4224,
        "step": 10668
    },
    {
        "loss": 0.949,
        "grad_norm": 4.11582612991333,
        "learning_rate": 0.00019160591734317114,
        "epoch": 1.4225333333333334,
        "step": 10669
    },
    {
        "loss": 1.4974,
        "grad_norm": 4.478520393371582,
        "learning_rate": 0.00019158065885196518,
        "epoch": 1.4226666666666667,
        "step": 10670
    },
    {
        "loss": 2.6468,
        "grad_norm": 3.1279659271240234,
        "learning_rate": 0.00019155536408534815,
        "epoch": 1.4228,
        "step": 10671
    },
    {
        "loss": 1.6457,
        "grad_norm": 4.179849147796631,
        "learning_rate": 0.00019153003305333953,
        "epoch": 1.4229333333333334,
        "step": 10672
    },
    {
        "loss": 1.9494,
        "grad_norm": 3.638859510421753,
        "learning_rate": 0.00019150466576597292,
        "epoch": 1.4230666666666667,
        "step": 10673
    },
    {
        "loss": 1.8261,
        "grad_norm": 4.220574855804443,
        "learning_rate": 0.0001914792622332964,
        "epoch": 1.4232,
        "step": 10674
    },
    {
        "loss": 2.4112,
        "grad_norm": 2.2380006313323975,
        "learning_rate": 0.00019145382246537235,
        "epoch": 1.4233333333333333,
        "step": 10675
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.6997978687286377,
        "learning_rate": 0.00019142834647227765,
        "epoch": 1.4234666666666667,
        "step": 10676
    },
    {
        "loss": 0.9673,
        "grad_norm": 3.8842785358428955,
        "learning_rate": 0.0001914028342641034,
        "epoch": 1.4236,
        "step": 10677
    },
    {
        "loss": 1.969,
        "grad_norm": 2.748570442199707,
        "learning_rate": 0.00019137728585095506,
        "epoch": 1.4237333333333333,
        "step": 10678
    },
    {
        "loss": 2.5178,
        "grad_norm": 3.1441450119018555,
        "learning_rate": 0.00019135170124295233,
        "epoch": 1.4238666666666666,
        "step": 10679
    },
    {
        "loss": 2.672,
        "grad_norm": 3.044973373413086,
        "learning_rate": 0.00019132608045022948,
        "epoch": 1.424,
        "step": 10680
    },
    {
        "loss": 2.2469,
        "grad_norm": 4.737809658050537,
        "learning_rate": 0.00019130042348293512,
        "epoch": 1.4241333333333333,
        "step": 10681
    },
    {
        "loss": 2.4892,
        "grad_norm": 4.1053466796875,
        "learning_rate": 0.0001912747303512318,
        "epoch": 1.4242666666666666,
        "step": 10682
    },
    {
        "loss": 1.906,
        "grad_norm": 3.4686989784240723,
        "learning_rate": 0.0001912490010652968,
        "epoch": 1.4243999999999999,
        "step": 10683
    },
    {
        "loss": 2.1692,
        "grad_norm": 5.452085494995117,
        "learning_rate": 0.00019122323563532155,
        "epoch": 1.4245333333333332,
        "step": 10684
    },
    {
        "loss": 1.9546,
        "grad_norm": 3.2170250415802,
        "learning_rate": 0.00019119743407151183,
        "epoch": 1.4246666666666667,
        "step": 10685
    },
    {
        "loss": 2.1456,
        "grad_norm": 4.146811485290527,
        "learning_rate": 0.00019117159638408775,
        "epoch": 1.4248,
        "step": 10686
    },
    {
        "loss": 0.9096,
        "grad_norm": 3.5009214878082275,
        "learning_rate": 0.0001911457225832837,
        "epoch": 1.4249333333333334,
        "step": 10687
    },
    {
        "loss": 1.5305,
        "grad_norm": 4.369009971618652,
        "learning_rate": 0.00019111981267934825,
        "epoch": 1.4250666666666667,
        "step": 10688
    },
    {
        "loss": 2.4879,
        "grad_norm": 2.305020570755005,
        "learning_rate": 0.00019109386668254464,
        "epoch": 1.4252,
        "step": 10689
    },
    {
        "loss": 1.2061,
        "grad_norm": 4.3666205406188965,
        "learning_rate": 0.00019106788460315001,
        "epoch": 1.4253333333333333,
        "step": 10690
    },
    {
        "loss": 2.4951,
        "grad_norm": 4.0039167404174805,
        "learning_rate": 0.00019104186645145594,
        "epoch": 1.4254666666666667,
        "step": 10691
    },
    {
        "loss": 0.956,
        "grad_norm": 3.5500450134277344,
        "learning_rate": 0.0001910158122377684,
        "epoch": 1.4256,
        "step": 10692
    },
    {
        "loss": 2.7349,
        "grad_norm": 2.0585174560546875,
        "learning_rate": 0.00019098972197240742,
        "epoch": 1.4257333333333333,
        "step": 10693
    },
    {
        "loss": 2.549,
        "grad_norm": 2.4375970363616943,
        "learning_rate": 0.00019096359566570762,
        "epoch": 1.4258666666666666,
        "step": 10694
    },
    {
        "loss": 2.5822,
        "grad_norm": 4.727666854858398,
        "learning_rate": 0.0001909374333280176,
        "epoch": 1.426,
        "step": 10695
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.5470759868621826,
        "learning_rate": 0.00019091123496970042,
        "epoch": 1.4261333333333333,
        "step": 10696
    },
    {
        "loss": 2.7885,
        "grad_norm": 4.607827186584473,
        "learning_rate": 0.00019088500060113322,
        "epoch": 1.4262666666666668,
        "step": 10697
    },
    {
        "loss": 2.046,
        "grad_norm": 3.9545743465423584,
        "learning_rate": 0.00019085873023270763,
        "epoch": 1.4264000000000001,
        "step": 10698
    },
    {
        "loss": 1.6699,
        "grad_norm": 4.520174026489258,
        "learning_rate": 0.00019083242387482943,
        "epoch": 1.4265333333333334,
        "step": 10699
    },
    {
        "loss": 1.9868,
        "grad_norm": 3.537672996520996,
        "learning_rate": 0.00019080608153791855,
        "epoch": 1.4266666666666667,
        "step": 10700
    },
    {
        "loss": 1.8518,
        "grad_norm": 4.979379177093506,
        "learning_rate": 0.00019077970323240942,
        "epoch": 1.4268,
        "step": 10701
    },
    {
        "loss": 2.165,
        "grad_norm": 3.768664836883545,
        "learning_rate": 0.00019075328896875053,
        "epoch": 1.4269333333333334,
        "step": 10702
    },
    {
        "loss": 1.2733,
        "grad_norm": 2.416504144668579,
        "learning_rate": 0.00019072683875740457,
        "epoch": 1.4270666666666667,
        "step": 10703
    },
    {
        "loss": 2.464,
        "grad_norm": 3.979529619216919,
        "learning_rate": 0.00019070035260884872,
        "epoch": 1.4272,
        "step": 10704
    },
    {
        "loss": 2.5319,
        "grad_norm": 2.2425382137298584,
        "learning_rate": 0.0001906738305335741,
        "epoch": 1.4273333333333333,
        "step": 10705
    },
    {
        "loss": 2.6219,
        "grad_norm": 5.220915794372559,
        "learning_rate": 0.0001906472725420863,
        "epoch": 1.4274666666666667,
        "step": 10706
    },
    {
        "loss": 2.7192,
        "grad_norm": 2.7016942501068115,
        "learning_rate": 0.00019062067864490488,
        "epoch": 1.4276,
        "step": 10707
    },
    {
        "loss": 2.0658,
        "grad_norm": 4.43166446685791,
        "learning_rate": 0.00019059404885256393,
        "epoch": 1.4277333333333333,
        "step": 10708
    },
    {
        "loss": 2.2954,
        "grad_norm": 4.622747421264648,
        "learning_rate": 0.00019056738317561146,
        "epoch": 1.4278666666666666,
        "step": 10709
    },
    {
        "loss": 1.284,
        "grad_norm": 3.219071626663208,
        "learning_rate": 0.00019054068162461008,
        "epoch": 1.428,
        "step": 10710
    },
    {
        "loss": 3.0824,
        "grad_norm": 3.1568634510040283,
        "learning_rate": 0.00019051394421013603,
        "epoch": 1.4281333333333333,
        "step": 10711
    },
    {
        "loss": 1.8957,
        "grad_norm": 3.6881701946258545,
        "learning_rate": 0.00019048717094278028,
        "epoch": 1.4282666666666666,
        "step": 10712
    },
    {
        "loss": 1.7266,
        "grad_norm": 5.5105085372924805,
        "learning_rate": 0.00019046036183314785,
        "epoch": 1.4284,
        "step": 10713
    },
    {
        "loss": 2.5548,
        "grad_norm": 3.3340508937835693,
        "learning_rate": 0.0001904335168918579,
        "epoch": 1.4285333333333332,
        "step": 10714
    },
    {
        "loss": 1.6925,
        "grad_norm": 2.6982643604278564,
        "learning_rate": 0.00019040663612954371,
        "epoch": 1.4286666666666665,
        "step": 10715
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.5839624404907227,
        "learning_rate": 0.00019037971955685286,
        "epoch": 1.4288,
        "step": 10716
    },
    {
        "loss": 2.2481,
        "grad_norm": 3.675487518310547,
        "learning_rate": 0.00019035276718444718,
        "epoch": 1.4289333333333334,
        "step": 10717
    },
    {
        "loss": 2.7145,
        "grad_norm": 2.582240343093872,
        "learning_rate": 0.0001903257790230025,
        "epoch": 1.4290666666666667,
        "step": 10718
    },
    {
        "loss": 1.3105,
        "grad_norm": 3.3304200172424316,
        "learning_rate": 0.00019029875508320908,
        "epoch": 1.4292,
        "step": 10719
    },
    {
        "loss": 1.0175,
        "grad_norm": 4.410892963409424,
        "learning_rate": 0.00019027169537577096,
        "epoch": 1.4293333333333333,
        "step": 10720
    },
    {
        "loss": 2.2326,
        "grad_norm": 4.337316989898682,
        "learning_rate": 0.00019024459991140674,
        "epoch": 1.4294666666666667,
        "step": 10721
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.3038222789764404,
        "learning_rate": 0.00019021746870084903,
        "epoch": 1.4296,
        "step": 10722
    },
    {
        "loss": 1.3463,
        "grad_norm": 2.2091851234436035,
        "learning_rate": 0.00019019030175484458,
        "epoch": 1.4297333333333333,
        "step": 10723
    },
    {
        "loss": 2.1903,
        "grad_norm": 3.1866302490234375,
        "learning_rate": 0.00019016309908415427,
        "epoch": 1.4298666666666666,
        "step": 10724
    },
    {
        "loss": 2.9111,
        "grad_norm": 4.090183258056641,
        "learning_rate": 0.00019013586069955313,
        "epoch": 1.43,
        "step": 10725
    },
    {
        "loss": 1.4851,
        "grad_norm": 4.340925693511963,
        "learning_rate": 0.00019010858661183048,
        "epoch": 1.4301333333333333,
        "step": 10726
    },
    {
        "loss": 1.9748,
        "grad_norm": 3.4137251377105713,
        "learning_rate": 0.00019008127683178965,
        "epoch": 1.4302666666666668,
        "step": 10727
    },
    {
        "loss": 1.6888,
        "grad_norm": 4.517080307006836,
        "learning_rate": 0.00019005393137024813,
        "epoch": 1.4304000000000001,
        "step": 10728
    },
    {
        "loss": 2.9529,
        "grad_norm": 3.9880881309509277,
        "learning_rate": 0.0001900265502380375,
        "epoch": 1.4305333333333334,
        "step": 10729
    },
    {
        "loss": 2.5105,
        "grad_norm": 3.619204044342041,
        "learning_rate": 0.00018999913344600354,
        "epoch": 1.4306666666666668,
        "step": 10730
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.7969958782196045,
        "learning_rate": 0.00018997168100500628,
        "epoch": 1.4308,
        "step": 10731
    },
    {
        "loss": 1.6009,
        "grad_norm": 4.033936977386475,
        "learning_rate": 0.00018994419292591949,
        "epoch": 1.4309333333333334,
        "step": 10732
    },
    {
        "loss": 2.5426,
        "grad_norm": 3.525258779525757,
        "learning_rate": 0.00018991666921963144,
        "epoch": 1.4310666666666667,
        "step": 10733
    },
    {
        "loss": 3.2473,
        "grad_norm": 3.561774253845215,
        "learning_rate": 0.0001898891098970443,
        "epoch": 1.4312,
        "step": 10734
    },
    {
        "loss": 2.2062,
        "grad_norm": 3.4441888332366943,
        "learning_rate": 0.00018986151496907448,
        "epoch": 1.4313333333333333,
        "step": 10735
    },
    {
        "loss": 1.9034,
        "grad_norm": 5.133240222930908,
        "learning_rate": 0.0001898338844466524,
        "epoch": 1.4314666666666667,
        "step": 10736
    },
    {
        "loss": 2.949,
        "grad_norm": 3.811643123626709,
        "learning_rate": 0.00018980621834072258,
        "epoch": 1.4316,
        "step": 10737
    },
    {
        "loss": 2.5403,
        "grad_norm": 3.524604082107544,
        "learning_rate": 0.00018977851666224363,
        "epoch": 1.4317333333333333,
        "step": 10738
    },
    {
        "loss": 1.4908,
        "grad_norm": 2.8092715740203857,
        "learning_rate": 0.00018975077942218828,
        "epoch": 1.4318666666666666,
        "step": 10739
    },
    {
        "loss": 1.1234,
        "grad_norm": 5.9354352951049805,
        "learning_rate": 0.00018972300663154353,
        "epoch": 1.432,
        "step": 10740
    },
    {
        "loss": 0.6699,
        "grad_norm": 3.784536123275757,
        "learning_rate": 0.00018969519830131,
        "epoch": 1.4321333333333333,
        "step": 10741
    },
    {
        "loss": 2.4996,
        "grad_norm": 3.094057321548462,
        "learning_rate": 0.00018966735444250285,
        "epoch": 1.4322666666666666,
        "step": 10742
    },
    {
        "loss": 1.5342,
        "grad_norm": 4.370885372161865,
        "learning_rate": 0.00018963947506615101,
        "epoch": 1.4324,
        "step": 10743
    },
    {
        "loss": 1.7596,
        "grad_norm": 3.527895450592041,
        "learning_rate": 0.00018961156018329774,
        "epoch": 1.4325333333333332,
        "step": 10744
    },
    {
        "loss": 2.4153,
        "grad_norm": 4.104220867156982,
        "learning_rate": 0.00018958360980500012,
        "epoch": 1.4326666666666665,
        "step": 10745
    },
    {
        "loss": 2.0016,
        "grad_norm": 3.73533034324646,
        "learning_rate": 0.00018955562394232939,
        "epoch": 1.4328,
        "step": 10746
    },
    {
        "loss": 1.966,
        "grad_norm": 4.837458610534668,
        "learning_rate": 0.00018952760260637083,
        "epoch": 1.4329333333333334,
        "step": 10747
    },
    {
        "loss": 1.4055,
        "grad_norm": 6.947359561920166,
        "learning_rate": 0.00018949954580822388,
        "epoch": 1.4330666666666667,
        "step": 10748
    },
    {
        "loss": 1.7542,
        "grad_norm": 3.8480846881866455,
        "learning_rate": 0.00018947145355900185,
        "epoch": 1.4332,
        "step": 10749
    },
    {
        "loss": 2.3263,
        "grad_norm": 2.144740343093872,
        "learning_rate": 0.00018944332586983214,
        "epoch": 1.4333333333333333,
        "step": 10750
    },
    {
        "loss": 1.8188,
        "grad_norm": 4.658660888671875,
        "learning_rate": 0.00018941516275185636,
        "epoch": 1.4334666666666667,
        "step": 10751
    },
    {
        "loss": 2.1639,
        "grad_norm": 3.76846981048584,
        "learning_rate": 0.00018938696421622994,
        "epoch": 1.4336,
        "step": 10752
    },
    {
        "loss": 2.9099,
        "grad_norm": 5.845498561859131,
        "learning_rate": 0.00018935873027412235,
        "epoch": 1.4337333333333333,
        "step": 10753
    },
    {
        "loss": 2.7165,
        "grad_norm": 2.7425289154052734,
        "learning_rate": 0.00018933046093671727,
        "epoch": 1.4338666666666666,
        "step": 10754
    },
    {
        "loss": 1.957,
        "grad_norm": 3.6765780448913574,
        "learning_rate": 0.00018930215621521223,
        "epoch": 1.434,
        "step": 10755
    },
    {
        "loss": 2.4747,
        "grad_norm": 4.367930889129639,
        "learning_rate": 0.00018927381612081877,
        "epoch": 1.4341333333333333,
        "step": 10756
    },
    {
        "loss": 2.2363,
        "grad_norm": 5.248530387878418,
        "learning_rate": 0.00018924544066476262,
        "epoch": 1.4342666666666668,
        "step": 10757
    },
    {
        "loss": 1.9662,
        "grad_norm": 4.080595970153809,
        "learning_rate": 0.0001892170298582833,
        "epoch": 1.4344000000000001,
        "step": 10758
    },
    {
        "loss": 2.4657,
        "grad_norm": 3.3622639179229736,
        "learning_rate": 0.00018918858371263443,
        "epoch": 1.4345333333333334,
        "step": 10759
    },
    {
        "loss": 1.9361,
        "grad_norm": 5.094418525695801,
        "learning_rate": 0.00018916010223908364,
        "epoch": 1.4346666666666668,
        "step": 10760
    },
    {
        "loss": 1.9674,
        "grad_norm": 3.687269449234009,
        "learning_rate": 0.00018913158544891262,
        "epoch": 1.4348,
        "step": 10761
    },
    {
        "loss": 2.2179,
        "grad_norm": 2.987194538116455,
        "learning_rate": 0.00018910303335341683,
        "epoch": 1.4349333333333334,
        "step": 10762
    },
    {
        "loss": 2.1018,
        "grad_norm": 3.3948135375976562,
        "learning_rate": 0.0001890744459639059,
        "epoch": 1.4350666666666667,
        "step": 10763
    },
    {
        "loss": 1.0604,
        "grad_norm": 5.772222995758057,
        "learning_rate": 0.0001890458232917035,
        "epoch": 1.4352,
        "step": 10764
    },
    {
        "loss": 2.0514,
        "grad_norm": 4.5643792152404785,
        "learning_rate": 0.00018901716534814705,
        "epoch": 1.4353333333333333,
        "step": 10765
    },
    {
        "loss": 2.3842,
        "grad_norm": 4.385972023010254,
        "learning_rate": 0.00018898847214458798,
        "epoch": 1.4354666666666667,
        "step": 10766
    },
    {
        "loss": 2.0733,
        "grad_norm": 4.711549282073975,
        "learning_rate": 0.000188959743692392,
        "epoch": 1.4356,
        "step": 10767
    },
    {
        "loss": 2.0499,
        "grad_norm": 7.891789436340332,
        "learning_rate": 0.0001889309800029383,
        "epoch": 1.4357333333333333,
        "step": 10768
    },
    {
        "loss": 2.2693,
        "grad_norm": 4.616641998291016,
        "learning_rate": 0.00018890218108762058,
        "epoch": 1.4358666666666666,
        "step": 10769
    },
    {
        "loss": 2.5324,
        "grad_norm": 4.6256914138793945,
        "learning_rate": 0.00018887334695784586,
        "epoch": 1.436,
        "step": 10770
    },
    {
        "loss": 2.1015,
        "grad_norm": 3.6798324584960938,
        "learning_rate": 0.00018884447762503557,
        "epoch": 1.4361333333333333,
        "step": 10771
    },
    {
        "loss": 2.8234,
        "grad_norm": 4.918757915496826,
        "learning_rate": 0.00018881557310062504,
        "epoch": 1.4362666666666666,
        "step": 10772
    },
    {
        "loss": 2.4687,
        "grad_norm": 2.3480427265167236,
        "learning_rate": 0.00018878663339606336,
        "epoch": 1.4364,
        "step": 10773
    },
    {
        "loss": 1.9699,
        "grad_norm": 3.912605047225952,
        "learning_rate": 0.00018875765852281365,
        "epoch": 1.4365333333333332,
        "step": 10774
    },
    {
        "loss": 0.5997,
        "grad_norm": 3.2966976165771484,
        "learning_rate": 0.00018872864849235293,
        "epoch": 1.4366666666666665,
        "step": 10775
    },
    {
        "loss": 1.5401,
        "grad_norm": 6.002673149108887,
        "learning_rate": 0.00018869960331617224,
        "epoch": 1.4368,
        "step": 10776
    },
    {
        "loss": 2.0143,
        "grad_norm": 2.9394497871398926,
        "learning_rate": 0.00018867052300577638,
        "epoch": 1.4369333333333334,
        "step": 10777
    },
    {
        "loss": 1.848,
        "grad_norm": 4.966421127319336,
        "learning_rate": 0.00018864140757268437,
        "epoch": 1.4370666666666667,
        "step": 10778
    },
    {
        "loss": 2.0108,
        "grad_norm": 2.9912283420562744,
        "learning_rate": 0.00018861225702842863,
        "epoch": 1.4372,
        "step": 10779
    },
    {
        "loss": 2.1814,
        "grad_norm": 4.505640983581543,
        "learning_rate": 0.00018858307138455596,
        "epoch": 1.4373333333333334,
        "step": 10780
    },
    {
        "loss": 1.8041,
        "grad_norm": 4.2182464599609375,
        "learning_rate": 0.00018855385065262695,
        "epoch": 1.4374666666666667,
        "step": 10781
    },
    {
        "loss": 2.7107,
        "grad_norm": 3.1174070835113525,
        "learning_rate": 0.00018852459484421593,
        "epoch": 1.4376,
        "step": 10782
    },
    {
        "loss": 0.9365,
        "grad_norm": 8.302437782287598,
        "learning_rate": 0.00018849530397091127,
        "epoch": 1.4377333333333333,
        "step": 10783
    },
    {
        "loss": 2.7831,
        "grad_norm": 5.082334518432617,
        "learning_rate": 0.00018846597804431507,
        "epoch": 1.4378666666666666,
        "step": 10784
    },
    {
        "loss": 2.4722,
        "grad_norm": 3.728872776031494,
        "learning_rate": 0.00018843661707604363,
        "epoch": 1.438,
        "step": 10785
    },
    {
        "loss": 2.0906,
        "grad_norm": 4.534241676330566,
        "learning_rate": 0.00018840722107772682,
        "epoch": 1.4381333333333333,
        "step": 10786
    },
    {
        "loss": 1.2066,
        "grad_norm": 5.312100887298584,
        "learning_rate": 0.0001883777900610085,
        "epoch": 1.4382666666666668,
        "step": 10787
    },
    {
        "loss": 1.7268,
        "grad_norm": 4.731456279754639,
        "learning_rate": 0.00018834832403754633,
        "epoch": 1.4384000000000001,
        "step": 10788
    },
    {
        "loss": 2.0222,
        "grad_norm": 3.874924898147583,
        "learning_rate": 0.00018831882301901195,
        "epoch": 1.4385333333333334,
        "step": 10789
    },
    {
        "loss": 2.2106,
        "grad_norm": 3.6660642623901367,
        "learning_rate": 0.00018828928701709096,
        "epoch": 1.4386666666666668,
        "step": 10790
    },
    {
        "loss": 2.6775,
        "grad_norm": 4.094704627990723,
        "learning_rate": 0.00018825971604348243,
        "epoch": 1.4388,
        "step": 10791
    },
    {
        "loss": 1.2365,
        "grad_norm": 5.681252956390381,
        "learning_rate": 0.00018823011010989972,
        "epoch": 1.4389333333333334,
        "step": 10792
    },
    {
        "loss": 2.2267,
        "grad_norm": 5.806280612945557,
        "learning_rate": 0.00018820046922806967,
        "epoch": 1.4390666666666667,
        "step": 10793
    },
    {
        "loss": 1.5223,
        "grad_norm": 3.2456488609313965,
        "learning_rate": 0.0001881707934097333,
        "epoch": 1.4392,
        "step": 10794
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.9998443126678467,
        "learning_rate": 0.00018814108266664522,
        "epoch": 1.4393333333333334,
        "step": 10795
    },
    {
        "loss": 1.9601,
        "grad_norm": 5.2374467849731445,
        "learning_rate": 0.00018811133701057396,
        "epoch": 1.4394666666666667,
        "step": 10796
    },
    {
        "loss": 2.4062,
        "grad_norm": 2.76466703414917,
        "learning_rate": 0.0001880815564533018,
        "epoch": 1.4396,
        "step": 10797
    },
    {
        "loss": 2.2317,
        "grad_norm": 5.2444939613342285,
        "learning_rate": 0.00018805174100662502,
        "epoch": 1.4397333333333333,
        "step": 10798
    },
    {
        "loss": 1.6093,
        "grad_norm": 4.889084339141846,
        "learning_rate": 0.00018802189068235374,
        "epoch": 1.4398666666666666,
        "step": 10799
    },
    {
        "loss": 1.9321,
        "grad_norm": 4.692442893981934,
        "learning_rate": 0.0001879920054923115,
        "epoch": 1.44,
        "step": 10800
    },
    {
        "loss": 1.9753,
        "grad_norm": 5.179899215698242,
        "learning_rate": 0.00018796208544833606,
        "epoch": 1.4401333333333333,
        "step": 10801
    },
    {
        "loss": 2.4951,
        "grad_norm": 3.121631383895874,
        "learning_rate": 0.00018793213056227885,
        "epoch": 1.4402666666666666,
        "step": 10802
    },
    {
        "loss": 2.0515,
        "grad_norm": 4.603891372680664,
        "learning_rate": 0.00018790214084600515,
        "epoch": 1.4404,
        "step": 10803
    },
    {
        "loss": 2.1938,
        "grad_norm": 3.398083448410034,
        "learning_rate": 0.00018787211631139395,
        "epoch": 1.4405333333333332,
        "step": 10804
    },
    {
        "loss": 1.9228,
        "grad_norm": 3.8860745429992676,
        "learning_rate": 0.00018784205697033808,
        "epoch": 1.4406666666666665,
        "step": 10805
    },
    {
        "loss": 2.4114,
        "grad_norm": 3.800814390182495,
        "learning_rate": 0.00018781196283474403,
        "epoch": 1.4408,
        "step": 10806
    },
    {
        "loss": 2.1698,
        "grad_norm": 4.030659198760986,
        "learning_rate": 0.00018778183391653235,
        "epoch": 1.4409333333333334,
        "step": 10807
    },
    {
        "loss": 1.3015,
        "grad_norm": 4.196597099304199,
        "learning_rate": 0.00018775167022763717,
        "epoch": 1.4410666666666667,
        "step": 10808
    },
    {
        "loss": 1.8967,
        "grad_norm": 2.0979316234588623,
        "learning_rate": 0.00018772147178000635,
        "epoch": 1.4412,
        "step": 10809
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.30247163772583,
        "learning_rate": 0.0001876912385856017,
        "epoch": 1.4413333333333334,
        "step": 10810
    },
    {
        "loss": 1.9777,
        "grad_norm": 3.388339042663574,
        "learning_rate": 0.00018766097065639863,
        "epoch": 1.4414666666666667,
        "step": 10811
    },
    {
        "loss": 2.8374,
        "grad_norm": 2.829770565032959,
        "learning_rate": 0.00018763066800438636,
        "epoch": 1.4416,
        "step": 10812
    },
    {
        "loss": 2.6238,
        "grad_norm": 4.3419365882873535,
        "learning_rate": 0.00018760033064156795,
        "epoch": 1.4417333333333333,
        "step": 10813
    },
    {
        "loss": 1.8665,
        "grad_norm": 4.242242336273193,
        "learning_rate": 0.00018756995857996004,
        "epoch": 1.4418666666666666,
        "step": 10814
    },
    {
        "loss": 2.2686,
        "grad_norm": 5.100877285003662,
        "learning_rate": 0.00018753955183159314,
        "epoch": 1.442,
        "step": 10815
    },
    {
        "loss": 3.0046,
        "grad_norm": 4.0892181396484375,
        "learning_rate": 0.0001875091104085115,
        "epoch": 1.4421333333333333,
        "step": 10816
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.507927179336548,
        "learning_rate": 0.00018747863432277308,
        "epoch": 1.4422666666666666,
        "step": 10817
    },
    {
        "loss": 2.2868,
        "grad_norm": 4.35629415512085,
        "learning_rate": 0.00018744812358644945,
        "epoch": 1.4424000000000001,
        "step": 10818
    },
    {
        "loss": 2.2605,
        "grad_norm": 3.4686598777770996,
        "learning_rate": 0.00018741757821162617,
        "epoch": 1.4425333333333334,
        "step": 10819
    },
    {
        "loss": 1.5083,
        "grad_norm": 4.89995813369751,
        "learning_rate": 0.00018738699821040227,
        "epoch": 1.4426666666666668,
        "step": 10820
    },
    {
        "loss": 1.7104,
        "grad_norm": 3.475261688232422,
        "learning_rate": 0.00018735638359489057,
        "epoch": 1.4428,
        "step": 10821
    },
    {
        "loss": 1.7306,
        "grad_norm": 5.079243183135986,
        "learning_rate": 0.00018732573437721771,
        "epoch": 1.4429333333333334,
        "step": 10822
    },
    {
        "loss": 2.075,
        "grad_norm": 2.558347225189209,
        "learning_rate": 0.00018729505056952394,
        "epoch": 1.4430666666666667,
        "step": 10823
    },
    {
        "loss": 2.1681,
        "grad_norm": 3.6412839889526367,
        "learning_rate": 0.0001872643321839632,
        "epoch": 1.4432,
        "step": 10824
    },
    {
        "loss": 2.1029,
        "grad_norm": 2.6537973880767822,
        "learning_rate": 0.00018723357923270304,
        "epoch": 1.4433333333333334,
        "step": 10825
    },
    {
        "loss": 2.2156,
        "grad_norm": 3.377835988998413,
        "learning_rate": 0.00018720279172792497,
        "epoch": 1.4434666666666667,
        "step": 10826
    },
    {
        "loss": 1.9988,
        "grad_norm": 3.791572332382202,
        "learning_rate": 0.00018717196968182388,
        "epoch": 1.4436,
        "step": 10827
    },
    {
        "loss": 2.3306,
        "grad_norm": 2.650426149368286,
        "learning_rate": 0.00018714111310660873,
        "epoch": 1.4437333333333333,
        "step": 10828
    },
    {
        "loss": 2.1887,
        "grad_norm": 3.938476324081421,
        "learning_rate": 0.0001871102220145016,
        "epoch": 1.4438666666666666,
        "step": 10829
    },
    {
        "loss": 2.6913,
        "grad_norm": 3.8163304328918457,
        "learning_rate": 0.0001870792964177387,
        "epoch": 1.444,
        "step": 10830
    },
    {
        "loss": 2.0439,
        "grad_norm": 3.5939297676086426,
        "learning_rate": 0.0001870483363285699,
        "epoch": 1.4441333333333333,
        "step": 10831
    },
    {
        "loss": 2.0872,
        "grad_norm": 4.406975746154785,
        "learning_rate": 0.00018701734175925841,
        "epoch": 1.4442666666666666,
        "step": 10832
    },
    {
        "loss": 1.9746,
        "grad_norm": 5.948700428009033,
        "learning_rate": 0.00018698631272208137,
        "epoch": 1.4444,
        "step": 10833
    },
    {
        "loss": 1.757,
        "grad_norm": 3.6475188732147217,
        "learning_rate": 0.00018695524922932938,
        "epoch": 1.4445333333333332,
        "step": 10834
    },
    {
        "loss": 2.2071,
        "grad_norm": 3.518528699874878,
        "learning_rate": 0.00018692415129330697,
        "epoch": 1.4446666666666665,
        "step": 10835
    },
    {
        "loss": 0.825,
        "grad_norm": 3.5605878829956055,
        "learning_rate": 0.00018689301892633195,
        "epoch": 1.4447999999999999,
        "step": 10836
    },
    {
        "loss": 2.4132,
        "grad_norm": 2.6372737884521484,
        "learning_rate": 0.00018686185214073623,
        "epoch": 1.4449333333333334,
        "step": 10837
    },
    {
        "loss": 2.54,
        "grad_norm": 4.2276201248168945,
        "learning_rate": 0.00018683065094886472,
        "epoch": 1.4450666666666667,
        "step": 10838
    },
    {
        "loss": 2.4763,
        "grad_norm": 4.246744155883789,
        "learning_rate": 0.0001867994153630765,
        "epoch": 1.4452,
        "step": 10839
    },
    {
        "loss": 1.1169,
        "grad_norm": 3.6627511978149414,
        "learning_rate": 0.00018676814539574422,
        "epoch": 1.4453333333333334,
        "step": 10840
    },
    {
        "loss": 2.2648,
        "grad_norm": 4.394289493560791,
        "learning_rate": 0.00018673684105925383,
        "epoch": 1.4454666666666667,
        "step": 10841
    },
    {
        "loss": 2.2573,
        "grad_norm": 5.2197184562683105,
        "learning_rate": 0.00018670550236600517,
        "epoch": 1.4456,
        "step": 10842
    },
    {
        "loss": 2.212,
        "grad_norm": 4.8795342445373535,
        "learning_rate": 0.00018667412932841151,
        "epoch": 1.4457333333333333,
        "step": 10843
    },
    {
        "loss": 2.3413,
        "grad_norm": 3.7159180641174316,
        "learning_rate": 0.0001866427219589,
        "epoch": 1.4458666666666666,
        "step": 10844
    },
    {
        "loss": 1.6773,
        "grad_norm": 6.600116729736328,
        "learning_rate": 0.000186611280269911,
        "epoch": 1.446,
        "step": 10845
    },
    {
        "loss": 2.3038,
        "grad_norm": 6.262161731719971,
        "learning_rate": 0.00018657980427389884,
        "epoch": 1.4461333333333333,
        "step": 10846
    },
    {
        "loss": 2.4755,
        "grad_norm": 5.9673848152160645,
        "learning_rate": 0.0001865482939833311,
        "epoch": 1.4462666666666666,
        "step": 10847
    },
    {
        "loss": 2.7776,
        "grad_norm": 2.452252149581909,
        "learning_rate": 0.0001865167494106892,
        "epoch": 1.4464000000000001,
        "step": 10848
    },
    {
        "loss": 1.744,
        "grad_norm": 4.2212114334106445,
        "learning_rate": 0.00018648517056846822,
        "epoch": 1.4465333333333334,
        "step": 10849
    },
    {
        "loss": 2.2678,
        "grad_norm": 3.597853660583496,
        "learning_rate": 0.00018645355746917632,
        "epoch": 1.4466666666666668,
        "step": 10850
    },
    {
        "loss": 2.5978,
        "grad_norm": 2.6033825874328613,
        "learning_rate": 0.00018642191012533584,
        "epoch": 1.4468,
        "step": 10851
    },
    {
        "loss": 2.9309,
        "grad_norm": 4.18062686920166,
        "learning_rate": 0.00018639022854948216,
        "epoch": 1.4469333333333334,
        "step": 10852
    },
    {
        "loss": 1.749,
        "grad_norm": 3.8676600456237793,
        "learning_rate": 0.0001863585127541647,
        "epoch": 1.4470666666666667,
        "step": 10853
    },
    {
        "loss": 1.9854,
        "grad_norm": 4.974432945251465,
        "learning_rate": 0.00018632676275194605,
        "epoch": 1.4472,
        "step": 10854
    },
    {
        "loss": 2.3732,
        "grad_norm": 3.183100461959839,
        "learning_rate": 0.00018629497855540252,
        "epoch": 1.4473333333333334,
        "step": 10855
    },
    {
        "loss": 2.0903,
        "grad_norm": 3.4066855907440186,
        "learning_rate": 0.00018626316017712389,
        "epoch": 1.4474666666666667,
        "step": 10856
    },
    {
        "loss": 0.9413,
        "grad_norm": 4.898577690124512,
        "learning_rate": 0.00018623130762971356,
        "epoch": 1.4476,
        "step": 10857
    },
    {
        "loss": 3.7399,
        "grad_norm": 5.961101055145264,
        "learning_rate": 0.0001861994209257886,
        "epoch": 1.4477333333333333,
        "step": 10858
    },
    {
        "loss": 2.6529,
        "grad_norm": 2.189587354660034,
        "learning_rate": 0.00018616750007797916,
        "epoch": 1.4478666666666666,
        "step": 10859
    },
    {
        "loss": 2.2191,
        "grad_norm": 4.5261549949646,
        "learning_rate": 0.0001861355450989294,
        "epoch": 1.448,
        "step": 10860
    },
    {
        "loss": 1.4245,
        "grad_norm": 2.951484203338623,
        "learning_rate": 0.00018610355600129664,
        "epoch": 1.4481333333333333,
        "step": 10861
    },
    {
        "loss": 1.555,
        "grad_norm": 4.071244716644287,
        "learning_rate": 0.00018607153279775207,
        "epoch": 1.4482666666666666,
        "step": 10862
    },
    {
        "loss": 2.2064,
        "grad_norm": 3.985037088394165,
        "learning_rate": 0.00018603947550098005,
        "epoch": 1.4484,
        "step": 10863
    },
    {
        "loss": 2.0732,
        "grad_norm": 4.801498889923096,
        "learning_rate": 0.0001860073841236786,
        "epoch": 1.4485333333333332,
        "step": 10864
    },
    {
        "loss": 2.496,
        "grad_norm": 2.9380056858062744,
        "learning_rate": 0.00018597525867855916,
        "epoch": 1.4486666666666665,
        "step": 10865
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.6835410594940186,
        "learning_rate": 0.0001859430991783469,
        "epoch": 1.4487999999999999,
        "step": 10866
    },
    {
        "loss": 1.8543,
        "grad_norm": 3.085562229156494,
        "learning_rate": 0.00018591090563578017,
        "epoch": 1.4489333333333334,
        "step": 10867
    },
    {
        "loss": 2.477,
        "grad_norm": 3.560119390487671,
        "learning_rate": 0.00018587867806361095,
        "epoch": 1.4490666666666667,
        "step": 10868
    },
    {
        "loss": 1.2646,
        "grad_norm": 2.9059898853302,
        "learning_rate": 0.0001858464164746048,
        "epoch": 1.4492,
        "step": 10869
    },
    {
        "loss": 1.9186,
        "grad_norm": 5.404336929321289,
        "learning_rate": 0.00018581412088154058,
        "epoch": 1.4493333333333334,
        "step": 10870
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.6266677379608154,
        "learning_rate": 0.00018578179129721055,
        "epoch": 1.4494666666666667,
        "step": 10871
    },
    {
        "loss": 0.8952,
        "grad_norm": 4.107480049133301,
        "learning_rate": 0.0001857494277344208,
        "epoch": 1.4496,
        "step": 10872
    },
    {
        "loss": 1.718,
        "grad_norm": 2.970419406890869,
        "learning_rate": 0.00018571703020599055,
        "epoch": 1.4497333333333333,
        "step": 10873
    },
    {
        "loss": 1.7795,
        "grad_norm": 5.355251789093018,
        "learning_rate": 0.00018568459872475245,
        "epoch": 1.4498666666666666,
        "step": 10874
    },
    {
        "loss": 1.3753,
        "grad_norm": 4.567697525024414,
        "learning_rate": 0.00018565213330355295,
        "epoch": 1.45,
        "step": 10875
    },
    {
        "loss": 2.4075,
        "grad_norm": 3.8130972385406494,
        "learning_rate": 0.00018561963395525156,
        "epoch": 1.4501333333333333,
        "step": 10876
    },
    {
        "loss": 1.2797,
        "grad_norm": 5.537316799163818,
        "learning_rate": 0.00018558710069272138,
        "epoch": 1.4502666666666666,
        "step": 10877
    },
    {
        "loss": 1.8749,
        "grad_norm": 3.7729642391204834,
        "learning_rate": 0.00018555453352884905,
        "epoch": 1.4504000000000001,
        "step": 10878
    },
    {
        "loss": 1.3673,
        "grad_norm": 3.5525784492492676,
        "learning_rate": 0.00018552193247653445,
        "epoch": 1.4505333333333335,
        "step": 10879
    },
    {
        "loss": 2.3879,
        "grad_norm": 3.103719711303711,
        "learning_rate": 0.0001854892975486909,
        "epoch": 1.4506666666666668,
        "step": 10880
    },
    {
        "loss": 0.553,
        "grad_norm": 3.3645217418670654,
        "learning_rate": 0.00018545662875824541,
        "epoch": 1.4508,
        "step": 10881
    },
    {
        "loss": 2.775,
        "grad_norm": 2.986419200897217,
        "learning_rate": 0.0001854239261181381,
        "epoch": 1.4509333333333334,
        "step": 10882
    },
    {
        "loss": 1.7644,
        "grad_norm": 6.407944679260254,
        "learning_rate": 0.0001853911896413225,
        "epoch": 1.4510666666666667,
        "step": 10883
    },
    {
        "loss": 2.664,
        "grad_norm": 3.7951183319091797,
        "learning_rate": 0.0001853584193407657,
        "epoch": 1.4512,
        "step": 10884
    },
    {
        "loss": 2.1447,
        "grad_norm": 4.22020149230957,
        "learning_rate": 0.0001853256152294482,
        "epoch": 1.4513333333333334,
        "step": 10885
    },
    {
        "loss": 2.4164,
        "grad_norm": 3.9495022296905518,
        "learning_rate": 0.0001852927773203637,
        "epoch": 1.4514666666666667,
        "step": 10886
    },
    {
        "loss": 2.0442,
        "grad_norm": 2.445537805557251,
        "learning_rate": 0.00018525990562651961,
        "epoch": 1.4516,
        "step": 10887
    },
    {
        "loss": 2.3709,
        "grad_norm": 5.1418561935424805,
        "learning_rate": 0.00018522700016093628,
        "epoch": 1.4517333333333333,
        "step": 10888
    },
    {
        "loss": 2.8114,
        "grad_norm": 2.881538152694702,
        "learning_rate": 0.00018519406093664778,
        "epoch": 1.4518666666666666,
        "step": 10889
    },
    {
        "loss": 1.2346,
        "grad_norm": 4.720684051513672,
        "learning_rate": 0.00018516108796670148,
        "epoch": 1.452,
        "step": 10890
    },
    {
        "loss": 2.3736,
        "grad_norm": 5.225306034088135,
        "learning_rate": 0.0001851280812641581,
        "epoch": 1.4521333333333333,
        "step": 10891
    },
    {
        "loss": 2.5983,
        "grad_norm": 4.042827129364014,
        "learning_rate": 0.00018509504084209162,
        "epoch": 1.4522666666666666,
        "step": 10892
    },
    {
        "loss": 2.7101,
        "grad_norm": 3.8295092582702637,
        "learning_rate": 0.00018506196671358945,
        "epoch": 1.4524,
        "step": 10893
    },
    {
        "loss": 1.8947,
        "grad_norm": 4.464874267578125,
        "learning_rate": 0.0001850288588917525,
        "epoch": 1.4525333333333332,
        "step": 10894
    },
    {
        "loss": 1.6548,
        "grad_norm": 6.638039588928223,
        "learning_rate": 0.00018499571738969474,
        "epoch": 1.4526666666666666,
        "step": 10895
    },
    {
        "loss": 2.186,
        "grad_norm": 3.5877420902252197,
        "learning_rate": 0.00018496254222054388,
        "epoch": 1.4527999999999999,
        "step": 10896
    },
    {
        "loss": 1.7027,
        "grad_norm": 2.8148725032806396,
        "learning_rate": 0.00018492933339744037,
        "epoch": 1.4529333333333334,
        "step": 10897
    },
    {
        "loss": 2.1023,
        "grad_norm": 2.501567840576172,
        "learning_rate": 0.00018489609093353852,
        "epoch": 1.4530666666666667,
        "step": 10898
    },
    {
        "loss": 2.266,
        "grad_norm": 3.512207508087158,
        "learning_rate": 0.00018486281484200582,
        "epoch": 1.4532,
        "step": 10899
    },
    {
        "loss": 2.2611,
        "grad_norm": 4.2597832679748535,
        "learning_rate": 0.00018482950513602303,
        "epoch": 1.4533333333333334,
        "step": 10900
    },
    {
        "loss": 2.325,
        "grad_norm": 5.722949028015137,
        "learning_rate": 0.00018479616182878422,
        "epoch": 1.4534666666666667,
        "step": 10901
    },
    {
        "loss": 2.3554,
        "grad_norm": 3.8135228157043457,
        "learning_rate": 0.00018476278493349666,
        "epoch": 1.4536,
        "step": 10902
    },
    {
        "loss": 1.5763,
        "grad_norm": 4.220558166503906,
        "learning_rate": 0.00018472937446338124,
        "epoch": 1.4537333333333333,
        "step": 10903
    },
    {
        "loss": 0.647,
        "grad_norm": 10.34703540802002,
        "learning_rate": 0.00018469593043167194,
        "epoch": 1.4538666666666666,
        "step": 10904
    },
    {
        "loss": 1.5872,
        "grad_norm": 3.9390902519226074,
        "learning_rate": 0.00018466245285161598,
        "epoch": 1.454,
        "step": 10905
    },
    {
        "loss": 1.85,
        "grad_norm": 6.6191253662109375,
        "learning_rate": 0.00018462894173647385,
        "epoch": 1.4541333333333333,
        "step": 10906
    },
    {
        "loss": 1.1852,
        "grad_norm": 5.949376106262207,
        "learning_rate": 0.00018459539709951957,
        "epoch": 1.4542666666666666,
        "step": 10907
    },
    {
        "loss": 2.9349,
        "grad_norm": 2.7714715003967285,
        "learning_rate": 0.00018456181895404038,
        "epoch": 1.4544000000000001,
        "step": 10908
    },
    {
        "loss": 1.8445,
        "grad_norm": 3.9939565658569336,
        "learning_rate": 0.00018452820731333644,
        "epoch": 1.4545333333333335,
        "step": 10909
    },
    {
        "loss": 2.8013,
        "grad_norm": 3.468873977661133,
        "learning_rate": 0.0001844945621907216,
        "epoch": 1.4546666666666668,
        "step": 10910
    },
    {
        "loss": 1.9139,
        "grad_norm": 4.325550079345703,
        "learning_rate": 0.0001844608835995227,
        "epoch": 1.4548,
        "step": 10911
    },
    {
        "loss": 1.8179,
        "grad_norm": 5.744748592376709,
        "learning_rate": 0.00018442717155308012,
        "epoch": 1.4549333333333334,
        "step": 10912
    },
    {
        "loss": 1.7871,
        "grad_norm": 5.515655517578125,
        "learning_rate": 0.00018439342606474718,
        "epoch": 1.4550666666666667,
        "step": 10913
    },
    {
        "loss": 1.6862,
        "grad_norm": 2.3446805477142334,
        "learning_rate": 0.00018435964714789063,
        "epoch": 1.4552,
        "step": 10914
    },
    {
        "loss": 1.8432,
        "grad_norm": 4.216383934020996,
        "learning_rate": 0.00018432583481589032,
        "epoch": 1.4553333333333334,
        "step": 10915
    },
    {
        "loss": 1.3764,
        "grad_norm": 3.102175235748291,
        "learning_rate": 0.00018429198908213946,
        "epoch": 1.4554666666666667,
        "step": 10916
    },
    {
        "loss": 2.1563,
        "grad_norm": 3.7097299098968506,
        "learning_rate": 0.0001842581099600447,
        "epoch": 1.4556,
        "step": 10917
    },
    {
        "loss": 1.5702,
        "grad_norm": 4.748629093170166,
        "learning_rate": 0.00018422419746302535,
        "epoch": 1.4557333333333333,
        "step": 10918
    },
    {
        "loss": 1.8392,
        "grad_norm": 4.191359519958496,
        "learning_rate": 0.00018419025160451444,
        "epoch": 1.4558666666666666,
        "step": 10919
    },
    {
        "loss": 2.272,
        "grad_norm": 3.637515068054199,
        "learning_rate": 0.00018415627239795794,
        "epoch": 1.456,
        "step": 10920
    },
    {
        "loss": 2.3341,
        "grad_norm": 4.658743381500244,
        "learning_rate": 0.00018412225985681526,
        "epoch": 1.4561333333333333,
        "step": 10921
    },
    {
        "loss": 2.1751,
        "grad_norm": 5.1566243171691895,
        "learning_rate": 0.0001840882139945588,
        "epoch": 1.4562666666666666,
        "step": 10922
    },
    {
        "loss": 2.6869,
        "grad_norm": 2.7987492084503174,
        "learning_rate": 0.00018405413482467428,
        "epoch": 1.4564,
        "step": 10923
    },
    {
        "loss": 0.9698,
        "grad_norm": 5.877991676330566,
        "learning_rate": 0.00018402002236066043,
        "epoch": 1.4565333333333332,
        "step": 10924
    },
    {
        "loss": 2.3199,
        "grad_norm": 3.4610095024108887,
        "learning_rate": 0.00018398587661602953,
        "epoch": 1.4566666666666666,
        "step": 10925
    },
    {
        "loss": 1.9986,
        "grad_norm": 5.147705078125,
        "learning_rate": 0.00018395169760430672,
        "epoch": 1.4567999999999999,
        "step": 10926
    },
    {
        "loss": 2.5202,
        "grad_norm": 4.44188117980957,
        "learning_rate": 0.00018391748533903038,
        "epoch": 1.4569333333333334,
        "step": 10927
    },
    {
        "loss": 1.9664,
        "grad_norm": 4.222095012664795,
        "learning_rate": 0.00018388323983375222,
        "epoch": 1.4570666666666667,
        "step": 10928
    },
    {
        "loss": 2.0016,
        "grad_norm": 3.5014257431030273,
        "learning_rate": 0.0001838489611020369,
        "epoch": 1.4572,
        "step": 10929
    },
    {
        "loss": 2.3502,
        "grad_norm": 3.7051963806152344,
        "learning_rate": 0.00018381464915746235,
        "epoch": 1.4573333333333334,
        "step": 10930
    },
    {
        "loss": 2.709,
        "grad_norm": 4.256037712097168,
        "learning_rate": 0.00018378030401361973,
        "epoch": 1.4574666666666667,
        "step": 10931
    },
    {
        "loss": 2.2316,
        "grad_norm": 3.0921058654785156,
        "learning_rate": 0.00018374592568411323,
        "epoch": 1.4576,
        "step": 10932
    },
    {
        "loss": 1.1861,
        "grad_norm": 5.507974624633789,
        "learning_rate": 0.0001837115141825602,
        "epoch": 1.4577333333333333,
        "step": 10933
    },
    {
        "loss": 2.4286,
        "grad_norm": 3.9700915813446045,
        "learning_rate": 0.0001836770695225911,
        "epoch": 1.4578666666666666,
        "step": 10934
    },
    {
        "loss": 1.3925,
        "grad_norm": 4.507211685180664,
        "learning_rate": 0.0001836425917178497,
        "epoch": 1.458,
        "step": 10935
    },
    {
        "loss": 1.0098,
        "grad_norm": 6.630268096923828,
        "learning_rate": 0.00018360808078199265,
        "epoch": 1.4581333333333333,
        "step": 10936
    },
    {
        "loss": 3.0735,
        "grad_norm": 2.8956141471862793,
        "learning_rate": 0.00018357353672868997,
        "epoch": 1.4582666666666666,
        "step": 10937
    },
    {
        "loss": 1.1961,
        "grad_norm": 2.105211019515991,
        "learning_rate": 0.00018353895957162462,
        "epoch": 1.4584,
        "step": 10938
    },
    {
        "loss": 1.9163,
        "grad_norm": 3.144798994064331,
        "learning_rate": 0.0001835043493244927,
        "epoch": 1.4585333333333335,
        "step": 10939
    },
    {
        "loss": 2.0468,
        "grad_norm": 5.2378830909729,
        "learning_rate": 0.0001834697060010035,
        "epoch": 1.4586666666666668,
        "step": 10940
    },
    {
        "loss": 2.1226,
        "grad_norm": 3.407230854034424,
        "learning_rate": 0.00018343502961487936,
        "epoch": 1.4588,
        "step": 10941
    },
    {
        "loss": 2.8995,
        "grad_norm": 3.403343915939331,
        "learning_rate": 0.0001834003201798557,
        "epoch": 1.4589333333333334,
        "step": 10942
    },
    {
        "loss": 2.1147,
        "grad_norm": 3.3856546878814697,
        "learning_rate": 0.00018336557770968096,
        "epoch": 1.4590666666666667,
        "step": 10943
    },
    {
        "loss": 2.0553,
        "grad_norm": 3.01792049407959,
        "learning_rate": 0.0001833308022181169,
        "epoch": 1.4592,
        "step": 10944
    },
    {
        "loss": 1.6817,
        "grad_norm": 5.20611572265625,
        "learning_rate": 0.00018329599371893807,
        "epoch": 1.4593333333333334,
        "step": 10945
    },
    {
        "loss": 2.6378,
        "grad_norm": 3.7826786041259766,
        "learning_rate": 0.00018326115222593245,
        "epoch": 1.4594666666666667,
        "step": 10946
    },
    {
        "loss": 2.1599,
        "grad_norm": 4.651321887969971,
        "learning_rate": 0.00018322627775290058,
        "epoch": 1.4596,
        "step": 10947
    },
    {
        "loss": 3.0558,
        "grad_norm": 4.83779764175415,
        "learning_rate": 0.0001831913703136565,
        "epoch": 1.4597333333333333,
        "step": 10948
    },
    {
        "loss": 2.0653,
        "grad_norm": 4.369771480560303,
        "learning_rate": 0.00018315642992202725,
        "epoch": 1.4598666666666666,
        "step": 10949
    },
    {
        "loss": 2.5272,
        "grad_norm": 3.979611873626709,
        "learning_rate": 0.00018312145659185274,
        "epoch": 1.46,
        "step": 10950
    },
    {
        "loss": 2.14,
        "grad_norm": 3.5870277881622314,
        "learning_rate": 0.00018308645033698605,
        "epoch": 1.4601333333333333,
        "step": 10951
    },
    {
        "loss": 2.5559,
        "grad_norm": 6.3368024826049805,
        "learning_rate": 0.00018305141117129324,
        "epoch": 1.4602666666666666,
        "step": 10952
    },
    {
        "loss": 2.0278,
        "grad_norm": 4.662106513977051,
        "learning_rate": 0.0001830163391086535,
        "epoch": 1.4604,
        "step": 10953
    },
    {
        "loss": 2.5892,
        "grad_norm": 3.338261365890503,
        "learning_rate": 0.0001829812341629589,
        "epoch": 1.4605333333333332,
        "step": 10954
    },
    {
        "loss": 1.1648,
        "grad_norm": 4.005377292633057,
        "learning_rate": 0.0001829460963481149,
        "epoch": 1.4606666666666666,
        "step": 10955
    },
    {
        "loss": 1.0911,
        "grad_norm": 3.90946364402771,
        "learning_rate": 0.00018291092567803926,
        "epoch": 1.4607999999999999,
        "step": 10956
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.819471597671509,
        "learning_rate": 0.0001828757221666635,
        "epoch": 1.4609333333333332,
        "step": 10957
    },
    {
        "loss": 2.3832,
        "grad_norm": 4.8926873207092285,
        "learning_rate": 0.00018284048582793187,
        "epoch": 1.4610666666666667,
        "step": 10958
    },
    {
        "loss": 1.8884,
        "grad_norm": 5.878482341766357,
        "learning_rate": 0.00018280521667580154,
        "epoch": 1.4612,
        "step": 10959
    },
    {
        "loss": 1.755,
        "grad_norm": 5.212961673736572,
        "learning_rate": 0.00018276991472424272,
        "epoch": 1.4613333333333334,
        "step": 10960
    },
    {
        "loss": 2.7871,
        "grad_norm": 3.2046611309051514,
        "learning_rate": 0.00018273457998723856,
        "epoch": 1.4614666666666667,
        "step": 10961
    },
    {
        "loss": 2.6092,
        "grad_norm": 5.697327613830566,
        "learning_rate": 0.00018269921247878544,
        "epoch": 1.4616,
        "step": 10962
    },
    {
        "loss": 2.5998,
        "grad_norm": 3.6019442081451416,
        "learning_rate": 0.0001826638122128925,
        "epoch": 1.4617333333333333,
        "step": 10963
    },
    {
        "loss": 2.07,
        "grad_norm": 4.73862886428833,
        "learning_rate": 0.00018262837920358184,
        "epoch": 1.4618666666666666,
        "step": 10964
    },
    {
        "loss": 2.7147,
        "grad_norm": 4.101524353027344,
        "learning_rate": 0.00018259291346488856,
        "epoch": 1.462,
        "step": 10965
    },
    {
        "loss": 1.8032,
        "grad_norm": 3.2829465866088867,
        "learning_rate": 0.00018255741501086082,
        "epoch": 1.4621333333333333,
        "step": 10966
    },
    {
        "loss": 2.201,
        "grad_norm": 2.975811719894409,
        "learning_rate": 0.00018252188385555984,
        "epoch": 1.4622666666666666,
        "step": 10967
    },
    {
        "loss": 2.3406,
        "grad_norm": 2.3968381881713867,
        "learning_rate": 0.00018248632001305933,
        "epoch": 1.4624,
        "step": 10968
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.7811646461486816,
        "learning_rate": 0.00018245072349744649,
        "epoch": 1.4625333333333335,
        "step": 10969
    },
    {
        "loss": 2.0926,
        "grad_norm": 3.437976360321045,
        "learning_rate": 0.00018241509432282102,
        "epoch": 1.4626666666666668,
        "step": 10970
    },
    {
        "loss": 2.7132,
        "grad_norm": 2.5805249214172363,
        "learning_rate": 0.00018237943250329596,
        "epoch": 1.4628,
        "step": 10971
    },
    {
        "loss": 1.8264,
        "grad_norm": 3.945521354675293,
        "learning_rate": 0.000182343738052997,
        "epoch": 1.4629333333333334,
        "step": 10972
    },
    {
        "loss": 1.5339,
        "grad_norm": 4.2962517738342285,
        "learning_rate": 0.00018230801098606278,
        "epoch": 1.4630666666666667,
        "step": 10973
    },
    {
        "loss": 2.2512,
        "grad_norm": 4.502353191375732,
        "learning_rate": 0.0001822722513166449,
        "epoch": 1.4632,
        "step": 10974
    },
    {
        "loss": 2.6279,
        "grad_norm": 3.0213570594787598,
        "learning_rate": 0.00018223645905890795,
        "epoch": 1.4633333333333334,
        "step": 10975
    },
    {
        "loss": 2.4976,
        "grad_norm": 4.193802833557129,
        "learning_rate": 0.0001822006342270295,
        "epoch": 1.4634666666666667,
        "step": 10976
    },
    {
        "loss": 1.102,
        "grad_norm": 4.76359224319458,
        "learning_rate": 0.00018216477683519953,
        "epoch": 1.4636,
        "step": 10977
    },
    {
        "loss": 2.1816,
        "grad_norm": 3.8683605194091797,
        "learning_rate": 0.00018212888689762163,
        "epoch": 1.4637333333333333,
        "step": 10978
    },
    {
        "loss": 2.2355,
        "grad_norm": 4.891716003417969,
        "learning_rate": 0.00018209296442851168,
        "epoch": 1.4638666666666666,
        "step": 10979
    },
    {
        "loss": 1.9699,
        "grad_norm": 3.819395065307617,
        "learning_rate": 0.00018205700944209889,
        "epoch": 1.464,
        "step": 10980
    },
    {
        "loss": 2.6411,
        "grad_norm": 3.569441318511963,
        "learning_rate": 0.00018202102195262506,
        "epoch": 1.4641333333333333,
        "step": 10981
    },
    {
        "loss": 2.4389,
        "grad_norm": 3.6810965538024902,
        "learning_rate": 0.00018198500197434494,
        "epoch": 1.4642666666666666,
        "step": 10982
    },
    {
        "loss": 1.9741,
        "grad_norm": 4.267824649810791,
        "learning_rate": 0.00018194894952152608,
        "epoch": 1.4644,
        "step": 10983
    },
    {
        "loss": 2.3322,
        "grad_norm": 3.7695515155792236,
        "learning_rate": 0.00018191286460844918,
        "epoch": 1.4645333333333332,
        "step": 10984
    },
    {
        "loss": 1.9409,
        "grad_norm": 4.238938331604004,
        "learning_rate": 0.0001818767472494075,
        "epoch": 1.4646666666666666,
        "step": 10985
    },
    {
        "loss": 1.8054,
        "grad_norm": 4.882244110107422,
        "learning_rate": 0.0001818405974587072,
        "epoch": 1.4647999999999999,
        "step": 10986
    },
    {
        "loss": 2.4156,
        "grad_norm": 5.139425754547119,
        "learning_rate": 0.0001818044152506674,
        "epoch": 1.4649333333333332,
        "step": 10987
    },
    {
        "loss": 0.5816,
        "grad_norm": 2.640390396118164,
        "learning_rate": 0.00018176820063962,
        "epoch": 1.4650666666666667,
        "step": 10988
    },
    {
        "loss": 1.8069,
        "grad_norm": 3.471358060836792,
        "learning_rate": 0.00018173195363990963,
        "epoch": 1.4652,
        "step": 10989
    },
    {
        "loss": 2.655,
        "grad_norm": 2.249260663986206,
        "learning_rate": 0.000181695674265894,
        "epoch": 1.4653333333333334,
        "step": 10990
    },
    {
        "loss": 1.3366,
        "grad_norm": 4.7039690017700195,
        "learning_rate": 0.00018165936253194343,
        "epoch": 1.4654666666666667,
        "step": 10991
    },
    {
        "loss": 2.4479,
        "grad_norm": 3.0466127395629883,
        "learning_rate": 0.00018162301845244113,
        "epoch": 1.4656,
        "step": 10992
    },
    {
        "loss": 2.4282,
        "grad_norm": 4.273029327392578,
        "learning_rate": 0.000181586642041783,
        "epoch": 1.4657333333333333,
        "step": 10993
    },
    {
        "loss": 1.9318,
        "grad_norm": 3.785299301147461,
        "learning_rate": 0.00018155023331437807,
        "epoch": 1.4658666666666667,
        "step": 10994
    },
    {
        "loss": 1.5391,
        "grad_norm": 4.713976860046387,
        "learning_rate": 0.0001815137922846477,
        "epoch": 1.466,
        "step": 10995
    },
    {
        "loss": 0.9654,
        "grad_norm": 3.7599637508392334,
        "learning_rate": 0.0001814773189670266,
        "epoch": 1.4661333333333333,
        "step": 10996
    },
    {
        "loss": 2.6449,
        "grad_norm": 3.218822717666626,
        "learning_rate": 0.0001814408133759618,
        "epoch": 1.4662666666666666,
        "step": 10997
    },
    {
        "loss": 2.8347,
        "grad_norm": 3.546477794647217,
        "learning_rate": 0.00018140427552591323,
        "epoch": 1.4664,
        "step": 10998
    },
    {
        "loss": 1.9186,
        "grad_norm": 4.628849506378174,
        "learning_rate": 0.00018136770543135383,
        "epoch": 1.4665333333333335,
        "step": 10999
    },
    {
        "loss": 1.8527,
        "grad_norm": 6.553224563598633,
        "learning_rate": 0.0001813311031067691,
        "epoch": 1.4666666666666668,
        "step": 11000
    },
    {
        "loss": 2.5892,
        "grad_norm": 3.221487522125244,
        "learning_rate": 0.00018129446856665724,
        "epoch": 1.4668,
        "step": 11001
    },
    {
        "loss": 1.7396,
        "grad_norm": 3.440380096435547,
        "learning_rate": 0.00018125780182552933,
        "epoch": 1.4669333333333334,
        "step": 11002
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.9659423828125,
        "learning_rate": 0.00018122110289790931,
        "epoch": 1.4670666666666667,
        "step": 11003
    },
    {
        "loss": 2.1134,
        "grad_norm": 4.333553314208984,
        "learning_rate": 0.0001811843717983336,
        "epoch": 1.4672,
        "step": 11004
    },
    {
        "loss": 2.8074,
        "grad_norm": 3.394085645675659,
        "learning_rate": 0.00018114760854135174,
        "epoch": 1.4673333333333334,
        "step": 11005
    },
    {
        "loss": 2.1551,
        "grad_norm": 2.7701940536499023,
        "learning_rate": 0.0001811108131415255,
        "epoch": 1.4674666666666667,
        "step": 11006
    },
    {
        "loss": 1.2446,
        "grad_norm": 4.327481746673584,
        "learning_rate": 0.00018107398561342976,
        "epoch": 1.4676,
        "step": 11007
    },
    {
        "loss": 1.9556,
        "grad_norm": 3.790861129760742,
        "learning_rate": 0.00018103712597165216,
        "epoch": 1.4677333333333333,
        "step": 11008
    },
    {
        "loss": 1.3905,
        "grad_norm": 3.0847339630126953,
        "learning_rate": 0.00018100023423079286,
        "epoch": 1.4678666666666667,
        "step": 11009
    },
    {
        "loss": 2.3126,
        "grad_norm": 4.1599345207214355,
        "learning_rate": 0.00018096331040546475,
        "epoch": 1.468,
        "step": 11010
    },
    {
        "loss": 2.2838,
        "grad_norm": 2.5990235805511475,
        "learning_rate": 0.00018092635451029344,
        "epoch": 1.4681333333333333,
        "step": 11011
    },
    {
        "loss": 1.5859,
        "grad_norm": 3.3412318229675293,
        "learning_rate": 0.00018088936655991745,
        "epoch": 1.4682666666666666,
        "step": 11012
    },
    {
        "loss": 2.7137,
        "grad_norm": 4.580042362213135,
        "learning_rate": 0.00018085234656898774,
        "epoch": 1.4684,
        "step": 11013
    },
    {
        "loss": 1.4401,
        "grad_norm": 4.075141429901123,
        "learning_rate": 0.00018081529455216803,
        "epoch": 1.4685333333333332,
        "step": 11014
    },
    {
        "loss": 2.3893,
        "grad_norm": 2.9162991046905518,
        "learning_rate": 0.00018077821052413473,
        "epoch": 1.4686666666666666,
        "step": 11015
    },
    {
        "loss": 2.0397,
        "grad_norm": 3.8362960815429688,
        "learning_rate": 0.00018074109449957697,
        "epoch": 1.4687999999999999,
        "step": 11016
    },
    {
        "loss": 0.7668,
        "grad_norm": 4.1092329025268555,
        "learning_rate": 0.00018070394649319667,
        "epoch": 1.4689333333333332,
        "step": 11017
    },
    {
        "loss": 2.482,
        "grad_norm": 3.8594655990600586,
        "learning_rate": 0.00018066676651970815,
        "epoch": 1.4690666666666667,
        "step": 11018
    },
    {
        "loss": 2.4055,
        "grad_norm": 3.057152509689331,
        "learning_rate": 0.00018062955459383857,
        "epoch": 1.4692,
        "step": 11019
    },
    {
        "loss": 2.4257,
        "grad_norm": 3.12955641746521,
        "learning_rate": 0.00018059231073032757,
        "epoch": 1.4693333333333334,
        "step": 11020
    },
    {
        "loss": 2.385,
        "grad_norm": 3.5161662101745605,
        "learning_rate": 0.00018055503494392776,
        "epoch": 1.4694666666666667,
        "step": 11021
    },
    {
        "loss": 2.3848,
        "grad_norm": 3.4846160411834717,
        "learning_rate": 0.0001805177272494041,
        "epoch": 1.4696,
        "step": 11022
    },
    {
        "loss": 2.2728,
        "grad_norm": 3.016251802444458,
        "learning_rate": 0.00018048038766153436,
        "epoch": 1.4697333333333333,
        "step": 11023
    },
    {
        "loss": 1.891,
        "grad_norm": 4.936440944671631,
        "learning_rate": 0.00018044301619510878,
        "epoch": 1.4698666666666667,
        "step": 11024
    },
    {
        "loss": 2.6677,
        "grad_norm": 3.7080631256103516,
        "learning_rate": 0.00018040561286493035,
        "epoch": 1.47,
        "step": 11025
    },
    {
        "loss": 1.5389,
        "grad_norm": 3.6640329360961914,
        "learning_rate": 0.00018036817768581482,
        "epoch": 1.4701333333333333,
        "step": 11026
    },
    {
        "loss": 1.6458,
        "grad_norm": 3.4797608852386475,
        "learning_rate": 0.00018033071067259014,
        "epoch": 1.4702666666666666,
        "step": 11027
    },
    {
        "loss": 2.3875,
        "grad_norm": 3.7551052570343018,
        "learning_rate": 0.00018029321184009732,
        "epoch": 1.4704,
        "step": 11028
    },
    {
        "loss": 2.1482,
        "grad_norm": 3.6299633979797363,
        "learning_rate": 0.00018025568120318957,
        "epoch": 1.4705333333333335,
        "step": 11029
    },
    {
        "loss": 2.9377,
        "grad_norm": 2.134587049484253,
        "learning_rate": 0.00018021811877673316,
        "epoch": 1.4706666666666668,
        "step": 11030
    },
    {
        "loss": 0.8179,
        "grad_norm": 4.255702018737793,
        "learning_rate": 0.00018018052457560656,
        "epoch": 1.4708,
        "step": 11031
    },
    {
        "loss": 2.2512,
        "grad_norm": 3.102285385131836,
        "learning_rate": 0.00018014289861470099,
        "epoch": 1.4709333333333334,
        "step": 11032
    },
    {
        "loss": 2.6908,
        "grad_norm": 3.626521110534668,
        "learning_rate": 0.00018010524090892008,
        "epoch": 1.4710666666666667,
        "step": 11033
    },
    {
        "loss": 2.4903,
        "grad_norm": 3.791020154953003,
        "learning_rate": 0.00018006755147318036,
        "epoch": 1.4712,
        "step": 11034
    },
    {
        "loss": 2.0078,
        "grad_norm": 3.4974968433380127,
        "learning_rate": 0.00018002983032241072,
        "epoch": 1.4713333333333334,
        "step": 11035
    },
    {
        "loss": 2.7911,
        "grad_norm": 5.665491580963135,
        "learning_rate": 0.0001799920774715525,
        "epoch": 1.4714666666666667,
        "step": 11036
    },
    {
        "loss": 2.6475,
        "grad_norm": 2.504579782485962,
        "learning_rate": 0.0001799542929355599,
        "epoch": 1.4716,
        "step": 11037
    },
    {
        "loss": 1.8319,
        "grad_norm": 4.275283336639404,
        "learning_rate": 0.00017991647672939935,
        "epoch": 1.4717333333333333,
        "step": 11038
    },
    {
        "loss": 3.0943,
        "grad_norm": 5.150078773498535,
        "learning_rate": 0.00017987862886805015,
        "epoch": 1.4718666666666667,
        "step": 11039
    },
    {
        "loss": 2.6836,
        "grad_norm": 3.3141939640045166,
        "learning_rate": 0.00017984074936650387,
        "epoch": 1.472,
        "step": 11040
    },
    {
        "loss": 1.1595,
        "grad_norm": 4.126765251159668,
        "learning_rate": 0.00017980283823976472,
        "epoch": 1.4721333333333333,
        "step": 11041
    },
    {
        "loss": 0.5824,
        "grad_norm": 3.0236775875091553,
        "learning_rate": 0.00017976489550284935,
        "epoch": 1.4722666666666666,
        "step": 11042
    },
    {
        "loss": 1.9137,
        "grad_norm": 4.88896369934082,
        "learning_rate": 0.00017972692117078714,
        "epoch": 1.4724,
        "step": 11043
    },
    {
        "loss": 2.7353,
        "grad_norm": 4.923532485961914,
        "learning_rate": 0.00017968891525861977,
        "epoch": 1.4725333333333332,
        "step": 11044
    },
    {
        "loss": 2.6804,
        "grad_norm": 3.952876091003418,
        "learning_rate": 0.0001796508777814015,
        "epoch": 1.4726666666666666,
        "step": 11045
    },
    {
        "loss": 2.3604,
        "grad_norm": 2.339207410812378,
        "learning_rate": 0.0001796128087541992,
        "epoch": 1.4727999999999999,
        "step": 11046
    },
    {
        "loss": 2.2612,
        "grad_norm": 4.877033233642578,
        "learning_rate": 0.00017957470819209204,
        "epoch": 1.4729333333333332,
        "step": 11047
    },
    {
        "loss": 2.602,
        "grad_norm": 2.8296451568603516,
        "learning_rate": 0.0001795365761101718,
        "epoch": 1.4730666666666667,
        "step": 11048
    },
    {
        "loss": 1.6887,
        "grad_norm": 4.141757488250732,
        "learning_rate": 0.0001794984125235428,
        "epoch": 1.4732,
        "step": 11049
    },
    {
        "loss": 1.8547,
        "grad_norm": 5.673433303833008,
        "learning_rate": 0.0001794602174473217,
        "epoch": 1.4733333333333334,
        "step": 11050
    },
    {
        "loss": 1.989,
        "grad_norm": 3.5034728050231934,
        "learning_rate": 0.00017942199089663775,
        "epoch": 1.4734666666666667,
        "step": 11051
    },
    {
        "loss": 0.5623,
        "grad_norm": 2.635225772857666,
        "learning_rate": 0.00017938373288663249,
        "epoch": 1.4736,
        "step": 11052
    },
    {
        "loss": 2.4381,
        "grad_norm": 3.4516589641571045,
        "learning_rate": 0.00017934544343246024,
        "epoch": 1.4737333333333333,
        "step": 11053
    },
    {
        "loss": 1.1521,
        "grad_norm": 5.4534807205200195,
        "learning_rate": 0.00017930712254928736,
        "epoch": 1.4738666666666667,
        "step": 11054
    },
    {
        "loss": 2.7149,
        "grad_norm": 4.207931995391846,
        "learning_rate": 0.0001792687702522931,
        "epoch": 1.474,
        "step": 11055
    },
    {
        "loss": 2.4282,
        "grad_norm": 4.93144416809082,
        "learning_rate": 0.00017923038655666887,
        "epoch": 1.4741333333333333,
        "step": 11056
    },
    {
        "loss": 2.5774,
        "grad_norm": 3.4366953372955322,
        "learning_rate": 0.00017919197147761841,
        "epoch": 1.4742666666666666,
        "step": 11057
    },
    {
        "loss": 2.6558,
        "grad_norm": 4.404740333557129,
        "learning_rate": 0.00017915352503035834,
        "epoch": 1.4744,
        "step": 11058
    },
    {
        "loss": 2.4891,
        "grad_norm": 3.7918388843536377,
        "learning_rate": 0.0001791150472301173,
        "epoch": 1.4745333333333333,
        "step": 11059
    },
    {
        "loss": 1.477,
        "grad_norm": 3.2100589275360107,
        "learning_rate": 0.00017907653809213646,
        "epoch": 1.4746666666666668,
        "step": 11060
    },
    {
        "loss": 1.9996,
        "grad_norm": 3.565793514251709,
        "learning_rate": 0.00017903799763166936,
        "epoch": 1.4748,
        "step": 11061
    },
    {
        "loss": 2.3802,
        "grad_norm": 4.535793781280518,
        "learning_rate": 0.00017899942586398215,
        "epoch": 1.4749333333333334,
        "step": 11062
    },
    {
        "loss": 1.744,
        "grad_norm": 5.2602219581604,
        "learning_rate": 0.00017896082280435311,
        "epoch": 1.4750666666666667,
        "step": 11063
    },
    {
        "loss": 2.5412,
        "grad_norm": 3.540235757827759,
        "learning_rate": 0.00017892218846807324,
        "epoch": 1.4752,
        "step": 11064
    },
    {
        "loss": 2.7953,
        "grad_norm": 4.974038600921631,
        "learning_rate": 0.00017888352287044544,
        "epoch": 1.4753333333333334,
        "step": 11065
    },
    {
        "loss": 1.9814,
        "grad_norm": 3.1022844314575195,
        "learning_rate": 0.00017884482602678546,
        "epoch": 1.4754666666666667,
        "step": 11066
    },
    {
        "loss": 1.0395,
        "grad_norm": 4.082306861877441,
        "learning_rate": 0.00017880609795242132,
        "epoch": 1.4756,
        "step": 11067
    },
    {
        "loss": 0.4742,
        "grad_norm": 2.2183032035827637,
        "learning_rate": 0.00017876733866269324,
        "epoch": 1.4757333333333333,
        "step": 11068
    },
    {
        "loss": 2.4919,
        "grad_norm": 4.100144386291504,
        "learning_rate": 0.00017872854817295393,
        "epoch": 1.4758666666666667,
        "step": 11069
    },
    {
        "loss": 2.7295,
        "grad_norm": 2.9140660762786865,
        "learning_rate": 0.00017868972649856833,
        "epoch": 1.476,
        "step": 11070
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.7694742679595947,
        "learning_rate": 0.00017865087365491404,
        "epoch": 1.4761333333333333,
        "step": 11071
    },
    {
        "loss": 3.3009,
        "grad_norm": 2.2837376594543457,
        "learning_rate": 0.0001786119896573807,
        "epoch": 1.4762666666666666,
        "step": 11072
    },
    {
        "loss": 1.6291,
        "grad_norm": 3.2700035572052,
        "learning_rate": 0.00017857307452137037,
        "epoch": 1.4764,
        "step": 11073
    },
    {
        "loss": 2.4144,
        "grad_norm": 4.746490478515625,
        "learning_rate": 0.0001785341282622974,
        "epoch": 1.4765333333333333,
        "step": 11074
    },
    {
        "loss": 1.6738,
        "grad_norm": 4.514294624328613,
        "learning_rate": 0.00017849515089558864,
        "epoch": 1.4766666666666666,
        "step": 11075
    },
    {
        "loss": 1.6436,
        "grad_norm": 3.4774529933929443,
        "learning_rate": 0.00017845614243668326,
        "epoch": 1.4768,
        "step": 11076
    },
    {
        "loss": 2.1673,
        "grad_norm": 4.029548168182373,
        "learning_rate": 0.0001784171029010325,
        "epoch": 1.4769333333333332,
        "step": 11077
    },
    {
        "loss": 1.8323,
        "grad_norm": 5.953658103942871,
        "learning_rate": 0.00017837803230410007,
        "epoch": 1.4770666666666667,
        "step": 11078
    },
    {
        "loss": 2.5702,
        "grad_norm": 3.930540084838867,
        "learning_rate": 0.0001783389306613619,
        "epoch": 1.4772,
        "step": 11079
    },
    {
        "loss": 1.3317,
        "grad_norm": 5.072010040283203,
        "learning_rate": 0.00017829979798830644,
        "epoch": 1.4773333333333334,
        "step": 11080
    },
    {
        "loss": 2.397,
        "grad_norm": 3.6954119205474854,
        "learning_rate": 0.00017826063430043422,
        "epoch": 1.4774666666666667,
        "step": 11081
    },
    {
        "loss": 2.316,
        "grad_norm": 5.5061750411987305,
        "learning_rate": 0.00017822143961325807,
        "epoch": 1.4776,
        "step": 11082
    },
    {
        "loss": 2.7442,
        "grad_norm": 4.382561206817627,
        "learning_rate": 0.0001781822139423031,
        "epoch": 1.4777333333333333,
        "step": 11083
    },
    {
        "loss": 2.3766,
        "grad_norm": 3.4695301055908203,
        "learning_rate": 0.00017814295730310677,
        "epoch": 1.4778666666666667,
        "step": 11084
    },
    {
        "loss": 2.1706,
        "grad_norm": 3.8622121810913086,
        "learning_rate": 0.00017810366971121895,
        "epoch": 1.478,
        "step": 11085
    },
    {
        "loss": 1.9557,
        "grad_norm": 4.06292724609375,
        "learning_rate": 0.00017806435118220123,
        "epoch": 1.4781333333333333,
        "step": 11086
    },
    {
        "loss": 1.8607,
        "grad_norm": 3.094174861907959,
        "learning_rate": 0.00017802500173162814,
        "epoch": 1.4782666666666666,
        "step": 11087
    },
    {
        "loss": 0.9001,
        "grad_norm": 3.1595535278320312,
        "learning_rate": 0.0001779856213750859,
        "epoch": 1.4784,
        "step": 11088
    },
    {
        "loss": 2.2528,
        "grad_norm": 4.038740634918213,
        "learning_rate": 0.00017794621012817339,
        "epoch": 1.4785333333333333,
        "step": 11089
    },
    {
        "loss": 1.5583,
        "grad_norm": 4.187063217163086,
        "learning_rate": 0.00017790676800650147,
        "epoch": 1.4786666666666668,
        "step": 11090
    },
    {
        "loss": 1.3149,
        "grad_norm": 4.109328269958496,
        "learning_rate": 0.00017786729502569323,
        "epoch": 1.4788000000000001,
        "step": 11091
    },
    {
        "loss": 1.5096,
        "grad_norm": 6.2381486892700195,
        "learning_rate": 0.00017782779120138406,
        "epoch": 1.4789333333333334,
        "step": 11092
    },
    {
        "loss": 1.9658,
        "grad_norm": 3.61286997795105,
        "learning_rate": 0.00017778825654922166,
        "epoch": 1.4790666666666668,
        "step": 11093
    },
    {
        "loss": 2.6037,
        "grad_norm": 3.475836753845215,
        "learning_rate": 0.0001777486910848658,
        "epoch": 1.4792,
        "step": 11094
    },
    {
        "loss": 1.804,
        "grad_norm": 3.560732841491699,
        "learning_rate": 0.00017770909482398844,
        "epoch": 1.4793333333333334,
        "step": 11095
    },
    {
        "loss": 2.1856,
        "grad_norm": 4.250005722045898,
        "learning_rate": 0.00017766946778227384,
        "epoch": 1.4794666666666667,
        "step": 11096
    },
    {
        "loss": 1.9356,
        "grad_norm": 3.3510520458221436,
        "learning_rate": 0.00017762980997541836,
        "epoch": 1.4796,
        "step": 11097
    },
    {
        "loss": 1.959,
        "grad_norm": 5.086796760559082,
        "learning_rate": 0.00017759012141913073,
        "epoch": 1.4797333333333333,
        "step": 11098
    },
    {
        "loss": 1.403,
        "grad_norm": 4.62578821182251,
        "learning_rate": 0.00017755040212913157,
        "epoch": 1.4798666666666667,
        "step": 11099
    },
    {
        "loss": 1.8796,
        "grad_norm": 2.966784715652466,
        "learning_rate": 0.0001775106521211539,
        "epoch": 1.48,
        "step": 11100
    },
    {
        "loss": 2.6187,
        "grad_norm": 4.464279651641846,
        "learning_rate": 0.00017747087141094277,
        "epoch": 1.4801333333333333,
        "step": 11101
    },
    {
        "loss": 2.5819,
        "grad_norm": 3.63189435005188,
        "learning_rate": 0.00017743106001425553,
        "epoch": 1.4802666666666666,
        "step": 11102
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.8502118587493896,
        "learning_rate": 0.0001773912179468616,
        "epoch": 1.4804,
        "step": 11103
    },
    {
        "loss": 0.8749,
        "grad_norm": 4.217092037200928,
        "learning_rate": 0.00017735134522454246,
        "epoch": 1.4805333333333333,
        "step": 11104
    },
    {
        "loss": 2.3039,
        "grad_norm": 4.24868631362915,
        "learning_rate": 0.000177311441863092,
        "epoch": 1.4806666666666666,
        "step": 11105
    },
    {
        "loss": 1.5115,
        "grad_norm": 2.9114999771118164,
        "learning_rate": 0.00017727150787831596,
        "epoch": 1.4808,
        "step": 11106
    },
    {
        "loss": 1.353,
        "grad_norm": 3.7755239009857178,
        "learning_rate": 0.00017723154328603226,
        "epoch": 1.4809333333333332,
        "step": 11107
    },
    {
        "loss": 2.0395,
        "grad_norm": 3.953876495361328,
        "learning_rate": 0.0001771915481020712,
        "epoch": 1.4810666666666665,
        "step": 11108
    },
    {
        "loss": 2.1003,
        "grad_norm": 3.25722336769104,
        "learning_rate": 0.00017715152234227494,
        "epoch": 1.4812,
        "step": 11109
    },
    {
        "loss": 1.8169,
        "grad_norm": 4.555938720703125,
        "learning_rate": 0.00017711146602249778,
        "epoch": 1.4813333333333334,
        "step": 11110
    },
    {
        "loss": 1.4453,
        "grad_norm": 4.373940467834473,
        "learning_rate": 0.0001770713791586061,
        "epoch": 1.4814666666666667,
        "step": 11111
    },
    {
        "loss": 1.3378,
        "grad_norm": 3.4564437866210938,
        "learning_rate": 0.00017703126176647858,
        "epoch": 1.4816,
        "step": 11112
    },
    {
        "loss": 2.1445,
        "grad_norm": 3.2481868267059326,
        "learning_rate": 0.00017699111386200572,
        "epoch": 1.4817333333333333,
        "step": 11113
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.8395156860351562,
        "learning_rate": 0.0001769509354610905,
        "epoch": 1.4818666666666667,
        "step": 11114
    },
    {
        "loss": 2.5717,
        "grad_norm": 3.3264946937561035,
        "learning_rate": 0.00017691072657964737,
        "epoch": 1.482,
        "step": 11115
    },
    {
        "loss": 2.5945,
        "grad_norm": 4.1576337814331055,
        "learning_rate": 0.00017687048723360335,
        "epoch": 1.4821333333333333,
        "step": 11116
    },
    {
        "loss": 1.4096,
        "grad_norm": 2.947869300842285,
        "learning_rate": 0.0001768302174388975,
        "epoch": 1.4822666666666666,
        "step": 11117
    },
    {
        "loss": 2.4201,
        "grad_norm": 3.0875766277313232,
        "learning_rate": 0.00017678991721148076,
        "epoch": 1.4824,
        "step": 11118
    },
    {
        "loss": 2.2979,
        "grad_norm": 3.62309193611145,
        "learning_rate": 0.00017674958656731615,
        "epoch": 1.4825333333333333,
        "step": 11119
    },
    {
        "loss": 2.3426,
        "grad_norm": 5.221842288970947,
        "learning_rate": 0.00017670922552237868,
        "epoch": 1.4826666666666668,
        "step": 11120
    },
    {
        "loss": 2.3459,
        "grad_norm": 3.929064989089966,
        "learning_rate": 0.0001766688340926557,
        "epoch": 1.4828000000000001,
        "step": 11121
    },
    {
        "loss": 1.7441,
        "grad_norm": 4.279304027557373,
        "learning_rate": 0.00017662841229414616,
        "epoch": 1.4829333333333334,
        "step": 11122
    },
    {
        "loss": 2.4253,
        "grad_norm": 4.077605247497559,
        "learning_rate": 0.00017658796014286164,
        "epoch": 1.4830666666666668,
        "step": 11123
    },
    {
        "loss": 1.0591,
        "grad_norm": 3.714655876159668,
        "learning_rate": 0.00017654747765482494,
        "epoch": 1.4832,
        "step": 11124
    },
    {
        "loss": 2.3853,
        "grad_norm": 5.427689075469971,
        "learning_rate": 0.00017650696484607152,
        "epoch": 1.4833333333333334,
        "step": 11125
    },
    {
        "loss": 2.2334,
        "grad_norm": 3.711091995239258,
        "learning_rate": 0.0001764664217326487,
        "epoch": 1.4834666666666667,
        "step": 11126
    },
    {
        "loss": 3.0932,
        "grad_norm": 3.615467071533203,
        "learning_rate": 0.00017642584833061574,
        "epoch": 1.4836,
        "step": 11127
    },
    {
        "loss": 2.4646,
        "grad_norm": 3.4842488765716553,
        "learning_rate": 0.00017638524465604376,
        "epoch": 1.4837333333333333,
        "step": 11128
    },
    {
        "loss": 1.9627,
        "grad_norm": 3.045161724090576,
        "learning_rate": 0.00017634461072501604,
        "epoch": 1.4838666666666667,
        "step": 11129
    },
    {
        "loss": 2.2741,
        "grad_norm": 4.2025628089904785,
        "learning_rate": 0.00017630394655362798,
        "epoch": 1.484,
        "step": 11130
    },
    {
        "loss": 1.9741,
        "grad_norm": 3.6644110679626465,
        "learning_rate": 0.0001762632521579867,
        "epoch": 1.4841333333333333,
        "step": 11131
    },
    {
        "loss": 2.0935,
        "grad_norm": 5.406545639038086,
        "learning_rate": 0.00017622252755421133,
        "epoch": 1.4842666666666666,
        "step": 11132
    },
    {
        "loss": 2.2728,
        "grad_norm": 4.386643409729004,
        "learning_rate": 0.000176181772758433,
        "epoch": 1.4844,
        "step": 11133
    },
    {
        "loss": 1.6262,
        "grad_norm": 4.202221870422363,
        "learning_rate": 0.0001761409877867949,
        "epoch": 1.4845333333333333,
        "step": 11134
    },
    {
        "loss": 2.2213,
        "grad_norm": 3.967801809310913,
        "learning_rate": 0.00017610017265545232,
        "epoch": 1.4846666666666666,
        "step": 11135
    },
    {
        "loss": 1.6443,
        "grad_norm": 6.4167866706848145,
        "learning_rate": 0.00017605932738057182,
        "epoch": 1.4848,
        "step": 11136
    },
    {
        "loss": 2.3928,
        "grad_norm": 3.029325485229492,
        "learning_rate": 0.00017601845197833267,
        "epoch": 1.4849333333333332,
        "step": 11137
    },
    {
        "loss": 2.2165,
        "grad_norm": 3.0577850341796875,
        "learning_rate": 0.0001759775464649256,
        "epoch": 1.4850666666666665,
        "step": 11138
    },
    {
        "loss": 1.38,
        "grad_norm": 4.5336503982543945,
        "learning_rate": 0.00017593661085655354,
        "epoch": 1.4852,
        "step": 11139
    },
    {
        "loss": 2.5251,
        "grad_norm": 3.0468127727508545,
        "learning_rate": 0.00017589564516943117,
        "epoch": 1.4853333333333334,
        "step": 11140
    },
    {
        "loss": 2.8438,
        "grad_norm": 4.236484050750732,
        "learning_rate": 0.00017585464941978517,
        "epoch": 1.4854666666666667,
        "step": 11141
    },
    {
        "loss": 2.3393,
        "grad_norm": 3.0488855838775635,
        "learning_rate": 0.00017581362362385398,
        "epoch": 1.4856,
        "step": 11142
    },
    {
        "loss": 2.7453,
        "grad_norm": 3.895810842514038,
        "learning_rate": 0.00017577256779788814,
        "epoch": 1.4857333333333334,
        "step": 11143
    },
    {
        "loss": 2.4424,
        "grad_norm": 3.087461233139038,
        "learning_rate": 0.00017573148195815016,
        "epoch": 1.4858666666666667,
        "step": 11144
    },
    {
        "loss": 1.6577,
        "grad_norm": 4.137110710144043,
        "learning_rate": 0.000175690366120914,
        "epoch": 1.486,
        "step": 11145
    },
    {
        "loss": 2.8755,
        "grad_norm": 3.4995133876800537,
        "learning_rate": 0.00017564922030246597,
        "epoch": 1.4861333333333333,
        "step": 11146
    },
    {
        "loss": 1.9918,
        "grad_norm": 3.1304521560668945,
        "learning_rate": 0.0001756080445191039,
        "epoch": 1.4862666666666666,
        "step": 11147
    },
    {
        "loss": 1.3831,
        "grad_norm": 5.334699630737305,
        "learning_rate": 0.00017556683878713786,
        "epoch": 1.4864,
        "step": 11148
    },
    {
        "loss": 2.4641,
        "grad_norm": 4.713803768157959,
        "learning_rate": 0.00017552560312288952,
        "epoch": 1.4865333333333333,
        "step": 11149
    },
    {
        "loss": 2.1109,
        "grad_norm": 3.5508875846862793,
        "learning_rate": 0.0001754843375426924,
        "epoch": 1.4866666666666668,
        "step": 11150
    },
    {
        "loss": 1.8647,
        "grad_norm": 3.404391050338745,
        "learning_rate": 0.00017544304206289191,
        "epoch": 1.4868000000000001,
        "step": 11151
    },
    {
        "loss": 2.5779,
        "grad_norm": 2.5547425746917725,
        "learning_rate": 0.0001754017166998455,
        "epoch": 1.4869333333333334,
        "step": 11152
    },
    {
        "loss": 2.1867,
        "grad_norm": 3.664076328277588,
        "learning_rate": 0.00017536036146992214,
        "epoch": 1.4870666666666668,
        "step": 11153
    },
    {
        "loss": 2.3327,
        "grad_norm": 3.389194965362549,
        "learning_rate": 0.00017531897638950274,
        "epoch": 1.4872,
        "step": 11154
    },
    {
        "loss": 1.7458,
        "grad_norm": 5.05403470993042,
        "learning_rate": 0.00017527756147498025,
        "epoch": 1.4873333333333334,
        "step": 11155
    },
    {
        "loss": 2.3591,
        "grad_norm": 3.540407180786133,
        "learning_rate": 0.00017523611674275907,
        "epoch": 1.4874666666666667,
        "step": 11156
    },
    {
        "loss": 2.3766,
        "grad_norm": 4.686452388763428,
        "learning_rate": 0.00017519464220925574,
        "epoch": 1.4876,
        "step": 11157
    },
    {
        "loss": 2.266,
        "grad_norm": 2.8899755477905273,
        "learning_rate": 0.00017515313789089846,
        "epoch": 1.4877333333333334,
        "step": 11158
    },
    {
        "loss": 2.6049,
        "grad_norm": 2.629135847091675,
        "learning_rate": 0.00017511160380412715,
        "epoch": 1.4878666666666667,
        "step": 11159
    },
    {
        "loss": 1.413,
        "grad_norm": 4.183874607086182,
        "learning_rate": 0.00017507003996539356,
        "epoch": 1.488,
        "step": 11160
    },
    {
        "loss": 1.7324,
        "grad_norm": 4.8091936111450195,
        "learning_rate": 0.00017502844639116143,
        "epoch": 1.4881333333333333,
        "step": 11161
    },
    {
        "loss": 2.087,
        "grad_norm": 5.330116271972656,
        "learning_rate": 0.00017498682309790604,
        "epoch": 1.4882666666666666,
        "step": 11162
    },
    {
        "loss": 2.2639,
        "grad_norm": 2.9363298416137695,
        "learning_rate": 0.0001749451701021144,
        "epoch": 1.4884,
        "step": 11163
    },
    {
        "loss": 1.6925,
        "grad_norm": 4.949453353881836,
        "learning_rate": 0.00017490348742028563,
        "epoch": 1.4885333333333333,
        "step": 11164
    },
    {
        "loss": 2.3718,
        "grad_norm": 5.2338995933532715,
        "learning_rate": 0.00017486177506893027,
        "epoch": 1.4886666666666666,
        "step": 11165
    },
    {
        "loss": 0.6437,
        "grad_norm": 4.024813175201416,
        "learning_rate": 0.0001748200330645706,
        "epoch": 1.4888,
        "step": 11166
    },
    {
        "loss": 2.3407,
        "grad_norm": 3.8153798580169678,
        "learning_rate": 0.000174778261423741,
        "epoch": 1.4889333333333332,
        "step": 11167
    },
    {
        "loss": 2.3359,
        "grad_norm": 5.044826507568359,
        "learning_rate": 0.00017473646016298724,
        "epoch": 1.4890666666666665,
        "step": 11168
    },
    {
        "loss": 1.716,
        "grad_norm": 3.946969509124756,
        "learning_rate": 0.00017469462929886695,
        "epoch": 1.4892,
        "step": 11169
    },
    {
        "loss": 1.1923,
        "grad_norm": 3.2954607009887695,
        "learning_rate": 0.00017465276884794938,
        "epoch": 1.4893333333333334,
        "step": 11170
    },
    {
        "loss": 2.5421,
        "grad_norm": 3.4685847759246826,
        "learning_rate": 0.00017461087882681583,
        "epoch": 1.4894666666666667,
        "step": 11171
    },
    {
        "loss": 2.5292,
        "grad_norm": 4.749502658843994,
        "learning_rate": 0.0001745689592520588,
        "epoch": 1.4896,
        "step": 11172
    },
    {
        "loss": 2.0048,
        "grad_norm": 5.08140754699707,
        "learning_rate": 0.00017452701014028312,
        "epoch": 1.4897333333333334,
        "step": 11173
    },
    {
        "loss": 1.8782,
        "grad_norm": 4.831202983856201,
        "learning_rate": 0.00017448503150810463,
        "epoch": 1.4898666666666667,
        "step": 11174
    },
    {
        "loss": 2.4383,
        "grad_norm": 3.327526569366455,
        "learning_rate": 0.00017444302337215134,
        "epoch": 1.49,
        "step": 11175
    },
    {
        "loss": 2.4516,
        "grad_norm": 4.326951503753662,
        "learning_rate": 0.0001744009857490629,
        "epoch": 1.4901333333333333,
        "step": 11176
    },
    {
        "loss": 0.5823,
        "grad_norm": 2.625572681427002,
        "learning_rate": 0.0001743589186554905,
        "epoch": 1.4902666666666666,
        "step": 11177
    },
    {
        "loss": 2.2802,
        "grad_norm": 4.767251491546631,
        "learning_rate": 0.00017431682210809708,
        "epoch": 1.4904,
        "step": 11178
    },
    {
        "loss": 2.2107,
        "grad_norm": 2.6746349334716797,
        "learning_rate": 0.00017427469612355708,
        "epoch": 1.4905333333333333,
        "step": 11179
    },
    {
        "loss": 1.9767,
        "grad_norm": 4.249734401702881,
        "learning_rate": 0.00017423254071855695,
        "epoch": 1.4906666666666666,
        "step": 11180
    },
    {
        "loss": 2.4572,
        "grad_norm": 4.686873435974121,
        "learning_rate": 0.00017419035590979443,
        "epoch": 1.4908000000000001,
        "step": 11181
    },
    {
        "loss": 2.6032,
        "grad_norm": 3.3540170192718506,
        "learning_rate": 0.00017414814171397928,
        "epoch": 1.4909333333333334,
        "step": 11182
    },
    {
        "loss": 2.1915,
        "grad_norm": 4.135111331939697,
        "learning_rate": 0.0001741058981478324,
        "epoch": 1.4910666666666668,
        "step": 11183
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.3999319076538086,
        "learning_rate": 0.00017406362522808671,
        "epoch": 1.4912,
        "step": 11184
    },
    {
        "loss": 2.7862,
        "grad_norm": 2.7624998092651367,
        "learning_rate": 0.00017402132297148683,
        "epoch": 1.4913333333333334,
        "step": 11185
    },
    {
        "loss": 2.869,
        "grad_norm": 4.676908493041992,
        "learning_rate": 0.00017397899139478867,
        "epoch": 1.4914666666666667,
        "step": 11186
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.7307934761047363,
        "learning_rate": 0.00017393663051475997,
        "epoch": 1.4916,
        "step": 11187
    },
    {
        "loss": 0.9736,
        "grad_norm": 4.456662178039551,
        "learning_rate": 0.0001738942403481799,
        "epoch": 1.4917333333333334,
        "step": 11188
    },
    {
        "loss": 2.4884,
        "grad_norm": 4.825345993041992,
        "learning_rate": 0.00017385182091183952,
        "epoch": 1.4918666666666667,
        "step": 11189
    },
    {
        "loss": 1.2872,
        "grad_norm": 6.325101852416992,
        "learning_rate": 0.00017380937222254123,
        "epoch": 1.492,
        "step": 11190
    },
    {
        "loss": 2.3734,
        "grad_norm": 4.071255207061768,
        "learning_rate": 0.0001737668942970991,
        "epoch": 1.4921333333333333,
        "step": 11191
    },
    {
        "loss": 2.7684,
        "grad_norm": 4.0894389152526855,
        "learning_rate": 0.00017372438715233872,
        "epoch": 1.4922666666666666,
        "step": 11192
    },
    {
        "loss": 2.3136,
        "grad_norm": 3.4986348152160645,
        "learning_rate": 0.00017368185080509735,
        "epoch": 1.4924,
        "step": 11193
    },
    {
        "loss": 2.0992,
        "grad_norm": 4.744475841522217,
        "learning_rate": 0.00017363928527222398,
        "epoch": 1.4925333333333333,
        "step": 11194
    },
    {
        "loss": 2.2859,
        "grad_norm": 3.0521163940429688,
        "learning_rate": 0.00017359669057057862,
        "epoch": 1.4926666666666666,
        "step": 11195
    },
    {
        "loss": 2.4724,
        "grad_norm": 3.4789774417877197,
        "learning_rate": 0.00017355406671703348,
        "epoch": 1.4928,
        "step": 11196
    },
    {
        "loss": 2.0702,
        "grad_norm": 2.905630588531494,
        "learning_rate": 0.00017351141372847178,
        "epoch": 1.4929333333333332,
        "step": 11197
    },
    {
        "loss": 2.3177,
        "grad_norm": 3.254260540008545,
        "learning_rate": 0.0001734687316217887,
        "epoch": 1.4930666666666665,
        "step": 11198
    },
    {
        "loss": 3.0172,
        "grad_norm": 5.018803119659424,
        "learning_rate": 0.00017342602041389067,
        "epoch": 1.4932,
        "step": 11199
    },
    {
        "loss": 2.1469,
        "grad_norm": 3.830350399017334,
        "learning_rate": 0.00017338328012169577,
        "epoch": 1.4933333333333334,
        "step": 11200
    },
    {
        "loss": 1.9592,
        "grad_norm": 3.803740978240967,
        "learning_rate": 0.00017334051076213348,
        "epoch": 1.4934666666666667,
        "step": 11201
    },
    {
        "loss": 1.7082,
        "grad_norm": 3.9677929878234863,
        "learning_rate": 0.00017329771235214498,
        "epoch": 1.4936,
        "step": 11202
    },
    {
        "loss": 2.5425,
        "grad_norm": 2.7163827419281006,
        "learning_rate": 0.00017325488490868305,
        "epoch": 1.4937333333333334,
        "step": 11203
    },
    {
        "loss": 1.5965,
        "grad_norm": 4.385868072509766,
        "learning_rate": 0.0001732120284487114,
        "epoch": 1.4938666666666667,
        "step": 11204
    },
    {
        "loss": 2.478,
        "grad_norm": 5.604902744293213,
        "learning_rate": 0.0001731691429892059,
        "epoch": 1.494,
        "step": 11205
    },
    {
        "loss": 1.5455,
        "grad_norm": 4.270903587341309,
        "learning_rate": 0.0001731262285471535,
        "epoch": 1.4941333333333333,
        "step": 11206
    },
    {
        "loss": 2.7536,
        "grad_norm": 3.3908920288085938,
        "learning_rate": 0.00017308328513955282,
        "epoch": 1.4942666666666666,
        "step": 11207
    },
    {
        "loss": 1.6229,
        "grad_norm": 4.5301032066345215,
        "learning_rate": 0.00017304031278341393,
        "epoch": 1.4944,
        "step": 11208
    },
    {
        "loss": 2.5126,
        "grad_norm": 4.148528575897217,
        "learning_rate": 0.00017299731149575823,
        "epoch": 1.4945333333333333,
        "step": 11209
    },
    {
        "loss": 1.7859,
        "grad_norm": 3.966952085494995,
        "learning_rate": 0.00017295428129361867,
        "epoch": 1.4946666666666666,
        "step": 11210
    },
    {
        "loss": 1.7661,
        "grad_norm": 3.886918067932129,
        "learning_rate": 0.00017291122219403974,
        "epoch": 1.4948000000000001,
        "step": 11211
    },
    {
        "loss": 2.3906,
        "grad_norm": 4.547793865203857,
        "learning_rate": 0.0001728681342140773,
        "epoch": 1.4949333333333334,
        "step": 11212
    },
    {
        "loss": 1.5782,
        "grad_norm": 4.316465377807617,
        "learning_rate": 0.0001728250173707985,
        "epoch": 1.4950666666666668,
        "step": 11213
    },
    {
        "loss": 1.4058,
        "grad_norm": 4.585460186004639,
        "learning_rate": 0.00017278187168128225,
        "epoch": 1.4952,
        "step": 11214
    },
    {
        "loss": 2.8476,
        "grad_norm": 2.3138599395751953,
        "learning_rate": 0.00017273869716261865,
        "epoch": 1.4953333333333334,
        "step": 11215
    },
    {
        "loss": 2.063,
        "grad_norm": 3.116872787475586,
        "learning_rate": 0.00017269549383190915,
        "epoch": 1.4954666666666667,
        "step": 11216
    },
    {
        "loss": 1.1552,
        "grad_norm": 4.440016269683838,
        "learning_rate": 0.00017265226170626695,
        "epoch": 1.4956,
        "step": 11217
    },
    {
        "loss": 2.1354,
        "grad_norm": 3.446622848510742,
        "learning_rate": 0.0001726090008028163,
        "epoch": 1.4957333333333334,
        "step": 11218
    },
    {
        "loss": 1.816,
        "grad_norm": 2.8613088130950928,
        "learning_rate": 0.00017256571113869298,
        "epoch": 1.4958666666666667,
        "step": 11219
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.9210598468780518,
        "learning_rate": 0.00017252239273104426,
        "epoch": 1.496,
        "step": 11220
    },
    {
        "loss": 2.3765,
        "grad_norm": 4.223464488983154,
        "learning_rate": 0.0001724790455970287,
        "epoch": 1.4961333333333333,
        "step": 11221
    },
    {
        "loss": 2.4454,
        "grad_norm": 5.228836536407471,
        "learning_rate": 0.0001724356697538161,
        "epoch": 1.4962666666666666,
        "step": 11222
    },
    {
        "loss": 2.6751,
        "grad_norm": 4.052644729614258,
        "learning_rate": 0.000172392265218588,
        "epoch": 1.4964,
        "step": 11223
    },
    {
        "loss": 2.391,
        "grad_norm": 3.405095338821411,
        "learning_rate": 0.00017234883200853695,
        "epoch": 1.4965333333333333,
        "step": 11224
    },
    {
        "loss": 1.6784,
        "grad_norm": 3.105318784713745,
        "learning_rate": 0.00017230537014086694,
        "epoch": 1.4966666666666666,
        "step": 11225
    },
    {
        "loss": 3.0629,
        "grad_norm": 2.705939531326294,
        "learning_rate": 0.00017226187963279355,
        "epoch": 1.4968,
        "step": 11226
    },
    {
        "loss": 2.039,
        "grad_norm": 4.0984601974487305,
        "learning_rate": 0.00017221836050154338,
        "epoch": 1.4969333333333332,
        "step": 11227
    },
    {
        "loss": 2.251,
        "grad_norm": 3.38374924659729,
        "learning_rate": 0.00017217481276435455,
        "epoch": 1.4970666666666665,
        "step": 11228
    },
    {
        "loss": 2.0964,
        "grad_norm": 2.9250807762145996,
        "learning_rate": 0.00017213123643847636,
        "epoch": 1.4971999999999999,
        "step": 11229
    },
    {
        "loss": 1.6811,
        "grad_norm": 3.084280252456665,
        "learning_rate": 0.00017208763154116972,
        "epoch": 1.4973333333333334,
        "step": 11230
    },
    {
        "loss": 0.6186,
        "grad_norm": 2.683532953262329,
        "learning_rate": 0.00017204399808970654,
        "epoch": 1.4974666666666667,
        "step": 11231
    },
    {
        "loss": 1.7993,
        "grad_norm": 3.4463117122650146,
        "learning_rate": 0.0001720003361013704,
        "epoch": 1.4976,
        "step": 11232
    },
    {
        "loss": 1.7152,
        "grad_norm": 2.791426658630371,
        "learning_rate": 0.00017195664559345559,
        "epoch": 1.4977333333333334,
        "step": 11233
    },
    {
        "loss": 0.6865,
        "grad_norm": 3.8782966136932373,
        "learning_rate": 0.0001719129265832683,
        "epoch": 1.4978666666666667,
        "step": 11234
    },
    {
        "loss": 2.4422,
        "grad_norm": 3.3292717933654785,
        "learning_rate": 0.00017186917908812584,
        "epoch": 1.498,
        "step": 11235
    },
    {
        "loss": 1.181,
        "grad_norm": 4.612534523010254,
        "learning_rate": 0.00017182540312535665,
        "epoch": 1.4981333333333333,
        "step": 11236
    },
    {
        "loss": 1.9052,
        "grad_norm": 3.78267765045166,
        "learning_rate": 0.00017178159871230055,
        "epoch": 1.4982666666666666,
        "step": 11237
    },
    {
        "loss": 2.3716,
        "grad_norm": 4.03751802444458,
        "learning_rate": 0.0001717377658663085,
        "epoch": 1.4984,
        "step": 11238
    },
    {
        "loss": 1.5422,
        "grad_norm": 4.521271228790283,
        "learning_rate": 0.00017169390460474303,
        "epoch": 1.4985333333333333,
        "step": 11239
    },
    {
        "loss": 2.7385,
        "grad_norm": 3.1728312969207764,
        "learning_rate": 0.0001716500149449776,
        "epoch": 1.4986666666666666,
        "step": 11240
    },
    {
        "loss": 2.2248,
        "grad_norm": 3.240903377532959,
        "learning_rate": 0.00017160609690439728,
        "epoch": 1.4988000000000001,
        "step": 11241
    },
    {
        "loss": 2.5462,
        "grad_norm": 2.74540114402771,
        "learning_rate": 0.00017156215050039778,
        "epoch": 1.4989333333333335,
        "step": 11242
    },
    {
        "loss": 1.6291,
        "grad_norm": 3.7962193489074707,
        "learning_rate": 0.00017151817575038666,
        "epoch": 1.4990666666666668,
        "step": 11243
    },
    {
        "loss": 2.7629,
        "grad_norm": 3.6489946842193604,
        "learning_rate": 0.00017147417267178252,
        "epoch": 1.4992,
        "step": 11244
    },
    {
        "loss": 1.7848,
        "grad_norm": 4.917552947998047,
        "learning_rate": 0.00017143014128201503,
        "epoch": 1.4993333333333334,
        "step": 11245
    },
    {
        "loss": 2.0477,
        "grad_norm": 4.562051296234131,
        "learning_rate": 0.00017138608159852518,
        "epoch": 1.4994666666666667,
        "step": 11246
    },
    {
        "loss": 2.0771,
        "grad_norm": 5.666045665740967,
        "learning_rate": 0.00017134199363876506,
        "epoch": 1.4996,
        "step": 11247
    },
    {
        "loss": 2.7489,
        "grad_norm": 3.458080768585205,
        "learning_rate": 0.0001712978774201983,
        "epoch": 1.4997333333333334,
        "step": 11248
    },
    {
        "loss": 1.7725,
        "grad_norm": 4.274613380432129,
        "learning_rate": 0.00017125373296029932,
        "epoch": 1.4998666666666667,
        "step": 11249
    },
    {
        "loss": 2.3256,
        "grad_norm": 4.178677082061768,
        "learning_rate": 0.00017120956027655396,
        "epoch": 1.5,
        "step": 11250
    },
    {
        "loss": 2.4212,
        "grad_norm": 2.5176286697387695,
        "learning_rate": 0.00017116535938645902,
        "epoch": 1.5001333333333333,
        "step": 11251
    },
    {
        "loss": 2.9021,
        "grad_norm": 4.238375663757324,
        "learning_rate": 0.00017112113030752274,
        "epoch": 1.5002666666666666,
        "step": 11252
    },
    {
        "loss": 1.3024,
        "grad_norm": 4.055665493011475,
        "learning_rate": 0.00017107687305726457,
        "epoch": 1.5004,
        "step": 11253
    },
    {
        "loss": 2.6394,
        "grad_norm": 4.254670143127441,
        "learning_rate": 0.0001710325876532146,
        "epoch": 1.5005333333333333,
        "step": 11254
    },
    {
        "loss": 2.8308,
        "grad_norm": 5.483039855957031,
        "learning_rate": 0.00017098827411291475,
        "epoch": 1.5006666666666666,
        "step": 11255
    },
    {
        "loss": 2.3329,
        "grad_norm": 4.433493614196777,
        "learning_rate": 0.00017094393245391753,
        "epoch": 1.5008,
        "step": 11256
    },
    {
        "loss": 2.267,
        "grad_norm": 3.987441301345825,
        "learning_rate": 0.00017089956269378704,
        "epoch": 1.5009333333333332,
        "step": 11257
    },
    {
        "loss": 1.4725,
        "grad_norm": 3.3192787170410156,
        "learning_rate": 0.00017085516485009815,
        "epoch": 1.5010666666666665,
        "step": 11258
    },
    {
        "loss": 1.5556,
        "grad_norm": 3.184938430786133,
        "learning_rate": 0.00017081073894043708,
        "epoch": 1.5011999999999999,
        "step": 11259
    },
    {
        "loss": 3.0145,
        "grad_norm": 3.0124094486236572,
        "learning_rate": 0.00017076628498240092,
        "epoch": 1.5013333333333332,
        "step": 11260
    },
    {
        "loss": 2.7371,
        "grad_norm": 3.08508563041687,
        "learning_rate": 0.0001707218029935981,
        "epoch": 1.5014666666666665,
        "step": 11261
    },
    {
        "loss": 2.6164,
        "grad_norm": 4.536094665527344,
        "learning_rate": 0.00017067729299164838,
        "epoch": 1.5016,
        "step": 11262
    },
    {
        "loss": 2.1445,
        "grad_norm": 4.661146640777588,
        "learning_rate": 0.00017063275499418185,
        "epoch": 1.5017333333333334,
        "step": 11263
    },
    {
        "loss": 0.6377,
        "grad_norm": 2.734616994857788,
        "learning_rate": 0.0001705881890188405,
        "epoch": 1.5018666666666667,
        "step": 11264
    },
    {
        "loss": 2.0262,
        "grad_norm": 4.521877288818359,
        "learning_rate": 0.0001705435950832769,
        "epoch": 1.502,
        "step": 11265
    },
    {
        "loss": 1.9581,
        "grad_norm": 4.699721336364746,
        "learning_rate": 0.00017049897320515497,
        "epoch": 1.5021333333333333,
        "step": 11266
    },
    {
        "loss": 2.5188,
        "grad_norm": 4.535784721374512,
        "learning_rate": 0.00017045432340214952,
        "epoch": 1.5022666666666666,
        "step": 11267
    },
    {
        "loss": 1.4595,
        "grad_norm": 4.665225505828857,
        "learning_rate": 0.00017040964569194655,
        "epoch": 1.5024,
        "step": 11268
    },
    {
        "loss": 2.221,
        "grad_norm": 3.833003520965576,
        "learning_rate": 0.0001703649400922429,
        "epoch": 1.5025333333333335,
        "step": 11269
    },
    {
        "loss": 1.8022,
        "grad_norm": 4.184795379638672,
        "learning_rate": 0.00017032020662074678,
        "epoch": 1.5026666666666668,
        "step": 11270
    },
    {
        "loss": 2.2258,
        "grad_norm": 4.568500518798828,
        "learning_rate": 0.00017027544529517723,
        "epoch": 1.5028000000000001,
        "step": 11271
    },
    {
        "loss": 2.7202,
        "grad_norm": 3.4814887046813965,
        "learning_rate": 0.00017023065613326426,
        "epoch": 1.5029333333333335,
        "step": 11272
    },
    {
        "loss": 2.4066,
        "grad_norm": 2.388462543487549,
        "learning_rate": 0.00017018583915274916,
        "epoch": 1.5030666666666668,
        "step": 11273
    },
    {
        "loss": 2.7694,
        "grad_norm": 4.192413330078125,
        "learning_rate": 0.00017014099437138406,
        "epoch": 1.5032,
        "step": 11274
    },
    {
        "loss": 1.2061,
        "grad_norm": 4.606496810913086,
        "learning_rate": 0.00017009612180693195,
        "epoch": 1.5033333333333334,
        "step": 11275
    },
    {
        "loss": 1.4068,
        "grad_norm": 6.479022979736328,
        "learning_rate": 0.00017005122147716723,
        "epoch": 1.5034666666666667,
        "step": 11276
    },
    {
        "loss": 2.3497,
        "grad_norm": 2.0425000190734863,
        "learning_rate": 0.00017000629339987503,
        "epoch": 1.5036,
        "step": 11277
    },
    {
        "loss": 2.3168,
        "grad_norm": 3.8203468322753906,
        "learning_rate": 0.00016996133759285135,
        "epoch": 1.5037333333333334,
        "step": 11278
    },
    {
        "loss": 1.5914,
        "grad_norm": 2.8252131938934326,
        "learning_rate": 0.00016991635407390357,
        "epoch": 1.5038666666666667,
        "step": 11279
    },
    {
        "loss": 1.3299,
        "grad_norm": 3.976672410964966,
        "learning_rate": 0.00016987134286084972,
        "epoch": 1.504,
        "step": 11280
    },
    {
        "loss": 1.2587,
        "grad_norm": 4.818986892700195,
        "learning_rate": 0.00016982630397151876,
        "epoch": 1.5041333333333333,
        "step": 11281
    },
    {
        "loss": 2.0492,
        "grad_norm": 5.039948463439941,
        "learning_rate": 0.000169781237423751,
        "epoch": 1.5042666666666666,
        "step": 11282
    },
    {
        "loss": 1.4524,
        "grad_norm": 5.861494541168213,
        "learning_rate": 0.00016973614323539731,
        "epoch": 1.5044,
        "step": 11283
    },
    {
        "loss": 2.3955,
        "grad_norm": 3.4118106365203857,
        "learning_rate": 0.00016969102142431954,
        "epoch": 1.5045333333333333,
        "step": 11284
    },
    {
        "loss": 1.5614,
        "grad_norm": 5.289982318878174,
        "learning_rate": 0.00016964587200839085,
        "epoch": 1.5046666666666666,
        "step": 11285
    },
    {
        "loss": 2.317,
        "grad_norm": 2.6283700466156006,
        "learning_rate": 0.00016960069500549492,
        "epoch": 1.5048,
        "step": 11286
    },
    {
        "loss": 1.481,
        "grad_norm": 3.5503995418548584,
        "learning_rate": 0.00016955549043352655,
        "epoch": 1.5049333333333332,
        "step": 11287
    },
    {
        "loss": 1.9021,
        "grad_norm": 5.531608581542969,
        "learning_rate": 0.0001695102583103913,
        "epoch": 1.5050666666666666,
        "step": 11288
    },
    {
        "loss": 1.9099,
        "grad_norm": 3.3339734077453613,
        "learning_rate": 0.000169464998654006,
        "epoch": 1.5051999999999999,
        "step": 11289
    },
    {
        "loss": 2.3803,
        "grad_norm": 3.1634933948516846,
        "learning_rate": 0.0001694197114822979,
        "epoch": 1.5053333333333332,
        "step": 11290
    },
    {
        "loss": 2.6113,
        "grad_norm": 4.459736347198486,
        "learning_rate": 0.00016937439681320582,
        "epoch": 1.5054666666666665,
        "step": 11291
    },
    {
        "loss": 2.3608,
        "grad_norm": 3.939253807067871,
        "learning_rate": 0.0001693290546646785,
        "epoch": 1.5056,
        "step": 11292
    },
    {
        "loss": 2.2127,
        "grad_norm": 4.480063438415527,
        "learning_rate": 0.00016928368505467645,
        "epoch": 1.5057333333333334,
        "step": 11293
    },
    {
        "loss": 3.129,
        "grad_norm": 3.150315761566162,
        "learning_rate": 0.00016923828800117073,
        "epoch": 1.5058666666666667,
        "step": 11294
    },
    {
        "loss": 2.3143,
        "grad_norm": 3.5726397037506104,
        "learning_rate": 0.00016919286352214322,
        "epoch": 1.506,
        "step": 11295
    },
    {
        "loss": 1.746,
        "grad_norm": 4.415836334228516,
        "learning_rate": 0.00016914741163558673,
        "epoch": 1.5061333333333333,
        "step": 11296
    },
    {
        "loss": 1.902,
        "grad_norm": 4.267858028411865,
        "learning_rate": 0.00016910193235950474,
        "epoch": 1.5062666666666666,
        "step": 11297
    },
    {
        "loss": 2.874,
        "grad_norm": 3.694561243057251,
        "learning_rate": 0.00016905642571191199,
        "epoch": 1.5064,
        "step": 11298
    },
    {
        "loss": 2.3524,
        "grad_norm": 4.057442665100098,
        "learning_rate": 0.0001690108917108336,
        "epoch": 1.5065333333333333,
        "step": 11299
    },
    {
        "loss": 2.5588,
        "grad_norm": 3.9505257606506348,
        "learning_rate": 0.0001689653303743061,
        "epoch": 1.5066666666666668,
        "step": 11300
    },
    {
        "loss": 2.02,
        "grad_norm": 3.2575628757476807,
        "learning_rate": 0.000168919741720376,
        "epoch": 1.5068000000000001,
        "step": 11301
    },
    {
        "loss": 2.9499,
        "grad_norm": 2.389364719390869,
        "learning_rate": 0.0001688741257671014,
        "epoch": 1.5069333333333335,
        "step": 11302
    },
    {
        "loss": 1.086,
        "grad_norm": 5.705771446228027,
        "learning_rate": 0.00016882848253255096,
        "epoch": 1.5070666666666668,
        "step": 11303
    },
    {
        "loss": 1.9251,
        "grad_norm": 2.2763559818267822,
        "learning_rate": 0.0001687828120348041,
        "epoch": 1.5072,
        "step": 11304
    },
    {
        "loss": 1.7046,
        "grad_norm": 4.2102742195129395,
        "learning_rate": 0.00016873711429195098,
        "epoch": 1.5073333333333334,
        "step": 11305
    },
    {
        "loss": 1.9614,
        "grad_norm": 3.280989646911621,
        "learning_rate": 0.00016869138932209258,
        "epoch": 1.5074666666666667,
        "step": 11306
    },
    {
        "loss": 1.936,
        "grad_norm": 5.939944744110107,
        "learning_rate": 0.00016864563714334097,
        "epoch": 1.5076,
        "step": 11307
    },
    {
        "loss": 2.0814,
        "grad_norm": 5.051661968231201,
        "learning_rate": 0.00016859985777381852,
        "epoch": 1.5077333333333334,
        "step": 11308
    },
    {
        "loss": 1.9742,
        "grad_norm": 4.006495952606201,
        "learning_rate": 0.00016855405123165869,
        "epoch": 1.5078666666666667,
        "step": 11309
    },
    {
        "loss": 2.224,
        "grad_norm": 3.331160068511963,
        "learning_rate": 0.00016850821753500546,
        "epoch": 1.508,
        "step": 11310
    },
    {
        "loss": 2.218,
        "grad_norm": 3.105180025100708,
        "learning_rate": 0.00016846235670201382,
        "epoch": 1.5081333333333333,
        "step": 11311
    },
    {
        "loss": 2.797,
        "grad_norm": 3.237116813659668,
        "learning_rate": 0.00016841646875084964,
        "epoch": 1.5082666666666666,
        "step": 11312
    },
    {
        "loss": 2.6967,
        "grad_norm": 2.9785847663879395,
        "learning_rate": 0.00016837055369968884,
        "epoch": 1.5084,
        "step": 11313
    },
    {
        "loss": 0.6206,
        "grad_norm": 3.184732675552368,
        "learning_rate": 0.00016832461156671884,
        "epoch": 1.5085333333333333,
        "step": 11314
    },
    {
        "loss": 2.7536,
        "grad_norm": 5.9160966873168945,
        "learning_rate": 0.00016827864237013734,
        "epoch": 1.5086666666666666,
        "step": 11315
    },
    {
        "loss": 1.3799,
        "grad_norm": 2.7724649906158447,
        "learning_rate": 0.000168232646128153,
        "epoch": 1.5088,
        "step": 11316
    },
    {
        "loss": 1.286,
        "grad_norm": 3.843367099761963,
        "learning_rate": 0.00016818662285898503,
        "epoch": 1.5089333333333332,
        "step": 11317
    },
    {
        "loss": 2.543,
        "grad_norm": 4.348833084106445,
        "learning_rate": 0.00016814057258086342,
        "epoch": 1.5090666666666666,
        "step": 11318
    },
    {
        "loss": 1.9767,
        "grad_norm": 3.7137742042541504,
        "learning_rate": 0.00016809449531202873,
        "epoch": 1.5091999999999999,
        "step": 11319
    },
    {
        "loss": 1.7215,
        "grad_norm": 4.948136806488037,
        "learning_rate": 0.0001680483910707324,
        "epoch": 1.5093333333333332,
        "step": 11320
    },
    {
        "loss": 1.5424,
        "grad_norm": 6.806296348571777,
        "learning_rate": 0.00016800225987523672,
        "epoch": 1.5094666666666665,
        "step": 11321
    },
    {
        "loss": 2.4327,
        "grad_norm": 3.4625515937805176,
        "learning_rate": 0.000167956101743814,
        "epoch": 1.5096,
        "step": 11322
    },
    {
        "loss": 2.747,
        "grad_norm": 3.666287660598755,
        "learning_rate": 0.00016790991669474788,
        "epoch": 1.5097333333333334,
        "step": 11323
    },
    {
        "loss": 2.1883,
        "grad_norm": 5.227997779846191,
        "learning_rate": 0.00016786370474633228,
        "epoch": 1.5098666666666667,
        "step": 11324
    },
    {
        "loss": 2.2612,
        "grad_norm": 2.9883968830108643,
        "learning_rate": 0.00016781746591687212,
        "epoch": 1.51,
        "step": 11325
    },
    {
        "loss": 2.1955,
        "grad_norm": 2.8374991416931152,
        "learning_rate": 0.0001677712002246826,
        "epoch": 1.5101333333333333,
        "step": 11326
    },
    {
        "loss": 1.8437,
        "grad_norm": 4.277966022491455,
        "learning_rate": 0.00016772490768808974,
        "epoch": 1.5102666666666666,
        "step": 11327
    },
    {
        "loss": 0.7849,
        "grad_norm": 4.382285118103027,
        "learning_rate": 0.0001676785883254301,
        "epoch": 1.5104,
        "step": 11328
    },
    {
        "loss": 2.1457,
        "grad_norm": 5.142185211181641,
        "learning_rate": 0.00016763224215505114,
        "epoch": 1.5105333333333333,
        "step": 11329
    },
    {
        "loss": 1.666,
        "grad_norm": 5.724437236785889,
        "learning_rate": 0.00016758586919531055,
        "epoch": 1.5106666666666668,
        "step": 11330
    },
    {
        "loss": 1.5311,
        "grad_norm": 3.895587921142578,
        "learning_rate": 0.0001675394694645768,
        "epoch": 1.5108000000000001,
        "step": 11331
    },
    {
        "loss": 2.5108,
        "grad_norm": 3.0645573139190674,
        "learning_rate": 0.00016749304298122918,
        "epoch": 1.5109333333333335,
        "step": 11332
    },
    {
        "loss": 2.9543,
        "grad_norm": 4.508907794952393,
        "learning_rate": 0.00016744658976365723,
        "epoch": 1.5110666666666668,
        "step": 11333
    },
    {
        "loss": 1.9047,
        "grad_norm": 4.4264326095581055,
        "learning_rate": 0.0001674001098302612,
        "epoch": 1.5112,
        "step": 11334
    },
    {
        "loss": 2.3519,
        "grad_norm": 3.8287363052368164,
        "learning_rate": 0.00016735360319945205,
        "epoch": 1.5113333333333334,
        "step": 11335
    },
    {
        "loss": 2.3851,
        "grad_norm": 4.347764015197754,
        "learning_rate": 0.0001673070698896512,
        "epoch": 1.5114666666666667,
        "step": 11336
    },
    {
        "loss": 2.4001,
        "grad_norm": 3.6646430492401123,
        "learning_rate": 0.00016726050991929052,
        "epoch": 1.5116,
        "step": 11337
    },
    {
        "loss": 2.0174,
        "grad_norm": 3.8232271671295166,
        "learning_rate": 0.00016721392330681277,
        "epoch": 1.5117333333333334,
        "step": 11338
    },
    {
        "loss": 2.4338,
        "grad_norm": 4.075198650360107,
        "learning_rate": 0.00016716731007067098,
        "epoch": 1.5118666666666667,
        "step": 11339
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.6739628314971924,
        "learning_rate": 0.0001671206702293287,
        "epoch": 1.512,
        "step": 11340
    },
    {
        "loss": 0.771,
        "grad_norm": 5.412998676300049,
        "learning_rate": 0.00016707400380126035,
        "epoch": 1.5121333333333333,
        "step": 11341
    },
    {
        "loss": 2.1826,
        "grad_norm": 3.6147611141204834,
        "learning_rate": 0.00016702731080495053,
        "epoch": 1.5122666666666666,
        "step": 11342
    },
    {
        "loss": 2.5681,
        "grad_norm": 2.515462636947632,
        "learning_rate": 0.00016698059125889441,
        "epoch": 1.5124,
        "step": 11343
    },
    {
        "loss": 2.5759,
        "grad_norm": 3.141587972640991,
        "learning_rate": 0.000166933845181598,
        "epoch": 1.5125333333333333,
        "step": 11344
    },
    {
        "loss": 2.4333,
        "grad_norm": 3.4934232234954834,
        "learning_rate": 0.0001668870725915774,
        "epoch": 1.5126666666666666,
        "step": 11345
    },
    {
        "loss": 2.5443,
        "grad_norm": 3.6872806549072266,
        "learning_rate": 0.00016684027350735942,
        "epoch": 1.5128,
        "step": 11346
    },
    {
        "loss": 2.6168,
        "grad_norm": 3.307422637939453,
        "learning_rate": 0.00016679344794748126,
        "epoch": 1.5129333333333332,
        "step": 11347
    },
    {
        "loss": 2.2033,
        "grad_norm": 5.942326068878174,
        "learning_rate": 0.0001667465959304909,
        "epoch": 1.5130666666666666,
        "step": 11348
    },
    {
        "loss": 1.4243,
        "grad_norm": 5.552105903625488,
        "learning_rate": 0.00016669971747494634,
        "epoch": 1.5131999999999999,
        "step": 11349
    },
    {
        "loss": 1.0587,
        "grad_norm": 3.8654775619506836,
        "learning_rate": 0.00016665281259941662,
        "epoch": 1.5133333333333332,
        "step": 11350
    },
    {
        "loss": 0.9568,
        "grad_norm": 3.9891016483306885,
        "learning_rate": 0.00016660588132248053,
        "epoch": 1.5134666666666665,
        "step": 11351
    },
    {
        "loss": 2.2867,
        "grad_norm": 4.725148677825928,
        "learning_rate": 0.00016655892366272787,
        "epoch": 1.5135999999999998,
        "step": 11352
    },
    {
        "loss": 1.6939,
        "grad_norm": 6.292198181152344,
        "learning_rate": 0.00016651193963875886,
        "epoch": 1.5137333333333334,
        "step": 11353
    },
    {
        "loss": 2.2582,
        "grad_norm": 5.649800777435303,
        "learning_rate": 0.00016646492926918394,
        "epoch": 1.5138666666666667,
        "step": 11354
    },
    {
        "loss": 2.8549,
        "grad_norm": 3.959388494491577,
        "learning_rate": 0.00016641789257262402,
        "epoch": 1.514,
        "step": 11355
    },
    {
        "loss": 2.7646,
        "grad_norm": 3.5592124462127686,
        "learning_rate": 0.00016637082956771047,
        "epoch": 1.5141333333333333,
        "step": 11356
    },
    {
        "loss": 1.7809,
        "grad_norm": 6.03020715713501,
        "learning_rate": 0.00016632374027308528,
        "epoch": 1.5142666666666666,
        "step": 11357
    },
    {
        "loss": 2.2902,
        "grad_norm": 4.391932010650635,
        "learning_rate": 0.00016627662470740048,
        "epoch": 1.5144,
        "step": 11358
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.160628318786621,
        "learning_rate": 0.000166229482889319,
        "epoch": 1.5145333333333333,
        "step": 11359
    },
    {
        "loss": 2.3891,
        "grad_norm": 4.6767683029174805,
        "learning_rate": 0.00016618231483751344,
        "epoch": 1.5146666666666668,
        "step": 11360
    },
    {
        "loss": 2.484,
        "grad_norm": 5.072598457336426,
        "learning_rate": 0.00016613512057066753,
        "epoch": 1.5148000000000001,
        "step": 11361
    },
    {
        "loss": 3.0664,
        "grad_norm": 3.552983045578003,
        "learning_rate": 0.00016608790010747506,
        "epoch": 1.5149333333333335,
        "step": 11362
    },
    {
        "loss": 2.2762,
        "grad_norm": 3.2592928409576416,
        "learning_rate": 0.00016604065346664018,
        "epoch": 1.5150666666666668,
        "step": 11363
    },
    {
        "loss": 1.9471,
        "grad_norm": 3.63673734664917,
        "learning_rate": 0.0001659933806668774,
        "epoch": 1.5152,
        "step": 11364
    },
    {
        "loss": 2.578,
        "grad_norm": 3.8833608627319336,
        "learning_rate": 0.00016594608172691163,
        "epoch": 1.5153333333333334,
        "step": 11365
    },
    {
        "loss": 1.7544,
        "grad_norm": 3.9036672115325928,
        "learning_rate": 0.00016589875666547826,
        "epoch": 1.5154666666666667,
        "step": 11366
    },
    {
        "loss": 1.0187,
        "grad_norm": 4.540670394897461,
        "learning_rate": 0.00016585140550132282,
        "epoch": 1.5156,
        "step": 11367
    },
    {
        "loss": 2.1873,
        "grad_norm": 3.746572256088257,
        "learning_rate": 0.0001658040282532013,
        "epoch": 1.5157333333333334,
        "step": 11368
    },
    {
        "loss": 2.1377,
        "grad_norm": 2.5366599559783936,
        "learning_rate": 0.00016575662493987985,
        "epoch": 1.5158666666666667,
        "step": 11369
    },
    {
        "loss": 1.9965,
        "grad_norm": 4.959872245788574,
        "learning_rate": 0.00016570919558013517,
        "epoch": 1.516,
        "step": 11370
    },
    {
        "loss": 2.1433,
        "grad_norm": 4.576566219329834,
        "learning_rate": 0.0001656617401927545,
        "epoch": 1.5161333333333333,
        "step": 11371
    },
    {
        "loss": 2.5623,
        "grad_norm": 4.368076801300049,
        "learning_rate": 0.00016561425879653454,
        "epoch": 1.5162666666666667,
        "step": 11372
    },
    {
        "loss": 2.375,
        "grad_norm": 5.13516092300415,
        "learning_rate": 0.00016556675141028322,
        "epoch": 1.5164,
        "step": 11373
    },
    {
        "loss": 2.2926,
        "grad_norm": 4.287834167480469,
        "learning_rate": 0.00016551921805281814,
        "epoch": 1.5165333333333333,
        "step": 11374
    },
    {
        "loss": 2.0376,
        "grad_norm": 4.727408409118652,
        "learning_rate": 0.00016547165874296763,
        "epoch": 1.5166666666666666,
        "step": 11375
    },
    {
        "loss": 2.3148,
        "grad_norm": 3.1815357208251953,
        "learning_rate": 0.00016542407349957004,
        "epoch": 1.5168,
        "step": 11376
    },
    {
        "loss": 1.6485,
        "grad_norm": 4.068893909454346,
        "learning_rate": 0.00016537646234147398,
        "epoch": 1.5169333333333332,
        "step": 11377
    },
    {
        "loss": 2.595,
        "grad_norm": 6.7608489990234375,
        "learning_rate": 0.00016532882528753833,
        "epoch": 1.5170666666666666,
        "step": 11378
    },
    {
        "loss": 1.7123,
        "grad_norm": 3.6974852085113525,
        "learning_rate": 0.00016528116235663238,
        "epoch": 1.5171999999999999,
        "step": 11379
    },
    {
        "loss": 2.7765,
        "grad_norm": 4.806746482849121,
        "learning_rate": 0.0001652334735676358,
        "epoch": 1.5173333333333332,
        "step": 11380
    },
    {
        "loss": 2.2008,
        "grad_norm": 3.4509265422821045,
        "learning_rate": 0.0001651857589394378,
        "epoch": 1.5174666666666665,
        "step": 11381
    },
    {
        "loss": 2.3117,
        "grad_norm": 2.8441853523254395,
        "learning_rate": 0.00016513801849093874,
        "epoch": 1.5175999999999998,
        "step": 11382
    },
    {
        "loss": 3.1209,
        "grad_norm": 4.692587375640869,
        "learning_rate": 0.00016509025224104845,
        "epoch": 1.5177333333333334,
        "step": 11383
    },
    {
        "loss": 3.0984,
        "grad_norm": 3.983738422393799,
        "learning_rate": 0.0001650424602086876,
        "epoch": 1.5178666666666667,
        "step": 11384
    },
    {
        "loss": 0.9582,
        "grad_norm": 3.958430051803589,
        "learning_rate": 0.0001649946424127866,
        "epoch": 1.518,
        "step": 11385
    },
    {
        "loss": 2.4082,
        "grad_norm": 4.03693151473999,
        "learning_rate": 0.00016494679887228632,
        "epoch": 1.5181333333333333,
        "step": 11386
    },
    {
        "loss": 2.2602,
        "grad_norm": 3.564608335494995,
        "learning_rate": 0.00016489892960613758,
        "epoch": 1.5182666666666667,
        "step": 11387
    },
    {
        "loss": 2.3148,
        "grad_norm": 3.2357800006866455,
        "learning_rate": 0.00016485103463330177,
        "epoch": 1.5184,
        "step": 11388
    },
    {
        "loss": 2.3373,
        "grad_norm": 4.298898220062256,
        "learning_rate": 0.0001648031139727502,
        "epoch": 1.5185333333333333,
        "step": 11389
    },
    {
        "loss": 2.7489,
        "grad_norm": 3.5916285514831543,
        "learning_rate": 0.00016475516764346423,
        "epoch": 1.5186666666666668,
        "step": 11390
    },
    {
        "loss": 1.8372,
        "grad_norm": 3.9755144119262695,
        "learning_rate": 0.00016470719566443585,
        "epoch": 1.5188000000000001,
        "step": 11391
    },
    {
        "loss": 2.3167,
        "grad_norm": 4.008152961730957,
        "learning_rate": 0.00016465919805466677,
        "epoch": 1.5189333333333335,
        "step": 11392
    },
    {
        "loss": 2.6454,
        "grad_norm": 3.6870734691619873,
        "learning_rate": 0.0001646111748331689,
        "epoch": 1.5190666666666668,
        "step": 11393
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.05613112449646,
        "learning_rate": 0.00016456312601896466,
        "epoch": 1.5192,
        "step": 11394
    },
    {
        "loss": 2.0778,
        "grad_norm": 3.4231455326080322,
        "learning_rate": 0.00016451505163108619,
        "epoch": 1.5193333333333334,
        "step": 11395
    },
    {
        "loss": 2.3362,
        "grad_norm": 3.839540481567383,
        "learning_rate": 0.00016446695168857592,
        "epoch": 1.5194666666666667,
        "step": 11396
    },
    {
        "loss": 1.2246,
        "grad_norm": 6.060304641723633,
        "learning_rate": 0.00016441882621048634,
        "epoch": 1.5196,
        "step": 11397
    },
    {
        "loss": 2.073,
        "grad_norm": 4.689317226409912,
        "learning_rate": 0.00016437067521588033,
        "epoch": 1.5197333333333334,
        "step": 11398
    },
    {
        "loss": 1.9603,
        "grad_norm": 4.03435754776001,
        "learning_rate": 0.00016432249872383036,
        "epoch": 1.5198666666666667,
        "step": 11399
    },
    {
        "loss": 2.5904,
        "grad_norm": 2.941466808319092,
        "learning_rate": 0.00016427429675341961,
        "epoch": 1.52,
        "step": 11400
    },
    {
        "loss": 1.7712,
        "grad_norm": 4.659811019897461,
        "learning_rate": 0.0001642260693237409,
        "epoch": 1.5201333333333333,
        "step": 11401
    },
    {
        "loss": 1.6107,
        "grad_norm": 4.411175727844238,
        "learning_rate": 0.0001641778164538972,
        "epoch": 1.5202666666666667,
        "step": 11402
    },
    {
        "loss": 2.265,
        "grad_norm": 3.6853575706481934,
        "learning_rate": 0.0001641295381630018,
        "epoch": 1.5204,
        "step": 11403
    },
    {
        "loss": 1.1046,
        "grad_norm": 4.507388591766357,
        "learning_rate": 0.00016408123447017781,
        "epoch": 1.5205333333333333,
        "step": 11404
    },
    {
        "loss": 1.9736,
        "grad_norm": 4.333652973175049,
        "learning_rate": 0.00016403290539455852,
        "epoch": 1.5206666666666666,
        "step": 11405
    },
    {
        "loss": 1.6852,
        "grad_norm": 4.1144232749938965,
        "learning_rate": 0.00016398455095528714,
        "epoch": 1.5208,
        "step": 11406
    },
    {
        "loss": 2.3924,
        "grad_norm": 3.8396382331848145,
        "learning_rate": 0.00016393617117151717,
        "epoch": 1.5209333333333332,
        "step": 11407
    },
    {
        "loss": 1.3147,
        "grad_norm": 4.572247505187988,
        "learning_rate": 0.00016388776606241184,
        "epoch": 1.5210666666666666,
        "step": 11408
    },
    {
        "loss": 2.3041,
        "grad_norm": 5.819430828094482,
        "learning_rate": 0.0001638393356471449,
        "epoch": 1.5211999999999999,
        "step": 11409
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.1309568881988525,
        "learning_rate": 0.00016379087994489936,
        "epoch": 1.5213333333333332,
        "step": 11410
    },
    {
        "loss": 1.0268,
        "grad_norm": 3.800853967666626,
        "learning_rate": 0.0001637423989748689,
        "epoch": 1.5214666666666665,
        "step": 11411
    },
    {
        "loss": 0.9274,
        "grad_norm": 4.754404067993164,
        "learning_rate": 0.00016369389275625715,
        "epoch": 1.5215999999999998,
        "step": 11412
    },
    {
        "loss": 2.3236,
        "grad_norm": 3.59535551071167,
        "learning_rate": 0.00016364536130827743,
        "epoch": 1.5217333333333334,
        "step": 11413
    },
    {
        "loss": 2.1485,
        "grad_norm": 3.941720962524414,
        "learning_rate": 0.00016359680465015324,
        "epoch": 1.5218666666666667,
        "step": 11414
    },
    {
        "loss": 2.0917,
        "grad_norm": 3.8876986503601074,
        "learning_rate": 0.0001635482228011179,
        "epoch": 1.522,
        "step": 11415
    },
    {
        "loss": 2.476,
        "grad_norm": 2.4417712688446045,
        "learning_rate": 0.0001634996157804151,
        "epoch": 1.5221333333333333,
        "step": 11416
    },
    {
        "loss": 2.38,
        "grad_norm": 3.2111833095550537,
        "learning_rate": 0.0001634509836072981,
        "epoch": 1.5222666666666667,
        "step": 11417
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.433743715286255,
        "learning_rate": 0.0001634023263010303,
        "epoch": 1.5224,
        "step": 11418
    },
    {
        "loss": 2.3143,
        "grad_norm": 3.8459222316741943,
        "learning_rate": 0.00016335364388088492,
        "epoch": 1.5225333333333333,
        "step": 11419
    },
    {
        "loss": 1.7348,
        "grad_norm": 4.450077533721924,
        "learning_rate": 0.0001633049363661453,
        "epoch": 1.5226666666666666,
        "step": 11420
    },
    {
        "loss": 2.8097,
        "grad_norm": 3.446551561355591,
        "learning_rate": 0.00016325620377610482,
        "epoch": 1.5228000000000002,
        "step": 11421
    },
    {
        "loss": 2.4123,
        "grad_norm": 6.197421073913574,
        "learning_rate": 0.00016320744613006647,
        "epoch": 1.5229333333333335,
        "step": 11422
    },
    {
        "loss": 2.594,
        "grad_norm": 3.175424575805664,
        "learning_rate": 0.00016315866344734329,
        "epoch": 1.5230666666666668,
        "step": 11423
    },
    {
        "loss": 3.0753,
        "grad_norm": 5.874927997589111,
        "learning_rate": 0.00016310985574725825,
        "epoch": 1.5232,
        "step": 11424
    },
    {
        "loss": 2.8335,
        "grad_norm": 3.442138195037842,
        "learning_rate": 0.00016306102304914442,
        "epoch": 1.5233333333333334,
        "step": 11425
    },
    {
        "loss": 2.5177,
        "grad_norm": 4.786776542663574,
        "learning_rate": 0.00016301216537234448,
        "epoch": 1.5234666666666667,
        "step": 11426
    },
    {
        "loss": 1.4232,
        "grad_norm": 4.961121559143066,
        "learning_rate": 0.00016296328273621112,
        "epoch": 1.5236,
        "step": 11427
    },
    {
        "loss": 2.6915,
        "grad_norm": 3.278513193130493,
        "learning_rate": 0.00016291437516010682,
        "epoch": 1.5237333333333334,
        "step": 11428
    },
    {
        "loss": 2.4941,
        "grad_norm": 4.306525707244873,
        "learning_rate": 0.0001628654426634041,
        "epoch": 1.5238666666666667,
        "step": 11429
    },
    {
        "loss": 0.9232,
        "grad_norm": 2.897871255874634,
        "learning_rate": 0.0001628164852654856,
        "epoch": 1.524,
        "step": 11430
    },
    {
        "loss": 1.6478,
        "grad_norm": 4.515336036682129,
        "learning_rate": 0.00016276750298574303,
        "epoch": 1.5241333333333333,
        "step": 11431
    },
    {
        "loss": 1.9801,
        "grad_norm": 4.326893329620361,
        "learning_rate": 0.00016271849584357874,
        "epoch": 1.5242666666666667,
        "step": 11432
    },
    {
        "loss": 2.492,
        "grad_norm": 2.8911688327789307,
        "learning_rate": 0.00016266946385840446,
        "epoch": 1.5244,
        "step": 11433
    },
    {
        "loss": 2.9737,
        "grad_norm": 2.567390203475952,
        "learning_rate": 0.00016262040704964208,
        "epoch": 1.5245333333333333,
        "step": 11434
    },
    {
        "loss": 1.8666,
        "grad_norm": 5.111596584320068,
        "learning_rate": 0.0001625713254367231,
        "epoch": 1.5246666666666666,
        "step": 11435
    },
    {
        "loss": 0.9894,
        "grad_norm": 7.001142501831055,
        "learning_rate": 0.0001625222190390889,
        "epoch": 1.5248,
        "step": 11436
    },
    {
        "loss": 2.0927,
        "grad_norm": 3.5283169746398926,
        "learning_rate": 0.0001624730878761906,
        "epoch": 1.5249333333333333,
        "step": 11437
    },
    {
        "loss": 1.3244,
        "grad_norm": 4.597082614898682,
        "learning_rate": 0.00016242393196748932,
        "epoch": 1.5250666666666666,
        "step": 11438
    },
    {
        "loss": 2.1248,
        "grad_norm": 3.8603274822235107,
        "learning_rate": 0.0001623747513324561,
        "epoch": 1.5252,
        "step": 11439
    },
    {
        "loss": 2.2572,
        "grad_norm": 3.343071699142456,
        "learning_rate": 0.00016232554599057112,
        "epoch": 1.5253333333333332,
        "step": 11440
    },
    {
        "loss": 2.5218,
        "grad_norm": 4.048669338226318,
        "learning_rate": 0.00016227631596132515,
        "epoch": 1.5254666666666665,
        "step": 11441
    },
    {
        "loss": 2.4507,
        "grad_norm": 3.866024971008301,
        "learning_rate": 0.00016222706126421807,
        "epoch": 1.5255999999999998,
        "step": 11442
    },
    {
        "loss": 1.5697,
        "grad_norm": 2.8898725509643555,
        "learning_rate": 0.00016217778191876013,
        "epoch": 1.5257333333333334,
        "step": 11443
    },
    {
        "loss": 1.1927,
        "grad_norm": 4.568312168121338,
        "learning_rate": 0.00016212847794447088,
        "epoch": 1.5258666666666667,
        "step": 11444
    },
    {
        "loss": 0.8911,
        "grad_norm": 4.510185718536377,
        "learning_rate": 0.00016207914936087984,
        "epoch": 1.526,
        "step": 11445
    },
    {
        "loss": 1.5483,
        "grad_norm": 4.9542036056518555,
        "learning_rate": 0.00016202979618752607,
        "epoch": 1.5261333333333333,
        "step": 11446
    },
    {
        "loss": 1.6622,
        "grad_norm": 3.410789728164673,
        "learning_rate": 0.0001619804184439588,
        "epoch": 1.5262666666666667,
        "step": 11447
    },
    {
        "loss": 1.5268,
        "grad_norm": 3.9001119136810303,
        "learning_rate": 0.00016193101614973658,
        "epoch": 1.5264,
        "step": 11448
    },
    {
        "loss": 1.836,
        "grad_norm": 4.177052021026611,
        "learning_rate": 0.00016188158932442775,
        "epoch": 1.5265333333333333,
        "step": 11449
    },
    {
        "loss": 2.4868,
        "grad_norm": 2.9658687114715576,
        "learning_rate": 0.00016183213798761064,
        "epoch": 1.5266666666666666,
        "step": 11450
    },
    {
        "loss": 2.3787,
        "grad_norm": 3.5648443698883057,
        "learning_rate": 0.00016178266215887297,
        "epoch": 1.5268000000000002,
        "step": 11451
    },
    {
        "loss": 1.7278,
        "grad_norm": 4.2480621337890625,
        "learning_rate": 0.0001617331618578122,
        "epoch": 1.5269333333333335,
        "step": 11452
    },
    {
        "loss": 2.7727,
        "grad_norm": 3.2314000129699707,
        "learning_rate": 0.0001616836371040358,
        "epoch": 1.5270666666666668,
        "step": 11453
    },
    {
        "loss": 1.8603,
        "grad_norm": 4.336633205413818,
        "learning_rate": 0.0001616340879171605,
        "epoch": 1.5272000000000001,
        "step": 11454
    },
    {
        "loss": 2.388,
        "grad_norm": 3.1934194564819336,
        "learning_rate": 0.000161584514316813,
        "epoch": 1.5273333333333334,
        "step": 11455
    },
    {
        "loss": 2.0747,
        "grad_norm": 4.364620208740234,
        "learning_rate": 0.00016153491632262942,
        "epoch": 1.5274666666666668,
        "step": 11456
    },
    {
        "loss": 1.5139,
        "grad_norm": 4.972351551055908,
        "learning_rate": 0.00016148529395425593,
        "epoch": 1.5276,
        "step": 11457
    },
    {
        "loss": 1.4177,
        "grad_norm": 4.901704788208008,
        "learning_rate": 0.00016143564723134786,
        "epoch": 1.5277333333333334,
        "step": 11458
    },
    {
        "loss": 2.7083,
        "grad_norm": 3.1106982231140137,
        "learning_rate": 0.00016138597617357072,
        "epoch": 1.5278666666666667,
        "step": 11459
    },
    {
        "loss": 2.4938,
        "grad_norm": 3.688300371170044,
        "learning_rate": 0.00016133628080059923,
        "epoch": 1.528,
        "step": 11460
    },
    {
        "loss": 2.7272,
        "grad_norm": 5.257813930511475,
        "learning_rate": 0.00016128656113211784,
        "epoch": 1.5281333333333333,
        "step": 11461
    },
    {
        "loss": 2.1871,
        "grad_norm": 5.038576126098633,
        "learning_rate": 0.00016123681718782087,
        "epoch": 1.5282666666666667,
        "step": 11462
    },
    {
        "loss": 2.8365,
        "grad_norm": 2.101696491241455,
        "learning_rate": 0.00016118704898741192,
        "epoch": 1.5284,
        "step": 11463
    },
    {
        "loss": 1.7419,
        "grad_norm": 5.5024094581604,
        "learning_rate": 0.00016113725655060435,
        "epoch": 1.5285333333333333,
        "step": 11464
    },
    {
        "loss": 2.6306,
        "grad_norm": 4.347757816314697,
        "learning_rate": 0.0001610874398971211,
        "epoch": 1.5286666666666666,
        "step": 11465
    },
    {
        "loss": 1.9477,
        "grad_norm": 4.385439872741699,
        "learning_rate": 0.00016103759904669484,
        "epoch": 1.5288,
        "step": 11466
    },
    {
        "loss": 2.4306,
        "grad_norm": 3.139063835144043,
        "learning_rate": 0.00016098773401906747,
        "epoch": 1.5289333333333333,
        "step": 11467
    },
    {
        "loss": 1.9593,
        "grad_norm": 2.2902615070343018,
        "learning_rate": 0.00016093784483399112,
        "epoch": 1.5290666666666666,
        "step": 11468
    },
    {
        "loss": 1.8471,
        "grad_norm": 3.3027729988098145,
        "learning_rate": 0.00016088793151122655,
        "epoch": 1.5292,
        "step": 11469
    },
    {
        "loss": 1.5653,
        "grad_norm": 5.6425251960754395,
        "learning_rate": 0.0001608379940705448,
        "epoch": 1.5293333333333332,
        "step": 11470
    },
    {
        "loss": 2.3195,
        "grad_norm": 4.0922770500183105,
        "learning_rate": 0.00016078803253172645,
        "epoch": 1.5294666666666665,
        "step": 11471
    },
    {
        "loss": 2.3912,
        "grad_norm": 3.249089241027832,
        "learning_rate": 0.0001607380469145613,
        "epoch": 1.5295999999999998,
        "step": 11472
    },
    {
        "loss": 1.609,
        "grad_norm": 2.6451151371002197,
        "learning_rate": 0.00016068803723884878,
        "epoch": 1.5297333333333332,
        "step": 11473
    },
    {
        "loss": 2.8854,
        "grad_norm": 2.619978666305542,
        "learning_rate": 0.00016063800352439782,
        "epoch": 1.5298666666666667,
        "step": 11474
    },
    {
        "loss": 2.4987,
        "grad_norm": 3.1818463802337646,
        "learning_rate": 0.00016058794579102713,
        "epoch": 1.53,
        "step": 11475
    },
    {
        "loss": 2.6108,
        "grad_norm": 2.656090021133423,
        "learning_rate": 0.00016053786405856467,
        "epoch": 1.5301333333333333,
        "step": 11476
    },
    {
        "loss": 1.2461,
        "grad_norm": 6.026514530181885,
        "learning_rate": 0.00016048775834684803,
        "epoch": 1.5302666666666667,
        "step": 11477
    },
    {
        "loss": 0.7818,
        "grad_norm": 2.8566482067108154,
        "learning_rate": 0.00016043762867572404,
        "epoch": 1.5304,
        "step": 11478
    },
    {
        "loss": 2.4015,
        "grad_norm": 3.4383325576782227,
        "learning_rate": 0.0001603874750650494,
        "epoch": 1.5305333333333333,
        "step": 11479
    },
    {
        "loss": 1.6715,
        "grad_norm": 4.433507442474365,
        "learning_rate": 0.00016033729753469025,
        "epoch": 1.5306666666666666,
        "step": 11480
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.6287474632263184,
        "learning_rate": 0.00016028709610452192,
        "epoch": 1.5308000000000002,
        "step": 11481
    },
    {
        "loss": 2.0953,
        "grad_norm": 3.7087478637695312,
        "learning_rate": 0.00016023687079442945,
        "epoch": 1.5309333333333335,
        "step": 11482
    },
    {
        "loss": 2.8838,
        "grad_norm": 5.160012722015381,
        "learning_rate": 0.00016018662162430706,
        "epoch": 1.5310666666666668,
        "step": 11483
    },
    {
        "loss": 1.4712,
        "grad_norm": 6.3209404945373535,
        "learning_rate": 0.00016013634861405888,
        "epoch": 1.5312000000000001,
        "step": 11484
    },
    {
        "loss": 2.5926,
        "grad_norm": 8.88002872467041,
        "learning_rate": 0.00016008605178359812,
        "epoch": 1.5313333333333334,
        "step": 11485
    },
    {
        "loss": 1.3285,
        "grad_norm": 3.8151047229766846,
        "learning_rate": 0.0001600357311528475,
        "epoch": 1.5314666666666668,
        "step": 11486
    },
    {
        "loss": 1.9995,
        "grad_norm": 3.6601901054382324,
        "learning_rate": 0.00015998538674173905,
        "epoch": 1.5316,
        "step": 11487
    },
    {
        "loss": 2.3642,
        "grad_norm": 2.279874801635742,
        "learning_rate": 0.00015993501857021456,
        "epoch": 1.5317333333333334,
        "step": 11488
    },
    {
        "loss": 2.627,
        "grad_norm": 3.4993176460266113,
        "learning_rate": 0.00015988462665822523,
        "epoch": 1.5318666666666667,
        "step": 11489
    },
    {
        "loss": 2.5655,
        "grad_norm": 3.4509940147399902,
        "learning_rate": 0.00015983421102573096,
        "epoch": 1.532,
        "step": 11490
    },
    {
        "loss": 2.546,
        "grad_norm": 3.975393533706665,
        "learning_rate": 0.00015978377169270197,
        "epoch": 1.5321333333333333,
        "step": 11491
    },
    {
        "loss": 1.9234,
        "grad_norm": 4.458584308624268,
        "learning_rate": 0.0001597333086791172,
        "epoch": 1.5322666666666667,
        "step": 11492
    },
    {
        "loss": 2.1116,
        "grad_norm": 4.3829121589660645,
        "learning_rate": 0.00015968282200496542,
        "epoch": 1.5324,
        "step": 11493
    },
    {
        "loss": 1.1833,
        "grad_norm": 2.3451988697052,
        "learning_rate": 0.00015963231169024446,
        "epoch": 1.5325333333333333,
        "step": 11494
    },
    {
        "loss": 2.6955,
        "grad_norm": 2.6943061351776123,
        "learning_rate": 0.00015958177775496165,
        "epoch": 1.5326666666666666,
        "step": 11495
    },
    {
        "loss": 2.1738,
        "grad_norm": 4.432714462280273,
        "learning_rate": 0.0001595312202191336,
        "epoch": 1.5328,
        "step": 11496
    },
    {
        "loss": 1.7338,
        "grad_norm": 3.9853250980377197,
        "learning_rate": 0.0001594806391027864,
        "epoch": 1.5329333333333333,
        "step": 11497
    },
    {
        "loss": 1.9292,
        "grad_norm": 3.8691065311431885,
        "learning_rate": 0.00015943003442595542,
        "epoch": 1.5330666666666666,
        "step": 11498
    },
    {
        "loss": 2.6113,
        "grad_norm": 3.105520009994507,
        "learning_rate": 0.0001593794062086852,
        "epoch": 1.5332,
        "step": 11499
    },
    {
        "loss": 2.6394,
        "grad_norm": 3.8021700382232666,
        "learning_rate": 0.00015932875447102995,
        "epoch": 1.5333333333333332,
        "step": 11500
    },
    {
        "loss": 2.2491,
        "grad_norm": 3.186582326889038,
        "learning_rate": 0.0001592780792330528,
        "epoch": 1.5334666666666665,
        "step": 11501
    },
    {
        "loss": 2.405,
        "grad_norm": 3.530385971069336,
        "learning_rate": 0.00015922738051482656,
        "epoch": 1.5335999999999999,
        "step": 11502
    },
    {
        "loss": 2.4501,
        "grad_norm": 4.387951374053955,
        "learning_rate": 0.0001591766583364331,
        "epoch": 1.5337333333333332,
        "step": 11503
    },
    {
        "loss": 1.7186,
        "grad_norm": 3.9528443813323975,
        "learning_rate": 0.00015912591271796363,
        "epoch": 1.5338666666666667,
        "step": 11504
    },
    {
        "loss": 1.6051,
        "grad_norm": 3.546959638595581,
        "learning_rate": 0.00015907514367951856,
        "epoch": 1.534,
        "step": 11505
    },
    {
        "loss": 2.2269,
        "grad_norm": 4.251094818115234,
        "learning_rate": 0.00015902435124120793,
        "epoch": 1.5341333333333333,
        "step": 11506
    },
    {
        "loss": 1.9972,
        "grad_norm": 3.6994073390960693,
        "learning_rate": 0.00015897353542315058,
        "epoch": 1.5342666666666667,
        "step": 11507
    },
    {
        "loss": 1.0718,
        "grad_norm": 5.892661094665527,
        "learning_rate": 0.00015892269624547486,
        "epoch": 1.5344,
        "step": 11508
    },
    {
        "loss": 2.6253,
        "grad_norm": 4.092916011810303,
        "learning_rate": 0.00015887183372831844,
        "epoch": 1.5345333333333333,
        "step": 11509
    },
    {
        "loss": 1.9825,
        "grad_norm": 4.102014064788818,
        "learning_rate": 0.0001588209478918281,
        "epoch": 1.5346666666666666,
        "step": 11510
    },
    {
        "loss": 0.8758,
        "grad_norm": 4.707536697387695,
        "learning_rate": 0.0001587700387561598,
        "epoch": 1.5348000000000002,
        "step": 11511
    },
    {
        "loss": 2.15,
        "grad_norm": 4.164804458618164,
        "learning_rate": 0.00015871910634147893,
        "epoch": 1.5349333333333335,
        "step": 11512
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.720686912536621,
        "learning_rate": 0.00015866815066796,
        "epoch": 1.5350666666666668,
        "step": 11513
    },
    {
        "loss": 1.4689,
        "grad_norm": 3.5156044960021973,
        "learning_rate": 0.0001586171717557867,
        "epoch": 1.5352000000000001,
        "step": 11514
    },
    {
        "loss": 2.5175,
        "grad_norm": 4.646420955657959,
        "learning_rate": 0.00015856616962515177,
        "epoch": 1.5353333333333334,
        "step": 11515
    },
    {
        "loss": 1.9647,
        "grad_norm": 5.750626087188721,
        "learning_rate": 0.00015851514429625766,
        "epoch": 1.5354666666666668,
        "step": 11516
    },
    {
        "loss": 2.474,
        "grad_norm": 3.7278342247009277,
        "learning_rate": 0.00015846409578931537,
        "epoch": 1.5356,
        "step": 11517
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.9724838733673096,
        "learning_rate": 0.00015841302412454575,
        "epoch": 1.5357333333333334,
        "step": 11518
    },
    {
        "loss": 2.9395,
        "grad_norm": 2.1863505840301514,
        "learning_rate": 0.00015836192932217803,
        "epoch": 1.5358666666666667,
        "step": 11519
    },
    {
        "loss": 2.9804,
        "grad_norm": 4.2176008224487305,
        "learning_rate": 0.00015831081140245124,
        "epoch": 1.536,
        "step": 11520
    },
    {
        "loss": 2.0951,
        "grad_norm": 4.913727760314941,
        "learning_rate": 0.00015825967038561345,
        "epoch": 1.5361333333333334,
        "step": 11521
    },
    {
        "loss": 0.6877,
        "grad_norm": 3.764625072479248,
        "learning_rate": 0.00015820850629192173,
        "epoch": 1.5362666666666667,
        "step": 11522
    },
    {
        "loss": 1.9038,
        "grad_norm": 3.767256021499634,
        "learning_rate": 0.0001581573191416423,
        "epoch": 1.5364,
        "step": 11523
    },
    {
        "loss": 1.133,
        "grad_norm": 4.45007848739624,
        "learning_rate": 0.0001581061089550505,
        "epoch": 1.5365333333333333,
        "step": 11524
    },
    {
        "loss": 2.3311,
        "grad_norm": 3.717712879180908,
        "learning_rate": 0.00015805487575243108,
        "epoch": 1.5366666666666666,
        "step": 11525
    },
    {
        "loss": 2.9016,
        "grad_norm": 4.353333473205566,
        "learning_rate": 0.0001580036195540774,
        "epoch": 1.5368,
        "step": 11526
    },
    {
        "loss": 2.6755,
        "grad_norm": 2.871044158935547,
        "learning_rate": 0.00015795234038029263,
        "epoch": 1.5369333333333333,
        "step": 11527
    },
    {
        "loss": 1.4436,
        "grad_norm": 5.351517200469971,
        "learning_rate": 0.00015790103825138815,
        "epoch": 1.5370666666666666,
        "step": 11528
    },
    {
        "loss": 2.7685,
        "grad_norm": 4.639861106872559,
        "learning_rate": 0.00015784971318768512,
        "epoch": 1.5372,
        "step": 11529
    },
    {
        "loss": 2.4464,
        "grad_norm": 3.47735857963562,
        "learning_rate": 0.00015779836520951368,
        "epoch": 1.5373333333333332,
        "step": 11530
    },
    {
        "loss": 2.5422,
        "grad_norm": 4.9951982498168945,
        "learning_rate": 0.00015774699433721283,
        "epoch": 1.5374666666666665,
        "step": 11531
    },
    {
        "loss": 2.506,
        "grad_norm": 4.074204444885254,
        "learning_rate": 0.00015769560059113074,
        "epoch": 1.5375999999999999,
        "step": 11532
    },
    {
        "loss": 1.1176,
        "grad_norm": 5.100162506103516,
        "learning_rate": 0.00015764418399162459,
        "epoch": 1.5377333333333332,
        "step": 11533
    },
    {
        "loss": 1.6817,
        "grad_norm": 4.686145782470703,
        "learning_rate": 0.00015759274455906085,
        "epoch": 1.5378666666666667,
        "step": 11534
    },
    {
        "loss": 2.2243,
        "grad_norm": 3.711191177368164,
        "learning_rate": 0.00015754128231381474,
        "epoch": 1.538,
        "step": 11535
    },
    {
        "loss": 2.5358,
        "grad_norm": 3.702442169189453,
        "learning_rate": 0.00015748979727627063,
        "epoch": 1.5381333333333334,
        "step": 11536
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.5697784423828125,
        "learning_rate": 0.00015743828946682183,
        "epoch": 1.5382666666666667,
        "step": 11537
    },
    {
        "loss": 2.6912,
        "grad_norm": 5.041306495666504,
        "learning_rate": 0.00015738675890587084,
        "epoch": 1.5384,
        "step": 11538
    },
    {
        "loss": 2.2603,
        "grad_norm": 2.78420352935791,
        "learning_rate": 0.00015733520561382922,
        "epoch": 1.5385333333333333,
        "step": 11539
    },
    {
        "loss": 2.1276,
        "grad_norm": 3.750910758972168,
        "learning_rate": 0.00015728362961111732,
        "epoch": 1.5386666666666666,
        "step": 11540
    },
    {
        "loss": 2.236,
        "grad_norm": 4.606919765472412,
        "learning_rate": 0.00015723203091816453,
        "epoch": 1.5388,
        "step": 11541
    },
    {
        "loss": 2.5491,
        "grad_norm": 4.1947197914123535,
        "learning_rate": 0.00015718040955540915,
        "epoch": 1.5389333333333335,
        "step": 11542
    },
    {
        "loss": 1.2146,
        "grad_norm": 4.231545448303223,
        "learning_rate": 0.00015712876554329884,
        "epoch": 1.5390666666666668,
        "step": 11543
    },
    {
        "loss": 1.4827,
        "grad_norm": 5.456099510192871,
        "learning_rate": 0.0001570770989022898,
        "epoch": 1.5392000000000001,
        "step": 11544
    },
    {
        "loss": 2.2976,
        "grad_norm": 4.366363048553467,
        "learning_rate": 0.00015702540965284743,
        "epoch": 1.5393333333333334,
        "step": 11545
    },
    {
        "loss": 2.2506,
        "grad_norm": 4.143458843231201,
        "learning_rate": 0.00015697369781544587,
        "epoch": 1.5394666666666668,
        "step": 11546
    },
    {
        "loss": 2.4317,
        "grad_norm": 2.40067195892334,
        "learning_rate": 0.0001569219634105685,
        "epoch": 1.5396,
        "step": 11547
    },
    {
        "loss": 2.3589,
        "grad_norm": 5.085430145263672,
        "learning_rate": 0.00015687020645870761,
        "epoch": 1.5397333333333334,
        "step": 11548
    },
    {
        "loss": 2.6709,
        "grad_norm": 3.3063271045684814,
        "learning_rate": 0.000156818426980364,
        "epoch": 1.5398666666666667,
        "step": 11549
    },
    {
        "loss": 2.3462,
        "grad_norm": 3.1889007091522217,
        "learning_rate": 0.00015676662499604797,
        "epoch": 1.54,
        "step": 11550
    },
    {
        "loss": 0.9373,
        "grad_norm": 4.285135269165039,
        "learning_rate": 0.00015671480052627823,
        "epoch": 1.5401333333333334,
        "step": 11551
    },
    {
        "loss": 1.6137,
        "grad_norm": 4.195211887359619,
        "learning_rate": 0.00015666295359158285,
        "epoch": 1.5402666666666667,
        "step": 11552
    },
    {
        "loss": 2.0377,
        "grad_norm": 4.332216262817383,
        "learning_rate": 0.00015661108421249851,
        "epoch": 1.5404,
        "step": 11553
    },
    {
        "loss": 2.1542,
        "grad_norm": 2.8118138313293457,
        "learning_rate": 0.00015655919240957077,
        "epoch": 1.5405333333333333,
        "step": 11554
    },
    {
        "loss": 2.5905,
        "grad_norm": 3.2628819942474365,
        "learning_rate": 0.00015650727820335414,
        "epoch": 1.5406666666666666,
        "step": 11555
    },
    {
        "loss": 1.0992,
        "grad_norm": 5.828790187835693,
        "learning_rate": 0.00015645534161441214,
        "epoch": 1.5408,
        "step": 11556
    },
    {
        "loss": 2.4992,
        "grad_norm": 3.0093648433685303,
        "learning_rate": 0.00015640338266331697,
        "epoch": 1.5409333333333333,
        "step": 11557
    },
    {
        "loss": 1.6034,
        "grad_norm": 4.667745590209961,
        "learning_rate": 0.0001563514013706496,
        "epoch": 1.5410666666666666,
        "step": 11558
    },
    {
        "loss": 0.7519,
        "grad_norm": 3.7473185062408447,
        "learning_rate": 0.00015629939775700029,
        "epoch": 1.5412,
        "step": 11559
    },
    {
        "loss": 2.7338,
        "grad_norm": 3.9250638484954834,
        "learning_rate": 0.00015624737184296756,
        "epoch": 1.5413333333333332,
        "step": 11560
    },
    {
        "loss": 1.7137,
        "grad_norm": 4.087926864624023,
        "learning_rate": 0.0001561953236491593,
        "epoch": 1.5414666666666665,
        "step": 11561
    },
    {
        "loss": 1.9826,
        "grad_norm": 5.196312427520752,
        "learning_rate": 0.00015614325319619188,
        "epoch": 1.5415999999999999,
        "step": 11562
    },
    {
        "loss": 1.9227,
        "grad_norm": 3.278218984603882,
        "learning_rate": 0.00015609116050469056,
        "epoch": 1.5417333333333332,
        "step": 11563
    },
    {
        "loss": 2.8326,
        "grad_norm": 3.40144419670105,
        "learning_rate": 0.00015603904559528928,
        "epoch": 1.5418666666666667,
        "step": 11564
    },
    {
        "loss": 1.8891,
        "grad_norm": 4.653090953826904,
        "learning_rate": 0.00015598690848863124,
        "epoch": 1.542,
        "step": 11565
    },
    {
        "loss": 2.9674,
        "grad_norm": 5.759960651397705,
        "learning_rate": 0.00015593474920536798,
        "epoch": 1.5421333333333334,
        "step": 11566
    },
    {
        "loss": 2.4267,
        "grad_norm": 3.5908215045928955,
        "learning_rate": 0.00015588256776615986,
        "epoch": 1.5422666666666667,
        "step": 11567
    },
    {
        "loss": 1.8786,
        "grad_norm": 5.628233432769775,
        "learning_rate": 0.00015583036419167633,
        "epoch": 1.5424,
        "step": 11568
    },
    {
        "loss": 2.2035,
        "grad_norm": 3.772193431854248,
        "learning_rate": 0.0001557781385025953,
        "epoch": 1.5425333333333333,
        "step": 11569
    },
    {
        "loss": 2.4815,
        "grad_norm": 3.836484432220459,
        "learning_rate": 0.00015572589071960344,
        "epoch": 1.5426666666666666,
        "step": 11570
    },
    {
        "loss": 2.5559,
        "grad_norm": 3.7890141010284424,
        "learning_rate": 0.00015567362086339653,
        "epoch": 1.5428,
        "step": 11571
    },
    {
        "loss": 2.0777,
        "grad_norm": 3.331712484359741,
        "learning_rate": 0.00015562132895467868,
        "epoch": 1.5429333333333335,
        "step": 11572
    },
    {
        "loss": 2.3479,
        "grad_norm": 3.2047548294067383,
        "learning_rate": 0.0001555690150141629,
        "epoch": 1.5430666666666668,
        "step": 11573
    },
    {
        "loss": 1.9254,
        "grad_norm": 3.395932674407959,
        "learning_rate": 0.0001555166790625708,
        "epoch": 1.5432000000000001,
        "step": 11574
    },
    {
        "loss": 2.0707,
        "grad_norm": 2.742797374725342,
        "learning_rate": 0.00015546432112063308,
        "epoch": 1.5433333333333334,
        "step": 11575
    },
    {
        "loss": 0.7772,
        "grad_norm": 3.9443185329437256,
        "learning_rate": 0.00015541194120908863,
        "epoch": 1.5434666666666668,
        "step": 11576
    },
    {
        "loss": 2.0224,
        "grad_norm": 3.118769407272339,
        "learning_rate": 0.0001553595393486857,
        "epoch": 1.5436,
        "step": 11577
    },
    {
        "loss": 2.1007,
        "grad_norm": 5.630537033081055,
        "learning_rate": 0.0001553071155601804,
        "epoch": 1.5437333333333334,
        "step": 11578
    },
    {
        "loss": 2.5668,
        "grad_norm": 5.879509925842285,
        "learning_rate": 0.00015525466986433814,
        "epoch": 1.5438666666666667,
        "step": 11579
    },
    {
        "loss": 3.1461,
        "grad_norm": 4.6556806564331055,
        "learning_rate": 0.000155202202281933,
        "epoch": 1.544,
        "step": 11580
    },
    {
        "loss": 2.4489,
        "grad_norm": 2.5721750259399414,
        "learning_rate": 0.00015514971283374745,
        "epoch": 1.5441333333333334,
        "step": 11581
    },
    {
        "loss": 1.0612,
        "grad_norm": 3.264472723007202,
        "learning_rate": 0.00015509720154057273,
        "epoch": 1.5442666666666667,
        "step": 11582
    },
    {
        "loss": 1.5822,
        "grad_norm": 2.715963840484619,
        "learning_rate": 0.00015504466842320856,
        "epoch": 1.5444,
        "step": 11583
    },
    {
        "loss": 2.576,
        "grad_norm": 5.025485992431641,
        "learning_rate": 0.00015499211350246387,
        "epoch": 1.5445333333333333,
        "step": 11584
    },
    {
        "loss": 1.4671,
        "grad_norm": 3.414250135421753,
        "learning_rate": 0.00015493953679915546,
        "epoch": 1.5446666666666666,
        "step": 11585
    },
    {
        "loss": 2.2794,
        "grad_norm": 3.865941047668457,
        "learning_rate": 0.0001548869383341096,
        "epoch": 1.5448,
        "step": 11586
    },
    {
        "loss": 1.9671,
        "grad_norm": 4.41373872756958,
        "learning_rate": 0.0001548343181281602,
        "epoch": 1.5449333333333333,
        "step": 11587
    },
    {
        "loss": 2.3314,
        "grad_norm": 3.3138437271118164,
        "learning_rate": 0.00015478167620215053,
        "epoch": 1.5450666666666666,
        "step": 11588
    },
    {
        "loss": 2.0858,
        "grad_norm": 4.715883731842041,
        "learning_rate": 0.0001547290125769324,
        "epoch": 1.5452,
        "step": 11589
    },
    {
        "loss": 1.8372,
        "grad_norm": 4.394828796386719,
        "learning_rate": 0.00015467632727336587,
        "epoch": 1.5453333333333332,
        "step": 11590
    },
    {
        "loss": 1.42,
        "grad_norm": 4.310125827789307,
        "learning_rate": 0.00015462362031231982,
        "epoch": 1.5454666666666665,
        "step": 11591
    },
    {
        "loss": 2.0874,
        "grad_norm": 3.907055139541626,
        "learning_rate": 0.0001545708917146715,
        "epoch": 1.5455999999999999,
        "step": 11592
    },
    {
        "loss": 2.7718,
        "grad_norm": 3.703420877456665,
        "learning_rate": 0.0001545181415013072,
        "epoch": 1.5457333333333332,
        "step": 11593
    },
    {
        "loss": 2.1626,
        "grad_norm": 3.561366319656372,
        "learning_rate": 0.00015446536969312123,
        "epoch": 1.5458666666666665,
        "step": 11594
    },
    {
        "loss": 2.6478,
        "grad_norm": 3.087972402572632,
        "learning_rate": 0.00015441257631101674,
        "epoch": 1.546,
        "step": 11595
    },
    {
        "loss": 1.5943,
        "grad_norm": 4.7115864753723145,
        "learning_rate": 0.00015435976137590527,
        "epoch": 1.5461333333333334,
        "step": 11596
    },
    {
        "loss": 1.7594,
        "grad_norm": 4.647741794586182,
        "learning_rate": 0.0001543069249087071,
        "epoch": 1.5462666666666667,
        "step": 11597
    },
    {
        "loss": 1.7549,
        "grad_norm": 2.9705262184143066,
        "learning_rate": 0.00015425406693035114,
        "epoch": 1.5464,
        "step": 11598
    },
    {
        "loss": 1.5033,
        "grad_norm": 4.107821941375732,
        "learning_rate": 0.00015420118746177418,
        "epoch": 1.5465333333333333,
        "step": 11599
    },
    {
        "loss": 2.1146,
        "grad_norm": 4.292457580566406,
        "learning_rate": 0.0001541482865239223,
        "epoch": 1.5466666666666666,
        "step": 11600
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.942321538925171,
        "learning_rate": 0.00015409536413774948,
        "epoch": 1.5468,
        "step": 11601
    },
    {
        "loss": 2.0051,
        "grad_norm": 4.265758514404297,
        "learning_rate": 0.00015404242032421875,
        "epoch": 1.5469333333333335,
        "step": 11602
    },
    {
        "loss": 1.7079,
        "grad_norm": 3.493704319000244,
        "learning_rate": 0.00015398945510430116,
        "epoch": 1.5470666666666668,
        "step": 11603
    },
    {
        "loss": 1.9443,
        "grad_norm": 3.6180291175842285,
        "learning_rate": 0.00015393646849897646,
        "epoch": 1.5472000000000001,
        "step": 11604
    },
    {
        "loss": 2.6279,
        "grad_norm": 3.303093194961548,
        "learning_rate": 0.00015388346052923267,
        "epoch": 1.5473333333333334,
        "step": 11605
    },
    {
        "loss": 2.3019,
        "grad_norm": 2.957853317260742,
        "learning_rate": 0.00015383043121606658,
        "epoch": 1.5474666666666668,
        "step": 11606
    },
    {
        "loss": 1.4314,
        "grad_norm": 4.080469131469727,
        "learning_rate": 0.00015377738058048346,
        "epoch": 1.5476,
        "step": 11607
    },
    {
        "loss": 1.5175,
        "grad_norm": 4.491522789001465,
        "learning_rate": 0.00015372430864349647,
        "epoch": 1.5477333333333334,
        "step": 11608
    },
    {
        "loss": 2.1557,
        "grad_norm": 3.547053575515747,
        "learning_rate": 0.0001536712154261278,
        "epoch": 1.5478666666666667,
        "step": 11609
    },
    {
        "loss": 1.5236,
        "grad_norm": 6.641111850738525,
        "learning_rate": 0.00015361810094940777,
        "epoch": 1.548,
        "step": 11610
    },
    {
        "loss": 1.7558,
        "grad_norm": 2.487959146499634,
        "learning_rate": 0.0001535649652343754,
        "epoch": 1.5481333333333334,
        "step": 11611
    },
    {
        "loss": 1.6478,
        "grad_norm": 4.611566543579102,
        "learning_rate": 0.00015351180830207772,
        "epoch": 1.5482666666666667,
        "step": 11612
    },
    {
        "loss": 3.0488,
        "grad_norm": 4.529089450836182,
        "learning_rate": 0.00015345863017357045,
        "epoch": 1.5484,
        "step": 11613
    },
    {
        "loss": 2.0205,
        "grad_norm": 4.097677230834961,
        "learning_rate": 0.0001534054308699175,
        "epoch": 1.5485333333333333,
        "step": 11614
    },
    {
        "loss": 2.4291,
        "grad_norm": 4.063856601715088,
        "learning_rate": 0.00015335221041219151,
        "epoch": 1.5486666666666666,
        "step": 11615
    },
    {
        "loss": 2.2198,
        "grad_norm": 5.5732035636901855,
        "learning_rate": 0.00015329896882147322,
        "epoch": 1.5488,
        "step": 11616
    },
    {
        "loss": 2.2919,
        "grad_norm": 2.996229648590088,
        "learning_rate": 0.0001532457061188516,
        "epoch": 1.5489333333333333,
        "step": 11617
    },
    {
        "loss": 1.4128,
        "grad_norm": 4.0785040855407715,
        "learning_rate": 0.0001531924223254245,
        "epoch": 1.5490666666666666,
        "step": 11618
    },
    {
        "loss": 2.2647,
        "grad_norm": 3.738643169403076,
        "learning_rate": 0.00015313911746229754,
        "epoch": 1.5492,
        "step": 11619
    },
    {
        "loss": 2.2026,
        "grad_norm": 3.890160322189331,
        "learning_rate": 0.0001530857915505852,
        "epoch": 1.5493333333333332,
        "step": 11620
    },
    {
        "loss": 2.3202,
        "grad_norm": 3.8598575592041016,
        "learning_rate": 0.00015303244461140997,
        "epoch": 1.5494666666666665,
        "step": 11621
    },
    {
        "loss": 2.2939,
        "grad_norm": 4.13703727722168,
        "learning_rate": 0.0001529790766659027,
        "epoch": 1.5495999999999999,
        "step": 11622
    },
    {
        "loss": 2.1872,
        "grad_norm": 3.986267328262329,
        "learning_rate": 0.00015292568773520254,
        "epoch": 1.5497333333333332,
        "step": 11623
    },
    {
        "loss": 2.3428,
        "grad_norm": 3.2506299018859863,
        "learning_rate": 0.00015287227784045726,
        "epoch": 1.5498666666666665,
        "step": 11624
    },
    {
        "loss": 2.4576,
        "grad_norm": 3.5292563438415527,
        "learning_rate": 0.00015281884700282255,
        "epoch": 1.55,
        "step": 11625
    },
    {
        "loss": 1.938,
        "grad_norm": 3.1785669326782227,
        "learning_rate": 0.00015276539524346246,
        "epoch": 1.5501333333333334,
        "step": 11626
    },
    {
        "loss": 0.626,
        "grad_norm": 3.9013898372650146,
        "learning_rate": 0.0001527119225835496,
        "epoch": 1.5502666666666667,
        "step": 11627
    },
    {
        "loss": 1.4299,
        "grad_norm": 5.310366630554199,
        "learning_rate": 0.00015265842904426465,
        "epoch": 1.5504,
        "step": 11628
    },
    {
        "loss": 2.3206,
        "grad_norm": 3.00932240486145,
        "learning_rate": 0.00015260491464679635,
        "epoch": 1.5505333333333333,
        "step": 11629
    },
    {
        "loss": 2.7849,
        "grad_norm": 3.487069845199585,
        "learning_rate": 0.00015255137941234227,
        "epoch": 1.5506666666666666,
        "step": 11630
    },
    {
        "loss": 1.615,
        "grad_norm": 5.3413920402526855,
        "learning_rate": 0.00015249782336210771,
        "epoch": 1.5508,
        "step": 11631
    },
    {
        "loss": 2.2746,
        "grad_norm": 3.8669846057891846,
        "learning_rate": 0.00015244424651730647,
        "epoch": 1.5509333333333335,
        "step": 11632
    },
    {
        "loss": 2.2325,
        "grad_norm": 3.2917470932006836,
        "learning_rate": 0.00015239064889916037,
        "epoch": 1.5510666666666668,
        "step": 11633
    },
    {
        "loss": 1.7084,
        "grad_norm": 4.8637566566467285,
        "learning_rate": 0.0001523370305288998,
        "epoch": 1.5512000000000001,
        "step": 11634
    },
    {
        "loss": 2.4528,
        "grad_norm": 5.179757595062256,
        "learning_rate": 0.00015228339142776302,
        "epoch": 1.5513333333333335,
        "step": 11635
    },
    {
        "loss": 2.0091,
        "grad_norm": 6.174899578094482,
        "learning_rate": 0.00015222973161699697,
        "epoch": 1.5514666666666668,
        "step": 11636
    },
    {
        "loss": 2.4093,
        "grad_norm": 3.726376533508301,
        "learning_rate": 0.00015217605111785603,
        "epoch": 1.5516,
        "step": 11637
    },
    {
        "loss": 2.3302,
        "grad_norm": 4.7220354080200195,
        "learning_rate": 0.00015212234995160343,
        "epoch": 1.5517333333333334,
        "step": 11638
    },
    {
        "loss": 1.7962,
        "grad_norm": 2.507929563522339,
        "learning_rate": 0.00015206862813951056,
        "epoch": 1.5518666666666667,
        "step": 11639
    },
    {
        "loss": 1.9283,
        "grad_norm": 3.6346001625061035,
        "learning_rate": 0.00015201488570285664,
        "epoch": 1.552,
        "step": 11640
    },
    {
        "loss": 1.2815,
        "grad_norm": 3.8050572872161865,
        "learning_rate": 0.00015196112266292928,
        "epoch": 1.5521333333333334,
        "step": 11641
    },
    {
        "loss": 2.3496,
        "grad_norm": 4.391203880310059,
        "learning_rate": 0.00015190733904102402,
        "epoch": 1.5522666666666667,
        "step": 11642
    },
    {
        "loss": 2.0153,
        "grad_norm": 4.842171669006348,
        "learning_rate": 0.00015185353485844507,
        "epoch": 1.5524,
        "step": 11643
    },
    {
        "loss": 1.6823,
        "grad_norm": 5.674897193908691,
        "learning_rate": 0.0001517997101365041,
        "epoch": 1.5525333333333333,
        "step": 11644
    },
    {
        "loss": 1.5902,
        "grad_norm": 4.417081356048584,
        "learning_rate": 0.00015174586489652173,
        "epoch": 1.5526666666666666,
        "step": 11645
    },
    {
        "loss": 2.6676,
        "grad_norm": 4.319263458251953,
        "learning_rate": 0.0001516919991598257,
        "epoch": 1.5528,
        "step": 11646
    },
    {
        "loss": 1.7255,
        "grad_norm": 3.914900541305542,
        "learning_rate": 0.00015163811294775266,
        "epoch": 1.5529333333333333,
        "step": 11647
    },
    {
        "loss": 1.4297,
        "grad_norm": 3.3549935817718506,
        "learning_rate": 0.00015158420628164724,
        "epoch": 1.5530666666666666,
        "step": 11648
    },
    {
        "loss": 2.8941,
        "grad_norm": 3.4624271392822266,
        "learning_rate": 0.00015153027918286198,
        "epoch": 1.5532,
        "step": 11649
    },
    {
        "loss": 2.4551,
        "grad_norm": 4.028698921203613,
        "learning_rate": 0.00015147633167275753,
        "epoch": 1.5533333333333332,
        "step": 11650
    },
    {
        "loss": 2.5981,
        "grad_norm": 3.0314419269561768,
        "learning_rate": 0.00015142236377270257,
        "epoch": 1.5534666666666666,
        "step": 11651
    },
    {
        "loss": 1.6572,
        "grad_norm": 2.1931164264678955,
        "learning_rate": 0.0001513683755040742,
        "epoch": 1.5535999999999999,
        "step": 11652
    },
    {
        "loss": 0.6539,
        "grad_norm": 4.122476100921631,
        "learning_rate": 0.00015131436688825732,
        "epoch": 1.5537333333333332,
        "step": 11653
    },
    {
        "loss": 2.1368,
        "grad_norm": 4.504002571105957,
        "learning_rate": 0.00015126033794664483,
        "epoch": 1.5538666666666665,
        "step": 11654
    },
    {
        "loss": 2.3936,
        "grad_norm": 4.037194728851318,
        "learning_rate": 0.0001512062887006377,
        "epoch": 1.554,
        "step": 11655
    },
    {
        "loss": 2.6413,
        "grad_norm": 4.600807189941406,
        "learning_rate": 0.00015115221917164515,
        "epoch": 1.5541333333333334,
        "step": 11656
    },
    {
        "loss": 2.5449,
        "grad_norm": 2.306865930557251,
        "learning_rate": 0.0001510981293810845,
        "epoch": 1.5542666666666667,
        "step": 11657
    },
    {
        "loss": 1.9315,
        "grad_norm": 4.659282207489014,
        "learning_rate": 0.00015104401935038046,
        "epoch": 1.5544,
        "step": 11658
    },
    {
        "loss": 2.4653,
        "grad_norm": 6.338620185852051,
        "learning_rate": 0.00015098988910096657,
        "epoch": 1.5545333333333333,
        "step": 11659
    },
    {
        "loss": 1.8535,
        "grad_norm": 4.155518054962158,
        "learning_rate": 0.0001509357386542837,
        "epoch": 1.5546666666666666,
        "step": 11660
    },
    {
        "loss": 2.7169,
        "grad_norm": 3.507624387741089,
        "learning_rate": 0.00015088156803178134,
        "epoch": 1.5548,
        "step": 11661
    },
    {
        "loss": 2.7239,
        "grad_norm": 3.6951375007629395,
        "learning_rate": 0.0001508273772549165,
        "epoch": 1.5549333333333333,
        "step": 11662
    },
    {
        "loss": 2.9613,
        "grad_norm": 3.6140694618225098,
        "learning_rate": 0.00015077316634515435,
        "epoch": 1.5550666666666668,
        "step": 11663
    },
    {
        "loss": 1.5355,
        "grad_norm": 4.223082065582275,
        "learning_rate": 0.00015071893532396787,
        "epoch": 1.5552000000000001,
        "step": 11664
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.782442092895508,
        "learning_rate": 0.0001506646842128383,
        "epoch": 1.5553333333333335,
        "step": 11665
    },
    {
        "loss": 3.1159,
        "grad_norm": 3.641165018081665,
        "learning_rate": 0.00015061041303325492,
        "epoch": 1.5554666666666668,
        "step": 11666
    },
    {
        "loss": 2.0757,
        "grad_norm": 2.8992044925689697,
        "learning_rate": 0.00015055612180671422,
        "epoch": 1.5556,
        "step": 11667
    },
    {
        "loss": 1.816,
        "grad_norm": 3.7971267700195312,
        "learning_rate": 0.00015050181055472156,
        "epoch": 1.5557333333333334,
        "step": 11668
    },
    {
        "loss": 1.8922,
        "grad_norm": 2.9821624755859375,
        "learning_rate": 0.00015044747929878952,
        "epoch": 1.5558666666666667,
        "step": 11669
    },
    {
        "loss": 2.7808,
        "grad_norm": 2.898667335510254,
        "learning_rate": 0.00015039312806043915,
        "epoch": 1.556,
        "step": 11670
    },
    {
        "loss": 2.8989,
        "grad_norm": 2.7364578247070312,
        "learning_rate": 0.00015033875686119904,
        "epoch": 1.5561333333333334,
        "step": 11671
    },
    {
        "loss": 2.0248,
        "grad_norm": 3.4420807361602783,
        "learning_rate": 0.00015028436572260586,
        "epoch": 1.5562666666666667,
        "step": 11672
    },
    {
        "loss": 2.4521,
        "grad_norm": 3.2628469467163086,
        "learning_rate": 0.0001502299546662039,
        "epoch": 1.5564,
        "step": 11673
    },
    {
        "loss": 1.7586,
        "grad_norm": 4.478895664215088,
        "learning_rate": 0.00015017552371354587,
        "epoch": 1.5565333333333333,
        "step": 11674
    },
    {
        "loss": 0.5936,
        "grad_norm": 3.346449375152588,
        "learning_rate": 0.00015012107288619195,
        "epoch": 1.5566666666666666,
        "step": 11675
    },
    {
        "loss": 1.823,
        "grad_norm": 5.06941032409668,
        "learning_rate": 0.00015006660220571015,
        "epoch": 1.5568,
        "step": 11676
    },
    {
        "loss": 2.2499,
        "grad_norm": 3.7631173133850098,
        "learning_rate": 0.00015001211169367675,
        "epoch": 1.5569333333333333,
        "step": 11677
    },
    {
        "loss": 1.7788,
        "grad_norm": 4.487437725067139,
        "learning_rate": 0.00014995760137167548,
        "epoch": 1.5570666666666666,
        "step": 11678
    },
    {
        "loss": 2.0189,
        "grad_norm": 3.4246585369110107,
        "learning_rate": 0.00014990307126129797,
        "epoch": 1.5572,
        "step": 11679
    },
    {
        "loss": 1.0903,
        "grad_norm": 3.999149799346924,
        "learning_rate": 0.000149848521384144,
        "epoch": 1.5573333333333332,
        "step": 11680
    },
    {
        "loss": 1.9112,
        "grad_norm": 5.602981090545654,
        "learning_rate": 0.00014979395176182084,
        "epoch": 1.5574666666666666,
        "step": 11681
    },
    {
        "loss": 2.3766,
        "grad_norm": 4.805748462677002,
        "learning_rate": 0.00014973936241594363,
        "epoch": 1.5575999999999999,
        "step": 11682
    },
    {
        "loss": 0.9134,
        "grad_norm": 3.9798104763031006,
        "learning_rate": 0.00014968475336813561,
        "epoch": 1.5577333333333332,
        "step": 11683
    },
    {
        "loss": 1.7063,
        "grad_norm": 3.962963342666626,
        "learning_rate": 0.00014963012464002748,
        "epoch": 1.5578666666666665,
        "step": 11684
    },
    {
        "loss": 2.2298,
        "grad_norm": 3.841796875,
        "learning_rate": 0.00014957547625325771,
        "epoch": 1.558,
        "step": 11685
    },
    {
        "loss": 1.913,
        "grad_norm": 4.083873748779297,
        "learning_rate": 0.00014952080822947303,
        "epoch": 1.5581333333333334,
        "step": 11686
    },
    {
        "loss": 2.0071,
        "grad_norm": 3.7254040241241455,
        "learning_rate": 0.00014946612059032747,
        "epoch": 1.5582666666666667,
        "step": 11687
    },
    {
        "loss": 1.553,
        "grad_norm": 4.913145542144775,
        "learning_rate": 0.00014941141335748286,
        "epoch": 1.5584,
        "step": 11688
    },
    {
        "loss": 2.7734,
        "grad_norm": 4.457531929016113,
        "learning_rate": 0.00014935668655260918,
        "epoch": 1.5585333333333333,
        "step": 11689
    },
    {
        "loss": 2.4766,
        "grad_norm": 2.72782564163208,
        "learning_rate": 0.00014930194019738377,
        "epoch": 1.5586666666666666,
        "step": 11690
    },
    {
        "loss": 2.4516,
        "grad_norm": 3.9570086002349854,
        "learning_rate": 0.00014924717431349187,
        "epoch": 1.5588,
        "step": 11691
    },
    {
        "loss": 1.2621,
        "grad_norm": 4.6355133056640625,
        "learning_rate": 0.00014919238892262629,
        "epoch": 1.5589333333333333,
        "step": 11692
    },
    {
        "loss": 1.5178,
        "grad_norm": 2.750652313232422,
        "learning_rate": 0.00014913758404648796,
        "epoch": 1.5590666666666668,
        "step": 11693
    },
    {
        "loss": 1.2521,
        "grad_norm": 3.9270684719085693,
        "learning_rate": 0.00014908275970678504,
        "epoch": 1.5592000000000001,
        "step": 11694
    },
    {
        "loss": 2.5137,
        "grad_norm": 3.9389805793762207,
        "learning_rate": 0.000149027915925234,
        "epoch": 1.5593333333333335,
        "step": 11695
    },
    {
        "loss": 2.2149,
        "grad_norm": 2.9602882862091064,
        "learning_rate": 0.00014897305272355813,
        "epoch": 1.5594666666666668,
        "step": 11696
    },
    {
        "loss": 2.9334,
        "grad_norm": 4.185690879821777,
        "learning_rate": 0.00014891817012348924,
        "epoch": 1.5596,
        "step": 11697
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.7879798412323,
        "learning_rate": 0.0001488632681467666,
        "epoch": 1.5597333333333334,
        "step": 11698
    },
    {
        "loss": 2.4271,
        "grad_norm": 5.268496990203857,
        "learning_rate": 0.00014880834681513694,
        "epoch": 1.5598666666666667,
        "step": 11699
    },
    {
        "loss": 1.3079,
        "grad_norm": 2.732671022415161,
        "learning_rate": 0.00014875340615035478,
        "epoch": 1.56,
        "step": 11700
    },
    {
        "loss": 0.745,
        "grad_norm": 3.076690912246704,
        "learning_rate": 0.00014869844617418221,
        "epoch": 1.5601333333333334,
        "step": 11701
    },
    {
        "loss": 2.1496,
        "grad_norm": 3.0961549282073975,
        "learning_rate": 0.00014864346690838932,
        "epoch": 1.5602666666666667,
        "step": 11702
    },
    {
        "loss": 2.6967,
        "grad_norm": 3.1620140075683594,
        "learning_rate": 0.0001485884683747533,
        "epoch": 1.5604,
        "step": 11703
    },
    {
        "loss": 1.4937,
        "grad_norm": 4.047466278076172,
        "learning_rate": 0.00014853345059505962,
        "epoch": 1.5605333333333333,
        "step": 11704
    },
    {
        "loss": 1.6299,
        "grad_norm": 5.075880527496338,
        "learning_rate": 0.00014847841359110056,
        "epoch": 1.5606666666666666,
        "step": 11705
    },
    {
        "loss": 1.3736,
        "grad_norm": 4.46140193939209,
        "learning_rate": 0.0001484233573846767,
        "epoch": 1.5608,
        "step": 11706
    },
    {
        "loss": 1.8785,
        "grad_norm": 5.4628448486328125,
        "learning_rate": 0.0001483682819975961,
        "epoch": 1.5609333333333333,
        "step": 11707
    },
    {
        "loss": 2.772,
        "grad_norm": 3.8418211936950684,
        "learning_rate": 0.00014831318745167418,
        "epoch": 1.5610666666666666,
        "step": 11708
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.5381181240081787,
        "learning_rate": 0.00014825807376873408,
        "epoch": 1.5612,
        "step": 11709
    },
    {
        "loss": 1.428,
        "grad_norm": 4.8435468673706055,
        "learning_rate": 0.00014820294097060642,
        "epoch": 1.5613333333333332,
        "step": 11710
    },
    {
        "loss": 1.9832,
        "grad_norm": 5.212118148803711,
        "learning_rate": 0.0001481477890791297,
        "epoch": 1.5614666666666666,
        "step": 11711
    },
    {
        "loss": 2.5841,
        "grad_norm": 3.429568290710449,
        "learning_rate": 0.00014809261811614969,
        "epoch": 1.5615999999999999,
        "step": 11712
    },
    {
        "loss": 2.5817,
        "grad_norm": 4.672146797180176,
        "learning_rate": 0.00014803742810351974,
        "epoch": 1.5617333333333332,
        "step": 11713
    },
    {
        "loss": 2.7116,
        "grad_norm": 4.893181800842285,
        "learning_rate": 0.00014798221906310073,
        "epoch": 1.5618666666666665,
        "step": 11714
    },
    {
        "loss": 1.3775,
        "grad_norm": 4.247783660888672,
        "learning_rate": 0.00014792699101676127,
        "epoch": 1.562,
        "step": 11715
    },
    {
        "loss": 2.2006,
        "grad_norm": 4.194294452667236,
        "learning_rate": 0.00014787174398637757,
        "epoch": 1.5621333333333334,
        "step": 11716
    },
    {
        "loss": 1.8111,
        "grad_norm": 2.7262070178985596,
        "learning_rate": 0.00014781647799383274,
        "epoch": 1.5622666666666667,
        "step": 11717
    },
    {
        "loss": 2.7833,
        "grad_norm": 3.931954860687256,
        "learning_rate": 0.00014776119306101818,
        "epoch": 1.5624,
        "step": 11718
    },
    {
        "loss": 2.4309,
        "grad_norm": 5.574167251586914,
        "learning_rate": 0.00014770588920983216,
        "epoch": 1.5625333333333333,
        "step": 11719
    },
    {
        "loss": 1.07,
        "grad_norm": 4.271238803863525,
        "learning_rate": 0.000147650566462181,
        "epoch": 1.5626666666666666,
        "step": 11720
    },
    {
        "loss": 1.2853,
        "grad_norm": 3.955063581466675,
        "learning_rate": 0.00014759522483997808,
        "epoch": 1.5628,
        "step": 11721
    },
    {
        "loss": 1.5207,
        "grad_norm": 8.008785247802734,
        "learning_rate": 0.00014753986436514445,
        "epoch": 1.5629333333333333,
        "step": 11722
    },
    {
        "loss": 1.4364,
        "grad_norm": 4.101154804229736,
        "learning_rate": 0.00014748448505960845,
        "epoch": 1.5630666666666668,
        "step": 11723
    },
    {
        "loss": 1.729,
        "grad_norm": 4.703610897064209,
        "learning_rate": 0.00014742908694530607,
        "epoch": 1.5632000000000001,
        "step": 11724
    },
    {
        "loss": 1.7712,
        "grad_norm": 3.517244577407837,
        "learning_rate": 0.000147373670044181,
        "epoch": 1.5633333333333335,
        "step": 11725
    },
    {
        "loss": 2.123,
        "grad_norm": 3.478396415710449,
        "learning_rate": 0.00014731823437818357,
        "epoch": 1.5634666666666668,
        "step": 11726
    },
    {
        "loss": 2.4241,
        "grad_norm": 4.578126430511475,
        "learning_rate": 0.0001472627799692724,
        "epoch": 1.5636,
        "step": 11727
    },
    {
        "loss": 2.4878,
        "grad_norm": 4.110692977905273,
        "learning_rate": 0.0001472073068394129,
        "epoch": 1.5637333333333334,
        "step": 11728
    },
    {
        "loss": 1.3971,
        "grad_norm": 4.142539024353027,
        "learning_rate": 0.00014715181501057844,
        "epoch": 1.5638666666666667,
        "step": 11729
    },
    {
        "loss": 1.4313,
        "grad_norm": 3.5802125930786133,
        "learning_rate": 0.0001470963045047494,
        "epoch": 1.564,
        "step": 11730
    },
    {
        "loss": 1.4618,
        "grad_norm": 4.494732856750488,
        "learning_rate": 0.00014704077534391364,
        "epoch": 1.5641333333333334,
        "step": 11731
    },
    {
        "loss": 2.5515,
        "grad_norm": 3.6758806705474854,
        "learning_rate": 0.00014698522755006636,
        "epoch": 1.5642666666666667,
        "step": 11732
    },
    {
        "loss": 1.0584,
        "grad_norm": 4.45814323425293,
        "learning_rate": 0.0001469296611452105,
        "epoch": 1.5644,
        "step": 11733
    },
    {
        "loss": 2.8592,
        "grad_norm": 3.9441487789154053,
        "learning_rate": 0.00014687407615135594,
        "epoch": 1.5645333333333333,
        "step": 11734
    },
    {
        "loss": 2.7333,
        "grad_norm": 4.011417865753174,
        "learning_rate": 0.00014681847259051992,
        "epoch": 1.5646666666666667,
        "step": 11735
    },
    {
        "loss": 2.121,
        "grad_norm": 4.453887939453125,
        "learning_rate": 0.00014676285048472755,
        "epoch": 1.5648,
        "step": 11736
    },
    {
        "loss": 2.3622,
        "grad_norm": 5.621573448181152,
        "learning_rate": 0.0001467072098560108,
        "epoch": 1.5649333333333333,
        "step": 11737
    },
    {
        "loss": 3.0895,
        "grad_norm": 2.9107890129089355,
        "learning_rate": 0.0001466515507264089,
        "epoch": 1.5650666666666666,
        "step": 11738
    },
    {
        "loss": 2.4531,
        "grad_norm": 5.3420867919921875,
        "learning_rate": 0.00014659587311796892,
        "epoch": 1.5652,
        "step": 11739
    },
    {
        "loss": 0.7759,
        "grad_norm": 4.245057106018066,
        "learning_rate": 0.00014654017705274482,
        "epoch": 1.5653333333333332,
        "step": 11740
    },
    {
        "loss": 1.9211,
        "grad_norm": 5.666773796081543,
        "learning_rate": 0.00014648446255279793,
        "epoch": 1.5654666666666666,
        "step": 11741
    },
    {
        "loss": 2.269,
        "grad_norm": 3.5424678325653076,
        "learning_rate": 0.00014642872964019713,
        "epoch": 1.5655999999999999,
        "step": 11742
    },
    {
        "loss": 2.69,
        "grad_norm": 4.595959186553955,
        "learning_rate": 0.00014637297833701828,
        "epoch": 1.5657333333333332,
        "step": 11743
    },
    {
        "loss": 2.0165,
        "grad_norm": 2.426145553588867,
        "learning_rate": 0.00014631720866534464,
        "epoch": 1.5658666666666665,
        "step": 11744
    },
    {
        "loss": 1.6604,
        "grad_norm": 3.7681615352630615,
        "learning_rate": 0.0001462614206472669,
        "epoch": 1.5659999999999998,
        "step": 11745
    },
    {
        "loss": 1.5692,
        "grad_norm": 2.4184257984161377,
        "learning_rate": 0.00014620561430488281,
        "epoch": 1.5661333333333334,
        "step": 11746
    },
    {
        "loss": 2.1043,
        "grad_norm": 4.817570209503174,
        "learning_rate": 0.00014614978966029737,
        "epoch": 1.5662666666666667,
        "step": 11747
    },
    {
        "loss": 2.1725,
        "grad_norm": 3.4288291931152344,
        "learning_rate": 0.00014609394673562308,
        "epoch": 1.5664,
        "step": 11748
    },
    {
        "loss": 1.872,
        "grad_norm": 4.191896915435791,
        "learning_rate": 0.00014603808555297947,
        "epoch": 1.5665333333333333,
        "step": 11749
    },
    {
        "loss": 1.6031,
        "grad_norm": 6.073318004608154,
        "learning_rate": 0.00014598220613449325,
        "epoch": 1.5666666666666667,
        "step": 11750
    },
    {
        "loss": 0.8951,
        "grad_norm": 6.009415149688721,
        "learning_rate": 0.0001459263085022984,
        "epoch": 1.5668,
        "step": 11751
    },
    {
        "loss": 2.4188,
        "grad_norm": 3.001035451889038,
        "learning_rate": 0.0001458703926785364,
        "epoch": 1.5669333333333333,
        "step": 11752
    },
    {
        "loss": 2.6206,
        "grad_norm": 3.9722964763641357,
        "learning_rate": 0.0001458144586853554,
        "epoch": 1.5670666666666668,
        "step": 11753
    },
    {
        "loss": 2.1566,
        "grad_norm": 4.515042781829834,
        "learning_rate": 0.00014575850654491147,
        "epoch": 1.5672000000000001,
        "step": 11754
    },
    {
        "loss": 2.5081,
        "grad_norm": 3.2801742553710938,
        "learning_rate": 0.00014570253627936694,
        "epoch": 1.5673333333333335,
        "step": 11755
    },
    {
        "loss": 0.9337,
        "grad_norm": 4.549760341644287,
        "learning_rate": 0.00014564654791089204,
        "epoch": 1.5674666666666668,
        "step": 11756
    },
    {
        "loss": 2.4752,
        "grad_norm": 3.169055461883545,
        "learning_rate": 0.00014559054146166413,
        "epoch": 1.5676,
        "step": 11757
    },
    {
        "loss": 2.3497,
        "grad_norm": 3.0532379150390625,
        "learning_rate": 0.0001455345169538674,
        "epoch": 1.5677333333333334,
        "step": 11758
    },
    {
        "loss": 2.1744,
        "grad_norm": 3.361914873123169,
        "learning_rate": 0.00014547847440969332,
        "epoch": 1.5678666666666667,
        "step": 11759
    },
    {
        "loss": 2.3989,
        "grad_norm": 4.211564540863037,
        "learning_rate": 0.00014542241385134046,
        "epoch": 1.568,
        "step": 11760
    },
    {
        "loss": 2.7845,
        "grad_norm": 3.3226451873779297,
        "learning_rate": 0.00014536633530101484,
        "epoch": 1.5681333333333334,
        "step": 11761
    },
    {
        "loss": 2.0359,
        "grad_norm": 4.602250099182129,
        "learning_rate": 0.0001453102387809291,
        "epoch": 1.5682666666666667,
        "step": 11762
    },
    {
        "loss": 2.0594,
        "grad_norm": 3.7091405391693115,
        "learning_rate": 0.0001452541243133036,
        "epoch": 1.5684,
        "step": 11763
    },
    {
        "loss": 2.4724,
        "grad_norm": 3.8089077472686768,
        "learning_rate": 0.000145197991920365,
        "epoch": 1.5685333333333333,
        "step": 11764
    },
    {
        "loss": 2.4775,
        "grad_norm": 2.8652126789093018,
        "learning_rate": 0.00014514184162434782,
        "epoch": 1.5686666666666667,
        "step": 11765
    },
    {
        "loss": 2.3678,
        "grad_norm": 4.753727436065674,
        "learning_rate": 0.0001450856734474935,
        "epoch": 1.5688,
        "step": 11766
    },
    {
        "loss": 1.2557,
        "grad_norm": 4.602828025817871,
        "learning_rate": 0.00014502948741205028,
        "epoch": 1.5689333333333333,
        "step": 11767
    },
    {
        "loss": 2.189,
        "grad_norm": 3.4497697353363037,
        "learning_rate": 0.00014497328354027365,
        "epoch": 1.5690666666666666,
        "step": 11768
    },
    {
        "loss": 1.0216,
        "grad_norm": 3.519948720932007,
        "learning_rate": 0.0001449170618544261,
        "epoch": 1.5692,
        "step": 11769
    },
    {
        "loss": 1.1061,
        "grad_norm": 5.000072002410889,
        "learning_rate": 0.00014486082237677737,
        "epoch": 1.5693333333333332,
        "step": 11770
    },
    {
        "loss": 2.5907,
        "grad_norm": 3.4773683547973633,
        "learning_rate": 0.00014480456512960403,
        "epoch": 1.5694666666666666,
        "step": 11771
    },
    {
        "loss": 2.4551,
        "grad_norm": 5.489565849304199,
        "learning_rate": 0.0001447482901351898,
        "epoch": 1.5695999999999999,
        "step": 11772
    },
    {
        "loss": 2.1671,
        "grad_norm": 5.084506034851074,
        "learning_rate": 0.0001446919974158252,
        "epoch": 1.5697333333333332,
        "step": 11773
    },
    {
        "loss": 2.1986,
        "grad_norm": 3.811166524887085,
        "learning_rate": 0.0001446356869938082,
        "epoch": 1.5698666666666665,
        "step": 11774
    },
    {
        "loss": 1.5157,
        "grad_norm": 4.464293003082275,
        "learning_rate": 0.00014457935889144373,
        "epoch": 1.5699999999999998,
        "step": 11775
    },
    {
        "loss": 2.4811,
        "grad_norm": 4.599534034729004,
        "learning_rate": 0.00014452301313104304,
        "epoch": 1.5701333333333334,
        "step": 11776
    },
    {
        "loss": 3.0995,
        "grad_norm": 3.733318567276001,
        "learning_rate": 0.00014446664973492528,
        "epoch": 1.5702666666666667,
        "step": 11777
    },
    {
        "loss": 1.7696,
        "grad_norm": 4.306236267089844,
        "learning_rate": 0.00014441026872541587,
        "epoch": 1.5704,
        "step": 11778
    },
    {
        "loss": 2.0702,
        "grad_norm": 3.38301157951355,
        "learning_rate": 0.00014435387012484786,
        "epoch": 1.5705333333333333,
        "step": 11779
    },
    {
        "loss": 2.6829,
        "grad_norm": 4.206433296203613,
        "learning_rate": 0.00014429745395556077,
        "epoch": 1.5706666666666667,
        "step": 11780
    },
    {
        "loss": 2.565,
        "grad_norm": 3.3072121143341064,
        "learning_rate": 0.0001442410202399012,
        "epoch": 1.5708,
        "step": 11781
    },
    {
        "loss": 2.0833,
        "grad_norm": 3.9435853958129883,
        "learning_rate": 0.00014418456900022264,
        "epoch": 1.5709333333333333,
        "step": 11782
    },
    {
        "loss": 2.1195,
        "grad_norm": 5.159060955047607,
        "learning_rate": 0.00014412810025888576,
        "epoch": 1.5710666666666666,
        "step": 11783
    },
    {
        "loss": 0.8092,
        "grad_norm": 4.610825538635254,
        "learning_rate": 0.00014407161403825827,
        "epoch": 1.5712000000000002,
        "step": 11784
    },
    {
        "loss": 2.6241,
        "grad_norm": 3.1433050632476807,
        "learning_rate": 0.00014401511036071403,
        "epoch": 1.5713333333333335,
        "step": 11785
    },
    {
        "loss": 0.908,
        "grad_norm": 4.455612659454346,
        "learning_rate": 0.0001439585892486347,
        "epoch": 1.5714666666666668,
        "step": 11786
    },
    {
        "loss": 1.2614,
        "grad_norm": 4.91748046875,
        "learning_rate": 0.00014390205072440827,
        "epoch": 1.5716,
        "step": 11787
    },
    {
        "loss": 1.5691,
        "grad_norm": 4.539181232452393,
        "learning_rate": 0.00014384549481043007,
        "epoch": 1.5717333333333334,
        "step": 11788
    },
    {
        "loss": 2.7456,
        "grad_norm": 2.8168327808380127,
        "learning_rate": 0.00014378892152910197,
        "epoch": 1.5718666666666667,
        "step": 11789
    },
    {
        "loss": 2.2001,
        "grad_norm": 3.6337618827819824,
        "learning_rate": 0.0001437323309028329,
        "epoch": 1.572,
        "step": 11790
    },
    {
        "loss": 1.6269,
        "grad_norm": 4.434875011444092,
        "learning_rate": 0.00014367572295403836,
        "epoch": 1.5721333333333334,
        "step": 11791
    },
    {
        "loss": 2.5425,
        "grad_norm": 4.573243141174316,
        "learning_rate": 0.0001436190977051413,
        "epoch": 1.5722666666666667,
        "step": 11792
    },
    {
        "loss": 2.2997,
        "grad_norm": 4.415255546569824,
        "learning_rate": 0.000143562455178571,
        "epoch": 1.5724,
        "step": 11793
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.640540838241577,
        "learning_rate": 0.0001435057953967636,
        "epoch": 1.5725333333333333,
        "step": 11794
    },
    {
        "loss": 1.5718,
        "grad_norm": 3.9837825298309326,
        "learning_rate": 0.0001434491183821626,
        "epoch": 1.5726666666666667,
        "step": 11795
    },
    {
        "loss": 1.8274,
        "grad_norm": 3.135254383087158,
        "learning_rate": 0.00014339242415721773,
        "epoch": 1.5728,
        "step": 11796
    },
    {
        "loss": 1.9807,
        "grad_norm": 2.6541523933410645,
        "learning_rate": 0.00014333571274438564,
        "epoch": 1.5729333333333333,
        "step": 11797
    },
    {
        "loss": 2.6788,
        "grad_norm": 2.952988624572754,
        "learning_rate": 0.0001432789841661302,
        "epoch": 1.5730666666666666,
        "step": 11798
    },
    {
        "loss": 2.6134,
        "grad_norm": 7.096518516540527,
        "learning_rate": 0.00014322223844492172,
        "epoch": 1.5732,
        "step": 11799
    },
    {
        "loss": 1.5427,
        "grad_norm": 3.1327638626098633,
        "learning_rate": 0.00014316547560323727,
        "epoch": 1.5733333333333333,
        "step": 11800
    },
    {
        "loss": 2.5556,
        "grad_norm": 2.0846474170684814,
        "learning_rate": 0.00014310869566356072,
        "epoch": 1.5734666666666666,
        "step": 11801
    },
    {
        "loss": 1.322,
        "grad_norm": 2.9393515586853027,
        "learning_rate": 0.00014305189864838305,
        "epoch": 1.5735999999999999,
        "step": 11802
    },
    {
        "loss": 2.6809,
        "grad_norm": 2.8661954402923584,
        "learning_rate": 0.00014299508458020152,
        "epoch": 1.5737333333333332,
        "step": 11803
    },
    {
        "loss": 1.5453,
        "grad_norm": 5.787250518798828,
        "learning_rate": 0.00014293825348152064,
        "epoch": 1.5738666666666665,
        "step": 11804
    },
    {
        "loss": 0.6221,
        "grad_norm": 5.218951225280762,
        "learning_rate": 0.0001428814053748512,
        "epoch": 1.5739999999999998,
        "step": 11805
    },
    {
        "loss": 1.199,
        "grad_norm": 9.836626052856445,
        "learning_rate": 0.00014282454028271088,
        "epoch": 1.5741333333333334,
        "step": 11806
    },
    {
        "loss": 1.2994,
        "grad_norm": 4.946749687194824,
        "learning_rate": 0.0001427676582276243,
        "epoch": 1.5742666666666667,
        "step": 11807
    },
    {
        "loss": 2.4167,
        "grad_norm": 3.9333999156951904,
        "learning_rate": 0.00014271075923212264,
        "epoch": 1.5744,
        "step": 11808
    },
    {
        "loss": 2.0737,
        "grad_norm": 4.7730302810668945,
        "learning_rate": 0.00014265384331874369,
        "epoch": 1.5745333333333333,
        "step": 11809
    },
    {
        "loss": 1.8648,
        "grad_norm": 5.509652614593506,
        "learning_rate": 0.00014259691051003197,
        "epoch": 1.5746666666666667,
        "step": 11810
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.607048988342285,
        "learning_rate": 0.00014253996082853892,
        "epoch": 1.5748,
        "step": 11811
    },
    {
        "loss": 1.5822,
        "grad_norm": 3.316650629043579,
        "learning_rate": 0.00014248299429682235,
        "epoch": 1.5749333333333333,
        "step": 11812
    },
    {
        "loss": 2.0993,
        "grad_norm": 3.262495756149292,
        "learning_rate": 0.0001424260109374472,
        "epoch": 1.5750666666666666,
        "step": 11813
    },
    {
        "loss": 2.631,
        "grad_norm": 3.3402340412139893,
        "learning_rate": 0.00014236901077298436,
        "epoch": 1.5752000000000002,
        "step": 11814
    },
    {
        "loss": 2.4999,
        "grad_norm": 3.7607743740081787,
        "learning_rate": 0.00014231199382601194,
        "epoch": 1.5753333333333335,
        "step": 11815
    },
    {
        "loss": 1.2431,
        "grad_norm": 5.160754680633545,
        "learning_rate": 0.0001422549601191147,
        "epoch": 1.5754666666666668,
        "step": 11816
    },
    {
        "loss": 1.7058,
        "grad_norm": 6.785472869873047,
        "learning_rate": 0.0001421979096748838,
        "epoch": 1.5756000000000001,
        "step": 11817
    },
    {
        "loss": 2.0926,
        "grad_norm": 4.916903972625732,
        "learning_rate": 0.0001421408425159171,
        "epoch": 1.5757333333333334,
        "step": 11818
    },
    {
        "loss": 3.1903,
        "grad_norm": 3.654348373413086,
        "learning_rate": 0.00014208375866481894,
        "epoch": 1.5758666666666667,
        "step": 11819
    },
    {
        "loss": 1.7459,
        "grad_norm": 2.3960084915161133,
        "learning_rate": 0.00014202665814420072,
        "epoch": 1.576,
        "step": 11820
    },
    {
        "loss": 1.9713,
        "grad_norm": 5.196069717407227,
        "learning_rate": 0.00014196954097667986,
        "epoch": 1.5761333333333334,
        "step": 11821
    },
    {
        "loss": 1.3947,
        "grad_norm": 5.503996849060059,
        "learning_rate": 0.00014191240718488111,
        "epoch": 1.5762666666666667,
        "step": 11822
    },
    {
        "loss": 2.0846,
        "grad_norm": 3.7589638233184814,
        "learning_rate": 0.00014185525679143474,
        "epoch": 1.5764,
        "step": 11823
    },
    {
        "loss": 2.3357,
        "grad_norm": 3.716320037841797,
        "learning_rate": 0.00014179808981897856,
        "epoch": 1.5765333333333333,
        "step": 11824
    },
    {
        "loss": 2.2117,
        "grad_norm": 3.7411746978759766,
        "learning_rate": 0.00014174090629015673,
        "epoch": 1.5766666666666667,
        "step": 11825
    },
    {
        "loss": 1.9493,
        "grad_norm": 4.2860798835754395,
        "learning_rate": 0.00014168370622761968,
        "epoch": 1.5768,
        "step": 11826
    },
    {
        "loss": 1.548,
        "grad_norm": 3.7728896141052246,
        "learning_rate": 0.00014162648965402455,
        "epoch": 1.5769333333333333,
        "step": 11827
    },
    {
        "loss": 2.7161,
        "grad_norm": 3.96604323387146,
        "learning_rate": 0.00014156925659203492,
        "epoch": 1.5770666666666666,
        "step": 11828
    },
    {
        "loss": 2.5562,
        "grad_norm": 3.373366355895996,
        "learning_rate": 0.0001415120070643212,
        "epoch": 1.5772,
        "step": 11829
    },
    {
        "loss": 2.4425,
        "grad_norm": 3.928912878036499,
        "learning_rate": 0.00014145474109356009,
        "epoch": 1.5773333333333333,
        "step": 11830
    },
    {
        "loss": 1.2308,
        "grad_norm": 7.184954643249512,
        "learning_rate": 0.00014139745870243476,
        "epoch": 1.5774666666666666,
        "step": 11831
    },
    {
        "loss": 2.3531,
        "grad_norm": 3.1557106971740723,
        "learning_rate": 0.00014134015991363485,
        "epoch": 1.5776,
        "step": 11832
    },
    {
        "loss": 2.1899,
        "grad_norm": 5.515297889709473,
        "learning_rate": 0.00014128284474985672,
        "epoch": 1.5777333333333332,
        "step": 11833
    },
    {
        "loss": 1.3653,
        "grad_norm": 4.210031032562256,
        "learning_rate": 0.00014122551323380336,
        "epoch": 1.5778666666666665,
        "step": 11834
    },
    {
        "loss": 2.0916,
        "grad_norm": 3.828289747238159,
        "learning_rate": 0.0001411681653881835,
        "epoch": 1.5779999999999998,
        "step": 11835
    },
    {
        "loss": 2.2327,
        "grad_norm": 4.24153470993042,
        "learning_rate": 0.00014111080123571318,
        "epoch": 1.5781333333333334,
        "step": 11836
    },
    {
        "loss": 2.5627,
        "grad_norm": 5.0169830322265625,
        "learning_rate": 0.00014105342079911428,
        "epoch": 1.5782666666666667,
        "step": 11837
    },
    {
        "loss": 2.2815,
        "grad_norm": 3.882235527038574,
        "learning_rate": 0.00014099602410111566,
        "epoch": 1.5784,
        "step": 11838
    },
    {
        "loss": 1.3455,
        "grad_norm": 5.27861213684082,
        "learning_rate": 0.00014093861116445217,
        "epoch": 1.5785333333333333,
        "step": 11839
    },
    {
        "loss": 1.3668,
        "grad_norm": 2.5633459091186523,
        "learning_rate": 0.0001408811820118653,
        "epoch": 1.5786666666666667,
        "step": 11840
    },
    {
        "loss": 1.7025,
        "grad_norm": 4.787212371826172,
        "learning_rate": 0.00014082373666610284,
        "epoch": 1.5788,
        "step": 11841
    },
    {
        "loss": 2.8968,
        "grad_norm": 2.7789368629455566,
        "learning_rate": 0.0001407662751499192,
        "epoch": 1.5789333333333333,
        "step": 11842
    },
    {
        "loss": 1.7894,
        "grad_norm": 4.699791431427002,
        "learning_rate": 0.00014070879748607527,
        "epoch": 1.5790666666666666,
        "step": 11843
    },
    {
        "loss": 2.3826,
        "grad_norm": 3.4516758918762207,
        "learning_rate": 0.00014065130369733772,
        "epoch": 1.5792000000000002,
        "step": 11844
    },
    {
        "loss": 2.5864,
        "grad_norm": 3.1643311977386475,
        "learning_rate": 0.0001405937938064804,
        "epoch": 1.5793333333333335,
        "step": 11845
    },
    {
        "loss": 2.4146,
        "grad_norm": 3.3433988094329834,
        "learning_rate": 0.00014053626783628285,
        "epoch": 1.5794666666666668,
        "step": 11846
    },
    {
        "loss": 2.2751,
        "grad_norm": 3.9017128944396973,
        "learning_rate": 0.00014047872580953165,
        "epoch": 1.5796000000000001,
        "step": 11847
    },
    {
        "loss": 1.613,
        "grad_norm": 4.428391456604004,
        "learning_rate": 0.00014042116774901924,
        "epoch": 1.5797333333333334,
        "step": 11848
    },
    {
        "loss": 1.4986,
        "grad_norm": 3.144524335861206,
        "learning_rate": 0.00014036359367754453,
        "epoch": 1.5798666666666668,
        "step": 11849
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.8016915321350098,
        "learning_rate": 0.0001403060036179127,
        "epoch": 1.58,
        "step": 11850
    },
    {
        "loss": 2.7624,
        "grad_norm": 2.6905558109283447,
        "learning_rate": 0.00014024839759293564,
        "epoch": 1.5801333333333334,
        "step": 11851
    },
    {
        "loss": 2.5525,
        "grad_norm": 2.4232685565948486,
        "learning_rate": 0.00014019077562543115,
        "epoch": 1.5802666666666667,
        "step": 11852
    },
    {
        "loss": 2.1505,
        "grad_norm": 4.3935136795043945,
        "learning_rate": 0.00014013313773822336,
        "epoch": 1.5804,
        "step": 11853
    },
    {
        "loss": 2.6664,
        "grad_norm": 2.749197006225586,
        "learning_rate": 0.00014007548395414307,
        "epoch": 1.5805333333333333,
        "step": 11854
    },
    {
        "loss": 0.8496,
        "grad_norm": 3.7666890621185303,
        "learning_rate": 0.00014001781429602705,
        "epoch": 1.5806666666666667,
        "step": 11855
    },
    {
        "loss": 2.2388,
        "grad_norm": 3.38843035697937,
        "learning_rate": 0.0001399601287867183,
        "epoch": 1.5808,
        "step": 11856
    },
    {
        "loss": 1.9624,
        "grad_norm": 4.6097259521484375,
        "learning_rate": 0.00013990242744906653,
        "epoch": 1.5809333333333333,
        "step": 11857
    },
    {
        "loss": 1.9481,
        "grad_norm": 4.454747676849365,
        "learning_rate": 0.00013984471030592722,
        "epoch": 1.5810666666666666,
        "step": 11858
    },
    {
        "loss": 2.7122,
        "grad_norm": 3.277822256088257,
        "learning_rate": 0.00013978697738016243,
        "epoch": 1.5812,
        "step": 11859
    },
    {
        "loss": 2.3661,
        "grad_norm": 3.1449785232543945,
        "learning_rate": 0.00013972922869464021,
        "epoch": 1.5813333333333333,
        "step": 11860
    },
    {
        "loss": 2.1652,
        "grad_norm": 2.1749789714813232,
        "learning_rate": 0.00013967146427223525,
        "epoch": 1.5814666666666666,
        "step": 11861
    },
    {
        "loss": 2.6271,
        "grad_norm": 3.1106784343719482,
        "learning_rate": 0.00013961368413582798,
        "epoch": 1.5816,
        "step": 11862
    },
    {
        "loss": 2.8601,
        "grad_norm": 3.3328263759613037,
        "learning_rate": 0.0001395558883083056,
        "epoch": 1.5817333333333332,
        "step": 11863
    },
    {
        "loss": 2.3784,
        "grad_norm": 4.3410964012146,
        "learning_rate": 0.00013949807681256103,
        "epoch": 1.5818666666666665,
        "step": 11864
    },
    {
        "loss": 1.983,
        "grad_norm": 4.039201259613037,
        "learning_rate": 0.00013944024967149353,
        "epoch": 1.5819999999999999,
        "step": 11865
    },
    {
        "loss": 1.9432,
        "grad_norm": 4.099494457244873,
        "learning_rate": 0.0001393824069080089,
        "epoch": 1.5821333333333332,
        "step": 11866
    },
    {
        "loss": 2.7688,
        "grad_norm": 4.1407928466796875,
        "learning_rate": 0.00013932454854501868,
        "epoch": 1.5822666666666667,
        "step": 11867
    },
    {
        "loss": 2.9963,
        "grad_norm": 4.052977561950684,
        "learning_rate": 0.0001392666746054408,
        "epoch": 1.5824,
        "step": 11868
    },
    {
        "loss": 2.1008,
        "grad_norm": 3.6407041549682617,
        "learning_rate": 0.00013920878511219917,
        "epoch": 1.5825333333333333,
        "step": 11869
    },
    {
        "loss": 2.4376,
        "grad_norm": 3.807157516479492,
        "learning_rate": 0.0001391508800882243,
        "epoch": 1.5826666666666667,
        "step": 11870
    },
    {
        "loss": 1.3167,
        "grad_norm": 4.643337249755859,
        "learning_rate": 0.00013909295955645228,
        "epoch": 1.5828,
        "step": 11871
    },
    {
        "loss": 3.3993,
        "grad_norm": 3.616734027862549,
        "learning_rate": 0.00013903502353982604,
        "epoch": 1.5829333333333333,
        "step": 11872
    },
    {
        "loss": 2.0468,
        "grad_norm": 2.8461568355560303,
        "learning_rate": 0.0001389770720612937,
        "epoch": 1.5830666666666666,
        "step": 11873
    },
    {
        "loss": 2.2373,
        "grad_norm": 2.5627639293670654,
        "learning_rate": 0.00013891910514381031,
        "epoch": 1.5832000000000002,
        "step": 11874
    },
    {
        "loss": 1.9226,
        "grad_norm": 3.0302884578704834,
        "learning_rate": 0.00013886112281033693,
        "epoch": 1.5833333333333335,
        "step": 11875
    },
    {
        "loss": 2.6113,
        "grad_norm": 4.289722442626953,
        "learning_rate": 0.00013880312508384032,
        "epoch": 1.5834666666666668,
        "step": 11876
    },
    {
        "loss": 2.1879,
        "grad_norm": 3.0984537601470947,
        "learning_rate": 0.00013874511198729367,
        "epoch": 1.5836000000000001,
        "step": 11877
    },
    {
        "loss": 2.2225,
        "grad_norm": 3.906196355819702,
        "learning_rate": 0.00013868708354367604,
        "epoch": 1.5837333333333334,
        "step": 11878
    },
    {
        "loss": 2.4418,
        "grad_norm": 4.093377113342285,
        "learning_rate": 0.0001386290397759729,
        "epoch": 1.5838666666666668,
        "step": 11879
    },
    {
        "loss": 2.0137,
        "grad_norm": 3.0886943340301514,
        "learning_rate": 0.00013857098070717546,
        "epoch": 1.584,
        "step": 11880
    },
    {
        "loss": 1.7304,
        "grad_norm": 4.571737766265869,
        "learning_rate": 0.00013851290636028116,
        "epoch": 1.5841333333333334,
        "step": 11881
    },
    {
        "loss": 2.3347,
        "grad_norm": 3.918578624725342,
        "learning_rate": 0.0001384548167582933,
        "epoch": 1.5842666666666667,
        "step": 11882
    },
    {
        "loss": 1.7114,
        "grad_norm": 8.014225959777832,
        "learning_rate": 0.00013839671192422145,
        "epoch": 1.5844,
        "step": 11883
    },
    {
        "loss": 1.5205,
        "grad_norm": 5.574245929718018,
        "learning_rate": 0.00013833859188108134,
        "epoch": 1.5845333333333333,
        "step": 11884
    },
    {
        "loss": 1.7467,
        "grad_norm": 4.691099166870117,
        "learning_rate": 0.00013828045665189434,
        "epoch": 1.5846666666666667,
        "step": 11885
    },
    {
        "loss": 2.4291,
        "grad_norm": 3.5830628871917725,
        "learning_rate": 0.00013822230625968804,
        "epoch": 1.5848,
        "step": 11886
    },
    {
        "loss": 2.0628,
        "grad_norm": 4.852511882781982,
        "learning_rate": 0.00013816414072749585,
        "epoch": 1.5849333333333333,
        "step": 11887
    },
    {
        "loss": 1.6223,
        "grad_norm": 3.485374689102173,
        "learning_rate": 0.0001381059600783576,
        "epoch": 1.5850666666666666,
        "step": 11888
    },
    {
        "loss": 1.5641,
        "grad_norm": 5.172717094421387,
        "learning_rate": 0.00013804776433531873,
        "epoch": 1.5852,
        "step": 11889
    },
    {
        "loss": 1.8473,
        "grad_norm": 3.526390790939331,
        "learning_rate": 0.00013798955352143074,
        "epoch": 1.5853333333333333,
        "step": 11890
    },
    {
        "loss": 2.2845,
        "grad_norm": 4.27018404006958,
        "learning_rate": 0.00013793132765975102,
        "epoch": 1.5854666666666666,
        "step": 11891
    },
    {
        "loss": 2.3925,
        "grad_norm": 3.2967653274536133,
        "learning_rate": 0.0001378730867733432,
        "epoch": 1.5856,
        "step": 11892
    },
    {
        "loss": 2.3735,
        "grad_norm": 2.70173716545105,
        "learning_rate": 0.00013781483088527691,
        "epoch": 1.5857333333333332,
        "step": 11893
    },
    {
        "loss": 1.5504,
        "grad_norm": 3.9796104431152344,
        "learning_rate": 0.00013775656001862699,
        "epoch": 1.5858666666666665,
        "step": 11894
    },
    {
        "loss": 2.7345,
        "grad_norm": 3.3052632808685303,
        "learning_rate": 0.00013769827419647513,
        "epoch": 1.5859999999999999,
        "step": 11895
    },
    {
        "loss": 2.2329,
        "grad_norm": 4.122829437255859,
        "learning_rate": 0.00013763997344190827,
        "epoch": 1.5861333333333332,
        "step": 11896
    },
    {
        "loss": 2.077,
        "grad_norm": 3.6897644996643066,
        "learning_rate": 0.00013758165777801978,
        "epoch": 1.5862666666666667,
        "step": 11897
    },
    {
        "loss": 2.7582,
        "grad_norm": 3.0513663291931152,
        "learning_rate": 0.0001375233272279086,
        "epoch": 1.5864,
        "step": 11898
    },
    {
        "loss": 1.7032,
        "grad_norm": 4.464141368865967,
        "learning_rate": 0.00013746498181467967,
        "epoch": 1.5865333333333334,
        "step": 11899
    },
    {
        "loss": 2.2666,
        "grad_norm": 4.102258682250977,
        "learning_rate": 0.00013740662156144366,
        "epoch": 1.5866666666666667,
        "step": 11900
    },
    {
        "loss": 2.2498,
        "grad_norm": 4.509221076965332,
        "learning_rate": 0.0001373482464913175,
        "epoch": 1.5868,
        "step": 11901
    },
    {
        "loss": 1.8614,
        "grad_norm": 3.204336166381836,
        "learning_rate": 0.00013728985662742364,
        "epoch": 1.5869333333333333,
        "step": 11902
    },
    {
        "loss": 1.5545,
        "grad_norm": 4.019227504730225,
        "learning_rate": 0.0001372314519928904,
        "epoch": 1.5870666666666666,
        "step": 11903
    },
    {
        "loss": 2.4426,
        "grad_norm": 3.668339729309082,
        "learning_rate": 0.0001371730326108523,
        "epoch": 1.5872000000000002,
        "step": 11904
    },
    {
        "loss": 2.17,
        "grad_norm": 3.1587319374084473,
        "learning_rate": 0.00013711459850444916,
        "epoch": 1.5873333333333335,
        "step": 11905
    },
    {
        "loss": 1.6089,
        "grad_norm": 4.843143939971924,
        "learning_rate": 0.0001370561496968272,
        "epoch": 1.5874666666666668,
        "step": 11906
    },
    {
        "loss": 2.7414,
        "grad_norm": 4.372585773468018,
        "learning_rate": 0.00013699768621113813,
        "epoch": 1.5876000000000001,
        "step": 11907
    },
    {
        "loss": 1.2359,
        "grad_norm": 3.5849039554595947,
        "learning_rate": 0.00013693920807053946,
        "epoch": 1.5877333333333334,
        "step": 11908
    },
    {
        "loss": 2.6555,
        "grad_norm": 2.6324291229248047,
        "learning_rate": 0.00013688071529819446,
        "epoch": 1.5878666666666668,
        "step": 11909
    },
    {
        "loss": 1.914,
        "grad_norm": 2.9048237800598145,
        "learning_rate": 0.0001368222079172726,
        "epoch": 1.588,
        "step": 11910
    },
    {
        "loss": 2.5422,
        "grad_norm": 2.7224032878875732,
        "learning_rate": 0.00013676368595094872,
        "epoch": 1.5881333333333334,
        "step": 11911
    },
    {
        "loss": 2.8446,
        "grad_norm": 2.1668002605438232,
        "learning_rate": 0.00013670514942240345,
        "epoch": 1.5882666666666667,
        "step": 11912
    },
    {
        "loss": 1.5552,
        "grad_norm": 4.5203962326049805,
        "learning_rate": 0.00013664659835482354,
        "epoch": 1.5884,
        "step": 11913
    },
    {
        "loss": 2.1588,
        "grad_norm": 4.798597812652588,
        "learning_rate": 0.00013658803277140114,
        "epoch": 1.5885333333333334,
        "step": 11914
    },
    {
        "loss": 0.6484,
        "grad_norm": 4.430902481079102,
        "learning_rate": 0.0001365294526953342,
        "epoch": 1.5886666666666667,
        "step": 11915
    },
    {
        "loss": 2.4478,
        "grad_norm": 4.2787089347839355,
        "learning_rate": 0.00013647085814982667,
        "epoch": 1.5888,
        "step": 11916
    },
    {
        "loss": 1.0193,
        "grad_norm": 5.232902526855469,
        "learning_rate": 0.00013641224915808797,
        "epoch": 1.5889333333333333,
        "step": 11917
    },
    {
        "loss": 1.9809,
        "grad_norm": 4.859481334686279,
        "learning_rate": 0.0001363536257433333,
        "epoch": 1.5890666666666666,
        "step": 11918
    },
    {
        "loss": 1.3408,
        "grad_norm": 2.600551128387451,
        "learning_rate": 0.00013629498792878345,
        "epoch": 1.5892,
        "step": 11919
    },
    {
        "loss": 2.3998,
        "grad_norm": 3.997413396835327,
        "learning_rate": 0.0001362363357376654,
        "epoch": 1.5893333333333333,
        "step": 11920
    },
    {
        "loss": 1.3462,
        "grad_norm": 5.084136962890625,
        "learning_rate": 0.00013617766919321112,
        "epoch": 1.5894666666666666,
        "step": 11921
    },
    {
        "loss": 2.2045,
        "grad_norm": 5.262906074523926,
        "learning_rate": 0.00013611898831865891,
        "epoch": 1.5896,
        "step": 11922
    },
    {
        "loss": 2.196,
        "grad_norm": 3.4965598583221436,
        "learning_rate": 0.00013606029313725237,
        "epoch": 1.5897333333333332,
        "step": 11923
    },
    {
        "loss": 1.7563,
        "grad_norm": 5.931966304779053,
        "learning_rate": 0.00013600158367224074,
        "epoch": 1.5898666666666665,
        "step": 11924
    },
    {
        "loss": 2.155,
        "grad_norm": 3.5589113235473633,
        "learning_rate": 0.00013594285994687928,
        "epoch": 1.5899999999999999,
        "step": 11925
    },
    {
        "loss": 1.5977,
        "grad_norm": 3.671491861343384,
        "learning_rate": 0.00013588412198442851,
        "epoch": 1.5901333333333332,
        "step": 11926
    },
    {
        "loss": 2.1651,
        "grad_norm": 4.044528484344482,
        "learning_rate": 0.0001358253698081547,
        "epoch": 1.5902666666666667,
        "step": 11927
    },
    {
        "loss": 2.2379,
        "grad_norm": 4.290951251983643,
        "learning_rate": 0.00013576660344132976,
        "epoch": 1.5904,
        "step": 11928
    },
    {
        "loss": 2.4676,
        "grad_norm": 2.915884017944336,
        "learning_rate": 0.00013570782290723145,
        "epoch": 1.5905333333333334,
        "step": 11929
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.6872377395629883,
        "learning_rate": 0.00013564902822914269,
        "epoch": 1.5906666666666667,
        "step": 11930
    },
    {
        "loss": 2.8946,
        "grad_norm": 3.801053285598755,
        "learning_rate": 0.00013559021943035263,
        "epoch": 1.5908,
        "step": 11931
    },
    {
        "loss": 1.7431,
        "grad_norm": 5.779089450836182,
        "learning_rate": 0.0001355313965341551,
        "epoch": 1.5909333333333333,
        "step": 11932
    },
    {
        "loss": 1.929,
        "grad_norm": 2.7195305824279785,
        "learning_rate": 0.00013547255956385034,
        "epoch": 1.5910666666666666,
        "step": 11933
    },
    {
        "loss": 1.6706,
        "grad_norm": 4.133820056915283,
        "learning_rate": 0.00013541370854274403,
        "epoch": 1.5912,
        "step": 11934
    },
    {
        "loss": 2.2022,
        "grad_norm": 3.0980618000030518,
        "learning_rate": 0.00013535484349414705,
        "epoch": 1.5913333333333335,
        "step": 11935
    },
    {
        "loss": 2.1674,
        "grad_norm": 3.367621898651123,
        "learning_rate": 0.00013529596444137614,
        "epoch": 1.5914666666666668,
        "step": 11936
    },
    {
        "loss": 2.3676,
        "grad_norm": 5.045135021209717,
        "learning_rate": 0.0001352370714077533,
        "epoch": 1.5916000000000001,
        "step": 11937
    },
    {
        "loss": 1.7521,
        "grad_norm": 2.6053507328033447,
        "learning_rate": 0.00013517816441660654,
        "epoch": 1.5917333333333334,
        "step": 11938
    },
    {
        "loss": 1.8103,
        "grad_norm": 5.34649658203125,
        "learning_rate": 0.00013511924349126895,
        "epoch": 1.5918666666666668,
        "step": 11939
    },
    {
        "loss": 2.5316,
        "grad_norm": 3.5579607486724854,
        "learning_rate": 0.0001350603086550794,
        "epoch": 1.592,
        "step": 11940
    },
    {
        "loss": 2.0197,
        "grad_norm": 5.855632305145264,
        "learning_rate": 0.00013500135993138197,
        "epoch": 1.5921333333333334,
        "step": 11941
    },
    {
        "loss": 2.2068,
        "grad_norm": 3.4656412601470947,
        "learning_rate": 0.00013494239734352664,
        "epoch": 1.5922666666666667,
        "step": 11942
    },
    {
        "loss": 1.8753,
        "grad_norm": 4.835492134094238,
        "learning_rate": 0.00013488342091486874,
        "epoch": 1.5924,
        "step": 11943
    },
    {
        "loss": 2.3336,
        "grad_norm": 4.238218784332275,
        "learning_rate": 0.00013482443066876898,
        "epoch": 1.5925333333333334,
        "step": 11944
    },
    {
        "loss": 2.2604,
        "grad_norm": 3.4946601390838623,
        "learning_rate": 0.00013476542662859355,
        "epoch": 1.5926666666666667,
        "step": 11945
    },
    {
        "loss": 1.599,
        "grad_norm": 3.9570443630218506,
        "learning_rate": 0.0001347064088177141,
        "epoch": 1.5928,
        "step": 11946
    },
    {
        "loss": 2.0056,
        "grad_norm": 4.629026889801025,
        "learning_rate": 0.00013464737725950792,
        "epoch": 1.5929333333333333,
        "step": 11947
    },
    {
        "loss": 2.6631,
        "grad_norm": 4.186687469482422,
        "learning_rate": 0.00013458833197735757,
        "epoch": 1.5930666666666666,
        "step": 11948
    },
    {
        "loss": 0.7226,
        "grad_norm": 3.2387752532958984,
        "learning_rate": 0.00013452927299465107,
        "epoch": 1.5932,
        "step": 11949
    },
    {
        "loss": 1.9018,
        "grad_norm": 3.8866751194000244,
        "learning_rate": 0.00013447020033478178,
        "epoch": 1.5933333333333333,
        "step": 11950
    },
    {
        "loss": 2.15,
        "grad_norm": 4.680272102355957,
        "learning_rate": 0.00013441111402114864,
        "epoch": 1.5934666666666666,
        "step": 11951
    },
    {
        "loss": 3.045,
        "grad_norm": 4.426159858703613,
        "learning_rate": 0.00013435201407715626,
        "epoch": 1.5936,
        "step": 11952
    },
    {
        "loss": 2.786,
        "grad_norm": 2.703328847885132,
        "learning_rate": 0.0001342929005262138,
        "epoch": 1.5937333333333332,
        "step": 11953
    },
    {
        "loss": 2.3238,
        "grad_norm": 3.299471616744995,
        "learning_rate": 0.0001342337733917367,
        "epoch": 1.5938666666666665,
        "step": 11954
    },
    {
        "loss": 2.5698,
        "grad_norm": 3.1752405166625977,
        "learning_rate": 0.0001341746326971452,
        "epoch": 1.5939999999999999,
        "step": 11955
    },
    {
        "loss": 1.8905,
        "grad_norm": 2.612311601638794,
        "learning_rate": 0.00013411547846586537,
        "epoch": 1.5941333333333332,
        "step": 11956
    },
    {
        "loss": 2.7243,
        "grad_norm": 4.4159979820251465,
        "learning_rate": 0.00013405631072132827,
        "epoch": 1.5942666666666667,
        "step": 11957
    },
    {
        "loss": 2.6619,
        "grad_norm": 2.4442107677459717,
        "learning_rate": 0.00013399712948697045,
        "epoch": 1.5944,
        "step": 11958
    },
    {
        "loss": 2.3274,
        "grad_norm": 4.287013053894043,
        "learning_rate": 0.00013393793478623368,
        "epoch": 1.5945333333333334,
        "step": 11959
    },
    {
        "loss": 2.136,
        "grad_norm": 4.089995384216309,
        "learning_rate": 0.0001338787266425654,
        "epoch": 1.5946666666666667,
        "step": 11960
    },
    {
        "loss": 2.5968,
        "grad_norm": 2.8458619117736816,
        "learning_rate": 0.00013381950507941807,
        "epoch": 1.5948,
        "step": 11961
    },
    {
        "loss": 1.3038,
        "grad_norm": 2.827340602874756,
        "learning_rate": 0.0001337602701202494,
        "epoch": 1.5949333333333333,
        "step": 11962
    },
    {
        "loss": 2.3496,
        "grad_norm": 3.133831262588501,
        "learning_rate": 0.00013370102178852285,
        "epoch": 1.5950666666666666,
        "step": 11963
    },
    {
        "loss": 2.6512,
        "grad_norm": 3.684218645095825,
        "learning_rate": 0.00013364176010770664,
        "epoch": 1.5952,
        "step": 11964
    },
    {
        "loss": 1.5933,
        "grad_norm": 5.29425573348999,
        "learning_rate": 0.00013358248510127472,
        "epoch": 1.5953333333333335,
        "step": 11965
    },
    {
        "loss": 2.629,
        "grad_norm": 4.739205360412598,
        "learning_rate": 0.00013352319679270597,
        "epoch": 1.5954666666666668,
        "step": 11966
    },
    {
        "loss": 2.1394,
        "grad_norm": 3.611962080001831,
        "learning_rate": 0.0001334638952054848,
        "epoch": 1.5956000000000001,
        "step": 11967
    },
    {
        "loss": 2.701,
        "grad_norm": 2.922354221343994,
        "learning_rate": 0.0001334045803631006,
        "epoch": 1.5957333333333334,
        "step": 11968
    },
    {
        "loss": 2.1842,
        "grad_norm": 2.116457223892212,
        "learning_rate": 0.00013334525228904837,
        "epoch": 1.5958666666666668,
        "step": 11969
    },
    {
        "loss": 2.5654,
        "grad_norm": 3.230837345123291,
        "learning_rate": 0.00013328591100682813,
        "epoch": 1.596,
        "step": 11970
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.8347179889678955,
        "learning_rate": 0.00013322655653994495,
        "epoch": 1.5961333333333334,
        "step": 11971
    },
    {
        "loss": 2.6417,
        "grad_norm": 2.5466861724853516,
        "learning_rate": 0.00013316718891190967,
        "epoch": 1.5962666666666667,
        "step": 11972
    },
    {
        "loss": 1.7026,
        "grad_norm": 3.849531650543213,
        "learning_rate": 0.00013310780814623785,
        "epoch": 1.5964,
        "step": 11973
    },
    {
        "loss": 3.1251,
        "grad_norm": 4.589749813079834,
        "learning_rate": 0.00013304841426645028,
        "epoch": 1.5965333333333334,
        "step": 11974
    },
    {
        "loss": 0.9481,
        "grad_norm": 4.586803436279297,
        "learning_rate": 0.00013298900729607333,
        "epoch": 1.5966666666666667,
        "step": 11975
    },
    {
        "loss": 1.9737,
        "grad_norm": 3.422621726989746,
        "learning_rate": 0.00013292958725863818,
        "epoch": 1.5968,
        "step": 11976
    },
    {
        "loss": 1.327,
        "grad_norm": 3.806680917739868,
        "learning_rate": 0.0001328701541776813,
        "epoch": 1.5969333333333333,
        "step": 11977
    },
    {
        "loss": 2.8402,
        "grad_norm": 4.063713550567627,
        "learning_rate": 0.00013281070807674424,
        "epoch": 1.5970666666666666,
        "step": 11978
    },
    {
        "loss": 2.0566,
        "grad_norm": 3.981853485107422,
        "learning_rate": 0.00013275124897937403,
        "epoch": 1.5972,
        "step": 11979
    },
    {
        "loss": 0.6786,
        "grad_norm": 3.8851981163024902,
        "learning_rate": 0.00013269177690912234,
        "epoch": 1.5973333333333333,
        "step": 11980
    },
    {
        "loss": 2.1713,
        "grad_norm": 3.163146734237671,
        "learning_rate": 0.00013263229188954675,
        "epoch": 1.5974666666666666,
        "step": 11981
    },
    {
        "loss": 1.2275,
        "grad_norm": 3.9176552295684814,
        "learning_rate": 0.00013257279394420887,
        "epoch": 1.5976,
        "step": 11982
    },
    {
        "loss": 2.6312,
        "grad_norm": 4.494384288787842,
        "learning_rate": 0.0001325132830966763,
        "epoch": 1.5977333333333332,
        "step": 11983
    },
    {
        "loss": 1.2392,
        "grad_norm": 4.060731410980225,
        "learning_rate": 0.00013245375937052164,
        "epoch": 1.5978666666666665,
        "step": 11984
    },
    {
        "loss": 2.0569,
        "grad_norm": 3.665508985519409,
        "learning_rate": 0.00013239422278932232,
        "epoch": 1.5979999999999999,
        "step": 11985
    },
    {
        "loss": 1.7333,
        "grad_norm": 3.7102532386779785,
        "learning_rate": 0.00013233467337666097,
        "epoch": 1.5981333333333332,
        "step": 11986
    },
    {
        "loss": 3.229,
        "grad_norm": 4.045559883117676,
        "learning_rate": 0.00013227511115612521,
        "epoch": 1.5982666666666665,
        "step": 11987
    },
    {
        "loss": 1.9266,
        "grad_norm": 3.819364070892334,
        "learning_rate": 0.00013221553615130804,
        "epoch": 1.5984,
        "step": 11988
    },
    {
        "loss": 2.2739,
        "grad_norm": 3.5549733638763428,
        "learning_rate": 0.00013215594838580713,
        "epoch": 1.5985333333333334,
        "step": 11989
    },
    {
        "loss": 2.0909,
        "grad_norm": 3.1822879314422607,
        "learning_rate": 0.00013209634788322573,
        "epoch": 1.5986666666666667,
        "step": 11990
    },
    {
        "loss": 2.6137,
        "grad_norm": 3.5525035858154297,
        "learning_rate": 0.0001320367346671713,
        "epoch": 1.5988,
        "step": 11991
    },
    {
        "loss": 2.2046,
        "grad_norm": 4.456181049346924,
        "learning_rate": 0.00013197710876125714,
        "epoch": 1.5989333333333333,
        "step": 11992
    },
    {
        "loss": 2.4037,
        "grad_norm": 2.881547451019287,
        "learning_rate": 0.00013191747018910134,
        "epoch": 1.5990666666666666,
        "step": 11993
    },
    {
        "loss": 1.1011,
        "grad_norm": 5.249776840209961,
        "learning_rate": 0.00013185781897432687,
        "epoch": 1.5992,
        "step": 11994
    },
    {
        "loss": 2.3815,
        "grad_norm": 4.3509626388549805,
        "learning_rate": 0.00013179815514056172,
        "epoch": 1.5993333333333335,
        "step": 11995
    },
    {
        "loss": 1.002,
        "grad_norm": 4.192498207092285,
        "learning_rate": 0.00013173847871143885,
        "epoch": 1.5994666666666668,
        "step": 11996
    },
    {
        "loss": 2.429,
        "grad_norm": 2.9281020164489746,
        "learning_rate": 0.00013167878971059657,
        "epoch": 1.5996000000000001,
        "step": 11997
    },
    {
        "loss": 2.7309,
        "grad_norm": 5.313795566558838,
        "learning_rate": 0.00013161908816167772,
        "epoch": 1.5997333333333335,
        "step": 11998
    },
    {
        "loss": 2.1109,
        "grad_norm": 4.706818580627441,
        "learning_rate": 0.0001315593740883303,
        "epoch": 1.5998666666666668,
        "step": 11999
    },
    {
        "loss": 1.8389,
        "grad_norm": 5.18646764755249,
        "learning_rate": 0.00013149964751420717,
        "epoch": 1.6,
        "step": 12000
    },
    {
        "loss": 2.3626,
        "grad_norm": 2.502307891845703,
        "learning_rate": 0.00013143990846296627,
        "epoch": 1.6001333333333334,
        "step": 12001
    },
    {
        "loss": 2.5276,
        "grad_norm": 2.6629719734191895,
        "learning_rate": 0.0001313801569582707,
        "epoch": 1.6002666666666667,
        "step": 12002
    },
    {
        "loss": 2.4163,
        "grad_norm": 3.071909189224243,
        "learning_rate": 0.00013132039302378798,
        "epoch": 1.6004,
        "step": 12003
    },
    {
        "loss": 2.452,
        "grad_norm": 4.616065979003906,
        "learning_rate": 0.0001312606166831909,
        "epoch": 1.6005333333333334,
        "step": 12004
    },
    {
        "loss": 1.949,
        "grad_norm": 5.28584098815918,
        "learning_rate": 0.00013120082796015686,
        "epoch": 1.6006666666666667,
        "step": 12005
    },
    {
        "loss": 2.979,
        "grad_norm": 2.1392111778259277,
        "learning_rate": 0.00013114102687836874,
        "epoch": 1.6008,
        "step": 12006
    },
    {
        "loss": 2.014,
        "grad_norm": 3.9626128673553467,
        "learning_rate": 0.00013108121346151372,
        "epoch": 1.6009333333333333,
        "step": 12007
    },
    {
        "loss": 1.7468,
        "grad_norm": 2.722943067550659,
        "learning_rate": 0.00013102138773328416,
        "epoch": 1.6010666666666666,
        "step": 12008
    },
    {
        "loss": 1.3212,
        "grad_norm": 5.815011024475098,
        "learning_rate": 0.00013096154971737706,
        "epoch": 1.6012,
        "step": 12009
    },
    {
        "loss": 0.7734,
        "grad_norm": 5.135636806488037,
        "learning_rate": 0.00013090169943749463,
        "epoch": 1.6013333333333333,
        "step": 12010
    },
    {
        "loss": 1.8559,
        "grad_norm": 3.7187206745147705,
        "learning_rate": 0.00013084183691734405,
        "epoch": 1.6014666666666666,
        "step": 12011
    },
    {
        "loss": 2.5349,
        "grad_norm": 2.5306553840637207,
        "learning_rate": 0.0001307819621806365,
        "epoch": 1.6016,
        "step": 12012
    },
    {
        "loss": 2.4324,
        "grad_norm": 3.4238193035125732,
        "learning_rate": 0.00013072207525108892,
        "epoch": 1.6017333333333332,
        "step": 12013
    },
    {
        "loss": 2.0998,
        "grad_norm": 2.8812005519866943,
        "learning_rate": 0.00013066217615242257,
        "epoch": 1.6018666666666665,
        "step": 12014
    },
    {
        "loss": 1.8269,
        "grad_norm": 4.7256646156311035,
        "learning_rate": 0.00013060226490836392,
        "epoch": 1.6019999999999999,
        "step": 12015
    },
    {
        "loss": 0.7776,
        "grad_norm": 3.7941529750823975,
        "learning_rate": 0.00013054234154264384,
        "epoch": 1.6021333333333332,
        "step": 12016
    },
    {
        "loss": 1.3807,
        "grad_norm": 3.6223456859588623,
        "learning_rate": 0.0001304824060789982,
        "epoch": 1.6022666666666665,
        "step": 12017
    },
    {
        "loss": 0.9795,
        "grad_norm": 3.679615020751953,
        "learning_rate": 0.0001304224585411675,
        "epoch": 1.6024,
        "step": 12018
    },
    {
        "loss": 2.2005,
        "grad_norm": 3.8502702713012695,
        "learning_rate": 0.00013036249895289744,
        "epoch": 1.6025333333333334,
        "step": 12019
    },
    {
        "loss": 1.5595,
        "grad_norm": 6.087352752685547,
        "learning_rate": 0.00013030252733793813,
        "epoch": 1.6026666666666667,
        "step": 12020
    },
    {
        "loss": 0.9787,
        "grad_norm": 4.698450565338135,
        "learning_rate": 0.0001302425437200443,
        "epoch": 1.6028,
        "step": 12021
    },
    {
        "loss": 1.4507,
        "grad_norm": 3.735917329788208,
        "learning_rate": 0.00013018254812297602,
        "epoch": 1.6029333333333333,
        "step": 12022
    },
    {
        "loss": 2.7873,
        "grad_norm": 2.6977856159210205,
        "learning_rate": 0.00013012254057049748,
        "epoch": 1.6030666666666666,
        "step": 12023
    },
    {
        "loss": 2.2428,
        "grad_norm": 4.242513656616211,
        "learning_rate": 0.00013006252108637806,
        "epoch": 1.6032,
        "step": 12024
    },
    {
        "loss": 2.8684,
        "grad_norm": 3.830075263977051,
        "learning_rate": 0.0001300024896943916,
        "epoch": 1.6033333333333335,
        "step": 12025
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.7397499084472656,
        "learning_rate": 0.00012994244641831678,
        "epoch": 1.6034666666666668,
        "step": 12026
    },
    {
        "loss": 2.8229,
        "grad_norm": 3.732020616531372,
        "learning_rate": 0.00012988239128193676,
        "epoch": 1.6036000000000001,
        "step": 12027
    },
    {
        "loss": 1.8688,
        "grad_norm": 5.706886291503906,
        "learning_rate": 0.00012982232430903986,
        "epoch": 1.6037333333333335,
        "step": 12028
    },
    {
        "loss": 3.0145,
        "grad_norm": 3.589606523513794,
        "learning_rate": 0.0001297622455234187,
        "epoch": 1.6038666666666668,
        "step": 12029
    },
    {
        "loss": 2.1446,
        "grad_norm": 4.114853382110596,
        "learning_rate": 0.00012970215494887053,
        "epoch": 1.604,
        "step": 12030
    },
    {
        "loss": 2.2233,
        "grad_norm": 2.954559087753296,
        "learning_rate": 0.0001296420526091977,
        "epoch": 1.6041333333333334,
        "step": 12031
    },
    {
        "loss": 1.2238,
        "grad_norm": 4.869239807128906,
        "learning_rate": 0.00012958193852820684,
        "epoch": 1.6042666666666667,
        "step": 12032
    },
    {
        "loss": 1.7148,
        "grad_norm": 3.791517972946167,
        "learning_rate": 0.0001295218127297092,
        "epoch": 1.6044,
        "step": 12033
    },
    {
        "loss": 1.3572,
        "grad_norm": 2.122519016265869,
        "learning_rate": 0.00012946167523752108,
        "epoch": 1.6045333333333334,
        "step": 12034
    },
    {
        "loss": 2.5295,
        "grad_norm": 4.562291622161865,
        "learning_rate": 0.000129401526075463,
        "epoch": 1.6046666666666667,
        "step": 12035
    },
    {
        "loss": 1.9617,
        "grad_norm": 3.0506558418273926,
        "learning_rate": 0.00012934136526736024,
        "epoch": 1.6048,
        "step": 12036
    },
    {
        "loss": 1.7434,
        "grad_norm": 4.721202373504639,
        "learning_rate": 0.00012928119283704264,
        "epoch": 1.6049333333333333,
        "step": 12037
    },
    {
        "loss": 2.7186,
        "grad_norm": 4.531570911407471,
        "learning_rate": 0.00012922100880834484,
        "epoch": 1.6050666666666666,
        "step": 12038
    },
    {
        "loss": 1.9745,
        "grad_norm": 3.3712873458862305,
        "learning_rate": 0.00012916081320510584,
        "epoch": 1.6052,
        "step": 12039
    },
    {
        "loss": 1.9171,
        "grad_norm": 3.390566825866699,
        "learning_rate": 0.00012910060605116956,
        "epoch": 1.6053333333333333,
        "step": 12040
    },
    {
        "loss": 0.8084,
        "grad_norm": 3.543083429336548,
        "learning_rate": 0.00012904038737038384,
        "epoch": 1.6054666666666666,
        "step": 12041
    },
    {
        "loss": 1.1006,
        "grad_norm": 4.723249912261963,
        "learning_rate": 0.0001289801571866017,
        "epoch": 1.6056,
        "step": 12042
    },
    {
        "loss": 1.8273,
        "grad_norm": 3.4090800285339355,
        "learning_rate": 0.0001289199155236807,
        "epoch": 1.6057333333333332,
        "step": 12043
    },
    {
        "loss": 2.126,
        "grad_norm": 3.9486117362976074,
        "learning_rate": 0.00012885966240548262,
        "epoch": 1.6058666666666666,
        "step": 12044
    },
    {
        "loss": 1.5161,
        "grad_norm": 4.040212631225586,
        "learning_rate": 0.00012879939785587395,
        "epoch": 1.6059999999999999,
        "step": 12045
    },
    {
        "loss": 1.8005,
        "grad_norm": 3.324192523956299,
        "learning_rate": 0.0001287391218987255,
        "epoch": 1.6061333333333332,
        "step": 12046
    },
    {
        "loss": 0.984,
        "grad_norm": 3.5948240756988525,
        "learning_rate": 0.00012867883455791306,
        "epoch": 1.6062666666666665,
        "step": 12047
    },
    {
        "loss": 2.5998,
        "grad_norm": 3.338613510131836,
        "learning_rate": 0.0001286185358573164,
        "epoch": 1.6064,
        "step": 12048
    },
    {
        "loss": 2.536,
        "grad_norm": 3.126631259918213,
        "learning_rate": 0.00012855822582082046,
        "epoch": 1.6065333333333334,
        "step": 12049
    },
    {
        "loss": 1.7496,
        "grad_norm": 4.688065528869629,
        "learning_rate": 0.00012849790447231365,
        "epoch": 1.6066666666666667,
        "step": 12050
    },
    {
        "loss": 1.81,
        "grad_norm": 8.14557933807373,
        "learning_rate": 0.00012843757183568982,
        "epoch": 1.6068,
        "step": 12051
    },
    {
        "loss": 2.0585,
        "grad_norm": 4.2907514572143555,
        "learning_rate": 0.00012837722793484702,
        "epoch": 1.6069333333333333,
        "step": 12052
    },
    {
        "loss": 0.7382,
        "grad_norm": 3.593954563140869,
        "learning_rate": 0.0001283168727936875,
        "epoch": 1.6070666666666666,
        "step": 12053
    },
    {
        "loss": 2.1213,
        "grad_norm": 4.445624828338623,
        "learning_rate": 0.0001282565064361182,
        "epoch": 1.6072,
        "step": 12054
    },
    {
        "loss": 2.985,
        "grad_norm": 4.475612163543701,
        "learning_rate": 0.00012819612888605031,
        "epoch": 1.6073333333333333,
        "step": 12055
    },
    {
        "loss": 2.3888,
        "grad_norm": 4.166566371917725,
        "learning_rate": 0.0001281357401673998,
        "epoch": 1.6074666666666668,
        "step": 12056
    },
    {
        "loss": 1.6264,
        "grad_norm": 3.3643691539764404,
        "learning_rate": 0.00012807534030408676,
        "epoch": 1.6076000000000001,
        "step": 12057
    },
    {
        "loss": 1.0565,
        "grad_norm": 5.140872001647949,
        "learning_rate": 0.00012801492932003577,
        "epoch": 1.6077333333333335,
        "step": 12058
    },
    {
        "loss": 2.005,
        "grad_norm": 5.082810878753662,
        "learning_rate": 0.00012795450723917566,
        "epoch": 1.6078666666666668,
        "step": 12059
    },
    {
        "loss": 1.2386,
        "grad_norm": 6.703566074371338,
        "learning_rate": 0.00012789407408543996,
        "epoch": 1.608,
        "step": 12060
    },
    {
        "loss": 2.1565,
        "grad_norm": 4.275944232940674,
        "learning_rate": 0.00012783362988276676,
        "epoch": 1.6081333333333334,
        "step": 12061
    },
    {
        "loss": 2.5293,
        "grad_norm": 2.7204623222351074,
        "learning_rate": 0.00012777317465509767,
        "epoch": 1.6082666666666667,
        "step": 12062
    },
    {
        "loss": 2.5754,
        "grad_norm": 2.956758499145508,
        "learning_rate": 0.00012771270842637958,
        "epoch": 1.6084,
        "step": 12063
    },
    {
        "loss": 1.6047,
        "grad_norm": 4.6463751792907715,
        "learning_rate": 0.00012765223122056314,
        "epoch": 1.6085333333333334,
        "step": 12064
    },
    {
        "loss": 1.2003,
        "grad_norm": 4.4657368659973145,
        "learning_rate": 0.0001275917430616038,
        "epoch": 1.6086666666666667,
        "step": 12065
    },
    {
        "loss": 1.6718,
        "grad_norm": 2.958984851837158,
        "learning_rate": 0.00012753124397346104,
        "epoch": 1.6088,
        "step": 12066
    },
    {
        "loss": 2.158,
        "grad_norm": 2.977238893508911,
        "learning_rate": 0.00012747073398009873,
        "epoch": 1.6089333333333333,
        "step": 12067
    },
    {
        "loss": 2.4421,
        "grad_norm": 4.630740165710449,
        "learning_rate": 0.00012741021310548492,
        "epoch": 1.6090666666666666,
        "step": 12068
    },
    {
        "loss": 1.8212,
        "grad_norm": 4.977935791015625,
        "learning_rate": 0.0001273496813735923,
        "epoch": 1.6092,
        "step": 12069
    },
    {
        "loss": 1.8616,
        "grad_norm": 3.2418618202209473,
        "learning_rate": 0.000127289138808398,
        "epoch": 1.6093333333333333,
        "step": 12070
    },
    {
        "loss": 1.8707,
        "grad_norm": 4.933463096618652,
        "learning_rate": 0.00012722858543388253,
        "epoch": 1.6094666666666666,
        "step": 12071
    },
    {
        "loss": 1.6094,
        "grad_norm": 4.406944751739502,
        "learning_rate": 0.00012716802127403176,
        "epoch": 1.6096,
        "step": 12072
    },
    {
        "loss": 2.0522,
        "grad_norm": 3.633612632751465,
        "learning_rate": 0.00012710744635283504,
        "epoch": 1.6097333333333332,
        "step": 12073
    },
    {
        "loss": 2.4719,
        "grad_norm": 4.34856653213501,
        "learning_rate": 0.00012704686069428658,
        "epoch": 1.6098666666666666,
        "step": 12074
    },
    {
        "loss": 2.7684,
        "grad_norm": 4.0671210289001465,
        "learning_rate": 0.00012698626432238442,
        "epoch": 1.6099999999999999,
        "step": 12075
    },
    {
        "loss": 1.77,
        "grad_norm": 4.7706217765808105,
        "learning_rate": 0.00012692565726113105,
        "epoch": 1.6101333333333332,
        "step": 12076
    },
    {
        "loss": 2.5015,
        "grad_norm": 3.709474563598633,
        "learning_rate": 0.00012686503953453294,
        "epoch": 1.6102666666666665,
        "step": 12077
    },
    {
        "loss": 1.496,
        "grad_norm": 5.54048490524292,
        "learning_rate": 0.00012680441116660126,
        "epoch": 1.6104,
        "step": 12078
    },
    {
        "loss": 3.2481,
        "grad_norm": 3.1124377250671387,
        "learning_rate": 0.000126743772181351,
        "epoch": 1.6105333333333334,
        "step": 12079
    },
    {
        "loss": 2.1878,
        "grad_norm": 6.693348407745361,
        "learning_rate": 0.0001266831226028013,
        "epoch": 1.6106666666666667,
        "step": 12080
    },
    {
        "loss": 2.0541,
        "grad_norm": 3.146311044692993,
        "learning_rate": 0.000126622462454976,
        "epoch": 1.6108,
        "step": 12081
    },
    {
        "loss": 2.8838,
        "grad_norm": 2.816331148147583,
        "learning_rate": 0.00012656179176190254,
        "epoch": 1.6109333333333333,
        "step": 12082
    },
    {
        "loss": 1.4301,
        "grad_norm": 4.414549350738525,
        "learning_rate": 0.00012650111054761273,
        "epoch": 1.6110666666666666,
        "step": 12083
    },
    {
        "loss": 2.5995,
        "grad_norm": 2.1423935890197754,
        "learning_rate": 0.0001264404188361429,
        "epoch": 1.6112,
        "step": 12084
    },
    {
        "loss": 2.1042,
        "grad_norm": 3.595064640045166,
        "learning_rate": 0.00012637971665153307,
        "epoch": 1.6113333333333333,
        "step": 12085
    },
    {
        "loss": 3.3853,
        "grad_norm": 6.0791144371032715,
        "learning_rate": 0.00012631900401782745,
        "epoch": 1.6114666666666668,
        "step": 12086
    },
    {
        "loss": 2.2818,
        "grad_norm": 2.827328681945801,
        "learning_rate": 0.00012625828095907477,
        "epoch": 1.6116000000000001,
        "step": 12087
    },
    {
        "loss": 2.0886,
        "grad_norm": 5.814490795135498,
        "learning_rate": 0.00012619754749932754,
        "epoch": 1.6117333333333335,
        "step": 12088
    },
    {
        "loss": 2.7325,
        "grad_norm": 1.9062005281448364,
        "learning_rate": 0.00012613680366264232,
        "epoch": 1.6118666666666668,
        "step": 12089
    },
    {
        "loss": 2.7948,
        "grad_norm": 2.4862630367279053,
        "learning_rate": 0.00012607604947308029,
        "epoch": 1.612,
        "step": 12090
    },
    {
        "loss": 2.4608,
        "grad_norm": 3.359635829925537,
        "learning_rate": 0.0001260152849547062,
        "epoch": 1.6121333333333334,
        "step": 12091
    },
    {
        "loss": 1.2526,
        "grad_norm": 4.184156894683838,
        "learning_rate": 0.00012595451013158897,
        "epoch": 1.6122666666666667,
        "step": 12092
    },
    {
        "loss": 0.6497,
        "grad_norm": 3.5496654510498047,
        "learning_rate": 0.00012589372502780195,
        "epoch": 1.6124,
        "step": 12093
    },
    {
        "loss": 2.3854,
        "grad_norm": 3.0523557662963867,
        "learning_rate": 0.00012583292966742224,
        "epoch": 1.6125333333333334,
        "step": 12094
    },
    {
        "loss": 2.6269,
        "grad_norm": 4.234904766082764,
        "learning_rate": 0.00012577212407453105,
        "epoch": 1.6126666666666667,
        "step": 12095
    },
    {
        "loss": 2.2066,
        "grad_norm": 3.8247740268707275,
        "learning_rate": 0.00012571130827321355,
        "epoch": 1.6128,
        "step": 12096
    },
    {
        "loss": 2.249,
        "grad_norm": 3.3754756450653076,
        "learning_rate": 0.00012565048228755938,
        "epoch": 1.6129333333333333,
        "step": 12097
    },
    {
        "loss": 2.1253,
        "grad_norm": 4.356853008270264,
        "learning_rate": 0.00012558964614166164,
        "epoch": 1.6130666666666666,
        "step": 12098
    },
    {
        "loss": 2.1128,
        "grad_norm": 4.590673923492432,
        "learning_rate": 0.00012552879985961815,
        "epoch": 1.6132,
        "step": 12099
    },
    {
        "loss": 1.7979,
        "grad_norm": 3.3097805976867676,
        "learning_rate": 0.00012546794346552975,
        "epoch": 1.6133333333333333,
        "step": 12100
    },
    {
        "loss": 1.9822,
        "grad_norm": 3.6438000202178955,
        "learning_rate": 0.00012540707698350222,
        "epoch": 1.6134666666666666,
        "step": 12101
    },
    {
        "loss": 2.4201,
        "grad_norm": 3.2426304817199707,
        "learning_rate": 0.000125346200437645,
        "epoch": 1.6136,
        "step": 12102
    },
    {
        "loss": 2.8634,
        "grad_norm": 3.6244876384735107,
        "learning_rate": 0.00012528531385207144,
        "epoch": 1.6137333333333332,
        "step": 12103
    },
    {
        "loss": 1.8811,
        "grad_norm": 4.948237419128418,
        "learning_rate": 0.0001252244172508989,
        "epoch": 1.6138666666666666,
        "step": 12104
    },
    {
        "loss": 2.298,
        "grad_norm": 7.838353157043457,
        "learning_rate": 0.00012516351065824862,
        "epoch": 1.6139999999999999,
        "step": 12105
    },
    {
        "loss": 2.4995,
        "grad_norm": 4.47149658203125,
        "learning_rate": 0.00012510259409824617,
        "epoch": 1.6141333333333332,
        "step": 12106
    },
    {
        "loss": 2.5417,
        "grad_norm": 4.137335300445557,
        "learning_rate": 0.00012504166759502047,
        "epoch": 1.6142666666666665,
        "step": 12107
    },
    {
        "loss": 1.9717,
        "grad_norm": 5.328989028930664,
        "learning_rate": 0.00012498073117270524,
        "epoch": 1.6143999999999998,
        "step": 12108
    },
    {
        "loss": 2.5116,
        "grad_norm": 3.782956123352051,
        "learning_rate": 0.000124919784855437,
        "epoch": 1.6145333333333334,
        "step": 12109
    },
    {
        "loss": 2.5564,
        "grad_norm": 3.5174448490142822,
        "learning_rate": 0.000124858828667357,
        "epoch": 1.6146666666666667,
        "step": 12110
    },
    {
        "loss": 2.7048,
        "grad_norm": 3.057229995727539,
        "learning_rate": 0.00012479786263261048,
        "epoch": 1.6148,
        "step": 12111
    },
    {
        "loss": 2.1999,
        "grad_norm": 3.6623377799987793,
        "learning_rate": 0.00012473688677534602,
        "epoch": 1.6149333333333333,
        "step": 12112
    },
    {
        "loss": 1.2685,
        "grad_norm": Infinity,
        "learning_rate": 0.00012473688677534602,
        "epoch": 1.6150666666666667,
        "step": 12113
    },
    {
        "loss": 2.4621,
        "grad_norm": 4.523600101470947,
        "learning_rate": 0.00012467590111971642,
        "epoch": 1.6152,
        "step": 12114
    },
    {
        "loss": 1.7896,
        "grad_norm": 3.8305647373199463,
        "learning_rate": 0.00012461490568987818,
        "epoch": 1.6153333333333333,
        "step": 12115
    },
    {
        "loss": 0.7214,
        "grad_norm": 3.524385452270508,
        "learning_rate": 0.00012455390050999208,
        "epoch": 1.6154666666666668,
        "step": 12116
    },
    {
        "loss": 2.0726,
        "grad_norm": 4.094615459442139,
        "learning_rate": 0.0001244928856042223,
        "epoch": 1.6156000000000001,
        "step": 12117
    },
    {
        "loss": 1.3803,
        "grad_norm": 3.7882819175720215,
        "learning_rate": 0.00012443186099673708,
        "epoch": 1.6157333333333335,
        "step": 12118
    },
    {
        "loss": 3.0527,
        "grad_norm": 5.405088901519775,
        "learning_rate": 0.00012437082671170834,
        "epoch": 1.6158666666666668,
        "step": 12119
    },
    {
        "loss": 2.3483,
        "grad_norm": 3.8665478229522705,
        "learning_rate": 0.00012430978277331212,
        "epoch": 1.616,
        "step": 12120
    },
    {
        "loss": 0.8268,
        "grad_norm": 3.143824815750122,
        "learning_rate": 0.00012424872920572833,
        "epoch": 1.6161333333333334,
        "step": 12121
    },
    {
        "loss": 1.9754,
        "grad_norm": 2.941101551055908,
        "learning_rate": 0.00012418766603314004,
        "epoch": 1.6162666666666667,
        "step": 12122
    },
    {
        "loss": 2.4277,
        "grad_norm": 3.7814009189605713,
        "learning_rate": 0.00012412659327973492,
        "epoch": 1.6164,
        "step": 12123
    },
    {
        "loss": 2.0491,
        "grad_norm": 4.138319492340088,
        "learning_rate": 0.00012406551096970388,
        "epoch": 1.6165333333333334,
        "step": 12124
    },
    {
        "loss": 2.4515,
        "grad_norm": 2.870229721069336,
        "learning_rate": 0.0001240044191272421,
        "epoch": 1.6166666666666667,
        "step": 12125
    },
    {
        "loss": 2.4235,
        "grad_norm": 4.541138648986816,
        "learning_rate": 0.00012394331777654805,
        "epoch": 1.6168,
        "step": 12126
    },
    {
        "loss": 2.5689,
        "grad_norm": 2.354264497756958,
        "learning_rate": 0.00012388220694182432,
        "epoch": 1.6169333333333333,
        "step": 12127
    },
    {
        "loss": 2.6342,
        "grad_norm": 3.777832269668579,
        "learning_rate": 0.00012382108664727687,
        "epoch": 1.6170666666666667,
        "step": 12128
    },
    {
        "loss": 3.1128,
        "grad_norm": 4.01049280166626,
        "learning_rate": 0.0001237599569171158,
        "epoch": 1.6172,
        "step": 12129
    },
    {
        "loss": 2.2905,
        "grad_norm": 3.748769760131836,
        "learning_rate": 0.00012369881777555514,
        "epoch": 1.6173333333333333,
        "step": 12130
    },
    {
        "loss": 1.8011,
        "grad_norm": 4.823848724365234,
        "learning_rate": 0.00012363766924681176,
        "epoch": 1.6174666666666666,
        "step": 12131
    },
    {
        "loss": 2.5917,
        "grad_norm": 3.466027021408081,
        "learning_rate": 0.00012357651135510715,
        "epoch": 1.6176,
        "step": 12132
    },
    {
        "loss": 2.69,
        "grad_norm": 2.8052659034729004,
        "learning_rate": 0.00012351534412466592,
        "epoch": 1.6177333333333332,
        "step": 12133
    },
    {
        "loss": 1.5535,
        "grad_norm": 3.9024181365966797,
        "learning_rate": 0.0001234541675797169,
        "epoch": 1.6178666666666666,
        "step": 12134
    },
    {
        "loss": 1.4215,
        "grad_norm": 4.558656692504883,
        "learning_rate": 0.00012339298174449218,
        "epoch": 1.6179999999999999,
        "step": 12135
    },
    {
        "loss": 2.4893,
        "grad_norm": 3.2331790924072266,
        "learning_rate": 0.0001233317866432277,
        "epoch": 1.6181333333333332,
        "step": 12136
    },
    {
        "loss": 1.7271,
        "grad_norm": 3.545086145401001,
        "learning_rate": 0.00012327058230016287,
        "epoch": 1.6182666666666665,
        "step": 12137
    },
    {
        "loss": 2.0979,
        "grad_norm": 4.962780475616455,
        "learning_rate": 0.00012320936873954125,
        "epoch": 1.6183999999999998,
        "step": 12138
    },
    {
        "loss": 1.6737,
        "grad_norm": 4.272084712982178,
        "learning_rate": 0.0001231481459856096,
        "epoch": 1.6185333333333334,
        "step": 12139
    },
    {
        "loss": 2.8771,
        "grad_norm": 3.588641881942749,
        "learning_rate": 0.00012308691406261827,
        "epoch": 1.6186666666666667,
        "step": 12140
    },
    {
        "loss": 2.1863,
        "grad_norm": 3.0123472213745117,
        "learning_rate": 0.00012302567299482179,
        "epoch": 1.6188,
        "step": 12141
    },
    {
        "loss": 1.5328,
        "grad_norm": 2.5607059001922607,
        "learning_rate": 0.00012296442280647775,
        "epoch": 1.6189333333333333,
        "step": 12142
    },
    {
        "loss": 1.8695,
        "grad_norm": 3.539435386657715,
        "learning_rate": 0.00012290316352184748,
        "epoch": 1.6190666666666667,
        "step": 12143
    },
    {
        "loss": 0.5049,
        "grad_norm": 2.587898015975952,
        "learning_rate": 0.0001228418951651962,
        "epoch": 1.6192,
        "step": 12144
    },
    {
        "loss": 1.9476,
        "grad_norm": 3.532576084136963,
        "learning_rate": 0.00012278061776079243,
        "epoch": 1.6193333333333333,
        "step": 12145
    },
    {
        "loss": 2.0119,
        "grad_norm": 2.9154884815216064,
        "learning_rate": 0.00012271933133290821,
        "epoch": 1.6194666666666668,
        "step": 12146
    },
    {
        "loss": 1.9921,
        "grad_norm": 4.2707953453063965,
        "learning_rate": 0.00012265803590581957,
        "epoch": 1.6196000000000002,
        "step": 12147
    },
    {
        "loss": 2.9109,
        "grad_norm": 4.7631001472473145,
        "learning_rate": 0.0001225967315038057,
        "epoch": 1.6197333333333335,
        "step": 12148
    },
    {
        "loss": 1.265,
        "grad_norm": 4.666522026062012,
        "learning_rate": 0.00012253541815114937,
        "epoch": 1.6198666666666668,
        "step": 12149
    },
    {
        "loss": 1.8849,
        "grad_norm": 4.283674240112305,
        "learning_rate": 0.00012247409587213724,
        "epoch": 1.62,
        "step": 12150
    },
    {
        "loss": 2.4418,
        "grad_norm": 3.2959110736846924,
        "learning_rate": 0.00012241276469105914,
        "epoch": 1.6201333333333334,
        "step": 12151
    },
    {
        "loss": 1.3089,
        "grad_norm": 4.146801471710205,
        "learning_rate": 0.00012235142463220845,
        "epoch": 1.6202666666666667,
        "step": 12152
    },
    {
        "loss": 2.4852,
        "grad_norm": 4.531195163726807,
        "learning_rate": 0.00012229007571988245,
        "epoch": 1.6204,
        "step": 12153
    },
    {
        "loss": 1.8119,
        "grad_norm": 4.780745029449463,
        "learning_rate": 0.00012222871797838152,
        "epoch": 1.6205333333333334,
        "step": 12154
    },
    {
        "loss": 1.126,
        "grad_norm": 6.594289302825928,
        "learning_rate": 0.00012216735143200962,
        "epoch": 1.6206666666666667,
        "step": 12155
    },
    {
        "loss": 2.2426,
        "grad_norm": 6.186102867126465,
        "learning_rate": 0.00012210597610507414,
        "epoch": 1.6208,
        "step": 12156
    },
    {
        "loss": 2.1158,
        "grad_norm": 5.5868916511535645,
        "learning_rate": 0.00012204459202188639,
        "epoch": 1.6209333333333333,
        "step": 12157
    },
    {
        "loss": 2.3482,
        "grad_norm": 3.8546314239501953,
        "learning_rate": 0.00012198319920676045,
        "epoch": 1.6210666666666667,
        "step": 12158
    },
    {
        "loss": 2.2152,
        "grad_norm": 4.023319244384766,
        "learning_rate": 0.00012192179768401465,
        "epoch": 1.6212,
        "step": 12159
    },
    {
        "loss": 2.1948,
        "grad_norm": 2.50132417678833,
        "learning_rate": 0.00012186038747796988,
        "epoch": 1.6213333333333333,
        "step": 12160
    },
    {
        "loss": 2.1536,
        "grad_norm": 3.422703266143799,
        "learning_rate": 0.0001217989686129511,
        "epoch": 1.6214666666666666,
        "step": 12161
    },
    {
        "loss": 1.95,
        "grad_norm": 4.773182392120361,
        "learning_rate": 0.00012173754111328672,
        "epoch": 1.6216,
        "step": 12162
    },
    {
        "loss": 2.7209,
        "grad_norm": 3.0913138389587402,
        "learning_rate": 0.00012167610500330823,
        "epoch": 1.6217333333333332,
        "step": 12163
    },
    {
        "loss": 1.5764,
        "grad_norm": 4.79524040222168,
        "learning_rate": 0.00012161466030735071,
        "epoch": 1.6218666666666666,
        "step": 12164
    },
    {
        "loss": 2.1687,
        "grad_norm": 3.683713436126709,
        "learning_rate": 0.00012155320704975243,
        "epoch": 1.6219999999999999,
        "step": 12165
    },
    {
        "loss": 1.9951,
        "grad_norm": 2.7483935356140137,
        "learning_rate": 0.00012149174525485556,
        "epoch": 1.6221333333333332,
        "step": 12166
    },
    {
        "loss": 1.9721,
        "grad_norm": 4.133510589599609,
        "learning_rate": 0.000121430274947005,
        "epoch": 1.6222666666666665,
        "step": 12167
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.0407121181488037,
        "learning_rate": 0.00012136879615054982,
        "epoch": 1.6223999999999998,
        "step": 12168
    },
    {
        "loss": 2.2377,
        "grad_norm": 7.273837089538574,
        "learning_rate": 0.00012130730888984137,
        "epoch": 1.6225333333333334,
        "step": 12169
    },
    {
        "loss": 2.8305,
        "grad_norm": 5.0231828689575195,
        "learning_rate": 0.00012124581318923526,
        "epoch": 1.6226666666666667,
        "step": 12170
    },
    {
        "loss": 2.3497,
        "grad_norm": 4.062270641326904,
        "learning_rate": 0.0001211843090730903,
        "epoch": 1.6228,
        "step": 12171
    },
    {
        "loss": 2.0554,
        "grad_norm": 3.825911045074463,
        "learning_rate": 0.00012112279656576835,
        "epoch": 1.6229333333333333,
        "step": 12172
    },
    {
        "loss": 2.3447,
        "grad_norm": 4.150272369384766,
        "learning_rate": 0.00012106127569163468,
        "epoch": 1.6230666666666667,
        "step": 12173
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.6275527477264404,
        "learning_rate": 0.0001209997464750578,
        "epoch": 1.6232,
        "step": 12174
    },
    {
        "loss": 0.7647,
        "grad_norm": 3.129216432571411,
        "learning_rate": 0.00012093820894040996,
        "epoch": 1.6233333333333333,
        "step": 12175
    },
    {
        "loss": 0.7306,
        "grad_norm": 3.3364555835723877,
        "learning_rate": 0.0001208766631120662,
        "epoch": 1.6234666666666666,
        "step": 12176
    },
    {
        "loss": 1.2202,
        "grad_norm": 3.797220230102539,
        "learning_rate": 0.00012081510901440502,
        "epoch": 1.6236000000000002,
        "step": 12177
    },
    {
        "loss": 1.9056,
        "grad_norm": 4.479047775268555,
        "learning_rate": 0.00012075354667180811,
        "epoch": 1.6237333333333335,
        "step": 12178
    },
    {
        "loss": 2.168,
        "grad_norm": 4.091497898101807,
        "learning_rate": 0.00012069197610866066,
        "epoch": 1.6238666666666668,
        "step": 12179
    },
    {
        "loss": 1.9702,
        "grad_norm": 3.479435682296753,
        "learning_rate": 0.00012063039734935125,
        "epoch": 1.624,
        "step": 12180
    },
    {
        "loss": 0.9562,
        "grad_norm": 3.7514681816101074,
        "learning_rate": 0.00012056881041827086,
        "epoch": 1.6241333333333334,
        "step": 12181
    },
    {
        "loss": 1.6352,
        "grad_norm": 4.941655158996582,
        "learning_rate": 0.00012050721533981471,
        "epoch": 1.6242666666666667,
        "step": 12182
    },
    {
        "loss": 2.3438,
        "grad_norm": 3.1063573360443115,
        "learning_rate": 0.00012044561213838054,
        "epoch": 1.6244,
        "step": 12183
    },
    {
        "loss": 1.8254,
        "grad_norm": 4.060374736785889,
        "learning_rate": 0.00012038400083836992,
        "epoch": 1.6245333333333334,
        "step": 12184
    },
    {
        "loss": 2.1676,
        "grad_norm": 4.719984531402588,
        "learning_rate": 0.00012032238146418707,
        "epoch": 1.6246666666666667,
        "step": 12185
    },
    {
        "loss": 2.0975,
        "grad_norm": 3.262812614440918,
        "learning_rate": 0.00012026075404023969,
        "epoch": 1.6248,
        "step": 12186
    },
    {
        "loss": 2.2776,
        "grad_norm": 4.387566566467285,
        "learning_rate": 0.00012019911859093844,
        "epoch": 1.6249333333333333,
        "step": 12187
    },
    {
        "loss": 2.6057,
        "grad_norm": 4.012837886810303,
        "learning_rate": 0.00012013747514069753,
        "epoch": 1.6250666666666667,
        "step": 12188
    },
    {
        "loss": 2.3219,
        "grad_norm": 3.0011212825775146,
        "learning_rate": 0.00012007582371393433,
        "epoch": 1.6252,
        "step": 12189
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.2606375217437744,
        "learning_rate": 0.00012001416433506871,
        "epoch": 1.6253333333333333,
        "step": 12190
    },
    {
        "loss": 2.1849,
        "grad_norm": 4.434166431427002,
        "learning_rate": 0.00011995249702852452,
        "epoch": 1.6254666666666666,
        "step": 12191
    },
    {
        "loss": 2.3373,
        "grad_norm": 4.2136616706848145,
        "learning_rate": 0.00011989082181872815,
        "epoch": 1.6256,
        "step": 12192
    },
    {
        "loss": 2.3928,
        "grad_norm": 3.4843828678131104,
        "learning_rate": 0.00011982913873010959,
        "epoch": 1.6257333333333333,
        "step": 12193
    },
    {
        "loss": 3.1305,
        "grad_norm": 3.197164297103882,
        "learning_rate": 0.00011976744778710162,
        "epoch": 1.6258666666666666,
        "step": 12194
    },
    {
        "loss": 2.4248,
        "grad_norm": 5.267982482910156,
        "learning_rate": 0.00011970574901414024,
        "epoch": 1.626,
        "step": 12195
    },
    {
        "loss": 2.0786,
        "grad_norm": 3.596482515335083,
        "learning_rate": 0.0001196440424356644,
        "epoch": 1.6261333333333332,
        "step": 12196
    },
    {
        "loss": 2.2018,
        "grad_norm": 4.175265312194824,
        "learning_rate": 0.00011958232807611653,
        "epoch": 1.6262666666666665,
        "step": 12197
    },
    {
        "loss": 2.1799,
        "grad_norm": 3.5435025691986084,
        "learning_rate": 0.00011952060595994179,
        "epoch": 1.6263999999999998,
        "step": 12198
    },
    {
        "loss": 1.7456,
        "grad_norm": 4.521811485290527,
        "learning_rate": 0.0001194588761115884,
        "epoch": 1.6265333333333334,
        "step": 12199
    },
    {
        "loss": 0.8629,
        "grad_norm": 3.388092279434204,
        "learning_rate": 0.00011939713855550798,
        "epoch": 1.6266666666666667,
        "step": 12200
    },
    {
        "loss": 2.3287,
        "grad_norm": 3.659667491912842,
        "learning_rate": 0.00011933539331615491,
        "epoch": 1.6268,
        "step": 12201
    },
    {
        "loss": 2.3773,
        "grad_norm": 3.770672559738159,
        "learning_rate": 0.00011927364041798653,
        "epoch": 1.6269333333333333,
        "step": 12202
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.909083366394043,
        "learning_rate": 0.00011921187988546361,
        "epoch": 1.6270666666666667,
        "step": 12203
    },
    {
        "loss": 2.8215,
        "grad_norm": 3.527561902999878,
        "learning_rate": 0.00011915011174304964,
        "epoch": 1.6272,
        "step": 12204
    },
    {
        "loss": 2.6578,
        "grad_norm": 3.915999412536621,
        "learning_rate": 0.00011908833601521099,
        "epoch": 1.6273333333333333,
        "step": 12205
    },
    {
        "loss": 2.3574,
        "grad_norm": 4.414649963378906,
        "learning_rate": 0.00011902655272641757,
        "epoch": 1.6274666666666666,
        "step": 12206
    },
    {
        "loss": 2.3732,
        "grad_norm": 2.378096103668213,
        "learning_rate": 0.00011896476190114177,
        "epoch": 1.6276000000000002,
        "step": 12207
    },
    {
        "loss": 2.9383,
        "grad_norm": 4.049506187438965,
        "learning_rate": 0.000118902963563859,
        "epoch": 1.6277333333333335,
        "step": 12208
    },
    {
        "loss": 1.7231,
        "grad_norm": 3.1618173122406006,
        "learning_rate": 0.00011884115773904811,
        "epoch": 1.6278666666666668,
        "step": 12209
    },
    {
        "loss": 2.398,
        "grad_norm": 3.2717297077178955,
        "learning_rate": 0.00011877934445119045,
        "epoch": 1.6280000000000001,
        "step": 12210
    },
    {
        "loss": 2.4235,
        "grad_norm": 2.9489731788635254,
        "learning_rate": 0.00011871752372477032,
        "epoch": 1.6281333333333334,
        "step": 12211
    },
    {
        "loss": 2.1164,
        "grad_norm": 4.055312156677246,
        "learning_rate": 0.00011865569558427538,
        "epoch": 1.6282666666666668,
        "step": 12212
    },
    {
        "loss": 2.2932,
        "grad_norm": 3.6752707958221436,
        "learning_rate": 0.00011859386005419588,
        "epoch": 1.6284,
        "step": 12213
    },
    {
        "loss": 2.6149,
        "grad_norm": 2.647047281265259,
        "learning_rate": 0.00011853201715902504,
        "epoch": 1.6285333333333334,
        "step": 12214
    },
    {
        "loss": 2.7727,
        "grad_norm": 2.7052316665649414,
        "learning_rate": 0.00011847016692325894,
        "epoch": 1.6286666666666667,
        "step": 12215
    },
    {
        "loss": 1.2028,
        "grad_norm": 4.46705436706543,
        "learning_rate": 0.00011840830937139689,
        "epoch": 1.6288,
        "step": 12216
    },
    {
        "loss": 2.188,
        "grad_norm": 2.453667402267456,
        "learning_rate": 0.00011834644452794066,
        "epoch": 1.6289333333333333,
        "step": 12217
    },
    {
        "loss": 2.4405,
        "grad_norm": 5.075511932373047,
        "learning_rate": 0.0001182845724173955,
        "epoch": 1.6290666666666667,
        "step": 12218
    },
    {
        "loss": 2.1468,
        "grad_norm": 3.1145477294921875,
        "learning_rate": 0.00011822269306426865,
        "epoch": 1.6292,
        "step": 12219
    },
    {
        "loss": 1.6857,
        "grad_norm": 3.040984630584717,
        "learning_rate": 0.00011816080649307096,
        "epoch": 1.6293333333333333,
        "step": 12220
    },
    {
        "loss": 2.9034,
        "grad_norm": 2.706953287124634,
        "learning_rate": 0.00011809891272831608,
        "epoch": 1.6294666666666666,
        "step": 12221
    },
    {
        "loss": 1.7115,
        "grad_norm": 5.676446437835693,
        "learning_rate": 0.00011803701179452023,
        "epoch": 1.6296,
        "step": 12222
    },
    {
        "loss": 2.1553,
        "grad_norm": 5.641708850860596,
        "learning_rate": 0.0001179751037162025,
        "epoch": 1.6297333333333333,
        "step": 12223
    },
    {
        "loss": 2.2576,
        "grad_norm": 4.341843128204346,
        "learning_rate": 0.00011791318851788482,
        "epoch": 1.6298666666666666,
        "step": 12224
    },
    {
        "loss": 2.2453,
        "grad_norm": 5.2722601890563965,
        "learning_rate": 0.00011785126622409228,
        "epoch": 1.63,
        "step": 12225
    },
    {
        "loss": 1.8663,
        "grad_norm": 3.9009766578674316,
        "learning_rate": 0.0001177893368593522,
        "epoch": 1.6301333333333332,
        "step": 12226
    },
    {
        "loss": 2.7822,
        "grad_norm": 4.054510116577148,
        "learning_rate": 0.00011772740044819548,
        "epoch": 1.6302666666666665,
        "step": 12227
    },
    {
        "loss": 2.6639,
        "grad_norm": 3.9416568279266357,
        "learning_rate": 0.00011766545701515474,
        "epoch": 1.6303999999999998,
        "step": 12228
    },
    {
        "loss": 3.1394,
        "grad_norm": 4.739563465118408,
        "learning_rate": 0.00011760350658476629,
        "epoch": 1.6305333333333332,
        "step": 12229
    },
    {
        "loss": 2.6258,
        "grad_norm": 4.749818325042725,
        "learning_rate": 0.00011754154918156903,
        "epoch": 1.6306666666666667,
        "step": 12230
    },
    {
        "loss": 0.745,
        "grad_norm": 4.529694080352783,
        "learning_rate": 0.00011747958483010437,
        "epoch": 1.6308,
        "step": 12231
    },
    {
        "loss": 2.8737,
        "grad_norm": 3.940459966659546,
        "learning_rate": 0.00011741761355491661,
        "epoch": 1.6309333333333333,
        "step": 12232
    },
    {
        "loss": 2.1425,
        "grad_norm": 3.260368585586548,
        "learning_rate": 0.00011735563538055263,
        "epoch": 1.6310666666666667,
        "step": 12233
    },
    {
        "loss": 1.4284,
        "grad_norm": 5.532871723175049,
        "learning_rate": 0.00011729365033156245,
        "epoch": 1.6312,
        "step": 12234
    },
    {
        "loss": 3.0046,
        "grad_norm": 2.8345837593078613,
        "learning_rate": 0.00011723165843249848,
        "epoch": 1.6313333333333333,
        "step": 12235
    },
    {
        "loss": 2.2123,
        "grad_norm": 3.2250006198883057,
        "learning_rate": 0.00011716965970791589,
        "epoch": 1.6314666666666666,
        "step": 12236
    },
    {
        "loss": 2.6184,
        "grad_norm": 2.428717851638794,
        "learning_rate": 0.00011710765418237245,
        "epoch": 1.6316000000000002,
        "step": 12237
    },
    {
        "loss": 1.6685,
        "grad_norm": 3.493133306503296,
        "learning_rate": 0.00011704564188042886,
        "epoch": 1.6317333333333335,
        "step": 12238
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.866436719894409,
        "learning_rate": 0.00011698362282664874,
        "epoch": 1.6318666666666668,
        "step": 12239
    },
    {
        "loss": 2.9618,
        "grad_norm": 4.733504295349121,
        "learning_rate": 0.00011692159704559749,
        "epoch": 1.6320000000000001,
        "step": 12240
    },
    {
        "loss": 2.6459,
        "grad_norm": 4.863287925720215,
        "learning_rate": 0.0001168595645618441,
        "epoch": 1.6321333333333334,
        "step": 12241
    },
    {
        "loss": 2.1015,
        "grad_norm": 3.6052348613739014,
        "learning_rate": 0.00011679752539995961,
        "epoch": 1.6322666666666668,
        "step": 12242
    },
    {
        "loss": 2.2589,
        "grad_norm": 4.094345569610596,
        "learning_rate": 0.0001167354795845182,
        "epoch": 1.6324,
        "step": 12243
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.9463233947753906,
        "learning_rate": 0.00011667342714009632,
        "epoch": 1.6325333333333334,
        "step": 12244
    },
    {
        "loss": 2.6793,
        "grad_norm": 3.3296260833740234,
        "learning_rate": 0.0001166113680912731,
        "epoch": 1.6326666666666667,
        "step": 12245
    },
    {
        "loss": 2.5883,
        "grad_norm": 3.1699652671813965,
        "learning_rate": 0.0001165493024626303,
        "epoch": 1.6328,
        "step": 12246
    },
    {
        "loss": 1.9636,
        "grad_norm": 3.6080658435821533,
        "learning_rate": 0.00011648723027875236,
        "epoch": 1.6329333333333333,
        "step": 12247
    },
    {
        "loss": 0.6544,
        "grad_norm": 2.838724136352539,
        "learning_rate": 0.00011642515156422662,
        "epoch": 1.6330666666666667,
        "step": 12248
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.051598072052002,
        "learning_rate": 0.00011636306634364208,
        "epoch": 1.6332,
        "step": 12249
    },
    {
        "loss": 2.0044,
        "grad_norm": 2.8323585987091064,
        "learning_rate": 0.00011630097464159137,
        "epoch": 1.6333333333333333,
        "step": 12250
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.9109444618225098,
        "learning_rate": 0.00011623887648266894,
        "epoch": 1.6334666666666666,
        "step": 12251
    },
    {
        "loss": 0.8723,
        "grad_norm": 3.3112871646881104,
        "learning_rate": 0.0001161767718914723,
        "epoch": 1.6336,
        "step": 12252
    },
    {
        "loss": 2.7455,
        "grad_norm": 2.812973737716675,
        "learning_rate": 0.00011611466089260122,
        "epoch": 1.6337333333333333,
        "step": 12253
    },
    {
        "loss": 2.4818,
        "grad_norm": 3.000981092453003,
        "learning_rate": 0.00011605254351065802,
        "epoch": 1.6338666666666666,
        "step": 12254
    },
    {
        "loss": 2.4862,
        "grad_norm": 5.463365077972412,
        "learning_rate": 0.00011599041977024751,
        "epoch": 1.634,
        "step": 12255
    },
    {
        "loss": 2.6512,
        "grad_norm": 3.0669548511505127,
        "learning_rate": 0.0001159282896959774,
        "epoch": 1.6341333333333332,
        "step": 12256
    },
    {
        "loss": 1.9636,
        "grad_norm": 3.9884748458862305,
        "learning_rate": 0.00011586615331245747,
        "epoch": 1.6342666666666665,
        "step": 12257
    },
    {
        "loss": 1.9517,
        "grad_norm": 4.992698669433594,
        "learning_rate": 0.0001158040106443,
        "epoch": 1.6343999999999999,
        "step": 12258
    },
    {
        "loss": 2.0409,
        "grad_norm": 3.4824600219726562,
        "learning_rate": 0.00011574186171612019,
        "epoch": 1.6345333333333332,
        "step": 12259
    },
    {
        "loss": 2.294,
        "grad_norm": 5.64583158493042,
        "learning_rate": 0.00011567970655253533,
        "epoch": 1.6346666666666667,
        "step": 12260
    },
    {
        "loss": 2.7255,
        "grad_norm": 4.289572715759277,
        "learning_rate": 0.00011561754517816515,
        "epoch": 1.6348,
        "step": 12261
    },
    {
        "loss": 2.4671,
        "grad_norm": 2.9892213344573975,
        "learning_rate": 0.00011555537761763222,
        "epoch": 1.6349333333333333,
        "step": 12262
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.943079948425293,
        "learning_rate": 0.0001154932038955612,
        "epoch": 1.6350666666666667,
        "step": 12263
    },
    {
        "loss": 2.5746,
        "grad_norm": 3.213449716567993,
        "learning_rate": 0.00011543102403657935,
        "epoch": 1.6352,
        "step": 12264
    },
    {
        "loss": 1.8351,
        "grad_norm": 4.87742280960083,
        "learning_rate": 0.00011536883806531617,
        "epoch": 1.6353333333333333,
        "step": 12265
    },
    {
        "loss": 2.2142,
        "grad_norm": 4.054487705230713,
        "learning_rate": 0.00011530664600640402,
        "epoch": 1.6354666666666666,
        "step": 12266
    },
    {
        "loss": 1.8053,
        "grad_norm": 6.418949604034424,
        "learning_rate": 0.00011524444788447703,
        "epoch": 1.6356000000000002,
        "step": 12267
    },
    {
        "loss": 2.2812,
        "grad_norm": 3.0428152084350586,
        "learning_rate": 0.00011518224372417247,
        "epoch": 1.6357333333333335,
        "step": 12268
    },
    {
        "loss": 1.7652,
        "grad_norm": 3.5966615676879883,
        "learning_rate": 0.00011512003355012944,
        "epoch": 1.6358666666666668,
        "step": 12269
    },
    {
        "loss": 2.5337,
        "grad_norm": 2.8073768615722656,
        "learning_rate": 0.00011505781738698944,
        "epoch": 1.6360000000000001,
        "step": 12270
    },
    {
        "loss": 2.0523,
        "grad_norm": 5.481007099151611,
        "learning_rate": 0.00011499559525939682,
        "epoch": 1.6361333333333334,
        "step": 12271
    },
    {
        "loss": 2.5522,
        "grad_norm": 3.0578715801239014,
        "learning_rate": 0.00011493336719199783,
        "epoch": 1.6362666666666668,
        "step": 12272
    },
    {
        "loss": 2.2375,
        "grad_norm": 3.9568545818328857,
        "learning_rate": 0.0001148711332094412,
        "epoch": 1.6364,
        "step": 12273
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.218519687652588,
        "learning_rate": 0.00011480889333637787,
        "epoch": 1.6365333333333334,
        "step": 12274
    },
    {
        "loss": 2.092,
        "grad_norm": 3.6683197021484375,
        "learning_rate": 0.0001147466475974616,
        "epoch": 1.6366666666666667,
        "step": 12275
    },
    {
        "loss": 1.8345,
        "grad_norm": 3.277146816253662,
        "learning_rate": 0.00011468439601734784,
        "epoch": 1.6368,
        "step": 12276
    },
    {
        "loss": 2.3387,
        "grad_norm": 2.915117025375366,
        "learning_rate": 0.00011462213862069504,
        "epoch": 1.6369333333333334,
        "step": 12277
    },
    {
        "loss": 2.2512,
        "grad_norm": 3.2052927017211914,
        "learning_rate": 0.00011455987543216303,
        "epoch": 1.6370666666666667,
        "step": 12278
    },
    {
        "loss": 2.267,
        "grad_norm": 3.808350086212158,
        "learning_rate": 0.00011449760647641474,
        "epoch": 1.6372,
        "step": 12279
    },
    {
        "loss": 1.7746,
        "grad_norm": 5.636516094207764,
        "learning_rate": 0.0001144353317781153,
        "epoch": 1.6373333333333333,
        "step": 12280
    },
    {
        "loss": 2.0896,
        "grad_norm": 4.20826530456543,
        "learning_rate": 0.0001143730513619317,
        "epoch": 1.6374666666666666,
        "step": 12281
    },
    {
        "loss": 1.5706,
        "grad_norm": 4.399662971496582,
        "learning_rate": 0.00011431076525253352,
        "epoch": 1.6376,
        "step": 12282
    },
    {
        "loss": 2.0786,
        "grad_norm": 3.861121892929077,
        "learning_rate": 0.00011424847347459233,
        "epoch": 1.6377333333333333,
        "step": 12283
    },
    {
        "loss": 2.7378,
        "grad_norm": 3.3532354831695557,
        "learning_rate": 0.00011418617605278237,
        "epoch": 1.6378666666666666,
        "step": 12284
    },
    {
        "loss": 2.3664,
        "grad_norm": 4.098688125610352,
        "learning_rate": 0.00011412387301177972,
        "epoch": 1.638,
        "step": 12285
    },
    {
        "loss": 1.4604,
        "grad_norm": 3.3414525985717773,
        "learning_rate": 0.00011406156437626285,
        "epoch": 1.6381333333333332,
        "step": 12286
    },
    {
        "loss": 2.3799,
        "grad_norm": 3.061990737915039,
        "learning_rate": 0.00011399925017091231,
        "epoch": 1.6382666666666665,
        "step": 12287
    },
    {
        "loss": 2.1382,
        "grad_norm": 3.62661075592041,
        "learning_rate": 0.00011393693042041102,
        "epoch": 1.6383999999999999,
        "step": 12288
    },
    {
        "loss": 2.0943,
        "grad_norm": 3.5461618900299072,
        "learning_rate": 0.0001138746051494443,
        "epoch": 1.6385333333333332,
        "step": 12289
    },
    {
        "loss": 2.5054,
        "grad_norm": 3.3025548458099365,
        "learning_rate": 0.00011381227438269917,
        "epoch": 1.6386666666666667,
        "step": 12290
    },
    {
        "loss": 1.1999,
        "grad_norm": 4.303236484527588,
        "learning_rate": 0.00011374993814486512,
        "epoch": 1.6388,
        "step": 12291
    },
    {
        "loss": 2.4854,
        "grad_norm": 4.0636186599731445,
        "learning_rate": 0.00011368759646063354,
        "epoch": 1.6389333333333334,
        "step": 12292
    },
    {
        "loss": 2.2402,
        "grad_norm": 3.2777762413024902,
        "learning_rate": 0.00011362524935469853,
        "epoch": 1.6390666666666667,
        "step": 12293
    },
    {
        "loss": 2.8858,
        "grad_norm": 4.254462242126465,
        "learning_rate": 0.00011356289685175583,
        "epoch": 1.6392,
        "step": 12294
    },
    {
        "loss": 2.0515,
        "grad_norm": 3.588566780090332,
        "learning_rate": 0.00011350053897650345,
        "epoch": 1.6393333333333333,
        "step": 12295
    },
    {
        "loss": 2.4504,
        "grad_norm": 4.242848873138428,
        "learning_rate": 0.00011343817575364144,
        "epoch": 1.6394666666666666,
        "step": 12296
    },
    {
        "loss": 2.2783,
        "grad_norm": 4.494828701019287,
        "learning_rate": 0.00011337580720787224,
        "epoch": 1.6396,
        "step": 12297
    },
    {
        "loss": 2.8272,
        "grad_norm": 3.6493141651153564,
        "learning_rate": 0.0001133134333639005,
        "epoch": 1.6397333333333335,
        "step": 12298
    },
    {
        "loss": 1.6969,
        "grad_norm": 5.177254676818848,
        "learning_rate": 0.00011325105424643217,
        "epoch": 1.6398666666666668,
        "step": 12299
    },
    {
        "loss": 2.8723,
        "grad_norm": 4.256132125854492,
        "learning_rate": 0.00011318866988017622,
        "epoch": 1.6400000000000001,
        "step": 12300
    },
    {
        "loss": 2.2314,
        "grad_norm": 3.7822229862213135,
        "learning_rate": 0.0001131262802898431,
        "epoch": 1.6401333333333334,
        "step": 12301
    },
    {
        "loss": 1.7703,
        "grad_norm": 3.4768435955047607,
        "learning_rate": 0.0001130638855001457,
        "epoch": 1.6402666666666668,
        "step": 12302
    },
    {
        "loss": 1.9142,
        "grad_norm": 3.5090582370758057,
        "learning_rate": 0.00011300148553579877,
        "epoch": 1.6404,
        "step": 12303
    },
    {
        "loss": 2.8335,
        "grad_norm": 4.579794883728027,
        "learning_rate": 0.00011293908042151913,
        "epoch": 1.6405333333333334,
        "step": 12304
    },
    {
        "loss": 1.9211,
        "grad_norm": 7.704380035400391,
        "learning_rate": 0.0001128766701820255,
        "epoch": 1.6406666666666667,
        "step": 12305
    },
    {
        "loss": 1.2571,
        "grad_norm": 4.013319492340088,
        "learning_rate": 0.00011281425484203893,
        "epoch": 1.6408,
        "step": 12306
    },
    {
        "loss": 2.2411,
        "grad_norm": 4.782970428466797,
        "learning_rate": 0.00011275183442628263,
        "epoch": 1.6409333333333334,
        "step": 12307
    },
    {
        "loss": 1.3452,
        "grad_norm": 2.62929105758667,
        "learning_rate": 0.00011268940895948099,
        "epoch": 1.6410666666666667,
        "step": 12308
    },
    {
        "loss": 2.4696,
        "grad_norm": 1.9065401554107666,
        "learning_rate": 0.00011262697846636139,
        "epoch": 1.6412,
        "step": 12309
    },
    {
        "loss": 2.656,
        "grad_norm": 3.5364458560943604,
        "learning_rate": 0.0001125645429716524,
        "epoch": 1.6413333333333333,
        "step": 12310
    },
    {
        "loss": 1.8784,
        "grad_norm": 3.6823084354400635,
        "learning_rate": 0.0001125021025000853,
        "epoch": 1.6414666666666666,
        "step": 12311
    },
    {
        "loss": 1.6237,
        "grad_norm": 4.749464988708496,
        "learning_rate": 0.00011243965707639279,
        "epoch": 1.6416,
        "step": 12312
    },
    {
        "loss": 2.1687,
        "grad_norm": 4.0907206535339355,
        "learning_rate": 0.00011237720672530973,
        "epoch": 1.6417333333333333,
        "step": 12313
    },
    {
        "loss": 3.4454,
        "grad_norm": 3.066019058227539,
        "learning_rate": 0.00011231475147157275,
        "epoch": 1.6418666666666666,
        "step": 12314
    },
    {
        "loss": 1.7109,
        "grad_norm": 3.7176873683929443,
        "learning_rate": 0.00011225229133992092,
        "epoch": 1.642,
        "step": 12315
    },
    {
        "loss": 2.7802,
        "grad_norm": 2.9402623176574707,
        "learning_rate": 0.00011218982635509474,
        "epoch": 1.6421333333333332,
        "step": 12316
    },
    {
        "loss": 0.8826,
        "grad_norm": 3.915195941925049,
        "learning_rate": 0.00011212735654183664,
        "epoch": 1.6422666666666665,
        "step": 12317
    },
    {
        "loss": 2.4485,
        "grad_norm": 3.105766534805298,
        "learning_rate": 0.00011206488192489146,
        "epoch": 1.6423999999999999,
        "step": 12318
    },
    {
        "loss": 1.6413,
        "grad_norm": 1.9690892696380615,
        "learning_rate": 0.00011200240252900542,
        "epoch": 1.6425333333333332,
        "step": 12319
    },
    {
        "loss": 2.363,
        "grad_norm": 4.522369384765625,
        "learning_rate": 0.0001119399183789267,
        "epoch": 1.6426666666666667,
        "step": 12320
    },
    {
        "loss": 2.5078,
        "grad_norm": 3.672868490219116,
        "learning_rate": 0.00011187742949940578,
        "epoch": 1.6428,
        "step": 12321
    },
    {
        "loss": 2.1523,
        "grad_norm": 3.536619186401367,
        "learning_rate": 0.00011181493591519456,
        "epoch": 1.6429333333333334,
        "step": 12322
    },
    {
        "loss": 1.7874,
        "grad_norm": 4.376106262207031,
        "learning_rate": 0.00011175243765104696,
        "epoch": 1.6430666666666667,
        "step": 12323
    },
    {
        "loss": 2.574,
        "grad_norm": 2.7602577209472656,
        "learning_rate": 0.00011168993473171863,
        "epoch": 1.6432,
        "step": 12324
    },
    {
        "loss": 1.6534,
        "grad_norm": 3.9958250522613525,
        "learning_rate": 0.00011162742718196746,
        "epoch": 1.6433333333333333,
        "step": 12325
    },
    {
        "loss": 2.07,
        "grad_norm": 3.79152250289917,
        "learning_rate": 0.00011156491502655262,
        "epoch": 1.6434666666666666,
        "step": 12326
    },
    {
        "loss": 1.9727,
        "grad_norm": 3.901230573654175,
        "learning_rate": 0.00011150239829023568,
        "epoch": 1.6436,
        "step": 12327
    },
    {
        "loss": 1.9995,
        "grad_norm": 5.916813373565674,
        "learning_rate": 0.00011143987699777956,
        "epoch": 1.6437333333333335,
        "step": 12328
    },
    {
        "loss": 2.1461,
        "grad_norm": 4.023244380950928,
        "learning_rate": 0.00011137735117394905,
        "epoch": 1.6438666666666668,
        "step": 12329
    },
    {
        "loss": 2.3435,
        "grad_norm": 3.194744825363159,
        "learning_rate": 0.00011131482084351112,
        "epoch": 1.6440000000000001,
        "step": 12330
    },
    {
        "loss": 2.1594,
        "grad_norm": 4.03235387802124,
        "learning_rate": 0.0001112522860312341,
        "epoch": 1.6441333333333334,
        "step": 12331
    },
    {
        "loss": 2.3207,
        "grad_norm": 4.019509792327881,
        "learning_rate": 0.00011118974676188823,
        "epoch": 1.6442666666666668,
        "step": 12332
    },
    {
        "loss": 2.3826,
        "grad_norm": 2.6383554935455322,
        "learning_rate": 0.00011112720306024543,
        "epoch": 1.6444,
        "step": 12333
    },
    {
        "loss": 1.4632,
        "grad_norm": 5.852569103240967,
        "learning_rate": 0.00011106465495107969,
        "epoch": 1.6445333333333334,
        "step": 12334
    },
    {
        "loss": 1.7884,
        "grad_norm": 4.03010368347168,
        "learning_rate": 0.00011100210245916627,
        "epoch": 1.6446666666666667,
        "step": 12335
    },
    {
        "loss": 2.6356,
        "grad_norm": 3.4901318550109863,
        "learning_rate": 0.00011093954560928288,
        "epoch": 1.6448,
        "step": 12336
    },
    {
        "loss": 1.4112,
        "grad_norm": 3.5947279930114746,
        "learning_rate": 0.00011087698442620794,
        "epoch": 1.6449333333333334,
        "step": 12337
    },
    {
        "loss": 2.1111,
        "grad_norm": 3.6642470359802246,
        "learning_rate": 0.00011081441893472238,
        "epoch": 1.6450666666666667,
        "step": 12338
    },
    {
        "loss": 2.0241,
        "grad_norm": 4.303849220275879,
        "learning_rate": 0.00011075184915960877,
        "epoch": 1.6452,
        "step": 12339
    },
    {
        "loss": 2.0535,
        "grad_norm": 4.64542818069458,
        "learning_rate": 0.00011068927512565109,
        "epoch": 1.6453333333333333,
        "step": 12340
    },
    {
        "loss": 1.9656,
        "grad_norm": 2.9548916816711426,
        "learning_rate": 0.0001106266968576351,
        "epoch": 1.6454666666666666,
        "step": 12341
    },
    {
        "loss": 2.2575,
        "grad_norm": 2.913076162338257,
        "learning_rate": 0.00011056411438034811,
        "epoch": 1.6456,
        "step": 12342
    },
    {
        "loss": 1.6154,
        "grad_norm": 3.548372268676758,
        "learning_rate": 0.00011050152771857957,
        "epoch": 1.6457333333333333,
        "step": 12343
    },
    {
        "loss": 1.72,
        "grad_norm": 3.550908327102661,
        "learning_rate": 0.00011043893689712015,
        "epoch": 1.6458666666666666,
        "step": 12344
    },
    {
        "loss": 1.459,
        "grad_norm": 3.6937077045440674,
        "learning_rate": 0.00011037634194076223,
        "epoch": 1.646,
        "step": 12345
    },
    {
        "loss": 1.3391,
        "grad_norm": 7.172516822814941,
        "learning_rate": 0.00011031374287429977,
        "epoch": 1.6461333333333332,
        "step": 12346
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.93324875831604,
        "learning_rate": 0.00011025113972252863,
        "epoch": 1.6462666666666665,
        "step": 12347
    },
    {
        "loss": 2.1724,
        "grad_norm": 3.8928818702697754,
        "learning_rate": 0.00011018853251024624,
        "epoch": 1.6463999999999999,
        "step": 12348
    },
    {
        "loss": 2.3607,
        "grad_norm": 2.7609050273895264,
        "learning_rate": 0.00011012592126225139,
        "epoch": 1.6465333333333332,
        "step": 12349
    },
    {
        "loss": 2.5294,
        "grad_norm": 2.8182413578033447,
        "learning_rate": 0.00011006330600334465,
        "epoch": 1.6466666666666665,
        "step": 12350
    },
    {
        "loss": 2.0174,
        "grad_norm": 4.68991756439209,
        "learning_rate": 0.00011000068675832797,
        "epoch": 1.6468,
        "step": 12351
    },
    {
        "loss": 0.9738,
        "grad_norm": 4.382431507110596,
        "learning_rate": 0.00010993806355200534,
        "epoch": 1.6469333333333334,
        "step": 12352
    },
    {
        "loss": 2.753,
        "grad_norm": 2.5484883785247803,
        "learning_rate": 0.00010987543640918184,
        "epoch": 1.6470666666666667,
        "step": 12353
    },
    {
        "loss": 1.7275,
        "grad_norm": 4.893653869628906,
        "learning_rate": 0.00010981280535466434,
        "epoch": 1.6472,
        "step": 12354
    },
    {
        "loss": 2.1189,
        "grad_norm": 5.141679763793945,
        "learning_rate": 0.00010975017041326108,
        "epoch": 1.6473333333333333,
        "step": 12355
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.8708629608154297,
        "learning_rate": 0.0001096875316097821,
        "epoch": 1.6474666666666666,
        "step": 12356
    },
    {
        "loss": 1.1413,
        "grad_norm": 4.548789978027344,
        "learning_rate": 0.00010962488896903908,
        "epoch": 1.6476,
        "step": 12357
    },
    {
        "loss": 1.4773,
        "grad_norm": 3.088263750076294,
        "learning_rate": 0.0001095622425158445,
        "epoch": 1.6477333333333335,
        "step": 12358
    },
    {
        "loss": 1.601,
        "grad_norm": 4.2418293952941895,
        "learning_rate": 0.0001094995922750132,
        "epoch": 1.6478666666666668,
        "step": 12359
    },
    {
        "loss": 2.6208,
        "grad_norm": 4.420146465301514,
        "learning_rate": 0.0001094369382713609,
        "epoch": 1.6480000000000001,
        "step": 12360
    },
    {
        "loss": 1.2499,
        "grad_norm": 4.726689338684082,
        "learning_rate": 0.00010937428052970533,
        "epoch": 1.6481333333333335,
        "step": 12361
    },
    {
        "loss": 1.8581,
        "grad_norm": 3.155306577682495,
        "learning_rate": 0.00010931161907486532,
        "epoch": 1.6482666666666668,
        "step": 12362
    },
    {
        "loss": 2.9544,
        "grad_norm": 2.484171152114868,
        "learning_rate": 0.00010924895393166125,
        "epoch": 1.6484,
        "step": 12363
    },
    {
        "loss": 1.8031,
        "grad_norm": 4.39971923828125,
        "learning_rate": 0.00010918628512491487,
        "epoch": 1.6485333333333334,
        "step": 12364
    },
    {
        "loss": 3.0816,
        "grad_norm": 3.0614192485809326,
        "learning_rate": 0.0001091236126794498,
        "epoch": 1.6486666666666667,
        "step": 12365
    },
    {
        "loss": 2.2751,
        "grad_norm": 3.3713860511779785,
        "learning_rate": 0.00010906093662009065,
        "epoch": 1.6488,
        "step": 12366
    },
    {
        "loss": 1.543,
        "grad_norm": 4.326511383056641,
        "learning_rate": 0.00010899825697166349,
        "epoch": 1.6489333333333334,
        "step": 12367
    },
    {
        "loss": 1.3923,
        "grad_norm": 3.867516279220581,
        "learning_rate": 0.0001089355737589962,
        "epoch": 1.6490666666666667,
        "step": 12368
    },
    {
        "loss": 2.164,
        "grad_norm": 3.3245933055877686,
        "learning_rate": 0.0001088728870069176,
        "epoch": 1.6492,
        "step": 12369
    },
    {
        "loss": 0.9809,
        "grad_norm": 4.19666051864624,
        "learning_rate": 0.00010881019674025836,
        "epoch": 1.6493333333333333,
        "step": 12370
    },
    {
        "loss": 2.0708,
        "grad_norm": 3.0896763801574707,
        "learning_rate": 0.00010874750298385012,
        "epoch": 1.6494666666666666,
        "step": 12371
    },
    {
        "loss": 2.135,
        "grad_norm": 4.579736232757568,
        "learning_rate": 0.0001086848057625262,
        "epoch": 1.6496,
        "step": 12372
    },
    {
        "loss": 1.2666,
        "grad_norm": 4.197947025299072,
        "learning_rate": 0.00010862210510112097,
        "epoch": 1.6497333333333333,
        "step": 12373
    },
    {
        "loss": 2.6474,
        "grad_norm": 3.5105631351470947,
        "learning_rate": 0.00010855940102447066,
        "epoch": 1.6498666666666666,
        "step": 12374
    },
    {
        "loss": 3.0419,
        "grad_norm": 3.069524049758911,
        "learning_rate": 0.00010849669355741243,
        "epoch": 1.65,
        "step": 12375
    },
    {
        "loss": 2.1974,
        "grad_norm": 3.6463942527770996,
        "learning_rate": 0.0001084339827247848,
        "epoch": 1.6501333333333332,
        "step": 12376
    },
    {
        "loss": 2.6715,
        "grad_norm": 3.584419012069702,
        "learning_rate": 0.00010837126855142803,
        "epoch": 1.6502666666666665,
        "step": 12377
    },
    {
        "loss": 2.872,
        "grad_norm": 4.304049491882324,
        "learning_rate": 0.00010830855106218325,
        "epoch": 1.6503999999999999,
        "step": 12378
    },
    {
        "loss": 2.6096,
        "grad_norm": 3.643871545791626,
        "learning_rate": 0.00010824583028189293,
        "epoch": 1.6505333333333332,
        "step": 12379
    },
    {
        "loss": 2.2309,
        "grad_norm": 3.5008838176727295,
        "learning_rate": 0.00010818310623540129,
        "epoch": 1.6506666666666665,
        "step": 12380
    },
    {
        "loss": 2.3129,
        "grad_norm": 3.2951271533966064,
        "learning_rate": 0.0001081203789475534,
        "epoch": 1.6508,
        "step": 12381
    },
    {
        "loss": 2.5423,
        "grad_norm": 3.4957079887390137,
        "learning_rate": 0.00010805764844319573,
        "epoch": 1.6509333333333334,
        "step": 12382
    },
    {
        "loss": 2.1147,
        "grad_norm": 4.901580333709717,
        "learning_rate": 0.00010799491474717588,
        "epoch": 1.6510666666666667,
        "step": 12383
    },
    {
        "loss": 1.3582,
        "grad_norm": 2.6027839183807373,
        "learning_rate": 0.00010793217788434324,
        "epoch": 1.6512,
        "step": 12384
    },
    {
        "loss": 2.7959,
        "grad_norm": 2.9245548248291016,
        "learning_rate": 0.00010786943787954775,
        "epoch": 1.6513333333333333,
        "step": 12385
    },
    {
        "loss": 2.4238,
        "grad_norm": 4.116555213928223,
        "learning_rate": 0.00010780669475764127,
        "epoch": 1.6514666666666666,
        "step": 12386
    },
    {
        "loss": 1.9626,
        "grad_norm": 3.24228835105896,
        "learning_rate": 0.00010774394854347638,
        "epoch": 1.6516,
        "step": 12387
    },
    {
        "loss": 1.1721,
        "grad_norm": 4.446828365325928,
        "learning_rate": 0.00010768119926190696,
        "epoch": 1.6517333333333335,
        "step": 12388
    },
    {
        "loss": 1.4152,
        "grad_norm": 4.7065510749816895,
        "learning_rate": 0.00010761844693778844,
        "epoch": 1.6518666666666668,
        "step": 12389
    },
    {
        "loss": 1.596,
        "grad_norm": 3.0911736488342285,
        "learning_rate": 0.00010755569159597717,
        "epoch": 1.6520000000000001,
        "step": 12390
    },
    {
        "loss": 2.3762,
        "grad_norm": 4.082271575927734,
        "learning_rate": 0.00010749293326133068,
        "epoch": 1.6521333333333335,
        "step": 12391
    },
    {
        "loss": 2.0935,
        "grad_norm": 3.147740364074707,
        "learning_rate": 0.00010743017195870768,
        "epoch": 1.6522666666666668,
        "step": 12392
    },
    {
        "loss": 2.4921,
        "grad_norm": 2.8875772953033447,
        "learning_rate": 0.00010736740771296836,
        "epoch": 1.6524,
        "step": 12393
    },
    {
        "loss": 2.3283,
        "grad_norm": 3.870030164718628,
        "learning_rate": 0.00010730464054897362,
        "epoch": 1.6525333333333334,
        "step": 12394
    },
    {
        "loss": 2.3696,
        "grad_norm": 3.4436211585998535,
        "learning_rate": 0.00010724187049158616,
        "epoch": 1.6526666666666667,
        "step": 12395
    },
    {
        "loss": 2.2601,
        "grad_norm": 3.6503548622131348,
        "learning_rate": 0.00010717909756566884,
        "epoch": 1.6528,
        "step": 12396
    },
    {
        "loss": 3.0452,
        "grad_norm": 3.065391778945923,
        "learning_rate": 0.00010711632179608648,
        "epoch": 1.6529333333333334,
        "step": 12397
    },
    {
        "loss": 2.3049,
        "grad_norm": 4.347115516662598,
        "learning_rate": 0.00010705354320770498,
        "epoch": 1.6530666666666667,
        "step": 12398
    },
    {
        "loss": 2.0832,
        "grad_norm": 4.0319037437438965,
        "learning_rate": 0.00010699076182539098,
        "epoch": 1.6532,
        "step": 12399
    },
    {
        "loss": 1.9708,
        "grad_norm": 3.5979998111724854,
        "learning_rate": 0.00010692797767401237,
        "epoch": 1.6533333333333333,
        "step": 12400
    },
    {
        "loss": 2.0566,
        "grad_norm": 3.4307594299316406,
        "learning_rate": 0.00010686519077843808,
        "epoch": 1.6534666666666666,
        "step": 12401
    },
    {
        "loss": 1.752,
        "grad_norm": 3.185243606567383,
        "learning_rate": 0.00010680240116353848,
        "epoch": 1.6536,
        "step": 12402
    },
    {
        "loss": 2.4838,
        "grad_norm": 2.3869755268096924,
        "learning_rate": 0.0001067396088541846,
        "epoch": 1.6537333333333333,
        "step": 12403
    },
    {
        "loss": 2.6202,
        "grad_norm": 3.2182719707489014,
        "learning_rate": 0.00010667681387524866,
        "epoch": 1.6538666666666666,
        "step": 12404
    },
    {
        "loss": 2.6179,
        "grad_norm": 4.2433977127075195,
        "learning_rate": 0.00010661401625160388,
        "epoch": 1.654,
        "step": 12405
    },
    {
        "loss": 0.9491,
        "grad_norm": 4.5028228759765625,
        "learning_rate": 0.00010655121600812472,
        "epoch": 1.6541333333333332,
        "step": 12406
    },
    {
        "loss": 2.5299,
        "grad_norm": 3.2825417518615723,
        "learning_rate": 0.0001064884131696867,
        "epoch": 1.6542666666666666,
        "step": 12407
    },
    {
        "loss": 1.4504,
        "grad_norm": 3.0489516258239746,
        "learning_rate": 0.00010642560776116613,
        "epoch": 1.6543999999999999,
        "step": 12408
    },
    {
        "loss": 2.3921,
        "grad_norm": 3.7555696964263916,
        "learning_rate": 0.00010636279980744043,
        "epoch": 1.6545333333333332,
        "step": 12409
    },
    {
        "loss": 1.8298,
        "grad_norm": 5.774641036987305,
        "learning_rate": 0.0001062999893333879,
        "epoch": 1.6546666666666665,
        "step": 12410
    },
    {
        "loss": 1.6219,
        "grad_norm": 3.5419344902038574,
        "learning_rate": 0.00010623717636388828,
        "epoch": 1.6548,
        "step": 12411
    },
    {
        "loss": 2.2539,
        "grad_norm": 3.643524646759033,
        "learning_rate": 0.00010617436092382185,
        "epoch": 1.6549333333333334,
        "step": 12412
    },
    {
        "loss": 2.4837,
        "grad_norm": 3.0376060009002686,
        "learning_rate": 0.00010611154303807001,
        "epoch": 1.6550666666666667,
        "step": 12413
    },
    {
        "loss": 2.6664,
        "grad_norm": 3.8737988471984863,
        "learning_rate": 0.000106048722731515,
        "epoch": 1.6552,
        "step": 12414
    },
    {
        "loss": 2.5746,
        "grad_norm": 3.299905776977539,
        "learning_rate": 0.00010598590002904032,
        "epoch": 1.6553333333333333,
        "step": 12415
    },
    {
        "loss": 1.9806,
        "grad_norm": 4.588198184967041,
        "learning_rate": 0.0001059230749555305,
        "epoch": 1.6554666666666666,
        "step": 12416
    },
    {
        "loss": 2.289,
        "grad_norm": 3.990274667739868,
        "learning_rate": 0.00010586024753587027,
        "epoch": 1.6556,
        "step": 12417
    },
    {
        "loss": 1.7199,
        "grad_norm": 3.116039752960205,
        "learning_rate": 0.00010579741779494612,
        "epoch": 1.6557333333333333,
        "step": 12418
    },
    {
        "loss": 1.9398,
        "grad_norm": 3.9965596199035645,
        "learning_rate": 0.00010573458575764492,
        "epoch": 1.6558666666666668,
        "step": 12419
    },
    {
        "loss": 1.9253,
        "grad_norm": 7.453904628753662,
        "learning_rate": 0.00010567175144885488,
        "epoch": 1.6560000000000001,
        "step": 12420
    },
    {
        "loss": 2.7815,
        "grad_norm": 3.1529922485351562,
        "learning_rate": 0.00010560891489346479,
        "epoch": 1.6561333333333335,
        "step": 12421
    },
    {
        "loss": 2.2575,
        "grad_norm": 5.009059906005859,
        "learning_rate": 0.00010554607611636436,
        "epoch": 1.6562666666666668,
        "step": 12422
    },
    {
        "loss": 1.0752,
        "grad_norm": 4.556021690368652,
        "learning_rate": 0.00010548323514244418,
        "epoch": 1.6564,
        "step": 12423
    },
    {
        "loss": 1.3109,
        "grad_norm": 4.446408748626709,
        "learning_rate": 0.000105420391996596,
        "epoch": 1.6565333333333334,
        "step": 12424
    },
    {
        "loss": 1.2384,
        "grad_norm": 3.746994733810425,
        "learning_rate": 0.00010535754670371207,
        "epoch": 1.6566666666666667,
        "step": 12425
    },
    {
        "loss": 2.8244,
        "grad_norm": 4.8373212814331055,
        "learning_rate": 0.00010529469928868552,
        "epoch": 1.6568,
        "step": 12426
    },
    {
        "loss": 2.1651,
        "grad_norm": 3.3964152336120605,
        "learning_rate": 0.00010523184977641065,
        "epoch": 1.6569333333333334,
        "step": 12427
    },
    {
        "loss": 2.466,
        "grad_norm": 3.59582781791687,
        "learning_rate": 0.00010516899819178211,
        "epoch": 1.6570666666666667,
        "step": 12428
    },
    {
        "loss": 2.1342,
        "grad_norm": 2.8368453979492188,
        "learning_rate": 0.00010510614455969597,
        "epoch": 1.6572,
        "step": 12429
    },
    {
        "loss": 1.7734,
        "grad_norm": 4.91087532043457,
        "learning_rate": 0.00010504328890504853,
        "epoch": 1.6573333333333333,
        "step": 12430
    },
    {
        "loss": 2.8054,
        "grad_norm": 5.147161960601807,
        "learning_rate": 0.00010498043125273718,
        "epoch": 1.6574666666666666,
        "step": 12431
    },
    {
        "loss": 3.1716,
        "grad_norm": 4.425758361816406,
        "learning_rate": 0.00010491757162765989,
        "epoch": 1.6576,
        "step": 12432
    },
    {
        "loss": 2.5815,
        "grad_norm": 2.0604379177093506,
        "learning_rate": 0.00010485471005471588,
        "epoch": 1.6577333333333333,
        "step": 12433
    },
    {
        "loss": 3.0618,
        "grad_norm": 2.2644100189208984,
        "learning_rate": 0.00010479184655880466,
        "epoch": 1.6578666666666666,
        "step": 12434
    },
    {
        "loss": 1.6292,
        "grad_norm": 2.6279051303863525,
        "learning_rate": 0.00010472898116482657,
        "epoch": 1.658,
        "step": 12435
    },
    {
        "loss": 3.3532,
        "grad_norm": 4.825000762939453,
        "learning_rate": 0.00010466611389768301,
        "epoch": 1.6581333333333332,
        "step": 12436
    },
    {
        "loss": 1.8952,
        "grad_norm": 4.035488605499268,
        "learning_rate": 0.0001046032447822759,
        "epoch": 1.6582666666666666,
        "step": 12437
    },
    {
        "loss": 2.155,
        "grad_norm": 2.4182469844818115,
        "learning_rate": 0.0001045403738435077,
        "epoch": 1.6583999999999999,
        "step": 12438
    },
    {
        "loss": 2.551,
        "grad_norm": 3.102766990661621,
        "learning_rate": 0.00010447750110628208,
        "epoch": 1.6585333333333332,
        "step": 12439
    },
    {
        "loss": 2.2614,
        "grad_norm": 4.279525279998779,
        "learning_rate": 0.00010441462659550299,
        "epoch": 1.6586666666666665,
        "step": 12440
    },
    {
        "loss": 2.1653,
        "grad_norm": 4.53872013092041,
        "learning_rate": 0.00010435175033607527,
        "epoch": 1.6588,
        "step": 12441
    },
    {
        "loss": 1.4041,
        "grad_norm": 4.173343658447266,
        "learning_rate": 0.0001042888723529043,
        "epoch": 1.6589333333333334,
        "step": 12442
    },
    {
        "loss": 2.6788,
        "grad_norm": 3.2125802040100098,
        "learning_rate": 0.00010422599267089652,
        "epoch": 1.6590666666666667,
        "step": 12443
    },
    {
        "loss": 2.0623,
        "grad_norm": 4.5310821533203125,
        "learning_rate": 0.00010416311131495846,
        "epoch": 1.6592,
        "step": 12444
    },
    {
        "loss": 2.3686,
        "grad_norm": 2.737168788909912,
        "learning_rate": 0.00010410022830999813,
        "epoch": 1.6593333333333333,
        "step": 12445
    },
    {
        "loss": 3.1158,
        "grad_norm": 2.132089138031006,
        "learning_rate": 0.00010403734368092312,
        "epoch": 1.6594666666666666,
        "step": 12446
    },
    {
        "loss": 2.445,
        "grad_norm": 4.106997013092041,
        "learning_rate": 0.00010397445745264252,
        "epoch": 1.6596,
        "step": 12447
    },
    {
        "loss": 2.4634,
        "grad_norm": 2.6645150184631348,
        "learning_rate": 0.0001039115696500659,
        "epoch": 1.6597333333333333,
        "step": 12448
    },
    {
        "loss": 2.2194,
        "grad_norm": 3.723301410675049,
        "learning_rate": 0.00010384868029810324,
        "epoch": 1.6598666666666668,
        "step": 12449
    },
    {
        "loss": 2.5929,
        "grad_norm": 3.748047113418579,
        "learning_rate": 0.00010378578942166519,
        "epoch": 1.6600000000000001,
        "step": 12450
    },
    {
        "loss": 2.8705,
        "grad_norm": 2.8336734771728516,
        "learning_rate": 0.0001037228970456629,
        "epoch": 1.6601333333333335,
        "step": 12451
    },
    {
        "loss": 1.9435,
        "grad_norm": 3.9342284202575684,
        "learning_rate": 0.00010366000319500857,
        "epoch": 1.6602666666666668,
        "step": 12452
    },
    {
        "loss": 2.5065,
        "grad_norm": 3.0273308753967285,
        "learning_rate": 0.00010359710789461435,
        "epoch": 1.6604,
        "step": 12453
    },
    {
        "loss": 2.2075,
        "grad_norm": 4.395012855529785,
        "learning_rate": 0.0001035342111693937,
        "epoch": 1.6605333333333334,
        "step": 12454
    },
    {
        "loss": 1.4946,
        "grad_norm": 4.39304256439209,
        "learning_rate": 0.0001034713130442597,
        "epoch": 1.6606666666666667,
        "step": 12455
    },
    {
        "loss": 1.0622,
        "grad_norm": 5.272836685180664,
        "learning_rate": 0.00010340841354412678,
        "epoch": 1.6608,
        "step": 12456
    },
    {
        "loss": 3.1873,
        "grad_norm": 2.6139438152313232,
        "learning_rate": 0.00010334551269390975,
        "epoch": 1.6609333333333334,
        "step": 12457
    },
    {
        "loss": 1.3313,
        "grad_norm": 6.376921653747559,
        "learning_rate": 0.00010328261051852378,
        "epoch": 1.6610666666666667,
        "step": 12458
    },
    {
        "loss": 1.7265,
        "grad_norm": 6.462012767791748,
        "learning_rate": 0.00010321970704288457,
        "epoch": 1.6612,
        "step": 12459
    },
    {
        "loss": 2.3462,
        "grad_norm": 2.7827725410461426,
        "learning_rate": 0.00010315680229190832,
        "epoch": 1.6613333333333333,
        "step": 12460
    },
    {
        "loss": 2.3052,
        "grad_norm": 3.6607649326324463,
        "learning_rate": 0.00010309389629051202,
        "epoch": 1.6614666666666666,
        "step": 12461
    },
    {
        "loss": 2.1155,
        "grad_norm": 5.080482482910156,
        "learning_rate": 0.0001030309890636129,
        "epoch": 1.6616,
        "step": 12462
    },
    {
        "loss": 2.4322,
        "grad_norm": 3.4966235160827637,
        "learning_rate": 0.00010296808063612865,
        "epoch": 1.6617333333333333,
        "step": 12463
    },
    {
        "loss": 2.5214,
        "grad_norm": 4.572464942932129,
        "learning_rate": 0.00010290517103297746,
        "epoch": 1.6618666666666666,
        "step": 12464
    },
    {
        "loss": 1.3812,
        "grad_norm": 4.8091559410095215,
        "learning_rate": 0.00010284226027907808,
        "epoch": 1.662,
        "step": 12465
    },
    {
        "loss": 2.0521,
        "grad_norm": 3.3545358180999756,
        "learning_rate": 0.00010277934839934999,
        "epoch": 1.6621333333333332,
        "step": 12466
    },
    {
        "loss": 1.6442,
        "grad_norm": 4.892148494720459,
        "learning_rate": 0.0001027164354187123,
        "epoch": 1.6622666666666666,
        "step": 12467
    },
    {
        "loss": 2.6831,
        "grad_norm": 3.2241036891937256,
        "learning_rate": 0.00010265352136208539,
        "epoch": 1.6623999999999999,
        "step": 12468
    },
    {
        "loss": 2.3446,
        "grad_norm": 2.268327474594116,
        "learning_rate": 0.00010259060625438948,
        "epoch": 1.6625333333333332,
        "step": 12469
    },
    {
        "loss": 1.5147,
        "grad_norm": 4.2290825843811035,
        "learning_rate": 0.00010252769012054576,
        "epoch": 1.6626666666666665,
        "step": 12470
    },
    {
        "loss": 2.2243,
        "grad_norm": 3.3224785327911377,
        "learning_rate": 0.00010246477298547537,
        "epoch": 1.6627999999999998,
        "step": 12471
    },
    {
        "loss": 1.9115,
        "grad_norm": 4.316567897796631,
        "learning_rate": 0.00010240185487409999,
        "epoch": 1.6629333333333334,
        "step": 12472
    },
    {
        "loss": 1.6954,
        "grad_norm": 5.0609965324401855,
        "learning_rate": 0.00010233893581134158,
        "epoch": 1.6630666666666667,
        "step": 12473
    },
    {
        "loss": 2.2568,
        "grad_norm": 4.25352668762207,
        "learning_rate": 0.00010227601582212271,
        "epoch": 1.6632,
        "step": 12474
    },
    {
        "loss": 1.9495,
        "grad_norm": 4.277975559234619,
        "learning_rate": 0.00010221309493136652,
        "epoch": 1.6633333333333333,
        "step": 12475
    },
    {
        "loss": 2.3854,
        "grad_norm": 4.010443687438965,
        "learning_rate": 0.0001021501731639956,
        "epoch": 1.6634666666666666,
        "step": 12476
    },
    {
        "loss": 1.9025,
        "grad_norm": 4.405201435089111,
        "learning_rate": 0.0001020872505449339,
        "epoch": 1.6636,
        "step": 12477
    },
    {
        "loss": 1.669,
        "grad_norm": 3.043236017227173,
        "learning_rate": 0.00010202432709910502,
        "epoch": 1.6637333333333333,
        "step": 12478
    },
    {
        "loss": 2.3485,
        "grad_norm": 4.247223377227783,
        "learning_rate": 0.00010196140285143339,
        "epoch": 1.6638666666666668,
        "step": 12479
    },
    {
        "loss": 2.502,
        "grad_norm": 3.2884445190429688,
        "learning_rate": 0.0001018984778268434,
        "epoch": 1.6640000000000001,
        "step": 12480
    },
    {
        "loss": 2.3954,
        "grad_norm": 2.215043544769287,
        "learning_rate": 0.00010183555205025992,
        "epoch": 1.6641333333333335,
        "step": 12481
    },
    {
        "loss": 2.4203,
        "grad_norm": 2.640969753265381,
        "learning_rate": 0.00010177262554660787,
        "epoch": 1.6642666666666668,
        "step": 12482
    },
    {
        "loss": 2.4353,
        "grad_norm": 3.6451895236968994,
        "learning_rate": 0.0001017096983408129,
        "epoch": 1.6644,
        "step": 12483
    },
    {
        "loss": 1.9385,
        "grad_norm": 3.0231170654296875,
        "learning_rate": 0.00010164677045780062,
        "epoch": 1.6645333333333334,
        "step": 12484
    },
    {
        "loss": 0.9995,
        "grad_norm": 4.409125328063965,
        "learning_rate": 0.00010158384192249681,
        "epoch": 1.6646666666666667,
        "step": 12485
    },
    {
        "loss": 2.4583,
        "grad_norm": 4.533542633056641,
        "learning_rate": 0.00010152091275982798,
        "epoch": 1.6648,
        "step": 12486
    },
    {
        "loss": 1.7812,
        "grad_norm": 3.967773199081421,
        "learning_rate": 0.00010145798299472025,
        "epoch": 1.6649333333333334,
        "step": 12487
    },
    {
        "loss": 2.3955,
        "grad_norm": 3.439871311187744,
        "learning_rate": 0.00010139505265210063,
        "epoch": 1.6650666666666667,
        "step": 12488
    },
    {
        "loss": 2.6265,
        "grad_norm": 4.281543254852295,
        "learning_rate": 0.00010133212175689593,
        "epoch": 1.6652,
        "step": 12489
    },
    {
        "loss": 2.2042,
        "grad_norm": 4.08740234375,
        "learning_rate": 0.00010126919033403324,
        "epoch": 1.6653333333333333,
        "step": 12490
    },
    {
        "loss": 1.9925,
        "grad_norm": 4.270390510559082,
        "learning_rate": 0.00010120625840843978,
        "epoch": 1.6654666666666667,
        "step": 12491
    },
    {
        "loss": 3.0561,
        "grad_norm": 5.541507720947266,
        "learning_rate": 0.00010114332600504341,
        "epoch": 1.6656,
        "step": 12492
    },
    {
        "loss": 2.5518,
        "grad_norm": 5.863452434539795,
        "learning_rate": 0.00010108039314877172,
        "epoch": 1.6657333333333333,
        "step": 12493
    },
    {
        "loss": 2.1973,
        "grad_norm": 4.324877738952637,
        "learning_rate": 0.00010101745986455247,
        "epoch": 1.6658666666666666,
        "step": 12494
    },
    {
        "loss": 1.7299,
        "grad_norm": 3.2119686603546143,
        "learning_rate": 0.00010095452617731405,
        "epoch": 1.666,
        "step": 12495
    },
    {
        "loss": 1.0385,
        "grad_norm": 6.685860633850098,
        "learning_rate": 0.00010089159211198456,
        "epoch": 1.6661333333333332,
        "step": 12496
    },
    {
        "loss": 1.8159,
        "grad_norm": 2.390256404876709,
        "learning_rate": 0.00010082865769349223,
        "epoch": 1.6662666666666666,
        "step": 12497
    },
    {
        "loss": 3.0026,
        "grad_norm": 4.961144924163818,
        "learning_rate": 0.00010076572294676592,
        "epoch": 1.6663999999999999,
        "step": 12498
    },
    {
        "loss": 1.7459,
        "grad_norm": 4.085185527801514,
        "learning_rate": 0.00010070278789673416,
        "epoch": 1.6665333333333332,
        "step": 12499
    },
    {
        "loss": 2.7347,
        "grad_norm": 2.5532376766204834,
        "learning_rate": 0.00010063985256832571,
        "epoch": 1.6666666666666665,
        "step": 12500
    },
    {
        "loss": 2.86,
        "grad_norm": 2.6617753505706787,
        "learning_rate": 0.00010057691698646932,
        "epoch": 1.6667999999999998,
        "step": 12501
    },
    {
        "loss": 1.7566,
        "grad_norm": 4.957930564880371,
        "learning_rate": 0.0001005139811760943,
        "epoch": 1.6669333333333334,
        "step": 12502
    },
    {
        "loss": 2.8687,
        "grad_norm": 3.4296629428863525,
        "learning_rate": 0.00010045104516212947,
        "epoch": 1.6670666666666667,
        "step": 12503
    },
    {
        "loss": 1.8837,
        "grad_norm": 4.804582595825195,
        "learning_rate": 0.00010038810896950435,
        "epoch": 1.6672,
        "step": 12504
    },
    {
        "loss": 1.9924,
        "grad_norm": 3.0574748516082764,
        "learning_rate": 0.00010032517262314767,
        "epoch": 1.6673333333333333,
        "step": 12505
    },
    {
        "loss": 2.8152,
        "grad_norm": 3.7834572792053223,
        "learning_rate": 0.00010026223614798905,
        "epoch": 1.6674666666666667,
        "step": 12506
    },
    {
        "loss": 2.4394,
        "grad_norm": 2.515742778778076,
        "learning_rate": 0.0001001992995689579,
        "epoch": 1.6676,
        "step": 12507
    },
    {
        "loss": 1.655,
        "grad_norm": 4.877845287322998,
        "learning_rate": 0.0001001363629109835,
        "epoch": 1.6677333333333333,
        "step": 12508
    },
    {
        "loss": 2.6761,
        "grad_norm": 3.740658760070801,
        "learning_rate": 0.00010007342619899531,
        "epoch": 1.6678666666666668,
        "step": 12509
    },
    {
        "loss": 1.7506,
        "grad_norm": 4.133520603179932,
        "learning_rate": 0.00010001048945792262,
        "epoch": 1.6680000000000001,
        "step": 12510
    },
    {
        "loss": 2.0823,
        "grad_norm": 4.238433837890625,
        "learning_rate": 9.994755271269515e-05,
        "epoch": 1.6681333333333335,
        "step": 12511
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.279458522796631,
        "learning_rate": 9.988461598824208e-05,
        "epoch": 1.6682666666666668,
        "step": 12512
    },
    {
        "loss": 2.2079,
        "grad_norm": 3.502962827682495,
        "learning_rate": 9.982167930949329e-05,
        "epoch": 1.6684,
        "step": 12513
    },
    {
        "loss": 1.6468,
        "grad_norm": 4.159642219543457,
        "learning_rate": 9.975874270137767e-05,
        "epoch": 1.6685333333333334,
        "step": 12514
    },
    {
        "loss": 2.5698,
        "grad_norm": 4.263581275939941,
        "learning_rate": 9.969580618882487e-05,
        "epoch": 1.6686666666666667,
        "step": 12515
    },
    {
        "loss": 2.2928,
        "grad_norm": 4.316155910491943,
        "learning_rate": 9.963286979676442e-05,
        "epoch": 1.6688,
        "step": 12516
    },
    {
        "loss": 1.762,
        "grad_norm": 5.451918601989746,
        "learning_rate": 9.95699335501255e-05,
        "epoch": 1.6689333333333334,
        "step": 12517
    },
    {
        "loss": 2.0152,
        "grad_norm": 4.171555995941162,
        "learning_rate": 9.950699747383738e-05,
        "epoch": 1.6690666666666667,
        "step": 12518
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.6520192623138428,
        "learning_rate": 9.944406159282907e-05,
        "epoch": 1.6692,
        "step": 12519
    },
    {
        "loss": 2.2432,
        "grad_norm": 3.916347026824951,
        "learning_rate": 9.938112593203001e-05,
        "epoch": 1.6693333333333333,
        "step": 12520
    },
    {
        "loss": 2.2999,
        "grad_norm": 3.5100009441375732,
        "learning_rate": 9.931819051636909e-05,
        "epoch": 1.6694666666666667,
        "step": 12521
    },
    {
        "loss": 0.7901,
        "grad_norm": 4.5941481590271,
        "learning_rate": 9.925525537077522e-05,
        "epoch": 1.6696,
        "step": 12522
    },
    {
        "loss": 1.1679,
        "grad_norm": 4.691432952880859,
        "learning_rate": 9.919232052017709e-05,
        "epoch": 1.6697333333333333,
        "step": 12523
    },
    {
        "loss": 2.5953,
        "grad_norm": 3.455331563949585,
        "learning_rate": 9.912938598950356e-05,
        "epoch": 1.6698666666666666,
        "step": 12524
    },
    {
        "loss": 1.6313,
        "grad_norm": 3.6860921382904053,
        "learning_rate": 9.906645180368349e-05,
        "epoch": 1.67,
        "step": 12525
    },
    {
        "loss": 2.8464,
        "grad_norm": 3.4616522789001465,
        "learning_rate": 9.900351798764475e-05,
        "epoch": 1.6701333333333332,
        "step": 12526
    },
    {
        "loss": 2.7279,
        "grad_norm": 2.7450497150421143,
        "learning_rate": 9.894058456631607e-05,
        "epoch": 1.6702666666666666,
        "step": 12527
    },
    {
        "loss": 1.5178,
        "grad_norm": 4.242990016937256,
        "learning_rate": 9.887765156462533e-05,
        "epoch": 1.6703999999999999,
        "step": 12528
    },
    {
        "loss": 1.2767,
        "grad_norm": 4.204915523529053,
        "learning_rate": 9.881471900750076e-05,
        "epoch": 1.6705333333333332,
        "step": 12529
    },
    {
        "loss": 1.8126,
        "grad_norm": 4.417420387268066,
        "learning_rate": 9.875178691987001e-05,
        "epoch": 1.6706666666666665,
        "step": 12530
    },
    {
        "loss": 2.42,
        "grad_norm": 3.222982406616211,
        "learning_rate": 9.868885532666075e-05,
        "epoch": 1.6707999999999998,
        "step": 12531
    },
    {
        "loss": 1.6082,
        "grad_norm": 6.753280162811279,
        "learning_rate": 9.862592425280021e-05,
        "epoch": 1.6709333333333334,
        "step": 12532
    },
    {
        "loss": 2.9109,
        "grad_norm": 3.811237096786499,
        "learning_rate": 9.856299372321574e-05,
        "epoch": 1.6710666666666667,
        "step": 12533
    },
    {
        "loss": 2.6266,
        "grad_norm": 3.8323631286621094,
        "learning_rate": 9.850006376283465e-05,
        "epoch": 1.6712,
        "step": 12534
    },
    {
        "loss": 2.0798,
        "grad_norm": 3.9208555221557617,
        "learning_rate": 9.843713439658313e-05,
        "epoch": 1.6713333333333333,
        "step": 12535
    },
    {
        "loss": 1.0123,
        "grad_norm": 6.3814778327941895,
        "learning_rate": 9.837420564938815e-05,
        "epoch": 1.6714666666666667,
        "step": 12536
    },
    {
        "loss": 2.7296,
        "grad_norm": 3.1111621856689453,
        "learning_rate": 9.831127754617569e-05,
        "epoch": 1.6716,
        "step": 12537
    },
    {
        "loss": 1.981,
        "grad_norm": 2.7550153732299805,
        "learning_rate": 9.82483501118721e-05,
        "epoch": 1.6717333333333333,
        "step": 12538
    },
    {
        "loss": 1.6124,
        "grad_norm": 4.472161293029785,
        "learning_rate": 9.818542337140301e-05,
        "epoch": 1.6718666666666666,
        "step": 12539
    },
    {
        "loss": 2.5056,
        "grad_norm": 4.60175895690918,
        "learning_rate": 9.812249734969389e-05,
        "epoch": 1.6720000000000002,
        "step": 12540
    },
    {
        "loss": 2.6355,
        "grad_norm": 4.062977313995361,
        "learning_rate": 9.805957207166982e-05,
        "epoch": 1.6721333333333335,
        "step": 12541
    },
    {
        "loss": 2.8205,
        "grad_norm": 3.091590166091919,
        "learning_rate": 9.799664756225603e-05,
        "epoch": 1.6722666666666668,
        "step": 12542
    },
    {
        "loss": 1.4514,
        "grad_norm": 4.067386150360107,
        "learning_rate": 9.793372384637695e-05,
        "epoch": 1.6724,
        "step": 12543
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.2947003841400146,
        "learning_rate": 9.787080094895675e-05,
        "epoch": 1.6725333333333334,
        "step": 12544
    },
    {
        "loss": 2.5986,
        "grad_norm": 3.6913163661956787,
        "learning_rate": 9.78078788949197e-05,
        "epoch": 1.6726666666666667,
        "step": 12545
    },
    {
        "loss": 1.8836,
        "grad_norm": 4.855546951293945,
        "learning_rate": 9.774495770918929e-05,
        "epoch": 1.6728,
        "step": 12546
    },
    {
        "loss": 1.7616,
        "grad_norm": 2.8281705379486084,
        "learning_rate": 9.768203741668868e-05,
        "epoch": 1.6729333333333334,
        "step": 12547
    },
    {
        "loss": 1.9412,
        "grad_norm": 4.859652996063232,
        "learning_rate": 9.761911804234107e-05,
        "epoch": 1.6730666666666667,
        "step": 12548
    },
    {
        "loss": 1.8671,
        "grad_norm": 3.9956603050231934,
        "learning_rate": 9.755619961106892e-05,
        "epoch": 1.6732,
        "step": 12549
    },
    {
        "loss": 2.2876,
        "grad_norm": 3.6043262481689453,
        "learning_rate": 9.74932821477943e-05,
        "epoch": 1.6733333333333333,
        "step": 12550
    },
    {
        "loss": 2.5668,
        "grad_norm": 2.986645221710205,
        "learning_rate": 9.743036567743932e-05,
        "epoch": 1.6734666666666667,
        "step": 12551
    },
    {
        "loss": 2.88,
        "grad_norm": 3.6611101627349854,
        "learning_rate": 9.73674502249252e-05,
        "epoch": 1.6736,
        "step": 12552
    },
    {
        "loss": 2.8629,
        "grad_norm": 2.228977680206299,
        "learning_rate": 9.730453581517293e-05,
        "epoch": 1.6737333333333333,
        "step": 12553
    },
    {
        "loss": 2.648,
        "grad_norm": 3.0369482040405273,
        "learning_rate": 9.724162247310332e-05,
        "epoch": 1.6738666666666666,
        "step": 12554
    },
    {
        "loss": 2.5726,
        "grad_norm": 4.605214595794678,
        "learning_rate": 9.717871022363644e-05,
        "epoch": 1.674,
        "step": 12555
    },
    {
        "loss": 1.4735,
        "grad_norm": 2.6954545974731445,
        "learning_rate": 9.711579909169193e-05,
        "epoch": 1.6741333333333333,
        "step": 12556
    },
    {
        "loss": 2.0435,
        "grad_norm": 3.137284994125366,
        "learning_rate": 9.705288910218937e-05,
        "epoch": 1.6742666666666666,
        "step": 12557
    },
    {
        "loss": 0.9478,
        "grad_norm": 3.4634640216827393,
        "learning_rate": 9.698998028004747e-05,
        "epoch": 1.6743999999999999,
        "step": 12558
    },
    {
        "loss": 1.851,
        "grad_norm": 4.656439781188965,
        "learning_rate": 9.692707265018463e-05,
        "epoch": 1.6745333333333332,
        "step": 12559
    },
    {
        "loss": 3.0283,
        "grad_norm": 3.8438849449157715,
        "learning_rate": 9.686416623751866e-05,
        "epoch": 1.6746666666666665,
        "step": 12560
    },
    {
        "loss": 2.9779,
        "grad_norm": 3.252229690551758,
        "learning_rate": 9.680126106696726e-05,
        "epoch": 1.6747999999999998,
        "step": 12561
    },
    {
        "loss": 2.6847,
        "grad_norm": 4.0298638343811035,
        "learning_rate": 9.67383571634471e-05,
        "epoch": 1.6749333333333334,
        "step": 12562
    },
    {
        "loss": 1.0062,
        "grad_norm": 5.59114933013916,
        "learning_rate": 9.667545455187508e-05,
        "epoch": 1.6750666666666667,
        "step": 12563
    },
    {
        "loss": 2.5149,
        "grad_norm": 3.5290606021881104,
        "learning_rate": 9.661255325716656e-05,
        "epoch": 1.6752,
        "step": 12564
    },
    {
        "loss": 2.6836,
        "grad_norm": 3.7279293537139893,
        "learning_rate": 9.654965330423722e-05,
        "epoch": 1.6753333333333333,
        "step": 12565
    },
    {
        "loss": 1.5395,
        "grad_norm": 5.048738956451416,
        "learning_rate": 9.648675471800212e-05,
        "epoch": 1.6754666666666667,
        "step": 12566
    },
    {
        "loss": 1.7531,
        "grad_norm": 5.3693108558654785,
        "learning_rate": 9.642385752337544e-05,
        "epoch": 1.6756,
        "step": 12567
    },
    {
        "loss": 2.5726,
        "grad_norm": 2.932727336883545,
        "learning_rate": 9.636096174527098e-05,
        "epoch": 1.6757333333333333,
        "step": 12568
    },
    {
        "loss": 2.5387,
        "grad_norm": 3.824526309967041,
        "learning_rate": 9.629806740860183e-05,
        "epoch": 1.6758666666666666,
        "step": 12569
    },
    {
        "loss": 2.2919,
        "grad_norm": 4.220600128173828,
        "learning_rate": 9.623517453828094e-05,
        "epoch": 1.6760000000000002,
        "step": 12570
    },
    {
        "loss": 1.9522,
        "grad_norm": 3.755642890930176,
        "learning_rate": 9.617228315922011e-05,
        "epoch": 1.6761333333333335,
        "step": 12571
    },
    {
        "loss": 2.4829,
        "grad_norm": 2.6327788829803467,
        "learning_rate": 9.610939329633124e-05,
        "epoch": 1.6762666666666668,
        "step": 12572
    },
    {
        "loss": 2.5042,
        "grad_norm": 3.462630271911621,
        "learning_rate": 9.604650497452465e-05,
        "epoch": 1.6764000000000001,
        "step": 12573
    },
    {
        "loss": 2.0659,
        "grad_norm": 4.36181116104126,
        "learning_rate": 9.598361821871093e-05,
        "epoch": 1.6765333333333334,
        "step": 12574
    },
    {
        "loss": 0.9972,
        "grad_norm": 4.089197635650635,
        "learning_rate": 9.592073305379983e-05,
        "epoch": 1.6766666666666667,
        "step": 12575
    },
    {
        "loss": 1.6945,
        "grad_norm": 3.6764514446258545,
        "learning_rate": 9.58578495047003e-05,
        "epoch": 1.6768,
        "step": 12576
    },
    {
        "loss": 1.1253,
        "grad_norm": 6.276796340942383,
        "learning_rate": 9.579496759632066e-05,
        "epoch": 1.6769333333333334,
        "step": 12577
    },
    {
        "loss": 1.6638,
        "grad_norm": 3.0101029872894287,
        "learning_rate": 9.573208735356856e-05,
        "epoch": 1.6770666666666667,
        "step": 12578
    },
    {
        "loss": 1.821,
        "grad_norm": 5.271343231201172,
        "learning_rate": 9.566920880135137e-05,
        "epoch": 1.6772,
        "step": 12579
    },
    {
        "loss": 2.0476,
        "grad_norm": 3.0695338249206543,
        "learning_rate": 9.56063319645753e-05,
        "epoch": 1.6773333333333333,
        "step": 12580
    },
    {
        "loss": 1.6414,
        "grad_norm": 3.997284173965454,
        "learning_rate": 9.55434568681461e-05,
        "epoch": 1.6774666666666667,
        "step": 12581
    },
    {
        "loss": 1.5677,
        "grad_norm": 2.5537140369415283,
        "learning_rate": 9.54805835369687e-05,
        "epoch": 1.6776,
        "step": 12582
    },
    {
        "loss": 1.6079,
        "grad_norm": 4.249908924102783,
        "learning_rate": 9.541771199594755e-05,
        "epoch": 1.6777333333333333,
        "step": 12583
    },
    {
        "loss": 2.2155,
        "grad_norm": 3.6048614978790283,
        "learning_rate": 9.53548422699866e-05,
        "epoch": 1.6778666666666666,
        "step": 12584
    },
    {
        "loss": 2.4891,
        "grad_norm": 6.641104221343994,
        "learning_rate": 9.529197438398813e-05,
        "epoch": 1.678,
        "step": 12585
    },
    {
        "loss": 2.4448,
        "grad_norm": 2.6131277084350586,
        "learning_rate": 9.522910836285481e-05,
        "epoch": 1.6781333333333333,
        "step": 12586
    },
    {
        "loss": 2.8609,
        "grad_norm": 2.3948683738708496,
        "learning_rate": 9.516624423148778e-05,
        "epoch": 1.6782666666666666,
        "step": 12587
    },
    {
        "loss": 2.0484,
        "grad_norm": 4.062244415283203,
        "learning_rate": 9.510338201478804e-05,
        "epoch": 1.6784,
        "step": 12588
    },
    {
        "loss": 1.7528,
        "grad_norm": 3.601205348968506,
        "learning_rate": 9.504052173765534e-05,
        "epoch": 1.6785333333333332,
        "step": 12589
    },
    {
        "loss": 1.8093,
        "grad_norm": 4.772258758544922,
        "learning_rate": 9.49776634249889e-05,
        "epoch": 1.6786666666666665,
        "step": 12590
    },
    {
        "loss": 1.8639,
        "grad_norm": 5.600247383117676,
        "learning_rate": 9.491480710168694e-05,
        "epoch": 1.6787999999999998,
        "step": 12591
    },
    {
        "loss": 1.7224,
        "grad_norm": 4.938470363616943,
        "learning_rate": 9.485195279264722e-05,
        "epoch": 1.6789333333333334,
        "step": 12592
    },
    {
        "loss": 1.882,
        "grad_norm": 4.473929405212402,
        "learning_rate": 9.478910052276682e-05,
        "epoch": 1.6790666666666667,
        "step": 12593
    },
    {
        "loss": 2.3967,
        "grad_norm": 4.652529239654541,
        "learning_rate": 9.47262503169412e-05,
        "epoch": 1.6792,
        "step": 12594
    },
    {
        "loss": 2.8661,
        "grad_norm": 5.886518478393555,
        "learning_rate": 9.46634022000659e-05,
        "epoch": 1.6793333333333333,
        "step": 12595
    },
    {
        "loss": 1.6771,
        "grad_norm": 5.6622633934021,
        "learning_rate": 9.460055619703504e-05,
        "epoch": 1.6794666666666667,
        "step": 12596
    },
    {
        "loss": 2.3481,
        "grad_norm": 4.319265842437744,
        "learning_rate": 9.453771233274243e-05,
        "epoch": 1.6796,
        "step": 12597
    },
    {
        "loss": 2.5986,
        "grad_norm": 4.166630268096924,
        "learning_rate": 9.447487063208055e-05,
        "epoch": 1.6797333333333333,
        "step": 12598
    },
    {
        "loss": 2.7578,
        "grad_norm": 2.8598473072052,
        "learning_rate": 9.441203111994125e-05,
        "epoch": 1.6798666666666666,
        "step": 12599
    },
    {
        "loss": 1.518,
        "grad_norm": 5.777069568634033,
        "learning_rate": 9.434919382121533e-05,
        "epoch": 1.6800000000000002,
        "step": 12600
    },
    {
        "loss": 1.843,
        "grad_norm": 3.9366519451141357,
        "learning_rate": 9.428635876079314e-05,
        "epoch": 1.6801333333333335,
        "step": 12601
    },
    {
        "loss": 2.6296,
        "grad_norm": 3.187642812728882,
        "learning_rate": 9.422352596356369e-05,
        "epoch": 1.6802666666666668,
        "step": 12602
    },
    {
        "loss": 3.0336,
        "grad_norm": 3.082354784011841,
        "learning_rate": 9.416069545441517e-05,
        "epoch": 1.6804000000000001,
        "step": 12603
    },
    {
        "loss": 2.5553,
        "grad_norm": 3.4179396629333496,
        "learning_rate": 9.40978672582352e-05,
        "epoch": 1.6805333333333334,
        "step": 12604
    },
    {
        "loss": 2.4264,
        "grad_norm": 2.5572147369384766,
        "learning_rate": 9.403504139991014e-05,
        "epoch": 1.6806666666666668,
        "step": 12605
    },
    {
        "loss": 2.1311,
        "grad_norm": 3.022718906402588,
        "learning_rate": 9.397221790432535e-05,
        "epoch": 1.6808,
        "step": 12606
    },
    {
        "loss": 1.1937,
        "grad_norm": 3.727419376373291,
        "learning_rate": 9.390939679636573e-05,
        "epoch": 1.6809333333333334,
        "step": 12607
    },
    {
        "loss": 1.3404,
        "grad_norm": 4.809071063995361,
        "learning_rate": 9.38465781009148e-05,
        "epoch": 1.6810666666666667,
        "step": 12608
    },
    {
        "loss": 1.8815,
        "grad_norm": 3.9382147789001465,
        "learning_rate": 9.37837618428551e-05,
        "epoch": 1.6812,
        "step": 12609
    },
    {
        "loss": 1.6913,
        "grad_norm": 3.889246702194214,
        "learning_rate": 9.372094804706867e-05,
        "epoch": 1.6813333333333333,
        "step": 12610
    },
    {
        "loss": 2.3641,
        "grad_norm": 3.6299612522125244,
        "learning_rate": 9.36581367384361e-05,
        "epoch": 1.6814666666666667,
        "step": 12611
    },
    {
        "loss": 2.4943,
        "grad_norm": 2.7396769523620605,
        "learning_rate": 9.359532794183704e-05,
        "epoch": 1.6816,
        "step": 12612
    },
    {
        "loss": 2.2931,
        "grad_norm": 4.7514424324035645,
        "learning_rate": 9.353252168215055e-05,
        "epoch": 1.6817333333333333,
        "step": 12613
    },
    {
        "loss": 2.0311,
        "grad_norm": 4.152301788330078,
        "learning_rate": 9.346971798425425e-05,
        "epoch": 1.6818666666666666,
        "step": 12614
    },
    {
        "loss": 0.8393,
        "grad_norm": 3.8800759315490723,
        "learning_rate": 9.34069168730248e-05,
        "epoch": 1.682,
        "step": 12615
    },
    {
        "loss": 2.9822,
        "grad_norm": 2.4407663345336914,
        "learning_rate": 9.334411837333814e-05,
        "epoch": 1.6821333333333333,
        "step": 12616
    },
    {
        "loss": 1.9705,
        "grad_norm": 4.345130443572998,
        "learning_rate": 9.328132251006889e-05,
        "epoch": 1.6822666666666666,
        "step": 12617
    },
    {
        "loss": 1.2493,
        "grad_norm": 3.4651427268981934,
        "learning_rate": 9.32185293080907e-05,
        "epoch": 1.6824,
        "step": 12618
    },
    {
        "loss": 2.3774,
        "grad_norm": 5.0473222732543945,
        "learning_rate": 9.3155738792276e-05,
        "epoch": 1.6825333333333332,
        "step": 12619
    },
    {
        "loss": 0.8591,
        "grad_norm": 3.9868640899658203,
        "learning_rate": 9.309295098749662e-05,
        "epoch": 1.6826666666666665,
        "step": 12620
    },
    {
        "loss": 2.7385,
        "grad_norm": 3.1707777976989746,
        "learning_rate": 9.303016591862276e-05,
        "epoch": 1.6827999999999999,
        "step": 12621
    },
    {
        "loss": 2.3332,
        "grad_norm": 4.759982585906982,
        "learning_rate": 9.296738361052421e-05,
        "epoch": 1.6829333333333332,
        "step": 12622
    },
    {
        "loss": 2.7098,
        "grad_norm": 3.230198621749878,
        "learning_rate": 9.290460408806871e-05,
        "epoch": 1.6830666666666667,
        "step": 12623
    },
    {
        "loss": 3.0409,
        "grad_norm": 4.5036444664001465,
        "learning_rate": 9.284182737612368e-05,
        "epoch": 1.6832,
        "step": 12624
    },
    {
        "loss": 2.5533,
        "grad_norm": 3.7507927417755127,
        "learning_rate": 9.277905349955537e-05,
        "epoch": 1.6833333333333333,
        "step": 12625
    },
    {
        "loss": 1.7175,
        "grad_norm": 4.71946382522583,
        "learning_rate": 9.271628248322858e-05,
        "epoch": 1.6834666666666667,
        "step": 12626
    },
    {
        "loss": 2.6075,
        "grad_norm": 3.1433160305023193,
        "learning_rate": 9.265351435200711e-05,
        "epoch": 1.6836,
        "step": 12627
    },
    {
        "loss": 2.246,
        "grad_norm": 3.557041645050049,
        "learning_rate": 9.259074913075349e-05,
        "epoch": 1.6837333333333333,
        "step": 12628
    },
    {
        "loss": 2.1321,
        "grad_norm": 2.9813954830169678,
        "learning_rate": 9.252798684432952e-05,
        "epoch": 1.6838666666666666,
        "step": 12629
    },
    {
        "loss": 2.1183,
        "grad_norm": 3.1341910362243652,
        "learning_rate": 9.246522751759529e-05,
        "epoch": 1.6840000000000002,
        "step": 12630
    },
    {
        "loss": 2.3887,
        "grad_norm": 3.5330557823181152,
        "learning_rate": 9.240247117541042e-05,
        "epoch": 1.6841333333333335,
        "step": 12631
    },
    {
        "loss": 2.5394,
        "grad_norm": 4.443350791931152,
        "learning_rate": 9.233971784263229e-05,
        "epoch": 1.6842666666666668,
        "step": 12632
    },
    {
        "loss": 1.3868,
        "grad_norm": 4.7217116355896,
        "learning_rate": 9.227696754411805e-05,
        "epoch": 1.6844000000000001,
        "step": 12633
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.5552942752838135,
        "learning_rate": 9.221422030472339e-05,
        "epoch": 1.6845333333333334,
        "step": 12634
    },
    {
        "loss": 2.7671,
        "grad_norm": 2.882704257965088,
        "learning_rate": 9.215147614930262e-05,
        "epoch": 1.6846666666666668,
        "step": 12635
    },
    {
        "loss": 2.3183,
        "grad_norm": 3.4140102863311768,
        "learning_rate": 9.208873510270889e-05,
        "epoch": 1.6848,
        "step": 12636
    },
    {
        "loss": 1.8869,
        "grad_norm": 3.916642427444458,
        "learning_rate": 9.202599718979401e-05,
        "epoch": 1.6849333333333334,
        "step": 12637
    },
    {
        "loss": 1.7383,
        "grad_norm": 3.6259148120880127,
        "learning_rate": 9.196326243540892e-05,
        "epoch": 1.6850666666666667,
        "step": 12638
    },
    {
        "loss": 1.6874,
        "grad_norm": 4.108752727508545,
        "learning_rate": 9.190053086440298e-05,
        "epoch": 1.6852,
        "step": 12639
    },
    {
        "loss": 2.4566,
        "grad_norm": 2.560448169708252,
        "learning_rate": 9.183780250162438e-05,
        "epoch": 1.6853333333333333,
        "step": 12640
    },
    {
        "loss": 2.2689,
        "grad_norm": 2.4678900241851807,
        "learning_rate": 9.177507737191989e-05,
        "epoch": 1.6854666666666667,
        "step": 12641
    },
    {
        "loss": 1.6097,
        "grad_norm": 5.73367166519165,
        "learning_rate": 9.171235550013525e-05,
        "epoch": 1.6856,
        "step": 12642
    },
    {
        "loss": 1.5336,
        "grad_norm": 4.805325031280518,
        "learning_rate": 9.164963691111512e-05,
        "epoch": 1.6857333333333333,
        "step": 12643
    },
    {
        "loss": 2.0222,
        "grad_norm": 4.303096294403076,
        "learning_rate": 9.1586921629702e-05,
        "epoch": 1.6858666666666666,
        "step": 12644
    },
    {
        "loss": 1.4953,
        "grad_norm": 4.678958415985107,
        "learning_rate": 9.152420968073801e-05,
        "epoch": 1.686,
        "step": 12645
    },
    {
        "loss": 2.997,
        "grad_norm": 2.7203173637390137,
        "learning_rate": 9.14615010890633e-05,
        "epoch": 1.6861333333333333,
        "step": 12646
    },
    {
        "loss": 1.7654,
        "grad_norm": 2.382300853729248,
        "learning_rate": 9.139879587951727e-05,
        "epoch": 1.6862666666666666,
        "step": 12647
    },
    {
        "loss": 3.2461,
        "grad_norm": 3.9847583770751953,
        "learning_rate": 9.133609407693748e-05,
        "epoch": 1.6864,
        "step": 12648
    },
    {
        "loss": 2.9634,
        "grad_norm": 4.427412033081055,
        "learning_rate": 9.127339570616035e-05,
        "epoch": 1.6865333333333332,
        "step": 12649
    },
    {
        "loss": 2.0002,
        "grad_norm": 3.7359580993652344,
        "learning_rate": 9.121070079202078e-05,
        "epoch": 1.6866666666666665,
        "step": 12650
    },
    {
        "loss": 2.3588,
        "grad_norm": 3.433138132095337,
        "learning_rate": 9.11480093593526e-05,
        "epoch": 1.6867999999999999,
        "step": 12651
    },
    {
        "loss": 1.5547,
        "grad_norm": 3.2500486373901367,
        "learning_rate": 9.108532143298831e-05,
        "epoch": 1.6869333333333332,
        "step": 12652
    },
    {
        "loss": 2.0131,
        "grad_norm": 3.5927741527557373,
        "learning_rate": 9.10226370377583e-05,
        "epoch": 1.6870666666666667,
        "step": 12653
    },
    {
        "loss": 1.7563,
        "grad_norm": 3.7588062286376953,
        "learning_rate": 9.095995619849252e-05,
        "epoch": 1.6872,
        "step": 12654
    },
    {
        "loss": 2.1902,
        "grad_norm": 5.0064616203308105,
        "learning_rate": 9.089727894001875e-05,
        "epoch": 1.6873333333333334,
        "step": 12655
    },
    {
        "loss": 1.9771,
        "grad_norm": 5.92867374420166,
        "learning_rate": 9.083460528716391e-05,
        "epoch": 1.6874666666666667,
        "step": 12656
    },
    {
        "loss": 2.3028,
        "grad_norm": 2.819619655609131,
        "learning_rate": 9.077193526475318e-05,
        "epoch": 1.6876,
        "step": 12657
    },
    {
        "loss": 2.2914,
        "grad_norm": 3.8692991733551025,
        "learning_rate": 9.070926889761033e-05,
        "epoch": 1.6877333333333333,
        "step": 12658
    },
    {
        "loss": 1.9674,
        "grad_norm": 4.265948295593262,
        "learning_rate": 9.064660621055761e-05,
        "epoch": 1.6878666666666666,
        "step": 12659
    },
    {
        "loss": 2.2527,
        "grad_norm": 4.610658645629883,
        "learning_rate": 9.058394722841613e-05,
        "epoch": 1.688,
        "step": 12660
    },
    {
        "loss": 1.4991,
        "grad_norm": 5.144154071807861,
        "learning_rate": 9.052129197600527e-05,
        "epoch": 1.6881333333333335,
        "step": 12661
    },
    {
        "loss": 1.0156,
        "grad_norm": 2.8459866046905518,
        "learning_rate": 9.045864047814282e-05,
        "epoch": 1.6882666666666668,
        "step": 12662
    },
    {
        "loss": 2.8015,
        "grad_norm": 2.5521719455718994,
        "learning_rate": 9.03959927596455e-05,
        "epoch": 1.6884000000000001,
        "step": 12663
    },
    {
        "loss": 2.1116,
        "grad_norm": 2.931215524673462,
        "learning_rate": 9.03333488453282e-05,
        "epoch": 1.6885333333333334,
        "step": 12664
    },
    {
        "loss": 1.0117,
        "grad_norm": 4.894437789916992,
        "learning_rate": 9.027070876000422e-05,
        "epoch": 1.6886666666666668,
        "step": 12665
    },
    {
        "loss": 1.766,
        "grad_norm": 5.2748003005981445,
        "learning_rate": 9.020807252848578e-05,
        "epoch": 1.6888,
        "step": 12666
    },
    {
        "loss": 1.5616,
        "grad_norm": 2.1915578842163086,
        "learning_rate": 9.014544017558316e-05,
        "epoch": 1.6889333333333334,
        "step": 12667
    },
    {
        "loss": 2.0226,
        "grad_norm": 3.0812203884124756,
        "learning_rate": 9.008281172610529e-05,
        "epoch": 1.6890666666666667,
        "step": 12668
    },
    {
        "loss": 1.7038,
        "grad_norm": 5.641822338104248,
        "learning_rate": 9.002018720485936e-05,
        "epoch": 1.6892,
        "step": 12669
    },
    {
        "loss": 2.3314,
        "grad_norm": 3.0820248126983643,
        "learning_rate": 8.99575666366514e-05,
        "epoch": 1.6893333333333334,
        "step": 12670
    },
    {
        "loss": 2.5024,
        "grad_norm": 3.980360984802246,
        "learning_rate": 8.989495004628538e-05,
        "epoch": 1.6894666666666667,
        "step": 12671
    },
    {
        "loss": 2.191,
        "grad_norm": 4.3666863441467285,
        "learning_rate": 8.983233745856422e-05,
        "epoch": 1.6896,
        "step": 12672
    },
    {
        "loss": 1.6068,
        "grad_norm": 4.481373310089111,
        "learning_rate": 8.976972889828883e-05,
        "epoch": 1.6897333333333333,
        "step": 12673
    },
    {
        "loss": 2.5305,
        "grad_norm": 3.651498556137085,
        "learning_rate": 8.970712439025855e-05,
        "epoch": 1.6898666666666666,
        "step": 12674
    },
    {
        "loss": 2.6192,
        "grad_norm": 3.131699800491333,
        "learning_rate": 8.96445239592715e-05,
        "epoch": 1.69,
        "step": 12675
    },
    {
        "loss": 2.0674,
        "grad_norm": 3.1158852577209473,
        "learning_rate": 8.958192763012382e-05,
        "epoch": 1.6901333333333333,
        "step": 12676
    },
    {
        "loss": 2.1331,
        "grad_norm": 5.484086990356445,
        "learning_rate": 8.951933542761008e-05,
        "epoch": 1.6902666666666666,
        "step": 12677
    },
    {
        "loss": 0.887,
        "grad_norm": 4.208882808685303,
        "learning_rate": 8.945674737652316e-05,
        "epoch": 1.6904,
        "step": 12678
    },
    {
        "loss": 2.7053,
        "grad_norm": 3.744039297103882,
        "learning_rate": 8.939416350165467e-05,
        "epoch": 1.6905333333333332,
        "step": 12679
    },
    {
        "loss": 1.5156,
        "grad_norm": 7.083469867706299,
        "learning_rate": 8.933158382779401e-05,
        "epoch": 1.6906666666666665,
        "step": 12680
    },
    {
        "loss": 1.8074,
        "grad_norm": 4.329987525939941,
        "learning_rate": 8.926900837972964e-05,
        "epoch": 1.6907999999999999,
        "step": 12681
    },
    {
        "loss": 2.8764,
        "grad_norm": 4.708902835845947,
        "learning_rate": 8.920643718224731e-05,
        "epoch": 1.6909333333333332,
        "step": 12682
    },
    {
        "loss": 1.3364,
        "grad_norm": 6.792953014373779,
        "learning_rate": 8.914387026013202e-05,
        "epoch": 1.6910666666666667,
        "step": 12683
    },
    {
        "loss": 2.6491,
        "grad_norm": 3.525862216949463,
        "learning_rate": 8.908130763816685e-05,
        "epoch": 1.6912,
        "step": 12684
    },
    {
        "loss": 1.253,
        "grad_norm": 3.858444929122925,
        "learning_rate": 8.901874934113289e-05,
        "epoch": 1.6913333333333334,
        "step": 12685
    },
    {
        "loss": 2.621,
        "grad_norm": 5.547397136688232,
        "learning_rate": 8.895619539380977e-05,
        "epoch": 1.6914666666666667,
        "step": 12686
    },
    {
        "loss": 2.023,
        "grad_norm": 4.188174247741699,
        "learning_rate": 8.889364582097515e-05,
        "epoch": 1.6916,
        "step": 12687
    },
    {
        "loss": 1.2432,
        "grad_norm": 4.331109523773193,
        "learning_rate": 8.883110064740539e-05,
        "epoch": 1.6917333333333333,
        "step": 12688
    },
    {
        "loss": 2.1401,
        "grad_norm": 4.669186592102051,
        "learning_rate": 8.876855989787457e-05,
        "epoch": 1.6918666666666666,
        "step": 12689
    },
    {
        "loss": 2.4982,
        "grad_norm": 3.09916353225708,
        "learning_rate": 8.870602359715572e-05,
        "epoch": 1.692,
        "step": 12690
    },
    {
        "loss": 2.2526,
        "grad_norm": 4.29437255859375,
        "learning_rate": 8.864349177001915e-05,
        "epoch": 1.6921333333333335,
        "step": 12691
    },
    {
        "loss": 1.696,
        "grad_norm": 3.9677743911743164,
        "learning_rate": 8.858096444123419e-05,
        "epoch": 1.6922666666666668,
        "step": 12692
    },
    {
        "loss": 1.7414,
        "grad_norm": 4.955322742462158,
        "learning_rate": 8.851844163556825e-05,
        "epoch": 1.6924000000000001,
        "step": 12693
    },
    {
        "loss": 2.6438,
        "grad_norm": 3.1758410930633545,
        "learning_rate": 8.845592337778671e-05,
        "epoch": 1.6925333333333334,
        "step": 12694
    },
    {
        "loss": 2.7217,
        "grad_norm": 3.065396308898926,
        "learning_rate": 8.839340969265325e-05,
        "epoch": 1.6926666666666668,
        "step": 12695
    },
    {
        "loss": 0.6697,
        "grad_norm": 3.215256452560425,
        "learning_rate": 8.833090060492961e-05,
        "epoch": 1.6928,
        "step": 12696
    },
    {
        "loss": 2.2883,
        "grad_norm": 3.562910318374634,
        "learning_rate": 8.826839613937615e-05,
        "epoch": 1.6929333333333334,
        "step": 12697
    },
    {
        "loss": 2.7639,
        "grad_norm": 4.001802921295166,
        "learning_rate": 8.820589632075096e-05,
        "epoch": 1.6930666666666667,
        "step": 12698
    },
    {
        "loss": 2.4036,
        "grad_norm": 4.462888717651367,
        "learning_rate": 8.814340117381041e-05,
        "epoch": 1.6932,
        "step": 12699
    },
    {
        "loss": 2.6381,
        "grad_norm": 3.631817102432251,
        "learning_rate": 8.808091072330892e-05,
        "epoch": 1.6933333333333334,
        "step": 12700
    },
    {
        "loss": 2.0759,
        "grad_norm": 3.250000476837158,
        "learning_rate": 8.801842499399933e-05,
        "epoch": 1.6934666666666667,
        "step": 12701
    },
    {
        "loss": 2.8556,
        "grad_norm": 5.079736232757568,
        "learning_rate": 8.795594401063268e-05,
        "epoch": 1.6936,
        "step": 12702
    },
    {
        "loss": 2.7273,
        "grad_norm": 2.956364631652832,
        "learning_rate": 8.789346779795735e-05,
        "epoch": 1.6937333333333333,
        "step": 12703
    },
    {
        "loss": 2.7758,
        "grad_norm": 4.143703937530518,
        "learning_rate": 8.783099638072085e-05,
        "epoch": 1.6938666666666666,
        "step": 12704
    },
    {
        "loss": 2.3415,
        "grad_norm": 2.749692678451538,
        "learning_rate": 8.7768529783668e-05,
        "epoch": 1.694,
        "step": 12705
    },
    {
        "loss": 2.3346,
        "grad_norm": 3.112431287765503,
        "learning_rate": 8.770606803154233e-05,
        "epoch": 1.6941333333333333,
        "step": 12706
    },
    {
        "loss": 0.8215,
        "grad_norm": 2.8965566158294678,
        "learning_rate": 8.764361114908497e-05,
        "epoch": 1.6942666666666666,
        "step": 12707
    },
    {
        "loss": 1.322,
        "grad_norm": 3.574172019958496,
        "learning_rate": 8.758115916103536e-05,
        "epoch": 1.6944,
        "step": 12708
    },
    {
        "loss": 2.4225,
        "grad_norm": 3.851729393005371,
        "learning_rate": 8.751871209213081e-05,
        "epoch": 1.6945333333333332,
        "step": 12709
    },
    {
        "loss": 1.8047,
        "grad_norm": 4.042520523071289,
        "learning_rate": 8.745626996710687e-05,
        "epoch": 1.6946666666666665,
        "step": 12710
    },
    {
        "loss": 2.4697,
        "grad_norm": 3.0345394611358643,
        "learning_rate": 8.739383281069745e-05,
        "epoch": 1.6947999999999999,
        "step": 12711
    },
    {
        "loss": 2.2726,
        "grad_norm": 3.077702522277832,
        "learning_rate": 8.733140064763354e-05,
        "epoch": 1.6949333333333332,
        "step": 12712
    },
    {
        "loss": 2.5549,
        "grad_norm": 4.192788600921631,
        "learning_rate": 8.726897350264513e-05,
        "epoch": 1.6950666666666667,
        "step": 12713
    },
    {
        "loss": 2.5069,
        "grad_norm": 2.7068755626678467,
        "learning_rate": 8.720655140045954e-05,
        "epoch": 1.6952,
        "step": 12714
    },
    {
        "loss": 2.5508,
        "grad_norm": 3.415430784225464,
        "learning_rate": 8.714413436580268e-05,
        "epoch": 1.6953333333333334,
        "step": 12715
    },
    {
        "loss": 3.0223,
        "grad_norm": 4.177433490753174,
        "learning_rate": 8.708172242339803e-05,
        "epoch": 1.6954666666666667,
        "step": 12716
    },
    {
        "loss": 2.2058,
        "grad_norm": 3.287829875946045,
        "learning_rate": 8.701931559796713e-05,
        "epoch": 1.6956,
        "step": 12717
    },
    {
        "loss": 0.9631,
        "grad_norm": 3.633974313735962,
        "learning_rate": 8.695691391422938e-05,
        "epoch": 1.6957333333333333,
        "step": 12718
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.723299026489258,
        "learning_rate": 8.689451739690265e-05,
        "epoch": 1.6958666666666666,
        "step": 12719
    },
    {
        "loss": 2.1397,
        "grad_norm": 4.184957027435303,
        "learning_rate": 8.683212607070223e-05,
        "epoch": 1.696,
        "step": 12720
    },
    {
        "loss": 2.799,
        "grad_norm": 4.125529766082764,
        "learning_rate": 8.67697399603414e-05,
        "epoch": 1.6961333333333335,
        "step": 12721
    },
    {
        "loss": 2.5378,
        "grad_norm": 2.603811740875244,
        "learning_rate": 8.670735909053179e-05,
        "epoch": 1.6962666666666668,
        "step": 12722
    },
    {
        "loss": 1.7561,
        "grad_norm": 4.689772129058838,
        "learning_rate": 8.664498348598252e-05,
        "epoch": 1.6964000000000001,
        "step": 12723
    },
    {
        "loss": 2.5957,
        "grad_norm": 2.5602633953094482,
        "learning_rate": 8.658261317140066e-05,
        "epoch": 1.6965333333333334,
        "step": 12724
    },
    {
        "loss": 1.8107,
        "grad_norm": 3.6952147483825684,
        "learning_rate": 8.652024817149161e-05,
        "epoch": 1.6966666666666668,
        "step": 12725
    },
    {
        "loss": 1.5824,
        "grad_norm": 2.966688871383667,
        "learning_rate": 8.645788851095817e-05,
        "epoch": 1.6968,
        "step": 12726
    },
    {
        "loss": 1.2781,
        "grad_norm": 4.722403526306152,
        "learning_rate": 8.639553421450125e-05,
        "epoch": 1.6969333333333334,
        "step": 12727
    },
    {
        "loss": 2.4646,
        "grad_norm": 3.8640005588531494,
        "learning_rate": 8.633318530681942e-05,
        "epoch": 1.6970666666666667,
        "step": 12728
    },
    {
        "loss": 2.5289,
        "grad_norm": 3.2888104915618896,
        "learning_rate": 8.627084181260961e-05,
        "epoch": 1.6972,
        "step": 12729
    },
    {
        "loss": 1.0941,
        "grad_norm": 5.152818202972412,
        "learning_rate": 8.620850375656601e-05,
        "epoch": 1.6973333333333334,
        "step": 12730
    },
    {
        "loss": 2.5159,
        "grad_norm": 2.007131814956665,
        "learning_rate": 8.614617116338121e-05,
        "epoch": 1.6974666666666667,
        "step": 12731
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.720660448074341,
        "learning_rate": 8.608384405774519e-05,
        "epoch": 1.6976,
        "step": 12732
    },
    {
        "loss": 2.9225,
        "grad_norm": 4.696610450744629,
        "learning_rate": 8.602152246434584e-05,
        "epoch": 1.6977333333333333,
        "step": 12733
    },
    {
        "loss": 2.5789,
        "grad_norm": 2.1721105575561523,
        "learning_rate": 8.59592064078692e-05,
        "epoch": 1.6978666666666666,
        "step": 12734
    },
    {
        "loss": 2.4291,
        "grad_norm": 2.714242935180664,
        "learning_rate": 8.589689591299876e-05,
        "epoch": 1.698,
        "step": 12735
    },
    {
        "loss": 2.1168,
        "grad_norm": 2.671931028366089,
        "learning_rate": 8.583459100441586e-05,
        "epoch": 1.6981333333333333,
        "step": 12736
    },
    {
        "loss": 2.253,
        "grad_norm": 3.6168038845062256,
        "learning_rate": 8.577229170679963e-05,
        "epoch": 1.6982666666666666,
        "step": 12737
    },
    {
        "loss": 0.8921,
        "grad_norm": 4.043737888336182,
        "learning_rate": 8.570999804482727e-05,
        "epoch": 1.6984,
        "step": 12738
    },
    {
        "loss": 2.2271,
        "grad_norm": 3.332235813140869,
        "learning_rate": 8.564771004317324e-05,
        "epoch": 1.6985333333333332,
        "step": 12739
    },
    {
        "loss": 2.107,
        "grad_norm": 3.5648386478424072,
        "learning_rate": 8.558542772651042e-05,
        "epoch": 1.6986666666666665,
        "step": 12740
    },
    {
        "loss": 1.8382,
        "grad_norm": 3.5501317977905273,
        "learning_rate": 8.552315111950855e-05,
        "epoch": 1.6987999999999999,
        "step": 12741
    },
    {
        "loss": 1.5985,
        "grad_norm": 3.6546695232391357,
        "learning_rate": 8.546088024683583e-05,
        "epoch": 1.6989333333333332,
        "step": 12742
    },
    {
        "loss": 2.4335,
        "grad_norm": 3.239673137664795,
        "learning_rate": 8.539861513315813e-05,
        "epoch": 1.6990666666666665,
        "step": 12743
    },
    {
        "loss": 2.3532,
        "grad_norm": 3.0534560680389404,
        "learning_rate": 8.533635580313874e-05,
        "epoch": 1.6992,
        "step": 12744
    },
    {
        "loss": 2.5686,
        "grad_norm": 3.6901164054870605,
        "learning_rate": 8.527410228143877e-05,
        "epoch": 1.6993333333333334,
        "step": 12745
    },
    {
        "loss": 2.6928,
        "grad_norm": 2.635843515396118,
        "learning_rate": 8.52118545927169e-05,
        "epoch": 1.6994666666666667,
        "step": 12746
    },
    {
        "loss": 2.3206,
        "grad_norm": 2.4010062217712402,
        "learning_rate": 8.514961276162998e-05,
        "epoch": 1.6996,
        "step": 12747
    },
    {
        "loss": 2.675,
        "grad_norm": 3.9923794269561768,
        "learning_rate": 8.508737681283206e-05,
        "epoch": 1.6997333333333333,
        "step": 12748
    },
    {
        "loss": 1.3275,
        "grad_norm": 4.876955032348633,
        "learning_rate": 8.502514677097498e-05,
        "epoch": 1.6998666666666666,
        "step": 12749
    },
    {
        "loss": 1.8524,
        "grad_norm": 3.284554958343506,
        "learning_rate": 8.496292266070815e-05,
        "epoch": 1.7,
        "step": 12750
    },
    {
        "loss": 2.2321,
        "grad_norm": 3.645200490951538,
        "learning_rate": 8.49007045066789e-05,
        "epoch": 1.7001333333333335,
        "step": 12751
    },
    {
        "loss": 1.825,
        "grad_norm": 4.082536220550537,
        "learning_rate": 8.483849233353218e-05,
        "epoch": 1.7002666666666668,
        "step": 12752
    },
    {
        "loss": 2.7734,
        "grad_norm": 4.791916847229004,
        "learning_rate": 8.477628616591031e-05,
        "epoch": 1.7004000000000001,
        "step": 12753
    },
    {
        "loss": 2.1493,
        "grad_norm": 3.083051919937134,
        "learning_rate": 8.471408602845334e-05,
        "epoch": 1.7005333333333335,
        "step": 12754
    },
    {
        "loss": 1.5592,
        "grad_norm": 5.463412761688232,
        "learning_rate": 8.465189194579888e-05,
        "epoch": 1.7006666666666668,
        "step": 12755
    },
    {
        "loss": 2.4105,
        "grad_norm": 3.80993390083313,
        "learning_rate": 8.458970394258243e-05,
        "epoch": 1.7008,
        "step": 12756
    },
    {
        "loss": 2.3705,
        "grad_norm": 3.7480661869049072,
        "learning_rate": 8.452752204343674e-05,
        "epoch": 1.7009333333333334,
        "step": 12757
    },
    {
        "loss": 1.9415,
        "grad_norm": 4.477534770965576,
        "learning_rate": 8.446534627299233e-05,
        "epoch": 1.7010666666666667,
        "step": 12758
    },
    {
        "loss": 2.6806,
        "grad_norm": 3.174194097518921,
        "learning_rate": 8.440317665587704e-05,
        "epoch": 1.7012,
        "step": 12759
    },
    {
        "loss": 1.6179,
        "grad_norm": 1.8875019550323486,
        "learning_rate": 8.434101321671662e-05,
        "epoch": 1.7013333333333334,
        "step": 12760
    },
    {
        "loss": 2.234,
        "grad_norm": 3.589184284210205,
        "learning_rate": 8.427885598013449e-05,
        "epoch": 1.7014666666666667,
        "step": 12761
    },
    {
        "loss": 2.5815,
        "grad_norm": 3.154123306274414,
        "learning_rate": 8.421670497075082e-05,
        "epoch": 1.7016,
        "step": 12762
    },
    {
        "loss": 1.8159,
        "grad_norm": 5.437628269195557,
        "learning_rate": 8.415456021318421e-05,
        "epoch": 1.7017333333333333,
        "step": 12763
    },
    {
        "loss": 2.415,
        "grad_norm": 3.726954698562622,
        "learning_rate": 8.409242173205017e-05,
        "epoch": 1.7018666666666666,
        "step": 12764
    },
    {
        "loss": 1.9707,
        "grad_norm": 4.255137920379639,
        "learning_rate": 8.403028955196222e-05,
        "epoch": 1.702,
        "step": 12765
    },
    {
        "loss": 2.4444,
        "grad_norm": 4.0608954429626465,
        "learning_rate": 8.396816369753099e-05,
        "epoch": 1.7021333333333333,
        "step": 12766
    },
    {
        "loss": 2.5224,
        "grad_norm": 2.613253116607666,
        "learning_rate": 8.390604419336474e-05,
        "epoch": 1.7022666666666666,
        "step": 12767
    },
    {
        "loss": 1.5944,
        "grad_norm": 2.111297130584717,
        "learning_rate": 8.384393106406908e-05,
        "epoch": 1.7024,
        "step": 12768
    },
    {
        "loss": 2.3037,
        "grad_norm": 3.5179107189178467,
        "learning_rate": 8.378182433424739e-05,
        "epoch": 1.7025333333333332,
        "step": 12769
    },
    {
        "loss": 2.2683,
        "grad_norm": 5.007373809814453,
        "learning_rate": 8.371972402850062e-05,
        "epoch": 1.7026666666666666,
        "step": 12770
    },
    {
        "loss": 2.4989,
        "grad_norm": 3.1191821098327637,
        "learning_rate": 8.365763017142636e-05,
        "epoch": 1.7027999999999999,
        "step": 12771
    },
    {
        "loss": 1.5718,
        "grad_norm": 3.8981916904449463,
        "learning_rate": 8.359554278762061e-05,
        "epoch": 1.7029333333333332,
        "step": 12772
    },
    {
        "loss": 2.4793,
        "grad_norm": 2.8623414039611816,
        "learning_rate": 8.353346190167613e-05,
        "epoch": 1.7030666666666665,
        "step": 12773
    },
    {
        "loss": 1.5957,
        "grad_norm": 4.890831470489502,
        "learning_rate": 8.347138753818362e-05,
        "epoch": 1.7032,
        "step": 12774
    },
    {
        "loss": 2.5964,
        "grad_norm": 3.4183247089385986,
        "learning_rate": 8.340931972173082e-05,
        "epoch": 1.7033333333333334,
        "step": 12775
    },
    {
        "loss": 2.4967,
        "grad_norm": 3.7773590087890625,
        "learning_rate": 8.334725847690302e-05,
        "epoch": 1.7034666666666667,
        "step": 12776
    },
    {
        "loss": 2.6688,
        "grad_norm": 7.223966598510742,
        "learning_rate": 8.328520382828276e-05,
        "epoch": 1.7036,
        "step": 12777
    },
    {
        "loss": 2.0766,
        "grad_norm": 5.157526969909668,
        "learning_rate": 8.322315580045034e-05,
        "epoch": 1.7037333333333333,
        "step": 12778
    },
    {
        "loss": 2.2413,
        "grad_norm": 3.8663876056671143,
        "learning_rate": 8.316111441798305e-05,
        "epoch": 1.7038666666666666,
        "step": 12779
    },
    {
        "loss": 2.4623,
        "grad_norm": 2.9171361923217773,
        "learning_rate": 8.309907970545561e-05,
        "epoch": 1.704,
        "step": 12780
    },
    {
        "loss": 2.0911,
        "grad_norm": 3.1926968097686768,
        "learning_rate": 8.303705168744043e-05,
        "epoch": 1.7041333333333335,
        "step": 12781
    },
    {
        "loss": 2.0259,
        "grad_norm": 5.252302169799805,
        "learning_rate": 8.297503038850685e-05,
        "epoch": 1.7042666666666668,
        "step": 12782
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.9016900062561035,
        "learning_rate": 8.291301583322161e-05,
        "epoch": 1.7044000000000001,
        "step": 12783
    },
    {
        "loss": 2.6351,
        "grad_norm": 4.118057727813721,
        "learning_rate": 8.285100804614918e-05,
        "epoch": 1.7045333333333335,
        "step": 12784
    },
    {
        "loss": 2.1243,
        "grad_norm": 5.667623519897461,
        "learning_rate": 8.278900705185088e-05,
        "epoch": 1.7046666666666668,
        "step": 12785
    },
    {
        "loss": 1.3267,
        "grad_norm": 6.718696117401123,
        "learning_rate": 8.272701287488555e-05,
        "epoch": 1.7048,
        "step": 12786
    },
    {
        "loss": 2.6893,
        "grad_norm": 3.4823720455169678,
        "learning_rate": 8.266502553980911e-05,
        "epoch": 1.7049333333333334,
        "step": 12787
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.6163625717163086,
        "learning_rate": 8.260304507117528e-05,
        "epoch": 1.7050666666666667,
        "step": 12788
    },
    {
        "loss": 1.2725,
        "grad_norm": 4.3438239097595215,
        "learning_rate": 8.254107149353442e-05,
        "epoch": 1.7052,
        "step": 12789
    },
    {
        "loss": 1.4805,
        "grad_norm": 3.5366504192352295,
        "learning_rate": 8.247910483143479e-05,
        "epoch": 1.7053333333333334,
        "step": 12790
    },
    {
        "loss": 1.9792,
        "grad_norm": 3.896317720413208,
        "learning_rate": 8.241714510942142e-05,
        "epoch": 1.7054666666666667,
        "step": 12791
    },
    {
        "loss": 1.3794,
        "grad_norm": 3.772296190261841,
        "learning_rate": 8.235519235203668e-05,
        "epoch": 1.7056,
        "step": 12792
    },
    {
        "loss": 2.0115,
        "grad_norm": 5.3965301513671875,
        "learning_rate": 8.229324658382047e-05,
        "epoch": 1.7057333333333333,
        "step": 12793
    },
    {
        "loss": 1.7722,
        "grad_norm": 3.0658633708953857,
        "learning_rate": 8.223130782930963e-05,
        "epoch": 1.7058666666666666,
        "step": 12794
    },
    {
        "loss": 2.6585,
        "grad_norm": 2.6464242935180664,
        "learning_rate": 8.216937611303832e-05,
        "epoch": 1.706,
        "step": 12795
    },
    {
        "loss": 2.0657,
        "grad_norm": 4.539927959442139,
        "learning_rate": 8.210745145953775e-05,
        "epoch": 1.7061333333333333,
        "step": 12796
    },
    {
        "loss": 2.5581,
        "grad_norm": 4.563098907470703,
        "learning_rate": 8.204553389333673e-05,
        "epoch": 1.7062666666666666,
        "step": 12797
    },
    {
        "loss": 2.2182,
        "grad_norm": 3.6924545764923096,
        "learning_rate": 8.198362343896082e-05,
        "epoch": 1.7064,
        "step": 12798
    },
    {
        "loss": 1.379,
        "grad_norm": 4.404379844665527,
        "learning_rate": 8.19217201209333e-05,
        "epoch": 1.7065333333333332,
        "step": 12799
    },
    {
        "loss": 1.3716,
        "grad_norm": 5.592084884643555,
        "learning_rate": 8.185982396377379e-05,
        "epoch": 1.7066666666666666,
        "step": 12800
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.8685970306396484,
        "learning_rate": 8.179793499199979e-05,
        "epoch": 1.7067999999999999,
        "step": 12801
    },
    {
        "loss": 2.4956,
        "grad_norm": 3.88991379737854,
        "learning_rate": 8.173605323012592e-05,
        "epoch": 1.7069333333333332,
        "step": 12802
    },
    {
        "loss": 2.702,
        "grad_norm": 2.919757843017578,
        "learning_rate": 8.167417870266359e-05,
        "epoch": 1.7070666666666665,
        "step": 12803
    },
    {
        "loss": 2.0878,
        "grad_norm": 4.538033485412598,
        "learning_rate": 8.161231143412151e-05,
        "epoch": 1.7072,
        "step": 12804
    },
    {
        "loss": 2.1195,
        "grad_norm": 4.9794745445251465,
        "learning_rate": 8.15504514490054e-05,
        "epoch": 1.7073333333333334,
        "step": 12805
    },
    {
        "loss": 1.2457,
        "grad_norm": 3.4071805477142334,
        "learning_rate": 8.148859877181848e-05,
        "epoch": 1.7074666666666667,
        "step": 12806
    },
    {
        "loss": 2.2204,
        "grad_norm": 4.0149383544921875,
        "learning_rate": 8.142675342706067e-05,
        "epoch": 1.7076,
        "step": 12807
    },
    {
        "loss": 2.1121,
        "grad_norm": 3.2667908668518066,
        "learning_rate": 8.136491543922908e-05,
        "epoch": 1.7077333333333333,
        "step": 12808
    },
    {
        "loss": 1.5678,
        "grad_norm": 5.711789608001709,
        "learning_rate": 8.130308483281787e-05,
        "epoch": 1.7078666666666666,
        "step": 12809
    },
    {
        "loss": 1.9807,
        "grad_norm": 3.7260172367095947,
        "learning_rate": 8.124126163231846e-05,
        "epoch": 1.708,
        "step": 12810
    },
    {
        "loss": 1.8111,
        "grad_norm": 2.944967031478882,
        "learning_rate": 8.117944586221937e-05,
        "epoch": 1.7081333333333333,
        "step": 12811
    },
    {
        "loss": 1.0756,
        "grad_norm": 3.354926824569702,
        "learning_rate": 8.111763754700591e-05,
        "epoch": 1.7082666666666668,
        "step": 12812
    },
    {
        "loss": 2.087,
        "grad_norm": 3.2559361457824707,
        "learning_rate": 8.105583671116056e-05,
        "epoch": 1.7084000000000001,
        "step": 12813
    },
    {
        "loss": 1.9296,
        "grad_norm": 2.900130033493042,
        "learning_rate": 8.09940433791627e-05,
        "epoch": 1.7085333333333335,
        "step": 12814
    },
    {
        "loss": 0.9535,
        "grad_norm": 4.701478958129883,
        "learning_rate": 8.093225757548914e-05,
        "epoch": 1.7086666666666668,
        "step": 12815
    },
    {
        "loss": 1.3514,
        "grad_norm": 3.521867036819458,
        "learning_rate": 8.087047932461334e-05,
        "epoch": 1.7088,
        "step": 12816
    },
    {
        "loss": 2.7364,
        "grad_norm": 3.7647857666015625,
        "learning_rate": 8.080870865100587e-05,
        "epoch": 1.7089333333333334,
        "step": 12817
    },
    {
        "loss": 0.7836,
        "grad_norm": 4.036783218383789,
        "learning_rate": 8.074694557913413e-05,
        "epoch": 1.7090666666666667,
        "step": 12818
    },
    {
        "loss": 2.1356,
        "grad_norm": 3.529721736907959,
        "learning_rate": 8.068519013346288e-05,
        "epoch": 1.7092,
        "step": 12819
    },
    {
        "loss": 2.5333,
        "grad_norm": 2.577878713607788,
        "learning_rate": 8.06234423384539e-05,
        "epoch": 1.7093333333333334,
        "step": 12820
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.913906216621399,
        "learning_rate": 8.056170221856516e-05,
        "epoch": 1.7094666666666667,
        "step": 12821
    },
    {
        "loss": 2.1691,
        "grad_norm": 5.060239315032959,
        "learning_rate": 8.049996979825253e-05,
        "epoch": 1.7096,
        "step": 12822
    },
    {
        "loss": 2.7149,
        "grad_norm": 4.0428290367126465,
        "learning_rate": 8.043824510196819e-05,
        "epoch": 1.7097333333333333,
        "step": 12823
    },
    {
        "loss": 2.12,
        "grad_norm": 4.415895462036133,
        "learning_rate": 8.037652815416174e-05,
        "epoch": 1.7098666666666666,
        "step": 12824
    },
    {
        "loss": 2.5651,
        "grad_norm": 2.6737048625946045,
        "learning_rate": 8.031481897927938e-05,
        "epoch": 1.71,
        "step": 12825
    },
    {
        "loss": 2.5976,
        "grad_norm": 3.4907402992248535,
        "learning_rate": 8.025311760176424e-05,
        "epoch": 1.7101333333333333,
        "step": 12826
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.5016229152679443,
        "learning_rate": 8.019142404605641e-05,
        "epoch": 1.7102666666666666,
        "step": 12827
    },
    {
        "loss": 1.6028,
        "grad_norm": 5.629286766052246,
        "learning_rate": 8.012973833659315e-05,
        "epoch": 1.7104,
        "step": 12828
    },
    {
        "loss": 2.0443,
        "grad_norm": 4.593733787536621,
        "learning_rate": 8.006806049780824e-05,
        "epoch": 1.7105333333333332,
        "step": 12829
    },
    {
        "loss": 1.6222,
        "grad_norm": 5.0165886878967285,
        "learning_rate": 8.000639055413232e-05,
        "epoch": 1.7106666666666666,
        "step": 12830
    },
    {
        "loss": 2.5471,
        "grad_norm": 3.9322125911712646,
        "learning_rate": 7.994472852999339e-05,
        "epoch": 1.7107999999999999,
        "step": 12831
    },
    {
        "loss": 1.2751,
        "grad_norm": 3.808690309524536,
        "learning_rate": 7.988307444981568e-05,
        "epoch": 1.7109333333333332,
        "step": 12832
    },
    {
        "loss": 1.9221,
        "grad_norm": 5.967789173126221,
        "learning_rate": 7.982142833802086e-05,
        "epoch": 1.7110666666666665,
        "step": 12833
    },
    {
        "loss": 1.9077,
        "grad_norm": 2.440624713897705,
        "learning_rate": 7.975979021902705e-05,
        "epoch": 1.7112,
        "step": 12834
    },
    {
        "loss": 2.131,
        "grad_norm": 3.1025583744049072,
        "learning_rate": 7.96981601172493e-05,
        "epoch": 1.7113333333333334,
        "step": 12835
    },
    {
        "loss": 1.5208,
        "grad_norm": 6.666853904724121,
        "learning_rate": 7.963653805709934e-05,
        "epoch": 1.7114666666666667,
        "step": 12836
    },
    {
        "loss": 2.5483,
        "grad_norm": 2.4516942501068115,
        "learning_rate": 7.957492406298621e-05,
        "epoch": 1.7116,
        "step": 12837
    },
    {
        "loss": 1.4512,
        "grad_norm": 5.408522129058838,
        "learning_rate": 7.951331815931524e-05,
        "epoch": 1.7117333333333333,
        "step": 12838
    },
    {
        "loss": 2.3342,
        "grad_norm": 3.054534435272217,
        "learning_rate": 7.945172037048864e-05,
        "epoch": 1.7118666666666666,
        "step": 12839
    },
    {
        "loss": 1.7412,
        "grad_norm": 5.2040605545043945,
        "learning_rate": 7.939013072090571e-05,
        "epoch": 1.712,
        "step": 12840
    },
    {
        "loss": 1.9101,
        "grad_norm": 3.4754397869110107,
        "learning_rate": 7.932854923496226e-05,
        "epoch": 1.7121333333333333,
        "step": 12841
    },
    {
        "loss": 2.382,
        "grad_norm": 2.994246006011963,
        "learning_rate": 7.926697593705076e-05,
        "epoch": 1.7122666666666668,
        "step": 12842
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.6599366664886475,
        "learning_rate": 7.920541085156086e-05,
        "epoch": 1.7124000000000001,
        "step": 12843
    },
    {
        "loss": 2.8696,
        "grad_norm": 3.6396145820617676,
        "learning_rate": 7.914385400287856e-05,
        "epoch": 1.7125333333333335,
        "step": 12844
    },
    {
        "loss": 2.0968,
        "grad_norm": 3.853350877761841,
        "learning_rate": 7.908230541538679e-05,
        "epoch": 1.7126666666666668,
        "step": 12845
    },
    {
        "loss": 1.3723,
        "grad_norm": 5.187039375305176,
        "learning_rate": 7.902076511346497e-05,
        "epoch": 1.7128,
        "step": 12846
    },
    {
        "loss": 1.8199,
        "grad_norm": 4.244455814361572,
        "learning_rate": 7.895923312148972e-05,
        "epoch": 1.7129333333333334,
        "step": 12847
    },
    {
        "loss": 2.3472,
        "grad_norm": 3.8370091915130615,
        "learning_rate": 7.889770946383376e-05,
        "epoch": 1.7130666666666667,
        "step": 12848
    },
    {
        "loss": 2.4332,
        "grad_norm": 1.9328365325927734,
        "learning_rate": 7.883619416486728e-05,
        "epoch": 1.7132,
        "step": 12849
    },
    {
        "loss": 2.0645,
        "grad_norm": 3.3731255531311035,
        "learning_rate": 7.877468724895614e-05,
        "epoch": 1.7133333333333334,
        "step": 12850
    },
    {
        "loss": 2.5725,
        "grad_norm": 3.531294584274292,
        "learning_rate": 7.871318874046368e-05,
        "epoch": 1.7134666666666667,
        "step": 12851
    },
    {
        "loss": 1.7158,
        "grad_norm": 6.0477447509765625,
        "learning_rate": 7.865169866374985e-05,
        "epoch": 1.7136,
        "step": 12852
    },
    {
        "loss": 2.2778,
        "grad_norm": 5.276218891143799,
        "learning_rate": 7.859021704317092e-05,
        "epoch": 1.7137333333333333,
        "step": 12853
    },
    {
        "loss": 3.7007,
        "grad_norm": 3.5470378398895264,
        "learning_rate": 7.852874390307997e-05,
        "epoch": 1.7138666666666666,
        "step": 12854
    },
    {
        "loss": 2.4503,
        "grad_norm": 2.850499391555786,
        "learning_rate": 7.846727926782658e-05,
        "epoch": 1.714,
        "step": 12855
    },
    {
        "loss": 2.2557,
        "grad_norm": 2.639221668243408,
        "learning_rate": 7.840582316175738e-05,
        "epoch": 1.7141333333333333,
        "step": 12856
    },
    {
        "loss": 2.6501,
        "grad_norm": 2.5119552612304688,
        "learning_rate": 7.834437560921508e-05,
        "epoch": 1.7142666666666666,
        "step": 12857
    },
    {
        "loss": 1.3525,
        "grad_norm": 4.623721122741699,
        "learning_rate": 7.828293663453966e-05,
        "epoch": 1.7144,
        "step": 12858
    },
    {
        "loss": 1.9098,
        "grad_norm": 4.71041202545166,
        "learning_rate": 7.822150626206673e-05,
        "epoch": 1.7145333333333332,
        "step": 12859
    },
    {
        "loss": 2.2288,
        "grad_norm": 4.108745574951172,
        "learning_rate": 7.816008451612942e-05,
        "epoch": 1.7146666666666666,
        "step": 12860
    },
    {
        "loss": 2.3627,
        "grad_norm": 4.292988300323486,
        "learning_rate": 7.809867142105718e-05,
        "epoch": 1.7147999999999999,
        "step": 12861
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.6131770610809326,
        "learning_rate": 7.803726700117579e-05,
        "epoch": 1.7149333333333332,
        "step": 12862
    },
    {
        "loss": 1.8175,
        "grad_norm": 2.799656629562378,
        "learning_rate": 7.797587128080781e-05,
        "epoch": 1.7150666666666665,
        "step": 12863
    },
    {
        "loss": 2.4041,
        "grad_norm": 3.035672664642334,
        "learning_rate": 7.791448428427212e-05,
        "epoch": 1.7151999999999998,
        "step": 12864
    },
    {
        "loss": 1.4931,
        "grad_norm": 5.635443210601807,
        "learning_rate": 7.785310603588457e-05,
        "epoch": 1.7153333333333334,
        "step": 12865
    },
    {
        "loss": 2.0091,
        "grad_norm": 2.8842146396636963,
        "learning_rate": 7.77917365599572e-05,
        "epoch": 1.7154666666666667,
        "step": 12866
    },
    {
        "loss": 2.3582,
        "grad_norm": 3.2208380699157715,
        "learning_rate": 7.773037588079867e-05,
        "epoch": 1.7156,
        "step": 12867
    },
    {
        "loss": 2.1432,
        "grad_norm": 3.3497021198272705,
        "learning_rate": 7.766902402271398e-05,
        "epoch": 1.7157333333333333,
        "step": 12868
    },
    {
        "loss": 1.6332,
        "grad_norm": 3.1259925365448,
        "learning_rate": 7.760768101000496e-05,
        "epoch": 1.7158666666666667,
        "step": 12869
    },
    {
        "loss": 2.1469,
        "grad_norm": 3.971975326538086,
        "learning_rate": 7.754634686696994e-05,
        "epoch": 1.716,
        "step": 12870
    },
    {
        "loss": 1.1538,
        "grad_norm": 4.5192060470581055,
        "learning_rate": 7.748502161790344e-05,
        "epoch": 1.7161333333333333,
        "step": 12871
    },
    {
        "loss": 1.9715,
        "grad_norm": 3.8945271968841553,
        "learning_rate": 7.74237052870966e-05,
        "epoch": 1.7162666666666668,
        "step": 12872
    },
    {
        "loss": 1.761,
        "grad_norm": 3.2147316932678223,
        "learning_rate": 7.736239789883685e-05,
        "epoch": 1.7164000000000001,
        "step": 12873
    },
    {
        "loss": 2.3206,
        "grad_norm": 2.0644032955169678,
        "learning_rate": 7.73010994774086e-05,
        "epoch": 1.7165333333333335,
        "step": 12874
    },
    {
        "loss": 1.9368,
        "grad_norm": 4.7734150886535645,
        "learning_rate": 7.723981004709217e-05,
        "epoch": 1.7166666666666668,
        "step": 12875
    },
    {
        "loss": 2.5008,
        "grad_norm": 2.576704740524292,
        "learning_rate": 7.717852963216455e-05,
        "epoch": 1.7168,
        "step": 12876
    },
    {
        "loss": 1.0304,
        "grad_norm": 4.940218925476074,
        "learning_rate": 7.711725825689895e-05,
        "epoch": 1.7169333333333334,
        "step": 12877
    },
    {
        "loss": 3.0834,
        "grad_norm": 3.612273931503296,
        "learning_rate": 7.705599594556536e-05,
        "epoch": 1.7170666666666667,
        "step": 12878
    },
    {
        "loss": 1.6681,
        "grad_norm": 3.9016101360321045,
        "learning_rate": 7.699474272243019e-05,
        "epoch": 1.7172,
        "step": 12879
    },
    {
        "loss": 2.8592,
        "grad_norm": 3.540357828140259,
        "learning_rate": 7.693349861175555e-05,
        "epoch": 1.7173333333333334,
        "step": 12880
    },
    {
        "loss": 1.3844,
        "grad_norm": 4.6194071769714355,
        "learning_rate": 7.687226363780086e-05,
        "epoch": 1.7174666666666667,
        "step": 12881
    },
    {
        "loss": 2.2746,
        "grad_norm": 4.714304447174072,
        "learning_rate": 7.68110378248212e-05,
        "epoch": 1.7176,
        "step": 12882
    },
    {
        "loss": 1.1134,
        "grad_norm": 2.6124792098999023,
        "learning_rate": 7.67498211970686e-05,
        "epoch": 1.7177333333333333,
        "step": 12883
    },
    {
        "loss": 2.3318,
        "grad_norm": 2.983328342437744,
        "learning_rate": 7.668861377879107e-05,
        "epoch": 1.7178666666666667,
        "step": 12884
    },
    {
        "loss": 0.9488,
        "grad_norm": 4.2240095138549805,
        "learning_rate": 7.662741559423304e-05,
        "epoch": 1.718,
        "step": 12885
    },
    {
        "loss": 2.2488,
        "grad_norm": 5.6120829582214355,
        "learning_rate": 7.656622666763522e-05,
        "epoch": 1.7181333333333333,
        "step": 12886
    },
    {
        "loss": 1.1932,
        "grad_norm": 4.322684288024902,
        "learning_rate": 7.650504702323496e-05,
        "epoch": 1.7182666666666666,
        "step": 12887
    },
    {
        "loss": 2.5169,
        "grad_norm": 4.053987979888916,
        "learning_rate": 7.644387668526566e-05,
        "epoch": 1.7184,
        "step": 12888
    },
    {
        "loss": 1.988,
        "grad_norm": 3.631042242050171,
        "learning_rate": 7.638271567795694e-05,
        "epoch": 1.7185333333333332,
        "step": 12889
    },
    {
        "loss": 1.4314,
        "grad_norm": 3.55713152885437,
        "learning_rate": 7.632156402553514e-05,
        "epoch": 1.7186666666666666,
        "step": 12890
    },
    {
        "loss": 0.4798,
        "grad_norm": 1.742445945739746,
        "learning_rate": 7.626042175222237e-05,
        "epoch": 1.7187999999999999,
        "step": 12891
    },
    {
        "loss": 1.9224,
        "grad_norm": 2.9999818801879883,
        "learning_rate": 7.619928888223757e-05,
        "epoch": 1.7189333333333332,
        "step": 12892
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.6429686546325684,
        "learning_rate": 7.613816543979555e-05,
        "epoch": 1.7190666666666665,
        "step": 12893
    },
    {
        "loss": 1.465,
        "grad_norm": 3.645869731903076,
        "learning_rate": 7.60770514491075e-05,
        "epoch": 1.7191999999999998,
        "step": 12894
    },
    {
        "loss": 2.5161,
        "grad_norm": 5.13153600692749,
        "learning_rate": 7.601594693438074e-05,
        "epoch": 1.7193333333333334,
        "step": 12895
    },
    {
        "loss": 2.0152,
        "grad_norm": 4.379322528839111,
        "learning_rate": 7.595485191981929e-05,
        "epoch": 1.7194666666666667,
        "step": 12896
    },
    {
        "loss": 2.072,
        "grad_norm": 4.289514541625977,
        "learning_rate": 7.589376642962292e-05,
        "epoch": 1.7196,
        "step": 12897
    },
    {
        "loss": 1.5731,
        "grad_norm": 5.184141635894775,
        "learning_rate": 7.583269048798763e-05,
        "epoch": 1.7197333333333333,
        "step": 12898
    },
    {
        "loss": 2.7798,
        "grad_norm": 4.288040637969971,
        "learning_rate": 7.577162411910617e-05,
        "epoch": 1.7198666666666667,
        "step": 12899
    },
    {
        "loss": 1.9322,
        "grad_norm": 5.075262069702148,
        "learning_rate": 7.571056734716693e-05,
        "epoch": 1.72,
        "step": 12900
    },
    {
        "loss": 2.3228,
        "grad_norm": 4.229163646697998,
        "learning_rate": 7.564952019635459e-05,
        "epoch": 1.7201333333333333,
        "step": 12901
    },
    {
        "loss": 1.609,
        "grad_norm": 3.4249327182769775,
        "learning_rate": 7.558848269085042e-05,
        "epoch": 1.7202666666666668,
        "step": 12902
    },
    {
        "loss": 1.8969,
        "grad_norm": 2.99603009223938,
        "learning_rate": 7.552745485483144e-05,
        "epoch": 1.7204000000000002,
        "step": 12903
    },
    {
        "loss": 0.5831,
        "grad_norm": 3.5503923892974854,
        "learning_rate": 7.546643671247097e-05,
        "epoch": 1.7205333333333335,
        "step": 12904
    },
    {
        "loss": 1.2734,
        "grad_norm": 4.0797953605651855,
        "learning_rate": 7.540542828793836e-05,
        "epoch": 1.7206666666666668,
        "step": 12905
    },
    {
        "loss": 2.7269,
        "grad_norm": 4.648885726928711,
        "learning_rate": 7.534442960539958e-05,
        "epoch": 1.7208,
        "step": 12906
    },
    {
        "loss": 2.1733,
        "grad_norm": 4.51671838760376,
        "learning_rate": 7.52834406890161e-05,
        "epoch": 1.7209333333333334,
        "step": 12907
    },
    {
        "loss": 0.6543,
        "grad_norm": 3.2694239616394043,
        "learning_rate": 7.522246156294625e-05,
        "epoch": 1.7210666666666667,
        "step": 12908
    },
    {
        "loss": 1.3607,
        "grad_norm": 4.315942287445068,
        "learning_rate": 7.516149225134351e-05,
        "epoch": 1.7212,
        "step": 12909
    },
    {
        "loss": 1.3977,
        "grad_norm": 4.034663200378418,
        "learning_rate": 7.510053277835836e-05,
        "epoch": 1.7213333333333334,
        "step": 12910
    },
    {
        "loss": 2.254,
        "grad_norm": 3.1774260997772217,
        "learning_rate": 7.503958316813714e-05,
        "epoch": 1.7214666666666667,
        "step": 12911
    },
    {
        "loss": 2.6953,
        "grad_norm": 2.762650728225708,
        "learning_rate": 7.497864344482205e-05,
        "epoch": 1.7216,
        "step": 12912
    },
    {
        "loss": 1.8087,
        "grad_norm": 5.061899185180664,
        "learning_rate": 7.491771363255159e-05,
        "epoch": 1.7217333333333333,
        "step": 12913
    },
    {
        "loss": 0.8343,
        "grad_norm": 2.981752634048462,
        "learning_rate": 7.485679375546009e-05,
        "epoch": 1.7218666666666667,
        "step": 12914
    },
    {
        "loss": 2.0207,
        "grad_norm": 4.0886030197143555,
        "learning_rate": 7.479588383767837e-05,
        "epoch": 1.722,
        "step": 12915
    },
    {
        "loss": 2.2973,
        "grad_norm": 4.132832050323486,
        "learning_rate": 7.473498390333281e-05,
        "epoch": 1.7221333333333333,
        "step": 12916
    },
    {
        "loss": 1.2435,
        "grad_norm": 4.21112060546875,
        "learning_rate": 7.467409397654649e-05,
        "epoch": 1.7222666666666666,
        "step": 12917
    },
    {
        "loss": 1.8485,
        "grad_norm": 6.114133358001709,
        "learning_rate": 7.461321408143758e-05,
        "epoch": 1.7224,
        "step": 12918
    },
    {
        "loss": 2.1297,
        "grad_norm": 3.537310838699341,
        "learning_rate": 7.455234424212106e-05,
        "epoch": 1.7225333333333332,
        "step": 12919
    },
    {
        "loss": 1.9729,
        "grad_norm": 5.832226753234863,
        "learning_rate": 7.449148448270785e-05,
        "epoch": 1.7226666666666666,
        "step": 12920
    },
    {
        "loss": 1.6838,
        "grad_norm": 3.3420963287353516,
        "learning_rate": 7.443063482730455e-05,
        "epoch": 1.7227999999999999,
        "step": 12921
    },
    {
        "loss": 1.7392,
        "grad_norm": 3.934929370880127,
        "learning_rate": 7.43697953000139e-05,
        "epoch": 1.7229333333333332,
        "step": 12922
    },
    {
        "loss": 2.2824,
        "grad_norm": 3.017216920852661,
        "learning_rate": 7.430896592493455e-05,
        "epoch": 1.7230666666666665,
        "step": 12923
    },
    {
        "loss": 2.021,
        "grad_norm": 3.362802267074585,
        "learning_rate": 7.424814672616146e-05,
        "epoch": 1.7231999999999998,
        "step": 12924
    },
    {
        "loss": 2.4219,
        "grad_norm": 3.297050952911377,
        "learning_rate": 7.418733772778522e-05,
        "epoch": 1.7233333333333334,
        "step": 12925
    },
    {
        "loss": 2.0098,
        "grad_norm": 3.4677586555480957,
        "learning_rate": 7.412653895389245e-05,
        "epoch": 1.7234666666666667,
        "step": 12926
    },
    {
        "loss": 2.5399,
        "grad_norm": 3.5233941078186035,
        "learning_rate": 7.40657504285656e-05,
        "epoch": 1.7236,
        "step": 12927
    },
    {
        "loss": 1.1479,
        "grad_norm": 5.219461917877197,
        "learning_rate": 7.400497217588341e-05,
        "epoch": 1.7237333333333333,
        "step": 12928
    },
    {
        "loss": 1.6573,
        "grad_norm": 3.3220114707946777,
        "learning_rate": 7.394420421992056e-05,
        "epoch": 1.7238666666666667,
        "step": 12929
    },
    {
        "loss": 1.9854,
        "grad_norm": 3.3446075916290283,
        "learning_rate": 7.388344658474695e-05,
        "epoch": 1.724,
        "step": 12930
    },
    {
        "loss": 1.736,
        "grad_norm": 2.597452402114868,
        "learning_rate": 7.382269929442926e-05,
        "epoch": 1.7241333333333333,
        "step": 12931
    },
    {
        "loss": 1.9023,
        "grad_norm": 4.641324043273926,
        "learning_rate": 7.376196237302947e-05,
        "epoch": 1.7242666666666666,
        "step": 12932
    },
    {
        "loss": 1.883,
        "grad_norm": 3.982708692550659,
        "learning_rate": 7.370123584460591e-05,
        "epoch": 1.7244000000000002,
        "step": 12933
    },
    {
        "loss": 1.6355,
        "grad_norm": 4.742696762084961,
        "learning_rate": 7.36405197332124e-05,
        "epoch": 1.7245333333333335,
        "step": 12934
    },
    {
        "loss": 2.5383,
        "grad_norm": 2.8511993885040283,
        "learning_rate": 7.357981406289889e-05,
        "epoch": 1.7246666666666668,
        "step": 12935
    },
    {
        "loss": 2.3958,
        "grad_norm": 4.000948429107666,
        "learning_rate": 7.351911885771092e-05,
        "epoch": 1.7248,
        "step": 12936
    },
    {
        "loss": 2.4229,
        "grad_norm": 5.972475528717041,
        "learning_rate": 7.345843414169018e-05,
        "epoch": 1.7249333333333334,
        "step": 12937
    },
    {
        "loss": 1.3897,
        "grad_norm": 2.351797580718994,
        "learning_rate": 7.339775993887439e-05,
        "epoch": 1.7250666666666667,
        "step": 12938
    },
    {
        "loss": 2.0078,
        "grad_norm": 4.048752307891846,
        "learning_rate": 7.333709627329627e-05,
        "epoch": 1.7252,
        "step": 12939
    },
    {
        "loss": 1.6756,
        "grad_norm": 4.478389263153076,
        "learning_rate": 7.32764431689853e-05,
        "epoch": 1.7253333333333334,
        "step": 12940
    },
    {
        "loss": 1.5726,
        "grad_norm": 6.75413703918457,
        "learning_rate": 7.321580064996611e-05,
        "epoch": 1.7254666666666667,
        "step": 12941
    },
    {
        "loss": 1.7366,
        "grad_norm": 4.448505878448486,
        "learning_rate": 7.315516874025968e-05,
        "epoch": 1.7256,
        "step": 12942
    },
    {
        "loss": 2.3549,
        "grad_norm": 3.308293581008911,
        "learning_rate": 7.30945474638824e-05,
        "epoch": 1.7257333333333333,
        "step": 12943
    },
    {
        "loss": 1.7812,
        "grad_norm": 4.357109546661377,
        "learning_rate": 7.303393684484654e-05,
        "epoch": 1.7258666666666667,
        "step": 12944
    },
    {
        "loss": 2.5811,
        "grad_norm": 3.9899449348449707,
        "learning_rate": 7.297333690716001e-05,
        "epoch": 1.726,
        "step": 12945
    },
    {
        "loss": 2.0252,
        "grad_norm": 3.658853530883789,
        "learning_rate": 7.291274767482701e-05,
        "epoch": 1.7261333333333333,
        "step": 12946
    },
    {
        "loss": 1.7533,
        "grad_norm": 4.201626300811768,
        "learning_rate": 7.285216917184694e-05,
        "epoch": 1.7262666666666666,
        "step": 12947
    },
    {
        "loss": 1.7111,
        "grad_norm": 3.7382113933563232,
        "learning_rate": 7.279160142221501e-05,
        "epoch": 1.7264,
        "step": 12948
    },
    {
        "loss": 1.3497,
        "grad_norm": 3.4918212890625,
        "learning_rate": 7.27310444499226e-05,
        "epoch": 1.7265333333333333,
        "step": 12949
    },
    {
        "loss": 1.7441,
        "grad_norm": 4.575438499450684,
        "learning_rate": 7.267049827895642e-05,
        "epoch": 1.7266666666666666,
        "step": 12950
    },
    {
        "loss": 1.8039,
        "grad_norm": 4.372361660003662,
        "learning_rate": 7.260996293329888e-05,
        "epoch": 1.7268,
        "step": 12951
    },
    {
        "loss": 2.5848,
        "grad_norm": 3.251513719558716,
        "learning_rate": 7.254943843692843e-05,
        "epoch": 1.7269333333333332,
        "step": 12952
    },
    {
        "loss": 0.4981,
        "grad_norm": 2.341980218887329,
        "learning_rate": 7.248892481381899e-05,
        "epoch": 1.7270666666666665,
        "step": 12953
    },
    {
        "loss": 1.506,
        "grad_norm": 7.066368103027344,
        "learning_rate": 7.242842208794004e-05,
        "epoch": 1.7271999999999998,
        "step": 12954
    },
    {
        "loss": 2.5872,
        "grad_norm": 4.065762996673584,
        "learning_rate": 7.236793028325717e-05,
        "epoch": 1.7273333333333334,
        "step": 12955
    },
    {
        "loss": 1.8744,
        "grad_norm": 3.811675786972046,
        "learning_rate": 7.230744942373126e-05,
        "epoch": 1.7274666666666667,
        "step": 12956
    },
    {
        "loss": 1.6242,
        "grad_norm": 3.2487058639526367,
        "learning_rate": 7.224697953331888e-05,
        "epoch": 1.7276,
        "step": 12957
    },
    {
        "loss": 2.1092,
        "grad_norm": 4.685348033905029,
        "learning_rate": 7.21865206359726e-05,
        "epoch": 1.7277333333333333,
        "step": 12958
    },
    {
        "loss": 2.2881,
        "grad_norm": 4.354223728179932,
        "learning_rate": 7.212607275564025e-05,
        "epoch": 1.7278666666666667,
        "step": 12959
    },
    {
        "loss": 2.0618,
        "grad_norm": 3.542449951171875,
        "learning_rate": 7.206563591626535e-05,
        "epoch": 1.728,
        "step": 12960
    },
    {
        "loss": 2.4807,
        "grad_norm": 4.805379867553711,
        "learning_rate": 7.200521014178738e-05,
        "epoch": 1.7281333333333333,
        "step": 12961
    },
    {
        "loss": 1.5943,
        "grad_norm": 7.78386926651001,
        "learning_rate": 7.194479545614106e-05,
        "epoch": 1.7282666666666666,
        "step": 12962
    },
    {
        "loss": 1.775,
        "grad_norm": 3.3830673694610596,
        "learning_rate": 7.188439188325683e-05,
        "epoch": 1.7284000000000002,
        "step": 12963
    },
    {
        "loss": 3.3886,
        "grad_norm": 5.427433490753174,
        "learning_rate": 7.182399944706067e-05,
        "epoch": 1.7285333333333335,
        "step": 12964
    },
    {
        "loss": 1.3519,
        "grad_norm": 3.603893518447876,
        "learning_rate": 7.176361817147448e-05,
        "epoch": 1.7286666666666668,
        "step": 12965
    },
    {
        "loss": 1.9146,
        "grad_norm": 4.251029014587402,
        "learning_rate": 7.170324808041518e-05,
        "epoch": 1.7288000000000001,
        "step": 12966
    },
    {
        "loss": 2.3287,
        "grad_norm": 3.7752156257629395,
        "learning_rate": 7.1642889197796e-05,
        "epoch": 1.7289333333333334,
        "step": 12967
    },
    {
        "loss": 2.0413,
        "grad_norm": 4.680201053619385,
        "learning_rate": 7.15825415475248e-05,
        "epoch": 1.7290666666666668,
        "step": 12968
    },
    {
        "loss": 1.1391,
        "grad_norm": 4.002781867980957,
        "learning_rate": 7.152220515350571e-05,
        "epoch": 1.7292,
        "step": 12969
    },
    {
        "loss": 0.9467,
        "grad_norm": 3.773766279220581,
        "learning_rate": 7.146188003963832e-05,
        "epoch": 1.7293333333333334,
        "step": 12970
    },
    {
        "loss": 1.3121,
        "grad_norm": 3.1601407527923584,
        "learning_rate": 7.140156622981749e-05,
        "epoch": 1.7294666666666667,
        "step": 12971
    },
    {
        "loss": 2.6209,
        "grad_norm": 3.4512650966644287,
        "learning_rate": 7.134126374793372e-05,
        "epoch": 1.7296,
        "step": 12972
    },
    {
        "loss": 1.5831,
        "grad_norm": 3.8656435012817383,
        "learning_rate": 7.12809726178729e-05,
        "epoch": 1.7297333333333333,
        "step": 12973
    },
    {
        "loss": 2.106,
        "grad_norm": 2.192927837371826,
        "learning_rate": 7.122069286351683e-05,
        "epoch": 1.7298666666666667,
        "step": 12974
    },
    {
        "loss": 0.8916,
        "grad_norm": 3.686302661895752,
        "learning_rate": 7.116042450874223e-05,
        "epoch": 1.73,
        "step": 12975
    },
    {
        "loss": 2.4065,
        "grad_norm": 2.9522132873535156,
        "learning_rate": 7.110016757742204e-05,
        "epoch": 1.7301333333333333,
        "step": 12976
    },
    {
        "loss": 2.4801,
        "grad_norm": 3.9797956943511963,
        "learning_rate": 7.103992209342365e-05,
        "epoch": 1.7302666666666666,
        "step": 12977
    },
    {
        "loss": 2.1474,
        "grad_norm": 3.8188726902008057,
        "learning_rate": 7.097968808061081e-05,
        "epoch": 1.7304,
        "step": 12978
    },
    {
        "loss": 2.3216,
        "grad_norm": 3.5245561599731445,
        "learning_rate": 7.091946556284254e-05,
        "epoch": 1.7305333333333333,
        "step": 12979
    },
    {
        "loss": 1.8861,
        "grad_norm": 4.236080646514893,
        "learning_rate": 7.085925456397307e-05,
        "epoch": 1.7306666666666666,
        "step": 12980
    },
    {
        "loss": 1.6536,
        "grad_norm": 5.268518447875977,
        "learning_rate": 7.079905510785212e-05,
        "epoch": 1.7308,
        "step": 12981
    },
    {
        "loss": 2.8536,
        "grad_norm": 2.3779006004333496,
        "learning_rate": 7.073886721832485e-05,
        "epoch": 1.7309333333333332,
        "step": 12982
    },
    {
        "loss": 1.6307,
        "grad_norm": 5.7313666343688965,
        "learning_rate": 7.06786909192321e-05,
        "epoch": 1.7310666666666665,
        "step": 12983
    },
    {
        "loss": 2.042,
        "grad_norm": 3.2213144302368164,
        "learning_rate": 7.061852623440977e-05,
        "epoch": 1.7311999999999999,
        "step": 12984
    },
    {
        "loss": 2.3587,
        "grad_norm": 2.465174674987793,
        "learning_rate": 7.055837318768931e-05,
        "epoch": 1.7313333333333332,
        "step": 12985
    },
    {
        "loss": 2.6496,
        "grad_norm": 2.7061641216278076,
        "learning_rate": 7.049823180289744e-05,
        "epoch": 1.7314666666666667,
        "step": 12986
    },
    {
        "loss": 2.6573,
        "grad_norm": 5.113960266113281,
        "learning_rate": 7.043810210385646e-05,
        "epoch": 1.7316,
        "step": 12987
    },
    {
        "loss": 1.8763,
        "grad_norm": 5.723028182983398,
        "learning_rate": 7.037798411438426e-05,
        "epoch": 1.7317333333333333,
        "step": 12988
    },
    {
        "loss": 1.8588,
        "grad_norm": 3.713909864425659,
        "learning_rate": 7.031787785829319e-05,
        "epoch": 1.7318666666666667,
        "step": 12989
    },
    {
        "loss": 1.0165,
        "grad_norm": 5.1715826988220215,
        "learning_rate": 7.025778335939199e-05,
        "epoch": 1.732,
        "step": 12990
    },
    {
        "loss": 2.3325,
        "grad_norm": 2.875930070877075,
        "learning_rate": 7.019770064148399e-05,
        "epoch": 1.7321333333333333,
        "step": 12991
    },
    {
        "loss": 2.5954,
        "grad_norm": 3.294161319732666,
        "learning_rate": 7.013762972836847e-05,
        "epoch": 1.7322666666666666,
        "step": 12992
    },
    {
        "loss": 2.102,
        "grad_norm": 4.890026569366455,
        "learning_rate": 7.007757064383957e-05,
        "epoch": 1.7324000000000002,
        "step": 12993
    },
    {
        "loss": 3.0533,
        "grad_norm": 3.2402572631835938,
        "learning_rate": 7.00175234116869e-05,
        "epoch": 1.7325333333333335,
        "step": 12994
    },
    {
        "loss": 2.6543,
        "grad_norm": 2.4863178730010986,
        "learning_rate": 6.995748805569519e-05,
        "epoch": 1.7326666666666668,
        "step": 12995
    },
    {
        "loss": 1.3011,
        "grad_norm": 4.620603084564209,
        "learning_rate": 6.989746459964489e-05,
        "epoch": 1.7328000000000001,
        "step": 12996
    },
    {
        "loss": 2.7296,
        "grad_norm": 2.6468522548675537,
        "learning_rate": 6.983745306731167e-05,
        "epoch": 1.7329333333333334,
        "step": 12997
    },
    {
        "loss": 2.6644,
        "grad_norm": 3.6838269233703613,
        "learning_rate": 6.977745348246582e-05,
        "epoch": 1.7330666666666668,
        "step": 12998
    },
    {
        "loss": 0.9305,
        "grad_norm": 3.84155535697937,
        "learning_rate": 6.971746586887372e-05,
        "epoch": 1.7332,
        "step": 12999
    },
    {
        "loss": 2.6444,
        "grad_norm": 5.6808247566223145,
        "learning_rate": 6.965749025029648e-05,
        "epoch": 1.7333333333333334,
        "step": 13000
    },
    {
        "loss": 1.6235,
        "grad_norm": 4.153781414031982,
        "learning_rate": 6.959752665049087e-05,
        "epoch": 1.7334666666666667,
        "step": 13001
    },
    {
        "loss": 1.286,
        "grad_norm": 4.379245758056641,
        "learning_rate": 6.953757509320852e-05,
        "epoch": 1.7336,
        "step": 13002
    },
    {
        "loss": 1.9217,
        "grad_norm": 4.047685146331787,
        "learning_rate": 6.947763560219648e-05,
        "epoch": 1.7337333333333333,
        "step": 13003
    },
    {
        "loss": 1.1181,
        "grad_norm": 5.23928689956665,
        "learning_rate": 6.941770820119687e-05,
        "epoch": 1.7338666666666667,
        "step": 13004
    },
    {
        "loss": 2.0741,
        "grad_norm": 2.827580690383911,
        "learning_rate": 6.935779291394737e-05,
        "epoch": 1.734,
        "step": 13005
    },
    {
        "loss": 2.7607,
        "grad_norm": 3.486093044281006,
        "learning_rate": 6.929788976418052e-05,
        "epoch": 1.7341333333333333,
        "step": 13006
    },
    {
        "loss": 2.0634,
        "grad_norm": 2.6701364517211914,
        "learning_rate": 6.923799877562399e-05,
        "epoch": 1.7342666666666666,
        "step": 13007
    },
    {
        "loss": 2.0888,
        "grad_norm": 3.9082229137420654,
        "learning_rate": 6.917811997200112e-05,
        "epoch": 1.7344,
        "step": 13008
    },
    {
        "loss": 2.1351,
        "grad_norm": 5.181674957275391,
        "learning_rate": 6.911825337702994e-05,
        "epoch": 1.7345333333333333,
        "step": 13009
    },
    {
        "loss": 2.3189,
        "grad_norm": 2.929824113845825,
        "learning_rate": 6.905839901442368e-05,
        "epoch": 1.7346666666666666,
        "step": 13010
    },
    {
        "loss": 1.7046,
        "grad_norm": 6.367469787597656,
        "learning_rate": 6.899855690789116e-05,
        "epoch": 1.7348,
        "step": 13011
    },
    {
        "loss": 2.5491,
        "grad_norm": 4.160314083099365,
        "learning_rate": 6.893872708113587e-05,
        "epoch": 1.7349333333333332,
        "step": 13012
    },
    {
        "loss": 2.5614,
        "grad_norm": 3.429508686065674,
        "learning_rate": 6.887890955785647e-05,
        "epoch": 1.7350666666666665,
        "step": 13013
    },
    {
        "loss": 1.3263,
        "grad_norm": 2.3176417350769043,
        "learning_rate": 6.88191043617472e-05,
        "epoch": 1.7351999999999999,
        "step": 13014
    },
    {
        "loss": 1.5853,
        "grad_norm": 5.279333114624023,
        "learning_rate": 6.875931151649692e-05,
        "epoch": 1.7353333333333332,
        "step": 13015
    },
    {
        "loss": 2.0203,
        "grad_norm": 3.5049643516540527,
        "learning_rate": 6.86995310457897e-05,
        "epoch": 1.7354666666666667,
        "step": 13016
    },
    {
        "loss": 1.8787,
        "grad_norm": 3.6765851974487305,
        "learning_rate": 6.863976297330498e-05,
        "epoch": 1.7356,
        "step": 13017
    },
    {
        "loss": 2.2469,
        "grad_norm": 3.0253429412841797,
        "learning_rate": 6.858000732271702e-05,
        "epoch": 1.7357333333333334,
        "step": 13018
    },
    {
        "loss": 2.0143,
        "grad_norm": 3.3521130084991455,
        "learning_rate": 6.85202641176951e-05,
        "epoch": 1.7358666666666667,
        "step": 13019
    },
    {
        "loss": 1.8228,
        "grad_norm": 3.464231252670288,
        "learning_rate": 6.846053338190393e-05,
        "epoch": 1.736,
        "step": 13020
    },
    {
        "loss": 0.7422,
        "grad_norm": 2.861398696899414,
        "learning_rate": 6.840081513900296e-05,
        "epoch": 1.7361333333333333,
        "step": 13021
    },
    {
        "loss": 2.2634,
        "grad_norm": 4.861997604370117,
        "learning_rate": 6.834110941264679e-05,
        "epoch": 1.7362666666666666,
        "step": 13022
    },
    {
        "loss": 2.603,
        "grad_norm": 3.6895391941070557,
        "learning_rate": 6.828141622648495e-05,
        "epoch": 1.7364000000000002,
        "step": 13023
    },
    {
        "loss": 1.6292,
        "grad_norm": 4.525726318359375,
        "learning_rate": 6.822173560416232e-05,
        "epoch": 1.7365333333333335,
        "step": 13024
    },
    {
        "loss": 2.3071,
        "grad_norm": 4.484885215759277,
        "learning_rate": 6.816206756931839e-05,
        "epoch": 1.7366666666666668,
        "step": 13025
    },
    {
        "loss": 1.6359,
        "grad_norm": 3.0978355407714844,
        "learning_rate": 6.810241214558824e-05,
        "epoch": 1.7368000000000001,
        "step": 13026
    },
    {
        "loss": 1.7518,
        "grad_norm": 4.61735725402832,
        "learning_rate": 6.804276935660106e-05,
        "epoch": 1.7369333333333334,
        "step": 13027
    },
    {
        "loss": 1.4602,
        "grad_norm": 4.557155132293701,
        "learning_rate": 6.798313922598182e-05,
        "epoch": 1.7370666666666668,
        "step": 13028
    },
    {
        "loss": 1.814,
        "grad_norm": 3.0929315090179443,
        "learning_rate": 6.792352177735038e-05,
        "epoch": 1.7372,
        "step": 13029
    },
    {
        "loss": 2.3008,
        "grad_norm": 3.1263630390167236,
        "learning_rate": 6.786391703432124e-05,
        "epoch": 1.7373333333333334,
        "step": 13030
    },
    {
        "loss": 3.081,
        "grad_norm": 3.196049213409424,
        "learning_rate": 6.780432502050407e-05,
        "epoch": 1.7374666666666667,
        "step": 13031
    },
    {
        "loss": 1.5475,
        "grad_norm": 4.141134738922119,
        "learning_rate": 6.774474575950334e-05,
        "epoch": 1.7376,
        "step": 13032
    },
    {
        "loss": 2.0594,
        "grad_norm": 6.0589399337768555,
        "learning_rate": 6.768517927491884e-05,
        "epoch": 1.7377333333333334,
        "step": 13033
    },
    {
        "loss": 2.5298,
        "grad_norm": 3.4297573566436768,
        "learning_rate": 6.76256255903448e-05,
        "epoch": 1.7378666666666667,
        "step": 13034
    },
    {
        "loss": 1.8893,
        "grad_norm": 2.8952701091766357,
        "learning_rate": 6.756608472937109e-05,
        "epoch": 1.738,
        "step": 13035
    },
    {
        "loss": 1.9772,
        "grad_norm": 3.8263065814971924,
        "learning_rate": 6.750655671558142e-05,
        "epoch": 1.7381333333333333,
        "step": 13036
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.7980613708496094,
        "learning_rate": 6.744704157255537e-05,
        "epoch": 1.7382666666666666,
        "step": 13037
    },
    {
        "loss": 2.8797,
        "grad_norm": 6.703031539916992,
        "learning_rate": 6.738753932386719e-05,
        "epoch": 1.7384,
        "step": 13038
    },
    {
        "loss": 2.5553,
        "grad_norm": 3.5016839504241943,
        "learning_rate": 6.732804999308585e-05,
        "epoch": 1.7385333333333333,
        "step": 13039
    },
    {
        "loss": 2.6467,
        "grad_norm": 2.2001850605010986,
        "learning_rate": 6.72685736037752e-05,
        "epoch": 1.7386666666666666,
        "step": 13040
    },
    {
        "loss": 1.8638,
        "grad_norm": 5.063591957092285,
        "learning_rate": 6.720911017949396e-05,
        "epoch": 1.7388,
        "step": 13041
    },
    {
        "loss": 1.6073,
        "grad_norm": 3.742459774017334,
        "learning_rate": 6.714965974379602e-05,
        "epoch": 1.7389333333333332,
        "step": 13042
    },
    {
        "loss": 3.0746,
        "grad_norm": 3.535320997238159,
        "learning_rate": 6.709022232022981e-05,
        "epoch": 1.7390666666666665,
        "step": 13043
    },
    {
        "loss": 2.6479,
        "grad_norm": 4.162395000457764,
        "learning_rate": 6.703079793233868e-05,
        "epoch": 1.7391999999999999,
        "step": 13044
    },
    {
        "loss": 2.1093,
        "grad_norm": 3.714266300201416,
        "learning_rate": 6.697138660366069e-05,
        "epoch": 1.7393333333333332,
        "step": 13045
    },
    {
        "loss": 2.4359,
        "grad_norm": 2.756465435028076,
        "learning_rate": 6.6911988357729e-05,
        "epoch": 1.7394666666666667,
        "step": 13046
    },
    {
        "loss": 2.6448,
        "grad_norm": 2.573530435562134,
        "learning_rate": 6.685260321807173e-05,
        "epoch": 1.7396,
        "step": 13047
    },
    {
        "loss": 2.4335,
        "grad_norm": 4.687833309173584,
        "learning_rate": 6.679323120821103e-05,
        "epoch": 1.7397333333333334,
        "step": 13048
    },
    {
        "loss": 1.5171,
        "grad_norm": 5.7720465660095215,
        "learning_rate": 6.673387235166471e-05,
        "epoch": 1.7398666666666667,
        "step": 13049
    },
    {
        "loss": 1.9028,
        "grad_norm": 3.062673807144165,
        "learning_rate": 6.667452667194478e-05,
        "epoch": 1.74,
        "step": 13050
    },
    {
        "loss": 2.5342,
        "grad_norm": 3.50529146194458,
        "learning_rate": 6.66151941925585e-05,
        "epoch": 1.7401333333333333,
        "step": 13051
    },
    {
        "loss": 1.9367,
        "grad_norm": 4.918645858764648,
        "learning_rate": 6.655587493700759e-05,
        "epoch": 1.7402666666666666,
        "step": 13052
    },
    {
        "loss": 2.4308,
        "grad_norm": 4.350762844085693,
        "learning_rate": 6.649656892878856e-05,
        "epoch": 1.7404,
        "step": 13053
    },
    {
        "loss": 2.2484,
        "grad_norm": 4.2016801834106445,
        "learning_rate": 6.643727619139259e-05,
        "epoch": 1.7405333333333335,
        "step": 13054
    },
    {
        "loss": 2.3425,
        "grad_norm": 3.7329893112182617,
        "learning_rate": 6.637799674830591e-05,
        "epoch": 1.7406666666666668,
        "step": 13055
    },
    {
        "loss": 2.1256,
        "grad_norm": 5.308940410614014,
        "learning_rate": 6.631873062300953e-05,
        "epoch": 1.7408000000000001,
        "step": 13056
    },
    {
        "loss": 3.01,
        "grad_norm": 4.7359619140625,
        "learning_rate": 6.625947783897845e-05,
        "epoch": 1.7409333333333334,
        "step": 13057
    },
    {
        "loss": 2.0443,
        "grad_norm": 3.205805540084839,
        "learning_rate": 6.62002384196833e-05,
        "epoch": 1.7410666666666668,
        "step": 13058
    },
    {
        "loss": 1.9349,
        "grad_norm": 5.1164679527282715,
        "learning_rate": 6.614101238858876e-05,
        "epoch": 1.7412,
        "step": 13059
    },
    {
        "loss": 1.5152,
        "grad_norm": 5.0060930252075195,
        "learning_rate": 6.608179976915469e-05,
        "epoch": 1.7413333333333334,
        "step": 13060
    },
    {
        "loss": 2.5205,
        "grad_norm": 3.925591230392456,
        "learning_rate": 6.602260058483534e-05,
        "epoch": 1.7414666666666667,
        "step": 13061
    },
    {
        "loss": 1.3698,
        "grad_norm": 3.8471872806549072,
        "learning_rate": 6.596341485907966e-05,
        "epoch": 1.7416,
        "step": 13062
    },
    {
        "loss": 1.7456,
        "grad_norm": 2.7777390480041504,
        "learning_rate": 6.590424261533121e-05,
        "epoch": 1.7417333333333334,
        "step": 13063
    },
    {
        "loss": 2.1698,
        "grad_norm": 3.662672519683838,
        "learning_rate": 6.584508387702854e-05,
        "epoch": 1.7418666666666667,
        "step": 13064
    },
    {
        "loss": 2.5527,
        "grad_norm": 2.456017017364502,
        "learning_rate": 6.578593866760454e-05,
        "epoch": 1.742,
        "step": 13065
    },
    {
        "loss": 2.5386,
        "grad_norm": 3.4886748790740967,
        "learning_rate": 6.57268070104867e-05,
        "epoch": 1.7421333333333333,
        "step": 13066
    },
    {
        "loss": 1.658,
        "grad_norm": 3.348822593688965,
        "learning_rate": 6.566768892909749e-05,
        "epoch": 1.7422666666666666,
        "step": 13067
    },
    {
        "loss": 1.8501,
        "grad_norm": 4.4833269119262695,
        "learning_rate": 6.560858444685369e-05,
        "epoch": 1.7424,
        "step": 13068
    },
    {
        "loss": 1.7246,
        "grad_norm": 4.572613716125488,
        "learning_rate": 6.554949358716667e-05,
        "epoch": 1.7425333333333333,
        "step": 13069
    },
    {
        "loss": 1.3939,
        "grad_norm": 4.9533915519714355,
        "learning_rate": 6.549041637344275e-05,
        "epoch": 1.7426666666666666,
        "step": 13070
    },
    {
        "loss": 1.4671,
        "grad_norm": 3.826756238937378,
        "learning_rate": 6.543135282908246e-05,
        "epoch": 1.7428,
        "step": 13071
    },
    {
        "loss": 1.9749,
        "grad_norm": 3.7673444747924805,
        "learning_rate": 6.5372302977481e-05,
        "epoch": 1.7429333333333332,
        "step": 13072
    },
    {
        "loss": 1.2215,
        "grad_norm": 5.340514183044434,
        "learning_rate": 6.531326684202846e-05,
        "epoch": 1.7430666666666665,
        "step": 13073
    },
    {
        "loss": 2.1711,
        "grad_norm": 3.434566020965576,
        "learning_rate": 6.52542444461091e-05,
        "epoch": 1.7431999999999999,
        "step": 13074
    },
    {
        "loss": 2.1058,
        "grad_norm": 2.796365737915039,
        "learning_rate": 6.519523581310177e-05,
        "epoch": 1.7433333333333332,
        "step": 13075
    },
    {
        "loss": 3.1755,
        "grad_norm": 4.3100152015686035,
        "learning_rate": 6.513624096638027e-05,
        "epoch": 1.7434666666666667,
        "step": 13076
    },
    {
        "loss": 1.8348,
        "grad_norm": 4.5436110496521,
        "learning_rate": 6.507725992931253e-05,
        "epoch": 1.7436,
        "step": 13077
    },
    {
        "loss": 1.9202,
        "grad_norm": 5.669493675231934,
        "learning_rate": 6.501829272526098e-05,
        "epoch": 1.7437333333333334,
        "step": 13078
    },
    {
        "loss": 2.5935,
        "grad_norm": 3.6042611598968506,
        "learning_rate": 6.495933937758302e-05,
        "epoch": 1.7438666666666667,
        "step": 13079
    },
    {
        "loss": 2.5076,
        "grad_norm": 2.8177576065063477,
        "learning_rate": 6.490039990963013e-05,
        "epoch": 1.744,
        "step": 13080
    },
    {
        "loss": 1.0404,
        "grad_norm": 4.031930446624756,
        "learning_rate": 6.484147434474843e-05,
        "epoch": 1.7441333333333333,
        "step": 13081
    },
    {
        "loss": 2.009,
        "grad_norm": 4.457045078277588,
        "learning_rate": 6.47825627062784e-05,
        "epoch": 1.7442666666666666,
        "step": 13082
    },
    {
        "loss": 1.888,
        "grad_norm": 3.816873073577881,
        "learning_rate": 6.472366501755542e-05,
        "epoch": 1.7444,
        "step": 13083
    },
    {
        "loss": 2.9259,
        "grad_norm": 3.9683120250701904,
        "learning_rate": 6.466478130190883e-05,
        "epoch": 1.7445333333333335,
        "step": 13084
    },
    {
        "loss": 2.0234,
        "grad_norm": 3.4050798416137695,
        "learning_rate": 6.460591158266299e-05,
        "epoch": 1.7446666666666668,
        "step": 13085
    },
    {
        "loss": 2.1713,
        "grad_norm": 2.5604066848754883,
        "learning_rate": 6.454705588313592e-05,
        "epoch": 1.7448000000000001,
        "step": 13086
    },
    {
        "loss": 1.1923,
        "grad_norm": 6.062148094177246,
        "learning_rate": 6.448821422664083e-05,
        "epoch": 1.7449333333333334,
        "step": 13087
    },
    {
        "loss": 1.8227,
        "grad_norm": 2.328887939453125,
        "learning_rate": 6.442938663648521e-05,
        "epoch": 1.7450666666666668,
        "step": 13088
    },
    {
        "loss": 2.2968,
        "grad_norm": 3.73282527923584,
        "learning_rate": 6.437057313597075e-05,
        "epoch": 1.7452,
        "step": 13089
    },
    {
        "loss": 2.672,
        "grad_norm": 4.728776931762695,
        "learning_rate": 6.431177374839367e-05,
        "epoch": 1.7453333333333334,
        "step": 13090
    },
    {
        "loss": 2.1864,
        "grad_norm": 3.3192076683044434,
        "learning_rate": 6.425298849704448e-05,
        "epoch": 1.7454666666666667,
        "step": 13091
    },
    {
        "loss": 2.5377,
        "grad_norm": 2.2348406314849854,
        "learning_rate": 6.41942174052085e-05,
        "epoch": 1.7456,
        "step": 13092
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.4659266471862793,
        "learning_rate": 6.413546049616486e-05,
        "epoch": 1.7457333333333334,
        "step": 13093
    },
    {
        "loss": 2.4933,
        "grad_norm": 2.9635417461395264,
        "learning_rate": 6.40767177931878e-05,
        "epoch": 1.7458666666666667,
        "step": 13094
    },
    {
        "loss": 0.7804,
        "grad_norm": 3.0362067222595215,
        "learning_rate": 6.401798931954497e-05,
        "epoch": 1.746,
        "step": 13095
    },
    {
        "loss": 1.5928,
        "grad_norm": 4.28227424621582,
        "learning_rate": 6.395927509849914e-05,
        "epoch": 1.7461333333333333,
        "step": 13096
    },
    {
        "loss": 1.7948,
        "grad_norm": 3.413054943084717,
        "learning_rate": 6.390057515330742e-05,
        "epoch": 1.7462666666666666,
        "step": 13097
    },
    {
        "loss": 2.1288,
        "grad_norm": 3.258758783340454,
        "learning_rate": 6.384188950722088e-05,
        "epoch": 1.7464,
        "step": 13098
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.776003837585449,
        "learning_rate": 6.378321818348511e-05,
        "epoch": 1.7465333333333333,
        "step": 13099
    },
    {
        "loss": 2.5046,
        "grad_norm": 3.466718912124634,
        "learning_rate": 6.372456120533988e-05,
        "epoch": 1.7466666666666666,
        "step": 13100
    },
    {
        "loss": 2.2546,
        "grad_norm": 2.9060659408569336,
        "learning_rate": 6.366591859601969e-05,
        "epoch": 1.7468,
        "step": 13101
    },
    {
        "loss": 2.614,
        "grad_norm": 3.1054317951202393,
        "learning_rate": 6.360729037875297e-05,
        "epoch": 1.7469333333333332,
        "step": 13102
    },
    {
        "loss": 2.0803,
        "grad_norm": 3.210085868835449,
        "learning_rate": 6.354867657676253e-05,
        "epoch": 1.7470666666666665,
        "step": 13103
    },
    {
        "loss": 1.8863,
        "grad_norm": 3.190107822418213,
        "learning_rate": 6.349007721326537e-05,
        "epoch": 1.7471999999999999,
        "step": 13104
    },
    {
        "loss": 2.4899,
        "grad_norm": 3.208624839782715,
        "learning_rate": 6.343149231147302e-05,
        "epoch": 1.7473333333333332,
        "step": 13105
    },
    {
        "loss": 1.8051,
        "grad_norm": 4.416419982910156,
        "learning_rate": 6.337292189459144e-05,
        "epoch": 1.7474666666666665,
        "step": 13106
    },
    {
        "loss": 1.3994,
        "grad_norm": 4.869303226470947,
        "learning_rate": 6.331436598582003e-05,
        "epoch": 1.7476,
        "step": 13107
    },
    {
        "loss": 2.564,
        "grad_norm": 5.1872406005859375,
        "learning_rate": 6.325582460835339e-05,
        "epoch": 1.7477333333333334,
        "step": 13108
    },
    {
        "loss": 2.4016,
        "grad_norm": 3.308678150177002,
        "learning_rate": 6.319729778537973e-05,
        "epoch": 1.7478666666666667,
        "step": 13109
    },
    {
        "loss": 2.0351,
        "grad_norm": 5.726978778839111,
        "learning_rate": 6.313878554008191e-05,
        "epoch": 1.748,
        "step": 13110
    },
    {
        "loss": 2.4721,
        "grad_norm": 3.5434019565582275,
        "learning_rate": 6.308028789563673e-05,
        "epoch": 1.7481333333333333,
        "step": 13111
    },
    {
        "loss": 1.5272,
        "grad_norm": 5.040860652923584,
        "learning_rate": 6.302180487521534e-05,
        "epoch": 1.7482666666666666,
        "step": 13112
    },
    {
        "loss": 1.8364,
        "grad_norm": 2.8539018630981445,
        "learning_rate": 6.296333650198292e-05,
        "epoch": 1.7484,
        "step": 13113
    },
    {
        "loss": 1.5509,
        "grad_norm": 7.915312767028809,
        "learning_rate": 6.290488279909909e-05,
        "epoch": 1.7485333333333335,
        "step": 13114
    },
    {
        "loss": 0.6432,
        "grad_norm": 3.032881259918213,
        "learning_rate": 6.284644378971785e-05,
        "epoch": 1.7486666666666668,
        "step": 13115
    },
    {
        "loss": 1.9525,
        "grad_norm": 3.917466878890991,
        "learning_rate": 6.278801949698657e-05,
        "epoch": 1.7488000000000001,
        "step": 13116
    },
    {
        "loss": 3.743,
        "grad_norm": 2.6549792289733887,
        "learning_rate": 6.272960994404768e-05,
        "epoch": 1.7489333333333335,
        "step": 13117
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.943875551223755,
        "learning_rate": 6.267121515403715e-05,
        "epoch": 1.7490666666666668,
        "step": 13118
    },
    {
        "loss": 1.944,
        "grad_norm": 4.506867408752441,
        "learning_rate": 6.261283515008562e-05,
        "epoch": 1.7492,
        "step": 13119
    },
    {
        "loss": 1.9456,
        "grad_norm": 4.638416767120361,
        "learning_rate": 6.255446995531751e-05,
        "epoch": 1.7493333333333334,
        "step": 13120
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.0753285884857178,
        "learning_rate": 6.249611959285142e-05,
        "epoch": 1.7494666666666667,
        "step": 13121
    },
    {
        "loss": 1.8548,
        "grad_norm": 4.273198127746582,
        "learning_rate": 6.243778408580002e-05,
        "epoch": 1.7496,
        "step": 13122
    },
    {
        "loss": 2.2109,
        "grad_norm": 2.5124850273132324,
        "learning_rate": 6.237946345727048e-05,
        "epoch": 1.7497333333333334,
        "step": 13123
    },
    {
        "loss": 2.3869,
        "grad_norm": 3.6876776218414307,
        "learning_rate": 6.232115773036364e-05,
        "epoch": 1.7498666666666667,
        "step": 13124
    },
    {
        "loss": 2.7498,
        "grad_norm": 4.411249160766602,
        "learning_rate": 6.226286692817448e-05,
        "epoch": 1.75,
        "step": 13125
    },
    {
        "loss": 1.3227,
        "grad_norm": 2.7702999114990234,
        "learning_rate": 6.220459107379245e-05,
        "epoch": 1.7501333333333333,
        "step": 13126
    },
    {
        "loss": 1.9752,
        "grad_norm": 3.721552610397339,
        "learning_rate": 6.214633019030069e-05,
        "epoch": 1.7502666666666666,
        "step": 13127
    },
    {
        "loss": 2.557,
        "grad_norm": 2.7271175384521484,
        "learning_rate": 6.208808430077637e-05,
        "epoch": 1.7504,
        "step": 13128
    },
    {
        "loss": 2.3534,
        "grad_norm": 4.288992404937744,
        "learning_rate": 6.20298534282912e-05,
        "epoch": 1.7505333333333333,
        "step": 13129
    },
    {
        "loss": 2.3243,
        "grad_norm": 3.9987592697143555,
        "learning_rate": 6.197163759591044e-05,
        "epoch": 1.7506666666666666,
        "step": 13130
    },
    {
        "loss": 1.6236,
        "grad_norm": 3.6160945892333984,
        "learning_rate": 6.191343682669363e-05,
        "epoch": 1.7508,
        "step": 13131
    },
    {
        "loss": 2.6954,
        "grad_norm": 3.8246593475341797,
        "learning_rate": 6.185525114369412e-05,
        "epoch": 1.7509333333333332,
        "step": 13132
    },
    {
        "loss": 2.538,
        "grad_norm": 3.0216927528381348,
        "learning_rate": 6.179708056995974e-05,
        "epoch": 1.7510666666666665,
        "step": 13133
    },
    {
        "loss": 1.7773,
        "grad_norm": 3.35433030128479,
        "learning_rate": 6.173892512853182e-05,
        "epoch": 1.7511999999999999,
        "step": 13134
    },
    {
        "loss": 1.7827,
        "grad_norm": 3.457521438598633,
        "learning_rate": 6.168078484244613e-05,
        "epoch": 1.7513333333333332,
        "step": 13135
    },
    {
        "loss": 2.5487,
        "grad_norm": 4.246823310852051,
        "learning_rate": 6.162265973473212e-05,
        "epoch": 1.7514666666666665,
        "step": 13136
    },
    {
        "loss": 1.8607,
        "grad_norm": 2.8612024784088135,
        "learning_rate": 6.156454982841326e-05,
        "epoch": 1.7516,
        "step": 13137
    },
    {
        "loss": 2.1128,
        "grad_norm": 2.5009241104125977,
        "learning_rate": 6.150645514650729e-05,
        "epoch": 1.7517333333333334,
        "step": 13138
    },
    {
        "loss": 0.6071,
        "grad_norm": 2.6409058570861816,
        "learning_rate": 6.14483757120256e-05,
        "epoch": 1.7518666666666667,
        "step": 13139
    },
    {
        "loss": 2.483,
        "grad_norm": 3.4020752906799316,
        "learning_rate": 6.139031154797365e-05,
        "epoch": 1.752,
        "step": 13140
    },
    {
        "loss": 2.3971,
        "grad_norm": 4.700544357299805,
        "learning_rate": 6.133226267735074e-05,
        "epoch": 1.7521333333333333,
        "step": 13141
    },
    {
        "loss": 2.5103,
        "grad_norm": 3.6592090129852295,
        "learning_rate": 6.127422912315046e-05,
        "epoch": 1.7522666666666666,
        "step": 13142
    },
    {
        "loss": 2.8786,
        "grad_norm": 3.974625825881958,
        "learning_rate": 6.121621090835983e-05,
        "epoch": 1.7524,
        "step": 13143
    },
    {
        "loss": 2.716,
        "grad_norm": 7.018012046813965,
        "learning_rate": 6.11582080559605e-05,
        "epoch": 1.7525333333333335,
        "step": 13144
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.957119941711426,
        "learning_rate": 6.110022058892704e-05,
        "epoch": 1.7526666666666668,
        "step": 13145
    },
    {
        "loss": 2.1017,
        "grad_norm": 3.6140832901000977,
        "learning_rate": 6.104224853022875e-05,
        "epoch": 1.7528000000000001,
        "step": 13146
    },
    {
        "loss": 1.0618,
        "grad_norm": 3.377152681350708,
        "learning_rate": 6.09842919028287e-05,
        "epoch": 1.7529333333333335,
        "step": 13147
    },
    {
        "loss": 1.6735,
        "grad_norm": 3.985178232192993,
        "learning_rate": 6.092635072968357e-05,
        "epoch": 1.7530666666666668,
        "step": 13148
    },
    {
        "loss": 0.6122,
        "grad_norm": 2.122126579284668,
        "learning_rate": 6.086842503374406e-05,
        "epoch": 1.7532,
        "step": 13149
    },
    {
        "loss": 2.1164,
        "grad_norm": 4.681003093719482,
        "learning_rate": 6.081051483795457e-05,
        "epoch": 1.7533333333333334,
        "step": 13150
    },
    {
        "loss": 2.4564,
        "grad_norm": 4.00991153717041,
        "learning_rate": 6.075262016525385e-05,
        "epoch": 1.7534666666666667,
        "step": 13151
    },
    {
        "loss": 1.7775,
        "grad_norm": 3.138545036315918,
        "learning_rate": 6.0694741038573886e-05,
        "epoch": 1.7536,
        "step": 13152
    },
    {
        "loss": 2.029,
        "grad_norm": 4.512443542480469,
        "learning_rate": 6.063687748084117e-05,
        "epoch": 1.7537333333333334,
        "step": 13153
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.490314245223999,
        "learning_rate": 6.057902951497514e-05,
        "epoch": 1.7538666666666667,
        "step": 13154
    },
    {
        "loss": 2.4828,
        "grad_norm": 3.3607888221740723,
        "learning_rate": 6.0521197163889875e-05,
        "epoch": 1.754,
        "step": 13155
    },
    {
        "loss": 1.8409,
        "grad_norm": 8.077913284301758,
        "learning_rate": 6.046338045049304e-05,
        "epoch": 1.7541333333333333,
        "step": 13156
    },
    {
        "loss": 2.3073,
        "grad_norm": 2.4238967895507812,
        "learning_rate": 6.040557939768594e-05,
        "epoch": 1.7542666666666666,
        "step": 13157
    },
    {
        "loss": 1.481,
        "grad_norm": 3.717398166656494,
        "learning_rate": 6.034779402836375e-05,
        "epoch": 1.7544,
        "step": 13158
    },
    {
        "loss": 1.9632,
        "grad_norm": 3.8640096187591553,
        "learning_rate": 6.029002436541533e-05,
        "epoch": 1.7545333333333333,
        "step": 13159
    },
    {
        "loss": 1.2087,
        "grad_norm": 3.0437958240509033,
        "learning_rate": 6.0232270431723695e-05,
        "epoch": 1.7546666666666666,
        "step": 13160
    },
    {
        "loss": 2.3414,
        "grad_norm": 4.885283946990967,
        "learning_rate": 6.017453225016525e-05,
        "epoch": 1.7548,
        "step": 13161
    },
    {
        "loss": 2.6798,
        "grad_norm": 3.645606756210327,
        "learning_rate": 6.0116809843610255e-05,
        "epoch": 1.7549333333333332,
        "step": 13162
    },
    {
        "loss": 2.1991,
        "grad_norm": 2.843865394592285,
        "learning_rate": 6.005910323492269e-05,
        "epoch": 1.7550666666666666,
        "step": 13163
    },
    {
        "loss": 1.639,
        "grad_norm": 3.2892706394195557,
        "learning_rate": 6.0001412446960405e-05,
        "epoch": 1.7551999999999999,
        "step": 13164
    },
    {
        "loss": 1.6284,
        "grad_norm": 4.5052289962768555,
        "learning_rate": 5.99437375025752e-05,
        "epoch": 1.7553333333333332,
        "step": 13165
    },
    {
        "loss": 1.7882,
        "grad_norm": 3.2045390605926514,
        "learning_rate": 5.9886078424611825e-05,
        "epoch": 1.7554666666666665,
        "step": 13166
    },
    {
        "loss": 2.7317,
        "grad_norm": 3.4017014503479004,
        "learning_rate": 5.982843523590957e-05,
        "epoch": 1.7556,
        "step": 13167
    },
    {
        "loss": 2.5261,
        "grad_norm": 3.3931498527526855,
        "learning_rate": 5.9770807959300876e-05,
        "epoch": 1.7557333333333334,
        "step": 13168
    },
    {
        "loss": 1.8546,
        "grad_norm": 4.418203353881836,
        "learning_rate": 5.9713196617612364e-05,
        "epoch": 1.7558666666666667,
        "step": 13169
    },
    {
        "loss": 0.819,
        "grad_norm": 3.265024423599243,
        "learning_rate": 5.965560123366396e-05,
        "epoch": 1.756,
        "step": 13170
    },
    {
        "loss": 1.9052,
        "grad_norm": 3.667515516281128,
        "learning_rate": 5.959802183026936e-05,
        "epoch": 1.7561333333333333,
        "step": 13171
    },
    {
        "loss": 2.239,
        "grad_norm": 4.305724620819092,
        "learning_rate": 5.954045843023583e-05,
        "epoch": 1.7562666666666666,
        "step": 13172
    },
    {
        "loss": 1.8688,
        "grad_norm": 4.160244941711426,
        "learning_rate": 5.948291105636462e-05,
        "epoch": 1.7564,
        "step": 13173
    },
    {
        "loss": 2.7442,
        "grad_norm": 2.2845559120178223,
        "learning_rate": 5.942537973145064e-05,
        "epoch": 1.7565333333333333,
        "step": 13174
    },
    {
        "loss": 1.7992,
        "grad_norm": 3.2774126529693604,
        "learning_rate": 5.9367864478281734e-05,
        "epoch": 1.7566666666666668,
        "step": 13175
    },
    {
        "loss": 1.929,
        "grad_norm": 3.2421975135803223,
        "learning_rate": 5.9310365319640295e-05,
        "epoch": 1.7568000000000001,
        "step": 13176
    },
    {
        "loss": 0.7881,
        "grad_norm": 4.464268207550049,
        "learning_rate": 5.925288227830168e-05,
        "epoch": 1.7569333333333335,
        "step": 13177
    },
    {
        "loss": 2.0812,
        "grad_norm": 4.346938610076904,
        "learning_rate": 5.919541537703537e-05,
        "epoch": 1.7570666666666668,
        "step": 13178
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.9111087322235107,
        "learning_rate": 5.9137964638604035e-05,
        "epoch": 1.7572,
        "step": 13179
    },
    {
        "loss": 3.243,
        "grad_norm": 3.263192892074585,
        "learning_rate": 5.9080530085764166e-05,
        "epoch": 1.7573333333333334,
        "step": 13180
    },
    {
        "loss": 2.6505,
        "grad_norm": 3.513582468032837,
        "learning_rate": 5.902311174126562e-05,
        "epoch": 1.7574666666666667,
        "step": 13181
    },
    {
        "loss": 1.9433,
        "grad_norm": 4.532567977905273,
        "learning_rate": 5.8965709627852284e-05,
        "epoch": 1.7576,
        "step": 13182
    },
    {
        "loss": 2.0552,
        "grad_norm": 3.1028292179107666,
        "learning_rate": 5.890832376826116e-05,
        "epoch": 1.7577333333333334,
        "step": 13183
    },
    {
        "loss": 1.7372,
        "grad_norm": 3.918466806411743,
        "learning_rate": 5.885095418522291e-05,
        "epoch": 1.7578666666666667,
        "step": 13184
    },
    {
        "loss": 1.1318,
        "grad_norm": 4.51624870300293,
        "learning_rate": 5.879360090146203e-05,
        "epoch": 1.758,
        "step": 13185
    },
    {
        "loss": 2.6591,
        "grad_norm": 3.4926483631134033,
        "learning_rate": 5.8736263939696245e-05,
        "epoch": 1.7581333333333333,
        "step": 13186
    },
    {
        "loss": 2.2917,
        "grad_norm": 3.3394665718078613,
        "learning_rate": 5.867894332263677e-05,
        "epoch": 1.7582666666666666,
        "step": 13187
    },
    {
        "loss": 2.1165,
        "grad_norm": 3.506033420562744,
        "learning_rate": 5.862163907298879e-05,
        "epoch": 1.7584,
        "step": 13188
    },
    {
        "loss": 2.6967,
        "grad_norm": 2.9299635887145996,
        "learning_rate": 5.8564351213450565e-05,
        "epoch": 1.7585333333333333,
        "step": 13189
    },
    {
        "loss": 1.7594,
        "grad_norm": 3.718644618988037,
        "learning_rate": 5.8507079766714014e-05,
        "epoch": 1.7586666666666666,
        "step": 13190
    },
    {
        "loss": 2.7488,
        "grad_norm": 3.9538159370422363,
        "learning_rate": 5.844982475546441e-05,
        "epoch": 1.7588,
        "step": 13191
    },
    {
        "loss": 2.2014,
        "grad_norm": 2.633866786956787,
        "learning_rate": 5.839258620238095e-05,
        "epoch": 1.7589333333333332,
        "step": 13192
    },
    {
        "loss": 1.7572,
        "grad_norm": 4.0186381340026855,
        "learning_rate": 5.833536413013573e-05,
        "epoch": 1.7590666666666666,
        "step": 13193
    },
    {
        "loss": 3.0445,
        "grad_norm": 4.2762131690979,
        "learning_rate": 5.8278158561394856e-05,
        "epoch": 1.7591999999999999,
        "step": 13194
    },
    {
        "loss": 2.0389,
        "grad_norm": 4.004541397094727,
        "learning_rate": 5.822096951881758e-05,
        "epoch": 1.7593333333333332,
        "step": 13195
    },
    {
        "loss": 1.9049,
        "grad_norm": 3.0438661575317383,
        "learning_rate": 5.8163797025056473e-05,
        "epoch": 1.7594666666666665,
        "step": 13196
    },
    {
        "loss": 1.513,
        "grad_norm": 2.7713422775268555,
        "learning_rate": 5.810664110275804e-05,
        "epoch": 1.7596,
        "step": 13197
    },
    {
        "loss": 1.8192,
        "grad_norm": 4.619964122772217,
        "learning_rate": 5.8049501774561855e-05,
        "epoch": 1.7597333333333334,
        "step": 13198
    },
    {
        "loss": 1.5497,
        "grad_norm": 2.8274855613708496,
        "learning_rate": 5.799237906310091e-05,
        "epoch": 1.7598666666666667,
        "step": 13199
    },
    {
        "loss": 2.1206,
        "grad_norm": 3.503880739212036,
        "learning_rate": 5.793527299100166e-05,
        "epoch": 1.76,
        "step": 13200
    },
    {
        "loss": 2.6339,
        "grad_norm": 3.146324872970581,
        "learning_rate": 5.787818358088425e-05,
        "epoch": 1.7601333333333333,
        "step": 13201
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.3581900596618652,
        "learning_rate": 5.782111085536168e-05,
        "epoch": 1.7602666666666666,
        "step": 13202
    },
    {
        "loss": 2.6528,
        "grad_norm": 3.474442720413208,
        "learning_rate": 5.776405483704113e-05,
        "epoch": 1.7604,
        "step": 13203
    },
    {
        "loss": 2.0541,
        "grad_norm": 2.27817702293396,
        "learning_rate": 5.770701554852207e-05,
        "epoch": 1.7605333333333333,
        "step": 13204
    },
    {
        "loss": 2.3831,
        "grad_norm": 2.407331705093384,
        "learning_rate": 5.7649993012398285e-05,
        "epoch": 1.7606666666666668,
        "step": 13205
    },
    {
        "loss": 1.988,
        "grad_norm": 2.8626859188079834,
        "learning_rate": 5.759298725125667e-05,
        "epoch": 1.7608000000000001,
        "step": 13206
    },
    {
        "loss": 1.6683,
        "grad_norm": 3.209421396255493,
        "learning_rate": 5.753599828767735e-05,
        "epoch": 1.7609333333333335,
        "step": 13207
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.8834164142608643,
        "learning_rate": 5.7479026144233774e-05,
        "epoch": 1.7610666666666668,
        "step": 13208
    },
    {
        "loss": 1.7184,
        "grad_norm": 4.036043643951416,
        "learning_rate": 5.7422070843492734e-05,
        "epoch": 1.7612,
        "step": 13209
    },
    {
        "loss": 2.3125,
        "grad_norm": 3.578613519668579,
        "learning_rate": 5.7365132408014597e-05,
        "epoch": 1.7613333333333334,
        "step": 13210
    },
    {
        "loss": 2.2011,
        "grad_norm": 3.618908166885376,
        "learning_rate": 5.7308210860352786e-05,
        "epoch": 1.7614666666666667,
        "step": 13211
    },
    {
        "loss": 1.9139,
        "grad_norm": 3.2177252769470215,
        "learning_rate": 5.725130622305417e-05,
        "epoch": 1.7616,
        "step": 13212
    },
    {
        "loss": 1.4869,
        "grad_norm": 4.843839168548584,
        "learning_rate": 5.719441851865864e-05,
        "epoch": 1.7617333333333334,
        "step": 13213
    },
    {
        "loss": 0.5282,
        "grad_norm": 2.575029134750366,
        "learning_rate": 5.7137547769699817e-05,
        "epoch": 1.7618666666666667,
        "step": 13214
    },
    {
        "loss": 1.5353,
        "grad_norm": 4.274555683135986,
        "learning_rate": 5.7080693998704515e-05,
        "epoch": 1.762,
        "step": 13215
    },
    {
        "loss": 2.2447,
        "grad_norm": 3.5385613441467285,
        "learning_rate": 5.7023857228192536e-05,
        "epoch": 1.7621333333333333,
        "step": 13216
    },
    {
        "loss": 2.9027,
        "grad_norm": 3.1856906414031982,
        "learning_rate": 5.696703748067713e-05,
        "epoch": 1.7622666666666666,
        "step": 13217
    },
    {
        "loss": 2.0917,
        "grad_norm": 4.441882133483887,
        "learning_rate": 5.691023477866475e-05,
        "epoch": 1.7624,
        "step": 13218
    },
    {
        "loss": 2.1018,
        "grad_norm": 2.571377754211426,
        "learning_rate": 5.685344914465527e-05,
        "epoch": 1.7625333333333333,
        "step": 13219
    },
    {
        "loss": 2.4066,
        "grad_norm": 5.392648220062256,
        "learning_rate": 5.679668060114165e-05,
        "epoch": 1.7626666666666666,
        "step": 13220
    },
    {
        "loss": 1.8685,
        "grad_norm": 5.784595966339111,
        "learning_rate": 5.6739929170609994e-05,
        "epoch": 1.7628,
        "step": 13221
    },
    {
        "loss": 1.3044,
        "grad_norm": 5.0817060470581055,
        "learning_rate": 5.66831948755397e-05,
        "epoch": 1.7629333333333332,
        "step": 13222
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.7561676502227783,
        "learning_rate": 5.662647773840355e-05,
        "epoch": 1.7630666666666666,
        "step": 13223
    },
    {
        "loss": 2.2574,
        "grad_norm": 4.085287570953369,
        "learning_rate": 5.656977778166754e-05,
        "epoch": 1.7631999999999999,
        "step": 13224
    },
    {
        "loss": 2.0475,
        "grad_norm": 5.018861770629883,
        "learning_rate": 5.6513095027790317e-05,
        "epoch": 1.7633333333333332,
        "step": 13225
    },
    {
        "loss": 1.7714,
        "grad_norm": 3.3917832374572754,
        "learning_rate": 5.645642949922449e-05,
        "epoch": 1.7634666666666665,
        "step": 13226
    },
    {
        "loss": 2.0136,
        "grad_norm": 3.115974187850952,
        "learning_rate": 5.639978121841514e-05,
        "epoch": 1.7635999999999998,
        "step": 13227
    },
    {
        "loss": 2.8667,
        "grad_norm": 4.081544876098633,
        "learning_rate": 5.6343150207801146e-05,
        "epoch": 1.7637333333333334,
        "step": 13228
    },
    {
        "loss": 2.3419,
        "grad_norm": 4.80880880355835,
        "learning_rate": 5.6286536489814145e-05,
        "epoch": 1.7638666666666667,
        "step": 13229
    },
    {
        "loss": 1.1864,
        "grad_norm": 4.871488094329834,
        "learning_rate": 5.622994008687894e-05,
        "epoch": 1.764,
        "step": 13230
    },
    {
        "loss": 0.8943,
        "grad_norm": 3.3025357723236084,
        "learning_rate": 5.617336102141351e-05,
        "epoch": 1.7641333333333333,
        "step": 13231
    },
    {
        "loss": 2.5075,
        "grad_norm": 3.3403375148773193,
        "learning_rate": 5.611679931582926e-05,
        "epoch": 1.7642666666666666,
        "step": 13232
    },
    {
        "loss": 1.8363,
        "grad_norm": 4.465170383453369,
        "learning_rate": 5.6060254992530294e-05,
        "epoch": 1.7644,
        "step": 13233
    },
    {
        "loss": 1.7473,
        "grad_norm": 3.5441389083862305,
        "learning_rate": 5.600372807391396e-05,
        "epoch": 1.7645333333333333,
        "step": 13234
    },
    {
        "loss": 2.4728,
        "grad_norm": 3.882274627685547,
        "learning_rate": 5.5947218582370945e-05,
        "epoch": 1.7646666666666668,
        "step": 13235
    },
    {
        "loss": 2.4601,
        "grad_norm": 4.133097171783447,
        "learning_rate": 5.589072654028462e-05,
        "epoch": 1.7648000000000001,
        "step": 13236
    },
    {
        "loss": 2.188,
        "grad_norm": 3.0494930744171143,
        "learning_rate": 5.583425197003198e-05,
        "epoch": 1.7649333333333335,
        "step": 13237
    },
    {
        "loss": 1.7511,
        "grad_norm": 3.3268017768859863,
        "learning_rate": 5.5777794893982584e-05,
        "epoch": 1.7650666666666668,
        "step": 13238
    },
    {
        "loss": 2.2955,
        "grad_norm": 3.9927141666412354,
        "learning_rate": 5.5721355334499316e-05,
        "epoch": 1.7652,
        "step": 13239
    },
    {
        "loss": 1.0668,
        "grad_norm": 3.7652699947357178,
        "learning_rate": 5.566493331393798e-05,
        "epoch": 1.7653333333333334,
        "step": 13240
    },
    {
        "loss": 2.3831,
        "grad_norm": 4.582450866699219,
        "learning_rate": 5.560852885464774e-05,
        "epoch": 1.7654666666666667,
        "step": 13241
    },
    {
        "loss": 2.1932,
        "grad_norm": 3.6496458053588867,
        "learning_rate": 5.55521419789705e-05,
        "epoch": 1.7656,
        "step": 13242
    },
    {
        "loss": 1.2528,
        "grad_norm": 7.2166428565979,
        "learning_rate": 5.549577270924111e-05,
        "epoch": 1.7657333333333334,
        "step": 13243
    },
    {
        "loss": 2.522,
        "grad_norm": 2.219493865966797,
        "learning_rate": 5.5439421067787925e-05,
        "epoch": 1.7658666666666667,
        "step": 13244
    },
    {
        "loss": 2.9416,
        "grad_norm": 1.233100414276123,
        "learning_rate": 5.5383087076931916e-05,
        "epoch": 1.766,
        "step": 13245
    },
    {
        "loss": 2.5483,
        "grad_norm": 3.454805374145508,
        "learning_rate": 5.532677075898699e-05,
        "epoch": 1.7661333333333333,
        "step": 13246
    },
    {
        "loss": 1.7189,
        "grad_norm": 6.19846248626709,
        "learning_rate": 5.527047213626051e-05,
        "epoch": 1.7662666666666667,
        "step": 13247
    },
    {
        "loss": 2.7517,
        "grad_norm": 2.6677327156066895,
        "learning_rate": 5.521419123105247e-05,
        "epoch": 1.7664,
        "step": 13248
    },
    {
        "loss": 0.7922,
        "grad_norm": 4.425394535064697,
        "learning_rate": 5.5157928065655854e-05,
        "epoch": 1.7665333333333333,
        "step": 13249
    },
    {
        "loss": 2.102,
        "grad_norm": 5.321425914764404,
        "learning_rate": 5.51016826623566e-05,
        "epoch": 1.7666666666666666,
        "step": 13250
    },
    {
        "loss": 2.3714,
        "grad_norm": 3.2713654041290283,
        "learning_rate": 5.504545504343399e-05,
        "epoch": 1.7668,
        "step": 13251
    },
    {
        "loss": 1.9475,
        "grad_norm": 3.1211283206939697,
        "learning_rate": 5.498924523115965e-05,
        "epoch": 1.7669333333333332,
        "step": 13252
    },
    {
        "loss": 1.7218,
        "grad_norm": 2.8873605728149414,
        "learning_rate": 5.493305324779877e-05,
        "epoch": 1.7670666666666666,
        "step": 13253
    },
    {
        "loss": 2.0608,
        "grad_norm": 3.5793046951293945,
        "learning_rate": 5.48768791156091e-05,
        "epoch": 1.7671999999999999,
        "step": 13254
    },
    {
        "loss": 2.2154,
        "grad_norm": 2.923262119293213,
        "learning_rate": 5.482072285684121e-05,
        "epoch": 1.7673333333333332,
        "step": 13255
    },
    {
        "loss": 1.5746,
        "grad_norm": 4.594674587249756,
        "learning_rate": 5.476458449373909e-05,
        "epoch": 1.7674666666666665,
        "step": 13256
    },
    {
        "loss": 2.5775,
        "grad_norm": 3.9393880367279053,
        "learning_rate": 5.4708464048539157e-05,
        "epoch": 1.7675999999999998,
        "step": 13257
    },
    {
        "loss": 2.765,
        "grad_norm": 4.807286739349365,
        "learning_rate": 5.465236154347094e-05,
        "epoch": 1.7677333333333334,
        "step": 13258
    },
    {
        "loss": 2.1277,
        "grad_norm": 3.409517288208008,
        "learning_rate": 5.4596277000756754e-05,
        "epoch": 1.7678666666666667,
        "step": 13259
    },
    {
        "loss": 2.1197,
        "grad_norm": 3.5217437744140625,
        "learning_rate": 5.4540210442612036e-05,
        "epoch": 1.768,
        "step": 13260
    },
    {
        "loss": 0.7718,
        "grad_norm": 3.7864887714385986,
        "learning_rate": 5.448416189124474e-05,
        "epoch": 1.7681333333333333,
        "step": 13261
    },
    {
        "loss": 1.5713,
        "grad_norm": 3.628877878189087,
        "learning_rate": 5.442813136885629e-05,
        "epoch": 1.7682666666666667,
        "step": 13262
    },
    {
        "loss": 1.0948,
        "grad_norm": 4.779870986938477,
        "learning_rate": 5.4372118897640025e-05,
        "epoch": 1.7684,
        "step": 13263
    },
    {
        "loss": 1.16,
        "grad_norm": 4.4625067710876465,
        "learning_rate": 5.431612449978292e-05,
        "epoch": 1.7685333333333333,
        "step": 13264
    },
    {
        "loss": 1.9757,
        "grad_norm": 4.041266918182373,
        "learning_rate": 5.4260148197464736e-05,
        "epoch": 1.7686666666666668,
        "step": 13265
    },
    {
        "loss": 1.4108,
        "grad_norm": 5.832973957061768,
        "learning_rate": 5.420419001285766e-05,
        "epoch": 1.7688000000000001,
        "step": 13266
    },
    {
        "loss": 2.3123,
        "grad_norm": 3.534097671508789,
        "learning_rate": 5.4148249968127e-05,
        "epoch": 1.7689333333333335,
        "step": 13267
    },
    {
        "loss": 1.9517,
        "grad_norm": 4.306421756744385,
        "learning_rate": 5.409232808543061e-05,
        "epoch": 1.7690666666666668,
        "step": 13268
    },
    {
        "loss": 2.2971,
        "grad_norm": 2.672191858291626,
        "learning_rate": 5.4036424386919604e-05,
        "epoch": 1.7692,
        "step": 13269
    },
    {
        "loss": 2.5702,
        "grad_norm": 2.219130516052246,
        "learning_rate": 5.398053889473755e-05,
        "epoch": 1.7693333333333334,
        "step": 13270
    },
    {
        "loss": 1.5957,
        "grad_norm": 5.873970985412598,
        "learning_rate": 5.392467163102082e-05,
        "epoch": 1.7694666666666667,
        "step": 13271
    },
    {
        "loss": 1.1084,
        "grad_norm": 3.6572837829589844,
        "learning_rate": 5.386882261789854e-05,
        "epoch": 1.7696,
        "step": 13272
    },
    {
        "loss": 0.997,
        "grad_norm": 4.421823024749756,
        "learning_rate": 5.3812991877492845e-05,
        "epoch": 1.7697333333333334,
        "step": 13273
    },
    {
        "loss": 0.8288,
        "grad_norm": 4.269050121307373,
        "learning_rate": 5.3757179431918515e-05,
        "epoch": 1.7698666666666667,
        "step": 13274
    },
    {
        "loss": 2.3042,
        "grad_norm": 3.3317675590515137,
        "learning_rate": 5.370138530328299e-05,
        "epoch": 1.77,
        "step": 13275
    },
    {
        "loss": 0.7345,
        "grad_norm": 3.4517593383789062,
        "learning_rate": 5.364560951368654e-05,
        "epoch": 1.7701333333333333,
        "step": 13276
    },
    {
        "loss": 1.9256,
        "grad_norm": 3.614586114883423,
        "learning_rate": 5.358985208522196e-05,
        "epoch": 1.7702666666666667,
        "step": 13277
    },
    {
        "loss": 2.2737,
        "grad_norm": 3.49686598777771,
        "learning_rate": 5.353411303997522e-05,
        "epoch": 1.7704,
        "step": 13278
    },
    {
        "loss": 1.4323,
        "grad_norm": 5.042789936065674,
        "learning_rate": 5.3478392400024676e-05,
        "epoch": 1.7705333333333333,
        "step": 13279
    },
    {
        "loss": 2.4061,
        "grad_norm": 4.247395038604736,
        "learning_rate": 5.342269018744137e-05,
        "epoch": 1.7706666666666666,
        "step": 13280
    },
    {
        "loss": 1.4888,
        "grad_norm": 3.4634690284729004,
        "learning_rate": 5.3367006424289126e-05,
        "epoch": 1.7708,
        "step": 13281
    },
    {
        "loss": 2.3927,
        "grad_norm": 4.078218936920166,
        "learning_rate": 5.331134113262448e-05,
        "epoch": 1.7709333333333332,
        "step": 13282
    },
    {
        "loss": 2.13,
        "grad_norm": 3.6014959812164307,
        "learning_rate": 5.325569433449695e-05,
        "epoch": 1.7710666666666666,
        "step": 13283
    },
    {
        "loss": 1.6947,
        "grad_norm": 4.448977470397949,
        "learning_rate": 5.3200066051947974e-05,
        "epoch": 1.7711999999999999,
        "step": 13284
    },
    {
        "loss": 2.1036,
        "grad_norm": 3.996204376220703,
        "learning_rate": 5.314445630701237e-05,
        "epoch": 1.7713333333333332,
        "step": 13285
    },
    {
        "loss": 2.3112,
        "grad_norm": 2.5074172019958496,
        "learning_rate": 5.308886512171717e-05,
        "epoch": 1.7714666666666665,
        "step": 13286
    },
    {
        "loss": 2.3883,
        "grad_norm": 4.579549312591553,
        "learning_rate": 5.303329251808249e-05,
        "epoch": 1.7715999999999998,
        "step": 13287
    },
    {
        "loss": 2.1444,
        "grad_norm": 3.020155668258667,
        "learning_rate": 5.2977738518120645e-05,
        "epoch": 1.7717333333333334,
        "step": 13288
    },
    {
        "loss": 0.5499,
        "grad_norm": 2.450230836868286,
        "learning_rate": 5.292220314383685e-05,
        "epoch": 1.7718666666666667,
        "step": 13289
    },
    {
        "loss": 2.4094,
        "grad_norm": 4.0106425285339355,
        "learning_rate": 5.286668641722863e-05,
        "epoch": 1.772,
        "step": 13290
    },
    {
        "loss": 2.074,
        "grad_norm": 3.742509126663208,
        "learning_rate": 5.281118836028664e-05,
        "epoch": 1.7721333333333333,
        "step": 13291
    },
    {
        "loss": 1.1191,
        "grad_norm": 4.20869779586792,
        "learning_rate": 5.275570899499377e-05,
        "epoch": 1.7722666666666667,
        "step": 13292
    },
    {
        "loss": 2.1973,
        "grad_norm": 4.3825907707214355,
        "learning_rate": 5.270024834332539e-05,
        "epoch": 1.7724,
        "step": 13293
    },
    {
        "loss": 2.6505,
        "grad_norm": 4.120748043060303,
        "learning_rate": 5.2644806427249935e-05,
        "epoch": 1.7725333333333333,
        "step": 13294
    },
    {
        "loss": 1.1277,
        "grad_norm": 6.063432693481445,
        "learning_rate": 5.258938326872793e-05,
        "epoch": 1.7726666666666666,
        "step": 13295
    },
    {
        "loss": 1.434,
        "grad_norm": 3.576904058456421,
        "learning_rate": 5.253397888971285e-05,
        "epoch": 1.7728000000000002,
        "step": 13296
    },
    {
        "loss": 2.1833,
        "grad_norm": 3.215515375137329,
        "learning_rate": 5.247859331215046e-05,
        "epoch": 1.7729333333333335,
        "step": 13297
    },
    {
        "loss": 2.5365,
        "grad_norm": 3.550415277481079,
        "learning_rate": 5.242322655797922e-05,
        "epoch": 1.7730666666666668,
        "step": 13298
    },
    {
        "loss": 1.6249,
        "grad_norm": 3.2783689498901367,
        "learning_rate": 5.236787864912991e-05,
        "epoch": 1.7732,
        "step": 13299
    },
    {
        "loss": 2.2455,
        "grad_norm": 4.810233116149902,
        "learning_rate": 5.2312549607526275e-05,
        "epoch": 1.7733333333333334,
        "step": 13300
    },
    {
        "loss": 2.571,
        "grad_norm": 4.268211841583252,
        "learning_rate": 5.225723945508427e-05,
        "epoch": 1.7734666666666667,
        "step": 13301
    },
    {
        "loss": 2.4175,
        "grad_norm": 3.9536478519439697,
        "learning_rate": 5.220194821371225e-05,
        "epoch": 1.7736,
        "step": 13302
    },
    {
        "loss": 1.8946,
        "grad_norm": 5.0490875244140625,
        "learning_rate": 5.214667590531156e-05,
        "epoch": 1.7737333333333334,
        "step": 13303
    },
    {
        "loss": 2.3153,
        "grad_norm": 3.0872855186462402,
        "learning_rate": 5.209142255177557e-05,
        "epoch": 1.7738666666666667,
        "step": 13304
    },
    {
        "loss": 1.7852,
        "grad_norm": 3.6299479007720947,
        "learning_rate": 5.203618817499022e-05,
        "epoch": 1.774,
        "step": 13305
    },
    {
        "loss": 1.8692,
        "grad_norm": 3.424609422683716,
        "learning_rate": 5.198097279683434e-05,
        "epoch": 1.7741333333333333,
        "step": 13306
    },
    {
        "loss": 2.3239,
        "grad_norm": 3.925629138946533,
        "learning_rate": 5.19257764391787e-05,
        "epoch": 1.7742666666666667,
        "step": 13307
    },
    {
        "loss": 1.1958,
        "grad_norm": 3.4440975189208984,
        "learning_rate": 5.187059912388683e-05,
        "epoch": 1.7744,
        "step": 13308
    },
    {
        "loss": 2.1668,
        "grad_norm": 4.014763832092285,
        "learning_rate": 5.181544087281456e-05,
        "epoch": 1.7745333333333333,
        "step": 13309
    },
    {
        "loss": 1.7675,
        "grad_norm": 4.279460906982422,
        "learning_rate": 5.1760301707810434e-05,
        "epoch": 1.7746666666666666,
        "step": 13310
    },
    {
        "loss": 1.4506,
        "grad_norm": 2.8687744140625,
        "learning_rate": 5.170518165071504e-05,
        "epoch": 1.7748,
        "step": 13311
    },
    {
        "loss": 2.2078,
        "grad_norm": 3.4066617488861084,
        "learning_rate": 5.165008072336204e-05,
        "epoch": 1.7749333333333333,
        "step": 13312
    },
    {
        "loss": 1.725,
        "grad_norm": 4.85319709777832,
        "learning_rate": 5.1594998947576535e-05,
        "epoch": 1.7750666666666666,
        "step": 13313
    },
    {
        "loss": 2.8578,
        "grad_norm": 4.99949836730957,
        "learning_rate": 5.1539936345176886e-05,
        "epoch": 1.7752,
        "step": 13314
    },
    {
        "loss": 2.4124,
        "grad_norm": 4.650872230529785,
        "learning_rate": 5.1484892937973735e-05,
        "epoch": 1.7753333333333332,
        "step": 13315
    },
    {
        "loss": 2.7311,
        "grad_norm": 4.830995082855225,
        "learning_rate": 5.142986874776976e-05,
        "epoch": 1.7754666666666665,
        "step": 13316
    },
    {
        "loss": 2.067,
        "grad_norm": 3.6505656242370605,
        "learning_rate": 5.137486379636032e-05,
        "epoch": 1.7755999999999998,
        "step": 13317
    },
    {
        "loss": 2.3079,
        "grad_norm": 3.6194052696228027,
        "learning_rate": 5.131987810553285e-05,
        "epoch": 1.7757333333333334,
        "step": 13318
    },
    {
        "loss": 2.3459,
        "grad_norm": 4.957754611968994,
        "learning_rate": 5.126491169706764e-05,
        "epoch": 1.7758666666666667,
        "step": 13319
    },
    {
        "loss": 2.0935,
        "grad_norm": 4.4849443435668945,
        "learning_rate": 5.1209964592736906e-05,
        "epoch": 1.776,
        "step": 13320
    },
    {
        "loss": 2.4183,
        "grad_norm": 4.126741409301758,
        "learning_rate": 5.1155036814305646e-05,
        "epoch": 1.7761333333333333,
        "step": 13321
    },
    {
        "loss": 2.7824,
        "grad_norm": 3.457227945327759,
        "learning_rate": 5.1100128383530486e-05,
        "epoch": 1.7762666666666667,
        "step": 13322
    },
    {
        "loss": 2.9108,
        "grad_norm": 5.550734996795654,
        "learning_rate": 5.1045239322161145e-05,
        "epoch": 1.7764,
        "step": 13323
    },
    {
        "loss": 2.4346,
        "grad_norm": 4.013641834259033,
        "learning_rate": 5.099036965193939e-05,
        "epoch": 1.7765333333333333,
        "step": 13324
    },
    {
        "loss": 1.2575,
        "grad_norm": 5.686812877655029,
        "learning_rate": 5.093551939459923e-05,
        "epoch": 1.7766666666666666,
        "step": 13325
    },
    {
        "loss": 2.5457,
        "grad_norm": 3.3922719955444336,
        "learning_rate": 5.088068857186705e-05,
        "epoch": 1.7768000000000002,
        "step": 13326
    },
    {
        "loss": 0.8795,
        "grad_norm": 4.348816394805908,
        "learning_rate": 5.0825877205461334e-05,
        "epoch": 1.7769333333333335,
        "step": 13327
    },
    {
        "loss": 2.1146,
        "grad_norm": 3.217453956604004,
        "learning_rate": 5.077108531709338e-05,
        "epoch": 1.7770666666666668,
        "step": 13328
    },
    {
        "loss": 2.1579,
        "grad_norm": 2.8190042972564697,
        "learning_rate": 5.071631292846623e-05,
        "epoch": 1.7772000000000001,
        "step": 13329
    },
    {
        "loss": 2.8037,
        "grad_norm": 3.4835212230682373,
        "learning_rate": 5.066156006127545e-05,
        "epoch": 1.7773333333333334,
        "step": 13330
    },
    {
        "loss": 1.948,
        "grad_norm": 3.3089425563812256,
        "learning_rate": 5.060682673720878e-05,
        "epoch": 1.7774666666666668,
        "step": 13331
    },
    {
        "loss": 1.1993,
        "grad_norm": 4.211324691772461,
        "learning_rate": 5.055211297794631e-05,
        "epoch": 1.7776,
        "step": 13332
    },
    {
        "loss": 2.5873,
        "grad_norm": 2.8363349437713623,
        "learning_rate": 5.049741880516059e-05,
        "epoch": 1.7777333333333334,
        "step": 13333
    },
    {
        "loss": 2.1887,
        "grad_norm": 7.508168697357178,
        "learning_rate": 5.044274424051578e-05,
        "epoch": 1.7778666666666667,
        "step": 13334
    },
    {
        "loss": 1.2657,
        "grad_norm": 4.852895259857178,
        "learning_rate": 5.038808930566892e-05,
        "epoch": 1.778,
        "step": 13335
    },
    {
        "loss": 1.8558,
        "grad_norm": 5.382432460784912,
        "learning_rate": 5.033345402226881e-05,
        "epoch": 1.7781333333333333,
        "step": 13336
    },
    {
        "loss": 2.8852,
        "grad_norm": 4.1798200607299805,
        "learning_rate": 5.0278838411956955e-05,
        "epoch": 1.7782666666666667,
        "step": 13337
    },
    {
        "loss": 1.5572,
        "grad_norm": 5.263110637664795,
        "learning_rate": 5.022424249636658e-05,
        "epoch": 1.7784,
        "step": 13338
    },
    {
        "loss": 1.9443,
        "grad_norm": 4.545278549194336,
        "learning_rate": 5.0169666297123405e-05,
        "epoch": 1.7785333333333333,
        "step": 13339
    },
    {
        "loss": 2.3487,
        "grad_norm": 2.816683530807495,
        "learning_rate": 5.011510983584504e-05,
        "epoch": 1.7786666666666666,
        "step": 13340
    },
    {
        "loss": 1.2132,
        "grad_norm": 3.8493356704711914,
        "learning_rate": 5.006057313414165e-05,
        "epoch": 1.7788,
        "step": 13341
    },
    {
        "loss": 2.5458,
        "grad_norm": 3.618195056915283,
        "learning_rate": 5.000605621361568e-05,
        "epoch": 1.7789333333333333,
        "step": 13342
    },
    {
        "loss": 2.5536,
        "grad_norm": 3.5532360076904297,
        "learning_rate": 4.995155909586092e-05,
        "epoch": 1.7790666666666666,
        "step": 13343
    },
    {
        "loss": 1.4845,
        "grad_norm": 4.239728927612305,
        "learning_rate": 4.9897081802464264e-05,
        "epoch": 1.7792,
        "step": 13344
    },
    {
        "loss": 1.8213,
        "grad_norm": 3.841156482696533,
        "learning_rate": 4.984262435500414e-05,
        "epoch": 1.7793333333333332,
        "step": 13345
    },
    {
        "loss": 1.8061,
        "grad_norm": 3.848198890686035,
        "learning_rate": 4.978818677505149e-05,
        "epoch": 1.7794666666666665,
        "step": 13346
    },
    {
        "loss": 1.8573,
        "grad_norm": 4.7546610832214355,
        "learning_rate": 4.973376908416916e-05,
        "epoch": 1.7795999999999998,
        "step": 13347
    },
    {
        "loss": 1.8668,
        "grad_norm": 5.3162360191345215,
        "learning_rate": 4.9679371303912214e-05,
        "epoch": 1.7797333333333332,
        "step": 13348
    },
    {
        "loss": 2.2198,
        "grad_norm": 3.0140345096588135,
        "learning_rate": 4.96249934558276e-05,
        "epoch": 1.7798666666666667,
        "step": 13349
    },
    {
        "loss": 2.6204,
        "grad_norm": 2.6300928592681885,
        "learning_rate": 4.957063556145487e-05,
        "epoch": 1.78,
        "step": 13350
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.7799038887023926,
        "learning_rate": 4.951629764232521e-05,
        "epoch": 1.7801333333333333,
        "step": 13351
    },
    {
        "loss": 2.2068,
        "grad_norm": 3.2580807209014893,
        "learning_rate": 4.9461979719961936e-05,
        "epoch": 1.7802666666666667,
        "step": 13352
    },
    {
        "loss": 2.3102,
        "grad_norm": 4.123266220092773,
        "learning_rate": 4.940768181588087e-05,
        "epoch": 1.7804,
        "step": 13353
    },
    {
        "loss": 1.9138,
        "grad_norm": 3.6474080085754395,
        "learning_rate": 4.935340395158926e-05,
        "epoch": 1.7805333333333333,
        "step": 13354
    },
    {
        "loss": 1.9918,
        "grad_norm": 2.9621195793151855,
        "learning_rate": 4.929914614858704e-05,
        "epoch": 1.7806666666666666,
        "step": 13355
    },
    {
        "loss": 2.9815,
        "grad_norm": 4.6095380783081055,
        "learning_rate": 4.9244908428365834e-05,
        "epoch": 1.7808000000000002,
        "step": 13356
    },
    {
        "loss": 2.4536,
        "grad_norm": 3.7337539196014404,
        "learning_rate": 4.919069081240929e-05,
        "epoch": 1.7809333333333335,
        "step": 13357
    },
    {
        "loss": 2.336,
        "grad_norm": 3.7290406227111816,
        "learning_rate": 4.9136493322193146e-05,
        "epoch": 1.7810666666666668,
        "step": 13358
    },
    {
        "loss": 1.3735,
        "grad_norm": 3.2433876991271973,
        "learning_rate": 4.9082315979185446e-05,
        "epoch": 1.7812000000000001,
        "step": 13359
    },
    {
        "loss": 2.5162,
        "grad_norm": 3.012984275817871,
        "learning_rate": 4.902815880484586e-05,
        "epoch": 1.7813333333333334,
        "step": 13360
    },
    {
        "loss": 2.2552,
        "grad_norm": 4.436289310455322,
        "learning_rate": 4.8974021820626126e-05,
        "epoch": 1.7814666666666668,
        "step": 13361
    },
    {
        "loss": 2.7901,
        "grad_norm": 3.3679795265197754,
        "learning_rate": 4.891990504797038e-05,
        "epoch": 1.7816,
        "step": 13362
    },
    {
        "loss": 1.2126,
        "grad_norm": 5.6568756103515625,
        "learning_rate": 4.886580850831426e-05,
        "epoch": 1.7817333333333334,
        "step": 13363
    },
    {
        "loss": 1.8105,
        "grad_norm": 3.431406259536743,
        "learning_rate": 4.881173222308554e-05,
        "epoch": 1.7818666666666667,
        "step": 13364
    },
    {
        "loss": 1.5704,
        "grad_norm": 4.08833646774292,
        "learning_rate": 4.875767621370419e-05,
        "epoch": 1.782,
        "step": 13365
    },
    {
        "loss": 2.1575,
        "grad_norm": 3.8430073261260986,
        "learning_rate": 4.8703640501581885e-05,
        "epoch": 1.7821333333333333,
        "step": 13366
    },
    {
        "loss": 0.8579,
        "grad_norm": 3.812175750732422,
        "learning_rate": 4.864962510812241e-05,
        "epoch": 1.7822666666666667,
        "step": 13367
    },
    {
        "loss": 1.4643,
        "grad_norm": 4.875378131866455,
        "learning_rate": 4.8595630054721244e-05,
        "epoch": 1.7824,
        "step": 13368
    },
    {
        "loss": 1.0816,
        "grad_norm": 3.4817450046539307,
        "learning_rate": 4.8541655362766295e-05,
        "epoch": 1.7825333333333333,
        "step": 13369
    },
    {
        "loss": 2.2524,
        "grad_norm": 2.6794238090515137,
        "learning_rate": 4.84877010536369e-05,
        "epoch": 1.7826666666666666,
        "step": 13370
    },
    {
        "loss": 1.1384,
        "grad_norm": 5.665217399597168,
        "learning_rate": 4.8433767148704854e-05,
        "epoch": 1.7828,
        "step": 13371
    },
    {
        "loss": 2.1594,
        "grad_norm": 2.19273042678833,
        "learning_rate": 4.837985366933311e-05,
        "epoch": 1.7829333333333333,
        "step": 13372
    },
    {
        "loss": 1.4911,
        "grad_norm": 3.498025417327881,
        "learning_rate": 4.832596063687725e-05,
        "epoch": 1.7830666666666666,
        "step": 13373
    },
    {
        "loss": 1.6139,
        "grad_norm": 4.346108436584473,
        "learning_rate": 4.827208807268455e-05,
        "epoch": 1.7832,
        "step": 13374
    },
    {
        "loss": 2.4016,
        "grad_norm": 6.690896987915039,
        "learning_rate": 4.821823599809405e-05,
        "epoch": 1.7833333333333332,
        "step": 13375
    },
    {
        "loss": 2.0255,
        "grad_norm": 2.8188087940216064,
        "learning_rate": 4.816440443443667e-05,
        "epoch": 1.7834666666666665,
        "step": 13376
    },
    {
        "loss": 2.1955,
        "grad_norm": 3.388570785522461,
        "learning_rate": 4.811059340303522e-05,
        "epoch": 1.7835999999999999,
        "step": 13377
    },
    {
        "loss": 2.3742,
        "grad_norm": 3.865579128265381,
        "learning_rate": 4.805680292520468e-05,
        "epoch": 1.7837333333333332,
        "step": 13378
    },
    {
        "loss": 1.3545,
        "grad_norm": 4.063482761383057,
        "learning_rate": 4.800303302225133e-05,
        "epoch": 1.7838666666666667,
        "step": 13379
    },
    {
        "loss": 2.4973,
        "grad_norm": 5.051766872406006,
        "learning_rate": 4.794928371547401e-05,
        "epoch": 1.784,
        "step": 13380
    },
    {
        "loss": 1.8381,
        "grad_norm": 4.684464931488037,
        "learning_rate": 4.7895555026162584e-05,
        "epoch": 1.7841333333333333,
        "step": 13381
    },
    {
        "loss": 1.9179,
        "grad_norm": 4.080322742462158,
        "learning_rate": 4.7841846975599314e-05,
        "epoch": 1.7842666666666667,
        "step": 13382
    },
    {
        "loss": 1.3052,
        "grad_norm": 4.171748161315918,
        "learning_rate": 4.7788159585058286e-05,
        "epoch": 1.7844,
        "step": 13383
    },
    {
        "loss": 2.35,
        "grad_norm": 4.592220306396484,
        "learning_rate": 4.7734492875805206e-05,
        "epoch": 1.7845333333333333,
        "step": 13384
    },
    {
        "loss": 1.488,
        "grad_norm": 3.3955390453338623,
        "learning_rate": 4.768084686909754e-05,
        "epoch": 1.7846666666666666,
        "step": 13385
    },
    {
        "loss": 1.996,
        "grad_norm": 3.6298625469207764,
        "learning_rate": 4.762722158618459e-05,
        "epoch": 1.7848000000000002,
        "step": 13386
    },
    {
        "loss": 2.0554,
        "grad_norm": 7.365545272827148,
        "learning_rate": 4.7573617048307774e-05,
        "epoch": 1.7849333333333335,
        "step": 13387
    },
    {
        "loss": 2.0253,
        "grad_norm": 3.846588611602783,
        "learning_rate": 4.752003327669985e-05,
        "epoch": 1.7850666666666668,
        "step": 13388
    },
    {
        "loss": 0.9358,
        "grad_norm": 3.3720998764038086,
        "learning_rate": 4.7466470292585605e-05,
        "epoch": 1.7852000000000001,
        "step": 13389
    },
    {
        "loss": 2.4527,
        "grad_norm": 2.1139895915985107,
        "learning_rate": 4.7412928117181334e-05,
        "epoch": 1.7853333333333334,
        "step": 13390
    },
    {
        "loss": 2.5077,
        "grad_norm": 3.1438968181610107,
        "learning_rate": 4.7359406771695413e-05,
        "epoch": 1.7854666666666668,
        "step": 13391
    },
    {
        "loss": 1.4332,
        "grad_norm": 4.717807292938232,
        "learning_rate": 4.730590627732811e-05,
        "epoch": 1.7856,
        "step": 13392
    },
    {
        "loss": 2.7575,
        "grad_norm": 3.295233726501465,
        "learning_rate": 4.725242665527061e-05,
        "epoch": 1.7857333333333334,
        "step": 13393
    },
    {
        "loss": 1.6919,
        "grad_norm": 3.7817981243133545,
        "learning_rate": 4.719896792670676e-05,
        "epoch": 1.7858666666666667,
        "step": 13394
    },
    {
        "loss": 1.4426,
        "grad_norm": 5.4876251220703125,
        "learning_rate": 4.7145530112811554e-05,
        "epoch": 1.786,
        "step": 13395
    },
    {
        "loss": 1.332,
        "grad_norm": 6.036735534667969,
        "learning_rate": 4.709211323475204e-05,
        "epoch": 1.7861333333333334,
        "step": 13396
    },
    {
        "loss": 2.8008,
        "grad_norm": 2.909897565841675,
        "learning_rate": 4.70387173136868e-05,
        "epoch": 1.7862666666666667,
        "step": 13397
    },
    {
        "loss": 2.0796,
        "grad_norm": 2.8734610080718994,
        "learning_rate": 4.698534237076605e-05,
        "epoch": 1.7864,
        "step": 13398
    },
    {
        "loss": 2.3926,
        "grad_norm": 3.591942310333252,
        "learning_rate": 4.693198842713171e-05,
        "epoch": 1.7865333333333333,
        "step": 13399
    },
    {
        "loss": 2.1448,
        "grad_norm": 3.830179214477539,
        "learning_rate": 4.687865550391759e-05,
        "epoch": 1.7866666666666666,
        "step": 13400
    },
    {
        "loss": 1.7696,
        "grad_norm": 2.0447933673858643,
        "learning_rate": 4.682534362224929e-05,
        "epoch": 1.7868,
        "step": 13401
    },
    {
        "loss": 1.8564,
        "grad_norm": 4.168493270874023,
        "learning_rate": 4.67720528032433e-05,
        "epoch": 1.7869333333333333,
        "step": 13402
    },
    {
        "loss": 2.5765,
        "grad_norm": 5.33675479888916,
        "learning_rate": 4.6718783068008734e-05,
        "epoch": 1.7870666666666666,
        "step": 13403
    },
    {
        "loss": 0.9371,
        "grad_norm": 4.491980075836182,
        "learning_rate": 4.6665534437645575e-05,
        "epoch": 1.7872,
        "step": 13404
    },
    {
        "loss": 0.7037,
        "grad_norm": 3.612069606781006,
        "learning_rate": 4.6612306933246086e-05,
        "epoch": 1.7873333333333332,
        "step": 13405
    },
    {
        "loss": 2.2306,
        "grad_norm": 3.003826856613159,
        "learning_rate": 4.655910057589376e-05,
        "epoch": 1.7874666666666665,
        "step": 13406
    },
    {
        "loss": 1.4577,
        "grad_norm": 4.0813679695129395,
        "learning_rate": 4.650591538666377e-05,
        "epoch": 1.7875999999999999,
        "step": 13407
    },
    {
        "loss": 1.6332,
        "grad_norm": 4.186287879943848,
        "learning_rate": 4.645275138662284e-05,
        "epoch": 1.7877333333333332,
        "step": 13408
    },
    {
        "loss": 1.9164,
        "grad_norm": 4.167849540710449,
        "learning_rate": 4.6399608596829705e-05,
        "epoch": 1.7878666666666667,
        "step": 13409
    },
    {
        "loss": 2.0666,
        "grad_norm": 3.656355142593384,
        "learning_rate": 4.6346487038334196e-05,
        "epoch": 1.788,
        "step": 13410
    },
    {
        "loss": 3.2569,
        "grad_norm": 3.116267442703247,
        "learning_rate": 4.6293386732177924e-05,
        "epoch": 1.7881333333333334,
        "step": 13411
    },
    {
        "loss": 2.1448,
        "grad_norm": 4.641077995300293,
        "learning_rate": 4.624030769939425e-05,
        "epoch": 1.7882666666666667,
        "step": 13412
    },
    {
        "loss": 2.3365,
        "grad_norm": 3.2519967555999756,
        "learning_rate": 4.618724996100786e-05,
        "epoch": 1.7884,
        "step": 13413
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.9941672086715698,
        "learning_rate": 4.613421353803506e-05,
        "epoch": 1.7885333333333333,
        "step": 13414
    },
    {
        "loss": 2.2777,
        "grad_norm": 4.637455940246582,
        "learning_rate": 4.60811984514839e-05,
        "epoch": 1.7886666666666666,
        "step": 13415
    },
    {
        "loss": 2.9381,
        "grad_norm": 2.09564471244812,
        "learning_rate": 4.6028204722353755e-05,
        "epoch": 1.7888,
        "step": 13416
    },
    {
        "loss": 1.5819,
        "grad_norm": 2.286923885345459,
        "learning_rate": 4.597523237163554e-05,
        "epoch": 1.7889333333333335,
        "step": 13417
    },
    {
        "loss": 1.3264,
        "grad_norm": 4.438654899597168,
        "learning_rate": 4.592228142031195e-05,
        "epoch": 1.7890666666666668,
        "step": 13418
    },
    {
        "loss": 1.5005,
        "grad_norm": 4.184385776519775,
        "learning_rate": 4.586935188935695e-05,
        "epoch": 1.7892000000000001,
        "step": 13419
    },
    {
        "loss": 2.3582,
        "grad_norm": 2.9636735916137695,
        "learning_rate": 4.581644379973604e-05,
        "epoch": 1.7893333333333334,
        "step": 13420
    },
    {
        "loss": 1.9888,
        "grad_norm": 4.122008323669434,
        "learning_rate": 4.576355717240645e-05,
        "epoch": 1.7894666666666668,
        "step": 13421
    },
    {
        "loss": 2.625,
        "grad_norm": 3.592406749725342,
        "learning_rate": 4.571069202831669e-05,
        "epoch": 1.7896,
        "step": 13422
    },
    {
        "loss": 2.7328,
        "grad_norm": 2.8427631855010986,
        "learning_rate": 4.5657848388406666e-05,
        "epoch": 1.7897333333333334,
        "step": 13423
    },
    {
        "loss": 2.6432,
        "grad_norm": 3.3473246097564697,
        "learning_rate": 4.560502627360816e-05,
        "epoch": 1.7898666666666667,
        "step": 13424
    },
    {
        "loss": 2.6593,
        "grad_norm": 3.2581984996795654,
        "learning_rate": 4.555222570484415e-05,
        "epoch": 1.79,
        "step": 13425
    },
    {
        "loss": 2.0317,
        "grad_norm": 3.8842830657958984,
        "learning_rate": 4.549944670302901e-05,
        "epoch": 1.7901333333333334,
        "step": 13426
    },
    {
        "loss": 1.6059,
        "grad_norm": 3.6893081665039062,
        "learning_rate": 4.5446689289068645e-05,
        "epoch": 1.7902666666666667,
        "step": 13427
    },
    {
        "loss": 2.2828,
        "grad_norm": 2.465731143951416,
        "learning_rate": 4.539395348386068e-05,
        "epoch": 1.7904,
        "step": 13428
    },
    {
        "loss": 2.0295,
        "grad_norm": 2.7381527423858643,
        "learning_rate": 4.5341239308293646e-05,
        "epoch": 1.7905333333333333,
        "step": 13429
    },
    {
        "loss": 1.7907,
        "grad_norm": 3.440347194671631,
        "learning_rate": 4.528854678324823e-05,
        "epoch": 1.7906666666666666,
        "step": 13430
    },
    {
        "loss": 2.3016,
        "grad_norm": 2.9117677211761475,
        "learning_rate": 4.52358759295956e-05,
        "epoch": 1.7908,
        "step": 13431
    },
    {
        "loss": 1.4004,
        "grad_norm": 5.980382442474365,
        "learning_rate": 4.5183226768199126e-05,
        "epoch": 1.7909333333333333,
        "step": 13432
    },
    {
        "loss": 2.3194,
        "grad_norm": 7.749920845031738,
        "learning_rate": 4.513059931991342e-05,
        "epoch": 1.7910666666666666,
        "step": 13433
    },
    {
        "loss": 2.5408,
        "grad_norm": 5.413943290710449,
        "learning_rate": 4.507799360558432e-05,
        "epoch": 1.7912,
        "step": 13434
    },
    {
        "loss": 2.1908,
        "grad_norm": 3.621020555496216,
        "learning_rate": 4.502540964604907e-05,
        "epoch": 1.7913333333333332,
        "step": 13435
    },
    {
        "loss": 1.9268,
        "grad_norm": 3.9377291202545166,
        "learning_rate": 4.497284746213629e-05,
        "epoch": 1.7914666666666665,
        "step": 13436
    },
    {
        "loss": 1.0396,
        "grad_norm": 3.979761838912964,
        "learning_rate": 4.4920307074666215e-05,
        "epoch": 1.7915999999999999,
        "step": 13437
    },
    {
        "loss": 1.215,
        "grad_norm": 4.498049259185791,
        "learning_rate": 4.486778850445007e-05,
        "epoch": 1.7917333333333332,
        "step": 13438
    },
    {
        "loss": 2.8419,
        "grad_norm": 3.72348952293396,
        "learning_rate": 4.481529177229101e-05,
        "epoch": 1.7918666666666667,
        "step": 13439
    },
    {
        "loss": 2.723,
        "grad_norm": 3.105360746383667,
        "learning_rate": 4.476281689898267e-05,
        "epoch": 1.792,
        "step": 13440
    },
    {
        "loss": 2.1774,
        "grad_norm": 4.323853969573975,
        "learning_rate": 4.471036390531077e-05,
        "epoch": 1.7921333333333334,
        "step": 13441
    },
    {
        "loss": 2.0743,
        "grad_norm": 3.4681098461151123,
        "learning_rate": 4.4657932812052274e-05,
        "epoch": 1.7922666666666667,
        "step": 13442
    },
    {
        "loss": 1.2625,
        "grad_norm": 5.038258075714111,
        "learning_rate": 4.460552363997511e-05,
        "epoch": 1.7924,
        "step": 13443
    },
    {
        "loss": 2.3606,
        "grad_norm": 3.437239170074463,
        "learning_rate": 4.4553136409838833e-05,
        "epoch": 1.7925333333333333,
        "step": 13444
    },
    {
        "loss": 1.7547,
        "grad_norm": 2.577500581741333,
        "learning_rate": 4.450077114239402e-05,
        "epoch": 1.7926666666666666,
        "step": 13445
    },
    {
        "loss": 2.1602,
        "grad_norm": 3.4069249629974365,
        "learning_rate": 4.444842785838296e-05,
        "epoch": 1.7928,
        "step": 13446
    },
    {
        "loss": 1.5169,
        "grad_norm": 5.214084625244141,
        "learning_rate": 4.4396106578538956e-05,
        "epoch": 1.7929333333333335,
        "step": 13447
    },
    {
        "loss": 2.9113,
        "grad_norm": 3.815617561340332,
        "learning_rate": 4.434380732358655e-05,
        "epoch": 1.7930666666666668,
        "step": 13448
    },
    {
        "loss": 1.1778,
        "grad_norm": 4.028295516967773,
        "learning_rate": 4.429153011424162e-05,
        "epoch": 1.7932000000000001,
        "step": 13449
    },
    {
        "loss": 2.6093,
        "grad_norm": 3.976625919342041,
        "learning_rate": 4.423927497121145e-05,
        "epoch": 1.7933333333333334,
        "step": 13450
    },
    {
        "loss": 1.4767,
        "grad_norm": 5.191496849060059,
        "learning_rate": 4.418704191519466e-05,
        "epoch": 1.7934666666666668,
        "step": 13451
    },
    {
        "loss": 3.2349,
        "grad_norm": 2.543133497238159,
        "learning_rate": 4.4134830966880525e-05,
        "epoch": 1.7936,
        "step": 13452
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.9868907928466797,
        "learning_rate": 4.4082642146950335e-05,
        "epoch": 1.7937333333333334,
        "step": 13453
    },
    {
        "loss": 2.2921,
        "grad_norm": 4.165401458740234,
        "learning_rate": 4.4030475476075996e-05,
        "epoch": 1.7938666666666667,
        "step": 13454
    },
    {
        "loss": 2.7422,
        "grad_norm": 5.4334211349487305,
        "learning_rate": 4.3978330974921133e-05,
        "epoch": 1.794,
        "step": 13455
    },
    {
        "loss": 2.4426,
        "grad_norm": 3.8643548488616943,
        "learning_rate": 4.392620866414029e-05,
        "epoch": 1.7941333333333334,
        "step": 13456
    },
    {
        "loss": 2.0912,
        "grad_norm": 2.608508586883545,
        "learning_rate": 4.3874108564379234e-05,
        "epoch": 1.7942666666666667,
        "step": 13457
    },
    {
        "loss": 1.0351,
        "grad_norm": 4.717961311340332,
        "learning_rate": 4.3822030696274944e-05,
        "epoch": 1.7944,
        "step": 13458
    },
    {
        "loss": 2.6683,
        "grad_norm": 3.96246337890625,
        "learning_rate": 4.376997508045568e-05,
        "epoch": 1.7945333333333333,
        "step": 13459
    },
    {
        "loss": 1.5743,
        "grad_norm": 3.271146535873413,
        "learning_rate": 4.3717941737541114e-05,
        "epoch": 1.7946666666666666,
        "step": 13460
    },
    {
        "loss": 2.1519,
        "grad_norm": 4.886495113372803,
        "learning_rate": 4.36659306881414e-05,
        "epoch": 1.7948,
        "step": 13461
    },
    {
        "loss": 1.6675,
        "grad_norm": 3.37524151802063,
        "learning_rate": 4.3613941952858585e-05,
        "epoch": 1.7949333333333333,
        "step": 13462
    },
    {
        "loss": 2.5078,
        "grad_norm": 2.6193032264709473,
        "learning_rate": 4.3561975552285376e-05,
        "epoch": 1.7950666666666666,
        "step": 13463
    },
    {
        "loss": 3.061,
        "grad_norm": 5.1857099533081055,
        "learning_rate": 4.35100315070061e-05,
        "epoch": 1.7952,
        "step": 13464
    },
    {
        "loss": 1.8663,
        "grad_norm": 4.16675329208374,
        "learning_rate": 4.345810983759581e-05,
        "epoch": 1.7953333333333332,
        "step": 13465
    },
    {
        "loss": 2.4054,
        "grad_norm": 3.7381539344787598,
        "learning_rate": 4.34062105646209e-05,
        "epoch": 1.7954666666666665,
        "step": 13466
    },
    {
        "loss": 2.6955,
        "grad_norm": 3.2654292583465576,
        "learning_rate": 4.335433370863867e-05,
        "epoch": 1.7955999999999999,
        "step": 13467
    },
    {
        "loss": 2.4217,
        "grad_norm": 4.681456089019775,
        "learning_rate": 4.3302479290198e-05,
        "epoch": 1.7957333333333332,
        "step": 13468
    },
    {
        "loss": 2.6534,
        "grad_norm": 3.2234444618225098,
        "learning_rate": 4.325064732983849e-05,
        "epoch": 1.7958666666666665,
        "step": 13469
    },
    {
        "loss": 2.1938,
        "grad_norm": 4.374874114990234,
        "learning_rate": 4.319883784809081e-05,
        "epoch": 1.796,
        "step": 13470
    },
    {
        "loss": 1.3423,
        "grad_norm": 3.7459769248962402,
        "learning_rate": 4.314705086547711e-05,
        "epoch": 1.7961333333333334,
        "step": 13471
    },
    {
        "loss": 2.3222,
        "grad_norm": 3.468013286590576,
        "learning_rate": 4.309528640251032e-05,
        "epoch": 1.7962666666666667,
        "step": 13472
    },
    {
        "loss": 2.28,
        "grad_norm": 2.9443118572235107,
        "learning_rate": 4.3043544479694344e-05,
        "epoch": 1.7964,
        "step": 13473
    },
    {
        "loss": 1.7027,
        "grad_norm": 4.85701322555542,
        "learning_rate": 4.2991825117524574e-05,
        "epoch": 1.7965333333333333,
        "step": 13474
    },
    {
        "loss": 2.3005,
        "grad_norm": 3.4611902236938477,
        "learning_rate": 4.2940128336487175e-05,
        "epoch": 1.7966666666666666,
        "step": 13475
    },
    {
        "loss": 2.5139,
        "grad_norm": 2.8510982990264893,
        "learning_rate": 4.288845415705921e-05,
        "epoch": 1.7968,
        "step": 13476
    },
    {
        "loss": 1.8091,
        "grad_norm": 4.282658100128174,
        "learning_rate": 4.283680259970927e-05,
        "epoch": 1.7969333333333335,
        "step": 13477
    },
    {
        "loss": 1.7308,
        "grad_norm": 3.909485340118408,
        "learning_rate": 4.278517368489663e-05,
        "epoch": 1.7970666666666668,
        "step": 13478
    },
    {
        "loss": 2.1112,
        "grad_norm": 3.4541804790496826,
        "learning_rate": 4.273356743307151e-05,
        "epoch": 1.7972000000000001,
        "step": 13479
    },
    {
        "loss": 2.4967,
        "grad_norm": 3.9397671222686768,
        "learning_rate": 4.268198386467557e-05,
        "epoch": 1.7973333333333334,
        "step": 13480
    },
    {
        "loss": 1.9102,
        "grad_norm": 5.014462471008301,
        "learning_rate": 4.2630423000141164e-05,
        "epoch": 1.7974666666666668,
        "step": 13481
    },
    {
        "loss": 2.667,
        "grad_norm": 1.5814241170883179,
        "learning_rate": 4.2578884859891534e-05,
        "epoch": 1.7976,
        "step": 13482
    },
    {
        "loss": 1.8662,
        "grad_norm": 3.6998751163482666,
        "learning_rate": 4.2527369464341426e-05,
        "epoch": 1.7977333333333334,
        "step": 13483
    },
    {
        "loss": 0.7696,
        "grad_norm": 4.639291286468506,
        "learning_rate": 4.247587683389607e-05,
        "epoch": 1.7978666666666667,
        "step": 13484
    },
    {
        "loss": 1.0452,
        "grad_norm": 4.455013275146484,
        "learning_rate": 4.2424406988951904e-05,
        "epoch": 1.798,
        "step": 13485
    },
    {
        "loss": 1.7998,
        "grad_norm": 1.9373829364776611,
        "learning_rate": 4.237295994989624e-05,
        "epoch": 1.7981333333333334,
        "step": 13486
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.491649627685547,
        "learning_rate": 4.2321535737107566e-05,
        "epoch": 1.7982666666666667,
        "step": 13487
    },
    {
        "loss": 2.3398,
        "grad_norm": 3.752963066101074,
        "learning_rate": 4.227013437095503e-05,
        "epoch": 1.7984,
        "step": 13488
    },
    {
        "loss": 0.9539,
        "grad_norm": 4.178788185119629,
        "learning_rate": 4.221875587179922e-05,
        "epoch": 1.7985333333333333,
        "step": 13489
    },
    {
        "loss": 2.1546,
        "grad_norm": 2.331186532974243,
        "learning_rate": 4.216740025999084e-05,
        "epoch": 1.7986666666666666,
        "step": 13490
    },
    {
        "loss": 2.2577,
        "grad_norm": 3.2957093715667725,
        "learning_rate": 4.2116067555872275e-05,
        "epoch": 1.7988,
        "step": 13491
    },
    {
        "loss": 1.4765,
        "grad_norm": 4.461855411529541,
        "learning_rate": 4.206475777977674e-05,
        "epoch": 1.7989333333333333,
        "step": 13492
    },
    {
        "loss": 2.3402,
        "grad_norm": 4.434079647064209,
        "learning_rate": 4.201347095202801e-05,
        "epoch": 1.7990666666666666,
        "step": 13493
    },
    {
        "loss": 1.8728,
        "grad_norm": 4.371685981750488,
        "learning_rate": 4.1962207092941065e-05,
        "epoch": 1.7992,
        "step": 13494
    },
    {
        "loss": 1.3361,
        "grad_norm": 4.921158313751221,
        "learning_rate": 4.19109662228215e-05,
        "epoch": 1.7993333333333332,
        "step": 13495
    },
    {
        "loss": 1.1947,
        "grad_norm": 3.3364450931549072,
        "learning_rate": 4.185974836196627e-05,
        "epoch": 1.7994666666666665,
        "step": 13496
    },
    {
        "loss": 2.263,
        "grad_norm": 2.9016482830047607,
        "learning_rate": 4.180855353066273e-05,
        "epoch": 1.7995999999999999,
        "step": 13497
    },
    {
        "loss": 2.4427,
        "grad_norm": 5.056146144866943,
        "learning_rate": 4.175738174918962e-05,
        "epoch": 1.7997333333333332,
        "step": 13498
    },
    {
        "loss": 2.3893,
        "grad_norm": 3.2041873931884766,
        "learning_rate": 4.170623303781582e-05,
        "epoch": 1.7998666666666665,
        "step": 13499
    },
    {
        "loss": 2.3301,
        "grad_norm": 3.7394888401031494,
        "learning_rate": 4.1655107416801785e-05,
        "epoch": 1.8,
        "step": 13500
    },
    {
        "loss": 1.4008,
        "grad_norm": 4.593653678894043,
        "learning_rate": 4.160400490639859e-05,
        "epoch": 1.8001333333333334,
        "step": 13501
    },
    {
        "loss": 2.364,
        "grad_norm": 4.463913440704346,
        "learning_rate": 4.155292552684802e-05,
        "epoch": 1.8002666666666667,
        "step": 13502
    },
    {
        "loss": 0.8862,
        "grad_norm": 2.0098772048950195,
        "learning_rate": 4.1501869298382845e-05,
        "epoch": 1.8004,
        "step": 13503
    },
    {
        "loss": 0.5335,
        "grad_norm": 2.6103403568267822,
        "learning_rate": 4.145083624122642e-05,
        "epoch": 1.8005333333333333,
        "step": 13504
    },
    {
        "loss": 2.2884,
        "grad_norm": 4.082267761230469,
        "learning_rate": 4.139982637559341e-05,
        "epoch": 1.8006666666666666,
        "step": 13505
    },
    {
        "loss": 2.7154,
        "grad_norm": 2.4703683853149414,
        "learning_rate": 4.1348839721688815e-05,
        "epoch": 1.8008,
        "step": 13506
    },
    {
        "loss": 2.5358,
        "grad_norm": 3.7011868953704834,
        "learning_rate": 4.129787629970865e-05,
        "epoch": 1.8009333333333335,
        "step": 13507
    },
    {
        "loss": 1.2674,
        "grad_norm": 7.308583736419678,
        "learning_rate": 4.124693612983963e-05,
        "epoch": 1.8010666666666668,
        "step": 13508
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.9739348888397217,
        "learning_rate": 4.1196019232259363e-05,
        "epoch": 1.8012000000000001,
        "step": 13509
    },
    {
        "loss": 2.6366,
        "grad_norm": 3.0686912536621094,
        "learning_rate": 4.114512562713648e-05,
        "epoch": 1.8013333333333335,
        "step": 13510
    },
    {
        "loss": 2.5165,
        "grad_norm": 5.522133827209473,
        "learning_rate": 4.10942553346297e-05,
        "epoch": 1.8014666666666668,
        "step": 13511
    },
    {
        "loss": 2.3701,
        "grad_norm": 5.086309909820557,
        "learning_rate": 4.104340837488915e-05,
        "epoch": 1.8016,
        "step": 13512
    },
    {
        "loss": 1.9796,
        "grad_norm": 4.599233627319336,
        "learning_rate": 4.0992584768055345e-05,
        "epoch": 1.8017333333333334,
        "step": 13513
    },
    {
        "loss": 2.1052,
        "grad_norm": 4.750973224639893,
        "learning_rate": 4.094178453425991e-05,
        "epoch": 1.8018666666666667,
        "step": 13514
    },
    {
        "loss": 1.612,
        "grad_norm": 4.077548980712891,
        "learning_rate": 4.0891007693624816e-05,
        "epoch": 1.802,
        "step": 13515
    },
    {
        "loss": 2.5561,
        "grad_norm": 3.1398563385009766,
        "learning_rate": 4.084025426626302e-05,
        "epoch": 1.8021333333333334,
        "step": 13516
    },
    {
        "loss": 2.7283,
        "grad_norm": 2.896890878677368,
        "learning_rate": 4.078952427227796e-05,
        "epoch": 1.8022666666666667,
        "step": 13517
    },
    {
        "loss": 2.5686,
        "grad_norm": 4.034061908721924,
        "learning_rate": 4.073881773176409e-05,
        "epoch": 1.8024,
        "step": 13518
    },
    {
        "loss": 2.0618,
        "grad_norm": 4.273348331451416,
        "learning_rate": 4.0688134664806684e-05,
        "epoch": 1.8025333333333333,
        "step": 13519
    },
    {
        "loss": 2.836,
        "grad_norm": 3.192089557647705,
        "learning_rate": 4.063747509148099e-05,
        "epoch": 1.8026666666666666,
        "step": 13520
    },
    {
        "loss": 1.7102,
        "grad_norm": 4.816628456115723,
        "learning_rate": 4.0586839031853795e-05,
        "epoch": 1.8028,
        "step": 13521
    },
    {
        "loss": 1.7438,
        "grad_norm": 3.474984884262085,
        "learning_rate": 4.0536226505982034e-05,
        "epoch": 1.8029333333333333,
        "step": 13522
    },
    {
        "loss": 2.6514,
        "grad_norm": 4.16533088684082,
        "learning_rate": 4.0485637533913644e-05,
        "epoch": 1.8030666666666666,
        "step": 13523
    },
    {
        "loss": 2.422,
        "grad_norm": 3.4928841590881348,
        "learning_rate": 4.043507213568701e-05,
        "epoch": 1.8032,
        "step": 13524
    },
    {
        "loss": 3.4255,
        "grad_norm": 5.5890021324157715,
        "learning_rate": 4.0384530331331304e-05,
        "epoch": 1.8033333333333332,
        "step": 13525
    },
    {
        "loss": 1.7106,
        "grad_norm": 3.2748868465423584,
        "learning_rate": 4.033401214086613e-05,
        "epoch": 1.8034666666666666,
        "step": 13526
    },
    {
        "loss": 2.6353,
        "grad_norm": 2.712904214859009,
        "learning_rate": 4.028351758430215e-05,
        "epoch": 1.8035999999999999,
        "step": 13527
    },
    {
        "loss": 2.5148,
        "grad_norm": 6.554592132568359,
        "learning_rate": 4.0233046681640384e-05,
        "epoch": 1.8037333333333332,
        "step": 13528
    },
    {
        "loss": 2.6966,
        "grad_norm": 2.452498197555542,
        "learning_rate": 4.018259945287234e-05,
        "epoch": 1.8038666666666665,
        "step": 13529
    },
    {
        "loss": 1.3017,
        "grad_norm": 4.9276251792907715,
        "learning_rate": 4.0132175917980596e-05,
        "epoch": 1.804,
        "step": 13530
    },
    {
        "loss": 0.4834,
        "grad_norm": 2.738455295562744,
        "learning_rate": 4.008177609693795e-05,
        "epoch": 1.8041333333333334,
        "step": 13531
    },
    {
        "loss": 1.8766,
        "grad_norm": 4.662381649017334,
        "learning_rate": 4.003140000970784e-05,
        "epoch": 1.8042666666666667,
        "step": 13532
    },
    {
        "loss": 1.7194,
        "grad_norm": 3.9674854278564453,
        "learning_rate": 3.998104767624467e-05,
        "epoch": 1.8044,
        "step": 13533
    },
    {
        "loss": 1.5223,
        "grad_norm": 4.094712734222412,
        "learning_rate": 3.993071911649299e-05,
        "epoch": 1.8045333333333333,
        "step": 13534
    },
    {
        "loss": 2.1376,
        "grad_norm": 3.6819496154785156,
        "learning_rate": 3.988041435038804e-05,
        "epoch": 1.8046666666666666,
        "step": 13535
    },
    {
        "loss": 2.8031,
        "grad_norm": 3.4031386375427246,
        "learning_rate": 3.983013339785596e-05,
        "epoch": 1.8048,
        "step": 13536
    },
    {
        "loss": 2.631,
        "grad_norm": 5.135417461395264,
        "learning_rate": 3.9779876278813034e-05,
        "epoch": 1.8049333333333333,
        "step": 13537
    },
    {
        "loss": 1.8357,
        "grad_norm": 3.1843454837799072,
        "learning_rate": 3.9729643013166196e-05,
        "epoch": 1.8050666666666668,
        "step": 13538
    },
    {
        "loss": 1.4303,
        "grad_norm": 4.589407444000244,
        "learning_rate": 3.967943362081328e-05,
        "epoch": 1.8052000000000001,
        "step": 13539
    },
    {
        "loss": 2.8898,
        "grad_norm": 2.177243947982788,
        "learning_rate": 3.962924812164219e-05,
        "epoch": 1.8053333333333335,
        "step": 13540
    },
    {
        "loss": 2.0521,
        "grad_norm": 2.3646721839904785,
        "learning_rate": 3.957908653553154e-05,
        "epoch": 1.8054666666666668,
        "step": 13541
    },
    {
        "loss": 2.2923,
        "grad_norm": 3.8852343559265137,
        "learning_rate": 3.952894888235072e-05,
        "epoch": 1.8056,
        "step": 13542
    },
    {
        "loss": 2.2144,
        "grad_norm": 2.2019782066345215,
        "learning_rate": 3.9478835181959273e-05,
        "epoch": 1.8057333333333334,
        "step": 13543
    },
    {
        "loss": 2.1681,
        "grad_norm": 3.942378282546997,
        "learning_rate": 3.942874545420746e-05,
        "epoch": 1.8058666666666667,
        "step": 13544
    },
    {
        "loss": 2.1156,
        "grad_norm": 2.6117916107177734,
        "learning_rate": 3.937867971893583e-05,
        "epoch": 1.806,
        "step": 13545
    },
    {
        "loss": 2.3321,
        "grad_norm": 3.435065746307373,
        "learning_rate": 3.9328637995975827e-05,
        "epoch": 1.8061333333333334,
        "step": 13546
    },
    {
        "loss": 2.2561,
        "grad_norm": 5.117383003234863,
        "learning_rate": 3.9278620305148974e-05,
        "epoch": 1.8062666666666667,
        "step": 13547
    },
    {
        "loss": 2.256,
        "grad_norm": 3.740104913711548,
        "learning_rate": 3.92286266662677e-05,
        "epoch": 1.8064,
        "step": 13548
    },
    {
        "loss": 1.9448,
        "grad_norm": 5.300362586975098,
        "learning_rate": 3.917865709913426e-05,
        "epoch": 1.8065333333333333,
        "step": 13549
    },
    {
        "loss": 2.0793,
        "grad_norm": 3.86030912399292,
        "learning_rate": 3.9128711623542024e-05,
        "epoch": 1.8066666666666666,
        "step": 13550
    },
    {
        "loss": 2.9808,
        "grad_norm": 2.9231536388397217,
        "learning_rate": 3.90787902592746e-05,
        "epoch": 1.8068,
        "step": 13551
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.4969189167022705,
        "learning_rate": 3.902889302610597e-05,
        "epoch": 1.8069333333333333,
        "step": 13552
    },
    {
        "loss": 2.3096,
        "grad_norm": 2.8023180961608887,
        "learning_rate": 3.8979019943800545e-05,
        "epoch": 1.8070666666666666,
        "step": 13553
    },
    {
        "loss": 2.2576,
        "grad_norm": 3.498373031616211,
        "learning_rate": 3.892917103211312e-05,
        "epoch": 1.8072,
        "step": 13554
    },
    {
        "loss": 1.6223,
        "grad_norm": 4.719191551208496,
        "learning_rate": 3.88793463107893e-05,
        "epoch": 1.8073333333333332,
        "step": 13555
    },
    {
        "loss": 0.6098,
        "grad_norm": 3.1285955905914307,
        "learning_rate": 3.882954579956452e-05,
        "epoch": 1.8074666666666666,
        "step": 13556
    },
    {
        "loss": 2.4134,
        "grad_norm": 5.10469388961792,
        "learning_rate": 3.877976951816529e-05,
        "epoch": 1.8075999999999999,
        "step": 13557
    },
    {
        "loss": 2.3373,
        "grad_norm": 4.725907325744629,
        "learning_rate": 3.873001748630779e-05,
        "epoch": 1.8077333333333332,
        "step": 13558
    },
    {
        "loss": 2.0354,
        "grad_norm": 3.577970027923584,
        "learning_rate": 3.8680289723699095e-05,
        "epoch": 1.8078666666666665,
        "step": 13559
    },
    {
        "loss": 1.739,
        "grad_norm": 3.4114716053009033,
        "learning_rate": 3.863058625003668e-05,
        "epoch": 1.808,
        "step": 13560
    },
    {
        "loss": 1.7394,
        "grad_norm": 2.8361687660217285,
        "learning_rate": 3.8580907085008214e-05,
        "epoch": 1.8081333333333334,
        "step": 13561
    },
    {
        "loss": 1.5384,
        "grad_norm": 2.8968887329101562,
        "learning_rate": 3.853125224829168e-05,
        "epoch": 1.8082666666666667,
        "step": 13562
    },
    {
        "loss": 1.8071,
        "grad_norm": 8.405801773071289,
        "learning_rate": 3.848162175955549e-05,
        "epoch": 1.8084,
        "step": 13563
    },
    {
        "loss": 2.6377,
        "grad_norm": 3.770069122314453,
        "learning_rate": 3.8432015638458675e-05,
        "epoch": 1.8085333333333333,
        "step": 13564
    },
    {
        "loss": 1.9008,
        "grad_norm": 3.6353111267089844,
        "learning_rate": 3.838243390465022e-05,
        "epoch": 1.8086666666666666,
        "step": 13565
    },
    {
        "loss": 2.1637,
        "grad_norm": 3.0937023162841797,
        "learning_rate": 3.8332876577769686e-05,
        "epoch": 1.8088,
        "step": 13566
    },
    {
        "loss": 2.5318,
        "grad_norm": 2.6303112506866455,
        "learning_rate": 3.8283343677446717e-05,
        "epoch": 1.8089333333333333,
        "step": 13567
    },
    {
        "loss": 2.1261,
        "grad_norm": 3.116101026535034,
        "learning_rate": 3.823383522330162e-05,
        "epoch": 1.8090666666666668,
        "step": 13568
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.909864664077759,
        "learning_rate": 3.818435123494509e-05,
        "epoch": 1.8092000000000001,
        "step": 13569
    },
    {
        "loss": 2.7744,
        "grad_norm": 3.0772616863250732,
        "learning_rate": 3.813489173197744e-05,
        "epoch": 1.8093333333333335,
        "step": 13570
    },
    {
        "loss": 1.4032,
        "grad_norm": 3.464168071746826,
        "learning_rate": 3.8085456733990064e-05,
        "epoch": 1.8094666666666668,
        "step": 13571
    },
    {
        "loss": 2.5394,
        "grad_norm": 3.961496114730835,
        "learning_rate": 3.8036046260564186e-05,
        "epoch": 1.8096,
        "step": 13572
    },
    {
        "loss": 1.9614,
        "grad_norm": 4.5863823890686035,
        "learning_rate": 3.798666033127158e-05,
        "epoch": 1.8097333333333334,
        "step": 13573
    },
    {
        "loss": 0.9297,
        "grad_norm": 3.4360241889953613,
        "learning_rate": 3.793729896567413e-05,
        "epoch": 1.8098666666666667,
        "step": 13574
    },
    {
        "loss": 1.4608,
        "grad_norm": 4.024065017700195,
        "learning_rate": 3.788796218332407e-05,
        "epoch": 1.81,
        "step": 13575
    },
    {
        "loss": 0.8862,
        "grad_norm": 3.0019946098327637,
        "learning_rate": 3.7838650003763676e-05,
        "epoch": 1.8101333333333334,
        "step": 13576
    },
    {
        "loss": 2.6816,
        "grad_norm": 3.8899359703063965,
        "learning_rate": 3.7789362446525836e-05,
        "epoch": 1.8102666666666667,
        "step": 13577
    },
    {
        "loss": 2.321,
        "grad_norm": 3.9087085723876953,
        "learning_rate": 3.7740099531133756e-05,
        "epoch": 1.8104,
        "step": 13578
    },
    {
        "loss": 2.7007,
        "grad_norm": 2.276338815689087,
        "learning_rate": 3.7690861277100156e-05,
        "epoch": 1.8105333333333333,
        "step": 13579
    },
    {
        "loss": 2.2844,
        "grad_norm": 3.1799397468566895,
        "learning_rate": 3.764164770392884e-05,
        "epoch": 1.8106666666666666,
        "step": 13580
    },
    {
        "loss": 2.2199,
        "grad_norm": 3.3110122680664062,
        "learning_rate": 3.759245883111322e-05,
        "epoch": 1.8108,
        "step": 13581
    },
    {
        "loss": 1.8156,
        "grad_norm": 4.0050530433654785,
        "learning_rate": 3.754329467813741e-05,
        "epoch": 1.8109333333333333,
        "step": 13582
    },
    {
        "loss": 2.7158,
        "grad_norm": 3.3996422290802,
        "learning_rate": 3.749415526447543e-05,
        "epoch": 1.8110666666666666,
        "step": 13583
    },
    {
        "loss": 1.7491,
        "grad_norm": 5.16589879989624,
        "learning_rate": 3.74450406095915e-05,
        "epoch": 1.8112,
        "step": 13584
    },
    {
        "loss": 2.2804,
        "grad_norm": 3.668243646621704,
        "learning_rate": 3.7395950732940024e-05,
        "epoch": 1.8113333333333332,
        "step": 13585
    },
    {
        "loss": 2.9348,
        "grad_norm": 2.4519541263580322,
        "learning_rate": 3.734688565396592e-05,
        "epoch": 1.8114666666666666,
        "step": 13586
    },
    {
        "loss": 2.6502,
        "grad_norm": 2.749558687210083,
        "learning_rate": 3.729784539210387e-05,
        "epoch": 1.8115999999999999,
        "step": 13587
    },
    {
        "loss": 2.2052,
        "grad_norm": 3.30324387550354,
        "learning_rate": 3.7248829966778795e-05,
        "epoch": 1.8117333333333332,
        "step": 13588
    },
    {
        "loss": 2.6893,
        "grad_norm": 3.2684762477874756,
        "learning_rate": 3.7199839397406143e-05,
        "epoch": 1.8118666666666665,
        "step": 13589
    },
    {
        "loss": 1.3829,
        "grad_norm": 2.056065797805786,
        "learning_rate": 3.715087370339104e-05,
        "epoch": 1.812,
        "step": 13590
    },
    {
        "loss": 1.9185,
        "grad_norm": 2.9433350563049316,
        "learning_rate": 3.7101932904128964e-05,
        "epoch": 1.8121333333333334,
        "step": 13591
    },
    {
        "loss": 2.7469,
        "grad_norm": 5.569040775299072,
        "learning_rate": 3.705301701900564e-05,
        "epoch": 1.8122666666666667,
        "step": 13592
    },
    {
        "loss": 2.7748,
        "grad_norm": 3.1061623096466064,
        "learning_rate": 3.700412606739677e-05,
        "epoch": 1.8124,
        "step": 13593
    },
    {
        "loss": 2.521,
        "grad_norm": 2.7399802207946777,
        "learning_rate": 3.695526006866825e-05,
        "epoch": 1.8125333333333333,
        "step": 13594
    },
    {
        "loss": 2.4302,
        "grad_norm": 4.194794178009033,
        "learning_rate": 3.69064190421759e-05,
        "epoch": 1.8126666666666666,
        "step": 13595
    },
    {
        "loss": 1.8301,
        "grad_norm": 2.786139965057373,
        "learning_rate": 3.685760300726605e-05,
        "epoch": 1.8128,
        "step": 13596
    },
    {
        "loss": 2.1674,
        "grad_norm": 3.627431869506836,
        "learning_rate": 3.680881198327474e-05,
        "epoch": 1.8129333333333333,
        "step": 13597
    },
    {
        "loss": 2.3708,
        "grad_norm": 3.571723699569702,
        "learning_rate": 3.676004598952839e-05,
        "epoch": 1.8130666666666668,
        "step": 13598
    },
    {
        "loss": 1.9764,
        "grad_norm": 4.038197994232178,
        "learning_rate": 3.6711305045343336e-05,
        "epoch": 1.8132000000000001,
        "step": 13599
    },
    {
        "loss": 2.2208,
        "grad_norm": 3.806361436843872,
        "learning_rate": 3.666258917002587e-05,
        "epoch": 1.8133333333333335,
        "step": 13600
    },
    {
        "loss": 2.4429,
        "grad_norm": 3.9368104934692383,
        "learning_rate": 3.6613898382872756e-05,
        "epoch": 1.8134666666666668,
        "step": 13601
    },
    {
        "loss": 2.5577,
        "grad_norm": 3.503718137741089,
        "learning_rate": 3.6565232703170514e-05,
        "epoch": 1.8136,
        "step": 13602
    },
    {
        "loss": 2.5728,
        "grad_norm": 2.9872074127197266,
        "learning_rate": 3.6516592150195706e-05,
        "epoch": 1.8137333333333334,
        "step": 13603
    },
    {
        "loss": 2.449,
        "grad_norm": 2.5337882041931152,
        "learning_rate": 3.646797674321496e-05,
        "epoch": 1.8138666666666667,
        "step": 13604
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.9034156799316406,
        "learning_rate": 3.641938650148524e-05,
        "epoch": 1.814,
        "step": 13605
    },
    {
        "loss": 2.0664,
        "grad_norm": 4.138585090637207,
        "learning_rate": 3.6370821444253045e-05,
        "epoch": 1.8141333333333334,
        "step": 13606
    },
    {
        "loss": 2.6548,
        "grad_norm": 5.168856143951416,
        "learning_rate": 3.6322281590755524e-05,
        "epoch": 1.8142666666666667,
        "step": 13607
    },
    {
        "loss": 1.5101,
        "grad_norm": 3.9446792602539062,
        "learning_rate": 3.627376696021899e-05,
        "epoch": 1.8144,
        "step": 13608
    },
    {
        "loss": 2.3302,
        "grad_norm": 4.466364860534668,
        "learning_rate": 3.6225277571860515e-05,
        "epoch": 1.8145333333333333,
        "step": 13609
    },
    {
        "loss": 2.6829,
        "grad_norm": 3.1296277046203613,
        "learning_rate": 3.6176813444887004e-05,
        "epoch": 1.8146666666666667,
        "step": 13610
    },
    {
        "loss": 1.5052,
        "grad_norm": 5.524569034576416,
        "learning_rate": 3.61283745984952e-05,
        "epoch": 1.8148,
        "step": 13611
    },
    {
        "loss": 1.3495,
        "grad_norm": 3.4033422470092773,
        "learning_rate": 3.607996105187182e-05,
        "epoch": 1.8149333333333333,
        "step": 13612
    },
    {
        "loss": 1.5964,
        "grad_norm": 5.533146381378174,
        "learning_rate": 3.6031572824193584e-05,
        "epoch": 1.8150666666666666,
        "step": 13613
    },
    {
        "loss": 1.8108,
        "grad_norm": 3.181746006011963,
        "learning_rate": 3.598320993462739e-05,
        "epoch": 1.8152,
        "step": 13614
    },
    {
        "loss": 1.5151,
        "grad_norm": 2.2637393474578857,
        "learning_rate": 3.5934872402329886e-05,
        "epoch": 1.8153333333333332,
        "step": 13615
    },
    {
        "loss": 2.4051,
        "grad_norm": 2.598050594329834,
        "learning_rate": 3.588656024644775e-05,
        "epoch": 1.8154666666666666,
        "step": 13616
    },
    {
        "loss": 2.57,
        "grad_norm": 4.450322151184082,
        "learning_rate": 3.5838273486117424e-05,
        "epoch": 1.8155999999999999,
        "step": 13617
    },
    {
        "loss": 2.6753,
        "grad_norm": 2.7994322776794434,
        "learning_rate": 3.579001214046559e-05,
        "epoch": 1.8157333333333332,
        "step": 13618
    },
    {
        "loss": 2.2876,
        "grad_norm": 3.4578335285186768,
        "learning_rate": 3.574177622860888e-05,
        "epoch": 1.8158666666666665,
        "step": 13619
    },
    {
        "loss": 1.6348,
        "grad_norm": 3.8735930919647217,
        "learning_rate": 3.5693565769653524e-05,
        "epoch": 1.8159999999999998,
        "step": 13620
    },
    {
        "loss": 1.5652,
        "grad_norm": 4.56707763671875,
        "learning_rate": 3.5645380782695924e-05,
        "epoch": 1.8161333333333334,
        "step": 13621
    },
    {
        "loss": 1.4731,
        "grad_norm": 4.166984558105469,
        "learning_rate": 3.559722128682214e-05,
        "epoch": 1.8162666666666667,
        "step": 13622
    },
    {
        "loss": 1.6879,
        "grad_norm": 3.447566270828247,
        "learning_rate": 3.554908730110855e-05,
        "epoch": 1.8164,
        "step": 13623
    },
    {
        "loss": 2.0638,
        "grad_norm": 3.7441632747650146,
        "learning_rate": 3.550097884462114e-05,
        "epoch": 1.8165333333333333,
        "step": 13624
    },
    {
        "loss": 1.4921,
        "grad_norm": 4.949899196624756,
        "learning_rate": 3.5452895936415733e-05,
        "epoch": 1.8166666666666667,
        "step": 13625
    },
    {
        "loss": 2.2925,
        "grad_norm": 4.173639297485352,
        "learning_rate": 3.540483859553808e-05,
        "epoch": 1.8168,
        "step": 13626
    },
    {
        "loss": 2.2264,
        "grad_norm": 3.2138288021087646,
        "learning_rate": 3.535680684102402e-05,
        "epoch": 1.8169333333333333,
        "step": 13627
    },
    {
        "loss": 1.9345,
        "grad_norm": 3.6792492866516113,
        "learning_rate": 3.530880069189918e-05,
        "epoch": 1.8170666666666668,
        "step": 13628
    },
    {
        "loss": 1.9721,
        "grad_norm": 4.22717809677124,
        "learning_rate": 3.526082016717862e-05,
        "epoch": 1.8172000000000001,
        "step": 13629
    },
    {
        "loss": 0.9467,
        "grad_norm": 3.8446030616760254,
        "learning_rate": 3.521286528586792e-05,
        "epoch": 1.8173333333333335,
        "step": 13630
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.9814600944519043,
        "learning_rate": 3.5164936066961916e-05,
        "epoch": 1.8174666666666668,
        "step": 13631
    },
    {
        "loss": 1.5523,
        "grad_norm": 3.499554395675659,
        "learning_rate": 3.511703252944575e-05,
        "epoch": 1.8176,
        "step": 13632
    },
    {
        "loss": 2.228,
        "grad_norm": 3.159276008605957,
        "learning_rate": 3.50691546922941e-05,
        "epoch": 1.8177333333333334,
        "step": 13633
    },
    {
        "loss": 2.2795,
        "grad_norm": 2.562368392944336,
        "learning_rate": 3.50213025744715e-05,
        "epoch": 1.8178666666666667,
        "step": 13634
    },
    {
        "loss": 1.4807,
        "grad_norm": 4.693306922912598,
        "learning_rate": 3.497347619493227e-05,
        "epoch": 1.818,
        "step": 13635
    },
    {
        "loss": 1.8698,
        "grad_norm": 4.096846103668213,
        "learning_rate": 3.492567557262072e-05,
        "epoch": 1.8181333333333334,
        "step": 13636
    },
    {
        "loss": 1.4835,
        "grad_norm": 4.218308448791504,
        "learning_rate": 3.487790072647097e-05,
        "epoch": 1.8182666666666667,
        "step": 13637
    },
    {
        "loss": 1.3091,
        "grad_norm": 3.5088226795196533,
        "learning_rate": 3.483015167540648e-05,
        "epoch": 1.8184,
        "step": 13638
    },
    {
        "loss": 1.7446,
        "grad_norm": 4.7932515144348145,
        "learning_rate": 3.478242843834103e-05,
        "epoch": 1.8185333333333333,
        "step": 13639
    },
    {
        "loss": 2.0436,
        "grad_norm": 3.0645017623901367,
        "learning_rate": 3.4734731034177816e-05,
        "epoch": 1.8186666666666667,
        "step": 13640
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.8358969688415527,
        "learning_rate": 3.4687059481810145e-05,
        "epoch": 1.8188,
        "step": 13641
    },
    {
        "loss": 2.5392,
        "grad_norm": 3.4782235622406006,
        "learning_rate": 3.463941380012073e-05,
        "epoch": 1.8189333333333333,
        "step": 13642
    },
    {
        "loss": 2.4901,
        "grad_norm": 2.5995702743530273,
        "learning_rate": 3.4591794007982215e-05,
        "epoch": 1.8190666666666666,
        "step": 13643
    },
    {
        "loss": 2.2162,
        "grad_norm": 3.6441047191619873,
        "learning_rate": 3.454420012425691e-05,
        "epoch": 1.8192,
        "step": 13644
    },
    {
        "loss": 1.9228,
        "grad_norm": 3.0298147201538086,
        "learning_rate": 3.4496632167797036e-05,
        "epoch": 1.8193333333333332,
        "step": 13645
    },
    {
        "loss": 2.7427,
        "grad_norm": 3.8454811573028564,
        "learning_rate": 3.4449090157444385e-05,
        "epoch": 1.8194666666666666,
        "step": 13646
    },
    {
        "loss": 2.7138,
        "grad_norm": 2.7939608097076416,
        "learning_rate": 3.4401574112030355e-05,
        "epoch": 1.8195999999999999,
        "step": 13647
    },
    {
        "loss": 1.6701,
        "grad_norm": 3.6674604415893555,
        "learning_rate": 3.435408405037643e-05,
        "epoch": 1.8197333333333332,
        "step": 13648
    },
    {
        "loss": 2.0356,
        "grad_norm": 3.417151689529419,
        "learning_rate": 3.430661999129354e-05,
        "epoch": 1.8198666666666665,
        "step": 13649
    },
    {
        "loss": 2.5941,
        "grad_norm": 2.6408207416534424,
        "learning_rate": 3.425918195358218e-05,
        "epoch": 1.8199999999999998,
        "step": 13650
    },
    {
        "loss": 2.418,
        "grad_norm": 4.622409343719482,
        "learning_rate": 3.421176995603296e-05,
        "epoch": 1.8201333333333334,
        "step": 13651
    },
    {
        "loss": 1.6271,
        "grad_norm": 3.4574835300445557,
        "learning_rate": 3.4164384017425843e-05,
        "epoch": 1.8202666666666667,
        "step": 13652
    },
    {
        "loss": 0.8931,
        "grad_norm": 4.295521259307861,
        "learning_rate": 3.411702415653053e-05,
        "epoch": 1.8204,
        "step": 13653
    },
    {
        "loss": 2.2718,
        "grad_norm": 3.8506317138671875,
        "learning_rate": 3.406969039210633e-05,
        "epoch": 1.8205333333333333,
        "step": 13654
    },
    {
        "loss": 1.9669,
        "grad_norm": 6.4121904373168945,
        "learning_rate": 3.402238274290256e-05,
        "epoch": 1.8206666666666667,
        "step": 13655
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.6213319301605225,
        "learning_rate": 3.397510122765766e-05,
        "epoch": 1.8208,
        "step": 13656
    },
    {
        "loss": 2.5128,
        "grad_norm": 2.881770133972168,
        "learning_rate": 3.392784586510024e-05,
        "epoch": 1.8209333333333333,
        "step": 13657
    },
    {
        "loss": 1.5715,
        "grad_norm": 4.712907314300537,
        "learning_rate": 3.388061667394824e-05,
        "epoch": 1.8210666666666666,
        "step": 13658
    },
    {
        "loss": 2.3043,
        "grad_norm": 3.3587119579315186,
        "learning_rate": 3.383341367290914e-05,
        "epoch": 1.8212000000000002,
        "step": 13659
    },
    {
        "loss": 2.2327,
        "grad_norm": 4.424499034881592,
        "learning_rate": 3.378623688068047e-05,
        "epoch": 1.8213333333333335,
        "step": 13660
    },
    {
        "loss": 1.6606,
        "grad_norm": 3.554917097091675,
        "learning_rate": 3.3739086315948966e-05,
        "epoch": 1.8214666666666668,
        "step": 13661
    },
    {
        "loss": 2.2835,
        "grad_norm": 3.3148374557495117,
        "learning_rate": 3.369196199739114e-05,
        "epoch": 1.8216,
        "step": 13662
    },
    {
        "loss": 2.3257,
        "grad_norm": 4.353216648101807,
        "learning_rate": 3.364486394367304e-05,
        "epoch": 1.8217333333333334,
        "step": 13663
    },
    {
        "loss": 2.452,
        "grad_norm": 4.671917915344238,
        "learning_rate": 3.3597792173450505e-05,
        "epoch": 1.8218666666666667,
        "step": 13664
    },
    {
        "loss": 1.8247,
        "grad_norm": 2.253403425216675,
        "learning_rate": 3.355074670536864e-05,
        "epoch": 1.822,
        "step": 13665
    },
    {
        "loss": 1.432,
        "grad_norm": 3.1954472064971924,
        "learning_rate": 3.350372755806267e-05,
        "epoch": 1.8221333333333334,
        "step": 13666
    },
    {
        "loss": 2.46,
        "grad_norm": 3.7804181575775146,
        "learning_rate": 3.3456734750156557e-05,
        "epoch": 1.8222666666666667,
        "step": 13667
    },
    {
        "loss": 1.6379,
        "grad_norm": 3.183987855911255,
        "learning_rate": 3.340976830026455e-05,
        "epoch": 1.8224,
        "step": 13668
    },
    {
        "loss": 2.398,
        "grad_norm": 2.1280786991119385,
        "learning_rate": 3.336282822699035e-05,
        "epoch": 1.8225333333333333,
        "step": 13669
    },
    {
        "loss": 2.3337,
        "grad_norm": 3.306525468826294,
        "learning_rate": 3.331591454892692e-05,
        "epoch": 1.8226666666666667,
        "step": 13670
    },
    {
        "loss": 1.9389,
        "grad_norm": 2.796041965484619,
        "learning_rate": 3.326902728465699e-05,
        "epoch": 1.8228,
        "step": 13671
    },
    {
        "loss": 2.6111,
        "grad_norm": 3.1030032634735107,
        "learning_rate": 3.3222166452752604e-05,
        "epoch": 1.8229333333333333,
        "step": 13672
    },
    {
        "loss": 1.1869,
        "grad_norm": 3.640651226043701,
        "learning_rate": 3.3175332071775724e-05,
        "epoch": 1.8230666666666666,
        "step": 13673
    },
    {
        "loss": 0.934,
        "grad_norm": 4.863485813140869,
        "learning_rate": 3.312852416027754e-05,
        "epoch": 1.8232,
        "step": 13674
    },
    {
        "loss": 1.939,
        "grad_norm": 8.233845710754395,
        "learning_rate": 3.308174273679876e-05,
        "epoch": 1.8233333333333333,
        "step": 13675
    },
    {
        "loss": 1.8252,
        "grad_norm": 4.790815353393555,
        "learning_rate": 3.30349878198696e-05,
        "epoch": 1.8234666666666666,
        "step": 13676
    },
    {
        "loss": 1.4519,
        "grad_norm": 4.513427734375,
        "learning_rate": 3.298825942800998e-05,
        "epoch": 1.8235999999999999,
        "step": 13677
    },
    {
        "loss": 2.5374,
        "grad_norm": 3.2494843006134033,
        "learning_rate": 3.2941557579729196e-05,
        "epoch": 1.8237333333333332,
        "step": 13678
    },
    {
        "loss": 2.1468,
        "grad_norm": 5.058602809906006,
        "learning_rate": 3.2894882293525966e-05,
        "epoch": 1.8238666666666665,
        "step": 13679
    },
    {
        "loss": 1.9058,
        "grad_norm": 3.793001890182495,
        "learning_rate": 3.2848233587888535e-05,
        "epoch": 1.8239999999999998,
        "step": 13680
    },
    {
        "loss": 2.2035,
        "grad_norm": 4.170968532562256,
        "learning_rate": 3.280161148129447e-05,
        "epoch": 1.8241333333333334,
        "step": 13681
    },
    {
        "loss": 1.8691,
        "grad_norm": 3.726959228515625,
        "learning_rate": 3.275501599221117e-05,
        "epoch": 1.8242666666666667,
        "step": 13682
    },
    {
        "loss": 2.4256,
        "grad_norm": 2.7701003551483154,
        "learning_rate": 3.2708447139095185e-05,
        "epoch": 1.8244,
        "step": 13683
    },
    {
        "loss": 1.7088,
        "grad_norm": 3.3788084983825684,
        "learning_rate": 3.2661904940392564e-05,
        "epoch": 1.8245333333333333,
        "step": 13684
    },
    {
        "loss": 2.5989,
        "grad_norm": 3.543842315673828,
        "learning_rate": 3.261538941453877e-05,
        "epoch": 1.8246666666666667,
        "step": 13685
    },
    {
        "loss": 1.7361,
        "grad_norm": 6.62698221206665,
        "learning_rate": 3.25689005799588e-05,
        "epoch": 1.8248,
        "step": 13686
    },
    {
        "loss": 2.299,
        "grad_norm": 3.444619655609131,
        "learning_rate": 3.252243845506728e-05,
        "epoch": 1.8249333333333333,
        "step": 13687
    },
    {
        "loss": 1.2095,
        "grad_norm": 4.46985387802124,
        "learning_rate": 3.247600305826765e-05,
        "epoch": 1.8250666666666666,
        "step": 13688
    },
    {
        "loss": 1.7844,
        "grad_norm": 4.643222808837891,
        "learning_rate": 3.242959440795334e-05,
        "epoch": 1.8252000000000002,
        "step": 13689
    },
    {
        "loss": 1.905,
        "grad_norm": 3.0490479469299316,
        "learning_rate": 3.238321252250685e-05,
        "epoch": 1.8253333333333335,
        "step": 13690
    },
    {
        "loss": 1.9572,
        "grad_norm": 4.468321800231934,
        "learning_rate": 3.233685742030039e-05,
        "epoch": 1.8254666666666668,
        "step": 13691
    },
    {
        "loss": 2.3275,
        "grad_norm": 2.6289987564086914,
        "learning_rate": 3.229052911969525e-05,
        "epoch": 1.8256000000000001,
        "step": 13692
    },
    {
        "loss": 2.1204,
        "grad_norm": 4.122384071350098,
        "learning_rate": 3.224422763904226e-05,
        "epoch": 1.8257333333333334,
        "step": 13693
    },
    {
        "loss": 2.3114,
        "grad_norm": 3.608522891998291,
        "learning_rate": 3.219795299668144e-05,
        "epoch": 1.8258666666666667,
        "step": 13694
    },
    {
        "loss": 2.2981,
        "grad_norm": 3.3800268173217773,
        "learning_rate": 3.215170521094257e-05,
        "epoch": 1.826,
        "step": 13695
    },
    {
        "loss": 1.197,
        "grad_norm": 5.109278678894043,
        "learning_rate": 3.210548430014448e-05,
        "epoch": 1.8261333333333334,
        "step": 13696
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.537252187728882,
        "learning_rate": 3.205929028259529e-05,
        "epoch": 1.8262666666666667,
        "step": 13697
    },
    {
        "loss": 1.9632,
        "grad_norm": 4.795890808105469,
        "learning_rate": 3.2013123176592807e-05,
        "epoch": 1.8264,
        "step": 13698
    },
    {
        "loss": 2.269,
        "grad_norm": 3.3629565238952637,
        "learning_rate": 3.196698300042382e-05,
        "epoch": 1.8265333333333333,
        "step": 13699
    },
    {
        "loss": 2.0572,
        "grad_norm": 3.5582761764526367,
        "learning_rate": 3.192086977236476e-05,
        "epoch": 1.8266666666666667,
        "step": 13700
    },
    {
        "loss": 2.3631,
        "grad_norm": 3.831505298614502,
        "learning_rate": 3.187478351068116e-05,
        "epoch": 1.8268,
        "step": 13701
    },
    {
        "loss": 1.3375,
        "grad_norm": 4.209169864654541,
        "learning_rate": 3.182872423362797e-05,
        "epoch": 1.8269333333333333,
        "step": 13702
    },
    {
        "loss": 2.1417,
        "grad_norm": 4.1010003089904785,
        "learning_rate": 3.1782691959449275e-05,
        "epoch": 1.8270666666666666,
        "step": 13703
    },
    {
        "loss": 1.8527,
        "grad_norm": 5.043578624725342,
        "learning_rate": 3.173668670637883e-05,
        "epoch": 1.8272,
        "step": 13704
    },
    {
        "loss": 2.6968,
        "grad_norm": 4.05817985534668,
        "learning_rate": 3.169070849263941e-05,
        "epoch": 1.8273333333333333,
        "step": 13705
    },
    {
        "loss": 2.4932,
        "grad_norm": 3.0358195304870605,
        "learning_rate": 3.1644757336442986e-05,
        "epoch": 1.8274666666666666,
        "step": 13706
    },
    {
        "loss": 2.5233,
        "grad_norm": 3.4632065296173096,
        "learning_rate": 3.1598833255991236e-05,
        "epoch": 1.8276,
        "step": 13707
    },
    {
        "loss": 2.5246,
        "grad_norm": 3.7560386657714844,
        "learning_rate": 3.155293626947466e-05,
        "epoch": 1.8277333333333332,
        "step": 13708
    },
    {
        "loss": 2.5296,
        "grad_norm": 2.146860361099243,
        "learning_rate": 3.150706639507317e-05,
        "epoch": 1.8278666666666665,
        "step": 13709
    },
    {
        "loss": 2.2169,
        "grad_norm": 2.5145347118377686,
        "learning_rate": 3.146122365095618e-05,
        "epoch": 1.8279999999999998,
        "step": 13710
    },
    {
        "loss": 2.1122,
        "grad_norm": 2.5253493785858154,
        "learning_rate": 3.1415408055282025e-05,
        "epoch": 1.8281333333333334,
        "step": 13711
    },
    {
        "loss": 2.1168,
        "grad_norm": 3.4379122257232666,
        "learning_rate": 3.136961962619843e-05,
        "epoch": 1.8282666666666667,
        "step": 13712
    },
    {
        "loss": 2.4449,
        "grad_norm": 3.5686967372894287,
        "learning_rate": 3.132385838184229e-05,
        "epoch": 1.8284,
        "step": 13713
    },
    {
        "loss": 1.9891,
        "grad_norm": 4.654528617858887,
        "learning_rate": 3.127812434033991e-05,
        "epoch": 1.8285333333333333,
        "step": 13714
    },
    {
        "loss": 0.8207,
        "grad_norm": 4.079766273498535,
        "learning_rate": 3.123241751980656e-05,
        "epoch": 1.8286666666666667,
        "step": 13715
    },
    {
        "loss": 2.7254,
        "grad_norm": 4.459889888763428,
        "learning_rate": 3.118673793834716e-05,
        "epoch": 1.8288,
        "step": 13716
    },
    {
        "loss": 2.6203,
        "grad_norm": 4.188510894775391,
        "learning_rate": 3.1141085614055124e-05,
        "epoch": 1.8289333333333333,
        "step": 13717
    },
    {
        "loss": 2.6927,
        "grad_norm": 2.8378522396087646,
        "learning_rate": 3.10954605650137e-05,
        "epoch": 1.8290666666666666,
        "step": 13718
    },
    {
        "loss": 2.4883,
        "grad_norm": 2.6410951614379883,
        "learning_rate": 3.104986280929527e-05,
        "epoch": 1.8292000000000002,
        "step": 13719
    },
    {
        "loss": 2.4422,
        "grad_norm": 3.1714186668395996,
        "learning_rate": 3.100429236496109e-05,
        "epoch": 1.8293333333333335,
        "step": 13720
    },
    {
        "loss": 1.8354,
        "grad_norm": 4.686129570007324,
        "learning_rate": 3.0958749250061825e-05,
        "epoch": 1.8294666666666668,
        "step": 13721
    },
    {
        "loss": 2.3458,
        "grad_norm": 1.8916683197021484,
        "learning_rate": 3.091323348263711e-05,
        "epoch": 1.8296000000000001,
        "step": 13722
    },
    {
        "loss": 1.6416,
        "grad_norm": 3.6974995136260986,
        "learning_rate": 3.086774508071611e-05,
        "epoch": 1.8297333333333334,
        "step": 13723
    },
    {
        "loss": 1.7749,
        "grad_norm": 4.1709747314453125,
        "learning_rate": 3.0822284062316784e-05,
        "epoch": 1.8298666666666668,
        "step": 13724
    },
    {
        "loss": 2.091,
        "grad_norm": 3.204815149307251,
        "learning_rate": 3.077685044544663e-05,
        "epoch": 1.83,
        "step": 13725
    },
    {
        "loss": 2.704,
        "grad_norm": 2.569612979888916,
        "learning_rate": 3.073144424810168e-05,
        "epoch": 1.8301333333333334,
        "step": 13726
    },
    {
        "loss": 2.5809,
        "grad_norm": 2.5840110778808594,
        "learning_rate": 3.068606548826773e-05,
        "epoch": 1.8302666666666667,
        "step": 13727
    },
    {
        "loss": 1.3527,
        "grad_norm": 3.1157946586608887,
        "learning_rate": 3.064071418391948e-05,
        "epoch": 1.8304,
        "step": 13728
    },
    {
        "loss": 2.3527,
        "grad_norm": 3.6397621631622314,
        "learning_rate": 3.0595390353020694e-05,
        "epoch": 1.8305333333333333,
        "step": 13729
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.786602735519409,
        "learning_rate": 3.05500940135243e-05,
        "epoch": 1.8306666666666667,
        "step": 13730
    },
    {
        "loss": 2.631,
        "grad_norm": 3.5202152729034424,
        "learning_rate": 3.0504825183372175e-05,
        "epoch": 1.8308,
        "step": 13731
    },
    {
        "loss": 1.8169,
        "grad_norm": 4.382386684417725,
        "learning_rate": 3.0459583880495735e-05,
        "epoch": 1.8309333333333333,
        "step": 13732
    },
    {
        "loss": 2.3194,
        "grad_norm": 3.0804946422576904,
        "learning_rate": 3.041437012281504e-05,
        "epoch": 1.8310666666666666,
        "step": 13733
    },
    {
        "loss": 0.9658,
        "grad_norm": 4.861147403717041,
        "learning_rate": 3.0369183928239443e-05,
        "epoch": 1.8312,
        "step": 13734
    },
    {
        "loss": 1.2047,
        "grad_norm": 3.5712544918060303,
        "learning_rate": 3.0324025314667283e-05,
        "epoch": 1.8313333333333333,
        "step": 13735
    },
    {
        "loss": 2.301,
        "grad_norm": 3.39355206489563,
        "learning_rate": 3.0278894299986082e-05,
        "epoch": 1.8314666666666666,
        "step": 13736
    },
    {
        "loss": 1.0188,
        "grad_norm": 3.239866256713867,
        "learning_rate": 3.0233790902072522e-05,
        "epoch": 1.8316,
        "step": 13737
    },
    {
        "loss": 2.2211,
        "grad_norm": 3.2556066513061523,
        "learning_rate": 3.0188715138792134e-05,
        "epoch": 1.8317333333333332,
        "step": 13738
    },
    {
        "loss": 2.5911,
        "grad_norm": 5.072684288024902,
        "learning_rate": 3.014366702799952e-05,
        "epoch": 1.8318666666666665,
        "step": 13739
    },
    {
        "loss": 2.6274,
        "grad_norm": 3.8756296634674072,
        "learning_rate": 3.009864658753834e-05,
        "epoch": 1.8319999999999999,
        "step": 13740
    },
    {
        "loss": 2.6415,
        "grad_norm": 3.3143796920776367,
        "learning_rate": 3.005365383524156e-05,
        "epoch": 1.8321333333333332,
        "step": 13741
    },
    {
        "loss": 0.8883,
        "grad_norm": 3.3433704376220703,
        "learning_rate": 3.0008688788930783e-05,
        "epoch": 1.8322666666666667,
        "step": 13742
    },
    {
        "loss": 0.9765,
        "grad_norm": 3.2282207012176514,
        "learning_rate": 2.9963751466416912e-05,
        "epoch": 1.8324,
        "step": 13743
    },
    {
        "loss": 1.9488,
        "grad_norm": 3.1334495544433594,
        "learning_rate": 2.9918841885499594e-05,
        "epoch": 1.8325333333333333,
        "step": 13744
    },
    {
        "loss": 0.6052,
        "grad_norm": 3.321122169494629,
        "learning_rate": 2.98739600639678e-05,
        "epoch": 1.8326666666666667,
        "step": 13745
    },
    {
        "loss": 1.7961,
        "grad_norm": 5.439815521240234,
        "learning_rate": 2.9829106019599606e-05,
        "epoch": 1.8328,
        "step": 13746
    },
    {
        "loss": 1.6274,
        "grad_norm": 3.812635898590088,
        "learning_rate": 2.978427977016138e-05,
        "epoch": 1.8329333333333333,
        "step": 13747
    },
    {
        "loss": 1.6359,
        "grad_norm": 3.1561927795410156,
        "learning_rate": 2.9739481333409302e-05,
        "epoch": 1.8330666666666666,
        "step": 13748
    },
    {
        "loss": 2.2467,
        "grad_norm": 3.4603352546691895,
        "learning_rate": 2.9694710727088003e-05,
        "epoch": 1.8332000000000002,
        "step": 13749
    },
    {
        "loss": 2.3914,
        "grad_norm": 2.764633893966675,
        "learning_rate": 2.9649967968931412e-05,
        "epoch": 1.8333333333333335,
        "step": 13750
    },
    {
        "loss": 1.3247,
        "grad_norm": 3.911193609237671,
        "learning_rate": 2.9605253076662222e-05,
        "epoch": 1.8334666666666668,
        "step": 13751
    },
    {
        "loss": 1.946,
        "grad_norm": 3.620565891265869,
        "learning_rate": 2.9560566067992178e-05,
        "epoch": 1.8336000000000001,
        "step": 13752
    },
    {
        "loss": 1.5606,
        "grad_norm": 1.922571063041687,
        "learning_rate": 2.951590696062181e-05,
        "epoch": 1.8337333333333334,
        "step": 13753
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.564915657043457,
        "learning_rate": 2.947127577224098e-05,
        "epoch": 1.8338666666666668,
        "step": 13754
    },
    {
        "loss": 1.995,
        "grad_norm": 6.95608377456665,
        "learning_rate": 2.942667252052811e-05,
        "epoch": 1.834,
        "step": 13755
    },
    {
        "loss": 1.1006,
        "grad_norm": 3.537830114364624,
        "learning_rate": 2.938209722315064e-05,
        "epoch": 1.8341333333333334,
        "step": 13756
    },
    {
        "loss": 1.8663,
        "grad_norm": 3.9200165271759033,
        "learning_rate": 2.9337549897765205e-05,
        "epoch": 1.8342666666666667,
        "step": 13757
    },
    {
        "loss": 2.3444,
        "grad_norm": 3.3546128273010254,
        "learning_rate": 2.929303056201691e-05,
        "epoch": 1.8344,
        "step": 13758
    },
    {
        "loss": 1.9095,
        "grad_norm": 4.164665699005127,
        "learning_rate": 2.9248539233540227e-05,
        "epoch": 1.8345333333333333,
        "step": 13759
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.765986442565918,
        "learning_rate": 2.9204075929958264e-05,
        "epoch": 1.8346666666666667,
        "step": 13760
    },
    {
        "loss": 1.7941,
        "grad_norm": 4.315688133239746,
        "learning_rate": 2.9159640668883027e-05,
        "epoch": 1.8348,
        "step": 13761
    },
    {
        "loss": 1.6508,
        "grad_norm": 4.641617298126221,
        "learning_rate": 2.9115233467915416e-05,
        "epoch": 1.8349333333333333,
        "step": 13762
    },
    {
        "loss": 2.245,
        "grad_norm": 3.114290952682495,
        "learning_rate": 2.9070854344645482e-05,
        "epoch": 1.8350666666666666,
        "step": 13763
    },
    {
        "loss": 2.1211,
        "grad_norm": 2.818107843399048,
        "learning_rate": 2.9026503316651787e-05,
        "epoch": 1.8352,
        "step": 13764
    },
    {
        "loss": 2.0715,
        "grad_norm": 6.41676139831543,
        "learning_rate": 2.898218040150188e-05,
        "epoch": 1.8353333333333333,
        "step": 13765
    },
    {
        "loss": 0.9046,
        "grad_norm": 4.977932929992676,
        "learning_rate": 2.8937885616752426e-05,
        "epoch": 1.8354666666666666,
        "step": 13766
    },
    {
        "loss": 1.017,
        "grad_norm": 5.023694038391113,
        "learning_rate": 2.8893618979948588e-05,
        "epoch": 1.8356,
        "step": 13767
    },
    {
        "loss": 1.5496,
        "grad_norm": 3.805098533630371,
        "learning_rate": 2.88493805086245e-05,
        "epoch": 1.8357333333333332,
        "step": 13768
    },
    {
        "loss": 2.4203,
        "grad_norm": 2.330158233642578,
        "learning_rate": 2.880517022030329e-05,
        "epoch": 1.8358666666666665,
        "step": 13769
    },
    {
        "loss": 1.372,
        "grad_norm": 4.529922008514404,
        "learning_rate": 2.8760988132496758e-05,
        "epoch": 1.8359999999999999,
        "step": 13770
    },
    {
        "loss": 2.0288,
        "grad_norm": 5.358341217041016,
        "learning_rate": 2.8716834262705584e-05,
        "epoch": 1.8361333333333332,
        "step": 13771
    },
    {
        "loss": 1.4206,
        "grad_norm": 4.691396713256836,
        "learning_rate": 2.867270862841913e-05,
        "epoch": 1.8362666666666667,
        "step": 13772
    },
    {
        "loss": 2.8625,
        "grad_norm": 2.782285451889038,
        "learning_rate": 2.8628611247115932e-05,
        "epoch": 1.8364,
        "step": 13773
    },
    {
        "loss": 3.285,
        "grad_norm": 2.895258665084839,
        "learning_rate": 2.8584542136262938e-05,
        "epoch": 1.8365333333333334,
        "step": 13774
    },
    {
        "loss": 2.3852,
        "grad_norm": 2.8212366104125977,
        "learning_rate": 2.8540501313316303e-05,
        "epoch": 1.8366666666666667,
        "step": 13775
    },
    {
        "loss": 1.9976,
        "grad_norm": 4.152133941650391,
        "learning_rate": 2.8496488795720445e-05,
        "epoch": 1.8368,
        "step": 13776
    },
    {
        "loss": 2.5267,
        "grad_norm": 4.009613513946533,
        "learning_rate": 2.8452504600908968e-05,
        "epoch": 1.8369333333333333,
        "step": 13777
    },
    {
        "loss": 2.551,
        "grad_norm": 2.863900661468506,
        "learning_rate": 2.840854874630432e-05,
        "epoch": 1.8370666666666666,
        "step": 13778
    },
    {
        "loss": 1.9654,
        "grad_norm": 3.145188331604004,
        "learning_rate": 2.836462124931748e-05,
        "epoch": 1.8372000000000002,
        "step": 13779
    },
    {
        "loss": 2.3329,
        "grad_norm": 3.110750913619995,
        "learning_rate": 2.832072212734822e-05,
        "epoch": 1.8373333333333335,
        "step": 13780
    },
    {
        "loss": 2.4766,
        "grad_norm": 3.926567316055298,
        "learning_rate": 2.8276851397785076e-05,
        "epoch": 1.8374666666666668,
        "step": 13781
    },
    {
        "loss": 2.407,
        "grad_norm": 2.724895715713501,
        "learning_rate": 2.8233009078005612e-05,
        "epoch": 1.8376000000000001,
        "step": 13782
    },
    {
        "loss": 2.005,
        "grad_norm": 3.3340959548950195,
        "learning_rate": 2.8189195185375672e-05,
        "epoch": 1.8377333333333334,
        "step": 13783
    },
    {
        "loss": 2.3714,
        "grad_norm": 2.8712689876556396,
        "learning_rate": 2.8145409737250384e-05,
        "epoch": 1.8378666666666668,
        "step": 13784
    },
    {
        "loss": 1.7643,
        "grad_norm": 2.465043306350708,
        "learning_rate": 2.8101652750972985e-05,
        "epoch": 1.838,
        "step": 13785
    },
    {
        "loss": 1.4987,
        "grad_norm": 5.26070499420166,
        "learning_rate": 2.8057924243875887e-05,
        "epoch": 1.8381333333333334,
        "step": 13786
    },
    {
        "loss": 1.2663,
        "grad_norm": 5.0615925788879395,
        "learning_rate": 2.801422423328023e-05,
        "epoch": 1.8382666666666667,
        "step": 13787
    },
    {
        "loss": 1.111,
        "grad_norm": 3.9758284091949463,
        "learning_rate": 2.797055273649567e-05,
        "epoch": 1.8384,
        "step": 13788
    },
    {
        "loss": 2.6684,
        "grad_norm": 4.829587459564209,
        "learning_rate": 2.7926909770820575e-05,
        "epoch": 1.8385333333333334,
        "step": 13789
    },
    {
        "loss": 0.7727,
        "grad_norm": 3.752142906188965,
        "learning_rate": 2.7883295353542005e-05,
        "epoch": 1.8386666666666667,
        "step": 13790
    },
    {
        "loss": 0.7515,
        "grad_norm": 3.728483200073242,
        "learning_rate": 2.7839709501936005e-05,
        "epoch": 1.8388,
        "step": 13791
    },
    {
        "loss": 2.3728,
        "grad_norm": 2.775627374649048,
        "learning_rate": 2.7796152233266926e-05,
        "epoch": 1.8389333333333333,
        "step": 13792
    },
    {
        "loss": 1.3899,
        "grad_norm": 3.9792087078094482,
        "learning_rate": 2.7752623564787995e-05,
        "epoch": 1.8390666666666666,
        "step": 13793
    },
    {
        "loss": 2.6428,
        "grad_norm": 2.9001190662384033,
        "learning_rate": 2.770912351374093e-05,
        "epoch": 1.8392,
        "step": 13794
    },
    {
        "loss": 3.3401,
        "grad_norm": 5.07489013671875,
        "learning_rate": 2.7665652097356365e-05,
        "epoch": 1.8393333333333333,
        "step": 13795
    },
    {
        "loss": 2.4813,
        "grad_norm": 2.4101412296295166,
        "learning_rate": 2.7622209332853698e-05,
        "epoch": 1.8394666666666666,
        "step": 13796
    },
    {
        "loss": 2.3688,
        "grad_norm": 3.826186180114746,
        "learning_rate": 2.7578795237440314e-05,
        "epoch": 1.8396,
        "step": 13797
    },
    {
        "loss": 0.979,
        "grad_norm": 4.469323635101318,
        "learning_rate": 2.7535409828312996e-05,
        "epoch": 1.8397333333333332,
        "step": 13798
    },
    {
        "loss": 1.4477,
        "grad_norm": 4.264206409454346,
        "learning_rate": 2.7492053122656702e-05,
        "epoch": 1.8398666666666665,
        "step": 13799
    },
    {
        "loss": 2.68,
        "grad_norm": 2.4305670261383057,
        "learning_rate": 2.7448725137645302e-05,
        "epoch": 1.8399999999999999,
        "step": 13800
    },
    {
        "loss": 1.664,
        "grad_norm": 5.430227279663086,
        "learning_rate": 2.7405425890441104e-05,
        "epoch": 1.8401333333333332,
        "step": 13801
    },
    {
        "loss": 2.4158,
        "grad_norm": 3.7034246921539307,
        "learning_rate": 2.7362155398195044e-05,
        "epoch": 1.8402666666666667,
        "step": 13802
    },
    {
        "loss": 1.9753,
        "grad_norm": 3.6527042388916016,
        "learning_rate": 2.7318913678046655e-05,
        "epoch": 1.8404,
        "step": 13803
    },
    {
        "loss": 2.7965,
        "grad_norm": 3.6177868843078613,
        "learning_rate": 2.7275700747124212e-05,
        "epoch": 1.8405333333333334,
        "step": 13804
    },
    {
        "loss": 2.0915,
        "grad_norm": 4.487675189971924,
        "learning_rate": 2.7232516622544714e-05,
        "epoch": 1.8406666666666667,
        "step": 13805
    },
    {
        "loss": 1.442,
        "grad_norm": 4.334377765655518,
        "learning_rate": 2.718936132141311e-05,
        "epoch": 1.8408,
        "step": 13806
    },
    {
        "loss": 0.8991,
        "grad_norm": 3.934746026992798,
        "learning_rate": 2.7146234860823715e-05,
        "epoch": 1.8409333333333333,
        "step": 13807
    },
    {
        "loss": 1.7754,
        "grad_norm": 3.650028944015503,
        "learning_rate": 2.71031372578588e-05,
        "epoch": 1.8410666666666666,
        "step": 13808
    },
    {
        "loss": 1.5288,
        "grad_norm": 5.2974629402160645,
        "learning_rate": 2.706006852958971e-05,
        "epoch": 1.8412,
        "step": 13809
    },
    {
        "loss": 1.9282,
        "grad_norm": 4.249048233032227,
        "learning_rate": 2.701702869307604e-05,
        "epoch": 1.8413333333333335,
        "step": 13810
    },
    {
        "loss": 2.6094,
        "grad_norm": 3.17321515083313,
        "learning_rate": 2.6974017765365967e-05,
        "epoch": 1.8414666666666668,
        "step": 13811
    },
    {
        "loss": 1.3584,
        "grad_norm": 4.5682268142700195,
        "learning_rate": 2.6931035763496182e-05,
        "epoch": 1.8416000000000001,
        "step": 13812
    },
    {
        "loss": 3.257,
        "grad_norm": 3.6761834621429443,
        "learning_rate": 2.6888082704492235e-05,
        "epoch": 1.8417333333333334,
        "step": 13813
    },
    {
        "loss": 1.9683,
        "grad_norm": 4.078298091888428,
        "learning_rate": 2.6845158605367827e-05,
        "epoch": 1.8418666666666668,
        "step": 13814
    },
    {
        "loss": 2.5487,
        "grad_norm": 2.9126296043395996,
        "learning_rate": 2.680226348312529e-05,
        "epoch": 1.842,
        "step": 13815
    },
    {
        "loss": 2.3973,
        "grad_norm": 2.5039193630218506,
        "learning_rate": 2.6759397354755666e-05,
        "epoch": 1.8421333333333334,
        "step": 13816
    },
    {
        "loss": 2.5777,
        "grad_norm": 3.1985557079315186,
        "learning_rate": 2.671656023723823e-05,
        "epoch": 1.8422666666666667,
        "step": 13817
    },
    {
        "loss": 2.5895,
        "grad_norm": 2.2824747562408447,
        "learning_rate": 2.667375214754111e-05,
        "epoch": 1.8424,
        "step": 13818
    },
    {
        "loss": 1.9673,
        "grad_norm": 4.40800666809082,
        "learning_rate": 2.6630973102620582e-05,
        "epoch": 1.8425333333333334,
        "step": 13819
    },
    {
        "loss": 2.5794,
        "grad_norm": 3.4861855506896973,
        "learning_rate": 2.6588223119421607e-05,
        "epoch": 1.8426666666666667,
        "step": 13820
    },
    {
        "loss": 1.6889,
        "grad_norm": 3.915548086166382,
        "learning_rate": 2.6545502214877538e-05,
        "epoch": 1.8428,
        "step": 13821
    },
    {
        "loss": 2.3317,
        "grad_norm": 3.0310449600219727,
        "learning_rate": 2.6502810405910393e-05,
        "epoch": 1.8429333333333333,
        "step": 13822
    },
    {
        "loss": 2.2195,
        "grad_norm": 3.8670785427093506,
        "learning_rate": 2.6460147709430517e-05,
        "epoch": 1.8430666666666666,
        "step": 13823
    },
    {
        "loss": 1.651,
        "grad_norm": 4.279284954071045,
        "learning_rate": 2.6417514142336586e-05,
        "epoch": 1.8432,
        "step": 13824
    },
    {
        "loss": 1.9295,
        "grad_norm": 5.301949501037598,
        "learning_rate": 2.6374909721516127e-05,
        "epoch": 1.8433333333333333,
        "step": 13825
    },
    {
        "loss": 1.4569,
        "grad_norm": 4.381906509399414,
        "learning_rate": 2.6332334463844822e-05,
        "epoch": 1.8434666666666666,
        "step": 13826
    },
    {
        "loss": 1.296,
        "grad_norm": 3.8773465156555176,
        "learning_rate": 2.6289788386186743e-05,
        "epoch": 1.8436,
        "step": 13827
    },
    {
        "loss": 1.719,
        "grad_norm": 4.904997825622559,
        "learning_rate": 2.6247271505394723e-05,
        "epoch": 1.8437333333333332,
        "step": 13828
    },
    {
        "loss": 1.2155,
        "grad_norm": 5.165574550628662,
        "learning_rate": 2.62047838383098e-05,
        "epoch": 1.8438666666666665,
        "step": 13829
    },
    {
        "loss": 3.0003,
        "grad_norm": 3.5335755348205566,
        "learning_rate": 2.6162325401761422e-05,
        "epoch": 1.8439999999999999,
        "step": 13830
    },
    {
        "loss": 1.1471,
        "grad_norm": 4.771327972412109,
        "learning_rate": 2.611989621256745e-05,
        "epoch": 1.8441333333333332,
        "step": 13831
    },
    {
        "loss": 2.0684,
        "grad_norm": 4.842530250549316,
        "learning_rate": 2.6077496287534463e-05,
        "epoch": 1.8442666666666667,
        "step": 13832
    },
    {
        "loss": 2.1022,
        "grad_norm": 4.295839309692383,
        "learning_rate": 2.6035125643456946e-05,
        "epoch": 1.8444,
        "step": 13833
    },
    {
        "loss": 2.0044,
        "grad_norm": 4.017265796661377,
        "learning_rate": 2.599278429711841e-05,
        "epoch": 1.8445333333333334,
        "step": 13834
    },
    {
        "loss": 2.3568,
        "grad_norm": 4.002762317657471,
        "learning_rate": 2.5950472265289982e-05,
        "epoch": 1.8446666666666667,
        "step": 13835
    },
    {
        "loss": 2.1109,
        "grad_norm": 4.509875774383545,
        "learning_rate": 2.59081895647318e-05,
        "epoch": 1.8448,
        "step": 13836
    },
    {
        "loss": 1.3111,
        "grad_norm": 4.453064441680908,
        "learning_rate": 2.5865936212192278e-05,
        "epoch": 1.8449333333333333,
        "step": 13837
    },
    {
        "loss": 2.655,
        "grad_norm": 3.919419288635254,
        "learning_rate": 2.582371222440807e-05,
        "epoch": 1.8450666666666666,
        "step": 13838
    },
    {
        "loss": 2.3465,
        "grad_norm": 3.8821280002593994,
        "learning_rate": 2.5781517618104157e-05,
        "epoch": 1.8452,
        "step": 13839
    },
    {
        "loss": 2.592,
        "grad_norm": 3.314676284790039,
        "learning_rate": 2.5739352409993957e-05,
        "epoch": 1.8453333333333335,
        "step": 13840
    },
    {
        "loss": 2.2655,
        "grad_norm": 2.1432437896728516,
        "learning_rate": 2.5697216616779362e-05,
        "epoch": 1.8454666666666668,
        "step": 13841
    },
    {
        "loss": 2.6504,
        "grad_norm": 3.7514007091522217,
        "learning_rate": 2.565511025515036e-05,
        "epoch": 1.8456000000000001,
        "step": 13842
    },
    {
        "loss": 2.2845,
        "grad_norm": 3.9049062728881836,
        "learning_rate": 2.561303334178571e-05,
        "epoch": 1.8457333333333334,
        "step": 13843
    },
    {
        "loss": 2.6376,
        "grad_norm": 2.087916135787964,
        "learning_rate": 2.5570985893351818e-05,
        "epoch": 1.8458666666666668,
        "step": 13844
    },
    {
        "loss": 2.0867,
        "grad_norm": 3.338510036468506,
        "learning_rate": 2.552896792650402e-05,
        "epoch": 1.846,
        "step": 13845
    },
    {
        "loss": 2.3871,
        "grad_norm": 2.128821849822998,
        "learning_rate": 2.5486979457885907e-05,
        "epoch": 1.8461333333333334,
        "step": 13846
    },
    {
        "loss": 2.3505,
        "grad_norm": 3.551152467727661,
        "learning_rate": 2.544502050412909e-05,
        "epoch": 1.8462666666666667,
        "step": 13847
    },
    {
        "loss": 2.2049,
        "grad_norm": 5.632122993469238,
        "learning_rate": 2.5403091081853726e-05,
        "epoch": 1.8464,
        "step": 13848
    },
    {
        "loss": 2.3644,
        "grad_norm": 3.264986515045166,
        "learning_rate": 2.5361191207668056e-05,
        "epoch": 1.8465333333333334,
        "step": 13849
    },
    {
        "loss": 2.2928,
        "grad_norm": 3.2277140617370605,
        "learning_rate": 2.531932089816895e-05,
        "epoch": 1.8466666666666667,
        "step": 13850
    },
    {
        "loss": 1.5857,
        "grad_norm": 4.522246837615967,
        "learning_rate": 2.5277480169941337e-05,
        "epoch": 1.8468,
        "step": 13851
    },
    {
        "loss": 1.9776,
        "grad_norm": 3.1850907802581787,
        "learning_rate": 2.523566903955841e-05,
        "epoch": 1.8469333333333333,
        "step": 13852
    },
    {
        "loss": 2.4584,
        "grad_norm": 2.767514944076538,
        "learning_rate": 2.519388752358164e-05,
        "epoch": 1.8470666666666666,
        "step": 13853
    },
    {
        "loss": 1.9863,
        "grad_norm": 3.6428773403167725,
        "learning_rate": 2.5152135638560946e-05,
        "epoch": 1.8472,
        "step": 13854
    },
    {
        "loss": 2.069,
        "grad_norm": 3.5757527351379395,
        "learning_rate": 2.5110413401034517e-05,
        "epoch": 1.8473333333333333,
        "step": 13855
    },
    {
        "loss": 1.7779,
        "grad_norm": 3.313554048538208,
        "learning_rate": 2.5068720827528337e-05,
        "epoch": 1.8474666666666666,
        "step": 13856
    },
    {
        "loss": 1.8705,
        "grad_norm": 3.1035940647125244,
        "learning_rate": 2.5027057934557284e-05,
        "epoch": 1.8476,
        "step": 13857
    },
    {
        "loss": 2.1507,
        "grad_norm": 2.7318501472473145,
        "learning_rate": 2.498542473862393e-05,
        "epoch": 1.8477333333333332,
        "step": 13858
    },
    {
        "loss": 1.8531,
        "grad_norm": 2.9987473487854004,
        "learning_rate": 2.4943821256219525e-05,
        "epoch": 1.8478666666666665,
        "step": 13859
    },
    {
        "loss": 0.8465,
        "grad_norm": 3.672714948654175,
        "learning_rate": 2.4902247503823293e-05,
        "epoch": 1.8479999999999999,
        "step": 13860
    },
    {
        "loss": 1.6397,
        "grad_norm": 4.56823205947876,
        "learning_rate": 2.4860703497902692e-05,
        "epoch": 1.8481333333333332,
        "step": 13861
    },
    {
        "loss": 2.7502,
        "grad_norm": 2.536942481994629,
        "learning_rate": 2.481918925491341e-05,
        "epoch": 1.8482666666666665,
        "step": 13862
    },
    {
        "loss": 1.023,
        "grad_norm": 4.31866979598999,
        "learning_rate": 2.4777704791299405e-05,
        "epoch": 1.8484,
        "step": 13863
    },
    {
        "loss": 1.9882,
        "grad_norm": 6.082968711853027,
        "learning_rate": 2.473625012349301e-05,
        "epoch": 1.8485333333333334,
        "step": 13864
    },
    {
        "loss": 2.2642,
        "grad_norm": 3.093327760696411,
        "learning_rate": 2.469482526791427e-05,
        "epoch": 1.8486666666666667,
        "step": 13865
    },
    {
        "loss": 1.8831,
        "grad_norm": 4.781398773193359,
        "learning_rate": 2.4653430240971877e-05,
        "epoch": 1.8488,
        "step": 13866
    },
    {
        "loss": 2.725,
        "grad_norm": 2.879206418991089,
        "learning_rate": 2.4612065059062427e-05,
        "epoch": 1.8489333333333333,
        "step": 13867
    },
    {
        "loss": 2.454,
        "grad_norm": 3.9582743644714355,
        "learning_rate": 2.4570729738570996e-05,
        "epoch": 1.8490666666666666,
        "step": 13868
    },
    {
        "loss": 2.7949,
        "grad_norm": 3.003106117248535,
        "learning_rate": 2.4529424295870507e-05,
        "epoch": 1.8492,
        "step": 13869
    },
    {
        "loss": 1.2868,
        "grad_norm": 4.101159572601318,
        "learning_rate": 2.448814874732225e-05,
        "epoch": 1.8493333333333335,
        "step": 13870
    },
    {
        "loss": 2.2523,
        "grad_norm": 4.301074504852295,
        "learning_rate": 2.4446903109275456e-05,
        "epoch": 1.8494666666666668,
        "step": 13871
    },
    {
        "loss": 1.1984,
        "grad_norm": 4.24298095703125,
        "learning_rate": 2.4405687398067857e-05,
        "epoch": 1.8496000000000001,
        "step": 13872
    },
    {
        "loss": 2.6407,
        "grad_norm": 3.918623924255371,
        "learning_rate": 2.4364501630025105e-05,
        "epoch": 1.8497333333333335,
        "step": 13873
    },
    {
        "loss": 0.7077,
        "grad_norm": 3.683802604675293,
        "learning_rate": 2.432334582146085e-05,
        "epoch": 1.8498666666666668,
        "step": 13874
    },
    {
        "loss": 2.5112,
        "grad_norm": 4.169713973999023,
        "learning_rate": 2.4282219988677268e-05,
        "epoch": 1.85,
        "step": 13875
    },
    {
        "loss": 2.1863,
        "grad_norm": 6.046755313873291,
        "learning_rate": 2.4241124147964377e-05,
        "epoch": 1.8501333333333334,
        "step": 13876
    },
    {
        "loss": 2.493,
        "grad_norm": 4.21958589553833,
        "learning_rate": 2.420005831560025e-05,
        "epoch": 1.8502666666666667,
        "step": 13877
    },
    {
        "loss": 2.4378,
        "grad_norm": 2.725914239883423,
        "learning_rate": 2.415902250785138e-05,
        "epoch": 1.8504,
        "step": 13878
    },
    {
        "loss": 1.8379,
        "grad_norm": 4.464720726013184,
        "learning_rate": 2.411801674097215e-05,
        "epoch": 1.8505333333333334,
        "step": 13879
    },
    {
        "loss": 1.2841,
        "grad_norm": 4.747089385986328,
        "learning_rate": 2.407704103120494e-05,
        "epoch": 1.8506666666666667,
        "step": 13880
    },
    {
        "loss": 0.9606,
        "grad_norm": 5.628176212310791,
        "learning_rate": 2.4036095394780557e-05,
        "epoch": 1.8508,
        "step": 13881
    },
    {
        "loss": 2.1358,
        "grad_norm": 4.372377395629883,
        "learning_rate": 2.399517984791766e-05,
        "epoch": 1.8509333333333333,
        "step": 13882
    },
    {
        "loss": 2.6015,
        "grad_norm": 4.366268157958984,
        "learning_rate": 2.3954294406822886e-05,
        "epoch": 1.8510666666666666,
        "step": 13883
    },
    {
        "loss": 2.2253,
        "grad_norm": 3.6362552642822266,
        "learning_rate": 2.3913439087691312e-05,
        "epoch": 1.8512,
        "step": 13884
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.649290084838867,
        "learning_rate": 2.38726139067058e-05,
        "epoch": 1.8513333333333333,
        "step": 13885
    },
    {
        "loss": 1.939,
        "grad_norm": 4.593398571014404,
        "learning_rate": 2.3831818880037193e-05,
        "epoch": 1.8514666666666666,
        "step": 13886
    },
    {
        "loss": 1.8524,
        "grad_norm": 3.922689914703369,
        "learning_rate": 2.3791054023844795e-05,
        "epoch": 1.8516,
        "step": 13887
    },
    {
        "loss": 0.8187,
        "grad_norm": 3.0894601345062256,
        "learning_rate": 2.375031935427553e-05,
        "epoch": 1.8517333333333332,
        "step": 13888
    },
    {
        "loss": 1.9439,
        "grad_norm": 3.3804545402526855,
        "learning_rate": 2.3709614887464594e-05,
        "epoch": 1.8518666666666665,
        "step": 13889
    },
    {
        "loss": 2.4514,
        "grad_norm": 3.939938545227051,
        "learning_rate": 2.36689406395351e-05,
        "epoch": 1.8519999999999999,
        "step": 13890
    },
    {
        "loss": 1.9882,
        "grad_norm": 5.432278633117676,
        "learning_rate": 2.3628296626598357e-05,
        "epoch": 1.8521333333333332,
        "step": 13891
    },
    {
        "loss": 1.559,
        "grad_norm": 4.135655879974365,
        "learning_rate": 2.3587682864753468e-05,
        "epoch": 1.8522666666666665,
        "step": 13892
    },
    {
        "loss": 2.731,
        "grad_norm": 2.873474597930908,
        "learning_rate": 2.3547099370087967e-05,
        "epoch": 1.8524,
        "step": 13893
    },
    {
        "loss": 2.2572,
        "grad_norm": 3.0774126052856445,
        "learning_rate": 2.35065461586767e-05,
        "epoch": 1.8525333333333334,
        "step": 13894
    },
    {
        "loss": 2.4016,
        "grad_norm": 3.074153184890747,
        "learning_rate": 2.3466023246583158e-05,
        "epoch": 1.8526666666666667,
        "step": 13895
    },
    {
        "loss": 1.9803,
        "grad_norm": 2.4928746223449707,
        "learning_rate": 2.3425530649858707e-05,
        "epoch": 1.8528,
        "step": 13896
    },
    {
        "loss": 1.9429,
        "grad_norm": 3.736595630645752,
        "learning_rate": 2.338506838454245e-05,
        "epoch": 1.8529333333333333,
        "step": 13897
    },
    {
        "loss": 1.8105,
        "grad_norm": 3.529917001724243,
        "learning_rate": 2.3344636466661695e-05,
        "epoch": 1.8530666666666666,
        "step": 13898
    },
    {
        "loss": 2.6092,
        "grad_norm": 4.228548049926758,
        "learning_rate": 2.3304234912231525e-05,
        "epoch": 1.8532,
        "step": 13899
    },
    {
        "loss": 2.1235,
        "grad_norm": 3.539961814880371,
        "learning_rate": 2.326386373725532e-05,
        "epoch": 1.8533333333333335,
        "step": 13900
    },
    {
        "loss": 2.0955,
        "grad_norm": 4.941009521484375,
        "learning_rate": 2.322352295772411e-05,
        "epoch": 1.8534666666666668,
        "step": 13901
    },
    {
        "loss": 2.4322,
        "grad_norm": 1.9681570529937744,
        "learning_rate": 2.3183212589617197e-05,
        "epoch": 1.8536000000000001,
        "step": 13902
    },
    {
        "loss": 2.2473,
        "grad_norm": 5.144045352935791,
        "learning_rate": 2.3142932648901362e-05,
        "epoch": 1.8537333333333335,
        "step": 13903
    },
    {
        "loss": 2.7618,
        "grad_norm": 3.0732436180114746,
        "learning_rate": 2.310268315153181e-05,
        "epoch": 1.8538666666666668,
        "step": 13904
    },
    {
        "loss": 1.3425,
        "grad_norm": 3.6820356845855713,
        "learning_rate": 2.3062464113451555e-05,
        "epoch": 1.854,
        "step": 13905
    },
    {
        "loss": 2.2657,
        "grad_norm": 3.4018068313598633,
        "learning_rate": 2.302227555059141e-05,
        "epoch": 1.8541333333333334,
        "step": 13906
    },
    {
        "loss": 3.2158,
        "grad_norm": 4.615509033203125,
        "learning_rate": 2.2982117478870247e-05,
        "epoch": 1.8542666666666667,
        "step": 13907
    },
    {
        "loss": 2.4017,
        "grad_norm": 4.240551948547363,
        "learning_rate": 2.2941989914194662e-05,
        "epoch": 1.8544,
        "step": 13908
    },
    {
        "loss": 1.848,
        "grad_norm": 3.2866649627685547,
        "learning_rate": 2.290189287245956e-05,
        "epoch": 1.8545333333333334,
        "step": 13909
    },
    {
        "loss": 2.165,
        "grad_norm": 4.572315692901611,
        "learning_rate": 2.286182636954737e-05,
        "epoch": 1.8546666666666667,
        "step": 13910
    },
    {
        "loss": 1.413,
        "grad_norm": 4.6906819343566895,
        "learning_rate": 2.2821790421328593e-05,
        "epoch": 1.8548,
        "step": 13911
    },
    {
        "loss": 2.3475,
        "grad_norm": 3.9448459148406982,
        "learning_rate": 2.278178504366155e-05,
        "epoch": 1.8549333333333333,
        "step": 13912
    },
    {
        "loss": 1.8056,
        "grad_norm": 4.823780059814453,
        "learning_rate": 2.274181025239255e-05,
        "epoch": 1.8550666666666666,
        "step": 13913
    },
    {
        "loss": 1.5066,
        "grad_norm": 3.9842376708984375,
        "learning_rate": 2.2701866063355914e-05,
        "epoch": 1.8552,
        "step": 13914
    },
    {
        "loss": 1.9447,
        "grad_norm": 4.850441932678223,
        "learning_rate": 2.2661952492373384e-05,
        "epoch": 1.8553333333333333,
        "step": 13915
    },
    {
        "loss": 2.8004,
        "grad_norm": 2.4424314498901367,
        "learning_rate": 2.2622069555255054e-05,
        "epoch": 1.8554666666666666,
        "step": 13916
    },
    {
        "loss": 2.6499,
        "grad_norm": 3.663483142852783,
        "learning_rate": 2.2582217267798545e-05,
        "epoch": 1.8556,
        "step": 13917
    },
    {
        "loss": 2.6801,
        "grad_norm": 3.51235032081604,
        "learning_rate": 2.2542395645789682e-05,
        "epoch": 1.8557333333333332,
        "step": 13918
    },
    {
        "loss": 1.3038,
        "grad_norm": 2.271165370941162,
        "learning_rate": 2.2502604705001807e-05,
        "epoch": 1.8558666666666666,
        "step": 13919
    },
    {
        "loss": 2.1785,
        "grad_norm": 3.7402589321136475,
        "learning_rate": 2.24628444611963e-05,
        "epoch": 1.8559999999999999,
        "step": 13920
    },
    {
        "loss": 1.1392,
        "grad_norm": 5.3273773193359375,
        "learning_rate": 2.242311493012219e-05,
        "epoch": 1.8561333333333332,
        "step": 13921
    },
    {
        "loss": 1.893,
        "grad_norm": 3.7050468921661377,
        "learning_rate": 2.238341612751659e-05,
        "epoch": 1.8562666666666665,
        "step": 13922
    },
    {
        "loss": 1.1369,
        "grad_norm": 3.3943932056427,
        "learning_rate": 2.234374806910454e-05,
        "epoch": 1.8564,
        "step": 13923
    },
    {
        "loss": 1.8722,
        "grad_norm": 3.487783908843994,
        "learning_rate": 2.2304110770598286e-05,
        "epoch": 1.8565333333333334,
        "step": 13924
    },
    {
        "loss": 1.5739,
        "grad_norm": 2.4285736083984375,
        "learning_rate": 2.226450424769858e-05,
        "epoch": 1.8566666666666667,
        "step": 13925
    },
    {
        "loss": 0.7482,
        "grad_norm": 4.048281669616699,
        "learning_rate": 2.2224928516093568e-05,
        "epoch": 1.8568,
        "step": 13926
    },
    {
        "loss": 1.3865,
        "grad_norm": 3.4365358352661133,
        "learning_rate": 2.218538359145943e-05,
        "epoch": 1.8569333333333333,
        "step": 13927
    },
    {
        "loss": 1.8495,
        "grad_norm": 3.8549065589904785,
        "learning_rate": 2.2145869489460004e-05,
        "epoch": 1.8570666666666666,
        "step": 13928
    },
    {
        "loss": 2.0072,
        "grad_norm": 3.779012441635132,
        "learning_rate": 2.210638622574699e-05,
        "epoch": 1.8572,
        "step": 13929
    },
    {
        "loss": 2.4292,
        "grad_norm": 2.2619638442993164,
        "learning_rate": 2.206693381595968e-05,
        "epoch": 1.8573333333333333,
        "step": 13930
    },
    {
        "loss": 1.6634,
        "grad_norm": 3.6810197830200195,
        "learning_rate": 2.202751227572556e-05,
        "epoch": 1.8574666666666668,
        "step": 13931
    },
    {
        "loss": 2.8546,
        "grad_norm": 3.531458854675293,
        "learning_rate": 2.1988121620659473e-05,
        "epoch": 1.8576000000000001,
        "step": 13932
    },
    {
        "loss": 1.8286,
        "grad_norm": 3.724733829498291,
        "learning_rate": 2.1948761866364166e-05,
        "epoch": 1.8577333333333335,
        "step": 13933
    },
    {
        "loss": 2.6287,
        "grad_norm": 3.103268623352051,
        "learning_rate": 2.1909433028430337e-05,
        "epoch": 1.8578666666666668,
        "step": 13934
    },
    {
        "loss": 2.3529,
        "grad_norm": 3.8727526664733887,
        "learning_rate": 2.187013512243614e-05,
        "epoch": 1.858,
        "step": 13935
    },
    {
        "loss": 3.0542,
        "grad_norm": 2.1894888877868652,
        "learning_rate": 2.183086816394756e-05,
        "epoch": 1.8581333333333334,
        "step": 13936
    },
    {
        "loss": 2.2668,
        "grad_norm": 3.693289041519165,
        "learning_rate": 2.179163216851857e-05,
        "epoch": 1.8582666666666667,
        "step": 13937
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.806406021118164,
        "learning_rate": 2.1752427151690546e-05,
        "epoch": 1.8584,
        "step": 13938
    },
    {
        "loss": 2.0153,
        "grad_norm": 2.0922162532806396,
        "learning_rate": 2.171325312899265e-05,
        "epoch": 1.8585333333333334,
        "step": 13939
    },
    {
        "loss": 1.4823,
        "grad_norm": 3.6545560359954834,
        "learning_rate": 2.167411011594207e-05,
        "epoch": 1.8586666666666667,
        "step": 13940
    },
    {
        "loss": 2.1366,
        "grad_norm": 3.9407413005828857,
        "learning_rate": 2.1634998128043328e-05,
        "epoch": 1.8588,
        "step": 13941
    },
    {
        "loss": 1.9323,
        "grad_norm": 4.4144158363342285,
        "learning_rate": 2.1595917180788772e-05,
        "epoch": 1.8589333333333333,
        "step": 13942
    },
    {
        "loss": 2.4711,
        "grad_norm": 2.7929015159606934,
        "learning_rate": 2.1556867289658678e-05,
        "epoch": 1.8590666666666666,
        "step": 13943
    },
    {
        "loss": 2.4774,
        "grad_norm": 3.5531888008117676,
        "learning_rate": 2.1517848470120715e-05,
        "epoch": 1.8592,
        "step": 13944
    },
    {
        "loss": 2.2508,
        "grad_norm": 4.25586462020874,
        "learning_rate": 2.1478860737630313e-05,
        "epoch": 1.8593333333333333,
        "step": 13945
    },
    {
        "loss": 1.1881,
        "grad_norm": 4.345691680908203,
        "learning_rate": 2.1439904107630836e-05,
        "epoch": 1.8594666666666666,
        "step": 13946
    },
    {
        "loss": 1.5114,
        "grad_norm": 3.235697031021118,
        "learning_rate": 2.1400978595553e-05,
        "epoch": 1.8596,
        "step": 13947
    },
    {
        "loss": 1.5287,
        "grad_norm": 3.971677541732788,
        "learning_rate": 2.1362084216815392e-05,
        "epoch": 1.8597333333333332,
        "step": 13948
    },
    {
        "loss": 2.1337,
        "grad_norm": 4.110576152801514,
        "learning_rate": 2.1323220986824067e-05,
        "epoch": 1.8598666666666666,
        "step": 13949
    },
    {
        "loss": 2.7459,
        "grad_norm": 2.950507879257202,
        "learning_rate": 2.1284388920973075e-05,
        "epoch": 1.8599999999999999,
        "step": 13950
    },
    {
        "loss": 1.6658,
        "grad_norm": 4.103028774261475,
        "learning_rate": 2.1245588034643792e-05,
        "epoch": 1.8601333333333332,
        "step": 13951
    },
    {
        "loss": 2.1861,
        "grad_norm": 4.529752731323242,
        "learning_rate": 2.120681834320558e-05,
        "epoch": 1.8602666666666665,
        "step": 13952
    },
    {
        "loss": 0.6687,
        "grad_norm": 13.388284683227539,
        "learning_rate": 2.1168079862014935e-05,
        "epoch": 1.8604,
        "step": 13953
    },
    {
        "loss": 1.9948,
        "grad_norm": 2.953744888305664,
        "learning_rate": 2.11293726064165e-05,
        "epoch": 1.8605333333333334,
        "step": 13954
    },
    {
        "loss": 2.2435,
        "grad_norm": 2.9430899620056152,
        "learning_rate": 2.1090696591742377e-05,
        "epoch": 1.8606666666666667,
        "step": 13955
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.7184946537017822,
        "learning_rate": 2.1052051833312237e-05,
        "epoch": 1.8608,
        "step": 13956
    },
    {
        "loss": 1.5784,
        "grad_norm": 3.3809807300567627,
        "learning_rate": 2.1013438346433347e-05,
        "epoch": 1.8609333333333333,
        "step": 13957
    },
    {
        "loss": 2.1522,
        "grad_norm": 3.5224673748016357,
        "learning_rate": 2.097485614640058e-05,
        "epoch": 1.8610666666666666,
        "step": 13958
    },
    {
        "loss": 2.3665,
        "grad_norm": 3.7375733852386475,
        "learning_rate": 2.093630524849668e-05,
        "epoch": 1.8612,
        "step": 13959
    },
    {
        "loss": 2.4556,
        "grad_norm": 3.3045148849487305,
        "learning_rate": 2.0897785667991555e-05,
        "epoch": 1.8613333333333333,
        "step": 13960
    },
    {
        "loss": 1.3974,
        "grad_norm": 3.590723752975464,
        "learning_rate": 2.085929742014323e-05,
        "epoch": 1.8614666666666668,
        "step": 13961
    },
    {
        "loss": 2.4858,
        "grad_norm": 3.379237651824951,
        "learning_rate": 2.0820840520196738e-05,
        "epoch": 1.8616000000000001,
        "step": 13962
    },
    {
        "loss": 1.4782,
        "grad_norm": 4.164764881134033,
        "learning_rate": 2.078241498338508e-05,
        "epoch": 1.8617333333333335,
        "step": 13963
    },
    {
        "loss": 2.1499,
        "grad_norm": 3.5406346321105957,
        "learning_rate": 2.0744020824928845e-05,
        "epoch": 1.8618666666666668,
        "step": 13964
    },
    {
        "loss": 1.4458,
        "grad_norm": 4.73471212387085,
        "learning_rate": 2.0705658060036058e-05,
        "epoch": 1.862,
        "step": 13965
    },
    {
        "loss": 2.2009,
        "grad_norm": 3.954927921295166,
        "learning_rate": 2.0667326703902267e-05,
        "epoch": 1.8621333333333334,
        "step": 13966
    },
    {
        "loss": 1.7891,
        "grad_norm": 3.9690918922424316,
        "learning_rate": 2.0629026771710613e-05,
        "epoch": 1.8622666666666667,
        "step": 13967
    },
    {
        "loss": 1.2327,
        "grad_norm": 4.095757961273193,
        "learning_rate": 2.0590758278632004e-05,
        "epoch": 1.8624,
        "step": 13968
    },
    {
        "loss": 2.028,
        "grad_norm": 3.2663354873657227,
        "learning_rate": 2.0552521239824607e-05,
        "epoch": 1.8625333333333334,
        "step": 13969
    },
    {
        "loss": 2.2754,
        "grad_norm": 3.589513063430786,
        "learning_rate": 2.051431567043427e-05,
        "epoch": 1.8626666666666667,
        "step": 13970
    },
    {
        "loss": 2.0185,
        "grad_norm": 4.577341079711914,
        "learning_rate": 2.0476141585594243e-05,
        "epoch": 1.8628,
        "step": 13971
    },
    {
        "loss": 2.4889,
        "grad_norm": 3.254356622695923,
        "learning_rate": 2.04379990004255e-05,
        "epoch": 1.8629333333333333,
        "step": 13972
    },
    {
        "loss": 0.9019,
        "grad_norm": 4.034478664398193,
        "learning_rate": 2.0399887930036642e-05,
        "epoch": 1.8630666666666666,
        "step": 13973
    },
    {
        "loss": 2.0423,
        "grad_norm": 3.1490633487701416,
        "learning_rate": 2.036180838952322e-05,
        "epoch": 1.8632,
        "step": 13974
    },
    {
        "loss": 1.6774,
        "grad_norm": 4.5236711502075195,
        "learning_rate": 2.0323760393968926e-05,
        "epoch": 1.8633333333333333,
        "step": 13975
    },
    {
        "loss": 1.7022,
        "grad_norm": 4.608521461486816,
        "learning_rate": 2.0285743958444582e-05,
        "epoch": 1.8634666666666666,
        "step": 13976
    },
    {
        "loss": 1.8319,
        "grad_norm": 2.746324062347412,
        "learning_rate": 2.0247759098008733e-05,
        "epoch": 1.8636,
        "step": 13977
    },
    {
        "loss": 1.9039,
        "grad_norm": 3.8341500759124756,
        "learning_rate": 2.0209805827707272e-05,
        "epoch": 1.8637333333333332,
        "step": 13978
    },
    {
        "loss": 2.5619,
        "grad_norm": 3.0081241130828857,
        "learning_rate": 2.0171884162573583e-05,
        "epoch": 1.8638666666666666,
        "step": 13979
    },
    {
        "loss": 0.9506,
        "grad_norm": 3.4065935611724854,
        "learning_rate": 2.0133994117628506e-05,
        "epoch": 1.8639999999999999,
        "step": 13980
    },
    {
        "loss": 1.1972,
        "grad_norm": 2.426496982574463,
        "learning_rate": 2.0096135707880492e-05,
        "epoch": 1.8641333333333332,
        "step": 13981
    },
    {
        "loss": 2.035,
        "grad_norm": 3.405519723892212,
        "learning_rate": 2.0058308948325578e-05,
        "epoch": 1.8642666666666665,
        "step": 13982
    },
    {
        "loss": 1.9823,
        "grad_norm": 3.3106842041015625,
        "learning_rate": 2.002051385394669e-05,
        "epoch": 1.8643999999999998,
        "step": 13983
    },
    {
        "loss": 1.0259,
        "grad_norm": 4.054046154022217,
        "learning_rate": 1.9982750439714902e-05,
        "epoch": 1.8645333333333334,
        "step": 13984
    },
    {
        "loss": 1.6815,
        "grad_norm": 3.889345645904541,
        "learning_rate": 1.9945018720588203e-05,
        "epoch": 1.8646666666666667,
        "step": 13985
    },
    {
        "loss": 2.6385,
        "grad_norm": 2.824115514755249,
        "learning_rate": 1.9907318711512457e-05,
        "epoch": 1.8648,
        "step": 13986
    },
    {
        "loss": 1.3735,
        "grad_norm": 4.175142765045166,
        "learning_rate": 1.9869650427420706e-05,
        "epoch": 1.8649333333333333,
        "step": 13987
    },
    {
        "loss": 2.3289,
        "grad_norm": 3.3204824924468994,
        "learning_rate": 1.9832013883233436e-05,
        "epoch": 1.8650666666666667,
        "step": 13988
    },
    {
        "loss": 3.0504,
        "grad_norm": 2.5786685943603516,
        "learning_rate": 1.9794409093858545e-05,
        "epoch": 1.8652,
        "step": 13989
    },
    {
        "loss": 1.8249,
        "grad_norm": 5.451969623565674,
        "learning_rate": 1.9756836074191608e-05,
        "epoch": 1.8653333333333333,
        "step": 13990
    },
    {
        "loss": 1.5935,
        "grad_norm": 4.161383152008057,
        "learning_rate": 1.97192948391153e-05,
        "epoch": 1.8654666666666668,
        "step": 13991
    },
    {
        "loss": 1.9342,
        "grad_norm": 3.240299701690674,
        "learning_rate": 1.9681785403499785e-05,
        "epoch": 1.8656000000000001,
        "step": 13992
    },
    {
        "loss": 2.3709,
        "grad_norm": 3.3677406311035156,
        "learning_rate": 1.964430778220284e-05,
        "epoch": 1.8657333333333335,
        "step": 13993
    },
    {
        "loss": 0.8866,
        "grad_norm": 4.419965744018555,
        "learning_rate": 1.9606861990069382e-05,
        "epoch": 1.8658666666666668,
        "step": 13994
    },
    {
        "loss": 2.5238,
        "grad_norm": 3.668339967727661,
        "learning_rate": 1.9569448041931747e-05,
        "epoch": 1.866,
        "step": 13995
    },
    {
        "loss": 2.6917,
        "grad_norm": 2.9951844215393066,
        "learning_rate": 1.9532065952609868e-05,
        "epoch": 1.8661333333333334,
        "step": 13996
    },
    {
        "loss": 1.7851,
        "grad_norm": 3.485149383544922,
        "learning_rate": 1.949471573691085e-05,
        "epoch": 1.8662666666666667,
        "step": 13997
    },
    {
        "loss": 2.1723,
        "grad_norm": 3.576625347137451,
        "learning_rate": 1.9457397409629273e-05,
        "epoch": 1.8664,
        "step": 13998
    },
    {
        "loss": 2.0139,
        "grad_norm": 6.910533428192139,
        "learning_rate": 1.9420110985546925e-05,
        "epoch": 1.8665333333333334,
        "step": 13999
    },
    {
        "loss": 1.8093,
        "grad_norm": 3.1094868183135986,
        "learning_rate": 1.9382856479433252e-05,
        "epoch": 1.8666666666666667,
        "step": 14000
    },
    {
        "loss": 2.3504,
        "grad_norm": 3.3871960639953613,
        "learning_rate": 1.934563390604478e-05,
        "epoch": 1.8668,
        "step": 14001
    },
    {
        "loss": 2.0241,
        "grad_norm": 3.7203173637390137,
        "learning_rate": 1.9308443280125587e-05,
        "epoch": 1.8669333333333333,
        "step": 14002
    },
    {
        "loss": 2.2329,
        "grad_norm": 3.7281858921051025,
        "learning_rate": 1.9271284616406993e-05,
        "epoch": 1.8670666666666667,
        "step": 14003
    },
    {
        "loss": 2.3954,
        "grad_norm": 3.651730537414551,
        "learning_rate": 1.923415792960752e-05,
        "epoch": 1.8672,
        "step": 14004
    },
    {
        "loss": 2.6975,
        "grad_norm": 4.152626037597656,
        "learning_rate": 1.919706323443339e-05,
        "epoch": 1.8673333333333333,
        "step": 14005
    },
    {
        "loss": 2.7103,
        "grad_norm": 4.083942413330078,
        "learning_rate": 1.916000054557786e-05,
        "epoch": 1.8674666666666666,
        "step": 14006
    },
    {
        "loss": 1.421,
        "grad_norm": 3.610947847366333,
        "learning_rate": 1.9122969877721542e-05,
        "epoch": 1.8676,
        "step": 14007
    },
    {
        "loss": 2.5247,
        "grad_norm": 3.086003541946411,
        "learning_rate": 1.908597124553235e-05,
        "epoch": 1.8677333333333332,
        "step": 14008
    },
    {
        "loss": 2.8191,
        "grad_norm": 1.8381617069244385,
        "learning_rate": 1.9049004663665782e-05,
        "epoch": 1.8678666666666666,
        "step": 14009
    },
    {
        "loss": 2.4436,
        "grad_norm": 3.947795867919922,
        "learning_rate": 1.9012070146764184e-05,
        "epoch": 1.8679999999999999,
        "step": 14010
    },
    {
        "loss": 2.1268,
        "grad_norm": 3.449970245361328,
        "learning_rate": 1.897516770945774e-05,
        "epoch": 1.8681333333333332,
        "step": 14011
    },
    {
        "loss": 1.9366,
        "grad_norm": 3.6008830070495605,
        "learning_rate": 1.8938297366363267e-05,
        "epoch": 1.8682666666666665,
        "step": 14012
    },
    {
        "loss": 2.0111,
        "grad_norm": 3.701688528060913,
        "learning_rate": 1.890145913208543e-05,
        "epoch": 1.8683999999999998,
        "step": 14013
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.379533529281616,
        "learning_rate": 1.8864653021216038e-05,
        "epoch": 1.8685333333333334,
        "step": 14014
    },
    {
        "loss": 1.4045,
        "grad_norm": 4.45151424407959,
        "learning_rate": 1.8827879048334074e-05,
        "epoch": 1.8686666666666667,
        "step": 14015
    },
    {
        "loss": 2.3878,
        "grad_norm": 3.2434937953948975,
        "learning_rate": 1.8791137228005784e-05,
        "epoch": 1.8688,
        "step": 14016
    },
    {
        "loss": 1.4797,
        "grad_norm": 3.63916015625,
        "learning_rate": 1.8754427574784695e-05,
        "epoch": 1.8689333333333333,
        "step": 14017
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.793867826461792,
        "learning_rate": 1.871775010321173e-05,
        "epoch": 1.8690666666666667,
        "step": 14018
    },
    {
        "loss": 2.0193,
        "grad_norm": 3.5531394481658936,
        "learning_rate": 1.8681104827814834e-05,
        "epoch": 1.8692,
        "step": 14019
    },
    {
        "loss": 2.6578,
        "grad_norm": 2.944063663482666,
        "learning_rate": 1.8644491763109583e-05,
        "epoch": 1.8693333333333333,
        "step": 14020
    },
    {
        "loss": 2.0962,
        "grad_norm": 3.700068712234497,
        "learning_rate": 1.8607910923598182e-05,
        "epoch": 1.8694666666666668,
        "step": 14021
    },
    {
        "loss": 2.5487,
        "grad_norm": 2.4508020877838135,
        "learning_rate": 1.857136232377059e-05,
        "epoch": 1.8696000000000002,
        "step": 14022
    },
    {
        "loss": 1.9505,
        "grad_norm": 3.738001823425293,
        "learning_rate": 1.8534845978103954e-05,
        "epoch": 1.8697333333333335,
        "step": 14023
    },
    {
        "loss": 2.0928,
        "grad_norm": 3.3471450805664062,
        "learning_rate": 1.849836190106239e-05,
        "epoch": 1.8698666666666668,
        "step": 14024
    },
    {
        "loss": 2.2929,
        "grad_norm": 2.7315728664398193,
        "learning_rate": 1.8461910107097414e-05,
        "epoch": 1.87,
        "step": 14025
    },
    {
        "loss": 2.3605,
        "grad_norm": 2.4355759620666504,
        "learning_rate": 1.8425490610647567e-05,
        "epoch": 1.8701333333333334,
        "step": 14026
    },
    {
        "loss": 2.2064,
        "grad_norm": 2.886427402496338,
        "learning_rate": 1.8389103426138933e-05,
        "epoch": 1.8702666666666667,
        "step": 14027
    },
    {
        "loss": 2.3574,
        "grad_norm": 3.9971389770507812,
        "learning_rate": 1.8352748567984522e-05,
        "epoch": 1.8704,
        "step": 14028
    },
    {
        "loss": 1.9407,
        "grad_norm": 3.862671136856079,
        "learning_rate": 1.8316426050584602e-05,
        "epoch": 1.8705333333333334,
        "step": 14029
    },
    {
        "loss": 2.3395,
        "grad_norm": 3.8329265117645264,
        "learning_rate": 1.8280135888326566e-05,
        "epoch": 1.8706666666666667,
        "step": 14030
    },
    {
        "loss": 2.0202,
        "grad_norm": 4.571950435638428,
        "learning_rate": 1.8243878095585198e-05,
        "epoch": 1.8708,
        "step": 14031
    },
    {
        "loss": 2.2483,
        "grad_norm": 4.213074207305908,
        "learning_rate": 1.8207652686722408e-05,
        "epoch": 1.8709333333333333,
        "step": 14032
    },
    {
        "loss": 3.1733,
        "grad_norm": 4.324668884277344,
        "learning_rate": 1.8171459676086945e-05,
        "epoch": 1.8710666666666667,
        "step": 14033
    },
    {
        "loss": 1.7511,
        "grad_norm": 4.661987781524658,
        "learning_rate": 1.813529907801522e-05,
        "epoch": 1.8712,
        "step": 14034
    },
    {
        "loss": 1.9145,
        "grad_norm": 4.454190254211426,
        "learning_rate": 1.8099170906830386e-05,
        "epoch": 1.8713333333333333,
        "step": 14035
    },
    {
        "loss": 2.116,
        "grad_norm": 3.531156539916992,
        "learning_rate": 1.8063075176843093e-05,
        "epoch": 1.8714666666666666,
        "step": 14036
    },
    {
        "loss": 1.8333,
        "grad_norm": 6.331772804260254,
        "learning_rate": 1.8027011902350943e-05,
        "epoch": 1.8716,
        "step": 14037
    },
    {
        "loss": 2.0949,
        "grad_norm": 2.8807127475738525,
        "learning_rate": 1.799098109763867e-05,
        "epoch": 1.8717333333333332,
        "step": 14038
    },
    {
        "loss": 1.9864,
        "grad_norm": 3.529996871948242,
        "learning_rate": 1.7954982776978156e-05,
        "epoch": 1.8718666666666666,
        "step": 14039
    },
    {
        "loss": 2.549,
        "grad_norm": 4.246068477630615,
        "learning_rate": 1.7919016954628544e-05,
        "epoch": 1.8719999999999999,
        "step": 14040
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.769078016281128,
        "learning_rate": 1.788308364483613e-05,
        "epoch": 1.8721333333333332,
        "step": 14041
    },
    {
        "loss": 2.1683,
        "grad_norm": 3.8183417320251465,
        "learning_rate": 1.7847182861833955e-05,
        "epoch": 1.8722666666666665,
        "step": 14042
    },
    {
        "loss": 1.1932,
        "grad_norm": 4.077828407287598,
        "learning_rate": 1.7811314619842632e-05,
        "epoch": 1.8723999999999998,
        "step": 14043
    },
    {
        "loss": 2.0511,
        "grad_norm": 7.007224082946777,
        "learning_rate": 1.7775478933069566e-05,
        "epoch": 1.8725333333333334,
        "step": 14044
    },
    {
        "loss": 2.6205,
        "grad_norm": 2.466972827911377,
        "learning_rate": 1.7739675815709568e-05,
        "epoch": 1.8726666666666667,
        "step": 14045
    },
    {
        "loss": 2.622,
        "grad_norm": 4.050962924957275,
        "learning_rate": 1.7703905281944256e-05,
        "epoch": 1.8728,
        "step": 14046
    },
    {
        "loss": 2.2689,
        "grad_norm": 3.1677324771881104,
        "learning_rate": 1.7668167345942467e-05,
        "epoch": 1.8729333333333333,
        "step": 14047
    },
    {
        "loss": 2.152,
        "grad_norm": 4.318985462188721,
        "learning_rate": 1.7632462021860084e-05,
        "epoch": 1.8730666666666667,
        "step": 14048
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.3118083477020264,
        "learning_rate": 1.7596789323840212e-05,
        "epoch": 1.8732,
        "step": 14049
    },
    {
        "loss": 2.275,
        "grad_norm": 3.6629891395568848,
        "learning_rate": 1.7561149266012887e-05,
        "epoch": 1.8733333333333333,
        "step": 14050
    },
    {
        "loss": 1.4108,
        "grad_norm": 5.907159805297852,
        "learning_rate": 1.7525541862495143e-05,
        "epoch": 1.8734666666666666,
        "step": 14051
    },
    {
        "loss": 1.5521,
        "grad_norm": 3.567291259765625,
        "learning_rate": 1.748996712739137e-05,
        "epoch": 1.8736000000000002,
        "step": 14052
    },
    {
        "loss": 1.589,
        "grad_norm": 3.902284860610962,
        "learning_rate": 1.745442507479279e-05,
        "epoch": 1.8737333333333335,
        "step": 14053
    },
    {
        "loss": 2.088,
        "grad_norm": 5.10132360458374,
        "learning_rate": 1.7418915718777605e-05,
        "epoch": 1.8738666666666668,
        "step": 14054
    },
    {
        "loss": 2.0584,
        "grad_norm": 3.515951156616211,
        "learning_rate": 1.7383439073411357e-05,
        "epoch": 1.874,
        "step": 14055
    },
    {
        "loss": 2.0237,
        "grad_norm": 2.8534250259399414,
        "learning_rate": 1.734799515274643e-05,
        "epoch": 1.8741333333333334,
        "step": 14056
    },
    {
        "loss": 2.4316,
        "grad_norm": 2.303825855255127,
        "learning_rate": 1.731258397082224e-05,
        "epoch": 1.8742666666666667,
        "step": 14057
    },
    {
        "loss": 2.7172,
        "grad_norm": 2.6340343952178955,
        "learning_rate": 1.7277205541665208e-05,
        "epoch": 1.8744,
        "step": 14058
    },
    {
        "loss": 3.83,
        "grad_norm": 3.440061569213867,
        "learning_rate": 1.7241859879289025e-05,
        "epoch": 1.8745333333333334,
        "step": 14059
    },
    {
        "loss": 0.8295,
        "grad_norm": 4.2043843269348145,
        "learning_rate": 1.7206546997694038e-05,
        "epoch": 1.8746666666666667,
        "step": 14060
    },
    {
        "loss": 1.9503,
        "grad_norm": 3.2259323596954346,
        "learning_rate": 1.717126691086798e-05,
        "epoch": 1.8748,
        "step": 14061
    },
    {
        "loss": 1.2478,
        "grad_norm": 4.12448263168335,
        "learning_rate": 1.713601963278535e-05,
        "epoch": 1.8749333333333333,
        "step": 14062
    },
    {
        "loss": 1.7835,
        "grad_norm": 3.830733299255371,
        "learning_rate": 1.7100805177407596e-05,
        "epoch": 1.8750666666666667,
        "step": 14063
    },
    {
        "loss": 2.4896,
        "grad_norm": 3.4516947269439697,
        "learning_rate": 1.7065623558683485e-05,
        "epoch": 1.8752,
        "step": 14064
    },
    {
        "loss": 2.3922,
        "grad_norm": 3.0512824058532715,
        "learning_rate": 1.7030474790548445e-05,
        "epoch": 1.8753333333333333,
        "step": 14065
    },
    {
        "loss": 2.207,
        "grad_norm": 5.417037010192871,
        "learning_rate": 1.699535888692505e-05,
        "epoch": 1.8754666666666666,
        "step": 14066
    },
    {
        "loss": 1.4201,
        "grad_norm": 2.4922380447387695,
        "learning_rate": 1.6960275861722785e-05,
        "epoch": 1.8756,
        "step": 14067
    },
    {
        "loss": 2.6096,
        "grad_norm": 2.7758660316467285,
        "learning_rate": 1.6925225728838258e-05,
        "epoch": 1.8757333333333333,
        "step": 14068
    },
    {
        "loss": 1.5971,
        "grad_norm": 2.644752264022827,
        "learning_rate": 1.6890208502154814e-05,
        "epoch": 1.8758666666666666,
        "step": 14069
    },
    {
        "loss": 3.1288,
        "grad_norm": 3.890002727508545,
        "learning_rate": 1.685522419554314e-05,
        "epoch": 1.876,
        "step": 14070
    },
    {
        "loss": 2.6939,
        "grad_norm": 3.6885805130004883,
        "learning_rate": 1.6820272822860296e-05,
        "epoch": 1.8761333333333332,
        "step": 14071
    },
    {
        "loss": 2.2234,
        "grad_norm": 3.413604259490967,
        "learning_rate": 1.6785354397950815e-05,
        "epoch": 1.8762666666666665,
        "step": 14072
    },
    {
        "loss": 2.2555,
        "grad_norm": 3.457862138748169,
        "learning_rate": 1.6750468934646113e-05,
        "epoch": 1.8763999999999998,
        "step": 14073
    },
    {
        "loss": 1.2019,
        "grad_norm": 3.5711581707000732,
        "learning_rate": 1.6715616446764292e-05,
        "epoch": 1.8765333333333334,
        "step": 14074
    },
    {
        "loss": 1.2878,
        "grad_norm": 2.944037437438965,
        "learning_rate": 1.6680796948110622e-05,
        "epoch": 1.8766666666666667,
        "step": 14075
    },
    {
        "loss": 1.9872,
        "grad_norm": 3.3204026222229004,
        "learning_rate": 1.664601045247708e-05,
        "epoch": 1.8768,
        "step": 14076
    },
    {
        "loss": 2.1617,
        "grad_norm": 3.6328225135803223,
        "learning_rate": 1.6611256973642886e-05,
        "epoch": 1.8769333333333333,
        "step": 14077
    },
    {
        "loss": 2.1958,
        "grad_norm": 2.688271999359131,
        "learning_rate": 1.6576536525373976e-05,
        "epoch": 1.8770666666666667,
        "step": 14078
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.5902106761932373,
        "learning_rate": 1.654184912142318e-05,
        "epoch": 1.8772,
        "step": 14079
    },
    {
        "loss": 2.8619,
        "grad_norm": 4.544755458831787,
        "learning_rate": 1.6507194775530255e-05,
        "epoch": 1.8773333333333333,
        "step": 14080
    },
    {
        "loss": 1.9465,
        "grad_norm": 2.9032247066497803,
        "learning_rate": 1.6472573501421995e-05,
        "epoch": 1.8774666666666666,
        "step": 14081
    },
    {
        "loss": 1.9377,
        "grad_norm": 3.6221210956573486,
        "learning_rate": 1.6437985312812032e-05,
        "epoch": 1.8776000000000002,
        "step": 14082
    },
    {
        "loss": 0.9274,
        "grad_norm": 3.237793445587158,
        "learning_rate": 1.6403430223400818e-05,
        "epoch": 1.8777333333333335,
        "step": 14083
    },
    {
        "loss": 2.6227,
        "grad_norm": 4.140291690826416,
        "learning_rate": 1.636890824687576e-05,
        "epoch": 1.8778666666666668,
        "step": 14084
    },
    {
        "loss": 2.0634,
        "grad_norm": 2.718592882156372,
        "learning_rate": 1.6334419396911016e-05,
        "epoch": 1.8780000000000001,
        "step": 14085
    },
    {
        "loss": 2.5508,
        "grad_norm": 4.9302778244018555,
        "learning_rate": 1.629996368716793e-05,
        "epoch": 1.8781333333333334,
        "step": 14086
    },
    {
        "loss": 2.1799,
        "grad_norm": 3.242703437805176,
        "learning_rate": 1.6265541131294404e-05,
        "epoch": 1.8782666666666668,
        "step": 14087
    },
    {
        "loss": 2.2919,
        "grad_norm": 2.8959641456604004,
        "learning_rate": 1.6231151742925365e-05,
        "epoch": 1.8784,
        "step": 14088
    },
    {
        "loss": 2.3709,
        "grad_norm": 2.8542561531066895,
        "learning_rate": 1.6196795535682498e-05,
        "epoch": 1.8785333333333334,
        "step": 14089
    },
    {
        "loss": 0.8434,
        "grad_norm": 3.9086432456970215,
        "learning_rate": 1.6162472523174443e-05,
        "epoch": 1.8786666666666667,
        "step": 14090
    },
    {
        "loss": 0.9245,
        "grad_norm": 3.4927375316619873,
        "learning_rate": 1.6128182718996843e-05,
        "epoch": 1.8788,
        "step": 14091
    },
    {
        "loss": 1.8741,
        "grad_norm": 3.5129644870758057,
        "learning_rate": 1.609392613673173e-05,
        "epoch": 1.8789333333333333,
        "step": 14092
    },
    {
        "loss": 2.227,
        "grad_norm": 4.462115287780762,
        "learning_rate": 1.605970278994843e-05,
        "epoch": 1.8790666666666667,
        "step": 14093
    },
    {
        "loss": 1.547,
        "grad_norm": 4.991530895233154,
        "learning_rate": 1.6025512692202806e-05,
        "epoch": 1.8792,
        "step": 14094
    },
    {
        "loss": 2.2499,
        "grad_norm": 3.1793453693389893,
        "learning_rate": 1.5991355857037827e-05,
        "epoch": 1.8793333333333333,
        "step": 14095
    },
    {
        "loss": 2.5567,
        "grad_norm": 3.3256852626800537,
        "learning_rate": 1.5957232297983028e-05,
        "epoch": 1.8794666666666666,
        "step": 14096
    },
    {
        "loss": 2.2304,
        "grad_norm": 3.5342423915863037,
        "learning_rate": 1.5923142028554895e-05,
        "epoch": 1.8796,
        "step": 14097
    },
    {
        "loss": 2.2289,
        "grad_norm": 3.853503704071045,
        "learning_rate": 1.5889085062256602e-05,
        "epoch": 1.8797333333333333,
        "step": 14098
    },
    {
        "loss": 1.7486,
        "grad_norm": 3.792038679122925,
        "learning_rate": 1.58550614125784e-05,
        "epoch": 1.8798666666666666,
        "step": 14099
    },
    {
        "loss": 2.0959,
        "grad_norm": 4.500261306762695,
        "learning_rate": 1.5821071092997096e-05,
        "epoch": 1.88,
        "step": 14100
    },
    {
        "loss": 2.2505,
        "grad_norm": 4.1080546379089355,
        "learning_rate": 1.5787114116976275e-05,
        "epoch": 1.8801333333333332,
        "step": 14101
    },
    {
        "loss": 1.921,
        "grad_norm": 4.04398250579834,
        "learning_rate": 1.5753190497966573e-05,
        "epoch": 1.8802666666666665,
        "step": 14102
    },
    {
        "loss": 1.7252,
        "grad_norm": 3.8859875202178955,
        "learning_rate": 1.5719300249405135e-05,
        "epoch": 1.8803999999999998,
        "step": 14103
    },
    {
        "loss": 1.5745,
        "grad_norm": 4.970889091491699,
        "learning_rate": 1.568544338471609e-05,
        "epoch": 1.8805333333333332,
        "step": 14104
    },
    {
        "loss": 1.9249,
        "grad_norm": 5.572854042053223,
        "learning_rate": 1.5651619917310224e-05,
        "epoch": 1.8806666666666667,
        "step": 14105
    },
    {
        "loss": 1.8875,
        "grad_norm": 3.967165231704712,
        "learning_rate": 1.5617829860585133e-05,
        "epoch": 1.8808,
        "step": 14106
    },
    {
        "loss": 1.069,
        "grad_norm": 6.57444953918457,
        "learning_rate": 1.558407322792507e-05,
        "epoch": 1.8809333333333333,
        "step": 14107
    },
    {
        "loss": 0.5335,
        "grad_norm": 3.0441434383392334,
        "learning_rate": 1.5550350032701312e-05,
        "epoch": 1.8810666666666667,
        "step": 14108
    },
    {
        "loss": 1.986,
        "grad_norm": 2.9091641902923584,
        "learning_rate": 1.551666028827168e-05,
        "epoch": 1.8812,
        "step": 14109
    },
    {
        "loss": 1.94,
        "grad_norm": 3.3870198726654053,
        "learning_rate": 1.5483004007980685e-05,
        "epoch": 1.8813333333333333,
        "step": 14110
    },
    {
        "loss": 1.5907,
        "grad_norm": 4.700798034667969,
        "learning_rate": 1.5449381205159884e-05,
        "epoch": 1.8814666666666666,
        "step": 14111
    },
    {
        "loss": 1.2348,
        "grad_norm": 3.940621852874756,
        "learning_rate": 1.541579189312726e-05,
        "epoch": 1.8816000000000002,
        "step": 14112
    },
    {
        "loss": 2.144,
        "grad_norm": 4.873880863189697,
        "learning_rate": 1.5382236085187616e-05,
        "epoch": 1.8817333333333335,
        "step": 14113
    },
    {
        "loss": 0.4756,
        "grad_norm": 3.0737192630767822,
        "learning_rate": 1.5348713794632673e-05,
        "epoch": 1.8818666666666668,
        "step": 14114
    },
    {
        "loss": 1.7211,
        "grad_norm": 5.148557186126709,
        "learning_rate": 1.531522503474062e-05,
        "epoch": 1.8820000000000001,
        "step": 14115
    },
    {
        "loss": 2.3453,
        "grad_norm": 3.3826656341552734,
        "learning_rate": 1.5281769818776482e-05,
        "epoch": 1.8821333333333334,
        "step": 14116
    },
    {
        "loss": 1.867,
        "grad_norm": 5.983887195587158,
        "learning_rate": 1.5248348159991943e-05,
        "epoch": 1.8822666666666668,
        "step": 14117
    },
    {
        "loss": 0.9845,
        "grad_norm": 4.998930931091309,
        "learning_rate": 1.521496007162554e-05,
        "epoch": 1.8824,
        "step": 14118
    },
    {
        "loss": 0.8121,
        "grad_norm": 3.540879726409912,
        "learning_rate": 1.5181605566902268e-05,
        "epoch": 1.8825333333333334,
        "step": 14119
    },
    {
        "loss": 2.694,
        "grad_norm": 2.780543327331543,
        "learning_rate": 1.514828465903414e-05,
        "epoch": 1.8826666666666667,
        "step": 14120
    },
    {
        "loss": 2.35,
        "grad_norm": 5.493919372558594,
        "learning_rate": 1.5114997361219562e-05,
        "epoch": 1.8828,
        "step": 14121
    },
    {
        "loss": 2.6826,
        "grad_norm": 2.952592372894287,
        "learning_rate": 1.508174368664369e-05,
        "epoch": 1.8829333333333333,
        "step": 14122
    },
    {
        "loss": 2.4362,
        "grad_norm": 3.5636231899261475,
        "learning_rate": 1.5048523648478597e-05,
        "epoch": 1.8830666666666667,
        "step": 14123
    },
    {
        "loss": 2.413,
        "grad_norm": 5.479444980621338,
        "learning_rate": 1.5015337259882733e-05,
        "epoch": 1.8832,
        "step": 14124
    },
    {
        "loss": 2.1369,
        "grad_norm": 3.626002550125122,
        "learning_rate": 1.4982184534001376e-05,
        "epoch": 1.8833333333333333,
        "step": 14125
    },
    {
        "loss": 2.1789,
        "grad_norm": 3.8496804237365723,
        "learning_rate": 1.4949065483966318e-05,
        "epoch": 1.8834666666666666,
        "step": 14126
    },
    {
        "loss": 1.7872,
        "grad_norm": 4.131923675537109,
        "learning_rate": 1.4915980122896301e-05,
        "epoch": 1.8836,
        "step": 14127
    },
    {
        "loss": 2.8177,
        "grad_norm": 3.7317116260528564,
        "learning_rate": 1.4882928463896428e-05,
        "epoch": 1.8837333333333333,
        "step": 14128
    },
    {
        "loss": 2.0867,
        "grad_norm": 3.971257448196411,
        "learning_rate": 1.484991052005873e-05,
        "epoch": 1.8838666666666666,
        "step": 14129
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.962671160697937,
        "learning_rate": 1.4816926304461476e-05,
        "epoch": 1.884,
        "step": 14130
    },
    {
        "loss": 1.8292,
        "grad_norm": 3.469697952270508,
        "learning_rate": 1.4783975830170005e-05,
        "epoch": 1.8841333333333332,
        "step": 14131
    },
    {
        "loss": 1.2928,
        "grad_norm": 4.333135604858398,
        "learning_rate": 1.475105911023611e-05,
        "epoch": 1.8842666666666665,
        "step": 14132
    },
    {
        "loss": 1.0835,
        "grad_norm": 3.8841969966888428,
        "learning_rate": 1.4718176157698204e-05,
        "epoch": 1.8843999999999999,
        "step": 14133
    },
    {
        "loss": 2.2844,
        "grad_norm": 4.0560808181762695,
        "learning_rate": 1.4685326985581294e-05,
        "epoch": 1.8845333333333332,
        "step": 14134
    },
    {
        "loss": 1.2091,
        "grad_norm": 3.637572765350342,
        "learning_rate": 1.465251160689699e-05,
        "epoch": 1.8846666666666667,
        "step": 14135
    },
    {
        "loss": 1.6127,
        "grad_norm": 3.3899600505828857,
        "learning_rate": 1.4619730034643753e-05,
        "epoch": 1.8848,
        "step": 14136
    },
    {
        "loss": 2.8506,
        "grad_norm": 3.056673526763916,
        "learning_rate": 1.4586982281806349e-05,
        "epoch": 1.8849333333333333,
        "step": 14137
    },
    {
        "loss": 1.5744,
        "grad_norm": 4.088911533355713,
        "learning_rate": 1.4554268361356293e-05,
        "epoch": 1.8850666666666667,
        "step": 14138
    },
    {
        "loss": 1.7044,
        "grad_norm": 2.9827871322631836,
        "learning_rate": 1.4521588286251653e-05,
        "epoch": 1.8852,
        "step": 14139
    },
    {
        "loss": 1.7599,
        "grad_norm": 4.996583461761475,
        "learning_rate": 1.4488942069437128e-05,
        "epoch": 1.8853333333333333,
        "step": 14140
    },
    {
        "loss": 2.9224,
        "grad_norm": 2.741544723510742,
        "learning_rate": 1.4456329723844087e-05,
        "epoch": 1.8854666666666666,
        "step": 14141
    },
    {
        "loss": 2.0447,
        "grad_norm": 4.109333038330078,
        "learning_rate": 1.442375126239035e-05,
        "epoch": 1.8856000000000002,
        "step": 14142
    },
    {
        "loss": 2.1269,
        "grad_norm": 2.1956701278686523,
        "learning_rate": 1.4391206697980309e-05,
        "epoch": 1.8857333333333335,
        "step": 14143
    },
    {
        "loss": 1.525,
        "grad_norm": 4.197793483734131,
        "learning_rate": 1.4358696043504938e-05,
        "epoch": 1.8858666666666668,
        "step": 14144
    },
    {
        "loss": 0.4962,
        "grad_norm": 2.8370161056518555,
        "learning_rate": 1.432621931184196e-05,
        "epoch": 1.8860000000000001,
        "step": 14145
    },
    {
        "loss": 1.1096,
        "grad_norm": 3.4539341926574707,
        "learning_rate": 1.4293776515855428e-05,
        "epoch": 1.8861333333333334,
        "step": 14146
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.8883893489837646,
        "learning_rate": 1.4261367668396064e-05,
        "epoch": 1.8862666666666668,
        "step": 14147
    },
    {
        "loss": 2.3083,
        "grad_norm": 4.305543899536133,
        "learning_rate": 1.4228992782301031e-05,
        "epoch": 1.8864,
        "step": 14148
    },
    {
        "loss": 2.0596,
        "grad_norm": 3.357473611831665,
        "learning_rate": 1.4196651870394218e-05,
        "epoch": 1.8865333333333334,
        "step": 14149
    },
    {
        "loss": 2.3213,
        "grad_norm": 2.6775403022766113,
        "learning_rate": 1.4164344945486119e-05,
        "epoch": 1.8866666666666667,
        "step": 14150
    },
    {
        "loss": 2.4492,
        "grad_norm": 3.402637004852295,
        "learning_rate": 1.413207202037331e-05,
        "epoch": 1.8868,
        "step": 14151
    },
    {
        "loss": 2.6893,
        "grad_norm": 3.8250272274017334,
        "learning_rate": 1.4099833107839422e-05,
        "epoch": 1.8869333333333334,
        "step": 14152
    },
    {
        "loss": 1.6159,
        "grad_norm": 4.83799409866333,
        "learning_rate": 1.4067628220654294e-05,
        "epoch": 1.8870666666666667,
        "step": 14153
    },
    {
        "loss": 1.5447,
        "grad_norm": 3.8561460971832275,
        "learning_rate": 1.403545737157449e-05,
        "epoch": 1.8872,
        "step": 14154
    },
    {
        "loss": 2.2241,
        "grad_norm": 2.7271947860717773,
        "learning_rate": 1.4003320573342926e-05,
        "epoch": 1.8873333333333333,
        "step": 14155
    },
    {
        "loss": 2.8441,
        "grad_norm": 1.895389199256897,
        "learning_rate": 1.3971217838689143e-05,
        "epoch": 1.8874666666666666,
        "step": 14156
    },
    {
        "loss": 2.202,
        "grad_norm": 4.205527305603027,
        "learning_rate": 1.3939149180329014e-05,
        "epoch": 1.8876,
        "step": 14157
    },
    {
        "loss": 2.0425,
        "grad_norm": 4.713328838348389,
        "learning_rate": 1.3907114610965233e-05,
        "epoch": 1.8877333333333333,
        "step": 14158
    },
    {
        "loss": 2.1122,
        "grad_norm": 3.9943578243255615,
        "learning_rate": 1.3875114143286682e-05,
        "epoch": 1.8878666666666666,
        "step": 14159
    },
    {
        "loss": 3.065,
        "grad_norm": 3.7636873722076416,
        "learning_rate": 1.3843147789968814e-05,
        "epoch": 1.888,
        "step": 14160
    },
    {
        "loss": 2.7719,
        "grad_norm": 2.491814136505127,
        "learning_rate": 1.3811215563673774e-05,
        "epoch": 1.8881333333333332,
        "step": 14161
    },
    {
        "loss": 2.1101,
        "grad_norm": 2.481473207473755,
        "learning_rate": 1.3779317477049835e-05,
        "epoch": 1.8882666666666665,
        "step": 14162
    },
    {
        "loss": 1.3361,
        "grad_norm": 3.213041067123413,
        "learning_rate": 1.3747453542732114e-05,
        "epoch": 1.8883999999999999,
        "step": 14163
    },
    {
        "loss": 1.5806,
        "grad_norm": 4.080686092376709,
        "learning_rate": 1.3715623773341946e-05,
        "epoch": 1.8885333333333332,
        "step": 14164
    },
    {
        "loss": 1.4003,
        "grad_norm": 6.204873561859131,
        "learning_rate": 1.3683828181487201e-05,
        "epoch": 1.8886666666666667,
        "step": 14165
    },
    {
        "loss": 3.2678,
        "grad_norm": 4.923323631286621,
        "learning_rate": 1.3652066779762151e-05,
        "epoch": 1.8888,
        "step": 14166
    },
    {
        "loss": 0.877,
        "grad_norm": 3.2989463806152344,
        "learning_rate": 1.3620339580747765e-05,
        "epoch": 1.8889333333333334,
        "step": 14167
    },
    {
        "loss": 2.4285,
        "grad_norm": 4.5475921630859375,
        "learning_rate": 1.358864659701118e-05,
        "epoch": 1.8890666666666667,
        "step": 14168
    },
    {
        "loss": 3.0098,
        "grad_norm": 3.3305931091308594,
        "learning_rate": 1.3556987841106061e-05,
        "epoch": 1.8892,
        "step": 14169
    },
    {
        "loss": 2.3306,
        "grad_norm": 3.510115146636963,
        "learning_rate": 1.3525363325572681e-05,
        "epoch": 1.8893333333333333,
        "step": 14170
    },
    {
        "loss": 1.6734,
        "grad_norm": 3.0730855464935303,
        "learning_rate": 1.349377306293752e-05,
        "epoch": 1.8894666666666666,
        "step": 14171
    },
    {
        "loss": 2.1835,
        "grad_norm": 3.669268846511841,
        "learning_rate": 1.3462217065713578e-05,
        "epoch": 1.8896,
        "step": 14172
    },
    {
        "loss": 1.3844,
        "grad_norm": 6.55505895614624,
        "learning_rate": 1.3430695346400368e-05,
        "epoch": 1.8897333333333335,
        "step": 14173
    },
    {
        "loss": 2.2383,
        "grad_norm": 4.8307785987854,
        "learning_rate": 1.3399207917483703e-05,
        "epoch": 1.8898666666666668,
        "step": 14174
    },
    {
        "loss": 1.1673,
        "grad_norm": 4.520597457885742,
        "learning_rate": 1.3367754791435893e-05,
        "epoch": 1.8900000000000001,
        "step": 14175
    },
    {
        "loss": 2.5682,
        "grad_norm": 2.6973159313201904,
        "learning_rate": 1.333633598071551e-05,
        "epoch": 1.8901333333333334,
        "step": 14176
    },
    {
        "loss": 2.2531,
        "grad_norm": 3.408182144165039,
        "learning_rate": 1.3304951497767815e-05,
        "epoch": 1.8902666666666668,
        "step": 14177
    },
    {
        "loss": 1.7827,
        "grad_norm": 4.61909294128418,
        "learning_rate": 1.327360135502419e-05,
        "epoch": 1.8904,
        "step": 14178
    },
    {
        "loss": 2.192,
        "grad_norm": 4.056055068969727,
        "learning_rate": 1.32422855649027e-05,
        "epoch": 1.8905333333333334,
        "step": 14179
    },
    {
        "loss": 2.453,
        "grad_norm": 4.18925666809082,
        "learning_rate": 1.3211004139807393e-05,
        "epoch": 1.8906666666666667,
        "step": 14180
    },
    {
        "loss": 2.3715,
        "grad_norm": 5.383749008178711,
        "learning_rate": 1.3179757092129064e-05,
        "epoch": 1.8908,
        "step": 14181
    },
    {
        "loss": 2.1228,
        "grad_norm": 3.2294721603393555,
        "learning_rate": 1.3148544434244847e-05,
        "epoch": 1.8909333333333334,
        "step": 14182
    },
    {
        "loss": 2.4439,
        "grad_norm": 3.1974756717681885,
        "learning_rate": 1.311736617851813e-05,
        "epoch": 1.8910666666666667,
        "step": 14183
    },
    {
        "loss": 1.6686,
        "grad_norm": 3.642869234085083,
        "learning_rate": 1.3086222337298703e-05,
        "epoch": 1.8912,
        "step": 14184
    },
    {
        "loss": 2.0357,
        "grad_norm": 2.0443925857543945,
        "learning_rate": 1.3055112922922696e-05,
        "epoch": 1.8913333333333333,
        "step": 14185
    },
    {
        "loss": 2.19,
        "grad_norm": 2.7662651538848877,
        "learning_rate": 1.3024037947712797e-05,
        "epoch": 1.8914666666666666,
        "step": 14186
    },
    {
        "loss": 2.7859,
        "grad_norm": 4.427947521209717,
        "learning_rate": 1.2992997423977748e-05,
        "epoch": 1.8916,
        "step": 14187
    },
    {
        "loss": 1.2681,
        "grad_norm": 4.331784725189209,
        "learning_rate": 1.2961991364013015e-05,
        "epoch": 1.8917333333333333,
        "step": 14188
    },
    {
        "loss": 2.2954,
        "grad_norm": 3.728750705718994,
        "learning_rate": 1.2931019780099972e-05,
        "epoch": 1.8918666666666666,
        "step": 14189
    },
    {
        "loss": 2.3652,
        "grad_norm": 3.699575662612915,
        "learning_rate": 1.2900082684506653e-05,
        "epoch": 1.892,
        "step": 14190
    },
    {
        "loss": 2.0651,
        "grad_norm": 2.9496216773986816,
        "learning_rate": 1.286918008948743e-05,
        "epoch": 1.8921333333333332,
        "step": 14191
    },
    {
        "loss": 2.5259,
        "grad_norm": 3.398468017578125,
        "learning_rate": 1.2838312007282892e-05,
        "epoch": 1.8922666666666665,
        "step": 14192
    },
    {
        "loss": 2.3952,
        "grad_norm": 3.418050527572632,
        "learning_rate": 1.2807478450119936e-05,
        "epoch": 1.8923999999999999,
        "step": 14193
    },
    {
        "loss": 2.1416,
        "grad_norm": 3.968395471572876,
        "learning_rate": 1.2776679430211813e-05,
        "epoch": 1.8925333333333332,
        "step": 14194
    },
    {
        "loss": 1.5457,
        "grad_norm": 3.5419363975524902,
        "learning_rate": 1.2745914959758243e-05,
        "epoch": 1.8926666666666667,
        "step": 14195
    },
    {
        "loss": 2.0081,
        "grad_norm": 4.044344902038574,
        "learning_rate": 1.2715185050945066e-05,
        "epoch": 1.8928,
        "step": 14196
    },
    {
        "loss": 2.1621,
        "grad_norm": 2.801562786102295,
        "learning_rate": 1.2684489715944514e-05,
        "epoch": 1.8929333333333334,
        "step": 14197
    },
    {
        "loss": 2.2944,
        "grad_norm": 3.4965438842773438,
        "learning_rate": 1.2653828966915027e-05,
        "epoch": 1.8930666666666667,
        "step": 14198
    },
    {
        "loss": 2.2321,
        "grad_norm": 2.8869411945343018,
        "learning_rate": 1.26232028160015e-05,
        "epoch": 1.8932,
        "step": 14199
    },
    {
        "loss": 2.4849,
        "grad_norm": 2.87786865234375,
        "learning_rate": 1.2592611275335165e-05,
        "epoch": 1.8933333333333333,
        "step": 14200
    },
    {
        "loss": 2.0192,
        "grad_norm": 4.147859573364258,
        "learning_rate": 1.2562054357033315e-05,
        "epoch": 1.8934666666666666,
        "step": 14201
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.8622419834136963,
        "learning_rate": 1.253153207319967e-05,
        "epoch": 1.8936,
        "step": 14202
    },
    {
        "loss": 2.437,
        "grad_norm": 3.651005983352661,
        "learning_rate": 1.250104443592416e-05,
        "epoch": 1.8937333333333335,
        "step": 14203
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.61348819732666,
        "learning_rate": 1.2470591457283143e-05,
        "epoch": 1.8938666666666668,
        "step": 14204
    },
    {
        "loss": 3.1093,
        "grad_norm": 3.028838634490967,
        "learning_rate": 1.244017314933913e-05,
        "epoch": 1.8940000000000001,
        "step": 14205
    },
    {
        "loss": 1.2517,
        "grad_norm": 4.270942211151123,
        "learning_rate": 1.2409789524140846e-05,
        "epoch": 1.8941333333333334,
        "step": 14206
    },
    {
        "loss": 2.3357,
        "grad_norm": 4.444881916046143,
        "learning_rate": 1.2379440593723335e-05,
        "epoch": 1.8942666666666668,
        "step": 14207
    },
    {
        "loss": 1.8852,
        "grad_norm": 2.824197769165039,
        "learning_rate": 1.2349126370107966e-05,
        "epoch": 1.8944,
        "step": 14208
    },
    {
        "loss": 1.4727,
        "grad_norm": 4.595622539520264,
        "learning_rate": 1.2318846865302457e-05,
        "epoch": 1.8945333333333334,
        "step": 14209
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.786099433898926,
        "learning_rate": 1.2288602091300316e-05,
        "epoch": 1.8946666666666667,
        "step": 14210
    },
    {
        "loss": 1.7601,
        "grad_norm": 6.652294635772705,
        "learning_rate": 1.2258392060081835e-05,
        "epoch": 1.8948,
        "step": 14211
    },
    {
        "loss": 1.4985,
        "grad_norm": 5.834820747375488,
        "learning_rate": 1.2228216783613156e-05,
        "epoch": 1.8949333333333334,
        "step": 14212
    },
    {
        "loss": 1.9915,
        "grad_norm": 3.3103368282318115,
        "learning_rate": 1.2198076273846936e-05,
        "epoch": 1.8950666666666667,
        "step": 14213
    },
    {
        "loss": 2.0711,
        "grad_norm": 3.508481025695801,
        "learning_rate": 1.2167970542721918e-05,
        "epoch": 1.8952,
        "step": 14214
    },
    {
        "loss": 1.3737,
        "grad_norm": 3.6501221656799316,
        "learning_rate": 1.2137899602163017e-05,
        "epoch": 1.8953333333333333,
        "step": 14215
    },
    {
        "loss": 2.7797,
        "grad_norm": 3.613325357437134,
        "learning_rate": 1.2107863464081403e-05,
        "epoch": 1.8954666666666666,
        "step": 14216
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.8645029067993164,
        "learning_rate": 1.2077862140374651e-05,
        "epoch": 1.8956,
        "step": 14217
    },
    {
        "loss": 2.2308,
        "grad_norm": 5.534887313842773,
        "learning_rate": 1.2047895642926254e-05,
        "epoch": 1.8957333333333333,
        "step": 14218
    },
    {
        "loss": 1.7164,
        "grad_norm": 3.3379087448120117,
        "learning_rate": 1.201796398360605e-05,
        "epoch": 1.8958666666666666,
        "step": 14219
    },
    {
        "loss": 1.5585,
        "grad_norm": 3.2238757610321045,
        "learning_rate": 1.1988067174270145e-05,
        "epoch": 1.896,
        "step": 14220
    },
    {
        "loss": 2.2092,
        "grad_norm": 2.2998721599578857,
        "learning_rate": 1.1958205226760666e-05,
        "epoch": 1.8961333333333332,
        "step": 14221
    },
    {
        "loss": 2.7252,
        "grad_norm": 3.180907964706421,
        "learning_rate": 1.1928378152906171e-05,
        "epoch": 1.8962666666666665,
        "step": 14222
    },
    {
        "loss": 1.6661,
        "grad_norm": 4.034546375274658,
        "learning_rate": 1.189858596452117e-05,
        "epoch": 1.8963999999999999,
        "step": 14223
    },
    {
        "loss": 2.7731,
        "grad_norm": 2.464374303817749,
        "learning_rate": 1.1868828673406462e-05,
        "epoch": 1.8965333333333332,
        "step": 14224
    },
    {
        "loss": 2.2265,
        "grad_norm": 3.910149574279785,
        "learning_rate": 1.1839106291348977e-05,
        "epoch": 1.8966666666666665,
        "step": 14225
    },
    {
        "loss": 1.2184,
        "grad_norm": 3.468906879425049,
        "learning_rate": 1.1809418830121933e-05,
        "epoch": 1.8968,
        "step": 14226
    },
    {
        "loss": 2.3375,
        "grad_norm": 3.3354766368865967,
        "learning_rate": 1.1779766301484619e-05,
        "epoch": 1.8969333333333334,
        "step": 14227
    },
    {
        "loss": 1.7946,
        "grad_norm": 5.818221569061279,
        "learning_rate": 1.1750148717182374e-05,
        "epoch": 1.8970666666666667,
        "step": 14228
    },
    {
        "loss": 1.7332,
        "grad_norm": 3.4262094497680664,
        "learning_rate": 1.1720566088947006e-05,
        "epoch": 1.8972,
        "step": 14229
    },
    {
        "loss": 0.5294,
        "grad_norm": 2.4433135986328125,
        "learning_rate": 1.1691018428496226e-05,
        "epoch": 1.8973333333333333,
        "step": 14230
    },
    {
        "loss": 2.2088,
        "grad_norm": 4.76542329788208,
        "learning_rate": 1.1661505747533875e-05,
        "epoch": 1.8974666666666666,
        "step": 14231
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.9052295684814453,
        "learning_rate": 1.1632028057750166e-05,
        "epoch": 1.8976,
        "step": 14232
    },
    {
        "loss": 2.5835,
        "grad_norm": 2.977321147918701,
        "learning_rate": 1.1602585370821262e-05,
        "epoch": 1.8977333333333335,
        "step": 14233
    },
    {
        "loss": 0.9166,
        "grad_norm": 3.1545047760009766,
        "learning_rate": 1.1573177698409487e-05,
        "epoch": 1.8978666666666668,
        "step": 14234
    },
    {
        "loss": 2.7477,
        "grad_norm": 3.646440267562866,
        "learning_rate": 1.154380505216327e-05,
        "epoch": 1.8980000000000001,
        "step": 14235
    },
    {
        "loss": 2.5364,
        "grad_norm": 3.9702341556549072,
        "learning_rate": 1.1514467443717358e-05,
        "epoch": 1.8981333333333335,
        "step": 14236
    },
    {
        "loss": 2.9562,
        "grad_norm": 5.64838171005249,
        "learning_rate": 1.1485164884692323e-05,
        "epoch": 1.8982666666666668,
        "step": 14237
    },
    {
        "loss": 2.6797,
        "grad_norm": 3.4959564208984375,
        "learning_rate": 1.1455897386695225e-05,
        "epoch": 1.8984,
        "step": 14238
    },
    {
        "loss": 1.7502,
        "grad_norm": 4.799505710601807,
        "learning_rate": 1.1426664961318734e-05,
        "epoch": 1.8985333333333334,
        "step": 14239
    },
    {
        "loss": 1.5334,
        "grad_norm": 3.5640597343444824,
        "learning_rate": 1.1397467620142055e-05,
        "epoch": 1.8986666666666667,
        "step": 14240
    },
    {
        "loss": 2.2411,
        "grad_norm": 3.0185999870300293,
        "learning_rate": 1.1368305374730437e-05,
        "epoch": 1.8988,
        "step": 14241
    },
    {
        "loss": 2.1377,
        "grad_norm": 4.288323402404785,
        "learning_rate": 1.133917823663505e-05,
        "epoch": 1.8989333333333334,
        "step": 14242
    },
    {
        "loss": 2.0893,
        "grad_norm": 3.9931530952453613,
        "learning_rate": 1.1310086217393268e-05,
        "epoch": 1.8990666666666667,
        "step": 14243
    },
    {
        "loss": 1.0464,
        "grad_norm": 3.8743410110473633,
        "learning_rate": 1.1281029328528492e-05,
        "epoch": 1.8992,
        "step": 14244
    },
    {
        "loss": 1.4443,
        "grad_norm": 4.380411148071289,
        "learning_rate": 1.125200758155035e-05,
        "epoch": 1.8993333333333333,
        "step": 14245
    },
    {
        "loss": 1.8537,
        "grad_norm": 3.1514675617218018,
        "learning_rate": 1.1223020987954346e-05,
        "epoch": 1.8994666666666666,
        "step": 14246
    },
    {
        "loss": 2.6959,
        "grad_norm": 2.8013341426849365,
        "learning_rate": 1.1194069559222365e-05,
        "epoch": 1.8996,
        "step": 14247
    },
    {
        "loss": 2.0912,
        "grad_norm": 4.691524982452393,
        "learning_rate": 1.1165153306821896e-05,
        "epoch": 1.8997333333333333,
        "step": 14248
    },
    {
        "loss": 1.7991,
        "grad_norm": 4.060257911682129,
        "learning_rate": 1.1136272242206892e-05,
        "epoch": 1.8998666666666666,
        "step": 14249
    },
    {
        "loss": 2.2307,
        "grad_norm": 3.6742265224456787,
        "learning_rate": 1.1107426376817332e-05,
        "epoch": 1.9,
        "step": 14250
    },
    {
        "loss": 1.6618,
        "grad_norm": 3.337563991546631,
        "learning_rate": 1.1078615722079055e-05,
        "epoch": 1.9001333333333332,
        "step": 14251
    },
    {
        "loss": 2.4729,
        "grad_norm": 3.5190653800964355,
        "learning_rate": 1.10498402894041e-05,
        "epoch": 1.9002666666666665,
        "step": 14252
    },
    {
        "loss": 2.181,
        "grad_norm": 2.933933973312378,
        "learning_rate": 1.1021100090190429e-05,
        "epoch": 1.9003999999999999,
        "step": 14253
    },
    {
        "loss": 2.5818,
        "grad_norm": 2.9077608585357666,
        "learning_rate": 1.0992395135822253e-05,
        "epoch": 1.9005333333333332,
        "step": 14254
    },
    {
        "loss": 2.2306,
        "grad_norm": 4.193023204803467,
        "learning_rate": 1.0963725437669669e-05,
        "epoch": 1.9006666666666665,
        "step": 14255
    },
    {
        "loss": 1.3474,
        "grad_norm": 4.576927185058594,
        "learning_rate": 1.0935091007088805e-05,
        "epoch": 1.9008,
        "step": 14256
    },
    {
        "loss": 2.1611,
        "grad_norm": 3.469010829925537,
        "learning_rate": 1.0906491855421807e-05,
        "epoch": 1.9009333333333334,
        "step": 14257
    },
    {
        "loss": 2.203,
        "grad_norm": 3.8240787982940674,
        "learning_rate": 1.0877927993996994e-05,
        "epoch": 1.9010666666666667,
        "step": 14258
    },
    {
        "loss": 1.8903,
        "grad_norm": 3.946606159210205,
        "learning_rate": 1.084939943412867e-05,
        "epoch": 1.9012,
        "step": 14259
    },
    {
        "loss": 2.7235,
        "grad_norm": 4.588605880737305,
        "learning_rate": 1.0820906187116898e-05,
        "epoch": 1.9013333333333333,
        "step": 14260
    },
    {
        "loss": 1.7178,
        "grad_norm": 3.356151819229126,
        "learning_rate": 1.0792448264248123e-05,
        "epoch": 1.9014666666666666,
        "step": 14261
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.3687710762023926,
        "learning_rate": 1.0764025676794476e-05,
        "epoch": 1.9016,
        "step": 14262
    },
    {
        "loss": 2.7707,
        "grad_norm": 2.8286116123199463,
        "learning_rate": 1.0735638436014384e-05,
        "epoch": 1.9017333333333335,
        "step": 14263
    },
    {
        "loss": 1.4423,
        "grad_norm": 3.6800951957702637,
        "learning_rate": 1.0707286553152096e-05,
        "epoch": 1.9018666666666668,
        "step": 14264
    },
    {
        "loss": 2.41,
        "grad_norm": 2.8006591796875,
        "learning_rate": 1.067897003943784e-05,
        "epoch": 1.9020000000000001,
        "step": 14265
    },
    {
        "loss": 2.9696,
        "grad_norm": 3.0423638820648193,
        "learning_rate": 1.0650688906087857e-05,
        "epoch": 1.9021333333333335,
        "step": 14266
    },
    {
        "loss": 1.3437,
        "grad_norm": 4.319216728210449,
        "learning_rate": 1.062244316430444e-05,
        "epoch": 1.9022666666666668,
        "step": 14267
    },
    {
        "loss": 2.3242,
        "grad_norm": 3.6421265602111816,
        "learning_rate": 1.059423282527594e-05,
        "epoch": 1.9024,
        "step": 14268
    },
    {
        "loss": 1.7141,
        "grad_norm": 4.253018379211426,
        "learning_rate": 1.0566057900176373e-05,
        "epoch": 1.9025333333333334,
        "step": 14269
    },
    {
        "loss": 1.899,
        "grad_norm": 2.9143383502960205,
        "learning_rate": 1.0537918400166036e-05,
        "epoch": 1.9026666666666667,
        "step": 14270
    },
    {
        "loss": 2.2633,
        "grad_norm": 3.281830072402954,
        "learning_rate": 1.050981433639101e-05,
        "epoch": 1.9028,
        "step": 14271
    },
    {
        "loss": 1.0112,
        "grad_norm": 5.036672592163086,
        "learning_rate": 1.0481745719983528e-05,
        "epoch": 1.9029333333333334,
        "step": 14272
    },
    {
        "loss": 2.2145,
        "grad_norm": 2.2033634185791016,
        "learning_rate": 1.0453712562061569e-05,
        "epoch": 1.9030666666666667,
        "step": 14273
    },
    {
        "loss": 2.1913,
        "grad_norm": 3.210613489151001,
        "learning_rate": 1.0425714873729209e-05,
        "epoch": 1.9032,
        "step": 14274
    },
    {
        "loss": 2.1708,
        "grad_norm": 4.632465839385986,
        "learning_rate": 1.0397752666076333e-05,
        "epoch": 1.9033333333333333,
        "step": 14275
    },
    {
        "loss": 0.4166,
        "grad_norm": 2.498307466506958,
        "learning_rate": 1.036982595017899e-05,
        "epoch": 1.9034666666666666,
        "step": 14276
    },
    {
        "loss": 2.2355,
        "grad_norm": 5.367341995239258,
        "learning_rate": 1.0341934737099012e-05,
        "epoch": 1.9036,
        "step": 14277
    },
    {
        "loss": 2.3824,
        "grad_norm": 3.1877243518829346,
        "learning_rate": 1.0314079037884117e-05,
        "epoch": 1.9037333333333333,
        "step": 14278
    },
    {
        "loss": 3.9288,
        "grad_norm": 4.261666774749756,
        "learning_rate": 1.0286258863568166e-05,
        "epoch": 1.9038666666666666,
        "step": 14279
    },
    {
        "loss": 1.9271,
        "grad_norm": 3.442920684814453,
        "learning_rate": 1.0258474225170789e-05,
        "epoch": 1.904,
        "step": 14280
    },
    {
        "loss": 2.5059,
        "grad_norm": 3.5204570293426514,
        "learning_rate": 1.0230725133697483e-05,
        "epoch": 1.9041333333333332,
        "step": 14281
    },
    {
        "loss": 2.9515,
        "grad_norm": 2.436967611312866,
        "learning_rate": 1.020301160013989e-05,
        "epoch": 1.9042666666666666,
        "step": 14282
    },
    {
        "loss": 1.9661,
        "grad_norm": 3.9491307735443115,
        "learning_rate": 1.0175333635475392e-05,
        "epoch": 1.9043999999999999,
        "step": 14283
    },
    {
        "loss": 2.1666,
        "grad_norm": 4.751116752624512,
        "learning_rate": 1.0147691250667235e-05,
        "epoch": 1.9045333333333332,
        "step": 14284
    },
    {
        "loss": 2.3854,
        "grad_norm": 2.648817539215088,
        "learning_rate": 1.0120084456664803e-05,
        "epoch": 1.9046666666666665,
        "step": 14285
    },
    {
        "loss": 1.1958,
        "grad_norm": 4.750461578369141,
        "learning_rate": 1.0092513264403192e-05,
        "epoch": 1.9048,
        "step": 14286
    },
    {
        "loss": 0.5731,
        "grad_norm": 2.407344341278076,
        "learning_rate": 1.0064977684803356e-05,
        "epoch": 1.9049333333333334,
        "step": 14287
    },
    {
        "loss": 2.0603,
        "grad_norm": 2.826333999633789,
        "learning_rate": 1.0037477728772383e-05,
        "epoch": 1.9050666666666667,
        "step": 14288
    },
    {
        "loss": 1.6984,
        "grad_norm": 3.926243782043457,
        "learning_rate": 1.001001340720299e-05,
        "epoch": 1.9052,
        "step": 14289
    },
    {
        "loss": 2.0328,
        "grad_norm": 8.65510368347168,
        "learning_rate": 9.982584730973876e-06,
        "epoch": 1.9053333333333333,
        "step": 14290
    },
    {
        "loss": 2.081,
        "grad_norm": 5.840713977813721,
        "learning_rate": 9.955191710949741e-06,
        "epoch": 1.9054666666666666,
        "step": 14291
    },
    {
        "loss": 2.73,
        "grad_norm": 2.423117160797119,
        "learning_rate": 9.92783435798098e-06,
        "epoch": 1.9056,
        "step": 14292
    },
    {
        "loss": 2.0905,
        "grad_norm": 4.486144065856934,
        "learning_rate": 9.90051268290394e-06,
        "epoch": 1.9057333333333333,
        "step": 14293
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.7695345878601074,
        "learning_rate": 9.873226696540793e-06,
        "epoch": 1.9058666666666668,
        "step": 14294
    },
    {
        "loss": 1.0982,
        "grad_norm": 3.395559310913086,
        "learning_rate": 9.845976409699687e-06,
        "epoch": 1.9060000000000001,
        "step": 14295
    },
    {
        "loss": 1.8147,
        "grad_norm": 3.6735188961029053,
        "learning_rate": 9.818761833174472e-06,
        "epoch": 1.9061333333333335,
        "step": 14296
    },
    {
        "loss": 2.311,
        "grad_norm": 3.584019184112549,
        "learning_rate": 9.791582977745107e-06,
        "epoch": 1.9062666666666668,
        "step": 14297
    },
    {
        "loss": 2.9145,
        "grad_norm": 3.0727763175964355,
        "learning_rate": 9.764439854176965e-06,
        "epoch": 1.9064,
        "step": 14298
    },
    {
        "loss": 2.2636,
        "grad_norm": 3.021259069442749,
        "learning_rate": 9.737332473221661e-06,
        "epoch": 1.9065333333333334,
        "step": 14299
    },
    {
        "loss": 2.3885,
        "grad_norm": 3.3226120471954346,
        "learning_rate": 9.710260845616615e-06,
        "epoch": 1.9066666666666667,
        "step": 14300
    },
    {
        "loss": 2.6248,
        "grad_norm": 3.616853952407837,
        "learning_rate": 9.683224982084881e-06,
        "epoch": 1.9068,
        "step": 14301
    },
    {
        "loss": 2.2146,
        "grad_norm": 3.4263408184051514,
        "learning_rate": 9.656224893335497e-06,
        "epoch": 1.9069333333333334,
        "step": 14302
    },
    {
        "loss": 0.9535,
        "grad_norm": 3.536867141723633,
        "learning_rate": 9.629260590063194e-06,
        "epoch": 1.9070666666666667,
        "step": 14303
    },
    {
        "loss": 2.1626,
        "grad_norm": 2.3200454711914062,
        "learning_rate": 9.602332082948761e-06,
        "epoch": 1.9072,
        "step": 14304
    },
    {
        "loss": 0.6974,
        "grad_norm": 3.481126308441162,
        "learning_rate": 9.575439382658569e-06,
        "epoch": 1.9073333333333333,
        "step": 14305
    },
    {
        "loss": 2.5893,
        "grad_norm": 2.0098958015441895,
        "learning_rate": 9.54858249984505e-06,
        "epoch": 1.9074666666666666,
        "step": 14306
    },
    {
        "loss": 2.5145,
        "grad_norm": 4.155604362487793,
        "learning_rate": 9.521761445146093e-06,
        "epoch": 1.9076,
        "step": 14307
    },
    {
        "loss": 2.5708,
        "grad_norm": 3.7559032440185547,
        "learning_rate": 9.494976229185747e-06,
        "epoch": 1.9077333333333333,
        "step": 14308
    },
    {
        "loss": 1.7495,
        "grad_norm": 2.4404382705688477,
        "learning_rate": 9.468226862573748e-06,
        "epoch": 1.9078666666666666,
        "step": 14309
    },
    {
        "loss": 1.9195,
        "grad_norm": 3.9362826347351074,
        "learning_rate": 9.441513355905562e-06,
        "epoch": 1.908,
        "step": 14310
    },
    {
        "loss": 1.519,
        "grad_norm": 3.7577269077301025,
        "learning_rate": 9.414835719762516e-06,
        "epoch": 1.9081333333333332,
        "step": 14311
    },
    {
        "loss": 2.0131,
        "grad_norm": 4.479343414306641,
        "learning_rate": 9.388193964711645e-06,
        "epoch": 1.9082666666666666,
        "step": 14312
    },
    {
        "loss": 2.8288,
        "grad_norm": 3.152817726135254,
        "learning_rate": 9.361588101305963e-06,
        "epoch": 1.9083999999999999,
        "step": 14313
    },
    {
        "loss": 2.29,
        "grad_norm": 4.933919906616211,
        "learning_rate": 9.33501814008405e-06,
        "epoch": 1.9085333333333332,
        "step": 14314
    },
    {
        "loss": 2.2014,
        "grad_norm": 2.851066827774048,
        "learning_rate": 9.308484091570401e-06,
        "epoch": 1.9086666666666665,
        "step": 14315
    },
    {
        "loss": 2.2447,
        "grad_norm": 4.004632949829102,
        "learning_rate": 9.281985966275175e-06,
        "epoch": 1.9088,
        "step": 14316
    },
    {
        "loss": 2.3849,
        "grad_norm": 3.625079393386841,
        "learning_rate": 9.255523774694419e-06,
        "epoch": 1.9089333333333334,
        "step": 14317
    },
    {
        "loss": 1.768,
        "grad_norm": 3.489964246749878,
        "learning_rate": 9.229097527310005e-06,
        "epoch": 1.9090666666666667,
        "step": 14318
    },
    {
        "loss": 1.7624,
        "grad_norm": 3.7928669452667236,
        "learning_rate": 9.202707234589258e-06,
        "epoch": 1.9092,
        "step": 14319
    },
    {
        "loss": 1.2707,
        "grad_norm": 4.688281059265137,
        "learning_rate": 9.176352906985597e-06,
        "epoch": 1.9093333333333333,
        "step": 14320
    },
    {
        "loss": 2.2288,
        "grad_norm": 4.503946304321289,
        "learning_rate": 9.150034554937969e-06,
        "epoch": 1.9094666666666666,
        "step": 14321
    },
    {
        "loss": 1.6056,
        "grad_norm": 3.28762149810791,
        "learning_rate": 9.123752188871292e-06,
        "epoch": 1.9096,
        "step": 14322
    },
    {
        "loss": 1.9465,
        "grad_norm": 3.2554521560668945,
        "learning_rate": 9.097505819196007e-06,
        "epoch": 1.9097333333333333,
        "step": 14323
    },
    {
        "loss": 2.4022,
        "grad_norm": 3.50136399269104,
        "learning_rate": 9.071295456308426e-06,
        "epoch": 1.9098666666666668,
        "step": 14324
    },
    {
        "loss": 1.2635,
        "grad_norm": 1.919890284538269,
        "learning_rate": 9.045121110590482e-06,
        "epoch": 1.9100000000000001,
        "step": 14325
    },
    {
        "loss": 2.4767,
        "grad_norm": 3.3011326789855957,
        "learning_rate": 9.01898279240998e-06,
        "epoch": 1.9101333333333335,
        "step": 14326
    },
    {
        "loss": 2.4214,
        "grad_norm": 2.3932271003723145,
        "learning_rate": 8.992880512120528e-06,
        "epoch": 1.9102666666666668,
        "step": 14327
    },
    {
        "loss": 1.9313,
        "grad_norm": 4.747595310211182,
        "learning_rate": 8.966814280061087e-06,
        "epoch": 1.9104,
        "step": 14328
    },
    {
        "loss": 1.9892,
        "grad_norm": 4.7689971923828125,
        "learning_rate": 8.940784106556743e-06,
        "epoch": 1.9105333333333334,
        "step": 14329
    },
    {
        "loss": 1.8199,
        "grad_norm": 4.903432846069336,
        "learning_rate": 8.914790001918061e-06,
        "epoch": 1.9106666666666667,
        "step": 14330
    },
    {
        "loss": 2.2521,
        "grad_norm": 3.3533554077148438,
        "learning_rate": 8.888831976441459e-06,
        "epoch": 1.9108,
        "step": 14331
    },
    {
        "loss": 2.3629,
        "grad_norm": 2.7912559509277344,
        "learning_rate": 8.862910040408967e-06,
        "epoch": 1.9109333333333334,
        "step": 14332
    },
    {
        "loss": 1.9993,
        "grad_norm": 5.142290115356445,
        "learning_rate": 8.837024204088363e-06,
        "epoch": 1.9110666666666667,
        "step": 14333
    },
    {
        "loss": 1.6082,
        "grad_norm": 3.4531750679016113,
        "learning_rate": 8.81117447773303e-06,
        "epoch": 1.9112,
        "step": 14334
    },
    {
        "loss": 2.5528,
        "grad_norm": 2.320490598678589,
        "learning_rate": 8.785360871582282e-06,
        "epoch": 1.9113333333333333,
        "step": 14335
    },
    {
        "loss": 2.1828,
        "grad_norm": 6.129931926727295,
        "learning_rate": 8.759583395860882e-06,
        "epoch": 1.9114666666666666,
        "step": 14336
    },
    {
        "loss": 2.6351,
        "grad_norm": 4.2634992599487305,
        "learning_rate": 8.73384206077933e-06,
        "epoch": 1.9116,
        "step": 14337
    },
    {
        "loss": 2.2278,
        "grad_norm": 3.5777201652526855,
        "learning_rate": 8.708136876534001e-06,
        "epoch": 1.9117333333333333,
        "step": 14338
    },
    {
        "loss": 3.4836,
        "grad_norm": 3.9635205268859863,
        "learning_rate": 8.682467853306697e-06,
        "epoch": 1.9118666666666666,
        "step": 14339
    },
    {
        "loss": 2.1362,
        "grad_norm": 3.6331028938293457,
        "learning_rate": 8.656835001264985e-06,
        "epoch": 1.912,
        "step": 14340
    },
    {
        "loss": 1.2989,
        "grad_norm": 3.4724903106689453,
        "learning_rate": 8.631238330562241e-06,
        "epoch": 1.9121333333333332,
        "step": 14341
    },
    {
        "loss": 2.5996,
        "grad_norm": 3.586581230163574,
        "learning_rate": 8.605677851337313e-06,
        "epoch": 1.9122666666666666,
        "step": 14342
    },
    {
        "loss": 2.6453,
        "grad_norm": 4.326554775238037,
        "learning_rate": 8.580153573714756e-06,
        "epoch": 1.9123999999999999,
        "step": 14343
    },
    {
        "loss": 2.2911,
        "grad_norm": 2.4951493740081787,
        "learning_rate": 8.554665507804938e-06,
        "epoch": 1.9125333333333332,
        "step": 14344
    },
    {
        "loss": 2.4315,
        "grad_norm": 3.2327098846435547,
        "learning_rate": 8.529213663703694e-06,
        "epoch": 1.9126666666666665,
        "step": 14345
    },
    {
        "loss": 1.9721,
        "grad_norm": 2.44598126411438,
        "learning_rate": 8.50379805149254e-06,
        "epoch": 1.9127999999999998,
        "step": 14346
    },
    {
        "loss": 1.5662,
        "grad_norm": 3.6824982166290283,
        "learning_rate": 8.478418681238809e-06,
        "epoch": 1.9129333333333334,
        "step": 14347
    },
    {
        "loss": 2.4076,
        "grad_norm": 3.082474946975708,
        "learning_rate": 8.453075562995284e-06,
        "epoch": 1.9130666666666667,
        "step": 14348
    },
    {
        "loss": 1.5509,
        "grad_norm": 4.3758416175842285,
        "learning_rate": 8.427768706800398e-06,
        "epoch": 1.9132,
        "step": 14349
    },
    {
        "loss": 2.2304,
        "grad_norm": 3.6822195053100586,
        "learning_rate": 8.402498122678437e-06,
        "epoch": 1.9133333333333333,
        "step": 14350
    },
    {
        "loss": 1.7771,
        "grad_norm": 2.0440518856048584,
        "learning_rate": 8.377263820639059e-06,
        "epoch": 1.9134666666666666,
        "step": 14351
    },
    {
        "loss": 2.2171,
        "grad_norm": 3.3461496829986572,
        "learning_rate": 8.35206581067769e-06,
        "epoch": 1.9136,
        "step": 14352
    },
    {
        "loss": 1.1881,
        "grad_norm": 6.374370574951172,
        "learning_rate": 8.326904102775269e-06,
        "epoch": 1.9137333333333333,
        "step": 14353
    },
    {
        "loss": 0.5976,
        "grad_norm": 2.482095956802368,
        "learning_rate": 8.301778706898555e-06,
        "epoch": 1.9138666666666668,
        "step": 14354
    },
    {
        "loss": 1.9621,
        "grad_norm": 3.8141183853149414,
        "learning_rate": 8.276689632999712e-06,
        "epoch": 1.9140000000000001,
        "step": 14355
    },
    {
        "loss": 2.4822,
        "grad_norm": 3.280494451522827,
        "learning_rate": 8.251636891016745e-06,
        "epoch": 1.9141333333333335,
        "step": 14356
    },
    {
        "loss": 1.7582,
        "grad_norm": 5.1771559715271,
        "learning_rate": 8.226620490872916e-06,
        "epoch": 1.9142666666666668,
        "step": 14357
    },
    {
        "loss": 1.5388,
        "grad_norm": 4.721559524536133,
        "learning_rate": 8.201640442477432e-06,
        "epoch": 1.9144,
        "step": 14358
    },
    {
        "loss": 1.7071,
        "grad_norm": 5.354249477386475,
        "learning_rate": 8.176696755725e-06,
        "epoch": 1.9145333333333334,
        "step": 14359
    },
    {
        "loss": 0.8758,
        "grad_norm": 4.680612087249756,
        "learning_rate": 8.151789440495893e-06,
        "epoch": 1.9146666666666667,
        "step": 14360
    },
    {
        "loss": 1.9311,
        "grad_norm": 3.797677993774414,
        "learning_rate": 8.126918506655923e-06,
        "epoch": 1.9148,
        "step": 14361
    },
    {
        "loss": 2.6897,
        "grad_norm": 2.567473888397217,
        "learning_rate": 8.10208396405654e-06,
        "epoch": 1.9149333333333334,
        "step": 14362
    },
    {
        "loss": 1.629,
        "grad_norm": 6.130931377410889,
        "learning_rate": 8.077285822534896e-06,
        "epoch": 1.9150666666666667,
        "step": 14363
    },
    {
        "loss": 1.6934,
        "grad_norm": 5.166573524475098,
        "learning_rate": 8.052524091913494e-06,
        "epoch": 1.9152,
        "step": 14364
    },
    {
        "loss": 1.7523,
        "grad_norm": 3.792113780975342,
        "learning_rate": 8.027798782000694e-06,
        "epoch": 1.9153333333333333,
        "step": 14365
    },
    {
        "loss": 1.6896,
        "grad_norm": 2.9861857891082764,
        "learning_rate": 8.003109902590112e-06,
        "epoch": 1.9154666666666667,
        "step": 14366
    },
    {
        "loss": 2.0172,
        "grad_norm": 4.9293742179870605,
        "learning_rate": 7.978457463461142e-06,
        "epoch": 1.9156,
        "step": 14367
    },
    {
        "loss": 1.6046,
        "grad_norm": 3.6113595962524414,
        "learning_rate": 7.953841474378787e-06,
        "epoch": 1.9157333333333333,
        "step": 14368
    },
    {
        "loss": 2.2555,
        "grad_norm": 5.081350803375244,
        "learning_rate": 7.92926194509347e-06,
        "epoch": 1.9158666666666666,
        "step": 14369
    },
    {
        "loss": 2.1929,
        "grad_norm": 3.635774850845337,
        "learning_rate": 7.904718885341212e-06,
        "epoch": 1.916,
        "step": 14370
    },
    {
        "loss": 2.3914,
        "grad_norm": 2.331613540649414,
        "learning_rate": 7.88021230484356e-06,
        "epoch": 1.9161333333333332,
        "step": 14371
    },
    {
        "loss": 2.0212,
        "grad_norm": 4.390538215637207,
        "learning_rate": 7.85574221330776e-06,
        "epoch": 1.9162666666666666,
        "step": 14372
    },
    {
        "loss": 2.0048,
        "grad_norm": 3.8486416339874268,
        "learning_rate": 7.831308620426436e-06,
        "epoch": 1.9163999999999999,
        "step": 14373
    },
    {
        "loss": 3.0076,
        "grad_norm": 4.143527984619141,
        "learning_rate": 7.806911535877847e-06,
        "epoch": 1.9165333333333332,
        "step": 14374
    },
    {
        "loss": 1.1621,
        "grad_norm": 3.755066394805908,
        "learning_rate": 7.782550969325664e-06,
        "epoch": 1.9166666666666665,
        "step": 14375
    },
    {
        "loss": 1.255,
        "grad_norm": 2.373292922973633,
        "learning_rate": 7.758226930419266e-06,
        "epoch": 1.9167999999999998,
        "step": 14376
    },
    {
        "loss": 1.7526,
        "grad_norm": 4.142899990081787,
        "learning_rate": 7.733939428793601e-06,
        "epoch": 1.9169333333333334,
        "step": 14377
    },
    {
        "loss": 1.8785,
        "grad_norm": 5.516386032104492,
        "learning_rate": 7.7096884740688e-06,
        "epoch": 1.9170666666666667,
        "step": 14378
    },
    {
        "loss": 1.1864,
        "grad_norm": 5.236581802368164,
        "learning_rate": 7.685474075850918e-06,
        "epoch": 1.9172,
        "step": 14379
    },
    {
        "loss": 2.8525,
        "grad_norm": 3.236727714538574,
        "learning_rate": 7.661296243731276e-06,
        "epoch": 1.9173333333333333,
        "step": 14380
    },
    {
        "loss": 2.2784,
        "grad_norm": 4.337564945220947,
        "learning_rate": 7.637154987286866e-06,
        "epoch": 1.9174666666666667,
        "step": 14381
    },
    {
        "loss": 1.5928,
        "grad_norm": 6.05146598815918,
        "learning_rate": 7.613050316080094e-06,
        "epoch": 1.9176,
        "step": 14382
    },
    {
        "loss": 2.8224,
        "grad_norm": 3.140352249145508,
        "learning_rate": 7.588982239658882e-06,
        "epoch": 1.9177333333333333,
        "step": 14383
    },
    {
        "loss": 2.345,
        "grad_norm": 2.4286704063415527,
        "learning_rate": 7.564950767556633e-06,
        "epoch": 1.9178666666666668,
        "step": 14384
    },
    {
        "loss": 2.5598,
        "grad_norm": 3.350114583969116,
        "learning_rate": 7.540955909292346e-06,
        "epoch": 1.9180000000000001,
        "step": 14385
    },
    {
        "loss": 1.7268,
        "grad_norm": 4.361501216888428,
        "learning_rate": 7.516997674370596e-06,
        "epoch": 1.9181333333333335,
        "step": 14386
    },
    {
        "loss": 2.4613,
        "grad_norm": 4.216433048248291,
        "learning_rate": 7.49307607228108e-06,
        "epoch": 1.9182666666666668,
        "step": 14387
    },
    {
        "loss": 0.8501,
        "grad_norm": 4.514991760253906,
        "learning_rate": 7.469191112499397e-06,
        "epoch": 1.9184,
        "step": 14388
    },
    {
        "loss": 0.7977,
        "grad_norm": 2.857394218444824,
        "learning_rate": 7.445342804486344e-06,
        "epoch": 1.9185333333333334,
        "step": 14389
    },
    {
        "loss": 2.1197,
        "grad_norm": 3.74261736869812,
        "learning_rate": 7.421531157688433e-06,
        "epoch": 1.9186666666666667,
        "step": 14390
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.5815138816833496,
        "learning_rate": 7.3977561815374965e-06,
        "epoch": 1.9188,
        "step": 14391
    },
    {
        "loss": 0.87,
        "grad_norm": 2.361938238143921,
        "learning_rate": 7.374017885450846e-06,
        "epoch": 1.9189333333333334,
        "step": 14392
    },
    {
        "loss": 2.2018,
        "grad_norm": 4.173382759094238,
        "learning_rate": 7.350316278831293e-06,
        "epoch": 1.9190666666666667,
        "step": 14393
    },
    {
        "loss": 2.4609,
        "grad_norm": 3.813447952270508,
        "learning_rate": 7.326651371067206e-06,
        "epoch": 1.9192,
        "step": 14394
    },
    {
        "loss": 2.2983,
        "grad_norm": 4.22772741317749,
        "learning_rate": 7.303023171532286e-06,
        "epoch": 1.9193333333333333,
        "step": 14395
    },
    {
        "loss": 2.1806,
        "grad_norm": 5.128103256225586,
        "learning_rate": 7.279431689585703e-06,
        "epoch": 1.9194666666666667,
        "step": 14396
    },
    {
        "loss": 2.1861,
        "grad_norm": 4.439038276672363,
        "learning_rate": 7.255876934572204e-06,
        "epoch": 1.9196,
        "step": 14397
    },
    {
        "loss": 3.214,
        "grad_norm": 4.0504255294799805,
        "learning_rate": 7.232358915821868e-06,
        "epoch": 1.9197333333333333,
        "step": 14398
    },
    {
        "loss": 2.3957,
        "grad_norm": 4.8078837394714355,
        "learning_rate": 7.2088776426502245e-06,
        "epoch": 1.9198666666666666,
        "step": 14399
    },
    {
        "loss": 2.3242,
        "grad_norm": 3.725569486618042,
        "learning_rate": 7.185433124358365e-06,
        "epoch": 1.92,
        "step": 14400
    },
    {
        "loss": 0.8586,
        "grad_norm": 3.828373908996582,
        "learning_rate": 7.162025370232706e-06,
        "epoch": 1.9201333333333332,
        "step": 14401
    },
    {
        "loss": 2.0157,
        "grad_norm": 4.474791049957275,
        "learning_rate": 7.138654389545097e-06,
        "epoch": 1.9202666666666666,
        "step": 14402
    },
    {
        "loss": 1.603,
        "grad_norm": 7.342514991760254,
        "learning_rate": 7.1153201915529325e-06,
        "epoch": 1.9203999999999999,
        "step": 14403
    },
    {
        "loss": 1.3591,
        "grad_norm": 5.0853424072265625,
        "learning_rate": 7.092022785498942e-06,
        "epoch": 1.9205333333333332,
        "step": 14404
    },
    {
        "loss": 1.7451,
        "grad_norm": 3.616346836090088,
        "learning_rate": 7.068762180611266e-06,
        "epoch": 1.9206666666666665,
        "step": 14405
    },
    {
        "loss": 2.8108,
        "grad_norm": 3.496633529663086,
        "learning_rate": 7.045538386103578e-06,
        "epoch": 1.9207999999999998,
        "step": 14406
    },
    {
        "loss": 1.9906,
        "grad_norm": 4.098811626434326,
        "learning_rate": 7.022351411174888e-06,
        "epoch": 1.9209333333333334,
        "step": 14407
    },
    {
        "loss": 0.6219,
        "grad_norm": 2.983588218688965,
        "learning_rate": 6.999201265009547e-06,
        "epoch": 1.9210666666666667,
        "step": 14408
    },
    {
        "loss": 1.5025,
        "grad_norm": 3.3934621810913086,
        "learning_rate": 6.976087956777533e-06,
        "epoch": 1.9212,
        "step": 14409
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.7211410999298096,
        "learning_rate": 6.953011495634065e-06,
        "epoch": 1.9213333333333333,
        "step": 14410
    },
    {
        "loss": 1.6593,
        "grad_norm": 4.8181304931640625,
        "learning_rate": 6.929971890719777e-06,
        "epoch": 1.9214666666666667,
        "step": 14411
    },
    {
        "loss": 1.9621,
        "grad_norm": 4.492029666900635,
        "learning_rate": 6.906969151160703e-06,
        "epoch": 1.9216,
        "step": 14412
    },
    {
        "loss": 1.9922,
        "grad_norm": 3.2690908908843994,
        "learning_rate": 6.884003286068419e-06,
        "epoch": 1.9217333333333333,
        "step": 14413
    },
    {
        "loss": 2.5292,
        "grad_norm": 4.344673156738281,
        "learning_rate": 6.86107430453965e-06,
        "epoch": 1.9218666666666666,
        "step": 14414
    },
    {
        "loss": 2.4283,
        "grad_norm": 3.1134347915649414,
        "learning_rate": 6.8381822156568296e-06,
        "epoch": 1.9220000000000002,
        "step": 14415
    },
    {
        "loss": 2.7572,
        "grad_norm": 4.003015041351318,
        "learning_rate": 6.815327028487362e-06,
        "epoch": 1.9221333333333335,
        "step": 14416
    },
    {
        "loss": 2.4293,
        "grad_norm": 4.226972579956055,
        "learning_rate": 6.79250875208437e-06,
        "epoch": 1.9222666666666668,
        "step": 14417
    },
    {
        "loss": 2.635,
        "grad_norm": 2.8603832721710205,
        "learning_rate": 6.769727395486314e-06,
        "epoch": 1.9224,
        "step": 14418
    },
    {
        "loss": 1.6824,
        "grad_norm": 2.2462000846862793,
        "learning_rate": 6.746982967716931e-06,
        "epoch": 1.9225333333333334,
        "step": 14419
    },
    {
        "loss": 1.8787,
        "grad_norm": 2.3852040767669678,
        "learning_rate": 6.724275477785325e-06,
        "epoch": 1.9226666666666667,
        "step": 14420
    },
    {
        "loss": 0.9389,
        "grad_norm": 3.160414218902588,
        "learning_rate": 6.701604934686001e-06,
        "epoch": 1.9228,
        "step": 14421
    },
    {
        "loss": 2.3777,
        "grad_norm": 2.298557758331299,
        "learning_rate": 6.6789713473988994e-06,
        "epoch": 1.9229333333333334,
        "step": 14422
    },
    {
        "loss": 1.154,
        "grad_norm": 4.498256206512451,
        "learning_rate": 6.65637472488918e-06,
        "epoch": 1.9230666666666667,
        "step": 14423
    },
    {
        "loss": 2.6112,
        "grad_norm": 3.8439700603485107,
        "learning_rate": 6.633815076107608e-06,
        "epoch": 1.9232,
        "step": 14424
    },
    {
        "loss": 2.1064,
        "grad_norm": 3.535266876220703,
        "learning_rate": 6.611292409989911e-06,
        "epoch": 1.9233333333333333,
        "step": 14425
    },
    {
        "loss": 2.3039,
        "grad_norm": 3.0058228969573975,
        "learning_rate": 6.588806735457498e-06,
        "epoch": 1.9234666666666667,
        "step": 14426
    },
    {
        "loss": 0.7119,
        "grad_norm": 3.725734233856201,
        "learning_rate": 6.566358061417099e-06,
        "epoch": 1.9236,
        "step": 14427
    },
    {
        "loss": 1.4043,
        "grad_norm": 3.7091023921966553,
        "learning_rate": 6.543946396760625e-06,
        "epoch": 1.9237333333333333,
        "step": 14428
    },
    {
        "loss": 1.0845,
        "grad_norm": 3.9626033306121826,
        "learning_rate": 6.521571750365429e-06,
        "epoch": 1.9238666666666666,
        "step": 14429
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.6996288299560547,
        "learning_rate": 6.499234131094134e-06,
        "epoch": 1.924,
        "step": 14430
    },
    {
        "loss": 2.2956,
        "grad_norm": 3.9673502445220947,
        "learning_rate": 6.476933547794839e-06,
        "epoch": 1.9241333333333333,
        "step": 14431
    },
    {
        "loss": 2.7597,
        "grad_norm": 2.300794839859009,
        "learning_rate": 6.454670009300856e-06,
        "epoch": 1.9242666666666666,
        "step": 14432
    },
    {
        "loss": 1.5623,
        "grad_norm": 2.3597190380096436,
        "learning_rate": 6.432443524430809e-06,
        "epoch": 1.9243999999999999,
        "step": 14433
    },
    {
        "loss": 2.1791,
        "grad_norm": 3.2053518295288086,
        "learning_rate": 6.410254101988656e-06,
        "epoch": 1.9245333333333332,
        "step": 14434
    },
    {
        "loss": 0.9162,
        "grad_norm": 4.497358798980713,
        "learning_rate": 6.388101750763775e-06,
        "epoch": 1.9246666666666665,
        "step": 14435
    },
    {
        "loss": 1.8931,
        "grad_norm": 4.965841770172119,
        "learning_rate": 6.365986479530839e-06,
        "epoch": 1.9247999999999998,
        "step": 14436
    },
    {
        "loss": 1.7247,
        "grad_norm": 5.3402485847473145,
        "learning_rate": 6.343908297049617e-06,
        "epoch": 1.9249333333333334,
        "step": 14437
    },
    {
        "loss": 2.4042,
        "grad_norm": 3.8486571311950684,
        "learning_rate": 6.321867212065513e-06,
        "epoch": 1.9250666666666667,
        "step": 14438
    },
    {
        "loss": 2.6487,
        "grad_norm": 3.600280523300171,
        "learning_rate": 6.299863233308934e-06,
        "epoch": 1.9252,
        "step": 14439
    },
    {
        "loss": 2.3108,
        "grad_norm": 3.432676315307617,
        "learning_rate": 6.27789636949585e-06,
        "epoch": 1.9253333333333333,
        "step": 14440
    },
    {
        "loss": 2.4597,
        "grad_norm": 2.4610280990600586,
        "learning_rate": 6.2559666293273925e-06,
        "epoch": 1.9254666666666667,
        "step": 14441
    },
    {
        "loss": 1.7513,
        "grad_norm": 4.155956745147705,
        "learning_rate": 6.234074021489944e-06,
        "epoch": 1.9256,
        "step": 14442
    },
    {
        "loss": 1.8099,
        "grad_norm": 4.274986743927002,
        "learning_rate": 6.212218554655258e-06,
        "epoch": 1.9257333333333333,
        "step": 14443
    },
    {
        "loss": 3.9909,
        "grad_norm": 7.310636520385742,
        "learning_rate": 6.190400237480354e-06,
        "epoch": 1.9258666666666666,
        "step": 14444
    },
    {
        "loss": 1.6356,
        "grad_norm": 4.859036922454834,
        "learning_rate": 6.168619078607663e-06,
        "epoch": 1.9260000000000002,
        "step": 14445
    },
    {
        "loss": 1.7038,
        "grad_norm": 3.2117795944213867,
        "learning_rate": 6.146875086664594e-06,
        "epoch": 1.9261333333333335,
        "step": 14446
    },
    {
        "loss": 2.1329,
        "grad_norm": 4.9341936111450195,
        "learning_rate": 6.125168270264137e-06,
        "epoch": 1.9262666666666668,
        "step": 14447
    },
    {
        "loss": 2.3972,
        "grad_norm": 2.592313289642334,
        "learning_rate": 6.103498638004357e-06,
        "epoch": 1.9264000000000001,
        "step": 14448
    },
    {
        "loss": 2.1158,
        "grad_norm": 3.5743017196655273,
        "learning_rate": 6.081866198468755e-06,
        "epoch": 1.9265333333333334,
        "step": 14449
    },
    {
        "loss": 3.5134,
        "grad_norm": 6.212795257568359,
        "learning_rate": 6.060270960225966e-06,
        "epoch": 1.9266666666666667,
        "step": 14450
    },
    {
        "loss": 2.5485,
        "grad_norm": 6.521677494049072,
        "learning_rate": 6.038712931829948e-06,
        "epoch": 1.9268,
        "step": 14451
    },
    {
        "loss": 1.7348,
        "grad_norm": 4.657598495483398,
        "learning_rate": 6.017192121819859e-06,
        "epoch": 1.9269333333333334,
        "step": 14452
    },
    {
        "loss": 0.6717,
        "grad_norm": 2.5385167598724365,
        "learning_rate": 5.995708538720246e-06,
        "epoch": 1.9270666666666667,
        "step": 14453
    },
    {
        "loss": 2.7531,
        "grad_norm": 5.371606349945068,
        "learning_rate": 5.974262191040803e-06,
        "epoch": 1.9272,
        "step": 14454
    },
    {
        "loss": 1.75,
        "grad_norm": 3.8245737552642822,
        "learning_rate": 5.952853087276444e-06,
        "epoch": 1.9273333333333333,
        "step": 14455
    },
    {
        "loss": 1.5046,
        "grad_norm": 4.4035820960998535,
        "learning_rate": 5.931481235907466e-06,
        "epoch": 1.9274666666666667,
        "step": 14456
    },
    {
        "loss": 1.8323,
        "grad_norm": 2.528717517852783,
        "learning_rate": 5.910146645399317e-06,
        "epoch": 1.9276,
        "step": 14457
    },
    {
        "loss": 1.7274,
        "grad_norm": 4.630784034729004,
        "learning_rate": 5.888849324202628e-06,
        "epoch": 1.9277333333333333,
        "step": 14458
    },
    {
        "loss": 2.2765,
        "grad_norm": 4.162567138671875,
        "learning_rate": 5.867589280753449e-06,
        "epoch": 1.9278666666666666,
        "step": 14459
    },
    {
        "loss": 3.0809,
        "grad_norm": 5.862351417541504,
        "learning_rate": 5.846366523472912e-06,
        "epoch": 1.928,
        "step": 14460
    },
    {
        "loss": 3.1881,
        "grad_norm": 2.950396776199341,
        "learning_rate": 5.825181060767404e-06,
        "epoch": 1.9281333333333333,
        "step": 14461
    },
    {
        "loss": 1.8713,
        "grad_norm": 3.4252476692199707,
        "learning_rate": 5.804032901028522e-06,
        "epoch": 1.9282666666666666,
        "step": 14462
    },
    {
        "loss": 2.4862,
        "grad_norm": 3.300360918045044,
        "learning_rate": 5.782922052633233e-06,
        "epoch": 1.9284,
        "step": 14463
    },
    {
        "loss": 2.3938,
        "grad_norm": 3.9140563011169434,
        "learning_rate": 5.761848523943503e-06,
        "epoch": 1.9285333333333332,
        "step": 14464
    },
    {
        "loss": 2.2131,
        "grad_norm": 3.3522183895111084,
        "learning_rate": 5.740812323306721e-06,
        "epoch": 1.9286666666666665,
        "step": 14465
    },
    {
        "loss": 2.3191,
        "grad_norm": 4.0468950271606445,
        "learning_rate": 5.719813459055368e-06,
        "epoch": 1.9287999999999998,
        "step": 14466
    },
    {
        "loss": 1.9339,
        "grad_norm": 3.970984935760498,
        "learning_rate": 5.698851939507099e-06,
        "epoch": 1.9289333333333334,
        "step": 14467
    },
    {
        "loss": 2.1135,
        "grad_norm": 3.098210334777832,
        "learning_rate": 5.6779277729649635e-06,
        "epoch": 1.9290666666666667,
        "step": 14468
    },
    {
        "loss": 3.1509,
        "grad_norm": 3.272153854370117,
        "learning_rate": 5.65704096771702e-06,
        "epoch": 1.9292,
        "step": 14469
    },
    {
        "loss": 1.8331,
        "grad_norm": 3.5949249267578125,
        "learning_rate": 5.636191532036605e-06,
        "epoch": 1.9293333333333333,
        "step": 14470
    },
    {
        "loss": 2.4206,
        "grad_norm": 5.616711139678955,
        "learning_rate": 5.615379474182236e-06,
        "epoch": 1.9294666666666667,
        "step": 14471
    },
    {
        "loss": 1.6033,
        "grad_norm": 3.873039484024048,
        "learning_rate": 5.594604802397696e-06,
        "epoch": 1.9296,
        "step": 14472
    },
    {
        "loss": 2.1676,
        "grad_norm": 2.801588773727417,
        "learning_rate": 5.573867524911825e-06,
        "epoch": 1.9297333333333333,
        "step": 14473
    },
    {
        "loss": 1.5122,
        "grad_norm": 4.460543632507324,
        "learning_rate": 5.553167649938884e-06,
        "epoch": 1.9298666666666666,
        "step": 14474
    },
    {
        "loss": 2.7447,
        "grad_norm": 3.201404333114624,
        "learning_rate": 5.5325051856779495e-06,
        "epoch": 1.9300000000000002,
        "step": 14475
    },
    {
        "loss": 1.7722,
        "grad_norm": 4.042083740234375,
        "learning_rate": 5.511880140313597e-06,
        "epoch": 1.9301333333333335,
        "step": 14476
    },
    {
        "loss": 2.2409,
        "grad_norm": 3.052388906478882,
        "learning_rate": 5.491292522015546e-06,
        "epoch": 1.9302666666666668,
        "step": 14477
    },
    {
        "loss": 2.428,
        "grad_norm": 4.106184482574463,
        "learning_rate": 5.47074233893854e-06,
        "epoch": 1.9304000000000001,
        "step": 14478
    },
    {
        "loss": 1.9321,
        "grad_norm": 4.333281517028809,
        "learning_rate": 5.4502295992226115e-06,
        "epoch": 1.9305333333333334,
        "step": 14479
    },
    {
        "loss": 2.2855,
        "grad_norm": 3.052295207977295,
        "learning_rate": 5.429754310992852e-06,
        "epoch": 1.9306666666666668,
        "step": 14480
    },
    {
        "loss": 1.8483,
        "grad_norm": 4.355035781860352,
        "learning_rate": 5.4093164823596946e-06,
        "epoch": 1.9308,
        "step": 14481
    },
    {
        "loss": 1.9499,
        "grad_norm": 4.319010257720947,
        "learning_rate": 5.3889161214185526e-06,
        "epoch": 1.9309333333333334,
        "step": 14482
    },
    {
        "loss": 1.4086,
        "grad_norm": 5.525063514709473,
        "learning_rate": 5.368553236250196e-06,
        "epoch": 1.9310666666666667,
        "step": 14483
    },
    {
        "loss": 1.4703,
        "grad_norm": 3.0137219429016113,
        "learning_rate": 5.3482278349202605e-06,
        "epoch": 1.9312,
        "step": 14484
    },
    {
        "loss": 1.1355,
        "grad_norm": 3.869518995285034,
        "learning_rate": 5.327939925479808e-06,
        "epoch": 1.9313333333333333,
        "step": 14485
    },
    {
        "loss": 2.5434,
        "grad_norm": 5.83984375,
        "learning_rate": 5.307689515964976e-06,
        "epoch": 1.9314666666666667,
        "step": 14486
    },
    {
        "loss": 1.7176,
        "grad_norm": 4.19331169128418,
        "learning_rate": 5.287476614396969e-06,
        "epoch": 1.9316,
        "step": 14487
    },
    {
        "loss": 2.1692,
        "grad_norm": 3.279371976852417,
        "learning_rate": 5.267301228782229e-06,
        "epoch": 1.9317333333333333,
        "step": 14488
    },
    {
        "loss": 2.3795,
        "grad_norm": 2.8400375843048096,
        "learning_rate": 5.2471633671121956e-06,
        "epoch": 1.9318666666666666,
        "step": 14489
    },
    {
        "loss": 2.1782,
        "grad_norm": 3.2002530097961426,
        "learning_rate": 5.227063037363678e-06,
        "epoch": 1.932,
        "step": 14490
    },
    {
        "loss": 2.3237,
        "grad_norm": 3.659590721130371,
        "learning_rate": 5.207000247498406e-06,
        "epoch": 1.9321333333333333,
        "step": 14491
    },
    {
        "loss": 3.124,
        "grad_norm": 4.994871616363525,
        "learning_rate": 5.186975005463346e-06,
        "epoch": 1.9322666666666666,
        "step": 14492
    },
    {
        "loss": 1.9752,
        "grad_norm": 3.7894277572631836,
        "learning_rate": 5.166987319190508e-06,
        "epoch": 1.9324,
        "step": 14493
    },
    {
        "loss": 2.1572,
        "grad_norm": 2.985806703567505,
        "learning_rate": 5.147037196597127e-06,
        "epoch": 1.9325333333333332,
        "step": 14494
    },
    {
        "loss": 1.694,
        "grad_norm": 4.477407455444336,
        "learning_rate": 5.127124645585601e-06,
        "epoch": 1.9326666666666665,
        "step": 14495
    },
    {
        "loss": 1.9497,
        "grad_norm": 5.006925582885742,
        "learning_rate": 5.107249674043191e-06,
        "epoch": 1.9327999999999999,
        "step": 14496
    },
    {
        "loss": 1.5232,
        "grad_norm": 4.628936767578125,
        "learning_rate": 5.087412289842564e-06,
        "epoch": 1.9329333333333332,
        "step": 14497
    },
    {
        "loss": 2.3837,
        "grad_norm": 4.046441078186035,
        "learning_rate": 5.06761250084129e-06,
        "epoch": 1.9330666666666667,
        "step": 14498
    },
    {
        "loss": 1.6201,
        "grad_norm": 6.147172451019287,
        "learning_rate": 5.047850314882241e-06,
        "epoch": 1.9332,
        "step": 14499
    },
    {
        "loss": 1.8512,
        "grad_norm": 3.8726189136505127,
        "learning_rate": 5.028125739793199e-06,
        "epoch": 1.9333333333333333,
        "step": 14500
    },
    {
        "loss": 2.0698,
        "grad_norm": 4.535205841064453,
        "learning_rate": 5.008438783387181e-06,
        "epoch": 1.9334666666666667,
        "step": 14501
    },
    {
        "loss": 1.6314,
        "grad_norm": 4.255928039550781,
        "learning_rate": 4.988789453462184e-06,
        "epoch": 1.9336,
        "step": 14502
    },
    {
        "loss": 2.5387,
        "grad_norm": 3.199113130569458,
        "learning_rate": 4.969177757801413e-06,
        "epoch": 1.9337333333333333,
        "step": 14503
    },
    {
        "loss": 2.0516,
        "grad_norm": 5.175173282623291,
        "learning_rate": 4.949603704173255e-06,
        "epoch": 1.9338666666666666,
        "step": 14504
    },
    {
        "loss": 2.5022,
        "grad_norm": 3.94466495513916,
        "learning_rate": 4.930067300330843e-06,
        "epoch": 1.9340000000000002,
        "step": 14505
    },
    {
        "loss": 2.5033,
        "grad_norm": 3.4408304691314697,
        "learning_rate": 4.910568554012751e-06,
        "epoch": 1.9341333333333335,
        "step": 14506
    },
    {
        "loss": 0.6794,
        "grad_norm": 3.2406864166259766,
        "learning_rate": 4.891107472942447e-06,
        "epoch": 1.9342666666666668,
        "step": 14507
    },
    {
        "loss": 2.7316,
        "grad_norm": 3.26360821723938,
        "learning_rate": 4.8716840648285745e-06,
        "epoch": 1.9344000000000001,
        "step": 14508
    },
    {
        "loss": 2.2005,
        "grad_norm": 4.652871131896973,
        "learning_rate": 4.85229833736478e-06,
        "epoch": 1.9345333333333334,
        "step": 14509
    },
    {
        "loss": 1.9869,
        "grad_norm": 4.026216983795166,
        "learning_rate": 4.832950298229832e-06,
        "epoch": 1.9346666666666668,
        "step": 14510
    },
    {
        "loss": 1.0746,
        "grad_norm": 4.120606422424316,
        "learning_rate": 4.813639955087501e-06,
        "epoch": 1.9348,
        "step": 14511
    },
    {
        "loss": 1.3224,
        "grad_norm": 5.074661731719971,
        "learning_rate": 4.794367315586756e-06,
        "epoch": 1.9349333333333334,
        "step": 14512
    },
    {
        "loss": 2.587,
        "grad_norm": 3.267329216003418,
        "learning_rate": 4.775132387361548e-06,
        "epoch": 1.9350666666666667,
        "step": 14513
    },
    {
        "loss": 2.6834,
        "grad_norm": 3.658889055252075,
        "learning_rate": 4.7559351780308015e-06,
        "epoch": 1.9352,
        "step": 14514
    },
    {
        "loss": 1.223,
        "grad_norm": 4.3545613288879395,
        "learning_rate": 4.7367756951987365e-06,
        "epoch": 1.9353333333333333,
        "step": 14515
    },
    {
        "loss": 1.5828,
        "grad_norm": 4.426870346069336,
        "learning_rate": 4.717653946454393e-06,
        "epoch": 1.9354666666666667,
        "step": 14516
    },
    {
        "loss": 1.6427,
        "grad_norm": 3.0738086700439453,
        "learning_rate": 4.698569939371955e-06,
        "epoch": 1.9356,
        "step": 14517
    },
    {
        "loss": 1.3078,
        "grad_norm": 4.2637224197387695,
        "learning_rate": 4.6795236815107554e-06,
        "epoch": 1.9357333333333333,
        "step": 14518
    },
    {
        "loss": 1.9098,
        "grad_norm": 3.484736442565918,
        "learning_rate": 4.660515180414993e-06,
        "epoch": 1.9358666666666666,
        "step": 14519
    },
    {
        "loss": 2.3515,
        "grad_norm": 3.8150532245635986,
        "learning_rate": 4.641544443614043e-06,
        "epoch": 1.936,
        "step": 14520
    },
    {
        "loss": 1.4777,
        "grad_norm": 4.260350227355957,
        "learning_rate": 4.6226114786222295e-06,
        "epoch": 1.9361333333333333,
        "step": 14521
    },
    {
        "loss": 2.503,
        "grad_norm": 2.1549692153930664,
        "learning_rate": 4.603716292939031e-06,
        "epoch": 1.9362666666666666,
        "step": 14522
    },
    {
        "loss": 2.5022,
        "grad_norm": 2.5827033519744873,
        "learning_rate": 4.5848588940488265e-06,
        "epoch": 1.9364,
        "step": 14523
    },
    {
        "loss": 2.5663,
        "grad_norm": 2.4640841484069824,
        "learning_rate": 4.566039289421176e-06,
        "epoch": 1.9365333333333332,
        "step": 14524
    },
    {
        "loss": 1.5594,
        "grad_norm": 4.453523635864258,
        "learning_rate": 4.547257486510548e-06,
        "epoch": 1.9366666666666665,
        "step": 14525
    },
    {
        "loss": 1.7971,
        "grad_norm": 4.194456577301025,
        "learning_rate": 4.528513492756425e-06,
        "epoch": 1.9367999999999999,
        "step": 14526
    },
    {
        "loss": 2.627,
        "grad_norm": 3.519920825958252,
        "learning_rate": 4.509807315583469e-06,
        "epoch": 1.9369333333333332,
        "step": 14527
    },
    {
        "loss": 2.6573,
        "grad_norm": 1.9784740209579468,
        "learning_rate": 4.491138962401209e-06,
        "epoch": 1.9370666666666667,
        "step": 14528
    },
    {
        "loss": 2.655,
        "grad_norm": 2.7101595401763916,
        "learning_rate": 4.4725084406042505e-06,
        "epoch": 1.9372,
        "step": 14529
    },
    {
        "loss": 0.9771,
        "grad_norm": 3.9495439529418945,
        "learning_rate": 4.453915757572147e-06,
        "epoch": 1.9373333333333334,
        "step": 14530
    },
    {
        "loss": 2.5221,
        "grad_norm": 2.850888252258301,
        "learning_rate": 4.435360920669618e-06,
        "epoch": 1.9374666666666667,
        "step": 14531
    },
    {
        "loss": 2.5983,
        "grad_norm": 3.8763320446014404,
        "learning_rate": 4.41684393724624e-06,
        "epoch": 1.9376,
        "step": 14532
    },
    {
        "loss": 2.3914,
        "grad_norm": 2.6573734283447266,
        "learning_rate": 4.398364814636724e-06,
        "epoch": 1.9377333333333333,
        "step": 14533
    },
    {
        "loss": 2.5587,
        "grad_norm": 4.407564640045166,
        "learning_rate": 4.3799235601605815e-06,
        "epoch": 1.9378666666666666,
        "step": 14534
    },
    {
        "loss": 1.6955,
        "grad_norm": 2.5724589824676514,
        "learning_rate": 4.361520181122547e-06,
        "epoch": 1.938,
        "step": 14535
    },
    {
        "loss": 2.269,
        "grad_norm": 2.9262804985046387,
        "learning_rate": 4.343154684812278e-06,
        "epoch": 1.9381333333333335,
        "step": 14536
    },
    {
        "loss": 1.4875,
        "grad_norm": 3.911815643310547,
        "learning_rate": 4.324827078504401e-06,
        "epoch": 1.9382666666666668,
        "step": 14537
    },
    {
        "loss": 2.3271,
        "grad_norm": 3.344728708267212,
        "learning_rate": 4.306537369458497e-06,
        "epoch": 1.9384000000000001,
        "step": 14538
    },
    {
        "loss": 2.6715,
        "grad_norm": 5.126493453979492,
        "learning_rate": 4.288285564919181e-06,
        "epoch": 1.9385333333333334,
        "step": 14539
    },
    {
        "loss": 2.4423,
        "grad_norm": 3.528639078140259,
        "learning_rate": 4.270071672116127e-06,
        "epoch": 1.9386666666666668,
        "step": 14540
    },
    {
        "loss": 2.5391,
        "grad_norm": 3.3437185287475586,
        "learning_rate": 4.251895698263864e-06,
        "epoch": 1.9388,
        "step": 14541
    },
    {
        "loss": 2.0496,
        "grad_norm": 4.373692512512207,
        "learning_rate": 4.233757650561964e-06,
        "epoch": 1.9389333333333334,
        "step": 14542
    },
    {
        "loss": 2.3632,
        "grad_norm": 4.419582366943359,
        "learning_rate": 4.215657536194961e-06,
        "epoch": 1.9390666666666667,
        "step": 14543
    },
    {
        "loss": 1.9889,
        "grad_norm": 3.017953634262085,
        "learning_rate": 4.197595362332362e-06,
        "epoch": 1.9392,
        "step": 14544
    },
    {
        "loss": 2.4442,
        "grad_norm": 3.3610620498657227,
        "learning_rate": 4.179571136128724e-06,
        "epoch": 1.9393333333333334,
        "step": 14545
    },
    {
        "loss": 1.6791,
        "grad_norm": 3.579519271850586,
        "learning_rate": 4.161584864723466e-06,
        "epoch": 1.9394666666666667,
        "step": 14546
    },
    {
        "loss": 1.7144,
        "grad_norm": 5.136475563049316,
        "learning_rate": 4.143636555240993e-06,
        "epoch": 1.9396,
        "step": 14547
    },
    {
        "loss": 2.0478,
        "grad_norm": 2.8814830780029297,
        "learning_rate": 4.125726214790659e-06,
        "epoch": 1.9397333333333333,
        "step": 14548
    },
    {
        "loss": 1.5516,
        "grad_norm": 2.2819957733154297,
        "learning_rate": 4.1078538504669135e-06,
        "epoch": 1.9398666666666666,
        "step": 14549
    },
    {
        "loss": 2.7328,
        "grad_norm": 3.283799171447754,
        "learning_rate": 4.090019469348994e-06,
        "epoch": 1.94,
        "step": 14550
    },
    {
        "loss": 2.4117,
        "grad_norm": 3.8418056964874268,
        "learning_rate": 4.07222307850117e-06,
        "epoch": 1.9401333333333333,
        "step": 14551
    },
    {
        "loss": 2.3104,
        "grad_norm": 2.847820281982422,
        "learning_rate": 4.054464684972603e-06,
        "epoch": 1.9402666666666666,
        "step": 14552
    },
    {
        "loss": 0.9452,
        "grad_norm": 4.610513210296631,
        "learning_rate": 4.036744295797501e-06,
        "epoch": 1.9404,
        "step": 14553
    },
    {
        "loss": 2.6781,
        "grad_norm": 3.292957067489624,
        "learning_rate": 4.01906191799506e-06,
        "epoch": 1.9405333333333332,
        "step": 14554
    },
    {
        "loss": 1.2256,
        "grad_norm": 3.2593472003936768,
        "learning_rate": 4.0014175585691535e-06,
        "epoch": 1.9406666666666665,
        "step": 14555
    },
    {
        "loss": 1.3244,
        "grad_norm": 4.550666809082031,
        "learning_rate": 3.983811224508893e-06,
        "epoch": 1.9407999999999999,
        "step": 14556
    },
    {
        "loss": 1.8724,
        "grad_norm": 3.985769510269165,
        "learning_rate": 3.966242922788155e-06,
        "epoch": 1.9409333333333332,
        "step": 14557
    },
    {
        "loss": 1.855,
        "grad_norm": 3.7011380195617676,
        "learning_rate": 3.948712660365838e-06,
        "epoch": 1.9410666666666667,
        "step": 14558
    },
    {
        "loss": 1.8468,
        "grad_norm": 4.010499000549316,
        "learning_rate": 3.931220444185735e-06,
        "epoch": 1.9412,
        "step": 14559
    },
    {
        "loss": 2.1291,
        "grad_norm": 2.9819109439849854,
        "learning_rate": 3.913766281176556e-06,
        "epoch": 1.9413333333333334,
        "step": 14560
    },
    {
        "loss": 2.7763,
        "grad_norm": 1.8885211944580078,
        "learning_rate": 3.896350178251929e-06,
        "epoch": 1.9414666666666667,
        "step": 14561
    },
    {
        "loss": 2.7012,
        "grad_norm": 3.081437110900879,
        "learning_rate": 3.8789721423105e-06,
        "epoch": 1.9416,
        "step": 14562
    },
    {
        "loss": 2.292,
        "grad_norm": 4.127068519592285,
        "learning_rate": 3.861632180235719e-06,
        "epoch": 1.9417333333333333,
        "step": 14563
    },
    {
        "loss": 2.1603,
        "grad_norm": 2.215172052383423,
        "learning_rate": 3.8443302988959705e-06,
        "epoch": 1.9418666666666666,
        "step": 14564
    },
    {
        "loss": 2.0978,
        "grad_norm": 3.4701075553894043,
        "learning_rate": 3.8270665051446945e-06,
        "epoch": 1.942,
        "step": 14565
    },
    {
        "loss": 1.678,
        "grad_norm": 2.867790460586548,
        "learning_rate": 3.80984080582002e-06,
        "epoch": 1.9421333333333335,
        "step": 14566
    },
    {
        "loss": 3.0068,
        "grad_norm": 3.589303970336914,
        "learning_rate": 3.7926532077452004e-06,
        "epoch": 1.9422666666666668,
        "step": 14567
    },
    {
        "loss": 2.293,
        "grad_norm": 3.374927043914795,
        "learning_rate": 3.7755037177282793e-06,
        "epoch": 1.9424000000000001,
        "step": 14568
    },
    {
        "loss": 0.6953,
        "grad_norm": 3.2785212993621826,
        "learning_rate": 3.7583923425622e-06,
        "epoch": 1.9425333333333334,
        "step": 14569
    },
    {
        "loss": 1.4786,
        "grad_norm": 3.6535215377807617,
        "learning_rate": 3.7413190890248085e-06,
        "epoch": 1.9426666666666668,
        "step": 14570
    },
    {
        "loss": 2.233,
        "grad_norm": 3.0542476177215576,
        "learning_rate": 3.7242839638789716e-06,
        "epoch": 1.9428,
        "step": 14571
    },
    {
        "loss": 1.9256,
        "grad_norm": 4.123633861541748,
        "learning_rate": 3.7072869738722925e-06,
        "epoch": 1.9429333333333334,
        "step": 14572
    },
    {
        "loss": 1.5353,
        "grad_norm": 4.13153600692749,
        "learning_rate": 3.6903281257373192e-06,
        "epoch": 1.9430666666666667,
        "step": 14573
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.9727606773376465,
        "learning_rate": 3.6734074261915886e-06,
        "epoch": 1.9432,
        "step": 14574
    },
    {
        "loss": 1.1407,
        "grad_norm": 3.325352668762207,
        "learning_rate": 3.6565248819373843e-06,
        "epoch": 1.9433333333333334,
        "step": 14575
    },
    {
        "loss": 1.4661,
        "grad_norm": 4.47194242477417,
        "learning_rate": 3.639680499661946e-06,
        "epoch": 1.9434666666666667,
        "step": 14576
    },
    {
        "loss": 0.9517,
        "grad_norm": 3.120570182800293,
        "learning_rate": 3.6228742860374145e-06,
        "epoch": 1.9436,
        "step": 14577
    },
    {
        "loss": 1.7237,
        "grad_norm": 3.769305467605591,
        "learning_rate": 3.606106247720775e-06,
        "epoch": 1.9437333333333333,
        "step": 14578
    },
    {
        "loss": 2.0521,
        "grad_norm": 2.353187084197998,
        "learning_rate": 3.5893763913539267e-06,
        "epoch": 1.9438666666666666,
        "step": 14579
    },
    {
        "loss": 1.6663,
        "grad_norm": 4.916329860687256,
        "learning_rate": 3.572684723563546e-06,
        "epoch": 1.944,
        "step": 14580
    },
    {
        "loss": 2.2797,
        "grad_norm": 3.967900037765503,
        "learning_rate": 3.5560312509613557e-06,
        "epoch": 1.9441333333333333,
        "step": 14581
    },
    {
        "loss": 1.8386,
        "grad_norm": 5.430537700653076,
        "learning_rate": 3.5394159801437786e-06,
        "epoch": 1.9442666666666666,
        "step": 14582
    },
    {
        "loss": 2.2114,
        "grad_norm": 4.576219081878662,
        "learning_rate": 3.5228389176922394e-06,
        "epoch": 1.9444,
        "step": 14583
    },
    {
        "loss": 1.6727,
        "grad_norm": 3.577491044998169,
        "learning_rate": 3.506300070172963e-06,
        "epoch": 1.9445333333333332,
        "step": 14584
    },
    {
        "loss": 1.899,
        "grad_norm": 2.979318141937256,
        "learning_rate": 3.4897994441369654e-06,
        "epoch": 1.9446666666666665,
        "step": 14585
    },
    {
        "loss": 1.4627,
        "grad_norm": 5.311326026916504,
        "learning_rate": 3.4733370461202954e-06,
        "epoch": 1.9447999999999999,
        "step": 14586
    },
    {
        "loss": 2.42,
        "grad_norm": 2.842499017715454,
        "learning_rate": 3.456912882643748e-06,
        "epoch": 1.9449333333333332,
        "step": 14587
    },
    {
        "loss": 2.1144,
        "grad_norm": 5.078373908996582,
        "learning_rate": 3.440526960212953e-06,
        "epoch": 1.9450666666666667,
        "step": 14588
    },
    {
        "loss": 1.6422,
        "grad_norm": 3.5815987586975098,
        "learning_rate": 3.424179285318407e-06,
        "epoch": 1.9452,
        "step": 14589
    },
    {
        "loss": 2.0631,
        "grad_norm": 3.412099838256836,
        "learning_rate": 3.4078698644355756e-06,
        "epoch": 1.9453333333333334,
        "step": 14590
    },
    {
        "loss": 1.8288,
        "grad_norm": 3.84955096244812,
        "learning_rate": 3.391598704024568e-06,
        "epoch": 1.9454666666666667,
        "step": 14591
    },
    {
        "loss": 1.9314,
        "grad_norm": 3.5527772903442383,
        "learning_rate": 3.375365810530573e-06,
        "epoch": 1.9456,
        "step": 14592
    },
    {
        "loss": 1.9429,
        "grad_norm": 3.320866584777832,
        "learning_rate": 3.359171190383359e-06,
        "epoch": 1.9457333333333333,
        "step": 14593
    },
    {
        "loss": 1.6054,
        "grad_norm": 4.760989665985107,
        "learning_rate": 3.3430148499977277e-06,
        "epoch": 1.9458666666666666,
        "step": 14594
    },
    {
        "loss": 2.2119,
        "grad_norm": 4.535792827606201,
        "learning_rate": 3.3268967957732934e-06,
        "epoch": 1.946,
        "step": 14595
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.3503382205963135,
        "learning_rate": 3.3108170340944487e-06,
        "epoch": 1.9461333333333335,
        "step": 14596
    },
    {
        "loss": 1.7113,
        "grad_norm": 3.6261980533599854,
        "learning_rate": 3.294775571330433e-06,
        "epoch": 1.9462666666666668,
        "step": 14597
    },
    {
        "loss": 2.2366,
        "grad_norm": 2.721925735473633,
        "learning_rate": 3.278772413835307e-06,
        "epoch": 1.9464000000000001,
        "step": 14598
    },
    {
        "loss": 2.0128,
        "grad_norm": 4.516877174377441,
        "learning_rate": 3.262807567948023e-06,
        "epoch": 1.9465333333333334,
        "step": 14599
    },
    {
        "loss": 2.2646,
        "grad_norm": 2.7056663036346436,
        "learning_rate": 3.2468810399922776e-06,
        "epoch": 1.9466666666666668,
        "step": 14600
    },
    {
        "loss": 0.9714,
        "grad_norm": 4.139199256896973,
        "learning_rate": 3.2309928362766472e-06,
        "epoch": 1.9468,
        "step": 14601
    },
    {
        "loss": 1.8198,
        "grad_norm": 3.6739437580108643,
        "learning_rate": 3.2151429630944307e-06,
        "epoch": 1.9469333333333334,
        "step": 14602
    },
    {
        "loss": 2.4409,
        "grad_norm": 3.5087127685546875,
        "learning_rate": 3.199331426723884e-06,
        "epoch": 1.9470666666666667,
        "step": 14603
    },
    {
        "loss": 1.9953,
        "grad_norm": 3.3452117443084717,
        "learning_rate": 3.1835582334280412e-06,
        "epoch": 1.9472,
        "step": 14604
    },
    {
        "loss": 2.7094,
        "grad_norm": 3.669801712036133,
        "learning_rate": 3.1678233894546716e-06,
        "epoch": 1.9473333333333334,
        "step": 14605
    },
    {
        "loss": 2.3653,
        "grad_norm": 4.395083427429199,
        "learning_rate": 3.1521269010364007e-06,
        "epoch": 1.9474666666666667,
        "step": 14606
    },
    {
        "loss": 1.115,
        "grad_norm": 3.7280819416046143,
        "learning_rate": 3.136468774390633e-06,
        "epoch": 1.9476,
        "step": 14607
    },
    {
        "loss": 2.3677,
        "grad_norm": 2.844447612762451,
        "learning_rate": 3.1208490157196734e-06,
        "epoch": 1.9477333333333333,
        "step": 14608
    },
    {
        "loss": 1.7447,
        "grad_norm": 6.2346649169921875,
        "learning_rate": 3.105267631210529e-06,
        "epoch": 1.9478666666666666,
        "step": 14609
    },
    {
        "loss": 0.4243,
        "grad_norm": 2.7094192504882812,
        "learning_rate": 3.089724627035029e-06,
        "epoch": 1.948,
        "step": 14610
    },
    {
        "loss": 2.5212,
        "grad_norm": 4.035776138305664,
        "learning_rate": 3.074220009349782e-06,
        "epoch": 1.9481333333333333,
        "step": 14611
    },
    {
        "loss": 1.9581,
        "grad_norm": 4.0721964836120605,
        "learning_rate": 3.0587537842962645e-06,
        "epoch": 1.9482666666666666,
        "step": 14612
    },
    {
        "loss": 1.4626,
        "grad_norm": 3.647522449493408,
        "learning_rate": 3.0433259580007645e-06,
        "epoch": 1.9484,
        "step": 14613
    },
    {
        "loss": 2.4585,
        "grad_norm": 4.631605625152588,
        "learning_rate": 3.0279365365741387e-06,
        "epoch": 1.9485333333333332,
        "step": 14614
    },
    {
        "loss": 1.9112,
        "grad_norm": 3.628096103668213,
        "learning_rate": 3.0125855261123326e-06,
        "epoch": 1.9486666666666665,
        "step": 14615
    },
    {
        "loss": 1.2229,
        "grad_norm": 7.304853439331055,
        "learning_rate": 2.997272932695827e-06,
        "epoch": 1.9487999999999999,
        "step": 14616
    },
    {
        "loss": 2.3856,
        "grad_norm": 5.073333263397217,
        "learning_rate": 2.98199876239007e-06,
        "epoch": 1.9489333333333332,
        "step": 14617
    },
    {
        "loss": 2.4058,
        "grad_norm": 4.31501579284668,
        "learning_rate": 2.9667630212452e-06,
        "epoch": 1.9490666666666665,
        "step": 14618
    },
    {
        "loss": 1.9148,
        "grad_norm": 6.188543796539307,
        "learning_rate": 2.9515657152961118e-06,
        "epoch": 1.9492,
        "step": 14619
    },
    {
        "loss": 1.633,
        "grad_norm": 3.8984670639038086,
        "learning_rate": 2.936406850562501e-06,
        "epoch": 1.9493333333333334,
        "step": 14620
    },
    {
        "loss": 1.1923,
        "grad_norm": 3.8743534088134766,
        "learning_rate": 2.9212864330489197e-06,
        "epoch": 1.9494666666666667,
        "step": 14621
    },
    {
        "loss": 1.192,
        "grad_norm": 4.82674503326416,
        "learning_rate": 2.9062044687445554e-06,
        "epoch": 1.9496,
        "step": 14622
    },
    {
        "loss": 2.7448,
        "grad_norm": 4.078732490539551,
        "learning_rate": 2.8911609636234183e-06,
        "epoch": 1.9497333333333333,
        "step": 14623
    },
    {
        "loss": 2.5244,
        "grad_norm": 1.741258978843689,
        "learning_rate": 2.8761559236443413e-06,
        "epoch": 1.9498666666666666,
        "step": 14624
    },
    {
        "loss": 3.1354,
        "grad_norm": 3.592778444290161,
        "learning_rate": 2.861189354750804e-06,
        "epoch": 1.95,
        "step": 14625
    },
    {
        "loss": 2.0358,
        "grad_norm": 2.8500113487243652,
        "learning_rate": 2.846261262871197e-06,
        "epoch": 1.9501333333333335,
        "step": 14626
    },
    {
        "loss": 1.9841,
        "grad_norm": 6.000746250152588,
        "learning_rate": 2.8313716539185463e-06,
        "epoch": 1.9502666666666668,
        "step": 14627
    },
    {
        "loss": 2.0155,
        "grad_norm": 3.777099609375,
        "learning_rate": 2.8165205337906675e-06,
        "epoch": 1.9504000000000001,
        "step": 14628
    },
    {
        "loss": 2.451,
        "grad_norm": 3.468968629837036,
        "learning_rate": 2.8017079083701323e-06,
        "epoch": 1.9505333333333335,
        "step": 14629
    },
    {
        "loss": 2.3098,
        "grad_norm": 3.3063907623291016,
        "learning_rate": 2.786933783524315e-06,
        "epoch": 1.9506666666666668,
        "step": 14630
    },
    {
        "loss": 0.5571,
        "grad_norm": 2.655074119567871,
        "learning_rate": 2.7721981651052776e-06,
        "epoch": 1.9508,
        "step": 14631
    },
    {
        "loss": 1.6336,
        "grad_norm": 3.3512496948242188,
        "learning_rate": 2.7575010589497963e-06,
        "epoch": 1.9509333333333334,
        "step": 14632
    },
    {
        "loss": 2.3222,
        "grad_norm": 3.236496925354004,
        "learning_rate": 2.7428424708795255e-06,
        "epoch": 1.9510666666666667,
        "step": 14633
    },
    {
        "loss": 0.6873,
        "grad_norm": 3.4627954959869385,
        "learning_rate": 2.7282224067007645e-06,
        "epoch": 1.9512,
        "step": 14634
    },
    {
        "loss": 2.491,
        "grad_norm": 3.534433603286743,
        "learning_rate": 2.7136408722045036e-06,
        "epoch": 1.9513333333333334,
        "step": 14635
    },
    {
        "loss": 1.7296,
        "grad_norm": 3.822070598602295,
        "learning_rate": 2.6990978731666227e-06,
        "epoch": 1.9514666666666667,
        "step": 14636
    },
    {
        "loss": 2.2156,
        "grad_norm": 3.3037774562835693,
        "learning_rate": 2.684593415347636e-06,
        "epoch": 1.9516,
        "step": 14637
    },
    {
        "loss": 2.2901,
        "grad_norm": 3.0714967250823975,
        "learning_rate": 2.6701275044927698e-06,
        "epoch": 1.9517333333333333,
        "step": 14638
    },
    {
        "loss": 2.898,
        "grad_norm": 3.986666202545166,
        "learning_rate": 2.6557001463320184e-06,
        "epoch": 1.9518666666666666,
        "step": 14639
    },
    {
        "loss": 1.9866,
        "grad_norm": 4.2021565437316895,
        "learning_rate": 2.6413113465801666e-06,
        "epoch": 1.952,
        "step": 14640
    },
    {
        "loss": 2.4786,
        "grad_norm": 2.253554105758667,
        "learning_rate": 2.626961110936599e-06,
        "epoch": 1.9521333333333333,
        "step": 14641
    },
    {
        "loss": 2.3383,
        "grad_norm": 3.614872932434082,
        "learning_rate": 2.612649445085591e-06,
        "epoch": 1.9522666666666666,
        "step": 14642
    },
    {
        "loss": 1.7994,
        "grad_norm": 2.8464057445526123,
        "learning_rate": 2.5983763546959083e-06,
        "epoch": 1.9524,
        "step": 14643
    },
    {
        "loss": 2.3312,
        "grad_norm": 4.116457939147949,
        "learning_rate": 2.5841418454212397e-06,
        "epoch": 1.9525333333333332,
        "step": 14644
    },
    {
        "loss": 1.8748,
        "grad_norm": 4.003597259521484,
        "learning_rate": 2.5699459228999633e-06,
        "epoch": 1.9526666666666666,
        "step": 14645
    },
    {
        "loss": 1.6302,
        "grad_norm": 4.261649131774902,
        "learning_rate": 2.555788592755082e-06,
        "epoch": 1.9527999999999999,
        "step": 14646
    },
    {
        "loss": 2.1779,
        "grad_norm": 3.424318313598633,
        "learning_rate": 2.541669860594376e-06,
        "epoch": 1.9529333333333332,
        "step": 14647
    },
    {
        "loss": 2.7361,
        "grad_norm": 4.750567436218262,
        "learning_rate": 2.5275897320102938e-06,
        "epoch": 1.9530666666666665,
        "step": 14648
    },
    {
        "loss": 1.7127,
        "grad_norm": 3.936579704284668,
        "learning_rate": 2.5135482125800635e-06,
        "epoch": 1.9532,
        "step": 14649
    },
    {
        "loss": 2.3049,
        "grad_norm": 5.674672603607178,
        "learning_rate": 2.4995453078655475e-06,
        "epoch": 1.9533333333333334,
        "step": 14650
    },
    {
        "loss": 2.5347,
        "grad_norm": 3.756100654602051,
        "learning_rate": 2.4855810234134193e-06,
        "epoch": 1.9534666666666667,
        "step": 14651
    },
    {
        "loss": 1.8811,
        "grad_norm": 4.101235866546631,
        "learning_rate": 2.4716553647548434e-06,
        "epoch": 1.9536,
        "step": 14652
    },
    {
        "loss": 2.9084,
        "grad_norm": 2.950791597366333,
        "learning_rate": 2.457768337405908e-06,
        "epoch": 1.9537333333333333,
        "step": 14653
    },
    {
        "loss": 1.3908,
        "grad_norm": 6.724797248840332,
        "learning_rate": 2.443919946867346e-06,
        "epoch": 1.9538666666666666,
        "step": 14654
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.635054111480713,
        "learning_rate": 2.43011019862448e-06,
        "epoch": 1.954,
        "step": 14655
    },
    {
        "loss": 2.0605,
        "grad_norm": 3.2506861686706543,
        "learning_rate": 2.4163390981474466e-06,
        "epoch": 1.9541333333333335,
        "step": 14656
    },
    {
        "loss": 1.7967,
        "grad_norm": 3.1708781719207764,
        "learning_rate": 2.4026066508909705e-06,
        "epoch": 1.9542666666666668,
        "step": 14657
    },
    {
        "loss": 1.8166,
        "grad_norm": 4.5371856689453125,
        "learning_rate": 2.388912862294568e-06,
        "epoch": 1.9544000000000001,
        "step": 14658
    },
    {
        "loss": 1.8087,
        "grad_norm": 3.2154645919799805,
        "learning_rate": 2.3752577377824105e-06,
        "epoch": 1.9545333333333335,
        "step": 14659
    },
    {
        "loss": 2.4165,
        "grad_norm": 3.1992053985595703,
        "learning_rate": 2.3616412827632826e-06,
        "epoch": 1.9546666666666668,
        "step": 14660
    },
    {
        "loss": 1.4514,
        "grad_norm": 2.718040704727173,
        "learning_rate": 2.348063502630726e-06,
        "epoch": 1.9548,
        "step": 14661
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.166060209274292,
        "learning_rate": 2.3345244027629607e-06,
        "epoch": 1.9549333333333334,
        "step": 14662
    },
    {
        "loss": 1.8655,
        "grad_norm": 3.6915862560272217,
        "learning_rate": 2.321023988522908e-06,
        "epoch": 1.9550666666666667,
        "step": 14663
    },
    {
        "loss": 2.2459,
        "grad_norm": 3.6509010791778564,
        "learning_rate": 2.307562265258034e-06,
        "epoch": 1.9552,
        "step": 14664
    },
    {
        "loss": 1.8768,
        "grad_norm": 4.111057758331299,
        "learning_rate": 2.2941392383006633e-06,
        "epoch": 1.9553333333333334,
        "step": 14665
    },
    {
        "loss": 1.8962,
        "grad_norm": 3.575244188308716,
        "learning_rate": 2.2807549129676197e-06,
        "epoch": 1.9554666666666667,
        "step": 14666
    },
    {
        "loss": 1.9574,
        "grad_norm": 2.051177740097046,
        "learning_rate": 2.267409294560563e-06,
        "epoch": 1.9556,
        "step": 14667
    },
    {
        "loss": 1.755,
        "grad_norm": 4.115375518798828,
        "learning_rate": 2.2541023883657085e-06,
        "epoch": 1.9557333333333333,
        "step": 14668
    },
    {
        "loss": 2.3287,
        "grad_norm": 3.1304993629455566,
        "learning_rate": 2.2408341996539516e-06,
        "epoch": 1.9558666666666666,
        "step": 14669
    },
    {
        "loss": 2.3453,
        "grad_norm": 4.375278472900391,
        "learning_rate": 2.2276047336808546e-06,
        "epoch": 1.956,
        "step": 14670
    },
    {
        "loss": 1.6679,
        "grad_norm": 4.091690540313721,
        "learning_rate": 2.2144139956866595e-06,
        "epoch": 1.9561333333333333,
        "step": 14671
    },
    {
        "loss": 2.3018,
        "grad_norm": 3.3190183639526367,
        "learning_rate": 2.2012619908963307e-06,
        "epoch": 1.9562666666666666,
        "step": 14672
    },
    {
        "loss": 1.756,
        "grad_norm": 2.2034099102020264,
        "learning_rate": 2.1881487245193345e-06,
        "epoch": 1.9564,
        "step": 14673
    },
    {
        "loss": 2.775,
        "grad_norm": 2.4529993534088135,
        "learning_rate": 2.175074201749916e-06,
        "epoch": 1.9565333333333332,
        "step": 14674
    },
    {
        "loss": 2.2738,
        "grad_norm": 3.7403199672698975,
        "learning_rate": 2.1620384277669214e-06,
        "epoch": 1.9566666666666666,
        "step": 14675
    },
    {
        "loss": 1.3347,
        "grad_norm": 2.027329683303833,
        "learning_rate": 2.149041407733909e-06,
        "epoch": 1.9567999999999999,
        "step": 14676
    },
    {
        "loss": 1.9854,
        "grad_norm": 3.1712002754211426,
        "learning_rate": 2.136083146799017e-06,
        "epoch": 1.9569333333333332,
        "step": 14677
    },
    {
        "loss": 2.3792,
        "grad_norm": 4.760099411010742,
        "learning_rate": 2.12316365009505e-06,
        "epoch": 1.9570666666666665,
        "step": 14678
    },
    {
        "loss": 2.495,
        "grad_norm": 3.9283435344696045,
        "learning_rate": 2.1102829227394373e-06,
        "epoch": 1.9572,
        "step": 14679
    },
    {
        "loss": 2.5209,
        "grad_norm": 4.088087558746338,
        "learning_rate": 2.0974409698343413e-06,
        "epoch": 1.9573333333333334,
        "step": 14680
    },
    {
        "loss": 2.405,
        "grad_norm": 6.226070404052734,
        "learning_rate": 2.084637796466471e-06,
        "epoch": 1.9574666666666667,
        "step": 14681
    },
    {
        "loss": 2.724,
        "grad_norm": 3.6561596393585205,
        "learning_rate": 2.0718734077071813e-06,
        "epoch": 1.9576,
        "step": 14682
    },
    {
        "loss": 1.2395,
        "grad_norm": 5.960134029388428,
        "learning_rate": 2.0591478086125383e-06,
        "epoch": 1.9577333333333333,
        "step": 14683
    },
    {
        "loss": 1.9829,
        "grad_norm": 4.05760383605957,
        "learning_rate": 2.046461004223155e-06,
        "epoch": 1.9578666666666666,
        "step": 14684
    },
    {
        "loss": 1.4281,
        "grad_norm": 6.394917964935303,
        "learning_rate": 2.033812999564355e-06,
        "epoch": 1.958,
        "step": 14685
    },
    {
        "loss": 2.1753,
        "grad_norm": 4.646021366119385,
        "learning_rate": 2.021203799646043e-06,
        "epoch": 1.9581333333333333,
        "step": 14686
    },
    {
        "loss": 1.1928,
        "grad_norm": 4.745598793029785,
        "learning_rate": 2.008633409462768e-06,
        "epoch": 1.9582666666666668,
        "step": 14687
    },
    {
        "loss": 2.4026,
        "grad_norm": 5.599135398864746,
        "learning_rate": 1.9961018339936578e-06,
        "epoch": 1.9584000000000001,
        "step": 14688
    },
    {
        "loss": 2.2024,
        "grad_norm": 3.6152188777923584,
        "learning_rate": 1.983609078202575e-06,
        "epoch": 1.9585333333333335,
        "step": 14689
    },
    {
        "loss": 1.7259,
        "grad_norm": 3.459139108657837,
        "learning_rate": 1.9711551470379288e-06,
        "epoch": 1.9586666666666668,
        "step": 14690
    },
    {
        "loss": 2.3644,
        "grad_norm": 3.2443273067474365,
        "learning_rate": 1.9587400454327274e-06,
        "epoch": 1.9588,
        "step": 14691
    },
    {
        "loss": 1.625,
        "grad_norm": 3.6245975494384766,
        "learning_rate": 1.946363778304683e-06,
        "epoch": 1.9589333333333334,
        "step": 14692
    },
    {
        "loss": 2.1779,
        "grad_norm": 2.7367796897888184,
        "learning_rate": 1.934026350556062e-06,
        "epoch": 1.9590666666666667,
        "step": 14693
    },
    {
        "loss": 2.2879,
        "grad_norm": 3.512800693511963,
        "learning_rate": 1.921727767073722e-06,
        "epoch": 1.9592,
        "step": 14694
    },
    {
        "loss": 2.4303,
        "grad_norm": 4.682027339935303,
        "learning_rate": 1.9094680327292337e-06,
        "epoch": 1.9593333333333334,
        "step": 14695
    },
    {
        "loss": 2.6562,
        "grad_norm": 3.068965196609497,
        "learning_rate": 1.8972471523787006e-06,
        "epoch": 1.9594666666666667,
        "step": 14696
    },
    {
        "loss": 2.487,
        "grad_norm": 3.5290679931640625,
        "learning_rate": 1.8850651308628287e-06,
        "epoch": 1.9596,
        "step": 14697
    },
    {
        "loss": 2.4745,
        "grad_norm": 5.043728828430176,
        "learning_rate": 1.872921973006969e-06,
        "epoch": 1.9597333333333333,
        "step": 14698
    },
    {
        "loss": 1.5113,
        "grad_norm": 4.6740007400512695,
        "learning_rate": 1.8608176836210966e-06,
        "epoch": 1.9598666666666666,
        "step": 14699
    },
    {
        "loss": 2.3378,
        "grad_norm": 3.7038066387176514,
        "learning_rate": 1.8487522674997094e-06,
        "epoch": 1.96,
        "step": 14700
    },
    {
        "loss": 2.2873,
        "grad_norm": 4.584629058837891,
        "learning_rate": 1.8367257294220619e-06,
        "epoch": 1.9601333333333333,
        "step": 14701
    },
    {
        "loss": 1.611,
        "grad_norm": 3.5282351970672607,
        "learning_rate": 1.8247380741517662e-06,
        "epoch": 1.9602666666666666,
        "step": 14702
    },
    {
        "loss": 2.0508,
        "grad_norm": 4.318721294403076,
        "learning_rate": 1.8127893064372458e-06,
        "epoch": 1.9604,
        "step": 14703
    },
    {
        "loss": 1.4059,
        "grad_norm": 5.076302528381348,
        "learning_rate": 1.8008794310114707e-06,
        "epoch": 1.9605333333333332,
        "step": 14704
    },
    {
        "loss": 2.7278,
        "grad_norm": 2.161990165710449,
        "learning_rate": 1.789008452591967e-06,
        "epoch": 1.9606666666666666,
        "step": 14705
    },
    {
        "loss": 1.5547,
        "grad_norm": 4.080681800842285,
        "learning_rate": 1.7771763758808512e-06,
        "epoch": 1.9607999999999999,
        "step": 14706
    },
    {
        "loss": 2.1253,
        "grad_norm": 3.8977437019348145,
        "learning_rate": 1.7653832055648412e-06,
        "epoch": 1.9609333333333332,
        "step": 14707
    },
    {
        "loss": 2.3109,
        "grad_norm": 3.1818747520446777,
        "learning_rate": 1.7536289463152887e-06,
        "epoch": 1.9610666666666665,
        "step": 14708
    },
    {
        "loss": 2.1309,
        "grad_norm": 4.093587875366211,
        "learning_rate": 1.741913602788059e-06,
        "epoch": 1.9612,
        "step": 14709
    },
    {
        "loss": 2.5323,
        "grad_norm": 2.40842342376709,
        "learning_rate": 1.730237179623695e-06,
        "epoch": 1.9613333333333334,
        "step": 14710
    },
    {
        "loss": 2.2366,
        "grad_norm": 3.227630853652954,
        "learning_rate": 1.7185996814471861e-06,
        "epoch": 1.9614666666666667,
        "step": 14711
    },
    {
        "loss": 1.7944,
        "grad_norm": 4.019619941711426,
        "learning_rate": 1.707001112868234e-06,
        "epoch": 1.9616,
        "step": 14712
    },
    {
        "loss": 1.9911,
        "grad_norm": 3.171834945678711,
        "learning_rate": 1.6954414784810857e-06,
        "epoch": 1.9617333333333333,
        "step": 14713
    },
    {
        "loss": 1.9268,
        "grad_norm": 3.538118362426758,
        "learning_rate": 1.6839207828645342e-06,
        "epoch": 1.9618666666666666,
        "step": 14714
    },
    {
        "loss": 1.7769,
        "grad_norm": 4.8413496017456055,
        "learning_rate": 1.6724390305819737e-06,
        "epoch": 1.962,
        "step": 14715
    },
    {
        "loss": 1.8333,
        "grad_norm": 5.159220218658447,
        "learning_rate": 1.6609962261813217e-06,
        "epoch": 1.9621333333333333,
        "step": 14716
    },
    {
        "loss": 1.9566,
        "grad_norm": 3.287635564804077,
        "learning_rate": 1.6495923741951747e-06,
        "epoch": 1.9622666666666668,
        "step": 14717
    },
    {
        "loss": 2.043,
        "grad_norm": 3.0545108318328857,
        "learning_rate": 1.6382274791406082e-06,
        "epoch": 1.9624000000000001,
        "step": 14718
    },
    {
        "loss": 2.5706,
        "grad_norm": 4.113823890686035,
        "learning_rate": 1.6269015455192882e-06,
        "epoch": 1.9625333333333335,
        "step": 14719
    },
    {
        "loss": 2.2588,
        "grad_norm": 2.309311866760254,
        "learning_rate": 1.615614577817448e-06,
        "epoch": 1.9626666666666668,
        "step": 14720
    },
    {
        "loss": 1.3829,
        "grad_norm": 4.6547017097473145,
        "learning_rate": 1.6043665805059115e-06,
        "epoch": 1.9628,
        "step": 14721
    },
    {
        "loss": 3.0174,
        "grad_norm": 3.2689504623413086,
        "learning_rate": 1.5931575580400927e-06,
        "epoch": 1.9629333333333334,
        "step": 14722
    },
    {
        "loss": 0.8562,
        "grad_norm": 3.8313450813293457,
        "learning_rate": 1.5819875148598285e-06,
        "epoch": 1.9630666666666667,
        "step": 14723
    },
    {
        "loss": 2.2546,
        "grad_norm": 4.764315128326416,
        "learning_rate": 1.5708564553896798e-06,
        "epoch": 1.9632,
        "step": 14724
    },
    {
        "loss": 2.1352,
        "grad_norm": 3.5758321285247803,
        "learning_rate": 1.5597643840386644e-06,
        "epoch": 1.9633333333333334,
        "step": 14725
    },
    {
        "loss": 2.4208,
        "grad_norm": 2.6341145038604736,
        "learning_rate": 1.5487113052004232e-06,
        "epoch": 1.9634666666666667,
        "step": 14726
    },
    {
        "loss": 1.9307,
        "grad_norm": 4.793696880340576,
        "learning_rate": 1.5376972232530984e-06,
        "epoch": 1.9636,
        "step": 14727
    },
    {
        "loss": 1.438,
        "grad_norm": 2.9592814445495605,
        "learning_rate": 1.5267221425594115e-06,
        "epoch": 1.9637333333333333,
        "step": 14728
    },
    {
        "loss": 2.2949,
        "grad_norm": 3.7440996170043945,
        "learning_rate": 1.5157860674665959e-06,
        "epoch": 1.9638666666666666,
        "step": 14729
    },
    {
        "loss": 2.637,
        "grad_norm": 3.153599262237549,
        "learning_rate": 1.5048890023064865e-06,
        "epoch": 1.964,
        "step": 14730
    },
    {
        "loss": 1.8864,
        "grad_norm": 4.592294692993164,
        "learning_rate": 1.4940309513955197e-06,
        "epoch": 1.9641333333333333,
        "step": 14731
    },
    {
        "loss": 0.9672,
        "grad_norm": 2.6914680004119873,
        "learning_rate": 1.4832119190344884e-06,
        "epoch": 1.9642666666666666,
        "step": 14732
    },
    {
        "loss": 0.8469,
        "grad_norm": 2.210134744644165,
        "learning_rate": 1.4724319095089312e-06,
        "epoch": 1.9644,
        "step": 14733
    },
    {
        "loss": 2.6524,
        "grad_norm": 2.8099281787872314,
        "learning_rate": 1.461690927088788e-06,
        "epoch": 1.9645333333333332,
        "step": 14734
    },
    {
        "loss": 1.3008,
        "grad_norm": 4.16333532333374,
        "learning_rate": 1.4509889760286555e-06,
        "epoch": 1.9646666666666666,
        "step": 14735
    },
    {
        "loss": 2.197,
        "grad_norm": 2.3010518550872803,
        "learning_rate": 1.4403260605675873e-06,
        "epoch": 1.9647999999999999,
        "step": 14736
    },
    {
        "loss": 2.5528,
        "grad_norm": 3.29704213142395,
        "learning_rate": 1.4297021849292048e-06,
        "epoch": 1.9649333333333332,
        "step": 14737
    },
    {
        "loss": 1.5676,
        "grad_norm": 4.461983680725098,
        "learning_rate": 1.4191173533216307e-06,
        "epoch": 1.9650666666666665,
        "step": 14738
    },
    {
        "loss": 2.1315,
        "grad_norm": 3.383258581161499,
        "learning_rate": 1.408571569937611e-06,
        "epoch": 1.9651999999999998,
        "step": 14739
    },
    {
        "loss": 2.4209,
        "grad_norm": 4.968255043029785,
        "learning_rate": 1.3980648389543272e-06,
        "epoch": 1.9653333333333334,
        "step": 14740
    },
    {
        "loss": 2.1106,
        "grad_norm": 4.018863677978516,
        "learning_rate": 1.3875971645335051e-06,
        "epoch": 1.9654666666666667,
        "step": 14741
    },
    {
        "loss": 1.3282,
        "grad_norm": 4.547243595123291,
        "learning_rate": 1.3771685508214949e-06,
        "epoch": 1.9656,
        "step": 14742
    },
    {
        "loss": 0.966,
        "grad_norm": 4.015307903289795,
        "learning_rate": 1.36677900194907e-06,
        "epoch": 1.9657333333333333,
        "step": 14743
    },
    {
        "loss": 2.1982,
        "grad_norm": 4.165349960327148,
        "learning_rate": 1.3564285220315386e-06,
        "epoch": 1.9658666666666667,
        "step": 14744
    },
    {
        "loss": 1.3334,
        "grad_norm": 3.60992169380188,
        "learning_rate": 1.3461171151688211e-06,
        "epoch": 1.966,
        "step": 14745
    },
    {
        "loss": 1.7646,
        "grad_norm": 4.862215995788574,
        "learning_rate": 1.3358447854452616e-06,
        "epoch": 1.9661333333333333,
        "step": 14746
    },
    {
        "loss": 1.936,
        "grad_norm": 3.756622076034546,
        "learning_rate": 1.3256115369297495e-06,
        "epoch": 1.9662666666666668,
        "step": 14747
    },
    {
        "loss": 2.1298,
        "grad_norm": 3.551820993423462,
        "learning_rate": 1.3154173736757647e-06,
        "epoch": 1.9664000000000001,
        "step": 14748
    },
    {
        "loss": 1.5603,
        "grad_norm": 3.205160140991211,
        "learning_rate": 1.3052622997211994e-06,
        "epoch": 1.9665333333333335,
        "step": 14749
    },
    {
        "loss": 2.3825,
        "grad_norm": 3.2193706035614014,
        "learning_rate": 1.295146319088525e-06,
        "epoch": 1.9666666666666668,
        "step": 14750
    },
    {
        "loss": 2.2264,
        "grad_norm": 2.566565752029419,
        "learning_rate": 1.285069435784736e-06,
        "epoch": 1.9668,
        "step": 14751
    },
    {
        "loss": 2.0091,
        "grad_norm": 3.465789794921875,
        "learning_rate": 1.2750316538013063e-06,
        "epoch": 1.9669333333333334,
        "step": 14752
    },
    {
        "loss": 2.2117,
        "grad_norm": 3.8090360164642334,
        "learning_rate": 1.2650329771142222e-06,
        "epoch": 1.9670666666666667,
        "step": 14753
    },
    {
        "loss": 1.4845,
        "grad_norm": 4.826535224914551,
        "learning_rate": 1.2550734096840266e-06,
        "epoch": 1.9672,
        "step": 14754
    },
    {
        "loss": 2.0804,
        "grad_norm": 4.8535075187683105,
        "learning_rate": 1.2451529554557085e-06,
        "epoch": 1.9673333333333334,
        "step": 14755
    },
    {
        "loss": 2.2524,
        "grad_norm": 4.219186782836914,
        "learning_rate": 1.2352716183588131e-06,
        "epoch": 1.9674666666666667,
        "step": 14756
    },
    {
        "loss": 2.3797,
        "grad_norm": 4.43595027923584,
        "learning_rate": 1.2254294023073432e-06,
        "epoch": 1.9676,
        "step": 14757
    },
    {
        "loss": 2.4534,
        "grad_norm": 3.525876522064209,
        "learning_rate": 1.215626311199869e-06,
        "epoch": 1.9677333333333333,
        "step": 14758
    },
    {
        "loss": 2.0224,
        "grad_norm": 3.024592161178589,
        "learning_rate": 1.2058623489193954e-06,
        "epoch": 1.9678666666666667,
        "step": 14759
    },
    {
        "loss": 2.1154,
        "grad_norm": 3.700510025024414,
        "learning_rate": 1.1961375193335066e-06,
        "epoch": 1.968,
        "step": 14760
    },
    {
        "loss": 2.2342,
        "grad_norm": 3.519395589828491,
        "learning_rate": 1.186451826294177e-06,
        "epoch": 1.9681333333333333,
        "step": 14761
    },
    {
        "loss": 1.8467,
        "grad_norm": 3.757171630859375,
        "learning_rate": 1.1768052736379931e-06,
        "epoch": 1.9682666666666666,
        "step": 14762
    },
    {
        "loss": 2.7262,
        "grad_norm": 3.5484020709991455,
        "learning_rate": 1.167197865185976e-06,
        "epoch": 1.9684,
        "step": 14763
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.962838888168335,
        "learning_rate": 1.1576296047436597e-06,
        "epoch": 1.9685333333333332,
        "step": 14764
    },
    {
        "loss": 2.0905,
        "grad_norm": 3.708163261413574,
        "learning_rate": 1.1481004961010455e-06,
        "epoch": 1.9686666666666666,
        "step": 14765
    },
    {
        "loss": 2.2457,
        "grad_norm": 4.259690284729004,
        "learning_rate": 1.1386105430326366e-06,
        "epoch": 1.9687999999999999,
        "step": 14766
    },
    {
        "loss": 2.4539,
        "grad_norm": 4.479750633239746,
        "learning_rate": 1.1291597492974703e-06,
        "epoch": 1.9689333333333332,
        "step": 14767
    },
    {
        "loss": 1.6789,
        "grad_norm": 4.120036602020264,
        "learning_rate": 1.119748118639008e-06,
        "epoch": 1.9690666666666665,
        "step": 14768
    },
    {
        "loss": 2.4167,
        "grad_norm": 3.311262845993042,
        "learning_rate": 1.1103756547852783e-06,
        "epoch": 1.9691999999999998,
        "step": 14769
    },
    {
        "loss": 2.3331,
        "grad_norm": 3.4290664196014404,
        "learning_rate": 1.1010423614486676e-06,
        "epoch": 1.9693333333333334,
        "step": 14770
    },
    {
        "loss": 2.5394,
        "grad_norm": 2.656510353088379,
        "learning_rate": 1.0917482423261737e-06,
        "epoch": 1.9694666666666667,
        "step": 14771
    },
    {
        "loss": 1.657,
        "grad_norm": 6.567721366882324,
        "learning_rate": 1.082493301099241e-06,
        "epoch": 1.9696,
        "step": 14772
    },
    {
        "loss": 0.6327,
        "grad_norm": 2.9916579723358154,
        "learning_rate": 1.0732775414337593e-06,
        "epoch": 1.9697333333333333,
        "step": 14773
    },
    {
        "loss": 2.6395,
        "grad_norm": 3.6807072162628174,
        "learning_rate": 1.0641009669801304e-06,
        "epoch": 1.9698666666666667,
        "step": 14774
    },
    {
        "loss": 0.6074,
        "grad_norm": 3.473257064819336,
        "learning_rate": 1.054963581373203e-06,
        "epoch": 1.97,
        "step": 14775
    },
    {
        "loss": 2.3084,
        "grad_norm": 2.2600018978118896,
        "learning_rate": 1.045865388232381e-06,
        "epoch": 1.9701333333333333,
        "step": 14776
    },
    {
        "loss": 2.731,
        "grad_norm": 2.4736225605010986,
        "learning_rate": 1.0368063911614379e-06,
        "epoch": 1.9702666666666668,
        "step": 14777
    },
    {
        "loss": 2.3511,
        "grad_norm": 2.550092935562134,
        "learning_rate": 1.0277865937487031e-06,
        "epoch": 1.9704000000000002,
        "step": 14778
    },
    {
        "loss": 2.001,
        "grad_norm": 2.6438653469085693,
        "learning_rate": 1.0188059995669187e-06,
        "epoch": 1.9705333333333335,
        "step": 14779
    },
    {
        "loss": 2.0755,
        "grad_norm": 4.891885757446289,
        "learning_rate": 1.009864612173339e-06,
        "epoch": 1.9706666666666668,
        "step": 14780
    },
    {
        "loss": 2.1258,
        "grad_norm": 3.767305612564087,
        "learning_rate": 1.0009624351097425e-06,
        "epoch": 1.9708,
        "step": 14781
    },
    {
        "loss": 1.1512,
        "grad_norm": 4.225345134735107,
        "learning_rate": 9.920994719022081e-07,
        "epoch": 1.9709333333333334,
        "step": 14782
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.4732015132904053,
        "learning_rate": 9.8327572606145e-07,
        "epoch": 1.9710666666666667,
        "step": 14783
    },
    {
        "loss": 0.8888,
        "grad_norm": 3.771012544631958,
        "learning_rate": 9.744912010825502e-07,
        "epoch": 1.9712,
        "step": 14784
    },
    {
        "loss": 1.5356,
        "grad_norm": 3.5110275745391846,
        "learning_rate": 9.657459004451253e-07,
        "epoch": 1.9713333333333334,
        "step": 14785
    },
    {
        "loss": 1.9933,
        "grad_norm": 2.502892255783081,
        "learning_rate": 9.570398276132043e-07,
        "epoch": 1.9714666666666667,
        "step": 14786
    },
    {
        "loss": 2.0049,
        "grad_norm": 2.6192657947540283,
        "learning_rate": 9.483729860352842e-07,
        "epoch": 1.9716,
        "step": 14787
    },
    {
        "loss": 2.5081,
        "grad_norm": 4.325199127197266,
        "learning_rate": 9.397453791443189e-07,
        "epoch": 1.9717333333333333,
        "step": 14788
    },
    {
        "loss": 2.4129,
        "grad_norm": 2.4336395263671875,
        "learning_rate": 9.311570103577416e-07,
        "epoch": 1.9718666666666667,
        "step": 14789
    },
    {
        "loss": 1.8182,
        "grad_norm": 4.009832859039307,
        "learning_rate": 9.226078830774864e-07,
        "epoch": 1.972,
        "step": 14790
    },
    {
        "loss": 1.2122,
        "grad_norm": 4.971655368804932,
        "learning_rate": 9.140980006898115e-07,
        "epoch": 1.9721333333333333,
        "step": 14791
    },
    {
        "loss": 0.6837,
        "grad_norm": 4.6702799797058105,
        "learning_rate": 9.056273665655646e-07,
        "epoch": 1.9722666666666666,
        "step": 14792
    },
    {
        "loss": 1.7722,
        "grad_norm": 3.671757936477661,
        "learning_rate": 8.971959840599398e-07,
        "epoch": 1.9724,
        "step": 14793
    },
    {
        "loss": 2.6111,
        "grad_norm": 3.2463219165802,
        "learning_rate": 8.888038565126989e-07,
        "epoch": 1.9725333333333332,
        "step": 14794
    },
    {
        "loss": 2.9315,
        "grad_norm": 4.43921422958374,
        "learning_rate": 8.804509872479716e-07,
        "epoch": 1.9726666666666666,
        "step": 14795
    },
    {
        "loss": 2.3894,
        "grad_norm": 2.4690802097320557,
        "learning_rate": 8.721373795743337e-07,
        "epoch": 1.9727999999999999,
        "step": 14796
    },
    {
        "loss": 2.6003,
        "grad_norm": 2.7316789627075195,
        "learning_rate": 8.638630367848399e-07,
        "epoch": 1.9729333333333332,
        "step": 14797
    },
    {
        "loss": 1.7945,
        "grad_norm": 4.286285400390625,
        "learning_rate": 8.55627962157024e-07,
        "epoch": 1.9730666666666665,
        "step": 14798
    },
    {
        "loss": 1.5719,
        "grad_norm": 2.054398775100708,
        "learning_rate": 8.474321589527768e-07,
        "epoch": 1.9731999999999998,
        "step": 14799
    },
    {
        "loss": 0.5238,
        "grad_norm": 3.1278467178344727,
        "learning_rate": 8.392756304185013e-07,
        "epoch": 1.9733333333333334,
        "step": 14800
    },
    {
        "loss": 2.2699,
        "grad_norm": 4.143907070159912,
        "learning_rate": 8.311583797850464e-07,
        "epoch": 1.9734666666666667,
        "step": 14801
    },
    {
        "loss": 2.6097,
        "grad_norm": 3.7659151554107666,
        "learning_rate": 8.230804102676626e-07,
        "epoch": 1.9736,
        "step": 14802
    },
    {
        "loss": 2.9883,
        "grad_norm": 4.461316108703613,
        "learning_rate": 8.150417250660569e-07,
        "epoch": 1.9737333333333333,
        "step": 14803
    },
    {
        "loss": 2.4777,
        "grad_norm": 2.3616294860839844,
        "learning_rate": 8.070423273643934e-07,
        "epoch": 1.9738666666666667,
        "step": 14804
    },
    {
        "loss": 1.6163,
        "grad_norm": 5.083498001098633,
        "learning_rate": 7.990822203312597e-07,
        "epoch": 1.974,
        "step": 14805
    },
    {
        "loss": 2.145,
        "grad_norm": 2.839414596557617,
        "learning_rate": 7.911614071196671e-07,
        "epoch": 1.9741333333333333,
        "step": 14806
    },
    {
        "loss": 2.6921,
        "grad_norm": 4.024412155151367,
        "learning_rate": 7.832798908671057e-07,
        "epoch": 1.9742666666666666,
        "step": 14807
    },
    {
        "loss": 2.4486,
        "grad_norm": 4.0170979499816895,
        "learning_rate": 7.754376746954451e-07,
        "epoch": 1.9744000000000002,
        "step": 14808
    },
    {
        "loss": 1.7878,
        "grad_norm": 4.082075595855713,
        "learning_rate": 7.676347617110003e-07,
        "epoch": 1.9745333333333335,
        "step": 14809
    },
    {
        "loss": 1.2875,
        "grad_norm": 3.1675145626068115,
        "learning_rate": 7.598711550045767e-07,
        "epoch": 1.9746666666666668,
        "step": 14810
    },
    {
        "loss": 2.1233,
        "grad_norm": 4.419548988342285,
        "learning_rate": 7.521468576513369e-07,
        "epoch": 1.9748,
        "step": 14811
    },
    {
        "loss": 2.0872,
        "grad_norm": 3.076321840286255,
        "learning_rate": 7.444618727108887e-07,
        "epoch": 1.9749333333333334,
        "step": 14812
    },
    {
        "loss": 2.8454,
        "grad_norm": 2.44034481048584,
        "learning_rate": 7.368162032273196e-07,
        "epoch": 1.9750666666666667,
        "step": 14813
    },
    {
        "loss": 1.2849,
        "grad_norm": 3.692894220352173,
        "learning_rate": 7.292098522290847e-07,
        "epoch": 1.9752,
        "step": 14814
    },
    {
        "loss": 2.3776,
        "grad_norm": 4.461585998535156,
        "learning_rate": 7.216428227290739e-07,
        "epoch": 1.9753333333333334,
        "step": 14815
    },
    {
        "loss": 1.9936,
        "grad_norm": 4.220776557922363,
        "learning_rate": 7.14115117724612e-07,
        "epoch": 1.9754666666666667,
        "step": 14816
    },
    {
        "loss": 2.5854,
        "grad_norm": 3.181781530380249,
        "learning_rate": 7.066267401974802e-07,
        "epoch": 1.9756,
        "step": 14817
    },
    {
        "loss": 1.7771,
        "grad_norm": 3.8622994422912598,
        "learning_rate": 6.991776931138061e-07,
        "epoch": 1.9757333333333333,
        "step": 14818
    },
    {
        "loss": 2.7712,
        "grad_norm": 5.0261125564575195,
        "learning_rate": 6.917679794242516e-07,
        "epoch": 1.9758666666666667,
        "step": 14819
    },
    {
        "loss": 2.2772,
        "grad_norm": 2.6061043739318848,
        "learning_rate": 6.843976020637466e-07,
        "epoch": 1.976,
        "step": 14820
    },
    {
        "loss": 2.4888,
        "grad_norm": 2.3795711994171143,
        "learning_rate": 6.77066563951756e-07,
        "epoch": 1.9761333333333333,
        "step": 14821
    },
    {
        "loss": 2.4516,
        "grad_norm": 4.943751811981201,
        "learning_rate": 6.697748679921567e-07,
        "epoch": 1.9762666666666666,
        "step": 14822
    },
    {
        "loss": 2.0632,
        "grad_norm": 3.691908359527588,
        "learning_rate": 6.625225170731831e-07,
        "epoch": 1.9764,
        "step": 14823
    },
    {
        "loss": 0.5174,
        "grad_norm": 2.863664150238037,
        "learning_rate": 6.553095140675369e-07,
        "epoch": 1.9765333333333333,
        "step": 14824
    },
    {
        "loss": 1.3236,
        "grad_norm": 3.9860308170318604,
        "learning_rate": 6.481358618322775e-07,
        "epoch": 1.9766666666666666,
        "step": 14825
    },
    {
        "loss": 2.5838,
        "grad_norm": 4.155876159667969,
        "learning_rate": 6.410015632089539e-07,
        "epoch": 1.9768,
        "step": 14826
    },
    {
        "loss": 1.0311,
        "grad_norm": 4.890911102294922,
        "learning_rate": 6.339066210234501e-07,
        "epoch": 1.9769333333333332,
        "step": 14827
    },
    {
        "loss": 2.1642,
        "grad_norm": 4.891534805297852,
        "learning_rate": 6.268510380861514e-07,
        "epoch": 1.9770666666666665,
        "step": 14828
    },
    {
        "loss": 1.0571,
        "grad_norm": 3.741295576095581,
        "learning_rate": 6.198348171917334e-07,
        "epoch": 1.9771999999999998,
        "step": 14829
    },
    {
        "loss": 1.7988,
        "grad_norm": 4.415388584136963,
        "learning_rate": 6.128579611193619e-07,
        "epoch": 1.9773333333333334,
        "step": 14830
    },
    {
        "loss": 1.6002,
        "grad_norm": 4.228199481964111,
        "learning_rate": 6.059204726326373e-07,
        "epoch": 1.9774666666666667,
        "step": 14831
    },
    {
        "loss": 2.2781,
        "grad_norm": 2.6349058151245117,
        "learning_rate": 5.990223544794726e-07,
        "epoch": 1.9776,
        "step": 14832
    },
    {
        "loss": 2.0392,
        "grad_norm": 2.929379463195801,
        "learning_rate": 5.921636093922711e-07,
        "epoch": 1.9777333333333333,
        "step": 14833
    },
    {
        "loss": 2.3183,
        "grad_norm": 1.6705939769744873,
        "learning_rate": 5.853442400877596e-07,
        "epoch": 1.9778666666666667,
        "step": 14834
    },
    {
        "loss": 2.7966,
        "grad_norm": 4.150497913360596,
        "learning_rate": 5.78564249267155e-07,
        "epoch": 1.978,
        "step": 14835
    },
    {
        "loss": 2.5575,
        "grad_norm": 3.9992804527282715,
        "learning_rate": 5.718236396160315e-07,
        "epoch": 1.9781333333333333,
        "step": 14836
    },
    {
        "loss": 2.1018,
        "grad_norm": 3.4442217350006104,
        "learning_rate": 5.651224138043532e-07,
        "epoch": 1.9782666666666666,
        "step": 14837
    },
    {
        "loss": 2.0458,
        "grad_norm": 2.9170548915863037,
        "learning_rate": 5.584605744864857e-07,
        "epoch": 1.9784000000000002,
        "step": 14838
    },
    {
        "loss": 1.5657,
        "grad_norm": 3.7935118675231934,
        "learning_rate": 5.518381243012293e-07,
        "epoch": 1.9785333333333335,
        "step": 14839
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.3101139068603516,
        "learning_rate": 5.452550658717858e-07,
        "epoch": 1.9786666666666668,
        "step": 14840
    },
    {
        "loss": 2.7386,
        "grad_norm": 6.032747745513916,
        "learning_rate": 5.387114018056582e-07,
        "epoch": 1.9788000000000001,
        "step": 14841
    },
    {
        "loss": 0.6558,
        "grad_norm": 3.1988637447357178,
        "learning_rate": 5.32207134694862e-07,
        "epoch": 1.9789333333333334,
        "step": 14842
    },
    {
        "loss": 2.9623,
        "grad_norm": 4.233367919921875,
        "learning_rate": 5.257422671157364e-07,
        "epoch": 1.9790666666666668,
        "step": 14843
    },
    {
        "loss": 0.4843,
        "grad_norm": 2.509190559387207,
        "learning_rate": 5.193168016290662e-07,
        "epoch": 1.9792,
        "step": 14844
    },
    {
        "loss": 2.6039,
        "grad_norm": 2.319720983505249,
        "learning_rate": 5.129307407799821e-07,
        "epoch": 1.9793333333333334,
        "step": 14845
    },
    {
        "loss": 1.0604,
        "grad_norm": 3.5110321044921875,
        "learning_rate": 5.065840870980276e-07,
        "epoch": 1.9794666666666667,
        "step": 14846
    },
    {
        "loss": 2.3998,
        "grad_norm": 3.4753670692443848,
        "learning_rate": 5.002768430971139e-07,
        "epoch": 1.9796,
        "step": 14847
    },
    {
        "loss": 2.4856,
        "grad_norm": 4.429169654846191,
        "learning_rate": 4.940090112755757e-07,
        "epoch": 1.9797333333333333,
        "step": 14848
    },
    {
        "loss": 2.1926,
        "grad_norm": 4.252570152282715,
        "learning_rate": 4.877805941161606e-07,
        "epoch": 1.9798666666666667,
        "step": 14849
    },
    {
        "loss": 2.6419,
        "grad_norm": 3.5741195678710938,
        "learning_rate": 4.815915940859062e-07,
        "epoch": 1.98,
        "step": 14850
    },
    {
        "loss": 2.2089,
        "grad_norm": 2.859269380569458,
        "learning_rate": 4.7544201363632913e-07,
        "epoch": 1.9801333333333333,
        "step": 14851
    },
    {
        "loss": 0.8131,
        "grad_norm": 4.066157341003418,
        "learning_rate": 4.6933185520328107e-07,
        "epoch": 1.9802666666666666,
        "step": 14852
    },
    {
        "loss": 1.5669,
        "grad_norm": 4.567944526672363,
        "learning_rate": 4.63261121207037e-07,
        "epoch": 1.9804,
        "step": 14853
    },
    {
        "loss": 2.5858,
        "grad_norm": 4.028976917266846,
        "learning_rate": 4.572298140522291e-07,
        "epoch": 1.9805333333333333,
        "step": 14854
    },
    {
        "loss": 1.4816,
        "grad_norm": 5.605715751647949,
        "learning_rate": 4.5123793612787957e-07,
        "epoch": 1.9806666666666666,
        "step": 14855
    },
    {
        "loss": 0.6294,
        "grad_norm": 4.834537506103516,
        "learning_rate": 4.452854898073788e-07,
        "epoch": 1.9808,
        "step": 14856
    },
    {
        "loss": 2.1072,
        "grad_norm": 3.9089741706848145,
        "learning_rate": 4.393724774485186e-07,
        "epoch": 1.9809333333333332,
        "step": 14857
    },
    {
        "loss": 1.6473,
        "grad_norm": 5.908636569976807,
        "learning_rate": 4.3349890139346983e-07,
        "epoch": 1.9810666666666665,
        "step": 14858
    },
    {
        "loss": 1.506,
        "grad_norm": 4.449748992919922,
        "learning_rate": 4.2766476396876034e-07,
        "epoch": 1.9811999999999999,
        "step": 14859
    },
    {
        "loss": 2.0781,
        "grad_norm": 2.954040288925171,
        "learning_rate": 4.2187006748533043e-07,
        "epoch": 1.9813333333333332,
        "step": 14860
    },
    {
        "loss": 1.4316,
        "grad_norm": 3.5296497344970703,
        "learning_rate": 4.1611481423847745e-07,
        "epoch": 1.9814666666666667,
        "step": 14861
    },
    {
        "loss": 1.4652,
        "grad_norm": 4.564733982086182,
        "learning_rate": 4.1039900650785556e-07,
        "epoch": 1.9816,
        "step": 14862
    },
    {
        "loss": 2.4134,
        "grad_norm": 4.802661418914795,
        "learning_rate": 4.0472264655754266e-07,
        "epoch": 1.9817333333333333,
        "step": 14863
    },
    {
        "loss": 1.9373,
        "grad_norm": 4.842861175537109,
        "learning_rate": 3.990857366359513e-07,
        "epoch": 1.9818666666666667,
        "step": 14864
    },
    {
        "loss": 3.0213,
        "grad_norm": 3.4108169078826904,
        "learning_rate": 3.9348827897587313e-07,
        "epoch": 1.982,
        "step": 14865
    },
    {
        "loss": 2.5734,
        "grad_norm": 2.918741226196289,
        "learning_rate": 3.879302757944903e-07,
        "epoch": 1.9821333333333333,
        "step": 14866
    },
    {
        "loss": 0.9096,
        "grad_norm": 2.5521600246429443,
        "learning_rate": 3.824117292933638e-07,
        "epoch": 1.9822666666666666,
        "step": 14867
    },
    {
        "loss": 1.9206,
        "grad_norm": 3.6199936866760254,
        "learning_rate": 3.769326416583563e-07,
        "epoch": 1.9824000000000002,
        "step": 14868
    },
    {
        "loss": 3.0166,
        "grad_norm": 5.0746917724609375,
        "learning_rate": 3.714930150598095e-07,
        "epoch": 1.9825333333333335,
        "step": 14869
    },
    {
        "loss": 2.0251,
        "grad_norm": 4.202572345733643,
        "learning_rate": 3.6609285165235543e-07,
        "epoch": 1.9826666666666668,
        "step": 14870
    },
    {
        "loss": 2.2755,
        "grad_norm": 4.785517692565918,
        "learning_rate": 3.6073215357500525e-07,
        "epoch": 1.9828000000000001,
        "step": 14871
    },
    {
        "loss": 1.8546,
        "grad_norm": 3.9198033809661865,
        "learning_rate": 3.554109229511715e-07,
        "epoch": 1.9829333333333334,
        "step": 14872
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.67901349067688,
        "learning_rate": 3.5012916188860157e-07,
        "epoch": 1.9830666666666668,
        "step": 14873
    },
    {
        "loss": 1.1046,
        "grad_norm": 3.6941988468170166,
        "learning_rate": 3.4488687247942186e-07,
        "epoch": 1.9832,
        "step": 14874
    },
    {
        "loss": 1.5247,
        "grad_norm": 5.4235968589782715,
        "learning_rate": 3.3968405680010476e-07,
        "epoch": 1.9833333333333334,
        "step": 14875
    },
    {
        "loss": 1.9812,
        "grad_norm": 3.9474730491638184,
        "learning_rate": 3.34520716911535e-07,
        "epoch": 1.9834666666666667,
        "step": 14876
    },
    {
        "loss": 2.455,
        "grad_norm": 3.4003686904907227,
        "learning_rate": 3.293968548588877e-07,
        "epoch": 1.9836,
        "step": 14877
    },
    {
        "loss": 2.2811,
        "grad_norm": 2.3940443992614746,
        "learning_rate": 3.24312472671795e-07,
        "epoch": 1.9837333333333333,
        "step": 14878
    },
    {
        "loss": 2.6963,
        "grad_norm": 3.0607943534851074,
        "learning_rate": 3.192675723641458e-07,
        "epoch": 1.9838666666666667,
        "step": 14879
    },
    {
        "loss": 1.7282,
        "grad_norm": 2.8628551959991455,
        "learning_rate": 3.14262155934264e-07,
        "epoch": 1.984,
        "step": 14880
    },
    {
        "loss": 2.2904,
        "grad_norm": 3.5285422801971436,
        "learning_rate": 3.0929622536481906e-07,
        "epoch": 1.9841333333333333,
        "step": 14881
    },
    {
        "loss": 1.982,
        "grad_norm": 3.3795559406280518,
        "learning_rate": 3.0436978262283754e-07,
        "epoch": 1.9842666666666666,
        "step": 14882
    },
    {
        "loss": 1.9321,
        "grad_norm": 5.202685356140137,
        "learning_rate": 2.9948282965968077e-07,
        "epoch": 1.9844,
        "step": 14883
    },
    {
        "loss": 1.2094,
        "grad_norm": 4.4403557777404785,
        "learning_rate": 2.946353684110892e-07,
        "epoch": 1.9845333333333333,
        "step": 14884
    },
    {
        "loss": 2.2302,
        "grad_norm": 3.414569139480591,
        "learning_rate": 2.898274007971824e-07,
        "epoch": 1.9846666666666666,
        "step": 14885
    },
    {
        "loss": 1.9259,
        "grad_norm": 4.9060235023498535,
        "learning_rate": 2.850589287223815e-07,
        "epoch": 1.9848,
        "step": 14886
    },
    {
        "loss": 2.3178,
        "grad_norm": 2.925290584564209,
        "learning_rate": 2.8032995407552e-07,
        "epoch": 1.9849333333333332,
        "step": 14887
    },
    {
        "loss": 2.6403,
        "grad_norm": 4.524522304534912,
        "learning_rate": 2.7564047872974396e-07,
        "epoch": 1.9850666666666665,
        "step": 14888
    },
    {
        "loss": 1.5442,
        "grad_norm": 3.0241730213165283,
        "learning_rate": 2.709905045425676e-07,
        "epoch": 1.9851999999999999,
        "step": 14889
    },
    {
        "loss": 2.0472,
        "grad_norm": 4.18385124206543,
        "learning_rate": 2.663800333558841e-07,
        "epoch": 1.9853333333333332,
        "step": 14890
    },
    {
        "loss": 1.6703,
        "grad_norm": 5.1900410652160645,
        "learning_rate": 2.6180906699589945e-07,
        "epoch": 1.9854666666666667,
        "step": 14891
    },
    {
        "loss": 0.6179,
        "grad_norm": 2.487034797668457,
        "learning_rate": 2.572776072731986e-07,
        "epoch": 1.9856,
        "step": 14892
    },
    {
        "loss": 0.861,
        "grad_norm": 4.251213073730469,
        "learning_rate": 2.5278565598269024e-07,
        "epoch": 1.9857333333333334,
        "step": 14893
    },
    {
        "loss": 2.3617,
        "grad_norm": 2.6681909561157227,
        "learning_rate": 2.483332149036732e-07,
        "epoch": 1.9858666666666667,
        "step": 14894
    },
    {
        "loss": 2.4818,
        "grad_norm": 2.937467575073242,
        "learning_rate": 2.439202857997702e-07,
        "epoch": 1.986,
        "step": 14895
    },
    {
        "loss": 1.2791,
        "grad_norm": 2.7132391929626465,
        "learning_rate": 2.3954687041893855e-07,
        "epoch": 1.9861333333333333,
        "step": 14896
    },
    {
        "loss": 1.0711,
        "grad_norm": 4.252165794372559,
        "learning_rate": 2.3521297049352576e-07,
        "epoch": 1.9862666666666666,
        "step": 14897
    },
    {
        "loss": 2.6522,
        "grad_norm": 4.449016094207764,
        "learning_rate": 2.3091858774018095e-07,
        "epoch": 1.9864000000000002,
        "step": 14898
    },
    {
        "loss": 2.2276,
        "grad_norm": 4.556036472320557,
        "learning_rate": 2.266637238599656e-07,
        "epoch": 1.9865333333333335,
        "step": 14899
    },
    {
        "loss": 2.4037,
        "grad_norm": 5.3193840980529785,
        "learning_rate": 2.2244838053819829e-07,
        "epoch": 1.9866666666666668,
        "step": 14900
    },
    {
        "loss": 2.2263,
        "grad_norm": 3.3980860710144043,
        "learning_rate": 2.182725594446211e-07,
        "epoch": 1.9868000000000001,
        "step": 14901
    },
    {
        "loss": 1.936,
        "grad_norm": 2.734159231185913,
        "learning_rate": 2.1413626223326655e-07,
        "epoch": 1.9869333333333334,
        "step": 14902
    },
    {
        "loss": 2.0432,
        "grad_norm": 4.521378993988037,
        "learning_rate": 2.1003949054256843e-07,
        "epoch": 1.9870666666666668,
        "step": 14903
    },
    {
        "loss": 3.1116,
        "grad_norm": 4.614744663238525,
        "learning_rate": 2.0598224599523985e-07,
        "epoch": 1.9872,
        "step": 14904
    },
    {
        "loss": 1.4913,
        "grad_norm": 3.4799349308013916,
        "learning_rate": 2.0196453019839522e-07,
        "epoch": 1.9873333333333334,
        "step": 14905
    },
    {
        "loss": 2.1898,
        "grad_norm": 3.1501166820526123,
        "learning_rate": 1.9798634474343935e-07,
        "epoch": 1.9874666666666667,
        "step": 14906
    },
    {
        "loss": 2.2858,
        "grad_norm": 5.129089832305908,
        "learning_rate": 1.9404769120616727e-07,
        "epoch": 1.9876,
        "step": 14907
    },
    {
        "loss": 0.9785,
        "grad_norm": 5.505057334899902,
        "learning_rate": 1.901485711466977e-07,
        "epoch": 1.9877333333333334,
        "step": 14908
    },
    {
        "loss": 1.4821,
        "grad_norm": 5.707417011260986,
        "learning_rate": 1.8628898610945078e-07,
        "epoch": 1.9878666666666667,
        "step": 14909
    },
    {
        "loss": 2.5498,
        "grad_norm": 3.2576403617858887,
        "learning_rate": 1.8246893762325913e-07,
        "epoch": 1.988,
        "step": 14910
    },
    {
        "loss": 2.4773,
        "grad_norm": 3.0396811962127686,
        "learning_rate": 1.7868842720122348e-07,
        "epoch": 1.9881333333333333,
        "step": 14911
    },
    {
        "loss": 2.6865,
        "grad_norm": 2.6301674842834473,
        "learning_rate": 1.7494745634085707e-07,
        "epoch": 1.9882666666666666,
        "step": 14912
    },
    {
        "loss": 1.924,
        "grad_norm": 6.594913482666016,
        "learning_rate": 1.712460265239413e-07,
        "epoch": 1.9884,
        "step": 14913
    },
    {
        "loss": 2.5105,
        "grad_norm": 2.5402140617370605,
        "learning_rate": 1.6758413921662553e-07,
        "epoch": 1.9885333333333333,
        "step": 14914
    },
    {
        "loss": 1.8742,
        "grad_norm": 4.33076810836792,
        "learning_rate": 1.6396179586940507e-07,
        "epoch": 1.9886666666666666,
        "step": 14915
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.8573570251464844,
        "learning_rate": 1.6037899791709888e-07,
        "epoch": 1.9888,
        "step": 14916
    },
    {
        "loss": 2.8299,
        "grad_norm": 4.319589614868164,
        "learning_rate": 1.568357467788717e-07,
        "epoch": 1.9889333333333332,
        "step": 14917
    },
    {
        "loss": 2.0335,
        "grad_norm": 4.379882335662842,
        "learning_rate": 1.53332043858212e-07,
        "epoch": 1.9890666666666665,
        "step": 14918
    },
    {
        "loss": 1.2042,
        "grad_norm": 5.47740364074707,
        "learning_rate": 1.4986789054295402e-07,
        "epoch": 1.9891999999999999,
        "step": 14919
    },
    {
        "loss": 1.9904,
        "grad_norm": 2.7908220291137695,
        "learning_rate": 1.4644328820524465e-07,
        "epoch": 1.9893333333333332,
        "step": 14920
    },
    {
        "loss": 2.2207,
        "grad_norm": 3.0663888454437256,
        "learning_rate": 1.430582382015877e-07,
        "epoch": 1.9894666666666667,
        "step": 14921
    },
    {
        "loss": 1.4023,
        "grad_norm": 4.779697895050049,
        "learning_rate": 1.397127418728328e-07,
        "epoch": 1.9896,
        "step": 14922
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.5098326206207275,
        "learning_rate": 1.364068005441199e-07,
        "epoch": 1.9897333333333334,
        "step": 14923
    },
    {
        "loss": 2.6353,
        "grad_norm": 2.3081703186035156,
        "learning_rate": 1.3314041552494606e-07,
        "epoch": 1.9898666666666667,
        "step": 14924
    },
    {
        "loss": 2.0149,
        "grad_norm": 3.46692156791687,
        "learning_rate": 1.299135881091429e-07,
        "epoch": 1.99,
        "step": 14925
    },
    {
        "loss": 2.5241,
        "grad_norm": 4.683887958526611,
        "learning_rate": 1.2672631957486581e-07,
        "epoch": 1.9901333333333333,
        "step": 14926
    },
    {
        "loss": 2.1928,
        "grad_norm": 3.65856671333313,
        "learning_rate": 1.235786111846049e-07,
        "epoch": 1.9902666666666666,
        "step": 14927
    },
    {
        "loss": 1.4633,
        "grad_norm": 4.853777885437012,
        "learning_rate": 1.2047046418518505e-07,
        "epoch": 1.9904,
        "step": 14928
    },
    {
        "loss": 1.9932,
        "grad_norm": 3.465160608291626,
        "learning_rate": 1.1740187980773254e-07,
        "epoch": 1.9905333333333335,
        "step": 14929
    },
    {
        "loss": 4.2065,
        "grad_norm": 5.265364170074463,
        "learning_rate": 1.1437285926774177e-07,
        "epoch": 1.9906666666666668,
        "step": 14930
    },
    {
        "loss": 2.2037,
        "grad_norm": 4.431689739227295,
        "learning_rate": 1.1138340376501966e-07,
        "epoch": 1.9908000000000001,
        "step": 14931
    },
    {
        "loss": 2.4947,
        "grad_norm": 3.7673704624176025,
        "learning_rate": 1.0843351448368566e-07,
        "epoch": 1.9909333333333334,
        "step": 14932
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.478592872619629,
        "learning_rate": 1.0552319259221621e-07,
        "epoch": 1.9910666666666668,
        "step": 14933
    },
    {
        "loss": 2.2204,
        "grad_norm": 3.809837818145752,
        "learning_rate": 1.0265243924337809e-07,
        "epoch": 1.9912,
        "step": 14934
    },
    {
        "loss": 2.765,
        "grad_norm": 4.085405349731445,
        "learning_rate": 9.98212555743061e-08,
        "epoch": 1.9913333333333334,
        "step": 14935
    },
    {
        "loss": 2.557,
        "grad_norm": 4.509561538696289,
        "learning_rate": 9.702964270643656e-08,
        "epoch": 1.9914666666666667,
        "step": 14936
    },
    {
        "loss": 1.309,
        "grad_norm": 4.4279069900512695,
        "learning_rate": 9.427760174555156e-08,
        "epoch": 1.9916,
        "step": 14937
    },
    {
        "loss": 1.1954,
        "grad_norm": 5.0840163230896,
        "learning_rate": 9.15651337817125e-08,
        "epoch": 1.9917333333333334,
        "step": 14938
    },
    {
        "loss": 2.4997,
        "grad_norm": 3.0607287883758545,
        "learning_rate": 8.88922398893488e-08,
        "epoch": 1.9918666666666667,
        "step": 14939
    },
    {
        "loss": 2.2426,
        "grad_norm": 4.603029251098633,
        "learning_rate": 8.625892112722466e-08,
        "epoch": 1.992,
        "step": 14940
    },
    {
        "loss": 1.1951,
        "grad_norm": 4.719459056854248,
        "learning_rate": 8.366517853838351e-08,
        "epoch": 1.9921333333333333,
        "step": 14941
    },
    {
        "loss": 1.7468,
        "grad_norm": 3.138129949569702,
        "learning_rate": 8.111101315022574e-08,
        "epoch": 1.9922666666666666,
        "step": 14942
    },
    {
        "loss": 1.7746,
        "grad_norm": 4.886938571929932,
        "learning_rate": 7.859642597444206e-08,
        "epoch": 1.9924,
        "step": 14943
    },
    {
        "loss": 1.1832,
        "grad_norm": 3.941844940185547,
        "learning_rate": 7.612141800710238e-08,
        "epoch": 1.9925333333333333,
        "step": 14944
    },
    {
        "loss": 1.9863,
        "grad_norm": 4.840452671051025,
        "learning_rate": 7.368599022855582e-08,
        "epoch": 1.9926666666666666,
        "step": 14945
    },
    {
        "loss": 1.886,
        "grad_norm": 3.8823626041412354,
        "learning_rate": 7.129014360347519e-08,
        "epoch": 1.9928,
        "step": 14946
    },
    {
        "loss": 2.6427,
        "grad_norm": 2.596449851989746,
        "learning_rate": 6.893387908085692e-08,
        "epoch": 1.9929333333333332,
        "step": 14947
    },
    {
        "loss": 1.7156,
        "grad_norm": 5.499551773071289,
        "learning_rate": 6.661719759404328e-08,
        "epoch": 1.9930666666666665,
        "step": 14948
    },
    {
        "loss": 1.5619,
        "grad_norm": 3.8728511333465576,
        "learning_rate": 6.434010006067804e-08,
        "epoch": 1.9931999999999999,
        "step": 14949
    },
    {
        "loss": 1.2789,
        "grad_norm": 3.9909026622772217,
        "learning_rate": 6.210258738271746e-08,
        "epoch": 1.9933333333333332,
        "step": 14950
    },
    {
        "loss": 2.6676,
        "grad_norm": 2.7500967979431152,
        "learning_rate": 5.99046604464526e-08,
        "epoch": 1.9934666666666667,
        "step": 14951
    },
    {
        "loss": 2.4253,
        "grad_norm": 6.539921283721924,
        "learning_rate": 5.774632012248704e-08,
        "epoch": 1.9936,
        "step": 14952
    },
    {
        "loss": 2.3757,
        "grad_norm": 3.360055446624756,
        "learning_rate": 5.562756726574803e-08,
        "epoch": 1.9937333333333334,
        "step": 14953
    },
    {
        "loss": 2.295,
        "grad_norm": 3.600841760635376,
        "learning_rate": 5.354840271548645e-08,
        "epoch": 1.9938666666666667,
        "step": 14954
    },
    {
        "loss": 2.4033,
        "grad_norm": 2.775923252105713,
        "learning_rate": 5.1508827295265735e-08,
        "epoch": 1.994,
        "step": 14955
    },
    {
        "loss": 2.0078,
        "grad_norm": 2.9840872287750244,
        "learning_rate": 4.950884181295079e-08,
        "epoch": 1.9941333333333333,
        "step": 14956
    },
    {
        "loss": 3.1793,
        "grad_norm": 4.937582015991211,
        "learning_rate": 4.754844706076345e-08,
        "epoch": 1.9942666666666666,
        "step": 14957
    },
    {
        "loss": 2.5136,
        "grad_norm": 3.5996315479278564,
        "learning_rate": 4.5627643815215894e-08,
        "epoch": 1.9944,
        "step": 14958
    },
    {
        "loss": 1.5842,
        "grad_norm": 3.0482959747314453,
        "learning_rate": 4.374643283714397e-08,
        "epoch": 1.9945333333333335,
        "step": 14959
    },
    {
        "loss": 2.0581,
        "grad_norm": 4.010248184204102,
        "learning_rate": 4.1904814871707167e-08,
        "epoch": 1.9946666666666668,
        "step": 14960
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.369335412979126,
        "learning_rate": 4.0102790648366416e-08,
        "epoch": 1.9948000000000001,
        "step": 14961
    },
    {
        "loss": 1.421,
        "grad_norm": 3.224498987197876,
        "learning_rate": 3.8340360880917415e-08,
        "epoch": 1.9949333333333334,
        "step": 14962
    },
    {
        "loss": 1.8702,
        "grad_norm": 4.432568073272705,
        "learning_rate": 3.661752626745729e-08,
        "epoch": 1.9950666666666668,
        "step": 14963
    },
    {
        "loss": 2.9613,
        "grad_norm": 3.9912006855010986,
        "learning_rate": 3.493428749041794e-08,
        "epoch": 1.9952,
        "step": 14964
    },
    {
        "loss": 1.2012,
        "grad_norm": 3.687788963317871,
        "learning_rate": 3.329064521653269e-08,
        "epoch": 1.9953333333333334,
        "step": 14965
    },
    {
        "loss": 2.4612,
        "grad_norm": 3.7515342235565186,
        "learning_rate": 3.168660009684743e-08,
        "epoch": 1.9954666666666667,
        "step": 14966
    },
    {
        "loss": 1.738,
        "grad_norm": 3.297438383102417,
        "learning_rate": 3.0122152766731695e-08,
        "epoch": 1.9956,
        "step": 14967
    },
    {
        "loss": 1.5681,
        "grad_norm": 4.665016174316406,
        "learning_rate": 2.859730384586756e-08,
        "epoch": 1.9957333333333334,
        "step": 14968
    },
    {
        "loss": 2.2275,
        "grad_norm": 3.6280574798583984,
        "learning_rate": 2.7112053938260773e-08,
        "epoch": 1.9958666666666667,
        "step": 14969
    },
    {
        "loss": 1.9961,
        "grad_norm": 4.263491153717041,
        "learning_rate": 2.5666403632218505e-08,
        "epoch": 1.996,
        "step": 14970
    },
    {
        "loss": 1.8263,
        "grad_norm": 3.193390369415283,
        "learning_rate": 2.426035350037159e-08,
        "epoch": 1.9961333333333333,
        "step": 14971
    },
    {
        "loss": 2.1418,
        "grad_norm": 6.988658905029297,
        "learning_rate": 2.2893904099652307e-08,
        "epoch": 1.9962666666666666,
        "step": 14972
    },
    {
        "loss": 1.5542,
        "grad_norm": 2.9038336277008057,
        "learning_rate": 2.156705597132769e-08,
        "epoch": 1.9964,
        "step": 14973
    },
    {
        "loss": 2.1384,
        "grad_norm": 3.683574676513672,
        "learning_rate": 2.0279809640944003e-08,
        "epoch": 1.9965333333333333,
        "step": 14974
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.991393566131592,
        "learning_rate": 1.9032165618415586e-08,
        "epoch": 1.9966666666666666,
        "step": 14975
    },
    {
        "loss": 2.6251,
        "grad_norm": 3.7246437072753906,
        "learning_rate": 1.7824124397924913e-08,
        "epoch": 1.9968,
        "step": 14976
    },
    {
        "loss": 1.8644,
        "grad_norm": 3.9941773414611816,
        "learning_rate": 1.6655686457967e-08,
        "epoch": 1.9969333333333332,
        "step": 14977
    },
    {
        "loss": 2.5401,
        "grad_norm": 3.44286847114563,
        "learning_rate": 1.552685226138273e-08,
        "epoch": 1.9970666666666665,
        "step": 14978
    },
    {
        "loss": 2.4386,
        "grad_norm": 3.720183849334717,
        "learning_rate": 1.4437622255303317e-08,
        "epoch": 1.9971999999999999,
        "step": 14979
    },
    {
        "loss": 1.9203,
        "grad_norm": 3.150761842727661,
        "learning_rate": 1.3387996871172537e-08,
        "epoch": 1.9973333333333332,
        "step": 14980
    },
    {
        "loss": 2.1023,
        "grad_norm": 3.3069164752960205,
        "learning_rate": 1.2377976524746704e-08,
        "epoch": 1.9974666666666665,
        "step": 14981
    },
    {
        "loss": 2.5783,
        "grad_norm": 3.1589183807373047,
        "learning_rate": 1.140756161611689e-08,
        "epoch": 1.9976,
        "step": 14982
    },
    {
        "loss": 2.1605,
        "grad_norm": 3.607606887817383,
        "learning_rate": 1.0476752529642309e-08,
        "epoch": 1.9977333333333334,
        "step": 14983
    },
    {
        "loss": 2.7605,
        "grad_norm": 3.799769639968872,
        "learning_rate": 9.585549634039127e-09,
        "epoch": 1.9978666666666667,
        "step": 14984
    },
    {
        "loss": 2.3014,
        "grad_norm": 2.229438543319702,
        "learning_rate": 8.733953282302754e-09,
        "epoch": 1.998,
        "step": 14985
    },
    {
        "loss": 0.6662,
        "grad_norm": 2.960428476333618,
        "learning_rate": 7.921963811763355e-09,
        "epoch": 1.9981333333333333,
        "step": 14986
    },
    {
        "loss": 2.0717,
        "grad_norm": 3.831102132797241,
        "learning_rate": 7.149581544052542e-09,
        "epoch": 1.9982666666666666,
        "step": 14987
    },
    {
        "loss": 1.7086,
        "grad_norm": 3.884018659591675,
        "learning_rate": 6.416806785103369e-09,
        "epoch": 1.9984,
        "step": 14988
    },
    {
        "loss": 2.5199,
        "grad_norm": 3.318621873855591,
        "learning_rate": 5.723639825172544e-09,
        "epoch": 1.9985333333333335,
        "step": 14989
    },
    {
        "loss": 1.5963,
        "grad_norm": 2.5223171710968018,
        "learning_rate": 5.070080938840427e-09,
        "epoch": 1.9986666666666668,
        "step": 14990
    },
    {
        "loss": 1.9516,
        "grad_norm": 3.71429181098938,
        "learning_rate": 4.456130384966617e-09,
        "epoch": 1.9988000000000001,
        "step": 14991
    },
    {
        "loss": 2.1947,
        "grad_norm": 4.183810234069824,
        "learning_rate": 3.88178840675657e-09,
        "epoch": 1.9989333333333335,
        "step": 14992
    },
    {
        "loss": 1.9713,
        "grad_norm": 2.981825351715088,
        "learning_rate": 3.347055231683882e-09,
        "epoch": 1.9990666666666668,
        "step": 14993
    },
    {
        "loss": 1.6536,
        "grad_norm": 3.225067615509033,
        "learning_rate": 2.8519310715791057e-09,
        "epoch": 1.9992,
        "step": 14994
    },
    {
        "loss": 2.2052,
        "grad_norm": 3.914828300476074,
        "learning_rate": 2.3964161225631386e-09,
        "epoch": 1.9993333333333334,
        "step": 14995
    },
    {
        "loss": 1.6036,
        "grad_norm": 5.297516822814941,
        "learning_rate": 1.9805105650472222e-09,
        "epoch": 1.9994666666666667,
        "step": 14996
    },
    {
        "loss": 2.4831,
        "grad_norm": 3.0681726932525635,
        "learning_rate": 1.6042145637995553e-09,
        "epoch": 1.9996,
        "step": 14997
    },
    {
        "loss": 0.9094,
        "grad_norm": 3.1155312061309814,
        "learning_rate": 1.2675282678453748e-09,
        "epoch": 1.9997333333333334,
        "step": 14998
    },
    {
        "loss": 2.7556,
        "grad_norm": 3.9597346782684326,
        "learning_rate": 9.704518105668748e-10,
        "epoch": 1.9998666666666667,
        "step": 14999
    },
    {
        "loss": 2.3901,
        "grad_norm": 2.451293468475342,
        "learning_rate": 7.129853096365935e-10,
        "epoch": 2.0,
        "step": 15000
    },
    {
        "train_runtime": 8989.4735,
        "train_samples_per_second": 3.337,
        "train_steps_per_second": 1.669,
        "total_flos": 1.0737124327981056e+17,
        "train_loss": 2.152770532361666,
        "epoch": 2.0,
        "step": 15000
    }
]