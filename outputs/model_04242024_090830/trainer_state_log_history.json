[
    {
        "loss": 3.7863,
        "grad_norm": 2.1820037364959717,
        "learning_rate": 6.666666666666667e-06,
        "epoch": 0.0001288992008249549,
        "step": 1
    },
    {
        "loss": 3.8487,
        "grad_norm": 2.296196937561035,
        "learning_rate": 1.3333333333333333e-05,
        "epoch": 0.0002577984016499098,
        "step": 2
    },
    {
        "loss": 3.8019,
        "grad_norm": 2.8535361289978027,
        "learning_rate": 2e-05,
        "epoch": 0.0003866976024748647,
        "step": 3
    },
    {
        "loss": 3.2213,
        "grad_norm": 1.9490941762924194,
        "learning_rate": 2.6666666666666667e-05,
        "epoch": 0.0005155968032998196,
        "step": 4
    },
    {
        "loss": 3.8845,
        "grad_norm": 2.9360835552215576,
        "learning_rate": 3.3333333333333335e-05,
        "epoch": 0.0006444960041247745,
        "step": 5
    },
    {
        "loss": 3.4792,
        "grad_norm": 2.46332049369812,
        "learning_rate": 4e-05,
        "epoch": 0.0007733952049497294,
        "step": 6
    },
    {
        "loss": 3.3748,
        "grad_norm": 1.7882364988327026,
        "learning_rate": 4.666666666666667e-05,
        "epoch": 0.0009022944057746842,
        "step": 7
    },
    {
        "loss": 3.4808,
        "grad_norm": 1.852133870124817,
        "learning_rate": 5.333333333333333e-05,
        "epoch": 0.0010311936065996391,
        "step": 8
    },
    {
        "loss": 3.755,
        "grad_norm": 2.842761993408203,
        "learning_rate": 6e-05,
        "epoch": 0.001160092807424594,
        "step": 9
    },
    {
        "loss": 3.6756,
        "grad_norm": 2.8610167503356934,
        "learning_rate": 6.666666666666667e-05,
        "epoch": 0.001288992008249549,
        "step": 10
    },
    {
        "loss": 3.4229,
        "grad_norm": 1.7378880977630615,
        "learning_rate": 7.333333333333333e-05,
        "epoch": 0.0014178912090745037,
        "step": 11
    },
    {
        "loss": 3.2787,
        "grad_norm": 3.0266261100769043,
        "learning_rate": 8e-05,
        "epoch": 0.0015467904098994587,
        "step": 12
    },
    {
        "loss": 3.2986,
        "grad_norm": 1.4966181516647339,
        "learning_rate": 8.666666666666667e-05,
        "epoch": 0.0016756896107244135,
        "step": 13
    },
    {
        "loss": 3.0989,
        "grad_norm": 1.3580482006072998,
        "learning_rate": 9.333333333333334e-05,
        "epoch": 0.0018045888115493685,
        "step": 14
    },
    {
        "loss": 3.2584,
        "grad_norm": 1.4449926614761353,
        "learning_rate": 0.0001,
        "epoch": 0.0019334880123743233,
        "step": 15
    },
    {
        "loss": 2.9617,
        "grad_norm": 2.07464337348938,
        "learning_rate": 9.999996296062147e-05,
        "epoch": 0.0020623872131992783,
        "step": 16
    },
    {
        "loss": 3.4282,
        "grad_norm": 1.7431367635726929,
        "learning_rate": 9.999985184254071e-05,
        "epoch": 0.002191286414024233,
        "step": 17
    },
    {
        "loss": 3.0633,
        "grad_norm": 3.171693801879883,
        "learning_rate": 9.999966664592237e-05,
        "epoch": 0.002320185614849188,
        "step": 18
    },
    {
        "loss": 2.9803,
        "grad_norm": 1.7686868906021118,
        "learning_rate": 9.999940737104086e-05,
        "epoch": 0.0024490848156741426,
        "step": 19
    },
    {
        "loss": 2.9225,
        "grad_norm": 3.6029937267303467,
        "learning_rate": 9.999907401828026e-05,
        "epoch": 0.002577984016499098,
        "step": 20
    },
    {
        "loss": 2.763,
        "grad_norm": 2.141814708709717,
        "learning_rate": 9.99986665881345e-05,
        "epoch": 0.0027068832173240526,
        "step": 21
    },
    {
        "loss": 3.1247,
        "grad_norm": 1.7809083461761475,
        "learning_rate": 9.999818508120722e-05,
        "epoch": 0.0028357824181490074,
        "step": 22
    },
    {
        "loss": 2.445,
        "grad_norm": 2.6917552947998047,
        "learning_rate": 9.999762949821176e-05,
        "epoch": 0.002964681618973962,
        "step": 23
    },
    {
        "loss": 2.9881,
        "grad_norm": 1.6869239807128906,
        "learning_rate": 9.999699983997131e-05,
        "epoch": 0.0030935808197989174,
        "step": 24
    },
    {
        "loss": 2.7739,
        "grad_norm": 1.3983345031738281,
        "learning_rate": 9.999629610741875e-05,
        "epoch": 0.003222480020623872,
        "step": 25
    },
    {
        "loss": 2.725,
        "grad_norm": 1.5956679582595825,
        "learning_rate": 9.999551830159668e-05,
        "epoch": 0.003351379221448827,
        "step": 26
    },
    {
        "loss": 3.0569,
        "grad_norm": 1.182682991027832,
        "learning_rate": 9.999466642365752e-05,
        "epoch": 0.0034802784222737818,
        "step": 27
    },
    {
        "loss": 2.5685,
        "grad_norm": 1.9309196472167969,
        "learning_rate": 9.999374047486334e-05,
        "epoch": 0.003609177623098737,
        "step": 28
    },
    {
        "loss": 2.6221,
        "grad_norm": 2.1962921619415283,
        "learning_rate": 9.999274045658605e-05,
        "epoch": 0.0037380768239236918,
        "step": 29
    },
    {
        "loss": 2.7528,
        "grad_norm": 2.1921708583831787,
        "learning_rate": 9.999166637030721e-05,
        "epoch": 0.0038669760247486465,
        "step": 30
    },
    {
        "loss": 2.3723,
        "grad_norm": 3.09136962890625,
        "learning_rate": 9.999051821761821e-05,
        "epoch": 0.003995875225573602,
        "step": 31
    },
    {
        "loss": 2.7126,
        "grad_norm": 1.7995318174362183,
        "learning_rate": 9.998929600022008e-05,
        "epoch": 0.0041247744263985565,
        "step": 32
    },
    {
        "loss": 2.4617,
        "grad_norm": 2.2397167682647705,
        "learning_rate": 9.998799971992367e-05,
        "epoch": 0.004253673627223511,
        "step": 33
    },
    {
        "loss": 2.9726,
        "grad_norm": 1.4250825643539429,
        "learning_rate": 9.998662937864947e-05,
        "epoch": 0.004382572828048466,
        "step": 34
    },
    {
        "loss": 2.6268,
        "grad_norm": 1.2175259590148926,
        "learning_rate": 9.998518497842777e-05,
        "epoch": 0.004511472028873421,
        "step": 35
    },
    {
        "loss": 3.1786,
        "grad_norm": 1.1912751197814941,
        "learning_rate": 9.998366652139856e-05,
        "epoch": 0.004640371229698376,
        "step": 36
    },
    {
        "loss": 2.6612,
        "grad_norm": 1.4465711116790771,
        "learning_rate": 9.998207400981154e-05,
        "epoch": 0.0047692704305233305,
        "step": 37
    },
    {
        "loss": 2.5435,
        "grad_norm": 1.6692451238632202,
        "learning_rate": 9.998040744602613e-05,
        "epoch": 0.004898169631348285,
        "step": 38
    },
    {
        "loss": 2.6632,
        "grad_norm": 2.0664777755737305,
        "learning_rate": 9.997866683251147e-05,
        "epoch": 0.005027068832173241,
        "step": 39
    },
    {
        "loss": 2.497,
        "grad_norm": 1.7753946781158447,
        "learning_rate": 9.997685217184643e-05,
        "epoch": 0.005155968032998196,
        "step": 40
    },
    {
        "loss": 2.7893,
        "grad_norm": 1.3672661781311035,
        "learning_rate": 9.997496346671953e-05,
        "epoch": 0.0052848672338231505,
        "step": 41
    },
    {
        "loss": 2.674,
        "grad_norm": 1.511479139328003,
        "learning_rate": 9.997300071992907e-05,
        "epoch": 0.005413766434648105,
        "step": 42
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.8061487674713135,
        "learning_rate": 9.997096393438298e-05,
        "epoch": 0.00554266563547306,
        "step": 43
    },
    {
        "loss": 2.6966,
        "grad_norm": 1.469186782836914,
        "learning_rate": 9.996885311309891e-05,
        "epoch": 0.005671564836298015,
        "step": 44
    },
    {
        "loss": 2.9288,
        "grad_norm": 1.232143759727478,
        "learning_rate": 9.996666825920422e-05,
        "epoch": 0.00580046403712297,
        "step": 45
    },
    {
        "loss": 3.0147,
        "grad_norm": 0.9016588926315308,
        "learning_rate": 9.996440937593592e-05,
        "epoch": 0.005929363237947924,
        "step": 46
    },
    {
        "loss": 2.497,
        "grad_norm": 1.5883475542068481,
        "learning_rate": 9.996207646664072e-05,
        "epoch": 0.00605826243877288,
        "step": 47
    },
    {
        "loss": 2.8348,
        "grad_norm": 1.4537907838821411,
        "learning_rate": 9.9959669534775e-05,
        "epoch": 0.006187161639597835,
        "step": 48
    },
    {
        "loss": 2.8886,
        "grad_norm": 1.5643260478973389,
        "learning_rate": 9.995718858390482e-05,
        "epoch": 0.00631606084042279,
        "step": 49
    },
    {
        "loss": 2.1939,
        "grad_norm": 2.098814010620117,
        "learning_rate": 9.995463361770585e-05,
        "epoch": 0.006444960041247744,
        "step": 50
    },
    {
        "loss": 2.0136,
        "grad_norm": 2.829552412033081,
        "learning_rate": 9.995200463996352e-05,
        "epoch": 0.006573859242072699,
        "step": 51
    },
    {
        "loss": 2.7184,
        "grad_norm": 1.5277798175811768,
        "learning_rate": 9.994930165457285e-05,
        "epoch": 0.006702758442897654,
        "step": 52
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.8492076396942139,
        "learning_rate": 9.99465246655385e-05,
        "epoch": 0.006831657643722609,
        "step": 53
    },
    {
        "loss": 2.7111,
        "grad_norm": 1.8246841430664062,
        "learning_rate": 9.994367367697478e-05,
        "epoch": 0.0069605568445475635,
        "step": 54
    },
    {
        "loss": 2.4134,
        "grad_norm": 1.6733994483947754,
        "learning_rate": 9.994074869310566e-05,
        "epoch": 0.007089456045372518,
        "step": 55
    },
    {
        "loss": 2.4026,
        "grad_norm": 1.8721816539764404,
        "learning_rate": 9.993774971826471e-05,
        "epoch": 0.007218355246197474,
        "step": 56
    },
    {
        "loss": 2.3766,
        "grad_norm": 1.8413774967193604,
        "learning_rate": 9.993467675689513e-05,
        "epoch": 0.007347254447022429,
        "step": 57
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.0839686393737793,
        "learning_rate": 9.993152981354979e-05,
        "epoch": 0.0074761536478473835,
        "step": 58
    },
    {
        "loss": 2.528,
        "grad_norm": 2.286641836166382,
        "learning_rate": 9.992830889289109e-05,
        "epoch": 0.007605052848672338,
        "step": 59
    },
    {
        "loss": 2.5828,
        "grad_norm": 1.264033555984497,
        "learning_rate": 9.992501399969106e-05,
        "epoch": 0.007733952049497293,
        "step": 60
    },
    {
        "loss": 2.6509,
        "grad_norm": 1.4870610237121582,
        "learning_rate": 9.992164513883133e-05,
        "epoch": 0.007862851250322248,
        "step": 61
    },
    {
        "loss": 2.5844,
        "grad_norm": 1.1404500007629395,
        "learning_rate": 9.991820231530313e-05,
        "epoch": 0.007991750451147204,
        "step": 62
    },
    {
        "loss": 2.5544,
        "grad_norm": 1.7225499153137207,
        "learning_rate": 9.991468553420726e-05,
        "epoch": 0.008120649651972157,
        "step": 63
    },
    {
        "loss": 1.7102,
        "grad_norm": 3.132333278656006,
        "learning_rate": 9.99110948007541e-05,
        "epoch": 0.008249548852797113,
        "step": 64
    },
    {
        "loss": 2.4936,
        "grad_norm": 1.452600359916687,
        "learning_rate": 9.99074301202636e-05,
        "epoch": 0.008378448053622067,
        "step": 65
    },
    {
        "loss": 2.3399,
        "grad_norm": 2.390664577484131,
        "learning_rate": 9.990369149816523e-05,
        "epoch": 0.008507347254447023,
        "step": 66
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.3056576251983643,
        "learning_rate": 9.989987893999807e-05,
        "epoch": 0.008636246455271977,
        "step": 67
    },
    {
        "loss": 1.4332,
        "grad_norm": 1.6752862930297852,
        "learning_rate": 9.989599245141069e-05,
        "epoch": 0.008765145656096932,
        "step": 68
    },
    {
        "loss": 1.9792,
        "grad_norm": 2.2084386348724365,
        "learning_rate": 9.989203203816123e-05,
        "epoch": 0.008894044856921888,
        "step": 69
    },
    {
        "loss": 2.4917,
        "grad_norm": 1.6396512985229492,
        "learning_rate": 9.988799770611734e-05,
        "epoch": 0.009022944057746842,
        "step": 70
    },
    {
        "loss": 2.7761,
        "grad_norm": 1.3581130504608154,
        "learning_rate": 9.988388946125615e-05,
        "epoch": 0.009151843258571797,
        "step": 71
    },
    {
        "loss": 2.5471,
        "grad_norm": 1.945707082748413,
        "learning_rate": 9.987970730966438e-05,
        "epoch": 0.009280742459396751,
        "step": 72
    },
    {
        "loss": 2.315,
        "grad_norm": 1.3132843971252441,
        "learning_rate": 9.987545125753819e-05,
        "epoch": 0.009409641660221707,
        "step": 73
    },
    {
        "loss": 2.6979,
        "grad_norm": 1.4899144172668457,
        "learning_rate": 9.987112131118323e-05,
        "epoch": 0.009538540861046661,
        "step": 74
    },
    {
        "loss": 2.2898,
        "grad_norm": 2.4027552604675293,
        "learning_rate": 9.986671747701466e-05,
        "epoch": 0.009667440061871617,
        "step": 75
    },
    {
        "loss": 2.7974,
        "grad_norm": 1.3178691864013672,
        "learning_rate": 9.986223976155706e-05,
        "epoch": 0.00979633926269657,
        "step": 76
    },
    {
        "loss": 2.6329,
        "grad_norm": 1.9338029623031616,
        "learning_rate": 9.985768817144453e-05,
        "epoch": 0.009925238463521526,
        "step": 77
    },
    {
        "loss": 2.5668,
        "grad_norm": 1.944339394569397,
        "learning_rate": 9.985306271342058e-05,
        "epoch": 0.010054137664346482,
        "step": 78
    },
    {
        "loss": 2.634,
        "grad_norm": 1.9203968048095703,
        "learning_rate": 9.984836339433816e-05,
        "epoch": 0.010183036865171436,
        "step": 79
    },
    {
        "loss": 2.6714,
        "grad_norm": 1.5658265352249146,
        "learning_rate": 9.98435902211597e-05,
        "epoch": 0.010311936065996391,
        "step": 80
    },
    {
        "loss": 1.5864,
        "grad_norm": 2.0049004554748535,
        "learning_rate": 9.983874320095698e-05,
        "epoch": 0.010440835266821345,
        "step": 81
    },
    {
        "loss": 2.3469,
        "grad_norm": 2.1481595039367676,
        "learning_rate": 9.983382234091126e-05,
        "epoch": 0.010569734467646301,
        "step": 82
    },
    {
        "loss": 2.4317,
        "grad_norm": 1.4787061214447021,
        "learning_rate": 9.982882764831315e-05,
        "epoch": 0.010698633668471255,
        "step": 83
    },
    {
        "loss": 1.6447,
        "grad_norm": 2.1604440212249756,
        "learning_rate": 9.982375913056262e-05,
        "epoch": 0.01082753286929621,
        "step": 84
    },
    {
        "loss": 2.469,
        "grad_norm": 1.7974226474761963,
        "learning_rate": 9.981861679516912e-05,
        "epoch": 0.010956432070121164,
        "step": 85
    },
    {
        "loss": 2.4142,
        "grad_norm": 1.8305113315582275,
        "learning_rate": 9.981340064975137e-05,
        "epoch": 0.01108533127094612,
        "step": 86
    },
    {
        "loss": 2.7929,
        "grad_norm": 1.4981238842010498,
        "learning_rate": 9.98081107020375e-05,
        "epoch": 0.011214230471771076,
        "step": 87
    },
    {
        "loss": 2.6091,
        "grad_norm": 1.3881500959396362,
        "learning_rate": 9.980274695986496e-05,
        "epoch": 0.01134312967259603,
        "step": 88
    },
    {
        "loss": 1.7628,
        "grad_norm": 1.9450459480285645,
        "learning_rate": 9.979730943118054e-05,
        "epoch": 0.011472028873420985,
        "step": 89
    },
    {
        "loss": 2.5845,
        "grad_norm": 2.144669532775879,
        "learning_rate": 9.979179812404033e-05,
        "epoch": 0.01160092807424594,
        "step": 90
    },
    {
        "loss": 2.1936,
        "grad_norm": 1.9964855909347534,
        "learning_rate": 9.978621304660978e-05,
        "epoch": 0.011729827275070895,
        "step": 91
    },
    {
        "loss": 2.0177,
        "grad_norm": 1.9231306314468384,
        "learning_rate": 9.978055420716357e-05,
        "epoch": 0.011858726475895849,
        "step": 92
    },
    {
        "loss": 2.236,
        "grad_norm": 2.9425227642059326,
        "learning_rate": 9.977482161408568e-05,
        "epoch": 0.011987625676720804,
        "step": 93
    },
    {
        "loss": 2.2389,
        "grad_norm": 2.1716666221618652,
        "learning_rate": 9.976901527586944e-05,
        "epoch": 0.01211652487754576,
        "step": 94
    },
    {
        "loss": 2.5441,
        "grad_norm": 1.3875678777694702,
        "learning_rate": 9.976313520111732e-05,
        "epoch": 0.012245424078370714,
        "step": 95
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.1656575202941895,
        "learning_rate": 9.975718139854113e-05,
        "epoch": 0.01237432327919567,
        "step": 96
    },
    {
        "loss": 2.2137,
        "grad_norm": 2.6994025707244873,
        "learning_rate": 9.975115387696186e-05,
        "epoch": 0.012503222480020624,
        "step": 97
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.163867712020874,
        "learning_rate": 9.974505264530972e-05,
        "epoch": 0.01263212168084558,
        "step": 98
    },
    {
        "loss": 2.4725,
        "grad_norm": 2.0123095512390137,
        "learning_rate": 9.973887771262415e-05,
        "epoch": 0.012761020881670533,
        "step": 99
    },
    {
        "loss": 2.8999,
        "grad_norm": 1.8911597728729248,
        "learning_rate": 9.973262908805381e-05,
        "epoch": 0.012889920082495489,
        "step": 100
    },
    {
        "loss": 2.4568,
        "grad_norm": 1.8975414037704468,
        "learning_rate": 9.972630678085646e-05,
        "epoch": 0.013018819283320443,
        "step": 101
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.7760130167007446,
        "learning_rate": 9.97199108003991e-05,
        "epoch": 0.013147718484145398,
        "step": 102
    },
    {
        "loss": 2.3797,
        "grad_norm": 2.256793737411499,
        "learning_rate": 9.971344115615785e-05,
        "epoch": 0.013276617684970354,
        "step": 103
    },
    {
        "loss": 2.6637,
        "grad_norm": 1.360243558883667,
        "learning_rate": 9.970689785771798e-05,
        "epoch": 0.013405516885795308,
        "step": 104
    },
    {
        "loss": 2.6062,
        "grad_norm": 1.2539234161376953,
        "learning_rate": 9.970028091477388e-05,
        "epoch": 0.013534416086620264,
        "step": 105
    },
    {
        "loss": 2.6595,
        "grad_norm": 1.2306928634643555,
        "learning_rate": 9.969359033712903e-05,
        "epoch": 0.013663315287445217,
        "step": 106
    },
    {
        "loss": 2.4335,
        "grad_norm": 1.2528401613235474,
        "learning_rate": 9.968682613469603e-05,
        "epoch": 0.013792214488270173,
        "step": 107
    },
    {
        "loss": 2.0884,
        "grad_norm": 3.6202921867370605,
        "learning_rate": 9.967998831749656e-05,
        "epoch": 0.013921113689095127,
        "step": 108
    },
    {
        "loss": 2.1138,
        "grad_norm": 2.441098928451538,
        "learning_rate": 9.967307689566137e-05,
        "epoch": 0.014050012889920083,
        "step": 109
    },
    {
        "loss": 2.5597,
        "grad_norm": 1.2149295806884766,
        "learning_rate": 9.966609187943023e-05,
        "epoch": 0.014178912090745037,
        "step": 110
    },
    {
        "loss": 2.5294,
        "grad_norm": 1.8417235612869263,
        "learning_rate": 9.965903327915198e-05,
        "epoch": 0.014307811291569992,
        "step": 111
    },
    {
        "loss": 1.6076,
        "grad_norm": 2.145308256149292,
        "learning_rate": 9.965190110528446e-05,
        "epoch": 0.014436710492394948,
        "step": 112
    },
    {
        "loss": 2.709,
        "grad_norm": 1.3088079690933228,
        "learning_rate": 9.964469536839454e-05,
        "epoch": 0.014565609693219902,
        "step": 113
    },
    {
        "loss": 2.0749,
        "grad_norm": 2.678767204284668,
        "learning_rate": 9.963741607915802e-05,
        "epoch": 0.014694508894044857,
        "step": 114
    },
    {
        "loss": 2.176,
        "grad_norm": 1.4148809909820557,
        "learning_rate": 9.963006324835975e-05,
        "epoch": 0.014823408094869811,
        "step": 115
    },
    {
        "loss": 2.4498,
        "grad_norm": 1.4465562105178833,
        "learning_rate": 9.962263688689349e-05,
        "epoch": 0.014952307295694767,
        "step": 116
    },
    {
        "loss": 1.6958,
        "grad_norm": 2.1090030670166016,
        "learning_rate": 9.961513700576195e-05,
        "epoch": 0.015081206496519721,
        "step": 117
    },
    {
        "loss": 1.7831,
        "grad_norm": 2.242748498916626,
        "learning_rate": 9.960756361607677e-05,
        "epoch": 0.015210105697344677,
        "step": 118
    },
    {
        "loss": 2.592,
        "grad_norm": 1.3612112998962402,
        "learning_rate": 9.95999167290585e-05,
        "epoch": 0.01533900489816963,
        "step": 119
    },
    {
        "loss": 1.8208,
        "grad_norm": 2.4111971855163574,
        "learning_rate": 9.959219635603656e-05,
        "epoch": 0.015467904098994586,
        "step": 120
    },
    {
        "loss": 2.5072,
        "grad_norm": 2.031440019607544,
        "learning_rate": 9.958440250844929e-05,
        "epoch": 0.015596803299819542,
        "step": 121
    },
    {
        "loss": 2.5835,
        "grad_norm": 1.6586756706237793,
        "learning_rate": 9.957653519784384e-05,
        "epoch": 0.015725702500644496,
        "step": 122
    },
    {
        "loss": 1.6286,
        "grad_norm": 2.828824281692505,
        "learning_rate": 9.956859443587625e-05,
        "epoch": 0.01585460170146945,
        "step": 123
    },
    {
        "loss": 2.2907,
        "grad_norm": 2.529667615890503,
        "learning_rate": 9.956058023431132e-05,
        "epoch": 0.015983500902294407,
        "step": 124
    },
    {
        "loss": 2.1578,
        "grad_norm": 3.3832483291625977,
        "learning_rate": 9.95524926050227e-05,
        "epoch": 0.01611240010311936,
        "step": 125
    },
    {
        "loss": 2.2203,
        "grad_norm": 2.1445705890655518,
        "learning_rate": 9.954433155999283e-05,
        "epoch": 0.016241299303944315,
        "step": 126
    },
    {
        "loss": 2.1524,
        "grad_norm": 2.238518476486206,
        "learning_rate": 9.95360971113129e-05,
        "epoch": 0.01637019850476927,
        "step": 127
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.4559646844863892,
        "learning_rate": 9.952778927118289e-05,
        "epoch": 0.016499097705594226,
        "step": 128
    },
    {
        "loss": 2.6749,
        "grad_norm": 2.234666347503662,
        "learning_rate": 9.951940805191147e-05,
        "epoch": 0.016627996906419182,
        "step": 129
    },
    {
        "loss": 2.7306,
        "grad_norm": 1.4651553630828857,
        "learning_rate": 9.951095346591604e-05,
        "epoch": 0.016756896107244134,
        "step": 130
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.6082860231399536,
        "learning_rate": 9.950242552572271e-05,
        "epoch": 0.01688579530806909,
        "step": 131
    },
    {
        "loss": 2.4866,
        "grad_norm": 1.9219121932983398,
        "learning_rate": 9.94938242439663e-05,
        "epoch": 0.017014694508894045,
        "step": 132
    },
    {
        "loss": 2.5408,
        "grad_norm": 1.8431158065795898,
        "learning_rate": 9.948514963339018e-05,
        "epoch": 0.017143593709719,
        "step": 133
    },
    {
        "loss": 2.6602,
        "grad_norm": 1.2845356464385986,
        "learning_rate": 9.94764017068465e-05,
        "epoch": 0.017272492910543953,
        "step": 134
    },
    {
        "loss": 1.7033,
        "grad_norm": 2.805206298828125,
        "learning_rate": 9.946758047729594e-05,
        "epoch": 0.01740139211136891,
        "step": 135
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.48396635055542,
        "learning_rate": 9.945868595780783e-05,
        "epoch": 0.017530291312193864,
        "step": 136
    },
    {
        "loss": 1.6373,
        "grad_norm": 3.2893526554107666,
        "learning_rate": 9.944971816156006e-05,
        "epoch": 0.01765919051301882,
        "step": 137
    },
    {
        "loss": 2.5092,
        "grad_norm": 1.4700599908828735,
        "learning_rate": 9.944067710183911e-05,
        "epoch": 0.017788089713843776,
        "step": 138
    },
    {
        "loss": 2.5385,
        "grad_norm": 2.2535240650177,
        "learning_rate": 9.943156279203997e-05,
        "epoch": 0.017916988914668728,
        "step": 139
    },
    {
        "loss": 2.4421,
        "grad_norm": 2.0924551486968994,
        "learning_rate": 9.942237524566618e-05,
        "epoch": 0.018045888115493684,
        "step": 140
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.0841422080993652,
        "learning_rate": 9.941311447632979e-05,
        "epoch": 0.01817478731631864,
        "step": 141
    },
    {
        "loss": 2.1646,
        "grad_norm": 2.6641294956207275,
        "learning_rate": 9.940378049775129e-05,
        "epoch": 0.018303686517143595,
        "step": 142
    },
    {
        "loss": 2.3514,
        "grad_norm": 2.1128358840942383,
        "learning_rate": 9.939437332375974e-05,
        "epoch": 0.018432585717968547,
        "step": 143
    },
    {
        "loss": 2.4461,
        "grad_norm": 1.359737753868103,
        "learning_rate": 9.93848929682925e-05,
        "epoch": 0.018561484918793503,
        "step": 144
    },
    {
        "loss": 2.4903,
        "grad_norm": 1.8547358512878418,
        "learning_rate": 9.937533944539547e-05,
        "epoch": 0.01869038411961846,
        "step": 145
    },
    {
        "loss": 2.4019,
        "grad_norm": 1.50257408618927,
        "learning_rate": 9.936571276922292e-05,
        "epoch": 0.018819283320443414,
        "step": 146
    },
    {
        "loss": 2.1106,
        "grad_norm": 1.2886905670166016,
        "learning_rate": 9.935601295403748e-05,
        "epoch": 0.01894818252126837,
        "step": 147
    },
    {
        "loss": 2.4134,
        "grad_norm": 1.0590955018997192,
        "learning_rate": 9.934624001421012e-05,
        "epoch": 0.019077081722093322,
        "step": 148
    },
    {
        "loss": 2.4604,
        "grad_norm": 1.7480474710464478,
        "learning_rate": 9.933639396422024e-05,
        "epoch": 0.019205980922918277,
        "step": 149
    },
    {
        "loss": 2.2358,
        "grad_norm": 1.603676676750183,
        "learning_rate": 9.93264748186555e-05,
        "epoch": 0.019334880123743233,
        "step": 150
    },
    {
        "loss": 2.8824,
        "grad_norm": 1.1864042282104492,
        "learning_rate": 9.931648259221181e-05,
        "epoch": 0.01946377932456819,
        "step": 151
    },
    {
        "loss": 2.4047,
        "grad_norm": 1.7900735139846802,
        "learning_rate": 9.930641729969344e-05,
        "epoch": 0.01959267852539314,
        "step": 152
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.825820803642273,
        "learning_rate": 9.929627895601288e-05,
        "epoch": 0.019721577726218097,
        "step": 153
    },
    {
        "loss": 2.3691,
        "grad_norm": 1.7961015701293945,
        "learning_rate": 9.928606757619084e-05,
        "epoch": 0.019850476927043052,
        "step": 154
    },
    {
        "loss": 2.6782,
        "grad_norm": 1.560817003250122,
        "learning_rate": 9.927578317535625e-05,
        "epoch": 0.019979376127868008,
        "step": 155
    },
    {
        "loss": 2.7658,
        "grad_norm": 1.7966718673706055,
        "learning_rate": 9.926542576874622e-05,
        "epoch": 0.020108275328692964,
        "step": 156
    },
    {
        "loss": 2.2235,
        "grad_norm": 1.6596215963363647,
        "learning_rate": 9.925499537170602e-05,
        "epoch": 0.020237174529517916,
        "step": 157
    },
    {
        "loss": 2.4765,
        "grad_norm": 1.7084981203079224,
        "learning_rate": 9.924449199968908e-05,
        "epoch": 0.02036607373034287,
        "step": 158
    },
    {
        "loss": 2.593,
        "grad_norm": 1.304551124572754,
        "learning_rate": 9.92339156682569e-05,
        "epoch": 0.020494972931167827,
        "step": 159
    },
    {
        "loss": 2.4417,
        "grad_norm": 1.8493330478668213,
        "learning_rate": 9.922326639307917e-05,
        "epoch": 0.020623872131992783,
        "step": 160
    },
    {
        "loss": 2.1176,
        "grad_norm": 2.4647164344787598,
        "learning_rate": 9.921254418993355e-05,
        "epoch": 0.020752771332817735,
        "step": 161
    },
    {
        "loss": 2.6069,
        "grad_norm": 1.314231276512146,
        "learning_rate": 9.920174907470579e-05,
        "epoch": 0.02088167053364269,
        "step": 162
    },
    {
        "loss": 1.9993,
        "grad_norm": 1.948944091796875,
        "learning_rate": 9.919088106338968e-05,
        "epoch": 0.021010569734467646,
        "step": 163
    },
    {
        "loss": 2.7038,
        "grad_norm": 1.7573648691177368,
        "learning_rate": 9.917994017208699e-05,
        "epoch": 0.021139468935292602,
        "step": 164
    },
    {
        "loss": 2.5343,
        "grad_norm": 1.449431300163269,
        "learning_rate": 9.916892641700746e-05,
        "epoch": 0.021268368136117557,
        "step": 165
    },
    {
        "loss": 2.2787,
        "grad_norm": 2.277519464492798,
        "learning_rate": 9.915783981446882e-05,
        "epoch": 0.02139726733694251,
        "step": 166
    },
    {
        "loss": 2.5355,
        "grad_norm": 1.4967252016067505,
        "learning_rate": 9.914668038089667e-05,
        "epoch": 0.021526166537767465,
        "step": 167
    },
    {
        "loss": 2.6441,
        "grad_norm": 1.3953500986099243,
        "learning_rate": 9.91354481328246e-05,
        "epoch": 0.02165506573859242,
        "step": 168
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.9306342601776123,
        "learning_rate": 9.912414308689397e-05,
        "epoch": 0.021783964939417377,
        "step": 169
    },
    {
        "loss": 1.9538,
        "grad_norm": 1.8753795623779297,
        "learning_rate": 9.911276525985411e-05,
        "epoch": 0.02191286414024233,
        "step": 170
    },
    {
        "loss": 1.947,
        "grad_norm": 2.925544261932373,
        "learning_rate": 9.910131466856209e-05,
        "epoch": 0.022041763341067284,
        "step": 171
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.6957978010177612,
        "learning_rate": 9.908979132998282e-05,
        "epoch": 0.02217066254189224,
        "step": 172
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.9824298620224,
        "learning_rate": 9.907819526118902e-05,
        "epoch": 0.022299561742717196,
        "step": 173
    },
    {
        "loss": 2.4493,
        "grad_norm": 2.048743486404419,
        "learning_rate": 9.906652647936113e-05,
        "epoch": 0.02242846094354215,
        "step": 174
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.355412006378174,
        "learning_rate": 9.905478500178729e-05,
        "epoch": 0.022557360144367104,
        "step": 175
    },
    {
        "loss": 2.0656,
        "grad_norm": 1.781720519065857,
        "learning_rate": 9.904297084586342e-05,
        "epoch": 0.02268625934519206,
        "step": 176
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.923647165298462,
        "learning_rate": 9.903108402909309e-05,
        "epoch": 0.022815158546017015,
        "step": 177
    },
    {
        "loss": 2.232,
        "grad_norm": 2.0130813121795654,
        "learning_rate": 9.901912456908748e-05,
        "epoch": 0.02294405774684197,
        "step": 178
    },
    {
        "loss": 2.5097,
        "grad_norm": 1.5504788160324097,
        "learning_rate": 9.900709248356544e-05,
        "epoch": 0.023072956947666923,
        "step": 179
    },
    {
        "loss": 1.1904,
        "grad_norm": 2.978635311126709,
        "learning_rate": 9.89949877903534e-05,
        "epoch": 0.02320185614849188,
        "step": 180
    },
    {
        "loss": 2.3556,
        "grad_norm": 1.5145690441131592,
        "learning_rate": 9.898281050738538e-05,
        "epoch": 0.023330755349316834,
        "step": 181
    },
    {
        "loss": 1.6225,
        "grad_norm": 2.91327166557312,
        "learning_rate": 9.897056065270296e-05,
        "epoch": 0.02345965455014179,
        "step": 182
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.8319133520126343,
        "learning_rate": 9.895823824445518e-05,
        "epoch": 0.023588553750966745,
        "step": 183
    },
    {
        "loss": 2.3735,
        "grad_norm": 1.585591435432434,
        "learning_rate": 9.894584330089866e-05,
        "epoch": 0.023717452951791698,
        "step": 184
    },
    {
        "loss": 2.127,
        "grad_norm": 2.5531094074249268,
        "learning_rate": 9.893337584039739e-05,
        "epoch": 0.023846352152616653,
        "step": 185
    },
    {
        "loss": 2.1312,
        "grad_norm": 1.452184796333313,
        "learning_rate": 9.89208358814229e-05,
        "epoch": 0.02397525135344161,
        "step": 186
    },
    {
        "loss": 1.6785,
        "grad_norm": 4.448779582977295,
        "learning_rate": 9.890822344255403e-05,
        "epoch": 0.024104150554266564,
        "step": 187
    },
    {
        "loss": 2.3784,
        "grad_norm": 2.1737608909606934,
        "learning_rate": 9.88955385424771e-05,
        "epoch": 0.02423304975509152,
        "step": 188
    },
    {
        "loss": 2.5944,
        "grad_norm": 2.126194953918457,
        "learning_rate": 9.888278119998573e-05,
        "epoch": 0.024361948955916472,
        "step": 189
    },
    {
        "loss": 1.4736,
        "grad_norm": 2.844942092895508,
        "learning_rate": 9.886995143398085e-05,
        "epoch": 0.024490848156741428,
        "step": 190
    },
    {
        "loss": 2.0794,
        "grad_norm": 2.5088515281677246,
        "learning_rate": 9.885704926347078e-05,
        "epoch": 0.024619747357566384,
        "step": 191
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.400733232498169,
        "learning_rate": 9.884407470757101e-05,
        "epoch": 0.02474864655839134,
        "step": 192
    },
    {
        "loss": 2.384,
        "grad_norm": 1.565294861793518,
        "learning_rate": 9.883102778550434e-05,
        "epoch": 0.02487754575921629,
        "step": 193
    },
    {
        "loss": 2.4964,
        "grad_norm": 1.4660345315933228,
        "learning_rate": 9.881790851660075e-05,
        "epoch": 0.025006444960041247,
        "step": 194
    },
    {
        "loss": 2.2317,
        "grad_norm": 2.7688257694244385,
        "learning_rate": 9.880471692029744e-05,
        "epoch": 0.025135344160866203,
        "step": 195
    },
    {
        "loss": 2.505,
        "grad_norm": 1.0676894187927246,
        "learning_rate": 9.879145301613874e-05,
        "epoch": 0.02526424336169116,
        "step": 196
    },
    {
        "loss": 1.7778,
        "grad_norm": 2.4726037979125977,
        "learning_rate": 9.877811682377613e-05,
        "epoch": 0.025393142562516114,
        "step": 197
    },
    {
        "loss": 2.1975,
        "grad_norm": 1.5374183654785156,
        "learning_rate": 9.876470836296815e-05,
        "epoch": 0.025522041763341066,
        "step": 198
    },
    {
        "loss": 2.6357,
        "grad_norm": 1.9634971618652344,
        "learning_rate": 9.875122765358049e-05,
        "epoch": 0.025650940964166022,
        "step": 199
    },
    {
        "loss": 1.8871,
        "grad_norm": 2.6445975303649902,
        "learning_rate": 9.873767471558581e-05,
        "epoch": 0.025779840164990978,
        "step": 200
    },
    {
        "loss": 1.964,
        "grad_norm": 2.2967355251312256,
        "learning_rate": 9.87240495690638e-05,
        "epoch": 0.025908739365815933,
        "step": 201
    },
    {
        "loss": 2.5283,
        "grad_norm": 1.5190706253051758,
        "learning_rate": 9.871035223420115e-05,
        "epoch": 0.026037638566640885,
        "step": 202
    },
    {
        "loss": 1.7731,
        "grad_norm": 2.9879238605499268,
        "learning_rate": 9.86965827312915e-05,
        "epoch": 0.02616653776746584,
        "step": 203
    },
    {
        "loss": 1.5834,
        "grad_norm": 2.5431253910064697,
        "learning_rate": 9.868274108073537e-05,
        "epoch": 0.026295436968290797,
        "step": 204
    },
    {
        "loss": 2.3421,
        "grad_norm": 2.0505669116973877,
        "learning_rate": 9.866882730304023e-05,
        "epoch": 0.026424336169115752,
        "step": 205
    },
    {
        "loss": 2.3128,
        "grad_norm": 1.5956339836120605,
        "learning_rate": 9.865484141882038e-05,
        "epoch": 0.026553235369940708,
        "step": 206
    },
    {
        "loss": 2.4299,
        "grad_norm": 1.2911604642868042,
        "learning_rate": 9.864078344879694e-05,
        "epoch": 0.02668213457076566,
        "step": 207
    },
    {
        "loss": 2.5062,
        "grad_norm": 1.2006722688674927,
        "learning_rate": 9.862665341379791e-05,
        "epoch": 0.026811033771590616,
        "step": 208
    },
    {
        "loss": 2.2659,
        "grad_norm": 1.6992921829223633,
        "learning_rate": 9.861245133475793e-05,
        "epoch": 0.02693993297241557,
        "step": 209
    },
    {
        "loss": 1.9838,
        "grad_norm": 2.570409059524536,
        "learning_rate": 9.859817723271848e-05,
        "epoch": 0.027068832173240527,
        "step": 210
    },
    {
        "loss": 2.2534,
        "grad_norm": 1.9810473918914795,
        "learning_rate": 9.858383112882772e-05,
        "epoch": 0.02719773137406548,
        "step": 211
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.9390398263931274,
        "learning_rate": 9.856941304434047e-05,
        "epoch": 0.027326630574890435,
        "step": 212
    },
    {
        "loss": 2.3836,
        "grad_norm": 2.0751867294311523,
        "learning_rate": 9.855492300061819e-05,
        "epoch": 0.02745552977571539,
        "step": 213
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.7608033418655396,
        "learning_rate": 9.8540361019129e-05,
        "epoch": 0.027584428976540346,
        "step": 214
    },
    {
        "loss": 2.2341,
        "grad_norm": 1.4030851125717163,
        "learning_rate": 9.852572712144755e-05,
        "epoch": 0.027713328177365302,
        "step": 215
    },
    {
        "loss": 2.4336,
        "grad_norm": 2.0101826190948486,
        "learning_rate": 9.851102132925507e-05,
        "epoch": 0.027842227378190254,
        "step": 216
    },
    {
        "loss": 2.295,
        "grad_norm": 2.3630752563476562,
        "learning_rate": 9.849624366433928e-05,
        "epoch": 0.02797112657901521,
        "step": 217
    },
    {
        "loss": 2.672,
        "grad_norm": 1.3957899808883667,
        "learning_rate": 9.848139414859441e-05,
        "epoch": 0.028100025779840165,
        "step": 218
    },
    {
        "loss": 2.0293,
        "grad_norm": 2.7966411113739014,
        "learning_rate": 9.846647280402114e-05,
        "epoch": 0.02822892498066512,
        "step": 219
    },
    {
        "loss": 2.0919,
        "grad_norm": 1.9899907112121582,
        "learning_rate": 9.845147965272656e-05,
        "epoch": 0.028357824181490073,
        "step": 220
    },
    {
        "loss": 2.1545,
        "grad_norm": 1.5015044212341309,
        "learning_rate": 9.843641471692415e-05,
        "epoch": 0.02848672338231503,
        "step": 221
    },
    {
        "loss": 1.8595,
        "grad_norm": 2.5050570964813232,
        "learning_rate": 9.842127801893373e-05,
        "epoch": 0.028615622583139984,
        "step": 222
    },
    {
        "loss": 1.299,
        "grad_norm": 2.9220194816589355,
        "learning_rate": 9.840606958118148e-05,
        "epoch": 0.02874452178396494,
        "step": 223
    },
    {
        "loss": 1.9584,
        "grad_norm": 1.9871071577072144,
        "learning_rate": 9.839078942619981e-05,
        "epoch": 0.028873420984789896,
        "step": 224
    },
    {
        "loss": 1.7399,
        "grad_norm": 2.940793991088867,
        "learning_rate": 9.837543757662746e-05,
        "epoch": 0.029002320185614848,
        "step": 225
    },
    {
        "loss": 2.4749,
        "grad_norm": 1.710331916809082,
        "learning_rate": 9.836001405520932e-05,
        "epoch": 0.029131219386439804,
        "step": 226
    },
    {
        "loss": 1.6854,
        "grad_norm": 3.1381945610046387,
        "learning_rate": 9.83445188847965e-05,
        "epoch": 0.02926011858726476,
        "step": 227
    },
    {
        "loss": 2.4269,
        "grad_norm": 2.5731942653656006,
        "learning_rate": 9.832895208834627e-05,
        "epoch": 0.029389017788089715,
        "step": 228
    },
    {
        "loss": 2.213,
        "grad_norm": 1.9383996725082397,
        "learning_rate": 9.8313313688922e-05,
        "epoch": 0.029517916988914667,
        "step": 229
    },
    {
        "loss": 2.4702,
        "grad_norm": 1.5048112869262695,
        "learning_rate": 9.829760370969317e-05,
        "epoch": 0.029646816189739623,
        "step": 230
    },
    {
        "loss": 2.65,
        "grad_norm": 1.8143138885498047,
        "learning_rate": 9.828182217393525e-05,
        "epoch": 0.02977571539056458,
        "step": 231
    },
    {
        "loss": 2.7112,
        "grad_norm": 1.439112663269043,
        "learning_rate": 9.826596910502981e-05,
        "epoch": 0.029904614591389534,
        "step": 232
    },
    {
        "loss": 2.198,
        "grad_norm": 2.0003128051757812,
        "learning_rate": 9.825004452646436e-05,
        "epoch": 0.03003351379221449,
        "step": 233
    },
    {
        "loss": 1.881,
        "grad_norm": 2.8592982292175293,
        "learning_rate": 9.823404846183235e-05,
        "epoch": 0.030162412993039442,
        "step": 234
    },
    {
        "loss": 1.733,
        "grad_norm": 2.2615935802459717,
        "learning_rate": 9.821798093483317e-05,
        "epoch": 0.030291312193864398,
        "step": 235
    },
    {
        "loss": 2.324,
        "grad_norm": 3.280069351196289,
        "learning_rate": 9.820184196927203e-05,
        "epoch": 0.030420211394689353,
        "step": 236
    },
    {
        "loss": 2.4247,
        "grad_norm": 1.7304357290267944,
        "learning_rate": 9.818563158906007e-05,
        "epoch": 0.03054911059551431,
        "step": 237
    },
    {
        "loss": 1.8729,
        "grad_norm": 2.4433796405792236,
        "learning_rate": 9.816934981821412e-05,
        "epoch": 0.03067800979633926,
        "step": 238
    },
    {
        "loss": 2.0125,
        "grad_norm": 1.7985504865646362,
        "learning_rate": 9.815299668085694e-05,
        "epoch": 0.030806908997164217,
        "step": 239
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.022580146789551,
        "learning_rate": 9.813657220121684e-05,
        "epoch": 0.030935808197989172,
        "step": 240
    },
    {
        "loss": 2.3346,
        "grad_norm": 2.1044423580169678,
        "learning_rate": 9.812007640362797e-05,
        "epoch": 0.031064707398814128,
        "step": 241
    },
    {
        "loss": 2.3991,
        "grad_norm": 2.188433885574341,
        "learning_rate": 9.810350931253009e-05,
        "epoch": 0.031193606599639084,
        "step": 242
    },
    {
        "loss": 2.7826,
        "grad_norm": 1.430855393409729,
        "learning_rate": 9.808687095246858e-05,
        "epoch": 0.031322505800464036,
        "step": 243
    },
    {
        "loss": 1.9681,
        "grad_norm": 1.7542985677719116,
        "learning_rate": 9.807016134809442e-05,
        "epoch": 0.03145140500128899,
        "step": 244
    },
    {
        "loss": 2.0295,
        "grad_norm": 2.0413405895233154,
        "learning_rate": 9.805338052416414e-05,
        "epoch": 0.03158030420211395,
        "step": 245
    },
    {
        "loss": 1.8232,
        "grad_norm": 2.626654624938965,
        "learning_rate": 9.803652850553981e-05,
        "epoch": 0.0317092034029389,
        "step": 246
    },
    {
        "loss": 2.6322,
        "grad_norm": 1.3092372417449951,
        "learning_rate": 9.801960531718896e-05,
        "epoch": 0.03183810260376386,
        "step": 247
    },
    {
        "loss": 2.283,
        "grad_norm": 2.2769863605499268,
        "learning_rate": 9.800261098418457e-05,
        "epoch": 0.031967001804588814,
        "step": 248
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.9611552953720093,
        "learning_rate": 9.798554553170498e-05,
        "epoch": 0.03209590100541377,
        "step": 249
    },
    {
        "loss": 1.6755,
        "grad_norm": 2.5939247608184814,
        "learning_rate": 9.796840898503398e-05,
        "epoch": 0.03222480020623872,
        "step": 250
    },
    {
        "loss": 1.7576,
        "grad_norm": 2.4387826919555664,
        "learning_rate": 9.795120136956065e-05,
        "epoch": 0.032353699407063674,
        "step": 251
    },
    {
        "loss": 2.6318,
        "grad_norm": 1.5830024480819702,
        "learning_rate": 9.793392271077935e-05,
        "epoch": 0.03248259860788863,
        "step": 252
    },
    {
        "loss": 2.0916,
        "grad_norm": 1.6661179065704346,
        "learning_rate": 9.791657303428971e-05,
        "epoch": 0.032611497808713585,
        "step": 253
    },
    {
        "loss": 2.4087,
        "grad_norm": 2.081285238265991,
        "learning_rate": 9.789915236579658e-05,
        "epoch": 0.03274039700953854,
        "step": 254
    },
    {
        "loss": 2.2236,
        "grad_norm": 2.1839828491210938,
        "learning_rate": 9.788166073111002e-05,
        "epoch": 0.0328692962103635,
        "step": 255
    },
    {
        "loss": 2.4832,
        "grad_norm": 1.9582462310791016,
        "learning_rate": 9.786409815614517e-05,
        "epoch": 0.03299819541118845,
        "step": 256
    },
    {
        "loss": 1.7182,
        "grad_norm": 2.539958953857422,
        "learning_rate": 9.78464646669223e-05,
        "epoch": 0.03312709461201341,
        "step": 257
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.049323320388794,
        "learning_rate": 9.782876028956676e-05,
        "epoch": 0.033255993812838364,
        "step": 258
    },
    {
        "loss": 2.2602,
        "grad_norm": 2.237274169921875,
        "learning_rate": 9.781098505030894e-05,
        "epoch": 0.03338489301366331,
        "step": 259
    },
    {
        "loss": 1.8039,
        "grad_norm": 2.5992226600646973,
        "learning_rate": 9.779313897548415e-05,
        "epoch": 0.03351379221448827,
        "step": 260
    },
    {
        "loss": 2.5701,
        "grad_norm": 1.5218071937561035,
        "learning_rate": 9.777522209153271e-05,
        "epoch": 0.033642691415313224,
        "step": 261
    },
    {
        "loss": 1.9609,
        "grad_norm": 1.3065245151519775,
        "learning_rate": 9.775723442499983e-05,
        "epoch": 0.03377159061613818,
        "step": 262
    },
    {
        "loss": 2.1294,
        "grad_norm": 1.8109501600265503,
        "learning_rate": 9.773917600253559e-05,
        "epoch": 0.033900489816963135,
        "step": 263
    },
    {
        "loss": 2.179,
        "grad_norm": 2.6669952869415283,
        "learning_rate": 9.772104685089492e-05,
        "epoch": 0.03402938901778809,
        "step": 264
    },
    {
        "loss": 1.9744,
        "grad_norm": 2.7510385513305664,
        "learning_rate": 9.770284699693748e-05,
        "epoch": 0.034158288218613046,
        "step": 265
    },
    {
        "loss": 2.6102,
        "grad_norm": 1.5634218454360962,
        "learning_rate": 9.768457646762774e-05,
        "epoch": 0.034287187419438,
        "step": 266
    },
    {
        "loss": 2.4129,
        "grad_norm": 1.4433366060256958,
        "learning_rate": 9.766623529003487e-05,
        "epoch": 0.03441608662026296,
        "step": 267
    },
    {
        "loss": 2.3671,
        "grad_norm": 2.1316912174224854,
        "learning_rate": 9.764782349133268e-05,
        "epoch": 0.034544985821087906,
        "step": 268
    },
    {
        "loss": 1.5488,
        "grad_norm": 2.5177133083343506,
        "learning_rate": 9.762934109879966e-05,
        "epoch": 0.03467388502191286,
        "step": 269
    },
    {
        "loss": 2.4184,
        "grad_norm": 1.7109547853469849,
        "learning_rate": 9.761078813981885e-05,
        "epoch": 0.03480278422273782,
        "step": 270
    },
    {
        "loss": 2.8439,
        "grad_norm": 1.5731422901153564,
        "learning_rate": 9.759216464187786e-05,
        "epoch": 0.03493168342356277,
        "step": 271
    },
    {
        "loss": 1.9727,
        "grad_norm": 0.9934974908828735,
        "learning_rate": 9.75734706325688e-05,
        "epoch": 0.03506058262438773,
        "step": 272
    },
    {
        "loss": 2.6392,
        "grad_norm": 1.9993077516555786,
        "learning_rate": 9.755470613958823e-05,
        "epoch": 0.035189481825212685,
        "step": 273
    },
    {
        "loss": 2.3246,
        "grad_norm": 2.2842485904693604,
        "learning_rate": 9.753587119073719e-05,
        "epoch": 0.03531838102603764,
        "step": 274
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.9791264533996582,
        "learning_rate": 9.751696581392105e-05,
        "epoch": 0.035447280226862596,
        "step": 275
    },
    {
        "loss": 1.5804,
        "grad_norm": 2.821307897567749,
        "learning_rate": 9.749799003714954e-05,
        "epoch": 0.03557617942768755,
        "step": 276
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.457247018814087,
        "learning_rate": 9.747894388853674e-05,
        "epoch": 0.0357050786285125,
        "step": 277
    },
    {
        "loss": 1.593,
        "grad_norm": 2.5649397373199463,
        "learning_rate": 9.745982739630089e-05,
        "epoch": 0.035833977829337456,
        "step": 278
    },
    {
        "loss": 2.6462,
        "grad_norm": 1.8921085596084595,
        "learning_rate": 9.744064058876454e-05,
        "epoch": 0.03596287703016241,
        "step": 279
    },
    {
        "loss": 2.6962,
        "grad_norm": 1.9354957342147827,
        "learning_rate": 9.74213834943544e-05,
        "epoch": 0.03609177623098737,
        "step": 280
    },
    {
        "loss": 2.1712,
        "grad_norm": 2.221815586090088,
        "learning_rate": 9.74020561416013e-05,
        "epoch": 0.03622067543181232,
        "step": 281
    },
    {
        "loss": 2.2803,
        "grad_norm": 1.8216747045516968,
        "learning_rate": 9.738265855914013e-05,
        "epoch": 0.03634957463263728,
        "step": 282
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.7584716081619263,
        "learning_rate": 9.73631907757099e-05,
        "epoch": 0.036478473833462234,
        "step": 283
    },
    {
        "loss": 2.1706,
        "grad_norm": 1.7363499402999878,
        "learning_rate": 9.734365282015359e-05,
        "epoch": 0.03660737303428719,
        "step": 284
    },
    {
        "loss": 2.5557,
        "grad_norm": 1.2194007635116577,
        "learning_rate": 9.732404472141814e-05,
        "epoch": 0.036736272235112145,
        "step": 285
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.3214945793151855,
        "learning_rate": 9.730436650855442e-05,
        "epoch": 0.036865171435937094,
        "step": 286
    },
    {
        "loss": 2.313,
        "grad_norm": 1.4495998620986938,
        "learning_rate": 9.72846182107172e-05,
        "epoch": 0.03699407063676205,
        "step": 287
    },
    {
        "loss": 1.3411,
        "grad_norm": 3.029820680618286,
        "learning_rate": 9.726479985716505e-05,
        "epoch": 0.037122969837587005,
        "step": 288
    },
    {
        "loss": 2.417,
        "grad_norm": 1.6333332061767578,
        "learning_rate": 9.724491147726035e-05,
        "epoch": 0.03725186903841196,
        "step": 289
    },
    {
        "loss": 1.7062,
        "grad_norm": 2.6073215007781982,
        "learning_rate": 9.722495310046922e-05,
        "epoch": 0.03738076823923692,
        "step": 290
    },
    {
        "loss": 2.3265,
        "grad_norm": 2.023134469985962,
        "learning_rate": 9.720492475636153e-05,
        "epoch": 0.03750966744006187,
        "step": 291
    },
    {
        "loss": 2.3206,
        "grad_norm": 1.9035934209823608,
        "learning_rate": 9.718482647461075e-05,
        "epoch": 0.03763856664088683,
        "step": 292
    },
    {
        "loss": 2.5959,
        "grad_norm": 1.5020536184310913,
        "learning_rate": 9.7164658284994e-05,
        "epoch": 0.037767465841711784,
        "step": 293
    },
    {
        "loss": 2.4838,
        "grad_norm": 1.4933444261550903,
        "learning_rate": 9.714442021739196e-05,
        "epoch": 0.03789636504253674,
        "step": 294
    },
    {
        "loss": 2.7582,
        "grad_norm": 1.2518495321273804,
        "learning_rate": 9.712411230178887e-05,
        "epoch": 0.03802526424336169,
        "step": 295
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.4277973175048828,
        "learning_rate": 9.710373456827242e-05,
        "epoch": 0.038154163444186644,
        "step": 296
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.670111060142517,
        "learning_rate": 9.708328704703373e-05,
        "epoch": 0.0382830626450116,
        "step": 297
    },
    {
        "loss": 2.1341,
        "grad_norm": 1.8636407852172852,
        "learning_rate": 9.706276976836737e-05,
        "epoch": 0.038411961845836555,
        "step": 298
    },
    {
        "loss": 2.4862,
        "grad_norm": 1.9614264965057373,
        "learning_rate": 9.704218276267123e-05,
        "epoch": 0.03854086104666151,
        "step": 299
    },
    {
        "loss": 2.026,
        "grad_norm": 1.6131043434143066,
        "learning_rate": 9.70215260604465e-05,
        "epoch": 0.038669760247486466,
        "step": 300
    },
    {
        "loss": 2.3852,
        "grad_norm": 1.814569354057312,
        "learning_rate": 9.700079969229762e-05,
        "epoch": 0.03879865944831142,
        "step": 301
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.657699704170227,
        "learning_rate": 9.698000368893228e-05,
        "epoch": 0.03892755864913638,
        "step": 302
    },
    {
        "loss": 1.3294,
        "grad_norm": 3.0344717502593994,
        "learning_rate": 9.695913808116132e-05,
        "epoch": 0.03905645784996133,
        "step": 303
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.663083553314209,
        "learning_rate": 9.69382028998987e-05,
        "epoch": 0.03918535705078628,
        "step": 304
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.7719314098358154,
        "learning_rate": 9.691719817616147e-05,
        "epoch": 0.03931425625161124,
        "step": 305
    },
    {
        "loss": 2.1546,
        "grad_norm": 1.181923508644104,
        "learning_rate": 9.689612394106972e-05,
        "epoch": 0.03944315545243619,
        "step": 306
    },
    {
        "loss": 1.7825,
        "grad_norm": 3.2598488330841064,
        "learning_rate": 9.687498022584646e-05,
        "epoch": 0.03957205465326115,
        "step": 307
    },
    {
        "loss": 2.5596,
        "grad_norm": 1.5765800476074219,
        "learning_rate": 9.685376706181777e-05,
        "epoch": 0.039700953854086105,
        "step": 308
    },
    {
        "loss": 2.9041,
        "grad_norm": 1.4818072319030762,
        "learning_rate": 9.683248448041249e-05,
        "epoch": 0.03982985305491106,
        "step": 309
    },
    {
        "loss": 2.3125,
        "grad_norm": 2.237074613571167,
        "learning_rate": 9.681113251316237e-05,
        "epoch": 0.039958752255736016,
        "step": 310
    },
    {
        "loss": 2.2186,
        "grad_norm": 2.111337661743164,
        "learning_rate": 9.678971119170198e-05,
        "epoch": 0.04008765145656097,
        "step": 311
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.9197864532470703,
        "learning_rate": 9.676822054776857e-05,
        "epoch": 0.04021655065738593,
        "step": 312
    },
    {
        "loss": 2.3745,
        "grad_norm": 2.2739744186401367,
        "learning_rate": 9.674666061320221e-05,
        "epoch": 0.040345449858210876,
        "step": 313
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.2889810800552368,
        "learning_rate": 9.672503141994551e-05,
        "epoch": 0.04047434905903583,
        "step": 314
    },
    {
        "loss": 1.6096,
        "grad_norm": 3.026216983795166,
        "learning_rate": 9.670333300004376e-05,
        "epoch": 0.04060324825986079,
        "step": 315
    },
    {
        "loss": 2.4062,
        "grad_norm": 1.5763920545578003,
        "learning_rate": 9.668156538564481e-05,
        "epoch": 0.04073214746068574,
        "step": 316
    },
    {
        "loss": 2.4844,
        "grad_norm": 2.0726559162139893,
        "learning_rate": 9.6659728608999e-05,
        "epoch": 0.0408610466615107,
        "step": 317
    },
    {
        "loss": 1.6799,
        "grad_norm": 2.291963577270508,
        "learning_rate": 9.663782270245918e-05,
        "epoch": 0.040989945862335654,
        "step": 318
    },
    {
        "loss": 2.4528,
        "grad_norm": 2.296208381652832,
        "learning_rate": 9.661584769848058e-05,
        "epoch": 0.04111884506316061,
        "step": 319
    },
    {
        "loss": 2.1556,
        "grad_norm": 1.63723623752594,
        "learning_rate": 9.65938036296208e-05,
        "epoch": 0.041247744263985565,
        "step": 320
    },
    {
        "loss": 2.5873,
        "grad_norm": 1.4590102434158325,
        "learning_rate": 9.657169052853983e-05,
        "epoch": 0.04137664346481052,
        "step": 321
    },
    {
        "loss": 1.9772,
        "grad_norm": 2.6982474327087402,
        "learning_rate": 9.654950842799984e-05,
        "epoch": 0.04150554266563547,
        "step": 322
    },
    {
        "loss": 2.0518,
        "grad_norm": 3.015923261642456,
        "learning_rate": 9.652725736086532e-05,
        "epoch": 0.041634441866460425,
        "step": 323
    },
    {
        "loss": 2.0658,
        "grad_norm": 2.273228168487549,
        "learning_rate": 9.650493736010289e-05,
        "epoch": 0.04176334106728538,
        "step": 324
    },
    {
        "loss": 2.6901,
        "grad_norm": 2.0841753482818604,
        "learning_rate": 9.648254845878128e-05,
        "epoch": 0.04189224026811034,
        "step": 325
    },
    {
        "loss": 1.5893,
        "grad_norm": 2.9162137508392334,
        "learning_rate": 9.646009069007136e-05,
        "epoch": 0.04202113946893529,
        "step": 326
    },
    {
        "loss": 2.1531,
        "grad_norm": 1.6046844720840454,
        "learning_rate": 9.6437564087246e-05,
        "epoch": 0.04215003866976025,
        "step": 327
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.0929595232009888,
        "learning_rate": 9.641496868368004e-05,
        "epoch": 0.042278937870585204,
        "step": 328
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.8775410652160645,
        "learning_rate": 9.639230451285029e-05,
        "epoch": 0.04240783707141016,
        "step": 329
    },
    {
        "loss": 1.6927,
        "grad_norm": 1.8097190856933594,
        "learning_rate": 9.636957160833538e-05,
        "epoch": 0.042536736272235115,
        "step": 330
    },
    {
        "loss": 2.6693,
        "grad_norm": 1.4291623830795288,
        "learning_rate": 9.634677000381586e-05,
        "epoch": 0.042665635473060064,
        "step": 331
    },
    {
        "loss": 2.4268,
        "grad_norm": 1.6577328443527222,
        "learning_rate": 9.632389973307401e-05,
        "epoch": 0.04279453467388502,
        "step": 332
    },
    {
        "loss": 1.794,
        "grad_norm": 3.2313029766082764,
        "learning_rate": 9.630096082999382e-05,
        "epoch": 0.042923433874709975,
        "step": 333
    },
    {
        "loss": 1.9619,
        "grad_norm": 2.575430393218994,
        "learning_rate": 9.627795332856107e-05,
        "epoch": 0.04305233307553493,
        "step": 334
    },
    {
        "loss": 2.2477,
        "grad_norm": 1.8627889156341553,
        "learning_rate": 9.625487726286302e-05,
        "epoch": 0.043181232276359886,
        "step": 335
    },
    {
        "loss": 2.5042,
        "grad_norm": 1.8470584154129028,
        "learning_rate": 9.623173266708865e-05,
        "epoch": 0.04331013147718484,
        "step": 336
    },
    {
        "loss": 2.1162,
        "grad_norm": 1.913529634475708,
        "learning_rate": 9.62085195755284e-05,
        "epoch": 0.0434390306780098,
        "step": 337
    },
    {
        "loss": 1.5869,
        "grad_norm": 2.1405112743377686,
        "learning_rate": 9.618523802257423e-05,
        "epoch": 0.04356792987883475,
        "step": 338
    },
    {
        "loss": 2.1686,
        "grad_norm": 1.5130025148391724,
        "learning_rate": 9.616188804271948e-05,
        "epoch": 0.04369682907965971,
        "step": 339
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.3191816806793213,
        "learning_rate": 9.613846967055891e-05,
        "epoch": 0.04382572828048466,
        "step": 340
    },
    {
        "loss": 1.1786,
        "grad_norm": 2.40940523147583,
        "learning_rate": 9.61149829407886e-05,
        "epoch": 0.04395462748130961,
        "step": 341
    },
    {
        "loss": 2.5808,
        "grad_norm": 1.969902515411377,
        "learning_rate": 9.60914278882059e-05,
        "epoch": 0.04408352668213457,
        "step": 342
    },
    {
        "loss": 2.496,
        "grad_norm": 1.6017755270004272,
        "learning_rate": 9.60678045477094e-05,
        "epoch": 0.044212425882959525,
        "step": 343
    },
    {
        "loss": 2.3172,
        "grad_norm": 1.3679405450820923,
        "learning_rate": 9.604411295429885e-05,
        "epoch": 0.04434132508378448,
        "step": 344
    },
    {
        "loss": 2.5427,
        "grad_norm": 1.4345529079437256,
        "learning_rate": 9.602035314307513e-05,
        "epoch": 0.044470224284609436,
        "step": 345
    },
    {
        "loss": 1.7818,
        "grad_norm": 2.686872720718384,
        "learning_rate": 9.599652514924018e-05,
        "epoch": 0.04459912348543439,
        "step": 346
    },
    {
        "loss": 2.7272,
        "grad_norm": 1.3875079154968262,
        "learning_rate": 9.597262900809696e-05,
        "epoch": 0.04472802268625935,
        "step": 347
    },
    {
        "loss": 2.0024,
        "grad_norm": 1.3321565389633179,
        "learning_rate": 9.59486647550494e-05,
        "epoch": 0.0448569218870843,
        "step": 348
    },
    {
        "loss": 2.4132,
        "grad_norm": 1.3783018589019775,
        "learning_rate": 9.592463242560235e-05,
        "epoch": 0.04498582108790925,
        "step": 349
    },
    {
        "loss": 2.315,
        "grad_norm": 2.652863025665283,
        "learning_rate": 9.590053205536149e-05,
        "epoch": 0.04511472028873421,
        "step": 350
    },
    {
        "loss": 2.0719,
        "grad_norm": 1.8623536825180054,
        "learning_rate": 9.587636368003337e-05,
        "epoch": 0.04524361948955916,
        "step": 351
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.021742105484009,
        "learning_rate": 9.585212733542521e-05,
        "epoch": 0.04537251869038412,
        "step": 352
    },
    {
        "loss": 2.438,
        "grad_norm": 1.1595548391342163,
        "learning_rate": 9.5827823057445e-05,
        "epoch": 0.045501417891209074,
        "step": 353
    },
    {
        "loss": 1.1284,
        "grad_norm": 1.9100569486618042,
        "learning_rate": 9.580345088210137e-05,
        "epoch": 0.04563031709203403,
        "step": 354
    },
    {
        "loss": 2.4025,
        "grad_norm": 1.6205493211746216,
        "learning_rate": 9.57790108455035e-05,
        "epoch": 0.045759216292858985,
        "step": 355
    },
    {
        "loss": 2.442,
        "grad_norm": 1.3124299049377441,
        "learning_rate": 9.575450298386112e-05,
        "epoch": 0.04588811549368394,
        "step": 356
    },
    {
        "loss": 2.6001,
        "grad_norm": 1.5162785053253174,
        "learning_rate": 9.572992733348453e-05,
        "epoch": 0.0460170146945089,
        "step": 357
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.6407201290130615,
        "learning_rate": 9.570528393078435e-05,
        "epoch": 0.046145913895333845,
        "step": 358
    },
    {
        "loss": 2.5111,
        "grad_norm": 1.6113659143447876,
        "learning_rate": 9.568057281227167e-05,
        "epoch": 0.0462748130961588,
        "step": 359
    },
    {
        "loss": 2.5473,
        "grad_norm": 1.509673833847046,
        "learning_rate": 9.565579401455784e-05,
        "epoch": 0.04640371229698376,
        "step": 360
    },
    {
        "loss": 1.8146,
        "grad_norm": 2.3398165702819824,
        "learning_rate": 9.563094757435453e-05,
        "epoch": 0.04653261149780871,
        "step": 361
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.6707202196121216,
        "learning_rate": 9.560603352847361e-05,
        "epoch": 0.04666151069863367,
        "step": 362
    },
    {
        "loss": 2.4049,
        "grad_norm": 1.99648916721344,
        "learning_rate": 9.55810519138271e-05,
        "epoch": 0.046790409899458624,
        "step": 363
    },
    {
        "loss": 2.8699,
        "grad_norm": 1.0745327472686768,
        "learning_rate": 9.555600276742713e-05,
        "epoch": 0.04691930910028358,
        "step": 364
    },
    {
        "loss": 2.6563,
        "grad_norm": 1.209702968597412,
        "learning_rate": 9.553088612638592e-05,
        "epoch": 0.047048208301108535,
        "step": 365
    },
    {
        "loss": 2.492,
        "grad_norm": 1.6280475854873657,
        "learning_rate": 9.550570202791563e-05,
        "epoch": 0.04717710750193349,
        "step": 366
    },
    {
        "loss": 2.1237,
        "grad_norm": 1.7671549320220947,
        "learning_rate": 9.548045050932841e-05,
        "epoch": 0.047306006702758446,
        "step": 367
    },
    {
        "loss": 2.018,
        "grad_norm": 1.9974989891052246,
        "learning_rate": 9.545513160803629e-05,
        "epoch": 0.047434905903583395,
        "step": 368
    },
    {
        "loss": 1.8634,
        "grad_norm": 2.250450372695923,
        "learning_rate": 9.542974536155111e-05,
        "epoch": 0.04756380510440835,
        "step": 369
    },
    {
        "loss": 2.4174,
        "grad_norm": 1.6146352291107178,
        "learning_rate": 9.540429180748453e-05,
        "epoch": 0.047692704305233306,
        "step": 370
    },
    {
        "loss": 2.6315,
        "grad_norm": 1.1612588167190552,
        "learning_rate": 9.537877098354786e-05,
        "epoch": 0.04782160350605826,
        "step": 371
    },
    {
        "loss": 2.2393,
        "grad_norm": 1.5240767002105713,
        "learning_rate": 9.535318292755214e-05,
        "epoch": 0.04795050270688322,
        "step": 372
    },
    {
        "loss": 1.6827,
        "grad_norm": 2.2384283542633057,
        "learning_rate": 9.5327527677408e-05,
        "epoch": 0.04807940190770817,
        "step": 373
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.3048605918884277,
        "learning_rate": 9.530180527112562e-05,
        "epoch": 0.04820830110853313,
        "step": 374
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.028050422668457,
        "learning_rate": 9.527601574681469e-05,
        "epoch": 0.048337200309358085,
        "step": 375
    },
    {
        "loss": 2.0256,
        "grad_norm": 2.4232680797576904,
        "learning_rate": 9.52501591426843e-05,
        "epoch": 0.04846609951018304,
        "step": 376
    },
    {
        "loss": 1.8614,
        "grad_norm": 1.3419028520584106,
        "learning_rate": 9.522423549704296e-05,
        "epoch": 0.04859499871100799,
        "step": 377
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.0951288938522339,
        "learning_rate": 9.519824484829852e-05,
        "epoch": 0.048723897911832945,
        "step": 378
    },
    {
        "loss": 2.7306,
        "grad_norm": 1.3794870376586914,
        "learning_rate": 9.517218723495805e-05,
        "epoch": 0.0488527971126579,
        "step": 379
    },
    {
        "loss": 2.7384,
        "grad_norm": 2.0583057403564453,
        "learning_rate": 9.514606269562786e-05,
        "epoch": 0.048981696313482856,
        "step": 380
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.091329574584961,
        "learning_rate": 9.511987126901347e-05,
        "epoch": 0.04911059551430781,
        "step": 381
    },
    {
        "loss": 2.6574,
        "grad_norm": 1.4090230464935303,
        "learning_rate": 9.509361299391938e-05,
        "epoch": 0.04923949471513277,
        "step": 382
    },
    {
        "loss": 2.0692,
        "grad_norm": 1.9088473320007324,
        "learning_rate": 9.506728790924926e-05,
        "epoch": 0.04936839391595772,
        "step": 383
    },
    {
        "loss": 2.4009,
        "grad_norm": 2.2572925090789795,
        "learning_rate": 9.504089605400565e-05,
        "epoch": 0.04949729311678268,
        "step": 384
    },
    {
        "loss": 1.6652,
        "grad_norm": 2.243436574935913,
        "learning_rate": 9.501443746729009e-05,
        "epoch": 0.049626192317607634,
        "step": 385
    },
    {
        "loss": 2.7494,
        "grad_norm": 1.4485307931900024,
        "learning_rate": 9.498791218830296e-05,
        "epoch": 0.04975509151843258,
        "step": 386
    },
    {
        "loss": 2.4029,
        "grad_norm": 1.1749118566513062,
        "learning_rate": 9.496132025634347e-05,
        "epoch": 0.04988399071925754,
        "step": 387
    },
    {
        "loss": 2.112,
        "grad_norm": 1.9628181457519531,
        "learning_rate": 9.493466171080954e-05,
        "epoch": 0.050012889920082494,
        "step": 388
    },
    {
        "loss": 1.7698,
        "grad_norm": 3.0891261100769043,
        "learning_rate": 9.49079365911978e-05,
        "epoch": 0.05014178912090745,
        "step": 389
    },
    {
        "loss": 2.4953,
        "grad_norm": 1.9263126850128174,
        "learning_rate": 9.488114493710357e-05,
        "epoch": 0.050270688321732405,
        "step": 390
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.576709508895874,
        "learning_rate": 9.485428678822065e-05,
        "epoch": 0.05039958752255736,
        "step": 391
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.7392348051071167,
        "learning_rate": 9.482736218434143e-05,
        "epoch": 0.05052848672338232,
        "step": 392
    },
    {
        "loss": 1.9994,
        "grad_norm": 2.190497398376465,
        "learning_rate": 9.480037116535674e-05,
        "epoch": 0.05065738592420727,
        "step": 393
    },
    {
        "loss": 2.324,
        "grad_norm": 2.040505886077881,
        "learning_rate": 9.477331377125577e-05,
        "epoch": 0.05078628512503223,
        "step": 394
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.8641031980514526,
        "learning_rate": 9.474619004212612e-05,
        "epoch": 0.05091518432585718,
        "step": 395
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.9479515552520752,
        "learning_rate": 9.471900001815361e-05,
        "epoch": 0.05104408352668213,
        "step": 396
    },
    {
        "loss": 2.0985,
        "grad_norm": 1.9661674499511719,
        "learning_rate": 9.469174373962233e-05,
        "epoch": 0.05117298272750709,
        "step": 397
    },
    {
        "loss": 2.4878,
        "grad_norm": 1.6959284543991089,
        "learning_rate": 9.466442124691447e-05,
        "epoch": 0.051301881928332044,
        "step": 398
    },
    {
        "loss": 2.4153,
        "grad_norm": 1.9756520986557007,
        "learning_rate": 9.463703258051037e-05,
        "epoch": 0.051430781129157,
        "step": 399
    },
    {
        "loss": 2.3398,
        "grad_norm": 2.265204906463623,
        "learning_rate": 9.46095777809884e-05,
        "epoch": 0.051559680329981955,
        "step": 400
    },
    {
        "loss": 2.4099,
        "grad_norm": 1.4454126358032227,
        "learning_rate": 9.458205688902493e-05,
        "epoch": 0.05168857953080691,
        "step": 401
    },
    {
        "loss": 2.5373,
        "grad_norm": 1.1251850128173828,
        "learning_rate": 9.455446994539418e-05,
        "epoch": 0.051817478731631866,
        "step": 402
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.36669659614563,
        "learning_rate": 9.452681699096832e-05,
        "epoch": 0.05194637793245682,
        "step": 403
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.489890217781067,
        "learning_rate": 9.449909806671727e-05,
        "epoch": 0.05207527713328177,
        "step": 404
    },
    {
        "loss": 2.1363,
        "grad_norm": 2.361387252807617,
        "learning_rate": 9.44713132137087e-05,
        "epoch": 0.052204176334106726,
        "step": 405
    },
    {
        "loss": 2.4394,
        "grad_norm": 1.604283094406128,
        "learning_rate": 9.444346247310794e-05,
        "epoch": 0.05233307553493168,
        "step": 406
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.687937617301941,
        "learning_rate": 9.441554588617796e-05,
        "epoch": 0.05246197473575664,
        "step": 407
    },
    {
        "loss": 2.491,
        "grad_norm": 1.607861876487732,
        "learning_rate": 9.43875634942793e-05,
        "epoch": 0.05259087393658159,
        "step": 408
    },
    {
        "loss": 2.4156,
        "grad_norm": 1.162880778312683,
        "learning_rate": 9.435951533886998e-05,
        "epoch": 0.05271977313740655,
        "step": 409
    },
    {
        "loss": 1.8496,
        "grad_norm": 2.6553642749786377,
        "learning_rate": 9.433140146150543e-05,
        "epoch": 0.052848672338231505,
        "step": 410
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.1555755138397217,
        "learning_rate": 9.430322190383845e-05,
        "epoch": 0.05297757153905646,
        "step": 411
    },
    {
        "loss": 2.0048,
        "grad_norm": 1.523267388343811,
        "learning_rate": 9.427497670761923e-05,
        "epoch": 0.053106470739881416,
        "step": 412
    },
    {
        "loss": 2.0976,
        "grad_norm": 1.8847763538360596,
        "learning_rate": 9.424666591469511e-05,
        "epoch": 0.053235369940706365,
        "step": 413
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.624341368675232,
        "learning_rate": 9.421828956701068e-05,
        "epoch": 0.05336426914153132,
        "step": 414
    },
    {
        "loss": 1.6955,
        "grad_norm": 2.678934097290039,
        "learning_rate": 9.41898477066076e-05,
        "epoch": 0.053493168342356276,
        "step": 415
    },
    {
        "loss": 2.4694,
        "grad_norm": 1.3776133060455322,
        "learning_rate": 9.416134037562465e-05,
        "epoch": 0.05362206754318123,
        "step": 416
    },
    {
        "loss": 2.2718,
        "grad_norm": 2.419618606567383,
        "learning_rate": 9.413276761629758e-05,
        "epoch": 0.05375096674400619,
        "step": 417
    },
    {
        "loss": 2.5804,
        "grad_norm": 1.2090818881988525,
        "learning_rate": 9.410412947095907e-05,
        "epoch": 0.05387986594483114,
        "step": 418
    },
    {
        "loss": 2.1608,
        "grad_norm": 1.5255883932113647,
        "learning_rate": 9.407542598203869e-05,
        "epoch": 0.0540087651456561,
        "step": 419
    },
    {
        "loss": 2.232,
        "grad_norm": 1.8447595834732056,
        "learning_rate": 9.404665719206283e-05,
        "epoch": 0.054137664346481054,
        "step": 420
    },
    {
        "loss": 2.2358,
        "grad_norm": 2.0774118900299072,
        "learning_rate": 9.401782314365457e-05,
        "epoch": 0.05426656354730601,
        "step": 421
    },
    {
        "loss": 2.4745,
        "grad_norm": 1.5581930875778198,
        "learning_rate": 9.398892387953376e-05,
        "epoch": 0.05439546274813096,
        "step": 422
    },
    {
        "loss": 2.2938,
        "grad_norm": 1.6839224100112915,
        "learning_rate": 9.395995944251683e-05,
        "epoch": 0.054524361948955914,
        "step": 423
    },
    {
        "loss": 1.5817,
        "grad_norm": 1.6917352676391602,
        "learning_rate": 9.393092987551675e-05,
        "epoch": 0.05465326114978087,
        "step": 424
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.172372579574585,
        "learning_rate": 9.390183522154302e-05,
        "epoch": 0.054782160350605826,
        "step": 425
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.4297983646392822,
        "learning_rate": 9.387267552370155e-05,
        "epoch": 0.05491105955143078,
        "step": 426
    },
    {
        "loss": 2.1174,
        "grad_norm": 2.849191427230835,
        "learning_rate": 9.384345082519462e-05,
        "epoch": 0.05503995875225574,
        "step": 427
    },
    {
        "loss": 2.624,
        "grad_norm": 1.7165379524230957,
        "learning_rate": 9.381416116932083e-05,
        "epoch": 0.05516885795308069,
        "step": 428
    },
    {
        "loss": 2.4556,
        "grad_norm": 1.3598655462265015,
        "learning_rate": 9.378480659947498e-05,
        "epoch": 0.05529775715390565,
        "step": 429
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.5193779468536377,
        "learning_rate": 9.37553871591481e-05,
        "epoch": 0.055426656354730604,
        "step": 430
    },
    {
        "loss": 2.264,
        "grad_norm": 1.5573418140411377,
        "learning_rate": 9.372590289192728e-05,
        "epoch": 0.05555555555555555,
        "step": 431
    },
    {
        "loss": 2.823,
        "grad_norm": 1.602521300315857,
        "learning_rate": 9.36963538414957e-05,
        "epoch": 0.05568445475638051,
        "step": 432
    },
    {
        "loss": 1.727,
        "grad_norm": 2.2536678314208984,
        "learning_rate": 9.366674005163248e-05,
        "epoch": 0.055813353957205464,
        "step": 433
    },
    {
        "loss": 2.3184,
        "grad_norm": 1.9343055486679077,
        "learning_rate": 9.363706156621268e-05,
        "epoch": 0.05594225315803042,
        "step": 434
    },
    {
        "loss": 2.0868,
        "grad_norm": 1.419851303100586,
        "learning_rate": 9.36073184292072e-05,
        "epoch": 0.056071152358855375,
        "step": 435
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.4825047254562378,
        "learning_rate": 9.357751068468275e-05,
        "epoch": 0.05620005155968033,
        "step": 436
    },
    {
        "loss": 2.4166,
        "grad_norm": 1.6691209077835083,
        "learning_rate": 9.354763837680171e-05,
        "epoch": 0.056328950760505286,
        "step": 437
    },
    {
        "loss": 1.3138,
        "grad_norm": 3.2271127700805664,
        "learning_rate": 9.35177015498222e-05,
        "epoch": 0.05645784996133024,
        "step": 438
    },
    {
        "loss": 2.281,
        "grad_norm": 1.4222242832183838,
        "learning_rate": 9.348770024809784e-05,
        "epoch": 0.0565867491621552,
        "step": 439
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.7165552377700806,
        "learning_rate": 9.345763451607782e-05,
        "epoch": 0.056715648362980146,
        "step": 440
    },
    {
        "loss": 2.217,
        "grad_norm": 1.8347240686416626,
        "learning_rate": 9.342750439830678e-05,
        "epoch": 0.0568445475638051,
        "step": 441
    },
    {
        "loss": 2.4303,
        "grad_norm": 1.5822629928588867,
        "learning_rate": 9.339730993942477e-05,
        "epoch": 0.05697344676463006,
        "step": 442
    },
    {
        "loss": 1.7879,
        "grad_norm": 2.8130714893341064,
        "learning_rate": 9.336705118416714e-05,
        "epoch": 0.05710234596545501,
        "step": 443
    },
    {
        "loss": 2.084,
        "grad_norm": 2.6232147216796875,
        "learning_rate": 9.333672817736448e-05,
        "epoch": 0.05723124516627997,
        "step": 444
    },
    {
        "loss": 2.2805,
        "grad_norm": 2.5271189212799072,
        "learning_rate": 9.330634096394264e-05,
        "epoch": 0.057360144367104925,
        "step": 445
    },
    {
        "loss": 2.0968,
        "grad_norm": 2.2469089031219482,
        "learning_rate": 9.327588958892255e-05,
        "epoch": 0.05748904356792988,
        "step": 446
    },
    {
        "loss": 1.8234,
        "grad_norm": 2.827892541885376,
        "learning_rate": 9.324537409742023e-05,
        "epoch": 0.057617942768754836,
        "step": 447
    },
    {
        "loss": 2.3598,
        "grad_norm": 1.651464581489563,
        "learning_rate": 9.321479453464663e-05,
        "epoch": 0.05774684196957979,
        "step": 448
    },
    {
        "loss": 1.7632,
        "grad_norm": 2.5707573890686035,
        "learning_rate": 9.318415094590769e-05,
        "epoch": 0.05787574117040474,
        "step": 449
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.8522965908050537,
        "learning_rate": 9.315344337660421e-05,
        "epoch": 0.058004640371229696,
        "step": 450
    },
    {
        "loss": 1.6071,
        "grad_norm": 2.226895570755005,
        "learning_rate": 9.312267187223174e-05,
        "epoch": 0.05813353957205465,
        "step": 451
    },
    {
        "loss": 1.7448,
        "grad_norm": 2.4319636821746826,
        "learning_rate": 9.309183647838057e-05,
        "epoch": 0.05826243877287961,
        "step": 452
    },
    {
        "loss": 1.9821,
        "grad_norm": 1.6293175220489502,
        "learning_rate": 9.306093724073568e-05,
        "epoch": 0.05839133797370456,
        "step": 453
    },
    {
        "loss": 2.2627,
        "grad_norm": 2.098250389099121,
        "learning_rate": 9.302997420507659e-05,
        "epoch": 0.05852023717452952,
        "step": 454
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.2961080074310303,
        "learning_rate": 9.299894741727737e-05,
        "epoch": 0.058649136375354474,
        "step": 455
    },
    {
        "loss": 2.1571,
        "grad_norm": 2.041076183319092,
        "learning_rate": 9.296785692330654e-05,
        "epoch": 0.05877803557617943,
        "step": 456
    },
    {
        "loss": 2.4887,
        "grad_norm": 1.402403473854065,
        "learning_rate": 9.293670276922699e-05,
        "epoch": 0.058906934777004386,
        "step": 457
    },
    {
        "loss": 1.2393,
        "grad_norm": 1.1014536619186401,
        "learning_rate": 9.290548500119595e-05,
        "epoch": 0.059035833977829334,
        "step": 458
    },
    {
        "loss": 2.0989,
        "grad_norm": 1.731357455253601,
        "learning_rate": 9.287420366546488e-05,
        "epoch": 0.05916473317865429,
        "step": 459
    },
    {
        "loss": 2.0549,
        "grad_norm": 2.251434087753296,
        "learning_rate": 9.284285880837946e-05,
        "epoch": 0.059293632379479246,
        "step": 460
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.8697495460510254,
        "learning_rate": 9.281145047637943e-05,
        "epoch": 0.0594225315803042,
        "step": 461
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.411259651184082,
        "learning_rate": 9.27799787159986e-05,
        "epoch": 0.05955143078112916,
        "step": 462
    },
    {
        "loss": 1.9916,
        "grad_norm": 1.7556284666061401,
        "learning_rate": 9.274844357386472e-05,
        "epoch": 0.05968032998195411,
        "step": 463
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.201381802558899,
        "learning_rate": 9.27168450966995e-05,
        "epoch": 0.05980922918277907,
        "step": 464
    },
    {
        "loss": 1.6878,
        "grad_norm": 2.7294559478759766,
        "learning_rate": 9.268518333131845e-05,
        "epoch": 0.059938128383604024,
        "step": 465
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.4332354068756104,
        "learning_rate": 9.265345832463087e-05,
        "epoch": 0.06006702758442898,
        "step": 466
    },
    {
        "loss": 1.8128,
        "grad_norm": 1.9292871952056885,
        "learning_rate": 9.262167012363972e-05,
        "epoch": 0.06019592678525393,
        "step": 467
    },
    {
        "loss": 2.4455,
        "grad_norm": 2.1466774940490723,
        "learning_rate": 9.258981877544162e-05,
        "epoch": 0.060324825986078884,
        "step": 468
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.403258204460144,
        "learning_rate": 9.255790432722672e-05,
        "epoch": 0.06045372518690384,
        "step": 469
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.808660864830017,
        "learning_rate": 9.252592682627871e-05,
        "epoch": 0.060582624387728795,
        "step": 470
    },
    {
        "loss": 2.233,
        "grad_norm": 1.4602898359298706,
        "learning_rate": 9.249388631997462e-05,
        "epoch": 0.06071152358855375,
        "step": 471
    },
    {
        "loss": 2.4667,
        "grad_norm": 1.7726974487304688,
        "learning_rate": 9.246178285578489e-05,
        "epoch": 0.060840422789378706,
        "step": 472
    },
    {
        "loss": 2.3714,
        "grad_norm": 1.778955340385437,
        "learning_rate": 9.242961648127321e-05,
        "epoch": 0.06096932199020366,
        "step": 473
    },
    {
        "loss": 1.5934,
        "grad_norm": 2.632794141769409,
        "learning_rate": 9.239738724409648e-05,
        "epoch": 0.06109822119102862,
        "step": 474
    },
    {
        "loss": 2.3278,
        "grad_norm": 1.4728636741638184,
        "learning_rate": 9.236509519200472e-05,
        "epoch": 0.06122712039185357,
        "step": 475
    },
    {
        "loss": 2.6355,
        "grad_norm": 1.8674789667129517,
        "learning_rate": 9.233274037284106e-05,
        "epoch": 0.06135601959267852,
        "step": 476
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.3243725299835205,
        "learning_rate": 9.230032283454159e-05,
        "epoch": 0.06148491879350348,
        "step": 477
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.8842741250991821,
        "learning_rate": 9.22678426251353e-05,
        "epoch": 0.06161381799432843,
        "step": 478
    },
    {
        "loss": 2.3709,
        "grad_norm": 1.1856473684310913,
        "learning_rate": 9.22352997927441e-05,
        "epoch": 0.06174271719515339,
        "step": 479
    },
    {
        "loss": 1.6384,
        "grad_norm": 2.520911931991577,
        "learning_rate": 9.220269438558263e-05,
        "epoch": 0.061871616395978345,
        "step": 480
    },
    {
        "loss": 2.4716,
        "grad_norm": 1.755144476890564,
        "learning_rate": 9.217002645195824e-05,
        "epoch": 0.0620005155968033,
        "step": 481
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.296893835067749,
        "learning_rate": 9.213729604027093e-05,
        "epoch": 0.062129414797628256,
        "step": 482
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.560232400894165,
        "learning_rate": 9.210450319901327e-05,
        "epoch": 0.06225831399845321,
        "step": 483
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.7732224464416504,
        "learning_rate": 9.207164797677031e-05,
        "epoch": 0.06238721319927817,
        "step": 484
    },
    {
        "loss": 1.9579,
        "grad_norm": 1.9525364637374878,
        "learning_rate": 9.203873042221955e-05,
        "epoch": 0.06251611240010312,
        "step": 485
    },
    {
        "loss": 2.4348,
        "grad_norm": 2.03847074508667,
        "learning_rate": 9.20057505841308e-05,
        "epoch": 0.06264501160092807,
        "step": 486
    },
    {
        "loss": 2.008,
        "grad_norm": 2.2658331394195557,
        "learning_rate": 9.197270851136617e-05,
        "epoch": 0.06277391080175303,
        "step": 487
    },
    {
        "loss": 2.1582,
        "grad_norm": 1.937638282775879,
        "learning_rate": 9.193960425287999e-05,
        "epoch": 0.06290281000257798,
        "step": 488
    },
    {
        "loss": 2.3337,
        "grad_norm": 2.1500349044799805,
        "learning_rate": 9.19064378577187e-05,
        "epoch": 0.06303170920340294,
        "step": 489
    },
    {
        "loss": 2.5643,
        "grad_norm": 1.7134358882904053,
        "learning_rate": 9.18732093750208e-05,
        "epoch": 0.0631606084042279,
        "step": 490
    },
    {
        "loss": 2.5073,
        "grad_norm": 1.4555717706680298,
        "learning_rate": 9.183991885401677e-05,
        "epoch": 0.06328950760505285,
        "step": 491
    },
    {
        "loss": 1.4726,
        "grad_norm": 2.7847445011138916,
        "learning_rate": 9.180656634402904e-05,
        "epoch": 0.0634184068058778,
        "step": 492
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.443306803703308,
        "learning_rate": 9.177315189447187e-05,
        "epoch": 0.06354730600670276,
        "step": 493
    },
    {
        "loss": 2.3399,
        "grad_norm": 1.7240068912506104,
        "learning_rate": 9.173967555485126e-05,
        "epoch": 0.06367620520752772,
        "step": 494
    },
    {
        "loss": 2.6319,
        "grad_norm": 1.6369543075561523,
        "learning_rate": 9.170613737476492e-05,
        "epoch": 0.06380510440835267,
        "step": 495
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.534576654434204,
        "learning_rate": 9.167253740390218e-05,
        "epoch": 0.06393400360917763,
        "step": 496
    },
    {
        "loss": 2.2299,
        "grad_norm": 2.1669576168060303,
        "learning_rate": 9.163887569204395e-05,
        "epoch": 0.06406290281000258,
        "step": 497
    },
    {
        "loss": 2.0048,
        "grad_norm": 2.020831346511841,
        "learning_rate": 9.160515228906255e-05,
        "epoch": 0.06419180201082754,
        "step": 498
    },
    {
        "loss": 2.5383,
        "grad_norm": 1.8344364166259766,
        "learning_rate": 9.157136724492176e-05,
        "epoch": 0.06432070121165248,
        "step": 499
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.7599543333053589,
        "learning_rate": 9.153752060967664e-05,
        "epoch": 0.06444960041247744,
        "step": 500
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.1561248302459717,
        "learning_rate": 9.150361243347354e-05,
        "epoch": 0.06457849961330239,
        "step": 501
    },
    {
        "loss": 2.706,
        "grad_norm": 1.5776032209396362,
        "learning_rate": 9.146964276654999e-05,
        "epoch": 0.06470739881412735,
        "step": 502
    },
    {
        "loss": 1.7437,
        "grad_norm": 2.806159257888794,
        "learning_rate": 9.143561165923455e-05,
        "epoch": 0.0648362980149523,
        "step": 503
    },
    {
        "loss": 1.3315,
        "grad_norm": 2.7131636142730713,
        "learning_rate": 9.140151916194693e-05,
        "epoch": 0.06496519721577726,
        "step": 504
    },
    {
        "loss": 2.2633,
        "grad_norm": 2.4285755157470703,
        "learning_rate": 9.136736532519766e-05,
        "epoch": 0.06509409641660222,
        "step": 505
    },
    {
        "loss": 2.3408,
        "grad_norm": 2.301974296569824,
        "learning_rate": 9.133315019958826e-05,
        "epoch": 0.06522299561742717,
        "step": 506
    },
    {
        "loss": 2.1888,
        "grad_norm": 2.142595052719116,
        "learning_rate": 9.1298873835811e-05,
        "epoch": 0.06535189481825213,
        "step": 507
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.5697988271713257,
        "learning_rate": 9.126453628464888e-05,
        "epoch": 0.06548079401907708,
        "step": 508
    },
    {
        "loss": 2.397,
        "grad_norm": 2.1886491775512695,
        "learning_rate": 9.123013759697557e-05,
        "epoch": 0.06560969321990204,
        "step": 509
    },
    {
        "loss": 1.8392,
        "grad_norm": 2.629406452178955,
        "learning_rate": 9.119567782375529e-05,
        "epoch": 0.065738592420727,
        "step": 510
    },
    {
        "loss": 2.2838,
        "grad_norm": 2.009246587753296,
        "learning_rate": 9.116115701604282e-05,
        "epoch": 0.06586749162155195,
        "step": 511
    },
    {
        "loss": 2.2679,
        "grad_norm": 1.6659449338912964,
        "learning_rate": 9.11265752249833e-05,
        "epoch": 0.0659963908223769,
        "step": 512
    },
    {
        "loss": 2.5298,
        "grad_norm": 1.6620725393295288,
        "learning_rate": 9.109193250181227e-05,
        "epoch": 0.06612529002320186,
        "step": 513
    },
    {
        "loss": 1.7148,
        "grad_norm": 2.3409671783447266,
        "learning_rate": 9.105722889785551e-05,
        "epoch": 0.06625418922402682,
        "step": 514
    },
    {
        "loss": 2.4928,
        "grad_norm": 1.8772987127304077,
        "learning_rate": 9.102246446452905e-05,
        "epoch": 0.06638308842485177,
        "step": 515
    },
    {
        "loss": 2.1736,
        "grad_norm": 2.0818631649017334,
        "learning_rate": 9.098763925333896e-05,
        "epoch": 0.06651198762567673,
        "step": 516
    },
    {
        "loss": 1.7031,
        "grad_norm": 2.242292642593384,
        "learning_rate": 9.095275331588146e-05,
        "epoch": 0.06664088682650167,
        "step": 517
    },
    {
        "loss": 1.9738,
        "grad_norm": 1.5281611680984497,
        "learning_rate": 9.091780670384264e-05,
        "epoch": 0.06676978602732662,
        "step": 518
    },
    {
        "loss": 1.7126,
        "grad_norm": 2.530128240585327,
        "learning_rate": 9.088279946899859e-05,
        "epoch": 0.06689868522815158,
        "step": 519
    },
    {
        "loss": 2.4848,
        "grad_norm": 1.9515833854675293,
        "learning_rate": 9.084773166321509e-05,
        "epoch": 0.06702758442897654,
        "step": 520
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.1245496273040771,
        "learning_rate": 9.081260333844776e-05,
        "epoch": 0.06715648362980149,
        "step": 521
    },
    {
        "loss": 2.3015,
        "grad_norm": 1.8334085941314697,
        "learning_rate": 9.077741454674187e-05,
        "epoch": 0.06728538283062645,
        "step": 522
    },
    {
        "loss": 2.4524,
        "grad_norm": 1.2597817182540894,
        "learning_rate": 9.074216534023225e-05,
        "epoch": 0.0674142820314514,
        "step": 523
    },
    {
        "loss": 2.3528,
        "grad_norm": 1.5394551753997803,
        "learning_rate": 9.070685577114323e-05,
        "epoch": 0.06754318123227636,
        "step": 524
    },
    {
        "loss": 1.7824,
        "grad_norm": 2.3107101917266846,
        "learning_rate": 9.067148589178862e-05,
        "epoch": 0.06767208043310131,
        "step": 525
    },
    {
        "loss": 2.3598,
        "grad_norm": 1.5131829977035522,
        "learning_rate": 9.063605575457152e-05,
        "epoch": 0.06780097963392627,
        "step": 526
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.2095859050750732,
        "learning_rate": 9.060056541198436e-05,
        "epoch": 0.06792987883475123,
        "step": 527
    },
    {
        "loss": 2.3267,
        "grad_norm": 1.149748682975769,
        "learning_rate": 9.056501491660876e-05,
        "epoch": 0.06805877803557618,
        "step": 528
    },
    {
        "loss": 2.4943,
        "grad_norm": 1.2160695791244507,
        "learning_rate": 9.052940432111542e-05,
        "epoch": 0.06818767723640114,
        "step": 529
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.6274105310440063,
        "learning_rate": 9.049373367826413e-05,
        "epoch": 0.06831657643722609,
        "step": 530
    },
    {
        "loss": 2.4196,
        "grad_norm": 1.6123126745224,
        "learning_rate": 9.045800304090364e-05,
        "epoch": 0.06844547563805105,
        "step": 531
    },
    {
        "loss": 2.4992,
        "grad_norm": 1.8094767332077026,
        "learning_rate": 9.042221246197154e-05,
        "epoch": 0.068574374838876,
        "step": 532
    },
    {
        "loss": 2.0323,
        "grad_norm": 2.0823042392730713,
        "learning_rate": 9.03863619944943e-05,
        "epoch": 0.06870327403970096,
        "step": 533
    },
    {
        "loss": 1.4115,
        "grad_norm": 2.872305154800415,
        "learning_rate": 9.035045169158707e-05,
        "epoch": 0.06883217324052592,
        "step": 534
    },
    {
        "loss": 2.4036,
        "grad_norm": 1.7609903812408447,
        "learning_rate": 9.031448160645365e-05,
        "epoch": 0.06896107244135086,
        "step": 535
    },
    {
        "loss": 2.3888,
        "grad_norm": 1.4458295106887817,
        "learning_rate": 9.027845179238643e-05,
        "epoch": 0.06908997164217581,
        "step": 536
    },
    {
        "loss": 1.8495,
        "grad_norm": 2.4248552322387695,
        "learning_rate": 9.024236230276629e-05,
        "epoch": 0.06921887084300077,
        "step": 537
    },
    {
        "loss": 1.6535,
        "grad_norm": 2.270966053009033,
        "learning_rate": 9.020621319106251e-05,
        "epoch": 0.06934777004382572,
        "step": 538
    },
    {
        "loss": 1.6995,
        "grad_norm": 2.7141921520233154,
        "learning_rate": 9.017000451083275e-05,
        "epoch": 0.06947666924465068,
        "step": 539
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.14737868309021,
        "learning_rate": 9.013373631572284e-05,
        "epoch": 0.06960556844547564,
        "step": 540
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.0721566677093506,
        "learning_rate": 9.009740865946688e-05,
        "epoch": 0.06973446764630059,
        "step": 541
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.219848155975342,
        "learning_rate": 9.006102159588697e-05,
        "epoch": 0.06986336684712555,
        "step": 542
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.432917833328247,
        "learning_rate": 9.002457517889333e-05,
        "epoch": 0.0699922660479505,
        "step": 543
    },
    {
        "loss": 2.2743,
        "grad_norm": 1.5208840370178223,
        "learning_rate": 8.998806946248406e-05,
        "epoch": 0.07012116524877546,
        "step": 544
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.946208119392395,
        "learning_rate": 8.99515045007451e-05,
        "epoch": 0.07025006444960041,
        "step": 545
    },
    {
        "loss": 2.1992,
        "grad_norm": 1.9932023286819458,
        "learning_rate": 8.991488034785021e-05,
        "epoch": 0.07037896365042537,
        "step": 546
    },
    {
        "loss": 2.0638,
        "grad_norm": 2.371025562286377,
        "learning_rate": 8.98781970580608e-05,
        "epoch": 0.07050786285125032,
        "step": 547
    },
    {
        "loss": 2.504,
        "grad_norm": 1.692246675491333,
        "learning_rate": 8.984145468572593e-05,
        "epoch": 0.07063676205207528,
        "step": 548
    },
    {
        "loss": 2.4157,
        "grad_norm": 1.8112211227416992,
        "learning_rate": 8.980465328528219e-05,
        "epoch": 0.07076566125290024,
        "step": 549
    },
    {
        "loss": 2.2898,
        "grad_norm": 2.2908637523651123,
        "learning_rate": 8.976779291125362e-05,
        "epoch": 0.07089456045372519,
        "step": 550
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.3959906101226807,
        "learning_rate": 8.973087361825163e-05,
        "epoch": 0.07102345965455015,
        "step": 551
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.5229893922805786,
        "learning_rate": 8.969389546097493e-05,
        "epoch": 0.0711523588553751,
        "step": 552
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.3232121467590332,
        "learning_rate": 8.965685849420945e-05,
        "epoch": 0.07128125805620004,
        "step": 553
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.2496916055679321,
        "learning_rate": 8.961976277282821e-05,
        "epoch": 0.071410157257025,
        "step": 554
    },
    {
        "loss": 2.3561,
        "grad_norm": 1.8373717069625854,
        "learning_rate": 8.958260835179134e-05,
        "epoch": 0.07153905645784996,
        "step": 555
    },
    {
        "loss": 2.3791,
        "grad_norm": 1.6674946546554565,
        "learning_rate": 8.954539528614587e-05,
        "epoch": 0.07166795565867491,
        "step": 556
    },
    {
        "loss": 2.4446,
        "grad_norm": 2.1987149715423584,
        "learning_rate": 8.950812363102581e-05,
        "epoch": 0.07179685485949987,
        "step": 557
    },
    {
        "loss": 2.2205,
        "grad_norm": 2.599134683609009,
        "learning_rate": 8.947079344165185e-05,
        "epoch": 0.07192575406032482,
        "step": 558
    },
    {
        "loss": 2.2219,
        "grad_norm": 2.212181329727173,
        "learning_rate": 8.943340477333152e-05,
        "epoch": 0.07205465326114978,
        "step": 559
    },
    {
        "loss": 1.9621,
        "grad_norm": 3.162838935852051,
        "learning_rate": 8.939595768145892e-05,
        "epoch": 0.07218355246197473,
        "step": 560
    },
    {
        "loss": 1.6487,
        "grad_norm": 2.1587982177734375,
        "learning_rate": 8.935845222151475e-05,
        "epoch": 0.07231245166279969,
        "step": 561
    },
    {
        "loss": 1.8215,
        "grad_norm": 2.3363826274871826,
        "learning_rate": 8.932088844906614e-05,
        "epoch": 0.07244135086362465,
        "step": 562
    },
    {
        "loss": 2.0596,
        "grad_norm": 1.8768086433410645,
        "learning_rate": 8.928326641976665e-05,
        "epoch": 0.0725702500644496,
        "step": 563
    },
    {
        "loss": 2.429,
        "grad_norm": 1.450218915939331,
        "learning_rate": 8.924558618935615e-05,
        "epoch": 0.07269914926527456,
        "step": 564
    },
    {
        "loss": 2.0526,
        "grad_norm": 2.237794876098633,
        "learning_rate": 8.920784781366072e-05,
        "epoch": 0.07282804846609951,
        "step": 565
    },
    {
        "loss": 2.4107,
        "grad_norm": 1.4859719276428223,
        "learning_rate": 8.917005134859263e-05,
        "epoch": 0.07295694766692447,
        "step": 566
    },
    {
        "loss": 2.6574,
        "grad_norm": 2.324509620666504,
        "learning_rate": 8.913219685015015e-05,
        "epoch": 0.07308584686774942,
        "step": 567
    },
    {
        "loss": 2.399,
        "grad_norm": 1.9348770380020142,
        "learning_rate": 8.909428437441758e-05,
        "epoch": 0.07321474606857438,
        "step": 568
    },
    {
        "loss": 2.2068,
        "grad_norm": 1.7485933303833008,
        "learning_rate": 8.905631397756511e-05,
        "epoch": 0.07334364526939934,
        "step": 569
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.864535927772522,
        "learning_rate": 8.90182857158487e-05,
        "epoch": 0.07347254447022429,
        "step": 570
    },
    {
        "loss": 2.2814,
        "grad_norm": 2.008859157562256,
        "learning_rate": 8.898019964561013e-05,
        "epoch": 0.07360144367104923,
        "step": 571
    },
    {
        "loss": 1.7827,
        "grad_norm": 1.865267276763916,
        "learning_rate": 8.894205582327672e-05,
        "epoch": 0.07373034287187419,
        "step": 572
    },
    {
        "loss": 2.212,
        "grad_norm": 2.023461103439331,
        "learning_rate": 8.890385430536145e-05,
        "epoch": 0.07385924207269914,
        "step": 573
    },
    {
        "loss": 2.2108,
        "grad_norm": 2.0034546852111816,
        "learning_rate": 8.886559514846272e-05,
        "epoch": 0.0739881412735241,
        "step": 574
    },
    {
        "loss": 2.2348,
        "grad_norm": 1.39994478225708,
        "learning_rate": 8.882727840926435e-05,
        "epoch": 0.07411704047434906,
        "step": 575
    },
    {
        "loss": 2.5037,
        "grad_norm": 1.3822029829025269,
        "learning_rate": 8.878890414453546e-05,
        "epoch": 0.07424593967517401,
        "step": 576
    },
    {
        "loss": 2.4085,
        "grad_norm": 1.7318235635757446,
        "learning_rate": 8.875047241113044e-05,
        "epoch": 0.07437483887599897,
        "step": 577
    },
    {
        "loss": 2.196,
        "grad_norm": 2.6956517696380615,
        "learning_rate": 8.871198326598876e-05,
        "epoch": 0.07450373807682392,
        "step": 578
    },
    {
        "loss": 1.9092,
        "grad_norm": 1.6302870512008667,
        "learning_rate": 8.867343676613497e-05,
        "epoch": 0.07463263727764888,
        "step": 579
    },
    {
        "loss": 2.5272,
        "grad_norm": 1.176032304763794,
        "learning_rate": 8.863483296867863e-05,
        "epoch": 0.07476153647847383,
        "step": 580
    },
    {
        "loss": 2.5162,
        "grad_norm": 1.860518217086792,
        "learning_rate": 8.859617193081417e-05,
        "epoch": 0.07489043567929879,
        "step": 581
    },
    {
        "loss": 1.9758,
        "grad_norm": 2.1782455444335938,
        "learning_rate": 8.85574537098208e-05,
        "epoch": 0.07501933488012374,
        "step": 582
    },
    {
        "loss": 2.6117,
        "grad_norm": 1.5902966260910034,
        "learning_rate": 8.85186783630625e-05,
        "epoch": 0.0751482340809487,
        "step": 583
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.07615327835083,
        "learning_rate": 8.847984594798783e-05,
        "epoch": 0.07527713328177366,
        "step": 584
    },
    {
        "loss": 1.4801,
        "grad_norm": 2.1213080883026123,
        "learning_rate": 8.844095652212995e-05,
        "epoch": 0.07540603248259861,
        "step": 585
    },
    {
        "loss": 1.668,
        "grad_norm": 2.382014274597168,
        "learning_rate": 8.840201014310647e-05,
        "epoch": 0.07553493168342357,
        "step": 586
    },
    {
        "loss": 2.2641,
        "grad_norm": 1.3793401718139648,
        "learning_rate": 8.836300686861936e-05,
        "epoch": 0.07566383088424852,
        "step": 587
    },
    {
        "loss": 1.8651,
        "grad_norm": 2.0150978565216064,
        "learning_rate": 8.832394675645492e-05,
        "epoch": 0.07579273008507348,
        "step": 588
    },
    {
        "loss": 1.7729,
        "grad_norm": 1.9782428741455078,
        "learning_rate": 8.828482986448363e-05,
        "epoch": 0.07592162928589843,
        "step": 589
    },
    {
        "loss": 2.1309,
        "grad_norm": 1.8728978633880615,
        "learning_rate": 8.824565625066013e-05,
        "epoch": 0.07605052848672338,
        "step": 590
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.297304391860962,
        "learning_rate": 8.820642597302302e-05,
        "epoch": 0.07617942768754833,
        "step": 591
    },
    {
        "loss": 2.2253,
        "grad_norm": 1.8170344829559326,
        "learning_rate": 8.816713908969496e-05,
        "epoch": 0.07630832688837329,
        "step": 592
    },
    {
        "loss": 1.772,
        "grad_norm": 2.210812568664551,
        "learning_rate": 8.812779565888237e-05,
        "epoch": 0.07643722608919824,
        "step": 593
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.748308539390564,
        "learning_rate": 8.808839573887554e-05,
        "epoch": 0.0765661252900232,
        "step": 594
    },
    {
        "loss": 2.3752,
        "grad_norm": 1.5260258913040161,
        "learning_rate": 8.80489393880484e-05,
        "epoch": 0.07669502449084815,
        "step": 595
    },
    {
        "loss": 2.4537,
        "grad_norm": 1.5085119009017944,
        "learning_rate": 8.800942666485849e-05,
        "epoch": 0.07682392369167311,
        "step": 596
    },
    {
        "loss": 2.2313,
        "grad_norm": 1.3042391538619995,
        "learning_rate": 8.796985762784686e-05,
        "epoch": 0.07695282289249807,
        "step": 597
    },
    {
        "loss": 2.2241,
        "grad_norm": 2.0079050064086914,
        "learning_rate": 8.793023233563807e-05,
        "epoch": 0.07708172209332302,
        "step": 598
    },
    {
        "loss": 2.3721,
        "grad_norm": 2.002418279647827,
        "learning_rate": 8.789055084693991e-05,
        "epoch": 0.07721062129414798,
        "step": 599
    },
    {
        "loss": 1.8038,
        "grad_norm": 1.8807215690612793,
        "learning_rate": 8.785081322054351e-05,
        "epoch": 0.07733952049497293,
        "step": 600
    },
    {
        "loss": 2.0712,
        "grad_norm": 2.0803298950195312,
        "learning_rate": 8.781101951532316e-05,
        "epoch": 0.07746841969579789,
        "step": 601
    },
    {
        "loss": 2.3813,
        "grad_norm": 1.9198945760726929,
        "learning_rate": 8.77711697902362e-05,
        "epoch": 0.07759731889662284,
        "step": 602
    },
    {
        "loss": 2.1513,
        "grad_norm": 1.0706818103790283,
        "learning_rate": 8.773126410432302e-05,
        "epoch": 0.0777262180974478,
        "step": 603
    },
    {
        "loss": 2.526,
        "grad_norm": 1.711478352546692,
        "learning_rate": 8.769130251670687e-05,
        "epoch": 0.07785511729827276,
        "step": 604
    },
    {
        "loss": 2.0325,
        "grad_norm": 3.0964155197143555,
        "learning_rate": 8.765128508659386e-05,
        "epoch": 0.07798401649909771,
        "step": 605
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.4179071187973022,
        "learning_rate": 8.761121187327282e-05,
        "epoch": 0.07811291569992267,
        "step": 606
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.2731547355651855,
        "learning_rate": 8.75710829361152e-05,
        "epoch": 0.07824181490074762,
        "step": 607
    },
    {
        "loss": 2.2973,
        "grad_norm": 1.9576278924942017,
        "learning_rate": 8.753089833457506e-05,
        "epoch": 0.07837071410157256,
        "step": 608
    },
    {
        "loss": 2.5478,
        "grad_norm": 1.1850392818450928,
        "learning_rate": 8.749065812818891e-05,
        "epoch": 0.07849961330239752,
        "step": 609
    },
    {
        "loss": 2.0875,
        "grad_norm": 2.336052417755127,
        "learning_rate": 8.745036237657562e-05,
        "epoch": 0.07862851250322248,
        "step": 610
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.9087378978729248,
        "learning_rate": 8.74100111394364e-05,
        "epoch": 0.07875741170404743,
        "step": 611
    },
    {
        "loss": 2.7058,
        "grad_norm": 1.8338688611984253,
        "learning_rate": 8.736960447655461e-05,
        "epoch": 0.07888631090487239,
        "step": 612
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.4071168899536133,
        "learning_rate": 8.732914244779578e-05,
        "epoch": 0.07901521010569734,
        "step": 613
    },
    {
        "loss": 2.1688,
        "grad_norm": 2.052396535873413,
        "learning_rate": 8.728862511310744e-05,
        "epoch": 0.0791441093065223,
        "step": 614
    },
    {
        "loss": 2.081,
        "grad_norm": 1.9693762063980103,
        "learning_rate": 8.724805253251905e-05,
        "epoch": 0.07927300850734725,
        "step": 615
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.373246431350708,
        "learning_rate": 8.720742476614197e-05,
        "epoch": 0.07940190770817221,
        "step": 616
    },
    {
        "loss": 1.4682,
        "grad_norm": 2.913011074066162,
        "learning_rate": 8.716674187416927e-05,
        "epoch": 0.07953080690899716,
        "step": 617
    },
    {
        "loss": 2.4204,
        "grad_norm": 1.084313154220581,
        "learning_rate": 8.712600391687569e-05,
        "epoch": 0.07965970610982212,
        "step": 618
    },
    {
        "loss": 2.4974,
        "grad_norm": 1.5722956657409668,
        "learning_rate": 8.708521095461761e-05,
        "epoch": 0.07978860531064708,
        "step": 619
    },
    {
        "loss": 1.8074,
        "grad_norm": 2.369406223297119,
        "learning_rate": 8.704436304783286e-05,
        "epoch": 0.07991750451147203,
        "step": 620
    },
    {
        "loss": 2.0298,
        "grad_norm": 2.1177761554718018,
        "learning_rate": 8.700346025704067e-05,
        "epoch": 0.08004640371229699,
        "step": 621
    },
    {
        "loss": 2.2794,
        "grad_norm": 2.158906936645508,
        "learning_rate": 8.69625026428416e-05,
        "epoch": 0.08017530291312194,
        "step": 622
    },
    {
        "loss": 2.3993,
        "grad_norm": 1.9123800992965698,
        "learning_rate": 8.692149026591745e-05,
        "epoch": 0.0803042021139469,
        "step": 623
    },
    {
        "loss": 2.5322,
        "grad_norm": 1.7379789352416992,
        "learning_rate": 8.688042318703111e-05,
        "epoch": 0.08043310131477185,
        "step": 624
    },
    {
        "loss": 1.8378,
        "grad_norm": 2.3199212551116943,
        "learning_rate": 8.68393014670266e-05,
        "epoch": 0.08056200051559681,
        "step": 625
    },
    {
        "loss": 2.488,
        "grad_norm": 1.2343981266021729,
        "learning_rate": 8.679812516682874e-05,
        "epoch": 0.08069089971642175,
        "step": 626
    },
    {
        "loss": 1.7257,
        "grad_norm": 2.778688669204712,
        "learning_rate": 8.675689434744341e-05,
        "epoch": 0.08081979891724671,
        "step": 627
    },
    {
        "loss": 2.266,
        "grad_norm": 1.9581996202468872,
        "learning_rate": 8.671560906995712e-05,
        "epoch": 0.08094869811807166,
        "step": 628
    },
    {
        "loss": 2.4325,
        "grad_norm": 2.606123685836792,
        "learning_rate": 8.667426939553713e-05,
        "epoch": 0.08107759731889662,
        "step": 629
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.6669354438781738,
        "learning_rate": 8.663287538543125e-05,
        "epoch": 0.08120649651972157,
        "step": 630
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.7835091352462769,
        "learning_rate": 8.659142710096783e-05,
        "epoch": 0.08133539572054653,
        "step": 631
    },
    {
        "loss": 2.4163,
        "grad_norm": 1.1764622926712036,
        "learning_rate": 8.654992460355564e-05,
        "epoch": 0.08146429492137149,
        "step": 632
    },
    {
        "loss": 2.0337,
        "grad_norm": 2.683622121810913,
        "learning_rate": 8.650836795468371e-05,
        "epoch": 0.08159319412219644,
        "step": 633
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.885457992553711,
        "learning_rate": 8.646675721592135e-05,
        "epoch": 0.0817220933230214,
        "step": 634
    },
    {
        "loss": 1.6061,
        "grad_norm": 2.43241810798645,
        "learning_rate": 8.642509244891801e-05,
        "epoch": 0.08185099252384635,
        "step": 635
    },
    {
        "loss": 1.38,
        "grad_norm": 3.1502068042755127,
        "learning_rate": 8.638337371540315e-05,
        "epoch": 0.08197989172467131,
        "step": 636
    },
    {
        "loss": 2.5914,
        "grad_norm": 2.4951257705688477,
        "learning_rate": 8.634160107718623e-05,
        "epoch": 0.08210879092549626,
        "step": 637
    },
    {
        "loss": 2.7547,
        "grad_norm": 1.3499475717544556,
        "learning_rate": 8.629977459615655e-05,
        "epoch": 0.08223769012632122,
        "step": 638
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.8290622234344482,
        "learning_rate": 8.625789433428317e-05,
        "epoch": 0.08236658932714618,
        "step": 639
    },
    {
        "loss": 2.3719,
        "grad_norm": 1.1733150482177734,
        "learning_rate": 8.621596035361486e-05,
        "epoch": 0.08249548852797113,
        "step": 640
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.072679042816162,
        "learning_rate": 8.617397271627995e-05,
        "epoch": 0.08262438772879609,
        "step": 641
    },
    {
        "loss": 1.1903,
        "grad_norm": 1.0083930492401123,
        "learning_rate": 8.61319314844863e-05,
        "epoch": 0.08275328692962104,
        "step": 642
    },
    {
        "loss": 1.7842,
        "grad_norm": 2.569214105606079,
        "learning_rate": 8.608983672052112e-05,
        "epoch": 0.082882186130446,
        "step": 643
    },
    {
        "loss": 2.5345,
        "grad_norm": 1.2691224813461304,
        "learning_rate": 8.6047688486751e-05,
        "epoch": 0.08301108533127094,
        "step": 644
    },
    {
        "loss": 2.5846,
        "grad_norm": 1.8209363222122192,
        "learning_rate": 8.600548684562169e-05,
        "epoch": 0.0831399845320959,
        "step": 645
    },
    {
        "loss": 1.6147,
        "grad_norm": 3.0684525966644287,
        "learning_rate": 8.596323185965813e-05,
        "epoch": 0.08326888373292085,
        "step": 646
    },
    {
        "loss": 2.1085,
        "grad_norm": 2.132099151611328,
        "learning_rate": 8.59209235914642e-05,
        "epoch": 0.0833977829337458,
        "step": 647
    },
    {
        "loss": 2.2477,
        "grad_norm": 2.0964884757995605,
        "learning_rate": 8.587856210372284e-05,
        "epoch": 0.08352668213457076,
        "step": 648
    },
    {
        "loss": 1.1847,
        "grad_norm": 2.9299004077911377,
        "learning_rate": 8.583614745919573e-05,
        "epoch": 0.08365558133539572,
        "step": 649
    },
    {
        "loss": 1.9264,
        "grad_norm": 3.1906261444091797,
        "learning_rate": 8.579367972072336e-05,
        "epoch": 0.08378448053622067,
        "step": 650
    },
    {
        "loss": 2.2973,
        "grad_norm": 1.377018690109253,
        "learning_rate": 8.575115895122492e-05,
        "epoch": 0.08391337973704563,
        "step": 651
    },
    {
        "loss": 2.6901,
        "grad_norm": 1.529729962348938,
        "learning_rate": 8.570858521369807e-05,
        "epoch": 0.08404227893787058,
        "step": 652
    },
    {
        "loss": 2.2924,
        "grad_norm": 2.6464743614196777,
        "learning_rate": 8.566595857121902e-05,
        "epoch": 0.08417117813869554,
        "step": 653
    },
    {
        "loss": 2.5193,
        "grad_norm": 1.1592432260513306,
        "learning_rate": 8.562327908694235e-05,
        "epoch": 0.0843000773395205,
        "step": 654
    },
    {
        "loss": 2.464,
        "grad_norm": 1.4317764043807983,
        "learning_rate": 8.558054682410091e-05,
        "epoch": 0.08442897654034545,
        "step": 655
    },
    {
        "loss": 2.2439,
        "grad_norm": 1.319879412651062,
        "learning_rate": 8.553776184600577e-05,
        "epoch": 0.08455787574117041,
        "step": 656
    },
    {
        "loss": 2.1434,
        "grad_norm": 2.4939045906066895,
        "learning_rate": 8.54949242160461e-05,
        "epoch": 0.08468677494199536,
        "step": 657
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.903812050819397,
        "learning_rate": 8.545203399768904e-05,
        "epoch": 0.08481567414282032,
        "step": 658
    },
    {
        "loss": 1.776,
        "grad_norm": 1.7078672647476196,
        "learning_rate": 8.540909125447969e-05,
        "epoch": 0.08494457334364527,
        "step": 659
    },
    {
        "loss": 2.4781,
        "grad_norm": 1.5305023193359375,
        "learning_rate": 8.536609605004095e-05,
        "epoch": 0.08507347254447023,
        "step": 660
    },
    {
        "loss": 1.9267,
        "grad_norm": 2.1827313899993896,
        "learning_rate": 8.532304844807344e-05,
        "epoch": 0.08520237174529519,
        "step": 661
    },
    {
        "loss": 2.5744,
        "grad_norm": 1.6028718948364258,
        "learning_rate": 8.527994851235542e-05,
        "epoch": 0.08533127094612013,
        "step": 662
    },
    {
        "loss": 2.4106,
        "grad_norm": 1.2717846632003784,
        "learning_rate": 8.523679630674268e-05,
        "epoch": 0.08546017014694508,
        "step": 663
    },
    {
        "loss": 1.6628,
        "grad_norm": 2.491905689239502,
        "learning_rate": 8.519359189516847e-05,
        "epoch": 0.08558906934777004,
        "step": 664
    },
    {
        "loss": 2.1085,
        "grad_norm": 1.0917942523956299,
        "learning_rate": 8.515033534164337e-05,
        "epoch": 0.085717968548595,
        "step": 665
    },
    {
        "loss": 2.7627,
        "grad_norm": 1.6848174333572388,
        "learning_rate": 8.510702671025517e-05,
        "epoch": 0.08584686774941995,
        "step": 666
    },
    {
        "loss": 1.558,
        "grad_norm": 2.169210433959961,
        "learning_rate": 8.506366606516893e-05,
        "epoch": 0.0859757669502449,
        "step": 667
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.8351449966430664,
        "learning_rate": 8.502025347062666e-05,
        "epoch": 0.08610466615106986,
        "step": 668
    },
    {
        "loss": 1.9011,
        "grad_norm": 2.381675958633423,
        "learning_rate": 8.497678899094739e-05,
        "epoch": 0.08623356535189482,
        "step": 669
    },
    {
        "loss": 2.6735,
        "grad_norm": 1.182513952255249,
        "learning_rate": 8.493327269052701e-05,
        "epoch": 0.08636246455271977,
        "step": 670
    },
    {
        "loss": 2.2248,
        "grad_norm": 1.8471782207489014,
        "learning_rate": 8.48897046338382e-05,
        "epoch": 0.08649136375354473,
        "step": 671
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.9255889654159546,
        "learning_rate": 8.484608488543029e-05,
        "epoch": 0.08662026295436968,
        "step": 672
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.5873680114746094,
        "learning_rate": 8.480241350992925e-05,
        "epoch": 0.08674916215519464,
        "step": 673
    },
    {
        "loss": 2.0428,
        "grad_norm": 1.3995000123977661,
        "learning_rate": 8.475869057203746e-05,
        "epoch": 0.0868780613560196,
        "step": 674
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.0465102195739746,
        "learning_rate": 8.471491613653377e-05,
        "epoch": 0.08700696055684455,
        "step": 675
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.4894593954086304,
        "learning_rate": 8.467109026827329e-05,
        "epoch": 0.0871358597576695,
        "step": 676
    },
    {
        "loss": 0.9776,
        "grad_norm": 2.9438295364379883,
        "learning_rate": 8.462721303218734e-05,
        "epoch": 0.08726475895849446,
        "step": 677
    },
    {
        "loss": 2.356,
        "grad_norm": 1.3736239671707153,
        "learning_rate": 8.458328449328331e-05,
        "epoch": 0.08739365815931942,
        "step": 678
    },
    {
        "loss": 1.1666,
        "grad_norm": 2.5841481685638428,
        "learning_rate": 8.453930471664468e-05,
        "epoch": 0.08752255736014437,
        "step": 679
    },
    {
        "loss": 2.5537,
        "grad_norm": 0.9882200956344604,
        "learning_rate": 8.449527376743076e-05,
        "epoch": 0.08765145656096932,
        "step": 680
    },
    {
        "loss": 2.252,
        "grad_norm": 1.3594834804534912,
        "learning_rate": 8.445119171087671e-05,
        "epoch": 0.08778035576179427,
        "step": 681
    },
    {
        "loss": 2.2776,
        "grad_norm": 2.5800540447235107,
        "learning_rate": 8.440705861229344e-05,
        "epoch": 0.08790925496261923,
        "step": 682
    },
    {
        "loss": 2.4671,
        "grad_norm": 1.361756443977356,
        "learning_rate": 8.436287453706744e-05,
        "epoch": 0.08803815416344418,
        "step": 683
    },
    {
        "loss": 1.7608,
        "grad_norm": 2.8040239810943604,
        "learning_rate": 8.43186395506607e-05,
        "epoch": 0.08816705336426914,
        "step": 684
    },
    {
        "loss": 1.775,
        "grad_norm": 2.5481464862823486,
        "learning_rate": 8.427435371861072e-05,
        "epoch": 0.0882959525650941,
        "step": 685
    },
    {
        "loss": 2.5485,
        "grad_norm": 1.8148150444030762,
        "learning_rate": 8.423001710653026e-05,
        "epoch": 0.08842485176591905,
        "step": 686
    },
    {
        "loss": 2.0135,
        "grad_norm": 1.9801936149597168,
        "learning_rate": 8.418562978010736e-05,
        "epoch": 0.088553750966744,
        "step": 687
    },
    {
        "loss": 2.5172,
        "grad_norm": 1.65571928024292,
        "learning_rate": 8.414119180510518e-05,
        "epoch": 0.08868265016756896,
        "step": 688
    },
    {
        "loss": 2.2986,
        "grad_norm": 2.2198283672332764,
        "learning_rate": 8.40967032473619e-05,
        "epoch": 0.08881154936839392,
        "step": 689
    },
    {
        "loss": 1.8918,
        "grad_norm": 2.44299578666687,
        "learning_rate": 8.405216417279069e-05,
        "epoch": 0.08894044856921887,
        "step": 690
    },
    {
        "loss": 2.4793,
        "grad_norm": 1.1896324157714844,
        "learning_rate": 8.400757464737951e-05,
        "epoch": 0.08906934777004383,
        "step": 691
    },
    {
        "loss": 1.3437,
        "grad_norm": 2.5924065113067627,
        "learning_rate": 8.39629347371911e-05,
        "epoch": 0.08919824697086878,
        "step": 692
    },
    {
        "loss": 2.5283,
        "grad_norm": 1.1996829509735107,
        "learning_rate": 8.391824450836285e-05,
        "epoch": 0.08932714617169374,
        "step": 693
    },
    {
        "loss": 2.4768,
        "grad_norm": 1.617030143737793,
        "learning_rate": 8.38735040271067e-05,
        "epoch": 0.0894560453725187,
        "step": 694
    },
    {
        "loss": 1.7079,
        "grad_norm": 1.4386520385742188,
        "learning_rate": 8.382871335970902e-05,
        "epoch": 0.08958494457334365,
        "step": 695
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.399461030960083,
        "learning_rate": 8.378387257253053e-05,
        "epoch": 0.0897138437741686,
        "step": 696
    },
    {
        "loss": 2.4174,
        "grad_norm": 1.3441530466079712,
        "learning_rate": 8.373898173200625e-05,
        "epoch": 0.08984274297499356,
        "step": 697
    },
    {
        "loss": 1.9698,
        "grad_norm": 1.331992506980896,
        "learning_rate": 8.369404090464534e-05,
        "epoch": 0.0899716421758185,
        "step": 698
    },
    {
        "loss": 2.4134,
        "grad_norm": 1.2997958660125732,
        "learning_rate": 8.364905015703099e-05,
        "epoch": 0.09010054137664346,
        "step": 699
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.624627709388733,
        "learning_rate": 8.360400955582039e-05,
        "epoch": 0.09022944057746841,
        "step": 700
    },
    {
        "loss": 2.4465,
        "grad_norm": 1.4884202480316162,
        "learning_rate": 8.355891916774458e-05,
        "epoch": 0.09035833977829337,
        "step": 701
    },
    {
        "loss": 2.0925,
        "grad_norm": 2.61606502532959,
        "learning_rate": 8.351377905960834e-05,
        "epoch": 0.09048723897911833,
        "step": 702
    },
    {
        "loss": 2.4294,
        "grad_norm": 1.495574712753296,
        "learning_rate": 8.346858929829015e-05,
        "epoch": 0.09061613817994328,
        "step": 703
    },
    {
        "loss": 2.276,
        "grad_norm": 1.824464201927185,
        "learning_rate": 8.342334995074201e-05,
        "epoch": 0.09074503738076824,
        "step": 704
    },
    {
        "loss": 2.3211,
        "grad_norm": 1.576811671257019,
        "learning_rate": 8.337806108398944e-05,
        "epoch": 0.09087393658159319,
        "step": 705
    },
    {
        "loss": 1.943,
        "grad_norm": 2.2272603511810303,
        "learning_rate": 8.33327227651313e-05,
        "epoch": 0.09100283578241815,
        "step": 706
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.3732918500900269,
        "learning_rate": 8.32873350613397e-05,
        "epoch": 0.0911317349832431,
        "step": 707
    },
    {
        "loss": 1.7417,
        "grad_norm": 2.6413979530334473,
        "learning_rate": 8.324189803985992e-05,
        "epoch": 0.09126063418406806,
        "step": 708
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.160165548324585,
        "learning_rate": 8.319641176801036e-05,
        "epoch": 0.09138953338489302,
        "step": 709
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.3252382278442383,
        "learning_rate": 8.315087631318233e-05,
        "epoch": 0.09151843258571797,
        "step": 710
    },
    {
        "loss": 1.5306,
        "grad_norm": 2.440350294113159,
        "learning_rate": 8.310529174284004e-05,
        "epoch": 0.09164733178654293,
        "step": 711
    },
    {
        "loss": 1.5542,
        "grad_norm": 2.384887456893921,
        "learning_rate": 8.305965812452041e-05,
        "epoch": 0.09177623098736788,
        "step": 712
    },
    {
        "loss": 1.8046,
        "grad_norm": 2.7024242877960205,
        "learning_rate": 8.301397552583314e-05,
        "epoch": 0.09190513018819284,
        "step": 713
    },
    {
        "loss": 1.7729,
        "grad_norm": 1.7782014608383179,
        "learning_rate": 8.29682440144604e-05,
        "epoch": 0.0920340293890178,
        "step": 714
    },
    {
        "loss": 2.1337,
        "grad_norm": 1.955970287322998,
        "learning_rate": 8.292246365815685e-05,
        "epoch": 0.09216292858984275,
        "step": 715
    },
    {
        "loss": 1.8704,
        "grad_norm": 2.0864787101745605,
        "learning_rate": 8.287663452474954e-05,
        "epoch": 0.09229182779066769,
        "step": 716
    },
    {
        "loss": 2.4331,
        "grad_norm": 1.764999508857727,
        "learning_rate": 8.283075668213778e-05,
        "epoch": 0.09242072699149265,
        "step": 717
    },
    {
        "loss": 2.1634,
        "grad_norm": 1.6215828657150269,
        "learning_rate": 8.278483019829304e-05,
        "epoch": 0.0925496261923176,
        "step": 718
    },
    {
        "loss": 2.4553,
        "grad_norm": 1.1719160079956055,
        "learning_rate": 8.273885514125885e-05,
        "epoch": 0.09267852539314256,
        "step": 719
    },
    {
        "loss": 2.5501,
        "grad_norm": 1.1421020030975342,
        "learning_rate": 8.269283157915071e-05,
        "epoch": 0.09280742459396751,
        "step": 720
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.9410372972488403,
        "learning_rate": 8.2646759580156e-05,
        "epoch": 0.09293632379479247,
        "step": 721
    },
    {
        "loss": 2.6266,
        "grad_norm": 1.4783575534820557,
        "learning_rate": 8.260063921253383e-05,
        "epoch": 0.09306522299561742,
        "step": 722
    },
    {
        "loss": 2.4867,
        "grad_norm": 1.3635951280593872,
        "learning_rate": 8.2554470544615e-05,
        "epoch": 0.09319412219644238,
        "step": 723
    },
    {
        "loss": 1.9395,
        "grad_norm": 2.6837241649627686,
        "learning_rate": 8.250825364480188e-05,
        "epoch": 0.09332302139726734,
        "step": 724
    },
    {
        "loss": 2.5409,
        "grad_norm": 1.2832890748977661,
        "learning_rate": 8.246198858156825e-05,
        "epoch": 0.09345192059809229,
        "step": 725
    },
    {
        "loss": 1.6753,
        "grad_norm": 2.699553966522217,
        "learning_rate": 8.241567542345927e-05,
        "epoch": 0.09358081979891725,
        "step": 726
    },
    {
        "loss": 2.4176,
        "grad_norm": 1.740269422531128,
        "learning_rate": 8.236931423909138e-05,
        "epoch": 0.0937097189997422,
        "step": 727
    },
    {
        "loss": 1.8332,
        "grad_norm": 1.7639213800430298,
        "learning_rate": 8.232290509715218e-05,
        "epoch": 0.09383861820056716,
        "step": 728
    },
    {
        "loss": 2.5742,
        "grad_norm": 1.788716435432434,
        "learning_rate": 8.227644806640026e-05,
        "epoch": 0.09396751740139211,
        "step": 729
    },
    {
        "loss": 2.46,
        "grad_norm": 1.287257432937622,
        "learning_rate": 8.222994321566524e-05,
        "epoch": 0.09409641660221707,
        "step": 730
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.8592829704284668,
        "learning_rate": 8.218339061384754e-05,
        "epoch": 0.09422531580304203,
        "step": 731
    },
    {
        "loss": 2.3442,
        "grad_norm": 1.8381874561309814,
        "learning_rate": 8.21367903299183e-05,
        "epoch": 0.09435421500386698,
        "step": 732
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.7006419897079468,
        "learning_rate": 8.209014243291939e-05,
        "epoch": 0.09448311420469194,
        "step": 733
    },
    {
        "loss": 2.2196,
        "grad_norm": 1.4254417419433594,
        "learning_rate": 8.204344699196315e-05,
        "epoch": 0.09461201340551689,
        "step": 734
    },
    {
        "loss": 2.4271,
        "grad_norm": 1.8900675773620605,
        "learning_rate": 8.199670407623241e-05,
        "epoch": 0.09474091260634183,
        "step": 735
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.646174430847168,
        "learning_rate": 8.194991375498029e-05,
        "epoch": 0.09486981180716679,
        "step": 736
    },
    {
        "loss": 2.2905,
        "grad_norm": 1.485151767730713,
        "learning_rate": 8.190307609753016e-05,
        "epoch": 0.09499871100799175,
        "step": 737
    },
    {
        "loss": 2.6062,
        "grad_norm": 1.3034470081329346,
        "learning_rate": 8.185619117327554e-05,
        "epoch": 0.0951276102088167,
        "step": 738
    },
    {
        "loss": 1.7436,
        "grad_norm": 2.3534610271453857,
        "learning_rate": 8.180925905167997e-05,
        "epoch": 0.09525650940964166,
        "step": 739
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.1338870525360107,
        "learning_rate": 8.176227980227694e-05,
        "epoch": 0.09538540861046661,
        "step": 740
    },
    {
        "loss": 2.3707,
        "grad_norm": 1.6086310148239136,
        "learning_rate": 8.171525349466969e-05,
        "epoch": 0.09551430781129157,
        "step": 741
    },
    {
        "loss": 2.3082,
        "grad_norm": 1.08331298828125,
        "learning_rate": 8.166818019853125e-05,
        "epoch": 0.09564320701211652,
        "step": 742
    },
    {
        "loss": 2.2377,
        "grad_norm": 2.592254877090454,
        "learning_rate": 8.162105998360423e-05,
        "epoch": 0.09577210621294148,
        "step": 743
    },
    {
        "loss": 2.1786,
        "grad_norm": 1.747373104095459,
        "learning_rate": 8.15738929197008e-05,
        "epoch": 0.09590100541376644,
        "step": 744
    },
    {
        "loss": 2.2834,
        "grad_norm": 1.4462653398513794,
        "learning_rate": 8.15266790767025e-05,
        "epoch": 0.09602990461459139,
        "step": 745
    },
    {
        "loss": 1.2303,
        "grad_norm": 2.6284868717193604,
        "learning_rate": 8.147941852456015e-05,
        "epoch": 0.09615880381541635,
        "step": 746
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.5782331228256226,
        "learning_rate": 8.143211133329386e-05,
        "epoch": 0.0962877030162413,
        "step": 747
    },
    {
        "loss": 2.2281,
        "grad_norm": 1.6901518106460571,
        "learning_rate": 8.138475757299277e-05,
        "epoch": 0.09641660221706626,
        "step": 748
    },
    {
        "loss": 2.389,
        "grad_norm": 1.8732341527938843,
        "learning_rate": 8.133735731381502e-05,
        "epoch": 0.09654550141789121,
        "step": 749
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.4710018634796143,
        "learning_rate": 8.128991062598769e-05,
        "epoch": 0.09667440061871617,
        "step": 750
    },
    {
        "loss": 2.4196,
        "grad_norm": 1.6718167066574097,
        "learning_rate": 8.124241757980657e-05,
        "epoch": 0.09680329981954112,
        "step": 751
    },
    {
        "loss": 2.4411,
        "grad_norm": 2.092797040939331,
        "learning_rate": 8.119487824563619e-05,
        "epoch": 0.09693219902036608,
        "step": 752
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.462732195854187,
        "learning_rate": 8.114729269390966e-05,
        "epoch": 0.09706109822119102,
        "step": 753
    },
    {
        "loss": 2.2386,
        "grad_norm": 2.0977001190185547,
        "learning_rate": 8.109966099512856e-05,
        "epoch": 0.09718999742201598,
        "step": 754
    },
    {
        "loss": 2.1719,
        "grad_norm": 1.6722791194915771,
        "learning_rate": 8.10519832198628e-05,
        "epoch": 0.09731889662284093,
        "step": 755
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.6057170629501343,
        "learning_rate": 8.10042594387506e-05,
        "epoch": 0.09744779582366589,
        "step": 756
    },
    {
        "loss": 2.1714,
        "grad_norm": 1.5365127325057983,
        "learning_rate": 8.095648972249834e-05,
        "epoch": 0.09757669502449084,
        "step": 757
    },
    {
        "loss": 2.3096,
        "grad_norm": 1.7120577096939087,
        "learning_rate": 8.090867414188041e-05,
        "epoch": 0.0977055942253158,
        "step": 758
    },
    {
        "loss": 2.4355,
        "grad_norm": 0.9838820695877075,
        "learning_rate": 8.086081276773923e-05,
        "epoch": 0.09783449342614076,
        "step": 759
    },
    {
        "loss": 1.985,
        "grad_norm": 2.1791574954986572,
        "learning_rate": 8.0812905670985e-05,
        "epoch": 0.09796339262696571,
        "step": 760
    },
    {
        "loss": 2.2662,
        "grad_norm": 2.3956217765808105,
        "learning_rate": 8.076495292259568e-05,
        "epoch": 0.09809229182779067,
        "step": 761
    },
    {
        "loss": 2.2149,
        "grad_norm": 2.0115599632263184,
        "learning_rate": 8.071695459361687e-05,
        "epoch": 0.09822119102861562,
        "step": 762
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.6256788969039917,
        "learning_rate": 8.06689107551617e-05,
        "epoch": 0.09835009022944058,
        "step": 763
    },
    {
        "loss": 2.4909,
        "grad_norm": 1.2058671712875366,
        "learning_rate": 8.062082147841075e-05,
        "epoch": 0.09847898943026553,
        "step": 764
    },
    {
        "loss": 1.6262,
        "grad_norm": 2.052243709564209,
        "learning_rate": 8.057268683461185e-05,
        "epoch": 0.09860788863109049,
        "step": 765
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.8294659852981567,
        "learning_rate": 8.052450689508015e-05,
        "epoch": 0.09873678783191545,
        "step": 766
    },
    {
        "loss": 2.5296,
        "grad_norm": 1.3906878232955933,
        "learning_rate": 8.047628173119781e-05,
        "epoch": 0.0988656870327404,
        "step": 767
    },
    {
        "loss": 2.3834,
        "grad_norm": 2.087465524673462,
        "learning_rate": 8.042801141441403e-05,
        "epoch": 0.09899458623356536,
        "step": 768
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.6965758800506592,
        "learning_rate": 8.037969601624495e-05,
        "epoch": 0.09912348543439031,
        "step": 769
    },
    {
        "loss": 2.039,
        "grad_norm": 2.067336320877075,
        "learning_rate": 8.033133560827344e-05,
        "epoch": 0.09925238463521527,
        "step": 770
    },
    {
        "loss": 2.1462,
        "grad_norm": 2.6584060192108154,
        "learning_rate": 8.028293026214905e-05,
        "epoch": 0.09938128383604021,
        "step": 771
    },
    {
        "loss": 2.1473,
        "grad_norm": 1.3269456624984741,
        "learning_rate": 8.0234480049588e-05,
        "epoch": 0.09951018303686517,
        "step": 772
    },
    {
        "loss": 1.9644,
        "grad_norm": 1.8759315013885498,
        "learning_rate": 8.018598504237287e-05,
        "epoch": 0.09963908223769012,
        "step": 773
    },
    {
        "loss": 2.086,
        "grad_norm": 1.8251522779464722,
        "learning_rate": 8.013744531235268e-05,
        "epoch": 0.09976798143851508,
        "step": 774
    },
    {
        "loss": 1.9409,
        "grad_norm": 1.8144947290420532,
        "learning_rate": 8.008886093144268e-05,
        "epoch": 0.09989688063934003,
        "step": 775
    },
    {
        "loss": 2.3887,
        "grad_norm": 1.4662601947784424,
        "learning_rate": 8.004023197162427e-05,
        "epoch": 0.10002577984016499,
        "step": 776
    },
    {
        "loss": 2.177,
        "grad_norm": 2.606267213821411,
        "learning_rate": 7.999155850494493e-05,
        "epoch": 0.10015467904098994,
        "step": 777
    },
    {
        "loss": 1.7232,
        "grad_norm": 2.2441012859344482,
        "learning_rate": 7.994284060351804e-05,
        "epoch": 0.1002835782418149,
        "step": 778
    },
    {
        "loss": 2.1717,
        "grad_norm": 2.2161948680877686,
        "learning_rate": 7.989407833952286e-05,
        "epoch": 0.10041247744263986,
        "step": 779
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.3840901851654053,
        "learning_rate": 7.98452717852043e-05,
        "epoch": 0.10054137664346481,
        "step": 780
    },
    {
        "loss": 2.0906,
        "grad_norm": 1.6650712490081787,
        "learning_rate": 7.9796421012873e-05,
        "epoch": 0.10067027584428977,
        "step": 781
    },
    {
        "loss": 2.4699,
        "grad_norm": 1.488461971282959,
        "learning_rate": 7.974752609490499e-05,
        "epoch": 0.10079917504511472,
        "step": 782
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.8843954801559448,
        "learning_rate": 7.969858710374178e-05,
        "epoch": 0.10092807424593968,
        "step": 783
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.8976964950561523,
        "learning_rate": 7.96496041118902e-05,
        "epoch": 0.10105697344676463,
        "step": 784
    },
    {
        "loss": 1.9299,
        "grad_norm": 2.315001964569092,
        "learning_rate": 7.96005771919222e-05,
        "epoch": 0.10118587264758959,
        "step": 785
    },
    {
        "loss": 2.4637,
        "grad_norm": 1.409079909324646,
        "learning_rate": 7.955150641647485e-05,
        "epoch": 0.10131477184841454,
        "step": 786
    },
    {
        "loss": 2.4858,
        "grad_norm": 1.7249349355697632,
        "learning_rate": 7.950239185825018e-05,
        "epoch": 0.1014436710492395,
        "step": 787
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.8308542966842651,
        "learning_rate": 7.945323359001513e-05,
        "epoch": 0.10157257025006446,
        "step": 788
    },
    {
        "loss": 2.1237,
        "grad_norm": 2.1858537197113037,
        "learning_rate": 7.940403168460133e-05,
        "epoch": 0.1017014694508894,
        "step": 789
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.3842406272888184,
        "learning_rate": 7.935478621490514e-05,
        "epoch": 0.10183036865171435,
        "step": 790
    },
    {
        "loss": 1.1531,
        "grad_norm": 2.8503191471099854,
        "learning_rate": 7.930549725388741e-05,
        "epoch": 0.10195926785253931,
        "step": 791
    },
    {
        "loss": 1.9073,
        "grad_norm": 2.37652587890625,
        "learning_rate": 7.925616487457341e-05,
        "epoch": 0.10208816705336426,
        "step": 792
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.2457263469696045,
        "learning_rate": 7.920678915005281e-05,
        "epoch": 0.10221706625418922,
        "step": 793
    },
    {
        "loss": 2.3587,
        "grad_norm": 1.890639305114746,
        "learning_rate": 7.915737015347942e-05,
        "epoch": 0.10234596545501418,
        "step": 794
    },
    {
        "loss": 2.2113,
        "grad_norm": 2.227681875228882,
        "learning_rate": 7.910790795807122e-05,
        "epoch": 0.10247486465583913,
        "step": 795
    },
    {
        "loss": 2.4141,
        "grad_norm": 1.7145477533340454,
        "learning_rate": 7.905840263711018e-05,
        "epoch": 0.10260376385666409,
        "step": 796
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.295890212059021,
        "learning_rate": 7.90088542639421e-05,
        "epoch": 0.10273266305748904,
        "step": 797
    },
    {
        "loss": 2.2257,
        "grad_norm": 2.565561294555664,
        "learning_rate": 7.895926291197668e-05,
        "epoch": 0.102861562258314,
        "step": 798
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.5307856798171997,
        "learning_rate": 7.890962865468719e-05,
        "epoch": 0.10299046145913895,
        "step": 799
    },
    {
        "loss": 2.0316,
        "grad_norm": 1.878690481185913,
        "learning_rate": 7.885995156561054e-05,
        "epoch": 0.10311936065996391,
        "step": 800
    },
    {
        "loss": 2.6468,
        "grad_norm": 1.5050358772277832,
        "learning_rate": 7.881023171834706e-05,
        "epoch": 0.10324825986078887,
        "step": 801
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.0348422527313232,
        "learning_rate": 7.876046918656044e-05,
        "epoch": 0.10337715906161382,
        "step": 802
    },
    {
        "loss": 2.404,
        "grad_norm": 1.2232639789581299,
        "learning_rate": 7.87106640439776e-05,
        "epoch": 0.10350605826243878,
        "step": 803
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.260826587677002,
        "learning_rate": 7.866081636438863e-05,
        "epoch": 0.10363495746326373,
        "step": 804
    },
    {
        "loss": 1.1485,
        "grad_norm": 2.693469524383545,
        "learning_rate": 7.861092622164657e-05,
        "epoch": 0.10376385666408869,
        "step": 805
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.2750842571258545,
        "learning_rate": 7.856099368966745e-05,
        "epoch": 0.10389275586491364,
        "step": 806
    },
    {
        "loss": 2.1084,
        "grad_norm": 2.243098020553589,
        "learning_rate": 7.851101884243007e-05,
        "epoch": 0.10402165506573859,
        "step": 807
    },
    {
        "loss": 2.4826,
        "grad_norm": 1.7401423454284668,
        "learning_rate": 7.846100175397588e-05,
        "epoch": 0.10415055426656354,
        "step": 808
    },
    {
        "loss": 2.0043,
        "grad_norm": 2.4041552543640137,
        "learning_rate": 7.8410942498409e-05,
        "epoch": 0.1042794534673885,
        "step": 809
    },
    {
        "loss": 1.6883,
        "grad_norm": 2.6540987491607666,
        "learning_rate": 7.836084114989597e-05,
        "epoch": 0.10440835266821345,
        "step": 810
    },
    {
        "loss": 2.35,
        "grad_norm": 1.7552798986434937,
        "learning_rate": 7.831069778266569e-05,
        "epoch": 0.10453725186903841,
        "step": 811
    },
    {
        "loss": 2.0959,
        "grad_norm": 2.371185064315796,
        "learning_rate": 7.826051247100931e-05,
        "epoch": 0.10466615106986336,
        "step": 812
    },
    {
        "loss": 2.1806,
        "grad_norm": 1.4113731384277344,
        "learning_rate": 7.821028528928018e-05,
        "epoch": 0.10479505027068832,
        "step": 813
    },
    {
        "loss": 2.2166,
        "grad_norm": 1.9758332967758179,
        "learning_rate": 7.816001631189363e-05,
        "epoch": 0.10492394947151328,
        "step": 814
    },
    {
        "loss": 2.0486,
        "grad_norm": 1.8114897012710571,
        "learning_rate": 7.810970561332692e-05,
        "epoch": 0.10505284867233823,
        "step": 815
    },
    {
        "loss": 1.9475,
        "grad_norm": 1.6238183975219727,
        "learning_rate": 7.805935326811912e-05,
        "epoch": 0.10518174787316319,
        "step": 816
    },
    {
        "loss": 2.2702,
        "grad_norm": 1.7127128839492798,
        "learning_rate": 7.800895935087103e-05,
        "epoch": 0.10531064707398814,
        "step": 817
    },
    {
        "loss": 2.0707,
        "grad_norm": 1.9523192644119263,
        "learning_rate": 7.795852393624503e-05,
        "epoch": 0.1054395462748131,
        "step": 818
    },
    {
        "loss": 1.4641,
        "grad_norm": 2.3410849571228027,
        "learning_rate": 7.790804709896496e-05,
        "epoch": 0.10556844547563805,
        "step": 819
    },
    {
        "loss": 1.6709,
        "grad_norm": 1.9853761196136475,
        "learning_rate": 7.785752891381606e-05,
        "epoch": 0.10569734467646301,
        "step": 820
    },
    {
        "loss": 2.2474,
        "grad_norm": 1.9929289817810059,
        "learning_rate": 7.780696945564481e-05,
        "epoch": 0.10582624387728796,
        "step": 821
    },
    {
        "loss": 2.012,
        "grad_norm": 2.4850876331329346,
        "learning_rate": 7.775636879935884e-05,
        "epoch": 0.10595514307811292,
        "step": 822
    },
    {
        "loss": 2.0143,
        "grad_norm": 1.4850980043411255,
        "learning_rate": 7.770572701992685e-05,
        "epoch": 0.10608404227893788,
        "step": 823
    },
    {
        "loss": 1.7646,
        "grad_norm": 2.3398163318634033,
        "learning_rate": 7.76550441923784e-05,
        "epoch": 0.10621294147976283,
        "step": 824
    },
    {
        "loss": 2.6009,
        "grad_norm": 1.5267362594604492,
        "learning_rate": 7.760432039180396e-05,
        "epoch": 0.10634184068058777,
        "step": 825
    },
    {
        "loss": 2.5738,
        "grad_norm": 1.6846871376037598,
        "learning_rate": 7.755355569335461e-05,
        "epoch": 0.10647073988141273,
        "step": 826
    },
    {
        "loss": 2.2163,
        "grad_norm": 2.636667490005493,
        "learning_rate": 7.750275017224207e-05,
        "epoch": 0.10659963908223768,
        "step": 827
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.6373982429504395,
        "learning_rate": 7.745190390373855e-05,
        "epoch": 0.10672853828306264,
        "step": 828
    },
    {
        "loss": 2.3408,
        "grad_norm": 1.7709200382232666,
        "learning_rate": 7.740101696317662e-05,
        "epoch": 0.1068574374838876,
        "step": 829
    },
    {
        "loss": 2.4871,
        "grad_norm": 1.6463555097579956,
        "learning_rate": 7.73500894259491e-05,
        "epoch": 0.10698633668471255,
        "step": 830
    },
    {
        "loss": 2.208,
        "grad_norm": 1.4209697246551514,
        "learning_rate": 7.729912136750896e-05,
        "epoch": 0.10711523588553751,
        "step": 831
    },
    {
        "loss": 2.4688,
        "grad_norm": 1.8015615940093994,
        "learning_rate": 7.724811286336922e-05,
        "epoch": 0.10724413508636246,
        "step": 832
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.4440758228302,
        "learning_rate": 7.71970639891028e-05,
        "epoch": 0.10737303428718742,
        "step": 833
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.8395748138427734,
        "learning_rate": 7.714597482034244e-05,
        "epoch": 0.10750193348801237,
        "step": 834
    },
    {
        "loss": 1.8194,
        "grad_norm": 2.6164495944976807,
        "learning_rate": 7.709484543278059e-05,
        "epoch": 0.10763083268883733,
        "step": 835
    },
    {
        "loss": 2.2631,
        "grad_norm": 2.206634759902954,
        "learning_rate": 7.704367590216928e-05,
        "epoch": 0.10775973188966229,
        "step": 836
    },
    {
        "loss": 1.2076,
        "grad_norm": 2.013782024383545,
        "learning_rate": 7.699246630432002e-05,
        "epoch": 0.10788863109048724,
        "step": 837
    },
    {
        "loss": 2.3511,
        "grad_norm": 1.2345890998840332,
        "learning_rate": 7.694121671510365e-05,
        "epoch": 0.1080175302913122,
        "step": 838
    },
    {
        "loss": 2.5065,
        "grad_norm": 1.4131181240081787,
        "learning_rate": 7.688992721045033e-05,
        "epoch": 0.10814642949213715,
        "step": 839
    },
    {
        "loss": 2.5982,
        "grad_norm": 1.6050137281417847,
        "learning_rate": 7.683859786634928e-05,
        "epoch": 0.10827532869296211,
        "step": 840
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.6755138635635376,
        "learning_rate": 7.67872287588488e-05,
        "epoch": 0.10840422789378706,
        "step": 841
    },
    {
        "loss": 1.9245,
        "grad_norm": 2.1621437072753906,
        "learning_rate": 7.673581996405608e-05,
        "epoch": 0.10853312709461202,
        "step": 842
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.2609460353851318,
        "learning_rate": 7.66843715581371e-05,
        "epoch": 0.10866202629543696,
        "step": 843
    },
    {
        "loss": 1.9628,
        "grad_norm": 2.037761926651001,
        "learning_rate": 7.663288361731656e-05,
        "epoch": 0.10879092549626192,
        "step": 844
    },
    {
        "loss": 2.2737,
        "grad_norm": 2.252774953842163,
        "learning_rate": 7.658135621787768e-05,
        "epoch": 0.10891982469708687,
        "step": 845
    },
    {
        "loss": 2.1221,
        "grad_norm": 1.918225884437561,
        "learning_rate": 7.65297894361622e-05,
        "epoch": 0.10904872389791183,
        "step": 846
    },
    {
        "loss": 2.3121,
        "grad_norm": 1.4270788431167603,
        "learning_rate": 7.647818334857018e-05,
        "epoch": 0.10917762309873678,
        "step": 847
    },
    {
        "loss": 2.5692,
        "grad_norm": 2.2593514919281006,
        "learning_rate": 7.642653803155991e-05,
        "epoch": 0.10930652229956174,
        "step": 848
    },
    {
        "loss": 2.4781,
        "grad_norm": 1.5599281787872314,
        "learning_rate": 7.637485356164782e-05,
        "epoch": 0.1094354215003867,
        "step": 849
    },
    {
        "loss": 2.1485,
        "grad_norm": 1.453858733177185,
        "learning_rate": 7.632313001540832e-05,
        "epoch": 0.10956432070121165,
        "step": 850
    },
    {
        "loss": 1.1771,
        "grad_norm": 2.717161178588867,
        "learning_rate": 7.627136746947374e-05,
        "epoch": 0.1096932199020366,
        "step": 851
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.789065957069397,
        "learning_rate": 7.621956600053418e-05,
        "epoch": 0.10982211910286156,
        "step": 852
    },
    {
        "loss": 2.4467,
        "grad_norm": 1.3365691900253296,
        "learning_rate": 7.616772568533741e-05,
        "epoch": 0.10995101830368652,
        "step": 853
    },
    {
        "loss": 1.9511,
        "grad_norm": 2.2342121601104736,
        "learning_rate": 7.611584660068874e-05,
        "epoch": 0.11007991750451147,
        "step": 854
    },
    {
        "loss": 1.6622,
        "grad_norm": 1.6932706832885742,
        "learning_rate": 7.606392882345095e-05,
        "epoch": 0.11020881670533643,
        "step": 855
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.3934156894683838,
        "learning_rate": 7.60119724305441e-05,
        "epoch": 0.11033771590616138,
        "step": 856
    },
    {
        "loss": 2.206,
        "grad_norm": 2.1228933334350586,
        "learning_rate": 7.595997749894554e-05,
        "epoch": 0.11046661510698634,
        "step": 857
    },
    {
        "loss": 1.6422,
        "grad_norm": 2.0018515586853027,
        "learning_rate": 7.590794410568963e-05,
        "epoch": 0.1105955143078113,
        "step": 858
    },
    {
        "loss": 1.5435,
        "grad_norm": 2.657172918319702,
        "learning_rate": 7.585587232786775e-05,
        "epoch": 0.11072441350863625,
        "step": 859
    },
    {
        "loss": 2.7775,
        "grad_norm": 1.3020925521850586,
        "learning_rate": 7.580376224262818e-05,
        "epoch": 0.11085331270946121,
        "step": 860
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.5445998907089233,
        "learning_rate": 7.575161392717589e-05,
        "epoch": 0.11098221191028615,
        "step": 861
    },
    {
        "loss": 1.8065,
        "grad_norm": 2.4299356937408447,
        "learning_rate": 7.569942745877257e-05,
        "epoch": 0.1111111111111111,
        "step": 862
    },
    {
        "loss": 1.795,
        "grad_norm": 2.377460479736328,
        "learning_rate": 7.564720291473635e-05,
        "epoch": 0.11124001031193606,
        "step": 863
    },
    {
        "loss": 1.0282,
        "grad_norm": 3.015892267227173,
        "learning_rate": 7.559494037244183e-05,
        "epoch": 0.11136890951276102,
        "step": 864
    },
    {
        "loss": 2.0345,
        "grad_norm": 1.8742097616195679,
        "learning_rate": 7.554263990931993e-05,
        "epoch": 0.11149780871358597,
        "step": 865
    },
    {
        "loss": 2.1595,
        "grad_norm": 1.6939477920532227,
        "learning_rate": 7.549030160285766e-05,
        "epoch": 0.11162670791441093,
        "step": 866
    },
    {
        "loss": 1.5889,
        "grad_norm": 2.288846969604492,
        "learning_rate": 7.543792553059818e-05,
        "epoch": 0.11175560711523588,
        "step": 867
    },
    {
        "loss": 2.4059,
        "grad_norm": 1.3516113758087158,
        "learning_rate": 7.53855117701406e-05,
        "epoch": 0.11188450631606084,
        "step": 868
    },
    {
        "loss": 2.2027,
        "grad_norm": 1.8955998420715332,
        "learning_rate": 7.53330603991398e-05,
        "epoch": 0.1120134055168858,
        "step": 869
    },
    {
        "loss": 2.311,
        "grad_norm": 1.8795737028121948,
        "learning_rate": 7.528057149530645e-05,
        "epoch": 0.11214230471771075,
        "step": 870
    },
    {
        "loss": 1.8516,
        "grad_norm": 1.7666765451431274,
        "learning_rate": 7.522804513640684e-05,
        "epoch": 0.1122712039185357,
        "step": 871
    },
    {
        "loss": 2.3875,
        "grad_norm": 1.6258351802825928,
        "learning_rate": 7.517548140026264e-05,
        "epoch": 0.11240010311936066,
        "step": 872
    },
    {
        "loss": 1.9344,
        "grad_norm": 1.925919532775879,
        "learning_rate": 7.512288036475103e-05,
        "epoch": 0.11252900232018562,
        "step": 873
    },
    {
        "loss": 2.0291,
        "grad_norm": 1.9318634271621704,
        "learning_rate": 7.50702421078044e-05,
        "epoch": 0.11265790152101057,
        "step": 874
    },
    {
        "loss": 2.554,
        "grad_norm": 1.5683244466781616,
        "learning_rate": 7.501756670741025e-05,
        "epoch": 0.11278680072183553,
        "step": 875
    },
    {
        "loss": 2.3873,
        "grad_norm": 1.6294769048690796,
        "learning_rate": 7.496485424161116e-05,
        "epoch": 0.11291569992266048,
        "step": 876
    },
    {
        "loss": 2.1667,
        "grad_norm": 1.917128562927246,
        "learning_rate": 7.49121047885046e-05,
        "epoch": 0.11304459912348544,
        "step": 877
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.834040641784668,
        "learning_rate": 7.485931842624289e-05,
        "epoch": 0.1131734983243104,
        "step": 878
    },
    {
        "loss": 2.0238,
        "grad_norm": 2.0623254776000977,
        "learning_rate": 7.480649523303294e-05,
        "epoch": 0.11330239752513535,
        "step": 879
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.6442524194717407,
        "learning_rate": 7.475363528713629e-05,
        "epoch": 0.11343129672596029,
        "step": 880
    },
    {
        "loss": 1.803,
        "grad_norm": 1.9245107173919678,
        "learning_rate": 7.470073866686895e-05,
        "epoch": 0.11356019592678525,
        "step": 881
    },
    {
        "loss": 2.6785,
        "grad_norm": 1.0964150428771973,
        "learning_rate": 7.46478054506012e-05,
        "epoch": 0.1136890951276102,
        "step": 882
    },
    {
        "loss": 1.8673,
        "grad_norm": 2.5853796005249023,
        "learning_rate": 7.459483571675762e-05,
        "epoch": 0.11381799432843516,
        "step": 883
    },
    {
        "loss": 2.3722,
        "grad_norm": 1.7672443389892578,
        "learning_rate": 7.454182954381681e-05,
        "epoch": 0.11394689352926012,
        "step": 884
    },
    {
        "loss": 2.1556,
        "grad_norm": 1.6950339078903198,
        "learning_rate": 7.448878701031142e-05,
        "epoch": 0.11407579273008507,
        "step": 885
    },
    {
        "loss": 1.8251,
        "grad_norm": 2.754317283630371,
        "learning_rate": 7.443570819482795e-05,
        "epoch": 0.11420469193091003,
        "step": 886
    },
    {
        "loss": 2.6165,
        "grad_norm": 1.572919487953186,
        "learning_rate": 7.438259317600664e-05,
        "epoch": 0.11433359113173498,
        "step": 887
    },
    {
        "loss": 1.3686,
        "grad_norm": 2.4776124954223633,
        "learning_rate": 7.43294420325414e-05,
        "epoch": 0.11446249033255994,
        "step": 888
    },
    {
        "loss": 2.0085,
        "grad_norm": 1.8573286533355713,
        "learning_rate": 7.427625484317963e-05,
        "epoch": 0.1145913895333849,
        "step": 889
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.8262532949447632,
        "learning_rate": 7.422303168672215e-05,
        "epoch": 0.11472028873420985,
        "step": 890
    },
    {
        "loss": 1.8658,
        "grad_norm": 2.20308518409729,
        "learning_rate": 7.416977264202308e-05,
        "epoch": 0.1148491879350348,
        "step": 891
    },
    {
        "loss": 1.5818,
        "grad_norm": 2.198194980621338,
        "learning_rate": 7.411647778798965e-05,
        "epoch": 0.11497808713585976,
        "step": 892
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.1895393133163452,
        "learning_rate": 7.406314720358225e-05,
        "epoch": 0.11510698633668472,
        "step": 893
    },
    {
        "loss": 1.8147,
        "grad_norm": 2.1646676063537598,
        "learning_rate": 7.400978096781413e-05,
        "epoch": 0.11523588553750967,
        "step": 894
    },
    {
        "loss": 2.6879,
        "grad_norm": 2.2359495162963867,
        "learning_rate": 7.395637915975135e-05,
        "epoch": 0.11536478473833463,
        "step": 895
    },
    {
        "loss": 2.7162,
        "grad_norm": 1.5491490364074707,
        "learning_rate": 7.39029418585127e-05,
        "epoch": 0.11549368393915958,
        "step": 896
    },
    {
        "loss": 2.621,
        "grad_norm": 1.6887818574905396,
        "learning_rate": 7.384946914326963e-05,
        "epoch": 0.11562258313998454,
        "step": 897
    },
    {
        "loss": 2.1857,
        "grad_norm": 1.7386614084243774,
        "learning_rate": 7.37959610932459e-05,
        "epoch": 0.11575148234080948,
        "step": 898
    },
    {
        "loss": 2.1353,
        "grad_norm": 2.4535703659057617,
        "learning_rate": 7.374241778771775e-05,
        "epoch": 0.11588038154163444,
        "step": 899
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.7051961421966553,
        "learning_rate": 7.368883930601361e-05,
        "epoch": 0.11600928074245939,
        "step": 900
    },
    {
        "loss": 2.1772,
        "grad_norm": 1.572179913520813,
        "learning_rate": 7.363522572751401e-05,
        "epoch": 0.11613817994328435,
        "step": 901
    },
    {
        "loss": 2.0331,
        "grad_norm": 1.3028606176376343,
        "learning_rate": 7.35815771316515e-05,
        "epoch": 0.1162670791441093,
        "step": 902
    },
    {
        "loss": 1.8936,
        "grad_norm": 1.7268716096878052,
        "learning_rate": 7.352789359791053e-05,
        "epoch": 0.11639597834493426,
        "step": 903
    },
    {
        "loss": 2.2125,
        "grad_norm": 2.052854061126709,
        "learning_rate": 7.347417520582724e-05,
        "epoch": 0.11652487754575921,
        "step": 904
    },
    {
        "loss": 1.5809,
        "grad_norm": 2.110522747039795,
        "learning_rate": 7.342042203498951e-05,
        "epoch": 0.11665377674658417,
        "step": 905
    },
    {
        "loss": 1.8463,
        "grad_norm": 1.682894229888916,
        "learning_rate": 7.336663416503669e-05,
        "epoch": 0.11678267594740913,
        "step": 906
    },
    {
        "loss": 1.5983,
        "grad_norm": 1.868262529373169,
        "learning_rate": 7.331281167565954e-05,
        "epoch": 0.11691157514823408,
        "step": 907
    },
    {
        "loss": 1.0002,
        "grad_norm": 2.977684259414673,
        "learning_rate": 7.325895464660012e-05,
        "epoch": 0.11704047434905904,
        "step": 908
    },
    {
        "loss": 2.5723,
        "grad_norm": 1.5842678546905518,
        "learning_rate": 7.320506315765168e-05,
        "epoch": 0.11716937354988399,
        "step": 909
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.8673276901245117,
        "learning_rate": 7.31511372886585e-05,
        "epoch": 0.11729827275070895,
        "step": 910
    },
    {
        "loss": 2.3325,
        "grad_norm": 1.0990339517593384,
        "learning_rate": 7.309717711951581e-05,
        "epoch": 0.1174271719515339,
        "step": 911
    },
    {
        "loss": 2.077,
        "grad_norm": 1.878089427947998,
        "learning_rate": 7.304318273016966e-05,
        "epoch": 0.11755607115235886,
        "step": 912
    },
    {
        "loss": 2.2059,
        "grad_norm": 2.0155742168426514,
        "learning_rate": 7.29891542006168e-05,
        "epoch": 0.11768497035318382,
        "step": 913
    },
    {
        "loss": 1.5692,
        "grad_norm": 2.7003278732299805,
        "learning_rate": 7.293509161090452e-05,
        "epoch": 0.11781386955400877,
        "step": 914
    },
    {
        "loss": 2.113,
        "grad_norm": 2.615554094314575,
        "learning_rate": 7.288099504113066e-05,
        "epoch": 0.11794276875483373,
        "step": 915
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.996849536895752,
        "learning_rate": 7.282686457144334e-05,
        "epoch": 0.11807166795565867,
        "step": 916
    },
    {
        "loss": 1.4292,
        "grad_norm": 2.0636956691741943,
        "learning_rate": 7.277270028204088e-05,
        "epoch": 0.11820056715648362,
        "step": 917
    },
    {
        "loss": 1.9456,
        "grad_norm": 1.8854241371154785,
        "learning_rate": 7.271850225317178e-05,
        "epoch": 0.11832946635730858,
        "step": 918
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.9567527770996094,
        "learning_rate": 7.26642705651345e-05,
        "epoch": 0.11845836555813354,
        "step": 919
    },
    {
        "loss": 2.4385,
        "grad_norm": 1.3458813428878784,
        "learning_rate": 7.261000529827733e-05,
        "epoch": 0.11858726475895849,
        "step": 920
    },
    {
        "loss": 2.0261,
        "grad_norm": 2.1216068267822266,
        "learning_rate": 7.255570653299837e-05,
        "epoch": 0.11871616395978345,
        "step": 921
    },
    {
        "loss": 2.1384,
        "grad_norm": 1.654500126838684,
        "learning_rate": 7.25013743497453e-05,
        "epoch": 0.1188450631606084,
        "step": 922
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.374800682067871,
        "learning_rate": 7.244700882901534e-05,
        "epoch": 0.11897396236143336,
        "step": 923
    },
    {
        "loss": 2.3584,
        "grad_norm": 1.609088659286499,
        "learning_rate": 7.239261005135509e-05,
        "epoch": 0.11910286156225831,
        "step": 924
    },
    {
        "loss": 2.2816,
        "grad_norm": 1.7218345403671265,
        "learning_rate": 7.233817809736044e-05,
        "epoch": 0.11923176076308327,
        "step": 925
    },
    {
        "loss": 1.8386,
        "grad_norm": 2.1035659313201904,
        "learning_rate": 7.22837130476764e-05,
        "epoch": 0.11936065996390822,
        "step": 926
    },
    {
        "loss": 1.9449,
        "grad_norm": 1.9208886623382568,
        "learning_rate": 7.222921498299704e-05,
        "epoch": 0.11948955916473318,
        "step": 927
    },
    {
        "loss": 1.9591,
        "grad_norm": 2.4611682891845703,
        "learning_rate": 7.217468398406537e-05,
        "epoch": 0.11961845836555814,
        "step": 928
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.1726465225219727,
        "learning_rate": 7.21201201316731e-05,
        "epoch": 0.11974735756638309,
        "step": 929
    },
    {
        "loss": 2.3893,
        "grad_norm": 2.1608033180236816,
        "learning_rate": 7.206552350666073e-05,
        "epoch": 0.11987625676720805,
        "step": 930
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.8414793014526367,
        "learning_rate": 7.201089418991722e-05,
        "epoch": 0.120005155968033,
        "step": 931
    },
    {
        "loss": 1.7424,
        "grad_norm": 1.8682783842086792,
        "learning_rate": 7.195623226238006e-05,
        "epoch": 0.12013405516885796,
        "step": 932
    },
    {
        "loss": 1.8791,
        "grad_norm": 2.3138885498046875,
        "learning_rate": 7.190153780503496e-05,
        "epoch": 0.12026295436968291,
        "step": 933
    },
    {
        "loss": 2.468,
        "grad_norm": 1.5594451427459717,
        "learning_rate": 7.184681089891589e-05,
        "epoch": 0.12039185357050786,
        "step": 934
    },
    {
        "loss": 1.4656,
        "grad_norm": 2.421973943710327,
        "learning_rate": 7.179205162510485e-05,
        "epoch": 0.12052075277133281,
        "step": 935
    },
    {
        "loss": 2.5245,
        "grad_norm": 2.185631275177002,
        "learning_rate": 7.173726006473186e-05,
        "epoch": 0.12064965197215777,
        "step": 936
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.3316044807434082,
        "learning_rate": 7.168243629897469e-05,
        "epoch": 0.12077855117298272,
        "step": 937
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.677226185798645,
        "learning_rate": 7.16275804090589e-05,
        "epoch": 0.12090745037380768,
        "step": 938
    },
    {
        "loss": 1.9399,
        "grad_norm": 2.0201480388641357,
        "learning_rate": 7.15726924762576e-05,
        "epoch": 0.12103634957463263,
        "step": 939
    },
    {
        "loss": 2.7235,
        "grad_norm": 1.6042102575302124,
        "learning_rate": 7.151777258189138e-05,
        "epoch": 0.12116524877545759,
        "step": 940
    },
    {
        "loss": 2.6585,
        "grad_norm": 1.5043888092041016,
        "learning_rate": 7.146282080732821e-05,
        "epoch": 0.12129414797628255,
        "step": 941
    },
    {
        "loss": 1.8311,
        "grad_norm": 1.6718744039535522,
        "learning_rate": 7.140783723398325e-05,
        "epoch": 0.1214230471771075,
        "step": 942
    },
    {
        "loss": 2.1171,
        "grad_norm": 1.6600672006607056,
        "learning_rate": 7.13528219433188e-05,
        "epoch": 0.12155194637793246,
        "step": 943
    },
    {
        "loss": 2.1231,
        "grad_norm": 2.28698468208313,
        "learning_rate": 7.129777501684418e-05,
        "epoch": 0.12168084557875741,
        "step": 944
    },
    {
        "loss": 2.2772,
        "grad_norm": 2.526892900466919,
        "learning_rate": 7.12426965361155e-05,
        "epoch": 0.12180974477958237,
        "step": 945
    },
    {
        "loss": 2.6707,
        "grad_norm": 1.4191690683364868,
        "learning_rate": 7.11875865827357e-05,
        "epoch": 0.12193864398040732,
        "step": 946
    },
    {
        "loss": 2.2319,
        "grad_norm": 2.155606985092163,
        "learning_rate": 7.113244523835428e-05,
        "epoch": 0.12206754318123228,
        "step": 947
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.1634721755981445,
        "learning_rate": 7.107727258466735e-05,
        "epoch": 0.12219644238205724,
        "step": 948
    },
    {
        "loss": 1.4803,
        "grad_norm": 2.6943857669830322,
        "learning_rate": 7.102206870341728e-05,
        "epoch": 0.12232534158288219,
        "step": 949
    },
    {
        "loss": 2.2787,
        "grad_norm": 2.08096981048584,
        "learning_rate": 7.096683367639278e-05,
        "epoch": 0.12245424078370715,
        "step": 950
    },
    {
        "loss": 2.4602,
        "grad_norm": 1.669201374053955,
        "learning_rate": 7.091156758542873e-05,
        "epoch": 0.1225831399845321,
        "step": 951
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.4221724271774292,
        "learning_rate": 7.085627051240597e-05,
        "epoch": 0.12271203918535704,
        "step": 952
    },
    {
        "loss": 1.7819,
        "grad_norm": 2.3955185413360596,
        "learning_rate": 7.080094253925125e-05,
        "epoch": 0.122840938386182,
        "step": 953
    },
    {
        "loss": 1.6349,
        "grad_norm": 2.6010046005249023,
        "learning_rate": 7.074558374793716e-05,
        "epoch": 0.12296983758700696,
        "step": 954
    },
    {
        "loss": 2.1491,
        "grad_norm": 1.1969292163848877,
        "learning_rate": 7.069019422048189e-05,
        "epoch": 0.12309873678783191,
        "step": 955
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.3625513315200806,
        "learning_rate": 7.063477403894916e-05,
        "epoch": 0.12322763598865687,
        "step": 956
    },
    {
        "loss": 2.1939,
        "grad_norm": 1.685867190361023,
        "learning_rate": 7.057932328544818e-05,
        "epoch": 0.12335653518948182,
        "step": 957
    },
    {
        "loss": 2.3641,
        "grad_norm": 1.5358161926269531,
        "learning_rate": 7.052384204213338e-05,
        "epoch": 0.12348543439030678,
        "step": 958
    },
    {
        "loss": 1.698,
        "grad_norm": 2.098759889602661,
        "learning_rate": 7.046833039120442e-05,
        "epoch": 0.12361433359113173,
        "step": 959
    },
    {
        "loss": 1.9007,
        "grad_norm": 2.145879030227661,
        "learning_rate": 7.041278841490594e-05,
        "epoch": 0.12374323279195669,
        "step": 960
    },
    {
        "loss": 1.2078,
        "grad_norm": 2.275244951248169,
        "learning_rate": 7.035721619552758e-05,
        "epoch": 0.12387213199278164,
        "step": 961
    },
    {
        "loss": 2.2085,
        "grad_norm": 1.5466941595077515,
        "learning_rate": 7.030161381540374e-05,
        "epoch": 0.1240010311936066,
        "step": 962
    },
    {
        "loss": 2.3,
        "grad_norm": 1.5692195892333984,
        "learning_rate": 7.024598135691354e-05,
        "epoch": 0.12412993039443156,
        "step": 963
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.323155164718628,
        "learning_rate": 7.019031890248064e-05,
        "epoch": 0.12425882959525651,
        "step": 964
    },
    {
        "loss": 1.7435,
        "grad_norm": 2.737858772277832,
        "learning_rate": 7.013462653457314e-05,
        "epoch": 0.12438772879608147,
        "step": 965
    },
    {
        "loss": 1.8368,
        "grad_norm": 2.822777271270752,
        "learning_rate": 7.007890433570349e-05,
        "epoch": 0.12451662799690642,
        "step": 966
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.0493614673614502,
        "learning_rate": 7.002315238842831e-05,
        "epoch": 0.12464552719773138,
        "step": 967
    },
    {
        "loss": 2.0589,
        "grad_norm": 2.990185260772705,
        "learning_rate": 6.996737077534829e-05,
        "epoch": 0.12477442639855633,
        "step": 968
    },
    {
        "loss": 2.4779,
        "grad_norm": 1.2185351848602295,
        "learning_rate": 6.991155957910808e-05,
        "epoch": 0.12490332559938129,
        "step": 969
    },
    {
        "loss": 1.4497,
        "grad_norm": 1.0236119031906128,
        "learning_rate": 6.985571888239619e-05,
        "epoch": 0.12503222480020623,
        "step": 970
    },
    {
        "loss": 2.3718,
        "grad_norm": 1.6939330101013184,
        "learning_rate": 6.979984876794474e-05,
        "epoch": 0.1251611240010312,
        "step": 971
    },
    {
        "loss": 2.5013,
        "grad_norm": 1.9231541156768799,
        "learning_rate": 6.974394931852956e-05,
        "epoch": 0.12529002320185614,
        "step": 972
    },
    {
        "loss": 2.0737,
        "grad_norm": 2.0776567459106445,
        "learning_rate": 6.968802061696988e-05,
        "epoch": 0.1254189224026811,
        "step": 973
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.016575574874878,
        "learning_rate": 6.963206274612825e-05,
        "epoch": 0.12554782160350605,
        "step": 974
    },
    {
        "loss": 1.7437,
        "grad_norm": 1.6804051399230957,
        "learning_rate": 6.95760757889105e-05,
        "epoch": 0.125676720804331,
        "step": 975
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.8717577457427979,
        "learning_rate": 6.952005982826547e-05,
        "epoch": 0.12580562000515597,
        "step": 976
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.1141881942749023,
        "learning_rate": 6.946401494718505e-05,
        "epoch": 0.12593451920598092,
        "step": 977
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.809259295463562,
        "learning_rate": 6.940794122870393e-05,
        "epoch": 0.12606341840680588,
        "step": 978
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.08314847946167,
        "learning_rate": 6.93518387558995e-05,
        "epoch": 0.12619231760763083,
        "step": 979
    },
    {
        "loss": 2.4794,
        "grad_norm": 1.0851926803588867,
        "learning_rate": 6.929570761189186e-05,
        "epoch": 0.1263212168084558,
        "step": 980
    },
    {
        "loss": 2.0745,
        "grad_norm": 1.8264188766479492,
        "learning_rate": 6.923954787984344e-05,
        "epoch": 0.12645011600928074,
        "step": 981
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.314080834388733,
        "learning_rate": 6.918335964295916e-05,
        "epoch": 0.1265790152101057,
        "step": 982
    },
    {
        "loss": 2.1677,
        "grad_norm": 1.9898656606674194,
        "learning_rate": 6.912714298448613e-05,
        "epoch": 0.12670791441093066,
        "step": 983
    },
    {
        "loss": 2.3137,
        "grad_norm": 1.4226617813110352,
        "learning_rate": 6.907089798771349e-05,
        "epoch": 0.1268368136117556,
        "step": 984
    },
    {
        "loss": 1.4778,
        "grad_norm": 2.2817790508270264,
        "learning_rate": 6.901462473597245e-05,
        "epoch": 0.12696571281258057,
        "step": 985
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.043175220489502,
        "learning_rate": 6.89583233126361e-05,
        "epoch": 0.12709461201340552,
        "step": 986
    },
    {
        "loss": 2.2058,
        "grad_norm": 2.239290237426758,
        "learning_rate": 6.890199380111921e-05,
        "epoch": 0.12722351121423048,
        "step": 987
    },
    {
        "loss": 2.4072,
        "grad_norm": 1.8166477680206299,
        "learning_rate": 6.884563628487815e-05,
        "epoch": 0.12735241041505543,
        "step": 988
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.6109884977340698,
        "learning_rate": 6.878925084741086e-05,
        "epoch": 0.1274813096158804,
        "step": 989
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.6621861457824707,
        "learning_rate": 6.873283757225657e-05,
        "epoch": 0.12761020881670534,
        "step": 990
    },
    {
        "loss": 2.1259,
        "grad_norm": 1.8859127759933472,
        "learning_rate": 6.86763965429958e-05,
        "epoch": 0.1277391080175303,
        "step": 991
    },
    {
        "loss": 2.014,
        "grad_norm": 2.551025867462158,
        "learning_rate": 6.861992784325018e-05,
        "epoch": 0.12786800721835526,
        "step": 992
    },
    {
        "loss": 2.3494,
        "grad_norm": 1.3396358489990234,
        "learning_rate": 6.856343155668232e-05,
        "epoch": 0.1279969064191802,
        "step": 993
    },
    {
        "loss": 1.8971,
        "grad_norm": 3.0764570236206055,
        "learning_rate": 6.850690776699573e-05,
        "epoch": 0.12812580562000517,
        "step": 994
    },
    {
        "loss": 2.3924,
        "grad_norm": 1.4371507167816162,
        "learning_rate": 6.845035655793464e-05,
        "epoch": 0.12825470482083012,
        "step": 995
    },
    {
        "loss": 2.2865,
        "grad_norm": 1.448263168334961,
        "learning_rate": 6.839377801328392e-05,
        "epoch": 0.12838360402165508,
        "step": 996
    },
    {
        "loss": 2.0777,
        "grad_norm": 2.4063103199005127,
        "learning_rate": 6.833717221686892e-05,
        "epoch": 0.12851250322248,
        "step": 997
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.936141848564148,
        "learning_rate": 6.828053925255539e-05,
        "epoch": 0.12864140242330496,
        "step": 998
    },
    {
        "loss": 0.9769,
        "grad_norm": 2.822690963745117,
        "learning_rate": 6.822387920424936e-05,
        "epoch": 0.12877030162412992,
        "step": 999
    },
    {
        "loss": 2.0066,
        "grad_norm": 2.2927839756011963,
        "learning_rate": 6.816719215589688e-05,
        "epoch": 0.12889920082495487,
        "step": 1000
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.8001980781555176,
        "learning_rate": 6.811047819148413e-05,
        "epoch": 0.12902810002577983,
        "step": 1001
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.7398217916488647,
        "learning_rate": 6.805373739503707e-05,
        "epoch": 0.12915699922660479,
        "step": 1002
    },
    {
        "loss": 1.3534,
        "grad_norm": 2.449031114578247,
        "learning_rate": 6.799696985062149e-05,
        "epoch": 0.12928589842742974,
        "step": 1003
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.6446210145950317,
        "learning_rate": 6.794017564234274e-05,
        "epoch": 0.1294147976282547,
        "step": 1004
    },
    {
        "loss": 2.5146,
        "grad_norm": 1.2029762268066406,
        "learning_rate": 6.788335485434572e-05,
        "epoch": 0.12954369682907965,
        "step": 1005
    },
    {
        "loss": 1.445,
        "grad_norm": 2.6038382053375244,
        "learning_rate": 6.782650757081471e-05,
        "epoch": 0.1296725960299046,
        "step": 1006
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.071861982345581,
        "learning_rate": 6.776963387597322e-05,
        "epoch": 0.12980149523072956,
        "step": 1007
    },
    {
        "loss": 2.3155,
        "grad_norm": 2.039374351501465,
        "learning_rate": 6.771273385408388e-05,
        "epoch": 0.12993039443155452,
        "step": 1008
    },
    {
        "loss": 2.671,
        "grad_norm": 1.4963420629501343,
        "learning_rate": 6.76558075894484e-05,
        "epoch": 0.13005929363237947,
        "step": 1009
    },
    {
        "loss": 2.6039,
        "grad_norm": 2.041557550430298,
        "learning_rate": 6.759885516640728e-05,
        "epoch": 0.13018819283320443,
        "step": 1010
    },
    {
        "loss": 2.4184,
        "grad_norm": 1.2583503723144531,
        "learning_rate": 6.75418766693398e-05,
        "epoch": 0.13031709203402939,
        "step": 1011
    },
    {
        "loss": 1.8298,
        "grad_norm": 2.5222108364105225,
        "learning_rate": 6.748487218266393e-05,
        "epoch": 0.13044599123485434,
        "step": 1012
    },
    {
        "loss": 1.9851,
        "grad_norm": 2.4253089427948,
        "learning_rate": 6.742784179083607e-05,
        "epoch": 0.1305748904356793,
        "step": 1013
    },
    {
        "loss": 2.234,
        "grad_norm": 2.396707057952881,
        "learning_rate": 6.737078557835104e-05,
        "epoch": 0.13070378963650425,
        "step": 1014
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.296737790107727,
        "learning_rate": 6.731370362974193e-05,
        "epoch": 0.1308326888373292,
        "step": 1015
    },
    {
        "loss": 1.2314,
        "grad_norm": 2.7525906562805176,
        "learning_rate": 6.725659602957987e-05,
        "epoch": 0.13096158803815416,
        "step": 1016
    },
    {
        "loss": 2.2325,
        "grad_norm": 1.9839895963668823,
        "learning_rate": 6.719946286247412e-05,
        "epoch": 0.13109048723897912,
        "step": 1017
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.634059190750122,
        "learning_rate": 6.714230421307175e-05,
        "epoch": 0.13121938643980408,
        "step": 1018
    },
    {
        "loss": 1.8419,
        "grad_norm": 1.617200493812561,
        "learning_rate": 6.708512016605758e-05,
        "epoch": 0.13134828564062903,
        "step": 1019
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.2889747619628906,
        "learning_rate": 6.702791080615407e-05,
        "epoch": 0.131477184841454,
        "step": 1020
    },
    {
        "loss": 2.4004,
        "grad_norm": 2.3308939933776855,
        "learning_rate": 6.69706762181212e-05,
        "epoch": 0.13160608404227894,
        "step": 1021
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.153013229370117,
        "learning_rate": 6.691341648675633e-05,
        "epoch": 0.1317349832431039,
        "step": 1022
    },
    {
        "loss": 2.3176,
        "grad_norm": 1.7814348936080933,
        "learning_rate": 6.6856131696894e-05,
        "epoch": 0.13186388244392885,
        "step": 1023
    },
    {
        "loss": 2.6254,
        "grad_norm": 1.4915562868118286,
        "learning_rate": 6.679882193340596e-05,
        "epoch": 0.1319927816447538,
        "step": 1024
    },
    {
        "loss": 1.872,
        "grad_norm": 1.480134129524231,
        "learning_rate": 6.674148728120096e-05,
        "epoch": 0.13212168084557877,
        "step": 1025
    },
    {
        "loss": 1.884,
        "grad_norm": 1.7197315692901611,
        "learning_rate": 6.668412782522455e-05,
        "epoch": 0.13225058004640372,
        "step": 1026
    },
    {
        "loss": 2.133,
        "grad_norm": 1.933217167854309,
        "learning_rate": 6.66267436504591e-05,
        "epoch": 0.13237947924722868,
        "step": 1027
    },
    {
        "loss": 1.6015,
        "grad_norm": 2.208787679672241,
        "learning_rate": 6.656933484192358e-05,
        "epoch": 0.13250837844805363,
        "step": 1028
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.364332675933838,
        "learning_rate": 6.651190148467342e-05,
        "epoch": 0.1326372776488786,
        "step": 1029
    },
    {
        "loss": 1.8204,
        "grad_norm": 2.2833216190338135,
        "learning_rate": 6.645444366380052e-05,
        "epoch": 0.13276617684970354,
        "step": 1030
    },
    {
        "loss": 1.9384,
        "grad_norm": 2.6494925022125244,
        "learning_rate": 6.639696146443288e-05,
        "epoch": 0.1328950760505285,
        "step": 1031
    },
    {
        "loss": 2.2423,
        "grad_norm": 2.2647340297698975,
        "learning_rate": 6.633945497173474e-05,
        "epoch": 0.13302397525135345,
        "step": 1032
    },
    {
        "loss": 2.008,
        "grad_norm": 2.1803152561187744,
        "learning_rate": 6.628192427090628e-05,
        "epoch": 0.1331528744521784,
        "step": 1033
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.3666788339614868,
        "learning_rate": 6.622436944718359e-05,
        "epoch": 0.13328177365300334,
        "step": 1034
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.5426210165023804,
        "learning_rate": 6.616679058583842e-05,
        "epoch": 0.1334106728538283,
        "step": 1035
    },
    {
        "loss": 2.438,
        "grad_norm": 2.288997173309326,
        "learning_rate": 6.610918777217818e-05,
        "epoch": 0.13353957205465325,
        "step": 1036
    },
    {
        "loss": 1.689,
        "grad_norm": 2.0194432735443115,
        "learning_rate": 6.605156109154579e-05,
        "epoch": 0.1336684712554782,
        "step": 1037
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.2980690002441406,
        "learning_rate": 6.599391062931953e-05,
        "epoch": 0.13379737045630316,
        "step": 1038
    },
    {
        "loss": 2.175,
        "grad_norm": 2.2524635791778564,
        "learning_rate": 6.593623647091281e-05,
        "epoch": 0.13392626965712812,
        "step": 1039
    },
    {
        "loss": 1.7462,
        "grad_norm": 2.123997688293457,
        "learning_rate": 6.587853870177433e-05,
        "epoch": 0.13405516885795307,
        "step": 1040
    },
    {
        "loss": 1.5647,
        "grad_norm": 2.5234081745147705,
        "learning_rate": 6.582081740738762e-05,
        "epoch": 0.13418406805877803,
        "step": 1041
    },
    {
        "loss": 1.4459,
        "grad_norm": 2.432673454284668,
        "learning_rate": 6.576307267327109e-05,
        "epoch": 0.13431296725960298,
        "step": 1042
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.164661169052124,
        "learning_rate": 6.570530458497795e-05,
        "epoch": 0.13444186646042794,
        "step": 1043
    },
    {
        "loss": 1.8968,
        "grad_norm": 1.9152274131774902,
        "learning_rate": 6.564751322809593e-05,
        "epoch": 0.1345707656612529,
        "step": 1044
    },
    {
        "loss": 1.9029,
        "grad_norm": 2.7809362411499023,
        "learning_rate": 6.55896986882473e-05,
        "epoch": 0.13469966486207785,
        "step": 1045
    },
    {
        "loss": 2.1462,
        "grad_norm": 1.7934937477111816,
        "learning_rate": 6.553186105108863e-05,
        "epoch": 0.1348285640629028,
        "step": 1046
    },
    {
        "loss": 2.4529,
        "grad_norm": 1.7345949411392212,
        "learning_rate": 6.54740004023107e-05,
        "epoch": 0.13495746326372776,
        "step": 1047
    },
    {
        "loss": 2.083,
        "grad_norm": 1.5165536403656006,
        "learning_rate": 6.541611682763844e-05,
        "epoch": 0.13508636246455272,
        "step": 1048
    },
    {
        "loss": 2.1126,
        "grad_norm": 1.4819591045379639,
        "learning_rate": 6.535821041283071e-05,
        "epoch": 0.13521526166537767,
        "step": 1049
    },
    {
        "loss": 2.2104,
        "grad_norm": 1.8633793592453003,
        "learning_rate": 6.530028124368022e-05,
        "epoch": 0.13534416086620263,
        "step": 1050
    },
    {
        "loss": 1.6765,
        "grad_norm": 2.78434681892395,
        "learning_rate": 6.524232940601335e-05,
        "epoch": 0.13547306006702758,
        "step": 1051
    },
    {
        "loss": 1.6916,
        "grad_norm": 1.7766107320785522,
        "learning_rate": 6.518435498569015e-05,
        "epoch": 0.13560195926785254,
        "step": 1052
    },
    {
        "loss": 2.1949,
        "grad_norm": 1.6999112367630005,
        "learning_rate": 6.512635806860405e-05,
        "epoch": 0.1357308584686775,
        "step": 1053
    },
    {
        "loss": 2.6824,
        "grad_norm": 1.8639419078826904,
        "learning_rate": 6.506833874068187e-05,
        "epoch": 0.13585975766950245,
        "step": 1054
    },
    {
        "loss": 2.4478,
        "grad_norm": 1.7908146381378174,
        "learning_rate": 6.501029708788357e-05,
        "epoch": 0.1359886568703274,
        "step": 1055
    },
    {
        "loss": 2.3855,
        "grad_norm": 1.4375979900360107,
        "learning_rate": 6.495223319620224e-05,
        "epoch": 0.13611755607115236,
        "step": 1056
    },
    {
        "loss": 1.9952,
        "grad_norm": 2.766923427581787,
        "learning_rate": 6.489414715166389e-05,
        "epoch": 0.13624645527197732,
        "step": 1057
    },
    {
        "loss": 1.5135,
        "grad_norm": 2.7198355197906494,
        "learning_rate": 6.483603904032738e-05,
        "epoch": 0.13637535447280227,
        "step": 1058
    },
    {
        "loss": 2.4241,
        "grad_norm": 1.799243688583374,
        "learning_rate": 6.477790894828421e-05,
        "epoch": 0.13650425367362723,
        "step": 1059
    },
    {
        "loss": 1.4685,
        "grad_norm": 2.884054183959961,
        "learning_rate": 6.471975696165849e-05,
        "epoch": 0.13663315287445219,
        "step": 1060
    },
    {
        "loss": 1.9664,
        "grad_norm": 1.8552405834197998,
        "learning_rate": 6.466158316660677e-05,
        "epoch": 0.13676205207527714,
        "step": 1061
    },
    {
        "loss": 2.192,
        "grad_norm": 1.436677098274231,
        "learning_rate": 6.460338764931791e-05,
        "epoch": 0.1368909512761021,
        "step": 1062
    },
    {
        "loss": 2.1413,
        "grad_norm": 2.7882745265960693,
        "learning_rate": 6.454517049601291e-05,
        "epoch": 0.13701985047692705,
        "step": 1063
    },
    {
        "loss": 1.6673,
        "grad_norm": 2.2707691192626953,
        "learning_rate": 6.448693179294486e-05,
        "epoch": 0.137148749677752,
        "step": 1064
    },
    {
        "loss": 1.9479,
        "grad_norm": 2.1858651638031006,
        "learning_rate": 6.44286716263988e-05,
        "epoch": 0.13727764887857696,
        "step": 1065
    },
    {
        "loss": 2.0973,
        "grad_norm": 1.3695601224899292,
        "learning_rate": 6.437039008269154e-05,
        "epoch": 0.13740654807940192,
        "step": 1066
    },
    {
        "loss": 2.379,
        "grad_norm": 1.368046522140503,
        "learning_rate": 6.431208724817153e-05,
        "epoch": 0.13753544728022687,
        "step": 1067
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.858168125152588,
        "learning_rate": 6.425376320921883e-05,
        "epoch": 0.13766434648105183,
        "step": 1068
    },
    {
        "loss": 2.2952,
        "grad_norm": 2.0695624351501465,
        "learning_rate": 6.41954180522449e-05,
        "epoch": 0.13779324568187679,
        "step": 1069
    },
    {
        "loss": 1.9641,
        "grad_norm": 1.7237097024917603,
        "learning_rate": 6.413705186369246e-05,
        "epoch": 0.1379221448827017,
        "step": 1070
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.0136349201202393,
        "learning_rate": 6.407866473003538e-05,
        "epoch": 0.13805104408352667,
        "step": 1071
    },
    {
        "loss": 1.3842,
        "grad_norm": 3.5724337100982666,
        "learning_rate": 6.402025673777863e-05,
        "epoch": 0.13817994328435163,
        "step": 1072
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.7548489570617676,
        "learning_rate": 6.3961827973458e-05,
        "epoch": 0.13830884248517658,
        "step": 1073
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.8894495964050293,
        "learning_rate": 6.390337852364013e-05,
        "epoch": 0.13843774168600154,
        "step": 1074
    },
    {
        "loss": 2.6247,
        "grad_norm": 1.303592324256897,
        "learning_rate": 6.384490847492225e-05,
        "epoch": 0.1385666408868265,
        "step": 1075
    },
    {
        "loss": 1.3476,
        "grad_norm": 2.53877329826355,
        "learning_rate": 6.378641791393211e-05,
        "epoch": 0.13869554008765145,
        "step": 1076
    },
    {
        "loss": 2.1564,
        "grad_norm": 1.2219572067260742,
        "learning_rate": 6.372790692732792e-05,
        "epoch": 0.1388244392884764,
        "step": 1077
    },
    {
        "loss": 2.3505,
        "grad_norm": 1.8963834047317505,
        "learning_rate": 6.366937560179808e-05,
        "epoch": 0.13895333848930136,
        "step": 1078
    },
    {
        "loss": 2.0544,
        "grad_norm": 2.0885510444641113,
        "learning_rate": 6.361082402406114e-05,
        "epoch": 0.13908223769012631,
        "step": 1079
    },
    {
        "loss": 1.9818,
        "grad_norm": 2.2110793590545654,
        "learning_rate": 6.355225228086566e-05,
        "epoch": 0.13921113689095127,
        "step": 1080
    },
    {
        "loss": 2.3883,
        "grad_norm": 1.4956563711166382,
        "learning_rate": 6.34936604589901e-05,
        "epoch": 0.13934003609177623,
        "step": 1081
    },
    {
        "loss": 1.8436,
        "grad_norm": 3.3463876247406006,
        "learning_rate": 6.343504864524264e-05,
        "epoch": 0.13946893529260118,
        "step": 1082
    },
    {
        "loss": 1.4943,
        "grad_norm": 2.872792959213257,
        "learning_rate": 6.337641692646106e-05,
        "epoch": 0.13959783449342614,
        "step": 1083
    },
    {
        "loss": 2.1904,
        "grad_norm": 1.8659489154815674,
        "learning_rate": 6.331776538951266e-05,
        "epoch": 0.1397267336942511,
        "step": 1084
    },
    {
        "loss": 2.3902,
        "grad_norm": 1.3160159587860107,
        "learning_rate": 6.325909412129413e-05,
        "epoch": 0.13985563289507605,
        "step": 1085
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.9624093770980835,
        "learning_rate": 6.320040320873137e-05,
        "epoch": 0.139984532095901,
        "step": 1086
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.3103885650634766,
        "learning_rate": 6.314169273877934e-05,
        "epoch": 0.14011343129672596,
        "step": 1087
    },
    {
        "loss": 2.012,
        "grad_norm": 2.0235342979431152,
        "learning_rate": 6.308296279842205e-05,
        "epoch": 0.14024233049755092,
        "step": 1088
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.5985583066940308,
        "learning_rate": 6.302421347467226e-05,
        "epoch": 0.14037122969837587,
        "step": 1089
    },
    {
        "loss": 1.6223,
        "grad_norm": 2.4448580741882324,
        "learning_rate": 6.296544485457156e-05,
        "epoch": 0.14050012889920083,
        "step": 1090
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.6005746126174927,
        "learning_rate": 6.290665702519008e-05,
        "epoch": 0.14062902810002578,
        "step": 1091
    },
    {
        "loss": 1.8257,
        "grad_norm": 1.6364152431488037,
        "learning_rate": 6.284785007362636e-05,
        "epoch": 0.14075792730085074,
        "step": 1092
    },
    {
        "loss": 1.8152,
        "grad_norm": 3.047415256500244,
        "learning_rate": 6.278902408700737e-05,
        "epoch": 0.1408868265016757,
        "step": 1093
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.631174087524414,
        "learning_rate": 6.27301791524882e-05,
        "epoch": 0.14101572570250065,
        "step": 1094
    },
    {
        "loss": 1.4882,
        "grad_norm": 2.5130789279937744,
        "learning_rate": 6.267131535725205e-05,
        "epoch": 0.1411446249033256,
        "step": 1095
    },
    {
        "loss": 2.4835,
        "grad_norm": 1.3067110776901245,
        "learning_rate": 6.261243278851006e-05,
        "epoch": 0.14127352410415056,
        "step": 1096
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.498504877090454,
        "learning_rate": 6.255353153350114e-05,
        "epoch": 0.14140242330497552,
        "step": 1097
    },
    {
        "loss": 2.4769,
        "grad_norm": 1.615096092224121,
        "learning_rate": 6.249461167949201e-05,
        "epoch": 0.14153132250580047,
        "step": 1098
    },
    {
        "loss": 1.5481,
        "grad_norm": 2.50408673286438,
        "learning_rate": 6.243567331377677e-05,
        "epoch": 0.14166022170662543,
        "step": 1099
    },
    {
        "loss": 1.6431,
        "grad_norm": 2.5830588340759277,
        "learning_rate": 6.237671652367708e-05,
        "epoch": 0.14178912090745038,
        "step": 1100
    },
    {
        "loss": 1.776,
        "grad_norm": 2.2641823291778564,
        "learning_rate": 6.231774139654188e-05,
        "epoch": 0.14191802010827534,
        "step": 1101
    },
    {
        "loss": 1.0919,
        "grad_norm": 2.4959352016448975,
        "learning_rate": 6.225874801974722e-05,
        "epoch": 0.1420469193091003,
        "step": 1102
    },
    {
        "loss": 2.161,
        "grad_norm": 2.5204029083251953,
        "learning_rate": 6.21997364806962e-05,
        "epoch": 0.14217581850992525,
        "step": 1103
    },
    {
        "loss": 1.967,
        "grad_norm": 1.6577057838439941,
        "learning_rate": 6.21407068668189e-05,
        "epoch": 0.1423047177107502,
        "step": 1104
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.027620315551758,
        "learning_rate": 6.208165926557208e-05,
        "epoch": 0.14243361691157516,
        "step": 1105
    },
    {
        "loss": 2.205,
        "grad_norm": 2.2655956745147705,
        "learning_rate": 6.202259376443924e-05,
        "epoch": 0.1425625161124001,
        "step": 1106
    },
    {
        "loss": 2.2118,
        "grad_norm": 2.238644599914551,
        "learning_rate": 6.196351045093033e-05,
        "epoch": 0.14269141531322505,
        "step": 1107
    },
    {
        "loss": 1.7557,
        "grad_norm": 2.678647041320801,
        "learning_rate": 6.190440941258174e-05,
        "epoch": 0.14282031451405,
        "step": 1108
    },
    {
        "loss": 1.8514,
        "grad_norm": 2.240612745285034,
        "learning_rate": 6.184529073695609e-05,
        "epoch": 0.14294921371487496,
        "step": 1109
    },
    {
        "loss": 1.6234,
        "grad_norm": 2.6493871212005615,
        "learning_rate": 6.178615451164212e-05,
        "epoch": 0.1430781129156999,
        "step": 1110
    },
    {
        "loss": 2.2477,
        "grad_norm": 1.7761462926864624,
        "learning_rate": 6.172700082425463e-05,
        "epoch": 0.14320701211652487,
        "step": 1111
    },
    {
        "loss": 2.0564,
        "grad_norm": 1.2907809019088745,
        "learning_rate": 6.166782976243421e-05,
        "epoch": 0.14333591131734982,
        "step": 1112
    },
    {
        "loss": 1.8993,
        "grad_norm": 2.4401118755340576,
        "learning_rate": 6.160864141384728e-05,
        "epoch": 0.14346481051817478,
        "step": 1113
    },
    {
        "loss": 2.6041,
        "grad_norm": 1.7714647054672241,
        "learning_rate": 6.154943586618578e-05,
        "epoch": 0.14359370971899973,
        "step": 1114
    },
    {
        "loss": 1.4303,
        "grad_norm": 2.2587337493896484,
        "learning_rate": 6.149021320716721e-05,
        "epoch": 0.1437226089198247,
        "step": 1115
    },
    {
        "loss": 2.0514,
        "grad_norm": 2.553617238998413,
        "learning_rate": 6.143097352453437e-05,
        "epoch": 0.14385150812064965,
        "step": 1116
    },
    {
        "loss": 1.847,
        "grad_norm": 1.8199549913406372,
        "learning_rate": 6.137171690605533e-05,
        "epoch": 0.1439804073214746,
        "step": 1117
    },
    {
        "loss": 2.7641,
        "grad_norm": 1.760597825050354,
        "learning_rate": 6.131244343952318e-05,
        "epoch": 0.14410930652229956,
        "step": 1118
    },
    {
        "loss": 2.4789,
        "grad_norm": 1.133905291557312,
        "learning_rate": 6.125315321275606e-05,
        "epoch": 0.1442382057231245,
        "step": 1119
    },
    {
        "loss": 2.4043,
        "grad_norm": 1.5452522039413452,
        "learning_rate": 6.119384631359686e-05,
        "epoch": 0.14436710492394947,
        "step": 1120
    },
    {
        "loss": 2.547,
        "grad_norm": 1.2204114198684692,
        "learning_rate": 6.113452282991323e-05,
        "epoch": 0.14449600412477442,
        "step": 1121
    },
    {
        "loss": 2.0746,
        "grad_norm": 1.6286977529525757,
        "learning_rate": 6.107518284959735e-05,
        "epoch": 0.14462490332559938,
        "step": 1122
    },
    {
        "loss": 1.9974,
        "grad_norm": 2.4489357471466064,
        "learning_rate": 6.1015826460565875e-05,
        "epoch": 0.14475380252642434,
        "step": 1123
    },
    {
        "loss": 2.7213,
        "grad_norm": 1.7237401008605957,
        "learning_rate": 6.0956453750759755e-05,
        "epoch": 0.1448827017272493,
        "step": 1124
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.7371597290039062,
        "learning_rate": 6.089706480814411e-05,
        "epoch": 0.14501160092807425,
        "step": 1125
    },
    {
        "loss": 1.5848,
        "grad_norm": 2.462615966796875,
        "learning_rate": 6.083765972070814e-05,
        "epoch": 0.1451405001288992,
        "step": 1126
    },
    {
        "loss": 2.3485,
        "grad_norm": 1.5555461645126343,
        "learning_rate": 6.077823857646493e-05,
        "epoch": 0.14526939932972416,
        "step": 1127
    },
    {
        "loss": 2.087,
        "grad_norm": 2.299130439758301,
        "learning_rate": 6.071880146345136e-05,
        "epoch": 0.1453982985305491,
        "step": 1128
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.263726234436035,
        "learning_rate": 6.065934846972802e-05,
        "epoch": 0.14552719773137407,
        "step": 1129
    },
    {
        "loss": 2.303,
        "grad_norm": 1.5591723918914795,
        "learning_rate": 6.0599879683378936e-05,
        "epoch": 0.14565609693219903,
        "step": 1130
    },
    {
        "loss": 1.567,
        "grad_norm": 2.690314769744873,
        "learning_rate": 6.054039519251161e-05,
        "epoch": 0.14578499613302398,
        "step": 1131
    },
    {
        "loss": 1.7285,
        "grad_norm": 2.8230533599853516,
        "learning_rate": 6.0480895085256785e-05,
        "epoch": 0.14591389533384894,
        "step": 1132
    },
    {
        "loss": 1.7919,
        "grad_norm": 2.2497446537017822,
        "learning_rate": 6.0421379449768355e-05,
        "epoch": 0.1460427945346739,
        "step": 1133
    },
    {
        "loss": 2.3408,
        "grad_norm": 1.7123006582260132,
        "learning_rate": 6.0361848374223174e-05,
        "epoch": 0.14617169373549885,
        "step": 1134
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.947202205657959,
        "learning_rate": 6.0302301946821014e-05,
        "epoch": 0.1463005929363238,
        "step": 1135
    },
    {
        "loss": 2.0924,
        "grad_norm": 1.1943109035491943,
        "learning_rate": 6.024274025578439e-05,
        "epoch": 0.14642949213714876,
        "step": 1136
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.9333434104919434,
        "learning_rate": 6.018316338935844e-05,
        "epoch": 0.14655839133797371,
        "step": 1137
    },
    {
        "loss": 1.5817,
        "grad_norm": 3.102224826812744,
        "learning_rate": 6.012357143581073e-05,
        "epoch": 0.14668729053879867,
        "step": 1138
    },
    {
        "loss": 2.3798,
        "grad_norm": 2.173826217651367,
        "learning_rate": 6.0063964483431235e-05,
        "epoch": 0.14681618973962363,
        "step": 1139
    },
    {
        "loss": 2.3398,
        "grad_norm": 2.558438539505005,
        "learning_rate": 6.000434262053214e-05,
        "epoch": 0.14694508894044858,
        "step": 1140
    },
    {
        "loss": 1.8809,
        "grad_norm": 2.0997724533081055,
        "learning_rate": 5.994470593544771e-05,
        "epoch": 0.14707398814127354,
        "step": 1141
    },
    {
        "loss": 2.5242,
        "grad_norm": 2.014610767364502,
        "learning_rate": 5.988505451653418e-05,
        "epoch": 0.14720288734209847,
        "step": 1142
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.2202982902526855,
        "learning_rate": 5.98253884521696e-05,
        "epoch": 0.14733178654292342,
        "step": 1143
    },
    {
        "loss": 2.077,
        "grad_norm": 1.9479962587356567,
        "learning_rate": 5.97657078307537e-05,
        "epoch": 0.14746068574374838,
        "step": 1144
    },
    {
        "loss": 2.189,
        "grad_norm": 1.6627485752105713,
        "learning_rate": 5.9706012740707897e-05,
        "epoch": 0.14758958494457333,
        "step": 1145
    },
    {
        "loss": 1.5342,
        "grad_norm": 2.3850598335266113,
        "learning_rate": 5.9646303270474845e-05,
        "epoch": 0.1477184841453983,
        "step": 1146
    },
    {
        "loss": 2.383,
        "grad_norm": 2.106299638748169,
        "learning_rate": 5.958657950851867e-05,
        "epoch": 0.14784738334622324,
        "step": 1147
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.9611537456512451,
        "learning_rate": 5.9526841543324594e-05,
        "epoch": 0.1479762825470482,
        "step": 1148
    },
    {
        "loss": 1.5567,
        "grad_norm": 2.3678109645843506,
        "learning_rate": 5.946708946339894e-05,
        "epoch": 0.14810518174787315,
        "step": 1149
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.3668525218963623,
        "learning_rate": 5.9407323357268854e-05,
        "epoch": 0.1482340809486981,
        "step": 1150
    },
    {
        "loss": 2.2102,
        "grad_norm": 1.310766339302063,
        "learning_rate": 5.9347543313482325e-05,
        "epoch": 0.14836298014952307,
        "step": 1151
    },
    {
        "loss": 2.5202,
        "grad_norm": 1.7093199491500854,
        "learning_rate": 5.9287749420607985e-05,
        "epoch": 0.14849187935034802,
        "step": 1152
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.5292582511901855,
        "learning_rate": 5.9227941767235005e-05,
        "epoch": 0.14862077855117298,
        "step": 1153
    },
    {
        "loss": 2.4177,
        "grad_norm": 1.8003000020980835,
        "learning_rate": 5.916812044197286e-05,
        "epoch": 0.14874967775199793,
        "step": 1154
    },
    {
        "loss": 1.6249,
        "grad_norm": 2.4468789100646973,
        "learning_rate": 5.910828553345139e-05,
        "epoch": 0.1488785769528229,
        "step": 1155
    },
    {
        "loss": 1.8491,
        "grad_norm": 2.5174973011016846,
        "learning_rate": 5.90484371303205e-05,
        "epoch": 0.14900747615364784,
        "step": 1156
    },
    {
        "loss": 1.1511,
        "grad_norm": 2.9840264320373535,
        "learning_rate": 5.8988575321250064e-05,
        "epoch": 0.1491363753544728,
        "step": 1157
    },
    {
        "loss": 2.327,
        "grad_norm": 1.89618718624115,
        "learning_rate": 5.8928700194929885e-05,
        "epoch": 0.14926527455529776,
        "step": 1158
    },
    {
        "loss": 1.472,
        "grad_norm": 2.3812363147735596,
        "learning_rate": 5.886881184006945e-05,
        "epoch": 0.1493941737561227,
        "step": 1159
    },
    {
        "loss": 2.2678,
        "grad_norm": 1.2708152532577515,
        "learning_rate": 5.880891034539785e-05,
        "epoch": 0.14952307295694767,
        "step": 1160
    },
    {
        "loss": 1.5903,
        "grad_norm": 3.4006292819976807,
        "learning_rate": 5.8748995799663676e-05,
        "epoch": 0.14965197215777262,
        "step": 1161
    },
    {
        "loss": 2.074,
        "grad_norm": 2.404435157775879,
        "learning_rate": 5.8689068291634785e-05,
        "epoch": 0.14978087135859758,
        "step": 1162
    },
    {
        "loss": 2.4761,
        "grad_norm": 1.3358207941055298,
        "learning_rate": 5.8629127910098316e-05,
        "epoch": 0.14990977055942253,
        "step": 1163
    },
    {
        "loss": 2.1676,
        "grad_norm": 2.100461483001709,
        "learning_rate": 5.856917474386044e-05,
        "epoch": 0.1500386697602475,
        "step": 1164
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.150245189666748,
        "learning_rate": 5.85092088817463e-05,
        "epoch": 0.15016756896107245,
        "step": 1165
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.7858763933181763,
        "learning_rate": 5.8449230412599776e-05,
        "epoch": 0.1502964681618974,
        "step": 1166
    },
    {
        "loss": 2.0143,
        "grad_norm": 2.2907981872558594,
        "learning_rate": 5.838923942528351e-05,
        "epoch": 0.15042536736272236,
        "step": 1167
    },
    {
        "loss": 2.3359,
        "grad_norm": 1.975926160812378,
        "learning_rate": 5.832923600867867e-05,
        "epoch": 0.1505542665635473,
        "step": 1168
    },
    {
        "loss": 1.9263,
        "grad_norm": 1.7890772819519043,
        "learning_rate": 5.826922025168481e-05,
        "epoch": 0.15068316576437227,
        "step": 1169
    },
    {
        "loss": 2.1827,
        "grad_norm": 2.283778190612793,
        "learning_rate": 5.8209192243219754e-05,
        "epoch": 0.15081206496519722,
        "step": 1170
    },
    {
        "loss": 1.7706,
        "grad_norm": 1.8156241178512573,
        "learning_rate": 5.814915207221957e-05,
        "epoch": 0.15094096416602218,
        "step": 1171
    },
    {
        "loss": 1.7739,
        "grad_norm": 2.560044527053833,
        "learning_rate": 5.808909982763825e-05,
        "epoch": 0.15106986336684713,
        "step": 1172
    },
    {
        "loss": 1.4939,
        "grad_norm": 2.5662732124328613,
        "learning_rate": 5.802903559844769e-05,
        "epoch": 0.1511987625676721,
        "step": 1173
    },
    {
        "loss": 2.6464,
        "grad_norm": 1.6726696491241455,
        "learning_rate": 5.796895947363758e-05,
        "epoch": 0.15132766176849705,
        "step": 1174
    },
    {
        "loss": 2.1234,
        "grad_norm": 2.1336703300476074,
        "learning_rate": 5.79088715422152e-05,
        "epoch": 0.151456560969322,
        "step": 1175
    },
    {
        "loss": 1.7237,
        "grad_norm": 2.237269163131714,
        "learning_rate": 5.784877189320536e-05,
        "epoch": 0.15158546017014696,
        "step": 1176
    },
    {
        "loss": 1.7782,
        "grad_norm": 2.2435152530670166,
        "learning_rate": 5.778866061565019e-05,
        "epoch": 0.1517143593709719,
        "step": 1177
    },
    {
        "loss": 2.2759,
        "grad_norm": 2.131136417388916,
        "learning_rate": 5.772853779860905e-05,
        "epoch": 0.15184325857179687,
        "step": 1178
    },
    {
        "loss": 2.2119,
        "grad_norm": 1.311219573020935,
        "learning_rate": 5.766840353115843e-05,
        "epoch": 0.1519721577726218,
        "step": 1179
    },
    {
        "loss": 1.475,
        "grad_norm": 2.4821534156799316,
        "learning_rate": 5.760825790239176e-05,
        "epoch": 0.15210105697344675,
        "step": 1180
    },
    {
        "loss": 2.1245,
        "grad_norm": 2.3315176963806152,
        "learning_rate": 5.7548101001419344e-05,
        "epoch": 0.1522299561742717,
        "step": 1181
    },
    {
        "loss": 1.4157,
        "grad_norm": 2.3514316082000732,
        "learning_rate": 5.748793291736807e-05,
        "epoch": 0.15235885537509666,
        "step": 1182
    },
    {
        "loss": 1.6675,
        "grad_norm": 1.9275022745132446,
        "learning_rate": 5.742775373938154e-05,
        "epoch": 0.15248775457592162,
        "step": 1183
    },
    {
        "loss": 1.9748,
        "grad_norm": 1.930832028388977,
        "learning_rate": 5.736756355661971e-05,
        "epoch": 0.15261665377674657,
        "step": 1184
    },
    {
        "loss": 2.3314,
        "grad_norm": 2.494251012802124,
        "learning_rate": 5.7307362458258874e-05,
        "epoch": 0.15274555297757153,
        "step": 1185
    },
    {
        "loss": 2.3075,
        "grad_norm": 1.2189544439315796,
        "learning_rate": 5.724715053349144e-05,
        "epoch": 0.15287445217839649,
        "step": 1186
    },
    {
        "loss": 1.9574,
        "grad_norm": 2.231217384338379,
        "learning_rate": 5.7186927871525955e-05,
        "epoch": 0.15300335137922144,
        "step": 1187
    },
    {
        "loss": 1.4777,
        "grad_norm": 3.3080809116363525,
        "learning_rate": 5.712669456158677e-05,
        "epoch": 0.1531322505800464,
        "step": 1188
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.9322935342788696,
        "learning_rate": 5.70664506929141e-05,
        "epoch": 0.15326114978087135,
        "step": 1189
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.3907428979873657,
        "learning_rate": 5.700619635476374e-05,
        "epoch": 0.1533900489816963,
        "step": 1190
    },
    {
        "loss": 1.6951,
        "grad_norm": 3.0240097045898438,
        "learning_rate": 5.6945931636407015e-05,
        "epoch": 0.15351894818252126,
        "step": 1191
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.2881176471710205,
        "learning_rate": 5.6885656627130634e-05,
        "epoch": 0.15364784738334622,
        "step": 1192
    },
    {
        "loss": 1.7985,
        "grad_norm": 2.5196022987365723,
        "learning_rate": 5.682537141623657e-05,
        "epoch": 0.15377674658417118,
        "step": 1193
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.052614212036133,
        "learning_rate": 5.676507609304189e-05,
        "epoch": 0.15390564578499613,
        "step": 1194
    },
    {
        "loss": 2.4555,
        "grad_norm": 1.5479648113250732,
        "learning_rate": 5.6704770746878624e-05,
        "epoch": 0.1540345449858211,
        "step": 1195
    },
    {
        "loss": 2.1783,
        "grad_norm": 1.775390863418579,
        "learning_rate": 5.664445546709368e-05,
        "epoch": 0.15416344418664604,
        "step": 1196
    },
    {
        "loss": 2.207,
        "grad_norm": 1.7798680067062378,
        "learning_rate": 5.6584130343048724e-05,
        "epoch": 0.154292343387471,
        "step": 1197
    },
    {
        "loss": 1.9167,
        "grad_norm": 2.0797181129455566,
        "learning_rate": 5.652379546411989e-05,
        "epoch": 0.15442124258829595,
        "step": 1198
    },
    {
        "loss": 2.4329,
        "grad_norm": 1.3916714191436768,
        "learning_rate": 5.646345091969786e-05,
        "epoch": 0.1545501417891209,
        "step": 1199
    },
    {
        "loss": 1.9763,
        "grad_norm": 2.243328094482422,
        "learning_rate": 5.640309679918766e-05,
        "epoch": 0.15467904098994587,
        "step": 1200
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.9303076267242432,
        "learning_rate": 5.6342733192008365e-05,
        "epoch": 0.15480794019077082,
        "step": 1201
    },
    {
        "loss": 2.2023,
        "grad_norm": 1.8563753366470337,
        "learning_rate": 5.628236018759327e-05,
        "epoch": 0.15493683939159578,
        "step": 1202
    },
    {
        "loss": 1.7165,
        "grad_norm": 1.5199105739593506,
        "learning_rate": 5.622197787538948e-05,
        "epoch": 0.15506573859242073,
        "step": 1203
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.010321617126465,
        "learning_rate": 5.616158634485794e-05,
        "epoch": 0.1551946377932457,
        "step": 1204
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.925012469291687,
        "learning_rate": 5.6101185685473234e-05,
        "epoch": 0.15532353699407064,
        "step": 1205
    },
    {
        "loss": 2.3744,
        "grad_norm": 1.5748735666275024,
        "learning_rate": 5.604077598672349e-05,
        "epoch": 0.1554524361948956,
        "step": 1206
    },
    {
        "loss": 2.406,
        "grad_norm": 1.5048506259918213,
        "learning_rate": 5.59803573381102e-05,
        "epoch": 0.15558133539572055,
        "step": 1207
    },
    {
        "loss": 1.9574,
        "grad_norm": 1.8768467903137207,
        "learning_rate": 5.591992982914816e-05,
        "epoch": 0.1557102345965455,
        "step": 1208
    },
    {
        "loss": 1.9677,
        "grad_norm": 2.523789405822754,
        "learning_rate": 5.585949354936522e-05,
        "epoch": 0.15583913379737047,
        "step": 1209
    },
    {
        "loss": 1.7932,
        "grad_norm": 3.5195841789245605,
        "learning_rate": 5.579904858830229e-05,
        "epoch": 0.15596803299819542,
        "step": 1210
    },
    {
        "loss": 2.3649,
        "grad_norm": 2.2255678176879883,
        "learning_rate": 5.573859503551314e-05,
        "epoch": 0.15609693219902038,
        "step": 1211
    },
    {
        "loss": 2.5986,
        "grad_norm": 1.469994068145752,
        "learning_rate": 5.567813298056425e-05,
        "epoch": 0.15622583139984533,
        "step": 1212
    },
    {
        "loss": 2.5176,
        "grad_norm": 1.3751503229141235,
        "learning_rate": 5.561766251303466e-05,
        "epoch": 0.1563547306006703,
        "step": 1213
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.709431529045105,
        "learning_rate": 5.555718372251595e-05,
        "epoch": 0.15648362980149524,
        "step": 1214
    },
    {
        "loss": 2.4424,
        "grad_norm": 1.2438831329345703,
        "learning_rate": 5.549669669861195e-05,
        "epoch": 0.15661252900232017,
        "step": 1215
    },
    {
        "loss": 1.7101,
        "grad_norm": 2.705186605453491,
        "learning_rate": 5.5436201530938804e-05,
        "epoch": 0.15674142820314513,
        "step": 1216
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.3522011041641235,
        "learning_rate": 5.537569830912458e-05,
        "epoch": 0.15687032740397008,
        "step": 1217
    },
    {
        "loss": 2.2818,
        "grad_norm": 1.6138802766799927,
        "learning_rate": 5.531518712280936e-05,
        "epoch": 0.15699922660479504,
        "step": 1218
    },
    {
        "loss": 1.9352,
        "grad_norm": 3.2965476512908936,
        "learning_rate": 5.5254668061645056e-05,
        "epoch": 0.15712812580562,
        "step": 1219
    },
    {
        "loss": 2.5982,
        "grad_norm": 2.2263760566711426,
        "learning_rate": 5.519414121529517e-05,
        "epoch": 0.15725702500644495,
        "step": 1220
    },
    {
        "loss": 1.8901,
        "grad_norm": 2.1958975791931152,
        "learning_rate": 5.513360667343476e-05,
        "epoch": 0.1573859242072699,
        "step": 1221
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.5380241870880127,
        "learning_rate": 5.5073064525750316e-05,
        "epoch": 0.15751482340809486,
        "step": 1222
    },
    {
        "loss": 2.5064,
        "grad_norm": 1.4998022317886353,
        "learning_rate": 5.501251486193959e-05,
        "epoch": 0.15764372260891982,
        "step": 1223
    },
    {
        "loss": 2.0233,
        "grad_norm": 1.8667199611663818,
        "learning_rate": 5.495195777171145e-05,
        "epoch": 0.15777262180974477,
        "step": 1224
    },
    {
        "loss": 2.3841,
        "grad_norm": 1.394951343536377,
        "learning_rate": 5.4891393344785746e-05,
        "epoch": 0.15790152101056973,
        "step": 1225
    },
    {
        "loss": 2.1705,
        "grad_norm": 1.816184639930725,
        "learning_rate": 5.483082167089329e-05,
        "epoch": 0.15803042021139468,
        "step": 1226
    },
    {
        "loss": 2.2939,
        "grad_norm": 1.3745039701461792,
        "learning_rate": 5.4770242839775496e-05,
        "epoch": 0.15815931941221964,
        "step": 1227
    },
    {
        "loss": 2.1953,
        "grad_norm": 1.5550107955932617,
        "learning_rate": 5.470965694118452e-05,
        "epoch": 0.1582882186130446,
        "step": 1228
    },
    {
        "loss": 2.2219,
        "grad_norm": 1.6929733753204346,
        "learning_rate": 5.464906406488289e-05,
        "epoch": 0.15841711781386955,
        "step": 1229
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.6220073699951172,
        "learning_rate": 5.4588464300643485e-05,
        "epoch": 0.1585460170146945,
        "step": 1230
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.925216555595398,
        "learning_rate": 5.452785773824945e-05,
        "epoch": 0.15867491621551946,
        "step": 1231
    },
    {
        "loss": 1.9345,
        "grad_norm": 2.079005002975464,
        "learning_rate": 5.446724446749396e-05,
        "epoch": 0.15880381541634442,
        "step": 1232
    },
    {
        "loss": 1.3166,
        "grad_norm": 2.6075973510742188,
        "learning_rate": 5.4406624578180096e-05,
        "epoch": 0.15893271461716937,
        "step": 1233
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.6116212606430054,
        "learning_rate": 5.434599816012079e-05,
        "epoch": 0.15906161381799433,
        "step": 1234
    },
    {
        "loss": 2.7791,
        "grad_norm": 1.4639827013015747,
        "learning_rate": 5.428536530313867e-05,
        "epoch": 0.15919051301881929,
        "step": 1235
    },
    {
        "loss": 2.3516,
        "grad_norm": 1.2461159229278564,
        "learning_rate": 5.4224726097065836e-05,
        "epoch": 0.15931941221964424,
        "step": 1236
    },
    {
        "loss": 1.8462,
        "grad_norm": 3.6676878929138184,
        "learning_rate": 5.416408063174382e-05,
        "epoch": 0.1594483114204692,
        "step": 1237
    },
    {
        "loss": 2.039,
        "grad_norm": 2.131033182144165,
        "learning_rate": 5.4103428997023466e-05,
        "epoch": 0.15957721062129415,
        "step": 1238
    },
    {
        "loss": 1.8728,
        "grad_norm": 2.574444055557251,
        "learning_rate": 5.4042771282764715e-05,
        "epoch": 0.1597061098221191,
        "step": 1239
    },
    {
        "loss": 1.4243,
        "grad_norm": 2.4892475605010986,
        "learning_rate": 5.3982107578836524e-05,
        "epoch": 0.15983500902294406,
        "step": 1240
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.9461581707000732,
        "learning_rate": 5.3921437975116726e-05,
        "epoch": 0.15996390822376902,
        "step": 1241
    },
    {
        "loss": 2.0769,
        "grad_norm": 2.080754518508911,
        "learning_rate": 5.386076256149192e-05,
        "epoch": 0.16009280742459397,
        "step": 1242
    },
    {
        "loss": 2.2291,
        "grad_norm": 1.75394606590271,
        "learning_rate": 5.380008142785726e-05,
        "epoch": 0.16022170662541893,
        "step": 1243
    },
    {
        "loss": 2.0363,
        "grad_norm": 1.0381109714508057,
        "learning_rate": 5.3739394664116426e-05,
        "epoch": 0.16035060582624389,
        "step": 1244
    },
    {
        "loss": 1.9049,
        "grad_norm": 2.3460958003997803,
        "learning_rate": 5.367870236018141e-05,
        "epoch": 0.16047950502706884,
        "step": 1245
    },
    {
        "loss": 1.0906,
        "grad_norm": 2.5294036865234375,
        "learning_rate": 5.361800460597243e-05,
        "epoch": 0.1606084042278938,
        "step": 1246
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.9927282333374023,
        "learning_rate": 5.3557301491417744e-05,
        "epoch": 0.16073730342871875,
        "step": 1247
    },
    {
        "loss": 2.2888,
        "grad_norm": 3.087204694747925,
        "learning_rate": 5.349659310645363e-05,
        "epoch": 0.1608662026295437,
        "step": 1248
    },
    {
        "loss": 2.0391,
        "grad_norm": 1.9348068237304688,
        "learning_rate": 5.343587954102407e-05,
        "epoch": 0.16099510183036866,
        "step": 1249
    },
    {
        "loss": 1.8389,
        "grad_norm": 2.3547420501708984,
        "learning_rate": 5.337516088508079e-05,
        "epoch": 0.16112400103119362,
        "step": 1250
    },
    {
        "loss": 1.5379,
        "grad_norm": 2.835984230041504,
        "learning_rate": 5.3314437228583035e-05,
        "epoch": 0.16125290023201855,
        "step": 1251
    },
    {
        "loss": 2.2021,
        "grad_norm": 1.2403446435928345,
        "learning_rate": 5.3253708661497505e-05,
        "epoch": 0.1613817994328435,
        "step": 1252
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.9056062698364258,
        "learning_rate": 5.3192975273798076e-05,
        "epoch": 0.16151069863366846,
        "step": 1253
    },
    {
        "loss": 1.4387,
        "grad_norm": 2.467315912246704,
        "learning_rate": 5.313223715546586e-05,
        "epoch": 0.16163959783449341,
        "step": 1254
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.424344301223755,
        "learning_rate": 5.307149439648894e-05,
        "epoch": 0.16176849703531837,
        "step": 1255
    },
    {
        "loss": 2.2069,
        "grad_norm": 1.7201220989227295,
        "learning_rate": 5.3010747086862266e-05,
        "epoch": 0.16189739623614333,
        "step": 1256
    },
    {
        "loss": 2.4605,
        "grad_norm": 1.7891898155212402,
        "learning_rate": 5.2949995316587555e-05,
        "epoch": 0.16202629543696828,
        "step": 1257
    },
    {
        "loss": 1.3759,
        "grad_norm": 2.4786577224731445,
        "learning_rate": 5.2889239175673124e-05,
        "epoch": 0.16215519463779324,
        "step": 1258
    },
    {
        "loss": 1.785,
        "grad_norm": 2.2204031944274902,
        "learning_rate": 5.2828478754133725e-05,
        "epoch": 0.1622840938386182,
        "step": 1259
    },
    {
        "loss": 2.7153,
        "grad_norm": 1.8951736688613892,
        "learning_rate": 5.276771414199056e-05,
        "epoch": 0.16241299303944315,
        "step": 1260
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.6361846923828125,
        "learning_rate": 5.270694542927088e-05,
        "epoch": 0.1625418922402681,
        "step": 1261
    },
    {
        "loss": 2.347,
        "grad_norm": 2.696272611618042,
        "learning_rate": 5.2646172706008156e-05,
        "epoch": 0.16267079144109306,
        "step": 1262
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.3054614067077637,
        "learning_rate": 5.258539606224173e-05,
        "epoch": 0.16279969064191802,
        "step": 1263
    },
    {
        "loss": 2.0032,
        "grad_norm": 2.0971884727478027,
        "learning_rate": 5.252461558801678e-05,
        "epoch": 0.16292858984274297,
        "step": 1264
    },
    {
        "loss": 2.5034,
        "grad_norm": 1.8393261432647705,
        "learning_rate": 5.246383137338412e-05,
        "epoch": 0.16305748904356793,
        "step": 1265
    },
    {
        "loss": 2.1841,
        "grad_norm": 2.467176675796509,
        "learning_rate": 5.240304350840014e-05,
        "epoch": 0.16318638824439288,
        "step": 1266
    },
    {
        "loss": 2.2201,
        "grad_norm": 2.229480028152466,
        "learning_rate": 5.234225208312663e-05,
        "epoch": 0.16331528744521784,
        "step": 1267
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.005248546600342,
        "learning_rate": 5.2281457187630686e-05,
        "epoch": 0.1634441866460428,
        "step": 1268
    },
    {
        "loss": 2.112,
        "grad_norm": 1.7997612953186035,
        "learning_rate": 5.222065891198447e-05,
        "epoch": 0.16357308584686775,
        "step": 1269
    },
    {
        "loss": 2.0021,
        "grad_norm": 1.6249083280563354,
        "learning_rate": 5.2159857346265196e-05,
        "epoch": 0.1637019850476927,
        "step": 1270
    },
    {
        "loss": 1.9891,
        "grad_norm": 2.230177640914917,
        "learning_rate": 5.2099052580554985e-05,
        "epoch": 0.16383088424851766,
        "step": 1271
    },
    {
        "loss": 2.1609,
        "grad_norm": 1.393994688987732,
        "learning_rate": 5.203824470494064e-05,
        "epoch": 0.16395978344934262,
        "step": 1272
    },
    {
        "loss": 1.3622,
        "grad_norm": 2.5839922428131104,
        "learning_rate": 5.197743380951362e-05,
        "epoch": 0.16408868265016757,
        "step": 1273
    },
    {
        "loss": 1.3122,
        "grad_norm": 2.892791748046875,
        "learning_rate": 5.191661998436982e-05,
        "epoch": 0.16421758185099253,
        "step": 1274
    },
    {
        "loss": 2.4084,
        "grad_norm": 1.8079358339309692,
        "learning_rate": 5.185580331960947e-05,
        "epoch": 0.16434648105181748,
        "step": 1275
    },
    {
        "loss": 2.1129,
        "grad_norm": 1.8223869800567627,
        "learning_rate": 5.17949839053371e-05,
        "epoch": 0.16447538025264244,
        "step": 1276
    },
    {
        "loss": 2.4444,
        "grad_norm": 1.6756186485290527,
        "learning_rate": 5.1734161831661166e-05,
        "epoch": 0.1646042794534674,
        "step": 1277
    },
    {
        "loss": 2.1903,
        "grad_norm": 1.842063307762146,
        "learning_rate": 5.167333718869417e-05,
        "epoch": 0.16473317865429235,
        "step": 1278
    },
    {
        "loss": 2.2373,
        "grad_norm": 1.9116851091384888,
        "learning_rate": 5.161251006655239e-05,
        "epoch": 0.1648620778551173,
        "step": 1279
    },
    {
        "loss": 2.1869,
        "grad_norm": 2.049186944961548,
        "learning_rate": 5.1551680555355805e-05,
        "epoch": 0.16499097705594226,
        "step": 1280
    },
    {
        "loss": 2.3716,
        "grad_norm": 1.8191744089126587,
        "learning_rate": 5.149084874522786e-05,
        "epoch": 0.16511987625676722,
        "step": 1281
    },
    {
        "loss": 1.9564,
        "grad_norm": 2.7475578784942627,
        "learning_rate": 5.1430014726295474e-05,
        "epoch": 0.16524877545759217,
        "step": 1282
    },
    {
        "loss": 2.1242,
        "grad_norm": 2.0710575580596924,
        "learning_rate": 5.13691785886888e-05,
        "epoch": 0.16537767465841713,
        "step": 1283
    },
    {
        "loss": 2.3254,
        "grad_norm": 1.3398314714431763,
        "learning_rate": 5.1308340422541214e-05,
        "epoch": 0.16550657385924208,
        "step": 1284
    },
    {
        "loss": 2.6078,
        "grad_norm": 1.5699071884155273,
        "learning_rate": 5.124750031798893e-05,
        "epoch": 0.16563547306006704,
        "step": 1285
    },
    {
        "loss": 2.5291,
        "grad_norm": 1.470262885093689,
        "learning_rate": 5.118665836517121e-05,
        "epoch": 0.165764372260892,
        "step": 1286
    },
    {
        "loss": 1.7073,
        "grad_norm": 2.6986289024353027,
        "learning_rate": 5.112581465422996e-05,
        "epoch": 0.16589327146171692,
        "step": 1287
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.4385334253311157,
        "learning_rate": 5.106496927530969e-05,
        "epoch": 0.16602217066254188,
        "step": 1288
    },
    {
        "loss": 1.6928,
        "grad_norm": 2.658811092376709,
        "learning_rate": 5.100412231855743e-05,
        "epoch": 0.16615106986336683,
        "step": 1289
    },
    {
        "loss": 1.9101,
        "grad_norm": 2.2473788261413574,
        "learning_rate": 5.094327387412249e-05,
        "epoch": 0.1662799690641918,
        "step": 1290
    },
    {
        "loss": 2.7025,
        "grad_norm": 1.6785329580307007,
        "learning_rate": 5.088242403215644e-05,
        "epoch": 0.16640886826501675,
        "step": 1291
    },
    {
        "loss": 2.169,
        "grad_norm": 2.104036331176758,
        "learning_rate": 5.082157288281287e-05,
        "epoch": 0.1665377674658417,
        "step": 1292
    },
    {
        "loss": 1.8681,
        "grad_norm": 2.870330572128296,
        "learning_rate": 5.076072051624734e-05,
        "epoch": 0.16666666666666666,
        "step": 1293
    },
    {
        "loss": 2.006,
        "grad_norm": 1.7219207286834717,
        "learning_rate": 5.06998670226172e-05,
        "epoch": 0.1667955658674916,
        "step": 1294
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.3279788494110107,
        "learning_rate": 5.063901249208148e-05,
        "epoch": 0.16692446506831657,
        "step": 1295
    },
    {
        "loss": 1.9293,
        "grad_norm": 2.2624993324279785,
        "learning_rate": 5.057815701480074e-05,
        "epoch": 0.16705336426914152,
        "step": 1296
    },
    {
        "loss": 2.6323,
        "grad_norm": 1.3596956729888916,
        "learning_rate": 5.051730068093694e-05,
        "epoch": 0.16718226346996648,
        "step": 1297
    },
    {
        "loss": 2.2003,
        "grad_norm": 2.0089399814605713,
        "learning_rate": 5.04564435806533e-05,
        "epoch": 0.16731116267079144,
        "step": 1298
    },
    {
        "loss": 1.2084,
        "grad_norm": 2.621861696243286,
        "learning_rate": 5.039558580411422e-05,
        "epoch": 0.1674400618716164,
        "step": 1299
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.176993727684021,
        "learning_rate": 5.033472744148502e-05,
        "epoch": 0.16756896107244135,
        "step": 1300
    },
    {
        "loss": 2.1962,
        "grad_norm": 1.8752800226211548,
        "learning_rate": 5.027386858293196e-05,
        "epoch": 0.1676978602732663,
        "step": 1301
    },
    {
        "loss": 2.4546,
        "grad_norm": 1.4004241228103638,
        "learning_rate": 5.021300931862203e-05,
        "epoch": 0.16782675947409126,
        "step": 1302
    },
    {
        "loss": 2.0859,
        "grad_norm": 1.6511040925979614,
        "learning_rate": 5.0152149738722795e-05,
        "epoch": 0.16795565867491621,
        "step": 1303
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.4547276496887207,
        "learning_rate": 5.009128993340228e-05,
        "epoch": 0.16808455787574117,
        "step": 1304
    },
    {
        "loss": 1.8079,
        "grad_norm": 2.968125343322754,
        "learning_rate": 5.003042999282887e-05,
        "epoch": 0.16821345707656613,
        "step": 1305
    },
    {
        "loss": 2.2581,
        "grad_norm": 2.9134304523468018,
        "learning_rate": 4.996957000717113e-05,
        "epoch": 0.16834235627739108,
        "step": 1306
    },
    {
        "loss": 2.3148,
        "grad_norm": 1.8151443004608154,
        "learning_rate": 4.9908710066597734e-05,
        "epoch": 0.16847125547821604,
        "step": 1307
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.5835920572280884,
        "learning_rate": 4.984785026127721e-05,
        "epoch": 0.168600154679041,
        "step": 1308
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.7951056957244873,
        "learning_rate": 4.978699068137798e-05,
        "epoch": 0.16872905387986595,
        "step": 1309
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.2204508781433105,
        "learning_rate": 4.972613141706805e-05,
        "epoch": 0.1688579530806909,
        "step": 1310
    },
    {
        "loss": 1.837,
        "grad_norm": 2.6943981647491455,
        "learning_rate": 4.9665272558515e-05,
        "epoch": 0.16898685228151586,
        "step": 1311
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.305516242980957,
        "learning_rate": 4.9604414195885816e-05,
        "epoch": 0.16911575148234081,
        "step": 1312
    },
    {
        "loss": 1.8644,
        "grad_norm": 1.873868703842163,
        "learning_rate": 4.954355641934672e-05,
        "epoch": 0.16924465068316577,
        "step": 1313
    },
    {
        "loss": 1.8307,
        "grad_norm": 2.0013554096221924,
        "learning_rate": 4.9482699319063064e-05,
        "epoch": 0.16937354988399073,
        "step": 1314
    },
    {
        "loss": 2.3393,
        "grad_norm": 1.9092375040054321,
        "learning_rate": 4.9421842985199284e-05,
        "epoch": 0.16950244908481568,
        "step": 1315
    },
    {
        "loss": 2.5196,
        "grad_norm": 1.4090337753295898,
        "learning_rate": 4.936098750791854e-05,
        "epoch": 0.16963134828564064,
        "step": 1316
    },
    {
        "loss": 2.1732,
        "grad_norm": 1.5845873355865479,
        "learning_rate": 4.93001329773828e-05,
        "epoch": 0.1697602474864656,
        "step": 1317
    },
    {
        "loss": 1.1214,
        "grad_norm": 2.078277826309204,
        "learning_rate": 4.923927948375267e-05,
        "epoch": 0.16988914668729055,
        "step": 1318
    },
    {
        "loss": 2.4478,
        "grad_norm": 2.3173530101776123,
        "learning_rate": 4.917842711718716e-05,
        "epoch": 0.1700180458881155,
        "step": 1319
    },
    {
        "loss": 2.1037,
        "grad_norm": 2.131695508956909,
        "learning_rate": 4.911757596784357e-05,
        "epoch": 0.17014694508894046,
        "step": 1320
    },
    {
        "loss": 1.9852,
        "grad_norm": 1.5074232816696167,
        "learning_rate": 4.9056726125877524e-05,
        "epoch": 0.17027584428976542,
        "step": 1321
    },
    {
        "loss": 2.5889,
        "grad_norm": 1.2650399208068848,
        "learning_rate": 4.899587768144258e-05,
        "epoch": 0.17040474349059037,
        "step": 1322
    },
    {
        "loss": 2.1667,
        "grad_norm": 1.5150312185287476,
        "learning_rate": 4.8935030724690314e-05,
        "epoch": 0.17053364269141533,
        "step": 1323
    },
    {
        "loss": 1.8824,
        "grad_norm": 2.701845407485962,
        "learning_rate": 4.887418534577004e-05,
        "epoch": 0.17066254189224025,
        "step": 1324
    },
    {
        "loss": 1.6269,
        "grad_norm": 1.9070042371749878,
        "learning_rate": 4.88133416348288e-05,
        "epoch": 0.1707914410930652,
        "step": 1325
    },
    {
        "loss": 1.737,
        "grad_norm": 2.8359014987945557,
        "learning_rate": 4.8752499682011056e-05,
        "epoch": 0.17092034029389017,
        "step": 1326
    },
    {
        "loss": 2.5051,
        "grad_norm": 1.876585602760315,
        "learning_rate": 4.869165957745881e-05,
        "epoch": 0.17104923949471512,
        "step": 1327
    },
    {
        "loss": 2.3157,
        "grad_norm": 1.7056230306625366,
        "learning_rate": 4.86308214113112e-05,
        "epoch": 0.17117813869554008,
        "step": 1328
    },
    {
        "loss": 1.699,
        "grad_norm": 3.1643283367156982,
        "learning_rate": 4.856998527370453e-05,
        "epoch": 0.17130703789636503,
        "step": 1329
    },
    {
        "loss": 1.6502,
        "grad_norm": 2.992842674255371,
        "learning_rate": 4.850915125477215e-05,
        "epoch": 0.17143593709719,
        "step": 1330
    },
    {
        "loss": 2.1386,
        "grad_norm": 1.9503093957901,
        "learning_rate": 4.844831944464423e-05,
        "epoch": 0.17156483629801494,
        "step": 1331
    },
    {
        "loss": 1.8381,
        "grad_norm": 1.8842418193817139,
        "learning_rate": 4.838748993344761e-05,
        "epoch": 0.1716937354988399,
        "step": 1332
    },
    {
        "loss": 2.0636,
        "grad_norm": 2.23246431350708,
        "learning_rate": 4.832666281130583e-05,
        "epoch": 0.17182263469966486,
        "step": 1333
    },
    {
        "loss": 2.2093,
        "grad_norm": 2.8989434242248535,
        "learning_rate": 4.8265838168338845e-05,
        "epoch": 0.1719515339004898,
        "step": 1334
    },
    {
        "loss": 2.4159,
        "grad_norm": 2.258610248565674,
        "learning_rate": 4.820501609466291e-05,
        "epoch": 0.17208043310131477,
        "step": 1335
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.3383586406707764,
        "learning_rate": 4.8144196680390526e-05,
        "epoch": 0.17220933230213972,
        "step": 1336
    },
    {
        "loss": 1.887,
        "grad_norm": 2.1879093647003174,
        "learning_rate": 4.808338001563019e-05,
        "epoch": 0.17233823150296468,
        "step": 1337
    },
    {
        "loss": 2.4162,
        "grad_norm": 1.043816328048706,
        "learning_rate": 4.802256619048639e-05,
        "epoch": 0.17246713070378963,
        "step": 1338
    },
    {
        "loss": 2.074,
        "grad_norm": 2.0739288330078125,
        "learning_rate": 4.796175529505937e-05,
        "epoch": 0.1725960299046146,
        "step": 1339
    },
    {
        "loss": 1.5445,
        "grad_norm": 3.0081231594085693,
        "learning_rate": 4.7900947419445026e-05,
        "epoch": 0.17272492910543955,
        "step": 1340
    },
    {
        "loss": 1.9573,
        "grad_norm": 2.4868125915527344,
        "learning_rate": 4.7840142653734816e-05,
        "epoch": 0.1728538283062645,
        "step": 1341
    },
    {
        "loss": 1.987,
        "grad_norm": 1.741091012954712,
        "learning_rate": 4.777934108801555e-05,
        "epoch": 0.17298272750708946,
        "step": 1342
    },
    {
        "loss": 2.3024,
        "grad_norm": 1.3537449836730957,
        "learning_rate": 4.771854281236934e-05,
        "epoch": 0.1731116267079144,
        "step": 1343
    },
    {
        "loss": 2.2228,
        "grad_norm": 1.8789851665496826,
        "learning_rate": 4.765774791687338e-05,
        "epoch": 0.17324052590873937,
        "step": 1344
    },
    {
        "loss": 1.9822,
        "grad_norm": 2.2225253582000732,
        "learning_rate": 4.759695649159987e-05,
        "epoch": 0.17336942510956432,
        "step": 1345
    },
    {
        "loss": 2.4408,
        "grad_norm": 1.7799865007400513,
        "learning_rate": 4.753616862661588e-05,
        "epoch": 0.17349832431038928,
        "step": 1346
    },
    {
        "loss": 2.1804,
        "grad_norm": 2.516932249069214,
        "learning_rate": 4.747538441198324e-05,
        "epoch": 0.17362722351121423,
        "step": 1347
    },
    {
        "loss": 2.0857,
        "grad_norm": 2.124555826187134,
        "learning_rate": 4.741460393775829e-05,
        "epoch": 0.1737561227120392,
        "step": 1348
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.6853148937225342,
        "learning_rate": 4.735382729399184e-05,
        "epoch": 0.17388502191286415,
        "step": 1349
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.107269287109375,
        "learning_rate": 4.729305457072913e-05,
        "epoch": 0.1740139211136891,
        "step": 1350
    },
    {
        "loss": 2.0138,
        "grad_norm": 2.1304736137390137,
        "learning_rate": 4.7232285858009475e-05,
        "epoch": 0.17414282031451406,
        "step": 1351
    },
    {
        "loss": 1.4847,
        "grad_norm": 2.229174852371216,
        "learning_rate": 4.717152124586627e-05,
        "epoch": 0.174271719515339,
        "step": 1352
    },
    {
        "loss": 2.0637,
        "grad_norm": 0.9585071802139282,
        "learning_rate": 4.711076082432689e-05,
        "epoch": 0.17440061871616397,
        "step": 1353
    },
    {
        "loss": 2.4359,
        "grad_norm": 1.617590308189392,
        "learning_rate": 4.705000468341245e-05,
        "epoch": 0.17452951791698892,
        "step": 1354
    },
    {
        "loss": 2.0966,
        "grad_norm": 1.9460318088531494,
        "learning_rate": 4.6989252913137746e-05,
        "epoch": 0.17465841711781388,
        "step": 1355
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.397172451019287,
        "learning_rate": 4.692850560351107e-05,
        "epoch": 0.17478731631863884,
        "step": 1356
    },
    {
        "loss": 1.7768,
        "grad_norm": 1.9176011085510254,
        "learning_rate": 4.686776284453416e-05,
        "epoch": 0.1749162155194638,
        "step": 1357
    },
    {
        "loss": 1.9377,
        "grad_norm": 2.34283709526062,
        "learning_rate": 4.680702472620193e-05,
        "epoch": 0.17504511472028875,
        "step": 1358
    },
    {
        "loss": 2.055,
        "grad_norm": 2.433828115463257,
        "learning_rate": 4.674629133850252e-05,
        "epoch": 0.1751740139211137,
        "step": 1359
    },
    {
        "loss": 2.5648,
        "grad_norm": 1.4069136381149292,
        "learning_rate": 4.6685562771416976e-05,
        "epoch": 0.17530291312193863,
        "step": 1360
    },
    {
        "loss": 2.4284,
        "grad_norm": 2.556851387023926,
        "learning_rate": 4.662483911491922e-05,
        "epoch": 0.1754318123227636,
        "step": 1361
    },
    {
        "loss": 2.6035,
        "grad_norm": 1.4603489637374878,
        "learning_rate": 4.6564120458975934e-05,
        "epoch": 0.17556071152358854,
        "step": 1362
    },
    {
        "loss": 1.5638,
        "grad_norm": 2.8243067264556885,
        "learning_rate": 4.650340689354639e-05,
        "epoch": 0.1756896107244135,
        "step": 1363
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.6198965311050415,
        "learning_rate": 4.644269850858224e-05,
        "epoch": 0.17581850992523845,
        "step": 1364
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.2718961238861084,
        "learning_rate": 4.6381995394027574e-05,
        "epoch": 0.1759474091260634,
        "step": 1365
    },
    {
        "loss": 2.0175,
        "grad_norm": 2.2055511474609375,
        "learning_rate": 4.632129763981859e-05,
        "epoch": 0.17607630832688836,
        "step": 1366
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.6358745098114014,
        "learning_rate": 4.626060533588358e-05,
        "epoch": 0.17620520752771332,
        "step": 1367
    },
    {
        "loss": 2.1001,
        "grad_norm": 2.3435614109039307,
        "learning_rate": 4.619991857214275e-05,
        "epoch": 0.17633410672853828,
        "step": 1368
    },
    {
        "loss": 1.9885,
        "grad_norm": 2.0444931983947754,
        "learning_rate": 4.613923743850809e-05,
        "epoch": 0.17646300592936323,
        "step": 1369
    },
    {
        "loss": 1.7551,
        "grad_norm": 2.0040509700775146,
        "learning_rate": 4.607856202488328e-05,
        "epoch": 0.1765919051301882,
        "step": 1370
    },
    {
        "loss": 2.481,
        "grad_norm": 1.3442058563232422,
        "learning_rate": 4.601789242116348e-05,
        "epoch": 0.17672080433101314,
        "step": 1371
    },
    {
        "loss": 1.933,
        "grad_norm": 2.3192896842956543,
        "learning_rate": 4.59572287172353e-05,
        "epoch": 0.1768497035318381,
        "step": 1372
    },
    {
        "loss": 1.9,
        "grad_norm": 2.0123789310455322,
        "learning_rate": 4.589657100297655e-05,
        "epoch": 0.17697860273266305,
        "step": 1373
    },
    {
        "loss": 2.2237,
        "grad_norm": 2.172950506210327,
        "learning_rate": 4.5835919368256184e-05,
        "epoch": 0.177107501933488,
        "step": 1374
    },
    {
        "loss": 2.1175,
        "grad_norm": 1.9271854162216187,
        "learning_rate": 4.577527390293419e-05,
        "epoch": 0.17723640113431297,
        "step": 1375
    },
    {
        "loss": 2.0267,
        "grad_norm": 1.643494725227356,
        "learning_rate": 4.571463469686134e-05,
        "epoch": 0.17736530033513792,
        "step": 1376
    },
    {
        "loss": 2.2861,
        "grad_norm": 2.4085755348205566,
        "learning_rate": 4.5654001839879215e-05,
        "epoch": 0.17749419953596288,
        "step": 1377
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.9024062156677246,
        "learning_rate": 4.559337542181991e-05,
        "epoch": 0.17762309873678783,
        "step": 1378
    },
    {
        "loss": 1.8979,
        "grad_norm": 2.6646690368652344,
        "learning_rate": 4.553275553250607e-05,
        "epoch": 0.1777519979376128,
        "step": 1379
    },
    {
        "loss": 2.4166,
        "grad_norm": 2.562190294265747,
        "learning_rate": 4.5472142261750565e-05,
        "epoch": 0.17788089713843774,
        "step": 1380
    },
    {
        "loss": 1.9429,
        "grad_norm": 2.4456326961517334,
        "learning_rate": 4.541153569935651e-05,
        "epoch": 0.1780097963392627,
        "step": 1381
    },
    {
        "loss": 1.6492,
        "grad_norm": 2.524932384490967,
        "learning_rate": 4.5350935935117146e-05,
        "epoch": 0.17813869554008765,
        "step": 1382
    },
    {
        "loss": 2.0305,
        "grad_norm": 1.6045671701431274,
        "learning_rate": 4.529034305881551e-05,
        "epoch": 0.1782675947409126,
        "step": 1383
    },
    {
        "loss": 2.2059,
        "grad_norm": 1.6309176683425903,
        "learning_rate": 4.522975716022451e-05,
        "epoch": 0.17839649394173757,
        "step": 1384
    },
    {
        "loss": 1.6671,
        "grad_norm": 1.5061242580413818,
        "learning_rate": 4.5169178329106724e-05,
        "epoch": 0.17852539314256252,
        "step": 1385
    },
    {
        "loss": 2.3313,
        "grad_norm": 1.341080665588379,
        "learning_rate": 4.510860665521427e-05,
        "epoch": 0.17865429234338748,
        "step": 1386
    },
    {
        "loss": 1.9073,
        "grad_norm": 2.798940420150757,
        "learning_rate": 4.504804222828857e-05,
        "epoch": 0.17878319154421243,
        "step": 1387
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.424848198890686,
        "learning_rate": 4.498748513806042e-05,
        "epoch": 0.1789120907450374,
        "step": 1388
    },
    {
        "loss": 2.3308,
        "grad_norm": 1.6797025203704834,
        "learning_rate": 4.4926935474249696e-05,
        "epoch": 0.17904098994586234,
        "step": 1389
    },
    {
        "loss": 2.4881,
        "grad_norm": 2.1770598888397217,
        "learning_rate": 4.4866393326565254e-05,
        "epoch": 0.1791698891466873,
        "step": 1390
    },
    {
        "loss": 2.2761,
        "grad_norm": 2.0123941898345947,
        "learning_rate": 4.480585878470486e-05,
        "epoch": 0.17929878834751226,
        "step": 1391
    },
    {
        "loss": 2.648,
        "grad_norm": 1.7385507822036743,
        "learning_rate": 4.4745331938354956e-05,
        "epoch": 0.1794276875483372,
        "step": 1392
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.075378894805908,
        "learning_rate": 4.4684812877190616e-05,
        "epoch": 0.17955658674916217,
        "step": 1393
    },
    {
        "loss": 1.7198,
        "grad_norm": 2.1158859729766846,
        "learning_rate": 4.462430169087544e-05,
        "epoch": 0.17968548594998712,
        "step": 1394
    },
    {
        "loss": 2.0921,
        "grad_norm": 2.314800500869751,
        "learning_rate": 4.456379846906122e-05,
        "epoch": 0.17981438515081208,
        "step": 1395
    },
    {
        "loss": 2.0735,
        "grad_norm": 2.0302979946136475,
        "learning_rate": 4.450330330138804e-05,
        "epoch": 0.179943284351637,
        "step": 1396
    },
    {
        "loss": 2.037,
        "grad_norm": 1.4908684492111206,
        "learning_rate": 4.444281627748407e-05,
        "epoch": 0.18007218355246196,
        "step": 1397
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.855137825012207,
        "learning_rate": 4.438233748696537e-05,
        "epoch": 0.18020108275328692,
        "step": 1398
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.387183904647827,
        "learning_rate": 4.432186701943577e-05,
        "epoch": 0.18032998195411187,
        "step": 1399
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.4196484088897705,
        "learning_rate": 4.426140496448686e-05,
        "epoch": 0.18045888115493683,
        "step": 1400
    },
    {
        "loss": 2.3368,
        "grad_norm": 2.005783796310425,
        "learning_rate": 4.420095141169771e-05,
        "epoch": 0.18058778035576178,
        "step": 1401
    },
    {
        "loss": 1.4495,
        "grad_norm": 2.8400216102600098,
        "learning_rate": 4.41405064506348e-05,
        "epoch": 0.18071667955658674,
        "step": 1402
    },
    {
        "loss": 2.4783,
        "grad_norm": 1.3496289253234863,
        "learning_rate": 4.408007017085186e-05,
        "epoch": 0.1808455787574117,
        "step": 1403
    },
    {
        "loss": 2.5135,
        "grad_norm": 2.7427127361297607,
        "learning_rate": 4.401964266188981e-05,
        "epoch": 0.18097447795823665,
        "step": 1404
    },
    {
        "loss": 2.1664,
        "grad_norm": 2.171875238418579,
        "learning_rate": 4.395922401327651e-05,
        "epoch": 0.1811033771590616,
        "step": 1405
    },
    {
        "loss": 2.0896,
        "grad_norm": 1.621886968612671,
        "learning_rate": 4.389881431452677e-05,
        "epoch": 0.18123227635988656,
        "step": 1406
    },
    {
        "loss": 2.0601,
        "grad_norm": 1.4620707035064697,
        "learning_rate": 4.383841365514208e-05,
        "epoch": 0.18136117556071152,
        "step": 1407
    },
    {
        "loss": 1.8038,
        "grad_norm": 3.013660192489624,
        "learning_rate": 4.377802212461053e-05,
        "epoch": 0.18149007476153647,
        "step": 1408
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.683229684829712,
        "learning_rate": 4.3717639812406745e-05,
        "epoch": 0.18161897396236143,
        "step": 1409
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.6079189777374268,
        "learning_rate": 4.365726680799164e-05,
        "epoch": 0.18174787316318639,
        "step": 1410
    },
    {
        "loss": 2.2691,
        "grad_norm": 1.7473551034927368,
        "learning_rate": 4.359690320081237e-05,
        "epoch": 0.18187677236401134,
        "step": 1411
    },
    {
        "loss": 2.1629,
        "grad_norm": 1.9368770122528076,
        "learning_rate": 4.3536549080302145e-05,
        "epoch": 0.1820056715648363,
        "step": 1412
    },
    {
        "loss": 2.3696,
        "grad_norm": 1.5296673774719238,
        "learning_rate": 4.347620453588012e-05,
        "epoch": 0.18213457076566125,
        "step": 1413
    },
    {
        "loss": 1.9668,
        "grad_norm": 2.292337656021118,
        "learning_rate": 4.341586965695131e-05,
        "epoch": 0.1822634699664862,
        "step": 1414
    },
    {
        "loss": 1.7137,
        "grad_norm": 2.316807746887207,
        "learning_rate": 4.335554453290632e-05,
        "epoch": 0.18239236916731116,
        "step": 1415
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.3751860857009888,
        "learning_rate": 4.329522925312138e-05,
        "epoch": 0.18252126836813612,
        "step": 1416
    },
    {
        "loss": 2.0055,
        "grad_norm": 2.5787811279296875,
        "learning_rate": 4.323492390695813e-05,
        "epoch": 0.18265016756896107,
        "step": 1417
    },
    {
        "loss": 2.2333,
        "grad_norm": 2.2086942195892334,
        "learning_rate": 4.3174628583763454e-05,
        "epoch": 0.18277906676978603,
        "step": 1418
    },
    {
        "loss": 2.2305,
        "grad_norm": 1.7306114435195923,
        "learning_rate": 4.311434337286937e-05,
        "epoch": 0.182907965970611,
        "step": 1419
    },
    {
        "loss": 2.2312,
        "grad_norm": 2.4344675540924072,
        "learning_rate": 4.3054068363593e-05,
        "epoch": 0.18303686517143594,
        "step": 1420
    },
    {
        "loss": 1.6128,
        "grad_norm": 1.26685631275177,
        "learning_rate": 4.2993803645236275e-05,
        "epoch": 0.1831657643722609,
        "step": 1421
    },
    {
        "loss": 2.0219,
        "grad_norm": 1.3930120468139648,
        "learning_rate": 4.293354930708591e-05,
        "epoch": 0.18329466357308585,
        "step": 1422
    },
    {
        "loss": 2.0423,
        "grad_norm": 2.4789459705352783,
        "learning_rate": 4.287330543841324e-05,
        "epoch": 0.1834235627739108,
        "step": 1423
    },
    {
        "loss": 2.1304,
        "grad_norm": 2.396911382675171,
        "learning_rate": 4.2813072128474056e-05,
        "epoch": 0.18355246197473576,
        "step": 1424
    },
    {
        "loss": 2.3944,
        "grad_norm": 1.4541394710540771,
        "learning_rate": 4.275284946650854e-05,
        "epoch": 0.18368136117556072,
        "step": 1425
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.7594107389450073,
        "learning_rate": 4.2692637541741144e-05,
        "epoch": 0.18381026037638568,
        "step": 1426
    },
    {
        "loss": 1.5381,
        "grad_norm": 2.0358872413635254,
        "learning_rate": 4.2632436443380305e-05,
        "epoch": 0.18393915957721063,
        "step": 1427
    },
    {
        "loss": 2.4384,
        "grad_norm": 1.3184927701950073,
        "learning_rate": 4.2572246260618456e-05,
        "epoch": 0.1840680587780356,
        "step": 1428
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.1023173332214355,
        "learning_rate": 4.251206708263193e-05,
        "epoch": 0.18419695797886054,
        "step": 1429
    },
    {
        "loss": 2.0529,
        "grad_norm": 1.4142522811889648,
        "learning_rate": 4.2451898998580694e-05,
        "epoch": 0.1843258571796855,
        "step": 1430
    },
    {
        "loss": 2.0935,
        "grad_norm": 2.011305809020996,
        "learning_rate": 4.239174209760823e-05,
        "epoch": 0.18445475638051045,
        "step": 1431
    },
    {
        "loss": 1.6307,
        "grad_norm": 2.4750254154205322,
        "learning_rate": 4.233159646884157e-05,
        "epoch": 0.18458365558133538,
        "step": 1432
    },
    {
        "loss": 2.1112,
        "grad_norm": 2.125962734222412,
        "learning_rate": 4.227146220139095e-05,
        "epoch": 0.18471255478216034,
        "step": 1433
    },
    {
        "loss": 1.2488,
        "grad_norm": 2.659670829772949,
        "learning_rate": 4.221133938434982e-05,
        "epoch": 0.1848414539829853,
        "step": 1434
    },
    {
        "loss": 2.0207,
        "grad_norm": 2.3997201919555664,
        "learning_rate": 4.2151228106794646e-05,
        "epoch": 0.18497035318381025,
        "step": 1435
    },
    {
        "loss": 2.0118,
        "grad_norm": 1.534890055656433,
        "learning_rate": 4.209112845778481e-05,
        "epoch": 0.1850992523846352,
        "step": 1436
    },
    {
        "loss": 2.4155,
        "grad_norm": 1.747869610786438,
        "learning_rate": 4.203104052636243e-05,
        "epoch": 0.18522815158546016,
        "step": 1437
    },
    {
        "loss": 1.9527,
        "grad_norm": 2.475761890411377,
        "learning_rate": 4.197096440155232e-05,
        "epoch": 0.18535705078628512,
        "step": 1438
    },
    {
        "loss": 2.113,
        "grad_norm": 2.0078303813934326,
        "learning_rate": 4.1910900172361764e-05,
        "epoch": 0.18548594998711007,
        "step": 1439
    },
    {
        "loss": 2.4413,
        "grad_norm": 1.4238313436508179,
        "learning_rate": 4.185084792778044e-05,
        "epoch": 0.18561484918793503,
        "step": 1440
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.9876631498336792,
        "learning_rate": 4.179080775678025e-05,
        "epoch": 0.18574374838875998,
        "step": 1441
    },
    {
        "loss": 2.5594,
        "grad_norm": 1.6710056066513062,
        "learning_rate": 4.173077974831521e-05,
        "epoch": 0.18587264758958494,
        "step": 1442
    },
    {
        "loss": 1.5707,
        "grad_norm": 2.9909441471099854,
        "learning_rate": 4.167076399132135e-05,
        "epoch": 0.1860015467904099,
        "step": 1443
    },
    {
        "loss": 1.9708,
        "grad_norm": 1.6230512857437134,
        "learning_rate": 4.16107605747165e-05,
        "epoch": 0.18613044599123485,
        "step": 1444
    },
    {
        "loss": 2.4376,
        "grad_norm": 1.6402333974838257,
        "learning_rate": 4.1550769587400216e-05,
        "epoch": 0.1862593451920598,
        "step": 1445
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.269559860229492,
        "learning_rate": 4.1490791118253736e-05,
        "epoch": 0.18638824439288476,
        "step": 1446
    },
    {
        "loss": 1.8931,
        "grad_norm": 1.9253789186477661,
        "learning_rate": 4.143082525613957e-05,
        "epoch": 0.18651714359370972,
        "step": 1447
    },
    {
        "loss": 1.6035,
        "grad_norm": 3.0218145847320557,
        "learning_rate": 4.137087208990168e-05,
        "epoch": 0.18664604279453467,
        "step": 1448
    },
    {
        "loss": 1.8584,
        "grad_norm": 1.4265573024749756,
        "learning_rate": 4.131093170836522e-05,
        "epoch": 0.18677494199535963,
        "step": 1449
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.3974745273590088,
        "learning_rate": 4.1251004200336356e-05,
        "epoch": 0.18690384119618458,
        "step": 1450
    },
    {
        "loss": 2.3484,
        "grad_norm": 2.193394422531128,
        "learning_rate": 4.1191089654602155e-05,
        "epoch": 0.18703274039700954,
        "step": 1451
    },
    {
        "loss": 2.5884,
        "grad_norm": 2.213158130645752,
        "learning_rate": 4.1131188159930565e-05,
        "epoch": 0.1871616395978345,
        "step": 1452
    },
    {
        "loss": 1.9659,
        "grad_norm": 2.370166540145874,
        "learning_rate": 4.107129980507012e-05,
        "epoch": 0.18729053879865945,
        "step": 1453
    },
    {
        "loss": 1.892,
        "grad_norm": 2.804137706756592,
        "learning_rate": 4.101142467874994e-05,
        "epoch": 0.1874194379994844,
        "step": 1454
    },
    {
        "loss": 2.1255,
        "grad_norm": 1.800714373588562,
        "learning_rate": 4.095156286967952e-05,
        "epoch": 0.18754833720030936,
        "step": 1455
    },
    {
        "loss": 2.0802,
        "grad_norm": 2.7247700691223145,
        "learning_rate": 4.089171446654862e-05,
        "epoch": 0.18767723640113432,
        "step": 1456
    },
    {
        "loss": 1.7129,
        "grad_norm": 1.2420529127120972,
        "learning_rate": 4.083187955802713e-05,
        "epoch": 0.18780613560195927,
        "step": 1457
    },
    {
        "loss": 2.2041,
        "grad_norm": 2.198108196258545,
        "learning_rate": 4.077205823276502e-05,
        "epoch": 0.18793503480278423,
        "step": 1458
    },
    {
        "loss": 2.6237,
        "grad_norm": 1.392401099205017,
        "learning_rate": 4.071225057939203e-05,
        "epoch": 0.18806393400360918,
        "step": 1459
    },
    {
        "loss": 2.066,
        "grad_norm": 1.3375457525253296,
        "learning_rate": 4.065245668651768e-05,
        "epoch": 0.18819283320443414,
        "step": 1460
    },
    {
        "loss": 2.4422,
        "grad_norm": 1.1675363779067993,
        "learning_rate": 4.059267664273115e-05,
        "epoch": 0.1883217324052591,
        "step": 1461
    },
    {
        "loss": 2.4593,
        "grad_norm": 1.7452526092529297,
        "learning_rate": 4.053291053660108e-05,
        "epoch": 0.18845063160608405,
        "step": 1462
    },
    {
        "loss": 1.5493,
        "grad_norm": 2.475882053375244,
        "learning_rate": 4.047315845667538e-05,
        "epoch": 0.188579530806909,
        "step": 1463
    },
    {
        "loss": 1.7047,
        "grad_norm": 2.6507232189178467,
        "learning_rate": 4.041342049148132e-05,
        "epoch": 0.18870843000773396,
        "step": 1464
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.6273075342178345,
        "learning_rate": 4.035369672952516e-05,
        "epoch": 0.18883732920855892,
        "step": 1465
    },
    {
        "loss": 2.1073,
        "grad_norm": 1.8183192014694214,
        "learning_rate": 4.029398725929212e-05,
        "epoch": 0.18896622840938387,
        "step": 1466
    },
    {
        "loss": 2.4948,
        "grad_norm": 1.5283926725387573,
        "learning_rate": 4.023429216924629e-05,
        "epoch": 0.18909512761020883,
        "step": 1467
    },
    {
        "loss": 2.5814,
        "grad_norm": 1.7068660259246826,
        "learning_rate": 4.0174611547830414e-05,
        "epoch": 0.18922402681103379,
        "step": 1468
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.6229283809661865,
        "learning_rate": 4.0114945483465834e-05,
        "epoch": 0.1893529260118587,
        "step": 1469
    },
    {
        "loss": 2.314,
        "grad_norm": 2.150921106338501,
        "learning_rate": 4.0055294064552306e-05,
        "epoch": 0.18948182521268367,
        "step": 1470
    },
    {
        "loss": 1.6426,
        "grad_norm": 2.6779327392578125,
        "learning_rate": 3.999565737946786e-05,
        "epoch": 0.18961072441350862,
        "step": 1471
    },
    {
        "loss": 2.8129,
        "grad_norm": 1.235876202583313,
        "learning_rate": 3.9936035516568777e-05,
        "epoch": 0.18973962361433358,
        "step": 1472
    },
    {
        "loss": 1.9091,
        "grad_norm": 1.7934058904647827,
        "learning_rate": 3.9876428564189286e-05,
        "epoch": 0.18986852281515854,
        "step": 1473
    },
    {
        "loss": 2.5936,
        "grad_norm": 1.8892923593521118,
        "learning_rate": 3.981683661064158e-05,
        "epoch": 0.1899974220159835,
        "step": 1474
    },
    {
        "loss": 2.3966,
        "grad_norm": 1.619036316871643,
        "learning_rate": 3.9757259744215623e-05,
        "epoch": 0.19012632121680845,
        "step": 1475
    },
    {
        "loss": 2.1738,
        "grad_norm": 1.3884309530258179,
        "learning_rate": 3.9697698053179e-05,
        "epoch": 0.1902552204176334,
        "step": 1476
    },
    {
        "loss": 1.8464,
        "grad_norm": 1.853788137435913,
        "learning_rate": 3.963815162577685e-05,
        "epoch": 0.19038411961845836,
        "step": 1477
    },
    {
        "loss": 2.1529,
        "grad_norm": 1.9272897243499756,
        "learning_rate": 3.957862055023167e-05,
        "epoch": 0.19051301881928331,
        "step": 1478
    },
    {
        "loss": 2.262,
        "grad_norm": 1.3987643718719482,
        "learning_rate": 3.951910491474323e-05,
        "epoch": 0.19064191802010827,
        "step": 1479
    },
    {
        "loss": 2.0721,
        "grad_norm": 2.2341599464416504,
        "learning_rate": 3.9459604807488396e-05,
        "epoch": 0.19077081722093323,
        "step": 1480
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.9052993059158325,
        "learning_rate": 3.940012031662109e-05,
        "epoch": 0.19089971642175818,
        "step": 1481
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.2404950857162476,
        "learning_rate": 3.9340651530272014e-05,
        "epoch": 0.19102861562258314,
        "step": 1482
    },
    {
        "loss": 2.0152,
        "grad_norm": 2.2616941928863525,
        "learning_rate": 3.9281198536548643e-05,
        "epoch": 0.1911575148234081,
        "step": 1483
    },
    {
        "loss": 2.0385,
        "grad_norm": 2.190467357635498,
        "learning_rate": 3.922176142353507e-05,
        "epoch": 0.19128641402423305,
        "step": 1484
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.2221219539642334,
        "learning_rate": 3.9162340279291874e-05,
        "epoch": 0.191415313225058,
        "step": 1485
    },
    {
        "loss": 2.3342,
        "grad_norm": 1.3708020448684692,
        "learning_rate": 3.9102935191855894e-05,
        "epoch": 0.19154421242588296,
        "step": 1486
    },
    {
        "loss": 1.952,
        "grad_norm": 1.5040854215621948,
        "learning_rate": 3.904354624924025e-05,
        "epoch": 0.19167311162670791,
        "step": 1487
    },
    {
        "loss": 2.7651,
        "grad_norm": 1.0329639911651611,
        "learning_rate": 3.898417353943413e-05,
        "epoch": 0.19180201082753287,
        "step": 1488
    },
    {
        "loss": 1.7247,
        "grad_norm": 2.356133222579956,
        "learning_rate": 3.892481715040266e-05,
        "epoch": 0.19193091002835783,
        "step": 1489
    },
    {
        "loss": 1.7697,
        "grad_norm": 2.790318012237549,
        "learning_rate": 3.8865477170086784e-05,
        "epoch": 0.19205980922918278,
        "step": 1490
    },
    {
        "loss": 2.4342,
        "grad_norm": 2.3316285610198975,
        "learning_rate": 3.880615368640314e-05,
        "epoch": 0.19218870843000774,
        "step": 1491
    },
    {
        "loss": 1.9251,
        "grad_norm": 1.6670106649398804,
        "learning_rate": 3.874684678724393e-05,
        "epoch": 0.1923176076308327,
        "step": 1492
    },
    {
        "loss": 2.0129,
        "grad_norm": 1.4391816854476929,
        "learning_rate": 3.8687556560476834e-05,
        "epoch": 0.19244650683165765,
        "step": 1493
    },
    {
        "loss": 2.4623,
        "grad_norm": 1.4933842420578003,
        "learning_rate": 3.8628283093944686e-05,
        "epoch": 0.1925754060324826,
        "step": 1494
    },
    {
        "loss": 2.5869,
        "grad_norm": 1.2454195022583008,
        "learning_rate": 3.856902647546562e-05,
        "epoch": 0.19270430523330756,
        "step": 1495
    },
    {
        "loss": 2.0543,
        "grad_norm": 2.7415945529937744,
        "learning_rate": 3.8509786792832797e-05,
        "epoch": 0.19283320443413252,
        "step": 1496
    },
    {
        "loss": 1.1093,
        "grad_norm": 3.1321890354156494,
        "learning_rate": 3.845056413381424e-05,
        "epoch": 0.19296210363495747,
        "step": 1497
    },
    {
        "loss": 2.1889,
        "grad_norm": 2.740581750869751,
        "learning_rate": 3.8391358586152734e-05,
        "epoch": 0.19309100283578243,
        "step": 1498
    },
    {
        "loss": 2.3279,
        "grad_norm": 2.095684051513672,
        "learning_rate": 3.83321702375658e-05,
        "epoch": 0.19321990203660738,
        "step": 1499
    },
    {
        "loss": 1.5846,
        "grad_norm": 2.4555299282073975,
        "learning_rate": 3.8272999175745386e-05,
        "epoch": 0.19334880123743234,
        "step": 1500
    },
    {
        "loss": 2.2525,
        "grad_norm": 1.2871413230895996,
        "learning_rate": 3.821384548835788e-05,
        "epoch": 0.1934777004382573,
        "step": 1501
    },
    {
        "loss": 2.0502,
        "grad_norm": 2.1461358070373535,
        "learning_rate": 3.815470926304392e-05,
        "epoch": 0.19360659963908225,
        "step": 1502
    },
    {
        "loss": 1.8138,
        "grad_norm": 2.136240243911743,
        "learning_rate": 3.8095590587418265e-05,
        "epoch": 0.1937354988399072,
        "step": 1503
    },
    {
        "loss": 2.0485,
        "grad_norm": 1.9918934106826782,
        "learning_rate": 3.803648954906968e-05,
        "epoch": 0.19386439804073216,
        "step": 1504
    },
    {
        "loss": 1.7665,
        "grad_norm": 2.5598134994506836,
        "learning_rate": 3.797740623556077e-05,
        "epoch": 0.1939932972415571,
        "step": 1505
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.485069990158081,
        "learning_rate": 3.791834073442793e-05,
        "epoch": 0.19412219644238204,
        "step": 1506
    },
    {
        "loss": 2.0192,
        "grad_norm": 1.6492085456848145,
        "learning_rate": 3.785929313318112e-05,
        "epoch": 0.194251095643207,
        "step": 1507
    },
    {
        "loss": 2.168,
        "grad_norm": 1.1346131563186646,
        "learning_rate": 3.7800263519303815e-05,
        "epoch": 0.19437999484403196,
        "step": 1508
    },
    {
        "loss": 1.8242,
        "grad_norm": 2.19781231880188,
        "learning_rate": 3.7741251980252815e-05,
        "epoch": 0.1945088940448569,
        "step": 1509
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.0368638038635254,
        "learning_rate": 3.7682258603458135e-05,
        "epoch": 0.19463779324568187,
        "step": 1510
    },
    {
        "loss": 2.2969,
        "grad_norm": 2.2799131870269775,
        "learning_rate": 3.762328347632292e-05,
        "epoch": 0.19476669244650682,
        "step": 1511
    },
    {
        "loss": 2.3796,
        "grad_norm": 1.3205033540725708,
        "learning_rate": 3.7564326686223234e-05,
        "epoch": 0.19489559164733178,
        "step": 1512
    },
    {
        "loss": 2.2287,
        "grad_norm": 2.0744433403015137,
        "learning_rate": 3.750538832050802e-05,
        "epoch": 0.19502449084815673,
        "step": 1513
    },
    {
        "loss": 1.9389,
        "grad_norm": 2.7042572498321533,
        "learning_rate": 3.7446468466498876e-05,
        "epoch": 0.1951533900489817,
        "step": 1514
    },
    {
        "loss": 2.0941,
        "grad_norm": 2.370051860809326,
        "learning_rate": 3.738756721148996e-05,
        "epoch": 0.19528228924980665,
        "step": 1515
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.9668214321136475,
        "learning_rate": 3.732868464274796e-05,
        "epoch": 0.1954111884506316,
        "step": 1516
    },
    {
        "loss": 1.7738,
        "grad_norm": 2.3744306564331055,
        "learning_rate": 3.7269820847511814e-05,
        "epoch": 0.19554008765145656,
        "step": 1517
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.415462613105774,
        "learning_rate": 3.721097591299263e-05,
        "epoch": 0.1956689868522815,
        "step": 1518
    },
    {
        "loss": 2.0933,
        "grad_norm": 1.9855642318725586,
        "learning_rate": 3.7152149926373636e-05,
        "epoch": 0.19579788605310647,
        "step": 1519
    },
    {
        "loss": 1.5605,
        "grad_norm": 2.106081008911133,
        "learning_rate": 3.709334297480993e-05,
        "epoch": 0.19592678525393142,
        "step": 1520
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.135568380355835,
        "learning_rate": 3.703455514542844e-05,
        "epoch": 0.19605568445475638,
        "step": 1521
    },
    {
        "loss": 1.9993,
        "grad_norm": 1.8484352827072144,
        "learning_rate": 3.697578652532775e-05,
        "epoch": 0.19618458365558133,
        "step": 1522
    },
    {
        "loss": 1.6367,
        "grad_norm": 2.28631591796875,
        "learning_rate": 3.691703720157798e-05,
        "epoch": 0.1963134828564063,
        "step": 1523
    },
    {
        "loss": 1.2323,
        "grad_norm": 2.807298183441162,
        "learning_rate": 3.685830726122065e-05,
        "epoch": 0.19644238205723125,
        "step": 1524
    },
    {
        "loss": 2.3356,
        "grad_norm": 1.8901121616363525,
        "learning_rate": 3.6799596791268636e-05,
        "epoch": 0.1965712812580562,
        "step": 1525
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.3817585706710815,
        "learning_rate": 3.674090587870587e-05,
        "epoch": 0.19670018045888116,
        "step": 1526
    },
    {
        "loss": 1.6889,
        "grad_norm": 2.8822476863861084,
        "learning_rate": 3.6682234610487334e-05,
        "epoch": 0.1968290796597061,
        "step": 1527
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.411841869354248,
        "learning_rate": 3.662358307353896e-05,
        "epoch": 0.19695797886053107,
        "step": 1528
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.6920089721679688,
        "learning_rate": 3.65649513547574e-05,
        "epoch": 0.19708687806135602,
        "step": 1529
    },
    {
        "loss": 2.1092,
        "grad_norm": 1.8344179391860962,
        "learning_rate": 3.650633954100991e-05,
        "epoch": 0.19721577726218098,
        "step": 1530
    },
    {
        "loss": 1.871,
        "grad_norm": 1.7445151805877686,
        "learning_rate": 3.644774771913434e-05,
        "epoch": 0.19734467646300594,
        "step": 1531
    },
    {
        "loss": 2.4005,
        "grad_norm": 2.569410562515259,
        "learning_rate": 3.638917597593887e-05,
        "epoch": 0.1974735756638309,
        "step": 1532
    },
    {
        "loss": 2.4734,
        "grad_norm": 1.3057373762130737,
        "learning_rate": 3.633062439820193e-05,
        "epoch": 0.19760247486465585,
        "step": 1533
    },
    {
        "loss": 2.3689,
        "grad_norm": 1.8474160432815552,
        "learning_rate": 3.627209307267208e-05,
        "epoch": 0.1977313740654808,
        "step": 1534
    },
    {
        "loss": 2.2871,
        "grad_norm": 2.0896239280700684,
        "learning_rate": 3.6213582086067884e-05,
        "epoch": 0.19786027326630576,
        "step": 1535
    },
    {
        "loss": 1.2784,
        "grad_norm": 2.98724627494812,
        "learning_rate": 3.615509152507777e-05,
        "epoch": 0.19798917246713071,
        "step": 1536
    },
    {
        "loss": 2.4884,
        "grad_norm": 1.436591625213623,
        "learning_rate": 3.6096621476359884e-05,
        "epoch": 0.19811807166795567,
        "step": 1537
    },
    {
        "loss": 2.1959,
        "grad_norm": 2.461150884628296,
        "learning_rate": 3.603817202654201e-05,
        "epoch": 0.19824697086878063,
        "step": 1538
    },
    {
        "loss": 1.9141,
        "grad_norm": 1.7295891046524048,
        "learning_rate": 3.597974326222138e-05,
        "epoch": 0.19837587006960558,
        "step": 1539
    },
    {
        "loss": 2.0512,
        "grad_norm": 1.6280759572982788,
        "learning_rate": 3.592133526996463e-05,
        "epoch": 0.19850476927043054,
        "step": 1540
    },
    {
        "loss": 1.9713,
        "grad_norm": 2.343996047973633,
        "learning_rate": 3.5862948136307554e-05,
        "epoch": 0.19863366847125546,
        "step": 1541
    },
    {
        "loss": 1.9955,
        "grad_norm": 1.2741026878356934,
        "learning_rate": 3.580458194775511e-05,
        "epoch": 0.19876256767208042,
        "step": 1542
    },
    {
        "loss": 1.0698,
        "grad_norm": 2.3920938968658447,
        "learning_rate": 3.574623679078118e-05,
        "epoch": 0.19889146687290538,
        "step": 1543
    },
    {
        "loss": 2.0247,
        "grad_norm": 1.963058590888977,
        "learning_rate": 3.568791275182848e-05,
        "epoch": 0.19902036607373033,
        "step": 1544
    },
    {
        "loss": 2.3262,
        "grad_norm": 1.5756174325942993,
        "learning_rate": 3.56296099173085e-05,
        "epoch": 0.1991492652745553,
        "step": 1545
    },
    {
        "loss": 1.8688,
        "grad_norm": 2.140882730484009,
        "learning_rate": 3.557132837360122e-05,
        "epoch": 0.19927816447538024,
        "step": 1546
    },
    {
        "loss": 2.3408,
        "grad_norm": 1.5088112354278564,
        "learning_rate": 3.551306820705514e-05,
        "epoch": 0.1994070636762052,
        "step": 1547
    },
    {
        "loss": 2.316,
        "grad_norm": 1.3014849424362183,
        "learning_rate": 3.545482950398711e-05,
        "epoch": 0.19953596287703015,
        "step": 1548
    },
    {
        "loss": 2.689,
        "grad_norm": 2.391871452331543,
        "learning_rate": 3.539661235068212e-05,
        "epoch": 0.1996648620778551,
        "step": 1549
    },
    {
        "loss": 2.6023,
        "grad_norm": 1.4345147609710693,
        "learning_rate": 3.533841683339323e-05,
        "epoch": 0.19979376127868007,
        "step": 1550
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.4726812839508057,
        "learning_rate": 3.5280243038341515e-05,
        "epoch": 0.19992266047950502,
        "step": 1551
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.7030445337295532,
        "learning_rate": 3.52220910517158e-05,
        "epoch": 0.20005155968032998,
        "step": 1552
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.9992411136627197,
        "learning_rate": 3.516396095967264e-05,
        "epoch": 0.20018045888115493,
        "step": 1553
    },
    {
        "loss": 1.6586,
        "grad_norm": 1.7546623945236206,
        "learning_rate": 3.5105852848336105e-05,
        "epoch": 0.2003093580819799,
        "step": 1554
    },
    {
        "loss": 1.8135,
        "grad_norm": 2.1466543674468994,
        "learning_rate": 3.5047766803797764e-05,
        "epoch": 0.20043825728280484,
        "step": 1555
    },
    {
        "loss": 1.8339,
        "grad_norm": 1.9812971353530884,
        "learning_rate": 3.498970291211642e-05,
        "epoch": 0.2005671564836298,
        "step": 1556
    },
    {
        "loss": 1.8869,
        "grad_norm": 2.1377885341644287,
        "learning_rate": 3.493166125931814e-05,
        "epoch": 0.20069605568445475,
        "step": 1557
    },
    {
        "loss": 2.6023,
        "grad_norm": 1.4126380681991577,
        "learning_rate": 3.4873641931395955e-05,
        "epoch": 0.2008249548852797,
        "step": 1558
    },
    {
        "loss": 2.3527,
        "grad_norm": 2.687305212020874,
        "learning_rate": 3.481564501430985e-05,
        "epoch": 0.20095385408610467,
        "step": 1559
    },
    {
        "loss": 2.4026,
        "grad_norm": 2.619513511657715,
        "learning_rate": 3.4757670593986644e-05,
        "epoch": 0.20108275328692962,
        "step": 1560
    },
    {
        "loss": 1.9396,
        "grad_norm": 3.7837016582489014,
        "learning_rate": 3.469971875631982e-05,
        "epoch": 0.20121165248775458,
        "step": 1561
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.154428243637085,
        "learning_rate": 3.4641789587169305e-05,
        "epoch": 0.20134055168857953,
        "step": 1562
    },
    {
        "loss": 2.3034,
        "grad_norm": 2.487406015396118,
        "learning_rate": 3.458388317236156e-05,
        "epoch": 0.2014694508894045,
        "step": 1563
    },
    {
        "loss": 1.8548,
        "grad_norm": 3.11285662651062,
        "learning_rate": 3.4525999597689316e-05,
        "epoch": 0.20159835009022944,
        "step": 1564
    },
    {
        "loss": 2.159,
        "grad_norm": 1.4197666645050049,
        "learning_rate": 3.446813894891139e-05,
        "epoch": 0.2017272492910544,
        "step": 1565
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.3404810428619385,
        "learning_rate": 3.4410301311752707e-05,
        "epoch": 0.20185614849187936,
        "step": 1566
    },
    {
        "loss": 2.0564,
        "grad_norm": 1.481333613395691,
        "learning_rate": 3.4352486771904074e-05,
        "epoch": 0.2019850476927043,
        "step": 1567
    },
    {
        "loss": 2.589,
        "grad_norm": 1.707486867904663,
        "learning_rate": 3.429469541502206e-05,
        "epoch": 0.20211394689352927,
        "step": 1568
    },
    {
        "loss": 1.7484,
        "grad_norm": 2.6518654823303223,
        "learning_rate": 3.423692732672892e-05,
        "epoch": 0.20224284609435422,
        "step": 1569
    },
    {
        "loss": 2.113,
        "grad_norm": 2.243048906326294,
        "learning_rate": 3.41791825926124e-05,
        "epoch": 0.20237174529517918,
        "step": 1570
    },
    {
        "loss": 1.4355,
        "grad_norm": 2.596107006072998,
        "learning_rate": 3.412146129822568e-05,
        "epoch": 0.20250064449600413,
        "step": 1571
    },
    {
        "loss": 2.3206,
        "grad_norm": 1.8510698080062866,
        "learning_rate": 3.406376352908719e-05,
        "epoch": 0.2026295436968291,
        "step": 1572
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.9057265520095825,
        "learning_rate": 3.40060893706805e-05,
        "epoch": 0.20275844289765405,
        "step": 1573
    },
    {
        "loss": 1.2443,
        "grad_norm": 2.5487565994262695,
        "learning_rate": 3.394843890845422e-05,
        "epoch": 0.202887342098479,
        "step": 1574
    },
    {
        "loss": 2.3853,
        "grad_norm": 1.8046315908432007,
        "learning_rate": 3.3890812227821834e-05,
        "epoch": 0.20301624129930396,
        "step": 1575
    },
    {
        "loss": 2.7015,
        "grad_norm": 1.2819297313690186,
        "learning_rate": 3.3833209414161616e-05,
        "epoch": 0.2031451405001289,
        "step": 1576
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.6856908798217773,
        "learning_rate": 3.377563055281644e-05,
        "epoch": 0.20327403970095384,
        "step": 1577
    },
    {
        "loss": 2.522,
        "grad_norm": 1.5360206365585327,
        "learning_rate": 3.3718075729093736e-05,
        "epoch": 0.2034029389017788,
        "step": 1578
    },
    {
        "loss": 2.2521,
        "grad_norm": 1.9972636699676514,
        "learning_rate": 3.366054502826527e-05,
        "epoch": 0.20353183810260375,
        "step": 1579
    },
    {
        "loss": 2.4307,
        "grad_norm": 2.38511061668396,
        "learning_rate": 3.360303853556714e-05,
        "epoch": 0.2036607373034287,
        "step": 1580
    },
    {
        "loss": 2.4459,
        "grad_norm": 1.7477166652679443,
        "learning_rate": 3.354555633619952e-05,
        "epoch": 0.20378963650425366,
        "step": 1581
    },
    {
        "loss": 2.2504,
        "grad_norm": 1.4797470569610596,
        "learning_rate": 3.3488098515326575e-05,
        "epoch": 0.20391853570507862,
        "step": 1582
    },
    {
        "loss": 2.6815,
        "grad_norm": 1.6288286447525024,
        "learning_rate": 3.343066515807642e-05,
        "epoch": 0.20404743490590357,
        "step": 1583
    },
    {
        "loss": 2.4229,
        "grad_norm": 1.4956213235855103,
        "learning_rate": 3.337325634954092e-05,
        "epoch": 0.20417633410672853,
        "step": 1584
    },
    {
        "loss": 2.1656,
        "grad_norm": 1.6785694360733032,
        "learning_rate": 3.331587217477545e-05,
        "epoch": 0.20430523330755349,
        "step": 1585
    },
    {
        "loss": 2.3344,
        "grad_norm": 1.891769289970398,
        "learning_rate": 3.325851271879905e-05,
        "epoch": 0.20443413250837844,
        "step": 1586
    },
    {
        "loss": 2.311,
        "grad_norm": 2.045403003692627,
        "learning_rate": 3.320117806659404e-05,
        "epoch": 0.2045630317092034,
        "step": 1587
    },
    {
        "loss": 1.9701,
        "grad_norm": 2.219348669052124,
        "learning_rate": 3.314386830310601e-05,
        "epoch": 0.20469193091002835,
        "step": 1588
    },
    {
        "loss": 1.9095,
        "grad_norm": 1.242665410041809,
        "learning_rate": 3.308658351324369e-05,
        "epoch": 0.2048208301108533,
        "step": 1589
    },
    {
        "loss": 2.3068,
        "grad_norm": 1.8013097047805786,
        "learning_rate": 3.3029323781878816e-05,
        "epoch": 0.20494972931167826,
        "step": 1590
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.478468656539917,
        "learning_rate": 3.297208919384593e-05,
        "epoch": 0.20507862851250322,
        "step": 1591
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.5065001249313354,
        "learning_rate": 3.291487983394245e-05,
        "epoch": 0.20520752771332817,
        "step": 1592
    },
    {
        "loss": 2.1605,
        "grad_norm": 1.8667800426483154,
        "learning_rate": 3.2857695786928264e-05,
        "epoch": 0.20533642691415313,
        "step": 1593
    },
    {
        "loss": 2.0177,
        "grad_norm": 2.743415355682373,
        "learning_rate": 3.2800537137525875e-05,
        "epoch": 0.2054653261149781,
        "step": 1594
    },
    {
        "loss": 2.0928,
        "grad_norm": 2.9939560890197754,
        "learning_rate": 3.2743403970420125e-05,
        "epoch": 0.20559422531580304,
        "step": 1595
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.7442049980163574,
        "learning_rate": 3.2686296370258106e-05,
        "epoch": 0.205723124516628,
        "step": 1596
    },
    {
        "loss": 1.7176,
        "grad_norm": 2.836825132369995,
        "learning_rate": 3.2629214421648957e-05,
        "epoch": 0.20585202371745295,
        "step": 1597
    },
    {
        "loss": 1.8685,
        "grad_norm": 2.7182602882385254,
        "learning_rate": 3.2572158209163936e-05,
        "epoch": 0.2059809229182779,
        "step": 1598
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.733617901802063,
        "learning_rate": 3.2515127817336064e-05,
        "epoch": 0.20610982211910286,
        "step": 1599
    },
    {
        "loss": 2.2107,
        "grad_norm": 2.2275540828704834,
        "learning_rate": 3.24581233306602e-05,
        "epoch": 0.20623872131992782,
        "step": 1600
    },
    {
        "loss": 2.0512,
        "grad_norm": 1.7099562883377075,
        "learning_rate": 3.240114483359275e-05,
        "epoch": 0.20636762052075278,
        "step": 1601
    },
    {
        "loss": 2.1945,
        "grad_norm": 1.2556288242340088,
        "learning_rate": 3.234419241055161e-05,
        "epoch": 0.20649651972157773,
        "step": 1602
    },
    {
        "loss": 2.1612,
        "grad_norm": 2.3404247760772705,
        "learning_rate": 3.2287266145916115e-05,
        "epoch": 0.2066254189224027,
        "step": 1603
    },
    {
        "loss": 2.7987,
        "grad_norm": 1.583533763885498,
        "learning_rate": 3.223036612402679e-05,
        "epoch": 0.20675431812322764,
        "step": 1604
    },
    {
        "loss": 2.135,
        "grad_norm": 2.264650821685791,
        "learning_rate": 3.21734924291853e-05,
        "epoch": 0.2068832173240526,
        "step": 1605
    },
    {
        "loss": 1.8883,
        "grad_norm": 2.571843385696411,
        "learning_rate": 3.211664514565428e-05,
        "epoch": 0.20701211652487755,
        "step": 1606
    },
    {
        "loss": 1.776,
        "grad_norm": 2.207453966140747,
        "learning_rate": 3.205982435765726e-05,
        "epoch": 0.2071410157257025,
        "step": 1607
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.976977825164795,
        "learning_rate": 3.200303014937852e-05,
        "epoch": 0.20726991492652747,
        "step": 1608
    },
    {
        "loss": 2.5859,
        "grad_norm": 2.59757137298584,
        "learning_rate": 3.194626260496293e-05,
        "epoch": 0.20739881412735242,
        "step": 1609
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.7953780889511108,
        "learning_rate": 3.188952180851589e-05,
        "epoch": 0.20752771332817738,
        "step": 1610
    },
    {
        "loss": 2.2056,
        "grad_norm": 2.0002663135528564,
        "learning_rate": 3.183280784410311e-05,
        "epoch": 0.20765661252900233,
        "step": 1611
    },
    {
        "loss": 2.2918,
        "grad_norm": 1.7367668151855469,
        "learning_rate": 3.177612079575066e-05,
        "epoch": 0.2077855117298273,
        "step": 1612
    },
    {
        "loss": 1.8648,
        "grad_norm": 1.649882435798645,
        "learning_rate": 3.171946074744462e-05,
        "epoch": 0.20791441093065224,
        "step": 1613
    },
    {
        "loss": 1.8688,
        "grad_norm": 2.4206745624542236,
        "learning_rate": 3.1662827783131086e-05,
        "epoch": 0.20804331013147717,
        "step": 1614
    },
    {
        "loss": 2.0281,
        "grad_norm": 1.6164743900299072,
        "learning_rate": 3.160622198671609e-05,
        "epoch": 0.20817220933230213,
        "step": 1615
    },
    {
        "loss": 1.9571,
        "grad_norm": 2.103085994720459,
        "learning_rate": 3.154964344206539e-05,
        "epoch": 0.20830110853312708,
        "step": 1616
    },
    {
        "loss": 1.9729,
        "grad_norm": 2.368938446044922,
        "learning_rate": 3.149309223300428e-05,
        "epoch": 0.20843000773395204,
        "step": 1617
    },
    {
        "loss": 2.2304,
        "grad_norm": 1.5255447626113892,
        "learning_rate": 3.1436568443317694e-05,
        "epoch": 0.208558906934777,
        "step": 1618
    },
    {
        "loss": 2.4274,
        "grad_norm": 1.780824899673462,
        "learning_rate": 3.138007215674984e-05,
        "epoch": 0.20868780613560195,
        "step": 1619
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.460773229598999,
        "learning_rate": 3.1323603457004214e-05,
        "epoch": 0.2088167053364269,
        "step": 1620
    },
    {
        "loss": 1.7521,
        "grad_norm": 2.1569669246673584,
        "learning_rate": 3.1267162427743446e-05,
        "epoch": 0.20894560453725186,
        "step": 1621
    },
    {
        "loss": 2.0525,
        "grad_norm": 2.4998672008514404,
        "learning_rate": 3.121074915258916e-05,
        "epoch": 0.20907450373807682,
        "step": 1622
    },
    {
        "loss": 2.3372,
        "grad_norm": 2.894104480743408,
        "learning_rate": 3.115436371512184e-05,
        "epoch": 0.20920340293890177,
        "step": 1623
    },
    {
        "loss": 2.3344,
        "grad_norm": 1.7331489324569702,
        "learning_rate": 3.109800619888081e-05,
        "epoch": 0.20933230213972673,
        "step": 1624
    },
    {
        "loss": 2.2629,
        "grad_norm": 1.6867691278457642,
        "learning_rate": 3.104167668736391e-05,
        "epoch": 0.20946120134055168,
        "step": 1625
    },
    {
        "loss": 2.4373,
        "grad_norm": 1.215867519378662,
        "learning_rate": 3.098537526402753e-05,
        "epoch": 0.20959010054137664,
        "step": 1626
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.0085562467575073,
        "learning_rate": 3.092910201228651e-05,
        "epoch": 0.2097189997422016,
        "step": 1627
    },
    {
        "loss": 2.1395,
        "grad_norm": 1.4979304075241089,
        "learning_rate": 3.08728570155139e-05,
        "epoch": 0.20984789894302655,
        "step": 1628
    },
    {
        "loss": 1.4316,
        "grad_norm": 2.680669069290161,
        "learning_rate": 3.081664035704083e-05,
        "epoch": 0.2099767981438515,
        "step": 1629
    },
    {
        "loss": 2.1334,
        "grad_norm": 2.332456350326538,
        "learning_rate": 3.076045212015656e-05,
        "epoch": 0.21010569734467646,
        "step": 1630
    },
    {
        "loss": 1.684,
        "grad_norm": 2.84057879447937,
        "learning_rate": 3.070429238810816e-05,
        "epoch": 0.21023459654550142,
        "step": 1631
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.7145391702651978,
        "learning_rate": 3.0648161244100505e-05,
        "epoch": 0.21036349574632637,
        "step": 1632
    },
    {
        "loss": 2.113,
        "grad_norm": 2.158846378326416,
        "learning_rate": 3.059205877129609e-05,
        "epoch": 0.21049239494715133,
        "step": 1633
    },
    {
        "loss": 1.8238,
        "grad_norm": 2.3270652294158936,
        "learning_rate": 3.053598505281496e-05,
        "epoch": 0.21062129414797628,
        "step": 1634
    },
    {
        "loss": 1.9243,
        "grad_norm": 2.5720417499542236,
        "learning_rate": 3.0479940171734533e-05,
        "epoch": 0.21075019334880124,
        "step": 1635
    },
    {
        "loss": 1.4395,
        "grad_norm": 3.0328562259674072,
        "learning_rate": 3.0423924211089505e-05,
        "epoch": 0.2108790925496262,
        "step": 1636
    },
    {
        "loss": 2.501,
        "grad_norm": 1.8216722011566162,
        "learning_rate": 3.0367937253871748e-05,
        "epoch": 0.21100799175045115,
        "step": 1637
    },
    {
        "loss": 1.9512,
        "grad_norm": 2.219529151916504,
        "learning_rate": 3.031197938303012e-05,
        "epoch": 0.2111368909512761,
        "step": 1638
    },
    {
        "loss": 2.1315,
        "grad_norm": 2.795387029647827,
        "learning_rate": 3.0256050681470444e-05,
        "epoch": 0.21126579015210106,
        "step": 1639
    },
    {
        "loss": 1.8879,
        "grad_norm": 2.514934539794922,
        "learning_rate": 3.0200151232055262e-05,
        "epoch": 0.21139468935292602,
        "step": 1640
    },
    {
        "loss": 2.0745,
        "grad_norm": 1.3980004787445068,
        "learning_rate": 3.0144281117603847e-05,
        "epoch": 0.21152358855375097,
        "step": 1641
    },
    {
        "loss": 2.0245,
        "grad_norm": 1.8995332717895508,
        "learning_rate": 3.0088440420891933e-05,
        "epoch": 0.21165248775457593,
        "step": 1642
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.428830146789551,
        "learning_rate": 3.003262922465171e-05,
        "epoch": 0.21178138695540089,
        "step": 1643
    },
    {
        "loss": 1.9769,
        "grad_norm": 2.428727388381958,
        "learning_rate": 2.9976847611571702e-05,
        "epoch": 0.21191028615622584,
        "step": 1644
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.0016119480133057,
        "learning_rate": 2.9921095664296516e-05,
        "epoch": 0.2120391853570508,
        "step": 1645
    },
    {
        "loss": 2.1375,
        "grad_norm": 1.7454596757888794,
        "learning_rate": 2.986537346542685e-05,
        "epoch": 0.21216808455787575,
        "step": 1646
    },
    {
        "loss": 2.3941,
        "grad_norm": 1.6728053092956543,
        "learning_rate": 2.9809681097519372e-05,
        "epoch": 0.2122969837587007,
        "step": 1647
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.9204915761947632,
        "learning_rate": 2.9754018643086485e-05,
        "epoch": 0.21242588295952566,
        "step": 1648
    },
    {
        "loss": 2.1239,
        "grad_norm": 1.709456205368042,
        "learning_rate": 2.969838618459627e-05,
        "epoch": 0.21255478216035062,
        "step": 1649
    },
    {
        "loss": 1.6517,
        "grad_norm": 2.178945541381836,
        "learning_rate": 2.964278380447244e-05,
        "epoch": 0.21268368136117555,
        "step": 1650
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.3899303674697876,
        "learning_rate": 2.9587211585094067e-05,
        "epoch": 0.2128125805620005,
        "step": 1651
    },
    {
        "loss": 2.2846,
        "grad_norm": 2.743126630783081,
        "learning_rate": 2.95316696087956e-05,
        "epoch": 0.21294147976282546,
        "step": 1652
    },
    {
        "loss": 2.2241,
        "grad_norm": 1.672149658203125,
        "learning_rate": 2.9476157957866622e-05,
        "epoch": 0.21307037896365041,
        "step": 1653
    },
    {
        "loss": 1.7501,
        "grad_norm": 1.999050498008728,
        "learning_rate": 2.942067671455182e-05,
        "epoch": 0.21319927816447537,
        "step": 1654
    },
    {
        "loss": 2.2209,
        "grad_norm": 1.7336701154708862,
        "learning_rate": 2.9365225961050846e-05,
        "epoch": 0.21332817736530033,
        "step": 1655
    },
    {
        "loss": 2.173,
        "grad_norm": 2.4739599227905273,
        "learning_rate": 2.930980577951813e-05,
        "epoch": 0.21345707656612528,
        "step": 1656
    },
    {
        "loss": 2.3309,
        "grad_norm": 1.9928637742996216,
        "learning_rate": 2.9254416252062856e-05,
        "epoch": 0.21358597576695024,
        "step": 1657
    },
    {
        "loss": 2.4597,
        "grad_norm": 1.364719033241272,
        "learning_rate": 2.9199057460748747e-05,
        "epoch": 0.2137148749677752,
        "step": 1658
    },
    {
        "loss": 2.3139,
        "grad_norm": 1.7767070531845093,
        "learning_rate": 2.914372948759406e-05,
        "epoch": 0.21384377416860015,
        "step": 1659
    },
    {
        "loss": 2.3871,
        "grad_norm": 2.3538453578948975,
        "learning_rate": 2.9088432414571286e-05,
        "epoch": 0.2139726733694251,
        "step": 1660
    },
    {
        "loss": 1.9921,
        "grad_norm": 1.534541368484497,
        "learning_rate": 2.903316632360721e-05,
        "epoch": 0.21410157257025006,
        "step": 1661
    },
    {
        "loss": 2.1454,
        "grad_norm": 2.4033145904541016,
        "learning_rate": 2.897793129658274e-05,
        "epoch": 0.21423047177107502,
        "step": 1662
    },
    {
        "loss": 1.5511,
        "grad_norm": 1.8908419609069824,
        "learning_rate": 2.892272741533269e-05,
        "epoch": 0.21435937097189997,
        "step": 1663
    },
    {
        "loss": 1.8588,
        "grad_norm": 2.38090443611145,
        "learning_rate": 2.8867554761645716e-05,
        "epoch": 0.21448827017272493,
        "step": 1664
    },
    {
        "loss": 1.6945,
        "grad_norm": 2.9814138412475586,
        "learning_rate": 2.8812413417264328e-05,
        "epoch": 0.21461716937354988,
        "step": 1665
    },
    {
        "loss": 1.4327,
        "grad_norm": 2.509064197540283,
        "learning_rate": 2.875730346388452e-05,
        "epoch": 0.21474606857437484,
        "step": 1666
    },
    {
        "loss": 2.2147,
        "grad_norm": 2.0279369354248047,
        "learning_rate": 2.8702224983155834e-05,
        "epoch": 0.2148749677751998,
        "step": 1667
    },
    {
        "loss": 2.5109,
        "grad_norm": 1.1049286127090454,
        "learning_rate": 2.8647178056681194e-05,
        "epoch": 0.21500386697602475,
        "step": 1668
    },
    {
        "loss": 2.6388,
        "grad_norm": 2.6089282035827637,
        "learning_rate": 2.8592162766016773e-05,
        "epoch": 0.2151327661768497,
        "step": 1669
    },
    {
        "loss": 2.2608,
        "grad_norm": 2.139946699142456,
        "learning_rate": 2.853717919267181e-05,
        "epoch": 0.21526166537767466,
        "step": 1670
    },
    {
        "loss": 2.105,
        "grad_norm": 1.923167109489441,
        "learning_rate": 2.8482227418108626e-05,
        "epoch": 0.21539056457849962,
        "step": 1671
    },
    {
        "loss": 2.5191,
        "grad_norm": 1.944063425064087,
        "learning_rate": 2.8427307523742424e-05,
        "epoch": 0.21551946377932457,
        "step": 1672
    },
    {
        "loss": 2.2474,
        "grad_norm": 2.121044635772705,
        "learning_rate": 2.8372419590941113e-05,
        "epoch": 0.21564836298014953,
        "step": 1673
    },
    {
        "loss": 1.815,
        "grad_norm": 2.452626943588257,
        "learning_rate": 2.8317563701025318e-05,
        "epoch": 0.21577726218097448,
        "step": 1674
    },
    {
        "loss": 2.0198,
        "grad_norm": 2.5357046127319336,
        "learning_rate": 2.826273993526817e-05,
        "epoch": 0.21590616138179944,
        "step": 1675
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.8205710649490356,
        "learning_rate": 2.8207948374895166e-05,
        "epoch": 0.2160350605826244,
        "step": 1676
    },
    {
        "loss": 2.3395,
        "grad_norm": 1.1572504043579102,
        "learning_rate": 2.8153189101084133e-05,
        "epoch": 0.21616395978344935,
        "step": 1677
    },
    {
        "loss": 1.852,
        "grad_norm": 2.5662097930908203,
        "learning_rate": 2.809846219496505e-05,
        "epoch": 0.2162928589842743,
        "step": 1678
    },
    {
        "loss": 1.5514,
        "grad_norm": 2.3989927768707275,
        "learning_rate": 2.8043767737619974e-05,
        "epoch": 0.21642175818509926,
        "step": 1679
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.4040653705596924,
        "learning_rate": 2.7989105810082795e-05,
        "epoch": 0.21655065738592422,
        "step": 1680
    },
    {
        "loss": 1.3204,
        "grad_norm": 2.584550619125366,
        "learning_rate": 2.79344764933393e-05,
        "epoch": 0.21667955658674917,
        "step": 1681
    },
    {
        "loss": 2.2032,
        "grad_norm": 1.7297542095184326,
        "learning_rate": 2.7879879868326918e-05,
        "epoch": 0.21680845578757413,
        "step": 1682
    },
    {
        "loss": 2.471,
        "grad_norm": 2.1537208557128906,
        "learning_rate": 2.782531601593467e-05,
        "epoch": 0.21693735498839908,
        "step": 1683
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.8657853603363037,
        "learning_rate": 2.7770785017002954e-05,
        "epoch": 0.21706625418922404,
        "step": 1684
    },
    {
        "loss": 2.389,
        "grad_norm": 2.4105465412139893,
        "learning_rate": 2.7716286952323618e-05,
        "epoch": 0.217195153390049,
        "step": 1685
    },
    {
        "loss": 1.2846,
        "grad_norm": 3.191047191619873,
        "learning_rate": 2.7661821902639573e-05,
        "epoch": 0.21732405259087392,
        "step": 1686
    },
    {
        "loss": 1.6298,
        "grad_norm": 3.0553932189941406,
        "learning_rate": 2.7607389948644913e-05,
        "epoch": 0.21745295179169888,
        "step": 1687
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.909048080444336,
        "learning_rate": 2.7552991170984677e-05,
        "epoch": 0.21758185099252383,
        "step": 1688
    },
    {
        "loss": 1.6277,
        "grad_norm": 1.9393360614776611,
        "learning_rate": 2.7498625650254718e-05,
        "epoch": 0.2177107501933488,
        "step": 1689
    },
    {
        "loss": 1.7376,
        "grad_norm": 2.4134786128997803,
        "learning_rate": 2.744429346700162e-05,
        "epoch": 0.21783964939417375,
        "step": 1690
    },
    {
        "loss": 1.4566,
        "grad_norm": 2.264232873916626,
        "learning_rate": 2.7389994701722667e-05,
        "epoch": 0.2179685485949987,
        "step": 1691
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.3037827014923096,
        "learning_rate": 2.733572943486552e-05,
        "epoch": 0.21809744779582366,
        "step": 1692
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.4363296031951904,
        "learning_rate": 2.72814977468282e-05,
        "epoch": 0.2182263469966486,
        "step": 1693
    },
    {
        "loss": 2.4596,
        "grad_norm": 2.383077383041382,
        "learning_rate": 2.722729971795912e-05,
        "epoch": 0.21835524619747357,
        "step": 1694
    },
    {
        "loss": 1.949,
        "grad_norm": 1.6123255491256714,
        "learning_rate": 2.7173135428556694e-05,
        "epoch": 0.21848414539829852,
        "step": 1695
    },
    {
        "loss": 2.4114,
        "grad_norm": 1.9269031286239624,
        "learning_rate": 2.7119004958869333e-05,
        "epoch": 0.21861304459912348,
        "step": 1696
    },
    {
        "loss": 2.3305,
        "grad_norm": 1.8326389789581299,
        "learning_rate": 2.7064908389095468e-05,
        "epoch": 0.21874194379994844,
        "step": 1697
    },
    {
        "loss": 1.6285,
        "grad_norm": 2.511415719985962,
        "learning_rate": 2.7010845799383216e-05,
        "epoch": 0.2188708430007734,
        "step": 1698
    },
    {
        "loss": 2.0683,
        "grad_norm": 1.7979481220245361,
        "learning_rate": 2.6956817269830344e-05,
        "epoch": 0.21899974220159835,
        "step": 1699
    },
    {
        "loss": 1.9605,
        "grad_norm": 1.841625452041626,
        "learning_rate": 2.6902822880484192e-05,
        "epoch": 0.2191286414024233,
        "step": 1700
    },
    {
        "loss": 2.1251,
        "grad_norm": 1.625135898590088,
        "learning_rate": 2.6848862711341517e-05,
        "epoch": 0.21925754060324826,
        "step": 1701
    },
    {
        "loss": 2.0728,
        "grad_norm": 1.3115309476852417,
        "learning_rate": 2.679493684234834e-05,
        "epoch": 0.2193864398040732,
        "step": 1702
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.9278950691223145,
        "learning_rate": 2.674104535339989e-05,
        "epoch": 0.21951533900489817,
        "step": 1703
    },
    {
        "loss": 2.5188,
        "grad_norm": 1.6862380504608154,
        "learning_rate": 2.668718832434049e-05,
        "epoch": 0.21964423820572312,
        "step": 1704
    },
    {
        "loss": 2.1549,
        "grad_norm": 3.3185477256774902,
        "learning_rate": 2.663336583496333e-05,
        "epoch": 0.21977313740654808,
        "step": 1705
    },
    {
        "loss": 2.3174,
        "grad_norm": 1.8292206525802612,
        "learning_rate": 2.65795779650105e-05,
        "epoch": 0.21990203660737304,
        "step": 1706
    },
    {
        "loss": 1.8886,
        "grad_norm": 1.8484634160995483,
        "learning_rate": 2.652582479417276e-05,
        "epoch": 0.220030935808198,
        "step": 1707
    },
    {
        "loss": 2.2604,
        "grad_norm": 1.304397702217102,
        "learning_rate": 2.64721064020895e-05,
        "epoch": 0.22015983500902295,
        "step": 1708
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.3311946392059326,
        "learning_rate": 2.641842286834851e-05,
        "epoch": 0.2202887342098479,
        "step": 1709
    },
    {
        "loss": 2.4949,
        "grad_norm": 1.088352084159851,
        "learning_rate": 2.6364774272485997e-05,
        "epoch": 0.22041763341067286,
        "step": 1710
    },
    {
        "loss": 2.0218,
        "grad_norm": 2.8615305423736572,
        "learning_rate": 2.6311160693986413e-05,
        "epoch": 0.22054653261149781,
        "step": 1711
    },
    {
        "loss": 1.7621,
        "grad_norm": 1.7702983617782593,
        "learning_rate": 2.625758221228226e-05,
        "epoch": 0.22067543181232277,
        "step": 1712
    },
    {
        "loss": 1.8457,
        "grad_norm": 2.633126735687256,
        "learning_rate": 2.6204038906754092e-05,
        "epoch": 0.22080433101314773,
        "step": 1713
    },
    {
        "loss": 1.8171,
        "grad_norm": 2.734281301498413,
        "learning_rate": 2.615053085673039e-05,
        "epoch": 0.22093323021397268,
        "step": 1714
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.5937095880508423,
        "learning_rate": 2.6097058141487306e-05,
        "epoch": 0.22106212941479764,
        "step": 1715
    },
    {
        "loss": 2.3183,
        "grad_norm": 2.3290860652923584,
        "learning_rate": 2.604362084024866e-05,
        "epoch": 0.2211910286156226,
        "step": 1716
    },
    {
        "loss": 1.5406,
        "grad_norm": 1.9202696084976196,
        "learning_rate": 2.5990219032185903e-05,
        "epoch": 0.22131992781644755,
        "step": 1717
    },
    {
        "loss": 1.7454,
        "grad_norm": 1.9678583145141602,
        "learning_rate": 2.593685279641776e-05,
        "epoch": 0.2214488270172725,
        "step": 1718
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.2130842208862305,
        "learning_rate": 2.588352221201035e-05,
        "epoch": 0.22157772621809746,
        "step": 1719
    },
    {
        "loss": 2.4806,
        "grad_norm": 1.4118107557296753,
        "learning_rate": 2.5830227357976933e-05,
        "epoch": 0.22170662541892242,
        "step": 1720
    },
    {
        "loss": 2.1686,
        "grad_norm": 2.4974076747894287,
        "learning_rate": 2.577696831327786e-05,
        "epoch": 0.22183552461974737,
        "step": 1721
    },
    {
        "loss": 2.4023,
        "grad_norm": 2.2706990242004395,
        "learning_rate": 2.5723745156820356e-05,
        "epoch": 0.2219644238205723,
        "step": 1722
    },
    {
        "loss": 1.7969,
        "grad_norm": 2.5570132732391357,
        "learning_rate": 2.5670557967458603e-05,
        "epoch": 0.22209332302139725,
        "step": 1723
    },
    {
        "loss": 2.0011,
        "grad_norm": 1.976008415222168,
        "learning_rate": 2.5617406823993373e-05,
        "epoch": 0.2222222222222222,
        "step": 1724
    },
    {
        "loss": 1.7457,
        "grad_norm": 1.8836275339126587,
        "learning_rate": 2.5564291805172048e-05,
        "epoch": 0.22235112142304717,
        "step": 1725
    },
    {
        "loss": 2.3849,
        "grad_norm": 1.5266066789627075,
        "learning_rate": 2.551121298968857e-05,
        "epoch": 0.22248002062387212,
        "step": 1726
    },
    {
        "loss": 2.5182,
        "grad_norm": 1.5632777214050293,
        "learning_rate": 2.545817045618322e-05,
        "epoch": 0.22260891982469708,
        "step": 1727
    },
    {
        "loss": 2.0944,
        "grad_norm": 3.059357166290283,
        "learning_rate": 2.540516428324239e-05,
        "epoch": 0.22273781902552203,
        "step": 1728
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.012939453125,
        "learning_rate": 2.5352194549398788e-05,
        "epoch": 0.222866718226347,
        "step": 1729
    },
    {
        "loss": 1.3895,
        "grad_norm": 2.727825164794922,
        "learning_rate": 2.5299261333131062e-05,
        "epoch": 0.22299561742717194,
        "step": 1730
    },
    {
        "loss": 1.112,
        "grad_norm": 2.86618709564209,
        "learning_rate": 2.5246364712863712e-05,
        "epoch": 0.2231245166279969,
        "step": 1731
    },
    {
        "loss": 2.034,
        "grad_norm": 1.5528349876403809,
        "learning_rate": 2.519350476696707e-05,
        "epoch": 0.22325341582882186,
        "step": 1732
    },
    {
        "loss": 2.5122,
        "grad_norm": 1.829312801361084,
        "learning_rate": 2.5140681573757136e-05,
        "epoch": 0.2233823150296468,
        "step": 1733
    },
    {
        "loss": 2.5928,
        "grad_norm": 1.9219021797180176,
        "learning_rate": 2.5087895211495406e-05,
        "epoch": 0.22351121423047177,
        "step": 1734
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.6228229999542236,
        "learning_rate": 2.5035145758388845e-05,
        "epoch": 0.22364011343129672,
        "step": 1735
    },
    {
        "loss": 1.7207,
        "grad_norm": 2.6225032806396484,
        "learning_rate": 2.4982433292589758e-05,
        "epoch": 0.22376901263212168,
        "step": 1736
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.6441149711608887,
        "learning_rate": 2.4929757892195628e-05,
        "epoch": 0.22389791183294663,
        "step": 1737
    },
    {
        "loss": 1.8139,
        "grad_norm": 2.2801058292388916,
        "learning_rate": 2.4877119635248976e-05,
        "epoch": 0.2240268110337716,
        "step": 1738
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.9602913856506348,
        "learning_rate": 2.4824518599737367e-05,
        "epoch": 0.22415571023459654,
        "step": 1739
    },
    {
        "loss": 2.3441,
        "grad_norm": 1.3528704643249512,
        "learning_rate": 2.4771954863593194e-05,
        "epoch": 0.2242846094354215,
        "step": 1740
    },
    {
        "loss": 2.6361,
        "grad_norm": 1.9220504760742188,
        "learning_rate": 2.4719428504693552e-05,
        "epoch": 0.22441350863624646,
        "step": 1741
    },
    {
        "loss": 1.5512,
        "grad_norm": 1.758296012878418,
        "learning_rate": 2.4666939600860188e-05,
        "epoch": 0.2245424078370714,
        "step": 1742
    },
    {
        "loss": 2.3384,
        "grad_norm": 1.8138978481292725,
        "learning_rate": 2.4614488229859433e-05,
        "epoch": 0.22467130703789637,
        "step": 1743
    },
    {
        "loss": 1.348,
        "grad_norm": 2.722177267074585,
        "learning_rate": 2.4562074469401836e-05,
        "epoch": 0.22480020623872132,
        "step": 1744
    },
    {
        "loss": 1.7585,
        "grad_norm": 2.3104844093322754,
        "learning_rate": 2.4509698397142346e-05,
        "epoch": 0.22492910543954628,
        "step": 1745
    },
    {
        "loss": 1.137,
        "grad_norm": 2.873764753341675,
        "learning_rate": 2.44573600906801e-05,
        "epoch": 0.22505800464037123,
        "step": 1746
    },
    {
        "loss": 2.0178,
        "grad_norm": 2.525521755218506,
        "learning_rate": 2.4405059627558195e-05,
        "epoch": 0.2251869038411962,
        "step": 1747
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.2353177070617676,
        "learning_rate": 2.435279708526366e-05,
        "epoch": 0.22531580304202115,
        "step": 1748
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.7871081829071045,
        "learning_rate": 2.4300572541227445e-05,
        "epoch": 0.2254447022428461,
        "step": 1749
    },
    {
        "loss": 1.6693,
        "grad_norm": 2.7996931076049805,
        "learning_rate": 2.424838607282414e-05,
        "epoch": 0.22557360144367106,
        "step": 1750
    },
    {
        "loss": 1.7768,
        "grad_norm": 2.6020991802215576,
        "learning_rate": 2.419623775737184e-05,
        "epoch": 0.225702500644496,
        "step": 1751
    },
    {
        "loss": 2.2862,
        "grad_norm": 1.5366127490997314,
        "learning_rate": 2.414412767213225e-05,
        "epoch": 0.22583139984532097,
        "step": 1752
    },
    {
        "loss": 1.9703,
        "grad_norm": 2.1438562870025635,
        "learning_rate": 2.409205589431039e-05,
        "epoch": 0.22596029904614592,
        "step": 1753
    },
    {
        "loss": 2.4073,
        "grad_norm": 1.5574034452438354,
        "learning_rate": 2.404002250105447e-05,
        "epoch": 0.22608919824697088,
        "step": 1754
    },
    {
        "loss": 1.7623,
        "grad_norm": 2.144127607345581,
        "learning_rate": 2.3988027569455895e-05,
        "epoch": 0.22621809744779584,
        "step": 1755
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.9609339237213135,
        "learning_rate": 2.3936071176549073e-05,
        "epoch": 0.2263469966486208,
        "step": 1756
    },
    {
        "loss": 2.0837,
        "grad_norm": 1.6064022779464722,
        "learning_rate": 2.388415339931126e-05,
        "epoch": 0.22647589584944575,
        "step": 1757
    },
    {
        "loss": 2.7279,
        "grad_norm": 1.5942424535751343,
        "learning_rate": 2.3832274314662607e-05,
        "epoch": 0.2266047950502707,
        "step": 1758
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.5242152214050293,
        "learning_rate": 2.378043399946583e-05,
        "epoch": 0.22673369425109563,
        "step": 1759
    },
    {
        "loss": 2.133,
        "grad_norm": 1.957679271697998,
        "learning_rate": 2.3728632530526262e-05,
        "epoch": 0.22686259345192059,
        "step": 1760
    },
    {
        "loss": 2.0776,
        "grad_norm": 1.9246677160263062,
        "learning_rate": 2.3676869984591676e-05,
        "epoch": 0.22699149265274554,
        "step": 1761
    },
    {
        "loss": 2.35,
        "grad_norm": 1.34086012840271,
        "learning_rate": 2.3625146438352196e-05,
        "epoch": 0.2271203918535705,
        "step": 1762
    },
    {
        "loss": 1.1905,
        "grad_norm": 3.4816980361938477,
        "learning_rate": 2.3573461968440098e-05,
        "epoch": 0.22724929105439545,
        "step": 1763
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.7199113368988037,
        "learning_rate": 2.3521816651429824e-05,
        "epoch": 0.2273781902552204,
        "step": 1764
    },
    {
        "loss": 1.8265,
        "grad_norm": 1.7953555583953857,
        "learning_rate": 2.3470210563837803e-05,
        "epoch": 0.22750708945604536,
        "step": 1765
    },
    {
        "loss": 2.1982,
        "grad_norm": 1.4219955205917358,
        "learning_rate": 2.341864378212234e-05,
        "epoch": 0.22763598865687032,
        "step": 1766
    },
    {
        "loss": 1.933,
        "grad_norm": 1.9401100873947144,
        "learning_rate": 2.3367116382683464e-05,
        "epoch": 0.22776488785769528,
        "step": 1767
    },
    {
        "loss": 2.1679,
        "grad_norm": 2.6486592292785645,
        "learning_rate": 2.3315628441862907e-05,
        "epoch": 0.22789378705852023,
        "step": 1768
    },
    {
        "loss": 1.953,
        "grad_norm": 1.7890104055404663,
        "learning_rate": 2.3264180035943943e-05,
        "epoch": 0.2280226862593452,
        "step": 1769
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.7421596050262451,
        "learning_rate": 2.321277124115121e-05,
        "epoch": 0.22815158546017014,
        "step": 1770
    },
    {
        "loss": 1.3489,
        "grad_norm": 2.2367489337921143,
        "learning_rate": 2.3161402133650724e-05,
        "epoch": 0.2282804846609951,
        "step": 1771
    },
    {
        "loss": 1.9123,
        "grad_norm": 2.1213700771331787,
        "learning_rate": 2.311007278954969e-05,
        "epoch": 0.22840938386182005,
        "step": 1772
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.0762171745300293,
        "learning_rate": 2.3058783284896357e-05,
        "epoch": 0.228538283062645,
        "step": 1773
    },
    {
        "loss": 2.5309,
        "grad_norm": 1.9397683143615723,
        "learning_rate": 2.300753369568e-05,
        "epoch": 0.22866718226346996,
        "step": 1774
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.0227675437927246,
        "learning_rate": 2.2956324097830728e-05,
        "epoch": 0.22879608146429492,
        "step": 1775
    },
    {
        "loss": 1.5556,
        "grad_norm": 1.8847547769546509,
        "learning_rate": 2.2905154567219435e-05,
        "epoch": 0.22892498066511988,
        "step": 1776
    },
    {
        "loss": 2.3066,
        "grad_norm": 2.3399064540863037,
        "learning_rate": 2.2854025179657562e-05,
        "epoch": 0.22905387986594483,
        "step": 1777
    },
    {
        "loss": 2.0911,
        "grad_norm": 1.5998274087905884,
        "learning_rate": 2.2802936010897218e-05,
        "epoch": 0.2291827790667698,
        "step": 1778
    },
    {
        "loss": 2.264,
        "grad_norm": 2.239647150039673,
        "learning_rate": 2.275188713663081e-05,
        "epoch": 0.22931167826759474,
        "step": 1779
    },
    {
        "loss": 2.235,
        "grad_norm": 1.8940309286117554,
        "learning_rate": 2.270087863249104e-05,
        "epoch": 0.2294405774684197,
        "step": 1780
    },
    {
        "loss": 1.4141,
        "grad_norm": 2.715982675552368,
        "learning_rate": 2.2649910574050897e-05,
        "epoch": 0.22956947666924465,
        "step": 1781
    },
    {
        "loss": 1.9406,
        "grad_norm": 2.484602689743042,
        "learning_rate": 2.259898303682339e-05,
        "epoch": 0.2296983758700696,
        "step": 1782
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.308889627456665,
        "learning_rate": 2.254809609626145e-05,
        "epoch": 0.22982727507089457,
        "step": 1783
    },
    {
        "loss": 1.3497,
        "grad_norm": 2.7168033123016357,
        "learning_rate": 2.2497249827757933e-05,
        "epoch": 0.22995617427171952,
        "step": 1784
    },
    {
        "loss": 2.6284,
        "grad_norm": 1.374394416809082,
        "learning_rate": 2.2446444306645415e-05,
        "epoch": 0.23008507347254448,
        "step": 1785
    },
    {
        "loss": 2.1173,
        "grad_norm": 1.7749758958816528,
        "learning_rate": 2.2395679608196062e-05,
        "epoch": 0.23021397267336943,
        "step": 1786
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.7983137369155884,
        "learning_rate": 2.2344955807621603e-05,
        "epoch": 0.2303428718741944,
        "step": 1787
    },
    {
        "loss": 1.4976,
        "grad_norm": 2.996350049972534,
        "learning_rate": 2.229427298007316e-05,
        "epoch": 0.23047177107501934,
        "step": 1788
    },
    {
        "loss": 1.2296,
        "grad_norm": 3.3693716526031494,
        "learning_rate": 2.2243631200641156e-05,
        "epoch": 0.2306006702758443,
        "step": 1789
    },
    {
        "loss": 2.155,
        "grad_norm": 1.5864150524139404,
        "learning_rate": 2.2193030544355208e-05,
        "epoch": 0.23072956947666926,
        "step": 1790
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.5447330474853516,
        "learning_rate": 2.2142471086183953e-05,
        "epoch": 0.2308584686774942,
        "step": 1791
    },
    {
        "loss": 1.8891,
        "grad_norm": 1.5982325077056885,
        "learning_rate": 2.209195290103504e-05,
        "epoch": 0.23098736787831917,
        "step": 1792
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.8946516513824463,
        "learning_rate": 2.2041476063754967e-05,
        "epoch": 0.23111626707914412,
        "step": 1793
    },
    {
        "loss": 2.145,
        "grad_norm": 2.6136186122894287,
        "learning_rate": 2.1991040649128974e-05,
        "epoch": 0.23124516627996908,
        "step": 1794
    },
    {
        "loss": 1.7999,
        "grad_norm": 1.9112211465835571,
        "learning_rate": 2.194064673188089e-05,
        "epoch": 0.231374065480794,
        "step": 1795
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.7462873458862305,
        "learning_rate": 2.1890294386673088e-05,
        "epoch": 0.23150296468161896,
        "step": 1796
    },
    {
        "loss": 1.9926,
        "grad_norm": 2.23699688911438,
        "learning_rate": 2.183998368810637e-05,
        "epoch": 0.23163186388244392,
        "step": 1797
    },
    {
        "loss": 2.49,
        "grad_norm": 1.3586676120758057,
        "learning_rate": 2.178971471071982e-05,
        "epoch": 0.23176076308326887,
        "step": 1798
    },
    {
        "loss": 2.371,
        "grad_norm": 1.2197599411010742,
        "learning_rate": 2.173948752899069e-05,
        "epoch": 0.23188966228409383,
        "step": 1799
    },
    {
        "loss": 1.515,
        "grad_norm": 1.7714303731918335,
        "learning_rate": 2.1689302217334317e-05,
        "epoch": 0.23201856148491878,
        "step": 1800
    },
    {
        "loss": 1.123,
        "grad_norm": 2.5246763229370117,
        "learning_rate": 2.1639158850104048e-05,
        "epoch": 0.23214746068574374,
        "step": 1801
    },
    {
        "loss": 2.2429,
        "grad_norm": 1.1903210878372192,
        "learning_rate": 2.1589057501591002e-05,
        "epoch": 0.2322763598865687,
        "step": 1802
    },
    {
        "loss": 2.209,
        "grad_norm": 1.5551761388778687,
        "learning_rate": 2.1538998246024117e-05,
        "epoch": 0.23240525908739365,
        "step": 1803
    },
    {
        "loss": 1.8067,
        "grad_norm": 2.417520523071289,
        "learning_rate": 2.1488981157569943e-05,
        "epoch": 0.2325341582882186,
        "step": 1804
    },
    {
        "loss": 2.4433,
        "grad_norm": 1.9352012872695923,
        "learning_rate": 2.143900631033256e-05,
        "epoch": 0.23266305748904356,
        "step": 1805
    },
    {
        "loss": 2.0107,
        "grad_norm": 2.2357168197631836,
        "learning_rate": 2.1389073778353437e-05,
        "epoch": 0.23279195668986852,
        "step": 1806
    },
    {
        "loss": 1.8077,
        "grad_norm": 2.257211923599243,
        "learning_rate": 2.1339183635611383e-05,
        "epoch": 0.23292085589069347,
        "step": 1807
    },
    {
        "loss": 2.2291,
        "grad_norm": 2.054471969604492,
        "learning_rate": 2.1289335956022417e-05,
        "epoch": 0.23304975509151843,
        "step": 1808
    },
    {
        "loss": 1.6172,
        "grad_norm": 2.021012783050537,
        "learning_rate": 2.123953081343956e-05,
        "epoch": 0.23317865429234338,
        "step": 1809
    },
    {
        "loss": 1.7095,
        "grad_norm": 2.214667797088623,
        "learning_rate": 2.118976828165295e-05,
        "epoch": 0.23330755349316834,
        "step": 1810
    },
    {
        "loss": 2.1704,
        "grad_norm": 1.3720910549163818,
        "learning_rate": 2.1140048434389465e-05,
        "epoch": 0.2334364526939933,
        "step": 1811
    },
    {
        "loss": 1.3594,
        "grad_norm": 2.817805528640747,
        "learning_rate": 2.109037134531281e-05,
        "epoch": 0.23356535189481825,
        "step": 1812
    },
    {
        "loss": 1.9709,
        "grad_norm": 2.2426509857177734,
        "learning_rate": 2.1040737088023323e-05,
        "epoch": 0.2336942510956432,
        "step": 1813
    },
    {
        "loss": 1.7235,
        "grad_norm": 2.6524205207824707,
        "learning_rate": 2.099114573605791e-05,
        "epoch": 0.23382315029646816,
        "step": 1814
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.2925891876220703,
        "learning_rate": 2.094159736288984e-05,
        "epoch": 0.23395204949729312,
        "step": 1815
    },
    {
        "loss": 1.9384,
        "grad_norm": 2.4485554695129395,
        "learning_rate": 2.089209204192878e-05,
        "epoch": 0.23408094869811807,
        "step": 1816
    },
    {
        "loss": 1.315,
        "grad_norm": 2.787407398223877,
        "learning_rate": 2.084262984652058e-05,
        "epoch": 0.23420984789894303,
        "step": 1817
    },
    {
        "loss": 2.1309,
        "grad_norm": 2.8343493938446045,
        "learning_rate": 2.0793210849947208e-05,
        "epoch": 0.23433874709976799,
        "step": 1818
    },
    {
        "loss": 1.9162,
        "grad_norm": 2.4158806800842285,
        "learning_rate": 2.0743835125426598e-05,
        "epoch": 0.23446764630059294,
        "step": 1819
    },
    {
        "loss": 1.7841,
        "grad_norm": 2.415658473968506,
        "learning_rate": 2.0694502746112606e-05,
        "epoch": 0.2345965455014179,
        "step": 1820
    },
    {
        "loss": 1.8149,
        "grad_norm": 3.1690568923950195,
        "learning_rate": 2.0645213785094858e-05,
        "epoch": 0.23472544470224285,
        "step": 1821
    },
    {
        "loss": 1.6481,
        "grad_norm": 2.486588716506958,
        "learning_rate": 2.0595968315398674e-05,
        "epoch": 0.2348543439030678,
        "step": 1822
    },
    {
        "loss": 1.6018,
        "grad_norm": 1.69950532913208,
        "learning_rate": 2.0546766409984887e-05,
        "epoch": 0.23498324310389276,
        "step": 1823
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.5549919605255127,
        "learning_rate": 2.0497608141749826e-05,
        "epoch": 0.23511214230471772,
        "step": 1824
    },
    {
        "loss": 2.0863,
        "grad_norm": 1.4177449941635132,
        "learning_rate": 2.0448493583525158e-05,
        "epoch": 0.23524104150554268,
        "step": 1825
    },
    {
        "loss": 2.4532,
        "grad_norm": 1.3117671012878418,
        "learning_rate": 2.0399422808077824e-05,
        "epoch": 0.23536994070636763,
        "step": 1826
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.4741780757904053,
        "learning_rate": 2.0350395888109814e-05,
        "epoch": 0.2354988399071926,
        "step": 1827
    },
    {
        "loss": 2.2382,
        "grad_norm": 2.581463575363159,
        "learning_rate": 2.0301412896258214e-05,
        "epoch": 0.23562773910801754,
        "step": 1828
    },
    {
        "loss": 2.1184,
        "grad_norm": 1.672295331954956,
        "learning_rate": 2.0252473905095015e-05,
        "epoch": 0.2357566383088425,
        "step": 1829
    },
    {
        "loss": 2.6871,
        "grad_norm": 1.3839658498764038,
        "learning_rate": 2.0203578987127024e-05,
        "epoch": 0.23588553750966745,
        "step": 1830
    },
    {
        "loss": 1.3124,
        "grad_norm": 3.4108119010925293,
        "learning_rate": 2.0154728214795702e-05,
        "epoch": 0.23601443671049238,
        "step": 1831
    },
    {
        "loss": 1.1912,
        "grad_norm": 3.5183284282684326,
        "learning_rate": 2.010592166047715e-05,
        "epoch": 0.23614333591131734,
        "step": 1832
    },
    {
        "loss": 1.7601,
        "grad_norm": 2.4510750770568848,
        "learning_rate": 2.005715939648195e-05,
        "epoch": 0.2362722351121423,
        "step": 1833
    },
    {
        "loss": 1.8853,
        "grad_norm": 2.8035199642181396,
        "learning_rate": 2.000844149505508e-05,
        "epoch": 0.23640113431296725,
        "step": 1834
    },
    {
        "loss": 1.9951,
        "grad_norm": 2.0935778617858887,
        "learning_rate": 1.9959768028375737e-05,
        "epoch": 0.2365300335137922,
        "step": 1835
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.9735913276672363,
        "learning_rate": 1.991113906855733e-05,
        "epoch": 0.23665893271461716,
        "step": 1836
    },
    {
        "loss": 1.4366,
        "grad_norm": 2.083794116973877,
        "learning_rate": 1.986255468764734e-05,
        "epoch": 0.23678783191544212,
        "step": 1837
    },
    {
        "loss": 2.3727,
        "grad_norm": 2.055537223815918,
        "learning_rate": 1.981401495762714e-05,
        "epoch": 0.23691673111626707,
        "step": 1838
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.053037643432617,
        "learning_rate": 1.976551995041201e-05,
        "epoch": 0.23704563031709203,
        "step": 1839
    },
    {
        "loss": 1.0114,
        "grad_norm": 4.1189284324646,
        "learning_rate": 1.971706973785094e-05,
        "epoch": 0.23717452951791698,
        "step": 1840
    },
    {
        "loss": 2.5266,
        "grad_norm": 2.0296549797058105,
        "learning_rate": 1.9668664391726584e-05,
        "epoch": 0.23730342871874194,
        "step": 1841
    },
    {
        "loss": 2.3022,
        "grad_norm": 1.4868451356887817,
        "learning_rate": 1.962030398375506e-05,
        "epoch": 0.2374323279195669,
        "step": 1842
    },
    {
        "loss": 2.3952,
        "grad_norm": 1.342405915260315,
        "learning_rate": 1.957198858558597e-05,
        "epoch": 0.23756122712039185,
        "step": 1843
    },
    {
        "loss": 2.5162,
        "grad_norm": 1.8440486192703247,
        "learning_rate": 1.95237182688022e-05,
        "epoch": 0.2376901263212168,
        "step": 1844
    },
    {
        "loss": 1.7448,
        "grad_norm": 1.706817388534546,
        "learning_rate": 1.947549310491987e-05,
        "epoch": 0.23781902552204176,
        "step": 1845
    },
    {
        "loss": 2.3352,
        "grad_norm": 2.252774238586426,
        "learning_rate": 1.9427313165388155e-05,
        "epoch": 0.23794792472286672,
        "step": 1846
    },
    {
        "loss": 1.871,
        "grad_norm": 2.4255101680755615,
        "learning_rate": 1.937917852158927e-05,
        "epoch": 0.23807682392369167,
        "step": 1847
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.8351093530654907,
        "learning_rate": 1.93310892448383e-05,
        "epoch": 0.23820572312451663,
        "step": 1848
    },
    {
        "loss": 2.3164,
        "grad_norm": 2.0514163970947266,
        "learning_rate": 1.9283045406383153e-05,
        "epoch": 0.23833462232534158,
        "step": 1849
    },
    {
        "loss": 1.3157,
        "grad_norm": 2.680755376815796,
        "learning_rate": 1.923504707740434e-05,
        "epoch": 0.23846352152616654,
        "step": 1850
    },
    {
        "loss": 2.4782,
        "grad_norm": 1.2725510597229004,
        "learning_rate": 1.9187094329015015e-05,
        "epoch": 0.2385924207269915,
        "step": 1851
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.665377140045166,
        "learning_rate": 1.9139187232260773e-05,
        "epoch": 0.23872131992781645,
        "step": 1852
    },
    {
        "loss": 2.1567,
        "grad_norm": 1.337189793586731,
        "learning_rate": 1.9091325858119595e-05,
        "epoch": 0.2388502191286414,
        "step": 1853
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.4553900957107544,
        "learning_rate": 1.9043510277501674e-05,
        "epoch": 0.23897911832946636,
        "step": 1854
    },
    {
        "loss": 1.7098,
        "grad_norm": 2.9416935443878174,
        "learning_rate": 1.8995740561249402e-05,
        "epoch": 0.23910801753029132,
        "step": 1855
    },
    {
        "loss": 2.4766,
        "grad_norm": 1.401955246925354,
        "learning_rate": 1.89480167801372e-05,
        "epoch": 0.23923691673111627,
        "step": 1856
    },
    {
        "loss": 2.1039,
        "grad_norm": 1.732087254524231,
        "learning_rate": 1.890033900487146e-05,
        "epoch": 0.23936581593194123,
        "step": 1857
    },
    {
        "loss": 1.7096,
        "grad_norm": 3.265597343444824,
        "learning_rate": 1.885270730609034e-05,
        "epoch": 0.23949471513276618,
        "step": 1858
    },
    {
        "loss": 1.64,
        "grad_norm": 2.225642442703247,
        "learning_rate": 1.8805121754363814e-05,
        "epoch": 0.23962361433359114,
        "step": 1859
    },
    {
        "loss": 2.1901,
        "grad_norm": 1.7824453115463257,
        "learning_rate": 1.8757582420193437e-05,
        "epoch": 0.2397525135344161,
        "step": 1860
    },
    {
        "loss": 2.4267,
        "grad_norm": 2.019440174102783,
        "learning_rate": 1.8710089374012337e-05,
        "epoch": 0.23988141273524105,
        "step": 1861
    },
    {
        "loss": 2.4541,
        "grad_norm": 1.9852361679077148,
        "learning_rate": 1.8662642686184966e-05,
        "epoch": 0.240010311936066,
        "step": 1862
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.1208927631378174,
        "learning_rate": 1.861524242700724e-05,
        "epoch": 0.24013921113689096,
        "step": 1863
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.962241768836975,
        "learning_rate": 1.856788866670614e-05,
        "epoch": 0.24026811033771592,
        "step": 1864
    },
    {
        "loss": 2.0125,
        "grad_norm": 1.0987844467163086,
        "learning_rate": 1.8520581475439842e-05,
        "epoch": 0.24039700953854087,
        "step": 1865
    },
    {
        "loss": 2.133,
        "grad_norm": 1.9555609226226807,
        "learning_rate": 1.8473320923297527e-05,
        "epoch": 0.24052590873936583,
        "step": 1866
    },
    {
        "loss": 1.6584,
        "grad_norm": 3.1048710346221924,
        "learning_rate": 1.842610708029921e-05,
        "epoch": 0.24065480794019076,
        "step": 1867
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.2920403480529785,
        "learning_rate": 1.8378940016395773e-05,
        "epoch": 0.2407837071410157,
        "step": 1868
    },
    {
        "loss": 1.9785,
        "grad_norm": 1.6982421875,
        "learning_rate": 1.8331819801468763e-05,
        "epoch": 0.24091260634184067,
        "step": 1869
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.1790244579315186,
        "learning_rate": 1.828474650533033e-05,
        "epoch": 0.24104150554266562,
        "step": 1870
    },
    {
        "loss": 2.0783,
        "grad_norm": 2.0312187671661377,
        "learning_rate": 1.8237720197723075e-05,
        "epoch": 0.24117040474349058,
        "step": 1871
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.5875887870788574,
        "learning_rate": 1.8190740948320017e-05,
        "epoch": 0.24129930394431554,
        "step": 1872
    },
    {
        "loss": 2.0153,
        "grad_norm": 1.9594042301177979,
        "learning_rate": 1.8143808826724468e-05,
        "epoch": 0.2414282031451405,
        "step": 1873
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.2009286880493164,
        "learning_rate": 1.8096923902469847e-05,
        "epoch": 0.24155710234596545,
        "step": 1874
    },
    {
        "loss": 1.7804,
        "grad_norm": 2.376262664794922,
        "learning_rate": 1.805008624501972e-05,
        "epoch": 0.2416860015467904,
        "step": 1875
    },
    {
        "loss": 1.5712,
        "grad_norm": 2.8880159854888916,
        "learning_rate": 1.8003295923767587e-05,
        "epoch": 0.24181490074761536,
        "step": 1876
    },
    {
        "loss": 2.0351,
        "grad_norm": 1.3723700046539307,
        "learning_rate": 1.7956553008036854e-05,
        "epoch": 0.2419437999484403,
        "step": 1877
    },
    {
        "loss": 1.555,
        "grad_norm": 1.5238780975341797,
        "learning_rate": 1.7909857567080618e-05,
        "epoch": 0.24207269914926527,
        "step": 1878
    },
    {
        "loss": 2.2767,
        "grad_norm": 2.9152493476867676,
        "learning_rate": 1.7863209670081703e-05,
        "epoch": 0.24220159835009022,
        "step": 1879
    },
    {
        "loss": 2.4245,
        "grad_norm": 2.376991033554077,
        "learning_rate": 1.7816609386152478e-05,
        "epoch": 0.24233049755091518,
        "step": 1880
    },
    {
        "loss": 2.2,
        "grad_norm": 1.5140783786773682,
        "learning_rate": 1.7770056784334776e-05,
        "epoch": 0.24245939675174014,
        "step": 1881
    },
    {
        "loss": 2.5546,
        "grad_norm": 1.4515421390533447,
        "learning_rate": 1.772355193359975e-05,
        "epoch": 0.2425882959525651,
        "step": 1882
    },
    {
        "loss": 2.1707,
        "grad_norm": 2.4907896518707275,
        "learning_rate": 1.7677094902847835e-05,
        "epoch": 0.24271719515339005,
        "step": 1883
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.5403101444244385,
        "learning_rate": 1.7630685760908622e-05,
        "epoch": 0.242846094354215,
        "step": 1884
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.427995204925537,
        "learning_rate": 1.758432457654074e-05,
        "epoch": 0.24297499355503996,
        "step": 1885
    },
    {
        "loss": 1.5017,
        "grad_norm": 2.225147247314453,
        "learning_rate": 1.7538011418431783e-05,
        "epoch": 0.24310389275586491,
        "step": 1886
    },
    {
        "loss": 2.1263,
        "grad_norm": 2.212392807006836,
        "learning_rate": 1.749174635519814e-05,
        "epoch": 0.24323279195668987,
        "step": 1887
    },
    {
        "loss": 1.997,
        "grad_norm": 1.8609354496002197,
        "learning_rate": 1.7445529455384994e-05,
        "epoch": 0.24336169115751483,
        "step": 1888
    },
    {
        "loss": 1.5128,
        "grad_norm": 3.696345090866089,
        "learning_rate": 1.7399360787466184e-05,
        "epoch": 0.24349059035833978,
        "step": 1889
    },
    {
        "loss": 2.0402,
        "grad_norm": 1.5352574586868286,
        "learning_rate": 1.7353240419844014e-05,
        "epoch": 0.24361948955916474,
        "step": 1890
    },
    {
        "loss": 1.8982,
        "grad_norm": 2.162858247756958,
        "learning_rate": 1.730716842084928e-05,
        "epoch": 0.2437483887599897,
        "step": 1891
    },
    {
        "loss": 1.7227,
        "grad_norm": 2.273451566696167,
        "learning_rate": 1.726114485874116e-05,
        "epoch": 0.24387728796081465,
        "step": 1892
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.0535122156143188,
        "learning_rate": 1.721516980170698e-05,
        "epoch": 0.2440061871616396,
        "step": 1893
    },
    {
        "loss": 2.1246,
        "grad_norm": 2.3517813682556152,
        "learning_rate": 1.7169243317862215e-05,
        "epoch": 0.24413508636246456,
        "step": 1894
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.6496785879135132,
        "learning_rate": 1.7123365475250468e-05,
        "epoch": 0.24426398556328952,
        "step": 1895
    },
    {
        "loss": 2.2084,
        "grad_norm": 2.02834415435791,
        "learning_rate": 1.7077536341843164e-05,
        "epoch": 0.24439288476411447,
        "step": 1896
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.219465494155884,
        "learning_rate": 1.7031755985539614e-05,
        "epoch": 0.24452178396493943,
        "step": 1897
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.1752327680587769,
        "learning_rate": 1.6986024474166877e-05,
        "epoch": 0.24465068316576438,
        "step": 1898
    },
    {
        "loss": 2.6105,
        "grad_norm": 2.4255518913269043,
        "learning_rate": 1.6940341875479593e-05,
        "epoch": 0.24477958236658934,
        "step": 1899
    },
    {
        "loss": 1.9407,
        "grad_norm": 2.5993595123291016,
        "learning_rate": 1.689470825715998e-05,
        "epoch": 0.2449084815674143,
        "step": 1900
    },
    {
        "loss": 1.538,
        "grad_norm": 2.7005510330200195,
        "learning_rate": 1.6849123686817676e-05,
        "epoch": 0.24503738076823925,
        "step": 1901
    },
    {
        "loss": 1.7146,
        "grad_norm": 2.465581178665161,
        "learning_rate": 1.6803588231989653e-05,
        "epoch": 0.2451662799690642,
        "step": 1902
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.4338467121124268,
        "learning_rate": 1.6758101960140087e-05,
        "epoch": 0.24529517916988916,
        "step": 1903
    },
    {
        "loss": 2.0332,
        "grad_norm": 1.3629472255706787,
        "learning_rate": 1.6712664938660318e-05,
        "epoch": 0.2454240783707141,
        "step": 1904
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.5749127864837646,
        "learning_rate": 1.6667277234868727e-05,
        "epoch": 0.24555297757153904,
        "step": 1905
    },
    {
        "loss": 1.9624,
        "grad_norm": 1.4943963289260864,
        "learning_rate": 1.6621938916010572e-05,
        "epoch": 0.245681876772364,
        "step": 1906
    },
    {
        "loss": 1.9244,
        "grad_norm": 2.2323782444000244,
        "learning_rate": 1.6576650049257998e-05,
        "epoch": 0.24581077597318896,
        "step": 1907
    },
    {
        "loss": 2.3021,
        "grad_norm": 1.9167722463607788,
        "learning_rate": 1.6531410701709866e-05,
        "epoch": 0.2459396751740139,
        "step": 1908
    },
    {
        "loss": 2.2418,
        "grad_norm": 2.021099805831909,
        "learning_rate": 1.6486220940391678e-05,
        "epoch": 0.24606857437483887,
        "step": 1909
    },
    {
        "loss": 2.3153,
        "grad_norm": 1.3680083751678467,
        "learning_rate": 1.6441080832255434e-05,
        "epoch": 0.24619747357566382,
        "step": 1910
    },
    {
        "loss": 1.8137,
        "grad_norm": 2.076482057571411,
        "learning_rate": 1.6395990444179615e-05,
        "epoch": 0.24632637277648878,
        "step": 1911
    },
    {
        "loss": 2.066,
        "grad_norm": 2.202934980392456,
        "learning_rate": 1.635094984296901e-05,
        "epoch": 0.24645527197731373,
        "step": 1912
    },
    {
        "loss": 1.5675,
        "grad_norm": 1.7590458393096924,
        "learning_rate": 1.6305959095354684e-05,
        "epoch": 0.2465841711781387,
        "step": 1913
    },
    {
        "loss": 1.7701,
        "grad_norm": 2.243663787841797,
        "learning_rate": 1.6261018267993745e-05,
        "epoch": 0.24671307037896364,
        "step": 1914
    },
    {
        "loss": 2.3602,
        "grad_norm": 2.0954203605651855,
        "learning_rate": 1.6216127427469484e-05,
        "epoch": 0.2468419695797886,
        "step": 1915
    },
    {
        "loss": 2.4026,
        "grad_norm": 2.1070470809936523,
        "learning_rate": 1.6171286640291e-05,
        "epoch": 0.24697086878061356,
        "step": 1916
    },
    {
        "loss": 2.5926,
        "grad_norm": 2.125016927719116,
        "learning_rate": 1.61264959728933e-05,
        "epoch": 0.2470997679814385,
        "step": 1917
    },
    {
        "loss": 2.2271,
        "grad_norm": 2.0454013347625732,
        "learning_rate": 1.6081755491637153e-05,
        "epoch": 0.24722866718226347,
        "step": 1918
    },
    {
        "loss": 2.687,
        "grad_norm": 1.5975115299224854,
        "learning_rate": 1.60370652628089e-05,
        "epoch": 0.24735756638308842,
        "step": 1919
    },
    {
        "loss": 2.6202,
        "grad_norm": 1.5170480012893677,
        "learning_rate": 1.5992425352620478e-05,
        "epoch": 0.24748646558391338,
        "step": 1920
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.787777066230774,
        "learning_rate": 1.594783582720933e-05,
        "epoch": 0.24761536478473833,
        "step": 1921
    },
    {
        "loss": 2.4559,
        "grad_norm": 2.4057726860046387,
        "learning_rate": 1.5903296752638108e-05,
        "epoch": 0.2477442639855633,
        "step": 1922
    },
    {
        "loss": 1.9232,
        "grad_norm": 1.3222379684448242,
        "learning_rate": 1.585880819489482e-05,
        "epoch": 0.24787316318638825,
        "step": 1923
    },
    {
        "loss": 2.1087,
        "grad_norm": 2.345371723175049,
        "learning_rate": 1.581437021989265e-05,
        "epoch": 0.2480020623872132,
        "step": 1924
    },
    {
        "loss": 2.407,
        "grad_norm": 1.793603539466858,
        "learning_rate": 1.5769982893469764e-05,
        "epoch": 0.24813096158803816,
        "step": 1925
    },
    {
        "loss": 2.0039,
        "grad_norm": 1.721114993095398,
        "learning_rate": 1.5725646281389288e-05,
        "epoch": 0.2482598607888631,
        "step": 1926
    },
    {
        "loss": 2.1065,
        "grad_norm": 2.582533359527588,
        "learning_rate": 1.5681360449339316e-05,
        "epoch": 0.24838875998968807,
        "step": 1927
    },
    {
        "loss": 1.6518,
        "grad_norm": 3.386704921722412,
        "learning_rate": 1.56371254629326e-05,
        "epoch": 0.24851765919051302,
        "step": 1928
    },
    {
        "loss": 2.009,
        "grad_norm": 2.274874210357666,
        "learning_rate": 1.559294138770656e-05,
        "epoch": 0.24864655839133798,
        "step": 1929
    },
    {
        "loss": 2.3475,
        "grad_norm": 2.0460400581359863,
        "learning_rate": 1.5548808289123273e-05,
        "epoch": 0.24877545759216294,
        "step": 1930
    },
    {
        "loss": 1.5826,
        "grad_norm": 2.2399027347564697,
        "learning_rate": 1.550472623256925e-05,
        "epoch": 0.2489043567929879,
        "step": 1931
    },
    {
        "loss": 1.9227,
        "grad_norm": 2.242027759552002,
        "learning_rate": 1.5460695283355325e-05,
        "epoch": 0.24903325599381285,
        "step": 1932
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.4340171813964844,
        "learning_rate": 1.5416715506716683e-05,
        "epoch": 0.2491621551946378,
        "step": 1933
    },
    {
        "loss": 2.2306,
        "grad_norm": 1.266737461090088,
        "learning_rate": 1.537278696781268e-05,
        "epoch": 0.24929105439546276,
        "step": 1934
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.25223708152771,
        "learning_rate": 1.5328909731726714e-05,
        "epoch": 0.2494199535962877,
        "step": 1935
    },
    {
        "loss": 2.4009,
        "grad_norm": 1.9113572835922241,
        "learning_rate": 1.528508386346623e-05,
        "epoch": 0.24954885279711267,
        "step": 1936
    },
    {
        "loss": 2.5525,
        "grad_norm": 1.7057369947433472,
        "learning_rate": 1.5241309427962535e-05,
        "epoch": 0.24967775199793762,
        "step": 1937
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.5987436771392822,
        "learning_rate": 1.519758649007077e-05,
        "epoch": 0.24980665119876258,
        "step": 1938
    },
    {
        "loss": 2.1527,
        "grad_norm": 2.0151779651641846,
        "learning_rate": 1.5153915114569717e-05,
        "epoch": 0.24993555039958754,
        "step": 1939
    },
    {
        "loss": 2.3081,
        "grad_norm": 1.912975788116455,
        "learning_rate": 1.5110295366161814e-05,
        "epoch": 0.25006444960041246,
        "step": 1940
    },
    {
        "loss": 2.2991,
        "grad_norm": 1.9715282917022705,
        "learning_rate": 1.5066727309473011e-05,
        "epoch": 0.25019334880123745,
        "step": 1941
    },
    {
        "loss": 2.0083,
        "grad_norm": 1.8224799633026123,
        "learning_rate": 1.5023211009052635e-05,
        "epoch": 0.2503222480020624,
        "step": 1942
    },
    {
        "loss": 2.3114,
        "grad_norm": 2.4924633502960205,
        "learning_rate": 1.4979746529373345e-05,
        "epoch": 0.25045114720288736,
        "step": 1943
    },
    {
        "loss": 1.9383,
        "grad_norm": 2.3434247970581055,
        "learning_rate": 1.4936333934831099e-05,
        "epoch": 0.2505800464037123,
        "step": 1944
    },
    {
        "loss": 2.2822,
        "grad_norm": 1.7319923639297485,
        "learning_rate": 1.4892973289744844e-05,
        "epoch": 0.25070894560453727,
        "step": 1945
    },
    {
        "loss": 1.655,
        "grad_norm": 2.828312873840332,
        "learning_rate": 1.4849664658356644e-05,
        "epoch": 0.2508378448053622,
        "step": 1946
    },
    {
        "loss": 2.1865,
        "grad_norm": 1.1983966827392578,
        "learning_rate": 1.4806408104831536e-05,
        "epoch": 0.2509667440061872,
        "step": 1947
    },
    {
        "loss": 1.9535,
        "grad_norm": 1.917644739151001,
        "learning_rate": 1.4763203693257339e-05,
        "epoch": 0.2510956432070121,
        "step": 1948
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.3672780990600586,
        "learning_rate": 1.4720051487644582e-05,
        "epoch": 0.2512245424078371,
        "step": 1949
    },
    {
        "loss": 1.7959,
        "grad_norm": 3.0071299076080322,
        "learning_rate": 1.4676951551926576e-05,
        "epoch": 0.251353441608662,
        "step": 1950
    },
    {
        "loss": 2.0913,
        "grad_norm": 2.1182940006256104,
        "learning_rate": 1.4633903949959077e-05,
        "epoch": 0.251482340809487,
        "step": 1951
    },
    {
        "loss": 1.8485,
        "grad_norm": 2.0353362560272217,
        "learning_rate": 1.4590908745520304e-05,
        "epoch": 0.25161124001031193,
        "step": 1952
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.795414924621582,
        "learning_rate": 1.4547966002310964e-05,
        "epoch": 0.2517401392111369,
        "step": 1953
    },
    {
        "loss": 2.1296,
        "grad_norm": 2.459179162979126,
        "learning_rate": 1.4505075783953925e-05,
        "epoch": 0.25186903841196184,
        "step": 1954
    },
    {
        "loss": 1.6547,
        "grad_norm": 2.610830783843994,
        "learning_rate": 1.4462238153994223e-05,
        "epoch": 0.2519979376127868,
        "step": 1955
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.8998596668243408,
        "learning_rate": 1.44194531758991e-05,
        "epoch": 0.25212683681361175,
        "step": 1956
    },
    {
        "loss": 1.6913,
        "grad_norm": 2.43105149269104,
        "learning_rate": 1.4376720913057662e-05,
        "epoch": 0.25225573601443674,
        "step": 1957
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.9894136190414429,
        "learning_rate": 1.4334041428780986e-05,
        "epoch": 0.25238463521526167,
        "step": 1958
    },
    {
        "loss": 2.2678,
        "grad_norm": 2.8655216693878174,
        "learning_rate": 1.4291414786301949e-05,
        "epoch": 0.2525135344160866,
        "step": 1959
    },
    {
        "loss": 1.5496,
        "grad_norm": 2.384272813796997,
        "learning_rate": 1.4248841048775097e-05,
        "epoch": 0.2526424336169116,
        "step": 1960
    },
    {
        "loss": 1.5392,
        "grad_norm": 2.770087480545044,
        "learning_rate": 1.420632027927663e-05,
        "epoch": 0.2527713328177365,
        "step": 1961
    },
    {
        "loss": 1.9116,
        "grad_norm": 2.8627400398254395,
        "learning_rate": 1.4163852540804284e-05,
        "epoch": 0.2529002320185615,
        "step": 1962
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.6323046684265137,
        "learning_rate": 1.4121437896277174e-05,
        "epoch": 0.2530291312193864,
        "step": 1963
    },
    {
        "loss": 1.8169,
        "grad_norm": 2.4541194438934326,
        "learning_rate": 1.4079076408535796e-05,
        "epoch": 0.2531580304202114,
        "step": 1964
    },
    {
        "loss": 2.0796,
        "grad_norm": 2.3347551822662354,
        "learning_rate": 1.4036768140341877e-05,
        "epoch": 0.2532869296210363,
        "step": 1965
    },
    {
        "loss": 2.3333,
        "grad_norm": 1.6103485822677612,
        "learning_rate": 1.3994513154378297e-05,
        "epoch": 0.2534158288218613,
        "step": 1966
    },
    {
        "loss": 2.4176,
        "grad_norm": 2.112840175628662,
        "learning_rate": 1.3952311513249012e-05,
        "epoch": 0.25354472802268624,
        "step": 1967
    },
    {
        "loss": 2.0294,
        "grad_norm": 1.9469552040100098,
        "learning_rate": 1.3910163279478888e-05,
        "epoch": 0.2536736272235112,
        "step": 1968
    },
    {
        "loss": 2.0955,
        "grad_norm": 2.147451400756836,
        "learning_rate": 1.3868068515513716e-05,
        "epoch": 0.25380252642433615,
        "step": 1969
    },
    {
        "loss": 2.2811,
        "grad_norm": 1.413415789604187,
        "learning_rate": 1.3826027283720067e-05,
        "epoch": 0.25393142562516113,
        "step": 1970
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.2316498756408691,
        "learning_rate": 1.3784039646385156e-05,
        "epoch": 0.25406032482598606,
        "step": 1971
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.9757286310195923,
        "learning_rate": 1.3742105665716836e-05,
        "epoch": 0.25418922402681104,
        "step": 1972
    },
    {
        "loss": 2.3436,
        "grad_norm": 1.4248125553131104,
        "learning_rate": 1.3700225403843459e-05,
        "epoch": 0.254318123227636,
        "step": 1973
    },
    {
        "loss": 2.5095,
        "grad_norm": 1.7113951444625854,
        "learning_rate": 1.365839892281378e-05,
        "epoch": 0.25444702242846096,
        "step": 1974
    },
    {
        "loss": 1.8254,
        "grad_norm": 2.643608331680298,
        "learning_rate": 1.3616626284596857e-05,
        "epoch": 0.2545759216292859,
        "step": 1975
    },
    {
        "loss": 1.7921,
        "grad_norm": 2.427335262298584,
        "learning_rate": 1.3574907551082e-05,
        "epoch": 0.25470482083011087,
        "step": 1976
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.053006887435913,
        "learning_rate": 1.3533242784078664e-05,
        "epoch": 0.2548337200309358,
        "step": 1977
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.4863313436508179,
        "learning_rate": 1.3491632045316304e-05,
        "epoch": 0.2549626192317608,
        "step": 1978
    },
    {
        "loss": 1.7435,
        "grad_norm": 2.437004804611206,
        "learning_rate": 1.3450075396444372e-05,
        "epoch": 0.2550915184325857,
        "step": 1979
    },
    {
        "loss": 2.5301,
        "grad_norm": 2.1169848442077637,
        "learning_rate": 1.3408572899032179e-05,
        "epoch": 0.2552204176334107,
        "step": 1980
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.9291032552719116,
        "learning_rate": 1.3367124614568728e-05,
        "epoch": 0.2553493168342356,
        "step": 1981
    },
    {
        "loss": 2.2588,
        "grad_norm": 1.6690056324005127,
        "learning_rate": 1.3325730604462866e-05,
        "epoch": 0.2554782160350606,
        "step": 1982
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.407379388809204,
        "learning_rate": 1.3284390930042895e-05,
        "epoch": 0.25560711523588553,
        "step": 1983
    },
    {
        "loss": 2.5962,
        "grad_norm": 2.007106304168701,
        "learning_rate": 1.3243105652556575e-05,
        "epoch": 0.2557360144367105,
        "step": 1984
    },
    {
        "loss": 1.2627,
        "grad_norm": 2.954591751098633,
        "learning_rate": 1.3201874833171252e-05,
        "epoch": 0.25586491363753544,
        "step": 1985
    },
    {
        "loss": 1.3928,
        "grad_norm": 2.837822675704956,
        "learning_rate": 1.316069853297343e-05,
        "epoch": 0.2559938128383604,
        "step": 1986
    },
    {
        "loss": 1.7207,
        "grad_norm": 2.8509583473205566,
        "learning_rate": 1.3119576812968882e-05,
        "epoch": 0.25612271203918535,
        "step": 1987
    },
    {
        "loss": 2.07,
        "grad_norm": 1.3470348119735718,
        "learning_rate": 1.307850973408256e-05,
        "epoch": 0.25625161124001034,
        "step": 1988
    },
    {
        "loss": 2.2591,
        "grad_norm": 1.6622281074523926,
        "learning_rate": 1.3037497357158407e-05,
        "epoch": 0.25638051044083526,
        "step": 1989
    },
    {
        "loss": 2.2662,
        "grad_norm": 1.9292672872543335,
        "learning_rate": 1.299653974295934e-05,
        "epoch": 0.25650940964166025,
        "step": 1990
    },
    {
        "loss": 2.1582,
        "grad_norm": 2.340461254119873,
        "learning_rate": 1.295563695216716e-05,
        "epoch": 0.2566383088424852,
        "step": 1991
    },
    {
        "loss": 2.1948,
        "grad_norm": 2.0423336029052734,
        "learning_rate": 1.2914789045382403e-05,
        "epoch": 0.25676720804331016,
        "step": 1992
    },
    {
        "loss": 1.5021,
        "grad_norm": 3.190051794052124,
        "learning_rate": 1.2873996083124312e-05,
        "epoch": 0.2568961072441351,
        "step": 1993
    },
    {
        "loss": 1.6575,
        "grad_norm": 2.146022081375122,
        "learning_rate": 1.2833258125830756e-05,
        "epoch": 0.25702500644496,
        "step": 1994
    },
    {
        "loss": 1.9203,
        "grad_norm": 1.3605155944824219,
        "learning_rate": 1.2792575233858028e-05,
        "epoch": 0.257153905645785,
        "step": 1995
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.0380163192749023,
        "learning_rate": 1.275194746748095e-05,
        "epoch": 0.2572828048466099,
        "step": 1996
    },
    {
        "loss": 1.3468,
        "grad_norm": 2.9561166763305664,
        "learning_rate": 1.2711374886892568e-05,
        "epoch": 0.2574117040474349,
        "step": 1997
    },
    {
        "loss": 2.5464,
        "grad_norm": 1.44639253616333,
        "learning_rate": 1.2670857552204219e-05,
        "epoch": 0.25754060324825984,
        "step": 1998
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.4366741180419922,
        "learning_rate": 1.2630395523445399e-05,
        "epoch": 0.2576695024490848,
        "step": 1999
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.322736382484436,
        "learning_rate": 1.2589988860563611e-05,
        "epoch": 0.25779840164990975,
        "step": 2000
    },
    {
        "loss": 2.3224,
        "grad_norm": 1.8231438398361206,
        "learning_rate": 1.2549637623424376e-05,
        "epoch": 0.25792730085073473,
        "step": 2001
    },
    {
        "loss": 1.7058,
        "grad_norm": 2.354914665222168,
        "learning_rate": 1.2509341871811103e-05,
        "epoch": 0.25805620005155966,
        "step": 2002
    },
    {
        "loss": 2.1744,
        "grad_norm": 1.656011700630188,
        "learning_rate": 1.2469101665424948e-05,
        "epoch": 0.25818509925238464,
        "step": 2003
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.0485188961029053,
        "learning_rate": 1.2428917063884804e-05,
        "epoch": 0.25831399845320957,
        "step": 2004
    },
    {
        "loss": 2.2022,
        "grad_norm": 1.577199101448059,
        "learning_rate": 1.238878812672719e-05,
        "epoch": 0.25844289765403455,
        "step": 2005
    },
    {
        "loss": 2.1151,
        "grad_norm": 2.1694681644439697,
        "learning_rate": 1.234871491340615e-05,
        "epoch": 0.2585717968548595,
        "step": 2006
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.4457051753997803,
        "learning_rate": 1.230869748329313e-05,
        "epoch": 0.25870069605568446,
        "step": 2007
    },
    {
        "loss": 2.188,
        "grad_norm": 2.7608790397644043,
        "learning_rate": 1.2268735895676981e-05,
        "epoch": 0.2588295952565094,
        "step": 2008
    },
    {
        "loss": 2.1168,
        "grad_norm": 2.752492904663086,
        "learning_rate": 1.2228830209763809e-05,
        "epoch": 0.2589584944573344,
        "step": 2009
    },
    {
        "loss": 2.2908,
        "grad_norm": 1.4314765930175781,
        "learning_rate": 1.2188980484676853e-05,
        "epoch": 0.2590873936581593,
        "step": 2010
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.9205719232559204,
        "learning_rate": 1.2149186779456478e-05,
        "epoch": 0.2592162928589843,
        "step": 2011
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.728754997253418,
        "learning_rate": 1.2109449153060114e-05,
        "epoch": 0.2593451920598092,
        "step": 2012
    },
    {
        "loss": 2.538,
        "grad_norm": 1.4145981073379517,
        "learning_rate": 1.2069767664361926e-05,
        "epoch": 0.2594740912606342,
        "step": 2013
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.3466699123382568,
        "learning_rate": 1.2030142372153131e-05,
        "epoch": 0.2596029904614591,
        "step": 2014
    },
    {
        "loss": 1.461,
        "grad_norm": 1.5919842720031738,
        "learning_rate": 1.1990573335141531e-05,
        "epoch": 0.2597318896622841,
        "step": 2015
    },
    {
        "loss": 2.2211,
        "grad_norm": 2.170685291290283,
        "learning_rate": 1.1951060611951603e-05,
        "epoch": 0.25986078886310904,
        "step": 2016
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.717584252357483,
        "learning_rate": 1.1911604261124464e-05,
        "epoch": 0.259989688063934,
        "step": 2017
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.10471510887146,
        "learning_rate": 1.1872204341117643e-05,
        "epoch": 0.26011858726475895,
        "step": 2018
    },
    {
        "loss": 2.3125,
        "grad_norm": 1.7306199073791504,
        "learning_rate": 1.1832860910305048e-05,
        "epoch": 0.26024748646558393,
        "step": 2019
    },
    {
        "loss": 2.1526,
        "grad_norm": 1.9043382406234741,
        "learning_rate": 1.1793574026976989e-05,
        "epoch": 0.26037638566640886,
        "step": 2020
    },
    {
        "loss": 2.0021,
        "grad_norm": 2.2959108352661133,
        "learning_rate": 1.1754343749339902e-05,
        "epoch": 0.26050528486723384,
        "step": 2021
    },
    {
        "loss": 2.1072,
        "grad_norm": 1.994956374168396,
        "learning_rate": 1.1715170135516363e-05,
        "epoch": 0.26063418406805877,
        "step": 2022
    },
    {
        "loss": 2.2599,
        "grad_norm": 2.244579315185547,
        "learning_rate": 1.1676053243545088e-05,
        "epoch": 0.26076308326888376,
        "step": 2023
    },
    {
        "loss": 1.6921,
        "grad_norm": 2.112623929977417,
        "learning_rate": 1.163699313138063e-05,
        "epoch": 0.2608919824697087,
        "step": 2024
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.7350351810455322,
        "learning_rate": 1.1597989856893537e-05,
        "epoch": 0.26102088167053367,
        "step": 2025
    },
    {
        "loss": 1.8916,
        "grad_norm": 2.3107705116271973,
        "learning_rate": 1.1559043477870068e-05,
        "epoch": 0.2611497808713586,
        "step": 2026
    },
    {
        "loss": 2.0909,
        "grad_norm": 1.8256137371063232,
        "learning_rate": 1.1520154052012173e-05,
        "epoch": 0.2612786800721836,
        "step": 2027
    },
    {
        "loss": 2.1443,
        "grad_norm": 2.1127536296844482,
        "learning_rate": 1.148132163693752e-05,
        "epoch": 0.2614075792730085,
        "step": 2028
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.7533867359161377,
        "learning_rate": 1.1442546290179224e-05,
        "epoch": 0.2615364784738335,
        "step": 2029
    },
    {
        "loss": 2.1186,
        "grad_norm": 1.887571930885315,
        "learning_rate": 1.140382806918584e-05,
        "epoch": 0.2616653776746584,
        "step": 2030
    },
    {
        "loss": 2.0425,
        "grad_norm": 2.509403944015503,
        "learning_rate": 1.1365167031321383e-05,
        "epoch": 0.26179427687548334,
        "step": 2031
    },
    {
        "loss": 1.8102,
        "grad_norm": 2.406683921813965,
        "learning_rate": 1.132656323386504e-05,
        "epoch": 0.26192317607630833,
        "step": 2032
    },
    {
        "loss": 1.218,
        "grad_norm": 2.8424384593963623,
        "learning_rate": 1.1288016734011259e-05,
        "epoch": 0.26205207527713326,
        "step": 2033
    },
    {
        "loss": 2.4197,
        "grad_norm": 1.2048171758651733,
        "learning_rate": 1.1249527588869563e-05,
        "epoch": 0.26218097447795824,
        "step": 2034
    },
    {
        "loss": 2.3665,
        "grad_norm": 1.8965202569961548,
        "learning_rate": 1.1211095855464543e-05,
        "epoch": 0.26230987367878317,
        "step": 2035
    },
    {
        "loss": 2.2403,
        "grad_norm": 1.6283715963363647,
        "learning_rate": 1.1172721590735658e-05,
        "epoch": 0.26243877287960815,
        "step": 2036
    },
    {
        "loss": 1.4407,
        "grad_norm": 2.357131004333496,
        "learning_rate": 1.113440485153729e-05,
        "epoch": 0.2625676720804331,
        "step": 2037
    },
    {
        "loss": 2.2906,
        "grad_norm": 1.344947099685669,
        "learning_rate": 1.1096145694638566e-05,
        "epoch": 0.26269657128125806,
        "step": 2038
    },
    {
        "loss": 1.8444,
        "grad_norm": 2.2679059505462646,
        "learning_rate": 1.1057944176723289e-05,
        "epoch": 0.262825470482083,
        "step": 2039
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.9032915830612183,
        "learning_rate": 1.1019800354389875e-05,
        "epoch": 0.262954369682908,
        "step": 2040
    },
    {
        "loss": 2.2257,
        "grad_norm": 2.6535723209381104,
        "learning_rate": 1.0981714284151312e-05,
        "epoch": 0.2630832688837329,
        "step": 2041
    },
    {
        "loss": 2.1838,
        "grad_norm": 1.11125910282135,
        "learning_rate": 1.0943686022434913e-05,
        "epoch": 0.2632121680845579,
        "step": 2042
    },
    {
        "loss": 1.238,
        "grad_norm": 2.736785888671875,
        "learning_rate": 1.0905715625582413e-05,
        "epoch": 0.2633410672853828,
        "step": 2043
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.6611933708190918,
        "learning_rate": 1.0867803149849853e-05,
        "epoch": 0.2634699664862078,
        "step": 2044
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.5520031452178955,
        "learning_rate": 1.082994865140739e-05,
        "epoch": 0.2635988656870327,
        "step": 2045
    },
    {
        "loss": 1.39,
        "grad_norm": 1.8278473615646362,
        "learning_rate": 1.0792152186339271e-05,
        "epoch": 0.2637277648878577,
        "step": 2046
    },
    {
        "loss": 2.1825,
        "grad_norm": 1.9275989532470703,
        "learning_rate": 1.0754413810643865e-05,
        "epoch": 0.26385666408868264,
        "step": 2047
    },
    {
        "loss": 1.4727,
        "grad_norm": 2.8072612285614014,
        "learning_rate": 1.0716733580233351e-05,
        "epoch": 0.2639855632895076,
        "step": 2048
    },
    {
        "loss": 1.6765,
        "grad_norm": 3.3899264335632324,
        "learning_rate": 1.0679111550933874e-05,
        "epoch": 0.26411446249033255,
        "step": 2049
    },
    {
        "loss": 1.8239,
        "grad_norm": 2.1718380451202393,
        "learning_rate": 1.0641547778485273e-05,
        "epoch": 0.26424336169115753,
        "step": 2050
    },
    {
        "loss": 2.4334,
        "grad_norm": 1.951020359992981,
        "learning_rate": 1.0604042318541074e-05,
        "epoch": 0.26437226089198246,
        "step": 2051
    },
    {
        "loss": 2.1953,
        "grad_norm": 2.291409969329834,
        "learning_rate": 1.0566595226668486e-05,
        "epoch": 0.26450116009280744,
        "step": 2052
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.4494165182113647,
        "learning_rate": 1.0529206558348165e-05,
        "epoch": 0.26463005929363237,
        "step": 2053
    },
    {
        "loss": 2.2061,
        "grad_norm": 1.198861837387085,
        "learning_rate": 1.0491876368974202e-05,
        "epoch": 0.26475895849445735,
        "step": 2054
    },
    {
        "loss": 1.9257,
        "grad_norm": 2.2107441425323486,
        "learning_rate": 1.0454604713854127e-05,
        "epoch": 0.2648878576952823,
        "step": 2055
    },
    {
        "loss": 2.4628,
        "grad_norm": 1.7774522304534912,
        "learning_rate": 1.0417391648208658e-05,
        "epoch": 0.26501675689610726,
        "step": 2056
    },
    {
        "loss": 2.32,
        "grad_norm": 1.922199010848999,
        "learning_rate": 1.038023722717179e-05,
        "epoch": 0.2651456560969322,
        "step": 2057
    },
    {
        "loss": 2.7286,
        "grad_norm": 1.5363478660583496,
        "learning_rate": 1.0343141505790566e-05,
        "epoch": 0.2652745552977572,
        "step": 2058
    },
    {
        "loss": 2.4008,
        "grad_norm": 1.201757788658142,
        "learning_rate": 1.0306104539025058e-05,
        "epoch": 0.2654034544985821,
        "step": 2059
    },
    {
        "loss": 2.3084,
        "grad_norm": 1.7324130535125732,
        "learning_rate": 1.0269126381748373e-05,
        "epoch": 0.2655323536994071,
        "step": 2060
    },
    {
        "loss": 2.544,
        "grad_norm": 1.3802472352981567,
        "learning_rate": 1.0232207088746399e-05,
        "epoch": 0.265661252900232,
        "step": 2061
    },
    {
        "loss": 2.3497,
        "grad_norm": 1.5096989870071411,
        "learning_rate": 1.0195346714717813e-05,
        "epoch": 0.265790152101057,
        "step": 2062
    },
    {
        "loss": 2.1694,
        "grad_norm": 2.004239559173584,
        "learning_rate": 1.0158545314274071e-05,
        "epoch": 0.2659190513018819,
        "step": 2063
    },
    {
        "loss": 2.1602,
        "grad_norm": 1.6681959629058838,
        "learning_rate": 1.0121802941939212e-05,
        "epoch": 0.2660479505027069,
        "step": 2064
    },
    {
        "loss": 2.4294,
        "grad_norm": 1.2085269689559937,
        "learning_rate": 1.0085119652149805e-05,
        "epoch": 0.26617684970353184,
        "step": 2065
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.618188500404358,
        "learning_rate": 1.00484954992549e-05,
        "epoch": 0.2663057489043568,
        "step": 2066
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.6639286279678345,
        "learning_rate": 1.0011930537515951e-05,
        "epoch": 0.26643464810518175,
        "step": 2067
    },
    {
        "loss": 1.4056,
        "grad_norm": 1.6523089408874512,
        "learning_rate": 9.975424821106666e-06,
        "epoch": 0.2665635473060067,
        "step": 2068
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.440407156944275,
        "learning_rate": 9.938978404113032e-06,
        "epoch": 0.26669244650683166,
        "step": 2069
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.5822054147720337,
        "learning_rate": 9.902591340533152e-06,
        "epoch": 0.2668213457076566,
        "step": 2070
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.3705921173095703,
        "learning_rate": 9.866263684277177e-06,
        "epoch": 0.26695024490848157,
        "step": 2071
    },
    {
        "loss": 2.274,
        "grad_norm": 2.410072088241577,
        "learning_rate": 9.829995489167253e-06,
        "epoch": 0.2670791441093065,
        "step": 2072
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.6216450929641724,
        "learning_rate": 9.793786808937489e-06,
        "epoch": 0.2672080433101315,
        "step": 2073
    },
    {
        "loss": 1.8218,
        "grad_norm": 3.2424912452697754,
        "learning_rate": 9.757637697233723e-06,
        "epoch": 0.2673369425109564,
        "step": 2074
    },
    {
        "loss": 2.1167,
        "grad_norm": 1.24948251247406,
        "learning_rate": 9.721548207613567e-06,
        "epoch": 0.2674658417117814,
        "step": 2075
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.3241047859191895,
        "learning_rate": 9.68551839354636e-06,
        "epoch": 0.2675947409126063,
        "step": 2076
    },
    {
        "loss": 2.1481,
        "grad_norm": 1.3848572969436646,
        "learning_rate": 9.649548308412954e-06,
        "epoch": 0.2677236401134313,
        "step": 2077
    },
    {
        "loss": 2.1442,
        "grad_norm": 2.1717987060546875,
        "learning_rate": 9.613638005505698e-06,
        "epoch": 0.26785253931425623,
        "step": 2078
    },
    {
        "loss": 1.5472,
        "grad_norm": 2.7048017978668213,
        "learning_rate": 9.577787538028465e-06,
        "epoch": 0.2679814385150812,
        "step": 2079
    },
    {
        "loss": 2.3655,
        "grad_norm": 1.3184738159179688,
        "learning_rate": 9.54199695909636e-06,
        "epoch": 0.26811033771590614,
        "step": 2080
    },
    {
        "loss": 2.0575,
        "grad_norm": 2.8671252727508545,
        "learning_rate": 9.506266321735874e-06,
        "epoch": 0.2682392369167311,
        "step": 2081
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.678119421005249,
        "learning_rate": 9.470595678884603e-06,
        "epoch": 0.26836813611755606,
        "step": 2082
    },
    {
        "loss": 1.8578,
        "grad_norm": 2.8206801414489746,
        "learning_rate": 9.434985083391252e-06,
        "epoch": 0.26849703531838104,
        "step": 2083
    },
    {
        "loss": 2.0147,
        "grad_norm": 1.9494385719299316,
        "learning_rate": 9.399434588015649e-06,
        "epoch": 0.26862593451920597,
        "step": 2084
    },
    {
        "loss": 2.5454,
        "grad_norm": 1.3696705102920532,
        "learning_rate": 9.363944245428502e-06,
        "epoch": 0.26875483372003095,
        "step": 2085
    },
    {
        "loss": 1.8491,
        "grad_norm": 2.208836555480957,
        "learning_rate": 9.328514108211389e-06,
        "epoch": 0.2688837329208559,
        "step": 2086
    },
    {
        "loss": 2.1185,
        "grad_norm": 2.540181875228882,
        "learning_rate": 9.293144228856775e-06,
        "epoch": 0.26901263212168086,
        "step": 2087
    },
    {
        "loss": 2.0327,
        "grad_norm": 1.870859980583191,
        "learning_rate": 9.257834659767745e-06,
        "epoch": 0.2691415313225058,
        "step": 2088
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.997491478919983,
        "learning_rate": 9.22258545325813e-06,
        "epoch": 0.2692704305233308,
        "step": 2089
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.0536080598831177,
        "learning_rate": 9.18739666155225e-06,
        "epoch": 0.2693993297241557,
        "step": 2090
    },
    {
        "loss": 1.1491,
        "grad_norm": 2.1714446544647217,
        "learning_rate": 9.152268336784914e-06,
        "epoch": 0.2695282289249807,
        "step": 2091
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.2062604427337646,
        "learning_rate": 9.117200531001424e-06,
        "epoch": 0.2696571281258056,
        "step": 2092
    },
    {
        "loss": 2.5226,
        "grad_norm": 1.3821886777877808,
        "learning_rate": 9.08219329615737e-06,
        "epoch": 0.2697860273266306,
        "step": 2093
    },
    {
        "loss": 2.094,
        "grad_norm": 1.345922827720642,
        "learning_rate": 9.047246684118548e-06,
        "epoch": 0.2699149265274555,
        "step": 2094
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.7825018167495728,
        "learning_rate": 9.012360746661036e-06,
        "epoch": 0.2700438257282805,
        "step": 2095
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.3756977319717407,
        "learning_rate": 8.977535535470965e-06,
        "epoch": 0.27017272492910543,
        "step": 2096
    },
    {
        "loss": 2.1009,
        "grad_norm": 1.758914828300476,
        "learning_rate": 8.942771102144493e-06,
        "epoch": 0.2703016241299304,
        "step": 2097
    },
    {
        "loss": 1.9553,
        "grad_norm": 1.5413490533828735,
        "learning_rate": 8.908067498187734e-06,
        "epoch": 0.27043052333075535,
        "step": 2098
    },
    {
        "loss": 2.6867,
        "grad_norm": 1.4759873151779175,
        "learning_rate": 8.873424775016715e-06,
        "epoch": 0.27055942253158033,
        "step": 2099
    },
    {
        "loss": 2.5017,
        "grad_norm": 1.788420557975769,
        "learning_rate": 8.838842983957196e-06,
        "epoch": 0.27068832173240526,
        "step": 2100
    },
    {
        "loss": 1.7885,
        "grad_norm": 2.3699791431427,
        "learning_rate": 8.804322176244717e-06,
        "epoch": 0.27081722093323024,
        "step": 2101
    },
    {
        "loss": 2.2366,
        "grad_norm": 1.4475735425949097,
        "learning_rate": 8.769862403024443e-06,
        "epoch": 0.27094612013405517,
        "step": 2102
    },
    {
        "loss": 2.4141,
        "grad_norm": 2.6583778858184814,
        "learning_rate": 8.735463715351139e-06,
        "epoch": 0.2710750193348801,
        "step": 2103
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.5981147289276123,
        "learning_rate": 8.701126164189e-06,
        "epoch": 0.2712039185357051,
        "step": 2104
    },
    {
        "loss": 2.2769,
        "grad_norm": 2.395296096801758,
        "learning_rate": 8.666849800411748e-06,
        "epoch": 0.27133281773653,
        "step": 2105
    },
    {
        "loss": 2.2272,
        "grad_norm": 2.226245164871216,
        "learning_rate": 8.632634674802354e-06,
        "epoch": 0.271461716937355,
        "step": 2106
    },
    {
        "loss": 2.5814,
        "grad_norm": 1.2276182174682617,
        "learning_rate": 8.59848083805308e-06,
        "epoch": 0.2715906161381799,
        "step": 2107
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.5565178394317627,
        "learning_rate": 8.56438834076545e-06,
        "epoch": 0.2717195153390049,
        "step": 2108
    },
    {
        "loss": 1.7687,
        "grad_norm": 2.053466320037842,
        "learning_rate": 8.53035723345002e-06,
        "epoch": 0.27184841453982983,
        "step": 2109
    },
    {
        "loss": 2.1637,
        "grad_norm": 2.505260944366455,
        "learning_rate": 8.496387566526454e-06,
        "epoch": 0.2719773137406548,
        "step": 2110
    },
    {
        "loss": 2.4187,
        "grad_norm": 2.613107919692993,
        "learning_rate": 8.462479390323368e-06,
        "epoch": 0.27210621294147974,
        "step": 2111
    },
    {
        "loss": 2.1648,
        "grad_norm": 2.288940906524658,
        "learning_rate": 8.428632755078258e-06,
        "epoch": 0.2722351121423047,
        "step": 2112
    },
    {
        "loss": 2.2938,
        "grad_norm": 2.0368762016296387,
        "learning_rate": 8.39484771093746e-06,
        "epoch": 0.27236401134312965,
        "step": 2113
    },
    {
        "loss": 0.9193,
        "grad_norm": 2.610382080078125,
        "learning_rate": 8.361124307956075e-06,
        "epoch": 0.27249291054395464,
        "step": 2114
    },
    {
        "loss": 2.5338,
        "grad_norm": 1.3158386945724487,
        "learning_rate": 8.327462596097818e-06,
        "epoch": 0.27262180974477956,
        "step": 2115
    },
    {
        "loss": 1.9485,
        "grad_norm": 2.3607516288757324,
        "learning_rate": 8.293862625235093e-06,
        "epoch": 0.27275070894560455,
        "step": 2116
    },
    {
        "loss": 2.4483,
        "grad_norm": 1.9225634336471558,
        "learning_rate": 8.260324445148765e-06,
        "epoch": 0.2728796081464295,
        "step": 2117
    },
    {
        "loss": 2.091,
        "grad_norm": 2.5357613563537598,
        "learning_rate": 8.226848105528134e-06,
        "epoch": 0.27300850734725446,
        "step": 2118
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.955349326133728,
        "learning_rate": 8.193433655970967e-06,
        "epoch": 0.2731374065480794,
        "step": 2119
    },
    {
        "loss": 2.328,
        "grad_norm": 1.6493200063705444,
        "learning_rate": 8.16008114598325e-06,
        "epoch": 0.27326630574890437,
        "step": 2120
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.976675033569336,
        "learning_rate": 8.126790624979202e-06,
        "epoch": 0.2733952049497293,
        "step": 2121
    },
    {
        "loss": 2.0814,
        "grad_norm": 1.8621660470962524,
        "learning_rate": 8.093562142281319e-06,
        "epoch": 0.2735241041505543,
        "step": 2122
    },
    {
        "loss": 2.2753,
        "grad_norm": 1.3585203886032104,
        "learning_rate": 8.060395747120008e-06,
        "epoch": 0.2736530033513792,
        "step": 2123
    },
    {
        "loss": 1.8178,
        "grad_norm": 2.3436057567596436,
        "learning_rate": 8.027291488633821e-06,
        "epoch": 0.2737819025522042,
        "step": 2124
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.9775314331054688,
        "learning_rate": 7.994249415869209e-06,
        "epoch": 0.2739108017530291,
        "step": 2125
    },
    {
        "loss": 2.275,
        "grad_norm": 1.631764531135559,
        "learning_rate": 7.961269577780457e-06,
        "epoch": 0.2740397009538541,
        "step": 2126
    },
    {
        "loss": 2.167,
        "grad_norm": 1.9728204011917114,
        "learning_rate": 7.928352023229684e-06,
        "epoch": 0.27416860015467903,
        "step": 2127
    },
    {
        "loss": 1.6397,
        "grad_norm": 1.750056505203247,
        "learning_rate": 7.89549680098674e-06,
        "epoch": 0.274297499355504,
        "step": 2128
    },
    {
        "loss": 2.2701,
        "grad_norm": 1.4062087535858154,
        "learning_rate": 7.862703959729078e-06,
        "epoch": 0.27442639855632894,
        "step": 2129
    },
    {
        "loss": 1.9748,
        "grad_norm": 2.1421985626220703,
        "learning_rate": 7.82997354804177e-06,
        "epoch": 0.2745552977571539,
        "step": 2130
    },
    {
        "loss": 2.7944,
        "grad_norm": 1.277188777923584,
        "learning_rate": 7.797305614417372e-06,
        "epoch": 0.27468419695797885,
        "step": 2131
    },
    {
        "loss": 2.4486,
        "grad_norm": 1.579761266708374,
        "learning_rate": 7.764700207255903e-06,
        "epoch": 0.27481309615880384,
        "step": 2132
    },
    {
        "loss": 1.7017,
        "grad_norm": 3.180718183517456,
        "learning_rate": 7.732157374864701e-06,
        "epoch": 0.27494199535962877,
        "step": 2133
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.5214215517044067,
        "learning_rate": 7.699677165458419e-06,
        "epoch": 0.27507089456045375,
        "step": 2134
    },
    {
        "loss": 2.2209,
        "grad_norm": 2.0676310062408447,
        "learning_rate": 7.667259627158952e-06,
        "epoch": 0.2751997937612787,
        "step": 2135
    },
    {
        "loss": 2.4991,
        "grad_norm": 1.3051244020462036,
        "learning_rate": 7.634904807995285e-06,
        "epoch": 0.27532869296210366,
        "step": 2136
    },
    {
        "loss": 1.4183,
        "grad_norm": 3.7092297077178955,
        "learning_rate": 7.602612755903537e-06,
        "epoch": 0.2754575921629286,
        "step": 2137
    },
    {
        "loss": 2.4345,
        "grad_norm": 1.5092283487319946,
        "learning_rate": 7.5703835187268094e-06,
        "epoch": 0.27558649136375357,
        "step": 2138
    },
    {
        "loss": 1.4955,
        "grad_norm": 2.5774664878845215,
        "learning_rate": 7.5382171442151096e-06,
        "epoch": 0.2757153905645785,
        "step": 2139
    },
    {
        "loss": 1.8992,
        "grad_norm": 2.2732841968536377,
        "learning_rate": 7.5061136800253895e-06,
        "epoch": 0.2758442897654034,
        "step": 2140
    },
    {
        "loss": 1.9392,
        "grad_norm": 1.749708890914917,
        "learning_rate": 7.474073173721297e-06,
        "epoch": 0.2759731889662284,
        "step": 2141
    },
    {
        "loss": 1.7005,
        "grad_norm": 2.806936502456665,
        "learning_rate": 7.442095672773275e-06,
        "epoch": 0.27610208816705334,
        "step": 2142
    },
    {
        "loss": 2.0377,
        "grad_norm": 1.7897040843963623,
        "learning_rate": 7.410181224558393e-06,
        "epoch": 0.2762309873678783,
        "step": 2143
    },
    {
        "loss": 2.3284,
        "grad_norm": 1.2036978006362915,
        "learning_rate": 7.37832987636029e-06,
        "epoch": 0.27635988656870325,
        "step": 2144
    },
    {
        "loss": 1.5962,
        "grad_norm": 2.638794422149658,
        "learning_rate": 7.34654167536914e-06,
        "epoch": 0.27648878576952823,
        "step": 2145
    },
    {
        "loss": 2.1069,
        "grad_norm": 1.8126775026321411,
        "learning_rate": 7.314816668681562e-06,
        "epoch": 0.27661768497035316,
        "step": 2146
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.7796401977539062,
        "learning_rate": 7.283154903300504e-06,
        "epoch": 0.27674658417117814,
        "step": 2147
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.1211607456207275,
        "learning_rate": 7.251556426135292e-06,
        "epoch": 0.2768754833720031,
        "step": 2148
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.4818474054336548,
        "learning_rate": 7.220021284001432e-06,
        "epoch": 0.27700438257282806,
        "step": 2149
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.75929594039917,
        "learning_rate": 7.1885495236205635e-06,
        "epoch": 0.277133281773653,
        "step": 2150
    },
    {
        "loss": 2.1843,
        "grad_norm": 2.3037400245666504,
        "learning_rate": 7.157141191620548e-06,
        "epoch": 0.27726218097447797,
        "step": 2151
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.0174641609191895,
        "learning_rate": 7.125796334535128e-06,
        "epoch": 0.2773910801753029,
        "step": 2152
    },
    {
        "loss": 1.4347,
        "grad_norm": 2.8741567134857178,
        "learning_rate": 7.094514998804047e-06,
        "epoch": 0.2775199793761279,
        "step": 2153
    },
    {
        "loss": 1.7072,
        "grad_norm": 2.360413074493408,
        "learning_rate": 7.063297230773019e-06,
        "epoch": 0.2776488785769528,
        "step": 2154
    },
    {
        "loss": 2.2894,
        "grad_norm": 1.3512941598892212,
        "learning_rate": 7.03214307669347e-06,
        "epoch": 0.2777777777777778,
        "step": 2155
    },
    {
        "loss": 2.0681,
        "grad_norm": 2.271967649459839,
        "learning_rate": 7.001052582722633e-06,
        "epoch": 0.2779066769786027,
        "step": 2156
    },
    {
        "loss": 2.4881,
        "grad_norm": 1.3510364294052124,
        "learning_rate": 6.970025794923418e-06,
        "epoch": 0.2780355761794277,
        "step": 2157
    },
    {
        "loss": 2.2757,
        "grad_norm": 1.5414042472839355,
        "learning_rate": 6.939062759264325e-06,
        "epoch": 0.27816447538025263,
        "step": 2158
    },
    {
        "loss": 0.9776,
        "grad_norm": 3.0719006061553955,
        "learning_rate": 6.908163521619426e-06,
        "epoch": 0.2782933745810776,
        "step": 2159
    },
    {
        "loss": 1.6199,
        "grad_norm": 2.2754385471343994,
        "learning_rate": 6.8773281277682736e-06,
        "epoch": 0.27842227378190254,
        "step": 2160
    },
    {
        "loss": 2.0682,
        "grad_norm": 2.34454083442688,
        "learning_rate": 6.8465566233957945e-06,
        "epoch": 0.2785511729827275,
        "step": 2161
    },
    {
        "loss": 2.3223,
        "grad_norm": 2.690279722213745,
        "learning_rate": 6.81584905409231e-06,
        "epoch": 0.27868007218355245,
        "step": 2162
    },
    {
        "loss": 1.7675,
        "grad_norm": 2.183198928833008,
        "learning_rate": 6.785205465353378e-06,
        "epoch": 0.27880897138437744,
        "step": 2163
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.771899700164795,
        "learning_rate": 6.754625902579792e-06,
        "epoch": 0.27893787058520236,
        "step": 2164
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.49308705329895,
        "learning_rate": 6.7241104110774555e-06,
        "epoch": 0.27906676978602735,
        "step": 2165
    },
    {
        "loss": 1.8829,
        "grad_norm": 1.7810214757919312,
        "learning_rate": 6.6936590360573645e-06,
        "epoch": 0.2791956689868523,
        "step": 2166
    },
    {
        "loss": 2.0848,
        "grad_norm": 2.0047991275787354,
        "learning_rate": 6.663271822635536e-06,
        "epoch": 0.27932456818767726,
        "step": 2167
    },
    {
        "loss": 2.2718,
        "grad_norm": 1.5670727491378784,
        "learning_rate": 6.63294881583289e-06,
        "epoch": 0.2794534673885022,
        "step": 2168
    },
    {
        "loss": 2.2141,
        "grad_norm": 2.531304359436035,
        "learning_rate": 6.602690060575245e-06,
        "epoch": 0.27958236658932717,
        "step": 2169
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.6254464387893677,
        "learning_rate": 6.572495601693223e-06,
        "epoch": 0.2797112657901521,
        "step": 2170
    },
    {
        "loss": 2.1499,
        "grad_norm": 2.645528793334961,
        "learning_rate": 6.542365483922186e-06,
        "epoch": 0.2798401649909771,
        "step": 2171
    },
    {
        "loss": 1.5446,
        "grad_norm": 2.769655704498291,
        "learning_rate": 6.512299751902173e-06,
        "epoch": 0.279969064191802,
        "step": 2172
    },
    {
        "loss": 1.9237,
        "grad_norm": 2.775224208831787,
        "learning_rate": 6.48229845017781e-06,
        "epoch": 0.280097963392627,
        "step": 2173
    },
    {
        "loss": 2.2089,
        "grad_norm": 2.012925148010254,
        "learning_rate": 6.452361623198283e-06,
        "epoch": 0.2802268625934519,
        "step": 2174
    },
    {
        "loss": 2.0538,
        "grad_norm": 2.131903886795044,
        "learning_rate": 6.422489315317271e-06,
        "epoch": 0.2803557617942769,
        "step": 2175
    },
    {
        "loss": 1.6596,
        "grad_norm": 2.1990623474121094,
        "learning_rate": 6.392681570792808e-06,
        "epoch": 0.28048466099510183,
        "step": 2176
    },
    {
        "loss": 2.3961,
        "grad_norm": 1.1378393173217773,
        "learning_rate": 6.362938433787335e-06,
        "epoch": 0.28061356019592676,
        "step": 2177
    },
    {
        "loss": 1.7391,
        "grad_norm": 2.4639158248901367,
        "learning_rate": 6.333259948367543e-06,
        "epoch": 0.28074245939675174,
        "step": 2178
    },
    {
        "loss": 2.0225,
        "grad_norm": 1.6431316137313843,
        "learning_rate": 6.303646158504295e-06,
        "epoch": 0.28087135859757667,
        "step": 2179
    },
    {
        "loss": 1.8281,
        "grad_norm": 2.1519904136657715,
        "learning_rate": 6.2740971080727295e-06,
        "epoch": 0.28100025779840165,
        "step": 2180
    },
    {
        "loss": 1.6426,
        "grad_norm": 2.774543046951294,
        "learning_rate": 6.2446128408519215e-06,
        "epoch": 0.2811291569992266,
        "step": 2181
    },
    {
        "loss": 2.2829,
        "grad_norm": 1.6644189357757568,
        "learning_rate": 6.215193400525016e-06,
        "epoch": 0.28125805620005156,
        "step": 2182
    },
    {
        "loss": 2.212,
        "grad_norm": 2.058560371398926,
        "learning_rate": 6.1858388306791855e-06,
        "epoch": 0.2813869554008765,
        "step": 2183
    },
    {
        "loss": 1.739,
        "grad_norm": 2.1911840438842773,
        "learning_rate": 6.156549174805404e-06,
        "epoch": 0.2815158546017015,
        "step": 2184
    },
    {
        "loss": 2.539,
        "grad_norm": 1.662915587425232,
        "learning_rate": 6.127324476298451e-06,
        "epoch": 0.2816447538025264,
        "step": 2185
    },
    {
        "loss": 2.3852,
        "grad_norm": 1.8285298347473145,
        "learning_rate": 6.098164778456988e-06,
        "epoch": 0.2817736530033514,
        "step": 2186
    },
    {
        "loss": 1.9773,
        "grad_norm": 2.113295316696167,
        "learning_rate": 6.069070124483251e-06,
        "epoch": 0.2819025522041763,
        "step": 2187
    },
    {
        "loss": 1.2571,
        "grad_norm": 2.5425262451171875,
        "learning_rate": 6.040040557483173e-06,
        "epoch": 0.2820314514050013,
        "step": 2188
    },
    {
        "loss": 2.0503,
        "grad_norm": 2.1771297454833984,
        "learning_rate": 6.011076120466241e-06,
        "epoch": 0.2821603506058262,
        "step": 2189
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.481713056564331,
        "learning_rate": 5.9821768563454336e-06,
        "epoch": 0.2822892498066512,
        "step": 2190
    },
    {
        "loss": 1.5058,
        "grad_norm": 3.1544768810272217,
        "learning_rate": 5.9533428079371846e-06,
        "epoch": 0.28241814900747614,
        "step": 2191
    },
    {
        "loss": 2.5925,
        "grad_norm": 1.6680620908737183,
        "learning_rate": 5.924574017961321e-06,
        "epoch": 0.2825470482083011,
        "step": 2192
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.1932041645050049,
        "learning_rate": 5.895870529040937e-06,
        "epoch": 0.28267594740912605,
        "step": 2193
    },
    {
        "loss": 1.0062,
        "grad_norm": 2.6619951725006104,
        "learning_rate": 5.8672323837024255e-06,
        "epoch": 0.28280484660995103,
        "step": 2194
    },
    {
        "loss": 2.1625,
        "grad_norm": 2.1667017936706543,
        "learning_rate": 5.838659624375348e-06,
        "epoch": 0.28293374581077596,
        "step": 2195
    },
    {
        "loss": 2.0523,
        "grad_norm": 2.3257803916931152,
        "learning_rate": 5.810152293392407e-06,
        "epoch": 0.28306264501160094,
        "step": 2196
    },
    {
        "loss": 1.8512,
        "grad_norm": 2.198760986328125,
        "learning_rate": 5.781710432989334e-06,
        "epoch": 0.28319154421242587,
        "step": 2197
    },
    {
        "loss": 2.0835,
        "grad_norm": 2.529343366622925,
        "learning_rate": 5.75333408530489e-06,
        "epoch": 0.28332044341325086,
        "step": 2198
    },
    {
        "loss": 1.6395,
        "grad_norm": 1.977555513381958,
        "learning_rate": 5.725023292380771e-06,
        "epoch": 0.2834493426140758,
        "step": 2199
    },
    {
        "loss": 2.2557,
        "grad_norm": 2.5826244354248047,
        "learning_rate": 5.696778096161553e-06,
        "epoch": 0.28357824181490077,
        "step": 2200
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.8114372491836548,
        "learning_rate": 5.668598538494591e-06,
        "epoch": 0.2837071410157257,
        "step": 2201
    },
    {
        "loss": 1.9012,
        "grad_norm": 1.9081014394760132,
        "learning_rate": 5.64048466113003e-06,
        "epoch": 0.2838360402165507,
        "step": 2202
    },
    {
        "loss": 1.7776,
        "grad_norm": 1.693832516670227,
        "learning_rate": 5.612436505720709e-06,
        "epoch": 0.2839649394173756,
        "step": 2203
    },
    {
        "loss": 2.404,
        "grad_norm": 1.9759379625320435,
        "learning_rate": 5.584454113822047e-06,
        "epoch": 0.2840938386182006,
        "step": 2204
    },
    {
        "loss": 1.7109,
        "grad_norm": 2.754467487335205,
        "learning_rate": 5.556537526892081e-06,
        "epoch": 0.2842227378190255,
        "step": 2205
    },
    {
        "loss": 1.7561,
        "grad_norm": 1.9340626001358032,
        "learning_rate": 5.52868678629132e-06,
        "epoch": 0.2843516370198505,
        "step": 2206
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.254415273666382,
        "learning_rate": 5.500901933282748e-06,
        "epoch": 0.28448053622067543,
        "step": 2207
    },
    {
        "loss": 2.679,
        "grad_norm": 1.6037355661392212,
        "learning_rate": 5.47318300903169e-06,
        "epoch": 0.2846094354215004,
        "step": 2208
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.1604249477386475,
        "learning_rate": 5.445530054605824e-06,
        "epoch": 0.28473833462232534,
        "step": 2209
    },
    {
        "loss": 2.2034,
        "grad_norm": 1.4592474699020386,
        "learning_rate": 5.4179431109750875e-06,
        "epoch": 0.2848672338231503,
        "step": 2210
    },
    {
        "loss": 2.4605,
        "grad_norm": 1.663613200187683,
        "learning_rate": 5.390422219011598e-06,
        "epoch": 0.28499613302397525,
        "step": 2211
    },
    {
        "loss": 2.4447,
        "grad_norm": 1.925406575202942,
        "learning_rate": 5.362967419489623e-06,
        "epoch": 0.2851250322248002,
        "step": 2212
    },
    {
        "loss": 1.7182,
        "grad_norm": 2.5757689476013184,
        "learning_rate": 5.335578753085546e-06,
        "epoch": 0.28525393142562516,
        "step": 2213
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.9595770835876465,
        "learning_rate": 5.308256260377664e-06,
        "epoch": 0.2853828306264501,
        "step": 2214
    },
    {
        "loss": 2.1949,
        "grad_norm": 1.7555201053619385,
        "learning_rate": 5.28099998184638e-06,
        "epoch": 0.2855117298272751,
        "step": 2215
    },
    {
        "loss": 2.4773,
        "grad_norm": 1.796432614326477,
        "learning_rate": 5.253809957873884e-06,
        "epoch": 0.2856406290281,
        "step": 2216
    },
    {
        "loss": 1.7982,
        "grad_norm": 3.8938939571380615,
        "learning_rate": 5.226686228744221e-06,
        "epoch": 0.285769528228925,
        "step": 2217
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.6119478940963745,
        "learning_rate": 5.199628834643272e-06,
        "epoch": 0.2858984274297499,
        "step": 2218
    },
    {
        "loss": 2.3955,
        "grad_norm": 2.6685421466827393,
        "learning_rate": 5.1726378156585816e-06,
        "epoch": 0.2860273266305749,
        "step": 2219
    },
    {
        "loss": 2.0531,
        "grad_norm": 2.120837450027466,
        "learning_rate": 5.145713211779352e-06,
        "epoch": 0.2861562258313998,
        "step": 2220
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.7365220785140991,
        "learning_rate": 5.118855062896444e-06,
        "epoch": 0.2862851250322248,
        "step": 2221
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.8443164825439453,
        "learning_rate": 5.0920634088022005e-06,
        "epoch": 0.28641402423304974,
        "step": 2222
    },
    {
        "loss": 2.0626,
        "grad_norm": 2.7307140827178955,
        "learning_rate": 5.065338289190475e-06,
        "epoch": 0.2865429234338747,
        "step": 2223
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.765507698059082,
        "learning_rate": 5.038679743656549e-06,
        "epoch": 0.28667182263469965,
        "step": 2224
    },
    {
        "loss": 2.1265,
        "grad_norm": 2.301741123199463,
        "learning_rate": 5.012087811697042e-06,
        "epoch": 0.28680072183552463,
        "step": 2225
    },
    {
        "loss": 1.4322,
        "grad_norm": 2.3052539825439453,
        "learning_rate": 4.985562532709909e-06,
        "epoch": 0.28692962103634956,
        "step": 2226
    },
    {
        "loss": 1.6809,
        "grad_norm": 2.8633925914764404,
        "learning_rate": 4.959103945994359e-06,
        "epoch": 0.28705852023717454,
        "step": 2227
    },
    {
        "loss": 2.1748,
        "grad_norm": 1.4332998991012573,
        "learning_rate": 4.93271209075074e-06,
        "epoch": 0.28718741943799947,
        "step": 2228
    },
    {
        "loss": 2.0506,
        "grad_norm": 1.5734649896621704,
        "learning_rate": 4.906387006080615e-06,
        "epoch": 0.28731631863882445,
        "step": 2229
    },
    {
        "loss": 2.2142,
        "grad_norm": 1.7462413311004639,
        "learning_rate": 4.880128730986538e-06,
        "epoch": 0.2874452178396494,
        "step": 2230
    },
    {
        "loss": 1.9501,
        "grad_norm": 3.02247953414917,
        "learning_rate": 4.85393730437213e-06,
        "epoch": 0.28757411704047436,
        "step": 2231
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.7482575178146362,
        "learning_rate": 4.827812765041972e-06,
        "epoch": 0.2877030162412993,
        "step": 2232
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.9857721328735352,
        "learning_rate": 4.801755151701503e-06,
        "epoch": 0.2878319154421243,
        "step": 2233
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.4862213134765625,
        "learning_rate": 4.775764502957053e-06,
        "epoch": 0.2879608146429492,
        "step": 2234
    },
    {
        "loss": 2.2439,
        "grad_norm": 1.9322431087493896,
        "learning_rate": 4.749840857315718e-06,
        "epoch": 0.2880897138437742,
        "step": 2235
    },
    {
        "loss": 2.0207,
        "grad_norm": 2.1829240322113037,
        "learning_rate": 4.723984253185332e-06,
        "epoch": 0.2882186130445991,
        "step": 2236
    },
    {
        "loss": 2.5636,
        "grad_norm": 2.005117654800415,
        "learning_rate": 4.698194728874383e-06,
        "epoch": 0.2883475122454241,
        "step": 2237
    },
    {
        "loss": 2.2242,
        "grad_norm": 2.2746388912200928,
        "learning_rate": 4.672472322592003e-06,
        "epoch": 0.288476411446249,
        "step": 2238
    },
    {
        "loss": 1.9352,
        "grad_norm": 2.3550937175750732,
        "learning_rate": 4.646817072447873e-06,
        "epoch": 0.288605310647074,
        "step": 2239
    },
    {
        "loss": 1.6675,
        "grad_norm": 2.2526705265045166,
        "learning_rate": 4.621229016452156e-06,
        "epoch": 0.28873420984789894,
        "step": 2240
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.7934352159500122,
        "learning_rate": 4.5957081925154746e-06,
        "epoch": 0.2888631090487239,
        "step": 2241
    },
    {
        "loss": 2.0492,
        "grad_norm": 2.258496046066284,
        "learning_rate": 4.570254638448895e-06,
        "epoch": 0.28899200824954885,
        "step": 2242
    },
    {
        "loss": 2.1963,
        "grad_norm": 1.8175824880599976,
        "learning_rate": 4.544868391963719e-06,
        "epoch": 0.28912090745037383,
        "step": 2243
    },
    {
        "loss": 2.186,
        "grad_norm": 1.7714037895202637,
        "learning_rate": 4.519549490671582e-06,
        "epoch": 0.28924980665119876,
        "step": 2244
    },
    {
        "loss": 2.0884,
        "grad_norm": 2.2270965576171875,
        "learning_rate": 4.494297972084377e-06,
        "epoch": 0.28937870585202374,
        "step": 2245
    },
    {
        "loss": 1.9762,
        "grad_norm": 2.066758632659912,
        "learning_rate": 4.469113873614084e-06,
        "epoch": 0.28950760505284867,
        "step": 2246
    },
    {
        "loss": 2.2871,
        "grad_norm": 1.323317527770996,
        "learning_rate": 4.4439972325728715e-06,
        "epoch": 0.28963650425367365,
        "step": 2247
    },
    {
        "loss": 1.8358,
        "grad_norm": 2.3371498584747314,
        "learning_rate": 4.418948086172914e-06,
        "epoch": 0.2897654034544986,
        "step": 2248
    },
    {
        "loss": 2.5593,
        "grad_norm": 1.3877359628677368,
        "learning_rate": 4.393966471526384e-06,
        "epoch": 0.2898943026553235,
        "step": 2249
    },
    {
        "loss": 2.1465,
        "grad_norm": 1.5023001432418823,
        "learning_rate": 4.3690524256454644e-06,
        "epoch": 0.2900232018561485,
        "step": 2250
    },
    {
        "loss": 1.9626,
        "grad_norm": 2.518016815185547,
        "learning_rate": 4.344205985442162e-06,
        "epoch": 0.2901521010569734,
        "step": 2251
    },
    {
        "loss": 2.2127,
        "grad_norm": 2.452176809310913,
        "learning_rate": 4.319427187728331e-06,
        "epoch": 0.2902810002577984,
        "step": 2252
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.477955937385559,
        "learning_rate": 4.294716069215654e-06,
        "epoch": 0.29040989945862333,
        "step": 2253
    },
    {
        "loss": 2.4326,
        "grad_norm": 1.9945716857910156,
        "learning_rate": 4.2700726665154786e-06,
        "epoch": 0.2905387986594483,
        "step": 2254
    },
    {
        "loss": 2.4265,
        "grad_norm": 1.2876336574554443,
        "learning_rate": 4.2454970161388774e-06,
        "epoch": 0.29066769786027324,
        "step": 2255
    },
    {
        "loss": 2.0316,
        "grad_norm": 1.585476279258728,
        "learning_rate": 4.22098915449653e-06,
        "epoch": 0.2907965970610982,
        "step": 2256
    },
    {
        "loss": 1.8882,
        "grad_norm": 2.459334373474121,
        "learning_rate": 4.196549117898641e-06,
        "epoch": 0.29092549626192316,
        "step": 2257
    },
    {
        "loss": 1.9966,
        "grad_norm": 2.8391823768615723,
        "learning_rate": 4.172176942555001e-06,
        "epoch": 0.29105439546274814,
        "step": 2258
    },
    {
        "loss": 2.1569,
        "grad_norm": 1.7365518808364868,
        "learning_rate": 4.147872664574803e-06,
        "epoch": 0.29118329466357307,
        "step": 2259
    },
    {
        "loss": 1.9551,
        "grad_norm": 2.660104513168335,
        "learning_rate": 4.12363631996664e-06,
        "epoch": 0.29131219386439805,
        "step": 2260
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.1697194576263428,
        "learning_rate": 4.099467944638513e-06,
        "epoch": 0.291441093065223,
        "step": 2261
    },
    {
        "loss": 2.3293,
        "grad_norm": 1.7458823919296265,
        "learning_rate": 4.075367574397665e-06,
        "epoch": 0.29156999226604796,
        "step": 2262
    },
    {
        "loss": 1.892,
        "grad_norm": 2.285510540008545,
        "learning_rate": 4.051335244950611e-06,
        "epoch": 0.2916988914668729,
        "step": 2263
    },
    {
        "loss": 2.0811,
        "grad_norm": 2.8222739696502686,
        "learning_rate": 4.027370991903051e-06,
        "epoch": 0.2918277906676979,
        "step": 2264
    },
    {
        "loss": 2.5502,
        "grad_norm": 1.4426971673965454,
        "learning_rate": 4.003474850759836e-06,
        "epoch": 0.2919566898685228,
        "step": 2265
    },
    {
        "loss": 1.9994,
        "grad_norm": 2.09169864654541,
        "learning_rate": 3.979646856924879e-06,
        "epoch": 0.2920855890693478,
        "step": 2266
    },
    {
        "loss": 1.853,
        "grad_norm": 2.255478858947754,
        "learning_rate": 3.955887045701151e-06,
        "epoch": 0.2922144882701727,
        "step": 2267
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.1942002773284912,
        "learning_rate": 3.9321954522906105e-06,
        "epoch": 0.2923433874709977,
        "step": 2268
    },
    {
        "loss": 1.6079,
        "grad_norm": 2.3605027198791504,
        "learning_rate": 3.908572111794106e-06,
        "epoch": 0.2924722866718226,
        "step": 2269
    },
    {
        "loss": 1.6735,
        "grad_norm": 2.7635457515716553,
        "learning_rate": 3.8850170592113985e-06,
        "epoch": 0.2926011858726476,
        "step": 2270
    },
    {
        "loss": 1.875,
        "grad_norm": 2.3236100673675537,
        "learning_rate": 3.861530329441104e-06,
        "epoch": 0.29273008507347253,
        "step": 2271
    },
    {
        "loss": 2.0995,
        "grad_norm": 2.322988748550415,
        "learning_rate": 3.838111957280532e-06,
        "epoch": 0.2928589842742975,
        "step": 2272
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.4559013843536377,
        "learning_rate": 3.814761977425768e-06,
        "epoch": 0.29298788347512245,
        "step": 2273
    },
    {
        "loss": 2.0607,
        "grad_norm": 1.54606294631958,
        "learning_rate": 3.791480424471594e-06,
        "epoch": 0.29311678267594743,
        "step": 2274
    },
    {
        "loss": 1.9106,
        "grad_norm": 1.6607394218444824,
        "learning_rate": 3.7682673329113516e-06,
        "epoch": 0.29324568187677236,
        "step": 2275
    },
    {
        "loss": 2.396,
        "grad_norm": 1.6216590404510498,
        "learning_rate": 3.7451227371369747e-06,
        "epoch": 0.29337458107759734,
        "step": 2276
    },
    {
        "loss": 1.9975,
        "grad_norm": 1.4800777435302734,
        "learning_rate": 3.72204667143895e-06,
        "epoch": 0.29350348027842227,
        "step": 2277
    },
    {
        "loss": 1.5274,
        "grad_norm": 2.6451637744903564,
        "learning_rate": 3.699039170006163e-06,
        "epoch": 0.29363237947924725,
        "step": 2278
    },
    {
        "loss": 1.8701,
        "grad_norm": 1.8897228240966797,
        "learning_rate": 3.6761002669260024e-06,
        "epoch": 0.2937612786800722,
        "step": 2279
    },
    {
        "loss": 2.3616,
        "grad_norm": 1.2721010446548462,
        "learning_rate": 3.6532299961841488e-06,
        "epoch": 0.29389017788089716,
        "step": 2280
    },
    {
        "loss": 1.2034,
        "grad_norm": 2.754598379135132,
        "learning_rate": 3.63042839166462e-06,
        "epoch": 0.2940190770817221,
        "step": 2281
    },
    {
        "loss": 1.9326,
        "grad_norm": 2.2516751289367676,
        "learning_rate": 3.607695487149726e-06,
        "epoch": 0.2941479762825471,
        "step": 2282
    },
    {
        "loss": 2.3313,
        "grad_norm": 1.7782810926437378,
        "learning_rate": 3.58503131631997e-06,
        "epoch": 0.294276875483372,
        "step": 2283
    },
    {
        "loss": 2.6663,
        "grad_norm": 1.9998574256896973,
        "learning_rate": 3.562435912754003e-06,
        "epoch": 0.29440577468419693,
        "step": 2284
    },
    {
        "loss": 1.195,
        "grad_norm": 2.9735591411590576,
        "learning_rate": 3.5399093099286407e-06,
        "epoch": 0.2945346738850219,
        "step": 2285
    },
    {
        "loss": 1.3359,
        "grad_norm": 2.708458662033081,
        "learning_rate": 3.5174515412187137e-06,
        "epoch": 0.29466357308584684,
        "step": 2286
    },
    {
        "loss": 2.6622,
        "grad_norm": 1.3535819053649902,
        "learning_rate": 3.4950626398971175e-06,
        "epoch": 0.2947924722866718,
        "step": 2287
    },
    {
        "loss": 1.8791,
        "grad_norm": 3.54636287689209,
        "learning_rate": 3.472742639134685e-06,
        "epoch": 0.29492137148749675,
        "step": 2288
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.2544288635253906,
        "learning_rate": 3.450491572000153e-06,
        "epoch": 0.29505027068832174,
        "step": 2289
    },
    {
        "loss": 1.8227,
        "grad_norm": 2.4231984615325928,
        "learning_rate": 3.4283094714601837e-06,
        "epoch": 0.29517916988914666,
        "step": 2290
    },
    {
        "loss": 1.9948,
        "grad_norm": 1.9212063550949097,
        "learning_rate": 3.4061963703792055e-06,
        "epoch": 0.29530806908997165,
        "step": 2291
    },
    {
        "loss": 2.3131,
        "grad_norm": 1.5965650081634521,
        "learning_rate": 3.384152301519433e-06,
        "epoch": 0.2954369682907966,
        "step": 2292
    },
    {
        "loss": 1.945,
        "grad_norm": 2.0019092559814453,
        "learning_rate": 3.3621772975408304e-06,
        "epoch": 0.29556586749162156,
        "step": 2293
    },
    {
        "loss": 2.0089,
        "grad_norm": 1.6597216129302979,
        "learning_rate": 3.3402713910010043e-06,
        "epoch": 0.2956947666924465,
        "step": 2294
    },
    {
        "loss": 2.7533,
        "grad_norm": 1.3209658861160278,
        "learning_rate": 3.3184346143551925e-06,
        "epoch": 0.29582366589327147,
        "step": 2295
    },
    {
        "loss": 2.063,
        "grad_norm": 1.6625584363937378,
        "learning_rate": 3.2966669999562383e-06,
        "epoch": 0.2959525650940964,
        "step": 2296
    },
    {
        "loss": 1.9034,
        "grad_norm": 2.4531590938568115,
        "learning_rate": 3.2749685800544993e-06,
        "epoch": 0.2960814642949214,
        "step": 2297
    },
    {
        "loss": 2.2044,
        "grad_norm": 1.9737491607666016,
        "learning_rate": 3.2533393867978044e-06,
        "epoch": 0.2962103634957463,
        "step": 2298
    },
    {
        "loss": 1.9611,
        "grad_norm": 2.5431861877441406,
        "learning_rate": 3.231779452231426e-06,
        "epoch": 0.2963392626965713,
        "step": 2299
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.8513391017913818,
        "learning_rate": 3.2102888082980396e-06,
        "epoch": 0.2964681618973962,
        "step": 2300
    },
    {
        "loss": 1.8139,
        "grad_norm": 2.753363847732544,
        "learning_rate": 3.1888674868376377e-06,
        "epoch": 0.2965970610982212,
        "step": 2301
    },
    {
        "loss": 1.9816,
        "grad_norm": 1.5399373769760132,
        "learning_rate": 3.1675155195875218e-06,
        "epoch": 0.29672596029904613,
        "step": 2302
    },
    {
        "loss": 1.9512,
        "grad_norm": 1.827060580253601,
        "learning_rate": 3.1462329381822365e-06,
        "epoch": 0.2968548594998711,
        "step": 2303
    },
    {
        "loss": 1.6942,
        "grad_norm": 2.9967892169952393,
        "learning_rate": 3.125019774153537e-06,
        "epoch": 0.29698375870069604,
        "step": 2304
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.46152663230896,
        "learning_rate": 3.1038760589302885e-06,
        "epoch": 0.297112657901521,
        "step": 2305
    },
    {
        "loss": 2.043,
        "grad_norm": 1.5452207326889038,
        "learning_rate": 3.082801823838527e-06,
        "epoch": 0.29724155710234595,
        "step": 2306
    },
    {
        "loss": 1.0193,
        "grad_norm": 3.362460136413574,
        "learning_rate": 3.0617971001013044e-06,
        "epoch": 0.29737045630317094,
        "step": 2307
    },
    {
        "loss": 1.2131,
        "grad_norm": 2.528029441833496,
        "learning_rate": 3.040861918838678e-06,
        "epoch": 0.29749935550399587,
        "step": 2308
    },
    {
        "loss": 2.0097,
        "grad_norm": 2.5681989192962646,
        "learning_rate": 3.0199963110677243e-06,
        "epoch": 0.29762825470482085,
        "step": 2309
    },
    {
        "loss": 2.414,
        "grad_norm": 1.4192944765090942,
        "learning_rate": 2.999200307702393e-06,
        "epoch": 0.2977571539056458,
        "step": 2310
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.9404141902923584,
        "learning_rate": 2.9784739395535055e-06,
        "epoch": 0.29788605310647076,
        "step": 2311
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.6206870079040527,
        "learning_rate": 2.957817237328775e-06,
        "epoch": 0.2980149523072957,
        "step": 2312
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.4801902770996094,
        "learning_rate": 2.937230231632626e-06,
        "epoch": 0.29814385150812067,
        "step": 2313
    },
    {
        "loss": 1.8479,
        "grad_norm": 2.66706919670105,
        "learning_rate": 2.916712952966272e-06,
        "epoch": 0.2982727507089456,
        "step": 2314
    },
    {
        "loss": 2.2982,
        "grad_norm": 2.2884140014648438,
        "learning_rate": 2.8962654317276015e-06,
        "epoch": 0.2984016499097706,
        "step": 2315
    },
    {
        "loss": 2.3696,
        "grad_norm": 1.6476572751998901,
        "learning_rate": 2.8758876982111326e-06,
        "epoch": 0.2985305491105955,
        "step": 2316
    },
    {
        "loss": 1.2215,
        "grad_norm": 2.17252254486084,
        "learning_rate": 2.855579782608042e-06,
        "epoch": 0.2986594483114205,
        "step": 2317
    },
    {
        "loss": 2.5631,
        "grad_norm": 1.7969329357147217,
        "learning_rate": 2.8353417150060122e-06,
        "epoch": 0.2987883475122454,
        "step": 2318
    },
    {
        "loss": 1.5536,
        "grad_norm": 2.2683157920837402,
        "learning_rate": 2.815173525389253e-06,
        "epoch": 0.2989172467130704,
        "step": 2319
    },
    {
        "loss": 1.9346,
        "grad_norm": 1.5890015363693237,
        "learning_rate": 2.795075243638473e-06,
        "epoch": 0.29904614591389533,
        "step": 2320
    },
    {
        "loss": 1.8168,
        "grad_norm": 2.5373620986938477,
        "learning_rate": 2.77504689953077e-06,
        "epoch": 0.29917504511472026,
        "step": 2321
    },
    {
        "loss": 2.3738,
        "grad_norm": 2.1840620040893555,
        "learning_rate": 2.7550885227396573e-06,
        "epoch": 0.29930394431554525,
        "step": 2322
    },
    {
        "loss": 2.2331,
        "grad_norm": 1.716679573059082,
        "learning_rate": 2.735200142834965e-06,
        "epoch": 0.2994328435163702,
        "step": 2323
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.4865531921386719,
        "learning_rate": 2.715381789282806e-06,
        "epoch": 0.29956174271719516,
        "step": 2324
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.5373241901397705,
        "learning_rate": 2.6956334914455763e-06,
        "epoch": 0.2996906419180201,
        "step": 2325
    },
    {
        "loss": 2.3583,
        "grad_norm": 1.4046300649642944,
        "learning_rate": 2.675955278581871e-06,
        "epoch": 0.29981954111884507,
        "step": 2326
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.3667629957199097,
        "learning_rate": 2.656347179846419e-06,
        "epoch": 0.29994844031967,
        "step": 2327
    },
    {
        "loss": 2.3894,
        "grad_norm": 1.8408093452453613,
        "learning_rate": 2.6368092242901044e-06,
        "epoch": 0.300077339520495,
        "step": 2328
    },
    {
        "loss": 2.139,
        "grad_norm": 2.0789735317230225,
        "learning_rate": 2.6173414408598827e-06,
        "epoch": 0.3002062387213199,
        "step": 2329
    },
    {
        "loss": 2.4571,
        "grad_norm": 1.4998646974563599,
        "learning_rate": 2.5979438583987215e-06,
        "epoch": 0.3003351379221449,
        "step": 2330
    },
    {
        "loss": 2.1215,
        "grad_norm": 2.578319787979126,
        "learning_rate": 2.5786165056456037e-06,
        "epoch": 0.3004640371229698,
        "step": 2331
    },
    {
        "loss": 2.1768,
        "grad_norm": 1.6193573474884033,
        "learning_rate": 2.5593594112354623e-06,
        "epoch": 0.3005929363237948,
        "step": 2332
    },
    {
        "loss": 2.0084,
        "grad_norm": 1.417060375213623,
        "learning_rate": 2.540172603699126e-06,
        "epoch": 0.30072183552461973,
        "step": 2333
    },
    {
        "loss": 1.8069,
        "grad_norm": 1.5934207439422607,
        "learning_rate": 2.5210561114632826e-06,
        "epoch": 0.3008507347254447,
        "step": 2334
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.9850889444351196,
        "learning_rate": 2.50200996285046e-06,
        "epoch": 0.30097963392626964,
        "step": 2335
    },
    {
        "loss": 2.4964,
        "grad_norm": 1.2120097875595093,
        "learning_rate": 2.483034186078964e-06,
        "epoch": 0.3011085331270946,
        "step": 2336
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.2770469188690186,
        "learning_rate": 2.464128809262811e-06,
        "epoch": 0.30123743232791955,
        "step": 2337
    },
    {
        "loss": 2.0839,
        "grad_norm": 2.1774489879608154,
        "learning_rate": 2.445293860411768e-06,
        "epoch": 0.30136633152874454,
        "step": 2338
    },
    {
        "loss": 0.9298,
        "grad_norm": 2.6466281414031982,
        "learning_rate": 2.4265293674312184e-06,
        "epoch": 0.30149523072956946,
        "step": 2339
    },
    {
        "loss": 1.642,
        "grad_norm": 3.147066593170166,
        "learning_rate": 2.4078353581221402e-06,
        "epoch": 0.30162412993039445,
        "step": 2340
    },
    {
        "loss": 2.0794,
        "grad_norm": 1.6963553428649902,
        "learning_rate": 2.3892118601811564e-06,
        "epoch": 0.3017530291312194,
        "step": 2341
    },
    {
        "loss": 2.1511,
        "grad_norm": 1.8290882110595703,
        "learning_rate": 2.3706589012003456e-06,
        "epoch": 0.30188192833204436,
        "step": 2342
    },
    {
        "loss": 2.18,
        "grad_norm": 1.2880743741989136,
        "learning_rate": 2.352176508667325e-06,
        "epoch": 0.3020108275328693,
        "step": 2343
    },
    {
        "loss": 1.242,
        "grad_norm": 2.8610620498657227,
        "learning_rate": 2.3337647099651527e-06,
        "epoch": 0.30213972673369427,
        "step": 2344
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.9514493942260742,
        "learning_rate": 2.3154235323722686e-06,
        "epoch": 0.3022686259345192,
        "step": 2345
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.4750304222106934,
        "learning_rate": 2.2971530030625353e-06,
        "epoch": 0.3023975251353442,
        "step": 2346
    },
    {
        "loss": 1.9922,
        "grad_norm": 2.1778979301452637,
        "learning_rate": 2.278953149105101e-06,
        "epoch": 0.3025264243361691,
        "step": 2347
    },
    {
        "loss": 2.0709,
        "grad_norm": 2.019597291946411,
        "learning_rate": 2.2608239974644063e-06,
        "epoch": 0.3026553235369941,
        "step": 2348
    },
    {
        "loss": 1.5016,
        "grad_norm": 3.588153600692749,
        "learning_rate": 2.242765575000172e-06,
        "epoch": 0.302784222737819,
        "step": 2349
    },
    {
        "loss": 2.2548,
        "grad_norm": 1.748390793800354,
        "learning_rate": 2.2247779084673015e-06,
        "epoch": 0.302913121938644,
        "step": 2350
    },
    {
        "loss": 2.0621,
        "grad_norm": 1.9442243576049805,
        "learning_rate": 2.206861024515855e-06,
        "epoch": 0.30304202113946893,
        "step": 2351
    },
    {
        "loss": 1.6554,
        "grad_norm": 2.569469928741455,
        "learning_rate": 2.189014949691076e-06,
        "epoch": 0.3031709203402939,
        "step": 2352
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.5511364936828613,
        "learning_rate": 2.1712397104332314e-06,
        "epoch": 0.30329981954111884,
        "step": 2353
    },
    {
        "loss": 2.2027,
        "grad_norm": 2.049644947052002,
        "learning_rate": 2.1535353330776996e-06,
        "epoch": 0.3034287187419438,
        "step": 2354
    },
    {
        "loss": 1.9672,
        "grad_norm": 2.6048829555511475,
        "learning_rate": 2.1359018438548428e-06,
        "epoch": 0.30355761794276875,
        "step": 2355
    },
    {
        "loss": 2.5591,
        "grad_norm": 1.5526598691940308,
        "learning_rate": 2.118339268889985e-06,
        "epoch": 0.30368651714359374,
        "step": 2356
    },
    {
        "loss": 1.9914,
        "grad_norm": 1.4330447912216187,
        "learning_rate": 2.100847634203412e-06,
        "epoch": 0.30381541634441867,
        "step": 2357
    },
    {
        "loss": 1.8567,
        "grad_norm": 1.8412737846374512,
        "learning_rate": 2.0834269657103e-06,
        "epoch": 0.3039443155452436,
        "step": 2358
    },
    {
        "loss": 1.8211,
        "grad_norm": 2.3991458415985107,
        "learning_rate": 2.0660772892206636e-06,
        "epoch": 0.3040732147460686,
        "step": 2359
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.8946518898010254,
        "learning_rate": 2.048798630439358e-06,
        "epoch": 0.3042021139468935,
        "step": 2360
    },
    {
        "loss": 1.9233,
        "grad_norm": 1.7388057708740234,
        "learning_rate": 2.0315910149660165e-06,
        "epoch": 0.3043310131477185,
        "step": 2361
    },
    {
        "loss": 1.6826,
        "grad_norm": 2.710261344909668,
        "learning_rate": 2.014454468295024e-06,
        "epoch": 0.3044599123485434,
        "step": 2362
    },
    {
        "loss": 2.2235,
        "grad_norm": 1.4026681184768677,
        "learning_rate": 1.997389015815443e-06,
        "epoch": 0.3045888115493684,
        "step": 2363
    },
    {
        "loss": 2.3518,
        "grad_norm": 1.2396352291107178,
        "learning_rate": 1.9803946828110375e-06,
        "epoch": 0.3047177107501933,
        "step": 2364
    },
    {
        "loss": 1.9766,
        "grad_norm": 1.9562031030654907,
        "learning_rate": 1.963471494460184e-06,
        "epoch": 0.3048466099510183,
        "step": 2365
    },
    {
        "loss": 2.1821,
        "grad_norm": 1.7649272680282593,
        "learning_rate": 1.946619475835859e-06,
        "epoch": 0.30497550915184324,
        "step": 2366
    },
    {
        "loss": 1.8981,
        "grad_norm": 2.288719654083252,
        "learning_rate": 1.9298386519055856e-06,
        "epoch": 0.3051044083526682,
        "step": 2367
    },
    {
        "loss": 1.9612,
        "grad_norm": 2.2611217498779297,
        "learning_rate": 1.913129047531437e-06,
        "epoch": 0.30523330755349315,
        "step": 2368
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.431850552558899,
        "learning_rate": 1.896490687469915e-06,
        "epoch": 0.30536220675431813,
        "step": 2369
    },
    {
        "loss": 2.2955,
        "grad_norm": 2.244640588760376,
        "learning_rate": 1.8799235963720351e-06,
        "epoch": 0.30549110595514306,
        "step": 2370
    },
    {
        "loss": 1.9095,
        "grad_norm": 2.0036520957946777,
        "learning_rate": 1.8634277987831682e-06,
        "epoch": 0.30562000515596804,
        "step": 2371
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.2841827869415283,
        "learning_rate": 1.8470033191430759e-06,
        "epoch": 0.30574890435679297,
        "step": 2372
    },
    {
        "loss": 2.3299,
        "grad_norm": 2.6647794246673584,
        "learning_rate": 1.8306501817858756e-06,
        "epoch": 0.30587780355761796,
        "step": 2373
    },
    {
        "loss": 1.483,
        "grad_norm": 3.5609796047210693,
        "learning_rate": 1.8143684109399483e-06,
        "epoch": 0.3060067027584429,
        "step": 2374
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.672268867492676,
        "learning_rate": 1.79815803072797e-06,
        "epoch": 0.30613560195926787,
        "step": 2375
    },
    {
        "loss": 2.2375,
        "grad_norm": 2.282323122024536,
        "learning_rate": 1.7820190651668457e-06,
        "epoch": 0.3062645011600928,
        "step": 2376
    },
    {
        "loss": 1.8312,
        "grad_norm": 2.889281988143921,
        "learning_rate": 1.7659515381676484e-06,
        "epoch": 0.3063934003609178,
        "step": 2377
    },
    {
        "loss": 2.0663,
        "grad_norm": 2.554133653640747,
        "learning_rate": 1.7499554735356415e-06,
        "epoch": 0.3065222995617427,
        "step": 2378
    },
    {
        "loss": 1.3388,
        "grad_norm": 2.6925792694091797,
        "learning_rate": 1.7340308949701957e-06,
        "epoch": 0.3066511987625677,
        "step": 2379
    },
    {
        "loss": 2.1451,
        "grad_norm": 1.9082698822021484,
        "learning_rate": 1.7181778260647541e-06,
        "epoch": 0.3067800979633926,
        "step": 2380
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.2804455757141113,
        "learning_rate": 1.7023962903068513e-06,
        "epoch": 0.3069089971642176,
        "step": 2381
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.951140880584717,
        "learning_rate": 1.6866863110780063e-06,
        "epoch": 0.30703789636504253,
        "step": 2382
    },
    {
        "loss": 2.4766,
        "grad_norm": 2.0168774127960205,
        "learning_rate": 1.6710479116537225e-06,
        "epoch": 0.3071667955658675,
        "step": 2383
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.2476904392242432,
        "learning_rate": 1.6554811152034944e-06,
        "epoch": 0.30729569476669244,
        "step": 2384
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.0300540924072266,
        "learning_rate": 1.6399859447906896e-06,
        "epoch": 0.3074245939675174,
        "step": 2385
    },
    {
        "loss": 2.258,
        "grad_norm": 1.4934138059616089,
        "learning_rate": 1.6245624233725387e-06,
        "epoch": 0.30755349316834235,
        "step": 2386
    },
    {
        "loss": 1.6578,
        "grad_norm": 2.549651861190796,
        "learning_rate": 1.609210573800185e-06,
        "epoch": 0.30768239236916733,
        "step": 2387
    },
    {
        "loss": 2.3326,
        "grad_norm": 1.6853607892990112,
        "learning_rate": 1.593930418818529e-06,
        "epoch": 0.30781129156999226,
        "step": 2388
    },
    {
        "loss": 1.8566,
        "grad_norm": 1.4479323625564575,
        "learning_rate": 1.5787219810662723e-06,
        "epoch": 0.30794019077081725,
        "step": 2389
    },
    {
        "loss": 1.0666,
        "grad_norm": 3.6869590282440186,
        "learning_rate": 1.5635852830758635e-06,
        "epoch": 0.3080690899716422,
        "step": 2390
    },
    {
        "loss": 2.2106,
        "grad_norm": 1.288339614868164,
        "learning_rate": 1.5485203472734467e-06,
        "epoch": 0.30819798917246716,
        "step": 2391
    },
    {
        "loss": 2.3324,
        "grad_norm": 1.9362013339996338,
        "learning_rate": 1.5335271959788623e-06,
        "epoch": 0.3083268883732921,
        "step": 2392
    },
    {
        "loss": 1.3978,
        "grad_norm": 2.604163408279419,
        "learning_rate": 1.5186058514055968e-06,
        "epoch": 0.308455787574117,
        "step": 2393
    },
    {
        "loss": 1.8512,
        "grad_norm": 2.3532097339630127,
        "learning_rate": 1.5037563356607276e-06,
        "epoch": 0.308584686774942,
        "step": 2394
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.6364550590515137,
        "learning_rate": 1.4889786707449394e-06,
        "epoch": 0.3087135859757669,
        "step": 2395
    },
    {
        "loss": 1.6562,
        "grad_norm": 3.0401012897491455,
        "learning_rate": 1.4742728785524517e-06,
        "epoch": 0.3088424851765919,
        "step": 2396
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.4101382493972778,
        "learning_rate": 1.4596389808710088e-06,
        "epoch": 0.30897138437741684,
        "step": 2397
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.6771739721298218,
        "learning_rate": 1.4450769993818114e-06,
        "epoch": 0.3091002835782418,
        "step": 2398
    },
    {
        "loss": 2.0096,
        "grad_norm": 2.992464065551758,
        "learning_rate": 1.4305869556595408e-06,
        "epoch": 0.30922918277906675,
        "step": 2399
    },
    {
        "loss": 2.4076,
        "grad_norm": 1.5241872072219849,
        "learning_rate": 1.4161688711722854e-06,
        "epoch": 0.30935808197989173,
        "step": 2400
    },
    {
        "loss": 2.2096,
        "grad_norm": 1.2561328411102295,
        "learning_rate": 1.4018227672815188e-06,
        "epoch": 0.30948698118071666,
        "step": 2401
    },
    {
        "loss": 1.875,
        "grad_norm": 1.9013582468032837,
        "learning_rate": 1.3875486652420722e-06,
        "epoch": 0.30961588038154164,
        "step": 2402
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.566756248474121,
        "learning_rate": 1.373346586202101e-06,
        "epoch": 0.30974477958236657,
        "step": 2403
    },
    {
        "loss": 2.1365,
        "grad_norm": 1.5437240600585938,
        "learning_rate": 1.3592165512030518e-06,
        "epoch": 0.30987367878319155,
        "step": 2404
    },
    {
        "loss": 1.4319,
        "grad_norm": 3.0003535747528076,
        "learning_rate": 1.3451585811796341e-06,
        "epoch": 0.3100025779840165,
        "step": 2405
    },
    {
        "loss": 2.3145,
        "grad_norm": 1.9146778583526611,
        "learning_rate": 1.3311726969597816e-06,
        "epoch": 0.31013147718484146,
        "step": 2406
    },
    {
        "loss": 2.4758,
        "grad_norm": 1.5935704708099365,
        "learning_rate": 1.3172589192646413e-06,
        "epoch": 0.3102603763856664,
        "step": 2407
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.1974565982818604,
        "learning_rate": 1.3034172687085177e-06,
        "epoch": 0.3103892755864914,
        "step": 2408
    },
    {
        "loss": 2.3787,
        "grad_norm": 1.8539656400680542,
        "learning_rate": 1.2896477657988514e-06,
        "epoch": 0.3105181747873163,
        "step": 2409
    },
    {
        "loss": 2.0636,
        "grad_norm": 1.616887092590332,
        "learning_rate": 1.2759504309362013e-06,
        "epoch": 0.3106470739881413,
        "step": 2410
    },
    {
        "loss": 1.7249,
        "grad_norm": 1.1821898221969604,
        "learning_rate": 1.2623252844141953e-06,
        "epoch": 0.3107759731889662,
        "step": 2411
    },
    {
        "loss": 2.1959,
        "grad_norm": 2.8455708026885986,
        "learning_rate": 1.2487723464195022e-06,
        "epoch": 0.3109048723897912,
        "step": 2412
    },
    {
        "loss": 1.8179,
        "grad_norm": 2.097719430923462,
        "learning_rate": 1.2352916370318435e-06,
        "epoch": 0.3110337715906161,
        "step": 2413
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.018040418624878,
        "learning_rate": 1.221883176223887e-06,
        "epoch": 0.3111626707914411,
        "step": 2414
    },
    {
        "loss": 2.0021,
        "grad_norm": 1.8861603736877441,
        "learning_rate": 1.2085469838612585e-06,
        "epoch": 0.31129156999226604,
        "step": 2415
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.3113865852355957,
        "learning_rate": 1.1952830797025638e-06,
        "epoch": 0.311420469193091,
        "step": 2416
    },
    {
        "loss": 1.9934,
        "grad_norm": 1.376349925994873,
        "learning_rate": 1.1820914833992558e-06,
        "epoch": 0.31154936839391595,
        "step": 2417
    },
    {
        "loss": 2.2347,
        "grad_norm": 2.5241944789886475,
        "learning_rate": 1.1689722144956616e-06,
        "epoch": 0.31167826759474093,
        "step": 2418
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.7315349578857422,
        "learning_rate": 1.1559252924289943e-06,
        "epoch": 0.31180716679556586,
        "step": 2419
    },
    {
        "loss": 2.2878,
        "grad_norm": 2.1806204319000244,
        "learning_rate": 1.1429507365292246e-06,
        "epoch": 0.31193606599639084,
        "step": 2420
    },
    {
        "loss": 1.8496,
        "grad_norm": 2.418475866317749,
        "learning_rate": 1.130048566019143e-06,
        "epoch": 0.31206496519721577,
        "step": 2421
    },
    {
        "loss": 2.7235,
        "grad_norm": 1.6170811653137207,
        "learning_rate": 1.1172188000142802e-06,
        "epoch": 0.31219386439804075,
        "step": 2422
    },
    {
        "loss": 1.7023,
        "grad_norm": 2.694999933242798,
        "learning_rate": 1.104461457522904e-06,
        "epoch": 0.3123227635988657,
        "step": 2423
    },
    {
        "loss": 1.88,
        "grad_norm": 3.152773380279541,
        "learning_rate": 1.0917765574459671e-06,
        "epoch": 0.31245166279969067,
        "step": 2424
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.6053298711776733,
        "learning_rate": 1.0791641185771139e-06,
        "epoch": 0.3125805620005156,
        "step": 2425
    },
    {
        "loss": 1.8425,
        "grad_norm": 2.33647084236145,
        "learning_rate": 1.066624159602614e-06,
        "epoch": 0.3127094612013406,
        "step": 2426
    },
    {
        "loss": 2.0122,
        "grad_norm": 2.2288618087768555,
        "learning_rate": 1.0541566991013553e-06,
        "epoch": 0.3128383604021655,
        "step": 2427
    },
    {
        "loss": 2.1613,
        "grad_norm": 1.7368099689483643,
        "learning_rate": 1.0417617555448177e-06,
        "epoch": 0.3129672596029905,
        "step": 2428
    },
    {
        "loss": 2.4989,
        "grad_norm": 1.5703951120376587,
        "learning_rate": 1.029439347297051e-06,
        "epoch": 0.3130961588038154,
        "step": 2429
    },
    {
        "loss": 1.7644,
        "grad_norm": 1.899951696395874,
        "learning_rate": 1.017189492614623e-06,
        "epoch": 0.31322505800464034,
        "step": 2430
    },
    {
        "loss": 1.5367,
        "grad_norm": 2.0086984634399414,
        "learning_rate": 1.0050122096466108e-06,
        "epoch": 0.3133539572054653,
        "step": 2431
    },
    {
        "loss": 2.5189,
        "grad_norm": 1.564742088317871,
        "learning_rate": 9.929075164345713e-07,
        "epoch": 0.31348285640629026,
        "step": 2432
    },
    {
        "loss": 0.9071,
        "grad_norm": 2.47099232673645,
        "learning_rate": 9.808754309125313e-07,
        "epoch": 0.31361175560711524,
        "step": 2433
    },
    {
        "loss": 2.5372,
        "grad_norm": 1.891676664352417,
        "learning_rate": 9.689159709069195e-07,
        "epoch": 0.31374065480794017,
        "step": 2434
    },
    {
        "loss": 1.8632,
        "grad_norm": 2.430943727493286,
        "learning_rate": 9.570291541365739e-07,
        "epoch": 0.31386955400876515,
        "step": 2435
    },
    {
        "loss": 1.6732,
        "grad_norm": 2.648742437362671,
        "learning_rate": 9.452149982127068e-07,
        "epoch": 0.3139984532095901,
        "step": 2436
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.4860092401504517,
        "learning_rate": 9.334735206388834e-07,
        "epoch": 0.31412735241041506,
        "step": 2437
    },
    {
        "loss": 2.1713,
        "grad_norm": 2.703058958053589,
        "learning_rate": 9.21804738810983e-07,
        "epoch": 0.31425625161124,
        "step": 2438
    },
    {
        "loss": 1.9077,
        "grad_norm": 2.6771321296691895,
        "learning_rate": 9.102086700171763e-07,
        "epoch": 0.314385150812065,
        "step": 2439
    },
    {
        "loss": 2.0541,
        "grad_norm": 2.543053388595581,
        "learning_rate": 8.986853314379207e-07,
        "epoch": 0.3145140500128899,
        "step": 2440
    },
    {
        "loss": 1.6618,
        "grad_norm": 2.53861665725708,
        "learning_rate": 8.87234740145898e-07,
        "epoch": 0.3146429492137149,
        "step": 2441
    },
    {
        "loss": 2.671,
        "grad_norm": 1.4788371324539185,
        "learning_rate": 8.758569131060267e-07,
        "epoch": 0.3147718484145398,
        "step": 2442
    },
    {
        "loss": 1.9652,
        "grad_norm": 2.747880458831787,
        "learning_rate": 8.645518671754172e-07,
        "epoch": 0.3149007476153648,
        "step": 2443
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.8492074012756348,
        "learning_rate": 8.53319619103321e-07,
        "epoch": 0.3150296468161897,
        "step": 2444
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.1728410720825195,
        "learning_rate": 8.421601855311878e-07,
        "epoch": 0.3151585460170147,
        "step": 2445
    },
    {
        "loss": 2.5003,
        "grad_norm": 1.9661527872085571,
        "learning_rate": 8.31073582992542e-07,
        "epoch": 0.31528744521783963,
        "step": 2446
    },
    {
        "loss": 1.9751,
        "grad_norm": 1.4464445114135742,
        "learning_rate": 8.200598279130168e-07,
        "epoch": 0.3154163444186646,
        "step": 2447
    },
    {
        "loss": 1.985,
        "grad_norm": 1.9194668531417847,
        "learning_rate": 8.091189366103258e-07,
        "epoch": 0.31554524361948955,
        "step": 2448
    },
    {
        "loss": 2.0129,
        "grad_norm": 2.0690581798553467,
        "learning_rate": 7.982509252942138e-07,
        "epoch": 0.31567414282031453,
        "step": 2449
    },
    {
        "loss": 2.1148,
        "grad_norm": 2.1518476009368896,
        "learning_rate": 7.874558100664564e-07,
        "epoch": 0.31580304202113946,
        "step": 2450
    },
    {
        "loss": 2.2222,
        "grad_norm": 1.6304069757461548,
        "learning_rate": 7.76733606920832e-07,
        "epoch": 0.31593194122196444,
        "step": 2451
    },
    {
        "loss": 1.5781,
        "grad_norm": 2.9329705238342285,
        "learning_rate": 7.660843317430944e-07,
        "epoch": 0.31606084042278937,
        "step": 2452
    },
    {
        "loss": 2.3002,
        "grad_norm": 1.7875189781188965,
        "learning_rate": 7.555080003109338e-07,
        "epoch": 0.31618973962361435,
        "step": 2453
    },
    {
        "loss": 1.2821,
        "grad_norm": 3.1485860347747803,
        "learning_rate": 7.450046282939882e-07,
        "epoch": 0.3163186388244393,
        "step": 2454
    },
    {
        "loss": 2.2109,
        "grad_norm": 1.6846559047698975,
        "learning_rate": 7.345742312537873e-07,
        "epoch": 0.31644753802526426,
        "step": 2455
    },
    {
        "loss": 1.9331,
        "grad_norm": 2.542715549468994,
        "learning_rate": 7.242168246437531e-07,
        "epoch": 0.3165764372260892,
        "step": 2456
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.52142596244812,
        "learning_rate": 7.139324238091661e-07,
        "epoch": 0.3167053364269142,
        "step": 2457
    },
    {
        "loss": 1.6056,
        "grad_norm": 2.470121383666992,
        "learning_rate": 7.037210439871211e-07,
        "epoch": 0.3168342356277391,
        "step": 2458
    },
    {
        "loss": 2.3291,
        "grad_norm": 1.876571536064148,
        "learning_rate": 6.93582700306561e-07,
        "epoch": 0.3169631348285641,
        "step": 2459
    },
    {
        "loss": 1.9079,
        "grad_norm": 2.5222647190093994,
        "learning_rate": 6.835174077881979e-07,
        "epoch": 0.317092034029389,
        "step": 2460
    },
    {
        "loss": 1.8294,
        "grad_norm": 1.8073625564575195,
        "learning_rate": 6.735251813445198e-07,
        "epoch": 0.317220933230214,
        "step": 2461
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.5873286724090576,
        "learning_rate": 6.636060357797624e-07,
        "epoch": 0.3173498324310389,
        "step": 2462
    },
    {
        "loss": 2.208,
        "grad_norm": 1.7216911315917969,
        "learning_rate": 6.537599857898813e-07,
        "epoch": 0.3174787316318639,
        "step": 2463
    },
    {
        "loss": 2.2175,
        "grad_norm": 2.5829219818115234,
        "learning_rate": 6.439870459625408e-07,
        "epoch": 0.31760763083268884,
        "step": 2464
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.588181734085083,
        "learning_rate": 6.342872307770919e-07,
        "epoch": 0.3177365300335138,
        "step": 2465
    },
    {
        "loss": 1.1316,
        "grad_norm": 2.6922390460968018,
        "learning_rate": 6.246605546045281e-07,
        "epoch": 0.31786542923433875,
        "step": 2466
    },
    {
        "loss": 2.3919,
        "grad_norm": 2.0994656085968018,
        "learning_rate": 6.151070317075014e-07,
        "epoch": 0.3179943284351637,
        "step": 2467
    },
    {
        "loss": 1.8122,
        "grad_norm": 2.6392788887023926,
        "learning_rate": 6.056266762402729e-07,
        "epoch": 0.31812322763598866,
        "step": 2468
    },
    {
        "loss": 2.1766,
        "grad_norm": 1.6925488710403442,
        "learning_rate": 5.962195022487071e-07,
        "epoch": 0.3182521268368136,
        "step": 2469
    },
    {
        "loss": 2.1756,
        "grad_norm": 1.964705228805542,
        "learning_rate": 5.86885523670222e-07,
        "epoch": 0.31838102603763857,
        "step": 2470
    },
    {
        "loss": 1.6999,
        "grad_norm": 2.6414196491241455,
        "learning_rate": 5.776247543338275e-07,
        "epoch": 0.3185099252384635,
        "step": 2471
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.2561330795288086,
        "learning_rate": 5.684372079600375e-07,
        "epoch": 0.3186388244392885,
        "step": 2472
    },
    {
        "loss": 2.3212,
        "grad_norm": 1.7393132448196411,
        "learning_rate": 5.593228981608967e-07,
        "epoch": 0.3187677236401134,
        "step": 2473
    },
    {
        "loss": 2.2069,
        "grad_norm": 1.7036311626434326,
        "learning_rate": 5.502818384399367e-07,
        "epoch": 0.3188966228409384,
        "step": 2474
    },
    {
        "loss": 2.3983,
        "grad_norm": 1.9451133012771606,
        "learning_rate": 5.413140421921703e-07,
        "epoch": 0.3190255220417633,
        "step": 2475
    },
    {
        "loss": 2.275,
        "grad_norm": 1.4353892803192139,
        "learning_rate": 5.32419522704064e-07,
        "epoch": 0.3191544212425883,
        "step": 2476
    },
    {
        "loss": 2.2821,
        "grad_norm": 1.8306164741516113,
        "learning_rate": 5.235982931535044e-07,
        "epoch": 0.31928332044341323,
        "step": 2477
    },
    {
        "loss": 1.8814,
        "grad_norm": 1.9881542921066284,
        "learning_rate": 5.148503666098259e-07,
        "epoch": 0.3194122196442382,
        "step": 2478
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.6325054168701172,
        "learning_rate": 5.061757560337166e-07,
        "epoch": 0.31954111884506314,
        "step": 2479
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.8669122457504272,
        "learning_rate": 4.975744742772848e-07,
        "epoch": 0.3196700180458881,
        "step": 2480
    },
    {
        "loss": 1.6059,
        "grad_norm": 2.3591978549957275,
        "learning_rate": 4.89046534083959e-07,
        "epoch": 0.31979891724671305,
        "step": 2481
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.505927324295044,
        "learning_rate": 4.805919480885323e-07,
        "epoch": 0.31992781644753804,
        "step": 2482
    },
    {
        "loss": 2.0854,
        "grad_norm": 1.9723650217056274,
        "learning_rate": 4.722107288171074e-07,
        "epoch": 0.32005671564836297,
        "step": 2483
    },
    {
        "loss": 2.3659,
        "grad_norm": 1.17603600025177,
        "learning_rate": 4.639028886870955e-07,
        "epoch": 0.32018561484918795,
        "step": 2484
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.0011990070343018,
        "learning_rate": 4.556684400071731e-07,
        "epoch": 0.3203145140500129,
        "step": 2485
    },
    {
        "loss": 2.3321,
        "grad_norm": 1.4279180765151978,
        "learning_rate": 4.475073949773034e-07,
        "epoch": 0.32044341325083786,
        "step": 2486
    },
    {
        "loss": 1.635,
        "grad_norm": 1.6140880584716797,
        "learning_rate": 4.394197656886867e-07,
        "epoch": 0.3205723124516628,
        "step": 2487
    },
    {
        "loss": 1.5684,
        "grad_norm": 2.88891863822937,
        "learning_rate": 4.314055641237602e-07,
        "epoch": 0.32070121165248777,
        "step": 2488
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.317199945449829,
        "learning_rate": 4.234648021561538e-07,
        "epoch": 0.3208301108533127,
        "step": 2489
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.7410047054290771,
        "learning_rate": 4.155974915507066e-07,
        "epoch": 0.3209590100541377,
        "step": 2490
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.3071184158325195,
        "learning_rate": 4.078036439634336e-07,
        "epoch": 0.3210879092549626,
        "step": 2491
    },
    {
        "loss": 2.5151,
        "grad_norm": 1.7004886865615845,
        "learning_rate": 4.000832709415092e-07,
        "epoch": 0.3212168084557876,
        "step": 2492
    },
    {
        "loss": 2.007,
        "grad_norm": 2.0998682975769043,
        "learning_rate": 3.9243638392323367e-07,
        "epoch": 0.3213457076566125,
        "step": 2493
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.3355093002319336,
        "learning_rate": 3.848629942380555e-07,
        "epoch": 0.3214746068574375,
        "step": 2494
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.3090925216674805,
        "learning_rate": 3.773631131065158e-07,
        "epoch": 0.32160350605826243,
        "step": 2495
    },
    {
        "loss": 2.429,
        "grad_norm": 2.214399814605713,
        "learning_rate": 3.699367516402541e-07,
        "epoch": 0.3217324052590874,
        "step": 2496
    },
    {
        "loss": 2.0857,
        "grad_norm": 2.856123208999634,
        "learning_rate": 3.625839208419857e-07,
        "epoch": 0.32186130445991235,
        "step": 2497
    },
    {
        "loss": 2.256,
        "grad_norm": 1.8402513265609741,
        "learning_rate": 3.553046316054742e-07,
        "epoch": 0.32199020366073733,
        "step": 2498
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.5530927181243896,
        "learning_rate": 3.480988947155428e-07,
        "epoch": 0.32211910286156226,
        "step": 2499
    },
    {
        "loss": 2.028,
        "grad_norm": 2.5465500354766846,
        "learning_rate": 3.4096672084802385e-07,
        "epoch": 0.32224800206238724,
        "step": 2500
    },
    {
        "loss": 1.8281,
        "grad_norm": 2.392407178878784,
        "learning_rate": 3.339081205697703e-07,
        "epoch": 0.32237690126321217,
        "step": 2501
    },
    {
        "loss": 2.4615,
        "grad_norm": 1.6779485940933228,
        "learning_rate": 3.269231043386334e-07,
        "epoch": 0.3225058004640371,
        "step": 2502
    },
    {
        "loss": 2.478,
        "grad_norm": 1.653923749923706,
        "learning_rate": 3.2001168250343496e-07,
        "epoch": 0.3226346996648621,
        "step": 2503
    },
    {
        "loss": 1.6103,
        "grad_norm": 2.926917314529419,
        "learning_rate": 3.1317386530397284e-07,
        "epoch": 0.322763598865687,
        "step": 2504
    },
    {
        "loss": 2.2589,
        "grad_norm": 2.3701696395874023,
        "learning_rate": 3.064096628709767e-07,
        "epoch": 0.322892498066512,
        "step": 2505
    },
    {
        "loss": 2.0242,
        "grad_norm": 1.4101378917694092,
        "learning_rate": 2.9971908522613e-07,
        "epoch": 0.3230213972673369,
        "step": 2506
    },
    {
        "loss": 2.2395,
        "grad_norm": 2.326483964920044,
        "learning_rate": 2.9310214228202013e-07,
        "epoch": 0.3231502964681619,
        "step": 2507
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.1199018955230713,
        "learning_rate": 2.8655884384214957e-07,
        "epoch": 0.32327919566898683,
        "step": 2508
    },
    {
        "loss": 2.1673,
        "grad_norm": 1.422124981880188,
        "learning_rate": 2.800891996009025e-07,
        "epoch": 0.3234080948698118,
        "step": 2509
    },
    {
        "loss": 2.6655,
        "grad_norm": 1.841962456703186,
        "learning_rate": 2.7369321914354486e-07,
        "epoch": 0.32353699407063674,
        "step": 2510
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.2883669137954712,
        "learning_rate": 2.67370911946202e-07,
        "epoch": 0.3236658932714617,
        "step": 2511
    },
    {
        "loss": 2.5353,
        "grad_norm": 3.1544551849365234,
        "learning_rate": 2.611222873758479e-07,
        "epoch": 0.32379479247228665,
        "step": 2512
    },
    {
        "loss": 1.8669,
        "grad_norm": 2.1949288845062256,
        "learning_rate": 2.549473546902936e-07,
        "epoch": 0.32392369167311164,
        "step": 2513
    },
    {
        "loss": 1.7867,
        "grad_norm": 2.4686686992645264,
        "learning_rate": 2.4884612303815447e-07,
        "epoch": 0.32405259087393656,
        "step": 2514
    },
    {
        "loss": 2.1069,
        "grad_norm": 1.6695268154144287,
        "learning_rate": 2.428186014588718e-07,
        "epoch": 0.32418149007476155,
        "step": 2515
    },
    {
        "loss": 2.0216,
        "grad_norm": 1.4805530309677124,
        "learning_rate": 2.3686479888267443e-07,
        "epoch": 0.3243103892755865,
        "step": 2516
    },
    {
        "loss": 1.9177,
        "grad_norm": 1.7011796236038208,
        "learning_rate": 2.3098472413056182e-07,
        "epoch": 0.32443928847641146,
        "step": 2517
    },
    {
        "loss": 1.6169,
        "grad_norm": 2.8093650341033936,
        "learning_rate": 2.2517838591430973e-07,
        "epoch": 0.3245681876772364,
        "step": 2518
    },
    {
        "loss": 1.804,
        "grad_norm": 2.064236640930176,
        "learning_rate": 2.194457928364424e-07,
        "epoch": 0.32469708687806137,
        "step": 2519
    },
    {
        "loss": 2.15,
        "grad_norm": 1.7609914541244507,
        "learning_rate": 2.1378695339023257e-07,
        "epoch": 0.3248259860788863,
        "step": 2520
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.6124420166015625,
        "learning_rate": 2.0820187595966824e-07,
        "epoch": 0.3249548852797113,
        "step": 2521
    },
    {
        "loss": 2.1115,
        "grad_norm": 1.0955820083618164,
        "learning_rate": 2.0269056881946358e-07,
        "epoch": 0.3250837844805362,
        "step": 2522
    },
    {
        "loss": 1.871,
        "grad_norm": 1.5183155536651611,
        "learning_rate": 1.9725304013504253e-07,
        "epoch": 0.3252126836813612,
        "step": 2523
    },
    {
        "loss": 2.2256,
        "grad_norm": 2.05367374420166,
        "learning_rate": 1.9188929796249976e-07,
        "epoch": 0.3253415828821861,
        "step": 2524
    },
    {
        "loss": 2.3279,
        "grad_norm": 1.3513636589050293,
        "learning_rate": 1.865993502486285e-07,
        "epoch": 0.3254704820830111,
        "step": 2525
    },
    {
        "loss": 2.005,
        "grad_norm": 2.173417568206787,
        "learning_rate": 1.8138320483088723e-07,
        "epoch": 0.32559938128383603,
        "step": 2526
    },
    {
        "loss": 2.2566,
        "grad_norm": 2.623537302017212,
        "learning_rate": 1.7624086943738293e-07,
        "epoch": 0.325728280484661,
        "step": 2527
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.366913080215454,
        "learning_rate": 1.711723516868713e-07,
        "epoch": 0.32585717968548594,
        "step": 2528
    },
    {
        "loss": 1.5497,
        "grad_norm": 2.9174160957336426,
        "learning_rate": 1.6617765908873983e-07,
        "epoch": 0.3259860788863109,
        "step": 2529
    },
    {
        "loss": 1.3964,
        "grad_norm": 3.097085475921631,
        "learning_rate": 1.6125679904301361e-07,
        "epoch": 0.32611497808713585,
        "step": 2530
    },
    {
        "loss": 1.387,
        "grad_norm": 2.5772762298583984,
        "learning_rate": 1.5640977884029962e-07,
        "epoch": 0.32624387728796084,
        "step": 2531
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.3133567571640015,
        "learning_rate": 1.5163660566183678e-07,
        "epoch": 0.32637277648878577,
        "step": 2532
    },
    {
        "loss": 2.2418,
        "grad_norm": 1.7802541255950928,
        "learning_rate": 1.469372865794294e-07,
        "epoch": 0.32650167568961075,
        "step": 2533
    },
    {
        "loss": 2.2001,
        "grad_norm": 2.084702968597412,
        "learning_rate": 1.423118285554803e-07,
        "epoch": 0.3266305748904357,
        "step": 2534
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.5820034742355347,
        "learning_rate": 1.3776023844294106e-07,
        "epoch": 0.32675947409126066,
        "step": 2535
    },
    {
        "loss": 2.2887,
        "grad_norm": 1.6213804483413696,
        "learning_rate": 1.332825229853507e-07,
        "epoch": 0.3268883732920856,
        "step": 2536
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.3047397136688232,
        "learning_rate": 1.2887868881676923e-07,
        "epoch": 0.32701727249291057,
        "step": 2537
    },
    {
        "loss": 2.2756,
        "grad_norm": 2.5104846954345703,
        "learning_rate": 1.245487424618108e-07,
        "epoch": 0.3271461716937355,
        "step": 2538
    },
    {
        "loss": 2.2885,
        "grad_norm": 1.6209356784820557,
        "learning_rate": 1.2029269033561607e-07,
        "epoch": 0.3272750708945604,
        "step": 2539
    },
    {
        "loss": 2.0183,
        "grad_norm": 1.727852702140808,
        "learning_rate": 1.1611053874384659e-07,
        "epoch": 0.3274039700953854,
        "step": 2540
    },
    {
        "loss": 1.6687,
        "grad_norm": 3.192007541656494,
        "learning_rate": 1.1200229388267369e-07,
        "epoch": 0.32753286929621034,
        "step": 2541
    },
    {
        "loss": 2.027,
        "grad_norm": 1.7695847749710083,
        "learning_rate": 1.0796796183877855e-07,
        "epoch": 0.3276617684970353,
        "step": 2542
    },
    {
        "loss": 1.8778,
        "grad_norm": 2.485440969467163,
        "learning_rate": 1.0400754858931328e-07,
        "epoch": 0.32779066769786025,
        "step": 2543
    },
    {
        "loss": 1.7991,
        "grad_norm": 2.2620561122894287,
        "learning_rate": 1.0012106000193422e-07,
        "epoch": 0.32791956689868523,
        "step": 2544
    },
    {
        "loss": 1.849,
        "grad_norm": 1.7953667640686035,
        "learning_rate": 9.63085018347687e-08,
        "epoch": 0.32804846609951016,
        "step": 2545
    },
    {
        "loss": 2.3916,
        "grad_norm": 1.2429882287979126,
        "learning_rate": 9.25698797364094e-08,
        "epoch": 0.32817736530033514,
        "step": 2546
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.325079917907715,
        "learning_rate": 8.89051992458978e-08,
        "epoch": 0.32830626450116007,
        "step": 2547
    },
    {
        "loss": 1.2623,
        "grad_norm": 3.2899560928344727,
        "learning_rate": 8.531446579274071e-08,
        "epoch": 0.32843516370198506,
        "step": 2548
    },
    {
        "loss": 2.2187,
        "grad_norm": 1.7664103507995605,
        "learning_rate": 8.17976846968771e-08,
        "epoch": 0.32856406290281,
        "step": 2549
    },
    {
        "loss": 1.891,
        "grad_norm": 2.1447677612304688,
        "learning_rate": 7.835486116868352e-08,
        "epoch": 0.32869296210363497,
        "step": 2550
    },
    {
        "loss": 2.2404,
        "grad_norm": 1.9131863117218018,
        "learning_rate": 7.498600030895752e-08,
        "epoch": 0.3288218613044599,
        "step": 2551
    },
    {
        "loss": 2.035,
        "grad_norm": 1.711563229560852,
        "learning_rate": 7.169110710892324e-08,
        "epoch": 0.3289507605052849,
        "step": 2552
    },
    {
        "loss": 2.099,
        "grad_norm": 1.6328128576278687,
        "learning_rate": 6.847018645021464e-08,
        "epoch": 0.3290796597061098,
        "step": 2553
    },
    {
        "loss": 2.4517,
        "grad_norm": 1.9857897758483887,
        "learning_rate": 6.532324310485894e-08,
        "epoch": 0.3292085589069348,
        "step": 2554
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.2844067811965942,
        "learning_rate": 6.225028173529878e-08,
        "epoch": 0.3293374581077597,
        "step": 2555
    },
    {
        "loss": 2.2499,
        "grad_norm": 2.0079288482666016,
        "learning_rate": 5.9251306894358985e-08,
        "epoch": 0.3294663573085847,
        "step": 2556
    },
    {
        "loss": 1.9292,
        "grad_norm": 2.699320077896118,
        "learning_rate": 5.6326323025235326e-08,
        "epoch": 0.32959525650940963,
        "step": 2557
    },
    {
        "loss": 1.9507,
        "grad_norm": 2.3450071811676025,
        "learning_rate": 5.347533446151687e-08,
        "epoch": 0.3297241557102346,
        "step": 2558
    },
    {
        "loss": 1.953,
        "grad_norm": 1.4361263513565063,
        "learning_rate": 5.069834542715812e-08,
        "epoch": 0.32985305491105954,
        "step": 2559
    },
    {
        "loss": 1.7438,
        "grad_norm": 1.9934656620025635,
        "learning_rate": 4.799536003647353e-08,
        "epoch": 0.3299819541118845,
        "step": 2560
    },
    {
        "loss": 2.117,
        "grad_norm": 2.665663719177246,
        "learning_rate": 4.536638229414858e-08,
        "epoch": 0.33011085331270945,
        "step": 2561
    },
    {
        "loss": 2.1251,
        "grad_norm": 2.1106674671173096,
        "learning_rate": 4.28114160952009e-08,
        "epoch": 0.33023975251353443,
        "step": 2562
    },
    {
        "loss": 2.301,
        "grad_norm": 1.384229302406311,
        "learning_rate": 4.033046522500805e-08,
        "epoch": 0.33036865171435936,
        "step": 2563
    },
    {
        "loss": 2.3621,
        "grad_norm": 2.178368091583252,
        "learning_rate": 3.792353335928533e-08,
        "epoch": 0.33049755091518435,
        "step": 2564
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.8594253063201904,
        "learning_rate": 3.55906240640802e-08,
        "epoch": 0.3306264501160093,
        "step": 2565
    },
    {
        "loss": 2.3661,
        "grad_norm": 1.7058883905410767,
        "learning_rate": 3.3331740795783384e-08,
        "epoch": 0.33075534931683426,
        "step": 2566
    },
    {
        "loss": 2.3355,
        "grad_norm": 1.504019021987915,
        "learning_rate": 3.1146886901090025e-08,
        "epoch": 0.3308842485176592,
        "step": 2567
    },
    {
        "loss": 2.208,
        "grad_norm": 1.8096280097961426,
        "learning_rate": 2.903606561702743e-08,
        "epoch": 0.33101314771848417,
        "step": 2568
    },
    {
        "loss": 2.1945,
        "grad_norm": 2.051905393600464,
        "learning_rate": 2.6999280070938436e-08,
        "epoch": 0.3311420469193091,
        "step": 2569
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.4852598905563354,
        "learning_rate": 2.5036533280470285e-08,
        "epoch": 0.3312709461201341,
        "step": 2570
    },
    {
        "loss": 2.1342,
        "grad_norm": 1.6289554834365845,
        "learning_rate": 2.314782815358574e-08,
        "epoch": 0.331399845320959,
        "step": 2571
    },
    {
        "loss": 1.6811,
        "grad_norm": 1.3302419185638428,
        "learning_rate": 2.1333167488535312e-08,
        "epoch": 0.331528744521784,
        "step": 2572
    },
    {
        "loss": 1.5867,
        "grad_norm": 2.495619773864746,
        "learning_rate": 1.959255397388504e-08,
        "epoch": 0.3316576437226089,
        "step": 2573
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.2392749786376953,
        "learning_rate": 1.7925990188472075e-08,
        "epoch": 0.33178654292343385,
        "step": 2574
    },
    {
        "loss": 1.1726,
        "grad_norm": 2.544570207595825,
        "learning_rate": 1.633347860144907e-08,
        "epoch": 0.33191544212425883,
        "step": 2575
    },
    {
        "loss": 2.4155,
        "grad_norm": 1.889263391494751,
        "learning_rate": 1.4815021572228694e-08,
        "epoch": 0.33204434132508376,
        "step": 2576
    },
    {
        "loss": 2.0907,
        "grad_norm": 2.477177381515503,
        "learning_rate": 1.3370621350533575e-08,
        "epoch": 0.33217324052590874,
        "step": 2577
    },
    {
        "loss": 1.7832,
        "grad_norm": 2.1059513092041016,
        "learning_rate": 1.2000280076335246e-08,
        "epoch": 0.33230213972673367,
        "step": 2578
    },
    {
        "loss": 2.2125,
        "grad_norm": 2.1964480876922607,
        "learning_rate": 1.0703999779915209e-08,
        "epoch": 0.33243103892755865,
        "step": 2579
    },
    {
        "loss": 2.067,
        "grad_norm": 1.957476019859314,
        "learning_rate": 9.481782381792759e-09,
        "epoch": 0.3325599381283836,
        "step": 2580
    },
    {
        "loss": 2.2609,
        "grad_norm": 1.6783818006515503,
        "learning_rate": 8.33362969278606e-09,
        "epoch": 0.33268883732920856,
        "step": 2581
    },
    {
        "loss": 1.3777,
        "grad_norm": 2.7157890796661377,
        "learning_rate": 7.259543413962178e-09,
        "epoch": 0.3328177365300335,
        "step": 2582
    },
    {
        "loss": 1.2746,
        "grad_norm": 2.3566102981567383,
        "learning_rate": 6.259525136670386e-09,
        "epoch": 0.3329466357308585,
        "step": 2583
    },
    {
        "loss": 2.0205,
        "grad_norm": 2.013650417327881,
        "learning_rate": 5.3335763424977595e-09,
        "epoch": 0.3330755349316834,
        "step": 2584
    },
    {
        "loss": 1.3733,
        "grad_norm": 4.3643012046813965,
        "learning_rate": 4.481698403324685e-09,
        "epoch": 0.3332044341325084,
        "step": 2585
    },
    {
        "loss": 1.7336,
        "grad_norm": 1.9744659662246704,
        "learning_rate": 3.7038925812582505e-09,
        "epoch": 0.3333333333333333,
        "step": 2586
    },
    {
        "loss": 2.1644,
        "grad_norm": 1.9539289474487305,
        "learning_rate": 3.0001600286877483e-09,
        "epoch": 0.3334622325341583,
        "step": 2587
    },
    {
        "loss": 2.2058,
        "grad_norm": 1.363904356956482,
        "learning_rate": 2.3705017882347246e-09,
        "epoch": 0.3335911317349832,
        "step": 2588
    },
    {
        "loss": 1.6512,
        "grad_norm": 1.7644925117492676,
        "learning_rate": 1.8149187927918309e-09,
        "epoch": 0.3337200309358082,
        "step": 2589
    },
    {
        "loss": 1.2549,
        "grad_norm": 2.507446765899658,
        "learning_rate": 1.333411865495071e-09,
        "epoch": 0.33384893013663314,
        "step": 2590
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.009087085723877,
        "learning_rate": 9.25981719734903e-10,
        "epoch": 0.3339778293374581,
        "step": 2591
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.2358224391937256,
        "learning_rate": 5.926289591506873e-10,
        "epoch": 0.33410672853828305,
        "step": 2592
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.726693034172058,
        "learning_rate": 3.3335407762513646e-10,
        "epoch": 0.33423562773910803,
        "step": 2593
    },
    {
        "loss": 1.8045,
        "grad_norm": 2.390700578689575,
        "learning_rate": 1.4815745929541713e-10,
        "epoch": 0.33436452693993296,
        "step": 2594
    },
    {
        "loss": 2.3106,
        "grad_norm": 1.7361187934875488,
        "learning_rate": 3.703937854204753e-11,
        "epoch": 0.33449342614075794,
        "step": 2595
    },
    {
        "loss": 1.5028,
        "grad_norm": 2.7096211910247803,
        "learning_rate": 0.0001,
        "epoch": 0.33462232534158287,
        "step": 2596
    },
    {
        "loss": 1.8582,
        "grad_norm": 2.200626850128174,
        "learning_rate": 9.999996296062147e-05,
        "epoch": 0.33475122454240785,
        "step": 2597
    },
    {
        "loss": 1.9215,
        "grad_norm": 2.5171170234680176,
        "learning_rate": 9.999985184254071e-05,
        "epoch": 0.3348801237432328,
        "step": 2598
    },
    {
        "loss": 1.9781,
        "grad_norm": 2.99465274810791,
        "learning_rate": 9.999966664592237e-05,
        "epoch": 0.33500902294405777,
        "step": 2599
    },
    {
        "loss": 1.9997,
        "grad_norm": 2.122739791870117,
        "learning_rate": 9.999940737104086e-05,
        "epoch": 0.3351379221448827,
        "step": 2600
    },
    {
        "loss": 1.8146,
        "grad_norm": 1.5615572929382324,
        "learning_rate": 9.999907401828026e-05,
        "epoch": 0.3352668213457077,
        "step": 2601
    },
    {
        "loss": 2.5802,
        "grad_norm": 2.062441349029541,
        "learning_rate": 9.99986665881345e-05,
        "epoch": 0.3353957205465326,
        "step": 2602
    },
    {
        "loss": 2.1838,
        "grad_norm": 2.264526844024658,
        "learning_rate": 9.999818508120722e-05,
        "epoch": 0.3355246197473576,
        "step": 2603
    },
    {
        "loss": 1.9016,
        "grad_norm": 2.229969024658203,
        "learning_rate": 9.999762949821176e-05,
        "epoch": 0.3356535189481825,
        "step": 2604
    },
    {
        "loss": 2.5376,
        "grad_norm": 1.616703748703003,
        "learning_rate": 9.999699983997131e-05,
        "epoch": 0.3357824181490075,
        "step": 2605
    },
    {
        "loss": 1.823,
        "grad_norm": 1.2479196786880493,
        "learning_rate": 9.999629610741875e-05,
        "epoch": 0.33591131734983243,
        "step": 2606
    },
    {
        "loss": 2.3198,
        "grad_norm": 2.0897104740142822,
        "learning_rate": 9.999551830159668e-05,
        "epoch": 0.3360402165506574,
        "step": 2607
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.1793298721313477,
        "learning_rate": 9.999466642365752e-05,
        "epoch": 0.33616911575148234,
        "step": 2608
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.5869176387786865,
        "learning_rate": 9.999374047486334e-05,
        "epoch": 0.3362980149523073,
        "step": 2609
    },
    {
        "loss": 2.335,
        "grad_norm": 1.9574793577194214,
        "learning_rate": 9.999274045658605e-05,
        "epoch": 0.33642691415313225,
        "step": 2610
    },
    {
        "loss": 1.5344,
        "grad_norm": 2.4251222610473633,
        "learning_rate": 9.999166637030721e-05,
        "epoch": 0.3365558133539572,
        "step": 2611
    },
    {
        "loss": 2.3082,
        "grad_norm": 1.5200178623199463,
        "learning_rate": 9.999051821761821e-05,
        "epoch": 0.33668471255478216,
        "step": 2612
    },
    {
        "loss": 1.8454,
        "grad_norm": 2.5092248916625977,
        "learning_rate": 9.998929600022008e-05,
        "epoch": 0.3368136117556071,
        "step": 2613
    },
    {
        "loss": 2.2048,
        "grad_norm": 2.2141339778900146,
        "learning_rate": 9.998799971992367e-05,
        "epoch": 0.3369425109564321,
        "step": 2614
    },
    {
        "loss": 2.2161,
        "grad_norm": 2.156481981277466,
        "learning_rate": 9.998662937864947e-05,
        "epoch": 0.337071410157257,
        "step": 2615
    },
    {
        "loss": 2.3886,
        "grad_norm": 2.0204906463623047,
        "learning_rate": 9.998518497842777e-05,
        "epoch": 0.337200309358082,
        "step": 2616
    },
    {
        "loss": 2.7076,
        "grad_norm": 1.242526650428772,
        "learning_rate": 9.998366652139856e-05,
        "epoch": 0.3373292085589069,
        "step": 2617
    },
    {
        "loss": 1.3247,
        "grad_norm": 2.8130807876586914,
        "learning_rate": 9.998207400981154e-05,
        "epoch": 0.3374581077597319,
        "step": 2618
    },
    {
        "loss": 2.0838,
        "grad_norm": 2.193432092666626,
        "learning_rate": 9.998040744602613e-05,
        "epoch": 0.3375870069605568,
        "step": 2619
    },
    {
        "loss": 2.4967,
        "grad_norm": 1.29916512966156,
        "learning_rate": 9.997866683251147e-05,
        "epoch": 0.3377159061613818,
        "step": 2620
    },
    {
        "loss": 2.1862,
        "grad_norm": 2.2235655784606934,
        "learning_rate": 9.997685217184643e-05,
        "epoch": 0.33784480536220673,
        "step": 2621
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.9441511631011963,
        "learning_rate": 9.997496346671953e-05,
        "epoch": 0.3379737045630317,
        "step": 2622
    },
    {
        "loss": 2.2114,
        "grad_norm": 2.040280818939209,
        "learning_rate": 9.997300071992907e-05,
        "epoch": 0.33810260376385665,
        "step": 2623
    },
    {
        "loss": 0.8044,
        "grad_norm": 3.26745867729187,
        "learning_rate": 9.997096393438298e-05,
        "epoch": 0.33823150296468163,
        "step": 2624
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.0247232913970947,
        "learning_rate": 9.996885311309891e-05,
        "epoch": 0.33836040216550656,
        "step": 2625
    },
    {
        "loss": 1.8726,
        "grad_norm": 1.5838571786880493,
        "learning_rate": 9.996666825920422e-05,
        "epoch": 0.33848930136633154,
        "step": 2626
    },
    {
        "loss": 2.0708,
        "grad_norm": 1.8724758625030518,
        "learning_rate": 9.996440937593592e-05,
        "epoch": 0.33861820056715647,
        "step": 2627
    },
    {
        "loss": 1.5867,
        "grad_norm": 2.675637722015381,
        "learning_rate": 9.996207646664072e-05,
        "epoch": 0.33874709976798145,
        "step": 2628
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.8139400482177734,
        "learning_rate": 9.9959669534775e-05,
        "epoch": 0.3388759989688064,
        "step": 2629
    },
    {
        "loss": 1.246,
        "grad_norm": 3.3735570907592773,
        "learning_rate": 9.995718858390482e-05,
        "epoch": 0.33900489816963136,
        "step": 2630
    },
    {
        "loss": 1.8625,
        "grad_norm": 2.317150592803955,
        "learning_rate": 9.995463361770585e-05,
        "epoch": 0.3391337973704563,
        "step": 2631
    },
    {
        "loss": 2.316,
        "grad_norm": 1.5854300260543823,
        "learning_rate": 9.995200463996352e-05,
        "epoch": 0.3392626965712813,
        "step": 2632
    },
    {
        "loss": 2.4133,
        "grad_norm": 1.8993982076644897,
        "learning_rate": 9.994930165457285e-05,
        "epoch": 0.3393915957721062,
        "step": 2633
    },
    {
        "loss": 1.7699,
        "grad_norm": 1.7124768495559692,
        "learning_rate": 9.99465246655385e-05,
        "epoch": 0.3395204949729312,
        "step": 2634
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.5200899839401245,
        "learning_rate": 9.994367367697478e-05,
        "epoch": 0.3396493941737561,
        "step": 2635
    },
    {
        "loss": 2.2917,
        "grad_norm": 1.8349006175994873,
        "learning_rate": 9.994074869310566e-05,
        "epoch": 0.3397782933745811,
        "step": 2636
    },
    {
        "loss": 1.9587,
        "grad_norm": 1.4453656673431396,
        "learning_rate": 9.993774971826471e-05,
        "epoch": 0.339907192575406,
        "step": 2637
    },
    {
        "loss": 2.3454,
        "grad_norm": 1.586363673210144,
        "learning_rate": 9.993467675689513e-05,
        "epoch": 0.340036091776231,
        "step": 2638
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.295330286026001,
        "learning_rate": 9.993152981354979e-05,
        "epoch": 0.34016499097705594,
        "step": 2639
    },
    {
        "loss": 2.4594,
        "grad_norm": 1.9691282510757446,
        "learning_rate": 9.992830889289109e-05,
        "epoch": 0.3402938901778809,
        "step": 2640
    },
    {
        "loss": 1.8359,
        "grad_norm": 2.771939516067505,
        "learning_rate": 9.992501399969106e-05,
        "epoch": 0.34042278937870585,
        "step": 2641
    },
    {
        "loss": 1.6691,
        "grad_norm": 1.9978396892547607,
        "learning_rate": 9.992164513883132e-05,
        "epoch": 0.34055168857953083,
        "step": 2642
    },
    {
        "loss": 1.8897,
        "grad_norm": 2.368771553039551,
        "learning_rate": 9.991820231530313e-05,
        "epoch": 0.34068058778035576,
        "step": 2643
    },
    {
        "loss": 2.4395,
        "grad_norm": 1.4782109260559082,
        "learning_rate": 9.991468553420726e-05,
        "epoch": 0.34080948698118074,
        "step": 2644
    },
    {
        "loss": 2.4624,
        "grad_norm": 1.217484474182129,
        "learning_rate": 9.99110948007541e-05,
        "epoch": 0.34093838618200567,
        "step": 2645
    },
    {
        "loss": 1.9008,
        "grad_norm": 2.4578707218170166,
        "learning_rate": 9.99074301202636e-05,
        "epoch": 0.34106728538283065,
        "step": 2646
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.84282386302948,
        "learning_rate": 9.990369149816523e-05,
        "epoch": 0.3411961845836556,
        "step": 2647
    },
    {
        "loss": 2.1677,
        "grad_norm": 2.2589564323425293,
        "learning_rate": 9.989987893999807e-05,
        "epoch": 0.3413250837844805,
        "step": 2648
    },
    {
        "loss": 2.4836,
        "grad_norm": 1.4733054637908936,
        "learning_rate": 9.989599245141069e-05,
        "epoch": 0.3414539829853055,
        "step": 2649
    },
    {
        "loss": 2.3178,
        "grad_norm": 1.8061039447784424,
        "learning_rate": 9.989203203816123e-05,
        "epoch": 0.3415828821861304,
        "step": 2650
    },
    {
        "loss": 1.7953,
        "grad_norm": 2.4749672412872314,
        "learning_rate": 9.988799770611732e-05,
        "epoch": 0.3417117813869554,
        "step": 2651
    },
    {
        "loss": 2.1045,
        "grad_norm": 1.543439507484436,
        "learning_rate": 9.988388946125615e-05,
        "epoch": 0.34184068058778033,
        "step": 2652
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.4533417224884033,
        "learning_rate": 9.987970730966438e-05,
        "epoch": 0.3419695797886053,
        "step": 2653
    },
    {
        "loss": 2.0819,
        "grad_norm": 2.60854172706604,
        "learning_rate": 9.987545125753819e-05,
        "epoch": 0.34209847898943024,
        "step": 2654
    },
    {
        "loss": 2.3827,
        "grad_norm": 1.4135661125183105,
        "learning_rate": 9.987112131118323e-05,
        "epoch": 0.3422273781902552,
        "step": 2655
    },
    {
        "loss": 1.8769,
        "grad_norm": 2.022094488143921,
        "learning_rate": 9.986671747701466e-05,
        "epoch": 0.34235627739108015,
        "step": 2656
    },
    {
        "loss": 2.3513,
        "grad_norm": 1.8730082511901855,
        "learning_rate": 9.986223976155706e-05,
        "epoch": 0.34248517659190514,
        "step": 2657
    },
    {
        "loss": 1.9547,
        "grad_norm": 3.096998453140259,
        "learning_rate": 9.985768817144453e-05,
        "epoch": 0.34261407579273007,
        "step": 2658
    },
    {
        "loss": 2.1834,
        "grad_norm": 1.9171850681304932,
        "learning_rate": 9.985306271342058e-05,
        "epoch": 0.34274297499355505,
        "step": 2659
    },
    {
        "loss": 2.4584,
        "grad_norm": 1.3699729442596436,
        "learning_rate": 9.984836339433816e-05,
        "epoch": 0.34287187419438,
        "step": 2660
    },
    {
        "loss": 2.1393,
        "grad_norm": 2.322737216949463,
        "learning_rate": 9.98435902211597e-05,
        "epoch": 0.34300077339520496,
        "step": 2661
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.685788869857788,
        "learning_rate": 9.983874320095698e-05,
        "epoch": 0.3431296725960299,
        "step": 2662
    },
    {
        "loss": 2.4597,
        "grad_norm": 1.6868994235992432,
        "learning_rate": 9.983382234091126e-05,
        "epoch": 0.34325857179685487,
        "step": 2663
    },
    {
        "loss": 2.3568,
        "grad_norm": 1.5341233015060425,
        "learning_rate": 9.982882764831315e-05,
        "epoch": 0.3433874709976798,
        "step": 2664
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.9994996786117554,
        "learning_rate": 9.982375913056262e-05,
        "epoch": 0.3435163701985048,
        "step": 2665
    },
    {
        "loss": 2.208,
        "grad_norm": 1.651817798614502,
        "learning_rate": 9.981861679516912e-05,
        "epoch": 0.3436452693993297,
        "step": 2666
    },
    {
        "loss": 2.419,
        "grad_norm": 2.192502498626709,
        "learning_rate": 9.981340064975137e-05,
        "epoch": 0.3437741686001547,
        "step": 2667
    },
    {
        "loss": 1.8806,
        "grad_norm": 2.0490353107452393,
        "learning_rate": 9.98081107020375e-05,
        "epoch": 0.3439030678009796,
        "step": 2668
    },
    {
        "loss": 0.9289,
        "grad_norm": 4.970241546630859,
        "learning_rate": 9.980274695986496e-05,
        "epoch": 0.3440319670018046,
        "step": 2669
    },
    {
        "loss": 1.7012,
        "grad_norm": 2.2068796157836914,
        "learning_rate": 9.979730943118054e-05,
        "epoch": 0.34416086620262953,
        "step": 2670
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.7332147359848022,
        "learning_rate": 9.979179812404033e-05,
        "epoch": 0.3442897654034545,
        "step": 2671
    },
    {
        "loss": 2.0804,
        "grad_norm": 1.6678811311721802,
        "learning_rate": 9.978621304660978e-05,
        "epoch": 0.34441866460427945,
        "step": 2672
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.582932710647583,
        "learning_rate": 9.978055420716357e-05,
        "epoch": 0.34454756380510443,
        "step": 2673
    },
    {
        "loss": 1.0426,
        "grad_norm": 2.5555684566497803,
        "learning_rate": 9.977482161408568e-05,
        "epoch": 0.34467646300592936,
        "step": 2674
    },
    {
        "loss": 2.3662,
        "grad_norm": 1.7973976135253906,
        "learning_rate": 9.976901527586944e-05,
        "epoch": 0.34480536220675434,
        "step": 2675
    },
    {
        "loss": 1.9801,
        "grad_norm": 1.774519920349121,
        "learning_rate": 9.976313520111732e-05,
        "epoch": 0.34493426140757927,
        "step": 2676
    },
    {
        "loss": 1.0258,
        "grad_norm": 2.960921287536621,
        "learning_rate": 9.975718139854113e-05,
        "epoch": 0.34506316060840425,
        "step": 2677
    },
    {
        "loss": 2.3579,
        "grad_norm": 3.321763038635254,
        "learning_rate": 9.975115387696186e-05,
        "epoch": 0.3451920598092292,
        "step": 2678
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.4029297828674316,
        "learning_rate": 9.974505264530972e-05,
        "epoch": 0.34532095901005416,
        "step": 2679
    },
    {
        "loss": 1.7225,
        "grad_norm": 2.3328402042388916,
        "learning_rate": 9.973887771262415e-05,
        "epoch": 0.3454498582108791,
        "step": 2680
    },
    {
        "loss": 2.3447,
        "grad_norm": 1.4572527408599854,
        "learning_rate": 9.973262908805381e-05,
        "epoch": 0.3455787574117041,
        "step": 2681
    },
    {
        "loss": 2.2444,
        "grad_norm": 1.7323464155197144,
        "learning_rate": 9.972630678085646e-05,
        "epoch": 0.345707656612529,
        "step": 2682
    },
    {
        "loss": 1.7806,
        "grad_norm": 3.5515506267547607,
        "learning_rate": 9.97199108003991e-05,
        "epoch": 0.34583655581335393,
        "step": 2683
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.5738705396652222,
        "learning_rate": 9.971344115615785e-05,
        "epoch": 0.3459654550141789,
        "step": 2684
    },
    {
        "loss": 1.7447,
        "grad_norm": 2.8880393505096436,
        "learning_rate": 9.970689785771798e-05,
        "epoch": 0.34609435421500384,
        "step": 2685
    },
    {
        "loss": 1.2308,
        "grad_norm": 3.206866502761841,
        "learning_rate": 9.970028091477388e-05,
        "epoch": 0.3462232534158288,
        "step": 2686
    },
    {
        "loss": 1.6025,
        "grad_norm": 2.322132110595703,
        "learning_rate": 9.969359033712903e-05,
        "epoch": 0.34635215261665375,
        "step": 2687
    },
    {
        "loss": 1.6852,
        "grad_norm": 3.0346641540527344,
        "learning_rate": 9.968682613469603e-05,
        "epoch": 0.34648105181747874,
        "step": 2688
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.85027015209198,
        "learning_rate": 9.967998831749656e-05,
        "epoch": 0.34660995101830366,
        "step": 2689
    },
    {
        "loss": 2.2465,
        "grad_norm": 2.0652174949645996,
        "learning_rate": 9.967307689566138e-05,
        "epoch": 0.34673885021912865,
        "step": 2690
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.9653947353363037,
        "learning_rate": 9.966609187943023e-05,
        "epoch": 0.3468677494199536,
        "step": 2691
    },
    {
        "loss": 1.736,
        "grad_norm": 1.8146215677261353,
        "learning_rate": 9.965903327915198e-05,
        "epoch": 0.34699664862077856,
        "step": 2692
    },
    {
        "loss": 2.1328,
        "grad_norm": 1.8847393989562988,
        "learning_rate": 9.965190110528446e-05,
        "epoch": 0.3471255478216035,
        "step": 2693
    },
    {
        "loss": 1.8099,
        "grad_norm": 2.1815621852874756,
        "learning_rate": 9.964469536839454e-05,
        "epoch": 0.34725444702242847,
        "step": 2694
    },
    {
        "loss": 2.2682,
        "grad_norm": 1.6845409870147705,
        "learning_rate": 9.963741607915802e-05,
        "epoch": 0.3473833462232534,
        "step": 2695
    },
    {
        "loss": 1.4666,
        "grad_norm": 3.195168972015381,
        "learning_rate": 9.963006324835975e-05,
        "epoch": 0.3475122454240784,
        "step": 2696
    },
    {
        "loss": 0.8248,
        "grad_norm": 8.171724319458008,
        "learning_rate": 9.962263688689349e-05,
        "epoch": 0.3476411446249033,
        "step": 2697
    },
    {
        "loss": 2.6699,
        "grad_norm": 1.45542311668396,
        "learning_rate": 9.961513700576195e-05,
        "epoch": 0.3477700438257283,
        "step": 2698
    },
    {
        "loss": 1.9559,
        "grad_norm": 1.6497044563293457,
        "learning_rate": 9.960756361607677e-05,
        "epoch": 0.3478989430265532,
        "step": 2699
    },
    {
        "loss": 2.0,
        "grad_norm": 2.5347483158111572,
        "learning_rate": 9.95999167290585e-05,
        "epoch": 0.3480278422273782,
        "step": 2700
    },
    {
        "loss": 2.3332,
        "grad_norm": 2.2430408000946045,
        "learning_rate": 9.959219635603656e-05,
        "epoch": 0.34815674142820313,
        "step": 2701
    },
    {
        "loss": 2.1925,
        "grad_norm": 1.8473082780838013,
        "learning_rate": 9.958440250844929e-05,
        "epoch": 0.3482856406290281,
        "step": 2702
    },
    {
        "loss": 1.5017,
        "grad_norm": 2.757805824279785,
        "learning_rate": 9.957653519784386e-05,
        "epoch": 0.34841453982985304,
        "step": 2703
    },
    {
        "loss": 1.9539,
        "grad_norm": 2.26729154586792,
        "learning_rate": 9.956859443587625e-05,
        "epoch": 0.348543439030678,
        "step": 2704
    },
    {
        "loss": 2.0284,
        "grad_norm": 2.2728664875030518,
        "learning_rate": 9.956058023431132e-05,
        "epoch": 0.34867233823150295,
        "step": 2705
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.6834790706634521,
        "learning_rate": 9.95524926050227e-05,
        "epoch": 0.34880123743232794,
        "step": 2706
    },
    {
        "loss": 2.3753,
        "grad_norm": 2.0759787559509277,
        "learning_rate": 9.954433155999283e-05,
        "epoch": 0.34893013663315287,
        "step": 2707
    },
    {
        "loss": 1.948,
        "grad_norm": 1.9999744892120361,
        "learning_rate": 9.95360971113129e-05,
        "epoch": 0.34905903583397785,
        "step": 2708
    },
    {
        "loss": 2.4714,
        "grad_norm": 1.8061749935150146,
        "learning_rate": 9.952778927118289e-05,
        "epoch": 0.3491879350348028,
        "step": 2709
    },
    {
        "loss": 2.4543,
        "grad_norm": 1.7118418216705322,
        "learning_rate": 9.951940805191147e-05,
        "epoch": 0.34931683423562776,
        "step": 2710
    },
    {
        "loss": 2.4052,
        "grad_norm": 1.2160937786102295,
        "learning_rate": 9.951095346591604e-05,
        "epoch": 0.3494457334364527,
        "step": 2711
    },
    {
        "loss": 2.7283,
        "grad_norm": 1.915519118309021,
        "learning_rate": 9.950242552572271e-05,
        "epoch": 0.34957463263727767,
        "step": 2712
    },
    {
        "loss": 2.2325,
        "grad_norm": 1.9173554182052612,
        "learning_rate": 9.949382424396628e-05,
        "epoch": 0.3497035318381026,
        "step": 2713
    },
    {
        "loss": 2.5799,
        "grad_norm": 1.5987415313720703,
        "learning_rate": 9.948514963339018e-05,
        "epoch": 0.3498324310389276,
        "step": 2714
    },
    {
        "loss": 1.964,
        "grad_norm": 2.720653772354126,
        "learning_rate": 9.947640170684649e-05,
        "epoch": 0.3499613302397525,
        "step": 2715
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.509683847427368,
        "learning_rate": 9.946758047729594e-05,
        "epoch": 0.3500902294405775,
        "step": 2716
    },
    {
        "loss": 2.259,
        "grad_norm": 1.3607232570648193,
        "learning_rate": 9.945868595780783e-05,
        "epoch": 0.3502191286414024,
        "step": 2717
    },
    {
        "loss": 1.3598,
        "grad_norm": 2.7780919075012207,
        "learning_rate": 9.944971816156006e-05,
        "epoch": 0.3503480278422274,
        "step": 2718
    },
    {
        "loss": 2.1151,
        "grad_norm": 2.957308530807495,
        "learning_rate": 9.944067710183911e-05,
        "epoch": 0.35047692704305233,
        "step": 2719
    },
    {
        "loss": 2.0545,
        "grad_norm": 3.00176739692688,
        "learning_rate": 9.943156279203997e-05,
        "epoch": 0.35060582624387726,
        "step": 2720
    },
    {
        "loss": 2.545,
        "grad_norm": 1.4089326858520508,
        "learning_rate": 9.942237524566618e-05,
        "epoch": 0.35073472544470224,
        "step": 2721
    },
    {
        "loss": 2.6703,
        "grad_norm": 1.6000903844833374,
        "learning_rate": 9.941311447632979e-05,
        "epoch": 0.3508636246455272,
        "step": 2722
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.6361533403396606,
        "learning_rate": 9.940378049775129e-05,
        "epoch": 0.35099252384635216,
        "step": 2723
    },
    {
        "loss": 2.1866,
        "grad_norm": 1.9466354846954346,
        "learning_rate": 9.939437332375974e-05,
        "epoch": 0.3511214230471771,
        "step": 2724
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.15985369682312,
        "learning_rate": 9.93848929682925e-05,
        "epoch": 0.35125032224800207,
        "step": 2725
    },
    {
        "loss": 2.399,
        "grad_norm": 1.612998366355896,
        "learning_rate": 9.937533944539547e-05,
        "epoch": 0.351379221448827,
        "step": 2726
    },
    {
        "loss": 2.3658,
        "grad_norm": 1.583369493484497,
        "learning_rate": 9.936571276922292e-05,
        "epoch": 0.351508120649652,
        "step": 2727
    },
    {
        "loss": 1.9653,
        "grad_norm": 2.9504055976867676,
        "learning_rate": 9.935601295403748e-05,
        "epoch": 0.3516370198504769,
        "step": 2728
    },
    {
        "loss": 1.7252,
        "grad_norm": 1.5603255033493042,
        "learning_rate": 9.934624001421012e-05,
        "epoch": 0.3517659190513019,
        "step": 2729
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.437490701675415,
        "learning_rate": 9.933639396422025e-05,
        "epoch": 0.3518948182521268,
        "step": 2730
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.8520479202270508,
        "learning_rate": 9.93264748186555e-05,
        "epoch": 0.3520237174529518,
        "step": 2731
    },
    {
        "loss": 2.1726,
        "grad_norm": 1.6356964111328125,
        "learning_rate": 9.931648259221181e-05,
        "epoch": 0.35215261665377673,
        "step": 2732
    },
    {
        "loss": 2.0937,
        "grad_norm": 2.885939359664917,
        "learning_rate": 9.930641729969344e-05,
        "epoch": 0.3522815158546017,
        "step": 2733
    },
    {
        "loss": 1.822,
        "grad_norm": 2.3061234951019287,
        "learning_rate": 9.929627895601288e-05,
        "epoch": 0.35241041505542664,
        "step": 2734
    },
    {
        "loss": 1.5947,
        "grad_norm": 2.1887309551239014,
        "learning_rate": 9.928606757619084e-05,
        "epoch": 0.3525393142562516,
        "step": 2735
    },
    {
        "loss": 1.2902,
        "grad_norm": 3.175035238265991,
        "learning_rate": 9.927578317535625e-05,
        "epoch": 0.35266821345707655,
        "step": 2736
    },
    {
        "loss": 1.8366,
        "grad_norm": 2.995197296142578,
        "learning_rate": 9.926542576874622e-05,
        "epoch": 0.35279711265790153,
        "step": 2737
    },
    {
        "loss": 1.4815,
        "grad_norm": 2.1633267402648926,
        "learning_rate": 9.925499537170602e-05,
        "epoch": 0.35292601185872646,
        "step": 2738
    },
    {
        "loss": 2.0134,
        "grad_norm": 2.470437526702881,
        "learning_rate": 9.924449199968908e-05,
        "epoch": 0.35305491105955145,
        "step": 2739
    },
    {
        "loss": 1.8992,
        "grad_norm": 1.4365383386611938,
        "learning_rate": 9.923391566825692e-05,
        "epoch": 0.3531838102603764,
        "step": 2740
    },
    {
        "loss": 1.3284,
        "grad_norm": 4.3183512687683105,
        "learning_rate": 9.922326639307917e-05,
        "epoch": 0.35331270946120136,
        "step": 2741
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.9547948837280273,
        "learning_rate": 9.921254418993355e-05,
        "epoch": 0.3534416086620263,
        "step": 2742
    },
    {
        "loss": 2.1528,
        "grad_norm": 1.4851000308990479,
        "learning_rate": 9.92017490747058e-05,
        "epoch": 0.35357050786285127,
        "step": 2743
    },
    {
        "loss": 2.6779,
        "grad_norm": 2.237697124481201,
        "learning_rate": 9.919088106338968e-05,
        "epoch": 0.3536994070636762,
        "step": 2744
    },
    {
        "loss": 2.1,
        "grad_norm": 2.2077622413635254,
        "learning_rate": 9.917994017208699e-05,
        "epoch": 0.3538283062645012,
        "step": 2745
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.299323081970215,
        "learning_rate": 9.916892641700746e-05,
        "epoch": 0.3539572054653261,
        "step": 2746
    },
    {
        "loss": 2.3051,
        "grad_norm": 1.7277100086212158,
        "learning_rate": 9.915783981446881e-05,
        "epoch": 0.3540861046661511,
        "step": 2747
    },
    {
        "loss": 1.9692,
        "grad_norm": 1.7677569389343262,
        "learning_rate": 9.914668038089669e-05,
        "epoch": 0.354215003866976,
        "step": 2748
    },
    {
        "loss": 1.9053,
        "grad_norm": 1.8473716974258423,
        "learning_rate": 9.913544813282461e-05,
        "epoch": 0.354343903067801,
        "step": 2749
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.2621541023254395,
        "learning_rate": 9.912414308689397e-05,
        "epoch": 0.35447280226862593,
        "step": 2750
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.5480942726135254,
        "learning_rate": 9.911276525985411e-05,
        "epoch": 0.3546017014694509,
        "step": 2751
    },
    {
        "loss": 1.432,
        "grad_norm": 1.9981740713119507,
        "learning_rate": 9.910131466856208e-05,
        "epoch": 0.35473060067027584,
        "step": 2752
    },
    {
        "loss": 1.989,
        "grad_norm": 1.900067687034607,
        "learning_rate": 9.908979132998282e-05,
        "epoch": 0.3548594998711008,
        "step": 2753
    },
    {
        "loss": 2.0232,
        "grad_norm": 1.2327165603637695,
        "learning_rate": 9.907819526118902e-05,
        "epoch": 0.35498839907192575,
        "step": 2754
    },
    {
        "loss": 1.9676,
        "grad_norm": 1.819305419921875,
        "learning_rate": 9.906652647936113e-05,
        "epoch": 0.35511729827275074,
        "step": 2755
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.3944461345672607,
        "learning_rate": 9.905478500178729e-05,
        "epoch": 0.35524619747357566,
        "step": 2756
    },
    {
        "loss": 2.086,
        "grad_norm": 1.788442611694336,
        "learning_rate": 9.904297084586345e-05,
        "epoch": 0.3553750966744006,
        "step": 2757
    },
    {
        "loss": 2.303,
        "grad_norm": 2.2486014366149902,
        "learning_rate": 9.903108402909309e-05,
        "epoch": 0.3555039958752256,
        "step": 2758
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.2180557250976562,
        "learning_rate": 9.901912456908748e-05,
        "epoch": 0.3556328950760505,
        "step": 2759
    },
    {
        "loss": 2.4233,
        "grad_norm": 1.6925853490829468,
        "learning_rate": 9.900709248356544e-05,
        "epoch": 0.3557617942768755,
        "step": 2760
    },
    {
        "loss": 2.3886,
        "grad_norm": 2.7917232513427734,
        "learning_rate": 9.89949877903534e-05,
        "epoch": 0.3558906934777004,
        "step": 2761
    },
    {
        "loss": 2.2522,
        "grad_norm": 2.1627395153045654,
        "learning_rate": 9.898281050738538e-05,
        "epoch": 0.3560195926785254,
        "step": 2762
    },
    {
        "loss": 2.6286,
        "grad_norm": 1.3477485179901123,
        "learning_rate": 9.897056065270296e-05,
        "epoch": 0.3561484918793503,
        "step": 2763
    },
    {
        "loss": 2.0537,
        "grad_norm": 1.8541615009307861,
        "learning_rate": 9.895823824445518e-05,
        "epoch": 0.3562773910801753,
        "step": 2764
    },
    {
        "loss": 1.9433,
        "grad_norm": 3.055652618408203,
        "learning_rate": 9.894584330089866e-05,
        "epoch": 0.35640629028100024,
        "step": 2765
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.2652978897094727,
        "learning_rate": 9.893337584039739e-05,
        "epoch": 0.3565351894818252,
        "step": 2766
    },
    {
        "loss": 2.3502,
        "grad_norm": 1.9636796712875366,
        "learning_rate": 9.89208358814229e-05,
        "epoch": 0.35666408868265015,
        "step": 2767
    },
    {
        "loss": 2.0615,
        "grad_norm": 2.0413455963134766,
        "learning_rate": 9.890822344255403e-05,
        "epoch": 0.35679298788347513,
        "step": 2768
    },
    {
        "loss": 2.1034,
        "grad_norm": 1.8225646018981934,
        "learning_rate": 9.88955385424771e-05,
        "epoch": 0.35692188708430006,
        "step": 2769
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.0207443237304688,
        "learning_rate": 9.888278119998573e-05,
        "epoch": 0.35705078628512504,
        "step": 2770
    },
    {
        "loss": 1.5357,
        "grad_norm": 2.581637382507324,
        "learning_rate": 9.886995143398085e-05,
        "epoch": 0.35717968548594997,
        "step": 2771
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.5051798820495605,
        "learning_rate": 9.885704926347079e-05,
        "epoch": 0.35730858468677495,
        "step": 2772
    },
    {
        "loss": 1.4591,
        "grad_norm": 2.4872896671295166,
        "learning_rate": 9.884407470757103e-05,
        "epoch": 0.3574374838875999,
        "step": 2773
    },
    {
        "loss": 2.1816,
        "grad_norm": 2.5375139713287354,
        "learning_rate": 9.883102778550433e-05,
        "epoch": 0.35756638308842487,
        "step": 2774
    },
    {
        "loss": 1.5581,
        "grad_norm": 3.2836878299713135,
        "learning_rate": 9.881790851660075e-05,
        "epoch": 0.3576952822892498,
        "step": 2775
    },
    {
        "loss": 2.4215,
        "grad_norm": 1.6800601482391357,
        "learning_rate": 9.880471692029743e-05,
        "epoch": 0.3578241814900748,
        "step": 2776
    },
    {
        "loss": 1.6547,
        "grad_norm": 2.3526225090026855,
        "learning_rate": 9.879145301613873e-05,
        "epoch": 0.3579530806908997,
        "step": 2777
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.0195419788360596,
        "learning_rate": 9.877811682377614e-05,
        "epoch": 0.3580819798917247,
        "step": 2778
    },
    {
        "loss": 1.2877,
        "grad_norm": 3.5607025623321533,
        "learning_rate": 9.876470836296815e-05,
        "epoch": 0.3582108790925496,
        "step": 2779
    },
    {
        "loss": 1.7697,
        "grad_norm": 3.1998980045318604,
        "learning_rate": 9.875122765358049e-05,
        "epoch": 0.3583397782933746,
        "step": 2780
    },
    {
        "loss": 1.5244,
        "grad_norm": 2.2115375995635986,
        "learning_rate": 9.873767471558581e-05,
        "epoch": 0.35846867749419953,
        "step": 2781
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.8261648416519165,
        "learning_rate": 9.87240495690638e-05,
        "epoch": 0.3585975766950245,
        "step": 2782
    },
    {
        "loss": 2.0701,
        "grad_norm": 2.645381450653076,
        "learning_rate": 9.871035223420115e-05,
        "epoch": 0.35872647589584944,
        "step": 2783
    },
    {
        "loss": 1.888,
        "grad_norm": 2.13305926322937,
        "learning_rate": 9.869658273129149e-05,
        "epoch": 0.3588553750966744,
        "step": 2784
    },
    {
        "loss": 2.2805,
        "grad_norm": 1.8626106977462769,
        "learning_rate": 9.868274108073537e-05,
        "epoch": 0.35898427429749935,
        "step": 2785
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.3202931880950928,
        "learning_rate": 9.866882730304023e-05,
        "epoch": 0.35911317349832433,
        "step": 2786
    },
    {
        "loss": 1.7376,
        "grad_norm": 2.8189828395843506,
        "learning_rate": 9.865484141882038e-05,
        "epoch": 0.35924207269914926,
        "step": 2787
    },
    {
        "loss": 2.261,
        "grad_norm": 1.4378010034561157,
        "learning_rate": 9.864078344879694e-05,
        "epoch": 0.35937097189997425,
        "step": 2788
    },
    {
        "loss": 2.2346,
        "grad_norm": 2.330281972885132,
        "learning_rate": 9.862665341379791e-05,
        "epoch": 0.3594998711007992,
        "step": 2789
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.7021241188049316,
        "learning_rate": 9.861245133475793e-05,
        "epoch": 0.35962877030162416,
        "step": 2790
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.5983121395111084,
        "learning_rate": 9.859817723271848e-05,
        "epoch": 0.3597576695024491,
        "step": 2791
    },
    {
        "loss": 1.4111,
        "grad_norm": 2.5875136852264404,
        "learning_rate": 9.858383112882772e-05,
        "epoch": 0.359886568703274,
        "step": 2792
    },
    {
        "loss": 2.6006,
        "grad_norm": 1.5384926795959473,
        "learning_rate": 9.856941304434047e-05,
        "epoch": 0.360015467904099,
        "step": 2793
    },
    {
        "loss": 1.675,
        "grad_norm": 2.8434908390045166,
        "learning_rate": 9.855492300061819e-05,
        "epoch": 0.3601443671049239,
        "step": 2794
    },
    {
        "loss": 2.0351,
        "grad_norm": 2.3753716945648193,
        "learning_rate": 9.8540361019129e-05,
        "epoch": 0.3602732663057489,
        "step": 2795
    },
    {
        "loss": 1.6942,
        "grad_norm": 2.723860502243042,
        "learning_rate": 9.852572712144755e-05,
        "epoch": 0.36040216550657383,
        "step": 2796
    },
    {
        "loss": 1.3896,
        "grad_norm": 2.001415967941284,
        "learning_rate": 9.851102132925507e-05,
        "epoch": 0.3605310647073988,
        "step": 2797
    },
    {
        "loss": 2.6304,
        "grad_norm": 1.4650321006774902,
        "learning_rate": 9.849624366433928e-05,
        "epoch": 0.36065996390822375,
        "step": 2798
    },
    {
        "loss": 1.3,
        "grad_norm": 2.4990460872650146,
        "learning_rate": 9.848139414859441e-05,
        "epoch": 0.36078886310904873,
        "step": 2799
    },
    {
        "loss": 1.9217,
        "grad_norm": 1.820264220237732,
        "learning_rate": 9.846647280402114e-05,
        "epoch": 0.36091776230987366,
        "step": 2800
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.4812395572662354,
        "learning_rate": 9.845147965272656e-05,
        "epoch": 0.36104666151069864,
        "step": 2801
    },
    {
        "loss": 2.1972,
        "grad_norm": 2.056474447250366,
        "learning_rate": 9.843641471692415e-05,
        "epoch": 0.36117556071152357,
        "step": 2802
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.6150916814804077,
        "learning_rate": 9.842127801893373e-05,
        "epoch": 0.36130445991234855,
        "step": 2803
    },
    {
        "loss": 2.0621,
        "grad_norm": 1.6417577266693115,
        "learning_rate": 9.840606958118148e-05,
        "epoch": 0.3614333591131735,
        "step": 2804
    },
    {
        "loss": 2.0646,
        "grad_norm": 2.064969539642334,
        "learning_rate": 9.839078942619981e-05,
        "epoch": 0.36156225831399846,
        "step": 2805
    },
    {
        "loss": 2.4725,
        "grad_norm": 1.4087138175964355,
        "learning_rate": 9.837543757662746e-05,
        "epoch": 0.3616911575148234,
        "step": 2806
    },
    {
        "loss": 1.9684,
        "grad_norm": 2.549018621444702,
        "learning_rate": 9.836001405520932e-05,
        "epoch": 0.3618200567156484,
        "step": 2807
    },
    {
        "loss": 1.9545,
        "grad_norm": 2.2806625366210938,
        "learning_rate": 9.83445188847965e-05,
        "epoch": 0.3619489559164733,
        "step": 2808
    },
    {
        "loss": 2.0016,
        "grad_norm": 2.0581061840057373,
        "learning_rate": 9.832895208834628e-05,
        "epoch": 0.3620778551172983,
        "step": 2809
    },
    {
        "loss": 1.8829,
        "grad_norm": 3.0642664432525635,
        "learning_rate": 9.8313313688922e-05,
        "epoch": 0.3622067543181232,
        "step": 2810
    },
    {
        "loss": 2.2903,
        "grad_norm": 1.695177674293518,
        "learning_rate": 9.829760370969316e-05,
        "epoch": 0.3623356535189482,
        "step": 2811
    },
    {
        "loss": 1.7441,
        "grad_norm": 2.807903528213501,
        "learning_rate": 9.828182217393526e-05,
        "epoch": 0.3624645527197731,
        "step": 2812
    },
    {
        "loss": 2.4102,
        "grad_norm": 1.9095878601074219,
        "learning_rate": 9.826596910502982e-05,
        "epoch": 0.3625934519205981,
        "step": 2813
    },
    {
        "loss": 1.524,
        "grad_norm": 2.7220635414123535,
        "learning_rate": 9.825004452646436e-05,
        "epoch": 0.36272235112142304,
        "step": 2814
    },
    {
        "loss": 2.385,
        "grad_norm": 1.5975943803787231,
        "learning_rate": 9.823404846183236e-05,
        "epoch": 0.362851250322248,
        "step": 2815
    },
    {
        "loss": 1.676,
        "grad_norm": 2.7490711212158203,
        "learning_rate": 9.821798093483317e-05,
        "epoch": 0.36298014952307295,
        "step": 2816
    },
    {
        "loss": 2.1634,
        "grad_norm": 2.075333595275879,
        "learning_rate": 9.820184196927203e-05,
        "epoch": 0.36310904872389793,
        "step": 2817
    },
    {
        "loss": 2.4148,
        "grad_norm": 1.542686939239502,
        "learning_rate": 9.818563158906007e-05,
        "epoch": 0.36323794792472286,
        "step": 2818
    },
    {
        "loss": 2.5058,
        "grad_norm": 1.777674913406372,
        "learning_rate": 9.816934981821412e-05,
        "epoch": 0.36336684712554784,
        "step": 2819
    },
    {
        "loss": 1.991,
        "grad_norm": 1.7729886770248413,
        "learning_rate": 9.815299668085692e-05,
        "epoch": 0.36349574632637277,
        "step": 2820
    },
    {
        "loss": 2.0133,
        "grad_norm": 2.084360361099243,
        "learning_rate": 9.813657220121685e-05,
        "epoch": 0.36362464552719775,
        "step": 2821
    },
    {
        "loss": 1.7673,
        "grad_norm": 2.5410730838775635,
        "learning_rate": 9.812007640362797e-05,
        "epoch": 0.3637535447280227,
        "step": 2822
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.17771577835083,
        "learning_rate": 9.810350931253009e-05,
        "epoch": 0.36388244392884767,
        "step": 2823
    },
    {
        "loss": 1.8163,
        "grad_norm": 2.5009329319000244,
        "learning_rate": 9.808687095246858e-05,
        "epoch": 0.3640113431296726,
        "step": 2824
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.9291338920593262,
        "learning_rate": 9.807016134809442e-05,
        "epoch": 0.3641402423304976,
        "step": 2825
    },
    {
        "loss": 2.4609,
        "grad_norm": 2.1130125522613525,
        "learning_rate": 9.805338052416414e-05,
        "epoch": 0.3642691415313225,
        "step": 2826
    },
    {
        "loss": 1.7785,
        "grad_norm": 2.537494659423828,
        "learning_rate": 9.803652850553981e-05,
        "epoch": 0.3643980407321475,
        "step": 2827
    },
    {
        "loss": 1.7865,
        "grad_norm": 3.4085123538970947,
        "learning_rate": 9.801960531718896e-05,
        "epoch": 0.3645269399329724,
        "step": 2828
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.5671417713165283,
        "learning_rate": 9.800261098418457e-05,
        "epoch": 0.36465583913379734,
        "step": 2829
    },
    {
        "loss": 1.9882,
        "grad_norm": 2.4207382202148438,
        "learning_rate": 9.798554553170498e-05,
        "epoch": 0.3647847383346223,
        "step": 2830
    },
    {
        "loss": 2.3637,
        "grad_norm": 1.5957832336425781,
        "learning_rate": 9.796840898503398e-05,
        "epoch": 0.36491363753544726,
        "step": 2831
    },
    {
        "loss": 2.0655,
        "grad_norm": 1.739932656288147,
        "learning_rate": 9.795120136956065e-05,
        "epoch": 0.36504253673627224,
        "step": 2832
    },
    {
        "loss": 2.3294,
        "grad_norm": 2.0143074989318848,
        "learning_rate": 9.793392271077935e-05,
        "epoch": 0.36517143593709717,
        "step": 2833
    },
    {
        "loss": 2.4043,
        "grad_norm": 1.3662785291671753,
        "learning_rate": 9.791657303428971e-05,
        "epoch": 0.36530033513792215,
        "step": 2834
    },
    {
        "loss": 2.2542,
        "grad_norm": 2.0903360843658447,
        "learning_rate": 9.789915236579658e-05,
        "epoch": 0.3654292343387471,
        "step": 2835
    },
    {
        "loss": 1.8778,
        "grad_norm": 1.4930827617645264,
        "learning_rate": 9.788166073111002e-05,
        "epoch": 0.36555813353957206,
        "step": 2836
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.7311861515045166,
        "learning_rate": 9.786409815614517e-05,
        "epoch": 0.365687032740397,
        "step": 2837
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.379714250564575,
        "learning_rate": 9.784646466692229e-05,
        "epoch": 0.365815931941222,
        "step": 2838
    },
    {
        "loss": 2.1428,
        "grad_norm": 2.805725574493408,
        "learning_rate": 9.782876028956678e-05,
        "epoch": 0.3659448311420469,
        "step": 2839
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.6423510313034058,
        "learning_rate": 9.781098505030894e-05,
        "epoch": 0.3660737303428719,
        "step": 2840
    },
    {
        "loss": 1.7669,
        "grad_norm": 1.9251506328582764,
        "learning_rate": 9.779313897548415e-05,
        "epoch": 0.3662026295436968,
        "step": 2841
    },
    {
        "loss": 2.0097,
        "grad_norm": 2.328951597213745,
        "learning_rate": 9.777522209153271e-05,
        "epoch": 0.3663315287445218,
        "step": 2842
    },
    {
        "loss": 2.4422,
        "grad_norm": 1.2207112312316895,
        "learning_rate": 9.775723442499982e-05,
        "epoch": 0.3664604279453467,
        "step": 2843
    },
    {
        "loss": 1.2362,
        "grad_norm": 2.136378526687622,
        "learning_rate": 9.773917600253559e-05,
        "epoch": 0.3665893271461717,
        "step": 2844
    },
    {
        "loss": 2.2778,
        "grad_norm": 1.4875043630599976,
        "learning_rate": 9.772104685089492e-05,
        "epoch": 0.36671822634699663,
        "step": 2845
    },
    {
        "loss": 2.0283,
        "grad_norm": 1.4736098051071167,
        "learning_rate": 9.770284699693747e-05,
        "epoch": 0.3668471255478216,
        "step": 2846
    },
    {
        "loss": 2.1857,
        "grad_norm": 2.0216331481933594,
        "learning_rate": 9.768457646762772e-05,
        "epoch": 0.36697602474864655,
        "step": 2847
    },
    {
        "loss": 2.5042,
        "grad_norm": 1.875553846359253,
        "learning_rate": 9.766623529003487e-05,
        "epoch": 0.36710492394947153,
        "step": 2848
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.566561222076416,
        "learning_rate": 9.764782349133268e-05,
        "epoch": 0.36723382315029646,
        "step": 2849
    },
    {
        "loss": 1.2557,
        "grad_norm": 2.71907901763916,
        "learning_rate": 9.762934109879966e-05,
        "epoch": 0.36736272235112144,
        "step": 2850
    },
    {
        "loss": 2.064,
        "grad_norm": 1.9214533567428589,
        "learning_rate": 9.761078813981885e-05,
        "epoch": 0.36749162155194637,
        "step": 2851
    },
    {
        "loss": 2.0282,
        "grad_norm": 2.240543842315674,
        "learning_rate": 9.759216464187786e-05,
        "epoch": 0.36762052075277135,
        "step": 2852
    },
    {
        "loss": 1.6719,
        "grad_norm": 1.6181633472442627,
        "learning_rate": 9.757347063256879e-05,
        "epoch": 0.3677494199535963,
        "step": 2853
    },
    {
        "loss": 2.0689,
        "grad_norm": 2.394221544265747,
        "learning_rate": 9.755470613958823e-05,
        "epoch": 0.36787831915442126,
        "step": 2854
    },
    {
        "loss": 2.2263,
        "grad_norm": 1.6640896797180176,
        "learning_rate": 9.753587119073719e-05,
        "epoch": 0.3680072183552462,
        "step": 2855
    },
    {
        "loss": 2.2015,
        "grad_norm": 2.5680148601531982,
        "learning_rate": 9.751696581392105e-05,
        "epoch": 0.3681361175560712,
        "step": 2856
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.4303311109542847,
        "learning_rate": 9.749799003714954e-05,
        "epoch": 0.3682650167568961,
        "step": 2857
    },
    {
        "loss": 2.1925,
        "grad_norm": 1.9097959995269775,
        "learning_rate": 9.747894388853673e-05,
        "epoch": 0.3683939159577211,
        "step": 2858
    },
    {
        "loss": 2.2545,
        "grad_norm": 2.300199031829834,
        "learning_rate": 9.745982739630089e-05,
        "epoch": 0.368522815158546,
        "step": 2859
    },
    {
        "loss": 1.2623,
        "grad_norm": 2.759575128555298,
        "learning_rate": 9.744064058876454e-05,
        "epoch": 0.368651714359371,
        "step": 2860
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.3505316972732544,
        "learning_rate": 9.74213834943544e-05,
        "epoch": 0.3687806135601959,
        "step": 2861
    },
    {
        "loss": 1.3814,
        "grad_norm": 2.958000659942627,
        "learning_rate": 9.74020561416013e-05,
        "epoch": 0.3689095127610209,
        "step": 2862
    },
    {
        "loss": 1.5638,
        "grad_norm": 2.143648862838745,
        "learning_rate": 9.738265855914013e-05,
        "epoch": 0.36903841196184584,
        "step": 2863
    },
    {
        "loss": 2.2597,
        "grad_norm": 1.696350336074829,
        "learning_rate": 9.73631907757099e-05,
        "epoch": 0.36916731116267076,
        "step": 2864
    },
    {
        "loss": 2.2817,
        "grad_norm": 2.07749605178833,
        "learning_rate": 9.734365282015359e-05,
        "epoch": 0.36929621036349575,
        "step": 2865
    },
    {
        "loss": 2.1385,
        "grad_norm": 2.510488748550415,
        "learning_rate": 9.732404472141814e-05,
        "epoch": 0.3694251095643207,
        "step": 2866
    },
    {
        "loss": 2.5957,
        "grad_norm": 1.9048793315887451,
        "learning_rate": 9.730436650855444e-05,
        "epoch": 0.36955400876514566,
        "step": 2867
    },
    {
        "loss": 1.9168,
        "grad_norm": 2.213578462600708,
        "learning_rate": 9.72846182107172e-05,
        "epoch": 0.3696829079659706,
        "step": 2868
    },
    {
        "loss": 2.4972,
        "grad_norm": 1.6157104969024658,
        "learning_rate": 9.726479985716506e-05,
        "epoch": 0.36981180716679557,
        "step": 2869
    },
    {
        "loss": 1.6,
        "grad_norm": 2.938934803009033,
        "learning_rate": 9.724491147726036e-05,
        "epoch": 0.3699407063676205,
        "step": 2870
    },
    {
        "loss": 1.0589,
        "grad_norm": 2.924651622772217,
        "learning_rate": 9.722495310046924e-05,
        "epoch": 0.3700696055684455,
        "step": 2871
    },
    {
        "loss": 1.2842,
        "grad_norm": 2.699784278869629,
        "learning_rate": 9.720492475636153e-05,
        "epoch": 0.3701985047692704,
        "step": 2872
    },
    {
        "loss": 2.364,
        "grad_norm": 1.305246114730835,
        "learning_rate": 9.718482647461075e-05,
        "epoch": 0.3703274039700954,
        "step": 2873
    },
    {
        "loss": 1.9514,
        "grad_norm": 2.3109467029571533,
        "learning_rate": 9.716465828499401e-05,
        "epoch": 0.3704563031709203,
        "step": 2874
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.895473837852478,
        "learning_rate": 9.714442021739196e-05,
        "epoch": 0.3705852023717453,
        "step": 2875
    },
    {
        "loss": 2.2381,
        "grad_norm": 2.809948444366455,
        "learning_rate": 9.712411230178888e-05,
        "epoch": 0.37071410157257023,
        "step": 2876
    },
    {
        "loss": 1.8857,
        "grad_norm": 1.8656588792800903,
        "learning_rate": 9.710373456827242e-05,
        "epoch": 0.3708430007733952,
        "step": 2877
    },
    {
        "loss": 2.5007,
        "grad_norm": 2.321141481399536,
        "learning_rate": 9.708328704703373e-05,
        "epoch": 0.37097189997422014,
        "step": 2878
    },
    {
        "loss": 1.4642,
        "grad_norm": 3.1624538898468018,
        "learning_rate": 9.706276976836739e-05,
        "epoch": 0.3711007991750451,
        "step": 2879
    },
    {
        "loss": 1.7385,
        "grad_norm": 2.580522298812866,
        "learning_rate": 9.704218276267125e-05,
        "epoch": 0.37122969837587005,
        "step": 2880
    },
    {
        "loss": 2.4365,
        "grad_norm": 1.5220942497253418,
        "learning_rate": 9.70215260604465e-05,
        "epoch": 0.37135859757669504,
        "step": 2881
    },
    {
        "loss": 2.1001,
        "grad_norm": 2.1945979595184326,
        "learning_rate": 9.700079969229762e-05,
        "epoch": 0.37148749677751997,
        "step": 2882
    },
    {
        "loss": 1.4246,
        "grad_norm": 3.02972412109375,
        "learning_rate": 9.698000368893228e-05,
        "epoch": 0.37161639597834495,
        "step": 2883
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.7701232433319092,
        "learning_rate": 9.695913808116132e-05,
        "epoch": 0.3717452951791699,
        "step": 2884
    },
    {
        "loss": 1.5696,
        "grad_norm": 2.6228814125061035,
        "learning_rate": 9.693820289989872e-05,
        "epoch": 0.37187419437999486,
        "step": 2885
    },
    {
        "loss": 1.5986,
        "grad_norm": 2.6707606315612793,
        "learning_rate": 9.691719817616147e-05,
        "epoch": 0.3720030935808198,
        "step": 2886
    },
    {
        "loss": 2.2919,
        "grad_norm": 1.9329314231872559,
        "learning_rate": 9.68961239410697e-05,
        "epoch": 0.37213199278164477,
        "step": 2887
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.9292443990707397,
        "learning_rate": 9.687498022584649e-05,
        "epoch": 0.3722608919824697,
        "step": 2888
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.8344444036483765,
        "learning_rate": 9.685376706181777e-05,
        "epoch": 0.3723897911832947,
        "step": 2889
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.411683201789856,
        "learning_rate": 9.683248448041249e-05,
        "epoch": 0.3725186903841196,
        "step": 2890
    },
    {
        "loss": 1.285,
        "grad_norm": 2.601588249206543,
        "learning_rate": 9.681113251316237e-05,
        "epoch": 0.3726475895849446,
        "step": 2891
    },
    {
        "loss": 2.0027,
        "grad_norm": 1.4004547595977783,
        "learning_rate": 9.678971119170198e-05,
        "epoch": 0.3727764887857695,
        "step": 2892
    },
    {
        "loss": 1.3889,
        "grad_norm": 2.47029972076416,
        "learning_rate": 9.676822054776857e-05,
        "epoch": 0.3729053879865945,
        "step": 2893
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.9424728155136108,
        "learning_rate": 9.674666061320221e-05,
        "epoch": 0.37303428718741943,
        "step": 2894
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.712207317352295,
        "learning_rate": 9.672503141994551e-05,
        "epoch": 0.3731631863882444,
        "step": 2895
    },
    {
        "loss": 2.0435,
        "grad_norm": 1.3833619356155396,
        "learning_rate": 9.670333300004377e-05,
        "epoch": 0.37329208558906934,
        "step": 2896
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.4580014944076538,
        "learning_rate": 9.668156538564481e-05,
        "epoch": 0.37342098478989433,
        "step": 2897
    },
    {
        "loss": 2.0971,
        "grad_norm": 1.4559906721115112,
        "learning_rate": 9.6659728608999e-05,
        "epoch": 0.37354988399071926,
        "step": 2898
    },
    {
        "loss": 1.8095,
        "grad_norm": 1.807806134223938,
        "learning_rate": 9.663782270245917e-05,
        "epoch": 0.37367878319154424,
        "step": 2899
    },
    {
        "loss": 1.8953,
        "grad_norm": 2.5683281421661377,
        "learning_rate": 9.661584769848058e-05,
        "epoch": 0.37380768239236917,
        "step": 2900
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.5110228061676025,
        "learning_rate": 9.65938036296208e-05,
        "epoch": 0.3739365815931941,
        "step": 2901
    },
    {
        "loss": 2.377,
        "grad_norm": 2.48740553855896,
        "learning_rate": 9.657169052853982e-05,
        "epoch": 0.3740654807940191,
        "step": 2902
    },
    {
        "loss": 1.8158,
        "grad_norm": 2.685302734375,
        "learning_rate": 9.654950842799986e-05,
        "epoch": 0.374194379994844,
        "step": 2903
    },
    {
        "loss": 2.185,
        "grad_norm": 2.3371434211730957,
        "learning_rate": 9.652725736086533e-05,
        "epoch": 0.374323279195669,
        "step": 2904
    },
    {
        "loss": 2.639,
        "grad_norm": 1.9988278150558472,
        "learning_rate": 9.650493736010288e-05,
        "epoch": 0.3744521783964939,
        "step": 2905
    },
    {
        "loss": 1.6386,
        "grad_norm": 2.761806011199951,
        "learning_rate": 9.648254845878128e-05,
        "epoch": 0.3745810775973189,
        "step": 2906
    },
    {
        "loss": 1.794,
        "grad_norm": 2.7960448265075684,
        "learning_rate": 9.646009069007136e-05,
        "epoch": 0.37470997679814383,
        "step": 2907
    },
    {
        "loss": 1.3129,
        "grad_norm": 2.809823751449585,
        "learning_rate": 9.6437564087246e-05,
        "epoch": 0.3748388759989688,
        "step": 2908
    },
    {
        "loss": 2.2708,
        "grad_norm": 1.7478569746017456,
        "learning_rate": 9.641496868368005e-05,
        "epoch": 0.37496777519979374,
        "step": 2909
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.5966733694076538,
        "learning_rate": 9.639230451285027e-05,
        "epoch": 0.3750966744006187,
        "step": 2910
    },
    {
        "loss": 2.1503,
        "grad_norm": 1.3954789638519287,
        "learning_rate": 9.636957160833537e-05,
        "epoch": 0.37522557360144365,
        "step": 2911
    },
    {
        "loss": 2.4028,
        "grad_norm": 2.3152716159820557,
        "learning_rate": 9.634677000381586e-05,
        "epoch": 0.37535447280226863,
        "step": 2912
    },
    {
        "loss": 2.4956,
        "grad_norm": 1.6740353107452393,
        "learning_rate": 9.6323899733074e-05,
        "epoch": 0.37548337200309356,
        "step": 2913
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.2281293869018555,
        "learning_rate": 9.630096082999382e-05,
        "epoch": 0.37561227120391855,
        "step": 2914
    },
    {
        "loss": 1.7853,
        "grad_norm": 2.463934898376465,
        "learning_rate": 9.627795332856106e-05,
        "epoch": 0.3757411704047435,
        "step": 2915
    },
    {
        "loss": 1.5773,
        "grad_norm": 2.093440532684326,
        "learning_rate": 9.625487726286302e-05,
        "epoch": 0.37587006960556846,
        "step": 2916
    },
    {
        "loss": 1.7968,
        "grad_norm": 3.077251672744751,
        "learning_rate": 9.623173266708864e-05,
        "epoch": 0.3759989688063934,
        "step": 2917
    },
    {
        "loss": 2.532,
        "grad_norm": 1.9591221809387207,
        "learning_rate": 9.62085195755284e-05,
        "epoch": 0.37612786800721837,
        "step": 2918
    },
    {
        "loss": 2.3892,
        "grad_norm": 2.353067636489868,
        "learning_rate": 9.618523802257421e-05,
        "epoch": 0.3762567672080433,
        "step": 2919
    },
    {
        "loss": 2.4474,
        "grad_norm": 1.2879729270935059,
        "learning_rate": 9.616188804271948e-05,
        "epoch": 0.3763856664088683,
        "step": 2920
    },
    {
        "loss": 1.7729,
        "grad_norm": 2.649142026901245,
        "learning_rate": 9.613846967055891e-05,
        "epoch": 0.3765145656096932,
        "step": 2921
    },
    {
        "loss": 2.4268,
        "grad_norm": 1.2264819145202637,
        "learning_rate": 9.61149829407886e-05,
        "epoch": 0.3766434648105182,
        "step": 2922
    },
    {
        "loss": 1.928,
        "grad_norm": 3.563678026199341,
        "learning_rate": 9.60914278882059e-05,
        "epoch": 0.3767723640113431,
        "step": 2923
    },
    {
        "loss": 2.256,
        "grad_norm": 1.9138751029968262,
        "learning_rate": 9.60678045477094e-05,
        "epoch": 0.3769012632121681,
        "step": 2924
    },
    {
        "loss": 2.0608,
        "grad_norm": 1.4298388957977295,
        "learning_rate": 9.604411295429885e-05,
        "epoch": 0.37703016241299303,
        "step": 2925
    },
    {
        "loss": 1.9378,
        "grad_norm": 2.451018810272217,
        "learning_rate": 9.602035314307513e-05,
        "epoch": 0.377159061613818,
        "step": 2926
    },
    {
        "loss": 1.2198,
        "grad_norm": 2.9290053844451904,
        "learning_rate": 9.599652514924018e-05,
        "epoch": 0.37728796081464294,
        "step": 2927
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.943692922592163,
        "learning_rate": 9.597262900809696e-05,
        "epoch": 0.3774168600154679,
        "step": 2928
    },
    {
        "loss": 1.6279,
        "grad_norm": 2.6740922927856445,
        "learning_rate": 9.59486647550494e-05,
        "epoch": 0.37754575921629285,
        "step": 2929
    },
    {
        "loss": 2.1765,
        "grad_norm": 2.2135303020477295,
        "learning_rate": 9.592463242560235e-05,
        "epoch": 0.37767465841711784,
        "step": 2930
    },
    {
        "loss": 2.3315,
        "grad_norm": 1.8659292459487915,
        "learning_rate": 9.590053205536149e-05,
        "epoch": 0.37780355761794276,
        "step": 2931
    },
    {
        "loss": 1.9404,
        "grad_norm": 3.112210750579834,
        "learning_rate": 9.587636368003337e-05,
        "epoch": 0.37793245681876775,
        "step": 2932
    },
    {
        "loss": 2.041,
        "grad_norm": 2.0976314544677734,
        "learning_rate": 9.585212733542521e-05,
        "epoch": 0.3780613560195927,
        "step": 2933
    },
    {
        "loss": 1.9285,
        "grad_norm": 2.1633284091949463,
        "learning_rate": 9.582782305744501e-05,
        "epoch": 0.37819025522041766,
        "step": 2934
    },
    {
        "loss": 1.4139,
        "grad_norm": 2.6307320594787598,
        "learning_rate": 9.580345088210137e-05,
        "epoch": 0.3783191544212426,
        "step": 2935
    },
    {
        "loss": 1.0552,
        "grad_norm": 2.8211913108825684,
        "learning_rate": 9.57790108455035e-05,
        "epoch": 0.37844805362206757,
        "step": 2936
    },
    {
        "loss": 1.5375,
        "grad_norm": 2.4359350204467773,
        "learning_rate": 9.575450298386112e-05,
        "epoch": 0.3785769528228925,
        "step": 2937
    },
    {
        "loss": 2.2291,
        "grad_norm": 2.7620227336883545,
        "learning_rate": 9.572992733348454e-05,
        "epoch": 0.3787058520237174,
        "step": 2938
    },
    {
        "loss": 1.4662,
        "grad_norm": 2.153657913208008,
        "learning_rate": 9.570528393078437e-05,
        "epoch": 0.3788347512245424,
        "step": 2939
    },
    {
        "loss": 2.1384,
        "grad_norm": 2.5892577171325684,
        "learning_rate": 9.568057281227167e-05,
        "epoch": 0.37896365042536734,
        "step": 2940
    },
    {
        "loss": 1.3451,
        "grad_norm": 2.713658571243286,
        "learning_rate": 9.565579401455787e-05,
        "epoch": 0.3790925496261923,
        "step": 2941
    },
    {
        "loss": 1.6439,
        "grad_norm": 3.0290091037750244,
        "learning_rate": 9.563094757435453e-05,
        "epoch": 0.37922144882701725,
        "step": 2942
    },
    {
        "loss": 2.0946,
        "grad_norm": 2.0980589389801025,
        "learning_rate": 9.560603352847361e-05,
        "epoch": 0.37935034802784223,
        "step": 2943
    },
    {
        "loss": 1.2781,
        "grad_norm": 2.791191577911377,
        "learning_rate": 9.558105191382711e-05,
        "epoch": 0.37947924722866716,
        "step": 2944
    },
    {
        "loss": 2.332,
        "grad_norm": 1.8247556686401367,
        "learning_rate": 9.555600276742713e-05,
        "epoch": 0.37960814642949214,
        "step": 2945
    },
    {
        "loss": 1.2306,
        "grad_norm": 2.8875484466552734,
        "learning_rate": 9.553088612638592e-05,
        "epoch": 0.37973704563031707,
        "step": 2946
    },
    {
        "loss": 1.7939,
        "grad_norm": 3.432492971420288,
        "learning_rate": 9.550570202791565e-05,
        "epoch": 0.37986594483114206,
        "step": 2947
    },
    {
        "loss": 2.1672,
        "grad_norm": 2.3676540851593018,
        "learning_rate": 9.548045050932841e-05,
        "epoch": 0.379994844031967,
        "step": 2948
    },
    {
        "loss": 0.8625,
        "grad_norm": 2.262788772583008,
        "learning_rate": 9.54551316080363e-05,
        "epoch": 0.38012374323279197,
        "step": 2949
    },
    {
        "loss": 2.0981,
        "grad_norm": 2.2292144298553467,
        "learning_rate": 9.542974536155111e-05,
        "epoch": 0.3802526424336169,
        "step": 2950
    },
    {
        "loss": 0.9287,
        "grad_norm": 2.6551170349121094,
        "learning_rate": 9.540429180748452e-05,
        "epoch": 0.3803815416344419,
        "step": 2951
    },
    {
        "loss": 1.977,
        "grad_norm": 1.793559193611145,
        "learning_rate": 9.537877098354787e-05,
        "epoch": 0.3805104408352668,
        "step": 2952
    },
    {
        "loss": 2.221,
        "grad_norm": 2.2114510536193848,
        "learning_rate": 9.535318292755213e-05,
        "epoch": 0.3806393400360918,
        "step": 2953
    },
    {
        "loss": 1.8098,
        "grad_norm": 2.2667734622955322,
        "learning_rate": 9.5327527677408e-05,
        "epoch": 0.3807682392369167,
        "step": 2954
    },
    {
        "loss": 1.5928,
        "grad_norm": 2.9480202198028564,
        "learning_rate": 9.530180527112565e-05,
        "epoch": 0.3808971384377417,
        "step": 2955
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.249814510345459,
        "learning_rate": 9.527601574681469e-05,
        "epoch": 0.38102603763856663,
        "step": 2956
    },
    {
        "loss": 2.1479,
        "grad_norm": 1.5116814374923706,
        "learning_rate": 9.52501591426843e-05,
        "epoch": 0.3811549368393916,
        "step": 2957
    },
    {
        "loss": 2.037,
        "grad_norm": 1.5124696493148804,
        "learning_rate": 9.522423549704296e-05,
        "epoch": 0.38128383604021654,
        "step": 2958
    },
    {
        "loss": 1.8506,
        "grad_norm": 1.852691650390625,
        "learning_rate": 9.519824484829852e-05,
        "epoch": 0.3814127352410415,
        "step": 2959
    },
    {
        "loss": 2.1836,
        "grad_norm": 1.4172841310501099,
        "learning_rate": 9.517218723495805e-05,
        "epoch": 0.38154163444186645,
        "step": 2960
    },
    {
        "loss": 2.273,
        "grad_norm": 1.246335506439209,
        "learning_rate": 9.514606269562786e-05,
        "epoch": 0.38167053364269143,
        "step": 2961
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.5146512985229492,
        "learning_rate": 9.511987126901347e-05,
        "epoch": 0.38179943284351636,
        "step": 2962
    },
    {
        "loss": 1.8577,
        "grad_norm": 2.5127806663513184,
        "learning_rate": 9.509361299391938e-05,
        "epoch": 0.38192833204434135,
        "step": 2963
    },
    {
        "loss": 2.5577,
        "grad_norm": 1.667192816734314,
        "learning_rate": 9.506728790924926e-05,
        "epoch": 0.3820572312451663,
        "step": 2964
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.9347026348114014,
        "learning_rate": 9.504089605400565e-05,
        "epoch": 0.38218613044599126,
        "step": 2965
    },
    {
        "loss": 1.7563,
        "grad_norm": 2.9122021198272705,
        "learning_rate": 9.501443746729009e-05,
        "epoch": 0.3823150296468162,
        "step": 2966
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.6044909954071045,
        "learning_rate": 9.498791218830296e-05,
        "epoch": 0.38244392884764117,
        "step": 2967
    },
    {
        "loss": 2.3781,
        "grad_norm": 1.8906841278076172,
        "learning_rate": 9.496132025634347e-05,
        "epoch": 0.3825728280484661,
        "step": 2968
    },
    {
        "loss": 1.935,
        "grad_norm": 1.5096882581710815,
        "learning_rate": 9.493466171080951e-05,
        "epoch": 0.3827017272492911,
        "step": 2969
    },
    {
        "loss": 1.49,
        "grad_norm": 1.970939040184021,
        "learning_rate": 9.49079365911978e-05,
        "epoch": 0.382830626450116,
        "step": 2970
    },
    {
        "loss": 2.1362,
        "grad_norm": 2.265085220336914,
        "learning_rate": 9.488114493710357e-05,
        "epoch": 0.382959525650941,
        "step": 2971
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.4453532695770264,
        "learning_rate": 9.485428678822064e-05,
        "epoch": 0.3830884248517659,
        "step": 2972
    },
    {
        "loss": 2.4216,
        "grad_norm": 1.964699625968933,
        "learning_rate": 9.482736218434144e-05,
        "epoch": 0.38321732405259085,
        "step": 2973
    },
    {
        "loss": 2.3943,
        "grad_norm": 2.373769998550415,
        "learning_rate": 9.480037116535671e-05,
        "epoch": 0.38334622325341583,
        "step": 2974
    },
    {
        "loss": 2.1597,
        "grad_norm": 2.1054880619049072,
        "learning_rate": 9.477331377125577e-05,
        "epoch": 0.38347512245424076,
        "step": 2975
    },
    {
        "loss": 1.8404,
        "grad_norm": 2.011063575744629,
        "learning_rate": 9.474619004212612e-05,
        "epoch": 0.38360402165506574,
        "step": 2976
    },
    {
        "loss": 1.8684,
        "grad_norm": 2.705962657928467,
        "learning_rate": 9.471900001815361e-05,
        "epoch": 0.38373292085589067,
        "step": 2977
    },
    {
        "loss": 2.5191,
        "grad_norm": 1.679095983505249,
        "learning_rate": 9.469174373962233e-05,
        "epoch": 0.38386182005671565,
        "step": 2978
    },
    {
        "loss": 2.1517,
        "grad_norm": 2.0455527305603027,
        "learning_rate": 9.466442124691448e-05,
        "epoch": 0.3839907192575406,
        "step": 2979
    },
    {
        "loss": 2.3163,
        "grad_norm": 2.437357187271118,
        "learning_rate": 9.463703258051037e-05,
        "epoch": 0.38411961845836556,
        "step": 2980
    },
    {
        "loss": 1.8873,
        "grad_norm": 2.133824586868286,
        "learning_rate": 9.46095777809884e-05,
        "epoch": 0.3842485176591905,
        "step": 2981
    },
    {
        "loss": 2.0846,
        "grad_norm": 2.461534023284912,
        "learning_rate": 9.458205688902493e-05,
        "epoch": 0.3843774168600155,
        "step": 2982
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.83261239528656,
        "learning_rate": 9.455446994539418e-05,
        "epoch": 0.3845063160608404,
        "step": 2983
    },
    {
        "loss": 2.0385,
        "grad_norm": 2.135115385055542,
        "learning_rate": 9.452681699096832e-05,
        "epoch": 0.3846352152616654,
        "step": 2984
    },
    {
        "loss": 1.709,
        "grad_norm": 2.9847686290740967,
        "learning_rate": 9.449909806671727e-05,
        "epoch": 0.3847641144624903,
        "step": 2985
    },
    {
        "loss": 1.3302,
        "grad_norm": 2.152454376220703,
        "learning_rate": 9.447131321370868e-05,
        "epoch": 0.3848930136633153,
        "step": 2986
    },
    {
        "loss": 1.6428,
        "grad_norm": 2.509584665298462,
        "learning_rate": 9.444346247310794e-05,
        "epoch": 0.3850219128641402,
        "step": 2987
    },
    {
        "loss": 2.507,
        "grad_norm": 1.2697447538375854,
        "learning_rate": 9.441554588617796e-05,
        "epoch": 0.3851508120649652,
        "step": 2988
    },
    {
        "loss": 2.3591,
        "grad_norm": 1.7082507610321045,
        "learning_rate": 9.43875634942793e-05,
        "epoch": 0.38527971126579014,
        "step": 2989
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.801533579826355,
        "learning_rate": 9.435951533886998e-05,
        "epoch": 0.3854086104666151,
        "step": 2990
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.4590169191360474,
        "learning_rate": 9.433140146150543e-05,
        "epoch": 0.38553750966744005,
        "step": 2991
    },
    {
        "loss": 1.9002,
        "grad_norm": 1.8198072910308838,
        "learning_rate": 9.430322190383845e-05,
        "epoch": 0.38566640886826503,
        "step": 2992
    },
    {
        "loss": 2.491,
        "grad_norm": 2.180572509765625,
        "learning_rate": 9.427497670761923e-05,
        "epoch": 0.38579530806908996,
        "step": 2993
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.4205355644226074,
        "learning_rate": 9.424666591469511e-05,
        "epoch": 0.38592420726991494,
        "step": 2994
    },
    {
        "loss": 1.7061,
        "grad_norm": 2.4737026691436768,
        "learning_rate": 9.421828956701068e-05,
        "epoch": 0.38605310647073987,
        "step": 2995
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.04722261428833,
        "learning_rate": 9.41898477066076e-05,
        "epoch": 0.38618200567156485,
        "step": 2996
    },
    {
        "loss": 2.0886,
        "grad_norm": 2.12073016166687,
        "learning_rate": 9.416134037562465e-05,
        "epoch": 0.3863109048723898,
        "step": 2997
    },
    {
        "loss": 1.6546,
        "grad_norm": 1.559630036354065,
        "learning_rate": 9.413276761629759e-05,
        "epoch": 0.38643980407321477,
        "step": 2998
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.9290720224380493,
        "learning_rate": 9.410412947095908e-05,
        "epoch": 0.3865687032740397,
        "step": 2999
    },
    {
        "loss": 2.2615,
        "grad_norm": 2.334688186645508,
        "learning_rate": 9.407542598203869e-05,
        "epoch": 0.3866976024748647,
        "step": 3000
    },
    {
        "loss": 2.569,
        "grad_norm": 1.3713027238845825,
        "learning_rate": 9.404665719206283e-05,
        "epoch": 0.3868265016756896,
        "step": 3001
    },
    {
        "loss": 1.6036,
        "grad_norm": 2.466872215270996,
        "learning_rate": 9.401782314365457e-05,
        "epoch": 0.3869554008765146,
        "step": 3002
    },
    {
        "loss": 1.607,
        "grad_norm": 2.5882654190063477,
        "learning_rate": 9.398892387953377e-05,
        "epoch": 0.3870843000773395,
        "step": 3003
    },
    {
        "loss": 2.2361,
        "grad_norm": 1.4264273643493652,
        "learning_rate": 9.395995944251683e-05,
        "epoch": 0.3872131992781645,
        "step": 3004
    },
    {
        "loss": 2.3209,
        "grad_norm": 2.07096266746521,
        "learning_rate": 9.393092987551675e-05,
        "epoch": 0.3873420984789894,
        "step": 3005
    },
    {
        "loss": 2.0386,
        "grad_norm": 2.1352899074554443,
        "learning_rate": 9.390183522154302e-05,
        "epoch": 0.3874709976798144,
        "step": 3006
    },
    {
        "loss": 1.6791,
        "grad_norm": 1.847198247909546,
        "learning_rate": 9.387267552370156e-05,
        "epoch": 0.38759989688063934,
        "step": 3007
    },
    {
        "loss": 2.086,
        "grad_norm": 2.1290383338928223,
        "learning_rate": 9.384345082519463e-05,
        "epoch": 0.3877287960814643,
        "step": 3008
    },
    {
        "loss": 2.369,
        "grad_norm": 2.0149593353271484,
        "learning_rate": 9.381416116932082e-05,
        "epoch": 0.38785769528228925,
        "step": 3009
    },
    {
        "loss": 2.6255,
        "grad_norm": 1.6589486598968506,
        "learning_rate": 9.378480659947498e-05,
        "epoch": 0.3879865944831142,
        "step": 3010
    },
    {
        "loss": 1.6167,
        "grad_norm": 2.8027029037475586,
        "learning_rate": 9.375538715914811e-05,
        "epoch": 0.38811549368393916,
        "step": 3011
    },
    {
        "loss": 2.6035,
        "grad_norm": 1.3667237758636475,
        "learning_rate": 9.372590289192728e-05,
        "epoch": 0.3882443928847641,
        "step": 3012
    },
    {
        "loss": 1.1969,
        "grad_norm": 3.091787576675415,
        "learning_rate": 9.369635384149571e-05,
        "epoch": 0.3883732920855891,
        "step": 3013
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.84834361076355,
        "learning_rate": 9.366674005163248e-05,
        "epoch": 0.388502191286414,
        "step": 3014
    },
    {
        "loss": 0.7789,
        "grad_norm": 4.059391498565674,
        "learning_rate": 9.363706156621267e-05,
        "epoch": 0.388631090487239,
        "step": 3015
    },
    {
        "loss": 2.3113,
        "grad_norm": 1.7760099172592163,
        "learning_rate": 9.360731842920721e-05,
        "epoch": 0.3887599896880639,
        "step": 3016
    },
    {
        "loss": 2.485,
        "grad_norm": 1.4878178834915161,
        "learning_rate": 9.357751068468275e-05,
        "epoch": 0.3888888888888889,
        "step": 3017
    },
    {
        "loss": 1.9114,
        "grad_norm": 2.2622945308685303,
        "learning_rate": 9.354763837680171e-05,
        "epoch": 0.3890177880897138,
        "step": 3018
    },
    {
        "loss": 1.9387,
        "grad_norm": 2.383702278137207,
        "learning_rate": 9.351770154982222e-05,
        "epoch": 0.3891466872905388,
        "step": 3019
    },
    {
        "loss": 1.9057,
        "grad_norm": 2.5949227809906006,
        "learning_rate": 9.348770024809784e-05,
        "epoch": 0.38927558649136373,
        "step": 3020
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.743011474609375,
        "learning_rate": 9.34576345160778e-05,
        "epoch": 0.3894044856921887,
        "step": 3021
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.7338200807571411,
        "learning_rate": 9.34275043983068e-05,
        "epoch": 0.38953338489301365,
        "step": 3022
    },
    {
        "loss": 1.7422,
        "grad_norm": 2.463920831680298,
        "learning_rate": 9.339730993942477e-05,
        "epoch": 0.38966228409383863,
        "step": 3023
    },
    {
        "loss": 2.3882,
        "grad_norm": 1.401018500328064,
        "learning_rate": 9.336705118416712e-05,
        "epoch": 0.38979118329466356,
        "step": 3024
    },
    {
        "loss": 2.4034,
        "grad_norm": 1.4943486452102661,
        "learning_rate": 9.333672817736448e-05,
        "epoch": 0.38992008249548854,
        "step": 3025
    },
    {
        "loss": 2.0136,
        "grad_norm": 2.4980902671813965,
        "learning_rate": 9.330634096394264e-05,
        "epoch": 0.39004898169631347,
        "step": 3026
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.9258413314819336,
        "learning_rate": 9.327588958892255e-05,
        "epoch": 0.39017788089713845,
        "step": 3027
    },
    {
        "loss": 1.7787,
        "grad_norm": 2.0907557010650635,
        "learning_rate": 9.324537409742023e-05,
        "epoch": 0.3903067800979634,
        "step": 3028
    },
    {
        "loss": 2.2987,
        "grad_norm": 2.2726266384124756,
        "learning_rate": 9.321479453464663e-05,
        "epoch": 0.39043567929878836,
        "step": 3029
    },
    {
        "loss": 2.0545,
        "grad_norm": 2.563349962234497,
        "learning_rate": 9.31841509459077e-05,
        "epoch": 0.3905645784996133,
        "step": 3030
    },
    {
        "loss": 2.6434,
        "grad_norm": 1.4442849159240723,
        "learning_rate": 9.315344337660421e-05,
        "epoch": 0.3906934777004383,
        "step": 3031
    },
    {
        "loss": 1.1983,
        "grad_norm": 3.1057662963867188,
        "learning_rate": 9.312267187223174e-05,
        "epoch": 0.3908223769012632,
        "step": 3032
    },
    {
        "loss": 2.1943,
        "grad_norm": 1.3518097400665283,
        "learning_rate": 9.309183647838056e-05,
        "epoch": 0.3909512761020882,
        "step": 3033
    },
    {
        "loss": 2.3035,
        "grad_norm": 1.465368390083313,
        "learning_rate": 9.306093724073568e-05,
        "epoch": 0.3910801753029131,
        "step": 3034
    },
    {
        "loss": 1.2437,
        "grad_norm": 2.2621898651123047,
        "learning_rate": 9.30299742050766e-05,
        "epoch": 0.3912090745037381,
        "step": 3035
    },
    {
        "loss": 1.8829,
        "grad_norm": 2.766716718673706,
        "learning_rate": 9.299894741727735e-05,
        "epoch": 0.391337973704563,
        "step": 3036
    },
    {
        "loss": 2.0737,
        "grad_norm": 1.737246036529541,
        "learning_rate": 9.296785692330654e-05,
        "epoch": 0.391466872905388,
        "step": 3037
    },
    {
        "loss": 0.9658,
        "grad_norm": 3.2163779735565186,
        "learning_rate": 9.293670276922699e-05,
        "epoch": 0.39159577210621294,
        "step": 3038
    },
    {
        "loss": 2.0258,
        "grad_norm": 2.1868937015533447,
        "learning_rate": 9.290548500119594e-05,
        "epoch": 0.3917246713070379,
        "step": 3039
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.7885140180587769,
        "learning_rate": 9.287420366546491e-05,
        "epoch": 0.39185357050786285,
        "step": 3040
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.324105978012085,
        "learning_rate": 9.284285880837946e-05,
        "epoch": 0.39198246970868783,
        "step": 3041
    },
    {
        "loss": 2.2017,
        "grad_norm": 1.870501160621643,
        "learning_rate": 9.281145047637943e-05,
        "epoch": 0.39211136890951276,
        "step": 3042
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.6388211250305176,
        "learning_rate": 9.27799787159986e-05,
        "epoch": 0.39224026811033774,
        "step": 3043
    },
    {
        "loss": 1.8972,
        "grad_norm": 1.4841326475143433,
        "learning_rate": 9.27484435738647e-05,
        "epoch": 0.39236916731116267,
        "step": 3044
    },
    {
        "loss": 2.1481,
        "grad_norm": 2.163043737411499,
        "learning_rate": 9.271684509669949e-05,
        "epoch": 0.39249806651198765,
        "step": 3045
    },
    {
        "loss": 2.1281,
        "grad_norm": 2.0703070163726807,
        "learning_rate": 9.268518333131847e-05,
        "epoch": 0.3926269657128126,
        "step": 3046
    },
    {
        "loss": 1.8213,
        "grad_norm": 2.824018716812134,
        "learning_rate": 9.265345832463086e-05,
        "epoch": 0.3927558649136375,
        "step": 3047
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.2254810333251953,
        "learning_rate": 9.262167012363972e-05,
        "epoch": 0.3928847641144625,
        "step": 3048
    },
    {
        "loss": 2.2954,
        "grad_norm": 2.7921016216278076,
        "learning_rate": 9.258981877544162e-05,
        "epoch": 0.3930136633152874,
        "step": 3049
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.462406873703003,
        "learning_rate": 9.255790432722672e-05,
        "epoch": 0.3931425625161124,
        "step": 3050
    },
    {
        "loss": 1.5491,
        "grad_norm": 2.672316551208496,
        "learning_rate": 9.25259268262787e-05,
        "epoch": 0.39327146171693733,
        "step": 3051
    },
    {
        "loss": 2.4179,
        "grad_norm": 1.6298243999481201,
        "learning_rate": 9.249388631997462e-05,
        "epoch": 0.3934003609177623,
        "step": 3052
    },
    {
        "loss": 2.3003,
        "grad_norm": 1.8996074199676514,
        "learning_rate": 9.246178285578489e-05,
        "epoch": 0.39352926011858724,
        "step": 3053
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.861735224723816,
        "learning_rate": 9.24296164812732e-05,
        "epoch": 0.3936581593194122,
        "step": 3054
    },
    {
        "loss": 1.5788,
        "grad_norm": Infinity,
        "learning_rate": 9.24296164812732e-05,
        "epoch": 0.39378705852023715,
        "step": 3055
    },
    {
        "loss": 1.869,
        "grad_norm": 2.853421449661255,
        "learning_rate": 9.239738724409647e-05,
        "epoch": 0.39391595772106214,
        "step": 3056
    },
    {
        "loss": 2.0524,
        "grad_norm": 2.098496198654175,
        "learning_rate": 9.236509519200471e-05,
        "epoch": 0.39404485692188707,
        "step": 3057
    },
    {
        "loss": 1.812,
        "grad_norm": 2.31503963470459,
        "learning_rate": 9.233274037284106e-05,
        "epoch": 0.39417375612271205,
        "step": 3058
    },
    {
        "loss": 1.3146,
        "grad_norm": 2.5385711193084717,
        "learning_rate": 9.230032283454159e-05,
        "epoch": 0.394302655323537,
        "step": 3059
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.4026694297790527,
        "learning_rate": 9.22678426251353e-05,
        "epoch": 0.39443155452436196,
        "step": 3060
    },
    {
        "loss": 2.1337,
        "grad_norm": 1.4495790004730225,
        "learning_rate": 9.22352997927441e-05,
        "epoch": 0.3945604537251869,
        "step": 3061
    },
    {
        "loss": 2.2823,
        "grad_norm": 1.333103060722351,
        "learning_rate": 9.220269438558263e-05,
        "epoch": 0.39468935292601187,
        "step": 3062
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.1564345359802246,
        "learning_rate": 9.217002645195824e-05,
        "epoch": 0.3948182521268368,
        "step": 3063
    },
    {
        "loss": 2.4645,
        "grad_norm": 1.7536289691925049,
        "learning_rate": 9.213729604027093e-05,
        "epoch": 0.3949471513276618,
        "step": 3064
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.537653684616089,
        "learning_rate": 9.210450319901327e-05,
        "epoch": 0.3950760505284867,
        "step": 3065
    },
    {
        "loss": 2.5628,
        "grad_norm": 1.830647587776184,
        "learning_rate": 9.207164797677032e-05,
        "epoch": 0.3952049497293117,
        "step": 3066
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.393622636795044,
        "learning_rate": 9.203873042221955e-05,
        "epoch": 0.3953338489301366,
        "step": 3067
    },
    {
        "loss": 2.297,
        "grad_norm": 1.5354483127593994,
        "learning_rate": 9.20057505841308e-05,
        "epoch": 0.3954627481309616,
        "step": 3068
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.2263723611831665,
        "learning_rate": 9.197270851136619e-05,
        "epoch": 0.39559164733178653,
        "step": 3069
    },
    {
        "loss": 2.1849,
        "grad_norm": 1.969435214996338,
        "learning_rate": 9.193960425287999e-05,
        "epoch": 0.3957205465326115,
        "step": 3070
    },
    {
        "loss": 1.491,
        "grad_norm": 2.747277021408081,
        "learning_rate": 9.190643785771871e-05,
        "epoch": 0.39584944573343644,
        "step": 3071
    },
    {
        "loss": 1.555,
        "grad_norm": 3.1004397869110107,
        "learning_rate": 9.18732093750208e-05,
        "epoch": 0.39597834493426143,
        "step": 3072
    },
    {
        "loss": 2.3994,
        "grad_norm": 2.238157272338867,
        "learning_rate": 9.183991885401677e-05,
        "epoch": 0.39610724413508636,
        "step": 3073
    },
    {
        "loss": 1.8492,
        "grad_norm": 3.392930507659912,
        "learning_rate": 9.180656634402904e-05,
        "epoch": 0.39623614333591134,
        "step": 3074
    },
    {
        "loss": 2.0132,
        "grad_norm": 1.581557273864746,
        "learning_rate": 9.177315189447188e-05,
        "epoch": 0.39636504253673627,
        "step": 3075
    },
    {
        "loss": 2.0147,
        "grad_norm": 2.5656614303588867,
        "learning_rate": 9.173967555485126e-05,
        "epoch": 0.39649394173756125,
        "step": 3076
    },
    {
        "loss": 2.2349,
        "grad_norm": 1.5062828063964844,
        "learning_rate": 9.17061373747649e-05,
        "epoch": 0.3966228409383862,
        "step": 3077
    },
    {
        "loss": 2.3095,
        "grad_norm": 2.1199119091033936,
        "learning_rate": 9.16725374039022e-05,
        "epoch": 0.39675174013921116,
        "step": 3078
    },
    {
        "loss": 2.2219,
        "grad_norm": 1.8666967153549194,
        "learning_rate": 9.163887569204395e-05,
        "epoch": 0.3968806393400361,
        "step": 3079
    },
    {
        "loss": 2.3643,
        "grad_norm": 2.460988759994507,
        "learning_rate": 9.160515228906255e-05,
        "epoch": 0.3970095385408611,
        "step": 3080
    },
    {
        "loss": 2.0186,
        "grad_norm": 1.9684268236160278,
        "learning_rate": 9.157136724492176e-05,
        "epoch": 0.397138437741686,
        "step": 3081
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.7249701023101807,
        "learning_rate": 9.153752060967664e-05,
        "epoch": 0.39726733694251093,
        "step": 3082
    },
    {
        "loss": 1.6238,
        "grad_norm": 2.3588311672210693,
        "learning_rate": 9.150361243347354e-05,
        "epoch": 0.3973962361433359,
        "step": 3083
    },
    {
        "loss": 2.2189,
        "grad_norm": 1.3636276721954346,
        "learning_rate": 9.146964276655e-05,
        "epoch": 0.39752513534416084,
        "step": 3084
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.4083845615386963,
        "learning_rate": 9.143561165923455e-05,
        "epoch": 0.3976540345449858,
        "step": 3085
    },
    {
        "loss": 1.2087,
        "grad_norm": 2.551029920578003,
        "learning_rate": 9.140151916194693e-05,
        "epoch": 0.39778293374581075,
        "step": 3086
    },
    {
        "loss": 1.4502,
        "grad_norm": 2.530263900756836,
        "learning_rate": 9.136736532519767e-05,
        "epoch": 0.39791183294663574,
        "step": 3087
    },
    {
        "loss": 2.0729,
        "grad_norm": 1.8915117979049683,
        "learning_rate": 9.133315019958826e-05,
        "epoch": 0.39804073214746066,
        "step": 3088
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.9612178802490234,
        "learning_rate": 9.1298873835811e-05,
        "epoch": 0.39816963134828565,
        "step": 3089
    },
    {
        "loss": 2.1805,
        "grad_norm": 2.1868736743927,
        "learning_rate": 9.126453628464888e-05,
        "epoch": 0.3982985305491106,
        "step": 3090
    },
    {
        "loss": 2.4453,
        "grad_norm": 1.927418828010559,
        "learning_rate": 9.123013759697557e-05,
        "epoch": 0.39842742974993556,
        "step": 3091
    },
    {
        "loss": 1.6642,
        "grad_norm": 2.5114574432373047,
        "learning_rate": 9.119567782375529e-05,
        "epoch": 0.3985563289507605,
        "step": 3092
    },
    {
        "loss": 1.8628,
        "grad_norm": 2.6890602111816406,
        "learning_rate": 9.116115701604282e-05,
        "epoch": 0.39868522815158547,
        "step": 3093
    },
    {
        "loss": 2.026,
        "grad_norm": 1.935796856880188,
        "learning_rate": 9.11265752249833e-05,
        "epoch": 0.3988141273524104,
        "step": 3094
    },
    {
        "loss": 1.761,
        "grad_norm": 2.591571569442749,
        "learning_rate": 9.109193250181227e-05,
        "epoch": 0.3989430265532354,
        "step": 3095
    },
    {
        "loss": 2.2532,
        "grad_norm": 1.753221035003662,
        "learning_rate": 9.105722889785552e-05,
        "epoch": 0.3990719257540603,
        "step": 3096
    },
    {
        "loss": 1.5839,
        "grad_norm": 1.6311132907867432,
        "learning_rate": 9.102246446452905e-05,
        "epoch": 0.3992008249548853,
        "step": 3097
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.4101667404174805,
        "learning_rate": 9.098763925333896e-05,
        "epoch": 0.3993297241557102,
        "step": 3098
    },
    {
        "loss": 2.0475,
        "grad_norm": 2.8462181091308594,
        "learning_rate": 9.095275331588147e-05,
        "epoch": 0.3994586233565352,
        "step": 3099
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.1599974632263184,
        "learning_rate": 9.091780670384264e-05,
        "epoch": 0.39958752255736013,
        "step": 3100
    },
    {
        "loss": 1.9331,
        "grad_norm": 1.9835196733474731,
        "learning_rate": 9.088279946899856e-05,
        "epoch": 0.3997164217581851,
        "step": 3101
    },
    {
        "loss": 1.9104,
        "grad_norm": 2.8004379272460938,
        "learning_rate": 9.08477316632151e-05,
        "epoch": 0.39984532095901004,
        "step": 3102
    },
    {
        "loss": 1.9038,
        "grad_norm": 2.4456303119659424,
        "learning_rate": 9.081260333844778e-05,
        "epoch": 0.399974220159835,
        "step": 3103
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.5186182260513306,
        "learning_rate": 9.077741454674187e-05,
        "epoch": 0.40010311936065995,
        "step": 3104
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.1270203590393066,
        "learning_rate": 9.074216534023225e-05,
        "epoch": 0.40023201856148494,
        "step": 3105
    },
    {
        "loss": 1.8896,
        "grad_norm": 2.5623180866241455,
        "learning_rate": 9.070685577114322e-05,
        "epoch": 0.40036091776230986,
        "step": 3106
    },
    {
        "loss": 2.4014,
        "grad_norm": 1.2590248584747314,
        "learning_rate": 9.06714858917886e-05,
        "epoch": 0.40048981696313485,
        "step": 3107
    },
    {
        "loss": 1.2795,
        "grad_norm": 3.044684886932373,
        "learning_rate": 9.063605575457152e-05,
        "epoch": 0.4006187161639598,
        "step": 3108
    },
    {
        "loss": 1.804,
        "grad_norm": 2.3192288875579834,
        "learning_rate": 9.060056541198436e-05,
        "epoch": 0.40074761536478476,
        "step": 3109
    },
    {
        "loss": 1.7274,
        "grad_norm": 2.779222011566162,
        "learning_rate": 9.056501491660873e-05,
        "epoch": 0.4008765145656097,
        "step": 3110
    },
    {
        "loss": 2.1529,
        "grad_norm": 1.7406560182571411,
        "learning_rate": 9.052940432111543e-05,
        "epoch": 0.40100541376643467,
        "step": 3111
    },
    {
        "loss": 2.579,
        "grad_norm": 1.8605897426605225,
        "learning_rate": 9.049373367826413e-05,
        "epoch": 0.4011343129672596,
        "step": 3112
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.9077539443969727,
        "learning_rate": 9.045800304090362e-05,
        "epoch": 0.4012632121680846,
        "step": 3113
    },
    {
        "loss": 2.153,
        "grad_norm": 2.3113932609558105,
        "learning_rate": 9.042221246197156e-05,
        "epoch": 0.4013921113689095,
        "step": 3114
    },
    {
        "loss": 1.8704,
        "grad_norm": 2.2336373329162598,
        "learning_rate": 9.03863619944943e-05,
        "epoch": 0.4015210105697345,
        "step": 3115
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.3021726608276367,
        "learning_rate": 9.035045169158707e-05,
        "epoch": 0.4016499097705594,
        "step": 3116
    },
    {
        "loss": 2.0666,
        "grad_norm": 2.2289016246795654,
        "learning_rate": 9.031448160645364e-05,
        "epoch": 0.4017788089713844,
        "step": 3117
    },
    {
        "loss": 2.0894,
        "grad_norm": 1.5086390972137451,
        "learning_rate": 9.027845179238643e-05,
        "epoch": 0.40190770817220933,
        "step": 3118
    },
    {
        "loss": 2.0072,
        "grad_norm": 1.9600605964660645,
        "learning_rate": 9.024236230276629e-05,
        "epoch": 0.40203660737303426,
        "step": 3119
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.781388282775879,
        "learning_rate": 9.020621319106251e-05,
        "epoch": 0.40216550657385924,
        "step": 3120
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.5022839307785034,
        "learning_rate": 9.017000451083275e-05,
        "epoch": 0.40229440577468417,
        "step": 3121
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.6106255054473877,
        "learning_rate": 9.013373631572284e-05,
        "epoch": 0.40242330497550916,
        "step": 3122
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.2731304168701172,
        "learning_rate": 9.009740865946686e-05,
        "epoch": 0.4025522041763341,
        "step": 3123
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.1562352180480957,
        "learning_rate": 9.006102159588697e-05,
        "epoch": 0.40268110337715907,
        "step": 3124
    },
    {
        "loss": 2.2274,
        "grad_norm": 2.5406582355499268,
        "learning_rate": 9.002457517889333e-05,
        "epoch": 0.402810002577984,
        "step": 3125
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.655606746673584,
        "learning_rate": 8.998806946248406e-05,
        "epoch": 0.402938901778809,
        "step": 3126
    },
    {
        "loss": 1.8197,
        "grad_norm": 2.392284870147705,
        "learning_rate": 8.99515045007451e-05,
        "epoch": 0.4030678009796339,
        "step": 3127
    },
    {
        "loss": 1.6094,
        "grad_norm": 4.831539154052734,
        "learning_rate": 8.991488034785021e-05,
        "epoch": 0.4031967001804589,
        "step": 3128
    },
    {
        "loss": 1.3935,
        "grad_norm": 2.9861927032470703,
        "learning_rate": 8.98781970580608e-05,
        "epoch": 0.4033255993812838,
        "step": 3129
    },
    {
        "loss": 2.1012,
        "grad_norm": 2.1780433654785156,
        "learning_rate": 8.984145468572593e-05,
        "epoch": 0.4034544985821088,
        "step": 3130
    },
    {
        "loss": 2.0833,
        "grad_norm": 1.8756057024002075,
        "learning_rate": 8.980465328528219e-05,
        "epoch": 0.40358339778293373,
        "step": 3131
    },
    {
        "loss": 2.4972,
        "grad_norm": 1.2622053623199463,
        "learning_rate": 8.976779291125363e-05,
        "epoch": 0.4037122969837587,
        "step": 3132
    },
    {
        "loss": 0.465,
        "grad_norm": 1.668549656867981,
        "learning_rate": 8.973087361825164e-05,
        "epoch": 0.40384119618458364,
        "step": 3133
    },
    {
        "loss": 2.2023,
        "grad_norm": 2.3045263290405273,
        "learning_rate": 8.969389546097495e-05,
        "epoch": 0.4039700953854086,
        "step": 3134
    },
    {
        "loss": 1.9118,
        "grad_norm": 1.6340235471725464,
        "learning_rate": 8.965685849420946e-05,
        "epoch": 0.40409899458623355,
        "step": 3135
    },
    {
        "loss": 1.9594,
        "grad_norm": 1.7830537557601929,
        "learning_rate": 8.961976277282821e-05,
        "epoch": 0.40422789378705853,
        "step": 3136
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.1284977197647095,
        "learning_rate": 8.958260835179136e-05,
        "epoch": 0.40435679298788346,
        "step": 3137
    },
    {
        "loss": 2.4829,
        "grad_norm": 1.9425677061080933,
        "learning_rate": 8.95453952861459e-05,
        "epoch": 0.40448569218870845,
        "step": 3138
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.3663125038146973,
        "learning_rate": 8.950812363102581e-05,
        "epoch": 0.4046145913895334,
        "step": 3139
    },
    {
        "loss": 1.6805,
        "grad_norm": 2.8055121898651123,
        "learning_rate": 8.947079344165187e-05,
        "epoch": 0.40474349059035836,
        "step": 3140
    },
    {
        "loss": 1.4326,
        "grad_norm": 3.7767722606658936,
        "learning_rate": 8.943340477333151e-05,
        "epoch": 0.4048723897911833,
        "step": 3141
    },
    {
        "loss": 2.3457,
        "grad_norm": 1.5659379959106445,
        "learning_rate": 8.939595768145894e-05,
        "epoch": 0.40500128899200827,
        "step": 3142
    },
    {
        "loss": 2.2516,
        "grad_norm": 2.2847492694854736,
        "learning_rate": 8.935845222151476e-05,
        "epoch": 0.4051301881928332,
        "step": 3143
    },
    {
        "loss": 2.6317,
        "grad_norm": 1.9253170490264893,
        "learning_rate": 8.932088844906613e-05,
        "epoch": 0.4052590873936582,
        "step": 3144
    },
    {
        "loss": 1.8046,
        "grad_norm": 2.8144705295562744,
        "learning_rate": 8.928326641976665e-05,
        "epoch": 0.4053879865944831,
        "step": 3145
    },
    {
        "loss": 2.2725,
        "grad_norm": 1.9568909406661987,
        "learning_rate": 8.924558618935617e-05,
        "epoch": 0.4055168857953081,
        "step": 3146
    },
    {
        "loss": 1.8912,
        "grad_norm": 2.4110116958618164,
        "learning_rate": 8.920784781366072e-05,
        "epoch": 0.405645784996133,
        "step": 3147
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.4910637140274048,
        "learning_rate": 8.917005134859264e-05,
        "epoch": 0.405774684196958,
        "step": 3148
    },
    {
        "loss": 1.7734,
        "grad_norm": 2.2184741497039795,
        "learning_rate": 8.913219685015015e-05,
        "epoch": 0.40590358339778293,
        "step": 3149
    },
    {
        "loss": 2.4889,
        "grad_norm": 1.4842454195022583,
        "learning_rate": 8.909428437441758e-05,
        "epoch": 0.4060324825986079,
        "step": 3150
    },
    {
        "loss": 2.0407,
        "grad_norm": 2.2988884449005127,
        "learning_rate": 8.905631397756512e-05,
        "epoch": 0.40616138179943284,
        "step": 3151
    },
    {
        "loss": 2.3966,
        "grad_norm": 1.5072444677352905,
        "learning_rate": 8.90182857158487e-05,
        "epoch": 0.4062902810002578,
        "step": 3152
    },
    {
        "loss": 2.6021,
        "grad_norm": 1.7993706464767456,
        "learning_rate": 8.898019964561012e-05,
        "epoch": 0.40641918020108275,
        "step": 3153
    },
    {
        "loss": 1.2987,
        "grad_norm": 2.748487949371338,
        "learning_rate": 8.894205582327675e-05,
        "epoch": 0.4065480794019077,
        "step": 3154
    },
    {
        "loss": 2.1104,
        "grad_norm": 1.8541346788406372,
        "learning_rate": 8.890385430536145e-05,
        "epoch": 0.40667697860273266,
        "step": 3155
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.4435245990753174,
        "learning_rate": 8.886559514846272e-05,
        "epoch": 0.4068058778035576,
        "step": 3156
    },
    {
        "loss": 2.0299,
        "grad_norm": 1.8701512813568115,
        "learning_rate": 8.882727840926435e-05,
        "epoch": 0.4069347770043826,
        "step": 3157
    },
    {
        "loss": 2.3676,
        "grad_norm": 1.539504051208496,
        "learning_rate": 8.878890414453546e-05,
        "epoch": 0.4070636762052075,
        "step": 3158
    },
    {
        "loss": 1.8334,
        "grad_norm": 2.3572075366973877,
        "learning_rate": 8.875047241113044e-05,
        "epoch": 0.4071925754060325,
        "step": 3159
    },
    {
        "loss": 2.17,
        "grad_norm": 1.889274001121521,
        "learning_rate": 8.871198326598876e-05,
        "epoch": 0.4073214746068574,
        "step": 3160
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.4201080799102783,
        "learning_rate": 8.867343676613497e-05,
        "epoch": 0.4074503738076824,
        "step": 3161
    },
    {
        "loss": 1.7937,
        "grad_norm": 2.0162692070007324,
        "learning_rate": 8.863483296867863e-05,
        "epoch": 0.4075792730085073,
        "step": 3162
    },
    {
        "loss": 1.9538,
        "grad_norm": 2.9931881427764893,
        "learning_rate": 8.859617193081417e-05,
        "epoch": 0.4077081722093323,
        "step": 3163
    },
    {
        "loss": 2.171,
        "grad_norm": 1.575510025024414,
        "learning_rate": 8.85574537098208e-05,
        "epoch": 0.40783707141015724,
        "step": 3164
    },
    {
        "loss": 1.4845,
        "grad_norm": 2.644926071166992,
        "learning_rate": 8.851867836306247e-05,
        "epoch": 0.4079659706109822,
        "step": 3165
    },
    {
        "loss": 2.6626,
        "grad_norm": 1.210445523262024,
        "learning_rate": 8.847984594798783e-05,
        "epoch": 0.40809486981180715,
        "step": 3166
    },
    {
        "loss": 1.9924,
        "grad_norm": 1.7869794368743896,
        "learning_rate": 8.844095652212996e-05,
        "epoch": 0.40822376901263213,
        "step": 3167
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.1093648672103882,
        "learning_rate": 8.840201014310645e-05,
        "epoch": 0.40835266821345706,
        "step": 3168
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.5267888307571411,
        "learning_rate": 8.836300686861937e-05,
        "epoch": 0.40848156741428204,
        "step": 3169
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.3658080101013184,
        "learning_rate": 8.832394675645492e-05,
        "epoch": 0.40861046661510697,
        "step": 3170
    },
    {
        "loss": 1.4386,
        "grad_norm": 2.334216356277466,
        "learning_rate": 8.828482986448362e-05,
        "epoch": 0.40873936581593195,
        "step": 3171
    },
    {
        "loss": 2.236,
        "grad_norm": 2.2571756839752197,
        "learning_rate": 8.824565625066013e-05,
        "epoch": 0.4088682650167569,
        "step": 3172
    },
    {
        "loss": 2.0269,
        "grad_norm": 2.8627898693084717,
        "learning_rate": 8.820642597302301e-05,
        "epoch": 0.40899716421758187,
        "step": 3173
    },
    {
        "loss": 2.1941,
        "grad_norm": 2.302368402481079,
        "learning_rate": 8.816713908969493e-05,
        "epoch": 0.4091260634184068,
        "step": 3174
    },
    {
        "loss": 2.0717,
        "grad_norm": 1.8307527303695679,
        "learning_rate": 8.812779565888239e-05,
        "epoch": 0.4092549626192318,
        "step": 3175
    },
    {
        "loss": 1.3237,
        "grad_norm": 3.026979684829712,
        "learning_rate": 8.808839573887554e-05,
        "epoch": 0.4093838618200567,
        "step": 3176
    },
    {
        "loss": 1.3665,
        "grad_norm": 2.2447190284729004,
        "learning_rate": 8.804893938804838e-05,
        "epoch": 0.4095127610208817,
        "step": 3177
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.2840903997421265,
        "learning_rate": 8.800942666485849e-05,
        "epoch": 0.4096416602217066,
        "step": 3178
    },
    {
        "loss": 1.6212,
        "grad_norm": 2.477938413619995,
        "learning_rate": 8.796985762784686e-05,
        "epoch": 0.4097705594225316,
        "step": 3179
    },
    {
        "loss": 1.8931,
        "grad_norm": 2.536086320877075,
        "learning_rate": 8.793023233563807e-05,
        "epoch": 0.4098994586233565,
        "step": 3180
    },
    {
        "loss": 1.9411,
        "grad_norm": 2.681321859359741,
        "learning_rate": 8.789055084693991e-05,
        "epoch": 0.4100283578241815,
        "step": 3181
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.072615385055542,
        "learning_rate": 8.785081322054351e-05,
        "epoch": 0.41015725702500644,
        "step": 3182
    },
    {
        "loss": 2.316,
        "grad_norm": 2.280480146408081,
        "learning_rate": 8.781101951532316e-05,
        "epoch": 0.4102861562258314,
        "step": 3183
    },
    {
        "loss": 2.1422,
        "grad_norm": 1.4556992053985596,
        "learning_rate": 8.77711697902362e-05,
        "epoch": 0.41041505542665635,
        "step": 3184
    },
    {
        "loss": 2.2581,
        "grad_norm": 1.3945624828338623,
        "learning_rate": 8.773126410432302e-05,
        "epoch": 0.41054395462748133,
        "step": 3185
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.2381722927093506,
        "learning_rate": 8.769130251670687e-05,
        "epoch": 0.41067285382830626,
        "step": 3186
    },
    {
        "loss": 1.7822,
        "grad_norm": 3.1180460453033447,
        "learning_rate": 8.765128508659386e-05,
        "epoch": 0.41080175302913124,
        "step": 3187
    },
    {
        "loss": 1.9726,
        "grad_norm": 1.5004361867904663,
        "learning_rate": 8.761121187327282e-05,
        "epoch": 0.4109306522299562,
        "step": 3188
    },
    {
        "loss": 1.6398,
        "grad_norm": 2.0707943439483643,
        "learning_rate": 8.75710829361152e-05,
        "epoch": 0.41105955143078116,
        "step": 3189
    },
    {
        "loss": 2.5034,
        "grad_norm": 1.9170948266983032,
        "learning_rate": 8.753089833457506e-05,
        "epoch": 0.4111884506316061,
        "step": 3190
    },
    {
        "loss": 2.3924,
        "grad_norm": 2.252674102783203,
        "learning_rate": 8.749065812818891e-05,
        "epoch": 0.411317349832431,
        "step": 3191
    },
    {
        "loss": 1.9483,
        "grad_norm": 3.2979660034179688,
        "learning_rate": 8.745036237657562e-05,
        "epoch": 0.411446249033256,
        "step": 3192
    },
    {
        "loss": 2.0611,
        "grad_norm": 2.7900922298431396,
        "learning_rate": 8.74100111394364e-05,
        "epoch": 0.4115751482340809,
        "step": 3193
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.717118978500366,
        "learning_rate": 8.736960447655461e-05,
        "epoch": 0.4117040474349059,
        "step": 3194
    },
    {
        "loss": 2.3819,
        "grad_norm": 1.9083619117736816,
        "learning_rate": 8.732914244779579e-05,
        "epoch": 0.41183294663573083,
        "step": 3195
    },
    {
        "loss": 2.5759,
        "grad_norm": 1.8584500551223755,
        "learning_rate": 8.728862511310744e-05,
        "epoch": 0.4119618458365558,
        "step": 3196
    },
    {
        "loss": 2.2505,
        "grad_norm": 2.126117467880249,
        "learning_rate": 8.724805253251907e-05,
        "epoch": 0.41209074503738075,
        "step": 3197
    },
    {
        "loss": 2.1762,
        "grad_norm": 1.555345058441162,
        "learning_rate": 8.720742476614197e-05,
        "epoch": 0.41221964423820573,
        "step": 3198
    },
    {
        "loss": 2.524,
        "grad_norm": 1.8389030694961548,
        "learning_rate": 8.716674187416927e-05,
        "epoch": 0.41234854343903066,
        "step": 3199
    },
    {
        "loss": 2.4222,
        "grad_norm": 1.3004777431488037,
        "learning_rate": 8.712600391687569e-05,
        "epoch": 0.41247744263985564,
        "step": 3200
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.3207075595855713,
        "learning_rate": 8.708521095461761e-05,
        "epoch": 0.41260634184068057,
        "step": 3201
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.324784755706787,
        "learning_rate": 8.704436304783286e-05,
        "epoch": 0.41273524104150555,
        "step": 3202
    },
    {
        "loss": 2.3322,
        "grad_norm": 1.4766122102737427,
        "learning_rate": 8.700346025704067e-05,
        "epoch": 0.4128641402423305,
        "step": 3203
    },
    {
        "loss": 2.4964,
        "grad_norm": 2.026332139968872,
        "learning_rate": 8.69625026428416e-05,
        "epoch": 0.41299303944315546,
        "step": 3204
    },
    {
        "loss": 2.3978,
        "grad_norm": 2.1616923809051514,
        "learning_rate": 8.692149026591745e-05,
        "epoch": 0.4131219386439804,
        "step": 3205
    },
    {
        "loss": 2.0417,
        "grad_norm": 1.6455692052841187,
        "learning_rate": 8.688042318703113e-05,
        "epoch": 0.4132508378448054,
        "step": 3206
    },
    {
        "loss": 2.1226,
        "grad_norm": 2.450076103210449,
        "learning_rate": 8.68393014670266e-05,
        "epoch": 0.4133797370456303,
        "step": 3207
    },
    {
        "loss": 2.2156,
        "grad_norm": 1.72061288356781,
        "learning_rate": 8.679812516682874e-05,
        "epoch": 0.4135086362464553,
        "step": 3208
    },
    {
        "loss": 2.4154,
        "grad_norm": 1.2949802875518799,
        "learning_rate": 8.675689434744343e-05,
        "epoch": 0.4136375354472802,
        "step": 3209
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.698393702507019,
        "learning_rate": 8.671560906995715e-05,
        "epoch": 0.4137664346481052,
        "step": 3210
    },
    {
        "loss": 2.6399,
        "grad_norm": 1.3269950151443481,
        "learning_rate": 8.667426939553712e-05,
        "epoch": 0.4138953338489301,
        "step": 3211
    },
    {
        "loss": 2.3417,
        "grad_norm": 1.7009023427963257,
        "learning_rate": 8.663287538543128e-05,
        "epoch": 0.4140242330497551,
        "step": 3212
    },
    {
        "loss": 2.016,
        "grad_norm": 2.3513123989105225,
        "learning_rate": 8.659142710096786e-05,
        "epoch": 0.41415313225058004,
        "step": 3213
    },
    {
        "loss": 2.1395,
        "grad_norm": 2.161837577819824,
        "learning_rate": 8.654992460355563e-05,
        "epoch": 0.414282031451405,
        "step": 3214
    },
    {
        "loss": 1.6053,
        "grad_norm": 2.789320945739746,
        "learning_rate": 8.650836795468372e-05,
        "epoch": 0.41441093065222995,
        "step": 3215
    },
    {
        "loss": 1.8852,
        "grad_norm": 2.238029956817627,
        "learning_rate": 8.646675721592135e-05,
        "epoch": 0.41453982985305493,
        "step": 3216
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.6764030456542969,
        "learning_rate": 8.6425092448918e-05,
        "epoch": 0.41466872905387986,
        "step": 3217
    },
    {
        "loss": 1.2076,
        "grad_norm": 2.6434903144836426,
        "learning_rate": 8.638337371540318e-05,
        "epoch": 0.41479762825470484,
        "step": 3218
    },
    {
        "loss": 1.6333,
        "grad_norm": 2.3312759399414062,
        "learning_rate": 8.634160107718623e-05,
        "epoch": 0.41492652745552977,
        "step": 3219
    },
    {
        "loss": 2.1748,
        "grad_norm": 3.4914608001708984,
        "learning_rate": 8.629977459615655e-05,
        "epoch": 0.41505542665635475,
        "step": 3220
    },
    {
        "loss": 2.2285,
        "grad_norm": 1.6139994859695435,
        "learning_rate": 8.62578943342832e-05,
        "epoch": 0.4151843258571797,
        "step": 3221
    },
    {
        "loss": 1.7722,
        "grad_norm": 1.992498517036438,
        "learning_rate": 8.621596035361486e-05,
        "epoch": 0.41531322505800466,
        "step": 3222
    },
    {
        "loss": 2.0839,
        "grad_norm": 1.9681211709976196,
        "learning_rate": 8.617397271627995e-05,
        "epoch": 0.4154421242588296,
        "step": 3223
    },
    {
        "loss": 1.4659,
        "grad_norm": 3.008150100708008,
        "learning_rate": 8.61319314844863e-05,
        "epoch": 0.4155710234596546,
        "step": 3224
    },
    {
        "loss": 1.511,
        "grad_norm": 1.5750550031661987,
        "learning_rate": 8.608983672052112e-05,
        "epoch": 0.4156999226604795,
        "step": 3225
    },
    {
        "loss": 1.9598,
        "grad_norm": 2.6589126586914062,
        "learning_rate": 8.6047688486751e-05,
        "epoch": 0.4158288218613045,
        "step": 3226
    },
    {
        "loss": 1.7749,
        "grad_norm": 2.0831823348999023,
        "learning_rate": 8.60054868456217e-05,
        "epoch": 0.4159577210621294,
        "step": 3227
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.8711693286895752,
        "learning_rate": 8.596323185965813e-05,
        "epoch": 0.41608662026295434,
        "step": 3228
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.326616048812866,
        "learning_rate": 8.592092359146422e-05,
        "epoch": 0.4162155194637793,
        "step": 3229
    },
    {
        "loss": 1.5643,
        "grad_norm": 2.7635653018951416,
        "learning_rate": 8.587856210372285e-05,
        "epoch": 0.41634441866460425,
        "step": 3230
    },
    {
        "loss": 1.7055,
        "grad_norm": 1.9370195865631104,
        "learning_rate": 8.583614745919574e-05,
        "epoch": 0.41647331786542924,
        "step": 3231
    },
    {
        "loss": 2.4706,
        "grad_norm": 2.4598209857940674,
        "learning_rate": 8.579367972072336e-05,
        "epoch": 0.41660221706625417,
        "step": 3232
    },
    {
        "loss": 2.3937,
        "grad_norm": 2.4302256107330322,
        "learning_rate": 8.575115895122492e-05,
        "epoch": 0.41673111626707915,
        "step": 3233
    },
    {
        "loss": 1.316,
        "grad_norm": 6.033386707305908,
        "learning_rate": 8.570858521369807e-05,
        "epoch": 0.4168600154679041,
        "step": 3234
    },
    {
        "loss": 1.9776,
        "grad_norm": 2.311647653579712,
        "learning_rate": 8.5665958571219e-05,
        "epoch": 0.41698891466872906,
        "step": 3235
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.10721492767334,
        "learning_rate": 8.562327908694235e-05,
        "epoch": 0.417117813869554,
        "step": 3236
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.499755859375,
        "learning_rate": 8.558054682410091e-05,
        "epoch": 0.41724671307037897,
        "step": 3237
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.0226521492004395,
        "learning_rate": 8.553776184600576e-05,
        "epoch": 0.4173756122712039,
        "step": 3238
    },
    {
        "loss": 1.8867,
        "grad_norm": 2.6988866329193115,
        "learning_rate": 8.54949242160461e-05,
        "epoch": 0.4175045114720289,
        "step": 3239
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.607635736465454,
        "learning_rate": 8.545203399768902e-05,
        "epoch": 0.4176334106728538,
        "step": 3240
    },
    {
        "loss": 2.227,
        "grad_norm": 2.163425922393799,
        "learning_rate": 8.540909125447966e-05,
        "epoch": 0.4177623098736788,
        "step": 3241
    },
    {
        "loss": 2.5141,
        "grad_norm": 2.115119218826294,
        "learning_rate": 8.536609605004095e-05,
        "epoch": 0.4178912090745037,
        "step": 3242
    },
    {
        "loss": 1.7468,
        "grad_norm": 3.2122511863708496,
        "learning_rate": 8.532304844807343e-05,
        "epoch": 0.4180201082753287,
        "step": 3243
    },
    {
        "loss": 2.2747,
        "grad_norm": 1.404571294784546,
        "learning_rate": 8.527994851235542e-05,
        "epoch": 0.41814900747615363,
        "step": 3244
    },
    {
        "loss": 2.3875,
        "grad_norm": 2.536113977432251,
        "learning_rate": 8.523679630674271e-05,
        "epoch": 0.4182779066769786,
        "step": 3245
    },
    {
        "loss": 2.1637,
        "grad_norm": 2.0747694969177246,
        "learning_rate": 8.519359189516847e-05,
        "epoch": 0.41840680587780354,
        "step": 3246
    },
    {
        "loss": 1.5802,
        "grad_norm": 2.5860753059387207,
        "learning_rate": 8.515033534164334e-05,
        "epoch": 0.41853570507862853,
        "step": 3247
    },
    {
        "loss": 2.1115,
        "grad_norm": 2.1827502250671387,
        "learning_rate": 8.510702671025517e-05,
        "epoch": 0.41866460427945346,
        "step": 3248
    },
    {
        "loss": 1.7042,
        "grad_norm": 2.074406862258911,
        "learning_rate": 8.506366606516892e-05,
        "epoch": 0.41879350348027844,
        "step": 3249
    },
    {
        "loss": 1.8883,
        "grad_norm": 2.573761224746704,
        "learning_rate": 8.502025347062666e-05,
        "epoch": 0.41892240268110337,
        "step": 3250
    },
    {
        "loss": 2.4085,
        "grad_norm": 1.1560521125793457,
        "learning_rate": 8.497678899094737e-05,
        "epoch": 0.41905130188192835,
        "step": 3251
    },
    {
        "loss": 1.825,
        "grad_norm": 2.188675880432129,
        "learning_rate": 8.4933272690527e-05,
        "epoch": 0.4191802010827533,
        "step": 3252
    },
    {
        "loss": 2.2985,
        "grad_norm": 1.611586332321167,
        "learning_rate": 8.488970463383818e-05,
        "epoch": 0.41930910028357826,
        "step": 3253
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.606344223022461,
        "learning_rate": 8.484608488543029e-05,
        "epoch": 0.4194379994844032,
        "step": 3254
    },
    {
        "loss": 2.4312,
        "grad_norm": 1.6129889488220215,
        "learning_rate": 8.480241350992924e-05,
        "epoch": 0.4195668986852282,
        "step": 3255
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.29146409034729,
        "learning_rate": 8.475869057203746e-05,
        "epoch": 0.4196957978860531,
        "step": 3256
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.1963388919830322,
        "learning_rate": 8.471491613653377e-05,
        "epoch": 0.4198246970868781,
        "step": 3257
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.2899231910705566,
        "learning_rate": 8.467109026827329e-05,
        "epoch": 0.419953596287703,
        "step": 3258
    },
    {
        "loss": 2.1566,
        "grad_norm": 1.281550407409668,
        "learning_rate": 8.462721303218734e-05,
        "epoch": 0.420082495488528,
        "step": 3259
    },
    {
        "loss": 1.3601,
        "grad_norm": 3.0909032821655273,
        "learning_rate": 8.458328449328331e-05,
        "epoch": 0.4202113946893529,
        "step": 3260
    },
    {
        "loss": 2.3235,
        "grad_norm": 2.650881767272949,
        "learning_rate": 8.453930471664468e-05,
        "epoch": 0.4203402938901779,
        "step": 3261
    },
    {
        "loss": 1.8518,
        "grad_norm": 2.353610038757324,
        "learning_rate": 8.449527376743076e-05,
        "epoch": 0.42046919309100284,
        "step": 3262
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.0263419151306152,
        "learning_rate": 8.445119171087671e-05,
        "epoch": 0.42059809229182776,
        "step": 3263
    },
    {
        "loss": 2.1174,
        "grad_norm": 1.3884270191192627,
        "learning_rate": 8.440705861229345e-05,
        "epoch": 0.42072699149265275,
        "step": 3264
    },
    {
        "loss": 2.7294,
        "grad_norm": 1.4888458251953125,
        "learning_rate": 8.436287453706744e-05,
        "epoch": 0.4208558906934777,
        "step": 3265
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.6061339378356934,
        "learning_rate": 8.431863955066071e-05,
        "epoch": 0.42098478989430266,
        "step": 3266
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.3179166316986084,
        "learning_rate": 8.427435371861072e-05,
        "epoch": 0.4211136890951276,
        "step": 3267
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.2877557277679443,
        "learning_rate": 8.423001710653026e-05,
        "epoch": 0.42124258829595257,
        "step": 3268
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.393000841140747,
        "learning_rate": 8.418562978010737e-05,
        "epoch": 0.4213714874967775,
        "step": 3269
    },
    {
        "loss": 2.1478,
        "grad_norm": 2.7317731380462646,
        "learning_rate": 8.414119180510518e-05,
        "epoch": 0.4215003866976025,
        "step": 3270
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.4704865217208862,
        "learning_rate": 8.409670324736192e-05,
        "epoch": 0.4216292858984274,
        "step": 3271
    },
    {
        "loss": 1.4585,
        "grad_norm": 2.3984999656677246,
        "learning_rate": 8.405216417279067e-05,
        "epoch": 0.4217581850992524,
        "step": 3272
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.787851572036743,
        "learning_rate": 8.400757464737951e-05,
        "epoch": 0.4218870843000773,
        "step": 3273
    },
    {
        "loss": 2.3769,
        "grad_norm": 1.5373284816741943,
        "learning_rate": 8.396293473719113e-05,
        "epoch": 0.4220159835009023,
        "step": 3274
    },
    {
        "loss": 2.0062,
        "grad_norm": 2.122853994369507,
        "learning_rate": 8.391824450836285e-05,
        "epoch": 0.42214488270172723,
        "step": 3275
    },
    {
        "loss": 1.9784,
        "grad_norm": 2.2443463802337646,
        "learning_rate": 8.387350402710671e-05,
        "epoch": 0.4222737819025522,
        "step": 3276
    },
    {
        "loss": 2.4357,
        "grad_norm": 1.8096307516098022,
        "learning_rate": 8.382871335970902e-05,
        "epoch": 0.42240268110337714,
        "step": 3277
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.8020328283309937,
        "learning_rate": 8.378387257253051e-05,
        "epoch": 0.4225315803042021,
        "step": 3278
    },
    {
        "loss": 1.6891,
        "grad_norm": 2.149010181427002,
        "learning_rate": 8.373898173200627e-05,
        "epoch": 0.42266047950502705,
        "step": 3279
    },
    {
        "loss": 1.6,
        "grad_norm": 2.6367077827453613,
        "learning_rate": 8.369404090464533e-05,
        "epoch": 0.42278937870585204,
        "step": 3280
    },
    {
        "loss": 1.6435,
        "grad_norm": 2.6941416263580322,
        "learning_rate": 8.364905015703098e-05,
        "epoch": 0.42291827790667696,
        "step": 3281
    },
    {
        "loss": 2.3104,
        "grad_norm": 1.4700983762741089,
        "learning_rate": 8.360400955582042e-05,
        "epoch": 0.42304717710750195,
        "step": 3282
    },
    {
        "loss": 2.6507,
        "grad_norm": 1.4403846263885498,
        "learning_rate": 8.355891916774458e-05,
        "epoch": 0.4231760763083269,
        "step": 3283
    },
    {
        "loss": 1.0518,
        "grad_norm": 2.5060527324676514,
        "learning_rate": 8.351377905960833e-05,
        "epoch": 0.42330497550915186,
        "step": 3284
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.3763337135314941,
        "learning_rate": 8.346858929829016e-05,
        "epoch": 0.4234338747099768,
        "step": 3285
    },
    {
        "loss": 1.6212,
        "grad_norm": 2.1552977561950684,
        "learning_rate": 8.342334995074201e-05,
        "epoch": 0.42356277391080177,
        "step": 3286
    },
    {
        "loss": 2.174,
        "grad_norm": 1.5906509160995483,
        "learning_rate": 8.337806108398944e-05,
        "epoch": 0.4236916731116267,
        "step": 3287
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.873089075088501,
        "learning_rate": 8.33327227651313e-05,
        "epoch": 0.4238205723124517,
        "step": 3288
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.3703513145446777,
        "learning_rate": 8.32873350613397e-05,
        "epoch": 0.4239494715132766,
        "step": 3289
    },
    {
        "loss": 2.4788,
        "grad_norm": 1.8092790842056274,
        "learning_rate": 8.324189803985992e-05,
        "epoch": 0.4240783707141016,
        "step": 3290
    },
    {
        "loss": 2.1454,
        "grad_norm": 1.18380606174469,
        "learning_rate": 8.319641176801036e-05,
        "epoch": 0.4242072699149265,
        "step": 3291
    },
    {
        "loss": 2.1117,
        "grad_norm": 1.4365262985229492,
        "learning_rate": 8.315087631318233e-05,
        "epoch": 0.4243361691157515,
        "step": 3292
    },
    {
        "loss": 1.6274,
        "grad_norm": 4.78048038482666,
        "learning_rate": 8.310529174284004e-05,
        "epoch": 0.42446506831657643,
        "step": 3293
    },
    {
        "loss": 1.5894,
        "grad_norm": 1.9547500610351562,
        "learning_rate": 8.305965812452041e-05,
        "epoch": 0.4245939675174014,
        "step": 3294
    },
    {
        "loss": 1.6573,
        "grad_norm": 2.3581037521362305,
        "learning_rate": 8.301397552583314e-05,
        "epoch": 0.42472286671822634,
        "step": 3295
    },
    {
        "loss": 1.6479,
        "grad_norm": 1.480320692062378,
        "learning_rate": 8.29682440144604e-05,
        "epoch": 0.4248517659190513,
        "step": 3296
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.9519158601760864,
        "learning_rate": 8.292246365815685e-05,
        "epoch": 0.42498066511987626,
        "step": 3297
    },
    {
        "loss": 1.9619,
        "grad_norm": 2.6773393154144287,
        "learning_rate": 8.287663452474954e-05,
        "epoch": 0.42510956432070124,
        "step": 3298
    },
    {
        "loss": 1.4836,
        "grad_norm": 2.5247654914855957,
        "learning_rate": 8.283075668213776e-05,
        "epoch": 0.42523846352152617,
        "step": 3299
    },
    {
        "loss": 2.3457,
        "grad_norm": 2.377291679382324,
        "learning_rate": 8.278483019829304e-05,
        "epoch": 0.4253673627223511,
        "step": 3300
    },
    {
        "loss": 2.1267,
        "grad_norm": 2.3039393424987793,
        "learning_rate": 8.273885514125885e-05,
        "epoch": 0.4254962619231761,
        "step": 3301
    },
    {
        "loss": 1.5004,
        "grad_norm": 3.130256414413452,
        "learning_rate": 8.26928315791507e-05,
        "epoch": 0.425625161124001,
        "step": 3302
    },
    {
        "loss": 1.7375,
        "grad_norm": 2.6967074871063232,
        "learning_rate": 8.264675958015601e-05,
        "epoch": 0.425754060324826,
        "step": 3303
    },
    {
        "loss": 1.6928,
        "grad_norm": 2.8665499687194824,
        "learning_rate": 8.260063921253384e-05,
        "epoch": 0.4258829595256509,
        "step": 3304
    },
    {
        "loss": 1.8633,
        "grad_norm": 1.5329300165176392,
        "learning_rate": 8.255447054461499e-05,
        "epoch": 0.4260118587264759,
        "step": 3305
    },
    {
        "loss": 1.9688,
        "grad_norm": 2.3264706134796143,
        "learning_rate": 8.250825364480188e-05,
        "epoch": 0.42614075792730083,
        "step": 3306
    },
    {
        "loss": 1.8486,
        "grad_norm": 2.5890214443206787,
        "learning_rate": 8.246198858156822e-05,
        "epoch": 0.4262696571281258,
        "step": 3307
    },
    {
        "loss": 1.6503,
        "grad_norm": 3.419138193130493,
        "learning_rate": 8.241567542345924e-05,
        "epoch": 0.42639855632895074,
        "step": 3308
    },
    {
        "loss": 1.991,
        "grad_norm": 2.252958059310913,
        "learning_rate": 8.23693142390914e-05,
        "epoch": 0.4265274555297757,
        "step": 3309
    },
    {
        "loss": 2.3595,
        "grad_norm": 1.9441508054733276,
        "learning_rate": 8.232290509715218e-05,
        "epoch": 0.42665635473060065,
        "step": 3310
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.4037189483642578,
        "learning_rate": 8.227644806640026e-05,
        "epoch": 0.42678525393142563,
        "step": 3311
    },
    {
        "loss": 1.9192,
        "grad_norm": 2.484851121902466,
        "learning_rate": 8.222994321566526e-05,
        "epoch": 0.42691415313225056,
        "step": 3312
    },
    {
        "loss": 2.2898,
        "grad_norm": 1.7149912118911743,
        "learning_rate": 8.218339061384752e-05,
        "epoch": 0.42704305233307555,
        "step": 3313
    },
    {
        "loss": 2.306,
        "grad_norm": 1.9658262729644775,
        "learning_rate": 8.21367903299183e-05,
        "epoch": 0.4271719515339005,
        "step": 3314
    },
    {
        "loss": 1.253,
        "grad_norm": 2.5332064628601074,
        "learning_rate": 8.209014243291939e-05,
        "epoch": 0.42730085073472546,
        "step": 3315
    },
    {
        "loss": 2.0976,
        "grad_norm": 1.5129222869873047,
        "learning_rate": 8.204344699196315e-05,
        "epoch": 0.4274297499355504,
        "step": 3316
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.381131172180176,
        "learning_rate": 8.199670407623241e-05,
        "epoch": 0.42755864913637537,
        "step": 3317
    },
    {
        "loss": 2.2538,
        "grad_norm": 1.9411594867706299,
        "learning_rate": 8.194991375498029e-05,
        "epoch": 0.4276875483372003,
        "step": 3318
    },
    {
        "loss": 2.1103,
        "grad_norm": 2.0581674575805664,
        "learning_rate": 8.190307609753016e-05,
        "epoch": 0.4278164475380253,
        "step": 3319
    },
    {
        "loss": 1.9024,
        "grad_norm": 2.0010576248168945,
        "learning_rate": 8.185619117327554e-05,
        "epoch": 0.4279453467388502,
        "step": 3320
    },
    {
        "loss": 1.9041,
        "grad_norm": 1.2215230464935303,
        "learning_rate": 8.180925905167997e-05,
        "epoch": 0.4280742459396752,
        "step": 3321
    },
    {
        "loss": 2.3573,
        "grad_norm": 1.8495320081710815,
        "learning_rate": 8.176227980227694e-05,
        "epoch": 0.4282031451405001,
        "step": 3322
    },
    {
        "loss": 2.5918,
        "grad_norm": 1.2572389841079712,
        "learning_rate": 8.171525349466969e-05,
        "epoch": 0.4283320443413251,
        "step": 3323
    },
    {
        "loss": 2.4978,
        "grad_norm": 1.5928200483322144,
        "learning_rate": 8.166818019853125e-05,
        "epoch": 0.42846094354215003,
        "step": 3324
    },
    {
        "loss": 2.0827,
        "grad_norm": 2.481200695037842,
        "learning_rate": 8.162105998360423e-05,
        "epoch": 0.428589842742975,
        "step": 3325
    },
    {
        "loss": 2.2292,
        "grad_norm": 1.5813136100769043,
        "learning_rate": 8.15738929197008e-05,
        "epoch": 0.42871874194379994,
        "step": 3326
    },
    {
        "loss": 2.1795,
        "grad_norm": 1.4573276042938232,
        "learning_rate": 8.15266790767025e-05,
        "epoch": 0.4288476411446249,
        "step": 3327
    },
    {
        "loss": 1.9652,
        "grad_norm": 2.01224946975708,
        "learning_rate": 8.147941852456015e-05,
        "epoch": 0.42897654034544985,
        "step": 3328
    },
    {
        "loss": 1.7851,
        "grad_norm": 1.0861965417861938,
        "learning_rate": 8.143211133329386e-05,
        "epoch": 0.42910543954627484,
        "step": 3329
    },
    {
        "loss": 2.1616,
        "grad_norm": 2.6086840629577637,
        "learning_rate": 8.138475757299277e-05,
        "epoch": 0.42923433874709976,
        "step": 3330
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.8571019172668457,
        "learning_rate": 8.133735731381503e-05,
        "epoch": 0.42936323794792475,
        "step": 3331
    },
    {
        "loss": 2.0662,
        "grad_norm": 2.1836483478546143,
        "learning_rate": 8.128991062598769e-05,
        "epoch": 0.4294921371487497,
        "step": 3332
    },
    {
        "loss": 1.9575,
        "grad_norm": 1.4537433385849,
        "learning_rate": 8.124241757980657e-05,
        "epoch": 0.42962103634957466,
        "step": 3333
    },
    {
        "loss": 2.2579,
        "grad_norm": 2.1489217281341553,
        "learning_rate": 8.11948782456362e-05,
        "epoch": 0.4297499355503996,
        "step": 3334
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.7171708345413208,
        "learning_rate": 8.114729269390968e-05,
        "epoch": 0.42987883475122457,
        "step": 3335
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.7391130924224854,
        "learning_rate": 8.109966099512856e-05,
        "epoch": 0.4300077339520495,
        "step": 3336
    },
    {
        "loss": 2.4213,
        "grad_norm": 2.1645147800445557,
        "learning_rate": 8.10519832198628e-05,
        "epoch": 0.4301366331528744,
        "step": 3337
    },
    {
        "loss": 2.1458,
        "grad_norm": 1.5623579025268555,
        "learning_rate": 8.100425943875062e-05,
        "epoch": 0.4302655323536994,
        "step": 3338
    },
    {
        "loss": 2.3201,
        "grad_norm": 1.8426729440689087,
        "learning_rate": 8.095648972249831e-05,
        "epoch": 0.43039443155452434,
        "step": 3339
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.7313222885131836,
        "learning_rate": 8.090867414188044e-05,
        "epoch": 0.4305233307553493,
        "step": 3340
    },
    {
        "loss": 2.1174,
        "grad_norm": 1.3135621547698975,
        "learning_rate": 8.086081276773924e-05,
        "epoch": 0.43065222995617425,
        "step": 3341
    },
    {
        "loss": 1.6821,
        "grad_norm": 2.357524871826172,
        "learning_rate": 8.081290567098498e-05,
        "epoch": 0.43078112915699923,
        "step": 3342
    },
    {
        "loss": 1.3953,
        "grad_norm": 4.475275993347168,
        "learning_rate": 8.076495292259569e-05,
        "epoch": 0.43091002835782416,
        "step": 3343
    },
    {
        "loss": 2.592,
        "grad_norm": 1.298221468925476,
        "learning_rate": 8.071695459361687e-05,
        "epoch": 0.43103892755864914,
        "step": 3344
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.938965082168579,
        "learning_rate": 8.06689107551617e-05,
        "epoch": 0.43116782675947407,
        "step": 3345
    },
    {
        "loss": 1.9473,
        "grad_norm": 2.2344746589660645,
        "learning_rate": 8.062082147841075e-05,
        "epoch": 0.43129672596029905,
        "step": 3346
    },
    {
        "loss": 2.1465,
        "grad_norm": 2.0659470558166504,
        "learning_rate": 8.057268683461185e-05,
        "epoch": 0.431425625161124,
        "step": 3347
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.0542423725128174,
        "learning_rate": 8.052450689508013e-05,
        "epoch": 0.43155452436194897,
        "step": 3348
    },
    {
        "loss": 2.6306,
        "grad_norm": 1.3875404596328735,
        "learning_rate": 8.047628173119784e-05,
        "epoch": 0.4316834235627739,
        "step": 3349
    },
    {
        "loss": 2.4517,
        "grad_norm": 1.47301185131073,
        "learning_rate": 8.042801141441403e-05,
        "epoch": 0.4318123227635989,
        "step": 3350
    },
    {
        "loss": 2.1118,
        "grad_norm": 1.934685230255127,
        "learning_rate": 8.037969601624495e-05,
        "epoch": 0.4319412219644238,
        "step": 3351
    },
    {
        "loss": 1.7963,
        "grad_norm": 2.883993625640869,
        "learning_rate": 8.033133560827346e-05,
        "epoch": 0.4320701211652488,
        "step": 3352
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.8224605321884155,
        "learning_rate": 8.028293026214905e-05,
        "epoch": 0.4321990203660737,
        "step": 3353
    },
    {
        "loss": 1.7474,
        "grad_norm": 2.439758777618408,
        "learning_rate": 8.0234480049588e-05,
        "epoch": 0.4323279195668987,
        "step": 3354
    },
    {
        "loss": 2.1533,
        "grad_norm": 1.6441521644592285,
        "learning_rate": 8.018598504237287e-05,
        "epoch": 0.4324568187677236,
        "step": 3355
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.9035587310791016,
        "learning_rate": 8.013744531235268e-05,
        "epoch": 0.4325857179685486,
        "step": 3356
    },
    {
        "loss": 2.259,
        "grad_norm": 1.5457890033721924,
        "learning_rate": 8.008886093144268e-05,
        "epoch": 0.43271461716937354,
        "step": 3357
    },
    {
        "loss": 2.1959,
        "grad_norm": 1.8677403926849365,
        "learning_rate": 8.004023197162427e-05,
        "epoch": 0.4328435163701985,
        "step": 3358
    },
    {
        "loss": 1.3942,
        "grad_norm": 2.37434720993042,
        "learning_rate": 7.999155850494493e-05,
        "epoch": 0.43297241557102345,
        "step": 3359
    },
    {
        "loss": 2.4936,
        "grad_norm": 1.7983267307281494,
        "learning_rate": 7.994284060351804e-05,
        "epoch": 0.43310131477184843,
        "step": 3360
    },
    {
        "loss": 2.2398,
        "grad_norm": 1.8724970817565918,
        "learning_rate": 7.989407833952286e-05,
        "epoch": 0.43323021397267336,
        "step": 3361
    },
    {
        "loss": 1.2852,
        "grad_norm": 3.2546701431274414,
        "learning_rate": 7.98452717852043e-05,
        "epoch": 0.43335911317349834,
        "step": 3362
    },
    {
        "loss": 1.8135,
        "grad_norm": 2.110325813293457,
        "learning_rate": 7.979642101287297e-05,
        "epoch": 0.4334880123743233,
        "step": 3363
    },
    {
        "loss": 1.9104,
        "grad_norm": 2.2671196460723877,
        "learning_rate": 7.974752609490499e-05,
        "epoch": 0.43361691157514826,
        "step": 3364
    },
    {
        "loss": 2.1964,
        "grad_norm": 2.087984085083008,
        "learning_rate": 7.96985871037418e-05,
        "epoch": 0.4337458107759732,
        "step": 3365
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.5990805625915527,
        "learning_rate": 7.964960411189017e-05,
        "epoch": 0.43387470997679817,
        "step": 3366
    },
    {
        "loss": 1.5937,
        "grad_norm": 4.1446967124938965,
        "learning_rate": 7.96005771919222e-05,
        "epoch": 0.4340036091776231,
        "step": 3367
    },
    {
        "loss": 2.7517,
        "grad_norm": 1.880619764328003,
        "learning_rate": 7.955150641647485e-05,
        "epoch": 0.4341325083784481,
        "step": 3368
    },
    {
        "loss": 2.5951,
        "grad_norm": 1.3639105558395386,
        "learning_rate": 7.950239185825017e-05,
        "epoch": 0.434261407579273,
        "step": 3369
    },
    {
        "loss": 2.067,
        "grad_norm": 2.8605918884277344,
        "learning_rate": 7.945323359001513e-05,
        "epoch": 0.434390306780098,
        "step": 3370
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.597109317779541,
        "learning_rate": 7.940403168460133e-05,
        "epoch": 0.4345192059809229,
        "step": 3371
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.542440414428711,
        "learning_rate": 7.935478621490514e-05,
        "epoch": 0.43464810518174785,
        "step": 3372
    },
    {
        "loss": 1.9195,
        "grad_norm": 1.9733495712280273,
        "learning_rate": 7.930549725388741e-05,
        "epoch": 0.43477700438257283,
        "step": 3373
    },
    {
        "loss": 2.3395,
        "grad_norm": 2.2243235111236572,
        "learning_rate": 7.92561648745734e-05,
        "epoch": 0.43490590358339776,
        "step": 3374
    },
    {
        "loss": 1.9459,
        "grad_norm": 1.7446541786193848,
        "learning_rate": 7.92067891500528e-05,
        "epoch": 0.43503480278422274,
        "step": 3375
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.419617176055908,
        "learning_rate": 7.915737015347944e-05,
        "epoch": 0.43516370198504767,
        "step": 3376
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.2654081583023071,
        "learning_rate": 7.910790795807121e-05,
        "epoch": 0.43529260118587265,
        "step": 3377
    },
    {
        "loss": 1.8172,
        "grad_norm": 2.403642416000366,
        "learning_rate": 7.905840263711015e-05,
        "epoch": 0.4354215003866976,
        "step": 3378
    },
    {
        "loss": 2.2513,
        "grad_norm": 2.5413708686828613,
        "learning_rate": 7.900885426394208e-05,
        "epoch": 0.43555039958752256,
        "step": 3379
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.2259654998779297,
        "learning_rate": 7.895926291197665e-05,
        "epoch": 0.4356792987883475,
        "step": 3380
    },
    {
        "loss": 1.8713,
        "grad_norm": 2.311965227127075,
        "learning_rate": 7.890962865468719e-05,
        "epoch": 0.4358081979891725,
        "step": 3381
    },
    {
        "loss": 2.689,
        "grad_norm": 1.1759263277053833,
        "learning_rate": 7.885995156561054e-05,
        "epoch": 0.4359370971899974,
        "step": 3382
    },
    {
        "loss": 2.3576,
        "grad_norm": 2.107699155807495,
        "learning_rate": 7.881023171834705e-05,
        "epoch": 0.4360659963908224,
        "step": 3383
    },
    {
        "loss": 1.7482,
        "grad_norm": 2.5284016132354736,
        "learning_rate": 7.876046918656044e-05,
        "epoch": 0.4361948955916473,
        "step": 3384
    },
    {
        "loss": 2.0049,
        "grad_norm": 1.60472571849823,
        "learning_rate": 7.87106640439776e-05,
        "epoch": 0.4363237947924723,
        "step": 3385
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.5305243730545044,
        "learning_rate": 7.866081636438863e-05,
        "epoch": 0.4364526939932972,
        "step": 3386
    },
    {
        "loss": 2.1745,
        "grad_norm": 2.3390676975250244,
        "learning_rate": 7.861092622164657e-05,
        "epoch": 0.4365815931941222,
        "step": 3387
    },
    {
        "loss": 2.0822,
        "grad_norm": 1.8145227432250977,
        "learning_rate": 7.856099368966745e-05,
        "epoch": 0.43671049239494714,
        "step": 3388
    },
    {
        "loss": 2.1503,
        "grad_norm": 2.335470676422119,
        "learning_rate": 7.851101884243007e-05,
        "epoch": 0.4368393915957721,
        "step": 3389
    },
    {
        "loss": 1.0768,
        "grad_norm": 2.6716971397399902,
        "learning_rate": 7.846100175397588e-05,
        "epoch": 0.43696829079659705,
        "step": 3390
    },
    {
        "loss": 1.8969,
        "grad_norm": 2.0843327045440674,
        "learning_rate": 7.8410942498409e-05,
        "epoch": 0.43709718999742203,
        "step": 3391
    },
    {
        "loss": 2.3041,
        "grad_norm": 1.620503544807434,
        "learning_rate": 7.836084114989597e-05,
        "epoch": 0.43722608919824696,
        "step": 3392
    },
    {
        "loss": 2.122,
        "grad_norm": 2.3373560905456543,
        "learning_rate": 7.831069778266569e-05,
        "epoch": 0.43735498839907194,
        "step": 3393
    },
    {
        "loss": 2.1846,
        "grad_norm": 2.658536195755005,
        "learning_rate": 7.826051247100931e-05,
        "epoch": 0.43748388759989687,
        "step": 3394
    },
    {
        "loss": 2.4865,
        "grad_norm": 1.883945107460022,
        "learning_rate": 7.821028528928018e-05,
        "epoch": 0.43761278680072185,
        "step": 3395
    },
    {
        "loss": 1.6292,
        "grad_norm": 2.4939167499542236,
        "learning_rate": 7.816001631189363e-05,
        "epoch": 0.4377416860015468,
        "step": 3396
    },
    {
        "loss": 2.317,
        "grad_norm": 2.139498233795166,
        "learning_rate": 7.810970561332692e-05,
        "epoch": 0.43787058520237176,
        "step": 3397
    },
    {
        "loss": 2.4483,
        "grad_norm": 1.5948032140731812,
        "learning_rate": 7.805935326811912e-05,
        "epoch": 0.4379994844031967,
        "step": 3398
    },
    {
        "loss": 2.6083,
        "grad_norm": 1.7568230628967285,
        "learning_rate": 7.800895935087104e-05,
        "epoch": 0.4381283836040217,
        "step": 3399
    },
    {
        "loss": 2.4913,
        "grad_norm": 2.197232961654663,
        "learning_rate": 7.795852393624503e-05,
        "epoch": 0.4382572828048466,
        "step": 3400
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.4182928800582886,
        "learning_rate": 7.790804709896497e-05,
        "epoch": 0.4383861820056716,
        "step": 3401
    },
    {
        "loss": 1.6792,
        "grad_norm": 2.032256603240967,
        "learning_rate": 7.785752891381608e-05,
        "epoch": 0.4385150812064965,
        "step": 3402
    },
    {
        "loss": 2.121,
        "grad_norm": 2.4835970401763916,
        "learning_rate": 7.780696945564482e-05,
        "epoch": 0.4386439804073215,
        "step": 3403
    },
    {
        "loss": 1.3322,
        "grad_norm": 3.088491201400757,
        "learning_rate": 7.775636879935886e-05,
        "epoch": 0.4387728796081464,
        "step": 3404
    },
    {
        "loss": 2.1828,
        "grad_norm": 2.4087963104248047,
        "learning_rate": 7.770572701992686e-05,
        "epoch": 0.4389017788089714,
        "step": 3405
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.7493280172348022,
        "learning_rate": 7.76550441923784e-05,
        "epoch": 0.43903067800979634,
        "step": 3406
    },
    {
        "loss": 1.9746,
        "grad_norm": 2.318392515182495,
        "learning_rate": 7.760432039180396e-05,
        "epoch": 0.4391595772106213,
        "step": 3407
    },
    {
        "loss": 1.8051,
        "grad_norm": 2.707949638366699,
        "learning_rate": 7.755355569335462e-05,
        "epoch": 0.43928847641144625,
        "step": 3408
    },
    {
        "loss": 2.0508,
        "grad_norm": 2.159846067428589,
        "learning_rate": 7.750275017224205e-05,
        "epoch": 0.4394173756122712,
        "step": 3409
    },
    {
        "loss": 2.32,
        "grad_norm": 1.703865647315979,
        "learning_rate": 7.745190390373858e-05,
        "epoch": 0.43954627481309616,
        "step": 3410
    },
    {
        "loss": 1.2112,
        "grad_norm": 2.6180121898651123,
        "learning_rate": 7.740101696317663e-05,
        "epoch": 0.4396751740139211,
        "step": 3411
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.9145526885986328,
        "learning_rate": 7.735008942594909e-05,
        "epoch": 0.43980407321474607,
        "step": 3412
    },
    {
        "loss": 1.462,
        "grad_norm": 2.2023539543151855,
        "learning_rate": 7.729912136750899e-05,
        "epoch": 0.439932972415571,
        "step": 3413
    },
    {
        "loss": 2.1982,
        "grad_norm": 2.303631544113159,
        "learning_rate": 7.724811286336922e-05,
        "epoch": 0.440061871616396,
        "step": 3414
    },
    {
        "loss": 1.9795,
        "grad_norm": 1.920522689819336,
        "learning_rate": 7.719706398910278e-05,
        "epoch": 0.4401907708172209,
        "step": 3415
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.3214452266693115,
        "learning_rate": 7.714597482034246e-05,
        "epoch": 0.4403196700180459,
        "step": 3416
    },
    {
        "loss": 1.6415,
        "grad_norm": 2.49438738822937,
        "learning_rate": 7.709484543278058e-05,
        "epoch": 0.4404485692188708,
        "step": 3417
    },
    {
        "loss": 2.3073,
        "grad_norm": 1.5085177421569824,
        "learning_rate": 7.704367590216928e-05,
        "epoch": 0.4405774684196958,
        "step": 3418
    },
    {
        "loss": 1.8942,
        "grad_norm": 2.140017509460449,
        "learning_rate": 7.699246630432004e-05,
        "epoch": 0.44070636762052073,
        "step": 3419
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.4655832052230835,
        "learning_rate": 7.694121671510365e-05,
        "epoch": 0.4408352668213457,
        "step": 3420
    },
    {
        "loss": 2.0395,
        "grad_norm": 2.83610200881958,
        "learning_rate": 7.688992721045032e-05,
        "epoch": 0.44096416602217064,
        "step": 3421
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.5645623207092285,
        "learning_rate": 7.683859786634928e-05,
        "epoch": 0.44109306522299563,
        "step": 3422
    },
    {
        "loss": 1.9448,
        "grad_norm": 1.6535212993621826,
        "learning_rate": 7.67872287588488e-05,
        "epoch": 0.44122196442382056,
        "step": 3423
    },
    {
        "loss": 2.5775,
        "grad_norm": 1.6109033823013306,
        "learning_rate": 7.673581996405608e-05,
        "epoch": 0.44135086362464554,
        "step": 3424
    },
    {
        "loss": 1.3296,
        "grad_norm": 2.925766944885254,
        "learning_rate": 7.66843715581371e-05,
        "epoch": 0.44147976282547047,
        "step": 3425
    },
    {
        "loss": 2.1693,
        "grad_norm": 1.5336813926696777,
        "learning_rate": 7.663288361731656e-05,
        "epoch": 0.44160866202629545,
        "step": 3426
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.1719858646392822,
        "learning_rate": 7.658135621787768e-05,
        "epoch": 0.4417375612271204,
        "step": 3427
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.8329012393951416,
        "learning_rate": 7.65297894361622e-05,
        "epoch": 0.44186646042794536,
        "step": 3428
    },
    {
        "loss": 2.0298,
        "grad_norm": 1.5003154277801514,
        "learning_rate": 7.647818334857018e-05,
        "epoch": 0.4419953596287703,
        "step": 3429
    },
    {
        "loss": 2.2264,
        "grad_norm": 2.9264206886291504,
        "learning_rate": 7.642653803155989e-05,
        "epoch": 0.4421242588295953,
        "step": 3430
    },
    {
        "loss": 2.411,
        "grad_norm": 1.578819751739502,
        "learning_rate": 7.637485356164782e-05,
        "epoch": 0.4422531580304202,
        "step": 3431
    },
    {
        "loss": 1.8248,
        "grad_norm": 3.06536865234375,
        "learning_rate": 7.632313001540832e-05,
        "epoch": 0.4423820572312452,
        "step": 3432
    },
    {
        "loss": 2.1957,
        "grad_norm": 2.217047691345215,
        "learning_rate": 7.627136746947372e-05,
        "epoch": 0.4425109564320701,
        "step": 3433
    },
    {
        "loss": 1.9603,
        "grad_norm": 1.6646654605865479,
        "learning_rate": 7.62195660005342e-05,
        "epoch": 0.4426398556328951,
        "step": 3434
    },
    {
        "loss": 2.14,
        "grad_norm": 2.022850513458252,
        "learning_rate": 7.616772568533741e-05,
        "epoch": 0.44276875483372,
        "step": 3435
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.0496814250946045,
        "learning_rate": 7.611584660068872e-05,
        "epoch": 0.442897654034545,
        "step": 3436
    },
    {
        "loss": 2.4001,
        "grad_norm": 1.6083182096481323,
        "learning_rate": 7.606392882345095e-05,
        "epoch": 0.44302655323536994,
        "step": 3437
    },
    {
        "loss": 1.0231,
        "grad_norm": 2.723281145095825,
        "learning_rate": 7.60119724305441e-05,
        "epoch": 0.4431554524361949,
        "step": 3438
    },
    {
        "loss": 1.8977,
        "grad_norm": 2.6696150302886963,
        "learning_rate": 7.595997749894552e-05,
        "epoch": 0.44328435163701985,
        "step": 3439
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.799922466278076,
        "learning_rate": 7.590794410568964e-05,
        "epoch": 0.44341325083784483,
        "step": 3440
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.6425793170928955,
        "learning_rate": 7.585587232786775e-05,
        "epoch": 0.44354215003866976,
        "step": 3441
    },
    {
        "loss": 1.9003,
        "grad_norm": 2.7521860599517822,
        "learning_rate": 7.580376224262815e-05,
        "epoch": 0.44367104923949474,
        "step": 3442
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.3787190914154053,
        "learning_rate": 7.575161392717592e-05,
        "epoch": 0.44379994844031967,
        "step": 3443
    },
    {
        "loss": 1.9442,
        "grad_norm": 1.9661837816238403,
        "learning_rate": 7.569942745877254e-05,
        "epoch": 0.4439288476411446,
        "step": 3444
    },
    {
        "loss": 1.9878,
        "grad_norm": 2.419980049133301,
        "learning_rate": 7.564720291473633e-05,
        "epoch": 0.4440577468419696,
        "step": 3445
    },
    {
        "loss": 1.511,
        "grad_norm": 2.813152551651001,
        "learning_rate": 7.559494037244183e-05,
        "epoch": 0.4441866460427945,
        "step": 3446
    },
    {
        "loss": 1.6554,
        "grad_norm": 2.903553009033203,
        "learning_rate": 7.55426399093199e-05,
        "epoch": 0.4443155452436195,
        "step": 3447
    },
    {
        "loss": 2.1793,
        "grad_norm": 2.458867311477661,
        "learning_rate": 7.549030160285765e-05,
        "epoch": 0.4444444444444444,
        "step": 3448
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.9150505065917969,
        "learning_rate": 7.543792553059817e-05,
        "epoch": 0.4445733436452694,
        "step": 3449
    },
    {
        "loss": 1.6064,
        "grad_norm": 1.9951488971710205,
        "learning_rate": 7.53855117701406e-05,
        "epoch": 0.44470224284609433,
        "step": 3450
    },
    {
        "loss": 1.5763,
        "grad_norm": 2.905043601989746,
        "learning_rate": 7.53330603991398e-05,
        "epoch": 0.4448311420469193,
        "step": 3451
    },
    {
        "loss": 1.9677,
        "grad_norm": 1.537114143371582,
        "learning_rate": 7.528057149530645e-05,
        "epoch": 0.44496004124774424,
        "step": 3452
    },
    {
        "loss": 2.1408,
        "grad_norm": 1.655453085899353,
        "learning_rate": 7.522804513640682e-05,
        "epoch": 0.4450889404485692,
        "step": 3453
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.357372522354126,
        "learning_rate": 7.517548140026264e-05,
        "epoch": 0.44521783964939415,
        "step": 3454
    },
    {
        "loss": 2.1049,
        "grad_norm": 1.4038910865783691,
        "learning_rate": 7.512288036475103e-05,
        "epoch": 0.44534673885021914,
        "step": 3455
    },
    {
        "loss": 2.2458,
        "grad_norm": 2.525552988052368,
        "learning_rate": 7.50702421078044e-05,
        "epoch": 0.44547563805104406,
        "step": 3456
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.742981433868408,
        "learning_rate": 7.501756670741025e-05,
        "epoch": 0.44560453725186905,
        "step": 3457
    },
    {
        "loss": 2.6304,
        "grad_norm": 1.9484281539916992,
        "learning_rate": 7.496485424161116e-05,
        "epoch": 0.445733436452694,
        "step": 3458
    },
    {
        "loss": 1.3891,
        "grad_norm": 2.6384806632995605,
        "learning_rate": 7.49121047885046e-05,
        "epoch": 0.44586233565351896,
        "step": 3459
    },
    {
        "loss": 1.5711,
        "grad_norm": 2.9162402153015137,
        "learning_rate": 7.485931842624289e-05,
        "epoch": 0.4459912348543439,
        "step": 3460
    },
    {
        "loss": 2.047,
        "grad_norm": 1.8966132402420044,
        "learning_rate": 7.480649523303294e-05,
        "epoch": 0.44612013405516887,
        "step": 3461
    },
    {
        "loss": 1.8103,
        "grad_norm": 2.2246172428131104,
        "learning_rate": 7.475363528713629e-05,
        "epoch": 0.4462490332559938,
        "step": 3462
    },
    {
        "loss": 2.432,
        "grad_norm": 1.9083317518234253,
        "learning_rate": 7.470073866686895e-05,
        "epoch": 0.4463779324568188,
        "step": 3463
    },
    {
        "loss": 1.8608,
        "grad_norm": 2.1501729488372803,
        "learning_rate": 7.464780545060121e-05,
        "epoch": 0.4465068316576437,
        "step": 3464
    },
    {
        "loss": 1.9248,
        "grad_norm": 2.828986883163452,
        "learning_rate": 7.459483571675762e-05,
        "epoch": 0.4466357308584687,
        "step": 3465
    },
    {
        "loss": 2.7176,
        "grad_norm": 1.3804693222045898,
        "learning_rate": 7.454182954381681e-05,
        "epoch": 0.4467646300592936,
        "step": 3466
    },
    {
        "loss": 1.9887,
        "grad_norm": 2.064976215362549,
        "learning_rate": 7.448878701031144e-05,
        "epoch": 0.4468935292601186,
        "step": 3467
    },
    {
        "loss": 2.1629,
        "grad_norm": 1.7857928276062012,
        "learning_rate": 7.443570819482796e-05,
        "epoch": 0.44702242846094353,
        "step": 3468
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.716853141784668,
        "learning_rate": 7.438259317600665e-05,
        "epoch": 0.4471513276617685,
        "step": 3469
    },
    {
        "loss": 1.6804,
        "grad_norm": 2.3663196563720703,
        "learning_rate": 7.432944203254139e-05,
        "epoch": 0.44728022686259344,
        "step": 3470
    },
    {
        "loss": 2.2137,
        "grad_norm": 2.656498670578003,
        "learning_rate": 7.427625484317963e-05,
        "epoch": 0.4474091260634184,
        "step": 3471
    },
    {
        "loss": 2.4258,
        "grad_norm": 1.3140008449554443,
        "learning_rate": 7.422303168672217e-05,
        "epoch": 0.44753802526424336,
        "step": 3472
    },
    {
        "loss": 1.966,
        "grad_norm": 2.1911559104919434,
        "learning_rate": 7.416977264202305e-05,
        "epoch": 0.44766692446506834,
        "step": 3473
    },
    {
        "loss": 2.3138,
        "grad_norm": 1.4241589307785034,
        "learning_rate": 7.411647778798967e-05,
        "epoch": 0.44779582366589327,
        "step": 3474
    },
    {
        "loss": 2.0121,
        "grad_norm": 2.54765248298645,
        "learning_rate": 7.406314720358228e-05,
        "epoch": 0.44792472286671825,
        "step": 3475
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.76764976978302,
        "learning_rate": 7.40097809678141e-05,
        "epoch": 0.4480536220675432,
        "step": 3476
    },
    {
        "loss": 1.104,
        "grad_norm": 2.5921170711517334,
        "learning_rate": 7.395637915975136e-05,
        "epoch": 0.44818252126836816,
        "step": 3477
    },
    {
        "loss": 2.5328,
        "grad_norm": 1.9780545234680176,
        "learning_rate": 7.390294185851274e-05,
        "epoch": 0.4483114204691931,
        "step": 3478
    },
    {
        "loss": 2.2429,
        "grad_norm": 1.258124589920044,
        "learning_rate": 7.38494691432696e-05,
        "epoch": 0.4484403196700181,
        "step": 3479
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.4752721786499023,
        "learning_rate": 7.379596109324593e-05,
        "epoch": 0.448569218870843,
        "step": 3480
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.4708998203277588,
        "learning_rate": 7.374241778771774e-05,
        "epoch": 0.44869811807166793,
        "step": 3481
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.71205472946167,
        "learning_rate": 7.368883930601361e-05,
        "epoch": 0.4488270172724929,
        "step": 3482
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.63161039352417,
        "learning_rate": 7.363522572751404e-05,
        "epoch": 0.44895591647331784,
        "step": 3483
    },
    {
        "loss": 2.495,
        "grad_norm": 1.8790183067321777,
        "learning_rate": 7.35815771316515e-05,
        "epoch": 0.4490848156741428,
        "step": 3484
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.7839478254318237,
        "learning_rate": 7.352789359791053e-05,
        "epoch": 0.44921371487496775,
        "step": 3485
    },
    {
        "loss": 2.4099,
        "grad_norm": 2.5562870502471924,
        "learning_rate": 7.347417520582727e-05,
        "epoch": 0.44934261407579273,
        "step": 3486
    },
    {
        "loss": 2.5151,
        "grad_norm": 1.6754357814788818,
        "learning_rate": 7.342042203498951e-05,
        "epoch": 0.44947151327661766,
        "step": 3487
    },
    {
        "loss": 1.1755,
        "grad_norm": 5.103466510772705,
        "learning_rate": 7.336663416503669e-05,
        "epoch": 0.44960041247744265,
        "step": 3488
    },
    {
        "loss": 1.201,
        "grad_norm": 2.507678508758545,
        "learning_rate": 7.331281167565954e-05,
        "epoch": 0.4497293116782676,
        "step": 3489
    },
    {
        "loss": 2.1183,
        "grad_norm": 1.6271824836730957,
        "learning_rate": 7.325895464660012e-05,
        "epoch": 0.44985821087909256,
        "step": 3490
    },
    {
        "loss": 2.1972,
        "grad_norm": 1.9092085361480713,
        "learning_rate": 7.320506315765168e-05,
        "epoch": 0.4499871100799175,
        "step": 3491
    },
    {
        "loss": 2.5157,
        "grad_norm": 1.5568578243255615,
        "learning_rate": 7.31511372886585e-05,
        "epoch": 0.45011600928074247,
        "step": 3492
    },
    {
        "loss": 1.7421,
        "grad_norm": 2.4185292720794678,
        "learning_rate": 7.309717711951581e-05,
        "epoch": 0.4502449084815674,
        "step": 3493
    },
    {
        "loss": 1.6553,
        "grad_norm": 3.3174450397491455,
        "learning_rate": 7.304318273016967e-05,
        "epoch": 0.4503738076823924,
        "step": 3494
    },
    {
        "loss": 2.196,
        "grad_norm": 2.594789505004883,
        "learning_rate": 7.29891542006168e-05,
        "epoch": 0.4505027068832173,
        "step": 3495
    },
    {
        "loss": 1.5084,
        "grad_norm": 1.773483157157898,
        "learning_rate": 7.293509161090454e-05,
        "epoch": 0.4506316060840423,
        "step": 3496
    },
    {
        "loss": 1.5124,
        "grad_norm": 2.1469833850860596,
        "learning_rate": 7.288099504113065e-05,
        "epoch": 0.4507605052848672,
        "step": 3497
    },
    {
        "loss": 2.2995,
        "grad_norm": 1.656424880027771,
        "learning_rate": 7.282686457144334e-05,
        "epoch": 0.4508894044856922,
        "step": 3498
    },
    {
        "loss": 2.6985,
        "grad_norm": 2.1955904960632324,
        "learning_rate": 7.277270028204089e-05,
        "epoch": 0.45101830368651713,
        "step": 3499
    },
    {
        "loss": 2.1625,
        "grad_norm": 1.885485291481018,
        "learning_rate": 7.271850225317176e-05,
        "epoch": 0.4511472028873421,
        "step": 3500
    },
    {
        "loss": 2.2271,
        "grad_norm": 1.7332303524017334,
        "learning_rate": 7.266427056513451e-05,
        "epoch": 0.45127610208816704,
        "step": 3501
    },
    {
        "loss": 1.9659,
        "grad_norm": 2.082005262374878,
        "learning_rate": 7.261000529827734e-05,
        "epoch": 0.451405001288992,
        "step": 3502
    },
    {
        "loss": 1.6453,
        "grad_norm": 2.8358335494995117,
        "learning_rate": 7.255570653299835e-05,
        "epoch": 0.45153390048981695,
        "step": 3503
    },
    {
        "loss": 2.3197,
        "grad_norm": 2.4934399127960205,
        "learning_rate": 7.250137434974531e-05,
        "epoch": 0.45166279969064194,
        "step": 3504
    },
    {
        "loss": 2.1065,
        "grad_norm": 1.4064412117004395,
        "learning_rate": 7.244700882901531e-05,
        "epoch": 0.45179169889146686,
        "step": 3505
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.7593764066696167,
        "learning_rate": 7.239261005135508e-05,
        "epoch": 0.45192059809229185,
        "step": 3506
    },
    {
        "loss": 1.8743,
        "grad_norm": 1.997173547744751,
        "learning_rate": 7.233817809736045e-05,
        "epoch": 0.4520494972931168,
        "step": 3507
    },
    {
        "loss": 1.9637,
        "grad_norm": 2.3136446475982666,
        "learning_rate": 7.228371304767638e-05,
        "epoch": 0.45217839649394176,
        "step": 3508
    },
    {
        "loss": 2.6442,
        "grad_norm": 1.8555032014846802,
        "learning_rate": 7.222921498299704e-05,
        "epoch": 0.4523072956947667,
        "step": 3509
    },
    {
        "loss": 2.2923,
        "grad_norm": 2.1116790771484375,
        "learning_rate": 7.217468398406537e-05,
        "epoch": 0.45243619489559167,
        "step": 3510
    },
    {
        "loss": 1.6634,
        "grad_norm": 1.9182438850402832,
        "learning_rate": 7.212012013167309e-05,
        "epoch": 0.4525650940964166,
        "step": 3511
    },
    {
        "loss": 2.1693,
        "grad_norm": 1.262150764465332,
        "learning_rate": 7.20655235066607e-05,
        "epoch": 0.4526939932972416,
        "step": 3512
    },
    {
        "loss": 1.9345,
        "grad_norm": 1.8738046884536743,
        "learning_rate": 7.201089418991722e-05,
        "epoch": 0.4528228924980665,
        "step": 3513
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.2621471881866455,
        "learning_rate": 7.195623226238005e-05,
        "epoch": 0.4529517916988915,
        "step": 3514
    },
    {
        "loss": 2.0015,
        "grad_norm": 2.2662408351898193,
        "learning_rate": 7.190153780503495e-05,
        "epoch": 0.4530806908997164,
        "step": 3515
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.8204708099365234,
        "learning_rate": 7.184681089891588e-05,
        "epoch": 0.4532095901005414,
        "step": 3516
    },
    {
        "loss": 1.9475,
        "grad_norm": 2.3861451148986816,
        "learning_rate": 7.179205162510485e-05,
        "epoch": 0.45333848930136633,
        "step": 3517
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.2010598182678223,
        "learning_rate": 7.173726006473184e-05,
        "epoch": 0.45346738850219126,
        "step": 3518
    },
    {
        "loss": 1.3183,
        "grad_norm": 2.693744659423828,
        "learning_rate": 7.168243629897469e-05,
        "epoch": 0.45359628770301624,
        "step": 3519
    },
    {
        "loss": 1.2039,
        "grad_norm": 2.3419768810272217,
        "learning_rate": 7.162758040905889e-05,
        "epoch": 0.45372518690384117,
        "step": 3520
    },
    {
        "loss": 1.8413,
        "grad_norm": 2.5368709564208984,
        "learning_rate": 7.15726924762576e-05,
        "epoch": 0.45385408610466615,
        "step": 3521
    },
    {
        "loss": 2.2419,
        "grad_norm": 2.2761287689208984,
        "learning_rate": 7.151777258189138e-05,
        "epoch": 0.4539829853054911,
        "step": 3522
    },
    {
        "loss": 1.5448,
        "grad_norm": 2.228273630142212,
        "learning_rate": 7.146282080732821e-05,
        "epoch": 0.45411188450631607,
        "step": 3523
    },
    {
        "loss": 1.895,
        "grad_norm": 1.7412065267562866,
        "learning_rate": 7.140783723398325e-05,
        "epoch": 0.454240783707141,
        "step": 3524
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.2299946546554565,
        "learning_rate": 7.13528219433188e-05,
        "epoch": 0.454369682907966,
        "step": 3525
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.785536289215088,
        "learning_rate": 7.129777501684418e-05,
        "epoch": 0.4544985821087909,
        "step": 3526
    },
    {
        "loss": 1.6126,
        "grad_norm": 2.716204881668091,
        "learning_rate": 7.12426965361155e-05,
        "epoch": 0.4546274813096159,
        "step": 3527
    },
    {
        "loss": 1.845,
        "grad_norm": 3.031944990158081,
        "learning_rate": 7.11875865827357e-05,
        "epoch": 0.4547563805104408,
        "step": 3528
    },
    {
        "loss": 2.0625,
        "grad_norm": 1.62447190284729,
        "learning_rate": 7.113244523835428e-05,
        "epoch": 0.4548852797112658,
        "step": 3529
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.315546751022339,
        "learning_rate": 7.107727258466735e-05,
        "epoch": 0.4550141789120907,
        "step": 3530
    },
    {
        "loss": 1.4206,
        "grad_norm": 2.3440022468566895,
        "learning_rate": 7.102206870341728e-05,
        "epoch": 0.4551430781129157,
        "step": 3531
    },
    {
        "loss": 2.1717,
        "grad_norm": 1.4055891036987305,
        "learning_rate": 7.09668336763928e-05,
        "epoch": 0.45527197731374064,
        "step": 3532
    },
    {
        "loss": 2.3286,
        "grad_norm": 1.4227347373962402,
        "learning_rate": 7.091156758542874e-05,
        "epoch": 0.4554008765145656,
        "step": 3533
    },
    {
        "loss": 2.4526,
        "grad_norm": 1.7682263851165771,
        "learning_rate": 7.085627051240597e-05,
        "epoch": 0.45552977571539055,
        "step": 3534
    },
    {
        "loss": 2.5198,
        "grad_norm": 1.7958366870880127,
        "learning_rate": 7.080094253925126e-05,
        "epoch": 0.45565867491621553,
        "step": 3535
    },
    {
        "loss": 1.9809,
        "grad_norm": 2.2328147888183594,
        "learning_rate": 7.074558374793717e-05,
        "epoch": 0.45578757411704046,
        "step": 3536
    },
    {
        "loss": 2.0227,
        "grad_norm": 1.5552494525909424,
        "learning_rate": 7.069019422048186e-05,
        "epoch": 0.45591647331786544,
        "step": 3537
    },
    {
        "loss": 1.7173,
        "grad_norm": 2.522826671600342,
        "learning_rate": 7.063477403894918e-05,
        "epoch": 0.4560453725186904,
        "step": 3538
    },
    {
        "loss": 1.431,
        "grad_norm": 2.0345804691314697,
        "learning_rate": 7.057932328544819e-05,
        "epoch": 0.45617427171951536,
        "step": 3539
    },
    {
        "loss": 2.0109,
        "grad_norm": 2.347686767578125,
        "learning_rate": 7.052384204213338e-05,
        "epoch": 0.4563031709203403,
        "step": 3540
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.8222211599349976,
        "learning_rate": 7.046833039120444e-05,
        "epoch": 0.45643207012116527,
        "step": 3541
    },
    {
        "loss": 2.0974,
        "grad_norm": 2.8829174041748047,
        "learning_rate": 7.041278841490596e-05,
        "epoch": 0.4565609693219902,
        "step": 3542
    },
    {
        "loss": 2.3196,
        "grad_norm": 1.386949062347412,
        "learning_rate": 7.035721619552756e-05,
        "epoch": 0.4566898685228152,
        "step": 3543
    },
    {
        "loss": 1.8465,
        "grad_norm": 2.119643211364746,
        "learning_rate": 7.030161381540375e-05,
        "epoch": 0.4568187677236401,
        "step": 3544
    },
    {
        "loss": 2.1952,
        "grad_norm": 2.0942089557647705,
        "learning_rate": 7.024598135691353e-05,
        "epoch": 0.4569476669244651,
        "step": 3545
    },
    {
        "loss": 2.3816,
        "grad_norm": 2.1684138774871826,
        "learning_rate": 7.019031890248063e-05,
        "epoch": 0.45707656612529,
        "step": 3546
    },
    {
        "loss": 1.5276,
        "grad_norm": 2.6001572608947754,
        "learning_rate": 7.013462653457317e-05,
        "epoch": 0.457205465326115,
        "step": 3547
    },
    {
        "loss": 2.5695,
        "grad_norm": 2.5104591846466064,
        "learning_rate": 7.007890433570349e-05,
        "epoch": 0.45733436452693993,
        "step": 3548
    },
    {
        "loss": 1.8805,
        "grad_norm": 2.6504526138305664,
        "learning_rate": 7.002315238842831e-05,
        "epoch": 0.4574632637277649,
        "step": 3549
    },
    {
        "loss": 1.4074,
        "grad_norm": 2.9010233879089355,
        "learning_rate": 6.996737077534832e-05,
        "epoch": 0.45759216292858984,
        "step": 3550
    },
    {
        "loss": 2.1907,
        "grad_norm": 2.5404131412506104,
        "learning_rate": 6.991155957910808e-05,
        "epoch": 0.4577210621294148,
        "step": 3551
    },
    {
        "loss": 2.5027,
        "grad_norm": 1.069510579109192,
        "learning_rate": 6.985571888239617e-05,
        "epoch": 0.45784996133023975,
        "step": 3552
    },
    {
        "loss": 1.7138,
        "grad_norm": 3.2233328819274902,
        "learning_rate": 6.979984876794474e-05,
        "epoch": 0.4579788605310647,
        "step": 3553
    },
    {
        "loss": 1.2975,
        "grad_norm": 3.042252540588379,
        "learning_rate": 6.974394931852956e-05,
        "epoch": 0.45810775973188966,
        "step": 3554
    },
    {
        "loss": 1.3327,
        "grad_norm": 3.1044812202453613,
        "learning_rate": 6.968802061696988e-05,
        "epoch": 0.4582366589327146,
        "step": 3555
    },
    {
        "loss": 2.3583,
        "grad_norm": 2.04093337059021,
        "learning_rate": 6.963206274612825e-05,
        "epoch": 0.4583655581335396,
        "step": 3556
    },
    {
        "loss": 2.148,
        "grad_norm": 2.0290651321411133,
        "learning_rate": 6.95760757889105e-05,
        "epoch": 0.4584944573343645,
        "step": 3557
    },
    {
        "loss": 1.5939,
        "grad_norm": 2.827240467071533,
        "learning_rate": 6.952005982826547e-05,
        "epoch": 0.4586233565351895,
        "step": 3558
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.3503320217132568,
        "learning_rate": 6.946401494718505e-05,
        "epoch": 0.4587522557360144,
        "step": 3559
    },
    {
        "loss": 2.02,
        "grad_norm": 2.0182230472564697,
        "learning_rate": 6.940794122870393e-05,
        "epoch": 0.4588811549368394,
        "step": 3560
    },
    {
        "loss": 1.3436,
        "grad_norm": 3.860886573791504,
        "learning_rate": 6.935183875589947e-05,
        "epoch": 0.4590100541376643,
        "step": 3561
    },
    {
        "loss": 2.3885,
        "grad_norm": 2.233813762664795,
        "learning_rate": 6.929570761189186e-05,
        "epoch": 0.4591389533384893,
        "step": 3562
    },
    {
        "loss": 2.1445,
        "grad_norm": 1.2982655763626099,
        "learning_rate": 6.923954787984344e-05,
        "epoch": 0.45926785253931424,
        "step": 3563
    },
    {
        "loss": 1.0928,
        "grad_norm": 3.2447757720947266,
        "learning_rate": 6.918335964295915e-05,
        "epoch": 0.4593967517401392,
        "step": 3564
    },
    {
        "loss": 1.994,
        "grad_norm": 1.953650712966919,
        "learning_rate": 6.912714298448613e-05,
        "epoch": 0.45952565094096415,
        "step": 3565
    },
    {
        "loss": 2.1367,
        "grad_norm": 1.6955631971359253,
        "learning_rate": 6.907089798771349e-05,
        "epoch": 0.45965455014178913,
        "step": 3566
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.4321534633636475,
        "learning_rate": 6.901462473597244e-05,
        "epoch": 0.45978344934261406,
        "step": 3567
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.5036916732788086,
        "learning_rate": 6.895832331263612e-05,
        "epoch": 0.45991234854343904,
        "step": 3568
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.089423418045044,
        "learning_rate": 6.890199380111921e-05,
        "epoch": 0.46004124774426397,
        "step": 3569
    },
    {
        "loss": 2.3212,
        "grad_norm": 1.8037737607955933,
        "learning_rate": 6.884563628487812e-05,
        "epoch": 0.46017014694508895,
        "step": 3570
    },
    {
        "loss": 2.3004,
        "grad_norm": 1.6032073497772217,
        "learning_rate": 6.878925084741086e-05,
        "epoch": 0.4602990461459139,
        "step": 3571
    },
    {
        "loss": 1.7208,
        "grad_norm": 1.8589149713516235,
        "learning_rate": 6.873283757225655e-05,
        "epoch": 0.46042794534673886,
        "step": 3572
    },
    {
        "loss": 1.0804,
        "grad_norm": 2.4923923015594482,
        "learning_rate": 6.867639654299578e-05,
        "epoch": 0.4605568445475638,
        "step": 3573
    },
    {
        "loss": 1.773,
        "grad_norm": 2.545548439025879,
        "learning_rate": 6.861992784325019e-05,
        "epoch": 0.4606857437483888,
        "step": 3574
    },
    {
        "loss": 2.2799,
        "grad_norm": 1.6794960498809814,
        "learning_rate": 6.85634315566823e-05,
        "epoch": 0.4608146429492137,
        "step": 3575
    },
    {
        "loss": 1.9065,
        "grad_norm": 2.92684268951416,
        "learning_rate": 6.850690776699571e-05,
        "epoch": 0.4609435421500387,
        "step": 3576
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.4969292879104614,
        "learning_rate": 6.845035655793466e-05,
        "epoch": 0.4610724413508636,
        "step": 3577
    },
    {
        "loss": 1.3827,
        "grad_norm": 3.22365403175354,
        "learning_rate": 6.839377801328389e-05,
        "epoch": 0.4612013405516886,
        "step": 3578
    },
    {
        "loss": 2.3068,
        "grad_norm": 2.4219894409179688,
        "learning_rate": 6.83371722168689e-05,
        "epoch": 0.4613302397525135,
        "step": 3579
    },
    {
        "loss": 2.4421,
        "grad_norm": 1.5154041051864624,
        "learning_rate": 6.828053925255539e-05,
        "epoch": 0.4614591389533385,
        "step": 3580
    },
    {
        "loss": 1.0668,
        "grad_norm": 3.114346981048584,
        "learning_rate": 6.822387920424934e-05,
        "epoch": 0.46158803815416344,
        "step": 3581
    },
    {
        "loss": 1.7597,
        "grad_norm": 2.0752782821655273,
        "learning_rate": 6.816719215589688e-05,
        "epoch": 0.4617169373549884,
        "step": 3582
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.727346420288086,
        "learning_rate": 6.811047819148413e-05,
        "epoch": 0.46184583655581335,
        "step": 3583
    },
    {
        "loss": 2.2602,
        "grad_norm": 2.529418468475342,
        "learning_rate": 6.805373739503707e-05,
        "epoch": 0.46197473575663833,
        "step": 3584
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.6869179010391235,
        "learning_rate": 6.799696985062149e-05,
        "epoch": 0.46210363495746326,
        "step": 3585
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.8724597692489624,
        "learning_rate": 6.794017564234274e-05,
        "epoch": 0.46223253415828824,
        "step": 3586
    },
    {
        "loss": 1.6651,
        "grad_norm": 2.381591796875,
        "learning_rate": 6.788335485434572e-05,
        "epoch": 0.46236143335911317,
        "step": 3587
    },
    {
        "loss": 1.8952,
        "grad_norm": 1.8807141780853271,
        "learning_rate": 6.782650757081471e-05,
        "epoch": 0.46249033255993816,
        "step": 3588
    },
    {
        "loss": 2.1854,
        "grad_norm": 2.5084052085876465,
        "learning_rate": 6.776963387597322e-05,
        "epoch": 0.4626192317607631,
        "step": 3589
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.2428994178771973,
        "learning_rate": 6.771273385408388e-05,
        "epoch": 0.462748130961588,
        "step": 3590
    },
    {
        "loss": 1.665,
        "grad_norm": 3.670497417449951,
        "learning_rate": 6.76558075894484e-05,
        "epoch": 0.462877030162413,
        "step": 3591
    },
    {
        "loss": 1.6679,
        "grad_norm": 2.952854633331299,
        "learning_rate": 6.759885516640728e-05,
        "epoch": 0.4630059293632379,
        "step": 3592
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.431602954864502,
        "learning_rate": 6.75418766693398e-05,
        "epoch": 0.4631348285640629,
        "step": 3593
    },
    {
        "loss": 1.8582,
        "grad_norm": 2.34008526802063,
        "learning_rate": 6.748487218266393e-05,
        "epoch": 0.46326372776488783,
        "step": 3594
    },
    {
        "loss": 2.0533,
        "grad_norm": 1.9755655527114868,
        "learning_rate": 6.742784179083608e-05,
        "epoch": 0.4633926269657128,
        "step": 3595
    },
    {
        "loss": 2.5185,
        "grad_norm": 1.4650639295578003,
        "learning_rate": 6.737078557835105e-05,
        "epoch": 0.46352152616653775,
        "step": 3596
    },
    {
        "loss": 2.3729,
        "grad_norm": 1.8401570320129395,
        "learning_rate": 6.731370362974193e-05,
        "epoch": 0.46365042536736273,
        "step": 3597
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.7623169422149658,
        "learning_rate": 6.725659602957988e-05,
        "epoch": 0.46377932456818766,
        "step": 3598
    },
    {
        "loss": 1.6167,
        "grad_norm": 2.565614938735962,
        "learning_rate": 6.719946286247414e-05,
        "epoch": 0.46390822376901264,
        "step": 3599
    },
    {
        "loss": 2.0555,
        "grad_norm": 2.1635236740112305,
        "learning_rate": 6.714230421307175e-05,
        "epoch": 0.46403712296983757,
        "step": 3600
    },
    {
        "loss": 1.7682,
        "grad_norm": 2.8344852924346924,
        "learning_rate": 6.708512016605759e-05,
        "epoch": 0.46416602217066255,
        "step": 3601
    },
    {
        "loss": 2.1465,
        "grad_norm": 2.2903623580932617,
        "learning_rate": 6.702791080615408e-05,
        "epoch": 0.4642949213714875,
        "step": 3602
    },
    {
        "loss": 1.834,
        "grad_norm": 1.9246560335159302,
        "learning_rate": 6.697067621812122e-05,
        "epoch": 0.46442382057231246,
        "step": 3603
    },
    {
        "loss": 1.7206,
        "grad_norm": 2.007570505142212,
        "learning_rate": 6.691341648675628e-05,
        "epoch": 0.4645527197731374,
        "step": 3604
    },
    {
        "loss": 1.6487,
        "grad_norm": 3.299143075942993,
        "learning_rate": 6.6856131696894e-05,
        "epoch": 0.4646816189739624,
        "step": 3605
    },
    {
        "loss": 1.0022,
        "grad_norm": 3.321889877319336,
        "learning_rate": 6.679882193340599e-05,
        "epoch": 0.4648105181747873,
        "step": 3606
    },
    {
        "loss": 2.582,
        "grad_norm": 1.7827383279800415,
        "learning_rate": 6.674148728120095e-05,
        "epoch": 0.4649394173756123,
        "step": 3607
    },
    {
        "loss": 1.2638,
        "grad_norm": 2.5927085876464844,
        "learning_rate": 6.668412782522457e-05,
        "epoch": 0.4650683165764372,
        "step": 3608
    },
    {
        "loss": 2.0505,
        "grad_norm": 2.377467393875122,
        "learning_rate": 6.662674365045913e-05,
        "epoch": 0.4651972157772622,
        "step": 3609
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.6180915832519531,
        "learning_rate": 6.656933484192358e-05,
        "epoch": 0.4653261149780871,
        "step": 3610
    },
    {
        "loss": 2.2359,
        "grad_norm": 1.8429043292999268,
        "learning_rate": 6.651190148467345e-05,
        "epoch": 0.4654550141789121,
        "step": 3611
    },
    {
        "loss": 1.8282,
        "grad_norm": 2.3300812244415283,
        "learning_rate": 6.645444366380049e-05,
        "epoch": 0.46558391337973704,
        "step": 3612
    },
    {
        "loss": 2.1044,
        "grad_norm": 1.9247742891311646,
        "learning_rate": 6.639696146443286e-05,
        "epoch": 0.465712812580562,
        "step": 3613
    },
    {
        "loss": 1.6378,
        "grad_norm": 2.276703357696533,
        "learning_rate": 6.633945497173476e-05,
        "epoch": 0.46584171178138695,
        "step": 3614
    },
    {
        "loss": 1.3618,
        "grad_norm": 2.2329213619232178,
        "learning_rate": 6.628192427090628e-05,
        "epoch": 0.46597061098221193,
        "step": 3615
    },
    {
        "loss": 2.1075,
        "grad_norm": 1.4654110670089722,
        "learning_rate": 6.622436944718357e-05,
        "epoch": 0.46609951018303686,
        "step": 3616
    },
    {
        "loss": 1.6374,
        "grad_norm": 1.9452205896377563,
        "learning_rate": 6.616679058583843e-05,
        "epoch": 0.46622840938386184,
        "step": 3617
    },
    {
        "loss": 2.3304,
        "grad_norm": 2.287651777267456,
        "learning_rate": 6.610918777217818e-05,
        "epoch": 0.46635730858468677,
        "step": 3618
    },
    {
        "loss": 1.6766,
        "grad_norm": 2.3359737396240234,
        "learning_rate": 6.605156109154579e-05,
        "epoch": 0.46648620778551175,
        "step": 3619
    },
    {
        "loss": 2.3234,
        "grad_norm": 2.113632917404175,
        "learning_rate": 6.599391062931951e-05,
        "epoch": 0.4666151069863367,
        "step": 3620
    },
    {
        "loss": 1.9861,
        "grad_norm": 2.0470592975616455,
        "learning_rate": 6.593623647091281e-05,
        "epoch": 0.46674400618716166,
        "step": 3621
    },
    {
        "loss": 2.0266,
        "grad_norm": 1.5951972007751465,
        "learning_rate": 6.587853870177433e-05,
        "epoch": 0.4668729053879866,
        "step": 3622
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.1983304023742676,
        "learning_rate": 6.582081740738762e-05,
        "epoch": 0.4670018045888116,
        "step": 3623
    },
    {
        "loss": 1.4391,
        "grad_norm": 2.887343406677246,
        "learning_rate": 6.576307267327109e-05,
        "epoch": 0.4671307037896365,
        "step": 3624
    },
    {
        "loss": 1.9075,
        "grad_norm": 2.785351514816284,
        "learning_rate": 6.570530458497795e-05,
        "epoch": 0.4672596029904615,
        "step": 3625
    },
    {
        "loss": 2.1589,
        "grad_norm": 2.152756690979004,
        "learning_rate": 6.564751322809593e-05,
        "epoch": 0.4673885021912864,
        "step": 3626
    },
    {
        "loss": 2.1571,
        "grad_norm": 1.3110483884811401,
        "learning_rate": 6.55896986882473e-05,
        "epoch": 0.46751740139211134,
        "step": 3627
    },
    {
        "loss": 1.9451,
        "grad_norm": 3.1922402381896973,
        "learning_rate": 6.553186105108858e-05,
        "epoch": 0.4676463005929363,
        "step": 3628
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.801592230796814,
        "learning_rate": 6.54740004023107e-05,
        "epoch": 0.46777519979376125,
        "step": 3629
    },
    {
        "loss": 2.4236,
        "grad_norm": 2.5646073818206787,
        "learning_rate": 6.541611682763844e-05,
        "epoch": 0.46790409899458624,
        "step": 3630
    },
    {
        "loss": 1.4352,
        "grad_norm": 3.091440200805664,
        "learning_rate": 6.535821041283068e-05,
        "epoch": 0.46803299819541117,
        "step": 3631
    },
    {
        "loss": 1.7536,
        "grad_norm": 3.384511709213257,
        "learning_rate": 6.530028124368022e-05,
        "epoch": 0.46816189739623615,
        "step": 3632
    },
    {
        "loss": 1.7104,
        "grad_norm": 2.0001232624053955,
        "learning_rate": 6.524232940601337e-05,
        "epoch": 0.4682907965970611,
        "step": 3633
    },
    {
        "loss": 1.9526,
        "grad_norm": 2.1137115955352783,
        "learning_rate": 6.518435498569013e-05,
        "epoch": 0.46841969579788606,
        "step": 3634
    },
    {
        "loss": 2.0732,
        "grad_norm": 2.3622567653656006,
        "learning_rate": 6.512635806860408e-05,
        "epoch": 0.468548594998711,
        "step": 3635
    },
    {
        "loss": 1.6886,
        "grad_norm": 1.722568392753601,
        "learning_rate": 6.506833874068185e-05,
        "epoch": 0.46867749419953597,
        "step": 3636
    },
    {
        "loss": 1.7856,
        "grad_norm": 2.249825954437256,
        "learning_rate": 6.501029708788355e-05,
        "epoch": 0.4688063934003609,
        "step": 3637
    },
    {
        "loss": 2.0591,
        "grad_norm": 2.517072916030884,
        "learning_rate": 6.495223319620225e-05,
        "epoch": 0.4689352926011859,
        "step": 3638
    },
    {
        "loss": 2.3228,
        "grad_norm": 1.5095593929290771,
        "learning_rate": 6.489414715166389e-05,
        "epoch": 0.4690641918020108,
        "step": 3639
    },
    {
        "loss": 1.8564,
        "grad_norm": 2.3582801818847656,
        "learning_rate": 6.483603904032735e-05,
        "epoch": 0.4691930910028358,
        "step": 3640
    },
    {
        "loss": 1.5585,
        "grad_norm": 3.2870609760284424,
        "learning_rate": 6.477790894828421e-05,
        "epoch": 0.4693219902036607,
        "step": 3641
    },
    {
        "loss": 2.2528,
        "grad_norm": 1.8703595399856567,
        "learning_rate": 6.471975696165849e-05,
        "epoch": 0.4694508894044857,
        "step": 3642
    },
    {
        "loss": 2.2809,
        "grad_norm": 2.2623281478881836,
        "learning_rate": 6.466158316660676e-05,
        "epoch": 0.46957978860531063,
        "step": 3643
    },
    {
        "loss": 2.0036,
        "grad_norm": 1.8148804903030396,
        "learning_rate": 6.46033876493179e-05,
        "epoch": 0.4697086878061356,
        "step": 3644
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.9562934637069702,
        "learning_rate": 6.45451704960129e-05,
        "epoch": 0.46983758700696054,
        "step": 3645
    },
    {
        "loss": 1.7643,
        "grad_norm": 2.457388401031494,
        "learning_rate": 6.448693179294485e-05,
        "epoch": 0.4699664862077855,
        "step": 3646
    },
    {
        "loss": 2.2855,
        "grad_norm": 2.5531721115112305,
        "learning_rate": 6.44286716263988e-05,
        "epoch": 0.47009538540861046,
        "step": 3647
    },
    {
        "loss": 2.4873,
        "grad_norm": 1.4492048025131226,
        "learning_rate": 6.437039008269151e-05,
        "epoch": 0.47022428460943544,
        "step": 3648
    },
    {
        "loss": 1.1804,
        "grad_norm": 3.644150733947754,
        "learning_rate": 6.431208724817153e-05,
        "epoch": 0.47035318381026037,
        "step": 3649
    },
    {
        "loss": 2.1913,
        "grad_norm": 2.035893678665161,
        "learning_rate": 6.425376320921883e-05,
        "epoch": 0.47048208301108535,
        "step": 3650
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.3015615940093994,
        "learning_rate": 6.41954180522449e-05,
        "epoch": 0.4706109822119103,
        "step": 3651
    },
    {
        "loss": 1.8302,
        "grad_norm": 1.9530211687088013,
        "learning_rate": 6.413705186369246e-05,
        "epoch": 0.47073988141273526,
        "step": 3652
    },
    {
        "loss": 1.3243,
        "grad_norm": 2.777482509613037,
        "learning_rate": 6.407866473003538e-05,
        "epoch": 0.4708687806135602,
        "step": 3653
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.865600824356079,
        "learning_rate": 6.402025673777863e-05,
        "epoch": 0.4709976798143852,
        "step": 3654
    },
    {
        "loss": 1.6392,
        "grad_norm": 3.0194449424743652,
        "learning_rate": 6.3961827973458e-05,
        "epoch": 0.4711265790152101,
        "step": 3655
    },
    {
        "loss": 2.3268,
        "grad_norm": 1.6005059480667114,
        "learning_rate": 6.390337852364013e-05,
        "epoch": 0.4712554782160351,
        "step": 3656
    },
    {
        "loss": 1.5078,
        "grad_norm": 3.269451379776001,
        "learning_rate": 6.384490847492225e-05,
        "epoch": 0.47138437741686,
        "step": 3657
    },
    {
        "loss": 2.5264,
        "grad_norm": 1.853520393371582,
        "learning_rate": 6.378641791393211e-05,
        "epoch": 0.471513276617685,
        "step": 3658
    },
    {
        "loss": 2.155,
        "grad_norm": 1.4468342065811157,
        "learning_rate": 6.372790692732792e-05,
        "epoch": 0.4716421758185099,
        "step": 3659
    },
    {
        "loss": 2.442,
        "grad_norm": 1.8368523120880127,
        "learning_rate": 6.366937560179808e-05,
        "epoch": 0.4717710750193349,
        "step": 3660
    },
    {
        "loss": 2.0633,
        "grad_norm": 1.7293556928634644,
        "learning_rate": 6.361082402406114e-05,
        "epoch": 0.47189997422015983,
        "step": 3661
    },
    {
        "loss": 2.4828,
        "grad_norm": 1.527031660079956,
        "learning_rate": 6.355225228086568e-05,
        "epoch": 0.47202887342098476,
        "step": 3662
    },
    {
        "loss": 1.9315,
        "grad_norm": 1.3470118045806885,
        "learning_rate": 6.349366045899011e-05,
        "epoch": 0.47215777262180975,
        "step": 3663
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.840718388557434,
        "learning_rate": 6.343504864524264e-05,
        "epoch": 0.4722866718226347,
        "step": 3664
    },
    {
        "loss": 1.9749,
        "grad_norm": 1.2598658800125122,
        "learning_rate": 6.337641692646106e-05,
        "epoch": 0.47241557102345966,
        "step": 3665
    },
    {
        "loss": 1.8813,
        "grad_norm": 2.128296375274658,
        "learning_rate": 6.331776538951268e-05,
        "epoch": 0.4725444702242846,
        "step": 3666
    },
    {
        "loss": 1.7123,
        "grad_norm": 1.763533353805542,
        "learning_rate": 6.325909412129416e-05,
        "epoch": 0.47267336942510957,
        "step": 3667
    },
    {
        "loss": 1.8899,
        "grad_norm": 2.3891594409942627,
        "learning_rate": 6.320040320873138e-05,
        "epoch": 0.4728022686259345,
        "step": 3668
    },
    {
        "loss": 1.7711,
        "grad_norm": 2.51818585395813,
        "learning_rate": 6.314169273877937e-05,
        "epoch": 0.4729311678267595,
        "step": 3669
    },
    {
        "loss": 1.5386,
        "grad_norm": 3.095392942428589,
        "learning_rate": 6.308296279842205e-05,
        "epoch": 0.4730600670275844,
        "step": 3670
    },
    {
        "loss": 2.171,
        "grad_norm": 2.0714073181152344,
        "learning_rate": 6.302421347467225e-05,
        "epoch": 0.4731889662284094,
        "step": 3671
    },
    {
        "loss": 2.3527,
        "grad_norm": 1.4821031093597412,
        "learning_rate": 6.296544485457159e-05,
        "epoch": 0.4733178654292343,
        "step": 3672
    },
    {
        "loss": 1.6333,
        "grad_norm": 2.632608413696289,
        "learning_rate": 6.29066570251901e-05,
        "epoch": 0.4734467646300593,
        "step": 3673
    },
    {
        "loss": 1.6844,
        "grad_norm": 2.168994188308716,
        "learning_rate": 6.284785007362636e-05,
        "epoch": 0.47357566383088423,
        "step": 3674
    },
    {
        "loss": 1.5749,
        "grad_norm": 1.480074405670166,
        "learning_rate": 6.27890240870074e-05,
        "epoch": 0.4737045630317092,
        "step": 3675
    },
    {
        "loss": 2.0176,
        "grad_norm": 1.5673129558563232,
        "learning_rate": 6.273017915248822e-05,
        "epoch": 0.47383346223253414,
        "step": 3676
    },
    {
        "loss": 1.8569,
        "grad_norm": 2.4758520126342773,
        "learning_rate": 6.267131535725204e-05,
        "epoch": 0.4739623614333591,
        "step": 3677
    },
    {
        "loss": 2.1963,
        "grad_norm": 2.3680648803710938,
        "learning_rate": 6.261243278851008e-05,
        "epoch": 0.47409126063418405,
        "step": 3678
    },
    {
        "loss": 1.7739,
        "grad_norm": 2.464290142059326,
        "learning_rate": 6.255353153350114e-05,
        "epoch": 0.47422015983500904,
        "step": 3679
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.3495430946350098,
        "learning_rate": 6.249461167949198e-05,
        "epoch": 0.47434905903583396,
        "step": 3680
    },
    {
        "loss": 2.2664,
        "grad_norm": 1.7923299074172974,
        "learning_rate": 6.24356733137768e-05,
        "epoch": 0.47447795823665895,
        "step": 3681
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.9842758178710938,
        "learning_rate": 6.237671652367708e-05,
        "epoch": 0.4746068574374839,
        "step": 3682
    },
    {
        "loss": 1.9992,
        "grad_norm": 1.5884932279586792,
        "learning_rate": 6.231774139654188e-05,
        "epoch": 0.47473575663830886,
        "step": 3683
    },
    {
        "loss": 1.0079,
        "grad_norm": 2.9582889080047607,
        "learning_rate": 6.225874801974724e-05,
        "epoch": 0.4748646558391338,
        "step": 3684
    },
    {
        "loss": 2.1197,
        "grad_norm": 2.465211868286133,
        "learning_rate": 6.21997364806962e-05,
        "epoch": 0.47499355503995877,
        "step": 3685
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.763609528541565,
        "learning_rate": 6.21407068668189e-05,
        "epoch": 0.4751224542407837,
        "step": 3686
    },
    {
        "loss": 2.1995,
        "grad_norm": 2.3252828121185303,
        "learning_rate": 6.208165926557208e-05,
        "epoch": 0.4752513534416087,
        "step": 3687
    },
    {
        "loss": 2.0237,
        "grad_norm": 2.3442296981811523,
        "learning_rate": 6.202259376443924e-05,
        "epoch": 0.4753802526424336,
        "step": 3688
    },
    {
        "loss": 1.9268,
        "grad_norm": 3.209329843521118,
        "learning_rate": 6.196351045093033e-05,
        "epoch": 0.4755091518432586,
        "step": 3689
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.7011940479278564,
        "learning_rate": 6.190440941258174e-05,
        "epoch": 0.4756380510440835,
        "step": 3690
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.986580491065979,
        "learning_rate": 6.184529073695609e-05,
        "epoch": 0.4757669502449085,
        "step": 3691
    },
    {
        "loss": 2.2877,
        "grad_norm": 2.405661106109619,
        "learning_rate": 6.178615451164212e-05,
        "epoch": 0.47589584944573343,
        "step": 3692
    },
    {
        "loss": 2.2131,
        "grad_norm": 1.8558690547943115,
        "learning_rate": 6.172700082425463e-05,
        "epoch": 0.4760247486465584,
        "step": 3693
    },
    {
        "loss": 2.0812,
        "grad_norm": 2.043160915374756,
        "learning_rate": 6.166782976243421e-05,
        "epoch": 0.47615364784738334,
        "step": 3694
    },
    {
        "loss": 0.9842,
        "grad_norm": 2.5285825729370117,
        "learning_rate": 6.160864141384724e-05,
        "epoch": 0.4762825470482083,
        "step": 3695
    },
    {
        "loss": 1.7389,
        "grad_norm": 1.3032127618789673,
        "learning_rate": 6.154943586618578e-05,
        "epoch": 0.47641144624903325,
        "step": 3696
    },
    {
        "loss": 2.1188,
        "grad_norm": 1.888841986656189,
        "learning_rate": 6.149021320716721e-05,
        "epoch": 0.47654034544985824,
        "step": 3697
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.5562924146652222,
        "learning_rate": 6.143097352453436e-05,
        "epoch": 0.47666924465068317,
        "step": 3698
    },
    {
        "loss": 1.8177,
        "grad_norm": 1.7831382751464844,
        "learning_rate": 6.137171690605533e-05,
        "epoch": 0.4767981438515081,
        "step": 3699
    },
    {
        "loss": 2.1407,
        "grad_norm": 2.1923038959503174,
        "learning_rate": 6.13124434395232e-05,
        "epoch": 0.4769270430523331,
        "step": 3700
    },
    {
        "loss": 2.1296,
        "grad_norm": 2.773864984512329,
        "learning_rate": 6.125315321275603e-05,
        "epoch": 0.477055942253158,
        "step": 3701
    },
    {
        "loss": 2.2365,
        "grad_norm": 1.205193281173706,
        "learning_rate": 6.119384631359688e-05,
        "epoch": 0.477184841453983,
        "step": 3702
    },
    {
        "loss": 2.0221,
        "grad_norm": 2.219912528991699,
        "learning_rate": 6.113452282991321e-05,
        "epoch": 0.4773137406548079,
        "step": 3703
    },
    {
        "loss": 2.2493,
        "grad_norm": 1.6700254678726196,
        "learning_rate": 6.107518284959733e-05,
        "epoch": 0.4774426398556329,
        "step": 3704
    },
    {
        "loss": 1.0354,
        "grad_norm": 2.8187851905822754,
        "learning_rate": 6.1015826460565895e-05,
        "epoch": 0.47757153905645783,
        "step": 3705
    },
    {
        "loss": 2.1364,
        "grad_norm": 1.3115156888961792,
        "learning_rate": 6.0956453750759734e-05,
        "epoch": 0.4777004382572828,
        "step": 3706
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.2438411712646484,
        "learning_rate": 6.08970648081441e-05,
        "epoch": 0.47782933745810774,
        "step": 3707
    },
    {
        "loss": 1.8758,
        "grad_norm": 1.944216251373291,
        "learning_rate": 6.0837659720708164e-05,
        "epoch": 0.4779582366589327,
        "step": 3708
    },
    {
        "loss": 1.8972,
        "grad_norm": 1.9315658807754517,
        "learning_rate": 6.077823857646492e-05,
        "epoch": 0.47808713585975765,
        "step": 3709
    },
    {
        "loss": 1.5681,
        "grad_norm": 2.542456865310669,
        "learning_rate": 6.071880146345136e-05,
        "epoch": 0.47821603506058263,
        "step": 3710
    },
    {
        "loss": 1.5893,
        "grad_norm": 2.764080762863159,
        "learning_rate": 6.0659348469728e-05,
        "epoch": 0.47834493426140756,
        "step": 3711
    },
    {
        "loss": 2.1823,
        "grad_norm": 2.384336471557617,
        "learning_rate": 6.059987968337892e-05,
        "epoch": 0.47847383346223255,
        "step": 3712
    },
    {
        "loss": 1.8558,
        "grad_norm": 2.3563992977142334,
        "learning_rate": 6.05403951925116e-05,
        "epoch": 0.4786027326630575,
        "step": 3713
    },
    {
        "loss": 1.8665,
        "grad_norm": 2.4329745769500732,
        "learning_rate": 6.0480895085256785e-05,
        "epoch": 0.47873163186388246,
        "step": 3714
    },
    {
        "loss": 2.3274,
        "grad_norm": 1.6215604543685913,
        "learning_rate": 6.042137944976834e-05,
        "epoch": 0.4788605310647074,
        "step": 3715
    },
    {
        "loss": 1.4503,
        "grad_norm": 2.484084129333496,
        "learning_rate": 6.036184837422316e-05,
        "epoch": 0.47898943026553237,
        "step": 3716
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.9330856800079346,
        "learning_rate": 6.0302301946821014e-05,
        "epoch": 0.4791183294663573,
        "step": 3717
    },
    {
        "loss": 1.6033,
        "grad_norm": 2.651442527770996,
        "learning_rate": 6.024274025578439e-05,
        "epoch": 0.4792472286671823,
        "step": 3718
    },
    {
        "loss": 1.9383,
        "grad_norm": 2.3456811904907227,
        "learning_rate": 6.0183163389358436e-05,
        "epoch": 0.4793761278680072,
        "step": 3719
    },
    {
        "loss": 2.23,
        "grad_norm": 1.6637520790100098,
        "learning_rate": 6.012357143581073e-05,
        "epoch": 0.4795050270688322,
        "step": 3720
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.7047761678695679,
        "learning_rate": 6.0063964483431235e-05,
        "epoch": 0.4796339262696571,
        "step": 3721
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.6655631065368652,
        "learning_rate": 6.000434262053214e-05,
        "epoch": 0.4797628254704821,
        "step": 3722
    },
    {
        "loss": 1.9489,
        "grad_norm": 1.6151050329208374,
        "learning_rate": 5.994470593544771e-05,
        "epoch": 0.47989172467130703,
        "step": 3723
    },
    {
        "loss": 2.1592,
        "grad_norm": 1.5925633907318115,
        "learning_rate": 5.988505451653418e-05,
        "epoch": 0.480020623872132,
        "step": 3724
    },
    {
        "loss": 1.2566,
        "grad_norm": 3.500680685043335,
        "learning_rate": 5.98253884521696e-05,
        "epoch": 0.48014952307295694,
        "step": 3725
    },
    {
        "loss": 2.1178,
        "grad_norm": 1.234136700630188,
        "learning_rate": 5.976570783075373e-05,
        "epoch": 0.4802784222737819,
        "step": 3726
    },
    {
        "loss": 2.1122,
        "grad_norm": 1.7044508457183838,
        "learning_rate": 5.9706012740707897e-05,
        "epoch": 0.48040732147460685,
        "step": 3727
    },
    {
        "loss": 2.1191,
        "grad_norm": 1.8528603315353394,
        "learning_rate": 5.9646303270474845e-05,
        "epoch": 0.48053622067543184,
        "step": 3728
    },
    {
        "loss": 2.293,
        "grad_norm": 1.5838011503219604,
        "learning_rate": 5.958657950851868e-05,
        "epoch": 0.48066511987625676,
        "step": 3729
    },
    {
        "loss": 1.54,
        "grad_norm": 2.7173235416412354,
        "learning_rate": 5.952684154332462e-05,
        "epoch": 0.48079401907708175,
        "step": 3730
    },
    {
        "loss": 2.0903,
        "grad_norm": 2.486381769180298,
        "learning_rate": 5.946708946339894e-05,
        "epoch": 0.4809229182779067,
        "step": 3731
    },
    {
        "loss": 1.9942,
        "grad_norm": 1.355014443397522,
        "learning_rate": 5.940732335726886e-05,
        "epoch": 0.48105181747873166,
        "step": 3732
    },
    {
        "loss": 2.6092,
        "grad_norm": 1.7883399724960327,
        "learning_rate": 5.934754331348233e-05,
        "epoch": 0.4811807166795566,
        "step": 3733
    },
    {
        "loss": 1.1119,
        "grad_norm": 2.5020053386688232,
        "learning_rate": 5.9287749420608005e-05,
        "epoch": 0.4813096158803815,
        "step": 3734
    },
    {
        "loss": 1.7767,
        "grad_norm": 2.0774850845336914,
        "learning_rate": 5.922794176723497e-05,
        "epoch": 0.4814385150812065,
        "step": 3735
    },
    {
        "loss": 1.8745,
        "grad_norm": 1.9132832288742065,
        "learning_rate": 5.9168120441972876e-05,
        "epoch": 0.4815674142820314,
        "step": 3736
    },
    {
        "loss": 0.7881,
        "grad_norm": 2.4702670574188232,
        "learning_rate": 5.910828553345141e-05,
        "epoch": 0.4816963134828564,
        "step": 3737
    },
    {
        "loss": 2.1122,
        "grad_norm": 3.02018666267395,
        "learning_rate": 5.9048437130320474e-05,
        "epoch": 0.48182521268368134,
        "step": 3738
    },
    {
        "loss": 1.7297,
        "grad_norm": 1.552362322807312,
        "learning_rate": 5.8988575321250084e-05,
        "epoch": 0.4819541118845063,
        "step": 3739
    },
    {
        "loss": 2.271,
        "grad_norm": 1.5099658966064453,
        "learning_rate": 5.89287001949299e-05,
        "epoch": 0.48208301108533125,
        "step": 3740
    },
    {
        "loss": 1.9143,
        "grad_norm": 1.723917841911316,
        "learning_rate": 5.886881184006944e-05,
        "epoch": 0.48221191028615623,
        "step": 3741
    },
    {
        "loss": 2.158,
        "grad_norm": 1.2044553756713867,
        "learning_rate": 5.8808910345397884e-05,
        "epoch": 0.48234080948698116,
        "step": 3742
    },
    {
        "loss": 2.3546,
        "grad_norm": 2.1965410709381104,
        "learning_rate": 5.8748995799663655e-05,
        "epoch": 0.48246970868780614,
        "step": 3743
    },
    {
        "loss": 1.6749,
        "grad_norm": 2.210608959197998,
        "learning_rate": 5.868906829163477e-05,
        "epoch": 0.48259860788863107,
        "step": 3744
    },
    {
        "loss": 2.0158,
        "grad_norm": 2.830015182495117,
        "learning_rate": 5.8629127910098336e-05,
        "epoch": 0.48272750708945605,
        "step": 3745
    },
    {
        "loss": 1.2844,
        "grad_norm": 2.092710494995117,
        "learning_rate": 5.856917474386044e-05,
        "epoch": 0.482856406290281,
        "step": 3746
    },
    {
        "loss": 1.5208,
        "grad_norm": 3.2830896377563477,
        "learning_rate": 5.850920888174628e-05,
        "epoch": 0.48298530549110597,
        "step": 3747
    },
    {
        "loss": 1.8254,
        "grad_norm": 1.7825175523757935,
        "learning_rate": 5.8449230412599796e-05,
        "epoch": 0.4831142046919309,
        "step": 3748
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.936834692955017,
        "learning_rate": 5.838923942528351e-05,
        "epoch": 0.4832431038927559,
        "step": 3749
    },
    {
        "loss": 1.8285,
        "grad_norm": 1.4568941593170166,
        "learning_rate": 5.832923600867867e-05,
        "epoch": 0.4833720030935808,
        "step": 3750
    },
    {
        "loss": 2.2588,
        "grad_norm": 2.615429162979126,
        "learning_rate": 5.826922025168483e-05,
        "epoch": 0.4835009022944058,
        "step": 3751
    },
    {
        "loss": 1.8715,
        "grad_norm": 1.9600909948349,
        "learning_rate": 5.8209192243219754e-05,
        "epoch": 0.4836298014952307,
        "step": 3752
    },
    {
        "loss": 1.9333,
        "grad_norm": 1.7920148372650146,
        "learning_rate": 5.814915207221957e-05,
        "epoch": 0.4837587006960557,
        "step": 3753
    },
    {
        "loss": 1.7182,
        "grad_norm": 1.8466670513153076,
        "learning_rate": 5.808909982763825e-05,
        "epoch": 0.4838875998968806,
        "step": 3754
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.0264627933502197,
        "learning_rate": 5.802903559844769e-05,
        "epoch": 0.4840164990977056,
        "step": 3755
    },
    {
        "loss": 2.2241,
        "grad_norm": 1.4147614240646362,
        "learning_rate": 5.796895947363758e-05,
        "epoch": 0.48414539829853054,
        "step": 3756
    },
    {
        "loss": 1.9698,
        "grad_norm": 2.147062301635742,
        "learning_rate": 5.79088715422152e-05,
        "epoch": 0.4842742974993555,
        "step": 3757
    },
    {
        "loss": 2.2476,
        "grad_norm": 2.0855109691619873,
        "learning_rate": 5.7848771893205365e-05,
        "epoch": 0.48440319670018045,
        "step": 3758
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.8024773597717285,
        "learning_rate": 5.778866061565019e-05,
        "epoch": 0.48453209590100543,
        "step": 3759
    },
    {
        "loss": 2.3693,
        "grad_norm": 1.5717803239822388,
        "learning_rate": 5.772853779860905e-05,
        "epoch": 0.48466099510183036,
        "step": 3760
    },
    {
        "loss": 2.6242,
        "grad_norm": 1.4176832437515259,
        "learning_rate": 5.7668403531158444e-05,
        "epoch": 0.48478989430265534,
        "step": 3761
    },
    {
        "loss": 1.3007,
        "grad_norm": 2.8137729167938232,
        "learning_rate": 5.760825790239174e-05,
        "epoch": 0.48491879350348027,
        "step": 3762
    },
    {
        "loss": 1.6852,
        "grad_norm": 2.2853972911834717,
        "learning_rate": 5.7548101001419344e-05,
        "epoch": 0.48504769270430526,
        "step": 3763
    },
    {
        "loss": 2.4018,
        "grad_norm": 2.1539647579193115,
        "learning_rate": 5.748793291736807e-05,
        "epoch": 0.4851765919051302,
        "step": 3764
    },
    {
        "loss": 2.0396,
        "grad_norm": 1.5972683429718018,
        "learning_rate": 5.7427753739381515e-05,
        "epoch": 0.48530549110595517,
        "step": 3765
    },
    {
        "loss": 0.8506,
        "grad_norm": 1.978010892868042,
        "learning_rate": 5.736756355661973e-05,
        "epoch": 0.4854343903067801,
        "step": 3766
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.9812045097351074,
        "learning_rate": 5.7307362458258874e-05,
        "epoch": 0.4855632895076051,
        "step": 3767
    },
    {
        "loss": 1.232,
        "grad_norm": 3.179291009902954,
        "learning_rate": 5.7247150533491436e-05,
        "epoch": 0.48569218870843,
        "step": 3768
    },
    {
        "loss": 1.5771,
        "grad_norm": 2.2398810386657715,
        "learning_rate": 5.718692787152596e-05,
        "epoch": 0.485821087909255,
        "step": 3769
    },
    {
        "loss": 1.936,
        "grad_norm": 2.7173545360565186,
        "learning_rate": 5.712669456158677e-05,
        "epoch": 0.4859499871100799,
        "step": 3770
    },
    {
        "loss": 1.7932,
        "grad_norm": 2.223360300064087,
        "learning_rate": 5.706645069291407e-05,
        "epoch": 0.48607888631090485,
        "step": 3771
    },
    {
        "loss": 2.229,
        "grad_norm": 1.2823861837387085,
        "learning_rate": 5.700619635476375e-05,
        "epoch": 0.48620778551172983,
        "step": 3772
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.8743491172790527,
        "learning_rate": 5.6945931636407e-05,
        "epoch": 0.48633668471255476,
        "step": 3773
    },
    {
        "loss": 1.7574,
        "grad_norm": 2.5987472534179688,
        "learning_rate": 5.688565662713062e-05,
        "epoch": 0.48646558391337974,
        "step": 3774
    },
    {
        "loss": 1.8566,
        "grad_norm": 2.459078311920166,
        "learning_rate": 5.682537141623659e-05,
        "epoch": 0.48659448311420467,
        "step": 3775
    },
    {
        "loss": 2.3323,
        "grad_norm": 1.3230608701705933,
        "learning_rate": 5.676507609304187e-05,
        "epoch": 0.48672338231502965,
        "step": 3776
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.216693639755249,
        "learning_rate": 5.670477074687861e-05,
        "epoch": 0.4868522815158546,
        "step": 3777
    },
    {
        "loss": 1.2013,
        "grad_norm": 2.575899839401245,
        "learning_rate": 5.664445546709368e-05,
        "epoch": 0.48698118071667956,
        "step": 3778
    },
    {
        "loss": 2.1469,
        "grad_norm": 1.9375874996185303,
        "learning_rate": 5.6584130343048704e-05,
        "epoch": 0.4871100799175045,
        "step": 3779
    },
    {
        "loss": 1.9856,
        "grad_norm": 1.6968739032745361,
        "learning_rate": 5.6523795464119874e-05,
        "epoch": 0.4872389791183295,
        "step": 3780
    },
    {
        "loss": 1.6738,
        "grad_norm": 2.6335973739624023,
        "learning_rate": 5.646345091969786e-05,
        "epoch": 0.4873678783191544,
        "step": 3781
    },
    {
        "loss": 2.0483,
        "grad_norm": 1.891766905784607,
        "learning_rate": 5.640309679918764e-05,
        "epoch": 0.4874967775199794,
        "step": 3782
    },
    {
        "loss": 2.2177,
        "grad_norm": 1.6006666421890259,
        "learning_rate": 5.6342733192008365e-05,
        "epoch": 0.4876256767208043,
        "step": 3783
    },
    {
        "loss": 1.9727,
        "grad_norm": 2.2755398750305176,
        "learning_rate": 5.628236018759327e-05,
        "epoch": 0.4877545759216293,
        "step": 3784
    },
    {
        "loss": 1.554,
        "grad_norm": 1.6106079816818237,
        "learning_rate": 5.622197787538948e-05,
        "epoch": 0.4878834751224542,
        "step": 3785
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.8247339725494385,
        "learning_rate": 5.616158634485793e-05,
        "epoch": 0.4880123743232792,
        "step": 3786
    },
    {
        "loss": 2.3617,
        "grad_norm": 2.514134168624878,
        "learning_rate": 5.6101185685473234e-05,
        "epoch": 0.48814127352410414,
        "step": 3787
    },
    {
        "loss": 2.4199,
        "grad_norm": 2.3578968048095703,
        "learning_rate": 5.604077598672349e-05,
        "epoch": 0.4882701727249291,
        "step": 3788
    },
    {
        "loss": 1.6234,
        "grad_norm": 2.4744391441345215,
        "learning_rate": 5.59803573381102e-05,
        "epoch": 0.48839907192575405,
        "step": 3789
    },
    {
        "loss": 2.4523,
        "grad_norm": 1.5061455965042114,
        "learning_rate": 5.591992982914816e-05,
        "epoch": 0.48852797112657903,
        "step": 3790
    },
    {
        "loss": 1.8846,
        "grad_norm": 1.968705415725708,
        "learning_rate": 5.585949354936522e-05,
        "epoch": 0.48865687032740396,
        "step": 3791
    },
    {
        "loss": 1.5976,
        "grad_norm": 3.2134199142456055,
        "learning_rate": 5.579904858830229e-05,
        "epoch": 0.48878576952822894,
        "step": 3792
    },
    {
        "loss": 1.587,
        "grad_norm": 2.728097438812256,
        "learning_rate": 5.573859503551315e-05,
        "epoch": 0.48891466872905387,
        "step": 3793
    },
    {
        "loss": 2.5997,
        "grad_norm": 1.6495935916900635,
        "learning_rate": 5.567813298056425e-05,
        "epoch": 0.48904356792987885,
        "step": 3794
    },
    {
        "loss": 2.2778,
        "grad_norm": 3.2118492126464844,
        "learning_rate": 5.561766251303466e-05,
        "epoch": 0.4891724671307038,
        "step": 3795
    },
    {
        "loss": 2.1374,
        "grad_norm": 2.339226007461548,
        "learning_rate": 5.555718372251595e-05,
        "epoch": 0.48930136633152876,
        "step": 3796
    },
    {
        "loss": 1.9473,
        "grad_norm": 1.9955469369888306,
        "learning_rate": 5.5496696698611974e-05,
        "epoch": 0.4894302655323537,
        "step": 3797
    },
    {
        "loss": 1.7828,
        "grad_norm": 2.470900774002075,
        "learning_rate": 5.5436201530938804e-05,
        "epoch": 0.4895591647331787,
        "step": 3798
    },
    {
        "loss": 2.3436,
        "grad_norm": 2.5922603607177734,
        "learning_rate": 5.537569830912459e-05,
        "epoch": 0.4896880639340036,
        "step": 3799
    },
    {
        "loss": 1.0429,
        "grad_norm": 2.649148464202881,
        "learning_rate": 5.531518712280939e-05,
        "epoch": 0.4898169631348286,
        "step": 3800
    },
    {
        "loss": 1.8295,
        "grad_norm": 2.0768871307373047,
        "learning_rate": 5.525466806164506e-05,
        "epoch": 0.4899458623356535,
        "step": 3801
    },
    {
        "loss": 2.1738,
        "grad_norm": 1.4178756475448608,
        "learning_rate": 5.519414121529515e-05,
        "epoch": 0.4900747615364785,
        "step": 3802
    },
    {
        "loss": 2.4358,
        "grad_norm": 1.3643869161605835,
        "learning_rate": 5.513360667343477e-05,
        "epoch": 0.4902036607373034,
        "step": 3803
    },
    {
        "loss": 1.7452,
        "grad_norm": 2.56490159034729,
        "learning_rate": 5.507306452575034e-05,
        "epoch": 0.4903325599381284,
        "step": 3804
    },
    {
        "loss": 2.1033,
        "grad_norm": 1.997726559638977,
        "learning_rate": 5.501251486193957e-05,
        "epoch": 0.49046145913895334,
        "step": 3805
    },
    {
        "loss": 1.5051,
        "grad_norm": 2.241469383239746,
        "learning_rate": 5.495195777171145e-05,
        "epoch": 0.4905903583397783,
        "step": 3806
    },
    {
        "loss": 1.3175,
        "grad_norm": 2.58349871635437,
        "learning_rate": 5.4891393344785766e-05,
        "epoch": 0.49071925754060325,
        "step": 3807
    },
    {
        "loss": 1.2369,
        "grad_norm": 2.4232029914855957,
        "learning_rate": 5.483082167089326e-05,
        "epoch": 0.4908481567414282,
        "step": 3808
    },
    {
        "loss": 0.6876,
        "grad_norm": 2.3626914024353027,
        "learning_rate": 5.477024283977552e-05,
        "epoch": 0.49097705594225316,
        "step": 3809
    },
    {
        "loss": 2.4575,
        "grad_norm": 1.7188857793807983,
        "learning_rate": 5.47096569411845e-05,
        "epoch": 0.4911059551430781,
        "step": 3810
    },
    {
        "loss": 2.0434,
        "grad_norm": 2.063138484954834,
        "learning_rate": 5.464906406488287e-05,
        "epoch": 0.49123485434390307,
        "step": 3811
    },
    {
        "loss": 2.3621,
        "grad_norm": 1.5874860286712646,
        "learning_rate": 5.458846430064352e-05,
        "epoch": 0.491363753544728,
        "step": 3812
    },
    {
        "loss": 2.0169,
        "grad_norm": 1.6586987972259521,
        "learning_rate": 5.452785773824945e-05,
        "epoch": 0.491492652745553,
        "step": 3813
    },
    {
        "loss": 0.913,
        "grad_norm": 3.070434093475342,
        "learning_rate": 5.446724446749395e-05,
        "epoch": 0.4916215519463779,
        "step": 3814
    },
    {
        "loss": 2.149,
        "grad_norm": 2.441267728805542,
        "learning_rate": 5.440662457818013e-05,
        "epoch": 0.4917504511472029,
        "step": 3815
    },
    {
        "loss": 1.7212,
        "grad_norm": 2.439945936203003,
        "learning_rate": 5.434599816012079e-05,
        "epoch": 0.4918793503480278,
        "step": 3816
    },
    {
        "loss": 1.0138,
        "grad_norm": 1.767949104309082,
        "learning_rate": 5.428536530313867e-05,
        "epoch": 0.4920082495488528,
        "step": 3817
    },
    {
        "loss": 1.4265,
        "grad_norm": 3.363384246826172,
        "learning_rate": 5.422472609706582e-05,
        "epoch": 0.49213714874967773,
        "step": 3818
    },
    {
        "loss": 2.4831,
        "grad_norm": 1.7401306629180908,
        "learning_rate": 5.416408063174382e-05,
        "epoch": 0.4922660479505027,
        "step": 3819
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.8429404497146606,
        "learning_rate": 5.4103428997023466e-05,
        "epoch": 0.49239494715132764,
        "step": 3820
    },
    {
        "loss": 1.9481,
        "grad_norm": 2.589733123779297,
        "learning_rate": 5.4042771282764715e-05,
        "epoch": 0.49252384635215263,
        "step": 3821
    },
    {
        "loss": 2.1736,
        "grad_norm": 2.039602279663086,
        "learning_rate": 5.3982107578836524e-05,
        "epoch": 0.49265274555297756,
        "step": 3822
    },
    {
        "loss": 2.5608,
        "grad_norm": 2.171858072280884,
        "learning_rate": 5.3921437975116726e-05,
        "epoch": 0.49278164475380254,
        "step": 3823
    },
    {
        "loss": 1.0732,
        "grad_norm": 2.6176681518554688,
        "learning_rate": 5.386076256149192e-05,
        "epoch": 0.49291054395462747,
        "step": 3824
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.6643927097320557,
        "learning_rate": 5.380008142785726e-05,
        "epoch": 0.49303944315545245,
        "step": 3825
    },
    {
        "loss": 2.5826,
        "grad_norm": 1.985298991203308,
        "learning_rate": 5.373939466411639e-05,
        "epoch": 0.4931683423562774,
        "step": 3826
    },
    {
        "loss": 2.1308,
        "grad_norm": 1.6584012508392334,
        "learning_rate": 5.367870236018141e-05,
        "epoch": 0.49329724155710236,
        "step": 3827
    },
    {
        "loss": 2.01,
        "grad_norm": 2.376816749572754,
        "learning_rate": 5.361800460597244e-05,
        "epoch": 0.4934261407579273,
        "step": 3828
    },
    {
        "loss": 1.2672,
        "grad_norm": 2.660668134689331,
        "learning_rate": 5.355730149141773e-05,
        "epoch": 0.4935550399587523,
        "step": 3829
    },
    {
        "loss": 1.9734,
        "grad_norm": 2.446542263031006,
        "learning_rate": 5.349659310645363e-05,
        "epoch": 0.4936839391595772,
        "step": 3830
    },
    {
        "loss": 1.575,
        "grad_norm": 2.99503493309021,
        "learning_rate": 5.3435879541024084e-05,
        "epoch": 0.4938128383604022,
        "step": 3831
    },
    {
        "loss": 1.9435,
        "grad_norm": 1.8935483694076538,
        "learning_rate": 5.337516088508077e-05,
        "epoch": 0.4939417375612271,
        "step": 3832
    },
    {
        "loss": 2.7054,
        "grad_norm": 1.996502161026001,
        "learning_rate": 5.3314437228583056e-05,
        "epoch": 0.4940706367620521,
        "step": 3833
    },
    {
        "loss": 1.6492,
        "grad_norm": 2.9260926246643066,
        "learning_rate": 5.325370866149747e-05,
        "epoch": 0.494199535962877,
        "step": 3834
    },
    {
        "loss": 1.1154,
        "grad_norm": 2.6305251121520996,
        "learning_rate": 5.319297527379805e-05,
        "epoch": 0.494328435163702,
        "step": 3835
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.199946880340576,
        "learning_rate": 5.313223715546588e-05,
        "epoch": 0.49445733436452693,
        "step": 3836
    },
    {
        "loss": 2.0217,
        "grad_norm": 1.9915106296539307,
        "learning_rate": 5.307149439648892e-05,
        "epoch": 0.4945862335653519,
        "step": 3837
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.7670176029205322,
        "learning_rate": 5.301074708686226e-05,
        "epoch": 0.49471513276617685,
        "step": 3838
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.1435787677764893,
        "learning_rate": 5.2949995316587576e-05,
        "epoch": 0.49484403196700183,
        "step": 3839
    },
    {
        "loss": 2.2664,
        "grad_norm": 3.044259548187256,
        "learning_rate": 5.28892391756731e-05,
        "epoch": 0.49497293116782676,
        "step": 3840
    },
    {
        "loss": 1.1952,
        "grad_norm": 2.8282389640808105,
        "learning_rate": 5.2828478754133725e-05,
        "epoch": 0.49510183036865174,
        "step": 3841
    },
    {
        "loss": 1.8579,
        "grad_norm": 2.781708002090454,
        "learning_rate": 5.2767714141990564e-05,
        "epoch": 0.49523072956947667,
        "step": 3842
    },
    {
        "loss": 2.0282,
        "grad_norm": 1.9883167743682861,
        "learning_rate": 5.270694542927087e-05,
        "epoch": 0.4953596287703016,
        "step": 3843
    },
    {
        "loss": 1.893,
        "grad_norm": 2.1502935886383057,
        "learning_rate": 5.264617270600814e-05,
        "epoch": 0.4954885279711266,
        "step": 3844
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.0766711235046387,
        "learning_rate": 5.258539606224173e-05,
        "epoch": 0.4956174271719515,
        "step": 3845
    },
    {
        "loss": 1.3697,
        "grad_norm": 3.3670198917388916,
        "learning_rate": 5.2524615588016767e-05,
        "epoch": 0.4957463263727765,
        "step": 3846
    },
    {
        "loss": 1.9527,
        "grad_norm": 2.045511484146118,
        "learning_rate": 5.2463831373384106e-05,
        "epoch": 0.4958752255736014,
        "step": 3847
    },
    {
        "loss": 2.0651,
        "grad_norm": 2.9586222171783447,
        "learning_rate": 5.240304350840014e-05,
        "epoch": 0.4960041247744264,
        "step": 3848
    },
    {
        "loss": 1.9402,
        "grad_norm": 2.3147189617156982,
        "learning_rate": 5.234225208312663e-05,
        "epoch": 0.49613302397525133,
        "step": 3849
    },
    {
        "loss": 1.1011,
        "grad_norm": 2.6942315101623535,
        "learning_rate": 5.228145718763068e-05,
        "epoch": 0.4962619231760763,
        "step": 3850
    },
    {
        "loss": 2.1088,
        "grad_norm": 2.198981761932373,
        "learning_rate": 5.222065891198447e-05,
        "epoch": 0.49639082237690124,
        "step": 3851
    },
    {
        "loss": 2.301,
        "grad_norm": 2.6595053672790527,
        "learning_rate": 5.2159857346265196e-05,
        "epoch": 0.4965197215777262,
        "step": 3852
    },
    {
        "loss": 1.4819,
        "grad_norm": 2.713061571121216,
        "learning_rate": 5.2099052580554985e-05,
        "epoch": 0.49664862077855115,
        "step": 3853
    },
    {
        "loss": 1.7958,
        "grad_norm": 3.066721200942993,
        "learning_rate": 5.203824470494064e-05,
        "epoch": 0.49677751997937614,
        "step": 3854
    },
    {
        "loss": 1.9631,
        "grad_norm": 1.5469300746917725,
        "learning_rate": 5.197743380951362e-05,
        "epoch": 0.49690641918020106,
        "step": 3855
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.0080010890960693,
        "learning_rate": 5.191661998436982e-05,
        "epoch": 0.49703531838102605,
        "step": 3856
    },
    {
        "loss": 2.4571,
        "grad_norm": 1.472230076789856,
        "learning_rate": 5.1855803319609486e-05,
        "epoch": 0.497164217581851,
        "step": 3857
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.3958265781402588,
        "learning_rate": 5.17949839053371e-05,
        "epoch": 0.49729311678267596,
        "step": 3858
    },
    {
        "loss": 2.0563,
        "grad_norm": 1.8646374940872192,
        "learning_rate": 5.1734161831661166e-05,
        "epoch": 0.4974220159835009,
        "step": 3859
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.6664350032806396,
        "learning_rate": 5.167333718869418e-05,
        "epoch": 0.49755091518432587,
        "step": 3860
    },
    {
        "loss": 1.1507,
        "grad_norm": 2.6699769496917725,
        "learning_rate": 5.16125100665524e-05,
        "epoch": 0.4976798143851508,
        "step": 3861
    },
    {
        "loss": 2.3614,
        "grad_norm": 1.982349157333374,
        "learning_rate": 5.1551680555355805e-05,
        "epoch": 0.4978087135859758,
        "step": 3862
    },
    {
        "loss": 0.9684,
        "grad_norm": 4.33062219619751,
        "learning_rate": 5.149084874522787e-05,
        "epoch": 0.4979376127868007,
        "step": 3863
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.7432339191436768,
        "learning_rate": 5.143001472629548e-05,
        "epoch": 0.4980665119876257,
        "step": 3864
    },
    {
        "loss": 2.466,
        "grad_norm": 2.3797640800476074,
        "learning_rate": 5.136917858868883e-05,
        "epoch": 0.4981954111884506,
        "step": 3865
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.7145934104919434,
        "learning_rate": 5.1308340422541214e-05,
        "epoch": 0.4983243103892756,
        "step": 3866
    },
    {
        "loss": 2.7333,
        "grad_norm": 1.4252212047576904,
        "learning_rate": 5.124750031798895e-05,
        "epoch": 0.49845320959010053,
        "step": 3867
    },
    {
        "loss": 1.9781,
        "grad_norm": 2.023988962173462,
        "learning_rate": 5.118665836517124e-05,
        "epoch": 0.4985821087909255,
        "step": 3868
    },
    {
        "loss": 2.2389,
        "grad_norm": 1.9895423650741577,
        "learning_rate": 5.112581465422994e-05,
        "epoch": 0.49871100799175044,
        "step": 3869
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.2817018032073975,
        "learning_rate": 5.106496927530972e-05,
        "epoch": 0.4988399071925754,
        "step": 3870
    },
    {
        "loss": 1.9489,
        "grad_norm": 1.9398257732391357,
        "learning_rate": 5.1004122318557445e-05,
        "epoch": 0.49896880639340035,
        "step": 3871
    },
    {
        "loss": 1.7668,
        "grad_norm": 2.722597360610962,
        "learning_rate": 5.0943273874122474e-05,
        "epoch": 0.49909770559422534,
        "step": 3872
    },
    {
        "loss": 2.5367,
        "grad_norm": 1.643086552619934,
        "learning_rate": 5.088242403215646e-05,
        "epoch": 0.49922660479505027,
        "step": 3873
    },
    {
        "loss": 2.0142,
        "grad_norm": 1.3529061079025269,
        "learning_rate": 5.0821572882812884e-05,
        "epoch": 0.49935550399587525,
        "step": 3874
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.8546956777572632,
        "learning_rate": 5.076072051624733e-05,
        "epoch": 0.4994844031967002,
        "step": 3875
    },
    {
        "loss": 1.2356,
        "grad_norm": 2.62429141998291,
        "learning_rate": 5.069986702261722e-05,
        "epoch": 0.49961330239752516,
        "step": 3876
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.6055277585983276,
        "learning_rate": 5.0639012492081474e-05,
        "epoch": 0.4997422015983501,
        "step": 3877
    },
    {
        "loss": 2.2564,
        "grad_norm": 2.7696588039398193,
        "learning_rate": 5.057815701480072e-05,
        "epoch": 0.49987110079917507,
        "step": 3878
    },
    {
        "loss": 1.9843,
        "grad_norm": 2.3813893795013428,
        "learning_rate": 5.051730068093696e-05,
        "epoch": 0.5,
        "step": 3879
    },
    {
        "loss": 1.4165,
        "grad_norm": 1.8171552419662476,
        "learning_rate": 5.04564435806533e-05,
        "epoch": 0.5001288992008249,
        "step": 3880
    },
    {
        "loss": 2.053,
        "grad_norm": 1.5672152042388916,
        "learning_rate": 5.0395585804114196e-05,
        "epoch": 0.5002577984016499,
        "step": 3881
    },
    {
        "loss": 1.8344,
        "grad_norm": 2.2303829193115234,
        "learning_rate": 5.033472744148504e-05,
        "epoch": 0.5003866976024749,
        "step": 3882
    },
    {
        "loss": 1.2865,
        "grad_norm": 2.745391845703125,
        "learning_rate": 5.027386858293196e-05,
        "epoch": 0.5005155968032998,
        "step": 3883
    },
    {
        "loss": 2.6252,
        "grad_norm": 1.3714872598648071,
        "learning_rate": 5.021300931862203e-05,
        "epoch": 0.5006444960041248,
        "step": 3884
    },
    {
        "loss": 2.4313,
        "grad_norm": 1.7324986457824707,
        "learning_rate": 5.0152149738722795e-05,
        "epoch": 0.5007733952049497,
        "step": 3885
    },
    {
        "loss": 1.9752,
        "grad_norm": 1.4650176763534546,
        "learning_rate": 5.009128993340228e-05,
        "epoch": 0.5009022944057747,
        "step": 3886
    },
    {
        "loss": 2.6069,
        "grad_norm": 1.5381730794906616,
        "learning_rate": 5.003042999282887e-05,
        "epoch": 0.5010311936065996,
        "step": 3887
    },
    {
        "loss": 1.2626,
        "grad_norm": 3.4703164100646973,
        "learning_rate": 4.9969570007171165e-05,
        "epoch": 0.5011600928074246,
        "step": 3888
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.3833056688308716,
        "learning_rate": 4.9908710066597734e-05,
        "epoch": 0.5012889920082495,
        "step": 3889
    },
    {
        "loss": 2.4195,
        "grad_norm": 1.232496976852417,
        "learning_rate": 4.9847850261277176e-05,
        "epoch": 0.5014178912090745,
        "step": 3890
    },
    {
        "loss": 1.4257,
        "grad_norm": 2.937164306640625,
        "learning_rate": 4.978699068137801e-05,
        "epoch": 0.5015467904098995,
        "step": 3891
    },
    {
        "loss": 1.7764,
        "grad_norm": 2.178274154663086,
        "learning_rate": 4.972613141706805e-05,
        "epoch": 0.5016756896107244,
        "step": 3892
    },
    {
        "loss": 1.7811,
        "grad_norm": 2.6326184272766113,
        "learning_rate": 4.966527255851497e-05,
        "epoch": 0.5018045888115493,
        "step": 3893
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.251370429992676,
        "learning_rate": 4.960441419588585e-05,
        "epoch": 0.5019334880123744,
        "step": 3894
    },
    {
        "loss": 2.189,
        "grad_norm": 2.2621827125549316,
        "learning_rate": 4.954355641934672e-05,
        "epoch": 0.5020623872131993,
        "step": 3895
    },
    {
        "loss": 2.6135,
        "grad_norm": 1.3952338695526123,
        "learning_rate": 4.948269931906305e-05,
        "epoch": 0.5021912864140242,
        "step": 3896
    },
    {
        "loss": 1.289,
        "grad_norm": 3.2038462162017822,
        "learning_rate": 4.9421842985199244e-05,
        "epoch": 0.5023201856148491,
        "step": 3897
    },
    {
        "loss": 1.5506,
        "grad_norm": 2.762599468231201,
        "learning_rate": 4.936098750791854e-05,
        "epoch": 0.5024490848156742,
        "step": 3898
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.7432241439819336,
        "learning_rate": 4.930013297738279e-05,
        "epoch": 0.5025779840164991,
        "step": 3899
    },
    {
        "loss": 1.8532,
        "grad_norm": 1.4553416967391968,
        "learning_rate": 4.923927948375265e-05,
        "epoch": 0.502706883217324,
        "step": 3900
    },
    {
        "loss": 2.127,
        "grad_norm": 2.1354751586914062,
        "learning_rate": 4.917842711718716e-05,
        "epoch": 0.502835782418149,
        "step": 3901
    },
    {
        "loss": 2.4493,
        "grad_norm": 2.309617280960083,
        "learning_rate": 4.9117575967843546e-05,
        "epoch": 0.502964681618974,
        "step": 3902
    },
    {
        "loss": 1.5529,
        "grad_norm": 3.3100674152374268,
        "learning_rate": 4.90567261258775e-05,
        "epoch": 0.5030935808197989,
        "step": 3903
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.3808863162994385,
        "learning_rate": 4.89958776814426e-05,
        "epoch": 0.5032224800206239,
        "step": 3904
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.1267528533935547,
        "learning_rate": 4.8935030724690294e-05,
        "epoch": 0.5033513792214488,
        "step": 3905
    },
    {
        "loss": 1.8469,
        "grad_norm": 1.6748976707458496,
        "learning_rate": 4.8874185345770036e-05,
        "epoch": 0.5034802784222738,
        "step": 3906
    },
    {
        "loss": 2.1032,
        "grad_norm": 2.5762734413146973,
        "learning_rate": 4.881334163482882e-05,
        "epoch": 0.5036091776230988,
        "step": 3907
    },
    {
        "loss": 1.9787,
        "grad_norm": 1.5435434579849243,
        "learning_rate": 4.8752499682011056e-05,
        "epoch": 0.5037380768239237,
        "step": 3908
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.0599377155303955,
        "learning_rate": 4.86916595774588e-05,
        "epoch": 0.5038669760247486,
        "step": 3909
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.1169748306274414,
        "learning_rate": 4.863082141131121e-05,
        "epoch": 0.5039958752255737,
        "step": 3910
    },
    {
        "loss": 1.514,
        "grad_norm": 2.029280424118042,
        "learning_rate": 4.856998527370453e-05,
        "epoch": 0.5041247744263986,
        "step": 3911
    },
    {
        "loss": 2.3071,
        "grad_norm": 1.7985248565673828,
        "learning_rate": 4.850915125477215e-05,
        "epoch": 0.5042536736272235,
        "step": 3912
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.428145408630371,
        "learning_rate": 4.8448319444644206e-05,
        "epoch": 0.5043825728280484,
        "step": 3913
    },
    {
        "loss": 1.7763,
        "grad_norm": 3.9776129722595215,
        "learning_rate": 4.838748993344761e-05,
        "epoch": 0.5045114720288735,
        "step": 3914
    },
    {
        "loss": 1.7484,
        "grad_norm": 1.537828803062439,
        "learning_rate": 4.832666281130583e-05,
        "epoch": 0.5046403712296984,
        "step": 3915
    },
    {
        "loss": 2.0481,
        "grad_norm": 2.2090351581573486,
        "learning_rate": 4.8265838168338845e-05,
        "epoch": 0.5047692704305233,
        "step": 3916
    },
    {
        "loss": 2.2253,
        "grad_norm": 1.4970768690109253,
        "learning_rate": 4.820501609466291e-05,
        "epoch": 0.5048981696313483,
        "step": 3917
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.0691778659820557,
        "learning_rate": 4.8144196680390526e-05,
        "epoch": 0.5050270688321732,
        "step": 3918
    },
    {
        "loss": 2.4324,
        "grad_norm": 1.7422760725021362,
        "learning_rate": 4.808338001563019e-05,
        "epoch": 0.5051559680329982,
        "step": 3919
    },
    {
        "loss": 1.1716,
        "grad_norm": 2.5432395935058594,
        "learning_rate": 4.802256619048639e-05,
        "epoch": 0.5052848672338232,
        "step": 3920
    },
    {
        "loss": 2.107,
        "grad_norm": 1.3055109977722168,
        "learning_rate": 4.796175529505937e-05,
        "epoch": 0.5054137664346481,
        "step": 3921
    },
    {
        "loss": 1.1907,
        "grad_norm": 3.4547183513641357,
        "learning_rate": 4.7900947419445026e-05,
        "epoch": 0.505542665635473,
        "step": 3922
    },
    {
        "loss": 2.0302,
        "grad_norm": 2.3797898292541504,
        "learning_rate": 4.7840142653734816e-05,
        "epoch": 0.505671564836298,
        "step": 3923
    },
    {
        "loss": 2.545,
        "grad_norm": 1.3207393884658813,
        "learning_rate": 4.777934108801555e-05,
        "epoch": 0.505800464037123,
        "step": 3924
    },
    {
        "loss": 1.1836,
        "grad_norm": 2.8704545497894287,
        "learning_rate": 4.771854281236934e-05,
        "epoch": 0.5059293632379479,
        "step": 3925
    },
    {
        "loss": 2.1502,
        "grad_norm": 2.119367837905884,
        "learning_rate": 4.765774791687338e-05,
        "epoch": 0.5060582624387728,
        "step": 3926
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.4614787101745605,
        "learning_rate": 4.759695649159987e-05,
        "epoch": 0.5061871616395979,
        "step": 3927
    },
    {
        "loss": 1.3862,
        "grad_norm": 2.78127384185791,
        "learning_rate": 4.75361686266159e-05,
        "epoch": 0.5063160608404228,
        "step": 3928
    },
    {
        "loss": 2.2236,
        "grad_norm": 1.826474666595459,
        "learning_rate": 4.7475384411983204e-05,
        "epoch": 0.5064449600412477,
        "step": 3929
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.0248026847839355,
        "learning_rate": 4.741460393775829e-05,
        "epoch": 0.5065738592420727,
        "step": 3930
    },
    {
        "loss": 2.3475,
        "grad_norm": 1.1453865766525269,
        "learning_rate": 4.735382729399186e-05,
        "epoch": 0.5067027584428977,
        "step": 3931
    },
    {
        "loss": 1.5972,
        "grad_norm": 2.1519615650177,
        "learning_rate": 4.72930545707291e-05,
        "epoch": 0.5068316576437226,
        "step": 3932
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.1956939697265625,
        "learning_rate": 4.7232285858009475e-05,
        "epoch": 0.5069605568445475,
        "step": 3933
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.4166145324707031,
        "learning_rate": 4.7171521245866286e-05,
        "epoch": 0.5070894560453725,
        "step": 3934
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.763190746307373,
        "learning_rate": 4.7110760824326874e-05,
        "epoch": 0.5072183552461975,
        "step": 3935
    },
    {
        "loss": 1.6989,
        "grad_norm": 3.4333584308624268,
        "learning_rate": 4.7050004683412477e-05,
        "epoch": 0.5073472544470224,
        "step": 3936
    },
    {
        "loss": 2.0854,
        "grad_norm": 1.5271388292312622,
        "learning_rate": 4.698925291313776e-05,
        "epoch": 0.5074761536478474,
        "step": 3937
    },
    {
        "loss": 1.8903,
        "grad_norm": 1.9805409908294678,
        "learning_rate": 4.6928505603511066e-05,
        "epoch": 0.5076050528486723,
        "step": 3938
    },
    {
        "loss": 1.814,
        "grad_norm": 2.5263943672180176,
        "learning_rate": 4.686776284453417e-05,
        "epoch": 0.5077339520494973,
        "step": 3939
    },
    {
        "loss": 2.0949,
        "grad_norm": 2.444737195968628,
        "learning_rate": 4.6807024726201956e-05,
        "epoch": 0.5078628512503223,
        "step": 3940
    },
    {
        "loss": 2.0277,
        "grad_norm": 1.9198322296142578,
        "learning_rate": 4.674629133850249e-05,
        "epoch": 0.5079917504511472,
        "step": 3941
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.4818480014801025,
        "learning_rate": 4.668556277141698e-05,
        "epoch": 0.5081206496519721,
        "step": 3942
    },
    {
        "loss": 1.4831,
        "grad_norm": 2.321532726287842,
        "learning_rate": 4.662483911491925e-05,
        "epoch": 0.5082495488527972,
        "step": 3943
    },
    {
        "loss": 1.138,
        "grad_norm": 3.6485772132873535,
        "learning_rate": 4.6564120458975934e-05,
        "epoch": 0.5083784480536221,
        "step": 3944
    },
    {
        "loss": 1.5907,
        "grad_norm": 1.7270492315292358,
        "learning_rate": 4.6503406893546416e-05,
        "epoch": 0.508507347254447,
        "step": 3945
    },
    {
        "loss": 2.3022,
        "grad_norm": 2.228947877883911,
        "learning_rate": 4.6442698508582274e-05,
        "epoch": 0.508636246455272,
        "step": 3946
    },
    {
        "loss": 2.1044,
        "grad_norm": 1.5104243755340576,
        "learning_rate": 4.6381995394027574e-05,
        "epoch": 0.508765145656097,
        "step": 3947
    },
    {
        "loss": 1.9713,
        "grad_norm": 2.4348385334014893,
        "learning_rate": 4.632129763981855e-05,
        "epoch": 0.5088940448569219,
        "step": 3948
    },
    {
        "loss": 2.4942,
        "grad_norm": 2.689397096633911,
        "learning_rate": 4.626060533588361e-05,
        "epoch": 0.5090229440577468,
        "step": 3949
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.9389240741729736,
        "learning_rate": 4.619991857214275e-05,
        "epoch": 0.5091518432585718,
        "step": 3950
    },
    {
        "loss": 2.1431,
        "grad_norm": 1.8334132432937622,
        "learning_rate": 4.613923743850806e-05,
        "epoch": 0.5092807424593968,
        "step": 3951
    },
    {
        "loss": 1.8839,
        "grad_norm": 2.2121403217315674,
        "learning_rate": 4.607856202488331e-05,
        "epoch": 0.5094096416602217,
        "step": 3952
    },
    {
        "loss": 2.0998,
        "grad_norm": 1.8114945888519287,
        "learning_rate": 4.601789242116348e-05,
        "epoch": 0.5095385408610467,
        "step": 3953
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.4532098770141602,
        "learning_rate": 4.595722871723527e-05,
        "epoch": 0.5096674400618716,
        "step": 3954
    },
    {
        "loss": 2.1312,
        "grad_norm": 1.7446871995925903,
        "learning_rate": 4.5896571002976587e-05,
        "epoch": 0.5097963392626965,
        "step": 3955
    },
    {
        "loss": 1.5988,
        "grad_norm": 2.254763126373291,
        "learning_rate": 4.5835919368256184e-05,
        "epoch": 0.5099252384635216,
        "step": 3956
    },
    {
        "loss": 1.5756,
        "grad_norm": 2.4623329639434814,
        "learning_rate": 4.5775273902934155e-05,
        "epoch": 0.5100541376643465,
        "step": 3957
    },
    {
        "loss": 1.6348,
        "grad_norm": 2.5911622047424316,
        "learning_rate": 4.571463469686139e-05,
        "epoch": 0.5101830368651714,
        "step": 3958
    },
    {
        "loss": 1.9396,
        "grad_norm": 2.1857802867889404,
        "learning_rate": 4.5654001839879215e-05,
        "epoch": 0.5103119360659963,
        "step": 3959
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.9476542472839355,
        "learning_rate": 4.559337542181988e-05,
        "epoch": 0.5104408352668214,
        "step": 3960
    },
    {
        "loss": 1.3698,
        "grad_norm": 3.1206719875335693,
        "learning_rate": 4.55327555325061e-05,
        "epoch": 0.5105697344676463,
        "step": 3961
    },
    {
        "loss": 2.1887,
        "grad_norm": 2.5374867916107178,
        "learning_rate": 4.5472142261750565e-05,
        "epoch": 0.5106986336684712,
        "step": 3962
    },
    {
        "loss": 1.8797,
        "grad_norm": 1.4675781726837158,
        "learning_rate": 4.541153569935649e-05,
        "epoch": 0.5108275328692962,
        "step": 3963
    },
    {
        "loss": 2.5395,
        "grad_norm": 2.396897792816162,
        "learning_rate": 4.535093593511711e-05,
        "epoch": 0.5109564320701212,
        "step": 3964
    },
    {
        "loss": 1.8334,
        "grad_norm": 2.1744234561920166,
        "learning_rate": 4.529034305881551e-05,
        "epoch": 0.5110853312709461,
        "step": 3965
    },
    {
        "loss": 2.3593,
        "grad_norm": 1.263821005821228,
        "learning_rate": 4.522975716022448e-05,
        "epoch": 0.5112142304717711,
        "step": 3966
    },
    {
        "loss": 1.7807,
        "grad_norm": 2.1126906871795654,
        "learning_rate": 4.516917832910672e-05,
        "epoch": 0.511343129672596,
        "step": 3967
    },
    {
        "loss": 2.0346,
        "grad_norm": 2.272251605987549,
        "learning_rate": 4.510860665521427e-05,
        "epoch": 0.511472028873421,
        "step": 3968
    },
    {
        "loss": 2.0099,
        "grad_norm": 1.4144421815872192,
        "learning_rate": 4.5048042228288554e-05,
        "epoch": 0.511600928074246,
        "step": 3969
    },
    {
        "loss": 2.398,
        "grad_norm": 1.6046977043151855,
        "learning_rate": 4.49874851380604e-05,
        "epoch": 0.5117298272750709,
        "step": 3970
    },
    {
        "loss": 2.2596,
        "grad_norm": 1.9723694324493408,
        "learning_rate": 4.492693547424971e-05,
        "epoch": 0.5118587264758958,
        "step": 3971
    },
    {
        "loss": 2.1314,
        "grad_norm": 1.8546123504638672,
        "learning_rate": 4.486639332656524e-05,
        "epoch": 0.5119876256767208,
        "step": 3972
    },
    {
        "loss": 2.0481,
        "grad_norm": 2.043476104736328,
        "learning_rate": 4.480585878470484e-05,
        "epoch": 0.5121165248775458,
        "step": 3973
    },
    {
        "loss": 1.1107,
        "grad_norm": 3.032541036605835,
        "learning_rate": 4.4745331938354976e-05,
        "epoch": 0.5122454240783707,
        "step": 3974
    },
    {
        "loss": 1.9659,
        "grad_norm": 1.41122567653656,
        "learning_rate": 4.4684812877190616e-05,
        "epoch": 0.5123743232791956,
        "step": 3975
    },
    {
        "loss": 1.9515,
        "grad_norm": 1.6733572483062744,
        "learning_rate": 4.462430169087542e-05,
        "epoch": 0.5125032224800207,
        "step": 3976
    },
    {
        "loss": 2.1439,
        "grad_norm": 2.12092661857605,
        "learning_rate": 4.456379846906124e-05,
        "epoch": 0.5126321216808456,
        "step": 3977
    },
    {
        "loss": 1.5193,
        "grad_norm": 1.85869562625885,
        "learning_rate": 4.450330330138804e-05,
        "epoch": 0.5127610208816705,
        "step": 3978
    },
    {
        "loss": 1.5217,
        "grad_norm": 2.0233471393585205,
        "learning_rate": 4.444281627748407e-05,
        "epoch": 0.5128899200824955,
        "step": 3979
    },
    {
        "loss": 1.7889,
        "grad_norm": 2.5308167934417725,
        "learning_rate": 4.4382337486965345e-05,
        "epoch": 0.5130188192833205,
        "step": 3980
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.6708709001541138,
        "learning_rate": 4.432186701943577e-05,
        "epoch": 0.5131477184841454,
        "step": 3981
    },
    {
        "loss": 2.1241,
        "grad_norm": 2.0532288551330566,
        "learning_rate": 4.426140496448686e-05,
        "epoch": 0.5132766176849703,
        "step": 3982
    },
    {
        "loss": 2.2585,
        "grad_norm": 1.3990403413772583,
        "learning_rate": 4.420095141169771e-05,
        "epoch": 0.5134055168857953,
        "step": 3983
    },
    {
        "loss": 1.7798,
        "grad_norm": 2.1029927730560303,
        "learning_rate": 4.41405064506348e-05,
        "epoch": 0.5135344160866203,
        "step": 3984
    },
    {
        "loss": 2.5147,
        "grad_norm": 3.1126749515533447,
        "learning_rate": 4.408007017085186e-05,
        "epoch": 0.5136633152874452,
        "step": 3985
    },
    {
        "loss": 2.8606,
        "grad_norm": 4.983621120452881,
        "learning_rate": 4.401964266188981e-05,
        "epoch": 0.5137922144882702,
        "step": 3986
    },
    {
        "loss": 2.39,
        "grad_norm": 1.813942313194275,
        "learning_rate": 4.395922401327651e-05,
        "epoch": 0.5139211136890951,
        "step": 3987
    },
    {
        "loss": 2.1447,
        "grad_norm": 2.3952581882476807,
        "learning_rate": 4.389881431452677e-05,
        "epoch": 0.51405001288992,
        "step": 3988
    },
    {
        "loss": 2.1315,
        "grad_norm": 2.067922353744507,
        "learning_rate": 4.383841365514208e-05,
        "epoch": 0.5141789120907451,
        "step": 3989
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.6900196075439453,
        "learning_rate": 4.377802212461053e-05,
        "epoch": 0.51430781129157,
        "step": 3990
    },
    {
        "loss": 2.3662,
        "grad_norm": 1.6818360090255737,
        "learning_rate": 4.3717639812406745e-05,
        "epoch": 0.5144367104923949,
        "step": 3991
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.2213845252990723,
        "learning_rate": 4.365726680799164e-05,
        "epoch": 0.5145656096932198,
        "step": 3992
    },
    {
        "loss": 2.5744,
        "grad_norm": 1.6383390426635742,
        "learning_rate": 4.359690320081237e-05,
        "epoch": 0.5146945088940449,
        "step": 3993
    },
    {
        "loss": 1.7036,
        "grad_norm": 3.469677209854126,
        "learning_rate": 4.3536549080302145e-05,
        "epoch": 0.5148234080948698,
        "step": 3994
    },
    {
        "loss": 1.2163,
        "grad_norm": 3.026628255844116,
        "learning_rate": 4.347620453588013e-05,
        "epoch": 0.5149523072956947,
        "step": 3995
    },
    {
        "loss": 2.236,
        "grad_norm": 1.686847448348999,
        "learning_rate": 4.3415869656951274e-05,
        "epoch": 0.5150812064965197,
        "step": 3996
    },
    {
        "loss": 2.0181,
        "grad_norm": 1.6659729480743408,
        "learning_rate": 4.335554453290632e-05,
        "epoch": 0.5152101056973447,
        "step": 3997
    },
    {
        "loss": 1.4572,
        "grad_norm": 2.4827089309692383,
        "learning_rate": 4.32952292531214e-05,
        "epoch": 0.5153390048981696,
        "step": 3998
    },
    {
        "loss": 2.2245,
        "grad_norm": 1.5347670316696167,
        "learning_rate": 4.32349239069581e-05,
        "epoch": 0.5154679040989946,
        "step": 3999
    },
    {
        "loss": 2.5083,
        "grad_norm": 1.6716374158859253,
        "learning_rate": 4.3174628583763454e-05,
        "epoch": 0.5155968032998195,
        "step": 4000
    },
    {
        "loss": 2.0282,
        "grad_norm": 1.7699819803237915,
        "learning_rate": 4.311434337286939e-05,
        "epoch": 0.5157257025006445,
        "step": 4001
    },
    {
        "loss": 2.2384,
        "grad_norm": 2.0181684494018555,
        "learning_rate": 4.305406836359298e-05,
        "epoch": 0.5158546017014695,
        "step": 4002
    },
    {
        "loss": 2.4332,
        "grad_norm": 1.3701320886611938,
        "learning_rate": 4.2993803645236295e-05,
        "epoch": 0.5159835009022944,
        "step": 4003
    },
    {
        "loss": 2.4927,
        "grad_norm": 1.4846112728118896,
        "learning_rate": 4.293354930708593e-05,
        "epoch": 0.5161124001031193,
        "step": 4004
    },
    {
        "loss": 1.832,
        "grad_norm": 2.5388853549957275,
        "learning_rate": 4.287330543841322e-05,
        "epoch": 0.5162412993039444,
        "step": 4005
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.714346170425415,
        "learning_rate": 4.2813072128474077e-05,
        "epoch": 0.5163701985047693,
        "step": 4006
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.6261370182037354,
        "learning_rate": 4.2752849466508575e-05,
        "epoch": 0.5164990977055942,
        "step": 4007
    },
    {
        "loss": 2.0963,
        "grad_norm": 2.1273534297943115,
        "learning_rate": 4.269263754174113e-05,
        "epoch": 0.5166279969064191,
        "step": 4008
    },
    {
        "loss": 1.9393,
        "grad_norm": 2.734362840652466,
        "learning_rate": 4.263243644338031e-05,
        "epoch": 0.5167568961072442,
        "step": 4009
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.1502081155776978,
        "learning_rate": 4.257224626061849e-05,
        "epoch": 0.5168857953080691,
        "step": 4010
    },
    {
        "loss": 1.919,
        "grad_norm": 2.539888620376587,
        "learning_rate": 4.251206708263193e-05,
        "epoch": 0.517014694508894,
        "step": 4011
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.674675464630127,
        "learning_rate": 4.245189899858064e-05,
        "epoch": 0.517143593709719,
        "step": 4012
    },
    {
        "loss": 1.8628,
        "grad_norm": 1.5764403343200684,
        "learning_rate": 4.2391742097608266e-05,
        "epoch": 0.517272492910544,
        "step": 4013
    },
    {
        "loss": 1.9261,
        "grad_norm": 2.2833588123321533,
        "learning_rate": 4.233159646884157e-05,
        "epoch": 0.5174013921113689,
        "step": 4014
    },
    {
        "loss": 1.5911,
        "grad_norm": 2.3079166412353516,
        "learning_rate": 4.227146220139092e-05,
        "epoch": 0.5175302913121939,
        "step": 4015
    },
    {
        "loss": 2.398,
        "grad_norm": 1.5023102760314941,
        "learning_rate": 4.221133938434986e-05,
        "epoch": 0.5176591905130188,
        "step": 4016
    },
    {
        "loss": 1.4417,
        "grad_norm": 3.5053293704986572,
        "learning_rate": 4.2151228106794646e-05,
        "epoch": 0.5177880897138438,
        "step": 4017
    },
    {
        "loss": 1.9532,
        "grad_norm": 2.386090040206909,
        "learning_rate": 4.2091128457784775e-05,
        "epoch": 0.5179169889146688,
        "step": 4018
    },
    {
        "loss": 2.1225,
        "grad_norm": 1.9216052293777466,
        "learning_rate": 4.203104052636246e-05,
        "epoch": 0.5180458881154937,
        "step": 4019
    },
    {
        "loss": 2.1689,
        "grad_norm": 2.2939085960388184,
        "learning_rate": 4.197096440155232e-05,
        "epoch": 0.5181747873163186,
        "step": 4020
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.913262963294983,
        "learning_rate": 4.191090017236173e-05,
        "epoch": 0.5183036865171436,
        "step": 4021
    },
    {
        "loss": 1.8726,
        "grad_norm": 1.8593823909759521,
        "learning_rate": 4.1850847927780474e-05,
        "epoch": 0.5184325857179686,
        "step": 4022
    },
    {
        "loss": 2.2332,
        "grad_norm": 2.163804769515991,
        "learning_rate": 4.179080775678025e-05,
        "epoch": 0.5185614849187935,
        "step": 4023
    },
    {
        "loss": 2.1623,
        "grad_norm": 1.9709213972091675,
        "learning_rate": 4.1730779748315173e-05,
        "epoch": 0.5186903841196184,
        "step": 4024
    },
    {
        "loss": 1.6452,
        "grad_norm": 2.4999639987945557,
        "learning_rate": 4.1670763991321384e-05,
        "epoch": 0.5188192833204434,
        "step": 4025
    },
    {
        "loss": 1.1447,
        "grad_norm": 2.4135568141937256,
        "learning_rate": 4.16107605747165e-05,
        "epoch": 0.5189481825212684,
        "step": 4026
    },
    {
        "loss": 2.4322,
        "grad_norm": 1.5772817134857178,
        "learning_rate": 4.155076958740021e-05,
        "epoch": 0.5190770817220933,
        "step": 4027
    },
    {
        "loss": 1.3098,
        "grad_norm": 2.6014556884765625,
        "learning_rate": 4.149079111825377e-05,
        "epoch": 0.5192059809229183,
        "step": 4028
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.069000244140625,
        "learning_rate": 4.143082525613957e-05,
        "epoch": 0.5193348801237432,
        "step": 4029
    },
    {
        "loss": 1.0187,
        "grad_norm": 2.4177463054656982,
        "learning_rate": 4.137087208990167e-05,
        "epoch": 0.5194637793245682,
        "step": 4030
    },
    {
        "loss": 1.5402,
        "grad_norm": 3.2975404262542725,
        "learning_rate": 4.13109317083652e-05,
        "epoch": 0.5195926785253931,
        "step": 4031
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.7453616857528687,
        "learning_rate": 4.1251004200336356e-05,
        "epoch": 0.5197215777262181,
        "step": 4032
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.5771681070327759,
        "learning_rate": 4.1191089654602135e-05,
        "epoch": 0.519850476927043,
        "step": 4033
    },
    {
        "loss": 1.6574,
        "grad_norm": 2.058643341064453,
        "learning_rate": 4.1131188159930545e-05,
        "epoch": 0.519979376127868,
        "step": 4034
    },
    {
        "loss": 1.4372,
        "grad_norm": 2.4650940895080566,
        "learning_rate": 4.107129980507014e-05,
        "epoch": 0.520108275328693,
        "step": 4035
    },
    {
        "loss": 1.5762,
        "grad_norm": 2.8210296630859375,
        "learning_rate": 4.101142467874992e-05,
        "epoch": 0.5202371745295179,
        "step": 4036
    },
    {
        "loss": 1.713,
        "grad_norm": 1.2996654510498047,
        "learning_rate": 4.0951562869679503e-05,
        "epoch": 0.5203660737303428,
        "step": 4037
    },
    {
        "loss": 2.0481,
        "grad_norm": 2.096585512161255,
        "learning_rate": 4.089171446654863e-05,
        "epoch": 0.5204949729311679,
        "step": 4038
    },
    {
        "loss": 1.9451,
        "grad_norm": 1.8222811222076416,
        "learning_rate": 4.083187955802713e-05,
        "epoch": 0.5206238721319928,
        "step": 4039
    },
    {
        "loss": 2.4634,
        "grad_norm": 1.2014859914779663,
        "learning_rate": 4.077205823276501e-05,
        "epoch": 0.5207527713328177,
        "step": 4040
    },
    {
        "loss": 2.2584,
        "grad_norm": 1.4445726871490479,
        "learning_rate": 4.071225057939204e-05,
        "epoch": 0.5208816705336426,
        "step": 4041
    },
    {
        "loss": 2.025,
        "grad_norm": 2.1110737323760986,
        "learning_rate": 4.065245668651768e-05,
        "epoch": 0.5210105697344677,
        "step": 4042
    },
    {
        "loss": 2.1675,
        "grad_norm": 3.6567180156707764,
        "learning_rate": 4.059267664273115e-05,
        "epoch": 0.5211394689352926,
        "step": 4043
    },
    {
        "loss": 1.6367,
        "grad_norm": 2.0992250442504883,
        "learning_rate": 4.05329105366011e-05,
        "epoch": 0.5212683681361175,
        "step": 4044
    },
    {
        "loss": 1.6135,
        "grad_norm": 2.6965601444244385,
        "learning_rate": 4.047315845667538e-05,
        "epoch": 0.5213972673369425,
        "step": 4045
    },
    {
        "loss": 2.3026,
        "grad_norm": 2.4374823570251465,
        "learning_rate": 4.041342049148132e-05,
        "epoch": 0.5215261665377675,
        "step": 4046
    },
    {
        "loss": 1.873,
        "grad_norm": 2.0214693546295166,
        "learning_rate": 4.035369672952516e-05,
        "epoch": 0.5216550657385924,
        "step": 4047
    },
    {
        "loss": 1.791,
        "grad_norm": 1.5654942989349365,
        "learning_rate": 4.029398725929212e-05,
        "epoch": 0.5217839649394174,
        "step": 4048
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.669333815574646,
        "learning_rate": 4.023429216924629e-05,
        "epoch": 0.5219128641402423,
        "step": 4049
    },
    {
        "loss": 2.4766,
        "grad_norm": 2.364718437194824,
        "learning_rate": 4.0174611547830414e-05,
        "epoch": 0.5220417633410673,
        "step": 4050
    },
    {
        "loss": 1.6288,
        "grad_norm": 2.755833148956299,
        "learning_rate": 4.0114945483465834e-05,
        "epoch": 0.5221706625418923,
        "step": 4051
    },
    {
        "loss": 2.1291,
        "grad_norm": 1.5319563150405884,
        "learning_rate": 4.0055294064552306e-05,
        "epoch": 0.5222995617427172,
        "step": 4052
    },
    {
        "loss": 1.058,
        "grad_norm": 2.875149965286255,
        "learning_rate": 3.999565737946786e-05,
        "epoch": 0.5224284609435421,
        "step": 4053
    },
    {
        "loss": 2.0711,
        "grad_norm": 1.5858708620071411,
        "learning_rate": 3.9936035516568777e-05,
        "epoch": 0.5225573601443672,
        "step": 4054
    },
    {
        "loss": 2.3149,
        "grad_norm": 2.4716169834136963,
        "learning_rate": 3.9876428564189286e-05,
        "epoch": 0.5226862593451921,
        "step": 4055
    },
    {
        "loss": 1.3029,
        "grad_norm": 2.199617385864258,
        "learning_rate": 3.981683661064158e-05,
        "epoch": 0.522815158546017,
        "step": 4056
    },
    {
        "loss": 1.0081,
        "grad_norm": 2.524256944656372,
        "learning_rate": 3.9757259744215623e-05,
        "epoch": 0.5229440577468419,
        "step": 4057
    },
    {
        "loss": 2.0925,
        "grad_norm": 1.5506772994995117,
        "learning_rate": 3.9697698053179e-05,
        "epoch": 0.523072956947667,
        "step": 4058
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.9308459758758545,
        "learning_rate": 3.963815162577685e-05,
        "epoch": 0.5232018561484919,
        "step": 4059
    },
    {
        "loss": 1.3342,
        "grad_norm": 3.6462934017181396,
        "learning_rate": 3.957862055023167e-05,
        "epoch": 0.5233307553493168,
        "step": 4060
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.725167155265808,
        "learning_rate": 3.951910491474323e-05,
        "epoch": 0.5234596545501418,
        "step": 4061
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.8632410764694214,
        "learning_rate": 3.9459604807488416e-05,
        "epoch": 0.5235885537509667,
        "step": 4062
    },
    {
        "loss": 1.8982,
        "grad_norm": 1.6810623407363892,
        "learning_rate": 3.940012031662106e-05,
        "epoch": 0.5237174529517917,
        "step": 4063
    },
    {
        "loss": 1.3725,
        "grad_norm": 2.3632030487060547,
        "learning_rate": 3.9340651530272014e-05,
        "epoch": 0.5238463521526167,
        "step": 4064
    },
    {
        "loss": 1.5379,
        "grad_norm": 2.166287660598755,
        "learning_rate": 3.928119853654865e-05,
        "epoch": 0.5239752513534416,
        "step": 4065
    },
    {
        "loss": 1.8938,
        "grad_norm": 2.1538684368133545,
        "learning_rate": 3.9221761423535065e-05,
        "epoch": 0.5241041505542665,
        "step": 4066
    },
    {
        "loss": 2.3207,
        "grad_norm": 1.8132270574569702,
        "learning_rate": 3.9162340279291874e-05,
        "epoch": 0.5242330497550916,
        "step": 4067
    },
    {
        "loss": 1.807,
        "grad_norm": 2.050542116165161,
        "learning_rate": 3.910293519185591e-05,
        "epoch": 0.5243619489559165,
        "step": 4068
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.53585684299469,
        "learning_rate": 3.9043546249240237e-05,
        "epoch": 0.5244908481567414,
        "step": 4069
    },
    {
        "loss": 2.0254,
        "grad_norm": 2.416027307510376,
        "learning_rate": 3.8984173539434143e-05,
        "epoch": 0.5246197473575663,
        "step": 4070
    },
    {
        "loss": 1.9676,
        "grad_norm": 1.7064396142959595,
        "learning_rate": 3.892481715040268e-05,
        "epoch": 0.5247486465583914,
        "step": 4071
    },
    {
        "loss": 2.0017,
        "grad_norm": 1.7758026123046875,
        "learning_rate": 3.886547717008676e-05,
        "epoch": 0.5248775457592163,
        "step": 4072
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.6346343755722046,
        "learning_rate": 3.880615368640317e-05,
        "epoch": 0.5250064449600412,
        "step": 4073
    },
    {
        "loss": 1.2771,
        "grad_norm": 2.370728015899658,
        "learning_rate": 3.874684678724398e-05,
        "epoch": 0.5251353441608662,
        "step": 4074
    },
    {
        "loss": 1.4601,
        "grad_norm": 2.7044570446014404,
        "learning_rate": 3.8687556560476814e-05,
        "epoch": 0.5252642433616912,
        "step": 4075
    },
    {
        "loss": 1.7482,
        "grad_norm": 1.890262484550476,
        "learning_rate": 3.862828309394471e-05,
        "epoch": 0.5253931425625161,
        "step": 4076
    },
    {
        "loss": 1.9095,
        "grad_norm": 2.7593398094177246,
        "learning_rate": 3.8569026475465655e-05,
        "epoch": 0.525522041763341,
        "step": 4077
    },
    {
        "loss": 1.5746,
        "grad_norm": 2.630341053009033,
        "learning_rate": 3.8509786792832797e-05,
        "epoch": 0.525650940964166,
        "step": 4078
    },
    {
        "loss": 2.158,
        "grad_norm": 3.2643346786499023,
        "learning_rate": 3.8450564133814186e-05,
        "epoch": 0.525779840164991,
        "step": 4079
    },
    {
        "loss": 1.4603,
        "grad_norm": 2.7761175632476807,
        "learning_rate": 3.839135858615277e-05,
        "epoch": 0.525908739365816,
        "step": 4080
    },
    {
        "loss": 2.1295,
        "grad_norm": 2.039100408554077,
        "learning_rate": 3.83321702375658e-05,
        "epoch": 0.5260376385666409,
        "step": 4081
    },
    {
        "loss": 2.0416,
        "grad_norm": 2.2029969692230225,
        "learning_rate": 3.827299917574535e-05,
        "epoch": 0.5261665377674658,
        "step": 4082
    },
    {
        "loss": 1.8479,
        "grad_norm": 2.2757813930511475,
        "learning_rate": 3.8213845488357916e-05,
        "epoch": 0.5262954369682908,
        "step": 4083
    },
    {
        "loss": 1.6795,
        "grad_norm": 2.0632753372192383,
        "learning_rate": 3.815470926304392e-05,
        "epoch": 0.5264243361691158,
        "step": 4084
    },
    {
        "loss": 2.3809,
        "grad_norm": 1.4511548280715942,
        "learning_rate": 3.809559058741823e-05,
        "epoch": 0.5265532353699407,
        "step": 4085
    },
    {
        "loss": 2.2044,
        "grad_norm": 1.797947883605957,
        "learning_rate": 3.803648954906971e-05,
        "epoch": 0.5266821345707656,
        "step": 4086
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.6671897172927856,
        "learning_rate": 3.797740623556077e-05,
        "epoch": 0.5268110337715907,
        "step": 4087
    },
    {
        "loss": 1.7917,
        "grad_norm": 2.2178921699523926,
        "learning_rate": 3.7918340734427895e-05,
        "epoch": 0.5269399329724156,
        "step": 4088
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.5251100063323975,
        "learning_rate": 3.785929313318116e-05,
        "epoch": 0.5270688321732405,
        "step": 4089
    },
    {
        "loss": 2.3271,
        "grad_norm": 2.4854416847229004,
        "learning_rate": 3.7800263519303815e-05,
        "epoch": 0.5271977313740654,
        "step": 4090
    },
    {
        "loss": 2.1219,
        "grad_norm": 2.117515802383423,
        "learning_rate": 3.774125198025277e-05,
        "epoch": 0.5273266305748905,
        "step": 4091
    },
    {
        "loss": 2.1698,
        "grad_norm": 1.1396750211715698,
        "learning_rate": 3.768225860345816e-05,
        "epoch": 0.5274555297757154,
        "step": 4092
    },
    {
        "loss": 1.9795,
        "grad_norm": 2.674346446990967,
        "learning_rate": 3.762328347632292e-05,
        "epoch": 0.5275844289765403,
        "step": 4093
    },
    {
        "loss": 2.0836,
        "grad_norm": 2.604790210723877,
        "learning_rate": 3.7564326686223214e-05,
        "epoch": 0.5277133281773653,
        "step": 4094
    },
    {
        "loss": 1.5936,
        "grad_norm": 2.6721370220184326,
        "learning_rate": 3.7505388320507986e-05,
        "epoch": 0.5278422273781903,
        "step": 4095
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.1584956645965576,
        "learning_rate": 3.7446468466498876e-05,
        "epoch": 0.5279711265790152,
        "step": 4096
    },
    {
        "loss": 1.3115,
        "grad_norm": 3.190875768661499,
        "learning_rate": 3.7387567211489935e-05,
        "epoch": 0.5281000257798402,
        "step": 4097
    },
    {
        "loss": 2.402,
        "grad_norm": 1.5591455698013306,
        "learning_rate": 3.732868464274794e-05,
        "epoch": 0.5282289249806651,
        "step": 4098
    },
    {
        "loss": 2.04,
        "grad_norm": 2.1838128566741943,
        "learning_rate": 3.7269820847511814e-05,
        "epoch": 0.52835782418149,
        "step": 4099
    },
    {
        "loss": 0.5601,
        "grad_norm": 2.523344039916992,
        "learning_rate": 3.721097591299262e-05,
        "epoch": 0.5284867233823151,
        "step": 4100
    },
    {
        "loss": 2.3902,
        "grad_norm": 1.8324809074401855,
        "learning_rate": 3.7152149926373616e-05,
        "epoch": 0.52861562258314,
        "step": 4101
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.4102842807769775,
        "learning_rate": 3.7093342974809955e-05,
        "epoch": 0.5287445217839649,
        "step": 4102
    },
    {
        "loss": 2.1517,
        "grad_norm": 1.5079715251922607,
        "learning_rate": 3.703455514542843e-05,
        "epoch": 0.5288734209847898,
        "step": 4103
    },
    {
        "loss": 2.3649,
        "grad_norm": 2.4314775466918945,
        "learning_rate": 3.697578652532773e-05,
        "epoch": 0.5290023201856149,
        "step": 4104
    },
    {
        "loss": 1.2241,
        "grad_norm": 2.4881675243377686,
        "learning_rate": 3.6917037201578e-05,
        "epoch": 0.5291312193864398,
        "step": 4105
    },
    {
        "loss": 1.977,
        "grad_norm": 3.2864742279052734,
        "learning_rate": 3.685830726122065e-05,
        "epoch": 0.5292601185872647,
        "step": 4106
    },
    {
        "loss": 1.4867,
        "grad_norm": 2.5282866954803467,
        "learning_rate": 3.679959679126862e-05,
        "epoch": 0.5293890177880897,
        "step": 4107
    },
    {
        "loss": 2.434,
        "grad_norm": 1.327528953552246,
        "learning_rate": 3.674090587870589e-05,
        "epoch": 0.5295179169889147,
        "step": 4108
    },
    {
        "loss": 2.2657,
        "grad_norm": 1.8654159307479858,
        "learning_rate": 3.6682234610487334e-05,
        "epoch": 0.5296468161897396,
        "step": 4109
    },
    {
        "loss": 2.3653,
        "grad_norm": 1.972904086112976,
        "learning_rate": 3.662358307353896e-05,
        "epoch": 0.5297757153905646,
        "step": 4110
    },
    {
        "loss": 1.8312,
        "grad_norm": 1.8774794340133667,
        "learning_rate": 3.656495135475738e-05,
        "epoch": 0.5299046145913895,
        "step": 4111
    },
    {
        "loss": 1.8324,
        "grad_norm": 1.8011109828948975,
        "learning_rate": 3.650633954100991e-05,
        "epoch": 0.5300335137922145,
        "step": 4112
    },
    {
        "loss": 2.5436,
        "grad_norm": 3.471500873565674,
        "learning_rate": 3.644774771913434e-05,
        "epoch": 0.5301624129930395,
        "step": 4113
    },
    {
        "loss": 1.3451,
        "grad_norm": 1.6912438869476318,
        "learning_rate": 3.638917597593887e-05,
        "epoch": 0.5302913121938644,
        "step": 4114
    },
    {
        "loss": 2.2566,
        "grad_norm": 1.4604301452636719,
        "learning_rate": 3.633062439820193e-05,
        "epoch": 0.5304202113946893,
        "step": 4115
    },
    {
        "loss": 2.444,
        "grad_norm": 2.068450450897217,
        "learning_rate": 3.627209307267208e-05,
        "epoch": 0.5305491105955144,
        "step": 4116
    },
    {
        "loss": 1.6574,
        "grad_norm": 4.11033821105957,
        "learning_rate": 3.6213582086067884e-05,
        "epoch": 0.5306780097963393,
        "step": 4117
    },
    {
        "loss": 1.6462,
        "grad_norm": 2.004387140274048,
        "learning_rate": 3.615509152507777e-05,
        "epoch": 0.5308069089971642,
        "step": 4118
    },
    {
        "loss": 2.0782,
        "grad_norm": 3.1527833938598633,
        "learning_rate": 3.6096621476359884e-05,
        "epoch": 0.5309358081979891,
        "step": 4119
    },
    {
        "loss": 1.8777,
        "grad_norm": 1.6758556365966797,
        "learning_rate": 3.603817202654201e-05,
        "epoch": 0.5310647073988142,
        "step": 4120
    },
    {
        "loss": 2.2673,
        "grad_norm": 2.3725409507751465,
        "learning_rate": 3.597974326222138e-05,
        "epoch": 0.5311936065996391,
        "step": 4121
    },
    {
        "loss": 1.8285,
        "grad_norm": 3.0075559616088867,
        "learning_rate": 3.592133526996463e-05,
        "epoch": 0.531322505800464,
        "step": 4122
    },
    {
        "loss": 1.646,
        "grad_norm": 1.5324152708053589,
        "learning_rate": 3.5862948136307554e-05,
        "epoch": 0.531451405001289,
        "step": 4123
    },
    {
        "loss": 1.8772,
        "grad_norm": 2.6632344722747803,
        "learning_rate": 3.580458194775511e-05,
        "epoch": 0.531580304202114,
        "step": 4124
    },
    {
        "loss": 2.0199,
        "grad_norm": 1.9375287294387817,
        "learning_rate": 3.574623679078118e-05,
        "epoch": 0.5317092034029389,
        "step": 4125
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.112919807434082,
        "learning_rate": 3.568791275182849e-05,
        "epoch": 0.5318381026037639,
        "step": 4126
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.9554041624069214,
        "learning_rate": 3.56296099173085e-05,
        "epoch": 0.5319670018045888,
        "step": 4127
    },
    {
        "loss": 1.8312,
        "grad_norm": 3.150426149368286,
        "learning_rate": 3.557132837360122e-05,
        "epoch": 0.5320959010054138,
        "step": 4128
    },
    {
        "loss": 2.6846,
        "grad_norm": 1.6875399351119995,
        "learning_rate": 3.551306820705516e-05,
        "epoch": 0.5322248002062387,
        "step": 4129
    },
    {
        "loss": 2.2062,
        "grad_norm": 1.3982555866241455,
        "learning_rate": 3.545482950398709e-05,
        "epoch": 0.5323536994070637,
        "step": 4130
    },
    {
        "loss": 2.355,
        "grad_norm": 2.492727756500244,
        "learning_rate": 3.539661235068212e-05,
        "epoch": 0.5324825986078886,
        "step": 4131
    },
    {
        "loss": 2.5366,
        "grad_norm": 2.0525708198547363,
        "learning_rate": 3.533841683339325e-05,
        "epoch": 0.5326114978087136,
        "step": 4132
    },
    {
        "loss": 2.2257,
        "grad_norm": 1.8485698699951172,
        "learning_rate": 3.52802430383415e-05,
        "epoch": 0.5327403970095386,
        "step": 4133
    },
    {
        "loss": 1.9974,
        "grad_norm": 2.705975294113159,
        "learning_rate": 3.5222091051715825e-05,
        "epoch": 0.5328692962103635,
        "step": 4134
    },
    {
        "loss": 2.294,
        "grad_norm": 1.5829209089279175,
        "learning_rate": 3.516396095967265e-05,
        "epoch": 0.5329981954111884,
        "step": 4135
    },
    {
        "loss": 2.0825,
        "grad_norm": 2.7173304557800293,
        "learning_rate": 3.51058528483361e-05,
        "epoch": 0.5331270946120134,
        "step": 4136
    },
    {
        "loss": 2.1864,
        "grad_norm": 1.7769821882247925,
        "learning_rate": 3.504776680379779e-05,
        "epoch": 0.5332559938128384,
        "step": 4137
    },
    {
        "loss": 2.4154,
        "grad_norm": 1.5287764072418213,
        "learning_rate": 3.498970291211645e-05,
        "epoch": 0.5333848930136633,
        "step": 4138
    },
    {
        "loss": 1.4792,
        "grad_norm": 1.8420391082763672,
        "learning_rate": 3.493166125931813e-05,
        "epoch": 0.5335137922144882,
        "step": 4139
    },
    {
        "loss": 1.7775,
        "grad_norm": 1.9272319078445435,
        "learning_rate": 3.487364193139597e-05,
        "epoch": 0.5336426914153132,
        "step": 4140
    },
    {
        "loss": 1.1126,
        "grad_norm": 3.303624153137207,
        "learning_rate": 3.4815645014309885e-05,
        "epoch": 0.5337715906161382,
        "step": 4141
    },
    {
        "loss": 2.7166,
        "grad_norm": 1.7072434425354004,
        "learning_rate": 3.4757670593986644e-05,
        "epoch": 0.5339004898169631,
        "step": 4142
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.5645098686218262,
        "learning_rate": 3.4699718756319823e-05,
        "epoch": 0.5340293890177881,
        "step": 4143
    },
    {
        "loss": 1.3608,
        "grad_norm": 2.5834717750549316,
        "learning_rate": 3.464178958716933e-05,
        "epoch": 0.534158288218613,
        "step": 4144
    },
    {
        "loss": 1.1977,
        "grad_norm": 2.7177395820617676,
        "learning_rate": 3.458388317236156e-05,
        "epoch": 0.534287187419438,
        "step": 4145
    },
    {
        "loss": 1.9404,
        "grad_norm": 1.7754920721054077,
        "learning_rate": 3.452599959768928e-05,
        "epoch": 0.534416086620263,
        "step": 4146
    },
    {
        "loss": 2.3536,
        "grad_norm": 1.5932990312576294,
        "learning_rate": 3.446813894891143e-05,
        "epoch": 0.5345449858210879,
        "step": 4147
    },
    {
        "loss": 1.9632,
        "grad_norm": 2.5504088401794434,
        "learning_rate": 3.4410301311752707e-05,
        "epoch": 0.5346738850219128,
        "step": 4148
    },
    {
        "loss": 1.975,
        "grad_norm": 2.0414185523986816,
        "learning_rate": 3.4352486771904034e-05,
        "epoch": 0.5348027842227379,
        "step": 4149
    },
    {
        "loss": 2.0752,
        "grad_norm": 2.5393006801605225,
        "learning_rate": 3.429469541502209e-05,
        "epoch": 0.5349316834235628,
        "step": 4150
    },
    {
        "loss": 1.5631,
        "grad_norm": 2.5974819660186768,
        "learning_rate": 3.423692732672892e-05,
        "epoch": 0.5350605826243877,
        "step": 4151
    },
    {
        "loss": 1.1485,
        "grad_norm": 3.041909694671631,
        "learning_rate": 3.417918259261237e-05,
        "epoch": 0.5351894818252126,
        "step": 4152
    },
    {
        "loss": 1.8388,
        "grad_norm": 2.118500232696533,
        "learning_rate": 3.4121461298225707e-05,
        "epoch": 0.5353183810260377,
        "step": 4153
    },
    {
        "loss": 1.7647,
        "grad_norm": 1.9841057062149048,
        "learning_rate": 3.406376352908719e-05,
        "epoch": 0.5354472802268626,
        "step": 4154
    },
    {
        "loss": 1.513,
        "grad_norm": 1.6060360670089722,
        "learning_rate": 3.4006089370680465e-05,
        "epoch": 0.5355761794276875,
        "step": 4155
    },
    {
        "loss": 1.7587,
        "grad_norm": 2.530555486679077,
        "learning_rate": 3.394843890845425e-05,
        "epoch": 0.5357050786285125,
        "step": 4156
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.0097830295562744,
        "learning_rate": 3.3890812227821834e-05,
        "epoch": 0.5358339778293375,
        "step": 4157
    },
    {
        "loss": 1.7901,
        "grad_norm": 1.9972314834594727,
        "learning_rate": 3.383320941416158e-05,
        "epoch": 0.5359628770301624,
        "step": 4158
    },
    {
        "loss": 1.7475,
        "grad_norm": 2.932051420211792,
        "learning_rate": 3.377563055281648e-05,
        "epoch": 0.5360917762309874,
        "step": 4159
    },
    {
        "loss": 2.4818,
        "grad_norm": 1.50953209400177,
        "learning_rate": 3.3718075729093736e-05,
        "epoch": 0.5362206754318123,
        "step": 4160
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.6270732879638672,
        "learning_rate": 3.3660545028265256e-05,
        "epoch": 0.5363495746326373,
        "step": 4161
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.3329566717147827,
        "learning_rate": 3.360303853556711e-05,
        "epoch": 0.5364784738334623,
        "step": 4162
    },
    {
        "loss": 2.1445,
        "grad_norm": 2.169250726699829,
        "learning_rate": 3.354555633619952e-05,
        "epoch": 0.5366073730342872,
        "step": 4163
    },
    {
        "loss": 2.117,
        "grad_norm": 1.4126211404800415,
        "learning_rate": 3.3488098515326554e-05,
        "epoch": 0.5367362722351121,
        "step": 4164
    },
    {
        "loss": 2.3909,
        "grad_norm": 1.9927477836608887,
        "learning_rate": 3.3430665158076415e-05,
        "epoch": 0.5368651714359371,
        "step": 4165
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.8665344715118408,
        "learning_rate": 3.337325634954092e-05,
        "epoch": 0.5369940706367621,
        "step": 4166
    },
    {
        "loss": 1.2695,
        "grad_norm": 2.536464214324951,
        "learning_rate": 3.331587217477543e-05,
        "epoch": 0.537122969837587,
        "step": 4167
    },
    {
        "loss": 2.2499,
        "grad_norm": 1.586828589439392,
        "learning_rate": 3.3258512718799037e-05,
        "epoch": 0.5372518690384119,
        "step": 4168
    },
    {
        "loss": 1.5611,
        "grad_norm": 2.927868604660034,
        "learning_rate": 3.320117806659405e-05,
        "epoch": 0.537380768239237,
        "step": 4169
    },
    {
        "loss": 1.7221,
        "grad_norm": 2.8753256797790527,
        "learning_rate": 3.3143868303105995e-05,
        "epoch": 0.5375096674400619,
        "step": 4170
    },
    {
        "loss": 1.7709,
        "grad_norm": 1.7590612173080444,
        "learning_rate": 3.308658351324367e-05,
        "epoch": 0.5376385666408868,
        "step": 4171
    },
    {
        "loss": 2.5148,
        "grad_norm": 1.8470942974090576,
        "learning_rate": 3.302932378187882e-05,
        "epoch": 0.5377674658417118,
        "step": 4172
    },
    {
        "loss": 1.9824,
        "grad_norm": 2.3570404052734375,
        "learning_rate": 3.297208919384593e-05,
        "epoch": 0.5378963650425367,
        "step": 4173
    },
    {
        "loss": 2.2472,
        "grad_norm": 1.9766263961791992,
        "learning_rate": 3.291487983394243e-05,
        "epoch": 0.5380252642433617,
        "step": 4174
    },
    {
        "loss": 2.2494,
        "grad_norm": 2.343441963195801,
        "learning_rate": 3.285769578692829e-05,
        "epoch": 0.5381541634441867,
        "step": 4175
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.228504180908203,
        "learning_rate": 3.2800537137525875e-05,
        "epoch": 0.5382830626450116,
        "step": 4176
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.1639938354492188,
        "learning_rate": 3.2743403970420125e-05,
        "epoch": 0.5384119618458365,
        "step": 4177
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.9886629581451416,
        "learning_rate": 3.268629637025809e-05,
        "epoch": 0.5385408610466615,
        "step": 4178
    },
    {
        "loss": 2.2705,
        "grad_norm": 1.2139859199523926,
        "learning_rate": 3.2629214421648957e-05,
        "epoch": 0.5386697602474865,
        "step": 4179
    },
    {
        "loss": 1.9118,
        "grad_norm": 1.7120630741119385,
        "learning_rate": 3.2572158209163936e-05,
        "epoch": 0.5387986594483114,
        "step": 4180
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.3768882751464844,
        "learning_rate": 3.2515127817336064e-05,
        "epoch": 0.5389275586491363,
        "step": 4181
    },
    {
        "loss": 1.5319,
        "grad_norm": 2.8867545127868652,
        "learning_rate": 3.24581233306602e-05,
        "epoch": 0.5390564578499614,
        "step": 4182
    },
    {
        "loss": 1.5782,
        "grad_norm": 2.3751235008239746,
        "learning_rate": 3.240114483359275e-05,
        "epoch": 0.5391853570507863,
        "step": 4183
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.7185137271881104,
        "learning_rate": 3.234419241055161e-05,
        "epoch": 0.5393142562516112,
        "step": 4184
    },
    {
        "loss": 2.5493,
        "grad_norm": 2.209963321685791,
        "learning_rate": 3.2287266145916115e-05,
        "epoch": 0.5394431554524362,
        "step": 4185
    },
    {
        "loss": 2.0144,
        "grad_norm": 2.26505708694458,
        "learning_rate": 3.223036612402679e-05,
        "epoch": 0.5395720546532612,
        "step": 4186
    },
    {
        "loss": 1.7895,
        "grad_norm": 2.125579357147217,
        "learning_rate": 3.21734924291853e-05,
        "epoch": 0.5397009538540861,
        "step": 4187
    },
    {
        "loss": 1.5876,
        "grad_norm": 2.8512773513793945,
        "learning_rate": 3.211664514565428e-05,
        "epoch": 0.539829853054911,
        "step": 4188
    },
    {
        "loss": 2.0945,
        "grad_norm": 1.6107983589172363,
        "learning_rate": 3.205982435765726e-05,
        "epoch": 0.539958752255736,
        "step": 4189
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.8157230615615845,
        "learning_rate": 3.200303014937852e-05,
        "epoch": 0.540087651456561,
        "step": 4190
    },
    {
        "loss": 1.2021,
        "grad_norm": 2.4665777683258057,
        "learning_rate": 3.194626260496293e-05,
        "epoch": 0.5402165506573859,
        "step": 4191
    },
    {
        "loss": 1.5127,
        "grad_norm": 2.7364425659179688,
        "learning_rate": 3.188952180851589e-05,
        "epoch": 0.5403454498582109,
        "step": 4192
    },
    {
        "loss": 2.4944,
        "grad_norm": 1.295844554901123,
        "learning_rate": 3.1832807844103125e-05,
        "epoch": 0.5404743490590358,
        "step": 4193
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.9385552406311035,
        "learning_rate": 3.1776120795750635e-05,
        "epoch": 0.5406032482598608,
        "step": 4194
    },
    {
        "loss": 1.635,
        "grad_norm": 2.1892027854919434,
        "learning_rate": 3.171946074744462e-05,
        "epoch": 0.5407321474606858,
        "step": 4195
    },
    {
        "loss": 2.0091,
        "grad_norm": 2.0892393589019775,
        "learning_rate": 3.16628277831311e-05,
        "epoch": 0.5408610466615107,
        "step": 4196
    },
    {
        "loss": 1.9494,
        "grad_norm": 1.873656988143921,
        "learning_rate": 3.1606221986716087e-05,
        "epoch": 0.5409899458623356,
        "step": 4197
    },
    {
        "loss": 1.9044,
        "grad_norm": 2.720768690109253,
        "learning_rate": 3.154964344206539e-05,
        "epoch": 0.5411188450631607,
        "step": 4198
    },
    {
        "loss": 2.608,
        "grad_norm": 1.3954212665557861,
        "learning_rate": 3.14930922330043e-05,
        "epoch": 0.5412477442639856,
        "step": 4199
    },
    {
        "loss": 1.2999,
        "grad_norm": 2.793612480163574,
        "learning_rate": 3.1436568443317674e-05,
        "epoch": 0.5413766434648105,
        "step": 4200
    },
    {
        "loss": 2.2525,
        "grad_norm": 1.6458463668823242,
        "learning_rate": 3.138007215674985e-05,
        "epoch": 0.5415055426656354,
        "step": 4201
    },
    {
        "loss": 2.4096,
        "grad_norm": 1.270970106124878,
        "learning_rate": 3.1323603457004234e-05,
        "epoch": 0.5416344418664605,
        "step": 4202
    },
    {
        "loss": 2.5756,
        "grad_norm": 1.7822272777557373,
        "learning_rate": 3.126716242774343e-05,
        "epoch": 0.5417633410672854,
        "step": 4203
    },
    {
        "loss": 1.7741,
        "grad_norm": 2.2045087814331055,
        "learning_rate": 3.121074915258918e-05,
        "epoch": 0.5418922402681103,
        "step": 4204
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.8127316236495972,
        "learning_rate": 3.1154363715121884e-05,
        "epoch": 0.5420211394689353,
        "step": 4205
    },
    {
        "loss": 2.1217,
        "grad_norm": 2.841857671737671,
        "learning_rate": 3.10980061988808e-05,
        "epoch": 0.5421500386697602,
        "step": 4206
    },
    {
        "loss": 2.0489,
        "grad_norm": 2.82248854637146,
        "learning_rate": 3.104167668736392e-05,
        "epoch": 0.5422789378705852,
        "step": 4207
    },
    {
        "loss": 1.6789,
        "grad_norm": 2.1802589893341064,
        "learning_rate": 3.0985375264027564e-05,
        "epoch": 0.5424078370714102,
        "step": 4208
    },
    {
        "loss": 2.0582,
        "grad_norm": 1.9526375532150269,
        "learning_rate": 3.092910201228651e-05,
        "epoch": 0.5425367362722351,
        "step": 4209
    },
    {
        "loss": 2.0182,
        "grad_norm": 2.2141947746276855,
        "learning_rate": 3.087285701551392e-05,
        "epoch": 0.54266563547306,
        "step": 4210
    },
    {
        "loss": 2.027,
        "grad_norm": 2.772430896759033,
        "learning_rate": 3.081664035704086e-05,
        "epoch": 0.542794534673885,
        "step": 4211
    },
    {
        "loss": 2.0471,
        "grad_norm": 1.8314684629440308,
        "learning_rate": 3.076045212015656e-05,
        "epoch": 0.54292343387471,
        "step": 4212
    },
    {
        "loss": 1.4117,
        "grad_norm": 2.293118476867676,
        "learning_rate": 3.0704292388108126e-05,
        "epoch": 0.5430523330755349,
        "step": 4213
    },
    {
        "loss": 2.5538,
        "grad_norm": 1.4298222064971924,
        "learning_rate": 3.064816124410054e-05,
        "epoch": 0.5431812322763598,
        "step": 4214
    },
    {
        "loss": 1.8231,
        "grad_norm": 2.712029218673706,
        "learning_rate": 3.059205877129609e-05,
        "epoch": 0.5433101314771849,
        "step": 4215
    },
    {
        "loss": 2.2879,
        "grad_norm": 2.330930471420288,
        "learning_rate": 3.053598505281493e-05,
        "epoch": 0.5434390306780098,
        "step": 4216
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.756870985031128,
        "learning_rate": 3.0479940171734567e-05,
        "epoch": 0.5435679298788347,
        "step": 4217
    },
    {
        "loss": 2.5056,
        "grad_norm": 1.8232449293136597,
        "learning_rate": 3.0423924211089505e-05,
        "epoch": 0.5436968290796597,
        "step": 4218
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.501474618911743,
        "learning_rate": 3.0367937253871714e-05,
        "epoch": 0.5438257282804847,
        "step": 4219
    },
    {
        "loss": 2.1919,
        "grad_norm": 2.1309139728546143,
        "learning_rate": 3.0311979383030155e-05,
        "epoch": 0.5439546274813096,
        "step": 4220
    },
    {
        "loss": 1.8401,
        "grad_norm": 2.46008038520813,
        "learning_rate": 3.0256050681470444e-05,
        "epoch": 0.5440835266821346,
        "step": 4221
    },
    {
        "loss": 1.5578,
        "grad_norm": 2.945722818374634,
        "learning_rate": 3.0200151232055235e-05,
        "epoch": 0.5442124258829595,
        "step": 4222
    },
    {
        "loss": 2.0415,
        "grad_norm": 2.676954746246338,
        "learning_rate": 3.0144281117603874e-05,
        "epoch": 0.5443413250837845,
        "step": 4223
    },
    {
        "loss": 2.4312,
        "grad_norm": 1.2477532625198364,
        "learning_rate": 3.0088440420891933e-05,
        "epoch": 0.5444702242846094,
        "step": 4224
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.795066237449646,
        "learning_rate": 3.0032629224651688e-05,
        "epoch": 0.5445991234854344,
        "step": 4225
    },
    {
        "loss": 1.8205,
        "grad_norm": 1.9037455320358276,
        "learning_rate": 2.9976847611571736e-05,
        "epoch": 0.5447280226862593,
        "step": 4226
    },
    {
        "loss": 2.5314,
        "grad_norm": 1.6067277193069458,
        "learning_rate": 2.9921095664296516e-05,
        "epoch": 0.5448569218870843,
        "step": 4227
    },
    {
        "loss": 1.91,
        "grad_norm": 1.9295021295547485,
        "learning_rate": 2.9865373465426837e-05,
        "epoch": 0.5449858210879093,
        "step": 4228
    },
    {
        "loss": 2.1986,
        "grad_norm": 1.5289145708084106,
        "learning_rate": 2.980968109751935e-05,
        "epoch": 0.5451147202887342,
        "step": 4229
    },
    {
        "loss": 1.3701,
        "grad_norm": 2.771493434906006,
        "learning_rate": 2.9754018643086485e-05,
        "epoch": 0.5452436194895591,
        "step": 4230
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.9528487920761108,
        "learning_rate": 2.969838618459626e-05,
        "epoch": 0.5453725186903842,
        "step": 4231
    },
    {
        "loss": 2.4555,
        "grad_norm": 1.7405680418014526,
        "learning_rate": 2.964278380447242e-05,
        "epoch": 0.5455014178912091,
        "step": 4232
    },
    {
        "loss": 1.6221,
        "grad_norm": 1.9902266263961792,
        "learning_rate": 2.9587211585094087e-05,
        "epoch": 0.545630317092034,
        "step": 4233
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.9166111946105957,
        "learning_rate": 2.9531669608795575e-05,
        "epoch": 0.545759216292859,
        "step": 4234
    },
    {
        "loss": 2.561,
        "grad_norm": 1.235584020614624,
        "learning_rate": 2.9476157957866602e-05,
        "epoch": 0.545888115493684,
        "step": 4235
    },
    {
        "loss": 1.4208,
        "grad_norm": 3.2848122119903564,
        "learning_rate": 2.9420676714551843e-05,
        "epoch": 0.5460170146945089,
        "step": 4236
    },
    {
        "loss": 1.092,
        "grad_norm": 2.501349687576294,
        "learning_rate": 2.9365225961050825e-05,
        "epoch": 0.5461459138953338,
        "step": 4237
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.5330464839935303,
        "learning_rate": 2.9309805779518118e-05,
        "epoch": 0.5462748130961588,
        "step": 4238
    },
    {
        "loss": 2.316,
        "grad_norm": 1.7238292694091797,
        "learning_rate": 2.9254416252062876e-05,
        "epoch": 0.5464037122969838,
        "step": 4239
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.9426666498184204,
        "learning_rate": 2.9199057460748747e-05,
        "epoch": 0.5465326114978087,
        "step": 4240
    },
    {
        "loss": 2.5704,
        "grad_norm": 1.6210037469863892,
        "learning_rate": 2.914372948759404e-05,
        "epoch": 0.5466615106986337,
        "step": 4241
    },
    {
        "loss": 2.0296,
        "grad_norm": 2.703887939453125,
        "learning_rate": 2.908843241457131e-05,
        "epoch": 0.5467904098994586,
        "step": 4242
    },
    {
        "loss": 1.8927,
        "grad_norm": 2.2110283374786377,
        "learning_rate": 2.903316632360721e-05,
        "epoch": 0.5469193091002835,
        "step": 4243
    },
    {
        "loss": 1.927,
        "grad_norm": 2.43593692779541,
        "learning_rate": 2.897793129658274e-05,
        "epoch": 0.5470482083011086,
        "step": 4244
    },
    {
        "loss": 1.8952,
        "grad_norm": 2.559893846511841,
        "learning_rate": 2.892272741533267e-05,
        "epoch": 0.5471771075019335,
        "step": 4245
    },
    {
        "loss": 1.9512,
        "grad_norm": 2.5021374225616455,
        "learning_rate": 2.8867554761645716e-05,
        "epoch": 0.5473060067027584,
        "step": 4246
    },
    {
        "loss": 2.0191,
        "grad_norm": 1.8152199983596802,
        "learning_rate": 2.8812413417264328e-05,
        "epoch": 0.5474349059035833,
        "step": 4247
    },
    {
        "loss": 1.6905,
        "grad_norm": 1.9900263547897339,
        "learning_rate": 2.875730346388452e-05,
        "epoch": 0.5475638051044084,
        "step": 4248
    },
    {
        "loss": 1.6264,
        "grad_norm": 2.993952751159668,
        "learning_rate": 2.8702224983155834e-05,
        "epoch": 0.5476927043052333,
        "step": 4249
    },
    {
        "loss": 2.212,
        "grad_norm": 1.3268375396728516,
        "learning_rate": 2.8647178056681194e-05,
        "epoch": 0.5478216035060582,
        "step": 4250
    },
    {
        "loss": 1.4731,
        "grad_norm": 4.474364757537842,
        "learning_rate": 2.8592162766016773e-05,
        "epoch": 0.5479505027068832,
        "step": 4251
    },
    {
        "loss": 1.5025,
        "grad_norm": 2.368913412094116,
        "learning_rate": 2.853717919267181e-05,
        "epoch": 0.5480794019077082,
        "step": 4252
    },
    {
        "loss": 2.0993,
        "grad_norm": 1.629510760307312,
        "learning_rate": 2.8482227418108626e-05,
        "epoch": 0.5482083011085331,
        "step": 4253
    },
    {
        "loss": 1.9578,
        "grad_norm": 1.9093607664108276,
        "learning_rate": 2.8427307523742424e-05,
        "epoch": 0.5483372003093581,
        "step": 4254
    },
    {
        "loss": 2.0634,
        "grad_norm": 2.315200090408325,
        "learning_rate": 2.8372419590941113e-05,
        "epoch": 0.548466099510183,
        "step": 4255
    },
    {
        "loss": 1.914,
        "grad_norm": 1.8305195569992065,
        "learning_rate": 2.8317563701025318e-05,
        "epoch": 0.548594998711008,
        "step": 4256
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.929164409637451,
        "learning_rate": 2.826273993526817e-05,
        "epoch": 0.548723897911833,
        "step": 4257
    },
    {
        "loss": 1.7842,
        "grad_norm": 1.5778132677078247,
        "learning_rate": 2.8207948374895166e-05,
        "epoch": 0.5488527971126579,
        "step": 4258
    },
    {
        "loss": 1.8157,
        "grad_norm": 2.8227336406707764,
        "learning_rate": 2.8153189101084133e-05,
        "epoch": 0.5489816963134828,
        "step": 4259
    },
    {
        "loss": 1.6277,
        "grad_norm": 2.5762908458709717,
        "learning_rate": 2.809846219496505e-05,
        "epoch": 0.5491105955143079,
        "step": 4260
    },
    {
        "loss": 2.0224,
        "grad_norm": 2.0444841384887695,
        "learning_rate": 2.804376773761993e-05,
        "epoch": 0.5492394947151328,
        "step": 4261
    },
    {
        "loss": 1.4674,
        "grad_norm": 2.328723192214966,
        "learning_rate": 2.7989105810082795e-05,
        "epoch": 0.5493683939159577,
        "step": 4262
    },
    {
        "loss": 2.5367,
        "grad_norm": 1.832026481628418,
        "learning_rate": 2.79344764933393e-05,
        "epoch": 0.5494972931167826,
        "step": 4263
    },
    {
        "loss": 1.9377,
        "grad_norm": 2.3556230068206787,
        "learning_rate": 2.7879879868326898e-05,
        "epoch": 0.5496261923176077,
        "step": 4264
    },
    {
        "loss": 1.6907,
        "grad_norm": 3.1386842727661133,
        "learning_rate": 2.782531601593467e-05,
        "epoch": 0.5497550915184326,
        "step": 4265
    },
    {
        "loss": 2.3097,
        "grad_norm": 1.4462968111038208,
        "learning_rate": 2.7770785017002978e-05,
        "epoch": 0.5498839907192575,
        "step": 4266
    },
    {
        "loss": 1.4459,
        "grad_norm": 2.8604907989501953,
        "learning_rate": 2.77162869523236e-05,
        "epoch": 0.5500128899200825,
        "step": 4267
    },
    {
        "loss": 1.5817,
        "grad_norm": 2.088334560394287,
        "learning_rate": 2.7661821902639594e-05,
        "epoch": 0.5501417891209075,
        "step": 4268
    },
    {
        "loss": 2.2848,
        "grad_norm": 1.9971805810928345,
        "learning_rate": 2.7607389948644937e-05,
        "epoch": 0.5502706883217324,
        "step": 4269
    },
    {
        "loss": 1.8604,
        "grad_norm": 2.2932679653167725,
        "learning_rate": 2.755299117098466e-05,
        "epoch": 0.5503995875225574,
        "step": 4270
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.8373374938964844,
        "learning_rate": 2.7498625650254735e-05,
        "epoch": 0.5505284867233823,
        "step": 4271
    },
    {
        "loss": 1.7077,
        "grad_norm": 2.533776044845581,
        "learning_rate": 2.744429346700166e-05,
        "epoch": 0.5506573859242073,
        "step": 4272
    },
    {
        "loss": 2.4528,
        "grad_norm": 1.6247833967208862,
        "learning_rate": 2.7389994701722667e-05,
        "epoch": 0.5507862851250322,
        "step": 4273
    },
    {
        "loss": 1.9606,
        "grad_norm": 2.0942280292510986,
        "learning_rate": 2.7335729434865535e-05,
        "epoch": 0.5509151843258572,
        "step": 4274
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.5375944375991821,
        "learning_rate": 2.7281497746828243e-05,
        "epoch": 0.5510440835266821,
        "step": 4275
    },
    {
        "loss": 1.3403,
        "grad_norm": 2.2764523029327393,
        "learning_rate": 2.722729971795912e-05,
        "epoch": 0.5511729827275071,
        "step": 4276
    },
    {
        "loss": 1.3901,
        "grad_norm": 3.279996156692505,
        "learning_rate": 2.7173135428556633e-05,
        "epoch": 0.5513018819283321,
        "step": 4277
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.3533577919006348,
        "learning_rate": 2.7119004958869377e-05,
        "epoch": 0.551430781129157,
        "step": 4278
    },
    {
        "loss": 2.6416,
        "grad_norm": 1.339981198310852,
        "learning_rate": 2.7064908389095468e-05,
        "epoch": 0.5515596803299819,
        "step": 4279
    },
    {
        "loss": 1.7244,
        "grad_norm": 2.341101884841919,
        "learning_rate": 2.701084579938318e-05,
        "epoch": 0.5516885795308069,
        "step": 4280
    },
    {
        "loss": 2.0449,
        "grad_norm": 2.329972982406616,
        "learning_rate": 2.695681726983038e-05,
        "epoch": 0.5518174787316319,
        "step": 4281
    },
    {
        "loss": 2.1126,
        "grad_norm": 1.0539597272872925,
        "learning_rate": 2.6902822880484192e-05,
        "epoch": 0.5519463779324568,
        "step": 4282
    },
    {
        "loss": 1.1259,
        "grad_norm": 2.284327745437622,
        "learning_rate": 2.6848862711341477e-05,
        "epoch": 0.5520752771332817,
        "step": 4283
    },
    {
        "loss": 2.2099,
        "grad_norm": 2.1122119426727295,
        "learning_rate": 2.6794936842348372e-05,
        "epoch": 0.5522041763341067,
        "step": 4284
    },
    {
        "loss": 1.8962,
        "grad_norm": 2.1649515628814697,
        "learning_rate": 2.674104535339989e-05,
        "epoch": 0.5523330755349317,
        "step": 4285
    },
    {
        "loss": 2.1363,
        "grad_norm": 1.704883098602295,
        "learning_rate": 2.668718832434045e-05,
        "epoch": 0.5524619747357566,
        "step": 4286
    },
    {
        "loss": 2.0369,
        "grad_norm": 2.728858232498169,
        "learning_rate": 2.6633365834963354e-05,
        "epoch": 0.5525908739365816,
        "step": 4287
    },
    {
        "loss": 1.8371,
        "grad_norm": 1.7651145458221436,
        "learning_rate": 2.65795779650105e-05,
        "epoch": 0.5527197731374065,
        "step": 4288
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.1954736709594727,
        "learning_rate": 2.6525824794172736e-05,
        "epoch": 0.5528486723382315,
        "step": 4289
    },
    {
        "loss": 1.7289,
        "grad_norm": 2.963827133178711,
        "learning_rate": 2.6472106402089518e-05,
        "epoch": 0.5529775715390565,
        "step": 4290
    },
    {
        "loss": 1.3981,
        "grad_norm": 2.5841007232666016,
        "learning_rate": 2.641842286834851e-05,
        "epoch": 0.5531064707398814,
        "step": 4291
    },
    {
        "loss": 2.2615,
        "grad_norm": 2.1054294109344482,
        "learning_rate": 2.636477427248598e-05,
        "epoch": 0.5532353699407063,
        "step": 4292
    },
    {
        "loss": 2.2246,
        "grad_norm": 2.34997820854187,
        "learning_rate": 2.631116069398638e-05,
        "epoch": 0.5533642691415314,
        "step": 4293
    },
    {
        "loss": 2.6247,
        "grad_norm": 1.052735447883606,
        "learning_rate": 2.625758221228226e-05,
        "epoch": 0.5534931683423563,
        "step": 4294
    },
    {
        "loss": 2.3017,
        "grad_norm": 1.6187078952789307,
        "learning_rate": 2.6204038906754092e-05,
        "epoch": 0.5536220675431812,
        "step": 4295
    },
    {
        "loss": 1.9336,
        "grad_norm": 2.5153632164001465,
        "learning_rate": 2.6150530856730372e-05,
        "epoch": 0.5537509667440061,
        "step": 4296
    },
    {
        "loss": 2.3205,
        "grad_norm": 2.016669988632202,
        "learning_rate": 2.6097058141487306e-05,
        "epoch": 0.5538798659448312,
        "step": 4297
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.4492270946502686,
        "learning_rate": 2.604362084024866e-05,
        "epoch": 0.5540087651456561,
        "step": 4298
    },
    {
        "loss": 0.8388,
        "grad_norm": 3.1680543422698975,
        "learning_rate": 2.599021903218588e-05,
        "epoch": 0.554137664346481,
        "step": 4299
    },
    {
        "loss": 1.9298,
        "grad_norm": 1.9069782495498657,
        "learning_rate": 2.5936852796417776e-05,
        "epoch": 0.554266563547306,
        "step": 4300
    },
    {
        "loss": 1.9531,
        "grad_norm": 2.1506800651550293,
        "learning_rate": 2.5883522212010325e-05,
        "epoch": 0.554395462748131,
        "step": 4301
    },
    {
        "loss": 2.38,
        "grad_norm": 1.7277199029922485,
        "learning_rate": 2.5830227357976933e-05,
        "epoch": 0.5545243619489559,
        "step": 4302
    },
    {
        "loss": 2.2727,
        "grad_norm": 1.6666947603225708,
        "learning_rate": 2.5776968313277883e-05,
        "epoch": 0.5546532611497809,
        "step": 4303
    },
    {
        "loss": 2.0602,
        "grad_norm": 2.2816929817199707,
        "learning_rate": 2.5723745156820356e-05,
        "epoch": 0.5547821603506058,
        "step": 4304
    },
    {
        "loss": 1.2287,
        "grad_norm": 2.2568018436431885,
        "learning_rate": 2.5670557967458603e-05,
        "epoch": 0.5549110595514308,
        "step": 4305
    },
    {
        "loss": 2.3008,
        "grad_norm": 2.0319724082946777,
        "learning_rate": 2.5617406823993373e-05,
        "epoch": 0.5550399587522558,
        "step": 4306
    },
    {
        "loss": 2.1407,
        "grad_norm": 1.5217589139938354,
        "learning_rate": 2.5564291805172048e-05,
        "epoch": 0.5551688579530807,
        "step": 4307
    },
    {
        "loss": 1.9514,
        "grad_norm": 1.2159417867660522,
        "learning_rate": 2.551121298968857e-05,
        "epoch": 0.5552977571539056,
        "step": 4308
    },
    {
        "loss": 2.0002,
        "grad_norm": 2.911775588989258,
        "learning_rate": 2.545817045618322e-05,
        "epoch": 0.5554266563547307,
        "step": 4309
    },
    {
        "loss": 2.474,
        "grad_norm": 2.1752679347991943,
        "learning_rate": 2.540516428324239e-05,
        "epoch": 0.5555555555555556,
        "step": 4310
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.5021179914474487,
        "learning_rate": 2.5352194549398788e-05,
        "epoch": 0.5556844547563805,
        "step": 4311
    },
    {
        "loss": 1.3691,
        "grad_norm": 2.933849573135376,
        "learning_rate": 2.5299261333131062e-05,
        "epoch": 0.5558133539572054,
        "step": 4312
    },
    {
        "loss": 2.1147,
        "grad_norm": 1.5727051496505737,
        "learning_rate": 2.5246364712863712e-05,
        "epoch": 0.5559422531580305,
        "step": 4313
    },
    {
        "loss": 1.9522,
        "grad_norm": 2.877429485321045,
        "learning_rate": 2.519350476696707e-05,
        "epoch": 0.5560711523588554,
        "step": 4314
    },
    {
        "loss": 2.0351,
        "grad_norm": 1.77699875831604,
        "learning_rate": 2.5140681573757136e-05,
        "epoch": 0.5562000515596803,
        "step": 4315
    },
    {
        "loss": 2.3164,
        "grad_norm": 1.190734624862671,
        "learning_rate": 2.5087895211495406e-05,
        "epoch": 0.5563289507605053,
        "step": 4316
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.191840887069702,
        "learning_rate": 2.5035145758388845e-05,
        "epoch": 0.5564578499613302,
        "step": 4317
    },
    {
        "loss": 1.3051,
        "grad_norm": 3.255624532699585,
        "learning_rate": 2.4982433292589758e-05,
        "epoch": 0.5565867491621552,
        "step": 4318
    },
    {
        "loss": 1.9066,
        "grad_norm": 1.921916127204895,
        "learning_rate": 2.4929757892195628e-05,
        "epoch": 0.5567156483629802,
        "step": 4319
    },
    {
        "loss": 1.6592,
        "grad_norm": 2.60996413230896,
        "learning_rate": 2.4877119635248976e-05,
        "epoch": 0.5568445475638051,
        "step": 4320
    },
    {
        "loss": 2.1013,
        "grad_norm": 1.767583966255188,
        "learning_rate": 2.4824518599737367e-05,
        "epoch": 0.55697344676463,
        "step": 4321
    },
    {
        "loss": 2.0121,
        "grad_norm": 1.8094756603240967,
        "learning_rate": 2.4771954863593194e-05,
        "epoch": 0.557102345965455,
        "step": 4322
    },
    {
        "loss": 1.7194,
        "grad_norm": 2.033552646636963,
        "learning_rate": 2.4719428504693552e-05,
        "epoch": 0.55723124516628,
        "step": 4323
    },
    {
        "loss": 2.2557,
        "grad_norm": 1.2660313844680786,
        "learning_rate": 2.4666939600860205e-05,
        "epoch": 0.5573601443671049,
        "step": 4324
    },
    {
        "loss": 2.1167,
        "grad_norm": 2.0327751636505127,
        "learning_rate": 2.4614488229859433e-05,
        "epoch": 0.5574890435679298,
        "step": 4325
    },
    {
        "loss": 1.9503,
        "grad_norm": 2.8405821323394775,
        "learning_rate": 2.4562074469401836e-05,
        "epoch": 0.5576179427687549,
        "step": 4326
    },
    {
        "loss": 2.0701,
        "grad_norm": 2.1351544857025146,
        "learning_rate": 2.4509698397142363e-05,
        "epoch": 0.5577468419695798,
        "step": 4327
    },
    {
        "loss": 1.9093,
        "grad_norm": 2.02976393699646,
        "learning_rate": 2.4457360090680076e-05,
        "epoch": 0.5578757411704047,
        "step": 4328
    },
    {
        "loss": 1.9687,
        "grad_norm": 3.1346781253814697,
        "learning_rate": 2.4405059627558195e-05,
        "epoch": 0.5580046403712297,
        "step": 4329
    },
    {
        "loss": 1.9313,
        "grad_norm": 1.4703348875045776,
        "learning_rate": 2.4352797085263685e-05,
        "epoch": 0.5581335395720547,
        "step": 4330
    },
    {
        "loss": 1.9787,
        "grad_norm": 1.830944299697876,
        "learning_rate": 2.4300572541227445e-05,
        "epoch": 0.5582624387728796,
        "step": 4331
    },
    {
        "loss": 1.638,
        "grad_norm": 1.4752358198165894,
        "learning_rate": 2.424838607282414e-05,
        "epoch": 0.5583913379737045,
        "step": 4332
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.963051199913025,
        "learning_rate": 2.4196237757371852e-05,
        "epoch": 0.5585202371745295,
        "step": 4333
    },
    {
        "loss": 1.7762,
        "grad_norm": 3.0035507678985596,
        "learning_rate": 2.414412767213225e-05,
        "epoch": 0.5586491363753545,
        "step": 4334
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.9971572160720825,
        "learning_rate": 2.409205589431039e-05,
        "epoch": 0.5587780355761794,
        "step": 4335
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.146108388900757,
        "learning_rate": 2.4040022501054487e-05,
        "epoch": 0.5589069347770044,
        "step": 4336
    },
    {
        "loss": 2.4275,
        "grad_norm": 2.0812342166900635,
        "learning_rate": 2.398802756945587e-05,
        "epoch": 0.5590358339778293,
        "step": 4337
    },
    {
        "loss": 2.1963,
        "grad_norm": 1.5536731481552124,
        "learning_rate": 2.3936071176549073e-05,
        "epoch": 0.5591647331786543,
        "step": 4338
    },
    {
        "loss": 1.8304,
        "grad_norm": 2.0563299655914307,
        "learning_rate": 2.38841533993113e-05,
        "epoch": 0.5592936323794793,
        "step": 4339
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.9157782793045044,
        "learning_rate": 2.383227431466259e-05,
        "epoch": 0.5594225315803042,
        "step": 4340
    },
    {
        "loss": 2.1621,
        "grad_norm": 1.3820464611053467,
        "learning_rate": 2.3780433999465844e-05,
        "epoch": 0.5595514307811291,
        "step": 4341
    },
    {
        "loss": 1.7672,
        "grad_norm": 1.963509202003479,
        "learning_rate": 2.3728632530526302e-05,
        "epoch": 0.5596803299819542,
        "step": 4342
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.0081210136413574,
        "learning_rate": 2.3676869984591676e-05,
        "epoch": 0.5598092291827791,
        "step": 4343
    },
    {
        "loss": 1.5601,
        "grad_norm": 2.7895209789276123,
        "learning_rate": 2.362514643835216e-05,
        "epoch": 0.559938128383604,
        "step": 4344
    },
    {
        "loss": 1.2174,
        "grad_norm": 2.481773853302002,
        "learning_rate": 2.3573461968440115e-05,
        "epoch": 0.560067027584429,
        "step": 4345
    },
    {
        "loss": 1.7826,
        "grad_norm": 3.3472044467926025,
        "learning_rate": 2.3521816651429824e-05,
        "epoch": 0.560195926785254,
        "step": 4346
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.1606619358062744,
        "learning_rate": 2.347021056383778e-05,
        "epoch": 0.5603248259860789,
        "step": 4347
    },
    {
        "loss": 2.6078,
        "grad_norm": 1.2162576913833618,
        "learning_rate": 2.3418643782122357e-05,
        "epoch": 0.5604537251869038,
        "step": 4348
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.6435285806655884,
        "learning_rate": 2.3367116382683464e-05,
        "epoch": 0.5605826243877288,
        "step": 4349
    },
    {
        "loss": 2.0757,
        "grad_norm": 1.7183701992034912,
        "learning_rate": 2.331562844186289e-05,
        "epoch": 0.5607115235885538,
        "step": 4350
    },
    {
        "loss": 2.0694,
        "grad_norm": 2.3875083923339844,
        "learning_rate": 2.326418003594396e-05,
        "epoch": 0.5608404227893787,
        "step": 4351
    },
    {
        "loss": 2.5674,
        "grad_norm": 1.297179937362671,
        "learning_rate": 2.321277124115121e-05,
        "epoch": 0.5609693219902037,
        "step": 4352
    },
    {
        "loss": 2.179,
        "grad_norm": 2.2576546669006348,
        "learning_rate": 2.3161402133650707e-05,
        "epoch": 0.5610982211910286,
        "step": 4353
    },
    {
        "loss": 2.4358,
        "grad_norm": 2.9342987537384033,
        "learning_rate": 2.3110072789549715e-05,
        "epoch": 0.5612271203918535,
        "step": 4354
    },
    {
        "loss": 0.9505,
        "grad_norm": 2.531398057937622,
        "learning_rate": 2.3058783284896357e-05,
        "epoch": 0.5613560195926786,
        "step": 4355
    },
    {
        "loss": 1.6491,
        "grad_norm": 2.248241424560547,
        "learning_rate": 2.3007533695679977e-05,
        "epoch": 0.5614849187935035,
        "step": 4356
    },
    {
        "loss": 1.5098,
        "grad_norm": 2.638145685195923,
        "learning_rate": 2.295632409783077e-05,
        "epoch": 0.5616138179943284,
        "step": 4357
    },
    {
        "loss": 2.1005,
        "grad_norm": 2.0718705654144287,
        "learning_rate": 2.2905154567219435e-05,
        "epoch": 0.5617427171951533,
        "step": 4358
    },
    {
        "loss": 1.7588,
        "grad_norm": 2.643737554550171,
        "learning_rate": 2.285402517965754e-05,
        "epoch": 0.5618716163959784,
        "step": 4359
    },
    {
        "loss": 1.7978,
        "grad_norm": 2.508054733276367,
        "learning_rate": 2.28029360108972e-05,
        "epoch": 0.5620005155968033,
        "step": 4360
    },
    {
        "loss": 1.363,
        "grad_norm": 3.2494592666625977,
        "learning_rate": 2.275188713663081e-05,
        "epoch": 0.5621294147976282,
        "step": 4361
    },
    {
        "loss": 1.8831,
        "grad_norm": 2.530359983444214,
        "learning_rate": 2.270087863249102e-05,
        "epoch": 0.5622583139984532,
        "step": 4362
    },
    {
        "loss": 1.9954,
        "grad_norm": 2.5878145694732666,
        "learning_rate": 2.2649910574050897e-05,
        "epoch": 0.5623872131992782,
        "step": 4363
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.6989225149154663,
        "learning_rate": 2.259898303682339e-05,
        "epoch": 0.5625161124001031,
        "step": 4364
    },
    {
        "loss": 1.9232,
        "grad_norm": 2.571756601333618,
        "learning_rate": 2.2548096096261433e-05,
        "epoch": 0.5626450116009281,
        "step": 4365
    },
    {
        "loss": 1.4231,
        "grad_norm": 3.393648386001587,
        "learning_rate": 2.2497249827757916e-05,
        "epoch": 0.562773910801753,
        "step": 4366
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.852224588394165,
        "learning_rate": 2.2446444306645415e-05,
        "epoch": 0.562902810002578,
        "step": 4367
    },
    {
        "loss": 1.7812,
        "grad_norm": 2.2385025024414062,
        "learning_rate": 2.2395679608196042e-05,
        "epoch": 0.563031709203403,
        "step": 4368
    },
    {
        "loss": 2.0563,
        "grad_norm": 2.930511236190796,
        "learning_rate": 2.2344955807621582e-05,
        "epoch": 0.5631606084042279,
        "step": 4369
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.8431540727615356,
        "learning_rate": 2.2294272980073178e-05,
        "epoch": 0.5632895076050528,
        "step": 4370
    },
    {
        "loss": 2.0983,
        "grad_norm": 2.851229667663574,
        "learning_rate": 2.2243631200641156e-05,
        "epoch": 0.5634184068058778,
        "step": 4371
    },
    {
        "loss": 2.0969,
        "grad_norm": 2.2169554233551025,
        "learning_rate": 2.2193030544355188e-05,
        "epoch": 0.5635473060067028,
        "step": 4372
    },
    {
        "loss": 1.871,
        "grad_norm": 2.092700242996216,
        "learning_rate": 2.214247108618397e-05,
        "epoch": 0.5636762052075277,
        "step": 4373
    },
    {
        "loss": 1.7738,
        "grad_norm": 2.15053653717041,
        "learning_rate": 2.209195290103504e-05,
        "epoch": 0.5638051044083526,
        "step": 4374
    },
    {
        "loss": 1.7172,
        "grad_norm": 1.9835023880004883,
        "learning_rate": 2.2041476063754967e-05,
        "epoch": 0.5639340036091777,
        "step": 4375
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.2044894695281982,
        "learning_rate": 2.1991040649128957e-05,
        "epoch": 0.5640629028100026,
        "step": 4376
    },
    {
        "loss": 2.2928,
        "grad_norm": 2.044929265975952,
        "learning_rate": 2.194064673188089e-05,
        "epoch": 0.5641918020108275,
        "step": 4377
    },
    {
        "loss": 1.7746,
        "grad_norm": 2.722620964050293,
        "learning_rate": 2.1890294386673088e-05,
        "epoch": 0.5643207012116525,
        "step": 4378
    },
    {
        "loss": 2.0242,
        "grad_norm": 1.375476360321045,
        "learning_rate": 2.183998368810637e-05,
        "epoch": 0.5644496004124775,
        "step": 4379
    },
    {
        "loss": 1.8181,
        "grad_norm": 2.817197322845459,
        "learning_rate": 2.178971471071982e-05,
        "epoch": 0.5645784996133024,
        "step": 4380
    },
    {
        "loss": 1.9894,
        "grad_norm": 3.247413158416748,
        "learning_rate": 2.173948752899069e-05,
        "epoch": 0.5647073988141273,
        "step": 4381
    },
    {
        "loss": 1.917,
        "grad_norm": 2.127289056777954,
        "learning_rate": 2.1689302217334317e-05,
        "epoch": 0.5648362980149523,
        "step": 4382
    },
    {
        "loss": 1.3547,
        "grad_norm": 2.560180425643921,
        "learning_rate": 2.1639158850104048e-05,
        "epoch": 0.5649651972157773,
        "step": 4383
    },
    {
        "loss": 1.8851,
        "grad_norm": 2.9395620822906494,
        "learning_rate": 2.1589057501591002e-05,
        "epoch": 0.5650940964166022,
        "step": 4384
    },
    {
        "loss": 2.6116,
        "grad_norm": 1.7538472414016724,
        "learning_rate": 2.1538998246024117e-05,
        "epoch": 0.5652229956174272,
        "step": 4385
    },
    {
        "loss": 1.4454,
        "grad_norm": 2.551684856414795,
        "learning_rate": 2.1488981157569943e-05,
        "epoch": 0.5653518948182521,
        "step": 4386
    },
    {
        "loss": 1.6441,
        "grad_norm": 2.3548593521118164,
        "learning_rate": 2.143900631033256e-05,
        "epoch": 0.565480794019077,
        "step": 4387
    },
    {
        "loss": 1.7623,
        "grad_norm": 2.7960708141326904,
        "learning_rate": 2.1389073778353437e-05,
        "epoch": 0.5656096932199021,
        "step": 4388
    },
    {
        "loss": 2.1122,
        "grad_norm": 2.04809308052063,
        "learning_rate": 2.1339183635611383e-05,
        "epoch": 0.565738592420727,
        "step": 4389
    },
    {
        "loss": 2.0094,
        "grad_norm": 1.8379855155944824,
        "learning_rate": 2.1289335956022417e-05,
        "epoch": 0.5658674916215519,
        "step": 4390
    },
    {
        "loss": 2.1723,
        "grad_norm": 2.262629985809326,
        "learning_rate": 2.1239530813439577e-05,
        "epoch": 0.5659963908223768,
        "step": 4391
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.7079988718032837,
        "learning_rate": 2.118976828165295e-05,
        "epoch": 0.5661252900232019,
        "step": 4392
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.761033296585083,
        "learning_rate": 2.1140048434389465e-05,
        "epoch": 0.5662541892240268,
        "step": 4393
    },
    {
        "loss": 1.9334,
        "grad_norm": 2.187871217727661,
        "learning_rate": 2.1090371345312826e-05,
        "epoch": 0.5663830884248517,
        "step": 4394
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.168522596359253,
        "learning_rate": 2.1040737088023306e-05,
        "epoch": 0.5665119876256767,
        "step": 4395
    },
    {
        "loss": 1.344,
        "grad_norm": 2.854142189025879,
        "learning_rate": 2.099114573605791e-05,
        "epoch": 0.5666408868265017,
        "step": 4396
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.035348892211914,
        "learning_rate": 2.0941597362889858e-05,
        "epoch": 0.5667697860273266,
        "step": 4397
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.2144689559936523,
        "learning_rate": 2.0892092041928762e-05,
        "epoch": 0.5668986852281516,
        "step": 4398
    },
    {
        "loss": 2.3373,
        "grad_norm": 2.100991725921631,
        "learning_rate": 2.0842629846520596e-05,
        "epoch": 0.5670275844289765,
        "step": 4399
    },
    {
        "loss": 2.3221,
        "grad_norm": 1.6310378313064575,
        "learning_rate": 2.0793210849947225e-05,
        "epoch": 0.5671564836298015,
        "step": 4400
    },
    {
        "loss": 1.1449,
        "grad_norm": 2.773975133895874,
        "learning_rate": 2.074383512542658e-05,
        "epoch": 0.5672853828306265,
        "step": 4401
    },
    {
        "loss": 2.1125,
        "grad_norm": 2.014986515045166,
        "learning_rate": 2.069450274611262e-05,
        "epoch": 0.5674142820314514,
        "step": 4402
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.2482821941375732,
        "learning_rate": 2.0645213785094875e-05,
        "epoch": 0.5675431812322763,
        "step": 4403
    },
    {
        "loss": 2.2506,
        "grad_norm": 2.4639220237731934,
        "learning_rate": 2.0595968315398657e-05,
        "epoch": 0.5676720804331014,
        "step": 4404
    },
    {
        "loss": 2.2139,
        "grad_norm": 1.6080018281936646,
        "learning_rate": 2.0546766409984904e-05,
        "epoch": 0.5678009796339263,
        "step": 4405
    },
    {
        "loss": 1.0433,
        "grad_norm": 2.60886549949646,
        "learning_rate": 2.0497608141749843e-05,
        "epoch": 0.5679298788347512,
        "step": 4406
    },
    {
        "loss": 2.1459,
        "grad_norm": 1.8350001573562622,
        "learning_rate": 2.0448493583525158e-05,
        "epoch": 0.5680587780355761,
        "step": 4407
    },
    {
        "loss": 2.3512,
        "grad_norm": 2.349419593811035,
        "learning_rate": 2.0399422808077838e-05,
        "epoch": 0.5681876772364012,
        "step": 4408
    },
    {
        "loss": 1.9905,
        "grad_norm": 2.2085344791412354,
        "learning_rate": 2.035039588810983e-05,
        "epoch": 0.5683165764372261,
        "step": 4409
    },
    {
        "loss": 1.7884,
        "grad_norm": 1.4819456338882446,
        "learning_rate": 2.0301412896258214e-05,
        "epoch": 0.568445475638051,
        "step": 4410
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.5750259160995483,
        "learning_rate": 2.0252473905094998e-05,
        "epoch": 0.568574374838876,
        "step": 4411
    },
    {
        "loss": 2.2203,
        "grad_norm": 1.153080701828003,
        "learning_rate": 2.020357898712704e-05,
        "epoch": 0.568703274039701,
        "step": 4412
    },
    {
        "loss": 0.5776,
        "grad_norm": 2.7909598350524902,
        "learning_rate": 2.0154728214795702e-05,
        "epoch": 0.5688321732405259,
        "step": 4413
    },
    {
        "loss": 1.1771,
        "grad_norm": 2.7948217391967773,
        "learning_rate": 2.0105921660477133e-05,
        "epoch": 0.5689610724413509,
        "step": 4414
    },
    {
        "loss": 1.7575,
        "grad_norm": 2.336005210876465,
        "learning_rate": 2.005715939648199e-05,
        "epoch": 0.5690899716421758,
        "step": 4415
    },
    {
        "loss": 2.1712,
        "grad_norm": 2.015732526779175,
        "learning_rate": 2.000844149505508e-05,
        "epoch": 0.5692188708430008,
        "step": 4416
    },
    {
        "loss": 2.5144,
        "grad_norm": 1.5897886753082275,
        "learning_rate": 1.9959768028375696e-05,
        "epoch": 0.5693477700438258,
        "step": 4417
    },
    {
        "loss": 1.7861,
        "grad_norm": 2.552863836288452,
        "learning_rate": 1.9911139068557366e-05,
        "epoch": 0.5694766692446507,
        "step": 4418
    },
    {
        "loss": 0.9858,
        "grad_norm": 3.1100823879241943,
        "learning_rate": 1.986255468764734e-05,
        "epoch": 0.5696055684454756,
        "step": 4419
    },
    {
        "loss": 1.7608,
        "grad_norm": 3.1004929542541504,
        "learning_rate": 1.9814014957627104e-05,
        "epoch": 0.5697344676463006,
        "step": 4420
    },
    {
        "loss": 2.1202,
        "grad_norm": 1.63942551612854,
        "learning_rate": 1.9765519950412042e-05,
        "epoch": 0.5698633668471256,
        "step": 4421
    },
    {
        "loss": 1.7927,
        "grad_norm": 1.5203090906143188,
        "learning_rate": 1.971706973785094e-05,
        "epoch": 0.5699922660479505,
        "step": 4422
    },
    {
        "loss": 1.9998,
        "grad_norm": 1.7894150018692017,
        "learning_rate": 1.966866439172655e-05,
        "epoch": 0.5701211652487754,
        "step": 4423
    },
    {
        "loss": 1.7912,
        "grad_norm": 2.434908628463745,
        "learning_rate": 1.9620303983755094e-05,
        "epoch": 0.5702500644496004,
        "step": 4424
    },
    {
        "loss": 1.5201,
        "grad_norm": 3.036175012588501,
        "learning_rate": 1.957198858558597e-05,
        "epoch": 0.5703789636504254,
        "step": 4425
    },
    {
        "loss": 1.7716,
        "grad_norm": 2.5292928218841553,
        "learning_rate": 1.9523718268802182e-05,
        "epoch": 0.5705078628512503,
        "step": 4426
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.4786078929901123,
        "learning_rate": 1.947549310491984e-05,
        "epoch": 0.5706367620520753,
        "step": 4427
    },
    {
        "loss": 2.003,
        "grad_norm": 1.988129734992981,
        "learning_rate": 1.9427313165388155e-05,
        "epoch": 0.5707656612529002,
        "step": 4428
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.9307726621627808,
        "learning_rate": 1.9379178521589254e-05,
        "epoch": 0.5708945604537252,
        "step": 4429
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.8948755264282227,
        "learning_rate": 1.9331089244838284e-05,
        "epoch": 0.5710234596545501,
        "step": 4430
    },
    {
        "loss": 1.9452,
        "grad_norm": 1.885909914970398,
        "learning_rate": 1.9283045406383153e-05,
        "epoch": 0.5711523588553751,
        "step": 4431
    },
    {
        "loss": 2.172,
        "grad_norm": 2.161038875579834,
        "learning_rate": 1.9235047077404324e-05,
        "epoch": 0.5712812580562,
        "step": 4432
    },
    {
        "loss": 1.5362,
        "grad_norm": 2.4937198162078857,
        "learning_rate": 1.9187094329014998e-05,
        "epoch": 0.571410157257025,
        "step": 4433
    },
    {
        "loss": 2.0951,
        "grad_norm": 1.3915491104125977,
        "learning_rate": 1.913918723226079e-05,
        "epoch": 0.57153905645785,
        "step": 4434
    },
    {
        "loss": 1.9949,
        "grad_norm": 2.338745355606079,
        "learning_rate": 1.9091325858119578e-05,
        "epoch": 0.5716679556586749,
        "step": 4435
    },
    {
        "loss": 1.4512,
        "grad_norm": 2.636868953704834,
        "learning_rate": 1.9043510277501657e-05,
        "epoch": 0.5717968548594998,
        "step": 4436
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.569383144378662,
        "learning_rate": 1.899574056124942e-05,
        "epoch": 0.5719257540603249,
        "step": 4437
    },
    {
        "loss": 1.9829,
        "grad_norm": 2.7390425205230713,
        "learning_rate": 1.89480167801372e-05,
        "epoch": 0.5720546532611498,
        "step": 4438
    },
    {
        "loss": 1.579,
        "grad_norm": 2.5581414699554443,
        "learning_rate": 1.890033900487144e-05,
        "epoch": 0.5721835524619747,
        "step": 4439
    },
    {
        "loss": 2.2915,
        "grad_norm": 1.9525768756866455,
        "learning_rate": 1.8852707306090357e-05,
        "epoch": 0.5723124516627996,
        "step": 4440
    },
    {
        "loss": 1.5665,
        "grad_norm": 3.245140314102173,
        "learning_rate": 1.8805121754363814e-05,
        "epoch": 0.5724413508636247,
        "step": 4441
    },
    {
        "loss": 1.7703,
        "grad_norm": 3.0670464038848877,
        "learning_rate": 1.8757582420193437e-05,
        "epoch": 0.5725702500644496,
        "step": 4442
    },
    {
        "loss": 1.61,
        "grad_norm": 3.504070281982422,
        "learning_rate": 1.871008937401232e-05,
        "epoch": 0.5726991492652745,
        "step": 4443
    },
    {
        "loss": 1.0798,
        "grad_norm": 4.16860818862915,
        "learning_rate": 1.8662642686184966e-05,
        "epoch": 0.5728280484660995,
        "step": 4444
    },
    {
        "loss": 1.9905,
        "grad_norm": 1.9913839101791382,
        "learning_rate": 1.861524242700724e-05,
        "epoch": 0.5729569476669245,
        "step": 4445
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.6601297855377197,
        "learning_rate": 1.856788866670614e-05,
        "epoch": 0.5730858468677494,
        "step": 4446
    },
    {
        "loss": 2.309,
        "grad_norm": 1.839450716972351,
        "learning_rate": 1.8520581475439842e-05,
        "epoch": 0.5732147460685744,
        "step": 4447
    },
    {
        "loss": 1.4467,
        "grad_norm": 3.325385093688965,
        "learning_rate": 1.8473320923297527e-05,
        "epoch": 0.5733436452693993,
        "step": 4448
    },
    {
        "loss": 1.905,
        "grad_norm": 1.8586182594299316,
        "learning_rate": 1.842610708029921e-05,
        "epoch": 0.5734725444702243,
        "step": 4449
    },
    {
        "loss": 2.3454,
        "grad_norm": 1.5747828483581543,
        "learning_rate": 1.8378940016395773e-05,
        "epoch": 0.5736014436710493,
        "step": 4450
    },
    {
        "loss": 1.4118,
        "grad_norm": 1.9929980039596558,
        "learning_rate": 1.8331819801468763e-05,
        "epoch": 0.5737303428718742,
        "step": 4451
    },
    {
        "loss": 1.8441,
        "grad_norm": 1.7273914813995361,
        "learning_rate": 1.828474650533033e-05,
        "epoch": 0.5738592420726991,
        "step": 4452
    },
    {
        "loss": 2.2445,
        "grad_norm": 2.221235990524292,
        "learning_rate": 1.8237720197723075e-05,
        "epoch": 0.5739881412735242,
        "step": 4453
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.6382869482040405,
        "learning_rate": 1.8190740948320017e-05,
        "epoch": 0.5741170404743491,
        "step": 4454
    },
    {
        "loss": 1.5243,
        "grad_norm": 2.6322755813598633,
        "learning_rate": 1.8143808826724468e-05,
        "epoch": 0.574245939675174,
        "step": 4455
    },
    {
        "loss": 1.8899,
        "grad_norm": 1.871994972229004,
        "learning_rate": 1.8096923902469847e-05,
        "epoch": 0.5743748388759989,
        "step": 4456
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.0079505443572998,
        "learning_rate": 1.805008624501972e-05,
        "epoch": 0.574503738076824,
        "step": 4457
    },
    {
        "loss": 1.3867,
        "grad_norm": 2.8499209880828857,
        "learning_rate": 1.8003295923767604e-05,
        "epoch": 0.5746326372776489,
        "step": 4458
    },
    {
        "loss": 2.0192,
        "grad_norm": 1.727985143661499,
        "learning_rate": 1.795655300803682e-05,
        "epoch": 0.5747615364784738,
        "step": 4459
    },
    {
        "loss": 2.2841,
        "grad_norm": 1.361790418624878,
        "learning_rate": 1.7909857567080618e-05,
        "epoch": 0.5748904356792988,
        "step": 4460
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.1874666213989258,
        "learning_rate": 1.7863209670081703e-05,
        "epoch": 0.5750193348801237,
        "step": 4461
    },
    {
        "loss": 1.223,
        "grad_norm": 2.9051809310913086,
        "learning_rate": 1.781660938615246e-05,
        "epoch": 0.5751482340809487,
        "step": 4462
    },
    {
        "loss": 1.6991,
        "grad_norm": 2.617781639099121,
        "learning_rate": 1.7770056784334776e-05,
        "epoch": 0.5752771332817737,
        "step": 4463
    },
    {
        "loss": 1.3726,
        "grad_norm": 2.610536813735962,
        "learning_rate": 1.772355193359975e-05,
        "epoch": 0.5754060324825986,
        "step": 4464
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.4396162033081055,
        "learning_rate": 1.7677094902847818e-05,
        "epoch": 0.5755349316834235,
        "step": 4465
    },
    {
        "loss": 2.041,
        "grad_norm": 1.854093074798584,
        "learning_rate": 1.763068576090864e-05,
        "epoch": 0.5756638308842486,
        "step": 4466
    },
    {
        "loss": 1.9044,
        "grad_norm": 1.967939019203186,
        "learning_rate": 1.7584324576540757e-05,
        "epoch": 0.5757927300850735,
        "step": 4467
    },
    {
        "loss": 1.4642,
        "grad_norm": 2.555561065673828,
        "learning_rate": 1.7538011418431766e-05,
        "epoch": 0.5759216292858984,
        "step": 4468
    },
    {
        "loss": 2.2433,
        "grad_norm": 1.5947407484054565,
        "learning_rate": 1.7491746355198157e-05,
        "epoch": 0.5760505284867233,
        "step": 4469
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.539176344871521,
        "learning_rate": 1.744552945538501e-05,
        "epoch": 0.5761794276875484,
        "step": 4470
    },
    {
        "loss": 2.2798,
        "grad_norm": 1.8120689392089844,
        "learning_rate": 1.7399360787466167e-05,
        "epoch": 0.5763083268883733,
        "step": 4471
    },
    {
        "loss": 1.6474,
        "grad_norm": 2.038886547088623,
        "learning_rate": 1.735324041984403e-05,
        "epoch": 0.5764372260891982,
        "step": 4472
    },
    {
        "loss": 1.9324,
        "grad_norm": 2.0162289142608643,
        "learning_rate": 1.730716842084931e-05,
        "epoch": 0.5765661252900232,
        "step": 4473
    },
    {
        "loss": 1.4977,
        "grad_norm": 2.646467447280884,
        "learning_rate": 1.726114485874116e-05,
        "epoch": 0.5766950244908482,
        "step": 4474
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.111448049545288,
        "learning_rate": 1.7215169801706933e-05,
        "epoch": 0.5768239236916731,
        "step": 4475
    },
    {
        "loss": 1.7185,
        "grad_norm": 2.796560525894165,
        "learning_rate": 1.716924331786225e-05,
        "epoch": 0.576952822892498,
        "step": 4476
    },
    {
        "loss": 2.1877,
        "grad_norm": 2.1630537509918213,
        "learning_rate": 1.7123365475250468e-05,
        "epoch": 0.577081722093323,
        "step": 4477
    },
    {
        "loss": 2.1322,
        "grad_norm": 2.568714141845703,
        "learning_rate": 1.707753634184313e-05,
        "epoch": 0.577210621294148,
        "step": 4478
    },
    {
        "loss": 2.4165,
        "grad_norm": 1.451399564743042,
        "learning_rate": 1.7031755985539648e-05,
        "epoch": 0.577339520494973,
        "step": 4479
    },
    {
        "loss": 1.7425,
        "grad_norm": 2.366621971130371,
        "learning_rate": 1.6986024474166877e-05,
        "epoch": 0.5774684196957979,
        "step": 4480
    },
    {
        "loss": 2.2621,
        "grad_norm": 1.8668538331985474,
        "learning_rate": 1.694034187547956e-05,
        "epoch": 0.5775973188966228,
        "step": 4481
    },
    {
        "loss": 2.0688,
        "grad_norm": 1.7534056901931763,
        "learning_rate": 1.6894708257160014e-05,
        "epoch": 0.5777262180974478,
        "step": 4482
    },
    {
        "loss": 2.2794,
        "grad_norm": 1.6913807392120361,
        "learning_rate": 1.6849123686817676e-05,
        "epoch": 0.5778551172982728,
        "step": 4483
    },
    {
        "loss": 2.7073,
        "grad_norm": 2.1448769569396973,
        "learning_rate": 1.680358823198962e-05,
        "epoch": 0.5779840164990977,
        "step": 4484
    },
    {
        "loss": 2.114,
        "grad_norm": 2.248331069946289,
        "learning_rate": 1.675810196014012e-05,
        "epoch": 0.5781129156999226,
        "step": 4485
    },
    {
        "loss": 1.0878,
        "grad_norm": 2.690396785736084,
        "learning_rate": 1.6712664938660318e-05,
        "epoch": 0.5782418149007477,
        "step": 4486
    },
    {
        "loss": 0.8948,
        "grad_norm": 2.204592704772949,
        "learning_rate": 1.6667277234868693e-05,
        "epoch": 0.5783707141015726,
        "step": 4487
    },
    {
        "loss": 2.028,
        "grad_norm": 1.3147506713867188,
        "learning_rate": 1.662193891601059e-05,
        "epoch": 0.5784996133023975,
        "step": 4488
    },
    {
        "loss": 1.832,
        "grad_norm": 2.0942108631134033,
        "learning_rate": 1.6576650049257998e-05,
        "epoch": 0.5786285125032224,
        "step": 4489
    },
    {
        "loss": 1.9484,
        "grad_norm": 2.6208372116088867,
        "learning_rate": 1.653141070170985e-05,
        "epoch": 0.5787574117040475,
        "step": 4490
    },
    {
        "loss": 2.0359,
        "grad_norm": 2.8611862659454346,
        "learning_rate": 1.6486220940391695e-05,
        "epoch": 0.5788863109048724,
        "step": 4491
    },
    {
        "loss": 1.92,
        "grad_norm": 1.8938206434249878,
        "learning_rate": 1.6441080832255434e-05,
        "epoch": 0.5790152101056973,
        "step": 4492
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.1859354972839355,
        "learning_rate": 1.6395990444179598e-05,
        "epoch": 0.5791441093065223,
        "step": 4493
    },
    {
        "loss": 2.3377,
        "grad_norm": 1.6064127683639526,
        "learning_rate": 1.6350949842969e-05,
        "epoch": 0.5792730085073473,
        "step": 4494
    },
    {
        "loss": 0.8803,
        "grad_norm": 2.26987361907959,
        "learning_rate": 1.6305959095354684e-05,
        "epoch": 0.5794019077081722,
        "step": 4495
    },
    {
        "loss": 1.9511,
        "grad_norm": 1.3158204555511475,
        "learning_rate": 1.6261018267993745e-05,
        "epoch": 0.5795308069089972,
        "step": 4496
    },
    {
        "loss": 2.108,
        "grad_norm": 1.4470096826553345,
        "learning_rate": 1.6216127427469467e-05,
        "epoch": 0.5796597061098221,
        "step": 4497
    },
    {
        "loss": 2.3565,
        "grad_norm": 1.6421020030975342,
        "learning_rate": 1.6171286640291016e-05,
        "epoch": 0.579788605310647,
        "step": 4498
    },
    {
        "loss": 2.296,
        "grad_norm": 2.4520342350006104,
        "learning_rate": 1.61264959728933e-05,
        "epoch": 0.5799175045114721,
        "step": 4499
    },
    {
        "loss": 1.1803,
        "grad_norm": 2.571220874786377,
        "learning_rate": 1.6081755491637136e-05,
        "epoch": 0.580046403712297,
        "step": 4500
    },
    {
        "loss": 1.7956,
        "grad_norm": 2.703083038330078,
        "learning_rate": 1.6037065262808916e-05,
        "epoch": 0.5801753029131219,
        "step": 4501
    },
    {
        "loss": 1.3393,
        "grad_norm": 2.8671953678131104,
        "learning_rate": 1.5992425352620478e-05,
        "epoch": 0.5803042021139468,
        "step": 4502
    },
    {
        "loss": 1.8096,
        "grad_norm": 2.7107083797454834,
        "learning_rate": 1.5947835827209312e-05,
        "epoch": 0.5804331013147719,
        "step": 4503
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.4625604152679443,
        "learning_rate": 1.5903296752638125e-05,
        "epoch": 0.5805620005155968,
        "step": 4504
    },
    {
        "loss": 1.9265,
        "grad_norm": 2.5375375747680664,
        "learning_rate": 1.585880819489482e-05,
        "epoch": 0.5806908997164217,
        "step": 4505
    },
    {
        "loss": 2.2269,
        "grad_norm": 1.8764994144439697,
        "learning_rate": 1.581437021989265e-05,
        "epoch": 0.5808197989172467,
        "step": 4506
    },
    {
        "loss": 1.8912,
        "grad_norm": 1.2351566553115845,
        "learning_rate": 1.5769982893469764e-05,
        "epoch": 0.5809486981180717,
        "step": 4507
    },
    {
        "loss": 1.8652,
        "grad_norm": 2.6907119750976562,
        "learning_rate": 1.5725646281389288e-05,
        "epoch": 0.5810775973188966,
        "step": 4508
    },
    {
        "loss": 2.1529,
        "grad_norm": 1.574986457824707,
        "learning_rate": 1.5681360449339316e-05,
        "epoch": 0.5812064965197216,
        "step": 4509
    },
    {
        "loss": 2.0057,
        "grad_norm": 1.8905209302902222,
        "learning_rate": 1.5637125462932584e-05,
        "epoch": 0.5813353957205465,
        "step": 4510
    },
    {
        "loss": 2.114,
        "grad_norm": 2.4826712608337402,
        "learning_rate": 1.559294138770656e-05,
        "epoch": 0.5814642949213715,
        "step": 4511
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.0640835762023926,
        "learning_rate": 1.5548808289123273e-05,
        "epoch": 0.5815931941221965,
        "step": 4512
    },
    {
        "loss": 0.996,
        "grad_norm": 2.8941543102264404,
        "learning_rate": 1.550472623256925e-05,
        "epoch": 0.5817220933230214,
        "step": 4513
    },
    {
        "loss": 1.4714,
        "grad_norm": 2.44158935546875,
        "learning_rate": 1.5460695283355325e-05,
        "epoch": 0.5818509925238463,
        "step": 4514
    },
    {
        "loss": 1.2791,
        "grad_norm": 2.4983768463134766,
        "learning_rate": 1.5416715506716683e-05,
        "epoch": 0.5819798917246713,
        "step": 4515
    },
    {
        "loss": 2.1351,
        "grad_norm": 1.7112892866134644,
        "learning_rate": 1.537278696781268e-05,
        "epoch": 0.5821087909254963,
        "step": 4516
    },
    {
        "loss": 1.9792,
        "grad_norm": 2.0749316215515137,
        "learning_rate": 1.5328909731726714e-05,
        "epoch": 0.5822376901263212,
        "step": 4517
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.971123695373535,
        "learning_rate": 1.528508386346623e-05,
        "epoch": 0.5823665893271461,
        "step": 4518
    },
    {
        "loss": 2.3952,
        "grad_norm": 1.452095866203308,
        "learning_rate": 1.5241309427962535e-05,
        "epoch": 0.5824954885279712,
        "step": 4519
    },
    {
        "loss": 1.5824,
        "grad_norm": 2.7832586765289307,
        "learning_rate": 1.519758649007077e-05,
        "epoch": 0.5826243877287961,
        "step": 4520
    },
    {
        "loss": 0.9494,
        "grad_norm": 2.8018479347229004,
        "learning_rate": 1.5153915114569717e-05,
        "epoch": 0.582753286929621,
        "step": 4521
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.2235305309295654,
        "learning_rate": 1.5110295366161814e-05,
        "epoch": 0.582882186130446,
        "step": 4522
    },
    {
        "loss": 2.0601,
        "grad_norm": 2.329453706741333,
        "learning_rate": 1.5066727309473011e-05,
        "epoch": 0.583011085331271,
        "step": 4523
    },
    {
        "loss": 2.1357,
        "grad_norm": 1.4776962995529175,
        "learning_rate": 1.5023211009052635e-05,
        "epoch": 0.5831399845320959,
        "step": 4524
    },
    {
        "loss": 1.8291,
        "grad_norm": 2.8281052112579346,
        "learning_rate": 1.4979746529373356e-05,
        "epoch": 0.5832688837329209,
        "step": 4525
    },
    {
        "loss": 2.5281,
        "grad_norm": 1.1823850870132446,
        "learning_rate": 1.4936333934831065e-05,
        "epoch": 0.5833977829337458,
        "step": 4526
    },
    {
        "loss": 1.7469,
        "grad_norm": 2.8728604316711426,
        "learning_rate": 1.4892973289744844e-05,
        "epoch": 0.5835266821345708,
        "step": 4527
    },
    {
        "loss": 1.8278,
        "grad_norm": 2.0432181358337402,
        "learning_rate": 1.484966465835666e-05,
        "epoch": 0.5836555813353957,
        "step": 4528
    },
    {
        "loss": 2.423,
        "grad_norm": 1.8463475704193115,
        "learning_rate": 1.4806408104831526e-05,
        "epoch": 0.5837844805362207,
        "step": 4529
    },
    {
        "loss": 2.2162,
        "grad_norm": 1.4882957935333252,
        "learning_rate": 1.4763203693257339e-05,
        "epoch": 0.5839133797370456,
        "step": 4530
    },
    {
        "loss": 2.3716,
        "grad_norm": 1.9330414533615112,
        "learning_rate": 1.4720051487644599e-05,
        "epoch": 0.5840422789378706,
        "step": 4531
    },
    {
        "loss": 1.8667,
        "grad_norm": 2.4789395332336426,
        "learning_rate": 1.467695155192656e-05,
        "epoch": 0.5841711781386956,
        "step": 4532
    },
    {
        "loss": 1.4157,
        "grad_norm": 4.483364582061768,
        "learning_rate": 1.4633903949959077e-05,
        "epoch": 0.5843000773395205,
        "step": 4533
    },
    {
        "loss": 2.0548,
        "grad_norm": 1.8579721450805664,
        "learning_rate": 1.4590908745520333e-05,
        "epoch": 0.5844289765403454,
        "step": 4534
    },
    {
        "loss": 2.1125,
        "grad_norm": 2.289051055908203,
        "learning_rate": 1.4547966002310964e-05,
        "epoch": 0.5845578757411704,
        "step": 4535
    },
    {
        "loss": 2.2526,
        "grad_norm": 1.8520888090133667,
        "learning_rate": 1.4505075783953925e-05,
        "epoch": 0.5846867749419954,
        "step": 4536
    },
    {
        "loss": 2.0018,
        "grad_norm": 2.8221542835235596,
        "learning_rate": 1.4462238153994257e-05,
        "epoch": 0.5848156741428203,
        "step": 4537
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.326600432395935,
        "learning_rate": 1.44194531758991e-05,
        "epoch": 0.5849445733436452,
        "step": 4538
    },
    {
        "loss": 1.8764,
        "grad_norm": 2.3638617992401123,
        "learning_rate": 1.4376720913057679e-05,
        "epoch": 0.5850734725444702,
        "step": 4539
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.8782730102539062,
        "learning_rate": 1.4334041428781015e-05,
        "epoch": 0.5852023717452952,
        "step": 4540
    },
    {
        "loss": 1.5516,
        "grad_norm": 2.6704487800598145,
        "learning_rate": 1.4291414786301932e-05,
        "epoch": 0.5853312709461201,
        "step": 4541
    },
    {
        "loss": 1.7516,
        "grad_norm": 1.9585899114608765,
        "learning_rate": 1.4248841048775063e-05,
        "epoch": 0.5854601701469451,
        "step": 4542
    },
    {
        "loss": 2.0038,
        "grad_norm": 1.4364124536514282,
        "learning_rate": 1.4206320279276657e-05,
        "epoch": 0.58558906934777,
        "step": 4543
    },
    {
        "loss": 1.5065,
        "grad_norm": 2.858877658843994,
        "learning_rate": 1.4163852540804267e-05,
        "epoch": 0.585717968548595,
        "step": 4544
    },
    {
        "loss": 2.4394,
        "grad_norm": 1.3593268394470215,
        "learning_rate": 1.4121437896277145e-05,
        "epoch": 0.58584686774942,
        "step": 4545
    },
    {
        "loss": 1.2483,
        "grad_norm": 2.5784201622009277,
        "learning_rate": 1.4079076408535813e-05,
        "epoch": 0.5859757669502449,
        "step": 4546
    },
    {
        "loss": 1.6299,
        "grad_norm": 1.6554104089736938,
        "learning_rate": 1.4036768140341877e-05,
        "epoch": 0.5861046661510698,
        "step": 4547
    },
    {
        "loss": 1.5941,
        "grad_norm": 2.2252888679504395,
        "learning_rate": 1.3994513154378285e-05,
        "epoch": 0.5862335653518949,
        "step": 4548
    },
    {
        "loss": 2.1027,
        "grad_norm": 2.2683210372924805,
        "learning_rate": 1.3952311513249028e-05,
        "epoch": 0.5863624645527198,
        "step": 4549
    },
    {
        "loss": 1.8301,
        "grad_norm": 1.7905819416046143,
        "learning_rate": 1.3910163279478888e-05,
        "epoch": 0.5864913637535447,
        "step": 4550
    },
    {
        "loss": 2.0991,
        "grad_norm": 1.7955154180526733,
        "learning_rate": 1.3868068515513699e-05,
        "epoch": 0.5866202629543696,
        "step": 4551
    },
    {
        "loss": 2.4096,
        "grad_norm": 1.5101213455200195,
        "learning_rate": 1.3826027283720082e-05,
        "epoch": 0.5867491621551947,
        "step": 4552
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.336775541305542,
        "learning_rate": 1.3784039646385156e-05,
        "epoch": 0.5868780613560196,
        "step": 4553
    },
    {
        "loss": 1.8753,
        "grad_norm": 2.285797119140625,
        "learning_rate": 1.3742105665716821e-05,
        "epoch": 0.5870069605568445,
        "step": 4554
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.9416844844818115,
        "learning_rate": 1.3700225403843486e-05,
        "epoch": 0.5871358597576695,
        "step": 4555
    },
    {
        "loss": 1.3635,
        "grad_norm": 2.8391215801239014,
        "learning_rate": 1.365839892281378e-05,
        "epoch": 0.5872647589584945,
        "step": 4556
    },
    {
        "loss": 2.1508,
        "grad_norm": 1.8587167263031006,
        "learning_rate": 1.3616626284596845e-05,
        "epoch": 0.5873936581593194,
        "step": 4557
    },
    {
        "loss": 1.8332,
        "grad_norm": 2.2589540481567383,
        "learning_rate": 1.3574907551081984e-05,
        "epoch": 0.5875225573601444,
        "step": 4558
    },
    {
        "loss": 1.181,
        "grad_norm": 1.763024926185608,
        "learning_rate": 1.3533242784078664e-05,
        "epoch": 0.5876514565609693,
        "step": 4559
    },
    {
        "loss": 1.1244,
        "grad_norm": 2.961658477783203,
        "learning_rate": 1.3491632045316277e-05,
        "epoch": 0.5877803557617943,
        "step": 4560
    },
    {
        "loss": 2.394,
        "grad_norm": 1.1923551559448242,
        "learning_rate": 1.3450075396444355e-05,
        "epoch": 0.5879092549626193,
        "step": 4561
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.2873306274414062,
        "learning_rate": 1.3408572899032179e-05,
        "epoch": 0.5880381541634442,
        "step": 4562
    },
    {
        "loss": 2.1241,
        "grad_norm": 2.4814674854278564,
        "learning_rate": 1.3367124614568728e-05,
        "epoch": 0.5881670533642691,
        "step": 4563
    },
    {
        "loss": 1.8327,
        "grad_norm": 3.3421475887298584,
        "learning_rate": 1.3325730604462866e-05,
        "epoch": 0.5882959525650941,
        "step": 4564
    },
    {
        "loss": 2.0102,
        "grad_norm": 2.455883741378784,
        "learning_rate": 1.3284390930042895e-05,
        "epoch": 0.5884248517659191,
        "step": 4565
    },
    {
        "loss": 2.0348,
        "grad_norm": 2.2190675735473633,
        "learning_rate": 1.3243105652556575e-05,
        "epoch": 0.588553750966744,
        "step": 4566
    },
    {
        "loss": 2.0607,
        "grad_norm": 1.6418739557266235,
        "learning_rate": 1.3201874833171252e-05,
        "epoch": 0.5886826501675689,
        "step": 4567
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.3485945463180542,
        "learning_rate": 1.316069853297343e-05,
        "epoch": 0.5888115493683939,
        "step": 4568
    },
    {
        "loss": 2.0588,
        "grad_norm": 2.2759366035461426,
        "learning_rate": 1.3119576812968882e-05,
        "epoch": 0.5889404485692189,
        "step": 4569
    },
    {
        "loss": 1.9696,
        "grad_norm": 2.599728584289551,
        "learning_rate": 1.307850973408255e-05,
        "epoch": 0.5890693477700438,
        "step": 4570
    },
    {
        "loss": 2.1813,
        "grad_norm": 2.5071091651916504,
        "learning_rate": 1.3037497357158424e-05,
        "epoch": 0.5891982469708688,
        "step": 4571
    },
    {
        "loss": 1.9343,
        "grad_norm": 2.4113388061523438,
        "learning_rate": 1.299653974295934e-05,
        "epoch": 0.5893271461716937,
        "step": 4572
    },
    {
        "loss": 1.6705,
        "grad_norm": 2.7165334224700928,
        "learning_rate": 1.2955636952167144e-05,
        "epoch": 0.5894560453725187,
        "step": 4573
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.8607877492904663,
        "learning_rate": 1.2914789045382415e-05,
        "epoch": 0.5895849445733437,
        "step": 4574
    },
    {
        "loss": 2.1676,
        "grad_norm": 2.092369318008423,
        "learning_rate": 1.2873996083124312e-05,
        "epoch": 0.5897138437741686,
        "step": 4575
    },
    {
        "loss": 1.9261,
        "grad_norm": 2.0295329093933105,
        "learning_rate": 1.2833258125830744e-05,
        "epoch": 0.5898427429749935,
        "step": 4576
    },
    {
        "loss": 2.1156,
        "grad_norm": 1.8210529088974,
        "learning_rate": 1.2792575233858028e-05,
        "epoch": 0.5899716421758185,
        "step": 4577
    },
    {
        "loss": 1.9473,
        "grad_norm": 1.9539783000946045,
        "learning_rate": 1.275194746748095e-05,
        "epoch": 0.5901005413766435,
        "step": 4578
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.6517362594604492,
        "learning_rate": 1.2711374886892568e-05,
        "epoch": 0.5902294405774684,
        "step": 4579
    },
    {
        "loss": 1.6974,
        "grad_norm": 2.9360105991363525,
        "learning_rate": 1.2670857552204219e-05,
        "epoch": 0.5903583397782933,
        "step": 4580
    },
    {
        "loss": 1.5026,
        "grad_norm": 2.5230956077575684,
        "learning_rate": 1.2630395523445399e-05,
        "epoch": 0.5904872389791184,
        "step": 4581
    },
    {
        "loss": 2.2674,
        "grad_norm": 2.551403522491455,
        "learning_rate": 1.2589988860563611e-05,
        "epoch": 0.5906161381799433,
        "step": 4582
    },
    {
        "loss": 1.9954,
        "grad_norm": 3.8966100215911865,
        "learning_rate": 1.2549637623424376e-05,
        "epoch": 0.5907450373807682,
        "step": 4583
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.1948511600494385,
        "learning_rate": 1.2509341871811103e-05,
        "epoch": 0.5908739365815932,
        "step": 4584
    },
    {
        "loss": 1.6995,
        "grad_norm": 3.515641927719116,
        "learning_rate": 1.2469101665424948e-05,
        "epoch": 0.5910028357824182,
        "step": 4585
    },
    {
        "loss": 1.879,
        "grad_norm": 3.671332597732544,
        "learning_rate": 1.2428917063884804e-05,
        "epoch": 0.5911317349832431,
        "step": 4586
    },
    {
        "loss": 1.372,
        "grad_norm": 3.5428996086120605,
        "learning_rate": 1.238878812672719e-05,
        "epoch": 0.591260634184068,
        "step": 4587
    },
    {
        "loss": 1.3922,
        "grad_norm": 1.7692135572433472,
        "learning_rate": 1.234871491340615e-05,
        "epoch": 0.591389533384893,
        "step": 4588
    },
    {
        "loss": 2.3233,
        "grad_norm": 2.221165418624878,
        "learning_rate": 1.230869748329313e-05,
        "epoch": 0.591518432585718,
        "step": 4589
    },
    {
        "loss": 1.4619,
        "grad_norm": 3.279447555541992,
        "learning_rate": 1.2268735895676981e-05,
        "epoch": 0.5916473317865429,
        "step": 4590
    },
    {
        "loss": 2.3735,
        "grad_norm": 2.1090941429138184,
        "learning_rate": 1.2228830209763809e-05,
        "epoch": 0.5917762309873679,
        "step": 4591
    },
    {
        "loss": 2.2265,
        "grad_norm": 2.2116177082061768,
        "learning_rate": 1.2188980484676853e-05,
        "epoch": 0.5919051301881928,
        "step": 4592
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.6398802995681763,
        "learning_rate": 1.2149186779456478e-05,
        "epoch": 0.5920340293890178,
        "step": 4593
    },
    {
        "loss": 2.0126,
        "grad_norm": 2.5073494911193848,
        "learning_rate": 1.2109449153060114e-05,
        "epoch": 0.5921629285898428,
        "step": 4594
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.685526967048645,
        "learning_rate": 1.2069767664361958e-05,
        "epoch": 0.5922918277906677,
        "step": 4595
    },
    {
        "loss": 1.3408,
        "grad_norm": 2.8763484954833984,
        "learning_rate": 1.2030142372153131e-05,
        "epoch": 0.5924207269914926,
        "step": 4596
    },
    {
        "loss": 2.0963,
        "grad_norm": 1.2271778583526611,
        "learning_rate": 1.1990573335141531e-05,
        "epoch": 0.5925496261923177,
        "step": 4597
    },
    {
        "loss": 1.6756,
        "grad_norm": 2.4329636096954346,
        "learning_rate": 1.1951060611951632e-05,
        "epoch": 0.5926785253931426,
        "step": 4598
    },
    {
        "loss": 2.2789,
        "grad_norm": 1.7412105798721313,
        "learning_rate": 1.1911604261124447e-05,
        "epoch": 0.5928074245939675,
        "step": 4599
    },
    {
        "loss": 1.2345,
        "grad_norm": 2.3181004524230957,
        "learning_rate": 1.1872204341117643e-05,
        "epoch": 0.5929363237947924,
        "step": 4600
    },
    {
        "loss": 1.3054,
        "grad_norm": 2.784247875213623,
        "learning_rate": 1.1832860910305077e-05,
        "epoch": 0.5930652229956175,
        "step": 4601
    },
    {
        "loss": 2.2832,
        "grad_norm": 1.7409694194793701,
        "learning_rate": 1.1793574026976972e-05,
        "epoch": 0.5931941221964424,
        "step": 4602
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.327005624771118,
        "learning_rate": 1.1754343749339902e-05,
        "epoch": 0.5933230213972673,
        "step": 4603
    },
    {
        "loss": 1.418,
        "grad_norm": 2.722768545150757,
        "learning_rate": 1.1715170135516378e-05,
        "epoch": 0.5934519205980923,
        "step": 4604
    },
    {
        "loss": 2.0124,
        "grad_norm": 1.97892165184021,
        "learning_rate": 1.1676053243545076e-05,
        "epoch": 0.5935808197989172,
        "step": 4605
    },
    {
        "loss": 2.4531,
        "grad_norm": 2.1590583324432373,
        "learning_rate": 1.1636993131380658e-05,
        "epoch": 0.5937097189997422,
        "step": 4606
    },
    {
        "loss": 2.013,
        "grad_norm": 1.9602206945419312,
        "learning_rate": 1.1597989856893554e-05,
        "epoch": 0.5938386182005672,
        "step": 4607
    },
    {
        "loss": 1.1995,
        "grad_norm": 3.672057628631592,
        "learning_rate": 1.1559043477870053e-05,
        "epoch": 0.5939675174013921,
        "step": 4608
    },
    {
        "loss": 1.3951,
        "grad_norm": 2.8074936866760254,
        "learning_rate": 1.1520154052012161e-05,
        "epoch": 0.594096416602217,
        "step": 4609
    },
    {
        "loss": 1.4601,
        "grad_norm": 2.835050344467163,
        "learning_rate": 1.1481321636937537e-05,
        "epoch": 0.594225315803042,
        "step": 4610
    },
    {
        "loss": 2.0524,
        "grad_norm": 1.8160079717636108,
        "learning_rate": 1.1442546290179207e-05,
        "epoch": 0.594354215003867,
        "step": 4611
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.154477119445801,
        "learning_rate": 1.1403828069185823e-05,
        "epoch": 0.5944831142046919,
        "step": 4612
    },
    {
        "loss": 1.2952,
        "grad_norm": 1.252658724784851,
        "learning_rate": 1.1365167031321394e-05,
        "epoch": 0.5946120134055168,
        "step": 4613
    },
    {
        "loss": 1.6234,
        "grad_norm": 1.696546196937561,
        "learning_rate": 1.132656323386504e-05,
        "epoch": 0.5947409126063419,
        "step": 4614
    },
    {
        "loss": 2.2971,
        "grad_norm": 1.3374300003051758,
        "learning_rate": 1.1288016734011242e-05,
        "epoch": 0.5948698118071668,
        "step": 4615
    },
    {
        "loss": 2.1547,
        "grad_norm": 2.8823697566986084,
        "learning_rate": 1.124952758886959e-05,
        "epoch": 0.5949987110079917,
        "step": 4616
    },
    {
        "loss": 2.5107,
        "grad_norm": 1.4336856603622437,
        "learning_rate": 1.1211095855464543e-05,
        "epoch": 0.5951276102088167,
        "step": 4617
    },
    {
        "loss": 1.8404,
        "grad_norm": 1.969184160232544,
        "learning_rate": 1.1172721590735629e-05,
        "epoch": 0.5952565094096417,
        "step": 4618
    },
    {
        "loss": 2.1175,
        "grad_norm": 2.485267400741577,
        "learning_rate": 1.1134404851537317e-05,
        "epoch": 0.5953854086104666,
        "step": 4619
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.4790866374969482,
        "learning_rate": 1.1096145694638566e-05,
        "epoch": 0.5955143078112916,
        "step": 4620
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.4367213249206543,
        "learning_rate": 1.1057944176723262e-05,
        "epoch": 0.5956432070121165,
        "step": 4621
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.467299222946167,
        "learning_rate": 1.1019800354389914e-05,
        "epoch": 0.5957721062129415,
        "step": 4622
    },
    {
        "loss": 2.623,
        "grad_norm": 2.044830560684204,
        "learning_rate": 1.0981714284151312e-05,
        "epoch": 0.5959010054137664,
        "step": 4623
    },
    {
        "loss": 1.7375,
        "grad_norm": 2.3352389335632324,
        "learning_rate": 1.0943686022434884e-05,
        "epoch": 0.5960299046145914,
        "step": 4624
    },
    {
        "loss": 1.3759,
        "grad_norm": 2.2099947929382324,
        "learning_rate": 1.0905715625582413e-05,
        "epoch": 0.5961588038154163,
        "step": 4625
    },
    {
        "loss": 2.0316,
        "grad_norm": 1.4134348630905151,
        "learning_rate": 1.0867803149849853e-05,
        "epoch": 0.5962877030162413,
        "step": 4626
    },
    {
        "loss": 1.7447,
        "grad_norm": 1.6293500661849976,
        "learning_rate": 1.0829948651407362e-05,
        "epoch": 0.5964166022170663,
        "step": 4627
    },
    {
        "loss": 2.1294,
        "grad_norm": 2.5524277687072754,
        "learning_rate": 1.0792152186339261e-05,
        "epoch": 0.5965455014178912,
        "step": 4628
    },
    {
        "loss": 2.038,
        "grad_norm": 1.1777000427246094,
        "learning_rate": 1.0754413810643865e-05,
        "epoch": 0.5966744006187161,
        "step": 4629
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.105957508087158,
        "learning_rate": 1.0716733580233351e-05,
        "epoch": 0.5968032998195412,
        "step": 4630
    },
    {
        "loss": 1.9913,
        "grad_norm": 1.4483642578125,
        "learning_rate": 1.0679111550933862e-05,
        "epoch": 0.5969321990203661,
        "step": 4631
    },
    {
        "loss": 2.121,
        "grad_norm": 2.076953411102295,
        "learning_rate": 1.0641547778485273e-05,
        "epoch": 0.597061098221191,
        "step": 4632
    },
    {
        "loss": 1.6575,
        "grad_norm": 2.281888246536255,
        "learning_rate": 1.0604042318541074e-05,
        "epoch": 0.597189997422016,
        "step": 4633
    },
    {
        "loss": 1.8889,
        "grad_norm": 1.8323050737380981,
        "learning_rate": 1.0566595226668474e-05,
        "epoch": 0.597318896622841,
        "step": 4634
    },
    {
        "loss": 1.4606,
        "grad_norm": 3.5237669944763184,
        "learning_rate": 1.0529206558348165e-05,
        "epoch": 0.5974477958236659,
        "step": 4635
    },
    {
        "loss": 2.0799,
        "grad_norm": 1.8044182062149048,
        "learning_rate": 1.0491876368974202e-05,
        "epoch": 0.5975766950244908,
        "step": 4636
    },
    {
        "loss": 2.1102,
        "grad_norm": 2.5095739364624023,
        "learning_rate": 1.0454604713854115e-05,
        "epoch": 0.5977055942253158,
        "step": 4637
    },
    {
        "loss": 1.9553,
        "grad_norm": 2.110015630722046,
        "learning_rate": 1.0417391648208685e-05,
        "epoch": 0.5978344934261408,
        "step": 4638
    },
    {
        "loss": 2.408,
        "grad_norm": 1.7447888851165771,
        "learning_rate": 1.038023722717179e-05,
        "epoch": 0.5979633926269657,
        "step": 4639
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.362283945083618,
        "learning_rate": 1.0343141505790554e-05,
        "epoch": 0.5980922918277907,
        "step": 4640
    },
    {
        "loss": 2.1287,
        "grad_norm": 1.928360939025879,
        "learning_rate": 1.0306104539025058e-05,
        "epoch": 0.5982211910286156,
        "step": 4641
    },
    {
        "loss": 1.87,
        "grad_norm": 2.4226789474487305,
        "learning_rate": 1.0269126381748373e-05,
        "epoch": 0.5983500902294405,
        "step": 4642
    },
    {
        "loss": 1.92,
        "grad_norm": 1.9968090057373047,
        "learning_rate": 1.0232207088746382e-05,
        "epoch": 0.5984789894302656,
        "step": 4643
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.8794691562652588,
        "learning_rate": 1.0195346714717813e-05,
        "epoch": 0.5986078886310905,
        "step": 4644
    },
    {
        "loss": 1.9817,
        "grad_norm": 2.3208563327789307,
        "learning_rate": 1.0158545314274071e-05,
        "epoch": 0.5987367878319154,
        "step": 4645
    },
    {
        "loss": 2.3331,
        "grad_norm": 2.4714739322662354,
        "learning_rate": 1.0121802941939212e-05,
        "epoch": 0.5988656870327403,
        "step": 4646
    },
    {
        "loss": 2.5171,
        "grad_norm": 1.2821763753890991,
        "learning_rate": 1.0085119652149805e-05,
        "epoch": 0.5989945862335654,
        "step": 4647
    },
    {
        "loss": 1.9413,
        "grad_norm": 2.393018960952759,
        "learning_rate": 1.00484954992549e-05,
        "epoch": 0.5991234854343903,
        "step": 4648
    },
    {
        "loss": 1.6841,
        "grad_norm": 3.0396203994750977,
        "learning_rate": 1.0011930537515951e-05,
        "epoch": 0.5992523846352152,
        "step": 4649
    },
    {
        "loss": 1.5892,
        "grad_norm": 2.758002758026123,
        "learning_rate": 9.975424821106666e-06,
        "epoch": 0.5993812838360402,
        "step": 4650
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.1173170804977417,
        "learning_rate": 9.938978404113032e-06,
        "epoch": 0.5995101830368652,
        "step": 4651
    },
    {
        "loss": 1.5544,
        "grad_norm": 2.4593586921691895,
        "learning_rate": 9.902591340533152e-06,
        "epoch": 0.5996390822376901,
        "step": 4652
    },
    {
        "loss": 1.5342,
        "grad_norm": 2.739630699157715,
        "learning_rate": 9.866263684277177e-06,
        "epoch": 0.5997679814385151,
        "step": 4653
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.8976126909255981,
        "learning_rate": 9.82999548916727e-06,
        "epoch": 0.59989688063934,
        "step": 4654
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.8245391845703125,
        "learning_rate": 9.793786808937489e-06,
        "epoch": 0.600025779840165,
        "step": 4655
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.6344738006591797,
        "learning_rate": 9.757637697233723e-06,
        "epoch": 0.60015467904099,
        "step": 4656
    },
    {
        "loss": 2.5482,
        "grad_norm": 2.3061647415161133,
        "learning_rate": 9.721548207613557e-06,
        "epoch": 0.6002835782418149,
        "step": 4657
    },
    {
        "loss": 2.1546,
        "grad_norm": 1.4616581201553345,
        "learning_rate": 9.68551839354636e-06,
        "epoch": 0.6004124774426398,
        "step": 4658
    },
    {
        "loss": 1.834,
        "grad_norm": 2.113429546356201,
        "learning_rate": 9.649548308412954e-06,
        "epoch": 0.6005413766434649,
        "step": 4659
    },
    {
        "loss": 1.4739,
        "grad_norm": 2.5407209396362305,
        "learning_rate": 9.613638005505687e-06,
        "epoch": 0.6006702758442898,
        "step": 4660
    },
    {
        "loss": 2.4883,
        "grad_norm": 1.2652082443237305,
        "learning_rate": 9.577787538028465e-06,
        "epoch": 0.6007991750451147,
        "step": 4661
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.816784381866455,
        "learning_rate": 9.541996959096377e-06,
        "epoch": 0.6009280742459396,
        "step": 4662
    },
    {
        "loss": 1.5376,
        "grad_norm": 1.6799389123916626,
        "learning_rate": 9.506266321735862e-06,
        "epoch": 0.6010569734467647,
        "step": 4663
    },
    {
        "loss": 2.2929,
        "grad_norm": 2.2828729152679443,
        "learning_rate": 9.470595678884603e-06,
        "epoch": 0.6011858726475896,
        "step": 4664
    },
    {
        "loss": 1.4855,
        "grad_norm": 2.9556946754455566,
        "learning_rate": 9.434985083391262e-06,
        "epoch": 0.6013147718484145,
        "step": 4665
    },
    {
        "loss": 2.1044,
        "grad_norm": 2.3903396129608154,
        "learning_rate": 9.399434588015637e-06,
        "epoch": 0.6014436710492395,
        "step": 4666
    },
    {
        "loss": 1.242,
        "grad_norm": 3.213726282119751,
        "learning_rate": 9.363944245428502e-06,
        "epoch": 0.6015725702500645,
        "step": 4667
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.0866904258728027,
        "learning_rate": 9.3285141082114e-06,
        "epoch": 0.6017014694508894,
        "step": 4668
    },
    {
        "loss": 1.9834,
        "grad_norm": 2.0540716648101807,
        "learning_rate": 9.293144228856765e-06,
        "epoch": 0.6018303686517144,
        "step": 4669
    },
    {
        "loss": 2.1629,
        "grad_norm": 2.4149014949798584,
        "learning_rate": 9.257834659767772e-06,
        "epoch": 0.6019592678525393,
        "step": 4670
    },
    {
        "loss": 2.3598,
        "grad_norm": 1.711442232131958,
        "learning_rate": 9.222585453258143e-06,
        "epoch": 0.6020881670533643,
        "step": 4671
    },
    {
        "loss": 2.2946,
        "grad_norm": 1.5288935899734497,
        "learning_rate": 9.187396661552234e-06,
        "epoch": 0.6022170662541892,
        "step": 4672
    },
    {
        "loss": 2.4256,
        "grad_norm": 2.434706926345825,
        "learning_rate": 9.152268336784941e-06,
        "epoch": 0.6023459654550142,
        "step": 4673
    },
    {
        "loss": 1.6111,
        "grad_norm": 2.8361315727233887,
        "learning_rate": 9.117200531001446e-06,
        "epoch": 0.6024748646558391,
        "step": 4674
    },
    {
        "loss": 1.5841,
        "grad_norm": 3.43733286857605,
        "learning_rate": 9.08219329615736e-06,
        "epoch": 0.6026037638566641,
        "step": 4675
    },
    {
        "loss": 1.9724,
        "grad_norm": 2.312143564224243,
        "learning_rate": 9.047246684118521e-06,
        "epoch": 0.6027326630574891,
        "step": 4676
    },
    {
        "loss": 1.1966,
        "grad_norm": 2.4670400619506836,
        "learning_rate": 9.012360746661058e-06,
        "epoch": 0.602861562258314,
        "step": 4677
    },
    {
        "loss": 2.1141,
        "grad_norm": 1.9972642660140991,
        "learning_rate": 8.977535535470965e-06,
        "epoch": 0.6029904614591389,
        "step": 4678
    },
    {
        "loss": 2.4896,
        "grad_norm": 1.5905812978744507,
        "learning_rate": 8.942771102144465e-06,
        "epoch": 0.6031193606599639,
        "step": 4679
    },
    {
        "loss": 2.3023,
        "grad_norm": 2.593214750289917,
        "learning_rate": 8.908067498187761e-06,
        "epoch": 0.6032482598607889,
        "step": 4680
    },
    {
        "loss": 1.6025,
        "grad_norm": 2.21441912651062,
        "learning_rate": 8.873424775016715e-06,
        "epoch": 0.6033771590616138,
        "step": 4681
    },
    {
        "loss": 2.0401,
        "grad_norm": 1.9610965251922607,
        "learning_rate": 8.838842983957169e-06,
        "epoch": 0.6035060582624387,
        "step": 4682
    },
    {
        "loss": 2.5216,
        "grad_norm": 1.5307867527008057,
        "learning_rate": 8.804322176244739e-06,
        "epoch": 0.6036349574632637,
        "step": 4683
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.6279948949813843,
        "learning_rate": 8.769862403024443e-06,
        "epoch": 0.6037638566640887,
        "step": 4684
    },
    {
        "loss": 1.6508,
        "grad_norm": 3.4782161712646484,
        "learning_rate": 8.735463715351111e-06,
        "epoch": 0.6038927558649136,
        "step": 4685
    },
    {
        "loss": 1.4504,
        "grad_norm": 3.6399004459381104,
        "learning_rate": 8.70112616418904e-06,
        "epoch": 0.6040216550657386,
        "step": 4686
    },
    {
        "loss": 2.1803,
        "grad_norm": 1.552825927734375,
        "learning_rate": 8.666849800411748e-06,
        "epoch": 0.6041505542665635,
        "step": 4687
    },
    {
        "loss": 1.3287,
        "grad_norm": 2.8343894481658936,
        "learning_rate": 8.632634674802325e-06,
        "epoch": 0.6042794534673885,
        "step": 4688
    },
    {
        "loss": 2.38,
        "grad_norm": 1.2918627262115479,
        "learning_rate": 8.598480838053103e-06,
        "epoch": 0.6044083526682135,
        "step": 4689
    },
    {
        "loss": 1.6406,
        "grad_norm": 1.6300138235092163,
        "learning_rate": 8.56438834076545e-06,
        "epoch": 0.6045372518690384,
        "step": 4690
    },
    {
        "loss": 1.7373,
        "grad_norm": 1.7006685733795166,
        "learning_rate": 8.53035723345001e-06,
        "epoch": 0.6046661510698633,
        "step": 4691
    },
    {
        "loss": 2.1842,
        "grad_norm": 2.674180746078491,
        "learning_rate": 8.496387566526443e-06,
        "epoch": 0.6047950502706884,
        "step": 4692
    },
    {
        "loss": 2.1244,
        "grad_norm": 2.4113690853118896,
        "learning_rate": 8.462479390323368e-06,
        "epoch": 0.6049239494715133,
        "step": 4693
    },
    {
        "loss": 1.8957,
        "grad_norm": 2.6330502033233643,
        "learning_rate": 8.428632755078243e-06,
        "epoch": 0.6050528486723382,
        "step": 4694
    },
    {
        "loss": 1.7776,
        "grad_norm": 2.1403229236602783,
        "learning_rate": 8.394847710937448e-06,
        "epoch": 0.6051817478731631,
        "step": 4695
    },
    {
        "loss": 1.1922,
        "grad_norm": 3.2992725372314453,
        "learning_rate": 8.361124307956075e-06,
        "epoch": 0.6053106470739882,
        "step": 4696
    },
    {
        "loss": 1.6157,
        "grad_norm": 2.588073492050171,
        "learning_rate": 8.327462596097818e-06,
        "epoch": 0.6054395462748131,
        "step": 4697
    },
    {
        "loss": 2.3032,
        "grad_norm": 2.253220796585083,
        "learning_rate": 8.293862625235083e-06,
        "epoch": 0.605568445475638,
        "step": 4698
    },
    {
        "loss": 1.7807,
        "grad_norm": 1.7065972089767456,
        "learning_rate": 8.260324445148765e-06,
        "epoch": 0.605697344676463,
        "step": 4699
    },
    {
        "loss": 2.0461,
        "grad_norm": 2.349855422973633,
        "learning_rate": 8.226848105528134e-06,
        "epoch": 0.605826243877288,
        "step": 4700
    },
    {
        "loss": 1.7722,
        "grad_norm": 1.7995274066925049,
        "learning_rate": 8.19343365597095e-06,
        "epoch": 0.6059551430781129,
        "step": 4701
    },
    {
        "loss": 1.752,
        "grad_norm": 1.9956234693527222,
        "learning_rate": 8.16008114598325e-06,
        "epoch": 0.6060840422789379,
        "step": 4702
    },
    {
        "loss": 1.2323,
        "grad_norm": 2.8327713012695312,
        "learning_rate": 8.126790624979202e-06,
        "epoch": 0.6062129414797628,
        "step": 4703
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.561887264251709,
        "learning_rate": 8.093562142281307e-06,
        "epoch": 0.6063418406805878,
        "step": 4704
    },
    {
        "loss": 2.3445,
        "grad_norm": 1.9496393203735352,
        "learning_rate": 8.06039574712003e-06,
        "epoch": 0.6064707398814128,
        "step": 4705
    },
    {
        "loss": 2.3531,
        "grad_norm": 2.071463108062744,
        "learning_rate": 8.027291488633821e-06,
        "epoch": 0.6065996390822377,
        "step": 4706
    },
    {
        "loss": 1.5608,
        "grad_norm": 2.4775869846343994,
        "learning_rate": 7.994249415869209e-06,
        "epoch": 0.6067285382830626,
        "step": 4707
    },
    {
        "loss": 2.0067,
        "grad_norm": 1.5597970485687256,
        "learning_rate": 7.961269577780457e-06,
        "epoch": 0.6068574374838877,
        "step": 4708
    },
    {
        "loss": 1.9896,
        "grad_norm": 1.3851531744003296,
        "learning_rate": 7.928352023229684e-06,
        "epoch": 0.6069863366847126,
        "step": 4709
    },
    {
        "loss": 2.1207,
        "grad_norm": 1.3331373929977417,
        "learning_rate": 7.89549680098674e-06,
        "epoch": 0.6071152358855375,
        "step": 4710
    },
    {
        "loss": 2.4422,
        "grad_norm": 1.9076796770095825,
        "learning_rate": 7.862703959729078e-06,
        "epoch": 0.6072441350863624,
        "step": 4711
    },
    {
        "loss": 2.1335,
        "grad_norm": 2.2554469108581543,
        "learning_rate": 7.82997354804177e-06,
        "epoch": 0.6073730342871875,
        "step": 4712
    },
    {
        "loss": 1.9758,
        "grad_norm": 2.9272148609161377,
        "learning_rate": 7.797305614417372e-06,
        "epoch": 0.6075019334880124,
        "step": 4713
    },
    {
        "loss": 2.1058,
        "grad_norm": 2.18910551071167,
        "learning_rate": 7.764700207255903e-06,
        "epoch": 0.6076308326888373,
        "step": 4714
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.060251474380493,
        "learning_rate": 7.732157374864701e-06,
        "epoch": 0.6077597318896623,
        "step": 4715
    },
    {
        "loss": 2.5781,
        "grad_norm": 2.0056686401367188,
        "learning_rate": 7.699677165458419e-06,
        "epoch": 0.6078886310904872,
        "step": 4716
    },
    {
        "loss": 1.6348,
        "grad_norm": 2.833216905593872,
        "learning_rate": 7.667259627158952e-06,
        "epoch": 0.6080175302913122,
        "step": 4717
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.527442216873169,
        "learning_rate": 7.634904807995285e-06,
        "epoch": 0.6081464294921372,
        "step": 4718
    },
    {
        "loss": 2.5233,
        "grad_norm": 1.9997690916061401,
        "learning_rate": 7.602612755903537e-06,
        "epoch": 0.6082753286929621,
        "step": 4719
    },
    {
        "loss": 1.7896,
        "grad_norm": 2.051985502243042,
        "learning_rate": 7.5703835187268094e-06,
        "epoch": 0.608404227893787,
        "step": 4720
    },
    {
        "loss": 1.7401,
        "grad_norm": 1.462344765663147,
        "learning_rate": 7.5382171442151206e-06,
        "epoch": 0.608533127094612,
        "step": 4721
    },
    {
        "loss": 1.862,
        "grad_norm": 1.7815313339233398,
        "learning_rate": 7.5061136800253895e-06,
        "epoch": 0.608662026295437,
        "step": 4722
    },
    {
        "loss": 1.5374,
        "grad_norm": 2.5623414516448975,
        "learning_rate": 7.474073173721297e-06,
        "epoch": 0.6087909254962619,
        "step": 4723
    },
    {
        "loss": 1.7091,
        "grad_norm": 2.6477699279785156,
        "learning_rate": 7.442095672773264e-06,
        "epoch": 0.6089198246970868,
        "step": 4724
    },
    {
        "loss": 1.7391,
        "grad_norm": 2.367347478866577,
        "learning_rate": 7.410181224558393e-06,
        "epoch": 0.6090487238979119,
        "step": 4725
    },
    {
        "loss": 1.7852,
        "grad_norm": 2.8384597301483154,
        "learning_rate": 7.37832987636029e-06,
        "epoch": 0.6091776230987368,
        "step": 4726
    },
    {
        "loss": 2.442,
        "grad_norm": 2.9972848892211914,
        "learning_rate": 7.346541675369129e-06,
        "epoch": 0.6093065222995617,
        "step": 4727
    },
    {
        "loss": 0.9965,
        "grad_norm": 3.5156118869781494,
        "learning_rate": 7.314816668681562e-06,
        "epoch": 0.6094354215003867,
        "step": 4728
    },
    {
        "loss": 2.2897,
        "grad_norm": 2.0344491004943848,
        "learning_rate": 7.283154903300515e-06,
        "epoch": 0.6095643207012117,
        "step": 4729
    },
    {
        "loss": 2.0403,
        "grad_norm": 2.4685862064361572,
        "learning_rate": 7.251556426135281e-06,
        "epoch": 0.6096932199020366,
        "step": 4730
    },
    {
        "loss": 2.2326,
        "grad_norm": 2.4919776916503906,
        "learning_rate": 7.220021284001432e-06,
        "epoch": 0.6098221191028615,
        "step": 4731
    },
    {
        "loss": 1.8372,
        "grad_norm": 1.424808382987976,
        "learning_rate": 7.1885495236205855e-06,
        "epoch": 0.6099510183036865,
        "step": 4732
    },
    {
        "loss": 1.952,
        "grad_norm": 2.3884053230285645,
        "learning_rate": 7.157141191620537e-06,
        "epoch": 0.6100799175045115,
        "step": 4733
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.197401523590088,
        "learning_rate": 7.125796334535128e-06,
        "epoch": 0.6102088167053364,
        "step": 4734
    },
    {
        "loss": 2.4299,
        "grad_norm": 2.054260730743408,
        "learning_rate": 7.09451499880407e-06,
        "epoch": 0.6103377159061614,
        "step": 4735
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.7456581592559814,
        "learning_rate": 7.063297230773019e-06,
        "epoch": 0.6104666151069863,
        "step": 4736
    },
    {
        "loss": 1.527,
        "grad_norm": 2.4819369316101074,
        "learning_rate": 7.032143076693493e-06,
        "epoch": 0.6105955143078113,
        "step": 4737
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.5032203197479248,
        "learning_rate": 7.001052582722656e-06,
        "epoch": 0.6107244135086363,
        "step": 4738
    },
    {
        "loss": 2.0963,
        "grad_norm": 2.1243600845336914,
        "learning_rate": 6.970025794923418e-06,
        "epoch": 0.6108533127094612,
        "step": 4739
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.7794142961502075,
        "learning_rate": 6.939062759264303e-06,
        "epoch": 0.6109822119102861,
        "step": 4740
    },
    {
        "loss": 2.0614,
        "grad_norm": 2.290562152862549,
        "learning_rate": 6.908163521619448e-06,
        "epoch": 0.6111111111111112,
        "step": 4741
    },
    {
        "loss": 1.999,
        "grad_norm": 2.0590991973876953,
        "learning_rate": 6.8773281277682625e-06,
        "epoch": 0.6112400103119361,
        "step": 4742
    },
    {
        "loss": 2.41,
        "grad_norm": 1.3629230260849,
        "learning_rate": 6.8465566233957724e-06,
        "epoch": 0.611368909512761,
        "step": 4743
    },
    {
        "loss": 1.7791,
        "grad_norm": 2.7983291149139404,
        "learning_rate": 6.815849054092333e-06,
        "epoch": 0.6114978087135859,
        "step": 4744
    },
    {
        "loss": 1.6997,
        "grad_norm": 2.842808246612549,
        "learning_rate": 6.785205465353378e-06,
        "epoch": 0.611626707914411,
        "step": 4745
    },
    {
        "loss": 1.8649,
        "grad_norm": 2.7382631301879883,
        "learning_rate": 6.75462590257977e-06,
        "epoch": 0.6117556071152359,
        "step": 4746
    },
    {
        "loss": 2.1735,
        "grad_norm": 2.249692678451538,
        "learning_rate": 6.7241104110774666e-06,
        "epoch": 0.6118845063160608,
        "step": 4747
    },
    {
        "loss": 1.6219,
        "grad_norm": 2.5126585960388184,
        "learning_rate": 6.6936590360573645e-06,
        "epoch": 0.6120134055168858,
        "step": 4748
    },
    {
        "loss": 1.7458,
        "grad_norm": 2.7417426109313965,
        "learning_rate": 6.663271822635514e-06,
        "epoch": 0.6121423047177108,
        "step": 4749
    },
    {
        "loss": 1.9833,
        "grad_norm": 2.4915966987609863,
        "learning_rate": 6.632948815832901e-06,
        "epoch": 0.6122712039185357,
        "step": 4750
    },
    {
        "loss": 1.6624,
        "grad_norm": 1.7667515277862549,
        "learning_rate": 6.602690060575245e-06,
        "epoch": 0.6124001031193607,
        "step": 4751
    },
    {
        "loss": 1.9653,
        "grad_norm": 2.0400116443634033,
        "learning_rate": 6.572495601693212e-06,
        "epoch": 0.6125290023201856,
        "step": 4752
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.3596550226211548,
        "learning_rate": 6.5423654839222085e-06,
        "epoch": 0.6126579015210105,
        "step": 4753
    },
    {
        "loss": 1.1314,
        "grad_norm": 2.959134101867676,
        "learning_rate": 6.512299751902173e-06,
        "epoch": 0.6127868007218356,
        "step": 4754
    },
    {
        "loss": 0.7242,
        "grad_norm": 2.090984344482422,
        "learning_rate": 6.482298450177798e-06,
        "epoch": 0.6129156999226605,
        "step": 4755
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.965351939201355,
        "learning_rate": 6.452361623198305e-06,
        "epoch": 0.6130445991234854,
        "step": 4756
    },
    {
        "loss": 2.3085,
        "grad_norm": 2.0141897201538086,
        "learning_rate": 6.422489315317271e-06,
        "epoch": 0.6131734983243103,
        "step": 4757
    },
    {
        "loss": 1.6317,
        "grad_norm": 2.621960401535034,
        "learning_rate": 6.392681570792797e-06,
        "epoch": 0.6133023975251354,
        "step": 4758
    },
    {
        "loss": 2.3363,
        "grad_norm": 2.08227801322937,
        "learning_rate": 6.362938433787324e-06,
        "epoch": 0.6134312967259603,
        "step": 4759
    },
    {
        "loss": 2.2621,
        "grad_norm": 1.8148674964904785,
        "learning_rate": 6.333259948367543e-06,
        "epoch": 0.6135601959267852,
        "step": 4760
    },
    {
        "loss": 2.1112,
        "grad_norm": 1.5954489707946777,
        "learning_rate": 6.303646158504295e-06,
        "epoch": 0.6136890951276102,
        "step": 4761
    },
    {
        "loss": 1.877,
        "grad_norm": 2.3303871154785156,
        "learning_rate": 6.2740971080727185e-06,
        "epoch": 0.6138179943284352,
        "step": 4762
    },
    {
        "loss": 2.4655,
        "grad_norm": 1.5889992713928223,
        "learning_rate": 6.2446128408519215e-06,
        "epoch": 0.6139468935292601,
        "step": 4763
    },
    {
        "loss": 0.8424,
        "grad_norm": 3.343313217163086,
        "learning_rate": 6.215193400525016e-06,
        "epoch": 0.6140757927300851,
        "step": 4764
    },
    {
        "loss": 1.4882,
        "grad_norm": 3.329529047012329,
        "learning_rate": 6.1858388306791855e-06,
        "epoch": 0.61420469193091,
        "step": 4765
    },
    {
        "loss": 1.7171,
        "grad_norm": 2.45888352394104,
        "learning_rate": 6.156549174805404e-06,
        "epoch": 0.614333591131735,
        "step": 4766
    },
    {
        "loss": 2.0604,
        "grad_norm": 2.1882569789886475,
        "learning_rate": 6.127324476298451e-06,
        "epoch": 0.61446249033256,
        "step": 4767
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.658344030380249,
        "learning_rate": 6.098164778456988e-06,
        "epoch": 0.6145913895333849,
        "step": 4768
    },
    {
        "loss": 0.8274,
        "grad_norm": 3.7806010246276855,
        "learning_rate": 6.069070124483262e-06,
        "epoch": 0.6147202887342098,
        "step": 4769
    },
    {
        "loss": 1.9182,
        "grad_norm": 2.7931458950042725,
        "learning_rate": 6.040040557483173e-06,
        "epoch": 0.6148491879350348,
        "step": 4770
    },
    {
        "loss": 1.7265,
        "grad_norm": 2.53828501701355,
        "learning_rate": 6.0110761204662354e-06,
        "epoch": 0.6149780871358598,
        "step": 4771
    },
    {
        "loss": 2.169,
        "grad_norm": 2.0041840076446533,
        "learning_rate": 5.982176856345445e-06,
        "epoch": 0.6151069863366847,
        "step": 4772
    },
    {
        "loss": 1.8936,
        "grad_norm": 2.1840269565582275,
        "learning_rate": 5.9533428079371846e-06,
        "epoch": 0.6152358855375096,
        "step": 4773
    },
    {
        "loss": 1.845,
        "grad_norm": 3.548659086227417,
        "learning_rate": 5.92457401796131e-06,
        "epoch": 0.6153647847383347,
        "step": 4774
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.5426268577575684,
        "learning_rate": 5.895870529040937e-06,
        "epoch": 0.6154936839391596,
        "step": 4775
    },
    {
        "loss": 2.4913,
        "grad_norm": 1.3553054332733154,
        "learning_rate": 5.8672323837024255e-06,
        "epoch": 0.6156225831399845,
        "step": 4776
    },
    {
        "loss": 1.4746,
        "grad_norm": 2.3939483165740967,
        "learning_rate": 5.838659624375348e-06,
        "epoch": 0.6157514823408095,
        "step": 4777
    },
    {
        "loss": 1.781,
        "grad_norm": 2.639540910720825,
        "learning_rate": 5.810152293392407e-06,
        "epoch": 0.6158803815416345,
        "step": 4778
    },
    {
        "loss": 1.1114,
        "grad_norm": 3.385193347930908,
        "learning_rate": 5.781710432989334e-06,
        "epoch": 0.6160092807424594,
        "step": 4779
    },
    {
        "loss": 1.918,
        "grad_norm": 2.9848644733428955,
        "learning_rate": 5.75333408530489e-06,
        "epoch": 0.6161381799432843,
        "step": 4780
    },
    {
        "loss": 1.7818,
        "grad_norm": 2.305386781692505,
        "learning_rate": 5.725023292380771e-06,
        "epoch": 0.6162670791441093,
        "step": 4781
    },
    {
        "loss": 1.7618,
        "grad_norm": 2.290336847305298,
        "learning_rate": 5.696778096161553e-06,
        "epoch": 0.6163959783449343,
        "step": 4782
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.5713229179382324,
        "learning_rate": 5.668598538494591e-06,
        "epoch": 0.6165248775457592,
        "step": 4783
    },
    {
        "loss": 1.5784,
        "grad_norm": 1.2663185596466064,
        "learning_rate": 5.64048466113003e-06,
        "epoch": 0.6166537767465842,
        "step": 4784
    },
    {
        "loss": 2.0996,
        "grad_norm": 1.7875641584396362,
        "learning_rate": 5.612436505720709e-06,
        "epoch": 0.6167826759474091,
        "step": 4785
    },
    {
        "loss": 1.9522,
        "grad_norm": 3.8612704277038574,
        "learning_rate": 5.584454113822047e-06,
        "epoch": 0.616911575148234,
        "step": 4786
    },
    {
        "loss": 1.2208,
        "grad_norm": 2.591045379638672,
        "learning_rate": 5.556537526892081e-06,
        "epoch": 0.6170404743490591,
        "step": 4787
    },
    {
        "loss": 1.6375,
        "grad_norm": 2.4684486389160156,
        "learning_rate": 5.52868678629132e-06,
        "epoch": 0.617169373549884,
        "step": 4788
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.2194714546203613,
        "learning_rate": 5.500901933282748e-06,
        "epoch": 0.6172982727507089,
        "step": 4789
    },
    {
        "loss": 2.0645,
        "grad_norm": 1.7310404777526855,
        "learning_rate": 5.47318300903169e-06,
        "epoch": 0.6174271719515338,
        "step": 4790
    },
    {
        "loss": 2.135,
        "grad_norm": 1.2073999643325806,
        "learning_rate": 5.445530054605813e-06,
        "epoch": 0.6175560711523589,
        "step": 4791
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.1776931285858154,
        "learning_rate": 5.4179431109750875e-06,
        "epoch": 0.6176849703531838,
        "step": 4792
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.0523171424865723,
        "learning_rate": 5.390422219011598e-06,
        "epoch": 0.6178138695540087,
        "step": 4793
    },
    {
        "loss": 2.5112,
        "grad_norm": 2.5603036880493164,
        "learning_rate": 5.362967419489623e-06,
        "epoch": 0.6179427687548337,
        "step": 4794
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.2337872982025146,
        "learning_rate": 5.335578753085546e-06,
        "epoch": 0.6180716679556587,
        "step": 4795
    },
    {
        "loss": 2.5545,
        "grad_norm": 1.6857290267944336,
        "learning_rate": 5.308256260377687e-06,
        "epoch": 0.6182005671564836,
        "step": 4796
    },
    {
        "loss": 2.1472,
        "grad_norm": 2.1653378009796143,
        "learning_rate": 5.28099998184638e-06,
        "epoch": 0.6183294663573086,
        "step": 4797
    },
    {
        "loss": 1.9672,
        "grad_norm": 1.5699232816696167,
        "learning_rate": 5.253809957873884e-06,
        "epoch": 0.6184583655581335,
        "step": 4798
    },
    {
        "loss": 1.2086,
        "grad_norm": 2.435004234313965,
        "learning_rate": 5.226686228744243e-06,
        "epoch": 0.6185872647589585,
        "step": 4799
    },
    {
        "loss": 1.4031,
        "grad_norm": 2.9246819019317627,
        "learning_rate": 5.199628834643267e-06,
        "epoch": 0.6187161639597835,
        "step": 4800
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.3620457649230957,
        "learning_rate": 5.1726378156585816e-06,
        "epoch": 0.6188450631606084,
        "step": 4801
    },
    {
        "loss": 2.014,
        "grad_norm": 2.6606178283691406,
        "learning_rate": 5.145713211779374e-06,
        "epoch": 0.6189739623614333,
        "step": 4802
    },
    {
        "loss": 2.0846,
        "grad_norm": 1.6704529523849487,
        "learning_rate": 5.118855062896438e-06,
        "epoch": 0.6191028615622584,
        "step": 4803
    },
    {
        "loss": 1.9207,
        "grad_norm": 2.1525535583496094,
        "learning_rate": 5.0920634088022115e-06,
        "epoch": 0.6192317607630833,
        "step": 4804
    },
    {
        "loss": 1.988,
        "grad_norm": 2.3403170108795166,
        "learning_rate": 5.065338289190491e-06,
        "epoch": 0.6193606599639082,
        "step": 4805
    },
    {
        "loss": 1.5395,
        "grad_norm": 3.03723406791687,
        "learning_rate": 5.038679743656538e-06,
        "epoch": 0.6194895591647331,
        "step": 4806
    },
    {
        "loss": 1.97,
        "grad_norm": 1.5779695510864258,
        "learning_rate": 5.012087811697019e-06,
        "epoch": 0.6196184583655582,
        "step": 4807
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.2170844078063965,
        "learning_rate": 4.98556253270992e-06,
        "epoch": 0.6197473575663831,
        "step": 4808
    },
    {
        "loss": 1.5283,
        "grad_norm": 2.3935539722442627,
        "learning_rate": 4.959103945994348e-06,
        "epoch": 0.619876256767208,
        "step": 4809
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.838010787963867,
        "learning_rate": 4.932712090750729e-06,
        "epoch": 0.620005155968033,
        "step": 4810
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.5119285583496094,
        "learning_rate": 4.906387006080626e-06,
        "epoch": 0.620134055168858,
        "step": 4811
    },
    {
        "loss": 1.2185,
        "grad_norm": 3.2321910858154297,
        "learning_rate": 4.880128730986538e-06,
        "epoch": 0.6202629543696829,
        "step": 4812
    },
    {
        "loss": 1.8906,
        "grad_norm": 1.4742460250854492,
        "learning_rate": 4.8539373043721245e-06,
        "epoch": 0.6203918535705079,
        "step": 4813
    },
    {
        "loss": 1.7058,
        "grad_norm": 3.171804189682007,
        "learning_rate": 4.827812765041978e-06,
        "epoch": 0.6205207527713328,
        "step": 4814
    },
    {
        "loss": 1.6178,
        "grad_norm": 2.174997329711914,
        "learning_rate": 4.801755151701503e-06,
        "epoch": 0.6206496519721578,
        "step": 4815
    },
    {
        "loss": 2.4237,
        "grad_norm": 1.3465681076049805,
        "learning_rate": 4.775764502957042e-06,
        "epoch": 0.6207785511729828,
        "step": 4816
    },
    {
        "loss": 2.0389,
        "grad_norm": 1.4304355382919312,
        "learning_rate": 4.749840857315735e-06,
        "epoch": 0.6209074503738077,
        "step": 4817
    },
    {
        "loss": 1.2908,
        "grad_norm": 2.186443328857422,
        "learning_rate": 4.723984253185332e-06,
        "epoch": 0.6210363495746326,
        "step": 4818
    },
    {
        "loss": 1.9769,
        "grad_norm": 1.7087994813919067,
        "learning_rate": 4.698194728874377e-06,
        "epoch": 0.6211652487754576,
        "step": 4819
    },
    {
        "loss": 2.2731,
        "grad_norm": 1.9439178705215454,
        "learning_rate": 4.67247232259202e-06,
        "epoch": 0.6212941479762826,
        "step": 4820
    },
    {
        "loss": 1.9963,
        "grad_norm": 1.9337952136993408,
        "learning_rate": 4.646817072447873e-06,
        "epoch": 0.6214230471771075,
        "step": 4821
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.8044158220291138,
        "learning_rate": 4.621229016452134e-06,
        "epoch": 0.6215519463779324,
        "step": 4822
    },
    {
        "loss": 1.6447,
        "grad_norm": 2.7075061798095703,
        "learning_rate": 4.5957081925154746e-06,
        "epoch": 0.6216808455787574,
        "step": 4823
    },
    {
        "loss": 1.7633,
        "grad_norm": 2.4036035537719727,
        "learning_rate": 4.570254638448895e-06,
        "epoch": 0.6218097447795824,
        "step": 4824
    },
    {
        "loss": 1.0078,
        "grad_norm": 2.7987797260284424,
        "learning_rate": 4.544868391963697e-06,
        "epoch": 0.6219386439804073,
        "step": 4825
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.465601682662964,
        "learning_rate": 4.519549490671582e-06,
        "epoch": 0.6220675431812323,
        "step": 4826
    },
    {
        "loss": 2.503,
        "grad_norm": 1.860971450805664,
        "learning_rate": 4.494297972084377e-06,
        "epoch": 0.6221964423820572,
        "step": 4827
    },
    {
        "loss": 1.7264,
        "grad_norm": 2.856649398803711,
        "learning_rate": 4.469113873614084e-06,
        "epoch": 0.6223253415828822,
        "step": 4828
    },
    {
        "loss": 1.8213,
        "grad_norm": 1.7831780910491943,
        "learning_rate": 4.4439972325728605e-06,
        "epoch": 0.6224542407837071,
        "step": 4829
    },
    {
        "loss": 1.9667,
        "grad_norm": 2.1765973567962646,
        "learning_rate": 4.418948086172914e-06,
        "epoch": 0.6225831399845321,
        "step": 4830
    },
    {
        "loss": 1.472,
        "grad_norm": 2.9603707790374756,
        "learning_rate": 4.393966471526384e-06,
        "epoch": 0.622712039185357,
        "step": 4831
    },
    {
        "loss": 1.1189,
        "grad_norm": 3.190591335296631,
        "learning_rate": 4.369052425645459e-06,
        "epoch": 0.622840938386182,
        "step": 4832
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.4475629329681396,
        "learning_rate": 4.344205985442162e-06,
        "epoch": 0.622969837587007,
        "step": 4833
    },
    {
        "loss": 2.0575,
        "grad_norm": 1.5830702781677246,
        "learning_rate": 4.319427187728331e-06,
        "epoch": 0.6230987367878319,
        "step": 4834
    },
    {
        "loss": 1.8367,
        "grad_norm": 2.4876797199249268,
        "learning_rate": 4.294716069215643e-06,
        "epoch": 0.6232276359886568,
        "step": 4835
    },
    {
        "loss": 2.2319,
        "grad_norm": 3.062861442565918,
        "learning_rate": 4.27007266651549e-06,
        "epoch": 0.6233565351894819,
        "step": 4836
    },
    {
        "loss": 1.239,
        "grad_norm": 2.685718297958374,
        "learning_rate": 4.2454970161388774e-06,
        "epoch": 0.6234854343903068,
        "step": 4837
    },
    {
        "loss": 1.6993,
        "grad_norm": 2.1024041175842285,
        "learning_rate": 4.220989154496518e-06,
        "epoch": 0.6236143335911317,
        "step": 4838
    },
    {
        "loss": 2.0587,
        "grad_norm": 2.0881762504577637,
        "learning_rate": 4.196549117898641e-06,
        "epoch": 0.6237432327919566,
        "step": 4839
    },
    {
        "loss": 1.5644,
        "grad_norm": 2.4980275630950928,
        "learning_rate": 4.172176942555001e-06,
        "epoch": 0.6238721319927817,
        "step": 4840
    },
    {
        "loss": 1.9408,
        "grad_norm": 2.4258298873901367,
        "learning_rate": 4.147872664574792e-06,
        "epoch": 0.6240010311936066,
        "step": 4841
    },
    {
        "loss": 1.9865,
        "grad_norm": 1.9575845003128052,
        "learning_rate": 4.12363631996664e-06,
        "epoch": 0.6241299303944315,
        "step": 4842
    },
    {
        "loss": 1.1915,
        "grad_norm": 3.5602447986602783,
        "learning_rate": 4.099467944638513e-06,
        "epoch": 0.6242588295952565,
        "step": 4843
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.61647629737854,
        "learning_rate": 4.075367574397665e-06,
        "epoch": 0.6243877287960815,
        "step": 4844
    },
    {
        "loss": 1.9769,
        "grad_norm": 1.678358793258667,
        "learning_rate": 4.051335244950611e-06,
        "epoch": 0.6245166279969064,
        "step": 4845
    },
    {
        "loss": 2.2468,
        "grad_norm": 2.096468925476074,
        "learning_rate": 4.027370991903051e-06,
        "epoch": 0.6246455271977314,
        "step": 4846
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.761577844619751,
        "learning_rate": 4.003474850759836e-06,
        "epoch": 0.6247744263985563,
        "step": 4847
    },
    {
        "loss": 1.5057,
        "grad_norm": 3.5658066272735596,
        "learning_rate": 3.979646856924879e-06,
        "epoch": 0.6249033255993813,
        "step": 4848
    },
    {
        "loss": 2.0726,
        "grad_norm": 1.8429886102676392,
        "learning_rate": 3.955887045701151e-06,
        "epoch": 0.6250322248002063,
        "step": 4849
    },
    {
        "loss": 1.961,
        "grad_norm": 2.5123860836029053,
        "learning_rate": 3.9321954522906105e-06,
        "epoch": 0.6251611240010312,
        "step": 4850
    },
    {
        "loss": 2.2885,
        "grad_norm": 2.115841865539551,
        "learning_rate": 3.908572111794106e-06,
        "epoch": 0.6252900232018561,
        "step": 4851
    },
    {
        "loss": 0.8797,
        "grad_norm": 2.641855239868164,
        "learning_rate": 3.8850170592114095e-06,
        "epoch": 0.6254189224026812,
        "step": 4852
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.9246747493743896,
        "learning_rate": 3.861530329441104e-06,
        "epoch": 0.6255478216035061,
        "step": 4853
    },
    {
        "loss": 2.0399,
        "grad_norm": 1.9611839056015015,
        "learning_rate": 3.838111957280532e-06,
        "epoch": 0.625676720804331,
        "step": 4854
    },
    {
        "loss": 2.1801,
        "grad_norm": 2.5775105953216553,
        "learning_rate": 3.814761977425779e-06,
        "epoch": 0.6258056200051559,
        "step": 4855
    },
    {
        "loss": 0.5563,
        "grad_norm": 1.4714442491531372,
        "learning_rate": 3.791480424471594e-06,
        "epoch": 0.625934519205981,
        "step": 4856
    },
    {
        "loss": 2.077,
        "grad_norm": 2.668902635574341,
        "learning_rate": 3.7682673329113516e-06,
        "epoch": 0.6260634184068059,
        "step": 4857
    },
    {
        "loss": 1.1771,
        "grad_norm": 1.8698235750198364,
        "learning_rate": 3.745122737136969e-06,
        "epoch": 0.6261923176076308,
        "step": 4858
    },
    {
        "loss": 2.2288,
        "grad_norm": 3.0312063694000244,
        "learning_rate": 3.72204667143895e-06,
        "epoch": 0.6263212168084558,
        "step": 4859
    },
    {
        "loss": 1.758,
        "grad_norm": 1.7549521923065186,
        "learning_rate": 3.6990391700061855e-06,
        "epoch": 0.6264501160092807,
        "step": 4860
    },
    {
        "loss": 1.5575,
        "grad_norm": 3.1382970809936523,
        "learning_rate": 3.676100266925997e-06,
        "epoch": 0.6265790152101057,
        "step": 4861
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.693854570388794,
        "learning_rate": 3.6532299961841488e-06,
        "epoch": 0.6267079144109307,
        "step": 4862
    },
    {
        "loss": 1.7882,
        "grad_norm": 2.3385257720947266,
        "learning_rate": 3.6304283916646364e-06,
        "epoch": 0.6268368136117556,
        "step": 4863
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.4558398723602295,
        "learning_rate": 3.6076954871497203e-06,
        "epoch": 0.6269657128125805,
        "step": 4864
    },
    {
        "loss": 1.8708,
        "grad_norm": 2.9263083934783936,
        "learning_rate": 3.58503131631997e-06,
        "epoch": 0.6270946120134056,
        "step": 4865
    },
    {
        "loss": 2.4228,
        "grad_norm": 1.2117550373077393,
        "learning_rate": 3.5624359127540086e-06,
        "epoch": 0.6272235112142305,
        "step": 4866
    },
    {
        "loss": 2.4844,
        "grad_norm": 1.3377883434295654,
        "learning_rate": 3.5399093099286352e-06,
        "epoch": 0.6273524104150554,
        "step": 4867
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.7933540344238281,
        "learning_rate": 3.5174515412187302e-06,
        "epoch": 0.6274813096158803,
        "step": 4868
    },
    {
        "loss": 1.7647,
        "grad_norm": 1.8312376737594604,
        "learning_rate": 3.4950626398971285e-06,
        "epoch": 0.6276102088167054,
        "step": 4869
    },
    {
        "loss": 1.9195,
        "grad_norm": 1.8741649389266968,
        "learning_rate": 3.4727426391346795e-06,
        "epoch": 0.6277391080175303,
        "step": 4870
    },
    {
        "loss": 1.8752,
        "grad_norm": 2.3476784229278564,
        "learning_rate": 3.4504915720001696e-06,
        "epoch": 0.6278680072183552,
        "step": 4871
    },
    {
        "loss": 1.9594,
        "grad_norm": 3.1296980381011963,
        "learning_rate": 3.4283094714601947e-06,
        "epoch": 0.6279969064191802,
        "step": 4872
    },
    {
        "loss": 1.596,
        "grad_norm": 2.6352763175964355,
        "learning_rate": 3.4061963703792e-06,
        "epoch": 0.6281258056200052,
        "step": 4873
    },
    {
        "loss": 2.0295,
        "grad_norm": 1.7673286199569702,
        "learning_rate": 3.384152301519422e-06,
        "epoch": 0.6282547048208301,
        "step": 4874
    },
    {
        "loss": 2.0793,
        "grad_norm": 2.002258062362671,
        "learning_rate": 3.362177297540836e-06,
        "epoch": 0.628383604021655,
        "step": 4875
    },
    {
        "loss": 1.7076,
        "grad_norm": 2.652876615524292,
        "learning_rate": 3.3402713910009988e-06,
        "epoch": 0.62851250322248,
        "step": 4876
    },
    {
        "loss": 2.1637,
        "grad_norm": 2.391167163848877,
        "learning_rate": 3.318434614355187e-06,
        "epoch": 0.628641402423305,
        "step": 4877
    },
    {
        "loss": 2.7561,
        "grad_norm": 2.25606107711792,
        "learning_rate": 3.296666999956255e-06,
        "epoch": 0.62877030162413,
        "step": 4878
    },
    {
        "loss": 2.5844,
        "grad_norm": 1.308676838874817,
        "learning_rate": 3.2749685800544993e-06,
        "epoch": 0.6288992008249549,
        "step": 4879
    },
    {
        "loss": 1.3043,
        "grad_norm": 1.926254391670227,
        "learning_rate": 3.253339386797788e-06,
        "epoch": 0.6290281000257798,
        "step": 4880
    },
    {
        "loss": 2.1083,
        "grad_norm": 1.846960186958313,
        "learning_rate": 3.2317794522314426e-06,
        "epoch": 0.6291569992266048,
        "step": 4881
    },
    {
        "loss": 1.9327,
        "grad_norm": 2.0116124153137207,
        "learning_rate": 3.2102888082980396e-06,
        "epoch": 0.6292858984274298,
        "step": 4882
    },
    {
        "loss": 1.9605,
        "grad_norm": 3.9356307983398438,
        "learning_rate": 3.1888674868376267e-06,
        "epoch": 0.6294147976282547,
        "step": 4883
    },
    {
        "loss": 2.1077,
        "grad_norm": 2.903563976287842,
        "learning_rate": 3.1675155195875383e-06,
        "epoch": 0.6295436968290796,
        "step": 4884
    },
    {
        "loss": 1.2766,
        "grad_norm": 2.89473557472229,
        "learning_rate": 3.1462329381822365e-06,
        "epoch": 0.6296725960299047,
        "step": 4885
    },
    {
        "loss": 1.8399,
        "grad_norm": 1.966294765472412,
        "learning_rate": 3.12501977415352e-06,
        "epoch": 0.6298014952307296,
        "step": 4886
    },
    {
        "loss": 2.1582,
        "grad_norm": 1.5785489082336426,
        "learning_rate": 3.1038760589303105e-06,
        "epoch": 0.6299303944315545,
        "step": 4887
    },
    {
        "loss": 1.8602,
        "grad_norm": 2.997636556625366,
        "learning_rate": 3.082801823838527e-06,
        "epoch": 0.6300592936323794,
        "step": 4888
    },
    {
        "loss": 2.1865,
        "grad_norm": 1.5358480215072632,
        "learning_rate": 3.061797100101288e-06,
        "epoch": 0.6301881928332045,
        "step": 4889
    },
    {
        "loss": 2.0658,
        "grad_norm": 1.9160537719726562,
        "learning_rate": 3.0408619188386665e-06,
        "epoch": 0.6303170920340294,
        "step": 4890
    },
    {
        "loss": 2.273,
        "grad_norm": 1.5325227975845337,
        "learning_rate": 3.0199963110677243e-06,
        "epoch": 0.6304459912348543,
        "step": 4891
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.4787039756774902,
        "learning_rate": 2.9992003077023766e-06,
        "epoch": 0.6305748904356793,
        "step": 4892
    },
    {
        "loss": 1.6571,
        "grad_norm": 2.8682024478912354,
        "learning_rate": 2.9784739395535e-06,
        "epoch": 0.6307037896365043,
        "step": 4893
    },
    {
        "loss": 2.2976,
        "grad_norm": 1.664747714996338,
        "learning_rate": 2.957817237328775e-06,
        "epoch": 0.6308326888373292,
        "step": 4894
    },
    {
        "loss": 2.005,
        "grad_norm": 1.210227131843567,
        "learning_rate": 2.937230231632626e-06,
        "epoch": 0.6309615880381542,
        "step": 4895
    },
    {
        "loss": 1.7867,
        "grad_norm": 2.704103708267212,
        "learning_rate": 2.9167129529662663e-06,
        "epoch": 0.6310904872389791,
        "step": 4896
    },
    {
        "loss": 1.5752,
        "grad_norm": 2.761723279953003,
        "learning_rate": 2.8962654317276015e-06,
        "epoch": 0.631219386439804,
        "step": 4897
    },
    {
        "loss": 0.8897,
        "grad_norm": 2.0143887996673584,
        "learning_rate": 2.8758876982111326e-06,
        "epoch": 0.6313482856406291,
        "step": 4898
    },
    {
        "loss": 1.6521,
        "grad_norm": 2.685487747192383,
        "learning_rate": 2.855579782608031e-06,
        "epoch": 0.631477184841454,
        "step": 4899
    },
    {
        "loss": 2.0445,
        "grad_norm": 2.3338000774383545,
        "learning_rate": 2.8353417150060122e-06,
        "epoch": 0.6316060840422789,
        "step": 4900
    },
    {
        "loss": 1.7801,
        "grad_norm": 3.3796603679656982,
        "learning_rate": 2.815173525389253e-06,
        "epoch": 0.6317349832431038,
        "step": 4901
    },
    {
        "loss": 1.0281,
        "grad_norm": 2.5562047958374023,
        "learning_rate": 2.7950752436384674e-06,
        "epoch": 0.6318638824439289,
        "step": 4902
    },
    {
        "loss": 2.3789,
        "grad_norm": 2.2358429431915283,
        "learning_rate": 2.7750468995307865e-06,
        "epoch": 0.6319927816447538,
        "step": 4903
    },
    {
        "loss": 2.0027,
        "grad_norm": 1.635378360748291,
        "learning_rate": 2.7550885227396573e-06,
        "epoch": 0.6321216808455787,
        "step": 4904
    },
    {
        "loss": 1.9478,
        "grad_norm": 1.4781428575515747,
        "learning_rate": 2.7352001428349593e-06,
        "epoch": 0.6322505800464037,
        "step": 4905
    },
    {
        "loss": 1.8508,
        "grad_norm": 2.7142162322998047,
        "learning_rate": 2.715381789282806e-06,
        "epoch": 0.6323794792472287,
        "step": 4906
    },
    {
        "loss": 1.7205,
        "grad_norm": 2.4726192951202393,
        "learning_rate": 2.6956334914455763e-06,
        "epoch": 0.6325083784480536,
        "step": 4907
    },
    {
        "loss": 1.6848,
        "grad_norm": 4.27254056930542,
        "learning_rate": 2.675955278581871e-06,
        "epoch": 0.6326372776488786,
        "step": 4908
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.5762674808502197,
        "learning_rate": 2.656347179846419e-06,
        "epoch": 0.6327661768497035,
        "step": 4909
    },
    {
        "loss": 2.0068,
        "grad_norm": 1.5481623411178589,
        "learning_rate": 2.6368092242901044e-06,
        "epoch": 0.6328950760505285,
        "step": 4910
    },
    {
        "loss": 1.7686,
        "grad_norm": 1.4278607368469238,
        "learning_rate": 2.6173414408598827e-06,
        "epoch": 0.6330239752513535,
        "step": 4911
    },
    {
        "loss": 2.0541,
        "grad_norm": 1.9374287128448486,
        "learning_rate": 2.5979438583987215e-06,
        "epoch": 0.6331528744521784,
        "step": 4912
    },
    {
        "loss": 1.4687,
        "grad_norm": 2.3722951412200928,
        "learning_rate": 2.5786165056456037e-06,
        "epoch": 0.6332817736530033,
        "step": 4913
    },
    {
        "loss": 1.9047,
        "grad_norm": 1.9791008234024048,
        "learning_rate": 2.5593594112354623e-06,
        "epoch": 0.6334106728538283,
        "step": 4914
    },
    {
        "loss": 2.1181,
        "grad_norm": 1.8096741437911987,
        "learning_rate": 2.540172603699126e-06,
        "epoch": 0.6335395720546533,
        "step": 4915
    },
    {
        "loss": 1.7639,
        "grad_norm": 1.8404415845870972,
        "learning_rate": 2.5210561114632826e-06,
        "epoch": 0.6336684712554782,
        "step": 4916
    },
    {
        "loss": 1.7513,
        "grad_norm": 2.877082109451294,
        "learning_rate": 2.50200996285046e-06,
        "epoch": 0.6337973704563031,
        "step": 4917
    },
    {
        "loss": 1.5687,
        "grad_norm": 3.444258213043213,
        "learning_rate": 2.483034186078964e-06,
        "epoch": 0.6339262696571282,
        "step": 4918
    },
    {
        "loss": 2.5119,
        "grad_norm": 1.7373663187026978,
        "learning_rate": 2.4641288092628166e-06,
        "epoch": 0.6340551688579531,
        "step": 4919
    },
    {
        "loss": 1.9709,
        "grad_norm": 3.0799953937530518,
        "learning_rate": 2.445293860411768e-06,
        "epoch": 0.634184068058778,
        "step": 4920
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.4036709070205688,
        "learning_rate": 2.4265293674312184e-06,
        "epoch": 0.634312967259603,
        "step": 4921
    },
    {
        "loss": 2.1704,
        "grad_norm": 1.7533533573150635,
        "learning_rate": 2.4078353581221347e-06,
        "epoch": 0.634441866460428,
        "step": 4922
    },
    {
        "loss": 1.8273,
        "grad_norm": 2.368797540664673,
        "learning_rate": 2.3892118601811564e-06,
        "epoch": 0.6345707656612529,
        "step": 4923
    },
    {
        "loss": 1.8913,
        "grad_norm": 1.7268457412719727,
        "learning_rate": 2.3706589012003456e-06,
        "epoch": 0.6346996648620779,
        "step": 4924
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.6242444515228271,
        "learning_rate": 2.352176508667314e-06,
        "epoch": 0.6348285640629028,
        "step": 4925
    },
    {
        "loss": 2.117,
        "grad_norm": 2.312490940093994,
        "learning_rate": 2.3337647099651527e-06,
        "epoch": 0.6349574632637278,
        "step": 4926
    },
    {
        "loss": 1.7099,
        "grad_norm": 2.3813488483428955,
        "learning_rate": 2.315423532372274e-06,
        "epoch": 0.6350863624645527,
        "step": 4927
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.6201401948928833,
        "learning_rate": 2.2971530030625298e-06,
        "epoch": 0.6352152616653777,
        "step": 4928
    },
    {
        "loss": 1.7954,
        "grad_norm": 2.4403796195983887,
        "learning_rate": 2.278953149105101e-06,
        "epoch": 0.6353441608662026,
        "step": 4929
    },
    {
        "loss": 1.6697,
        "grad_norm": 2.453385353088379,
        "learning_rate": 2.260823997464412e-06,
        "epoch": 0.6354730600670276,
        "step": 4930
    },
    {
        "loss": 2.3306,
        "grad_norm": 2.4726486206054688,
        "learning_rate": 2.2427655750001665e-06,
        "epoch": 0.6356019592678526,
        "step": 4931
    },
    {
        "loss": 2.1424,
        "grad_norm": 2.13580060005188,
        "learning_rate": 2.2247779084673015e-06,
        "epoch": 0.6357308584686775,
        "step": 4932
    },
    {
        "loss": 1.8738,
        "grad_norm": 1.747684359550476,
        "learning_rate": 2.206861024515866e-06,
        "epoch": 0.6358597576695024,
        "step": 4933
    },
    {
        "loss": 1.4222,
        "grad_norm": 3.3253443241119385,
        "learning_rate": 2.1890149496910705e-06,
        "epoch": 0.6359886568703274,
        "step": 4934
    },
    {
        "loss": 1.6178,
        "grad_norm": 3.13836407661438,
        "learning_rate": 2.1712397104332484e-06,
        "epoch": 0.6361175560711524,
        "step": 4935
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.2445967197418213,
        "learning_rate": 2.153535333077711e-06,
        "epoch": 0.6362464552719773,
        "step": 4936
    },
    {
        "loss": 2.1266,
        "grad_norm": 1.819212555885315,
        "learning_rate": 2.1359018438548428e-06,
        "epoch": 0.6363753544728022,
        "step": 4937
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.9534908533096313,
        "learning_rate": 2.118339268889996e-06,
        "epoch": 0.6365042536736272,
        "step": 4938
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.8593130111694336,
        "learning_rate": 2.1008476342034235e-06,
        "epoch": 0.6366331528744522,
        "step": 4939
    },
    {
        "loss": 1.4523,
        "grad_norm": 2.699639320373535,
        "learning_rate": 2.0834269657103e-06,
        "epoch": 0.6367620520752771,
        "step": 4940
    },
    {
        "loss": 2.5045,
        "grad_norm": 1.6328409910202026,
        "learning_rate": 2.066077289220647e-06,
        "epoch": 0.6368909512761021,
        "step": 4941
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.7744243144989014,
        "learning_rate": 2.048798630439369e-06,
        "epoch": 0.637019850476927,
        "step": 4942
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.8367351293563843,
        "learning_rate": 2.0315910149660165e-06,
        "epoch": 0.637148749677752,
        "step": 4943
    },
    {
        "loss": 1.3799,
        "grad_norm": 3.0313491821289062,
        "learning_rate": 2.0144544682950126e-06,
        "epoch": 0.637277648878577,
        "step": 4944
    },
    {
        "loss": 1.3756,
        "grad_norm": 3.1021816730499268,
        "learning_rate": 1.9973890158154596e-06,
        "epoch": 0.6374065480794019,
        "step": 4945
    },
    {
        "loss": 2.395,
        "grad_norm": 1.5297191143035889,
        "learning_rate": 1.9803946828110375e-06,
        "epoch": 0.6375354472802268,
        "step": 4946
    },
    {
        "loss": 2.5216,
        "grad_norm": 1.9337942600250244,
        "learning_rate": 1.9634714944601727e-06,
        "epoch": 0.6376643464810519,
        "step": 4947
    },
    {
        "loss": 1.9331,
        "grad_norm": 2.892263889312744,
        "learning_rate": 1.94661947583587e-06,
        "epoch": 0.6377932456818768,
        "step": 4948
    },
    {
        "loss": 1.814,
        "grad_norm": 2.0805015563964844,
        "learning_rate": 1.9298386519055856e-06,
        "epoch": 0.6379221448827017,
        "step": 4949
    },
    {
        "loss": 2.0746,
        "grad_norm": 2.5872154235839844,
        "learning_rate": 1.9131290475314255e-06,
        "epoch": 0.6380510440835266,
        "step": 4950
    },
    {
        "loss": 2.1445,
        "grad_norm": 1.5590288639068604,
        "learning_rate": 1.8964906874699317e-06,
        "epoch": 0.6381799432843517,
        "step": 4951
    },
    {
        "loss": 2.0555,
        "grad_norm": 2.8597569465637207,
        "learning_rate": 1.8799235963720351e-06,
        "epoch": 0.6383088424851766,
        "step": 4952
    },
    {
        "loss": 2.0716,
        "grad_norm": 2.3454489707946777,
        "learning_rate": 1.8634277987831627e-06,
        "epoch": 0.6384377416860015,
        "step": 4953
    },
    {
        "loss": 1.7299,
        "grad_norm": 1.5378376245498657,
        "learning_rate": 1.8470033191430869e-06,
        "epoch": 0.6385666408868265,
        "step": 4954
    },
    {
        "loss": 1.7954,
        "grad_norm": 2.4201157093048096,
        "learning_rate": 1.8306501817858756e-06,
        "epoch": 0.6386955400876515,
        "step": 4955
    },
    {
        "loss": 2.2352,
        "grad_norm": 1.9958250522613525,
        "learning_rate": 1.8143684109399428e-06,
        "epoch": 0.6388244392884764,
        "step": 4956
    },
    {
        "loss": 1.4994,
        "grad_norm": 2.6492130756378174,
        "learning_rate": 1.7981580307279645e-06,
        "epoch": 0.6389533384893014,
        "step": 4957
    },
    {
        "loss": 1.9168,
        "grad_norm": 3.021059513092041,
        "learning_rate": 1.7820190651668457e-06,
        "epoch": 0.6390822376901263,
        "step": 4958
    },
    {
        "loss": 1.1248,
        "grad_norm": 2.334131956100464,
        "learning_rate": 1.7659515381676484e-06,
        "epoch": 0.6392111368909513,
        "step": 4959
    },
    {
        "loss": 2.0153,
        "grad_norm": 3.598140001296997,
        "learning_rate": 1.749955473535636e-06,
        "epoch": 0.6393400360917763,
        "step": 4960
    },
    {
        "loss": 2.3205,
        "grad_norm": 1.5779011249542236,
        "learning_rate": 1.7340308949701957e-06,
        "epoch": 0.6394689352926012,
        "step": 4961
    },
    {
        "loss": 1.1463,
        "grad_norm": 2.7033872604370117,
        "learning_rate": 1.7181778260647541e-06,
        "epoch": 0.6395978344934261,
        "step": 4962
    },
    {
        "loss": 2.3005,
        "grad_norm": 1.8419289588928223,
        "learning_rate": 1.7023962903068458e-06,
        "epoch": 0.6397267336942511,
        "step": 4963
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.9676299095153809,
        "learning_rate": 1.6866863110780063e-06,
        "epoch": 0.6398556328950761,
        "step": 4964
    },
    {
        "loss": 1.9569,
        "grad_norm": 1.9999319314956665,
        "learning_rate": 1.6710479116537225e-06,
        "epoch": 0.639984532095901,
        "step": 4965
    },
    {
        "loss": 2.132,
        "grad_norm": 1.697911262512207,
        "learning_rate": 1.6554811152034944e-06,
        "epoch": 0.6401134312967259,
        "step": 4966
    },
    {
        "loss": 2.4342,
        "grad_norm": 1.9535930156707764,
        "learning_rate": 1.6399859447906896e-06,
        "epoch": 0.6402423304975509,
        "step": 4967
    },
    {
        "loss": 1.6276,
        "grad_norm": 2.902954339981079,
        "learning_rate": 1.6245624233725387e-06,
        "epoch": 0.6403712296983759,
        "step": 4968
    },
    {
        "loss": 0.3517,
        "grad_norm": 2.241011142730713,
        "learning_rate": 1.609210573800185e-06,
        "epoch": 0.6405001288992008,
        "step": 4969
    },
    {
        "loss": 1.5581,
        "grad_norm": 3.034695625305176,
        "learning_rate": 1.5939304188185344e-06,
        "epoch": 0.6406290281000258,
        "step": 4970
    },
    {
        "loss": 2.216,
        "grad_norm": 1.580540418624878,
        "learning_rate": 1.5787219810662723e-06,
        "epoch": 0.6407579273008507,
        "step": 4971
    },
    {
        "loss": 1.6807,
        "grad_norm": 1.334445834159851,
        "learning_rate": 1.563585283075858e-06,
        "epoch": 0.6408868265016757,
        "step": 4972
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.806252121925354,
        "learning_rate": 1.5485203472734467e-06,
        "epoch": 0.6410157257025006,
        "step": 4973
    },
    {
        "loss": 2.4802,
        "grad_norm": 1.9191522598266602,
        "learning_rate": 1.5335271959788623e-06,
        "epoch": 0.6411446249033256,
        "step": 4974
    },
    {
        "loss": 2.193,
        "grad_norm": 2.1747429370880127,
        "learning_rate": 1.5186058514055912e-06,
        "epoch": 0.6412735241041505,
        "step": 4975
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.5965911149978638,
        "learning_rate": 1.5037563356607276e-06,
        "epoch": 0.6414024233049755,
        "step": 4976
    },
    {
        "loss": 2.0336,
        "grad_norm": 2.200273275375366,
        "learning_rate": 1.4889786707449394e-06,
        "epoch": 0.6415313225058005,
        "step": 4977
    },
    {
        "loss": 1.0261,
        "grad_norm": 2.8339078426361084,
        "learning_rate": 1.4742728785524517e-06,
        "epoch": 0.6416602217066254,
        "step": 4978
    },
    {
        "loss": 2.5777,
        "grad_norm": 1.733733892440796,
        "learning_rate": 1.4596389808710088e-06,
        "epoch": 0.6417891209074503,
        "step": 4979
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.312422275543213,
        "learning_rate": 1.4450769993818114e-06,
        "epoch": 0.6419180201082754,
        "step": 4980
    },
    {
        "loss": 2.0368,
        "grad_norm": 2.350360155105591,
        "learning_rate": 1.4305869556595408e-06,
        "epoch": 0.6420469193091003,
        "step": 4981
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.0424864292144775,
        "learning_rate": 1.4161688711722854e-06,
        "epoch": 0.6421758185099252,
        "step": 4982
    },
    {
        "loss": 2.262,
        "grad_norm": 1.31874418258667,
        "learning_rate": 1.4018227672815188e-06,
        "epoch": 0.6423047177107502,
        "step": 4983
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.435861110687256,
        "learning_rate": 1.3875486652420722e-06,
        "epoch": 0.6424336169115752,
        "step": 4984
    },
    {
        "loss": 2.172,
        "grad_norm": 1.9477545022964478,
        "learning_rate": 1.373346586202101e-06,
        "epoch": 0.6425625161124001,
        "step": 4985
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.0325496196746826,
        "learning_rate": 1.3592165512030518e-06,
        "epoch": 0.642691415313225,
        "step": 4986
    },
    {
        "loss": 1.4947,
        "grad_norm": 2.6804420948028564,
        "learning_rate": 1.3451585811796341e-06,
        "epoch": 0.64282031451405,
        "step": 4987
    },
    {
        "loss": 1.5758,
        "grad_norm": 1.6460720300674438,
        "learning_rate": 1.3311726969597816e-06,
        "epoch": 0.642949213714875,
        "step": 4988
    },
    {
        "loss": 2.1835,
        "grad_norm": 1.4391933679580688,
        "learning_rate": 1.3172589192646356e-06,
        "epoch": 0.6430781129156999,
        "step": 4989
    },
    {
        "loss": 1.6427,
        "grad_norm": 3.0603420734405518,
        "learning_rate": 1.3034172687085177e-06,
        "epoch": 0.6432070121165249,
        "step": 4990
    },
    {
        "loss": 1.8483,
        "grad_norm": 2.832798480987549,
        "learning_rate": 1.2896477657988514e-06,
        "epoch": 0.6433359113173498,
        "step": 4991
    },
    {
        "loss": 2.2887,
        "grad_norm": 2.2666659355163574,
        "learning_rate": 1.2759504309361958e-06,
        "epoch": 0.6434648105181748,
        "step": 4992
    },
    {
        "loss": 1.5807,
        "grad_norm": 1.9139995574951172,
        "learning_rate": 1.2623252844141953e-06,
        "epoch": 0.6435937097189998,
        "step": 4993
    },
    {
        "loss": 1.7643,
        "grad_norm": 2.4134938716888428,
        "learning_rate": 1.2487723464195133e-06,
        "epoch": 0.6437226089198247,
        "step": 4994
    },
    {
        "loss": 1.7396,
        "grad_norm": 3.771639108657837,
        "learning_rate": 1.2352916370318435e-06,
        "epoch": 0.6438515081206496,
        "step": 4995
    },
    {
        "loss": 1.6029,
        "grad_norm": 1.9142972230911255,
        "learning_rate": 1.221883176223887e-06,
        "epoch": 0.6439804073214747,
        "step": 4996
    },
    {
        "loss": 1.5218,
        "grad_norm": 3.1419103145599365,
        "learning_rate": 1.2085469838612695e-06,
        "epoch": 0.6441093065222996,
        "step": 4997
    },
    {
        "loss": 1.9867,
        "grad_norm": 2.1418943405151367,
        "learning_rate": 1.1952830797025638e-06,
        "epoch": 0.6442382057231245,
        "step": 4998
    },
    {
        "loss": 1.4494,
        "grad_norm": 3.0832619667053223,
        "learning_rate": 1.1820914833992558e-06,
        "epoch": 0.6443671049239494,
        "step": 4999
    },
    {
        "loss": 1.8093,
        "grad_norm": 1.7148113250732422,
        "learning_rate": 1.1689722144956728e-06,
        "epoch": 0.6444960041247745,
        "step": 5000
    },
    {
        "loss": 2.2668,
        "grad_norm": 1.7866289615631104,
        "learning_rate": 1.1559252924289888e-06,
        "epoch": 0.6446249033255994,
        "step": 5001
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.430986166000366,
        "learning_rate": 1.1429507365292303e-06,
        "epoch": 0.6447538025264243,
        "step": 5002
    },
    {
        "loss": 2.1945,
        "grad_norm": 1.9139654636383057,
        "learning_rate": 1.1300485660191484e-06,
        "epoch": 0.6448827017272493,
        "step": 5003
    },
    {
        "loss": 1.162,
        "grad_norm": 3.1035237312316895,
        "learning_rate": 1.1172188000142802e-06,
        "epoch": 0.6450116009280742,
        "step": 5004
    },
    {
        "loss": 1.9821,
        "grad_norm": 1.851660966873169,
        "learning_rate": 1.104461457522893e-06,
        "epoch": 0.6451405001288992,
        "step": 5005
    },
    {
        "loss": 2.214,
        "grad_norm": 2.047107458114624,
        "learning_rate": 1.0917765574459782e-06,
        "epoch": 0.6452693993297242,
        "step": 5006
    },
    {
        "loss": 1.2923,
        "grad_norm": 3.720466375350952,
        "learning_rate": 1.0791641185771139e-06,
        "epoch": 0.6453982985305491,
        "step": 5007
    },
    {
        "loss": 2.1485,
        "grad_norm": 2.1924378871917725,
        "learning_rate": 1.0666241596026027e-06,
        "epoch": 0.645527197731374,
        "step": 5008
    },
    {
        "loss": 1.919,
        "grad_norm": 1.8484724760055542,
        "learning_rate": 1.0541566991013553e-06,
        "epoch": 0.645656096932199,
        "step": 5009
    },
    {
        "loss": 1.2889,
        "grad_norm": 3.1155130863189697,
        "learning_rate": 1.0417617555448177e-06,
        "epoch": 0.645784996133024,
        "step": 5010
    },
    {
        "loss": 1.5793,
        "grad_norm": 2.899949312210083,
        "learning_rate": 1.0294393472970453e-06,
        "epoch": 0.6459138953338489,
        "step": 5011
    },
    {
        "loss": 1.8813,
        "grad_norm": 2.7000184059143066,
        "learning_rate": 1.017189492614623e-06,
        "epoch": 0.6460427945346738,
        "step": 5012
    },
    {
        "loss": 2.4563,
        "grad_norm": 1.8190897703170776,
        "learning_rate": 1.0050122096466108e-06,
        "epoch": 0.6461716937354989,
        "step": 5013
    },
    {
        "loss": 1.1585,
        "grad_norm": 3.6655423641204834,
        "learning_rate": 9.929075164345713e-07,
        "epoch": 0.6463005929363238,
        "step": 5014
    },
    {
        "loss": 1.9498,
        "grad_norm": 2.5134243965148926,
        "learning_rate": 9.808754309125368e-07,
        "epoch": 0.6464294921371487,
        "step": 5015
    },
    {
        "loss": 1.9086,
        "grad_norm": 1.7633442878723145,
        "learning_rate": 9.689159709069195e-07,
        "epoch": 0.6465583913379737,
        "step": 5016
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.4751760959625244,
        "learning_rate": 9.570291541365684e-07,
        "epoch": 0.6466872905387987,
        "step": 5017
    },
    {
        "loss": 1.814,
        "grad_norm": 2.098179578781128,
        "learning_rate": 9.452149982127179e-07,
        "epoch": 0.6468161897396236,
        "step": 5018
    },
    {
        "loss": 2.1682,
        "grad_norm": 1.3909239768981934,
        "learning_rate": 9.334735206388834e-07,
        "epoch": 0.6469450889404486,
        "step": 5019
    },
    {
        "loss": 2.4416,
        "grad_norm": 2.241973638534546,
        "learning_rate": 9.218047388109774e-07,
        "epoch": 0.6470739881412735,
        "step": 5020
    },
    {
        "loss": 2.0292,
        "grad_norm": 2.314204454421997,
        "learning_rate": 9.102086700171708e-07,
        "epoch": 0.6472028873420985,
        "step": 5021
    },
    {
        "loss": 2.0061,
        "grad_norm": 1.6787123680114746,
        "learning_rate": 8.986853314379207e-07,
        "epoch": 0.6473317865429234,
        "step": 5022
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.514800548553467,
        "learning_rate": 8.872347401458925e-07,
        "epoch": 0.6474606857437484,
        "step": 5023
    },
    {
        "loss": 2.3762,
        "grad_norm": 1.5597615242004395,
        "learning_rate": 8.758569131060267e-07,
        "epoch": 0.6475895849445733,
        "step": 5024
    },
    {
        "loss": 0.6389,
        "grad_norm": 3.0746164321899414,
        "learning_rate": 8.645518671754172e-07,
        "epoch": 0.6477184841453983,
        "step": 5025
    },
    {
        "loss": 1.9735,
        "grad_norm": 2.1170716285705566,
        "learning_rate": 8.53319619103321e-07,
        "epoch": 0.6478473833462233,
        "step": 5026
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.5528674125671387,
        "learning_rate": 8.421601855311878e-07,
        "epoch": 0.6479762825470482,
        "step": 5027
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.8113682270050049,
        "learning_rate": 8.31073582992542e-07,
        "epoch": 0.6481051817478731,
        "step": 5028
    },
    {
        "loss": 1.8686,
        "grad_norm": 2.2261962890625,
        "learning_rate": 8.200598279130168e-07,
        "epoch": 0.6482340809486982,
        "step": 5029
    },
    {
        "loss": 1.9301,
        "grad_norm": 1.749454379081726,
        "learning_rate": 8.091189366103258e-07,
        "epoch": 0.6483629801495231,
        "step": 5030
    },
    {
        "loss": 2.3308,
        "grad_norm": 2.4831783771514893,
        "learning_rate": 7.982509252942138e-07,
        "epoch": 0.648491879350348,
        "step": 5031
    },
    {
        "loss": 1.8281,
        "grad_norm": 1.7263175249099731,
        "learning_rate": 7.874558100664564e-07,
        "epoch": 0.648620778551173,
        "step": 5032
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.746126174926758,
        "learning_rate": 7.76733606920832e-07,
        "epoch": 0.648749677751998,
        "step": 5033
    },
    {
        "loss": 1.232,
        "grad_norm": 3.9975175857543945,
        "learning_rate": 7.660843317430944e-07,
        "epoch": 0.6488785769528229,
        "step": 5034
    },
    {
        "loss": 1.4185,
        "grad_norm": 3.771941661834717,
        "learning_rate": 7.555080003109338e-07,
        "epoch": 0.6490074761536478,
        "step": 5035
    },
    {
        "loss": 1.6589,
        "grad_norm": 2.7128379344940186,
        "learning_rate": 7.450046282939882e-07,
        "epoch": 0.6491363753544728,
        "step": 5036
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.3827143907546997,
        "learning_rate": 7.345742312537929e-07,
        "epoch": 0.6492652745552978,
        "step": 5037
    },
    {
        "loss": 1.5808,
        "grad_norm": 2.0876431465148926,
        "learning_rate": 7.242168246437531e-07,
        "epoch": 0.6493941737561227,
        "step": 5038
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.3195748329162598,
        "learning_rate": 7.139324238091605e-07,
        "epoch": 0.6495230729569477,
        "step": 5039
    },
    {
        "loss": 2.4733,
        "grad_norm": 1.4250884056091309,
        "learning_rate": 7.037210439871211e-07,
        "epoch": 0.6496519721577726,
        "step": 5040
    },
    {
        "loss": 2.225,
        "grad_norm": 2.3255181312561035,
        "learning_rate": 6.93582700306561e-07,
        "epoch": 0.6497808713585975,
        "step": 5041
    },
    {
        "loss": 1.5259,
        "grad_norm": 3.0429537296295166,
        "learning_rate": 6.835174077881979e-07,
        "epoch": 0.6499097705594226,
        "step": 5042
    },
    {
        "loss": 1.5419,
        "grad_norm": 3.113621711730957,
        "learning_rate": 6.735251813445198e-07,
        "epoch": 0.6500386697602475,
        "step": 5043
    },
    {
        "loss": 0.5529,
        "grad_norm": 2.3676106929779053,
        "learning_rate": 6.636060357797624e-07,
        "epoch": 0.6501675689610724,
        "step": 5044
    },
    {
        "loss": 1.7978,
        "grad_norm": 1.8234235048294067,
        "learning_rate": 6.537599857898813e-07,
        "epoch": 0.6502964681618973,
        "step": 5045
    },
    {
        "loss": 2.3592,
        "grad_norm": 1.4848041534423828,
        "learning_rate": 6.439870459625408e-07,
        "epoch": 0.6504253673627224,
        "step": 5046
    },
    {
        "loss": 1.8186,
        "grad_norm": 2.9857635498046875,
        "learning_rate": 6.342872307770919e-07,
        "epoch": 0.6505542665635473,
        "step": 5047
    },
    {
        "loss": 1.9021,
        "grad_norm": 2.2715494632720947,
        "learning_rate": 6.246605546045281e-07,
        "epoch": 0.6506831657643722,
        "step": 5048
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.313321590423584,
        "learning_rate": 6.151070317075014e-07,
        "epoch": 0.6508120649651972,
        "step": 5049
    },
    {
        "loss": 1.9164,
        "grad_norm": 2.5535707473754883,
        "learning_rate": 6.056266762402729e-07,
        "epoch": 0.6509409641660222,
        "step": 5050
    },
    {
        "loss": 1.9531,
        "grad_norm": 2.0972189903259277,
        "learning_rate": 5.962195022487071e-07,
        "epoch": 0.6510698633668471,
        "step": 5051
    },
    {
        "loss": 2.234,
        "grad_norm": 1.8452517986297607,
        "learning_rate": 5.86885523670222e-07,
        "epoch": 0.6511987625676721,
        "step": 5052
    },
    {
        "loss": 1.466,
        "grad_norm": 2.7249107360839844,
        "learning_rate": 5.776247543338275e-07,
        "epoch": 0.651327661768497,
        "step": 5053
    },
    {
        "loss": 1.9942,
        "grad_norm": 1.5750701427459717,
        "learning_rate": 5.684372079600375e-07,
        "epoch": 0.651456560969322,
        "step": 5054
    },
    {
        "loss": 1.754,
        "grad_norm": 3.0843963623046875,
        "learning_rate": 5.593228981608967e-07,
        "epoch": 0.651585460170147,
        "step": 5055
    },
    {
        "loss": 2.3331,
        "grad_norm": 1.6371197700500488,
        "learning_rate": 5.502818384399367e-07,
        "epoch": 0.6517143593709719,
        "step": 5056
    },
    {
        "loss": 2.1967,
        "grad_norm": 2.166145086288452,
        "learning_rate": 5.413140421921703e-07,
        "epoch": 0.6518432585717968,
        "step": 5057
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.5163613557815552,
        "learning_rate": 5.32419522704064e-07,
        "epoch": 0.6519721577726219,
        "step": 5058
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.411390542984009,
        "learning_rate": 5.235982931535044e-07,
        "epoch": 0.6521010569734468,
        "step": 5059
    },
    {
        "loss": 1.6346,
        "grad_norm": 2.7286834716796875,
        "learning_rate": 5.148503666098259e-07,
        "epoch": 0.6522299561742717,
        "step": 5060
    },
    {
        "loss": 2.0474,
        "grad_norm": 3.2071926593780518,
        "learning_rate": 5.061757560337221e-07,
        "epoch": 0.6523588553750966,
        "step": 5061
    },
    {
        "loss": 1.7328,
        "grad_norm": 2.607163667678833,
        "learning_rate": 4.975744742772793e-07,
        "epoch": 0.6524877545759217,
        "step": 5062
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.2660942077636719,
        "learning_rate": 4.89046534083959e-07,
        "epoch": 0.6526166537767466,
        "step": 5063
    },
    {
        "loss": 1.9424,
        "grad_norm": 2.4452250003814697,
        "learning_rate": 4.805919480885379e-07,
        "epoch": 0.6527455529775715,
        "step": 5064
    },
    {
        "loss": 2.487,
        "grad_norm": 1.368268609046936,
        "learning_rate": 4.722107288171074e-07,
        "epoch": 0.6528744521783965,
        "step": 5065
    },
    {
        "loss": 2.2306,
        "grad_norm": 2.2425713539123535,
        "learning_rate": 4.639028886870955e-07,
        "epoch": 0.6530033513792215,
        "step": 5066
    },
    {
        "loss": 2.0245,
        "grad_norm": 2.6406428813934326,
        "learning_rate": 4.556684400071731e-07,
        "epoch": 0.6531322505800464,
        "step": 5067
    },
    {
        "loss": 2.1382,
        "grad_norm": 3.4214131832122803,
        "learning_rate": 4.475073949773034e-07,
        "epoch": 0.6532611497808714,
        "step": 5068
    },
    {
        "loss": 2.1818,
        "grad_norm": 2.2131242752075195,
        "learning_rate": 4.3941976568869226e-07,
        "epoch": 0.6533900489816963,
        "step": 5069
    },
    {
        "loss": 1.6773,
        "grad_norm": 1.9375934600830078,
        "learning_rate": 4.314055641237602e-07,
        "epoch": 0.6535189481825213,
        "step": 5070
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.8784741163253784,
        "learning_rate": 4.2346480215614826e-07,
        "epoch": 0.6536478473833462,
        "step": 5071
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.2234954833984375,
        "learning_rate": 4.1559749155070106e-07,
        "epoch": 0.6537767465841712,
        "step": 5072
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.6294564008712769,
        "learning_rate": 4.0780364396343917e-07,
        "epoch": 0.6539056457849961,
        "step": 5073
    },
    {
        "loss": 2.5199,
        "grad_norm": 1.1157647371292114,
        "learning_rate": 4.0008327094150367e-07,
        "epoch": 0.6540345449858211,
        "step": 5074
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.039677858352661,
        "learning_rate": 3.924363839232281e-07,
        "epoch": 0.6541634441866461,
        "step": 5075
    },
    {
        "loss": 1.8822,
        "grad_norm": 1.7247527837753296,
        "learning_rate": 3.8486299423806107e-07,
        "epoch": 0.654292343387471,
        "step": 5076
    },
    {
        "loss": 1.7857,
        "grad_norm": 1.802526831626892,
        "learning_rate": 3.773631131065158e-07,
        "epoch": 0.6544212425882959,
        "step": 5077
    },
    {
        "loss": 1.6696,
        "grad_norm": 2.670168161392212,
        "learning_rate": 3.699367516402541e-07,
        "epoch": 0.6545501417891209,
        "step": 5078
    },
    {
        "loss": 2.0866,
        "grad_norm": 1.8972922563552856,
        "learning_rate": 3.6258392084199124e-07,
        "epoch": 0.6546790409899459,
        "step": 5079
    },
    {
        "loss": 1.657,
        "grad_norm": 4.4806623458862305,
        "learning_rate": 3.553046316054742e-07,
        "epoch": 0.6548079401907708,
        "step": 5080
    },
    {
        "loss": 2.1339,
        "grad_norm": 2.15720796585083,
        "learning_rate": 3.4809889471553723e-07,
        "epoch": 0.6549368393915957,
        "step": 5081
    },
    {
        "loss": 2.0681,
        "grad_norm": 2.1706180572509766,
        "learning_rate": 3.4096672084802385e-07,
        "epoch": 0.6550657385924207,
        "step": 5082
    },
    {
        "loss": 1.7417,
        "grad_norm": 2.0573830604553223,
        "learning_rate": 3.339081205697703e-07,
        "epoch": 0.6551946377932457,
        "step": 5083
    },
    {
        "loss": 1.2368,
        "grad_norm": 3.2129552364349365,
        "learning_rate": 3.2692310433862784e-07,
        "epoch": 0.6553235369940706,
        "step": 5084
    },
    {
        "loss": 1.8619,
        "grad_norm": 2.489877462387085,
        "learning_rate": 3.2001168250344047e-07,
        "epoch": 0.6554524361948956,
        "step": 5085
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.933712959289551,
        "learning_rate": 3.1317386530397284e-07,
        "epoch": 0.6555813353957205,
        "step": 5086
    },
    {
        "loss": 2.0128,
        "grad_norm": 2.0302958488464355,
        "learning_rate": 3.064096628709767e-07,
        "epoch": 0.6557102345965455,
        "step": 5087
    },
    {
        "loss": 2.192,
        "grad_norm": 1.6503878831863403,
        "learning_rate": 2.9971908522613e-07,
        "epoch": 0.6558391337973705,
        "step": 5088
    },
    {
        "loss": 2.4804,
        "grad_norm": 2.2380456924438477,
        "learning_rate": 2.9310214228202013e-07,
        "epoch": 0.6559680329981954,
        "step": 5089
    },
    {
        "loss": 1.8928,
        "grad_norm": 2.629143476486206,
        "learning_rate": 2.8655884384214406e-07,
        "epoch": 0.6560969321990203,
        "step": 5090
    },
    {
        "loss": 1.8949,
        "grad_norm": 3.130185842514038,
        "learning_rate": 2.8008919960089695e-07,
        "epoch": 0.6562258313998454,
        "step": 5091
    },
    {
        "loss": 1.6314,
        "grad_norm": 2.8604116439819336,
        "learning_rate": 2.7369321914354486e-07,
        "epoch": 0.6563547306006703,
        "step": 5092
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.110642194747925,
        "learning_rate": 2.67370911946202e-07,
        "epoch": 0.6564836298014952,
        "step": 5093
    },
    {
        "loss": 2.1923,
        "grad_norm": 1.6037633419036865,
        "learning_rate": 2.611222873758479e-07,
        "epoch": 0.6566125290023201,
        "step": 5094
    },
    {
        "loss": 2.3934,
        "grad_norm": 1.4159294366836548,
        "learning_rate": 2.549473546902936e-07,
        "epoch": 0.6567414282031452,
        "step": 5095
    },
    {
        "loss": 1.5433,
        "grad_norm": 3.0336499214172363,
        "learning_rate": 2.4884612303815447e-07,
        "epoch": 0.6568703274039701,
        "step": 5096
    },
    {
        "loss": 2.1432,
        "grad_norm": 2.1753013134002686,
        "learning_rate": 2.428186014588718e-07,
        "epoch": 0.656999226604795,
        "step": 5097
    },
    {
        "loss": 1.2803,
        "grad_norm": 2.6037704944610596,
        "learning_rate": 2.3686479888267443e-07,
        "epoch": 0.65712812580562,
        "step": 5098
    },
    {
        "loss": 2.058,
        "grad_norm": 2.259789228439331,
        "learning_rate": 2.3098472413056182e-07,
        "epoch": 0.657257025006445,
        "step": 5099
    },
    {
        "loss": 1.8118,
        "grad_norm": 1.7646933794021606,
        "learning_rate": 2.2517838591430973e-07,
        "epoch": 0.6573859242072699,
        "step": 5100
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.610313057899475,
        "learning_rate": 2.1944579283644795e-07,
        "epoch": 0.6575148234080949,
        "step": 5101
    },
    {
        "loss": 2.202,
        "grad_norm": 2.3206241130828857,
        "learning_rate": 2.1378695339023257e-07,
        "epoch": 0.6576437226089198,
        "step": 5102
    },
    {
        "loss": 1.8109,
        "grad_norm": 1.9054665565490723,
        "learning_rate": 2.0820187595966824e-07,
        "epoch": 0.6577726218097448,
        "step": 5103
    },
    {
        "loss": 2.297,
        "grad_norm": 2.1814990043640137,
        "learning_rate": 2.0269056881946358e-07,
        "epoch": 0.6579015210105698,
        "step": 5104
    },
    {
        "loss": 1.1315,
        "grad_norm": 2.7994017601013184,
        "learning_rate": 1.9725304013504253e-07,
        "epoch": 0.6580304202113947,
        "step": 5105
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.096200704574585,
        "learning_rate": 1.9188929796249976e-07,
        "epoch": 0.6581593194122196,
        "step": 5106
    },
    {
        "loss": 2.2896,
        "grad_norm": 3.030198574066162,
        "learning_rate": 1.865993502486285e-07,
        "epoch": 0.6582882186130447,
        "step": 5107
    },
    {
        "loss": 2.3547,
        "grad_norm": 1.58397376537323,
        "learning_rate": 1.8138320483088723e-07,
        "epoch": 0.6584171178138696,
        "step": 5108
    },
    {
        "loss": 1.539,
        "grad_norm": 2.62041974067688,
        "learning_rate": 1.7624086943738293e-07,
        "epoch": 0.6585460170146945,
        "step": 5109
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.490010380744934,
        "learning_rate": 1.711723516868713e-07,
        "epoch": 0.6586749162155194,
        "step": 5110
    },
    {
        "loss": 1.442,
        "grad_norm": 3.926203727722168,
        "learning_rate": 1.6617765908873983e-07,
        "epoch": 0.6588038154163445,
        "step": 5111
    },
    {
        "loss": 1.9353,
        "grad_norm": 2.1039633750915527,
        "learning_rate": 1.6125679904301361e-07,
        "epoch": 0.6589327146171694,
        "step": 5112
    },
    {
        "loss": 2.013,
        "grad_norm": 2.071502447128296,
        "learning_rate": 1.5640977884029962e-07,
        "epoch": 0.6590616138179943,
        "step": 5113
    },
    {
        "loss": 1.6918,
        "grad_norm": 2.566094160079956,
        "learning_rate": 1.5163660566183678e-07,
        "epoch": 0.6591905130188193,
        "step": 5114
    },
    {
        "loss": 2.0291,
        "grad_norm": 2.3549044132232666,
        "learning_rate": 1.469372865794294e-07,
        "epoch": 0.6593194122196442,
        "step": 5115
    },
    {
        "loss": 1.4281,
        "grad_norm": 2.911673069000244,
        "learning_rate": 1.423118285554803e-07,
        "epoch": 0.6594483114204692,
        "step": 5116
    },
    {
        "loss": 1.8962,
        "grad_norm": 2.70892333984375,
        "learning_rate": 1.3776023844294106e-07,
        "epoch": 0.6595772106212942,
        "step": 5117
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.164022445678711,
        "learning_rate": 1.332825229853507e-07,
        "epoch": 0.6597061098221191,
        "step": 5118
    },
    {
        "loss": 2.3892,
        "grad_norm": 1.913709282875061,
        "learning_rate": 1.2887868881676923e-07,
        "epoch": 0.659835009022944,
        "step": 5119
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.8559237718582153,
        "learning_rate": 1.245487424618108e-07,
        "epoch": 0.659963908223769,
        "step": 5120
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.402606725692749,
        "learning_rate": 1.2029269033561607e-07,
        "epoch": 0.660092807424594,
        "step": 5121
    },
    {
        "loss": 1.8845,
        "grad_norm": 2.1358842849731445,
        "learning_rate": 1.1611053874384659e-07,
        "epoch": 0.6602217066254189,
        "step": 5122
    },
    {
        "loss": 1.701,
        "grad_norm": 3.184755325317383,
        "learning_rate": 1.1200229388267369e-07,
        "epoch": 0.6603506058262438,
        "step": 5123
    },
    {
        "loss": 2.5497,
        "grad_norm": 2.0564115047454834,
        "learning_rate": 1.0796796183877855e-07,
        "epoch": 0.6604795050270689,
        "step": 5124
    },
    {
        "loss": 1.2769,
        "grad_norm": 2.767925262451172,
        "learning_rate": 1.0400754858931328e-07,
        "epoch": 0.6606084042278938,
        "step": 5125
    },
    {
        "loss": 1.4996,
        "grad_norm": 3.0298500061035156,
        "learning_rate": 1.0012106000193422e-07,
        "epoch": 0.6607373034287187,
        "step": 5126
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.2896909713745117,
        "learning_rate": 9.63085018347687e-08,
        "epoch": 0.6608662026295437,
        "step": 5127
    },
    {
        "loss": 1.9968,
        "grad_norm": 1.7195768356323242,
        "learning_rate": 9.25698797364094e-08,
        "epoch": 0.6609951018303687,
        "step": 5128
    },
    {
        "loss": 1.6148,
        "grad_norm": 3.3558685779571533,
        "learning_rate": 8.89051992458978e-08,
        "epoch": 0.6611240010311936,
        "step": 5129
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.6235902309417725,
        "learning_rate": 8.531446579274071e-08,
        "epoch": 0.6612529002320185,
        "step": 5130
    },
    {
        "loss": 1.5823,
        "grad_norm": 2.196282386779785,
        "learning_rate": 8.17976846968771e-08,
        "epoch": 0.6613817994328435,
        "step": 5131
    },
    {
        "loss": 2.2516,
        "grad_norm": 1.9520593881607056,
        "learning_rate": 7.835486116867796e-08,
        "epoch": 0.6615106986336685,
        "step": 5132
    },
    {
        "loss": 1.9745,
        "grad_norm": 1.2520285844802856,
        "learning_rate": 7.498600030895752e-08,
        "epoch": 0.6616395978344934,
        "step": 5133
    },
    {
        "loss": 1.4517,
        "grad_norm": 2.780327796936035,
        "learning_rate": 7.169110710892324e-08,
        "epoch": 0.6617684970353184,
        "step": 5134
    },
    {
        "loss": 2.311,
        "grad_norm": 2.061729669570923,
        "learning_rate": 6.847018645020908e-08,
        "epoch": 0.6618973962361433,
        "step": 5135
    },
    {
        "loss": 2.1044,
        "grad_norm": 1.7723612785339355,
        "learning_rate": 6.532324310485894e-08,
        "epoch": 0.6620262954369683,
        "step": 5136
    },
    {
        "loss": 2.328,
        "grad_norm": 3.360656261444092,
        "learning_rate": 6.225028173529878e-08,
        "epoch": 0.6621551946377933,
        "step": 5137
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.5974677801132202,
        "learning_rate": 5.925130689435343e-08,
        "epoch": 0.6622840938386182,
        "step": 5138
    },
    {
        "loss": 2.3071,
        "grad_norm": 2.6675684452056885,
        "learning_rate": 5.6326323025229774e-08,
        "epoch": 0.6624129930394431,
        "step": 5139
    },
    {
        "loss": 1.4702,
        "grad_norm": 2.9608094692230225,
        "learning_rate": 5.347533446151687e-08,
        "epoch": 0.6625418922402682,
        "step": 5140
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.3047876358032227,
        "learning_rate": 5.069834542715812e-08,
        "epoch": 0.6626707914410931,
        "step": 5141
    },
    {
        "loss": 0.9136,
        "grad_norm": 3.5347330570220947,
        "learning_rate": 4.799536003647353e-08,
        "epoch": 0.662799690641918,
        "step": 5142
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.6569055318832397,
        "learning_rate": 4.536638229414858e-08,
        "epoch": 0.6629285898427429,
        "step": 5143
    },
    {
        "loss": 2.3386,
        "grad_norm": 2.0317623615264893,
        "learning_rate": 4.28114160952009e-08,
        "epoch": 0.663057489043568,
        "step": 5144
    },
    {
        "loss": 2.0065,
        "grad_norm": 2.002917528152466,
        "learning_rate": 4.03304652250025e-08,
        "epoch": 0.6631863882443929,
        "step": 5145
    },
    {
        "loss": 1.0728,
        "grad_norm": 4.87143611907959,
        "learning_rate": 3.792353335928533e-08,
        "epoch": 0.6633152874452178,
        "step": 5146
    },
    {
        "loss": 1.9966,
        "grad_norm": 2.676287889480591,
        "learning_rate": 3.55906240640802e-08,
        "epoch": 0.6634441866460428,
        "step": 5147
    },
    {
        "loss": 1.736,
        "grad_norm": 2.733582019805908,
        "learning_rate": 3.3331740795783384e-08,
        "epoch": 0.6635730858468677,
        "step": 5148
    },
    {
        "loss": 2.3041,
        "grad_norm": 1.4971226453781128,
        "learning_rate": 3.1146886901090025e-08,
        "epoch": 0.6637019850476927,
        "step": 5149
    },
    {
        "loss": 2.1931,
        "grad_norm": 1.6954203844070435,
        "learning_rate": 2.903606561702743e-08,
        "epoch": 0.6638308842485177,
        "step": 5150
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.6262121200561523,
        "learning_rate": 2.6999280070938436e-08,
        "epoch": 0.6639597834493426,
        "step": 5151
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.426274538040161,
        "learning_rate": 2.5036533280470285e-08,
        "epoch": 0.6640886826501675,
        "step": 5152
    },
    {
        "loss": 1.8678,
        "grad_norm": 2.5333333015441895,
        "learning_rate": 2.314782815358574e-08,
        "epoch": 0.6642175818509926,
        "step": 5153
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.9185582399368286,
        "learning_rate": 2.1333167488535312e-08,
        "epoch": 0.6643464810518175,
        "step": 5154
    },
    {
        "loss": 1.9459,
        "grad_norm": 2.9352667331695557,
        "learning_rate": 1.959255397387949e-08,
        "epoch": 0.6644753802526424,
        "step": 5155
    },
    {
        "loss": 1.9479,
        "grad_norm": 1.8970824480056763,
        "learning_rate": 1.7925990188472075e-08,
        "epoch": 0.6646042794534673,
        "step": 5156
    },
    {
        "loss": 2.1725,
        "grad_norm": 1.495253324508667,
        "learning_rate": 1.633347860144907e-08,
        "epoch": 0.6647331786542924,
        "step": 5157
    },
    {
        "loss": 1.7685,
        "grad_norm": 1.9551081657409668,
        "learning_rate": 1.4815021572228694e-08,
        "epoch": 0.6648620778551173,
        "step": 5158
    },
    {
        "loss": 2.1562,
        "grad_norm": 1.6131535768508911,
        "learning_rate": 1.3370621350533575e-08,
        "epoch": 0.6649909770559422,
        "step": 5159
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.6768549680709839,
        "learning_rate": 1.2000280076335246e-08,
        "epoch": 0.6651198762567672,
        "step": 5160
    },
    {
        "loss": 1.6082,
        "grad_norm": 1.8547852039337158,
        "learning_rate": 1.0703999779909657e-08,
        "epoch": 0.6652487754575922,
        "step": 5161
    },
    {
        "loss": 2.4517,
        "grad_norm": 3.0480387210845947,
        "learning_rate": 9.481782381792759e-09,
        "epoch": 0.6653776746584171,
        "step": 5162
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.395160675048828,
        "learning_rate": 8.33362969278606e-09,
        "epoch": 0.6655065738592421,
        "step": 5163
    },
    {
        "loss": 2.1787,
        "grad_norm": 1.928530216217041,
        "learning_rate": 7.259543413962178e-09,
        "epoch": 0.665635473060067,
        "step": 5164
    },
    {
        "loss": 2.122,
        "grad_norm": 2.596467971801758,
        "learning_rate": 6.259525136670386e-09,
        "epoch": 0.665764372260892,
        "step": 5165
    },
    {
        "loss": 1.5785,
        "grad_norm": 2.8471522331237793,
        "learning_rate": 5.3335763424977595e-09,
        "epoch": 0.665893271461717,
        "step": 5166
    },
    {
        "loss": 2.2548,
        "grad_norm": 2.062800645828247,
        "learning_rate": 4.481698403324685e-09,
        "epoch": 0.6660221706625419,
        "step": 5167
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.5386402606964111,
        "learning_rate": 3.7038925812582505e-09,
        "epoch": 0.6661510698633668,
        "step": 5168
    },
    {
        "loss": 2.3326,
        "grad_norm": 1.1584153175354004,
        "learning_rate": 3.0001600286877483e-09,
        "epoch": 0.6662799690641918,
        "step": 5169
    },
    {
        "loss": 2.1482,
        "grad_norm": 1.8675518035888672,
        "learning_rate": 2.3705017882347246e-09,
        "epoch": 0.6664088682650168,
        "step": 5170
    },
    {
        "loss": 1.2065,
        "grad_norm": 5.246448040008545,
        "learning_rate": 1.8149187927918309e-09,
        "epoch": 0.6665377674658417,
        "step": 5171
    },
    {
        "loss": 1.9757,
        "grad_norm": 3.235326051712036,
        "learning_rate": 1.333411865495071e-09,
        "epoch": 0.6666666666666666,
        "step": 5172
    },
    {
        "loss": 1.1896,
        "grad_norm": 3.3072519302368164,
        "learning_rate": 9.25981719734903e-10,
        "epoch": 0.6667955658674917,
        "step": 5173
    },
    {
        "loss": 2.418,
        "grad_norm": 1.6096336841583252,
        "learning_rate": 5.926289591506873e-10,
        "epoch": 0.6669244650683166,
        "step": 5174
    },
    {
        "loss": 2.1105,
        "grad_norm": 1.292833685874939,
        "learning_rate": 3.3335407762513646e-10,
        "epoch": 0.6670533642691415,
        "step": 5175
    },
    {
        "loss": 1.8586,
        "grad_norm": 2.428818464279175,
        "learning_rate": 1.4815745929541713e-10,
        "epoch": 0.6671822634699665,
        "step": 5176
    },
    {
        "loss": 1.3827,
        "grad_norm": 2.9813222885131836,
        "learning_rate": 3.703937854204753e-11,
        "epoch": 0.6673111626707915,
        "step": 5177
    },
    {
        "loss": 2.4315,
        "grad_norm": 1.300218105316162,
        "learning_rate": 0.0001,
        "epoch": 0.6674400618716164,
        "step": 5178
    },
    {
        "loss": 1.173,
        "grad_norm": 3.0360405445098877,
        "learning_rate": 9.999996296062147e-05,
        "epoch": 0.6675689610724413,
        "step": 5179
    },
    {
        "loss": 2.0702,
        "grad_norm": 1.7970384359359741,
        "learning_rate": 9.999985184254071e-05,
        "epoch": 0.6676978602732663,
        "step": 5180
    },
    {
        "loss": 2.2977,
        "grad_norm": 2.3226840496063232,
        "learning_rate": 9.999966664592237e-05,
        "epoch": 0.6678267594740913,
        "step": 5181
    },
    {
        "loss": 2.043,
        "grad_norm": 2.311955213546753,
        "learning_rate": 9.999940737104086e-05,
        "epoch": 0.6679556586749162,
        "step": 5182
    },
    {
        "loss": 1.9933,
        "grad_norm": 2.333652973175049,
        "learning_rate": 9.999907401828026e-05,
        "epoch": 0.6680845578757412,
        "step": 5183
    },
    {
        "loss": 2.1266,
        "grad_norm": 2.1502535343170166,
        "learning_rate": 9.99986665881345e-05,
        "epoch": 0.6682134570765661,
        "step": 5184
    },
    {
        "loss": 2.0725,
        "grad_norm": 1.8049982786178589,
        "learning_rate": 9.999818508120722e-05,
        "epoch": 0.668342356277391,
        "step": 5185
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.9762283563613892,
        "learning_rate": 9.999762949821176e-05,
        "epoch": 0.6684712554782161,
        "step": 5186
    },
    {
        "loss": 2.3785,
        "grad_norm": 1.7409441471099854,
        "learning_rate": 9.999699983997131e-05,
        "epoch": 0.668600154679041,
        "step": 5187
    },
    {
        "loss": 1.7309,
        "grad_norm": 1.9587502479553223,
        "learning_rate": 9.999629610741875e-05,
        "epoch": 0.6687290538798659,
        "step": 5188
    },
    {
        "loss": 2.3224,
        "grad_norm": 2.2009780406951904,
        "learning_rate": 9.999551830159668e-05,
        "epoch": 0.6688579530806908,
        "step": 5189
    },
    {
        "loss": 1.4303,
        "grad_norm": 1.4510223865509033,
        "learning_rate": 9.999466642365752e-05,
        "epoch": 0.6689868522815159,
        "step": 5190
    },
    {
        "loss": 2.5626,
        "grad_norm": 1.953495740890503,
        "learning_rate": 9.999374047486334e-05,
        "epoch": 0.6691157514823408,
        "step": 5191
    },
    {
        "loss": 1.4398,
        "grad_norm": 1.7456403970718384,
        "learning_rate": 9.999274045658605e-05,
        "epoch": 0.6692446506831657,
        "step": 5192
    },
    {
        "loss": 1.6523,
        "grad_norm": 3.710963249206543,
        "learning_rate": 9.999166637030721e-05,
        "epoch": 0.6693735498839907,
        "step": 5193
    },
    {
        "loss": 1.9658,
        "grad_norm": 2.489123582839966,
        "learning_rate": 9.999051821761821e-05,
        "epoch": 0.6695024490848157,
        "step": 5194
    },
    {
        "loss": 2.4464,
        "grad_norm": 2.0413668155670166,
        "learning_rate": 9.998929600022008e-05,
        "epoch": 0.6696313482856406,
        "step": 5195
    },
    {
        "loss": 1.975,
        "grad_norm": 1.7516961097717285,
        "learning_rate": 9.998799971992367e-05,
        "epoch": 0.6697602474864656,
        "step": 5196
    },
    {
        "loss": 2.0956,
        "grad_norm": 2.485262870788574,
        "learning_rate": 9.998662937864947e-05,
        "epoch": 0.6698891466872905,
        "step": 5197
    },
    {
        "loss": 1.4574,
        "grad_norm": 3.5941615104675293,
        "learning_rate": 9.998518497842777e-05,
        "epoch": 0.6700180458881155,
        "step": 5198
    },
    {
        "loss": 1.5953,
        "grad_norm": 2.141366481781006,
        "learning_rate": 9.998366652139856e-05,
        "epoch": 0.6701469450889405,
        "step": 5199
    },
    {
        "loss": 1.5408,
        "grad_norm": 2.1242587566375732,
        "learning_rate": 9.998207400981154e-05,
        "epoch": 0.6702758442897654,
        "step": 5200
    },
    {
        "loss": 0.9332,
        "grad_norm": 3.4138433933258057,
        "learning_rate": 9.998040744602613e-05,
        "epoch": 0.6704047434905903,
        "step": 5201
    },
    {
        "loss": 1.9449,
        "grad_norm": 2.3957669734954834,
        "learning_rate": 9.997866683251147e-05,
        "epoch": 0.6705336426914154,
        "step": 5202
    },
    {
        "loss": 2.5281,
        "grad_norm": 1.7277612686157227,
        "learning_rate": 9.997685217184643e-05,
        "epoch": 0.6706625418922403,
        "step": 5203
    },
    {
        "loss": 1.9811,
        "grad_norm": 1.7839207649230957,
        "learning_rate": 9.997496346671953e-05,
        "epoch": 0.6707914410930652,
        "step": 5204
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.8549647331237793,
        "learning_rate": 9.997300071992907e-05,
        "epoch": 0.6709203402938901,
        "step": 5205
    },
    {
        "loss": 1.7955,
        "grad_norm": 2.469600200653076,
        "learning_rate": 9.997096393438298e-05,
        "epoch": 0.6710492394947152,
        "step": 5206
    },
    {
        "loss": 2.1106,
        "grad_norm": 1.5921002626419067,
        "learning_rate": 9.996885311309891e-05,
        "epoch": 0.6711781386955401,
        "step": 5207
    },
    {
        "loss": 1.7295,
        "grad_norm": 1.9463012218475342,
        "learning_rate": 9.996666825920422e-05,
        "epoch": 0.671307037896365,
        "step": 5208
    },
    {
        "loss": 1.799,
        "grad_norm": 3.772322177886963,
        "learning_rate": 9.996440937593592e-05,
        "epoch": 0.67143593709719,
        "step": 5209
    },
    {
        "loss": 1.7756,
        "grad_norm": 1.8811442852020264,
        "learning_rate": 9.996207646664073e-05,
        "epoch": 0.671564836298015,
        "step": 5210
    },
    {
        "loss": 2.1389,
        "grad_norm": 2.477736473083496,
        "learning_rate": 9.9959669534775e-05,
        "epoch": 0.6716937354988399,
        "step": 5211
    },
    {
        "loss": 2.3322,
        "grad_norm": 1.441414713859558,
        "learning_rate": 9.99571885839048e-05,
        "epoch": 0.6718226346996649,
        "step": 5212
    },
    {
        "loss": 2.4719,
        "grad_norm": 1.7862805128097534,
        "learning_rate": 9.995463361770587e-05,
        "epoch": 0.6719515339004898,
        "step": 5213
    },
    {
        "loss": 2.3465,
        "grad_norm": 2.1475353240966797,
        "learning_rate": 9.995200463996352e-05,
        "epoch": 0.6720804331013148,
        "step": 5214
    },
    {
        "loss": 2.2835,
        "grad_norm": 1.3571629524230957,
        "learning_rate": 9.994930165457285e-05,
        "epoch": 0.6722093323021398,
        "step": 5215
    },
    {
        "loss": 1.7161,
        "grad_norm": 2.1182522773742676,
        "learning_rate": 9.99465246655385e-05,
        "epoch": 0.6723382315029647,
        "step": 5216
    },
    {
        "loss": 2.4733,
        "grad_norm": 1.883983850479126,
        "learning_rate": 9.994367367697478e-05,
        "epoch": 0.6724671307037896,
        "step": 5217
    },
    {
        "loss": 2.1125,
        "grad_norm": 1.7926640510559082,
        "learning_rate": 9.994074869310565e-05,
        "epoch": 0.6725960299046146,
        "step": 5218
    },
    {
        "loss": 1.3657,
        "grad_norm": 3.0383472442626953,
        "learning_rate": 9.993774971826471e-05,
        "epoch": 0.6727249291054396,
        "step": 5219
    },
    {
        "loss": 2.2174,
        "grad_norm": 1.7844209671020508,
        "learning_rate": 9.993467675689513e-05,
        "epoch": 0.6728538283062645,
        "step": 5220
    },
    {
        "loss": 1.6249,
        "grad_norm": 2.7302749156951904,
        "learning_rate": 9.993152981354978e-05,
        "epoch": 0.6729827275070894,
        "step": 5221
    },
    {
        "loss": 2.3016,
        "grad_norm": 1.8294105529785156,
        "learning_rate": 9.992830889289109e-05,
        "epoch": 0.6731116267079144,
        "step": 5222
    },
    {
        "loss": 0.7309,
        "grad_norm": 2.9493777751922607,
        "learning_rate": 9.992501399969106e-05,
        "epoch": 0.6732405259087394,
        "step": 5223
    },
    {
        "loss": 1.6418,
        "grad_norm": 1.1317821741104126,
        "learning_rate": 9.992164513883132e-05,
        "epoch": 0.6733694251095643,
        "step": 5224
    },
    {
        "loss": 1.2568,
        "grad_norm": 4.2285590171813965,
        "learning_rate": 9.991820231530313e-05,
        "epoch": 0.6734983243103893,
        "step": 5225
    },
    {
        "loss": 1.3749,
        "grad_norm": 2.6720845699310303,
        "learning_rate": 9.991468553420726e-05,
        "epoch": 0.6736272235112142,
        "step": 5226
    },
    {
        "loss": 1.2149,
        "grad_norm": 3.4568469524383545,
        "learning_rate": 9.99110948007541e-05,
        "epoch": 0.6737561227120392,
        "step": 5227
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.4335907697677612,
        "learning_rate": 9.99074301202636e-05,
        "epoch": 0.6738850219128641,
        "step": 5228
    },
    {
        "loss": 0.4735,
        "grad_norm": 4.6489386558532715,
        "learning_rate": 9.990369149816523e-05,
        "epoch": 0.6740139211136891,
        "step": 5229
    },
    {
        "loss": 2.0477,
        "grad_norm": 3.121025323867798,
        "learning_rate": 9.989987893999807e-05,
        "epoch": 0.674142820314514,
        "step": 5230
    },
    {
        "loss": 2.3645,
        "grad_norm": 1.6386823654174805,
        "learning_rate": 9.989599245141069e-05,
        "epoch": 0.674271719515339,
        "step": 5231
    },
    {
        "loss": 1.7707,
        "grad_norm": 4.286370277404785,
        "learning_rate": 9.989203203816123e-05,
        "epoch": 0.674400618716164,
        "step": 5232
    },
    {
        "loss": 2.2379,
        "grad_norm": 3.201174020767212,
        "learning_rate": 9.988799770611732e-05,
        "epoch": 0.6745295179169889,
        "step": 5233
    },
    {
        "loss": 1.7097,
        "grad_norm": 3.1053502559661865,
        "learning_rate": 9.988388946125615e-05,
        "epoch": 0.6746584171178138,
        "step": 5234
    },
    {
        "loss": 1.8166,
        "grad_norm": 1.5975885391235352,
        "learning_rate": 9.987970730966438e-05,
        "epoch": 0.6747873163186389,
        "step": 5235
    },
    {
        "loss": 1.8764,
        "grad_norm": 2.9324586391448975,
        "learning_rate": 9.987545125753819e-05,
        "epoch": 0.6749162155194638,
        "step": 5236
    },
    {
        "loss": 2.0082,
        "grad_norm": 1.3963004350662231,
        "learning_rate": 9.987112131118323e-05,
        "epoch": 0.6750451147202887,
        "step": 5237
    },
    {
        "loss": 1.5522,
        "grad_norm": 2.9097042083740234,
        "learning_rate": 9.986671747701466e-05,
        "epoch": 0.6751740139211136,
        "step": 5238
    },
    {
        "loss": 2.2479,
        "grad_norm": 1.7272007465362549,
        "learning_rate": 9.986223976155706e-05,
        "epoch": 0.6753029131219387,
        "step": 5239
    },
    {
        "loss": 1.9197,
        "grad_norm": 2.4627912044525146,
        "learning_rate": 9.985768817144453e-05,
        "epoch": 0.6754318123227636,
        "step": 5240
    },
    {
        "loss": 2.1473,
        "grad_norm": 1.2902038097381592,
        "learning_rate": 9.985306271342058e-05,
        "epoch": 0.6755607115235885,
        "step": 5241
    },
    {
        "loss": 1.6081,
        "grad_norm": 3.584031105041504,
        "learning_rate": 9.984836339433816e-05,
        "epoch": 0.6756896107244135,
        "step": 5242
    },
    {
        "loss": 1.7675,
        "grad_norm": 3.214657783508301,
        "learning_rate": 9.98435902211597e-05,
        "epoch": 0.6758185099252385,
        "step": 5243
    },
    {
        "loss": 2.5194,
        "grad_norm": 1.923446536064148,
        "learning_rate": 9.983874320095698e-05,
        "epoch": 0.6759474091260634,
        "step": 5244
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.5475552082061768,
        "learning_rate": 9.983382234091126e-05,
        "epoch": 0.6760763083268884,
        "step": 5245
    },
    {
        "loss": 1.8064,
        "grad_norm": 1.8760031461715698,
        "learning_rate": 9.982882764831315e-05,
        "epoch": 0.6762052075277133,
        "step": 5246
    },
    {
        "loss": 2.656,
        "grad_norm": 2.238560914993286,
        "learning_rate": 9.982375913056262e-05,
        "epoch": 0.6763341067285383,
        "step": 5247
    },
    {
        "loss": 0.9417,
        "grad_norm": 2.9576873779296875,
        "learning_rate": 9.981861679516912e-05,
        "epoch": 0.6764630059293633,
        "step": 5248
    },
    {
        "loss": 2.167,
        "grad_norm": 2.37336802482605,
        "learning_rate": 9.981340064975137e-05,
        "epoch": 0.6765919051301882,
        "step": 5249
    },
    {
        "loss": 2.5316,
        "grad_norm": 1.8851181268692017,
        "learning_rate": 9.98081107020375e-05,
        "epoch": 0.6767208043310131,
        "step": 5250
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.6238584518432617,
        "learning_rate": 9.980274695986496e-05,
        "epoch": 0.6768497035318382,
        "step": 5251
    },
    {
        "loss": 1.917,
        "grad_norm": 1.9744532108306885,
        "learning_rate": 9.979730943118054e-05,
        "epoch": 0.6769786027326631,
        "step": 5252
    },
    {
        "loss": 1.4761,
        "grad_norm": 2.952862024307251,
        "learning_rate": 9.979179812404033e-05,
        "epoch": 0.677107501933488,
        "step": 5253
    },
    {
        "loss": 2.746,
        "grad_norm": 1.6531459093093872,
        "learning_rate": 9.978621304660977e-05,
        "epoch": 0.6772364011343129,
        "step": 5254
    },
    {
        "loss": 1.4306,
        "grad_norm": 2.7815840244293213,
        "learning_rate": 9.978055420716357e-05,
        "epoch": 0.677365300335138,
        "step": 5255
    },
    {
        "loss": 2.0342,
        "grad_norm": 2.297266960144043,
        "learning_rate": 9.977482161408568e-05,
        "epoch": 0.6774941995359629,
        "step": 5256
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.487393379211426,
        "learning_rate": 9.976901527586944e-05,
        "epoch": 0.6776230987367878,
        "step": 5257
    },
    {
        "loss": 1.8471,
        "grad_norm": 3.387075901031494,
        "learning_rate": 9.976313520111732e-05,
        "epoch": 0.6777519979376128,
        "step": 5258
    },
    {
        "loss": 2.1093,
        "grad_norm": 1.9866708517074585,
        "learning_rate": 9.975718139854113e-05,
        "epoch": 0.6778808971384377,
        "step": 5259
    },
    {
        "loss": 1.5465,
        "grad_norm": 3.326742649078369,
        "learning_rate": 9.975115387696185e-05,
        "epoch": 0.6780097963392627,
        "step": 5260
    },
    {
        "loss": 2.1444,
        "grad_norm": 1.8964133262634277,
        "learning_rate": 9.974505264530972e-05,
        "epoch": 0.6781386955400877,
        "step": 5261
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.1166586875915527,
        "learning_rate": 9.973887771262415e-05,
        "epoch": 0.6782675947409126,
        "step": 5262
    },
    {
        "loss": 1.0572,
        "grad_norm": 3.092543601989746,
        "learning_rate": 9.973262908805381e-05,
        "epoch": 0.6783964939417375,
        "step": 5263
    },
    {
        "loss": 1.1887,
        "grad_norm": 3.152251720428467,
        "learning_rate": 9.972630678085646e-05,
        "epoch": 0.6785253931425625,
        "step": 5264
    },
    {
        "loss": 2.4681,
        "grad_norm": 1.9835047721862793,
        "learning_rate": 9.97199108003991e-05,
        "epoch": 0.6786542923433875,
        "step": 5265
    },
    {
        "loss": 1.4776,
        "grad_norm": 2.860748767852783,
        "learning_rate": 9.971344115615785e-05,
        "epoch": 0.6787831915442124,
        "step": 5266
    },
    {
        "loss": 1.7961,
        "grad_norm": 2.888514518737793,
        "learning_rate": 9.970689785771798e-05,
        "epoch": 0.6789120907450373,
        "step": 5267
    },
    {
        "loss": 2.2123,
        "grad_norm": 2.4974420070648193,
        "learning_rate": 9.970028091477388e-05,
        "epoch": 0.6790409899458624,
        "step": 5268
    },
    {
        "loss": 2.3249,
        "grad_norm": 2.020171880722046,
        "learning_rate": 9.969359033712903e-05,
        "epoch": 0.6791698891466873,
        "step": 5269
    },
    {
        "loss": 1.7286,
        "grad_norm": 1.9738889932632446,
        "learning_rate": 9.968682613469602e-05,
        "epoch": 0.6792987883475122,
        "step": 5270
    },
    {
        "loss": 1.857,
        "grad_norm": 2.2369918823242188,
        "learning_rate": 9.967998831749656e-05,
        "epoch": 0.6794276875483372,
        "step": 5271
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.9304773807525635,
        "learning_rate": 9.967307689566137e-05,
        "epoch": 0.6795565867491622,
        "step": 5272
    },
    {
        "loss": 1.7664,
        "grad_norm": 3.9005541801452637,
        "learning_rate": 9.966609187943023e-05,
        "epoch": 0.6796854859499871,
        "step": 5273
    },
    {
        "loss": 1.3498,
        "grad_norm": 4.466278076171875,
        "learning_rate": 9.965903327915198e-05,
        "epoch": 0.679814385150812,
        "step": 5274
    },
    {
        "loss": 1.653,
        "grad_norm": 3.028930902481079,
        "learning_rate": 9.965190110528446e-05,
        "epoch": 0.679943284351637,
        "step": 5275
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.3444302082061768,
        "learning_rate": 9.964469536839452e-05,
        "epoch": 0.680072183552462,
        "step": 5276
    },
    {
        "loss": 1.7214,
        "grad_norm": 2.528352737426758,
        "learning_rate": 9.963741607915802e-05,
        "epoch": 0.680201082753287,
        "step": 5277
    },
    {
        "loss": 2.5605,
        "grad_norm": 1.8022792339324951,
        "learning_rate": 9.963006324835975e-05,
        "epoch": 0.6803299819541119,
        "step": 5278
    },
    {
        "loss": 1.4181,
        "grad_norm": 4.297356605529785,
        "learning_rate": 9.962263688689349e-05,
        "epoch": 0.6804588811549368,
        "step": 5279
    },
    {
        "loss": 2.0359,
        "grad_norm": 2.799983263015747,
        "learning_rate": 9.961513700576195e-05,
        "epoch": 0.6805877803557618,
        "step": 5280
    },
    {
        "loss": 1.8112,
        "grad_norm": 4.455437183380127,
        "learning_rate": 9.960756361607677e-05,
        "epoch": 0.6807166795565868,
        "step": 5281
    },
    {
        "loss": 1.7865,
        "grad_norm": 2.917198657989502,
        "learning_rate": 9.95999167290585e-05,
        "epoch": 0.6808455787574117,
        "step": 5282
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.689628005027771,
        "learning_rate": 9.959219635603658e-05,
        "epoch": 0.6809744779582366,
        "step": 5283
    },
    {
        "loss": 1.2494,
        "grad_norm": 3.188993453979492,
        "learning_rate": 9.958440250844929e-05,
        "epoch": 0.6811033771590617,
        "step": 5284
    },
    {
        "loss": 2.2833,
        "grad_norm": 1.9081288576126099,
        "learning_rate": 9.957653519784384e-05,
        "epoch": 0.6812322763598866,
        "step": 5285
    },
    {
        "loss": 2.5675,
        "grad_norm": 1.3222802877426147,
        "learning_rate": 9.956859443587625e-05,
        "epoch": 0.6813611755607115,
        "step": 5286
    },
    {
        "loss": 2.6605,
        "grad_norm": 2.006619453430176,
        "learning_rate": 9.956058023431132e-05,
        "epoch": 0.6814900747615364,
        "step": 5287
    },
    {
        "loss": 1.8314,
        "grad_norm": 2.386120080947876,
        "learning_rate": 9.95524926050227e-05,
        "epoch": 0.6816189739623615,
        "step": 5288
    },
    {
        "loss": 2.3314,
        "grad_norm": 2.0382063388824463,
        "learning_rate": 9.954433155999283e-05,
        "epoch": 0.6817478731631864,
        "step": 5289
    },
    {
        "loss": 2.4504,
        "grad_norm": 1.3746498823165894,
        "learning_rate": 9.95360971113129e-05,
        "epoch": 0.6818767723640113,
        "step": 5290
    },
    {
        "loss": 2.3058,
        "grad_norm": 2.1487154960632324,
        "learning_rate": 9.952778927118289e-05,
        "epoch": 0.6820056715648363,
        "step": 5291
    },
    {
        "loss": 1.5059,
        "grad_norm": 2.7273809909820557,
        "learning_rate": 9.951940805191147e-05,
        "epoch": 0.6821345707656613,
        "step": 5292
    },
    {
        "loss": 2.0969,
        "grad_norm": 2.738020181655884,
        "learning_rate": 9.951095346591606e-05,
        "epoch": 0.6822634699664862,
        "step": 5293
    },
    {
        "loss": 1.8271,
        "grad_norm": 1.8635389804840088,
        "learning_rate": 9.950242552572271e-05,
        "epoch": 0.6823923691673112,
        "step": 5294
    },
    {
        "loss": 2.0244,
        "grad_norm": 1.757743239402771,
        "learning_rate": 9.949382424396628e-05,
        "epoch": 0.6825212683681361,
        "step": 5295
    },
    {
        "loss": 2.5427,
        "grad_norm": 1.887067437171936,
        "learning_rate": 9.948514963339018e-05,
        "epoch": 0.682650167568961,
        "step": 5296
    },
    {
        "loss": 1.944,
        "grad_norm": 1.5385026931762695,
        "learning_rate": 9.947640170684649e-05,
        "epoch": 0.6827790667697861,
        "step": 5297
    },
    {
        "loss": 1.9757,
        "grad_norm": 1.9067022800445557,
        "learning_rate": 9.946758047729594e-05,
        "epoch": 0.682907965970611,
        "step": 5298
    },
    {
        "loss": 2.1143,
        "grad_norm": 1.7714647054672241,
        "learning_rate": 9.945868595780786e-05,
        "epoch": 0.6830368651714359,
        "step": 5299
    },
    {
        "loss": 1.1782,
        "grad_norm": 2.4989206790924072,
        "learning_rate": 9.944971816156006e-05,
        "epoch": 0.6831657643722608,
        "step": 5300
    },
    {
        "loss": 1.5319,
        "grad_norm": 3.3679039478302,
        "learning_rate": 9.944067710183911e-05,
        "epoch": 0.6832946635730859,
        "step": 5301
    },
    {
        "loss": 1.8688,
        "grad_norm": 2.1843464374542236,
        "learning_rate": 9.943156279203997e-05,
        "epoch": 0.6834235627739108,
        "step": 5302
    },
    {
        "loss": 2.1352,
        "grad_norm": 1.8179625272750854,
        "learning_rate": 9.942237524566618e-05,
        "epoch": 0.6835524619747357,
        "step": 5303
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.884769082069397,
        "learning_rate": 9.941311447632979e-05,
        "epoch": 0.6836813611755607,
        "step": 5304
    },
    {
        "loss": 2.2839,
        "grad_norm": 1.7281420230865479,
        "learning_rate": 9.940378049775129e-05,
        "epoch": 0.6838102603763857,
        "step": 5305
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.5528695583343506,
        "learning_rate": 9.939437332375974e-05,
        "epoch": 0.6839391595772106,
        "step": 5306
    },
    {
        "loss": 1.9807,
        "grad_norm": 1.6813002824783325,
        "learning_rate": 9.93848929682925e-05,
        "epoch": 0.6840680587780356,
        "step": 5307
    },
    {
        "loss": 1.5254,
        "grad_norm": 2.6066248416900635,
        "learning_rate": 9.937533944539547e-05,
        "epoch": 0.6841969579788605,
        "step": 5308
    },
    {
        "loss": 1.587,
        "grad_norm": 2.8499162197113037,
        "learning_rate": 9.936571276922292e-05,
        "epoch": 0.6843258571796855,
        "step": 5309
    },
    {
        "loss": 2.134,
        "grad_norm": 2.3068902492523193,
        "learning_rate": 9.935601295403748e-05,
        "epoch": 0.6844547563805105,
        "step": 5310
    },
    {
        "loss": 1.4708,
        "grad_norm": 2.795475482940674,
        "learning_rate": 9.934624001421012e-05,
        "epoch": 0.6845836555813354,
        "step": 5311
    },
    {
        "loss": 1.9157,
        "grad_norm": 1.845610499382019,
        "learning_rate": 9.933639396422025e-05,
        "epoch": 0.6847125547821603,
        "step": 5312
    },
    {
        "loss": 1.5652,
        "grad_norm": 2.6148362159729004,
        "learning_rate": 9.93264748186555e-05,
        "epoch": 0.6848414539829853,
        "step": 5313
    },
    {
        "loss": 2.4101,
        "grad_norm": 2.269331932067871,
        "learning_rate": 9.931648259221181e-05,
        "epoch": 0.6849703531838103,
        "step": 5314
    },
    {
        "loss": 1.5332,
        "grad_norm": 3.6590356826782227,
        "learning_rate": 9.930641729969344e-05,
        "epoch": 0.6850992523846352,
        "step": 5315
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.775745391845703,
        "learning_rate": 9.929627895601288e-05,
        "epoch": 0.6852281515854601,
        "step": 5316
    },
    {
        "loss": 2.4365,
        "grad_norm": 2.477553606033325,
        "learning_rate": 9.928606757619084e-05,
        "epoch": 0.6853570507862852,
        "step": 5317
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.7467976808547974,
        "learning_rate": 9.927578317535625e-05,
        "epoch": 0.6854859499871101,
        "step": 5318
    },
    {
        "loss": 2.4496,
        "grad_norm": 2.651827812194824,
        "learning_rate": 9.926542576874622e-05,
        "epoch": 0.685614849187935,
        "step": 5319
    },
    {
        "loss": 2.1275,
        "grad_norm": 1.84149968624115,
        "learning_rate": 9.925499537170602e-05,
        "epoch": 0.68574374838876,
        "step": 5320
    },
    {
        "loss": 1.9428,
        "grad_norm": 2.2224645614624023,
        "learning_rate": 9.924449199968906e-05,
        "epoch": 0.685872647589585,
        "step": 5321
    },
    {
        "loss": 1.1126,
        "grad_norm": 3.6478970050811768,
        "learning_rate": 9.923391566825692e-05,
        "epoch": 0.6860015467904099,
        "step": 5322
    },
    {
        "loss": 1.8532,
        "grad_norm": 2.01421856880188,
        "learning_rate": 9.922326639307917e-05,
        "epoch": 0.6861304459912348,
        "step": 5323
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.4536304473876953,
        "learning_rate": 9.921254418993355e-05,
        "epoch": 0.6862593451920598,
        "step": 5324
    },
    {
        "loss": 1.4018,
        "grad_norm": 2.383131504058838,
        "learning_rate": 9.92017490747058e-05,
        "epoch": 0.6863882443928848,
        "step": 5325
    },
    {
        "loss": 2.341,
        "grad_norm": 2.154959201812744,
        "learning_rate": 9.919088106338969e-05,
        "epoch": 0.6865171435937097,
        "step": 5326
    },
    {
        "loss": 1.8336,
        "grad_norm": 2.3175487518310547,
        "learning_rate": 9.917994017208699e-05,
        "epoch": 0.6866460427945347,
        "step": 5327
    },
    {
        "loss": 1.4802,
        "grad_norm": 2.889709234237671,
        "learning_rate": 9.916892641700746e-05,
        "epoch": 0.6867749419953596,
        "step": 5328
    },
    {
        "loss": 1.5548,
        "grad_norm": 2.4772839546203613,
        "learning_rate": 9.915783981446882e-05,
        "epoch": 0.6869038411961846,
        "step": 5329
    },
    {
        "loss": 2.1247,
        "grad_norm": 1.9025071859359741,
        "learning_rate": 9.914668038089669e-05,
        "epoch": 0.6870327403970096,
        "step": 5330
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.5599536895751953,
        "learning_rate": 9.913544813282461e-05,
        "epoch": 0.6871616395978345,
        "step": 5331
    },
    {
        "loss": 2.3785,
        "grad_norm": 1.6734353303909302,
        "learning_rate": 9.912414308689399e-05,
        "epoch": 0.6872905387986594,
        "step": 5332
    },
    {
        "loss": 2.1532,
        "grad_norm": 2.4250266551971436,
        "learning_rate": 9.91127652598541e-05,
        "epoch": 0.6874194379994844,
        "step": 5333
    },
    {
        "loss": 2.2084,
        "grad_norm": 2.3109726905822754,
        "learning_rate": 9.910131466856209e-05,
        "epoch": 0.6875483372003094,
        "step": 5334
    },
    {
        "loss": 1.8075,
        "grad_norm": 2.351217269897461,
        "learning_rate": 9.908979132998284e-05,
        "epoch": 0.6876772364011343,
        "step": 5335
    },
    {
        "loss": 1.9862,
        "grad_norm": 2.804330825805664,
        "learning_rate": 9.907819526118902e-05,
        "epoch": 0.6878061356019592,
        "step": 5336
    },
    {
        "loss": 1.5896,
        "grad_norm": 2.658397674560547,
        "learning_rate": 9.906652647936113e-05,
        "epoch": 0.6879350348027842,
        "step": 5337
    },
    {
        "loss": 1.8701,
        "grad_norm": 2.3500330448150635,
        "learning_rate": 9.90547850017873e-05,
        "epoch": 0.6880639340036092,
        "step": 5338
    },
    {
        "loss": 2.1625,
        "grad_norm": 2.454190492630005,
        "learning_rate": 9.904297084586342e-05,
        "epoch": 0.6881928332044341,
        "step": 5339
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.1692922115325928,
        "learning_rate": 9.903108402909309e-05,
        "epoch": 0.6883217324052591,
        "step": 5340
    },
    {
        "loss": 1.6854,
        "grad_norm": 3.2963175773620605,
        "learning_rate": 9.901912456908749e-05,
        "epoch": 0.688450631606084,
        "step": 5341
    },
    {
        "loss": 1.1433,
        "grad_norm": 3.1789157390594482,
        "learning_rate": 9.900709248356544e-05,
        "epoch": 0.688579530806909,
        "step": 5342
    },
    {
        "loss": 2.1546,
        "grad_norm": 2.5223042964935303,
        "learning_rate": 9.899498779035339e-05,
        "epoch": 0.688708430007734,
        "step": 5343
    },
    {
        "loss": 1.4587,
        "grad_norm": 3.0517079830169678,
        "learning_rate": 9.898281050738539e-05,
        "epoch": 0.6888373292085589,
        "step": 5344
    },
    {
        "loss": 2.1892,
        "grad_norm": 1.9499621391296387,
        "learning_rate": 9.897056065270294e-05,
        "epoch": 0.6889662284093838,
        "step": 5345
    },
    {
        "loss": 1.0826,
        "grad_norm": 2.6265828609466553,
        "learning_rate": 9.895823824445518e-05,
        "epoch": 0.6890951276102089,
        "step": 5346
    },
    {
        "loss": 1.1357,
        "grad_norm": 4.023231506347656,
        "learning_rate": 9.894584330089867e-05,
        "epoch": 0.6892240268110338,
        "step": 5347
    },
    {
        "loss": 2.1196,
        "grad_norm": 2.177320957183838,
        "learning_rate": 9.89333758403974e-05,
        "epoch": 0.6893529260118587,
        "step": 5348
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.7232975959777832,
        "learning_rate": 9.89208358814229e-05,
        "epoch": 0.6894818252126836,
        "step": 5349
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.2438430786132812,
        "learning_rate": 9.890822344255403e-05,
        "epoch": 0.6896107244135087,
        "step": 5350
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.2847355604171753,
        "learning_rate": 9.88955385424771e-05,
        "epoch": 0.6897396236143336,
        "step": 5351
    },
    {
        "loss": 0.9795,
        "grad_norm": 3.4133834838867188,
        "learning_rate": 9.888278119998571e-05,
        "epoch": 0.6898685228151585,
        "step": 5352
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.5992590188980103,
        "learning_rate": 9.886995143398085e-05,
        "epoch": 0.6899974220159835,
        "step": 5353
    },
    {
        "loss": 2.7233,
        "grad_norm": 1.7912017107009888,
        "learning_rate": 9.885704926347079e-05,
        "epoch": 0.6901263212168085,
        "step": 5354
    },
    {
        "loss": 2.0497,
        "grad_norm": 2.667879104614258,
        "learning_rate": 9.884407470757101e-05,
        "epoch": 0.6902552204176334,
        "step": 5355
    },
    {
        "loss": 2.021,
        "grad_norm": 2.4299702644348145,
        "learning_rate": 9.883102778550433e-05,
        "epoch": 0.6903841196184584,
        "step": 5356
    },
    {
        "loss": 2.1673,
        "grad_norm": 1.7644978761672974,
        "learning_rate": 9.881790851660075e-05,
        "epoch": 0.6905130188192833,
        "step": 5357
    },
    {
        "loss": 1.7754,
        "grad_norm": 2.171527624130249,
        "learning_rate": 9.880471692029743e-05,
        "epoch": 0.6906419180201083,
        "step": 5358
    },
    {
        "loss": 2.2494,
        "grad_norm": 2.0191922187805176,
        "learning_rate": 9.879145301613873e-05,
        "epoch": 0.6907708172209333,
        "step": 5359
    },
    {
        "loss": 1.7965,
        "grad_norm": 3.5383100509643555,
        "learning_rate": 9.877811682377614e-05,
        "epoch": 0.6908997164217582,
        "step": 5360
    },
    {
        "loss": 1.7345,
        "grad_norm": 2.0199239253997803,
        "learning_rate": 9.876470836296815e-05,
        "epoch": 0.6910286156225831,
        "step": 5361
    },
    {
        "loss": 2.0801,
        "grad_norm": 1.497113823890686,
        "learning_rate": 9.875122765358049e-05,
        "epoch": 0.6911575148234081,
        "step": 5362
    },
    {
        "loss": 1.925,
        "grad_norm": 1.9515405893325806,
        "learning_rate": 9.873767471558581e-05,
        "epoch": 0.6912864140242331,
        "step": 5363
    },
    {
        "loss": 2.0132,
        "grad_norm": 2.8962831497192383,
        "learning_rate": 9.87240495690638e-05,
        "epoch": 0.691415313225058,
        "step": 5364
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.7606830596923828,
        "learning_rate": 9.871035223420115e-05,
        "epoch": 0.6915442124258829,
        "step": 5365
    },
    {
        "loss": 2.5387,
        "grad_norm": 2.915858745574951,
        "learning_rate": 9.86965827312915e-05,
        "epoch": 0.6916731116267079,
        "step": 5366
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.8530116081237793,
        "learning_rate": 9.868274108073537e-05,
        "epoch": 0.6918020108275329,
        "step": 5367
    },
    {
        "loss": 1.2901,
        "grad_norm": 3.1594645977020264,
        "learning_rate": 9.866882730304023e-05,
        "epoch": 0.6919309100283578,
        "step": 5368
    },
    {
        "loss": 2.3211,
        "grad_norm": 2.6869306564331055,
        "learning_rate": 9.865484141882038e-05,
        "epoch": 0.6920598092291828,
        "step": 5369
    },
    {
        "loss": 1.7059,
        "grad_norm": 2.755171775817871,
        "learning_rate": 9.864078344879694e-05,
        "epoch": 0.6921887084300077,
        "step": 5370
    },
    {
        "loss": 2.1953,
        "grad_norm": 1.9395227432250977,
        "learning_rate": 9.862665341379791e-05,
        "epoch": 0.6923176076308327,
        "step": 5371
    },
    {
        "loss": 1.7158,
        "grad_norm": 2.235452651977539,
        "learning_rate": 9.861245133475793e-05,
        "epoch": 0.6924465068316576,
        "step": 5372
    },
    {
        "loss": 2.3345,
        "grad_norm": 1.4711737632751465,
        "learning_rate": 9.859817723271848e-05,
        "epoch": 0.6925754060324826,
        "step": 5373
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.369805335998535,
        "learning_rate": 9.858383112882772e-05,
        "epoch": 0.6927043052333075,
        "step": 5374
    },
    {
        "loss": 1.438,
        "grad_norm": 2.4614100456237793,
        "learning_rate": 9.856941304434047e-05,
        "epoch": 0.6928332044341325,
        "step": 5375
    },
    {
        "loss": 2.3373,
        "grad_norm": 2.980634927749634,
        "learning_rate": 9.855492300061819e-05,
        "epoch": 0.6929621036349575,
        "step": 5376
    },
    {
        "loss": 2.2421,
        "grad_norm": 2.6424026489257812,
        "learning_rate": 9.8540361019129e-05,
        "epoch": 0.6930910028357824,
        "step": 5377
    },
    {
        "loss": 2.1454,
        "grad_norm": 2.535022020339966,
        "learning_rate": 9.852572712144755e-05,
        "epoch": 0.6932199020366073,
        "step": 5378
    },
    {
        "loss": 1.9381,
        "grad_norm": 2.74310040473938,
        "learning_rate": 9.851102132925507e-05,
        "epoch": 0.6933488012374324,
        "step": 5379
    },
    {
        "loss": 1.2608,
        "grad_norm": 2.9667932987213135,
        "learning_rate": 9.849624366433928e-05,
        "epoch": 0.6934777004382573,
        "step": 5380
    },
    {
        "loss": 2.1951,
        "grad_norm": 3.205461025238037,
        "learning_rate": 9.848139414859441e-05,
        "epoch": 0.6936065996390822,
        "step": 5381
    },
    {
        "loss": 1.9662,
        "grad_norm": 3.3158230781555176,
        "learning_rate": 9.846647280402114e-05,
        "epoch": 0.6937354988399071,
        "step": 5382
    },
    {
        "loss": 2.58,
        "grad_norm": 1.7282335758209229,
        "learning_rate": 9.845147965272656e-05,
        "epoch": 0.6938643980407322,
        "step": 5383
    },
    {
        "loss": 0.9578,
        "grad_norm": 3.6286957263946533,
        "learning_rate": 9.843641471692415e-05,
        "epoch": 0.6939932972415571,
        "step": 5384
    },
    {
        "loss": 2.1825,
        "grad_norm": 2.3767573833465576,
        "learning_rate": 9.842127801893373e-05,
        "epoch": 0.694122196442382,
        "step": 5385
    },
    {
        "loss": 2.2819,
        "grad_norm": 2.9271047115325928,
        "learning_rate": 9.840606958118148e-05,
        "epoch": 0.694251095643207,
        "step": 5386
    },
    {
        "loss": 1.6236,
        "grad_norm": 2.4932565689086914,
        "learning_rate": 9.839078942619981e-05,
        "epoch": 0.694379994844032,
        "step": 5387
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.579737663269043,
        "learning_rate": 9.837543757662744e-05,
        "epoch": 0.6945088940448569,
        "step": 5388
    },
    {
        "loss": 1.8436,
        "grad_norm": 2.8517844676971436,
        "learning_rate": 9.836001405520932e-05,
        "epoch": 0.6946377932456819,
        "step": 5389
    },
    {
        "loss": 1.5181,
        "grad_norm": 2.06756854057312,
        "learning_rate": 9.83445188847965e-05,
        "epoch": 0.6947666924465068,
        "step": 5390
    },
    {
        "loss": 1.9866,
        "grad_norm": 1.9557608366012573,
        "learning_rate": 9.832895208834628e-05,
        "epoch": 0.6948955916473318,
        "step": 5391
    },
    {
        "loss": 1.8181,
        "grad_norm": 3.619515895843506,
        "learning_rate": 9.8313313688922e-05,
        "epoch": 0.6950244908481568,
        "step": 5392
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.6281133890151978,
        "learning_rate": 9.829760370969317e-05,
        "epoch": 0.6951533900489817,
        "step": 5393
    },
    {
        "loss": 1.5928,
        "grad_norm": 2.6915085315704346,
        "learning_rate": 9.828182217393525e-05,
        "epoch": 0.6952822892498066,
        "step": 5394
    },
    {
        "loss": 1.865,
        "grad_norm": 2.1942710876464844,
        "learning_rate": 9.826596910502982e-05,
        "epoch": 0.6954111884506317,
        "step": 5395
    },
    {
        "loss": 2.4395,
        "grad_norm": 1.5438001155853271,
        "learning_rate": 9.825004452646437e-05,
        "epoch": 0.6955400876514566,
        "step": 5396
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.6408631801605225,
        "learning_rate": 9.823404846183236e-05,
        "epoch": 0.6956689868522815,
        "step": 5397
    },
    {
        "loss": 1.5911,
        "grad_norm": 2.5900168418884277,
        "learning_rate": 9.821798093483317e-05,
        "epoch": 0.6957978860531064,
        "step": 5398
    },
    {
        "loss": 1.6604,
        "grad_norm": 2.3344342708587646,
        "learning_rate": 9.820184196927204e-05,
        "epoch": 0.6959267852539315,
        "step": 5399
    },
    {
        "loss": 1.5484,
        "grad_norm": 2.5135812759399414,
        "learning_rate": 9.818563158906005e-05,
        "epoch": 0.6960556844547564,
        "step": 5400
    },
    {
        "loss": 1.916,
        "grad_norm": 1.3300904035568237,
        "learning_rate": 9.816934981821414e-05,
        "epoch": 0.6961845836555813,
        "step": 5401
    },
    {
        "loss": 1.9206,
        "grad_norm": 3.8389923572540283,
        "learning_rate": 9.815299668085694e-05,
        "epoch": 0.6963134828564063,
        "step": 5402
    },
    {
        "loss": 2.1483,
        "grad_norm": 2.8919730186462402,
        "learning_rate": 9.813657220121685e-05,
        "epoch": 0.6964423820572312,
        "step": 5403
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.3443500995635986,
        "learning_rate": 9.812007640362796e-05,
        "epoch": 0.6965712812580562,
        "step": 5404
    },
    {
        "loss": 1.9319,
        "grad_norm": 2.481691598892212,
        "learning_rate": 9.810350931253009e-05,
        "epoch": 0.6967001804588812,
        "step": 5405
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.701303243637085,
        "learning_rate": 9.808687095246856e-05,
        "epoch": 0.6968290796597061,
        "step": 5406
    },
    {
        "loss": 1.6933,
        "grad_norm": 1.2798082828521729,
        "learning_rate": 9.807016134809442e-05,
        "epoch": 0.696957978860531,
        "step": 5407
    },
    {
        "loss": 1.2779,
        "grad_norm": 2.863274335861206,
        "learning_rate": 9.805338052416414e-05,
        "epoch": 0.697086878061356,
        "step": 5408
    },
    {
        "loss": 1.1001,
        "grad_norm": 3.820786714553833,
        "learning_rate": 9.803652850553984e-05,
        "epoch": 0.697215777262181,
        "step": 5409
    },
    {
        "loss": 1.8,
        "grad_norm": 1.89155912399292,
        "learning_rate": 9.801960531718896e-05,
        "epoch": 0.6973446764630059,
        "step": 5410
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.8474366664886475,
        "learning_rate": 9.800261098418458e-05,
        "epoch": 0.6974735756638308,
        "step": 5411
    },
    {
        "loss": 1.7078,
        "grad_norm": 2.2487494945526123,
        "learning_rate": 9.798554553170498e-05,
        "epoch": 0.6976024748646559,
        "step": 5412
    },
    {
        "loss": 1.6669,
        "grad_norm": 2.6290197372436523,
        "learning_rate": 9.796840898503397e-05,
        "epoch": 0.6977313740654808,
        "step": 5413
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.6843860149383545,
        "learning_rate": 9.795120136956066e-05,
        "epoch": 0.6978602732663057,
        "step": 5414
    },
    {
        "loss": 2.1682,
        "grad_norm": 1.4915525913238525,
        "learning_rate": 9.793392271077935e-05,
        "epoch": 0.6979891724671307,
        "step": 5415
    },
    {
        "loss": 2.7142,
        "grad_norm": 1.361487627029419,
        "learning_rate": 9.79165730342897e-05,
        "epoch": 0.6981180716679557,
        "step": 5416
    },
    {
        "loss": 2.2247,
        "grad_norm": 1.868780493736267,
        "learning_rate": 9.789915236579661e-05,
        "epoch": 0.6982469708687806,
        "step": 5417
    },
    {
        "loss": 1.2215,
        "grad_norm": 2.272360324859619,
        "learning_rate": 9.788166073111001e-05,
        "epoch": 0.6983758700696056,
        "step": 5418
    },
    {
        "loss": 2.4743,
        "grad_norm": 1.9172697067260742,
        "learning_rate": 9.786409815614516e-05,
        "epoch": 0.6985047692704305,
        "step": 5419
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.5617520809173584,
        "learning_rate": 9.784646466692229e-05,
        "epoch": 0.6986336684712555,
        "step": 5420
    },
    {
        "loss": 1.4001,
        "grad_norm": 2.7406768798828125,
        "learning_rate": 9.782876028956678e-05,
        "epoch": 0.6987625676720804,
        "step": 5421
    },
    {
        "loss": 2.1462,
        "grad_norm": 1.7556771039962769,
        "learning_rate": 9.781098505030893e-05,
        "epoch": 0.6988914668729054,
        "step": 5422
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.444624423980713,
        "learning_rate": 9.779313897548415e-05,
        "epoch": 0.6990203660737303,
        "step": 5423
    },
    {
        "loss": 2.1671,
        "grad_norm": 2.4014909267425537,
        "learning_rate": 9.777522209153271e-05,
        "epoch": 0.6991492652745553,
        "step": 5424
    },
    {
        "loss": 2.3586,
        "grad_norm": 1.5739673376083374,
        "learning_rate": 9.775723442499982e-05,
        "epoch": 0.6992781644753803,
        "step": 5425
    },
    {
        "loss": 1.9134,
        "grad_norm": 2.5105137825012207,
        "learning_rate": 9.773917600253559e-05,
        "epoch": 0.6994070636762052,
        "step": 5426
    },
    {
        "loss": 2.4077,
        "grad_norm": 1.487850546836853,
        "learning_rate": 9.772104685089493e-05,
        "epoch": 0.6995359628770301,
        "step": 5427
    },
    {
        "loss": 1.9005,
        "grad_norm": 1.7567659616470337,
        "learning_rate": 9.770284699693747e-05,
        "epoch": 0.6996648620778552,
        "step": 5428
    },
    {
        "loss": 2.3249,
        "grad_norm": 1.7575576305389404,
        "learning_rate": 9.768457646762772e-05,
        "epoch": 0.6997937612786801,
        "step": 5429
    },
    {
        "loss": 1.8775,
        "grad_norm": 1.4977213144302368,
        "learning_rate": 9.766623529003486e-05,
        "epoch": 0.699922660479505,
        "step": 5430
    },
    {
        "loss": 1.3165,
        "grad_norm": 1.9467136859893799,
        "learning_rate": 9.764782349133268e-05,
        "epoch": 0.70005155968033,
        "step": 5431
    },
    {
        "loss": 2.1749,
        "grad_norm": 2.0843405723571777,
        "learning_rate": 9.762934109879966e-05,
        "epoch": 0.700180458881155,
        "step": 5432
    },
    {
        "loss": 1.7281,
        "grad_norm": 3.796530246734619,
        "learning_rate": 9.761078813981888e-05,
        "epoch": 0.7003093580819799,
        "step": 5433
    },
    {
        "loss": 2.5538,
        "grad_norm": 1.6998497247695923,
        "learning_rate": 9.759216464187786e-05,
        "epoch": 0.7004382572828048,
        "step": 5434
    },
    {
        "loss": 1.6117,
        "grad_norm": 2.416459798812866,
        "learning_rate": 9.757347063256879e-05,
        "epoch": 0.7005671564836298,
        "step": 5435
    },
    {
        "loss": 2.3031,
        "grad_norm": 1.8118594884872437,
        "learning_rate": 9.755470613958823e-05,
        "epoch": 0.7006960556844548,
        "step": 5436
    },
    {
        "loss": 1.6909,
        "grad_norm": 3.0341110229492188,
        "learning_rate": 9.753587119073719e-05,
        "epoch": 0.7008249548852797,
        "step": 5437
    },
    {
        "loss": 1.7404,
        "grad_norm": 1.6785141229629517,
        "learning_rate": 9.751696581392105e-05,
        "epoch": 0.7009538540861047,
        "step": 5438
    },
    {
        "loss": 2.0873,
        "grad_norm": 1.9257436990737915,
        "learning_rate": 9.749799003714954e-05,
        "epoch": 0.7010827532869296,
        "step": 5439
    },
    {
        "loss": 1.8468,
        "grad_norm": 1.9448902606964111,
        "learning_rate": 9.747894388853673e-05,
        "epoch": 0.7012116524877545,
        "step": 5440
    },
    {
        "loss": 1.6884,
        "grad_norm": 2.9459922313690186,
        "learning_rate": 9.745982739630089e-05,
        "epoch": 0.7013405516885796,
        "step": 5441
    },
    {
        "loss": 1.8341,
        "grad_norm": 3.1674323081970215,
        "learning_rate": 9.744064058876454e-05,
        "epoch": 0.7014694508894045,
        "step": 5442
    },
    {
        "loss": 1.8742,
        "grad_norm": 2.349050760269165,
        "learning_rate": 9.74213834943544e-05,
        "epoch": 0.7015983500902294,
        "step": 5443
    },
    {
        "loss": 1.613,
        "grad_norm": 2.6088552474975586,
        "learning_rate": 9.74020561416013e-05,
        "epoch": 0.7017272492910543,
        "step": 5444
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.3969967365264893,
        "learning_rate": 9.738265855914013e-05,
        "epoch": 0.7018561484918794,
        "step": 5445
    },
    {
        "loss": 2.4037,
        "grad_norm": 1.6120154857635498,
        "learning_rate": 9.73631907757099e-05,
        "epoch": 0.7019850476927043,
        "step": 5446
    },
    {
        "loss": 1.2473,
        "grad_norm": 3.366595983505249,
        "learning_rate": 9.734365282015359e-05,
        "epoch": 0.7021139468935292,
        "step": 5447
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.273951292037964,
        "learning_rate": 9.732404472141814e-05,
        "epoch": 0.7022428460943542,
        "step": 5448
    },
    {
        "loss": 1.864,
        "grad_norm": 3.6297221183776855,
        "learning_rate": 9.730436650855444e-05,
        "epoch": 0.7023717452951792,
        "step": 5449
    },
    {
        "loss": 1.7168,
        "grad_norm": 3.18642520904541,
        "learning_rate": 9.72846182107172e-05,
        "epoch": 0.7025006444960041,
        "step": 5450
    },
    {
        "loss": 1.5455,
        "grad_norm": 2.485537528991699,
        "learning_rate": 9.726479985716506e-05,
        "epoch": 0.7026295436968291,
        "step": 5451
    },
    {
        "loss": 2.0075,
        "grad_norm": 2.3289339542388916,
        "learning_rate": 9.724491147726036e-05,
        "epoch": 0.702758442897654,
        "step": 5452
    },
    {
        "loss": 1.9705,
        "grad_norm": 3.984851360321045,
        "learning_rate": 9.722495310046924e-05,
        "epoch": 0.702887342098479,
        "step": 5453
    },
    {
        "loss": 1.977,
        "grad_norm": 2.050020694732666,
        "learning_rate": 9.720492475636153e-05,
        "epoch": 0.703016241299304,
        "step": 5454
    },
    {
        "loss": 2.1169,
        "grad_norm": 2.255016803741455,
        "learning_rate": 9.718482647461074e-05,
        "epoch": 0.7031451405001289,
        "step": 5455
    },
    {
        "loss": 2.3746,
        "grad_norm": 2.4678189754486084,
        "learning_rate": 9.716465828499401e-05,
        "epoch": 0.7032740397009538,
        "step": 5456
    },
    {
        "loss": 1.2589,
        "grad_norm": 2.502683162689209,
        "learning_rate": 9.714442021739197e-05,
        "epoch": 0.7034029389017789,
        "step": 5457
    },
    {
        "loss": 1.957,
        "grad_norm": 2.3058319091796875,
        "learning_rate": 9.712411230178888e-05,
        "epoch": 0.7035318381026038,
        "step": 5458
    },
    {
        "loss": 1.7638,
        "grad_norm": 3.188425302505493,
        "learning_rate": 9.710373456827242e-05,
        "epoch": 0.7036607373034287,
        "step": 5459
    },
    {
        "loss": 2.2698,
        "grad_norm": 2.2144811153411865,
        "learning_rate": 9.708328704703373e-05,
        "epoch": 0.7037896365042536,
        "step": 5460
    },
    {
        "loss": 1.685,
        "grad_norm": 2.5102298259735107,
        "learning_rate": 9.706276976836736e-05,
        "epoch": 0.7039185357050787,
        "step": 5461
    },
    {
        "loss": 2.4487,
        "grad_norm": 2.6815648078918457,
        "learning_rate": 9.704218276267125e-05,
        "epoch": 0.7040474349059036,
        "step": 5462
    },
    {
        "loss": 2.4683,
        "grad_norm": 2.1277201175689697,
        "learning_rate": 9.70215260604465e-05,
        "epoch": 0.7041763341067285,
        "step": 5463
    },
    {
        "loss": 2.0646,
        "grad_norm": 1.9904190301895142,
        "learning_rate": 9.700079969229762e-05,
        "epoch": 0.7043052333075535,
        "step": 5464
    },
    {
        "loss": 1.9041,
        "grad_norm": 2.655925750732422,
        "learning_rate": 9.69800036889323e-05,
        "epoch": 0.7044341325083785,
        "step": 5465
    },
    {
        "loss": 1.8311,
        "grad_norm": 3.2637178897857666,
        "learning_rate": 9.695913808116132e-05,
        "epoch": 0.7045630317092034,
        "step": 5466
    },
    {
        "loss": 1.8286,
        "grad_norm": 2.743814706802368,
        "learning_rate": 9.69382028998987e-05,
        "epoch": 0.7046919309100284,
        "step": 5467
    },
    {
        "loss": 1.3467,
        "grad_norm": 2.492713689804077,
        "learning_rate": 9.691719817616147e-05,
        "epoch": 0.7048208301108533,
        "step": 5468
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.1009812355041504,
        "learning_rate": 9.689612394106973e-05,
        "epoch": 0.7049497293116783,
        "step": 5469
    },
    {
        "loss": 2.3027,
        "grad_norm": 1.9137403964996338,
        "learning_rate": 9.687498022584649e-05,
        "epoch": 0.7050786285125032,
        "step": 5470
    },
    {
        "loss": 1.1427,
        "grad_norm": 2.2679085731506348,
        "learning_rate": 9.685376706181775e-05,
        "epoch": 0.7052075277133282,
        "step": 5471
    },
    {
        "loss": 1.659,
        "grad_norm": 2.4859752655029297,
        "learning_rate": 9.683248448041249e-05,
        "epoch": 0.7053364269141531,
        "step": 5472
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.3705722093582153,
        "learning_rate": 9.681113251316237e-05,
        "epoch": 0.7054653261149781,
        "step": 5473
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.6776676177978516,
        "learning_rate": 9.678971119170195e-05,
        "epoch": 0.7055942253158031,
        "step": 5474
    },
    {
        "loss": 1.8935,
        "grad_norm": 2.6728641986846924,
        "learning_rate": 9.67682205477686e-05,
        "epoch": 0.705723124516628,
        "step": 5475
    },
    {
        "loss": 1.3317,
        "grad_norm": 2.645782232284546,
        "learning_rate": 9.674666061320221e-05,
        "epoch": 0.7058520237174529,
        "step": 5476
    },
    {
        "loss": 1.9603,
        "grad_norm": 1.4374113082885742,
        "learning_rate": 9.672503141994548e-05,
        "epoch": 0.7059809229182779,
        "step": 5477
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.49454665184021,
        "learning_rate": 9.670333300004377e-05,
        "epoch": 0.7061098221191029,
        "step": 5478
    },
    {
        "loss": 2.0975,
        "grad_norm": 2.1590473651885986,
        "learning_rate": 9.66815653856448e-05,
        "epoch": 0.7062387213199278,
        "step": 5479
    },
    {
        "loss": 1.8609,
        "grad_norm": 2.120736837387085,
        "learning_rate": 9.665972860899899e-05,
        "epoch": 0.7063676205207527,
        "step": 5480
    },
    {
        "loss": 1.7401,
        "grad_norm": 2.338843584060669,
        "learning_rate": 9.663782270245921e-05,
        "epoch": 0.7064965197215777,
        "step": 5481
    },
    {
        "loss": 2.2073,
        "grad_norm": 3.312150239944458,
        "learning_rate": 9.661584769848059e-05,
        "epoch": 0.7066254189224027,
        "step": 5482
    },
    {
        "loss": 2.152,
        "grad_norm": 1.5999022722244263,
        "learning_rate": 9.659380362962079e-05,
        "epoch": 0.7067543181232276,
        "step": 5483
    },
    {
        "loss": 2.0472,
        "grad_norm": 2.708876132965088,
        "learning_rate": 9.657169052853983e-05,
        "epoch": 0.7068832173240526,
        "step": 5484
    },
    {
        "loss": 1.8004,
        "grad_norm": 2.995070219039917,
        "learning_rate": 9.654950842799983e-05,
        "epoch": 0.7070121165248775,
        "step": 5485
    },
    {
        "loss": 1.9764,
        "grad_norm": 2.470599889755249,
        "learning_rate": 9.652725736086532e-05,
        "epoch": 0.7071410157257025,
        "step": 5486
    },
    {
        "loss": 1.9326,
        "grad_norm": 2.2516965866088867,
        "learning_rate": 9.650493736010288e-05,
        "epoch": 0.7072699149265275,
        "step": 5487
    },
    {
        "loss": 2.2503,
        "grad_norm": 1.532758116722107,
        "learning_rate": 9.648254845878131e-05,
        "epoch": 0.7073988141273524,
        "step": 5488
    },
    {
        "loss": 2.381,
        "grad_norm": 2.2461280822753906,
        "learning_rate": 9.646009069007136e-05,
        "epoch": 0.7075277133281773,
        "step": 5489
    },
    {
        "loss": 2.3461,
        "grad_norm": 1.4869120121002197,
        "learning_rate": 9.6437564087246e-05,
        "epoch": 0.7076566125290024,
        "step": 5490
    },
    {
        "loss": 2.2927,
        "grad_norm": 2.5235772132873535,
        "learning_rate": 9.641496868368002e-05,
        "epoch": 0.7077855117298273,
        "step": 5491
    },
    {
        "loss": 2.1976,
        "grad_norm": 2.372898578643799,
        "learning_rate": 9.639230451285027e-05,
        "epoch": 0.7079144109306522,
        "step": 5492
    },
    {
        "loss": 0.9586,
        "grad_norm": 2.866097927093506,
        "learning_rate": 9.636957160833537e-05,
        "epoch": 0.7080433101314771,
        "step": 5493
    },
    {
        "loss": 1.3253,
        "grad_norm": 2.5131723880767822,
        "learning_rate": 9.634677000381588e-05,
        "epoch": 0.7081722093323022,
        "step": 5494
    },
    {
        "loss": 2.5181,
        "grad_norm": 1.0576337575912476,
        "learning_rate": 9.6323899733074e-05,
        "epoch": 0.7083011085331271,
        "step": 5495
    },
    {
        "loss": 1.6636,
        "grad_norm": 2.2637503147125244,
        "learning_rate": 9.630096082999382e-05,
        "epoch": 0.708430007733952,
        "step": 5496
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.1179559230804443,
        "learning_rate": 9.627795332856106e-05,
        "epoch": 0.708558906934777,
        "step": 5497
    },
    {
        "loss": 2.3951,
        "grad_norm": 1.2806508541107178,
        "learning_rate": 9.625487726286302e-05,
        "epoch": 0.708687806135602,
        "step": 5498
    },
    {
        "loss": 1.8723,
        "grad_norm": 1.7508257627487183,
        "learning_rate": 9.623173266708864e-05,
        "epoch": 0.7088167053364269,
        "step": 5499
    },
    {
        "loss": 1.9681,
        "grad_norm": 2.7048697471618652,
        "learning_rate": 9.620851957552843e-05,
        "epoch": 0.7089456045372519,
        "step": 5500
    },
    {
        "loss": 1.8308,
        "grad_norm": 3.004326343536377,
        "learning_rate": 9.618523802257421e-05,
        "epoch": 0.7090745037380768,
        "step": 5501
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.026890277862549,
        "learning_rate": 9.616188804271948e-05,
        "epoch": 0.7092034029389018,
        "step": 5502
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.030761957168579,
        "learning_rate": 9.613846967055891e-05,
        "epoch": 0.7093323021397268,
        "step": 5503
    },
    {
        "loss": 2.6239,
        "grad_norm": 2.31498646736145,
        "learning_rate": 9.61149829407886e-05,
        "epoch": 0.7094612013405517,
        "step": 5504
    },
    {
        "loss": 2.3847,
        "grad_norm": 1.3332027196884155,
        "learning_rate": 9.60914278882059e-05,
        "epoch": 0.7095901005413766,
        "step": 5505
    },
    {
        "loss": 2.1053,
        "grad_norm": 2.653512477874756,
        "learning_rate": 9.60678045477094e-05,
        "epoch": 0.7097189997422017,
        "step": 5506
    },
    {
        "loss": 1.787,
        "grad_norm": 2.7206571102142334,
        "learning_rate": 9.604411295429885e-05,
        "epoch": 0.7098478989430266,
        "step": 5507
    },
    {
        "loss": 1.8022,
        "grad_norm": 2.950455904006958,
        "learning_rate": 9.602035314307513e-05,
        "epoch": 0.7099767981438515,
        "step": 5508
    },
    {
        "loss": 1.1628,
        "grad_norm": 3.3824265003204346,
        "learning_rate": 9.599652514924018e-05,
        "epoch": 0.7101056973446764,
        "step": 5509
    },
    {
        "loss": 1.6348,
        "grad_norm": 1.834240436553955,
        "learning_rate": 9.597262900809696e-05,
        "epoch": 0.7102345965455015,
        "step": 5510
    },
    {
        "loss": 1.8744,
        "grad_norm": 2.7205843925476074,
        "learning_rate": 9.59486647550494e-05,
        "epoch": 0.7103634957463264,
        "step": 5511
    },
    {
        "loss": 1.7312,
        "grad_norm": 2.5970652103424072,
        "learning_rate": 9.592463242560235e-05,
        "epoch": 0.7104923949471513,
        "step": 5512
    },
    {
        "loss": 1.9508,
        "grad_norm": 2.093700408935547,
        "learning_rate": 9.590053205536149e-05,
        "epoch": 0.7106212941479763,
        "step": 5513
    },
    {
        "loss": 2.2378,
        "grad_norm": 1.5784071683883667,
        "learning_rate": 9.587636368003337e-05,
        "epoch": 0.7107501933488012,
        "step": 5514
    },
    {
        "loss": 1.7655,
        "grad_norm": 2.49394154548645,
        "learning_rate": 9.585212733542521e-05,
        "epoch": 0.7108790925496262,
        "step": 5515
    },
    {
        "loss": 2.3038,
        "grad_norm": 2.047189474105835,
        "learning_rate": 9.582782305744501e-05,
        "epoch": 0.7110079917504512,
        "step": 5516
    },
    {
        "loss": 1.8837,
        "grad_norm": 2.4532711505889893,
        "learning_rate": 9.580345088210137e-05,
        "epoch": 0.7111368909512761,
        "step": 5517
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.5607143640518188,
        "learning_rate": 9.57790108455035e-05,
        "epoch": 0.711265790152101,
        "step": 5518
    },
    {
        "loss": 1.5384,
        "grad_norm": 3.6841025352478027,
        "learning_rate": 9.575450298386112e-05,
        "epoch": 0.711394689352926,
        "step": 5519
    },
    {
        "loss": 2.4305,
        "grad_norm": 2.626760959625244,
        "learning_rate": 9.572992733348454e-05,
        "epoch": 0.711523588553751,
        "step": 5520
    },
    {
        "loss": 2.0363,
        "grad_norm": 2.045381546020508,
        "learning_rate": 9.570528393078437e-05,
        "epoch": 0.7116524877545759,
        "step": 5521
    },
    {
        "loss": 2.1418,
        "grad_norm": 3.2209019660949707,
        "learning_rate": 9.568057281227165e-05,
        "epoch": 0.7117813869554008,
        "step": 5522
    },
    {
        "loss": 1.86,
        "grad_norm": 2.036210060119629,
        "learning_rate": 9.565579401455787e-05,
        "epoch": 0.7119102861562259,
        "step": 5523
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.791499376296997,
        "learning_rate": 9.563094757435454e-05,
        "epoch": 0.7120391853570508,
        "step": 5524
    },
    {
        "loss": 2.1601,
        "grad_norm": 1.4233875274658203,
        "learning_rate": 9.560603352847361e-05,
        "epoch": 0.7121680845578757,
        "step": 5525
    },
    {
        "loss": 2.3686,
        "grad_norm": 1.9819157123565674,
        "learning_rate": 9.558105191382711e-05,
        "epoch": 0.7122969837587007,
        "step": 5526
    },
    {
        "loss": 1.8036,
        "grad_norm": 1.9308934211730957,
        "learning_rate": 9.555600276742715e-05,
        "epoch": 0.7124258829595257,
        "step": 5527
    },
    {
        "loss": 2.3256,
        "grad_norm": 2.4181833267211914,
        "learning_rate": 9.55308861263859e-05,
        "epoch": 0.7125547821603506,
        "step": 5528
    },
    {
        "loss": 2.6867,
        "grad_norm": 1.6300413608551025,
        "learning_rate": 9.550570202791565e-05,
        "epoch": 0.7126836813611755,
        "step": 5529
    },
    {
        "loss": 2.1142,
        "grad_norm": 2.2357873916625977,
        "learning_rate": 9.548045050932844e-05,
        "epoch": 0.7128125805620005,
        "step": 5530
    },
    {
        "loss": 1.856,
        "grad_norm": 3.887357234954834,
        "learning_rate": 9.54551316080363e-05,
        "epoch": 0.7129414797628255,
        "step": 5531
    },
    {
        "loss": 2.5633,
        "grad_norm": 1.7455334663391113,
        "learning_rate": 9.542974536155114e-05,
        "epoch": 0.7130703789636504,
        "step": 5532
    },
    {
        "loss": 2.2143,
        "grad_norm": 1.5823668241500854,
        "learning_rate": 9.540429180748453e-05,
        "epoch": 0.7131992781644754,
        "step": 5533
    },
    {
        "loss": 1.1987,
        "grad_norm": 2.3323206901550293,
        "learning_rate": 9.537877098354784e-05,
        "epoch": 0.7133281773653003,
        "step": 5534
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.2687437534332275,
        "learning_rate": 9.535318292755213e-05,
        "epoch": 0.7134570765661253,
        "step": 5535
    },
    {
        "loss": 2.2364,
        "grad_norm": 2.8650002479553223,
        "learning_rate": 9.532752767740803e-05,
        "epoch": 0.7135859757669503,
        "step": 5536
    },
    {
        "loss": 1.8813,
        "grad_norm": 2.081098794937134,
        "learning_rate": 9.530180527112565e-05,
        "epoch": 0.7137148749677752,
        "step": 5537
    },
    {
        "loss": 1.5549,
        "grad_norm": 2.8840131759643555,
        "learning_rate": 9.527601574681466e-05,
        "epoch": 0.7138437741686001,
        "step": 5538
    },
    {
        "loss": 2.5179,
        "grad_norm": 1.3030586242675781,
        "learning_rate": 9.52501591426843e-05,
        "epoch": 0.7139726733694252,
        "step": 5539
    },
    {
        "loss": 2.3411,
        "grad_norm": 2.1563103199005127,
        "learning_rate": 9.522423549704294e-05,
        "epoch": 0.7141015725702501,
        "step": 5540
    },
    {
        "loss": 2.0802,
        "grad_norm": 2.12947678565979,
        "learning_rate": 9.519824484829849e-05,
        "epoch": 0.714230471771075,
        "step": 5541
    },
    {
        "loss": 1.8871,
        "grad_norm": 2.587693214416504,
        "learning_rate": 9.517218723495805e-05,
        "epoch": 0.7143593709718999,
        "step": 5542
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.6336803436279297,
        "learning_rate": 9.514606269562789e-05,
        "epoch": 0.714488270172725,
        "step": 5543
    },
    {
        "loss": 1.7663,
        "grad_norm": 1.5350370407104492,
        "learning_rate": 9.511987126901345e-05,
        "epoch": 0.7146171693735499,
        "step": 5544
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.1084585189819336,
        "learning_rate": 9.509361299391941e-05,
        "epoch": 0.7147460685743748,
        "step": 5545
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.2066478729248047,
        "learning_rate": 9.506728790924925e-05,
        "epoch": 0.7148749677751998,
        "step": 5546
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.3247973918914795,
        "learning_rate": 9.504089605400564e-05,
        "epoch": 0.7150038669760247,
        "step": 5547
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.488476514816284,
        "learning_rate": 9.50144374672901e-05,
        "epoch": 0.7151327661768497,
        "step": 5548
    },
    {
        "loss": 1.7538,
        "grad_norm": 2.4956777095794678,
        "learning_rate": 9.498791218830299e-05,
        "epoch": 0.7152616653776747,
        "step": 5549
    },
    {
        "loss": 1.4524,
        "grad_norm": 2.1106574535369873,
        "learning_rate": 9.496132025634346e-05,
        "epoch": 0.7153905645784996,
        "step": 5550
    },
    {
        "loss": 1.6359,
        "grad_norm": 2.785048723220825,
        "learning_rate": 9.493466171080951e-05,
        "epoch": 0.7155194637793245,
        "step": 5551
    },
    {
        "loss": 2.5776,
        "grad_norm": 2.044395923614502,
        "learning_rate": 9.490793659119779e-05,
        "epoch": 0.7156483629801496,
        "step": 5552
    },
    {
        "loss": 2.0233,
        "grad_norm": 2.4093918800354004,
        "learning_rate": 9.488114493710357e-05,
        "epoch": 0.7157772621809745,
        "step": 5553
    },
    {
        "loss": 2.2722,
        "grad_norm": 1.6221787929534912,
        "learning_rate": 9.485428678822064e-05,
        "epoch": 0.7159061613817994,
        "step": 5554
    },
    {
        "loss": 1.7327,
        "grad_norm": 2.4997398853302,
        "learning_rate": 9.482736218434146e-05,
        "epoch": 0.7160350605826243,
        "step": 5555
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.3319790363311768,
        "learning_rate": 9.480037116535671e-05,
        "epoch": 0.7161639597834494,
        "step": 5556
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.5020198822021484,
        "learning_rate": 9.477331377125577e-05,
        "epoch": 0.7162928589842743,
        "step": 5557
    },
    {
        "loss": 1.2401,
        "grad_norm": 3.045006513595581,
        "learning_rate": 9.474619004212612e-05,
        "epoch": 0.7164217581850992,
        "step": 5558
    },
    {
        "loss": 1.9551,
        "grad_norm": 1.6598566770553589,
        "learning_rate": 9.471900001815361e-05,
        "epoch": 0.7165506573859242,
        "step": 5559
    },
    {
        "loss": 2.1784,
        "grad_norm": 2.587491273880005,
        "learning_rate": 9.469174373962233e-05,
        "epoch": 0.7166795565867492,
        "step": 5560
    },
    {
        "loss": 1.5471,
        "grad_norm": 2.8178796768188477,
        "learning_rate": 9.466442124691451e-05,
        "epoch": 0.7168084557875741,
        "step": 5561
    },
    {
        "loss": 2.1559,
        "grad_norm": 1.529983639717102,
        "learning_rate": 9.463703258051037e-05,
        "epoch": 0.7169373549883991,
        "step": 5562
    },
    {
        "loss": 2.0953,
        "grad_norm": 2.075995922088623,
        "learning_rate": 9.46095777809884e-05,
        "epoch": 0.717066254189224,
        "step": 5563
    },
    {
        "loss": 2.4025,
        "grad_norm": 1.851433515548706,
        "learning_rate": 9.458205688902493e-05,
        "epoch": 0.717195153390049,
        "step": 5564
    },
    {
        "loss": 1.2902,
        "grad_norm": 2.9266867637634277,
        "learning_rate": 9.455446994539418e-05,
        "epoch": 0.717324052590874,
        "step": 5565
    },
    {
        "loss": 2.4013,
        "grad_norm": 1.379538893699646,
        "learning_rate": 9.452681699096832e-05,
        "epoch": 0.7174529517916989,
        "step": 5566
    },
    {
        "loss": 1.6261,
        "grad_norm": 2.5239615440368652,
        "learning_rate": 9.449909806671727e-05,
        "epoch": 0.7175818509925238,
        "step": 5567
    },
    {
        "loss": 1.9118,
        "grad_norm": 2.818122625350952,
        "learning_rate": 9.447131321370868e-05,
        "epoch": 0.7177107501933488,
        "step": 5568
    },
    {
        "loss": 1.9965,
        "grad_norm": 1.9447567462921143,
        "learning_rate": 9.444346247310794e-05,
        "epoch": 0.7178396493941738,
        "step": 5569
    },
    {
        "loss": 1.3683,
        "grad_norm": 2.573873996734619,
        "learning_rate": 9.441554588617796e-05,
        "epoch": 0.7179685485949987,
        "step": 5570
    },
    {
        "loss": 2.163,
        "grad_norm": 1.9036450386047363,
        "learning_rate": 9.43875634942793e-05,
        "epoch": 0.7180974477958236,
        "step": 5571
    },
    {
        "loss": 1.9363,
        "grad_norm": 2.4639337062835693,
        "learning_rate": 9.435951533886998e-05,
        "epoch": 0.7182263469966487,
        "step": 5572
    },
    {
        "loss": 1.7861,
        "grad_norm": 2.5823519229888916,
        "learning_rate": 9.433140146150543e-05,
        "epoch": 0.7183552461974736,
        "step": 5573
    },
    {
        "loss": 2.1392,
        "grad_norm": 2.056535243988037,
        "learning_rate": 9.430322190383845e-05,
        "epoch": 0.7184841453982985,
        "step": 5574
    },
    {
        "loss": 2.2248,
        "grad_norm": 2.3911917209625244,
        "learning_rate": 9.427497670761923e-05,
        "epoch": 0.7186130445991235,
        "step": 5575
    },
    {
        "loss": 2.135,
        "grad_norm": 1.716072678565979,
        "learning_rate": 9.424666591469511e-05,
        "epoch": 0.7187419437999485,
        "step": 5576
    },
    {
        "loss": 2.0441,
        "grad_norm": 2.700423002243042,
        "learning_rate": 9.421828956701068e-05,
        "epoch": 0.7188708430007734,
        "step": 5577
    },
    {
        "loss": 1.2394,
        "grad_norm": 2.730212450027466,
        "learning_rate": 9.41898477066076e-05,
        "epoch": 0.7189997422015983,
        "step": 5578
    },
    {
        "loss": 1.7293,
        "grad_norm": 2.752073287963867,
        "learning_rate": 9.416134037562465e-05,
        "epoch": 0.7191286414024233,
        "step": 5579
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.957250714302063,
        "learning_rate": 9.413276761629759e-05,
        "epoch": 0.7192575406032483,
        "step": 5580
    },
    {
        "loss": 2.1497,
        "grad_norm": 2.1417829990386963,
        "learning_rate": 9.410412947095908e-05,
        "epoch": 0.7193864398040732,
        "step": 5581
    },
    {
        "loss": 2.1708,
        "grad_norm": 1.8707882165908813,
        "learning_rate": 9.407542598203869e-05,
        "epoch": 0.7195153390048982,
        "step": 5582
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.6474552154541016,
        "learning_rate": 9.404665719206283e-05,
        "epoch": 0.7196442382057231,
        "step": 5583
    },
    {
        "loss": 1.9464,
        "grad_norm": 2.0079505443573,
        "learning_rate": 9.401782314365457e-05,
        "epoch": 0.719773137406548,
        "step": 5584
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.878201723098755,
        "learning_rate": 9.398892387953377e-05,
        "epoch": 0.7199020366073731,
        "step": 5585
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.89347243309021,
        "learning_rate": 9.395995944251683e-05,
        "epoch": 0.720030935808198,
        "step": 5586
    },
    {
        "loss": 1.3976,
        "grad_norm": 2.4884066581726074,
        "learning_rate": 9.393092987551675e-05,
        "epoch": 0.7201598350090229,
        "step": 5587
    },
    {
        "loss": 1.9905,
        "grad_norm": 2.3257076740264893,
        "learning_rate": 9.390183522154302e-05,
        "epoch": 0.7202887342098478,
        "step": 5588
    },
    {
        "loss": 1.4635,
        "grad_norm": 2.5264370441436768,
        "learning_rate": 9.387267552370151e-05,
        "epoch": 0.7204176334106729,
        "step": 5589
    },
    {
        "loss": 1.8979,
        "grad_norm": 2.1861538887023926,
        "learning_rate": 9.384345082519463e-05,
        "epoch": 0.7205465326114978,
        "step": 5590
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.8430147171020508,
        "learning_rate": 9.381416116932083e-05,
        "epoch": 0.7206754318123227,
        "step": 5591
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.462867498397827,
        "learning_rate": 9.378480659947498e-05,
        "epoch": 0.7208043310131477,
        "step": 5592
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.7744765281677246,
        "learning_rate": 9.375538715914811e-05,
        "epoch": 0.7209332302139727,
        "step": 5593
    },
    {
        "loss": 1.8553,
        "grad_norm": 2.612415313720703,
        "learning_rate": 9.372590289192728e-05,
        "epoch": 0.7210621294147976,
        "step": 5594
    },
    {
        "loss": 2.673,
        "grad_norm": 1.7921816110610962,
        "learning_rate": 9.369635384149566e-05,
        "epoch": 0.7211910286156226,
        "step": 5595
    },
    {
        "loss": 2.6517,
        "grad_norm": 1.6015044450759888,
        "learning_rate": 9.366674005163249e-05,
        "epoch": 0.7213199278164475,
        "step": 5596
    },
    {
        "loss": 1.8047,
        "grad_norm": 2.41129207611084,
        "learning_rate": 9.363706156621268e-05,
        "epoch": 0.7214488270172725,
        "step": 5597
    },
    {
        "loss": 1.0728,
        "grad_norm": 5.041646480560303,
        "learning_rate": 9.360731842920721e-05,
        "epoch": 0.7215777262180975,
        "step": 5598
    },
    {
        "loss": 1.8633,
        "grad_norm": 2.646606922149658,
        "learning_rate": 9.357751068468276e-05,
        "epoch": 0.7217066254189224,
        "step": 5599
    },
    {
        "loss": 2.2914,
        "grad_norm": 3.4476537704467773,
        "learning_rate": 9.354763837680174e-05,
        "epoch": 0.7218355246197473,
        "step": 5600
    },
    {
        "loss": 1.9019,
        "grad_norm": 2.210207223892212,
        "learning_rate": 9.351770154982217e-05,
        "epoch": 0.7219644238205724,
        "step": 5601
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.8764665126800537,
        "learning_rate": 9.348770024809782e-05,
        "epoch": 0.7220933230213973,
        "step": 5602
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.4875295162200928,
        "learning_rate": 9.345763451607783e-05,
        "epoch": 0.7222222222222222,
        "step": 5603
    },
    {
        "loss": 1.0109,
        "grad_norm": 3.8282928466796875,
        "learning_rate": 9.34275043983068e-05,
        "epoch": 0.7223511214230471,
        "step": 5604
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.9961256980895996,
        "learning_rate": 9.339730993942475e-05,
        "epoch": 0.7224800206238722,
        "step": 5605
    },
    {
        "loss": 1.5517,
        "grad_norm": 2.5537312030792236,
        "learning_rate": 9.336705118416714e-05,
        "epoch": 0.7226089198246971,
        "step": 5606
    },
    {
        "loss": 2.4767,
        "grad_norm": 1.9292075634002686,
        "learning_rate": 9.333672817736445e-05,
        "epoch": 0.722737819025522,
        "step": 5607
    },
    {
        "loss": 1.8534,
        "grad_norm": 2.0698158740997314,
        "learning_rate": 9.330634096394262e-05,
        "epoch": 0.722866718226347,
        "step": 5608
    },
    {
        "loss": 1.7088,
        "grad_norm": 2.8333427906036377,
        "learning_rate": 9.327588958892258e-05,
        "epoch": 0.722995617427172,
        "step": 5609
    },
    {
        "loss": 1.9319,
        "grad_norm": 1.7968997955322266,
        "learning_rate": 9.324537409742023e-05,
        "epoch": 0.7231245166279969,
        "step": 5610
    },
    {
        "loss": 2.0054,
        "grad_norm": 1.6032049655914307,
        "learning_rate": 9.32147945346466e-05,
        "epoch": 0.7232534158288219,
        "step": 5611
    },
    {
        "loss": 1.9578,
        "grad_norm": 2.306440830230713,
        "learning_rate": 9.318415094590772e-05,
        "epoch": 0.7233823150296468,
        "step": 5612
    },
    {
        "loss": 2.4628,
        "grad_norm": 2.217454671859741,
        "learning_rate": 9.31534433766042e-05,
        "epoch": 0.7235112142304718,
        "step": 5613
    },
    {
        "loss": 2.3003,
        "grad_norm": 2.4535064697265625,
        "learning_rate": 9.312267187223173e-05,
        "epoch": 0.7236401134312967,
        "step": 5614
    },
    {
        "loss": 2.218,
        "grad_norm": 1.9239941835403442,
        "learning_rate": 9.30918364783806e-05,
        "epoch": 0.7237690126321217,
        "step": 5615
    },
    {
        "loss": 2.1118,
        "grad_norm": 2.0421769618988037,
        "learning_rate": 9.30609372407357e-05,
        "epoch": 0.7238979118329466,
        "step": 5616
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.201909303665161,
        "learning_rate": 9.302997420507657e-05,
        "epoch": 0.7240268110337716,
        "step": 5617
    },
    {
        "loss": 2.1027,
        "grad_norm": 1.8784228563308716,
        "learning_rate": 9.299894741727735e-05,
        "epoch": 0.7241557102345966,
        "step": 5618
    },
    {
        "loss": 1.6729,
        "grad_norm": 2.149998903274536,
        "learning_rate": 9.296785692330652e-05,
        "epoch": 0.7242846094354215,
        "step": 5619
    },
    {
        "loss": 1.7193,
        "grad_norm": 1.754184603691101,
        "learning_rate": 9.293670276922697e-05,
        "epoch": 0.7244135086362464,
        "step": 5620
    },
    {
        "loss": 1.6397,
        "grad_norm": 2.7926652431488037,
        "learning_rate": 9.290548500119594e-05,
        "epoch": 0.7245424078370714,
        "step": 5621
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.778939962387085,
        "learning_rate": 9.287420366546492e-05,
        "epoch": 0.7246713070378964,
        "step": 5622
    },
    {
        "loss": 2.2409,
        "grad_norm": 3.08156156539917,
        "learning_rate": 9.284285880837946e-05,
        "epoch": 0.7248002062387213,
        "step": 5623
    },
    {
        "loss": 1.9018,
        "grad_norm": 2.996281147003174,
        "learning_rate": 9.281145047637943e-05,
        "epoch": 0.7249291054395463,
        "step": 5624
    },
    {
        "loss": 2.596,
        "grad_norm": 1.6279770135879517,
        "learning_rate": 9.277997871599857e-05,
        "epoch": 0.7250580046403712,
        "step": 5625
    },
    {
        "loss": 1.8899,
        "grad_norm": 2.7644121646881104,
        "learning_rate": 9.27484435738647e-05,
        "epoch": 0.7251869038411962,
        "step": 5626
    },
    {
        "loss": 2.3293,
        "grad_norm": 1.3218802213668823,
        "learning_rate": 9.271684509669949e-05,
        "epoch": 0.7253158030420211,
        "step": 5627
    },
    {
        "loss": 1.9745,
        "grad_norm": 2.021005153656006,
        "learning_rate": 9.268518333131848e-05,
        "epoch": 0.7254447022428461,
        "step": 5628
    },
    {
        "loss": 1.8921,
        "grad_norm": 2.519601821899414,
        "learning_rate": 9.265345832463086e-05,
        "epoch": 0.725573601443671,
        "step": 5629
    },
    {
        "loss": 2.5439,
        "grad_norm": 2.6227779388427734,
        "learning_rate": 9.262167012363972e-05,
        "epoch": 0.725702500644496,
        "step": 5630
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.9015400409698486,
        "learning_rate": 9.258981877544162e-05,
        "epoch": 0.725831399845321,
        "step": 5631
    },
    {
        "loss": 1.7016,
        "grad_norm": 2.6893391609191895,
        "learning_rate": 9.255790432722672e-05,
        "epoch": 0.7259602990461459,
        "step": 5632
    },
    {
        "loss": 2.2124,
        "grad_norm": 3.0634829998016357,
        "learning_rate": 9.25259268262787e-05,
        "epoch": 0.7260891982469708,
        "step": 5633
    },
    {
        "loss": 2.1776,
        "grad_norm": 1.6048463582992554,
        "learning_rate": 9.249388631997462e-05,
        "epoch": 0.7262180974477959,
        "step": 5634
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.991397738456726,
        "learning_rate": 9.246178285578489e-05,
        "epoch": 0.7263469966486208,
        "step": 5635
    },
    {
        "loss": 1.7597,
        "grad_norm": 2.592052459716797,
        "learning_rate": 9.24296164812732e-05,
        "epoch": 0.7264758958494457,
        "step": 5636
    },
    {
        "loss": 1.6354,
        "grad_norm": 1.8783104419708252,
        "learning_rate": 9.239738724409647e-05,
        "epoch": 0.7266047950502706,
        "step": 5637
    },
    {
        "loss": 2.453,
        "grad_norm": 1.6345070600509644,
        "learning_rate": 9.236509519200471e-05,
        "epoch": 0.7267336942510957,
        "step": 5638
    },
    {
        "loss": 1.8774,
        "grad_norm": 2.1583447456359863,
        "learning_rate": 9.233274037284106e-05,
        "epoch": 0.7268625934519206,
        "step": 5639
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.677909255027771,
        "learning_rate": 9.230032283454159e-05,
        "epoch": 0.7269914926527455,
        "step": 5640
    },
    {
        "loss": 1.5873,
        "grad_norm": 2.1193296909332275,
        "learning_rate": 9.22678426251353e-05,
        "epoch": 0.7271203918535705,
        "step": 5641
    },
    {
        "loss": 1.7024,
        "grad_norm": 2.1818106174468994,
        "learning_rate": 9.22352997927441e-05,
        "epoch": 0.7272492910543955,
        "step": 5642
    },
    {
        "loss": 2.3364,
        "grad_norm": 2.834216356277466,
        "learning_rate": 9.220269438558263e-05,
        "epoch": 0.7273781902552204,
        "step": 5643
    },
    {
        "loss": 2.202,
        "grad_norm": 2.170545816421509,
        "learning_rate": 9.217002645195824e-05,
        "epoch": 0.7275070894560454,
        "step": 5644
    },
    {
        "loss": 2.459,
        "grad_norm": 1.57683527469635,
        "learning_rate": 9.213729604027093e-05,
        "epoch": 0.7276359886568703,
        "step": 5645
    },
    {
        "loss": 1.1483,
        "grad_norm": 3.312868595123291,
        "learning_rate": 9.210450319901327e-05,
        "epoch": 0.7277648878576953,
        "step": 5646
    },
    {
        "loss": 2.3182,
        "grad_norm": 2.482161521911621,
        "learning_rate": 9.207164797677032e-05,
        "epoch": 0.7278937870585203,
        "step": 5647
    },
    {
        "loss": 2.4221,
        "grad_norm": 3.4446098804473877,
        "learning_rate": 9.203873042221955e-05,
        "epoch": 0.7280226862593452,
        "step": 5648
    },
    {
        "loss": 2.1874,
        "grad_norm": 1.4659788608551025,
        "learning_rate": 9.20057505841308e-05,
        "epoch": 0.7281515854601701,
        "step": 5649
    },
    {
        "loss": 2.2646,
        "grad_norm": 2.2892327308654785,
        "learning_rate": 9.197270851136615e-05,
        "epoch": 0.7282804846609952,
        "step": 5650
    },
    {
        "loss": 1.8925,
        "grad_norm": 1.8516825437545776,
        "learning_rate": 9.193960425287999e-05,
        "epoch": 0.7284093838618201,
        "step": 5651
    },
    {
        "loss": 1.7719,
        "grad_norm": 2.381232261657715,
        "learning_rate": 9.190643785771871e-05,
        "epoch": 0.728538283062645,
        "step": 5652
    },
    {
        "loss": 2.0511,
        "grad_norm": 2.365323305130005,
        "learning_rate": 9.18732093750208e-05,
        "epoch": 0.7286671822634699,
        "step": 5653
    },
    {
        "loss": 2.0552,
        "grad_norm": 2.3765437602996826,
        "learning_rate": 9.183991885401677e-05,
        "epoch": 0.728796081464295,
        "step": 5654
    },
    {
        "loss": 2.4712,
        "grad_norm": 1.7146574258804321,
        "learning_rate": 9.180656634402906e-05,
        "epoch": 0.7289249806651199,
        "step": 5655
    },
    {
        "loss": 1.1173,
        "grad_norm": 3.4131221771240234,
        "learning_rate": 9.177315189447184e-05,
        "epoch": 0.7290538798659448,
        "step": 5656
    },
    {
        "loss": 1.8568,
        "grad_norm": 2.644439220428467,
        "learning_rate": 9.173967555485126e-05,
        "epoch": 0.7291827790667698,
        "step": 5657
    },
    {
        "loss": 1.6577,
        "grad_norm": 1.8577097654342651,
        "learning_rate": 9.170613737476492e-05,
        "epoch": 0.7293116782675947,
        "step": 5658
    },
    {
        "loss": 1.7286,
        "grad_norm": 2.7804975509643555,
        "learning_rate": 9.16725374039022e-05,
        "epoch": 0.7294405774684197,
        "step": 5659
    },
    {
        "loss": 1.8675,
        "grad_norm": 1.228867530822754,
        "learning_rate": 9.163887569204395e-05,
        "epoch": 0.7295694766692447,
        "step": 5660
    },
    {
        "loss": 2.2964,
        "grad_norm": 1.5032058954238892,
        "learning_rate": 9.160515228906257e-05,
        "epoch": 0.7296983758700696,
        "step": 5661
    },
    {
        "loss": 2.2197,
        "grad_norm": 1.5246741771697998,
        "learning_rate": 9.157136724492172e-05,
        "epoch": 0.7298272750708945,
        "step": 5662
    },
    {
        "loss": 1.7665,
        "grad_norm": 1.781550645828247,
        "learning_rate": 9.153752060967667e-05,
        "epoch": 0.7299561742717195,
        "step": 5663
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.56260347366333,
        "learning_rate": 9.150361243347356e-05,
        "epoch": 0.7300850734725445,
        "step": 5664
    },
    {
        "loss": 1.928,
        "grad_norm": 2.5167458057403564,
        "learning_rate": 9.146964276655e-05,
        "epoch": 0.7302139726733694,
        "step": 5665
    },
    {
        "loss": 1.9478,
        "grad_norm": 2.844177484512329,
        "learning_rate": 9.143561165923458e-05,
        "epoch": 0.7303428718741943,
        "step": 5666
    },
    {
        "loss": 2.1857,
        "grad_norm": 1.7620110511779785,
        "learning_rate": 9.140151916194694e-05,
        "epoch": 0.7304717710750194,
        "step": 5667
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.1105775833129883,
        "learning_rate": 9.136736532519765e-05,
        "epoch": 0.7306006702758443,
        "step": 5668
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.5870232582092285,
        "learning_rate": 9.133315019958824e-05,
        "epoch": 0.7307295694766692,
        "step": 5669
    },
    {
        "loss": 1.3518,
        "grad_norm": 2.75445556640625,
        "learning_rate": 9.129887383581102e-05,
        "epoch": 0.7308584686774942,
        "step": 5670
    },
    {
        "loss": 1.633,
        "grad_norm": 3.6720070838928223,
        "learning_rate": 9.126453628464889e-05,
        "epoch": 0.7309873678783192,
        "step": 5671
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.7893943786621094,
        "learning_rate": 9.123013759697554e-05,
        "epoch": 0.7311162670791441,
        "step": 5672
    },
    {
        "loss": 1.3349,
        "grad_norm": 2.3660905361175537,
        "learning_rate": 9.11956778237553e-05,
        "epoch": 0.731245166279969,
        "step": 5673
    },
    {
        "loss": 1.1163,
        "grad_norm": 2.5352423191070557,
        "learning_rate": 9.11611570160428e-05,
        "epoch": 0.731374065480794,
        "step": 5674
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.9919244050979614,
        "learning_rate": 9.11265752249833e-05,
        "epoch": 0.731502964681619,
        "step": 5675
    },
    {
        "loss": 2.2971,
        "grad_norm": 1.7709064483642578,
        "learning_rate": 9.10919325018123e-05,
        "epoch": 0.731631863882444,
        "step": 5676
    },
    {
        "loss": 1.5113,
        "grad_norm": 4.417484283447266,
        "learning_rate": 9.105722889785554e-05,
        "epoch": 0.7317607630832689,
        "step": 5677
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.8534601926803589,
        "learning_rate": 9.102246446452902e-05,
        "epoch": 0.7318896622840938,
        "step": 5678
    },
    {
        "loss": 1.5786,
        "grad_norm": 2.6350765228271484,
        "learning_rate": 9.0987639253339e-05,
        "epoch": 0.7320185614849188,
        "step": 5679
    },
    {
        "loss": 2.0206,
        "grad_norm": 2.4382734298706055,
        "learning_rate": 9.095275331588144e-05,
        "epoch": 0.7321474606857438,
        "step": 5680
    },
    {
        "loss": 1.5156,
        "grad_norm": 1.8851436376571655,
        "learning_rate": 9.091780670384264e-05,
        "epoch": 0.7322763598865687,
        "step": 5681
    },
    {
        "loss": 1.8059,
        "grad_norm": 2.2704758644104004,
        "learning_rate": 9.08827994689986e-05,
        "epoch": 0.7324052590873936,
        "step": 5682
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.023683786392212,
        "learning_rate": 9.084773166321511e-05,
        "epoch": 0.7325341582882187,
        "step": 5683
    },
    {
        "loss": 2.0498,
        "grad_norm": 2.1189610958099365,
        "learning_rate": 9.081260333844775e-05,
        "epoch": 0.7326630574890436,
        "step": 5684
    },
    {
        "loss": 2.1038,
        "grad_norm": 1.7655285596847534,
        "learning_rate": 9.077741454674187e-05,
        "epoch": 0.7327919566898685,
        "step": 5685
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.171358346939087,
        "learning_rate": 9.074216534023223e-05,
        "epoch": 0.7329208558906934,
        "step": 5686
    },
    {
        "loss": 1.5536,
        "grad_norm": 3.057084798812866,
        "learning_rate": 9.070685577114322e-05,
        "epoch": 0.7330497550915185,
        "step": 5687
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.129333734512329,
        "learning_rate": 9.06714858917886e-05,
        "epoch": 0.7331786542923434,
        "step": 5688
    },
    {
        "loss": 2.1,
        "grad_norm": 2.282011032104492,
        "learning_rate": 9.063605575457155e-05,
        "epoch": 0.7333075534931683,
        "step": 5689
    },
    {
        "loss": 2.0342,
        "grad_norm": 1.7744687795639038,
        "learning_rate": 9.060056541198436e-05,
        "epoch": 0.7334364526939933,
        "step": 5690
    },
    {
        "loss": 1.8653,
        "grad_norm": 2.451766014099121,
        "learning_rate": 9.056501491660873e-05,
        "epoch": 0.7335653518948183,
        "step": 5691
    },
    {
        "loss": 2.2961,
        "grad_norm": 1.3910975456237793,
        "learning_rate": 9.05294043211154e-05,
        "epoch": 0.7336942510956432,
        "step": 5692
    },
    {
        "loss": 2.077,
        "grad_norm": 1.5400938987731934,
        "learning_rate": 9.049373367826413e-05,
        "epoch": 0.7338231502964682,
        "step": 5693
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.926841139793396,
        "learning_rate": 9.045800304090362e-05,
        "epoch": 0.7339520494972931,
        "step": 5694
    },
    {
        "loss": 2.2156,
        "grad_norm": 2.6705877780914307,
        "learning_rate": 9.042221246197158e-05,
        "epoch": 0.734080948698118,
        "step": 5695
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.8026779890060425,
        "learning_rate": 9.03863619944943e-05,
        "epoch": 0.7342098478989431,
        "step": 5696
    },
    {
        "loss": 2.3432,
        "grad_norm": 2.122239828109741,
        "learning_rate": 9.035045169158707e-05,
        "epoch": 0.734338747099768,
        "step": 5697
    },
    {
        "loss": 1.7746,
        "grad_norm": 2.0094709396362305,
        "learning_rate": 9.031448160645364e-05,
        "epoch": 0.7344676463005929,
        "step": 5698
    },
    {
        "loss": 1.7444,
        "grad_norm": 2.646253824234009,
        "learning_rate": 9.027845179238643e-05,
        "epoch": 0.7345965455014178,
        "step": 5699
    },
    {
        "loss": 2.0608,
        "grad_norm": 2.1426897048950195,
        "learning_rate": 9.024236230276629e-05,
        "epoch": 0.7347254447022429,
        "step": 5700
    },
    {
        "loss": 1.8197,
        "grad_norm": 1.5865681171417236,
        "learning_rate": 9.020621319106251e-05,
        "epoch": 0.7348543439030678,
        "step": 5701
    },
    {
        "loss": 2.4223,
        "grad_norm": 2.723123788833618,
        "learning_rate": 9.017000451083275e-05,
        "epoch": 0.7349832431038927,
        "step": 5702
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.2022886276245117,
        "learning_rate": 9.013373631572284e-05,
        "epoch": 0.7351121423047177,
        "step": 5703
    },
    {
        "loss": 1.5407,
        "grad_norm": 2.187340497970581,
        "learning_rate": 9.009740865946686e-05,
        "epoch": 0.7352410415055427,
        "step": 5704
    },
    {
        "loss": 1.7325,
        "grad_norm": 2.223351001739502,
        "learning_rate": 9.006102159588697e-05,
        "epoch": 0.7353699407063676,
        "step": 5705
    },
    {
        "loss": 2.2296,
        "grad_norm": 2.813786029815674,
        "learning_rate": 9.002457517889333e-05,
        "epoch": 0.7354988399071926,
        "step": 5706
    },
    {
        "loss": 1.4988,
        "grad_norm": 3.1311585903167725,
        "learning_rate": 8.998806946248406e-05,
        "epoch": 0.7356277391080175,
        "step": 5707
    },
    {
        "loss": 1.764,
        "grad_norm": 2.498690128326416,
        "learning_rate": 8.99515045007451e-05,
        "epoch": 0.7357566383088425,
        "step": 5708
    },
    {
        "loss": 1.7252,
        "grad_norm": 2.259152412414551,
        "learning_rate": 8.991488034785021e-05,
        "epoch": 0.7358855375096675,
        "step": 5709
    },
    {
        "loss": 2.203,
        "grad_norm": 1.3458020687103271,
        "learning_rate": 8.98781970580608e-05,
        "epoch": 0.7360144367104924,
        "step": 5710
    },
    {
        "loss": 2.0105,
        "grad_norm": 2.933586597442627,
        "learning_rate": 8.984145468572593e-05,
        "epoch": 0.7361433359113173,
        "step": 5711
    },
    {
        "loss": 1.5034,
        "grad_norm": 3.049959659576416,
        "learning_rate": 8.980465328528219e-05,
        "epoch": 0.7362722351121423,
        "step": 5712
    },
    {
        "loss": 1.2177,
        "grad_norm": 2.936272144317627,
        "learning_rate": 8.976779291125363e-05,
        "epoch": 0.7364011343129673,
        "step": 5713
    },
    {
        "loss": 1.043,
        "grad_norm": 3.389096975326538,
        "learning_rate": 8.973087361825164e-05,
        "epoch": 0.7365300335137922,
        "step": 5714
    },
    {
        "loss": 1.9662,
        "grad_norm": 1.6299896240234375,
        "learning_rate": 8.969389546097495e-05,
        "epoch": 0.7366589327146171,
        "step": 5715
    },
    {
        "loss": 2.2533,
        "grad_norm": 1.919815182685852,
        "learning_rate": 8.965685849420946e-05,
        "epoch": 0.7367878319154422,
        "step": 5716
    },
    {
        "loss": 1.9783,
        "grad_norm": 2.087249517440796,
        "learning_rate": 8.961976277282817e-05,
        "epoch": 0.7369167311162671,
        "step": 5717
    },
    {
        "loss": 2.0579,
        "grad_norm": 1.866882085800171,
        "learning_rate": 8.958260835179136e-05,
        "epoch": 0.737045630317092,
        "step": 5718
    },
    {
        "loss": 2.1002,
        "grad_norm": 2.1750588417053223,
        "learning_rate": 8.95453952861459e-05,
        "epoch": 0.737174529517917,
        "step": 5719
    },
    {
        "loss": 2.5712,
        "grad_norm": 1.3486952781677246,
        "learning_rate": 8.950812363102581e-05,
        "epoch": 0.737303428718742,
        "step": 5720
    },
    {
        "loss": 2.2789,
        "grad_norm": 2.0642807483673096,
        "learning_rate": 8.947079344165187e-05,
        "epoch": 0.7374323279195669,
        "step": 5721
    },
    {
        "loss": 2.4875,
        "grad_norm": 1.7053940296173096,
        "learning_rate": 8.943340477333153e-05,
        "epoch": 0.7375612271203918,
        "step": 5722
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.807604432106018,
        "learning_rate": 8.93959576814589e-05,
        "epoch": 0.7376901263212168,
        "step": 5723
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.8623663187026978,
        "learning_rate": 8.935845222151476e-05,
        "epoch": 0.7378190255220418,
        "step": 5724
    },
    {
        "loss": 1.4877,
        "grad_norm": 3.4006781578063965,
        "learning_rate": 8.932088844906614e-05,
        "epoch": 0.7379479247228667,
        "step": 5725
    },
    {
        "loss": 1.4987,
        "grad_norm": 2.4750490188598633,
        "learning_rate": 8.928326641976665e-05,
        "epoch": 0.7380768239236917,
        "step": 5726
    },
    {
        "loss": 1.7307,
        "grad_norm": 3.018681287765503,
        "learning_rate": 8.924558618935617e-05,
        "epoch": 0.7382057231245166,
        "step": 5727
    },
    {
        "loss": 2.2169,
        "grad_norm": 1.7332879304885864,
        "learning_rate": 8.920784781366074e-05,
        "epoch": 0.7383346223253415,
        "step": 5728
    },
    {
        "loss": 2.0203,
        "grad_norm": 2.1402206420898438,
        "learning_rate": 8.91700513485926e-05,
        "epoch": 0.7384635215261666,
        "step": 5729
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.6498693227767944,
        "learning_rate": 8.913219685015018e-05,
        "epoch": 0.7385924207269915,
        "step": 5730
    },
    {
        "loss": 2.4306,
        "grad_norm": 2.551849126815796,
        "learning_rate": 8.909428437441761e-05,
        "epoch": 0.7387213199278164,
        "step": 5731
    },
    {
        "loss": 1.9317,
        "grad_norm": 1.6597678661346436,
        "learning_rate": 8.905631397756512e-05,
        "epoch": 0.7388502191286414,
        "step": 5732
    },
    {
        "loss": 2.3252,
        "grad_norm": 1.7537802457809448,
        "learning_rate": 8.901828571584868e-05,
        "epoch": 0.7389791183294664,
        "step": 5733
    },
    {
        "loss": 2.1858,
        "grad_norm": 2.172269582748413,
        "learning_rate": 8.898019964561013e-05,
        "epoch": 0.7391080175302913,
        "step": 5734
    },
    {
        "loss": 0.95,
        "grad_norm": 2.1802470684051514,
        "learning_rate": 8.894205582327671e-05,
        "epoch": 0.7392369167311162,
        "step": 5735
    },
    {
        "loss": 1.2573,
        "grad_norm": 2.6296188831329346,
        "learning_rate": 8.890385430536142e-05,
        "epoch": 0.7393658159319412,
        "step": 5736
    },
    {
        "loss": 1.5506,
        "grad_norm": 2.398158073425293,
        "learning_rate": 8.886559514846274e-05,
        "epoch": 0.7394947151327662,
        "step": 5737
    },
    {
        "loss": 2.0516,
        "grad_norm": 2.45698881149292,
        "learning_rate": 8.882727840926437e-05,
        "epoch": 0.7396236143335911,
        "step": 5738
    },
    {
        "loss": 2.2529,
        "grad_norm": 2.4109160900115967,
        "learning_rate": 8.878890414453545e-05,
        "epoch": 0.7397525135344161,
        "step": 5739
    },
    {
        "loss": 1.9297,
        "grad_norm": 2.253065347671509,
        "learning_rate": 8.875047241113045e-05,
        "epoch": 0.739881412735241,
        "step": 5740
    },
    {
        "loss": 1.7799,
        "grad_norm": 3.096421241760254,
        "learning_rate": 8.871198326598873e-05,
        "epoch": 0.740010311936066,
        "step": 5741
    },
    {
        "loss": 1.7545,
        "grad_norm": 2.4227311611175537,
        "learning_rate": 8.867343676613496e-05,
        "epoch": 0.740139211136891,
        "step": 5742
    },
    {
        "loss": 1.5458,
        "grad_norm": 2.303058624267578,
        "learning_rate": 8.863483296867864e-05,
        "epoch": 0.7402681103377159,
        "step": 5743
    },
    {
        "loss": 1.8757,
        "grad_norm": 1.9938774108886719,
        "learning_rate": 8.859617193081418e-05,
        "epoch": 0.7403970095385408,
        "step": 5744
    },
    {
        "loss": 2.1026,
        "grad_norm": 1.8672930002212524,
        "learning_rate": 8.855745370982078e-05,
        "epoch": 0.7405259087393659,
        "step": 5745
    },
    {
        "loss": 1.686,
        "grad_norm": 2.539767265319824,
        "learning_rate": 8.851867836306251e-05,
        "epoch": 0.7406548079401908,
        "step": 5746
    },
    {
        "loss": 2.4975,
        "grad_norm": 2.174201726913452,
        "learning_rate": 8.847984594798781e-05,
        "epoch": 0.7407837071410157,
        "step": 5747
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.49063241481781,
        "learning_rate": 8.844095652212994e-05,
        "epoch": 0.7409126063418406,
        "step": 5748
    },
    {
        "loss": 2.0759,
        "grad_norm": 1.6999492645263672,
        "learning_rate": 8.840201014310645e-05,
        "epoch": 0.7410415055426657,
        "step": 5749
    },
    {
        "loss": 1.4807,
        "grad_norm": 4.403191566467285,
        "learning_rate": 8.83630068686194e-05,
        "epoch": 0.7411704047434906,
        "step": 5750
    },
    {
        "loss": 2.0495,
        "grad_norm": 3.114793300628662,
        "learning_rate": 8.83239467564549e-05,
        "epoch": 0.7412993039443155,
        "step": 5751
    },
    {
        "loss": 2.5033,
        "grad_norm": 1.461220145225525,
        "learning_rate": 8.828482986448362e-05,
        "epoch": 0.7414282031451405,
        "step": 5752
    },
    {
        "loss": 2.4202,
        "grad_norm": 2.195711135864258,
        "learning_rate": 8.82456562506601e-05,
        "epoch": 0.7415571023459655,
        "step": 5753
    },
    {
        "loss": 1.6362,
        "grad_norm": 2.9335105419158936,
        "learning_rate": 8.820642597302301e-05,
        "epoch": 0.7416860015467904,
        "step": 5754
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.092545747756958,
        "learning_rate": 8.816713908969493e-05,
        "epoch": 0.7418149007476154,
        "step": 5755
    },
    {
        "loss": 1.9457,
        "grad_norm": 1.96579110622406,
        "learning_rate": 8.81277956588824e-05,
        "epoch": 0.7419437999484403,
        "step": 5756
    },
    {
        "loss": 1.9263,
        "grad_norm": 2.2110064029693604,
        "learning_rate": 8.808839573887554e-05,
        "epoch": 0.7420726991492653,
        "step": 5757
    },
    {
        "loss": 1.8666,
        "grad_norm": 1.3827728033065796,
        "learning_rate": 8.804893938804838e-05,
        "epoch": 0.7422015983500903,
        "step": 5758
    },
    {
        "loss": 1.2435,
        "grad_norm": 3.0803349018096924,
        "learning_rate": 8.800942666485848e-05,
        "epoch": 0.7423304975509152,
        "step": 5759
    },
    {
        "loss": 1.3019,
        "grad_norm": 3.579294443130493,
        "learning_rate": 8.796985762784686e-05,
        "epoch": 0.7424593967517401,
        "step": 5760
    },
    {
        "loss": 1.9346,
        "grad_norm": 2.232419490814209,
        "learning_rate": 8.793023233563807e-05,
        "epoch": 0.7425882959525651,
        "step": 5761
    },
    {
        "loss": 1.4888,
        "grad_norm": 2.690958261489868,
        "learning_rate": 8.789055084693996e-05,
        "epoch": 0.7427171951533901,
        "step": 5762
    },
    {
        "loss": 1.2687,
        "grad_norm": 2.387505292892456,
        "learning_rate": 8.785081322054351e-05,
        "epoch": 0.742846094354215,
        "step": 5763
    },
    {
        "loss": 2.0983,
        "grad_norm": 2.541374921798706,
        "learning_rate": 8.781101951532316e-05,
        "epoch": 0.7429749935550399,
        "step": 5764
    },
    {
        "loss": 2.2451,
        "grad_norm": 1.8456289768218994,
        "learning_rate": 8.77711697902362e-05,
        "epoch": 0.7431038927558649,
        "step": 5765
    },
    {
        "loss": 1.7972,
        "grad_norm": 3.0535812377929688,
        "learning_rate": 8.773126410432302e-05,
        "epoch": 0.7432327919566899,
        "step": 5766
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.079763412475586,
        "learning_rate": 8.769130251670687e-05,
        "epoch": 0.7433616911575148,
        "step": 5767
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.4597744941711426,
        "learning_rate": 8.765128508659386e-05,
        "epoch": 0.7434905903583398,
        "step": 5768
    },
    {
        "loss": 2.3155,
        "grad_norm": 1.8138830661773682,
        "learning_rate": 8.761121187327282e-05,
        "epoch": 0.7436194895591647,
        "step": 5769
    },
    {
        "loss": 2.3311,
        "grad_norm": 1.684288501739502,
        "learning_rate": 8.75710829361152e-05,
        "epoch": 0.7437483887599897,
        "step": 5770
    },
    {
        "loss": 1.5806,
        "grad_norm": 4.112788677215576,
        "learning_rate": 8.753089833457506e-05,
        "epoch": 0.7438772879608146,
        "step": 5771
    },
    {
        "loss": 1.5766,
        "grad_norm": 2.723921537399292,
        "learning_rate": 8.749065812818891e-05,
        "epoch": 0.7440061871616396,
        "step": 5772
    },
    {
        "loss": 1.9969,
        "grad_norm": 2.336273670196533,
        "learning_rate": 8.745036237657562e-05,
        "epoch": 0.7441350863624645,
        "step": 5773
    },
    {
        "loss": 1.0444,
        "grad_norm": 3.2047202587127686,
        "learning_rate": 8.74100111394364e-05,
        "epoch": 0.7442639855632895,
        "step": 5774
    },
    {
        "loss": 2.2518,
        "grad_norm": 1.9067683219909668,
        "learning_rate": 8.736960447655461e-05,
        "epoch": 0.7443928847641145,
        "step": 5775
    },
    {
        "loss": 2.2488,
        "grad_norm": 1.44832181930542,
        "learning_rate": 8.732914244779579e-05,
        "epoch": 0.7445217839649394,
        "step": 5776
    },
    {
        "loss": 2.3085,
        "grad_norm": 1.4437118768692017,
        "learning_rate": 8.728862511310744e-05,
        "epoch": 0.7446506831657643,
        "step": 5777
    },
    {
        "loss": 1.9582,
        "grad_norm": 1.9649866819381714,
        "learning_rate": 8.724805253251907e-05,
        "epoch": 0.7447795823665894,
        "step": 5778
    },
    {
        "loss": 1.45,
        "grad_norm": 2.221406936645508,
        "learning_rate": 8.720742476614197e-05,
        "epoch": 0.7449084815674143,
        "step": 5779
    },
    {
        "loss": 1.9591,
        "grad_norm": 2.2190442085266113,
        "learning_rate": 8.716674187416927e-05,
        "epoch": 0.7450373807682392,
        "step": 5780
    },
    {
        "loss": 2.1063,
        "grad_norm": 2.182525634765625,
        "learning_rate": 8.712600391687569e-05,
        "epoch": 0.7451662799690641,
        "step": 5781
    },
    {
        "loss": 1.391,
        "grad_norm": 4.060326099395752,
        "learning_rate": 8.708521095461761e-05,
        "epoch": 0.7452951791698892,
        "step": 5782
    },
    {
        "loss": 1.9063,
        "grad_norm": 2.71421217918396,
        "learning_rate": 8.704436304783286e-05,
        "epoch": 0.7454240783707141,
        "step": 5783
    },
    {
        "loss": 1.573,
        "grad_norm": 1.95702064037323,
        "learning_rate": 8.700346025704063e-05,
        "epoch": 0.745552977571539,
        "step": 5784
    },
    {
        "loss": 2.3584,
        "grad_norm": 2.3527252674102783,
        "learning_rate": 8.69625026428416e-05,
        "epoch": 0.745681876772364,
        "step": 5785
    },
    {
        "loss": 2.3668,
        "grad_norm": 1.5730159282684326,
        "learning_rate": 8.692149026591745e-05,
        "epoch": 0.745810775973189,
        "step": 5786
    },
    {
        "loss": 1.9611,
        "grad_norm": 3.5792977809906006,
        "learning_rate": 8.688042318703113e-05,
        "epoch": 0.7459396751740139,
        "step": 5787
    },
    {
        "loss": 2.0739,
        "grad_norm": 2.72845196723938,
        "learning_rate": 8.68393014670266e-05,
        "epoch": 0.7460685743748389,
        "step": 5788
    },
    {
        "loss": 1.7676,
        "grad_norm": 2.1379830837249756,
        "learning_rate": 8.679812516682877e-05,
        "epoch": 0.7461974735756638,
        "step": 5789
    },
    {
        "loss": 2.0975,
        "grad_norm": 2.713620662689209,
        "learning_rate": 8.675689434744339e-05,
        "epoch": 0.7463263727764888,
        "step": 5790
    },
    {
        "loss": 2.1194,
        "grad_norm": 2.383533477783203,
        "learning_rate": 8.671560906995715e-05,
        "epoch": 0.7464552719773138,
        "step": 5791
    },
    {
        "loss": 2.0421,
        "grad_norm": 1.933205246925354,
        "learning_rate": 8.667426939553715e-05,
        "epoch": 0.7465841711781387,
        "step": 5792
    },
    {
        "loss": 2.1169,
        "grad_norm": 2.7307543754577637,
        "learning_rate": 8.663287538543128e-05,
        "epoch": 0.7467130703789636,
        "step": 5793
    },
    {
        "loss": 1.4918,
        "grad_norm": 2.0929694175720215,
        "learning_rate": 8.659142710096786e-05,
        "epoch": 0.7468419695797887,
        "step": 5794
    },
    {
        "loss": 1.7312,
        "grad_norm": 2.2079832553863525,
        "learning_rate": 8.654992460355565e-05,
        "epoch": 0.7469708687806136,
        "step": 5795
    },
    {
        "loss": 2.1181,
        "grad_norm": 1.782629370689392,
        "learning_rate": 8.650836795468368e-05,
        "epoch": 0.7470997679814385,
        "step": 5796
    },
    {
        "loss": 2.0784,
        "grad_norm": 2.254333019256592,
        "learning_rate": 8.646675721592136e-05,
        "epoch": 0.7472286671822634,
        "step": 5797
    },
    {
        "loss": 1.734,
        "grad_norm": 2.6125741004943848,
        "learning_rate": 8.642509244891803e-05,
        "epoch": 0.7473575663830885,
        "step": 5798
    },
    {
        "loss": 1.9019,
        "grad_norm": 1.6392484903335571,
        "learning_rate": 8.638337371540318e-05,
        "epoch": 0.7474864655839134,
        "step": 5799
    },
    {
        "loss": 1.986,
        "grad_norm": 2.52917742729187,
        "learning_rate": 8.634160107718621e-05,
        "epoch": 0.7476153647847383,
        "step": 5800
    },
    {
        "loss": 2.3202,
        "grad_norm": 2.6108288764953613,
        "learning_rate": 8.629977459615657e-05,
        "epoch": 0.7477442639855633,
        "step": 5801
    },
    {
        "loss": 1.8695,
        "grad_norm": 2.0965609550476074,
        "learning_rate": 8.625789433428316e-05,
        "epoch": 0.7478731631863882,
        "step": 5802
    },
    {
        "loss": 1.6679,
        "grad_norm": 2.089954376220703,
        "learning_rate": 8.621596035361483e-05,
        "epoch": 0.7480020623872132,
        "step": 5803
    },
    {
        "loss": 1.3652,
        "grad_norm": 3.0731725692749023,
        "learning_rate": 8.617397271627998e-05,
        "epoch": 0.7481309615880382,
        "step": 5804
    },
    {
        "loss": 1.7088,
        "grad_norm": 2.221561908721924,
        "learning_rate": 8.613193148448631e-05,
        "epoch": 0.7482598607888631,
        "step": 5805
    },
    {
        "loss": 1.8106,
        "grad_norm": 2.6053547859191895,
        "learning_rate": 8.608983672052108e-05,
        "epoch": 0.748388759989688,
        "step": 5806
    },
    {
        "loss": 1.6639,
        "grad_norm": 4.42511510848999,
        "learning_rate": 8.604768848675102e-05,
        "epoch": 0.748517659190513,
        "step": 5807
    },
    {
        "loss": 2.116,
        "grad_norm": 1.3177764415740967,
        "learning_rate": 8.600548684562169e-05,
        "epoch": 0.748646558391338,
        "step": 5808
    },
    {
        "loss": 1.8907,
        "grad_norm": 2.4093849658966064,
        "learning_rate": 8.596323185965812e-05,
        "epoch": 0.7487754575921629,
        "step": 5809
    },
    {
        "loss": 2.0733,
        "grad_norm": 1.8724852800369263,
        "learning_rate": 8.592092359146424e-05,
        "epoch": 0.7489043567929878,
        "step": 5810
    },
    {
        "loss": 2.0907,
        "grad_norm": 1.7681505680084229,
        "learning_rate": 8.587856210372287e-05,
        "epoch": 0.7490332559938129,
        "step": 5811
    },
    {
        "loss": 1.7623,
        "grad_norm": 1.825759768486023,
        "learning_rate": 8.583614745919572e-05,
        "epoch": 0.7491621551946378,
        "step": 5812
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.005784034729004,
        "learning_rate": 8.57936797207234e-05,
        "epoch": 0.7492910543954627,
        "step": 5813
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.829801321029663,
        "learning_rate": 8.57511589512249e-05,
        "epoch": 0.7494199535962877,
        "step": 5814
    },
    {
        "loss": 2.279,
        "grad_norm": 1.7741799354553223,
        "learning_rate": 8.570858521369804e-05,
        "epoch": 0.7495488527971127,
        "step": 5815
    },
    {
        "loss": 1.7798,
        "grad_norm": 3.171001434326172,
        "learning_rate": 8.5665958571219e-05,
        "epoch": 0.7496777519979376,
        "step": 5816
    },
    {
        "loss": 2.2197,
        "grad_norm": 2.595674753189087,
        "learning_rate": 8.562327908694238e-05,
        "epoch": 0.7498066511987626,
        "step": 5817
    },
    {
        "loss": 2.082,
        "grad_norm": 1.7967263460159302,
        "learning_rate": 8.55805468241009e-05,
        "epoch": 0.7499355503995875,
        "step": 5818
    },
    {
        "loss": 2.4308,
        "grad_norm": 2.2797467708587646,
        "learning_rate": 8.553776184600576e-05,
        "epoch": 0.7500644496004125,
        "step": 5819
    },
    {
        "loss": 2.0069,
        "grad_norm": 2.1205379962921143,
        "learning_rate": 8.549492421604608e-05,
        "epoch": 0.7501933488012374,
        "step": 5820
    },
    {
        "loss": 1.4897,
        "grad_norm": 3.2295703887939453,
        "learning_rate": 8.545203399768902e-05,
        "epoch": 0.7503222480020624,
        "step": 5821
    },
    {
        "loss": 1.5064,
        "grad_norm": 2.492798089981079,
        "learning_rate": 8.540909125447966e-05,
        "epoch": 0.7504511472028873,
        "step": 5822
    },
    {
        "loss": 1.9864,
        "grad_norm": 3.0220837593078613,
        "learning_rate": 8.536609605004098e-05,
        "epoch": 0.7505800464037123,
        "step": 5823
    },
    {
        "loss": 1.1653,
        "grad_norm": 3.024622678756714,
        "learning_rate": 8.532304844807343e-05,
        "epoch": 0.7507089456045373,
        "step": 5824
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.9479870796203613,
        "learning_rate": 8.527994851235542e-05,
        "epoch": 0.7508378448053622,
        "step": 5825
    },
    {
        "loss": 2.0996,
        "grad_norm": 2.324876308441162,
        "learning_rate": 8.523679630674268e-05,
        "epoch": 0.7509667440061871,
        "step": 5826
    },
    {
        "loss": 1.9371,
        "grad_norm": 3.093482732772827,
        "learning_rate": 8.519359189516847e-05,
        "epoch": 0.7510956432070122,
        "step": 5827
    },
    {
        "loss": 1.48,
        "grad_norm": 3.986386299133301,
        "learning_rate": 8.515033534164334e-05,
        "epoch": 0.7512245424078371,
        "step": 5828
    },
    {
        "loss": 2.0545,
        "grad_norm": 2.9801158905029297,
        "learning_rate": 8.51070267102552e-05,
        "epoch": 0.751353441608662,
        "step": 5829
    },
    {
        "loss": 1.7375,
        "grad_norm": 3.567645788192749,
        "learning_rate": 8.506366606516892e-05,
        "epoch": 0.751482340809487,
        "step": 5830
    },
    {
        "loss": 2.5993,
        "grad_norm": 1.5367035865783691,
        "learning_rate": 8.502025347062666e-05,
        "epoch": 0.751611240010312,
        "step": 5831
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.31992769241333,
        "learning_rate": 8.497678899094737e-05,
        "epoch": 0.7517401392111369,
        "step": 5832
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.5948336124420166,
        "learning_rate": 8.4933272690527e-05,
        "epoch": 0.7518690384119618,
        "step": 5833
    },
    {
        "loss": 2.3916,
        "grad_norm": 2.3863284587860107,
        "learning_rate": 8.488970463383818e-05,
        "epoch": 0.7519979376127868,
        "step": 5834
    },
    {
        "loss": 1.5636,
        "grad_norm": 3.053983211517334,
        "learning_rate": 8.484608488543029e-05,
        "epoch": 0.7521268368136118,
        "step": 5835
    },
    {
        "loss": 2.0595,
        "grad_norm": 2.3823235034942627,
        "learning_rate": 8.480241350992924e-05,
        "epoch": 0.7522557360144367,
        "step": 5836
    },
    {
        "loss": 1.9961,
        "grad_norm": 1.7939196825027466,
        "learning_rate": 8.475869057203746e-05,
        "epoch": 0.7523846352152617,
        "step": 5837
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.641033411026001,
        "learning_rate": 8.471491613653377e-05,
        "epoch": 0.7525135344160866,
        "step": 5838
    },
    {
        "loss": 1.9885,
        "grad_norm": 1.6676307916641235,
        "learning_rate": 8.467109026827329e-05,
        "epoch": 0.7526424336169115,
        "step": 5839
    },
    {
        "loss": 1.0331,
        "grad_norm": 3.1421985626220703,
        "learning_rate": 8.462721303218734e-05,
        "epoch": 0.7527713328177366,
        "step": 5840
    },
    {
        "loss": 2.4924,
        "grad_norm": 1.7023563385009766,
        "learning_rate": 8.458328449328331e-05,
        "epoch": 0.7529002320185615,
        "step": 5841
    },
    {
        "loss": 0.9061,
        "grad_norm": 3.027261972427368,
        "learning_rate": 8.453930471664468e-05,
        "epoch": 0.7530291312193864,
        "step": 5842
    },
    {
        "loss": 1.8373,
        "grad_norm": 2.1236414909362793,
        "learning_rate": 8.449527376743076e-05,
        "epoch": 0.7531580304202113,
        "step": 5843
    },
    {
        "loss": 2.2337,
        "grad_norm": 1.6681323051452637,
        "learning_rate": 8.445119171087671e-05,
        "epoch": 0.7532869296210364,
        "step": 5844
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.5253058671951294,
        "learning_rate": 8.440705861229345e-05,
        "epoch": 0.7534158288218613,
        "step": 5845
    },
    {
        "loss": 2.0376,
        "grad_norm": 2.4913947582244873,
        "learning_rate": 8.436287453706744e-05,
        "epoch": 0.7535447280226862,
        "step": 5846
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.7991268634796143,
        "learning_rate": 8.431863955066071e-05,
        "epoch": 0.7536736272235112,
        "step": 5847
    },
    {
        "loss": 1.7095,
        "grad_norm": 3.5346322059631348,
        "learning_rate": 8.427435371861072e-05,
        "epoch": 0.7538025264243362,
        "step": 5848
    },
    {
        "loss": 2.3277,
        "grad_norm": 2.1191446781158447,
        "learning_rate": 8.423001710653026e-05,
        "epoch": 0.7539314256251611,
        "step": 5849
    },
    {
        "loss": 1.8518,
        "grad_norm": 3.680758476257324,
        "learning_rate": 8.418562978010737e-05,
        "epoch": 0.7540603248259861,
        "step": 5850
    },
    {
        "loss": 2.3026,
        "grad_norm": 2.403561592102051,
        "learning_rate": 8.414119180510513e-05,
        "epoch": 0.754189224026811,
        "step": 5851
    },
    {
        "loss": 1.4187,
        "grad_norm": 3.736156940460205,
        "learning_rate": 8.409670324736192e-05,
        "epoch": 0.754318123227636,
        "step": 5852
    },
    {
        "loss": 1.7616,
        "grad_norm": 3.006648540496826,
        "learning_rate": 8.405216417279069e-05,
        "epoch": 0.754447022428461,
        "step": 5853
    },
    {
        "loss": 2.3762,
        "grad_norm": 1.872791051864624,
        "learning_rate": 8.400757464737951e-05,
        "epoch": 0.7545759216292859,
        "step": 5854
    },
    {
        "loss": 2.2368,
        "grad_norm": 2.268533229827881,
        "learning_rate": 8.396293473719113e-05,
        "epoch": 0.7547048208301108,
        "step": 5855
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.5094271898269653,
        "learning_rate": 8.391824450836288e-05,
        "epoch": 0.7548337200309359,
        "step": 5856
    },
    {
        "loss": 1.6214,
        "grad_norm": 1.853895664215088,
        "learning_rate": 8.387350402710665e-05,
        "epoch": 0.7549626192317608,
        "step": 5857
    },
    {
        "loss": 2.1661,
        "grad_norm": 1.7296934127807617,
        "learning_rate": 8.382871335970902e-05,
        "epoch": 0.7550915184325857,
        "step": 5858
    },
    {
        "loss": 1.9833,
        "grad_norm": 2.8365581035614014,
        "learning_rate": 8.378387257253054e-05,
        "epoch": 0.7552204176334106,
        "step": 5859
    },
    {
        "loss": 2.328,
        "grad_norm": 1.3881279230117798,
        "learning_rate": 8.373898173200627e-05,
        "epoch": 0.7553493168342357,
        "step": 5860
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.2626309394836426,
        "learning_rate": 8.369404090464535e-05,
        "epoch": 0.7554782160350606,
        "step": 5861
    },
    {
        "loss": 1.658,
        "grad_norm": 2.3117172718048096,
        "learning_rate": 8.364905015703102e-05,
        "epoch": 0.7556071152358855,
        "step": 5862
    },
    {
        "loss": 2.1333,
        "grad_norm": 1.0650135278701782,
        "learning_rate": 8.360400955582036e-05,
        "epoch": 0.7557360144367105,
        "step": 5863
    },
    {
        "loss": 2.1271,
        "grad_norm": 2.030629873275757,
        "learning_rate": 8.35589191677446e-05,
        "epoch": 0.7558649136375355,
        "step": 5864
    },
    {
        "loss": 1.9535,
        "grad_norm": 1.3139638900756836,
        "learning_rate": 8.351377905960835e-05,
        "epoch": 0.7559938128383604,
        "step": 5865
    },
    {
        "loss": 1.9933,
        "grad_norm": 2.29384183883667,
        "learning_rate": 8.346858929829016e-05,
        "epoch": 0.7561227120391854,
        "step": 5866
    },
    {
        "loss": 2.0879,
        "grad_norm": 1.5006904602050781,
        "learning_rate": 8.342334995074199e-05,
        "epoch": 0.7562516112400103,
        "step": 5867
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.981133222579956,
        "learning_rate": 8.337806108398947e-05,
        "epoch": 0.7563805104408353,
        "step": 5868
    },
    {
        "loss": 2.7209,
        "grad_norm": 1.608693242073059,
        "learning_rate": 8.333272276513127e-05,
        "epoch": 0.7565094096416602,
        "step": 5869
    },
    {
        "loss": 1.0409,
        "grad_norm": 2.931553840637207,
        "learning_rate": 8.328733506133965e-05,
        "epoch": 0.7566383088424852,
        "step": 5870
    },
    {
        "loss": 2.4074,
        "grad_norm": 2.394254446029663,
        "learning_rate": 8.324189803985996e-05,
        "epoch": 0.7567672080433101,
        "step": 5871
    },
    {
        "loss": 2.355,
        "grad_norm": 1.983015775680542,
        "learning_rate": 8.319641176801039e-05,
        "epoch": 0.7568961072441351,
        "step": 5872
    },
    {
        "loss": 2.1246,
        "grad_norm": 2.3077211380004883,
        "learning_rate": 8.31508763131823e-05,
        "epoch": 0.7570250064449601,
        "step": 5873
    },
    {
        "loss": 1.8833,
        "grad_norm": 2.0418953895568848,
        "learning_rate": 8.310529174284005e-05,
        "epoch": 0.757153905645785,
        "step": 5874
    },
    {
        "loss": 2.5511,
        "grad_norm": 1.637437105178833,
        "learning_rate": 8.30596581245204e-05,
        "epoch": 0.7572828048466099,
        "step": 5875
    },
    {
        "loss": 1.4138,
        "grad_norm": 2.1614418029785156,
        "learning_rate": 8.301397552583313e-05,
        "epoch": 0.7574117040474349,
        "step": 5876
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.5279960632324219,
        "learning_rate": 8.296824401446042e-05,
        "epoch": 0.7575406032482599,
        "step": 5877
    },
    {
        "loss": 1.3272,
        "grad_norm": 2.8586134910583496,
        "learning_rate": 8.292246365815687e-05,
        "epoch": 0.7576695024490848,
        "step": 5878
    },
    {
        "loss": 2.0799,
        "grad_norm": 2.153827667236328,
        "learning_rate": 8.287663452474951e-05,
        "epoch": 0.7577984016499097,
        "step": 5879
    },
    {
        "loss": 2.1741,
        "grad_norm": 1.5867279767990112,
        "learning_rate": 8.283075668213781e-05,
        "epoch": 0.7579273008507347,
        "step": 5880
    },
    {
        "loss": 1.5039,
        "grad_norm": 4.016031742095947,
        "learning_rate": 8.278483019829302e-05,
        "epoch": 0.7580562000515597,
        "step": 5881
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.0300559997558594,
        "learning_rate": 8.273885514125884e-05,
        "epoch": 0.7581850992523846,
        "step": 5882
    },
    {
        "loss": 1.9158,
        "grad_norm": 3.7912821769714355,
        "learning_rate": 8.26928315791507e-05,
        "epoch": 0.7583139984532096,
        "step": 5883
    },
    {
        "loss": 2.1355,
        "grad_norm": 1.6922311782836914,
        "learning_rate": 8.264675958015604e-05,
        "epoch": 0.7584428976540345,
        "step": 5884
    },
    {
        "loss": 1.7148,
        "grad_norm": 1.9139736890792847,
        "learning_rate": 8.260063921253382e-05,
        "epoch": 0.7585717968548595,
        "step": 5885
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.62856924533844,
        "learning_rate": 8.255447054461499e-05,
        "epoch": 0.7587006960556845,
        "step": 5886
    },
    {
        "loss": 2.2732,
        "grad_norm": 2.330920934677124,
        "learning_rate": 8.250825364480186e-05,
        "epoch": 0.7588295952565094,
        "step": 5887
    },
    {
        "loss": 1.8086,
        "grad_norm": 2.005851984024048,
        "learning_rate": 8.246198858156822e-05,
        "epoch": 0.7589584944573343,
        "step": 5888
    },
    {
        "loss": 1.951,
        "grad_norm": 2.568814516067505,
        "learning_rate": 8.241567542345924e-05,
        "epoch": 0.7590873936581594,
        "step": 5889
    },
    {
        "loss": 1.734,
        "grad_norm": 3.584699869155884,
        "learning_rate": 8.236931423909142e-05,
        "epoch": 0.7592162928589843,
        "step": 5890
    },
    {
        "loss": 2.236,
        "grad_norm": 3.004258155822754,
        "learning_rate": 8.232290509715218e-05,
        "epoch": 0.7593451920598092,
        "step": 5891
    },
    {
        "loss": 2.1347,
        "grad_norm": 1.7118381261825562,
        "learning_rate": 8.227644806640026e-05,
        "epoch": 0.7594740912606341,
        "step": 5892
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.722562551498413,
        "learning_rate": 8.222994321566524e-05,
        "epoch": 0.7596029904614592,
        "step": 5893
    },
    {
        "loss": 2.0227,
        "grad_norm": 1.3218573331832886,
        "learning_rate": 8.218339061384752e-05,
        "epoch": 0.7597318896622841,
        "step": 5894
    },
    {
        "loss": 2.1434,
        "grad_norm": 1.5337131023406982,
        "learning_rate": 8.21367903299183e-05,
        "epoch": 0.759860788863109,
        "step": 5895
    },
    {
        "loss": 2.5382,
        "grad_norm": 2.0388686656951904,
        "learning_rate": 8.209014243291943e-05,
        "epoch": 0.759989688063934,
        "step": 5896
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.3053653240203857,
        "learning_rate": 8.204344699196315e-05,
        "epoch": 0.760118587264759,
        "step": 5897
    },
    {
        "loss": 1.8485,
        "grad_norm": 1.6922345161437988,
        "learning_rate": 8.199670407623241e-05,
        "epoch": 0.7602474864655839,
        "step": 5898
    },
    {
        "loss": 1.9302,
        "grad_norm": 1.8629721403121948,
        "learning_rate": 8.194991375498029e-05,
        "epoch": 0.7603763856664089,
        "step": 5899
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.4412174224853516,
        "learning_rate": 8.190307609753016e-05,
        "epoch": 0.7605052848672338,
        "step": 5900
    },
    {
        "loss": 2.0085,
        "grad_norm": 2.543489694595337,
        "learning_rate": 8.185619117327554e-05,
        "epoch": 0.7606341840680588,
        "step": 5901
    },
    {
        "loss": 1.9293,
        "grad_norm": 2.7534451484680176,
        "learning_rate": 8.180925905167997e-05,
        "epoch": 0.7607630832688838,
        "step": 5902
    },
    {
        "loss": 1.83,
        "grad_norm": 2.9597113132476807,
        "learning_rate": 8.176227980227694e-05,
        "epoch": 0.7608919824697087,
        "step": 5903
    },
    {
        "loss": 1.5168,
        "grad_norm": 2.655724048614502,
        "learning_rate": 8.171525349466969e-05,
        "epoch": 0.7610208816705336,
        "step": 5904
    },
    {
        "loss": 1.4729,
        "grad_norm": 2.947054862976074,
        "learning_rate": 8.166818019853125e-05,
        "epoch": 0.7611497808713586,
        "step": 5905
    },
    {
        "loss": 2.3016,
        "grad_norm": 1.8767244815826416,
        "learning_rate": 8.162105998360423e-05,
        "epoch": 0.7612786800721836,
        "step": 5906
    },
    {
        "loss": 2.3785,
        "grad_norm": 1.9799124002456665,
        "learning_rate": 8.15738929197008e-05,
        "epoch": 0.7614075792730085,
        "step": 5907
    },
    {
        "loss": 2.3211,
        "grad_norm": 1.497515082359314,
        "learning_rate": 8.15266790767025e-05,
        "epoch": 0.7615364784738334,
        "step": 5908
    },
    {
        "loss": 1.9062,
        "grad_norm": 2.1264970302581787,
        "learning_rate": 8.147941852456015e-05,
        "epoch": 0.7616653776746585,
        "step": 5909
    },
    {
        "loss": 1.9665,
        "grad_norm": 2.464794397354126,
        "learning_rate": 8.143211133329386e-05,
        "epoch": 0.7617942768754834,
        "step": 5910
    },
    {
        "loss": 2.003,
        "grad_norm": 1.366331934928894,
        "learning_rate": 8.138475757299277e-05,
        "epoch": 0.7619231760763083,
        "step": 5911
    },
    {
        "loss": 1.9146,
        "grad_norm": 2.379626512527466,
        "learning_rate": 8.133735731381503e-05,
        "epoch": 0.7620520752771333,
        "step": 5912
    },
    {
        "loss": 0.7045,
        "grad_norm": 3.5141608715057373,
        "learning_rate": 8.128991062598769e-05,
        "epoch": 0.7621809744779582,
        "step": 5913
    },
    {
        "loss": 1.7122,
        "grad_norm": 3.0168445110321045,
        "learning_rate": 8.124241757980657e-05,
        "epoch": 0.7623098736787832,
        "step": 5914
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.3570142984390259,
        "learning_rate": 8.11948782456362e-05,
        "epoch": 0.7624387728796082,
        "step": 5915
    },
    {
        "loss": 2.0457,
        "grad_norm": 2.676717758178711,
        "learning_rate": 8.114729269390968e-05,
        "epoch": 0.7625676720804331,
        "step": 5916
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.7646863460540771,
        "learning_rate": 8.109966099512856e-05,
        "epoch": 0.762696571281258,
        "step": 5917
    },
    {
        "loss": 1.7249,
        "grad_norm": 3.06145977973938,
        "learning_rate": 8.105198321986277e-05,
        "epoch": 0.762825470482083,
        "step": 5918
    },
    {
        "loss": 2.013,
        "grad_norm": 2.022770643234253,
        "learning_rate": 8.100425943875062e-05,
        "epoch": 0.762954369682908,
        "step": 5919
    },
    {
        "loss": 1.8138,
        "grad_norm": 3.162238121032715,
        "learning_rate": 8.095648972249835e-05,
        "epoch": 0.7630832688837329,
        "step": 5920
    },
    {
        "loss": 2.2531,
        "grad_norm": 2.1676712036132812,
        "learning_rate": 8.090867414188044e-05,
        "epoch": 0.7632121680845578,
        "step": 5921
    },
    {
        "loss": 1.6604,
        "grad_norm": 1.5264240503311157,
        "learning_rate": 8.086081276773924e-05,
        "epoch": 0.7633410672853829,
        "step": 5922
    },
    {
        "loss": 2.1145,
        "grad_norm": 2.1883246898651123,
        "learning_rate": 8.081290567098501e-05,
        "epoch": 0.7634699664862078,
        "step": 5923
    },
    {
        "loss": 2.3892,
        "grad_norm": 2.0513968467712402,
        "learning_rate": 8.076495292259562e-05,
        "epoch": 0.7635988656870327,
        "step": 5924
    },
    {
        "loss": 2.149,
        "grad_norm": 1.4604185819625854,
        "learning_rate": 8.071695459361687e-05,
        "epoch": 0.7637277648878577,
        "step": 5925
    },
    {
        "loss": 2.0068,
        "grad_norm": 2.645962953567505,
        "learning_rate": 8.066891075516172e-05,
        "epoch": 0.7638566640886827,
        "step": 5926
    },
    {
        "loss": 1.5588,
        "grad_norm": 3.783051013946533,
        "learning_rate": 8.062082147841075e-05,
        "epoch": 0.7639855632895076,
        "step": 5927
    },
    {
        "loss": 0.5783,
        "grad_norm": 2.361454725265503,
        "learning_rate": 8.057268683461188e-05,
        "epoch": 0.7641144624903325,
        "step": 5928
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.989892601966858,
        "learning_rate": 8.052450689508017e-05,
        "epoch": 0.7642433616911575,
        "step": 5929
    },
    {
        "loss": 1.7649,
        "grad_norm": 2.643373966217041,
        "learning_rate": 8.047628173119777e-05,
        "epoch": 0.7643722608919825,
        "step": 5930
    },
    {
        "loss": 2.2147,
        "grad_norm": 2.4713330268859863,
        "learning_rate": 8.0428011414414e-05,
        "epoch": 0.7645011600928074,
        "step": 5931
    },
    {
        "loss": 2.0255,
        "grad_norm": 1.2552891969680786,
        "learning_rate": 8.037969601624498e-05,
        "epoch": 0.7646300592936324,
        "step": 5932
    },
    {
        "loss": 2.2186,
        "grad_norm": 2.0338001251220703,
        "learning_rate": 8.033133560827346e-05,
        "epoch": 0.7647589584944573,
        "step": 5933
    },
    {
        "loss": 2.0282,
        "grad_norm": 2.381504774093628,
        "learning_rate": 8.028293026214904e-05,
        "epoch": 0.7648878576952823,
        "step": 5934
    },
    {
        "loss": 1.7692,
        "grad_norm": 2.9491610527038574,
        "learning_rate": 8.023448004958802e-05,
        "epoch": 0.7650167568961073,
        "step": 5935
    },
    {
        "loss": 1.8708,
        "grad_norm": 2.473339080810547,
        "learning_rate": 8.018598504237284e-05,
        "epoch": 0.7651456560969322,
        "step": 5936
    },
    {
        "loss": 2.1171,
        "grad_norm": 2.894552230834961,
        "learning_rate": 8.013744531235265e-05,
        "epoch": 0.7652745552977571,
        "step": 5937
    },
    {
        "loss": 1.4809,
        "grad_norm": 2.840726375579834,
        "learning_rate": 8.00888609314427e-05,
        "epoch": 0.7654034544985822,
        "step": 5938
    },
    {
        "loss": 1.6721,
        "grad_norm": 2.540818929672241,
        "learning_rate": 8.00402319716243e-05,
        "epoch": 0.7655323536994071,
        "step": 5939
    },
    {
        "loss": 1.2911,
        "grad_norm": 2.8737378120422363,
        "learning_rate": 7.99915585049449e-05,
        "epoch": 0.765661252900232,
        "step": 5940
    },
    {
        "loss": 2.0129,
        "grad_norm": 2.2810349464416504,
        "learning_rate": 7.994284060351808e-05,
        "epoch": 0.7657901521010569,
        "step": 5941
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.444258213043213,
        "learning_rate": 7.989407833952282e-05,
        "epoch": 0.765919051301882,
        "step": 5942
    },
    {
        "loss": 2.339,
        "grad_norm": 2.248471260070801,
        "learning_rate": 7.984527178520429e-05,
        "epoch": 0.7660479505027069,
        "step": 5943
    },
    {
        "loss": 1.7438,
        "grad_norm": 1.948189616203308,
        "learning_rate": 7.979642101287301e-05,
        "epoch": 0.7661768497035318,
        "step": 5944
    },
    {
        "loss": 1.9971,
        "grad_norm": 2.3824350833892822,
        "learning_rate": 7.9747526094905e-05,
        "epoch": 0.7663057489043568,
        "step": 5945
    },
    {
        "loss": 2.1248,
        "grad_norm": 2.306046724319458,
        "learning_rate": 7.969858710374176e-05,
        "epoch": 0.7664346481051817,
        "step": 5946
    },
    {
        "loss": 2.6003,
        "grad_norm": 1.3375816345214844,
        "learning_rate": 7.964960411189024e-05,
        "epoch": 0.7665635473060067,
        "step": 5947
    },
    {
        "loss": 2.39,
        "grad_norm": 1.634404182434082,
        "learning_rate": 7.960057719192217e-05,
        "epoch": 0.7666924465068317,
        "step": 5948
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.843048572540283,
        "learning_rate": 7.955150641647482e-05,
        "epoch": 0.7668213457076566,
        "step": 5949
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.3039863109588623,
        "learning_rate": 7.950239185825017e-05,
        "epoch": 0.7669502449084815,
        "step": 5950
    },
    {
        "loss": 2.5088,
        "grad_norm": 1.7483570575714111,
        "learning_rate": 7.945323359001517e-05,
        "epoch": 0.7670791441093066,
        "step": 5951
    },
    {
        "loss": 1.661,
        "grad_norm": 2.8344709873199463,
        "learning_rate": 7.940403168460133e-05,
        "epoch": 0.7672080433101315,
        "step": 5952
    },
    {
        "loss": 1.4641,
        "grad_norm": 2.1876869201660156,
        "learning_rate": 7.935478621490514e-05,
        "epoch": 0.7673369425109564,
        "step": 5953
    },
    {
        "loss": 2.029,
        "grad_norm": 1.5677626132965088,
        "learning_rate": 7.930549725388739e-05,
        "epoch": 0.7674658417117813,
        "step": 5954
    },
    {
        "loss": 1.4445,
        "grad_norm": 2.7388617992401123,
        "learning_rate": 7.92561648745734e-05,
        "epoch": 0.7675947409126064,
        "step": 5955
    },
    {
        "loss": 1.5613,
        "grad_norm": 1.938436508178711,
        "learning_rate": 7.92067891500528e-05,
        "epoch": 0.7677236401134313,
        "step": 5956
    },
    {
        "loss": 2.5843,
        "grad_norm": 1.8321377038955688,
        "learning_rate": 7.915737015347946e-05,
        "epoch": 0.7678525393142562,
        "step": 5957
    },
    {
        "loss": 1.9702,
        "grad_norm": 3.0694079399108887,
        "learning_rate": 7.910790795807121e-05,
        "epoch": 0.7679814385150812,
        "step": 5958
    },
    {
        "loss": 1.8785,
        "grad_norm": 2.153630495071411,
        "learning_rate": 7.905840263711015e-05,
        "epoch": 0.7681103377159062,
        "step": 5959
    },
    {
        "loss": 1.8122,
        "grad_norm": 2.9770660400390625,
        "learning_rate": 7.900885426394208e-05,
        "epoch": 0.7682392369167311,
        "step": 5960
    },
    {
        "loss": 1.8772,
        "grad_norm": 2.0233776569366455,
        "learning_rate": 7.895926291197665e-05,
        "epoch": 0.768368136117556,
        "step": 5961
    },
    {
        "loss": 1.7742,
        "grad_norm": 1.2794439792633057,
        "learning_rate": 7.890962865468719e-05,
        "epoch": 0.768497035318381,
        "step": 5962
    },
    {
        "loss": 1.903,
        "grad_norm": 2.0882465839385986,
        "learning_rate": 7.88599515656106e-05,
        "epoch": 0.768625934519206,
        "step": 5963
    },
    {
        "loss": 2.1653,
        "grad_norm": 2.280825614929199,
        "learning_rate": 7.881023171834705e-05,
        "epoch": 0.768754833720031,
        "step": 5964
    },
    {
        "loss": 2.0093,
        "grad_norm": 2.544783115386963,
        "learning_rate": 7.876046918656044e-05,
        "epoch": 0.7688837329208559,
        "step": 5965
    },
    {
        "loss": 2.1172,
        "grad_norm": 3.0137276649475098,
        "learning_rate": 7.87106640439776e-05,
        "epoch": 0.7690126321216808,
        "step": 5966
    },
    {
        "loss": 2.0407,
        "grad_norm": 2.894512176513672,
        "learning_rate": 7.866081636438863e-05,
        "epoch": 0.7691415313225058,
        "step": 5967
    },
    {
        "loss": 1.7951,
        "grad_norm": 3.386483907699585,
        "learning_rate": 7.861092622164657e-05,
        "epoch": 0.7692704305233308,
        "step": 5968
    },
    {
        "loss": 2.2767,
        "grad_norm": 2.3135976791381836,
        "learning_rate": 7.856099368966745e-05,
        "epoch": 0.7693993297241557,
        "step": 5969
    },
    {
        "loss": 0.8893,
        "grad_norm": 3.8011138439178467,
        "learning_rate": 7.851101884243007e-05,
        "epoch": 0.7695282289249806,
        "step": 5970
    },
    {
        "loss": 1.8721,
        "grad_norm": 2.4517412185668945,
        "learning_rate": 7.846100175397588e-05,
        "epoch": 0.7696571281258057,
        "step": 5971
    },
    {
        "loss": 1.2087,
        "grad_norm": 3.1819818019866943,
        "learning_rate": 7.8410942498409e-05,
        "epoch": 0.7697860273266306,
        "step": 5972
    },
    {
        "loss": 1.9397,
        "grad_norm": 1.6158950328826904,
        "learning_rate": 7.836084114989597e-05,
        "epoch": 0.7699149265274555,
        "step": 5973
    },
    {
        "loss": 2.1095,
        "grad_norm": 2.2268800735473633,
        "learning_rate": 7.831069778266569e-05,
        "epoch": 0.7700438257282805,
        "step": 5974
    },
    {
        "loss": 1.9136,
        "grad_norm": 1.5673136711120605,
        "learning_rate": 7.826051247100931e-05,
        "epoch": 0.7701727249291055,
        "step": 5975
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.7046946287155151,
        "learning_rate": 7.821028528928018e-05,
        "epoch": 0.7703016241299304,
        "step": 5976
    },
    {
        "loss": 2.3775,
        "grad_norm": 1.8725591897964478,
        "learning_rate": 7.816001631189363e-05,
        "epoch": 0.7704305233307553,
        "step": 5977
    },
    {
        "loss": 2.4868,
        "grad_norm": 1.8544163703918457,
        "learning_rate": 7.810970561332692e-05,
        "epoch": 0.7705594225315803,
        "step": 5978
    },
    {
        "loss": 2.1766,
        "grad_norm": 2.293808937072754,
        "learning_rate": 7.805935326811912e-05,
        "epoch": 0.7706883217324053,
        "step": 5979
    },
    {
        "loss": 2.393,
        "grad_norm": 1.3373514413833618,
        "learning_rate": 7.800895935087104e-05,
        "epoch": 0.7708172209332302,
        "step": 5980
    },
    {
        "loss": 2.1747,
        "grad_norm": 1.6294904947280884,
        "learning_rate": 7.795852393624503e-05,
        "epoch": 0.7709461201340552,
        "step": 5981
    },
    {
        "loss": 1.3804,
        "grad_norm": 2.357422351837158,
        "learning_rate": 7.790804709896497e-05,
        "epoch": 0.7710750193348801,
        "step": 5982
    },
    {
        "loss": 2.49,
        "grad_norm": 1.4386637210845947,
        "learning_rate": 7.785752891381608e-05,
        "epoch": 0.771203918535705,
        "step": 5983
    },
    {
        "loss": 1.3977,
        "grad_norm": 2.4293675422668457,
        "learning_rate": 7.780696945564482e-05,
        "epoch": 0.7713328177365301,
        "step": 5984
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.0836074352264404,
        "learning_rate": 7.775636879935879e-05,
        "epoch": 0.771461716937355,
        "step": 5985
    },
    {
        "loss": 1.8549,
        "grad_norm": 1.9522273540496826,
        "learning_rate": 7.770572701992686e-05,
        "epoch": 0.7715906161381799,
        "step": 5986
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.1132354736328125,
        "learning_rate": 7.765504419237841e-05,
        "epoch": 0.7717195153390048,
        "step": 5987
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.854120135307312,
        "learning_rate": 7.760432039180396e-05,
        "epoch": 0.7718484145398299,
        "step": 5988
    },
    {
        "loss": 1.2523,
        "grad_norm": 3.249206781387329,
        "learning_rate": 7.755355569335462e-05,
        "epoch": 0.7719773137406548,
        "step": 5989
    },
    {
        "loss": 1.5048,
        "grad_norm": 2.8833048343658447,
        "learning_rate": 7.750275017224209e-05,
        "epoch": 0.7721062129414797,
        "step": 5990
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.4357998371124268,
        "learning_rate": 7.745190390373851e-05,
        "epoch": 0.7722351121423047,
        "step": 5991
    },
    {
        "loss": 1.2888,
        "grad_norm": 2.945791244506836,
        "learning_rate": 7.740101696317663e-05,
        "epoch": 0.7723640113431297,
        "step": 5992
    },
    {
        "loss": 1.8828,
        "grad_norm": 2.2066543102264404,
        "learning_rate": 7.735008942594912e-05,
        "epoch": 0.7724929105439546,
        "step": 5993
    },
    {
        "loss": 1.7891,
        "grad_norm": 2.4921212196350098,
        "learning_rate": 7.729912136750899e-05,
        "epoch": 0.7726218097447796,
        "step": 5994
    },
    {
        "loss": 1.9023,
        "grad_norm": 2.314215660095215,
        "learning_rate": 7.724811286336923e-05,
        "epoch": 0.7727507089456045,
        "step": 5995
    },
    {
        "loss": 1.6708,
        "grad_norm": 2.04809308052063,
        "learning_rate": 7.719706398910281e-05,
        "epoch": 0.7728796081464295,
        "step": 5996
    },
    {
        "loss": 2.4839,
        "grad_norm": 1.7487127780914307,
        "learning_rate": 7.71459748203424e-05,
        "epoch": 0.7730085073472545,
        "step": 5997
    },
    {
        "loss": 1.5878,
        "grad_norm": 1.7812392711639404,
        "learning_rate": 7.709484543278056e-05,
        "epoch": 0.7731374065480794,
        "step": 5998
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.529212474822998,
        "learning_rate": 7.70436759021693e-05,
        "epoch": 0.7732663057489043,
        "step": 5999
    },
    {
        "loss": 2.0476,
        "grad_norm": 2.3455660343170166,
        "learning_rate": 7.699246630432004e-05,
        "epoch": 0.7733952049497294,
        "step": 6000
    },
    {
        "loss": 2.1347,
        "grad_norm": 1.8574721813201904,
        "learning_rate": 7.694121671510362e-05,
        "epoch": 0.7735241041505543,
        "step": 6001
    },
    {
        "loss": 1.5232,
        "grad_norm": 2.0920774936676025,
        "learning_rate": 7.688992721045035e-05,
        "epoch": 0.7736530033513792,
        "step": 6002
    },
    {
        "loss": 1.7736,
        "grad_norm": 1.9053455591201782,
        "learning_rate": 7.683859786634925e-05,
        "epoch": 0.7737819025522041,
        "step": 6003
    },
    {
        "loss": 1.5616,
        "grad_norm": 2.3437933921813965,
        "learning_rate": 7.678722875884877e-05,
        "epoch": 0.7739108017530292,
        "step": 6004
    },
    {
        "loss": 2.5624,
        "grad_norm": 1.7570741176605225,
        "learning_rate": 7.67358199640561e-05,
        "epoch": 0.7740397009538541,
        "step": 6005
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.8672903776168823,
        "learning_rate": 7.668437155813712e-05,
        "epoch": 0.774168600154679,
        "step": 6006
    },
    {
        "loss": 1.7319,
        "grad_norm": 3.1724002361297607,
        "learning_rate": 7.663288361731652e-05,
        "epoch": 0.774297499355504,
        "step": 6007
    },
    {
        "loss": 2.3411,
        "grad_norm": 2.2158288955688477,
        "learning_rate": 7.658135621787771e-05,
        "epoch": 0.774426398556329,
        "step": 6008
    },
    {
        "loss": 2.1224,
        "grad_norm": 2.468806505203247,
        "learning_rate": 7.652978943616216e-05,
        "epoch": 0.7745552977571539,
        "step": 6009
    },
    {
        "loss": 1.8655,
        "grad_norm": 1.9608126878738403,
        "learning_rate": 7.647818334857015e-05,
        "epoch": 0.7746841969579789,
        "step": 6010
    },
    {
        "loss": 2.1125,
        "grad_norm": 2.5695319175720215,
        "learning_rate": 7.642653803155994e-05,
        "epoch": 0.7748130961588038,
        "step": 6011
    },
    {
        "loss": 1.8222,
        "grad_norm": 3.459225654602051,
        "learning_rate": 7.637485356164785e-05,
        "epoch": 0.7749419953596288,
        "step": 6012
    },
    {
        "loss": 2.3735,
        "grad_norm": 1.9225958585739136,
        "learning_rate": 7.632313001540829e-05,
        "epoch": 0.7750708945604537,
        "step": 6013
    },
    {
        "loss": 1.9188,
        "grad_norm": 1.6480427980422974,
        "learning_rate": 7.627136746947372e-05,
        "epoch": 0.7751997937612787,
        "step": 6014
    },
    {
        "loss": 2.0769,
        "grad_norm": 1.548330307006836,
        "learning_rate": 7.621956600053417e-05,
        "epoch": 0.7753286929621036,
        "step": 6015
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.9580512046813965,
        "learning_rate": 7.616772568533739e-05,
        "epoch": 0.7754575921629286,
        "step": 6016
    },
    {
        "loss": 2.409,
        "grad_norm": 1.7590250968933105,
        "learning_rate": 7.611584660068872e-05,
        "epoch": 0.7755864913637536,
        "step": 6017
    },
    {
        "loss": 1.794,
        "grad_norm": 2.2697699069976807,
        "learning_rate": 7.606392882345099e-05,
        "epoch": 0.7757153905645785,
        "step": 6018
    },
    {
        "loss": 1.7669,
        "grad_norm": 3.3664474487304688,
        "learning_rate": 7.60119724305441e-05,
        "epoch": 0.7758442897654034,
        "step": 6019
    },
    {
        "loss": 2.3048,
        "grad_norm": 1.7666652202606201,
        "learning_rate": 7.595997749894552e-05,
        "epoch": 0.7759731889662284,
        "step": 6020
    },
    {
        "loss": 1.8961,
        "grad_norm": 2.393793821334839,
        "learning_rate": 7.590794410568961e-05,
        "epoch": 0.7761020881670534,
        "step": 6021
    },
    {
        "loss": 1.8273,
        "grad_norm": 1.6962093114852905,
        "learning_rate": 7.585587232786775e-05,
        "epoch": 0.7762309873678783,
        "step": 6022
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.1804463863372803,
        "learning_rate": 7.580376224262815e-05,
        "epoch": 0.7763598865687033,
        "step": 6023
    },
    {
        "loss": 2.5281,
        "grad_norm": 1.9856986999511719,
        "learning_rate": 7.575161392717593e-05,
        "epoch": 0.7764887857695282,
        "step": 6024
    },
    {
        "loss": 1.9986,
        "grad_norm": 2.6259448528289795,
        "learning_rate": 7.569942745877254e-05,
        "epoch": 0.7766176849703532,
        "step": 6025
    },
    {
        "loss": 1.9266,
        "grad_norm": 2.3414597511291504,
        "learning_rate": 7.564720291473633e-05,
        "epoch": 0.7767465841711781,
        "step": 6026
    },
    {
        "loss": 2.576,
        "grad_norm": 1.5524914264678955,
        "learning_rate": 7.559494037244183e-05,
        "epoch": 0.7768754833720031,
        "step": 6027
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.048816442489624,
        "learning_rate": 7.55426399093199e-05,
        "epoch": 0.777004382572828,
        "step": 6028
    },
    {
        "loss": 1.5135,
        "grad_norm": 2.351774215698242,
        "learning_rate": 7.549030160285765e-05,
        "epoch": 0.777133281773653,
        "step": 6029
    },
    {
        "loss": 1.9583,
        "grad_norm": 1.8571882247924805,
        "learning_rate": 7.543792553059824e-05,
        "epoch": 0.777262180974478,
        "step": 6030
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.2551276683807373,
        "learning_rate": 7.53855117701406e-05,
        "epoch": 0.7773910801753029,
        "step": 6031
    },
    {
        "loss": 1.084,
        "grad_norm": 3.471925973892212,
        "learning_rate": 7.53330603991398e-05,
        "epoch": 0.7775199793761278,
        "step": 6032
    },
    {
        "loss": 1.618,
        "grad_norm": 3.8990323543548584,
        "learning_rate": 7.528057149530645e-05,
        "epoch": 0.7776488785769529,
        "step": 6033
    },
    {
        "loss": 1.2423,
        "grad_norm": 3.513272523880005,
        "learning_rate": 7.522804513640682e-05,
        "epoch": 0.7777777777777778,
        "step": 6034
    },
    {
        "loss": 2.1093,
        "grad_norm": 1.5924651622772217,
        "learning_rate": 7.517548140026264e-05,
        "epoch": 0.7779066769786027,
        "step": 6035
    },
    {
        "loss": 1.5862,
        "grad_norm": 2.059217929840088,
        "learning_rate": 7.512288036475103e-05,
        "epoch": 0.7780355761794276,
        "step": 6036
    },
    {
        "loss": 1.8329,
        "grad_norm": 3.2162134647369385,
        "learning_rate": 7.50702421078044e-05,
        "epoch": 0.7781644753802527,
        "step": 6037
    },
    {
        "loss": 2.2852,
        "grad_norm": 2.2857165336608887,
        "learning_rate": 7.501756670741025e-05,
        "epoch": 0.7782933745810776,
        "step": 6038
    },
    {
        "loss": 1.0906,
        "grad_norm": 2.9839887619018555,
        "learning_rate": 7.496485424161116e-05,
        "epoch": 0.7784222737819025,
        "step": 6039
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.0815937519073486,
        "learning_rate": 7.49121047885046e-05,
        "epoch": 0.7785511729827275,
        "step": 6040
    },
    {
        "loss": 1.7483,
        "grad_norm": 3.049118995666504,
        "learning_rate": 7.485931842624289e-05,
        "epoch": 0.7786800721835525,
        "step": 6041
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.788856863975525,
        "learning_rate": 7.480649523303294e-05,
        "epoch": 0.7788089713843774,
        "step": 6042
    },
    {
        "loss": 2.0321,
        "grad_norm": 1.993603229522705,
        "learning_rate": 7.475363528713629e-05,
        "epoch": 0.7789378705852024,
        "step": 6043
    },
    {
        "loss": 1.4299,
        "grad_norm": 2.6159050464630127,
        "learning_rate": 7.470073866686895e-05,
        "epoch": 0.7790667697860273,
        "step": 6044
    },
    {
        "loss": 2.3733,
        "grad_norm": 2.325251817703247,
        "learning_rate": 7.464780545060121e-05,
        "epoch": 0.7791956689868523,
        "step": 6045
    },
    {
        "loss": 1.2055,
        "grad_norm": 2.846149206161499,
        "learning_rate": 7.459483571675762e-05,
        "epoch": 0.7793245681876773,
        "step": 6046
    },
    {
        "loss": 1.6762,
        "grad_norm": 2.662937879562378,
        "learning_rate": 7.454182954381681e-05,
        "epoch": 0.7794534673885022,
        "step": 6047
    },
    {
        "loss": 1.934,
        "grad_norm": 2.5058934688568115,
        "learning_rate": 7.448878701031144e-05,
        "epoch": 0.7795823665893271,
        "step": 6048
    },
    {
        "loss": 2.1991,
        "grad_norm": 2.5983073711395264,
        "learning_rate": 7.443570819482796e-05,
        "epoch": 0.7797112657901522,
        "step": 6049
    },
    {
        "loss": 1.417,
        "grad_norm": 3.8449838161468506,
        "learning_rate": 7.438259317600665e-05,
        "epoch": 0.7798401649909771,
        "step": 6050
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.2764089107513428,
        "learning_rate": 7.432944203254142e-05,
        "epoch": 0.779969064191802,
        "step": 6051
    },
    {
        "loss": 1.699,
        "grad_norm": 2.582977056503296,
        "learning_rate": 7.427625484317958e-05,
        "epoch": 0.7800979633926269,
        "step": 6052
    },
    {
        "loss": 2.3482,
        "grad_norm": 1.1699000597000122,
        "learning_rate": 7.422303168672217e-05,
        "epoch": 0.780226862593452,
        "step": 6053
    },
    {
        "loss": 1.8204,
        "grad_norm": 2.294346809387207,
        "learning_rate": 7.416977264202309e-05,
        "epoch": 0.7803557617942769,
        "step": 6054
    },
    {
        "loss": 1.9261,
        "grad_norm": 2.8526227474212646,
        "learning_rate": 7.411647778798967e-05,
        "epoch": 0.7804846609951018,
        "step": 6055
    },
    {
        "loss": 2.4237,
        "grad_norm": 1.216357946395874,
        "learning_rate": 7.406314720358228e-05,
        "epoch": 0.7806135601959268,
        "step": 6056
    },
    {
        "loss": 2.2529,
        "grad_norm": 2.1909635066986084,
        "learning_rate": 7.400978096781413e-05,
        "epoch": 0.7807424593967517,
        "step": 6057
    },
    {
        "loss": 2.1142,
        "grad_norm": 1.375014305114746,
        "learning_rate": 7.395637915975129e-05,
        "epoch": 0.7808713585975767,
        "step": 6058
    },
    {
        "loss": 2.0442,
        "grad_norm": 3.592343807220459,
        "learning_rate": 7.390294185851274e-05,
        "epoch": 0.7810002577984017,
        "step": 6059
    },
    {
        "loss": 1.2828,
        "grad_norm": 3.2254691123962402,
        "learning_rate": 7.384946914326965e-05,
        "epoch": 0.7811291569992266,
        "step": 6060
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.899765968322754,
        "learning_rate": 7.379596109324593e-05,
        "epoch": 0.7812580562000515,
        "step": 6061
    },
    {
        "loss": 2.155,
        "grad_norm": 2.057588815689087,
        "learning_rate": 7.374241778771778e-05,
        "epoch": 0.7813869554008765,
        "step": 6062
    },
    {
        "loss": 1.6691,
        "grad_norm": 2.0826616287231445,
        "learning_rate": 7.368883930601364e-05,
        "epoch": 0.7815158546017015,
        "step": 6063
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.6268672943115234,
        "learning_rate": 7.363522572751397e-05,
        "epoch": 0.7816447538025264,
        "step": 6064
    },
    {
        "loss": 2.3099,
        "grad_norm": 1.913322925567627,
        "learning_rate": 7.358157713165147e-05,
        "epoch": 0.7817736530033513,
        "step": 6065
    },
    {
        "loss": 2.3605,
        "grad_norm": 2.054839611053467,
        "learning_rate": 7.352789359791054e-05,
        "epoch": 0.7819025522041764,
        "step": 6066
    },
    {
        "loss": 1.3975,
        "grad_norm": 2.5116710662841797,
        "learning_rate": 7.347417520582727e-05,
        "epoch": 0.7820314514050013,
        "step": 6067
    },
    {
        "loss": 1.9087,
        "grad_norm": 1.5956528186798096,
        "learning_rate": 7.342042203498949e-05,
        "epoch": 0.7821603506058262,
        "step": 6068
    },
    {
        "loss": 1.7627,
        "grad_norm": 2.185154438018799,
        "learning_rate": 7.33666341650367e-05,
        "epoch": 0.7822892498066512,
        "step": 6069
    },
    {
        "loss": 1.7232,
        "grad_norm": 2.1455368995666504,
        "learning_rate": 7.33128116756595e-05,
        "epoch": 0.7824181490074762,
        "step": 6070
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.768986940383911,
        "learning_rate": 7.325895464660009e-05,
        "epoch": 0.7825470482083011,
        "step": 6071
    },
    {
        "loss": 1.9311,
        "grad_norm": 2.094627618789673,
        "learning_rate": 7.32050631576517e-05,
        "epoch": 0.782675947409126,
        "step": 6072
    },
    {
        "loss": 1.9183,
        "grad_norm": 2.6302311420440674,
        "learning_rate": 7.315113728865853e-05,
        "epoch": 0.782804846609951,
        "step": 6073
    },
    {
        "loss": 0.8707,
        "grad_norm": 3.14928936958313,
        "learning_rate": 7.309717711951579e-05,
        "epoch": 0.782933745810776,
        "step": 6074
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.173612594604492,
        "learning_rate": 7.30431827301697e-05,
        "epoch": 0.7830626450116009,
        "step": 6075
    },
    {
        "loss": 2.0591,
        "grad_norm": 2.558495044708252,
        "learning_rate": 7.298915420061675e-05,
        "epoch": 0.7831915442124259,
        "step": 6076
    },
    {
        "loss": 1.9677,
        "grad_norm": 2.065342426300049,
        "learning_rate": 7.293509161090451e-05,
        "epoch": 0.7833204434132508,
        "step": 6077
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.6715102195739746,
        "learning_rate": 7.28809950411307e-05,
        "epoch": 0.7834493426140758,
        "step": 6078
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.0464093685150146,
        "learning_rate": 7.282686457144337e-05,
        "epoch": 0.7835782418149008,
        "step": 6079
    },
    {
        "loss": 1.3555,
        "grad_norm": 2.327066421508789,
        "learning_rate": 7.277270028204087e-05,
        "epoch": 0.7837071410157257,
        "step": 6080
    },
    {
        "loss": 1.9181,
        "grad_norm": 1.5994857549667358,
        "learning_rate": 7.271850225317176e-05,
        "epoch": 0.7838360402165506,
        "step": 6081
    },
    {
        "loss": 2.1186,
        "grad_norm": 2.8785603046417236,
        "learning_rate": 7.266427056513449e-05,
        "epoch": 0.7839649394173757,
        "step": 6082
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.976865291595459,
        "learning_rate": 7.261000529827732e-05,
        "epoch": 0.7840938386182006,
        "step": 6083
    },
    {
        "loss": 2.1018,
        "grad_norm": 1.9296109676361084,
        "learning_rate": 7.255570653299835e-05,
        "epoch": 0.7842227378190255,
        "step": 6084
    },
    {
        "loss": 2.1729,
        "grad_norm": 2.147916793823242,
        "learning_rate": 7.250137434974534e-05,
        "epoch": 0.7843516370198504,
        "step": 6085
    },
    {
        "loss": 1.4986,
        "grad_norm": 2.954399585723877,
        "learning_rate": 7.244700882901531e-05,
        "epoch": 0.7844805362206755,
        "step": 6086
    },
    {
        "loss": 1.9047,
        "grad_norm": 1.6476696729660034,
        "learning_rate": 7.239261005135508e-05,
        "epoch": 0.7846094354215004,
        "step": 6087
    },
    {
        "loss": 1.7221,
        "grad_norm": 2.393219470977783,
        "learning_rate": 7.233817809736041e-05,
        "epoch": 0.7847383346223253,
        "step": 6088
    },
    {
        "loss": 2.4184,
        "grad_norm": 2.9385266304016113,
        "learning_rate": 7.228371304767638e-05,
        "epoch": 0.7848672338231503,
        "step": 6089
    },
    {
        "loss": 1.8507,
        "grad_norm": 3.2093183994293213,
        "learning_rate": 7.222921498299704e-05,
        "epoch": 0.7849961330239753,
        "step": 6090
    },
    {
        "loss": 2.3464,
        "grad_norm": 3.184936046600342,
        "learning_rate": 7.21746839840654e-05,
        "epoch": 0.7851250322248002,
        "step": 6091
    },
    {
        "loss": 2.1737,
        "grad_norm": 2.0863471031188965,
        "learning_rate": 7.212012013167309e-05,
        "epoch": 0.7852539314256252,
        "step": 6092
    },
    {
        "loss": 1.9545,
        "grad_norm": 2.281008005142212,
        "learning_rate": 7.20655235066607e-05,
        "epoch": 0.7853828306264501,
        "step": 6093
    },
    {
        "loss": 2.0955,
        "grad_norm": 1.8634707927703857,
        "learning_rate": 7.201089418991722e-05,
        "epoch": 0.785511729827275,
        "step": 6094
    },
    {
        "loss": 1.9945,
        "grad_norm": 2.327686071395874,
        "learning_rate": 7.195623226238005e-05,
        "epoch": 0.7856406290281001,
        "step": 6095
    },
    {
        "loss": 1.4755,
        "grad_norm": 2.6705820560455322,
        "learning_rate": 7.190153780503495e-05,
        "epoch": 0.785769528228925,
        "step": 6096
    },
    {
        "loss": 1.6273,
        "grad_norm": 3.104147434234619,
        "learning_rate": 7.184681089891588e-05,
        "epoch": 0.7858984274297499,
        "step": 6097
    },
    {
        "loss": 1.8382,
        "grad_norm": 2.4007608890533447,
        "learning_rate": 7.179205162510485e-05,
        "epoch": 0.7860273266305748,
        "step": 6098
    },
    {
        "loss": 1.8611,
        "grad_norm": 2.320847988128662,
        "learning_rate": 7.173726006473184e-05,
        "epoch": 0.7861562258313999,
        "step": 6099
    },
    {
        "loss": 2.3906,
        "grad_norm": 2.1704654693603516,
        "learning_rate": 7.168243629897469e-05,
        "epoch": 0.7862851250322248,
        "step": 6100
    },
    {
        "loss": 2.085,
        "grad_norm": 1.7921035289764404,
        "learning_rate": 7.162758040905889e-05,
        "epoch": 0.7864140242330497,
        "step": 6101
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.1964359283447266,
        "learning_rate": 7.15726924762576e-05,
        "epoch": 0.7865429234338747,
        "step": 6102
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.7562832832336426,
        "learning_rate": 7.151777258189138e-05,
        "epoch": 0.7866718226346997,
        "step": 6103
    },
    {
        "loss": 2.2835,
        "grad_norm": 2.25757098197937,
        "learning_rate": 7.146282080732821e-05,
        "epoch": 0.7868007218355246,
        "step": 6104
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.8439866304397583,
        "learning_rate": 7.140783723398325e-05,
        "epoch": 0.7869296210363496,
        "step": 6105
    },
    {
        "loss": 1.7791,
        "grad_norm": 2.8928167819976807,
        "learning_rate": 7.13528219433188e-05,
        "epoch": 0.7870585202371745,
        "step": 6106
    },
    {
        "loss": 1.7495,
        "grad_norm": 2.2554941177368164,
        "learning_rate": 7.129777501684418e-05,
        "epoch": 0.7871874194379995,
        "step": 6107
    },
    {
        "loss": 1.6001,
        "grad_norm": 2.118136167526245,
        "learning_rate": 7.12426965361155e-05,
        "epoch": 0.7873163186388245,
        "step": 6108
    },
    {
        "loss": 2.4224,
        "grad_norm": 1.872989296913147,
        "learning_rate": 7.11875865827357e-05,
        "epoch": 0.7874452178396494,
        "step": 6109
    },
    {
        "loss": 2.3111,
        "grad_norm": 2.0764386653900146,
        "learning_rate": 7.113244523835428e-05,
        "epoch": 0.7875741170404743,
        "step": 6110
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.4889384508132935,
        "learning_rate": 7.107727258466735e-05,
        "epoch": 0.7877030162412993,
        "step": 6111
    },
    {
        "loss": 1.976,
        "grad_norm": 2.1599440574645996,
        "learning_rate": 7.102206870341728e-05,
        "epoch": 0.7878319154421243,
        "step": 6112
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.6628954410552979,
        "learning_rate": 7.096683367639273e-05,
        "epoch": 0.7879608146429492,
        "step": 6113
    },
    {
        "loss": 1.7446,
        "grad_norm": 1.4368181228637695,
        "learning_rate": 7.091156758542874e-05,
        "epoch": 0.7880897138437741,
        "step": 6114
    },
    {
        "loss": 2.1839,
        "grad_norm": 2.2188992500305176,
        "learning_rate": 7.085627051240597e-05,
        "epoch": 0.7882186130445992,
        "step": 6115
    },
    {
        "loss": 1.8879,
        "grad_norm": 1.7933427095413208,
        "learning_rate": 7.080094253925126e-05,
        "epoch": 0.7883475122454241,
        "step": 6116
    },
    {
        "loss": 1.6264,
        "grad_norm": 2.687194347381592,
        "learning_rate": 7.074558374793717e-05,
        "epoch": 0.788476411446249,
        "step": 6117
    },
    {
        "loss": 1.8365,
        "grad_norm": 1.9155298471450806,
        "learning_rate": 7.069019422048189e-05,
        "epoch": 0.788605310647074,
        "step": 6118
    },
    {
        "loss": 1.7002,
        "grad_norm": 2.3119900226593018,
        "learning_rate": 7.063477403894911e-05,
        "epoch": 0.788734209847899,
        "step": 6119
    },
    {
        "loss": 1.7045,
        "grad_norm": 3.1999571323394775,
        "learning_rate": 7.057932328544819e-05,
        "epoch": 0.7888631090487239,
        "step": 6120
    },
    {
        "loss": 0.8788,
        "grad_norm": 2.969092845916748,
        "learning_rate": 7.052384204213341e-05,
        "epoch": 0.7889920082495488,
        "step": 6121
    },
    {
        "loss": 1.2605,
        "grad_norm": 3.154503583908081,
        "learning_rate": 7.046833039120444e-05,
        "epoch": 0.7891209074503738,
        "step": 6122
    },
    {
        "loss": 1.8588,
        "grad_norm": 2.953328847885132,
        "learning_rate": 7.041278841490596e-05,
        "epoch": 0.7892498066511988,
        "step": 6123
    },
    {
        "loss": 1.53,
        "grad_norm": 3.9489080905914307,
        "learning_rate": 7.03572161955276e-05,
        "epoch": 0.7893787058520237,
        "step": 6124
    },
    {
        "loss": 1.7663,
        "grad_norm": 2.807559013366699,
        "learning_rate": 7.030161381540369e-05,
        "epoch": 0.7895076050528487,
        "step": 6125
    },
    {
        "loss": 2.3327,
        "grad_norm": 2.082014322280884,
        "learning_rate": 7.024598135691355e-05,
        "epoch": 0.7896365042536736,
        "step": 6126
    },
    {
        "loss": 1.8834,
        "grad_norm": 2.937377691268921,
        "learning_rate": 7.019031890248067e-05,
        "epoch": 0.7897654034544985,
        "step": 6127
    },
    {
        "loss": 1.5291,
        "grad_norm": 3.2253448963165283,
        "learning_rate": 7.013462653457317e-05,
        "epoch": 0.7898943026553236,
        "step": 6128
    },
    {
        "loss": 1.7534,
        "grad_norm": 3.28658390045166,
        "learning_rate": 7.007890433570352e-05,
        "epoch": 0.7900232018561485,
        "step": 6129
    },
    {
        "loss": 1.9409,
        "grad_norm": 2.8201348781585693,
        "learning_rate": 7.002315238842834e-05,
        "epoch": 0.7901521010569734,
        "step": 6130
    },
    {
        "loss": 2.2663,
        "grad_norm": 2.1922271251678467,
        "learning_rate": 6.996737077534827e-05,
        "epoch": 0.7902810002577983,
        "step": 6131
    },
    {
        "loss": 2.4033,
        "grad_norm": 1.6545764207839966,
        "learning_rate": 6.991155957910804e-05,
        "epoch": 0.7904098994586234,
        "step": 6132
    },
    {
        "loss": 1.8332,
        "grad_norm": 2.6283228397369385,
        "learning_rate": 6.98557188823962e-05,
        "epoch": 0.7905387986594483,
        "step": 6133
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.3924858570098877,
        "learning_rate": 6.979984876794478e-05,
        "epoch": 0.7906676978602732,
        "step": 6134
    },
    {
        "loss": 2.5353,
        "grad_norm": 1.4300873279571533,
        "learning_rate": 6.974394931852953e-05,
        "epoch": 0.7907965970610982,
        "step": 6135
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.2120416164398193,
        "learning_rate": 6.968802061696992e-05,
        "epoch": 0.7909254962619232,
        "step": 6136
    },
    {
        "loss": 1.6889,
        "grad_norm": 2.46597957611084,
        "learning_rate": 6.963206274612824e-05,
        "epoch": 0.7910543954627481,
        "step": 6137
    },
    {
        "loss": 1.4645,
        "grad_norm": 2.213498830795288,
        "learning_rate": 6.957607578891048e-05,
        "epoch": 0.7911832946635731,
        "step": 6138
    },
    {
        "loss": 2.1002,
        "grad_norm": 2.2314505577087402,
        "learning_rate": 6.952005982826551e-05,
        "epoch": 0.791312193864398,
        "step": 6139
    },
    {
        "loss": 0.6294,
        "grad_norm": 3.375174045562744,
        "learning_rate": 6.946401494718509e-05,
        "epoch": 0.791441093065223,
        "step": 6140
    },
    {
        "loss": 1.9298,
        "grad_norm": 1.9371744394302368,
        "learning_rate": 6.940794122870389e-05,
        "epoch": 0.791569992266048,
        "step": 6141
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.2324717044830322,
        "learning_rate": 6.935183875589954e-05,
        "epoch": 0.7916988914668729,
        "step": 6142
    },
    {
        "loss": 2.2676,
        "grad_norm": 1.7164512872695923,
        "learning_rate": 6.929570761189183e-05,
        "epoch": 0.7918277906676978,
        "step": 6143
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.5129859447479248,
        "learning_rate": 6.923954787984341e-05,
        "epoch": 0.7919566898685229,
        "step": 6144
    },
    {
        "loss": 1.625,
        "grad_norm": 1.2492996454238892,
        "learning_rate": 6.918335964295922e-05,
        "epoch": 0.7920855890693478,
        "step": 6145
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.9908808469772339,
        "learning_rate": 6.912714298448615e-05,
        "epoch": 0.7922144882701727,
        "step": 6146
    },
    {
        "loss": 2.0436,
        "grad_norm": 3.012448787689209,
        "learning_rate": 6.907089798771347e-05,
        "epoch": 0.7923433874709976,
        "step": 6147
    },
    {
        "loss": 2.3531,
        "grad_norm": 1.6913082599639893,
        "learning_rate": 6.901462473597244e-05,
        "epoch": 0.7924722866718227,
        "step": 6148
    },
    {
        "loss": 1.1149,
        "grad_norm": 2.22977876663208,
        "learning_rate": 6.895832331263609e-05,
        "epoch": 0.7926011858726476,
        "step": 6149
    },
    {
        "loss": 1.4033,
        "grad_norm": 2.8440353870391846,
        "learning_rate": 6.890199380111918e-05,
        "epoch": 0.7927300850734725,
        "step": 6150
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.040055274963379,
        "learning_rate": 6.884563628487812e-05,
        "epoch": 0.7928589842742975,
        "step": 6151
    },
    {
        "loss": 1.3655,
        "grad_norm": 2.3981757164001465,
        "learning_rate": 6.878925084741089e-05,
        "epoch": 0.7929878834751225,
        "step": 6152
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.4975006580352783,
        "learning_rate": 6.873283757225655e-05,
        "epoch": 0.7931167826759474,
        "step": 6153
    },
    {
        "loss": 1.8399,
        "grad_norm": 3.8478071689605713,
        "learning_rate": 6.867639654299578e-05,
        "epoch": 0.7932456818767724,
        "step": 6154
    },
    {
        "loss": 2.3544,
        "grad_norm": 1.8011173009872437,
        "learning_rate": 6.861992784325015e-05,
        "epoch": 0.7933745810775973,
        "step": 6155
    },
    {
        "loss": 2.5727,
        "grad_norm": 1.8473647832870483,
        "learning_rate": 6.85634315566823e-05,
        "epoch": 0.7935034802784223,
        "step": 6156
    },
    {
        "loss": 1.4656,
        "grad_norm": 2.3226094245910645,
        "learning_rate": 6.850690776699571e-05,
        "epoch": 0.7936323794792473,
        "step": 6157
    },
    {
        "loss": 2.1647,
        "grad_norm": 1.401036024093628,
        "learning_rate": 6.845035655793468e-05,
        "epoch": 0.7937612786800722,
        "step": 6158
    },
    {
        "loss": 2.3635,
        "grad_norm": 2.1293575763702393,
        "learning_rate": 6.839377801328389e-05,
        "epoch": 0.7938901778808971,
        "step": 6159
    },
    {
        "loss": 1.0476,
        "grad_norm": 3.0439035892486572,
        "learning_rate": 6.83371722168689e-05,
        "epoch": 0.7940190770817221,
        "step": 6160
    },
    {
        "loss": 2.0616,
        "grad_norm": 2.5900814533233643,
        "learning_rate": 6.828053925255539e-05,
        "epoch": 0.7941479762825471,
        "step": 6161
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.3200883865356445,
        "learning_rate": 6.822387920424934e-05,
        "epoch": 0.794276875483372,
        "step": 6162
    },
    {
        "loss": 1.6789,
        "grad_norm": 1.795115351676941,
        "learning_rate": 6.816719215589688e-05,
        "epoch": 0.7944057746841969,
        "step": 6163
    },
    {
        "loss": 1.7899,
        "grad_norm": 1.9192965030670166,
        "learning_rate": 6.811047819148413e-05,
        "epoch": 0.7945346738850219,
        "step": 6164
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.8668248653411865,
        "learning_rate": 6.805373739503707e-05,
        "epoch": 0.7946635730858469,
        "step": 6165
    },
    {
        "loss": 2.5264,
        "grad_norm": 1.3099560737609863,
        "learning_rate": 6.799696985062149e-05,
        "epoch": 0.7947924722866718,
        "step": 6166
    },
    {
        "loss": 2.1402,
        "grad_norm": 1.2730845212936401,
        "learning_rate": 6.794017564234274e-05,
        "epoch": 0.7949213714874968,
        "step": 6167
    },
    {
        "loss": 1.2843,
        "grad_norm": 2.8651225566864014,
        "learning_rate": 6.788335485434572e-05,
        "epoch": 0.7950502706883217,
        "step": 6168
    },
    {
        "loss": 1.862,
        "grad_norm": 1.8322869539260864,
        "learning_rate": 6.782650757081471e-05,
        "epoch": 0.7951791698891467,
        "step": 6169
    },
    {
        "loss": 2.4315,
        "grad_norm": 1.5786757469177246,
        "learning_rate": 6.776963387597322e-05,
        "epoch": 0.7953080690899716,
        "step": 6170
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.0633950233459473,
        "learning_rate": 6.771273385408388e-05,
        "epoch": 0.7954369682907966,
        "step": 6171
    },
    {
        "loss": 1.5741,
        "grad_norm": 1.9618533849716187,
        "learning_rate": 6.76558075894484e-05,
        "epoch": 0.7955658674916215,
        "step": 6172
    },
    {
        "loss": 2.3365,
        "grad_norm": 1.303317666053772,
        "learning_rate": 6.759885516640728e-05,
        "epoch": 0.7956947666924465,
        "step": 6173
    },
    {
        "loss": 2.263,
        "grad_norm": 1.5772334337234497,
        "learning_rate": 6.75418766693398e-05,
        "epoch": 0.7958236658932715,
        "step": 6174
    },
    {
        "loss": 1.973,
        "grad_norm": 2.0413670539855957,
        "learning_rate": 6.748487218266393e-05,
        "epoch": 0.7959525650940964,
        "step": 6175
    },
    {
        "loss": 2.4749,
        "grad_norm": 2.0805437564849854,
        "learning_rate": 6.742784179083608e-05,
        "epoch": 0.7960814642949213,
        "step": 6176
    },
    {
        "loss": 2.4073,
        "grad_norm": 1.9583271741867065,
        "learning_rate": 6.737078557835105e-05,
        "epoch": 0.7962103634957464,
        "step": 6177
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.8107688426971436,
        "learning_rate": 6.731370362974193e-05,
        "epoch": 0.7963392626965713,
        "step": 6178
    },
    {
        "loss": 2.1976,
        "grad_norm": 2.0839223861694336,
        "learning_rate": 6.725659602957988e-05,
        "epoch": 0.7964681618973962,
        "step": 6179
    },
    {
        "loss": 1.9933,
        "grad_norm": 1.7621382474899292,
        "learning_rate": 6.719946286247406e-05,
        "epoch": 0.7965970610982211,
        "step": 6180
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.4647462368011475,
        "learning_rate": 6.714230421307175e-05,
        "epoch": 0.7967259602990462,
        "step": 6181
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.5592857599258423,
        "learning_rate": 6.708512016605759e-05,
        "epoch": 0.7968548594998711,
        "step": 6182
    },
    {
        "loss": 1.9878,
        "grad_norm": 2.3787662982940674,
        "learning_rate": 6.702791080615408e-05,
        "epoch": 0.796983758700696,
        "step": 6183
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.2119452953338623,
        "learning_rate": 6.697067621812122e-05,
        "epoch": 0.797112657901521,
        "step": 6184
    },
    {
        "loss": 2.2792,
        "grad_norm": 2.1212546825408936,
        "learning_rate": 6.691341648675633e-05,
        "epoch": 0.797241557102346,
        "step": 6185
    },
    {
        "loss": 1.5071,
        "grad_norm": 2.9761643409729004,
        "learning_rate": 6.685613169689395e-05,
        "epoch": 0.7973704563031709,
        "step": 6186
    },
    {
        "loss": 1.2105,
        "grad_norm": 2.406161308288574,
        "learning_rate": 6.679882193340599e-05,
        "epoch": 0.7974993555039959,
        "step": 6187
    },
    {
        "loss": 2.1432,
        "grad_norm": 2.55576491355896,
        "learning_rate": 6.674148728120097e-05,
        "epoch": 0.7976282547048208,
        "step": 6188
    },
    {
        "loss": 2.1982,
        "grad_norm": 2.996082067489624,
        "learning_rate": 6.668412782522457e-05,
        "epoch": 0.7977571539056458,
        "step": 6189
    },
    {
        "loss": 1.9968,
        "grad_norm": 2.174064874649048,
        "learning_rate": 6.662674365045913e-05,
        "epoch": 0.7978860531064708,
        "step": 6190
    },
    {
        "loss": 1.0774,
        "grad_norm": 3.320542335510254,
        "learning_rate": 6.65693348419236e-05,
        "epoch": 0.7980149523072957,
        "step": 6191
    },
    {
        "loss": 1.8349,
        "grad_norm": 1.7645748853683472,
        "learning_rate": 6.651190148467338e-05,
        "epoch": 0.7981438515081206,
        "step": 6192
    },
    {
        "loss": 2.2522,
        "grad_norm": 1.9049060344696045,
        "learning_rate": 6.645444366380052e-05,
        "epoch": 0.7982727507089457,
        "step": 6193
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.9007800817489624,
        "learning_rate": 6.63969614644329e-05,
        "epoch": 0.7984016499097706,
        "step": 6194
    },
    {
        "loss": 1.0574,
        "grad_norm": 2.5194289684295654,
        "learning_rate": 6.633945497173476e-05,
        "epoch": 0.7985305491105955,
        "step": 6195
    },
    {
        "loss": 1.4608,
        "grad_norm": 2.9324989318847656,
        "learning_rate": 6.628192427090625e-05,
        "epoch": 0.7986594483114204,
        "step": 6196
    },
    {
        "loss": 1.6472,
        "grad_norm": 2.0805749893188477,
        "learning_rate": 6.62243694471836e-05,
        "epoch": 0.7987883475122455,
        "step": 6197
    },
    {
        "loss": 2.1934,
        "grad_norm": 1.5319997072219849,
        "learning_rate": 6.616679058583836e-05,
        "epoch": 0.7989172467130704,
        "step": 6198
    },
    {
        "loss": 2.0007,
        "grad_norm": 3.4426114559173584,
        "learning_rate": 6.610918777217814e-05,
        "epoch": 0.7990461459138953,
        "step": 6199
    },
    {
        "loss": 1.7943,
        "grad_norm": 2.382596731185913,
        "learning_rate": 6.605156109154582e-05,
        "epoch": 0.7991750451147203,
        "step": 6200
    },
    {
        "loss": 1.9036,
        "grad_norm": 1.6540281772613525,
        "learning_rate": 6.599391062931954e-05,
        "epoch": 0.7993039443155452,
        "step": 6201
    },
    {
        "loss": 2.4043,
        "grad_norm": 1.3365638256072998,
        "learning_rate": 6.593623647091278e-05,
        "epoch": 0.7994328435163702,
        "step": 6202
    },
    {
        "loss": 2.1284,
        "grad_norm": 2.2433927059173584,
        "learning_rate": 6.587853870177436e-05,
        "epoch": 0.7995617427171952,
        "step": 6203
    },
    {
        "loss": 2.2565,
        "grad_norm": 1.7802578210830688,
        "learning_rate": 6.582081740738758e-05,
        "epoch": 0.7996906419180201,
        "step": 6204
    },
    {
        "loss": 2.249,
        "grad_norm": 2.1957428455352783,
        "learning_rate": 6.576307267327106e-05,
        "epoch": 0.799819541118845,
        "step": 6205
    },
    {
        "loss": 1.0183,
        "grad_norm": 3.3958024978637695,
        "learning_rate": 6.570530458497799e-05,
        "epoch": 0.79994844031967,
        "step": 6206
    },
    {
        "loss": 1.5593,
        "grad_norm": 3.0585885047912598,
        "learning_rate": 6.564751322809598e-05,
        "epoch": 0.800077339520495,
        "step": 6207
    },
    {
        "loss": 1.9414,
        "grad_norm": 2.429704189300537,
        "learning_rate": 6.558969868824727e-05,
        "epoch": 0.8002062387213199,
        "step": 6208
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.5061287879943848,
        "learning_rate": 6.553186105108866e-05,
        "epoch": 0.8003351379221448,
        "step": 6209
    },
    {
        "loss": 1.7059,
        "grad_norm": 2.545943021774292,
        "learning_rate": 6.547400040231066e-05,
        "epoch": 0.8004640371229699,
        "step": 6210
    },
    {
        "loss": 2.0056,
        "grad_norm": 3.0078392028808594,
        "learning_rate": 6.541611682763842e-05,
        "epoch": 0.8005929363237948,
        "step": 6211
    },
    {
        "loss": 1.9116,
        "grad_norm": 2.5469682216644287,
        "learning_rate": 6.535821041283076e-05,
        "epoch": 0.8007218355246197,
        "step": 6212
    },
    {
        "loss": 1.6138,
        "grad_norm": 2.624876022338867,
        "learning_rate": 6.530028124368026e-05,
        "epoch": 0.8008507347254447,
        "step": 6213
    },
    {
        "loss": 0.5144,
        "grad_norm": 3.340474843978882,
        "learning_rate": 6.524232940601333e-05,
        "epoch": 0.8009796339262697,
        "step": 6214
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.514486312866211,
        "learning_rate": 6.518435498569013e-05,
        "epoch": 0.8011085331270946,
        "step": 6215
    },
    {
        "loss": 2.092,
        "grad_norm": 2.5684096813201904,
        "learning_rate": 6.512635806860404e-05,
        "epoch": 0.8012374323279196,
        "step": 6216
    },
    {
        "loss": 2.5701,
        "grad_norm": 2.4558145999908447,
        "learning_rate": 6.506833874068185e-05,
        "epoch": 0.8013663315287445,
        "step": 6217
    },
    {
        "loss": 1.9926,
        "grad_norm": 1.6905908584594727,
        "learning_rate": 6.501029708788355e-05,
        "epoch": 0.8014952307295695,
        "step": 6218
    },
    {
        "loss": 2.0534,
        "grad_norm": 2.013676166534424,
        "learning_rate": 6.49522331962023e-05,
        "epoch": 0.8016241299303944,
        "step": 6219
    },
    {
        "loss": 2.2251,
        "grad_norm": 2.3540189266204834,
        "learning_rate": 6.489414715166389e-05,
        "epoch": 0.8017530291312194,
        "step": 6220
    },
    {
        "loss": 1.7437,
        "grad_norm": 2.328037738800049,
        "learning_rate": 6.483603904032735e-05,
        "epoch": 0.8018819283320443,
        "step": 6221
    },
    {
        "loss": 2.4553,
        "grad_norm": 1.5996134281158447,
        "learning_rate": 6.47779089482842e-05,
        "epoch": 0.8020108275328693,
        "step": 6222
    },
    {
        "loss": 1.625,
        "grad_norm": 2.2552921772003174,
        "learning_rate": 6.471975696165849e-05,
        "epoch": 0.8021397267336943,
        "step": 6223
    },
    {
        "loss": 1.3727,
        "grad_norm": 2.625581979751587,
        "learning_rate": 6.466158316660676e-05,
        "epoch": 0.8022686259345192,
        "step": 6224
    },
    {
        "loss": 1.4949,
        "grad_norm": 4.392089366912842,
        "learning_rate": 6.460338764931795e-05,
        "epoch": 0.8023975251353441,
        "step": 6225
    },
    {
        "loss": 2.4852,
        "grad_norm": 1.676744818687439,
        "learning_rate": 6.45451704960129e-05,
        "epoch": 0.8025264243361692,
        "step": 6226
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.54761004447937,
        "learning_rate": 6.448693179294485e-05,
        "epoch": 0.8026553235369941,
        "step": 6227
    },
    {
        "loss": 2.2516,
        "grad_norm": 1.4881237745285034,
        "learning_rate": 6.44286716263988e-05,
        "epoch": 0.802784222737819,
        "step": 6228
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.8823530673980713,
        "learning_rate": 6.437039008269151e-05,
        "epoch": 0.802913121938644,
        "step": 6229
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.9945508241653442,
        "learning_rate": 6.431208724817153e-05,
        "epoch": 0.803042021139469,
        "step": 6230
    },
    {
        "loss": 2.3652,
        "grad_norm": 1.829088807106018,
        "learning_rate": 6.425376320921883e-05,
        "epoch": 0.8031709203402939,
        "step": 6231
    },
    {
        "loss": 2.1743,
        "grad_norm": 1.8929078578948975,
        "learning_rate": 6.41954180522449e-05,
        "epoch": 0.8032998195411188,
        "step": 6232
    },
    {
        "loss": 0.9746,
        "grad_norm": 2.6889946460723877,
        "learning_rate": 6.413705186369246e-05,
        "epoch": 0.8034287187419438,
        "step": 6233
    },
    {
        "loss": 1.5758,
        "grad_norm": 2.7218966484069824,
        "learning_rate": 6.407866473003538e-05,
        "epoch": 0.8035576179427688,
        "step": 6234
    },
    {
        "loss": 1.7273,
        "grad_norm": 2.6317365169525146,
        "learning_rate": 6.402025673777863e-05,
        "epoch": 0.8036865171435937,
        "step": 6235
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.5182077884674072,
        "learning_rate": 6.3961827973458e-05,
        "epoch": 0.8038154163444187,
        "step": 6236
    },
    {
        "loss": 2.3611,
        "grad_norm": 1.8274298906326294,
        "learning_rate": 6.390337852364013e-05,
        "epoch": 0.8039443155452436,
        "step": 6237
    },
    {
        "loss": 1.5729,
        "grad_norm": 2.124969482421875,
        "learning_rate": 6.384490847492225e-05,
        "epoch": 0.8040732147460685,
        "step": 6238
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.3893280029296875,
        "learning_rate": 6.378641791393211e-05,
        "epoch": 0.8042021139468936,
        "step": 6239
    },
    {
        "loss": 1.8838,
        "grad_norm": 3.124117374420166,
        "learning_rate": 6.372790692732792e-05,
        "epoch": 0.8043310131477185,
        "step": 6240
    },
    {
        "loss": 2.554,
        "grad_norm": 1.6171693801879883,
        "learning_rate": 6.366937560179808e-05,
        "epoch": 0.8044599123485434,
        "step": 6241
    },
    {
        "loss": 1.63,
        "grad_norm": 2.5968174934387207,
        "learning_rate": 6.361082402406114e-05,
        "epoch": 0.8045888115493683,
        "step": 6242
    },
    {
        "loss": 1.8892,
        "grad_norm": 1.9220939874649048,
        "learning_rate": 6.355225228086568e-05,
        "epoch": 0.8047177107501934,
        "step": 6243
    },
    {
        "loss": 1.3343,
        "grad_norm": 2.510437250137329,
        "learning_rate": 6.349366045899011e-05,
        "epoch": 0.8048466099510183,
        "step": 6244
    },
    {
        "loss": 1.2561,
        "grad_norm": 3.151803731918335,
        "learning_rate": 6.343504864524264e-05,
        "epoch": 0.8049755091518432,
        "step": 6245
    },
    {
        "loss": 1.8439,
        "grad_norm": 2.2903549671173096,
        "learning_rate": 6.337641692646106e-05,
        "epoch": 0.8051044083526682,
        "step": 6246
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.9367316961288452,
        "learning_rate": 6.331776538951262e-05,
        "epoch": 0.8052333075534932,
        "step": 6247
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.548926591873169,
        "learning_rate": 6.325909412129416e-05,
        "epoch": 0.8053622067543181,
        "step": 6248
    },
    {
        "loss": 2.066,
        "grad_norm": 2.2327797412872314,
        "learning_rate": 6.320040320873138e-05,
        "epoch": 0.8054911059551431,
        "step": 6249
    },
    {
        "loss": 1.9339,
        "grad_norm": 1.5693124532699585,
        "learning_rate": 6.314169273877937e-05,
        "epoch": 0.805620005155968,
        "step": 6250
    },
    {
        "loss": 1.8824,
        "grad_norm": 1.697625994682312,
        "learning_rate": 6.308296279842205e-05,
        "epoch": 0.805748904356793,
        "step": 6251
    },
    {
        "loss": 2.099,
        "grad_norm": 2.2576804161071777,
        "learning_rate": 6.302421347467229e-05,
        "epoch": 0.805877803557618,
        "step": 6252
    },
    {
        "loss": 1.8588,
        "grad_norm": 1.8088639974594116,
        "learning_rate": 6.296544485457152e-05,
        "epoch": 0.8060067027584429,
        "step": 6253
    },
    {
        "loss": 2.3608,
        "grad_norm": 1.8112080097198486,
        "learning_rate": 6.29066570251901e-05,
        "epoch": 0.8061356019592678,
        "step": 6254
    },
    {
        "loss": 2.1218,
        "grad_norm": 1.6446884870529175,
        "learning_rate": 6.284785007362639e-05,
        "epoch": 0.8062645011600929,
        "step": 6255
    },
    {
        "loss": 2.5043,
        "grad_norm": 2.1441404819488525,
        "learning_rate": 6.27890240870074e-05,
        "epoch": 0.8063934003609178,
        "step": 6256
    },
    {
        "loss": 2.3475,
        "grad_norm": 2.219144344329834,
        "learning_rate": 6.273017915248822e-05,
        "epoch": 0.8065222995617427,
        "step": 6257
    },
    {
        "loss": 1.3631,
        "grad_norm": 2.888388156890869,
        "learning_rate": 6.267131535725207e-05,
        "epoch": 0.8066511987625676,
        "step": 6258
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.1454179286956787,
        "learning_rate": 6.261243278851002e-05,
        "epoch": 0.8067800979633927,
        "step": 6259
    },
    {
        "loss": 1.932,
        "grad_norm": 2.298048496246338,
        "learning_rate": 6.255353153350118e-05,
        "epoch": 0.8069089971642176,
        "step": 6260
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.4952096939086914,
        "learning_rate": 6.249461167949203e-05,
        "epoch": 0.8070378963650425,
        "step": 6261
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.3871746063232422,
        "learning_rate": 6.24356733137768e-05,
        "epoch": 0.8071667955658675,
        "step": 6262
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.135603666305542,
        "learning_rate": 6.237671652367705e-05,
        "epoch": 0.8072956947666925,
        "step": 6263
    },
    {
        "loss": 2.4598,
        "grad_norm": 1.4049396514892578,
        "learning_rate": 6.23177413965419e-05,
        "epoch": 0.8074245939675174,
        "step": 6264
    },
    {
        "loss": 1.7782,
        "grad_norm": 2.9236574172973633,
        "learning_rate": 6.225874801974718e-05,
        "epoch": 0.8075534931683424,
        "step": 6265
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.7604477405548096,
        "learning_rate": 6.219973648069616e-05,
        "epoch": 0.8076823923691673,
        "step": 6266
    },
    {
        "loss": 2.1163,
        "grad_norm": 2.154128313064575,
        "learning_rate": 6.214070686681893e-05,
        "epoch": 0.8078112915699923,
        "step": 6267
    },
    {
        "loss": 2.1173,
        "grad_norm": 2.032644510269165,
        "learning_rate": 6.208165926557212e-05,
        "epoch": 0.8079401907708172,
        "step": 6268
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.6727652549743652,
        "learning_rate": 6.202259376443922e-05,
        "epoch": 0.8080690899716422,
        "step": 6269
    },
    {
        "loss": 1.6573,
        "grad_norm": 2.8195154666900635,
        "learning_rate": 6.196351045093039e-05,
        "epoch": 0.8081979891724671,
        "step": 6270
    },
    {
        "loss": 2.0497,
        "grad_norm": 1.4151161909103394,
        "learning_rate": 6.190440941258171e-05,
        "epoch": 0.8083268883732921,
        "step": 6271
    },
    {
        "loss": 2.1257,
        "grad_norm": 2.23433780670166,
        "learning_rate": 6.184529073695605e-05,
        "epoch": 0.8084557875741171,
        "step": 6272
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.249760389328003,
        "learning_rate": 6.178615451164216e-05,
        "epoch": 0.808584686774942,
        "step": 6273
    },
    {
        "loss": 1.5552,
        "grad_norm": 3.0305254459381104,
        "learning_rate": 6.172700082425466e-05,
        "epoch": 0.8087135859757669,
        "step": 6274
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.4278564453125,
        "learning_rate": 6.166782976243418e-05,
        "epoch": 0.8088424851765919,
        "step": 6275
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.9485962390899658,
        "learning_rate": 6.160864141384731e-05,
        "epoch": 0.8089713843774169,
        "step": 6276
    },
    {
        "loss": 1.596,
        "grad_norm": 2.6433396339416504,
        "learning_rate": 6.154943586618574e-05,
        "epoch": 0.8091002835782418,
        "step": 6277
    },
    {
        "loss": 1.2554,
        "grad_norm": 2.8961639404296875,
        "learning_rate": 6.149021320716718e-05,
        "epoch": 0.8092291827790667,
        "step": 6278
    },
    {
        "loss": 2.5035,
        "grad_norm": 1.8593000173568726,
        "learning_rate": 6.143097352453436e-05,
        "epoch": 0.8093580819798917,
        "step": 6279
    },
    {
        "loss": 1.5391,
        "grad_norm": 2.2608985900878906,
        "learning_rate": 6.137171690605536e-05,
        "epoch": 0.8094869811807167,
        "step": 6280
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.2308757305145264,
        "learning_rate": 6.131244343952316e-05,
        "epoch": 0.8096158803815416,
        "step": 6281
    },
    {
        "loss": 2.1582,
        "grad_norm": 2.049046277999878,
        "learning_rate": 6.125315321275603e-05,
        "epoch": 0.8097447795823666,
        "step": 6282
    },
    {
        "loss": 2.2083,
        "grad_norm": 2.1002469062805176,
        "learning_rate": 6.119384631359683e-05,
        "epoch": 0.8098736787831915,
        "step": 6283
    },
    {
        "loss": 1.2259,
        "grad_norm": 1.7179641723632812,
        "learning_rate": 6.113452282991321e-05,
        "epoch": 0.8100025779840165,
        "step": 6284
    },
    {
        "loss": 1.1982,
        "grad_norm": 1.701570749282837,
        "learning_rate": 6.107518284959733e-05,
        "epoch": 0.8101314771848415,
        "step": 6285
    },
    {
        "loss": 1.5244,
        "grad_norm": 2.7419586181640625,
        "learning_rate": 6.101582646056593e-05,
        "epoch": 0.8102603763856664,
        "step": 6286
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.7857410907745361,
        "learning_rate": 6.0956453750759734e-05,
        "epoch": 0.8103892755864913,
        "step": 6287
    },
    {
        "loss": 1.8877,
        "grad_norm": 2.038022518157959,
        "learning_rate": 6.08970648081441e-05,
        "epoch": 0.8105181747873164,
        "step": 6288
    },
    {
        "loss": 2.2745,
        "grad_norm": 1.6717666387557983,
        "learning_rate": 6.083765972070813e-05,
        "epoch": 0.8106470739881413,
        "step": 6289
    },
    {
        "loss": 1.1373,
        "grad_norm": 3.544980049133301,
        "learning_rate": 6.077823857646492e-05,
        "epoch": 0.8107759731889662,
        "step": 6290
    },
    {
        "loss": 0.8036,
        "grad_norm": 3.3796803951263428,
        "learning_rate": 6.071880146345136e-05,
        "epoch": 0.8109048723897911,
        "step": 6291
    },
    {
        "loss": 2.2026,
        "grad_norm": 2.7442383766174316,
        "learning_rate": 6.065934846972807e-05,
        "epoch": 0.8110337715906162,
        "step": 6292
    },
    {
        "loss": 1.4577,
        "grad_norm": 2.5692412853240967,
        "learning_rate": 6.059987968337892e-05,
        "epoch": 0.8111626707914411,
        "step": 6293
    },
    {
        "loss": 1.6037,
        "grad_norm": 2.9120497703552246,
        "learning_rate": 6.05403951925116e-05,
        "epoch": 0.811291569992266,
        "step": 6294
    },
    {
        "loss": 0.9671,
        "grad_norm": 2.9059863090515137,
        "learning_rate": 6.0480895085256785e-05,
        "epoch": 0.811420469193091,
        "step": 6295
    },
    {
        "loss": 2.4993,
        "grad_norm": 1.698568344116211,
        "learning_rate": 6.042137944976834e-05,
        "epoch": 0.811549368393916,
        "step": 6296
    },
    {
        "loss": 1.7995,
        "grad_norm": 2.7606704235076904,
        "learning_rate": 6.036184837422316e-05,
        "epoch": 0.8116782675947409,
        "step": 6297
    },
    {
        "loss": 2.0198,
        "grad_norm": 1.9077937602996826,
        "learning_rate": 6.0302301946821014e-05,
        "epoch": 0.8118071667955659,
        "step": 6298
    },
    {
        "loss": 2.1191,
        "grad_norm": 2.5134828090667725,
        "learning_rate": 6.024274025578439e-05,
        "epoch": 0.8119360659963908,
        "step": 6299
    },
    {
        "loss": 1.9768,
        "grad_norm": 3.0788161754608154,
        "learning_rate": 6.0183163389358436e-05,
        "epoch": 0.8120649651972158,
        "step": 6300
    },
    {
        "loss": 2.1921,
        "grad_norm": 1.7958322763442993,
        "learning_rate": 6.012357143581073e-05,
        "epoch": 0.8121938643980408,
        "step": 6301
    },
    {
        "loss": 2.0532,
        "grad_norm": 2.276535749435425,
        "learning_rate": 6.0063964483431235e-05,
        "epoch": 0.8123227635988657,
        "step": 6302
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.6054039001464844,
        "learning_rate": 6.000434262053214e-05,
        "epoch": 0.8124516627996906,
        "step": 6303
    },
    {
        "loss": 1.67,
        "grad_norm": 2.11967134475708,
        "learning_rate": 5.994470593544771e-05,
        "epoch": 0.8125805620005156,
        "step": 6304
    },
    {
        "loss": 2.0492,
        "grad_norm": 2.660065174102783,
        "learning_rate": 5.988505451653418e-05,
        "epoch": 0.8127094612013406,
        "step": 6305
    },
    {
        "loss": 1.2988,
        "grad_norm": 2.6807093620300293,
        "learning_rate": 5.98253884521696e-05,
        "epoch": 0.8128383604021655,
        "step": 6306
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.6646771430969238,
        "learning_rate": 5.976570783075373e-05,
        "epoch": 0.8129672596029904,
        "step": 6307
    },
    {
        "loss": 2.2419,
        "grad_norm": 2.8061399459838867,
        "learning_rate": 5.9706012740707897e-05,
        "epoch": 0.8130961588038154,
        "step": 6308
    },
    {
        "loss": 1.9868,
        "grad_norm": 1.8297889232635498,
        "learning_rate": 5.9646303270474845e-05,
        "epoch": 0.8132250580046404,
        "step": 6309
    },
    {
        "loss": 1.8313,
        "grad_norm": 4.170698642730713,
        "learning_rate": 5.958657950851868e-05,
        "epoch": 0.8133539572054653,
        "step": 6310
    },
    {
        "loss": 2.3862,
        "grad_norm": 1.7455579042434692,
        "learning_rate": 5.952684154332462e-05,
        "epoch": 0.8134828564062903,
        "step": 6311
    },
    {
        "loss": 0.7229,
        "grad_norm": 3.414623260498047,
        "learning_rate": 5.946708946339894e-05,
        "epoch": 0.8136117556071152,
        "step": 6312
    },
    {
        "loss": 0.8829,
        "grad_norm": 3.239142656326294,
        "learning_rate": 5.940732335726886e-05,
        "epoch": 0.8137406548079402,
        "step": 6313
    },
    {
        "loss": 1.7035,
        "grad_norm": 2.0755012035369873,
        "learning_rate": 5.934754331348227e-05,
        "epoch": 0.8138695540087652,
        "step": 6314
    },
    {
        "loss": 0.3961,
        "grad_norm": 1.6546698808670044,
        "learning_rate": 5.9287749420608005e-05,
        "epoch": 0.8139984532095901,
        "step": 6315
    },
    {
        "loss": 1.355,
        "grad_norm": 4.352694511413574,
        "learning_rate": 5.9227941767235005e-05,
        "epoch": 0.814127352410415,
        "step": 6316
    },
    {
        "loss": 2.0945,
        "grad_norm": 3.0293891429901123,
        "learning_rate": 5.9168120441972876e-05,
        "epoch": 0.81425625161124,
        "step": 6317
    },
    {
        "loss": 1.5411,
        "grad_norm": 3.597505569458008,
        "learning_rate": 5.910828553345141e-05,
        "epoch": 0.814385150812065,
        "step": 6318
    },
    {
        "loss": 1.0257,
        "grad_norm": 3.9485764503479004,
        "learning_rate": 5.904843713032051e-05,
        "epoch": 0.8145140500128899,
        "step": 6319
    },
    {
        "loss": 1.5941,
        "grad_norm": 2.6107780933380127,
        "learning_rate": 5.8988575321250016e-05,
        "epoch": 0.8146429492137148,
        "step": 6320
    },
    {
        "loss": 1.9601,
        "grad_norm": 3.2707841396331787,
        "learning_rate": 5.89287001949299e-05,
        "epoch": 0.8147718484145399,
        "step": 6321
    },
    {
        "loss": 1.7974,
        "grad_norm": 2.1929097175598145,
        "learning_rate": 5.8868811840069474e-05,
        "epoch": 0.8149007476153648,
        "step": 6322
    },
    {
        "loss": 1.934,
        "grad_norm": 2.5786349773406982,
        "learning_rate": 5.8808910345397884e-05,
        "epoch": 0.8150296468161897,
        "step": 6323
    },
    {
        "loss": 1.9713,
        "grad_norm": 1.6700986623764038,
        "learning_rate": 5.874899579966369e-05,
        "epoch": 0.8151585460170147,
        "step": 6324
    },
    {
        "loss": 2.246,
        "grad_norm": 1.5604499578475952,
        "learning_rate": 5.8689068291634805e-05,
        "epoch": 0.8152874452178397,
        "step": 6325
    },
    {
        "loss": 1.3382,
        "grad_norm": 3.015577793121338,
        "learning_rate": 5.862912791009827e-05,
        "epoch": 0.8154163444186646,
        "step": 6326
    },
    {
        "loss": 1.5561,
        "grad_norm": 2.8146121501922607,
        "learning_rate": 5.8569174743860476e-05,
        "epoch": 0.8155452436194895,
        "step": 6327
    },
    {
        "loss": 1.3866,
        "grad_norm": 2.8557937145233154,
        "learning_rate": 5.850920888174632e-05,
        "epoch": 0.8156741428203145,
        "step": 6328
    },
    {
        "loss": 2.018,
        "grad_norm": 3.4000017642974854,
        "learning_rate": 5.8449230412599796e-05,
        "epoch": 0.8158030420211395,
        "step": 6329
    },
    {
        "loss": 1.7573,
        "grad_norm": 2.4133265018463135,
        "learning_rate": 5.838923942528347e-05,
        "epoch": 0.8159319412219644,
        "step": 6330
    },
    {
        "loss": 2.6533,
        "grad_norm": 2.324270725250244,
        "learning_rate": 5.83292360086787e-05,
        "epoch": 0.8160608404227894,
        "step": 6331
    },
    {
        "loss": 1.2836,
        "grad_norm": 2.878904342651367,
        "learning_rate": 5.8269220251684764e-05,
        "epoch": 0.8161897396236143,
        "step": 6332
    },
    {
        "loss": 2.252,
        "grad_norm": 1.8129045963287354,
        "learning_rate": 5.820919224321972e-05,
        "epoch": 0.8163186388244393,
        "step": 6333
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.4878904819488525,
        "learning_rate": 5.8149152072219605e-05,
        "epoch": 0.8164475380252643,
        "step": 6334
    },
    {
        "loss": 2.65,
        "grad_norm": 1.2588231563568115,
        "learning_rate": 5.808909982763828e-05,
        "epoch": 0.8165764372260892,
        "step": 6335
    },
    {
        "loss": 2.3514,
        "grad_norm": 2.485196828842163,
        "learning_rate": 5.8029035598447655e-05,
        "epoch": 0.8167053364269141,
        "step": 6336
    },
    {
        "loss": 2.2609,
        "grad_norm": 2.1682159900665283,
        "learning_rate": 5.796895947363762e-05,
        "epoch": 0.8168342356277392,
        "step": 6337
    },
    {
        "loss": 1.9471,
        "grad_norm": 2.840801954269409,
        "learning_rate": 5.790887154221517e-05,
        "epoch": 0.8169631348285641,
        "step": 6338
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.0651986598968506,
        "learning_rate": 5.784877189320533e-05,
        "epoch": 0.817092034029389,
        "step": 6339
    },
    {
        "loss": 2.129,
        "grad_norm": 1.2221430540084839,
        "learning_rate": 5.7788660615650226e-05,
        "epoch": 0.8172209332302139,
        "step": 6340
    },
    {
        "loss": 2.2135,
        "grad_norm": 2.382127523422241,
        "learning_rate": 5.7728537798609086e-05,
        "epoch": 0.817349832431039,
        "step": 6341
    },
    {
        "loss": 1.6591,
        "grad_norm": 2.1934781074523926,
        "learning_rate": 5.766840353115841e-05,
        "epoch": 0.8174787316318639,
        "step": 6342
    },
    {
        "loss": 2.2377,
        "grad_norm": 2.6758296489715576,
        "learning_rate": 5.7608257902391806e-05,
        "epoch": 0.8176076308326888,
        "step": 6343
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.282794237136841,
        "learning_rate": 5.754810100141931e-05,
        "epoch": 0.8177365300335138,
        "step": 6344
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.4450123310089111,
        "learning_rate": 5.7487932917368046e-05,
        "epoch": 0.8178654292343387,
        "step": 6345
    },
    {
        "loss": 1.4202,
        "grad_norm": 2.0461010932922363,
        "learning_rate": 5.7427753739381515e-05,
        "epoch": 0.8179943284351637,
        "step": 6346
    },
    {
        "loss": 1.2881,
        "grad_norm": 2.7533929347991943,
        "learning_rate": 5.736756355661976e-05,
        "epoch": 0.8181232276359887,
        "step": 6347
    },
    {
        "loss": 1.9842,
        "grad_norm": 1.9970909357070923,
        "learning_rate": 5.7307362458258854e-05,
        "epoch": 0.8182521268368136,
        "step": 6348
    },
    {
        "loss": 2.1239,
        "grad_norm": 2.4878957271575928,
        "learning_rate": 5.7247150533491436e-05,
        "epoch": 0.8183810260376385,
        "step": 6349
    },
    {
        "loss": 1.9772,
        "grad_norm": 3.19881272315979,
        "learning_rate": 5.718692787152593e-05,
        "epoch": 0.8185099252384636,
        "step": 6350
    },
    {
        "loss": 2.06,
        "grad_norm": 2.9952542781829834,
        "learning_rate": 5.712669456158677e-05,
        "epoch": 0.8186388244392885,
        "step": 6351
    },
    {
        "loss": 1.6451,
        "grad_norm": 2.9575085639953613,
        "learning_rate": 5.706645069291407e-05,
        "epoch": 0.8187677236401134,
        "step": 6352
    },
    {
        "loss": 2.0675,
        "grad_norm": 2.2816309928894043,
        "learning_rate": 5.7006196354763784e-05,
        "epoch": 0.8188966228409383,
        "step": 6353
    },
    {
        "loss": 0.8051,
        "grad_norm": 2.6726489067077637,
        "learning_rate": 5.6945931636407e-05,
        "epoch": 0.8190255220417634,
        "step": 6354
    },
    {
        "loss": 2.4243,
        "grad_norm": 2.1917779445648193,
        "learning_rate": 5.688565662713062e-05,
        "epoch": 0.8191544212425883,
        "step": 6355
    },
    {
        "loss": 2.1409,
        "grad_norm": 2.210944414138794,
        "learning_rate": 5.6825371416236565e-05,
        "epoch": 0.8192833204434132,
        "step": 6356
    },
    {
        "loss": 1.7947,
        "grad_norm": 2.4381420612335205,
        "learning_rate": 5.676507609304187e-05,
        "epoch": 0.8194122196442382,
        "step": 6357
    },
    {
        "loss": 2.0607,
        "grad_norm": 3.747037410736084,
        "learning_rate": 5.670477074687861e-05,
        "epoch": 0.8195411188450632,
        "step": 6358
    },
    {
        "loss": 1.5644,
        "grad_norm": 2.9275588989257812,
        "learning_rate": 5.664445546709375e-05,
        "epoch": 0.8196700180458881,
        "step": 6359
    },
    {
        "loss": 2.3586,
        "grad_norm": 1.7841823101043701,
        "learning_rate": 5.6584130343048704e-05,
        "epoch": 0.819798917246713,
        "step": 6360
    },
    {
        "loss": 1.3793,
        "grad_norm": 2.6837241649627686,
        "learning_rate": 5.6523795464119874e-05,
        "epoch": 0.819927816447538,
        "step": 6361
    },
    {
        "loss": 0.4588,
        "grad_norm": 3.6761252880096436,
        "learning_rate": 5.646345091969786e-05,
        "epoch": 0.820056715648363,
        "step": 6362
    },
    {
        "loss": 1.4992,
        "grad_norm": 2.421886920928955,
        "learning_rate": 5.640309679918764e-05,
        "epoch": 0.820185614849188,
        "step": 6363
    },
    {
        "loss": 2.2867,
        "grad_norm": 1.7255563735961914,
        "learning_rate": 5.6342733192008365e-05,
        "epoch": 0.8203145140500129,
        "step": 6364
    },
    {
        "loss": 1.6207,
        "grad_norm": 3.129826068878174,
        "learning_rate": 5.628236018759327e-05,
        "epoch": 0.8204434132508378,
        "step": 6365
    },
    {
        "loss": 1.5731,
        "grad_norm": 2.9373722076416016,
        "learning_rate": 5.622197787538948e-05,
        "epoch": 0.8205723124516628,
        "step": 6366
    },
    {
        "loss": 2.2129,
        "grad_norm": 2.314544916152954,
        "learning_rate": 5.616158634485793e-05,
        "epoch": 0.8207012116524878,
        "step": 6367
    },
    {
        "loss": 1.2367,
        "grad_norm": 3.1191036701202393,
        "learning_rate": 5.6101185685473234e-05,
        "epoch": 0.8208301108533127,
        "step": 6368
    },
    {
        "loss": 1.9871,
        "grad_norm": 2.672506809234619,
        "learning_rate": 5.604077598672349e-05,
        "epoch": 0.8209590100541376,
        "step": 6369
    },
    {
        "loss": 1.8192,
        "grad_norm": 3.115884304046631,
        "learning_rate": 5.59803573381102e-05,
        "epoch": 0.8210879092549627,
        "step": 6370
    },
    {
        "loss": 1.7266,
        "grad_norm": 2.5098013877868652,
        "learning_rate": 5.591992982914816e-05,
        "epoch": 0.8212168084557876,
        "step": 6371
    },
    {
        "loss": 1.6469,
        "grad_norm": 2.656306743621826,
        "learning_rate": 5.585949354936522e-05,
        "epoch": 0.8213457076566125,
        "step": 6372
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.9474914073944092,
        "learning_rate": 5.579904858830229e-05,
        "epoch": 0.8214746068574375,
        "step": 6373
    },
    {
        "loss": 2.091,
        "grad_norm": 1.6402263641357422,
        "learning_rate": 5.573859503551315e-05,
        "epoch": 0.8216035060582625,
        "step": 6374
    },
    {
        "loss": 1.8656,
        "grad_norm": 3.0185623168945312,
        "learning_rate": 5.567813298056425e-05,
        "epoch": 0.8217324052590874,
        "step": 6375
    },
    {
        "loss": 1.9684,
        "grad_norm": 1.730931282043457,
        "learning_rate": 5.561766251303466e-05,
        "epoch": 0.8218613044599123,
        "step": 6376
    },
    {
        "loss": 1.3623,
        "grad_norm": 3.3753163814544678,
        "learning_rate": 5.555718372251595e-05,
        "epoch": 0.8219902036607373,
        "step": 6377
    },
    {
        "loss": 2.2759,
        "grad_norm": 1.7123125791549683,
        "learning_rate": 5.5496696698611974e-05,
        "epoch": 0.8221191028615623,
        "step": 6378
    },
    {
        "loss": 1.9039,
        "grad_norm": 1.3724617958068848,
        "learning_rate": 5.5436201530938804e-05,
        "epoch": 0.8222480020623872,
        "step": 6379
    },
    {
        "loss": 1.7891,
        "grad_norm": 1.95259428024292,
        "learning_rate": 5.537569830912459e-05,
        "epoch": 0.8223769012632122,
        "step": 6380
    },
    {
        "loss": 1.5318,
        "grad_norm": 2.5369722843170166,
        "learning_rate": 5.531518712280931e-05,
        "epoch": 0.8225058004640371,
        "step": 6381
    },
    {
        "loss": 2.3845,
        "grad_norm": 2.1501405239105225,
        "learning_rate": 5.525466806164506e-05,
        "epoch": 0.822634699664862,
        "step": 6382
    },
    {
        "loss": 2.3276,
        "grad_norm": 2.1048691272735596,
        "learning_rate": 5.519414121529518e-05,
        "epoch": 0.8227635988656871,
        "step": 6383
    },
    {
        "loss": 2.1161,
        "grad_norm": 1.3151353597640991,
        "learning_rate": 5.513360667343477e-05,
        "epoch": 0.822892498066512,
        "step": 6384
    },
    {
        "loss": 2.3378,
        "grad_norm": 2.2927067279815674,
        "learning_rate": 5.507306452575034e-05,
        "epoch": 0.8230213972673369,
        "step": 6385
    },
    {
        "loss": 1.7705,
        "grad_norm": 2.716702699661255,
        "learning_rate": 5.501251486193961e-05,
        "epoch": 0.8231502964681618,
        "step": 6386
    },
    {
        "loss": 2.0649,
        "grad_norm": 1.5887433290481567,
        "learning_rate": 5.4951957771711383e-05,
        "epoch": 0.8232791956689869,
        "step": 6387
    },
    {
        "loss": 1.7517,
        "grad_norm": 3.190624475479126,
        "learning_rate": 5.4891393344785766e-05,
        "epoch": 0.8234080948698118,
        "step": 6388
    },
    {
        "loss": 2.2829,
        "grad_norm": 1.8520995378494263,
        "learning_rate": 5.4830821670893294e-05,
        "epoch": 0.8235369940706367,
        "step": 6389
    },
    {
        "loss": 1.9761,
        "grad_norm": 2.2198662757873535,
        "learning_rate": 5.477024283977552e-05,
        "epoch": 0.8236658932714617,
        "step": 6390
    },
    {
        "loss": 1.4373,
        "grad_norm": 2.0866339206695557,
        "learning_rate": 5.470965694118454e-05,
        "epoch": 0.8237947924722867,
        "step": 6391
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.4857581853866577,
        "learning_rate": 5.4649064064882907e-05,
        "epoch": 0.8239236916731116,
        "step": 6392
    },
    {
        "loss": 1.8253,
        "grad_norm": 1.8947275876998901,
        "learning_rate": 5.458846430064344e-05,
        "epoch": 0.8240525908739366,
        "step": 6393
    },
    {
        "loss": 1.3666,
        "grad_norm": 3.0253331661224365,
        "learning_rate": 5.4527857738249474e-05,
        "epoch": 0.8241814900747615,
        "step": 6394
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.3486809730529785,
        "learning_rate": 5.446724446749398e-05,
        "epoch": 0.8243103892755865,
        "step": 6395
    },
    {
        "loss": 1.5945,
        "grad_norm": 2.9211854934692383,
        "learning_rate": 5.440662457818013e-05,
        "epoch": 0.8244392884764115,
        "step": 6396
    },
    {
        "loss": 1.1208,
        "grad_norm": 3.161470413208008,
        "learning_rate": 5.4345998160120756e-05,
        "epoch": 0.8245681876772364,
        "step": 6397
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.9218714237213135,
        "learning_rate": 5.4285365303138704e-05,
        "epoch": 0.8246970868780613,
        "step": 6398
    },
    {
        "loss": 2.1438,
        "grad_norm": 2.0609560012817383,
        "learning_rate": 5.4224726097065795e-05,
        "epoch": 0.8248259860788864,
        "step": 6399
    },
    {
        "loss": 1.6428,
        "grad_norm": 2.2956347465515137,
        "learning_rate": 5.4164080631743786e-05,
        "epoch": 0.8249548852797113,
        "step": 6400
    },
    {
        "loss": 1.9061,
        "grad_norm": 2.6253297328948975,
        "learning_rate": 5.41034289970235e-05,
        "epoch": 0.8250837844805362,
        "step": 6401
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.1943256855010986,
        "learning_rate": 5.404277128276475e-05,
        "epoch": 0.8252126836813611,
        "step": 6402
    },
    {
        "loss": 2.1058,
        "grad_norm": 2.2183234691619873,
        "learning_rate": 5.3982107578836484e-05,
        "epoch": 0.8253415828821862,
        "step": 6403
    },
    {
        "loss": 2.0327,
        "grad_norm": 1.6596955060958862,
        "learning_rate": 5.392143797511676e-05,
        "epoch": 0.8254704820830111,
        "step": 6404
    },
    {
        "loss": 1.5432,
        "grad_norm": 2.617417812347412,
        "learning_rate": 5.386076256149189e-05,
        "epoch": 0.825599381283836,
        "step": 6405
    },
    {
        "loss": 1.5148,
        "grad_norm": 2.3611366748809814,
        "learning_rate": 5.380008142785723e-05,
        "epoch": 0.825728280484661,
        "step": 6406
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.288536548614502,
        "learning_rate": 5.373939466411646e-05,
        "epoch": 0.825857179685486,
        "step": 6407
    },
    {
        "loss": 1.9769,
        "grad_norm": 1.9348145723342896,
        "learning_rate": 5.367870236018145e-05,
        "epoch": 0.8259860788863109,
        "step": 6408
    },
    {
        "loss": 2.3588,
        "grad_norm": 1.8054769039154053,
        "learning_rate": 5.3618004605972403e-05,
        "epoch": 0.8261149780871359,
        "step": 6409
    },
    {
        "loss": 2.1435,
        "grad_norm": 2.038386344909668,
        "learning_rate": 5.35573014914178e-05,
        "epoch": 0.8262438772879608,
        "step": 6410
    },
    {
        "loss": 1.5613,
        "grad_norm": 2.549095630645752,
        "learning_rate": 5.3496593106453596e-05,
        "epoch": 0.8263727764887858,
        "step": 6411
    },
    {
        "loss": 1.3085,
        "grad_norm": 4.517751216888428,
        "learning_rate": 5.343587954102405e-05,
        "epoch": 0.8265016756896107,
        "step": 6412
    },
    {
        "loss": 1.9559,
        "grad_norm": 2.1148324012756348,
        "learning_rate": 5.337516088508077e-05,
        "epoch": 0.8266305748904357,
        "step": 6413
    },
    {
        "loss": 2.4937,
        "grad_norm": 3.2384870052337646,
        "learning_rate": 5.331443722858309e-05,
        "epoch": 0.8267594740912606,
        "step": 6414
    },
    {
        "loss": 1.6818,
        "grad_norm": 2.8288745880126953,
        "learning_rate": 5.325370866149747e-05,
        "epoch": 0.8268883732920856,
        "step": 6415
    },
    {
        "loss": 2.2114,
        "grad_norm": 2.2784571647644043,
        "learning_rate": 5.319297527379805e-05,
        "epoch": 0.8270172724929106,
        "step": 6416
    },
    {
        "loss": 1.5074,
        "grad_norm": 2.7734806537628174,
        "learning_rate": 5.3132237155465844e-05,
        "epoch": 0.8271461716937355,
        "step": 6417
    },
    {
        "loss": 2.4181,
        "grad_norm": 3.108888864517212,
        "learning_rate": 5.307149439648892e-05,
        "epoch": 0.8272750708945604,
        "step": 6418
    },
    {
        "loss": 1.973,
        "grad_norm": 2.0007617473602295,
        "learning_rate": 5.301074708686226e-05,
        "epoch": 0.8274039700953854,
        "step": 6419
    },
    {
        "loss": 2.2385,
        "grad_norm": 2.1935527324676514,
        "learning_rate": 5.294999531658761e-05,
        "epoch": 0.8275328692962104,
        "step": 6420
    },
    {
        "loss": 1.7715,
        "grad_norm": 2.448108196258545,
        "learning_rate": 5.28892391756731e-05,
        "epoch": 0.8276617684970353,
        "step": 6421
    },
    {
        "loss": 2.0278,
        "grad_norm": 1.73671555519104,
        "learning_rate": 5.2828478754133725e-05,
        "epoch": 0.8277906676978602,
        "step": 6422
    },
    {
        "loss": 1.6957,
        "grad_norm": 2.796100616455078,
        "learning_rate": 5.276771414199053e-05,
        "epoch": 0.8279195668986852,
        "step": 6423
    },
    {
        "loss": 2.01,
        "grad_norm": 3.1527931690216064,
        "learning_rate": 5.270694542927087e-05,
        "epoch": 0.8280484660995102,
        "step": 6424
    },
    {
        "loss": 2.1881,
        "grad_norm": 1.204058289527893,
        "learning_rate": 5.264617270600814e-05,
        "epoch": 0.8281773653003351,
        "step": 6425
    },
    {
        "loss": 1.5409,
        "grad_norm": 2.5842580795288086,
        "learning_rate": 5.25853960622418e-05,
        "epoch": 0.8283062645011601,
        "step": 6426
    },
    {
        "loss": 1.1608,
        "grad_norm": 2.858029365539551,
        "learning_rate": 5.2524615588016767e-05,
        "epoch": 0.828435163701985,
        "step": 6427
    },
    {
        "loss": 2.1556,
        "grad_norm": 2.803712844848633,
        "learning_rate": 5.2463831373384106e-05,
        "epoch": 0.82856406290281,
        "step": 6428
    },
    {
        "loss": 1.4525,
        "grad_norm": 3.026397705078125,
        "learning_rate": 5.240304350840014e-05,
        "epoch": 0.828692962103635,
        "step": 6429
    },
    {
        "loss": 1.9119,
        "grad_norm": 2.0466554164886475,
        "learning_rate": 5.234225208312663e-05,
        "epoch": 0.8288218613044599,
        "step": 6430
    },
    {
        "loss": 1.919,
        "grad_norm": 3.4100944995880127,
        "learning_rate": 5.228145718763068e-05,
        "epoch": 0.8289507605052848,
        "step": 6431
    },
    {
        "loss": 1.1268,
        "grad_norm": 2.862584352493286,
        "learning_rate": 5.222065891198447e-05,
        "epoch": 0.8290796597061099,
        "step": 6432
    },
    {
        "loss": 1.6253,
        "grad_norm": 2.3159735202789307,
        "learning_rate": 5.2159857346265196e-05,
        "epoch": 0.8292085589069348,
        "step": 6433
    },
    {
        "loss": 1.3504,
        "grad_norm": 3.4621760845184326,
        "learning_rate": 5.2099052580554985e-05,
        "epoch": 0.8293374581077597,
        "step": 6434
    },
    {
        "loss": 2.4201,
        "grad_norm": 1.6373027563095093,
        "learning_rate": 5.203824470494064e-05,
        "epoch": 0.8294663573085846,
        "step": 6435
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.5788437128067017,
        "learning_rate": 5.197743380951362e-05,
        "epoch": 0.8295952565094097,
        "step": 6436
    },
    {
        "loss": 1.7173,
        "grad_norm": 1.8669424057006836,
        "learning_rate": 5.191661998436982e-05,
        "epoch": 0.8297241557102346,
        "step": 6437
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.037482976913452,
        "learning_rate": 5.1855803319609486e-05,
        "epoch": 0.8298530549110595,
        "step": 6438
    },
    {
        "loss": 2.4721,
        "grad_norm": 1.5249909162521362,
        "learning_rate": 5.17949839053371e-05,
        "epoch": 0.8299819541118845,
        "step": 6439
    },
    {
        "loss": 2.013,
        "grad_norm": 2.41333270072937,
        "learning_rate": 5.1734161831661166e-05,
        "epoch": 0.8301108533127095,
        "step": 6440
    },
    {
        "loss": 2.164,
        "grad_norm": 2.1342947483062744,
        "learning_rate": 5.167333718869418e-05,
        "epoch": 0.8302397525135344,
        "step": 6441
    },
    {
        "loss": 1.5271,
        "grad_norm": 2.43868088722229,
        "learning_rate": 5.16125100665524e-05,
        "epoch": 0.8303686517143594,
        "step": 6442
    },
    {
        "loss": 2.0386,
        "grad_norm": 2.3433709144592285,
        "learning_rate": 5.1551680555355805e-05,
        "epoch": 0.8304975509151843,
        "step": 6443
    },
    {
        "loss": 1.4861,
        "grad_norm": 3.2933132648468018,
        "learning_rate": 5.149084874522787e-05,
        "epoch": 0.8306264501160093,
        "step": 6444
    },
    {
        "loss": 1.6474,
        "grad_norm": 1.7320877313613892,
        "learning_rate": 5.143001472629548e-05,
        "epoch": 0.8307553493168343,
        "step": 6445
    },
    {
        "loss": 1.5481,
        "grad_norm": 3.517953395843506,
        "learning_rate": 5.136917858868883e-05,
        "epoch": 0.8308842485176592,
        "step": 6446
    },
    {
        "loss": 1.927,
        "grad_norm": 2.379499673843384,
        "learning_rate": 5.1308340422541214e-05,
        "epoch": 0.8310131477184841,
        "step": 6447
    },
    {
        "loss": 2.3906,
        "grad_norm": 1.923258662223816,
        "learning_rate": 5.124750031798888e-05,
        "epoch": 0.8311420469193092,
        "step": 6448
    },
    {
        "loss": 1.5436,
        "grad_norm": 3.3373682498931885,
        "learning_rate": 5.118665836517124e-05,
        "epoch": 0.8312709461201341,
        "step": 6449
    },
    {
        "loss": 1.8228,
        "grad_norm": 2.3140008449554443,
        "learning_rate": 5.1125814654229976e-05,
        "epoch": 0.831399845320959,
        "step": 6450
    },
    {
        "loss": 0.8259,
        "grad_norm": 1.9804234504699707,
        "learning_rate": 5.106496927530972e-05,
        "epoch": 0.8315287445217839,
        "step": 6451
    },
    {
        "loss": 1.7768,
        "grad_norm": 2.7197821140289307,
        "learning_rate": 5.1004122318557445e-05,
        "epoch": 0.831657643722609,
        "step": 6452
    },
    {
        "loss": 1.7632,
        "grad_norm": 4.535125732421875,
        "learning_rate": 5.094327387412251e-05,
        "epoch": 0.8317865429234339,
        "step": 6453
    },
    {
        "loss": 1.4941,
        "grad_norm": 1.4818694591522217,
        "learning_rate": 5.0882424032156384e-05,
        "epoch": 0.8319154421242588,
        "step": 6454
    },
    {
        "loss": 1.7037,
        "grad_norm": 2.364266872406006,
        "learning_rate": 5.0821572882812884e-05,
        "epoch": 0.8320443413250838,
        "step": 6455
    },
    {
        "loss": 2.2086,
        "grad_norm": 2.1338553428649902,
        "learning_rate": 5.076072051624736e-05,
        "epoch": 0.8321732405259087,
        "step": 6456
    },
    {
        "loss": 1.8184,
        "grad_norm": 3.342235565185547,
        "learning_rate": 5.069986702261722e-05,
        "epoch": 0.8323021397267337,
        "step": 6457
    },
    {
        "loss": 1.6151,
        "grad_norm": 3.4458582401275635,
        "learning_rate": 5.063901249208151e-05,
        "epoch": 0.8324310389275587,
        "step": 6458
    },
    {
        "loss": 2.1366,
        "grad_norm": 1.5353047847747803,
        "learning_rate": 5.057815701480076e-05,
        "epoch": 0.8325599381283836,
        "step": 6459
    },
    {
        "loss": 2.0447,
        "grad_norm": 2.9275543689727783,
        "learning_rate": 5.05173006809369e-05,
        "epoch": 0.8326888373292085,
        "step": 6460
    },
    {
        "loss": 1.2786,
        "grad_norm": 2.879702091217041,
        "learning_rate": 5.045644358065325e-05,
        "epoch": 0.8328177365300335,
        "step": 6461
    },
    {
        "loss": 2.3547,
        "grad_norm": 2.3631374835968018,
        "learning_rate": 5.039558580411423e-05,
        "epoch": 0.8329466357308585,
        "step": 6462
    },
    {
        "loss": 2.0892,
        "grad_norm": 1.6804147958755493,
        "learning_rate": 5.033472744148504e-05,
        "epoch": 0.8330755349316834,
        "step": 6463
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.954944133758545,
        "learning_rate": 5.027386858293193e-05,
        "epoch": 0.8332044341325083,
        "step": 6464
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.5607364177703857,
        "learning_rate": 5.021300931862206e-05,
        "epoch": 0.8333333333333334,
        "step": 6465
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.20563006401062,
        "learning_rate": 5.0152149738722754e-05,
        "epoch": 0.8334622325341583,
        "step": 6466
    },
    {
        "loss": 1.2312,
        "grad_norm": 2.5922751426696777,
        "learning_rate": 5.009128993340224e-05,
        "epoch": 0.8335911317349832,
        "step": 6467
    },
    {
        "loss": 1.3381,
        "grad_norm": 2.711331367492676,
        "learning_rate": 5.003042999282891e-05,
        "epoch": 0.8337200309358082,
        "step": 6468
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.135389566421509,
        "learning_rate": 4.9969570007171165e-05,
        "epoch": 0.8338489301366332,
        "step": 6469
    },
    {
        "loss": 1.9941,
        "grad_norm": 1.8494619131088257,
        "learning_rate": 4.99087100665977e-05,
        "epoch": 0.8339778293374581,
        "step": 6470
    },
    {
        "loss": 1.8321,
        "grad_norm": 1.8891910314559937,
        "learning_rate": 4.984785026127726e-05,
        "epoch": 0.834106728538283,
        "step": 6471
    },
    {
        "loss": 1.0536,
        "grad_norm": 2.718799352645874,
        "learning_rate": 4.9786990681377943e-05,
        "epoch": 0.834235627739108,
        "step": 6472
    },
    {
        "loss": 2.0087,
        "grad_norm": 1.7267729043960571,
        "learning_rate": 4.9726131417068015e-05,
        "epoch": 0.834364526939933,
        "step": 6473
    },
    {
        "loss": 1.2957,
        "grad_norm": 3.1314303874969482,
        "learning_rate": 4.966527255851504e-05,
        "epoch": 0.8344934261407579,
        "step": 6474
    },
    {
        "loss": 1.8344,
        "grad_norm": 1.3320285081863403,
        "learning_rate": 4.960441419588585e-05,
        "epoch": 0.8346223253415829,
        "step": 6475
    },
    {
        "loss": 1.1724,
        "grad_norm": 1.789730191230774,
        "learning_rate": 4.9543556419346684e-05,
        "epoch": 0.8347512245424078,
        "step": 6476
    },
    {
        "loss": 1.9631,
        "grad_norm": 2.209930419921875,
        "learning_rate": 4.948269931906305e-05,
        "epoch": 0.8348801237432328,
        "step": 6477
    },
    {
        "loss": 1.0458,
        "grad_norm": 3.8062047958374023,
        "learning_rate": 4.9421842985199244e-05,
        "epoch": 0.8350090229440578,
        "step": 6478
    },
    {
        "loss": 2.1701,
        "grad_norm": 2.1153640747070312,
        "learning_rate": 4.9360987507918504e-05,
        "epoch": 0.8351379221448827,
        "step": 6479
    },
    {
        "loss": 1.924,
        "grad_norm": 2.0384254455566406,
        "learning_rate": 4.930013297738279e-05,
        "epoch": 0.8352668213457076,
        "step": 6480
    },
    {
        "loss": 2.5127,
        "grad_norm": 1.6272165775299072,
        "learning_rate": 4.9239279483752724e-05,
        "epoch": 0.8353957205465327,
        "step": 6481
    },
    {
        "loss": 1.729,
        "grad_norm": 2.8886072635650635,
        "learning_rate": 4.917842711718713e-05,
        "epoch": 0.8355246197473576,
        "step": 6482
    },
    {
        "loss": 2.0275,
        "grad_norm": 1.8646445274353027,
        "learning_rate": 4.9117575967843546e-05,
        "epoch": 0.8356535189481825,
        "step": 6483
    },
    {
        "loss": 1.5382,
        "grad_norm": 2.9360246658325195,
        "learning_rate": 4.90567261258775e-05,
        "epoch": 0.8357824181490074,
        "step": 6484
    },
    {
        "loss": 1.937,
        "grad_norm": 2.3804609775543213,
        "learning_rate": 4.899587768144257e-05,
        "epoch": 0.8359113173498325,
        "step": 6485
    },
    {
        "loss": 2.0562,
        "grad_norm": 1.8372583389282227,
        "learning_rate": 4.8935030724690294e-05,
        "epoch": 0.8360402165506574,
        "step": 6486
    },
    {
        "loss": 2.1883,
        "grad_norm": 1.9091264009475708,
        "learning_rate": 4.8874185345770104e-05,
        "epoch": 0.8361691157514823,
        "step": 6487
    },
    {
        "loss": 1.9219,
        "grad_norm": 1.6968010663986206,
        "learning_rate": 4.881334163482877e-05,
        "epoch": 0.8362980149523073,
        "step": 6488
    },
    {
        "loss": 1.9516,
        "grad_norm": 2.0833206176757812,
        "learning_rate": 4.8752499682011056e-05,
        "epoch": 0.8364269141531323,
        "step": 6489
    },
    {
        "loss": 1.6238,
        "grad_norm": 3.763723134994507,
        "learning_rate": 4.86916595774588e-05,
        "epoch": 0.8365558133539572,
        "step": 6490
    },
    {
        "loss": 1.5446,
        "grad_norm": 1.9820098876953125,
        "learning_rate": 4.8630821411311176e-05,
        "epoch": 0.8366847125547822,
        "step": 6491
    },
    {
        "loss": 1.8518,
        "grad_norm": 2.6820921897888184,
        "learning_rate": 4.856998527370453e-05,
        "epoch": 0.8368136117556071,
        "step": 6492
    },
    {
        "loss": 2.0512,
        "grad_norm": 2.384740114212036,
        "learning_rate": 4.850915125477222e-05,
        "epoch": 0.836942510956432,
        "step": 6493
    },
    {
        "loss": 1.301,
        "grad_norm": 2.5648000240325928,
        "learning_rate": 4.8448319444644206e-05,
        "epoch": 0.8370714101572571,
        "step": 6494
    },
    {
        "loss": 2.5264,
        "grad_norm": 1.4527770280838013,
        "learning_rate": 4.838748993344761e-05,
        "epoch": 0.837200309358082,
        "step": 6495
    },
    {
        "loss": 1.9888,
        "grad_norm": 2.4067275524139404,
        "learning_rate": 4.832666281130583e-05,
        "epoch": 0.8373292085589069,
        "step": 6496
    },
    {
        "loss": 1.6764,
        "grad_norm": 1.9899570941925049,
        "learning_rate": 4.8265838168338845e-05,
        "epoch": 0.8374581077597318,
        "step": 6497
    },
    {
        "loss": 1.8481,
        "grad_norm": 3.8679516315460205,
        "learning_rate": 4.820501609466291e-05,
        "epoch": 0.8375870069605569,
        "step": 6498
    },
    {
        "loss": 1.6431,
        "grad_norm": 3.9192628860473633,
        "learning_rate": 4.8144196680390526e-05,
        "epoch": 0.8377159061613818,
        "step": 6499
    },
    {
        "loss": 1.7203,
        "grad_norm": 3.189105272293091,
        "learning_rate": 4.808338001563019e-05,
        "epoch": 0.8378448053622067,
        "step": 6500
    },
    {
        "loss": 1.831,
        "grad_norm": 2.341925859451294,
        "learning_rate": 4.802256619048639e-05,
        "epoch": 0.8379737045630317,
        "step": 6501
    },
    {
        "loss": 2.1684,
        "grad_norm": 1.7291048765182495,
        "learning_rate": 4.796175529505937e-05,
        "epoch": 0.8381026037638567,
        "step": 6502
    },
    {
        "loss": 1.389,
        "grad_norm": 2.8919060230255127,
        "learning_rate": 4.7900947419445026e-05,
        "epoch": 0.8382315029646816,
        "step": 6503
    },
    {
        "loss": 1.963,
        "grad_norm": 2.95361065864563,
        "learning_rate": 4.7840142653734816e-05,
        "epoch": 0.8383604021655066,
        "step": 6504
    },
    {
        "loss": 1.1197,
        "grad_norm": 2.8193914890289307,
        "learning_rate": 4.777934108801555e-05,
        "epoch": 0.8384893013663315,
        "step": 6505
    },
    {
        "loss": 1.9706,
        "grad_norm": 1.5443425178527832,
        "learning_rate": 4.771854281236934e-05,
        "epoch": 0.8386182005671565,
        "step": 6506
    },
    {
        "loss": 1.5729,
        "grad_norm": 3.872636556625366,
        "learning_rate": 4.765774791687338e-05,
        "epoch": 0.8387470997679815,
        "step": 6507
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.9374430179595947,
        "learning_rate": 4.759695649159987e-05,
        "epoch": 0.8388759989688064,
        "step": 6508
    },
    {
        "loss": 1.1932,
        "grad_norm": 4.269039630889893,
        "learning_rate": 4.75361686266159e-05,
        "epoch": 0.8390048981696313,
        "step": 6509
    },
    {
        "loss": 1.4801,
        "grad_norm": 2.810166835784912,
        "learning_rate": 4.747538441198324e-05,
        "epoch": 0.8391337973704563,
        "step": 6510
    },
    {
        "loss": 1.9801,
        "grad_norm": 1.4609384536743164,
        "learning_rate": 4.741460393775829e-05,
        "epoch": 0.8392626965712813,
        "step": 6511
    },
    {
        "loss": 1.9381,
        "grad_norm": 2.712709665298462,
        "learning_rate": 4.735382729399186e-05,
        "epoch": 0.8393915957721062,
        "step": 6512
    },
    {
        "loss": 2.219,
        "grad_norm": 2.158057689666748,
        "learning_rate": 4.729305457072914e-05,
        "epoch": 0.8395204949729311,
        "step": 6513
    },
    {
        "loss": 1.6732,
        "grad_norm": 2.1958982944488525,
        "learning_rate": 4.7232285858009475e-05,
        "epoch": 0.8396493941737562,
        "step": 6514
    },
    {
        "loss": 2.152,
        "grad_norm": 2.8670477867126465,
        "learning_rate": 4.717152124586622e-05,
        "epoch": 0.8397782933745811,
        "step": 6515
    },
    {
        "loss": 1.6091,
        "grad_norm": 2.693371295928955,
        "learning_rate": 4.711076082432691e-05,
        "epoch": 0.839907192575406,
        "step": 6516
    },
    {
        "loss": 1.9318,
        "grad_norm": 1.7272135019302368,
        "learning_rate": 4.7050004683412477e-05,
        "epoch": 0.840036091776231,
        "step": 6517
    },
    {
        "loss": 1.6413,
        "grad_norm": 2.24066162109375,
        "learning_rate": 4.698925291313776e-05,
        "epoch": 0.840164990977056,
        "step": 6518
    },
    {
        "loss": 1.6807,
        "grad_norm": 2.0745348930358887,
        "learning_rate": 4.69285056035111e-05,
        "epoch": 0.8402938901778809,
        "step": 6519
    },
    {
        "loss": 1.8665,
        "grad_norm": 2.92716908454895,
        "learning_rate": 4.686776284453417e-05,
        "epoch": 0.8404227893787058,
        "step": 6520
    },
    {
        "loss": 2.2184,
        "grad_norm": 2.0873711109161377,
        "learning_rate": 4.6807024726201895e-05,
        "epoch": 0.8405516885795308,
        "step": 6521
    },
    {
        "loss": 2.072,
        "grad_norm": 2.787111282348633,
        "learning_rate": 4.674629133850254e-05,
        "epoch": 0.8406805877803558,
        "step": 6522
    },
    {
        "loss": 1.6629,
        "grad_norm": 3.2026586532592773,
        "learning_rate": 4.668556277141698e-05,
        "epoch": 0.8408094869811807,
        "step": 6523
    },
    {
        "loss": 1.384,
        "grad_norm": 2.309372663497925,
        "learning_rate": 4.662483911491925e-05,
        "epoch": 0.8409383861820057,
        "step": 6524
    },
    {
        "loss": 2.1548,
        "grad_norm": 2.276062250137329,
        "learning_rate": 4.656412045897597e-05,
        "epoch": 0.8410672853828306,
        "step": 6525
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.640933871269226,
        "learning_rate": 4.6503406893546416e-05,
        "epoch": 0.8411961845836555,
        "step": 6526
    },
    {
        "loss": 2.1956,
        "grad_norm": 2.0718319416046143,
        "learning_rate": 4.6442698508582207e-05,
        "epoch": 0.8413250837844806,
        "step": 6527
    },
    {
        "loss": 2.5322,
        "grad_norm": 1.443922758102417,
        "learning_rate": 4.638199539402755e-05,
        "epoch": 0.8414539829853055,
        "step": 6528
    },
    {
        "loss": 2.5565,
        "grad_norm": 1.7775861024856567,
        "learning_rate": 4.632129763981863e-05,
        "epoch": 0.8415828821861304,
        "step": 6529
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.3617947101593018,
        "learning_rate": 4.626060533588361e-05,
        "epoch": 0.8417117813869553,
        "step": 6530
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.6203022003173828,
        "learning_rate": 4.619991857214272e-05,
        "epoch": 0.8418406805877804,
        "step": 6531
    },
    {
        "loss": 1.8433,
        "grad_norm": 2.168560028076172,
        "learning_rate": 4.6139237438508125e-05,
        "epoch": 0.8419695797886053,
        "step": 6532
    },
    {
        "loss": 2.0253,
        "grad_norm": 1.7319085597991943,
        "learning_rate": 4.6078562024883245e-05,
        "epoch": 0.8420984789894302,
        "step": 6533
    },
    {
        "loss": 1.2823,
        "grad_norm": 2.7996485233306885,
        "learning_rate": 4.601789242116345e-05,
        "epoch": 0.8422273781902552,
        "step": 6534
    },
    {
        "loss": 1.7853,
        "grad_norm": 2.5776329040527344,
        "learning_rate": 4.595722871723534e-05,
        "epoch": 0.8423562773910802,
        "step": 6535
    },
    {
        "loss": 1.5307,
        "grad_norm": 2.5301103591918945,
        "learning_rate": 4.5896571002976587e-05,
        "epoch": 0.8424851765919051,
        "step": 6536
    },
    {
        "loss": 1.739,
        "grad_norm": 2.353426456451416,
        "learning_rate": 4.583591936825615e-05,
        "epoch": 0.8426140757927301,
        "step": 6537
    },
    {
        "loss": 2.3311,
        "grad_norm": 2.4251816272735596,
        "learning_rate": 4.577527390293422e-05,
        "epoch": 0.842742974993555,
        "step": 6538
    },
    {
        "loss": 2.7127,
        "grad_norm": 1.9412782192230225,
        "learning_rate": 4.571463469686131e-05,
        "epoch": 0.84287187419438,
        "step": 6539
    },
    {
        "loss": 1.6587,
        "grad_norm": 2.2881946563720703,
        "learning_rate": 4.565400183987919e-05,
        "epoch": 0.843000773395205,
        "step": 6540
    },
    {
        "loss": 1.2341,
        "grad_norm": 3.1744863986968994,
        "learning_rate": 4.559337542181996e-05,
        "epoch": 0.8431296725960299,
        "step": 6541
    },
    {
        "loss": 2.0694,
        "grad_norm": 1.2824985980987549,
        "learning_rate": 4.55327555325061e-05,
        "epoch": 0.8432585717968548,
        "step": 6542
    },
    {
        "loss": 1.6345,
        "grad_norm": 3.9911534786224365,
        "learning_rate": 4.547214226175053e-05,
        "epoch": 0.8433874709976799,
        "step": 6543
    },
    {
        "loss": 2.3484,
        "grad_norm": 2.5260233879089355,
        "learning_rate": 4.541153569935649e-05,
        "epoch": 0.8435163701985048,
        "step": 6544
    },
    {
        "loss": 2.1115,
        "grad_norm": 2.5008623600006104,
        "learning_rate": 4.535093593511711e-05,
        "epoch": 0.8436452693993297,
        "step": 6545
    },
    {
        "loss": 1.9078,
        "grad_norm": 2.8539836406707764,
        "learning_rate": 4.529034305881547e-05,
        "epoch": 0.8437741686001546,
        "step": 6546
    },
    {
        "loss": 2.4269,
        "grad_norm": 1.5490360260009766,
        "learning_rate": 4.522975716022448e-05,
        "epoch": 0.8439030678009797,
        "step": 6547
    },
    {
        "loss": 2.57,
        "grad_norm": 2.4425041675567627,
        "learning_rate": 4.516917832910678e-05,
        "epoch": 0.8440319670018046,
        "step": 6548
    },
    {
        "loss": 1.9732,
        "grad_norm": 2.145354747772217,
        "learning_rate": 4.510860665521424e-05,
        "epoch": 0.8441608662026295,
        "step": 6549
    },
    {
        "loss": 1.8275,
        "grad_norm": 3.3406548500061035,
        "learning_rate": 4.5048042228288554e-05,
        "epoch": 0.8442897654034545,
        "step": 6550
    },
    {
        "loss": 1.3952,
        "grad_norm": 4.356006622314453,
        "learning_rate": 4.49874851380604e-05,
        "epoch": 0.8444186646042795,
        "step": 6551
    },
    {
        "loss": 1.2738,
        "grad_norm": 3.994912624359131,
        "learning_rate": 4.4926935474249675e-05,
        "epoch": 0.8445475638051044,
        "step": 6552
    },
    {
        "loss": 2.0285,
        "grad_norm": 2.269472360610962,
        "learning_rate": 4.486639332656524e-05,
        "epoch": 0.8446764630059294,
        "step": 6553
    },
    {
        "loss": 2.3809,
        "grad_norm": 1.8049252033233643,
        "learning_rate": 4.4805858784704905e-05,
        "epoch": 0.8448053622067543,
        "step": 6554
    },
    {
        "loss": 1.7443,
        "grad_norm": 2.2918753623962402,
        "learning_rate": 4.474533193835494e-05,
        "epoch": 0.8449342614075793,
        "step": 6555
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.732783317565918,
        "learning_rate": 4.4684812877190616e-05,
        "epoch": 0.8450631606084043,
        "step": 6556
    },
    {
        "loss": 2.4307,
        "grad_norm": 2.034661054611206,
        "learning_rate": 4.462430169087542e-05,
        "epoch": 0.8451920598092292,
        "step": 6557
    },
    {
        "loss": 1.6566,
        "grad_norm": 2.883986234664917,
        "learning_rate": 4.456379846906121e-05,
        "epoch": 0.8453209590100541,
        "step": 6558
    },
    {
        "loss": 2.4696,
        "grad_norm": 2.12229061126709,
        "learning_rate": 4.450330330138804e-05,
        "epoch": 0.8454498582108791,
        "step": 6559
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.6148626804351807,
        "learning_rate": 4.444281627748407e-05,
        "epoch": 0.8455787574117041,
        "step": 6560
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.0739357471466064,
        "learning_rate": 4.4382337486965345e-05,
        "epoch": 0.845707656612529,
        "step": 6561
    },
    {
        "loss": 1.1086,
        "grad_norm": 5.680506229400635,
        "learning_rate": 4.432186701943577e-05,
        "epoch": 0.8458365558133539,
        "step": 6562
    },
    {
        "loss": 1.4759,
        "grad_norm": 2.8152291774749756,
        "learning_rate": 4.426140496448686e-05,
        "epoch": 0.8459654550141789,
        "step": 6563
    },
    {
        "loss": 1.988,
        "grad_norm": 1.600966453552246,
        "learning_rate": 4.420095141169771e-05,
        "epoch": 0.8460943542150039,
        "step": 6564
    },
    {
        "loss": 2.3116,
        "grad_norm": 1.351309061050415,
        "learning_rate": 4.41405064506348e-05,
        "epoch": 0.8462232534158288,
        "step": 6565
    },
    {
        "loss": 2.2015,
        "grad_norm": 1.4865318536758423,
        "learning_rate": 4.408007017085186e-05,
        "epoch": 0.8463521526166538,
        "step": 6566
    },
    {
        "loss": 1.878,
        "grad_norm": 2.318605899810791,
        "learning_rate": 4.401964266188981e-05,
        "epoch": 0.8464810518174787,
        "step": 6567
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.7813713550567627,
        "learning_rate": 4.395922401327651e-05,
        "epoch": 0.8466099510183037,
        "step": 6568
    },
    {
        "loss": 0.8606,
        "grad_norm": 2.1895742416381836,
        "learning_rate": 4.389881431452677e-05,
        "epoch": 0.8467388502191286,
        "step": 6569
    },
    {
        "loss": 2.4,
        "grad_norm": 1.6922208070755005,
        "learning_rate": 4.383841365514208e-05,
        "epoch": 0.8468677494199536,
        "step": 6570
    },
    {
        "loss": 1.899,
        "grad_norm": 1.4266570806503296,
        "learning_rate": 4.377802212461053e-05,
        "epoch": 0.8469966486207785,
        "step": 6571
    },
    {
        "loss": 2.1435,
        "grad_norm": 2.750037908554077,
        "learning_rate": 4.3717639812406745e-05,
        "epoch": 0.8471255478216035,
        "step": 6572
    },
    {
        "loss": 1.9723,
        "grad_norm": 2.7895805835723877,
        "learning_rate": 4.365726680799164e-05,
        "epoch": 0.8472544470224285,
        "step": 6573
    },
    {
        "loss": 1.3337,
        "grad_norm": 3.0413432121276855,
        "learning_rate": 4.359690320081237e-05,
        "epoch": 0.8473833462232534,
        "step": 6574
    },
    {
        "loss": 1.1122,
        "grad_norm": 6.48079252243042,
        "learning_rate": 4.3536549080302145e-05,
        "epoch": 0.8475122454240783,
        "step": 6575
    },
    {
        "loss": 2.2833,
        "grad_norm": 3.193734645843506,
        "learning_rate": 4.347620453588013e-05,
        "epoch": 0.8476411446249034,
        "step": 6576
    },
    {
        "loss": 2.0189,
        "grad_norm": 3.09208083152771,
        "learning_rate": 4.341586965695131e-05,
        "epoch": 0.8477700438257283,
        "step": 6577
    },
    {
        "loss": 2.0439,
        "grad_norm": 2.4892265796661377,
        "learning_rate": 4.335554453290632e-05,
        "epoch": 0.8478989430265532,
        "step": 6578
    },
    {
        "loss": 2.242,
        "grad_norm": 1.9081416130065918,
        "learning_rate": 4.32952292531214e-05,
        "epoch": 0.8480278422273781,
        "step": 6579
    },
    {
        "loss": 1.5247,
        "grad_norm": 2.17706561088562,
        "learning_rate": 4.323492390695815e-05,
        "epoch": 0.8481567414282032,
        "step": 6580
    },
    {
        "loss": 1.9784,
        "grad_norm": 1.6960135698318481,
        "learning_rate": 4.3174628583763454e-05,
        "epoch": 0.8482856406290281,
        "step": 6581
    },
    {
        "loss": 1.8357,
        "grad_norm": 2.3160266876220703,
        "learning_rate": 4.311434337286932e-05,
        "epoch": 0.848414539829853,
        "step": 6582
    },
    {
        "loss": 1.6398,
        "grad_norm": 2.1971755027770996,
        "learning_rate": 4.305406836359302e-05,
        "epoch": 0.848543439030678,
        "step": 6583
    },
    {
        "loss": 2.1339,
        "grad_norm": 1.8367637395858765,
        "learning_rate": 4.2993803645236295e-05,
        "epoch": 0.848672338231503,
        "step": 6584
    },
    {
        "loss": 1.4128,
        "grad_norm": 2.243669271469116,
        "learning_rate": 4.293354930708593e-05,
        "epoch": 0.8488012374323279,
        "step": 6585
    },
    {
        "loss": 1.8003,
        "grad_norm": 2.1273038387298584,
        "learning_rate": 4.2873305438413244e-05,
        "epoch": 0.8489301366331529,
        "step": 6586
    },
    {
        "loss": 1.8578,
        "grad_norm": 1.364306926727295,
        "learning_rate": 4.2813072128474077e-05,
        "epoch": 0.8490590358339778,
        "step": 6587
    },
    {
        "loss": 1.9119,
        "grad_norm": 1.8082125186920166,
        "learning_rate": 4.275284946650851e-05,
        "epoch": 0.8491879350348028,
        "step": 6588
    },
    {
        "loss": 2.1806,
        "grad_norm": 2.278932571411133,
        "learning_rate": 4.2692637541741165e-05,
        "epoch": 0.8493168342356278,
        "step": 6589
    },
    {
        "loss": 2.3691,
        "grad_norm": 1.7176154851913452,
        "learning_rate": 4.263243644338031e-05,
        "epoch": 0.8494457334364527,
        "step": 6590
    },
    {
        "loss": 2.0046,
        "grad_norm": 2.6952054500579834,
        "learning_rate": 4.257224626061849e-05,
        "epoch": 0.8495746326372776,
        "step": 6591
    },
    {
        "loss": 2.3259,
        "grad_norm": 1.87967848777771,
        "learning_rate": 4.2512067082631966e-05,
        "epoch": 0.8497035318381027,
        "step": 6592
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.9889534711837769,
        "learning_rate": 4.245189899858071e-05,
        "epoch": 0.8498324310389276,
        "step": 6593
    },
    {
        "loss": 1.5363,
        "grad_norm": 2.1525163650512695,
        "learning_rate": 4.23917420976082e-05,
        "epoch": 0.8499613302397525,
        "step": 6594
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.8099589347839355,
        "learning_rate": 4.2331596468841534e-05,
        "epoch": 0.8500902294405774,
        "step": 6595
    },
    {
        "loss": 1.7589,
        "grad_norm": 3.3700027465820312,
        "learning_rate": 4.227146220139099e-05,
        "epoch": 0.8502191286414025,
        "step": 6596
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.6865530014038086,
        "learning_rate": 4.221133938434986e-05,
        "epoch": 0.8503480278422274,
        "step": 6597
    },
    {
        "loss": 2.308,
        "grad_norm": 1.7448432445526123,
        "learning_rate": 4.215122810679461e-05,
        "epoch": 0.8504769270430523,
        "step": 6598
    },
    {
        "loss": 1.9037,
        "grad_norm": 2.344916343688965,
        "learning_rate": 4.209112845778484e-05,
        "epoch": 0.8506058262438773,
        "step": 6599
    },
    {
        "loss": 2.2198,
        "grad_norm": 3.153454542160034,
        "learning_rate": 4.2031040526362395e-05,
        "epoch": 0.8507347254447022,
        "step": 6600
    },
    {
        "loss": 1.6994,
        "grad_norm": 1.7661023139953613,
        "learning_rate": 4.1970964401552296e-05,
        "epoch": 0.8508636246455272,
        "step": 6601
    },
    {
        "loss": 1.8916,
        "grad_norm": 1.670444369316101,
        "learning_rate": 4.191090017236181e-05,
        "epoch": 0.8509925238463522,
        "step": 6602
    },
    {
        "loss": 1.827,
        "grad_norm": 3.091219186782837,
        "learning_rate": 4.1850847927780474e-05,
        "epoch": 0.8511214230471771,
        "step": 6603
    },
    {
        "loss": 2.0835,
        "grad_norm": 2.3017022609710693,
        "learning_rate": 4.17908077567802e-05,
        "epoch": 0.851250322248002,
        "step": 6604
    },
    {
        "loss": 1.667,
        "grad_norm": 1.8990687131881714,
        "learning_rate": 4.173077974831524e-05,
        "epoch": 0.851379221448827,
        "step": 6605
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.2934494018554688,
        "learning_rate": 4.1670763991321316e-05,
        "epoch": 0.851508120649652,
        "step": 6606
    },
    {
        "loss": 2.065,
        "grad_norm": 3.141082763671875,
        "learning_rate": 4.1610760574716465e-05,
        "epoch": 0.8516370198504769,
        "step": 6607
    },
    {
        "loss": 1.8009,
        "grad_norm": 2.237727165222168,
        "learning_rate": 4.155076958740028e-05,
        "epoch": 0.8517659190513018,
        "step": 6608
    },
    {
        "loss": 1.8222,
        "grad_norm": 2.758769989013672,
        "learning_rate": 4.149079111825377e-05,
        "epoch": 0.8518948182521269,
        "step": 6609
    },
    {
        "loss": 0.7875,
        "grad_norm": 3.1294424533843994,
        "learning_rate": 4.1430825256139536e-05,
        "epoch": 0.8520237174529518,
        "step": 6610
    },
    {
        "loss": 1.4457,
        "grad_norm": 2.7648262977600098,
        "learning_rate": 4.137087208990167e-05,
        "epoch": 0.8521526166537767,
        "step": 6611
    },
    {
        "loss": 1.795,
        "grad_norm": 3.1522529125213623,
        "learning_rate": 4.13109317083652e-05,
        "epoch": 0.8522815158546017,
        "step": 6612
    },
    {
        "loss": 1.976,
        "grad_norm": 2.1126370429992676,
        "learning_rate": 4.125100420033632e-05,
        "epoch": 0.8524104150554267,
        "step": 6613
    },
    {
        "loss": 1.9732,
        "grad_norm": 2.323270082473755,
        "learning_rate": 4.1191089654602135e-05,
        "epoch": 0.8525393142562516,
        "step": 6614
    },
    {
        "loss": 1.2384,
        "grad_norm": 2.7595536708831787,
        "learning_rate": 4.1131188159930606e-05,
        "epoch": 0.8526682134570766,
        "step": 6615
    },
    {
        "loss": 2.5269,
        "grad_norm": 1.9944689273834229,
        "learning_rate": 4.1071299805070107e-05,
        "epoch": 0.8527971126579015,
        "step": 6616
    },
    {
        "loss": 1.4555,
        "grad_norm": 2.975893259048462,
        "learning_rate": 4.101142467874992e-05,
        "epoch": 0.8529260118587265,
        "step": 6617
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.8832799196243286,
        "learning_rate": 4.0951562869679503e-05,
        "epoch": 0.8530549110595514,
        "step": 6618
    },
    {
        "loss": 1.8407,
        "grad_norm": 1.2941802740097046,
        "learning_rate": 4.08917144665486e-05,
        "epoch": 0.8531838102603764,
        "step": 6619
    },
    {
        "loss": 1.7137,
        "grad_norm": 2.8570621013641357,
        "learning_rate": 4.083187955802713e-05,
        "epoch": 0.8533127094612013,
        "step": 6620
    },
    {
        "loss": 2.0593,
        "grad_norm": 2.9349513053894043,
        "learning_rate": 4.0772058232765075e-05,
        "epoch": 0.8534416086620263,
        "step": 6621
    },
    {
        "loss": 2.1542,
        "grad_norm": 2.969541072845459,
        "learning_rate": 4.0712250579392006e-05,
        "epoch": 0.8535705078628513,
        "step": 6622
    },
    {
        "loss": 1.7321,
        "grad_norm": 2.5716941356658936,
        "learning_rate": 4.065245668651768e-05,
        "epoch": 0.8536994070636762,
        "step": 6623
    },
    {
        "loss": 2.2272,
        "grad_norm": 1.6047347784042358,
        "learning_rate": 4.059267664273115e-05,
        "epoch": 0.8538283062645011,
        "step": 6624
    },
    {
        "loss": 2.0288,
        "grad_norm": 2.384112596511841,
        "learning_rate": 4.0532910536601064e-05,
        "epoch": 0.8539572054653262,
        "step": 6625
    },
    {
        "loss": 1.5022,
        "grad_norm": 3.0743441581726074,
        "learning_rate": 4.047315845667538e-05,
        "epoch": 0.8540861046661511,
        "step": 6626
    },
    {
        "loss": 2.0475,
        "grad_norm": 2.2482995986938477,
        "learning_rate": 4.041342049148132e-05,
        "epoch": 0.854215003866976,
        "step": 6627
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.784885048866272,
        "learning_rate": 4.035369672952516e-05,
        "epoch": 0.854343903067801,
        "step": 6628
    },
    {
        "loss": 2.2831,
        "grad_norm": 1.7710374593734741,
        "learning_rate": 4.029398725929212e-05,
        "epoch": 0.854472802268626,
        "step": 6629
    },
    {
        "loss": 2.2361,
        "grad_norm": 3.44509220123291,
        "learning_rate": 4.023429216924629e-05,
        "epoch": 0.8546017014694509,
        "step": 6630
    },
    {
        "loss": 1.4205,
        "grad_norm": 2.565279960632324,
        "learning_rate": 4.0174611547830414e-05,
        "epoch": 0.8547306006702758,
        "step": 6631
    },
    {
        "loss": 1.4383,
        "grad_norm": 2.3375673294067383,
        "learning_rate": 4.0114945483465834e-05,
        "epoch": 0.8548594998711008,
        "step": 6632
    },
    {
        "loss": 1.5566,
        "grad_norm": 2.0434677600860596,
        "learning_rate": 4.0055294064552306e-05,
        "epoch": 0.8549883990719258,
        "step": 6633
    },
    {
        "loss": 2.056,
        "grad_norm": 1.469034194946289,
        "learning_rate": 3.999565737946786e-05,
        "epoch": 0.8551172982727507,
        "step": 6634
    },
    {
        "loss": 1.5521,
        "grad_norm": 2.2513413429260254,
        "learning_rate": 3.9936035516568777e-05,
        "epoch": 0.8552461974735757,
        "step": 6635
    },
    {
        "loss": 2.0861,
        "grad_norm": 2.5068159103393555,
        "learning_rate": 3.9876428564189286e-05,
        "epoch": 0.8553750966744006,
        "step": 6636
    },
    {
        "loss": 1.9678,
        "grad_norm": 1.544313669204712,
        "learning_rate": 3.981683661064158e-05,
        "epoch": 0.8555039958752255,
        "step": 6637
    },
    {
        "loss": 1.354,
        "grad_norm": 2.2659406661987305,
        "learning_rate": 3.9757259744215623e-05,
        "epoch": 0.8556328950760506,
        "step": 6638
    },
    {
        "loss": 2.0655,
        "grad_norm": 3.106309175491333,
        "learning_rate": 3.9697698053179e-05,
        "epoch": 0.8557617942768755,
        "step": 6639
    },
    {
        "loss": 2.072,
        "grad_norm": 2.0480923652648926,
        "learning_rate": 3.963815162577685e-05,
        "epoch": 0.8558906934777004,
        "step": 6640
    },
    {
        "loss": 1.6711,
        "grad_norm": 2.193580389022827,
        "learning_rate": 3.957862055023167e-05,
        "epoch": 0.8560195926785253,
        "step": 6641
    },
    {
        "loss": 1.6642,
        "grad_norm": 1.5906389951705933,
        "learning_rate": 3.951910491474323e-05,
        "epoch": 0.8561484918793504,
        "step": 6642
    },
    {
        "loss": 2.1747,
        "grad_norm": 2.3907992839813232,
        "learning_rate": 3.9459604807488335e-05,
        "epoch": 0.8562773910801753,
        "step": 6643
    },
    {
        "loss": 2.1473,
        "grad_norm": 2.2231569290161133,
        "learning_rate": 3.940012031662109e-05,
        "epoch": 0.8564062902810002,
        "step": 6644
    },
    {
        "loss": 2.0041,
        "grad_norm": 2.726780652999878,
        "learning_rate": 3.9340651530272014e-05,
        "epoch": 0.8565351894818252,
        "step": 6645
    },
    {
        "loss": 1.7188,
        "grad_norm": 1.8904094696044922,
        "learning_rate": 3.928119853654865e-05,
        "epoch": 0.8566640886826502,
        "step": 6646
    },
    {
        "loss": 1.8549,
        "grad_norm": 2.2499303817749023,
        "learning_rate": 3.92217614235351e-05,
        "epoch": 0.8567929878834751,
        "step": 6647
    },
    {
        "loss": 1.9553,
        "grad_norm": 3.307729959487915,
        "learning_rate": 3.9162340279291874e-05,
        "epoch": 0.8569218870843001,
        "step": 6648
    },
    {
        "loss": 1.548,
        "grad_norm": 4.22144079208374,
        "learning_rate": 3.910293519185584e-05,
        "epoch": 0.857050786285125,
        "step": 6649
    },
    {
        "loss": 2.4425,
        "grad_norm": 2.2803449630737305,
        "learning_rate": 3.904354624924027e-05,
        "epoch": 0.85717968548595,
        "step": 6650
    },
    {
        "loss": 2.3634,
        "grad_norm": 2.5201303958892822,
        "learning_rate": 3.8984173539434143e-05,
        "epoch": 0.857308584686775,
        "step": 6651
    },
    {
        "loss": 0.8934,
        "grad_norm": 2.7207255363464355,
        "learning_rate": 3.892481715040268e-05,
        "epoch": 0.8574374838875999,
        "step": 6652
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.5780494213104248,
        "learning_rate": 3.88654771700868e-05,
        "epoch": 0.8575663830884248,
        "step": 6653
    },
    {
        "loss": 2.0909,
        "grad_norm": 1.4392575025558472,
        "learning_rate": 3.880615368640317e-05,
        "epoch": 0.8576952822892498,
        "step": 6654
    },
    {
        "loss": 1.996,
        "grad_norm": 2.819566249847412,
        "learning_rate": 3.8746846787243906e-05,
        "epoch": 0.8578241814900748,
        "step": 6655
    },
    {
        "loss": 1.9743,
        "grad_norm": 1.5606276988983154,
        "learning_rate": 3.868755656047685e-05,
        "epoch": 0.8579530806908997,
        "step": 6656
    },
    {
        "loss": 2.0303,
        "grad_norm": 2.2622668743133545,
        "learning_rate": 3.862828309394471e-05,
        "epoch": 0.8580819798917246,
        "step": 6657
    },
    {
        "loss": 1.2142,
        "grad_norm": 3.029886245727539,
        "learning_rate": 3.8569026475465655e-05,
        "epoch": 0.8582108790925497,
        "step": 6658
    },
    {
        "loss": 1.7676,
        "grad_norm": 2.6024692058563232,
        "learning_rate": 3.850978679283276e-05,
        "epoch": 0.8583397782933746,
        "step": 6659
    },
    {
        "loss": 1.8836,
        "grad_norm": 1.8927249908447266,
        "learning_rate": 3.845056413381427e-05,
        "epoch": 0.8584686774941995,
        "step": 6660
    },
    {
        "loss": 1.2741,
        "grad_norm": 2.913801908493042,
        "learning_rate": 3.8391358586152707e-05,
        "epoch": 0.8585975766950245,
        "step": 6661
    },
    {
        "loss": 2.4292,
        "grad_norm": 1.5461885929107666,
        "learning_rate": 3.8332170237565755e-05,
        "epoch": 0.8587264758958495,
        "step": 6662
    },
    {
        "loss": 1.98,
        "grad_norm": 2.3312599658966064,
        "learning_rate": 3.827299917574542e-05,
        "epoch": 0.8588553750966744,
        "step": 6663
    },
    {
        "loss": 1.5436,
        "grad_norm": 3.4222841262817383,
        "learning_rate": 3.8213845488357916e-05,
        "epoch": 0.8589842742974994,
        "step": 6664
    },
    {
        "loss": 1.8488,
        "grad_norm": 2.0578529834747314,
        "learning_rate": 3.815470926304389e-05,
        "epoch": 0.8591131734983243,
        "step": 6665
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.5572493076324463,
        "learning_rate": 3.80955905874183e-05,
        "epoch": 0.8592420726991493,
        "step": 6666
    },
    {
        "loss": 1.6392,
        "grad_norm": 2.812350273132324,
        "learning_rate": 3.803648954906963e-05,
        "epoch": 0.8593709718999742,
        "step": 6667
    },
    {
        "loss": 1.3362,
        "grad_norm": 3.8250465393066406,
        "learning_rate": 3.797740623556073e-05,
        "epoch": 0.8594998711007992,
        "step": 6668
    },
    {
        "loss": 1.5948,
        "grad_norm": 2.2329587936401367,
        "learning_rate": 3.791834073442796e-05,
        "epoch": 0.8596287703016241,
        "step": 6669
    },
    {
        "loss": 1.8783,
        "grad_norm": 2.5705604553222656,
        "learning_rate": 3.785929313318116e-05,
        "epoch": 0.8597576695024491,
        "step": 6670
    },
    {
        "loss": 1.5639,
        "grad_norm": 3.11275315284729,
        "learning_rate": 3.780026351930378e-05,
        "epoch": 0.8598865687032741,
        "step": 6671
    },
    {
        "loss": 2.5821,
        "grad_norm": 1.341264009475708,
        "learning_rate": 3.774125198025284e-05,
        "epoch": 0.860015467904099,
        "step": 6672
    },
    {
        "loss": 2.082,
        "grad_norm": 2.093909740447998,
        "learning_rate": 3.76822586034581e-05,
        "epoch": 0.8601443671049239,
        "step": 6673
    },
    {
        "loss": 2.233,
        "grad_norm": 2.2385449409484863,
        "learning_rate": 3.762328347632289e-05,
        "epoch": 0.8602732663057489,
        "step": 6674
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.2063345909118652,
        "learning_rate": 3.7564326686223275e-05,
        "epoch": 0.8604021655065739,
        "step": 6675
    },
    {
        "loss": 2.3866,
        "grad_norm": 1.8983187675476074,
        "learning_rate": 3.7505388320508054e-05,
        "epoch": 0.8605310647073988,
        "step": 6676
    },
    {
        "loss": 2.137,
        "grad_norm": 1.5377795696258545,
        "learning_rate": 3.744646846649883e-05,
        "epoch": 0.8606599639082237,
        "step": 6677
    },
    {
        "loss": 1.5961,
        "grad_norm": 2.7219831943511963,
        "learning_rate": 3.7387567211489935e-05,
        "epoch": 0.8607888631090487,
        "step": 6678
    },
    {
        "loss": 1.4713,
        "grad_norm": 3.2752845287323,
        "learning_rate": 3.732868464274794e-05,
        "epoch": 0.8609177623098737,
        "step": 6679
    },
    {
        "loss": 2.1952,
        "grad_norm": 1.8635077476501465,
        "learning_rate": 3.726982084751178e-05,
        "epoch": 0.8610466615106986,
        "step": 6680
    },
    {
        "loss": 2.0985,
        "grad_norm": 1.8130098581314087,
        "learning_rate": 3.721097591299262e-05,
        "epoch": 0.8611755607115236,
        "step": 6681
    },
    {
        "loss": 1.97,
        "grad_norm": 1.670114278793335,
        "learning_rate": 3.715214992637369e-05,
        "epoch": 0.8613044599123485,
        "step": 6682
    },
    {
        "loss": 2.3356,
        "grad_norm": 2.6046249866485596,
        "learning_rate": 3.709334297480992e-05,
        "epoch": 0.8614333591131735,
        "step": 6683
    },
    {
        "loss": 2.6006,
        "grad_norm": 1.9882503747940063,
        "learning_rate": 3.703455514542843e-05,
        "epoch": 0.8615622583139985,
        "step": 6684
    },
    {
        "loss": 1.8992,
        "grad_norm": 2.6508355140686035,
        "learning_rate": 3.697578652532773e-05,
        "epoch": 0.8616911575148234,
        "step": 6685
    },
    {
        "loss": 1.0076,
        "grad_norm": 2.8568994998931885,
        "learning_rate": 3.6917037201577965e-05,
        "epoch": 0.8618200567156483,
        "step": 6686
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.956780433654785,
        "learning_rate": 3.685830726122065e-05,
        "epoch": 0.8619489559164734,
        "step": 6687
    },
    {
        "loss": 2.2351,
        "grad_norm": 2.438788652420044,
        "learning_rate": 3.6799596791268684e-05,
        "epoch": 0.8620778551172983,
        "step": 6688
    },
    {
        "loss": 1.2606,
        "grad_norm": 4.202708721160889,
        "learning_rate": 3.6740905878705847e-05,
        "epoch": 0.8622067543181232,
        "step": 6689
    },
    {
        "loss": 2.1714,
        "grad_norm": 2.361097574234009,
        "learning_rate": 3.6682234610487334e-05,
        "epoch": 0.8623356535189481,
        "step": 6690
    },
    {
        "loss": 2.3211,
        "grad_norm": 2.066164970397949,
        "learning_rate": 3.662358307353896e-05,
        "epoch": 0.8624645527197732,
        "step": 6691
    },
    {
        "loss": 1.149,
        "grad_norm": 4.394233226776123,
        "learning_rate": 3.656495135475738e-05,
        "epoch": 0.8625934519205981,
        "step": 6692
    },
    {
        "loss": 1.9641,
        "grad_norm": 2.288451910018921,
        "learning_rate": 3.650633954100991e-05,
        "epoch": 0.862722351121423,
        "step": 6693
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.0402121543884277,
        "learning_rate": 3.644774771913434e-05,
        "epoch": 0.862851250322248,
        "step": 6694
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.467801094055176,
        "learning_rate": 3.638917597593887e-05,
        "epoch": 0.862980149523073,
        "step": 6695
    },
    {
        "loss": 1.3742,
        "grad_norm": 2.3922390937805176,
        "learning_rate": 3.633062439820193e-05,
        "epoch": 0.8631090487238979,
        "step": 6696
    },
    {
        "loss": 1.0191,
        "grad_norm": 1.9568493366241455,
        "learning_rate": 3.627209307267208e-05,
        "epoch": 0.8632379479247229,
        "step": 6697
    },
    {
        "loss": 0.8653,
        "grad_norm": 3.274810314178467,
        "learning_rate": 3.6213582086067884e-05,
        "epoch": 0.8633668471255478,
        "step": 6698
    },
    {
        "loss": 1.9063,
        "grad_norm": 3.46277117729187,
        "learning_rate": 3.615509152507777e-05,
        "epoch": 0.8634957463263728,
        "step": 6699
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.04268741607666,
        "learning_rate": 3.6096621476359884e-05,
        "epoch": 0.8636246455271978,
        "step": 6700
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.0312747955322266,
        "learning_rate": 3.603817202654201e-05,
        "epoch": 0.8637535447280227,
        "step": 6701
    },
    {
        "loss": 2.1407,
        "grad_norm": 2.3870325088500977,
        "learning_rate": 3.597974326222138e-05,
        "epoch": 0.8638824439288476,
        "step": 6702
    },
    {
        "loss": 2.1678,
        "grad_norm": 2.2160472869873047,
        "learning_rate": 3.592133526996463e-05,
        "epoch": 0.8640113431296726,
        "step": 6703
    },
    {
        "loss": 1.4947,
        "grad_norm": 2.872236490249634,
        "learning_rate": 3.5862948136307554e-05,
        "epoch": 0.8641402423304976,
        "step": 6704
    },
    {
        "loss": 1.5095,
        "grad_norm": 3.125988006591797,
        "learning_rate": 3.580458194775511e-05,
        "epoch": 0.8642691415313225,
        "step": 6705
    },
    {
        "loss": 1.3329,
        "grad_norm": 3.3642821311950684,
        "learning_rate": 3.574623679078118e-05,
        "epoch": 0.8643980407321474,
        "step": 6706
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.2559423446655273,
        "learning_rate": 3.568791275182849e-05,
        "epoch": 0.8645269399329724,
        "step": 6707
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.116859197616577,
        "learning_rate": 3.56296099173085e-05,
        "epoch": 0.8646558391337974,
        "step": 6708
    },
    {
        "loss": 1.8462,
        "grad_norm": 2.6892693042755127,
        "learning_rate": 3.557132837360122e-05,
        "epoch": 0.8647847383346223,
        "step": 6709
    },
    {
        "loss": 1.7748,
        "grad_norm": 2.4216861724853516,
        "learning_rate": 3.55130682070551e-05,
        "epoch": 0.8649136375354473,
        "step": 6710
    },
    {
        "loss": 2.0827,
        "grad_norm": 2.140929937362671,
        "learning_rate": 3.545482950398712e-05,
        "epoch": 0.8650425367362722,
        "step": 6711
    },
    {
        "loss": 1.5115,
        "grad_norm": 3.075052261352539,
        "learning_rate": 3.539661235068212e-05,
        "epoch": 0.8651714359370972,
        "step": 6712
    },
    {
        "loss": 1.8853,
        "grad_norm": 2.919384241104126,
        "learning_rate": 3.533841683339325e-05,
        "epoch": 0.8653003351379221,
        "step": 6713
    },
    {
        "loss": 1.8544,
        "grad_norm": 2.292541742324829,
        "learning_rate": 3.528024303834153e-05,
        "epoch": 0.8654292343387471,
        "step": 6714
    },
    {
        "loss": 1.5461,
        "grad_norm": 3.0469541549682617,
        "learning_rate": 3.5222091051715825e-05,
        "epoch": 0.865558133539572,
        "step": 6715
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.5907524824142456,
        "learning_rate": 3.516396095967258e-05,
        "epoch": 0.865687032740397,
        "step": 6716
    },
    {
        "loss": 2.1722,
        "grad_norm": 2.5590438842773438,
        "learning_rate": 3.510585284833613e-05,
        "epoch": 0.865815931941222,
        "step": 6717
    },
    {
        "loss": 1.7967,
        "grad_norm": 2.802168607711792,
        "learning_rate": 3.504776680379779e-05,
        "epoch": 0.8659448311420469,
        "step": 6718
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.8519823551177979,
        "learning_rate": 3.498970291211645e-05,
        "epoch": 0.8660737303428718,
        "step": 6719
    },
    {
        "loss": 1.4433,
        "grad_norm": 3.506575345993042,
        "learning_rate": 3.493166125931816e-05,
        "epoch": 0.8662026295436969,
        "step": 6720
    },
    {
        "loss": 1.8806,
        "grad_norm": 2.306687831878662,
        "learning_rate": 3.487364193139597e-05,
        "epoch": 0.8663315287445218,
        "step": 6721
    },
    {
        "loss": 1.9254,
        "grad_norm": 2.727839708328247,
        "learning_rate": 3.481564501430982e-05,
        "epoch": 0.8664604279453467,
        "step": 6722
    },
    {
        "loss": 1.4552,
        "grad_norm": 2.880039930343628,
        "learning_rate": 3.4757670593986685e-05,
        "epoch": 0.8665893271461717,
        "step": 6723
    },
    {
        "loss": 1.7581,
        "grad_norm": 2.6655592918395996,
        "learning_rate": 3.4699718756319823e-05,
        "epoch": 0.8667182263469967,
        "step": 6724
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.407855987548828,
        "learning_rate": 3.464178958716933e-05,
        "epoch": 0.8668471255478216,
        "step": 6725
    },
    {
        "loss": 1.491,
        "grad_norm": 2.6950485706329346,
        "learning_rate": 3.458388317236153e-05,
        "epoch": 0.8669760247486465,
        "step": 6726
    },
    {
        "loss": 2.3135,
        "grad_norm": 1.6851780414581299,
        "learning_rate": 3.452599959768934e-05,
        "epoch": 0.8671049239494715,
        "step": 6727
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.1019344329833984,
        "learning_rate": 3.446813894891135e-05,
        "epoch": 0.8672338231502965,
        "step": 6728
    },
    {
        "loss": 1.8255,
        "grad_norm": 1.7398691177368164,
        "learning_rate": 3.441030131175268e-05,
        "epoch": 0.8673627223511214,
        "step": 6729
    },
    {
        "loss": 1.835,
        "grad_norm": 2.0775458812713623,
        "learning_rate": 3.435248677190411e-05,
        "epoch": 0.8674916215519464,
        "step": 6730
    },
    {
        "loss": 2.1217,
        "grad_norm": 2.282170534133911,
        "learning_rate": 3.429469541502209e-05,
        "epoch": 0.8676205207527713,
        "step": 6731
    },
    {
        "loss": 1.7483,
        "grad_norm": 2.3102948665618896,
        "learning_rate": 3.423692732672888e-05,
        "epoch": 0.8677494199535963,
        "step": 6732
    },
    {
        "loss": 1.4249,
        "grad_norm": 2.4584169387817383,
        "learning_rate": 3.4179182592612436e-05,
        "epoch": 0.8678783191544213,
        "step": 6733
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.916215181350708,
        "learning_rate": 3.412146129822565e-05,
        "epoch": 0.8680072183552462,
        "step": 6734
    },
    {
        "loss": 1.5496,
        "grad_norm": 3.6830639839172363,
        "learning_rate": 3.406376352908715e-05,
        "epoch": 0.8681361175560711,
        "step": 6735
    },
    {
        "loss": 2.0102,
        "grad_norm": 1.5429881811141968,
        "learning_rate": 3.4006089370680526e-05,
        "epoch": 0.8682650167568962,
        "step": 6736
    },
    {
        "loss": 2.4122,
        "grad_norm": 1.4138312339782715,
        "learning_rate": 3.394843890845425e-05,
        "epoch": 0.8683939159577211,
        "step": 6737
    },
    {
        "loss": 1.9822,
        "grad_norm": 1.341582179069519,
        "learning_rate": 3.38908122278218e-05,
        "epoch": 0.868522815158546,
        "step": 6738
    },
    {
        "loss": 1.5704,
        "grad_norm": 3.1898741722106934,
        "learning_rate": 3.383320941416165e-05,
        "epoch": 0.8686517143593709,
        "step": 6739
    },
    {
        "loss": 1.9683,
        "grad_norm": 2.2830004692077637,
        "learning_rate": 3.37756305528164e-05,
        "epoch": 0.868780613560196,
        "step": 6740
    },
    {
        "loss": 1.4035,
        "grad_norm": 2.992457866668701,
        "learning_rate": 3.37180757290937e-05,
        "epoch": 0.8689095127610209,
        "step": 6741
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.3157246112823486,
        "learning_rate": 3.3660545028265256e-05,
        "epoch": 0.8690384119618458,
        "step": 6742
    },
    {
        "loss": 2.3879,
        "grad_norm": 2.0265421867370605,
        "learning_rate": 3.3603038535567176e-05,
        "epoch": 0.8691673111626708,
        "step": 6743
    },
    {
        "loss": 1.9105,
        "grad_norm": 2.0078465938568115,
        "learning_rate": 3.354555633619949e-05,
        "epoch": 0.8692962103634957,
        "step": 6744
    },
    {
        "loss": 2.1223,
        "grad_norm": 3.036073923110962,
        "learning_rate": 3.3488098515326554e-05,
        "epoch": 0.8694251095643207,
        "step": 6745
    },
    {
        "loss": 1.5581,
        "grad_norm": 3.012197256088257,
        "learning_rate": 3.3430665158076415e-05,
        "epoch": 0.8695540087651457,
        "step": 6746
    },
    {
        "loss": 1.959,
        "grad_norm": 1.862138032913208,
        "learning_rate": 3.337325634954088e-05,
        "epoch": 0.8696829079659706,
        "step": 6747
    },
    {
        "loss": 1.5577,
        "grad_norm": 3.3328170776367188,
        "learning_rate": 3.331587217477543e-05,
        "epoch": 0.8698118071667955,
        "step": 6748
    },
    {
        "loss": 1.9685,
        "grad_norm": 2.202749490737915,
        "learning_rate": 3.32585127187991e-05,
        "epoch": 0.8699407063676206,
        "step": 6749
    },
    {
        "loss": 1.6939,
        "grad_norm": 2.5602402687072754,
        "learning_rate": 3.320117806659402e-05,
        "epoch": 0.8700696055684455,
        "step": 6750
    },
    {
        "loss": 1.9136,
        "grad_norm": 1.1554051637649536,
        "learning_rate": 3.3143868303105995e-05,
        "epoch": 0.8701985047692704,
        "step": 6751
    },
    {
        "loss": 1.7395,
        "grad_norm": 3.2350399494171143,
        "learning_rate": 3.308658351324367e-05,
        "epoch": 0.8703274039700953,
        "step": 6752
    },
    {
        "loss": 1.4632,
        "grad_norm": 2.348818778991699,
        "learning_rate": 3.302932378187879e-05,
        "epoch": 0.8704563031709204,
        "step": 6753
    },
    {
        "loss": 1.0502,
        "grad_norm": 3.840092182159424,
        "learning_rate": 3.297208919384593e-05,
        "epoch": 0.8705852023717453,
        "step": 6754
    },
    {
        "loss": 1.4701,
        "grad_norm": 2.9917473793029785,
        "learning_rate": 3.291487983394249e-05,
        "epoch": 0.8707141015725702,
        "step": 6755
    },
    {
        "loss": 1.7437,
        "grad_norm": 2.818295955657959,
        "learning_rate": 3.285769578692826e-05,
        "epoch": 0.8708430007733952,
        "step": 6756
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.8275879621505737,
        "learning_rate": 3.2800537137525875e-05,
        "epoch": 0.8709718999742202,
        "step": 6757
    },
    {
        "loss": 1.7681,
        "grad_norm": 2.526230812072754,
        "learning_rate": 3.2743403970420125e-05,
        "epoch": 0.8711007991750451,
        "step": 6758
    },
    {
        "loss": 1.9092,
        "grad_norm": 2.468806505203247,
        "learning_rate": 3.268629637025809e-05,
        "epoch": 0.87122969837587,
        "step": 6759
    },
    {
        "loss": 1.8041,
        "grad_norm": 1.93470299243927,
        "learning_rate": 3.2629214421648957e-05,
        "epoch": 0.871358597576695,
        "step": 6760
    },
    {
        "loss": 1.9177,
        "grad_norm": 1.972370982170105,
        "learning_rate": 3.2572158209163936e-05,
        "epoch": 0.87148749677752,
        "step": 6761
    },
    {
        "loss": 1.3398,
        "grad_norm": 2.2060458660125732,
        "learning_rate": 3.2515127817336064e-05,
        "epoch": 0.871616395978345,
        "step": 6762
    },
    {
        "loss": 2.1484,
        "grad_norm": 1.8229079246520996,
        "learning_rate": 3.24581233306602e-05,
        "epoch": 0.8717452951791699,
        "step": 6763
    },
    {
        "loss": 1.6394,
        "grad_norm": 2.9282095432281494,
        "learning_rate": 3.240114483359275e-05,
        "epoch": 0.8718741943799948,
        "step": 6764
    },
    {
        "loss": 1.5046,
        "grad_norm": 2.7936997413635254,
        "learning_rate": 3.234419241055161e-05,
        "epoch": 0.8720030935808198,
        "step": 6765
    },
    {
        "loss": 1.8722,
        "grad_norm": 3.0621423721313477,
        "learning_rate": 3.2287266145916115e-05,
        "epoch": 0.8721319927816448,
        "step": 6766
    },
    {
        "loss": 2.1965,
        "grad_norm": 2.574073076248169,
        "learning_rate": 3.223036612402679e-05,
        "epoch": 0.8722608919824697,
        "step": 6767
    },
    {
        "loss": 1.5392,
        "grad_norm": 2.7135605812072754,
        "learning_rate": 3.21734924291853e-05,
        "epoch": 0.8723897911832946,
        "step": 6768
    },
    {
        "loss": 2.0469,
        "grad_norm": 2.975285291671753,
        "learning_rate": 3.211664514565428e-05,
        "epoch": 0.8725186903841197,
        "step": 6769
    },
    {
        "loss": 2.0745,
        "grad_norm": 3.0684993267059326,
        "learning_rate": 3.205982435765726e-05,
        "epoch": 0.8726475895849446,
        "step": 6770
    },
    {
        "loss": 1.9615,
        "grad_norm": 2.699530601501465,
        "learning_rate": 3.200303014937852e-05,
        "epoch": 0.8727764887857695,
        "step": 6771
    },
    {
        "loss": 1.8617,
        "grad_norm": 2.449291229248047,
        "learning_rate": 3.194626260496293e-05,
        "epoch": 0.8729053879865944,
        "step": 6772
    },
    {
        "loss": 1.1103,
        "grad_norm": 3.1460793018341064,
        "learning_rate": 3.188952180851589e-05,
        "epoch": 0.8730342871874195,
        "step": 6773
    },
    {
        "loss": 2.1246,
        "grad_norm": 2.896125555038452,
        "learning_rate": 3.1832807844103125e-05,
        "epoch": 0.8731631863882444,
        "step": 6774
    },
    {
        "loss": 1.7811,
        "grad_norm": 2.0582759380340576,
        "learning_rate": 3.177612079575066e-05,
        "epoch": 0.8732920855890693,
        "step": 6775
    },
    {
        "loss": 2.4082,
        "grad_norm": 1.786295771598816,
        "learning_rate": 3.171946074744462e-05,
        "epoch": 0.8734209847898943,
        "step": 6776
    },
    {
        "loss": 1.975,
        "grad_norm": 1.9143787622451782,
        "learning_rate": 3.166282778313104e-05,
        "epoch": 0.8735498839907193,
        "step": 6777
    },
    {
        "loss": 1.165,
        "grad_norm": 2.0839903354644775,
        "learning_rate": 3.1606221986716114e-05,
        "epoch": 0.8736787831915442,
        "step": 6778
    },
    {
        "loss": 2.1279,
        "grad_norm": 3.1272873878479004,
        "learning_rate": 3.154964344206539e-05,
        "epoch": 0.8738076823923692,
        "step": 6779
    },
    {
        "loss": 1.317,
        "grad_norm": 2.7272651195526123,
        "learning_rate": 3.14930922330043e-05,
        "epoch": 0.8739365815931941,
        "step": 6780
    },
    {
        "loss": 1.5779,
        "grad_norm": 2.225395441055298,
        "learning_rate": 3.143656844331771e-05,
        "epoch": 0.874065480794019,
        "step": 6781
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.6400814056396484,
        "learning_rate": 3.138007215674985e-05,
        "epoch": 0.8741943799948441,
        "step": 6782
    },
    {
        "loss": 1.647,
        "grad_norm": 2.215362071990967,
        "learning_rate": 3.1323603457004166e-05,
        "epoch": 0.874323279195669,
        "step": 6783
    },
    {
        "loss": 1.3976,
        "grad_norm": 2.5157699584960938,
        "learning_rate": 3.126716242774346e-05,
        "epoch": 0.8744521783964939,
        "step": 6784
    },
    {
        "loss": 2.0968,
        "grad_norm": 2.5860679149627686,
        "learning_rate": 3.121074915258918e-05,
        "epoch": 0.8745810775973188,
        "step": 6785
    },
    {
        "loss": 2.1179,
        "grad_norm": 1.464295744895935,
        "learning_rate": 3.1154363715121884e-05,
        "epoch": 0.8747099767981439,
        "step": 6786
    },
    {
        "loss": 2.1435,
        "grad_norm": 1.8672538995742798,
        "learning_rate": 3.1098006198880825e-05,
        "epoch": 0.8748388759989688,
        "step": 6787
    },
    {
        "loss": 1.0658,
        "grad_norm": 2.627441883087158,
        "learning_rate": 3.104167668736392e-05,
        "epoch": 0.8749677751997937,
        "step": 6788
    },
    {
        "loss": 2.0359,
        "grad_norm": 1.7797642946243286,
        "learning_rate": 3.09853752640275e-05,
        "epoch": 0.8750966744006187,
        "step": 6789
    },
    {
        "loss": 1.2027,
        "grad_norm": 3.1113202571868896,
        "learning_rate": 3.0929102012286545e-05,
        "epoch": 0.8752255736014437,
        "step": 6790
    },
    {
        "loss": 2.0297,
        "grad_norm": 1.5866711139678955,
        "learning_rate": 3.087285701551392e-05,
        "epoch": 0.8753544728022686,
        "step": 6791
    },
    {
        "loss": 1.5856,
        "grad_norm": 2.477656126022339,
        "learning_rate": 3.081664035704086e-05,
        "epoch": 0.8754833720030936,
        "step": 6792
    },
    {
        "loss": 2.1343,
        "grad_norm": 2.2907533645629883,
        "learning_rate": 3.0760452120156516e-05,
        "epoch": 0.8756122712039185,
        "step": 6793
    },
    {
        "loss": 1.9131,
        "grad_norm": 1.9704153537750244,
        "learning_rate": 3.0704292388108194e-05,
        "epoch": 0.8757411704047435,
        "step": 6794
    },
    {
        "loss": 0.7878,
        "grad_norm": 2.0906519889831543,
        "learning_rate": 3.064816124410048e-05,
        "epoch": 0.8758700696055685,
        "step": 6795
    },
    {
        "loss": 1.2399,
        "grad_norm": 3.214454412460327,
        "learning_rate": 3.059205877129606e-05,
        "epoch": 0.8759989688063934,
        "step": 6796
    },
    {
        "loss": 1.5883,
        "grad_norm": 3.103325366973877,
        "learning_rate": 3.053598505281499e-05,
        "epoch": 0.8761278680072183,
        "step": 6797
    },
    {
        "loss": 1.4642,
        "grad_norm": 4.046505451202393,
        "learning_rate": 3.0479940171734567e-05,
        "epoch": 0.8762567672080434,
        "step": 6798
    },
    {
        "loss": 1.8827,
        "grad_norm": 2.3342320919036865,
        "learning_rate": 3.042392421108947e-05,
        "epoch": 0.8763856664088683,
        "step": 6799
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.3637943267822266,
        "learning_rate": 3.0367937253871782e-05,
        "epoch": 0.8765145656096932,
        "step": 6800
    },
    {
        "loss": 2.2533,
        "grad_norm": 1.8805773258209229,
        "learning_rate": 3.031197938303009e-05,
        "epoch": 0.8766434648105181,
        "step": 6801
    },
    {
        "loss": 1.1417,
        "grad_norm": 2.4893343448638916,
        "learning_rate": 3.0256050681470417e-05,
        "epoch": 0.8767723640113432,
        "step": 6802
    },
    {
        "loss": 1.9941,
        "grad_norm": 3.053840160369873,
        "learning_rate": 3.0200151232055306e-05,
        "epoch": 0.8769012632121681,
        "step": 6803
    },
    {
        "loss": 1.0203,
        "grad_norm": 2.724332332611084,
        "learning_rate": 3.0144281117603874e-05,
        "epoch": 0.877030162412993,
        "step": 6804
    },
    {
        "loss": 1.8057,
        "grad_norm": 1.7156803607940674,
        "learning_rate": 3.0088440420891906e-05,
        "epoch": 0.877159061613818,
        "step": 6805
    },
    {
        "loss": 1.8046,
        "grad_norm": 2.8555049896240234,
        "learning_rate": 3.0032629224651752e-05,
        "epoch": 0.877287960814643,
        "step": 6806
    },
    {
        "loss": 1.8789,
        "grad_norm": 3.383758544921875,
        "learning_rate": 2.9976847611571672e-05,
        "epoch": 0.8774168600154679,
        "step": 6807
    },
    {
        "loss": 1.1818,
        "grad_norm": 5.968928813934326,
        "learning_rate": 2.9921095664296482e-05,
        "epoch": 0.8775457592162929,
        "step": 6808
    },
    {
        "loss": 0.9057,
        "grad_norm": 2.1822540760040283,
        "learning_rate": 2.9865373465426837e-05,
        "epoch": 0.8776746584171178,
        "step": 6809
    },
    {
        "loss": 1.4904,
        "grad_norm": 2.464536428451538,
        "learning_rate": 2.9809681097519416e-05,
        "epoch": 0.8778035576179428,
        "step": 6810
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.637202024459839,
        "learning_rate": 2.9754018643086458e-05,
        "epoch": 0.8779324568187677,
        "step": 6811
    },
    {
        "loss": 2.2492,
        "grad_norm": 2.705733299255371,
        "learning_rate": 2.969838618459626e-05,
        "epoch": 0.8780613560195927,
        "step": 6812
    },
    {
        "loss": 2.1245,
        "grad_norm": 2.3896725177764893,
        "learning_rate": 2.964278380447242e-05,
        "epoch": 0.8781902552204176,
        "step": 6813
    },
    {
        "loss": 1.7387,
        "grad_norm": 3.1185991764068604,
        "learning_rate": 2.9587211585094053e-05,
        "epoch": 0.8783191544212426,
        "step": 6814
    },
    {
        "loss": 1.6738,
        "grad_norm": 1.9439029693603516,
        "learning_rate": 2.9531669608795575e-05,
        "epoch": 0.8784480536220676,
        "step": 6815
    },
    {
        "loss": 1.8883,
        "grad_norm": 1.8612136840820312,
        "learning_rate": 2.9476157957866663e-05,
        "epoch": 0.8785769528228925,
        "step": 6816
    },
    {
        "loss": 1.9736,
        "grad_norm": 2.4808461666107178,
        "learning_rate": 2.942067671455181e-05,
        "epoch": 0.8787058520237174,
        "step": 6817
    },
    {
        "loss": 2.3228,
        "grad_norm": 1.71711266040802,
        "learning_rate": 2.9365225961050825e-05,
        "epoch": 0.8788347512245424,
        "step": 6818
    },
    {
        "loss": 1.7598,
        "grad_norm": 3.9670658111572266,
        "learning_rate": 2.9309805779518118e-05,
        "epoch": 0.8789636504253674,
        "step": 6819
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.0407278537750244,
        "learning_rate": 2.9254416252062832e-05,
        "epoch": 0.8790925496261923,
        "step": 6820
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.6149816513061523,
        "learning_rate": 2.9199057460748747e-05,
        "epoch": 0.8792214488270172,
        "step": 6821
    },
    {
        "loss": 2.0021,
        "grad_norm": 2.2910170555114746,
        "learning_rate": 2.9143729487594102e-05,
        "epoch": 0.8793503480278422,
        "step": 6822
    },
    {
        "loss": 2.0494,
        "grad_norm": 2.260099172592163,
        "learning_rate": 2.9088432414571272e-05,
        "epoch": 0.8794792472286672,
        "step": 6823
    },
    {
        "loss": 1.7084,
        "grad_norm": 2.450334072113037,
        "learning_rate": 2.903316632360721e-05,
        "epoch": 0.8796081464294921,
        "step": 6824
    },
    {
        "loss": 1.7547,
        "grad_norm": 2.785038709640503,
        "learning_rate": 2.897793129658274e-05,
        "epoch": 0.8797370456303171,
        "step": 6825
    },
    {
        "loss": 2.0619,
        "grad_norm": 1.1097595691680908,
        "learning_rate": 2.892272741533267e-05,
        "epoch": 0.879865944831142,
        "step": 6826
    },
    {
        "loss": 1.4984,
        "grad_norm": 3.0469648838043213,
        "learning_rate": 2.8867554761645716e-05,
        "epoch": 0.879994844031967,
        "step": 6827
    },
    {
        "loss": 2.0495,
        "grad_norm": 2.118607997894287,
        "learning_rate": 2.8812413417264328e-05,
        "epoch": 0.880123743232792,
        "step": 6828
    },
    {
        "loss": 1.9726,
        "grad_norm": 2.144914150238037,
        "learning_rate": 2.875730346388452e-05,
        "epoch": 0.8802526424336169,
        "step": 6829
    },
    {
        "loss": 1.9045,
        "grad_norm": 3.1027064323425293,
        "learning_rate": 2.8702224983155834e-05,
        "epoch": 0.8803815416344418,
        "step": 6830
    },
    {
        "loss": 2.4153,
        "grad_norm": 1.449937105178833,
        "learning_rate": 2.8647178056681194e-05,
        "epoch": 0.8805104408352669,
        "step": 6831
    },
    {
        "loss": 1.7289,
        "grad_norm": 2.970000982284546,
        "learning_rate": 2.8592162766016773e-05,
        "epoch": 0.8806393400360918,
        "step": 6832
    },
    {
        "loss": 1.6163,
        "grad_norm": 1.487969994544983,
        "learning_rate": 2.853717919267181e-05,
        "epoch": 0.8807682392369167,
        "step": 6833
    },
    {
        "loss": 1.9699,
        "grad_norm": 2.7358663082122803,
        "learning_rate": 2.8482227418108626e-05,
        "epoch": 0.8808971384377416,
        "step": 6834
    },
    {
        "loss": 2.0458,
        "grad_norm": 1.9593112468719482,
        "learning_rate": 2.8427307523742424e-05,
        "epoch": 0.8810260376385667,
        "step": 6835
    },
    {
        "loss": 1.4563,
        "grad_norm": 3.193533182144165,
        "learning_rate": 2.8372419590941113e-05,
        "epoch": 0.8811549368393916,
        "step": 6836
    },
    {
        "loss": 1.0839,
        "grad_norm": 2.3497915267944336,
        "learning_rate": 2.8317563701025318e-05,
        "epoch": 0.8812838360402165,
        "step": 6837
    },
    {
        "loss": 0.7239,
        "grad_norm": 3.763275623321533,
        "learning_rate": 2.826273993526817e-05,
        "epoch": 0.8814127352410415,
        "step": 6838
    },
    {
        "loss": 1.8349,
        "grad_norm": 2.2325327396392822,
        "learning_rate": 2.8207948374895166e-05,
        "epoch": 0.8815416344418665,
        "step": 6839
    },
    {
        "loss": 2.3558,
        "grad_norm": 1.7638241052627563,
        "learning_rate": 2.8153189101084133e-05,
        "epoch": 0.8816705336426914,
        "step": 6840
    },
    {
        "loss": 1.8677,
        "grad_norm": 2.129596710205078,
        "learning_rate": 2.809846219496505e-05,
        "epoch": 0.8817994328435164,
        "step": 6841
    },
    {
        "loss": 1.9034,
        "grad_norm": 2.0272789001464844,
        "learning_rate": 2.8043767737619974e-05,
        "epoch": 0.8819283320443413,
        "step": 6842
    },
    {
        "loss": 2.4209,
        "grad_norm": 1.6943014860153198,
        "learning_rate": 2.7989105810082795e-05,
        "epoch": 0.8820572312451663,
        "step": 6843
    },
    {
        "loss": 1.4949,
        "grad_norm": 3.6774799823760986,
        "learning_rate": 2.7934476493339235e-05,
        "epoch": 0.8821861304459913,
        "step": 6844
    },
    {
        "loss": 1.8933,
        "grad_norm": 1.535537838935852,
        "learning_rate": 2.7879879868326935e-05,
        "epoch": 0.8823150296468162,
        "step": 6845
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.3494882583618164,
        "learning_rate": 2.782531601593467e-05,
        "epoch": 0.8824439288476411,
        "step": 6846
    },
    {
        "loss": 2.5943,
        "grad_norm": 2.4458799362182617,
        "learning_rate": 2.7770785017002978e-05,
        "epoch": 0.8825728280484662,
        "step": 6847
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.489473581314087,
        "learning_rate": 2.7716286952323618e-05,
        "epoch": 0.8827017272492911,
        "step": 6848
    },
    {
        "loss": 2.5181,
        "grad_norm": 1.7368125915527344,
        "learning_rate": 2.7661821902639594e-05,
        "epoch": 0.882830626450116,
        "step": 6849
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.3981080055236816,
        "learning_rate": 2.760738994864487e-05,
        "epoch": 0.8829595256509409,
        "step": 6850
    },
    {
        "loss": 1.0534,
        "grad_norm": 3.1198747158050537,
        "learning_rate": 2.7552991170984677e-05,
        "epoch": 0.883088424851766,
        "step": 6851
    },
    {
        "loss": 1.7445,
        "grad_norm": 2.6791341304779053,
        "learning_rate": 2.7498625650254735e-05,
        "epoch": 0.8832173240525909,
        "step": 6852
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.9378283023834229,
        "learning_rate": 2.744429346700166e-05,
        "epoch": 0.8833462232534158,
        "step": 6853
    },
    {
        "loss": 2.3883,
        "grad_norm": 2.052767276763916,
        "learning_rate": 2.7389994701722687e-05,
        "epoch": 0.8834751224542408,
        "step": 6854
    },
    {
        "loss": 2.3976,
        "grad_norm": 2.6940009593963623,
        "learning_rate": 2.7335729434865535e-05,
        "epoch": 0.8836040216550657,
        "step": 6855
    },
    {
        "loss": 2.0596,
        "grad_norm": 2.6821281909942627,
        "learning_rate": 2.7281497746828182e-05,
        "epoch": 0.8837329208558907,
        "step": 6856
    },
    {
        "loss": 1.9408,
        "grad_norm": 3.1175143718719482,
        "learning_rate": 2.7227299717959143e-05,
        "epoch": 0.8838618200567157,
        "step": 6857
    },
    {
        "loss": 2.1965,
        "grad_norm": 1.4814947843551636,
        "learning_rate": 2.7173135428556694e-05,
        "epoch": 0.8839907192575406,
        "step": 6858
    },
    {
        "loss": 2.4808,
        "grad_norm": 1.346221923828125,
        "learning_rate": 2.7119004958869377e-05,
        "epoch": 0.8841196184583655,
        "step": 6859
    },
    {
        "loss": 1.7913,
        "grad_norm": 3.3252170085906982,
        "learning_rate": 2.7064908389095444e-05,
        "epoch": 0.8842485176591905,
        "step": 6860
    },
    {
        "loss": 1.3604,
        "grad_norm": 3.7004342079162598,
        "learning_rate": 2.7010845799383233e-05,
        "epoch": 0.8843774168600155,
        "step": 6861
    },
    {
        "loss": 2.2196,
        "grad_norm": 2.5727217197418213,
        "learning_rate": 2.6956817269830303e-05,
        "epoch": 0.8845063160608404,
        "step": 6862
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.8840605020523071,
        "learning_rate": 2.690282288048417e-05,
        "epoch": 0.8846352152616653,
        "step": 6863
    },
    {
        "loss": 1.5052,
        "grad_norm": 3.009767770767212,
        "learning_rate": 2.6848862711341538e-05,
        "epoch": 0.8847641144624904,
        "step": 6864
    },
    {
        "loss": 1.5719,
        "grad_norm": 3.4048819541931152,
        "learning_rate": 2.6794936842348372e-05,
        "epoch": 0.8848930136633153,
        "step": 6865
    },
    {
        "loss": 2.099,
        "grad_norm": 2.1444668769836426,
        "learning_rate": 2.674104535339987e-05,
        "epoch": 0.8850219128641402,
        "step": 6866
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.131169319152832,
        "learning_rate": 2.6687188324340508e-05,
        "epoch": 0.8851508120649652,
        "step": 6867
    },
    {
        "loss": 1.8912,
        "grad_norm": 2.590424060821533,
        "learning_rate": 2.6633365834963293e-05,
        "epoch": 0.8852797112657902,
        "step": 6868
    },
    {
        "loss": 1.0687,
        "grad_norm": 3.5275423526763916,
        "learning_rate": 2.6579577965010484e-05,
        "epoch": 0.8854086104666151,
        "step": 6869
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.6634219884872437,
        "learning_rate": 2.6525824794172797e-05,
        "epoch": 0.88553750966744,
        "step": 6870
    },
    {
        "loss": 1.7688,
        "grad_norm": 2.8847501277923584,
        "learning_rate": 2.6472106402089518e-05,
        "epoch": 0.885666408868265,
        "step": 6871
    },
    {
        "loss": 1.9379,
        "grad_norm": 3.3662428855895996,
        "learning_rate": 2.6418422868348468e-05,
        "epoch": 0.88579530806909,
        "step": 6872
    },
    {
        "loss": 2.3677,
        "grad_norm": 1.845774531364441,
        "learning_rate": 2.6364774272486038e-05,
        "epoch": 0.8859242072699149,
        "step": 6873
    },
    {
        "loss": 2.3584,
        "grad_norm": 2.5616703033447266,
        "learning_rate": 2.631116069398638e-05,
        "epoch": 0.8860531064707399,
        "step": 6874
    },
    {
        "loss": 1.4709,
        "grad_norm": 2.474653959274292,
        "learning_rate": 2.6257582212282227e-05,
        "epoch": 0.8861820056715648,
        "step": 6875
    },
    {
        "loss": 2.3664,
        "grad_norm": 2.1676158905029297,
        "learning_rate": 2.6204038906754092e-05,
        "epoch": 0.8863109048723898,
        "step": 6876
    },
    {
        "loss": 1.9736,
        "grad_norm": 2.095033884048462,
        "learning_rate": 2.6150530856730426e-05,
        "epoch": 0.8864398040732148,
        "step": 6877
    },
    {
        "loss": 2.1616,
        "grad_norm": 1.9792808294296265,
        "learning_rate": 2.609705814148727e-05,
        "epoch": 0.8865687032740397,
        "step": 6878
    },
    {
        "loss": 2.2731,
        "grad_norm": 2.019935131072998,
        "learning_rate": 2.604362084024866e-05,
        "epoch": 0.8866976024748646,
        "step": 6879
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.4726793766021729,
        "learning_rate": 2.599021903218588e-05,
        "epoch": 0.8868265016756897,
        "step": 6880
    },
    {
        "loss": 1.756,
        "grad_norm": 1.8373723030090332,
        "learning_rate": 2.5936852796417742e-05,
        "epoch": 0.8869554008765146,
        "step": 6881
    },
    {
        "loss": 1.127,
        "grad_norm": 3.9068238735198975,
        "learning_rate": 2.5883522212010325e-05,
        "epoch": 0.8870843000773395,
        "step": 6882
    },
    {
        "loss": 2.249,
        "grad_norm": 2.0106639862060547,
        "learning_rate": 2.5830227357976987e-05,
        "epoch": 0.8872131992781644,
        "step": 6883
    },
    {
        "loss": 1.8179,
        "grad_norm": 2.479058027267456,
        "learning_rate": 2.5776968313277842e-05,
        "epoch": 0.8873420984789895,
        "step": 6884
    },
    {
        "loss": 1.4245,
        "grad_norm": 3.7415459156036377,
        "learning_rate": 2.5723745156820356e-05,
        "epoch": 0.8874709976798144,
        "step": 6885
    },
    {
        "loss": 1.7021,
        "grad_norm": 1.853050708770752,
        "learning_rate": 2.5670557967458603e-05,
        "epoch": 0.8875998968806393,
        "step": 6886
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.377256155014038,
        "learning_rate": 2.561740682399335e-05,
        "epoch": 0.8877287960814643,
        "step": 6887
    },
    {
        "loss": 1.7743,
        "grad_norm": 1.819831132888794,
        "learning_rate": 2.5564291805172048e-05,
        "epoch": 0.8878576952822892,
        "step": 6888
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.308183193206787,
        "learning_rate": 2.5511212989688647e-05,
        "epoch": 0.8879865944831142,
        "step": 6889
    },
    {
        "loss": 1.7874,
        "grad_norm": 5.262726783752441,
        "learning_rate": 2.5458170456183195e-05,
        "epoch": 0.8881154936839392,
        "step": 6890
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.80387544631958,
        "learning_rate": 2.540516428324239e-05,
        "epoch": 0.8882443928847641,
        "step": 6891
    },
    {
        "loss": 2.0484,
        "grad_norm": 1.8329190015792847,
        "learning_rate": 2.5352194549398788e-05,
        "epoch": 0.888373292085589,
        "step": 6892
    },
    {
        "loss": 2.4795,
        "grad_norm": 3.038698434829712,
        "learning_rate": 2.5299261333131062e-05,
        "epoch": 0.8885021912864141,
        "step": 6893
    },
    {
        "loss": 2.2394,
        "grad_norm": 3.0238778591156006,
        "learning_rate": 2.5246364712863712e-05,
        "epoch": 0.888631090487239,
        "step": 6894
    },
    {
        "loss": 1.6175,
        "grad_norm": 2.3927810192108154,
        "learning_rate": 2.519350476696707e-05,
        "epoch": 0.8887599896880639,
        "step": 6895
    },
    {
        "loss": 1.8833,
        "grad_norm": 2.5075764656066895,
        "learning_rate": 2.5140681573757136e-05,
        "epoch": 0.8888888888888888,
        "step": 6896
    },
    {
        "loss": 2.1051,
        "grad_norm": 2.222353935241699,
        "learning_rate": 2.5087895211495406e-05,
        "epoch": 0.8890177880897139,
        "step": 6897
    },
    {
        "loss": 2.0102,
        "grad_norm": Infinity,
        "learning_rate": 2.5087895211495406e-05,
        "epoch": 0.8891466872905388,
        "step": 6898
    },
    {
        "loss": 1.5859,
        "grad_norm": 6.253272533416748,
        "learning_rate": 2.5035145758388845e-05,
        "epoch": 0.8892755864913637,
        "step": 6899
    },
    {
        "loss": 1.5269,
        "grad_norm": 3.767005443572998,
        "learning_rate": 2.4982433292589758e-05,
        "epoch": 0.8894044856921887,
        "step": 6900
    },
    {
        "loss": 1.852,
        "grad_norm": 3.2938411235809326,
        "learning_rate": 2.4929757892195628e-05,
        "epoch": 0.8895333848930137,
        "step": 6901
    },
    {
        "loss": 2.2652,
        "grad_norm": 2.672133207321167,
        "learning_rate": 2.4877119635248976e-05,
        "epoch": 0.8896622840938386,
        "step": 6902
    },
    {
        "loss": 2.1322,
        "grad_norm": 2.5119760036468506,
        "learning_rate": 2.4824518599737367e-05,
        "epoch": 0.8897911832946636,
        "step": 6903
    },
    {
        "loss": 2.3604,
        "grad_norm": 2.5071427822113037,
        "learning_rate": 2.4771954863593194e-05,
        "epoch": 0.8899200824954885,
        "step": 6904
    },
    {
        "loss": 2.2222,
        "grad_norm": 2.0089807510375977,
        "learning_rate": 2.4719428504693552e-05,
        "epoch": 0.8900489816963135,
        "step": 6905
    },
    {
        "loss": 1.671,
        "grad_norm": 3.339463472366333,
        "learning_rate": 2.4666939600860205e-05,
        "epoch": 0.8901778808971385,
        "step": 6906
    },
    {
        "loss": 2.323,
        "grad_norm": 2.094561815261841,
        "learning_rate": 2.4614488229859433e-05,
        "epoch": 0.8903067800979634,
        "step": 6907
    },
    {
        "loss": 1.5519,
        "grad_norm": 2.8505144119262695,
        "learning_rate": 2.4562074469401836e-05,
        "epoch": 0.8904356792987883,
        "step": 6908
    },
    {
        "loss": 1.7553,
        "grad_norm": 2.8571243286132812,
        "learning_rate": 2.4509698397142363e-05,
        "epoch": 0.8905645784996133,
        "step": 6909
    },
    {
        "loss": 0.8129,
        "grad_norm": 3.0406010150909424,
        "learning_rate": 2.44573600906801e-05,
        "epoch": 0.8906934777004383,
        "step": 6910
    },
    {
        "loss": 1.5741,
        "grad_norm": 3.8154549598693848,
        "learning_rate": 2.4405059627558195e-05,
        "epoch": 0.8908223769012632,
        "step": 6911
    },
    {
        "loss": 1.1475,
        "grad_norm": 3.3291685581207275,
        "learning_rate": 2.4352797085263624e-05,
        "epoch": 0.8909512761020881,
        "step": 6912
    },
    {
        "loss": 1.6708,
        "grad_norm": 2.9567410945892334,
        "learning_rate": 2.4300572541227462e-05,
        "epoch": 0.8910801753029132,
        "step": 6913
    },
    {
        "loss": 1.983,
        "grad_norm": 2.432255506515503,
        "learning_rate": 2.424838607282414e-05,
        "epoch": 0.8912090745037381,
        "step": 6914
    },
    {
        "loss": 1.6662,
        "grad_norm": 1.84346604347229,
        "learning_rate": 2.4196237757371852e-05,
        "epoch": 0.891337973704563,
        "step": 6915
    },
    {
        "loss": 2.035,
        "grad_norm": 2.452460289001465,
        "learning_rate": 2.4144127672132267e-05,
        "epoch": 0.891466872905388,
        "step": 6916
    },
    {
        "loss": 1.5368,
        "grad_norm": 2.3146495819091797,
        "learning_rate": 2.409205589431039e-05,
        "epoch": 0.891595772106213,
        "step": 6917
    },
    {
        "loss": 1.6584,
        "grad_norm": 3.1246330738067627,
        "learning_rate": 2.4040022501054432e-05,
        "epoch": 0.8917246713070379,
        "step": 6918
    },
    {
        "loss": 0.8616,
        "grad_norm": 1.7803778648376465,
        "learning_rate": 2.3988027569455912e-05,
        "epoch": 0.8918535705078628,
        "step": 6919
    },
    {
        "loss": 1.303,
        "grad_norm": 3.0183255672454834,
        "learning_rate": 2.3936071176549073e-05,
        "epoch": 0.8919824697086878,
        "step": 6920
    },
    {
        "loss": 2.4806,
        "grad_norm": 1.9239987134933472,
        "learning_rate": 2.38841533993113e-05,
        "epoch": 0.8921113689095128,
        "step": 6921
    },
    {
        "loss": 1.8956,
        "grad_norm": 2.2537448406219482,
        "learning_rate": 2.3832274314662627e-05,
        "epoch": 0.8922402681103377,
        "step": 6922
    },
    {
        "loss": 1.949,
        "grad_norm": 1.5119903087615967,
        "learning_rate": 2.3780433999465844e-05,
        "epoch": 0.8923691673111627,
        "step": 6923
    },
    {
        "loss": 1.7538,
        "grad_norm": 2.0319368839263916,
        "learning_rate": 2.3728632530526224e-05,
        "epoch": 0.8924980665119876,
        "step": 6924
    },
    {
        "loss": 2.232,
        "grad_norm": 2.529419183731079,
        "learning_rate": 2.367686998459166e-05,
        "epoch": 0.8926269657128125,
        "step": 6925
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.927894949913025,
        "learning_rate": 2.3625146438352213e-05,
        "epoch": 0.8927558649136376,
        "step": 6926
    },
    {
        "loss": 1.9993,
        "grad_norm": 2.209052085876465,
        "learning_rate": 2.3573461968440115e-05,
        "epoch": 0.8928847641144625,
        "step": 6927
    },
    {
        "loss": 1.3611,
        "grad_norm": 2.6209566593170166,
        "learning_rate": 2.3521816651429807e-05,
        "epoch": 0.8930136633152874,
        "step": 6928
    },
    {
        "loss": 1.7462,
        "grad_norm": 3.695802927017212,
        "learning_rate": 2.347021056383784e-05,
        "epoch": 0.8931425625161123,
        "step": 6929
    },
    {
        "loss": 1.5505,
        "grad_norm": 2.5655131340026855,
        "learning_rate": 2.3418643782122303e-05,
        "epoch": 0.8932714617169374,
        "step": 6930
    },
    {
        "loss": 2.3351,
        "grad_norm": 1.628190040588379,
        "learning_rate": 2.3367116382683423e-05,
        "epoch": 0.8934003609177623,
        "step": 6931
    },
    {
        "loss": 1.9505,
        "grad_norm": 2.2601728439331055,
        "learning_rate": 2.3315628441862947e-05,
        "epoch": 0.8935292601185872,
        "step": 6932
    },
    {
        "loss": 1.648,
        "grad_norm": 2.086331605911255,
        "learning_rate": 2.326418003594396e-05,
        "epoch": 0.8936581593194122,
        "step": 6933
    },
    {
        "loss": 1.6062,
        "grad_norm": 2.427253484725952,
        "learning_rate": 2.3212771241151178e-05,
        "epoch": 0.8937870585202372,
        "step": 6934
    },
    {
        "loss": 1.8679,
        "grad_norm": 2.0138654708862305,
        "learning_rate": 2.316140213365076e-05,
        "epoch": 0.8939159577210621,
        "step": 6935
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.2580366134643555,
        "learning_rate": 2.3110072789549654e-05,
        "epoch": 0.8940448569218871,
        "step": 6936
    },
    {
        "loss": 0.9263,
        "grad_norm": 5.163296699523926,
        "learning_rate": 2.3058783284896324e-05,
        "epoch": 0.894173756122712,
        "step": 6937
    },
    {
        "loss": 1.5084,
        "grad_norm": 2.904604434967041,
        "learning_rate": 2.3007533695680035e-05,
        "epoch": 0.894302655323537,
        "step": 6938
    },
    {
        "loss": 1.5442,
        "grad_norm": 2.0410799980163574,
        "learning_rate": 2.295632409783077e-05,
        "epoch": 0.894431554524362,
        "step": 6939
    },
    {
        "loss": 1.7315,
        "grad_norm": 3.063767194747925,
        "learning_rate": 2.2905154567219394e-05,
        "epoch": 0.8945604537251869,
        "step": 6940
    },
    {
        "loss": 2.1593,
        "grad_norm": 2.4628570079803467,
        "learning_rate": 2.2854025179657616e-05,
        "epoch": 0.8946893529260118,
        "step": 6941
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.7980597019195557,
        "learning_rate": 2.28029360108972e-05,
        "epoch": 0.8948182521268369,
        "step": 6942
    },
    {
        "loss": 1.4212,
        "grad_norm": 3.084078788757324,
        "learning_rate": 2.2751887136630774e-05,
        "epoch": 0.8949471513276618,
        "step": 6943
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.5900955200195312,
        "learning_rate": 2.270087863249102e-05,
        "epoch": 0.8950760505284867,
        "step": 6944
    },
    {
        "loss": 1.9869,
        "grad_norm": 2.4994254112243652,
        "learning_rate": 2.2649910574050954e-05,
        "epoch": 0.8952049497293116,
        "step": 6945
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.2183539867401123,
        "learning_rate": 2.2598983036823367e-05,
        "epoch": 0.8953338489301367,
        "step": 6946
    },
    {
        "loss": 1.1753,
        "grad_norm": 2.4469919204711914,
        "learning_rate": 2.2548096096261433e-05,
        "epoch": 0.8954627481309616,
        "step": 6947
    },
    {
        "loss": 1.1527,
        "grad_norm": 3.77358341217041,
        "learning_rate": 2.2497249827757916e-05,
        "epoch": 0.8955916473317865,
        "step": 6948
    },
    {
        "loss": 1.4455,
        "grad_norm": 3.4049153327941895,
        "learning_rate": 2.2446444306645398e-05,
        "epoch": 0.8957205465326115,
        "step": 6949
    },
    {
        "loss": 1.0164,
        "grad_norm": 2.6972055435180664,
        "learning_rate": 2.2395679608196042e-05,
        "epoch": 0.8958494457334365,
        "step": 6950
    },
    {
        "loss": 1.4906,
        "grad_norm": 3.526545524597168,
        "learning_rate": 2.234495580762166e-05,
        "epoch": 0.8959783449342614,
        "step": 6951
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.150542974472046,
        "learning_rate": 2.229427298007316e-05,
        "epoch": 0.8961072441350864,
        "step": 6952
    },
    {
        "loss": 2.1394,
        "grad_norm": 2.8030242919921875,
        "learning_rate": 2.2243631200641156e-05,
        "epoch": 0.8962361433359113,
        "step": 6953
    },
    {
        "loss": 1.709,
        "grad_norm": 2.517348051071167,
        "learning_rate": 2.2193030544355188e-05,
        "epoch": 0.8963650425367363,
        "step": 6954
    },
    {
        "loss": 1.5989,
        "grad_norm": 3.609992504119873,
        "learning_rate": 2.2142471086183953e-05,
        "epoch": 0.8964939417375613,
        "step": 6955
    },
    {
        "loss": 1.8344,
        "grad_norm": 2.2485930919647217,
        "learning_rate": 2.209195290103504e-05,
        "epoch": 0.8966228409383862,
        "step": 6956
    },
    {
        "loss": 1.7639,
        "grad_norm": 2.6980326175689697,
        "learning_rate": 2.2041476063755025e-05,
        "epoch": 0.8967517401392111,
        "step": 6957
    },
    {
        "loss": 2.25,
        "grad_norm": 2.4645769596099854,
        "learning_rate": 2.1991040649128957e-05,
        "epoch": 0.8968806393400361,
        "step": 6958
    },
    {
        "loss": 1.4384,
        "grad_norm": 1.746004581451416,
        "learning_rate": 2.194064673188089e-05,
        "epoch": 0.8970095385408611,
        "step": 6959
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.0912110805511475,
        "learning_rate": 2.1890294386673088e-05,
        "epoch": 0.897138437741686,
        "step": 6960
    },
    {
        "loss": 2.204,
        "grad_norm": 1.7434895038604736,
        "learning_rate": 2.183998368810637e-05,
        "epoch": 0.8972673369425109,
        "step": 6961
    },
    {
        "loss": 2.2243,
        "grad_norm": 2.9723215103149414,
        "learning_rate": 2.178971471071982e-05,
        "epoch": 0.8973962361433359,
        "step": 6962
    },
    {
        "loss": 1.8672,
        "grad_norm": 2.6886777877807617,
        "learning_rate": 2.173948752899069e-05,
        "epoch": 0.8975251353441609,
        "step": 6963
    },
    {
        "loss": 1.1217,
        "grad_norm": 3.0398499965667725,
        "learning_rate": 2.1689302217334317e-05,
        "epoch": 0.8976540345449858,
        "step": 6964
    },
    {
        "loss": 1.4961,
        "grad_norm": 3.945594310760498,
        "learning_rate": 2.1639158850104048e-05,
        "epoch": 0.8977829337458108,
        "step": 6965
    },
    {
        "loss": 1.6116,
        "grad_norm": 2.3805863857269287,
        "learning_rate": 2.1589057501591002e-05,
        "epoch": 0.8979118329466357,
        "step": 6966
    },
    {
        "loss": 1.9577,
        "grad_norm": 2.524535655975342,
        "learning_rate": 2.1538998246024117e-05,
        "epoch": 0.8980407321474607,
        "step": 6967
    },
    {
        "loss": 1.722,
        "grad_norm": 2.881006956100464,
        "learning_rate": 2.1488981157569943e-05,
        "epoch": 0.8981696313482856,
        "step": 6968
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.9132258892059326,
        "learning_rate": 2.143900631033256e-05,
        "epoch": 0.8982985305491106,
        "step": 6969
    },
    {
        "loss": 2.2742,
        "grad_norm": 2.493137836456299,
        "learning_rate": 2.1389073778353437e-05,
        "epoch": 0.8984274297499355,
        "step": 6970
    },
    {
        "loss": 2.2961,
        "grad_norm": 2.0201494693756104,
        "learning_rate": 2.1339183635611383e-05,
        "epoch": 0.8985563289507605,
        "step": 6971
    },
    {
        "loss": 2.0926,
        "grad_norm": 1.6881749629974365,
        "learning_rate": 2.1289335956022417e-05,
        "epoch": 0.8986852281515855,
        "step": 6972
    },
    {
        "loss": 1.4258,
        "grad_norm": 3.2144248485565186,
        "learning_rate": 2.1239530813439577e-05,
        "epoch": 0.8988141273524104,
        "step": 6973
    },
    {
        "loss": 1.4227,
        "grad_norm": 2.947676658630371,
        "learning_rate": 2.118976828165295e-05,
        "epoch": 0.8989430265532353,
        "step": 6974
    },
    {
        "loss": 1.5595,
        "grad_norm": 3.097548484802246,
        "learning_rate": 2.1140048434389465e-05,
        "epoch": 0.8990719257540604,
        "step": 6975
    },
    {
        "loss": 1.8537,
        "grad_norm": 2.3084511756896973,
        "learning_rate": 2.1090371345312826e-05,
        "epoch": 0.8992008249548853,
        "step": 6976
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.2051424980163574,
        "learning_rate": 2.1040737088023343e-05,
        "epoch": 0.8993297241557102,
        "step": 6977
    },
    {
        "loss": 2.0678,
        "grad_norm": 2.6033847332000732,
        "learning_rate": 2.099114573605791e-05,
        "epoch": 0.8994586233565351,
        "step": 6978
    },
    {
        "loss": 2.3289,
        "grad_norm": 1.7957727909088135,
        "learning_rate": 2.0941597362889807e-05,
        "epoch": 0.8995875225573602,
        "step": 6979
    },
    {
        "loss": 2.2152,
        "grad_norm": 2.4543046951293945,
        "learning_rate": 2.0892092041928803e-05,
        "epoch": 0.8997164217581851,
        "step": 6980
    },
    {
        "loss": 1.8624,
        "grad_norm": 3.0098540782928467,
        "learning_rate": 2.0842629846520596e-05,
        "epoch": 0.89984532095901,
        "step": 6981
    },
    {
        "loss": 1.3629,
        "grad_norm": 2.984381675720215,
        "learning_rate": 2.0793210849947225e-05,
        "epoch": 0.899974220159835,
        "step": 6982
    },
    {
        "loss": 2.1027,
        "grad_norm": 1.9716694355010986,
        "learning_rate": 2.0743835125426615e-05,
        "epoch": 0.90010311936066,
        "step": 6983
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.8330276012420654,
        "learning_rate": 2.069450274611262e-05,
        "epoch": 0.9002320185614849,
        "step": 6984
    },
    {
        "loss": 1.2592,
        "grad_norm": 2.9657692909240723,
        "learning_rate": 2.064521378509482e-05,
        "epoch": 0.9003609177623099,
        "step": 6985
    },
    {
        "loss": 0.8982,
        "grad_norm": 3.4823219776153564,
        "learning_rate": 2.059596831539869e-05,
        "epoch": 0.9004898169631348,
        "step": 6986
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.631616473197937,
        "learning_rate": 2.0546766409984904e-05,
        "epoch": 0.9006187161639598,
        "step": 6987
    },
    {
        "loss": 2.306,
        "grad_norm": 2.753838539123535,
        "learning_rate": 2.0497608141749843e-05,
        "epoch": 0.9007476153647848,
        "step": 6988
    },
    {
        "loss": 1.8172,
        "grad_norm": 2.7848873138427734,
        "learning_rate": 2.0448493583525195e-05,
        "epoch": 0.9008765145656097,
        "step": 6989
    },
    {
        "loss": 0.4645,
        "grad_norm": 2.0008561611175537,
        "learning_rate": 2.0399422808077838e-05,
        "epoch": 0.9010054137664346,
        "step": 6990
    },
    {
        "loss": 2.0277,
        "grad_norm": 3.3394246101379395,
        "learning_rate": 2.0350395888109773e-05,
        "epoch": 0.9011343129672597,
        "step": 6991
    },
    {
        "loss": 2.1152,
        "grad_norm": 2.461167097091675,
        "learning_rate": 2.030141289625818e-05,
        "epoch": 0.9012632121680846,
        "step": 6992
    },
    {
        "loss": 1.5027,
        "grad_norm": 3.033421277999878,
        "learning_rate": 2.0252473905095053e-05,
        "epoch": 0.9013921113689095,
        "step": 6993
    },
    {
        "loss": 1.7228,
        "grad_norm": 2.5596935749053955,
        "learning_rate": 2.020357898712704e-05,
        "epoch": 0.9015210105697344,
        "step": 6994
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.522275924682617,
        "learning_rate": 2.0154728214795665e-05,
        "epoch": 0.9016499097705595,
        "step": 6995
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.373115301132202,
        "learning_rate": 2.0105921660477184e-05,
        "epoch": 0.9017788089713844,
        "step": 6996
    },
    {
        "loss": 1.3768,
        "grad_norm": 2.979799270629883,
        "learning_rate": 2.0057159396481933e-05,
        "epoch": 0.9019077081722093,
        "step": 6997
    },
    {
        "loss": 2.5111,
        "grad_norm": 2.2567849159240723,
        "learning_rate": 2.0008441495055045e-05,
        "epoch": 0.9020366073730343,
        "step": 6998
    },
    {
        "loss": 2.2417,
        "grad_norm": 1.6565977334976196,
        "learning_rate": 1.995976802837577e-05,
        "epoch": 0.9021655065738592,
        "step": 6999
    },
    {
        "loss": 2.4565,
        "grad_norm": 1.4330641031265259,
        "learning_rate": 1.9911139068557366e-05,
        "epoch": 0.9022944057746842,
        "step": 7000
    },
    {
        "loss": 2.1013,
        "grad_norm": 2.4773826599121094,
        "learning_rate": 1.9862554687647306e-05,
        "epoch": 0.9024233049755092,
        "step": 7001
    },
    {
        "loss": 2.3155,
        "grad_norm": 1.793260097503662,
        "learning_rate": 1.9814014957627158e-05,
        "epoch": 0.9025522041763341,
        "step": 7002
    },
    {
        "loss": 1.3489,
        "grad_norm": 3.101392984390259,
        "learning_rate": 1.9765519950411988e-05,
        "epoch": 0.902681103377159,
        "step": 7003
    },
    {
        "loss": 1.9736,
        "grad_norm": 3.5226058959960938,
        "learning_rate": 1.9717069737850925e-05,
        "epoch": 0.902810002577984,
        "step": 7004
    },
    {
        "loss": 0.8526,
        "grad_norm": 2.6706125736236572,
        "learning_rate": 1.96686643917266e-05,
        "epoch": 0.902938901778809,
        "step": 7005
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.7267519235610962,
        "learning_rate": 1.9620303983755094e-05,
        "epoch": 0.9030678009796339,
        "step": 7006
    },
    {
        "loss": 1.8355,
        "grad_norm": 2.211595296859741,
        "learning_rate": 1.9571988585585947e-05,
        "epoch": 0.9031967001804588,
        "step": 7007
    },
    {
        "loss": 1.6984,
        "grad_norm": 2.5601797103881836,
        "learning_rate": 1.9523718268802182e-05,
        "epoch": 0.9033255993812839,
        "step": 7008
    },
    {
        "loss": 1.9367,
        "grad_norm": 1.7238609790802002,
        "learning_rate": 1.947549310491984e-05,
        "epoch": 0.9034544985821088,
        "step": 7009
    },
    {
        "loss": 1.7053,
        "grad_norm": 2.3706929683685303,
        "learning_rate": 1.9427313165388138e-05,
        "epoch": 0.9035833977829337,
        "step": 7010
    },
    {
        "loss": 2.2344,
        "grad_norm": 2.2836406230926514,
        "learning_rate": 1.9379178521589254e-05,
        "epoch": 0.9037122969837587,
        "step": 7011
    },
    {
        "loss": 1.6207,
        "grad_norm": 2.3904428482055664,
        "learning_rate": 1.933108924483834e-05,
        "epoch": 0.9038411961845837,
        "step": 7012
    },
    {
        "loss": 1.0268,
        "grad_norm": 6.02879524230957,
        "learning_rate": 1.9283045406383136e-05,
        "epoch": 0.9039700953854086,
        "step": 7013
    },
    {
        "loss": 2.0984,
        "grad_norm": 1.897108554840088,
        "learning_rate": 1.9235047077404324e-05,
        "epoch": 0.9040989945862336,
        "step": 7014
    },
    {
        "loss": 2.2555,
        "grad_norm": 2.562091588973999,
        "learning_rate": 1.9187094329014998e-05,
        "epoch": 0.9042278937870585,
        "step": 7015
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.3399412631988525,
        "learning_rate": 1.9139187232260753e-05,
        "epoch": 0.9043567929878835,
        "step": 7016
    },
    {
        "loss": 2.0822,
        "grad_norm": 2.736619234085083,
        "learning_rate": 1.9091325858119578e-05,
        "epoch": 0.9044856921887084,
        "step": 7017
    },
    {
        "loss": 1.7252,
        "grad_norm": 2.6072475910186768,
        "learning_rate": 1.9043510277501714e-05,
        "epoch": 0.9046145913895334,
        "step": 7018
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.0747548341751099,
        "learning_rate": 1.8995740561249385e-05,
        "epoch": 0.9047434905903583,
        "step": 7019
    },
    {
        "loss": 2.2215,
        "grad_norm": 1.8138236999511719,
        "learning_rate": 1.89480167801372e-05,
        "epoch": 0.9048723897911833,
        "step": 7020
    },
    {
        "loss": 2.2705,
        "grad_norm": 2.362283945083618,
        "learning_rate": 1.890033900487144e-05,
        "epoch": 0.9050012889920083,
        "step": 7021
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.5719366073608398,
        "learning_rate": 1.8852707306090323e-05,
        "epoch": 0.9051301881928332,
        "step": 7022
    },
    {
        "loss": 1.1874,
        "grad_norm": 4.020153045654297,
        "learning_rate": 1.8805121754363814e-05,
        "epoch": 0.9052590873936581,
        "step": 7023
    },
    {
        "loss": 1.8433,
        "grad_norm": 1.6452014446258545,
        "learning_rate": 1.8757582420193437e-05,
        "epoch": 0.9053879865944832,
        "step": 7024
    },
    {
        "loss": 1.7731,
        "grad_norm": 1.4696969985961914,
        "learning_rate": 1.871008937401232e-05,
        "epoch": 0.9055168857953081,
        "step": 7025
    },
    {
        "loss": 2.3635,
        "grad_norm": 1.8002761602401733,
        "learning_rate": 1.8662642686184966e-05,
        "epoch": 0.905645784996133,
        "step": 7026
    },
    {
        "loss": 1.1561,
        "grad_norm": 2.8551738262176514,
        "learning_rate": 1.861524242700724e-05,
        "epoch": 0.905774684196958,
        "step": 7027
    },
    {
        "loss": 1.4228,
        "grad_norm": 2.368225336074829,
        "learning_rate": 1.856788866670614e-05,
        "epoch": 0.905903583397783,
        "step": 7028
    },
    {
        "loss": 2.4812,
        "grad_norm": 2.0384163856506348,
        "learning_rate": 1.8520581475439842e-05,
        "epoch": 0.9060324825986079,
        "step": 7029
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.226149559020996,
        "learning_rate": 1.8473320923297527e-05,
        "epoch": 0.9061613817994328,
        "step": 7030
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.0089662075042725,
        "learning_rate": 1.842610708029921e-05,
        "epoch": 0.9062902810002578,
        "step": 7031
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.6208406686782837,
        "learning_rate": 1.8378940016395773e-05,
        "epoch": 0.9064191802010828,
        "step": 7032
    },
    {
        "loss": 2.0193,
        "grad_norm": 2.239595413208008,
        "learning_rate": 1.8331819801468763e-05,
        "epoch": 0.9065480794019077,
        "step": 7033
    },
    {
        "loss": 1.5881,
        "grad_norm": 3.856168031692505,
        "learning_rate": 1.828474650533033e-05,
        "epoch": 0.9066769786027327,
        "step": 7034
    },
    {
        "loss": 1.857,
        "grad_norm": 3.5970335006713867,
        "learning_rate": 1.8237720197723075e-05,
        "epoch": 0.9068058778035576,
        "step": 7035
    },
    {
        "loss": 1.7532,
        "grad_norm": 2.576763391494751,
        "learning_rate": 1.8190740948320017e-05,
        "epoch": 0.9069347770043825,
        "step": 7036
    },
    {
        "loss": 2.514,
        "grad_norm": 1.5335822105407715,
        "learning_rate": 1.8143808826724468e-05,
        "epoch": 0.9070636762052076,
        "step": 7037
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.8663842678070068,
        "learning_rate": 1.8096923902469847e-05,
        "epoch": 0.9071925754060325,
        "step": 7038
    },
    {
        "loss": 1.2858,
        "grad_norm": 3.472764730453491,
        "learning_rate": 1.805008624501972e-05,
        "epoch": 0.9073214746068574,
        "step": 7039
    },
    {
        "loss": 1.8819,
        "grad_norm": 3.351527452468872,
        "learning_rate": 1.8003295923767604e-05,
        "epoch": 0.9074503738076823,
        "step": 7040
    },
    {
        "loss": 1.7788,
        "grad_norm": 3.026869058609009,
        "learning_rate": 1.7956553008036854e-05,
        "epoch": 0.9075792730085074,
        "step": 7041
    },
    {
        "loss": 2.1742,
        "grad_norm": 2.8662168979644775,
        "learning_rate": 1.7909857567080618e-05,
        "epoch": 0.9077081722093323,
        "step": 7042
    },
    {
        "loss": 1.9806,
        "grad_norm": 2.512784481048584,
        "learning_rate": 1.7863209670081703e-05,
        "epoch": 0.9078370714101572,
        "step": 7043
    },
    {
        "loss": 0.932,
        "grad_norm": 2.847064971923828,
        "learning_rate": 1.7816609386152494e-05,
        "epoch": 0.9079659706109822,
        "step": 7044
    },
    {
        "loss": 1.8628,
        "grad_norm": 3.4488275051116943,
        "learning_rate": 1.7770056784334776e-05,
        "epoch": 0.9080948698118072,
        "step": 7045
    },
    {
        "loss": 2.3908,
        "grad_norm": 1.1674517393112183,
        "learning_rate": 1.7723551933599698e-05,
        "epoch": 0.9082237690126321,
        "step": 7046
    },
    {
        "loss": 1.4732,
        "grad_norm": 2.9470014572143555,
        "learning_rate": 1.7677094902847852e-05,
        "epoch": 0.9083526682134571,
        "step": 7047
    },
    {
        "loss": 1.947,
        "grad_norm": 2.2615466117858887,
        "learning_rate": 1.763068576090864e-05,
        "epoch": 0.908481567414282,
        "step": 7048
    },
    {
        "loss": 0.8606,
        "grad_norm": 1.4707539081573486,
        "learning_rate": 1.7584324576540757e-05,
        "epoch": 0.908610466615107,
        "step": 7049
    },
    {
        "loss": 1.8502,
        "grad_norm": 3.184901714324951,
        "learning_rate": 1.75380114184318e-05,
        "epoch": 0.908739365815932,
        "step": 7050
    },
    {
        "loss": 1.2054,
        "grad_norm": 2.659858226776123,
        "learning_rate": 1.7491746355198157e-05,
        "epoch": 0.9088682650167569,
        "step": 7051
    },
    {
        "loss": 1.743,
        "grad_norm": 1.753845453262329,
        "learning_rate": 1.7445529455384964e-05,
        "epoch": 0.9089971642175818,
        "step": 7052
    },
    {
        "loss": 2.4731,
        "grad_norm": 1.7934136390686035,
        "learning_rate": 1.7399360787466184e-05,
        "epoch": 0.9091260634184068,
        "step": 7053
    },
    {
        "loss": 1.5403,
        "grad_norm": 2.963963270187378,
        "learning_rate": 1.735324041984403e-05,
        "epoch": 0.9092549626192318,
        "step": 7054
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.7815377712249756,
        "learning_rate": 1.730716842084931e-05,
        "epoch": 0.9093838618200567,
        "step": 7055
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.6200312376022339,
        "learning_rate": 1.7261144858741176e-05,
        "epoch": 0.9095127610208816,
        "step": 7056
    },
    {
        "loss": 1.299,
        "grad_norm": 2.925112247467041,
        "learning_rate": 1.7215169801706997e-05,
        "epoch": 0.9096416602217067,
        "step": 7057
    },
    {
        "loss": 1.7595,
        "grad_norm": 3.005836248397827,
        "learning_rate": 1.7169243317862198e-05,
        "epoch": 0.9097705594225316,
        "step": 7058
    },
    {
        "loss": 2.2414,
        "grad_norm": 3.5887138843536377,
        "learning_rate": 1.7123365475250434e-05,
        "epoch": 0.9098994586233565,
        "step": 7059
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.5901739597320557,
        "learning_rate": 1.707753634184318e-05,
        "epoch": 0.9100283578241815,
        "step": 7060
    },
    {
        "loss": 2.1283,
        "grad_norm": 2.8621039390563965,
        "learning_rate": 1.7031755985539648e-05,
        "epoch": 0.9101572570250065,
        "step": 7061
    },
    {
        "loss": 1.9401,
        "grad_norm": 3.0546252727508545,
        "learning_rate": 1.6986024474166844e-05,
        "epoch": 0.9102861562258314,
        "step": 7062
    },
    {
        "loss": 2.8567,
        "grad_norm": 1.517328143119812,
        "learning_rate": 1.694034187547961e-05,
        "epoch": 0.9104150554266563,
        "step": 7063
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.6102889776229858,
        "learning_rate": 1.6894708257159963e-05,
        "epoch": 0.9105439546274813,
        "step": 7064
    },
    {
        "loss": 1.704,
        "grad_norm": 1.6918319463729858,
        "learning_rate": 1.684912368681766e-05,
        "epoch": 0.9106728538283063,
        "step": 7065
    },
    {
        "loss": 1.756,
        "grad_norm": 2.531731605529785,
        "learning_rate": 1.680358823198967e-05,
        "epoch": 0.9108017530291312,
        "step": 7066
    },
    {
        "loss": 1.9393,
        "grad_norm": 2.054547071456909,
        "learning_rate": 1.675810196014012e-05,
        "epoch": 0.9109306522299562,
        "step": 7067
    },
    {
        "loss": 1.0332,
        "grad_norm": 3.4575655460357666,
        "learning_rate": 1.67126649386603e-05,
        "epoch": 0.9110595514307811,
        "step": 7068
    },
    {
        "loss": 2.2621,
        "grad_norm": 1.465024471282959,
        "learning_rate": 1.6667277234868744e-05,
        "epoch": 0.9111884506316061,
        "step": 7069
    },
    {
        "loss": 1.8813,
        "grad_norm": 1.6188366413116455,
        "learning_rate": 1.6621938916010538e-05,
        "epoch": 0.9113173498324311,
        "step": 7070
    },
    {
        "loss": 1.6378,
        "grad_norm": 2.0452027320861816,
        "learning_rate": 1.657665004925798e-05,
        "epoch": 0.911446249033256,
        "step": 7071
    },
    {
        "loss": 2.0947,
        "grad_norm": 2.4661812782287598,
        "learning_rate": 1.65314107017099e-05,
        "epoch": 0.9115751482340809,
        "step": 7072
    },
    {
        "loss": 1.0378,
        "grad_norm": 1.3374696969985962,
        "learning_rate": 1.6486220940391695e-05,
        "epoch": 0.9117040474349059,
        "step": 7073
    },
    {
        "loss": 1.8442,
        "grad_norm": 1.860835313796997,
        "learning_rate": 1.64410808322554e-05,
        "epoch": 0.9118329466357309,
        "step": 7074
    },
    {
        "loss": 1.8739,
        "grad_norm": 3.14888334274292,
        "learning_rate": 1.6395990444179598e-05,
        "epoch": 0.9119618458365558,
        "step": 7075
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.7414262294769287,
        "learning_rate": 1.6350949842969e-05,
        "epoch": 0.9120907450373807,
        "step": 7076
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.958504557609558,
        "learning_rate": 1.630595909535465e-05,
        "epoch": 0.9122196442382057,
        "step": 7077
    },
    {
        "loss": 0.9428,
        "grad_norm": 2.891275405883789,
        "learning_rate": 1.6261018267993745e-05,
        "epoch": 0.9123485434390307,
        "step": 7078
    },
    {
        "loss": 1.9744,
        "grad_norm": 2.6609225273132324,
        "learning_rate": 1.6216127427469518e-05,
        "epoch": 0.9124774426398556,
        "step": 7079
    },
    {
        "loss": 2.1518,
        "grad_norm": 1.6315053701400757,
        "learning_rate": 1.6171286640290982e-05,
        "epoch": 0.9126063418406806,
        "step": 7080
    },
    {
        "loss": 2.1963,
        "grad_norm": 1.8143765926361084,
        "learning_rate": 1.61264959728933e-05,
        "epoch": 0.9127352410415055,
        "step": 7081
    },
    {
        "loss": 1.5071,
        "grad_norm": 2.6425974369049072,
        "learning_rate": 1.6081755491637136e-05,
        "epoch": 0.9128641402423305,
        "step": 7082
    },
    {
        "loss": 1.3691,
        "grad_norm": 2.054807424545288,
        "learning_rate": 1.6037065262808886e-05,
        "epoch": 0.9129930394431555,
        "step": 7083
    },
    {
        "loss": 2.4418,
        "grad_norm": 1.5101221799850464,
        "learning_rate": 1.5992425352620478e-05,
        "epoch": 0.9131219386439804,
        "step": 7084
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.8071520328521729,
        "learning_rate": 1.5947835827209363e-05,
        "epoch": 0.9132508378448053,
        "step": 7085
    },
    {
        "loss": 1.6948,
        "grad_norm": 2.903345823287964,
        "learning_rate": 1.5903296752638098e-05,
        "epoch": 0.9133797370456304,
        "step": 7086
    },
    {
        "loss": 2.2995,
        "grad_norm": 1.7185816764831543,
        "learning_rate": 1.585880819489482e-05,
        "epoch": 0.9135086362464553,
        "step": 7087
    },
    {
        "loss": 2.0548,
        "grad_norm": 2.964721918106079,
        "learning_rate": 1.581437021989265e-05,
        "epoch": 0.9136375354472802,
        "step": 7088
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.8006062507629395,
        "learning_rate": 1.5769982893469747e-05,
        "epoch": 0.9137664346481051,
        "step": 7089
    },
    {
        "loss": 1.6222,
        "grad_norm": 2.132497549057007,
        "learning_rate": 1.5725646281389288e-05,
        "epoch": 0.9138953338489302,
        "step": 7090
    },
    {
        "loss": 1.79,
        "grad_norm": 3.846187114715576,
        "learning_rate": 1.5681360449339316e-05,
        "epoch": 0.9140242330497551,
        "step": 7091
    },
    {
        "loss": 1.8673,
        "grad_norm": 2.5034589767456055,
        "learning_rate": 1.5637125462932584e-05,
        "epoch": 0.91415313225058,
        "step": 7092
    },
    {
        "loss": 1.7066,
        "grad_norm": 2.5159928798675537,
        "learning_rate": 1.559294138770656e-05,
        "epoch": 0.914282031451405,
        "step": 7093
    },
    {
        "loss": 1.1464,
        "grad_norm": 2.7849466800689697,
        "learning_rate": 1.5548808289123273e-05,
        "epoch": 0.91441093065223,
        "step": 7094
    },
    {
        "loss": 1.9196,
        "grad_norm": 2.066394329071045,
        "learning_rate": 1.550472623256925e-05,
        "epoch": 0.9145398298530549,
        "step": 7095
    },
    {
        "loss": 1.9712,
        "grad_norm": 2.278411865234375,
        "learning_rate": 1.5460695283355325e-05,
        "epoch": 0.9146687290538799,
        "step": 7096
    },
    {
        "loss": 1.6788,
        "grad_norm": 3.7828991413116455,
        "learning_rate": 1.5416715506716683e-05,
        "epoch": 0.9147976282547048,
        "step": 7097
    },
    {
        "loss": 1.3643,
        "grad_norm": 3.387047529220581,
        "learning_rate": 1.537278696781268e-05,
        "epoch": 0.9149265274555298,
        "step": 7098
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.3112144470214844,
        "learning_rate": 1.5328909731726714e-05,
        "epoch": 0.9150554266563548,
        "step": 7099
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.6973323822021484,
        "learning_rate": 1.528508386346623e-05,
        "epoch": 0.9151843258571797,
        "step": 7100
    },
    {
        "loss": 1.1991,
        "grad_norm": 2.561218738555908,
        "learning_rate": 1.5241309427962535e-05,
        "epoch": 0.9153132250580046,
        "step": 7101
    },
    {
        "loss": 1.6639,
        "grad_norm": 6.109325885772705,
        "learning_rate": 1.519758649007077e-05,
        "epoch": 0.9154421242588296,
        "step": 7102
    },
    {
        "loss": 1.7358,
        "grad_norm": 1.8108577728271484,
        "learning_rate": 1.5153915114569717e-05,
        "epoch": 0.9155710234596546,
        "step": 7103
    },
    {
        "loss": 1.2938,
        "grad_norm": 3.9720377922058105,
        "learning_rate": 1.5110295366161814e-05,
        "epoch": 0.9156999226604795,
        "step": 7104
    },
    {
        "loss": 1.0285,
        "grad_norm": 2.664893388748169,
        "learning_rate": 1.5066727309473011e-05,
        "epoch": 0.9158288218613044,
        "step": 7105
    },
    {
        "loss": 1.7864,
        "grad_norm": 2.365358352661133,
        "learning_rate": 1.5023211009052635e-05,
        "epoch": 0.9159577210621294,
        "step": 7106
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.339085340499878,
        "learning_rate": 1.4979746529373312e-05,
        "epoch": 0.9160866202629544,
        "step": 7107
    },
    {
        "loss": 2.4001,
        "grad_norm": 2.0364272594451904,
        "learning_rate": 1.4936333934831099e-05,
        "epoch": 0.9162155194637793,
        "step": 7108
    },
    {
        "loss": 1.5554,
        "grad_norm": 1.9450243711471558,
        "learning_rate": 1.4892973289744844e-05,
        "epoch": 0.9163444186646043,
        "step": 7109
    },
    {
        "loss": 2.0258,
        "grad_norm": 2.192063808441162,
        "learning_rate": 1.484966465835666e-05,
        "epoch": 0.9164733178654292,
        "step": 7110
    },
    {
        "loss": 1.3271,
        "grad_norm": 4.402833938598633,
        "learning_rate": 1.4806408104831536e-05,
        "epoch": 0.9166022170662542,
        "step": 7111
    },
    {
        "loss": 1.9422,
        "grad_norm": 2.5275967121124268,
        "learning_rate": 1.4763203693257339e-05,
        "epoch": 0.9167311162670791,
        "step": 7112
    },
    {
        "loss": 1.0618,
        "grad_norm": 3.3752453327178955,
        "learning_rate": 1.4720051487644548e-05,
        "epoch": 0.9168600154679041,
        "step": 7113
    },
    {
        "loss": 1.4616,
        "grad_norm": 3.8453493118286133,
        "learning_rate": 1.4676951551926576e-05,
        "epoch": 0.916988914668729,
        "step": 7114
    },
    {
        "loss": 1.6726,
        "grad_norm": 2.957482099533081,
        "learning_rate": 1.4633903949959077e-05,
        "epoch": 0.917117813869554,
        "step": 7115
    },
    {
        "loss": 2.0613,
        "grad_norm": 3.089261531829834,
        "learning_rate": 1.4590908745520333e-05,
        "epoch": 0.917246713070379,
        "step": 7116
    },
    {
        "loss": 1.9235,
        "grad_norm": 2.877951145172119,
        "learning_rate": 1.4547966002310981e-05,
        "epoch": 0.9173756122712039,
        "step": 7117
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.705951452255249,
        "learning_rate": 1.4505075783953925e-05,
        "epoch": 0.9175045114720288,
        "step": 7118
    },
    {
        "loss": 1.9935,
        "grad_norm": 3.6534852981567383,
        "learning_rate": 1.4462238153994206e-05,
        "epoch": 0.9176334106728539,
        "step": 7119
    },
    {
        "loss": 2.2053,
        "grad_norm": 1.7732183933258057,
        "learning_rate": 1.4419453175899117e-05,
        "epoch": 0.9177623098736788,
        "step": 7120
    },
    {
        "loss": 2.2523,
        "grad_norm": 2.4396002292633057,
        "learning_rate": 1.4376720913057679e-05,
        "epoch": 0.9178912090745037,
        "step": 7121
    },
    {
        "loss": 1.9931,
        "grad_norm": 3.287339448928833,
        "learning_rate": 1.4334041428781015e-05,
        "epoch": 0.9180201082753287,
        "step": 7122
    },
    {
        "loss": 1.7399,
        "grad_norm": 4.130831241607666,
        "learning_rate": 1.4291414786301966e-05,
        "epoch": 0.9181490074761537,
        "step": 7123
    },
    {
        "loss": 1.9702,
        "grad_norm": 2.033475399017334,
        "learning_rate": 1.4248841048775109e-05,
        "epoch": 0.9182779066769786,
        "step": 7124
    },
    {
        "loss": 1.7895,
        "grad_norm": 1.9828791618347168,
        "learning_rate": 1.4206320279276596e-05,
        "epoch": 0.9184068058778035,
        "step": 7125
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.116389751434326,
        "learning_rate": 1.416385254080425e-05,
        "epoch": 0.9185357050786285,
        "step": 7126
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.4651541709899902,
        "learning_rate": 1.412143789627719e-05,
        "epoch": 0.9186646042794535,
        "step": 7127
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.7745981216430664,
        "learning_rate": 1.4079076408535813e-05,
        "epoch": 0.9187935034802784,
        "step": 7128
    },
    {
        "loss": 1.8256,
        "grad_norm": 2.5570998191833496,
        "learning_rate": 1.403676814034186e-05,
        "epoch": 0.9189224026811034,
        "step": 7129
    },
    {
        "loss": 1.6948,
        "grad_norm": 2.459774971008301,
        "learning_rate": 1.3994513154378331e-05,
        "epoch": 0.9190513018819283,
        "step": 7130
    },
    {
        "loss": 1.4993,
        "grad_norm": 2.089017629623413,
        "learning_rate": 1.3952311513248978e-05,
        "epoch": 0.9191802010827533,
        "step": 7131
    },
    {
        "loss": 1.5249,
        "grad_norm": 2.9730288982391357,
        "learning_rate": 1.3910163279478855e-05,
        "epoch": 0.9193091002835783,
        "step": 7132
    },
    {
        "loss": 1.814,
        "grad_norm": 2.782447338104248,
        "learning_rate": 1.3868068515513743e-05,
        "epoch": 0.9194379994844032,
        "step": 7133
    },
    {
        "loss": 1.2339,
        "grad_norm": 1.9727364778518677,
        "learning_rate": 1.3826027283720082e-05,
        "epoch": 0.9195668986852281,
        "step": 7134
    },
    {
        "loss": 2.2893,
        "grad_norm": 2.0798425674438477,
        "learning_rate": 1.3784039646385122e-05,
        "epoch": 0.9196957978860532,
        "step": 7135
    },
    {
        "loss": 1.7795,
        "grad_norm": 3.206799030303955,
        "learning_rate": 1.374210566571687e-05,
        "epoch": 0.9198246970868781,
        "step": 7136
    },
    {
        "loss": 1.9318,
        "grad_norm": 1.7093505859375,
        "learning_rate": 1.3700225403843442e-05,
        "epoch": 0.919953596287703,
        "step": 7137
    },
    {
        "loss": 2.179,
        "grad_norm": 1.374594807624817,
        "learning_rate": 1.3658398922813754e-05,
        "epoch": 0.9200824954885279,
        "step": 7138
    },
    {
        "loss": 1.992,
        "grad_norm": 3.037020206451416,
        "learning_rate": 1.3616626284596889e-05,
        "epoch": 0.920211394689353,
        "step": 7139
    },
    {
        "loss": 1.6387,
        "grad_norm": 2.694085121154785,
        "learning_rate": 1.3574907551082033e-05,
        "epoch": 0.9203402938901779,
        "step": 7140
    },
    {
        "loss": 1.8297,
        "grad_norm": 1.8513656854629517,
        "learning_rate": 1.3533242784078637e-05,
        "epoch": 0.9204691930910028,
        "step": 7141
    },
    {
        "loss": 1.2698,
        "grad_norm": 2.734966993331909,
        "learning_rate": 1.3491632045316277e-05,
        "epoch": 0.9205980922918278,
        "step": 7142
    },
    {
        "loss": 1.8646,
        "grad_norm": 2.7801733016967773,
        "learning_rate": 1.3450075396444355e-05,
        "epoch": 0.9207269914926527,
        "step": 7143
    },
    {
        "loss": 2.0187,
        "grad_norm": 2.2092976570129395,
        "learning_rate": 1.3408572899032146e-05,
        "epoch": 0.9208558906934777,
        "step": 7144
    },
    {
        "loss": 1.8457,
        "grad_norm": 1.8162933588027954,
        "learning_rate": 1.3367124614568728e-05,
        "epoch": 0.9209847898943027,
        "step": 7145
    },
    {
        "loss": 0.7886,
        "grad_norm": 2.7195844650268555,
        "learning_rate": 1.332573060446291e-05,
        "epoch": 0.9211136890951276,
        "step": 7146
    },
    {
        "loss": 2.1733,
        "grad_norm": 1.7624094486236572,
        "learning_rate": 1.3284390930042868e-05,
        "epoch": 0.9212425882959525,
        "step": 7147
    },
    {
        "loss": 1.7027,
        "grad_norm": 1.8444044589996338,
        "learning_rate": 1.3243105652556575e-05,
        "epoch": 0.9213714874967776,
        "step": 7148
    },
    {
        "loss": 2.3417,
        "grad_norm": 2.717116594314575,
        "learning_rate": 1.3201874833171252e-05,
        "epoch": 0.9215003866976025,
        "step": 7149
    },
    {
        "loss": 2.2979,
        "grad_norm": 2.4051456451416016,
        "learning_rate": 1.316069853297342e-05,
        "epoch": 0.9216292858984274,
        "step": 7150
    },
    {
        "loss": 1.3453,
        "grad_norm": 2.012408494949341,
        "learning_rate": 1.3119576812968882e-05,
        "epoch": 0.9217581850992523,
        "step": 7151
    },
    {
        "loss": 1.6694,
        "grad_norm": 2.520134210586548,
        "learning_rate": 1.3078509734082611e-05,
        "epoch": 0.9218870843000774,
        "step": 7152
    },
    {
        "loss": 1.9522,
        "grad_norm": 1.8908880949020386,
        "learning_rate": 1.3037497357158407e-05,
        "epoch": 0.9220159835009023,
        "step": 7153
    },
    {
        "loss": 1.5549,
        "grad_norm": 2.8352506160736084,
        "learning_rate": 1.299653974295934e-05,
        "epoch": 0.9221448827017272,
        "step": 7154
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.0841641426086426,
        "learning_rate": 1.2955636952167144e-05,
        "epoch": 0.9222737819025522,
        "step": 7155
    },
    {
        "loss": 1.559,
        "grad_norm": 3.941824436187744,
        "learning_rate": 1.2914789045382403e-05,
        "epoch": 0.9224026811033772,
        "step": 7156
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.4684652090072632,
        "learning_rate": 1.2873996083124312e-05,
        "epoch": 0.9225315803042021,
        "step": 7157
    },
    {
        "loss": 1.0683,
        "grad_norm": 3.367701768875122,
        "learning_rate": 1.2833258125830744e-05,
        "epoch": 0.922660479505027,
        "step": 7158
    },
    {
        "loss": 2.286,
        "grad_norm": 1.9240387678146362,
        "learning_rate": 1.2792575233858028e-05,
        "epoch": 0.922789378705852,
        "step": 7159
    },
    {
        "loss": 2.0253,
        "grad_norm": 2.4491655826568604,
        "learning_rate": 1.275194746748095e-05,
        "epoch": 0.922918277906677,
        "step": 7160
    },
    {
        "loss": 1.5263,
        "grad_norm": 3.886024236679077,
        "learning_rate": 1.2711374886892568e-05,
        "epoch": 0.923047177107502,
        "step": 7161
    },
    {
        "loss": 1.854,
        "grad_norm": 3.382866144180298,
        "learning_rate": 1.2670857552204219e-05,
        "epoch": 0.9231760763083269,
        "step": 7162
    },
    {
        "loss": 1.7395,
        "grad_norm": 2.3254363536834717,
        "learning_rate": 1.2630395523445399e-05,
        "epoch": 0.9233049755091518,
        "step": 7163
    },
    {
        "loss": 1.7953,
        "grad_norm": 2.68049955368042,
        "learning_rate": 1.2589988860563611e-05,
        "epoch": 0.9234338747099768,
        "step": 7164
    },
    {
        "loss": 1.9693,
        "grad_norm": 2.565988540649414,
        "learning_rate": 1.2549637623424376e-05,
        "epoch": 0.9235627739108018,
        "step": 7165
    },
    {
        "loss": 1.3107,
        "grad_norm": 2.885239362716675,
        "learning_rate": 1.2509341871811103e-05,
        "epoch": 0.9236916731116267,
        "step": 7166
    },
    {
        "loss": 1.6964,
        "grad_norm": 3.3395514488220215,
        "learning_rate": 1.2469101665424948e-05,
        "epoch": 0.9238205723124516,
        "step": 7167
    },
    {
        "loss": 2.29,
        "grad_norm": 1.7108629941940308,
        "learning_rate": 1.2428917063884804e-05,
        "epoch": 0.9239494715132767,
        "step": 7168
    },
    {
        "loss": 1.5451,
        "grad_norm": 3.561187982559204,
        "learning_rate": 1.238878812672719e-05,
        "epoch": 0.9240783707141016,
        "step": 7169
    },
    {
        "loss": 1.149,
        "grad_norm": 2.75704288482666,
        "learning_rate": 1.234871491340615e-05,
        "epoch": 0.9242072699149265,
        "step": 7170
    },
    {
        "loss": 1.4002,
        "grad_norm": 2.948336362838745,
        "learning_rate": 1.230869748329313e-05,
        "epoch": 0.9243361691157514,
        "step": 7171
    },
    {
        "loss": 1.5492,
        "grad_norm": 2.9065420627593994,
        "learning_rate": 1.2268735895676981e-05,
        "epoch": 0.9244650683165765,
        "step": 7172
    },
    {
        "loss": 1.9128,
        "grad_norm": 3.0363986492156982,
        "learning_rate": 1.2228830209763809e-05,
        "epoch": 0.9245939675174014,
        "step": 7173
    },
    {
        "loss": 1.3758,
        "grad_norm": 2.8247971534729004,
        "learning_rate": 1.218898048467681e-05,
        "epoch": 0.9247228667182263,
        "step": 7174
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.359708547592163,
        "learning_rate": 1.2149186779456495e-05,
        "epoch": 0.9248517659190513,
        "step": 7175
    },
    {
        "loss": 1.7338,
        "grad_norm": 2.4734954833984375,
        "learning_rate": 1.2109449153060114e-05,
        "epoch": 0.9249806651198763,
        "step": 7176
    },
    {
        "loss": 2.2219,
        "grad_norm": 3.101785659790039,
        "learning_rate": 1.2069767664361958e-05,
        "epoch": 0.9251095643207012,
        "step": 7177
    },
    {
        "loss": 2.532,
        "grad_norm": 1.6506469249725342,
        "learning_rate": 1.2030142372153146e-05,
        "epoch": 0.9252384635215262,
        "step": 7178
    },
    {
        "loss": 2.2249,
        "grad_norm": 2.1942412853240967,
        "learning_rate": 1.1990573335141531e-05,
        "epoch": 0.9253673627223511,
        "step": 7179
    },
    {
        "loss": 2.1655,
        "grad_norm": 2.056643486022949,
        "learning_rate": 1.1951060611951587e-05,
        "epoch": 0.925496261923176,
        "step": 7180
    },
    {
        "loss": 2.0945,
        "grad_norm": 2.422956705093384,
        "learning_rate": 1.1911604261124481e-05,
        "epoch": 0.9256251611240011,
        "step": 7181
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.6348471641540527,
        "learning_rate": 1.1872204341117643e-05,
        "epoch": 0.925754060324826,
        "step": 7182
    },
    {
        "loss": 2.6286,
        "grad_norm": 2.1278493404388428,
        "learning_rate": 1.1832860910305077e-05,
        "epoch": 0.9258829595256509,
        "step": 7183
    },
    {
        "loss": 1.2905,
        "grad_norm": 2.748229742050171,
        "learning_rate": 1.1793574026977e-05,
        "epoch": 0.9260118587264758,
        "step": 7184
    },
    {
        "loss": 2.3961,
        "grad_norm": 2.153925657272339,
        "learning_rate": 1.1754343749339902e-05,
        "epoch": 0.9261407579273009,
        "step": 7185
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.0811691284179688,
        "learning_rate": 1.1715170135516334e-05,
        "epoch": 0.9262696571281258,
        "step": 7186
    },
    {
        "loss": 1.4772,
        "grad_norm": 3.356065034866333,
        "learning_rate": 1.1676053243545104e-05,
        "epoch": 0.9263985563289507,
        "step": 7187
    },
    {
        "loss": 0.9611,
        "grad_norm": 3.2406563758850098,
        "learning_rate": 1.1636993131380658e-05,
        "epoch": 0.9265274555297757,
        "step": 7188
    },
    {
        "loss": 1.104,
        "grad_norm": 3.5997159481048584,
        "learning_rate": 1.1597989856893554e-05,
        "epoch": 0.9266563547306007,
        "step": 7189
    },
    {
        "loss": 2.0363,
        "grad_norm": 2.3765344619750977,
        "learning_rate": 1.155904347787004e-05,
        "epoch": 0.9267852539314256,
        "step": 7190
    },
    {
        "loss": 1.8592,
        "grad_norm": 2.3584444522857666,
        "learning_rate": 1.15201540520122e-05,
        "epoch": 0.9269141531322506,
        "step": 7191
    },
    {
        "loss": 1.7298,
        "grad_norm": 3.1515073776245117,
        "learning_rate": 1.1481321636937492e-05,
        "epoch": 0.9270430523330755,
        "step": 7192
    },
    {
        "loss": 1.0667,
        "grad_norm": 3.438514471054077,
        "learning_rate": 1.144254629017918e-05,
        "epoch": 0.9271719515339005,
        "step": 7193
    },
    {
        "loss": 1.5573,
        "grad_norm": 3.1179068088531494,
        "learning_rate": 1.1403828069185869e-05,
        "epoch": 0.9273008507347255,
        "step": 7194
    },
    {
        "loss": 2.0949,
        "grad_norm": 2.960132360458374,
        "learning_rate": 1.1365167031321394e-05,
        "epoch": 0.9274297499355504,
        "step": 7195
    },
    {
        "loss": 1.4568,
        "grad_norm": 2.926072359085083,
        "learning_rate": 1.1326563233865012e-05,
        "epoch": 0.9275586491363753,
        "step": 7196
    },
    {
        "loss": 2.3728,
        "grad_norm": 1.3001410961151123,
        "learning_rate": 1.1288016734011286e-05,
        "epoch": 0.9276875483372004,
        "step": 7197
    },
    {
        "loss": 1.4339,
        "grad_norm": 3.2254269123077393,
        "learning_rate": 1.1249527588869551e-05,
        "epoch": 0.9278164475380253,
        "step": 7198
    },
    {
        "loss": 1.9292,
        "grad_norm": 3.4507060050964355,
        "learning_rate": 1.1211095855464515e-05,
        "epoch": 0.9279453467388502,
        "step": 7199
    },
    {
        "loss": 1.816,
        "grad_norm": 3.055133819580078,
        "learning_rate": 1.1172721590735685e-05,
        "epoch": 0.9280742459396751,
        "step": 7200
    },
    {
        "loss": 1.4069,
        "grad_norm": 2.862351894378662,
        "learning_rate": 1.1134404851537317e-05,
        "epoch": 0.9282031451405002,
        "step": 7201
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.9180964231491089,
        "learning_rate": 1.1096145694638539e-05,
        "epoch": 0.9283320443413251,
        "step": 7202
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.576612949371338,
        "learning_rate": 1.1057944176723318e-05,
        "epoch": 0.92846094354215,
        "step": 7203
    },
    {
        "loss": 1.9747,
        "grad_norm": 2.3317718505859375,
        "learning_rate": 1.1019800354389875e-05,
        "epoch": 0.928589842742975,
        "step": 7204
    },
    {
        "loss": 1.0915,
        "grad_norm": 2.8483948707580566,
        "learning_rate": 1.0981714284151285e-05,
        "epoch": 0.9287187419438,
        "step": 7205
    },
    {
        "loss": 1.5636,
        "grad_norm": 2.285590648651123,
        "learning_rate": 1.0943686022434884e-05,
        "epoch": 0.9288476411446249,
        "step": 7206
    },
    {
        "loss": 1.797,
        "grad_norm": 1.8140050172805786,
        "learning_rate": 1.0905715625582457e-05,
        "epoch": 0.9289765403454499,
        "step": 7207
    },
    {
        "loss": 2.077,
        "grad_norm": 2.2129077911376953,
        "learning_rate": 1.0867803149849842e-05,
        "epoch": 0.9291054395462748,
        "step": 7208
    },
    {
        "loss": 2.4043,
        "grad_norm": 2.669771671295166,
        "learning_rate": 1.0829948651407362e-05,
        "epoch": 0.9292343387470998,
        "step": 7209
    },
    {
        "loss": 2.5691,
        "grad_norm": 1.9998455047607422,
        "learning_rate": 1.0792152186339261e-05,
        "epoch": 0.9293632379479247,
        "step": 7210
    },
    {
        "loss": 1.9119,
        "grad_norm": 1.8812255859375,
        "learning_rate": 1.0754413810643855e-05,
        "epoch": 0.9294921371487497,
        "step": 7211
    },
    {
        "loss": 1.8685,
        "grad_norm": 1.5840721130371094,
        "learning_rate": 1.0716733580233351e-05,
        "epoch": 0.9296210363495746,
        "step": 7212
    },
    {
        "loss": 1.4193,
        "grad_norm": 3.549745798110962,
        "learning_rate": 1.0679111550933901e-05,
        "epoch": 0.9297499355503996,
        "step": 7213
    },
    {
        "loss": 1.963,
        "grad_norm": 3.336233377456665,
        "learning_rate": 1.0641547778485261e-05,
        "epoch": 0.9298788347512246,
        "step": 7214
    },
    {
        "loss": 1.2685,
        "grad_norm": 2.5204474925994873,
        "learning_rate": 1.0604042318541074e-05,
        "epoch": 0.9300077339520495,
        "step": 7215
    },
    {
        "loss": 1.8815,
        "grad_norm": 1.767587661743164,
        "learning_rate": 1.0566595226668474e-05,
        "epoch": 0.9301366331528744,
        "step": 7216
    },
    {
        "loss": 2.4801,
        "grad_norm": 2.3990535736083984,
        "learning_rate": 1.0529206558348148e-05,
        "epoch": 0.9302655323536994,
        "step": 7217
    },
    {
        "loss": 1.0627,
        "grad_norm": 2.8267199993133545,
        "learning_rate": 1.0491876368974202e-05,
        "epoch": 0.9303944315545244,
        "step": 7218
    },
    {
        "loss": 1.8319,
        "grad_norm": 2.045936346054077,
        "learning_rate": 1.0454604713854154e-05,
        "epoch": 0.9305233307553493,
        "step": 7219
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.373861074447632,
        "learning_rate": 1.0417391648208658e-05,
        "epoch": 0.9306522299561742,
        "step": 7220
    },
    {
        "loss": 0.4428,
        "grad_norm": 2.0271551609039307,
        "learning_rate": 1.038023722717179e-05,
        "epoch": 0.9307811291569992,
        "step": 7221
    },
    {
        "loss": 1.9401,
        "grad_norm": 1.8762502670288086,
        "learning_rate": 1.0343141505790554e-05,
        "epoch": 0.9309100283578242,
        "step": 7222
    },
    {
        "loss": 1.9666,
        "grad_norm": 1.9886380434036255,
        "learning_rate": 1.0306104539025058e-05,
        "epoch": 0.9310389275586491,
        "step": 7223
    },
    {
        "loss": 2.3935,
        "grad_norm": 2.0176124572753906,
        "learning_rate": 1.0269126381748373e-05,
        "epoch": 0.9311678267594741,
        "step": 7224
    },
    {
        "loss": 1.8473,
        "grad_norm": 1.618959903717041,
        "learning_rate": 1.0232207088746382e-05,
        "epoch": 0.931296725960299,
        "step": 7225
    },
    {
        "loss": 1.3912,
        "grad_norm": 2.618467092514038,
        "learning_rate": 1.0195346714717813e-05,
        "epoch": 0.931425625161124,
        "step": 7226
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.4169790744781494,
        "learning_rate": 1.0158545314274071e-05,
        "epoch": 0.931554524361949,
        "step": 7227
    },
    {
        "loss": 2.2051,
        "grad_norm": 2.5255231857299805,
        "learning_rate": 1.0121802941939212e-05,
        "epoch": 0.9316834235627739,
        "step": 7228
    },
    {
        "loss": 1.6663,
        "grad_norm": 3.216000556945801,
        "learning_rate": 1.0085119652149805e-05,
        "epoch": 0.9318123227635988,
        "step": 7229
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.953172445297241,
        "learning_rate": 1.00484954992549e-05,
        "epoch": 0.9319412219644239,
        "step": 7230
    },
    {
        "loss": 2.1311,
        "grad_norm": 2.439547538757324,
        "learning_rate": 1.0011930537515951e-05,
        "epoch": 0.9320701211652488,
        "step": 7231
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.3300600051879883,
        "learning_rate": 9.975424821106666e-06,
        "epoch": 0.9321990203660737,
        "step": 7232
    },
    {
        "loss": 1.4646,
        "grad_norm": 2.4115231037139893,
        "learning_rate": 9.938978404113032e-06,
        "epoch": 0.9323279195668986,
        "step": 7233
    },
    {
        "loss": 1.5249,
        "grad_norm": 2.803622245788574,
        "learning_rate": 9.902591340533152e-06,
        "epoch": 0.9324568187677237,
        "step": 7234
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.9806742668151855,
        "learning_rate": 9.866263684277177e-06,
        "epoch": 0.9325857179685486,
        "step": 7235
    },
    {
        "loss": 1.5212,
        "grad_norm": 3.243475914001465,
        "learning_rate": 9.82999548916727e-06,
        "epoch": 0.9327146171693735,
        "step": 7236
    },
    {
        "loss": 2.0512,
        "grad_norm": 2.5530996322631836,
        "learning_rate": 9.793786808937489e-06,
        "epoch": 0.9328435163701985,
        "step": 7237
    },
    {
        "loss": 1.9625,
        "grad_norm": 1.4441086053848267,
        "learning_rate": 9.757637697233723e-06,
        "epoch": 0.9329724155710235,
        "step": 7238
    },
    {
        "loss": 2.2785,
        "grad_norm": 2.2282087802886963,
        "learning_rate": 9.721548207613584e-06,
        "epoch": 0.9331013147718484,
        "step": 7239
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.150585412979126,
        "learning_rate": 9.68551839354636e-06,
        "epoch": 0.9332302139726734,
        "step": 7240
    },
    {
        "loss": 2.0773,
        "grad_norm": 1.7372338771820068,
        "learning_rate": 9.649548308412898e-06,
        "epoch": 0.9333591131734983,
        "step": 7241
    },
    {
        "loss": 1.4415,
        "grad_norm": 2.2088911533355713,
        "learning_rate": 9.61363800550571e-06,
        "epoch": 0.9334880123743233,
        "step": 7242
    },
    {
        "loss": 2.0433,
        "grad_norm": 2.5385067462921143,
        "learning_rate": 9.577787538028465e-06,
        "epoch": 0.9336169115751483,
        "step": 7243
    },
    {
        "loss": 1.7465,
        "grad_norm": 1.8728801012039185,
        "learning_rate": 9.541996959096377e-06,
        "epoch": 0.9337458107759732,
        "step": 7244
    },
    {
        "loss": 1.8097,
        "grad_norm": 2.2990753650665283,
        "learning_rate": 9.506266321735891e-06,
        "epoch": 0.9338747099767981,
        "step": 7245
    },
    {
        "loss": 2.1347,
        "grad_norm": 2.3679463863372803,
        "learning_rate": 9.470595678884603e-06,
        "epoch": 0.9340036091776232,
        "step": 7246
    },
    {
        "loss": 2.034,
        "grad_norm": 2.0253031253814697,
        "learning_rate": 9.434985083391223e-06,
        "epoch": 0.9341325083784481,
        "step": 7247
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.7684454917907715,
        "learning_rate": 9.399434588015659e-06,
        "epoch": 0.934261407579273,
        "step": 7248
    },
    {
        "loss": 1.4777,
        "grad_norm": 2.757983922958374,
        "learning_rate": 9.363944245428502e-06,
        "epoch": 0.9343903067800979,
        "step": 7249
    },
    {
        "loss": 1.7915,
        "grad_norm": 2.465555191040039,
        "learning_rate": 9.3285141082114e-06,
        "epoch": 0.934519205980923,
        "step": 7250
    },
    {
        "loss": 1.8445,
        "grad_norm": 2.133470296859741,
        "learning_rate": 9.293144228856792e-06,
        "epoch": 0.9346481051817479,
        "step": 7251
    },
    {
        "loss": 1.4935,
        "grad_norm": 3.049621343612671,
        "learning_rate": 9.257834659767772e-06,
        "epoch": 0.9347770043825728,
        "step": 7252
    },
    {
        "loss": 2.0653,
        "grad_norm": 2.4558045864105225,
        "learning_rate": 9.222585453258104e-06,
        "epoch": 0.9349059035833978,
        "step": 7253
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.579245090484619,
        "learning_rate": 9.18739666155225e-06,
        "epoch": 0.9350348027842227,
        "step": 7254
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.972022294998169,
        "learning_rate": 9.152268336784941e-06,
        "epoch": 0.9351637019850477,
        "step": 7255
    },
    {
        "loss": 2.234,
        "grad_norm": 2.337948799133301,
        "learning_rate": 9.117200531001446e-06,
        "epoch": 0.9352926011858727,
        "step": 7256
    },
    {
        "loss": 1.916,
        "grad_norm": 4.049737453460693,
        "learning_rate": 9.082193296157332e-06,
        "epoch": 0.9354215003866976,
        "step": 7257
    },
    {
        "loss": 1.0052,
        "grad_norm": 3.5766944885253906,
        "learning_rate": 9.047246684118572e-06,
        "epoch": 0.9355503995875225,
        "step": 7258
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.6396052837371826,
        "learning_rate": 9.01236074666102e-06,
        "epoch": 0.9356792987883475,
        "step": 7259
    },
    {
        "loss": 1.6069,
        "grad_norm": 2.3147830963134766,
        "learning_rate": 8.977535535470943e-06,
        "epoch": 0.9358081979891725,
        "step": 7260
    },
    {
        "loss": 2.4369,
        "grad_norm": 1.7551394701004028,
        "learning_rate": 8.942771102144515e-06,
        "epoch": 0.9359370971899974,
        "step": 7261
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.243562936782837,
        "learning_rate": 8.908067498187761e-06,
        "epoch": 0.9360659963908223,
        "step": 7262
    },
    {
        "loss": 1.9375,
        "grad_norm": 3.699279546737671,
        "learning_rate": 8.873424775016686e-06,
        "epoch": 0.9361948955916474,
        "step": 7263
    },
    {
        "loss": 0.7848,
        "grad_norm": 4.000422477722168,
        "learning_rate": 8.838842983957208e-06,
        "epoch": 0.9363237947924723,
        "step": 7264
    },
    {
        "loss": 1.833,
        "grad_norm": 1.7825915813446045,
        "learning_rate": 8.8043221762447e-06,
        "epoch": 0.9364526939932972,
        "step": 7265
    },
    {
        "loss": 2.2449,
        "grad_norm": 2.388984203338623,
        "learning_rate": 8.76986240302443e-06,
        "epoch": 0.9365815931941222,
        "step": 7266
    },
    {
        "loss": 2.1593,
        "grad_norm": 1.6497256755828857,
        "learning_rate": 8.73546371535115e-06,
        "epoch": 0.9367104923949472,
        "step": 7267
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.7794559001922607,
        "learning_rate": 8.70112616418904e-06,
        "epoch": 0.9368393915957721,
        "step": 7268
    },
    {
        "loss": 1.8288,
        "grad_norm": 1.721704363822937,
        "learning_rate": 8.666849800411731e-06,
        "epoch": 0.936968290796597,
        "step": 7269
    },
    {
        "loss": 1.7001,
        "grad_norm": 2.530963659286499,
        "learning_rate": 8.632634674802364e-06,
        "epoch": 0.937097189997422,
        "step": 7270
    },
    {
        "loss": 1.0928,
        "grad_norm": 2.9797253608703613,
        "learning_rate": 8.598480838053064e-06,
        "epoch": 0.937226089198247,
        "step": 7271
    },
    {
        "loss": 2.4398,
        "grad_norm": 1.4962339401245117,
        "learning_rate": 8.564388340765433e-06,
        "epoch": 0.9373549883990719,
        "step": 7272
    },
    {
        "loss": 1.4962,
        "grad_norm": 2.8824286460876465,
        "learning_rate": 8.53035723345001e-06,
        "epoch": 0.9374838875998969,
        "step": 7273
    },
    {
        "loss": 1.2637,
        "grad_norm": 4.642855167388916,
        "learning_rate": 8.496387566526476e-06,
        "epoch": 0.9376127868007218,
        "step": 7274
    },
    {
        "loss": 1.6823,
        "grad_norm": 2.3001816272735596,
        "learning_rate": 8.462479390323358e-06,
        "epoch": 0.9377416860015468,
        "step": 7275
    },
    {
        "loss": 1.9274,
        "grad_norm": 1.7790471315383911,
        "learning_rate": 8.428632755078243e-06,
        "epoch": 0.9378705852023718,
        "step": 7276
    },
    {
        "loss": 1.6847,
        "grad_norm": 2.6561501026153564,
        "learning_rate": 8.394847710937448e-06,
        "epoch": 0.9379994844031967,
        "step": 7277
    },
    {
        "loss": 2.0779,
        "grad_norm": 1.7160539627075195,
        "learning_rate": 8.361124307956053e-06,
        "epoch": 0.9381283836040216,
        "step": 7278
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.068861722946167,
        "learning_rate": 8.327462596097818e-06,
        "epoch": 0.9382572828048467,
        "step": 7279
    },
    {
        "loss": 2.1477,
        "grad_norm": 1.8995425701141357,
        "learning_rate": 8.293862625235122e-06,
        "epoch": 0.9383861820056716,
        "step": 7280
    },
    {
        "loss": 2.011,
        "grad_norm": 2.328157424926758,
        "learning_rate": 8.260324445148743e-06,
        "epoch": 0.9385150812064965,
        "step": 7281
    },
    {
        "loss": 1.5821,
        "grad_norm": 2.278114080429077,
        "learning_rate": 8.226848105528134e-06,
        "epoch": 0.9386439804073214,
        "step": 7282
    },
    {
        "loss": 1.6388,
        "grad_norm": 2.274808168411255,
        "learning_rate": 8.19343365597095e-06,
        "epoch": 0.9387728796081465,
        "step": 7283
    },
    {
        "loss": 1.8711,
        "grad_norm": 2.0390522480010986,
        "learning_rate": 8.160081145983228e-06,
        "epoch": 0.9389017788089714,
        "step": 7284
    },
    {
        "loss": 2.4441,
        "grad_norm": 1.4924522638320923,
        "learning_rate": 8.126790624979202e-06,
        "epoch": 0.9390306780097963,
        "step": 7285
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.633054256439209,
        "learning_rate": 8.093562142281346e-06,
        "epoch": 0.9391595772106213,
        "step": 7286
    },
    {
        "loss": 1.5058,
        "grad_norm": 1.9628126621246338,
        "learning_rate": 8.060395747120008e-06,
        "epoch": 0.9392884764114462,
        "step": 7287
    },
    {
        "loss": 1.6268,
        "grad_norm": 2.608673095703125,
        "learning_rate": 8.027291488633821e-06,
        "epoch": 0.9394173756122712,
        "step": 7288
    },
    {
        "loss": 1.7819,
        "grad_norm": 3.7211880683898926,
        "learning_rate": 7.994249415869209e-06,
        "epoch": 0.9395462748130962,
        "step": 7289
    },
    {
        "loss": 1.8981,
        "grad_norm": 2.0515055656433105,
        "learning_rate": 7.961269577780457e-06,
        "epoch": 0.9396751740139211,
        "step": 7290
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.2268319129943848,
        "learning_rate": 7.928352023229684e-06,
        "epoch": 0.939804073214746,
        "step": 7291
    },
    {
        "loss": 1.9802,
        "grad_norm": 1.8114376068115234,
        "learning_rate": 7.89549680098674e-06,
        "epoch": 0.939932972415571,
        "step": 7292
    },
    {
        "loss": 1.9,
        "grad_norm": 2.303680419921875,
        "learning_rate": 7.862703959729078e-06,
        "epoch": 0.940061871616396,
        "step": 7293
    },
    {
        "loss": 2.3705,
        "grad_norm": 1.7152037620544434,
        "learning_rate": 7.82997354804177e-06,
        "epoch": 0.9401907708172209,
        "step": 7294
    },
    {
        "loss": 2.0542,
        "grad_norm": 2.5804593563079834,
        "learning_rate": 7.797305614417372e-06,
        "epoch": 0.9403196700180458,
        "step": 7295
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.9919755458831787,
        "learning_rate": 7.764700207255903e-06,
        "epoch": 0.9404485692188709,
        "step": 7296
    },
    {
        "loss": 1.4861,
        "grad_norm": 2.662449836730957,
        "learning_rate": 7.732157374864701e-06,
        "epoch": 0.9405774684196958,
        "step": 7297
    },
    {
        "loss": 1.8981,
        "grad_norm": 3.2828896045684814,
        "learning_rate": 7.699677165458419e-06,
        "epoch": 0.9407063676205207,
        "step": 7298
    },
    {
        "loss": 2.148,
        "grad_norm": 1.7496609687805176,
        "learning_rate": 7.667259627158952e-06,
        "epoch": 0.9408352668213457,
        "step": 7299
    },
    {
        "loss": 0.5354,
        "grad_norm": 1.5836949348449707,
        "learning_rate": 7.634904807995285e-06,
        "epoch": 0.9409641660221707,
        "step": 7300
    },
    {
        "loss": 1.3919,
        "grad_norm": 2.4434831142425537,
        "learning_rate": 7.602612755903537e-06,
        "epoch": 0.9410930652229956,
        "step": 7301
    },
    {
        "loss": 1.6514,
        "grad_norm": 3.6556832790374756,
        "learning_rate": 7.5703835187268094e-06,
        "epoch": 0.9412219644238206,
        "step": 7302
    },
    {
        "loss": 2.3263,
        "grad_norm": 1.8340182304382324,
        "learning_rate": 7.5382171442151206e-06,
        "epoch": 0.9413508636246455,
        "step": 7303
    },
    {
        "loss": 2.1682,
        "grad_norm": 2.401066541671753,
        "learning_rate": 7.5061136800253895e-06,
        "epoch": 0.9414797628254705,
        "step": 7304
    },
    {
        "loss": 1.8434,
        "grad_norm": 1.8622742891311646,
        "learning_rate": 7.474073173721297e-06,
        "epoch": 0.9416086620262955,
        "step": 7305
    },
    {
        "loss": 2.0843,
        "grad_norm": 2.7352821826934814,
        "learning_rate": 7.442095672773286e-06,
        "epoch": 0.9417375612271204,
        "step": 7306
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.2285916805267334,
        "learning_rate": 7.410181224558393e-06,
        "epoch": 0.9418664604279453,
        "step": 7307
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.1121118068695068,
        "learning_rate": 7.378329876360257e-06,
        "epoch": 0.9419953596287703,
        "step": 7308
    },
    {
        "loss": 2.2983,
        "grad_norm": 1.8992090225219727,
        "learning_rate": 7.346541675369151e-06,
        "epoch": 0.9421242588295953,
        "step": 7309
    },
    {
        "loss": 1.5893,
        "grad_norm": 2.7416887283325195,
        "learning_rate": 7.314816668681562e-06,
        "epoch": 0.9422531580304202,
        "step": 7310
    },
    {
        "loss": 1.7255,
        "grad_norm": 3.2391648292541504,
        "learning_rate": 7.283154903300515e-06,
        "epoch": 0.9423820572312451,
        "step": 7311
    },
    {
        "loss": 1.7386,
        "grad_norm": 2.954718828201294,
        "learning_rate": 7.251556426135292e-06,
        "epoch": 0.9425109564320702,
        "step": 7312
    },
    {
        "loss": 2.5073,
        "grad_norm": 1.683640480041504,
        "learning_rate": 7.220021284001432e-06,
        "epoch": 0.9426398556328951,
        "step": 7313
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.383989095687866,
        "learning_rate": 7.1885495236205525e-06,
        "epoch": 0.94276875483372,
        "step": 7314
    },
    {
        "loss": 2.056,
        "grad_norm": 2.3186495304107666,
        "learning_rate": 7.157141191620548e-06,
        "epoch": 0.942897654034545,
        "step": 7315
    },
    {
        "loss": 2.1067,
        "grad_norm": 1.9375869035720825,
        "learning_rate": 7.125796334535128e-06,
        "epoch": 0.94302655323537,
        "step": 7316
    },
    {
        "loss": 2.4766,
        "grad_norm": 1.8163410425186157,
        "learning_rate": 7.09451499880407e-06,
        "epoch": 0.9431554524361949,
        "step": 7317
    },
    {
        "loss": 1.7,
        "grad_norm": 2.4805526733398438,
        "learning_rate": 7.06329723077303e-06,
        "epoch": 0.9432843516370198,
        "step": 7318
    },
    {
        "loss": 1.9885,
        "grad_norm": 1.4401726722717285,
        "learning_rate": 7.032143076693493e-06,
        "epoch": 0.9434132508378448,
        "step": 7319
    },
    {
        "loss": 1.4782,
        "grad_norm": 3.2334678173065186,
        "learning_rate": 7.001052582722622e-06,
        "epoch": 0.9435421500386698,
        "step": 7320
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.1908137798309326,
        "learning_rate": 6.970025794923429e-06,
        "epoch": 0.9436710492394947,
        "step": 7321
    },
    {
        "loss": 1.7875,
        "grad_norm": 2.9920918941497803,
        "learning_rate": 6.939062759264336e-06,
        "epoch": 0.9437999484403197,
        "step": 7322
    },
    {
        "loss": 1.9054,
        "grad_norm": 2.869246482849121,
        "learning_rate": 6.908163521619448e-06,
        "epoch": 0.9439288476411446,
        "step": 7323
    },
    {
        "loss": 2.1785,
        "grad_norm": 2.3617823123931885,
        "learning_rate": 6.877328127768251e-06,
        "epoch": 0.9440577468419695,
        "step": 7324
    },
    {
        "loss": 1.3526,
        "grad_norm": 4.39805269241333,
        "learning_rate": 6.8465566233958055e-06,
        "epoch": 0.9441866460427946,
        "step": 7325
    },
    {
        "loss": 2.0097,
        "grad_norm": 2.6271119117736816,
        "learning_rate": 6.815849054092288e-06,
        "epoch": 0.9443155452436195,
        "step": 7326
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.06278395652771,
        "learning_rate": 6.7852054653533666e-06,
        "epoch": 0.9444444444444444,
        "step": 7327
    },
    {
        "loss": 2.1255,
        "grad_norm": 1.322677493095398,
        "learning_rate": 6.754625902579803e-06,
        "epoch": 0.9445733436452693,
        "step": 7328
    },
    {
        "loss": 1.0742,
        "grad_norm": 2.997807502746582,
        "learning_rate": 6.7241104110774666e-06,
        "epoch": 0.9447022428460944,
        "step": 7329
    },
    {
        "loss": 2.0099,
        "grad_norm": 3.1598517894744873,
        "learning_rate": 6.6936590360573535e-06,
        "epoch": 0.9448311420469193,
        "step": 7330
    },
    {
        "loss": 1.8015,
        "grad_norm": 2.860102653503418,
        "learning_rate": 6.663271822635547e-06,
        "epoch": 0.9449600412477442,
        "step": 7331
    },
    {
        "loss": 2.1324,
        "grad_norm": 2.0778491497039795,
        "learning_rate": 6.632948815832868e-06,
        "epoch": 0.9450889404485692,
        "step": 7332
    },
    {
        "loss": 2.0035,
        "grad_norm": 2.629434108734131,
        "learning_rate": 6.602690060575234e-06,
        "epoch": 0.9452178396493942,
        "step": 7333
    },
    {
        "loss": 1.6012,
        "grad_norm": 3.6912670135498047,
        "learning_rate": 6.572495601693246e-06,
        "epoch": 0.9453467388502191,
        "step": 7334
    },
    {
        "loss": 1.6796,
        "grad_norm": 2.6545603275299072,
        "learning_rate": 6.5423654839222085e-06,
        "epoch": 0.9454756380510441,
        "step": 7335
    },
    {
        "loss": 1.3784,
        "grad_norm": 2.898348569869995,
        "learning_rate": 6.512299751902151e-06,
        "epoch": 0.945604537251869,
        "step": 7336
    },
    {
        "loss": 1.8381,
        "grad_norm": 1.8073488473892212,
        "learning_rate": 6.482298450177832e-06,
        "epoch": 0.945733436452694,
        "step": 7337
    },
    {
        "loss": 2.195,
        "grad_norm": 1.9960631132125854,
        "learning_rate": 6.452361623198272e-06,
        "epoch": 0.945862335653519,
        "step": 7338
    },
    {
        "loss": 1.5741,
        "grad_norm": 3.28853178024292,
        "learning_rate": 6.422489315317248e-06,
        "epoch": 0.9459912348543439,
        "step": 7339
    },
    {
        "loss": 2.0903,
        "grad_norm": 2.3934643268585205,
        "learning_rate": 6.392681570792797e-06,
        "epoch": 0.9461201340551688,
        "step": 7340
    },
    {
        "loss": 1.5837,
        "grad_norm": 2.343904733657837,
        "learning_rate": 6.3629384337873574e-06,
        "epoch": 0.9462490332559939,
        "step": 7341
    },
    {
        "loss": 1.6026,
        "grad_norm": 2.8580331802368164,
        "learning_rate": 6.33325994836752e-06,
        "epoch": 0.9463779324568188,
        "step": 7342
    },
    {
        "loss": 2.1402,
        "grad_norm": 2.2792959213256836,
        "learning_rate": 6.303646158504295e-06,
        "epoch": 0.9465068316576437,
        "step": 7343
    },
    {
        "loss": 1.9619,
        "grad_norm": 3.2879512310028076,
        "learning_rate": 6.2740971080727185e-06,
        "epoch": 0.9466357308584686,
        "step": 7344
    },
    {
        "loss": 2.1684,
        "grad_norm": 2.5866241455078125,
        "learning_rate": 6.244612840851899e-06,
        "epoch": 0.9467646300592937,
        "step": 7345
    },
    {
        "loss": 2.2407,
        "grad_norm": 2.3864850997924805,
        "learning_rate": 6.215193400525016e-06,
        "epoch": 0.9468935292601186,
        "step": 7346
    },
    {
        "loss": 1.2263,
        "grad_norm": 2.6983561515808105,
        "learning_rate": 6.1858388306792134e-06,
        "epoch": 0.9470224284609435,
        "step": 7347
    },
    {
        "loss": 2.2663,
        "grad_norm": 2.202303409576416,
        "learning_rate": 6.156549174805382e-06,
        "epoch": 0.9471513276617685,
        "step": 7348
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.699100136756897,
        "learning_rate": 6.127324476298451e-06,
        "epoch": 0.9472802268625935,
        "step": 7349
    },
    {
        "loss": 1.7043,
        "grad_norm": 2.466296911239624,
        "learning_rate": 6.098164778456988e-06,
        "epoch": 0.9474091260634184,
        "step": 7350
    },
    {
        "loss": 1.8501,
        "grad_norm": 2.25630784034729,
        "learning_rate": 6.069070124483251e-06,
        "epoch": 0.9475380252642434,
        "step": 7351
    },
    {
        "loss": 1.2208,
        "grad_norm": 2.415048837661743,
        "learning_rate": 6.040040557483173e-06,
        "epoch": 0.9476669244650683,
        "step": 7352
    },
    {
        "loss": 2.4461,
        "grad_norm": 1.4680354595184326,
        "learning_rate": 6.011076120466274e-06,
        "epoch": 0.9477958236658933,
        "step": 7353
    },
    {
        "loss": 1.8354,
        "grad_norm": 2.7655179500579834,
        "learning_rate": 5.9821768563454336e-06,
        "epoch": 0.9479247228667183,
        "step": 7354
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.8999474048614502,
        "learning_rate": 5.9533428079371846e-06,
        "epoch": 0.9480536220675432,
        "step": 7355
    },
    {
        "loss": 1.7834,
        "grad_norm": 1.9644583463668823,
        "learning_rate": 5.92457401796131e-06,
        "epoch": 0.9481825212683681,
        "step": 7356
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.371133327484131,
        "learning_rate": 5.895870529040937e-06,
        "epoch": 0.9483114204691931,
        "step": 7357
    },
    {
        "loss": 1.274,
        "grad_norm": 2.754605531692505,
        "learning_rate": 5.8672323837024255e-06,
        "epoch": 0.9484403196700181,
        "step": 7358
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.5174717903137207,
        "learning_rate": 5.838659624375348e-06,
        "epoch": 0.948569218870843,
        "step": 7359
    },
    {
        "loss": 1.6553,
        "grad_norm": 1.8293195962905884,
        "learning_rate": 5.810152293392407e-06,
        "epoch": 0.9486981180716679,
        "step": 7360
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.515885591506958,
        "learning_rate": 5.781710432989334e-06,
        "epoch": 0.9488270172724929,
        "step": 7361
    },
    {
        "loss": 1.7198,
        "grad_norm": 2.0295863151550293,
        "learning_rate": 5.75333408530489e-06,
        "epoch": 0.9489559164733179,
        "step": 7362
    },
    {
        "loss": 1.8093,
        "grad_norm": 1.7560375928878784,
        "learning_rate": 5.725023292380771e-06,
        "epoch": 0.9490848156741428,
        "step": 7363
    },
    {
        "loss": 2.046,
        "grad_norm": 2.606947660446167,
        "learning_rate": 5.696778096161553e-06,
        "epoch": 0.9492137148749678,
        "step": 7364
    },
    {
        "loss": 1.2833,
        "grad_norm": 2.613461494445801,
        "learning_rate": 5.668598538494591e-06,
        "epoch": 0.9493426140757927,
        "step": 7365
    },
    {
        "loss": 2.523,
        "grad_norm": 2.104022264480591,
        "learning_rate": 5.64048466113003e-06,
        "epoch": 0.9494715132766177,
        "step": 7366
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.8724656105041504,
        "learning_rate": 5.612436505720709e-06,
        "epoch": 0.9496004124774426,
        "step": 7367
    },
    {
        "loss": 2.5073,
        "grad_norm": 2.2867367267608643,
        "learning_rate": 5.584454113822047e-06,
        "epoch": 0.9497293116782676,
        "step": 7368
    },
    {
        "loss": 2.5043,
        "grad_norm": 1.518858790397644,
        "learning_rate": 5.556537526892081e-06,
        "epoch": 0.9498582108790925,
        "step": 7369
    },
    {
        "loss": 1.9918,
        "grad_norm": 8.907081604003906,
        "learning_rate": 5.52868678629132e-06,
        "epoch": 0.9499871100799175,
        "step": 7370
    },
    {
        "loss": 2.3523,
        "grad_norm": 2.806771993637085,
        "learning_rate": 5.500901933282748e-06,
        "epoch": 0.9501160092807425,
        "step": 7371
    },
    {
        "loss": 1.7209,
        "grad_norm": 2.079958200454712,
        "learning_rate": 5.47318300903169e-06,
        "epoch": 0.9502449084815674,
        "step": 7372
    },
    {
        "loss": 1.4249,
        "grad_norm": 3.225719928741455,
        "learning_rate": 5.445530054605824e-06,
        "epoch": 0.9503738076823923,
        "step": 7373
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.4826250076293945,
        "learning_rate": 5.4179431109750875e-06,
        "epoch": 0.9505027068832174,
        "step": 7374
    },
    {
        "loss": 2.0505,
        "grad_norm": 1.4607045650482178,
        "learning_rate": 5.390422219011571e-06,
        "epoch": 0.9506316060840423,
        "step": 7375
    },
    {
        "loss": 1.9545,
        "grad_norm": 1.658836007118225,
        "learning_rate": 5.362967419489634e-06,
        "epoch": 0.9507605052848672,
        "step": 7376
    },
    {
        "loss": 2.2069,
        "grad_norm": 2.056828022003174,
        "learning_rate": 5.335578753085546e-06,
        "epoch": 0.9508894044856921,
        "step": 7377
    },
    {
        "loss": 1.7486,
        "grad_norm": 4.00571346282959,
        "learning_rate": 5.308256260377687e-06,
        "epoch": 0.9510183036865172,
        "step": 7378
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.6323438882827759,
        "learning_rate": 5.280999981846391e-06,
        "epoch": 0.9511472028873421,
        "step": 7379
    },
    {
        "loss": 2.2806,
        "grad_norm": 1.7032946348190308,
        "learning_rate": 5.253809957873884e-06,
        "epoch": 0.951276102088167,
        "step": 7380
    },
    {
        "loss": 1.0086,
        "grad_norm": 3.1743831634521484,
        "learning_rate": 5.22668622874421e-06,
        "epoch": 0.951405001288992,
        "step": 7381
    },
    {
        "loss": 2.1941,
        "grad_norm": 2.5227744579315186,
        "learning_rate": 5.199628834643283e-06,
        "epoch": 0.951533900489817,
        "step": 7382
    },
    {
        "loss": 1.2268,
        "grad_norm": 2.950176477432251,
        "learning_rate": 5.1726378156585816e-06,
        "epoch": 0.9516627996906419,
        "step": 7383
    },
    {
        "loss": 2.051,
        "grad_norm": 2.5052073001861572,
        "learning_rate": 5.145713211779374e-06,
        "epoch": 0.9517916988914669,
        "step": 7384
    },
    {
        "loss": 1.9709,
        "grad_norm": 2.4634854793548584,
        "learning_rate": 5.118855062896455e-06,
        "epoch": 0.9519205980922918,
        "step": 7385
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.0702648162841797,
        "learning_rate": 5.0920634088022115e-06,
        "epoch": 0.9520494972931168,
        "step": 7386
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.98585045337677,
        "learning_rate": 5.065338289190452e-06,
        "epoch": 0.9521783964939418,
        "step": 7387
    },
    {
        "loss": 1.8023,
        "grad_norm": 2.2710907459259033,
        "learning_rate": 5.038679743656527e-06,
        "epoch": 0.9523072956947667,
        "step": 7388
    },
    {
        "loss": 2.1653,
        "grad_norm": 1.3487337827682495,
        "learning_rate": 5.012087811697053e-06,
        "epoch": 0.9524361948955916,
        "step": 7389
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.3833444118499756,
        "learning_rate": 4.98556253270992e-06,
        "epoch": 0.9525650940964167,
        "step": 7390
    },
    {
        "loss": 0.8142,
        "grad_norm": 2.6302058696746826,
        "learning_rate": 4.959103945994342e-06,
        "epoch": 0.9526939932972416,
        "step": 7391
    },
    {
        "loss": 1.7517,
        "grad_norm": 2.582735061645508,
        "learning_rate": 4.932712090750763e-06,
        "epoch": 0.9528228924980665,
        "step": 7392
    },
    {
        "loss": 1.507,
        "grad_norm": 2.8538427352905273,
        "learning_rate": 4.9063870060805925e-06,
        "epoch": 0.9529517916988914,
        "step": 7393
    },
    {
        "loss": 1.4442,
        "grad_norm": 2.452451229095459,
        "learning_rate": 4.880128730986521e-06,
        "epoch": 0.9530806908997165,
        "step": 7394
    },
    {
        "loss": 1.7548,
        "grad_norm": 3.060519218444824,
        "learning_rate": 4.853937304372152e-06,
        "epoch": 0.9532095901005414,
        "step": 7395
    },
    {
        "loss": 1.7566,
        "grad_norm": 2.5263421535491943,
        "learning_rate": 4.827812765041978e-06,
        "epoch": 0.9533384893013663,
        "step": 7396
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.6061618328094482,
        "learning_rate": 4.801755151701487e-06,
        "epoch": 0.9534673885021913,
        "step": 7397
    },
    {
        "loss": 2.1259,
        "grad_norm": 1.7436696290969849,
        "learning_rate": 4.77576450295707e-06,
        "epoch": 0.9535962877030162,
        "step": 7398
    },
    {
        "loss": 2.0194,
        "grad_norm": 1.641481876373291,
        "learning_rate": 4.749840857315707e-06,
        "epoch": 0.9537251869038412,
        "step": 7399
    },
    {
        "loss": 1.317,
        "grad_norm": 3.444387197494507,
        "learning_rate": 4.723984253185315e-06,
        "epoch": 0.9538540861046662,
        "step": 7400
    },
    {
        "loss": 2.4715,
        "grad_norm": 1.4269613027572632,
        "learning_rate": 4.698194728874405e-06,
        "epoch": 0.9539829853054911,
        "step": 7401
    },
    {
        "loss": 1.8708,
        "grad_norm": 2.261918067932129,
        "learning_rate": 4.67247232259202e-06,
        "epoch": 0.954111884506316,
        "step": 7402
    },
    {
        "loss": 2.826,
        "grad_norm": 1.5027803182601929,
        "learning_rate": 4.646817072447851e-06,
        "epoch": 0.954240783707141,
        "step": 7403
    },
    {
        "loss": 1.1155,
        "grad_norm": 2.7378978729248047,
        "learning_rate": 4.621229016452172e-06,
        "epoch": 0.954369682907966,
        "step": 7404
    },
    {
        "loss": 2.2676,
        "grad_norm": 2.6946167945861816,
        "learning_rate": 4.5957081925154746e-06,
        "epoch": 0.9544985821087909,
        "step": 7405
    },
    {
        "loss": 2.158,
        "grad_norm": 1.6363208293914795,
        "learning_rate": 4.570254638448878e-06,
        "epoch": 0.9546274813096158,
        "step": 7406
    },
    {
        "loss": 2.312,
        "grad_norm": 2.1290690898895264,
        "learning_rate": 4.544868391963697e-06,
        "epoch": 0.9547563805104409,
        "step": 7407
    },
    {
        "loss": 1.2478,
        "grad_norm": 3.426569938659668,
        "learning_rate": 4.519549490671609e-06,
        "epoch": 0.9548852797112658,
        "step": 7408
    },
    {
        "loss": 2.234,
        "grad_norm": 2.4909558296203613,
        "learning_rate": 4.4942979720843655e-06,
        "epoch": 0.9550141789120907,
        "step": 7409
    },
    {
        "loss": 1.0106,
        "grad_norm": 3.0281107425689697,
        "learning_rate": 4.469113873614084e-06,
        "epoch": 0.9551430781129157,
        "step": 7410
    },
    {
        "loss": 2.0556,
        "grad_norm": 1.7308319807052612,
        "learning_rate": 4.4439972325728605e-06,
        "epoch": 0.9552719773137407,
        "step": 7411
    },
    {
        "loss": 2.061,
        "grad_norm": 2.5169029235839844,
        "learning_rate": 4.418948086172903e-06,
        "epoch": 0.9554008765145656,
        "step": 7412
    },
    {
        "loss": 1.3241,
        "grad_norm": 2.8926219940185547,
        "learning_rate": 4.393966471526384e-06,
        "epoch": 0.9555297757153906,
        "step": 7413
    },
    {
        "loss": 1.6796,
        "grad_norm": 2.6601812839508057,
        "learning_rate": 4.369052425645481e-06,
        "epoch": 0.9556586749162155,
        "step": 7414
    },
    {
        "loss": 1.6613,
        "grad_norm": 3.2940711975097656,
        "learning_rate": 4.344205985442157e-06,
        "epoch": 0.9557875741170405,
        "step": 7415
    },
    {
        "loss": 2.306,
        "grad_norm": 1.394042730331421,
        "learning_rate": 4.319427187728331e-06,
        "epoch": 0.9559164733178654,
        "step": 7416
    },
    {
        "loss": 1.5423,
        "grad_norm": 2.652047634124756,
        "learning_rate": 4.294716069215643e-06,
        "epoch": 0.9560453725186904,
        "step": 7417
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.9763617515563965,
        "learning_rate": 4.2700726665154786e-06,
        "epoch": 0.9561742717195153,
        "step": 7418
    },
    {
        "loss": 1.5885,
        "grad_norm": 2.2525291442871094,
        "learning_rate": 4.2454970161388774e-06,
        "epoch": 0.9563031709203403,
        "step": 7419
    },
    {
        "loss": 1.8512,
        "grad_norm": 2.6018354892730713,
        "learning_rate": 4.220989154496546e-06,
        "epoch": 0.9564320701211653,
        "step": 7420
    },
    {
        "loss": 1.803,
        "grad_norm": 2.4439802169799805,
        "learning_rate": 4.196549117898641e-06,
        "epoch": 0.9565609693219902,
        "step": 7421
    },
    {
        "loss": 1.6473,
        "grad_norm": 2.5443716049194336,
        "learning_rate": 4.172176942555001e-06,
        "epoch": 0.9566898685228151,
        "step": 7422
    },
    {
        "loss": 1.4012,
        "grad_norm": 2.976816177368164,
        "learning_rate": 4.147872664574792e-06,
        "epoch": 0.9568187677236402,
        "step": 7423
    },
    {
        "loss": 1.5997,
        "grad_norm": 2.323298931121826,
        "learning_rate": 4.12363631996664e-06,
        "epoch": 0.9569476669244651,
        "step": 7424
    },
    {
        "loss": 1.8172,
        "grad_norm": 2.4456112384796143,
        "learning_rate": 4.099467944638513e-06,
        "epoch": 0.95707656612529,
        "step": 7425
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.2994260787963867,
        "learning_rate": 4.075367574397665e-06,
        "epoch": 0.957205465326115,
        "step": 7426
    },
    {
        "loss": 2.2051,
        "grad_norm": 2.5188047885894775,
        "learning_rate": 4.051335244950611e-06,
        "epoch": 0.95733436452694,
        "step": 7427
    },
    {
        "loss": 1.7923,
        "grad_norm": 2.3983922004699707,
        "learning_rate": 4.027370991903051e-06,
        "epoch": 0.9574632637277649,
        "step": 7428
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.192254066467285,
        "learning_rate": 4.003474850759836e-06,
        "epoch": 0.9575921629285898,
        "step": 7429
    },
    {
        "loss": 1.9158,
        "grad_norm": 4.352578163146973,
        "learning_rate": 3.979646856924879e-06,
        "epoch": 0.9577210621294148,
        "step": 7430
    },
    {
        "loss": 1.822,
        "grad_norm": 5.188709259033203,
        "learning_rate": 3.955887045701151e-06,
        "epoch": 0.9578499613302398,
        "step": 7431
    },
    {
        "loss": 2.0477,
        "grad_norm": 1.4262241125106812,
        "learning_rate": 3.9321954522906105e-06,
        "epoch": 0.9579788605310647,
        "step": 7432
    },
    {
        "loss": 1.7906,
        "grad_norm": 4.740821838378906,
        "learning_rate": 3.908572111794106e-06,
        "epoch": 0.9581077597318897,
        "step": 7433
    },
    {
        "loss": 1.8403,
        "grad_norm": 2.5443782806396484,
        "learning_rate": 3.8850170592114095e-06,
        "epoch": 0.9582366589327146,
        "step": 7434
    },
    {
        "loss": 1.4732,
        "grad_norm": 3.0475008487701416,
        "learning_rate": 3.861530329441104e-06,
        "epoch": 0.9583655581335395,
        "step": 7435
    },
    {
        "loss": 2.0779,
        "grad_norm": 2.128577709197998,
        "learning_rate": 3.838111957280532e-06,
        "epoch": 0.9584944573343646,
        "step": 7436
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.0921454429626465,
        "learning_rate": 3.814761977425779e-06,
        "epoch": 0.9586233565351895,
        "step": 7437
    },
    {
        "loss": 1.8093,
        "grad_norm": 2.0645928382873535,
        "learning_rate": 3.791480424471594e-06,
        "epoch": 0.9587522557360144,
        "step": 7438
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.5336066484451294,
        "learning_rate": 3.7682673329113516e-06,
        "epoch": 0.9588811549368393,
        "step": 7439
    },
    {
        "loss": 2.4301,
        "grad_norm": 1.3581297397613525,
        "learning_rate": 3.7451227371369857e-06,
        "epoch": 0.9590100541376644,
        "step": 7440
    },
    {
        "loss": 2.0678,
        "grad_norm": 1.9373199939727783,
        "learning_rate": 3.72204667143895e-06,
        "epoch": 0.9591389533384893,
        "step": 7441
    },
    {
        "loss": 1.6287,
        "grad_norm": 2.419665813446045,
        "learning_rate": 3.6990391700061466e-06,
        "epoch": 0.9592678525393142,
        "step": 7442
    },
    {
        "loss": 1.4598,
        "grad_norm": 2.5756821632385254,
        "learning_rate": 3.6761002669260134e-06,
        "epoch": 0.9593967517401392,
        "step": 7443
    },
    {
        "loss": 2.3631,
        "grad_norm": 1.6275818347930908,
        "learning_rate": 3.6532299961841488e-06,
        "epoch": 0.9595256509409642,
        "step": 7444
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.2095119953155518,
        "learning_rate": 3.6304283916646364e-06,
        "epoch": 0.9596545501417891,
        "step": 7445
    },
    {
        "loss": 1.6538,
        "grad_norm": 2.562042236328125,
        "learning_rate": 3.6076954871497372e-06,
        "epoch": 0.9597834493426141,
        "step": 7446
    },
    {
        "loss": 2.1979,
        "grad_norm": 2.4136111736297607,
        "learning_rate": 3.58503131631997e-06,
        "epoch": 0.959912348543439,
        "step": 7447
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.6517434120178223,
        "learning_rate": 3.562435912753986e-06,
        "epoch": 0.960041247744264,
        "step": 7448
    },
    {
        "loss": 2.0874,
        "grad_norm": 2.50211501121521,
        "learning_rate": 3.5399093099286517e-06,
        "epoch": 0.960170146945089,
        "step": 7449
    },
    {
        "loss": 1.4727,
        "grad_norm": 1.9727301597595215,
        "learning_rate": 3.5174515412187302e-06,
        "epoch": 0.9602990461459139,
        "step": 7450
    },
    {
        "loss": 0.8215,
        "grad_norm": 2.9741995334625244,
        "learning_rate": 3.4950626398971285e-06,
        "epoch": 0.9604279453467388,
        "step": 7451
    },
    {
        "loss": 2.0308,
        "grad_norm": 3.732546329498291,
        "learning_rate": 3.472742639134696e-06,
        "epoch": 0.9605568445475638,
        "step": 7452
    },
    {
        "loss": 2.5202,
        "grad_norm": 2.4146792888641357,
        "learning_rate": 3.4504915720001696e-06,
        "epoch": 0.9606857437483888,
        "step": 7453
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.713857889175415,
        "learning_rate": 3.428309471460167e-06,
        "epoch": 0.9608146429492137,
        "step": 7454
    },
    {
        "loss": 1.4927,
        "grad_norm": 3.180292844772339,
        "learning_rate": 3.4061963703791835e-06,
        "epoch": 0.9609435421500386,
        "step": 7455
    },
    {
        "loss": 1.3467,
        "grad_norm": 4.276804447174072,
        "learning_rate": 3.3841523015194445e-06,
        "epoch": 0.9610724413508637,
        "step": 7456
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.5625964403152466,
        "learning_rate": 3.362177297540836e-06,
        "epoch": 0.9612013405516886,
        "step": 7457
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.0978729724884033,
        "learning_rate": 3.340271391000982e-06,
        "epoch": 0.9613302397525135,
        "step": 7458
    },
    {
        "loss": 1.7038,
        "grad_norm": 1.9029779434204102,
        "learning_rate": 3.318434614355209e-06,
        "epoch": 0.9614591389533385,
        "step": 7459
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.8745249509811401,
        "learning_rate": 3.296666999956233e-06,
        "epoch": 0.9615880381541635,
        "step": 7460
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.509253740310669,
        "learning_rate": 3.2749685800544883e-06,
        "epoch": 0.9617169373549884,
        "step": 7461
    },
    {
        "loss": 2.3021,
        "grad_norm": 1.918286681175232,
        "learning_rate": 3.2533393867978214e-06,
        "epoch": 0.9618458365558133,
        "step": 7462
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.725616931915283,
        "learning_rate": 3.2317794522314426e-06,
        "epoch": 0.9619747357566383,
        "step": 7463
    },
    {
        "loss": 1.4251,
        "grad_norm": 3.7134487628936768,
        "learning_rate": 3.2102888082980286e-06,
        "epoch": 0.9621036349574633,
        "step": 7464
    },
    {
        "loss": 1.6154,
        "grad_norm": 2.545098066329956,
        "learning_rate": 3.1888674868376488e-06,
        "epoch": 0.9622325341582882,
        "step": 7465
    },
    {
        "loss": 1.9595,
        "grad_norm": 1.6395801305770874,
        "learning_rate": 3.1675155195875163e-06,
        "epoch": 0.9623614333591132,
        "step": 7466
    },
    {
        "loss": 1.5434,
        "grad_norm": 2.3659780025482178,
        "learning_rate": 3.146232938182231e-06,
        "epoch": 0.9624903325599381,
        "step": 7467
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.6066075563430786,
        "learning_rate": 3.1250197741535426e-06,
        "epoch": 0.962619231760763,
        "step": 7468
    },
    {
        "loss": 1.9123,
        "grad_norm": 3.1360461711883545,
        "learning_rate": 3.1038760589303105e-06,
        "epoch": 0.9627481309615881,
        "step": 7469
    },
    {
        "loss": 1.3273,
        "grad_norm": 2.5573627948760986,
        "learning_rate": 3.0828018238385214e-06,
        "epoch": 0.962877030162413,
        "step": 7470
    },
    {
        "loss": 2.0961,
        "grad_norm": 1.7018450498580933,
        "learning_rate": 3.061797100101288e-06,
        "epoch": 0.9630059293632379,
        "step": 7471
    },
    {
        "loss": 1.4295,
        "grad_norm": 2.9021456241607666,
        "learning_rate": 3.0408619188386665e-06,
        "epoch": 0.9631348285640629,
        "step": 7472
    },
    {
        "loss": 1.4192,
        "grad_norm": 2.2664284706115723,
        "learning_rate": 3.0199963110677132e-06,
        "epoch": 0.9632637277648879,
        "step": 7473
    },
    {
        "loss": 2.5436,
        "grad_norm": 2.2138664722442627,
        "learning_rate": 2.9992003077023766e-06,
        "epoch": 0.9633926269657128,
        "step": 7474
    },
    {
        "loss": 1.8497,
        "grad_norm": 1.8383307456970215,
        "learning_rate": 2.978473939553522e-06,
        "epoch": 0.9635215261665377,
        "step": 7475
    },
    {
        "loss": 1.3845,
        "grad_norm": 3.639805555343628,
        "learning_rate": 2.9578172373287695e-06,
        "epoch": 0.9636504253673627,
        "step": 7476
    },
    {
        "loss": 1.0445,
        "grad_norm": 3.28546404838562,
        "learning_rate": 2.937230231632626e-06,
        "epoch": 0.9637793245681877,
        "step": 7477
    },
    {
        "loss": 2.2635,
        "grad_norm": 1.3449305295944214,
        "learning_rate": 2.9167129529662663e-06,
        "epoch": 0.9639082237690126,
        "step": 7478
    },
    {
        "loss": 2.01,
        "grad_norm": 1.7340621948242188,
        "learning_rate": 2.896265431727585e-06,
        "epoch": 0.9640371229698376,
        "step": 7479
    },
    {
        "loss": 1.2461,
        "grad_norm": 3.404914617538452,
        "learning_rate": 2.8758876982111326e-06,
        "epoch": 0.9641660221706625,
        "step": 7480
    },
    {
        "loss": 2.139,
        "grad_norm": 1.4977658987045288,
        "learning_rate": 2.855579782608053e-06,
        "epoch": 0.9642949213714875,
        "step": 7481
    },
    {
        "loss": 2.037,
        "grad_norm": 2.3358216285705566,
        "learning_rate": 2.8353417150059957e-06,
        "epoch": 0.9644238205723125,
        "step": 7482
    },
    {
        "loss": 2.3065,
        "grad_norm": 2.335268259048462,
        "learning_rate": 2.815173525389253e-06,
        "epoch": 0.9645527197731374,
        "step": 7483
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.981890082359314,
        "learning_rate": 2.7950752436384674e-06,
        "epoch": 0.9646816189739623,
        "step": 7484
    },
    {
        "loss": 1.1275,
        "grad_norm": 3.263082981109619,
        "learning_rate": 2.77504689953077e-06,
        "epoch": 0.9648105181747874,
        "step": 7485
    },
    {
        "loss": 2.3878,
        "grad_norm": 1.7532944679260254,
        "learning_rate": 2.7550885227396573e-06,
        "epoch": 0.9649394173756123,
        "step": 7486
    },
    {
        "loss": 1.2576,
        "grad_norm": 2.7346551418304443,
        "learning_rate": 2.7352001428349818e-06,
        "epoch": 0.9650683165764372,
        "step": 7487
    },
    {
        "loss": 2.0926,
        "grad_norm": 2.3364434242248535,
        "learning_rate": 2.715381789282806e-06,
        "epoch": 0.9651972157772621,
        "step": 7488
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.285813808441162,
        "learning_rate": 2.6956334914455763e-06,
        "epoch": 0.9653261149780872,
        "step": 7489
    },
    {
        "loss": 1.203,
        "grad_norm": 3.906609058380127,
        "learning_rate": 2.675955278581871e-06,
        "epoch": 0.9654550141789121,
        "step": 7490
    },
    {
        "loss": 1.9266,
        "grad_norm": 2.97204852104187,
        "learning_rate": 2.656347179846419e-06,
        "epoch": 0.965583913379737,
        "step": 7491
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.7051565647125244,
        "learning_rate": 2.6368092242901044e-06,
        "epoch": 0.965712812580562,
        "step": 7492
    },
    {
        "loss": 1.9118,
        "grad_norm": 2.606135129928589,
        "learning_rate": 2.6173414408598827e-06,
        "epoch": 0.965841711781387,
        "step": 7493
    },
    {
        "loss": 2.0351,
        "grad_norm": 1.694442868232727,
        "learning_rate": 2.5979438583987215e-06,
        "epoch": 0.9659706109822119,
        "step": 7494
    },
    {
        "loss": 1.7747,
        "grad_norm": 1.9488080739974976,
        "learning_rate": 2.5786165056456037e-06,
        "epoch": 0.9660995101830369,
        "step": 7495
    },
    {
        "loss": 1.4027,
        "grad_norm": 2.301298141479492,
        "learning_rate": 2.5593594112354623e-06,
        "epoch": 0.9662284093838618,
        "step": 7496
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.417954444885254,
        "learning_rate": 2.540172603699126e-06,
        "epoch": 0.9663573085846868,
        "step": 7497
    },
    {
        "loss": 2.2227,
        "grad_norm": 1.7440457344055176,
        "learning_rate": 2.5210561114632826e-06,
        "epoch": 0.9664862077855118,
        "step": 7498
    },
    {
        "loss": 0.8527,
        "grad_norm": 2.0407955646514893,
        "learning_rate": 2.50200996285046e-06,
        "epoch": 0.9666151069863367,
        "step": 7499
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.322272539138794,
        "learning_rate": 2.483034186078964e-06,
        "epoch": 0.9667440061871616,
        "step": 7500
    },
    {
        "loss": 1.8233,
        "grad_norm": 3.4890358448028564,
        "learning_rate": 2.4641288092628166e-06,
        "epoch": 0.9668729053879866,
        "step": 7501
    },
    {
        "loss": 1.8173,
        "grad_norm": 1.8746237754821777,
        "learning_rate": 2.445293860411768e-06,
        "epoch": 0.9670018045888116,
        "step": 7502
    },
    {
        "loss": 1.8716,
        "grad_norm": 1.9802560806274414,
        "learning_rate": 2.4265293674312184e-06,
        "epoch": 0.9671307037896365,
        "step": 7503
    },
    {
        "loss": 2.0799,
        "grad_norm": 1.5159410238265991,
        "learning_rate": 2.4078353581221457e-06,
        "epoch": 0.9672596029904614,
        "step": 7504
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.801795721054077,
        "learning_rate": 2.3892118601811564e-06,
        "epoch": 0.9673885021912864,
        "step": 7505
    },
    {
        "loss": 2.3468,
        "grad_norm": 2.1002676486968994,
        "learning_rate": 2.3706589012003456e-06,
        "epoch": 0.9675174013921114,
        "step": 7506
    },
    {
        "loss": 1.7576,
        "grad_norm": 2.092595338821411,
        "learning_rate": 2.3521765086673307e-06,
        "epoch": 0.9676463005929363,
        "step": 7507
    },
    {
        "loss": 1.0885,
        "grad_norm": 1.3864659070968628,
        "learning_rate": 2.3337647099651527e-06,
        "epoch": 0.9677751997937613,
        "step": 7508
    },
    {
        "loss": 1.7565,
        "grad_norm": 1.8791778087615967,
        "learning_rate": 2.3154235323722516e-06,
        "epoch": 0.9679040989945862,
        "step": 7509
    },
    {
        "loss": 1.3328,
        "grad_norm": 2.130051612854004,
        "learning_rate": 2.297153003062541e-06,
        "epoch": 0.9680329981954112,
        "step": 7510
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.7640637159347534,
        "learning_rate": 2.278953149105101e-06,
        "epoch": 0.9681618973962361,
        "step": 7511
    },
    {
        "loss": 2.0383,
        "grad_norm": 2.725749969482422,
        "learning_rate": 2.260823997464412e-06,
        "epoch": 0.9682907965970611,
        "step": 7512
    },
    {
        "loss": 2.3626,
        "grad_norm": 1.957938551902771,
        "learning_rate": 2.242765575000172e-06,
        "epoch": 0.968419695797886,
        "step": 7513
    },
    {
        "loss": 1.8415,
        "grad_norm": 2.6463735103607178,
        "learning_rate": 2.2247779084673015e-06,
        "epoch": 0.968548594998711,
        "step": 7514
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.258704423904419,
        "learning_rate": 2.206861024515844e-06,
        "epoch": 0.968677494199536,
        "step": 7515
    },
    {
        "loss": 1.8268,
        "grad_norm": 3.101654529571533,
        "learning_rate": 2.189014949691076e-06,
        "epoch": 0.9688063934003609,
        "step": 7516
    },
    {
        "loss": 1.2122,
        "grad_norm": 2.7952067852020264,
        "learning_rate": 2.1712397104332484e-06,
        "epoch": 0.9689352926011858,
        "step": 7517
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.7851641178131104,
        "learning_rate": 2.153535333077711e-06,
        "epoch": 0.9690641918020109,
        "step": 7518
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.57085120677948,
        "learning_rate": 2.1359018438548483e-06,
        "epoch": 0.9691930910028358,
        "step": 7519
    },
    {
        "loss": 1.9819,
        "grad_norm": 2.0514461994171143,
        "learning_rate": 2.118339268889996e-06,
        "epoch": 0.9693219902036607,
        "step": 7520
    },
    {
        "loss": 2.4342,
        "grad_norm": 1.4511171579360962,
        "learning_rate": 2.1008476342034066e-06,
        "epoch": 0.9694508894044856,
        "step": 7521
    },
    {
        "loss": 1.6002,
        "grad_norm": 1.5095921754837036,
        "learning_rate": 2.0834269657102834e-06,
        "epoch": 0.9695797886053107,
        "step": 7522
    },
    {
        "loss": 0.7914,
        "grad_norm": 3.126615524291992,
        "learning_rate": 2.066077289220669e-06,
        "epoch": 0.9697086878061356,
        "step": 7523
    },
    {
        "loss": 1.6911,
        "grad_norm": 2.786761522293091,
        "learning_rate": 2.048798630439369e-06,
        "epoch": 0.9698375870069605,
        "step": 7524
    },
    {
        "loss": 2.0048,
        "grad_norm": 2.5515739917755127,
        "learning_rate": 2.031591014966011e-06,
        "epoch": 0.9699664862077855,
        "step": 7525
    },
    {
        "loss": 1.9105,
        "grad_norm": 2.2524359226226807,
        "learning_rate": 2.0144544682950296e-06,
        "epoch": 0.9700953854086105,
        "step": 7526
    },
    {
        "loss": 2.1347,
        "grad_norm": 2.7738475799560547,
        "learning_rate": 1.9973890158154316e-06,
        "epoch": 0.9702242846094354,
        "step": 7527
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.8496077060699463,
        "learning_rate": 1.980394682811032e-06,
        "epoch": 0.9703531838102604,
        "step": 7528
    },
    {
        "loss": 1.4486,
        "grad_norm": 3.4127988815307617,
        "learning_rate": 1.9634714944601897e-06,
        "epoch": 0.9704820830110853,
        "step": 7529
    },
    {
        "loss": 1.7063,
        "grad_norm": 2.05023455619812,
        "learning_rate": 1.94661947583587e-06,
        "epoch": 0.9706109822119103,
        "step": 7530
    },
    {
        "loss": 0.5239,
        "grad_norm": 4.147817611694336,
        "learning_rate": 1.92983865190558e-06,
        "epoch": 0.9707398814127353,
        "step": 7531
    },
    {
        "loss": 1.4664,
        "grad_norm": 2.0881779193878174,
        "learning_rate": 1.9131290475314424e-06,
        "epoch": 0.9708687806135602,
        "step": 7532
    },
    {
        "loss": 2.6094,
        "grad_norm": 1.330631136894226,
        "learning_rate": 1.8964906874699095e-06,
        "epoch": 0.9709976798143851,
        "step": 7533
    },
    {
        "loss": 1.3703,
        "grad_norm": 2.34304141998291,
        "learning_rate": 1.8799235963720296e-06,
        "epoch": 0.9711265790152102,
        "step": 7534
    },
    {
        "loss": 1.9078,
        "grad_norm": 2.777200222015381,
        "learning_rate": 1.8634277987831794e-06,
        "epoch": 0.9712554782160351,
        "step": 7535
    },
    {
        "loss": 1.1199,
        "grad_norm": 3.525678873062134,
        "learning_rate": 1.8470033191430869e-06,
        "epoch": 0.97138437741686,
        "step": 7536
    },
    {
        "loss": 1.8705,
        "grad_norm": 1.8161910772323608,
        "learning_rate": 1.8306501817858646e-06,
        "epoch": 0.9715132766176849,
        "step": 7537
    },
    {
        "loss": 2.4554,
        "grad_norm": 1.5992618799209595,
        "learning_rate": 1.8143684109399428e-06,
        "epoch": 0.97164217581851,
        "step": 7538
    },
    {
        "loss": 1.9061,
        "grad_norm": 2.3304755687713623,
        "learning_rate": 1.7981580307279645e-06,
        "epoch": 0.9717710750193349,
        "step": 7539
    },
    {
        "loss": 2.3175,
        "grad_norm": 2.3033111095428467,
        "learning_rate": 1.7820190651668344e-06,
        "epoch": 0.9718999742201598,
        "step": 7540
    },
    {
        "loss": 1.3188,
        "grad_norm": 2.80043363571167,
        "learning_rate": 1.7659515381676484e-06,
        "epoch": 0.9720288734209848,
        "step": 7541
    },
    {
        "loss": 1.6548,
        "grad_norm": 2.0263378620147705,
        "learning_rate": 1.7499554735356528e-06,
        "epoch": 0.9721577726218097,
        "step": 7542
    },
    {
        "loss": 2.11,
        "grad_norm": 2.307640314102173,
        "learning_rate": 1.7340308949701844e-06,
        "epoch": 0.9722866718226347,
        "step": 7543
    },
    {
        "loss": 1.7766,
        "grad_norm": 2.9083151817321777,
        "learning_rate": 1.7181778260647541e-06,
        "epoch": 0.9724155710234597,
        "step": 7544
    },
    {
        "loss": 1.5684,
        "grad_norm": 2.5868258476257324,
        "learning_rate": 1.7023962903068458e-06,
        "epoch": 0.9725444702242846,
        "step": 7545
    },
    {
        "loss": 1.8834,
        "grad_norm": 1.985350489616394,
        "learning_rate": 1.686686311077995e-06,
        "epoch": 0.9726733694251095,
        "step": 7546
    },
    {
        "loss": 2.1414,
        "grad_norm": 3.789669990539551,
        "learning_rate": 1.6710479116537225e-06,
        "epoch": 0.9728022686259346,
        "step": 7547
    },
    {
        "loss": 2.7389,
        "grad_norm": 2.012976884841919,
        "learning_rate": 1.655481115203511e-06,
        "epoch": 0.9729311678267595,
        "step": 7548
    },
    {
        "loss": 2.0088,
        "grad_norm": 2.6019954681396484,
        "learning_rate": 1.6399859447906784e-06,
        "epoch": 0.9730600670275844,
        "step": 7549
    },
    {
        "loss": 1.4972,
        "grad_norm": 3.4678220748901367,
        "learning_rate": 1.6245624233725387e-06,
        "epoch": 0.9731889662284093,
        "step": 7550
    },
    {
        "loss": 2.1244,
        "grad_norm": 2.6037368774414062,
        "learning_rate": 1.609210573800185e-06,
        "epoch": 0.9733178654292344,
        "step": 7551
    },
    {
        "loss": 2.1332,
        "grad_norm": 2.0963776111602783,
        "learning_rate": 1.593930418818529e-06,
        "epoch": 0.9734467646300593,
        "step": 7552
    },
    {
        "loss": 1.2949,
        "grad_norm": 2.917539358139038,
        "learning_rate": 1.5787219810662723e-06,
        "epoch": 0.9735756638308842,
        "step": 7553
    },
    {
        "loss": 1.9727,
        "grad_norm": 2.3786373138427734,
        "learning_rate": 1.563585283075858e-06,
        "epoch": 0.9737045630317092,
        "step": 7554
    },
    {
        "loss": 1.2292,
        "grad_norm": 3.2235753536224365,
        "learning_rate": 1.5485203472734467e-06,
        "epoch": 0.9738334622325342,
        "step": 7555
    },
    {
        "loss": 1.7963,
        "grad_norm": 1.5102601051330566,
        "learning_rate": 1.5335271959788623e-06,
        "epoch": 0.9739623614333591,
        "step": 7556
    },
    {
        "loss": 1.8834,
        "grad_norm": 3.175199270248413,
        "learning_rate": 1.5186058514055912e-06,
        "epoch": 0.974091260634184,
        "step": 7557
    },
    {
        "loss": 2.0783,
        "grad_norm": 2.306678533554077,
        "learning_rate": 1.5037563356607276e-06,
        "epoch": 0.974220159835009,
        "step": 7558
    },
    {
        "loss": 1.1187,
        "grad_norm": 1.8892487287521362,
        "learning_rate": 1.4889786707449394e-06,
        "epoch": 0.974349059035834,
        "step": 7559
    },
    {
        "loss": 2.0742,
        "grad_norm": 3.0699574947357178,
        "learning_rate": 1.4742728785524517e-06,
        "epoch": 0.974477958236659,
        "step": 7560
    },
    {
        "loss": 1.6781,
        "grad_norm": 5.519489765167236,
        "learning_rate": 1.4596389808710088e-06,
        "epoch": 0.9746068574374839,
        "step": 7561
    },
    {
        "loss": 1.9985,
        "grad_norm": 1.9819116592407227,
        "learning_rate": 1.4450769993818114e-06,
        "epoch": 0.9747357566383088,
        "step": 7562
    },
    {
        "loss": 1.8152,
        "grad_norm": 3.8935911655426025,
        "learning_rate": 1.4305869556595408e-06,
        "epoch": 0.9748646558391338,
        "step": 7563
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.621969699859619,
        "learning_rate": 1.4161688711722854e-06,
        "epoch": 0.9749935550399588,
        "step": 7564
    },
    {
        "loss": 1.7629,
        "grad_norm": 1.9527124166488647,
        "learning_rate": 1.4018227672815188e-06,
        "epoch": 0.9751224542407837,
        "step": 7565
    },
    {
        "loss": 1.7275,
        "grad_norm": 3.354086399078369,
        "learning_rate": 1.3875486652420722e-06,
        "epoch": 0.9752513534416086,
        "step": 7566
    },
    {
        "loss": 1.882,
        "grad_norm": 2.295623540878296,
        "learning_rate": 1.373346586202101e-06,
        "epoch": 0.9753802526424337,
        "step": 7567
    },
    {
        "loss": 1.1329,
        "grad_norm": 5.017364501953125,
        "learning_rate": 1.3592165512030518e-06,
        "epoch": 0.9755091518432586,
        "step": 7568
    },
    {
        "loss": 1.7764,
        "grad_norm": 2.5184266567230225,
        "learning_rate": 1.3451585811796341e-06,
        "epoch": 0.9756380510440835,
        "step": 7569
    },
    {
        "loss": 1.8217,
        "grad_norm": 2.489382028579712,
        "learning_rate": 1.3311726969597704e-06,
        "epoch": 0.9757669502449084,
        "step": 7570
    },
    {
        "loss": 2.3579,
        "grad_norm": 1.750909686088562,
        "learning_rate": 1.3172589192646468e-06,
        "epoch": 0.9758958494457335,
        "step": 7571
    },
    {
        "loss": 1.5572,
        "grad_norm": 3.9496548175811768,
        "learning_rate": 1.3034172687085177e-06,
        "epoch": 0.9760247486465584,
        "step": 7572
    },
    {
        "loss": 1.8243,
        "grad_norm": 2.0878400802612305,
        "learning_rate": 1.2896477657988514e-06,
        "epoch": 0.9761536478473833,
        "step": 7573
    },
    {
        "loss": 2.4014,
        "grad_norm": 1.9150886535644531,
        "learning_rate": 1.2759504309362013e-06,
        "epoch": 0.9762825470482083,
        "step": 7574
    },
    {
        "loss": 1.5286,
        "grad_norm": 2.7339670658111572,
        "learning_rate": 1.2623252844141953e-06,
        "epoch": 0.9764114462490333,
        "step": 7575
    },
    {
        "loss": 2.294,
        "grad_norm": 1.736497163772583,
        "learning_rate": 1.2487723464194967e-06,
        "epoch": 0.9765403454498582,
        "step": 7576
    },
    {
        "loss": 1.3175,
        "grad_norm": 2.407327890396118,
        "learning_rate": 1.2352916370318435e-06,
        "epoch": 0.9766692446506832,
        "step": 7577
    },
    {
        "loss": 1.714,
        "grad_norm": 2.0328481197357178,
        "learning_rate": 1.221883176223887e-06,
        "epoch": 0.9767981438515081,
        "step": 7578
    },
    {
        "loss": 2.0522,
        "grad_norm": 1.36764395236969,
        "learning_rate": 1.2085469838612695e-06,
        "epoch": 0.976927043052333,
        "step": 7579
    },
    {
        "loss": 1.0307,
        "grad_norm": 3.499781370162964,
        "learning_rate": 1.1952830797025693e-06,
        "epoch": 0.9770559422531581,
        "step": 7580
    },
    {
        "loss": 1.6527,
        "grad_norm": 2.4744668006896973,
        "learning_rate": 1.1820914833992558e-06,
        "epoch": 0.977184841453983,
        "step": 7581
    },
    {
        "loss": 1.6194,
        "grad_norm": 2.875006675720215,
        "learning_rate": 1.168972214495656e-06,
        "epoch": 0.9773137406548079,
        "step": 7582
    },
    {
        "loss": 2.2823,
        "grad_norm": 1.4020147323608398,
        "learning_rate": 1.1559252924289998e-06,
        "epoch": 0.9774426398556328,
        "step": 7583
    },
    {
        "loss": 1.7727,
        "grad_norm": 3.494485378265381,
        "learning_rate": 1.1429507365292303e-06,
        "epoch": 0.9775715390564579,
        "step": 7584
    },
    {
        "loss": 1.2765,
        "grad_norm": 2.848958969116211,
        "learning_rate": 1.1300485660191484e-06,
        "epoch": 0.9777004382572828,
        "step": 7585
    },
    {
        "loss": 1.8185,
        "grad_norm": 1.9444178342819214,
        "learning_rate": 1.1172188000142857e-06,
        "epoch": 0.9778293374581077,
        "step": 7586
    },
    {
        "loss": 1.7608,
        "grad_norm": 3.656648874282837,
        "learning_rate": 1.1044614575229095e-06,
        "epoch": 0.9779582366589327,
        "step": 7587
    },
    {
        "loss": 2.1639,
        "grad_norm": 1.5793925523757935,
        "learning_rate": 1.0917765574459614e-06,
        "epoch": 0.9780871358597577,
        "step": 7588
    },
    {
        "loss": 1.69,
        "grad_norm": 1.931107521057129,
        "learning_rate": 1.0791641185771084e-06,
        "epoch": 0.9782160350605826,
        "step": 7589
    },
    {
        "loss": 1.8722,
        "grad_norm": 3.218518018722534,
        "learning_rate": 1.0666241596026194e-06,
        "epoch": 0.9783449342614076,
        "step": 7590
    },
    {
        "loss": 2.1363,
        "grad_norm": 1.6246260404586792,
        "learning_rate": 1.0541566991013553e-06,
        "epoch": 0.9784738334622325,
        "step": 7591
    },
    {
        "loss": 1.5973,
        "grad_norm": 2.186934471130371,
        "learning_rate": 1.0417617555448122e-06,
        "epoch": 0.9786027326630575,
        "step": 7592
    },
    {
        "loss": 2.2772,
        "grad_norm": 2.6228995323181152,
        "learning_rate": 1.0294393472970565e-06,
        "epoch": 0.9787316318638825,
        "step": 7593
    },
    {
        "loss": 1.9928,
        "grad_norm": 1.4875469207763672,
        "learning_rate": 1.017189492614612e-06,
        "epoch": 0.9788605310647074,
        "step": 7594
    },
    {
        "loss": 1.2306,
        "grad_norm": 2.3079264163970947,
        "learning_rate": 1.0050122096465998e-06,
        "epoch": 0.9789894302655323,
        "step": 7595
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.777570128440857,
        "learning_rate": 9.929075164345825e-07,
        "epoch": 0.9791183294663574,
        "step": 7596
    },
    {
        "loss": 2.0281,
        "grad_norm": 1.6004661321640015,
        "learning_rate": 9.808754309125368e-07,
        "epoch": 0.9792472286671823,
        "step": 7597
    },
    {
        "loss": 2.7349,
        "grad_norm": 1.4438108205795288,
        "learning_rate": 9.689159709069085e-07,
        "epoch": 0.9793761278680072,
        "step": 7598
    },
    {
        "loss": 1.8293,
        "grad_norm": 2.125131130218506,
        "learning_rate": 9.570291541365794e-07,
        "epoch": 0.9795050270688321,
        "step": 7599
    },
    {
        "loss": 2.2606,
        "grad_norm": 2.65844464302063,
        "learning_rate": 9.452149982127013e-07,
        "epoch": 0.9796339262696572,
        "step": 7600
    },
    {
        "loss": 1.8808,
        "grad_norm": 2.6222453117370605,
        "learning_rate": 9.334735206388778e-07,
        "epoch": 0.9797628254704821,
        "step": 7601
    },
    {
        "loss": 1.4921,
        "grad_norm": 3.0995500087738037,
        "learning_rate": 9.218047388109885e-07,
        "epoch": 0.979891724671307,
        "step": 7602
    },
    {
        "loss": 1.0161,
        "grad_norm": 2.568756341934204,
        "learning_rate": 9.102086700171875e-07,
        "epoch": 0.980020623872132,
        "step": 7603
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.6134510040283203,
        "learning_rate": 8.986853314379151e-07,
        "epoch": 0.980149523072957,
        "step": 7604
    },
    {
        "loss": 1.1411,
        "grad_norm": 4.203671455383301,
        "learning_rate": 8.872347401458925e-07,
        "epoch": 0.9802784222737819,
        "step": 7605
    },
    {
        "loss": 1.3903,
        "grad_norm": 1.8583805561065674,
        "learning_rate": 8.758569131060267e-07,
        "epoch": 0.9804073214746069,
        "step": 7606
    },
    {
        "loss": 1.7227,
        "grad_norm": 2.5470521450042725,
        "learning_rate": 8.645518671754061e-07,
        "epoch": 0.9805362206754318,
        "step": 7607
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.0043134689331055,
        "learning_rate": 8.53319619103321e-07,
        "epoch": 0.9806651198762568,
        "step": 7608
    },
    {
        "loss": 2.6475,
        "grad_norm": 1.557172179222107,
        "learning_rate": 8.421601855311989e-07,
        "epoch": 0.9807940190770817,
        "step": 7609
    },
    {
        "loss": 2.1222,
        "grad_norm": 2.1828291416168213,
        "learning_rate": 8.310735829925364e-07,
        "epoch": 0.9809229182779067,
        "step": 7610
    },
    {
        "loss": 2.0453,
        "grad_norm": 2.43723726272583,
        "learning_rate": 8.200598279130168e-07,
        "epoch": 0.9810518174787316,
        "step": 7611
    },
    {
        "loss": 1.8214,
        "grad_norm": 1.9712718725204468,
        "learning_rate": 8.091189366103258e-07,
        "epoch": 0.9811807166795566,
        "step": 7612
    },
    {
        "loss": 1.7482,
        "grad_norm": 1.7869009971618652,
        "learning_rate": 7.982509252942083e-07,
        "epoch": 0.9813096158803816,
        "step": 7613
    },
    {
        "loss": 1.026,
        "grad_norm": 2.4446616172790527,
        "learning_rate": 7.874558100664564e-07,
        "epoch": 0.9814385150812065,
        "step": 7614
    },
    {
        "loss": 1.9271,
        "grad_norm": 1.508873462677002,
        "learning_rate": 7.767336069208486e-07,
        "epoch": 0.9815674142820314,
        "step": 7615
    },
    {
        "loss": 2.369,
        "grad_norm": 2.3391785621643066,
        "learning_rate": 7.660843317430944e-07,
        "epoch": 0.9816963134828564,
        "step": 7616
    },
    {
        "loss": 1.2953,
        "grad_norm": 2.217629909515381,
        "learning_rate": 7.555080003109338e-07,
        "epoch": 0.9818252126836814,
        "step": 7617
    },
    {
        "loss": 2.1783,
        "grad_norm": 2.6782572269439697,
        "learning_rate": 7.450046282939882e-07,
        "epoch": 0.9819541118845063,
        "step": 7618
    },
    {
        "loss": 1.904,
        "grad_norm": 2.3101606369018555,
        "learning_rate": 7.345742312537873e-07,
        "epoch": 0.9820830110853312,
        "step": 7619
    },
    {
        "loss": 1.4413,
        "grad_norm": 2.9173431396484375,
        "learning_rate": 7.242168246437531e-07,
        "epoch": 0.9822119102861562,
        "step": 7620
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.6248964071273804,
        "learning_rate": 7.139324238091605e-07,
        "epoch": 0.9823408094869812,
        "step": 7621
    },
    {
        "loss": 0.5273,
        "grad_norm": 2.4984772205352783,
        "learning_rate": 7.037210439871211e-07,
        "epoch": 0.9824697086878061,
        "step": 7622
    },
    {
        "loss": 1.9836,
        "grad_norm": 2.861562490463257,
        "learning_rate": 6.93582700306561e-07,
        "epoch": 0.9825986078886311,
        "step": 7623
    },
    {
        "loss": 1.136,
        "grad_norm": 3.881762742996216,
        "learning_rate": 6.835174077881979e-07,
        "epoch": 0.982727507089456,
        "step": 7624
    },
    {
        "loss": 1.7151,
        "grad_norm": 1.891188383102417,
        "learning_rate": 6.735251813445198e-07,
        "epoch": 0.982856406290281,
        "step": 7625
    },
    {
        "loss": 1.4653,
        "grad_norm": 2.7589173316955566,
        "learning_rate": 6.636060357797624e-07,
        "epoch": 0.982985305491106,
        "step": 7626
    },
    {
        "loss": 0.9498,
        "grad_norm": 3.4800143241882324,
        "learning_rate": 6.537599857898813e-07,
        "epoch": 0.9831142046919309,
        "step": 7627
    },
    {
        "loss": 2.0765,
        "grad_norm": 3.6738028526306152,
        "learning_rate": 6.439870459625408e-07,
        "epoch": 0.9832431038927558,
        "step": 7628
    },
    {
        "loss": 2.3562,
        "grad_norm": 2.466425895690918,
        "learning_rate": 6.342872307770919e-07,
        "epoch": 0.9833720030935809,
        "step": 7629
    },
    {
        "loss": 2.047,
        "grad_norm": 2.312784433364868,
        "learning_rate": 6.246605546045281e-07,
        "epoch": 0.9835009022944058,
        "step": 7630
    },
    {
        "loss": 2.4748,
        "grad_norm": 1.3742575645446777,
        "learning_rate": 6.151070317075014e-07,
        "epoch": 0.9836298014952307,
        "step": 7631
    },
    {
        "loss": 2.1326,
        "grad_norm": 2.777318000793457,
        "learning_rate": 6.056266762402729e-07,
        "epoch": 0.9837587006960556,
        "step": 7632
    },
    {
        "loss": 1.6504,
        "grad_norm": 2.671588897705078,
        "learning_rate": 5.962195022487071e-07,
        "epoch": 0.9838875998968807,
        "step": 7633
    },
    {
        "loss": 1.8702,
        "grad_norm": 1.739652395248413,
        "learning_rate": 5.86885523670222e-07,
        "epoch": 0.9840164990977056,
        "step": 7634
    },
    {
        "loss": 1.7276,
        "grad_norm": 3.2875964641571045,
        "learning_rate": 5.776247543338275e-07,
        "epoch": 0.9841453982985305,
        "step": 7635
    },
    {
        "loss": 2.1909,
        "grad_norm": 1.974142074584961,
        "learning_rate": 5.684372079600375e-07,
        "epoch": 0.9842742974993555,
        "step": 7636
    },
    {
        "loss": 2.0727,
        "grad_norm": 2.1582260131835938,
        "learning_rate": 5.593228981608856e-07,
        "epoch": 0.9844031967001805,
        "step": 7637
    },
    {
        "loss": 1.616,
        "grad_norm": 2.8573837280273438,
        "learning_rate": 5.502818384399367e-07,
        "epoch": 0.9845320959010054,
        "step": 7638
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.522468090057373,
        "learning_rate": 5.413140421921703e-07,
        "epoch": 0.9846609951018304,
        "step": 7639
    },
    {
        "loss": 2.166,
        "grad_norm": 2.3875656127929688,
        "learning_rate": 5.32419522704064e-07,
        "epoch": 0.9847898943026553,
        "step": 7640
    },
    {
        "loss": 2.1039,
        "grad_norm": 2.5274314880371094,
        "learning_rate": 5.235982931535099e-07,
        "epoch": 0.9849187935034803,
        "step": 7641
    },
    {
        "loss": 2.2605,
        "grad_norm": 3.6375231742858887,
        "learning_rate": 5.148503666098259e-07,
        "epoch": 0.9850476927043053,
        "step": 7642
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.6620235443115234,
        "learning_rate": 5.06175756033711e-07,
        "epoch": 0.9851765919051302,
        "step": 7643
    },
    {
        "loss": 1.6363,
        "grad_norm": 2.9292616844177246,
        "learning_rate": 4.975744742772848e-07,
        "epoch": 0.9853054911059551,
        "step": 7644
    },
    {
        "loss": 2.1865,
        "grad_norm": 1.9168286323547363,
        "learning_rate": 4.89046534083959e-07,
        "epoch": 0.9854343903067802,
        "step": 7645
    },
    {
        "loss": 1.9294,
        "grad_norm": 2.1996653079986572,
        "learning_rate": 4.805919480885379e-07,
        "epoch": 0.9855632895076051,
        "step": 7646
    },
    {
        "loss": 2.1458,
        "grad_norm": 2.072417736053467,
        "learning_rate": 4.7221072881711295e-07,
        "epoch": 0.98569218870843,
        "step": 7647
    },
    {
        "loss": 2.1579,
        "grad_norm": 2.6379361152648926,
        "learning_rate": 4.639028886870955e-07,
        "epoch": 0.9858210879092549,
        "step": 7648
    },
    {
        "loss": 1.7512,
        "grad_norm": 2.942967653274536,
        "learning_rate": 4.5566844000716755e-07,
        "epoch": 0.98594998711008,
        "step": 7649
    },
    {
        "loss": 1.6609,
        "grad_norm": 2.5650763511657715,
        "learning_rate": 4.47507394977309e-07,
        "epoch": 0.9860788863109049,
        "step": 7650
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.1502163410186768,
        "learning_rate": 4.3941976568869226e-07,
        "epoch": 0.9862077855117298,
        "step": 7651
    },
    {
        "loss": 1.0225,
        "grad_norm": 3.0912654399871826,
        "learning_rate": 4.314055641237602e-07,
        "epoch": 0.9863366847125548,
        "step": 7652
    },
    {
        "loss": 1.8843,
        "grad_norm": 2.5338873863220215,
        "learning_rate": 4.234648021561427e-07,
        "epoch": 0.9864655839133797,
        "step": 7653
    },
    {
        "loss": 1.1301,
        "grad_norm": 2.7048282623291016,
        "learning_rate": 4.155974915507121e-07,
        "epoch": 0.9865944831142047,
        "step": 7654
    },
    {
        "loss": 1.933,
        "grad_norm": 2.136462450027466,
        "learning_rate": 4.0780364396342805e-07,
        "epoch": 0.9867233823150297,
        "step": 7655
    },
    {
        "loss": 1.9239,
        "grad_norm": 2.454440116882324,
        "learning_rate": 4.000832709414981e-07,
        "epoch": 0.9868522815158546,
        "step": 7656
    },
    {
        "loss": 1.7891,
        "grad_norm": 2.5279712677001953,
        "learning_rate": 3.924363839232392e-07,
        "epoch": 0.9869811807166795,
        "step": 7657
    },
    {
        "loss": 1.2021,
        "grad_norm": 2.5824458599090576,
        "learning_rate": 3.8486299423806107e-07,
        "epoch": 0.9871100799175045,
        "step": 7658
    },
    {
        "loss": 2.0689,
        "grad_norm": 1.8039544820785522,
        "learning_rate": 3.7736311310651027e-07,
        "epoch": 0.9872389791183295,
        "step": 7659
    },
    {
        "loss": 1.9356,
        "grad_norm": 2.3401596546173096,
        "learning_rate": 3.6993675164025967e-07,
        "epoch": 0.9873678783191544,
        "step": 7660
    },
    {
        "loss": 1.9391,
        "grad_norm": 2.548145055770874,
        "learning_rate": 3.625839208419801e-07,
        "epoch": 0.9874967775199793,
        "step": 7661
    },
    {
        "loss": 1.3967,
        "grad_norm": 1.9472657442092896,
        "learning_rate": 3.5530463160546866e-07,
        "epoch": 0.9876256767208044,
        "step": 7662
    },
    {
        "loss": 1.7376,
        "grad_norm": 3.123699188232422,
        "learning_rate": 3.4809889471554835e-07,
        "epoch": 0.9877545759216293,
        "step": 7663
    },
    {
        "loss": 1.9841,
        "grad_norm": 2.0462148189544678,
        "learning_rate": 3.4096672084802385e-07,
        "epoch": 0.9878834751224542,
        "step": 7664
    },
    {
        "loss": 2.1194,
        "grad_norm": 1.6985054016113281,
        "learning_rate": 3.339081205697647e-07,
        "epoch": 0.9880123743232792,
        "step": 7665
    },
    {
        "loss": 1.852,
        "grad_norm": 2.9698548316955566,
        "learning_rate": 3.269231043386389e-07,
        "epoch": 0.9881412735241042,
        "step": 7666
    },
    {
        "loss": 2.0301,
        "grad_norm": 2.2803127765655518,
        "learning_rate": 3.2001168250343496e-07,
        "epoch": 0.9882701727249291,
        "step": 7667
    },
    {
        "loss": 2.1141,
        "grad_norm": 2.7346649169921875,
        "learning_rate": 3.131738653039673e-07,
        "epoch": 0.988399071925754,
        "step": 7668
    },
    {
        "loss": 2.2539,
        "grad_norm": 2.09438157081604,
        "learning_rate": 3.0640966287098227e-07,
        "epoch": 0.988527971126579,
        "step": 7669
    },
    {
        "loss": 1.303,
        "grad_norm": 3.1518309116363525,
        "learning_rate": 2.9971908522613556e-07,
        "epoch": 0.988656870327404,
        "step": 7670
    },
    {
        "loss": 1.5254,
        "grad_norm": 3.038187265396118,
        "learning_rate": 2.9310214228202013e-07,
        "epoch": 0.9887857695282289,
        "step": 7671
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.8167107105255127,
        "learning_rate": 2.8655884384214406e-07,
        "epoch": 0.9889146687290539,
        "step": 7672
    },
    {
        "loss": 2.506,
        "grad_norm": 1.3075307607650757,
        "learning_rate": 2.8008919960089695e-07,
        "epoch": 0.9890435679298788,
        "step": 7673
    },
    {
        "loss": 1.6091,
        "grad_norm": 3.319037675857544,
        "learning_rate": 2.7369321914354486e-07,
        "epoch": 0.9891724671307038,
        "step": 7674
    },
    {
        "loss": 1.5227,
        "grad_norm": 3.177280902862549,
        "learning_rate": 2.67370911946202e-07,
        "epoch": 0.9893013663315288,
        "step": 7675
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.6437559127807617,
        "learning_rate": 2.6112228737585344e-07,
        "epoch": 0.9894302655323537,
        "step": 7676
    },
    {
        "loss": 1.6368,
        "grad_norm": 2.1370062828063965,
        "learning_rate": 2.549473546902881e-07,
        "epoch": 0.9895591647331786,
        "step": 7677
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.820225477218628,
        "learning_rate": 2.4884612303815447e-07,
        "epoch": 0.9896880639340037,
        "step": 7678
    },
    {
        "loss": 1.9478,
        "grad_norm": 2.531297206878662,
        "learning_rate": 2.428186014588718e-07,
        "epoch": 0.9898169631348286,
        "step": 7679
    },
    {
        "loss": 1.7588,
        "grad_norm": 3.269575357437134,
        "learning_rate": 2.3686479888267443e-07,
        "epoch": 0.9899458623356535,
        "step": 7680
    },
    {
        "loss": 1.2118,
        "grad_norm": 3.6179723739624023,
        "learning_rate": 2.3098472413056182e-07,
        "epoch": 0.9900747615364784,
        "step": 7681
    },
    {
        "loss": 1.9562,
        "grad_norm": 2.1154134273529053,
        "learning_rate": 2.251783859143153e-07,
        "epoch": 0.9902036607373035,
        "step": 7682
    },
    {
        "loss": 1.7655,
        "grad_norm": 2.3835079669952393,
        "learning_rate": 2.194457928364424e-07,
        "epoch": 0.9903325599381284,
        "step": 7683
    },
    {
        "loss": 1.5323,
        "grad_norm": 2.6839916706085205,
        "learning_rate": 2.1378695339023257e-07,
        "epoch": 0.9904614591389533,
        "step": 7684
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.293114185333252,
        "learning_rate": 2.0820187595966824e-07,
        "epoch": 0.9905903583397783,
        "step": 7685
    },
    {
        "loss": 1.8359,
        "grad_norm": 2.389816999435425,
        "learning_rate": 2.0269056881946358e-07,
        "epoch": 0.9907192575406032,
        "step": 7686
    },
    {
        "loss": 1.7598,
        "grad_norm": 1.8731132745742798,
        "learning_rate": 1.9725304013504253e-07,
        "epoch": 0.9908481567414282,
        "step": 7687
    },
    {
        "loss": 1.7967,
        "grad_norm": 2.9473280906677246,
        "learning_rate": 1.9188929796249976e-07,
        "epoch": 0.9909770559422532,
        "step": 7688
    },
    {
        "loss": 2.013,
        "grad_norm": 2.2366981506347656,
        "learning_rate": 1.865993502486285e-07,
        "epoch": 0.9911059551430781,
        "step": 7689
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.9309357404708862,
        "learning_rate": 1.8138320483088723e-07,
        "epoch": 0.991234854343903,
        "step": 7690
    },
    {
        "loss": 1.8891,
        "grad_norm": 3.28312611579895,
        "learning_rate": 1.7624086943738293e-07,
        "epoch": 0.991363753544728,
        "step": 7691
    },
    {
        "loss": 1.8594,
        "grad_norm": 2.0082342624664307,
        "learning_rate": 1.711723516868713e-07,
        "epoch": 0.991492652745553,
        "step": 7692
    },
    {
        "loss": 1.541,
        "grad_norm": 2.1381032466888428,
        "learning_rate": 1.6617765908873983e-07,
        "epoch": 0.9916215519463779,
        "step": 7693
    },
    {
        "loss": 1.1106,
        "grad_norm": 2.48254132270813,
        "learning_rate": 1.6125679904301361e-07,
        "epoch": 0.9917504511472028,
        "step": 7694
    },
    {
        "loss": 2.0627,
        "grad_norm": 3.011402130126953,
        "learning_rate": 1.5640977884029962e-07,
        "epoch": 0.9918793503480279,
        "step": 7695
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.3743728399276733,
        "learning_rate": 1.5163660566183678e-07,
        "epoch": 0.9920082495488528,
        "step": 7696
    },
    {
        "loss": 2.0438,
        "grad_norm": 2.49220871925354,
        "learning_rate": 1.469372865794294e-07,
        "epoch": 0.9921371487496777,
        "step": 7697
    },
    {
        "loss": 1.2444,
        "grad_norm": 4.426816463470459,
        "learning_rate": 1.423118285554803e-07,
        "epoch": 0.9922660479505027,
        "step": 7698
    },
    {
        "loss": 2.1776,
        "grad_norm": 1.4591712951660156,
        "learning_rate": 1.3776023844294106e-07,
        "epoch": 0.9923949471513277,
        "step": 7699
    },
    {
        "loss": 1.8771,
        "grad_norm": 2.3101115226745605,
        "learning_rate": 1.332825229853507e-07,
        "epoch": 0.9925238463521526,
        "step": 7700
    },
    {
        "loss": 1.7316,
        "grad_norm": 3.1529483795166016,
        "learning_rate": 1.2887868881676923e-07,
        "epoch": 0.9926527455529776,
        "step": 7701
    },
    {
        "loss": 2.2629,
        "grad_norm": 2.293980836868286,
        "learning_rate": 1.245487424618108e-07,
        "epoch": 0.9927816447538025,
        "step": 7702
    },
    {
        "loss": 1.8334,
        "grad_norm": 2.783444404602051,
        "learning_rate": 1.2029269033561607e-07,
        "epoch": 0.9929105439546275,
        "step": 7703
    },
    {
        "loss": 1.949,
        "grad_norm": 1.7810992002487183,
        "learning_rate": 1.1611053874384103e-07,
        "epoch": 0.9930394431554525,
        "step": 7704
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.8848413228988647,
        "learning_rate": 1.1200229388267925e-07,
        "epoch": 0.9931683423562774,
        "step": 7705
    },
    {
        "loss": 2.0763,
        "grad_norm": 2.3049161434173584,
        "learning_rate": 1.0796796183877855e-07,
        "epoch": 0.9932972415571023,
        "step": 7706
    },
    {
        "loss": 1.8321,
        "grad_norm": 4.61424446105957,
        "learning_rate": 1.0400754858931328e-07,
        "epoch": 0.9934261407579273,
        "step": 7707
    },
    {
        "loss": 2.1455,
        "grad_norm": 1.9007750749588013,
        "learning_rate": 1.0012106000193976e-07,
        "epoch": 0.9935550399587523,
        "step": 7708
    },
    {
        "loss": 2.526,
        "grad_norm": 1.7536827325820923,
        "learning_rate": 9.63085018347687e-08,
        "epoch": 0.9936839391595772,
        "step": 7709
    },
    {
        "loss": 1.9739,
        "grad_norm": 2.4398224353790283,
        "learning_rate": 9.256987973640385e-08,
        "epoch": 0.9938128383604021,
        "step": 7710
    },
    {
        "loss": 1.641,
        "grad_norm": 3.2324750423431396,
        "learning_rate": 8.89051992458978e-08,
        "epoch": 0.9939417375612272,
        "step": 7711
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.795900821685791,
        "learning_rate": 8.531446579274071e-08,
        "epoch": 0.9940706367620521,
        "step": 7712
    },
    {
        "loss": 1.748,
        "grad_norm": 3.2220938205718994,
        "learning_rate": 8.17976846968771e-08,
        "epoch": 0.994199535962877,
        "step": 7713
    },
    {
        "loss": 1.9303,
        "grad_norm": 2.5670347213745117,
        "learning_rate": 7.835486116868352e-08,
        "epoch": 0.994328435163702,
        "step": 7714
    },
    {
        "loss": 1.0608,
        "grad_norm": 1.9109188318252563,
        "learning_rate": 7.498600030895752e-08,
        "epoch": 0.994457334364527,
        "step": 7715
    },
    {
        "loss": 1.6854,
        "grad_norm": 4.52608585357666,
        "learning_rate": 7.169110710891768e-08,
        "epoch": 0.9945862335653519,
        "step": 7716
    },
    {
        "loss": 1.3715,
        "grad_norm": 2.8356404304504395,
        "learning_rate": 6.847018645021464e-08,
        "epoch": 0.9947151327661768,
        "step": 7717
    },
    {
        "loss": 2.5542,
        "grad_norm": 2.220885992050171,
        "learning_rate": 6.532324310485894e-08,
        "epoch": 0.9948440319670018,
        "step": 7718
    },
    {
        "loss": 1.8638,
        "grad_norm": 2.5257675647735596,
        "learning_rate": 6.225028173529878e-08,
        "epoch": 0.9949729311678268,
        "step": 7719
    },
    {
        "loss": 1.69,
        "grad_norm": 3.2988290786743164,
        "learning_rate": 5.925130689435343e-08,
        "epoch": 0.9951018303686517,
        "step": 7720
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.8458977937698364,
        "learning_rate": 5.6326323025235326e-08,
        "epoch": 0.9952307295694767,
        "step": 7721
    },
    {
        "loss": 1.9818,
        "grad_norm": 3.0336480140686035,
        "learning_rate": 5.3475334461511316e-08,
        "epoch": 0.9953596287703016,
        "step": 7722
    },
    {
        "loss": 1.7335,
        "grad_norm": 1.4801175594329834,
        "learning_rate": 5.069834542715257e-08,
        "epoch": 0.9954885279711265,
        "step": 7723
    },
    {
        "loss": 1.1209,
        "grad_norm": 3.0021870136260986,
        "learning_rate": 4.799536003647353e-08,
        "epoch": 0.9956174271719516,
        "step": 7724
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.5974555015563965,
        "learning_rate": 4.536638229414858e-08,
        "epoch": 0.9957463263727765,
        "step": 7725
    },
    {
        "loss": 1.3577,
        "grad_norm": 3.8537588119506836,
        "learning_rate": 4.2811416095195345e-08,
        "epoch": 0.9958752255736014,
        "step": 7726
    },
    {
        "loss": 1.8165,
        "grad_norm": 2.959749937057495,
        "learning_rate": 4.033046522500805e-08,
        "epoch": 0.9960041247744263,
        "step": 7727
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.7815866470336914,
        "learning_rate": 3.792353335928533e-08,
        "epoch": 0.9961330239752514,
        "step": 7728
    },
    {
        "loss": 2.3925,
        "grad_norm": 2.83746337890625,
        "learning_rate": 3.55906240640802e-08,
        "epoch": 0.9962619231760763,
        "step": 7729
    },
    {
        "loss": 1.6228,
        "grad_norm": 2.546396017074585,
        "learning_rate": 3.3331740795783384e-08,
        "epoch": 0.9963908223769012,
        "step": 7730
    },
    {
        "loss": 1.4178,
        "grad_norm": 3.1825270652770996,
        "learning_rate": 3.1146886901090025e-08,
        "epoch": 0.9965197215777262,
        "step": 7731
    },
    {
        "loss": 2.0767,
        "grad_norm": 2.2864954471588135,
        "learning_rate": 2.903606561702743e-08,
        "epoch": 0.9966486207785512,
        "step": 7732
    },
    {
        "loss": 1.4782,
        "grad_norm": 2.420327663421631,
        "learning_rate": 2.6999280070938436e-08,
        "epoch": 0.9967775199793761,
        "step": 7733
    },
    {
        "loss": 1.8738,
        "grad_norm": 4.4513163566589355,
        "learning_rate": 2.5036533280470285e-08,
        "epoch": 0.9969064191802011,
        "step": 7734
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.8370325565338135,
        "learning_rate": 2.314782815358574e-08,
        "epoch": 0.997035318381026,
        "step": 7735
    },
    {
        "loss": 2.2714,
        "grad_norm": 2.3992176055908203,
        "learning_rate": 2.1333167488535312e-08,
        "epoch": 0.997164217581851,
        "step": 7736
    },
    {
        "loss": 2.4077,
        "grad_norm": 2.7671830654144287,
        "learning_rate": 1.959255397388504e-08,
        "epoch": 0.997293116782676,
        "step": 7737
    },
    {
        "loss": 1.9166,
        "grad_norm": 2.196765899658203,
        "learning_rate": 1.7925990188472075e-08,
        "epoch": 0.9974220159835009,
        "step": 7738
    },
    {
        "loss": 1.7596,
        "grad_norm": 2.70540189743042,
        "learning_rate": 1.633347860144907e-08,
        "epoch": 0.9975509151843258,
        "step": 7739
    },
    {
        "loss": 2.4217,
        "grad_norm": 1.851719856262207,
        "learning_rate": 1.4815021572228694e-08,
        "epoch": 0.9976798143851509,
        "step": 7740
    },
    {
        "loss": 1.7997,
        "grad_norm": 2.6587555408477783,
        "learning_rate": 1.3370621350533575e-08,
        "epoch": 0.9978087135859758,
        "step": 7741
    },
    {
        "loss": 2.178,
        "grad_norm": 2.0119292736053467,
        "learning_rate": 1.2000280076335246e-08,
        "epoch": 0.9979376127868007,
        "step": 7742
    },
    {
        "loss": 1.8935,
        "grad_norm": 2.9197356700897217,
        "learning_rate": 1.0703999779915209e-08,
        "epoch": 0.9980665119876256,
        "step": 7743
    },
    {
        "loss": 2.0536,
        "grad_norm": 2.2854013442993164,
        "learning_rate": 9.481782381792759e-09,
        "epoch": 0.9981954111884507,
        "step": 7744
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.2452805042266846,
        "learning_rate": 8.33362969278606e-09,
        "epoch": 0.9983243103892756,
        "step": 7745
    },
    {
        "loss": 2.2506,
        "grad_norm": 2.544379472732544,
        "learning_rate": 7.259543413962178e-09,
        "epoch": 0.9984532095901005,
        "step": 7746
    },
    {
        "loss": 1.861,
        "grad_norm": 2.7445266246795654,
        "learning_rate": 6.259525136664835e-09,
        "epoch": 0.9985821087909255,
        "step": 7747
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.0876095294952393,
        "learning_rate": 5.3335763424977595e-09,
        "epoch": 0.9987110079917505,
        "step": 7748
    },
    {
        "loss": 0.5475,
        "grad_norm": 4.238905906677246,
        "learning_rate": 4.481698403324685e-09,
        "epoch": 0.9988399071925754,
        "step": 7749
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.382633924484253,
        "learning_rate": 3.7038925812582505e-09,
        "epoch": 0.9989688063934004,
        "step": 7750
    },
    {
        "loss": 1.7143,
        "grad_norm": 2.09718918800354,
        "learning_rate": 3.0001600286877483e-09,
        "epoch": 0.9990977055942253,
        "step": 7751
    },
    {
        "loss": 1.5013,
        "grad_norm": 2.7867205142974854,
        "learning_rate": 2.3705017882347246e-09,
        "epoch": 0.9992266047950503,
        "step": 7752
    },
    {
        "loss": 2.159,
        "grad_norm": 2.092305898666382,
        "learning_rate": 1.8149187927918309e-09,
        "epoch": 0.9993555039958752,
        "step": 7753
    },
    {
        "loss": 1.8208,
        "grad_norm": 1.9386329650878906,
        "learning_rate": 1.333411865495071e-09,
        "epoch": 0.9994844031967002,
        "step": 7754
    },
    {
        "loss": 1.6204,
        "grad_norm": 2.313384532928467,
        "learning_rate": 9.25981719734903e-10,
        "epoch": 0.9996133023975251,
        "step": 7755
    },
    {
        "loss": 2.0314,
        "grad_norm": 1.4675205945968628,
        "learning_rate": 5.926289591506873e-10,
        "epoch": 0.9997422015983501,
        "step": 7756
    },
    {
        "loss": 2.1339,
        "grad_norm": 2.6589648723602295,
        "learning_rate": 3.3335407762513646e-10,
        "epoch": 0.9998711007991751,
        "step": 7757
    },
    {
        "loss": 1.1979,
        "grad_norm": 3.6927597522735596,
        "learning_rate": 1.4815745929541713e-10,
        "epoch": 1.0,
        "step": 7758
    },
    {
        "train_runtime": 21384.3962,
        "train_samples_per_second": 0.726,
        "train_steps_per_second": 0.363,
        "total_flos": 3.1712249195126784e+17,
        "train_loss": 1.9970678969176658,
        "epoch": 1.0,
        "step": 7758
    }
]