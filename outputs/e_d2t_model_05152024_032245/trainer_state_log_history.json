[
    {
        "loss": 5.9529,
        "grad_norm": 6.679808616638184,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 7.452120128176467e-05,
        "step": 1
    },
    {
        "loss": 4.7344,
        "grad_norm": 4.103719711303711,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.00014904240256352933,
        "step": 2
    },
    {
        "loss": 5.0354,
        "grad_norm": 5.589086532592773,
        "learning_rate": 2.4e-05,
        "epoch": 0.000223563603845294,
        "step": 3
    },
    {
        "loss": 3.9856,
        "grad_norm": 3.101287841796875,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.00029808480512705867,
        "step": 4
    },
    {
        "loss": 5.3924,
        "grad_norm": 5.876624584197998,
        "learning_rate": 4e-05,
        "epoch": 0.00037260600640882333,
        "step": 5
    },
    {
        "loss": 5.8902,
        "grad_norm": 5.935253143310547,
        "learning_rate": 4.8e-05,
        "epoch": 0.000447127207690588,
        "step": 6
    },
    {
        "loss": 3.0378,
        "grad_norm": 2.7625181674957275,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0005216484089723526,
        "step": 7
    },
    {
        "loss": 3.6313,
        "grad_norm": 2.597459077835083,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0005961696102541173,
        "step": 8
    },
    {
        "loss": 3.5802,
        "grad_norm": 2.6042745113372803,
        "learning_rate": 7.2e-05,
        "epoch": 0.0006706908115358819,
        "step": 9
    },
    {
        "loss": 3.9063,
        "grad_norm": 3.379915475845337,
        "learning_rate": 8e-05,
        "epoch": 0.0007452120128176467,
        "step": 10
    },
    {
        "loss": 4.0734,
        "grad_norm": 5.257033824920654,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0008197332140994113,
        "step": 11
    },
    {
        "loss": 4.0548,
        "grad_norm": 4.740084171295166,
        "learning_rate": 9.6e-05,
        "epoch": 0.000894254415381176,
        "step": 12
    },
    {
        "loss": 3.1685,
        "grad_norm": 2.4693493843078613,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0009687756166629406,
        "step": 13
    },
    {
        "loss": 3.9923,
        "grad_norm": 3.276355743408203,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0010432968179447052,
        "step": 14
    },
    {
        "loss": 3.4586,
        "grad_norm": Infinity,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0011178180192264698,
        "step": 15
    },
    {
        "loss": 4.2204,
        "grad_norm": 4.2834882736206055,
        "learning_rate": 0.00012,
        "epoch": 0.0011923392205082347,
        "step": 16
    },
    {
        "loss": 4.1518,
        "grad_norm": 5.905289173126221,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0012668604217899993,
        "step": 17
    },
    {
        "loss": 3.3133,
        "grad_norm": 3.3790321350097656,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0013413816230717639,
        "step": 18
    },
    {
        "loss": 3.619,
        "grad_norm": 6.223319053649902,
        "learning_rate": 0.000144,
        "epoch": 0.0014159028243535285,
        "step": 19
    },
    {
        "loss": 3.3916,
        "grad_norm": 5.598274230957031,
        "learning_rate": 0.000152,
        "epoch": 0.0014904240256352933,
        "step": 20
    },
    {
        "loss": 3.8747,
        "grad_norm": 6.337136268615723,
        "learning_rate": 0.00016,
        "epoch": 0.001564945226917058,
        "step": 21
    },
    {
        "loss": 3.4822,
        "grad_norm": 4.914961338043213,
        "learning_rate": 0.000168,
        "epoch": 0.0016394664281988226,
        "step": 22
    },
    {
        "loss": 3.8089,
        "grad_norm": 6.978173732757568,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0017139876294805872,
        "step": 23
    },
    {
        "loss": 3.6205,
        "grad_norm": 4.329580783843994,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.001788508830762352,
        "step": 24
    },
    {
        "loss": 2.746,
        "grad_norm": 7.624200820922852,
        "learning_rate": 0.000192,
        "epoch": 0.0018630300320441166,
        "step": 25
    },
    {
        "loss": 3.3247,
        "grad_norm": 2.9280471801757812,
        "learning_rate": 0.0002,
        "epoch": 0.0019375512333258812,
        "step": 26
    },
    {
        "loss": 3.0828,
        "grad_norm": 4.140373229980469,
        "learning_rate": 0.0001999999752433628,
        "epoch": 0.002012072434607646,
        "step": 27
    },
    {
        "loss": 3.2913,
        "grad_norm": 6.381428241729736,
        "learning_rate": 0.00019999990097346353,
        "epoch": 0.0020865936358894104,
        "step": 28
    },
    {
        "loss": 2.8142,
        "grad_norm": 3.923600196838379,
        "learning_rate": 0.00019999977719033888,
        "epoch": 0.0021611148371711753,
        "step": 29
    },
    {
        "loss": 3.0219,
        "grad_norm": 2.9779553413391113,
        "learning_rate": 0.00019999960389405018,
        "epoch": 0.0022356360384529397,
        "step": 30
    },
    {
        "loss": 3.2976,
        "grad_norm": 2.416954517364502,
        "learning_rate": 0.00019999938108468323,
        "epoch": 0.0023101572397347045,
        "step": 31
    },
    {
        "loss": 3.3641,
        "grad_norm": 5.468579292297363,
        "learning_rate": 0.00019999910876234835,
        "epoch": 0.0023846784410164693,
        "step": 32
    },
    {
        "loss": 3.2194,
        "grad_norm": 3.957080364227295,
        "learning_rate": 0.0001999987869271804,
        "epoch": 0.0024591996422982337,
        "step": 33
    },
    {
        "loss": 3.3642,
        "grad_norm": 2.937100648880005,
        "learning_rate": 0.00019999841557933866,
        "epoch": 0.0025337208435799985,
        "step": 34
    },
    {
        "loss": 3.1358,
        "grad_norm": 1.782873272895813,
        "learning_rate": 0.0001999979947190071,
        "epoch": 0.0026082420448617634,
        "step": 35
    },
    {
        "loss": 2.4914,
        "grad_norm": 2.5984015464782715,
        "learning_rate": 0.00019999752434639404,
        "epoch": 0.0026827632461435278,
        "step": 36
    },
    {
        "loss": 2.9167,
        "grad_norm": 1.467373013496399,
        "learning_rate": 0.00019999700446173235,
        "epoch": 0.0027572844474252926,
        "step": 37
    },
    {
        "loss": 2.4415,
        "grad_norm": 3.668558120727539,
        "learning_rate": 0.00019999643506527948,
        "epoch": 0.002831805648707057,
        "step": 38
    },
    {
        "loss": 3.2331,
        "grad_norm": 2.7504396438598633,
        "learning_rate": 0.00019999581615731736,
        "epoch": 0.002906326849988822,
        "step": 39
    },
    {
        "loss": 2.7833,
        "grad_norm": 4.057546138763428,
        "learning_rate": 0.00019999514773815245,
        "epoch": 0.0029808480512705867,
        "step": 40
    },
    {
        "loss": 2.6738,
        "grad_norm": 3.6127700805664062,
        "learning_rate": 0.00019999442980811564,
        "epoch": 0.003055369252552351,
        "step": 41
    },
    {
        "loss": 3.2745,
        "grad_norm": 2.5343587398529053,
        "learning_rate": 0.00019999366236756247,
        "epoch": 0.003129890453834116,
        "step": 42
    },
    {
        "loss": 2.3541,
        "grad_norm": 4.231149196624756,
        "learning_rate": 0.0001999928454168729,
        "epoch": 0.0032044116551158803,
        "step": 43
    },
    {
        "loss": 2.57,
        "grad_norm": 2.9071435928344727,
        "learning_rate": 0.00019999197895645142,
        "epoch": 0.003278932856397645,
        "step": 44
    },
    {
        "loss": 2.1886,
        "grad_norm": 1.6091413497924805,
        "learning_rate": 0.000199991062986727,
        "epoch": 0.00335345405767941,
        "step": 45
    },
    {
        "loss": 2.6191,
        "grad_norm": 2.582373857498169,
        "learning_rate": 0.00019999009750815329,
        "epoch": 0.0034279752589611743,
        "step": 46
    },
    {
        "loss": 2.8454,
        "grad_norm": 2.3325393199920654,
        "learning_rate": 0.0001999890825212082,
        "epoch": 0.003502496460242939,
        "step": 47
    },
    {
        "loss": 3.3024,
        "grad_norm": 3.9627678394317627,
        "learning_rate": 0.00019998801802639435,
        "epoch": 0.003577017661524704,
        "step": 48
    },
    {
        "loss": 2.4084,
        "grad_norm": 3.565702438354492,
        "learning_rate": 0.0001999869040242388,
        "epoch": 0.0036515388628064684,
        "step": 49
    },
    {
        "loss": 3.0212,
        "grad_norm": 1.9491229057312012,
        "learning_rate": 0.00019998574051529312,
        "epoch": 0.003726060064088233,
        "step": 50
    },
    {
        "loss": 2.5661,
        "grad_norm": 2.445896625518799,
        "learning_rate": 0.0001999845275001334,
        "epoch": 0.0038005812653699976,
        "step": 51
    },
    {
        "loss": 2.2147,
        "grad_norm": 5.250332355499268,
        "learning_rate": 0.00019998326497936027,
        "epoch": 0.0038751024666517624,
        "step": 52
    },
    {
        "loss": 2.555,
        "grad_norm": 2.5829548835754395,
        "learning_rate": 0.00019998195295359877,
        "epoch": 0.003949623667933527,
        "step": 53
    },
    {
        "loss": 2.6158,
        "grad_norm": 3.9559073448181152,
        "learning_rate": 0.00019998059142349863,
        "epoch": 0.004024144869215292,
        "step": 54
    },
    {
        "loss": 2.8204,
        "grad_norm": 1.8230260610580444,
        "learning_rate": 0.00019997918038973392,
        "epoch": 0.004098666070497056,
        "step": 55
    },
    {
        "loss": 2.885,
        "grad_norm": 2.091204881668091,
        "learning_rate": 0.00019997771985300332,
        "epoch": 0.004173187271778821,
        "step": 56
    },
    {
        "loss": 2.9327,
        "grad_norm": 1.8881871700286865,
        "learning_rate": 0.00019997620981402996,
        "epoch": 0.004247708473060586,
        "step": 57
    },
    {
        "loss": 2.7075,
        "grad_norm": 2.363948345184326,
        "learning_rate": 0.00019997465027356154,
        "epoch": 0.0043222296743423505,
        "step": 58
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.1102025508880615,
        "learning_rate": 0.0001999730412323702,
        "epoch": 0.004396750875624115,
        "step": 59
    },
    {
        "loss": 2.8899,
        "grad_norm": 3.378216028213501,
        "learning_rate": 0.0001999713826912527,
        "epoch": 0.004471272076905879,
        "step": 60
    },
    {
        "loss": 2.6638,
        "grad_norm": 2.8436524868011475,
        "learning_rate": 0.00019996967465103017,
        "epoch": 0.004545793278187644,
        "step": 61
    },
    {
        "loss": 2.7537,
        "grad_norm": 1.3695940971374512,
        "learning_rate": 0.00019996791711254832,
        "epoch": 0.004620314479469409,
        "step": 62
    },
    {
        "loss": 3.0271,
        "grad_norm": 2.785648822784424,
        "learning_rate": 0.00019996611007667742,
        "epoch": 0.004694835680751174,
        "step": 63
    },
    {
        "loss": 2.5099,
        "grad_norm": 2.335237741470337,
        "learning_rate": 0.00019996425354431213,
        "epoch": 0.004769356882032939,
        "step": 64
    },
    {
        "loss": 3.1067,
        "grad_norm": 2.7167882919311523,
        "learning_rate": 0.00019996234751637173,
        "epoch": 0.004843878083314703,
        "step": 65
    },
    {
        "loss": 2.4812,
        "grad_norm": 3.5517029762268066,
        "learning_rate": 0.00019996039199379992,
        "epoch": 0.0049183992845964674,
        "step": 66
    },
    {
        "loss": 3.0718,
        "grad_norm": 3.790817975997925,
        "learning_rate": 0.00019995838697756498,
        "epoch": 0.004992920485878232,
        "step": 67
    },
    {
        "loss": 2.2636,
        "grad_norm": 2.5364339351654053,
        "learning_rate": 0.00019995633246865959,
        "epoch": 0.005067441687159997,
        "step": 68
    },
    {
        "loss": 2.2166,
        "grad_norm": 3.3795318603515625,
        "learning_rate": 0.0001999542284681011,
        "epoch": 0.005141962888441762,
        "step": 69
    },
    {
        "loss": 3.7789,
        "grad_norm": 5.0629072189331055,
        "learning_rate": 0.00019995207497693117,
        "epoch": 0.005216484089723527,
        "step": 70
    },
    {
        "loss": 2.4942,
        "grad_norm": 2.2424004077911377,
        "learning_rate": 0.00019994987199621617,
        "epoch": 0.005291005291005291,
        "step": 71
    },
    {
        "loss": 3.0026,
        "grad_norm": 3.2612948417663574,
        "learning_rate": 0.00019994761952704676,
        "epoch": 0.0053655264922870555,
        "step": 72
    },
    {
        "loss": 2.8867,
        "grad_norm": 1.9243760108947754,
        "learning_rate": 0.00019994531757053826,
        "epoch": 0.00544004769356882,
        "step": 73
    },
    {
        "loss": 2.2354,
        "grad_norm": 5.11698055267334,
        "learning_rate": 0.0001999429661278305,
        "epoch": 0.005514568894850585,
        "step": 74
    },
    {
        "loss": 1.8862,
        "grad_norm": 4.177558422088623,
        "learning_rate": 0.00019994056520008767,
        "epoch": 0.00558909009613235,
        "step": 75
    },
    {
        "loss": 2.8637,
        "grad_norm": 3.2024307250976562,
        "learning_rate": 0.0001999381147884986,
        "epoch": 0.005663611297414114,
        "step": 76
    },
    {
        "loss": 2.9824,
        "grad_norm": 2.8134684562683105,
        "learning_rate": 0.00019993561489427654,
        "epoch": 0.005738132498695879,
        "step": 77
    },
    {
        "loss": 1.9179,
        "grad_norm": 4.458132266998291,
        "learning_rate": 0.0001999330655186593,
        "epoch": 0.005812653699977644,
        "step": 78
    },
    {
        "loss": 2.6912,
        "grad_norm": 2.7304999828338623,
        "learning_rate": 0.0001999304666629091,
        "epoch": 0.0058871749012594085,
        "step": 79
    },
    {
        "loss": 3.6379,
        "grad_norm": 1.9269851446151733,
        "learning_rate": 0.00019992781832831277,
        "epoch": 0.005961696102541173,
        "step": 80
    },
    {
        "loss": 2.9255,
        "grad_norm": 2.5198066234588623,
        "learning_rate": 0.0001999251205161816,
        "epoch": 0.006036217303822937,
        "step": 81
    },
    {
        "loss": 2.9062,
        "grad_norm": 2.6411359310150146,
        "learning_rate": 0.00019992237322785132,
        "epoch": 0.006110738505104702,
        "step": 82
    },
    {
        "loss": 2.6752,
        "grad_norm": 1.8501752614974976,
        "learning_rate": 0.00019991957646468226,
        "epoch": 0.006185259706386467,
        "step": 83
    },
    {
        "loss": 3.0628,
        "grad_norm": 2.7040183544158936,
        "learning_rate": 0.00019991673022805913,
        "epoch": 0.006259780907668232,
        "step": 84
    },
    {
        "loss": 2.7265,
        "grad_norm": 2.5949671268463135,
        "learning_rate": 0.0001999138345193912,
        "epoch": 0.006334302108949997,
        "step": 85
    },
    {
        "loss": 2.0318,
        "grad_norm": 4.82371711730957,
        "learning_rate": 0.00019991088934011227,
        "epoch": 0.0064088233102317605,
        "step": 86
    },
    {
        "loss": 2.3834,
        "grad_norm": 2.1234447956085205,
        "learning_rate": 0.00019990789469168053,
        "epoch": 0.006483344511513525,
        "step": 87
    },
    {
        "loss": 2.6179,
        "grad_norm": 2.7723007202148438,
        "learning_rate": 0.00019990485057557882,
        "epoch": 0.00655786571279529,
        "step": 88
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.7273943424224854,
        "learning_rate": 0.0001999017569933143,
        "epoch": 0.006632386914077055,
        "step": 89
    },
    {
        "loss": 3.1869,
        "grad_norm": 3.5132923126220703,
        "learning_rate": 0.00019989861394641874,
        "epoch": 0.00670690811535882,
        "step": 90
    },
    {
        "loss": 3.299,
        "grad_norm": 2.9764461517333984,
        "learning_rate": 0.00019989542143644838,
        "epoch": 0.006781429316640584,
        "step": 91
    },
    {
        "loss": 2.2231,
        "grad_norm": 3.321889638900757,
        "learning_rate": 0.0001998921794649839,
        "epoch": 0.006855950517922349,
        "step": 92
    },
    {
        "loss": 2.7327,
        "grad_norm": 3.444026470184326,
        "learning_rate": 0.00019988888803363053,
        "epoch": 0.0069304717192041135,
        "step": 93
    },
    {
        "loss": 2.6807,
        "grad_norm": 3.276664972305298,
        "learning_rate": 0.00019988554714401796,
        "epoch": 0.007004992920485878,
        "step": 94
    },
    {
        "loss": 2.5146,
        "grad_norm": 2.6137681007385254,
        "learning_rate": 0.00019988215679780038,
        "epoch": 0.007079514121767643,
        "step": 95
    },
    {
        "loss": 2.4351,
        "grad_norm": 2.8296704292297363,
        "learning_rate": 0.00019987871699665645,
        "epoch": 0.007154035323049408,
        "step": 96
    },
    {
        "loss": 2.8396,
        "grad_norm": 3.115832805633545,
        "learning_rate": 0.00019987522774228932,
        "epoch": 0.007228556524331172,
        "step": 97
    },
    {
        "loss": 2.8994,
        "grad_norm": 2.9842894077301025,
        "learning_rate": 0.00019987168903642667,
        "epoch": 0.007303077725612937,
        "step": 98
    },
    {
        "loss": 2.8866,
        "grad_norm": 2.6867196559906006,
        "learning_rate": 0.00019986810088082056,
        "epoch": 0.007377598926894702,
        "step": 99
    },
    {
        "loss": 2.7454,
        "grad_norm": 3.374387502670288,
        "learning_rate": 0.0001998644632772477,
        "epoch": 0.007452120128176466,
        "step": 100
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.613551378250122,
        "learning_rate": 0.0001998607762275091,
        "epoch": 0.007526641329458231,
        "step": 101
    },
    {
        "loss": 1.4796,
        "grad_norm": 4.263508319854736,
        "learning_rate": 0.00019985703973343042,
        "epoch": 0.007601162530739995,
        "step": 102
    },
    {
        "loss": 3.2075,
        "grad_norm": 2.79797625541687,
        "learning_rate": 0.00019985325379686165,
        "epoch": 0.00767568373202176,
        "step": 103
    },
    {
        "loss": 1.7711,
        "grad_norm": 5.1773271560668945,
        "learning_rate": 0.00019984941841967734,
        "epoch": 0.007750204933303525,
        "step": 104
    },
    {
        "loss": 2.3668,
        "grad_norm": 2.537395715713501,
        "learning_rate": 0.00019984553360377657,
        "epoch": 0.00782472613458529,
        "step": 105
    },
    {
        "loss": 2.6367,
        "grad_norm": 2.2597641944885254,
        "learning_rate": 0.00019984159935108276,
        "epoch": 0.007899247335867055,
        "step": 106
    },
    {
        "loss": 2.5317,
        "grad_norm": 2.398838520050049,
        "learning_rate": 0.00019983761566354393,
        "epoch": 0.00797376853714882,
        "step": 107
    },
    {
        "loss": 2.5388,
        "grad_norm": 2.8818020820617676,
        "learning_rate": 0.0001998335825431325,
        "epoch": 0.008048289738430584,
        "step": 108
    },
    {
        "loss": 2.4677,
        "grad_norm": 2.5147504806518555,
        "learning_rate": 0.00019982949999184548,
        "epoch": 0.008122810939712349,
        "step": 109
    },
    {
        "loss": 2.7392,
        "grad_norm": 3.0164949893951416,
        "learning_rate": 0.0001998253680117042,
        "epoch": 0.008197332140994112,
        "step": 110
    },
    {
        "loss": 2.7276,
        "grad_norm": 2.1639444828033447,
        "learning_rate": 0.00019982118660475458,
        "epoch": 0.008271853342275877,
        "step": 111
    },
    {
        "loss": 2.1847,
        "grad_norm": 3.9591715335845947,
        "learning_rate": 0.0001998169557730669,
        "epoch": 0.008346374543557642,
        "step": 112
    },
    {
        "loss": 2.6789,
        "grad_norm": 2.1693596839904785,
        "learning_rate": 0.0001998126755187361,
        "epoch": 0.008420895744839407,
        "step": 113
    },
    {
        "loss": 2.0091,
        "grad_norm": 3.6458113193511963,
        "learning_rate": 0.0001998083458438814,
        "epoch": 0.008495416946121171,
        "step": 114
    },
    {
        "loss": 2.6579,
        "grad_norm": 2.8248302936553955,
        "learning_rate": 0.00019980396675064657,
        "epoch": 0.008569938147402936,
        "step": 115
    },
    {
        "loss": 2.1899,
        "grad_norm": 2.8737003803253174,
        "learning_rate": 0.0001997995382411998,
        "epoch": 0.008644459348684701,
        "step": 116
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.914163589477539,
        "learning_rate": 0.0001997950603177339,
        "epoch": 0.008718980549966466,
        "step": 117
    },
    {
        "loss": 2.5451,
        "grad_norm": 3.390716791152954,
        "learning_rate": 0.00019979053298246595,
        "epoch": 0.00879350175124823,
        "step": 118
    },
    {
        "loss": 2.6715,
        "grad_norm": 3.2030019760131836,
        "learning_rate": 0.00019978595623763761,
        "epoch": 0.008868022952529996,
        "step": 119
    },
    {
        "loss": 2.1716,
        "grad_norm": 3.2102720737457275,
        "learning_rate": 0.00019978133008551497,
        "epoch": 0.008942544153811759,
        "step": 120
    },
    {
        "loss": 2.9862,
        "grad_norm": 2.873950719833374,
        "learning_rate": 0.00019977665452838855,
        "epoch": 0.009017065355093523,
        "step": 121
    },
    {
        "loss": 2.6039,
        "grad_norm": 2.6346025466918945,
        "learning_rate": 0.00019977192956857346,
        "epoch": 0.009091586556375288,
        "step": 122
    },
    {
        "loss": 2.9462,
        "grad_norm": 2.665004253387451,
        "learning_rate": 0.00019976715520840914,
        "epoch": 0.009166107757657053,
        "step": 123
    },
    {
        "loss": 2.9557,
        "grad_norm": 2.7643890380859375,
        "learning_rate": 0.00019976233145025952,
        "epoch": 0.009240628958938818,
        "step": 124
    },
    {
        "loss": 3.0748,
        "grad_norm": 2.3486125469207764,
        "learning_rate": 0.00019975745829651297,
        "epoch": 0.009315150160220583,
        "step": 125
    },
    {
        "loss": 3.0872,
        "grad_norm": 1.8184871673583984,
        "learning_rate": 0.00019975253574958243,
        "epoch": 0.009389671361502348,
        "step": 126
    },
    {
        "loss": 2.8693,
        "grad_norm": 2.3522684574127197,
        "learning_rate": 0.00019974756381190516,
        "epoch": 0.009464192562784112,
        "step": 127
    },
    {
        "loss": 3.2644,
        "grad_norm": 2.6720330715179443,
        "learning_rate": 0.00019974254248594293,
        "epoch": 0.009538713764065877,
        "step": 128
    },
    {
        "loss": 2.5643,
        "grad_norm": 3.5137574672698975,
        "learning_rate": 0.000199737471774182,
        "epoch": 0.009613234965347642,
        "step": 129
    },
    {
        "loss": 2.4643,
        "grad_norm": 2.7833714485168457,
        "learning_rate": 0.000199732351679133,
        "epoch": 0.009687756166629405,
        "step": 130
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.975745439529419,
        "learning_rate": 0.00019972718220333103,
        "epoch": 0.00976227736791117,
        "step": 131
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.203991413116455,
        "learning_rate": 0.00019972196334933578,
        "epoch": 0.009836798569192935,
        "step": 132
    },
    {
        "loss": 2.9453,
        "grad_norm": 2.0888984203338623,
        "learning_rate": 0.00019971669511973115,
        "epoch": 0.0099113197704747,
        "step": 133
    },
    {
        "loss": 2.0348,
        "grad_norm": 3.1946074962615967,
        "learning_rate": 0.0001997113775171257,
        "epoch": 0.009985840971756465,
        "step": 134
    },
    {
        "loss": 2.2805,
        "grad_norm": 3.142944812774658,
        "learning_rate": 0.0001997060105441523,
        "epoch": 0.01006036217303823,
        "step": 135
    },
    {
        "loss": 2.3745,
        "grad_norm": 2.039764881134033,
        "learning_rate": 0.00019970059420346833,
        "epoch": 0.010134883374319994,
        "step": 136
    },
    {
        "loss": 2.7082,
        "grad_norm": 2.583636999130249,
        "learning_rate": 0.00019969512849775565,
        "epoch": 0.010209404575601759,
        "step": 137
    },
    {
        "loss": 2.7745,
        "grad_norm": 2.0732436180114746,
        "learning_rate": 0.00019968961342972042,
        "epoch": 0.010283925776883524,
        "step": 138
    },
    {
        "loss": 1.8364,
        "grad_norm": 2.4250807762145996,
        "learning_rate": 0.00019968404900209337,
        "epoch": 0.010358446978165289,
        "step": 139
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.0762507915496826,
        "learning_rate": 0.0001996784352176296,
        "epoch": 0.010432968179447054,
        "step": 140
    },
    {
        "loss": 3.1325,
        "grad_norm": 2.0380547046661377,
        "learning_rate": 0.00019967277207910878,
        "epoch": 0.010507489380728817,
        "step": 141
    },
    {
        "loss": 2.9868,
        "grad_norm": 2.055330991744995,
        "learning_rate": 0.0001996670595893348,
        "epoch": 0.010582010582010581,
        "step": 142
    },
    {
        "loss": 3.191,
        "grad_norm": 2.4279870986938477,
        "learning_rate": 0.00019966129775113615,
        "epoch": 0.010656531783292346,
        "step": 143
    },
    {
        "loss": 2.8712,
        "grad_norm": 3.6823551654815674,
        "learning_rate": 0.0001996554865673657,
        "epoch": 0.010731052984574111,
        "step": 144
    },
    {
        "loss": 2.8806,
        "grad_norm": 2.753755569458008,
        "learning_rate": 0.00019964962604090077,
        "epoch": 0.010805574185855876,
        "step": 145
    },
    {
        "loss": 2.6019,
        "grad_norm": 3.1438159942626953,
        "learning_rate": 0.00019964371617464308,
        "epoch": 0.01088009538713764,
        "step": 146
    },
    {
        "loss": 2.9294,
        "grad_norm": 2.5791819095611572,
        "learning_rate": 0.0001996377569715188,
        "epoch": 0.010954616588419406,
        "step": 147
    },
    {
        "loss": 2.5878,
        "grad_norm": 2.4080889225006104,
        "learning_rate": 0.00019963174843447854,
        "epoch": 0.01102913778970117,
        "step": 148
    },
    {
        "loss": 2.303,
        "grad_norm": 2.127222776412964,
        "learning_rate": 0.0001996256905664973,
        "epoch": 0.011103658990982935,
        "step": 149
    },
    {
        "loss": 2.729,
        "grad_norm": 2.166748285293579,
        "learning_rate": 0.00019961958337057456,
        "epoch": 0.0111781801922647,
        "step": 150
    },
    {
        "loss": 1.883,
        "grad_norm": 3.9497690200805664,
        "learning_rate": 0.00019961342684973418,
        "epoch": 0.011252701393546463,
        "step": 151
    },
    {
        "loss": 2.7224,
        "grad_norm": 2.653865098953247,
        "learning_rate": 0.00019960722100702442,
        "epoch": 0.011327222594828228,
        "step": 152
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.8777074813842773,
        "learning_rate": 0.00019960096584551807,
        "epoch": 0.011401743796109993,
        "step": 153
    },
    {
        "loss": 2.5076,
        "grad_norm": 3.814218282699585,
        "learning_rate": 0.00019959466136831218,
        "epoch": 0.011476264997391758,
        "step": 154
    },
    {
        "loss": 2.7264,
        "grad_norm": 3.0829086303710938,
        "learning_rate": 0.00019958830757852835,
        "epoch": 0.011550786198673522,
        "step": 155
    },
    {
        "loss": 2.2685,
        "grad_norm": 3.4524316787719727,
        "learning_rate": 0.00019958190447931257,
        "epoch": 0.011625307399955287,
        "step": 156
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.3283531665802,
        "learning_rate": 0.0001995754520738352,
        "epoch": 0.011699828601237052,
        "step": 157
    },
    {
        "loss": 2.3373,
        "grad_norm": 3.3159170150756836,
        "learning_rate": 0.00019956895036529103,
        "epoch": 0.011774349802518817,
        "step": 158
    },
    {
        "loss": 2.9153,
        "grad_norm": 2.7225735187530518,
        "learning_rate": 0.00019956239935689925,
        "epoch": 0.011848871003800582,
        "step": 159
    },
    {
        "loss": 2.0236,
        "grad_norm": 2.1878504753112793,
        "learning_rate": 0.00019955579905190352,
        "epoch": 0.011923392205082347,
        "step": 160
    },
    {
        "loss": 2.619,
        "grad_norm": 2.519227981567383,
        "learning_rate": 0.00019954914945357187,
        "epoch": 0.011997913406364111,
        "step": 161
    },
    {
        "loss": 2.4908,
        "grad_norm": 2.770979642868042,
        "learning_rate": 0.00019954245056519668,
        "epoch": 0.012072434607645875,
        "step": 162
    },
    {
        "loss": 2.3015,
        "grad_norm": 2.3921327590942383,
        "learning_rate": 0.00019953570239009482,
        "epoch": 0.01214695580892764,
        "step": 163
    },
    {
        "loss": 2.6112,
        "grad_norm": 2.336256265640259,
        "learning_rate": 0.00019952890493160757,
        "epoch": 0.012221477010209404,
        "step": 164
    },
    {
        "loss": 3.2018,
        "grad_norm": 3.5353925228118896,
        "learning_rate": 0.00019952205819310053,
        "epoch": 0.012295998211491169,
        "step": 165
    },
    {
        "loss": 2.1971,
        "grad_norm": 2.4386634826660156,
        "learning_rate": 0.00019951516217796376,
        "epoch": 0.012370519412772934,
        "step": 166
    },
    {
        "loss": 2.8866,
        "grad_norm": 3.354740619659424,
        "learning_rate": 0.00019950821688961167,
        "epoch": 0.012445040614054699,
        "step": 167
    },
    {
        "loss": 2.3226,
        "grad_norm": 2.3424127101898193,
        "learning_rate": 0.00019950122233148315,
        "epoch": 0.012519561815336464,
        "step": 168
    },
    {
        "loss": 2.4278,
        "grad_norm": 5.2722392082214355,
        "learning_rate": 0.0001994941785070414,
        "epoch": 0.012594083016618228,
        "step": 169
    },
    {
        "loss": 2.6993,
        "grad_norm": 2.199944019317627,
        "learning_rate": 0.00019948708541977407,
        "epoch": 0.012668604217899993,
        "step": 170
    },
    {
        "loss": 3.0434,
        "grad_norm": 1.762083888053894,
        "learning_rate": 0.00019947994307319315,
        "epoch": 0.012743125419181758,
        "step": 171
    },
    {
        "loss": 1.759,
        "grad_norm": 5.414395332336426,
        "learning_rate": 0.0001994727514708351,
        "epoch": 0.012817646620463521,
        "step": 172
    },
    {
        "loss": 3.2871,
        "grad_norm": 2.1649038791656494,
        "learning_rate": 0.00019946551061626066,
        "epoch": 0.012892167821745286,
        "step": 173
    },
    {
        "loss": 2.7056,
        "grad_norm": 1.9875465631484985,
        "learning_rate": 0.00019945822051305507,
        "epoch": 0.01296668902302705,
        "step": 174
    },
    {
        "loss": 2.4758,
        "grad_norm": 4.124451160430908,
        "learning_rate": 0.00019945088116482787,
        "epoch": 0.013041210224308816,
        "step": 175
    },
    {
        "loss": 3.0061,
        "grad_norm": 3.2071871757507324,
        "learning_rate": 0.00019944349257521298,
        "epoch": 0.01311573142559058,
        "step": 176
    },
    {
        "loss": 3.2164,
        "grad_norm": 1.7232329845428467,
        "learning_rate": 0.00019943605474786877,
        "epoch": 0.013190252626872345,
        "step": 177
    },
    {
        "loss": 2.5958,
        "grad_norm": 2.8448843955993652,
        "learning_rate": 0.000199428567686478,
        "epoch": 0.01326477382815411,
        "step": 178
    },
    {
        "loss": 2.968,
        "grad_norm": 1.8664032220840454,
        "learning_rate": 0.00019942103139474764,
        "epoch": 0.013339295029435875,
        "step": 179
    },
    {
        "loss": 3.2188,
        "grad_norm": 2.5587098598480225,
        "learning_rate": 0.00019941344587640924,
        "epoch": 0.01341381623071764,
        "step": 180
    },
    {
        "loss": 2.7607,
        "grad_norm": 1.6376852989196777,
        "learning_rate": 0.0001994058111352186,
        "epoch": 0.013488337431999405,
        "step": 181
    },
    {
        "loss": 2.434,
        "grad_norm": 2.478742837905884,
        "learning_rate": 0.000199398127174956,
        "epoch": 0.013562858633281168,
        "step": 182
    },
    {
        "loss": 2.7145,
        "grad_norm": 2.2412688732147217,
        "learning_rate": 0.00019939039399942594,
        "epoch": 0.013637379834562932,
        "step": 183
    },
    {
        "loss": 2.1,
        "grad_norm": 3.001227378845215,
        "learning_rate": 0.00019938261161245739,
        "epoch": 0.013711901035844697,
        "step": 184
    },
    {
        "loss": 2.0462,
        "grad_norm": 2.6059887409210205,
        "learning_rate": 0.0001993747800179037,
        "epoch": 0.013786422237126462,
        "step": 185
    },
    {
        "loss": 2.5987,
        "grad_norm": 3.4518320560455322,
        "learning_rate": 0.00019936689921964252,
        "epoch": 0.013860943438408227,
        "step": 186
    },
    {
        "loss": 2.3899,
        "grad_norm": 3.822659730911255,
        "learning_rate": 0.00019935896922157587,
        "epoch": 0.013935464639689992,
        "step": 187
    },
    {
        "loss": 2.3704,
        "grad_norm": 2.5640311241149902,
        "learning_rate": 0.00019935099002763016,
        "epoch": 0.014009985840971757,
        "step": 188
    },
    {
        "loss": 2.3417,
        "grad_norm": 2.6169826984405518,
        "learning_rate": 0.0001993429616417562,
        "epoch": 0.014084507042253521,
        "step": 189
    },
    {
        "loss": 2.6378,
        "grad_norm": 2.3103809356689453,
        "learning_rate": 0.00019933488406792907,
        "epoch": 0.014159028243535286,
        "step": 190
    },
    {
        "loss": 1.6314,
        "grad_norm": 2.9091238975524902,
        "learning_rate": 0.00019932675731014824,
        "epoch": 0.014233549444817051,
        "step": 191
    },
    {
        "loss": 2.3949,
        "grad_norm": 2.626035690307617,
        "learning_rate": 0.0001993185813724375,
        "epoch": 0.014308070646098816,
        "step": 192
    },
    {
        "loss": 2.9821,
        "grad_norm": 2.308361053466797,
        "learning_rate": 0.0001993103562588451,
        "epoch": 0.014382591847380579,
        "step": 193
    },
    {
        "loss": 1.7396,
        "grad_norm": 3.840651512145996,
        "learning_rate": 0.00019930208197344352,
        "epoch": 0.014457113048662344,
        "step": 194
    },
    {
        "loss": 2.5427,
        "grad_norm": 2.145146608352661,
        "learning_rate": 0.0001992937585203296,
        "epoch": 0.014531634249944109,
        "step": 195
    },
    {
        "loss": 2.564,
        "grad_norm": 4.03846549987793,
        "learning_rate": 0.00019928538590362465,
        "epoch": 0.014606155451225874,
        "step": 196
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.244967222213745,
        "learning_rate": 0.0001992769641274741,
        "epoch": 0.014680676652507638,
        "step": 197
    },
    {
        "loss": 1.8414,
        "grad_norm": 4.436456680297852,
        "learning_rate": 0.00019926849319604797,
        "epoch": 0.014755197853789403,
        "step": 198
    },
    {
        "loss": 2.5606,
        "grad_norm": 3.2029995918273926,
        "learning_rate": 0.0001992599731135404,
        "epoch": 0.014829719055071168,
        "step": 199
    },
    {
        "loss": 2.3739,
        "grad_norm": 3.1755433082580566,
        "learning_rate": 0.00019925140388417,
        "epoch": 0.014904240256352933,
        "step": 200
    },
    {
        "loss": 3.1058,
        "grad_norm": 4.186275005340576,
        "learning_rate": 0.0001992427855121797,
        "epoch": 0.014978761457634698,
        "step": 201
    },
    {
        "loss": 2.6764,
        "grad_norm": 2.5861494541168213,
        "learning_rate": 0.0001992341180018367,
        "epoch": 0.015053282658916462,
        "step": 202
    },
    {
        "loss": 1.896,
        "grad_norm": 3.2078323364257812,
        "learning_rate": 0.00019922540135743256,
        "epoch": 0.015127803860198226,
        "step": 203
    },
    {
        "loss": 2.5663,
        "grad_norm": 1.823121190071106,
        "learning_rate": 0.00019921663558328324,
        "epoch": 0.01520232506147999,
        "step": 204
    },
    {
        "loss": 2.356,
        "grad_norm": 2.6730875968933105,
        "learning_rate": 0.0001992078206837289,
        "epoch": 0.015276846262761755,
        "step": 205
    },
    {
        "loss": 2.8053,
        "grad_norm": 2.4690606594085693,
        "learning_rate": 0.00019919895666313409,
        "epoch": 0.01535136746404352,
        "step": 206
    },
    {
        "loss": 2.595,
        "grad_norm": 3.2538399696350098,
        "learning_rate": 0.00019919004352588767,
        "epoch": 0.015425888665325285,
        "step": 207
    },
    {
        "loss": 2.5315,
        "grad_norm": 2.979210376739502,
        "learning_rate": 0.00019918108127640291,
        "epoch": 0.01550040986660705,
        "step": 208
    },
    {
        "loss": 2.8617,
        "grad_norm": 2.4511921405792236,
        "learning_rate": 0.00019917206991911722,
        "epoch": 0.015574931067888815,
        "step": 209
    },
    {
        "loss": 2.3953,
        "grad_norm": 3.2106404304504395,
        "learning_rate": 0.00019916300945849248,
        "epoch": 0.01564945226917058,
        "step": 210
    },
    {
        "loss": 2.887,
        "grad_norm": 2.6526010036468506,
        "learning_rate": 0.00019915389989901474,
        "epoch": 0.015723973470452342,
        "step": 211
    },
    {
        "loss": 2.3721,
        "grad_norm": 2.768454074859619,
        "learning_rate": 0.0001991447412451945,
        "epoch": 0.01579849467173411,
        "step": 212
    },
    {
        "loss": 2.6884,
        "grad_norm": 1.9068617820739746,
        "learning_rate": 0.00019913553350156652,
        "epoch": 0.015873015873015872,
        "step": 213
    },
    {
        "loss": 2.3196,
        "grad_norm": 3.3647849559783936,
        "learning_rate": 0.00019912627667268982,
        "epoch": 0.01594753707429764,
        "step": 214
    },
    {
        "loss": 2.8321,
        "grad_norm": 2.7849252223968506,
        "learning_rate": 0.00019911697076314776,
        "epoch": 0.016022058275579402,
        "step": 215
    },
    {
        "loss": 1.7653,
        "grad_norm": 1.4575477838516235,
        "learning_rate": 0.00019910761577754805,
        "epoch": 0.01609657947686117,
        "step": 216
    },
    {
        "loss": 1.7832,
        "grad_norm": 3.09619402885437,
        "learning_rate": 0.0001990982117205226,
        "epoch": 0.01617110067814293,
        "step": 217
    },
    {
        "loss": 2.7971,
        "grad_norm": 2.7243049144744873,
        "learning_rate": 0.0001990887585967277,
        "epoch": 0.016245621879424698,
        "step": 218
    },
    {
        "loss": 3.0701,
        "grad_norm": 2.429110527038574,
        "learning_rate": 0.00019907925641084387,
        "epoch": 0.01632014308070646,
        "step": 219
    },
    {
        "loss": 2.7459,
        "grad_norm": 2.5954830646514893,
        "learning_rate": 0.00019906970516757596,
        "epoch": 0.016394664281988224,
        "step": 220
    },
    {
        "loss": 2.7458,
        "grad_norm": 2.0329482555389404,
        "learning_rate": 0.00019906010487165312,
        "epoch": 0.01646918548326999,
        "step": 221
    },
    {
        "loss": 2.6429,
        "grad_norm": 2.2746496200561523,
        "learning_rate": 0.00019905045552782874,
        "epoch": 0.016543706684551754,
        "step": 222
    },
    {
        "loss": 2.1171,
        "grad_norm": 2.846360206604004,
        "learning_rate": 0.00019904075714088056,
        "epoch": 0.01661822788583352,
        "step": 223
    },
    {
        "loss": 2.3401,
        "grad_norm": 4.6883955001831055,
        "learning_rate": 0.00019903100971561054,
        "epoch": 0.016692749087115284,
        "step": 224
    },
    {
        "loss": 2.622,
        "grad_norm": 4.029910564422607,
        "learning_rate": 0.00019902121325684498,
        "epoch": 0.01676727028839705,
        "step": 225
    },
    {
        "loss": 1.3946,
        "grad_norm": 3.8945302963256836,
        "learning_rate": 0.00019901136776943442,
        "epoch": 0.016841791489678813,
        "step": 226
    },
    {
        "loss": 2.5977,
        "grad_norm": 2.7454214096069336,
        "learning_rate": 0.00019900147325825366,
        "epoch": 0.01691631269096058,
        "step": 227
    },
    {
        "loss": 3.0007,
        "grad_norm": 2.3223321437835693,
        "learning_rate": 0.00019899152972820182,
        "epoch": 0.016990833892242343,
        "step": 228
    },
    {
        "loss": 2.6393,
        "grad_norm": 2.1814303398132324,
        "learning_rate": 0.00019898153718420227,
        "epoch": 0.017065355093524106,
        "step": 229
    },
    {
        "loss": 2.7086,
        "grad_norm": 2.68386173248291,
        "learning_rate": 0.0001989714956312026,
        "epoch": 0.017139876294805872,
        "step": 230
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.5535459518432617,
        "learning_rate": 0.00019896140507417477,
        "epoch": 0.017214397496087636,
        "step": 231
    },
    {
        "loss": 2.5948,
        "grad_norm": 3.0297393798828125,
        "learning_rate": 0.00019895126551811493,
        "epoch": 0.017288918697369402,
        "step": 232
    },
    {
        "loss": 1.6642,
        "grad_norm": 3.8526134490966797,
        "learning_rate": 0.0001989410769680435,
        "epoch": 0.017363439898651165,
        "step": 233
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.9698033332824707,
        "learning_rate": 0.00019893083942900511,
        "epoch": 0.017437961099932932,
        "step": 234
    },
    {
        "loss": 3.1271,
        "grad_norm": 2.3001248836517334,
        "learning_rate": 0.0001989205529060688,
        "epoch": 0.017512482301214695,
        "step": 235
    },
    {
        "loss": 2.939,
        "grad_norm": 2.622828483581543,
        "learning_rate": 0.00019891021740432773,
        "epoch": 0.01758700350249646,
        "step": 236
    },
    {
        "loss": 1.8576,
        "grad_norm": 2.630411386489868,
        "learning_rate": 0.00019889983292889932,
        "epoch": 0.017661524703778225,
        "step": 237
    },
    {
        "loss": 2.5136,
        "grad_norm": 3.8164753913879395,
        "learning_rate": 0.00019888939948492526,
        "epoch": 0.01773604590505999,
        "step": 238
    },
    {
        "loss": 2.7378,
        "grad_norm": 2.0301287174224854,
        "learning_rate": 0.0001988789170775715,
        "epoch": 0.017810567106341754,
        "step": 239
    },
    {
        "loss": 2.8179,
        "grad_norm": 2.560516595840454,
        "learning_rate": 0.00019886838571202828,
        "epoch": 0.017885088307623517,
        "step": 240
    },
    {
        "loss": 2.7149,
        "grad_norm": 2.5391452312469482,
        "learning_rate": 0.00019885780539350992,
        "epoch": 0.017959609508905284,
        "step": 241
    },
    {
        "loss": 2.5906,
        "grad_norm": 4.159440040588379,
        "learning_rate": 0.00019884717612725516,
        "epoch": 0.018034130710187047,
        "step": 242
    },
    {
        "loss": 2.9205,
        "grad_norm": 2.1416995525360107,
        "learning_rate": 0.00019883649791852686,
        "epoch": 0.018108651911468814,
        "step": 243
    },
    {
        "loss": 2.1565,
        "grad_norm": 2.92232346534729,
        "learning_rate": 0.0001988257707726122,
        "epoch": 0.018183173112750577,
        "step": 244
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.9518226385116577,
        "learning_rate": 0.00019881499469482248,
        "epoch": 0.018257694314032343,
        "step": 245
    },
    {
        "loss": 2.2158,
        "grad_norm": 2.126664400100708,
        "learning_rate": 0.0001988041696904933,
        "epoch": 0.018332215515314106,
        "step": 246
    },
    {
        "loss": 2.9385,
        "grad_norm": 2.940429449081421,
        "learning_rate": 0.0001987932957649845,
        "epoch": 0.018406736716595873,
        "step": 247
    },
    {
        "loss": 3.0105,
        "grad_norm": 2.726485013961792,
        "learning_rate": 0.00019878237292368013,
        "epoch": 0.018481257917877636,
        "step": 248
    },
    {
        "loss": 3.2835,
        "grad_norm": 1.9591838121414185,
        "learning_rate": 0.00019877140117198838,
        "epoch": 0.018555779119159403,
        "step": 249
    },
    {
        "loss": 2.1852,
        "grad_norm": 3.7339296340942383,
        "learning_rate": 0.0001987603805153418,
        "epoch": 0.018630300320441166,
        "step": 250
    },
    {
        "loss": 2.8841,
        "grad_norm": 1.9732619524002075,
        "learning_rate": 0.00019874931095919705,
        "epoch": 0.01870482152172293,
        "step": 251
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.2590150833129883,
        "learning_rate": 0.00019873819250903498,
        "epoch": 0.018779342723004695,
        "step": 252
    },
    {
        "loss": 2.6543,
        "grad_norm": 3.338024854660034,
        "learning_rate": 0.00019872702517036075,
        "epoch": 0.01885386392428646,
        "step": 253
    },
    {
        "loss": 2.4274,
        "grad_norm": 3.3795573711395264,
        "learning_rate": 0.0001987158089487037,
        "epoch": 0.018928385125568225,
        "step": 254
    },
    {
        "loss": 2.6941,
        "grad_norm": 2.1145410537719727,
        "learning_rate": 0.0001987045438496173,
        "epoch": 0.019002906326849988,
        "step": 255
    },
    {
        "loss": 3.1406,
        "grad_norm": 2.3448731899261475,
        "learning_rate": 0.00019869322987867927,
        "epoch": 0.019077427528131755,
        "step": 256
    },
    {
        "loss": 2.378,
        "grad_norm": 3.238302230834961,
        "learning_rate": 0.00019868186704149156,
        "epoch": 0.019151948729413518,
        "step": 257
    },
    {
        "loss": 2.5523,
        "grad_norm": 1.9785072803497314,
        "learning_rate": 0.00019867045534368026,
        "epoch": 0.019226469930695284,
        "step": 258
    },
    {
        "loss": 3.0625,
        "grad_norm": 2.960566520690918,
        "learning_rate": 0.00019865899479089568,
        "epoch": 0.019300991131977047,
        "step": 259
    },
    {
        "loss": 2.1438,
        "grad_norm": 3.9925146102905273,
        "learning_rate": 0.00019864748538881227,
        "epoch": 0.01937551233325881,
        "step": 260
    },
    {
        "loss": 2.6987,
        "grad_norm": 3.240563154220581,
        "learning_rate": 0.00019863592714312883,
        "epoch": 0.019450033534540577,
        "step": 261
    },
    {
        "loss": 2.8293,
        "grad_norm": 2.642512559890747,
        "learning_rate": 0.0001986243200595681,
        "epoch": 0.01952455473582234,
        "step": 262
    },
    {
        "loss": 2.6605,
        "grad_norm": 2.584266424179077,
        "learning_rate": 0.0001986126641438772,
        "epoch": 0.019599075937104107,
        "step": 263
    },
    {
        "loss": 2.7446,
        "grad_norm": 2.3157761096954346,
        "learning_rate": 0.00019860095940182735,
        "epoch": 0.01967359713838587,
        "step": 264
    },
    {
        "loss": 2.3032,
        "grad_norm": 3.08469295501709,
        "learning_rate": 0.0001985892058392139,
        "epoch": 0.019748118339667636,
        "step": 265
    },
    {
        "loss": 2.7314,
        "grad_norm": 3.41928768157959,
        "learning_rate": 0.00019857740346185653,
        "epoch": 0.0198226395409494,
        "step": 266
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.8182146549224854,
        "learning_rate": 0.00019856555227559884,
        "epoch": 0.019897160742231166,
        "step": 267
    },
    {
        "loss": 2.8132,
        "grad_norm": 2.268284797668457,
        "learning_rate": 0.00019855365228630884,
        "epoch": 0.01997168194351293,
        "step": 268
    },
    {
        "loss": 2.6103,
        "grad_norm": 2.5079588890075684,
        "learning_rate": 0.00019854170349987857,
        "epoch": 0.020046203144794696,
        "step": 269
    },
    {
        "loss": 2.1624,
        "grad_norm": 2.9155490398406982,
        "learning_rate": 0.0001985297059222243,
        "epoch": 0.02012072434607646,
        "step": 270
    },
    {
        "loss": 2.5239,
        "grad_norm": 2.2956299781799316,
        "learning_rate": 0.00019851765955928636,
        "epoch": 0.020195245547358222,
        "step": 271
    },
    {
        "loss": 3.0006,
        "grad_norm": 1.9349948167800903,
        "learning_rate": 0.00019850556441702936,
        "epoch": 0.02026976674863999,
        "step": 272
    },
    {
        "loss": 2.6875,
        "grad_norm": 2.486175298690796,
        "learning_rate": 0.00019849342050144196,
        "epoch": 0.02034428794992175,
        "step": 273
    },
    {
        "loss": 2.082,
        "grad_norm": 2.956130027770996,
        "learning_rate": 0.00019848122781853707,
        "epoch": 0.020418809151203518,
        "step": 274
    },
    {
        "loss": 2.4475,
        "grad_norm": 3.2856709957122803,
        "learning_rate": 0.0001984689863743516,
        "epoch": 0.02049333035248528,
        "step": 275
    },
    {
        "loss": 2.866,
        "grad_norm": 4.019532203674316,
        "learning_rate": 0.00019845669617494675,
        "epoch": 0.020567851553767048,
        "step": 276
    },
    {
        "loss": 2.4897,
        "grad_norm": 5.001373291015625,
        "learning_rate": 0.00019844435722640778,
        "epoch": 0.02064237275504881,
        "step": 277
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.7539031505584717,
        "learning_rate": 0.00019843196953484414,
        "epoch": 0.020716893956330577,
        "step": 278
    },
    {
        "loss": 2.2693,
        "grad_norm": 2.1882076263427734,
        "learning_rate": 0.0001984195331063893,
        "epoch": 0.02079141515761234,
        "step": 279
    },
    {
        "loss": 2.5131,
        "grad_norm": 2.372434616088867,
        "learning_rate": 0.00019840704794720103,
        "epoch": 0.020865936358894107,
        "step": 280
    },
    {
        "loss": 2.7099,
        "grad_norm": 2.6150646209716797,
        "learning_rate": 0.0001983945140634611,
        "epoch": 0.02094045756017587,
        "step": 281
    },
    {
        "loss": 2.3567,
        "grad_norm": 1.622429609298706,
        "learning_rate": 0.00019838193146137545,
        "epoch": 0.021014978761457633,
        "step": 282
    },
    {
        "loss": 2.2564,
        "grad_norm": 2.590038537979126,
        "learning_rate": 0.00019836930014717415,
        "epoch": 0.0210894999627394,
        "step": 283
    },
    {
        "loss": 1.5954,
        "grad_norm": 3.74820613861084,
        "learning_rate": 0.00019835662012711135,
        "epoch": 0.021164021164021163,
        "step": 284
    },
    {
        "loss": 2.0106,
        "grad_norm": 1.7170683145523071,
        "learning_rate": 0.0001983438914074654,
        "epoch": 0.02123854236530293,
        "step": 285
    },
    {
        "loss": 2.8875,
        "grad_norm": 4.194796085357666,
        "learning_rate": 0.0001983311139945386,
        "epoch": 0.021313063566584693,
        "step": 286
    },
    {
        "loss": 2.3476,
        "grad_norm": 2.538874864578247,
        "learning_rate": 0.00019831828789465758,
        "epoch": 0.02138758476786646,
        "step": 287
    },
    {
        "loss": 2.7938,
        "grad_norm": 3.387640953063965,
        "learning_rate": 0.0001983054131141729,
        "epoch": 0.021462105969148222,
        "step": 288
    },
    {
        "loss": 2.3662,
        "grad_norm": 3.1883394718170166,
        "learning_rate": 0.00019829248965945933,
        "epoch": 0.02153662717042999,
        "step": 289
    },
    {
        "loss": 2.5134,
        "grad_norm": 2.3273732662200928,
        "learning_rate": 0.00019827951753691566,
        "epoch": 0.021611148371711752,
        "step": 290
    },
    {
        "loss": 2.3581,
        "grad_norm": 3.475860118865967,
        "learning_rate": 0.00019826649675296477,
        "epoch": 0.02168566957299352,
        "step": 291
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.0376126766204834,
        "learning_rate": 0.00019825342731405378,
        "epoch": 0.02176019077427528,
        "step": 292
    },
    {
        "loss": 2.2834,
        "grad_norm": 3.149214506149292,
        "learning_rate": 0.00019824030922665372,
        "epoch": 0.021834711975557045,
        "step": 293
    },
    {
        "loss": 3.247,
        "grad_norm": 2.127793550491333,
        "learning_rate": 0.00019822714249725983,
        "epoch": 0.02190923317683881,
        "step": 294
    },
    {
        "loss": 2.7287,
        "grad_norm": 3.3594088554382324,
        "learning_rate": 0.00019821392713239132,
        "epoch": 0.021983754378120574,
        "step": 295
    },
    {
        "loss": 1.9573,
        "grad_norm": 6.0756354331970215,
        "learning_rate": 0.0001982006631385916,
        "epoch": 0.02205827557940234,
        "step": 296
    },
    {
        "loss": 2.855,
        "grad_norm": 3.7720212936401367,
        "learning_rate": 0.00019818735052242817,
        "epoch": 0.022132796780684104,
        "step": 297
    },
    {
        "loss": 2.8654,
        "grad_norm": 2.683230400085449,
        "learning_rate": 0.00019817398929049244,
        "epoch": 0.02220731798196587,
        "step": 298
    },
    {
        "loss": 2.947,
        "grad_norm": 1.400028109550476,
        "learning_rate": 0.00019816057944940002,
        "epoch": 0.022281839183247634,
        "step": 299
    },
    {
        "loss": 1.5732,
        "grad_norm": 3.918003797531128,
        "learning_rate": 0.00019814712100579056,
        "epoch": 0.0223563603845294,
        "step": 300
    },
    {
        "loss": 2.2332,
        "grad_norm": 2.646216630935669,
        "learning_rate": 0.0001981336139663278,
        "epoch": 0.022430881585811163,
        "step": 301
    },
    {
        "loss": 2.8363,
        "grad_norm": 2.9640867710113525,
        "learning_rate": 0.0001981200583376995,
        "epoch": 0.022505402787092926,
        "step": 302
    },
    {
        "loss": 1.6881,
        "grad_norm": 3.7513363361358643,
        "learning_rate": 0.0001981064541266175,
        "epoch": 0.022579923988374693,
        "step": 303
    },
    {
        "loss": 2.494,
        "grad_norm": 2.4711685180664062,
        "learning_rate": 0.0001980928013398177,
        "epoch": 0.022654445189656456,
        "step": 304
    },
    {
        "loss": 2.6762,
        "grad_norm": 2.8529529571533203,
        "learning_rate": 0.00019807909998406002,
        "epoch": 0.022728966390938223,
        "step": 305
    },
    {
        "loss": 2.4142,
        "grad_norm": 2.6943719387054443,
        "learning_rate": 0.00019806535006612846,
        "epoch": 0.022803487592219986,
        "step": 306
    },
    {
        "loss": 2.2973,
        "grad_norm": 2.643862247467041,
        "learning_rate": 0.0001980515515928311,
        "epoch": 0.022878008793501752,
        "step": 307
    },
    {
        "loss": 2.629,
        "grad_norm": 1.8901622295379639,
        "learning_rate": 0.00019803770457099993,
        "epoch": 0.022952529994783515,
        "step": 308
    },
    {
        "loss": 2.5769,
        "grad_norm": 3.0920157432556152,
        "learning_rate": 0.0001980238090074911,
        "epoch": 0.023027051196065282,
        "step": 309
    },
    {
        "loss": 2.5271,
        "grad_norm": 2.4249215126037598,
        "learning_rate": 0.0001980098649091848,
        "epoch": 0.023101572397347045,
        "step": 310
    },
    {
        "loss": 2.6664,
        "grad_norm": 2.827904462814331,
        "learning_rate": 0.00019799587228298512,
        "epoch": 0.02317609359862881,
        "step": 311
    },
    {
        "loss": 2.617,
        "grad_norm": 3.3442771434783936,
        "learning_rate": 0.00019798183113582036,
        "epoch": 0.023250614799910575,
        "step": 312
    },
    {
        "loss": 2.0326,
        "grad_norm": 4.2532243728637695,
        "learning_rate": 0.0001979677414746427,
        "epoch": 0.023325136001192338,
        "step": 313
    },
    {
        "loss": 2.4381,
        "grad_norm": 2.3706700801849365,
        "learning_rate": 0.00019795360330642842,
        "epoch": 0.023399657202474104,
        "step": 314
    },
    {
        "loss": 2.4059,
        "grad_norm": 4.382699012756348,
        "learning_rate": 0.00019793941663817774,
        "epoch": 0.023474178403755867,
        "step": 315
    },
    {
        "loss": 2.2942,
        "grad_norm": 3.040595293045044,
        "learning_rate": 0.000197925181476915,
        "epoch": 0.023548699605037634,
        "step": 316
    },
    {
        "loss": 1.5593,
        "grad_norm": 1.6303625106811523,
        "learning_rate": 0.0001979108978296885,
        "epoch": 0.023623220806319397,
        "step": 317
    },
    {
        "loss": 2.4854,
        "grad_norm": 3.5126068592071533,
        "learning_rate": 0.00019789656570357053,
        "epoch": 0.023697742007601164,
        "step": 318
    },
    {
        "loss": 2.4874,
        "grad_norm": 3.0700509548187256,
        "learning_rate": 0.00019788218510565734,
        "epoch": 0.023772263208882927,
        "step": 319
    },
    {
        "loss": 2.5872,
        "grad_norm": 2.8456859588623047,
        "learning_rate": 0.00019786775604306928,
        "epoch": 0.023846784410164693,
        "step": 320
    },
    {
        "loss": 2.5298,
        "grad_norm": 3.611297130584717,
        "learning_rate": 0.00019785327852295064,
        "epoch": 0.023921305611446456,
        "step": 321
    },
    {
        "loss": 2.4032,
        "grad_norm": 2.6204423904418945,
        "learning_rate": 0.00019783875255246973,
        "epoch": 0.023995826812728223,
        "step": 322
    },
    {
        "loss": 2.8305,
        "grad_norm": 2.554288864135742,
        "learning_rate": 0.00019782417813881884,
        "epoch": 0.024070348014009986,
        "step": 323
    },
    {
        "loss": 2.2377,
        "grad_norm": 3.011873245239258,
        "learning_rate": 0.0001978095552892142,
        "epoch": 0.02414486921529175,
        "step": 324
    },
    {
        "loss": 2.9421,
        "grad_norm": 2.904772996902466,
        "learning_rate": 0.0001977948840108961,
        "epoch": 0.024219390416573516,
        "step": 325
    },
    {
        "loss": 2.7137,
        "grad_norm": 2.6669118404388428,
        "learning_rate": 0.00019778016431112877,
        "epoch": 0.02429391161785528,
        "step": 326
    },
    {
        "loss": 2.5945,
        "grad_norm": 1.9552549123764038,
        "learning_rate": 0.00019776539619720037,
        "epoch": 0.024368432819137045,
        "step": 327
    },
    {
        "loss": 2.3157,
        "grad_norm": 3.494276762008667,
        "learning_rate": 0.00019775057967642314,
        "epoch": 0.02444295402041881,
        "step": 328
    },
    {
        "loss": 2.6824,
        "grad_norm": 2.700328826904297,
        "learning_rate": 0.00019773571475613318,
        "epoch": 0.024517475221700575,
        "step": 329
    },
    {
        "loss": 2.3932,
        "grad_norm": 2.8805081844329834,
        "learning_rate": 0.0001977208014436906,
        "epoch": 0.024591996422982338,
        "step": 330
    },
    {
        "loss": 3.072,
        "grad_norm": 1.9124774932861328,
        "learning_rate": 0.0001977058397464795,
        "epoch": 0.024666517624264105,
        "step": 331
    },
    {
        "loss": 3.1997,
        "grad_norm": 3.8823533058166504,
        "learning_rate": 0.00019769082967190788,
        "epoch": 0.024741038825545868,
        "step": 332
    },
    {
        "loss": 2.5209,
        "grad_norm": 3.3829410076141357,
        "learning_rate": 0.00019767577122740772,
        "epoch": 0.02481556002682763,
        "step": 333
    },
    {
        "loss": 2.1662,
        "grad_norm": 3.774583578109741,
        "learning_rate": 0.00019766066442043499,
        "epoch": 0.024890081228109397,
        "step": 334
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.60721492767334,
        "learning_rate": 0.00019764550925846947,
        "epoch": 0.02496460242939116,
        "step": 335
    },
    {
        "loss": 2.8911,
        "grad_norm": 1.8366999626159668,
        "learning_rate": 0.0001976303057490151,
        "epoch": 0.025039123630672927,
        "step": 336
    },
    {
        "loss": 2.4713,
        "grad_norm": 2.103527545928955,
        "learning_rate": 0.00019761505389959952,
        "epoch": 0.02511364483195469,
        "step": 337
    },
    {
        "loss": 2.7956,
        "grad_norm": 2.4063899517059326,
        "learning_rate": 0.00019759975371777452,
        "epoch": 0.025188166033236457,
        "step": 338
    },
    {
        "loss": 3.2288,
        "grad_norm": 2.9827322959899902,
        "learning_rate": 0.00019758440521111564,
        "epoch": 0.02526268723451822,
        "step": 339
    },
    {
        "loss": 1.6956,
        "grad_norm": 3.1609859466552734,
        "learning_rate": 0.00019756900838722245,
        "epoch": 0.025337208435799986,
        "step": 340
    },
    {
        "loss": 2.6761,
        "grad_norm": 4.255390644073486,
        "learning_rate": 0.00019755356325371848,
        "epoch": 0.02541172963708175,
        "step": 341
    },
    {
        "loss": 2.8027,
        "grad_norm": 2.3134005069732666,
        "learning_rate": 0.00019753806981825104,
        "epoch": 0.025486250838363516,
        "step": 342
    },
    {
        "loss": 2.7985,
        "grad_norm": 2.116445779800415,
        "learning_rate": 0.00019752252808849147,
        "epoch": 0.02556077203964528,
        "step": 343
    },
    {
        "loss": 2.5831,
        "grad_norm": 3.6499202251434326,
        "learning_rate": 0.00019750693807213501,
        "epoch": 0.025635293240927042,
        "step": 344
    },
    {
        "loss": 2.4828,
        "grad_norm": 2.2698731422424316,
        "learning_rate": 0.00019749129977690078,
        "epoch": 0.02570981444220881,
        "step": 345
    },
    {
        "loss": 2.4668,
        "grad_norm": 2.695357322692871,
        "learning_rate": 0.00019747561321053177,
        "epoch": 0.025784335643490572,
        "step": 346
    },
    {
        "loss": 2.211,
        "grad_norm": 2.8577880859375,
        "learning_rate": 0.00019745987838079495,
        "epoch": 0.02585885684477234,
        "step": 347
    },
    {
        "loss": 2.4541,
        "grad_norm": 2.3343734741210938,
        "learning_rate": 0.00019744409529548116,
        "epoch": 0.0259333780460541,
        "step": 348
    },
    {
        "loss": 1.3717,
        "grad_norm": 3.315256118774414,
        "learning_rate": 0.00019742826396240508,
        "epoch": 0.026007899247335868,
        "step": 349
    },
    {
        "loss": 2.9471,
        "grad_norm": 4.50032901763916,
        "learning_rate": 0.0001974123843894054,
        "epoch": 0.02608242044861763,
        "step": 350
    },
    {
        "loss": 1.9956,
        "grad_norm": 3.719123125076294,
        "learning_rate": 0.00019739645658434452,
        "epoch": 0.026156941649899398,
        "step": 351
    },
    {
        "loss": 2.7239,
        "grad_norm": 2.0048928260803223,
        "learning_rate": 0.00019738048055510885,
        "epoch": 0.02623146285118116,
        "step": 352
    },
    {
        "loss": 2.3854,
        "grad_norm": 2.7422430515289307,
        "learning_rate": 0.0001973644563096087,
        "epoch": 0.026305984052462927,
        "step": 353
    },
    {
        "loss": 2.2774,
        "grad_norm": 3.2837564945220947,
        "learning_rate": 0.0001973483838557781,
        "epoch": 0.02638050525374469,
        "step": 354
    },
    {
        "loss": 2.8133,
        "grad_norm": 2.7649314403533936,
        "learning_rate": 0.00019733226320157513,
        "epoch": 0.026455026455026454,
        "step": 355
    },
    {
        "loss": 3.1481,
        "grad_norm": 2.2611446380615234,
        "learning_rate": 0.00019731609435498165,
        "epoch": 0.02652954765630822,
        "step": 356
    },
    {
        "loss": 2.4443,
        "grad_norm": 2.61075758934021,
        "learning_rate": 0.00019729987732400336,
        "epoch": 0.026604068857589983,
        "step": 357
    },
    {
        "loss": 2.4118,
        "grad_norm": 3.693150043487549,
        "learning_rate": 0.00019728361211666983,
        "epoch": 0.02667859005887175,
        "step": 358
    },
    {
        "loss": 3.0418,
        "grad_norm": 2.3773810863494873,
        "learning_rate": 0.00019726729874103448,
        "epoch": 0.026753111260153513,
        "step": 359
    },
    {
        "loss": 2.8046,
        "grad_norm": 1.8005473613739014,
        "learning_rate": 0.00019725093720517466,
        "epoch": 0.02682763246143528,
        "step": 360
    },
    {
        "loss": 2.5153,
        "grad_norm": 3.001236915588379,
        "learning_rate": 0.00019723452751719146,
        "epoch": 0.026902153662717043,
        "step": 361
    },
    {
        "loss": 2.8957,
        "grad_norm": 2.2142300605773926,
        "learning_rate": 0.0001972180696852099,
        "epoch": 0.02697667486399881,
        "step": 362
    },
    {
        "loss": 2.7448,
        "grad_norm": 2.424281358718872,
        "learning_rate": 0.0001972015637173787,
        "epoch": 0.027051196065280572,
        "step": 363
    },
    {
        "loss": 2.6522,
        "grad_norm": 2.023641586303711,
        "learning_rate": 0.0001971850096218706,
        "epoch": 0.027125717266562335,
        "step": 364
    },
    {
        "loss": 2.8087,
        "grad_norm": 2.575103282928467,
        "learning_rate": 0.00019716840740688203,
        "epoch": 0.027200238467844102,
        "step": 365
    },
    {
        "loss": 2.8013,
        "grad_norm": 1.4115049839019775,
        "learning_rate": 0.0001971517570806333,
        "epoch": 0.027274759669125865,
        "step": 366
    },
    {
        "loss": 2.2012,
        "grad_norm": 2.6406962871551514,
        "learning_rate": 0.0001971350586513685,
        "epoch": 0.02734928087040763,
        "step": 367
    },
    {
        "loss": 2.3936,
        "grad_norm": 2.038818836212158,
        "learning_rate": 0.00019711831212735562,
        "epoch": 0.027423802071689395,
        "step": 368
    },
    {
        "loss": 2.8437,
        "grad_norm": 2.5446627140045166,
        "learning_rate": 0.0001971015175168864,
        "epoch": 0.02749832327297116,
        "step": 369
    },
    {
        "loss": 2.6649,
        "grad_norm": 3.021127462387085,
        "learning_rate": 0.00019708467482827636,
        "epoch": 0.027572844474252924,
        "step": 370
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.8594107627868652,
        "learning_rate": 0.00019706778406986493,
        "epoch": 0.02764736567553469,
        "step": 371
    },
    {
        "loss": 2.6612,
        "grad_norm": 2.369182586669922,
        "learning_rate": 0.00019705084525001524,
        "epoch": 0.027721886876816454,
        "step": 372
    },
    {
        "loss": 2.1699,
        "grad_norm": 1.8465054035186768,
        "learning_rate": 0.00019703385837711426,
        "epoch": 0.02779640807809822,
        "step": 373
    },
    {
        "loss": 2.4848,
        "grad_norm": 2.4027531147003174,
        "learning_rate": 0.00019701682345957272,
        "epoch": 0.027870929279379984,
        "step": 374
    },
    {
        "loss": 2.4726,
        "grad_norm": 2.3395607471466064,
        "learning_rate": 0.00019699974050582523,
        "epoch": 0.027945450480661747,
        "step": 375
    },
    {
        "loss": 1.6152,
        "grad_norm": 2.7072043418884277,
        "learning_rate": 0.00019698260952433005,
        "epoch": 0.028019971681943513,
        "step": 376
    },
    {
        "loss": 2.0082,
        "grad_norm": 3.27215313911438,
        "learning_rate": 0.00019696543052356936,
        "epoch": 0.028094492883225276,
        "step": 377
    },
    {
        "loss": 2.5283,
        "grad_norm": 2.629842519760132,
        "learning_rate": 0.00019694820351204894,
        "epoch": 0.028169014084507043,
        "step": 378
    },
    {
        "loss": 2.0268,
        "grad_norm": 4.273860454559326,
        "learning_rate": 0.00019693092849829856,
        "epoch": 0.028243535285788806,
        "step": 379
    },
    {
        "loss": 2.5221,
        "grad_norm": 2.7584073543548584,
        "learning_rate": 0.0001969136054908716,
        "epoch": 0.028318056487070573,
        "step": 380
    },
    {
        "loss": 2.3141,
        "grad_norm": 3.679084062576294,
        "learning_rate": 0.00019689623449834525,
        "epoch": 0.028392577688352336,
        "step": 381
    },
    {
        "loss": 2.7533,
        "grad_norm": 1.5922222137451172,
        "learning_rate": 0.00019687881552932045,
        "epoch": 0.028467098889634102,
        "step": 382
    },
    {
        "loss": 2.3973,
        "grad_norm": 3.3599467277526855,
        "learning_rate": 0.0001968613485924219,
        "epoch": 0.028541620090915865,
        "step": 383
    },
    {
        "loss": 2.899,
        "grad_norm": 1.5694407224655151,
        "learning_rate": 0.00019684383369629807,
        "epoch": 0.028616141292197632,
        "step": 384
    },
    {
        "loss": 2.5432,
        "grad_norm": 2.209960699081421,
        "learning_rate": 0.00019682627084962112,
        "epoch": 0.028690662493479395,
        "step": 385
    },
    {
        "loss": 2.5553,
        "grad_norm": 3.123081922531128,
        "learning_rate": 0.00019680866006108704,
        "epoch": 0.028765183694761158,
        "step": 386
    },
    {
        "loss": 2.4846,
        "grad_norm": 3.3743200302124023,
        "learning_rate": 0.0001967910013394155,
        "epoch": 0.028839704896042925,
        "step": 387
    },
    {
        "loss": 2.0942,
        "grad_norm": 2.6629111766815186,
        "learning_rate": 0.0001967732946933499,
        "epoch": 0.028914226097324688,
        "step": 388
    },
    {
        "loss": 2.9699,
        "grad_norm": 3.4908769130706787,
        "learning_rate": 0.00019675554013165735,
        "epoch": 0.028988747298606454,
        "step": 389
    },
    {
        "loss": 2.255,
        "grad_norm": 2.6308538913726807,
        "learning_rate": 0.00019673773766312874,
        "epoch": 0.029063268499888217,
        "step": 390
    },
    {
        "loss": 2.387,
        "grad_norm": 2.347405195236206,
        "learning_rate": 0.0001967198872965787,
        "epoch": 0.029137789701169984,
        "step": 391
    },
    {
        "loss": 2.3197,
        "grad_norm": 2.4875314235687256,
        "learning_rate": 0.00019670198904084548,
        "epoch": 0.029212310902451747,
        "step": 392
    },
    {
        "loss": 2.371,
        "grad_norm": 2.732041120529175,
        "learning_rate": 0.00019668404290479105,
        "epoch": 0.029286832103733514,
        "step": 393
    },
    {
        "loss": 2.325,
        "grad_norm": 1.8649812936782837,
        "learning_rate": 0.00019666604889730124,
        "epoch": 0.029361353305015277,
        "step": 394
    },
    {
        "loss": 2.1438,
        "grad_norm": 3.100490093231201,
        "learning_rate": 0.0001966480070272854,
        "epoch": 0.02943587450629704,
        "step": 395
    },
    {
        "loss": 1.8539,
        "grad_norm": 3.4573135375976562,
        "learning_rate": 0.00019662991730367663,
        "epoch": 0.029510395707578806,
        "step": 396
    },
    {
        "loss": 2.2445,
        "grad_norm": 2.788825511932373,
        "learning_rate": 0.0001966117797354318,
        "epoch": 0.02958491690886057,
        "step": 397
    },
    {
        "loss": 2.8713,
        "grad_norm": 2.234334945678711,
        "learning_rate": 0.0001965935943315314,
        "epoch": 0.029659438110142336,
        "step": 398
    },
    {
        "loss": 2.7221,
        "grad_norm": 3.2205967903137207,
        "learning_rate": 0.0001965753611009796,
        "epoch": 0.0297339593114241,
        "step": 399
    },
    {
        "loss": 2.9518,
        "grad_norm": 3.6430883407592773,
        "learning_rate": 0.00019655708005280432,
        "epoch": 0.029808480512705866,
        "step": 400
    },
    {
        "loss": 2.4945,
        "grad_norm": 3.5233659744262695,
        "learning_rate": 0.00019653875119605703,
        "epoch": 0.02988300171398763,
        "step": 401
    },
    {
        "loss": 2.018,
        "grad_norm": 4.188154697418213,
        "learning_rate": 0.00019652037453981298,
        "epoch": 0.029957522915269395,
        "step": 402
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.0686733722686768,
        "learning_rate": 0.00019650195009317107,
        "epoch": 0.03003204411655116,
        "step": 403
    },
    {
        "loss": 2.7299,
        "grad_norm": 2.5840682983398438,
        "learning_rate": 0.00019648347786525383,
        "epoch": 0.030106565317832925,
        "step": 404
    },
    {
        "loss": 1.9001,
        "grad_norm": 4.9304375648498535,
        "learning_rate": 0.00019646495786520743,
        "epoch": 0.030181086519114688,
        "step": 405
    },
    {
        "loss": 2.0905,
        "grad_norm": 3.172098398208618,
        "learning_rate": 0.00019644639010220184,
        "epoch": 0.03025560772039645,
        "step": 406
    },
    {
        "loss": 1.9686,
        "grad_norm": 2.9583959579467773,
        "learning_rate": 0.00019642777458543046,
        "epoch": 0.030330128921678218,
        "step": 407
    },
    {
        "loss": 2.1219,
        "grad_norm": 3.128852128982544,
        "learning_rate": 0.00019640911132411048,
        "epoch": 0.03040465012295998,
        "step": 408
    },
    {
        "loss": 2.8632,
        "grad_norm": 2.1199092864990234,
        "learning_rate": 0.00019639040032748268,
        "epoch": 0.030479171324241747,
        "step": 409
    },
    {
        "loss": 2.9835,
        "grad_norm": 4.150411128997803,
        "learning_rate": 0.0001963716416048115,
        "epoch": 0.03055369252552351,
        "step": 410
    },
    {
        "loss": 1.8259,
        "grad_norm": 3.4391422271728516,
        "learning_rate": 0.00019635283516538502,
        "epoch": 0.030628213726805277,
        "step": 411
    },
    {
        "loss": 2.7738,
        "grad_norm": 2.041335105895996,
        "learning_rate": 0.00019633398101851488,
        "epoch": 0.03070273492808704,
        "step": 412
    },
    {
        "loss": 1.9211,
        "grad_norm": 3.157114028930664,
        "learning_rate": 0.00019631507917353637,
        "epoch": 0.030777256129368807,
        "step": 413
    },
    {
        "loss": 2.9258,
        "grad_norm": 3.0928096771240234,
        "learning_rate": 0.00019629612963980848,
        "epoch": 0.03085177733065057,
        "step": 414
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.8241201639175415,
        "learning_rate": 0.00019627713242671374,
        "epoch": 0.030926298531932336,
        "step": 415
    },
    {
        "loss": 2.6357,
        "grad_norm": 2.389460563659668,
        "learning_rate": 0.00019625808754365819,
        "epoch": 0.0310008197332141,
        "step": 416
    },
    {
        "loss": 2.7937,
        "grad_norm": 2.87245512008667,
        "learning_rate": 0.0001962389950000717,
        "epoch": 0.031075340934495863,
        "step": 417
    },
    {
        "loss": 2.4939,
        "grad_norm": 1.9763050079345703,
        "learning_rate": 0.00019621985480540756,
        "epoch": 0.03114986213577763,
        "step": 418
    },
    {
        "loss": 2.2104,
        "grad_norm": 3.2452850341796875,
        "learning_rate": 0.00019620066696914267,
        "epoch": 0.031224383337059392,
        "step": 419
    },
    {
        "loss": 2.5202,
        "grad_norm": 3.1127798557281494,
        "learning_rate": 0.00019618143150077762,
        "epoch": 0.03129890453834116,
        "step": 420
    },
    {
        "loss": 2.484,
        "grad_norm": 2.781947374343872,
        "learning_rate": 0.0001961621484098365,
        "epoch": 0.03137342573962292,
        "step": 421
    },
    {
        "loss": 2.71,
        "grad_norm": 2.336365222930908,
        "learning_rate": 0.000196142817705867,
        "epoch": 0.031447946940904685,
        "step": 422
    },
    {
        "loss": 1.9202,
        "grad_norm": 3.038015127182007,
        "learning_rate": 0.00019612343939844035,
        "epoch": 0.031522468142186455,
        "step": 423
    },
    {
        "loss": 2.7224,
        "grad_norm": 3.59535813331604,
        "learning_rate": 0.00019610401349715143,
        "epoch": 0.03159698934346822,
        "step": 424
    },
    {
        "loss": 2.8113,
        "grad_norm": 3.4378561973571777,
        "learning_rate": 0.00019608454001161863,
        "epoch": 0.03167151054474998,
        "step": 425
    },
    {
        "loss": 2.7844,
        "grad_norm": 3.0014524459838867,
        "learning_rate": 0.0001960650189514839,
        "epoch": 0.031746031746031744,
        "step": 426
    },
    {
        "loss": 1.8907,
        "grad_norm": 4.316168785095215,
        "learning_rate": 0.00019604545032641277,
        "epoch": 0.03182055294731351,
        "step": 427
    },
    {
        "loss": 2.6135,
        "grad_norm": 2.7536263465881348,
        "learning_rate": 0.00019602583414609427,
        "epoch": 0.03189507414859528,
        "step": 428
    },
    {
        "loss": 2.1118,
        "grad_norm": 3.327190637588501,
        "learning_rate": 0.00019600617042024108,
        "epoch": 0.03196959534987704,
        "step": 429
    },
    {
        "loss": 2.5534,
        "grad_norm": 3.2235677242279053,
        "learning_rate": 0.00019598645915858926,
        "epoch": 0.032044116551158804,
        "step": 430
    },
    {
        "loss": 1.7973,
        "grad_norm": 1.4016414880752563,
        "learning_rate": 0.0001959667003708986,
        "epoch": 0.03211863775244057,
        "step": 431
    },
    {
        "loss": 2.1372,
        "grad_norm": 3.7784202098846436,
        "learning_rate": 0.00019594689406695227,
        "epoch": 0.03219315895372234,
        "step": 432
    },
    {
        "loss": 2.5829,
        "grad_norm": 3.9792943000793457,
        "learning_rate": 0.000195927040256557,
        "epoch": 0.0322676801550041,
        "step": 433
    },
    {
        "loss": 2.7064,
        "grad_norm": 2.7565011978149414,
        "learning_rate": 0.0001959071389495431,
        "epoch": 0.03234220135628586,
        "step": 434
    },
    {
        "loss": 2.2314,
        "grad_norm": 3.7370519638061523,
        "learning_rate": 0.00019588719015576435,
        "epoch": 0.032416722557567626,
        "step": 435
    },
    {
        "loss": 2.8835,
        "grad_norm": 3.196357488632202,
        "learning_rate": 0.0001958671938850981,
        "epoch": 0.032491243758849396,
        "step": 436
    },
    {
        "loss": 2.4643,
        "grad_norm": 1.991490125656128,
        "learning_rate": 0.00019584715014744504,
        "epoch": 0.03256576496013116,
        "step": 437
    },
    {
        "loss": 2.7479,
        "grad_norm": 2.5214669704437256,
        "learning_rate": 0.00019582705895272956,
        "epoch": 0.03264028616141292,
        "step": 438
    },
    {
        "loss": 2.2312,
        "grad_norm": 2.820760726928711,
        "learning_rate": 0.00019580692031089946,
        "epoch": 0.032714807362694685,
        "step": 439
    },
    {
        "loss": 2.3955,
        "grad_norm": 3.3214077949523926,
        "learning_rate": 0.00019578673423192606,
        "epoch": 0.03278932856397645,
        "step": 440
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.23748517036438,
        "learning_rate": 0.0001957665007258041,
        "epoch": 0.03286384976525822,
        "step": 441
    },
    {
        "loss": 2.5388,
        "grad_norm": 2.1780388355255127,
        "learning_rate": 0.0001957462198025519,
        "epoch": 0.03293837096653998,
        "step": 442
    },
    {
        "loss": 1.7502,
        "grad_norm": 2.897679328918457,
        "learning_rate": 0.00019572589147221118,
        "epoch": 0.033012892167821745,
        "step": 443
    },
    {
        "loss": 1.8659,
        "grad_norm": 3.614058017730713,
        "learning_rate": 0.00019570551574484718,
        "epoch": 0.03308741336910351,
        "step": 444
    },
    {
        "loss": 2.0915,
        "grad_norm": 3.2268660068511963,
        "learning_rate": 0.00019568509263054854,
        "epoch": 0.03316193457038528,
        "step": 445
    },
    {
        "loss": 2.1187,
        "grad_norm": 2.5859975814819336,
        "learning_rate": 0.0001956646221394275,
        "epoch": 0.03323645577166704,
        "step": 446
    },
    {
        "loss": 2.9255,
        "grad_norm": 3.0709824562072754,
        "learning_rate": 0.00019564410428161958,
        "epoch": 0.033310976972948804,
        "step": 447
    },
    {
        "loss": 2.7098,
        "grad_norm": 2.3888182640075684,
        "learning_rate": 0.0001956235390672839,
        "epoch": 0.03338549817423057,
        "step": 448
    },
    {
        "loss": 2.9864,
        "grad_norm": 1.2579023838043213,
        "learning_rate": 0.00019560292650660298,
        "epoch": 0.03346001937551233,
        "step": 449
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.8283355236053467,
        "learning_rate": 0.00019558226660978273,
        "epoch": 0.0335345405767941,
        "step": 450
    },
    {
        "loss": 2.7824,
        "grad_norm": 2.035475015640259,
        "learning_rate": 0.00019556155938705254,
        "epoch": 0.03360906177807586,
        "step": 451
    },
    {
        "loss": 1.8968,
        "grad_norm": 2.7076053619384766,
        "learning_rate": 0.00019554080484866527,
        "epoch": 0.033683582979357626,
        "step": 452
    },
    {
        "loss": 3.0422,
        "grad_norm": 2.147251605987549,
        "learning_rate": 0.00019552000300489714,
        "epoch": 0.03375810418063939,
        "step": 453
    },
    {
        "loss": 2.642,
        "grad_norm": 3.5825695991516113,
        "learning_rate": 0.00019549915386604788,
        "epoch": 0.03383262538192116,
        "step": 454
    },
    {
        "loss": 2.6963,
        "grad_norm": 2.7010397911071777,
        "learning_rate": 0.00019547825744244054,
        "epoch": 0.03390714658320292,
        "step": 455
    },
    {
        "loss": 2.6959,
        "grad_norm": 1.2668355703353882,
        "learning_rate": 0.0001954573137444216,
        "epoch": 0.033981667784484686,
        "step": 456
    },
    {
        "loss": 2.7097,
        "grad_norm": 2.0493383407592773,
        "learning_rate": 0.00019543632278236098,
        "epoch": 0.03405618898576645,
        "step": 457
    },
    {
        "loss": 3.142,
        "grad_norm": 2.7662439346313477,
        "learning_rate": 0.00019541528456665206,
        "epoch": 0.03413071018704821,
        "step": 458
    },
    {
        "loss": 2.4854,
        "grad_norm": 2.3229005336761475,
        "learning_rate": 0.00019539419910771146,
        "epoch": 0.03420523138832998,
        "step": 459
    },
    {
        "loss": 2.6727,
        "grad_norm": 3.684997797012329,
        "learning_rate": 0.0001953730664159793,
        "epoch": 0.034279752589611745,
        "step": 460
    },
    {
        "loss": 2.4497,
        "grad_norm": 2.186490535736084,
        "learning_rate": 0.00019535188650191912,
        "epoch": 0.03435427379089351,
        "step": 461
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.134640693664551,
        "learning_rate": 0.00019533065937601778,
        "epoch": 0.03442879499217527,
        "step": 462
    },
    {
        "loss": 2.5093,
        "grad_norm": 2.7135796546936035,
        "learning_rate": 0.00019530938504878548,
        "epoch": 0.03450331619345704,
        "step": 463
    },
    {
        "loss": 2.7064,
        "grad_norm": 5.597998142242432,
        "learning_rate": 0.00019528806353075586,
        "epoch": 0.034577837394738804,
        "step": 464
    },
    {
        "loss": 2.5524,
        "grad_norm": 2.547795057296753,
        "learning_rate": 0.00019526669483248588,
        "epoch": 0.03465235859602057,
        "step": 465
    },
    {
        "loss": 2.6438,
        "grad_norm": 1.977738618850708,
        "learning_rate": 0.00019524527896455591,
        "epoch": 0.03472687979730233,
        "step": 466
    },
    {
        "loss": 2.7936,
        "grad_norm": 1.2870203256607056,
        "learning_rate": 0.0001952238159375696,
        "epoch": 0.0348014009985841,
        "step": 467
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.2418160438537598,
        "learning_rate": 0.0001952023057621541,
        "epoch": 0.034875922199865864,
        "step": 468
    },
    {
        "loss": 2.773,
        "grad_norm": 2.5336780548095703,
        "learning_rate": 0.00019518074844895972,
        "epoch": 0.03495044340114763,
        "step": 469
    },
    {
        "loss": 2.3836,
        "grad_norm": 2.561138153076172,
        "learning_rate": 0.0001951591440086602,
        "epoch": 0.03502496460242939,
        "step": 470
    },
    {
        "loss": 1.7986,
        "grad_norm": 3.7722420692443848,
        "learning_rate": 0.00019513749245195263,
        "epoch": 0.03509948580371115,
        "step": 471
    },
    {
        "loss": 1.8944,
        "grad_norm": 3.6974966526031494,
        "learning_rate": 0.00019511579378955738,
        "epoch": 0.03517400700499292,
        "step": 472
    },
    {
        "loss": 2.8665,
        "grad_norm": 3.3503899574279785,
        "learning_rate": 0.0001950940480322182,
        "epoch": 0.035248528206274686,
        "step": 473
    },
    {
        "loss": 2.5435,
        "grad_norm": 2.0120646953582764,
        "learning_rate": 0.00019507225519070206,
        "epoch": 0.03532304940755645,
        "step": 474
    },
    {
        "loss": 2.4185,
        "grad_norm": 2.3241941928863525,
        "learning_rate": 0.00019505041527579937,
        "epoch": 0.03539757060883821,
        "step": 475
    },
    {
        "loss": 2.5714,
        "grad_norm": 2.8531177043914795,
        "learning_rate": 0.0001950285282983238,
        "epoch": 0.03547209181011998,
        "step": 476
    },
    {
        "loss": 2.1941,
        "grad_norm": 3.2165565490722656,
        "learning_rate": 0.00019500659426911225,
        "epoch": 0.035546613011401745,
        "step": 477
    },
    {
        "loss": 1.8278,
        "grad_norm": 2.3156678676605225,
        "learning_rate": 0.000194984613199025,
        "epoch": 0.03562113421268351,
        "step": 478
    },
    {
        "loss": 2.7378,
        "grad_norm": 2.547528028488159,
        "learning_rate": 0.0001949625850989456,
        "epoch": 0.03569565541396527,
        "step": 479
    },
    {
        "loss": 2.4091,
        "grad_norm": 3.0338833332061768,
        "learning_rate": 0.00019494050997978093,
        "epoch": 0.035770176615247035,
        "step": 480
    },
    {
        "loss": 3.1403,
        "grad_norm": 1.6107752323150635,
        "learning_rate": 0.00019491838785246104,
        "epoch": 0.035844697816528805,
        "step": 481
    },
    {
        "loss": 2.766,
        "grad_norm": 2.55322527885437,
        "learning_rate": 0.00019489621872793934,
        "epoch": 0.03591921901781057,
        "step": 482
    },
    {
        "loss": 2.8892,
        "grad_norm": 3.658453941345215,
        "learning_rate": 0.00019487400261719248,
        "epoch": 0.03599374021909233,
        "step": 483
    },
    {
        "loss": 2.1884,
        "grad_norm": 2.496629238128662,
        "learning_rate": 0.00019485173953122042,
        "epoch": 0.036068261420374094,
        "step": 484
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.826436161994934,
        "learning_rate": 0.0001948294294810463,
        "epoch": 0.036142782621655864,
        "step": 485
    },
    {
        "loss": 2.3357,
        "grad_norm": 2.959642171859741,
        "learning_rate": 0.00019480707247771658,
        "epoch": 0.03621730382293763,
        "step": 486
    },
    {
        "loss": 2.6964,
        "grad_norm": 2.688350200653076,
        "learning_rate": 0.00019478466853230093,
        "epoch": 0.03629182502421939,
        "step": 487
    },
    {
        "loss": 2.7745,
        "grad_norm": 1.8517959117889404,
        "learning_rate": 0.00019476221765589232,
        "epoch": 0.03636634622550115,
        "step": 488
    },
    {
        "loss": 3.004,
        "grad_norm": 3.405113458633423,
        "learning_rate": 0.00019473971985960683,
        "epoch": 0.036440867426782916,
        "step": 489
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.615107774734497,
        "learning_rate": 0.00019471717515458394,
        "epoch": 0.036515388628064686,
        "step": 490
    },
    {
        "loss": 2.0033,
        "grad_norm": 2.279130220413208,
        "learning_rate": 0.00019469458355198622,
        "epoch": 0.03658990982934645,
        "step": 491
    },
    {
        "loss": 1.4612,
        "grad_norm": 3.7391560077667236,
        "learning_rate": 0.00019467194506299953,
        "epoch": 0.03666443103062821,
        "step": 492
    },
    {
        "loss": 2.1656,
        "grad_norm": 3.250063180923462,
        "learning_rate": 0.0001946492596988329,
        "epoch": 0.036738952231909976,
        "step": 493
    },
    {
        "loss": 2.2188,
        "grad_norm": 2.7629027366638184,
        "learning_rate": 0.00019462652747071868,
        "epoch": 0.036813473433191746,
        "step": 494
    },
    {
        "loss": 2.3116,
        "grad_norm": 2.171297550201416,
        "learning_rate": 0.00019460374838991225,
        "epoch": 0.03688799463447351,
        "step": 495
    },
    {
        "loss": 2.5222,
        "grad_norm": 2.494650363922119,
        "learning_rate": 0.00019458092246769232,
        "epoch": 0.03696251583575527,
        "step": 496
    },
    {
        "loss": 2.9531,
        "grad_norm": 4.594224452972412,
        "learning_rate": 0.00019455804971536074,
        "epoch": 0.037037037037037035,
        "step": 497
    },
    {
        "loss": 2.414,
        "grad_norm": 1.7454830408096313,
        "learning_rate": 0.00019453513014424257,
        "epoch": 0.037111558238318805,
        "step": 498
    },
    {
        "loss": 2.5289,
        "grad_norm": 3.7516279220581055,
        "learning_rate": 0.00019451216376568603,
        "epoch": 0.03718607943960057,
        "step": 499
    },
    {
        "loss": 2.7787,
        "grad_norm": 2.827455520629883,
        "learning_rate": 0.00019448915059106252,
        "epoch": 0.03726060064088233,
        "step": 500
    },
    {
        "loss": 2.9595,
        "grad_norm": 2.629161834716797,
        "learning_rate": 0.00019446609063176662,
        "epoch": 0.037335121842164094,
        "step": 501
    },
    {
        "loss": 2.5423,
        "grad_norm": 2.7278032302856445,
        "learning_rate": 0.00019444298389921608,
        "epoch": 0.03740964304344586,
        "step": 502
    },
    {
        "loss": 2.6222,
        "grad_norm": 2.986443519592285,
        "learning_rate": 0.0001944198304048518,
        "epoch": 0.03748416424472763,
        "step": 503
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.7224302291870117,
        "learning_rate": 0.00019439663016013783,
        "epoch": 0.03755868544600939,
        "step": 504
    },
    {
        "loss": 2.7095,
        "grad_norm": 2.7724366188049316,
        "learning_rate": 0.00019437338317656138,
        "epoch": 0.037633206647291154,
        "step": 505
    },
    {
        "loss": 2.6648,
        "grad_norm": 1.8088126182556152,
        "learning_rate": 0.00019435008946563275,
        "epoch": 0.03770772784857292,
        "step": 506
    },
    {
        "loss": 2.516,
        "grad_norm": 3.657231092453003,
        "learning_rate": 0.00019432674903888548,
        "epoch": 0.03778224904985469,
        "step": 507
    },
    {
        "loss": 2.2355,
        "grad_norm": 3.403333902359009,
        "learning_rate": 0.00019430336190787614,
        "epoch": 0.03785677025113645,
        "step": 508
    },
    {
        "loss": 2.9976,
        "grad_norm": 2.3453691005706787,
        "learning_rate": 0.00019427992808418448,
        "epoch": 0.03793129145241821,
        "step": 509
    },
    {
        "loss": 2.7308,
        "grad_norm": 3.498668670654297,
        "learning_rate": 0.00019425644757941336,
        "epoch": 0.038005812653699976,
        "step": 510
    },
    {
        "loss": 1.9144,
        "grad_norm": 2.7888214588165283,
        "learning_rate": 0.0001942329204051887,
        "epoch": 0.03808033385498174,
        "step": 511
    },
    {
        "loss": 2.67,
        "grad_norm": 2.1441843509674072,
        "learning_rate": 0.00019420934657315964,
        "epoch": 0.03815485505626351,
        "step": 512
    },
    {
        "loss": 2.8064,
        "grad_norm": 2.9778025150299072,
        "learning_rate": 0.00019418572609499832,
        "epoch": 0.03822937625754527,
        "step": 513
    },
    {
        "loss": 2.7811,
        "grad_norm": 2.5222442150115967,
        "learning_rate": 0.0001941620589824,
        "epoch": 0.038303897458827035,
        "step": 514
    },
    {
        "loss": 3.0064,
        "grad_norm": 2.298313617706299,
        "learning_rate": 0.00019413834524708307,
        "epoch": 0.0383784186601088,
        "step": 515
    },
    {
        "loss": 2.3857,
        "grad_norm": 2.888869285583496,
        "learning_rate": 0.00019411458490078897,
        "epoch": 0.03845293986139057,
        "step": 516
    },
    {
        "loss": 2.0539,
        "grad_norm": 3.014204502105713,
        "learning_rate": 0.0001940907779552822,
        "epoch": 0.03852746106267233,
        "step": 517
    },
    {
        "loss": 2.4063,
        "grad_norm": 1.6636782884597778,
        "learning_rate": 0.00019406692442235044,
        "epoch": 0.038601982263954095,
        "step": 518
    },
    {
        "loss": 2.9147,
        "grad_norm": 1.907292127609253,
        "learning_rate": 0.00019404302431380422,
        "epoch": 0.03867650346523586,
        "step": 519
    },
    {
        "loss": 2.7509,
        "grad_norm": 1.8704203367233276,
        "learning_rate": 0.00019401907764147742,
        "epoch": 0.03875102466651762,
        "step": 520
    },
    {
        "loss": 1.2236,
        "grad_norm": 5.644522190093994,
        "learning_rate": 0.00019399508441722668,
        "epoch": 0.03882554586779939,
        "step": 521
    },
    {
        "loss": 2.6807,
        "grad_norm": 1.6952179670333862,
        "learning_rate": 0.00019397104465293194,
        "epoch": 0.038900067069081154,
        "step": 522
    },
    {
        "loss": 2.4567,
        "grad_norm": 1.3942365646362305,
        "learning_rate": 0.000193946958360496,
        "epoch": 0.03897458827036292,
        "step": 523
    },
    {
        "loss": 2.7393,
        "grad_norm": 0.969687283039093,
        "learning_rate": 0.0001939228255518448,
        "epoch": 0.03904910947164468,
        "step": 524
    },
    {
        "loss": 2.4662,
        "grad_norm": 3.51859188079834,
        "learning_rate": 0.0001938986462389273,
        "epoch": 0.03912363067292645,
        "step": 525
    },
    {
        "loss": 2.8991,
        "grad_norm": 2.3158559799194336,
        "learning_rate": 0.00019387442043371545,
        "epoch": 0.03919815187420821,
        "step": 526
    },
    {
        "loss": 2.5896,
        "grad_norm": 3.1463327407836914,
        "learning_rate": 0.00019385014814820428,
        "epoch": 0.039272673075489976,
        "step": 527
    },
    {
        "loss": 2.2152,
        "grad_norm": 5.592113494873047,
        "learning_rate": 0.0001938258293944117,
        "epoch": 0.03934719427677174,
        "step": 528
    },
    {
        "loss": 2.0829,
        "grad_norm": 3.199692726135254,
        "learning_rate": 0.00019380146418437882,
        "epoch": 0.03942171547805351,
        "step": 529
    },
    {
        "loss": 2.0958,
        "grad_norm": 3.016124963760376,
        "learning_rate": 0.0001937770525301696,
        "epoch": 0.03949623667933527,
        "step": 530
    },
    {
        "loss": 2.0519,
        "grad_norm": 3.196704149246216,
        "learning_rate": 0.00019375259444387105,
        "epoch": 0.039570757880617036,
        "step": 531
    },
    {
        "loss": 2.7415,
        "grad_norm": 2.393556833267212,
        "learning_rate": 0.00019372808993759318,
        "epoch": 0.0396452790818988,
        "step": 532
    },
    {
        "loss": 2.5003,
        "grad_norm": 2.1475789546966553,
        "learning_rate": 0.000193703539023469,
        "epoch": 0.03971980028318056,
        "step": 533
    },
    {
        "loss": 2.5978,
        "grad_norm": 2.3071045875549316,
        "learning_rate": 0.00019367894171365443,
        "epoch": 0.03979432148446233,
        "step": 534
    },
    {
        "loss": 2.6861,
        "grad_norm": 1.8916410207748413,
        "learning_rate": 0.00019365429802032839,
        "epoch": 0.039868842685744095,
        "step": 535
    },
    {
        "loss": 2.181,
        "grad_norm": 5.518014907836914,
        "learning_rate": 0.00019362960795569283,
        "epoch": 0.03994336388702586,
        "step": 536
    },
    {
        "loss": 2.3482,
        "grad_norm": 4.178539276123047,
        "learning_rate": 0.00019360487153197257,
        "epoch": 0.04001788508830762,
        "step": 537
    },
    {
        "loss": 2.3415,
        "grad_norm": 3.053260087966919,
        "learning_rate": 0.00019358008876141548,
        "epoch": 0.04009240628958939,
        "step": 538
    },
    {
        "loss": 2.6469,
        "grad_norm": 3.306222438812256,
        "learning_rate": 0.00019355525965629228,
        "epoch": 0.040166927490871154,
        "step": 539
    },
    {
        "loss": 2.6347,
        "grad_norm": 3.711024284362793,
        "learning_rate": 0.00019353038422889664,
        "epoch": 0.04024144869215292,
        "step": 540
    },
    {
        "loss": 2.8781,
        "grad_norm": 2.0757620334625244,
        "learning_rate": 0.00019350546249154526,
        "epoch": 0.04031596989343468,
        "step": 541
    },
    {
        "loss": 2.1758,
        "grad_norm": 2.6602559089660645,
        "learning_rate": 0.00019348049445657768,
        "epoch": 0.040390491094716444,
        "step": 542
    },
    {
        "loss": 2.2058,
        "grad_norm": 3.0673415660858154,
        "learning_rate": 0.00019345548013635638,
        "epoch": 0.040465012295998214,
        "step": 543
    },
    {
        "loss": 2.0595,
        "grad_norm": 3.084914207458496,
        "learning_rate": 0.0001934304195432668,
        "epoch": 0.04053953349727998,
        "step": 544
    },
    {
        "loss": 1.9239,
        "grad_norm": 4.469943523406982,
        "learning_rate": 0.00019340531268971725,
        "epoch": 0.04061405469856174,
        "step": 545
    },
    {
        "loss": 2.2986,
        "grad_norm": 3.4630870819091797,
        "learning_rate": 0.00019338015958813892,
        "epoch": 0.0406885758998435,
        "step": 546
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.817458391189575,
        "learning_rate": 0.00019335496025098598,
        "epoch": 0.04076309710112527,
        "step": 547
    },
    {
        "loss": 2.4758,
        "grad_norm": 1.4657132625579834,
        "learning_rate": 0.00019332971469073544,
        "epoch": 0.040837618302407036,
        "step": 548
    },
    {
        "loss": 2.8509,
        "grad_norm": 2.137195348739624,
        "learning_rate": 0.00019330442291988717,
        "epoch": 0.0409121395036888,
        "step": 549
    },
    {
        "loss": 2.8713,
        "grad_norm": 2.56345796585083,
        "learning_rate": 0.000193279084950964,
        "epoch": 0.04098666070497056,
        "step": 550
    },
    {
        "loss": 2.4998,
        "grad_norm": 2.4078662395477295,
        "learning_rate": 0.00019325370079651153,
        "epoch": 0.041061181906252325,
        "step": 551
    },
    {
        "loss": 2.3543,
        "grad_norm": 1.8701459169387817,
        "learning_rate": 0.00019322827046909836,
        "epoch": 0.041135703107534095,
        "step": 552
    },
    {
        "loss": 3.1538,
        "grad_norm": 3.9978108406066895,
        "learning_rate": 0.00019320279398131582,
        "epoch": 0.04121022430881586,
        "step": 553
    },
    {
        "loss": 2.653,
        "grad_norm": 2.9433698654174805,
        "learning_rate": 0.0001931772713457782,
        "epoch": 0.04128474551009762,
        "step": 554
    },
    {
        "loss": 2.6382,
        "grad_norm": 3.2774147987365723,
        "learning_rate": 0.0001931517025751225,
        "epoch": 0.041359266711379385,
        "step": 555
    },
    {
        "loss": 2.8375,
        "grad_norm": 2.025595188140869,
        "learning_rate": 0.00019312608768200877,
        "epoch": 0.041433787912661155,
        "step": 556
    },
    {
        "loss": 2.4836,
        "grad_norm": 2.2449212074279785,
        "learning_rate": 0.00019310042667911975,
        "epoch": 0.04150830911394292,
        "step": 557
    },
    {
        "loss": 2.7679,
        "grad_norm": 2.2291512489318848,
        "learning_rate": 0.000193074719579161,
        "epoch": 0.04158283031522468,
        "step": 558
    },
    {
        "loss": 2.6437,
        "grad_norm": 3.556933879852295,
        "learning_rate": 0.00019304896639486094,
        "epoch": 0.041657351516506444,
        "step": 559
    },
    {
        "loss": 2.6583,
        "grad_norm": 2.3195066452026367,
        "learning_rate": 0.0001930231671389709,
        "epoch": 0.041731872717788214,
        "step": 560
    },
    {
        "loss": 2.0511,
        "grad_norm": 2.796298027038574,
        "learning_rate": 0.00019299732182426484,
        "epoch": 0.04180639391906998,
        "step": 561
    },
    {
        "loss": 2.8232,
        "grad_norm": 2.2224934101104736,
        "learning_rate": 0.00019297143046353968,
        "epoch": 0.04188091512035174,
        "step": 562
    },
    {
        "loss": 2.2473,
        "grad_norm": 2.617108106613159,
        "learning_rate": 0.0001929454930696151,
        "epoch": 0.0419554363216335,
        "step": 563
    },
    {
        "loss": 2.4084,
        "grad_norm": 2.90020489692688,
        "learning_rate": 0.00019291950965533348,
        "epoch": 0.042029957522915266,
        "step": 564
    },
    {
        "loss": 2.6153,
        "grad_norm": 3.821059465408325,
        "learning_rate": 0.0001928934802335601,
        "epoch": 0.042104478724197036,
        "step": 565
    },
    {
        "loss": 1.4747,
        "grad_norm": 3.99151611328125,
        "learning_rate": 0.000192867404817183,
        "epoch": 0.0421789999254788,
        "step": 566
    },
    {
        "loss": 2.6363,
        "grad_norm": 2.2600481510162354,
        "learning_rate": 0.00019284128341911293,
        "epoch": 0.04225352112676056,
        "step": 567
    },
    {
        "loss": 2.4997,
        "grad_norm": 2.756286144256592,
        "learning_rate": 0.0001928151160522835,
        "epoch": 0.042328042328042326,
        "step": 568
    },
    {
        "loss": 2.7341,
        "grad_norm": 2.7038750648498535,
        "learning_rate": 0.00019278890272965096,
        "epoch": 0.042402563529324096,
        "step": 569
    },
    {
        "loss": 2.6279,
        "grad_norm": 2.91078519821167,
        "learning_rate": 0.0001927626434641944,
        "epoch": 0.04247708473060586,
        "step": 570
    },
    {
        "loss": 2.9344,
        "grad_norm": 2.0491230487823486,
        "learning_rate": 0.00019273633826891572,
        "epoch": 0.04255160593188762,
        "step": 571
    },
    {
        "loss": 2.6166,
        "grad_norm": 2.3475582599639893,
        "learning_rate": 0.0001927099871568394,
        "epoch": 0.042626127133169385,
        "step": 572
    },
    {
        "loss": 3.1871,
        "grad_norm": 2.926210880279541,
        "learning_rate": 0.00019268359014101277,
        "epoch": 0.04270064833445115,
        "step": 573
    },
    {
        "loss": 1.9493,
        "grad_norm": 3.0681824684143066,
        "learning_rate": 0.0001926571472345059,
        "epoch": 0.04277516953573292,
        "step": 574
    },
    {
        "loss": 2.6987,
        "grad_norm": 4.245543003082275,
        "learning_rate": 0.00019263065845041144,
        "epoch": 0.04284969073701468,
        "step": 575
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.1313183307647705,
        "learning_rate": 0.00019260412380184492,
        "epoch": 0.042924211938296444,
        "step": 576
    },
    {
        "loss": 2.0657,
        "grad_norm": 3.372443914413452,
        "learning_rate": 0.0001925775433019445,
        "epoch": 0.04299873313957821,
        "step": 577
    },
    {
        "loss": 2.3507,
        "grad_norm": 2.38507080078125,
        "learning_rate": 0.00019255091696387106,
        "epoch": 0.04307325434085998,
        "step": 578
    },
    {
        "loss": 2.6172,
        "grad_norm": 2.5680880546569824,
        "learning_rate": 0.00019252424480080817,
        "epoch": 0.04314777554214174,
        "step": 579
    },
    {
        "loss": 2.177,
        "grad_norm": 2.406524658203125,
        "learning_rate": 0.00019249752682596208,
        "epoch": 0.043222296743423504,
        "step": 580
    },
    {
        "loss": 3.5251,
        "grad_norm": 2.9018566608428955,
        "learning_rate": 0.00019247076305256176,
        "epoch": 0.04329681794470527,
        "step": 581
    },
    {
        "loss": 1.7878,
        "grad_norm": 3.8368966579437256,
        "learning_rate": 0.0001924439534938588,
        "epoch": 0.04337133914598704,
        "step": 582
    },
    {
        "loss": 2.5899,
        "grad_norm": 2.5866379737854004,
        "learning_rate": 0.0001924170981631275,
        "epoch": 0.0434458603472688,
        "step": 583
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.9798855781555176,
        "learning_rate": 0.00019239019707366483,
        "epoch": 0.04352038154855056,
        "step": 584
    },
    {
        "loss": 1.852,
        "grad_norm": 2.756971597671509,
        "learning_rate": 0.0001923632502387904,
        "epoch": 0.043594902749832326,
        "step": 585
    },
    {
        "loss": 2.3538,
        "grad_norm": 2.2509355545043945,
        "learning_rate": 0.00019233625767184643,
        "epoch": 0.04366942395111409,
        "step": 586
    },
    {
        "loss": 2.8962,
        "grad_norm": 1.8008718490600586,
        "learning_rate": 0.0001923092193861979,
        "epoch": 0.04374394515239586,
        "step": 587
    },
    {
        "loss": 2.5233,
        "grad_norm": 2.6520042419433594,
        "learning_rate": 0.00019228213539523224,
        "epoch": 0.04381846635367762,
        "step": 588
    },
    {
        "loss": 2.7526,
        "grad_norm": 2.7956717014312744,
        "learning_rate": 0.00019225500571235974,
        "epoch": 0.043892987554959385,
        "step": 589
    },
    {
        "loss": 2.7589,
        "grad_norm": 1.8486732244491577,
        "learning_rate": 0.0001922278303510131,
        "epoch": 0.04396750875624115,
        "step": 590
    },
    {
        "loss": 2.6886,
        "grad_norm": 2.2131428718566895,
        "learning_rate": 0.0001922006093246478,
        "epoch": 0.04404202995752292,
        "step": 591
    },
    {
        "loss": 2.8018,
        "grad_norm": 2.5645461082458496,
        "learning_rate": 0.0001921733426467418,
        "epoch": 0.04411655115880468,
        "step": 592
    },
    {
        "loss": 2.6331,
        "grad_norm": 2.1706457138061523,
        "learning_rate": 0.00019214603033079576,
        "epoch": 0.044191072360086445,
        "step": 593
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.833562135696411,
        "learning_rate": 0.00019211867239033294,
        "epoch": 0.04426559356136821,
        "step": 594
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.377983331680298,
        "learning_rate": 0.0001920912688388991,
        "epoch": 0.04434011476264997,
        "step": 595
    },
    {
        "loss": 2.097,
        "grad_norm": 2.966510057449341,
        "learning_rate": 0.0001920638196900626,
        "epoch": 0.04441463596393174,
        "step": 596
    },
    {
        "loss": 2.052,
        "grad_norm": 3.2175416946411133,
        "learning_rate": 0.0001920363249574145,
        "epoch": 0.044489157165213504,
        "step": 597
    },
    {
        "loss": 2.2789,
        "grad_norm": 2.4949047565460205,
        "learning_rate": 0.00019200878465456826,
        "epoch": 0.04456367836649527,
        "step": 598
    },
    {
        "loss": 2.1199,
        "grad_norm": 2.9058637619018555,
        "learning_rate": 0.00019198119879516005,
        "epoch": 0.04463819956777703,
        "step": 599
    },
    {
        "loss": 2.9195,
        "grad_norm": 3.0688304901123047,
        "learning_rate": 0.00019195356739284852,
        "epoch": 0.0447127207690588,
        "step": 600
    },
    {
        "loss": 2.9256,
        "grad_norm": 1.9582576751708984,
        "learning_rate": 0.00019192589046131486,
        "epoch": 0.04478724197034056,
        "step": 601
    },
    {
        "loss": 1.9682,
        "grad_norm": 3.6765472888946533,
        "learning_rate": 0.00019189816801426282,
        "epoch": 0.044861763171622326,
        "step": 602
    },
    {
        "loss": 2.6092,
        "grad_norm": 2.8763327598571777,
        "learning_rate": 0.00019187040006541872,
        "epoch": 0.04493628437290409,
        "step": 603
    },
    {
        "loss": 3.1455,
        "grad_norm": 2.1087727546691895,
        "learning_rate": 0.00019184258662853138,
        "epoch": 0.04501080557418585,
        "step": 604
    },
    {
        "loss": 1.8571,
        "grad_norm": 3.618858814239502,
        "learning_rate": 0.00019181472771737212,
        "epoch": 0.04508532677546762,
        "step": 605
    },
    {
        "loss": 2.9267,
        "grad_norm": 3.236912488937378,
        "learning_rate": 0.00019178682334573482,
        "epoch": 0.045159847976749386,
        "step": 606
    },
    {
        "loss": 2.4915,
        "grad_norm": 3.150592565536499,
        "learning_rate": 0.0001917588735274358,
        "epoch": 0.04523436917803115,
        "step": 607
    },
    {
        "loss": 3.0214,
        "grad_norm": 2.199498176574707,
        "learning_rate": 0.000191730878276314,
        "epoch": 0.04530889037931291,
        "step": 608
    },
    {
        "loss": 2.1191,
        "grad_norm": 2.5138731002807617,
        "learning_rate": 0.00019170283760623077,
        "epoch": 0.04538341158059468,
        "step": 609
    },
    {
        "loss": 2.0333,
        "grad_norm": 3.5659279823303223,
        "learning_rate": 0.0001916747515310699,
        "epoch": 0.045457932781876445,
        "step": 610
    },
    {
        "loss": 2.7307,
        "grad_norm": 2.215580701828003,
        "learning_rate": 0.00019164662006473782,
        "epoch": 0.04553245398315821,
        "step": 611
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.8025290966033936,
        "learning_rate": 0.00019161844322116326,
        "epoch": 0.04560697518443997,
        "step": 612
    },
    {
        "loss": 2.59,
        "grad_norm": 2.632917881011963,
        "learning_rate": 0.00019159022101429755,
        "epoch": 0.04568149638572174,
        "step": 613
    },
    {
        "loss": 2.6566,
        "grad_norm": 2.39347767829895,
        "learning_rate": 0.00019156195345811444,
        "epoch": 0.045756017587003504,
        "step": 614
    },
    {
        "loss": 2.4571,
        "grad_norm": 2.4371674060821533,
        "learning_rate": 0.00019153364056661003,
        "epoch": 0.04583053878828527,
        "step": 615
    },
    {
        "loss": 2.8108,
        "grad_norm": 2.022071123123169,
        "learning_rate": 0.00019150528235380306,
        "epoch": 0.04590505998956703,
        "step": 616
    },
    {
        "loss": 2.2219,
        "grad_norm": 2.914642095565796,
        "learning_rate": 0.00019147687883373454,
        "epoch": 0.045979581190848794,
        "step": 617
    },
    {
        "loss": 2.7113,
        "grad_norm": 2.288459062576294,
        "learning_rate": 0.00019144843002046806,
        "epoch": 0.046054102392130564,
        "step": 618
    },
    {
        "loss": 2.8936,
        "grad_norm": 2.7904064655303955,
        "learning_rate": 0.0001914199359280895,
        "epoch": 0.04612862359341233,
        "step": 619
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.9100570678710938,
        "learning_rate": 0.00019139139657070722,
        "epoch": 0.04620314479469409,
        "step": 620
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.7449865341186523,
        "learning_rate": 0.000191362811962452,
        "epoch": 0.04627766599597585,
        "step": 621
    },
    {
        "loss": 2.377,
        "grad_norm": 2.561859130859375,
        "learning_rate": 0.000191334182117477,
        "epoch": 0.04635218719725762,
        "step": 622
    },
    {
        "loss": 2.2536,
        "grad_norm": 3.033621311187744,
        "learning_rate": 0.00019130550704995783,
        "epoch": 0.046426708398539386,
        "step": 623
    },
    {
        "loss": 2.2059,
        "grad_norm": 4.056593418121338,
        "learning_rate": 0.00019127678677409245,
        "epoch": 0.04650122959982115,
        "step": 624
    },
    {
        "loss": 0.9999,
        "grad_norm": 3.5273241996765137,
        "learning_rate": 0.00019124802130410118,
        "epoch": 0.04657575080110291,
        "step": 625
    },
    {
        "loss": 3.2057,
        "grad_norm": 3.069594621658325,
        "learning_rate": 0.00019121921065422677,
        "epoch": 0.046650272002384675,
        "step": 626
    },
    {
        "loss": 1.8173,
        "grad_norm": 2.6051249504089355,
        "learning_rate": 0.0001911903548387343,
        "epoch": 0.046724793203666445,
        "step": 627
    },
    {
        "loss": 1.9142,
        "grad_norm": 3.9260096549987793,
        "learning_rate": 0.00019116145387191124,
        "epoch": 0.04679931440494821,
        "step": 628
    },
    {
        "loss": 2.6352,
        "grad_norm": 2.344435691833496,
        "learning_rate": 0.00019113250776806742,
        "epoch": 0.04687383560622997,
        "step": 629
    },
    {
        "loss": 2.4707,
        "grad_norm": 3.6410930156707764,
        "learning_rate": 0.00019110351654153494,
        "epoch": 0.046948356807511735,
        "step": 630
    },
    {
        "loss": 1.9464,
        "grad_norm": 2.3207693099975586,
        "learning_rate": 0.00019107448020666838,
        "epoch": 0.047022878008793505,
        "step": 631
    },
    {
        "loss": 2.491,
        "grad_norm": 1.7522268295288086,
        "learning_rate": 0.00019104539877784455,
        "epoch": 0.04709739921007527,
        "step": 632
    },
    {
        "loss": 2.0769,
        "grad_norm": 3.8703277111053467,
        "learning_rate": 0.0001910162722694626,
        "epoch": 0.04717192041135703,
        "step": 633
    },
    {
        "loss": 2.5411,
        "grad_norm": 2.524768114089966,
        "learning_rate": 0.00019098710069594406,
        "epoch": 0.047246441612638794,
        "step": 634
    },
    {
        "loss": 2.8124,
        "grad_norm": 2.0117671489715576,
        "learning_rate": 0.0001909578840717327,
        "epoch": 0.04732096281392056,
        "step": 635
    },
    {
        "loss": 1.938,
        "grad_norm": 3.444199323654175,
        "learning_rate": 0.00019092862241129464,
        "epoch": 0.04739548401520233,
        "step": 636
    },
    {
        "loss": 2.5493,
        "grad_norm": 3.2736549377441406,
        "learning_rate": 0.0001908993157291183,
        "epoch": 0.04747000521648409,
        "step": 637
    },
    {
        "loss": 2.8956,
        "grad_norm": 1.6907609701156616,
        "learning_rate": 0.00019086996403971433,
        "epoch": 0.04754452641776585,
        "step": 638
    },
    {
        "loss": 2.6582,
        "grad_norm": 1.8965599536895752,
        "learning_rate": 0.00019084056735761573,
        "epoch": 0.047619047619047616,
        "step": 639
    },
    {
        "loss": 3.0842,
        "grad_norm": 1.7745532989501953,
        "learning_rate": 0.0001908111256973778,
        "epoch": 0.047693568820329386,
        "step": 640
    },
    {
        "loss": 1.8503,
        "grad_norm": 3.064831495285034,
        "learning_rate": 0.00019078163907357802,
        "epoch": 0.04776809002161115,
        "step": 641
    },
    {
        "loss": 1.9116,
        "grad_norm": 3.8983154296875,
        "learning_rate": 0.00019075210750081625,
        "epoch": 0.04784261122289291,
        "step": 642
    },
    {
        "loss": 2.3901,
        "grad_norm": 2.906601667404175,
        "learning_rate": 0.00019072253099371443,
        "epoch": 0.047917132424174676,
        "step": 643
    },
    {
        "loss": 2.3277,
        "grad_norm": 3.390504837036133,
        "learning_rate": 0.00019069290956691697,
        "epoch": 0.047991653625456446,
        "step": 644
    },
    {
        "loss": 2.4117,
        "grad_norm": 2.876337766647339,
        "learning_rate": 0.00019066324323509034,
        "epoch": 0.04806617482673821,
        "step": 645
    },
    {
        "loss": 2.1881,
        "grad_norm": 2.8521664142608643,
        "learning_rate": 0.00019063353201292333,
        "epoch": 0.04814069602801997,
        "step": 646
    },
    {
        "loss": 1.7052,
        "grad_norm": 4.54654598236084,
        "learning_rate": 0.00019060377591512695,
        "epoch": 0.048215217229301735,
        "step": 647
    },
    {
        "loss": 2.7162,
        "grad_norm": 1.9950966835021973,
        "learning_rate": 0.00019057397495643439,
        "epoch": 0.0482897384305835,
        "step": 648
    },
    {
        "loss": 1.9632,
        "grad_norm": 4.160831928253174,
        "learning_rate": 0.00019054412915160113,
        "epoch": 0.04836425963186527,
        "step": 649
    },
    {
        "loss": 2.6567,
        "grad_norm": 2.2657740116119385,
        "learning_rate": 0.00019051423851540472,
        "epoch": 0.04843878083314703,
        "step": 650
    },
    {
        "loss": 2.4042,
        "grad_norm": 3.193761110305786,
        "learning_rate": 0.0001904843030626451,
        "epoch": 0.048513302034428794,
        "step": 651
    },
    {
        "loss": 2.478,
        "grad_norm": 2.842343807220459,
        "learning_rate": 0.0001904543228081442,
        "epoch": 0.04858782323571056,
        "step": 652
    },
    {
        "loss": 2.3379,
        "grad_norm": 2.64667010307312,
        "learning_rate": 0.00019042429776674628,
        "epoch": 0.04866234443699233,
        "step": 653
    },
    {
        "loss": 2.1565,
        "grad_norm": 2.8140392303466797,
        "learning_rate": 0.00019039422795331771,
        "epoch": 0.04873686563827409,
        "step": 654
    },
    {
        "loss": 2.8517,
        "grad_norm": 2.4199087619781494,
        "learning_rate": 0.00019036411338274703,
        "epoch": 0.048811386839555854,
        "step": 655
    },
    {
        "loss": 2.0382,
        "grad_norm": 3.3467020988464355,
        "learning_rate": 0.00019033395406994498,
        "epoch": 0.04888590804083762,
        "step": 656
    },
    {
        "loss": 2.927,
        "grad_norm": 3.102889060974121,
        "learning_rate": 0.00019030375002984437,
        "epoch": 0.04896042924211938,
        "step": 657
    },
    {
        "loss": 3.1628,
        "grad_norm": 2.820340633392334,
        "learning_rate": 0.00019027350127740026,
        "epoch": 0.04903495044340115,
        "step": 658
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.055462121963501,
        "learning_rate": 0.00019024320782758976,
        "epoch": 0.04910947164468291,
        "step": 659
    },
    {
        "loss": 2.6416,
        "grad_norm": 2.96091890335083,
        "learning_rate": 0.00019021286969541216,
        "epoch": 0.049183992845964676,
        "step": 660
    },
    {
        "loss": 1.8867,
        "grad_norm": 5.085737705230713,
        "learning_rate": 0.0001901824868958889,
        "epoch": 0.04925851404724644,
        "step": 661
    },
    {
        "loss": 2.2272,
        "grad_norm": 3.271460771560669,
        "learning_rate": 0.00019015205944406343,
        "epoch": 0.04933303524852821,
        "step": 662
    },
    {
        "loss": 2.051,
        "grad_norm": 3.3268392086029053,
        "learning_rate": 0.00019012158735500143,
        "epoch": 0.04940755644980997,
        "step": 663
    },
    {
        "loss": 1.7088,
        "grad_norm": 3.1809239387512207,
        "learning_rate": 0.0001900910706437906,
        "epoch": 0.049482077651091735,
        "step": 664
    },
    {
        "loss": 2.9092,
        "grad_norm": 2.0661985874176025,
        "learning_rate": 0.00019006050932554082,
        "epoch": 0.0495565988523735,
        "step": 665
    },
    {
        "loss": 2.4205,
        "grad_norm": 3.5693211555480957,
        "learning_rate": 0.0001900299034153839,
        "epoch": 0.04963112005365526,
        "step": 666
    },
    {
        "loss": 2.3423,
        "grad_norm": 2.940855026245117,
        "learning_rate": 0.0001899992529284739,
        "epoch": 0.04970564125493703,
        "step": 667
    },
    {
        "loss": 2.3443,
        "grad_norm": 4.5124192237854,
        "learning_rate": 0.00018996855787998686,
        "epoch": 0.049780162456218795,
        "step": 668
    },
    {
        "loss": 2.3322,
        "grad_norm": 3.999748945236206,
        "learning_rate": 0.00018993781828512092,
        "epoch": 0.04985468365750056,
        "step": 669
    },
    {
        "loss": 2.581,
        "grad_norm": 2.1557538509368896,
        "learning_rate": 0.00018990703415909624,
        "epoch": 0.04992920485878232,
        "step": 670
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.686724901199341,
        "learning_rate": 0.00018987620551715506,
        "epoch": 0.05000372606006409,
        "step": 671
    },
    {
        "loss": 2.705,
        "grad_norm": 3.1190319061279297,
        "learning_rate": 0.00018984533237456167,
        "epoch": 0.050078247261345854,
        "step": 672
    },
    {
        "loss": 1.4241,
        "grad_norm": 4.96881628036499,
        "learning_rate": 0.00018981441474660228,
        "epoch": 0.05015276846262762,
        "step": 673
    },
    {
        "loss": 2.6126,
        "grad_norm": 2.565077066421509,
        "learning_rate": 0.00018978345264858534,
        "epoch": 0.05022728966390938,
        "step": 674
    },
    {
        "loss": 2.7713,
        "grad_norm": 3.356178045272827,
        "learning_rate": 0.00018975244609584116,
        "epoch": 0.05030181086519115,
        "step": 675
    },
    {
        "loss": 3.0382,
        "grad_norm": 3.4069719314575195,
        "learning_rate": 0.00018972139510372205,
        "epoch": 0.05037633206647291,
        "step": 676
    },
    {
        "loss": 2.4231,
        "grad_norm": 1.6224290132522583,
        "learning_rate": 0.00018969029968760243,
        "epoch": 0.050450853267754676,
        "step": 677
    },
    {
        "loss": 2.1682,
        "grad_norm": 2.2527072429656982,
        "learning_rate": 0.00018965915986287867,
        "epoch": 0.05052537446903644,
        "step": 678
    },
    {
        "loss": 2.5561,
        "grad_norm": 3.1711087226867676,
        "learning_rate": 0.00018962797564496905,
        "epoch": 0.0505998956703182,
        "step": 679
    },
    {
        "loss": 2.8179,
        "grad_norm": 3.2499728202819824,
        "learning_rate": 0.00018959674704931392,
        "epoch": 0.05067441687159997,
        "step": 680
    },
    {
        "loss": 2.8234,
        "grad_norm": 3.0265047550201416,
        "learning_rate": 0.0001895654740913756,
        "epoch": 0.050748938072881736,
        "step": 681
    },
    {
        "loss": 2.4754,
        "grad_norm": 3.233220338821411,
        "learning_rate": 0.00018953415678663838,
        "epoch": 0.0508234592741635,
        "step": 682
    },
    {
        "loss": 2.7664,
        "grad_norm": 1.3851332664489746,
        "learning_rate": 0.00018950279515060842,
        "epoch": 0.05089798047544526,
        "step": 683
    },
    {
        "loss": 2.342,
        "grad_norm": 2.5989067554473877,
        "learning_rate": 0.00018947138919881394,
        "epoch": 0.05097250167672703,
        "step": 684
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.421827793121338,
        "learning_rate": 0.00018943993894680503,
        "epoch": 0.051047022878008795,
        "step": 685
    },
    {
        "loss": 2.5421,
        "grad_norm": 2.584139347076416,
        "learning_rate": 0.00018940844441015376,
        "epoch": 0.05112154407929056,
        "step": 686
    },
    {
        "loss": 2.0558,
        "grad_norm": 3.352818727493286,
        "learning_rate": 0.00018937690560445408,
        "epoch": 0.05119606528057232,
        "step": 687
    },
    {
        "loss": 2.5536,
        "grad_norm": 2.6260335445404053,
        "learning_rate": 0.00018934532254532194,
        "epoch": 0.051270586481854084,
        "step": 688
    },
    {
        "loss": 1.6402,
        "grad_norm": 6.967348575592041,
        "learning_rate": 0.00018931369524839508,
        "epoch": 0.051345107683135854,
        "step": 689
    },
    {
        "loss": 3.0735,
        "grad_norm": 2.9986536502838135,
        "learning_rate": 0.00018928202372933326,
        "epoch": 0.05141962888441762,
        "step": 690
    },
    {
        "loss": 2.7405,
        "grad_norm": 3.5878241062164307,
        "learning_rate": 0.00018925030800381803,
        "epoch": 0.05149415008569938,
        "step": 691
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.985567331314087,
        "learning_rate": 0.00018921854808755294,
        "epoch": 0.051568671286981144,
        "step": 692
    },
    {
        "loss": 2.5637,
        "grad_norm": 3.1549274921417236,
        "learning_rate": 0.00018918674399626332,
        "epoch": 0.051643192488262914,
        "step": 693
    },
    {
        "loss": 3.0393,
        "grad_norm": 2.967703342437744,
        "learning_rate": 0.00018915489574569647,
        "epoch": 0.05171771368954468,
        "step": 694
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.660398244857788,
        "learning_rate": 0.00018912300335162144,
        "epoch": 0.05179223489082644,
        "step": 695
    },
    {
        "loss": 1.9742,
        "grad_norm": 2.877993583679199,
        "learning_rate": 0.00018909106682982928,
        "epoch": 0.0518667560921082,
        "step": 696
    },
    {
        "loss": 2.5267,
        "grad_norm": 2.1872811317443848,
        "learning_rate": 0.00018905908619613272,
        "epoch": 0.051941277293389966,
        "step": 697
    },
    {
        "loss": 1.6633,
        "grad_norm": 5.3625617027282715,
        "learning_rate": 0.00018902706146636645,
        "epoch": 0.052015798494671736,
        "step": 698
    },
    {
        "loss": 2.6263,
        "grad_norm": 2.218595266342163,
        "learning_rate": 0.00018899499265638702,
        "epoch": 0.0520903196959535,
        "step": 699
    },
    {
        "loss": 2.2755,
        "grad_norm": 2.7844202518463135,
        "learning_rate": 0.00018896287978207264,
        "epoch": 0.05216484089723526,
        "step": 700
    },
    {
        "loss": 2.09,
        "grad_norm": 2.371257781982422,
        "learning_rate": 0.00018893072285932353,
        "epoch": 0.052239362098517025,
        "step": 701
    },
    {
        "loss": 2.2917,
        "grad_norm": 2.8714916706085205,
        "learning_rate": 0.00018889852190406164,
        "epoch": 0.052313883299798795,
        "step": 702
    },
    {
        "loss": 1.6897,
        "grad_norm": 3.328766345977783,
        "learning_rate": 0.00018886627693223064,
        "epoch": 0.05238840450108056,
        "step": 703
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.6993401050567627,
        "learning_rate": 0.00018883398795979613,
        "epoch": 0.05246292570236232,
        "step": 704
    },
    {
        "loss": 2.2872,
        "grad_norm": 3.248945713043213,
        "learning_rate": 0.0001888016550027454,
        "epoch": 0.052537446903644085,
        "step": 705
    },
    {
        "loss": 2.4415,
        "grad_norm": 3.7193548679351807,
        "learning_rate": 0.0001887692780770876,
        "epoch": 0.052611968104925855,
        "step": 706
    },
    {
        "loss": 3.2129,
        "grad_norm": 2.9758920669555664,
        "learning_rate": 0.0001887368571988536,
        "epoch": 0.05268648930620762,
        "step": 707
    },
    {
        "loss": 2.1752,
        "grad_norm": 4.189254283905029,
        "learning_rate": 0.00018870439238409601,
        "epoch": 0.05276101050748938,
        "step": 708
    },
    {
        "loss": 2.8836,
        "grad_norm": 2.706449270248413,
        "learning_rate": 0.00018867188364888923,
        "epoch": 0.052835531708771144,
        "step": 709
    },
    {
        "loss": 2.6141,
        "grad_norm": 2.6428558826446533,
        "learning_rate": 0.00018863933100932942,
        "epoch": 0.05291005291005291,
        "step": 710
    },
    {
        "loss": 2.6482,
        "grad_norm": 3.4415862560272217,
        "learning_rate": 0.00018860673448153445,
        "epoch": 0.05298457411133468,
        "step": 711
    },
    {
        "loss": 2.5555,
        "grad_norm": 2.522810459136963,
        "learning_rate": 0.00018857409408164392,
        "epoch": 0.05305909531261644,
        "step": 712
    },
    {
        "loss": 2.7097,
        "grad_norm": 2.758913040161133,
        "learning_rate": 0.00018854140982581914,
        "epoch": 0.0531336165138982,
        "step": 713
    },
    {
        "loss": 2.7027,
        "grad_norm": 2.2416961193084717,
        "learning_rate": 0.0001885086817302432,
        "epoch": 0.053208137715179966,
        "step": 714
    },
    {
        "loss": 2.3616,
        "grad_norm": 2.597578287124634,
        "learning_rate": 0.00018847590981112085,
        "epoch": 0.053282658916461736,
        "step": 715
    },
    {
        "loss": 3.0437,
        "grad_norm": 2.000016927719116,
        "learning_rate": 0.0001884430940846785,
        "epoch": 0.0533571801177435,
        "step": 716
    },
    {
        "loss": 1.7313,
        "grad_norm": 3.7972283363342285,
        "learning_rate": 0.0001884102345671643,
        "epoch": 0.05343170131902526,
        "step": 717
    },
    {
        "loss": 2.6909,
        "grad_norm": 2.6327924728393555,
        "learning_rate": 0.0001883773312748481,
        "epoch": 0.053506222520307026,
        "step": 718
    },
    {
        "loss": 2.3244,
        "grad_norm": 3.866539478302002,
        "learning_rate": 0.00018834438422402137,
        "epoch": 0.05358074372158879,
        "step": 719
    },
    {
        "loss": 2.7765,
        "grad_norm": 1.8372375965118408,
        "learning_rate": 0.0001883113934309973,
        "epoch": 0.05365526492287056,
        "step": 720
    },
    {
        "loss": 2.4242,
        "grad_norm": 2.3151674270629883,
        "learning_rate": 0.00018827835891211067,
        "epoch": 0.05372978612415232,
        "step": 721
    },
    {
        "loss": 2.257,
        "grad_norm": 3.166598320007324,
        "learning_rate": 0.00018824528068371802,
        "epoch": 0.053804307325434085,
        "step": 722
    },
    {
        "loss": 2.5536,
        "grad_norm": 1.9816703796386719,
        "learning_rate": 0.0001882121587621974,
        "epoch": 0.05387882852671585,
        "step": 723
    },
    {
        "loss": 2.9936,
        "grad_norm": 1.6006783246994019,
        "learning_rate": 0.00018817899316394857,
        "epoch": 0.05395334972799762,
        "step": 724
    },
    {
        "loss": 3.0358,
        "grad_norm": 1.8635454177856445,
        "learning_rate": 0.00018814578390539293,
        "epoch": 0.05402787092927938,
        "step": 725
    },
    {
        "loss": 2.1048,
        "grad_norm": 3.9677295684814453,
        "learning_rate": 0.00018811253100297343,
        "epoch": 0.054102392130561144,
        "step": 726
    },
    {
        "loss": 2.6262,
        "grad_norm": 3.5320467948913574,
        "learning_rate": 0.00018807923447315475,
        "epoch": 0.05417691333184291,
        "step": 727
    },
    {
        "loss": 2.441,
        "grad_norm": 2.375364303588867,
        "learning_rate": 0.000188045894332423,
        "epoch": 0.05425143453312467,
        "step": 728
    },
    {
        "loss": 2.5513,
        "grad_norm": 2.9200024604797363,
        "learning_rate": 0.00018801251059728604,
        "epoch": 0.05432595573440644,
        "step": 729
    },
    {
        "loss": 3.0086,
        "grad_norm": 3.0499753952026367,
        "learning_rate": 0.0001879790832842732,
        "epoch": 0.054400476935688204,
        "step": 730
    },
    {
        "loss": 1.9576,
        "grad_norm": 4.11575984954834,
        "learning_rate": 0.00018794561240993547,
        "epoch": 0.05447499813696997,
        "step": 731
    },
    {
        "loss": 2.4424,
        "grad_norm": 3.0319085121154785,
        "learning_rate": 0.00018791209799084538,
        "epoch": 0.05454951933825173,
        "step": 732
    },
    {
        "loss": 2.1914,
        "grad_norm": 3.3695664405822754,
        "learning_rate": 0.00018787854004359697,
        "epoch": 0.0546240405395335,
        "step": 733
    },
    {
        "loss": 2.6894,
        "grad_norm": 4.587344169616699,
        "learning_rate": 0.00018784493858480596,
        "epoch": 0.05469856174081526,
        "step": 734
    },
    {
        "loss": 2.625,
        "grad_norm": 3.6677072048187256,
        "learning_rate": 0.00018781129363110947,
        "epoch": 0.054773082942097026,
        "step": 735
    },
    {
        "loss": 2.636,
        "grad_norm": 2.210407257080078,
        "learning_rate": 0.0001877776051991662,
        "epoch": 0.05484760414337879,
        "step": 736
    },
    {
        "loss": 1.5673,
        "grad_norm": 3.459719181060791,
        "learning_rate": 0.00018774387330565645,
        "epoch": 0.05492212534466056,
        "step": 737
    },
    {
        "loss": 2.651,
        "grad_norm": 1.8236972093582153,
        "learning_rate": 0.000187710097967282,
        "epoch": 0.05499664654594232,
        "step": 738
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.9255027770996094,
        "learning_rate": 0.00018767627920076603,
        "epoch": 0.055071167747224085,
        "step": 739
    },
    {
        "loss": 2.7861,
        "grad_norm": 3.072920799255371,
        "learning_rate": 0.00018764241702285341,
        "epoch": 0.05514568894850585,
        "step": 740
    },
    {
        "loss": 2.8809,
        "grad_norm": 1.3814440965652466,
        "learning_rate": 0.00018760851145031038,
        "epoch": 0.05522021014978761,
        "step": 741
    },
    {
        "loss": 2.4107,
        "grad_norm": 1.9116425514221191,
        "learning_rate": 0.00018757456249992468,
        "epoch": 0.05529473135106938,
        "step": 742
    },
    {
        "loss": 2.6511,
        "grad_norm": 1.977785348892212,
        "learning_rate": 0.0001875405701885056,
        "epoch": 0.055369252552351145,
        "step": 743
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.5403122901916504,
        "learning_rate": 0.00018750653453288382,
        "epoch": 0.05544377375363291,
        "step": 744
    },
    {
        "loss": 2.5707,
        "grad_norm": 1.7649257183074951,
        "learning_rate": 0.00018747245554991148,
        "epoch": 0.05551829495491467,
        "step": 745
    },
    {
        "loss": 2.4306,
        "grad_norm": 3.5328428745269775,
        "learning_rate": 0.00018743833325646222,
        "epoch": 0.05559281615619644,
        "step": 746
    },
    {
        "loss": 2.7464,
        "grad_norm": 2.4515938758850098,
        "learning_rate": 0.0001874041676694311,
        "epoch": 0.055667337357478204,
        "step": 747
    },
    {
        "loss": 2.6611,
        "grad_norm": 3.4553439617156982,
        "learning_rate": 0.00018736995880573468,
        "epoch": 0.05574185855875997,
        "step": 748
    },
    {
        "loss": 2.6473,
        "grad_norm": 2.647573232650757,
        "learning_rate": 0.0001873357066823108,
        "epoch": 0.05581637976004173,
        "step": 749
    },
    {
        "loss": 3.2149,
        "grad_norm": 2.57475209236145,
        "learning_rate": 0.00018730141131611882,
        "epoch": 0.05589090096132349,
        "step": 750
    },
    {
        "loss": 2.3166,
        "grad_norm": 3.17410945892334,
        "learning_rate": 0.00018726707272413956,
        "epoch": 0.05596542216260526,
        "step": 751
    },
    {
        "loss": 2.6701,
        "grad_norm": 2.441697597503662,
        "learning_rate": 0.0001872326909233751,
        "epoch": 0.056039943363887026,
        "step": 752
    },
    {
        "loss": 2.7426,
        "grad_norm": 1.9693635702133179,
        "learning_rate": 0.0001871982659308491,
        "epoch": 0.05611446456516879,
        "step": 753
    },
    {
        "loss": 2.6827,
        "grad_norm": 1.770601511001587,
        "learning_rate": 0.0001871637977636064,
        "epoch": 0.05618898576645055,
        "step": 754
    },
    {
        "loss": 2.8216,
        "grad_norm": 2.295085906982422,
        "learning_rate": 0.0001871292864387134,
        "epoch": 0.05626350696773232,
        "step": 755
    },
    {
        "loss": 2.5437,
        "grad_norm": 2.1311981678009033,
        "learning_rate": 0.00018709473197325768,
        "epoch": 0.056338028169014086,
        "step": 756
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.1948020458221436,
        "learning_rate": 0.0001870601343843484,
        "epoch": 0.05641254937029585,
        "step": 757
    },
    {
        "loss": 2.1488,
        "grad_norm": 3.2644951343536377,
        "learning_rate": 0.0001870254936891159,
        "epoch": 0.05648707057157761,
        "step": 758
    },
    {
        "loss": 2.5376,
        "grad_norm": 2.0744192600250244,
        "learning_rate": 0.00018699080990471192,
        "epoch": 0.056561591772859375,
        "step": 759
    },
    {
        "loss": 2.6199,
        "grad_norm": 2.8455049991607666,
        "learning_rate": 0.00018695608304830958,
        "epoch": 0.056636112974141145,
        "step": 760
    },
    {
        "loss": 1.9501,
        "grad_norm": 3.0352039337158203,
        "learning_rate": 0.00018692131313710326,
        "epoch": 0.05671063417542291,
        "step": 761
    },
    {
        "loss": 2.7004,
        "grad_norm": 2.0456838607788086,
        "learning_rate": 0.00018688650018830865,
        "epoch": 0.05678515537670467,
        "step": 762
    },
    {
        "loss": 2.1677,
        "grad_norm": 1.8726738691329956,
        "learning_rate": 0.0001868516442191628,
        "epoch": 0.056859676577986434,
        "step": 763
    },
    {
        "loss": 2.6868,
        "grad_norm": 3.284356117248535,
        "learning_rate": 0.0001868167452469241,
        "epoch": 0.056934197779268204,
        "step": 764
    },
    {
        "loss": 2.5585,
        "grad_norm": 2.5965418815612793,
        "learning_rate": 0.0001867818032888721,
        "epoch": 0.05700871898054997,
        "step": 765
    },
    {
        "loss": 2.8435,
        "grad_norm": 1.9872688055038452,
        "learning_rate": 0.0001867468183623077,
        "epoch": 0.05708324018183173,
        "step": 766
    },
    {
        "loss": 2.1242,
        "grad_norm": 2.379251003265381,
        "learning_rate": 0.00018671179048455314,
        "epoch": 0.057157761383113494,
        "step": 767
    },
    {
        "loss": 2.7369,
        "grad_norm": 1.7583969831466675,
        "learning_rate": 0.00018667671967295183,
        "epoch": 0.057232282584395264,
        "step": 768
    },
    {
        "loss": 2.9502,
        "grad_norm": 2.825230121612549,
        "learning_rate": 0.00018664160594486853,
        "epoch": 0.05730680378567703,
        "step": 769
    },
    {
        "loss": 2.4377,
        "grad_norm": 2.884610652923584,
        "learning_rate": 0.00018660644931768912,
        "epoch": 0.05738132498695879,
        "step": 770
    },
    {
        "loss": 2.6534,
        "grad_norm": 3.180023670196533,
        "learning_rate": 0.00018657124980882083,
        "epoch": 0.05745584618824055,
        "step": 771
    },
    {
        "loss": 3.0479,
        "grad_norm": 2.415966272354126,
        "learning_rate": 0.0001865360074356921,
        "epoch": 0.057530367389522316,
        "step": 772
    },
    {
        "loss": 2.0458,
        "grad_norm": 3.5288782119750977,
        "learning_rate": 0.0001865007222157526,
        "epoch": 0.057604888590804086,
        "step": 773
    },
    {
        "loss": 2.9923,
        "grad_norm": 2.1342647075653076,
        "learning_rate": 0.00018646539416647314,
        "epoch": 0.05767940979208585,
        "step": 774
    },
    {
        "loss": 2.5657,
        "grad_norm": 1.646796703338623,
        "learning_rate": 0.00018643002330534584,
        "epoch": 0.05775393099336761,
        "step": 775
    },
    {
        "loss": 1.7278,
        "grad_norm": 3.2939741611480713,
        "learning_rate": 0.00018639460964988398,
        "epoch": 0.057828452194649375,
        "step": 776
    },
    {
        "loss": 3.5127,
        "grad_norm": 2.4838311672210693,
        "learning_rate": 0.00018635915321762198,
        "epoch": 0.057902973395931145,
        "step": 777
    },
    {
        "loss": 2.7635,
        "grad_norm": 3.2758636474609375,
        "learning_rate": 0.0001863236540261155,
        "epoch": 0.05797749459721291,
        "step": 778
    },
    {
        "loss": 2.231,
        "grad_norm": 1.7791972160339355,
        "learning_rate": 0.00018628811209294138,
        "epoch": 0.05805201579849467,
        "step": 779
    },
    {
        "loss": 2.7475,
        "grad_norm": 2.341657876968384,
        "learning_rate": 0.00018625252743569753,
        "epoch": 0.058126536999776435,
        "step": 780
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.7216665744781494,
        "learning_rate": 0.00018621690007200313,
        "epoch": 0.0582010582010582,
        "step": 781
    },
    {
        "loss": 2.8022,
        "grad_norm": 2.7431282997131348,
        "learning_rate": 0.00018618123001949846,
        "epoch": 0.05827557940233997,
        "step": 782
    },
    {
        "loss": 2.8922,
        "grad_norm": 3.3946917057037354,
        "learning_rate": 0.0001861455172958449,
        "epoch": 0.05835010060362173,
        "step": 783
    },
    {
        "loss": 2.4853,
        "grad_norm": 2.1622660160064697,
        "learning_rate": 0.00018610976191872497,
        "epoch": 0.058424621804903494,
        "step": 784
    },
    {
        "loss": 2.4011,
        "grad_norm": 2.130390167236328,
        "learning_rate": 0.00018607396390584238,
        "epoch": 0.05849914300618526,
        "step": 785
    },
    {
        "loss": 2.9864,
        "grad_norm": 2.7074954509735107,
        "learning_rate": 0.0001860381232749219,
        "epoch": 0.05857366420746703,
        "step": 786
    },
    {
        "loss": 2.324,
        "grad_norm": 2.955249071121216,
        "learning_rate": 0.00018600224004370936,
        "epoch": 0.05864818540874879,
        "step": 787
    },
    {
        "loss": 3.1402,
        "grad_norm": 2.509577989578247,
        "learning_rate": 0.00018596631422997172,
        "epoch": 0.05872270661003055,
        "step": 788
    },
    {
        "loss": 3.3438,
        "grad_norm": 2.490569591522217,
        "learning_rate": 0.00018593034585149708,
        "epoch": 0.058797227811312316,
        "step": 789
    },
    {
        "loss": 2.9372,
        "grad_norm": 2.3525166511535645,
        "learning_rate": 0.00018589433492609451,
        "epoch": 0.05887174901259408,
        "step": 790
    },
    {
        "loss": 2.4723,
        "grad_norm": 3.388371467590332,
        "learning_rate": 0.0001858582814715942,
        "epoch": 0.05894627021387585,
        "step": 791
    },
    {
        "loss": 2.7155,
        "grad_norm": 1.935847282409668,
        "learning_rate": 0.00018582218550584746,
        "epoch": 0.05902079141515761,
        "step": 792
    },
    {
        "loss": 2.3457,
        "grad_norm": 3.7613747119903564,
        "learning_rate": 0.0001857860470467265,
        "epoch": 0.059095312616439376,
        "step": 793
    },
    {
        "loss": 2.1287,
        "grad_norm": 3.469794273376465,
        "learning_rate": 0.0001857498661121247,
        "epoch": 0.05916983381772114,
        "step": 794
    },
    {
        "loss": 2.2704,
        "grad_norm": 2.6045994758605957,
        "learning_rate": 0.00018571364271995644,
        "epoch": 0.05924435501900291,
        "step": 795
    },
    {
        "loss": 2.8998,
        "grad_norm": 2.399203300476074,
        "learning_rate": 0.00018567737688815705,
        "epoch": 0.05931887622028467,
        "step": 796
    },
    {
        "loss": 2.4697,
        "grad_norm": 1.7719253301620483,
        "learning_rate": 0.00018564106863468296,
        "epoch": 0.059393397421566435,
        "step": 797
    },
    {
        "loss": 2.6252,
        "grad_norm": 3.0399465560913086,
        "learning_rate": 0.0001856047179775116,
        "epoch": 0.0594679186228482,
        "step": 798
    },
    {
        "loss": 3.0128,
        "grad_norm": 2.356065511703491,
        "learning_rate": 0.0001855683249346414,
        "epoch": 0.05954243982412997,
        "step": 799
    },
    {
        "loss": 2.9347,
        "grad_norm": 2.306387186050415,
        "learning_rate": 0.00018553188952409163,
        "epoch": 0.05961696102541173,
        "step": 800
    },
    {
        "loss": 2.7686,
        "grad_norm": 1.875821590423584,
        "learning_rate": 0.00018549541176390272,
        "epoch": 0.059691482226693494,
        "step": 801
    },
    {
        "loss": 2.9431,
        "grad_norm": 3.2511491775512695,
        "learning_rate": 0.00018545889167213603,
        "epoch": 0.05976600342797526,
        "step": 802
    },
    {
        "loss": 2.7758,
        "grad_norm": 1.4462335109710693,
        "learning_rate": 0.00018542232926687383,
        "epoch": 0.05984052462925702,
        "step": 803
    },
    {
        "loss": 1.9889,
        "grad_norm": 2.4139790534973145,
        "learning_rate": 0.00018538572456621935,
        "epoch": 0.05991504583053879,
        "step": 804
    },
    {
        "loss": 2.5984,
        "grad_norm": 1.3946319818496704,
        "learning_rate": 0.0001853490775882968,
        "epoch": 0.059989567031820554,
        "step": 805
    },
    {
        "loss": 2.7799,
        "grad_norm": 1.6912949085235596,
        "learning_rate": 0.0001853123883512513,
        "epoch": 0.06006408823310232,
        "step": 806
    },
    {
        "loss": 2.3798,
        "grad_norm": 3.170031785964966,
        "learning_rate": 0.00018527565687324887,
        "epoch": 0.06013860943438408,
        "step": 807
    },
    {
        "loss": 2.6412,
        "grad_norm": 3.7517311573028564,
        "learning_rate": 0.00018523888317247645,
        "epoch": 0.06021313063566585,
        "step": 808
    },
    {
        "loss": 2.527,
        "grad_norm": 1.9546457529067993,
        "learning_rate": 0.000185202067267142,
        "epoch": 0.06028765183694761,
        "step": 809
    },
    {
        "loss": 2.6131,
        "grad_norm": 2.564039945602417,
        "learning_rate": 0.00018516520917547416,
        "epoch": 0.060362173038229376,
        "step": 810
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.9870940446853638,
        "learning_rate": 0.00018512830891572265,
        "epoch": 0.06043669423951114,
        "step": 811
    },
    {
        "loss": 2.072,
        "grad_norm": 3.8296315670013428,
        "learning_rate": 0.00018509136650615796,
        "epoch": 0.0605112154407929,
        "step": 812
    },
    {
        "loss": 2.5072,
        "grad_norm": 3.234105110168457,
        "learning_rate": 0.00018505438196507153,
        "epoch": 0.06058573664207467,
        "step": 813
    },
    {
        "loss": 2.8848,
        "grad_norm": 2.588836431503296,
        "learning_rate": 0.0001850173553107756,
        "epoch": 0.060660257843356435,
        "step": 814
    },
    {
        "loss": 2.4656,
        "grad_norm": 2.539318084716797,
        "learning_rate": 0.00018498028656160323,
        "epoch": 0.0607347790446382,
        "step": 815
    },
    {
        "loss": 2.4427,
        "grad_norm": 2.3626341819763184,
        "learning_rate": 0.0001849431757359084,
        "epoch": 0.06080930024591996,
        "step": 816
    },
    {
        "loss": 2.2381,
        "grad_norm": 2.855501890182495,
        "learning_rate": 0.00018490602285206595,
        "epoch": 0.06088382144720173,
        "step": 817
    },
    {
        "loss": 2.8225,
        "grad_norm": 2.0536277294158936,
        "learning_rate": 0.00018486882792847142,
        "epoch": 0.060958342648483495,
        "step": 818
    },
    {
        "loss": 2.9839,
        "grad_norm": 1.91978120803833,
        "learning_rate": 0.00018483159098354126,
        "epoch": 0.06103286384976526,
        "step": 819
    },
    {
        "loss": 2.4657,
        "grad_norm": 2.509080648422241,
        "learning_rate": 0.0001847943120357127,
        "epoch": 0.06110738505104702,
        "step": 820
    },
    {
        "loss": 2.5368,
        "grad_norm": 2.04099178314209,
        "learning_rate": 0.00018475699110344379,
        "epoch": 0.06118190625232879,
        "step": 821
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.9977903366088867,
        "learning_rate": 0.00018471962820521332,
        "epoch": 0.061256427453610554,
        "step": 822
    },
    {
        "loss": 3.0015,
        "grad_norm": 2.642784833908081,
        "learning_rate": 0.00018468222335952088,
        "epoch": 0.06133094865489232,
        "step": 823
    },
    {
        "loss": 2.1946,
        "grad_norm": 1.7416746616363525,
        "learning_rate": 0.00018464477658488684,
        "epoch": 0.06140546985617408,
        "step": 824
    },
    {
        "loss": 1.8088,
        "grad_norm": 3.2594945430755615,
        "learning_rate": 0.00018460728789985233,
        "epoch": 0.06147999105745584,
        "step": 825
    },
    {
        "loss": 2.1541,
        "grad_norm": 2.8197968006134033,
        "learning_rate": 0.00018456975732297922,
        "epoch": 0.06155451225873761,
        "step": 826
    },
    {
        "loss": 2.6272,
        "grad_norm": 2.49885892868042,
        "learning_rate": 0.00018453218487285013,
        "epoch": 0.061629033460019376,
        "step": 827
    },
    {
        "loss": 2.2559,
        "grad_norm": 3.4966516494750977,
        "learning_rate": 0.0001844945705680684,
        "epoch": 0.06170355466130114,
        "step": 828
    },
    {
        "loss": 2.5859,
        "grad_norm": 1.9034106731414795,
        "learning_rate": 0.00018445691442725814,
        "epoch": 0.0617780758625829,
        "step": 829
    },
    {
        "loss": 2.2353,
        "grad_norm": 3.301002264022827,
        "learning_rate": 0.0001844192164690641,
        "epoch": 0.06185259706386467,
        "step": 830
    },
    {
        "loss": 2.3737,
        "grad_norm": 3.34852933883667,
        "learning_rate": 0.00018438147671215175,
        "epoch": 0.061927118265146436,
        "step": 831
    },
    {
        "loss": 2.6271,
        "grad_norm": 2.65263032913208,
        "learning_rate": 0.00018434369517520736,
        "epoch": 0.0620016394664282,
        "step": 832
    },
    {
        "loss": 2.2614,
        "grad_norm": 3.210165500640869,
        "learning_rate": 0.00018430587187693776,
        "epoch": 0.06207616066770996,
        "step": 833
    },
    {
        "loss": 3.0274,
        "grad_norm": 2.9891626834869385,
        "learning_rate": 0.0001842680068360705,
        "epoch": 0.062150681868991725,
        "step": 834
    },
    {
        "loss": 2.5041,
        "grad_norm": 2.0833795070648193,
        "learning_rate": 0.00018423010007135378,
        "epoch": 0.062225203070273495,
        "step": 835
    },
    {
        "loss": 2.792,
        "grad_norm": 2.7020273208618164,
        "learning_rate": 0.00018419215160155652,
        "epoch": 0.06229972427155526,
        "step": 836
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.0134708881378174,
        "learning_rate": 0.00018415416144546825,
        "epoch": 0.06237424547283702,
        "step": 837
    },
    {
        "loss": 2.4585,
        "grad_norm": 1.8776321411132812,
        "learning_rate": 0.00018411612962189914,
        "epoch": 0.062448766674118784,
        "step": 838
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.7235798835754395,
        "learning_rate": 0.00018407805614967994,
        "epoch": 0.06252328787540055,
        "step": 839
    },
    {
        "loss": 2.2513,
        "grad_norm": 2.252075672149658,
        "learning_rate": 0.00018403994104766212,
        "epoch": 0.06259780907668232,
        "step": 840
    },
    {
        "loss": 1.6936,
        "grad_norm": 2.9821932315826416,
        "learning_rate": 0.00018400178433471771,
        "epoch": 0.06267233027796408,
        "step": 841
    },
    {
        "loss": 3.1368,
        "grad_norm": 1.6814299821853638,
        "learning_rate": 0.00018396358602973934,
        "epoch": 0.06274685147924584,
        "step": 842
    },
    {
        "loss": 2.6484,
        "grad_norm": 2.442450523376465,
        "learning_rate": 0.00018392534615164027,
        "epoch": 0.0628213726805276,
        "step": 843
    },
    {
        "loss": 1.6758,
        "grad_norm": 5.026469707489014,
        "learning_rate": 0.0001838870647193543,
        "epoch": 0.06289589388180937,
        "step": 844
    },
    {
        "loss": 2.1424,
        "grad_norm": 4.026561737060547,
        "learning_rate": 0.00018384874175183575,
        "epoch": 0.06297041508309113,
        "step": 845
    },
    {
        "loss": 2.8815,
        "grad_norm": 3.184335708618164,
        "learning_rate": 0.00018381037726805966,
        "epoch": 0.06304493628437291,
        "step": 846
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.9253458976745605,
        "learning_rate": 0.00018377197128702157,
        "epoch": 0.06311945748565467,
        "step": 847
    },
    {
        "loss": 2.1906,
        "grad_norm": 3.196505069732666,
        "learning_rate": 0.00018373352382773742,
        "epoch": 0.06319397868693644,
        "step": 848
    },
    {
        "loss": 2.7879,
        "grad_norm": 2.2277116775512695,
        "learning_rate": 0.00018369503490924395,
        "epoch": 0.0632684998882182,
        "step": 849
    },
    {
        "loss": 2.5035,
        "grad_norm": 2.8004608154296875,
        "learning_rate": 0.00018365650455059817,
        "epoch": 0.06334302108949996,
        "step": 850
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.1351397037506104,
        "learning_rate": 0.00018361793277087772,
        "epoch": 0.06341754229078173,
        "step": 851
    },
    {
        "loss": 2.231,
        "grad_norm": 2.4702258110046387,
        "learning_rate": 0.00018357931958918082,
        "epoch": 0.06349206349206349,
        "step": 852
    },
    {
        "loss": 2.6216,
        "grad_norm": 1.6248457431793213,
        "learning_rate": 0.0001835406650246261,
        "epoch": 0.06356658469334525,
        "step": 853
    },
    {
        "loss": 1.6882,
        "grad_norm": 3.2722909450531006,
        "learning_rate": 0.00018350196909635268,
        "epoch": 0.06364110589462701,
        "step": 854
    },
    {
        "loss": 2.5312,
        "grad_norm": 3.066082715988159,
        "learning_rate": 0.00018346323182352023,
        "epoch": 0.06371562709590879,
        "step": 855
    },
    {
        "loss": 2.6585,
        "grad_norm": 2.7342658042907715,
        "learning_rate": 0.00018342445322530878,
        "epoch": 0.06379014829719055,
        "step": 856
    },
    {
        "loss": 2.9798,
        "grad_norm": 4.208226680755615,
        "learning_rate": 0.00018338563332091892,
        "epoch": 0.06386466949847232,
        "step": 857
    },
    {
        "loss": 2.7042,
        "grad_norm": 1.9765199422836304,
        "learning_rate": 0.00018334677212957163,
        "epoch": 0.06393919069975408,
        "step": 858
    },
    {
        "loss": 2.8563,
        "grad_norm": 1.811025619506836,
        "learning_rate": 0.00018330786967050836,
        "epoch": 0.06401371190103584,
        "step": 859
    },
    {
        "loss": 2.591,
        "grad_norm": 2.258577585220337,
        "learning_rate": 0.00018326892596299104,
        "epoch": 0.06408823310231761,
        "step": 860
    },
    {
        "loss": 3.0542,
        "grad_norm": 2.852186441421509,
        "learning_rate": 0.00018322994102630192,
        "epoch": 0.06416275430359937,
        "step": 861
    },
    {
        "loss": 3.1559,
        "grad_norm": 3.659963369369507,
        "learning_rate": 0.00018319091487974375,
        "epoch": 0.06423727550488113,
        "step": 862
    },
    {
        "loss": 2.8511,
        "grad_norm": 1.7721561193466187,
        "learning_rate": 0.00018315184754263964,
        "epoch": 0.0643117967061629,
        "step": 863
    },
    {
        "loss": 2.6934,
        "grad_norm": 2.72758412361145,
        "learning_rate": 0.0001831127390343331,
        "epoch": 0.06438631790744467,
        "step": 864
    },
    {
        "loss": 1.899,
        "grad_norm": 3.344249725341797,
        "learning_rate": 0.00018307358937418803,
        "epoch": 0.06446083910872644,
        "step": 865
    },
    {
        "loss": 2.6014,
        "grad_norm": 2.697248697280884,
        "learning_rate": 0.00018303439858158876,
        "epoch": 0.0645353603100082,
        "step": 866
    },
    {
        "loss": 2.4059,
        "grad_norm": 2.7992560863494873,
        "learning_rate": 0.00018299516667593987,
        "epoch": 0.06460988151128996,
        "step": 867
    },
    {
        "loss": 2.4535,
        "grad_norm": 2.3817410469055176,
        "learning_rate": 0.0001829558936766664,
        "epoch": 0.06468440271257173,
        "step": 868
    },
    {
        "loss": 2.7041,
        "grad_norm": 1.7387772798538208,
        "learning_rate": 0.00018291657960321369,
        "epoch": 0.06475892391385349,
        "step": 869
    },
    {
        "loss": 2.8302,
        "grad_norm": 2.1391615867614746,
        "learning_rate": 0.00018287722447504737,
        "epoch": 0.06483344511513525,
        "step": 870
    },
    {
        "loss": 2.3293,
        "grad_norm": 3.1237399578094482,
        "learning_rate": 0.00018283782831165354,
        "epoch": 0.06490796631641702,
        "step": 871
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.1148388385772705,
        "learning_rate": 0.00018279839113253851,
        "epoch": 0.06498248751769879,
        "step": 872
    },
    {
        "loss": 2.8136,
        "grad_norm": 1.9880446195602417,
        "learning_rate": 0.0001827589129572289,
        "epoch": 0.06505700871898056,
        "step": 873
    },
    {
        "loss": 2.753,
        "grad_norm": 2.0013480186462402,
        "learning_rate": 0.00018271939380527162,
        "epoch": 0.06513152992026232,
        "step": 874
    },
    {
        "loss": 2.3857,
        "grad_norm": 3.3197567462921143,
        "learning_rate": 0.00018267983369623395,
        "epoch": 0.06520605112154408,
        "step": 875
    },
    {
        "loss": 3.0574,
        "grad_norm": 2.4838311672210693,
        "learning_rate": 0.00018264023264970335,
        "epoch": 0.06528057232282584,
        "step": 876
    },
    {
        "loss": 2.3311,
        "grad_norm": 4.037443161010742,
        "learning_rate": 0.00018260059068528762,
        "epoch": 0.06535509352410761,
        "step": 877
    },
    {
        "loss": 2.2386,
        "grad_norm": 3.1238744258880615,
        "learning_rate": 0.00018256090782261478,
        "epoch": 0.06542961472538937,
        "step": 878
    },
    {
        "loss": 2.918,
        "grad_norm": 3.3608741760253906,
        "learning_rate": 0.00018252118408133315,
        "epoch": 0.06550413592667113,
        "step": 879
    },
    {
        "loss": 1.9721,
        "grad_norm": 3.252471923828125,
        "learning_rate": 0.0001824814194811112,
        "epoch": 0.0655786571279529,
        "step": 880
    },
    {
        "loss": 2.899,
        "grad_norm": 1.9893975257873535,
        "learning_rate": 0.00018244161404163775,
        "epoch": 0.06565317832923467,
        "step": 881
    },
    {
        "loss": 2.5388,
        "grad_norm": 3.768742084503174,
        "learning_rate": 0.00018240176778262175,
        "epoch": 0.06572769953051644,
        "step": 882
    },
    {
        "loss": 2.2704,
        "grad_norm": 2.9871160984039307,
        "learning_rate": 0.00018236188072379234,
        "epoch": 0.0658022207317982,
        "step": 883
    },
    {
        "loss": 2.0937,
        "grad_norm": 3.081516742706299,
        "learning_rate": 0.00018232195288489896,
        "epoch": 0.06587674193307996,
        "step": 884
    },
    {
        "loss": 2.5776,
        "grad_norm": 1.9897358417510986,
        "learning_rate": 0.00018228198428571116,
        "epoch": 0.06595126313436173,
        "step": 885
    },
    {
        "loss": 2.1569,
        "grad_norm": 2.395277738571167,
        "learning_rate": 0.00018224197494601875,
        "epoch": 0.06602578433564349,
        "step": 886
    },
    {
        "loss": 2.1867,
        "grad_norm": 3.512078285217285,
        "learning_rate": 0.00018220192488563165,
        "epoch": 0.06610030553692525,
        "step": 887
    },
    {
        "loss": 2.6666,
        "grad_norm": 2.4536635875701904,
        "learning_rate": 0.0001821618341243799,
        "epoch": 0.06617482673820702,
        "step": 888
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.974862575531006,
        "learning_rate": 0.00018212170268211382,
        "epoch": 0.06624934793948878,
        "step": 889
    },
    {
        "loss": 2.1901,
        "grad_norm": 2.7598533630371094,
        "learning_rate": 0.00018208153057870373,
        "epoch": 0.06632386914077056,
        "step": 890
    },
    {
        "loss": 2.3722,
        "grad_norm": 2.689563512802124,
        "learning_rate": 0.00018204131783404023,
        "epoch": 0.06639839034205232,
        "step": 891
    },
    {
        "loss": 2.5501,
        "grad_norm": 2.553400993347168,
        "learning_rate": 0.00018200106446803393,
        "epoch": 0.06647291154333408,
        "step": 892
    },
    {
        "loss": 2.4416,
        "grad_norm": 4.589992046356201,
        "learning_rate": 0.00018196077050061557,
        "epoch": 0.06654743274461584,
        "step": 893
    },
    {
        "loss": 2.5933,
        "grad_norm": 2.9693167209625244,
        "learning_rate": 0.00018192043595173605,
        "epoch": 0.06662195394589761,
        "step": 894
    },
    {
        "loss": 1.618,
        "grad_norm": 1.850019097328186,
        "learning_rate": 0.0001818800608413663,
        "epoch": 0.06669647514717937,
        "step": 895
    },
    {
        "loss": 2.115,
        "grad_norm": 2.2679343223571777,
        "learning_rate": 0.0001818396451894974,
        "epoch": 0.06677099634846113,
        "step": 896
    },
    {
        "loss": 2.6756,
        "grad_norm": 3.193559408187866,
        "learning_rate": 0.0001817991890161404,
        "epoch": 0.0668455175497429,
        "step": 897
    },
    {
        "loss": 2.4229,
        "grad_norm": 3.3654701709747314,
        "learning_rate": 0.00018175869234132648,
        "epoch": 0.06692003875102466,
        "step": 898
    },
    {
        "loss": 2.7352,
        "grad_norm": 3.6666760444641113,
        "learning_rate": 0.00018171815518510696,
        "epoch": 0.06699455995230644,
        "step": 899
    },
    {
        "loss": 2.3094,
        "grad_norm": 2.9970035552978516,
        "learning_rate": 0.00018167757756755304,
        "epoch": 0.0670690811535882,
        "step": 900
    },
    {
        "loss": 2.2846,
        "grad_norm": 2.9351160526275635,
        "learning_rate": 0.000181636959508756,
        "epoch": 0.06714360235486996,
        "step": 901
    },
    {
        "loss": 2.1917,
        "grad_norm": 4.803550720214844,
        "learning_rate": 0.0001815963010288272,
        "epoch": 0.06721812355615173,
        "step": 902
    },
    {
        "loss": 2.8951,
        "grad_norm": 2.8668434619903564,
        "learning_rate": 0.00018155560214789805,
        "epoch": 0.06729264475743349,
        "step": 903
    },
    {
        "loss": 2.5666,
        "grad_norm": 3.042909622192383,
        "learning_rate": 0.0001815148628861198,
        "epoch": 0.06736716595871525,
        "step": 904
    },
    {
        "loss": 2.499,
        "grad_norm": 2.4627933502197266,
        "learning_rate": 0.00018147408326366383,
        "epoch": 0.06744168715999702,
        "step": 905
    },
    {
        "loss": 2.736,
        "grad_norm": 1.9442559480667114,
        "learning_rate": 0.0001814332633007215,
        "epoch": 0.06751620836127878,
        "step": 906
    },
    {
        "loss": 1.9578,
        "grad_norm": 3.1463985443115234,
        "learning_rate": 0.00018139240301750405,
        "epoch": 0.06759072956256054,
        "step": 907
    },
    {
        "loss": 2.2393,
        "grad_norm": 2.8547451496124268,
        "learning_rate": 0.0001813515024342428,
        "epoch": 0.06766525076384232,
        "step": 908
    },
    {
        "loss": 2.8702,
        "grad_norm": 2.0032224655151367,
        "learning_rate": 0.00018131056157118892,
        "epoch": 0.06773977196512408,
        "step": 909
    },
    {
        "loss": 2.6996,
        "grad_norm": 3.288325309753418,
        "learning_rate": 0.00018126958044861361,
        "epoch": 0.06781429316640585,
        "step": 910
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.0889337062835693,
        "learning_rate": 0.00018122855908680796,
        "epoch": 0.06788881436768761,
        "step": 911
    },
    {
        "loss": 2.3066,
        "grad_norm": 2.067315101623535,
        "learning_rate": 0.00018118749750608296,
        "epoch": 0.06796333556896937,
        "step": 912
    },
    {
        "loss": 3.0563,
        "grad_norm": 2.956988573074341,
        "learning_rate": 0.00018114639572676958,
        "epoch": 0.06803785677025113,
        "step": 913
    },
    {
        "loss": 2.6004,
        "grad_norm": 3.9504966735839844,
        "learning_rate": 0.00018110525376921862,
        "epoch": 0.0681123779715329,
        "step": 914
    },
    {
        "loss": 2.9574,
        "grad_norm": 3.663421392440796,
        "learning_rate": 0.00018106407165380083,
        "epoch": 0.06818689917281466,
        "step": 915
    },
    {
        "loss": 1.8772,
        "grad_norm": 5.097745895385742,
        "learning_rate": 0.00018102284940090685,
        "epoch": 0.06826142037409642,
        "step": 916
    },
    {
        "loss": 2.4072,
        "grad_norm": 2.3090128898620605,
        "learning_rate": 0.0001809815870309471,
        "epoch": 0.0683359415753782,
        "step": 917
    },
    {
        "loss": 2.8587,
        "grad_norm": 3.5510523319244385,
        "learning_rate": 0.00018094028456435202,
        "epoch": 0.06841046277665996,
        "step": 918
    },
    {
        "loss": 2.4244,
        "grad_norm": 2.2602622509002686,
        "learning_rate": 0.00018089894202157173,
        "epoch": 0.06848498397794173,
        "step": 919
    },
    {
        "loss": 2.3966,
        "grad_norm": 3.3710031509399414,
        "learning_rate": 0.0001808575594230763,
        "epoch": 0.06855950517922349,
        "step": 920
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.4625244140625,
        "learning_rate": 0.00018081613678935565,
        "epoch": 0.06863402638050525,
        "step": 921
    },
    {
        "loss": 2.7487,
        "grad_norm": 2.569934368133545,
        "learning_rate": 0.00018077467414091944,
        "epoch": 0.06870854758178702,
        "step": 922
    },
    {
        "loss": 2.4963,
        "grad_norm": 1.9597947597503662,
        "learning_rate": 0.0001807331714982972,
        "epoch": 0.06878306878306878,
        "step": 923
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.1183407306671143,
        "learning_rate": 0.00018069162888203826,
        "epoch": 0.06885758998435054,
        "step": 924
    },
    {
        "loss": 1.8086,
        "grad_norm": 3.063314914703369,
        "learning_rate": 0.0001806500463127117,
        "epoch": 0.06893211118563232,
        "step": 925
    },
    {
        "loss": 1.7929,
        "grad_norm": 4.483255863189697,
        "learning_rate": 0.00018060842381090644,
        "epoch": 0.06900663238691408,
        "step": 926
    },
    {
        "loss": 2.7414,
        "grad_norm": 3.169476270675659,
        "learning_rate": 0.00018056676139723113,
        "epoch": 0.06908115358819585,
        "step": 927
    },
    {
        "loss": 2.3608,
        "grad_norm": 2.859732151031494,
        "learning_rate": 0.00018052505909231417,
        "epoch": 0.06915567478947761,
        "step": 928
    },
    {
        "loss": 2.5153,
        "grad_norm": 1.824558973312378,
        "learning_rate": 0.00018048331691680378,
        "epoch": 0.06923019599075937,
        "step": 929
    },
    {
        "loss": 2.7111,
        "grad_norm": 2.7795112133026123,
        "learning_rate": 0.00018044153489136787,
        "epoch": 0.06930471719204113,
        "step": 930
    },
    {
        "loss": 2.5681,
        "grad_norm": 3.3860723972320557,
        "learning_rate": 0.00018039971303669407,
        "epoch": 0.0693792383933229,
        "step": 931
    },
    {
        "loss": 2.524,
        "grad_norm": 2.8495121002197266,
        "learning_rate": 0.00018035785137348974,
        "epoch": 0.06945375959460466,
        "step": 932
    },
    {
        "loss": 3.1762,
        "grad_norm": 3.8605804443359375,
        "learning_rate": 0.00018031594992248202,
        "epoch": 0.06952828079588642,
        "step": 933
    },
    {
        "loss": 2.4285,
        "grad_norm": 3.4811646938323975,
        "learning_rate": 0.00018027400870441763,
        "epoch": 0.0696028019971682,
        "step": 934
    },
    {
        "loss": 2.6593,
        "grad_norm": 1.98222815990448,
        "learning_rate": 0.00018023202774006305,
        "epoch": 0.06967732319844996,
        "step": 935
    },
    {
        "loss": 2.8251,
        "grad_norm": 2.124671459197998,
        "learning_rate": 0.00018019000705020444,
        "epoch": 0.06975184439973173,
        "step": 936
    },
    {
        "loss": 3.2509,
        "grad_norm": 1.8879104852676392,
        "learning_rate": 0.0001801479466556476,
        "epoch": 0.06982636560101349,
        "step": 937
    },
    {
        "loss": 2.68,
        "grad_norm": 2.5459461212158203,
        "learning_rate": 0.00018010584657721806,
        "epoch": 0.06990088680229525,
        "step": 938
    },
    {
        "loss": 2.0767,
        "grad_norm": 2.7917182445526123,
        "learning_rate": 0.0001800637068357609,
        "epoch": 0.06997540800357702,
        "step": 939
    },
    {
        "loss": 1.3439,
        "grad_norm": 2.511190891265869,
        "learning_rate": 0.00018002152745214093,
        "epoch": 0.07004992920485878,
        "step": 940
    },
    {
        "loss": 2.824,
        "grad_norm": 2.860816717147827,
        "learning_rate": 0.0001799793084472425,
        "epoch": 0.07012445040614054,
        "step": 941
    },
    {
        "loss": 2.6109,
        "grad_norm": 2.0285120010375977,
        "learning_rate": 0.0001799370498419696,
        "epoch": 0.0701989716074223,
        "step": 942
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.2229137420654297,
        "learning_rate": 0.0001798947516572459,
        "epoch": 0.07027349280870408,
        "step": 943
    },
    {
        "loss": 2.2978,
        "grad_norm": 2.736968755722046,
        "learning_rate": 0.00017985241391401462,
        "epoch": 0.07034801400998585,
        "step": 944
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.9406286478042603,
        "learning_rate": 0.00017981003663323857,
        "epoch": 0.07042253521126761,
        "step": 945
    },
    {
        "loss": 2.7006,
        "grad_norm": 2.5702712535858154,
        "learning_rate": 0.00017976761983590006,
        "epoch": 0.07049705641254937,
        "step": 946
    },
    {
        "loss": 2.5765,
        "grad_norm": 4.053980350494385,
        "learning_rate": 0.00017972516354300107,
        "epoch": 0.07057157761383114,
        "step": 947
    },
    {
        "loss": 2.8348,
        "grad_norm": 1.7708544731140137,
        "learning_rate": 0.00017968266777556316,
        "epoch": 0.0706460988151129,
        "step": 948
    },
    {
        "loss": 3.0687,
        "grad_norm": 2.3896024227142334,
        "learning_rate": 0.0001796401325546273,
        "epoch": 0.07072062001639466,
        "step": 949
    },
    {
        "loss": 2.0009,
        "grad_norm": 2.661651372909546,
        "learning_rate": 0.0001795975579012541,
        "epoch": 0.07079514121767642,
        "step": 950
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.359619140625,
        "learning_rate": 0.00017955494383652365,
        "epoch": 0.07086966241895819,
        "step": 951
    },
    {
        "loss": 2.379,
        "grad_norm": 2.7511160373687744,
        "learning_rate": 0.00017951229038153557,
        "epoch": 0.07094418362023996,
        "step": 952
    },
    {
        "loss": 2.8464,
        "grad_norm": 2.488043785095215,
        "learning_rate": 0.00017946959755740904,
        "epoch": 0.07101870482152173,
        "step": 953
    },
    {
        "loss": 2.7054,
        "grad_norm": 2.427750587463379,
        "learning_rate": 0.00017942686538528258,
        "epoch": 0.07109322602280349,
        "step": 954
    },
    {
        "loss": 2.3623,
        "grad_norm": 2.0811798572540283,
        "learning_rate": 0.00017938409388631437,
        "epoch": 0.07116774722408525,
        "step": 955
    },
    {
        "loss": 1.9468,
        "grad_norm": 4.125721454620361,
        "learning_rate": 0.00017934128308168192,
        "epoch": 0.07124226842536702,
        "step": 956
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.6148509979248047,
        "learning_rate": 0.0001792984329925823,
        "epoch": 0.07131678962664878,
        "step": 957
    },
    {
        "loss": 2.1237,
        "grad_norm": 2.6018567085266113,
        "learning_rate": 0.00017925554364023197,
        "epoch": 0.07139131082793054,
        "step": 958
    },
    {
        "loss": 2.4252,
        "grad_norm": 2.8754022121429443,
        "learning_rate": 0.00017921261504586685,
        "epoch": 0.0714658320292123,
        "step": 959
    },
    {
        "loss": 1.55,
        "grad_norm": 3.8496663570404053,
        "learning_rate": 0.00017916964723074235,
        "epoch": 0.07154035323049407,
        "step": 960
    },
    {
        "loss": 2.3742,
        "grad_norm": 2.060807943344116,
        "learning_rate": 0.00017912664021613317,
        "epoch": 0.07161487443177585,
        "step": 961
    },
    {
        "loss": 2.8561,
        "grad_norm": 3.639167070388794,
        "learning_rate": 0.0001790835940233335,
        "epoch": 0.07168939563305761,
        "step": 962
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.1905505657196045,
        "learning_rate": 0.00017904050867365693,
        "epoch": 0.07176391683433937,
        "step": 963
    },
    {
        "loss": 1.6833,
        "grad_norm": 2.7998363971710205,
        "learning_rate": 0.00017899738418843646,
        "epoch": 0.07183843803562114,
        "step": 964
    },
    {
        "loss": 2.8663,
        "grad_norm": 1.564578652381897,
        "learning_rate": 0.0001789542205890244,
        "epoch": 0.0719129592369029,
        "step": 965
    },
    {
        "loss": 2.5569,
        "grad_norm": 1.6940810680389404,
        "learning_rate": 0.00017891101789679247,
        "epoch": 0.07198748043818466,
        "step": 966
    },
    {
        "loss": 2.6306,
        "grad_norm": 2.8661179542541504,
        "learning_rate": 0.00017886777613313176,
        "epoch": 0.07206200163946642,
        "step": 967
    },
    {
        "loss": 2.5524,
        "grad_norm": 3.0571742057800293,
        "learning_rate": 0.00017882449531945262,
        "epoch": 0.07213652284074819,
        "step": 968
    },
    {
        "loss": 2.2509,
        "grad_norm": 2.5446105003356934,
        "learning_rate": 0.00017878117547718487,
        "epoch": 0.07221104404202995,
        "step": 969
    },
    {
        "loss": 2.033,
        "grad_norm": 2.881587266921997,
        "learning_rate": 0.00017873781662777755,
        "epoch": 0.07228556524331173,
        "step": 970
    },
    {
        "loss": 2.9908,
        "grad_norm": 2.433131217956543,
        "learning_rate": 0.00017869441879269903,
        "epoch": 0.07236008644459349,
        "step": 971
    },
    {
        "loss": 2.3947,
        "grad_norm": 3.4479668140411377,
        "learning_rate": 0.00017865098199343704,
        "epoch": 0.07243460764587525,
        "step": 972
    },
    {
        "loss": 2.3556,
        "grad_norm": 1.79069983959198,
        "learning_rate": 0.00017860750625149852,
        "epoch": 0.07250912884715702,
        "step": 973
    },
    {
        "loss": 2.4951,
        "grad_norm": 3.3904030323028564,
        "learning_rate": 0.00017856399158840973,
        "epoch": 0.07258365004843878,
        "step": 974
    },
    {
        "loss": 2.6466,
        "grad_norm": 2.1811647415161133,
        "learning_rate": 0.00017852043802571628,
        "epoch": 0.07265817124972054,
        "step": 975
    },
    {
        "loss": 2.8159,
        "grad_norm": 5.061034202575684,
        "learning_rate": 0.00017847684558498287,
        "epoch": 0.0727326924510023,
        "step": 976
    },
    {
        "loss": 1.2304,
        "grad_norm": 3.3389389514923096,
        "learning_rate": 0.00017843321428779357,
        "epoch": 0.07280721365228407,
        "step": 977
    },
    {
        "loss": 3.1066,
        "grad_norm": 3.0789332389831543,
        "learning_rate": 0.00017838954415575172,
        "epoch": 0.07288173485356583,
        "step": 978
    },
    {
        "loss": 2.1504,
        "grad_norm": 4.077793121337891,
        "learning_rate": 0.00017834583521047977,
        "epoch": 0.07295625605484761,
        "step": 979
    },
    {
        "loss": 2.0725,
        "grad_norm": 4.463075160980225,
        "learning_rate": 0.00017830208747361952,
        "epoch": 0.07303077725612937,
        "step": 980
    },
    {
        "loss": 2.4185,
        "grad_norm": 2.775345802307129,
        "learning_rate": 0.0001782583009668318,
        "epoch": 0.07310529845741114,
        "step": 981
    },
    {
        "loss": 2.9236,
        "grad_norm": 2.822721004486084,
        "learning_rate": 0.00017821447571179683,
        "epoch": 0.0731798196586929,
        "step": 982
    },
    {
        "loss": 2.7466,
        "grad_norm": 2.5996017456054688,
        "learning_rate": 0.0001781706117302139,
        "epoch": 0.07325434085997466,
        "step": 983
    },
    {
        "loss": 2.4278,
        "grad_norm": 2.9919939041137695,
        "learning_rate": 0.00017812670904380148,
        "epoch": 0.07332886206125643,
        "step": 984
    },
    {
        "loss": 2.4544,
        "grad_norm": 4.0872979164123535,
        "learning_rate": 0.0001780827676742973,
        "epoch": 0.07340338326253819,
        "step": 985
    },
    {
        "loss": 2.383,
        "grad_norm": 2.983492374420166,
        "learning_rate": 0.00017803878764345807,
        "epoch": 0.07347790446381995,
        "step": 986
    },
    {
        "loss": 2.5464,
        "grad_norm": 1.9228509664535522,
        "learning_rate": 0.00017799476897305982,
        "epoch": 0.07355242566510173,
        "step": 987
    },
    {
        "loss": 2.827,
        "grad_norm": 3.1437244415283203,
        "learning_rate": 0.0001779507116848976,
        "epoch": 0.07362694686638349,
        "step": 988
    },
    {
        "loss": 2.4373,
        "grad_norm": 2.0100653171539307,
        "learning_rate": 0.00017790661580078564,
        "epoch": 0.07370146806766525,
        "step": 989
    },
    {
        "loss": 2.6364,
        "grad_norm": 2.6547672748565674,
        "learning_rate": 0.00017786248134255724,
        "epoch": 0.07377598926894702,
        "step": 990
    },
    {
        "loss": 1.8598,
        "grad_norm": 2.700425148010254,
        "learning_rate": 0.00017781830833206485,
        "epoch": 0.07385051047022878,
        "step": 991
    },
    {
        "loss": 2.5847,
        "grad_norm": 2.828256368637085,
        "learning_rate": 0.0001777740967911799,
        "epoch": 0.07392503167151054,
        "step": 992
    },
    {
        "loss": 1.6248,
        "grad_norm": 4.3953375816345215,
        "learning_rate": 0.00017772984674179303,
        "epoch": 0.0739995528727923,
        "step": 993
    },
    {
        "loss": 2.2288,
        "grad_norm": 2.548388719558716,
        "learning_rate": 0.00017768555820581386,
        "epoch": 0.07407407407407407,
        "step": 994
    },
    {
        "loss": 2.7582,
        "grad_norm": 3.22003436088562,
        "learning_rate": 0.00017764123120517114,
        "epoch": 0.07414859527535583,
        "step": 995
    },
    {
        "loss": 1.7083,
        "grad_norm": 4.067584991455078,
        "learning_rate": 0.00017759686576181255,
        "epoch": 0.07422311647663761,
        "step": 996
    },
    {
        "loss": 2.7373,
        "grad_norm": 2.097181558609009,
        "learning_rate": 0.0001775524618977049,
        "epoch": 0.07429763767791937,
        "step": 997
    },
    {
        "loss": 2.7829,
        "grad_norm": 2.4021215438842773,
        "learning_rate": 0.00017750801963483404,
        "epoch": 0.07437215887920114,
        "step": 998
    },
    {
        "loss": 1.6808,
        "grad_norm": 3.2853360176086426,
        "learning_rate": 0.00017746353899520477,
        "epoch": 0.0744466800804829,
        "step": 999
    },
    {
        "loss": 2.3333,
        "grad_norm": 3.5493359565734863,
        "learning_rate": 0.00017741902000084086,
        "epoch": 0.07452120128176466,
        "step": 1000
    },
    {
        "loss": 3.0805,
        "grad_norm": 2.3014795780181885,
        "learning_rate": 0.00017737446267378516,
        "epoch": 0.07459572248304643,
        "step": 1001
    },
    {
        "loss": 2.0195,
        "grad_norm": 2.718836545944214,
        "learning_rate": 0.0001773298670360995,
        "epoch": 0.07467024368432819,
        "step": 1002
    },
    {
        "loss": 2.2492,
        "grad_norm": 2.90549898147583,
        "learning_rate": 0.0001772852331098646,
        "epoch": 0.07474476488560995,
        "step": 1003
    },
    {
        "loss": 2.6005,
        "grad_norm": 2.656773567199707,
        "learning_rate": 0.00017724056091718014,
        "epoch": 0.07481928608689171,
        "step": 1004
    },
    {
        "loss": 2.6249,
        "grad_norm": 2.4872238636016846,
        "learning_rate": 0.00017719585048016484,
        "epoch": 0.07489380728817349,
        "step": 1005
    },
    {
        "loss": 2.6234,
        "grad_norm": 1.6178998947143555,
        "learning_rate": 0.0001771511018209563,
        "epoch": 0.07496832848945525,
        "step": 1006
    },
    {
        "loss": 2.7012,
        "grad_norm": 2.4607014656066895,
        "learning_rate": 0.00017710631496171102,
        "epoch": 0.07504284969073702,
        "step": 1007
    },
    {
        "loss": 2.6334,
        "grad_norm": 1.968675136566162,
        "learning_rate": 0.00017706148992460444,
        "epoch": 0.07511737089201878,
        "step": 1008
    },
    {
        "loss": 3.2153,
        "grad_norm": 3.2338063716888428,
        "learning_rate": 0.00017701662673183096,
        "epoch": 0.07519189209330054,
        "step": 1009
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.6306333541870117,
        "learning_rate": 0.00017697172540560375,
        "epoch": 0.07526641329458231,
        "step": 1010
    },
    {
        "loss": 2.7322,
        "grad_norm": 1.8639285564422607,
        "learning_rate": 0.00017692678596815492,
        "epoch": 0.07534093449586407,
        "step": 1011
    },
    {
        "loss": 2.4001,
        "grad_norm": 3.17510724067688,
        "learning_rate": 0.00017688180844173553,
        "epoch": 0.07541545569714583,
        "step": 1012
    },
    {
        "loss": 2.6056,
        "grad_norm": 1.8716059923171997,
        "learning_rate": 0.00017683679284861536,
        "epoch": 0.0754899768984276,
        "step": 1013
    },
    {
        "loss": 2.5558,
        "grad_norm": 3.467186212539673,
        "learning_rate": 0.00017679173921108311,
        "epoch": 0.07556449809970937,
        "step": 1014
    },
    {
        "loss": 2.1932,
        "grad_norm": 2.95954966545105,
        "learning_rate": 0.00017674664755144636,
        "epoch": 0.07563901930099114,
        "step": 1015
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.8548567295074463,
        "learning_rate": 0.00017670151789203145,
        "epoch": 0.0757135405022729,
        "step": 1016
    },
    {
        "loss": 2.6031,
        "grad_norm": 2.7502851486206055,
        "learning_rate": 0.0001766563502551835,
        "epoch": 0.07578806170355466,
        "step": 1017
    },
    {
        "loss": 2.5071,
        "grad_norm": 1.8837898969650269,
        "learning_rate": 0.00017661114466326655,
        "epoch": 0.07586258290483643,
        "step": 1018
    },
    {
        "loss": 2.1733,
        "grad_norm": 3.06356143951416,
        "learning_rate": 0.00017656590113866334,
        "epoch": 0.07593710410611819,
        "step": 1019
    },
    {
        "loss": 2.3121,
        "grad_norm": 2.0804266929626465,
        "learning_rate": 0.0001765206197037754,
        "epoch": 0.07601162530739995,
        "step": 1020
    },
    {
        "loss": 2.6597,
        "grad_norm": 3.0074446201324463,
        "learning_rate": 0.00017647530038102308,
        "epoch": 0.07608614650868172,
        "step": 1021
    },
    {
        "loss": 2.2872,
        "grad_norm": 2.9688560962677,
        "learning_rate": 0.00017642994319284547,
        "epoch": 0.07616066770996348,
        "step": 1022
    },
    {
        "loss": 2.4296,
        "grad_norm": 2.881373643875122,
        "learning_rate": 0.00017638454816170038,
        "epoch": 0.07623518891124526,
        "step": 1023
    },
    {
        "loss": 2.6838,
        "grad_norm": 1.8311395645141602,
        "learning_rate": 0.00017633911531006436,
        "epoch": 0.07630971011252702,
        "step": 1024
    },
    {
        "loss": 2.6932,
        "grad_norm": 2.244722843170166,
        "learning_rate": 0.00017629364466043273,
        "epoch": 0.07638423131380878,
        "step": 1025
    },
    {
        "loss": 2.6775,
        "grad_norm": 2.419670581817627,
        "learning_rate": 0.00017624813623531952,
        "epoch": 0.07645875251509054,
        "step": 1026
    },
    {
        "loss": 2.7569,
        "grad_norm": 1.2952452898025513,
        "learning_rate": 0.0001762025900572574,
        "epoch": 0.07653327371637231,
        "step": 1027
    },
    {
        "loss": 1.9697,
        "grad_norm": 2.6744842529296875,
        "learning_rate": 0.00017615700614879775,
        "epoch": 0.07660779491765407,
        "step": 1028
    },
    {
        "loss": 2.4139,
        "grad_norm": 3.0624918937683105,
        "learning_rate": 0.00017611138453251071,
        "epoch": 0.07668231611893583,
        "step": 1029
    },
    {
        "loss": 1.9795,
        "grad_norm": 3.148043632507324,
        "learning_rate": 0.000176065725230985,
        "epoch": 0.0767568373202176,
        "step": 1030
    },
    {
        "loss": 2.6329,
        "grad_norm": 1.7263827323913574,
        "learning_rate": 0.00017602002826682807,
        "epoch": 0.07683135852149936,
        "step": 1031
    },
    {
        "loss": 2.7583,
        "grad_norm": 2.584340810775757,
        "learning_rate": 0.000175974293662666,
        "epoch": 0.07690587972278114,
        "step": 1032
    },
    {
        "loss": 2.5358,
        "grad_norm": 2.8037240505218506,
        "learning_rate": 0.0001759285214411434,
        "epoch": 0.0769804009240629,
        "step": 1033
    },
    {
        "loss": 2.7839,
        "grad_norm": 2.955397129058838,
        "learning_rate": 0.00017588271162492368,
        "epoch": 0.07705492212534466,
        "step": 1034
    },
    {
        "loss": 1.8675,
        "grad_norm": 2.648005962371826,
        "learning_rate": 0.00017583686423668874,
        "epoch": 0.07712944332662643,
        "step": 1035
    },
    {
        "loss": 2.8667,
        "grad_norm": 3.718036413192749,
        "learning_rate": 0.00017579097929913915,
        "epoch": 0.07720396452790819,
        "step": 1036
    },
    {
        "loss": 2.3763,
        "grad_norm": 1.9327675104141235,
        "learning_rate": 0.000175745056834994,
        "epoch": 0.07727848572918995,
        "step": 1037
    },
    {
        "loss": 2.2414,
        "grad_norm": 3.701033592224121,
        "learning_rate": 0.00017569909686699107,
        "epoch": 0.07735300693047172,
        "step": 1038
    },
    {
        "loss": 2.8332,
        "grad_norm": 3.0699236392974854,
        "learning_rate": 0.00017565309941788658,
        "epoch": 0.07742752813175348,
        "step": 1039
    },
    {
        "loss": 2.8129,
        "grad_norm": 2.09805965423584,
        "learning_rate": 0.00017560706451045542,
        "epoch": 0.07750204933303524,
        "step": 1040
    },
    {
        "loss": 2.7691,
        "grad_norm": 1.9365153312683105,
        "learning_rate": 0.00017556099216749097,
        "epoch": 0.07757657053431702,
        "step": 1041
    },
    {
        "loss": 2.892,
        "grad_norm": 2.0593087673187256,
        "learning_rate": 0.00017551488241180512,
        "epoch": 0.07765109173559878,
        "step": 1042
    },
    {
        "loss": 1.2565,
        "grad_norm": 3.7272753715515137,
        "learning_rate": 0.0001754687352662284,
        "epoch": 0.07772561293688054,
        "step": 1043
    },
    {
        "loss": 2.8845,
        "grad_norm": 2.281503200531006,
        "learning_rate": 0.00017542255075360966,
        "epoch": 0.07780013413816231,
        "step": 1044
    },
    {
        "loss": 3.0664,
        "grad_norm": 2.051579236984253,
        "learning_rate": 0.00017537632889681648,
        "epoch": 0.07787465533944407,
        "step": 1045
    },
    {
        "loss": 2.8867,
        "grad_norm": 2.1821911334991455,
        "learning_rate": 0.00017533006971873474,
        "epoch": 0.07794917654072583,
        "step": 1046
    },
    {
        "loss": 1.6222,
        "grad_norm": 1.656121015548706,
        "learning_rate": 0.0001752837732422689,
        "epoch": 0.0780236977420076,
        "step": 1047
    },
    {
        "loss": 2.1165,
        "grad_norm": 4.171192169189453,
        "learning_rate": 0.00017523743949034181,
        "epoch": 0.07809821894328936,
        "step": 1048
    },
    {
        "loss": 2.5193,
        "grad_norm": 2.638291358947754,
        "learning_rate": 0.00017519106848589493,
        "epoch": 0.07817274014457114,
        "step": 1049
    },
    {
        "loss": 2.4415,
        "grad_norm": 2.064788579940796,
        "learning_rate": 0.00017514466025188798,
        "epoch": 0.0782472613458529,
        "step": 1050
    },
    {
        "loss": 2.7553,
        "grad_norm": 2.5742146968841553,
        "learning_rate": 0.00017509821481129922,
        "epoch": 0.07832178254713466,
        "step": 1051
    },
    {
        "loss": 1.6243,
        "grad_norm": 3.065117359161377,
        "learning_rate": 0.00017505173218712532,
        "epoch": 0.07839630374841643,
        "step": 1052
    },
    {
        "loss": 2.8,
        "grad_norm": 2.8345983028411865,
        "learning_rate": 0.00017500521240238132,
        "epoch": 0.07847082494969819,
        "step": 1053
    },
    {
        "loss": 2.6015,
        "grad_norm": 2.5507259368896484,
        "learning_rate": 0.00017495865548010074,
        "epoch": 0.07854534615097995,
        "step": 1054
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.8214526176452637,
        "learning_rate": 0.0001749120614433354,
        "epoch": 0.07861986735226172,
        "step": 1055
    },
    {
        "loss": 2.0562,
        "grad_norm": 2.9534053802490234,
        "learning_rate": 0.0001748654303151555,
        "epoch": 0.07869438855354348,
        "step": 1056
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.997333288192749,
        "learning_rate": 0.00017481876211864977,
        "epoch": 0.07876890975482524,
        "step": 1057
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.053590774536133,
        "learning_rate": 0.00017477205687692498,
        "epoch": 0.07884343095610702,
        "step": 1058
    },
    {
        "loss": 2.1653,
        "grad_norm": 2.628408670425415,
        "learning_rate": 0.00017472531461310654,
        "epoch": 0.07891795215738878,
        "step": 1059
    },
    {
        "loss": 2.0668,
        "grad_norm": 3.4110805988311768,
        "learning_rate": 0.00017467853535033806,
        "epoch": 0.07899247335867055,
        "step": 1060
    },
    {
        "loss": 2.7632,
        "grad_norm": 1.926558017730713,
        "learning_rate": 0.00017463171911178145,
        "epoch": 0.07906699455995231,
        "step": 1061
    },
    {
        "loss": 3.0086,
        "grad_norm": 2.101457357406616,
        "learning_rate": 0.00017458486592061704,
        "epoch": 0.07914151576123407,
        "step": 1062
    },
    {
        "loss": 3.179,
        "grad_norm": 3.1038477420806885,
        "learning_rate": 0.00017453797580004325,
        "epoch": 0.07921603696251583,
        "step": 1063
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.5863430500030518,
        "learning_rate": 0.000174491048773277,
        "epoch": 0.0792905581637976,
        "step": 1064
    },
    {
        "loss": 2.5126,
        "grad_norm": 2.6827616691589355,
        "learning_rate": 0.00017444408486355344,
        "epoch": 0.07936507936507936,
        "step": 1065
    },
    {
        "loss": 2.6342,
        "grad_norm": 1.9389134645462036,
        "learning_rate": 0.00017439708409412586,
        "epoch": 0.07943960056636112,
        "step": 1066
    },
    {
        "loss": 2.1237,
        "grad_norm": 3.777202844619751,
        "learning_rate": 0.0001743500464882659,
        "epoch": 0.0795141217676429,
        "step": 1067
    },
    {
        "loss": 2.4174,
        "grad_norm": 4.287668704986572,
        "learning_rate": 0.00017430297206926345,
        "epoch": 0.07958864296892466,
        "step": 1068
    },
    {
        "loss": 2.0364,
        "grad_norm": 3.021178960800171,
        "learning_rate": 0.00017425586086042654,
        "epoch": 0.07966316417020643,
        "step": 1069
    },
    {
        "loss": 1.7283,
        "grad_norm": 2.2581498622894287,
        "learning_rate": 0.0001742087128850815,
        "epoch": 0.07973768537148819,
        "step": 1070
    },
    {
        "loss": 2.8209,
        "grad_norm": 2.754503011703491,
        "learning_rate": 0.00017416152816657288,
        "epoch": 0.07981220657276995,
        "step": 1071
    },
    {
        "loss": 2.6124,
        "grad_norm": 3.649827480316162,
        "learning_rate": 0.0001741143067282633,
        "epoch": 0.07988672777405172,
        "step": 1072
    },
    {
        "loss": 2.6738,
        "grad_norm": 3.4152257442474365,
        "learning_rate": 0.0001740670485935337,
        "epoch": 0.07996124897533348,
        "step": 1073
    },
    {
        "loss": 2.8951,
        "grad_norm": 1.747841477394104,
        "learning_rate": 0.0001740197537857831,
        "epoch": 0.08003577017661524,
        "step": 1074
    },
    {
        "loss": 2.5064,
        "grad_norm": 2.822463035583496,
        "learning_rate": 0.00017397242232842873,
        "epoch": 0.080110291377897,
        "step": 1075
    },
    {
        "loss": 2.0682,
        "grad_norm": 3.663320302963257,
        "learning_rate": 0.00017392505424490592,
        "epoch": 0.08018481257917878,
        "step": 1076
    },
    {
        "loss": 2.5284,
        "grad_norm": 2.2499313354492188,
        "learning_rate": 0.00017387764955866818,
        "epoch": 0.08025933378046055,
        "step": 1077
    },
    {
        "loss": 2.925,
        "grad_norm": 2.283238172531128,
        "learning_rate": 0.0001738302082931871,
        "epoch": 0.08033385498174231,
        "step": 1078
    },
    {
        "loss": 2.1961,
        "grad_norm": 3.101884365081787,
        "learning_rate": 0.00017378273047195244,
        "epoch": 0.08040837618302407,
        "step": 1079
    },
    {
        "loss": 2.4607,
        "grad_norm": 3.321024179458618,
        "learning_rate": 0.000173735216118472,
        "epoch": 0.08048289738430583,
        "step": 1080
    },
    {
        "loss": 2.5186,
        "grad_norm": 2.634310722351074,
        "learning_rate": 0.00017368766525627168,
        "epoch": 0.0805574185855876,
        "step": 1081
    },
    {
        "loss": 2.3806,
        "grad_norm": 3.16306471824646,
        "learning_rate": 0.00017364007790889548,
        "epoch": 0.08063193978686936,
        "step": 1082
    },
    {
        "loss": 2.5736,
        "grad_norm": 2.228506565093994,
        "learning_rate": 0.00017359245409990544,
        "epoch": 0.08070646098815112,
        "step": 1083
    },
    {
        "loss": 2.6084,
        "grad_norm": 2.1001393795013428,
        "learning_rate": 0.0001735447938528817,
        "epoch": 0.08078098218943289,
        "step": 1084
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.5486912727355957,
        "learning_rate": 0.0001734970971914224,
        "epoch": 0.08085550339071466,
        "step": 1085
    },
    {
        "loss": 2.5436,
        "grad_norm": 3.1493947505950928,
        "learning_rate": 0.0001734493641391437,
        "epoch": 0.08093002459199643,
        "step": 1086
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.5377697944641113,
        "learning_rate": 0.00017340159471967984,
        "epoch": 0.08100454579327819,
        "step": 1087
    },
    {
        "loss": 2.4807,
        "grad_norm": 1.6928062438964844,
        "learning_rate": 0.00017335378895668295,
        "epoch": 0.08107906699455995,
        "step": 1088
    },
    {
        "loss": 2.8105,
        "grad_norm": 2.136040210723877,
        "learning_rate": 0.00017330594687382328,
        "epoch": 0.08115358819584172,
        "step": 1089
    },
    {
        "loss": 2.4683,
        "grad_norm": 2.925431728363037,
        "learning_rate": 0.000173258068494789,
        "epoch": 0.08122810939712348,
        "step": 1090
    },
    {
        "loss": 2.7593,
        "grad_norm": 2.6571853160858154,
        "learning_rate": 0.00017321015384328629,
        "epoch": 0.08130263059840524,
        "step": 1091
    },
    {
        "loss": 2.8749,
        "grad_norm": 2.6916637420654297,
        "learning_rate": 0.00017316220294303922,
        "epoch": 0.081377151799687,
        "step": 1092
    },
    {
        "loss": 2.6771,
        "grad_norm": 1.6979146003723145,
        "learning_rate": 0.00017311421581778986,
        "epoch": 0.08145167300096877,
        "step": 1093
    },
    {
        "loss": 2.5949,
        "grad_norm": 2.6835100650787354,
        "learning_rate": 0.00017306619249129822,
        "epoch": 0.08152619420225055,
        "step": 1094
    },
    {
        "loss": 2.6048,
        "grad_norm": 1.7031666040420532,
        "learning_rate": 0.00017301813298734224,
        "epoch": 0.08160071540353231,
        "step": 1095
    },
    {
        "loss": 1.7021,
        "grad_norm": 2.9443695545196533,
        "learning_rate": 0.0001729700373297177,
        "epoch": 0.08167523660481407,
        "step": 1096
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.5213422775268555,
        "learning_rate": 0.00017292190554223842,
        "epoch": 0.08174975780609584,
        "step": 1097
    },
    {
        "loss": 2.179,
        "grad_norm": 3.2816309928894043,
        "learning_rate": 0.00017287373764873589,
        "epoch": 0.0818242790073776,
        "step": 1098
    },
    {
        "loss": 2.5292,
        "grad_norm": 3.465534210205078,
        "learning_rate": 0.00017282553367305975,
        "epoch": 0.08189880020865936,
        "step": 1099
    },
    {
        "loss": 2.5349,
        "grad_norm": 2.9084832668304443,
        "learning_rate": 0.0001727772936390773,
        "epoch": 0.08197332140994112,
        "step": 1100
    },
    {
        "loss": 2.5526,
        "grad_norm": 2.4624760150909424,
        "learning_rate": 0.00017272901757067378,
        "epoch": 0.08204784261122289,
        "step": 1101
    },
    {
        "loss": 2.3606,
        "grad_norm": 2.043800115585327,
        "learning_rate": 0.0001726807054917522,
        "epoch": 0.08212236381250465,
        "step": 1102
    },
    {
        "loss": 3.0021,
        "grad_norm": 2.6509780883789062,
        "learning_rate": 0.00017263235742623353,
        "epoch": 0.08219688501378643,
        "step": 1103
    },
    {
        "loss": 1.0947,
        "grad_norm": 2.982170820236206,
        "learning_rate": 0.0001725839733980564,
        "epoch": 0.08227140621506819,
        "step": 1104
    },
    {
        "loss": 2.3124,
        "grad_norm": 2.959118366241455,
        "learning_rate": 0.0001725355534311774,
        "epoch": 0.08234592741634995,
        "step": 1105
    },
    {
        "loss": 2.9688,
        "grad_norm": 2.7568349838256836,
        "learning_rate": 0.00017248709754957083,
        "epoch": 0.08242044861763172,
        "step": 1106
    },
    {
        "loss": 2.3893,
        "grad_norm": 2.9606640338897705,
        "learning_rate": 0.00017243860577722874,
        "epoch": 0.08249496981891348,
        "step": 1107
    },
    {
        "loss": 2.6027,
        "grad_norm": 3.397765636444092,
        "learning_rate": 0.00017239007813816103,
        "epoch": 0.08256949102019524,
        "step": 1108
    },
    {
        "loss": 2.5159,
        "grad_norm": 2.3981611728668213,
        "learning_rate": 0.0001723415146563953,
        "epoch": 0.082644012221477,
        "step": 1109
    },
    {
        "loss": 1.8586,
        "grad_norm": 2.9125499725341797,
        "learning_rate": 0.00017229291535597697,
        "epoch": 0.08271853342275877,
        "step": 1110
    },
    {
        "loss": 1.886,
        "grad_norm": 3.383613109588623,
        "learning_rate": 0.00017224428026096903,
        "epoch": 0.08279305462404055,
        "step": 1111
    },
    {
        "loss": 2.5871,
        "grad_norm": 2.60630464553833,
        "learning_rate": 0.00017219560939545246,
        "epoch": 0.08286757582532231,
        "step": 1112
    },
    {
        "loss": 2.4673,
        "grad_norm": 3.414982318878174,
        "learning_rate": 0.0001721469027835257,
        "epoch": 0.08294209702660407,
        "step": 1113
    },
    {
        "loss": 2.8488,
        "grad_norm": 2.225019693374634,
        "learning_rate": 0.000172098160449305,
        "epoch": 0.08301661822788584,
        "step": 1114
    },
    {
        "loss": 3.2012,
        "grad_norm": 2.7846827507019043,
        "learning_rate": 0.0001720493824169243,
        "epoch": 0.0830911394291676,
        "step": 1115
    },
    {
        "loss": 2.1677,
        "grad_norm": 2.7489588260650635,
        "learning_rate": 0.00017200056871053521,
        "epoch": 0.08316566063044936,
        "step": 1116
    },
    {
        "loss": 2.545,
        "grad_norm": 2.4876976013183594,
        "learning_rate": 0.000171951719354307,
        "epoch": 0.08324018183173112,
        "step": 1117
    },
    {
        "loss": 2.5685,
        "grad_norm": 2.554004192352295,
        "learning_rate": 0.00017190283437242655,
        "epoch": 0.08331470303301289,
        "step": 1118
    },
    {
        "loss": 2.2182,
        "grad_norm": 2.4640090465545654,
        "learning_rate": 0.00017185391378909842,
        "epoch": 0.08338922423429465,
        "step": 1119
    },
    {
        "loss": 2.2542,
        "grad_norm": 5.07118558883667,
        "learning_rate": 0.00017180495762854483,
        "epoch": 0.08346374543557643,
        "step": 1120
    },
    {
        "loss": 2.7669,
        "grad_norm": 2.2345407009124756,
        "learning_rate": 0.00017175596591500556,
        "epoch": 0.08353826663685819,
        "step": 1121
    },
    {
        "loss": 2.6956,
        "grad_norm": 6.995604038238525,
        "learning_rate": 0.000171706938672738,
        "epoch": 0.08361278783813995,
        "step": 1122
    },
    {
        "loss": 2.7455,
        "grad_norm": 1.7926909923553467,
        "learning_rate": 0.00017165787592601713,
        "epoch": 0.08368730903942172,
        "step": 1123
    },
    {
        "loss": 2.9625,
        "grad_norm": 2.8130428791046143,
        "learning_rate": 0.00017160877769913557,
        "epoch": 0.08376183024070348,
        "step": 1124
    },
    {
        "loss": 2.6099,
        "grad_norm": 1.5346312522888184,
        "learning_rate": 0.00017155964401640339,
        "epoch": 0.08383635144198524,
        "step": 1125
    },
    {
        "loss": 2.6903,
        "grad_norm": 2.994683027267456,
        "learning_rate": 0.00017151047490214838,
        "epoch": 0.083910872643267,
        "step": 1126
    },
    {
        "loss": 2.9015,
        "grad_norm": 2.665123701095581,
        "learning_rate": 0.00017146127038071568,
        "epoch": 0.08398539384454877,
        "step": 1127
    },
    {
        "loss": 2.9037,
        "grad_norm": 2.482407808303833,
        "learning_rate": 0.00017141203047646816,
        "epoch": 0.08405991504583053,
        "step": 1128
    },
    {
        "loss": 2.8087,
        "grad_norm": 2.5227952003479004,
        "learning_rate": 0.00017136275521378603,
        "epoch": 0.08413443624711231,
        "step": 1129
    },
    {
        "loss": 2.2035,
        "grad_norm": 2.528407573699951,
        "learning_rate": 0.0001713134446170671,
        "epoch": 0.08420895744839407,
        "step": 1130
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.2901360988616943,
        "learning_rate": 0.00017126409871072665,
        "epoch": 0.08428347864967584,
        "step": 1131
    },
    {
        "loss": 2.4708,
        "grad_norm": 2.75248646736145,
        "learning_rate": 0.0001712147175191975,
        "epoch": 0.0843579998509576,
        "step": 1132
    },
    {
        "loss": 2.9161,
        "grad_norm": 2.785081148147583,
        "learning_rate": 0.00017116530106692988,
        "epoch": 0.08443252105223936,
        "step": 1133
    },
    {
        "loss": 2.375,
        "grad_norm": 2.602146625518799,
        "learning_rate": 0.00017111584937839146,
        "epoch": 0.08450704225352113,
        "step": 1134
    },
    {
        "loss": 2.0641,
        "grad_norm": 3.3233554363250732,
        "learning_rate": 0.00017106636247806744,
        "epoch": 0.08458156345480289,
        "step": 1135
    },
    {
        "loss": 2.9786,
        "grad_norm": 1.799813985824585,
        "learning_rate": 0.00017101684039046036,
        "epoch": 0.08465608465608465,
        "step": 1136
    },
    {
        "loss": 2.41,
        "grad_norm": 4.569229602813721,
        "learning_rate": 0.00017096728314009026,
        "epoch": 0.08473060585736641,
        "step": 1137
    },
    {
        "loss": 2.4475,
        "grad_norm": 2.5230460166931152,
        "learning_rate": 0.00017091769075149453,
        "epoch": 0.08480512705864819,
        "step": 1138
    },
    {
        "loss": 1.9347,
        "grad_norm": 3.737330913543701,
        "learning_rate": 0.000170868063249228,
        "epoch": 0.08487964825992995,
        "step": 1139
    },
    {
        "loss": 2.7704,
        "grad_norm": 2.822002410888672,
        "learning_rate": 0.0001708184006578629,
        "epoch": 0.08495416946121172,
        "step": 1140
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.8756414651870728,
        "learning_rate": 0.00017076870300198875,
        "epoch": 0.08502869066249348,
        "step": 1141
    },
    {
        "loss": 2.1501,
        "grad_norm": 2.805429458618164,
        "learning_rate": 0.0001707189703062125,
        "epoch": 0.08510321186377524,
        "step": 1142
    },
    {
        "loss": 2.2845,
        "grad_norm": 4.080375671386719,
        "learning_rate": 0.00017066920259515848,
        "epoch": 0.085177733065057,
        "step": 1143
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.9499716758728027,
        "learning_rate": 0.00017061939989346827,
        "epoch": 0.08525225426633877,
        "step": 1144
    },
    {
        "loss": 2.5321,
        "grad_norm": 2.920236110687256,
        "learning_rate": 0.00017056956222580083,
        "epoch": 0.08532677546762053,
        "step": 1145
    },
    {
        "loss": 2.8336,
        "grad_norm": 2.0959739685058594,
        "learning_rate": 0.00017051968961683241,
        "epoch": 0.0854012966689023,
        "step": 1146
    },
    {
        "loss": 2.6665,
        "grad_norm": 2.5762088298797607,
        "learning_rate": 0.00017046978209125658,
        "epoch": 0.08547581787018407,
        "step": 1147
    },
    {
        "loss": 1.8765,
        "grad_norm": 4.310521125793457,
        "learning_rate": 0.00017041983967378422,
        "epoch": 0.08555033907146584,
        "step": 1148
    },
    {
        "loss": 2.7739,
        "grad_norm": 1.359788179397583,
        "learning_rate": 0.00017036986238914338,
        "epoch": 0.0856248602727476,
        "step": 1149
    },
    {
        "loss": 2.6338,
        "grad_norm": 1.9898755550384521,
        "learning_rate": 0.00017031985026207954,
        "epoch": 0.08569938147402936,
        "step": 1150
    },
    {
        "loss": 1.9847,
        "grad_norm": 3.0400235652923584,
        "learning_rate": 0.00017026980331735528,
        "epoch": 0.08577390267531113,
        "step": 1151
    },
    {
        "loss": 2.6302,
        "grad_norm": 2.346775531768799,
        "learning_rate": 0.0001702197215797505,
        "epoch": 0.08584842387659289,
        "step": 1152
    },
    {
        "loss": 2.436,
        "grad_norm": 2.4617421627044678,
        "learning_rate": 0.00017016960507406232,
        "epoch": 0.08592294507787465,
        "step": 1153
    },
    {
        "loss": 2.464,
        "grad_norm": 2.7320775985717773,
        "learning_rate": 0.00017011945382510506,
        "epoch": 0.08599746627915641,
        "step": 1154
    },
    {
        "loss": 2.4723,
        "grad_norm": 2.256145715713501,
        "learning_rate": 0.00017006926785771023,
        "epoch": 0.08607198748043818,
        "step": 1155
    },
    {
        "loss": 2.5795,
        "grad_norm": 3.6646080017089844,
        "learning_rate": 0.0001700190471967266,
        "epoch": 0.08614650868171995,
        "step": 1156
    },
    {
        "loss": 2.4761,
        "grad_norm": 1.9549020528793335,
        "learning_rate": 0.00016996879186701995,
        "epoch": 0.08622102988300172,
        "step": 1157
    },
    {
        "loss": 2.4931,
        "grad_norm": 2.4042279720306396,
        "learning_rate": 0.00016991850189347342,
        "epoch": 0.08629555108428348,
        "step": 1158
    },
    {
        "loss": 3.2026,
        "grad_norm": 5.508156776428223,
        "learning_rate": 0.00016986817730098723,
        "epoch": 0.08637007228556524,
        "step": 1159
    },
    {
        "loss": 2.7961,
        "grad_norm": 2.8204150199890137,
        "learning_rate": 0.00016981781811447867,
        "epoch": 0.08644459348684701,
        "step": 1160
    },
    {
        "loss": 2.8081,
        "grad_norm": 3.740338087081909,
        "learning_rate": 0.00016976742435888232,
        "epoch": 0.08651911468812877,
        "step": 1161
    },
    {
        "loss": 2.0764,
        "grad_norm": 4.4436140060424805,
        "learning_rate": 0.0001697169960591497,
        "epoch": 0.08659363588941053,
        "step": 1162
    },
    {
        "loss": 2.8168,
        "grad_norm": 2.5156409740448,
        "learning_rate": 0.00016966653324024952,
        "epoch": 0.0866681570906923,
        "step": 1163
    },
    {
        "loss": 1.8909,
        "grad_norm": 3.848200798034668,
        "learning_rate": 0.00016961603592716764,
        "epoch": 0.08674267829197407,
        "step": 1164
    },
    {
        "loss": 2.1782,
        "grad_norm": 2.477240800857544,
        "learning_rate": 0.00016956550414490683,
        "epoch": 0.08681719949325584,
        "step": 1165
    },
    {
        "loss": 2.9466,
        "grad_norm": 2.0604724884033203,
        "learning_rate": 0.00016951493791848713,
        "epoch": 0.0868917206945376,
        "step": 1166
    },
    {
        "loss": 2.4905,
        "grad_norm": 3.087707996368408,
        "learning_rate": 0.00016946433727294544,
        "epoch": 0.08696624189581936,
        "step": 1167
    },
    {
        "loss": 1.9622,
        "grad_norm": 3.0494401454925537,
        "learning_rate": 0.0001694137022333359,
        "epoch": 0.08704076309710113,
        "step": 1168
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.545231580734253,
        "learning_rate": 0.00016936303282472946,
        "epoch": 0.08711528429838289,
        "step": 1169
    },
    {
        "loss": 2.7515,
        "grad_norm": 1.586410403251648,
        "learning_rate": 0.00016931232907221427,
        "epoch": 0.08718980549966465,
        "step": 1170
    },
    {
        "loss": 2.6035,
        "grad_norm": 2.702104330062866,
        "learning_rate": 0.00016926159100089544,
        "epoch": 0.08726432670094642,
        "step": 1171
    },
    {
        "loss": 2.2091,
        "grad_norm": 3.3055825233459473,
        "learning_rate": 0.00016921081863589504,
        "epoch": 0.08733884790222818,
        "step": 1172
    },
    {
        "loss": 2.7396,
        "grad_norm": 2.7901973724365234,
        "learning_rate": 0.0001691600120023521,
        "epoch": 0.08741336910350996,
        "step": 1173
    },
    {
        "loss": 3.1671,
        "grad_norm": 7.3934736251831055,
        "learning_rate": 0.00016910917112542264,
        "epoch": 0.08748789030479172,
        "step": 1174
    },
    {
        "loss": 3.1006,
        "grad_norm": 2.487717628479004,
        "learning_rate": 0.00016905829603027964,
        "epoch": 0.08756241150607348,
        "step": 1175
    },
    {
        "loss": 2.7818,
        "grad_norm": 1.138866662979126,
        "learning_rate": 0.0001690073867421131,
        "epoch": 0.08763693270735524,
        "step": 1176
    },
    {
        "loss": 2.6824,
        "grad_norm": 2.272555351257324,
        "learning_rate": 0.00016895644328612984,
        "epoch": 0.08771145390863701,
        "step": 1177
    },
    {
        "loss": 2.4235,
        "grad_norm": 2.3471312522888184,
        "learning_rate": 0.00016890546568755355,
        "epoch": 0.08778597510991877,
        "step": 1178
    },
    {
        "loss": 2.8365,
        "grad_norm": 1.1881612539291382,
        "learning_rate": 0.00016885445397162501,
        "epoch": 0.08786049631120053,
        "step": 1179
    },
    {
        "loss": 1.6969,
        "grad_norm": 2.9947733879089355,
        "learning_rate": 0.00016880340816360177,
        "epoch": 0.0879350175124823,
        "step": 1180
    },
    {
        "loss": 2.6628,
        "grad_norm": 3.0353646278381348,
        "learning_rate": 0.00016875232828875825,
        "epoch": 0.08800953871376406,
        "step": 1181
    },
    {
        "loss": 2.6816,
        "grad_norm": 2.865290403366089,
        "learning_rate": 0.0001687012143723858,
        "epoch": 0.08808405991504584,
        "step": 1182
    },
    {
        "loss": 2.4404,
        "grad_norm": 3.5281004905700684,
        "learning_rate": 0.00016865006643979256,
        "epoch": 0.0881585811163276,
        "step": 1183
    },
    {
        "loss": 2.7483,
        "grad_norm": 3.0655319690704346,
        "learning_rate": 0.00016859888451630357,
        "epoch": 0.08823310231760936,
        "step": 1184
    },
    {
        "loss": 2.3988,
        "grad_norm": 3.415221929550171,
        "learning_rate": 0.00016854766862726067,
        "epoch": 0.08830762351889113,
        "step": 1185
    },
    {
        "loss": 1.7775,
        "grad_norm": 2.090373992919922,
        "learning_rate": 0.00016849641879802251,
        "epoch": 0.08838214472017289,
        "step": 1186
    },
    {
        "loss": 2.6969,
        "grad_norm": 2.2221546173095703,
        "learning_rate": 0.00016844513505396463,
        "epoch": 0.08845666592145465,
        "step": 1187
    },
    {
        "loss": 2.7195,
        "grad_norm": 2.2321367263793945,
        "learning_rate": 0.0001683938174204792,
        "epoch": 0.08853118712273642,
        "step": 1188
    },
    {
        "loss": 2.5991,
        "grad_norm": 5.312994003295898,
        "learning_rate": 0.00016834246592297529,
        "epoch": 0.08860570832401818,
        "step": 1189
    },
    {
        "loss": 2.3914,
        "grad_norm": 3.570979356765747,
        "learning_rate": 0.00016829108058687871,
        "epoch": 0.08868022952529994,
        "step": 1190
    },
    {
        "loss": 2.4531,
        "grad_norm": 1.9294732809066772,
        "learning_rate": 0.00016823966143763206,
        "epoch": 0.08875475072658172,
        "step": 1191
    },
    {
        "loss": 2.0057,
        "grad_norm": 2.8120317459106445,
        "learning_rate": 0.0001681882085006946,
        "epoch": 0.08882927192786348,
        "step": 1192
    },
    {
        "loss": 2.4211,
        "grad_norm": 3.2870490550994873,
        "learning_rate": 0.00016813672180154233,
        "epoch": 0.08890379312914524,
        "step": 1193
    },
    {
        "loss": 1.8384,
        "grad_norm": 3.7335383892059326,
        "learning_rate": 0.0001680852013656681,
        "epoch": 0.08897831433042701,
        "step": 1194
    },
    {
        "loss": 1.8424,
        "grad_norm": 3.4281461238861084,
        "learning_rate": 0.00016803364721858127,
        "epoch": 0.08905283553170877,
        "step": 1195
    },
    {
        "loss": 2.5611,
        "grad_norm": 4.162748336791992,
        "learning_rate": 0.00016798205938580807,
        "epoch": 0.08912735673299053,
        "step": 1196
    },
    {
        "loss": 2.9333,
        "grad_norm": 2.806723117828369,
        "learning_rate": 0.00016793043789289125,
        "epoch": 0.0892018779342723,
        "step": 1197
    },
    {
        "loss": 2.1518,
        "grad_norm": 4.513915538787842,
        "learning_rate": 0.00016787878276539034,
        "epoch": 0.08927639913555406,
        "step": 1198
    },
    {
        "loss": 2.9824,
        "grad_norm": 2.1977295875549316,
        "learning_rate": 0.00016782709402888147,
        "epoch": 0.08935092033683582,
        "step": 1199
    },
    {
        "loss": 2.7345,
        "grad_norm": 2.1533701419830322,
        "learning_rate": 0.00016777537170895745,
        "epoch": 0.0894254415381176,
        "step": 1200
    },
    {
        "loss": 2.2743,
        "grad_norm": 3.0009257793426514,
        "learning_rate": 0.0001677236158312277,
        "epoch": 0.08949996273939936,
        "step": 1201
    },
    {
        "loss": 2.7676,
        "grad_norm": 2.159048557281494,
        "learning_rate": 0.00016767182642131817,
        "epoch": 0.08957448394068113,
        "step": 1202
    },
    {
        "loss": 3.102,
        "grad_norm": 2.0829620361328125,
        "learning_rate": 0.00016762000350487158,
        "epoch": 0.08964900514196289,
        "step": 1203
    },
    {
        "loss": 2.6961,
        "grad_norm": 2.7140252590179443,
        "learning_rate": 0.0001675681471075471,
        "epoch": 0.08972352634324465,
        "step": 1204
    },
    {
        "loss": 2.5219,
        "grad_norm": 2.444693088531494,
        "learning_rate": 0.00016751625725502058,
        "epoch": 0.08979804754452642,
        "step": 1205
    },
    {
        "loss": 2.725,
        "grad_norm": 2.5722501277923584,
        "learning_rate": 0.00016746433397298437,
        "epoch": 0.08987256874580818,
        "step": 1206
    },
    {
        "loss": 2.7191,
        "grad_norm": 1.8309868574142456,
        "learning_rate": 0.00016741237728714735,
        "epoch": 0.08994708994708994,
        "step": 1207
    },
    {
        "loss": 2.2375,
        "grad_norm": 3.0024428367614746,
        "learning_rate": 0.00016736038722323503,
        "epoch": 0.0900216111483717,
        "step": 1208
    },
    {
        "loss": 1.9159,
        "grad_norm": 2.7492153644561768,
        "learning_rate": 0.00016730836380698935,
        "epoch": 0.09009613234965348,
        "step": 1209
    },
    {
        "loss": 2.8705,
        "grad_norm": 2.355767011642456,
        "learning_rate": 0.0001672563070641688,
        "epoch": 0.09017065355093525,
        "step": 1210
    },
    {
        "loss": 2.4798,
        "grad_norm": 2.244542121887207,
        "learning_rate": 0.00016720421702054843,
        "epoch": 0.09024517475221701,
        "step": 1211
    },
    {
        "loss": 2.3713,
        "grad_norm": 2.710347890853882,
        "learning_rate": 0.00016715209370191968,
        "epoch": 0.09031969595349877,
        "step": 1212
    },
    {
        "loss": 1.6919,
        "grad_norm": 1.4798223972320557,
        "learning_rate": 0.00016709993713409052,
        "epoch": 0.09039421715478053,
        "step": 1213
    },
    {
        "loss": 2.497,
        "grad_norm": 2.9344356060028076,
        "learning_rate": 0.00016704774734288539,
        "epoch": 0.0904687383560623,
        "step": 1214
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.834092617034912,
        "learning_rate": 0.0001669955243541452,
        "epoch": 0.09054325955734406,
        "step": 1215
    },
    {
        "loss": 2.3391,
        "grad_norm": 2.7518410682678223,
        "learning_rate": 0.00016694326819372717,
        "epoch": 0.09061778075862582,
        "step": 1216
    },
    {
        "loss": 2.6096,
        "grad_norm": 1.9467657804489136,
        "learning_rate": 0.0001668909788875051,
        "epoch": 0.09069230195990759,
        "step": 1217
    },
    {
        "loss": 1.972,
        "grad_norm": 2.793548583984375,
        "learning_rate": 0.0001668386564613691,
        "epoch": 0.09076682316118936,
        "step": 1218
    },
    {
        "loss": 2.1312,
        "grad_norm": 4.342168807983398,
        "learning_rate": 0.00016678630094122573,
        "epoch": 0.09084134436247113,
        "step": 1219
    },
    {
        "loss": 2.3622,
        "grad_norm": 2.5463409423828125,
        "learning_rate": 0.000166733912352998,
        "epoch": 0.09091586556375289,
        "step": 1220
    },
    {
        "loss": 2.7265,
        "grad_norm": 2.2042863368988037,
        "learning_rate": 0.0001666814907226251,
        "epoch": 0.09099038676503465,
        "step": 1221
    },
    {
        "loss": 1.672,
        "grad_norm": 2.4184389114379883,
        "learning_rate": 0.00016662903607606272,
        "epoch": 0.09106490796631642,
        "step": 1222
    },
    {
        "loss": 2.6771,
        "grad_norm": 2.9037976264953613,
        "learning_rate": 0.00016657654843928293,
        "epoch": 0.09113942916759818,
        "step": 1223
    },
    {
        "loss": 2.6228,
        "grad_norm": 2.2732293605804443,
        "learning_rate": 0.00016652402783827404,
        "epoch": 0.09121395036887994,
        "step": 1224
    },
    {
        "loss": 2.4424,
        "grad_norm": 2.9511311054229736,
        "learning_rate": 0.0001664714742990407,
        "epoch": 0.0912884715701617,
        "step": 1225
    },
    {
        "loss": 2.0198,
        "grad_norm": 3.2443947792053223,
        "learning_rate": 0.00016641888784760394,
        "epoch": 0.09136299277144348,
        "step": 1226
    },
    {
        "loss": 2.5649,
        "grad_norm": 3.0010218620300293,
        "learning_rate": 0.000166366268510001,
        "epoch": 0.09143751397272525,
        "step": 1227
    },
    {
        "loss": 2.1836,
        "grad_norm": 3.033198118209839,
        "learning_rate": 0.0001663136163122854,
        "epoch": 0.09151203517400701,
        "step": 1228
    },
    {
        "loss": 2.3445,
        "grad_norm": 4.287193298339844,
        "learning_rate": 0.00016626093128052706,
        "epoch": 0.09158655637528877,
        "step": 1229
    },
    {
        "loss": 2.6965,
        "grad_norm": 2.1284003257751465,
        "learning_rate": 0.000166208213440812,
        "epoch": 0.09166107757657053,
        "step": 1230
    },
    {
        "loss": 2.5308,
        "grad_norm": 4.084939956665039,
        "learning_rate": 0.00016615546281924254,
        "epoch": 0.0917355987778523,
        "step": 1231
    },
    {
        "loss": 2.2536,
        "grad_norm": 3.396402359008789,
        "learning_rate": 0.00016610267944193726,
        "epoch": 0.09181011997913406,
        "step": 1232
    },
    {
        "loss": 2.6932,
        "grad_norm": 2.053142547607422,
        "learning_rate": 0.00016604986333503097,
        "epoch": 0.09188464118041582,
        "step": 1233
    },
    {
        "loss": 2.7168,
        "grad_norm": 3.7739171981811523,
        "learning_rate": 0.00016599701452467462,
        "epoch": 0.09195916238169759,
        "step": 1234
    },
    {
        "loss": 1.8327,
        "grad_norm": 3.0851659774780273,
        "learning_rate": 0.00016594413303703536,
        "epoch": 0.09203368358297936,
        "step": 1235
    },
    {
        "loss": 2.88,
        "grad_norm": 3.3283908367156982,
        "learning_rate": 0.00016589121889829658,
        "epoch": 0.09210820478426113,
        "step": 1236
    },
    {
        "loss": 2.5312,
        "grad_norm": 2.329218626022339,
        "learning_rate": 0.00016583827213465785,
        "epoch": 0.09218272598554289,
        "step": 1237
    },
    {
        "loss": 2.6553,
        "grad_norm": 3.7112534046173096,
        "learning_rate": 0.00016578529277233475,
        "epoch": 0.09225724718682465,
        "step": 1238
    },
    {
        "loss": 1.9603,
        "grad_norm": 3.2809393405914307,
        "learning_rate": 0.00016573228083755913,
        "epoch": 0.09233176838810642,
        "step": 1239
    },
    {
        "loss": 2.8083,
        "grad_norm": 4.982350826263428,
        "learning_rate": 0.00016567923635657899,
        "epoch": 0.09240628958938818,
        "step": 1240
    },
    {
        "loss": 2.7827,
        "grad_norm": 2.2411348819732666,
        "learning_rate": 0.0001656261593556583,
        "epoch": 0.09248081079066994,
        "step": 1241
    },
    {
        "loss": 2.346,
        "grad_norm": 3.6084415912628174,
        "learning_rate": 0.00016557304986107728,
        "epoch": 0.0925553319919517,
        "step": 1242
    },
    {
        "loss": 2.9325,
        "grad_norm": 3.117821455001831,
        "learning_rate": 0.00016551990789913222,
        "epoch": 0.09262985319323347,
        "step": 1243
    },
    {
        "loss": 3.0656,
        "grad_norm": 2.0041816234588623,
        "learning_rate": 0.00016546673349613532,
        "epoch": 0.09270437439451525,
        "step": 1244
    },
    {
        "loss": 2.6705,
        "grad_norm": 2.419564962387085,
        "learning_rate": 0.0001654135266784151,
        "epoch": 0.09277889559579701,
        "step": 1245
    },
    {
        "loss": 2.7133,
        "grad_norm": 3.658278226852417,
        "learning_rate": 0.00016536028747231587,
        "epoch": 0.09285341679707877,
        "step": 1246
    },
    {
        "loss": 2.2065,
        "grad_norm": 4.026121139526367,
        "learning_rate": 0.00016530701590419824,
        "epoch": 0.09292793799836054,
        "step": 1247
    },
    {
        "loss": 2.8731,
        "grad_norm": 3.362929344177246,
        "learning_rate": 0.0001652537120004386,
        "epoch": 0.0930024591996423,
        "step": 1248
    },
    {
        "loss": 2.1546,
        "grad_norm": 4.111806869506836,
        "learning_rate": 0.0001652003757874295,
        "epoch": 0.09307698040092406,
        "step": 1249
    },
    {
        "loss": 2.838,
        "grad_norm": 2.3360559940338135,
        "learning_rate": 0.00016514700729157949,
        "epoch": 0.09315150160220582,
        "step": 1250
    },
    {
        "loss": 3.0225,
        "grad_norm": 2.157423496246338,
        "learning_rate": 0.00016509360653931297,
        "epoch": 0.09322602280348759,
        "step": 1251
    },
    {
        "loss": 2.0056,
        "grad_norm": 2.97619891166687,
        "learning_rate": 0.00016504017355707044,
        "epoch": 0.09330054400476935,
        "step": 1252
    },
    {
        "loss": 2.6173,
        "grad_norm": 3.121598243713379,
        "learning_rate": 0.00016498670837130833,
        "epoch": 0.09337506520605113,
        "step": 1253
    },
    {
        "loss": 3.0743,
        "grad_norm": 2.235250473022461,
        "learning_rate": 0.000164933211008499,
        "epoch": 0.09344958640733289,
        "step": 1254
    },
    {
        "loss": 2.4924,
        "grad_norm": 2.766894817352295,
        "learning_rate": 0.00016487968149513075,
        "epoch": 0.09352410760861465,
        "step": 1255
    },
    {
        "loss": 2.3752,
        "grad_norm": 2.6769165992736816,
        "learning_rate": 0.00016482611985770777,
        "epoch": 0.09359862880989642,
        "step": 1256
    },
    {
        "loss": 3.1393,
        "grad_norm": 3.4030168056488037,
        "learning_rate": 0.00016477252612275023,
        "epoch": 0.09367315001117818,
        "step": 1257
    },
    {
        "loss": 2.824,
        "grad_norm": 2.188257932662964,
        "learning_rate": 0.0001647189003167941,
        "epoch": 0.09374767121245994,
        "step": 1258
    },
    {
        "loss": 2.6494,
        "grad_norm": 4.3093695640563965,
        "learning_rate": 0.00016466524246639127,
        "epoch": 0.0938221924137417,
        "step": 1259
    },
    {
        "loss": 2.3828,
        "grad_norm": 3.0045173168182373,
        "learning_rate": 0.00016461155259810955,
        "epoch": 0.09389671361502347,
        "step": 1260
    },
    {
        "loss": 2.7598,
        "grad_norm": 2.536494016647339,
        "learning_rate": 0.00016455783073853248,
        "epoch": 0.09397123481630523,
        "step": 1261
    },
    {
        "loss": 2.6603,
        "grad_norm": 3.2331414222717285,
        "learning_rate": 0.00016450407691425957,
        "epoch": 0.09404575601758701,
        "step": 1262
    },
    {
        "loss": 1.825,
        "grad_norm": 3.8043582439422607,
        "learning_rate": 0.00016445029115190604,
        "epoch": 0.09412027721886877,
        "step": 1263
    },
    {
        "loss": 2.8915,
        "grad_norm": 2.3457748889923096,
        "learning_rate": 0.00016439647347810303,
        "epoch": 0.09419479842015054,
        "step": 1264
    },
    {
        "loss": 2.3281,
        "grad_norm": 2.043731689453125,
        "learning_rate": 0.00016434262391949744,
        "epoch": 0.0942693196214323,
        "step": 1265
    },
    {
        "loss": 2.8625,
        "grad_norm": 2.1695871353149414,
        "learning_rate": 0.00016428874250275192,
        "epoch": 0.09434384082271406,
        "step": 1266
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.4605586528778076,
        "learning_rate": 0.00016423482925454494,
        "epoch": 0.09441836202399582,
        "step": 1267
    },
    {
        "loss": 2.4189,
        "grad_norm": 2.5436055660247803,
        "learning_rate": 0.00016418088420157073,
        "epoch": 0.09449288322527759,
        "step": 1268
    },
    {
        "loss": 2.1877,
        "grad_norm": 4.338475227355957,
        "learning_rate": 0.00016412690737053914,
        "epoch": 0.09456740442655935,
        "step": 1269
    },
    {
        "loss": 2.6132,
        "grad_norm": 5.329386234283447,
        "learning_rate": 0.00016407289878817604,
        "epoch": 0.09464192562784111,
        "step": 1270
    },
    {
        "loss": 2.5848,
        "grad_norm": 2.2744064331054688,
        "learning_rate": 0.00016401885848122273,
        "epoch": 0.09471644682912289,
        "step": 1271
    },
    {
        "loss": 2.2834,
        "grad_norm": 4.158293724060059,
        "learning_rate": 0.00016396478647643642,
        "epoch": 0.09479096803040465,
        "step": 1272
    },
    {
        "loss": 2.139,
        "grad_norm": 2.810615301132202,
        "learning_rate": 0.0001639106828005898,
        "epoch": 0.09486548923168642,
        "step": 1273
    },
    {
        "loss": 2.2059,
        "grad_norm": 3.501544237136841,
        "learning_rate": 0.0001638565474804715,
        "epoch": 0.09494001043296818,
        "step": 1274
    },
    {
        "loss": 2.9333,
        "grad_norm": 4.27040958404541,
        "learning_rate": 0.00016380238054288564,
        "epoch": 0.09501453163424994,
        "step": 1275
    },
    {
        "loss": 2.2554,
        "grad_norm": 2.1904897689819336,
        "learning_rate": 0.00016374818201465204,
        "epoch": 0.0950890528355317,
        "step": 1276
    },
    {
        "loss": 2.9117,
        "grad_norm": 2.9912736415863037,
        "learning_rate": 0.00016369395192260616,
        "epoch": 0.09516357403681347,
        "step": 1277
    },
    {
        "loss": 1.6144,
        "grad_norm": 2.8656325340270996,
        "learning_rate": 0.00016363969029359915,
        "epoch": 0.09523809523809523,
        "step": 1278
    },
    {
        "loss": 1.8977,
        "grad_norm": 3.460463523864746,
        "learning_rate": 0.00016358539715449758,
        "epoch": 0.095312616439377,
        "step": 1279
    },
    {
        "loss": 2.3486,
        "grad_norm": 3.2692909240722656,
        "learning_rate": 0.00016353107253218392,
        "epoch": 0.09538713764065877,
        "step": 1280
    },
    {
        "loss": 2.3754,
        "grad_norm": 2.7165780067443848,
        "learning_rate": 0.000163476716453556,
        "epoch": 0.09546165884194054,
        "step": 1281
    },
    {
        "loss": 2.4961,
        "grad_norm": 2.7279961109161377,
        "learning_rate": 0.00016342232894552732,
        "epoch": 0.0955361800432223,
        "step": 1282
    },
    {
        "loss": 2.5328,
        "grad_norm": 3.448808193206787,
        "learning_rate": 0.00016336791003502684,
        "epoch": 0.09561070124450406,
        "step": 1283
    },
    {
        "loss": 2.1273,
        "grad_norm": 3.560720205307007,
        "learning_rate": 0.00016331345974899923,
        "epoch": 0.09568522244578583,
        "step": 1284
    },
    {
        "loss": 2.4589,
        "grad_norm": 2.047870397567749,
        "learning_rate": 0.00016325897811440458,
        "epoch": 0.09575974364706759,
        "step": 1285
    },
    {
        "loss": 1.9843,
        "grad_norm": 3.6722233295440674,
        "learning_rate": 0.00016320446515821852,
        "epoch": 0.09583426484834935,
        "step": 1286
    },
    {
        "loss": 2.6021,
        "grad_norm": 2.286874771118164,
        "learning_rate": 0.00016314992090743225,
        "epoch": 0.09590878604963111,
        "step": 1287
    },
    {
        "loss": 2.27,
        "grad_norm": 2.8316338062286377,
        "learning_rate": 0.00016309534538905233,
        "epoch": 0.09598330725091289,
        "step": 1288
    },
    {
        "loss": 2.8281,
        "grad_norm": 2.6780154705047607,
        "learning_rate": 0.00016304073863010094,
        "epoch": 0.09605782845219465,
        "step": 1289
    },
    {
        "loss": 1.9692,
        "grad_norm": 2.5470597743988037,
        "learning_rate": 0.00016298610065761568,
        "epoch": 0.09613234965347642,
        "step": 1290
    },
    {
        "loss": 2.4755,
        "grad_norm": 3.263805627822876,
        "learning_rate": 0.00016293143149864958,
        "epoch": 0.09620687085475818,
        "step": 1291
    },
    {
        "loss": 2.4083,
        "grad_norm": 2.3280017375946045,
        "learning_rate": 0.00016287673118027112,
        "epoch": 0.09628139205603994,
        "step": 1292
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.908313512802124,
        "learning_rate": 0.00016282199972956425,
        "epoch": 0.09635591325732171,
        "step": 1293
    },
    {
        "loss": 2.4547,
        "grad_norm": 2.257300615310669,
        "learning_rate": 0.00016276723717362828,
        "epoch": 0.09643043445860347,
        "step": 1294
    },
    {
        "loss": 2.8275,
        "grad_norm": 3.1932578086853027,
        "learning_rate": 0.00016271244353957792,
        "epoch": 0.09650495565988523,
        "step": 1295
    },
    {
        "loss": 3.1023,
        "grad_norm": 2.415677785873413,
        "learning_rate": 0.00016265761885454337,
        "epoch": 0.096579476861167,
        "step": 1296
    },
    {
        "loss": 1.915,
        "grad_norm": 2.512894868850708,
        "learning_rate": 0.00016260276314567006,
        "epoch": 0.09665399806244877,
        "step": 1297
    },
    {
        "loss": 2.6659,
        "grad_norm": 2.1524417400360107,
        "learning_rate": 0.00016254787644011889,
        "epoch": 0.09672851926373054,
        "step": 1298
    },
    {
        "loss": 2.3964,
        "grad_norm": 2.635854482650757,
        "learning_rate": 0.000162492958765066,
        "epoch": 0.0968030404650123,
        "step": 1299
    },
    {
        "loss": 2.4105,
        "grad_norm": 2.9801089763641357,
        "learning_rate": 0.00016243801014770304,
        "epoch": 0.09687756166629406,
        "step": 1300
    },
    {
        "loss": 1.7735,
        "grad_norm": 4.447754859924316,
        "learning_rate": 0.00016238303061523674,
        "epoch": 0.09695208286757583,
        "step": 1301
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.9833574295043945,
        "learning_rate": 0.00016232802019488935,
        "epoch": 0.09702660406885759,
        "step": 1302
    },
    {
        "loss": 1.9834,
        "grad_norm": 2.334597587585449,
        "learning_rate": 0.00016227297891389833,
        "epoch": 0.09710112527013935,
        "step": 1303
    },
    {
        "loss": 2.7212,
        "grad_norm": 2.8197550773620605,
        "learning_rate": 0.00016221790679951637,
        "epoch": 0.09717564647142111,
        "step": 1304
    },
    {
        "loss": 2.1527,
        "grad_norm": 2.5132341384887695,
        "learning_rate": 0.00016216280387901152,
        "epoch": 0.09725016767270288,
        "step": 1305
    },
    {
        "loss": 2.5764,
        "grad_norm": 2.9993648529052734,
        "learning_rate": 0.000162107670179667,
        "epoch": 0.09732468887398465,
        "step": 1306
    },
    {
        "loss": 2.991,
        "grad_norm": 2.275736093521118,
        "learning_rate": 0.00016205250572878138,
        "epoch": 0.09739921007526642,
        "step": 1307
    },
    {
        "loss": 2.5799,
        "grad_norm": 3.7361090183258057,
        "learning_rate": 0.0001619973105536683,
        "epoch": 0.09747373127654818,
        "step": 1308
    },
    {
        "loss": 2.4922,
        "grad_norm": 3.658912181854248,
        "learning_rate": 0.0001619420846816568,
        "epoch": 0.09754825247782994,
        "step": 1309
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.5031120777130127,
        "learning_rate": 0.0001618868281400909,
        "epoch": 0.09762277367911171,
        "step": 1310
    },
    {
        "loss": 2.276,
        "grad_norm": 4.122741222381592,
        "learning_rate": 0.00016183154095633003,
        "epoch": 0.09769729488039347,
        "step": 1311
    },
    {
        "loss": 2.2457,
        "grad_norm": 4.269916534423828,
        "learning_rate": 0.00016177622315774858,
        "epoch": 0.09777181608167523,
        "step": 1312
    },
    {
        "loss": 3.0108,
        "grad_norm": 4.709852695465088,
        "learning_rate": 0.0001617208747717363,
        "epoch": 0.097846337282957,
        "step": 1313
    },
    {
        "loss": 2.4563,
        "grad_norm": 2.803060293197632,
        "learning_rate": 0.00016166549582569795,
        "epoch": 0.09792085848423876,
        "step": 1314
    },
    {
        "loss": 2.7861,
        "grad_norm": 3.0109541416168213,
        "learning_rate": 0.00016161008634705347,
        "epoch": 0.09799537968552054,
        "step": 1315
    },
    {
        "loss": 2.8847,
        "grad_norm": 2.790832757949829,
        "learning_rate": 0.00016155464636323784,
        "epoch": 0.0980699008868023,
        "step": 1316
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.5628979206085205,
        "learning_rate": 0.0001614991759017013,
        "epoch": 0.09814442208808406,
        "step": 1317
    },
    {
        "loss": 2.6856,
        "grad_norm": 2.7690236568450928,
        "learning_rate": 0.00016144367498990906,
        "epoch": 0.09821894328936583,
        "step": 1318
    },
    {
        "loss": 2.576,
        "grad_norm": 2.9737770557403564,
        "learning_rate": 0.00016138814365534144,
        "epoch": 0.09829346449064759,
        "step": 1319
    },
    {
        "loss": 1.7991,
        "grad_norm": 4.075636386871338,
        "learning_rate": 0.00016133258192549383,
        "epoch": 0.09836798569192935,
        "step": 1320
    },
    {
        "loss": 2.8294,
        "grad_norm": 1.7212800979614258,
        "learning_rate": 0.0001612769898278766,
        "epoch": 0.09844250689321112,
        "step": 1321
    },
    {
        "loss": 2.6797,
        "grad_norm": 2.7666516304016113,
        "learning_rate": 0.00016122136739001532,
        "epoch": 0.09851702809449288,
        "step": 1322
    },
    {
        "loss": 2.5794,
        "grad_norm": 3.0465619564056396,
        "learning_rate": 0.00016116571463945043,
        "epoch": 0.09859154929577464,
        "step": 1323
    },
    {
        "loss": 2.2565,
        "grad_norm": 3.1938271522521973,
        "learning_rate": 0.0001611100316037374,
        "epoch": 0.09866607049705642,
        "step": 1324
    },
    {
        "loss": 2.1144,
        "grad_norm": 3.8960835933685303,
        "learning_rate": 0.00016105431831044676,
        "epoch": 0.09874059169833818,
        "step": 1325
    },
    {
        "loss": 2.453,
        "grad_norm": 2.4172985553741455,
        "learning_rate": 0.00016099857478716397,
        "epoch": 0.09881511289961994,
        "step": 1326
    },
    {
        "loss": 2.8348,
        "grad_norm": 2.1108782291412354,
        "learning_rate": 0.0001609428010614895,
        "epoch": 0.09888963410090171,
        "step": 1327
    },
    {
        "loss": 1.4717,
        "grad_norm": 3.2709832191467285,
        "learning_rate": 0.00016088699716103872,
        "epoch": 0.09896415530218347,
        "step": 1328
    },
    {
        "loss": 2.4467,
        "grad_norm": 3.686600685119629,
        "learning_rate": 0.000160831163113442,
        "epoch": 0.09903867650346523,
        "step": 1329
    },
    {
        "loss": 2.9272,
        "grad_norm": 2.41192889213562,
        "learning_rate": 0.00016077529894634456,
        "epoch": 0.099113197704747,
        "step": 1330
    },
    {
        "loss": 2.9038,
        "grad_norm": 2.321439504623413,
        "learning_rate": 0.0001607194046874066,
        "epoch": 0.09918771890602876,
        "step": 1331
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.8009376525878906,
        "learning_rate": 0.0001606634803643032,
        "epoch": 0.09926224010731052,
        "step": 1332
    },
    {
        "loss": 3.4481,
        "grad_norm": 3.276931047439575,
        "learning_rate": 0.00016060752600472432,
        "epoch": 0.0993367613085923,
        "step": 1333
    },
    {
        "loss": 1.9529,
        "grad_norm": 3.3903913497924805,
        "learning_rate": 0.00016055154163637482,
        "epoch": 0.09941128250987406,
        "step": 1334
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.630819797515869,
        "learning_rate": 0.00016049552728697437,
        "epoch": 0.09948580371115583,
        "step": 1335
    },
    {
        "loss": 1.262,
        "grad_norm": 3.0763890743255615,
        "learning_rate": 0.00016043948298425747,
        "epoch": 0.09956032491243759,
        "step": 1336
    },
    {
        "loss": 2.2247,
        "grad_norm": 2.1576857566833496,
        "learning_rate": 0.0001603834087559736,
        "epoch": 0.09963484611371935,
        "step": 1337
    },
    {
        "loss": 2.35,
        "grad_norm": 5.25393009185791,
        "learning_rate": 0.0001603273046298868,
        "epoch": 0.09970936731500112,
        "step": 1338
    },
    {
        "loss": 2.4737,
        "grad_norm": 1.9122167825698853,
        "learning_rate": 0.0001602711706337762,
        "epoch": 0.09978388851628288,
        "step": 1339
    },
    {
        "loss": 2.4784,
        "grad_norm": 4.200868606567383,
        "learning_rate": 0.00016021500679543545,
        "epoch": 0.09985840971756464,
        "step": 1340
    },
    {
        "loss": 2.7456,
        "grad_norm": 3.432088851928711,
        "learning_rate": 0.00016015881314267324,
        "epoch": 0.0999329309188464,
        "step": 1341
    },
    {
        "loss": 2.6762,
        "grad_norm": 2.693114995956421,
        "learning_rate": 0.00016010258970331279,
        "epoch": 0.10000745212012818,
        "step": 1342
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.967662811279297,
        "learning_rate": 0.00016004633650519218,
        "epoch": 0.10008197332140994,
        "step": 1343
    },
    {
        "loss": 1.6901,
        "grad_norm": 3.767578601837158,
        "learning_rate": 0.00015999005357616425,
        "epoch": 0.10015649452269171,
        "step": 1344
    },
    {
        "loss": 2.9645,
        "grad_norm": 2.756650447845459,
        "learning_rate": 0.00015993374094409648,
        "epoch": 0.10023101572397347,
        "step": 1345
    },
    {
        "loss": 2.3566,
        "grad_norm": 1.7339320182800293,
        "learning_rate": 0.00015987739863687107,
        "epoch": 0.10030553692525523,
        "step": 1346
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.3951199054718018,
        "learning_rate": 0.00015982102668238505,
        "epoch": 0.100380058126537,
        "step": 1347
    },
    {
        "loss": 3.013,
        "grad_norm": 2.592817544937134,
        "learning_rate": 0.00015976462510854998,
        "epoch": 0.10045457932781876,
        "step": 1348
    },
    {
        "loss": 2.27,
        "grad_norm": 2.697890043258667,
        "learning_rate": 0.00015970819394329203,
        "epoch": 0.10052910052910052,
        "step": 1349
    },
    {
        "loss": 2.4785,
        "grad_norm": 1.9762979745864868,
        "learning_rate": 0.00015965173321455222,
        "epoch": 0.1006036217303823,
        "step": 1350
    },
    {
        "loss": 1.9872,
        "grad_norm": 3.103593349456787,
        "learning_rate": 0.0001595952429502861,
        "epoch": 0.10067814293166406,
        "step": 1351
    },
    {
        "loss": 2.5545,
        "grad_norm": 2.626250743865967,
        "learning_rate": 0.00015953872317846378,
        "epoch": 0.10075266413294583,
        "step": 1352
    },
    {
        "loss": 2.6605,
        "grad_norm": 3.445732355117798,
        "learning_rate": 0.0001594821739270701,
        "epoch": 0.10082718533422759,
        "step": 1353
    },
    {
        "loss": 2.8064,
        "grad_norm": 2.5695455074310303,
        "learning_rate": 0.00015942559522410447,
        "epoch": 0.10090170653550935,
        "step": 1354
    },
    {
        "loss": 3.0815,
        "grad_norm": 2.3746178150177,
        "learning_rate": 0.0001593689870975808,
        "epoch": 0.10097622773679112,
        "step": 1355
    },
    {
        "loss": 2.6188,
        "grad_norm": 2.8777401447296143,
        "learning_rate": 0.00015931234957552767,
        "epoch": 0.10105074893807288,
        "step": 1356
    },
    {
        "loss": 2.3829,
        "grad_norm": 2.578566312789917,
        "learning_rate": 0.00015925568268598816,
        "epoch": 0.10112527013935464,
        "step": 1357
    },
    {
        "loss": 2.2748,
        "grad_norm": 3.3524112701416016,
        "learning_rate": 0.0001591989864570199,
        "epoch": 0.1011997913406364,
        "step": 1358
    },
    {
        "loss": 2.3947,
        "grad_norm": 2.648043632507324,
        "learning_rate": 0.00015914226091669504,
        "epoch": 0.10127431254191818,
        "step": 1359
    },
    {
        "loss": 2.8759,
        "grad_norm": 2.224273920059204,
        "learning_rate": 0.00015908550609310023,
        "epoch": 0.10134883374319995,
        "step": 1360
    },
    {
        "loss": 2.2533,
        "grad_norm": 2.993364095687866,
        "learning_rate": 0.00015902872201433671,
        "epoch": 0.10142335494448171,
        "step": 1361
    },
    {
        "loss": 3.0266,
        "grad_norm": 2.3828978538513184,
        "learning_rate": 0.00015897190870852013,
        "epoch": 0.10149787614576347,
        "step": 1362
    },
    {
        "loss": 1.8057,
        "grad_norm": 2.4121010303497314,
        "learning_rate": 0.00015891506620378053,
        "epoch": 0.10157239734704523,
        "step": 1363
    },
    {
        "loss": 2.2296,
        "grad_norm": 2.508357524871826,
        "learning_rate": 0.00015885819452826261,
        "epoch": 0.101646918548327,
        "step": 1364
    },
    {
        "loss": 2.4761,
        "grad_norm": 3.9606375694274902,
        "learning_rate": 0.0001588012937101253,
        "epoch": 0.10172143974960876,
        "step": 1365
    },
    {
        "loss": 1.7742,
        "grad_norm": 3.5051779747009277,
        "learning_rate": 0.0001587443637775421,
        "epoch": 0.10179596095089052,
        "step": 1366
    },
    {
        "loss": 2.65,
        "grad_norm": 2.7571864128112793,
        "learning_rate": 0.0001586874047587009,
        "epoch": 0.10187048215217229,
        "step": 1367
    },
    {
        "loss": 2.5201,
        "grad_norm": 3.3472037315368652,
        "learning_rate": 0.00015863041668180393,
        "epoch": 0.10194500335345406,
        "step": 1368
    },
    {
        "loss": 2.6359,
        "grad_norm": 2.362445592880249,
        "learning_rate": 0.00015857339957506793,
        "epoch": 0.10201952455473583,
        "step": 1369
    },
    {
        "loss": 2.7833,
        "grad_norm": 2.905845880508423,
        "learning_rate": 0.00015851635346672387,
        "epoch": 0.10209404575601759,
        "step": 1370
    },
    {
        "loss": 1.6696,
        "grad_norm": 2.7692105770111084,
        "learning_rate": 0.00015845927838501714,
        "epoch": 0.10216856695729935,
        "step": 1371
    },
    {
        "loss": 2.344,
        "grad_norm": 2.7714619636535645,
        "learning_rate": 0.00015840217435820754,
        "epoch": 0.10224308815858112,
        "step": 1372
    },
    {
        "loss": 2.0435,
        "grad_norm": 2.724100351333618,
        "learning_rate": 0.00015834504141456907,
        "epoch": 0.10231760935986288,
        "step": 1373
    },
    {
        "loss": 2.3119,
        "grad_norm": 4.2352094650268555,
        "learning_rate": 0.0001582878795823902,
        "epoch": 0.10239213056114464,
        "step": 1374
    },
    {
        "loss": 2.8791,
        "grad_norm": 2.1701793670654297,
        "learning_rate": 0.00015823068888997356,
        "epoch": 0.1024666517624264,
        "step": 1375
    },
    {
        "loss": 2.1624,
        "grad_norm": 3.2605926990509033,
        "learning_rate": 0.00015817346936563618,
        "epoch": 0.10254117296370817,
        "step": 1376
    },
    {
        "loss": 2.0506,
        "grad_norm": 3.230959415435791,
        "learning_rate": 0.00015811622103770932,
        "epoch": 0.10261569416498995,
        "step": 1377
    },
    {
        "loss": 2.2743,
        "grad_norm": 1.8244167566299438,
        "learning_rate": 0.00015805894393453843,
        "epoch": 0.10269021536627171,
        "step": 1378
    },
    {
        "loss": 2.2691,
        "grad_norm": 1.3932511806488037,
        "learning_rate": 0.00015800163808448337,
        "epoch": 0.10276473656755347,
        "step": 1379
    },
    {
        "loss": 2.52,
        "grad_norm": 3.115523099899292,
        "learning_rate": 0.00015794430351591808,
        "epoch": 0.10283925776883523,
        "step": 1380
    },
    {
        "loss": 2.8926,
        "grad_norm": 1.8826165199279785,
        "learning_rate": 0.0001578869402572308,
        "epoch": 0.102913778970117,
        "step": 1381
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.707267999649048,
        "learning_rate": 0.000157829548336824,
        "epoch": 0.10298830017139876,
        "step": 1382
    },
    {
        "loss": 1.8028,
        "grad_norm": 2.961416721343994,
        "learning_rate": 0.00015777212778311423,
        "epoch": 0.10306282137268052,
        "step": 1383
    },
    {
        "loss": 2.2261,
        "grad_norm": 3.3475329875946045,
        "learning_rate": 0.00015771467862453234,
        "epoch": 0.10313734257396229,
        "step": 1384
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.375720977783203,
        "learning_rate": 0.00015765720088952326,
        "epoch": 0.10321186377524405,
        "step": 1385
    },
    {
        "loss": 2.6406,
        "grad_norm": 2.7829437255859375,
        "learning_rate": 0.0001575996946065461,
        "epoch": 0.10328638497652583,
        "step": 1386
    },
    {
        "loss": 2.8006,
        "grad_norm": 1.9382712841033936,
        "learning_rate": 0.00015754215980407415,
        "epoch": 0.10336090617780759,
        "step": 1387
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.685887098312378,
        "learning_rate": 0.00015748459651059466,
        "epoch": 0.10343542737908935,
        "step": 1388
    },
    {
        "loss": 2.4341,
        "grad_norm": 2.0671985149383545,
        "learning_rate": 0.00015742700475460925,
        "epoch": 0.10350994858037112,
        "step": 1389
    },
    {
        "loss": 2.8682,
        "grad_norm": 6.493702411651611,
        "learning_rate": 0.0001573693845646334,
        "epoch": 0.10358446978165288,
        "step": 1390
    },
    {
        "loss": 1.8518,
        "grad_norm": 1.9060877561569214,
        "learning_rate": 0.00015731173596919673,
        "epoch": 0.10365899098293464,
        "step": 1391
    },
    {
        "loss": 2.332,
        "grad_norm": 5.485016822814941,
        "learning_rate": 0.000157254058996843,
        "epoch": 0.1037335121842164,
        "step": 1392
    },
    {
        "loss": 2.4749,
        "grad_norm": 2.108952760696411,
        "learning_rate": 0.00015719635367612993,
        "epoch": 0.10380803338549817,
        "step": 1393
    },
    {
        "loss": 2.7444,
        "grad_norm": 1.9753047227859497,
        "learning_rate": 0.00015713862003562937,
        "epoch": 0.10388255458677993,
        "step": 1394
    },
    {
        "loss": 2.0442,
        "grad_norm": 3.545008659362793,
        "learning_rate": 0.0001570808581039271,
        "epoch": 0.10395707578806171,
        "step": 1395
    },
    {
        "loss": 2.5486,
        "grad_norm": 2.385990619659424,
        "learning_rate": 0.0001570230679096229,
        "epoch": 0.10403159698934347,
        "step": 1396
    },
    {
        "loss": 2.6128,
        "grad_norm": 2.2596957683563232,
        "learning_rate": 0.00015696524948133065,
        "epoch": 0.10410611819062524,
        "step": 1397
    },
    {
        "loss": 2.2917,
        "grad_norm": 4.117349147796631,
        "learning_rate": 0.00015690740284767814,
        "epoch": 0.104180639391907,
        "step": 1398
    },
    {
        "loss": 2.8891,
        "grad_norm": 2.373546600341797,
        "learning_rate": 0.0001568495280373071,
        "epoch": 0.10425516059318876,
        "step": 1399
    },
    {
        "loss": 2.9333,
        "grad_norm": 2.3458127975463867,
        "learning_rate": 0.00015679162507887328,
        "epoch": 0.10432968179447052,
        "step": 1400
    },
    {
        "loss": 2.2344,
        "grad_norm": 3.415517807006836,
        "learning_rate": 0.0001567336940010463,
        "epoch": 0.10440420299575229,
        "step": 1401
    },
    {
        "loss": 2.8683,
        "grad_norm": 2.029371500015259,
        "learning_rate": 0.00015667573483250973,
        "epoch": 0.10447872419703405,
        "step": 1402
    },
    {
        "loss": 2.6612,
        "grad_norm": 4.290287017822266,
        "learning_rate": 0.0001566177476019611,
        "epoch": 0.10455324539831583,
        "step": 1403
    },
    {
        "loss": 2.8063,
        "grad_norm": 2.454984426498413,
        "learning_rate": 0.00015655973233811175,
        "epoch": 0.10462776659959759,
        "step": 1404
    },
    {
        "loss": 2.2453,
        "grad_norm": 3.364112377166748,
        "learning_rate": 0.00015650168906968694,
        "epoch": 0.10470228780087935,
        "step": 1405
    },
    {
        "loss": 2.1416,
        "grad_norm": 3.9254744052886963,
        "learning_rate": 0.00015644361782542578,
        "epoch": 0.10477680900216112,
        "step": 1406
    },
    {
        "loss": 1.8237,
        "grad_norm": 3.252063751220703,
        "learning_rate": 0.00015638551863408128,
        "epoch": 0.10485133020344288,
        "step": 1407
    },
    {
        "loss": 2.3005,
        "grad_norm": 3.9523608684539795,
        "learning_rate": 0.00015632739152442022,
        "epoch": 0.10492585140472464,
        "step": 1408
    },
    {
        "loss": 2.3999,
        "grad_norm": 2.205153465270996,
        "learning_rate": 0.00015626923652522327,
        "epoch": 0.1050003726060064,
        "step": 1409
    },
    {
        "loss": 2.4399,
        "grad_norm": 3.0461719036102295,
        "learning_rate": 0.00015621105366528482,
        "epoch": 0.10507489380728817,
        "step": 1410
    },
    {
        "loss": 2.2518,
        "grad_norm": 2.342698812484741,
        "learning_rate": 0.00015615284297341314,
        "epoch": 0.10514941500856993,
        "step": 1411
    },
    {
        "loss": 2.2338,
        "grad_norm": 2.801258087158203,
        "learning_rate": 0.0001560946044784303,
        "epoch": 0.10522393620985171,
        "step": 1412
    },
    {
        "loss": 2.6932,
        "grad_norm": 2.7797813415527344,
        "learning_rate": 0.00015603633820917198,
        "epoch": 0.10529845741113347,
        "step": 1413
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.5015838146209717,
        "learning_rate": 0.0001559780441944878,
        "epoch": 0.10537297861241524,
        "step": 1414
    },
    {
        "loss": 2.8477,
        "grad_norm": 3.585301160812378,
        "learning_rate": 0.00015591972246324097,
        "epoch": 0.105447499813697,
        "step": 1415
    },
    {
        "loss": 2.7907,
        "grad_norm": 2.2161788940429688,
        "learning_rate": 0.00015586137304430858,
        "epoch": 0.10552202101497876,
        "step": 1416
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.626948595046997,
        "learning_rate": 0.00015580299596658129,
        "epoch": 0.10559654221626052,
        "step": 1417
    },
    {
        "loss": 2.5352,
        "grad_norm": 2.3541812896728516,
        "learning_rate": 0.00015574459125896345,
        "epoch": 0.10567106341754229,
        "step": 1418
    },
    {
        "loss": 3.0075,
        "grad_norm": 2.088451385498047,
        "learning_rate": 0.00015568615895037322,
        "epoch": 0.10574558461882405,
        "step": 1419
    },
    {
        "loss": 2.2647,
        "grad_norm": 2.1972129344940186,
        "learning_rate": 0.00015562769906974234,
        "epoch": 0.10582010582010581,
        "step": 1420
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.4284982681274414,
        "learning_rate": 0.00015556921164601613,
        "epoch": 0.10589462702138759,
        "step": 1421
    },
    {
        "loss": 3.0272,
        "grad_norm": 2.502596855163574,
        "learning_rate": 0.00015551069670815374,
        "epoch": 0.10596914822266935,
        "step": 1422
    },
    {
        "loss": 2.9437,
        "grad_norm": 3.399831533432007,
        "learning_rate": 0.0001554521542851278,
        "epoch": 0.10604366942395112,
        "step": 1423
    },
    {
        "loss": 2.5591,
        "grad_norm": 2.171224355697632,
        "learning_rate": 0.00015539358440592449,
        "epoch": 0.10611819062523288,
        "step": 1424
    },
    {
        "loss": 2.8457,
        "grad_norm": 2.7155661582946777,
        "learning_rate": 0.0001553349870995438,
        "epoch": 0.10619271182651464,
        "step": 1425
    },
    {
        "loss": 2.9067,
        "grad_norm": 2.371568202972412,
        "learning_rate": 0.00015527636239499914,
        "epoch": 0.1062672330277964,
        "step": 1426
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.948150873184204,
        "learning_rate": 0.00015521771032131745,
        "epoch": 0.10634175422907817,
        "step": 1427
    },
    {
        "loss": 2.016,
        "grad_norm": 2.2791173458099365,
        "learning_rate": 0.0001551590309075394,
        "epoch": 0.10641627543035993,
        "step": 1428
    },
    {
        "loss": 1.7782,
        "grad_norm": 3.4830985069274902,
        "learning_rate": 0.00015510032418271897,
        "epoch": 0.1064907966316417,
        "step": 1429
    },
    {
        "loss": 2.5492,
        "grad_norm": 2.7763302326202393,
        "learning_rate": 0.00015504159017592391,
        "epoch": 0.10656531783292347,
        "step": 1430
    },
    {
        "loss": 2.86,
        "grad_norm": 3.150252342224121,
        "learning_rate": 0.00015498282891623525,
        "epoch": 0.10663983903420524,
        "step": 1431
    },
    {
        "loss": 2.7911,
        "grad_norm": 3.043766498565674,
        "learning_rate": 0.0001549240404327477,
        "epoch": 0.106714360235487,
        "step": 1432
    },
    {
        "loss": 2.3411,
        "grad_norm": 2.2945425510406494,
        "learning_rate": 0.00015486522475456927,
        "epoch": 0.10678888143676876,
        "step": 1433
    },
    {
        "loss": 0.7858,
        "grad_norm": 3.7888293266296387,
        "learning_rate": 0.00015480638191082155,
        "epoch": 0.10686340263805053,
        "step": 1434
    },
    {
        "loss": 2.5747,
        "grad_norm": 3.031144618988037,
        "learning_rate": 0.00015474751193063962,
        "epoch": 0.10693792383933229,
        "step": 1435
    },
    {
        "loss": 2.3144,
        "grad_norm": 3.002728223800659,
        "learning_rate": 0.00015468861484317187,
        "epoch": 0.10701244504061405,
        "step": 1436
    },
    {
        "loss": 2.1523,
        "grad_norm": 2.808234691619873,
        "learning_rate": 0.00015462969067758025,
        "epoch": 0.10708696624189581,
        "step": 1437
    },
    {
        "loss": 2.724,
        "grad_norm": 3.3157968521118164,
        "learning_rate": 0.00015457073946303993,
        "epoch": 0.10716148744317758,
        "step": 1438
    },
    {
        "loss": 2.0198,
        "grad_norm": 3.2694010734558105,
        "learning_rate": 0.00015451176122873968,
        "epoch": 0.10723600864445935,
        "step": 1439
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.603200912475586,
        "learning_rate": 0.00015445275600388153,
        "epoch": 0.10731052984574112,
        "step": 1440
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.866942048072815,
        "learning_rate": 0.0001543937238176809,
        "epoch": 0.10738505104702288,
        "step": 1441
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.0954458713531494,
        "learning_rate": 0.00015433466469936654,
        "epoch": 0.10745957224830464,
        "step": 1442
    },
    {
        "loss": 2.7353,
        "grad_norm": 2.1640923023223877,
        "learning_rate": 0.0001542755786781806,
        "epoch": 0.10753409344958641,
        "step": 1443
    },
    {
        "loss": 2.8646,
        "grad_norm": 2.4137253761291504,
        "learning_rate": 0.00015421646578337844,
        "epoch": 0.10760861465086817,
        "step": 1444
    },
    {
        "loss": 2.699,
        "grad_norm": 2.083935022354126,
        "learning_rate": 0.00015415732604422884,
        "epoch": 0.10768313585214993,
        "step": 1445
    },
    {
        "loss": 2.4778,
        "grad_norm": 1.70050048828125,
        "learning_rate": 0.00015409815949001378,
        "epoch": 0.1077576570534317,
        "step": 1446
    },
    {
        "loss": 2.0955,
        "grad_norm": 4.859811305999756,
        "learning_rate": 0.00015403896615002858,
        "epoch": 0.10783217825471346,
        "step": 1447
    },
    {
        "loss": 2.4008,
        "grad_norm": 3.4588305950164795,
        "learning_rate": 0.00015397974605358184,
        "epoch": 0.10790669945599524,
        "step": 1448
    },
    {
        "loss": 2.6417,
        "grad_norm": 2.162865161895752,
        "learning_rate": 0.00015392049922999528,
        "epoch": 0.107981220657277,
        "step": 1449
    },
    {
        "loss": 2.6069,
        "grad_norm": 2.540046453475952,
        "learning_rate": 0.00015386122570860402,
        "epoch": 0.10805574185855876,
        "step": 1450
    },
    {
        "loss": 2.4592,
        "grad_norm": 3.15266752243042,
        "learning_rate": 0.0001538019255187563,
        "epoch": 0.10813026305984053,
        "step": 1451
    },
    {
        "loss": 2.4983,
        "grad_norm": 4.056089401245117,
        "learning_rate": 0.00015374259868981355,
        "epoch": 0.10820478426112229,
        "step": 1452
    },
    {
        "loss": 2.2508,
        "grad_norm": 4.223364353179932,
        "learning_rate": 0.00015368324525115047,
        "epoch": 0.10827930546240405,
        "step": 1453
    },
    {
        "loss": 2.689,
        "grad_norm": 3.7786173820495605,
        "learning_rate": 0.0001536238652321549,
        "epoch": 0.10835382666368581,
        "step": 1454
    },
    {
        "loss": 2.3603,
        "grad_norm": 2.6913623809814453,
        "learning_rate": 0.0001535644586622278,
        "epoch": 0.10842834786496758,
        "step": 1455
    },
    {
        "loss": 1.8856,
        "grad_norm": 3.7408816814422607,
        "learning_rate": 0.0001535050255707833,
        "epoch": 0.10850286906624934,
        "step": 1456
    },
    {
        "loss": 2.1726,
        "grad_norm": 2.9368391036987305,
        "learning_rate": 0.00015344556598724868,
        "epoch": 0.10857739026753112,
        "step": 1457
    },
    {
        "loss": 2.5646,
        "grad_norm": 3.1217613220214844,
        "learning_rate": 0.00015338607994106433,
        "epoch": 0.10865191146881288,
        "step": 1458
    },
    {
        "loss": 2.6264,
        "grad_norm": 2.155919075012207,
        "learning_rate": 0.00015332656746168378,
        "epoch": 0.10872643267009464,
        "step": 1459
    },
    {
        "loss": 2.7924,
        "grad_norm": 2.149787664413452,
        "learning_rate": 0.00015326702857857354,
        "epoch": 0.10880095387137641,
        "step": 1460
    },
    {
        "loss": 2.2173,
        "grad_norm": 2.801114559173584,
        "learning_rate": 0.00015320746332121328,
        "epoch": 0.10887547507265817,
        "step": 1461
    },
    {
        "loss": 2.9078,
        "grad_norm": 1.8880040645599365,
        "learning_rate": 0.00015314787171909571,
        "epoch": 0.10894999627393993,
        "step": 1462
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.9424943923950195,
        "learning_rate": 0.00015308825380172663,
        "epoch": 0.1090245174752217,
        "step": 1463
    },
    {
        "loss": 2.8722,
        "grad_norm": 2.3121325969696045,
        "learning_rate": 0.00015302860959862477,
        "epoch": 0.10909903867650346,
        "step": 1464
    },
    {
        "loss": 2.1725,
        "grad_norm": 2.750154495239258,
        "learning_rate": 0.00015296893913932196,
        "epoch": 0.10917355987778524,
        "step": 1465
    },
    {
        "loss": 2.9641,
        "grad_norm": 1.885016918182373,
        "learning_rate": 0.00015290924245336295,
        "epoch": 0.109248081079067,
        "step": 1466
    },
    {
        "loss": 1.5706,
        "grad_norm": 3.752523183822632,
        "learning_rate": 0.00015284951957030555,
        "epoch": 0.10932260228034876,
        "step": 1467
    },
    {
        "loss": 2.7442,
        "grad_norm": 2.2825734615325928,
        "learning_rate": 0.00015278977051972055,
        "epoch": 0.10939712348163053,
        "step": 1468
    },
    {
        "loss": 2.735,
        "grad_norm": 2.070889472961426,
        "learning_rate": 0.00015272999533119162,
        "epoch": 0.10947164468291229,
        "step": 1469
    },
    {
        "loss": 2.2515,
        "grad_norm": 3.0513715744018555,
        "learning_rate": 0.00015267019403431543,
        "epoch": 0.10954616588419405,
        "step": 1470
    },
    {
        "loss": 2.4941,
        "grad_norm": 2.905519485473633,
        "learning_rate": 0.00015261036665870155,
        "epoch": 0.10962068708547582,
        "step": 1471
    },
    {
        "loss": 2.6066,
        "grad_norm": 1.974334478378296,
        "learning_rate": 0.00015255051323397246,
        "epoch": 0.10969520828675758,
        "step": 1472
    },
    {
        "loss": 2.5401,
        "grad_norm": 2.5849082469940186,
        "learning_rate": 0.0001524906337897636,
        "epoch": 0.10976972948803934,
        "step": 1473
    },
    {
        "loss": 2.481,
        "grad_norm": 3.4528298377990723,
        "learning_rate": 0.00015243072835572318,
        "epoch": 0.10984425068932112,
        "step": 1474
    },
    {
        "loss": 2.1937,
        "grad_norm": 1.335618257522583,
        "learning_rate": 0.00015237079696151236,
        "epoch": 0.10991877189060288,
        "step": 1475
    },
    {
        "loss": 3.0147,
        "grad_norm": 2.352888345718384,
        "learning_rate": 0.00015231083963680522,
        "epoch": 0.10999329309188464,
        "step": 1476
    },
    {
        "loss": 2.5835,
        "grad_norm": 3.359703540802002,
        "learning_rate": 0.00015225085641128847,
        "epoch": 0.11006781429316641,
        "step": 1477
    },
    {
        "loss": 3.065,
        "grad_norm": 2.893221855163574,
        "learning_rate": 0.0001521908473146618,
        "epoch": 0.11014233549444817,
        "step": 1478
    },
    {
        "loss": 2.5302,
        "grad_norm": 3.6130478382110596,
        "learning_rate": 0.00015213081237663774,
        "epoch": 0.11021685669572993,
        "step": 1479
    },
    {
        "loss": 2.4373,
        "grad_norm": 2.371882677078247,
        "learning_rate": 0.0001520707516269415,
        "epoch": 0.1102913778970117,
        "step": 1480
    },
    {
        "loss": 1.5872,
        "grad_norm": 2.451173782348633,
        "learning_rate": 0.00015201066509531116,
        "epoch": 0.11036589909829346,
        "step": 1481
    },
    {
        "loss": 2.4299,
        "grad_norm": 3.8348069190979004,
        "learning_rate": 0.0001519505528114975,
        "epoch": 0.11044042029957522,
        "step": 1482
    },
    {
        "loss": 1.5268,
        "grad_norm": 3.918522834777832,
        "learning_rate": 0.0001518904148052641,
        "epoch": 0.110514941500857,
        "step": 1483
    },
    {
        "loss": 1.7342,
        "grad_norm": 3.8096835613250732,
        "learning_rate": 0.00015183025110638724,
        "epoch": 0.11058946270213876,
        "step": 1484
    },
    {
        "loss": 2.7136,
        "grad_norm": 1.862021565437317,
        "learning_rate": 0.0001517700617446559,
        "epoch": 0.11066398390342053,
        "step": 1485
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.4603662490844727,
        "learning_rate": 0.00015170984674987189,
        "epoch": 0.11073850510470229,
        "step": 1486
    },
    {
        "loss": 2.5006,
        "grad_norm": 2.154871940612793,
        "learning_rate": 0.00015164960615184954,
        "epoch": 0.11081302630598405,
        "step": 1487
    },
    {
        "loss": 2.5725,
        "grad_norm": 2.7091712951660156,
        "learning_rate": 0.00015158933998041601,
        "epoch": 0.11088754750726582,
        "step": 1488
    },
    {
        "loss": 2.8458,
        "grad_norm": 3.931015729904175,
        "learning_rate": 0.00015152904826541103,
        "epoch": 0.11096206870854758,
        "step": 1489
    },
    {
        "loss": 2.0024,
        "grad_norm": 3.3386006355285645,
        "learning_rate": 0.000151468731036687,
        "epoch": 0.11103658990982934,
        "step": 1490
    },
    {
        "loss": 1.865,
        "grad_norm": 3.6080594062805176,
        "learning_rate": 0.00015140838832410896,
        "epoch": 0.1111111111111111,
        "step": 1491
    },
    {
        "loss": 3.139,
        "grad_norm": 2.5237655639648438,
        "learning_rate": 0.00015134802015755451,
        "epoch": 0.11118563231239288,
        "step": 1492
    },
    {
        "loss": 3.1441,
        "grad_norm": 1.7671741247177124,
        "learning_rate": 0.00015128762656691402,
        "epoch": 0.11126015351367465,
        "step": 1493
    },
    {
        "loss": 2.2609,
        "grad_norm": 2.411388397216797,
        "learning_rate": 0.00015122720758209022,
        "epoch": 0.11133467471495641,
        "step": 1494
    },
    {
        "loss": 2.717,
        "grad_norm": 2.5626325607299805,
        "learning_rate": 0.00015116676323299858,
        "epoch": 0.11140919591623817,
        "step": 1495
    },
    {
        "loss": 1.9665,
        "grad_norm": 2.708648443222046,
        "learning_rate": 0.00015110629354956711,
        "epoch": 0.11148371711751993,
        "step": 1496
    },
    {
        "loss": 1.908,
        "grad_norm": 1.5490591526031494,
        "learning_rate": 0.00015104579856173628,
        "epoch": 0.1115582383188017,
        "step": 1497
    },
    {
        "loss": 2.3663,
        "grad_norm": 3.240098714828491,
        "learning_rate": 0.00015098527829945915,
        "epoch": 0.11163275952008346,
        "step": 1498
    },
    {
        "loss": 2.7413,
        "grad_norm": 2.2275054454803467,
        "learning_rate": 0.00015092473279270128,
        "epoch": 0.11170728072136522,
        "step": 1499
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.3202316761016846,
        "learning_rate": 0.00015086416207144075,
        "epoch": 0.11178180192264699,
        "step": 1500
    },
    {
        "loss": 2.8533,
        "grad_norm": 2.699610710144043,
        "learning_rate": 0.00015080356616566806,
        "epoch": 0.11185632312392876,
        "step": 1501
    },
    {
        "loss": 2.8357,
        "grad_norm": 2.427239179611206,
        "learning_rate": 0.0001507429451053863,
        "epoch": 0.11193084432521053,
        "step": 1502
    },
    {
        "loss": 2.5029,
        "grad_norm": 2.6680185794830322,
        "learning_rate": 0.00015068229892061088,
        "epoch": 0.11200536552649229,
        "step": 1503
    },
    {
        "loss": 3.1663,
        "grad_norm": 2.149287462234497,
        "learning_rate": 0.00015062162764136976,
        "epoch": 0.11207988672777405,
        "step": 1504
    },
    {
        "loss": 2.2214,
        "grad_norm": 2.3067567348480225,
        "learning_rate": 0.00015056093129770323,
        "epoch": 0.11215440792905582,
        "step": 1505
    },
    {
        "loss": 1.8134,
        "grad_norm": 2.481065511703491,
        "learning_rate": 0.00015050020991966406,
        "epoch": 0.11222892913033758,
        "step": 1506
    },
    {
        "loss": 1.774,
        "grad_norm": 2.881054401397705,
        "learning_rate": 0.0001504394635373174,
        "epoch": 0.11230345033161934,
        "step": 1507
    },
    {
        "loss": 2.4999,
        "grad_norm": 1.6619576215744019,
        "learning_rate": 0.00015037869218074076,
        "epoch": 0.1123779715329011,
        "step": 1508
    },
    {
        "loss": 2.4815,
        "grad_norm": 2.2629923820495605,
        "learning_rate": 0.00015031789588002407,
        "epoch": 0.11245249273418287,
        "step": 1509
    },
    {
        "loss": 2.5638,
        "grad_norm": 3.769435167312622,
        "learning_rate": 0.0001502570746652695,
        "epoch": 0.11252701393546465,
        "step": 1510
    },
    {
        "loss": 2.2157,
        "grad_norm": 5.15852689743042,
        "learning_rate": 0.0001501962285665917,
        "epoch": 0.11260153513674641,
        "step": 1511
    },
    {
        "loss": 2.5924,
        "grad_norm": 2.1548678874969482,
        "learning_rate": 0.0001501353576141175,
        "epoch": 0.11267605633802817,
        "step": 1512
    },
    {
        "loss": 2.2461,
        "grad_norm": 2.509840250015259,
        "learning_rate": 0.00015007446183798608,
        "epoch": 0.11275057753930993,
        "step": 1513
    },
    {
        "loss": 2.4478,
        "grad_norm": 2.3777737617492676,
        "learning_rate": 0.00015001354126834905,
        "epoch": 0.1128250987405917,
        "step": 1514
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.896054267883301,
        "learning_rate": 0.00014995259593537002,
        "epoch": 0.11289961994187346,
        "step": 1515
    },
    {
        "loss": 1.566,
        "grad_norm": 3.382084846496582,
        "learning_rate": 0.0001498916258692252,
        "epoch": 0.11297414114315522,
        "step": 1516
    },
    {
        "loss": 2.5283,
        "grad_norm": 3.51493763923645,
        "learning_rate": 0.00014983063110010273,
        "epoch": 0.11304866234443699,
        "step": 1517
    },
    {
        "loss": 1.8052,
        "grad_norm": 3.3842291831970215,
        "learning_rate": 0.00014976961165820318,
        "epoch": 0.11312318354571875,
        "step": 1518
    },
    {
        "loss": 1.6729,
        "grad_norm": 3.2748546600341797,
        "learning_rate": 0.00014970856757373925,
        "epoch": 0.11319770474700053,
        "step": 1519
    },
    {
        "loss": 2.0135,
        "grad_norm": 2.5835649967193604,
        "learning_rate": 0.00014964749887693587,
        "epoch": 0.11327222594828229,
        "step": 1520
    },
    {
        "loss": 1.9005,
        "grad_norm": 3.0343873500823975,
        "learning_rate": 0.00014958640559803012,
        "epoch": 0.11334674714956405,
        "step": 1521
    },
    {
        "loss": 2.6159,
        "grad_norm": 3.691485643386841,
        "learning_rate": 0.00014952528776727135,
        "epoch": 0.11342126835084582,
        "step": 1522
    },
    {
        "loss": 2.6643,
        "grad_norm": 2.831902265548706,
        "learning_rate": 0.00014946414541492094,
        "epoch": 0.11349578955212758,
        "step": 1523
    },
    {
        "loss": 2.5159,
        "grad_norm": 4.057521343231201,
        "learning_rate": 0.00014940297857125252,
        "epoch": 0.11357031075340934,
        "step": 1524
    },
    {
        "loss": 2.713,
        "grad_norm": 2.9185116291046143,
        "learning_rate": 0.00014934178726655177,
        "epoch": 0.1136448319546911,
        "step": 1525
    },
    {
        "loss": 2.5573,
        "grad_norm": 2.1567327976226807,
        "learning_rate": 0.00014928057153111647,
        "epoch": 0.11371935315597287,
        "step": 1526
    },
    {
        "loss": 2.5746,
        "grad_norm": 2.4002737998962402,
        "learning_rate": 0.00014921933139525664,
        "epoch": 0.11379387435725465,
        "step": 1527
    },
    {
        "loss": 3.0683,
        "grad_norm": 2.1043214797973633,
        "learning_rate": 0.00014915806688929414,
        "epoch": 0.11386839555853641,
        "step": 1528
    },
    {
        "loss": 2.5286,
        "grad_norm": 2.3071436882019043,
        "learning_rate": 0.00014909677804356315,
        "epoch": 0.11394291675981817,
        "step": 1529
    },
    {
        "loss": 2.4172,
        "grad_norm": 4.840843200683594,
        "learning_rate": 0.00014903546488840975,
        "epoch": 0.11401743796109994,
        "step": 1530
    },
    {
        "loss": 2.1476,
        "grad_norm": 4.052730560302734,
        "learning_rate": 0.00014897412745419207,
        "epoch": 0.1140919591623817,
        "step": 1531
    },
    {
        "loss": 1.9647,
        "grad_norm": 2.2548370361328125,
        "learning_rate": 0.00014891276577128028,
        "epoch": 0.11416648036366346,
        "step": 1532
    },
    {
        "loss": 1.3115,
        "grad_norm": 3.04266619682312,
        "learning_rate": 0.00014885137987005653,
        "epoch": 0.11424100156494522,
        "step": 1533
    },
    {
        "loss": 2.7289,
        "grad_norm": 2.4179842472076416,
        "learning_rate": 0.00014878996978091508,
        "epoch": 0.11431552276622699,
        "step": 1534
    },
    {
        "loss": 1.902,
        "grad_norm": 3.5823311805725098,
        "learning_rate": 0.000148728535534262,
        "epoch": 0.11439004396750875,
        "step": 1535
    },
    {
        "loss": 2.6262,
        "grad_norm": 2.297821283340454,
        "learning_rate": 0.00014866707716051548,
        "epoch": 0.11446456516879053,
        "step": 1536
    },
    {
        "loss": 2.028,
        "grad_norm": 3.0539045333862305,
        "learning_rate": 0.00014860559469010543,
        "epoch": 0.11453908637007229,
        "step": 1537
    },
    {
        "loss": 2.7264,
        "grad_norm": 2.030301094055176,
        "learning_rate": 0.000148544088153474,
        "epoch": 0.11461360757135405,
        "step": 1538
    },
    {
        "loss": 2.2218,
        "grad_norm": 3.7847836017608643,
        "learning_rate": 0.00014848255758107497,
        "epoch": 0.11468812877263582,
        "step": 1539
    },
    {
        "loss": 1.7242,
        "grad_norm": 5.368911266326904,
        "learning_rate": 0.00014842100300337417,
        "epoch": 0.11476264997391758,
        "step": 1540
    },
    {
        "loss": 1.8551,
        "grad_norm": 2.588879346847534,
        "learning_rate": 0.0001483594244508493,
        "epoch": 0.11483717117519934,
        "step": 1541
    },
    {
        "loss": 2.4419,
        "grad_norm": 4.12217903137207,
        "learning_rate": 0.00014829782195398993,
        "epoch": 0.1149116923764811,
        "step": 1542
    },
    {
        "loss": 2.6537,
        "grad_norm": 2.157125234603882,
        "learning_rate": 0.00014823619554329745,
        "epoch": 0.11498621357776287,
        "step": 1543
    },
    {
        "loss": 1.989,
        "grad_norm": 4.464763641357422,
        "learning_rate": 0.00014817454524928517,
        "epoch": 0.11506073477904463,
        "step": 1544
    },
    {
        "loss": 3.0831,
        "grad_norm": 2.3522567749023438,
        "learning_rate": 0.0001481128711024781,
        "epoch": 0.11513525598032641,
        "step": 1545
    },
    {
        "loss": 2.4958,
        "grad_norm": 2.533205986022949,
        "learning_rate": 0.0001480511731334131,
        "epoch": 0.11520977718160817,
        "step": 1546
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.7790181636810303,
        "learning_rate": 0.00014798945137263895,
        "epoch": 0.11528429838288994,
        "step": 1547
    },
    {
        "loss": 2.2982,
        "grad_norm": 4.214208602905273,
        "learning_rate": 0.00014792770585071605,
        "epoch": 0.1153588195841717,
        "step": 1548
    },
    {
        "loss": 2.5308,
        "grad_norm": 2.176156520843506,
        "learning_rate": 0.0001478659365982167,
        "epoch": 0.11543334078545346,
        "step": 1549
    },
    {
        "loss": 2.2257,
        "grad_norm": 3.214611768722534,
        "learning_rate": 0.0001478041436457248,
        "epoch": 0.11550786198673522,
        "step": 1550
    },
    {
        "loss": 2.8102,
        "grad_norm": 3.1399483680725098,
        "learning_rate": 0.00014774232702383607,
        "epoch": 0.11558238318801699,
        "step": 1551
    },
    {
        "loss": 2.6656,
        "grad_norm": 2.622781276702881,
        "learning_rate": 0.000147680486763158,
        "epoch": 0.11565690438929875,
        "step": 1552
    },
    {
        "loss": 3.0218,
        "grad_norm": 2.316038131713867,
        "learning_rate": 0.0001476186228943097,
        "epoch": 0.11573142559058051,
        "step": 1553
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.872941017150879,
        "learning_rate": 0.00014755673544792196,
        "epoch": 0.11580594679186229,
        "step": 1554
    },
    {
        "loss": 1.6785,
        "grad_norm": 3.249215841293335,
        "learning_rate": 0.00014749482445463734,
        "epoch": 0.11588046799314405,
        "step": 1555
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.2452123165130615,
        "learning_rate": 0.00014743288994510995,
        "epoch": 0.11595498919442582,
        "step": 1556
    },
    {
        "loss": 2.4303,
        "grad_norm": 3.6542460918426514,
        "learning_rate": 0.00014737093195000566,
        "epoch": 0.11602951039570758,
        "step": 1557
    },
    {
        "loss": 3.0976,
        "grad_norm": 2.7192132472991943,
        "learning_rate": 0.00014730895050000183,
        "epoch": 0.11610403159698934,
        "step": 1558
    },
    {
        "loss": 2.5206,
        "grad_norm": 2.5127687454223633,
        "learning_rate": 0.00014724694562578755,
        "epoch": 0.1161785527982711,
        "step": 1559
    },
    {
        "loss": 2.8293,
        "grad_norm": 2.0152060985565186,
        "learning_rate": 0.00014718491735806345,
        "epoch": 0.11625307399955287,
        "step": 1560
    },
    {
        "loss": 2.0987,
        "grad_norm": 4.126082420349121,
        "learning_rate": 0.00014712286572754173,
        "epoch": 0.11632759520083463,
        "step": 1561
    },
    {
        "loss": 2.0499,
        "grad_norm": 3.1533987522125244,
        "learning_rate": 0.00014706079076494622,
        "epoch": 0.1164021164021164,
        "step": 1562
    },
    {
        "loss": 2.634,
        "grad_norm": 2.7581686973571777,
        "learning_rate": 0.00014699869250101228,
        "epoch": 0.11647663760339817,
        "step": 1563
    },
    {
        "loss": 2.4344,
        "grad_norm": 4.283756256103516,
        "learning_rate": 0.00014693657096648675,
        "epoch": 0.11655115880467994,
        "step": 1564
    },
    {
        "loss": 2.8099,
        "grad_norm": 2.8932933807373047,
        "learning_rate": 0.00014687442619212807,
        "epoch": 0.1166256800059617,
        "step": 1565
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.826082706451416,
        "learning_rate": 0.00014681225820870614,
        "epoch": 0.11670020120724346,
        "step": 1566
    },
    {
        "loss": 2.6768,
        "grad_norm": 2.410130739212036,
        "learning_rate": 0.00014675006704700235,
        "epoch": 0.11677472240852523,
        "step": 1567
    },
    {
        "loss": 1.6474,
        "grad_norm": 3.017347574234009,
        "learning_rate": 0.00014668785273780956,
        "epoch": 0.11684924360980699,
        "step": 1568
    },
    {
        "loss": 2.9046,
        "grad_norm": 3.610050916671753,
        "learning_rate": 0.00014662561531193218,
        "epoch": 0.11692376481108875,
        "step": 1569
    },
    {
        "loss": 2.5993,
        "grad_norm": 2.2908811569213867,
        "learning_rate": 0.000146563354800186,
        "epoch": 0.11699828601237051,
        "step": 1570
    },
    {
        "loss": 1.7008,
        "grad_norm": 2.0591647624969482,
        "learning_rate": 0.00014650107123339814,
        "epoch": 0.11707280721365228,
        "step": 1571
    },
    {
        "loss": 2.6877,
        "grad_norm": 3.4205703735351562,
        "learning_rate": 0.00014643876464240734,
        "epoch": 0.11714732841493405,
        "step": 1572
    },
    {
        "loss": 2.7412,
        "grad_norm": 2.2770211696624756,
        "learning_rate": 0.00014637643505806356,
        "epoch": 0.11722184961621582,
        "step": 1573
    },
    {
        "loss": 2.7216,
        "grad_norm": 2.819951295852661,
        "learning_rate": 0.00014631408251122822,
        "epoch": 0.11729637081749758,
        "step": 1574
    },
    {
        "loss": 2.3612,
        "grad_norm": 1.6346397399902344,
        "learning_rate": 0.00014625170703277414,
        "epoch": 0.11737089201877934,
        "step": 1575
    },
    {
        "loss": 2.6248,
        "grad_norm": 1.9965301752090454,
        "learning_rate": 0.00014618930865358548,
        "epoch": 0.1174454132200611,
        "step": 1576
    },
    {
        "loss": 2.6005,
        "grad_norm": 3.386707305908203,
        "learning_rate": 0.00014612688740455772,
        "epoch": 0.11751993442134287,
        "step": 1577
    },
    {
        "loss": 2.518,
        "grad_norm": 3.2067317962646484,
        "learning_rate": 0.0001460644433165976,
        "epoch": 0.11759445562262463,
        "step": 1578
    },
    {
        "loss": 2.1364,
        "grad_norm": 4.039116859436035,
        "learning_rate": 0.00014600197642062327,
        "epoch": 0.1176689768239064,
        "step": 1579
    },
    {
        "loss": 1.9229,
        "grad_norm": 3.3729541301727295,
        "learning_rate": 0.00014593948674756417,
        "epoch": 0.11774349802518816,
        "step": 1580
    },
    {
        "loss": 2.7416,
        "grad_norm": 2.25635027885437,
        "learning_rate": 0.0001458769743283609,
        "epoch": 0.11781801922646994,
        "step": 1581
    },
    {
        "loss": 3.0061,
        "grad_norm": 1.8743679523468018,
        "learning_rate": 0.00014581443919396548,
        "epoch": 0.1178925404277517,
        "step": 1582
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.8155708312988281,
        "learning_rate": 0.0001457518813753411,
        "epoch": 0.11796706162903346,
        "step": 1583
    },
    {
        "loss": 1.8221,
        "grad_norm": 3.948572874069214,
        "learning_rate": 0.00014568930090346215,
        "epoch": 0.11804158283031523,
        "step": 1584
    },
    {
        "loss": 2.9119,
        "grad_norm": 2.6853301525115967,
        "learning_rate": 0.00014562669780931428,
        "epoch": 0.11811610403159699,
        "step": 1585
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.8887786865234375,
        "learning_rate": 0.00014556407212389436,
        "epoch": 0.11819062523287875,
        "step": 1586
    },
    {
        "loss": 2.7429,
        "grad_norm": 2.1259045600891113,
        "learning_rate": 0.00014550142387821037,
        "epoch": 0.11826514643416051,
        "step": 1587
    },
    {
        "loss": 1.8419,
        "grad_norm": 3.4770724773406982,
        "learning_rate": 0.00014543875310328155,
        "epoch": 0.11833966763544228,
        "step": 1588
    },
    {
        "loss": 1.9177,
        "grad_norm": 3.374040365219116,
        "learning_rate": 0.0001453760598301382,
        "epoch": 0.11841418883672405,
        "step": 1589
    },
    {
        "loss": 3.0846,
        "grad_norm": 2.1689157485961914,
        "learning_rate": 0.00014531334408982184,
        "epoch": 0.11848871003800582,
        "step": 1590
    },
    {
        "loss": 2.153,
        "grad_norm": 3.235661745071411,
        "learning_rate": 0.00014525060591338512,
        "epoch": 0.11856323123928758,
        "step": 1591
    },
    {
        "loss": 2.8004,
        "grad_norm": 2.5130796432495117,
        "learning_rate": 0.00014518784533189177,
        "epoch": 0.11863775244056934,
        "step": 1592
    },
    {
        "loss": 2.7323,
        "grad_norm": 2.5611679553985596,
        "learning_rate": 0.00014512506237641655,
        "epoch": 0.11871227364185111,
        "step": 1593
    },
    {
        "loss": 2.6989,
        "grad_norm": 3.273502826690674,
        "learning_rate": 0.00014506225707804538,
        "epoch": 0.11878679484313287,
        "step": 1594
    },
    {
        "loss": 2.6735,
        "grad_norm": 2.4434003829956055,
        "learning_rate": 0.0001449994294678752,
        "epoch": 0.11886131604441463,
        "step": 1595
    },
    {
        "loss": 2.4815,
        "grad_norm": 2.4471991062164307,
        "learning_rate": 0.00014493657957701406,
        "epoch": 0.1189358372456964,
        "step": 1596
    },
    {
        "loss": 2.8314,
        "grad_norm": 3.9470956325531006,
        "learning_rate": 0.00014487370743658098,
        "epoch": 0.11901035844697816,
        "step": 1597
    },
    {
        "loss": 2.234,
        "grad_norm": 2.229297399520874,
        "learning_rate": 0.00014481081307770604,
        "epoch": 0.11908487964825994,
        "step": 1598
    },
    {
        "loss": 1.9003,
        "grad_norm": 1.9684345722198486,
        "learning_rate": 0.00014474789653153022,
        "epoch": 0.1191594008495417,
        "step": 1599
    },
    {
        "loss": 2.2486,
        "grad_norm": 2.9066874980926514,
        "learning_rate": 0.00014468495782920567,
        "epoch": 0.11923392205082346,
        "step": 1600
    },
    {
        "loss": 2.5993,
        "grad_norm": 3.293445587158203,
        "learning_rate": 0.00014462199700189532,
        "epoch": 0.11930844325210523,
        "step": 1601
    },
    {
        "loss": 2.9216,
        "grad_norm": 2.6593306064605713,
        "learning_rate": 0.00014455901408077314,
        "epoch": 0.11938296445338699,
        "step": 1602
    },
    {
        "loss": 2.435,
        "grad_norm": 3.1531386375427246,
        "learning_rate": 0.00014449600909702409,
        "epoch": 0.11945748565466875,
        "step": 1603
    },
    {
        "loss": 2.0695,
        "grad_norm": 3.4774863719940186,
        "learning_rate": 0.00014443298208184394,
        "epoch": 0.11953200685595052,
        "step": 1604
    },
    {
        "loss": 3.0394,
        "grad_norm": 1.1971378326416016,
        "learning_rate": 0.0001443699330664395,
        "epoch": 0.11960652805723228,
        "step": 1605
    },
    {
        "loss": 2.4123,
        "grad_norm": 3.3699893951416016,
        "learning_rate": 0.00014430686208202835,
        "epoch": 0.11968104925851404,
        "step": 1606
    },
    {
        "loss": 2.261,
        "grad_norm": 2.6908257007598877,
        "learning_rate": 0.00014424376915983902,
        "epoch": 0.11975557045979582,
        "step": 1607
    },
    {
        "loss": 3.2394,
        "grad_norm": 2.891688823699951,
        "learning_rate": 0.00014418065433111085,
        "epoch": 0.11983009166107758,
        "step": 1608
    },
    {
        "loss": 2.6765,
        "grad_norm": 4.714570045471191,
        "learning_rate": 0.00014411751762709408,
        "epoch": 0.11990461286235934,
        "step": 1609
    },
    {
        "loss": 2.642,
        "grad_norm": 2.4052329063415527,
        "learning_rate": 0.00014405435907904977,
        "epoch": 0.11997913406364111,
        "step": 1610
    },
    {
        "loss": 2.6714,
        "grad_norm": 1.8889576196670532,
        "learning_rate": 0.00014399117871824978,
        "epoch": 0.12005365526492287,
        "step": 1611
    },
    {
        "loss": 2.0987,
        "grad_norm": 2.5975937843322754,
        "learning_rate": 0.00014392797657597676,
        "epoch": 0.12012817646620463,
        "step": 1612
    },
    {
        "loss": 2.815,
        "grad_norm": 3.2564187049865723,
        "learning_rate": 0.0001438647526835242,
        "epoch": 0.1202026976674864,
        "step": 1613
    },
    {
        "loss": 1.7825,
        "grad_norm": 4.02506685256958,
        "learning_rate": 0.00014380150707219626,
        "epoch": 0.12027721886876816,
        "step": 1614
    },
    {
        "loss": 2.3498,
        "grad_norm": 2.3966524600982666,
        "learning_rate": 0.00014373823977330792,
        "epoch": 0.12035174007004992,
        "step": 1615
    },
    {
        "loss": 2.2319,
        "grad_norm": 2.3554091453552246,
        "learning_rate": 0.00014367495081818495,
        "epoch": 0.1204262612713317,
        "step": 1616
    },
    {
        "loss": 1.414,
        "grad_norm": 3.534125566482544,
        "learning_rate": 0.00014361164023816376,
        "epoch": 0.12050078247261346,
        "step": 1617
    },
    {
        "loss": 2.7535,
        "grad_norm": 1.9215434789657593,
        "learning_rate": 0.00014354830806459148,
        "epoch": 0.12057530367389523,
        "step": 1618
    },
    {
        "loss": 2.6376,
        "grad_norm": 2.2998433113098145,
        "learning_rate": 0.0001434849543288259,
        "epoch": 0.12064982487517699,
        "step": 1619
    },
    {
        "loss": 3.0175,
        "grad_norm": 2.5214030742645264,
        "learning_rate": 0.00014342157906223563,
        "epoch": 0.12072434607645875,
        "step": 1620
    },
    {
        "loss": 1.5694,
        "grad_norm": 3.9974746704101562,
        "learning_rate": 0.00014335818229619976,
        "epoch": 0.12079886727774052,
        "step": 1621
    },
    {
        "loss": 2.0594,
        "grad_norm": 2.70222806930542,
        "learning_rate": 0.0001432947640621081,
        "epoch": 0.12087338847902228,
        "step": 1622
    },
    {
        "loss": 2.7453,
        "grad_norm": 2.80273175239563,
        "learning_rate": 0.00014323132439136114,
        "epoch": 0.12094790968030404,
        "step": 1623
    },
    {
        "loss": 2.4275,
        "grad_norm": 2.949451446533203,
        "learning_rate": 0.0001431678633153699,
        "epoch": 0.1210224308815858,
        "step": 1624
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.8585314750671387,
        "learning_rate": 0.0001431043808655561,
        "epoch": 0.12109695208286758,
        "step": 1625
    },
    {
        "loss": 1.4053,
        "grad_norm": 2.0540380477905273,
        "learning_rate": 0.00014304087707335192,
        "epoch": 0.12117147328414934,
        "step": 1626
    },
    {
        "loss": 2.8755,
        "grad_norm": 2.2549304962158203,
        "learning_rate": 0.00014297735197020013,
        "epoch": 0.12124599448543111,
        "step": 1627
    },
    {
        "loss": 2.8189,
        "grad_norm": 1.701377511024475,
        "learning_rate": 0.00014291380558755423,
        "epoch": 0.12132051568671287,
        "step": 1628
    },
    {
        "loss": 3.1644,
        "grad_norm": 2.2650699615478516,
        "learning_rate": 0.00014285023795687795,
        "epoch": 0.12139503688799463,
        "step": 1629
    },
    {
        "loss": 2.8384,
        "grad_norm": 2.7169549465179443,
        "learning_rate": 0.0001427866491096458,
        "epoch": 0.1214695580892764,
        "step": 1630
    },
    {
        "loss": 3.1929,
        "grad_norm": 5.671013832092285,
        "learning_rate": 0.0001427230390773427,
        "epoch": 0.12154407929055816,
        "step": 1631
    },
    {
        "loss": 1.579,
        "grad_norm": 4.024968147277832,
        "learning_rate": 0.00014265940789146402,
        "epoch": 0.12161860049183992,
        "step": 1632
    },
    {
        "loss": 2.2877,
        "grad_norm": 3.3802332878112793,
        "learning_rate": 0.00014259575558351568,
        "epoch": 0.12169312169312169,
        "step": 1633
    },
    {
        "loss": 2.8216,
        "grad_norm": 2.259371519088745,
        "learning_rate": 0.000142532082185014,
        "epoch": 0.12176764289440346,
        "step": 1634
    },
    {
        "loss": 1.8012,
        "grad_norm": 4.20619535446167,
        "learning_rate": 0.00014246838772748576,
        "epoch": 0.12184216409568523,
        "step": 1635
    },
    {
        "loss": 1.1316,
        "grad_norm": 3.404620885848999,
        "learning_rate": 0.0001424046722424682,
        "epoch": 0.12191668529696699,
        "step": 1636
    },
    {
        "loss": 2.7019,
        "grad_norm": 2.956851005554199,
        "learning_rate": 0.00014234093576150894,
        "epoch": 0.12199120649824875,
        "step": 1637
    },
    {
        "loss": 1.7541,
        "grad_norm": 3.4990124702453613,
        "learning_rate": 0.00014227717831616593,
        "epoch": 0.12206572769953052,
        "step": 1638
    },
    {
        "loss": 1.844,
        "grad_norm": 3.2891921997070312,
        "learning_rate": 0.00014221339993800765,
        "epoch": 0.12214024890081228,
        "step": 1639
    },
    {
        "loss": 2.6691,
        "grad_norm": 3.801738739013672,
        "learning_rate": 0.00014214960065861285,
        "epoch": 0.12221477010209404,
        "step": 1640
    },
    {
        "loss": 1.8458,
        "grad_norm": 3.310720920562744,
        "learning_rate": 0.00014208578050957065,
        "epoch": 0.1222892913033758,
        "step": 1641
    },
    {
        "loss": 2.3904,
        "grad_norm": 2.3780195713043213,
        "learning_rate": 0.00014202193952248042,
        "epoch": 0.12236381250465758,
        "step": 1642
    },
    {
        "loss": 2.8128,
        "grad_norm": 2.095865488052368,
        "learning_rate": 0.00014195807772895202,
        "epoch": 0.12243833370593935,
        "step": 1643
    },
    {
        "loss": 2.2899,
        "grad_norm": 2.282660961151123,
        "learning_rate": 0.00014189419516060544,
        "epoch": 0.12251285490722111,
        "step": 1644
    },
    {
        "loss": 2.7271,
        "grad_norm": 2.211918592453003,
        "learning_rate": 0.00014183029184907105,
        "epoch": 0.12258737610850287,
        "step": 1645
    },
    {
        "loss": 1.6376,
        "grad_norm": 2.8737618923187256,
        "learning_rate": 0.00014176636782598957,
        "epoch": 0.12266189730978463,
        "step": 1646
    },
    {
        "loss": 2.7039,
        "grad_norm": 4.220503807067871,
        "learning_rate": 0.0001417024231230117,
        "epoch": 0.1227364185110664,
        "step": 1647
    },
    {
        "loss": 2.3258,
        "grad_norm": 2.271271228790283,
        "learning_rate": 0.0001416384577717987,
        "epoch": 0.12281093971234816,
        "step": 1648
    },
    {
        "loss": 2.9744,
        "grad_norm": 2.6889660358428955,
        "learning_rate": 0.00014157447180402185,
        "epoch": 0.12288546091362992,
        "step": 1649
    },
    {
        "loss": 2.2849,
        "grad_norm": 2.3334624767303467,
        "learning_rate": 0.00014151046525136268,
        "epoch": 0.12295998211491169,
        "step": 1650
    },
    {
        "loss": 1.2865,
        "grad_norm": 2.8907692432403564,
        "learning_rate": 0.00014144643814551297,
        "epoch": 0.12303450331619346,
        "step": 1651
    },
    {
        "loss": 2.5473,
        "grad_norm": 2.722334623336792,
        "learning_rate": 0.00014138239051817464,
        "epoch": 0.12310902451747523,
        "step": 1652
    },
    {
        "loss": 2.699,
        "grad_norm": 1.4128799438476562,
        "learning_rate": 0.00014131832240105974,
        "epoch": 0.12318354571875699,
        "step": 1653
    },
    {
        "loss": 2.5512,
        "grad_norm": 2.8296780586242676,
        "learning_rate": 0.0001412542338258905,
        "epoch": 0.12325806692003875,
        "step": 1654
    },
    {
        "loss": 2.1298,
        "grad_norm": 3.476008176803589,
        "learning_rate": 0.00014119012482439927,
        "epoch": 0.12333258812132052,
        "step": 1655
    },
    {
        "loss": 2.7473,
        "grad_norm": 2.9157845973968506,
        "learning_rate": 0.00014112599542832855,
        "epoch": 0.12340710932260228,
        "step": 1656
    },
    {
        "loss": 2.4403,
        "grad_norm": 1.2191592454910278,
        "learning_rate": 0.00014106184566943085,
        "epoch": 0.12348163052388404,
        "step": 1657
    },
    {
        "loss": 2.3986,
        "grad_norm": 3.5558643341064453,
        "learning_rate": 0.00014099767557946882,
        "epoch": 0.1235561517251658,
        "step": 1658
    },
    {
        "loss": 2.6922,
        "grad_norm": 2.54779052734375,
        "learning_rate": 0.00014093348519021522,
        "epoch": 0.12363067292644757,
        "step": 1659
    },
    {
        "loss": 1.7955,
        "grad_norm": 1.4402989149093628,
        "learning_rate": 0.00014086927453345278,
        "epoch": 0.12370519412772935,
        "step": 1660
    },
    {
        "loss": 2.4579,
        "grad_norm": 3.2703945636749268,
        "learning_rate": 0.00014080504364097433,
        "epoch": 0.12377971532901111,
        "step": 1661
    },
    {
        "loss": 1.7465,
        "grad_norm": 2.24112606048584,
        "learning_rate": 0.00014074079254458263,
        "epoch": 0.12385423653029287,
        "step": 1662
    },
    {
        "loss": 2.5491,
        "grad_norm": 2.5092577934265137,
        "learning_rate": 0.00014067652127609058,
        "epoch": 0.12392875773157463,
        "step": 1663
    },
    {
        "loss": 2.4009,
        "grad_norm": 2.2688674926757812,
        "learning_rate": 0.00014061222986732087,
        "epoch": 0.1240032789328564,
        "step": 1664
    },
    {
        "loss": 3.1022,
        "grad_norm": 2.50418758392334,
        "learning_rate": 0.00014054791835010642,
        "epoch": 0.12407780013413816,
        "step": 1665
    },
    {
        "loss": 1.8965,
        "grad_norm": 4.8107123374938965,
        "learning_rate": 0.00014048358675628988,
        "epoch": 0.12415232133541992,
        "step": 1666
    },
    {
        "loss": 2.0295,
        "grad_norm": 2.6669747829437256,
        "learning_rate": 0.00014041923511772398,
        "epoch": 0.12422684253670169,
        "step": 1667
    },
    {
        "loss": 2.2218,
        "grad_norm": 3.43141770362854,
        "learning_rate": 0.00014035486346627125,
        "epoch": 0.12430136373798345,
        "step": 1668
    },
    {
        "loss": 2.6224,
        "grad_norm": 2.401261329650879,
        "learning_rate": 0.0001402904718338043,
        "epoch": 0.12437588493926523,
        "step": 1669
    },
    {
        "loss": 2.1648,
        "grad_norm": 3.259521007537842,
        "learning_rate": 0.00014022606025220543,
        "epoch": 0.12445040614054699,
        "step": 1670
    },
    {
        "loss": 3.0566,
        "grad_norm": 3.0089755058288574,
        "learning_rate": 0.00014016162875336703,
        "epoch": 0.12452492734182875,
        "step": 1671
    },
    {
        "loss": 2.3942,
        "grad_norm": 3.142951488494873,
        "learning_rate": 0.00014009717736919114,
        "epoch": 0.12459944854311052,
        "step": 1672
    },
    {
        "loss": 2.7194,
        "grad_norm": 2.572340965270996,
        "learning_rate": 0.00014003270613158984,
        "epoch": 0.12467396974439228,
        "step": 1673
    },
    {
        "loss": 1.6681,
        "grad_norm": 2.839165687561035,
        "learning_rate": 0.00013996821507248489,
        "epoch": 0.12474849094567404,
        "step": 1674
    },
    {
        "loss": 2.4093,
        "grad_norm": 2.0188825130462646,
        "learning_rate": 0.000139903704223808,
        "epoch": 0.1248230121469558,
        "step": 1675
    },
    {
        "loss": 2.4912,
        "grad_norm": 4.1587815284729,
        "learning_rate": 0.0001398391736175005,
        "epoch": 0.12489753334823757,
        "step": 1676
    },
    {
        "loss": 2.5637,
        "grad_norm": 2.467736005783081,
        "learning_rate": 0.00013977462328551368,
        "epoch": 0.12497205454951933,
        "step": 1677
    },
    {
        "loss": 2.298,
        "grad_norm": 2.8282558917999268,
        "learning_rate": 0.00013971005325980849,
        "epoch": 0.1250465757508011,
        "step": 1678
    },
    {
        "loss": 3.1028,
        "grad_norm": 3.2349069118499756,
        "learning_rate": 0.0001396454635723557,
        "epoch": 0.12512109695208287,
        "step": 1679
    },
    {
        "loss": 3.2129,
        "grad_norm": 2.6371896266937256,
        "learning_rate": 0.00013958085425513572,
        "epoch": 0.12519561815336464,
        "step": 1680
    },
    {
        "loss": 2.1561,
        "grad_norm": 3.4471864700317383,
        "learning_rate": 0.00013951622534013878,
        "epoch": 0.1252701393546464,
        "step": 1681
    },
    {
        "loss": 1.9893,
        "grad_norm": 2.2498433589935303,
        "learning_rate": 0.0001394515768593648,
        "epoch": 0.12534466055592816,
        "step": 1682
    },
    {
        "loss": 2.1221,
        "grad_norm": 3.0061404705047607,
        "learning_rate": 0.0001393869088448233,
        "epoch": 0.12541918175720992,
        "step": 1683
    },
    {
        "loss": 1.7317,
        "grad_norm": 3.9887330532073975,
        "learning_rate": 0.00013932222132853354,
        "epoch": 0.1254937029584917,
        "step": 1684
    },
    {
        "loss": 2.3608,
        "grad_norm": 1.8971866369247437,
        "learning_rate": 0.00013925751434252446,
        "epoch": 0.12556822415977345,
        "step": 1685
    },
    {
        "loss": 2.0947,
        "grad_norm": 3.4570934772491455,
        "learning_rate": 0.00013919278791883462,
        "epoch": 0.1256427453610552,
        "step": 1686
    },
    {
        "loss": 2.6649,
        "grad_norm": 2.5530622005462646,
        "learning_rate": 0.0001391280420895121,
        "epoch": 0.12571726656233698,
        "step": 1687
    },
    {
        "loss": 1.7414,
        "grad_norm": 4.383750915527344,
        "learning_rate": 0.00013906327688661477,
        "epoch": 0.12579178776361874,
        "step": 1688
    },
    {
        "loss": 2.6941,
        "grad_norm": 2.7743382453918457,
        "learning_rate": 0.00013899849234220998,
        "epoch": 0.1258663089649005,
        "step": 1689
    },
    {
        "loss": 2.2768,
        "grad_norm": 3.742354154586792,
        "learning_rate": 0.0001389336884883747,
        "epoch": 0.12594083016618227,
        "step": 1690
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.0106470584869385,
        "learning_rate": 0.0001388688653571954,
        "epoch": 0.12601535136746406,
        "step": 1691
    },
    {
        "loss": 2.4072,
        "grad_norm": 4.014799118041992,
        "learning_rate": 0.00013880402298076816,
        "epoch": 0.12608987256874582,
        "step": 1692
    },
    {
        "loss": 2.7682,
        "grad_norm": 3.8641343116760254,
        "learning_rate": 0.0001387391613911985,
        "epoch": 0.12616439377002758,
        "step": 1693
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.739764451980591,
        "learning_rate": 0.00013867428062060163,
        "epoch": 0.12623891497130935,
        "step": 1694
    },
    {
        "loss": 2.4139,
        "grad_norm": 2.624476909637451,
        "learning_rate": 0.00013860938070110209,
        "epoch": 0.1263134361725911,
        "step": 1695
    },
    {
        "loss": 3.1245,
        "grad_norm": 2.629603147506714,
        "learning_rate": 0.0001385444616648339,
        "epoch": 0.12638795737387287,
        "step": 1696
    },
    {
        "loss": 2.3497,
        "grad_norm": 3.1476571559906006,
        "learning_rate": 0.00013847952354394068,
        "epoch": 0.12646247857515464,
        "step": 1697
    },
    {
        "loss": 2.6467,
        "grad_norm": 3.2553374767303467,
        "learning_rate": 0.00013841456637057536,
        "epoch": 0.1265369997764364,
        "step": 1698
    },
    {
        "loss": 2.2569,
        "grad_norm": 3.1312804222106934,
        "learning_rate": 0.00013834959017690042,
        "epoch": 0.12661152097771816,
        "step": 1699
    },
    {
        "loss": 2.2981,
        "grad_norm": 2.0417981147766113,
        "learning_rate": 0.00013828459499508767,
        "epoch": 0.12668604217899992,
        "step": 1700
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.9171175956726074,
        "learning_rate": 0.00013821958085731835,
        "epoch": 0.1267605633802817,
        "step": 1701
    },
    {
        "loss": 2.7507,
        "grad_norm": 2.140187978744507,
        "learning_rate": 0.0001381545477957831,
        "epoch": 0.12683508458156345,
        "step": 1702
    },
    {
        "loss": 1.8108,
        "grad_norm": 3.102358102798462,
        "learning_rate": 0.00013808949584268195,
        "epoch": 0.12690960578284521,
        "step": 1703
    },
    {
        "loss": 2.3937,
        "grad_norm": 2.7441301345825195,
        "learning_rate": 0.00013802442503022417,
        "epoch": 0.12698412698412698,
        "step": 1704
    },
    {
        "loss": 2.7755,
        "grad_norm": 2.811979055404663,
        "learning_rate": 0.00013795933539062852,
        "epoch": 0.12705864818540874,
        "step": 1705
    },
    {
        "loss": 2.5006,
        "grad_norm": 1.9437726736068726,
        "learning_rate": 0.00013789422695612298,
        "epoch": 0.1271331693866905,
        "step": 1706
    },
    {
        "loss": 2.587,
        "grad_norm": 2.803115129470825,
        "learning_rate": 0.00013782909975894485,
        "epoch": 0.12720769058797227,
        "step": 1707
    },
    {
        "loss": 2.759,
        "grad_norm": 2.841231107711792,
        "learning_rate": 0.0001377639538313408,
        "epoch": 0.12728221178925403,
        "step": 1708
    },
    {
        "loss": 2.0695,
        "grad_norm": 3.6289150714874268,
        "learning_rate": 0.00013769878920556666,
        "epoch": 0.12735673299053582,
        "step": 1709
    },
    {
        "loss": 2.0406,
        "grad_norm": 5.137981414794922,
        "learning_rate": 0.00013763360591388757,
        "epoch": 0.12743125419181758,
        "step": 1710
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.742709159851074,
        "learning_rate": 0.00013756840398857794,
        "epoch": 0.12750577539309935,
        "step": 1711
    },
    {
        "loss": 2.665,
        "grad_norm": 1.876932144165039,
        "learning_rate": 0.00013750318346192138,
        "epoch": 0.1275802965943811,
        "step": 1712
    },
    {
        "loss": 1.9913,
        "grad_norm": 4.316612720489502,
        "learning_rate": 0.00013743794436621066,
        "epoch": 0.12765481779566287,
        "step": 1713
    },
    {
        "loss": 1.6983,
        "grad_norm": 3.1106555461883545,
        "learning_rate": 0.00013737268673374783,
        "epoch": 0.12772933899694464,
        "step": 1714
    },
    {
        "loss": 2.0292,
        "grad_norm": 2.1469802856445312,
        "learning_rate": 0.00013730741059684415,
        "epoch": 0.1278038601982264,
        "step": 1715
    },
    {
        "loss": 2.6762,
        "grad_norm": 1.9218637943267822,
        "learning_rate": 0.00013724211598781978,
        "epoch": 0.12787838139950816,
        "step": 1716
    },
    {
        "loss": 1.7075,
        "grad_norm": 1.5707321166992188,
        "learning_rate": 0.00013717680293900446,
        "epoch": 0.12795290260078993,
        "step": 1717
    },
    {
        "loss": 2.7793,
        "grad_norm": 2.262863874435425,
        "learning_rate": 0.00013711147148273662,
        "epoch": 0.1280274238020717,
        "step": 1718
    },
    {
        "loss": 2.2042,
        "grad_norm": 2.8802649974823,
        "learning_rate": 0.0001370461216513641,
        "epoch": 0.12810194500335345,
        "step": 1719
    },
    {
        "loss": 2.0105,
        "grad_norm": 3.7267603874206543,
        "learning_rate": 0.00013698075347724374,
        "epoch": 0.12817646620463521,
        "step": 1720
    },
    {
        "loss": 2.5603,
        "grad_norm": 3.2217578887939453,
        "learning_rate": 0.00013691536699274139,
        "epoch": 0.12825098740591698,
        "step": 1721
    },
    {
        "loss": 2.2932,
        "grad_norm": 4.255764007568359,
        "learning_rate": 0.00013684996223023216,
        "epoch": 0.12832550860719874,
        "step": 1722
    },
    {
        "loss": 2.333,
        "grad_norm": 3.2734622955322266,
        "learning_rate": 0.0001367845392221,
        "epoch": 0.1284000298084805,
        "step": 1723
    },
    {
        "loss": 2.2875,
        "grad_norm": 3.8156967163085938,
        "learning_rate": 0.00013671909800073802,
        "epoch": 0.12847455100976227,
        "step": 1724
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.9252068996429443,
        "learning_rate": 0.0001366536385985483,
        "epoch": 0.12854907221104403,
        "step": 1725
    },
    {
        "loss": 2.9015,
        "grad_norm": 2.337446928024292,
        "learning_rate": 0.00013658816104794187,
        "epoch": 0.1286235934123258,
        "step": 1726
    },
    {
        "loss": 2.9629,
        "grad_norm": 2.310076951980591,
        "learning_rate": 0.00013652266538133893,
        "epoch": 0.12869811461360758,
        "step": 1727
    },
    {
        "loss": 2.7233,
        "grad_norm": 1.899407148361206,
        "learning_rate": 0.00013645715163116846,
        "epoch": 0.12877263581488935,
        "step": 1728
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.9478360414505005,
        "learning_rate": 0.00013639161982986848,
        "epoch": 0.1288471570161711,
        "step": 1729
    },
    {
        "loss": 2.2193,
        "grad_norm": 3.071493625640869,
        "learning_rate": 0.00013632607000988592,
        "epoch": 0.12892167821745287,
        "step": 1730
    },
    {
        "loss": 1.6469,
        "grad_norm": 4.71882438659668,
        "learning_rate": 0.00013626050220367665,
        "epoch": 0.12899619941873464,
        "step": 1731
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.9018586874008179,
        "learning_rate": 0.00013619491644370545,
        "epoch": 0.1290707206200164,
        "step": 1732
    },
    {
        "loss": 2.0859,
        "grad_norm": 3.311768054962158,
        "learning_rate": 0.0001361293127624459,
        "epoch": 0.12914524182129816,
        "step": 1733
    },
    {
        "loss": 1.5282,
        "grad_norm": 4.549822807312012,
        "learning_rate": 0.00013606369119238066,
        "epoch": 0.12921976302257993,
        "step": 1734
    },
    {
        "loss": 2.287,
        "grad_norm": 1.1267057657241821,
        "learning_rate": 0.000135998051766001,
        "epoch": 0.1292942842238617,
        "step": 1735
    },
    {
        "loss": 2.0244,
        "grad_norm": 2.717130661010742,
        "learning_rate": 0.00013593239451580727,
        "epoch": 0.12936880542514345,
        "step": 1736
    },
    {
        "loss": 2.292,
        "grad_norm": 2.0813746452331543,
        "learning_rate": 0.0001358667194743084,
        "epoch": 0.12944332662642521,
        "step": 1737
    },
    {
        "loss": 2.7163,
        "grad_norm": 2.5747005939483643,
        "learning_rate": 0.00013580102667402235,
        "epoch": 0.12951784782770698,
        "step": 1738
    },
    {
        "loss": 2.2898,
        "grad_norm": 3.2002148628234863,
        "learning_rate": 0.00013573531614747566,
        "epoch": 0.12959236902898874,
        "step": 1739
    },
    {
        "loss": 1.8658,
        "grad_norm": 3.1652204990386963,
        "learning_rate": 0.0001356695879272039,
        "epoch": 0.1296668902302705,
        "step": 1740
    },
    {
        "loss": 2.12,
        "grad_norm": 3.0383975505828857,
        "learning_rate": 0.00013560384204575117,
        "epoch": 0.12974141143155227,
        "step": 1741
    },
    {
        "loss": 2.5077,
        "grad_norm": 1.7612216472625732,
        "learning_rate": 0.00013553807853567044,
        "epoch": 0.12981593263283403,
        "step": 1742
    },
    {
        "loss": 2.1819,
        "grad_norm": 1.7187517881393433,
        "learning_rate": 0.0001354722974295234,
        "epoch": 0.1298904538341158,
        "step": 1743
    },
    {
        "loss": 2.5988,
        "grad_norm": 3.800886869430542,
        "learning_rate": 0.00013540649875988037,
        "epoch": 0.12996497503539758,
        "step": 1744
    },
    {
        "loss": 2.3429,
        "grad_norm": 1.4523067474365234,
        "learning_rate": 0.00013534068255932055,
        "epoch": 0.13003949623667935,
        "step": 1745
    },
    {
        "loss": 3.0411,
        "grad_norm": 2.015247106552124,
        "learning_rate": 0.00013527484886043152,
        "epoch": 0.1301140174379611,
        "step": 1746
    },
    {
        "loss": 2.9007,
        "grad_norm": 2.0660524368286133,
        "learning_rate": 0.0001352089976958098,
        "epoch": 0.13018853863924287,
        "step": 1747
    },
    {
        "loss": 2.6747,
        "grad_norm": 2.035762310028076,
        "learning_rate": 0.00013514312909806048,
        "epoch": 0.13026305984052464,
        "step": 1748
    },
    {
        "loss": 2.4224,
        "grad_norm": 3.2081239223480225,
        "learning_rate": 0.00013507724309979718,
        "epoch": 0.1303375810418064,
        "step": 1749
    },
    {
        "loss": 2.5424,
        "grad_norm": 4.100547790527344,
        "learning_rate": 0.00013501133973364233,
        "epoch": 0.13041210224308816,
        "step": 1750
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.019259452819824,
        "learning_rate": 0.00013494541903222675,
        "epoch": 0.13048662344436993,
        "step": 1751
    },
    {
        "loss": 2.1625,
        "grad_norm": 2.810343027114868,
        "learning_rate": 0.00013487948102818996,
        "epoch": 0.1305611446456517,
        "step": 1752
    },
    {
        "loss": 2.3374,
        "grad_norm": 3.11665940284729,
        "learning_rate": 0.00013481352575418,
        "epoch": 0.13063566584693345,
        "step": 1753
    },
    {
        "loss": 2.2425,
        "grad_norm": 3.423954725265503,
        "learning_rate": 0.00013474755324285352,
        "epoch": 0.13071018704821522,
        "step": 1754
    },
    {
        "loss": 2.6426,
        "grad_norm": 1.689116358757019,
        "learning_rate": 0.00013468156352687568,
        "epoch": 0.13078470824949698,
        "step": 1755
    },
    {
        "loss": 2.8643,
        "grad_norm": 2.231189250946045,
        "learning_rate": 0.00013461555663892014,
        "epoch": 0.13085922945077874,
        "step": 1756
    },
    {
        "loss": 2.5793,
        "grad_norm": 1.856690526008606,
        "learning_rate": 0.00013454953261166907,
        "epoch": 0.1309337506520605,
        "step": 1757
    },
    {
        "loss": 2.7594,
        "grad_norm": 3.3804495334625244,
        "learning_rate": 0.0001344834914778131,
        "epoch": 0.13100827185334227,
        "step": 1758
    },
    {
        "loss": 1.9428,
        "grad_norm": 4.4650373458862305,
        "learning_rate": 0.00013441743327005144,
        "epoch": 0.13108279305462403,
        "step": 1759
    },
    {
        "loss": 2.9076,
        "grad_norm": 3.0181543827056885,
        "learning_rate": 0.00013435135802109153,
        "epoch": 0.1311573142559058,
        "step": 1760
    },
    {
        "loss": 2.646,
        "grad_norm": 2.562354803085327,
        "learning_rate": 0.0001342852657636495,
        "epoch": 0.13123183545718756,
        "step": 1761
    },
    {
        "loss": 2.0192,
        "grad_norm": 3.1061317920684814,
        "learning_rate": 0.00013421915653044977,
        "epoch": 0.13130635665846935,
        "step": 1762
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.4413108825683594,
        "learning_rate": 0.00013415303035422518,
        "epoch": 0.1313808778597511,
        "step": 1763
    },
    {
        "loss": 2.6326,
        "grad_norm": 2.206667900085449,
        "learning_rate": 0.00013408688726771696,
        "epoch": 0.13145539906103287,
        "step": 1764
    },
    {
        "loss": 2.8516,
        "grad_norm": 2.9614124298095703,
        "learning_rate": 0.00013402072730367475,
        "epoch": 0.13152992026231464,
        "step": 1765
    },
    {
        "loss": 2.2478,
        "grad_norm": 2.6818764209747314,
        "learning_rate": 0.00013395455049485644,
        "epoch": 0.1316044414635964,
        "step": 1766
    },
    {
        "loss": 2.6441,
        "grad_norm": 4.615907192230225,
        "learning_rate": 0.0001338883568740284,
        "epoch": 0.13167896266487816,
        "step": 1767
    },
    {
        "loss": 2.6808,
        "grad_norm": 2.422764539718628,
        "learning_rate": 0.00013382214647396525,
        "epoch": 0.13175348386615993,
        "step": 1768
    },
    {
        "loss": 2.5344,
        "grad_norm": 3.484734296798706,
        "learning_rate": 0.0001337559193274499,
        "epoch": 0.1318280050674417,
        "step": 1769
    },
    {
        "loss": 2.4201,
        "grad_norm": 3.973667621612549,
        "learning_rate": 0.0001336896754672736,
        "epoch": 0.13190252626872345,
        "step": 1770
    },
    {
        "loss": 2.4984,
        "grad_norm": 3.0360522270202637,
        "learning_rate": 0.0001336234149262359,
        "epoch": 0.13197704747000522,
        "step": 1771
    },
    {
        "loss": 2.1102,
        "grad_norm": 3.358947277069092,
        "learning_rate": 0.0001335571377371444,
        "epoch": 0.13205156867128698,
        "step": 1772
    },
    {
        "loss": 2.2246,
        "grad_norm": 3.8184666633605957,
        "learning_rate": 0.0001334908439328153,
        "epoch": 0.13212608987256874,
        "step": 1773
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.8486237525939941,
        "learning_rate": 0.0001334245335460728,
        "epoch": 0.1322006110738505,
        "step": 1774
    },
    {
        "loss": 2.9979,
        "grad_norm": 2.396507978439331,
        "learning_rate": 0.00013335820660974923,
        "epoch": 0.13227513227513227,
        "step": 1775
    },
    {
        "loss": 2.3217,
        "grad_norm": 3.0135984420776367,
        "learning_rate": 0.0001332918631566853,
        "epoch": 0.13234965347641403,
        "step": 1776
    },
    {
        "loss": 2.7805,
        "grad_norm": 2.422372817993164,
        "learning_rate": 0.0001332255032197298,
        "epoch": 0.1324241746776958,
        "step": 1777
    },
    {
        "loss": 2.6447,
        "grad_norm": 1.3622941970825195,
        "learning_rate": 0.0001331591268317398,
        "epoch": 0.13249869587897756,
        "step": 1778
    },
    {
        "loss": 2.7414,
        "grad_norm": 2.163645029067993,
        "learning_rate": 0.0001330927340255803,
        "epoch": 0.13257321708025932,
        "step": 1779
    },
    {
        "loss": 2.1902,
        "grad_norm": 2.6739468574523926,
        "learning_rate": 0.0001330263248341246,
        "epoch": 0.1326477382815411,
        "step": 1780
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.8718907833099365,
        "learning_rate": 0.00013295989929025407,
        "epoch": 0.13272225948282287,
        "step": 1781
    },
    {
        "loss": 2.2032,
        "grad_norm": 2.0058627128601074,
        "learning_rate": 0.0001328934574268582,
        "epoch": 0.13279678068410464,
        "step": 1782
    },
    {
        "loss": 2.2126,
        "grad_norm": 3.8181240558624268,
        "learning_rate": 0.0001328269992768345,
        "epoch": 0.1328713018853864,
        "step": 1783
    },
    {
        "loss": 2.6898,
        "grad_norm": 2.63671612739563,
        "learning_rate": 0.00013276052487308854,
        "epoch": 0.13294582308666816,
        "step": 1784
    },
    {
        "loss": 2.1467,
        "grad_norm": 2.654721260070801,
        "learning_rate": 0.00013269403424853404,
        "epoch": 0.13302034428794993,
        "step": 1785
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.7492676973342896,
        "learning_rate": 0.00013262752743609267,
        "epoch": 0.1330948654892317,
        "step": 1786
    },
    {
        "loss": 2.5396,
        "grad_norm": 2.976820945739746,
        "learning_rate": 0.00013256100446869408,
        "epoch": 0.13316938669051345,
        "step": 1787
    },
    {
        "loss": 2.5982,
        "grad_norm": 2.5130295753479004,
        "learning_rate": 0.00013249446537927603,
        "epoch": 0.13324390789179522,
        "step": 1788
    },
    {
        "loss": 2.909,
        "grad_norm": 2.8098912239074707,
        "learning_rate": 0.0001324279102007842,
        "epoch": 0.13331842909307698,
        "step": 1789
    },
    {
        "loss": 1.1114,
        "grad_norm": 5.036162853240967,
        "learning_rate": 0.0001323613389661722,
        "epoch": 0.13339295029435874,
        "step": 1790
    },
    {
        "loss": 2.5953,
        "grad_norm": 2.492832660675049,
        "learning_rate": 0.00013229475170840167,
        "epoch": 0.1334674714956405,
        "step": 1791
    },
    {
        "loss": 2.0499,
        "grad_norm": 4.011933326721191,
        "learning_rate": 0.00013222814846044212,
        "epoch": 0.13354199269692227,
        "step": 1792
    },
    {
        "loss": 2.6915,
        "grad_norm": 2.7020764350891113,
        "learning_rate": 0.00013216152925527095,
        "epoch": 0.13361651389820403,
        "step": 1793
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.797159433364868,
        "learning_rate": 0.0001320948941258736,
        "epoch": 0.1336910350994858,
        "step": 1794
    },
    {
        "loss": 2.5573,
        "grad_norm": 3.5254828929901123,
        "learning_rate": 0.00013202824310524323,
        "epoch": 0.13376555630076756,
        "step": 1795
    },
    {
        "loss": 1.6419,
        "grad_norm": 3.671480417251587,
        "learning_rate": 0.000131961576226381,
        "epoch": 0.13384007750204932,
        "step": 1796
    },
    {
        "loss": 2.782,
        "grad_norm": 1.6861047744750977,
        "learning_rate": 0.00013189489352229583,
        "epoch": 0.1339145987033311,
        "step": 1797
    },
    {
        "loss": 2.4738,
        "grad_norm": 2.991896867752075,
        "learning_rate": 0.0001318281950260045,
        "epoch": 0.13398911990461287,
        "step": 1798
    },
    {
        "loss": 2.5486,
        "grad_norm": 1.9683711528778076,
        "learning_rate": 0.0001317614807705317,
        "epoch": 0.13406364110589464,
        "step": 1799
    },
    {
        "loss": 2.1155,
        "grad_norm": 2.6374025344848633,
        "learning_rate": 0.00013169475078890968,
        "epoch": 0.1341381623071764,
        "step": 1800
    },
    {
        "loss": 2.1411,
        "grad_norm": 2.5646796226501465,
        "learning_rate": 0.00013162800511417884,
        "epoch": 0.13421268350845816,
        "step": 1801
    },
    {
        "loss": 2.7895,
        "grad_norm": 2.903973340988159,
        "learning_rate": 0.00013156124377938699,
        "epoch": 0.13428720470973993,
        "step": 1802
    },
    {
        "loss": 1.9589,
        "grad_norm": 2.9422779083251953,
        "learning_rate": 0.0001314944668175899,
        "epoch": 0.1343617259110217,
        "step": 1803
    },
    {
        "loss": 2.5819,
        "grad_norm": 2.385241985321045,
        "learning_rate": 0.00013142767426185112,
        "epoch": 0.13443624711230345,
        "step": 1804
    },
    {
        "loss": 2.0988,
        "grad_norm": 3.216285228729248,
        "learning_rate": 0.00013136086614524167,
        "epoch": 0.13451076831358522,
        "step": 1805
    },
    {
        "loss": 2.4721,
        "grad_norm": 2.265240430831909,
        "learning_rate": 0.00013129404250084058,
        "epoch": 0.13458528951486698,
        "step": 1806
    },
    {
        "loss": 2.098,
        "grad_norm": 2.1494343280792236,
        "learning_rate": 0.00013122720336173434,
        "epoch": 0.13465981071614874,
        "step": 1807
    },
    {
        "loss": 2.553,
        "grad_norm": 1.6910924911499023,
        "learning_rate": 0.0001311603487610172,
        "epoch": 0.1347343319174305,
        "step": 1808
    },
    {
        "loss": 2.2555,
        "grad_norm": 3.2759945392608643,
        "learning_rate": 0.0001310934787317911,
        "epoch": 0.13480885311871227,
        "step": 1809
    },
    {
        "loss": 2.9821,
        "grad_norm": 1.8794292211532593,
        "learning_rate": 0.00013102659330716557,
        "epoch": 0.13488337431999403,
        "step": 1810
    },
    {
        "loss": 2.4496,
        "grad_norm": 2.5189881324768066,
        "learning_rate": 0.00013095969252025776,
        "epoch": 0.1349578955212758,
        "step": 1811
    },
    {
        "loss": 1.4576,
        "grad_norm": 3.1913411617279053,
        "learning_rate": 0.00013089277640419245,
        "epoch": 0.13503241672255756,
        "step": 1812
    },
    {
        "loss": 1.5958,
        "grad_norm": 3.2428104877471924,
        "learning_rate": 0.00013082584499210197,
        "epoch": 0.13510693792383932,
        "step": 1813
    },
    {
        "loss": 1.4733,
        "grad_norm": 3.890740156173706,
        "learning_rate": 0.00013075889831712633,
        "epoch": 0.13518145912512108,
        "step": 1814
    },
    {
        "loss": 2.7907,
        "grad_norm": 2.086014986038208,
        "learning_rate": 0.00013069193641241294,
        "epoch": 0.13525598032640287,
        "step": 1815
    },
    {
        "loss": 2.373,
        "grad_norm": 2.25209903717041,
        "learning_rate": 0.00013062495931111687,
        "epoch": 0.13533050152768464,
        "step": 1816
    },
    {
        "loss": 2.7121,
        "grad_norm": 2.1474809646606445,
        "learning_rate": 0.00013055796704640067,
        "epoch": 0.1354050227289664,
        "step": 1817
    },
    {
        "loss": 2.0956,
        "grad_norm": 3.691767454147339,
        "learning_rate": 0.00013049095965143441,
        "epoch": 0.13547954393024816,
        "step": 1818
    },
    {
        "loss": 2.4067,
        "grad_norm": 4.87809419631958,
        "learning_rate": 0.00013042393715939564,
        "epoch": 0.13555406513152993,
        "step": 1819
    },
    {
        "loss": 2.624,
        "grad_norm": 2.4534902572631836,
        "learning_rate": 0.00013035689960346936,
        "epoch": 0.1356285863328117,
        "step": 1820
    },
    {
        "loss": 2.4264,
        "grad_norm": 3.613553762435913,
        "learning_rate": 0.00013028984701684814,
        "epoch": 0.13570310753409345,
        "step": 1821
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.9769212007522583,
        "learning_rate": 0.0001302227794327318,
        "epoch": 0.13577762873537522,
        "step": 1822
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.0636417865753174,
        "learning_rate": 0.0001301556968843278,
        "epoch": 0.13585214993665698,
        "step": 1823
    },
    {
        "loss": 2.525,
        "grad_norm": 2.6667709350585938,
        "learning_rate": 0.00013008859940485086,
        "epoch": 0.13592667113793874,
        "step": 1824
    },
    {
        "loss": 2.1146,
        "grad_norm": 3.5362887382507324,
        "learning_rate": 0.00013002148702752312,
        "epoch": 0.1360011923392205,
        "step": 1825
    },
    {
        "loss": 2.1932,
        "grad_norm": 3.735410690307617,
        "learning_rate": 0.00012995435978557413,
        "epoch": 0.13607571354050227,
        "step": 1826
    },
    {
        "loss": 2.1728,
        "grad_norm": 2.7436060905456543,
        "learning_rate": 0.0001298872177122408,
        "epoch": 0.13615023474178403,
        "step": 1827
    },
    {
        "loss": 2.3154,
        "grad_norm": 2.160098075866699,
        "learning_rate": 0.00012982006084076737,
        "epoch": 0.1362247559430658,
        "step": 1828
    },
    {
        "loss": 2.4323,
        "grad_norm": 2.2526376247406006,
        "learning_rate": 0.00012975288920440543,
        "epoch": 0.13629927714434756,
        "step": 1829
    },
    {
        "loss": 2.5816,
        "grad_norm": 2.031893253326416,
        "learning_rate": 0.00012968570283641379,
        "epoch": 0.13637379834562932,
        "step": 1830
    },
    {
        "loss": 2.217,
        "grad_norm": 2.842817544937134,
        "learning_rate": 0.00012961850177005863,
        "epoch": 0.13644831954691108,
        "step": 1831
    },
    {
        "loss": 2.8156,
        "grad_norm": 1.7462626695632935,
        "learning_rate": 0.0001295512860386135,
        "epoch": 0.13652284074819285,
        "step": 1832
    },
    {
        "loss": 2.9986,
        "grad_norm": 2.6232168674468994,
        "learning_rate": 0.00012948405567535896,
        "epoch": 0.13659736194947464,
        "step": 1833
    },
    {
        "loss": 2.9069,
        "grad_norm": 1.3425335884094238,
        "learning_rate": 0.00012941681071358305,
        "epoch": 0.1366718831507564,
        "step": 1834
    },
    {
        "loss": 1.1209,
        "grad_norm": 2.1513493061065674,
        "learning_rate": 0.00012934955118658095,
        "epoch": 0.13674640435203816,
        "step": 1835
    },
    {
        "loss": 2.7474,
        "grad_norm": 3.245171308517456,
        "learning_rate": 0.00012928227712765504,
        "epoch": 0.13682092555331993,
        "step": 1836
    },
    {
        "loss": 2.9601,
        "grad_norm": 1.9688218832015991,
        "learning_rate": 0.00012921498857011493,
        "epoch": 0.1368954467546017,
        "step": 1837
    },
    {
        "loss": 1.5998,
        "grad_norm": 3.0751898288726807,
        "learning_rate": 0.00012914768554727734,
        "epoch": 0.13696996795588345,
        "step": 1838
    },
    {
        "loss": 2.4684,
        "grad_norm": 2.9411327838897705,
        "learning_rate": 0.0001290803680924663,
        "epoch": 0.13704448915716522,
        "step": 1839
    },
    {
        "loss": 2.996,
        "grad_norm": 4.788318634033203,
        "learning_rate": 0.00012901303623901272,
        "epoch": 0.13711901035844698,
        "step": 1840
    },
    {
        "loss": 1.4558,
        "grad_norm": 1.1720855236053467,
        "learning_rate": 0.00012894569002025493,
        "epoch": 0.13719353155972874,
        "step": 1841
    },
    {
        "loss": 2.311,
        "grad_norm": 2.6917881965637207,
        "learning_rate": 0.0001288783294695383,
        "epoch": 0.1372680527610105,
        "step": 1842
    },
    {
        "loss": 2.7543,
        "grad_norm": 3.8262407779693604,
        "learning_rate": 0.00012881095462021505,
        "epoch": 0.13734257396229227,
        "step": 1843
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.0497519969940186,
        "learning_rate": 0.00012874356550564485,
        "epoch": 0.13741709516357403,
        "step": 1844
    },
    {
        "loss": 1.7565,
        "grad_norm": 4.119709491729736,
        "learning_rate": 0.0001286761621591942,
        "epoch": 0.1374916163648558,
        "step": 1845
    },
    {
        "loss": 2.2715,
        "grad_norm": 2.2106773853302,
        "learning_rate": 0.0001286087446142367,
        "epoch": 0.13756613756613756,
        "step": 1846
    },
    {
        "loss": 1.9235,
        "grad_norm": 3.6442158222198486,
        "learning_rate": 0.000128541312904153,
        "epoch": 0.13764065876741932,
        "step": 1847
    },
    {
        "loss": 2.192,
        "grad_norm": 3.34683895111084,
        "learning_rate": 0.00012847386706233067,
        "epoch": 0.13771517996870108,
        "step": 1848
    },
    {
        "loss": 2.8792,
        "grad_norm": 2.790717124938965,
        "learning_rate": 0.00012840640712216448,
        "epoch": 0.13778970116998285,
        "step": 1849
    },
    {
        "loss": 1.6226,
        "grad_norm": 2.500713348388672,
        "learning_rate": 0.00012833893311705596,
        "epoch": 0.13786422237126464,
        "step": 1850
    },
    {
        "loss": 1.824,
        "grad_norm": 4.707770824432373,
        "learning_rate": 0.00012827144508041372,
        "epoch": 0.1379387435725464,
        "step": 1851
    },
    {
        "loss": 2.046,
        "grad_norm": 4.276216983795166,
        "learning_rate": 0.00012820394304565333,
        "epoch": 0.13801326477382816,
        "step": 1852
    },
    {
        "loss": 2.1421,
        "grad_norm": 3.139453649520874,
        "learning_rate": 0.00012813642704619725,
        "epoch": 0.13808778597510993,
        "step": 1853
    },
    {
        "loss": 2.7428,
        "grad_norm": 2.093142509460449,
        "learning_rate": 0.00012806889711547482,
        "epoch": 0.1381623071763917,
        "step": 1854
    },
    {
        "loss": 2.5742,
        "grad_norm": 4.329431056976318,
        "learning_rate": 0.00012800135328692235,
        "epoch": 0.13823682837767345,
        "step": 1855
    },
    {
        "loss": 2.6976,
        "grad_norm": 2.3934895992279053,
        "learning_rate": 0.00012793379559398302,
        "epoch": 0.13831134957895522,
        "step": 1856
    },
    {
        "loss": 2.7864,
        "grad_norm": 2.0629422664642334,
        "learning_rate": 0.0001278662240701068,
        "epoch": 0.13838587078023698,
        "step": 1857
    },
    {
        "loss": 1.3316,
        "grad_norm": 4.861928462982178,
        "learning_rate": 0.00012779863874875065,
        "epoch": 0.13846039198151874,
        "step": 1858
    },
    {
        "loss": 1.9359,
        "grad_norm": 4.686794757843018,
        "learning_rate": 0.00012773103966337817,
        "epoch": 0.1385349131828005,
        "step": 1859
    },
    {
        "loss": 2.27,
        "grad_norm": 3.9409685134887695,
        "learning_rate": 0.00012766342684745997,
        "epoch": 0.13860943438408227,
        "step": 1860
    },
    {
        "loss": 2.396,
        "grad_norm": 2.5430362224578857,
        "learning_rate": 0.00012759580033447332,
        "epoch": 0.13868395558536403,
        "step": 1861
    },
    {
        "loss": 2.4988,
        "grad_norm": 2.323199987411499,
        "learning_rate": 0.0001275281601579023,
        "epoch": 0.1387584767866458,
        "step": 1862
    },
    {
        "loss": 3.3103,
        "grad_norm": 2.643737316131592,
        "learning_rate": 0.0001274605063512379,
        "epoch": 0.13883299798792756,
        "step": 1863
    },
    {
        "loss": 1.9367,
        "grad_norm": 3.1127541065216064,
        "learning_rate": 0.00012739283894797757,
        "epoch": 0.13890751918920932,
        "step": 1864
    },
    {
        "loss": 2.5267,
        "grad_norm": 2.164415121078491,
        "learning_rate": 0.00012732515798162577,
        "epoch": 0.13898204039049109,
        "step": 1865
    },
    {
        "loss": 1.6103,
        "grad_norm": 2.6337008476257324,
        "learning_rate": 0.00012725746348569346,
        "epoch": 0.13905656159177285,
        "step": 1866
    },
    {
        "loss": 2.4966,
        "grad_norm": 1.594496488571167,
        "learning_rate": 0.00012718975549369854,
        "epoch": 0.1391310827930546,
        "step": 1867
    },
    {
        "loss": 2.3492,
        "grad_norm": 3.175088882446289,
        "learning_rate": 0.0001271220340391654,
        "epoch": 0.1392056039943364,
        "step": 1868
    },
    {
        "loss": 2.6604,
        "grad_norm": 1.9899005889892578,
        "learning_rate": 0.00012705429915562506,
        "epoch": 0.13928012519561817,
        "step": 1869
    },
    {
        "loss": 2.2309,
        "grad_norm": 3.4150617122650146,
        "learning_rate": 0.0001269865508766154,
        "epoch": 0.13935464639689993,
        "step": 1870
    },
    {
        "loss": 2.691,
        "grad_norm": 2.8806076049804688,
        "learning_rate": 0.00012691878923568072,
        "epoch": 0.1394291675981817,
        "step": 1871
    },
    {
        "loss": 2.5306,
        "grad_norm": 2.6058428287506104,
        "learning_rate": 0.00012685101426637208,
        "epoch": 0.13950368879946345,
        "step": 1872
    },
    {
        "loss": 1.9121,
        "grad_norm": 3.66190767288208,
        "learning_rate": 0.00012678322600224708,
        "epoch": 0.13957821000074522,
        "step": 1873
    },
    {
        "loss": 2.5175,
        "grad_norm": 2.813559055328369,
        "learning_rate": 0.0001267154244768699,
        "epoch": 0.13965273120202698,
        "step": 1874
    },
    {
        "loss": 2.0555,
        "grad_norm": 2.7199151515960693,
        "learning_rate": 0.00012664760972381134,
        "epoch": 0.13972725240330874,
        "step": 1875
    },
    {
        "loss": 3.0531,
        "grad_norm": 2.4123079776763916,
        "learning_rate": 0.0001265797817766486,
        "epoch": 0.1398017736045905,
        "step": 1876
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.2965810298919678,
        "learning_rate": 0.00012651194066896562,
        "epoch": 0.13987629480587227,
        "step": 1877
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.646650791168213,
        "learning_rate": 0.0001264440864343527,
        "epoch": 0.13995081600715403,
        "step": 1878
    },
    {
        "loss": 1.958,
        "grad_norm": 3.4632349014282227,
        "learning_rate": 0.00012637621910640672,
        "epoch": 0.1400253372084358,
        "step": 1879
    },
    {
        "loss": 2.3615,
        "grad_norm": 2.1481194496154785,
        "learning_rate": 0.000126308338718731,
        "epoch": 0.14009985840971756,
        "step": 1880
    },
    {
        "loss": 2.0564,
        "grad_norm": 3.4800972938537598,
        "learning_rate": 0.0001262404453049353,
        "epoch": 0.14017437961099932,
        "step": 1881
    },
    {
        "loss": 2.9527,
        "grad_norm": 2.536020278930664,
        "learning_rate": 0.000126172538898636,
        "epoch": 0.14024890081228109,
        "step": 1882
    },
    {
        "loss": 2.3428,
        "grad_norm": 2.017868995666504,
        "learning_rate": 0.00012610461953345564,
        "epoch": 0.14032342201356285,
        "step": 1883
    },
    {
        "loss": 2.6821,
        "grad_norm": 1.6450990438461304,
        "learning_rate": 0.00012603668724302345,
        "epoch": 0.1403979432148446,
        "step": 1884
    },
    {
        "loss": 2.023,
        "grad_norm": 4.605782508850098,
        "learning_rate": 0.0001259687420609748,
        "epoch": 0.14047246441612637,
        "step": 1885
    },
    {
        "loss": 2.8519,
        "grad_norm": 2.897003173828125,
        "learning_rate": 0.0001259007840209517,
        "epoch": 0.14054698561740817,
        "step": 1886
    },
    {
        "loss": 2.7419,
        "grad_norm": 2.648665189743042,
        "learning_rate": 0.00012583281315660229,
        "epoch": 0.14062150681868993,
        "step": 1887
    },
    {
        "loss": 2.318,
        "grad_norm": 3.310521125793457,
        "learning_rate": 0.00012576482950158124,
        "epoch": 0.1406960280199717,
        "step": 1888
    },
    {
        "loss": 1.9669,
        "grad_norm": 3.129004716873169,
        "learning_rate": 0.0001256968330895495,
        "epoch": 0.14077054922125345,
        "step": 1889
    },
    {
        "loss": 1.9706,
        "grad_norm": 3.561568260192871,
        "learning_rate": 0.00012562882395417426,
        "epoch": 0.14084507042253522,
        "step": 1890
    },
    {
        "loss": 2.5679,
        "grad_norm": 3.4466793537139893,
        "learning_rate": 0.0001255608021291291,
        "epoch": 0.14091959162381698,
        "step": 1891
    },
    {
        "loss": 2.6764,
        "grad_norm": 2.7963507175445557,
        "learning_rate": 0.00012549276764809382,
        "epoch": 0.14099411282509874,
        "step": 1892
    },
    {
        "loss": 2.8413,
        "grad_norm": 3.359304666519165,
        "learning_rate": 0.00012542472054475462,
        "epoch": 0.1410686340263805,
        "step": 1893
    },
    {
        "loss": 2.8286,
        "grad_norm": 2.069093704223633,
        "learning_rate": 0.0001253566608528037,
        "epoch": 0.14114315522766227,
        "step": 1894
    },
    {
        "loss": 2.2459,
        "grad_norm": 2.844633102416992,
        "learning_rate": 0.00012528858860593977,
        "epoch": 0.14121767642894403,
        "step": 1895
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.474073648452759,
        "learning_rate": 0.00012522050383786758,
        "epoch": 0.1412921976302258,
        "step": 1896
    },
    {
        "loss": 2.3463,
        "grad_norm": 2.4338057041168213,
        "learning_rate": 0.00012515240658229809,
        "epoch": 0.14136671883150756,
        "step": 1897
    },
    {
        "loss": 2.434,
        "grad_norm": 3.0760843753814697,
        "learning_rate": 0.00012508429687294856,
        "epoch": 0.14144124003278932,
        "step": 1898
    },
    {
        "loss": 1.237,
        "grad_norm": 2.7066338062286377,
        "learning_rate": 0.0001250161747435423,
        "epoch": 0.14151576123407109,
        "step": 1899
    },
    {
        "loss": 2.4695,
        "grad_norm": 3.0849320888519287,
        "learning_rate": 0.00012494804022780874,
        "epoch": 0.14159028243535285,
        "step": 1900
    },
    {
        "loss": 2.8267,
        "grad_norm": 2.045729160308838,
        "learning_rate": 0.0001248798933594836,
        "epoch": 0.1416648036366346,
        "step": 1901
    },
    {
        "loss": 2.2699,
        "grad_norm": 3.0888564586639404,
        "learning_rate": 0.0001248117341723086,
        "epoch": 0.14173932483791638,
        "step": 1902
    },
    {
        "loss": 2.8565,
        "grad_norm": 1.6271692514419556,
        "learning_rate": 0.0001247435627000316,
        "epoch": 0.14181384603919814,
        "step": 1903
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.0895464420318604,
        "learning_rate": 0.0001246753789764065,
        "epoch": 0.14188836724047993,
        "step": 1904
    },
    {
        "loss": 2.4047,
        "grad_norm": 2.7014033794403076,
        "learning_rate": 0.0001246071830351933,
        "epoch": 0.1419628884417617,
        "step": 1905
    },
    {
        "loss": 2.2282,
        "grad_norm": 2.755429267883301,
        "learning_rate": 0.00012453897491015802,
        "epoch": 0.14203740964304346,
        "step": 1906
    },
    {
        "loss": 2.1842,
        "grad_norm": 3.403585195541382,
        "learning_rate": 0.00012447075463507278,
        "epoch": 0.14211193084432522,
        "step": 1907
    },
    {
        "loss": 2.0665,
        "grad_norm": 2.5254969596862793,
        "learning_rate": 0.0001244025222437157,
        "epoch": 0.14218645204560698,
        "step": 1908
    },
    {
        "loss": 2.468,
        "grad_norm": 2.356574535369873,
        "learning_rate": 0.00012433427776987078,
        "epoch": 0.14226097324688874,
        "step": 1909
    },
    {
        "loss": 2.7705,
        "grad_norm": 3.1553895473480225,
        "learning_rate": 0.0001242660212473282,
        "epoch": 0.1423354944481705,
        "step": 1910
    },
    {
        "loss": 2.3553,
        "grad_norm": 3.3787434101104736,
        "learning_rate": 0.0001241977527098839,
        "epoch": 0.14241001564945227,
        "step": 1911
    },
    {
        "loss": 2.907,
        "grad_norm": 3.4201135635375977,
        "learning_rate": 0.0001241294721913399,
        "epoch": 0.14248453685073403,
        "step": 1912
    },
    {
        "loss": 2.4498,
        "grad_norm": 3.2464678287506104,
        "learning_rate": 0.00012406117972550414,
        "epoch": 0.1425590580520158,
        "step": 1913
    },
    {
        "loss": 2.0771,
        "grad_norm": 4.280351161956787,
        "learning_rate": 0.00012399287534619043,
        "epoch": 0.14263357925329756,
        "step": 1914
    },
    {
        "loss": 2.038,
        "grad_norm": 2.5440239906311035,
        "learning_rate": 0.00012392455908721853,
        "epoch": 0.14270810045457932,
        "step": 1915
    },
    {
        "loss": 2.4147,
        "grad_norm": 4.48037052154541,
        "learning_rate": 0.00012385623098241404,
        "epoch": 0.14278262165586109,
        "step": 1916
    },
    {
        "loss": 2.4048,
        "grad_norm": 3.621945381164551,
        "learning_rate": 0.00012378789106560846,
        "epoch": 0.14285714285714285,
        "step": 1917
    },
    {
        "loss": 2.4964,
        "grad_norm": 2.512939453125,
        "learning_rate": 0.0001237195393706391,
        "epoch": 0.1429316640584246,
        "step": 1918
    },
    {
        "loss": 1.4074,
        "grad_norm": 3.7157633304595947,
        "learning_rate": 0.00012365117593134915,
        "epoch": 0.14300618525970638,
        "step": 1919
    },
    {
        "loss": 1.9925,
        "grad_norm": 3.1554908752441406,
        "learning_rate": 0.00012358280078158753,
        "epoch": 0.14308070646098814,
        "step": 1920
    },
    {
        "loss": 2.6042,
        "grad_norm": 3.0833935737609863,
        "learning_rate": 0.00012351441395520906,
        "epoch": 0.14315522766226993,
        "step": 1921
    },
    {
        "loss": 2.4087,
        "grad_norm": 3.2339518070220947,
        "learning_rate": 0.00012344601548607432,
        "epoch": 0.1432297488635517,
        "step": 1922
    },
    {
        "loss": 2.6292,
        "grad_norm": 2.263000726699829,
        "learning_rate": 0.0001233776054080496,
        "epoch": 0.14330427006483346,
        "step": 1923
    },
    {
        "loss": 2.076,
        "grad_norm": 3.0115578174591064,
        "learning_rate": 0.00012330918375500697,
        "epoch": 0.14337879126611522,
        "step": 1924
    },
    {
        "loss": 2.4894,
        "grad_norm": 2.980346202850342,
        "learning_rate": 0.00012324075056082423,
        "epoch": 0.14345331246739698,
        "step": 1925
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.905891180038452,
        "learning_rate": 0.00012317230585938492,
        "epoch": 0.14352783366867874,
        "step": 1926
    },
    {
        "loss": 2.2972,
        "grad_norm": 3.4857571125030518,
        "learning_rate": 0.00012310384968457815,
        "epoch": 0.1436023548699605,
        "step": 1927
    },
    {
        "loss": 2.048,
        "grad_norm": 3.477606773376465,
        "learning_rate": 0.00012303538207029897,
        "epoch": 0.14367687607124227,
        "step": 1928
    },
    {
        "loss": 2.4377,
        "grad_norm": 4.2376227378845215,
        "learning_rate": 0.00012296690305044786,
        "epoch": 0.14375139727252403,
        "step": 1929
    },
    {
        "loss": 2.3828,
        "grad_norm": 2.8975958824157715,
        "learning_rate": 0.000122898412658931,
        "epoch": 0.1438259184738058,
        "step": 1930
    },
    {
        "loss": 2.2144,
        "grad_norm": 2.4447438716888428,
        "learning_rate": 0.0001228299109296603,
        "epoch": 0.14390043967508756,
        "step": 1931
    },
    {
        "loss": 2.1867,
        "grad_norm": 2.0402534008026123,
        "learning_rate": 0.0001227613978965531,
        "epoch": 0.14397496087636932,
        "step": 1932
    },
    {
        "loss": 2.5642,
        "grad_norm": 3.1852896213531494,
        "learning_rate": 0.00012269287359353254,
        "epoch": 0.1440494820776511,
        "step": 1933
    },
    {
        "loss": 2.3755,
        "grad_norm": 2.1263864040374756,
        "learning_rate": 0.0001226243380545272,
        "epoch": 0.14412400327893285,
        "step": 1934
    },
    {
        "loss": 2.9608,
        "grad_norm": 2.986337423324585,
        "learning_rate": 0.0001225557913134713,
        "epoch": 0.1441985244802146,
        "step": 1935
    },
    {
        "loss": 2.5823,
        "grad_norm": 2.2889485359191895,
        "learning_rate": 0.00012248723340430457,
        "epoch": 0.14427304568149638,
        "step": 1936
    },
    {
        "loss": 1.7588,
        "grad_norm": 2.3222835063934326,
        "learning_rate": 0.0001224186643609722,
        "epoch": 0.14434756688277814,
        "step": 1937
    },
    {
        "loss": 2.1642,
        "grad_norm": 2.998345375061035,
        "learning_rate": 0.00012235008421742513,
        "epoch": 0.1444220880840599,
        "step": 1938
    },
    {
        "loss": 2.1838,
        "grad_norm": 3.42541241645813,
        "learning_rate": 0.00012228149300761947,
        "epoch": 0.1444966092853417,
        "step": 1939
    },
    {
        "loss": 2.3869,
        "grad_norm": 2.597182035446167,
        "learning_rate": 0.00012221289076551705,
        "epoch": 0.14457113048662346,
        "step": 1940
    },
    {
        "loss": 2.4095,
        "grad_norm": 2.6129162311553955,
        "learning_rate": 0.0001221442775250851,
        "epoch": 0.14464565168790522,
        "step": 1941
    },
    {
        "loss": 1.8179,
        "grad_norm": 4.483838081359863,
        "learning_rate": 0.00012207565332029624,
        "epoch": 0.14472017288918698,
        "step": 1942
    },
    {
        "loss": 1.8232,
        "grad_norm": 4.261911869049072,
        "learning_rate": 0.00012200701818512863,
        "epoch": 0.14479469409046875,
        "step": 1943
    },
    {
        "loss": 2.6215,
        "grad_norm": 3.1225314140319824,
        "learning_rate": 0.00012193837215356567,
        "epoch": 0.1448692152917505,
        "step": 1944
    },
    {
        "loss": 2.6338,
        "grad_norm": 2.178734064102173,
        "learning_rate": 0.00012186971525959634,
        "epoch": 0.14494373649303227,
        "step": 1945
    },
    {
        "loss": 2.7074,
        "grad_norm": 3.8163440227508545,
        "learning_rate": 0.00012180104753721491,
        "epoch": 0.14501825769431403,
        "step": 1946
    },
    {
        "loss": 1.653,
        "grad_norm": 2.1826043128967285,
        "learning_rate": 0.00012173236902042096,
        "epoch": 0.1450927788955958,
        "step": 1947
    },
    {
        "loss": 3.0725,
        "grad_norm": 3.895806074142456,
        "learning_rate": 0.00012166367974321948,
        "epoch": 0.14516730009687756,
        "step": 1948
    },
    {
        "loss": 2.324,
        "grad_norm": 2.733320713043213,
        "learning_rate": 0.00012159497973962084,
        "epoch": 0.14524182129815932,
        "step": 1949
    },
    {
        "loss": 2.7129,
        "grad_norm": 2.5468928813934326,
        "learning_rate": 0.00012152626904364067,
        "epoch": 0.1453163424994411,
        "step": 1950
    },
    {
        "loss": 2.0553,
        "grad_norm": 3.2748754024505615,
        "learning_rate": 0.0001214575476892998,
        "epoch": 0.14539086370072285,
        "step": 1951
    },
    {
        "loss": 2.6729,
        "grad_norm": 2.5460448265075684,
        "learning_rate": 0.0001213888157106245,
        "epoch": 0.1454653849020046,
        "step": 1952
    },
    {
        "loss": 2.3735,
        "grad_norm": 2.590423107147217,
        "learning_rate": 0.00012132007314164618,
        "epoch": 0.14553990610328638,
        "step": 1953
    },
    {
        "loss": 2.599,
        "grad_norm": 2.3181326389312744,
        "learning_rate": 0.00012125132001640152,
        "epoch": 0.14561442730456814,
        "step": 1954
    },
    {
        "loss": 2.4085,
        "grad_norm": 3.566798448562622,
        "learning_rate": 0.00012118255636893247,
        "epoch": 0.1456889485058499,
        "step": 1955
    },
    {
        "loss": 1.9642,
        "grad_norm": 3.210681200027466,
        "learning_rate": 0.00012111378223328622,
        "epoch": 0.14576346970713167,
        "step": 1956
    },
    {
        "loss": 2.6493,
        "grad_norm": 3.2372241020202637,
        "learning_rate": 0.00012104499764351503,
        "epoch": 0.14583799090841346,
        "step": 1957
    },
    {
        "loss": 2.645,
        "grad_norm": 3.0927555561065674,
        "learning_rate": 0.00012097620263367637,
        "epoch": 0.14591251210969522,
        "step": 1958
    },
    {
        "loss": 2.6249,
        "grad_norm": 1.9102072715759277,
        "learning_rate": 0.00012090739723783301,
        "epoch": 0.14598703331097698,
        "step": 1959
    },
    {
        "loss": 2.8983,
        "grad_norm": 2.25502872467041,
        "learning_rate": 0.00012083858149005263,
        "epoch": 0.14606155451225875,
        "step": 1960
    },
    {
        "loss": 1.3884,
        "grad_norm": 4.843634128570557,
        "learning_rate": 0.00012076975542440823,
        "epoch": 0.1461360757135405,
        "step": 1961
    },
    {
        "loss": 2.8016,
        "grad_norm": 2.5610570907592773,
        "learning_rate": 0.00012070091907497787,
        "epoch": 0.14621059691482227,
        "step": 1962
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.5094934701919556,
        "learning_rate": 0.00012063207247584463,
        "epoch": 0.14628511811610403,
        "step": 1963
    },
    {
        "loss": 2.4546,
        "grad_norm": 3.0791494846343994,
        "learning_rate": 0.00012056321566109679,
        "epoch": 0.1463596393173858,
        "step": 1964
    },
    {
        "loss": 1.9932,
        "grad_norm": 3.228039026260376,
        "learning_rate": 0.00012049434866482747,
        "epoch": 0.14643416051866756,
        "step": 1965
    },
    {
        "loss": 2.3968,
        "grad_norm": 2.659017324447632,
        "learning_rate": 0.00012042547152113515,
        "epoch": 0.14650868171994932,
        "step": 1966
    },
    {
        "loss": 2.2289,
        "grad_norm": 2.3246710300445557,
        "learning_rate": 0.00012035658426412301,
        "epoch": 0.1465832029212311,
        "step": 1967
    },
    {
        "loss": 2.239,
        "grad_norm": 4.176474094390869,
        "learning_rate": 0.00012028768692789947,
        "epoch": 0.14665772412251285,
        "step": 1968
    },
    {
        "loss": 2.5805,
        "grad_norm": 3.166224718093872,
        "learning_rate": 0.00012021877954657784,
        "epoch": 0.1467322453237946,
        "step": 1969
    },
    {
        "loss": 2.5832,
        "grad_norm": 1.8894054889678955,
        "learning_rate": 0.00012014986215427639,
        "epoch": 0.14680676652507638,
        "step": 1970
    },
    {
        "loss": 2.3978,
        "grad_norm": 2.448484182357788,
        "learning_rate": 0.00012008093478511845,
        "epoch": 0.14688128772635814,
        "step": 1971
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.9877372980117798,
        "learning_rate": 0.00012001199747323216,
        "epoch": 0.1469558089276399,
        "step": 1972
    },
    {
        "loss": 1.62,
        "grad_norm": 4.045199394226074,
        "learning_rate": 0.00011994305025275065,
        "epoch": 0.14703033012892167,
        "step": 1973
    },
    {
        "loss": 2.3519,
        "grad_norm": 2.0678536891937256,
        "learning_rate": 0.00011987409315781195,
        "epoch": 0.14710485133020346,
        "step": 1974
    },
    {
        "loss": 2.1647,
        "grad_norm": 3.3390541076660156,
        "learning_rate": 0.00011980512622255894,
        "epoch": 0.14717937253148522,
        "step": 1975
    },
    {
        "loss": 3.0657,
        "grad_norm": 2.190340757369995,
        "learning_rate": 0.00011973614948113951,
        "epoch": 0.14725389373276698,
        "step": 1976
    },
    {
        "loss": 2.4618,
        "grad_norm": 3.0328221321105957,
        "learning_rate": 0.00011966716296770619,
        "epoch": 0.14732841493404875,
        "step": 1977
    },
    {
        "loss": 2.922,
        "grad_norm": 2.1737215518951416,
        "learning_rate": 0.00011959816671641655,
        "epoch": 0.1474029361353305,
        "step": 1978
    },
    {
        "loss": 2.1801,
        "grad_norm": 3.2272043228149414,
        "learning_rate": 0.00011952916076143284,
        "epoch": 0.14747745733661227,
        "step": 1979
    },
    {
        "loss": 2.5003,
        "grad_norm": 2.5852041244506836,
        "learning_rate": 0.00011946014513692216,
        "epoch": 0.14755197853789404,
        "step": 1980
    },
    {
        "loss": 2.5161,
        "grad_norm": 2.7373435497283936,
        "learning_rate": 0.00011939111987705642,
        "epoch": 0.1476264997391758,
        "step": 1981
    },
    {
        "loss": 2.2744,
        "grad_norm": 3.0035183429718018,
        "learning_rate": 0.0001193220850160123,
        "epoch": 0.14770102094045756,
        "step": 1982
    },
    {
        "loss": 2.4585,
        "grad_norm": 2.4441397190093994,
        "learning_rate": 0.00011925304058797118,
        "epoch": 0.14777554214173932,
        "step": 1983
    },
    {
        "loss": 2.0717,
        "grad_norm": 2.677406072616577,
        "learning_rate": 0.00011918398662711929,
        "epoch": 0.1478500633430211,
        "step": 1984
    },
    {
        "loss": 2.4941,
        "grad_norm": 2.066403388977051,
        "learning_rate": 0.00011911492316764747,
        "epoch": 0.14792458454430285,
        "step": 1985
    },
    {
        "loss": 2.3025,
        "grad_norm": 2.699538230895996,
        "learning_rate": 0.00011904585024375128,
        "epoch": 0.1479991057455846,
        "step": 1986
    },
    {
        "loss": 1.949,
        "grad_norm": 3.3585898876190186,
        "learning_rate": 0.00011897676788963101,
        "epoch": 0.14807362694686638,
        "step": 1987
    },
    {
        "loss": 2.38,
        "grad_norm": 2.1220319271087646,
        "learning_rate": 0.00011890767613949157,
        "epoch": 0.14814814814814814,
        "step": 1988
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.9868199825286865,
        "learning_rate": 0.00011883857502754257,
        "epoch": 0.1482226693494299,
        "step": 1989
    },
    {
        "loss": 2.1712,
        "grad_norm": 3.4749526977539062,
        "learning_rate": 0.00011876946458799824,
        "epoch": 0.14829719055071167,
        "step": 1990
    },
    {
        "loss": 2.3958,
        "grad_norm": 2.6735360622406006,
        "learning_rate": 0.0001187003448550774,
        "epoch": 0.14837171175199343,
        "step": 1991
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.545141577720642,
        "learning_rate": 0.00011863121586300355,
        "epoch": 0.14844623295327522,
        "step": 1992
    },
    {
        "loss": 1.725,
        "grad_norm": 2.9684629440307617,
        "learning_rate": 0.00011856207764600458,
        "epoch": 0.14852075415455698,
        "step": 1993
    },
    {
        "loss": 2.0261,
        "grad_norm": 1.6393907070159912,
        "learning_rate": 0.00011849293023831325,
        "epoch": 0.14859527535583875,
        "step": 1994
    },
    {
        "loss": 2.6353,
        "grad_norm": 2.3088204860687256,
        "learning_rate": 0.00011842377367416658,
        "epoch": 0.1486697965571205,
        "step": 1995
    },
    {
        "loss": 2.2023,
        "grad_norm": 4.036890029907227,
        "learning_rate": 0.00011835460798780631,
        "epoch": 0.14874431775840227,
        "step": 1996
    },
    {
        "loss": 2.4369,
        "grad_norm": 3.6677744388580322,
        "learning_rate": 0.00011828543321347866,
        "epoch": 0.14881883895968404,
        "step": 1997
    },
    {
        "loss": 2.6293,
        "grad_norm": 1.9551531076431274,
        "learning_rate": 0.00011821624938543428,
        "epoch": 0.1488933601609658,
        "step": 1998
    },
    {
        "loss": 2.6047,
        "grad_norm": 2.6369831562042236,
        "learning_rate": 0.00011814705653792837,
        "epoch": 0.14896788136224756,
        "step": 1999
    },
    {
        "loss": 3.1418,
        "grad_norm": 1.8524075746536255,
        "learning_rate": 0.00011807785470522055,
        "epoch": 0.14904240256352932,
        "step": 2000
    },
    {
        "loss": 2.4341,
        "grad_norm": 2.150054693222046,
        "learning_rate": 0.00011800864392157491,
        "epoch": 0.1491169237648111,
        "step": 2001
    },
    {
        "loss": 1.77,
        "grad_norm": 4.2833781242370605,
        "learning_rate": 0.00011793942422126003,
        "epoch": 0.14919144496609285,
        "step": 2002
    },
    {
        "loss": 2.5964,
        "grad_norm": 2.6064670085906982,
        "learning_rate": 0.0001178701956385488,
        "epoch": 0.14926596616737461,
        "step": 2003
    },
    {
        "loss": 1.4558,
        "grad_norm": 4.5150346755981445,
        "learning_rate": 0.00011780095820771856,
        "epoch": 0.14934048736865638,
        "step": 2004
    },
    {
        "loss": 2.05,
        "grad_norm": 2.567858934402466,
        "learning_rate": 0.00011773171196305106,
        "epoch": 0.14941500856993814,
        "step": 2005
    },
    {
        "loss": 2.4488,
        "grad_norm": 2.299400806427002,
        "learning_rate": 0.00011766245693883238,
        "epoch": 0.1494895297712199,
        "step": 2006
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.4244606494903564,
        "learning_rate": 0.0001175931931693529,
        "epoch": 0.14956405097250167,
        "step": 2007
    },
    {
        "loss": 2.366,
        "grad_norm": 2.9778668880462646,
        "learning_rate": 0.0001175239206889074,
        "epoch": 0.14963857217378343,
        "step": 2008
    },
    {
        "loss": 2.5446,
        "grad_norm": 3.245302677154541,
        "learning_rate": 0.00011745463953179501,
        "epoch": 0.1497130933750652,
        "step": 2009
    },
    {
        "loss": 1.7324,
        "grad_norm": 2.8354990482330322,
        "learning_rate": 0.00011738534973231905,
        "epoch": 0.14978761457634698,
        "step": 2010
    },
    {
        "loss": 2.0929,
        "grad_norm": 5.098066329956055,
        "learning_rate": 0.00011731605132478718,
        "epoch": 0.14986213577762875,
        "step": 2011
    },
    {
        "loss": 2.4348,
        "grad_norm": 3.340893030166626,
        "learning_rate": 0.0001172467443435113,
        "epoch": 0.1499366569789105,
        "step": 2012
    },
    {
        "loss": 2.8914,
        "grad_norm": 2.480304002761841,
        "learning_rate": 0.00011717742882280758,
        "epoch": 0.15001117818019227,
        "step": 2013
    },
    {
        "loss": 2.6392,
        "grad_norm": 2.1942028999328613,
        "learning_rate": 0.00011710810479699639,
        "epoch": 0.15008569938147404,
        "step": 2014
    },
    {
        "loss": 2.5938,
        "grad_norm": 2.3388493061065674,
        "learning_rate": 0.0001170387723004023,
        "epoch": 0.1501602205827558,
        "step": 2015
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.663869619369507,
        "learning_rate": 0.00011696943136735416,
        "epoch": 0.15023474178403756,
        "step": 2016
    },
    {
        "loss": 2.6558,
        "grad_norm": 1.9952242374420166,
        "learning_rate": 0.00011690008203218493,
        "epoch": 0.15030926298531933,
        "step": 2017
    },
    {
        "loss": 2.1416,
        "grad_norm": 2.347883462905884,
        "learning_rate": 0.00011683072432923168,
        "epoch": 0.1503837841866011,
        "step": 2018
    },
    {
        "loss": 2.3804,
        "grad_norm": 2.35805082321167,
        "learning_rate": 0.00011676135829283572,
        "epoch": 0.15045830538788285,
        "step": 2019
    },
    {
        "loss": 2.8976,
        "grad_norm": 2.179212808609009,
        "learning_rate": 0.00011669198395734247,
        "epoch": 0.15053282658916461,
        "step": 2020
    },
    {
        "loss": 2.5849,
        "grad_norm": 4.749556064605713,
        "learning_rate": 0.00011662260135710135,
        "epoch": 0.15060734779044638,
        "step": 2021
    },
    {
        "loss": 3.0307,
        "grad_norm": 2.7640950679779053,
        "learning_rate": 0.00011655321052646606,
        "epoch": 0.15068186899172814,
        "step": 2022
    },
    {
        "loss": 2.7113,
        "grad_norm": 2.305560350418091,
        "learning_rate": 0.00011648381149979419,
        "epoch": 0.1507563901930099,
        "step": 2023
    },
    {
        "loss": 2.2519,
        "grad_norm": 1.4239221811294556,
        "learning_rate": 0.0001164144043114475,
        "epoch": 0.15083091139429167,
        "step": 2024
    },
    {
        "loss": 2.6173,
        "grad_norm": 2.1977121829986572,
        "learning_rate": 0.00011634498899579182,
        "epoch": 0.15090543259557343,
        "step": 2025
    },
    {
        "loss": 2.7926,
        "grad_norm": 3.885889768600464,
        "learning_rate": 0.00011627556558719684,
        "epoch": 0.1509799537968552,
        "step": 2026
    },
    {
        "loss": 1.5998,
        "grad_norm": 4.241421222686768,
        "learning_rate": 0.00011620613412003647,
        "epoch": 0.15105447499813696,
        "step": 2027
    },
    {
        "loss": 1.7887,
        "grad_norm": 2.385404586791992,
        "learning_rate": 0.00011613669462868839,
        "epoch": 0.15112899619941875,
        "step": 2028
    },
    {
        "loss": 2.1575,
        "grad_norm": 2.367436170578003,
        "learning_rate": 0.00011606724714753445,
        "epoch": 0.1512035174007005,
        "step": 2029
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.186184883117676,
        "learning_rate": 0.00011599779171096035,
        "epoch": 0.15127803860198227,
        "step": 2030
    },
    {
        "loss": 2.0832,
        "grad_norm": 3.95806884765625,
        "learning_rate": 0.0001159283283533557,
        "epoch": 0.15135255980326404,
        "step": 2031
    },
    {
        "loss": 2.3532,
        "grad_norm": 2.542210578918457,
        "learning_rate": 0.0001158588571091142,
        "epoch": 0.1514270810045458,
        "step": 2032
    },
    {
        "loss": 2.7398,
        "grad_norm": 3.1014060974121094,
        "learning_rate": 0.00011578937801263324,
        "epoch": 0.15150160220582756,
        "step": 2033
    },
    {
        "loss": 2.9145,
        "grad_norm": 2.2680935859680176,
        "learning_rate": 0.0001157198910983142,
        "epoch": 0.15157612340710933,
        "step": 2034
    },
    {
        "loss": 1.5763,
        "grad_norm": 2.857499122619629,
        "learning_rate": 0.00011565039640056239,
        "epoch": 0.1516506446083911,
        "step": 2035
    },
    {
        "loss": 2.0817,
        "grad_norm": 2.7575418949127197,
        "learning_rate": 0.00011558089395378681,
        "epoch": 0.15172516580967285,
        "step": 2036
    },
    {
        "loss": 2.5819,
        "grad_norm": 2.364755630493164,
        "learning_rate": 0.0001155113837924005,
        "epoch": 0.15179968701095461,
        "step": 2037
    },
    {
        "loss": 2.3454,
        "grad_norm": 3.0407025814056396,
        "learning_rate": 0.00011544186595082016,
        "epoch": 0.15187420821223638,
        "step": 2038
    },
    {
        "loss": 1.8142,
        "grad_norm": 3.022653341293335,
        "learning_rate": 0.00011537234046346639,
        "epoch": 0.15194872941351814,
        "step": 2039
    },
    {
        "loss": 2.1987,
        "grad_norm": 3.788787364959717,
        "learning_rate": 0.00011530280736476349,
        "epoch": 0.1520232506147999,
        "step": 2040
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.8048107624053955,
        "learning_rate": 0.0001152332666891396,
        "epoch": 0.15209777181608167,
        "step": 2041
    },
    {
        "loss": 1.4529,
        "grad_norm": 3.614070177078247,
        "learning_rate": 0.00011516371847102656,
        "epoch": 0.15217229301736343,
        "step": 2042
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.842294216156006,
        "learning_rate": 0.00011509416274485998,
        "epoch": 0.1522468142186452,
        "step": 2043
    },
    {
        "loss": 2.5754,
        "grad_norm": 2.4802310466766357,
        "learning_rate": 0.00011502459954507921,
        "epoch": 0.15232133541992696,
        "step": 2044
    },
    {
        "loss": 3.1188,
        "grad_norm": 3.80035400390625,
        "learning_rate": 0.00011495502890612723,
        "epoch": 0.15239585662120875,
        "step": 2045
    },
    {
        "loss": 2.7421,
        "grad_norm": 1.885945200920105,
        "learning_rate": 0.00011488545086245078,
        "epoch": 0.1524703778224905,
        "step": 2046
    },
    {
        "loss": 2.0865,
        "grad_norm": 3.1910555362701416,
        "learning_rate": 0.00011481586544850018,
        "epoch": 0.15254489902377227,
        "step": 2047
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.51714825630188,
        "learning_rate": 0.00011474627269872946,
        "epoch": 0.15261942022505404,
        "step": 2048
    },
    {
        "loss": 2.786,
        "grad_norm": 2.1196560859680176,
        "learning_rate": 0.0001146766726475963,
        "epoch": 0.1526939414263358,
        "step": 2049
    },
    {
        "loss": 2.2638,
        "grad_norm": 2.0765812397003174,
        "learning_rate": 0.00011460706532956196,
        "epoch": 0.15276846262761756,
        "step": 2050
    },
    {
        "loss": 2.6956,
        "grad_norm": 3.6199796199798584,
        "learning_rate": 0.00011453745077909124,
        "epoch": 0.15284298382889933,
        "step": 2051
    },
    {
        "loss": 2.6056,
        "grad_norm": 2.0487053394317627,
        "learning_rate": 0.00011446782903065263,
        "epoch": 0.1529175050301811,
        "step": 2052
    },
    {
        "loss": 1.8876,
        "grad_norm": 3.2199199199676514,
        "learning_rate": 0.0001143982001187182,
        "epoch": 0.15299202623146285,
        "step": 2053
    },
    {
        "loss": 2.3802,
        "grad_norm": 2.6354804039001465,
        "learning_rate": 0.00011432856407776336,
        "epoch": 0.15306654743274462,
        "step": 2054
    },
    {
        "loss": 1.9547,
        "grad_norm": 3.487506628036499,
        "learning_rate": 0.00011425892094226733,
        "epoch": 0.15314106863402638,
        "step": 2055
    },
    {
        "loss": 2.1111,
        "grad_norm": 3.13018798828125,
        "learning_rate": 0.00011418927074671263,
        "epoch": 0.15321558983530814,
        "step": 2056
    },
    {
        "loss": 2.474,
        "grad_norm": 2.741921901702881,
        "learning_rate": 0.00011411961352558538,
        "epoch": 0.1532901110365899,
        "step": 2057
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.2688851356506348,
        "learning_rate": 0.00011404994931337513,
        "epoch": 0.15336463223787167,
        "step": 2058
    },
    {
        "loss": 2.7886,
        "grad_norm": 1.793479084968567,
        "learning_rate": 0.00011398027814457492,
        "epoch": 0.15343915343915343,
        "step": 2059
    },
    {
        "loss": 2.8908,
        "grad_norm": 2.870384931564331,
        "learning_rate": 0.00011391060005368129,
        "epoch": 0.1535136746404352,
        "step": 2060
    },
    {
        "loss": 2.914,
        "grad_norm": 3.553908348083496,
        "learning_rate": 0.00011384091507519403,
        "epoch": 0.15358819584171696,
        "step": 2061
    },
    {
        "loss": 1.5898,
        "grad_norm": 2.0092062950134277,
        "learning_rate": 0.00011377122324361654,
        "epoch": 0.15366271704299872,
        "step": 2062
    },
    {
        "loss": 2.105,
        "grad_norm": 3.2579009532928467,
        "learning_rate": 0.00011370152459345551,
        "epoch": 0.1537372382442805,
        "step": 2063
    },
    {
        "loss": 2.039,
        "grad_norm": 4.125622749328613,
        "learning_rate": 0.00011363181915922096,
        "epoch": 0.15381175944556227,
        "step": 2064
    },
    {
        "loss": 3.0013,
        "grad_norm": 2.0103776454925537,
        "learning_rate": 0.00011356210697542646,
        "epoch": 0.15388628064684404,
        "step": 2065
    },
    {
        "loss": 3.0318,
        "grad_norm": 3.5748496055603027,
        "learning_rate": 0.00011349238807658866,
        "epoch": 0.1539608018481258,
        "step": 2066
    },
    {
        "loss": 2.2863,
        "grad_norm": 2.315324544906616,
        "learning_rate": 0.00011342266249722777,
        "epoch": 0.15403532304940756,
        "step": 2067
    },
    {
        "loss": 2.4527,
        "grad_norm": 2.4842584133148193,
        "learning_rate": 0.00011335293027186717,
        "epoch": 0.15410984425068933,
        "step": 2068
    },
    {
        "loss": 1.0595,
        "grad_norm": 3.082561492919922,
        "learning_rate": 0.00011328319143503355,
        "epoch": 0.1541843654519711,
        "step": 2069
    },
    {
        "loss": 2.5148,
        "grad_norm": 1.7657749652862549,
        "learning_rate": 0.00011321344602125694,
        "epoch": 0.15425888665325285,
        "step": 2070
    },
    {
        "loss": 2.8586,
        "grad_norm": 2.588197946548462,
        "learning_rate": 0.00011314369406507051,
        "epoch": 0.15433340785453462,
        "step": 2071
    },
    {
        "loss": 1.8783,
        "grad_norm": 3.1020443439483643,
        "learning_rate": 0.00011307393560101079,
        "epoch": 0.15440792905581638,
        "step": 2072
    },
    {
        "loss": 2.5866,
        "grad_norm": 2.373103380203247,
        "learning_rate": 0.00011300417066361747,
        "epoch": 0.15448245025709814,
        "step": 2073
    },
    {
        "loss": 2.2881,
        "grad_norm": 2.7906599044799805,
        "learning_rate": 0.00011293439928743345,
        "epoch": 0.1545569714583799,
        "step": 2074
    },
    {
        "loss": 1.7609,
        "grad_norm": 4.126556396484375,
        "learning_rate": 0.0001128646215070048,
        "epoch": 0.15463149265966167,
        "step": 2075
    },
    {
        "loss": 2.1463,
        "grad_norm": 3.4753401279449463,
        "learning_rate": 0.0001127948373568808,
        "epoch": 0.15470601386094343,
        "step": 2076
    },
    {
        "loss": 3.3247,
        "grad_norm": 2.851529359817505,
        "learning_rate": 0.00011272504687161392,
        "epoch": 0.1547805350622252,
        "step": 2077
    },
    {
        "loss": 2.7128,
        "grad_norm": 2.8381521701812744,
        "learning_rate": 0.00011265525008575963,
        "epoch": 0.15485505626350696,
        "step": 2078
    },
    {
        "loss": 3.3352,
        "grad_norm": 2.7222535610198975,
        "learning_rate": 0.00011258544703387668,
        "epoch": 0.15492957746478872,
        "step": 2079
    },
    {
        "loss": 2.4828,
        "grad_norm": 3.3447537422180176,
        "learning_rate": 0.00011251563775052676,
        "epoch": 0.15500409866607048,
        "step": 2080
    },
    {
        "loss": 2.2766,
        "grad_norm": 1.9240940809249878,
        "learning_rate": 0.00011244582227027484,
        "epoch": 0.15507861986735227,
        "step": 2081
    },
    {
        "loss": 1.9002,
        "grad_norm": 2.789135694503784,
        "learning_rate": 0.00011237600062768873,
        "epoch": 0.15515314106863404,
        "step": 2082
    },
    {
        "loss": 2.4167,
        "grad_norm": 3.1336638927459717,
        "learning_rate": 0.0001123061728573395,
        "epoch": 0.1552276622699158,
        "step": 2083
    },
    {
        "loss": 2.5536,
        "grad_norm": 2.542137384414673,
        "learning_rate": 0.00011223633899380114,
        "epoch": 0.15530218347119756,
        "step": 2084
    },
    {
        "loss": 2.565,
        "grad_norm": 2.6609866619110107,
        "learning_rate": 0.00011216649907165069,
        "epoch": 0.15537670467247933,
        "step": 2085
    },
    {
        "loss": 2.6888,
        "grad_norm": 2.3548262119293213,
        "learning_rate": 0.00011209665312546817,
        "epoch": 0.1554512258737611,
        "step": 2086
    },
    {
        "loss": 3.1559,
        "grad_norm": 2.5570685863494873,
        "learning_rate": 0.00011202680118983655,
        "epoch": 0.15552574707504285,
        "step": 2087
    },
    {
        "loss": 1.9044,
        "grad_norm": 3.880493402481079,
        "learning_rate": 0.00011195694329934194,
        "epoch": 0.15560026827632462,
        "step": 2088
    },
    {
        "loss": 1.7988,
        "grad_norm": 2.9265248775482178,
        "learning_rate": 0.00011188707948857313,
        "epoch": 0.15567478947760638,
        "step": 2089
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.6475753784179688,
        "learning_rate": 0.00011181720979212205,
        "epoch": 0.15574931067888814,
        "step": 2090
    },
    {
        "loss": 2.2838,
        "grad_norm": 2.0044803619384766,
        "learning_rate": 0.00011174733424458348,
        "epoch": 0.1558238318801699,
        "step": 2091
    },
    {
        "loss": 3.1575,
        "grad_norm": 2.5077414512634277,
        "learning_rate": 0.00011167745288055505,
        "epoch": 0.15589835308145167,
        "step": 2092
    },
    {
        "loss": 2.2334,
        "grad_norm": 3.219968795776367,
        "learning_rate": 0.00011160756573463735,
        "epoch": 0.15597287428273343,
        "step": 2093
    },
    {
        "loss": 2.2369,
        "grad_norm": 2.800821542739868,
        "learning_rate": 0.00011153767284143377,
        "epoch": 0.1560473954840152,
        "step": 2094
    },
    {
        "loss": 1.8579,
        "grad_norm": 3.3244173526763916,
        "learning_rate": 0.00011146777423555057,
        "epoch": 0.15612191668529696,
        "step": 2095
    },
    {
        "loss": 2.0602,
        "grad_norm": 3.873868465423584,
        "learning_rate": 0.00011139786995159689,
        "epoch": 0.15619643788657872,
        "step": 2096
    },
    {
        "loss": 2.6106,
        "grad_norm": 3.0033881664276123,
        "learning_rate": 0.00011132796002418452,
        "epoch": 0.15627095908786048,
        "step": 2097
    },
    {
        "loss": 2.717,
        "grad_norm": 2.6786561012268066,
        "learning_rate": 0.00011125804448792831,
        "epoch": 0.15634548028914227,
        "step": 2098
    },
    {
        "loss": 2.9153,
        "grad_norm": 2.843125343322754,
        "learning_rate": 0.00011118812337744556,
        "epoch": 0.15642000149042404,
        "step": 2099
    },
    {
        "loss": 2.1358,
        "grad_norm": 2.386695623397827,
        "learning_rate": 0.00011111819672735664,
        "epoch": 0.1564945226917058,
        "step": 2100
    },
    {
        "loss": 2.5256,
        "grad_norm": 1.7592593431472778,
        "learning_rate": 0.00011104826457228446,
        "epoch": 0.15656904389298756,
        "step": 2101
    },
    {
        "loss": 2.4018,
        "grad_norm": 3.642202377319336,
        "learning_rate": 0.00011097832694685469,
        "epoch": 0.15664356509426933,
        "step": 2102
    },
    {
        "loss": 2.3238,
        "grad_norm": 2.916468620300293,
        "learning_rate": 0.00011090838388569584,
        "epoch": 0.1567180862955511,
        "step": 2103
    },
    {
        "loss": 1.8385,
        "grad_norm": 3.299389600753784,
        "learning_rate": 0.00011083843542343891,
        "epoch": 0.15679260749683285,
        "step": 2104
    },
    {
        "loss": 2.0147,
        "grad_norm": 2.5634381771087646,
        "learning_rate": 0.00011076848159471771,
        "epoch": 0.15686712869811462,
        "step": 2105
    },
    {
        "loss": 2.5651,
        "grad_norm": 3.8597636222839355,
        "learning_rate": 0.00011069852243416867,
        "epoch": 0.15694164989939638,
        "step": 2106
    },
    {
        "loss": 2.5095,
        "grad_norm": 3.434075117111206,
        "learning_rate": 0.00011062855797643089,
        "epoch": 0.15701617110067814,
        "step": 2107
    },
    {
        "loss": 2.6009,
        "grad_norm": 3.6529784202575684,
        "learning_rate": 0.000110558588256146,
        "epoch": 0.1570906923019599,
        "step": 2108
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.587509870529175,
        "learning_rate": 0.00011048861330795835,
        "epoch": 0.15716521350324167,
        "step": 2109
    },
    {
        "loss": 2.6777,
        "grad_norm": 2.4573163986206055,
        "learning_rate": 0.00011041863316651482,
        "epoch": 0.15723973470452343,
        "step": 2110
    },
    {
        "loss": 2.0713,
        "grad_norm": 4.233527183532715,
        "learning_rate": 0.00011034864786646488,
        "epoch": 0.1573142559058052,
        "step": 2111
    },
    {
        "loss": 2.5092,
        "grad_norm": 2.1402993202209473,
        "learning_rate": 0.00011027865744246049,
        "epoch": 0.15738877710708696,
        "step": 2112
    },
    {
        "loss": 2.7681,
        "grad_norm": 2.0380139350891113,
        "learning_rate": 0.00011020866192915625,
        "epoch": 0.15746329830836872,
        "step": 2113
    },
    {
        "loss": 1.8717,
        "grad_norm": 1.998613953590393,
        "learning_rate": 0.00011013866136120927,
        "epoch": 0.15753781950965048,
        "step": 2114
    },
    {
        "loss": 2.2256,
        "grad_norm": 2.884739637374878,
        "learning_rate": 0.00011006865577327899,
        "epoch": 0.15761234071093225,
        "step": 2115
    },
    {
        "loss": 2.5215,
        "grad_norm": 1.9006307125091553,
        "learning_rate": 0.0001099986452000276,
        "epoch": 0.15768686191221404,
        "step": 2116
    },
    {
        "loss": 2.6506,
        "grad_norm": 2.23740816116333,
        "learning_rate": 0.00010992862967611959,
        "epoch": 0.1577613831134958,
        "step": 2117
    },
    {
        "loss": 2.1044,
        "grad_norm": 2.725504159927368,
        "learning_rate": 0.00010985860923622189,
        "epoch": 0.15783590431477756,
        "step": 2118
    },
    {
        "loss": 2.6582,
        "grad_norm": 2.460552453994751,
        "learning_rate": 0.000109788583915004,
        "epoch": 0.15791042551605933,
        "step": 2119
    },
    {
        "loss": 2.8863,
        "grad_norm": 3.4079952239990234,
        "learning_rate": 0.00010971855374713764,
        "epoch": 0.1579849467173411,
        "step": 2120
    },
    {
        "loss": 2.0975,
        "grad_norm": 4.820918083190918,
        "learning_rate": 0.00010964851876729714,
        "epoch": 0.15805946791862285,
        "step": 2121
    },
    {
        "loss": 2.1995,
        "grad_norm": 4.9136834144592285,
        "learning_rate": 0.00010957847901015905,
        "epoch": 0.15813398911990462,
        "step": 2122
    },
    {
        "loss": 2.8224,
        "grad_norm": 2.7058215141296387,
        "learning_rate": 0.00010950843451040236,
        "epoch": 0.15820851032118638,
        "step": 2123
    },
    {
        "loss": 2.3741,
        "grad_norm": 2.6516573429107666,
        "learning_rate": 0.00010943838530270844,
        "epoch": 0.15828303152246814,
        "step": 2124
    },
    {
        "loss": 2.392,
        "grad_norm": 2.950788974761963,
        "learning_rate": 0.00010936833142176084,
        "epoch": 0.1583575527237499,
        "step": 2125
    },
    {
        "loss": 1.9954,
        "grad_norm": 3.001783609390259,
        "learning_rate": 0.00010929827290224565,
        "epoch": 0.15843207392503167,
        "step": 2126
    },
    {
        "loss": 2.3609,
        "grad_norm": 2.4946494102478027,
        "learning_rate": 0.00010922820977885106,
        "epoch": 0.15850659512631343,
        "step": 2127
    },
    {
        "loss": 2.2526,
        "grad_norm": 2.5688717365264893,
        "learning_rate": 0.00010915814208626769,
        "epoch": 0.1585811163275952,
        "step": 2128
    },
    {
        "loss": 2.1999,
        "grad_norm": 3.3390145301818848,
        "learning_rate": 0.00010908806985918827,
        "epoch": 0.15865563752887696,
        "step": 2129
    },
    {
        "loss": 2.7174,
        "grad_norm": 2.8424770832061768,
        "learning_rate": 0.00010901799313230785,
        "epoch": 0.15873015873015872,
        "step": 2130
    },
    {
        "loss": 2.533,
        "grad_norm": 2.407871723175049,
        "learning_rate": 0.00010894791194032383,
        "epoch": 0.15880467993144048,
        "step": 2131
    },
    {
        "loss": 2.314,
        "grad_norm": 3.1684725284576416,
        "learning_rate": 0.00010887782631793555,
        "epoch": 0.15887920113272225,
        "step": 2132
    },
    {
        "loss": 1.8137,
        "grad_norm": 3.609372615814209,
        "learning_rate": 0.00010880773629984482,
        "epoch": 0.158953722334004,
        "step": 2133
    },
    {
        "loss": 1.8817,
        "grad_norm": 3.179581880569458,
        "learning_rate": 0.0001087376419207554,
        "epoch": 0.1590282435352858,
        "step": 2134
    },
    {
        "loss": 2.4452,
        "grad_norm": 2.418596029281616,
        "learning_rate": 0.00010866754321537341,
        "epoch": 0.15910276473656756,
        "step": 2135
    },
    {
        "loss": 2.4701,
        "grad_norm": 3.2025017738342285,
        "learning_rate": 0.00010859744021840692,
        "epoch": 0.15917728593784933,
        "step": 2136
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.5605993270874023,
        "learning_rate": 0.00010852733296456628,
        "epoch": 0.1592518071391311,
        "step": 2137
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.0278146266937256,
        "learning_rate": 0.00010845722148856388,
        "epoch": 0.15932632834041285,
        "step": 2138
    },
    {
        "loss": 2.5039,
        "grad_norm": 2.5478124618530273,
        "learning_rate": 0.00010838710582511419,
        "epoch": 0.15940084954169462,
        "step": 2139
    },
    {
        "loss": 1.871,
        "grad_norm": 2.4945454597473145,
        "learning_rate": 0.00010831698600893379,
        "epoch": 0.15947537074297638,
        "step": 2140
    },
    {
        "loss": 2.5973,
        "grad_norm": 2.5532145500183105,
        "learning_rate": 0.00010824686207474126,
        "epoch": 0.15954989194425814,
        "step": 2141
    },
    {
        "loss": 2.5097,
        "grad_norm": 3.61655592918396,
        "learning_rate": 0.0001081767340572573,
        "epoch": 0.1596244131455399,
        "step": 2142
    },
    {
        "loss": 2.3125,
        "grad_norm": 3.5162174701690674,
        "learning_rate": 0.00010810660199120459,
        "epoch": 0.15969893434682167,
        "step": 2143
    },
    {
        "loss": 2.2543,
        "grad_norm": 3.2343873977661133,
        "learning_rate": 0.0001080364659113078,
        "epoch": 0.15977345554810343,
        "step": 2144
    },
    {
        "loss": 3.0449,
        "grad_norm": 1.9858529567718506,
        "learning_rate": 0.00010796632585229361,
        "epoch": 0.1598479767493852,
        "step": 2145
    },
    {
        "loss": 2.4591,
        "grad_norm": 2.55515456199646,
        "learning_rate": 0.00010789618184889059,
        "epoch": 0.15992249795066696,
        "step": 2146
    },
    {
        "loss": 1.8859,
        "grad_norm": 3.7993760108947754,
        "learning_rate": 0.00010782603393582941,
        "epoch": 0.15999701915194872,
        "step": 2147
    },
    {
        "loss": 2.6256,
        "grad_norm": 1.9262633323669434,
        "learning_rate": 0.00010775588214784254,
        "epoch": 0.16007154035323048,
        "step": 2148
    },
    {
        "loss": 2.7135,
        "grad_norm": 2.446998357772827,
        "learning_rate": 0.00010768572651966451,
        "epoch": 0.16014606155451225,
        "step": 2149
    },
    {
        "loss": 2.5194,
        "grad_norm": 2.8649685382843018,
        "learning_rate": 0.0001076155670860316,
        "epoch": 0.160220582755794,
        "step": 2150
    },
    {
        "loss": 2.3083,
        "grad_norm": 2.1251742839813232,
        "learning_rate": 0.00010754540388168206,
        "epoch": 0.1602951039570758,
        "step": 2151
    },
    {
        "loss": 2.5127,
        "grad_norm": 2.491457462310791,
        "learning_rate": 0.00010747523694135605,
        "epoch": 0.16036962515835756,
        "step": 2152
    },
    {
        "loss": 2.6679,
        "grad_norm": 2.607848882675171,
        "learning_rate": 0.00010740506629979537,
        "epoch": 0.16044414635963933,
        "step": 2153
    },
    {
        "loss": 1.5357,
        "grad_norm": 4.551296710968018,
        "learning_rate": 0.00010733489199174396,
        "epoch": 0.1605186675609211,
        "step": 2154
    },
    {
        "loss": 2.7899,
        "grad_norm": 2.768965244293213,
        "learning_rate": 0.00010726471405194731,
        "epoch": 0.16059318876220285,
        "step": 2155
    },
    {
        "loss": 2.5496,
        "grad_norm": 2.3947396278381348,
        "learning_rate": 0.00010719453251515288,
        "epoch": 0.16066770996348462,
        "step": 2156
    },
    {
        "loss": 2.6004,
        "grad_norm": 3.1367409229278564,
        "learning_rate": 0.00010712434741610986,
        "epoch": 0.16074223116476638,
        "step": 2157
    },
    {
        "loss": 2.8806,
        "grad_norm": 2.061774969100952,
        "learning_rate": 0.00010705415878956908,
        "epoch": 0.16081675236604814,
        "step": 2158
    },
    {
        "loss": 2.8078,
        "grad_norm": 2.994894504547119,
        "learning_rate": 0.00010698396667028339,
        "epoch": 0.1608912735673299,
        "step": 2159
    },
    {
        "loss": 2.7167,
        "grad_norm": 2.160504102706909,
        "learning_rate": 0.00010691377109300707,
        "epoch": 0.16096579476861167,
        "step": 2160
    },
    {
        "loss": 1.6115,
        "grad_norm": 2.2358391284942627,
        "learning_rate": 0.0001068435720924963,
        "epoch": 0.16104031596989343,
        "step": 2161
    },
    {
        "loss": 2.1082,
        "grad_norm": 3.234358310699463,
        "learning_rate": 0.00010677336970350891,
        "epoch": 0.1611148371711752,
        "step": 2162
    },
    {
        "loss": 2.1855,
        "grad_norm": 2.3659300804138184,
        "learning_rate": 0.00010670316396080438,
        "epoch": 0.16118935837245696,
        "step": 2163
    },
    {
        "loss": 2.1165,
        "grad_norm": 3.1277756690979004,
        "learning_rate": 0.00010663295489914393,
        "epoch": 0.16126387957373872,
        "step": 2164
    },
    {
        "loss": 2.4989,
        "grad_norm": 1.9296514987945557,
        "learning_rate": 0.00010656274255329029,
        "epoch": 0.16133840077502049,
        "step": 2165
    },
    {
        "loss": 2.678,
        "grad_norm": 3.6791629791259766,
        "learning_rate": 0.00010649252695800793,
        "epoch": 0.16141292197630225,
        "step": 2166
    },
    {
        "loss": 2.2378,
        "grad_norm": 3.1166186332702637,
        "learning_rate": 0.00010642230814806288,
        "epoch": 0.161487443177584,
        "step": 2167
    },
    {
        "loss": 2.832,
        "grad_norm": 2.5467798709869385,
        "learning_rate": 0.00010635208615822277,
        "epoch": 0.16156196437886577,
        "step": 2168
    },
    {
        "loss": 2.5207,
        "grad_norm": 2.5625181198120117,
        "learning_rate": 0.00010628186102325681,
        "epoch": 0.16163648558014757,
        "step": 2169
    },
    {
        "loss": 2.3419,
        "grad_norm": 2.9080841541290283,
        "learning_rate": 0.00010621163277793575,
        "epoch": 0.16171100678142933,
        "step": 2170
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.6159017086029053,
        "learning_rate": 0.00010614140145703195,
        "epoch": 0.1617855279827111,
        "step": 2171
    },
    {
        "loss": 2.4811,
        "grad_norm": 2.605344772338867,
        "learning_rate": 0.00010607116709531918,
        "epoch": 0.16186004918399285,
        "step": 2172
    },
    {
        "loss": 2.4779,
        "grad_norm": 1.6775940656661987,
        "learning_rate": 0.0001060009297275728,
        "epoch": 0.16193457038527462,
        "step": 2173
    },
    {
        "loss": 2.9196,
        "grad_norm": 2.2229857444763184,
        "learning_rate": 0.00010593068938856961,
        "epoch": 0.16200909158655638,
        "step": 2174
    },
    {
        "loss": 2.6502,
        "grad_norm": 2.309361219406128,
        "learning_rate": 0.00010586044611308792,
        "epoch": 0.16208361278783814,
        "step": 2175
    },
    {
        "loss": 2.9406,
        "grad_norm": 1.9796531200408936,
        "learning_rate": 0.00010579019993590742,
        "epoch": 0.1621581339891199,
        "step": 2176
    },
    {
        "loss": 2.7563,
        "grad_norm": 3.291290521621704,
        "learning_rate": 0.00010571995089180937,
        "epoch": 0.16223265519040167,
        "step": 2177
    },
    {
        "loss": 2.8691,
        "grad_norm": 1.8977783918380737,
        "learning_rate": 0.00010564969901557637,
        "epoch": 0.16230717639168343,
        "step": 2178
    },
    {
        "loss": 2.4096,
        "grad_norm": 3.189422369003296,
        "learning_rate": 0.00010557944434199236,
        "epoch": 0.1623816975929652,
        "step": 2179
    },
    {
        "loss": 2.7065,
        "grad_norm": 3.2369604110717773,
        "learning_rate": 0.00010550918690584277,
        "epoch": 0.16245621879424696,
        "step": 2180
    },
    {
        "loss": 2.2338,
        "grad_norm": 2.557438850402832,
        "learning_rate": 0.00010543892674191436,
        "epoch": 0.16253073999552872,
        "step": 2181
    },
    {
        "loss": 2.3219,
        "grad_norm": 1.9975180625915527,
        "learning_rate": 0.00010536866388499522,
        "epoch": 0.16260526119681049,
        "step": 2182
    },
    {
        "loss": 2.133,
        "grad_norm": 3.0124471187591553,
        "learning_rate": 0.0001052983983698748,
        "epoch": 0.16267978239809225,
        "step": 2183
    },
    {
        "loss": 2.8493,
        "grad_norm": 3.122805595397949,
        "learning_rate": 0.00010522813023134386,
        "epoch": 0.162754303599374,
        "step": 2184
    },
    {
        "loss": 2.858,
        "grad_norm": 3.4823007583618164,
        "learning_rate": 0.00010515785950419448,
        "epoch": 0.16282882480065577,
        "step": 2185
    },
    {
        "loss": 3.0739,
        "grad_norm": 1.5807145833969116,
        "learning_rate": 0.00010508758622321993,
        "epoch": 0.16290334600193754,
        "step": 2186
    },
    {
        "loss": 2.5185,
        "grad_norm": 2.894657611846924,
        "learning_rate": 0.0001050173104232149,
        "epoch": 0.16297786720321933,
        "step": 2187
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.034322738647461,
        "learning_rate": 0.00010494703213897514,
        "epoch": 0.1630523884045011,
        "step": 2188
    },
    {
        "loss": 2.6795,
        "grad_norm": 2.7672674655914307,
        "learning_rate": 0.0001048767514052978,
        "epoch": 0.16312690960578285,
        "step": 2189
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.803048610687256,
        "learning_rate": 0.00010480646825698118,
        "epoch": 0.16320143080706462,
        "step": 2190
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.8374855518341064,
        "learning_rate": 0.0001047361827288247,
        "epoch": 0.16327595200834638,
        "step": 2191
    },
    {
        "loss": 2.6798,
        "grad_norm": 3.8203961849212646,
        "learning_rate": 0.00010466589485562915,
        "epoch": 0.16335047320962814,
        "step": 2192
    },
    {
        "loss": 2.5497,
        "grad_norm": 3.830563545227051,
        "learning_rate": 0.0001045956046721962,
        "epoch": 0.1634249944109099,
        "step": 2193
    },
    {
        "loss": 2.6222,
        "grad_norm": 3.1583166122436523,
        "learning_rate": 0.00010452531221332895,
        "epoch": 0.16349951561219167,
        "step": 2194
    },
    {
        "loss": 2.798,
        "grad_norm": 2.9789252281188965,
        "learning_rate": 0.0001044550175138314,
        "epoch": 0.16357403681347343,
        "step": 2195
    },
    {
        "loss": 2.119,
        "grad_norm": 3.299339771270752,
        "learning_rate": 0.00010438472060850884,
        "epoch": 0.1636485580147552,
        "step": 2196
    },
    {
        "loss": 1.8181,
        "grad_norm": 2.8912930488586426,
        "learning_rate": 0.00010431442153216753,
        "epoch": 0.16372307921603696,
        "step": 2197
    },
    {
        "loss": 2.6063,
        "grad_norm": 2.769498109817505,
        "learning_rate": 0.00010424412031961484,
        "epoch": 0.16379760041731872,
        "step": 2198
    },
    {
        "loss": 2.25,
        "grad_norm": 2.6541495323181152,
        "learning_rate": 0.00010417381700565922,
        "epoch": 0.16387212161860049,
        "step": 2199
    },
    {
        "loss": 2.7694,
        "grad_norm": 4.406632423400879,
        "learning_rate": 0.00010410351162511015,
        "epoch": 0.16394664281988225,
        "step": 2200
    },
    {
        "loss": 2.2337,
        "grad_norm": 2.8036341667175293,
        "learning_rate": 0.00010403320421277809,
        "epoch": 0.164021164021164,
        "step": 2201
    },
    {
        "loss": 2.2108,
        "grad_norm": 2.2563586235046387,
        "learning_rate": 0.00010396289480347454,
        "epoch": 0.16409568522244578,
        "step": 2202
    },
    {
        "loss": 2.5993,
        "grad_norm": 2.3832931518554688,
        "learning_rate": 0.00010389258343201205,
        "epoch": 0.16417020642372754,
        "step": 2203
    },
    {
        "loss": 2.5549,
        "grad_norm": 3.0484201908111572,
        "learning_rate": 0.000103822270133204,
        "epoch": 0.1642447276250093,
        "step": 2204
    },
    {
        "loss": 2.1755,
        "grad_norm": 2.8391220569610596,
        "learning_rate": 0.0001037519549418649,
        "epoch": 0.1643192488262911,
        "step": 2205
    },
    {
        "loss": 2.4984,
        "grad_norm": 3.031144857406616,
        "learning_rate": 0.00010368163789281005,
        "epoch": 0.16439377002757286,
        "step": 2206
    },
    {
        "loss": 2.4391,
        "grad_norm": 3.359788417816162,
        "learning_rate": 0.0001036113190208557,
        "epoch": 0.16446829122885462,
        "step": 2207
    },
    {
        "loss": 2.48,
        "grad_norm": 1.927021861076355,
        "learning_rate": 0.00010354099836081909,
        "epoch": 0.16454281243013638,
        "step": 2208
    },
    {
        "loss": 3.0005,
        "grad_norm": Infinity,
        "learning_rate": 0.00010354099836081909,
        "epoch": 0.16461733363141814,
        "step": 2209
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.483494281768799,
        "learning_rate": 0.0001034706759475182,
        "epoch": 0.1646918548326999,
        "step": 2210
    },
    {
        "loss": 2.5879,
        "grad_norm": 3.5404655933380127,
        "learning_rate": 0.00010340035181577204,
        "epoch": 0.16476637603398167,
        "step": 2211
    },
    {
        "loss": 2.5449,
        "grad_norm": 2.043034553527832,
        "learning_rate": 0.00010333002600040036,
        "epoch": 0.16484089723526343,
        "step": 2212
    },
    {
        "loss": 2.6064,
        "grad_norm": 4.4278950691223145,
        "learning_rate": 0.00010325969853622375,
        "epoch": 0.1649154184365452,
        "step": 2213
    },
    {
        "loss": 2.4231,
        "grad_norm": 2.155700445175171,
        "learning_rate": 0.0001031893694580637,
        "epoch": 0.16498993963782696,
        "step": 2214
    },
    {
        "loss": 2.0694,
        "grad_norm": 2.6198596954345703,
        "learning_rate": 0.00010311903880074232,
        "epoch": 0.16506446083910872,
        "step": 2215
    },
    {
        "loss": 2.6197,
        "grad_norm": 2.75140118598938,
        "learning_rate": 0.00010304870659908277,
        "epoch": 0.1651389820403905,
        "step": 2216
    },
    {
        "loss": 2.4946,
        "grad_norm": 1.6468238830566406,
        "learning_rate": 0.00010297837288790873,
        "epoch": 0.16521350324167225,
        "step": 2217
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.49106764793396,
        "learning_rate": 0.00010290803770204473,
        "epoch": 0.165288024442954,
        "step": 2218
    },
    {
        "loss": 2.466,
        "grad_norm": 2.6488003730773926,
        "learning_rate": 0.00010283770107631609,
        "epoch": 0.16536254564423578,
        "step": 2219
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.1503684520721436,
        "learning_rate": 0.00010276736304554868,
        "epoch": 0.16543706684551754,
        "step": 2220
    },
    {
        "loss": 2.0685,
        "grad_norm": 3.1954057216644287,
        "learning_rate": 0.00010269702364456928,
        "epoch": 0.1655115880467993,
        "step": 2221
    },
    {
        "loss": 2.6453,
        "grad_norm": 2.4604220390319824,
        "learning_rate": 0.00010262668290820511,
        "epoch": 0.1655861092480811,
        "step": 2222
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.221550464630127,
        "learning_rate": 0.00010255634087128425,
        "epoch": 0.16566063044936286,
        "step": 2223
    },
    {
        "loss": 2.4535,
        "grad_norm": 2.089607000350952,
        "learning_rate": 0.0001024859975686353,
        "epoch": 0.16573515165064462,
        "step": 2224
    },
    {
        "loss": 2.7003,
        "grad_norm": 3.463376760482788,
        "learning_rate": 0.00010241565303508755,
        "epoch": 0.16580967285192638,
        "step": 2225
    },
    {
        "loss": 2.499,
        "grad_norm": 3.424586057662964,
        "learning_rate": 0.00010234530730547092,
        "epoch": 0.16588419405320814,
        "step": 2226
    },
    {
        "loss": 2.6867,
        "grad_norm": 2.0441977977752686,
        "learning_rate": 0.00010227496041461581,
        "epoch": 0.1659587152544899,
        "step": 2227
    },
    {
        "loss": 2.3448,
        "grad_norm": 2.3748691082000732,
        "learning_rate": 0.00010220461239735335,
        "epoch": 0.16603323645577167,
        "step": 2228
    },
    {
        "loss": 2.5781,
        "grad_norm": 2.2547264099121094,
        "learning_rate": 0.00010213426328851508,
        "epoch": 0.16610775765705343,
        "step": 2229
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.3992526531219482,
        "learning_rate": 0.00010206391312293316,
        "epoch": 0.1661822788583352,
        "step": 2230
    },
    {
        "loss": 2.7359,
        "grad_norm": 3.0697720050811768,
        "learning_rate": 0.00010199356193544028,
        "epoch": 0.16625680005961696,
        "step": 2231
    },
    {
        "loss": 2.1159,
        "grad_norm": 2.580766201019287,
        "learning_rate": 0.0001019232097608696,
        "epoch": 0.16633132126089872,
        "step": 2232
    },
    {
        "loss": 1.5784,
        "grad_norm": 3.9026196002960205,
        "learning_rate": 0.00010185285663405482,
        "epoch": 0.1664058424621805,
        "step": 2233
    },
    {
        "loss": 2.6559,
        "grad_norm": 2.8162426948547363,
        "learning_rate": 0.00010178250258983005,
        "epoch": 0.16648036366346225,
        "step": 2234
    },
    {
        "loss": 2.1458,
        "grad_norm": 4.219244956970215,
        "learning_rate": 0.00010171214766302988,
        "epoch": 0.166554884864744,
        "step": 2235
    },
    {
        "loss": 2.639,
        "grad_norm": 2.6563546657562256,
        "learning_rate": 0.0001016417918884893,
        "epoch": 0.16662940606602578,
        "step": 2236
    },
    {
        "loss": 2.4153,
        "grad_norm": 2.9051668643951416,
        "learning_rate": 0.00010157143530104382,
        "epoch": 0.16670392726730754,
        "step": 2237
    },
    {
        "loss": 2.5076,
        "grad_norm": 2.996685266494751,
        "learning_rate": 0.0001015010779355293,
        "epoch": 0.1667784484685893,
        "step": 2238
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.948122501373291,
        "learning_rate": 0.00010143071982678189,
        "epoch": 0.16685296966987107,
        "step": 2239
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.587226629257202,
        "learning_rate": 0.0001013603610096383,
        "epoch": 0.16692749087115286,
        "step": 2240
    },
    {
        "loss": 2.5318,
        "grad_norm": 2.234684705734253,
        "learning_rate": 0.0001012900015189354,
        "epoch": 0.16700201207243462,
        "step": 2241
    },
    {
        "loss": 2.7659,
        "grad_norm": 2.096137762069702,
        "learning_rate": 0.00010121964138951055,
        "epoch": 0.16707653327371638,
        "step": 2242
    },
    {
        "loss": 2.654,
        "grad_norm": 2.5180280208587646,
        "learning_rate": 0.00010114928065620124,
        "epoch": 0.16715105447499815,
        "step": 2243
    },
    {
        "loss": 2.4129,
        "grad_norm": 2.591127634048462,
        "learning_rate": 0.0001010789193538455,
        "epoch": 0.1672255756762799,
        "step": 2244
    },
    {
        "loss": 2.3018,
        "grad_norm": 3.13187837600708,
        "learning_rate": 0.00010100855751728142,
        "epoch": 0.16730009687756167,
        "step": 2245
    },
    {
        "loss": 2.1572,
        "grad_norm": 3.187502384185791,
        "learning_rate": 0.0001009381951813475,
        "epoch": 0.16737461807884343,
        "step": 2246
    },
    {
        "loss": 1.9784,
        "grad_norm": 3.1925604343414307,
        "learning_rate": 0.00010086783238088246,
        "epoch": 0.1674491392801252,
        "step": 2247
    },
    {
        "loss": 2.9197,
        "grad_norm": 2.7546026706695557,
        "learning_rate": 0.00010079746915072515,
        "epoch": 0.16752366048140696,
        "step": 2248
    },
    {
        "loss": 3.0528,
        "grad_norm": 1.8481501340866089,
        "learning_rate": 0.00010072710552571481,
        "epoch": 0.16759818168268872,
        "step": 2249
    },
    {
        "loss": 1.2759,
        "grad_norm": 2.7213051319122314,
        "learning_rate": 0.00010065674154069067,
        "epoch": 0.1676727028839705,
        "step": 2250
    },
    {
        "loss": 2.8292,
        "grad_norm": 2.848911762237549,
        "learning_rate": 0.00010058637723049227,
        "epoch": 0.16774722408525225,
        "step": 2251
    },
    {
        "loss": 1.3908,
        "grad_norm": 4.260092735290527,
        "learning_rate": 0.00010051601262995936,
        "epoch": 0.167821745286534,
        "step": 2252
    },
    {
        "loss": 2.3677,
        "grad_norm": 2.2442455291748047,
        "learning_rate": 0.00010044564777393161,
        "epoch": 0.16789626648781578,
        "step": 2253
    },
    {
        "loss": 2.0595,
        "grad_norm": 3.1951544284820557,
        "learning_rate": 0.00010037528269724917,
        "epoch": 0.16797078768909754,
        "step": 2254
    },
    {
        "loss": 2.7167,
        "grad_norm": 2.8120648860931396,
        "learning_rate": 0.0001003049174347519,
        "epoch": 0.1680453088903793,
        "step": 2255
    },
    {
        "loss": 2.6363,
        "grad_norm": 1.9195581674575806,
        "learning_rate": 0.00010023455202128008,
        "epoch": 0.16811983009166107,
        "step": 2256
    },
    {
        "loss": 2.7104,
        "grad_norm": 2.3461077213287354,
        "learning_rate": 0.00010016418649167383,
        "epoch": 0.16819435129294283,
        "step": 2257
    },
    {
        "loss": 2.4291,
        "grad_norm": 2.5175435543060303,
        "learning_rate": 0.00010009382088077346,
        "epoch": 0.16826887249422462,
        "step": 2258
    },
    {
        "loss": 2.2089,
        "grad_norm": 3.0355446338653564,
        "learning_rate": 0.00010002345522341933,
        "epoch": 0.16834339369550638,
        "step": 2259
    },
    {
        "loss": 2.8874,
        "grad_norm": 3.152676582336426,
        "learning_rate": 9.995308955445173e-05,
        "epoch": 0.16841791489678815,
        "step": 2260
    },
    {
        "loss": 2.1241,
        "grad_norm": 3.099609375,
        "learning_rate": 9.988272390871106e-05,
        "epoch": 0.1684924360980699,
        "step": 2261
    },
    {
        "loss": 2.5659,
        "grad_norm": 3.050320863723755,
        "learning_rate": 9.981235832103761e-05,
        "epoch": 0.16856695729935167,
        "step": 2262
    },
    {
        "loss": 3.5252,
        "grad_norm": 3.1910805702209473,
        "learning_rate": 9.974199282627171e-05,
        "epoch": 0.16864147850063343,
        "step": 2263
    },
    {
        "loss": 2.1587,
        "grad_norm": 2.964590549468994,
        "learning_rate": 9.967162745925356e-05,
        "epoch": 0.1687159997019152,
        "step": 2264
    },
    {
        "loss": 2.5488,
        "grad_norm": 2.0213119983673096,
        "learning_rate": 9.960126225482345e-05,
        "epoch": 0.16879052090319696,
        "step": 2265
    },
    {
        "loss": 2.765,
        "grad_norm": 2.370793581008911,
        "learning_rate": 9.953089724782143e-05,
        "epoch": 0.16886504210447872,
        "step": 2266
    },
    {
        "loss": 2.5683,
        "grad_norm": 3.2594826221466064,
        "learning_rate": 9.946053247308758e-05,
        "epoch": 0.1689395633057605,
        "step": 2267
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.6470465660095215,
        "learning_rate": 9.939016796546174e-05,
        "epoch": 0.16901408450704225,
        "step": 2268
    },
    {
        "loss": 2.7967,
        "grad_norm": 4.03353214263916,
        "learning_rate": 9.931980375978368e-05,
        "epoch": 0.169088605708324,
        "step": 2269
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.9710960388183594,
        "learning_rate": 9.924943989089315e-05,
        "epoch": 0.16916312690960578,
        "step": 2270
    },
    {
        "loss": 2.2358,
        "grad_norm": 3.986374855041504,
        "learning_rate": 9.91790763936294e-05,
        "epoch": 0.16923764811088754,
        "step": 2271
    },
    {
        "loss": 2.103,
        "grad_norm": 3.5728251934051514,
        "learning_rate": 9.910871330283185e-05,
        "epoch": 0.1693121693121693,
        "step": 2272
    },
    {
        "loss": 2.2274,
        "grad_norm": 2.4258975982666016,
        "learning_rate": 9.903835065333952e-05,
        "epoch": 0.16938669051345107,
        "step": 2273
    },
    {
        "loss": 1.4224,
        "grad_norm": 2.593808650970459,
        "learning_rate": 9.896798847999125e-05,
        "epoch": 0.16946121171473283,
        "step": 2274
    },
    {
        "loss": 2.9691,
        "grad_norm": 2.463282823562622,
        "learning_rate": 9.889762681762576e-05,
        "epoch": 0.16953573291601462,
        "step": 2275
    },
    {
        "loss": 1.5131,
        "grad_norm": 3.0324811935424805,
        "learning_rate": 9.882726570108123e-05,
        "epoch": 0.16961025411729638,
        "step": 2276
    },
    {
        "loss": 2.4354,
        "grad_norm": 3.9054408073425293,
        "learning_rate": 9.875690516519592e-05,
        "epoch": 0.16968477531857815,
        "step": 2277
    },
    {
        "loss": 1.8015,
        "grad_norm": 4.054358005523682,
        "learning_rate": 9.868654524480753e-05,
        "epoch": 0.1697592965198599,
        "step": 2278
    },
    {
        "loss": 2.408,
        "grad_norm": 3.066197156906128,
        "learning_rate": 9.861618597475356e-05,
        "epoch": 0.16983381772114167,
        "step": 2279
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.19878089427948,
        "learning_rate": 9.854582738987131e-05,
        "epoch": 0.16990833892242344,
        "step": 2280
    },
    {
        "loss": 2.442,
        "grad_norm": 3.4915947914123535,
        "learning_rate": 9.847546952499745e-05,
        "epoch": 0.1699828601237052,
        "step": 2281
    },
    {
        "loss": 2.8214,
        "grad_norm": 1.7761292457580566,
        "learning_rate": 9.840511241496864e-05,
        "epoch": 0.17005738132498696,
        "step": 2282
    },
    {
        "loss": 2.8036,
        "grad_norm": 2.368382215499878,
        "learning_rate": 9.833475609462082e-05,
        "epoch": 0.17013190252626872,
        "step": 2283
    },
    {
        "loss": 2.3148,
        "grad_norm": 2.975069999694824,
        "learning_rate": 9.826440059878977e-05,
        "epoch": 0.1702064237275505,
        "step": 2284
    },
    {
        "loss": 2.5498,
        "grad_norm": 2.854130744934082,
        "learning_rate": 9.819404596231089e-05,
        "epoch": 0.17028094492883225,
        "step": 2285
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.4743499755859375,
        "learning_rate": 9.812369222001893e-05,
        "epoch": 0.170355466130114,
        "step": 2286
    },
    {
        "loss": 2.4159,
        "grad_norm": 2.6651406288146973,
        "learning_rate": 9.805333940674845e-05,
        "epoch": 0.17042998733139578,
        "step": 2287
    },
    {
        "loss": 2.847,
        "grad_norm": 1.991982102394104,
        "learning_rate": 9.79829875573333e-05,
        "epoch": 0.17050450853267754,
        "step": 2288
    },
    {
        "loss": 2.4883,
        "grad_norm": 3.133202314376831,
        "learning_rate": 9.791263670660708e-05,
        "epoch": 0.1705790297339593,
        "step": 2289
    },
    {
        "loss": 2.6548,
        "grad_norm": 2.3020410537719727,
        "learning_rate": 9.784228688940281e-05,
        "epoch": 0.17065355093524107,
        "step": 2290
    },
    {
        "loss": 1.8437,
        "grad_norm": 4.142044544219971,
        "learning_rate": 9.777193814055288e-05,
        "epoch": 0.17072807213652283,
        "step": 2291
    },
    {
        "loss": 2.5734,
        "grad_norm": 2.8575079441070557,
        "learning_rate": 9.77015904948894e-05,
        "epoch": 0.1708025933378046,
        "step": 2292
    },
    {
        "loss": 2.5777,
        "grad_norm": 2.3363993167877197,
        "learning_rate": 9.76312439872437e-05,
        "epoch": 0.17087711453908638,
        "step": 2293
    },
    {
        "loss": 2.7827,
        "grad_norm": 3.1380057334899902,
        "learning_rate": 9.756089865244664e-05,
        "epoch": 0.17095163574036815,
        "step": 2294
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.685786247253418,
        "learning_rate": 9.749055452532856e-05,
        "epoch": 0.1710261569416499,
        "step": 2295
    },
    {
        "loss": 1.8845,
        "grad_norm": 3.4024791717529297,
        "learning_rate": 9.74202116407191e-05,
        "epoch": 0.17110067814293167,
        "step": 2296
    },
    {
        "loss": 2.4365,
        "grad_norm": 3.340653419494629,
        "learning_rate": 9.734987003344732e-05,
        "epoch": 0.17117519934421344,
        "step": 2297
    },
    {
        "loss": 2.2666,
        "grad_norm": 2.062382221221924,
        "learning_rate": 9.727952973834164e-05,
        "epoch": 0.1712497205454952,
        "step": 2298
    },
    {
        "loss": 1.7464,
        "grad_norm": 3.4350531101226807,
        "learning_rate": 9.720919079022988e-05,
        "epoch": 0.17132424174677696,
        "step": 2299
    },
    {
        "loss": 2.2524,
        "grad_norm": 2.524915933609009,
        "learning_rate": 9.713885322393914e-05,
        "epoch": 0.17139876294805872,
        "step": 2300
    },
    {
        "loss": 2.4079,
        "grad_norm": 1.9400343894958496,
        "learning_rate": 9.706851707429586e-05,
        "epoch": 0.1714732841493405,
        "step": 2301
    },
    {
        "loss": 1.9607,
        "grad_norm": 2.944162368774414,
        "learning_rate": 9.699818237612578e-05,
        "epoch": 0.17154780535062225,
        "step": 2302
    },
    {
        "loss": 1.7627,
        "grad_norm": 5.24951171875,
        "learning_rate": 9.692784916425389e-05,
        "epoch": 0.17162232655190401,
        "step": 2303
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.1069955825805664,
        "learning_rate": 9.685751747350442e-05,
        "epoch": 0.17169684775318578,
        "step": 2304
    },
    {
        "loss": 2.944,
        "grad_norm": 2.2279417514801025,
        "learning_rate": 9.6787187338701e-05,
        "epoch": 0.17177136895446754,
        "step": 2305
    },
    {
        "loss": 2.5136,
        "grad_norm": 3.6280198097229004,
        "learning_rate": 9.671685879466634e-05,
        "epoch": 0.1718458901557493,
        "step": 2306
    },
    {
        "loss": 2.7821,
        "grad_norm": 1.4648796319961548,
        "learning_rate": 9.664653187622234e-05,
        "epoch": 0.17192041135703107,
        "step": 2307
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.8023415803909302,
        "learning_rate": 9.657620661819025e-05,
        "epoch": 0.17199493255831283,
        "step": 2308
    },
    {
        "loss": 2.6811,
        "grad_norm": 2.552074432373047,
        "learning_rate": 9.650588305539034e-05,
        "epoch": 0.1720694537595946,
        "step": 2309
    },
    {
        "loss": 2.7132,
        "grad_norm": 2.0216403007507324,
        "learning_rate": 9.643556122264217e-05,
        "epoch": 0.17214397496087636,
        "step": 2310
    },
    {
        "loss": 2.671,
        "grad_norm": 2.513467311859131,
        "learning_rate": 9.636524115476435e-05,
        "epoch": 0.17221849616215815,
        "step": 2311
    },
    {
        "loss": 1.6674,
        "grad_norm": 3.4704270362854004,
        "learning_rate": 9.629492288657465e-05,
        "epoch": 0.1722930173634399,
        "step": 2312
    },
    {
        "loss": 2.3074,
        "grad_norm": 4.5350341796875,
        "learning_rate": 9.622460645288993e-05,
        "epoch": 0.17236753856472167,
        "step": 2313
    },
    {
        "loss": 2.9681,
        "grad_norm": 2.3648862838745117,
        "learning_rate": 9.615429188852617e-05,
        "epoch": 0.17244205976600344,
        "step": 2314
    },
    {
        "loss": 1.8699,
        "grad_norm": 3.4547438621520996,
        "learning_rate": 9.608397922829843e-05,
        "epoch": 0.1725165809672852,
        "step": 2315
    },
    {
        "loss": 2.6462,
        "grad_norm": 2.2997915744781494,
        "learning_rate": 9.601366850702082e-05,
        "epoch": 0.17259110216856696,
        "step": 2316
    },
    {
        "loss": 2.6076,
        "grad_norm": 3.4608335494995117,
        "learning_rate": 9.594335975950645e-05,
        "epoch": 0.17266562336984873,
        "step": 2317
    },
    {
        "loss": 2.7661,
        "grad_norm": 2.6843690872192383,
        "learning_rate": 9.587305302056749e-05,
        "epoch": 0.1727401445711305,
        "step": 2318
    },
    {
        "loss": 2.4347,
        "grad_norm": 2.1632444858551025,
        "learning_rate": 9.580274832501507e-05,
        "epoch": 0.17281466577241225,
        "step": 2319
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.133509397506714,
        "learning_rate": 9.573244570765944e-05,
        "epoch": 0.17288918697369401,
        "step": 2320
    },
    {
        "loss": 2.6394,
        "grad_norm": 2.6653265953063965,
        "learning_rate": 9.566214520330966e-05,
        "epoch": 0.17296370817497578,
        "step": 2321
    },
    {
        "loss": 2.8902,
        "grad_norm": 2.33388090133667,
        "learning_rate": 9.559184684677383e-05,
        "epoch": 0.17303822937625754,
        "step": 2322
    },
    {
        "loss": 2.6107,
        "grad_norm": 3.216151714324951,
        "learning_rate": 9.552155067285898e-05,
        "epoch": 0.1731127505775393,
        "step": 2323
    },
    {
        "loss": 3.0582,
        "grad_norm": 2.8927109241485596,
        "learning_rate": 9.545125671637101e-05,
        "epoch": 0.17318727177882107,
        "step": 2324
    },
    {
        "loss": 1.9645,
        "grad_norm": 2.597676992416382,
        "learning_rate": 9.538096501211477e-05,
        "epoch": 0.17326179298010283,
        "step": 2325
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.870957136154175,
        "learning_rate": 9.531067559489403e-05,
        "epoch": 0.1733363141813846,
        "step": 2326
    },
    {
        "loss": 2.0855,
        "grad_norm": 2.839282274246216,
        "learning_rate": 9.524038849951133e-05,
        "epoch": 0.17341083538266636,
        "step": 2327
    },
    {
        "loss": 2.0606,
        "grad_norm": 3.317948341369629,
        "learning_rate": 9.517010376076814e-05,
        "epoch": 0.17348535658394815,
        "step": 2328
    },
    {
        "loss": 1.9846,
        "grad_norm": 3.880659580230713,
        "learning_rate": 9.509982141346473e-05,
        "epoch": 0.1735598777852299,
        "step": 2329
    },
    {
        "loss": 2.3037,
        "grad_norm": 3.405367851257324,
        "learning_rate": 9.502954149240014e-05,
        "epoch": 0.17363439898651167,
        "step": 2330
    },
    {
        "loss": 2.7061,
        "grad_norm": 3.2507548332214355,
        "learning_rate": 9.495926403237236e-05,
        "epoch": 0.17370892018779344,
        "step": 2331
    },
    {
        "loss": 2.2921,
        "grad_norm": 2.7742769718170166,
        "learning_rate": 9.488898906817803e-05,
        "epoch": 0.1737834413890752,
        "step": 2332
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.252382755279541,
        "learning_rate": 9.481871663461253e-05,
        "epoch": 0.17385796259035696,
        "step": 2333
    },
    {
        "loss": 2.9491,
        "grad_norm": 3.2493412494659424,
        "learning_rate": 9.474844676647007e-05,
        "epoch": 0.17393248379163873,
        "step": 2334
    },
    {
        "loss": 2.1829,
        "grad_norm": 2.477177858352661,
        "learning_rate": 9.467817949854358e-05,
        "epoch": 0.1740070049929205,
        "step": 2335
    },
    {
        "loss": 2.274,
        "grad_norm": 3.2591679096221924,
        "learning_rate": 9.460791486562473e-05,
        "epoch": 0.17408152619420225,
        "step": 2336
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.9269559383392334,
        "learning_rate": 9.453765290250369e-05,
        "epoch": 0.17415604739548401,
        "step": 2337
    },
    {
        "loss": 2.9081,
        "grad_norm": 1.9929084777832031,
        "learning_rate": 9.446739364396962e-05,
        "epoch": 0.17423056859676578,
        "step": 2338
    },
    {
        "loss": 2.5627,
        "grad_norm": 3.0406689643859863,
        "learning_rate": 9.439713712481012e-05,
        "epoch": 0.17430508979804754,
        "step": 2339
    },
    {
        "loss": 2.2571,
        "grad_norm": 3.1572351455688477,
        "learning_rate": 9.432688337981144e-05,
        "epoch": 0.1743796109993293,
        "step": 2340
    },
    {
        "loss": 2.5032,
        "grad_norm": 3.206559896469116,
        "learning_rate": 9.42566324437586e-05,
        "epoch": 0.17445413220061107,
        "step": 2341
    },
    {
        "loss": 2.1745,
        "grad_norm": 3.0448477268218994,
        "learning_rate": 9.418638435143509e-05,
        "epoch": 0.17452865340189283,
        "step": 2342
    },
    {
        "loss": 2.3257,
        "grad_norm": 2.965399742126465,
        "learning_rate": 9.411613913762306e-05,
        "epoch": 0.1746031746031746,
        "step": 2343
    },
    {
        "loss": 2.2225,
        "grad_norm": 2.9680211544036865,
        "learning_rate": 9.404589683710316e-05,
        "epoch": 0.17467769580445636,
        "step": 2344
    },
    {
        "loss": 1.4324,
        "grad_norm": 2.560654401779175,
        "learning_rate": 9.397565748465467e-05,
        "epoch": 0.17475221700573812,
        "step": 2345
    },
    {
        "loss": 2.0418,
        "grad_norm": 2.6651172637939453,
        "learning_rate": 9.390542111505554e-05,
        "epoch": 0.1748267382070199,
        "step": 2346
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.350992202758789,
        "learning_rate": 9.383518776308186e-05,
        "epoch": 0.17490125940830167,
        "step": 2347
    },
    {
        "loss": 2.5639,
        "grad_norm": 2.1921226978302,
        "learning_rate": 9.376495746350866e-05,
        "epoch": 0.17497578060958344,
        "step": 2348
    },
    {
        "loss": 2.6163,
        "grad_norm": 3.9910387992858887,
        "learning_rate": 9.36947302511091e-05,
        "epoch": 0.1750503018108652,
        "step": 2349
    },
    {
        "loss": 2.5693,
        "grad_norm": 2.251281499862671,
        "learning_rate": 9.362450616065511e-05,
        "epoch": 0.17512482301214696,
        "step": 2350
    },
    {
        "loss": 2.6196,
        "grad_norm": 2.7073910236358643,
        "learning_rate": 9.355428522691685e-05,
        "epoch": 0.17519934421342873,
        "step": 2351
    },
    {
        "loss": 2.7984,
        "grad_norm": 1.9860717058181763,
        "learning_rate": 9.3484067484663e-05,
        "epoch": 0.1752738654147105,
        "step": 2352
    },
    {
        "loss": 2.3371,
        "grad_norm": 4.256314277648926,
        "learning_rate": 9.341385296866077e-05,
        "epoch": 0.17534838661599225,
        "step": 2353
    },
    {
        "loss": 2.6559,
        "grad_norm": 2.096663236618042,
        "learning_rate": 9.334364171367554e-05,
        "epoch": 0.17542290781727402,
        "step": 2354
    },
    {
        "loss": 2.1453,
        "grad_norm": 3.117568016052246,
        "learning_rate": 9.327343375447125e-05,
        "epoch": 0.17549742901855578,
        "step": 2355
    },
    {
        "loss": 2.0478,
        "grad_norm": 4.240108489990234,
        "learning_rate": 9.320322912581018e-05,
        "epoch": 0.17557195021983754,
        "step": 2356
    },
    {
        "loss": 2.4755,
        "grad_norm": 3.205047369003296,
        "learning_rate": 9.313302786245294e-05,
        "epoch": 0.1756464714211193,
        "step": 2357
    },
    {
        "loss": 2.2567,
        "grad_norm": 1.9821176528930664,
        "learning_rate": 9.306282999915844e-05,
        "epoch": 0.17572099262240107,
        "step": 2358
    },
    {
        "loss": 2.6256,
        "grad_norm": 2.598764181137085,
        "learning_rate": 9.299263557068391e-05,
        "epoch": 0.17579551382368283,
        "step": 2359
    },
    {
        "loss": 2.1234,
        "grad_norm": 4.945767402648926,
        "learning_rate": 9.292244461178501e-05,
        "epoch": 0.1758700350249646,
        "step": 2360
    },
    {
        "loss": 2.1217,
        "grad_norm": 3.6051406860351562,
        "learning_rate": 9.285225715721552e-05,
        "epoch": 0.17594455622624636,
        "step": 2361
    },
    {
        "loss": 1.7414,
        "grad_norm": 2.8667924404144287,
        "learning_rate": 9.278207324172754e-05,
        "epoch": 0.17601907742752812,
        "step": 2362
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.9220573902130127,
        "learning_rate": 9.271189290007145e-05,
        "epoch": 0.17609359862880988,
        "step": 2363
    },
    {
        "loss": 2.6602,
        "grad_norm": 2.8692333698272705,
        "learning_rate": 9.264171616699583e-05,
        "epoch": 0.17616811983009167,
        "step": 2364
    },
    {
        "loss": 2.7305,
        "grad_norm": 2.1193134784698486,
        "learning_rate": 9.257154307724743e-05,
        "epoch": 0.17624264103137344,
        "step": 2365
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.911515951156616,
        "learning_rate": 9.250137366557135e-05,
        "epoch": 0.1763171622326552,
        "step": 2366
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.729562520980835,
        "learning_rate": 9.243120796671068e-05,
        "epoch": 0.17639168343393696,
        "step": 2367
    },
    {
        "loss": 2.4741,
        "grad_norm": 2.3430917263031006,
        "learning_rate": 9.236104601540675e-05,
        "epoch": 0.17646620463521873,
        "step": 2368
    },
    {
        "loss": 1.9903,
        "grad_norm": 2.5861599445343018,
        "learning_rate": 9.229088784639911e-05,
        "epoch": 0.1765407258365005,
        "step": 2369
    },
    {
        "loss": 2.4356,
        "grad_norm": 2.5541036128997803,
        "learning_rate": 9.222073349442525e-05,
        "epoch": 0.17661524703778225,
        "step": 2370
    },
    {
        "loss": 2.6919,
        "grad_norm": 3.21573543548584,
        "learning_rate": 9.215058299422103e-05,
        "epoch": 0.17668976823906402,
        "step": 2371
    },
    {
        "loss": 2.7494,
        "grad_norm": 2.069578170776367,
        "learning_rate": 9.208043638052018e-05,
        "epoch": 0.17676428944034578,
        "step": 2372
    },
    {
        "loss": 2.7934,
        "grad_norm": 3.9970099925994873,
        "learning_rate": 9.20102936880546e-05,
        "epoch": 0.17683881064162754,
        "step": 2373
    },
    {
        "loss": 2.0368,
        "grad_norm": 3.4086122512817383,
        "learning_rate": 9.194015495155421e-05,
        "epoch": 0.1769133318429093,
        "step": 2374
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.6516122817993164,
        "learning_rate": 9.187002020574697e-05,
        "epoch": 0.17698785304419107,
        "step": 2375
    },
    {
        "loss": 2.2565,
        "grad_norm": 2.954540967941284,
        "learning_rate": 9.179988948535899e-05,
        "epoch": 0.17706237424547283,
        "step": 2376
    },
    {
        "loss": 2.7972,
        "grad_norm": 1.963207721710205,
        "learning_rate": 9.172976282511421e-05,
        "epoch": 0.1771368954467546,
        "step": 2377
    },
    {
        "loss": 2.8229,
        "grad_norm": 1.8384026288986206,
        "learning_rate": 9.165964025973466e-05,
        "epoch": 0.17721141664803636,
        "step": 2378
    },
    {
        "loss": 2.535,
        "grad_norm": 2.8229682445526123,
        "learning_rate": 9.158952182394033e-05,
        "epoch": 0.17728593784931812,
        "step": 2379
    },
    {
        "loss": 2.9816,
        "grad_norm": 2.635364532470703,
        "learning_rate": 9.151940755244908e-05,
        "epoch": 0.17736045905059988,
        "step": 2380
    },
    {
        "loss": 2.3328,
        "grad_norm": 3.2282724380493164,
        "learning_rate": 9.144929747997687e-05,
        "epoch": 0.17743498025188165,
        "step": 2381
    },
    {
        "loss": 2.7168,
        "grad_norm": 2.3541207313537598,
        "learning_rate": 9.137919164123747e-05,
        "epoch": 0.17750950145316344,
        "step": 2382
    },
    {
        "loss": 2.4993,
        "grad_norm": 3.4425394535064697,
        "learning_rate": 9.130909007094256e-05,
        "epoch": 0.1775840226544452,
        "step": 2383
    },
    {
        "loss": 2.4122,
        "grad_norm": 3.3657257556915283,
        "learning_rate": 9.123899280380173e-05,
        "epoch": 0.17765854385572696,
        "step": 2384
    },
    {
        "loss": 2.3595,
        "grad_norm": 2.956404447555542,
        "learning_rate": 9.116889987452238e-05,
        "epoch": 0.17773306505700873,
        "step": 2385
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.813150405883789,
        "learning_rate": 9.109881131780993e-05,
        "epoch": 0.1778075862582905,
        "step": 2386
    },
    {
        "loss": 2.7233,
        "grad_norm": 2.7215991020202637,
        "learning_rate": 9.102872716836744e-05,
        "epoch": 0.17788210745957225,
        "step": 2387
    },
    {
        "loss": 2.6898,
        "grad_norm": 2.1717822551727295,
        "learning_rate": 9.09586474608959e-05,
        "epoch": 0.17795662866085402,
        "step": 2388
    },
    {
        "loss": 2.2413,
        "grad_norm": 3.1211071014404297,
        "learning_rate": 9.0888572230094e-05,
        "epoch": 0.17803114986213578,
        "step": 2389
    },
    {
        "loss": 1.5767,
        "grad_norm": 2.342507839202881,
        "learning_rate": 9.081850151065838e-05,
        "epoch": 0.17810567106341754,
        "step": 2390
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.8941009044647217,
        "learning_rate": 9.074843533728325e-05,
        "epoch": 0.1781801922646993,
        "step": 2391
    },
    {
        "loss": 2.7237,
        "grad_norm": 2.8429043292999268,
        "learning_rate": 9.067837374466075e-05,
        "epoch": 0.17825471346598107,
        "step": 2392
    },
    {
        "loss": 2.4547,
        "grad_norm": 1.7640776634216309,
        "learning_rate": 9.060831676748062e-05,
        "epoch": 0.17832923466726283,
        "step": 2393
    },
    {
        "loss": 2.756,
        "grad_norm": 3.6004345417022705,
        "learning_rate": 9.053826444043042e-05,
        "epoch": 0.1784037558685446,
        "step": 2394
    },
    {
        "loss": 2.772,
        "grad_norm": 2.230315685272217,
        "learning_rate": 9.046821679819527e-05,
        "epoch": 0.17847827706982636,
        "step": 2395
    },
    {
        "loss": 2.1388,
        "grad_norm": 3.7095768451690674,
        "learning_rate": 9.039817387545806e-05,
        "epoch": 0.17855279827110812,
        "step": 2396
    },
    {
        "loss": 2.4115,
        "grad_norm": 2.3460490703582764,
        "learning_rate": 9.032813570689942e-05,
        "epoch": 0.17862731947238988,
        "step": 2397
    },
    {
        "loss": 2.9949,
        "grad_norm": 2.356435775756836,
        "learning_rate": 9.025810232719743e-05,
        "epoch": 0.17870184067367165,
        "step": 2398
    },
    {
        "loss": 2.5521,
        "grad_norm": 1.667386770248413,
        "learning_rate": 9.018807377102801e-05,
        "epoch": 0.17877636187495344,
        "step": 2399
    },
    {
        "loss": 2.1647,
        "grad_norm": 2.497187614440918,
        "learning_rate": 9.011805007306453e-05,
        "epoch": 0.1788508830762352,
        "step": 2400
    },
    {
        "loss": 2.744,
        "grad_norm": 2.3955206871032715,
        "learning_rate": 9.0048031267978e-05,
        "epoch": 0.17892540427751696,
        "step": 2401
    },
    {
        "loss": 2.0534,
        "grad_norm": 3.1586148738861084,
        "learning_rate": 8.997801739043711e-05,
        "epoch": 0.17899992547879873,
        "step": 2402
    },
    {
        "loss": 2.3641,
        "grad_norm": 2.426201581954956,
        "learning_rate": 8.990800847510792e-05,
        "epoch": 0.1790744466800805,
        "step": 2403
    },
    {
        "loss": 2.9768,
        "grad_norm": 1.9135793447494507,
        "learning_rate": 8.983800455665425e-05,
        "epoch": 0.17914896788136225,
        "step": 2404
    },
    {
        "loss": 2.2167,
        "grad_norm": 3.064777374267578,
        "learning_rate": 8.97680056697372e-05,
        "epoch": 0.17922348908264402,
        "step": 2405
    },
    {
        "loss": 2.3788,
        "grad_norm": 1.8752373456954956,
        "learning_rate": 8.969801184901555e-05,
        "epoch": 0.17929801028392578,
        "step": 2406
    },
    {
        "loss": 2.2291,
        "grad_norm": 2.9464969635009766,
        "learning_rate": 8.962802312914566e-05,
        "epoch": 0.17937253148520754,
        "step": 2407
    },
    {
        "loss": 2.3735,
        "grad_norm": 6.354891300201416,
        "learning_rate": 8.955803954478105e-05,
        "epoch": 0.1794470526864893,
        "step": 2408
    },
    {
        "loss": 2.4788,
        "grad_norm": 1.9028292894363403,
        "learning_rate": 8.948806113057304e-05,
        "epoch": 0.17952157388777107,
        "step": 2409
    },
    {
        "loss": 2.3212,
        "grad_norm": 2.4900314807891846,
        "learning_rate": 8.94180879211701e-05,
        "epoch": 0.17959609508905283,
        "step": 2410
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.3502395153045654,
        "learning_rate": 8.934811995121834e-05,
        "epoch": 0.1796706162903346,
        "step": 2411
    },
    {
        "loss": 2.1578,
        "grad_norm": 3.1159849166870117,
        "learning_rate": 8.92781572553612e-05,
        "epoch": 0.17974513749161636,
        "step": 2412
    },
    {
        "loss": 2.2398,
        "grad_norm": 2.3228328227996826,
        "learning_rate": 8.920819986823942e-05,
        "epoch": 0.17981965869289812,
        "step": 2413
    },
    {
        "loss": 1.6457,
        "grad_norm": 1.8195441961288452,
        "learning_rate": 8.913824782449134e-05,
        "epoch": 0.17989417989417988,
        "step": 2414
    },
    {
        "loss": 2.1978,
        "grad_norm": 4.556515216827393,
        "learning_rate": 8.906830115875234e-05,
        "epoch": 0.17996870109546165,
        "step": 2415
    },
    {
        "loss": 2.958,
        "grad_norm": 1.9530853033065796,
        "learning_rate": 8.89983599056554e-05,
        "epoch": 0.1800432222967434,
        "step": 2416
    },
    {
        "loss": 2.0905,
        "grad_norm": 2.0976970195770264,
        "learning_rate": 8.892842409983069e-05,
        "epoch": 0.1801177434980252,
        "step": 2417
    },
    {
        "loss": 1.9579,
        "grad_norm": 3.615907669067383,
        "learning_rate": 8.885849377590577e-05,
        "epoch": 0.18019226469930696,
        "step": 2418
    },
    {
        "loss": 2.3874,
        "grad_norm": 2.605367422103882,
        "learning_rate": 8.87885689685054e-05,
        "epoch": 0.18026678590058873,
        "step": 2419
    },
    {
        "loss": 1.8469,
        "grad_norm": 2.8824269771575928,
        "learning_rate": 8.871864971225158e-05,
        "epoch": 0.1803413071018705,
        "step": 2420
    },
    {
        "loss": 2.4338,
        "grad_norm": 2.5022027492523193,
        "learning_rate": 8.864873604176372e-05,
        "epoch": 0.18041582830315225,
        "step": 2421
    },
    {
        "loss": 1.7003,
        "grad_norm": 2.9312684535980225,
        "learning_rate": 8.857882799165834e-05,
        "epoch": 0.18049034950443402,
        "step": 2422
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.903270959854126,
        "learning_rate": 8.850892559654917e-05,
        "epoch": 0.18056487070571578,
        "step": 2423
    },
    {
        "loss": 2.1228,
        "grad_norm": 3.7211642265319824,
        "learning_rate": 8.843902889104721e-05,
        "epoch": 0.18063939190699754,
        "step": 2424
    },
    {
        "loss": 2.5799,
        "grad_norm": 2.6457014083862305,
        "learning_rate": 8.836913790976062e-05,
        "epoch": 0.1807139131082793,
        "step": 2425
    },
    {
        "loss": 2.7846,
        "grad_norm": 2.447481393814087,
        "learning_rate": 8.829925268729462e-05,
        "epoch": 0.18078843430956107,
        "step": 2426
    },
    {
        "loss": 2.5935,
        "grad_norm": 2.040027618408203,
        "learning_rate": 8.822937325825179e-05,
        "epoch": 0.18086295551084283,
        "step": 2427
    },
    {
        "loss": 2.3864,
        "grad_norm": 2.6763150691986084,
        "learning_rate": 8.815949965723167e-05,
        "epoch": 0.1809374767121246,
        "step": 2428
    },
    {
        "loss": 2.5143,
        "grad_norm": 2.4539382457733154,
        "learning_rate": 8.808963191883099e-05,
        "epoch": 0.18101199791340636,
        "step": 2429
    },
    {
        "loss": 2.7219,
        "grad_norm": 3.0166876316070557,
        "learning_rate": 8.801977007764351e-05,
        "epoch": 0.18108651911468812,
        "step": 2430
    },
    {
        "loss": 3.0449,
        "grad_norm": 1.9719091653823853,
        "learning_rate": 8.794991416826013e-05,
        "epoch": 0.18116104031596988,
        "step": 2431
    },
    {
        "loss": 2.4694,
        "grad_norm": 2.026515483856201,
        "learning_rate": 8.788006422526881e-05,
        "epoch": 0.18123556151725165,
        "step": 2432
    },
    {
        "loss": 1.4643,
        "grad_norm": 3.9404189586639404,
        "learning_rate": 8.781022028325457e-05,
        "epoch": 0.1813100827185334,
        "step": 2433
    },
    {
        "loss": 2.3834,
        "grad_norm": 2.8741564750671387,
        "learning_rate": 8.774038237679941e-05,
        "epoch": 0.18138460391981517,
        "step": 2434
    },
    {
        "loss": 2.2168,
        "grad_norm": 3.1170706748962402,
        "learning_rate": 8.767055054048235e-05,
        "epoch": 0.18145912512109696,
        "step": 2435
    },
    {
        "loss": 2.8542,
        "grad_norm": 3.1866536140441895,
        "learning_rate": 8.760072480887941e-05,
        "epoch": 0.18153364632237873,
        "step": 2436
    },
    {
        "loss": 2.2231,
        "grad_norm": 3.438906192779541,
        "learning_rate": 8.753090521656364e-05,
        "epoch": 0.1816081675236605,
        "step": 2437
    },
    {
        "loss": 2.1738,
        "grad_norm": 3.5541388988494873,
        "learning_rate": 8.746109179810496e-05,
        "epoch": 0.18168268872494225,
        "step": 2438
    },
    {
        "loss": 2.371,
        "grad_norm": 2.226486921310425,
        "learning_rate": 8.739128458807033e-05,
        "epoch": 0.18175720992622402,
        "step": 2439
    },
    {
        "loss": 3.1146,
        "grad_norm": 2.0892703533172607,
        "learning_rate": 8.732148362102356e-05,
        "epoch": 0.18183173112750578,
        "step": 2440
    },
    {
        "loss": 2.7767,
        "grad_norm": 1.8298672437667847,
        "learning_rate": 8.725168893152533e-05,
        "epoch": 0.18190625232878754,
        "step": 2441
    },
    {
        "loss": 2.1408,
        "grad_norm": 3.747565507888794,
        "learning_rate": 8.71819005541334e-05,
        "epoch": 0.1819807735300693,
        "step": 2442
    },
    {
        "loss": 2.3365,
        "grad_norm": 4.9654669761657715,
        "learning_rate": 8.711211852340222e-05,
        "epoch": 0.18205529473135107,
        "step": 2443
    },
    {
        "loss": 2.6118,
        "grad_norm": 1.9584729671478271,
        "learning_rate": 8.704234287388315e-05,
        "epoch": 0.18212981593263283,
        "step": 2444
    },
    {
        "loss": 2.568,
        "grad_norm": 3.2946972846984863,
        "learning_rate": 8.697257364012438e-05,
        "epoch": 0.1822043371339146,
        "step": 2445
    },
    {
        "loss": 2.883,
        "grad_norm": 2.0307154655456543,
        "learning_rate": 8.690281085667095e-05,
        "epoch": 0.18227885833519636,
        "step": 2446
    },
    {
        "loss": 3.2323,
        "grad_norm": 2.6822240352630615,
        "learning_rate": 8.683305455806474e-05,
        "epoch": 0.18235337953647812,
        "step": 2447
    },
    {
        "loss": 2.34,
        "grad_norm": 3.469660997390747,
        "learning_rate": 8.676330477884438e-05,
        "epoch": 0.18242790073775988,
        "step": 2448
    },
    {
        "loss": 2.2008,
        "grad_norm": 3.2484798431396484,
        "learning_rate": 8.669356155354523e-05,
        "epoch": 0.18250242193904165,
        "step": 2449
    },
    {
        "loss": 2.796,
        "grad_norm": 2.7009291648864746,
        "learning_rate": 8.662382491669946e-05,
        "epoch": 0.1825769431403234,
        "step": 2450
    },
    {
        "loss": 3.0117,
        "grad_norm": 2.5622875690460205,
        "learning_rate": 8.655409490283594e-05,
        "epoch": 0.18265146434160517,
        "step": 2451
    },
    {
        "loss": 2.0086,
        "grad_norm": 4.412351131439209,
        "learning_rate": 8.64843715464803e-05,
        "epoch": 0.18272598554288697,
        "step": 2452
    },
    {
        "loss": 2.1381,
        "grad_norm": 3.400791645050049,
        "learning_rate": 8.641465488215486e-05,
        "epoch": 0.18280050674416873,
        "step": 2453
    },
    {
        "loss": 3.1942,
        "grad_norm": 3.586409091949463,
        "learning_rate": 8.634494494437863e-05,
        "epoch": 0.1828750279454505,
        "step": 2454
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.6146764755249023,
        "learning_rate": 8.62752417676673e-05,
        "epoch": 0.18294954914673225,
        "step": 2455
    },
    {
        "loss": 2.3759,
        "grad_norm": 3.918776035308838,
        "learning_rate": 8.620554538653315e-05,
        "epoch": 0.18302407034801402,
        "step": 2456
    },
    {
        "loss": 2.7022,
        "grad_norm": 2.1931614875793457,
        "learning_rate": 8.613585583548515e-05,
        "epoch": 0.18309859154929578,
        "step": 2457
    },
    {
        "loss": 1.5248,
        "grad_norm": 1.8009288311004639,
        "learning_rate": 8.606617314902893e-05,
        "epoch": 0.18317311275057754,
        "step": 2458
    },
    {
        "loss": 2.1112,
        "grad_norm": 2.9826714992523193,
        "learning_rate": 8.599649736166656e-05,
        "epoch": 0.1832476339518593,
        "step": 2459
    },
    {
        "loss": 2.5076,
        "grad_norm": 2.62905216217041,
        "learning_rate": 8.592682850789692e-05,
        "epoch": 0.18332215515314107,
        "step": 2460
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.7852524518966675,
        "learning_rate": 8.585716662221531e-05,
        "epoch": 0.18339667635442283,
        "step": 2461
    },
    {
        "loss": 2.1825,
        "grad_norm": 1.7586336135864258,
        "learning_rate": 8.578751173911354e-05,
        "epoch": 0.1834711975557046,
        "step": 2462
    },
    {
        "loss": 1.8567,
        "grad_norm": 4.016661167144775,
        "learning_rate": 8.571786389308017e-05,
        "epoch": 0.18354571875698636,
        "step": 2463
    },
    {
        "loss": 1.9305,
        "grad_norm": 3.2695810794830322,
        "learning_rate": 8.564822311859993e-05,
        "epoch": 0.18362023995826812,
        "step": 2464
    },
    {
        "loss": 1.8589,
        "grad_norm": 2.7491390705108643,
        "learning_rate": 8.557858945015444e-05,
        "epoch": 0.18369476115954989,
        "step": 2465
    },
    {
        "loss": 3.001,
        "grad_norm": 2.011884927749634,
        "learning_rate": 8.550896292222146e-05,
        "epoch": 0.18376928236083165,
        "step": 2466
    },
    {
        "loss": 2.382,
        "grad_norm": 2.8228800296783447,
        "learning_rate": 8.543934356927543e-05,
        "epoch": 0.1838438035621134,
        "step": 2467
    },
    {
        "loss": 2.2019,
        "grad_norm": 4.611060619354248,
        "learning_rate": 8.536973142578724e-05,
        "epoch": 0.18391832476339517,
        "step": 2468
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.471311092376709,
        "learning_rate": 8.530012652622397e-05,
        "epoch": 0.18399284596467694,
        "step": 2469
    },
    {
        "loss": 2.4498,
        "grad_norm": 2.887495994567871,
        "learning_rate": 8.523052890504947e-05,
        "epoch": 0.18406736716595873,
        "step": 2470
    },
    {
        "loss": 2.604,
        "grad_norm": 2.0894997119903564,
        "learning_rate": 8.516093859672369e-05,
        "epoch": 0.1841418883672405,
        "step": 2471
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.964709758758545,
        "learning_rate": 8.509135563570307e-05,
        "epoch": 0.18421640956852225,
        "step": 2472
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.4894922971725464,
        "learning_rate": 8.502178005644049e-05,
        "epoch": 0.18429093076980402,
        "step": 2473
    },
    {
        "loss": 2.0098,
        "grad_norm": 3.6088194847106934,
        "learning_rate": 8.495221189338498e-05,
        "epoch": 0.18436545197108578,
        "step": 2474
    },
    {
        "loss": 2.6403,
        "grad_norm": 3.2901246547698975,
        "learning_rate": 8.488265118098217e-05,
        "epoch": 0.18443997317236754,
        "step": 2475
    },
    {
        "loss": 2.6453,
        "grad_norm": 2.575995683670044,
        "learning_rate": 8.481309795367369e-05,
        "epoch": 0.1845144943736493,
        "step": 2476
    },
    {
        "loss": 2.0629,
        "grad_norm": 3.381791830062866,
        "learning_rate": 8.474355224589772e-05,
        "epoch": 0.18458901557493107,
        "step": 2477
    },
    {
        "loss": 2.2702,
        "grad_norm": 3.5868704319000244,
        "learning_rate": 8.467401409208858e-05,
        "epoch": 0.18466353677621283,
        "step": 2478
    },
    {
        "loss": 2.3314,
        "grad_norm": 2.6574325561523438,
        "learning_rate": 8.460448352667686e-05,
        "epoch": 0.1847380579774946,
        "step": 2479
    },
    {
        "loss": 1.888,
        "grad_norm": 2.705789804458618,
        "learning_rate": 8.453496058408951e-05,
        "epoch": 0.18481257917877636,
        "step": 2480
    },
    {
        "loss": 1.7449,
        "grad_norm": 3.2541425228118896,
        "learning_rate": 8.44654452987495e-05,
        "epoch": 0.18488710038005812,
        "step": 2481
    },
    {
        "loss": 2.5342,
        "grad_norm": 2.7365269660949707,
        "learning_rate": 8.439593770507622e-05,
        "epoch": 0.18496162158133989,
        "step": 2482
    },
    {
        "loss": 2.4528,
        "grad_norm": 4.34114408493042,
        "learning_rate": 8.432643783748512e-05,
        "epoch": 0.18503614278262165,
        "step": 2483
    },
    {
        "loss": 2.6683,
        "grad_norm": 1.8490656614303589,
        "learning_rate": 8.425694573038786e-05,
        "epoch": 0.1851106639839034,
        "step": 2484
    },
    {
        "loss": 2.2578,
        "grad_norm": 1.961042046546936,
        "learning_rate": 8.418746141819223e-05,
        "epoch": 0.18518518518518517,
        "step": 2485
    },
    {
        "loss": 2.4672,
        "grad_norm": 4.034591197967529,
        "learning_rate": 8.41179849353022e-05,
        "epoch": 0.18525970638646694,
        "step": 2486
    },
    {
        "loss": 2.05,
        "grad_norm": 3.0969886779785156,
        "learning_rate": 8.40485163161179e-05,
        "epoch": 0.1853342275877487,
        "step": 2487
    },
    {
        "loss": 2.9528,
        "grad_norm": 2.2889952659606934,
        "learning_rate": 8.397905559503547e-05,
        "epoch": 0.1854087487890305,
        "step": 2488
    },
    {
        "loss": 2.2882,
        "grad_norm": 2.4754364490509033,
        "learning_rate": 8.390960280644721e-05,
        "epoch": 0.18548326999031226,
        "step": 2489
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.137223958969116,
        "learning_rate": 8.384015798474147e-05,
        "epoch": 0.18555779119159402,
        "step": 2490
    },
    {
        "loss": 1.8969,
        "grad_norm": 3.2318406105041504,
        "learning_rate": 8.377072116430265e-05,
        "epoch": 0.18563231239287578,
        "step": 2491
    },
    {
        "loss": 2.4224,
        "grad_norm": 2.6513359546661377,
        "learning_rate": 8.370129237951114e-05,
        "epoch": 0.18570683359415754,
        "step": 2492
    },
    {
        "loss": 1.6408,
        "grad_norm": 2.7149553298950195,
        "learning_rate": 8.363187166474346e-05,
        "epoch": 0.1857813547954393,
        "step": 2493
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.1804187297821045,
        "learning_rate": 8.356245905437211e-05,
        "epoch": 0.18585587599672107,
        "step": 2494
    },
    {
        "loss": 2.5924,
        "grad_norm": 1.1232911348342896,
        "learning_rate": 8.349305458276551e-05,
        "epoch": 0.18593039719800283,
        "step": 2495
    },
    {
        "loss": 1.8165,
        "grad_norm": 3.1087255477905273,
        "learning_rate": 8.342365828428806e-05,
        "epoch": 0.1860049183992846,
        "step": 2496
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.9121992588043213,
        "learning_rate": 8.335427019330015e-05,
        "epoch": 0.18607943960056636,
        "step": 2497
    },
    {
        "loss": 2.5309,
        "grad_norm": 2.2113726139068604,
        "learning_rate": 8.328489034415814e-05,
        "epoch": 0.18615396080184812,
        "step": 2498
    },
    {
        "loss": 2.0207,
        "grad_norm": 4.903589725494385,
        "learning_rate": 8.321551877121424e-05,
        "epoch": 0.18622848200312989,
        "step": 2499
    },
    {
        "loss": 2.4396,
        "grad_norm": 3.155385971069336,
        "learning_rate": 8.314615550881656e-05,
        "epoch": 0.18630300320441165,
        "step": 2500
    },
    {
        "loss": 1.838,
        "grad_norm": 1.178094506263733,
        "learning_rate": 8.307680059130918e-05,
        "epoch": 0.1863775244056934,
        "step": 2501
    },
    {
        "loss": 2.536,
        "grad_norm": 3.1671862602233887,
        "learning_rate": 8.30074540530319e-05,
        "epoch": 0.18645204560697518,
        "step": 2502
    },
    {
        "loss": 2.8639,
        "grad_norm": 3.1132562160491943,
        "learning_rate": 8.293811592832056e-05,
        "epoch": 0.18652656680825694,
        "step": 2503
    },
    {
        "loss": 2.4317,
        "grad_norm": 2.210721254348755,
        "learning_rate": 8.28687862515067e-05,
        "epoch": 0.1866010880095387,
        "step": 2504
    },
    {
        "loss": 2.5213,
        "grad_norm": 2.928457021713257,
        "learning_rate": 8.279946505691769e-05,
        "epoch": 0.1866756092108205,
        "step": 2505
    },
    {
        "loss": 2.2201,
        "grad_norm": 3.5801451206207275,
        "learning_rate": 8.273015237887673e-05,
        "epoch": 0.18675013041210226,
        "step": 2506
    },
    {
        "loss": 1.461,
        "grad_norm": 4.183590888977051,
        "learning_rate": 8.266084825170282e-05,
        "epoch": 0.18682465161338402,
        "step": 2507
    },
    {
        "loss": 2.041,
        "grad_norm": 3.541590690612793,
        "learning_rate": 8.259155270971068e-05,
        "epoch": 0.18689917281466578,
        "step": 2508
    },
    {
        "loss": 2.8675,
        "grad_norm": 2.9609251022338867,
        "learning_rate": 8.252226578721082e-05,
        "epoch": 0.18697369401594754,
        "step": 2509
    },
    {
        "loss": 2.2871,
        "grad_norm": 3.2227954864501953,
        "learning_rate": 8.245298751850946e-05,
        "epoch": 0.1870482152172293,
        "step": 2510
    },
    {
        "loss": 2.5819,
        "grad_norm": 1.797105312347412,
        "learning_rate": 8.238371793790855e-05,
        "epoch": 0.18712273641851107,
        "step": 2511
    },
    {
        "loss": 2.3967,
        "grad_norm": 4.635092258453369,
        "learning_rate": 8.231445707970568e-05,
        "epoch": 0.18719725761979283,
        "step": 2512
    },
    {
        "loss": 2.0164,
        "grad_norm": 2.993276596069336,
        "learning_rate": 8.224520497819419e-05,
        "epoch": 0.1872717788210746,
        "step": 2513
    },
    {
        "loss": 2.3923,
        "grad_norm": 3.2875277996063232,
        "learning_rate": 8.217596166766311e-05,
        "epoch": 0.18734630002235636,
        "step": 2514
    },
    {
        "loss": 2.363,
        "grad_norm": 2.2705349922180176,
        "learning_rate": 8.2106727182397e-05,
        "epoch": 0.18742082122363812,
        "step": 2515
    },
    {
        "loss": 1.9377,
        "grad_norm": 3.645214319229126,
        "learning_rate": 8.203750155667618e-05,
        "epoch": 0.1874953424249199,
        "step": 2516
    },
    {
        "loss": 2.0951,
        "grad_norm": 2.131147623062134,
        "learning_rate": 8.196828482477651e-05,
        "epoch": 0.18756986362620165,
        "step": 2517
    },
    {
        "loss": 2.2016,
        "grad_norm": 3.4821810722351074,
        "learning_rate": 8.189907702096939e-05,
        "epoch": 0.1876443848274834,
        "step": 2518
    },
    {
        "loss": 2.155,
        "grad_norm": 2.866224765777588,
        "learning_rate": 8.182987817952201e-05,
        "epoch": 0.18771890602876518,
        "step": 2519
    },
    {
        "loss": 2.6656,
        "grad_norm": 2.054260015487671,
        "learning_rate": 8.176068833469685e-05,
        "epoch": 0.18779342723004694,
        "step": 2520
    },
    {
        "loss": 2.668,
        "grad_norm": 2.4798758029937744,
        "learning_rate": 8.169150752075212e-05,
        "epoch": 0.1878679484313287,
        "step": 2521
    },
    {
        "loss": 2.0923,
        "grad_norm": 4.057361602783203,
        "learning_rate": 8.162233577194151e-05,
        "epoch": 0.18794246963261046,
        "step": 2522
    },
    {
        "loss": 2.9156,
        "grad_norm": 2.0178651809692383,
        "learning_rate": 8.155317312251419e-05,
        "epoch": 0.18801699083389226,
        "step": 2523
    },
    {
        "loss": 2.5796,
        "grad_norm": 2.7499637603759766,
        "learning_rate": 8.148401960671495e-05,
        "epoch": 0.18809151203517402,
        "step": 2524
    },
    {
        "loss": 2.44,
        "grad_norm": 3.274906873703003,
        "learning_rate": 8.14148752587838e-05,
        "epoch": 0.18816603323645578,
        "step": 2525
    },
    {
        "loss": 2.3914,
        "grad_norm": 2.7804441452026367,
        "learning_rate": 8.134574011295652e-05,
        "epoch": 0.18824055443773755,
        "step": 2526
    },
    {
        "loss": 2.6502,
        "grad_norm": 1.5800342559814453,
        "learning_rate": 8.127661420346408e-05,
        "epoch": 0.1883150756390193,
        "step": 2527
    },
    {
        "loss": 2.1365,
        "grad_norm": 2.4615862369537354,
        "learning_rate": 8.120749756453302e-05,
        "epoch": 0.18838959684030107,
        "step": 2528
    },
    {
        "loss": 2.9421,
        "grad_norm": 2.219287872314453,
        "learning_rate": 8.11383902303853e-05,
        "epoch": 0.18846411804158283,
        "step": 2529
    },
    {
        "loss": 3.0815,
        "grad_norm": 2.1969332695007324,
        "learning_rate": 8.10692922352381e-05,
        "epoch": 0.1885386392428646,
        "step": 2530
    },
    {
        "loss": 2.7943,
        "grad_norm": 2.3644626140594482,
        "learning_rate": 8.100020361330428e-05,
        "epoch": 0.18861316044414636,
        "step": 2531
    },
    {
        "loss": 2.5017,
        "grad_norm": 3.9282360076904297,
        "learning_rate": 8.093112439879168e-05,
        "epoch": 0.18868768164542812,
        "step": 2532
    },
    {
        "loss": 2.3078,
        "grad_norm": 2.2645955085754395,
        "learning_rate": 8.086205462590379e-05,
        "epoch": 0.1887622028467099,
        "step": 2533
    },
    {
        "loss": 2.6939,
        "grad_norm": 1.8551440238952637,
        "learning_rate": 8.079299432883934e-05,
        "epoch": 0.18883672404799165,
        "step": 2534
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.3157782554626465,
        "learning_rate": 8.072394354179228e-05,
        "epoch": 0.1889112452492734,
        "step": 2535
    },
    {
        "loss": 2.2257,
        "grad_norm": 3.658712863922119,
        "learning_rate": 8.065490229895199e-05,
        "epoch": 0.18898576645055518,
        "step": 2536
    },
    {
        "loss": 2.749,
        "grad_norm": 2.793630838394165,
        "learning_rate": 8.058587063450293e-05,
        "epoch": 0.18906028765183694,
        "step": 2537
    },
    {
        "loss": 2.2552,
        "grad_norm": 2.715449810028076,
        "learning_rate": 8.051684858262507e-05,
        "epoch": 0.1891348088531187,
        "step": 2538
    },
    {
        "loss": 1.5744,
        "grad_norm": 2.6445205211639404,
        "learning_rate": 8.044783617749341e-05,
        "epoch": 0.18920933005440047,
        "step": 2539
    },
    {
        "loss": 2.104,
        "grad_norm": 2.1851770877838135,
        "learning_rate": 8.037883345327823e-05,
        "epoch": 0.18928385125568223,
        "step": 2540
    },
    {
        "loss": 2.466,
        "grad_norm": 2.829101800918579,
        "learning_rate": 8.030984044414514e-05,
        "epoch": 0.18935837245696402,
        "step": 2541
    },
    {
        "loss": 2.9293,
        "grad_norm": 2.079916000366211,
        "learning_rate": 8.024085718425472e-05,
        "epoch": 0.18943289365824578,
        "step": 2542
    },
    {
        "loss": 2.886,
        "grad_norm": 2.953321933746338,
        "learning_rate": 8.017188370776292e-05,
        "epoch": 0.18950741485952755,
        "step": 2543
    },
    {
        "loss": 2.1575,
        "grad_norm": 3.5277843475341797,
        "learning_rate": 8.010292004882077e-05,
        "epoch": 0.1895819360608093,
        "step": 2544
    },
    {
        "loss": 2.7943,
        "grad_norm": 2.1589086055755615,
        "learning_rate": 8.003396624157438e-05,
        "epoch": 0.18965645726209107,
        "step": 2545
    },
    {
        "loss": 2.2886,
        "grad_norm": 3.10683012008667,
        "learning_rate": 7.996502232016508e-05,
        "epoch": 0.18973097846337283,
        "step": 2546
    },
    {
        "loss": 2.9344,
        "grad_norm": 3.5220882892608643,
        "learning_rate": 7.989608831872921e-05,
        "epoch": 0.1898054996646546,
        "step": 2547
    },
    {
        "loss": 2.718,
        "grad_norm": 2.3527233600616455,
        "learning_rate": 7.982716427139833e-05,
        "epoch": 0.18988002086593636,
        "step": 2548
    },
    {
        "loss": 1.892,
        "grad_norm": 3.4813997745513916,
        "learning_rate": 7.975825021229892e-05,
        "epoch": 0.18995454206721812,
        "step": 2549
    },
    {
        "loss": 2.4172,
        "grad_norm": 3.0767033100128174,
        "learning_rate": 7.968934617555266e-05,
        "epoch": 0.1900290632684999,
        "step": 2550
    },
    {
        "loss": 2.8895,
        "grad_norm": 3.6789090633392334,
        "learning_rate": 7.962045219527615e-05,
        "epoch": 0.19010358446978165,
        "step": 2551
    },
    {
        "loss": 1.4395,
        "grad_norm": 3.5297534465789795,
        "learning_rate": 7.955156830558106e-05,
        "epoch": 0.1901781056710634,
        "step": 2552
    },
    {
        "loss": 2.5372,
        "grad_norm": 2.9357595443725586,
        "learning_rate": 7.948269454057402e-05,
        "epoch": 0.19025262687234518,
        "step": 2553
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.399646043777466,
        "learning_rate": 7.941383093435677e-05,
        "epoch": 0.19032714807362694,
        "step": 2554
    },
    {
        "loss": 1.9885,
        "grad_norm": 3.0986969470977783,
        "learning_rate": 7.934497752102587e-05,
        "epoch": 0.1904016692749087,
        "step": 2555
    },
    {
        "loss": 2.5706,
        "grad_norm": 1.8981600999832153,
        "learning_rate": 7.927613433467293e-05,
        "epoch": 0.19047619047619047,
        "step": 2556
    },
    {
        "loss": 2.3265,
        "grad_norm": 2.953666925430298,
        "learning_rate": 7.920730140938447e-05,
        "epoch": 0.19055071167747223,
        "step": 2557
    },
    {
        "loss": 1.4267,
        "grad_norm": 2.975358486175537,
        "learning_rate": 7.913847877924187e-05,
        "epoch": 0.190625232878754,
        "step": 2558
    },
    {
        "loss": 2.3756,
        "grad_norm": 2.19205641746521,
        "learning_rate": 7.906966647832154e-05,
        "epoch": 0.19069975408003578,
        "step": 2559
    },
    {
        "loss": 1.6666,
        "grad_norm": 3.770606517791748,
        "learning_rate": 7.90008645406947e-05,
        "epoch": 0.19077427528131755,
        "step": 2560
    },
    {
        "loss": 2.813,
        "grad_norm": 2.2724058628082275,
        "learning_rate": 7.89320730004274e-05,
        "epoch": 0.1908487964825993,
        "step": 2561
    },
    {
        "loss": 1.6239,
        "grad_norm": 3.778409004211426,
        "learning_rate": 7.886329189158057e-05,
        "epoch": 0.19092331768388107,
        "step": 2562
    },
    {
        "loss": 2.4042,
        "grad_norm": 4.337523460388184,
        "learning_rate": 7.879452124821e-05,
        "epoch": 0.19099783888516284,
        "step": 2563
    },
    {
        "loss": 2.2767,
        "grad_norm": 3.205540418624878,
        "learning_rate": 7.872576110436637e-05,
        "epoch": 0.1910723600864446,
        "step": 2564
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.6112169027328491,
        "learning_rate": 7.8657011494095e-05,
        "epoch": 0.19114688128772636,
        "step": 2565
    },
    {
        "loss": 2.4252,
        "grad_norm": 2.908189058303833,
        "learning_rate": 7.85882724514361e-05,
        "epoch": 0.19122140248900812,
        "step": 2566
    },
    {
        "loss": 1.811,
        "grad_norm": 2.503629446029663,
        "learning_rate": 7.85195440104246e-05,
        "epoch": 0.1912959236902899,
        "step": 2567
    },
    {
        "loss": 2.2114,
        "grad_norm": 3.016705274581909,
        "learning_rate": 7.845082620509017e-05,
        "epoch": 0.19137044489157165,
        "step": 2568
    },
    {
        "loss": 2.6138,
        "grad_norm": 2.7835280895233154,
        "learning_rate": 7.838211906945732e-05,
        "epoch": 0.1914449660928534,
        "step": 2569
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.361769199371338,
        "learning_rate": 7.831342263754515e-05,
        "epoch": 0.19151948729413518,
        "step": 2570
    },
    {
        "loss": 2.4588,
        "grad_norm": 2.174908399581909,
        "learning_rate": 7.824473694336755e-05,
        "epoch": 0.19159400849541694,
        "step": 2571
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.336845874786377,
        "learning_rate": 7.817606202093306e-05,
        "epoch": 0.1916685296966987,
        "step": 2572
    },
    {
        "loss": 2.24,
        "grad_norm": 3.36496901512146,
        "learning_rate": 7.810739790424481e-05,
        "epoch": 0.19174305089798047,
        "step": 2573
    },
    {
        "loss": 2.586,
        "grad_norm": 3.5716731548309326,
        "learning_rate": 7.803874462730072e-05,
        "epoch": 0.19181757209926223,
        "step": 2574
    },
    {
        "loss": 2.5593,
        "grad_norm": 3.1319456100463867,
        "learning_rate": 7.797010222409329e-05,
        "epoch": 0.191892093300544,
        "step": 2575
    },
    {
        "loss": 2.1669,
        "grad_norm": 3.0095038414001465,
        "learning_rate": 7.790147072860956e-05,
        "epoch": 0.19196661450182578,
        "step": 2576
    },
    {
        "loss": 2.0646,
        "grad_norm": 3.399143934249878,
        "learning_rate": 7.783285017483127e-05,
        "epoch": 0.19204113570310755,
        "step": 2577
    },
    {
        "loss": 2.1906,
        "grad_norm": 2.8999710083007812,
        "learning_rate": 7.776424059673471e-05,
        "epoch": 0.1921156569043893,
        "step": 2578
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.3296611309051514,
        "learning_rate": 7.769564202829066e-05,
        "epoch": 0.19219017810567107,
        "step": 2579
    },
    {
        "loss": 2.5618,
        "grad_norm": 1.8688126802444458,
        "learning_rate": 7.762705450346462e-05,
        "epoch": 0.19226469930695284,
        "step": 2580
    },
    {
        "loss": 2.4868,
        "grad_norm": 1.7127419710159302,
        "learning_rate": 7.755847805621646e-05,
        "epoch": 0.1923392205082346,
        "step": 2581
    },
    {
        "loss": 2.6356,
        "grad_norm": 2.042924642562866,
        "learning_rate": 7.748991272050064e-05,
        "epoch": 0.19241374170951636,
        "step": 2582
    },
    {
        "loss": 1.6098,
        "grad_norm": 3.005732774734497,
        "learning_rate": 7.742135853026607e-05,
        "epoch": 0.19248826291079812,
        "step": 2583
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.1230597496032715,
        "learning_rate": 7.73528155194562e-05,
        "epoch": 0.1925627841120799,
        "step": 2584
    },
    {
        "loss": 1.9456,
        "grad_norm": 3.214655876159668,
        "learning_rate": 7.728428372200897e-05,
        "epoch": 0.19263730531336165,
        "step": 2585
    },
    {
        "loss": 1.9872,
        "grad_norm": 3.2169055938720703,
        "learning_rate": 7.721576317185659e-05,
        "epoch": 0.19271182651464341,
        "step": 2586
    },
    {
        "loss": 1.6925,
        "grad_norm": 3.953545331954956,
        "learning_rate": 7.714725390292596e-05,
        "epoch": 0.19278634771592518,
        "step": 2587
    },
    {
        "loss": 2.3482,
        "grad_norm": 3.919415235519409,
        "learning_rate": 7.70787559491382e-05,
        "epoch": 0.19286086891720694,
        "step": 2588
    },
    {
        "loss": 2.442,
        "grad_norm": 4.593941688537598,
        "learning_rate": 7.701026934440886e-05,
        "epoch": 0.1929353901184887,
        "step": 2589
    },
    {
        "loss": 1.9386,
        "grad_norm": 3.8522276878356934,
        "learning_rate": 7.694179412264799e-05,
        "epoch": 0.19300991131977047,
        "step": 2590
    },
    {
        "loss": 2.774,
        "grad_norm": 2.0989468097686768,
        "learning_rate": 7.687333031775982e-05,
        "epoch": 0.19308443252105223,
        "step": 2591
    },
    {
        "loss": 2.1112,
        "grad_norm": 4.315040111541748,
        "learning_rate": 7.680487796364313e-05,
        "epoch": 0.193158953722334,
        "step": 2592
    },
    {
        "loss": 2.2382,
        "grad_norm": 3.8760552406311035,
        "learning_rate": 7.673643709419078e-05,
        "epoch": 0.19323347492361576,
        "step": 2593
    },
    {
        "loss": 2.3896,
        "grad_norm": 1.432633638381958,
        "learning_rate": 7.666800774329015e-05,
        "epoch": 0.19330799612489755,
        "step": 2594
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.9444310665130615,
        "learning_rate": 7.659958994482295e-05,
        "epoch": 0.1933825173261793,
        "step": 2595
    },
    {
        "loss": 3.151,
        "grad_norm": 1.5487828254699707,
        "learning_rate": 7.65311837326649e-05,
        "epoch": 0.19345703852746107,
        "step": 2596
    },
    {
        "loss": 3.2026,
        "grad_norm": 2.912569522857666,
        "learning_rate": 7.646278914068631e-05,
        "epoch": 0.19353155972874284,
        "step": 2597
    },
    {
        "loss": 2.5124,
        "grad_norm": 2.0813393592834473,
        "learning_rate": 7.639440620275145e-05,
        "epoch": 0.1936060809300246,
        "step": 2598
    },
    {
        "loss": 2.3636,
        "grad_norm": 2.945195198059082,
        "learning_rate": 7.632603495271905e-05,
        "epoch": 0.19368060213130636,
        "step": 2599
    },
    {
        "loss": 2.9458,
        "grad_norm": 3.267650604248047,
        "learning_rate": 7.625767542444192e-05,
        "epoch": 0.19375512333258813,
        "step": 2600
    },
    {
        "loss": 2.5282,
        "grad_norm": 3.5020639896392822,
        "learning_rate": 7.618932765176706e-05,
        "epoch": 0.1938296445338699,
        "step": 2601
    },
    {
        "loss": 2.2163,
        "grad_norm": 3.8815083503723145,
        "learning_rate": 7.612099166853579e-05,
        "epoch": 0.19390416573515165,
        "step": 2602
    },
    {
        "loss": 2.6069,
        "grad_norm": 2.4754233360290527,
        "learning_rate": 7.60526675085834e-05,
        "epoch": 0.19397868693643341,
        "step": 2603
    },
    {
        "loss": 1.5292,
        "grad_norm": 4.056480884552002,
        "learning_rate": 7.598435520573944e-05,
        "epoch": 0.19405320813771518,
        "step": 2604
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.8124791383743286,
        "learning_rate": 7.59160547938276e-05,
        "epoch": 0.19412772933899694,
        "step": 2605
    },
    {
        "loss": 2.4481,
        "grad_norm": 2.8367183208465576,
        "learning_rate": 7.584776630666563e-05,
        "epoch": 0.1942022505402787,
        "step": 2606
    },
    {
        "loss": 2.517,
        "grad_norm": 3.089911699295044,
        "learning_rate": 7.577948977806542e-05,
        "epoch": 0.19427677174156047,
        "step": 2607
    },
    {
        "loss": 2.357,
        "grad_norm": 2.728215456008911,
        "learning_rate": 7.571122524183283e-05,
        "epoch": 0.19435129294284223,
        "step": 2608
    },
    {
        "loss": 2.5429,
        "grad_norm": 3.142951488494873,
        "learning_rate": 7.564297273176796e-05,
        "epoch": 0.194425814144124,
        "step": 2609
    },
    {
        "loss": 2.8422,
        "grad_norm": 2.418609380722046,
        "learning_rate": 7.557473228166482e-05,
        "epoch": 0.19450033534540576,
        "step": 2610
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.2924740314483643,
        "learning_rate": 7.550650392531152e-05,
        "epoch": 0.19457485654668752,
        "step": 2611
    },
    {
        "loss": 2.6595,
        "grad_norm": 2.3469724655151367,
        "learning_rate": 7.543828769649014e-05,
        "epoch": 0.1946493777479693,
        "step": 2612
    },
    {
        "loss": 2.7478,
        "grad_norm": 2.8019306659698486,
        "learning_rate": 7.537008362897677e-05,
        "epoch": 0.19472389894925107,
        "step": 2613
    },
    {
        "loss": 2.4246,
        "grad_norm": 2.9895095825195312,
        "learning_rate": 7.530189175654142e-05,
        "epoch": 0.19479842015053284,
        "step": 2614
    },
    {
        "loss": 1.7924,
        "grad_norm": 3.7724056243896484,
        "learning_rate": 7.52337121129482e-05,
        "epoch": 0.1948729413518146,
        "step": 2615
    },
    {
        "loss": 1.986,
        "grad_norm": 2.6169490814208984,
        "learning_rate": 7.516554473195508e-05,
        "epoch": 0.19494746255309636,
        "step": 2616
    },
    {
        "loss": 2.6736,
        "grad_norm": 2.7042806148529053,
        "learning_rate": 7.509738964731389e-05,
        "epoch": 0.19502198375437813,
        "step": 2617
    },
    {
        "loss": 2.3128,
        "grad_norm": 3.5028622150421143,
        "learning_rate": 7.502924689277053e-05,
        "epoch": 0.1950965049556599,
        "step": 2618
    },
    {
        "loss": 1.9727,
        "grad_norm": 3.0890212059020996,
        "learning_rate": 7.49611165020646e-05,
        "epoch": 0.19517102615694165,
        "step": 2619
    },
    {
        "loss": 2.5924,
        "grad_norm": 2.605583667755127,
        "learning_rate": 7.489299850892984e-05,
        "epoch": 0.19524554735822341,
        "step": 2620
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.442293167114258,
        "learning_rate": 7.482489294709359e-05,
        "epoch": 0.19532006855950518,
        "step": 2621
    },
    {
        "loss": 2.1423,
        "grad_norm": 3.0285773277282715,
        "learning_rate": 7.475679985027716e-05,
        "epoch": 0.19539458976078694,
        "step": 2622
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.5175271034240723,
        "learning_rate": 7.468871925219567e-05,
        "epoch": 0.1954691109620687,
        "step": 2623
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.785301923751831,
        "learning_rate": 7.462065118655806e-05,
        "epoch": 0.19554363216335047,
        "step": 2624
    },
    {
        "loss": 2.4605,
        "grad_norm": 2.1023359298706055,
        "learning_rate": 7.455259568706707e-05,
        "epoch": 0.19561815336463223,
        "step": 2625
    },
    {
        "loss": 2.9736,
        "grad_norm": 2.344663143157959,
        "learning_rate": 7.448455278741922e-05,
        "epoch": 0.195692674565914,
        "step": 2626
    },
    {
        "loss": 2.3348,
        "grad_norm": 3.2231199741363525,
        "learning_rate": 7.441652252130475e-05,
        "epoch": 0.19576719576719576,
        "step": 2627
    },
    {
        "loss": 2.4276,
        "grad_norm": 2.4639437198638916,
        "learning_rate": 7.434850492240768e-05,
        "epoch": 0.19584171696847752,
        "step": 2628
    },
    {
        "loss": 1.9196,
        "grad_norm": 2.7871479988098145,
        "learning_rate": 7.428050002440572e-05,
        "epoch": 0.1959162381697593,
        "step": 2629
    },
    {
        "loss": 2.7585,
        "grad_norm": 2.295438766479492,
        "learning_rate": 7.421250786097039e-05,
        "epoch": 0.19599075937104107,
        "step": 2630
    },
    {
        "loss": 2.3607,
        "grad_norm": 2.448032855987549,
        "learning_rate": 7.414452846576675e-05,
        "epoch": 0.19606528057232284,
        "step": 2631
    },
    {
        "loss": 2.4461,
        "grad_norm": 2.8217215538024902,
        "learning_rate": 7.407656187245372e-05,
        "epoch": 0.1961398017736046,
        "step": 2632
    },
    {
        "loss": 2.5535,
        "grad_norm": 3.069389581680298,
        "learning_rate": 7.40086081146837e-05,
        "epoch": 0.19621432297488636,
        "step": 2633
    },
    {
        "loss": 2.5918,
        "grad_norm": 3.3530490398406982,
        "learning_rate": 7.394066722610286e-05,
        "epoch": 0.19628884417616813,
        "step": 2634
    },
    {
        "loss": 2.509,
        "grad_norm": 2.883711576461792,
        "learning_rate": 7.387273924035098e-05,
        "epoch": 0.1963633653774499,
        "step": 2635
    },
    {
        "loss": 2.7278,
        "grad_norm": 2.493450403213501,
        "learning_rate": 7.38048241910614e-05,
        "epoch": 0.19643788657873165,
        "step": 2636
    },
    {
        "loss": 2.4351,
        "grad_norm": 3.240433692932129,
        "learning_rate": 7.37369221118611e-05,
        "epoch": 0.19651240778001342,
        "step": 2637
    },
    {
        "loss": 2.4825,
        "grad_norm": 3.035079002380371,
        "learning_rate": 7.366903303637058e-05,
        "epoch": 0.19658692898129518,
        "step": 2638
    },
    {
        "loss": 2.1772,
        "grad_norm": 2.7679171562194824,
        "learning_rate": 7.360115699820402e-05,
        "epoch": 0.19666145018257694,
        "step": 2639
    },
    {
        "loss": 2.0932,
        "grad_norm": 2.5099709033966064,
        "learning_rate": 7.353329403096897e-05,
        "epoch": 0.1967359713838587,
        "step": 2640
    },
    {
        "loss": 1.9077,
        "grad_norm": 2.5551881790161133,
        "learning_rate": 7.34654441682667e-05,
        "epoch": 0.19681049258514047,
        "step": 2641
    },
    {
        "loss": 2.4723,
        "grad_norm": 2.212505578994751,
        "learning_rate": 7.339760744369188e-05,
        "epoch": 0.19688501378642223,
        "step": 2642
    },
    {
        "loss": 2.3545,
        "grad_norm": 5.335332870483398,
        "learning_rate": 7.332978389083267e-05,
        "epoch": 0.196959534987704,
        "step": 2643
    },
    {
        "loss": 2.6645,
        "grad_norm": 3.1593241691589355,
        "learning_rate": 7.326197354327071e-05,
        "epoch": 0.19703405618898576,
        "step": 2644
    },
    {
        "loss": 2.28,
        "grad_norm": 3.297447919845581,
        "learning_rate": 7.319417643458116e-05,
        "epoch": 0.19710857739026752,
        "step": 2645
    },
    {
        "loss": 2.5408,
        "grad_norm": 1.9813114404678345,
        "learning_rate": 7.312639259833263e-05,
        "epoch": 0.19718309859154928,
        "step": 2646
    },
    {
        "loss": 2.5337,
        "grad_norm": 2.896963119506836,
        "learning_rate": 7.305862206808697e-05,
        "epoch": 0.19725761979283107,
        "step": 2647
    },
    {
        "loss": 2.6045,
        "grad_norm": 1.8834985494613647,
        "learning_rate": 7.299086487739976e-05,
        "epoch": 0.19733214099411284,
        "step": 2648
    },
    {
        "loss": 2.2175,
        "grad_norm": 3.1881585121154785,
        "learning_rate": 7.292312105981973e-05,
        "epoch": 0.1974066621953946,
        "step": 2649
    },
    {
        "loss": 2.731,
        "grad_norm": 1.9502140283584595,
        "learning_rate": 7.2855390648889e-05,
        "epoch": 0.19748118339667636,
        "step": 2650
    },
    {
        "loss": 2.5734,
        "grad_norm": 1.988344669342041,
        "learning_rate": 7.278767367814325e-05,
        "epoch": 0.19755570459795813,
        "step": 2651
    },
    {
        "loss": 2.6762,
        "grad_norm": 1.5723477602005005,
        "learning_rate": 7.271997018111125e-05,
        "epoch": 0.1976302257992399,
        "step": 2652
    },
    {
        "loss": 2.5186,
        "grad_norm": 2.7034151554107666,
        "learning_rate": 7.265228019131531e-05,
        "epoch": 0.19770474700052165,
        "step": 2653
    },
    {
        "loss": 2.5342,
        "grad_norm": 2.7835261821746826,
        "learning_rate": 7.258460374227085e-05,
        "epoch": 0.19777926820180342,
        "step": 2654
    },
    {
        "loss": 1.912,
        "grad_norm": 1.4068858623504639,
        "learning_rate": 7.251694086748675e-05,
        "epoch": 0.19785378940308518,
        "step": 2655
    },
    {
        "loss": 2.2161,
        "grad_norm": 2.7468271255493164,
        "learning_rate": 7.244929160046521e-05,
        "epoch": 0.19792831060436694,
        "step": 2656
    },
    {
        "loss": 3.0612,
        "grad_norm": 2.4209659099578857,
        "learning_rate": 7.238165597470143e-05,
        "epoch": 0.1980028318056487,
        "step": 2657
    },
    {
        "loss": 2.7006,
        "grad_norm": 2.034177541732788,
        "learning_rate": 7.231403402368418e-05,
        "epoch": 0.19807735300693047,
        "step": 2658
    },
    {
        "loss": 2.1425,
        "grad_norm": 3.963890314102173,
        "learning_rate": 7.224642578089516e-05,
        "epoch": 0.19815187420821223,
        "step": 2659
    },
    {
        "loss": 2.7603,
        "grad_norm": 2.783503770828247,
        "learning_rate": 7.217883127980953e-05,
        "epoch": 0.198226395409494,
        "step": 2660
    },
    {
        "loss": 2.8601,
        "grad_norm": 3.7918004989624023,
        "learning_rate": 7.211125055389549e-05,
        "epoch": 0.19830091661077576,
        "step": 2661
    },
    {
        "loss": 2.108,
        "grad_norm": 5.976246356964111,
        "learning_rate": 7.204368363661444e-05,
        "epoch": 0.19837543781205752,
        "step": 2662
    },
    {
        "loss": 1.9967,
        "grad_norm": 3.3528692722320557,
        "learning_rate": 7.197613056142112e-05,
        "epoch": 0.19844995901333928,
        "step": 2663
    },
    {
        "loss": 1.9151,
        "grad_norm": 2.600430727005005,
        "learning_rate": 7.190859136176308e-05,
        "epoch": 0.19852448021462105,
        "step": 2664
    },
    {
        "loss": 2.1749,
        "grad_norm": 3.8884708881378174,
        "learning_rate": 7.18410660710813e-05,
        "epoch": 0.19859900141590284,
        "step": 2665
    },
    {
        "loss": 2.4474,
        "grad_norm": 2.3301968574523926,
        "learning_rate": 7.177355472280974e-05,
        "epoch": 0.1986735226171846,
        "step": 2666
    },
    {
        "loss": 1.9347,
        "grad_norm": 2.6762499809265137,
        "learning_rate": 7.170605735037543e-05,
        "epoch": 0.19874804381846636,
        "step": 2667
    },
    {
        "loss": 2.3929,
        "grad_norm": 1.6995726823806763,
        "learning_rate": 7.163857398719867e-05,
        "epoch": 0.19882256501974813,
        "step": 2668
    },
    {
        "loss": 2.9222,
        "grad_norm": 1.964482069015503,
        "learning_rate": 7.15711046666925e-05,
        "epoch": 0.1988970862210299,
        "step": 2669
    },
    {
        "loss": 2.3711,
        "grad_norm": 2.145338535308838,
        "learning_rate": 7.150364942226332e-05,
        "epoch": 0.19897160742231165,
        "step": 2670
    },
    {
        "loss": 2.7605,
        "grad_norm": 3.0351288318634033,
        "learning_rate": 7.143620828731038e-05,
        "epoch": 0.19904612862359342,
        "step": 2671
    },
    {
        "loss": 2.6062,
        "grad_norm": 1.7903321981430054,
        "learning_rate": 7.1368781295226e-05,
        "epoch": 0.19912064982487518,
        "step": 2672
    },
    {
        "loss": 3.015,
        "grad_norm": 2.058154582977295,
        "learning_rate": 7.13013684793955e-05,
        "epoch": 0.19919517102615694,
        "step": 2673
    },
    {
        "loss": 2.1518,
        "grad_norm": 3.2214159965515137,
        "learning_rate": 7.123396987319714e-05,
        "epoch": 0.1992696922274387,
        "step": 2674
    },
    {
        "loss": 2.6914,
        "grad_norm": 2.4648642539978027,
        "learning_rate": 7.116658551000223e-05,
        "epoch": 0.19934421342872047,
        "step": 2675
    },
    {
        "loss": 2.2042,
        "grad_norm": 2.542961835861206,
        "learning_rate": 7.109921542317496e-05,
        "epoch": 0.19941873463000223,
        "step": 2676
    },
    {
        "loss": 2.6285,
        "grad_norm": 3.2850966453552246,
        "learning_rate": 7.103185964607244e-05,
        "epoch": 0.199493255831284,
        "step": 2677
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.75697922706604,
        "learning_rate": 7.096451821204473e-05,
        "epoch": 0.19956777703256576,
        "step": 2678
    },
    {
        "loss": 2.196,
        "grad_norm": 3.085716962814331,
        "learning_rate": 7.089719115443478e-05,
        "epoch": 0.19964229823384752,
        "step": 2679
    },
    {
        "loss": 2.5327,
        "grad_norm": 3.0102415084838867,
        "learning_rate": 7.08298785065784e-05,
        "epoch": 0.19971681943512928,
        "step": 2680
    },
    {
        "loss": 2.4667,
        "grad_norm": 2.001054048538208,
        "learning_rate": 7.076258030180436e-05,
        "epoch": 0.19979134063641105,
        "step": 2681
    },
    {
        "loss": 2.0605,
        "grad_norm": 3.5419788360595703,
        "learning_rate": 7.069529657343415e-05,
        "epoch": 0.1998658618376928,
        "step": 2682
    },
    {
        "loss": 2.3159,
        "grad_norm": 2.7926652431488037,
        "learning_rate": 7.062802735478214e-05,
        "epoch": 0.1999403830389746,
        "step": 2683
    },
    {
        "loss": 2.1715,
        "grad_norm": 3.775914430618286,
        "learning_rate": 7.056077267915552e-05,
        "epoch": 0.20001490424025636,
        "step": 2684
    },
    {
        "loss": 1.9167,
        "grad_norm": 3.9876112937927246,
        "learning_rate": 7.04935325798543e-05,
        "epoch": 0.20008942544153813,
        "step": 2685
    },
    {
        "loss": 2.4517,
        "grad_norm": 1.8243896961212158,
        "learning_rate": 7.042630709017126e-05,
        "epoch": 0.2001639466428199,
        "step": 2686
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.3322490453720093,
        "learning_rate": 7.035909624339191e-05,
        "epoch": 0.20023846784410165,
        "step": 2687
    },
    {
        "loss": 2.1905,
        "grad_norm": 3.035162925720215,
        "learning_rate": 7.02919000727946e-05,
        "epoch": 0.20031298904538342,
        "step": 2688
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.0667853355407715,
        "learning_rate": 7.022471861165033e-05,
        "epoch": 0.20038751024666518,
        "step": 2689
    },
    {
        "loss": 2.7884,
        "grad_norm": 3.378582715988159,
        "learning_rate": 7.015755189322276e-05,
        "epoch": 0.20046203144794694,
        "step": 2690
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.192249298095703,
        "learning_rate": 7.009039995076844e-05,
        "epoch": 0.2005365526492287,
        "step": 2691
    },
    {
        "loss": 1.9849,
        "grad_norm": 3.1176557540893555,
        "learning_rate": 7.002326281753648e-05,
        "epoch": 0.20061107385051047,
        "step": 2692
    },
    {
        "loss": 2.8268,
        "grad_norm": 2.5481796264648438,
        "learning_rate": 6.995614052676863e-05,
        "epoch": 0.20068559505179223,
        "step": 2693
    },
    {
        "loss": 2.3641,
        "grad_norm": 1.9277846813201904,
        "learning_rate": 6.988903311169936e-05,
        "epoch": 0.200760116253074,
        "step": 2694
    },
    {
        "loss": 2.535,
        "grad_norm": 1.5666002035140991,
        "learning_rate": 6.982194060555569e-05,
        "epoch": 0.20083463745435576,
        "step": 2695
    },
    {
        "loss": 2.3808,
        "grad_norm": 3.5108964443206787,
        "learning_rate": 6.975486304155743e-05,
        "epoch": 0.20090915865563752,
        "step": 2696
    },
    {
        "loss": 2.5411,
        "grad_norm": 1.7613255977630615,
        "learning_rate": 6.968780045291681e-05,
        "epoch": 0.20098367985691928,
        "step": 2697
    },
    {
        "loss": 2.8294,
        "grad_norm": 2.9459340572357178,
        "learning_rate": 6.962075287283873e-05,
        "epoch": 0.20105820105820105,
        "step": 2698
    },
    {
        "loss": 2.3178,
        "grad_norm": 2.460416793823242,
        "learning_rate": 6.95537203345206e-05,
        "epoch": 0.2011327222594828,
        "step": 2699
    },
    {
        "loss": 2.8449,
        "grad_norm": 3.6373486518859863,
        "learning_rate": 6.948670287115248e-05,
        "epoch": 0.2012072434607646,
        "step": 2700
    },
    {
        "loss": 2.304,
        "grad_norm": 2.1532301902770996,
        "learning_rate": 6.941970051591685e-05,
        "epoch": 0.20128176466204636,
        "step": 2701
    },
    {
        "loss": 2.0504,
        "grad_norm": 3.6570489406585693,
        "learning_rate": 6.935271330198884e-05,
        "epoch": 0.20135628586332813,
        "step": 2702
    },
    {
        "loss": 2.2137,
        "grad_norm": 2.639970302581787,
        "learning_rate": 6.928574126253598e-05,
        "epoch": 0.2014308070646099,
        "step": 2703
    },
    {
        "loss": 2.8213,
        "grad_norm": 2.4286208152770996,
        "learning_rate": 6.921878443071834e-05,
        "epoch": 0.20150532826589165,
        "step": 2704
    },
    {
        "loss": 2.3497,
        "grad_norm": 2.895414113998413,
        "learning_rate": 6.915184283968841e-05,
        "epoch": 0.20157984946717342,
        "step": 2705
    },
    {
        "loss": 2.3146,
        "grad_norm": 3.469726800918579,
        "learning_rate": 6.908491652259113e-05,
        "epoch": 0.20165437066845518,
        "step": 2706
    },
    {
        "loss": 2.3961,
        "grad_norm": 2.0713322162628174,
        "learning_rate": 6.9018005512564e-05,
        "epoch": 0.20172889186973694,
        "step": 2707
    },
    {
        "loss": 2.963,
        "grad_norm": 1.8552433252334595,
        "learning_rate": 6.895110984273678e-05,
        "epoch": 0.2018034130710187,
        "step": 2708
    },
    {
        "loss": 2.6581,
        "grad_norm": 1.7959489822387695,
        "learning_rate": 6.888422954623175e-05,
        "epoch": 0.20187793427230047,
        "step": 2709
    },
    {
        "loss": 1.8682,
        "grad_norm": 3.651982307434082,
        "learning_rate": 6.881736465616353e-05,
        "epoch": 0.20195245547358223,
        "step": 2710
    },
    {
        "loss": 2.301,
        "grad_norm": 2.611053943634033,
        "learning_rate": 6.875051520563906e-05,
        "epoch": 0.202026976674864,
        "step": 2711
    },
    {
        "loss": 2.3321,
        "grad_norm": 3.0796873569488525,
        "learning_rate": 6.868368122775779e-05,
        "epoch": 0.20210149787614576,
        "step": 2712
    },
    {
        "loss": 2.2129,
        "grad_norm": 2.9386727809906006,
        "learning_rate": 6.861686275561132e-05,
        "epoch": 0.20217601907742752,
        "step": 2713
    },
    {
        "loss": 2.5552,
        "grad_norm": 3.244293689727783,
        "learning_rate": 6.855005982228371e-05,
        "epoch": 0.20225054027870928,
        "step": 2714
    },
    {
        "loss": 1.7885,
        "grad_norm": 2.0719900131225586,
        "learning_rate": 6.848327246085129e-05,
        "epoch": 0.20232506147999105,
        "step": 2715
    },
    {
        "loss": 2.1913,
        "grad_norm": 3.8364856243133545,
        "learning_rate": 6.841650070438261e-05,
        "epoch": 0.2023995826812728,
        "step": 2716
    },
    {
        "loss": 2.9724,
        "grad_norm": 1.9661767482757568,
        "learning_rate": 6.834974458593866e-05,
        "epoch": 0.20247410388255457,
        "step": 2717
    },
    {
        "loss": 2.5882,
        "grad_norm": 1.7818855047225952,
        "learning_rate": 6.828300413857244e-05,
        "epoch": 0.20254862508383636,
        "step": 2718
    },
    {
        "loss": 2.1267,
        "grad_norm": 2.972130060195923,
        "learning_rate": 6.821627939532946e-05,
        "epoch": 0.20262314628511813,
        "step": 2719
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.9096336364746094,
        "learning_rate": 6.814957038924724e-05,
        "epoch": 0.2026976674863999,
        "step": 2720
    },
    {
        "loss": 2.3053,
        "grad_norm": 2.877502918243408,
        "learning_rate": 6.80828771533556e-05,
        "epoch": 0.20277218868768165,
        "step": 2721
    },
    {
        "loss": 1.949,
        "grad_norm": 2.485244035720825,
        "learning_rate": 6.801619972067662e-05,
        "epoch": 0.20284670988896342,
        "step": 2722
    },
    {
        "loss": 2.6921,
        "grad_norm": 2.482987880706787,
        "learning_rate": 6.794953812422438e-05,
        "epoch": 0.20292123109024518,
        "step": 2723
    },
    {
        "loss": 2.3732,
        "grad_norm": 2.3949055671691895,
        "learning_rate": 6.788289239700534e-05,
        "epoch": 0.20299575229152694,
        "step": 2724
    },
    {
        "loss": 2.2418,
        "grad_norm": 2.4416697025299072,
        "learning_rate": 6.781626257201781e-05,
        "epoch": 0.2030702734928087,
        "step": 2725
    },
    {
        "loss": 2.4321,
        "grad_norm": 2.749288320541382,
        "learning_rate": 6.774964868225256e-05,
        "epoch": 0.20314479469409047,
        "step": 2726
    },
    {
        "loss": 2.2187,
        "grad_norm": 1.973567247390747,
        "learning_rate": 6.768305076069221e-05,
        "epoch": 0.20321931589537223,
        "step": 2727
    },
    {
        "loss": 2.8295,
        "grad_norm": 1.6858305931091309,
        "learning_rate": 6.761646884031162e-05,
        "epoch": 0.203293837096654,
        "step": 2728
    },
    {
        "loss": 1.9059,
        "grad_norm": 3.1607377529144287,
        "learning_rate": 6.75499029540777e-05,
        "epoch": 0.20336835829793576,
        "step": 2729
    },
    {
        "loss": 1.8893,
        "grad_norm": 3.287886381149292,
        "learning_rate": 6.74833531349493e-05,
        "epoch": 0.20344287949921752,
        "step": 2730
    },
    {
        "loss": 2.3411,
        "grad_norm": 3.066998243331909,
        "learning_rate": 6.741681941587752e-05,
        "epoch": 0.20351740070049928,
        "step": 2731
    },
    {
        "loss": 2.7312,
        "grad_norm": 1.6301296949386597,
        "learning_rate": 6.735030182980535e-05,
        "epoch": 0.20359192190178105,
        "step": 2732
    },
    {
        "loss": 2.1825,
        "grad_norm": 3.232940912246704,
        "learning_rate": 6.728380040966782e-05,
        "epoch": 0.2036664431030628,
        "step": 2733
    },
    {
        "loss": 2.1995,
        "grad_norm": 5.215601921081543,
        "learning_rate": 6.721731518839191e-05,
        "epoch": 0.20374096430434457,
        "step": 2734
    },
    {
        "loss": 2.1445,
        "grad_norm": 3.656963348388672,
        "learning_rate": 6.715084619889671e-05,
        "epoch": 0.20381548550562634,
        "step": 2735
    },
    {
        "loss": 2.7468,
        "grad_norm": 2.2285542488098145,
        "learning_rate": 6.708439347409315e-05,
        "epoch": 0.20389000670690813,
        "step": 2736
    },
    {
        "loss": 2.4294,
        "grad_norm": 2.811154842376709,
        "learning_rate": 6.701795704688418e-05,
        "epoch": 0.2039645279081899,
        "step": 2737
    },
    {
        "loss": 2.1761,
        "grad_norm": 3.2077038288116455,
        "learning_rate": 6.695153695016463e-05,
        "epoch": 0.20403904910947165,
        "step": 2738
    },
    {
        "loss": 2.5348,
        "grad_norm": 2.7506322860717773,
        "learning_rate": 6.688513321682126e-05,
        "epoch": 0.20411357031075342,
        "step": 2739
    },
    {
        "loss": 3.3656,
        "grad_norm": 4.34564208984375,
        "learning_rate": 6.681874587973273e-05,
        "epoch": 0.20418809151203518,
        "step": 2740
    },
    {
        "loss": 2.2379,
        "grad_norm": 2.608651638031006,
        "learning_rate": 6.675237497176958e-05,
        "epoch": 0.20426261271331694,
        "step": 2741
    },
    {
        "loss": 2.708,
        "grad_norm": 2.524697780609131,
        "learning_rate": 6.668602052579424e-05,
        "epoch": 0.2043371339145987,
        "step": 2742
    },
    {
        "loss": 2.2088,
        "grad_norm": 2.8796279430389404,
        "learning_rate": 6.661968257466098e-05,
        "epoch": 0.20441165511588047,
        "step": 2743
    },
    {
        "loss": 2.5565,
        "grad_norm": 3.3127827644348145,
        "learning_rate": 6.655336115121589e-05,
        "epoch": 0.20448617631716223,
        "step": 2744
    },
    {
        "loss": 2.594,
        "grad_norm": 2.518425226211548,
        "learning_rate": 6.648705628829685e-05,
        "epoch": 0.204560697518444,
        "step": 2745
    },
    {
        "loss": 2.7488,
        "grad_norm": 2.2401187419891357,
        "learning_rate": 6.642076801873353e-05,
        "epoch": 0.20463521871972576,
        "step": 2746
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.714395523071289,
        "learning_rate": 6.635449637534752e-05,
        "epoch": 0.20470973992100752,
        "step": 2747
    },
    {
        "loss": 2.5159,
        "grad_norm": 2.779554605484009,
        "learning_rate": 6.628824139095203e-05,
        "epoch": 0.20478426112228929,
        "step": 2748
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.084068536758423,
        "learning_rate": 6.622200309835206e-05,
        "epoch": 0.20485878232357105,
        "step": 2749
    },
    {
        "loss": 2.7081,
        "grad_norm": 2.777042865753174,
        "learning_rate": 6.615578153034441e-05,
        "epoch": 0.2049333035248528,
        "step": 2750
    },
    {
        "loss": 2.0556,
        "grad_norm": 2.581353187561035,
        "learning_rate": 6.608957671971744e-05,
        "epoch": 0.20500782472613457,
        "step": 2751
    },
    {
        "loss": 2.1724,
        "grad_norm": 2.0869534015655518,
        "learning_rate": 6.602338869925147e-05,
        "epoch": 0.20508234592741634,
        "step": 2752
    },
    {
        "loss": 2.8187,
        "grad_norm": 2.947175979614258,
        "learning_rate": 6.595721750171825e-05,
        "epoch": 0.20515686712869813,
        "step": 2753
    },
    {
        "loss": 2.4905,
        "grad_norm": 3.6113359928131104,
        "learning_rate": 6.589106315988133e-05,
        "epoch": 0.2052313883299799,
        "step": 2754
    },
    {
        "loss": 2.1359,
        "grad_norm": 3.2958498001098633,
        "learning_rate": 6.582492570649587e-05,
        "epoch": 0.20530590953126165,
        "step": 2755
    },
    {
        "loss": 1.2642,
        "grad_norm": 4.80462121963501,
        "learning_rate": 6.575880517430874e-05,
        "epoch": 0.20538043073254342,
        "step": 2756
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.058288812637329,
        "learning_rate": 6.569270159605834e-05,
        "epoch": 0.20545495193382518,
        "step": 2757
    },
    {
        "loss": 3.0767,
        "grad_norm": 3.1340184211730957,
        "learning_rate": 6.562661500447474e-05,
        "epoch": 0.20552947313510694,
        "step": 2758
    },
    {
        "loss": 1.7826,
        "grad_norm": 3.5028045177459717,
        "learning_rate": 6.556054543227956e-05,
        "epoch": 0.2056039943363887,
        "step": 2759
    },
    {
        "loss": 2.2166,
        "grad_norm": 2.6770057678222656,
        "learning_rate": 6.549449291218602e-05,
        "epoch": 0.20567851553767047,
        "step": 2760
    },
    {
        "loss": 1.9063,
        "grad_norm": 2.8422818183898926,
        "learning_rate": 6.542845747689884e-05,
        "epoch": 0.20575303673895223,
        "step": 2761
    },
    {
        "loss": 2.4466,
        "grad_norm": 1.69176185131073,
        "learning_rate": 6.53624391591144e-05,
        "epoch": 0.205827557940234,
        "step": 2762
    },
    {
        "loss": 2.185,
        "grad_norm": 2.5410776138305664,
        "learning_rate": 6.529643799152049e-05,
        "epoch": 0.20590207914151576,
        "step": 2763
    },
    {
        "loss": 1.6848,
        "grad_norm": 2.7294764518737793,
        "learning_rate": 6.523045400679646e-05,
        "epoch": 0.20597660034279752,
        "step": 2764
    },
    {
        "loss": 1.5896,
        "grad_norm": 3.449068784713745,
        "learning_rate": 6.516448723761315e-05,
        "epoch": 0.20605112154407929,
        "step": 2765
    },
    {
        "loss": 2.508,
        "grad_norm": 3.3624966144561768,
        "learning_rate": 6.509853771663284e-05,
        "epoch": 0.20612564274536105,
        "step": 2766
    },
    {
        "loss": 2.4215,
        "grad_norm": 1.4498517513275146,
        "learning_rate": 6.503260547650927e-05,
        "epoch": 0.2062001639466428,
        "step": 2767
    },
    {
        "loss": 2.7961,
        "grad_norm": 3.4848315715789795,
        "learning_rate": 6.496669054988773e-05,
        "epoch": 0.20627468514792457,
        "step": 2768
    },
    {
        "loss": 2.829,
        "grad_norm": 1.808854579925537,
        "learning_rate": 6.490079296940484e-05,
        "epoch": 0.20634920634920634,
        "step": 2769
    },
    {
        "loss": 2.7619,
        "grad_norm": 2.480591058731079,
        "learning_rate": 6.483491276768858e-05,
        "epoch": 0.2064237275504881,
        "step": 2770
    },
    {
        "loss": 2.5463,
        "grad_norm": 1.9036734104156494,
        "learning_rate": 6.476904997735849e-05,
        "epoch": 0.2064982487517699,
        "step": 2771
    },
    {
        "loss": 2.1655,
        "grad_norm": 2.8135249614715576,
        "learning_rate": 6.47032046310253e-05,
        "epoch": 0.20657276995305165,
        "step": 2772
    },
    {
        "loss": 2.5527,
        "grad_norm": 1.980072259902954,
        "learning_rate": 6.463737676129133e-05,
        "epoch": 0.20664729115433342,
        "step": 2773
    },
    {
        "loss": 2.6305,
        "grad_norm": 3.593367099761963,
        "learning_rate": 6.457156640074994e-05,
        "epoch": 0.20672181235561518,
        "step": 2774
    },
    {
        "loss": 2.308,
        "grad_norm": 2.2789108753204346,
        "learning_rate": 6.450577358198613e-05,
        "epoch": 0.20679633355689694,
        "step": 2775
    },
    {
        "loss": 2.4975,
        "grad_norm": 2.9489500522613525,
        "learning_rate": 6.443999833757602e-05,
        "epoch": 0.2068708547581787,
        "step": 2776
    },
    {
        "loss": 1.9937,
        "grad_norm": 4.002600193023682,
        "learning_rate": 6.437424070008709e-05,
        "epoch": 0.20694537595946047,
        "step": 2777
    },
    {
        "loss": 2.4142,
        "grad_norm": 2.400561571121216,
        "learning_rate": 6.430850070207814e-05,
        "epoch": 0.20701989716074223,
        "step": 2778
    },
    {
        "loss": 2.3669,
        "grad_norm": 1.6782904863357544,
        "learning_rate": 6.424277837609913e-05,
        "epoch": 0.207094418362024,
        "step": 2779
    },
    {
        "loss": 1.9749,
        "grad_norm": 3.3829660415649414,
        "learning_rate": 6.417707375469142e-05,
        "epoch": 0.20716893956330576,
        "step": 2780
    },
    {
        "loss": 1.8307,
        "grad_norm": 3.1363396644592285,
        "learning_rate": 6.411138687038742e-05,
        "epoch": 0.20724346076458752,
        "step": 2781
    },
    {
        "loss": 3.2082,
        "grad_norm": 3.557955026626587,
        "learning_rate": 6.40457177557109e-05,
        "epoch": 0.20731798196586929,
        "step": 2782
    },
    {
        "loss": 2.0739,
        "grad_norm": 2.7521424293518066,
        "learning_rate": 6.398006644317687e-05,
        "epoch": 0.20739250316715105,
        "step": 2783
    },
    {
        "loss": 2.4919,
        "grad_norm": 1.7061951160430908,
        "learning_rate": 6.391443296529129e-05,
        "epoch": 0.2074670243684328,
        "step": 2784
    },
    {
        "loss": 2.4356,
        "grad_norm": 2.8947997093200684,
        "learning_rate": 6.384881735455161e-05,
        "epoch": 0.20754154556971458,
        "step": 2785
    },
    {
        "loss": 1.758,
        "grad_norm": 4.831747055053711,
        "learning_rate": 6.378321964344611e-05,
        "epoch": 0.20761606677099634,
        "step": 2786
    },
    {
        "loss": 2.1145,
        "grad_norm": 4.372970104217529,
        "learning_rate": 6.371763986445447e-05,
        "epoch": 0.2076905879722781,
        "step": 2787
    },
    {
        "loss": 2.9163,
        "grad_norm": 2.5200936794281006,
        "learning_rate": 6.365207805004736e-05,
        "epoch": 0.20776510917355986,
        "step": 2788
    },
    {
        "loss": 2.1571,
        "grad_norm": 2.8657538890838623,
        "learning_rate": 6.358653423268653e-05,
        "epoch": 0.20783963037484166,
        "step": 2789
    },
    {
        "loss": 2.7958,
        "grad_norm": 2.856093645095825,
        "learning_rate": 6.352100844482496e-05,
        "epoch": 0.20791415157612342,
        "step": 2790
    },
    {
        "loss": 2.0928,
        "grad_norm": 3.2027125358581543,
        "learning_rate": 6.345550071890652e-05,
        "epoch": 0.20798867277740518,
        "step": 2791
    },
    {
        "loss": 3.0838,
        "grad_norm": 2.442070722579956,
        "learning_rate": 6.339001108736632e-05,
        "epoch": 0.20806319397868694,
        "step": 2792
    },
    {
        "loss": 2.6166,
        "grad_norm": 3.572772979736328,
        "learning_rate": 6.332453958263039e-05,
        "epoch": 0.2081377151799687,
        "step": 2793
    },
    {
        "loss": 2.4013,
        "grad_norm": 3.5337271690368652,
        "learning_rate": 6.325908623711578e-05,
        "epoch": 0.20821223638125047,
        "step": 2794
    },
    {
        "loss": 2.6314,
        "grad_norm": 2.9496941566467285,
        "learning_rate": 6.319365108323059e-05,
        "epoch": 0.20828675758253223,
        "step": 2795
    },
    {
        "loss": 2.1639,
        "grad_norm": 3.5844802856445312,
        "learning_rate": 6.312823415337395e-05,
        "epoch": 0.208361278783814,
        "step": 2796
    },
    {
        "loss": 2.0793,
        "grad_norm": 2.9191319942474365,
        "learning_rate": 6.306283547993587e-05,
        "epoch": 0.20843579998509576,
        "step": 2797
    },
    {
        "loss": 2.2337,
        "grad_norm": 3.043478012084961,
        "learning_rate": 6.299745509529745e-05,
        "epoch": 0.20851032118637752,
        "step": 2798
    },
    {
        "loss": 2.5391,
        "grad_norm": 2.7575764656066895,
        "learning_rate": 6.29320930318306e-05,
        "epoch": 0.20858484238765929,
        "step": 2799
    },
    {
        "loss": 2.0503,
        "grad_norm": 3.1873631477355957,
        "learning_rate": 6.286674932189824e-05,
        "epoch": 0.20865936358894105,
        "step": 2800
    },
    {
        "loss": 2.3031,
        "grad_norm": 2.0353407859802246,
        "learning_rate": 6.280142399785416e-05,
        "epoch": 0.2087338847902228,
        "step": 2801
    },
    {
        "loss": 2.6368,
        "grad_norm": 3.4588263034820557,
        "learning_rate": 6.273611709204304e-05,
        "epoch": 0.20880840599150458,
        "step": 2802
    },
    {
        "loss": 2.4346,
        "grad_norm": 2.1955103874206543,
        "learning_rate": 6.267082863680056e-05,
        "epoch": 0.20888292719278634,
        "step": 2803
    },
    {
        "loss": 2.6049,
        "grad_norm": 4.18366813659668,
        "learning_rate": 6.260555866445309e-05,
        "epoch": 0.2089574483940681,
        "step": 2804
    },
    {
        "loss": 2.0056,
        "grad_norm": 3.297330379486084,
        "learning_rate": 6.254030720731798e-05,
        "epoch": 0.20903196959534986,
        "step": 2805
    },
    {
        "loss": 2.6371,
        "grad_norm": 2.5801737308502197,
        "learning_rate": 6.247507429770334e-05,
        "epoch": 0.20910649079663166,
        "step": 2806
    },
    {
        "loss": 2.8157,
        "grad_norm": 2.134763717651367,
        "learning_rate": 6.240985996790809e-05,
        "epoch": 0.20918101199791342,
        "step": 2807
    },
    {
        "loss": 1.3556,
        "grad_norm": 1.9112086296081543,
        "learning_rate": 6.234466425022204e-05,
        "epoch": 0.20925553319919518,
        "step": 2808
    },
    {
        "loss": 2.8648,
        "grad_norm": 3.40570068359375,
        "learning_rate": 6.22794871769257e-05,
        "epoch": 0.20933005440047694,
        "step": 2809
    },
    {
        "loss": 2.5499,
        "grad_norm": 3.535804271697998,
        "learning_rate": 6.221432878029036e-05,
        "epoch": 0.2094045756017587,
        "step": 2810
    },
    {
        "loss": 2.5393,
        "grad_norm": 2.022904872894287,
        "learning_rate": 6.214918909257808e-05,
        "epoch": 0.20947909680304047,
        "step": 2811
    },
    {
        "loss": 2.6254,
        "grad_norm": 3.028759479522705,
        "learning_rate": 6.208406814604164e-05,
        "epoch": 0.20955361800432223,
        "step": 2812
    },
    {
        "loss": 2.6937,
        "grad_norm": 2.1900768280029297,
        "learning_rate": 6.20189659729246e-05,
        "epoch": 0.209628139205604,
        "step": 2813
    },
    {
        "loss": 1.7796,
        "grad_norm": 3.3562440872192383,
        "learning_rate": 6.195388260546115e-05,
        "epoch": 0.20970266040688576,
        "step": 2814
    },
    {
        "loss": 2.2155,
        "grad_norm": 2.8234879970550537,
        "learning_rate": 6.18888180758762e-05,
        "epoch": 0.20977718160816752,
        "step": 2815
    },
    {
        "loss": 2.7564,
        "grad_norm": 1.6059801578521729,
        "learning_rate": 6.18237724163853e-05,
        "epoch": 0.2098517028094493,
        "step": 2816
    },
    {
        "loss": 2.4455,
        "grad_norm": 3.0045721530914307,
        "learning_rate": 6.175874565919471e-05,
        "epoch": 0.20992622401073105,
        "step": 2817
    },
    {
        "loss": 3.4904,
        "grad_norm": 3.5441360473632812,
        "learning_rate": 6.16937378365013e-05,
        "epoch": 0.2100007452120128,
        "step": 2818
    },
    {
        "loss": 0.6923,
        "grad_norm": 2.9943606853485107,
        "learning_rate": 6.162874898049259e-05,
        "epoch": 0.21007526641329458,
        "step": 2819
    },
    {
        "loss": 2.8617,
        "grad_norm": 2.475891590118408,
        "learning_rate": 6.156377912334668e-05,
        "epoch": 0.21014978761457634,
        "step": 2820
    },
    {
        "loss": 2.0859,
        "grad_norm": 3.1057932376861572,
        "learning_rate": 6.149882829723228e-05,
        "epoch": 0.2102243088158581,
        "step": 2821
    },
    {
        "loss": 2.8884,
        "grad_norm": 3.280656337738037,
        "learning_rate": 6.143389653430862e-05,
        "epoch": 0.21029883001713987,
        "step": 2822
    },
    {
        "loss": 2.8834,
        "grad_norm": 1.4319758415222168,
        "learning_rate": 6.136898386672562e-05,
        "epoch": 0.21037335121842163,
        "step": 2823
    },
    {
        "loss": 1.1716,
        "grad_norm": 3.6071794033050537,
        "learning_rate": 6.130409032662366e-05,
        "epoch": 0.21044787241970342,
        "step": 2824
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.9051382541656494,
        "learning_rate": 6.123921594613358e-05,
        "epoch": 0.21052239362098518,
        "step": 2825
    },
    {
        "loss": 1.3272,
        "grad_norm": 3.2158546447753906,
        "learning_rate": 6.117436075737689e-05,
        "epoch": 0.21059691482226695,
        "step": 2826
    },
    {
        "loss": 2.4642,
        "grad_norm": 1.6123770475387573,
        "learning_rate": 6.110952479246545e-05,
        "epoch": 0.2106714360235487,
        "step": 2827
    },
    {
        "loss": 2.33,
        "grad_norm": 3.169011116027832,
        "learning_rate": 6.10447080835017e-05,
        "epoch": 0.21074595722483047,
        "step": 2828
    },
    {
        "loss": 2.6942,
        "grad_norm": 1.7451927661895752,
        "learning_rate": 6.0979910662578556e-05,
        "epoch": 0.21082047842611223,
        "step": 2829
    },
    {
        "loss": 2.4983,
        "grad_norm": 3.1243157386779785,
        "learning_rate": 6.09151325617793e-05,
        "epoch": 0.210894999627394,
        "step": 2830
    },
    {
        "loss": 1.7481,
        "grad_norm": 3.0251317024230957,
        "learning_rate": 6.0850373813177686e-05,
        "epoch": 0.21096952082867576,
        "step": 2831
    },
    {
        "loss": 2.5057,
        "grad_norm": 2.718083143234253,
        "learning_rate": 6.0785634448837916e-05,
        "epoch": 0.21104404202995752,
        "step": 2832
    },
    {
        "loss": 2.8624,
        "grad_norm": 2.587334156036377,
        "learning_rate": 6.0720914500814507e-05,
        "epoch": 0.2111185632312393,
        "step": 2833
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.530703544616699,
        "learning_rate": 6.0656214001152556e-05,
        "epoch": 0.21119308443252105,
        "step": 2834
    },
    {
        "loss": 1.9764,
        "grad_norm": 3.3115193843841553,
        "learning_rate": 6.0591532981887245e-05,
        "epoch": 0.2112676056338028,
        "step": 2835
    },
    {
        "loss": 2.8708,
        "grad_norm": 2.5711066722869873,
        "learning_rate": 6.0526871475044367e-05,
        "epoch": 0.21134212683508458,
        "step": 2836
    },
    {
        "loss": 2.215,
        "grad_norm": 1.9394335746765137,
        "learning_rate": 6.0462229512639937e-05,
        "epoch": 0.21141664803636634,
        "step": 2837
    },
    {
        "loss": 2.576,
        "grad_norm": 2.6610474586486816,
        "learning_rate": 6.039760712668023e-05,
        "epoch": 0.2114911692376481,
        "step": 2838
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.745643973350525,
        "learning_rate": 6.033300434916203e-05,
        "epoch": 0.21156569043892987,
        "step": 2839
    },
    {
        "loss": 2.659,
        "grad_norm": 2.025057077407837,
        "learning_rate": 6.026842121207216e-05,
        "epoch": 0.21164021164021163,
        "step": 2840
    },
    {
        "loss": 2.5546,
        "grad_norm": 2.8542380332946777,
        "learning_rate": 6.0203857747387995e-05,
        "epoch": 0.2117147328414934,
        "step": 2841
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.3963119983673096,
        "learning_rate": 6.013931398707684e-05,
        "epoch": 0.21178925404277518,
        "step": 2842
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.9923784732818604,
        "learning_rate": 6.0074789963096525e-05,
        "epoch": 0.21186377524405695,
        "step": 2843
    },
    {
        "loss": 1.7498,
        "grad_norm": 3.2047133445739746,
        "learning_rate": 6.001028570739506e-05,
        "epoch": 0.2119382964453387,
        "step": 2844
    },
    {
        "loss": 1.6997,
        "grad_norm": 3.9190406799316406,
        "learning_rate": 5.994580125191051e-05,
        "epoch": 0.21201281764662047,
        "step": 2845
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.7430624961853027,
        "learning_rate": 5.9881336628571336e-05,
        "epoch": 0.21208733884790223,
        "step": 2846
    },
    {
        "loss": 3.0654,
        "grad_norm": 2.889767646789551,
        "learning_rate": 5.981689186929598e-05,
        "epoch": 0.212161860049184,
        "step": 2847
    },
    {
        "loss": 2.4509,
        "grad_norm": 2.9318859577178955,
        "learning_rate": 5.9752467005993216e-05,
        "epoch": 0.21223638125046576,
        "step": 2848
    },
    {
        "loss": 2.6109,
        "grad_norm": 3.1512458324432373,
        "learning_rate": 5.9688062070561915e-05,
        "epoch": 0.21231090245174752,
        "step": 2849
    },
    {
        "loss": 2.4392,
        "grad_norm": 2.9099535942077637,
        "learning_rate": 5.9623677094890994e-05,
        "epoch": 0.2123854236530293,
        "step": 2850
    },
    {
        "loss": 2.5605,
        "grad_norm": 2.4536373615264893,
        "learning_rate": 5.9559312110859676e-05,
        "epoch": 0.21245994485431105,
        "step": 2851
    },
    {
        "loss": 1.6514,
        "grad_norm": 2.507460117340088,
        "learning_rate": 5.9494967150337054e-05,
        "epoch": 0.2125344660555928,
        "step": 2852
    },
    {
        "loss": 2.094,
        "grad_norm": 2.9898669719696045,
        "learning_rate": 5.943064224518253e-05,
        "epoch": 0.21260898725687458,
        "step": 2853
    },
    {
        "loss": 2.5384,
        "grad_norm": 3.5709495544433594,
        "learning_rate": 5.936633742724541e-05,
        "epoch": 0.21268350845815634,
        "step": 2854
    },
    {
        "loss": 2.4392,
        "grad_norm": 3.182039976119995,
        "learning_rate": 5.9302052728365086e-05,
        "epoch": 0.2127580296594381,
        "step": 2855
    },
    {
        "loss": 2.0805,
        "grad_norm": 3.8130576610565186,
        "learning_rate": 5.923778818037116e-05,
        "epoch": 0.21283255086071987,
        "step": 2856
    },
    {
        "loss": 2.3669,
        "grad_norm": 5.271152496337891,
        "learning_rate": 5.917354381508291e-05,
        "epoch": 0.21290707206200163,
        "step": 2857
    },
    {
        "loss": 2.0264,
        "grad_norm": 3.6061019897460938,
        "learning_rate": 5.910931966430997e-05,
        "epoch": 0.2129815932632834,
        "step": 2858
    },
    {
        "loss": 2.877,
        "grad_norm": 2.5597972869873047,
        "learning_rate": 5.9045115759851765e-05,
        "epoch": 0.21305611446456516,
        "step": 2859
    },
    {
        "loss": 2.4465,
        "grad_norm": 3.9202282428741455,
        "learning_rate": 5.898093213349778e-05,
        "epoch": 0.21313063566584695,
        "step": 2860
    },
    {
        "loss": 2.5616,
        "grad_norm": 3.7169978618621826,
        "learning_rate": 5.8916768817027426e-05,
        "epoch": 0.2132051568671287,
        "step": 2861
    },
    {
        "loss": 1.9603,
        "grad_norm": 3.161341905593872,
        "learning_rate": 5.8852625842209984e-05,
        "epoch": 0.21327967806841047,
        "step": 2862
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.880018711090088,
        "learning_rate": 5.878850324080487e-05,
        "epoch": 0.21335419926969224,
        "step": 2863
    },
    {
        "loss": 2.3118,
        "grad_norm": 2.7559962272644043,
        "learning_rate": 5.87244010445612e-05,
        "epoch": 0.213428720470974,
        "step": 2864
    },
    {
        "loss": 2.6611,
        "grad_norm": 1.745188593864441,
        "learning_rate": 5.866031928521809e-05,
        "epoch": 0.21350324167225576,
        "step": 2865
    },
    {
        "loss": 1.9064,
        "grad_norm": 3.9381377696990967,
        "learning_rate": 5.859625799450452e-05,
        "epoch": 0.21357776287353752,
        "step": 2866
    },
    {
        "loss": 1.9058,
        "grad_norm": 5.2444915771484375,
        "learning_rate": 5.853221720413935e-05,
        "epoch": 0.2136522840748193,
        "step": 2867
    },
    {
        "loss": 2.9226,
        "grad_norm": 3.5572452545166016,
        "learning_rate": 5.84681969458312e-05,
        "epoch": 0.21372680527610105,
        "step": 2868
    },
    {
        "loss": 2.5797,
        "grad_norm": 2.7638657093048096,
        "learning_rate": 5.840419725127868e-05,
        "epoch": 0.2138013264773828,
        "step": 2869
    },
    {
        "loss": 1.6864,
        "grad_norm": 3.1637818813323975,
        "learning_rate": 5.83402181521701e-05,
        "epoch": 0.21387584767866458,
        "step": 2870
    },
    {
        "loss": 2.555,
        "grad_norm": 2.0341286659240723,
        "learning_rate": 5.827625968018364e-05,
        "epoch": 0.21395036887994634,
        "step": 2871
    },
    {
        "loss": 2.3872,
        "grad_norm": 3.3333077430725098,
        "learning_rate": 5.821232186698716e-05,
        "epoch": 0.2140248900812281,
        "step": 2872
    },
    {
        "loss": 2.5804,
        "grad_norm": 3.4166200160980225,
        "learning_rate": 5.8148404744238416e-05,
        "epoch": 0.21409941128250987,
        "step": 2873
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.1731836795806885,
        "learning_rate": 5.808450834358485e-05,
        "epoch": 0.21417393248379163,
        "step": 2874
    },
    {
        "loss": 2.3219,
        "grad_norm": 3.3976569175720215,
        "learning_rate": 5.8020632696663734e-05,
        "epoch": 0.2142484536850734,
        "step": 2875
    },
    {
        "loss": 2.6052,
        "grad_norm": 2.592952013015747,
        "learning_rate": 5.795677783510187e-05,
        "epoch": 0.21432297488635516,
        "step": 2876
    },
    {
        "loss": 2.4403,
        "grad_norm": 1.9639027118682861,
        "learning_rate": 5.789294379051596e-05,
        "epoch": 0.21439749608763695,
        "step": 2877
    },
    {
        "loss": 3.0503,
        "grad_norm": 3.3620426654815674,
        "learning_rate": 5.782913059451233e-05,
        "epoch": 0.2144720172889187,
        "step": 2878
    },
    {
        "loss": 2.1244,
        "grad_norm": 3.497321367263794,
        "learning_rate": 5.776533827868696e-05,
        "epoch": 0.21454653849020047,
        "step": 2879
    },
    {
        "loss": 2.8179,
        "grad_norm": 2.5502102375030518,
        "learning_rate": 5.770156687462558e-05,
        "epoch": 0.21462105969148224,
        "step": 2880
    },
    {
        "loss": 2.7906,
        "grad_norm": 2.7435061931610107,
        "learning_rate": 5.76378164139034e-05,
        "epoch": 0.214695580892764,
        "step": 2881
    },
    {
        "loss": 2.6321,
        "grad_norm": 2.8958282470703125,
        "learning_rate": 5.757408692808541e-05,
        "epoch": 0.21477010209404576,
        "step": 2882
    },
    {
        "loss": 2.4125,
        "grad_norm": 4.361073970794678,
        "learning_rate": 5.751037844872618e-05,
        "epoch": 0.21484462329532752,
        "step": 2883
    },
    {
        "loss": 2.5283,
        "grad_norm": 2.1319329738616943,
        "learning_rate": 5.744669100736983e-05,
        "epoch": 0.2149191444966093,
        "step": 2884
    },
    {
        "loss": 2.0253,
        "grad_norm": 2.390779972076416,
        "learning_rate": 5.738302463555017e-05,
        "epoch": 0.21499366569789105,
        "step": 2885
    },
    {
        "loss": 2.2355,
        "grad_norm": 2.6721770763397217,
        "learning_rate": 5.731937936479039e-05,
        "epoch": 0.21506818689917281,
        "step": 2886
    },
    {
        "loss": 2.246,
        "grad_norm": 1.61513090133667,
        "learning_rate": 5.725575522660347e-05,
        "epoch": 0.21514270810045458,
        "step": 2887
    },
    {
        "loss": 1.98,
        "grad_norm": 3.842778205871582,
        "learning_rate": 5.7192152252491705e-05,
        "epoch": 0.21521722930173634,
        "step": 2888
    },
    {
        "loss": 2.497,
        "grad_norm": 2.8563520908355713,
        "learning_rate": 5.712857047394704e-05,
        "epoch": 0.2152917505030181,
        "step": 2889
    },
    {
        "loss": 2.689,
        "grad_norm": 2.6159026622772217,
        "learning_rate": 5.7065009922450905e-05,
        "epoch": 0.21536627170429987,
        "step": 2890
    },
    {
        "loss": 2.0427,
        "grad_norm": 2.8456594944000244,
        "learning_rate": 5.7001470629474206e-05,
        "epoch": 0.21544079290558163,
        "step": 2891
    },
    {
        "loss": 2.6145,
        "grad_norm": 2.7963736057281494,
        "learning_rate": 5.6937952626477384e-05,
        "epoch": 0.2155153141068634,
        "step": 2892
    },
    {
        "loss": 2.6138,
        "grad_norm": 2.13246750831604,
        "learning_rate": 5.687445594491019e-05,
        "epoch": 0.21558983530814516,
        "step": 2893
    },
    {
        "loss": 2.5036,
        "grad_norm": 3.9310061931610107,
        "learning_rate": 5.681098061621193e-05,
        "epoch": 0.21566435650942692,
        "step": 2894
    },
    {
        "loss": 1.4561,
        "grad_norm": 2.8184428215026855,
        "learning_rate": 5.674752667181138e-05,
        "epoch": 0.2157388777107087,
        "step": 2895
    },
    {
        "loss": 2.6827,
        "grad_norm": 2.3895487785339355,
        "learning_rate": 5.66840941431266e-05,
        "epoch": 0.21581339891199047,
        "step": 2896
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.5163092613220215,
        "learning_rate": 5.662068306156512e-05,
        "epoch": 0.21588792011327224,
        "step": 2897
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.857952117919922,
        "learning_rate": 5.655729345852385e-05,
        "epoch": 0.215962441314554,
        "step": 2898
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.1355302333831787,
        "learning_rate": 5.649392536538909e-05,
        "epoch": 0.21603696251583576,
        "step": 2899
    },
    {
        "loss": 2.584,
        "grad_norm": 3.0416927337646484,
        "learning_rate": 5.643057881353646e-05,
        "epoch": 0.21611148371711753,
        "step": 2900
    },
    {
        "loss": 2.2685,
        "grad_norm": 5.206952095031738,
        "learning_rate": 5.636725383433084e-05,
        "epoch": 0.2161860049183993,
        "step": 2901
    },
    {
        "loss": 2.5841,
        "grad_norm": 2.5372467041015625,
        "learning_rate": 5.630395045912655e-05,
        "epoch": 0.21626052611968105,
        "step": 2902
    },
    {
        "loss": 2.1922,
        "grad_norm": 2.12300181388855,
        "learning_rate": 5.624066871926715e-05,
        "epoch": 0.21633504732096281,
        "step": 2903
    },
    {
        "loss": 2.3724,
        "grad_norm": 2.925522565841675,
        "learning_rate": 5.617740864608552e-05,
        "epoch": 0.21640956852224458,
        "step": 2904
    },
    {
        "loss": 2.4055,
        "grad_norm": 1.7981623411178589,
        "learning_rate": 5.611417027090382e-05,
        "epoch": 0.21648408972352634,
        "step": 2905
    },
    {
        "loss": 1.8503,
        "grad_norm": 1.6567285060882568,
        "learning_rate": 5.605095362503338e-05,
        "epoch": 0.2165586109248081,
        "step": 2906
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.3867955207824707,
        "learning_rate": 5.5987758739774884e-05,
        "epoch": 0.21663313212608987,
        "step": 2907
    },
    {
        "loss": 2.9038,
        "grad_norm": 1.7517765760421753,
        "learning_rate": 5.5924585646418115e-05,
        "epoch": 0.21670765332737163,
        "step": 2908
    },
    {
        "loss": 2.1633,
        "grad_norm": 3.069279909133911,
        "learning_rate": 5.586143437624216e-05,
        "epoch": 0.2167821745286534,
        "step": 2909
    },
    {
        "loss": 2.2081,
        "grad_norm": 3.472158432006836,
        "learning_rate": 5.579830496051538e-05,
        "epoch": 0.21685669572993516,
        "step": 2910
    },
    {
        "loss": 3.2274,
        "grad_norm": 6.4155378341674805,
        "learning_rate": 5.573519743049511e-05,
        "epoch": 0.21693121693121692,
        "step": 2911
    },
    {
        "loss": 3.2307,
        "grad_norm": 2.323370933532715,
        "learning_rate": 5.5672111817428016e-05,
        "epoch": 0.21700573813249868,
        "step": 2912
    },
    {
        "loss": 2.7413,
        "grad_norm": 1.9840433597564697,
        "learning_rate": 5.5609048152549794e-05,
        "epoch": 0.21708025933378047,
        "step": 2913
    },
    {
        "loss": 2.3561,
        "grad_norm": 3.026644229888916,
        "learning_rate": 5.5546006467085344e-05,
        "epoch": 0.21715478053506224,
        "step": 2914
    },
    {
        "loss": 2.2824,
        "grad_norm": 3.083592653274536,
        "learning_rate": 5.5482986792248725e-05,
        "epoch": 0.217229301736344,
        "step": 2915
    },
    {
        "loss": 1.9963,
        "grad_norm": 3.079197645187378,
        "learning_rate": 5.541998915924291e-05,
        "epoch": 0.21730382293762576,
        "step": 2916
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.1471478939056396,
        "learning_rate": 5.535701359926028e-05,
        "epoch": 0.21737834413890753,
        "step": 2917
    },
    {
        "loss": 2.0573,
        "grad_norm": 3.600600242614746,
        "learning_rate": 5.529406014348194e-05,
        "epoch": 0.2174528653401893,
        "step": 2918
    },
    {
        "loss": 2.0677,
        "grad_norm": 3.569195508956909,
        "learning_rate": 5.523112882307827e-05,
        "epoch": 0.21752738654147105,
        "step": 2919
    },
    {
        "loss": 1.6553,
        "grad_norm": 1.7147983312606812,
        "learning_rate": 5.5168219669208656e-05,
        "epoch": 0.21760190774275281,
        "step": 2920
    },
    {
        "loss": 2.711,
        "grad_norm": 2.406118154525757,
        "learning_rate": 5.510533271302141e-05,
        "epoch": 0.21767642894403458,
        "step": 2921
    },
    {
        "loss": 2.779,
        "grad_norm": 2.394965887069702,
        "learning_rate": 5.504246798565398e-05,
        "epoch": 0.21775095014531634,
        "step": 2922
    },
    {
        "loss": 2.224,
        "grad_norm": 2.7958486080169678,
        "learning_rate": 5.497962551823266e-05,
        "epoch": 0.2178254713465981,
        "step": 2923
    },
    {
        "loss": 2.96,
        "grad_norm": 2.7059543132781982,
        "learning_rate": 5.491680534187297e-05,
        "epoch": 0.21789999254787987,
        "step": 2924
    },
    {
        "loss": 2.0432,
        "grad_norm": 4.315718173980713,
        "learning_rate": 5.485400748767909e-05,
        "epoch": 0.21797451374916163,
        "step": 2925
    },
    {
        "loss": 2.4512,
        "grad_norm": 3.4858899116516113,
        "learning_rate": 5.479123198674436e-05,
        "epoch": 0.2180490349504434,
        "step": 2926
    },
    {
        "loss": 2.8239,
        "grad_norm": 3.257296085357666,
        "learning_rate": 5.472847887015102e-05,
        "epoch": 0.21812355615172516,
        "step": 2927
    },
    {
        "loss": 2.7363,
        "grad_norm": 2.532057285308838,
        "learning_rate": 5.46657481689701e-05,
        "epoch": 0.21819807735300692,
        "step": 2928
    },
    {
        "loss": 2.6805,
        "grad_norm": 3.1456689834594727,
        "learning_rate": 5.460303991426168e-05,
        "epoch": 0.21827259855428868,
        "step": 2929
    },
    {
        "loss": 2.7725,
        "grad_norm": 3.717052459716797,
        "learning_rate": 5.4540354137074676e-05,
        "epoch": 0.21834711975557047,
        "step": 2930
    },
    {
        "loss": 2.9781,
        "grad_norm": 1.8647468090057373,
        "learning_rate": 5.447769086844684e-05,
        "epoch": 0.21842164095685224,
        "step": 2931
    },
    {
        "loss": 1.929,
        "grad_norm": 2.885807752609253,
        "learning_rate": 5.441505013940487e-05,
        "epoch": 0.218496162158134,
        "step": 2932
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.4931800365448,
        "learning_rate": 5.435243198096416e-05,
        "epoch": 0.21857068335941576,
        "step": 2933
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.9430716037750244,
        "learning_rate": 5.428983642412905e-05,
        "epoch": 0.21864520456069753,
        "step": 2934
    },
    {
        "loss": 2.3158,
        "grad_norm": 2.8685097694396973,
        "learning_rate": 5.422726349989264e-05,
        "epoch": 0.2187197257619793,
        "step": 2935
    },
    {
        "loss": 2.6537,
        "grad_norm": 2.2999603748321533,
        "learning_rate": 5.416471323923689e-05,
        "epoch": 0.21879424696326105,
        "step": 2936
    },
    {
        "loss": 1.3266,
        "grad_norm": 1.8836177587509155,
        "learning_rate": 5.410218567313239e-05,
        "epoch": 0.21886876816454282,
        "step": 2937
    },
    {
        "loss": 2.4239,
        "grad_norm": 2.95072078704834,
        "learning_rate": 5.403968083253863e-05,
        "epoch": 0.21894328936582458,
        "step": 2938
    },
    {
        "loss": 2.3041,
        "grad_norm": 3.3650400638580322,
        "learning_rate": 5.397719874840381e-05,
        "epoch": 0.21901781056710634,
        "step": 2939
    },
    {
        "loss": 1.6744,
        "grad_norm": 4.297959804534912,
        "learning_rate": 5.391473945166483e-05,
        "epoch": 0.2190923317683881,
        "step": 2940
    },
    {
        "loss": 2.8782,
        "grad_norm": 2.2853524684906006,
        "learning_rate": 5.3852302973247414e-05,
        "epoch": 0.21916685296966987,
        "step": 2941
    },
    {
        "loss": 2.5038,
        "grad_norm": 3.580836772918701,
        "learning_rate": 5.3789889344065794e-05,
        "epoch": 0.21924137417095163,
        "step": 2942
    },
    {
        "loss": 2.2535,
        "grad_norm": 2.6117005348205566,
        "learning_rate": 5.3727498595023086e-05,
        "epoch": 0.2193158953722334,
        "step": 2943
    },
    {
        "loss": 2.2918,
        "grad_norm": 2.7125051021575928,
        "learning_rate": 5.366513075701087e-05,
        "epoch": 0.21939041657351516,
        "step": 2944
    },
    {
        "loss": 2.3556,
        "grad_norm": 2.5932912826538086,
        "learning_rate": 5.3602785860909685e-05,
        "epoch": 0.21946493777479692,
        "step": 2945
    },
    {
        "loss": 2.7603,
        "grad_norm": 1.5890198945999146,
        "learning_rate": 5.3540463937588405e-05,
        "epoch": 0.21953945897607868,
        "step": 2946
    },
    {
        "loss": 2.5736,
        "grad_norm": 2.339085817337036,
        "learning_rate": 5.347816501790468e-05,
        "epoch": 0.21961398017736045,
        "step": 2947
    },
    {
        "loss": 2.0815,
        "grad_norm": 3.108837366104126,
        "learning_rate": 5.341588913270479e-05,
        "epoch": 0.21968850137864224,
        "step": 2948
    },
    {
        "loss": 2.5558,
        "grad_norm": 2.0162782669067383,
        "learning_rate": 5.335363631282345e-05,
        "epoch": 0.219763022579924,
        "step": 2949
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.2368381023406982,
        "learning_rate": 5.329140658908423e-05,
        "epoch": 0.21983754378120576,
        "step": 2950
    },
    {
        "loss": 2.6532,
        "grad_norm": 2.669074058532715,
        "learning_rate": 5.322919999229898e-05,
        "epoch": 0.21991206498248753,
        "step": 2951
    },
    {
        "loss": 2.8071,
        "grad_norm": 3.668841600418091,
        "learning_rate": 5.316701655326828e-05,
        "epoch": 0.2199865861837693,
        "step": 2952
    },
    {
        "loss": 1.8669,
        "grad_norm": 2.646355628967285,
        "learning_rate": 5.310485630278119e-05,
        "epoch": 0.22006110738505105,
        "step": 2953
    },
    {
        "loss": 2.7692,
        "grad_norm": 2.780125617980957,
        "learning_rate": 5.3042719271615234e-05,
        "epoch": 0.22013562858633282,
        "step": 2954
    },
    {
        "loss": 2.5421,
        "grad_norm": 3.0263471603393555,
        "learning_rate": 5.298060549053652e-05,
        "epoch": 0.22021014978761458,
        "step": 2955
    },
    {
        "loss": 2.4418,
        "grad_norm": 2.5725817680358887,
        "learning_rate": 5.291851499029963e-05,
        "epoch": 0.22028467098889634,
        "step": 2956
    },
    {
        "loss": 1.609,
        "grad_norm": 2.8101463317871094,
        "learning_rate": 5.2856447801647614e-05,
        "epoch": 0.2203591921901781,
        "step": 2957
    },
    {
        "loss": 2.7664,
        "grad_norm": 3.034844160079956,
        "learning_rate": 5.279440395531192e-05,
        "epoch": 0.22043371339145987,
        "step": 2958
    },
    {
        "loss": 2.3132,
        "grad_norm": 3.027845859527588,
        "learning_rate": 5.2732383482012505e-05,
        "epoch": 0.22050823459274163,
        "step": 2959
    },
    {
        "loss": 2.2973,
        "grad_norm": 3.040325880050659,
        "learning_rate": 5.267038641245776e-05,
        "epoch": 0.2205827557940234,
        "step": 2960
    },
    {
        "loss": 1.4148,
        "grad_norm": 3.0167994499206543,
        "learning_rate": 5.2608412777344486e-05,
        "epoch": 0.22065727699530516,
        "step": 2961
    },
    {
        "loss": 1.8254,
        "grad_norm": 2.595430612564087,
        "learning_rate": 5.2546462607357784e-05,
        "epoch": 0.22073179819658692,
        "step": 2962
    },
    {
        "loss": 2.355,
        "grad_norm": 2.9008984565734863,
        "learning_rate": 5.248453593317124e-05,
        "epoch": 0.22080631939786868,
        "step": 2963
    },
    {
        "loss": 2.6001,
        "grad_norm": 3.301851749420166,
        "learning_rate": 5.242263278544684e-05,
        "epoch": 0.22088084059915045,
        "step": 2964
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.720960855484009,
        "learning_rate": 5.2360753194834735e-05,
        "epoch": 0.2209553618004322,
        "step": 2965
    },
    {
        "loss": 2.6545,
        "grad_norm": 1.8908143043518066,
        "learning_rate": 5.2298897191973675e-05,
        "epoch": 0.221029883001714,
        "step": 2966
    },
    {
        "loss": 2.0595,
        "grad_norm": 3.5471835136413574,
        "learning_rate": 5.2237064807490485e-05,
        "epoch": 0.22110440420299576,
        "step": 2967
    },
    {
        "loss": 1.9008,
        "grad_norm": 3.382359743118286,
        "learning_rate": 5.217525607200051e-05,
        "epoch": 0.22117892540427753,
        "step": 2968
    },
    {
        "loss": 2.5282,
        "grad_norm": 2.375880718231201,
        "learning_rate": 5.2113471016107154e-05,
        "epoch": 0.2212534466055593,
        "step": 2969
    },
    {
        "loss": 1.4492,
        "grad_norm": 1.7689309120178223,
        "learning_rate": 5.205170967040225e-05,
        "epoch": 0.22132796780684105,
        "step": 2970
    },
    {
        "loss": 1.9971,
        "grad_norm": 3.5634396076202393,
        "learning_rate": 5.198997206546596e-05,
        "epoch": 0.22140248900812282,
        "step": 2971
    },
    {
        "loss": 2.721,
        "grad_norm": 2.286409378051758,
        "learning_rate": 5.192825823186651e-05,
        "epoch": 0.22147701020940458,
        "step": 2972
    },
    {
        "loss": 2.1318,
        "grad_norm": 2.861111879348755,
        "learning_rate": 5.1866568200160494e-05,
        "epoch": 0.22155153141068634,
        "step": 2973
    },
    {
        "loss": 1.0989,
        "grad_norm": 3.3921844959259033,
        "learning_rate": 5.180490200089258e-05,
        "epoch": 0.2216260526119681,
        "step": 2974
    },
    {
        "loss": 2.1366,
        "grad_norm": 2.3241851329803467,
        "learning_rate": 5.174325966459577e-05,
        "epoch": 0.22170057381324987,
        "step": 2975
    },
    {
        "loss": 2.335,
        "grad_norm": 3.4512462615966797,
        "learning_rate": 5.1681641221791256e-05,
        "epoch": 0.22177509501453163,
        "step": 2976
    },
    {
        "loss": 2.5963,
        "grad_norm": 2.4920198917388916,
        "learning_rate": 5.1620046702988215e-05,
        "epoch": 0.2218496162158134,
        "step": 2977
    },
    {
        "loss": 1.9015,
        "grad_norm": 3.994450807571411,
        "learning_rate": 5.1558476138684276e-05,
        "epoch": 0.22192413741709516,
        "step": 2978
    },
    {
        "loss": 2.8925,
        "grad_norm": 2.71573805809021,
        "learning_rate": 5.14969295593649e-05,
        "epoch": 0.22199865861837692,
        "step": 2979
    },
    {
        "loss": 2.6293,
        "grad_norm": 2.159324884414673,
        "learning_rate": 5.1435406995503886e-05,
        "epoch": 0.22207317981965868,
        "step": 2980
    },
    {
        "loss": 1.7542,
        "grad_norm": 2.781602382659912,
        "learning_rate": 5.137390847756309e-05,
        "epoch": 0.22214770102094045,
        "step": 2981
    },
    {
        "loss": 2.8094,
        "grad_norm": 2.397681951522827,
        "learning_rate": 5.1312434035992375e-05,
        "epoch": 0.2222222222222222,
        "step": 2982
    },
    {
        "loss": 2.8713,
        "grad_norm": 2.0412917137145996,
        "learning_rate": 5.1250983701229806e-05,
        "epoch": 0.222296743423504,
        "step": 2983
    },
    {
        "loss": 3.1583,
        "grad_norm": 2.3556244373321533,
        "learning_rate": 5.118955750370136e-05,
        "epoch": 0.22237126462478576,
        "step": 2984
    },
    {
        "loss": 2.8432,
        "grad_norm": 3.7810301780700684,
        "learning_rate": 5.1128155473821306e-05,
        "epoch": 0.22244578582606753,
        "step": 2985
    },
    {
        "loss": 2.893,
        "grad_norm": 5.276148796081543,
        "learning_rate": 5.106677764199169e-05,
        "epoch": 0.2225203070273493,
        "step": 2986
    },
    {
        "loss": 3.1199,
        "grad_norm": 2.954806327819824,
        "learning_rate": 5.1005424038602724e-05,
        "epoch": 0.22259482822863105,
        "step": 2987
    },
    {
        "loss": 2.4181,
        "grad_norm": 3.3899221420288086,
        "learning_rate": 5.094409469403261e-05,
        "epoch": 0.22266934942991282,
        "step": 2988
    },
    {
        "loss": 1.9969,
        "grad_norm": 2.789848566055298,
        "learning_rate": 5.088278963864746e-05,
        "epoch": 0.22274387063119458,
        "step": 2989
    },
    {
        "loss": 2.4163,
        "grad_norm": 2.431476354598999,
        "learning_rate": 5.082150890280143e-05,
        "epoch": 0.22281839183247634,
        "step": 2990
    },
    {
        "loss": 2.7072,
        "grad_norm": 2.6657612323760986,
        "learning_rate": 5.0760252516836626e-05,
        "epoch": 0.2228929130337581,
        "step": 2991
    },
    {
        "loss": 1.3384,
        "grad_norm": 3.7421772480010986,
        "learning_rate": 5.069902051108315e-05,
        "epoch": 0.22296743423503987,
        "step": 2992
    },
    {
        "loss": 2.0936,
        "grad_norm": 2.767284870147705,
        "learning_rate": 5.0637812915858865e-05,
        "epoch": 0.22304195543632163,
        "step": 2993
    },
    {
        "loss": 2.505,
        "grad_norm": 3.0077521800994873,
        "learning_rate": 5.057662976146971e-05,
        "epoch": 0.2231164766376034,
        "step": 2994
    },
    {
        "loss": 2.3722,
        "grad_norm": 2.3560256958007812,
        "learning_rate": 5.051547107820947e-05,
        "epoch": 0.22319099783888516,
        "step": 2995
    },
    {
        "loss": 2.2694,
        "grad_norm": 3.951441764831543,
        "learning_rate": 5.04543368963598e-05,
        "epoch": 0.22326551904016692,
        "step": 2996
    },
    {
        "loss": 2.0824,
        "grad_norm": 4.134122371673584,
        "learning_rate": 5.0393227246190286e-05,
        "epoch": 0.22334004024144868,
        "step": 2997
    },
    {
        "loss": 2.5544,
        "grad_norm": 2.3548102378845215,
        "learning_rate": 5.033214215795823e-05,
        "epoch": 0.22341456144273045,
        "step": 2998
    },
    {
        "loss": 2.8631,
        "grad_norm": 3.236663341522217,
        "learning_rate": 5.0271081661908904e-05,
        "epoch": 0.2234890826440122,
        "step": 2999
    },
    {
        "loss": 2.5998,
        "grad_norm": 2.939894437789917,
        "learning_rate": 5.0210045788275375e-05,
        "epoch": 0.22356360384529397,
        "step": 3000
    },
    {
        "loss": 2.366,
        "grad_norm": 2.425126075744629,
        "learning_rate": 5.0149034567278466e-05,
        "epoch": 0.22363812504657576,
        "step": 3001
    },
    {
        "loss": 2.7941,
        "grad_norm": 2.435197353363037,
        "learning_rate": 5.008804802912689e-05,
        "epoch": 0.22371264624785753,
        "step": 3002
    },
    {
        "loss": 2.6169,
        "grad_norm": 3.365874767303467,
        "learning_rate": 5.0027086204017e-05,
        "epoch": 0.2237871674491393,
        "step": 3003
    },
    {
        "loss": 2.2944,
        "grad_norm": 3.317298650741577,
        "learning_rate": 4.9966149122133074e-05,
        "epoch": 0.22386168865042105,
        "step": 3004
    },
    {
        "loss": 1.8427,
        "grad_norm": 3.2829766273498535,
        "learning_rate": 4.990523681364694e-05,
        "epoch": 0.22393620985170282,
        "step": 3005
    },
    {
        "loss": 3.0203,
        "grad_norm": 2.516338348388672,
        "learning_rate": 4.984434930871842e-05,
        "epoch": 0.22401073105298458,
        "step": 3006
    },
    {
        "loss": 1.9662,
        "grad_norm": 4.121791362762451,
        "learning_rate": 4.97834866374948e-05,
        "epoch": 0.22408525225426634,
        "step": 3007
    },
    {
        "loss": 2.4583,
        "grad_norm": 2.213606119155884,
        "learning_rate": 4.9722648830111216e-05,
        "epoch": 0.2241597734555481,
        "step": 3008
    },
    {
        "loss": 1.4235,
        "grad_norm": 2.641150951385498,
        "learning_rate": 4.966183591669051e-05,
        "epoch": 0.22423429465682987,
        "step": 3009
    },
    {
        "loss": 2.7437,
        "grad_norm": 2.1688289642333984,
        "learning_rate": 4.960104792734304e-05,
        "epoch": 0.22430881585811163,
        "step": 3010
    },
    {
        "loss": 2.7033,
        "grad_norm": 2.9236252307891846,
        "learning_rate": 4.9540284892167044e-05,
        "epoch": 0.2243833370593934,
        "step": 3011
    },
    {
        "loss": 2.1764,
        "grad_norm": 4.758607864379883,
        "learning_rate": 4.947954684124821e-05,
        "epoch": 0.22445785826067516,
        "step": 3012
    },
    {
        "loss": 2.5918,
        "grad_norm": 2.2915854454040527,
        "learning_rate": 4.941883380466e-05,
        "epoch": 0.22453237946195692,
        "step": 3013
    },
    {
        "loss": 2.5784,
        "grad_norm": 1.8236066102981567,
        "learning_rate": 4.935814581246335e-05,
        "epoch": 0.22460690066323868,
        "step": 3014
    },
    {
        "loss": 2.0417,
        "grad_norm": 3.8227202892303467,
        "learning_rate": 4.9297482894706914e-05,
        "epoch": 0.22468142186452045,
        "step": 3015
    },
    {
        "loss": 2.7583,
        "grad_norm": 2.009396553039551,
        "learning_rate": 4.923684508142688e-05,
        "epoch": 0.2247559430658022,
        "step": 3016
    },
    {
        "loss": 2.3412,
        "grad_norm": 2.3474087715148926,
        "learning_rate": 4.917623240264704e-05,
        "epoch": 0.22483046426708397,
        "step": 3017
    },
    {
        "loss": 2.3963,
        "grad_norm": 5.104853630065918,
        "learning_rate": 4.911564488837872e-05,
        "epoch": 0.22490498546836574,
        "step": 3018
    },
    {
        "loss": 2.2298,
        "grad_norm": 1.8819658756256104,
        "learning_rate": 4.905508256862073e-05,
        "epoch": 0.22497950666964753,
        "step": 3019
    },
    {
        "loss": 1.9528,
        "grad_norm": 2.8020145893096924,
        "learning_rate": 4.899454547335948e-05,
        "epoch": 0.2250540278709293,
        "step": 3020
    },
    {
        "loss": 2.0945,
        "grad_norm": 3.1063344478607178,
        "learning_rate": 4.8934033632568874e-05,
        "epoch": 0.22512854907221105,
        "step": 3021
    },
    {
        "loss": 1.9105,
        "grad_norm": 2.688257932662964,
        "learning_rate": 4.887354707621035e-05,
        "epoch": 0.22520307027349282,
        "step": 3022
    },
    {
        "loss": 2.636,
        "grad_norm": 1.8689470291137695,
        "learning_rate": 4.8813085834232695e-05,
        "epoch": 0.22527759147477458,
        "step": 3023
    },
    {
        "loss": 2.3583,
        "grad_norm": 2.4471676349639893,
        "learning_rate": 4.8752649936572304e-05,
        "epoch": 0.22535211267605634,
        "step": 3024
    },
    {
        "loss": 2.4807,
        "grad_norm": 2.4471797943115234,
        "learning_rate": 4.8692239413152986e-05,
        "epoch": 0.2254266338773381,
        "step": 3025
    },
    {
        "loss": 2.3172,
        "grad_norm": 3.2800700664520264,
        "learning_rate": 4.863185429388588e-05,
        "epoch": 0.22550115507861987,
        "step": 3026
    },
    {
        "loss": 2.3626,
        "grad_norm": 2.1672189235687256,
        "learning_rate": 4.857149460866976e-05,
        "epoch": 0.22557567627990163,
        "step": 3027
    },
    {
        "loss": 3.0536,
        "grad_norm": 2.068650722503662,
        "learning_rate": 4.851116038739059e-05,
        "epoch": 0.2256501974811834,
        "step": 3028
    },
    {
        "loss": 2.7231,
        "grad_norm": 1.8938407897949219,
        "learning_rate": 4.845085165992189e-05,
        "epoch": 0.22572471868246516,
        "step": 3029
    },
    {
        "loss": 2.3194,
        "grad_norm": 3.285416841506958,
        "learning_rate": 4.83905684561244e-05,
        "epoch": 0.22579923988374692,
        "step": 3030
    },
    {
        "loss": 2.3898,
        "grad_norm": 2.793884754180908,
        "learning_rate": 4.833031080584631e-05,
        "epoch": 0.22587376108502868,
        "step": 3031
    },
    {
        "loss": 2.0179,
        "grad_norm": 3.3425655364990234,
        "learning_rate": 4.827007873892333e-05,
        "epoch": 0.22594828228631045,
        "step": 3032
    },
    {
        "loss": 2.3242,
        "grad_norm": 3.3075954914093018,
        "learning_rate": 4.820987228517807e-05,
        "epoch": 0.2260228034875922,
        "step": 3033
    },
    {
        "loss": 2.5937,
        "grad_norm": 2.5036606788635254,
        "learning_rate": 4.8149691474420924e-05,
        "epoch": 0.22609732468887397,
        "step": 3034
    },
    {
        "loss": 2.9865,
        "grad_norm": 2.936249256134033,
        "learning_rate": 4.808953633644927e-05,
        "epoch": 0.22617184589015574,
        "step": 3035
    },
    {
        "loss": 2.7503,
        "grad_norm": 1.8606901168823242,
        "learning_rate": 4.8029406901047914e-05,
        "epoch": 0.2262463670914375,
        "step": 3036
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.821197271347046,
        "learning_rate": 4.796930319798895e-05,
        "epoch": 0.2263208882927193,
        "step": 3037
    },
    {
        "loss": 1.6684,
        "grad_norm": 2.654060125350952,
        "learning_rate": 4.7909225257031565e-05,
        "epoch": 0.22639540949400105,
        "step": 3038
    },
    {
        "loss": 1.9094,
        "grad_norm": 2.829219102859497,
        "learning_rate": 4.784917310792253e-05,
        "epoch": 0.22646993069528282,
        "step": 3039
    },
    {
        "loss": 2.685,
        "grad_norm": 1.8900893926620483,
        "learning_rate": 4.778914678039538e-05,
        "epoch": 0.22654445189656458,
        "step": 3040
    },
    {
        "loss": 2.5635,
        "grad_norm": 2.481496810913086,
        "learning_rate": 4.772914630417128e-05,
        "epoch": 0.22661897309784634,
        "step": 3041
    },
    {
        "loss": 2.0766,
        "grad_norm": 4.658313274383545,
        "learning_rate": 4.766917170895843e-05,
        "epoch": 0.2266934942991281,
        "step": 3042
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.9632132053375244,
        "learning_rate": 4.760922302445212e-05,
        "epoch": 0.22676801550040987,
        "step": 3043
    },
    {
        "loss": 2.4586,
        "grad_norm": 1.5497936010360718,
        "learning_rate": 4.754930028033501e-05,
        "epoch": 0.22684253670169163,
        "step": 3044
    },
    {
        "loss": 2.5482,
        "grad_norm": 2.4472947120666504,
        "learning_rate": 4.748940350627669e-05,
        "epoch": 0.2269170579029734,
        "step": 3045
    },
    {
        "loss": 2.7034,
        "grad_norm": 1.6559982299804688,
        "learning_rate": 4.742953273193416e-05,
        "epoch": 0.22699157910425516,
        "step": 3046
    },
    {
        "loss": 2.6928,
        "grad_norm": 3.560641050338745,
        "learning_rate": 4.736968798695128e-05,
        "epoch": 0.22706610030553692,
        "step": 3047
    },
    {
        "loss": 2.0377,
        "grad_norm": 3.4302494525909424,
        "learning_rate": 4.730986930095921e-05,
        "epoch": 0.22714062150681869,
        "step": 3048
    },
    {
        "loss": 1.4167,
        "grad_norm": 3.3605668544769287,
        "learning_rate": 4.725007670357614e-05,
        "epoch": 0.22721514270810045,
        "step": 3049
    },
    {
        "loss": 2.051,
        "grad_norm": 2.6529974937438965,
        "learning_rate": 4.7190310224407266e-05,
        "epoch": 0.2272896639093822,
        "step": 3050
    },
    {
        "loss": 2.0239,
        "grad_norm": 3.0447373390197754,
        "learning_rate": 4.713056989304504e-05,
        "epoch": 0.22736418511066397,
        "step": 3051
    },
    {
        "loss": 2.3863,
        "grad_norm": 2.7991950511932373,
        "learning_rate": 4.7070855739068787e-05,
        "epoch": 0.22743870631194574,
        "step": 3052
    },
    {
        "loss": 2.714,
        "grad_norm": 2.472195625305176,
        "learning_rate": 4.7011167792044976e-05,
        "epoch": 0.2275132275132275,
        "step": 3053
    },
    {
        "loss": 2.1908,
        "grad_norm": 2.9972965717315674,
        "learning_rate": 4.695150608152702e-05,
        "epoch": 0.2275877487145093,
        "step": 3054
    },
    {
        "loss": 2.6673,
        "grad_norm": 2.0820438861846924,
        "learning_rate": 4.68918706370554e-05,
        "epoch": 0.22766226991579105,
        "step": 3055
    },
    {
        "loss": 2.4259,
        "grad_norm": 2.5633416175842285,
        "learning_rate": 4.683226148815758e-05,
        "epoch": 0.22773679111707282,
        "step": 3056
    },
    {
        "loss": 2.2427,
        "grad_norm": 3.332916021347046,
        "learning_rate": 4.6772678664348025e-05,
        "epoch": 0.22781131231835458,
        "step": 3057
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.980907678604126,
        "learning_rate": 4.6713122195128146e-05,
        "epoch": 0.22788583351963634,
        "step": 3058
    },
    {
        "loss": 2.6928,
        "grad_norm": 2.137273073196411,
        "learning_rate": 4.6653592109986254e-05,
        "epoch": 0.2279603547209181,
        "step": 3059
    },
    {
        "loss": 2.5538,
        "grad_norm": 3.9806904792785645,
        "learning_rate": 4.65940884383977e-05,
        "epoch": 0.22803487592219987,
        "step": 3060
    },
    {
        "loss": 2.5288,
        "grad_norm": 3.421254873275757,
        "learning_rate": 4.65346112098246e-05,
        "epoch": 0.22810939712348163,
        "step": 3061
    },
    {
        "loss": 1.2385,
        "grad_norm": 3.1715338230133057,
        "learning_rate": 4.64751604537162e-05,
        "epoch": 0.2281839183247634,
        "step": 3062
    },
    {
        "loss": 2.1371,
        "grad_norm": 2.976402997970581,
        "learning_rate": 4.6415736199508455e-05,
        "epoch": 0.22825843952604516,
        "step": 3063
    },
    {
        "loss": 2.3588,
        "grad_norm": 3.6364669799804688,
        "learning_rate": 4.635633847662425e-05,
        "epoch": 0.22833296072732692,
        "step": 3064
    },
    {
        "loss": 2.6808,
        "grad_norm": 3.233133554458618,
        "learning_rate": 4.6296967314473406e-05,
        "epoch": 0.22840748192860869,
        "step": 3065
    },
    {
        "loss": 2.7002,
        "grad_norm": 2.3655264377593994,
        "learning_rate": 4.623762274245239e-05,
        "epoch": 0.22848200312989045,
        "step": 3066
    },
    {
        "loss": 1.9224,
        "grad_norm": 3.1497385501861572,
        "learning_rate": 4.6178304789944826e-05,
        "epoch": 0.2285565243311722,
        "step": 3067
    },
    {
        "loss": 2.7159,
        "grad_norm": 2.00011944770813,
        "learning_rate": 4.611901348632085e-05,
        "epoch": 0.22863104553245397,
        "step": 3068
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.5492935180664062,
        "learning_rate": 4.605974886093755e-05,
        "epoch": 0.22870556673373574,
        "step": 3069
    },
    {
        "loss": 1.6644,
        "grad_norm": 4.044398784637451,
        "learning_rate": 4.600051094313883e-05,
        "epoch": 0.2287800879350175,
        "step": 3070
    },
    {
        "loss": 2.0495,
        "grad_norm": 3.098818778991699,
        "learning_rate": 4.594129976225522e-05,
        "epoch": 0.22885460913629926,
        "step": 3071
    },
    {
        "loss": 2.7744,
        "grad_norm": 3.2947022914886475,
        "learning_rate": 4.588211534760426e-05,
        "epoch": 0.22892913033758105,
        "step": 3072
    },
    {
        "loss": 1.2111,
        "grad_norm": 3.7507777214050293,
        "learning_rate": 4.582295772848997e-05,
        "epoch": 0.22900365153886282,
        "step": 3073
    },
    {
        "loss": 2.2329,
        "grad_norm": 2.7156009674072266,
        "learning_rate": 4.5763826934203305e-05,
        "epoch": 0.22907817274014458,
        "step": 3074
    },
    {
        "loss": 2.4037,
        "grad_norm": 3.2803750038146973,
        "learning_rate": 4.5704722994021776e-05,
        "epoch": 0.22915269394142634,
        "step": 3075
    },
    {
        "loss": 2.2363,
        "grad_norm": 4.096912384033203,
        "learning_rate": 4.5645645937209734e-05,
        "epoch": 0.2292272151427081,
        "step": 3076
    },
    {
        "loss": 2.1888,
        "grad_norm": 2.9752724170684814,
        "learning_rate": 4.558659579301815e-05,
        "epoch": 0.22930173634398987,
        "step": 3077
    },
    {
        "loss": 2.7978,
        "grad_norm": 2.543588399887085,
        "learning_rate": 4.552757259068468e-05,
        "epoch": 0.22937625754527163,
        "step": 3078
    },
    {
        "loss": 2.2531,
        "grad_norm": 3.510291814804077,
        "learning_rate": 4.546857635943369e-05,
        "epoch": 0.2294507787465534,
        "step": 3079
    },
    {
        "loss": 1.8867,
        "grad_norm": 3.009059190750122,
        "learning_rate": 4.540960712847607e-05,
        "epoch": 0.22952529994783516,
        "step": 3080
    },
    {
        "loss": 2.6241,
        "grad_norm": 2.8214645385742188,
        "learning_rate": 4.53506649270095e-05,
        "epoch": 0.22959982114911692,
        "step": 3081
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.018980026245117,
        "learning_rate": 4.5291749784218074e-05,
        "epoch": 0.22967434235039869,
        "step": 3082
    },
    {
        "loss": 2.8745,
        "grad_norm": 2.2484545707702637,
        "learning_rate": 4.523286172927275e-05,
        "epoch": 0.22974886355168045,
        "step": 3083
    },
    {
        "loss": 2.2624,
        "grad_norm": 2.0734028816223145,
        "learning_rate": 4.5174000791330825e-05,
        "epoch": 0.2298233847529622,
        "step": 3084
    },
    {
        "loss": 2.3191,
        "grad_norm": 2.8212313652038574,
        "learning_rate": 4.5115166999536315e-05,
        "epoch": 0.22989790595424398,
        "step": 3085
    },
    {
        "loss": 2.4595,
        "grad_norm": 1.413622260093689,
        "learning_rate": 4.50563603830198e-05,
        "epoch": 0.22997242715552574,
        "step": 3086
    },
    {
        "loss": 2.4178,
        "grad_norm": 3.8701436519622803,
        "learning_rate": 4.4997580970898244e-05,
        "epoch": 0.2300469483568075,
        "step": 3087
    },
    {
        "loss": 2.6979,
        "grad_norm": 3.61826491355896,
        "learning_rate": 4.493882879227541e-05,
        "epoch": 0.23012146955808926,
        "step": 3088
    },
    {
        "loss": 1.9308,
        "grad_norm": 3.2417209148406982,
        "learning_rate": 4.4880103876241305e-05,
        "epoch": 0.23019599075937103,
        "step": 3089
    },
    {
        "loss": 2.0624,
        "grad_norm": 2.766479253768921,
        "learning_rate": 4.4821406251872654e-05,
        "epoch": 0.23027051196065282,
        "step": 3090
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.7382359504699707,
        "learning_rate": 4.476273594823247e-05,
        "epoch": 0.23034503316193458,
        "step": 3091
    },
    {
        "loss": 2.2757,
        "grad_norm": 2.0877060890197754,
        "learning_rate": 4.470409299437036e-05,
        "epoch": 0.23041955436321634,
        "step": 3092
    },
    {
        "loss": 2.3197,
        "grad_norm": 3.3670663833618164,
        "learning_rate": 4.4645477419322524e-05,
        "epoch": 0.2304940755644981,
        "step": 3093
    },
    {
        "loss": 2.055,
        "grad_norm": 1.684128999710083,
        "learning_rate": 4.4586889252111206e-05,
        "epoch": 0.23056859676577987,
        "step": 3094
    },
    {
        "loss": 2.8273,
        "grad_norm": 2.1188981533050537,
        "learning_rate": 4.452832852174553e-05,
        "epoch": 0.23064311796706163,
        "step": 3095
    },
    {
        "loss": 2.208,
        "grad_norm": 1.5285717248916626,
        "learning_rate": 4.4469795257220716e-05,
        "epoch": 0.2307176391683434,
        "step": 3096
    },
    {
        "loss": 2.1822,
        "grad_norm": 3.2867770195007324,
        "learning_rate": 4.441128948751854e-05,
        "epoch": 0.23079216036962516,
        "step": 3097
    },
    {
        "loss": 1.5812,
        "grad_norm": 3.7842178344726562,
        "learning_rate": 4.435281124160715e-05,
        "epoch": 0.23086668157090692,
        "step": 3098
    },
    {
        "loss": 2.6112,
        "grad_norm": 2.0563416481018066,
        "learning_rate": 4.429436054844095e-05,
        "epoch": 0.2309412027721887,
        "step": 3099
    },
    {
        "loss": 2.0868,
        "grad_norm": 5.06516695022583,
        "learning_rate": 4.423593743696095e-05,
        "epoch": 0.23101572397347045,
        "step": 3100
    },
    {
        "loss": 2.8268,
        "grad_norm": 3.031911849975586,
        "learning_rate": 4.4177541936094146e-05,
        "epoch": 0.2310902451747522,
        "step": 3101
    },
    {
        "loss": 2.0126,
        "grad_norm": 2.8002350330352783,
        "learning_rate": 4.4119174074754234e-05,
        "epoch": 0.23116476637603398,
        "step": 3102
    },
    {
        "loss": 2.8665,
        "grad_norm": 3.692640542984009,
        "learning_rate": 4.4060833881840947e-05,
        "epoch": 0.23123928757731574,
        "step": 3103
    },
    {
        "loss": 3.1289,
        "grad_norm": 2.608079671859741,
        "learning_rate": 4.4002521386240466e-05,
        "epoch": 0.2313138087785975,
        "step": 3104
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.6933281421661377,
        "learning_rate": 4.394423661682525e-05,
        "epoch": 0.23138832997987926,
        "step": 3105
    },
    {
        "loss": 2.8092,
        "grad_norm": 2.7245688438415527,
        "learning_rate": 4.388597960245389e-05,
        "epoch": 0.23146285118116103,
        "step": 3106
    },
    {
        "loss": 2.608,
        "grad_norm": 2.8855082988739014,
        "learning_rate": 4.3827750371971473e-05,
        "epoch": 0.23153737238244282,
        "step": 3107
    },
    {
        "loss": 1.8243,
        "grad_norm": 3.0739641189575195,
        "learning_rate": 4.376954895420912e-05,
        "epoch": 0.23161189358372458,
        "step": 3108
    },
    {
        "loss": 2.7072,
        "grad_norm": 2.7006380558013916,
        "learning_rate": 4.37113753779843e-05,
        "epoch": 0.23168641478500634,
        "step": 3109
    },
    {
        "loss": 2.2494,
        "grad_norm": 3.035304307937622,
        "learning_rate": 4.365322967210058e-05,
        "epoch": 0.2317609359862881,
        "step": 3110
    },
    {
        "loss": 1.7128,
        "grad_norm": 3.681046962738037,
        "learning_rate": 4.3595111865347835e-05,
        "epoch": 0.23183545718756987,
        "step": 3111
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.967928647994995,
        "learning_rate": 4.353702198650218e-05,
        "epoch": 0.23190997838885163,
        "step": 3112
    },
    {
        "loss": 2.3504,
        "grad_norm": 2.353454113006592,
        "learning_rate": 4.3478960064325714e-05,
        "epoch": 0.2319844995901334,
        "step": 3113
    },
    {
        "loss": 2.5304,
        "grad_norm": 2.960766553878784,
        "learning_rate": 4.342092612756686e-05,
        "epoch": 0.23205902079141516,
        "step": 3114
    },
    {
        "loss": 2.5072,
        "grad_norm": 2.762194871902466,
        "learning_rate": 4.336292020496006e-05,
        "epoch": 0.23213354199269692,
        "step": 3115
    },
    {
        "loss": 2.7804,
        "grad_norm": 3.7415499687194824,
        "learning_rate": 4.330494232522597e-05,
        "epoch": 0.2322080631939787,
        "step": 3116
    },
    {
        "loss": 1.7697,
        "grad_norm": 2.869767904281616,
        "learning_rate": 4.324699251707135e-05,
        "epoch": 0.23228258439526045,
        "step": 3117
    },
    {
        "loss": 2.7625,
        "grad_norm": 2.2193031311035156,
        "learning_rate": 4.3189070809189034e-05,
        "epoch": 0.2323571055965422,
        "step": 3118
    },
    {
        "loss": 2.1465,
        "grad_norm": 2.792438507080078,
        "learning_rate": 4.3131177230258e-05,
        "epoch": 0.23243162679782398,
        "step": 3119
    },
    {
        "loss": 2.2198,
        "grad_norm": 2.6424171924591064,
        "learning_rate": 4.307331180894319e-05,
        "epoch": 0.23250614799910574,
        "step": 3120
    },
    {
        "loss": 2.5883,
        "grad_norm": 2.827864408493042,
        "learning_rate": 4.301547457389572e-05,
        "epoch": 0.2325806692003875,
        "step": 3121
    },
    {
        "loss": 2.0648,
        "grad_norm": 2.985316038131714,
        "learning_rate": 4.2957665553752615e-05,
        "epoch": 0.23265519040166927,
        "step": 3122
    },
    {
        "loss": 2.4853,
        "grad_norm": 1.4829884767532349,
        "learning_rate": 4.289988477713713e-05,
        "epoch": 0.23272971160295103,
        "step": 3123
    },
    {
        "loss": 2.5532,
        "grad_norm": 2.6261658668518066,
        "learning_rate": 4.284213227265834e-05,
        "epoch": 0.2328042328042328,
        "step": 3124
    },
    {
        "loss": 2.3069,
        "grad_norm": 3.2415072917938232,
        "learning_rate": 4.278440806891142e-05,
        "epoch": 0.23287875400551458,
        "step": 3125
    },
    {
        "loss": 2.7983,
        "grad_norm": 2.777815818786621,
        "learning_rate": 4.272671219447755e-05,
        "epoch": 0.23295327520679635,
        "step": 3126
    },
    {
        "loss": 2.1922,
        "grad_norm": 4.023280143737793,
        "learning_rate": 4.266904467792374e-05,
        "epoch": 0.2330277964080781,
        "step": 3127
    },
    {
        "loss": 2.6125,
        "grad_norm": 2.4343278408050537,
        "learning_rate": 4.261140554780322e-05,
        "epoch": 0.23310231760935987,
        "step": 3128
    },
    {
        "loss": 2.3454,
        "grad_norm": 2.0928287506103516,
        "learning_rate": 4.255379483265488e-05,
        "epoch": 0.23317683881064163,
        "step": 3129
    },
    {
        "loss": 2.8534,
        "grad_norm": 2.8819162845611572,
        "learning_rate": 4.2496212561003766e-05,
        "epoch": 0.2332513600119234,
        "step": 3130
    },
    {
        "loss": 2.6049,
        "grad_norm": 2.2111153602600098,
        "learning_rate": 4.243865876136067e-05,
        "epoch": 0.23332588121320516,
        "step": 3131
    },
    {
        "loss": 1.6124,
        "grad_norm": 3.774606704711914,
        "learning_rate": 4.2381133462222386e-05,
        "epoch": 0.23340040241448692,
        "step": 3132
    },
    {
        "loss": 1.9906,
        "grad_norm": 3.235902786254883,
        "learning_rate": 4.23236366920716e-05,
        "epoch": 0.2334749236157687,
        "step": 3133
    },
    {
        "loss": 1.6177,
        "grad_norm": 1.9848549365997314,
        "learning_rate": 4.226616847937681e-05,
        "epoch": 0.23354944481705045,
        "step": 3134
    },
    {
        "loss": 2.0797,
        "grad_norm": 2.743389844894409,
        "learning_rate": 4.220872885259247e-05,
        "epoch": 0.2336239660183322,
        "step": 3135
    },
    {
        "loss": 2.253,
        "grad_norm": 3.1027352809906006,
        "learning_rate": 4.215131784015873e-05,
        "epoch": 0.23369848721961398,
        "step": 3136
    },
    {
        "loss": 2.5282,
        "grad_norm": 2.397965431213379,
        "learning_rate": 4.209393547050172e-05,
        "epoch": 0.23377300842089574,
        "step": 3137
    },
    {
        "loss": 2.7085,
        "grad_norm": 2.988161325454712,
        "learning_rate": 4.203658177203332e-05,
        "epoch": 0.2338475296221775,
        "step": 3138
    },
    {
        "loss": 1.6863,
        "grad_norm": 2.864133834838867,
        "learning_rate": 4.197925677315122e-05,
        "epoch": 0.23392205082345927,
        "step": 3139
    },
    {
        "loss": 2.6075,
        "grad_norm": 3.6271626949310303,
        "learning_rate": 4.192196050223894e-05,
        "epoch": 0.23399657202474103,
        "step": 3140
    },
    {
        "loss": 2.9215,
        "grad_norm": 1.5851258039474487,
        "learning_rate": 4.186469298766569e-05,
        "epoch": 0.2340710932260228,
        "step": 3141
    },
    {
        "loss": 2.4611,
        "grad_norm": 2.7720789909362793,
        "learning_rate": 4.1807454257786525e-05,
        "epoch": 0.23414561442730455,
        "step": 3142
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.947898030281067,
        "learning_rate": 4.175024434094214e-05,
        "epoch": 0.23422013562858635,
        "step": 3143
    },
    {
        "loss": 2.3845,
        "grad_norm": 3.1608710289001465,
        "learning_rate": 4.169306326545915e-05,
        "epoch": 0.2342946568298681,
        "step": 3144
    },
    {
        "loss": 2.6032,
        "grad_norm": 2.7060115337371826,
        "learning_rate": 4.16359110596497e-05,
        "epoch": 0.23436917803114987,
        "step": 3145
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.1526503562927246,
        "learning_rate": 4.157878775181174e-05,
        "epoch": 0.23444369923243163,
        "step": 3146
    },
    {
        "loss": 2.4436,
        "grad_norm": 2.4500699043273926,
        "learning_rate": 4.152169337022893e-05,
        "epoch": 0.2345182204337134,
        "step": 3147
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.9952950477600098,
        "learning_rate": 4.1464627943170465e-05,
        "epoch": 0.23459274163499516,
        "step": 3148
    },
    {
        "loss": 1.9256,
        "grad_norm": 2.398698329925537,
        "learning_rate": 4.140759149889147e-05,
        "epoch": 0.23466726283627692,
        "step": 3149
    },
    {
        "loss": 2.7007,
        "grad_norm": 2.167630672454834,
        "learning_rate": 4.135058406563236e-05,
        "epoch": 0.2347417840375587,
        "step": 3150
    },
    {
        "loss": 2.65,
        "grad_norm": 3.181455373764038,
        "learning_rate": 4.129360567161954e-05,
        "epoch": 0.23481630523884045,
        "step": 3151
    },
    {
        "loss": 2.1398,
        "grad_norm": 2.9919075965881348,
        "learning_rate": 4.1236656345064794e-05,
        "epoch": 0.2348908264401222,
        "step": 3152
    },
    {
        "loss": 1.4529,
        "grad_norm": 2.077768564224243,
        "learning_rate": 4.1179736114165615e-05,
        "epoch": 0.23496534764140398,
        "step": 3153
    },
    {
        "loss": 2.7204,
        "grad_norm": 2.317963123321533,
        "learning_rate": 4.112284500710512e-05,
        "epoch": 0.23503986884268574,
        "step": 3154
    },
    {
        "loss": 2.7805,
        "grad_norm": 2.6162891387939453,
        "learning_rate": 4.106598305205185e-05,
        "epoch": 0.2351143900439675,
        "step": 3155
    },
    {
        "loss": 2.3609,
        "grad_norm": 2.6875627040863037,
        "learning_rate": 4.1009150277160155e-05,
        "epoch": 0.23518891124524927,
        "step": 3156
    },
    {
        "loss": 2.4577,
        "grad_norm": 3.13440203666687,
        "learning_rate": 4.095234671056971e-05,
        "epoch": 0.23526343244653103,
        "step": 3157
    },
    {
        "loss": 2.4399,
        "grad_norm": 3.4045615196228027,
        "learning_rate": 4.089557238040584e-05,
        "epoch": 0.2353379536478128,
        "step": 3158
    },
    {
        "loss": 2.6001,
        "grad_norm": 2.0196926593780518,
        "learning_rate": 4.083882731477944e-05,
        "epoch": 0.23541247484909456,
        "step": 3159
    },
    {
        "loss": 2.42,
        "grad_norm": 1.9635822772979736,
        "learning_rate": 4.07821115417867e-05,
        "epoch": 0.23548699605037632,
        "step": 3160
    },
    {
        "loss": 2.7761,
        "grad_norm": 1.9040888547897339,
        "learning_rate": 4.0725425089509675e-05,
        "epoch": 0.2355615172516581,
        "step": 3161
    },
    {
        "loss": 2.9115,
        "grad_norm": 6.588644981384277,
        "learning_rate": 4.0668767986015444e-05,
        "epoch": 0.23563603845293987,
        "step": 3162
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.757878541946411,
        "learning_rate": 4.061214025935698e-05,
        "epoch": 0.23571055965422164,
        "step": 3163
    },
    {
        "loss": 2.0833,
        "grad_norm": 4.008387565612793,
        "learning_rate": 4.0555541937572415e-05,
        "epoch": 0.2357850808555034,
        "step": 3164
    },
    {
        "loss": 2.7215,
        "grad_norm": 1.9144339561462402,
        "learning_rate": 4.049897304868549e-05,
        "epoch": 0.23585960205678516,
        "step": 3165
    },
    {
        "loss": 2.4907,
        "grad_norm": 2.8556151390075684,
        "learning_rate": 4.044243362070531e-05,
        "epoch": 0.23593412325806692,
        "step": 3166
    },
    {
        "loss": 2.2426,
        "grad_norm": 3.063087224960327,
        "learning_rate": 4.038592368162632e-05,
        "epoch": 0.2360086444593487,
        "step": 3167
    },
    {
        "loss": 2.3216,
        "grad_norm": 3.0251522064208984,
        "learning_rate": 4.032944325942856e-05,
        "epoch": 0.23608316566063045,
        "step": 3168
    },
    {
        "loss": 2.8282,
        "grad_norm": 2.001793146133423,
        "learning_rate": 4.0272992382077254e-05,
        "epoch": 0.2361576868619122,
        "step": 3169
    },
    {
        "loss": 2.2482,
        "grad_norm": 3.7088701725006104,
        "learning_rate": 4.021657107752314e-05,
        "epoch": 0.23623220806319398,
        "step": 3170
    },
    {
        "loss": 2.1967,
        "grad_norm": 2.8530588150024414,
        "learning_rate": 4.016017937370218e-05,
        "epoch": 0.23630672926447574,
        "step": 3171
    },
    {
        "loss": 2.4354,
        "grad_norm": 2.868053913116455,
        "learning_rate": 4.0103817298535794e-05,
        "epoch": 0.2363812504657575,
        "step": 3172
    },
    {
        "loss": 2.064,
        "grad_norm": 2.933283567428589,
        "learning_rate": 4.00474848799307e-05,
        "epoch": 0.23645577166703927,
        "step": 3173
    },
    {
        "loss": 2.3911,
        "grad_norm": 2.971087694168091,
        "learning_rate": 3.999118214577889e-05,
        "epoch": 0.23653029286832103,
        "step": 3174
    },
    {
        "loss": 3.0565,
        "grad_norm": 1.960662841796875,
        "learning_rate": 3.993490912395775e-05,
        "epoch": 0.2366048140696028,
        "step": 3175
    },
    {
        "loss": 1.866,
        "grad_norm": 3.21866774559021,
        "learning_rate": 3.987866584232984e-05,
        "epoch": 0.23667933527088456,
        "step": 3176
    },
    {
        "loss": 2.1375,
        "grad_norm": 3.038024663925171,
        "learning_rate": 3.9822452328743084e-05,
        "epoch": 0.23675385647216632,
        "step": 3177
    },
    {
        "loss": 2.8289,
        "grad_norm": 2.2389166355133057,
        "learning_rate": 3.9766268611030566e-05,
        "epoch": 0.2368283776734481,
        "step": 3178
    },
    {
        "loss": 2.4362,
        "grad_norm": 2.2272400856018066,
        "learning_rate": 3.971011471701079e-05,
        "epoch": 0.23690289887472987,
        "step": 3179
    },
    {
        "loss": 2.295,
        "grad_norm": 3.696096897125244,
        "learning_rate": 3.965399067448732e-05,
        "epoch": 0.23697742007601164,
        "step": 3180
    },
    {
        "loss": 2.5357,
        "grad_norm": 2.12959361076355,
        "learning_rate": 3.9597896511249e-05,
        "epoch": 0.2370519412772934,
        "step": 3181
    },
    {
        "loss": 2.331,
        "grad_norm": 2.346100330352783,
        "learning_rate": 3.9541832255069954e-05,
        "epoch": 0.23712646247857516,
        "step": 3182
    },
    {
        "loss": 2.1762,
        "grad_norm": 3.082613229751587,
        "learning_rate": 3.948579793370931e-05,
        "epoch": 0.23720098367985692,
        "step": 3183
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.6348485946655273,
        "learning_rate": 3.942979357491163e-05,
        "epoch": 0.2372755048811387,
        "step": 3184
    },
    {
        "loss": 2.0963,
        "grad_norm": 4.026986598968506,
        "learning_rate": 3.937381920640642e-05,
        "epoch": 0.23735002608242045,
        "step": 3185
    },
    {
        "loss": 2.3371,
        "grad_norm": 2.742156744003296,
        "learning_rate": 3.931787485590846e-05,
        "epoch": 0.23742454728370221,
        "step": 3186
    },
    {
        "loss": 2.3576,
        "grad_norm": 1.9388397932052612,
        "learning_rate": 3.926196055111764e-05,
        "epoch": 0.23749906848498398,
        "step": 3187
    },
    {
        "loss": 2.5032,
        "grad_norm": 2.6901538372039795,
        "learning_rate": 3.920607631971888e-05,
        "epoch": 0.23757358968626574,
        "step": 3188
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.7902990579605103,
        "learning_rate": 3.9150222189382415e-05,
        "epoch": 0.2376481108875475,
        "step": 3189
    },
    {
        "loss": 1.863,
        "grad_norm": 2.668308734893799,
        "learning_rate": 3.9094398187763356e-05,
        "epoch": 0.23772263208882927,
        "step": 3190
    },
    {
        "loss": 2.4474,
        "grad_norm": 4.737125873565674,
        "learning_rate": 3.903860434250208e-05,
        "epoch": 0.23779715329011103,
        "step": 3191
    },
    {
        "loss": 2.873,
        "grad_norm": 1.5803362131118774,
        "learning_rate": 3.8982840681223854e-05,
        "epoch": 0.2378716744913928,
        "step": 3192
    },
    {
        "loss": 2.2549,
        "grad_norm": 2.1909122467041016,
        "learning_rate": 3.892710723153914e-05,
        "epoch": 0.23794619569267456,
        "step": 3193
    },
    {
        "loss": 2.1776,
        "grad_norm": 3.556252956390381,
        "learning_rate": 3.887140402104339e-05,
        "epoch": 0.23802071689395632,
        "step": 3194
    },
    {
        "loss": 3.0611,
        "grad_norm": 2.5198519229888916,
        "learning_rate": 3.8815731077317086e-05,
        "epoch": 0.23809523809523808,
        "step": 3195
    },
    {
        "loss": 2.4544,
        "grad_norm": 2.7152891159057617,
        "learning_rate": 3.876008842792576e-05,
        "epoch": 0.23816975929651987,
        "step": 3196
    },
    {
        "loss": 2.1151,
        "grad_norm": 3.563112735748291,
        "learning_rate": 3.870447610041984e-05,
        "epoch": 0.23824428049780164,
        "step": 3197
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.3282570838928223,
        "learning_rate": 3.864889412233485e-05,
        "epoch": 0.2383188016990834,
        "step": 3198
    },
    {
        "loss": 2.8839,
        "grad_norm": 1.7628419399261475,
        "learning_rate": 3.859334252119125e-05,
        "epoch": 0.23839332290036516,
        "step": 3199
    },
    {
        "loss": 2.3834,
        "grad_norm": 2.7709579467773438,
        "learning_rate": 3.8537821324494486e-05,
        "epoch": 0.23846784410164693,
        "step": 3200
    },
    {
        "loss": 2.0499,
        "grad_norm": 2.1715543270111084,
        "learning_rate": 3.848233055973483e-05,
        "epoch": 0.2385423653029287,
        "step": 3201
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.9234815835952759,
        "learning_rate": 3.842687025438766e-05,
        "epoch": 0.23861688650421045,
        "step": 3202
    },
    {
        "loss": 2.1696,
        "grad_norm": 3.1768407821655273,
        "learning_rate": 3.837144043591317e-05,
        "epoch": 0.23869140770549221,
        "step": 3203
    },
    {
        "loss": 1.6311,
        "grad_norm": 6.729640960693359,
        "learning_rate": 3.831604113175642e-05,
        "epoch": 0.23876592890677398,
        "step": 3204
    },
    {
        "loss": 2.8342,
        "grad_norm": 2.2404098510742188,
        "learning_rate": 3.826067236934754e-05,
        "epoch": 0.23884045010805574,
        "step": 3205
    },
    {
        "loss": 2.5626,
        "grad_norm": 2.4735515117645264,
        "learning_rate": 3.8205334176101305e-05,
        "epoch": 0.2389149713093375,
        "step": 3206
    },
    {
        "loss": 1.7677,
        "grad_norm": 3.757713794708252,
        "learning_rate": 3.81500265794175e-05,
        "epoch": 0.23898949251061927,
        "step": 3207
    },
    {
        "loss": 2.2224,
        "grad_norm": 4.150792121887207,
        "learning_rate": 3.809474960668078e-05,
        "epoch": 0.23906401371190103,
        "step": 3208
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.524364471435547,
        "learning_rate": 3.803950328526046e-05,
        "epoch": 0.2391385349131828,
        "step": 3209
    },
    {
        "loss": 2.6494,
        "grad_norm": 2.6676931381225586,
        "learning_rate": 3.7984287642510994e-05,
        "epoch": 0.23921305611446456,
        "step": 3210
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.970825672149658,
        "learning_rate": 3.792910270577124e-05,
        "epoch": 0.23928757731574632,
        "step": 3211
    },
    {
        "loss": 1.9325,
        "grad_norm": 3.2678346633911133,
        "learning_rate": 3.7873948502365234e-05,
        "epoch": 0.23936209851702808,
        "step": 3212
    },
    {
        "loss": 2.1165,
        "grad_norm": 2.8240156173706055,
        "learning_rate": 3.781882505960153e-05,
        "epoch": 0.23943661971830985,
        "step": 3213
    },
    {
        "loss": 2.139,
        "grad_norm": 4.218731880187988,
        "learning_rate": 3.776373240477358e-05,
        "epoch": 0.23951114091959164,
        "step": 3214
    },
    {
        "loss": 2.7994,
        "grad_norm": 2.1732723712921143,
        "learning_rate": 3.77086705651596e-05,
        "epoch": 0.2395856621208734,
        "step": 3215
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.7178550958633423,
        "learning_rate": 3.76536395680224e-05,
        "epoch": 0.23966018332215516,
        "step": 3216
    },
    {
        "loss": 2.1007,
        "grad_norm": 3.3024845123291016,
        "learning_rate": 3.759863944060982e-05,
        "epoch": 0.23973470452343693,
        "step": 3217
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.7019729614257812,
        "learning_rate": 3.754367021015399e-05,
        "epoch": 0.2398092257247187,
        "step": 3218
    },
    {
        "loss": 2.1732,
        "grad_norm": 3.0683820247650146,
        "learning_rate": 3.7488731903872135e-05,
        "epoch": 0.23988374692600045,
        "step": 3219
    },
    {
        "loss": 2.802,
        "grad_norm": 2.340794801712036,
        "learning_rate": 3.743382454896599e-05,
        "epoch": 0.23995826812728221,
        "step": 3220
    },
    {
        "loss": 2.1112,
        "grad_norm": 3.0090959072113037,
        "learning_rate": 3.737894817262194e-05,
        "epoch": 0.24003278932856398,
        "step": 3221
    },
    {
        "loss": 2.7239,
        "grad_norm": 1.7028751373291016,
        "learning_rate": 3.7324102802011116e-05,
        "epoch": 0.24010731052984574,
        "step": 3222
    },
    {
        "loss": 2.2841,
        "grad_norm": 2.9680662155151367,
        "learning_rate": 3.726928846428919e-05,
        "epoch": 0.2401818317311275,
        "step": 3223
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.047372817993164,
        "learning_rate": 3.7214505186596636e-05,
        "epoch": 0.24025635293240927,
        "step": 3224
    },
    {
        "loss": 2.291,
        "grad_norm": 2.7269580364227295,
        "learning_rate": 3.715975299605837e-05,
        "epoch": 0.24033087413369103,
        "step": 3225
    },
    {
        "loss": 2.6378,
        "grad_norm": 1.8199468851089478,
        "learning_rate": 3.7105031919784014e-05,
        "epoch": 0.2404053953349728,
        "step": 3226
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.2320258617401123,
        "learning_rate": 3.705034198486781e-05,
        "epoch": 0.24047991653625456,
        "step": 3227
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.7501180171966553,
        "learning_rate": 3.699568321838842e-05,
        "epoch": 0.24055443773753632,
        "step": 3228
    },
    {
        "loss": 2.9702,
        "grad_norm": 2.6086161136627197,
        "learning_rate": 3.694105564740934e-05,
        "epoch": 0.24062895893881808,
        "step": 3229
    },
    {
        "loss": 2.3428,
        "grad_norm": 2.5276291370391846,
        "learning_rate": 3.688645929897836e-05,
        "epoch": 0.24070348014009985,
        "step": 3230
    },
    {
        "loss": 2.2713,
        "grad_norm": 2.6697998046875,
        "learning_rate": 3.683189420012799e-05,
        "epoch": 0.24077800134138164,
        "step": 3231
    },
    {
        "loss": 2.0525,
        "grad_norm": 3.795835494995117,
        "learning_rate": 3.677736037787513e-05,
        "epoch": 0.2408525225426634,
        "step": 3232
    },
    {
        "loss": 2.0357,
        "grad_norm": 3.058079481124878,
        "learning_rate": 3.672285785922128e-05,
        "epoch": 0.24092704374394516,
        "step": 3233
    },
    {
        "loss": 2.3329,
        "grad_norm": 2.9005725383758545,
        "learning_rate": 3.6668386671152435e-05,
        "epoch": 0.24100156494522693,
        "step": 3234
    },
    {
        "loss": 2.6835,
        "grad_norm": 2.51216459274292,
        "learning_rate": 3.6613946840639066e-05,
        "epoch": 0.2410760861465087,
        "step": 3235
    },
    {
        "loss": 2.7563,
        "grad_norm": 1.7331196069717407,
        "learning_rate": 3.655953839463615e-05,
        "epoch": 0.24115060734779045,
        "step": 3236
    },
    {
        "loss": 2.4853,
        "grad_norm": 3.3685574531555176,
        "learning_rate": 3.650516136008302e-05,
        "epoch": 0.24122512854907222,
        "step": 3237
    },
    {
        "loss": 2.3706,
        "grad_norm": 2.59232497215271,
        "learning_rate": 3.6450815763903556e-05,
        "epoch": 0.24129964975035398,
        "step": 3238
    },
    {
        "loss": 1.7746,
        "grad_norm": 2.96976900100708,
        "learning_rate": 3.6396501633006043e-05,
        "epoch": 0.24137417095163574,
        "step": 3239
    },
    {
        "loss": 2.2945,
        "grad_norm": 3.087916612625122,
        "learning_rate": 3.634221899428323e-05,
        "epoch": 0.2414486921529175,
        "step": 3240
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.3378381729125977,
        "learning_rate": 3.628796787461214e-05,
        "epoch": 0.24152321335419927,
        "step": 3241
    },
    {
        "loss": 2.5904,
        "grad_norm": 1.8268758058547974,
        "learning_rate": 3.623374830085433e-05,
        "epoch": 0.24159773455548103,
        "step": 3242
    },
    {
        "loss": 2.7301,
        "grad_norm": 3.5805256366729736,
        "learning_rate": 3.617956029985572e-05,
        "epoch": 0.2416722557567628,
        "step": 3243
    },
    {
        "loss": 1.6182,
        "grad_norm": 3.143963098526001,
        "learning_rate": 3.612540389844645e-05,
        "epoch": 0.24174677695804456,
        "step": 3244
    },
    {
        "loss": 1.698,
        "grad_norm": 4.559035301208496,
        "learning_rate": 3.607127912344128e-05,
        "epoch": 0.24182129815932632,
        "step": 3245
    },
    {
        "loss": 2.0437,
        "grad_norm": 3.0153324604034424,
        "learning_rate": 3.6017186001639036e-05,
        "epoch": 0.24189581936060808,
        "step": 3246
    },
    {
        "loss": 2.0155,
        "grad_norm": 3.415416717529297,
        "learning_rate": 3.596312455982308e-05,
        "epoch": 0.24197034056188985,
        "step": 3247
    },
    {
        "loss": 2.074,
        "grad_norm": 3.4805519580841064,
        "learning_rate": 3.590909482476092e-05,
        "epoch": 0.2420448617631716,
        "step": 3248
    },
    {
        "loss": 3.0174,
        "grad_norm": 3.0008742809295654,
        "learning_rate": 3.5855096823204505e-05,
        "epoch": 0.2421193829644534,
        "step": 3249
    },
    {
        "loss": 2.6496,
        "grad_norm": 3.4965994358062744,
        "learning_rate": 3.5801130581889985e-05,
        "epoch": 0.24219390416573516,
        "step": 3250
    },
    {
        "loss": 2.0793,
        "grad_norm": 3.2772951126098633,
        "learning_rate": 3.5747196127537817e-05,
        "epoch": 0.24226842536701693,
        "step": 3251
    },
    {
        "loss": 2.7184,
        "grad_norm": 2.033921241760254,
        "learning_rate": 3.569329348685277e-05,
        "epoch": 0.2423429465682987,
        "step": 3252
    },
    {
        "loss": 2.7277,
        "grad_norm": 3.4761977195739746,
        "learning_rate": 3.563942268652372e-05,
        "epoch": 0.24241746776958045,
        "step": 3253
    },
    {
        "loss": 0.9089,
        "grad_norm": 3.6297242641448975,
        "learning_rate": 3.5585583753223906e-05,
        "epoch": 0.24249198897086222,
        "step": 3254
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.0919198989868164,
        "learning_rate": 3.553177671361074e-05,
        "epoch": 0.24256651017214398,
        "step": 3255
    },
    {
        "loss": 2.5491,
        "grad_norm": 3.0501413345336914,
        "learning_rate": 3.547800159432586e-05,
        "epoch": 0.24264103137342574,
        "step": 3256
    },
    {
        "loss": 1.4598,
        "grad_norm": 2.7108333110809326,
        "learning_rate": 3.5424258421995105e-05,
        "epoch": 0.2427155525747075,
        "step": 3257
    },
    {
        "loss": 2.0235,
        "grad_norm": 2.112185001373291,
        "learning_rate": 3.537054722322844e-05,
        "epoch": 0.24279007377598927,
        "step": 3258
    },
    {
        "loss": 1.9559,
        "grad_norm": 3.333172082901001,
        "learning_rate": 3.531686802462004e-05,
        "epoch": 0.24286459497727103,
        "step": 3259
    },
    {
        "loss": 2.3711,
        "grad_norm": 2.080853223800659,
        "learning_rate": 3.5263220852748244e-05,
        "epoch": 0.2429391161785528,
        "step": 3260
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.6299285888671875,
        "learning_rate": 3.5209605734175574e-05,
        "epoch": 0.24301363737983456,
        "step": 3261
    },
    {
        "loss": 2.2749,
        "grad_norm": 3.067981719970703,
        "learning_rate": 3.515602269544854e-05,
        "epoch": 0.24308815858111632,
        "step": 3262
    },
    {
        "loss": 2.732,
        "grad_norm": 3.4060089588165283,
        "learning_rate": 3.5102471763097924e-05,
        "epoch": 0.24316267978239808,
        "step": 3263
    },
    {
        "loss": 1.9913,
        "grad_norm": 3.214448928833008,
        "learning_rate": 3.5048952963638537e-05,
        "epoch": 0.24323720098367985,
        "step": 3264
    },
    {
        "loss": 2.7618,
        "grad_norm": 1.8423525094985962,
        "learning_rate": 3.4995466323569216e-05,
        "epoch": 0.2433117221849616,
        "step": 3265
    },
    {
        "loss": 2.3806,
        "grad_norm": 3.2086751461029053,
        "learning_rate": 3.4942011869373084e-05,
        "epoch": 0.24338624338624337,
        "step": 3266
    },
    {
        "loss": 2.4652,
        "grad_norm": 3.0071513652801514,
        "learning_rate": 3.4888589627517074e-05,
        "epoch": 0.24346076458752516,
        "step": 3267
    },
    {
        "loss": 2.3037,
        "grad_norm": 3.1001555919647217,
        "learning_rate": 3.483519962445237e-05,
        "epoch": 0.24353528578880693,
        "step": 3268
    },
    {
        "loss": 2.3356,
        "grad_norm": 3.6163806915283203,
        "learning_rate": 3.478184188661404e-05,
        "epoch": 0.2436098069900887,
        "step": 3269
    },
    {
        "loss": 2.0368,
        "grad_norm": 3.508575201034546,
        "learning_rate": 3.472851644042127e-05,
        "epoch": 0.24368432819137045,
        "step": 3270
    },
    {
        "loss": 2.7848,
        "grad_norm": 2.1734848022460938,
        "learning_rate": 3.467522331227728e-05,
        "epoch": 0.24375884939265222,
        "step": 3271
    },
    {
        "loss": 2.6196,
        "grad_norm": 2.6412665843963623,
        "learning_rate": 3.462196252856914e-05,
        "epoch": 0.24383337059393398,
        "step": 3272
    },
    {
        "loss": 2.2288,
        "grad_norm": 2.648486852645874,
        "learning_rate": 3.4568734115668135e-05,
        "epoch": 0.24390789179521574,
        "step": 3273
    },
    {
        "loss": 2.7391,
        "grad_norm": 2.8420660495758057,
        "learning_rate": 3.4515538099929304e-05,
        "epoch": 0.2439824129964975,
        "step": 3274
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.780592918395996,
        "learning_rate": 3.446237450769176e-05,
        "epoch": 0.24405693419777927,
        "step": 3275
    },
    {
        "loss": 2.6411,
        "grad_norm": 3.471756935119629,
        "learning_rate": 3.440924336527859e-05,
        "epoch": 0.24413145539906103,
        "step": 3276
    },
    {
        "loss": 2.1985,
        "grad_norm": 2.5124008655548096,
        "learning_rate": 3.435614469899664e-05,
        "epoch": 0.2442059766003428,
        "step": 3277
    },
    {
        "loss": 0.7429,
        "grad_norm": 2.9224181175231934,
        "learning_rate": 3.430307853513698e-05,
        "epoch": 0.24428049780162456,
        "step": 3278
    },
    {
        "loss": 2.7797,
        "grad_norm": 2.295083522796631,
        "learning_rate": 3.4250044899974186e-05,
        "epoch": 0.24435501900290632,
        "step": 3279
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.391726493835449,
        "learning_rate": 3.4197043819767115e-05,
        "epoch": 0.24442954020418808,
        "step": 3280
    },
    {
        "loss": 2.3371,
        "grad_norm": 3.603729486465454,
        "learning_rate": 3.414407532075831e-05,
        "epoch": 0.24450406140546985,
        "step": 3281
    },
    {
        "loss": 3.2477,
        "grad_norm": 2.613445520401001,
        "learning_rate": 3.4091139429174135e-05,
        "epoch": 0.2445785826067516,
        "step": 3282
    },
    {
        "loss": 2.2484,
        "grad_norm": 3.155745506286621,
        "learning_rate": 3.403823617122498e-05,
        "epoch": 0.24465310380803337,
        "step": 3283
    },
    {
        "loss": 2.6416,
        "grad_norm": 2.1982853412628174,
        "learning_rate": 3.3985365573104856e-05,
        "epoch": 0.24472762500931516,
        "step": 3284
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.7435227632522583,
        "learning_rate": 3.3932527660991845e-05,
        "epoch": 0.24480214621059693,
        "step": 3285
    },
    {
        "loss": 1.7845,
        "grad_norm": 2.645998001098633,
        "learning_rate": 3.387972246104771e-05,
        "epoch": 0.2448766674118787,
        "step": 3286
    },
    {
        "loss": 2.4467,
        "grad_norm": 3.3147735595703125,
        "learning_rate": 3.3826949999417976e-05,
        "epoch": 0.24495118861316045,
        "step": 3287
    },
    {
        "loss": 2.8204,
        "grad_norm": 1.5381348133087158,
        "learning_rate": 3.377421030223209e-05,
        "epoch": 0.24502570981444222,
        "step": 3288
    },
    {
        "loss": 1.6863,
        "grad_norm": 2.228163957595825,
        "learning_rate": 3.372150339560313e-05,
        "epoch": 0.24510023101572398,
        "step": 3289
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.9469618797302246,
        "learning_rate": 3.366882930562804e-05,
        "epoch": 0.24517475221700574,
        "step": 3290
    },
    {
        "loss": 1.8642,
        "grad_norm": 3.144843816757202,
        "learning_rate": 3.361618805838749e-05,
        "epoch": 0.2452492734182875,
        "step": 3291
    },
    {
        "loss": 1.9726,
        "grad_norm": 3.945831775665283,
        "learning_rate": 3.356357967994588e-05,
        "epoch": 0.24532379461956927,
        "step": 3292
    },
    {
        "loss": 2.4966,
        "grad_norm": 3.2319328784942627,
        "learning_rate": 3.3511004196351394e-05,
        "epoch": 0.24539831582085103,
        "step": 3293
    },
    {
        "loss": 1.4334,
        "grad_norm": 3.563185214996338,
        "learning_rate": 3.345846163363579e-05,
        "epoch": 0.2454728370221328,
        "step": 3294
    },
    {
        "loss": 2.5019,
        "grad_norm": 3.4239230155944824,
        "learning_rate": 3.340595201781465e-05,
        "epoch": 0.24554735822341456,
        "step": 3295
    },
    {
        "loss": 2.9907,
        "grad_norm": 3.523484945297241,
        "learning_rate": 3.33534753748872e-05,
        "epoch": 0.24562187942469632,
        "step": 3296
    },
    {
        "loss": 2.5759,
        "grad_norm": 2.6265318393707275,
        "learning_rate": 3.330103173083639e-05,
        "epoch": 0.24569640062597808,
        "step": 3297
    },
    {
        "loss": 2.4979,
        "grad_norm": 3.0688459873199463,
        "learning_rate": 3.32486211116287e-05,
        "epoch": 0.24577092182725985,
        "step": 3298
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.765984058380127,
        "learning_rate": 3.319624354321439e-05,
        "epoch": 0.2458454430285416,
        "step": 3299
    },
    {
        "loss": 2.8278,
        "grad_norm": 1.713837742805481,
        "learning_rate": 3.314389905152731e-05,
        "epoch": 0.24591996422982337,
        "step": 3300
    },
    {
        "loss": 2.6131,
        "grad_norm": 1.7717658281326294,
        "learning_rate": 3.309158766248496e-05,
        "epoch": 0.24599448543110514,
        "step": 3301
    },
    {
        "loss": 2.4805,
        "grad_norm": 2.3464694023132324,
        "learning_rate": 3.303930940198835e-05,
        "epoch": 0.24606900663238693,
        "step": 3302
    },
    {
        "loss": 2.3782,
        "grad_norm": 3.790869951248169,
        "learning_rate": 3.29870642959222e-05,
        "epoch": 0.2461435278336687,
        "step": 3303
    },
    {
        "loss": 2.4143,
        "grad_norm": 2.5825302600860596,
        "learning_rate": 3.293485237015481e-05,
        "epoch": 0.24621804903495045,
        "step": 3304
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.261758804321289,
        "learning_rate": 3.288267365053791e-05,
        "epoch": 0.24629257023623222,
        "step": 3305
    },
    {
        "loss": 2.0407,
        "grad_norm": 2.7669365406036377,
        "learning_rate": 3.2830528162907014e-05,
        "epoch": 0.24636709143751398,
        "step": 3306
    },
    {
        "loss": 3.0627,
        "grad_norm": 2.677907705307007,
        "learning_rate": 3.2778415933080983e-05,
        "epoch": 0.24644161263879574,
        "step": 3307
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.8405308723449707,
        "learning_rate": 3.2726336986862336e-05,
        "epoch": 0.2465161338400775,
        "step": 3308
    },
    {
        "loss": 1.8049,
        "grad_norm": 2.7024943828582764,
        "learning_rate": 3.2674291350037e-05,
        "epoch": 0.24659065504135927,
        "step": 3309
    },
    {
        "loss": 2.0721,
        "grad_norm": 2.019852876663208,
        "learning_rate": 3.262227904837453e-05,
        "epoch": 0.24666517624264103,
        "step": 3310
    },
    {
        "loss": 2.2038,
        "grad_norm": 3.4570281505584717,
        "learning_rate": 3.2570300107627885e-05,
        "epoch": 0.2467396974439228,
        "step": 3311
    },
    {
        "loss": 2.6058,
        "grad_norm": 2.4311230182647705,
        "learning_rate": 3.2518354553533555e-05,
        "epoch": 0.24681421864520456,
        "step": 3312
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.6785396337509155,
        "learning_rate": 3.246644241181153e-05,
        "epoch": 0.24688873984648632,
        "step": 3313
    },
    {
        "loss": 2.4374,
        "grad_norm": 3.281524658203125,
        "learning_rate": 3.2414563708165126e-05,
        "epoch": 0.24696326104776808,
        "step": 3314
    },
    {
        "loss": 2.0935,
        "grad_norm": 2.6124963760375977,
        "learning_rate": 3.236271846828124e-05,
        "epoch": 0.24703778224904985,
        "step": 3315
    },
    {
        "loss": 2.7398,
        "grad_norm": 1.6605759859085083,
        "learning_rate": 3.2310906717830125e-05,
        "epoch": 0.2471123034503316,
        "step": 3316
    },
    {
        "loss": 2.0997,
        "grad_norm": 3.305685043334961,
        "learning_rate": 3.225912848246553e-05,
        "epoch": 0.24718682465161337,
        "step": 3317
    },
    {
        "loss": 2.8945,
        "grad_norm": 2.3705525398254395,
        "learning_rate": 3.2207383787824476e-05,
        "epoch": 0.24726134585289514,
        "step": 3318
    },
    {
        "loss": 1.8422,
        "grad_norm": 3.5946316719055176,
        "learning_rate": 3.215567265952749e-05,
        "epoch": 0.2473358670541769,
        "step": 3319
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.726104974746704,
        "learning_rate": 3.2103995123178454e-05,
        "epoch": 0.2474103882554587,
        "step": 3320
    },
    {
        "loss": 2.9915,
        "grad_norm": 2.3460516929626465,
        "learning_rate": 3.205235120436459e-05,
        "epoch": 0.24748490945674045,
        "step": 3321
    },
    {
        "loss": 2.362,
        "grad_norm": 3.181361436843872,
        "learning_rate": 3.200074092865655e-05,
        "epoch": 0.24755943065802222,
        "step": 3322
    },
    {
        "loss": 2.2531,
        "grad_norm": 2.467419147491455,
        "learning_rate": 3.194916432160818e-05,
        "epoch": 0.24763395185930398,
        "step": 3323
    },
    {
        "loss": 2.1521,
        "grad_norm": 2.1802988052368164,
        "learning_rate": 3.1897621408756804e-05,
        "epoch": 0.24770847306058574,
        "step": 3324
    },
    {
        "loss": 1.7668,
        "grad_norm": 2.14556622505188,
        "learning_rate": 3.1846112215623024e-05,
        "epoch": 0.2477829942618675,
        "step": 3325
    },
    {
        "loss": 2.2909,
        "grad_norm": 3.0250093936920166,
        "learning_rate": 3.179463676771064e-05,
        "epoch": 0.24785751546314927,
        "step": 3326
    },
    {
        "loss": 3.3429,
        "grad_norm": 2.7878036499023438,
        "learning_rate": 3.174319509050695e-05,
        "epoch": 0.24793203666443103,
        "step": 3327
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.6740915775299072,
        "learning_rate": 3.169178720948232e-05,
        "epoch": 0.2480065578657128,
        "step": 3328
    },
    {
        "loss": 2.8856,
        "grad_norm": 3.0762014389038086,
        "learning_rate": 3.1640413150090544e-05,
        "epoch": 0.24808107906699456,
        "step": 3329
    },
    {
        "loss": 2.5957,
        "grad_norm": 2.2427821159362793,
        "learning_rate": 3.158907293776853e-05,
        "epoch": 0.24815560026827632,
        "step": 3330
    },
    {
        "loss": 2.8768,
        "grad_norm": 2.6039979457855225,
        "learning_rate": 3.153776659793654e-05,
        "epoch": 0.24823012146955808,
        "step": 3331
    },
    {
        "loss": 2.2958,
        "grad_norm": 2.398663282394409,
        "learning_rate": 3.1486494155998005e-05,
        "epoch": 0.24830464267083985,
        "step": 3332
    },
    {
        "loss": 2.0742,
        "grad_norm": 2.4549686908721924,
        "learning_rate": 3.14352556373396e-05,
        "epoch": 0.2483791638721216,
        "step": 3333
    },
    {
        "loss": 2.3925,
        "grad_norm": 3.2057790756225586,
        "learning_rate": 3.138405106733124e-05,
        "epoch": 0.24845368507340337,
        "step": 3334
    },
    {
        "loss": 1.8946,
        "grad_norm": 3.1151187419891357,
        "learning_rate": 3.13328804713259e-05,
        "epoch": 0.24852820627468514,
        "step": 3335
    },
    {
        "loss": 2.4918,
        "grad_norm": 2.9648361206054688,
        "learning_rate": 3.128174387465986e-05,
        "epoch": 0.2486027274759669,
        "step": 3336
    },
    {
        "loss": 2.4946,
        "grad_norm": 2.98439359664917,
        "learning_rate": 3.1230641302652554e-05,
        "epoch": 0.24867724867724866,
        "step": 3337
    },
    {
        "loss": 2.4596,
        "grad_norm": 4.560739040374756,
        "learning_rate": 3.117957278060647e-05,
        "epoch": 0.24875176987853045,
        "step": 3338
    },
    {
        "loss": 2.7082,
        "grad_norm": 2.1286168098449707,
        "learning_rate": 3.112853833380733e-05,
        "epoch": 0.24882629107981222,
        "step": 3339
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.092736005783081,
        "learning_rate": 3.107753798752398e-05,
        "epoch": 0.24890081228109398,
        "step": 3340
    },
    {
        "loss": 2.1717,
        "grad_norm": 2.816112756729126,
        "learning_rate": 3.102657176700835e-05,
        "epoch": 0.24897533348237574,
        "step": 3341
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.5126945972442627,
        "learning_rate": 3.097563969749552e-05,
        "epoch": 0.2490498546836575,
        "step": 3342
    },
    {
        "loss": 2.4368,
        "grad_norm": 2.421912908554077,
        "learning_rate": 3.0924741804203564e-05,
        "epoch": 0.24912437588493927,
        "step": 3343
    },
    {
        "loss": 2.1219,
        "grad_norm": 2.4752674102783203,
        "learning_rate": 3.087387811233374e-05,
        "epoch": 0.24919889708622103,
        "step": 3344
    },
    {
        "loss": 2.5246,
        "grad_norm": 4.167730808258057,
        "learning_rate": 3.0823048647070265e-05,
        "epoch": 0.2492734182875028,
        "step": 3345
    },
    {
        "loss": 2.5007,
        "grad_norm": 2.633223056793213,
        "learning_rate": 3.0772253433580534e-05,
        "epoch": 0.24934793948878456,
        "step": 3346
    },
    {
        "loss": 2.6236,
        "grad_norm": 2.0999786853790283,
        "learning_rate": 3.072149249701494e-05,
        "epoch": 0.24942246069006632,
        "step": 3347
    },
    {
        "loss": 2.2121,
        "grad_norm": 3.088524580001831,
        "learning_rate": 3.067076586250681e-05,
        "epoch": 0.24949698189134809,
        "step": 3348
    },
    {
        "loss": 2.0213,
        "grad_norm": 1.5425081253051758,
        "learning_rate": 3.0620073555172635e-05,
        "epoch": 0.24957150309262985,
        "step": 3349
    },
    {
        "loss": 2.8617,
        "grad_norm": 3.012005567550659,
        "learning_rate": 3.056941560011177e-05,
        "epoch": 0.2496460242939116,
        "step": 3350
    },
    {
        "loss": 2.149,
        "grad_norm": 3.1511216163635254,
        "learning_rate": 3.051879202240666e-05,
        "epoch": 0.24972054549519337,
        "step": 3351
    },
    {
        "loss": 2.2097,
        "grad_norm": 3.878002643585205,
        "learning_rate": 3.0468202847122695e-05,
        "epoch": 0.24979506669647514,
        "step": 3352
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.2048001289367676,
        "learning_rate": 3.0417648099308226e-05,
        "epoch": 0.2498695878977569,
        "step": 3353
    },
    {
        "loss": 2.396,
        "grad_norm": 1.851462721824646,
        "learning_rate": 3.03671278039946e-05,
        "epoch": 0.24994410909903866,
        "step": 3354
    },
    {
        "loss": 2.4734,
        "grad_norm": 2.6439929008483887,
        "learning_rate": 3.0316641986196014e-05,
        "epoch": 0.25001863030032045,
        "step": 3355
    },
    {
        "loss": 2.2194,
        "grad_norm": 2.8199210166931152,
        "learning_rate": 3.0266190670909665e-05,
        "epoch": 0.2500931515016022,
        "step": 3356
    },
    {
        "loss": 1.142,
        "grad_norm": 3.6605331897735596,
        "learning_rate": 3.0215773883115706e-05,
        "epoch": 0.250167672702884,
        "step": 3357
    },
    {
        "loss": 2.1282,
        "grad_norm": 3.0638153553009033,
        "learning_rate": 3.0165391647777052e-05,
        "epoch": 0.25024219390416574,
        "step": 3358
    },
    {
        "loss": 2.8497,
        "grad_norm": 1.6838738918304443,
        "learning_rate": 3.0115043989839642e-05,
        "epoch": 0.2503167151054475,
        "step": 3359
    },
    {
        "loss": 2.6098,
        "grad_norm": 2.382249593734741,
        "learning_rate": 3.0064730934232244e-05,
        "epoch": 0.25039123630672927,
        "step": 3360
    },
    {
        "loss": 2.1813,
        "grad_norm": 1.6094553470611572,
        "learning_rate": 3.0014452505866498e-05,
        "epoch": 0.25046575750801103,
        "step": 3361
    },
    {
        "loss": 2.4002,
        "grad_norm": 3.8824262619018555,
        "learning_rate": 2.9964208729636944e-05,
        "epoch": 0.2505402787092928,
        "step": 3362
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.409679889678955,
        "learning_rate": 2.991399963042085e-05,
        "epoch": 0.25061479991057456,
        "step": 3363
    },
    {
        "loss": 2.5754,
        "grad_norm": 3.7385740280151367,
        "learning_rate": 2.9863825233078414e-05,
        "epoch": 0.2506893211118563,
        "step": 3364
    },
    {
        "loss": 2.9659,
        "grad_norm": 2.693012237548828,
        "learning_rate": 2.9813685562452653e-05,
        "epoch": 0.2507638423131381,
        "step": 3365
    },
    {
        "loss": 2.5652,
        "grad_norm": 3.385101795196533,
        "learning_rate": 2.9763580643369282e-05,
        "epoch": 0.25083836351441985,
        "step": 3366
    },
    {
        "loss": 2.4211,
        "grad_norm": 1.8061513900756836,
        "learning_rate": 2.9713510500636987e-05,
        "epoch": 0.2509128847157016,
        "step": 3367
    },
    {
        "loss": 2.874,
        "grad_norm": 2.335092782974243,
        "learning_rate": 2.966347515904706e-05,
        "epoch": 0.2509874059169834,
        "step": 3368
    },
    {
        "loss": 1.6612,
        "grad_norm": 2.5032742023468018,
        "learning_rate": 2.9613474643373696e-05,
        "epoch": 0.25106192711826514,
        "step": 3369
    },
    {
        "loss": 1.3742,
        "grad_norm": 3.4437801837921143,
        "learning_rate": 2.956350897837371e-05,
        "epoch": 0.2511364483195469,
        "step": 3370
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.3274173736572266,
        "learning_rate": 2.9513578188786796e-05,
        "epoch": 0.25121096952082866,
        "step": 3371
    },
    {
        "loss": 2.4397,
        "grad_norm": 3.6288795471191406,
        "learning_rate": 2.9463682299335306e-05,
        "epoch": 0.2512854907221104,
        "step": 3372
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.314710855484009,
        "learning_rate": 2.9413821334724334e-05,
        "epoch": 0.2513600119233922,
        "step": 3373
    },
    {
        "loss": 2.4346,
        "grad_norm": 2.527554750442505,
        "learning_rate": 2.93639953196417e-05,
        "epoch": 0.25143453312467395,
        "step": 3374
    },
    {
        "loss": 2.4414,
        "grad_norm": 1.8812636137008667,
        "learning_rate": 2.931420427875784e-05,
        "epoch": 0.2515090543259557,
        "step": 3375
    },
    {
        "loss": 2.1622,
        "grad_norm": 3.1603076457977295,
        "learning_rate": 2.9264448236725995e-05,
        "epoch": 0.2515835755272375,
        "step": 3376
    },
    {
        "loss": 2.6377,
        "grad_norm": 2.515979528427124,
        "learning_rate": 2.9214727218181905e-05,
        "epoch": 0.25165809672851924,
        "step": 3377
    },
    {
        "loss": 2.4614,
        "grad_norm": 1.717041015625,
        "learning_rate": 2.9165041247744206e-05,
        "epoch": 0.251732617929801,
        "step": 3378
    },
    {
        "loss": 2.4141,
        "grad_norm": 3.2840583324432373,
        "learning_rate": 2.911539035001396e-05,
        "epoch": 0.25180713913108277,
        "step": 3379
    },
    {
        "loss": 2.1117,
        "grad_norm": 3.2941064834594727,
        "learning_rate": 2.9065774549574974e-05,
        "epoch": 0.25188166033236453,
        "step": 3380
    },
    {
        "loss": 2.5173,
        "grad_norm": 2.5240774154663086,
        "learning_rate": 2.9016193870993693e-05,
        "epoch": 0.2519561815336463,
        "step": 3381
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.2428290843963623,
        "learning_rate": 2.8966648338819047e-05,
        "epoch": 0.2520307027349281,
        "step": 3382
    },
    {
        "loss": 2.9791,
        "grad_norm": 2.0682971477508545,
        "learning_rate": 2.8917137977582753e-05,
        "epoch": 0.2521052239362099,
        "step": 3383
    },
    {
        "loss": 2.5301,
        "grad_norm": 2.413351058959961,
        "learning_rate": 2.8867662811798946e-05,
        "epoch": 0.25217974513749164,
        "step": 3384
    },
    {
        "loss": 2.8568,
        "grad_norm": 1.6851223707199097,
        "learning_rate": 2.881822286596445e-05,
        "epoch": 0.2522542663387734,
        "step": 3385
    },
    {
        "loss": 1.9122,
        "grad_norm": 2.5630998611450195,
        "learning_rate": 2.876881816455853e-05,
        "epoch": 0.25232878754005517,
        "step": 3386
    },
    {
        "loss": 2.6194,
        "grad_norm": 1.2805899381637573,
        "learning_rate": 2.8719448732043118e-05,
        "epoch": 0.25240330874133693,
        "step": 3387
    },
    {
        "loss": 2.6389,
        "grad_norm": 2.13546085357666,
        "learning_rate": 2.867011459286263e-05,
        "epoch": 0.2524778299426187,
        "step": 3388
    },
    {
        "loss": 2.6885,
        "grad_norm": 1.8362540006637573,
        "learning_rate": 2.8620815771444008e-05,
        "epoch": 0.25255235114390046,
        "step": 3389
    },
    {
        "loss": 2.6658,
        "grad_norm": 2.478048324584961,
        "learning_rate": 2.8571552292196756e-05,
        "epoch": 0.2526268723451822,
        "step": 3390
    },
    {
        "loss": 1.7032,
        "grad_norm": 3.2223761081695557,
        "learning_rate": 2.8522324179512772e-05,
        "epoch": 0.252701393546464,
        "step": 3391
    },
    {
        "loss": 2.2049,
        "grad_norm": 4.483249664306641,
        "learning_rate": 2.8473131457766524e-05,
        "epoch": 0.25277591474774574,
        "step": 3392
    },
    {
        "loss": 2.7751,
        "grad_norm": 1.6874481439590454,
        "learning_rate": 2.8423974151314948e-05,
        "epoch": 0.2528504359490275,
        "step": 3393
    },
    {
        "loss": 2.3485,
        "grad_norm": 3.75372576713562,
        "learning_rate": 2.8374852284497446e-05,
        "epoch": 0.25292495715030927,
        "step": 3394
    },
    {
        "loss": 2.2912,
        "grad_norm": 3.351717948913574,
        "learning_rate": 2.8325765881635903e-05,
        "epoch": 0.25299947835159103,
        "step": 3395
    },
    {
        "loss": 2.179,
        "grad_norm": 2.810681104660034,
        "learning_rate": 2.8276714967034468e-05,
        "epoch": 0.2530739995528728,
        "step": 3396
    },
    {
        "loss": 3.0656,
        "grad_norm": 2.678685188293457,
        "learning_rate": 2.822769956497997e-05,
        "epoch": 0.25314852075415456,
        "step": 3397
    },
    {
        "loss": 1.6637,
        "grad_norm": 3.3545525074005127,
        "learning_rate": 2.8178719699741508e-05,
        "epoch": 0.2532230419554363,
        "step": 3398
    },
    {
        "loss": 2.6785,
        "grad_norm": 2.4631876945495605,
        "learning_rate": 2.8129775395570656e-05,
        "epoch": 0.2532975631567181,
        "step": 3399
    },
    {
        "loss": 2.5179,
        "grad_norm": 2.7760238647460938,
        "learning_rate": 2.80808666767013e-05,
        "epoch": 0.25337208435799985,
        "step": 3400
    },
    {
        "loss": 2.0794,
        "grad_norm": 2.840792655944824,
        "learning_rate": 2.80319935673497e-05,
        "epoch": 0.2534466055592816,
        "step": 3401
    },
    {
        "loss": 3.0792,
        "grad_norm": 4.126444339752197,
        "learning_rate": 2.7983156091714613e-05,
        "epoch": 0.2535211267605634,
        "step": 3402
    },
    {
        "loss": 2.6042,
        "grad_norm": 1.9534416198730469,
        "learning_rate": 2.7934354273977037e-05,
        "epoch": 0.25359564796184514,
        "step": 3403
    },
    {
        "loss": 2.7279,
        "grad_norm": 2.1140997409820557,
        "learning_rate": 2.7885588138300433e-05,
        "epoch": 0.2536701691631269,
        "step": 3404
    },
    {
        "loss": 2.4809,
        "grad_norm": 3.2828609943389893,
        "learning_rate": 2.7836857708830367e-05,
        "epoch": 0.25374469036440866,
        "step": 3405
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.476652145385742,
        "learning_rate": 2.7788163009694944e-05,
        "epoch": 0.25381921156569043,
        "step": 3406
    },
    {
        "loss": 2.3442,
        "grad_norm": 2.486785888671875,
        "learning_rate": 2.773950406500446e-05,
        "epoch": 0.2538937327669722,
        "step": 3407
    },
    {
        "loss": 1.6931,
        "grad_norm": 4.448548316955566,
        "learning_rate": 2.7690880898851666e-05,
        "epoch": 0.25396825396825395,
        "step": 3408
    },
    {
        "loss": 1.6649,
        "grad_norm": 2.607456922531128,
        "learning_rate": 2.7642293535311404e-05,
        "epoch": 0.2540427751695357,
        "step": 3409
    },
    {
        "loss": 1.6838,
        "grad_norm": 2.426119089126587,
        "learning_rate": 2.7593741998440847e-05,
        "epoch": 0.2541172963708175,
        "step": 3410
    },
    {
        "loss": 2.6394,
        "grad_norm": 2.580533027648926,
        "learning_rate": 2.7545226312279482e-05,
        "epoch": 0.25419181757209924,
        "step": 3411
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.563939332962036,
        "learning_rate": 2.749674650084897e-05,
        "epoch": 0.254266338773381,
        "step": 3412
    },
    {
        "loss": 2.1452,
        "grad_norm": 2.8841049671173096,
        "learning_rate": 2.744830258815335e-05,
        "epoch": 0.25434085997466277,
        "step": 3413
    },
    {
        "loss": 2.1276,
        "grad_norm": 4.352569103240967,
        "learning_rate": 2.7399894598178745e-05,
        "epoch": 0.25441538117594453,
        "step": 3414
    },
    {
        "loss": 1.556,
        "grad_norm": 3.089754819869995,
        "learning_rate": 2.7351522554893483e-05,
        "epoch": 0.2544899023772263,
        "step": 3415
    },
    {
        "loss": 2.7284,
        "grad_norm": 2.252129077911377,
        "learning_rate": 2.7303186482248188e-05,
        "epoch": 0.25456442357850806,
        "step": 3416
    },
    {
        "loss": 2.1536,
        "grad_norm": 3.233076572418213,
        "learning_rate": 2.7254886404175605e-05,
        "epoch": 0.2546389447797899,
        "step": 3417
    },
    {
        "loss": 2.3164,
        "grad_norm": 2.372807264328003,
        "learning_rate": 2.7206622344590806e-05,
        "epoch": 0.25471346598107164,
        "step": 3418
    },
    {
        "loss": 2.506,
        "grad_norm": 3.378920555114746,
        "learning_rate": 2.7158394327390734e-05,
        "epoch": 0.2547879871823534,
        "step": 3419
    },
    {
        "loss": 1.6867,
        "grad_norm": 3.4307219982147217,
        "learning_rate": 2.7110202376454753e-05,
        "epoch": 0.25486250838363517,
        "step": 3420
    },
    {
        "loss": 3.1126,
        "grad_norm": 3.107288122177124,
        "learning_rate": 2.7062046515644244e-05,
        "epoch": 0.25493702958491693,
        "step": 3421
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.3855931758880615,
        "learning_rate": 2.7013926768802777e-05,
        "epoch": 0.2550115507861987,
        "step": 3422
    },
    {
        "loss": 2.7052,
        "grad_norm": 2.6599080562591553,
        "learning_rate": 2.696584315975603e-05,
        "epoch": 0.25508607198748046,
        "step": 3423
    },
    {
        "loss": 2.2135,
        "grad_norm": 2.9981346130371094,
        "learning_rate": 2.691779571231172e-05,
        "epoch": 0.2551605931887622,
        "step": 3424
    },
    {
        "loss": 2.1859,
        "grad_norm": 2.7745304107666016,
        "learning_rate": 2.686978445025977e-05,
        "epoch": 0.255235114390044,
        "step": 3425
    },
    {
        "loss": 2.7224,
        "grad_norm": 2.4343502521514893,
        "learning_rate": 2.682180939737202e-05,
        "epoch": 0.25530963559132575,
        "step": 3426
    },
    {
        "loss": 2.4476,
        "grad_norm": 2.1895599365234375,
        "learning_rate": 2.6773870577402617e-05,
        "epoch": 0.2553841567926075,
        "step": 3427
    },
    {
        "loss": 1.6233,
        "grad_norm": 2.583451509475708,
        "learning_rate": 2.6725968014087598e-05,
        "epoch": 0.25545867799388927,
        "step": 3428
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.8165267705917358,
        "learning_rate": 2.6678101731145054e-05,
        "epoch": 0.25553319919517103,
        "step": 3429
    },
    {
        "loss": 2.2567,
        "grad_norm": 3.593729019165039,
        "learning_rate": 2.6630271752275214e-05,
        "epoch": 0.2556077203964528,
        "step": 3430
    },
    {
        "loss": 2.9085,
        "grad_norm": 2.657721996307373,
        "learning_rate": 2.6582478101160167e-05,
        "epoch": 0.25568224159773456,
        "step": 3431
    },
    {
        "loss": 2.8337,
        "grad_norm": 1.7168205976486206,
        "learning_rate": 2.6534720801464262e-05,
        "epoch": 0.2557567627990163,
        "step": 3432
    },
    {
        "loss": 2.6449,
        "grad_norm": 1.7524415254592896,
        "learning_rate": 2.6486999876833528e-05,
        "epoch": 0.2558312840002981,
        "step": 3433
    },
    {
        "loss": 2.3846,
        "grad_norm": 2.6969070434570312,
        "learning_rate": 2.6439315350896277e-05,
        "epoch": 0.25590580520157985,
        "step": 3434
    },
    {
        "loss": 2.2474,
        "grad_norm": 1.923885703086853,
        "learning_rate": 2.6391667247262676e-05,
        "epoch": 0.2559803264028616,
        "step": 3435
    },
    {
        "loss": 2.859,
        "grad_norm": 2.365412712097168,
        "learning_rate": 2.6344055589524764e-05,
        "epoch": 0.2560548476041434,
        "step": 3436
    },
    {
        "loss": 2.2195,
        "grad_norm": 3.7541887760162354,
        "learning_rate": 2.6296480401256807e-05,
        "epoch": 0.25612936880542514,
        "step": 3437
    },
    {
        "loss": 1.8025,
        "grad_norm": 2.770637273788452,
        "learning_rate": 2.624894170601463e-05,
        "epoch": 0.2562038900067069,
        "step": 3438
    },
    {
        "loss": 2.3053,
        "grad_norm": 2.583609104156494,
        "learning_rate": 2.6201439527336356e-05,
        "epoch": 0.25627841120798867,
        "step": 3439
    },
    {
        "loss": 1.8513,
        "grad_norm": 3.7381107807159424,
        "learning_rate": 2.6153973888741788e-05,
        "epoch": 0.25635293240927043,
        "step": 3440
    },
    {
        "loss": 1.8537,
        "grad_norm": 2.532334566116333,
        "learning_rate": 2.6106544813732747e-05,
        "epoch": 0.2564274536105522,
        "step": 3441
    },
    {
        "loss": 1.8334,
        "grad_norm": 3.0868847370147705,
        "learning_rate": 2.6059152325792946e-05,
        "epoch": 0.25650197481183395,
        "step": 3442
    },
    {
        "loss": 2.9678,
        "grad_norm": 2.5070838928222656,
        "learning_rate": 2.601179644838786e-05,
        "epoch": 0.2565764960131157,
        "step": 3443
    },
    {
        "loss": 1.8198,
        "grad_norm": 3.646275043487549,
        "learning_rate": 2.5964477204965065e-05,
        "epoch": 0.2566510172143975,
        "step": 3444
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.1838182210922241,
        "learning_rate": 2.5917194618953778e-05,
        "epoch": 0.25672553841567924,
        "step": 3445
    },
    {
        "loss": 2.3912,
        "grad_norm": 2.6582741737365723,
        "learning_rate": 2.5869948713765214e-05,
        "epoch": 0.256800059616961,
        "step": 3446
    },
    {
        "loss": 2.613,
        "grad_norm": 3.1352298259735107,
        "learning_rate": 2.5822739512792306e-05,
        "epoch": 0.25687458081824277,
        "step": 3447
    },
    {
        "loss": 2.3137,
        "grad_norm": 3.001534938812256,
        "learning_rate": 2.577556703940991e-05,
        "epoch": 0.25694910201952453,
        "step": 3448
    },
    {
        "loss": 2.0361,
        "grad_norm": 4.212582588195801,
        "learning_rate": 2.5728431316974654e-05,
        "epoch": 0.2570236232208063,
        "step": 3449
    },
    {
        "loss": 2.7525,
        "grad_norm": 2.7467234134674072,
        "learning_rate": 2.5681332368824974e-05,
        "epoch": 0.25709814442208806,
        "step": 3450
    },
    {
        "loss": 2.2025,
        "grad_norm": 4.027013301849365,
        "learning_rate": 2.5634270218281174e-05,
        "epoch": 0.2571726656233698,
        "step": 3451
    },
    {
        "loss": 1.9347,
        "grad_norm": 2.456223726272583,
        "learning_rate": 2.5587244888645113e-05,
        "epoch": 0.2572471868246516,
        "step": 3452
    },
    {
        "loss": 2.3805,
        "grad_norm": 2.3066530227661133,
        "learning_rate": 2.554025640320069e-05,
        "epoch": 0.2573217080259334,
        "step": 3453
    },
    {
        "loss": 2.6499,
        "grad_norm": 3.3467977046966553,
        "learning_rate": 2.5493304785213413e-05,
        "epoch": 0.25739622922721517,
        "step": 3454
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.648499011993408,
        "learning_rate": 2.5446390057930593e-05,
        "epoch": 0.25747075042849693,
        "step": 3455
    },
    {
        "loss": 1.9374,
        "grad_norm": 2.549297571182251,
        "learning_rate": 2.5399512244581213e-05,
        "epoch": 0.2575452716297787,
        "step": 3456
    },
    {
        "loss": 2.5587,
        "grad_norm": 2.193955659866333,
        "learning_rate": 2.535267136837597e-05,
        "epoch": 0.25761979283106046,
        "step": 3457
    },
    {
        "loss": 2.0363,
        "grad_norm": 2.546978235244751,
        "learning_rate": 2.530586745250738e-05,
        "epoch": 0.2576943140323422,
        "step": 3458
    },
    {
        "loss": 1.7376,
        "grad_norm": 1.4277317523956299,
        "learning_rate": 2.5259100520149594e-05,
        "epoch": 0.257768835233624,
        "step": 3459
    },
    {
        "loss": 2.605,
        "grad_norm": 3.606266498565674,
        "learning_rate": 2.5212370594458457e-05,
        "epoch": 0.25784335643490575,
        "step": 3460
    },
    {
        "loss": 1.9286,
        "grad_norm": 2.7812066078186035,
        "learning_rate": 2.5165677698571476e-05,
        "epoch": 0.2579178776361875,
        "step": 3461
    },
    {
        "loss": 2.6811,
        "grad_norm": 2.29305362701416,
        "learning_rate": 2.511902185560775e-05,
        "epoch": 0.2579923988374693,
        "step": 3462
    },
    {
        "loss": 2.3652,
        "grad_norm": 3.2824575901031494,
        "learning_rate": 2.5072403088668228e-05,
        "epoch": 0.25806692003875104,
        "step": 3463
    },
    {
        "loss": 1.8776,
        "grad_norm": 2.441009044647217,
        "learning_rate": 2.5025821420835337e-05,
        "epoch": 0.2581414412400328,
        "step": 3464
    },
    {
        "loss": 2.69,
        "grad_norm": 1.2794567346572876,
        "learning_rate": 2.4979276875173253e-05,
        "epoch": 0.25821596244131456,
        "step": 3465
    },
    {
        "loss": 2.5734,
        "grad_norm": 2.8772952556610107,
        "learning_rate": 2.4932769474727592e-05,
        "epoch": 0.2582904836425963,
        "step": 3466
    },
    {
        "loss": 2.1098,
        "grad_norm": 1.7661818265914917,
        "learning_rate": 2.4886299242525746e-05,
        "epoch": 0.2583650048438781,
        "step": 3467
    },
    {
        "loss": 2.0249,
        "grad_norm": 3.17738676071167,
        "learning_rate": 2.4839866201576612e-05,
        "epoch": 0.25843952604515985,
        "step": 3468
    },
    {
        "loss": 2.357,
        "grad_norm": 3.896843194961548,
        "learning_rate": 2.479347037487081e-05,
        "epoch": 0.2585140472464416,
        "step": 3469
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.897430419921875,
        "learning_rate": 2.4747111785380372e-05,
        "epoch": 0.2585885684477234,
        "step": 3470
    },
    {
        "loss": 1.2337,
        "grad_norm": 3.4850692749023438,
        "learning_rate": 2.470079045605892e-05,
        "epoch": 0.25866308964900514,
        "step": 3471
    },
    {
        "loss": 2.2062,
        "grad_norm": 3.1288089752197266,
        "learning_rate": 2.465450640984168e-05,
        "epoch": 0.2587376108502869,
        "step": 3472
    },
    {
        "loss": 2.6924,
        "grad_norm": 1.9995445013046265,
        "learning_rate": 2.4608259669645394e-05,
        "epoch": 0.25881213205156867,
        "step": 3473
    },
    {
        "loss": 2.8803,
        "grad_norm": 2.4092659950256348,
        "learning_rate": 2.4562050258368396e-05,
        "epoch": 0.25888665325285043,
        "step": 3474
    },
    {
        "loss": 2.9016,
        "grad_norm": 2.2192392349243164,
        "learning_rate": 2.451587819889045e-05,
        "epoch": 0.2589611744541322,
        "step": 3475
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.8983311653137207,
        "learning_rate": 2.4469743514072795e-05,
        "epoch": 0.25903569565541396,
        "step": 3476
    },
    {
        "loss": 2.6825,
        "grad_norm": 3.5020768642425537,
        "learning_rate": 2.442364622675828e-05,
        "epoch": 0.2591102168566957,
        "step": 3477
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.323315143585205,
        "learning_rate": 2.4377586359771176e-05,
        "epoch": 0.2591847380579775,
        "step": 3478
    },
    {
        "loss": 2.8169,
        "grad_norm": 2.3904004096984863,
        "learning_rate": 2.4331563935917245e-05,
        "epoch": 0.25925925925925924,
        "step": 3479
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.4341437816619873,
        "learning_rate": 2.4285578977983648e-05,
        "epoch": 0.259333780460541,
        "step": 3480
    },
    {
        "loss": 2.1993,
        "grad_norm": 2.0458908081054688,
        "learning_rate": 2.423963150873907e-05,
        "epoch": 0.25940830166182277,
        "step": 3481
    },
    {
        "loss": 1.9376,
        "grad_norm": 3.233604907989502,
        "learning_rate": 2.4193721550933613e-05,
        "epoch": 0.25948282286310453,
        "step": 3482
    },
    {
        "loss": 2.2036,
        "grad_norm": 3.7401318550109863,
        "learning_rate": 2.4147849127298806e-05,
        "epoch": 0.2595573440643863,
        "step": 3483
    },
    {
        "loss": 2.1149,
        "grad_norm": 3.0330352783203125,
        "learning_rate": 2.410201426054759e-05,
        "epoch": 0.25963186526566806,
        "step": 3484
    },
    {
        "loss": 1.4698,
        "grad_norm": 4.1344099044799805,
        "learning_rate": 2.405621697337428e-05,
        "epoch": 0.2597063864669498,
        "step": 3485
    },
    {
        "loss": 2.2351,
        "grad_norm": 3.8641557693481445,
        "learning_rate": 2.4010457288454657e-05,
        "epoch": 0.2597809076682316,
        "step": 3486
    },
    {
        "loss": 2.4714,
        "grad_norm": 3.2505509853363037,
        "learning_rate": 2.3964735228445755e-05,
        "epoch": 0.25985542886951335,
        "step": 3487
    },
    {
        "loss": 2.872,
        "grad_norm": 3.6200408935546875,
        "learning_rate": 2.3919050815986143e-05,
        "epoch": 0.25992995007079517,
        "step": 3488
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.1858701705932617,
        "learning_rate": 2.3873404073695683e-05,
        "epoch": 0.26000447127207693,
        "step": 3489
    },
    {
        "loss": 2.8034,
        "grad_norm": 2.2104973793029785,
        "learning_rate": 2.382779502417549e-05,
        "epoch": 0.2600789924733587,
        "step": 3490
    },
    {
        "loss": 2.7523,
        "grad_norm": 2.3780131340026855,
        "learning_rate": 2.3782223690008188e-05,
        "epoch": 0.26015351367464046,
        "step": 3491
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.8994967937469482,
        "learning_rate": 2.373669009375753e-05,
        "epoch": 0.2602280348759222,
        "step": 3492
    },
    {
        "loss": 2.3521,
        "grad_norm": 3.6556520462036133,
        "learning_rate": 2.3691194257968842e-05,
        "epoch": 0.260302556077204,
        "step": 3493
    },
    {
        "loss": 3.0951,
        "grad_norm": 4.055505752563477,
        "learning_rate": 2.3645736205168424e-05,
        "epoch": 0.26037707727848575,
        "step": 3494
    },
    {
        "loss": 2.1159,
        "grad_norm": 2.9976983070373535,
        "learning_rate": 2.3600315957864196e-05,
        "epoch": 0.2604515984797675,
        "step": 3495
    },
    {
        "loss": 2.6065,
        "grad_norm": 2.036468267440796,
        "learning_rate": 2.355493353854511e-05,
        "epoch": 0.2605261196810493,
        "step": 3496
    },
    {
        "loss": 2.0822,
        "grad_norm": 1.991429090499878,
        "learning_rate": 2.350958896968153e-05,
        "epoch": 0.26060064088233104,
        "step": 3497
    },
    {
        "loss": 1.8631,
        "grad_norm": 2.79259991645813,
        "learning_rate": 2.346428227372506e-05,
        "epoch": 0.2606751620836128,
        "step": 3498
    },
    {
        "loss": 2.2197,
        "grad_norm": 2.8887107372283936,
        "learning_rate": 2.3419013473108443e-05,
        "epoch": 0.26074968328489456,
        "step": 3499
    },
    {
        "loss": 1.8965,
        "grad_norm": 2.2651114463806152,
        "learning_rate": 2.3373782590245863e-05,
        "epoch": 0.2608242044861763,
        "step": 3500
    },
    {
        "loss": 2.9243,
        "grad_norm": 2.219564914703369,
        "learning_rate": 2.332858964753253e-05,
        "epoch": 0.2608987256874581,
        "step": 3501
    },
    {
        "loss": 1.9938,
        "grad_norm": 3.5628693103790283,
        "learning_rate": 2.328343466734496e-05,
        "epoch": 0.26097324688873985,
        "step": 3502
    },
    {
        "loss": 2.6656,
        "grad_norm": 3.983292818069458,
        "learning_rate": 2.323831767204091e-05,
        "epoch": 0.2610477680900216,
        "step": 3503
    },
    {
        "loss": 2.3772,
        "grad_norm": 2.5118632316589355,
        "learning_rate": 2.3193238683959183e-05,
        "epoch": 0.2611222892913034,
        "step": 3504
    },
    {
        "loss": 2.0674,
        "grad_norm": 4.313234806060791,
        "learning_rate": 2.3148197725419983e-05,
        "epoch": 0.26119681049258514,
        "step": 3505
    },
    {
        "loss": 2.5384,
        "grad_norm": 3.6717114448547363,
        "learning_rate": 2.3103194818724472e-05,
        "epoch": 0.2612713316938669,
        "step": 3506
    },
    {
        "loss": 2.3224,
        "grad_norm": 3.1134328842163086,
        "learning_rate": 2.30582299861551e-05,
        "epoch": 0.26134585289514867,
        "step": 3507
    },
    {
        "loss": 2.7191,
        "grad_norm": 2.2888705730438232,
        "learning_rate": 2.301330324997545e-05,
        "epoch": 0.26142037409643043,
        "step": 3508
    },
    {
        "loss": 2.9055,
        "grad_norm": 4.105466365814209,
        "learning_rate": 2.2968414632430157e-05,
        "epoch": 0.2614948952977122,
        "step": 3509
    },
    {
        "loss": 2.0791,
        "grad_norm": 2.905717134475708,
        "learning_rate": 2.2923564155745093e-05,
        "epoch": 0.26156941649899396,
        "step": 3510
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.595289945602417,
        "learning_rate": 2.2878751842127178e-05,
        "epoch": 0.2616439377002757,
        "step": 3511
    },
    {
        "loss": 2.8636,
        "grad_norm": 2.684131622314453,
        "learning_rate": 2.283397771376452e-05,
        "epoch": 0.2617184589015575,
        "step": 3512
    },
    {
        "loss": 1.9935,
        "grad_norm": 3.8119707107543945,
        "learning_rate": 2.2789241792826123e-05,
        "epoch": 0.26179298010283925,
        "step": 3513
    },
    {
        "loss": 2.703,
        "grad_norm": 2.0780088901519775,
        "learning_rate": 2.2744544101462295e-05,
        "epoch": 0.261867501304121,
        "step": 3514
    },
    {
        "loss": 2.5835,
        "grad_norm": 2.0533058643341064,
        "learning_rate": 2.2699884661804327e-05,
        "epoch": 0.26194202250540277,
        "step": 3515
    },
    {
        "loss": 2.2277,
        "grad_norm": 3.109071731567383,
        "learning_rate": 2.2655263495964586e-05,
        "epoch": 0.26201654370668453,
        "step": 3516
    },
    {
        "loss": 2.301,
        "grad_norm": 3.196988582611084,
        "learning_rate": 2.261068062603644e-05,
        "epoch": 0.2620910649079663,
        "step": 3517
    },
    {
        "loss": 2.3559,
        "grad_norm": 3.0615673065185547,
        "learning_rate": 2.256613607409428e-05,
        "epoch": 0.26216558610924806,
        "step": 3518
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.9699759483337402,
        "learning_rate": 2.252162986219365e-05,
        "epoch": 0.2622401073105298,
        "step": 3519
    },
    {
        "loss": 2.866,
        "grad_norm": 2.309255838394165,
        "learning_rate": 2.2477162012371e-05,
        "epoch": 0.2623146285118116,
        "step": 3520
    },
    {
        "loss": 2.0206,
        "grad_norm": 2.2070086002349854,
        "learning_rate": 2.2432732546643863e-05,
        "epoch": 0.26238914971309335,
        "step": 3521
    },
    {
        "loss": 2.3111,
        "grad_norm": 2.5014138221740723,
        "learning_rate": 2.238834148701068e-05,
        "epoch": 0.2624636709143751,
        "step": 3522
    },
    {
        "loss": 2.2162,
        "grad_norm": 2.681832790374756,
        "learning_rate": 2.234398885545089e-05,
        "epoch": 0.26253819211565693,
        "step": 3523
    },
    {
        "loss": 2.2409,
        "grad_norm": 3.4628031253814697,
        "learning_rate": 2.229967467392494e-05,
        "epoch": 0.2626127133169387,
        "step": 3524
    },
    {
        "loss": 2.0477,
        "grad_norm": 3.0240447521209717,
        "learning_rate": 2.225539896437432e-05,
        "epoch": 0.26268723451822046,
        "step": 3525
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.2299602031707764,
        "learning_rate": 2.221116174872131e-05,
        "epoch": 0.2627617557195022,
        "step": 3526
    },
    {
        "loss": 2.3427,
        "grad_norm": 2.712031841278076,
        "learning_rate": 2.2166963048869195e-05,
        "epoch": 0.262836276920784,
        "step": 3527
    },
    {
        "loss": 2.4268,
        "grad_norm": 2.710606336593628,
        "learning_rate": 2.212280288670222e-05,
        "epoch": 0.26291079812206575,
        "step": 3528
    },
    {
        "loss": 2.441,
        "grad_norm": 3.140387773513794,
        "learning_rate": 2.20786812840855e-05,
        "epoch": 0.2629853193233475,
        "step": 3529
    },
    {
        "loss": 2.0406,
        "grad_norm": 2.197199821472168,
        "learning_rate": 2.2034598262865158e-05,
        "epoch": 0.2630598405246293,
        "step": 3530
    },
    {
        "loss": 2.5094,
        "grad_norm": 2.505815029144287,
        "learning_rate": 2.19905538448681e-05,
        "epoch": 0.26313436172591104,
        "step": 3531
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.8315831422805786,
        "learning_rate": 2.194654805190213e-05,
        "epoch": 0.2632088829271928,
        "step": 3532
    },
    {
        "loss": 2.4614,
        "grad_norm": 6.572648525238037,
        "learning_rate": 2.1902580905755978e-05,
        "epoch": 0.26328340412847456,
        "step": 3533
    },
    {
        "loss": 2.6398,
        "grad_norm": 1.8996570110321045,
        "learning_rate": 2.185865242819919e-05,
        "epoch": 0.2633579253297563,
        "step": 3534
    },
    {
        "loss": 2.5457,
        "grad_norm": 2.297638416290283,
        "learning_rate": 2.181476264098228e-05,
        "epoch": 0.2634324465310381,
        "step": 3535
    },
    {
        "loss": 2.3437,
        "grad_norm": 2.9005415439605713,
        "learning_rate": 2.177091156583646e-05,
        "epoch": 0.26350696773231985,
        "step": 3536
    },
    {
        "loss": 2.4236,
        "grad_norm": 3.410005569458008,
        "learning_rate": 2.1727099224473822e-05,
        "epoch": 0.2635814889336016,
        "step": 3537
    },
    {
        "loss": 2.4659,
        "grad_norm": 2.0552585124969482,
        "learning_rate": 2.168332563858729e-05,
        "epoch": 0.2636560101348834,
        "step": 3538
    },
    {
        "loss": 2.1936,
        "grad_norm": 2.9417874813079834,
        "learning_rate": 2.163959082985062e-05,
        "epoch": 0.26373053133616514,
        "step": 3539
    },
    {
        "loss": 3.7051,
        "grad_norm": 2.989743232727051,
        "learning_rate": 2.1595894819918374e-05,
        "epoch": 0.2638050525374469,
        "step": 3540
    },
    {
        "loss": 2.0831,
        "grad_norm": 5.240734100341797,
        "learning_rate": 2.155223763042582e-05,
        "epoch": 0.26387957373872867,
        "step": 3541
    },
    {
        "loss": 1.8297,
        "grad_norm": 3.9118473529815674,
        "learning_rate": 2.1508619282989108e-05,
        "epoch": 0.26395409494001043,
        "step": 3542
    },
    {
        "loss": 2.4677,
        "grad_norm": 2.3120274543762207,
        "learning_rate": 2.1465039799205044e-05,
        "epoch": 0.2640286161412922,
        "step": 3543
    },
    {
        "loss": 2.0598,
        "grad_norm": 2.862436056137085,
        "learning_rate": 2.142149920065132e-05,
        "epoch": 0.26410313734257396,
        "step": 3544
    },
    {
        "loss": 1.5625,
        "grad_norm": 2.6256484985351562,
        "learning_rate": 2.137799750888633e-05,
        "epoch": 0.2641776585438557,
        "step": 3545
    },
    {
        "loss": 2.118,
        "grad_norm": 3.411606788635254,
        "learning_rate": 2.133453474544912e-05,
        "epoch": 0.2642521797451375,
        "step": 3546
    },
    {
        "loss": 2.147,
        "grad_norm": 2.77085018157959,
        "learning_rate": 2.129111093185959e-05,
        "epoch": 0.26432670094641925,
        "step": 3547
    },
    {
        "loss": 2.3246,
        "grad_norm": 2.507066249847412,
        "learning_rate": 2.1247726089618204e-05,
        "epoch": 0.264401222147701,
        "step": 3548
    },
    {
        "loss": 2.8282,
        "grad_norm": 2.1969189643859863,
        "learning_rate": 2.12043802402063e-05,
        "epoch": 0.2644757433489828,
        "step": 3549
    },
    {
        "loss": 2.4845,
        "grad_norm": 2.8758349418640137,
        "learning_rate": 2.1161073405085842e-05,
        "epoch": 0.26455026455026454,
        "step": 3550
    },
    {
        "loss": 2.328,
        "grad_norm": 2.287031888961792,
        "learning_rate": 2.1117805605699383e-05,
        "epoch": 0.2646247857515463,
        "step": 3551
    },
    {
        "loss": 1.818,
        "grad_norm": 2.3587276935577393,
        "learning_rate": 2.1074576863470297e-05,
        "epoch": 0.26469930695282806,
        "step": 3552
    },
    {
        "loss": 2.5372,
        "grad_norm": 2.3071975708007812,
        "learning_rate": 2.1031387199802456e-05,
        "epoch": 0.2647738281541098,
        "step": 3553
    },
    {
        "loss": 2.2274,
        "grad_norm": 4.754051685333252,
        "learning_rate": 2.0988236636080573e-05,
        "epoch": 0.2648483493553916,
        "step": 3554
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.1456637382507324,
        "learning_rate": 2.09451251936699e-05,
        "epoch": 0.26492287055667335,
        "step": 3555
    },
    {
        "loss": 2.54,
        "grad_norm": 2.267228841781616,
        "learning_rate": 2.0902052893916302e-05,
        "epoch": 0.2649973917579551,
        "step": 3556
    },
    {
        "loss": 2.8676,
        "grad_norm": 3.7779181003570557,
        "learning_rate": 2.0859019758146238e-05,
        "epoch": 0.2650719129592369,
        "step": 3557
    },
    {
        "loss": 2.5413,
        "grad_norm": 2.946143627166748,
        "learning_rate": 2.081602580766687e-05,
        "epoch": 0.26514643416051864,
        "step": 3558
    },
    {
        "loss": 2.6374,
        "grad_norm": 2.691683530807495,
        "learning_rate": 2.0773071063765938e-05,
        "epoch": 0.26522095536180046,
        "step": 3559
    },
    {
        "loss": 2.6321,
        "grad_norm": 1.1628936529159546,
        "learning_rate": 2.073015554771164e-05,
        "epoch": 0.2652954765630822,
        "step": 3560
    },
    {
        "loss": 1.9865,
        "grad_norm": 3.2398157119750977,
        "learning_rate": 2.068727928075298e-05,
        "epoch": 0.265369997764364,
        "step": 3561
    },
    {
        "loss": 1.3022,
        "grad_norm": 3.625166416168213,
        "learning_rate": 2.0644442284119313e-05,
        "epoch": 0.26544451896564575,
        "step": 3562
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.9432506561279297,
        "learning_rate": 2.0601644579020684e-05,
        "epoch": 0.2655190401669275,
        "step": 3563
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.730036973953247,
        "learning_rate": 2.055888618664763e-05,
        "epoch": 0.2655935613682093,
        "step": 3564
    },
    {
        "loss": 2.8557,
        "grad_norm": 2.063807249069214,
        "learning_rate": 2.0516167128171204e-05,
        "epoch": 0.26566808256949104,
        "step": 3565
    },
    {
        "loss": 2.8116,
        "grad_norm": 2.658129930496216,
        "learning_rate": 2.0473487424743032e-05,
        "epoch": 0.2657426037707728,
        "step": 3566
    },
    {
        "loss": 2.1564,
        "grad_norm": 2.485694646835327,
        "learning_rate": 2.043084709749523e-05,
        "epoch": 0.26581712497205456,
        "step": 3567
    },
    {
        "loss": 2.1046,
        "grad_norm": 2.5336575508117676,
        "learning_rate": 2.0388246167540425e-05,
        "epoch": 0.2658916461733363,
        "step": 3568
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.9737639427185059,
        "learning_rate": 2.0345684655971754e-05,
        "epoch": 0.2659661673746181,
        "step": 3569
    },
    {
        "loss": 2.1717,
        "grad_norm": 3.5698401927948,
        "learning_rate": 2.0303162583862767e-05,
        "epoch": 0.26604068857589985,
        "step": 3570
    },
    {
        "loss": 2.3976,
        "grad_norm": 2.478853225708008,
        "learning_rate": 2.0260679972267548e-05,
        "epoch": 0.2661152097771816,
        "step": 3571
    },
    {
        "loss": 2.3269,
        "grad_norm": 1.5746214389801025,
        "learning_rate": 2.021823684222064e-05,
        "epoch": 0.2661897309784634,
        "step": 3572
    },
    {
        "loss": 2.0306,
        "grad_norm": 2.617218255996704,
        "learning_rate": 2.0175833214737083e-05,
        "epoch": 0.26626425217974514,
        "step": 3573
    },
    {
        "loss": 2.3505,
        "grad_norm": 2.941143274307251,
        "learning_rate": 2.013346911081215e-05,
        "epoch": 0.2663387733810269,
        "step": 3574
    },
    {
        "loss": 2.6446,
        "grad_norm": 2.8670716285705566,
        "learning_rate": 2.0091144551421825e-05,
        "epoch": 0.26641329458230867,
        "step": 3575
    },
    {
        "loss": 2.0526,
        "grad_norm": 1.7691160440444946,
        "learning_rate": 2.004885955752235e-05,
        "epoch": 0.26648781578359043,
        "step": 3576
    },
    {
        "loss": 2.3983,
        "grad_norm": 2.639354944229126,
        "learning_rate": 2.000661415005043e-05,
        "epoch": 0.2665623369848722,
        "step": 3577
    },
    {
        "loss": 2.3427,
        "grad_norm": 2.437748908996582,
        "learning_rate": 1.9964408349923124e-05,
        "epoch": 0.26663685818615396,
        "step": 3578
    },
    {
        "loss": 1.9956,
        "grad_norm": 2.500178813934326,
        "learning_rate": 1.9922242178037864e-05,
        "epoch": 0.2667113793874357,
        "step": 3579
    },
    {
        "loss": 2.4722,
        "grad_norm": 1.8264058828353882,
        "learning_rate": 1.988011565527257e-05,
        "epoch": 0.2667859005887175,
        "step": 3580
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.809814691543579,
        "learning_rate": 1.983802880248543e-05,
        "epoch": 0.26686042178999925,
        "step": 3581
    },
    {
        "loss": 2.4491,
        "grad_norm": 2.8377749919891357,
        "learning_rate": 1.9795981640515072e-05,
        "epoch": 0.266934942991281,
        "step": 3582
    },
    {
        "loss": 2.4188,
        "grad_norm": 2.57027268409729,
        "learning_rate": 1.975397419018037e-05,
        "epoch": 0.2670094641925628,
        "step": 3583
    },
    {
        "loss": 2.807,
        "grad_norm": 2.2503557205200195,
        "learning_rate": 1.9712006472280587e-05,
        "epoch": 0.26708398539384454,
        "step": 3584
    },
    {
        "loss": 1.9943,
        "grad_norm": 3.363121271133423,
        "learning_rate": 1.967007850759529e-05,
        "epoch": 0.2671585065951263,
        "step": 3585
    },
    {
        "loss": 2.5589,
        "grad_norm": 2.012924909591675,
        "learning_rate": 1.9628190316884487e-05,
        "epoch": 0.26723302779640806,
        "step": 3586
    },
    {
        "loss": 2.6053,
        "grad_norm": 2.613708972930908,
        "learning_rate": 1.958634192088833e-05,
        "epoch": 0.2673075489976898,
        "step": 3587
    },
    {
        "loss": 2.6915,
        "grad_norm": 2.425074577331543,
        "learning_rate": 1.9544533340327297e-05,
        "epoch": 0.2673820701989716,
        "step": 3588
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.4174585342407227,
        "learning_rate": 1.9502764595902224e-05,
        "epoch": 0.26745659140025335,
        "step": 3589
    },
    {
        "loss": 2.2448,
        "grad_norm": 2.8514997959136963,
        "learning_rate": 1.9461035708294163e-05,
        "epoch": 0.2675311126015351,
        "step": 3590
    },
    {
        "loss": 2.2125,
        "grad_norm": 2.6245760917663574,
        "learning_rate": 1.941934669816451e-05,
        "epoch": 0.2676056338028169,
        "step": 3591
    },
    {
        "loss": 2.7079,
        "grad_norm": 3.41414737701416,
        "learning_rate": 1.937769758615482e-05,
        "epoch": 0.26768015500409864,
        "step": 3592
    },
    {
        "loss": 2.5135,
        "grad_norm": 2.9264938831329346,
        "learning_rate": 1.933608839288691e-05,
        "epoch": 0.2677546762053804,
        "step": 3593
    },
    {
        "loss": 2.7008,
        "grad_norm": 1.929552674293518,
        "learning_rate": 1.9294519138962875e-05,
        "epoch": 0.2678291974066622,
        "step": 3594
    },
    {
        "loss": 1.6128,
        "grad_norm": 2.438706636428833,
        "learning_rate": 1.9252989844965008e-05,
        "epoch": 0.267903718607944,
        "step": 3595
    },
    {
        "loss": 2.5202,
        "grad_norm": 2.1088666915893555,
        "learning_rate": 1.9211500531455828e-05,
        "epoch": 0.26797823980922575,
        "step": 3596
    },
    {
        "loss": 2.8658,
        "grad_norm": 1.9567219018936157,
        "learning_rate": 1.917005121897808e-05,
        "epoch": 0.2680527610105075,
        "step": 3597
    },
    {
        "loss": 2.5717,
        "grad_norm": 3.130493640899658,
        "learning_rate": 1.9128641928054624e-05,
        "epoch": 0.2681272822117893,
        "step": 3598
    },
    {
        "loss": 1.6502,
        "grad_norm": 4.356070518493652,
        "learning_rate": 1.9087272679188574e-05,
        "epoch": 0.26820180341307104,
        "step": 3599
    },
    {
        "loss": 2.1137,
        "grad_norm": 3.5957071781158447,
        "learning_rate": 1.9045943492863204e-05,
        "epoch": 0.2682763246143528,
        "step": 3600
    },
    {
        "loss": 2.7705,
        "grad_norm": 1.8717833757400513,
        "learning_rate": 1.9004654389541954e-05,
        "epoch": 0.26835084581563456,
        "step": 3601
    },
    {
        "loss": 2.2377,
        "grad_norm": 3.4519972801208496,
        "learning_rate": 1.8963405389668433e-05,
        "epoch": 0.2684253670169163,
        "step": 3602
    },
    {
        "loss": 2.6541,
        "grad_norm": 1.866668701171875,
        "learning_rate": 1.8922196513666345e-05,
        "epoch": 0.2684998882181981,
        "step": 3603
    },
    {
        "loss": 1.8684,
        "grad_norm": 4.1466779708862305,
        "learning_rate": 1.8881027781939497e-05,
        "epoch": 0.26857440941947985,
        "step": 3604
    },
    {
        "loss": 2.8311,
        "grad_norm": 1.4362179040908813,
        "learning_rate": 1.883989921487196e-05,
        "epoch": 0.2686489306207616,
        "step": 3605
    },
    {
        "loss": 2.6297,
        "grad_norm": 3.207514762878418,
        "learning_rate": 1.879881083282784e-05,
        "epoch": 0.2687234518220434,
        "step": 3606
    },
    {
        "loss": 1.937,
        "grad_norm": 3.3963747024536133,
        "learning_rate": 1.875776265615127e-05,
        "epoch": 0.26879797302332514,
        "step": 3607
    },
    {
        "loss": 2.3052,
        "grad_norm": 2.6518747806549072,
        "learning_rate": 1.871675470516662e-05,
        "epoch": 0.2688724942246069,
        "step": 3608
    },
    {
        "loss": 1.9228,
        "grad_norm": 1.7536816596984863,
        "learning_rate": 1.8675787000178168e-05,
        "epoch": 0.26894701542588867,
        "step": 3609
    },
    {
        "loss": 2.3229,
        "grad_norm": 2.858219861984253,
        "learning_rate": 1.8634859561470464e-05,
        "epoch": 0.26902153662717043,
        "step": 3610
    },
    {
        "loss": 1.9909,
        "grad_norm": 2.655362606048584,
        "learning_rate": 1.8593972409308013e-05,
        "epoch": 0.2690960578284522,
        "step": 3611
    },
    {
        "loss": 1.7466,
        "grad_norm": 3.4505884647369385,
        "learning_rate": 1.8553125563935358e-05,
        "epoch": 0.26917057902973396,
        "step": 3612
    },
    {
        "loss": 2.2289,
        "grad_norm": 1.8548574447631836,
        "learning_rate": 1.8512319045577085e-05,
        "epoch": 0.2692451002310157,
        "step": 3613
    },
    {
        "loss": 2.2276,
        "grad_norm": 2.838499069213867,
        "learning_rate": 1.8471552874437868e-05,
        "epoch": 0.2693196214322975,
        "step": 3614
    },
    {
        "loss": 2.3662,
        "grad_norm": 2.6202919483184814,
        "learning_rate": 1.843082707070235e-05,
        "epoch": 0.26939414263357925,
        "step": 3615
    },
    {
        "loss": 2.4243,
        "grad_norm": 2.9483983516693115,
        "learning_rate": 1.8390141654535265e-05,
        "epoch": 0.269468663834861,
        "step": 3616
    },
    {
        "loss": 2.2096,
        "grad_norm": 4.096780776977539,
        "learning_rate": 1.8349496646081265e-05,
        "epoch": 0.2695431850361428,
        "step": 3617
    },
    {
        "loss": 2.5912,
        "grad_norm": 1.987810492515564,
        "learning_rate": 1.8308892065465e-05,
        "epoch": 0.26961770623742454,
        "step": 3618
    },
    {
        "loss": 1.4665,
        "grad_norm": 2.966658592224121,
        "learning_rate": 1.826832793279113e-05,
        "epoch": 0.2696922274387063,
        "step": 3619
    },
    {
        "loss": 1.6365,
        "grad_norm": 3.0791866779327393,
        "learning_rate": 1.8227804268144345e-05,
        "epoch": 0.26976674863998806,
        "step": 3620
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.5395333766937256,
        "learning_rate": 1.8187321091589126e-05,
        "epoch": 0.2698412698412698,
        "step": 3621
    },
    {
        "loss": 2.1612,
        "grad_norm": 3.2810726165771484,
        "learning_rate": 1.8146878423170143e-05,
        "epoch": 0.2699157910425516,
        "step": 3622
    },
    {
        "loss": 2.3089,
        "grad_norm": 3.598267078399658,
        "learning_rate": 1.810647628291181e-05,
        "epoch": 0.26999031224383335,
        "step": 3623
    },
    {
        "loss": 2.8006,
        "grad_norm": 2.0051143169403076,
        "learning_rate": 1.806611469081856e-05,
        "epoch": 0.2700648334451151,
        "step": 3624
    },
    {
        "loss": 2.6247,
        "grad_norm": 3.3320882320404053,
        "learning_rate": 1.802579366687478e-05,
        "epoch": 0.2701393546463969,
        "step": 3625
    },
    {
        "loss": 2.7302,
        "grad_norm": 2.295849323272705,
        "learning_rate": 1.798551323104467e-05,
        "epoch": 0.27021387584767864,
        "step": 3626
    },
    {
        "loss": 2.6108,
        "grad_norm": 2.7814438343048096,
        "learning_rate": 1.7945273403272422e-05,
        "epoch": 0.2702883970489604,
        "step": 3627
    },
    {
        "loss": 2.2834,
        "grad_norm": 1.9665522575378418,
        "learning_rate": 1.7905074203482086e-05,
        "epoch": 0.27036291825024217,
        "step": 3628
    },
    {
        "loss": 2.7514,
        "grad_norm": 2.1356070041656494,
        "learning_rate": 1.7864915651577608e-05,
        "epoch": 0.27043743945152393,
        "step": 3629
    },
    {
        "loss": 2.7147,
        "grad_norm": 2.233396291732788,
        "learning_rate": 1.7824797767442825e-05,
        "epoch": 0.27051196065280575,
        "step": 3630
    },
    {
        "loss": 1.5723,
        "grad_norm": 4.327304363250732,
        "learning_rate": 1.778472057094137e-05,
        "epoch": 0.2705864818540875,
        "step": 3631
    },
    {
        "loss": 2.2303,
        "grad_norm": 3.3609964847564697,
        "learning_rate": 1.7744684081916796e-05,
        "epoch": 0.2706610030553693,
        "step": 3632
    },
    {
        "loss": 2.6602,
        "grad_norm": 2.5690674781799316,
        "learning_rate": 1.77046883201925e-05,
        "epoch": 0.27073552425665104,
        "step": 3633
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.7397181987762451,
        "learning_rate": 1.7664733305571666e-05,
        "epoch": 0.2708100454579328,
        "step": 3634
    },
    {
        "loss": 3.0114,
        "grad_norm": 1.4291070699691772,
        "learning_rate": 1.7624819057837307e-05,
        "epoch": 0.27088456665921457,
        "step": 3635
    },
    {
        "loss": 1.923,
        "grad_norm": 3.2661969661712646,
        "learning_rate": 1.758494559675231e-05,
        "epoch": 0.27095908786049633,
        "step": 3636
    },
    {
        "loss": 1.6513,
        "grad_norm": 3.028560161590576,
        "learning_rate": 1.7545112942059326e-05,
        "epoch": 0.2710336090617781,
        "step": 3637
    },
    {
        "loss": 2.8837,
        "grad_norm": 2.7313060760498047,
        "learning_rate": 1.7505321113480843e-05,
        "epoch": 0.27110813026305985,
        "step": 3638
    },
    {
        "loss": 2.7201,
        "grad_norm": 2.4460465908050537,
        "learning_rate": 1.7465570130719043e-05,
        "epoch": 0.2711826514643416,
        "step": 3639
    },
    {
        "loss": 2.1259,
        "grad_norm": 3.979262351989746,
        "learning_rate": 1.742586001345595e-05,
        "epoch": 0.2712571726656234,
        "step": 3640
    },
    {
        "loss": 1.7435,
        "grad_norm": 3.591353416442871,
        "learning_rate": 1.7386190781353317e-05,
        "epoch": 0.27133169386690514,
        "step": 3641
    },
    {
        "loss": 1.3646,
        "grad_norm": 3.4652621746063232,
        "learning_rate": 1.734656245405274e-05,
        "epoch": 0.2714062150681869,
        "step": 3642
    },
    {
        "loss": 2.2048,
        "grad_norm": 2.585291624069214,
        "learning_rate": 1.7306975051175488e-05,
        "epoch": 0.27148073626946867,
        "step": 3643
    },
    {
        "loss": 1.9058,
        "grad_norm": 2.640695810317993,
        "learning_rate": 1.7267428592322578e-05,
        "epoch": 0.27155525747075043,
        "step": 3644
    },
    {
        "loss": 2.4163,
        "grad_norm": 2.8231289386749268,
        "learning_rate": 1.7227923097074717e-05,
        "epoch": 0.2716297786720322,
        "step": 3645
    },
    {
        "loss": 2.9307,
        "grad_norm": 3.5819573402404785,
        "learning_rate": 1.7188458584992373e-05,
        "epoch": 0.27170429987331396,
        "step": 3646
    },
    {
        "loss": 2.2065,
        "grad_norm": 2.38560152053833,
        "learning_rate": 1.7149035075615794e-05,
        "epoch": 0.2717788210745957,
        "step": 3647
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.3076162338256836,
        "learning_rate": 1.7109652588464775e-05,
        "epoch": 0.2718533422758775,
        "step": 3648
    },
    {
        "loss": 2.4115,
        "grad_norm": 2.857821464538574,
        "learning_rate": 1.707031114303892e-05,
        "epoch": 0.27192786347715925,
        "step": 3649
    },
    {
        "loss": 3.0731,
        "grad_norm": 2.6199638843536377,
        "learning_rate": 1.7031010758817424e-05,
        "epoch": 0.272002384678441,
        "step": 3650
    },
    {
        "loss": 1.9084,
        "grad_norm": 3.2555534839630127,
        "learning_rate": 1.6991751455259187e-05,
        "epoch": 0.2720769058797228,
        "step": 3651
    },
    {
        "loss": 2.5888,
        "grad_norm": 2.1247105598449707,
        "learning_rate": 1.695253325180286e-05,
        "epoch": 0.27215142708100454,
        "step": 3652
    },
    {
        "loss": 2.2798,
        "grad_norm": 2.293283700942993,
        "learning_rate": 1.69133561678666e-05,
        "epoch": 0.2722259482822863,
        "step": 3653
    },
    {
        "loss": 2.1054,
        "grad_norm": 3.029101610183716,
        "learning_rate": 1.6874220222848246e-05,
        "epoch": 0.27230046948356806,
        "step": 3654
    },
    {
        "loss": 2.2797,
        "grad_norm": 2.945479393005371,
        "learning_rate": 1.683512543612531e-05,
        "epoch": 0.2723749906848498,
        "step": 3655
    },
    {
        "loss": 2.7193,
        "grad_norm": 2.020260810852051,
        "learning_rate": 1.6796071827054893e-05,
        "epoch": 0.2724495118861316,
        "step": 3656
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.982206344604492,
        "learning_rate": 1.675705941497373e-05,
        "epoch": 0.27252403308741335,
        "step": 3657
    },
    {
        "loss": 1.2853,
        "grad_norm": 1.9053281545639038,
        "learning_rate": 1.6718088219198156e-05,
        "epoch": 0.2725985542886951,
        "step": 3658
    },
    {
        "loss": 1.9437,
        "grad_norm": 2.5125746726989746,
        "learning_rate": 1.6679158259024064e-05,
        "epoch": 0.2726730754899769,
        "step": 3659
    },
    {
        "loss": 2.5082,
        "grad_norm": 2.853245496749878,
        "learning_rate": 1.6640269553726907e-05,
        "epoch": 0.27274759669125864,
        "step": 3660
    },
    {
        "loss": 2.0606,
        "grad_norm": 2.5668716430664062,
        "learning_rate": 1.6601422122561825e-05,
        "epoch": 0.2728221178925404,
        "step": 3661
    },
    {
        "loss": 2.1203,
        "grad_norm": 2.5046651363372803,
        "learning_rate": 1.6562615984763443e-05,
        "epoch": 0.27289663909382217,
        "step": 3662
    },
    {
        "loss": 2.309,
        "grad_norm": 5.0228705406188965,
        "learning_rate": 1.6523851159545946e-05,
        "epoch": 0.27297116029510393,
        "step": 3663
    },
    {
        "loss": 1.1057,
        "grad_norm": 1.1670116186141968,
        "learning_rate": 1.648512766610306e-05,
        "epoch": 0.2730456814963857,
        "step": 3664
    },
    {
        "loss": 2.6882,
        "grad_norm": 2.4182589054107666,
        "learning_rate": 1.644644552360802e-05,
        "epoch": 0.2731202026976675,
        "step": 3665
    },
    {
        "loss": 2.6491,
        "grad_norm": 3.5823440551757812,
        "learning_rate": 1.6407804751213673e-05,
        "epoch": 0.2731947238989493,
        "step": 3666
    },
    {
        "loss": 2.3894,
        "grad_norm": 2.6650326251983643,
        "learning_rate": 1.6369205368052342e-05,
        "epoch": 0.27326924510023104,
        "step": 3667
    },
    {
        "loss": 2.959,
        "grad_norm": 3.417534112930298,
        "learning_rate": 1.63306473932358e-05,
        "epoch": 0.2733437663015128,
        "step": 3668
    },
    {
        "loss": 2.3811,
        "grad_norm": 3.0146727561950684,
        "learning_rate": 1.629213084585539e-05,
        "epoch": 0.27341828750279457,
        "step": 3669
    },
    {
        "loss": 2.2436,
        "grad_norm": 3.007143974304199,
        "learning_rate": 1.6253655744981866e-05,
        "epoch": 0.27349280870407633,
        "step": 3670
    },
    {
        "loss": 2.8935,
        "grad_norm": 2.210981607437134,
        "learning_rate": 1.6215222109665573e-05,
        "epoch": 0.2735673299053581,
        "step": 3671
    },
    {
        "loss": 2.6455,
        "grad_norm": 2.1079177856445312,
        "learning_rate": 1.617682995893627e-05,
        "epoch": 0.27364185110663986,
        "step": 3672
    },
    {
        "loss": 2.6112,
        "grad_norm": 1.7998212575912476,
        "learning_rate": 1.6138479311803135e-05,
        "epoch": 0.2737163723079216,
        "step": 3673
    },
    {
        "loss": 2.5273,
        "grad_norm": 2.065876007080078,
        "learning_rate": 1.6100170187254804e-05,
        "epoch": 0.2737908935092034,
        "step": 3674
    },
    {
        "loss": 2.0483,
        "grad_norm": 3.387824773788452,
        "learning_rate": 1.6061902604259415e-05,
        "epoch": 0.27386541471048514,
        "step": 3675
    },
    {
        "loss": 2.3797,
        "grad_norm": 3.6674137115478516,
        "learning_rate": 1.6023676581764456e-05,
        "epoch": 0.2739399359117669,
        "step": 3676
    },
    {
        "loss": 2.3931,
        "grad_norm": 4.033468723297119,
        "learning_rate": 1.598549213869698e-05,
        "epoch": 0.27401445711304867,
        "step": 3677
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.175705909729004,
        "learning_rate": 1.59473492939633e-05,
        "epoch": 0.27408897831433043,
        "step": 3678
    },
    {
        "loss": 2.6065,
        "grad_norm": 1.812480092048645,
        "learning_rate": 1.5909248066449147e-05,
        "epoch": 0.2741634995156122,
        "step": 3679
    },
    {
        "loss": 2.0134,
        "grad_norm": 2.787482500076294,
        "learning_rate": 1.5871188475019714e-05,
        "epoch": 0.27423802071689396,
        "step": 3680
    },
    {
        "loss": 2.5477,
        "grad_norm": 2.2072460651397705,
        "learning_rate": 1.5833170538519605e-05,
        "epoch": 0.2743125419181757,
        "step": 3681
    },
    {
        "loss": 2.1751,
        "grad_norm": 2.9705464839935303,
        "learning_rate": 1.579519427577266e-05,
        "epoch": 0.2743870631194575,
        "step": 3682
    },
    {
        "loss": 2.145,
        "grad_norm": 2.2847719192504883,
        "learning_rate": 1.5757259705582206e-05,
        "epoch": 0.27446158432073925,
        "step": 3683
    },
    {
        "loss": 1.8819,
        "grad_norm": 3.31516695022583,
        "learning_rate": 1.57193668467309e-05,
        "epoch": 0.274536105522021,
        "step": 3684
    },
    {
        "loss": 1.9233,
        "grad_norm": 3.3095409870147705,
        "learning_rate": 1.5681515717980732e-05,
        "epoch": 0.2746106267233028,
        "step": 3685
    },
    {
        "loss": 2.3028,
        "grad_norm": 3.9055514335632324,
        "learning_rate": 1.5643706338073062e-05,
        "epoch": 0.27468514792458454,
        "step": 3686
    },
    {
        "loss": 2.5904,
        "grad_norm": 2.8095474243164062,
        "learning_rate": 1.5605938725728484e-05,
        "epoch": 0.2747596691258663,
        "step": 3687
    },
    {
        "loss": 2.8678,
        "grad_norm": 3.233733654022217,
        "learning_rate": 1.556821289964704e-05,
        "epoch": 0.27483419032714806,
        "step": 3688
    },
    {
        "loss": 2.8106,
        "grad_norm": 1.8421841859817505,
        "learning_rate": 1.553052887850799e-05,
        "epoch": 0.2749087115284298,
        "step": 3689
    },
    {
        "loss": 2.7693,
        "grad_norm": 2.763869285583496,
        "learning_rate": 1.5492886680969943e-05,
        "epoch": 0.2749832327297116,
        "step": 3690
    },
    {
        "loss": 2.5484,
        "grad_norm": 2.5644149780273438,
        "learning_rate": 1.545528632567079e-05,
        "epoch": 0.27505775393099335,
        "step": 3691
    },
    {
        "loss": 2.5366,
        "grad_norm": 2.7725517749786377,
        "learning_rate": 1.541772783122768e-05,
        "epoch": 0.2751322751322751,
        "step": 3692
    },
    {
        "loss": 2.397,
        "grad_norm": 6.191019535064697,
        "learning_rate": 1.538021121623705e-05,
        "epoch": 0.2752067963335569,
        "step": 3693
    },
    {
        "loss": 2.4533,
        "grad_norm": 4.337368965148926,
        "learning_rate": 1.5342736499274646e-05,
        "epoch": 0.27528131753483864,
        "step": 3694
    },
    {
        "loss": 2.8966,
        "grad_norm": 2.6321611404418945,
        "learning_rate": 1.5305303698895347e-05,
        "epoch": 0.2753558387361204,
        "step": 3695
    },
    {
        "loss": 2.5092,
        "grad_norm": 2.2950637340545654,
        "learning_rate": 1.5267912833633448e-05,
        "epoch": 0.27543035993740217,
        "step": 3696
    },
    {
        "loss": 2.634,
        "grad_norm": 2.345747232437134,
        "learning_rate": 1.5230563922002327e-05,
        "epoch": 0.27550488113868393,
        "step": 3697
    },
    {
        "loss": 2.3825,
        "grad_norm": 1.2365500926971436,
        "learning_rate": 1.5193256982494687e-05,
        "epoch": 0.2755794023399657,
        "step": 3698
    },
    {
        "loss": 2.4206,
        "grad_norm": 1.86837899684906,
        "learning_rate": 1.5155992033582422e-05,
        "epoch": 0.27565392354124746,
        "step": 3699
    },
    {
        "loss": 2.4517,
        "grad_norm": 4.209588050842285,
        "learning_rate": 1.5118769093716611e-05,
        "epoch": 0.2757284447425293,
        "step": 3700
    },
    {
        "loss": 2.0725,
        "grad_norm": 2.891604423522949,
        "learning_rate": 1.5081588181327521e-05,
        "epoch": 0.27580296594381104,
        "step": 3701
    },
    {
        "loss": 1.8138,
        "grad_norm": 2.1917011737823486,
        "learning_rate": 1.5044449314824649e-05,
        "epoch": 0.2758774871450928,
        "step": 3702
    },
    {
        "loss": 2.568,
        "grad_norm": 2.752302885055542,
        "learning_rate": 1.5007352512596728e-05,
        "epoch": 0.27595200834637457,
        "step": 3703
    },
    {
        "loss": 2.3872,
        "grad_norm": 1.886305570602417,
        "learning_rate": 1.4970297793011545e-05,
        "epoch": 0.27602652954765633,
        "step": 3704
    },
    {
        "loss": 2.4816,
        "grad_norm": 2.198430061340332,
        "learning_rate": 1.4933285174416122e-05,
        "epoch": 0.2761010507489381,
        "step": 3705
    },
    {
        "loss": 2.4626,
        "grad_norm": 1.429189682006836,
        "learning_rate": 1.489631467513659e-05,
        "epoch": 0.27617557195021986,
        "step": 3706
    },
    {
        "loss": 1.5851,
        "grad_norm": 2.8606011867523193,
        "learning_rate": 1.4859386313478263e-05,
        "epoch": 0.2762500931515016,
        "step": 3707
    },
    {
        "loss": 2.8355,
        "grad_norm": 2.895955801010132,
        "learning_rate": 1.4822500107725624e-05,
        "epoch": 0.2763246143527834,
        "step": 3708
    },
    {
        "loss": 2.6693,
        "grad_norm": 2.6418282985687256,
        "learning_rate": 1.4785656076142197e-05,
        "epoch": 0.27639913555406515,
        "step": 3709
    },
    {
        "loss": 2.6754,
        "grad_norm": 1.7912253141403198,
        "learning_rate": 1.474885423697071e-05,
        "epoch": 0.2764736567553469,
        "step": 3710
    },
    {
        "loss": 2.323,
        "grad_norm": 4.219856262207031,
        "learning_rate": 1.4712094608432903e-05,
        "epoch": 0.27654817795662867,
        "step": 3711
    },
    {
        "loss": 1.873,
        "grad_norm": 3.12731671333313,
        "learning_rate": 1.467537720872968e-05,
        "epoch": 0.27662269915791043,
        "step": 3712
    },
    {
        "loss": 3.3178,
        "grad_norm": 3.8712708950042725,
        "learning_rate": 1.4638702056041087e-05,
        "epoch": 0.2766972203591922,
        "step": 3713
    },
    {
        "loss": 1.8784,
        "grad_norm": 2.6877706050872803,
        "learning_rate": 1.4602069168526155e-05,
        "epoch": 0.27677174156047396,
        "step": 3714
    },
    {
        "loss": 1.1154,
        "grad_norm": 5.2089314460754395,
        "learning_rate": 1.456547856432301e-05,
        "epoch": 0.2768462627617557,
        "step": 3715
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.036376476287842,
        "learning_rate": 1.4528930261548868e-05,
        "epoch": 0.2769207839630375,
        "step": 3716
    },
    {
        "loss": 1.8149,
        "grad_norm": 3.2142889499664307,
        "learning_rate": 1.44924242783e-05,
        "epoch": 0.27699530516431925,
        "step": 3717
    },
    {
        "loss": 1.8987,
        "grad_norm": 2.987647771835327,
        "learning_rate": 1.44559606326517e-05,
        "epoch": 0.277069826365601,
        "step": 3718
    },
    {
        "loss": 2.3637,
        "grad_norm": 2.8905627727508545,
        "learning_rate": 1.441953934265835e-05,
        "epoch": 0.2771443475668828,
        "step": 3719
    },
    {
        "loss": 2.0723,
        "grad_norm": 2.8149068355560303,
        "learning_rate": 1.4383160426353292e-05,
        "epoch": 0.27721886876816454,
        "step": 3720
    },
    {
        "loss": 2.3673,
        "grad_norm": 2.660285472869873,
        "learning_rate": 1.4346823901748884e-05,
        "epoch": 0.2772933899694463,
        "step": 3721
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.9180848598480225,
        "learning_rate": 1.4310529786836591e-05,
        "epoch": 0.27736791117072807,
        "step": 3722
    },
    {
        "loss": 2.4963,
        "grad_norm": 2.443760395050049,
        "learning_rate": 1.427427809958678e-05,
        "epoch": 0.27744243237200983,
        "step": 3723
    },
    {
        "loss": 2.916,
        "grad_norm": 2.1120986938476562,
        "learning_rate": 1.4238068857948894e-05,
        "epoch": 0.2775169535732916,
        "step": 3724
    },
    {
        "loss": 2.598,
        "grad_norm": 2.5278878211975098,
        "learning_rate": 1.420190207985128e-05,
        "epoch": 0.27759147477457335,
        "step": 3725
    },
    {
        "loss": 2.9935,
        "grad_norm": 1.0708739757537842,
        "learning_rate": 1.4165777783201262e-05,
        "epoch": 0.2776659959758551,
        "step": 3726
    },
    {
        "loss": 2.682,
        "grad_norm": 3.3627915382385254,
        "learning_rate": 1.4129695985885206e-05,
        "epoch": 0.2777405171771369,
        "step": 3727
    },
    {
        "loss": 2.2993,
        "grad_norm": 2.2021420001983643,
        "learning_rate": 1.409365670576841e-05,
        "epoch": 0.27781503837841864,
        "step": 3728
    },
    {
        "loss": 1.9275,
        "grad_norm": 5.110633850097656,
        "learning_rate": 1.4057659960695069e-05,
        "epoch": 0.2778895595797004,
        "step": 3729
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.8296525478363037,
        "learning_rate": 1.4021705768488336e-05,
        "epoch": 0.27796408078098217,
        "step": 3730
    },
    {
        "loss": 2.5136,
        "grad_norm": 3.5253891944885254,
        "learning_rate": 1.3985794146950338e-05,
        "epoch": 0.27803860198226393,
        "step": 3731
    },
    {
        "loss": 2.6526,
        "grad_norm": 2.398090362548828,
        "learning_rate": 1.394992511386205e-05,
        "epoch": 0.2781131231835457,
        "step": 3732
    },
    {
        "loss": 1.8832,
        "grad_norm": 2.7283740043640137,
        "learning_rate": 1.3914098686983479e-05,
        "epoch": 0.27818764438482746,
        "step": 3733
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.658651113510132,
        "learning_rate": 1.3878314884053434e-05,
        "epoch": 0.2782621655861092,
        "step": 3734
    },
    {
        "loss": 1.9057,
        "grad_norm": 2.365433931350708,
        "learning_rate": 1.3842573722789599e-05,
        "epoch": 0.278336686787391,
        "step": 3735
    },
    {
        "loss": 2.5846,
        "grad_norm": 2.0544931888580322,
        "learning_rate": 1.3806875220888637e-05,
        "epoch": 0.2784112079886728,
        "step": 3736
    },
    {
        "loss": 2.2918,
        "grad_norm": 4.203524112701416,
        "learning_rate": 1.3771219396026024e-05,
        "epoch": 0.27848572918995457,
        "step": 3737
    },
    {
        "loss": 3.1235,
        "grad_norm": 2.60534405708313,
        "learning_rate": 1.3735606265856172e-05,
        "epoch": 0.27856025039123633,
        "step": 3738
    },
    {
        "loss": 2.6806,
        "grad_norm": 2.798961877822876,
        "learning_rate": 1.3700035848012283e-05,
        "epoch": 0.2786347715925181,
        "step": 3739
    },
    {
        "loss": 1.0618,
        "grad_norm": 3.2193970680236816,
        "learning_rate": 1.3664508160106404e-05,
        "epoch": 0.27870929279379986,
        "step": 3740
    },
    {
        "loss": 2.7699,
        "grad_norm": 1.1920580863952637,
        "learning_rate": 1.3629023219729487e-05,
        "epoch": 0.2787838139950816,
        "step": 3741
    },
    {
        "loss": 2.0156,
        "grad_norm": 3.224107503890991,
        "learning_rate": 1.3593581044451253e-05,
        "epoch": 0.2788583351963634,
        "step": 3742
    },
    {
        "loss": 1.8836,
        "grad_norm": 3.509692907333374,
        "learning_rate": 1.355818165182039e-05,
        "epoch": 0.27893285639764515,
        "step": 3743
    },
    {
        "loss": 3.3753,
        "grad_norm": 2.816340446472168,
        "learning_rate": 1.3522825059364164e-05,
        "epoch": 0.2790073775989269,
        "step": 3744
    },
    {
        "loss": 2.2642,
        "grad_norm": 2.642371892929077,
        "learning_rate": 1.3487511284588838e-05,
        "epoch": 0.27908189880020867,
        "step": 3745
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.160684823989868,
        "learning_rate": 1.3452240344979417e-05,
        "epoch": 0.27915642000149044,
        "step": 3746
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.9733686447143555,
        "learning_rate": 1.3417012257999729e-05,
        "epoch": 0.2792309412027722,
        "step": 3747
    },
    {
        "loss": 2.3322,
        "grad_norm": 3.4294769763946533,
        "learning_rate": 1.3381827041092287e-05,
        "epoch": 0.27930546240405396,
        "step": 3748
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.7774925231933594,
        "learning_rate": 1.3346684711678492e-05,
        "epoch": 0.2793799836053357,
        "step": 3749
    },
    {
        "loss": 2.6307,
        "grad_norm": 2.179668664932251,
        "learning_rate": 1.3311585287158467e-05,
        "epoch": 0.2794545048066175,
        "step": 3750
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.0549209117889404,
        "learning_rate": 1.3276528784911025e-05,
        "epoch": 0.27952902600789925,
        "step": 3751
    },
    {
        "loss": 2.5394,
        "grad_norm": 3.315892457962036,
        "learning_rate": 1.3241515222293876e-05,
        "epoch": 0.279603547209181,
        "step": 3752
    },
    {
        "loss": 2.2179,
        "grad_norm": 4.705098628997803,
        "learning_rate": 1.3206544616643313e-05,
        "epoch": 0.2796780684104628,
        "step": 3753
    },
    {
        "loss": 2.5522,
        "grad_norm": 1.8151782751083374,
        "learning_rate": 1.3171616985274448e-05,
        "epoch": 0.27975258961174454,
        "step": 3754
    },
    {
        "loss": 2.2197,
        "grad_norm": 1.4097126722335815,
        "learning_rate": 1.3136732345481129e-05,
        "epoch": 0.2798271108130263,
        "step": 3755
    },
    {
        "loss": 2.7109,
        "grad_norm": 3.2721235752105713,
        "learning_rate": 1.3101890714535813e-05,
        "epoch": 0.27990163201430807,
        "step": 3756
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.7744572162628174,
        "learning_rate": 1.3067092109689805e-05,
        "epoch": 0.27997615321558983,
        "step": 3757
    },
    {
        "loss": 2.6824,
        "grad_norm": 1.9512157440185547,
        "learning_rate": 1.3032336548172985e-05,
        "epoch": 0.2800506744168716,
        "step": 3758
    },
    {
        "loss": 2.5231,
        "grad_norm": 1.9670966863632202,
        "learning_rate": 1.299762404719399e-05,
        "epoch": 0.28012519561815336,
        "step": 3759
    },
    {
        "loss": 2.5956,
        "grad_norm": 3.087775230407715,
        "learning_rate": 1.2962954623940127e-05,
        "epoch": 0.2801997168194351,
        "step": 3760
    },
    {
        "loss": 2.4244,
        "grad_norm": 1.7538530826568604,
        "learning_rate": 1.2928328295577352e-05,
        "epoch": 0.2802742380207169,
        "step": 3761
    },
    {
        "loss": 2.0311,
        "grad_norm": 3.4543240070343018,
        "learning_rate": 1.2893745079250274e-05,
        "epoch": 0.28034875922199864,
        "step": 3762
    },
    {
        "loss": 2.0723,
        "grad_norm": 3.3384358882904053,
        "learning_rate": 1.2859204992082164e-05,
        "epoch": 0.2804232804232804,
        "step": 3763
    },
    {
        "loss": 3.0379,
        "grad_norm": 3.411217451095581,
        "learning_rate": 1.2824708051175016e-05,
        "epoch": 0.28049780162456217,
        "step": 3764
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.355931043624878,
        "learning_rate": 1.2790254273609338e-05,
        "epoch": 0.28057232282584393,
        "step": 3765
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.7811568975448608,
        "learning_rate": 1.2755843676444367e-05,
        "epoch": 0.2806468440271257,
        "step": 3766
    },
    {
        "loss": 2.862,
        "grad_norm": 2.238633394241333,
        "learning_rate": 1.2721476276717869e-05,
        "epoch": 0.28072136522840746,
        "step": 3767
    },
    {
        "loss": 2.3292,
        "grad_norm": 3.773681640625,
        "learning_rate": 1.2687152091446265e-05,
        "epoch": 0.2807958864296892,
        "step": 3768
    },
    {
        "loss": 2.5561,
        "grad_norm": 1.6899104118347168,
        "learning_rate": 1.2652871137624656e-05,
        "epoch": 0.280870407630971,
        "step": 3769
    },
    {
        "loss": 2.6043,
        "grad_norm": 2.450230836868286,
        "learning_rate": 1.2618633432226602e-05,
        "epoch": 0.28094492883225275,
        "step": 3770
    },
    {
        "loss": 2.3681,
        "grad_norm": 2.4719927310943604,
        "learning_rate": 1.258443899220434e-05,
        "epoch": 0.28101945003353457,
        "step": 3771
    },
    {
        "loss": 1.9702,
        "grad_norm": 4.400785446166992,
        "learning_rate": 1.2550287834488628e-05,
        "epoch": 0.28109397123481633,
        "step": 3772
    },
    {
        "loss": 2.8005,
        "grad_norm": 2.9547741413116455,
        "learning_rate": 1.251617997598884e-05,
        "epoch": 0.2811684924360981,
        "step": 3773
    },
    {
        "loss": 2.425,
        "grad_norm": 2.021336793899536,
        "learning_rate": 1.2482115433592889e-05,
        "epoch": 0.28124301363737986,
        "step": 3774
    },
    {
        "loss": 2.5931,
        "grad_norm": 2.4025444984436035,
        "learning_rate": 1.2448094224167273e-05,
        "epoch": 0.2813175348386616,
        "step": 3775
    },
    {
        "loss": 2.2501,
        "grad_norm": 2.7466976642608643,
        "learning_rate": 1.2414116364556983e-05,
        "epoch": 0.2813920560399434,
        "step": 3776
    },
    {
        "loss": 1.7293,
        "grad_norm": 2.5219533443450928,
        "learning_rate": 1.2380181871585527e-05,
        "epoch": 0.28146657724122515,
        "step": 3777
    },
    {
        "loss": 2.0411,
        "grad_norm": 2.567610502243042,
        "learning_rate": 1.2346290762055046e-05,
        "epoch": 0.2815410984425069,
        "step": 3778
    },
    {
        "loss": 2.4846,
        "grad_norm": 1.7127854824066162,
        "learning_rate": 1.2312443052746126e-05,
        "epoch": 0.2816156196437887,
        "step": 3779
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.2302768230438232,
        "learning_rate": 1.2278638760417882e-05,
        "epoch": 0.28169014084507044,
        "step": 3780
    },
    {
        "loss": 2.7913,
        "grad_norm": 2.685570240020752,
        "learning_rate": 1.2244877901807916e-05,
        "epoch": 0.2817646620463522,
        "step": 3781
    },
    {
        "loss": 2.0856,
        "grad_norm": 2.625890016555786,
        "learning_rate": 1.221116049363229e-05,
        "epoch": 0.28183918324763396,
        "step": 3782
    },
    {
        "loss": 2.2313,
        "grad_norm": 2.461102247238159,
        "learning_rate": 1.217748655258566e-05,
        "epoch": 0.2819137044489157,
        "step": 3783
    },
    {
        "loss": 2.6322,
        "grad_norm": 3.1777775287628174,
        "learning_rate": 1.2143856095341067e-05,
        "epoch": 0.2819882256501975,
        "step": 3784
    },
    {
        "loss": 2.0941,
        "grad_norm": 4.6725568771362305,
        "learning_rate": 1.211026913855009e-05,
        "epoch": 0.28206274685147925,
        "step": 3785
    },
    {
        "loss": 2.0858,
        "grad_norm": 3.2981784343719482,
        "learning_rate": 1.2076725698842694e-05,
        "epoch": 0.282137268052761,
        "step": 3786
    },
    {
        "loss": 2.0457,
        "grad_norm": 2.78604793548584,
        "learning_rate": 1.2043225792827306e-05,
        "epoch": 0.2822117892540428,
        "step": 3787
    },
    {
        "loss": 2.4355,
        "grad_norm": 3.412247657775879,
        "learning_rate": 1.2009769437090878e-05,
        "epoch": 0.28228631045532454,
        "step": 3788
    },
    {
        "loss": 2.5005,
        "grad_norm": 3.5972707271575928,
        "learning_rate": 1.1976356648198728e-05,
        "epoch": 0.2823608316566063,
        "step": 3789
    },
    {
        "loss": 2.7334,
        "grad_norm": 2.111279010772705,
        "learning_rate": 1.1942987442694663e-05,
        "epoch": 0.28243535285788807,
        "step": 3790
    },
    {
        "loss": 2.1154,
        "grad_norm": 4.049753665924072,
        "learning_rate": 1.1909661837100805e-05,
        "epoch": 0.28250987405916983,
        "step": 3791
    },
    {
        "loss": 2.6761,
        "grad_norm": 1.630957007408142,
        "learning_rate": 1.1876379847917762e-05,
        "epoch": 0.2825843952604516,
        "step": 3792
    },
    {
        "loss": 1.6357,
        "grad_norm": 4.596818447113037,
        "learning_rate": 1.1843141491624544e-05,
        "epoch": 0.28265891646173336,
        "step": 3793
    },
    {
        "loss": 2.7269,
        "grad_norm": 2.613283157348633,
        "learning_rate": 1.1809946784678594e-05,
        "epoch": 0.2827334376630151,
        "step": 3794
    },
    {
        "loss": 2.0322,
        "grad_norm": 3.0792436599731445,
        "learning_rate": 1.1776795743515657e-05,
        "epoch": 0.2828079588642969,
        "step": 3795
    },
    {
        "loss": 2.854,
        "grad_norm": 1.9054746627807617,
        "learning_rate": 1.174368838454989e-05,
        "epoch": 0.28288248006557865,
        "step": 3796
    },
    {
        "loss": 3.2063,
        "grad_norm": 3.250991106033325,
        "learning_rate": 1.1710624724173847e-05,
        "epoch": 0.2829570012668604,
        "step": 3797
    },
    {
        "loss": 2.4688,
        "grad_norm": 1.7634750604629517,
        "learning_rate": 1.16776047787584e-05,
        "epoch": 0.28303152246814217,
        "step": 3798
    },
    {
        "loss": 3.1645,
        "grad_norm": 3.289806365966797,
        "learning_rate": 1.1644628564652903e-05,
        "epoch": 0.28310604366942393,
        "step": 3799
    },
    {
        "loss": 2.1116,
        "grad_norm": 4.409717559814453,
        "learning_rate": 1.1611696098184844e-05,
        "epoch": 0.2831805648707057,
        "step": 3800
    },
    {
        "loss": 1.7163,
        "grad_norm": 3.4446427822113037,
        "learning_rate": 1.1578807395660207e-05,
        "epoch": 0.28325508607198746,
        "step": 3801
    },
    {
        "loss": 2.6856,
        "grad_norm": 3.1084628105163574,
        "learning_rate": 1.154596247336327e-05,
        "epoch": 0.2833296072732692,
        "step": 3802
    },
    {
        "loss": 2.0713,
        "grad_norm": 3.6973328590393066,
        "learning_rate": 1.1513161347556622e-05,
        "epoch": 0.283404128474551,
        "step": 3803
    },
    {
        "loss": 2.6361,
        "grad_norm": 2.446586847305298,
        "learning_rate": 1.1480404034481207e-05,
        "epoch": 0.28347864967583275,
        "step": 3804
    },
    {
        "loss": 2.7898,
        "grad_norm": 2.0897185802459717,
        "learning_rate": 1.1447690550356205e-05,
        "epoch": 0.2835531708771145,
        "step": 3805
    },
    {
        "loss": 1.6455,
        "grad_norm": 2.214371681213379,
        "learning_rate": 1.1415020911379138e-05,
        "epoch": 0.2836276920783963,
        "step": 3806
    },
    {
        "loss": 1.778,
        "grad_norm": 3.5281248092651367,
        "learning_rate": 1.1382395133725809e-05,
        "epoch": 0.2837022132796781,
        "step": 3807
    },
    {
        "loss": 1.5519,
        "grad_norm": 1.9055249691009521,
        "learning_rate": 1.1349813233550355e-05,
        "epoch": 0.28377673448095986,
        "step": 3808
    },
    {
        "loss": 2.3842,
        "grad_norm": 3.2316219806671143,
        "learning_rate": 1.131727522698508e-05,
        "epoch": 0.2838512556822416,
        "step": 3809
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.163940668106079,
        "learning_rate": 1.1284781130140653e-05,
        "epoch": 0.2839257768835234,
        "step": 3810
    },
    {
        "loss": 2.2238,
        "grad_norm": 3.042652130126953,
        "learning_rate": 1.1252330959105971e-05,
        "epoch": 0.28400029808480515,
        "step": 3811
    },
    {
        "loss": 1.9587,
        "grad_norm": 3.404822587966919,
        "learning_rate": 1.1219924729948116e-05,
        "epoch": 0.2840748192860869,
        "step": 3812
    },
    {
        "loss": 2.3963,
        "grad_norm": 2.568509101867676,
        "learning_rate": 1.1187562458712563e-05,
        "epoch": 0.2841493404873687,
        "step": 3813
    },
    {
        "loss": 2.5406,
        "grad_norm": 3.0191688537597656,
        "learning_rate": 1.1155244161422874e-05,
        "epoch": 0.28422386168865044,
        "step": 3814
    },
    {
        "loss": 3.0909,
        "grad_norm": 1.675898790359497,
        "learning_rate": 1.11229698540809e-05,
        "epoch": 0.2842983828899322,
        "step": 3815
    },
    {
        "loss": 2.6969,
        "grad_norm": 1.9989206790924072,
        "learning_rate": 1.109073955266674e-05,
        "epoch": 0.28437290409121396,
        "step": 3816
    },
    {
        "loss": 3.0686,
        "grad_norm": 1.8165671825408936,
        "learning_rate": 1.1058553273138606e-05,
        "epoch": 0.2844474252924957,
        "step": 3817
    },
    {
        "loss": 1.9347,
        "grad_norm": 3.4020755290985107,
        "learning_rate": 1.1026411031433093e-05,
        "epoch": 0.2845219464937775,
        "step": 3818
    },
    {
        "loss": 2.0289,
        "grad_norm": 3.0428099632263184,
        "learning_rate": 1.0994312843464738e-05,
        "epoch": 0.28459646769505925,
        "step": 3819
    },
    {
        "loss": 2.2936,
        "grad_norm": 3.132225513458252,
        "learning_rate": 1.0962258725126507e-05,
        "epoch": 0.284670988896341,
        "step": 3820
    },
    {
        "loss": 2.287,
        "grad_norm": 2.5199904441833496,
        "learning_rate": 1.0930248692289402e-05,
        "epoch": 0.2847455100976228,
        "step": 3821
    },
    {
        "loss": 2.6321,
        "grad_norm": 3.23880934715271,
        "learning_rate": 1.0898282760802658e-05,
        "epoch": 0.28482003129890454,
        "step": 3822
    },
    {
        "loss": 2.3747,
        "grad_norm": 3.2935385704040527,
        "learning_rate": 1.0866360946493625e-05,
        "epoch": 0.2848945525001863,
        "step": 3823
    },
    {
        "loss": 2.3751,
        "grad_norm": 4.625612735748291,
        "learning_rate": 1.0834483265167828e-05,
        "epoch": 0.28496907370146807,
        "step": 3824
    },
    {
        "loss": 2.4876,
        "grad_norm": 3.568709373474121,
        "learning_rate": 1.0802649732609028e-05,
        "epoch": 0.28504359490274983,
        "step": 3825
    },
    {
        "loss": 2.8314,
        "grad_norm": 1.4437897205352783,
        "learning_rate": 1.0770860364578984e-05,
        "epoch": 0.2851181161040316,
        "step": 3826
    },
    {
        "loss": 2.5905,
        "grad_norm": 2.39715313911438,
        "learning_rate": 1.0739115176817672e-05,
        "epoch": 0.28519263730531336,
        "step": 3827
    },
    {
        "loss": 1.7643,
        "grad_norm": 3.1092679500579834,
        "learning_rate": 1.0707414185043163e-05,
        "epoch": 0.2852671585065951,
        "step": 3828
    },
    {
        "loss": 2.6872,
        "grad_norm": 2.5492405891418457,
        "learning_rate": 1.0675757404951647e-05,
        "epoch": 0.2853416797078769,
        "step": 3829
    },
    {
        "loss": 2.5599,
        "grad_norm": 1.7791448831558228,
        "learning_rate": 1.0644144852217474e-05,
        "epoch": 0.28541620090915865,
        "step": 3830
    },
    {
        "loss": 1.8118,
        "grad_norm": 4.001297950744629,
        "learning_rate": 1.0612576542493025e-05,
        "epoch": 0.2854907221104404,
        "step": 3831
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.1093482971191406,
        "learning_rate": 1.0581052491408815e-05,
        "epoch": 0.28556524331172217,
        "step": 3832
    },
    {
        "loss": 2.6361,
        "grad_norm": 2.90183424949646,
        "learning_rate": 1.054957271457342e-05,
        "epoch": 0.28563976451300394,
        "step": 3833
    },
    {
        "loss": 2.0738,
        "grad_norm": 2.8723130226135254,
        "learning_rate": 1.051813722757351e-05,
        "epoch": 0.2857142857142857,
        "step": 3834
    },
    {
        "loss": 2.2361,
        "grad_norm": 2.5664353370666504,
        "learning_rate": 1.0486746045973828e-05,
        "epoch": 0.28578880691556746,
        "step": 3835
    },
    {
        "loss": 2.3131,
        "grad_norm": 2.6769204139709473,
        "learning_rate": 1.0455399185317172e-05,
        "epoch": 0.2858633281168492,
        "step": 3836
    },
    {
        "loss": 1.8918,
        "grad_norm": 2.653156280517578,
        "learning_rate": 1.042409666112445e-05,
        "epoch": 0.285937849318131,
        "step": 3837
    },
    {
        "loss": 2.3326,
        "grad_norm": 2.3160345554351807,
        "learning_rate": 1.0392838488894463e-05,
        "epoch": 0.28601237051941275,
        "step": 3838
    },
    {
        "loss": 2.0067,
        "grad_norm": 3.647062063217163,
        "learning_rate": 1.036162468410422e-05,
        "epoch": 0.2860868917206945,
        "step": 3839
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.642850637435913,
        "learning_rate": 1.0330455262208704e-05,
        "epoch": 0.2861614129219763,
        "step": 3840
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.0157792568206787,
        "learning_rate": 1.0299330238640925e-05,
        "epoch": 0.28623593412325804,
        "step": 3841
    },
    {
        "loss": 2.3748,
        "grad_norm": 3.5517191886901855,
        "learning_rate": 1.0268249628811866e-05,
        "epoch": 0.28631045532453986,
        "step": 3842
    },
    {
        "loss": 2.5432,
        "grad_norm": 3.8503060340881348,
        "learning_rate": 1.0237213448110539e-05,
        "epoch": 0.2863849765258216,
        "step": 3843
    },
    {
        "loss": 2.239,
        "grad_norm": 2.5489301681518555,
        "learning_rate": 1.020622171190403e-05,
        "epoch": 0.2864594977271034,
        "step": 3844
    },
    {
        "loss": 2.4272,
        "grad_norm": 1.9118382930755615,
        "learning_rate": 1.0175274435537319e-05,
        "epoch": 0.28653401892838515,
        "step": 3845
    },
    {
        "loss": 2.5045,
        "grad_norm": 2.776128053665161,
        "learning_rate": 1.0144371634333482e-05,
        "epoch": 0.2866085401296669,
        "step": 3846
    },
    {
        "loss": 1.9128,
        "grad_norm": 2.6411197185516357,
        "learning_rate": 1.0113513323593416e-05,
        "epoch": 0.2866830613309487,
        "step": 3847
    },
    {
        "loss": 1.9175,
        "grad_norm": 3.0201303958892822,
        "learning_rate": 1.0082699518596117e-05,
        "epoch": 0.28675758253223044,
        "step": 3848
    },
    {
        "loss": 2.4915,
        "grad_norm": 1.9367949962615967,
        "learning_rate": 1.0051930234598505e-05,
        "epoch": 0.2868321037335122,
        "step": 3849
    },
    {
        "loss": 2.4967,
        "grad_norm": 2.2419302463531494,
        "learning_rate": 1.0021205486835505e-05,
        "epoch": 0.28690662493479396,
        "step": 3850
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.037320137023926,
        "learning_rate": 9.990525290519914e-06,
        "epoch": 0.2869811461360757,
        "step": 3851
    },
    {
        "loss": 2.625,
        "grad_norm": 3.0566396713256836,
        "learning_rate": 9.95988966084247e-06,
        "epoch": 0.2870556673373575,
        "step": 3852
    },
    {
        "loss": 1.9448,
        "grad_norm": 2.908987522125244,
        "learning_rate": 9.929298612971904e-06,
        "epoch": 0.28713018853863925,
        "step": 3853
    },
    {
        "loss": 2.6666,
        "grad_norm": 2.908761978149414,
        "learning_rate": 9.898752162054836e-06,
        "epoch": 0.287204709739921,
        "step": 3854
    },
    {
        "loss": 2.7819,
        "grad_norm": 1.6143364906311035,
        "learning_rate": 9.868250323215855e-06,
        "epoch": 0.2872792309412028,
        "step": 3855
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.2672834396362305,
        "learning_rate": 9.837793111557392e-06,
        "epoch": 0.28735375214248454,
        "step": 3856
    },
    {
        "loss": 2.4026,
        "grad_norm": 1.6685270071029663,
        "learning_rate": 9.807380542159795e-06,
        "epoch": 0.2874282733437663,
        "step": 3857
    },
    {
        "loss": 2.634,
        "grad_norm": 2.977247953414917,
        "learning_rate": 9.777012630081317e-06,
        "epoch": 0.28750279454504807,
        "step": 3858
    },
    {
        "loss": 2.5394,
        "grad_norm": 1.9176292419433594,
        "learning_rate": 9.746689390358121e-06,
        "epoch": 0.28757731574632983,
        "step": 3859
    },
    {
        "loss": 2.8923,
        "grad_norm": 2.1932928562164307,
        "learning_rate": 9.716410838004286e-06,
        "epoch": 0.2876518369476116,
        "step": 3860
    },
    {
        "loss": 2.2597,
        "grad_norm": 2.9437825679779053,
        "learning_rate": 9.68617698801163e-06,
        "epoch": 0.28772635814889336,
        "step": 3861
    },
    {
        "loss": 2.2462,
        "grad_norm": 4.330979347229004,
        "learning_rate": 9.655987855349969e-06,
        "epoch": 0.2878008793501751,
        "step": 3862
    },
    {
        "loss": 1.7721,
        "grad_norm": 2.804901361465454,
        "learning_rate": 9.62584345496691e-06,
        "epoch": 0.2878754005514569,
        "step": 3863
    },
    {
        "loss": 1.799,
        "grad_norm": 1.7591725587844849,
        "learning_rate": 9.595743801787948e-06,
        "epoch": 0.28794992175273865,
        "step": 3864
    },
    {
        "loss": 2.0891,
        "grad_norm": 3.1668474674224854,
        "learning_rate": 9.56568891071642e-06,
        "epoch": 0.2880244429540204,
        "step": 3865
    },
    {
        "loss": 2.6858,
        "grad_norm": 2.275597095489502,
        "learning_rate": 9.53567879663345e-06,
        "epoch": 0.2880989641553022,
        "step": 3866
    },
    {
        "loss": 2.3522,
        "grad_norm": 2.154484272003174,
        "learning_rate": 9.50571347439807e-06,
        "epoch": 0.28817348535658394,
        "step": 3867
    },
    {
        "loss": 2.507,
        "grad_norm": 2.8957316875457764,
        "learning_rate": 9.475792958847029e-06,
        "epoch": 0.2882480065578657,
        "step": 3868
    },
    {
        "loss": 2.6813,
        "grad_norm": 2.755573034286499,
        "learning_rate": 9.445917264795034e-06,
        "epoch": 0.28832252775914746,
        "step": 3869
    },
    {
        "loss": 2.7158,
        "grad_norm": 3.3814914226531982,
        "learning_rate": 9.416086407034464e-06,
        "epoch": 0.2883970489604292,
        "step": 3870
    },
    {
        "loss": 2.75,
        "grad_norm": 3.0878913402557373,
        "learning_rate": 9.386300400335567e-06,
        "epoch": 0.288471570161711,
        "step": 3871
    },
    {
        "loss": 3.1738,
        "grad_norm": 2.141031503677368,
        "learning_rate": 9.356559259446396e-06,
        "epoch": 0.28854609136299275,
        "step": 3872
    },
    {
        "loss": 2.0771,
        "grad_norm": 3.046243190765381,
        "learning_rate": 9.3268629990927e-06,
        "epoch": 0.2886206125642745,
        "step": 3873
    },
    {
        "loss": 3.0731,
        "grad_norm": 2.972865343093872,
        "learning_rate": 9.297211633978142e-06,
        "epoch": 0.2886951337655563,
        "step": 3874
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.4706053733825684,
        "learning_rate": 9.267605178784033e-06,
        "epoch": 0.28876965496683804,
        "step": 3875
    },
    {
        "loss": 2.2705,
        "grad_norm": 2.38199520111084,
        "learning_rate": 9.238043648169525e-06,
        "epoch": 0.2888441761681198,
        "step": 3876
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.948603868484497,
        "learning_rate": 9.20852705677151e-06,
        "epoch": 0.2889186973694016,
        "step": 3877
    },
    {
        "loss": 2.5427,
        "grad_norm": 3.2998223304748535,
        "learning_rate": 9.179055419204563e-06,
        "epoch": 0.2889932185706834,
        "step": 3878
    },
    {
        "loss": 2.6691,
        "grad_norm": 1.9512557983398438,
        "learning_rate": 9.149628750061146e-06,
        "epoch": 0.28906773977196515,
        "step": 3879
    },
    {
        "loss": 2.589,
        "grad_norm": 1.7568608522415161,
        "learning_rate": 9.12024706391127e-06,
        "epoch": 0.2891422609732469,
        "step": 3880
    },
    {
        "loss": 2.7407,
        "grad_norm": 2.856977939605713,
        "learning_rate": 9.090910375302842e-06,
        "epoch": 0.2892167821745287,
        "step": 3881
    },
    {
        "loss": 2.571,
        "grad_norm": 4.109293460845947,
        "learning_rate": 9.061618698761376e-06,
        "epoch": 0.28929130337581044,
        "step": 3882
    },
    {
        "loss": 2.9629,
        "grad_norm": 2.483152151107788,
        "learning_rate": 9.03237204879015e-06,
        "epoch": 0.2893658245770922,
        "step": 3883
    },
    {
        "loss": 2.3839,
        "grad_norm": 3.06181263923645,
        "learning_rate": 9.003170439870168e-06,
        "epoch": 0.28944034577837396,
        "step": 3884
    },
    {
        "loss": 1.7686,
        "grad_norm": 3.4440720081329346,
        "learning_rate": 8.974013886460042e-06,
        "epoch": 0.2895148669796557,
        "step": 3885
    },
    {
        "loss": 2.7602,
        "grad_norm": 2.9545326232910156,
        "learning_rate": 8.944902402996203e-06,
        "epoch": 0.2895893881809375,
        "step": 3886
    },
    {
        "loss": 2.9691,
        "grad_norm": 2.919236898422241,
        "learning_rate": 8.915836003892652e-06,
        "epoch": 0.28966390938221925,
        "step": 3887
    },
    {
        "loss": 2.5438,
        "grad_norm": 2.568743944168091,
        "learning_rate": 8.886814703541147e-06,
        "epoch": 0.289738430583501,
        "step": 3888
    },
    {
        "loss": 1.8078,
        "grad_norm": 4.589808940887451,
        "learning_rate": 8.857838516311046e-06,
        "epoch": 0.2898129517847828,
        "step": 3889
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.824810028076172,
        "learning_rate": 8.828907456549441e-06,
        "epoch": 0.28988747298606454,
        "step": 3890
    },
    {
        "loss": 2.674,
        "grad_norm": 1.9284898042678833,
        "learning_rate": 8.800021538581027e-06,
        "epoch": 0.2899619941873463,
        "step": 3891
    },
    {
        "loss": 2.1889,
        "grad_norm": 2.488558769226074,
        "learning_rate": 8.771180776708188e-06,
        "epoch": 0.29003651538862807,
        "step": 3892
    },
    {
        "loss": 2.3439,
        "grad_norm": 2.1009018421173096,
        "learning_rate": 8.742385185210944e-06,
        "epoch": 0.29011103658990983,
        "step": 3893
    },
    {
        "loss": 1.4168,
        "grad_norm": 2.9360225200653076,
        "learning_rate": 8.713634778346868e-06,
        "epoch": 0.2901855577911916,
        "step": 3894
    },
    {
        "loss": 2.2213,
        "grad_norm": 3.44807767868042,
        "learning_rate": 8.684929570351275e-06,
        "epoch": 0.29026007899247336,
        "step": 3895
    },
    {
        "loss": 2.6571,
        "grad_norm": 2.4778480529785156,
        "learning_rate": 8.65626957543706e-06,
        "epoch": 0.2903346001937551,
        "step": 3896
    },
    {
        "loss": 2.6449,
        "grad_norm": 2.412743091583252,
        "learning_rate": 8.627654807794705e-06,
        "epoch": 0.2904091213950369,
        "step": 3897
    },
    {
        "loss": 1.2751,
        "grad_norm": 3.3769521713256836,
        "learning_rate": 8.599085281592355e-06,
        "epoch": 0.29048364259631865,
        "step": 3898
    },
    {
        "loss": 2.5756,
        "grad_norm": 4.547764778137207,
        "learning_rate": 8.570561010975654e-06,
        "epoch": 0.2905581637976004,
        "step": 3899
    },
    {
        "loss": 2.5894,
        "grad_norm": 2.3154516220092773,
        "learning_rate": 8.542082010067942e-06,
        "epoch": 0.2906326849988822,
        "step": 3900
    },
    {
        "loss": 2.6276,
        "grad_norm": 2.563039779663086,
        "learning_rate": 8.513648292970111e-06,
        "epoch": 0.29070720620016394,
        "step": 3901
    },
    {
        "loss": 2.7414,
        "grad_norm": 2.368342399597168,
        "learning_rate": 8.48525987376062e-06,
        "epoch": 0.2907817274014457,
        "step": 3902
    },
    {
        "loss": 2.8008,
        "grad_norm": 3.19722843170166,
        "learning_rate": 8.456916766495515e-06,
        "epoch": 0.29085624860272746,
        "step": 3903
    },
    {
        "loss": 1.8174,
        "grad_norm": 1.959891438484192,
        "learning_rate": 8.428618985208336e-06,
        "epoch": 0.2909307698040092,
        "step": 3904
    },
    {
        "loss": 2.9712,
        "grad_norm": 2.941584587097168,
        "learning_rate": 8.400366543910298e-06,
        "epoch": 0.291005291005291,
        "step": 3905
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.6516659259796143,
        "learning_rate": 8.372159456590112e-06,
        "epoch": 0.29107981220657275,
        "step": 3906
    },
    {
        "loss": 2.2151,
        "grad_norm": 4.472414970397949,
        "learning_rate": 8.343997737214048e-06,
        "epoch": 0.2911543334078545,
        "step": 3907
    },
    {
        "loss": 2.016,
        "grad_norm": 3.419881820678711,
        "learning_rate": 8.315881399725823e-06,
        "epoch": 0.2912288546091363,
        "step": 3908
    },
    {
        "loss": 2.773,
        "grad_norm": 2.2039999961853027,
        "learning_rate": 8.287810458046808e-06,
        "epoch": 0.29130337581041804,
        "step": 3909
    },
    {
        "loss": 1.0826,
        "grad_norm": 3.0439884662628174,
        "learning_rate": 8.259784926075808e-06,
        "epoch": 0.2913778970116998,
        "step": 3910
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.7757731676101685,
        "learning_rate": 8.231804817689248e-06,
        "epoch": 0.29145241821298157,
        "step": 3911
    },
    {
        "loss": 2.3966,
        "grad_norm": 3.29948091506958,
        "learning_rate": 8.203870146740956e-06,
        "epoch": 0.29152693941426333,
        "step": 3912
    },
    {
        "loss": 2.5271,
        "grad_norm": 2.8506019115448,
        "learning_rate": 8.175980927062288e-06,
        "epoch": 0.29160146061554515,
        "step": 3913
    },
    {
        "loss": 2.3997,
        "grad_norm": 2.337498664855957,
        "learning_rate": 8.148137172462112e-06,
        "epoch": 0.2916759818168269,
        "step": 3914
    },
    {
        "loss": 2.1339,
        "grad_norm": 3.964179277420044,
        "learning_rate": 8.120338896726786e-06,
        "epoch": 0.2917505030181087,
        "step": 3915
    },
    {
        "loss": 1.4397,
        "grad_norm": 3.329416275024414,
        "learning_rate": 8.092586113620215e-06,
        "epoch": 0.29182502421939044,
        "step": 3916
    },
    {
        "loss": 1.876,
        "grad_norm": 2.9988009929656982,
        "learning_rate": 8.064878836883605e-06,
        "epoch": 0.2918995454206722,
        "step": 3917
    },
    {
        "loss": 1.691,
        "grad_norm": 3.4871702194213867,
        "learning_rate": 8.037217080235792e-06,
        "epoch": 0.29197406662195396,
        "step": 3918
    },
    {
        "loss": 2.7815,
        "grad_norm": 3.9811418056488037,
        "learning_rate": 8.009600857373e-06,
        "epoch": 0.29204858782323573,
        "step": 3919
    },
    {
        "loss": 1.9589,
        "grad_norm": 3.0645532608032227,
        "learning_rate": 7.982030181968914e-06,
        "epoch": 0.2921231090245175,
        "step": 3920
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.0341060161590576,
        "learning_rate": 7.954505067674756e-06,
        "epoch": 0.29219763022579925,
        "step": 3921
    },
    {
        "loss": 2.8293,
        "grad_norm": 2.2650680541992188,
        "learning_rate": 7.927025528119014e-06,
        "epoch": 0.292272151427081,
        "step": 3922
    },
    {
        "loss": 2.1673,
        "grad_norm": 3.131366729736328,
        "learning_rate": 7.89959157690775e-06,
        "epoch": 0.2923466726283628,
        "step": 3923
    },
    {
        "loss": 2.06,
        "grad_norm": 3.418900966644287,
        "learning_rate": 7.872203227624398e-06,
        "epoch": 0.29242119382964454,
        "step": 3924
    },
    {
        "loss": 2.5258,
        "grad_norm": 2.5037434101104736,
        "learning_rate": 7.844860493829841e-06,
        "epoch": 0.2924957150309263,
        "step": 3925
    },
    {
        "loss": 2.582,
        "grad_norm": 1.9793187379837036,
        "learning_rate": 7.817563389062377e-06,
        "epoch": 0.29257023623220807,
        "step": 3926
    },
    {
        "loss": 0.8921,
        "grad_norm": 2.8675670623779297,
        "learning_rate": 7.790311926837656e-06,
        "epoch": 0.29264475743348983,
        "step": 3927
    },
    {
        "loss": 1.0374,
        "grad_norm": 3.58225417137146,
        "learning_rate": 7.763106120648812e-06,
        "epoch": 0.2927192786347716,
        "step": 3928
    },
    {
        "loss": 2.4444,
        "grad_norm": 2.7939655780792236,
        "learning_rate": 7.735945983966275e-06,
        "epoch": 0.29279379983605336,
        "step": 3929
    },
    {
        "loss": 2.3897,
        "grad_norm": 3.2101385593414307,
        "learning_rate": 7.708831530237959e-06,
        "epoch": 0.2928683210373351,
        "step": 3930
    },
    {
        "loss": 2.2478,
        "grad_norm": 2.7744481563568115,
        "learning_rate": 7.681762772889134e-06,
        "epoch": 0.2929428422386169,
        "step": 3931
    },
    {
        "loss": 2.6059,
        "grad_norm": 3.076321601867676,
        "learning_rate": 7.654739725322391e-06,
        "epoch": 0.29301736343989865,
        "step": 3932
    },
    {
        "loss": 2.4099,
        "grad_norm": 2.2252485752105713,
        "learning_rate": 7.627762400917748e-06,
        "epoch": 0.2930918846411804,
        "step": 3933
    },
    {
        "loss": 1.5784,
        "grad_norm": 2.7767598628997803,
        "learning_rate": 7.600830813032511e-06,
        "epoch": 0.2931664058424622,
        "step": 3934
    },
    {
        "loss": 2.503,
        "grad_norm": 1.8249064683914185,
        "learning_rate": 7.573944975001502e-06,
        "epoch": 0.29324092704374394,
        "step": 3935
    },
    {
        "loss": 1.7688,
        "grad_norm": 5.192152500152588,
        "learning_rate": 7.547104900136648e-06,
        "epoch": 0.2933154482450257,
        "step": 3936
    },
    {
        "loss": 2.871,
        "grad_norm": 2.0796244144439697,
        "learning_rate": 7.5203106017274425e-06,
        "epoch": 0.29338996944630746,
        "step": 3937
    },
    {
        "loss": 2.7282,
        "grad_norm": 3.428914785385132,
        "learning_rate": 7.493562093040574e-06,
        "epoch": 0.2934644906475892,
        "step": 3938
    },
    {
        "loss": 2.3507,
        "grad_norm": 4.009809970855713,
        "learning_rate": 7.466859387320124e-06,
        "epoch": 0.293539011848871,
        "step": 3939
    },
    {
        "loss": 2.8163,
        "grad_norm": 2.109416961669922,
        "learning_rate": 7.440202497787485e-06,
        "epoch": 0.29361353305015275,
        "step": 3940
    },
    {
        "loss": 2.6797,
        "grad_norm": 3.9911930561065674,
        "learning_rate": 7.41359143764131e-06,
        "epoch": 0.2936880542514345,
        "step": 3941
    },
    {
        "loss": 1.2224,
        "grad_norm": 2.5265536308288574,
        "learning_rate": 7.38702622005768e-06,
        "epoch": 0.2937625754527163,
        "step": 3942
    },
    {
        "loss": 2.0894,
        "grad_norm": 3.3460135459899902,
        "learning_rate": 7.3605068581898414e-06,
        "epoch": 0.29383709665399804,
        "step": 3943
    },
    {
        "loss": 1.8492,
        "grad_norm": 3.3336281776428223,
        "learning_rate": 7.334033365168413e-06,
        "epoch": 0.2939116178552798,
        "step": 3944
    },
    {
        "loss": 2.285,
        "grad_norm": 2.2360687255859375,
        "learning_rate": 7.307605754101321e-06,
        "epoch": 0.29398613905656157,
        "step": 3945
    },
    {
        "loss": 1.8005,
        "grad_norm": 3.626500129699707,
        "learning_rate": 7.281224038073675e-06,
        "epoch": 0.29406066025784333,
        "step": 3946
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.076218843460083,
        "learning_rate": 7.254888230148005e-06,
        "epoch": 0.2941351814591251,
        "step": 3947
    },
    {
        "loss": 2.6182,
        "grad_norm": 2.5723390579223633,
        "learning_rate": 7.2285983433639795e-06,
        "epoch": 0.2942097026604069,
        "step": 3948
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.1234583854675293,
        "learning_rate": 7.202354390738608e-06,
        "epoch": 0.2942842238616887,
        "step": 3949
    },
    {
        "loss": 1.8252,
        "grad_norm": 2.673039674758911,
        "learning_rate": 7.176156385266098e-06,
        "epoch": 0.29435874506297044,
        "step": 3950
    },
    {
        "loss": 2.2747,
        "grad_norm": 3.0620269775390625,
        "learning_rate": 7.15000433991797e-06,
        "epoch": 0.2944332662642522,
        "step": 3951
    },
    {
        "loss": 2.3209,
        "grad_norm": 2.077155113220215,
        "learning_rate": 7.123898267642947e-06,
        "epoch": 0.29450778746553397,
        "step": 3952
    },
    {
        "loss": 1.9377,
        "grad_norm": 3.265540838241577,
        "learning_rate": 7.097838181367e-06,
        "epoch": 0.29458230866681573,
        "step": 3953
    },
    {
        "loss": 2.1298,
        "grad_norm": 2.6849939823150635,
        "learning_rate": 7.071824093993373e-06,
        "epoch": 0.2946568298680975,
        "step": 3954
    },
    {
        "loss": 1.9851,
        "grad_norm": 3.8781850337982178,
        "learning_rate": 7.045856018402397e-06,
        "epoch": 0.29473135106937925,
        "step": 3955
    },
    {
        "loss": 1.624,
        "grad_norm": 3.5286009311676025,
        "learning_rate": 7.019933967451786e-06,
        "epoch": 0.294805872270661,
        "step": 3956
    },
    {
        "loss": 2.4338,
        "grad_norm": 2.7815306186676025,
        "learning_rate": 6.994057953976385e-06,
        "epoch": 0.2948803934719428,
        "step": 3957
    },
    {
        "loss": 2.233,
        "grad_norm": 2.07800030708313,
        "learning_rate": 6.968227990788267e-06,
        "epoch": 0.29495491467322454,
        "step": 3958
    },
    {
        "loss": 2.6925,
        "grad_norm": 2.1029772758483887,
        "learning_rate": 6.9424440906766805e-06,
        "epoch": 0.2950294358745063,
        "step": 3959
    },
    {
        "loss": 1.7171,
        "grad_norm": 5.935577392578125,
        "learning_rate": 6.916706266408046e-06,
        "epoch": 0.29510395707578807,
        "step": 3960
    },
    {
        "loss": 1.6578,
        "grad_norm": 2.5055503845214844,
        "learning_rate": 6.891014530726059e-06,
        "epoch": 0.29517847827706983,
        "step": 3961
    },
    {
        "loss": 1.9213,
        "grad_norm": 3.044778347015381,
        "learning_rate": 6.8653688963514985e-06,
        "epoch": 0.2952529994783516,
        "step": 3962
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.716156482696533,
        "learning_rate": 6.839769375982408e-06,
        "epoch": 0.29532752067963336,
        "step": 3963
    },
    {
        "loss": 2.2454,
        "grad_norm": 2.1297647953033447,
        "learning_rate": 6.814215982293892e-06,
        "epoch": 0.2954020418809151,
        "step": 3964
    },
    {
        "loss": 1.9437,
        "grad_norm": 5.276693820953369,
        "learning_rate": 6.788708727938264e-06,
        "epoch": 0.2954765630821969,
        "step": 3965
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.0858571529388428,
        "learning_rate": 6.763247625545055e-06,
        "epoch": 0.29555108428347865,
        "step": 3966
    },
    {
        "loss": 2.1796,
        "grad_norm": 3.5324928760528564,
        "learning_rate": 6.737832687720869e-06,
        "epoch": 0.2956256054847604,
        "step": 3967
    },
    {
        "loss": 2.4107,
        "grad_norm": 2.6999826431274414,
        "learning_rate": 6.7124639270494945e-06,
        "epoch": 0.2957001266860422,
        "step": 3968
    },
    {
        "loss": 2.7593,
        "grad_norm": 2.6836979389190674,
        "learning_rate": 6.687141356091786e-06,
        "epoch": 0.29577464788732394,
        "step": 3969
    },
    {
        "loss": 2.3362,
        "grad_norm": 2.7630691528320312,
        "learning_rate": 6.661864987385802e-06,
        "epoch": 0.2958491690886057,
        "step": 3970
    },
    {
        "loss": 2.7598,
        "grad_norm": 3.722127676010132,
        "learning_rate": 6.636634833446676e-06,
        "epoch": 0.29592369028988746,
        "step": 3971
    },
    {
        "loss": 2.6027,
        "grad_norm": 2.6052019596099854,
        "learning_rate": 6.6114509067667585e-06,
        "epoch": 0.2959982114911692,
        "step": 3972
    },
    {
        "loss": 2.5946,
        "grad_norm": 2.132923126220703,
        "learning_rate": 6.5863132198153765e-06,
        "epoch": 0.296072732692451,
        "step": 3973
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.9495022296905518,
        "learning_rate": 6.561221785039018e-06,
        "epoch": 0.29614725389373275,
        "step": 3974
    },
    {
        "loss": 2.0013,
        "grad_norm": 3.0252840518951416,
        "learning_rate": 6.536176614861289e-06,
        "epoch": 0.2962217750950145,
        "step": 3975
    },
    {
        "loss": 2.7363,
        "grad_norm": 2.4756128787994385,
        "learning_rate": 6.51117772168286e-06,
        "epoch": 0.2962962962962963,
        "step": 3976
    },
    {
        "loss": 2.1196,
        "grad_norm": 2.3333961963653564,
        "learning_rate": 6.48622511788154e-06,
        "epoch": 0.29637081749757804,
        "step": 3977
    },
    {
        "loss": 2.5731,
        "grad_norm": 3.1359851360321045,
        "learning_rate": 6.461318815812156e-06,
        "epoch": 0.2964453386988598,
        "step": 3978
    },
    {
        "loss": 2.6006,
        "grad_norm": 2.6955559253692627,
        "learning_rate": 6.436458827806624e-06,
        "epoch": 0.29651985990014157,
        "step": 3979
    },
    {
        "loss": 1.3798,
        "grad_norm": 2.4379591941833496,
        "learning_rate": 6.411645166173941e-06,
        "epoch": 0.29659438110142333,
        "step": 3980
    },
    {
        "loss": 2.3004,
        "grad_norm": 2.6472766399383545,
        "learning_rate": 6.386877843200167e-06,
        "epoch": 0.2966689023027051,
        "step": 3981
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.854412794113159,
        "learning_rate": 6.362156871148428e-06,
        "epoch": 0.29674342350398686,
        "step": 3982
    },
    {
        "loss": 1.6802,
        "grad_norm": 2.9300923347473145,
        "learning_rate": 6.337482262258864e-06,
        "epoch": 0.2968179447052686,
        "step": 3983
    },
    {
        "loss": 1.9852,
        "grad_norm": 4.048649311065674,
        "learning_rate": 6.312854028748705e-06,
        "epoch": 0.29689246590655044,
        "step": 3984
    },
    {
        "loss": 1.5456,
        "grad_norm": 2.3293263912200928,
        "learning_rate": 6.28827218281216e-06,
        "epoch": 0.2969669871078322,
        "step": 3985
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.9207096099853516,
        "learning_rate": 6.26373673662054e-06,
        "epoch": 0.29704150830911397,
        "step": 3986
    },
    {
        "loss": 1.5418,
        "grad_norm": 3.396660089492798,
        "learning_rate": 6.239247702322149e-06,
        "epoch": 0.29711602951039573,
        "step": 3987
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.4891910552978516,
        "learning_rate": 6.214805092042286e-06,
        "epoch": 0.2971905507116775,
        "step": 3988
    },
    {
        "loss": 2.0762,
        "grad_norm": 1.8347591161727905,
        "learning_rate": 6.1904089178833166e-06,
        "epoch": 0.29726507191295926,
        "step": 3989
    },
    {
        "loss": 2.2912,
        "grad_norm": 2.744464635848999,
        "learning_rate": 6.166059191924534e-06,
        "epoch": 0.297339593114241,
        "step": 3990
    },
    {
        "loss": 2.609,
        "grad_norm": 3.1583919525146484,
        "learning_rate": 6.141755926222337e-06,
        "epoch": 0.2974141143155228,
        "step": 3991
    },
    {
        "loss": 2.6077,
        "grad_norm": 2.836270332336426,
        "learning_rate": 6.117499132810067e-06,
        "epoch": 0.29748863551680454,
        "step": 3992
    },
    {
        "loss": 2.8287,
        "grad_norm": 2.338895320892334,
        "learning_rate": 6.093288823698029e-06,
        "epoch": 0.2975631567180863,
        "step": 3993
    },
    {
        "loss": 2.7066,
        "grad_norm": 2.2552649974823,
        "learning_rate": 6.069125010873555e-06,
        "epoch": 0.29763767791936807,
        "step": 3994
    },
    {
        "loss": 2.5011,
        "grad_norm": 3.0552330017089844,
        "learning_rate": 6.045007706300909e-06,
        "epoch": 0.29771219912064983,
        "step": 3995
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.6769392490386963,
        "learning_rate": 6.020936921921439e-06,
        "epoch": 0.2977867203219316,
        "step": 3996
    },
    {
        "loss": 1.7688,
        "grad_norm": 3.441903591156006,
        "learning_rate": 5.996912669653265e-06,
        "epoch": 0.29786124152321336,
        "step": 3997
    },
    {
        "loss": 1.845,
        "grad_norm": 3.3310015201568604,
        "learning_rate": 5.972934961391685e-06,
        "epoch": 0.2979357627244951,
        "step": 3998
    },
    {
        "loss": 2.1323,
        "grad_norm": 2.9170427322387695,
        "learning_rate": 5.949003809008779e-06,
        "epoch": 0.2980102839257769,
        "step": 3999
    },
    {
        "loss": 2.6573,
        "grad_norm": 2.5666120052337646,
        "learning_rate": 5.92511922435367e-06,
        "epoch": 0.29808480512705865,
        "step": 4000
    },
    {
        "loss": 2.2098,
        "grad_norm": 3.0096662044525146,
        "learning_rate": 5.901281219252408e-06,
        "epoch": 0.2981593263283404,
        "step": 4001
    },
    {
        "loss": 2.6202,
        "grad_norm": 3.4632654190063477,
        "learning_rate": 5.8774898055079295e-06,
        "epoch": 0.2982338475296222,
        "step": 4002
    },
    {
        "loss": 2.4291,
        "grad_norm": 2.5723066329956055,
        "learning_rate": 5.853744994900201e-06,
        "epoch": 0.29830836873090394,
        "step": 4003
    },
    {
        "loss": 1.9748,
        "grad_norm": 2.9106342792510986,
        "learning_rate": 5.830046799186018e-06,
        "epoch": 0.2983828899321857,
        "step": 4004
    },
    {
        "loss": 2.3999,
        "grad_norm": 2.920222282409668,
        "learning_rate": 5.8063952300991374e-06,
        "epoch": 0.29845741113346747,
        "step": 4005
    },
    {
        "loss": 1.9201,
        "grad_norm": 2.215284824371338,
        "learning_rate": 5.782790299350238e-06,
        "epoch": 0.29853193233474923,
        "step": 4006
    },
    {
        "loss": 2.8356,
        "grad_norm": 1.5909993648529053,
        "learning_rate": 5.75923201862687e-06,
        "epoch": 0.298606453536031,
        "step": 4007
    },
    {
        "loss": 2.6566,
        "grad_norm": 2.3174166679382324,
        "learning_rate": 5.735720399593536e-06,
        "epoch": 0.29868097473731275,
        "step": 4008
    },
    {
        "loss": 1.8997,
        "grad_norm": 3.1676905155181885,
        "learning_rate": 5.71225545389158e-06,
        "epoch": 0.2987554959385945,
        "step": 4009
    },
    {
        "loss": 2.0485,
        "grad_norm": 2.8943397998809814,
        "learning_rate": 5.688837193139307e-06,
        "epoch": 0.2988300171398763,
        "step": 4010
    },
    {
        "loss": 2.263,
        "grad_norm": 4.5749640464782715,
        "learning_rate": 5.6654656289318206e-06,
        "epoch": 0.29890453834115804,
        "step": 4011
    },
    {
        "loss": 1.8506,
        "grad_norm": 3.31990385055542,
        "learning_rate": 5.6421407728411755e-06,
        "epoch": 0.2989790595424398,
        "step": 4012
    },
    {
        "loss": 1.9403,
        "grad_norm": 2.679949998855591,
        "learning_rate": 5.618862636416256e-06,
        "epoch": 0.29905358074372157,
        "step": 4013
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.7277474403381348,
        "learning_rate": 5.595631231182829e-06,
        "epoch": 0.29912810194500333,
        "step": 4014
    },
    {
        "loss": 2.8303,
        "grad_norm": 2.5860495567321777,
        "learning_rate": 5.5724465686435635e-06,
        "epoch": 0.2992026231462851,
        "step": 4015
    },
    {
        "loss": 1.8382,
        "grad_norm": 3.0776097774505615,
        "learning_rate": 5.549308660277874e-06,
        "epoch": 0.29927714434756686,
        "step": 4016
    },
    {
        "loss": 2.575,
        "grad_norm": 3.4920599460601807,
        "learning_rate": 5.526217517542154e-06,
        "epoch": 0.2993516655488486,
        "step": 4017
    },
    {
        "loss": 2.3901,
        "grad_norm": 1.8502777814865112,
        "learning_rate": 5.503173151869556e-06,
        "epoch": 0.2994261867501304,
        "step": 4018
    },
    {
        "loss": 2.3027,
        "grad_norm": 2.2914631366729736,
        "learning_rate": 5.480175574670132e-06,
        "epoch": 0.2995007079514122,
        "step": 4019
    },
    {
        "loss": 2.8134,
        "grad_norm": 2.3120594024658203,
        "learning_rate": 5.457224797330706e-06,
        "epoch": 0.29957522915269397,
        "step": 4020
    },
    {
        "loss": 2.5479,
        "grad_norm": 2.013576030731201,
        "learning_rate": 5.434320831214945e-06,
        "epoch": 0.29964975035397573,
        "step": 4021
    },
    {
        "loss": 2.498,
        "grad_norm": 2.579174041748047,
        "learning_rate": 5.411463687663377e-06,
        "epoch": 0.2997242715552575,
        "step": 4022
    },
    {
        "loss": 1.6778,
        "grad_norm": 2.619126796722412,
        "learning_rate": 5.388653377993324e-06,
        "epoch": 0.29979879275653926,
        "step": 4023
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.078310012817383,
        "learning_rate": 5.365889913498934e-06,
        "epoch": 0.299873313957821,
        "step": 4024
    },
    {
        "loss": 2.799,
        "grad_norm": 2.8646671772003174,
        "learning_rate": 5.343173305451121e-06,
        "epoch": 0.2999478351591028,
        "step": 4025
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.6526787281036377,
        "learning_rate": 5.320503565097601e-06,
        "epoch": 0.30002235636038455,
        "step": 4026
    },
    {
        "loss": 2.4106,
        "grad_norm": 3.085888624191284,
        "learning_rate": 5.2978807036629165e-06,
        "epoch": 0.3000968775616663,
        "step": 4027
    },
    {
        "loss": 1.404,
        "grad_norm": 3.5625369548797607,
        "learning_rate": 5.2753047323484185e-06,
        "epoch": 0.30017139876294807,
        "step": 4028
    },
    {
        "loss": 0.91,
        "grad_norm": 7.154783725738525,
        "learning_rate": 5.252775662332199e-06,
        "epoch": 0.30024591996422983,
        "step": 4029
    },
    {
        "loss": 1.6947,
        "grad_norm": 4.065482139587402,
        "learning_rate": 5.2302935047691015e-06,
        "epoch": 0.3003204411655116,
        "step": 4030
    },
    {
        "loss": 2.4134,
        "grad_norm": 1.8363327980041504,
        "learning_rate": 5.2078582707908105e-06,
        "epoch": 0.30039496236679336,
        "step": 4031
    },
    {
        "loss": 2.6216,
        "grad_norm": 2.5815484523773193,
        "learning_rate": 5.18546997150573e-06,
        "epoch": 0.3004694835680751,
        "step": 4032
    },
    {
        "loss": 2.8224,
        "grad_norm": 2.52286434173584,
        "learning_rate": 5.163128617999069e-06,
        "epoch": 0.3005440047693569,
        "step": 4033
    },
    {
        "loss": 1.983,
        "grad_norm": 2.4515559673309326,
        "learning_rate": 5.140834221332746e-06,
        "epoch": 0.30061852597063865,
        "step": 4034
    },
    {
        "loss": 2.5605,
        "grad_norm": 2.1338062286376953,
        "learning_rate": 5.118586792545421e-06,
        "epoch": 0.3006930471719204,
        "step": 4035
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.1652960777282715,
        "learning_rate": 5.096386342652559e-06,
        "epoch": 0.3007675683732022,
        "step": 4036
    },
    {
        "loss": 2.9086,
        "grad_norm": 3.2697649002075195,
        "learning_rate": 5.0742328826463015e-06,
        "epoch": 0.30084208957448394,
        "step": 4037
    },
    {
        "loss": 2.6857,
        "grad_norm": 1.6421799659729004,
        "learning_rate": 5.052126423495585e-06,
        "epoch": 0.3009166107757657,
        "step": 4038
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.805757999420166,
        "learning_rate": 5.030066976146031e-06,
        "epoch": 0.30099113197704747,
        "step": 4039
    },
    {
        "loss": 2.7092,
        "grad_norm": 2.9023044109344482,
        "learning_rate": 5.00805455151998e-06,
        "epoch": 0.30106565317832923,
        "step": 4040
    },
    {
        "loss": 2.6397,
        "grad_norm": 3.0374553203582764,
        "learning_rate": 4.986089160516505e-06,
        "epoch": 0.301140174379611,
        "step": 4041
    },
    {
        "loss": 2.2463,
        "grad_norm": 3.3284192085266113,
        "learning_rate": 4.964170814011404e-06,
        "epoch": 0.30121469558089276,
        "step": 4042
    },
    {
        "loss": 2.0182,
        "grad_norm": 3.3364548683166504,
        "learning_rate": 4.942299522857163e-06,
        "epoch": 0.3012892167821745,
        "step": 4043
    },
    {
        "loss": 3.0247,
        "grad_norm": 2.2609572410583496,
        "learning_rate": 4.920475297882965e-06,
        "epoch": 0.3013637379834563,
        "step": 4044
    },
    {
        "loss": 2.4836,
        "grad_norm": 6.269146919250488,
        "learning_rate": 4.89869814989472e-06,
        "epoch": 0.30143825918473804,
        "step": 4045
    },
    {
        "loss": 3.1266,
        "grad_norm": 2.8999428749084473,
        "learning_rate": 4.8769680896749606e-06,
        "epoch": 0.3015127803860198,
        "step": 4046
    },
    {
        "loss": 2.5974,
        "grad_norm": 2.180528163909912,
        "learning_rate": 4.855285127983e-06,
        "epoch": 0.30158730158730157,
        "step": 4047
    },
    {
        "loss": 2.5747,
        "grad_norm": 3.5350921154022217,
        "learning_rate": 4.833649275554786e-06,
        "epoch": 0.30166182278858333,
        "step": 4048
    },
    {
        "loss": 2.2094,
        "grad_norm": 3.1321823596954346,
        "learning_rate": 4.812060543102903e-06,
        "epoch": 0.3017363439898651,
        "step": 4049
    },
    {
        "loss": 1.8483,
        "grad_norm": 1.7796214818954468,
        "learning_rate": 4.7905189413166575e-06,
        "epoch": 0.30181086519114686,
        "step": 4050
    },
    {
        "loss": 2.5904,
        "grad_norm": 1.793522596359253,
        "learning_rate": 4.769024480861972e-06,
        "epoch": 0.3018853863924286,
        "step": 4051
    },
    {
        "loss": 2.4966,
        "grad_norm": 2.684781074523926,
        "learning_rate": 4.747577172381512e-06,
        "epoch": 0.3019599075937104,
        "step": 4052
    },
    {
        "loss": 1.9223,
        "grad_norm": 5.056115627288818,
        "learning_rate": 4.726177026494516e-06,
        "epoch": 0.30203442879499215,
        "step": 4053
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.769536018371582,
        "learning_rate": 4.704824053796897e-06,
        "epoch": 0.3021089499962739,
        "step": 4054
    },
    {
        "loss": 1.9098,
        "grad_norm": 2.2572011947631836,
        "learning_rate": 4.683518264861186e-06,
        "epoch": 0.30218347119755573,
        "step": 4055
    },
    {
        "loss": 2.5119,
        "grad_norm": 2.225224733352661,
        "learning_rate": 4.6622596702366126e-06,
        "epoch": 0.3022579923988375,
        "step": 4056
    },
    {
        "loss": 1.9644,
        "grad_norm": 3.021939516067505,
        "learning_rate": 4.641048280449001e-06,
        "epoch": 0.30233251360011926,
        "step": 4057
    },
    {
        "loss": 2.1475,
        "grad_norm": 3.85404109954834,
        "learning_rate": 4.619884106000771e-06,
        "epoch": 0.302407034801401,
        "step": 4058
    },
    {
        "loss": 2.6381,
        "grad_norm": 2.1495473384857178,
        "learning_rate": 4.598767157371042e-06,
        "epoch": 0.3024815560026828,
        "step": 4059
    },
    {
        "loss": 2.5116,
        "grad_norm": 2.2404556274414062,
        "learning_rate": 4.577697445015472e-06,
        "epoch": 0.30255607720396455,
        "step": 4060
    },
    {
        "loss": 2.2378,
        "grad_norm": 3.9940803050994873,
        "learning_rate": 4.556674979366382e-06,
        "epoch": 0.3026305984052463,
        "step": 4061
    },
    {
        "loss": 2.2578,
        "grad_norm": 4.201022148132324,
        "learning_rate": 4.535699770832691e-06,
        "epoch": 0.3027051196065281,
        "step": 4062
    },
    {
        "loss": 1.9871,
        "grad_norm": 2.9098525047302246,
        "learning_rate": 4.51477182979988e-06,
        "epoch": 0.30277964080780984,
        "step": 4063
    },
    {
        "loss": 2.9556,
        "grad_norm": 3.473266363143921,
        "learning_rate": 4.493891166630104e-06,
        "epoch": 0.3028541620090916,
        "step": 4064
    },
    {
        "loss": 2.4185,
        "grad_norm": 2.330556869506836,
        "learning_rate": 4.473057791662017e-06,
        "epoch": 0.30292868321037336,
        "step": 4065
    },
    {
        "loss": 2.402,
        "grad_norm": 3.543779134750366,
        "learning_rate": 4.452271715210943e-06,
        "epoch": 0.3030032044116551,
        "step": 4066
    },
    {
        "loss": 2.61,
        "grad_norm": 3.1920166015625,
        "learning_rate": 4.4315329475687284e-06,
        "epoch": 0.3030777256129369,
        "step": 4067
    },
    {
        "loss": 2.0168,
        "grad_norm": 3.5577175617218018,
        "learning_rate": 4.410841499003815e-06,
        "epoch": 0.30315224681421865,
        "step": 4068
    },
    {
        "loss": 2.6107,
        "grad_norm": 2.3324978351593018,
        "learning_rate": 4.390197379761218e-06,
        "epoch": 0.3032267680155004,
        "step": 4069
    },
    {
        "loss": 2.6009,
        "grad_norm": 2.6088039875030518,
        "learning_rate": 4.369600600062529e-06,
        "epoch": 0.3033012892167822,
        "step": 4070
    },
    {
        "loss": 2.555,
        "grad_norm": 2.4913511276245117,
        "learning_rate": 4.3490511701058775e-06,
        "epoch": 0.30337581041806394,
        "step": 4071
    },
    {
        "loss": 2.6808,
        "grad_norm": 3.3357179164886475,
        "learning_rate": 4.32854910006597e-06,
        "epoch": 0.3034503316193457,
        "step": 4072
    },
    {
        "loss": 3.0569,
        "grad_norm": 1.2466661930084229,
        "learning_rate": 4.308094400094031e-06,
        "epoch": 0.30352485282062747,
        "step": 4073
    },
    {
        "loss": 1.935,
        "grad_norm": 3.0517237186431885,
        "learning_rate": 4.2876870803178595e-06,
        "epoch": 0.30359937402190923,
        "step": 4074
    },
    {
        "loss": 2.4247,
        "grad_norm": 2.8140478134155273,
        "learning_rate": 4.2673271508418155e-06,
        "epoch": 0.303673895223191,
        "step": 4075
    },
    {
        "loss": 2.5013,
        "grad_norm": 3.000579833984375,
        "learning_rate": 4.247014621746736e-06,
        "epoch": 0.30374841642447276,
        "step": 4076
    },
    {
        "loss": 1.9657,
        "grad_norm": 3.628535509109497,
        "learning_rate": 4.226749503090011e-06,
        "epoch": 0.3038229376257545,
        "step": 4077
    },
    {
        "loss": 2.5542,
        "grad_norm": 3.0262346267700195,
        "learning_rate": 4.206531804905589e-06,
        "epoch": 0.3038974588270363,
        "step": 4078
    },
    {
        "loss": 2.5475,
        "grad_norm": 2.139758348464966,
        "learning_rate": 4.186361537203909e-06,
        "epoch": 0.30397198002831805,
        "step": 4079
    },
    {
        "loss": 2.5454,
        "grad_norm": 2.4128105640411377,
        "learning_rate": 4.166238709971937e-06,
        "epoch": 0.3040465012295998,
        "step": 4080
    },
    {
        "loss": 2.1432,
        "grad_norm": 1.793664813041687,
        "learning_rate": 4.146163333173136e-06,
        "epoch": 0.30412102243088157,
        "step": 4081
    },
    {
        "loss": 3.1297,
        "grad_norm": 3.1427745819091797,
        "learning_rate": 4.1261354167474565e-06,
        "epoch": 0.30419554363216333,
        "step": 4082
    },
    {
        "loss": 2.3197,
        "grad_norm": 3.9508590698242188,
        "learning_rate": 4.106154970611409e-06,
        "epoch": 0.3042700648334451,
        "step": 4083
    },
    {
        "loss": 2.3606,
        "grad_norm": 2.94504976272583,
        "learning_rate": 4.086222004657969e-06,
        "epoch": 0.30434458603472686,
        "step": 4084
    },
    {
        "loss": 1.5891,
        "grad_norm": 4.006267070770264,
        "learning_rate": 4.066336528756587e-06,
        "epoch": 0.3044191072360086,
        "step": 4085
    },
    {
        "loss": 1.839,
        "grad_norm": 3.651548385620117,
        "learning_rate": 4.046498552753231e-06,
        "epoch": 0.3044936284372904,
        "step": 4086
    },
    {
        "loss": 2.4402,
        "grad_norm": 2.0082530975341797,
        "learning_rate": 4.026708086470299e-06,
        "epoch": 0.30456814963857215,
        "step": 4087
    },
    {
        "loss": 2.6748,
        "grad_norm": 2.1916446685791016,
        "learning_rate": 4.006965139706709e-06,
        "epoch": 0.3046426708398539,
        "step": 4088
    },
    {
        "loss": 2.5642,
        "grad_norm": 2.5109927654266357,
        "learning_rate": 3.987269722237863e-06,
        "epoch": 0.3047171920411357,
        "step": 4089
    },
    {
        "loss": 2.2992,
        "grad_norm": 3.2299554347991943,
        "learning_rate": 3.967621843815605e-06,
        "epoch": 0.3047917132424175,
        "step": 4090
    },
    {
        "loss": 1.2799,
        "grad_norm": 3.5137710571289062,
        "learning_rate": 3.94802151416821e-06,
        "epoch": 0.30486623444369926,
        "step": 4091
    },
    {
        "loss": 2.329,
        "grad_norm": 3.208883285522461,
        "learning_rate": 3.928468743000469e-06,
        "epoch": 0.304940755644981,
        "step": 4092
    },
    {
        "loss": 2.0679,
        "grad_norm": 3.5612823963165283,
        "learning_rate": 3.908963539993582e-06,
        "epoch": 0.3050152768462628,
        "step": 4093
    },
    {
        "loss": 2.0575,
        "grad_norm": 2.814809560775757,
        "learning_rate": 3.889505914805247e-06,
        "epoch": 0.30508979804754455,
        "step": 4094
    },
    {
        "loss": 1.6356,
        "grad_norm": 3.028632879257202,
        "learning_rate": 3.870095877069557e-06,
        "epoch": 0.3051643192488263,
        "step": 4095
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.8721914291381836,
        "learning_rate": 3.850733436397025e-06,
        "epoch": 0.3052388404501081,
        "step": 4096
    },
    {
        "loss": 2.6535,
        "grad_norm": 1.651157021522522,
        "learning_rate": 3.83141860237467e-06,
        "epoch": 0.30531336165138984,
        "step": 4097
    },
    {
        "loss": 2.4687,
        "grad_norm": 2.212686538696289,
        "learning_rate": 3.8121513845658764e-06,
        "epoch": 0.3053878828526716,
        "step": 4098
    },
    {
        "loss": 2.1782,
        "grad_norm": 2.793651819229126,
        "learning_rate": 3.792931792510479e-06,
        "epoch": 0.30546240405395336,
        "step": 4099
    },
    {
        "loss": 2.2396,
        "grad_norm": 3.172226667404175,
        "learning_rate": 3.7737598357247437e-06,
        "epoch": 0.3055369252552351,
        "step": 4100
    },
    {
        "loss": 2.2462,
        "grad_norm": 3.2768218517303467,
        "learning_rate": 3.754635523701322e-06,
        "epoch": 0.3056114464565169,
        "step": 4101
    },
    {
        "loss": 2.7975,
        "grad_norm": 3.0043699741363525,
        "learning_rate": 3.735558865909261e-06,
        "epoch": 0.30568596765779865,
        "step": 4102
    },
    {
        "loss": 2.4569,
        "grad_norm": 2.5084567070007324,
        "learning_rate": 3.716529871794072e-06,
        "epoch": 0.3057604888590804,
        "step": 4103
    },
    {
        "loss": 1.9958,
        "grad_norm": 4.568702220916748,
        "learning_rate": 3.697548550777641e-06,
        "epoch": 0.3058350100603622,
        "step": 4104
    },
    {
        "loss": 1.6106,
        "grad_norm": 3.0155646800994873,
        "learning_rate": 3.678614912258216e-06,
        "epoch": 0.30590953126164394,
        "step": 4105
    },
    {
        "loss": 1.7365,
        "grad_norm": 3.9334347248077393,
        "learning_rate": 3.659728965610476e-06,
        "epoch": 0.3059840524629257,
        "step": 4106
    },
    {
        "loss": 1.9254,
        "grad_norm": 3.307666778564453,
        "learning_rate": 3.6408907201854413e-06,
        "epoch": 0.30605857366420747,
        "step": 4107
    },
    {
        "loss": 2.7207,
        "grad_norm": 2.1478123664855957,
        "learning_rate": 3.622100185310573e-06,
        "epoch": 0.30613309486548923,
        "step": 4108
    },
    {
        "loss": 2.3573,
        "grad_norm": 2.2124922275543213,
        "learning_rate": 3.603357370289695e-06,
        "epoch": 0.306207616066771,
        "step": 4109
    },
    {
        "loss": 2.9639,
        "grad_norm": 2.650632381439209,
        "learning_rate": 3.584662284402962e-06,
        "epoch": 0.30628213726805276,
        "step": 4110
    },
    {
        "loss": 2.7452,
        "grad_norm": 2.2804996967315674,
        "learning_rate": 3.566014936906936e-06,
        "epoch": 0.3063566584693345,
        "step": 4111
    },
    {
        "loss": 2.3865,
        "grad_norm": 2.7274858951568604,
        "learning_rate": 3.5474153370344986e-06,
        "epoch": 0.3064311796706163,
        "step": 4112
    },
    {
        "loss": 2.6161,
        "grad_norm": 3.13405704498291,
        "learning_rate": 3.5288634939949607e-06,
        "epoch": 0.30650570087189805,
        "step": 4113
    },
    {
        "loss": 3.2214,
        "grad_norm": 2.463373899459839,
        "learning_rate": 3.5103594169739516e-06,
        "epoch": 0.3065802220731798,
        "step": 4114
    },
    {
        "loss": 2.4259,
        "grad_norm": 3.302731990814209,
        "learning_rate": 3.49190311513341e-06,
        "epoch": 0.30665474327446157,
        "step": 4115
    },
    {
        "loss": 1.524,
        "grad_norm": 2.7213969230651855,
        "learning_rate": 3.4734945976116705e-06,
        "epoch": 0.30672926447574334,
        "step": 4116
    },
    {
        "loss": 1.7977,
        "grad_norm": 2.3760485649108887,
        "learning_rate": 3.4551338735233975e-06,
        "epoch": 0.3068037856770251,
        "step": 4117
    },
    {
        "loss": 2.6464,
        "grad_norm": 3.076245069503784,
        "learning_rate": 3.4368209519595627e-06,
        "epoch": 0.30687830687830686,
        "step": 4118
    },
    {
        "loss": 1.5762,
        "grad_norm": 3.558867931365967,
        "learning_rate": 3.4185558419875362e-06,
        "epoch": 0.3069528280795886,
        "step": 4119
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.762384057044983,
        "learning_rate": 3.4003385526509613e-06,
        "epoch": 0.3070273492808704,
        "step": 4120
    },
    {
        "loss": 2.791,
        "grad_norm": 1.952003836631775,
        "learning_rate": 3.382169092969778e-06,
        "epoch": 0.30710187048215215,
        "step": 4121
    },
    {
        "loss": 2.8746,
        "grad_norm": 2.699953079223633,
        "learning_rate": 3.3640474719403127e-06,
        "epoch": 0.3071763916834339,
        "step": 4122
    },
    {
        "loss": 2.066,
        "grad_norm": 2.1815764904022217,
        "learning_rate": 3.3459736985351765e-06,
        "epoch": 0.3072509128847157,
        "step": 4123
    },
    {
        "loss": 1.7117,
        "grad_norm": 3.276580572128296,
        "learning_rate": 3.327947781703267e-06,
        "epoch": 0.30732543408599744,
        "step": 4124
    },
    {
        "loss": 2.197,
        "grad_norm": 2.884425640106201,
        "learning_rate": 3.3099697303698105e-06,
        "epoch": 0.30739995528727926,
        "step": 4125
    },
    {
        "loss": 1.6264,
        "grad_norm": 3.231235980987549,
        "learning_rate": 3.2920395534363323e-06,
        "epoch": 0.307474476488561,
        "step": 4126
    },
    {
        "loss": 1.6715,
        "grad_norm": 2.609470844268799,
        "learning_rate": 3.2741572597806527e-06,
        "epoch": 0.3075489976898428,
        "step": 4127
    },
    {
        "loss": 2.4627,
        "grad_norm": 2.3915348052978516,
        "learning_rate": 3.2563228582568883e-06,
        "epoch": 0.30762351889112455,
        "step": 4128
    },
    {
        "loss": 2.0981,
        "grad_norm": 2.248692750930786,
        "learning_rate": 3.238536357695421e-06,
        "epoch": 0.3076980400924063,
        "step": 4129
    },
    {
        "loss": 2.2912,
        "grad_norm": 2.2231452465057373,
        "learning_rate": 3.2207977669029277e-06,
        "epoch": 0.3077725612936881,
        "step": 4130
    },
    {
        "loss": 1.9502,
        "grad_norm": 2.349296808242798,
        "learning_rate": 3.203107094662383e-06,
        "epoch": 0.30784708249496984,
        "step": 4131
    },
    {
        "loss": 1.7488,
        "grad_norm": 1.939327597618103,
        "learning_rate": 3.185464349733003e-06,
        "epoch": 0.3079216036962516,
        "step": 4132
    },
    {
        "loss": 3.0244,
        "grad_norm": 3.187241315841675,
        "learning_rate": 3.167869540850299e-06,
        "epoch": 0.30799612489753336,
        "step": 4133
    },
    {
        "loss": 1.9947,
        "grad_norm": 2.921933650970459,
        "learning_rate": 3.1503226767260252e-06,
        "epoch": 0.3080706460988151,
        "step": 4134
    },
    {
        "loss": 2.8223,
        "grad_norm": 2.099067449569702,
        "learning_rate": 3.1328237660482096e-06,
        "epoch": 0.3081451673000969,
        "step": 4135
    },
    {
        "loss": 2.5002,
        "grad_norm": 3.6646766662597656,
        "learning_rate": 3.1153728174811548e-06,
        "epoch": 0.30821968850137865,
        "step": 4136
    },
    {
        "loss": 2.1749,
        "grad_norm": 2.765885591506958,
        "learning_rate": 3.0979698396653823e-06,
        "epoch": 0.3082942097026604,
        "step": 4137
    },
    {
        "loss": 2.819,
        "grad_norm": 2.5024540424346924,
        "learning_rate": 3.080614841217655e-06,
        "epoch": 0.3083687309039422,
        "step": 4138
    },
    {
        "loss": 1.7768,
        "grad_norm": 4.333698749542236,
        "learning_rate": 3.0633078307310324e-06,
        "epoch": 0.30844325210522394,
        "step": 4139
    },
    {
        "loss": 1.4902,
        "grad_norm": 3.4331934452056885,
        "learning_rate": 3.0460488167747715e-06,
        "epoch": 0.3085177733065057,
        "step": 4140
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.059413194656372,
        "learning_rate": 3.028837807894391e-06,
        "epoch": 0.30859229450778747,
        "step": 4141
    },
    {
        "loss": 1.0608,
        "grad_norm": 3.0214929580688477,
        "learning_rate": 3.011674812611609e-06,
        "epoch": 0.30866681570906923,
        "step": 4142
    },
    {
        "loss": 2.8252,
        "grad_norm": 2.3371176719665527,
        "learning_rate": 2.9945598394243713e-06,
        "epoch": 0.308741336910351,
        "step": 4143
    },
    {
        "loss": 2.6749,
        "grad_norm": 2.5948612689971924,
        "learning_rate": 2.977492896806866e-06,
        "epoch": 0.30881585811163276,
        "step": 4144
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.7150180339813232,
        "learning_rate": 2.9604739932095338e-06,
        "epoch": 0.3088903793129145,
        "step": 4145
    },
    {
        "loss": 2.8551,
        "grad_norm": 2.243256092071533,
        "learning_rate": 2.943503137058956e-06,
        "epoch": 0.3089649005141963,
        "step": 4146
    },
    {
        "loss": 2.5094,
        "grad_norm": 2.4513425827026367,
        "learning_rate": 2.926580336757978e-06,
        "epoch": 0.30903942171547805,
        "step": 4147
    },
    {
        "loss": 2.6286,
        "grad_norm": 1.5488148927688599,
        "learning_rate": 2.9097056006856082e-06,
        "epoch": 0.3091139429167598,
        "step": 4148
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.3725335597991943,
        "learning_rate": 2.8928789371970745e-06,
        "epoch": 0.3091884641180416,
        "step": 4149
    },
    {
        "loss": 2.1626,
        "grad_norm": 3.441598653793335,
        "learning_rate": 2.8761003546238564e-06,
        "epoch": 0.30926298531932334,
        "step": 4150
    },
    {
        "loss": 1.411,
        "grad_norm": 3.8304359912872314,
        "learning_rate": 2.859369861273564e-06,
        "epoch": 0.3093375065206051,
        "step": 4151
    },
    {
        "loss": 2.4187,
        "grad_norm": 2.43754243850708,
        "learning_rate": 2.8426874654299718e-06,
        "epoch": 0.30941202772188686,
        "step": 4152
    },
    {
        "loss": 2.4028,
        "grad_norm": 2.585653781890869,
        "learning_rate": 2.826053175353116e-06,
        "epoch": 0.3094865489231686,
        "step": 4153
    },
    {
        "loss": 2.3215,
        "grad_norm": 2.9248769283294678,
        "learning_rate": 2.8094669992791645e-06,
        "epoch": 0.3095610701244504,
        "step": 4154
    },
    {
        "loss": 2.2402,
        "grad_norm": 2.750135660171509,
        "learning_rate": 2.7929289454205034e-06,
        "epoch": 0.30963559132573215,
        "step": 4155
    },
    {
        "loss": 2.3593,
        "grad_norm": 3.064310073852539,
        "learning_rate": 2.7764390219656377e-06,
        "epoch": 0.3097101125270139,
        "step": 4156
    },
    {
        "loss": 2.1342,
        "grad_norm": 2.5111587047576904,
        "learning_rate": 2.7599972370792703e-06,
        "epoch": 0.3097846337282957,
        "step": 4157
    },
    {
        "loss": 2.8278,
        "grad_norm": 2.4209976196289062,
        "learning_rate": 2.7436035989022667e-06,
        "epoch": 0.30985915492957744,
        "step": 4158
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.32672917842865,
        "learning_rate": 2.7272581155516674e-06,
        "epoch": 0.3099336761308592,
        "step": 4159
    },
    {
        "loss": 2.5318,
        "grad_norm": 1.3822935819625854,
        "learning_rate": 2.7109607951206426e-06,
        "epoch": 0.31000819733214097,
        "step": 4160
    },
    {
        "loss": 2.1959,
        "grad_norm": 1.9647130966186523,
        "learning_rate": 2.694711645678538e-06,
        "epoch": 0.3100827185334228,
        "step": 4161
    },
    {
        "loss": 2.7259,
        "grad_norm": 2.8143656253814697,
        "learning_rate": 2.67851067527084e-06,
        "epoch": 0.31015723973470455,
        "step": 4162
    },
    {
        "loss": 2.2647,
        "grad_norm": 2.228661060333252,
        "learning_rate": 2.662357891919165e-06,
        "epoch": 0.3102317609359863,
        "step": 4163
    },
    {
        "loss": 2.2276,
        "grad_norm": 2.918520450592041,
        "learning_rate": 2.6462533036212933e-06,
        "epoch": 0.3103062821372681,
        "step": 4164
    },
    {
        "loss": 2.0517,
        "grad_norm": 3.284396171569824,
        "learning_rate": 2.6301969183511467e-06,
        "epoch": 0.31038080333854984,
        "step": 4165
    },
    {
        "loss": 2.7344,
        "grad_norm": 3.4396259784698486,
        "learning_rate": 2.614188744058765e-06,
        "epoch": 0.3104553245398316,
        "step": 4166
    },
    {
        "loss": 1.5056,
        "grad_norm": 2.876624822616577,
        "learning_rate": 2.5982287886703095e-06,
        "epoch": 0.31052984574111336,
        "step": 4167
    },
    {
        "loss": 2.6385,
        "grad_norm": 2.7533905506134033,
        "learning_rate": 2.5823170600880574e-06,
        "epoch": 0.3106043669423951,
        "step": 4168
    },
    {
        "loss": 2.4191,
        "grad_norm": 2.4127795696258545,
        "learning_rate": 2.566453566190452e-06,
        "epoch": 0.3106788881436769,
        "step": 4169
    },
    {
        "loss": 1.9244,
        "grad_norm": 3.158435821533203,
        "learning_rate": 2.5506383148320435e-06,
        "epoch": 0.31075340934495865,
        "step": 4170
    },
    {
        "loss": 2.4169,
        "grad_norm": 3.584939956665039,
        "learning_rate": 2.5348713138434564e-06,
        "epoch": 0.3108279305462404,
        "step": 4171
    },
    {
        "loss": 2.5817,
        "grad_norm": 2.4827687740325928,
        "learning_rate": 2.519152571031447e-06,
        "epoch": 0.3109024517475222,
        "step": 4172
    },
    {
        "loss": 2.1498,
        "grad_norm": 2.4794936180114746,
        "learning_rate": 2.503482094178877e-06,
        "epoch": 0.31097697294880394,
        "step": 4173
    },
    {
        "loss": 2.6968,
        "grad_norm": 2.3417019844055176,
        "learning_rate": 2.4878598910447303e-06,
        "epoch": 0.3110514941500857,
        "step": 4174
    },
    {
        "loss": 2.5196,
        "grad_norm": 3.3811757564544678,
        "learning_rate": 2.4722859693640745e-06,
        "epoch": 0.31112601535136747,
        "step": 4175
    },
    {
        "loss": 1.5779,
        "grad_norm": 1.6441047191619873,
        "learning_rate": 2.456760336848052e-06,
        "epoch": 0.31120053655264923,
        "step": 4176
    },
    {
        "loss": 2.2097,
        "grad_norm": 4.32444429397583,
        "learning_rate": 2.441283001183914e-06,
        "epoch": 0.311275057753931,
        "step": 4177
    },
    {
        "loss": 1.4398,
        "grad_norm": 2.527829885482788,
        "learning_rate": 2.425853970034997e-06,
        "epoch": 0.31134957895521276,
        "step": 4178
    },
    {
        "loss": 1.2594,
        "grad_norm": 3.419588088989258,
        "learning_rate": 2.4104732510407123e-06,
        "epoch": 0.3114241001564945,
        "step": 4179
    },
    {
        "loss": 2.0372,
        "grad_norm": 3.832476854324341,
        "learning_rate": 2.39514085181658e-06,
        "epoch": 0.3114986213577763,
        "step": 4180
    },
    {
        "loss": 2.3257,
        "grad_norm": 1.913496732711792,
        "learning_rate": 2.3798567799541706e-06,
        "epoch": 0.31157314255905805,
        "step": 4181
    },
    {
        "loss": 2.0757,
        "grad_norm": 3.3160181045532227,
        "learning_rate": 2.364621043021098e-06,
        "epoch": 0.3116476637603398,
        "step": 4182
    },
    {
        "loss": 2.1734,
        "grad_norm": 2.368180513381958,
        "learning_rate": 2.3494336485610945e-06,
        "epoch": 0.3117221849616216,
        "step": 4183
    },
    {
        "loss": 2.2415,
        "grad_norm": 2.57002329826355,
        "learning_rate": 2.334294604093956e-06,
        "epoch": 0.31179670616290334,
        "step": 4184
    },
    {
        "loss": 1.8698,
        "grad_norm": 4.380834102630615,
        "learning_rate": 2.3192039171154755e-06,
        "epoch": 0.3118712273641851,
        "step": 4185
    },
    {
        "loss": 1.9345,
        "grad_norm": 2.968811273574829,
        "learning_rate": 2.304161595097587e-06,
        "epoch": 0.31194574856546686,
        "step": 4186
    },
    {
        "loss": 2.6933,
        "grad_norm": 2.032642364501953,
        "learning_rate": 2.2891676454881996e-06,
        "epoch": 0.3120202697667486,
        "step": 4187
    },
    {
        "loss": 2.2583,
        "grad_norm": 2.9933853149414062,
        "learning_rate": 2.2742220757113407e-06,
        "epoch": 0.3120947909680304,
        "step": 4188
    },
    {
        "loss": 1.9929,
        "grad_norm": 2.674316883087158,
        "learning_rate": 2.2593248931670473e-06,
        "epoch": 0.31216931216931215,
        "step": 4189
    },
    {
        "loss": 2.2092,
        "grad_norm": 3.221095561981201,
        "learning_rate": 2.2444761052313856e-06,
        "epoch": 0.3122438333705939,
        "step": 4190
    },
    {
        "loss": 2.7126,
        "grad_norm": 1.889775037765503,
        "learning_rate": 2.2296757192564855e-06,
        "epoch": 0.3123183545718757,
        "step": 4191
    },
    {
        "loss": 2.0716,
        "grad_norm": 3.4053170680999756,
        "learning_rate": 2.2149237425705073e-06,
        "epoch": 0.31239287577315744,
        "step": 4192
    },
    {
        "loss": 2.6375,
        "grad_norm": 2.858539342880249,
        "learning_rate": 2.2002201824776193e-06,
        "epoch": 0.3124673969744392,
        "step": 4193
    },
    {
        "loss": 2.3268,
        "grad_norm": 1.6201657056808472,
        "learning_rate": 2.1855650462580646e-06,
        "epoch": 0.31254191817572097,
        "step": 4194
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.132692337036133,
        "learning_rate": 2.17095834116805e-06,
        "epoch": 0.31261643937700273,
        "step": 4195
    },
    {
        "loss": 2.6285,
        "grad_norm": 3.4423017501831055,
        "learning_rate": 2.1564000744398573e-06,
        "epoch": 0.31269096057828455,
        "step": 4196
    },
    {
        "loss": 2.4064,
        "grad_norm": 2.8456547260284424,
        "learning_rate": 2.141890253281753e-06,
        "epoch": 0.3127654817795663,
        "step": 4197
    },
    {
        "loss": 2.6789,
        "grad_norm": 2.2697274684906006,
        "learning_rate": 2.1274288848780355e-06,
        "epoch": 0.3128400029808481,
        "step": 4198
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.929016590118408,
        "learning_rate": 2.1130159763889654e-06,
        "epoch": 0.31291452418212984,
        "step": 4199
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.2589259147644043,
        "learning_rate": 2.0986515349508907e-06,
        "epoch": 0.3129890453834116,
        "step": 4200
    },
    {
        "loss": 2.6135,
        "grad_norm": 2.486257553100586,
        "learning_rate": 2.0843355676760878e-06,
        "epoch": 0.31306356658469336,
        "step": 4201
    },
    {
        "loss": 3.0608,
        "grad_norm": 2.1457669734954834,
        "learning_rate": 2.070068081652876e-06,
        "epoch": 0.3131380877859751,
        "step": 4202
    },
    {
        "loss": 2.8017,
        "grad_norm": 2.4807565212249756,
        "learning_rate": 2.055849083945549e-06,
        "epoch": 0.3132126089872569,
        "step": 4203
    },
    {
        "loss": 2.1921,
        "grad_norm": 3.6165454387664795,
        "learning_rate": 2.0416785815943973e-06,
        "epoch": 0.31328713018853865,
        "step": 4204
    },
    {
        "loss": 2.6697,
        "grad_norm": 2.18224835395813,
        "learning_rate": 2.027556581615686e-06,
        "epoch": 0.3133616513898204,
        "step": 4205
    },
    {
        "loss": 2.8113,
        "grad_norm": 2.6129846572875977,
        "learning_rate": 2.013483091001711e-06,
        "epoch": 0.3134361725911022,
        "step": 4206
    },
    {
        "loss": 2.538,
        "grad_norm": 1.7254321575164795,
        "learning_rate": 1.999458116720698e-06,
        "epoch": 0.31351069379238394,
        "step": 4207
    },
    {
        "loss": 2.0645,
        "grad_norm": 2.6207876205444336,
        "learning_rate": 1.985481665716882e-06,
        "epoch": 0.3135852149936657,
        "step": 4208
    },
    {
        "loss": 1.8925,
        "grad_norm": 2.851156234741211,
        "learning_rate": 1.971553744910448e-06,
        "epoch": 0.31365973619494747,
        "step": 4209
    },
    {
        "loss": 2.2934,
        "grad_norm": 3.5544631481170654,
        "learning_rate": 1.95767436119757e-06,
        "epoch": 0.31373425739622923,
        "step": 4210
    },
    {
        "loss": 1.6814,
        "grad_norm": 3.8489925861358643,
        "learning_rate": 1.9438435214503947e-06,
        "epoch": 0.313808778597511,
        "step": 4211
    },
    {
        "loss": 2.2573,
        "grad_norm": 2.8792264461517334,
        "learning_rate": 1.930061232517011e-06,
        "epoch": 0.31388329979879276,
        "step": 4212
    },
    {
        "loss": 1.2273,
        "grad_norm": 2.4010636806488037,
        "learning_rate": 1.916327501221493e-06,
        "epoch": 0.3139578210000745,
        "step": 4213
    },
    {
        "loss": 2.2455,
        "grad_norm": 2.4121391773223877,
        "learning_rate": 1.9026423343638466e-06,
        "epoch": 0.3140323422013563,
        "step": 4214
    },
    {
        "loss": 2.3249,
        "grad_norm": 2.7774603366851807,
        "learning_rate": 1.8890057387200622e-06,
        "epoch": 0.31410686340263805,
        "step": 4215
    },
    {
        "loss": 2.1909,
        "grad_norm": 3.2436532974243164,
        "learning_rate": 1.8754177210420498e-06,
        "epoch": 0.3141813846039198,
        "step": 4216
    },
    {
        "loss": 2.8609,
        "grad_norm": 2.400881290435791,
        "learning_rate": 1.8618782880576945e-06,
        "epoch": 0.3142559058052016,
        "step": 4217
    },
    {
        "loss": 2.4687,
        "grad_norm": 1.6919323205947876,
        "learning_rate": 1.8483874464708118e-06,
        "epoch": 0.31433042700648334,
        "step": 4218
    },
    {
        "loss": 2.4057,
        "grad_norm": 3.697727918624878,
        "learning_rate": 1.834945202961147e-06,
        "epoch": 0.3144049482077651,
        "step": 4219
    },
    {
        "loss": 2.2691,
        "grad_norm": 3.377284288406372,
        "learning_rate": 1.8215515641843983e-06,
        "epoch": 0.31447946940904686,
        "step": 4220
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.3971338272094727,
        "learning_rate": 1.8082065367722057e-06,
        "epoch": 0.3145539906103286,
        "step": 4221
    },
    {
        "loss": 2.469,
        "grad_norm": 3.5609452724456787,
        "learning_rate": 1.794910127332128e-06,
        "epoch": 0.3146285118116104,
        "step": 4222
    },
    {
        "loss": 2.6868,
        "grad_norm": 1.492832899093628,
        "learning_rate": 1.7816623424476542e-06,
        "epoch": 0.31470303301289215,
        "step": 4223
    },
    {
        "loss": 1.5111,
        "grad_norm": 4.751928806304932,
        "learning_rate": 1.76846318867816e-06,
        "epoch": 0.3147775542141739,
        "step": 4224
    },
    {
        "loss": 2.4009,
        "grad_norm": 2.9829225540161133,
        "learning_rate": 1.7553126725590286e-06,
        "epoch": 0.3148520754154557,
        "step": 4225
    },
    {
        "loss": 2.8343,
        "grad_norm": 2.04137921333313,
        "learning_rate": 1.7422108006014847e-06,
        "epoch": 0.31492659661673744,
        "step": 4226
    },
    {
        "loss": 2.475,
        "grad_norm": 3.182701349258423,
        "learning_rate": 1.7291575792927173e-06,
        "epoch": 0.3150011178180192,
        "step": 4227
    },
    {
        "loss": 1.6538,
        "grad_norm": 1.6103756427764893,
        "learning_rate": 1.7161530150957784e-06,
        "epoch": 0.31507563901930097,
        "step": 4228
    },
    {
        "loss": 1.7056,
        "grad_norm": 3.7548935413360596,
        "learning_rate": 1.7031971144496506e-06,
        "epoch": 0.31515016022058273,
        "step": 4229
    },
    {
        "loss": 2.7014,
        "grad_norm": 3.1262266635894775,
        "learning_rate": 1.6902898837692361e-06,
        "epoch": 0.3152246814218645,
        "step": 4230
    },
    {
        "loss": 2.4328,
        "grad_norm": 3.603376626968384,
        "learning_rate": 1.6774313294453448e-06,
        "epoch": 0.31529920262314626,
        "step": 4231
    },
    {
        "loss": 2.9378,
        "grad_norm": 2.285310745239258,
        "learning_rate": 1.6646214578446506e-06,
        "epoch": 0.3153737238244281,
        "step": 4232
    },
    {
        "loss": 2.06,
        "grad_norm": 4.742151737213135,
        "learning_rate": 1.6518602753097246e-06,
        "epoch": 0.31544824502570984,
        "step": 4233
    },
    {
        "loss": 2.0143,
        "grad_norm": 3.871742010116577,
        "learning_rate": 1.6391477881590677e-06,
        "epoch": 0.3155227662269916,
        "step": 4234
    },
    {
        "loss": 2.4033,
        "grad_norm": 2.7443034648895264,
        "learning_rate": 1.6264840026870344e-06,
        "epoch": 0.31559728742827337,
        "step": 4235
    },
    {
        "loss": 2.3128,
        "grad_norm": 2.399348735809326,
        "learning_rate": 1.6138689251638972e-06,
        "epoch": 0.31567180862955513,
        "step": 4236
    },
    {
        "loss": 2.1668,
        "grad_norm": 2.802830219268799,
        "learning_rate": 1.6013025618357936e-06,
        "epoch": 0.3157463298308369,
        "step": 4237
    },
    {
        "loss": 2.4173,
        "grad_norm": 2.646979808807373,
        "learning_rate": 1.588784918924724e-06,
        "epoch": 0.31582085103211865,
        "step": 4238
    },
    {
        "loss": 2.2302,
        "grad_norm": 3.417720079421997,
        "learning_rate": 1.5763160026285862e-06,
        "epoch": 0.3158953722334004,
        "step": 4239
    },
    {
        "loss": 1.6926,
        "grad_norm": 3.2712697982788086,
        "learning_rate": 1.5638958191211417e-06,
        "epoch": 0.3159698934346822,
        "step": 4240
    },
    {
        "loss": 2.4931,
        "grad_norm": 3.3232157230377197,
        "learning_rate": 1.5515243745520825e-06,
        "epoch": 0.31604441463596394,
        "step": 4241
    },
    {
        "loss": 2.5684,
        "grad_norm": 2.185336112976074,
        "learning_rate": 1.5392016750468418e-06,
        "epoch": 0.3161189358372457,
        "step": 4242
    },
    {
        "loss": 1.965,
        "grad_norm": 3.4837777614593506,
        "learning_rate": 1.5269277267068394e-06,
        "epoch": 0.31619345703852747,
        "step": 4243
    },
    {
        "loss": 2.6832,
        "grad_norm": 2.289854049682617,
        "learning_rate": 1.5147025356092914e-06,
        "epoch": 0.31626797823980923,
        "step": 4244
    },
    {
        "loss": 2.3463,
        "grad_norm": 3.0302541255950928,
        "learning_rate": 1.5025261078073005e-06,
        "epoch": 0.316342499441091,
        "step": 4245
    },
    {
        "loss": 2.4427,
        "grad_norm": 2.015273332595825,
        "learning_rate": 1.4903984493298106e-06,
        "epoch": 0.31641702064237276,
        "step": 4246
    },
    {
        "loss": 2.461,
        "grad_norm": 2.855517625808716,
        "learning_rate": 1.4783195661816074e-06,
        "epoch": 0.3164915418436545,
        "step": 4247
    },
    {
        "loss": 2.6655,
        "grad_norm": 2.0902316570281982,
        "learning_rate": 1.4662894643433623e-06,
        "epoch": 0.3165660630449363,
        "step": 4248
    },
    {
        "loss": 1.9835,
        "grad_norm": 2.6379358768463135,
        "learning_rate": 1.4543081497715661e-06,
        "epoch": 0.31664058424621805,
        "step": 4249
    },
    {
        "loss": 1.2596,
        "grad_norm": 4.244597911834717,
        "learning_rate": 1.4423756283985623e-06,
        "epoch": 0.3167151054474998,
        "step": 4250
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.2475733757019043,
        "learning_rate": 1.430491906132525e-06,
        "epoch": 0.3167896266487816,
        "step": 4251
    },
    {
        "loss": 2.1131,
        "grad_norm": 4.321876049041748,
        "learning_rate": 1.4186569888574808e-06,
        "epoch": 0.31686414785006334,
        "step": 4252
    },
    {
        "loss": 2.2168,
        "grad_norm": 3.3885865211486816,
        "learning_rate": 1.4068708824332866e-06,
        "epoch": 0.3169386690513451,
        "step": 4253
    },
    {
        "loss": 2.5877,
        "grad_norm": 2.0521929264068604,
        "learning_rate": 1.395133592695619e-06,
        "epoch": 0.31701319025262686,
        "step": 4254
    },
    {
        "loss": 2.5717,
        "grad_norm": 3.359813690185547,
        "learning_rate": 1.3834451254560066e-06,
        "epoch": 0.3170877114539086,
        "step": 4255
    },
    {
        "loss": 2.0011,
        "grad_norm": 3.5288872718811035,
        "learning_rate": 1.371805486501776e-06,
        "epoch": 0.3171622326551904,
        "step": 4256
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.122464418411255,
        "learning_rate": 1.360214681596117e-06,
        "epoch": 0.31723675385647215,
        "step": 4257
    },
    {
        "loss": 1.7351,
        "grad_norm": 1.7300316095352173,
        "learning_rate": 1.3486727164780055e-06,
        "epoch": 0.3173112750577539,
        "step": 4258
    },
    {
        "loss": 2.7457,
        "grad_norm": 2.892016887664795,
        "learning_rate": 1.3371795968622257e-06,
        "epoch": 0.3173857962590357,
        "step": 4259
    },
    {
        "loss": 2.6465,
        "grad_norm": 2.432352304458618,
        "learning_rate": 1.325735328439448e-06,
        "epoch": 0.31746031746031744,
        "step": 4260
    },
    {
        "loss": 2.8732,
        "grad_norm": 2.184515953063965,
        "learning_rate": 1.3143399168760506e-06,
        "epoch": 0.3175348386615992,
        "step": 4261
    },
    {
        "loss": 2.0513,
        "grad_norm": 3.6719324588775635,
        "learning_rate": 1.3029933678142981e-06,
        "epoch": 0.31760935986288097,
        "step": 4262
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.4898486137390137,
        "learning_rate": 1.2916956868722407e-06,
        "epoch": 0.31768388106416273,
        "step": 4263
    },
    {
        "loss": 2.5426,
        "grad_norm": 2.3701164722442627,
        "learning_rate": 1.2804468796437374e-06,
        "epoch": 0.3177584022654445,
        "step": 4264
    },
    {
        "loss": 2.7202,
        "grad_norm": 2.2034239768981934,
        "learning_rate": 1.2692469516984219e-06,
        "epoch": 0.31783292346672626,
        "step": 4265
    },
    {
        "loss": 2.8206,
        "grad_norm": 2.1804914474487305,
        "learning_rate": 1.2580959085817467e-06,
        "epoch": 0.317907444668008,
        "step": 4266
    },
    {
        "loss": 2.5771,
        "grad_norm": 3.1509909629821777,
        "learning_rate": 1.2469937558149846e-06,
        "epoch": 0.31798196586928984,
        "step": 4267
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.669098138809204,
        "learning_rate": 1.2359404988951383e-06,
        "epoch": 0.3180564870705716,
        "step": 4268
    },
    {
        "loss": 2.8247,
        "grad_norm": 2.9366559982299805,
        "learning_rate": 1.2249361432950746e-06,
        "epoch": 0.31813100827185337,
        "step": 4269
    },
    {
        "loss": 2.3276,
        "grad_norm": 2.684739589691162,
        "learning_rate": 1.213980694463368e-06,
        "epoch": 0.31820552947313513,
        "step": 4270
    },
    {
        "loss": 2.6428,
        "grad_norm": 3.023214101791382,
        "learning_rate": 1.2030741578244465e-06,
        "epoch": 0.3182800506744169,
        "step": 4271
    },
    {
        "loss": 2.3639,
        "grad_norm": 2.9431111812591553,
        "learning_rate": 1.192216538778501e-06,
        "epoch": 0.31835457187569866,
        "step": 4272
    },
    {
        "loss": 2.728,
        "grad_norm": 2.1777076721191406,
        "learning_rate": 1.1814078427014874e-06,
        "epoch": 0.3184290930769804,
        "step": 4273
    },
    {
        "loss": 2.7289,
        "grad_norm": 2.078320026397705,
        "learning_rate": 1.1706480749451354e-06,
        "epoch": 0.3185036142782622,
        "step": 4274
    },
    {
        "loss": 2.2902,
        "grad_norm": 2.76593279838562,
        "learning_rate": 1.1599372408369614e-06,
        "epoch": 0.31857813547954394,
        "step": 4275
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.8050501346588135,
        "learning_rate": 1.1492753456802452e-06,
        "epoch": 0.3186526566808257,
        "step": 4276
    },
    {
        "loss": 2.5671,
        "grad_norm": 3.3028969764709473,
        "learning_rate": 1.138662394754053e-06,
        "epoch": 0.31872717788210747,
        "step": 4277
    },
    {
        "loss": 3.0288,
        "grad_norm": 2.2804746627807617,
        "learning_rate": 1.1280983933132038e-06,
        "epoch": 0.31880169908338923,
        "step": 4278
    },
    {
        "loss": 2.331,
        "grad_norm": 2.1621410846710205,
        "learning_rate": 1.11758334658828e-06,
        "epoch": 0.318876220284671,
        "step": 4279
    },
    {
        "loss": 2.2562,
        "grad_norm": 2.162766933441162,
        "learning_rate": 1.107117259785606e-06,
        "epoch": 0.31895074148595276,
        "step": 4280
    },
    {
        "loss": 0.9488,
        "grad_norm": 3.35526967048645,
        "learning_rate": 1.0967001380873031e-06,
        "epoch": 0.3190252626872345,
        "step": 4281
    },
    {
        "loss": 2.5299,
        "grad_norm": 2.1362149715423584,
        "learning_rate": 1.0863319866512234e-06,
        "epoch": 0.3190997838885163,
        "step": 4282
    },
    {
        "loss": 2.3271,
        "grad_norm": 2.5101141929626465,
        "learning_rate": 1.0760128106109935e-06,
        "epoch": 0.31917430508979805,
        "step": 4283
    },
    {
        "loss": 2.911,
        "grad_norm": 2.2916834354400635,
        "learning_rate": 1.0657426150759597e-06,
        "epoch": 0.3192488262910798,
        "step": 4284
    },
    {
        "loss": 2.327,
        "grad_norm": 2.144172430038452,
        "learning_rate": 1.0555214051312102e-06,
        "epoch": 0.3193233474923616,
        "step": 4285
    },
    {
        "loss": 2.7394,
        "grad_norm": 2.100074052810669,
        "learning_rate": 1.0453491858376408e-06,
        "epoch": 0.31939786869364334,
        "step": 4286
    },
    {
        "loss": 2.6979,
        "grad_norm": 2.3776447772979736,
        "learning_rate": 1.035225962231834e-06,
        "epoch": 0.3194723898949251,
        "step": 4287
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.2179553508758545,
        "learning_rate": 1.0251517393261355e-06,
        "epoch": 0.31954691109620686,
        "step": 4288
    },
    {
        "loss": 2.6507,
        "grad_norm": 2.0753018856048584,
        "learning_rate": 1.0151265221086225e-06,
        "epoch": 0.31962143229748863,
        "step": 4289
    },
    {
        "loss": 2.2001,
        "grad_norm": 2.9886369705200195,
        "learning_rate": 1.0051503155430908e-06,
        "epoch": 0.3196959534987704,
        "step": 4290
    },
    {
        "loss": 2.6922,
        "grad_norm": 3.2855191230773926,
        "learning_rate": 9.952231245691e-07,
        "epoch": 0.31977047470005215,
        "step": 4291
    },
    {
        "loss": 2.1898,
        "grad_norm": 2.2698750495910645,
        "learning_rate": 9.8534495410193e-07,
        "epoch": 0.3198449959013339,
        "step": 4292
    },
    {
        "loss": 2.2273,
        "grad_norm": 2.339938163757324,
        "learning_rate": 9.755158090325789e-07,
        "epoch": 0.3199195171026157,
        "step": 4293
    },
    {
        "loss": 2.8959,
        "grad_norm": 2.4526309967041016,
        "learning_rate": 9.657356942277873e-07,
        "epoch": 0.31999403830389744,
        "step": 4294
    },
    {
        "loss": 2.4484,
        "grad_norm": 3.8719942569732666,
        "learning_rate": 9.560046145300038e-07,
        "epoch": 0.3200685595051792,
        "step": 4295
    },
    {
        "loss": 1.655,
        "grad_norm": 3.9157652854919434,
        "learning_rate": 9.463225747573967e-07,
        "epoch": 0.32014308070646097,
        "step": 4296
    },
    {
        "loss": 2.2906,
        "grad_norm": 5.493773460388184,
        "learning_rate": 9.366895797038866e-07,
        "epoch": 0.32021760190774273,
        "step": 4297
    },
    {
        "loss": 2.4219,
        "grad_norm": 2.1121628284454346,
        "learning_rate": 9.271056341390582e-07,
        "epoch": 0.3202921231090245,
        "step": 4298
    },
    {
        "loss": 2.5291,
        "grad_norm": 1.70021390914917,
        "learning_rate": 9.175707428082492e-07,
        "epoch": 0.32036664431030626,
        "step": 4299
    },
    {
        "loss": 2.6015,
        "grad_norm": 3.236752986907959,
        "learning_rate": 9.08084910432494e-07,
        "epoch": 0.320441165511588,
        "step": 4300
    },
    {
        "loss": 2.9125,
        "grad_norm": 2.291656494140625,
        "learning_rate": 8.986481417085246e-07,
        "epoch": 0.3205156867128698,
        "step": 4301
    },
    {
        "loss": 2.5751,
        "grad_norm": 2.706246852874756,
        "learning_rate": 8.892604413088368e-07,
        "epoch": 0.3205902079141516,
        "step": 4302
    },
    {
        "loss": 2.7812,
        "grad_norm": 2.063185930252075,
        "learning_rate": 8.799218138815457e-07,
        "epoch": 0.32066472911543337,
        "step": 4303
    },
    {
        "loss": 1.7488,
        "grad_norm": 3.3977108001708984,
        "learning_rate": 8.706322640505304e-07,
        "epoch": 0.32073925031671513,
        "step": 4304
    },
    {
        "loss": 2.4216,
        "grad_norm": 2.0288383960723877,
        "learning_rate": 8.613917964153339e-07,
        "epoch": 0.3208137715179969,
        "step": 4305
    },
    {
        "loss": 1.8089,
        "grad_norm": 2.8019707202911377,
        "learning_rate": 8.522004155512409e-07,
        "epoch": 0.32088829271927866,
        "step": 4306
    },
    {
        "loss": 2.2714,
        "grad_norm": 3.695366382598877,
        "learning_rate": 8.430581260091886e-07,
        "epoch": 0.3209628139205604,
        "step": 4307
    },
    {
        "loss": 2.3283,
        "grad_norm": 2.7598588466644287,
        "learning_rate": 8.339649323158227e-07,
        "epoch": 0.3210373351218422,
        "step": 4308
    },
    {
        "loss": 2.3945,
        "grad_norm": 2.584705114364624,
        "learning_rate": 8.249208389734974e-07,
        "epoch": 0.32111185632312395,
        "step": 4309
    },
    {
        "loss": 2.1493,
        "grad_norm": 3.1091227531433105,
        "learning_rate": 8.159258504602085e-07,
        "epoch": 0.3211863775244057,
        "step": 4310
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.486393690109253,
        "learning_rate": 8.069799712297044e-07,
        "epoch": 0.32126089872568747,
        "step": 4311
    },
    {
        "loss": 1.9254,
        "grad_norm": 3.624643325805664,
        "learning_rate": 7.980832057113641e-07,
        "epoch": 0.32133541992696923,
        "step": 4312
    },
    {
        "loss": 2.1676,
        "grad_norm": 2.4177486896514893,
        "learning_rate": 7.89235558310264e-07,
        "epoch": 0.321409941128251,
        "step": 4313
    },
    {
        "loss": 1.938,
        "grad_norm": 2.5001492500305176,
        "learning_rate": 7.804370334071665e-07,
        "epoch": 0.32148446232953276,
        "step": 4314
    },
    {
        "loss": 2.4064,
        "grad_norm": 2.6147897243499756,
        "learning_rate": 7.7168763535852e-07,
        "epoch": 0.3215589835308145,
        "step": 4315
    },
    {
        "loss": 1.8804,
        "grad_norm": 1.8416614532470703,
        "learning_rate": 7.629873684964262e-07,
        "epoch": 0.3216335047320963,
        "step": 4316
    },
    {
        "loss": 2.5883,
        "grad_norm": 2.202467203140259,
        "learning_rate": 7.543362371286833e-07,
        "epoch": 0.32170802593337805,
        "step": 4317
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.8527591228485107,
        "learning_rate": 7.457342455387429e-07,
        "epoch": 0.3217825471346598,
        "step": 4318
    },
    {
        "loss": 2.6425,
        "grad_norm": 3.5132522583007812,
        "learning_rate": 7.371813979857312e-07,
        "epoch": 0.3218570683359416,
        "step": 4319
    },
    {
        "loss": 2.105,
        "grad_norm": 2.5732414722442627,
        "learning_rate": 7.286776987044385e-07,
        "epoch": 0.32193158953722334,
        "step": 4320
    },
    {
        "loss": 2.5498,
        "grad_norm": 2.638157606124878,
        "learning_rate": 7.202231519053416e-07,
        "epoch": 0.3220061107385051,
        "step": 4321
    },
    {
        "loss": 2.6821,
        "grad_norm": 2.039910316467285,
        "learning_rate": 7.118177617745359e-07,
        "epoch": 0.32208063193978687,
        "step": 4322
    },
    {
        "loss": 2.8274,
        "grad_norm": 2.3174736499786377,
        "learning_rate": 7.034615324738369e-07,
        "epoch": 0.32215515314106863,
        "step": 4323
    },
    {
        "loss": 2.1865,
        "grad_norm": 3.5250611305236816,
        "learning_rate": 6.951544681406685e-07,
        "epoch": 0.3222296743423504,
        "step": 4324
    },
    {
        "loss": 2.3362,
        "grad_norm": 2.7722928524017334,
        "learning_rate": 6.868965728881405e-07,
        "epoch": 0.32230419554363215,
        "step": 4325
    },
    {
        "loss": 1.8008,
        "grad_norm": 1.9399470090866089,
        "learning_rate": 6.786878508049931e-07,
        "epoch": 0.3223787167449139,
        "step": 4326
    },
    {
        "loss": 2.4734,
        "grad_norm": 2.3891732692718506,
        "learning_rate": 6.705283059556422e-07,
        "epoch": 0.3224532379461957,
        "step": 4327
    },
    {
        "loss": 2.1878,
        "grad_norm": 2.260707378387451,
        "learning_rate": 6.624179423801558e-07,
        "epoch": 0.32252775914747744,
        "step": 4328
    },
    {
        "loss": 2.4568,
        "grad_norm": 2.3321292400360107,
        "learning_rate": 6.543567640942216e-07,
        "epoch": 0.3226022803487592,
        "step": 4329
    },
    {
        "loss": 2.0412,
        "grad_norm": 1.8629399538040161,
        "learning_rate": 6.463447750892027e-07,
        "epoch": 0.32267680155004097,
        "step": 4330
    },
    {
        "loss": 2.2217,
        "grad_norm": 4.121469020843506,
        "learning_rate": 6.383819793320922e-07,
        "epoch": 0.32275132275132273,
        "step": 4331
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.721696615219116,
        "learning_rate": 6.304683807655365e-07,
        "epoch": 0.3228258439526045,
        "step": 4332
    },
    {
        "loss": 2.5463,
        "grad_norm": 2.1987249851226807,
        "learning_rate": 6.226039833078123e-07,
        "epoch": 0.32290036515388626,
        "step": 4333
    },
    {
        "loss": 2.4688,
        "grad_norm": 3.3325698375701904,
        "learning_rate": 6.147887908528382e-07,
        "epoch": 0.322974886355168,
        "step": 4334
    },
    {
        "loss": 2.4228,
        "grad_norm": 1.809061884880066,
        "learning_rate": 6.070228072701855e-07,
        "epoch": 0.3230494075564498,
        "step": 4335
    },
    {
        "loss": 2.1303,
        "grad_norm": 2.6823534965515137,
        "learning_rate": 5.99306036405034e-07,
        "epoch": 0.32312392875773155,
        "step": 4336
    },
    {
        "loss": 2.0429,
        "grad_norm": 3.3143506050109863,
        "learning_rate": 5.916384820782161e-07,
        "epoch": 0.3231984499590133,
        "step": 4337
    },
    {
        "loss": 2.3514,
        "grad_norm": 1.9984451532363892,
        "learning_rate": 5.840201480861951e-07,
        "epoch": 0.32327297116029513,
        "step": 4338
    },
    {
        "loss": 2.3717,
        "grad_norm": 3.0109643936157227,
        "learning_rate": 5.764510382010424e-07,
        "epoch": 0.3233474923615769,
        "step": 4339
    },
    {
        "loss": 1.8763,
        "grad_norm": 2.8709981441497803,
        "learning_rate": 5.689311561704824e-07,
        "epoch": 0.32342201356285866,
        "step": 4340
    },
    {
        "loss": 2.6815,
        "grad_norm": 2.0864713191986084,
        "learning_rate": 5.614605057178368e-07,
        "epoch": 0.3234965347641404,
        "step": 4341
    },
    {
        "loss": 1.6942,
        "grad_norm": 2.4820845127105713,
        "learning_rate": 5.540390905420912e-07,
        "epoch": 0.3235710559654222,
        "step": 4342
    },
    {
        "loss": 1.9352,
        "grad_norm": 3.126070261001587,
        "learning_rate": 5.466669143178282e-07,
        "epoch": 0.32364557716670395,
        "step": 4343
    },
    {
        "loss": 2.247,
        "grad_norm": 1.9418256282806396,
        "learning_rate": 5.393439806952505e-07,
        "epoch": 0.3237200983679857,
        "step": 4344
    },
    {
        "loss": 2.5948,
        "grad_norm": 3.846952438354492,
        "learning_rate": 5.320702933001909e-07,
        "epoch": 0.32379461956926747,
        "step": 4345
    },
    {
        "loss": 1.7895,
        "grad_norm": 3.3442535400390625,
        "learning_rate": 5.248458557340574e-07,
        "epoch": 0.32386914077054924,
        "step": 4346
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.6451416015625,
        "learning_rate": 5.176706715739443e-07,
        "epoch": 0.323943661971831,
        "step": 4347
    },
    {
        "loss": 2.4907,
        "grad_norm": 2.5219264030456543,
        "learning_rate": 5.105447443724986e-07,
        "epoch": 0.32401818317311276,
        "step": 4348
    },
    {
        "loss": 1.9432,
        "grad_norm": 2.420611619949341,
        "learning_rate": 5.034680776580091e-07,
        "epoch": 0.3240927043743945,
        "step": 4349
    },
    {
        "loss": 2.5508,
        "grad_norm": 2.1044275760650635,
        "learning_rate": 4.96440674934362e-07,
        "epoch": 0.3241672255756763,
        "step": 4350
    },
    {
        "loss": 2.3161,
        "grad_norm": 3.1860666275024414,
        "learning_rate": 4.894625396810625e-07,
        "epoch": 0.32424174677695805,
        "step": 4351
    },
    {
        "loss": 2.6446,
        "grad_norm": 3.7724533081054688,
        "learning_rate": 4.825336753531917e-07,
        "epoch": 0.3243162679782398,
        "step": 4352
    },
    {
        "loss": 1.8141,
        "grad_norm": 3.4583988189697266,
        "learning_rate": 4.7565408538148283e-07,
        "epoch": 0.3243907891795216,
        "step": 4353
    },
    {
        "loss": 2.4014,
        "grad_norm": 2.460573673248291,
        "learning_rate": 4.6882377317223336e-07,
        "epoch": 0.32446531038080334,
        "step": 4354
    },
    {
        "loss": 1.5151,
        "grad_norm": 2.460853099822998,
        "learning_rate": 4.620427421073492e-07,
        "epoch": 0.3245398315820851,
        "step": 4355
    },
    {
        "loss": 2.6623,
        "grad_norm": 2.735100507736206,
        "learning_rate": 4.5531099554435576e-07,
        "epoch": 0.32461435278336687,
        "step": 4356
    },
    {
        "loss": 2.3545,
        "grad_norm": 3.1674678325653076,
        "learning_rate": 4.486285368163423e-07,
        "epoch": 0.32468887398464863,
        "step": 4357
    },
    {
        "loss": 1.3859,
        "grad_norm": 2.762665271759033,
        "learning_rate": 4.4199536923204e-07,
        "epoch": 0.3247633951859304,
        "step": 4358
    },
    {
        "loss": 1.5862,
        "grad_norm": 3.1759181022644043,
        "learning_rate": 4.354114960757327e-07,
        "epoch": 0.32483791638721216,
        "step": 4359
    },
    {
        "loss": 2.3241,
        "grad_norm": 1.8848453760147095,
        "learning_rate": 4.2887692060729066e-07,
        "epoch": 0.3249124375884939,
        "step": 4360
    },
    {
        "loss": 2.9383,
        "grad_norm": 2.3829169273376465,
        "learning_rate": 4.223916460622257e-07,
        "epoch": 0.3249869587897757,
        "step": 4361
    },
    {
        "loss": 2.5946,
        "grad_norm": 2.4553158283233643,
        "learning_rate": 4.1595567565160255e-07,
        "epoch": 0.32506147999105744,
        "step": 4362
    },
    {
        "loss": 2.2332,
        "grad_norm": 2.8572843074798584,
        "learning_rate": 4.095690125620832e-07,
        "epoch": 0.3251360011923392,
        "step": 4363
    },
    {
        "loss": 2.9223,
        "grad_norm": 3.2660746574401855,
        "learning_rate": 4.0323165995589386e-07,
        "epoch": 0.32521052239362097,
        "step": 4364
    },
    {
        "loss": 1.6308,
        "grad_norm": 1.5841563940048218,
        "learning_rate": 3.96943620970891e-07,
        "epoch": 0.32528504359490273,
        "step": 4365
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.3975682258605957,
        "learning_rate": 3.907048987204731e-07,
        "epoch": 0.3253595647961845,
        "step": 4366
    },
    {
        "loss": 2.4992,
        "grad_norm": 3.010509729385376,
        "learning_rate": 3.845154962936359e-07,
        "epoch": 0.32543408599746626,
        "step": 4367
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.3968894481658936,
        "learning_rate": 3.7837541675497246e-07,
        "epoch": 0.325508607198748,
        "step": 4368
    },
    {
        "loss": 2.6084,
        "grad_norm": 2.9400792121887207,
        "learning_rate": 3.722846631446175e-07,
        "epoch": 0.3255831284000298,
        "step": 4369
    },
    {
        "loss": 2.8855,
        "grad_norm": 1.7010891437530518,
        "learning_rate": 3.662432384783032e-07,
        "epoch": 0.32565764960131155,
        "step": 4370
    },
    {
        "loss": 1.966,
        "grad_norm": 3.482215642929077,
        "learning_rate": 3.6025114574734785e-07,
        "epoch": 0.3257321708025933,
        "step": 4371
    },
    {
        "loss": 2.5381,
        "grad_norm": 2.1789369583129883,
        "learning_rate": 3.5430838791863373e-07,
        "epoch": 0.3258066920038751,
        "step": 4372
    },
    {
        "loss": 2.0633,
        "grad_norm": 4.862247943878174,
        "learning_rate": 3.4841496793459604e-07,
        "epoch": 0.3258812132051569,
        "step": 4373
    },
    {
        "loss": 2.3985,
        "grad_norm": 1.8343125581741333,
        "learning_rate": 3.4257088871327837e-07,
        "epoch": 0.32595573440643866,
        "step": 4374
    },
    {
        "loss": 1.729,
        "grad_norm": 4.342755317687988,
        "learning_rate": 3.3677615314827714e-07,
        "epoch": 0.3260302556077204,
        "step": 4375
    },
    {
        "loss": 2.5569,
        "grad_norm": 2.6613171100616455,
        "learning_rate": 3.310307641087418e-07,
        "epoch": 0.3261047768090022,
        "step": 4376
    },
    {
        "loss": 2.4589,
        "grad_norm": 3.8055543899536133,
        "learning_rate": 3.2533472443941893e-07,
        "epoch": 0.32617929801028395,
        "step": 4377
    },
    {
        "loss": 2.3864,
        "grad_norm": 2.37809157371521,
        "learning_rate": 3.196880369605859e-07,
        "epoch": 0.3262538192115657,
        "step": 4378
    },
    {
        "loss": 1.4525,
        "grad_norm": 4.5697503089904785,
        "learning_rate": 3.140907044681063e-07,
        "epoch": 0.3263283404128475,
        "step": 4379
    },
    {
        "loss": 2.7054,
        "grad_norm": 2.61529278755188,
        "learning_rate": 3.085427297334187e-07,
        "epoch": 0.32640286161412924,
        "step": 4380
    },
    {
        "loss": 1.5755,
        "grad_norm": 2.978557825088501,
        "learning_rate": 3.030441155034924e-07,
        "epoch": 0.326477382815411,
        "step": 4381
    },
    {
        "loss": 2.5914,
        "grad_norm": 3.0253496170043945,
        "learning_rate": 2.9759486450087194e-07,
        "epoch": 0.32655190401669276,
        "step": 4382
    },
    {
        "loss": 2.6226,
        "grad_norm": 2.8995909690856934,
        "learning_rate": 2.921949794236656e-07,
        "epoch": 0.3266264252179745,
        "step": 4383
    },
    {
        "loss": 3.0583,
        "grad_norm": 1.9476203918457031,
        "learning_rate": 2.868444629455347e-07,
        "epoch": 0.3267009464192563,
        "step": 4384
    },
    {
        "loss": 2.4446,
        "grad_norm": 4.140487194061279,
        "learning_rate": 2.8154331771569343e-07,
        "epoch": 0.32677546762053805,
        "step": 4385
    },
    {
        "loss": 2.3548,
        "grad_norm": 4.344213485717773,
        "learning_rate": 2.7629154635889775e-07,
        "epoch": 0.3268499888218198,
        "step": 4386
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.161529779434204,
        "learning_rate": 2.7108915147550097e-07,
        "epoch": 0.3269245100231016,
        "step": 4387
    },
    {
        "loss": 2.3769,
        "grad_norm": 2.843287467956543,
        "learning_rate": 2.6593613564134256e-07,
        "epoch": 0.32699903122438334,
        "step": 4388
    },
    {
        "loss": 1.8843,
        "grad_norm": 2.822476625442505,
        "learning_rate": 2.6083250140788163e-07,
        "epoch": 0.3270735524256651,
        "step": 4389
    },
    {
        "loss": 2.2114,
        "grad_norm": 2.2729697227478027,
        "learning_rate": 2.5577825130208566e-07,
        "epoch": 0.32714807362694687,
        "step": 4390
    },
    {
        "loss": 1.7303,
        "grad_norm": 2.809516191482544,
        "learning_rate": 2.50773387826464e-07,
        "epoch": 0.32722259482822863,
        "step": 4391
    },
    {
        "loss": 2.46,
        "grad_norm": 3.6477677822113037,
        "learning_rate": 2.458179134591121e-07,
        "epoch": 0.3272971160295104,
        "step": 4392
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.449556589126587,
        "learning_rate": 2.409118306536229e-07,
        "epoch": 0.32737163723079216,
        "step": 4393
    },
    {
        "loss": 2.5694,
        "grad_norm": 2.558279037475586,
        "learning_rate": 2.3605514183918652e-07,
        "epoch": 0.3274461584320739,
        "step": 4394
    },
    {
        "loss": 1.7666,
        "grad_norm": 2.2912449836730957,
        "learning_rate": 2.312478494204795e-07,
        "epoch": 0.3275206796333557,
        "step": 4395
    },
    {
        "loss": 2.6136,
        "grad_norm": 2.2972097396850586,
        "learning_rate": 2.2648995577777554e-07,
        "epoch": 0.32759520083463745,
        "step": 4396
    },
    {
        "loss": 2.3567,
        "grad_norm": 3.1591498851776123,
        "learning_rate": 2.217814632668458e-07,
        "epoch": 0.3276697220359192,
        "step": 4397
    },
    {
        "loss": 2.7067,
        "grad_norm": 2.082634449005127,
        "learning_rate": 2.1712237421902537e-07,
        "epoch": 0.32774424323720097,
        "step": 4398
    },
    {
        "loss": 1.5454,
        "grad_norm": 3.0641613006591797,
        "learning_rate": 2.1251269094118008e-07,
        "epoch": 0.32781876443848273,
        "step": 4399
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.9805798530578613,
        "learning_rate": 2.0795241571572866e-07,
        "epoch": 0.3278932856397645,
        "step": 4400
    },
    {
        "loss": 2.3733,
        "grad_norm": 2.4528422355651855,
        "learning_rate": 2.0344155080058712e-07,
        "epoch": 0.32796780684104626,
        "step": 4401
    },
    {
        "loss": 2.486,
        "grad_norm": 3.3783764839172363,
        "learning_rate": 1.989800984292467e-07,
        "epoch": 0.328042328042328,
        "step": 4402
    },
    {
        "loss": 2.4929,
        "grad_norm": 3.174421548843384,
        "learning_rate": 1.945680608107181e-07,
        "epoch": 0.3281168492436098,
        "step": 4403
    },
    {
        "loss": 2.7553,
        "grad_norm": 3.7458510398864746,
        "learning_rate": 1.9020544012955388e-07,
        "epoch": 0.32819137044489155,
        "step": 4404
    },
    {
        "loss": 2.8377,
        "grad_norm": 3.0250885486602783,
        "learning_rate": 1.8589223854581506e-07,
        "epoch": 0.3282658916461733,
        "step": 4405
    },
    {
        "loss": 2.2783,
        "grad_norm": 3.298107147216797,
        "learning_rate": 1.8162845819511553e-07,
        "epoch": 0.3283404128474551,
        "step": 4406
    },
    {
        "loss": 2.1831,
        "grad_norm": 3.14046311378479,
        "learning_rate": 1.7741410118858881e-07,
        "epoch": 0.32841493404873684,
        "step": 4407
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.8841867446899414,
        "learning_rate": 1.7324916961291015e-07,
        "epoch": 0.3284894552500186,
        "step": 4408
    },
    {
        "loss": 2.114,
        "grad_norm": 3.128629684448242,
        "learning_rate": 1.6913366553027443e-07,
        "epoch": 0.3285639764513004,
        "step": 4409
    },
    {
        "loss": 2.6717,
        "grad_norm": 3.43161678314209,
        "learning_rate": 1.6506759097839609e-07,
        "epoch": 0.3286384976525822,
        "step": 4410
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.6207001209259033,
        "learning_rate": 1.6105094797050912e-07,
        "epoch": 0.32871301885386395,
        "step": 4411
    },
    {
        "loss": 2.6566,
        "grad_norm": 2.009352684020996,
        "learning_rate": 1.5708373849541158e-07,
        "epoch": 0.3287875400551457,
        "step": 4412
    },
    {
        "loss": 1.8622,
        "grad_norm": 3.036731004714966,
        "learning_rate": 1.5316596451736554e-07,
        "epoch": 0.3288620612564275,
        "step": 4413
    },
    {
        "loss": 1.8018,
        "grad_norm": 2.609516143798828,
        "learning_rate": 1.4929762797623037e-07,
        "epoch": 0.32893658245770924,
        "step": 4414
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.1450588703155518,
        "learning_rate": 1.4547873078731845e-07,
        "epoch": 0.329011103658991,
        "step": 4415
    },
    {
        "loss": 2.6168,
        "grad_norm": 2.6915318965911865,
        "learning_rate": 1.41709274841495e-07,
        "epoch": 0.32908562486027276,
        "step": 4416
    },
    {
        "loss": 2.7894,
        "grad_norm": 2.408637285232544,
        "learning_rate": 1.3798926200513374e-07,
        "epoch": 0.3291601460615545,
        "step": 4417
    },
    {
        "loss": 1.3033,
        "grad_norm": 3.9089415073394775,
        "learning_rate": 1.343186941201502e-07,
        "epoch": 0.3292346672628363,
        "step": 4418
    },
    {
        "loss": 2.1798,
        "grad_norm": 2.8574838638305664,
        "learning_rate": 1.3069757300396833e-07,
        "epoch": 0.32930918846411805,
        "step": 4419
    },
    {
        "loss": 2.3873,
        "grad_norm": 2.666454792022705,
        "learning_rate": 1.2712590044949845e-07,
        "epoch": 0.3293837096653998,
        "step": 4420
    },
    {
        "loss": 2.632,
        "grad_norm": 3.7946078777313232,
        "learning_rate": 1.2360367822521478e-07,
        "epoch": 0.3294582308666816,
        "step": 4421
    },
    {
        "loss": 2.4176,
        "grad_norm": 2.8921332359313965,
        "learning_rate": 1.2013090807506678e-07,
        "epoch": 0.32953275206796334,
        "step": 4422
    },
    {
        "loss": 2.5224,
        "grad_norm": 2.001312017440796,
        "learning_rate": 1.1670759171855672e-07,
        "epoch": 0.3296072732692451,
        "step": 4423
    },
    {
        "loss": 1.3484,
        "grad_norm": 1.0692812204360962,
        "learning_rate": 1.1333373085066213e-07,
        "epoch": 0.32968179447052687,
        "step": 4424
    },
    {
        "loss": 2.6587,
        "grad_norm": 2.5224430561065674,
        "learning_rate": 1.1000932714190226e-07,
        "epoch": 0.32975631567180863,
        "step": 4425
    },
    {
        "loss": 3.111,
        "grad_norm": 2.019021987915039,
        "learning_rate": 1.067343822382938e-07,
        "epoch": 0.3298308368730904,
        "step": 4426
    },
    {
        "loss": 2.4542,
        "grad_norm": 2.830251693725586,
        "learning_rate": 1.0350889776138406e-07,
        "epoch": 0.32990535807437216,
        "step": 4427
    },
    {
        "loss": 1.7451,
        "grad_norm": 3.8911211490631104,
        "learning_rate": 1.0033287530818447e-07,
        "epoch": 0.3299798792756539,
        "step": 4428
    },
    {
        "loss": 1.9524,
        "grad_norm": 3.053267240524292,
        "learning_rate": 9.720631645128154e-08,
        "epoch": 0.3300544004769357,
        "step": 4429
    },
    {
        "loss": 2.8418,
        "grad_norm": 3.61259126663208,
        "learning_rate": 9.412922273871471e-08,
        "epoch": 0.33012892167821745,
        "step": 4430
    },
    {
        "loss": 2.6457,
        "grad_norm": 1.8619616031646729,
        "learning_rate": 9.110159569406529e-08,
        "epoch": 0.3302034428794992,
        "step": 4431
    },
    {
        "loss": 2.3632,
        "grad_norm": 3.101825475692749,
        "learning_rate": 8.812343681640079e-08,
        "epoch": 0.330277964080781,
        "step": 4432
    },
    {
        "loss": 2.4014,
        "grad_norm": 3.7938477993011475,
        "learning_rate": 8.519474758030832e-08,
        "epoch": 0.33035248528206274,
        "step": 4433
    },
    {
        "loss": 3.0268,
        "grad_norm": 4.437655448913574,
        "learning_rate": 8.23155294358946e-08,
        "epoch": 0.3304270064833445,
        "step": 4434
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.9524600505828857,
        "learning_rate": 7.948578380873039e-08,
        "epoch": 0.33050152768462626,
        "step": 4435
    },
    {
        "loss": 2.4907,
        "grad_norm": 3.744741678237915,
        "learning_rate": 7.670551209992826e-08,
        "epoch": 0.330576048885908,
        "step": 4436
    },
    {
        "loss": 2.7402,
        "grad_norm": 2.491986036300659,
        "learning_rate": 7.397471568607595e-08,
        "epoch": 0.3306505700871898,
        "step": 4437
    },
    {
        "loss": 2.3647,
        "grad_norm": 3.0656895637512207,
        "learning_rate": 7.129339591931406e-08,
        "epoch": 0.33072509128847155,
        "step": 4438
    },
    {
        "loss": 2.3758,
        "grad_norm": 2.0579516887664795,
        "learning_rate": 6.866155412721398e-08,
        "epoch": 0.3307996124897533,
        "step": 4439
    },
    {
        "loss": 2.472,
        "grad_norm": 2.3445849418640137,
        "learning_rate": 6.60791916129222e-08,
        "epoch": 0.3308741336910351,
        "step": 4440
    },
    {
        "loss": 2.3216,
        "grad_norm": 2.0267040729522705,
        "learning_rate": 6.354630965501595e-08,
        "epoch": 0.33094865489231684,
        "step": 4441
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.7522966861724854,
        "learning_rate": 6.106290950762539e-08,
        "epoch": 0.3310231760935986,
        "step": 4442
    },
    {
        "loss": 2.8104,
        "grad_norm": 2.624147653579712,
        "learning_rate": 5.8628992400378e-08,
        "epoch": 0.33109769729488037,
        "step": 4443
    },
    {
        "loss": 1.8095,
        "grad_norm": 3.3550167083740234,
        "learning_rate": 5.624455953835428e-08,
        "epoch": 0.3311722184961622,
        "step": 4444
    },
    {
        "loss": 2.5051,
        "grad_norm": 2.497502326965332,
        "learning_rate": 5.390961210218759e-08,
        "epoch": 0.33124673969744395,
        "step": 4445
    },
    {
        "loss": 1.8512,
        "grad_norm": 3.0547170639038086,
        "learning_rate": 5.162415124797537e-08,
        "epoch": 0.3313212608987257,
        "step": 4446
    },
    {
        "loss": 1.9487,
        "grad_norm": 2.0143849849700928,
        "learning_rate": 4.938817810733465e-08,
        "epoch": 0.3313957821000075,
        "step": 4447
    },
    {
        "loss": 2.3727,
        "grad_norm": 3.214061737060547,
        "learning_rate": 4.7201693787357616e-08,
        "epoch": 0.33147030330128924,
        "step": 4448
    },
    {
        "loss": 2.159,
        "grad_norm": 2.1065146923065186,
        "learning_rate": 4.506469937065605e-08,
        "epoch": 0.331544824502571,
        "step": 4449
    },
    {
        "loss": 2.4149,
        "grad_norm": 2.580965995788574,
        "learning_rate": 4.2977195915316906e-08,
        "epoch": 0.33161934570385276,
        "step": 4450
    },
    {
        "loss": 1.6406,
        "grad_norm": 1.967617154121399,
        "learning_rate": 4.093918445493561e-08,
        "epoch": 0.3316938669051345,
        "step": 4451
    },
    {
        "loss": 2.0881,
        "grad_norm": 2.6732356548309326,
        "learning_rate": 3.8950665998593874e-08,
        "epoch": 0.3317683881064163,
        "step": 4452
    },
    {
        "loss": 2.8772,
        "grad_norm": 3.212259292602539,
        "learning_rate": 3.701164153088188e-08,
        "epoch": 0.33184290930769805,
        "step": 4453
    },
    {
        "loss": 2.373,
        "grad_norm": 1.7223299741744995,
        "learning_rate": 3.5122112011865e-08,
        "epoch": 0.3319174305089798,
        "step": 4454
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.626978635787964,
        "learning_rate": 3.328207837711705e-08,
        "epoch": 0.3319919517102616,
        "step": 4455
    },
    {
        "loss": 2.6992,
        "grad_norm": 1.7698496580123901,
        "learning_rate": 3.1491541537687076e-08,
        "epoch": 0.33206647291154334,
        "step": 4456
    },
    {
        "loss": 2.0606,
        "grad_norm": 2.6489226818084717,
        "learning_rate": 2.975050238014365e-08,
        "epoch": 0.3321409941128251,
        "step": 4457
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.9162324666976929,
        "learning_rate": 2.8058961766519452e-08,
        "epoch": 0.33221551531410687,
        "step": 4458
    },
    {
        "loss": 2.415,
        "grad_norm": 3.161926507949829,
        "learning_rate": 2.6416920534366728e-08,
        "epoch": 0.33229003651538863,
        "step": 4459
    },
    {
        "loss": 2.5207,
        "grad_norm": 4.002931118011475,
        "learning_rate": 2.48243794967018e-08,
        "epoch": 0.3323645577166704,
        "step": 4460
    },
    {
        "loss": 1.7065,
        "grad_norm": 2.561166286468506,
        "learning_rate": 2.3281339442049466e-08,
        "epoch": 0.33243907891795216,
        "step": 4461
    },
    {
        "loss": 2.488,
        "grad_norm": 2.3493072986602783,
        "learning_rate": 2.1787801134409702e-08,
        "epoch": 0.3325136001192339,
        "step": 4462
    },
    {
        "loss": 2.8527,
        "grad_norm": 2.0162503719329834,
        "learning_rate": 2.0343765313302066e-08,
        "epoch": 0.3325881213205157,
        "step": 4463
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.214749813079834,
        "learning_rate": 1.894923269368798e-08,
        "epoch": 0.33266264252179745,
        "step": 4464
    },
    {
        "loss": 1.7958,
        "grad_norm": 2.0435922145843506,
        "learning_rate": 1.7604203966070655e-08,
        "epoch": 0.3327371637230792,
        "step": 4465
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.6700339317321777,
        "learning_rate": 1.630867979641737e-08,
        "epoch": 0.332811684924361,
        "step": 4466
    },
    {
        "loss": 2.3777,
        "grad_norm": 2.4075510501861572,
        "learning_rate": 1.506266082615948e-08,
        "epoch": 0.33288620612564274,
        "step": 4467
    },
    {
        "loss": 2.1907,
        "grad_norm": 3.47615122795105,
        "learning_rate": 1.3866147672270124e-08,
        "epoch": 0.3329607273269245,
        "step": 4468
    },
    {
        "loss": 1.5621,
        "grad_norm": 2.7782065868377686,
        "learning_rate": 1.2719140927164308e-08,
        "epoch": 0.33303524852820626,
        "step": 4469
    },
    {
        "loss": 1.6155,
        "grad_norm": 2.491543769836426,
        "learning_rate": 1.162164115877662e-08,
        "epoch": 0.333109769729488,
        "step": 4470
    },
    {
        "loss": 2.6769,
        "grad_norm": 2.0741584300994873,
        "learning_rate": 1.0573648910494616e-08,
        "epoch": 0.3331842909307698,
        "step": 4471
    },
    {
        "loss": 2.2845,
        "grad_norm": 2.0018699169158936,
        "learning_rate": 9.575164701236538e-09,
        "epoch": 0.33325881213205155,
        "step": 4472
    },
    {
        "loss": 2.7715,
        "grad_norm": 2.176603317260742,
        "learning_rate": 8.6261890253736e-09,
        "epoch": 0.3333333333333333,
        "step": 4473
    },
    {
        "loss": 2.5078,
        "grad_norm": 2.919800043106079,
        "learning_rate": 7.726722352774386e-09,
        "epoch": 0.3334078545346151,
        "step": 4474
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.6384015083312988,
        "learning_rate": 6.876765128793761e-09,
        "epoch": 0.33348237573589684,
        "step": 4475
    },
    {
        "loss": 2.4861,
        "grad_norm": 2.363881826400757,
        "learning_rate": 6.0763177742839686e-09,
        "epoch": 0.3335568969371786,
        "step": 4476
    },
    {
        "loss": 2.7268,
        "grad_norm": 2.493293046951294,
        "learning_rate": 5.325380685550219e-09,
        "epoch": 0.33363141813846037,
        "step": 4477
    },
    {
        "loss": 2.5507,
        "grad_norm": 1.9609788656234741,
        "learning_rate": 4.623954234428407e-09,
        "epoch": 0.33370593933974213,
        "step": 4478
    },
    {
        "loss": 2.8916,
        "grad_norm": 2.8372342586517334,
        "learning_rate": 3.972038768207398e-09,
        "epoch": 0.33378046054102395,
        "step": 4479
    },
    {
        "loss": 2.6918,
        "grad_norm": 2.2677078247070312,
        "learning_rate": 3.3696346096734334e-09,
        "epoch": 0.3338549817423057,
        "step": 4480
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.9818503856658936,
        "learning_rate": 2.816742057087929e-09,
        "epoch": 0.3339295029435875,
        "step": 4481
    },
    {
        "loss": 2.4776,
        "grad_norm": 1.25377357006073,
        "learning_rate": 2.3133613842318824e-09,
        "epoch": 0.33400402414486924,
        "step": 4482
    },
    {
        "loss": 2.3209,
        "grad_norm": 2.346599578857422,
        "learning_rate": 1.8594928403170565e-09,
        "epoch": 0.334078545346151,
        "step": 4483
    },
    {
        "loss": 2.7952,
        "grad_norm": 1.8165227174758911,
        "learning_rate": 1.4551366500747953e-09,
        "epoch": 0.33415306654743276,
        "step": 4484
    },
    {
        "loss": 2.1868,
        "grad_norm": 3.2702414989471436,
        "learning_rate": 1.1002930137338218e-09,
        "epoch": 0.3342275877487145,
        "step": 4485
    },
    {
        "loss": 2.3857,
        "grad_norm": 3.0267252922058105,
        "learning_rate": 7.949621069647251e-10,
        "epoch": 0.3343021089499963,
        "step": 4486
    },
    {
        "loss": 2.8084,
        "grad_norm": 2.434504747390747,
        "learning_rate": 5.391440809576765e-10,
        "epoch": 0.33437663015127805,
        "step": 4487
    },
    {
        "loss": 1.3207,
        "grad_norm": 2.0000126361846924,
        "learning_rate": 3.328390623891231e-10,
        "epoch": 0.3344511513525598,
        "step": 4488
    },
    {
        "loss": 1.9494,
        "grad_norm": 3.6400976181030273,
        "learning_rate": 1.7604715337737888e-10,
        "epoch": 0.3345256725538416,
        "step": 4489
    },
    {
        "loss": 1.8401,
        "grad_norm": 3.515911102294922,
        "learning_rate": 6.876843158254432e-11,
        "epoch": 0.33460019375512334,
        "step": 4490
    },
    {
        "loss": 1.8657,
        "grad_norm": 3.062264919281006,
        "learning_rate": 1.1002950117688926e-11,
        "epoch": 0.3346747149564051,
        "step": 4491
    },
    {
        "loss": 2.9016,
        "grad_norm": 2.902514934539795,
        "learning_rate": 0.00019999999724926244,
        "epoch": 0.33474923615768687,
        "step": 4492
    },
    {
        "loss": 1.903,
        "grad_norm": 5.5487470626831055,
        "learning_rate": 0.00019999995598820198,
        "epoch": 0.33482375735896863,
        "step": 4493
    },
    {
        "loss": 1.7995,
        "grad_norm": 2.4875786304473877,
        "learning_rate": 0.00019999986521388894,
        "epoch": 0.3348982785602504,
        "step": 4494
    },
    {
        "loss": 2.2179,
        "grad_norm": 2.6527152061462402,
        "learning_rate": 0.00019999972492636825,
        "epoch": 0.33497279976153216,
        "step": 4495
    },
    {
        "loss": 2.6282,
        "grad_norm": 2.58316707611084,
        "learning_rate": 0.0001999995351257094,
        "epoch": 0.3350473209628139,
        "step": 4496
    },
    {
        "loss": 2.694,
        "grad_norm": 2.9555249214172363,
        "learning_rate": 0.00019999929581200634,
        "epoch": 0.3351218421640957,
        "step": 4497
    },
    {
        "loss": 2.5248,
        "grad_norm": 2.146115303039551,
        "learning_rate": 0.00019999900698537758,
        "epoch": 0.33519636336537745,
        "step": 4498
    },
    {
        "loss": 2.2371,
        "grad_norm": 3.292423725128174,
        "learning_rate": 0.0001999986686459661,
        "epoch": 0.3352708845666592,
        "step": 4499
    },
    {
        "loss": 2.6356,
        "grad_norm": 2.3856775760650635,
        "learning_rate": 0.00019999828079393948,
        "epoch": 0.335345405767941,
        "step": 4500
    },
    {
        "loss": 2.3561,
        "grad_norm": 3.0750062465667725,
        "learning_rate": 0.00019999784342948968,
        "epoch": 0.33541992696922274,
        "step": 4501
    },
    {
        "loss": 2.4136,
        "grad_norm": 3.886064052581787,
        "learning_rate": 0.0001999973565528333,
        "epoch": 0.3354944481705045,
        "step": 4502
    },
    {
        "loss": 2.4587,
        "grad_norm": 3.0268332958221436,
        "learning_rate": 0.00019999682016421145,
        "epoch": 0.33556896937178626,
        "step": 4503
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.5336973667144775,
        "learning_rate": 0.00019999623426388962,
        "epoch": 0.335643490573068,
        "step": 4504
    },
    {
        "loss": 2.4285,
        "grad_norm": 2.339158535003662,
        "learning_rate": 0.00019999559885215798,
        "epoch": 0.3357180117743498,
        "step": 4505
    },
    {
        "loss": 2.1209,
        "grad_norm": 3.13393497467041,
        "learning_rate": 0.0001999949139293311,
        "epoch": 0.33579253297563155,
        "step": 4506
    },
    {
        "loss": 2.0789,
        "grad_norm": 2.6923015117645264,
        "learning_rate": 0.00019999417949574815,
        "epoch": 0.3358670541769133,
        "step": 4507
    },
    {
        "loss": 2.3923,
        "grad_norm": 2.860208749771118,
        "learning_rate": 0.0001999933955517727,
        "epoch": 0.3359415753781951,
        "step": 4508
    },
    {
        "loss": 2.9743,
        "grad_norm": 3.539665699005127,
        "learning_rate": 0.00019999256209779303,
        "epoch": 0.33601609657947684,
        "step": 4509
    },
    {
        "loss": 2.2987,
        "grad_norm": 2.2815613746643066,
        "learning_rate": 0.0001999916791342217,
        "epoch": 0.3360906177807586,
        "step": 4510
    },
    {
        "loss": 2.377,
        "grad_norm": 2.687788248062134,
        "learning_rate": 0.0001999907466614959,
        "epoch": 0.33616513898204037,
        "step": 4511
    },
    {
        "loss": 1.5963,
        "grad_norm": 2.2058169841766357,
        "learning_rate": 0.00019998976468007742,
        "epoch": 0.33623966018332213,
        "step": 4512
    },
    {
        "loss": 2.682,
        "grad_norm": 2.9703023433685303,
        "learning_rate": 0.00019998873319045237,
        "epoch": 0.3363141813846039,
        "step": 4513
    },
    {
        "loss": 2.4457,
        "grad_norm": 2.280186414718628,
        "learning_rate": 0.00019998765219313153,
        "epoch": 0.33638870258588566,
        "step": 4514
    },
    {
        "loss": 2.6365,
        "grad_norm": 2.881018877029419,
        "learning_rate": 0.00019998652168865008,
        "epoch": 0.3364632237871675,
        "step": 4515
    },
    {
        "loss": 1.8009,
        "grad_norm": 3.4015228748321533,
        "learning_rate": 0.00019998534167756785,
        "epoch": 0.33653774498844924,
        "step": 4516
    },
    {
        "loss": 2.4454,
        "grad_norm": 2.263660430908203,
        "learning_rate": 0.00019998411216046904,
        "epoch": 0.336612266189731,
        "step": 4517
    },
    {
        "loss": 1.8049,
        "grad_norm": 2.9696245193481445,
        "learning_rate": 0.00019998283313796245,
        "epoch": 0.33668678739101277,
        "step": 4518
    },
    {
        "loss": 2.4742,
        "grad_norm": 2.5064854621887207,
        "learning_rate": 0.00019998150461068135,
        "epoch": 0.33676130859229453,
        "step": 4519
    },
    {
        "loss": 2.5358,
        "grad_norm": 2.044588804244995,
        "learning_rate": 0.0001999801265792836,
        "epoch": 0.3368358297935763,
        "step": 4520
    },
    {
        "loss": 2.3479,
        "grad_norm": 2.64084792137146,
        "learning_rate": 0.00019997869904445138,
        "epoch": 0.33691035099485805,
        "step": 4521
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.955279588699341,
        "learning_rate": 0.00019997722200689163,
        "epoch": 0.3369848721961398,
        "step": 4522
    },
    {
        "loss": 3.1648,
        "grad_norm": 3.7780282497406006,
        "learning_rate": 0.00019997569546733565,
        "epoch": 0.3370593933974216,
        "step": 4523
    },
    {
        "loss": 2.186,
        "grad_norm": 3.1977803707122803,
        "learning_rate": 0.00019997411942653925,
        "epoch": 0.33713391459870334,
        "step": 4524
    },
    {
        "loss": 2.6904,
        "grad_norm": 3.2767229080200195,
        "learning_rate": 0.0001999724938852828,
        "epoch": 0.3372084357999851,
        "step": 4525
    },
    {
        "loss": 2.148,
        "grad_norm": 3.998683214187622,
        "learning_rate": 0.00019997081884437113,
        "epoch": 0.33728295700126687,
        "step": 4526
    },
    {
        "loss": 2.6285,
        "grad_norm": 2.1352531909942627,
        "learning_rate": 0.0001999690943046337,
        "epoch": 0.33735747820254863,
        "step": 4527
    },
    {
        "loss": 1.9785,
        "grad_norm": 3.493520736694336,
        "learning_rate": 0.00019996732026692423,
        "epoch": 0.3374319994038304,
        "step": 4528
    },
    {
        "loss": 1.7926,
        "grad_norm": 4.302706241607666,
        "learning_rate": 0.00019996549673212126,
        "epoch": 0.33750652060511216,
        "step": 4529
    },
    {
        "loss": 2.6705,
        "grad_norm": 2.499216318130493,
        "learning_rate": 0.00019996362370112758,
        "epoch": 0.3375810418063939,
        "step": 4530
    },
    {
        "loss": 2.2587,
        "grad_norm": 3.121147871017456,
        "learning_rate": 0.00019996170117487063,
        "epoch": 0.3376555630076757,
        "step": 4531
    },
    {
        "loss": 2.5992,
        "grad_norm": 3.3474762439727783,
        "learning_rate": 0.00019995972915430233,
        "epoch": 0.33773008420895745,
        "step": 4532
    },
    {
        "loss": 2.8375,
        "grad_norm": 2.993973970413208,
        "learning_rate": 0.00019995770764039903,
        "epoch": 0.3378046054102392,
        "step": 4533
    },
    {
        "loss": 1.5535,
        "grad_norm": 2.5085535049438477,
        "learning_rate": 0.00019995563663416174,
        "epoch": 0.337879126611521,
        "step": 4534
    },
    {
        "loss": 2.8163,
        "grad_norm": 2.635652780532837,
        "learning_rate": 0.00019995351613661576,
        "epoch": 0.33795364781280274,
        "step": 4535
    },
    {
        "loss": 2.9312,
        "grad_norm": 3.125436305999756,
        "learning_rate": 0.00019995134614881114,
        "epoch": 0.3380281690140845,
        "step": 4536
    },
    {
        "loss": 2.061,
        "grad_norm": 3.605541467666626,
        "learning_rate": 0.00019994912667182228,
        "epoch": 0.33810269021536626,
        "step": 4537
    },
    {
        "loss": 2.1855,
        "grad_norm": 2.6871066093444824,
        "learning_rate": 0.00019994685770674805,
        "epoch": 0.338177211416648,
        "step": 4538
    },
    {
        "loss": 2.244,
        "grad_norm": 3.4073753356933594,
        "learning_rate": 0.00019994453925471195,
        "epoch": 0.3382517326179298,
        "step": 4539
    },
    {
        "loss": 2.464,
        "grad_norm": 2.9750373363494873,
        "learning_rate": 0.00019994217131686192,
        "epoch": 0.33832625381921155,
        "step": 4540
    },
    {
        "loss": 1.8134,
        "grad_norm": 5.041102886199951,
        "learning_rate": 0.00019993975389437038,
        "epoch": 0.3384007750204933,
        "step": 4541
    },
    {
        "loss": 2.4759,
        "grad_norm": 3.0163040161132812,
        "learning_rate": 0.0001999372869884343,
        "epoch": 0.3384752962217751,
        "step": 4542
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.6837196350097656,
        "learning_rate": 0.0001999347706002751,
        "epoch": 0.33854981742305684,
        "step": 4543
    },
    {
        "loss": 2.3331,
        "grad_norm": 2.5457072257995605,
        "learning_rate": 0.00019993220473113875,
        "epoch": 0.3386243386243386,
        "step": 4544
    },
    {
        "loss": 2.6102,
        "grad_norm": 3.1662168502807617,
        "learning_rate": 0.00019992958938229566,
        "epoch": 0.33869885982562037,
        "step": 4545
    },
    {
        "loss": 2.2,
        "grad_norm": 2.5234580039978027,
        "learning_rate": 0.0001999269245550408,
        "epoch": 0.33877338102690213,
        "step": 4546
    },
    {
        "loss": 1.9235,
        "grad_norm": 5.850022315979004,
        "learning_rate": 0.00019992421025069365,
        "epoch": 0.3388479022281839,
        "step": 4547
    },
    {
        "loss": 3.1995,
        "grad_norm": 2.0795416831970215,
        "learning_rate": 0.00019992144647059808,
        "epoch": 0.33892242342946566,
        "step": 4548
    },
    {
        "loss": 1.6096,
        "grad_norm": 4.262163162231445,
        "learning_rate": 0.00019991863321612257,
        "epoch": 0.3389969446307474,
        "step": 4549
    },
    {
        "loss": 2.216,
        "grad_norm": 2.576763153076172,
        "learning_rate": 0.00019991577048866004,
        "epoch": 0.33907146583202924,
        "step": 4550
    },
    {
        "loss": 2.9039,
        "grad_norm": 2.367140054702759,
        "learning_rate": 0.0001999128582896279,
        "epoch": 0.339145987033311,
        "step": 4551
    },
    {
        "loss": 2.8017,
        "grad_norm": 2.459472894668579,
        "learning_rate": 0.00019990989662046815,
        "epoch": 0.33922050823459277,
        "step": 4552
    },
    {
        "loss": 2.3127,
        "grad_norm": 2.9638853073120117,
        "learning_rate": 0.00019990688548264713,
        "epoch": 0.33929502943587453,
        "step": 4553
    },
    {
        "loss": 2.2222,
        "grad_norm": 2.516465425491333,
        "learning_rate": 0.00019990382487765582,
        "epoch": 0.3393695506371563,
        "step": 4554
    },
    {
        "loss": 2.3918,
        "grad_norm": 3.091522455215454,
        "learning_rate": 0.00019990071480700957,
        "epoch": 0.33944407183843806,
        "step": 4555
    },
    {
        "loss": 2.8542,
        "grad_norm": 2.825204849243164,
        "learning_rate": 0.00019989755527224828,
        "epoch": 0.3395185930397198,
        "step": 4556
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.5756049156188965,
        "learning_rate": 0.00019989434627493634,
        "epoch": 0.3395931142410016,
        "step": 4557
    },
    {
        "loss": 3.2464,
        "grad_norm": 4.1077094078063965,
        "learning_rate": 0.0001998910878166627,
        "epoch": 0.33966763544228334,
        "step": 4558
    },
    {
        "loss": 2.9272,
        "grad_norm": 2.332148551940918,
        "learning_rate": 0.0001998877798990406,
        "epoch": 0.3397421566435651,
        "step": 4559
    },
    {
        "loss": 2.5597,
        "grad_norm": 2.5322468280792236,
        "learning_rate": 0.00019988442252370805,
        "epoch": 0.33981667784484687,
        "step": 4560
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.787452220916748,
        "learning_rate": 0.0001998810156923273,
        "epoch": 0.33989119904612863,
        "step": 4561
    },
    {
        "loss": 1.6756,
        "grad_norm": 2.9477672576904297,
        "learning_rate": 0.00019987755940658518,
        "epoch": 0.3399657202474104,
        "step": 4562
    },
    {
        "loss": 2.4971,
        "grad_norm": 2.1024487018585205,
        "learning_rate": 0.00019987405366819302,
        "epoch": 0.34004024144869216,
        "step": 4563
    },
    {
        "loss": 2.5607,
        "grad_norm": 3.31119966506958,
        "learning_rate": 0.00019987049847888666,
        "epoch": 0.3401147626499739,
        "step": 4564
    },
    {
        "loss": 2.5353,
        "grad_norm": 2.493942975997925,
        "learning_rate": 0.00019986689384042638,
        "epoch": 0.3401892838512557,
        "step": 4565
    },
    {
        "loss": 2.9105,
        "grad_norm": 3.1902527809143066,
        "learning_rate": 0.00019986323975459692,
        "epoch": 0.34026380505253745,
        "step": 4566
    },
    {
        "loss": 2.2517,
        "grad_norm": 3.3360133171081543,
        "learning_rate": 0.00019985953622320759,
        "epoch": 0.3403383262538192,
        "step": 4567
    },
    {
        "loss": 2.6989,
        "grad_norm": 3.367750406265259,
        "learning_rate": 0.00019985578324809212,
        "epoch": 0.340412847455101,
        "step": 4568
    },
    {
        "loss": 2.5958,
        "grad_norm": 3.247809648513794,
        "learning_rate": 0.00019985198083110866,
        "epoch": 0.34048736865638274,
        "step": 4569
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.990591287612915,
        "learning_rate": 0.00019984812897414,
        "epoch": 0.3405618898576645,
        "step": 4570
    },
    {
        "loss": 2.0017,
        "grad_norm": 2.443925380706787,
        "learning_rate": 0.00019984422767909326,
        "epoch": 0.34063641105894626,
        "step": 4571
    },
    {
        "loss": 2.0008,
        "grad_norm": 4.009923458099365,
        "learning_rate": 0.00019984027694790012,
        "epoch": 0.340710932260228,
        "step": 4572
    },
    {
        "loss": 2.4829,
        "grad_norm": 3.400853157043457,
        "learning_rate": 0.00019983627678251672,
        "epoch": 0.3407854534615098,
        "step": 4573
    },
    {
        "loss": 2.2644,
        "grad_norm": 4.41611909866333,
        "learning_rate": 0.0001998322271849237,
        "epoch": 0.34085997466279155,
        "step": 4574
    },
    {
        "loss": 2.3795,
        "grad_norm": 6.088033199310303,
        "learning_rate": 0.0001998281281571261,
        "epoch": 0.3409344958640733,
        "step": 4575
    },
    {
        "loss": 2.3166,
        "grad_norm": 2.850370407104492,
        "learning_rate": 0.00019982397970115352,
        "epoch": 0.3410090170653551,
        "step": 4576
    },
    {
        "loss": 2.3233,
        "grad_norm": 3.399608612060547,
        "learning_rate": 0.00019981978181905996,
        "epoch": 0.34108353826663684,
        "step": 4577
    },
    {
        "loss": 2.2324,
        "grad_norm": 2.8464837074279785,
        "learning_rate": 0.00019981553451292396,
        "epoch": 0.3411580594679186,
        "step": 4578
    },
    {
        "loss": 2.2905,
        "grad_norm": 2.976909637451172,
        "learning_rate": 0.0001998112377848485,
        "epoch": 0.34123258066920037,
        "step": 4579
    },
    {
        "loss": 2.1968,
        "grad_norm": 2.849118709564209,
        "learning_rate": 0.000199806891636961,
        "epoch": 0.34130710187048213,
        "step": 4580
    },
    {
        "loss": 2.6859,
        "grad_norm": 2.0146429538726807,
        "learning_rate": 0.00019980249607141342,
        "epoch": 0.3413816230717639,
        "step": 4581
    },
    {
        "loss": 2.1426,
        "grad_norm": 2.7423338890075684,
        "learning_rate": 0.0001997980510903821,
        "epoch": 0.34145614427304566,
        "step": 4582
    },
    {
        "loss": 2.187,
        "grad_norm": 3.496668577194214,
        "learning_rate": 0.00019979355669606794,
        "epoch": 0.3415306654743274,
        "step": 4583
    },
    {
        "loss": 2.3389,
        "grad_norm": 3.1095852851867676,
        "learning_rate": 0.00019978901289069624,
        "epoch": 0.3416051866756092,
        "step": 4584
    },
    {
        "loss": 1.1698,
        "grad_norm": 3.7161686420440674,
        "learning_rate": 0.0001997844196765168,
        "epoch": 0.34167970787689095,
        "step": 4585
    },
    {
        "loss": 2.3053,
        "grad_norm": 3.187260389328003,
        "learning_rate": 0.00019977977705580387,
        "epoch": 0.34175422907817277,
        "step": 4586
    },
    {
        "loss": 2.966,
        "grad_norm": 3.714893341064453,
        "learning_rate": 0.00019977508503085615,
        "epoch": 0.34182875027945453,
        "step": 4587
    },
    {
        "loss": 2.7081,
        "grad_norm": 2.781890869140625,
        "learning_rate": 0.0001997703436039968,
        "epoch": 0.3419032714807363,
        "step": 4588
    },
    {
        "loss": 2.4141,
        "grad_norm": 2.6399714946746826,
        "learning_rate": 0.00019976555277757352,
        "epoch": 0.34197779268201806,
        "step": 4589
    },
    {
        "loss": 2.7666,
        "grad_norm": 1.6512850522994995,
        "learning_rate": 0.00019976071255395833,
        "epoch": 0.3420523138832998,
        "step": 4590
    },
    {
        "loss": 2.5969,
        "grad_norm": 2.9230470657348633,
        "learning_rate": 0.00019975582293554783,
        "epoch": 0.3421268350845816,
        "step": 4591
    },
    {
        "loss": 2.842,
        "grad_norm": 3.3810548782348633,
        "learning_rate": 0.00019975088392476303,
        "epoch": 0.34220135628586335,
        "step": 4592
    },
    {
        "loss": 2.918,
        "grad_norm": 3.2668793201446533,
        "learning_rate": 0.00019974589552404937,
        "epoch": 0.3422758774871451,
        "step": 4593
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.9389153718948364,
        "learning_rate": 0.00019974085773587677,
        "epoch": 0.34235039868842687,
        "step": 4594
    },
    {
        "loss": 2.5985,
        "grad_norm": 2.225705623626709,
        "learning_rate": 0.00019973577056273963,
        "epoch": 0.34242491988970863,
        "step": 4595
    },
    {
        "loss": 2.3535,
        "grad_norm": 3.5818235874176025,
        "learning_rate": 0.00019973063400715674,
        "epoch": 0.3424994410909904,
        "step": 4596
    },
    {
        "loss": 2.0038,
        "grad_norm": 3.5282533168792725,
        "learning_rate": 0.00019972544807167144,
        "epoch": 0.34257396229227216,
        "step": 4597
    },
    {
        "loss": 2.7467,
        "grad_norm": 2.658862352371216,
        "learning_rate": 0.0001997202127588514,
        "epoch": 0.3426484834935539,
        "step": 4598
    },
    {
        "loss": 2.6107,
        "grad_norm": 2.648524761199951,
        "learning_rate": 0.0001997149280712888,
        "epoch": 0.3427230046948357,
        "step": 4599
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.3555908203125,
        "learning_rate": 0.00019970959401160028,
        "epoch": 0.34279752589611745,
        "step": 4600
    },
    {
        "loss": 2.6184,
        "grad_norm": 2.120206594467163,
        "learning_rate": 0.0001997042105824269,
        "epoch": 0.3428720470973992,
        "step": 4601
    },
    {
        "loss": 2.1302,
        "grad_norm": 3.44465708732605,
        "learning_rate": 0.0001996987777864342,
        "epoch": 0.342946568298681,
        "step": 4602
    },
    {
        "loss": 2.3213,
        "grad_norm": 2.8623697757720947,
        "learning_rate": 0.00019969329562631206,
        "epoch": 0.34302108949996274,
        "step": 4603
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.7166950702667236,
        "learning_rate": 0.0001996877641047749,
        "epoch": 0.3430956107012445,
        "step": 4604
    },
    {
        "loss": 2.1566,
        "grad_norm": 3.053257942199707,
        "learning_rate": 0.00019968218322456168,
        "epoch": 0.34317013190252627,
        "step": 4605
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.929159164428711,
        "learning_rate": 0.0001996765529884355,
        "epoch": 0.34324465310380803,
        "step": 4606
    },
    {
        "loss": 1.7914,
        "grad_norm": 3.2658324241638184,
        "learning_rate": 0.00019967087339918418,
        "epoch": 0.3433191743050898,
        "step": 4607
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.594149589538574,
        "learning_rate": 0.00019966514445961982,
        "epoch": 0.34339369550637155,
        "step": 4608
    },
    {
        "loss": 2.3767,
        "grad_norm": 3.022683620452881,
        "learning_rate": 0.00019965936617257905,
        "epoch": 0.3434682167076533,
        "step": 4609
    },
    {
        "loss": 2.4567,
        "grad_norm": 2.320462703704834,
        "learning_rate": 0.00019965353854092287,
        "epoch": 0.3435427379089351,
        "step": 4610
    },
    {
        "loss": 2.4637,
        "grad_norm": 3.6336402893066406,
        "learning_rate": 0.00019964766156753674,
        "epoch": 0.34361725911021684,
        "step": 4611
    },
    {
        "loss": 1.7945,
        "grad_norm": 3.503364324569702,
        "learning_rate": 0.0001996417352553305,
        "epoch": 0.3436917803114986,
        "step": 4612
    },
    {
        "loss": 2.0299,
        "grad_norm": 3.640235662460327,
        "learning_rate": 0.0001996357596072385,
        "epoch": 0.34376630151278037,
        "step": 4613
    },
    {
        "loss": 1.9271,
        "grad_norm": 3.010550022125244,
        "learning_rate": 0.0001996297346262195,
        "epoch": 0.34384082271406213,
        "step": 4614
    },
    {
        "loss": 2.3892,
        "grad_norm": 4.142738342285156,
        "learning_rate": 0.00019962366031525664,
        "epoch": 0.3439153439153439,
        "step": 4615
    },
    {
        "loss": 2.5678,
        "grad_norm": 3.1147427558898926,
        "learning_rate": 0.00019961753667735747,
        "epoch": 0.34398986511662566,
        "step": 4616
    },
    {
        "loss": 2.1761,
        "grad_norm": 3.0362563133239746,
        "learning_rate": 0.00019961136371555408,
        "epoch": 0.3440643863179074,
        "step": 4617
    },
    {
        "loss": 2.1739,
        "grad_norm": 5.658728122711182,
        "learning_rate": 0.00019960514143290282,
        "epoch": 0.3441389075191892,
        "step": 4618
    },
    {
        "loss": 2.0886,
        "grad_norm": 3.8680906295776367,
        "learning_rate": 0.00019959886983248463,
        "epoch": 0.34421342872047095,
        "step": 4619
    },
    {
        "loss": 2.1516,
        "grad_norm": 2.9489598274230957,
        "learning_rate": 0.0001995925489174047,
        "epoch": 0.3442879499217527,
        "step": 4620
    },
    {
        "loss": 2.3451,
        "grad_norm": 3.5085136890411377,
        "learning_rate": 0.00019958617869079282,
        "epoch": 0.34436247112303453,
        "step": 4621
    },
    {
        "loss": 1.9012,
        "grad_norm": 3.7778842449188232,
        "learning_rate": 0.00019957975915580302,
        "epoch": 0.3444369923243163,
        "step": 4622
    },
    {
        "loss": 1.9973,
        "grad_norm": 3.45143723487854,
        "learning_rate": 0.00019957329031561384,
        "epoch": 0.34451151352559806,
        "step": 4623
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.509197235107422,
        "learning_rate": 0.00019956677217342826,
        "epoch": 0.3445860347268798,
        "step": 4624
    },
    {
        "loss": 2.8843,
        "grad_norm": 3.2914490699768066,
        "learning_rate": 0.00019956020473247356,
        "epoch": 0.3446605559281616,
        "step": 4625
    },
    {
        "loss": 2.6843,
        "grad_norm": 2.682619094848633,
        "learning_rate": 0.00019955358799600154,
        "epoch": 0.34473507712944335,
        "step": 4626
    },
    {
        "loss": 2.0874,
        "grad_norm": 2.7003989219665527,
        "learning_rate": 0.0001995469219672883,
        "epoch": 0.3448095983307251,
        "step": 4627
    },
    {
        "loss": 2.8748,
        "grad_norm": 1.834065318107605,
        "learning_rate": 0.00019954020664963453,
        "epoch": 0.34488411953200687,
        "step": 4628
    },
    {
        "loss": 2.6807,
        "grad_norm": 2.300750255584717,
        "learning_rate": 0.0001995334420463651,
        "epoch": 0.34495864073328864,
        "step": 4629
    },
    {
        "loss": 2.0534,
        "grad_norm": 3.9707980155944824,
        "learning_rate": 0.0001995266281608294,
        "epoch": 0.3450331619345704,
        "step": 4630
    },
    {
        "loss": 3.7482,
        "grad_norm": 3.2593233585357666,
        "learning_rate": 0.00019951976499640124,
        "epoch": 0.34510768313585216,
        "step": 4631
    },
    {
        "loss": 2.2864,
        "grad_norm": 3.48289155960083,
        "learning_rate": 0.00019951285255647875,
        "epoch": 0.3451822043371339,
        "step": 4632
    },
    {
        "loss": 2.1086,
        "grad_norm": 3.8929221630096436,
        "learning_rate": 0.0001995058908444846,
        "epoch": 0.3452567255384157,
        "step": 4633
    },
    {
        "loss": 2.7813,
        "grad_norm": 3.352797269821167,
        "learning_rate": 0.00019949887986386565,
        "epoch": 0.34533124673969745,
        "step": 4634
    },
    {
        "loss": 1.8611,
        "grad_norm": 2.310037612915039,
        "learning_rate": 0.0001994918196180933,
        "epoch": 0.3454057679409792,
        "step": 4635
    },
    {
        "loss": 2.8305,
        "grad_norm": 3.1722793579101562,
        "learning_rate": 0.00019948471011066338,
        "epoch": 0.345480289142261,
        "step": 4636
    },
    {
        "loss": 2.6554,
        "grad_norm": 2.2603940963745117,
        "learning_rate": 0.00019947755134509595,
        "epoch": 0.34555481034354274,
        "step": 4637
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.0991597175598145,
        "learning_rate": 0.00019947034332493558,
        "epoch": 0.3456293315448245,
        "step": 4638
    },
    {
        "loss": 2.4196,
        "grad_norm": 2.631894826889038,
        "learning_rate": 0.00019946308605375119,
        "epoch": 0.34570385274610627,
        "step": 4639
    },
    {
        "loss": 2.0447,
        "grad_norm": 3.0818259716033936,
        "learning_rate": 0.00019945577953513612,
        "epoch": 0.34577837394738803,
        "step": 4640
    },
    {
        "loss": 2.3478,
        "grad_norm": 2.9482064247131348,
        "learning_rate": 0.00019944842377270802,
        "epoch": 0.3458528951486698,
        "step": 4641
    },
    {
        "loss": 2.6135,
        "grad_norm": 2.4520034790039062,
        "learning_rate": 0.00019944101877010905,
        "epoch": 0.34592741634995156,
        "step": 4642
    },
    {
        "loss": 2.0325,
        "grad_norm": 4.119709014892578,
        "learning_rate": 0.00019943356453100557,
        "epoch": 0.3460019375512333,
        "step": 4643
    },
    {
        "loss": 2.7323,
        "grad_norm": 2.1397628784179688,
        "learning_rate": 0.00019942606105908847,
        "epoch": 0.3460764587525151,
        "step": 4644
    },
    {
        "loss": 2.8614,
        "grad_norm": 3.4750444889068604,
        "learning_rate": 0.00019941850835807298,
        "epoch": 0.34615097995379684,
        "step": 4645
    },
    {
        "loss": 2.7082,
        "grad_norm": 2.8420417308807373,
        "learning_rate": 0.00019941090643169865,
        "epoch": 0.3462255011550786,
        "step": 4646
    },
    {
        "loss": 2.4851,
        "grad_norm": 2.580826997756958,
        "learning_rate": 0.0001994032552837295,
        "epoch": 0.34630002235636037,
        "step": 4647
    },
    {
        "loss": 2.6171,
        "grad_norm": 3.0797176361083984,
        "learning_rate": 0.0001993955549179538,
        "epoch": 0.34637454355764213,
        "step": 4648
    },
    {
        "loss": 1.673,
        "grad_norm": 2.9903674125671387,
        "learning_rate": 0.00019938780533818428,
        "epoch": 0.3464490647589239,
        "step": 4649
    },
    {
        "loss": 1.7427,
        "grad_norm": 3.309281826019287,
        "learning_rate": 0.00019938000654825798,
        "epoch": 0.34652358596020566,
        "step": 4650
    },
    {
        "loss": 1.6007,
        "grad_norm": 3.4452052116394043,
        "learning_rate": 0.00019937215855203637,
        "epoch": 0.3465981071614874,
        "step": 4651
    },
    {
        "loss": 2.9177,
        "grad_norm": 2.112926721572876,
        "learning_rate": 0.00019936426135340528,
        "epoch": 0.3466726283627692,
        "step": 4652
    },
    {
        "loss": 2.5481,
        "grad_norm": 1.9188525676727295,
        "learning_rate": 0.0001993563149562748,
        "epoch": 0.34674714956405095,
        "step": 4653
    },
    {
        "loss": 2.7699,
        "grad_norm": 2.8793768882751465,
        "learning_rate": 0.00019934831936457953,
        "epoch": 0.3468216707653327,
        "step": 4654
    },
    {
        "loss": 2.5892,
        "grad_norm": 1.6506435871124268,
        "learning_rate": 0.00019934027458227827,
        "epoch": 0.3468961919666145,
        "step": 4655
    },
    {
        "loss": 2.4244,
        "grad_norm": 2.836238384246826,
        "learning_rate": 0.00019933218061335433,
        "epoch": 0.3469707131678963,
        "step": 4656
    },
    {
        "loss": 2.3427,
        "grad_norm": 3.458221435546875,
        "learning_rate": 0.00019932403746181522,
        "epoch": 0.34704523436917806,
        "step": 4657
    },
    {
        "loss": 2.839,
        "grad_norm": 2.778865337371826,
        "learning_rate": 0.00019931584513169294,
        "epoch": 0.3471197555704598,
        "step": 4658
    },
    {
        "loss": 2.0761,
        "grad_norm": 3.725267171859741,
        "learning_rate": 0.00019930760362704377,
        "epoch": 0.3471942767717416,
        "step": 4659
    },
    {
        "loss": 2.8111,
        "grad_norm": 2.300485372543335,
        "learning_rate": 0.00019929931295194835,
        "epoch": 0.34726879797302335,
        "step": 4660
    },
    {
        "loss": 1.6894,
        "grad_norm": 2.527512788772583,
        "learning_rate": 0.00019929097311051166,
        "epoch": 0.3473433191743051,
        "step": 4661
    },
    {
        "loss": 2.7313,
        "grad_norm": 1.9936282634735107,
        "learning_rate": 0.00019928258410686298,
        "epoch": 0.3474178403755869,
        "step": 4662
    },
    {
        "loss": 2.5721,
        "grad_norm": 1.7924658060073853,
        "learning_rate": 0.00019927414594515604,
        "epoch": 0.34749236157686864,
        "step": 4663
    },
    {
        "loss": 2.1406,
        "grad_norm": 3.6006786823272705,
        "learning_rate": 0.00019926565862956886,
        "epoch": 0.3475668827781504,
        "step": 4664
    },
    {
        "loss": 2.4721,
        "grad_norm": 3.1147868633270264,
        "learning_rate": 0.00019925712216430379,
        "epoch": 0.34764140397943216,
        "step": 4665
    },
    {
        "loss": 2.1343,
        "grad_norm": 3.016312599182129,
        "learning_rate": 0.00019924853655358744,
        "epoch": 0.3477159251807139,
        "step": 4666
    },
    {
        "loss": 2.3233,
        "grad_norm": 2.9736194610595703,
        "learning_rate": 0.0001992399018016709,
        "epoch": 0.3477904463819957,
        "step": 4667
    },
    {
        "loss": 1.8667,
        "grad_norm": 4.458350658416748,
        "learning_rate": 0.00019923121791282948,
        "epoch": 0.34786496758327745,
        "step": 4668
    },
    {
        "loss": 2.7641,
        "grad_norm": 1.7586841583251953,
        "learning_rate": 0.00019922248489136289,
        "epoch": 0.3479394887845592,
        "step": 4669
    },
    {
        "loss": 1.8044,
        "grad_norm": 1.6597925424575806,
        "learning_rate": 0.0001992137027415951,
        "epoch": 0.348014009985841,
        "step": 4670
    },
    {
        "loss": 2.5435,
        "grad_norm": 1.8892862796783447,
        "learning_rate": 0.00019920487146787443,
        "epoch": 0.34808853118712274,
        "step": 4671
    },
    {
        "loss": 2.6265,
        "grad_norm": 2.9726264476776123,
        "learning_rate": 0.00019919599107457358,
        "epoch": 0.3481630523884045,
        "step": 4672
    },
    {
        "loss": 1.6862,
        "grad_norm": 3.833235502243042,
        "learning_rate": 0.00019918706156608952,
        "epoch": 0.34823757358968627,
        "step": 4673
    },
    {
        "loss": 2.6095,
        "grad_norm": 2.4810330867767334,
        "learning_rate": 0.00019917808294684348,
        "epoch": 0.34831209479096803,
        "step": 4674
    },
    {
        "loss": 2.6333,
        "grad_norm": 3.145280361175537,
        "learning_rate": 0.00019916905522128118,
        "epoch": 0.3483866159922498,
        "step": 4675
    },
    {
        "loss": 2.1458,
        "grad_norm": 3.495032548904419,
        "learning_rate": 0.00019915997839387243,
        "epoch": 0.34846113719353156,
        "step": 4676
    },
    {
        "loss": 2.2496,
        "grad_norm": 2.0689034461975098,
        "learning_rate": 0.00019915085246911148,
        "epoch": 0.3485356583948133,
        "step": 4677
    },
    {
        "loss": 3.0597,
        "grad_norm": 2.0684728622436523,
        "learning_rate": 0.00019914167745151696,
        "epoch": 0.3486101795960951,
        "step": 4678
    },
    {
        "loss": 2.9574,
        "grad_norm": 2.3336493968963623,
        "learning_rate": 0.00019913245334563165,
        "epoch": 0.34868470079737685,
        "step": 4679
    },
    {
        "loss": 3.0935,
        "grad_norm": 1.8286890983581543,
        "learning_rate": 0.00019912318015602272,
        "epoch": 0.3487592219986586,
        "step": 4680
    },
    {
        "loss": 2.9595,
        "grad_norm": 2.702382802963257,
        "learning_rate": 0.0001991138578872816,
        "epoch": 0.34883374319994037,
        "step": 4681
    },
    {
        "loss": 2.4896,
        "grad_norm": 1.7706241607666016,
        "learning_rate": 0.0001991044865440241,
        "epoch": 0.34890826440122213,
        "step": 4682
    },
    {
        "loss": 2.3755,
        "grad_norm": 2.8916401863098145,
        "learning_rate": 0.00019909506613089025,
        "epoch": 0.3489827856025039,
        "step": 4683
    },
    {
        "loss": 2.3344,
        "grad_norm": 2.841017961502075,
        "learning_rate": 0.00019908559665254445,
        "epoch": 0.34905730680378566,
        "step": 4684
    },
    {
        "loss": 3.0324,
        "grad_norm": 3.8925416469573975,
        "learning_rate": 0.0001990760781136753,
        "epoch": 0.3491318280050674,
        "step": 4685
    },
    {
        "loss": 2.7388,
        "grad_norm": 2.4048099517822266,
        "learning_rate": 0.00019906651051899575,
        "epoch": 0.3492063492063492,
        "step": 4686
    },
    {
        "loss": 2.456,
        "grad_norm": 1.5647966861724854,
        "learning_rate": 0.00019905689387324306,
        "epoch": 0.34928087040763095,
        "step": 4687
    },
    {
        "loss": 2.587,
        "grad_norm": 2.7426085472106934,
        "learning_rate": 0.0001990472281811787,
        "epoch": 0.3493553916089127,
        "step": 4688
    },
    {
        "loss": 1.9273,
        "grad_norm": 2.0775907039642334,
        "learning_rate": 0.0001990375134475885,
        "epoch": 0.3494299128101945,
        "step": 4689
    },
    {
        "loss": 2.9462,
        "grad_norm": 2.373090982437134,
        "learning_rate": 0.0001990277496772825,
        "epoch": 0.34950443401147624,
        "step": 4690
    },
    {
        "loss": 2.3727,
        "grad_norm": 2.1455907821655273,
        "learning_rate": 0.00019901793687509512,
        "epoch": 0.349578955212758,
        "step": 4691
    },
    {
        "loss": 2.5681,
        "grad_norm": 3.2248668670654297,
        "learning_rate": 0.00019900807504588496,
        "epoch": 0.3496534764140398,
        "step": 4692
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.3996634483337402,
        "learning_rate": 0.00019899816419453498,
        "epoch": 0.3497279976153216,
        "step": 4693
    },
    {
        "loss": 2.2032,
        "grad_norm": 5.39795446395874,
        "learning_rate": 0.00019898820432595233,
        "epoch": 0.34980251881660335,
        "step": 4694
    },
    {
        "loss": 2.7036,
        "grad_norm": 1.9178005456924438,
        "learning_rate": 0.00019897819544506846,
        "epoch": 0.3498770400178851,
        "step": 4695
    },
    {
        "loss": 2.1832,
        "grad_norm": 4.271969795227051,
        "learning_rate": 0.00019896813755683913,
        "epoch": 0.3499515612191669,
        "step": 4696
    },
    {
        "loss": 1.8185,
        "grad_norm": 3.44525146484375,
        "learning_rate": 0.0001989580306662443,
        "epoch": 0.35002608242044864,
        "step": 4697
    },
    {
        "loss": 2.4058,
        "grad_norm": 2.530346632003784,
        "learning_rate": 0.00019894787477828824,
        "epoch": 0.3501006036217304,
        "step": 4698
    },
    {
        "loss": 3.0664,
        "grad_norm": 2.262664318084717,
        "learning_rate": 0.00019893766989799945,
        "epoch": 0.35017512482301216,
        "step": 4699
    },
    {
        "loss": 2.7869,
        "grad_norm": 2.66498064994812,
        "learning_rate": 0.00019892741603043067,
        "epoch": 0.3502496460242939,
        "step": 4700
    },
    {
        "loss": 2.4113,
        "grad_norm": 4.707139015197754,
        "learning_rate": 0.000198917113180659,
        "epoch": 0.3503241672255757,
        "step": 4701
    },
    {
        "loss": 2.3727,
        "grad_norm": 3.904768228530884,
        "learning_rate": 0.00019890676135378565,
        "epoch": 0.35039868842685745,
        "step": 4702
    },
    {
        "loss": 2.2271,
        "grad_norm": 3.341076135635376,
        "learning_rate": 0.0001988963605549362,
        "epoch": 0.3504732096281392,
        "step": 4703
    },
    {
        "loss": 1.991,
        "grad_norm": 2.6936519145965576,
        "learning_rate": 0.0001988859107892604,
        "epoch": 0.350547730829421,
        "step": 4704
    },
    {
        "loss": 2.152,
        "grad_norm": 2.9644765853881836,
        "learning_rate": 0.0001988754120619323,
        "epoch": 0.35062225203070274,
        "step": 4705
    },
    {
        "loss": 1.5626,
        "grad_norm": 4.258762359619141,
        "learning_rate": 0.0001988648643781501,
        "epoch": 0.3506967732319845,
        "step": 4706
    },
    {
        "loss": 2.4728,
        "grad_norm": 3.9457690715789795,
        "learning_rate": 0.00019885426774313633,
        "epoch": 0.35077129443326627,
        "step": 4707
    },
    {
        "loss": 2.7329,
        "grad_norm": 3.1673929691314697,
        "learning_rate": 0.00019884362216213773,
        "epoch": 0.35084581563454803,
        "step": 4708
    },
    {
        "loss": 3.1284,
        "grad_norm": 2.41996693611145,
        "learning_rate": 0.00019883292764042535,
        "epoch": 0.3509203368358298,
        "step": 4709
    },
    {
        "loss": 2.5815,
        "grad_norm": 2.07523512840271,
        "learning_rate": 0.0001988221841832943,
        "epoch": 0.35099485803711156,
        "step": 4710
    },
    {
        "loss": 1.5673,
        "grad_norm": 3.8012938499450684,
        "learning_rate": 0.00019881139179606406,
        "epoch": 0.3510693792383933,
        "step": 4711
    },
    {
        "loss": 2.0938,
        "grad_norm": 3.7085559368133545,
        "learning_rate": 0.00019880055048407828,
        "epoch": 0.3511439004396751,
        "step": 4712
    },
    {
        "loss": 1.9412,
        "grad_norm": 3.4447507858276367,
        "learning_rate": 0.00019878966025270486,
        "epoch": 0.35121842164095685,
        "step": 4713
    },
    {
        "loss": 1.3401,
        "grad_norm": 4.032456874847412,
        "learning_rate": 0.00019877872110733594,
        "epoch": 0.3512929428422386,
        "step": 4714
    },
    {
        "loss": 2.7766,
        "grad_norm": 3.0048024654388428,
        "learning_rate": 0.00019876773305338777,
        "epoch": 0.35136746404352037,
        "step": 4715
    },
    {
        "loss": 2.5797,
        "grad_norm": 1.926439642906189,
        "learning_rate": 0.00019875669609630095,
        "epoch": 0.35144198524480214,
        "step": 4716
    },
    {
        "loss": 2.0448,
        "grad_norm": 2.8124396800994873,
        "learning_rate": 0.00019874561024154022,
        "epoch": 0.3515165064460839,
        "step": 4717
    },
    {
        "loss": 2.9208,
        "grad_norm": 2.291964292526245,
        "learning_rate": 0.0001987344754945946,
        "epoch": 0.35159102764736566,
        "step": 4718
    },
    {
        "loss": 1.7404,
        "grad_norm": 3.5721378326416016,
        "learning_rate": 0.00019872329186097721,
        "epoch": 0.3516655488486474,
        "step": 4719
    },
    {
        "loss": 2.8507,
        "grad_norm": 4.125566005706787,
        "learning_rate": 0.00019871205934622545,
        "epoch": 0.3517400700499292,
        "step": 4720
    },
    {
        "loss": 2.4877,
        "grad_norm": 3.395237922668457,
        "learning_rate": 0.00019870077795590088,
        "epoch": 0.35181459125121095,
        "step": 4721
    },
    {
        "loss": 2.9687,
        "grad_norm": 1.3983616828918457,
        "learning_rate": 0.00019868944769558934,
        "epoch": 0.3518891124524927,
        "step": 4722
    },
    {
        "loss": 1.7715,
        "grad_norm": 3.292572498321533,
        "learning_rate": 0.00019867806857090073,
        "epoch": 0.3519636336537745,
        "step": 4723
    },
    {
        "loss": 1.1592,
        "grad_norm": 4.011482238769531,
        "learning_rate": 0.00019866664058746937,
        "epoch": 0.35203815485505624,
        "step": 4724
    },
    {
        "loss": 2.8108,
        "grad_norm": 2.118220090866089,
        "learning_rate": 0.0001986551637509535,
        "epoch": 0.352112676056338,
        "step": 4725
    },
    {
        "loss": 2.2926,
        "grad_norm": 2.3761157989501953,
        "learning_rate": 0.0001986436380670357,
        "epoch": 0.35218719725761977,
        "step": 4726
    },
    {
        "loss": 2.196,
        "grad_norm": 5.189093112945557,
        "learning_rate": 0.00019863206354142277,
        "epoch": 0.3522617184589016,
        "step": 4727
    },
    {
        "loss": 2.3733,
        "grad_norm": 2.0461013317108154,
        "learning_rate": 0.00019862044017984553,
        "epoch": 0.35233623966018335,
        "step": 4728
    },
    {
        "loss": 2.2294,
        "grad_norm": 2.2987985610961914,
        "learning_rate": 0.00019860876798805923,
        "epoch": 0.3524107608614651,
        "step": 4729
    },
    {
        "loss": 2.5622,
        "grad_norm": 2.88835072517395,
        "learning_rate": 0.00019859704697184304,
        "epoch": 0.3524852820627469,
        "step": 4730
    },
    {
        "loss": 1.9362,
        "grad_norm": 3.292311668395996,
        "learning_rate": 0.00019858527713700048,
        "epoch": 0.35255980326402864,
        "step": 4731
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.8371546268463135,
        "learning_rate": 0.00019857345848935912,
        "epoch": 0.3526343244653104,
        "step": 4732
    },
    {
        "loss": 2.0158,
        "grad_norm": 2.8052453994750977,
        "learning_rate": 0.00019856159103477086,
        "epoch": 0.35270884566659216,
        "step": 4733
    },
    {
        "loss": 2.5545,
        "grad_norm": 3.089419364929199,
        "learning_rate": 0.00019854967477911156,
        "epoch": 0.3527833668678739,
        "step": 4734
    },
    {
        "loss": 1.8095,
        "grad_norm": 3.5291097164154053,
        "learning_rate": 0.0001985377097282814,
        "epoch": 0.3528578880691557,
        "step": 4735
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.8575563430786133,
        "learning_rate": 0.0001985256958882047,
        "epoch": 0.35293240927043745,
        "step": 4736
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.269517421722412,
        "learning_rate": 0.0001985136332648298,
        "epoch": 0.3530069304717192,
        "step": 4737
    },
    {
        "loss": 2.3238,
        "grad_norm": 2.917210817337036,
        "learning_rate": 0.0001985015218641294,
        "epoch": 0.353081451673001,
        "step": 4738
    },
    {
        "loss": 2.4578,
        "grad_norm": 4.01777458190918,
        "learning_rate": 0.00019848936169210022,
        "epoch": 0.35315597287428274,
        "step": 4739
    },
    {
        "loss": 2.6163,
        "grad_norm": 2.539057731628418,
        "learning_rate": 0.00019847715275476317,
        "epoch": 0.3532304940755645,
        "step": 4740
    },
    {
        "loss": 2.2668,
        "grad_norm": 3.616149663925171,
        "learning_rate": 0.00019846489505816325,
        "epoch": 0.35330501527684627,
        "step": 4741
    },
    {
        "loss": 2.2414,
        "grad_norm": 3.0511889457702637,
        "learning_rate": 0.00019845258860836968,
        "epoch": 0.35337953647812803,
        "step": 4742
    },
    {
        "loss": 2.122,
        "grad_norm": 5.007050514221191,
        "learning_rate": 0.0001984402334114758,
        "epoch": 0.3534540576794098,
        "step": 4743
    },
    {
        "loss": 2.5924,
        "grad_norm": 4.435083866119385,
        "learning_rate": 0.00019842782947359906,
        "epoch": 0.35352857888069156,
        "step": 4744
    },
    {
        "loss": 2.0863,
        "grad_norm": 3.589468002319336,
        "learning_rate": 0.00019841537680088104,
        "epoch": 0.3536031000819733,
        "step": 4745
    },
    {
        "loss": 2.2407,
        "grad_norm": 3.293797492980957,
        "learning_rate": 0.0001984028753994875,
        "epoch": 0.3536776212832551,
        "step": 4746
    },
    {
        "loss": 1.5552,
        "grad_norm": 4.161263942718506,
        "learning_rate": 0.00019839032527560824,
        "epoch": 0.35375214248453685,
        "step": 4747
    },
    {
        "loss": 2.6946,
        "grad_norm": 2.6491541862487793,
        "learning_rate": 0.0001983777264354573,
        "epoch": 0.3538266636858186,
        "step": 4748
    },
    {
        "loss": 2.4225,
        "grad_norm": 1.9865800142288208,
        "learning_rate": 0.0001983650788852727,
        "epoch": 0.3539011848871004,
        "step": 4749
    },
    {
        "loss": 1.5441,
        "grad_norm": 3.0310163497924805,
        "learning_rate": 0.00019835238263131677,
        "epoch": 0.35397570608838214,
        "step": 4750
    },
    {
        "loss": 2.6785,
        "grad_norm": 3.1604480743408203,
        "learning_rate": 0.00019833963767987574,
        "epoch": 0.3540502272896639,
        "step": 4751
    },
    {
        "loss": 2.4786,
        "grad_norm": 3.17824387550354,
        "learning_rate": 0.00019832684403726009,
        "epoch": 0.35412474849094566,
        "step": 4752
    },
    {
        "loss": 2.3306,
        "grad_norm": 2.841794967651367,
        "learning_rate": 0.00019831400170980436,
        "epoch": 0.3541992696922274,
        "step": 4753
    },
    {
        "loss": 2.1707,
        "grad_norm": 3.713552474975586,
        "learning_rate": 0.0001983011107038672,
        "epoch": 0.3542737908935092,
        "step": 4754
    },
    {
        "loss": 2.3553,
        "grad_norm": 3.873941659927368,
        "learning_rate": 0.0001982881710258314,
        "epoch": 0.35434831209479095,
        "step": 4755
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.347163200378418,
        "learning_rate": 0.00019827518268210383,
        "epoch": 0.3544228332960727,
        "step": 4756
    },
    {
        "loss": 2.1237,
        "grad_norm": 3.998650550842285,
        "learning_rate": 0.0001982621456791154,
        "epoch": 0.3544973544973545,
        "step": 4757
    },
    {
        "loss": 2.6458,
        "grad_norm": 1.3370015621185303,
        "learning_rate": 0.0001982490600233212,
        "epoch": 0.35457187569863624,
        "step": 4758
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.6095340251922607,
        "learning_rate": 0.0001982359257212003,
        "epoch": 0.354646396899918,
        "step": 4759
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.8533356189727783,
        "learning_rate": 0.00019822274277925598,
        "epoch": 0.35472091810119977,
        "step": 4760
    },
    {
        "loss": 2.7959,
        "grad_norm": 2.8218016624450684,
        "learning_rate": 0.0001982095112040155,
        "epoch": 0.35479543930248153,
        "step": 4761
    },
    {
        "loss": 3.0209,
        "grad_norm": 2.6998355388641357,
        "learning_rate": 0.00019819623100203034,
        "epoch": 0.3548699605037633,
        "step": 4762
    },
    {
        "loss": 2.8639,
        "grad_norm": 2.3100829124450684,
        "learning_rate": 0.00019818290217987587,
        "epoch": 0.3549444817050451,
        "step": 4763
    },
    {
        "loss": 2.7671,
        "grad_norm": 3.505831718444824,
        "learning_rate": 0.00019816952474415163,
        "epoch": 0.3550190029063269,
        "step": 4764
    },
    {
        "loss": 2.3919,
        "grad_norm": 2.507577419281006,
        "learning_rate": 0.0001981560987014813,
        "epoch": 0.35509352410760864,
        "step": 4765
    },
    {
        "loss": 2.7786,
        "grad_norm": 3.7894272804260254,
        "learning_rate": 0.00019814262405851247,
        "epoch": 0.3551680453088904,
        "step": 4766
    },
    {
        "loss": 2.7763,
        "grad_norm": 2.485393762588501,
        "learning_rate": 0.0001981291008219169,
        "epoch": 0.35524256651017216,
        "step": 4767
    },
    {
        "loss": 2.3855,
        "grad_norm": 3.027397394180298,
        "learning_rate": 0.00019811552899839043,
        "epoch": 0.3553170877114539,
        "step": 4768
    },
    {
        "loss": 2.2954,
        "grad_norm": 3.9181108474731445,
        "learning_rate": 0.0001981019085946529,
        "epoch": 0.3553916089127357,
        "step": 4769
    },
    {
        "loss": 2.3943,
        "grad_norm": 3.0418872833251953,
        "learning_rate": 0.00019808823961744816,
        "epoch": 0.35546613011401745,
        "step": 4770
    },
    {
        "loss": 2.5369,
        "grad_norm": 2.7744009494781494,
        "learning_rate": 0.00019807452207354421,
        "epoch": 0.3555406513152992,
        "step": 4771
    },
    {
        "loss": 2.1398,
        "grad_norm": 2.2279045581817627,
        "learning_rate": 0.0001980607559697331,
        "epoch": 0.355615172516581,
        "step": 4772
    },
    {
        "loss": 2.86,
        "grad_norm": 4.081901550292969,
        "learning_rate": 0.0001980469413128308,
        "epoch": 0.35568969371786274,
        "step": 4773
    },
    {
        "loss": 2.5877,
        "grad_norm": 2.2100703716278076,
        "learning_rate": 0.00019803307810967744,
        "epoch": 0.3557642149191445,
        "step": 4774
    },
    {
        "loss": 2.0575,
        "grad_norm": 3.8503329753875732,
        "learning_rate": 0.00019801916636713712,
        "epoch": 0.35583873612042627,
        "step": 4775
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.502373456954956,
        "learning_rate": 0.00019800520609209804,
        "epoch": 0.35591325732170803,
        "step": 4776
    },
    {
        "loss": 2.0316,
        "grad_norm": 5.022486209869385,
        "learning_rate": 0.0001979911972914724,
        "epoch": 0.3559877785229898,
        "step": 4777
    },
    {
        "loss": 2.6714,
        "grad_norm": 3.2041077613830566,
        "learning_rate": 0.00019797713997219633,
        "epoch": 0.35606229972427156,
        "step": 4778
    },
    {
        "loss": 2.1115,
        "grad_norm": 3.0767245292663574,
        "learning_rate": 0.00019796303414123013,
        "epoch": 0.3561368209255533,
        "step": 4779
    },
    {
        "loss": 2.5804,
        "grad_norm": 3.6649882793426514,
        "learning_rate": 0.00019794887980555806,
        "epoch": 0.3562113421268351,
        "step": 4780
    },
    {
        "loss": 2.3612,
        "grad_norm": 3.489898204803467,
        "learning_rate": 0.0001979346769721884,
        "epoch": 0.35628586332811685,
        "step": 4781
    },
    {
        "loss": 2.8377,
        "grad_norm": 1.8136711120605469,
        "learning_rate": 0.00019792042564815342,
        "epoch": 0.3563603845293986,
        "step": 4782
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.557141065597534,
        "learning_rate": 0.0001979061258405094,
        "epoch": 0.3564349057306804,
        "step": 4783
    },
    {
        "loss": 2.6732,
        "grad_norm": 1.9745434522628784,
        "learning_rate": 0.00019789177755633665,
        "epoch": 0.35650942693196214,
        "step": 4784
    },
    {
        "loss": 2.1858,
        "grad_norm": 3.2649025917053223,
        "learning_rate": 0.00019787738080273952,
        "epoch": 0.3565839481332439,
        "step": 4785
    },
    {
        "loss": 2.5855,
        "grad_norm": 3.5737977027893066,
        "learning_rate": 0.0001978629355868463,
        "epoch": 0.35665846933452566,
        "step": 4786
    },
    {
        "loss": 2.2794,
        "grad_norm": 2.558190107345581,
        "learning_rate": 0.00019784844191580926,
        "epoch": 0.3567329905358074,
        "step": 4787
    },
    {
        "loss": 2.0038,
        "grad_norm": 2.4292497634887695,
        "learning_rate": 0.00019783389979680468,
        "epoch": 0.3568075117370892,
        "step": 4788
    },
    {
        "loss": 2.7552,
        "grad_norm": 2.6738150119781494,
        "learning_rate": 0.00019781930923703288,
        "epoch": 0.35688203293837095,
        "step": 4789
    },
    {
        "loss": 2.5421,
        "grad_norm": 2.6343061923980713,
        "learning_rate": 0.0001978046702437181,
        "epoch": 0.3569565541396527,
        "step": 4790
    },
    {
        "loss": 2.7563,
        "grad_norm": 2.152682065963745,
        "learning_rate": 0.0001977899828241086,
        "epoch": 0.3570310753409345,
        "step": 4791
    },
    {
        "loss": 2.097,
        "grad_norm": 2.3497211933135986,
        "learning_rate": 0.0001977752469854766,
        "epoch": 0.35710559654221624,
        "step": 4792
    },
    {
        "loss": 1.683,
        "grad_norm": 1.2589682340621948,
        "learning_rate": 0.00019776046273511827,
        "epoch": 0.357180117743498,
        "step": 4793
    },
    {
        "loss": 1.8758,
        "grad_norm": 2.957676887512207,
        "learning_rate": 0.00019774563008035382,
        "epoch": 0.35725463894477977,
        "step": 4794
    },
    {
        "loss": 1.8329,
        "grad_norm": 2.9605348110198975,
        "learning_rate": 0.00019773074902852735,
        "epoch": 0.35732916014606153,
        "step": 4795
    },
    {
        "loss": 2.7925,
        "grad_norm": 2.886406183242798,
        "learning_rate": 0.00019771581958700696,
        "epoch": 0.3574036813473433,
        "step": 4796
    },
    {
        "loss": 2.8577,
        "grad_norm": 3.2338693141937256,
        "learning_rate": 0.00019770084176318472,
        "epoch": 0.35747820254862506,
        "step": 4797
    },
    {
        "loss": 2.7106,
        "grad_norm": 2.4798524379730225,
        "learning_rate": 0.00019768581556447662,
        "epoch": 0.3575527237499069,
        "step": 4798
    },
    {
        "loss": 1.8096,
        "grad_norm": 3.6988167762756348,
        "learning_rate": 0.00019767074099832265,
        "epoch": 0.35762724495118864,
        "step": 4799
    },
    {
        "loss": 2.6079,
        "grad_norm": 1.9439233541488647,
        "learning_rate": 0.0001976556180721867,
        "epoch": 0.3577017661524704,
        "step": 4800
    },
    {
        "loss": 2.6055,
        "grad_norm": 3.942401647567749,
        "learning_rate": 0.00019764044679355665,
        "epoch": 0.35777628735375216,
        "step": 4801
    },
    {
        "loss": 2.696,
        "grad_norm": 2.717543125152588,
        "learning_rate": 0.00019762522716994428,
        "epoch": 0.35785080855503393,
        "step": 4802
    },
    {
        "loss": 2.3484,
        "grad_norm": 4.103456974029541,
        "learning_rate": 0.00019760995920888527,
        "epoch": 0.3579253297563157,
        "step": 4803
    },
    {
        "loss": 1.4122,
        "grad_norm": 3.700347423553467,
        "learning_rate": 0.0001975946429179394,
        "epoch": 0.35799985095759745,
        "step": 4804
    },
    {
        "loss": 3.2359,
        "grad_norm": 3.2577195167541504,
        "learning_rate": 0.0001975792783046902,
        "epoch": 0.3580743721588792,
        "step": 4805
    },
    {
        "loss": 2.6445,
        "grad_norm": 3.173736333847046,
        "learning_rate": 0.0001975638653767452,
        "epoch": 0.358148893360161,
        "step": 4806
    },
    {
        "loss": 2.1067,
        "grad_norm": 2.9477224349975586,
        "learning_rate": 0.0001975484041417358,
        "epoch": 0.35822341456144274,
        "step": 4807
    },
    {
        "loss": 2.3181,
        "grad_norm": 3.666212320327759,
        "learning_rate": 0.00019753289460731744,
        "epoch": 0.3582979357627245,
        "step": 4808
    },
    {
        "loss": 2.3335,
        "grad_norm": 3.2841086387634277,
        "learning_rate": 0.00019751733678116939,
        "epoch": 0.35837245696400627,
        "step": 4809
    },
    {
        "loss": 1.9351,
        "grad_norm": 3.570516347885132,
        "learning_rate": 0.0001975017306709948,
        "epoch": 0.35844697816528803,
        "step": 4810
    },
    {
        "loss": 1.9475,
        "grad_norm": 3.7682764530181885,
        "learning_rate": 0.0001974860762845208,
        "epoch": 0.3585214993665698,
        "step": 4811
    },
    {
        "loss": 2.5263,
        "grad_norm": 2.625701665878296,
        "learning_rate": 0.00019747037362949837,
        "epoch": 0.35859602056785156,
        "step": 4812
    },
    {
        "loss": 2.064,
        "grad_norm": 2.8295843601226807,
        "learning_rate": 0.00019745462271370236,
        "epoch": 0.3586705417691333,
        "step": 4813
    },
    {
        "loss": 2.7445,
        "grad_norm": 2.496673822402954,
        "learning_rate": 0.0001974388235449317,
        "epoch": 0.3587450629704151,
        "step": 4814
    },
    {
        "loss": 2.0522,
        "grad_norm": 2.7179245948791504,
        "learning_rate": 0.00019742297613100892,
        "epoch": 0.35881958417169685,
        "step": 4815
    },
    {
        "loss": 2.2996,
        "grad_norm": 3.3312206268310547,
        "learning_rate": 0.0001974070804797807,
        "epoch": 0.3588941053729786,
        "step": 4816
    },
    {
        "loss": 2.5584,
        "grad_norm": 3.0535600185394287,
        "learning_rate": 0.00019739113659911746,
        "epoch": 0.3589686265742604,
        "step": 4817
    },
    {
        "loss": 1.6032,
        "grad_norm": 2.7800180912017822,
        "learning_rate": 0.00019737514449691355,
        "epoch": 0.35904314777554214,
        "step": 4818
    },
    {
        "loss": 2.9116,
        "grad_norm": 1.9309242963790894,
        "learning_rate": 0.00019735910418108716,
        "epoch": 0.3591176689768239,
        "step": 4819
    },
    {
        "loss": 2.9221,
        "grad_norm": 2.7849233150482178,
        "learning_rate": 0.00019734301565958037,
        "epoch": 0.35919219017810566,
        "step": 4820
    },
    {
        "loss": 2.3182,
        "grad_norm": 4.244751453399658,
        "learning_rate": 0.0001973268789403592,
        "epoch": 0.3592667113793874,
        "step": 4821
    },
    {
        "loss": 2.9045,
        "grad_norm": 1.6333099603652954,
        "learning_rate": 0.0001973106940314134,
        "epoch": 0.3593412325806692,
        "step": 4822
    },
    {
        "loss": 1.7218,
        "grad_norm": 3.339789867401123,
        "learning_rate": 0.00019729446094075666,
        "epoch": 0.35941575378195095,
        "step": 4823
    },
    {
        "loss": 2.8584,
        "grad_norm": 1.7967058420181274,
        "learning_rate": 0.00019727817967642652,
        "epoch": 0.3594902749832327,
        "step": 4824
    },
    {
        "loss": 2.6365,
        "grad_norm": 2.6267056465148926,
        "learning_rate": 0.00019726185024648436,
        "epoch": 0.3595647961845145,
        "step": 4825
    },
    {
        "loss": 2.0347,
        "grad_norm": 3.5562639236450195,
        "learning_rate": 0.00019724547265901544,
        "epoch": 0.35963931738579624,
        "step": 4826
    },
    {
        "loss": 2.6159,
        "grad_norm": 2.308088779449463,
        "learning_rate": 0.00019722904692212885,
        "epoch": 0.359713838587078,
        "step": 4827
    },
    {
        "loss": 2.0449,
        "grad_norm": 3.331730604171753,
        "learning_rate": 0.0001972125730439575,
        "epoch": 0.35978835978835977,
        "step": 4828
    },
    {
        "loss": 1.5393,
        "grad_norm": 1.4239393472671509,
        "learning_rate": 0.00019719605103265807,
        "epoch": 0.35986288098964153,
        "step": 4829
    },
    {
        "loss": 2.7278,
        "grad_norm": 2.348606824874878,
        "learning_rate": 0.00019717948089641122,
        "epoch": 0.3599374021909233,
        "step": 4830
    },
    {
        "loss": 2.3545,
        "grad_norm": 2.7547571659088135,
        "learning_rate": 0.0001971628626434214,
        "epoch": 0.36001192339220506,
        "step": 4831
    },
    {
        "loss": 2.8261,
        "grad_norm": 2.966498374938965,
        "learning_rate": 0.00019714619628191679,
        "epoch": 0.3600864445934868,
        "step": 4832
    },
    {
        "loss": 2.9515,
        "grad_norm": 1.643703818321228,
        "learning_rate": 0.0001971294818201495,
        "epoch": 0.36016096579476864,
        "step": 4833
    },
    {
        "loss": 2.7049,
        "grad_norm": 4.384919166564941,
        "learning_rate": 0.00019711271926639535,
        "epoch": 0.3602354869960504,
        "step": 4834
    },
    {
        "loss": 1.7282,
        "grad_norm": 5.1328582763671875,
        "learning_rate": 0.00019709590862895403,
        "epoch": 0.36031000819733217,
        "step": 4835
    },
    {
        "loss": 1.7088,
        "grad_norm": 1.7334272861480713,
        "learning_rate": 0.00019707904991614908,
        "epoch": 0.36038452939861393,
        "step": 4836
    },
    {
        "loss": 2.5816,
        "grad_norm": 2.6655161380767822,
        "learning_rate": 0.00019706214313632781,
        "epoch": 0.3604590505998957,
        "step": 4837
    },
    {
        "loss": 2.5539,
        "grad_norm": 1.691535472869873,
        "learning_rate": 0.00019704518829786132,
        "epoch": 0.36053357180117745,
        "step": 4838
    },
    {
        "loss": 1.9491,
        "grad_norm": 4.236907958984375,
        "learning_rate": 0.00019702818540914442,
        "epoch": 0.3606080930024592,
        "step": 4839
    },
    {
        "loss": 2.1648,
        "grad_norm": 3.5735907554626465,
        "learning_rate": 0.0001970111344785959,
        "epoch": 0.360682614203741,
        "step": 4840
    },
    {
        "loss": 3.02,
        "grad_norm": 2.254056692123413,
        "learning_rate": 0.0001969940355146582,
        "epoch": 0.36075713540502274,
        "step": 4841
    },
    {
        "loss": 2.4036,
        "grad_norm": 1.8754531145095825,
        "learning_rate": 0.00019697688852579755,
        "epoch": 0.3608316566063045,
        "step": 4842
    },
    {
        "loss": 2.0559,
        "grad_norm": 4.3999924659729,
        "learning_rate": 0.00019695969352050401,
        "epoch": 0.36090617780758627,
        "step": 4843
    },
    {
        "loss": 2.1541,
        "grad_norm": 4.233309268951416,
        "learning_rate": 0.00019694245050729136,
        "epoch": 0.36098069900886803,
        "step": 4844
    },
    {
        "loss": 1.6704,
        "grad_norm": 3.16772198677063,
        "learning_rate": 0.00019692515949469724,
        "epoch": 0.3610552202101498,
        "step": 4845
    },
    {
        "loss": 2.9884,
        "grad_norm": 2.8306071758270264,
        "learning_rate": 0.00019690782049128292,
        "epoch": 0.36112974141143156,
        "step": 4846
    },
    {
        "loss": 2.8773,
        "grad_norm": 2.6758840084075928,
        "learning_rate": 0.0001968904335056336,
        "epoch": 0.3612042626127133,
        "step": 4847
    },
    {
        "loss": 2.5657,
        "grad_norm": 2.3350720405578613,
        "learning_rate": 0.00019687299854635808,
        "epoch": 0.3612787838139951,
        "step": 4848
    },
    {
        "loss": 1.7725,
        "grad_norm": 2.7416865825653076,
        "learning_rate": 0.00019685551562208898,
        "epoch": 0.36135330501527685,
        "step": 4849
    },
    {
        "loss": 2.5297,
        "grad_norm": 2.255265951156616,
        "learning_rate": 0.0001968379847414827,
        "epoch": 0.3614278262165586,
        "step": 4850
    },
    {
        "loss": 2.3224,
        "grad_norm": 4.016800880432129,
        "learning_rate": 0.00019682040591321935,
        "epoch": 0.3615023474178404,
        "step": 4851
    },
    {
        "loss": 2.4005,
        "grad_norm": 2.9166934490203857,
        "learning_rate": 0.00019680277914600275,
        "epoch": 0.36157686861912214,
        "step": 4852
    },
    {
        "loss": 2.6811,
        "grad_norm": 2.0154690742492676,
        "learning_rate": 0.0001967851044485605,
        "epoch": 0.3616513898204039,
        "step": 4853
    },
    {
        "loss": 1.771,
        "grad_norm": 2.9891860485076904,
        "learning_rate": 0.00019676738182964395,
        "epoch": 0.36172591102168566,
        "step": 4854
    },
    {
        "loss": 1.9727,
        "grad_norm": 3.5236964225769043,
        "learning_rate": 0.00019674961129802814,
        "epoch": 0.3618004322229674,
        "step": 4855
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.6761832237243652,
        "learning_rate": 0.00019673179286251182,
        "epoch": 0.3618749534242492,
        "step": 4856
    },
    {
        "loss": 2.8387,
        "grad_norm": 2.379934549331665,
        "learning_rate": 0.00019671392653191749,
        "epoch": 0.36194947462553095,
        "step": 4857
    },
    {
        "loss": 2.1165,
        "grad_norm": 2.635270357131958,
        "learning_rate": 0.00019669601231509139,
        "epoch": 0.3620239958268127,
        "step": 4858
    },
    {
        "loss": 1.7866,
        "grad_norm": 3.707859992980957,
        "learning_rate": 0.00019667805022090335,
        "epoch": 0.3620985170280945,
        "step": 4859
    },
    {
        "loss": 2.3921,
        "grad_norm": 3.3055317401885986,
        "learning_rate": 0.00019666004025824707,
        "epoch": 0.36217303822937624,
        "step": 4860
    },
    {
        "loss": 1.7971,
        "grad_norm": 3.3371472358703613,
        "learning_rate": 0.00019664198243603986,
        "epoch": 0.362247559430658,
        "step": 4861
    },
    {
        "loss": 2.3714,
        "grad_norm": 2.6814608573913574,
        "learning_rate": 0.00019662387676322272,
        "epoch": 0.36232208063193977,
        "step": 4862
    },
    {
        "loss": 2.1859,
        "grad_norm": 3.3485021591186523,
        "learning_rate": 0.00019660572324876037,
        "epoch": 0.36239660183322153,
        "step": 4863
    },
    {
        "loss": 2.3173,
        "grad_norm": 3.7250466346740723,
        "learning_rate": 0.00019658752190164117,
        "epoch": 0.3624711230345033,
        "step": 4864
    },
    {
        "loss": 2.2109,
        "grad_norm": 4.153345584869385,
        "learning_rate": 0.00019656927273087725,
        "epoch": 0.36254564423578506,
        "step": 4865
    },
    {
        "loss": 2.538,
        "grad_norm": 3.7303168773651123,
        "learning_rate": 0.00019655097574550442,
        "epoch": 0.3626201654370668,
        "step": 4866
    },
    {
        "loss": 2.3995,
        "grad_norm": 3.1828556060791016,
        "learning_rate": 0.00019653263095458203,
        "epoch": 0.3626946866383486,
        "step": 4867
    },
    {
        "loss": 2.6714,
        "grad_norm": 1.8276375532150269,
        "learning_rate": 0.0001965142383671932,
        "epoch": 0.36276920783963035,
        "step": 4868
    },
    {
        "loss": 1.8179,
        "grad_norm": 3.4521851539611816,
        "learning_rate": 0.0001964957979924447,
        "epoch": 0.36284372904091217,
        "step": 4869
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.0954997539520264,
        "learning_rate": 0.00019647730983946704,
        "epoch": 0.36291825024219393,
        "step": 4870
    },
    {
        "loss": 2.624,
        "grad_norm": 2.9725940227508545,
        "learning_rate": 0.0001964587739174142,
        "epoch": 0.3629927714434757,
        "step": 4871
    },
    {
        "loss": 1.8811,
        "grad_norm": 3.1978793144226074,
        "learning_rate": 0.000196440190235464,
        "epoch": 0.36306729264475746,
        "step": 4872
    },
    {
        "loss": 3.1918,
        "grad_norm": 2.5985825061798096,
        "learning_rate": 0.0001964215588028178,
        "epoch": 0.3631418138460392,
        "step": 4873
    },
    {
        "loss": 2.3549,
        "grad_norm": 3.1187071800231934,
        "learning_rate": 0.00019640287962870062,
        "epoch": 0.363216335047321,
        "step": 4874
    },
    {
        "loss": 2.2689,
        "grad_norm": 3.578807830810547,
        "learning_rate": 0.00019638415272236115,
        "epoch": 0.36329085624860274,
        "step": 4875
    },
    {
        "loss": 2.5124,
        "grad_norm": 2.2859044075012207,
        "learning_rate": 0.00019636537809307168,
        "epoch": 0.3633653774498845,
        "step": 4876
    },
    {
        "loss": 2.2575,
        "grad_norm": 3.314873456954956,
        "learning_rate": 0.00019634655575012818,
        "epoch": 0.36343989865116627,
        "step": 4877
    },
    {
        "loss": 2.9653,
        "grad_norm": 2.9729809761047363,
        "learning_rate": 0.00019632768570285015,
        "epoch": 0.36351441985244803,
        "step": 4878
    },
    {
        "loss": 2.3299,
        "grad_norm": 2.73486590385437,
        "learning_rate": 0.00019630876796058083,
        "epoch": 0.3635889410537298,
        "step": 4879
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.2878698110580444,
        "learning_rate": 0.000196289802532687,
        "epoch": 0.36366346225501156,
        "step": 4880
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.135979175567627,
        "learning_rate": 0.000196270789428559,
        "epoch": 0.3637379834562933,
        "step": 4881
    },
    {
        "loss": 2.5641,
        "grad_norm": 6.404880523681641,
        "learning_rate": 0.00019625172865761094,
        "epoch": 0.3638125046575751,
        "step": 4882
    },
    {
        "loss": 2.5393,
        "grad_norm": 1.3030022382736206,
        "learning_rate": 0.0001962326202292803,
        "epoch": 0.36388702585885685,
        "step": 4883
    },
    {
        "loss": 2.3558,
        "grad_norm": 3.4376301765441895,
        "learning_rate": 0.00019621346415302844,
        "epoch": 0.3639615470601386,
        "step": 4884
    },
    {
        "loss": 2.3982,
        "grad_norm": 2.8438875675201416,
        "learning_rate": 0.0001961942604383401,
        "epoch": 0.3640360682614204,
        "step": 4885
    },
    {
        "loss": 2.3995,
        "grad_norm": 2.266427993774414,
        "learning_rate": 0.0001961750090947236,
        "epoch": 0.36411058946270214,
        "step": 4886
    },
    {
        "loss": 2.066,
        "grad_norm": 3.054624080657959,
        "learning_rate": 0.000196155710131711,
        "epoch": 0.3641851106639839,
        "step": 4887
    },
    {
        "loss": 2.5071,
        "grad_norm": 2.8245646953582764,
        "learning_rate": 0.0001961363635588578,
        "epoch": 0.36425963186526567,
        "step": 4888
    },
    {
        "loss": 2.5588,
        "grad_norm": 3.2736878395080566,
        "learning_rate": 0.00019611696938574315,
        "epoch": 0.36433415306654743,
        "step": 4889
    },
    {
        "loss": 1.9881,
        "grad_norm": 3.826890230178833,
        "learning_rate": 0.00019609752762196975,
        "epoch": 0.3644086742678292,
        "step": 4890
    },
    {
        "loss": 2.3688,
        "grad_norm": 2.4628517627716064,
        "learning_rate": 0.00019607803827716378,
        "epoch": 0.36448319546911095,
        "step": 4891
    },
    {
        "loss": 2.8418,
        "grad_norm": 1.980412244796753,
        "learning_rate": 0.00019605850136097515,
        "epoch": 0.3645577166703927,
        "step": 4892
    },
    {
        "loss": 2.8435,
        "grad_norm": 3.1281721591949463,
        "learning_rate": 0.00019603891688307713,
        "epoch": 0.3646322378716745,
        "step": 4893
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.5900213718414307,
        "learning_rate": 0.00019601928485316677,
        "epoch": 0.36470675907295624,
        "step": 4894
    },
    {
        "loss": 2.016,
        "grad_norm": 3.1180055141448975,
        "learning_rate": 0.00019599960528096438,
        "epoch": 0.364781280274238,
        "step": 4895
    },
    {
        "loss": 1.1833,
        "grad_norm": 3.89616322517395,
        "learning_rate": 0.00019597987817621405,
        "epoch": 0.36485580147551977,
        "step": 4896
    },
    {
        "loss": 1.8503,
        "grad_norm": 3.072654962539673,
        "learning_rate": 0.0001959601035486833,
        "epoch": 0.36493032267680153,
        "step": 4897
    },
    {
        "loss": 2.468,
        "grad_norm": 3.6371591091156006,
        "learning_rate": 0.00019594028140816318,
        "epoch": 0.3650048438780833,
        "step": 4898
    },
    {
        "loss": 1.7756,
        "grad_norm": 3.816906690597534,
        "learning_rate": 0.00019592041176446831,
        "epoch": 0.36507936507936506,
        "step": 4899
    },
    {
        "loss": 2.6535,
        "grad_norm": 3.7257120609283447,
        "learning_rate": 0.0001959004946274368,
        "epoch": 0.3651538862806468,
        "step": 4900
    },
    {
        "loss": 2.6388,
        "grad_norm": 3.06772518157959,
        "learning_rate": 0.00019588053000693022,
        "epoch": 0.3652284074819286,
        "step": 4901
    },
    {
        "loss": 2.9251,
        "grad_norm": 1.907077431678772,
        "learning_rate": 0.00019586051791283375,
        "epoch": 0.36530292868321035,
        "step": 4902
    },
    {
        "loss": 2.2608,
        "grad_norm": 2.6364850997924805,
        "learning_rate": 0.00019584045835505605,
        "epoch": 0.3653774498844921,
        "step": 4903
    },
    {
        "loss": 1.4052,
        "grad_norm": 1.7459893226623535,
        "learning_rate": 0.00019582035134352923,
        "epoch": 0.36545197108577393,
        "step": 4904
    },
    {
        "loss": 2.5059,
        "grad_norm": 3.113651990890503,
        "learning_rate": 0.00019580019688820896,
        "epoch": 0.3655264922870557,
        "step": 4905
    },
    {
        "loss": 1.0372,
        "grad_norm": 1.3050564527511597,
        "learning_rate": 0.00019577999499907433,
        "epoch": 0.36560101348833746,
        "step": 4906
    },
    {
        "loss": 2.6577,
        "grad_norm": 3.1619362831115723,
        "learning_rate": 0.000195759745686128,
        "epoch": 0.3656755346896192,
        "step": 4907
    },
    {
        "loss": 1.9791,
        "grad_norm": 2.3356235027313232,
        "learning_rate": 0.00019573944895939603,
        "epoch": 0.365750055890901,
        "step": 4908
    },
    {
        "loss": 2.5547,
        "grad_norm": 3.133735418319702,
        "learning_rate": 0.00019571910482892805,
        "epoch": 0.36582457709218275,
        "step": 4909
    },
    {
        "loss": 2.8346,
        "grad_norm": 2.602940320968628,
        "learning_rate": 0.00019569871330479702,
        "epoch": 0.3658990982934645,
        "step": 4910
    },
    {
        "loss": 2.6494,
        "grad_norm": 3.556442975997925,
        "learning_rate": 0.00019567827439709952,
        "epoch": 0.36597361949474627,
        "step": 4911
    },
    {
        "loss": 2.9686,
        "grad_norm": 2.7534120082855225,
        "learning_rate": 0.0001956577881159555,
        "epoch": 0.36604814069602803,
        "step": 4912
    },
    {
        "loss": 1.2919,
        "grad_norm": 4.046455383300781,
        "learning_rate": 0.00019563725447150842,
        "epoch": 0.3661226618973098,
        "step": 4913
    },
    {
        "loss": 3.112,
        "grad_norm": 2.6900014877319336,
        "learning_rate": 0.0001956166734739251,
        "epoch": 0.36619718309859156,
        "step": 4914
    },
    {
        "loss": 2.381,
        "grad_norm": 2.434661388397217,
        "learning_rate": 0.00019559604513339588,
        "epoch": 0.3662717042998733,
        "step": 4915
    },
    {
        "loss": 2.5756,
        "grad_norm": 2.448233127593994,
        "learning_rate": 0.0001955753694601346,
        "epoch": 0.3663462255011551,
        "step": 4916
    },
    {
        "loss": 2.3187,
        "grad_norm": 2.8342349529266357,
        "learning_rate": 0.00019555464646437836,
        "epoch": 0.36642074670243685,
        "step": 4917
    },
    {
        "loss": 2.7987,
        "grad_norm": 1.8893988132476807,
        "learning_rate": 0.00019553387615638785,
        "epoch": 0.3664952679037186,
        "step": 4918
    },
    {
        "loss": 2.8076,
        "grad_norm": 2.401777505874634,
        "learning_rate": 0.00019551305854644712,
        "epoch": 0.3665697891050004,
        "step": 4919
    },
    {
        "loss": 2.1016,
        "grad_norm": 3.7595200538635254,
        "learning_rate": 0.00019549219364486364,
        "epoch": 0.36664431030628214,
        "step": 4920
    },
    {
        "loss": 1.3222,
        "grad_norm": 3.555260181427002,
        "learning_rate": 0.0001954712814619683,
        "epoch": 0.3667188315075639,
        "step": 4921
    },
    {
        "loss": 2.0998,
        "grad_norm": 3.3891708850860596,
        "learning_rate": 0.0001954503220081155,
        "epoch": 0.36679335270884567,
        "step": 4922
    },
    {
        "loss": 2.5318,
        "grad_norm": 2.297039270401001,
        "learning_rate": 0.0001954293152936828,
        "epoch": 0.36686787391012743,
        "step": 4923
    },
    {
        "loss": 1.5754,
        "grad_norm": 3.296717405319214,
        "learning_rate": 0.0001954082613290714,
        "epoch": 0.3669423951114092,
        "step": 4924
    },
    {
        "loss": 2.2794,
        "grad_norm": 3.1006851196289062,
        "learning_rate": 0.0001953871601247058,
        "epoch": 0.36701691631269096,
        "step": 4925
    },
    {
        "loss": 2.2554,
        "grad_norm": 2.708670139312744,
        "learning_rate": 0.00019536601169103387,
        "epoch": 0.3670914375139727,
        "step": 4926
    },
    {
        "loss": 1.9108,
        "grad_norm": 1.8194196224212646,
        "learning_rate": 0.00019534481603852695,
        "epoch": 0.3671659587152545,
        "step": 4927
    },
    {
        "loss": 2.6226,
        "grad_norm": 2.2859747409820557,
        "learning_rate": 0.00019532357317767965,
        "epoch": 0.36724047991653624,
        "step": 4928
    },
    {
        "loss": 1.6851,
        "grad_norm": 5.876798629760742,
        "learning_rate": 0.00019530228311901002,
        "epoch": 0.367315001117818,
        "step": 4929
    },
    {
        "loss": 1.7733,
        "grad_norm": 4.216641426086426,
        "learning_rate": 0.00019528094587305942,
        "epoch": 0.36738952231909977,
        "step": 4930
    },
    {
        "loss": 2.6836,
        "grad_norm": 2.67148494720459,
        "learning_rate": 0.0001952595614503927,
        "epoch": 0.36746404352038153,
        "step": 4931
    },
    {
        "loss": 2.0746,
        "grad_norm": 3.6093719005584717,
        "learning_rate": 0.000195238129861598,
        "epoch": 0.3675385647216633,
        "step": 4932
    },
    {
        "loss": 2.5621,
        "grad_norm": 3.471864938735962,
        "learning_rate": 0.0001952166511172867,
        "epoch": 0.36761308592294506,
        "step": 4933
    },
    {
        "loss": 2.6763,
        "grad_norm": 3.7033731937408447,
        "learning_rate": 0.00019519512522809369,
        "epoch": 0.3676876071242268,
        "step": 4934
    },
    {
        "loss": 2.7945,
        "grad_norm": 3.2859649658203125,
        "learning_rate": 0.00019517355220467712,
        "epoch": 0.3677621283255086,
        "step": 4935
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.7280759811401367,
        "learning_rate": 0.00019515193205771856,
        "epoch": 0.36783664952679035,
        "step": 4936
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.149078130722046,
        "learning_rate": 0.00019513026479792278,
        "epoch": 0.3679111707280721,
        "step": 4937
    },
    {
        "loss": 2.6235,
        "grad_norm": 4.073020935058594,
        "learning_rate": 0.00019510855043601794,
        "epoch": 0.3679856919293539,
        "step": 4938
    },
    {
        "loss": 2.2079,
        "grad_norm": 2.542019844055176,
        "learning_rate": 0.00019508678898275564,
        "epoch": 0.36806021313063564,
        "step": 4939
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.9920644760131836,
        "learning_rate": 0.0001950649804489106,
        "epoch": 0.36813473433191746,
        "step": 4940
    },
    {
        "loss": 1.6368,
        "grad_norm": 3.9550559520721436,
        "learning_rate": 0.00019504312484528095,
        "epoch": 0.3682092555331992,
        "step": 4941
    },
    {
        "loss": 2.0542,
        "grad_norm": 2.8938369750976562,
        "learning_rate": 0.0001950212221826881,
        "epoch": 0.368283776734481,
        "step": 4942
    },
    {
        "loss": 2.1419,
        "grad_norm": 4.655568599700928,
        "learning_rate": 0.00019499927247197682,
        "epoch": 0.36835829793576275,
        "step": 4943
    },
    {
        "loss": 2.4886,
        "grad_norm": 3.0991761684417725,
        "learning_rate": 0.00019497727572401509,
        "epoch": 0.3684328191370445,
        "step": 4944
    },
    {
        "loss": 2.6965,
        "grad_norm": 2.239989757537842,
        "learning_rate": 0.00019495523194969425,
        "epoch": 0.3685073403383263,
        "step": 4945
    },
    {
        "loss": 2.6701,
        "grad_norm": 3.09995436668396,
        "learning_rate": 0.00019493314115992888,
        "epoch": 0.36858186153960804,
        "step": 4946
    },
    {
        "loss": 2.5929,
        "grad_norm": 1.9734134674072266,
        "learning_rate": 0.00019491100336565685,
        "epoch": 0.3686563827408898,
        "step": 4947
    },
    {
        "loss": 2.7091,
        "grad_norm": 2.051527738571167,
        "learning_rate": 0.00019488881857783935,
        "epoch": 0.36873090394217156,
        "step": 4948
    },
    {
        "loss": 2.4318,
        "grad_norm": 2.234686851501465,
        "learning_rate": 0.00019486658680746072,
        "epoch": 0.3688054251434533,
        "step": 4949
    },
    {
        "loss": 2.7096,
        "grad_norm": 3.94866681098938,
        "learning_rate": 0.00019484430806552871,
        "epoch": 0.3688799463447351,
        "step": 4950
    },
    {
        "loss": 2.4598,
        "grad_norm": 2.694025754928589,
        "learning_rate": 0.00019482198236307418,
        "epoch": 0.36895446754601685,
        "step": 4951
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.8096513748168945,
        "learning_rate": 0.00019479960971115136,
        "epoch": 0.3690289887472986,
        "step": 4952
    },
    {
        "loss": 1.7908,
        "grad_norm": 3.3458750247955322,
        "learning_rate": 0.0001947771901208377,
        "epoch": 0.3691035099485804,
        "step": 4953
    },
    {
        "loss": 2.192,
        "grad_norm": 3.038759708404541,
        "learning_rate": 0.00019475472360323384,
        "epoch": 0.36917803114986214,
        "step": 4954
    },
    {
        "loss": 2.3673,
        "grad_norm": 2.758150339126587,
        "learning_rate": 0.0001947322101694637,
        "epoch": 0.3692525523511439,
        "step": 4955
    },
    {
        "loss": 1.7516,
        "grad_norm": 3.9011290073394775,
        "learning_rate": 0.0001947096498306744,
        "epoch": 0.36932707355242567,
        "step": 4956
    },
    {
        "loss": 2.6952,
        "grad_norm": 2.4523613452911377,
        "learning_rate": 0.00019468704259803637,
        "epoch": 0.36940159475370743,
        "step": 4957
    },
    {
        "loss": 2.1142,
        "grad_norm": 4.466222286224365,
        "learning_rate": 0.0001946643884827431,
        "epoch": 0.3694761159549892,
        "step": 4958
    },
    {
        "loss": 2.8088,
        "grad_norm": 3.5727503299713135,
        "learning_rate": 0.00019464168749601144,
        "epoch": 0.36955063715627096,
        "step": 4959
    },
    {
        "loss": 2.9192,
        "grad_norm": 2.6812329292297363,
        "learning_rate": 0.0001946189396490814,
        "epoch": 0.3696251583575527,
        "step": 4960
    },
    {
        "loss": 1.9994,
        "grad_norm": 3.1945574283599854,
        "learning_rate": 0.00019459614495321613,
        "epoch": 0.3696996795588345,
        "step": 4961
    },
    {
        "loss": 2.3156,
        "grad_norm": 4.08042049407959,
        "learning_rate": 0.00019457330341970206,
        "epoch": 0.36977420076011625,
        "step": 4962
    },
    {
        "loss": 2.3377,
        "grad_norm": 3.2254748344421387,
        "learning_rate": 0.0001945504150598488,
        "epoch": 0.369848721961398,
        "step": 4963
    },
    {
        "loss": 1.9261,
        "grad_norm": 1.3787449598312378,
        "learning_rate": 0.00019452747988498917,
        "epoch": 0.36992324316267977,
        "step": 4964
    },
    {
        "loss": 2.9482,
        "grad_norm": 2.757493734359741,
        "learning_rate": 0.00019450449790647895,
        "epoch": 0.36999776436396153,
        "step": 4965
    },
    {
        "loss": 2.9987,
        "grad_norm": 2.49065899848938,
        "learning_rate": 0.00019448146913569745,
        "epoch": 0.3700722855652433,
        "step": 4966
    },
    {
        "loss": 2.6462,
        "grad_norm": 2.4460155963897705,
        "learning_rate": 0.0001944583935840469,
        "epoch": 0.37014680676652506,
        "step": 4967
    },
    {
        "loss": 0.946,
        "grad_norm": 2.8226633071899414,
        "learning_rate": 0.00019443527126295274,
        "epoch": 0.3702213279678068,
        "step": 4968
    },
    {
        "loss": 2.7842,
        "grad_norm": 2.410606622695923,
        "learning_rate": 0.00019441210218386362,
        "epoch": 0.3702958491690886,
        "step": 4969
    },
    {
        "loss": 2.6764,
        "grad_norm": 1.9123932123184204,
        "learning_rate": 0.00019438888635825134,
        "epoch": 0.37037037037037035,
        "step": 4970
    },
    {
        "loss": 2.1408,
        "grad_norm": 4.46937894821167,
        "learning_rate": 0.00019436562379761076,
        "epoch": 0.3704448915716521,
        "step": 4971
    },
    {
        "loss": 2.382,
        "grad_norm": 2.8438920974731445,
        "learning_rate": 0.00019434231451345997,
        "epoch": 0.3705194127729339,
        "step": 4972
    },
    {
        "loss": 2.7277,
        "grad_norm": 2.571572780609131,
        "learning_rate": 0.00019431895851734012,
        "epoch": 0.37059393397421564,
        "step": 4973
    },
    {
        "loss": 2.3597,
        "grad_norm": 2.183295965194702,
        "learning_rate": 0.00019429555582081557,
        "epoch": 0.3706684551754974,
        "step": 4974
    },
    {
        "loss": 3.0052,
        "grad_norm": 3.503821849822998,
        "learning_rate": 0.00019427210643547374,
        "epoch": 0.3707429763767792,
        "step": 4975
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.125657320022583,
        "learning_rate": 0.00019424861037292516,
        "epoch": 0.370817497578061,
        "step": 4976
    },
    {
        "loss": 2.3437,
        "grad_norm": 3.107466697692871,
        "learning_rate": 0.00019422506764480354,
        "epoch": 0.37089201877934275,
        "step": 4977
    },
    {
        "loss": 2.6779,
        "grad_norm": 3.6856491565704346,
        "learning_rate": 0.0001942014782627657,
        "epoch": 0.3709665399806245,
        "step": 4978
    },
    {
        "loss": 2.6256,
        "grad_norm": 1.7387431859970093,
        "learning_rate": 0.00019417784223849143,
        "epoch": 0.3710410611819063,
        "step": 4979
    },
    {
        "loss": 2.3921,
        "grad_norm": 1.8639566898345947,
        "learning_rate": 0.00019415415958368374,
        "epoch": 0.37111558238318804,
        "step": 4980
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.8117196559906006,
        "learning_rate": 0.00019413043031006868,
        "epoch": 0.3711901035844698,
        "step": 4981
    },
    {
        "loss": 2.8057,
        "grad_norm": 1.9782536029815674,
        "learning_rate": 0.00019410665442939537,
        "epoch": 0.37126462478575156,
        "step": 4982
    },
    {
        "loss": 2.1178,
        "grad_norm": 3.344727039337158,
        "learning_rate": 0.0001940828319534361,
        "epoch": 0.3713391459870333,
        "step": 4983
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.4846620559692383,
        "learning_rate": 0.00019405896289398608,
        "epoch": 0.3714136671883151,
        "step": 4984
    },
    {
        "loss": 2.6872,
        "grad_norm": 2.2409088611602783,
        "learning_rate": 0.0001940350472628637,
        "epoch": 0.37148818838959685,
        "step": 4985
    },
    {
        "loss": 2.2242,
        "grad_norm": 4.2965497970581055,
        "learning_rate": 0.0001940110850719103,
        "epoch": 0.3715627095908786,
        "step": 4986
    },
    {
        "loss": 2.6853,
        "grad_norm": 5.1251959800720215,
        "learning_rate": 0.00019398707633299045,
        "epoch": 0.3716372307921604,
        "step": 4987
    },
    {
        "loss": 2.3543,
        "grad_norm": 3.3148109912872314,
        "learning_rate": 0.00019396302105799165,
        "epoch": 0.37171175199344214,
        "step": 4988
    },
    {
        "loss": 2.6175,
        "grad_norm": 2.6369681358337402,
        "learning_rate": 0.00019393891925882436,
        "epoch": 0.3717862731947239,
        "step": 4989
    },
    {
        "loss": 2.8369,
        "grad_norm": 2.859865188598633,
        "learning_rate": 0.0001939147709474223,
        "epoch": 0.37186079439600567,
        "step": 4990
    },
    {
        "loss": 2.5711,
        "grad_norm": 3.347240924835205,
        "learning_rate": 0.00019389057613574198,
        "epoch": 0.37193531559728743,
        "step": 4991
    },
    {
        "loss": 2.8315,
        "grad_norm": 4.361225605010986,
        "learning_rate": 0.0001938663348357631,
        "epoch": 0.3720098367985692,
        "step": 4992
    },
    {
        "loss": 1.9246,
        "grad_norm": 5.480679035186768,
        "learning_rate": 0.00019384204705948828,
        "epoch": 0.37208435799985096,
        "step": 4993
    },
    {
        "loss": 2.5867,
        "grad_norm": 2.9267494678497314,
        "learning_rate": 0.0001938177128189433,
        "epoch": 0.3721588792011327,
        "step": 4994
    },
    {
        "loss": 2.8655,
        "grad_norm": 2.7871816158294678,
        "learning_rate": 0.0001937933321261767,
        "epoch": 0.3722334004024145,
        "step": 4995
    },
    {
        "loss": 2.7105,
        "grad_norm": 2.1114652156829834,
        "learning_rate": 0.00019376890499326021,
        "epoch": 0.37230792160369625,
        "step": 4996
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.3209316730499268,
        "learning_rate": 0.00019374443143228856,
        "epoch": 0.372382442804978,
        "step": 4997
    },
    {
        "loss": 2.1408,
        "grad_norm": 3.182011127471924,
        "learning_rate": 0.00019371991145537934,
        "epoch": 0.37245696400625977,
        "step": 4998
    },
    {
        "loss": 1.6247,
        "grad_norm": 2.7305915355682373,
        "learning_rate": 0.0001936953450746732,
        "epoch": 0.37253148520754154,
        "step": 4999
    },
    {
        "loss": 2.2822,
        "grad_norm": 1.6576731204986572,
        "learning_rate": 0.00019367073230233382,
        "epoch": 0.3726060064088233,
        "step": 5000
    },
    {
        "loss": 2.7577,
        "grad_norm": 2.6721768379211426,
        "learning_rate": 0.0001936460731505477,
        "epoch": 0.37268052761010506,
        "step": 5001
    },
    {
        "loss": 1.8168,
        "grad_norm": 2.57757306098938,
        "learning_rate": 0.00019362136763152447,
        "epoch": 0.3727550488113868,
        "step": 5002
    },
    {
        "loss": 2.1312,
        "grad_norm": 3.854203224182129,
        "learning_rate": 0.00019359661575749662,
        "epoch": 0.3728295700126686,
        "step": 5003
    },
    {
        "loss": 2.7297,
        "grad_norm": 2.246695041656494,
        "learning_rate": 0.0001935718175407196,
        "epoch": 0.37290409121395035,
        "step": 5004
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.1447184085845947,
        "learning_rate": 0.0001935469729934718,
        "epoch": 0.3729786124152321,
        "step": 5005
    },
    {
        "loss": 2.1067,
        "grad_norm": 2.4705004692077637,
        "learning_rate": 0.0001935220821280546,
        "epoch": 0.3730531336165139,
        "step": 5006
    },
    {
        "loss": 2.5194,
        "grad_norm": 4.524463176727295,
        "learning_rate": 0.0001934971449567923,
        "epoch": 0.37312765481779564,
        "step": 5007
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.439126968383789,
        "learning_rate": 0.00019347216149203207,
        "epoch": 0.3732021760190774,
        "step": 5008
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.1144864559173584,
        "learning_rate": 0.00019344713174614404,
        "epoch": 0.37327669722035917,
        "step": 5009
    },
    {
        "loss": 2.9008,
        "grad_norm": 3.84903621673584,
        "learning_rate": 0.0001934220557315213,
        "epoch": 0.373351218421641,
        "step": 5010
    },
    {
        "loss": 2.0631,
        "grad_norm": 3.394914150238037,
        "learning_rate": 0.0001933969334605798,
        "epoch": 0.37342573962292275,
        "step": 5011
    },
    {
        "loss": 2.2989,
        "grad_norm": 2.6756227016448975,
        "learning_rate": 0.00019337176494575834,
        "epoch": 0.3735002608242045,
        "step": 5012
    },
    {
        "loss": 2.6503,
        "grad_norm": 1.8913248777389526,
        "learning_rate": 0.00019334655019951873,
        "epoch": 0.3735747820254863,
        "step": 5013
    },
    {
        "loss": 1.848,
        "grad_norm": 2.4209580421447754,
        "learning_rate": 0.0001933212892343456,
        "epoch": 0.37364930322676804,
        "step": 5014
    },
    {
        "loss": 2.8863,
        "grad_norm": 2.9981799125671387,
        "learning_rate": 0.00019329598206274648,
        "epoch": 0.3737238244280498,
        "step": 5015
    },
    {
        "loss": 2.7992,
        "grad_norm": 2.1117095947265625,
        "learning_rate": 0.00019327062869725178,
        "epoch": 0.37379834562933156,
        "step": 5016
    },
    {
        "loss": 1.9455,
        "grad_norm": 2.991426706314087,
        "learning_rate": 0.0001932452291504148,
        "epoch": 0.3738728668306133,
        "step": 5017
    },
    {
        "loss": 2.817,
        "grad_norm": 2.14009428024292,
        "learning_rate": 0.00019321978343481163,
        "epoch": 0.3739473880318951,
        "step": 5018
    },
    {
        "loss": 2.9337,
        "grad_norm": 3.008344888687134,
        "learning_rate": 0.00019319429156304136,
        "epoch": 0.37402190923317685,
        "step": 5019
    },
    {
        "loss": 2.7902,
        "grad_norm": 2.0062637329101562,
        "learning_rate": 0.00019316875354772578,
        "epoch": 0.3740964304344586,
        "step": 5020
    },
    {
        "loss": 2.2271,
        "grad_norm": 5.488974571228027,
        "learning_rate": 0.00019314316940150967,
        "epoch": 0.3741709516357404,
        "step": 5021
    },
    {
        "loss": 3.0024,
        "grad_norm": 3.5925729274749756,
        "learning_rate": 0.0001931175391370605,
        "epoch": 0.37424547283702214,
        "step": 5022
    },
    {
        "loss": 1.1121,
        "grad_norm": 4.644268989562988,
        "learning_rate": 0.00019309186276706867,
        "epoch": 0.3743199940383039,
        "step": 5023
    },
    {
        "loss": 2.7356,
        "grad_norm": 2.184865951538086,
        "learning_rate": 0.00019306614030424745,
        "epoch": 0.37439451523958567,
        "step": 5024
    },
    {
        "loss": 2.2104,
        "grad_norm": 2.306649684906006,
        "learning_rate": 0.0001930403717613328,
        "epoch": 0.37446903644086743,
        "step": 5025
    },
    {
        "loss": 1.9033,
        "grad_norm": 3.8295016288757324,
        "learning_rate": 0.00019301455715108363,
        "epoch": 0.3745435576421492,
        "step": 5026
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.7640485763549805,
        "learning_rate": 0.00019298869648628152,
        "epoch": 0.37461807884343096,
        "step": 5027
    },
    {
        "loss": 2.7271,
        "grad_norm": 2.6498939990997314,
        "learning_rate": 0.00019296278977973103,
        "epoch": 0.3746926000447127,
        "step": 5028
    },
    {
        "loss": 1.842,
        "grad_norm": 2.7848169803619385,
        "learning_rate": 0.00019293683704425934,
        "epoch": 0.3747671212459945,
        "step": 5029
    },
    {
        "loss": 2.7696,
        "grad_norm": 1.9201419353485107,
        "learning_rate": 0.00019291083829271657,
        "epoch": 0.37484164244727625,
        "step": 5030
    },
    {
        "loss": 2.3951,
        "grad_norm": 2.2465524673461914,
        "learning_rate": 0.0001928847935379755,
        "epoch": 0.374916163648558,
        "step": 5031
    },
    {
        "loss": 2.2885,
        "grad_norm": 2.6079845428466797,
        "learning_rate": 0.00019285870279293175,
        "epoch": 0.3749906848498398,
        "step": 5032
    },
    {
        "loss": 1.8836,
        "grad_norm": 4.017765998840332,
        "learning_rate": 0.0001928325660705037,
        "epoch": 0.37506520605112154,
        "step": 5033
    },
    {
        "loss": 2.4276,
        "grad_norm": 3.37650203704834,
        "learning_rate": 0.0001928063833836325,
        "epoch": 0.3751397272524033,
        "step": 5034
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.63718581199646,
        "learning_rate": 0.00019278015474528208,
        "epoch": 0.37521424845368506,
        "step": 5035
    },
    {
        "loss": 2.1046,
        "grad_norm": 2.731626033782959,
        "learning_rate": 0.00019275388016843904,
        "epoch": 0.3752887696549668,
        "step": 5036
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.8673956394195557,
        "learning_rate": 0.00019272755966611284,
        "epoch": 0.3753632908562486,
        "step": 5037
    },
    {
        "loss": 2.6278,
        "grad_norm": 2.26764178276062,
        "learning_rate": 0.00019270119325133557,
        "epoch": 0.37543781205753035,
        "step": 5038
    },
    {
        "loss": 2.7543,
        "grad_norm": 2.648895025253296,
        "learning_rate": 0.00019267478093716218,
        "epoch": 0.3755123332588121,
        "step": 5039
    },
    {
        "loss": 2.4868,
        "grad_norm": 2.5726430416107178,
        "learning_rate": 0.00019264832273667025,
        "epoch": 0.3755868544600939,
        "step": 5040
    },
    {
        "loss": 2.4843,
        "grad_norm": 3.9098196029663086,
        "learning_rate": 0.00019262181866296004,
        "epoch": 0.37566137566137564,
        "step": 5041
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.989144802093506,
        "learning_rate": 0.00019259526872915465,
        "epoch": 0.3757358968626574,
        "step": 5042
    },
    {
        "loss": 2.7742,
        "grad_norm": 2.4423866271972656,
        "learning_rate": 0.00019256867294839975,
        "epoch": 0.37581041806393917,
        "step": 5043
    },
    {
        "loss": 2.1329,
        "grad_norm": 3.490558624267578,
        "learning_rate": 0.00019254203133386392,
        "epoch": 0.37588493926522093,
        "step": 5044
    },
    {
        "loss": 1.9736,
        "grad_norm": 2.839068651199341,
        "learning_rate": 0.00019251534389873816,
        "epoch": 0.3759594604665027,
        "step": 5045
    },
    {
        "loss": 2.5973,
        "grad_norm": 2.8233449459075928,
        "learning_rate": 0.0001924886106562363,
        "epoch": 0.3760339816677845,
        "step": 5046
    },
    {
        "loss": 2.6915,
        "grad_norm": 2.372422933578491,
        "learning_rate": 0.00019246183161959488,
        "epoch": 0.3761085028690663,
        "step": 5047
    },
    {
        "loss": 2.5536,
        "grad_norm": 4.4023613929748535,
        "learning_rate": 0.0001924350068020731,
        "epoch": 0.37618302407034804,
        "step": 5048
    },
    {
        "loss": 2.5662,
        "grad_norm": 2.5235507488250732,
        "learning_rate": 0.00019240813621695282,
        "epoch": 0.3762575452716298,
        "step": 5049
    },
    {
        "loss": 2.9811,
        "grad_norm": 4.548243522644043,
        "learning_rate": 0.00019238121987753844,
        "epoch": 0.37633206647291156,
        "step": 5050
    },
    {
        "loss": 2.5274,
        "grad_norm": 3.508845806121826,
        "learning_rate": 0.0001923542577971573,
        "epoch": 0.3764065876741933,
        "step": 5051
    },
    {
        "loss": 1.5038,
        "grad_norm": 2.8221960067749023,
        "learning_rate": 0.00019232724998915902,
        "epoch": 0.3764811088754751,
        "step": 5052
    },
    {
        "loss": 1.8465,
        "grad_norm": 3.098038673400879,
        "learning_rate": 0.00019230019646691613,
        "epoch": 0.37655563007675685,
        "step": 5053
    },
    {
        "loss": 2.8083,
        "grad_norm": 2.9628522396087646,
        "learning_rate": 0.00019227309724382375,
        "epoch": 0.3766301512780386,
        "step": 5054
    },
    {
        "loss": 2.4179,
        "grad_norm": 2.28408145904541,
        "learning_rate": 0.00019224595233329955,
        "epoch": 0.3767046724793204,
        "step": 5055
    },
    {
        "loss": 2.5048,
        "grad_norm": 2.692948579788208,
        "learning_rate": 0.00019221876174878386,
        "epoch": 0.37677919368060214,
        "step": 5056
    },
    {
        "loss": 2.5968,
        "grad_norm": 2.2118873596191406,
        "learning_rate": 0.00019219152550373965,
        "epoch": 0.3768537148818839,
        "step": 5057
    },
    {
        "loss": 1.8979,
        "grad_norm": 3.2081563472747803,
        "learning_rate": 0.00019216424361165246,
        "epoch": 0.37692823608316567,
        "step": 5058
    },
    {
        "loss": 2.8549,
        "grad_norm": 2.649881362915039,
        "learning_rate": 0.00019213691608603047,
        "epoch": 0.37700275728444743,
        "step": 5059
    },
    {
        "loss": 2.7231,
        "grad_norm": 2.33713698387146,
        "learning_rate": 0.00019210954294040437,
        "epoch": 0.3770772784857292,
        "step": 5060
    },
    {
        "loss": 1.6849,
        "grad_norm": 3.867389440536499,
        "learning_rate": 0.00019208212418832764,
        "epoch": 0.37715179968701096,
        "step": 5061
    },
    {
        "loss": 2.4019,
        "grad_norm": 5.719390392303467,
        "learning_rate": 0.00019205465984337603,
        "epoch": 0.3772263208882927,
        "step": 5062
    },
    {
        "loss": 2.9117,
        "grad_norm": 2.2314743995666504,
        "learning_rate": 0.00019202714991914816,
        "epoch": 0.3773008420895745,
        "step": 5063
    },
    {
        "loss": 2.705,
        "grad_norm": 2.538726568222046,
        "learning_rate": 0.00019199959442926505,
        "epoch": 0.37737536329085625,
        "step": 5064
    },
    {
        "loss": 2.3513,
        "grad_norm": 3.943758249282837,
        "learning_rate": 0.00019197199338737034,
        "epoch": 0.377449884492138,
        "step": 5065
    },
    {
        "loss": 2.6266,
        "grad_norm": 2.690908193588257,
        "learning_rate": 0.00019194434680713015,
        "epoch": 0.3775244056934198,
        "step": 5066
    },
    {
        "loss": 2.4592,
        "grad_norm": 2.0423741340637207,
        "learning_rate": 0.00019191665470223327,
        "epoch": 0.37759892689470154,
        "step": 5067
    },
    {
        "loss": 2.3225,
        "grad_norm": 3.8024652004241943,
        "learning_rate": 0.000191888917086391,
        "epoch": 0.3776734480959833,
        "step": 5068
    },
    {
        "loss": 2.5761,
        "grad_norm": 3.4655978679656982,
        "learning_rate": 0.00019186113397333702,
        "epoch": 0.37774796929726506,
        "step": 5069
    },
    {
        "loss": 2.323,
        "grad_norm": 2.1518139839172363,
        "learning_rate": 0.00019183330537682775,
        "epoch": 0.3778224904985468,
        "step": 5070
    },
    {
        "loss": 1.501,
        "grad_norm": 4.892368316650391,
        "learning_rate": 0.0001918054313106421,
        "epoch": 0.3778970116998286,
        "step": 5071
    },
    {
        "loss": 2.6965,
        "grad_norm": 1.9857457876205444,
        "learning_rate": 0.00019177751178858126,
        "epoch": 0.37797153290111035,
        "step": 5072
    },
    {
        "loss": 2.6177,
        "grad_norm": 2.2429068088531494,
        "learning_rate": 0.00019174954682446926,
        "epoch": 0.3780460541023921,
        "step": 5073
    },
    {
        "loss": 2.2874,
        "grad_norm": 2.9367196559906006,
        "learning_rate": 0.0001917215364321524,
        "epoch": 0.3781205753036739,
        "step": 5074
    },
    {
        "loss": 2.308,
        "grad_norm": 3.1548819541931152,
        "learning_rate": 0.00019169348062549952,
        "epoch": 0.37819509650495564,
        "step": 5075
    },
    {
        "loss": 2.715,
        "grad_norm": 2.411386489868164,
        "learning_rate": 0.00019166537941840196,
        "epoch": 0.3782696177062374,
        "step": 5076
    },
    {
        "loss": 2.0947,
        "grad_norm": 3.3536155223846436,
        "learning_rate": 0.00019163723282477366,
        "epoch": 0.37834413890751917,
        "step": 5077
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.3441355228424072,
        "learning_rate": 0.00019160904085855078,
        "epoch": 0.37841866010880093,
        "step": 5078
    },
    {
        "loss": 2.7381,
        "grad_norm": 3.6742632389068604,
        "learning_rate": 0.00019158080353369217,
        "epoch": 0.3784931813100827,
        "step": 5079
    },
    {
        "loss": 2.4773,
        "grad_norm": 2.3462131023406982,
        "learning_rate": 0.00019155252086417904,
        "epoch": 0.37856770251136446,
        "step": 5080
    },
    {
        "loss": 2.2736,
        "grad_norm": 2.588024854660034,
        "learning_rate": 0.00019152419286401506,
        "epoch": 0.3786422237126463,
        "step": 5081
    },
    {
        "loss": 1.941,
        "grad_norm": 4.549804210662842,
        "learning_rate": 0.00019149581954722638,
        "epoch": 0.37871674491392804,
        "step": 5082
    },
    {
        "loss": 2.4985,
        "grad_norm": 3.0086703300476074,
        "learning_rate": 0.00019146740092786146,
        "epoch": 0.3787912661152098,
        "step": 5083
    },
    {
        "loss": 2.7529,
        "grad_norm": 2.0449323654174805,
        "learning_rate": 0.00019143893701999138,
        "epoch": 0.37886578731649156,
        "step": 5084
    },
    {
        "loss": 2.5368,
        "grad_norm": 3.1748900413513184,
        "learning_rate": 0.00019141042783770958,
        "epoch": 0.3789403085177733,
        "step": 5085
    },
    {
        "loss": 2.1886,
        "grad_norm": 3.626584053039551,
        "learning_rate": 0.00019138187339513176,
        "epoch": 0.3790148297190551,
        "step": 5086
    },
    {
        "loss": 2.844,
        "grad_norm": 2.1368279457092285,
        "learning_rate": 0.0001913532737063963,
        "epoch": 0.37908935092033685,
        "step": 5087
    },
    {
        "loss": 1.0746,
        "grad_norm": 3.6931726932525635,
        "learning_rate": 0.00019132462878566374,
        "epoch": 0.3791638721216186,
        "step": 5088
    },
    {
        "loss": 3.1303,
        "grad_norm": 2.743224859237671,
        "learning_rate": 0.00019129593864711716,
        "epoch": 0.3792383933229004,
        "step": 5089
    },
    {
        "loss": 2.5779,
        "grad_norm": 2.9733128547668457,
        "learning_rate": 0.000191267203304962,
        "epoch": 0.37931291452418214,
        "step": 5090
    },
    {
        "loss": 2.653,
        "grad_norm": 2.746730327606201,
        "learning_rate": 0.00019123842277342606,
        "epoch": 0.3793874357254639,
        "step": 5091
    },
    {
        "loss": 2.3425,
        "grad_norm": 4.113101482391357,
        "learning_rate": 0.00019120959706675955,
        "epoch": 0.37946195692674567,
        "step": 5092
    },
    {
        "loss": 2.1459,
        "grad_norm": 3.699021100997925,
        "learning_rate": 0.00019118072619923494,
        "epoch": 0.37953647812802743,
        "step": 5093
    },
    {
        "loss": 2.404,
        "grad_norm": 3.4729676246643066,
        "learning_rate": 0.0001911518101851472,
        "epoch": 0.3796109993293092,
        "step": 5094
    },
    {
        "loss": 2.7912,
        "grad_norm": 2.1968443393707275,
        "learning_rate": 0.0001911228490388136,
        "epoch": 0.37968552053059096,
        "step": 5095
    },
    {
        "loss": 2.821,
        "grad_norm": 2.526848077774048,
        "learning_rate": 0.00019109384277457374,
        "epoch": 0.3797600417318727,
        "step": 5096
    },
    {
        "loss": 2.9581,
        "grad_norm": 3.2793076038360596,
        "learning_rate": 0.00019106479140678953,
        "epoch": 0.3798345629331545,
        "step": 5097
    },
    {
        "loss": 2.3702,
        "grad_norm": 2.670006275177002,
        "learning_rate": 0.0001910356949498453,
        "epoch": 0.37990908413443625,
        "step": 5098
    },
    {
        "loss": 2.4948,
        "grad_norm": 3.704479932785034,
        "learning_rate": 0.00019100655341814767,
        "epoch": 0.379983605335718,
        "step": 5099
    },
    {
        "loss": 1.9127,
        "grad_norm": 3.575134038925171,
        "learning_rate": 0.00019097736682612554,
        "epoch": 0.3800581265369998,
        "step": 5100
    },
    {
        "loss": 2.2523,
        "grad_norm": 4.454570770263672,
        "learning_rate": 0.00019094813518823021,
        "epoch": 0.38013264773828154,
        "step": 5101
    },
    {
        "loss": 2.6207,
        "grad_norm": 3.0382065773010254,
        "learning_rate": 0.0001909188585189351,
        "epoch": 0.3802071689395633,
        "step": 5102
    },
    {
        "loss": 1.667,
        "grad_norm": 3.170898914337158,
        "learning_rate": 0.00019088953683273614,
        "epoch": 0.38028169014084506,
        "step": 5103
    },
    {
        "loss": 2.2303,
        "grad_norm": 2.6801106929779053,
        "learning_rate": 0.00019086017014415143,
        "epoch": 0.3803562113421268,
        "step": 5104
    },
    {
        "loss": 2.7068,
        "grad_norm": 2.754307270050049,
        "learning_rate": 0.0001908307584677214,
        "epoch": 0.3804307325434086,
        "step": 5105
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.358826160430908,
        "learning_rate": 0.00019080130181800867,
        "epoch": 0.38050525374469035,
        "step": 5106
    },
    {
        "loss": 2.5568,
        "grad_norm": 3.20849347114563,
        "learning_rate": 0.00019077180020959827,
        "epoch": 0.3805797749459721,
        "step": 5107
    },
    {
        "loss": 3.0957,
        "grad_norm": 2.5086309909820557,
        "learning_rate": 0.00019074225365709735,
        "epoch": 0.3806542961472539,
        "step": 5108
    },
    {
        "loss": 2.6094,
        "grad_norm": 3.318991184234619,
        "learning_rate": 0.00019071266217513542,
        "epoch": 0.38072881734853564,
        "step": 5109
    },
    {
        "loss": 2.28,
        "grad_norm": 2.869262456893921,
        "learning_rate": 0.0001906830257783642,
        "epoch": 0.3808033385498174,
        "step": 5110
    },
    {
        "loss": 2.5634,
        "grad_norm": 2.6343212127685547,
        "learning_rate": 0.00019065334448145757,
        "epoch": 0.38087785975109917,
        "step": 5111
    },
    {
        "loss": 2.2692,
        "grad_norm": 3.091233730316162,
        "learning_rate": 0.00019062361829911181,
        "epoch": 0.38095238095238093,
        "step": 5112
    },
    {
        "loss": 2.1233,
        "grad_norm": 3.6915442943573,
        "learning_rate": 0.00019059384724604524,
        "epoch": 0.3810269021536627,
        "step": 5113
    },
    {
        "loss": 2.4281,
        "grad_norm": 2.684969663619995,
        "learning_rate": 0.0001905640313369985,
        "epoch": 0.38110142335494446,
        "step": 5114
    },
    {
        "loss": 2.2541,
        "grad_norm": 3.0958499908447266,
        "learning_rate": 0.00019053417058673448,
        "epoch": 0.3811759445562262,
        "step": 5115
    },
    {
        "loss": 1.9114,
        "grad_norm": 4.5541839599609375,
        "learning_rate": 0.00019050426501003817,
        "epoch": 0.381250465757508,
        "step": 5116
    },
    {
        "loss": 2.7924,
        "grad_norm": 2.3580610752105713,
        "learning_rate": 0.00019047431462171677,
        "epoch": 0.3813249869587898,
        "step": 5117
    },
    {
        "loss": 2.7422,
        "grad_norm": 3.1774563789367676,
        "learning_rate": 0.00019044431943659977,
        "epoch": 0.38139950816007157,
        "step": 5118
    },
    {
        "loss": 2.5652,
        "grad_norm": 2.9691896438598633,
        "learning_rate": 0.0001904142794695387,
        "epoch": 0.38147402936135333,
        "step": 5119
    },
    {
        "loss": 2.4443,
        "grad_norm": 2.8775222301483154,
        "learning_rate": 0.00019038419473540742,
        "epoch": 0.3815485505626351,
        "step": 5120
    },
    {
        "loss": 2.2806,
        "grad_norm": 3.08004093170166,
        "learning_rate": 0.00019035406524910172,
        "epoch": 0.38162307176391685,
        "step": 5121
    },
    {
        "loss": 2.0393,
        "grad_norm": 3.6784276962280273,
        "learning_rate": 0.0001903238910255399,
        "epoch": 0.3816975929651986,
        "step": 5122
    },
    {
        "loss": 2.7924,
        "grad_norm": 3.990870714187622,
        "learning_rate": 0.000190293672079662,
        "epoch": 0.3817721141664804,
        "step": 5123
    },
    {
        "loss": 1.8822,
        "grad_norm": 6.157779693603516,
        "learning_rate": 0.00019026340842643057,
        "epoch": 0.38184663536776214,
        "step": 5124
    },
    {
        "loss": 2.6268,
        "grad_norm": 3.016972780227661,
        "learning_rate": 0.00019023310008082998,
        "epoch": 0.3819211565690439,
        "step": 5125
    },
    {
        "loss": 2.4126,
        "grad_norm": 3.2021350860595703,
        "learning_rate": 0.00019020274705786706,
        "epoch": 0.38199567777032567,
        "step": 5126
    },
    {
        "loss": 1.4918,
        "grad_norm": 3.9780309200286865,
        "learning_rate": 0.00019017234937257043,
        "epoch": 0.38207019897160743,
        "step": 5127
    },
    {
        "loss": 2.5609,
        "grad_norm": 1.9342765808105469,
        "learning_rate": 0.00019014190703999108,
        "epoch": 0.3821447201728892,
        "step": 5128
    },
    {
        "loss": 2.3045,
        "grad_norm": 2.2720401287078857,
        "learning_rate": 0.00019011142007520197,
        "epoch": 0.38221924137417096,
        "step": 5129
    },
    {
        "loss": 2.2648,
        "grad_norm": 2.4789717197418213,
        "learning_rate": 0.0001900808884932982,
        "epoch": 0.3822937625754527,
        "step": 5130
    },
    {
        "loss": 2.4294,
        "grad_norm": 3.279802083969116,
        "learning_rate": 0.00019005031230939694,
        "epoch": 0.3823682837767345,
        "step": 5131
    },
    {
        "loss": 2.5522,
        "grad_norm": 3.3983747959136963,
        "learning_rate": 0.00019001969153863748,
        "epoch": 0.38244280497801625,
        "step": 5132
    },
    {
        "loss": 1.2033,
        "grad_norm": 2.3068387508392334,
        "learning_rate": 0.00018998902619618116,
        "epoch": 0.382517326179298,
        "step": 5133
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.9557151794433594,
        "learning_rate": 0.0001899583162972114,
        "epoch": 0.3825918473805798,
        "step": 5134
    },
    {
        "loss": 1.8788,
        "grad_norm": 2.4402806758880615,
        "learning_rate": 0.00018992756185693365,
        "epoch": 0.38266636858186154,
        "step": 5135
    },
    {
        "loss": 1.9822,
        "grad_norm": 2.7423219680786133,
        "learning_rate": 0.00018989676289057545,
        "epoch": 0.3827408897831433,
        "step": 5136
    },
    {
        "loss": 1.8694,
        "grad_norm": 2.79750394821167,
        "learning_rate": 0.0001898659194133864,
        "epoch": 0.38281541098442506,
        "step": 5137
    },
    {
        "loss": 2.6477,
        "grad_norm": 3.7685749530792236,
        "learning_rate": 0.0001898350314406381,
        "epoch": 0.3828899321857068,
        "step": 5138
    },
    {
        "loss": 3.2271,
        "grad_norm": 3.6273186206817627,
        "learning_rate": 0.00018980409898762424,
        "epoch": 0.3829644533869886,
        "step": 5139
    },
    {
        "loss": 2.2846,
        "grad_norm": 3.7516956329345703,
        "learning_rate": 0.0001897731220696604,
        "epoch": 0.38303897458827035,
        "step": 5140
    },
    {
        "loss": 1.9574,
        "grad_norm": 2.4787185192108154,
        "learning_rate": 0.00018974210070208433,
        "epoch": 0.3831134957895521,
        "step": 5141
    },
    {
        "loss": 2.2386,
        "grad_norm": 3.4266326427459717,
        "learning_rate": 0.0001897110349002557,
        "epoch": 0.3831880169908339,
        "step": 5142
    },
    {
        "loss": 2.7439,
        "grad_norm": 2.1603963375091553,
        "learning_rate": 0.00018967992467955626,
        "epoch": 0.38326253819211564,
        "step": 5143
    },
    {
        "loss": 1.8625,
        "grad_norm": 2.0181334018707275,
        "learning_rate": 0.00018964877005538962,
        "epoch": 0.3833370593933974,
        "step": 5144
    },
    {
        "loss": 2.7532,
        "grad_norm": 2.844306230545044,
        "learning_rate": 0.00018961757104318146,
        "epoch": 0.38341158059467917,
        "step": 5145
    },
    {
        "loss": 2.5153,
        "grad_norm": 2.8222992420196533,
        "learning_rate": 0.00018958632765837954,
        "epoch": 0.38348610179596093,
        "step": 5146
    },
    {
        "loss": 2.8581,
        "grad_norm": 2.812974691390991,
        "learning_rate": 0.00018955503991645336,
        "epoch": 0.3835606229972427,
        "step": 5147
    },
    {
        "loss": 1.5843,
        "grad_norm": 4.528482437133789,
        "learning_rate": 0.00018952370783289455,
        "epoch": 0.38363514419852446,
        "step": 5148
    },
    {
        "loss": 2.226,
        "grad_norm": 3.331080675125122,
        "learning_rate": 0.00018949233142321665,
        "epoch": 0.3837096653998062,
        "step": 5149
    },
    {
        "loss": 2.8473,
        "grad_norm": 2.0444223880767822,
        "learning_rate": 0.00018946091070295515,
        "epoch": 0.383784186601088,
        "step": 5150
    },
    {
        "loss": 2.4666,
        "grad_norm": 2.161190986633301,
        "learning_rate": 0.00018942944568766748,
        "epoch": 0.38385870780236975,
        "step": 5151
    },
    {
        "loss": 2.6391,
        "grad_norm": 2.6393885612487793,
        "learning_rate": 0.000189397936392933,
        "epoch": 0.38393322900365157,
        "step": 5152
    },
    {
        "loss": 2.7851,
        "grad_norm": 2.2599635124206543,
        "learning_rate": 0.000189366382834353,
        "epoch": 0.38400775020493333,
        "step": 5153
    },
    {
        "loss": 2.1315,
        "grad_norm": 2.160470724105835,
        "learning_rate": 0.00018933478502755066,
        "epoch": 0.3840822714062151,
        "step": 5154
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.2180843353271484,
        "learning_rate": 0.00018930314298817104,
        "epoch": 0.38415679260749686,
        "step": 5155
    },
    {
        "loss": 2.6722,
        "grad_norm": 4.4529924392700195,
        "learning_rate": 0.00018927145673188132,
        "epoch": 0.3842313138087786,
        "step": 5156
    },
    {
        "loss": 2.094,
        "grad_norm": 4.474613189697266,
        "learning_rate": 0.00018923972627437024,
        "epoch": 0.3843058350100604,
        "step": 5157
    },
    {
        "loss": 2.2637,
        "grad_norm": 2.5246167182922363,
        "learning_rate": 0.0001892079516313486,
        "epoch": 0.38438035621134214,
        "step": 5158
    },
    {
        "loss": 2.3139,
        "grad_norm": 3.1028528213500977,
        "learning_rate": 0.00018917613281854915,
        "epoch": 0.3844548774126239,
        "step": 5159
    },
    {
        "loss": 1.8911,
        "grad_norm": 3.2929775714874268,
        "learning_rate": 0.00018914426985172634,
        "epoch": 0.38452939861390567,
        "step": 5160
    },
    {
        "loss": 2.0892,
        "grad_norm": 3.149489164352417,
        "learning_rate": 0.00018911236274665662,
        "epoch": 0.38460391981518743,
        "step": 5161
    },
    {
        "loss": 2.6692,
        "grad_norm": 2.6246955394744873,
        "learning_rate": 0.0001890804115191383,
        "epoch": 0.3846784410164692,
        "step": 5162
    },
    {
        "loss": 2.1096,
        "grad_norm": 3.2903215885162354,
        "learning_rate": 0.00018904841618499137,
        "epoch": 0.38475296221775096,
        "step": 5163
    },
    {
        "loss": 2.6028,
        "grad_norm": 3.4073076248168945,
        "learning_rate": 0.0001890163767600578,
        "epoch": 0.3848274834190327,
        "step": 5164
    },
    {
        "loss": 1.6343,
        "grad_norm": 5.669996738433838,
        "learning_rate": 0.00018898429326020134,
        "epoch": 0.3849020046203145,
        "step": 5165
    },
    {
        "loss": 2.2485,
        "grad_norm": 3.6389646530151367,
        "learning_rate": 0.00018895216570130767,
        "epoch": 0.38497652582159625,
        "step": 5166
    },
    {
        "loss": 1.9463,
        "grad_norm": 4.09474515914917,
        "learning_rate": 0.0001889199940992841,
        "epoch": 0.385051047022878,
        "step": 5167
    },
    {
        "loss": 1.1467,
        "grad_norm": 4.946636199951172,
        "learning_rate": 0.00018888777847005987,
        "epoch": 0.3851255682241598,
        "step": 5168
    },
    {
        "loss": 2.201,
        "grad_norm": 3.7054378986358643,
        "learning_rate": 0.00018885551882958602,
        "epoch": 0.38520008942544154,
        "step": 5169
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.865670919418335,
        "learning_rate": 0.0001888232151938353,
        "epoch": 0.3852746106267233,
        "step": 5170
    },
    {
        "loss": 2.479,
        "grad_norm": 3.5616769790649414,
        "learning_rate": 0.0001887908675788024,
        "epoch": 0.38534913182800506,
        "step": 5171
    },
    {
        "loss": 2.6314,
        "grad_norm": 3.011381149291992,
        "learning_rate": 0.00018875847600050355,
        "epoch": 0.38542365302928683,
        "step": 5172
    },
    {
        "loss": 2.7353,
        "grad_norm": 2.6292693614959717,
        "learning_rate": 0.00018872604047497703,
        "epoch": 0.3854981742305686,
        "step": 5173
    },
    {
        "loss": 2.8104,
        "grad_norm": 2.6138737201690674,
        "learning_rate": 0.00018869356101828256,
        "epoch": 0.38557269543185035,
        "step": 5174
    },
    {
        "loss": 1.8002,
        "grad_norm": 3.7043540477752686,
        "learning_rate": 0.0001886610376465019,
        "epoch": 0.3856472166331321,
        "step": 5175
    },
    {
        "loss": 3.0813,
        "grad_norm": 3.737703800201416,
        "learning_rate": 0.00018862847037573842,
        "epoch": 0.3857217378344139,
        "step": 5176
    },
    {
        "loss": 2.1175,
        "grad_norm": 3.3340606689453125,
        "learning_rate": 0.00018859585922211726,
        "epoch": 0.38579625903569564,
        "step": 5177
    },
    {
        "loss": 2.7761,
        "grad_norm": 1.8363101482391357,
        "learning_rate": 0.00018856320420178522,
        "epoch": 0.3858707802369774,
        "step": 5178
    },
    {
        "loss": 1.4566,
        "grad_norm": 3.844787120819092,
        "learning_rate": 0.00018853050533091086,
        "epoch": 0.38594530143825917,
        "step": 5179
    },
    {
        "loss": 2.5599,
        "grad_norm": 3.695298194885254,
        "learning_rate": 0.00018849776262568448,
        "epoch": 0.38601982263954093,
        "step": 5180
    },
    {
        "loss": 1.4719,
        "grad_norm": 3.8007125854492188,
        "learning_rate": 0.00018846497610231812,
        "epoch": 0.3860943438408227,
        "step": 5181
    },
    {
        "loss": 2.3524,
        "grad_norm": 3.5229744911193848,
        "learning_rate": 0.0001884321457770454,
        "epoch": 0.38616886504210446,
        "step": 5182
    },
    {
        "loss": 2.0744,
        "grad_norm": 3.4227733612060547,
        "learning_rate": 0.00018839927166612174,
        "epoch": 0.3862433862433862,
        "step": 5183
    },
    {
        "loss": 2.5683,
        "grad_norm": 3.63189959526062,
        "learning_rate": 0.0001883663537858241,
        "epoch": 0.386317907444668,
        "step": 5184
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.512427806854248,
        "learning_rate": 0.0001883333921524513,
        "epoch": 0.38639242864594975,
        "step": 5185
    },
    {
        "loss": 2.6997,
        "grad_norm": 1.985187292098999,
        "learning_rate": 0.00018830038678232364,
        "epoch": 0.3864669498472315,
        "step": 5186
    },
    {
        "loss": 2.4884,
        "grad_norm": 2.990398406982422,
        "learning_rate": 0.00018826733769178325,
        "epoch": 0.3865414710485133,
        "step": 5187
    },
    {
        "loss": 2.2012,
        "grad_norm": 3.312061309814453,
        "learning_rate": 0.0001882342448971937,
        "epoch": 0.3866159922497951,
        "step": 5188
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.3040711879730225,
        "learning_rate": 0.00018820110841494042,
        "epoch": 0.38669051345107686,
        "step": 5189
    },
    {
        "loss": 2.3671,
        "grad_norm": 2.545116424560547,
        "learning_rate": 0.00018816792826143036,
        "epoch": 0.3867650346523586,
        "step": 5190
    },
    {
        "loss": 2.3581,
        "grad_norm": 2.5459187030792236,
        "learning_rate": 0.00018813470445309204,
        "epoch": 0.3868395558536404,
        "step": 5191
    },
    {
        "loss": 2.9481,
        "grad_norm": 2.774320363998413,
        "learning_rate": 0.00018810143700637565,
        "epoch": 0.38691407705492215,
        "step": 5192
    },
    {
        "loss": 2.1569,
        "grad_norm": 2.9093520641326904,
        "learning_rate": 0.00018806812593775307,
        "epoch": 0.3869885982562039,
        "step": 5193
    },
    {
        "loss": 2.8127,
        "grad_norm": 2.649839401245117,
        "learning_rate": 0.00018803477126371764,
        "epoch": 0.38706311945748567,
        "step": 5194
    },
    {
        "loss": 1.9264,
        "grad_norm": 5.249670028686523,
        "learning_rate": 0.0001880013730007844,
        "epoch": 0.38713764065876743,
        "step": 5195
    },
    {
        "loss": 2.7159,
        "grad_norm": 2.3176698684692383,
        "learning_rate": 0.0001879679311654899,
        "epoch": 0.3872121618600492,
        "step": 5196
    },
    {
        "loss": 2.5419,
        "grad_norm": 3.771763324737549,
        "learning_rate": 0.00018793444577439226,
        "epoch": 0.38728668306133096,
        "step": 5197
    },
    {
        "loss": 2.8938,
        "grad_norm": 3.7410693168640137,
        "learning_rate": 0.0001879009168440712,
        "epoch": 0.3873612042626127,
        "step": 5198
    },
    {
        "loss": 1.4367,
        "grad_norm": 3.5268890857696533,
        "learning_rate": 0.00018786734439112804,
        "epoch": 0.3874357254638945,
        "step": 5199
    },
    {
        "loss": 2.9882,
        "grad_norm": 4.929476737976074,
        "learning_rate": 0.00018783372843218552,
        "epoch": 0.38751024666517625,
        "step": 5200
    },
    {
        "loss": 1.7843,
        "grad_norm": 4.345337390899658,
        "learning_rate": 0.00018780006898388806,
        "epoch": 0.387584767866458,
        "step": 5201
    },
    {
        "loss": 2.2515,
        "grad_norm": 3.4684464931488037,
        "learning_rate": 0.00018776636606290155,
        "epoch": 0.3876592890677398,
        "step": 5202
    },
    {
        "loss": 2.633,
        "grad_norm": 2.582441806793213,
        "learning_rate": 0.00018773261968591342,
        "epoch": 0.38773381026902154,
        "step": 5203
    },
    {
        "loss": 2.1206,
        "grad_norm": 3.0923335552215576,
        "learning_rate": 0.0001876988298696326,
        "epoch": 0.3878083314703033,
        "step": 5204
    },
    {
        "loss": 2.3606,
        "grad_norm": 2.0300099849700928,
        "learning_rate": 0.00018766499663078952,
        "epoch": 0.38788285267158507,
        "step": 5205
    },
    {
        "loss": 1.8633,
        "grad_norm": 2.39154314994812,
        "learning_rate": 0.00018763111998613608,
        "epoch": 0.38795737387286683,
        "step": 5206
    },
    {
        "loss": 2.9685,
        "grad_norm": 2.6392905712127686,
        "learning_rate": 0.00018759719995244584,
        "epoch": 0.3880318950741486,
        "step": 5207
    },
    {
        "loss": 2.8724,
        "grad_norm": 2.3252780437469482,
        "learning_rate": 0.00018756323654651354,
        "epoch": 0.38810641627543035,
        "step": 5208
    },
    {
        "loss": 2.1921,
        "grad_norm": 2.3757381439208984,
        "learning_rate": 0.0001875292297851558,
        "epoch": 0.3881809374767121,
        "step": 5209
    },
    {
        "loss": 2.9549,
        "grad_norm": 2.7913098335266113,
        "learning_rate": 0.0001874951796852103,
        "epoch": 0.3882554586779939,
        "step": 5210
    },
    {
        "loss": 2.5641,
        "grad_norm": 3.795278549194336,
        "learning_rate": 0.00018746108626353637,
        "epoch": 0.38832997987927564,
        "step": 5211
    },
    {
        "loss": 2.0526,
        "grad_norm": 3.849564552307129,
        "learning_rate": 0.00018742694953701488,
        "epoch": 0.3884045010805574,
        "step": 5212
    },
    {
        "loss": 2.4496,
        "grad_norm": 2.502781629562378,
        "learning_rate": 0.00018739276952254802,
        "epoch": 0.38847902228183917,
        "step": 5213
    },
    {
        "loss": 2.0715,
        "grad_norm": 3.118907928466797,
        "learning_rate": 0.00018735854623705937,
        "epoch": 0.38855354348312093,
        "step": 5214
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.0407509803771973,
        "learning_rate": 0.00018732427969749403,
        "epoch": 0.3886280646844027,
        "step": 5215
    },
    {
        "loss": 2.4063,
        "grad_norm": 2.852958917617798,
        "learning_rate": 0.00018728996992081845,
        "epoch": 0.38870258588568446,
        "step": 5216
    },
    {
        "loss": 2.7323,
        "grad_norm": 3.0626087188720703,
        "learning_rate": 0.00018725561692402065,
        "epoch": 0.3887771070869662,
        "step": 5217
    },
    {
        "loss": 2.6942,
        "grad_norm": 2.497520685195923,
        "learning_rate": 0.0001872212207241098,
        "epoch": 0.388851628288248,
        "step": 5218
    },
    {
        "loss": 2.769,
        "grad_norm": 3.5186679363250732,
        "learning_rate": 0.0001871867813381166,
        "epoch": 0.38892614948952975,
        "step": 5219
    },
    {
        "loss": 2.5974,
        "grad_norm": 3.97514009475708,
        "learning_rate": 0.00018715229878309316,
        "epoch": 0.3890006706908115,
        "step": 5220
    },
    {
        "loss": 2.3932,
        "grad_norm": 3.320882558822632,
        "learning_rate": 0.00018711777307611285,
        "epoch": 0.3890751918920933,
        "step": 5221
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.776888847351074,
        "learning_rate": 0.00018708320423427058,
        "epoch": 0.38914971309337504,
        "step": 5222
    },
    {
        "loss": 2.0311,
        "grad_norm": 3.8596432209014893,
        "learning_rate": 0.00018704859227468247,
        "epoch": 0.38922423429465686,
        "step": 5223
    },
    {
        "loss": 2.6604,
        "grad_norm": 2.567793130874634,
        "learning_rate": 0.00018701393721448604,
        "epoch": 0.3892987554959386,
        "step": 5224
    },
    {
        "loss": 2.6737,
        "grad_norm": 2.787123441696167,
        "learning_rate": 0.00018697923907084007,
        "epoch": 0.3893732766972204,
        "step": 5225
    },
    {
        "loss": 2.3296,
        "grad_norm": 3.455612897872925,
        "learning_rate": 0.00018694449786092485,
        "epoch": 0.38944779789850215,
        "step": 5226
    },
    {
        "loss": 2.9396,
        "grad_norm": 3.0536093711853027,
        "learning_rate": 0.00018690971360194186,
        "epoch": 0.3895223190997839,
        "step": 5227
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.2927329540252686,
        "learning_rate": 0.00018687488631111392,
        "epoch": 0.38959684030106567,
        "step": 5228
    },
    {
        "loss": 1.6114,
        "grad_norm": 2.8635141849517822,
        "learning_rate": 0.00018684001600568513,
        "epoch": 0.38967136150234744,
        "step": 5229
    },
    {
        "loss": 2.4782,
        "grad_norm": 1.9627840518951416,
        "learning_rate": 0.00018680510270292094,
        "epoch": 0.3897458827036292,
        "step": 5230
    },
    {
        "loss": 2.2566,
        "grad_norm": 3.6197235584259033,
        "learning_rate": 0.0001867701464201081,
        "epoch": 0.38982040390491096,
        "step": 5231
    },
    {
        "loss": 1.8518,
        "grad_norm": 2.8951923847198486,
        "learning_rate": 0.0001867351471745546,
        "epoch": 0.3898949251061927,
        "step": 5232
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.877851963043213,
        "learning_rate": 0.0001867001049835897,
        "epoch": 0.3899694463074745,
        "step": 5233
    },
    {
        "loss": 2.8357,
        "grad_norm": 3.091594696044922,
        "learning_rate": 0.00018666501986456396,
        "epoch": 0.39004396750875625,
        "step": 5234
    },
    {
        "loss": 2.7313,
        "grad_norm": 2.2308907508850098,
        "learning_rate": 0.00018662989183484909,
        "epoch": 0.390118488710038,
        "step": 5235
    },
    {
        "loss": 2.2402,
        "grad_norm": 2.2960262298583984,
        "learning_rate": 0.0001865947209118382,
        "epoch": 0.3901930099113198,
        "step": 5236
    },
    {
        "loss": 2.1653,
        "grad_norm": 2.684912919998169,
        "learning_rate": 0.0001865595071129456,
        "epoch": 0.39026753111260154,
        "step": 5237
    },
    {
        "loss": 1.5509,
        "grad_norm": 4.117492198944092,
        "learning_rate": 0.00018652425045560673,
        "epoch": 0.3903420523138833,
        "step": 5238
    },
    {
        "loss": 2.5442,
        "grad_norm": 3.0806076526641846,
        "learning_rate": 0.0001864889509572783,
        "epoch": 0.39041657351516507,
        "step": 5239
    },
    {
        "loss": 2.7325,
        "grad_norm": 2.94490122795105,
        "learning_rate": 0.00018645360863543827,
        "epoch": 0.39049109471644683,
        "step": 5240
    },
    {
        "loss": 3.0647,
        "grad_norm": 4.014965057373047,
        "learning_rate": 0.0001864182235075858,
        "epoch": 0.3905656159177286,
        "step": 5241
    },
    {
        "loss": 2.6096,
        "grad_norm": 3.045869827270508,
        "learning_rate": 0.00018638279559124126,
        "epoch": 0.39064013711901036,
        "step": 5242
    },
    {
        "loss": 2.4188,
        "grad_norm": 4.012025356292725,
        "learning_rate": 0.0001863473249039461,
        "epoch": 0.3907146583202921,
        "step": 5243
    },
    {
        "loss": 1.6609,
        "grad_norm": 2.8576183319091797,
        "learning_rate": 0.00018631181146326305,
        "epoch": 0.3907891795215739,
        "step": 5244
    },
    {
        "loss": 2.4054,
        "grad_norm": 3.167355537414551,
        "learning_rate": 0.00018627625528677596,
        "epoch": 0.39086370072285564,
        "step": 5245
    },
    {
        "loss": 2.4263,
        "grad_norm": 2.346224546432495,
        "learning_rate": 0.00018624065639208988,
        "epoch": 0.3909382219241374,
        "step": 5246
    },
    {
        "loss": 2.7606,
        "grad_norm": 2.464263916015625,
        "learning_rate": 0.00018620501479683097,
        "epoch": 0.39101274312541917,
        "step": 5247
    },
    {
        "loss": 2.4583,
        "grad_norm": 1.896375060081482,
        "learning_rate": 0.0001861693305186466,
        "epoch": 0.39108726432670093,
        "step": 5248
    },
    {
        "loss": 2.5968,
        "grad_norm": 2.0577242374420166,
        "learning_rate": 0.00018613360357520512,
        "epoch": 0.3911617855279827,
        "step": 5249
    },
    {
        "loss": 1.5978,
        "grad_norm": 3.770174264907837,
        "learning_rate": 0.00018609783398419621,
        "epoch": 0.39123630672926446,
        "step": 5250
    },
    {
        "loss": 2.0035,
        "grad_norm": 1.7476924657821655,
        "learning_rate": 0.00018606202176333055,
        "epoch": 0.3913108279305462,
        "step": 5251
    },
    {
        "loss": 2.032,
        "grad_norm": 2.916868209838867,
        "learning_rate": 0.00018602616693033988,
        "epoch": 0.391385349131828,
        "step": 5252
    },
    {
        "loss": 3.0247,
        "grad_norm": 3.7624917030334473,
        "learning_rate": 0.00018599026950297714,
        "epoch": 0.39145987033310975,
        "step": 5253
    },
    {
        "loss": 2.7397,
        "grad_norm": 2.545044422149658,
        "learning_rate": 0.0001859543294990164,
        "epoch": 0.3915343915343915,
        "step": 5254
    },
    {
        "loss": 1.8339,
        "grad_norm": 3.999976396560669,
        "learning_rate": 0.00018591834693625257,
        "epoch": 0.3916089127356733,
        "step": 5255
    },
    {
        "loss": 2.3301,
        "grad_norm": 2.9992079734802246,
        "learning_rate": 0.00018588232183250195,
        "epoch": 0.39168343393695504,
        "step": 5256
    },
    {
        "loss": 2.7657,
        "grad_norm": 2.4553468227386475,
        "learning_rate": 0.00018584625420560165,
        "epoch": 0.3917579551382368,
        "step": 5257
    },
    {
        "loss": 2.717,
        "grad_norm": 2.1192405223846436,
        "learning_rate": 0.00018581014407341,
        "epoch": 0.3918324763395186,
        "step": 5258
    },
    {
        "loss": 2.3055,
        "grad_norm": 2.354337453842163,
        "learning_rate": 0.00018577399145380623,
        "epoch": 0.3919069975408004,
        "step": 5259
    },
    {
        "loss": 2.716,
        "grad_norm": 2.4549686908721924,
        "learning_rate": 0.00018573779636469072,
        "epoch": 0.39198151874208215,
        "step": 5260
    },
    {
        "loss": 2.0139,
        "grad_norm": 3.239901065826416,
        "learning_rate": 0.0001857015588239849,
        "epoch": 0.3920560399433639,
        "step": 5261
    },
    {
        "loss": 2.6007,
        "grad_norm": 4.3087310791015625,
        "learning_rate": 0.0001856652788496311,
        "epoch": 0.3921305611446457,
        "step": 5262
    },
    {
        "loss": 2.5263,
        "grad_norm": 4.6524152755737305,
        "learning_rate": 0.00018562895645959273,
        "epoch": 0.39220508234592744,
        "step": 5263
    },
    {
        "loss": 2.1992,
        "grad_norm": 3.066307306289673,
        "learning_rate": 0.00018559259167185422,
        "epoch": 0.3922796035472092,
        "step": 5264
    },
    {
        "loss": 2.3712,
        "grad_norm": 3.0543010234832764,
        "learning_rate": 0.00018555618450442096,
        "epoch": 0.39235412474849096,
        "step": 5265
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.4220163822174072,
        "learning_rate": 0.0001855197349753193,
        "epoch": 0.3924286459497727,
        "step": 5266
    },
    {
        "loss": 1.8098,
        "grad_norm": 2.382538318634033,
        "learning_rate": 0.0001854832431025966,
        "epoch": 0.3925031671510545,
        "step": 5267
    },
    {
        "loss": 1.7996,
        "grad_norm": 2.829702377319336,
        "learning_rate": 0.00018544670890432126,
        "epoch": 0.39257768835233625,
        "step": 5268
    },
    {
        "loss": 2.732,
        "grad_norm": 2.2689154148101807,
        "learning_rate": 0.00018541013239858244,
        "epoch": 0.392652209553618,
        "step": 5269
    },
    {
        "loss": 2.2572,
        "grad_norm": 4.784815788269043,
        "learning_rate": 0.00018537351360349046,
        "epoch": 0.3927267307548998,
        "step": 5270
    },
    {
        "loss": 2.4558,
        "grad_norm": 3.4819746017456055,
        "learning_rate": 0.00018533685253717644,
        "epoch": 0.39280125195618154,
        "step": 5271
    },
    {
        "loss": 2.2074,
        "grad_norm": 4.846090316772461,
        "learning_rate": 0.00018530014921779245,
        "epoch": 0.3928757731574633,
        "step": 5272
    },
    {
        "loss": 2.187,
        "grad_norm": 4.195381164550781,
        "learning_rate": 0.00018526340366351157,
        "epoch": 0.39295029435874507,
        "step": 5273
    },
    {
        "loss": 2.2567,
        "grad_norm": 3.012145519256592,
        "learning_rate": 0.00018522661589252768,
        "epoch": 0.39302481556002683,
        "step": 5274
    },
    {
        "loss": 2.894,
        "grad_norm": 2.2867605686187744,
        "learning_rate": 0.00018518978592305566,
        "epoch": 0.3930993367613086,
        "step": 5275
    },
    {
        "loss": 2.4177,
        "grad_norm": 2.229764223098755,
        "learning_rate": 0.00018515291377333115,
        "epoch": 0.39317385796259036,
        "step": 5276
    },
    {
        "loss": 2.8216,
        "grad_norm": 2.7991743087768555,
        "learning_rate": 0.0001851159994616108,
        "epoch": 0.3932483791638721,
        "step": 5277
    },
    {
        "loss": 2.6833,
        "grad_norm": 1.7314631938934326,
        "learning_rate": 0.00018507904300617213,
        "epoch": 0.3933229003651539,
        "step": 5278
    },
    {
        "loss": 2.6723,
        "grad_norm": 3.6292715072631836,
        "learning_rate": 0.0001850420444253135,
        "epoch": 0.39339742156643565,
        "step": 5279
    },
    {
        "loss": 2.6183,
        "grad_norm": 1.8390607833862305,
        "learning_rate": 0.000185005003737354,
        "epoch": 0.3934719427677174,
        "step": 5280
    },
    {
        "loss": 2.482,
        "grad_norm": 2.9289815425872803,
        "learning_rate": 0.0001849679209606338,
        "epoch": 0.39354646396899917,
        "step": 5281
    },
    {
        "loss": 2.6487,
        "grad_norm": 2.6920325756073,
        "learning_rate": 0.00018493079611351377,
        "epoch": 0.39362098517028093,
        "step": 5282
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.2401695251464844,
        "learning_rate": 0.0001848936292143756,
        "epoch": 0.3936955063715627,
        "step": 5283
    },
    {
        "loss": 2.6688,
        "grad_norm": 2.2471981048583984,
        "learning_rate": 0.00018485642028162193,
        "epoch": 0.39377002757284446,
        "step": 5284
    },
    {
        "loss": 2.0326,
        "grad_norm": 3.4260873794555664,
        "learning_rate": 0.00018481916933367606,
        "epoch": 0.3938445487741262,
        "step": 5285
    },
    {
        "loss": 2.7844,
        "grad_norm": 1.7900588512420654,
        "learning_rate": 0.00018478187638898213,
        "epoch": 0.393919069975408,
        "step": 5286
    },
    {
        "loss": 2.052,
        "grad_norm": 3.825930595397949,
        "learning_rate": 0.0001847445414660051,
        "epoch": 0.39399359117668975,
        "step": 5287
    },
    {
        "loss": 1.963,
        "grad_norm": 2.5165035724639893,
        "learning_rate": 0.0001847071645832308,
        "epoch": 0.3940681123779715,
        "step": 5288
    },
    {
        "loss": 2.0926,
        "grad_norm": 6.35041618347168,
        "learning_rate": 0.00018466974575916565,
        "epoch": 0.3941426335792533,
        "step": 5289
    },
    {
        "loss": 2.9556,
        "grad_norm": 3.5812673568725586,
        "learning_rate": 0.00018463228501233695,
        "epoch": 0.39421715478053504,
        "step": 5290
    },
    {
        "loss": 2.857,
        "grad_norm": 2.1318986415863037,
        "learning_rate": 0.0001845947823612928,
        "epoch": 0.3942916759818168,
        "step": 5291
    },
    {
        "loss": 2.5753,
        "grad_norm": 3.009003162384033,
        "learning_rate": 0.0001845572378246019,
        "epoch": 0.39436619718309857,
        "step": 5292
    },
    {
        "loss": 2.5655,
        "grad_norm": 3.1493170261383057,
        "learning_rate": 0.0001845196514208539,
        "epoch": 0.39444071838438033,
        "step": 5293
    },
    {
        "loss": 2.3368,
        "grad_norm": 4.4434967041015625,
        "learning_rate": 0.00018448202316865894,
        "epoch": 0.39451523958566215,
        "step": 5294
    },
    {
        "loss": 1.4713,
        "grad_norm": 2.932133674621582,
        "learning_rate": 0.0001844443530866481,
        "epoch": 0.3945897607869439,
        "step": 5295
    },
    {
        "loss": 2.5734,
        "grad_norm": 2.1962852478027344,
        "learning_rate": 0.000184406641193473,
        "epoch": 0.3946642819882257,
        "step": 5296
    },
    {
        "loss": 2.1769,
        "grad_norm": 2.438410997390747,
        "learning_rate": 0.00018436888750780604,
        "epoch": 0.39473880318950744,
        "step": 5297
    },
    {
        "loss": 2.6941,
        "grad_norm": 2.7950873374938965,
        "learning_rate": 0.00018433109204834037,
        "epoch": 0.3948133243907892,
        "step": 5298
    },
    {
        "loss": 2.8239,
        "grad_norm": 2.7541141510009766,
        "learning_rate": 0.00018429325483378968,
        "epoch": 0.39488784559207096,
        "step": 5299
    },
    {
        "loss": 2.6774,
        "grad_norm": 2.684126138687134,
        "learning_rate": 0.00018425537588288843,
        "epoch": 0.3949623667933527,
        "step": 5300
    },
    {
        "loss": 2.6749,
        "grad_norm": 4.079489231109619,
        "learning_rate": 0.00018421745521439176,
        "epoch": 0.3950368879946345,
        "step": 5301
    },
    {
        "loss": 2.1055,
        "grad_norm": 3.191040515899658,
        "learning_rate": 0.00018417949284707538,
        "epoch": 0.39511140919591625,
        "step": 5302
    },
    {
        "loss": 2.393,
        "grad_norm": 3.3997268676757812,
        "learning_rate": 0.00018414148879973582,
        "epoch": 0.395185930397198,
        "step": 5303
    },
    {
        "loss": 2.5192,
        "grad_norm": 2.1596615314483643,
        "learning_rate": 0.00018410344309118998,
        "epoch": 0.3952604515984798,
        "step": 5304
    },
    {
        "loss": 2.4351,
        "grad_norm": 3.326608180999756,
        "learning_rate": 0.00018406535574027566,
        "epoch": 0.39533497279976154,
        "step": 5305
    },
    {
        "loss": 2.679,
        "grad_norm": 2.6943013668060303,
        "learning_rate": 0.00018402722676585107,
        "epoch": 0.3954094940010433,
        "step": 5306
    },
    {
        "loss": 2.478,
        "grad_norm": 2.162348747253418,
        "learning_rate": 0.00018398905618679514,
        "epoch": 0.39548401520232507,
        "step": 5307
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.8566784858703613,
        "learning_rate": 0.00018395084402200737,
        "epoch": 0.39555853640360683,
        "step": 5308
    },
    {
        "loss": 2.577,
        "grad_norm": 3.0213427543640137,
        "learning_rate": 0.0001839125902904079,
        "epoch": 0.3956330576048886,
        "step": 5309
    },
    {
        "loss": 1.6751,
        "grad_norm": 2.7916576862335205,
        "learning_rate": 0.00018387429501093733,
        "epoch": 0.39570757880617036,
        "step": 5310
    },
    {
        "loss": 2.1755,
        "grad_norm": 3.329941987991333,
        "learning_rate": 0.00018383595820255694,
        "epoch": 0.3957821000074521,
        "step": 5311
    },
    {
        "loss": 2.3116,
        "grad_norm": 3.2092244625091553,
        "learning_rate": 0.00018379757988424856,
        "epoch": 0.3958566212087339,
        "step": 5312
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.357147455215454,
        "learning_rate": 0.00018375916007501452,
        "epoch": 0.39593114241001565,
        "step": 5313
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.7122867107391357,
        "learning_rate": 0.00018372069879387773,
        "epoch": 0.3960056636112974,
        "step": 5314
    },
    {
        "loss": 2.4605,
        "grad_norm": 2.032573699951172,
        "learning_rate": 0.00018368219605988166,
        "epoch": 0.3960801848125792,
        "step": 5315
    },
    {
        "loss": 2.85,
        "grad_norm": 2.6672396659851074,
        "learning_rate": 0.00018364365189209024,
        "epoch": 0.39615470601386094,
        "step": 5316
    },
    {
        "loss": 1.9854,
        "grad_norm": 2.722198009490967,
        "learning_rate": 0.000183605066309588,
        "epoch": 0.3962292272151427,
        "step": 5317
    },
    {
        "loss": 2.4237,
        "grad_norm": 2.6069581508636475,
        "learning_rate": 0.00018356643933147986,
        "epoch": 0.39630374841642446,
        "step": 5318
    },
    {
        "loss": 2.287,
        "grad_norm": 4.088675498962402,
        "learning_rate": 0.00018352777097689133,
        "epoch": 0.3963782696177062,
        "step": 5319
    },
    {
        "loss": 1.9775,
        "grad_norm": 4.653242111206055,
        "learning_rate": 0.00018348906126496836,
        "epoch": 0.396452790818988,
        "step": 5320
    },
    {
        "loss": 2.1145,
        "grad_norm": 3.0200393199920654,
        "learning_rate": 0.00018345031021487743,
        "epoch": 0.39652731202026975,
        "step": 5321
    },
    {
        "loss": 2.557,
        "grad_norm": 2.7096199989318848,
        "learning_rate": 0.00018341151784580543,
        "epoch": 0.3966018332215515,
        "step": 5322
    },
    {
        "loss": 2.0565,
        "grad_norm": 3.052006721496582,
        "learning_rate": 0.00018337268417695975,
        "epoch": 0.3966763544228333,
        "step": 5323
    },
    {
        "loss": 1.9382,
        "grad_norm": 3.8582537174224854,
        "learning_rate": 0.0001833338092275682,
        "epoch": 0.39675087562411504,
        "step": 5324
    },
    {
        "loss": 1.4557,
        "grad_norm": 2.7701680660247803,
        "learning_rate": 0.00018329489301687903,
        "epoch": 0.3968253968253968,
        "step": 5325
    },
    {
        "loss": 2.3818,
        "grad_norm": 3.2157516479492188,
        "learning_rate": 0.00018325593556416098,
        "epoch": 0.39689991802667857,
        "step": 5326
    },
    {
        "loss": 1.1428,
        "grad_norm": 3.4304819107055664,
        "learning_rate": 0.00018321693688870308,
        "epoch": 0.39697443922796033,
        "step": 5327
    },
    {
        "loss": 2.6136,
        "grad_norm": 3.059216260910034,
        "learning_rate": 0.0001831778970098149,
        "epoch": 0.3970489604292421,
        "step": 5328
    },
    {
        "loss": 2.5831,
        "grad_norm": 3.148132085800171,
        "learning_rate": 0.00018313881594682636,
        "epoch": 0.3971234816305239,
        "step": 5329
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.944338321685791,
        "learning_rate": 0.00018309969371908775,
        "epoch": 0.3971980028318057,
        "step": 5330
    },
    {
        "loss": 1.5617,
        "grad_norm": 1.0240930318832397,
        "learning_rate": 0.0001830605303459698,
        "epoch": 0.39727252403308744,
        "step": 5331
    },
    {
        "loss": 2.686,
        "grad_norm": 2.6657779216766357,
        "learning_rate": 0.00018302132584686358,
        "epoch": 0.3973470452343692,
        "step": 5332
    },
    {
        "loss": 2.7785,
        "grad_norm": 2.382918357849121,
        "learning_rate": 0.0001829820802411805,
        "epoch": 0.39742156643565096,
        "step": 5333
    },
    {
        "loss": 2.5556,
        "grad_norm": 2.147822141647339,
        "learning_rate": 0.0001829427935483523,
        "epoch": 0.3974960876369327,
        "step": 5334
    },
    {
        "loss": 2.7601,
        "grad_norm": 2.565322160720825,
        "learning_rate": 0.00018290346578783123,
        "epoch": 0.3975706088382145,
        "step": 5335
    },
    {
        "loss": 3.0748,
        "grad_norm": 2.8352935314178467,
        "learning_rate": 0.00018286409697908967,
        "epoch": 0.39764513003949625,
        "step": 5336
    },
    {
        "loss": 2.6025,
        "grad_norm": 3.0350828170776367,
        "learning_rate": 0.00018282468714162037,
        "epoch": 0.397719651240778,
        "step": 5337
    },
    {
        "loss": 2.7446,
        "grad_norm": 3.8702218532562256,
        "learning_rate": 0.00018278523629493648,
        "epoch": 0.3977941724420598,
        "step": 5338
    },
    {
        "loss": 3.0856,
        "grad_norm": 2.007202386856079,
        "learning_rate": 0.00018274574445857145,
        "epoch": 0.39786869364334154,
        "step": 5339
    },
    {
        "loss": 2.3691,
        "grad_norm": 2.2689332962036133,
        "learning_rate": 0.00018270621165207893,
        "epoch": 0.3979432148446233,
        "step": 5340
    },
    {
        "loss": 2.9525,
        "grad_norm": 2.483469009399414,
        "learning_rate": 0.00018266663789503289,
        "epoch": 0.39801773604590507,
        "step": 5341
    },
    {
        "loss": 2.3635,
        "grad_norm": 2.6666603088378906,
        "learning_rate": 0.0001826270232070276,
        "epoch": 0.39809225724718683,
        "step": 5342
    },
    {
        "loss": 2.7234,
        "grad_norm": 2.387199878692627,
        "learning_rate": 0.0001825873676076776,
        "epoch": 0.3981667784484686,
        "step": 5343
    },
    {
        "loss": 2.7784,
        "grad_norm": 1.9417635202407837,
        "learning_rate": 0.00018254767111661765,
        "epoch": 0.39824129964975036,
        "step": 5344
    },
    {
        "loss": 2.0731,
        "grad_norm": 4.213559627532959,
        "learning_rate": 0.0001825079337535029,
        "epoch": 0.3983158208510321,
        "step": 5345
    },
    {
        "loss": 1.872,
        "grad_norm": 4.257843494415283,
        "learning_rate": 0.0001824681555380085,
        "epoch": 0.3983903420523139,
        "step": 5346
    },
    {
        "loss": 1.9833,
        "grad_norm": 1.93191397190094,
        "learning_rate": 0.00018242833648982995,
        "epoch": 0.39846486325359565,
        "step": 5347
    },
    {
        "loss": 2.0553,
        "grad_norm": 2.9527344703674316,
        "learning_rate": 0.00018238847662868296,
        "epoch": 0.3985393844548774,
        "step": 5348
    },
    {
        "loss": 2.0959,
        "grad_norm": 3.2617931365966797,
        "learning_rate": 0.00018234857597430358,
        "epoch": 0.3986139056561592,
        "step": 5349
    },
    {
        "loss": 2.1753,
        "grad_norm": 3.817941904067993,
        "learning_rate": 0.0001823086345464478,
        "epoch": 0.39868842685744094,
        "step": 5350
    },
    {
        "loss": 2.6561,
        "grad_norm": 2.712735414505005,
        "learning_rate": 0.00018226865236489195,
        "epoch": 0.3987629480587227,
        "step": 5351
    },
    {
        "loss": 2.6891,
        "grad_norm": 2.7040598392486572,
        "learning_rate": 0.0001822286294494325,
        "epoch": 0.39883746926000446,
        "step": 5352
    },
    {
        "loss": 2.7484,
        "grad_norm": 2.243809461593628,
        "learning_rate": 0.00018218856581988615,
        "epoch": 0.3989119904612862,
        "step": 5353
    },
    {
        "loss": 2.5947,
        "grad_norm": 2.30530047416687,
        "learning_rate": 0.0001821484614960897,
        "epoch": 0.398986511662568,
        "step": 5354
    },
    {
        "loss": 2.2769,
        "grad_norm": 3.7924506664276123,
        "learning_rate": 0.00018210831649790015,
        "epoch": 0.39906103286384975,
        "step": 5355
    },
    {
        "loss": 2.3186,
        "grad_norm": 2.440805435180664,
        "learning_rate": 0.0001820681308451946,
        "epoch": 0.3991355540651315,
        "step": 5356
    },
    {
        "loss": 2.804,
        "grad_norm": 2.1818063259124756,
        "learning_rate": 0.00018202790455787017,
        "epoch": 0.3992100752664133,
        "step": 5357
    },
    {
        "loss": 1.1883,
        "grad_norm": 3.8144750595092773,
        "learning_rate": 0.0001819876376558443,
        "epoch": 0.39928459646769504,
        "step": 5358
    },
    {
        "loss": 2.1355,
        "grad_norm": 3.0313074588775635,
        "learning_rate": 0.0001819473301590545,
        "epoch": 0.3993591176689768,
        "step": 5359
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.960231065750122,
        "learning_rate": 0.00018190698208745828,
        "epoch": 0.39943363887025857,
        "step": 5360
    },
    {
        "loss": 1.6665,
        "grad_norm": 2.7378602027893066,
        "learning_rate": 0.00018186659346103326,
        "epoch": 0.39950816007154033,
        "step": 5361
    },
    {
        "loss": 2.8304,
        "grad_norm": 2.6934025287628174,
        "learning_rate": 0.00018182616429977716,
        "epoch": 0.3995826812728221,
        "step": 5362
    },
    {
        "loss": 2.0983,
        "grad_norm": 2.889993190765381,
        "learning_rate": 0.00018178569462370785,
        "epoch": 0.39965720247410386,
        "step": 5363
    },
    {
        "loss": 2.5261,
        "grad_norm": 3.1547141075134277,
        "learning_rate": 0.0001817451844528632,
        "epoch": 0.3997317236753856,
        "step": 5364
    },
    {
        "loss": 2.2363,
        "grad_norm": 4.020959377288818,
        "learning_rate": 0.00018170463380730103,
        "epoch": 0.39980624487666744,
        "step": 5365
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.670687675476074,
        "learning_rate": 0.0001816640427070994,
        "epoch": 0.3998807660779492,
        "step": 5366
    },
    {
        "loss": 2.63,
        "grad_norm": 2.8556907176971436,
        "learning_rate": 0.0001816234111723562,
        "epoch": 0.39995528727923096,
        "step": 5367
    },
    {
        "loss": 2.4444,
        "grad_norm": 3.737865686416626,
        "learning_rate": 0.00018158273922318948,
        "epoch": 0.4000298084805127,
        "step": 5368
    },
    {
        "loss": 2.4844,
        "grad_norm": 2.9530348777770996,
        "learning_rate": 0.00018154202687973724,
        "epoch": 0.4001043296817945,
        "step": 5369
    },
    {
        "loss": 2.8199,
        "grad_norm": 3.037508726119995,
        "learning_rate": 0.00018150127416215756,
        "epoch": 0.40017885088307625,
        "step": 5370
    },
    {
        "loss": 2.8178,
        "grad_norm": 1.7430256605148315,
        "learning_rate": 0.00018146048109062831,
        "epoch": 0.400253372084358,
        "step": 5371
    },
    {
        "loss": 2.0172,
        "grad_norm": 2.823697328567505,
        "learning_rate": 0.0001814196476853476,
        "epoch": 0.4003278932856398,
        "step": 5372
    },
    {
        "loss": 1.9794,
        "grad_norm": 3.013688087463379,
        "learning_rate": 0.0001813787739665333,
        "epoch": 0.40040241448692154,
        "step": 5373
    },
    {
        "loss": 2.7449,
        "grad_norm": 1.7556307315826416,
        "learning_rate": 0.00018133785995442337,
        "epoch": 0.4004769356882033,
        "step": 5374
    },
    {
        "loss": 2.7364,
        "grad_norm": 2.918508529663086,
        "learning_rate": 0.00018129690566927567,
        "epoch": 0.40055145688948507,
        "step": 5375
    },
    {
        "loss": 2.6707,
        "grad_norm": 2.6696574687957764,
        "learning_rate": 0.00018125591113136806,
        "epoch": 0.40062597809076683,
        "step": 5376
    },
    {
        "loss": 2.2851,
        "grad_norm": 3.892672300338745,
        "learning_rate": 0.00018121487636099816,
        "epoch": 0.4007004992920486,
        "step": 5377
    },
    {
        "loss": 2.455,
        "grad_norm": 2.764915943145752,
        "learning_rate": 0.00018117380137848375,
        "epoch": 0.40077502049333036,
        "step": 5378
    },
    {
        "loss": 2.2294,
        "grad_norm": 3.009795904159546,
        "learning_rate": 0.0001811326862041623,
        "epoch": 0.4008495416946121,
        "step": 5379
    },
    {
        "loss": 3.034,
        "grad_norm": 4.246597766876221,
        "learning_rate": 0.00018109153085839138,
        "epoch": 0.4009240628958939,
        "step": 5380
    },
    {
        "loss": 2.342,
        "grad_norm": 6.642449378967285,
        "learning_rate": 0.00018105033536154822,
        "epoch": 0.40099858409717565,
        "step": 5381
    },
    {
        "loss": 2.3538,
        "grad_norm": 3.140002965927124,
        "learning_rate": 0.00018100909973403019,
        "epoch": 0.4010731052984574,
        "step": 5382
    },
    {
        "loss": 2.4331,
        "grad_norm": 2.0788261890411377,
        "learning_rate": 0.00018096782399625432,
        "epoch": 0.4011476264997392,
        "step": 5383
    },
    {
        "loss": 1.8195,
        "grad_norm": 3.7872314453125,
        "learning_rate": 0.00018092650816865758,
        "epoch": 0.40122214770102094,
        "step": 5384
    },
    {
        "loss": 2.3766,
        "grad_norm": 3.300652027130127,
        "learning_rate": 0.0001808851522716968,
        "epoch": 0.4012966689023027,
        "step": 5385
    },
    {
        "loss": 2.479,
        "grad_norm": 3.291574478149414,
        "learning_rate": 0.00018084375632584871,
        "epoch": 0.40137119010358446,
        "step": 5386
    },
    {
        "loss": 2.3031,
        "grad_norm": 2.6652636528015137,
        "learning_rate": 0.00018080232035160972,
        "epoch": 0.4014457113048662,
        "step": 5387
    },
    {
        "loss": 2.2628,
        "grad_norm": 2.7422449588775635,
        "learning_rate": 0.0001807608443694961,
        "epoch": 0.401520232506148,
        "step": 5388
    },
    {
        "loss": 2.7931,
        "grad_norm": 1.9924010038375854,
        "learning_rate": 0.00018071932840004406,
        "epoch": 0.40159475370742975,
        "step": 5389
    },
    {
        "loss": 2.1546,
        "grad_norm": 2.673738718032837,
        "learning_rate": 0.00018067777246380952,
        "epoch": 0.4016692749087115,
        "step": 5390
    },
    {
        "loss": 2.9146,
        "grad_norm": 2.003756046295166,
        "learning_rate": 0.00018063617658136807,
        "epoch": 0.4017437961099933,
        "step": 5391
    },
    {
        "loss": 2.0039,
        "grad_norm": 3.6936562061309814,
        "learning_rate": 0.0001805945407733153,
        "epoch": 0.40181831731127504,
        "step": 5392
    },
    {
        "loss": 1.6789,
        "grad_norm": 2.655916213989258,
        "learning_rate": 0.0001805528650602664,
        "epoch": 0.4018928385125568,
        "step": 5393
    },
    {
        "loss": 2.1337,
        "grad_norm": 1.7394388914108276,
        "learning_rate": 0.0001805111494628564,
        "epoch": 0.40196735971383857,
        "step": 5394
    },
    {
        "loss": 1.73,
        "grad_norm": 3.5991547107696533,
        "learning_rate": 0.00018046939400174008,
        "epoch": 0.40204188091512033,
        "step": 5395
    },
    {
        "loss": 3.0282,
        "grad_norm": 5.2014079093933105,
        "learning_rate": 0.00018042759869759195,
        "epoch": 0.4021164021164021,
        "step": 5396
    },
    {
        "loss": 2.0415,
        "grad_norm": 3.233928918838501,
        "learning_rate": 0.00018038576357110622,
        "epoch": 0.40219092331768386,
        "step": 5397
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.1147024631500244,
        "learning_rate": 0.00018034388864299677,
        "epoch": 0.4022654445189656,
        "step": 5398
    },
    {
        "loss": 1.5022,
        "grad_norm": 3.899346113204956,
        "learning_rate": 0.0001803019739339973,
        "epoch": 0.4023399657202474,
        "step": 5399
    },
    {
        "loss": 2.4109,
        "grad_norm": 3.2361228466033936,
        "learning_rate": 0.00018026001946486116,
        "epoch": 0.4024144869215292,
        "step": 5400
    },
    {
        "loss": 2.1556,
        "grad_norm": 3.294987678527832,
        "learning_rate": 0.00018021802525636135,
        "epoch": 0.40248900812281097,
        "step": 5401
    },
    {
        "loss": 2.6093,
        "grad_norm": 1.961673617362976,
        "learning_rate": 0.00018017599132929063,
        "epoch": 0.40256352932409273,
        "step": 5402
    },
    {
        "loss": 2.6822,
        "grad_norm": 4.17476749420166,
        "learning_rate": 0.0001801339177044613,
        "epoch": 0.4026380505253745,
        "step": 5403
    },
    {
        "loss": 2.7683,
        "grad_norm": 2.965036630630493,
        "learning_rate": 0.00018009180440270542,
        "epoch": 0.40271257172665625,
        "step": 5404
    },
    {
        "loss": 3.0088,
        "grad_norm": 5.024105072021484,
        "learning_rate": 0.0001800496514448747,
        "epoch": 0.402787092927938,
        "step": 5405
    },
    {
        "loss": 2.4802,
        "grad_norm": 3.856374979019165,
        "learning_rate": 0.00018000745885184046,
        "epoch": 0.4028616141292198,
        "step": 5406
    },
    {
        "loss": 2.8031,
        "grad_norm": 2.509897470474243,
        "learning_rate": 0.0001799652266444936,
        "epoch": 0.40293613533050154,
        "step": 5407
    },
    {
        "loss": 2.3589,
        "grad_norm": 2.5430288314819336,
        "learning_rate": 0.00017992295484374463,
        "epoch": 0.4030106565317833,
        "step": 5408
    },
    {
        "loss": 2.8342,
        "grad_norm": 2.5876212120056152,
        "learning_rate": 0.00017988064347052374,
        "epoch": 0.40308517773306507,
        "step": 5409
    },
    {
        "loss": 2.0415,
        "grad_norm": 2.6775200366973877,
        "learning_rate": 0.0001798382925457807,
        "epoch": 0.40315969893434683,
        "step": 5410
    },
    {
        "loss": 2.8128,
        "grad_norm": 2.2006053924560547,
        "learning_rate": 0.00017979590209048484,
        "epoch": 0.4032342201356286,
        "step": 5411
    },
    {
        "loss": 2.8133,
        "grad_norm": 2.8571932315826416,
        "learning_rate": 0.000179753472125625,
        "epoch": 0.40330874133691036,
        "step": 5412
    },
    {
        "loss": 2.6573,
        "grad_norm": 1.8470274209976196,
        "learning_rate": 0.00017971100267220966,
        "epoch": 0.4033832625381921,
        "step": 5413
    },
    {
        "loss": 1.6674,
        "grad_norm": 4.617443561553955,
        "learning_rate": 0.00017966849375126688,
        "epoch": 0.4034577837394739,
        "step": 5414
    },
    {
        "loss": 2.936,
        "grad_norm": 3.0895931720733643,
        "learning_rate": 0.0001796259453838442,
        "epoch": 0.40353230494075565,
        "step": 5415
    },
    {
        "loss": 1.6806,
        "grad_norm": 3.655334711074829,
        "learning_rate": 0.00017958335759100873,
        "epoch": 0.4036068261420374,
        "step": 5416
    },
    {
        "loss": 2.5876,
        "grad_norm": 2.2944445610046387,
        "learning_rate": 0.00017954073039384708,
        "epoch": 0.4036813473433192,
        "step": 5417
    },
    {
        "loss": 2.1537,
        "grad_norm": 2.9307849407196045,
        "learning_rate": 0.0001794980638134653,
        "epoch": 0.40375586854460094,
        "step": 5418
    },
    {
        "loss": 2.7688,
        "grad_norm": 2.3494722843170166,
        "learning_rate": 0.00017945535787098905,
        "epoch": 0.4038303897458827,
        "step": 5419
    },
    {
        "loss": 1.5124,
        "grad_norm": 1.194589376449585,
        "learning_rate": 0.00017941261258756352,
        "epoch": 0.40390491094716446,
        "step": 5420
    },
    {
        "loss": 2.8149,
        "grad_norm": 2.955836296081543,
        "learning_rate": 0.00017936982798435325,
        "epoch": 0.4039794321484462,
        "step": 5421
    },
    {
        "loss": 2.555,
        "grad_norm": 2.629110813140869,
        "learning_rate": 0.0001793270040825422,
        "epoch": 0.404053953349728,
        "step": 5422
    },
    {
        "loss": 1.7693,
        "grad_norm": 4.433311939239502,
        "learning_rate": 0.00017928414090333396,
        "epoch": 0.40412847455100975,
        "step": 5423
    },
    {
        "loss": 2.4102,
        "grad_norm": 1.8096729516983032,
        "learning_rate": 0.00017924123846795154,
        "epoch": 0.4042029957522915,
        "step": 5424
    },
    {
        "loss": 1.0939,
        "grad_norm": 1.9697407484054565,
        "learning_rate": 0.00017919829679763732,
        "epoch": 0.4042775169535733,
        "step": 5425
    },
    {
        "loss": 1.9997,
        "grad_norm": 3.025588035583496,
        "learning_rate": 0.00017915531591365305,
        "epoch": 0.40435203815485504,
        "step": 5426
    },
    {
        "loss": 2.4328,
        "grad_norm": 3.712160587310791,
        "learning_rate": 0.0001791122958372801,
        "epoch": 0.4044265593561368,
        "step": 5427
    },
    {
        "loss": 1.9032,
        "grad_norm": 3.7100558280944824,
        "learning_rate": 0.000179069236589819,
        "epoch": 0.40450108055741857,
        "step": 5428
    },
    {
        "loss": 2.3749,
        "grad_norm": 4.289542198181152,
        "learning_rate": 0.00017902613819258982,
        "epoch": 0.40457560175870033,
        "step": 5429
    },
    {
        "loss": 1.5145,
        "grad_norm": 4.530159950256348,
        "learning_rate": 0.00017898300066693205,
        "epoch": 0.4046501229599821,
        "step": 5430
    },
    {
        "loss": 2.7213,
        "grad_norm": 3.523912191390991,
        "learning_rate": 0.00017893982403420448,
        "epoch": 0.40472464416126386,
        "step": 5431
    },
    {
        "loss": 2.046,
        "grad_norm": 3.9565224647521973,
        "learning_rate": 0.0001788966083157852,
        "epoch": 0.4047991653625456,
        "step": 5432
    },
    {
        "loss": 2.0522,
        "grad_norm": 3.513441562652588,
        "learning_rate": 0.00017885335353307177,
        "epoch": 0.4048736865638274,
        "step": 5433
    },
    {
        "loss": 2.4955,
        "grad_norm": 4.27238130569458,
        "learning_rate": 0.0001788100597074811,
        "epoch": 0.40494820776510915,
        "step": 5434
    },
    {
        "loss": 3.0434,
        "grad_norm": 2.102264404296875,
        "learning_rate": 0.0001787667268604493,
        "epoch": 0.40502272896639097,
        "step": 5435
    },
    {
        "loss": 2.5965,
        "grad_norm": 2.7500600814819336,
        "learning_rate": 0.00017872335501343186,
        "epoch": 0.40509725016767273,
        "step": 5436
    },
    {
        "loss": 2.2615,
        "grad_norm": 3.704235792160034,
        "learning_rate": 0.00017867994418790375,
        "epoch": 0.4051717713689545,
        "step": 5437
    },
    {
        "loss": 2.1597,
        "grad_norm": 3.0823209285736084,
        "learning_rate": 0.00017863649440535894,
        "epoch": 0.40524629257023626,
        "step": 5438
    },
    {
        "loss": 2.2385,
        "grad_norm": 3.2160067558288574,
        "learning_rate": 0.00017859300568731095,
        "epoch": 0.405320813771518,
        "step": 5439
    },
    {
        "loss": 2.1761,
        "grad_norm": 2.541578531265259,
        "learning_rate": 0.00017854947805529236,
        "epoch": 0.4053953349727998,
        "step": 5440
    },
    {
        "loss": 1.9593,
        "grad_norm": 5.147805213928223,
        "learning_rate": 0.00017850591153085522,
        "epoch": 0.40546985617408154,
        "step": 5441
    },
    {
        "loss": 2.7626,
        "grad_norm": 2.1416823863983154,
        "learning_rate": 0.00017846230613557067,
        "epoch": 0.4055443773753633,
        "step": 5442
    },
    {
        "loss": 2.2364,
        "grad_norm": 4.824481964111328,
        "learning_rate": 0.0001784186618910292,
        "epoch": 0.40561889857664507,
        "step": 5443
    },
    {
        "loss": 2.0928,
        "grad_norm": 2.830570936203003,
        "learning_rate": 0.00017837497881884057,
        "epoch": 0.40569341977792683,
        "step": 5444
    },
    {
        "loss": 2.3257,
        "grad_norm": 3.757338762283325,
        "learning_rate": 0.00017833125694063356,
        "epoch": 0.4057679409792086,
        "step": 5445
    },
    {
        "loss": 2.2748,
        "grad_norm": 3.125091552734375,
        "learning_rate": 0.00017828749627805642,
        "epoch": 0.40584246218049036,
        "step": 5446
    },
    {
        "loss": 1.2401,
        "grad_norm": 3.3108346462249756,
        "learning_rate": 0.00017824369685277643,
        "epoch": 0.4059169833817721,
        "step": 5447
    },
    {
        "loss": 2.5091,
        "grad_norm": 2.888289451599121,
        "learning_rate": 0.00017819985868648014,
        "epoch": 0.4059915045830539,
        "step": 5448
    },
    {
        "loss": 2.352,
        "grad_norm": 2.934504747390747,
        "learning_rate": 0.00017815598180087328,
        "epoch": 0.40606602578433565,
        "step": 5449
    },
    {
        "loss": 2.7371,
        "grad_norm": 1.9369581937789917,
        "learning_rate": 0.00017811206621768072,
        "epoch": 0.4061405469856174,
        "step": 5450
    },
    {
        "loss": 2.3306,
        "grad_norm": 3.7424893379211426,
        "learning_rate": 0.00017806811195864649,
        "epoch": 0.4062150681868992,
        "step": 5451
    },
    {
        "loss": 2.5952,
        "grad_norm": 2.838089942932129,
        "learning_rate": 0.00017802411904553375,
        "epoch": 0.40628958938818094,
        "step": 5452
    },
    {
        "loss": 2.6024,
        "grad_norm": 2.2897727489471436,
        "learning_rate": 0.0001779800875001249,
        "epoch": 0.4063641105894627,
        "step": 5453
    },
    {
        "loss": 1.1709,
        "grad_norm": 2.5802416801452637,
        "learning_rate": 0.00017793601734422137,
        "epoch": 0.40643863179074446,
        "step": 5454
    },
    {
        "loss": 2.4446,
        "grad_norm": 2.6193742752075195,
        "learning_rate": 0.00017789190859964372,
        "epoch": 0.4065131529920262,
        "step": 5455
    },
    {
        "loss": 3.2046,
        "grad_norm": 3.4697282314300537,
        "learning_rate": 0.00017784776128823164,
        "epoch": 0.406587674193308,
        "step": 5456
    },
    {
        "loss": 2.8691,
        "grad_norm": 2.914113998413086,
        "learning_rate": 0.00017780357543184397,
        "epoch": 0.40666219539458975,
        "step": 5457
    },
    {
        "loss": 1.8048,
        "grad_norm": 2.9049344062805176,
        "learning_rate": 0.0001777593510523585,
        "epoch": 0.4067367165958715,
        "step": 5458
    },
    {
        "loss": 2.4393,
        "grad_norm": 3.8000621795654297,
        "learning_rate": 0.00017771508817167217,
        "epoch": 0.4068112377971533,
        "step": 5459
    },
    {
        "loss": 2.8975,
        "grad_norm": 3.3000478744506836,
        "learning_rate": 0.000177670786811701,
        "epoch": 0.40688575899843504,
        "step": 5460
    },
    {
        "loss": 2.4651,
        "grad_norm": 2.458185911178589,
        "learning_rate": 0.0001776264469943801,
        "epoch": 0.4069602801997168,
        "step": 5461
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.6065523624420166,
        "learning_rate": 0.00017758206874166345,
        "epoch": 0.40703480140099857,
        "step": 5462
    },
    {
        "loss": 1.7135,
        "grad_norm": 8.423245429992676,
        "learning_rate": 0.00017753765207552432,
        "epoch": 0.40710932260228033,
        "step": 5463
    },
    {
        "loss": 2.7003,
        "grad_norm": 3.086649179458618,
        "learning_rate": 0.00017749319701795472,
        "epoch": 0.4071838438035621,
        "step": 5464
    },
    {
        "loss": 2.1661,
        "grad_norm": 3.1328389644622803,
        "learning_rate": 0.00017744870359096582,
        "epoch": 0.40725836500484386,
        "step": 5465
    },
    {
        "loss": 2.8165,
        "grad_norm": 2.8765106201171875,
        "learning_rate": 0.00017740417181658785,
        "epoch": 0.4073328862061256,
        "step": 5466
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.1386826038360596,
        "learning_rate": 0.00017735960171686994,
        "epoch": 0.4074074074074074,
        "step": 5467
    },
    {
        "loss": 2.4916,
        "grad_norm": 3.4406983852386475,
        "learning_rate": 0.00017731499331388018,
        "epoch": 0.40748192860868915,
        "step": 5468
    },
    {
        "loss": 3.0055,
        "grad_norm": 4.02673864364624,
        "learning_rate": 0.0001772703466297056,
        "epoch": 0.4075564498099709,
        "step": 5469
    },
    {
        "loss": 2.6864,
        "grad_norm": 3.3476529121398926,
        "learning_rate": 0.00017722566168645228,
        "epoch": 0.4076309710112527,
        "step": 5470
    },
    {
        "loss": 2.2623,
        "grad_norm": 3.4729764461517334,
        "learning_rate": 0.00017718093850624527,
        "epoch": 0.4077054922125345,
        "step": 5471
    },
    {
        "loss": 2.9014,
        "grad_norm": 2.713172197341919,
        "learning_rate": 0.0001771361771112284,
        "epoch": 0.40778001341381626,
        "step": 5472
    },
    {
        "loss": 2.1569,
        "grad_norm": 3.72355318069458,
        "learning_rate": 0.00017709137752356445,
        "epoch": 0.407854534615098,
        "step": 5473
    },
    {
        "loss": 1.8236,
        "grad_norm": 3.9322712421417236,
        "learning_rate": 0.0001770465397654353,
        "epoch": 0.4079290558163798,
        "step": 5474
    },
    {
        "loss": 2.3181,
        "grad_norm": 2.7218873500823975,
        "learning_rate": 0.00017700166385904145,
        "epoch": 0.40800357701766155,
        "step": 5475
    },
    {
        "loss": 2.49,
        "grad_norm": 2.3048572540283203,
        "learning_rate": 0.0001769567498266025,
        "epoch": 0.4080780982189433,
        "step": 5476
    },
    {
        "loss": 2.6626,
        "grad_norm": 3.7270028591156006,
        "learning_rate": 0.00017691179769035694,
        "epoch": 0.40815261942022507,
        "step": 5477
    },
    {
        "loss": 2.3446,
        "grad_norm": 3.9163076877593994,
        "learning_rate": 0.00017686680747256196,
        "epoch": 0.40822714062150683,
        "step": 5478
    },
    {
        "loss": 2.476,
        "grad_norm": 2.9634299278259277,
        "learning_rate": 0.00017682177919549363,
        "epoch": 0.4083016618227886,
        "step": 5479
    },
    {
        "loss": 2.7792,
        "grad_norm": 4.007535457611084,
        "learning_rate": 0.000176776712881447,
        "epoch": 0.40837618302407036,
        "step": 5480
    },
    {
        "loss": 2.535,
        "grad_norm": 3.7480597496032715,
        "learning_rate": 0.00017673160855273589,
        "epoch": 0.4084507042253521,
        "step": 5481
    },
    {
        "loss": 2.9116,
        "grad_norm": 1.9742587804794312,
        "learning_rate": 0.00017668646623169294,
        "epoch": 0.4085252254266339,
        "step": 5482
    },
    {
        "loss": 2.663,
        "grad_norm": 4.670033931732178,
        "learning_rate": 0.00017664128594066948,
        "epoch": 0.40859974662791565,
        "step": 5483
    },
    {
        "loss": 2.3216,
        "grad_norm": 2.4358248710632324,
        "learning_rate": 0.00017659606770203587,
        "epoch": 0.4086742678291974,
        "step": 5484
    },
    {
        "loss": 1.3246,
        "grad_norm": 2.8348612785339355,
        "learning_rate": 0.00017655081153818104,
        "epoch": 0.4087487890304792,
        "step": 5485
    },
    {
        "loss": 2.8009,
        "grad_norm": 2.891235828399658,
        "learning_rate": 0.00017650551747151296,
        "epoch": 0.40882331023176094,
        "step": 5486
    },
    {
        "loss": 1.8833,
        "grad_norm": 3.3500583171844482,
        "learning_rate": 0.00017646018552445805,
        "epoch": 0.4088978314330427,
        "step": 5487
    },
    {
        "loss": 2.8823,
        "grad_norm": 4.399341106414795,
        "learning_rate": 0.0001764148157194617,
        "epoch": 0.40897235263432447,
        "step": 5488
    },
    {
        "loss": 1.4529,
        "grad_norm": 5.177864074707031,
        "learning_rate": 0.00017636940807898795,
        "epoch": 0.40904687383560623,
        "step": 5489
    },
    {
        "loss": 2.2109,
        "grad_norm": 2.378915548324585,
        "learning_rate": 0.00017632396262551967,
        "epoch": 0.409121395036888,
        "step": 5490
    },
    {
        "loss": 2.5757,
        "grad_norm": 1.1712502241134644,
        "learning_rate": 0.00017627847938155838,
        "epoch": 0.40919591623816975,
        "step": 5491
    },
    {
        "loss": 2.4949,
        "grad_norm": 2.482736825942993,
        "learning_rate": 0.00017623295836962432,
        "epoch": 0.4092704374394515,
        "step": 5492
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.8647048473358154,
        "learning_rate": 0.00017618739961225636,
        "epoch": 0.4093449586407333,
        "step": 5493
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.317413330078125,
        "learning_rate": 0.0001761418031320122,
        "epoch": 0.40941947984201504,
        "step": 5494
    },
    {
        "loss": 2.4006,
        "grad_norm": 3.0086371898651123,
        "learning_rate": 0.00017609616895146815,
        "epoch": 0.4094940010432968,
        "step": 5495
    },
    {
        "loss": 1.9759,
        "grad_norm": 3.5157928466796875,
        "learning_rate": 0.0001760504970932192,
        "epoch": 0.40956852224457857,
        "step": 5496
    },
    {
        "loss": 1.6542,
        "grad_norm": 5.134592533111572,
        "learning_rate": 0.00017600478757987898,
        "epoch": 0.40964304344586033,
        "step": 5497
    },
    {
        "loss": 2.4169,
        "grad_norm": 3.0281124114990234,
        "learning_rate": 0.00017595904043407972,
        "epoch": 0.4097175646471421,
        "step": 5498
    },
    {
        "loss": 2.4099,
        "grad_norm": 4.850915431976318,
        "learning_rate": 0.00017591325567847235,
        "epoch": 0.40979208584842386,
        "step": 5499
    },
    {
        "loss": 2.378,
        "grad_norm": 2.3917107582092285,
        "learning_rate": 0.00017586743333572645,
        "epoch": 0.4098666070497056,
        "step": 5500
    },
    {
        "loss": 2.5701,
        "grad_norm": 3.2550365924835205,
        "learning_rate": 0.00017582157342853008,
        "epoch": 0.4099411282509874,
        "step": 5501
    },
    {
        "loss": 2.8256,
        "grad_norm": 2.5419604778289795,
        "learning_rate": 0.00017577567597959007,
        "epoch": 0.41001564945226915,
        "step": 5502
    },
    {
        "loss": 2.7802,
        "grad_norm": 2.571795701980591,
        "learning_rate": 0.00017572974101163165,
        "epoch": 0.4100901706535509,
        "step": 5503
    },
    {
        "loss": 2.5036,
        "grad_norm": 3.4011073112487793,
        "learning_rate": 0.00017568376854739875,
        "epoch": 0.4101646918548327,
        "step": 5504
    },
    {
        "loss": 2.5348,
        "grad_norm": 3.007526159286499,
        "learning_rate": 0.00017563775860965396,
        "epoch": 0.41023921305611444,
        "step": 5505
    },
    {
        "loss": 1.8033,
        "grad_norm": 4.14028263092041,
        "learning_rate": 0.00017559171122117818,
        "epoch": 0.41031373425739626,
        "step": 5506
    },
    {
        "loss": 2.4861,
        "grad_norm": 2.1431000232696533,
        "learning_rate": 0.000175545626404771,
        "epoch": 0.410388255458678,
        "step": 5507
    },
    {
        "loss": 2.3,
        "grad_norm": 3.65431547164917,
        "learning_rate": 0.00017549950418325062,
        "epoch": 0.4104627766599598,
        "step": 5508
    },
    {
        "loss": 1.9773,
        "grad_norm": 2.6447408199310303,
        "learning_rate": 0.0001754533445794535,
        "epoch": 0.41053729786124155,
        "step": 5509
    },
    {
        "loss": 2.4059,
        "grad_norm": 2.3388328552246094,
        "learning_rate": 0.0001754071476162349,
        "epoch": 0.4106118190625233,
        "step": 5510
    },
    {
        "loss": 1.7121,
        "grad_norm": 4.518731594085693,
        "learning_rate": 0.00017536091331646835,
        "epoch": 0.41068634026380507,
        "step": 5511
    },
    {
        "loss": 1.9467,
        "grad_norm": 3.428074836730957,
        "learning_rate": 0.0001753146417030461,
        "epoch": 0.41076086146508684,
        "step": 5512
    },
    {
        "loss": 2.777,
        "grad_norm": 2.6183528900146484,
        "learning_rate": 0.0001752683327988786,
        "epoch": 0.4108353826663686,
        "step": 5513
    },
    {
        "loss": 2.9433,
        "grad_norm": 3.0369961261749268,
        "learning_rate": 0.00017522198662689496,
        "epoch": 0.41090990386765036,
        "step": 5514
    },
    {
        "loss": 1.8366,
        "grad_norm": Infinity,
        "learning_rate": 0.00017522198662689496,
        "epoch": 0.4109844250689321,
        "step": 5515
    },
    {
        "loss": 2.6579,
        "grad_norm": 7.7743964195251465,
        "learning_rate": 0.00017517560321004272,
        "epoch": 0.4110589462702139,
        "step": 5516
    },
    {
        "loss": 2.4003,
        "grad_norm": 2.7184062004089355,
        "learning_rate": 0.00017512918257128778,
        "epoch": 0.41113346747149565,
        "step": 5517
    },
    {
        "loss": 2.5654,
        "grad_norm": 2.2957682609558105,
        "learning_rate": 0.0001750827247336145,
        "epoch": 0.4112079886727774,
        "step": 5518
    },
    {
        "loss": 1.7045,
        "grad_norm": 3.402982711791992,
        "learning_rate": 0.00017503622972002582,
        "epoch": 0.4112825098740592,
        "step": 5519
    },
    {
        "loss": 1.9654,
        "grad_norm": 3.4157893657684326,
        "learning_rate": 0.0001749896975535428,
        "epoch": 0.41135703107534094,
        "step": 5520
    },
    {
        "loss": 2.1225,
        "grad_norm": 6.968979835510254,
        "learning_rate": 0.00017494312825720504,
        "epoch": 0.4114315522766227,
        "step": 5521
    },
    {
        "loss": 2.0641,
        "grad_norm": 5.123592376708984,
        "learning_rate": 0.00017489652185407058,
        "epoch": 0.41150607347790447,
        "step": 5522
    },
    {
        "loss": 2.5577,
        "grad_norm": 3.3262386322021484,
        "learning_rate": 0.0001748498783672158,
        "epoch": 0.41158059467918623,
        "step": 5523
    },
    {
        "loss": 2.5941,
        "grad_norm": 3.053760051727295,
        "learning_rate": 0.0001748031978197353,
        "epoch": 0.411655115880468,
        "step": 5524
    },
    {
        "loss": 2.5048,
        "grad_norm": 3.5750017166137695,
        "learning_rate": 0.0001747564802347423,
        "epoch": 0.41172963708174976,
        "step": 5525
    },
    {
        "loss": 2.6804,
        "grad_norm": 2.4322409629821777,
        "learning_rate": 0.00017470972563536806,
        "epoch": 0.4118041582830315,
        "step": 5526
    },
    {
        "loss": 2.8362,
        "grad_norm": 2.6444644927978516,
        "learning_rate": 0.0001746629340447624,
        "epoch": 0.4118786794843133,
        "step": 5527
    },
    {
        "loss": 2.2911,
        "grad_norm": 3.0856337547302246,
        "learning_rate": 0.00017461610548609335,
        "epoch": 0.41195320068559504,
        "step": 5528
    },
    {
        "loss": 2.8508,
        "grad_norm": 2.4623992443084717,
        "learning_rate": 0.0001745692399825473,
        "epoch": 0.4120277218868768,
        "step": 5529
    },
    {
        "loss": 2.0823,
        "grad_norm": 2.025385856628418,
        "learning_rate": 0.00017452233755732888,
        "epoch": 0.41210224308815857,
        "step": 5530
    },
    {
        "loss": 2.1968,
        "grad_norm": 3.706057548522949,
        "learning_rate": 0.00017447539823366092,
        "epoch": 0.41217676428944033,
        "step": 5531
    },
    {
        "loss": 2.7426,
        "grad_norm": 3.114109992980957,
        "learning_rate": 0.00017442842203478471,
        "epoch": 0.4122512854907221,
        "step": 5532
    },
    {
        "loss": 2.8684,
        "grad_norm": 1.931275486946106,
        "learning_rate": 0.00017438140898395975,
        "epoch": 0.41232580669200386,
        "step": 5533
    },
    {
        "loss": 1.9417,
        "grad_norm": 2.7676494121551514,
        "learning_rate": 0.00017433435910446364,
        "epoch": 0.4124003278932856,
        "step": 5534
    },
    {
        "loss": 1.9429,
        "grad_norm": 4.427141189575195,
        "learning_rate": 0.00017428727241959233,
        "epoch": 0.4124748490945674,
        "step": 5535
    },
    {
        "loss": 1.6066,
        "grad_norm": 4.51753568649292,
        "learning_rate": 0.00017424014895265998,
        "epoch": 0.41254937029584915,
        "step": 5536
    },
    {
        "loss": 1.676,
        "grad_norm": 3.9884793758392334,
        "learning_rate": 0.000174192988726999,
        "epoch": 0.4126238914971309,
        "step": 5537
    },
    {
        "loss": 2.6908,
        "grad_norm": 2.3356118202209473,
        "learning_rate": 0.00017414579176595996,
        "epoch": 0.4126984126984127,
        "step": 5538
    },
    {
        "loss": 2.8437,
        "grad_norm": 3.0843725204467773,
        "learning_rate": 0.00017409855809291162,
        "epoch": 0.41277293389969444,
        "step": 5539
    },
    {
        "loss": 2.5371,
        "grad_norm": 1.9367446899414062,
        "learning_rate": 0.00017405128773124092,
        "epoch": 0.4128474551009762,
        "step": 5540
    },
    {
        "loss": 2.5408,
        "grad_norm": 2.8140833377838135,
        "learning_rate": 0.00017400398070435293,
        "epoch": 0.41292197630225796,
        "step": 5541
    },
    {
        "loss": 2.6434,
        "grad_norm": 2.250307083129883,
        "learning_rate": 0.0001739566370356709,
        "epoch": 0.4129964975035398,
        "step": 5542
    },
    {
        "loss": 1.7068,
        "grad_norm": 3.501495361328125,
        "learning_rate": 0.0001739092567486363,
        "epoch": 0.41307101870482155,
        "step": 5543
    },
    {
        "loss": 2.3067,
        "grad_norm": 3.413304328918457,
        "learning_rate": 0.0001738618398667086,
        "epoch": 0.4131455399061033,
        "step": 5544
    },
    {
        "loss": 1.6652,
        "grad_norm": 4.198398590087891,
        "learning_rate": 0.00017381438641336544,
        "epoch": 0.4132200611073851,
        "step": 5545
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.779127597808838,
        "learning_rate": 0.00017376689641210264,
        "epoch": 0.41329458230866684,
        "step": 5546
    },
    {
        "loss": 1.9247,
        "grad_norm": 2.7801690101623535,
        "learning_rate": 0.00017371936988643398,
        "epoch": 0.4133691035099486,
        "step": 5547
    },
    {
        "loss": 2.5682,
        "grad_norm": 1.69906747341156,
        "learning_rate": 0.00017367180685989153,
        "epoch": 0.41344362471123036,
        "step": 5548
    },
    {
        "loss": 1.9607,
        "grad_norm": 3.5669376850128174,
        "learning_rate": 0.0001736242073560251,
        "epoch": 0.4135181459125121,
        "step": 5549
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.821803092956543,
        "learning_rate": 0.000173576571398403,
        "epoch": 0.4135926671137939,
        "step": 5550
    },
    {
        "loss": 3.1807,
        "grad_norm": 4.127379417419434,
        "learning_rate": 0.00017352889901061112,
        "epoch": 0.41366718831507565,
        "step": 5551
    },
    {
        "loss": 2.1352,
        "grad_norm": 4.322855472564697,
        "learning_rate": 0.0001734811902162538,
        "epoch": 0.4137417095163574,
        "step": 5552
    },
    {
        "loss": 1.9755,
        "grad_norm": 4.118196964263916,
        "learning_rate": 0.00017343344503895314,
        "epoch": 0.4138162307176392,
        "step": 5553
    },
    {
        "loss": 2.5665,
        "grad_norm": 2.521784782409668,
        "learning_rate": 0.00017338566350234938,
        "epoch": 0.41389075191892094,
        "step": 5554
    },
    {
        "loss": 2.0537,
        "grad_norm": 3.7539916038513184,
        "learning_rate": 0.00017333784563010064,
        "epoch": 0.4139652731202027,
        "step": 5555
    },
    {
        "loss": 2.4817,
        "grad_norm": 2.4527978897094727,
        "learning_rate": 0.00017328999144588316,
        "epoch": 0.41403979432148447,
        "step": 5556
    },
    {
        "loss": 1.7815,
        "grad_norm": 4.37209415435791,
        "learning_rate": 0.00017324210097339115,
        "epoch": 0.41411431552276623,
        "step": 5557
    },
    {
        "loss": 2.4803,
        "grad_norm": 2.9776182174682617,
        "learning_rate": 0.00017319417423633678,
        "epoch": 0.414188836724048,
        "step": 5558
    },
    {
        "loss": 1.9393,
        "grad_norm": 4.468202590942383,
        "learning_rate": 0.00017314621125845005,
        "epoch": 0.41426335792532976,
        "step": 5559
    },
    {
        "loss": 2.8431,
        "grad_norm": 4.392726898193359,
        "learning_rate": 0.00017309821206347908,
        "epoch": 0.4143378791266115,
        "step": 5560
    },
    {
        "loss": 2.9394,
        "grad_norm": 2.187222719192505,
        "learning_rate": 0.00017305017667518976,
        "epoch": 0.4144124003278933,
        "step": 5561
    },
    {
        "loss": 2.544,
        "grad_norm": 2.404153347015381,
        "learning_rate": 0.00017300210511736607,
        "epoch": 0.41448692152917505,
        "step": 5562
    },
    {
        "loss": 2.4511,
        "grad_norm": 2.4450478553771973,
        "learning_rate": 0.00017295399741380974,
        "epoch": 0.4145614427304568,
        "step": 5563
    },
    {
        "loss": 2.6134,
        "grad_norm": 3.438620090484619,
        "learning_rate": 0.00017290585358834055,
        "epoch": 0.41463596393173857,
        "step": 5564
    },
    {
        "loss": 1.7608,
        "grad_norm": 3.195169448852539,
        "learning_rate": 0.000172857673664796,
        "epoch": 0.41471048513302033,
        "step": 5565
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.577014923095703,
        "learning_rate": 0.00017280945766703158,
        "epoch": 0.4147850063343021,
        "step": 5566
    },
    {
        "loss": 2.3912,
        "grad_norm": 2.3113603591918945,
        "learning_rate": 0.00017276120561892066,
        "epoch": 0.41485952753558386,
        "step": 5567
    },
    {
        "loss": 2.5464,
        "grad_norm": 4.224843502044678,
        "learning_rate": 0.00017271291754435432,
        "epoch": 0.4149340487368656,
        "step": 5568
    },
    {
        "loss": 1.0399,
        "grad_norm": 3.6127679347991943,
        "learning_rate": 0.00017266459346724163,
        "epoch": 0.4150085699381474,
        "step": 5569
    },
    {
        "loss": 2.2944,
        "grad_norm": 2.397091865539551,
        "learning_rate": 0.00017261623341150944,
        "epoch": 0.41508309113942915,
        "step": 5570
    },
    {
        "loss": 2.5323,
        "grad_norm": 2.94504976272583,
        "learning_rate": 0.0001725678374011023,
        "epoch": 0.4151576123407109,
        "step": 5571
    },
    {
        "loss": 2.601,
        "grad_norm": 3.5732920169830322,
        "learning_rate": 0.00017251940545998275,
        "epoch": 0.4152321335419927,
        "step": 5572
    },
    {
        "loss": 1.6696,
        "grad_norm": 3.3445072174072266,
        "learning_rate": 0.000172470937612131,
        "epoch": 0.41530665474327444,
        "step": 5573
    },
    {
        "loss": 2.2101,
        "grad_norm": 5.133798122406006,
        "learning_rate": 0.0001724224338815451,
        "epoch": 0.4153811759445562,
        "step": 5574
    },
    {
        "loss": 1.9282,
        "grad_norm": 3.7074520587921143,
        "learning_rate": 0.00017237389429224074,
        "epoch": 0.41545569714583797,
        "step": 5575
    },
    {
        "loss": 1.7692,
        "grad_norm": 2.2398130893707275,
        "learning_rate": 0.00017232531886825157,
        "epoch": 0.41553021834711973,
        "step": 5576
    },
    {
        "loss": 1.7015,
        "grad_norm": 2.108625650405884,
        "learning_rate": 0.0001722767076336288,
        "epoch": 0.41560473954840155,
        "step": 5577
    },
    {
        "loss": 2.1515,
        "grad_norm": 3.417595624923706,
        "learning_rate": 0.00017222806061244146,
        "epoch": 0.4156792607496833,
        "step": 5578
    },
    {
        "loss": 2.7426,
        "grad_norm": 2.206862449645996,
        "learning_rate": 0.0001721793778287763,
        "epoch": 0.4157537819509651,
        "step": 5579
    },
    {
        "loss": 1.6708,
        "grad_norm": 4.1565961837768555,
        "learning_rate": 0.0001721306593067378,
        "epoch": 0.41582830315224684,
        "step": 5580
    },
    {
        "loss": 2.1377,
        "grad_norm": 3.5804991722106934,
        "learning_rate": 0.00017208190507044804,
        "epoch": 0.4159028243535286,
        "step": 5581
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.5937418937683105,
        "learning_rate": 0.00017203311514404681,
        "epoch": 0.41597734555481036,
        "step": 5582
    },
    {
        "loss": 2.8988,
        "grad_norm": 2.634186267852783,
        "learning_rate": 0.00017198428955169165,
        "epoch": 0.4160518667560921,
        "step": 5583
    },
    {
        "loss": 2.8756,
        "grad_norm": 2.554711103439331,
        "learning_rate": 0.00017193542831755773,
        "epoch": 0.4161263879573739,
        "step": 5584
    },
    {
        "loss": 2.8665,
        "grad_norm": 3.6349375247955322,
        "learning_rate": 0.00017188653146583778,
        "epoch": 0.41620090915865565,
        "step": 5585
    },
    {
        "loss": 1.9151,
        "grad_norm": 3.218937873840332,
        "learning_rate": 0.0001718375990207423,
        "epoch": 0.4162754303599374,
        "step": 5586
    },
    {
        "loss": 2.7411,
        "grad_norm": 2.6124050617218018,
        "learning_rate": 0.00017178863100649928,
        "epoch": 0.4163499515612192,
        "step": 5587
    },
    {
        "loss": 2.107,
        "grad_norm": 5.453056335449219,
        "learning_rate": 0.0001717396274473544,
        "epoch": 0.41642447276250094,
        "step": 5588
    },
    {
        "loss": 2.8316,
        "grad_norm": 2.5753090381622314,
        "learning_rate": 0.00017169058836757098,
        "epoch": 0.4164989939637827,
        "step": 5589
    },
    {
        "loss": 1.9175,
        "grad_norm": 3.2183773517608643,
        "learning_rate": 0.00017164151379142988,
        "epoch": 0.41657351516506447,
        "step": 5590
    },
    {
        "loss": 2.6905,
        "grad_norm": 3.784881591796875,
        "learning_rate": 0.00017159240374322948,
        "epoch": 0.41664803636634623,
        "step": 5591
    },
    {
        "loss": 2.4798,
        "grad_norm": 2.4953598976135254,
        "learning_rate": 0.00017154325824728575,
        "epoch": 0.416722557567628,
        "step": 5592
    },
    {
        "loss": 2.4697,
        "grad_norm": 3.157571315765381,
        "learning_rate": 0.00017149407732793223,
        "epoch": 0.41679707876890976,
        "step": 5593
    },
    {
        "loss": 2.6711,
        "grad_norm": 1.8337823152542114,
        "learning_rate": 0.00017144486100952012,
        "epoch": 0.4168715999701915,
        "step": 5594
    },
    {
        "loss": 2.2008,
        "grad_norm": 3.4530844688415527,
        "learning_rate": 0.00017139560931641795,
        "epoch": 0.4169461211714733,
        "step": 5595
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.183558464050293,
        "learning_rate": 0.0001713463222730118,
        "epoch": 0.41702064237275505,
        "step": 5596
    },
    {
        "loss": 2.2724,
        "grad_norm": 2.6454477310180664,
        "learning_rate": 0.00017129699990370536,
        "epoch": 0.4170951635740368,
        "step": 5597
    },
    {
        "loss": 2.0008,
        "grad_norm": 3.376580238342285,
        "learning_rate": 0.00017124764223291973,
        "epoch": 0.41716968477531857,
        "step": 5598
    },
    {
        "loss": 2.743,
        "grad_norm": 3.0178534984588623,
        "learning_rate": 0.00017119824928509353,
        "epoch": 0.41724420597660034,
        "step": 5599
    },
    {
        "loss": 3.2036,
        "grad_norm": 2.315939426422119,
        "learning_rate": 0.0001711488210846828,
        "epoch": 0.4173187271778821,
        "step": 5600
    },
    {
        "loss": 1.9535,
        "grad_norm": 2.866105079650879,
        "learning_rate": 0.0001710993576561611,
        "epoch": 0.41739324837916386,
        "step": 5601
    },
    {
        "loss": 2.8267,
        "grad_norm": 3.836883068084717,
        "learning_rate": 0.00017104985902401937,
        "epoch": 0.4174677695804456,
        "step": 5602
    },
    {
        "loss": 2.5436,
        "grad_norm": 2.8641421794891357,
        "learning_rate": 0.00017100032521276596,
        "epoch": 0.4175422907817274,
        "step": 5603
    },
    {
        "loss": 2.8675,
        "grad_norm": 2.560133934020996,
        "learning_rate": 0.00017095075624692677,
        "epoch": 0.41761681198300915,
        "step": 5604
    },
    {
        "loss": 2.902,
        "grad_norm": 3.001769781112671,
        "learning_rate": 0.000170901152151045,
        "epoch": 0.4176913331842909,
        "step": 5605
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.30083966255188,
        "learning_rate": 0.00017085151294968112,
        "epoch": 0.4177658543855727,
        "step": 5606
    },
    {
        "loss": 2.053,
        "grad_norm": 3.4723920822143555,
        "learning_rate": 0.0001708018386674133,
        "epoch": 0.41784037558685444,
        "step": 5607
    },
    {
        "loss": 2.53,
        "grad_norm": 3.183593988418579,
        "learning_rate": 0.00017075212932883676,
        "epoch": 0.4179148967881362,
        "step": 5608
    },
    {
        "loss": 1.9895,
        "grad_norm": 3.902456760406494,
        "learning_rate": 0.0001707023849585644,
        "epoch": 0.41798941798941797,
        "step": 5609
    },
    {
        "loss": 2.5562,
        "grad_norm": 4.304164886474609,
        "learning_rate": 0.00017065260558122615,
        "epoch": 0.41806393919069973,
        "step": 5610
    },
    {
        "loss": 2.1145,
        "grad_norm": 2.6694064140319824,
        "learning_rate": 0.00017060279122146948,
        "epoch": 0.4181384603919815,
        "step": 5611
    },
    {
        "loss": 2.9208,
        "grad_norm": 3.4425928592681885,
        "learning_rate": 0.00017055294190395902,
        "epoch": 0.4182129815932633,
        "step": 5612
    },
    {
        "loss": 2.4862,
        "grad_norm": 3.3561673164367676,
        "learning_rate": 0.00017050305765337685,
        "epoch": 0.4182875027945451,
        "step": 5613
    },
    {
        "loss": 2.2066,
        "grad_norm": 3.542856454849243,
        "learning_rate": 0.00017045313849442238,
        "epoch": 0.41836202399582684,
        "step": 5614
    },
    {
        "loss": 1.8615,
        "grad_norm": 5.297613143920898,
        "learning_rate": 0.00017040318445181213,
        "epoch": 0.4184365451971086,
        "step": 5615
    },
    {
        "loss": 2.3426,
        "grad_norm": 3.184107542037964,
        "learning_rate": 0.00017035319555027995,
        "epoch": 0.41851106639839036,
        "step": 5616
    },
    {
        "loss": 2.4422,
        "grad_norm": 3.617027521133423,
        "learning_rate": 0.000170303171814577,
        "epoch": 0.4185855875996721,
        "step": 5617
    },
    {
        "loss": 2.3166,
        "grad_norm": 3.3707494735717773,
        "learning_rate": 0.00017025311326947175,
        "epoch": 0.4186601088009539,
        "step": 5618
    },
    {
        "loss": 2.1524,
        "grad_norm": 3.352843761444092,
        "learning_rate": 0.00017020301993974979,
        "epoch": 0.41873463000223565,
        "step": 5619
    },
    {
        "loss": 2.7456,
        "grad_norm": 4.054963111877441,
        "learning_rate": 0.0001701528918502139,
        "epoch": 0.4188091512035174,
        "step": 5620
    },
    {
        "loss": 2.7424,
        "grad_norm": 4.765679836273193,
        "learning_rate": 0.00017010272902568424,
        "epoch": 0.4188836724047992,
        "step": 5621
    },
    {
        "loss": 2.7058,
        "grad_norm": 2.5511374473571777,
        "learning_rate": 0.00017005253149099793,
        "epoch": 0.41895819360608094,
        "step": 5622
    },
    {
        "loss": 2.4633,
        "grad_norm": 2.7718236446380615,
        "learning_rate": 0.0001700022992710096,
        "epoch": 0.4190327148073627,
        "step": 5623
    },
    {
        "loss": 2.6294,
        "grad_norm": 3.5201573371887207,
        "learning_rate": 0.00016995203239059069,
        "epoch": 0.41910723600864447,
        "step": 5624
    },
    {
        "loss": 2.4098,
        "grad_norm": 2.8988513946533203,
        "learning_rate": 0.0001699017308746301,
        "epoch": 0.41918175720992623,
        "step": 5625
    },
    {
        "loss": 2.3016,
        "grad_norm": 3.162741184234619,
        "learning_rate": 0.00016985139474803366,
        "epoch": 0.419256278411208,
        "step": 5626
    },
    {
        "loss": 2.6863,
        "grad_norm": 3.0912599563598633,
        "learning_rate": 0.0001698010240357245,
        "epoch": 0.41933079961248976,
        "step": 5627
    },
    {
        "loss": 2.7704,
        "grad_norm": 2.8044064044952393,
        "learning_rate": 0.00016975061876264283,
        "epoch": 0.4194053208137715,
        "step": 5628
    },
    {
        "loss": 3.0227,
        "grad_norm": 2.40140700340271,
        "learning_rate": 0.00016970017895374587,
        "epoch": 0.4194798420150533,
        "step": 5629
    },
    {
        "loss": 1.805,
        "grad_norm": 3.746128797531128,
        "learning_rate": 0.00016964970463400803,
        "epoch": 0.41955436321633505,
        "step": 5630
    },
    {
        "loss": 3.1109,
        "grad_norm": 2.439436197280884,
        "learning_rate": 0.00016959919582842093,
        "epoch": 0.4196288844176168,
        "step": 5631
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.738739252090454,
        "learning_rate": 0.00016954865256199296,
        "epoch": 0.4197034056188986,
        "step": 5632
    },
    {
        "loss": 2.8653,
        "grad_norm": 2.663947582244873,
        "learning_rate": 0.00016949807485974988,
        "epoch": 0.41977792682018034,
        "step": 5633
    },
    {
        "loss": 2.9024,
        "grad_norm": 2.065492630004883,
        "learning_rate": 0.0001694474627467343,
        "epoch": 0.4198524480214621,
        "step": 5634
    },
    {
        "loss": 2.0473,
        "grad_norm": 3.5226595401763916,
        "learning_rate": 0.00016939681624800597,
        "epoch": 0.41992696922274386,
        "step": 5635
    },
    {
        "loss": 2.7347,
        "grad_norm": 3.3170318603515625,
        "learning_rate": 0.00016934613538864156,
        "epoch": 0.4200014904240256,
        "step": 5636
    },
    {
        "loss": 2.849,
        "grad_norm": 2.0327646732330322,
        "learning_rate": 0.0001692954201937348,
        "epoch": 0.4200760116253074,
        "step": 5637
    },
    {
        "loss": 2.1982,
        "grad_norm": 4.4729509353637695,
        "learning_rate": 0.00016924467068839665,
        "epoch": 0.42015053282658915,
        "step": 5638
    },
    {
        "loss": 2.8041,
        "grad_norm": 2.795872449874878,
        "learning_rate": 0.0001691938868977546,
        "epoch": 0.4202250540278709,
        "step": 5639
    },
    {
        "loss": 2.2203,
        "grad_norm": 3.4439449310302734,
        "learning_rate": 0.00016914306884695352,
        "epoch": 0.4202995752291527,
        "step": 5640
    },
    {
        "loss": 2.4724,
        "grad_norm": 2.48586106300354,
        "learning_rate": 0.0001690922165611551,
        "epoch": 0.42037409643043444,
        "step": 5641
    },
    {
        "loss": 2.066,
        "grad_norm": 3.121354818344116,
        "learning_rate": 0.00016904133006553793,
        "epoch": 0.4204486176317162,
        "step": 5642
    },
    {
        "loss": 2.0009,
        "grad_norm": 4.8888983726501465,
        "learning_rate": 0.0001689904093852975,
        "epoch": 0.42052313883299797,
        "step": 5643
    },
    {
        "loss": 2.0748,
        "grad_norm": 3.022561550140381,
        "learning_rate": 0.00016893945454564638,
        "epoch": 0.42059766003427973,
        "step": 5644
    },
    {
        "loss": 2.35,
        "grad_norm": 3.5818467140197754,
        "learning_rate": 0.00016888846557181402,
        "epoch": 0.4206721812355615,
        "step": 5645
    },
    {
        "loss": 2.5641,
        "grad_norm": 2.5572354793548584,
        "learning_rate": 0.00016883744248904668,
        "epoch": 0.42074670243684326,
        "step": 5646
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.569382429122925,
        "learning_rate": 0.00016878638532260757,
        "epoch": 0.420821223638125,
        "step": 5647
    },
    {
        "loss": 2.6582,
        "grad_norm": 2.299468994140625,
        "learning_rate": 0.00016873529409777675,
        "epoch": 0.42089574483940684,
        "step": 5648
    },
    {
        "loss": 2.0394,
        "grad_norm": 4.495323657989502,
        "learning_rate": 0.0001686841688398512,
        "epoch": 0.4209702660406886,
        "step": 5649
    },
    {
        "loss": 2.2874,
        "grad_norm": 2.561420440673828,
        "learning_rate": 0.0001686330095741446,
        "epoch": 0.42104478724197036,
        "step": 5650
    },
    {
        "loss": 2.2243,
        "grad_norm": 2.079256534576416,
        "learning_rate": 0.00016858181632598774,
        "epoch": 0.4211193084432521,
        "step": 5651
    },
    {
        "loss": 1.5861,
        "grad_norm": 3.8133654594421387,
        "learning_rate": 0.00016853058912072802,
        "epoch": 0.4211938296445339,
        "step": 5652
    },
    {
        "loss": 2.2972,
        "grad_norm": 3.6890830993652344,
        "learning_rate": 0.0001684793279837296,
        "epoch": 0.42126835084581565,
        "step": 5653
    },
    {
        "loss": 2.8019,
        "grad_norm": 1.5594135522842407,
        "learning_rate": 0.00016842803294037366,
        "epoch": 0.4213428720470974,
        "step": 5654
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.687049388885498,
        "learning_rate": 0.00016837670401605802,
        "epoch": 0.4214173932483792,
        "step": 5655
    },
    {
        "loss": 2.213,
        "grad_norm": 3.8009355068206787,
        "learning_rate": 0.00016832534123619735,
        "epoch": 0.42149191444966094,
        "step": 5656
    },
    {
        "loss": 1.9799,
        "grad_norm": 2.9658117294311523,
        "learning_rate": 0.00016827394462622298,
        "epoch": 0.4215664356509427,
        "step": 5657
    },
    {
        "loss": 2.3854,
        "grad_norm": 2.907825469970703,
        "learning_rate": 0.00016822251421158303,
        "epoch": 0.42164095685222447,
        "step": 5658
    },
    {
        "loss": 1.9575,
        "grad_norm": 3.5591328144073486,
        "learning_rate": 0.00016817105001774247,
        "epoch": 0.42171547805350623,
        "step": 5659
    },
    {
        "loss": 2.1487,
        "grad_norm": 3.123729944229126,
        "learning_rate": 0.00016811955207018283,
        "epoch": 0.421789999254788,
        "step": 5660
    },
    {
        "loss": 1.7415,
        "grad_norm": 2.941762924194336,
        "learning_rate": 0.00016806802039440256,
        "epoch": 0.42186452045606976,
        "step": 5661
    },
    {
        "loss": 1.754,
        "grad_norm": 4.466666221618652,
        "learning_rate": 0.00016801645501591657,
        "epoch": 0.4219390416573515,
        "step": 5662
    },
    {
        "loss": 2.8832,
        "grad_norm": 2.0748119354248047,
        "learning_rate": 0.00016796485596025655,
        "epoch": 0.4220135628586333,
        "step": 5663
    },
    {
        "loss": 2.6873,
        "grad_norm": 2.697253704071045,
        "learning_rate": 0.0001679132232529709,
        "epoch": 0.42208808405991505,
        "step": 5664
    },
    {
        "loss": 2.8179,
        "grad_norm": 2.2912838459014893,
        "learning_rate": 0.00016786155691962478,
        "epoch": 0.4221626052611968,
        "step": 5665
    },
    {
        "loss": 2.0462,
        "grad_norm": 3.8030002117156982,
        "learning_rate": 0.00016780985698579975,
        "epoch": 0.4222371264624786,
        "step": 5666
    },
    {
        "loss": 2.7986,
        "grad_norm": 2.463409900665283,
        "learning_rate": 0.00016775812347709414,
        "epoch": 0.42231164766376034,
        "step": 5667
    },
    {
        "loss": 2.5379,
        "grad_norm": 3.1610519886016846,
        "learning_rate": 0.00016770635641912294,
        "epoch": 0.4223861688650421,
        "step": 5668
    },
    {
        "loss": 2.4423,
        "grad_norm": 4.213657855987549,
        "learning_rate": 0.00016765455583751771,
        "epoch": 0.42246069006632386,
        "step": 5669
    },
    {
        "loss": 2.0312,
        "grad_norm": 3.4123196601867676,
        "learning_rate": 0.0001676027217579267,
        "epoch": 0.4225352112676056,
        "step": 5670
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.91351056098938,
        "learning_rate": 0.0001675508542060145,
        "epoch": 0.4226097324688874,
        "step": 5671
    },
    {
        "loss": 2.0205,
        "grad_norm": 2.7001211643218994,
        "learning_rate": 0.00016749895320746262,
        "epoch": 0.42268425367016915,
        "step": 5672
    },
    {
        "loss": 2.0963,
        "grad_norm": 3.735872507095337,
        "learning_rate": 0.00016744701878796872,
        "epoch": 0.4227587748714509,
        "step": 5673
    },
    {
        "loss": 2.5808,
        "grad_norm": 2.7315585613250732,
        "learning_rate": 0.00016739505097324738,
        "epoch": 0.4228332960727327,
        "step": 5674
    },
    {
        "loss": 2.3688,
        "grad_norm": 4.057600498199463,
        "learning_rate": 0.0001673430497890296,
        "epoch": 0.42290781727401444,
        "step": 5675
    },
    {
        "loss": 2.9635,
        "grad_norm": 3.7419753074645996,
        "learning_rate": 0.00016729101526106278,
        "epoch": 0.4229823384752962,
        "step": 5676
    },
    {
        "loss": 1.9093,
        "grad_norm": 4.008883476257324,
        "learning_rate": 0.0001672389474151109,
        "epoch": 0.42305685967657797,
        "step": 5677
    },
    {
        "loss": 1.5916,
        "grad_norm": 4.219283103942871,
        "learning_rate": 0.00016718684627695454,
        "epoch": 0.42313138087785973,
        "step": 5678
    },
    {
        "loss": 1.783,
        "grad_norm": 2.9315683841705322,
        "learning_rate": 0.0001671347118723906,
        "epoch": 0.4232059020791415,
        "step": 5679
    },
    {
        "loss": 2.1707,
        "grad_norm": 3.5405428409576416,
        "learning_rate": 0.00016708254422723263,
        "epoch": 0.42328042328042326,
        "step": 5680
    },
    {
        "loss": 2.6069,
        "grad_norm": 2.745666265487671,
        "learning_rate": 0.00016703034336731042,
        "epoch": 0.423354944481705,
        "step": 5681
    },
    {
        "loss": 2.205,
        "grad_norm": 3.056467294692993,
        "learning_rate": 0.00016697810931847045,
        "epoch": 0.4234294656829868,
        "step": 5682
    },
    {
        "loss": 2.1984,
        "grad_norm": 2.99721097946167,
        "learning_rate": 0.0001669258421065754,
        "epoch": 0.4235039868842686,
        "step": 5683
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.973633289337158,
        "learning_rate": 0.0001668735417575045,
        "epoch": 0.42357850808555036,
        "step": 5684
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.4905970096588135,
        "learning_rate": 0.00016682120829715345,
        "epoch": 0.42365302928683213,
        "step": 5685
    },
    {
        "loss": 2.522,
        "grad_norm": 2.512716293334961,
        "learning_rate": 0.00016676884175143418,
        "epoch": 0.4237275504881139,
        "step": 5686
    },
    {
        "loss": 2.5284,
        "grad_norm": 3.1651976108551025,
        "learning_rate": 0.00016671644214627507,
        "epoch": 0.42380207168939565,
        "step": 5687
    },
    {
        "loss": 2.2416,
        "grad_norm": 3.671865463256836,
        "learning_rate": 0.0001666640095076209,
        "epoch": 0.4238765928906774,
        "step": 5688
    },
    {
        "loss": 2.6874,
        "grad_norm": 2.0572333335876465,
        "learning_rate": 0.00016661154386143283,
        "epoch": 0.4239511140919592,
        "step": 5689
    },
    {
        "loss": 1.8815,
        "grad_norm": 3.315267562866211,
        "learning_rate": 0.00016655904523368823,
        "epoch": 0.42402563529324094,
        "step": 5690
    },
    {
        "loss": 1.229,
        "grad_norm": 2.1528091430664062,
        "learning_rate": 0.00016650651365038096,
        "epoch": 0.4241001564945227,
        "step": 5691
    },
    {
        "loss": 1.8155,
        "grad_norm": 3.728442668914795,
        "learning_rate": 0.00016645394913752118,
        "epoch": 0.42417467769580447,
        "step": 5692
    },
    {
        "loss": 2.5841,
        "grad_norm": 2.1458675861358643,
        "learning_rate": 0.00016640135172113512,
        "epoch": 0.42424919889708623,
        "step": 5693
    },
    {
        "loss": 2.5642,
        "grad_norm": 3.1711275577545166,
        "learning_rate": 0.0001663487214272657,
        "epoch": 0.424323720098368,
        "step": 5694
    },
    {
        "loss": 2.3728,
        "grad_norm": 2.564765453338623,
        "learning_rate": 0.00016629605828197175,
        "epoch": 0.42439824129964976,
        "step": 5695
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.76920223236084,
        "learning_rate": 0.0001662433623113286,
        "epoch": 0.4244727625009315,
        "step": 5696
    },
    {
        "loss": 2.0715,
        "grad_norm": 3.8882482051849365,
        "learning_rate": 0.00016619063354142766,
        "epoch": 0.4245472837022133,
        "step": 5697
    },
    {
        "loss": 2.214,
        "grad_norm": 1.8010973930358887,
        "learning_rate": 0.0001661378719983767,
        "epoch": 0.42462180490349505,
        "step": 5698
    },
    {
        "loss": 1.9209,
        "grad_norm": 3.671787977218628,
        "learning_rate": 0.0001660850777082998,
        "epoch": 0.4246963261047768,
        "step": 5699
    },
    {
        "loss": 2.7161,
        "grad_norm": 2.07131290435791,
        "learning_rate": 0.00016603225069733698,
        "epoch": 0.4247708473060586,
        "step": 5700
    },
    {
        "loss": 2.1786,
        "grad_norm": 3.9611551761627197,
        "learning_rate": 0.0001659793909916447,
        "epoch": 0.42484536850734034,
        "step": 5701
    },
    {
        "loss": 2.5651,
        "grad_norm": 2.8557777404785156,
        "learning_rate": 0.00016592649861739558,
        "epoch": 0.4249198897086221,
        "step": 5702
    },
    {
        "loss": 2.7069,
        "grad_norm": 2.273803949356079,
        "learning_rate": 0.00016587357360077834,
        "epoch": 0.42499441090990386,
        "step": 5703
    },
    {
        "loss": 1.4146,
        "grad_norm": 3.281521797180176,
        "learning_rate": 0.00016582061596799778,
        "epoch": 0.4250689321111856,
        "step": 5704
    },
    {
        "loss": 1.1207,
        "grad_norm": 3.144666910171509,
        "learning_rate": 0.00016576762574527508,
        "epoch": 0.4251434533124674,
        "step": 5705
    },
    {
        "loss": 2.7298,
        "grad_norm": 1.1973631381988525,
        "learning_rate": 0.0001657146029588474,
        "epoch": 0.42521797451374915,
        "step": 5706
    },
    {
        "loss": 2.9608,
        "grad_norm": 3.6777265071868896,
        "learning_rate": 0.00016566154763496802,
        "epoch": 0.4252924957150309,
        "step": 5707
    },
    {
        "loss": 2.0844,
        "grad_norm": 2.7931888103485107,
        "learning_rate": 0.00016560845979990643,
        "epoch": 0.4253670169163127,
        "step": 5708
    },
    {
        "loss": 2.674,
        "grad_norm": 2.805067539215088,
        "learning_rate": 0.00016555533947994812,
        "epoch": 0.42544153811759444,
        "step": 5709
    },
    {
        "loss": 2.523,
        "grad_norm": 2.787154197692871,
        "learning_rate": 0.0001655021867013947,
        "epoch": 0.4255160593188762,
        "step": 5710
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.551135778427124,
        "learning_rate": 0.00016544900149056385,
        "epoch": 0.42559058052015797,
        "step": 5711
    },
    {
        "loss": 2.5781,
        "grad_norm": 3.0345287322998047,
        "learning_rate": 0.00016539578387378935,
        "epoch": 0.42566510172143973,
        "step": 5712
    },
    {
        "loss": 1.2518,
        "grad_norm": 4.663123607635498,
        "learning_rate": 0.00016534253387742097,
        "epoch": 0.4257396229227215,
        "step": 5713
    },
    {
        "loss": 2.031,
        "grad_norm": 2.046614170074463,
        "learning_rate": 0.00016528925152782447,
        "epoch": 0.42581414412400326,
        "step": 5714
    },
    {
        "loss": 1.7049,
        "grad_norm": 2.6651759147644043,
        "learning_rate": 0.0001652359368513817,
        "epoch": 0.425888665325285,
        "step": 5715
    },
    {
        "loss": 2.5997,
        "grad_norm": 2.1969499588012695,
        "learning_rate": 0.00016518258987449058,
        "epoch": 0.4259631865265668,
        "step": 5716
    },
    {
        "loss": 2.0793,
        "grad_norm": 2.2433011531829834,
        "learning_rate": 0.0001651292106235649,
        "epoch": 0.42603770772784855,
        "step": 5717
    },
    {
        "loss": 2.3559,
        "grad_norm": 3.7557687759399414,
        "learning_rate": 0.00016507579912503442,
        "epoch": 0.4261122289291303,
        "step": 5718
    },
    {
        "loss": 2.5396,
        "grad_norm": 1.8715128898620605,
        "learning_rate": 0.00016502235540534496,
        "epoch": 0.42618675013041213,
        "step": 5719
    },
    {
        "loss": 2.8394,
        "grad_norm": 2.862762928009033,
        "learning_rate": 0.00016496887949095826,
        "epoch": 0.4262612713316939,
        "step": 5720
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.6032607555389404,
        "learning_rate": 0.000164915371408352,
        "epoch": 0.42633579253297565,
        "step": 5721
    },
    {
        "loss": 3.1106,
        "grad_norm": 4.17368221282959,
        "learning_rate": 0.00016486183118401987,
        "epoch": 0.4264103137342574,
        "step": 5722
    },
    {
        "loss": 2.0836,
        "grad_norm": 2.7934634685516357,
        "learning_rate": 0.00016480825884447126,
        "epoch": 0.4264848349355392,
        "step": 5723
    },
    {
        "loss": 2.4127,
        "grad_norm": 3.938499689102173,
        "learning_rate": 0.0001647546544162316,
        "epoch": 0.42655935613682094,
        "step": 5724
    },
    {
        "loss": 2.6909,
        "grad_norm": 2.222700595855713,
        "learning_rate": 0.00016470101792584221,
        "epoch": 0.4266338773381027,
        "step": 5725
    },
    {
        "loss": 2.7459,
        "grad_norm": 2.0022499561309814,
        "learning_rate": 0.00016464734939986036,
        "epoch": 0.42670839853938447,
        "step": 5726
    },
    {
        "loss": 2.5112,
        "grad_norm": 3.3427984714508057,
        "learning_rate": 0.00016459364886485905,
        "epoch": 0.42678291974066623,
        "step": 5727
    },
    {
        "loss": 2.1059,
        "grad_norm": 3.983466386795044,
        "learning_rate": 0.00016453991634742706,
        "epoch": 0.426857440941948,
        "step": 5728
    },
    {
        "loss": 2.3199,
        "grad_norm": 2.089907169342041,
        "learning_rate": 0.0001644861518741692,
        "epoch": 0.42693196214322976,
        "step": 5729
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.6449546813964844,
        "learning_rate": 0.00016443235547170605,
        "epoch": 0.4270064833445115,
        "step": 5730
    },
    {
        "loss": 2.3939,
        "grad_norm": 3.2288320064544678,
        "learning_rate": 0.00016437852716667397,
        "epoch": 0.4270810045457933,
        "step": 5731
    },
    {
        "loss": 2.5298,
        "grad_norm": 2.4156851768493652,
        "learning_rate": 0.0001643246669857251,
        "epoch": 0.42715552574707505,
        "step": 5732
    },
    {
        "loss": 2.1545,
        "grad_norm": 3.1842026710510254,
        "learning_rate": 0.0001642707749555274,
        "epoch": 0.4272300469483568,
        "step": 5733
    },
    {
        "loss": 2.4488,
        "grad_norm": 4.513912677764893,
        "learning_rate": 0.00016421685110276453,
        "epoch": 0.4273045681496386,
        "step": 5734
    },
    {
        "loss": 2.4451,
        "grad_norm": 3.6686925888061523,
        "learning_rate": 0.00016416289545413598,
        "epoch": 0.42737908935092034,
        "step": 5735
    },
    {
        "loss": 2.1703,
        "grad_norm": 2.8085503578186035,
        "learning_rate": 0.00016410890803635696,
        "epoch": 0.4274536105522021,
        "step": 5736
    },
    {
        "loss": 2.7111,
        "grad_norm": 2.254865884780884,
        "learning_rate": 0.00016405488887615843,
        "epoch": 0.42752813175348386,
        "step": 5737
    },
    {
        "loss": 2.5079,
        "grad_norm": 2.3125035762786865,
        "learning_rate": 0.00016400083800028695,
        "epoch": 0.4276026529547656,
        "step": 5738
    },
    {
        "loss": 2.4825,
        "grad_norm": 2.2795045375823975,
        "learning_rate": 0.00016394675543550497,
        "epoch": 0.4276771741560474,
        "step": 5739
    },
    {
        "loss": 2.371,
        "grad_norm": 2.357215166091919,
        "learning_rate": 0.00016389264120859055,
        "epoch": 0.42775169535732915,
        "step": 5740
    },
    {
        "loss": 2.274,
        "grad_norm": 3.8676490783691406,
        "learning_rate": 0.0001638384953463374,
        "epoch": 0.4278262165586109,
        "step": 5741
    },
    {
        "loss": 2.586,
        "grad_norm": 2.68119478225708,
        "learning_rate": 0.00016378431787555483,
        "epoch": 0.4279007377598927,
        "step": 5742
    },
    {
        "loss": 2.4276,
        "grad_norm": 3.0244195461273193,
        "learning_rate": 0.000163730108823068,
        "epoch": 0.42797525896117444,
        "step": 5743
    },
    {
        "loss": 2.2216,
        "grad_norm": 2.992143154144287,
        "learning_rate": 0.0001636758682157175,
        "epoch": 0.4280497801624562,
        "step": 5744
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.358388900756836,
        "learning_rate": 0.00016362159608035963,
        "epoch": 0.42812430136373797,
        "step": 5745
    },
    {
        "loss": 2.2957,
        "grad_norm": 2.034010171890259,
        "learning_rate": 0.00016356729244386645,
        "epoch": 0.42819882256501973,
        "step": 5746
    },
    {
        "loss": 2.0172,
        "grad_norm": 3.4775075912475586,
        "learning_rate": 0.00016351295733312528,
        "epoch": 0.4282733437663015,
        "step": 5747
    },
    {
        "loss": 2.4639,
        "grad_norm": 3.504725694656372,
        "learning_rate": 0.00016345859077503925,
        "epoch": 0.42834786496758326,
        "step": 5748
    },
    {
        "loss": 2.4811,
        "grad_norm": 3.1547563076019287,
        "learning_rate": 0.00016340419279652706,
        "epoch": 0.428422386168865,
        "step": 5749
    },
    {
        "loss": 2.8137,
        "grad_norm": 3.4676337242126465,
        "learning_rate": 0.00016334976342452298,
        "epoch": 0.4284969073701468,
        "step": 5750
    },
    {
        "loss": 2.3545,
        "grad_norm": 2.691164255142212,
        "learning_rate": 0.00016329530268597667,
        "epoch": 0.42857142857142855,
        "step": 5751
    },
    {
        "loss": 2.5464,
        "grad_norm": 3.294471502304077,
        "learning_rate": 0.00016324081060785345,
        "epoch": 0.4286459497727103,
        "step": 5752
    },
    {
        "loss": 2.0465,
        "grad_norm": 3.4665684700012207,
        "learning_rate": 0.00016318628721713425,
        "epoch": 0.4287204709739921,
        "step": 5753
    },
    {
        "loss": 2.198,
        "grad_norm": 3.079293966293335,
        "learning_rate": 0.0001631317325408152,
        "epoch": 0.4287949921752739,
        "step": 5754
    },
    {
        "loss": 2.4226,
        "grad_norm": 4.103312015533447,
        "learning_rate": 0.0001630771466059083,
        "epoch": 0.42886951337655566,
        "step": 5755
    },
    {
        "loss": 2.1986,
        "grad_norm": 3.374500036239624,
        "learning_rate": 0.00016302252943944067,
        "epoch": 0.4289440345778374,
        "step": 5756
    },
    {
        "loss": 2.1717,
        "grad_norm": 3.6532418727874756,
        "learning_rate": 0.00016296788106845512,
        "epoch": 0.4290185557791192,
        "step": 5757
    },
    {
        "loss": 2.3717,
        "grad_norm": 2.018603801727295,
        "learning_rate": 0.00016291320152000987,
        "epoch": 0.42909307698040094,
        "step": 5758
    },
    {
        "loss": 1.6903,
        "grad_norm": 3.899461030960083,
        "learning_rate": 0.0001628584908211785,
        "epoch": 0.4291675981816827,
        "step": 5759
    },
    {
        "loss": 2.6216,
        "grad_norm": 1.8783609867095947,
        "learning_rate": 0.00016280374899905017,
        "epoch": 0.42924211938296447,
        "step": 5760
    },
    {
        "loss": 2.4584,
        "grad_norm": 3.2616372108459473,
        "learning_rate": 0.0001627489760807292,
        "epoch": 0.42931664058424623,
        "step": 5761
    },
    {
        "loss": 1.8967,
        "grad_norm": 3.793628692626953,
        "learning_rate": 0.00016269417209333552,
        "epoch": 0.429391161785528,
        "step": 5762
    },
    {
        "loss": 2.2886,
        "grad_norm": 2.492666006088257,
        "learning_rate": 0.00016263933706400451,
        "epoch": 0.42946568298680976,
        "step": 5763
    },
    {
        "loss": 2.5902,
        "grad_norm": 1.8113616704940796,
        "learning_rate": 0.00016258447101988663,
        "epoch": 0.4295402041880915,
        "step": 5764
    },
    {
        "loss": 2.3247,
        "grad_norm": 2.9063401222229004,
        "learning_rate": 0.00016252957398814788,
        "epoch": 0.4296147253893733,
        "step": 5765
    },
    {
        "loss": 2.4613,
        "grad_norm": 2.8835418224334717,
        "learning_rate": 0.00016247464599596957,
        "epoch": 0.42968924659065505,
        "step": 5766
    },
    {
        "loss": 2.623,
        "grad_norm": 3.2254559993743896,
        "learning_rate": 0.00016241968707054842,
        "epoch": 0.4297637677919368,
        "step": 5767
    },
    {
        "loss": 2.953,
        "grad_norm": 1.987134575843811,
        "learning_rate": 0.00016236469723909634,
        "epoch": 0.4298382889932186,
        "step": 5768
    },
    {
        "loss": 1.8488,
        "grad_norm": 2.855109453201294,
        "learning_rate": 0.0001623096765288406,
        "epoch": 0.42991281019450034,
        "step": 5769
    },
    {
        "loss": 1.3533,
        "grad_norm": 2.635634660720825,
        "learning_rate": 0.00016225462496702374,
        "epoch": 0.4299873313957821,
        "step": 5770
    },
    {
        "loss": 2.464,
        "grad_norm": 1.6785892248153687,
        "learning_rate": 0.0001621995425809036,
        "epoch": 0.43006185259706387,
        "step": 5771
    },
    {
        "loss": 3.1104,
        "grad_norm": 3.6015357971191406,
        "learning_rate": 0.0001621444293977533,
        "epoch": 0.43013637379834563,
        "step": 5772
    },
    {
        "loss": 3.0586,
        "grad_norm": 3.0582311153411865,
        "learning_rate": 0.0001620892854448612,
        "epoch": 0.4302108949996274,
        "step": 5773
    },
    {
        "loss": 2.2322,
        "grad_norm": 4.431338787078857,
        "learning_rate": 0.00016203411074953083,
        "epoch": 0.43028541620090915,
        "step": 5774
    },
    {
        "loss": 2.1934,
        "grad_norm": 2.9605798721313477,
        "learning_rate": 0.00016197890533908095,
        "epoch": 0.4303599374021909,
        "step": 5775
    },
    {
        "loss": 2.5231,
        "grad_norm": 3.2625694274902344,
        "learning_rate": 0.00016192366924084564,
        "epoch": 0.4304344586034727,
        "step": 5776
    },
    {
        "loss": 2.4711,
        "grad_norm": 3.876491069793701,
        "learning_rate": 0.0001618684024821741,
        "epoch": 0.43050897980475444,
        "step": 5777
    },
    {
        "loss": 2.8525,
        "grad_norm": 3.1142361164093018,
        "learning_rate": 0.00016181310509043065,
        "epoch": 0.4305835010060362,
        "step": 5778
    },
    {
        "loss": 2.0612,
        "grad_norm": 2.52404522895813,
        "learning_rate": 0.00016175777709299496,
        "epoch": 0.43065802220731797,
        "step": 5779
    },
    {
        "loss": 2.1679,
        "grad_norm": 2.8815529346466064,
        "learning_rate": 0.00016170241851726154,
        "epoch": 0.43073254340859973,
        "step": 5780
    },
    {
        "loss": 1.5568,
        "grad_norm": 1.8901665210723877,
        "learning_rate": 0.00016164702939064037,
        "epoch": 0.4308070646098815,
        "step": 5781
    },
    {
        "loss": 2.714,
        "grad_norm": 2.164980411529541,
        "learning_rate": 0.0001615916097405564,
        "epoch": 0.43088158581116326,
        "step": 5782
    },
    {
        "loss": 2.5115,
        "grad_norm": 2.7832772731781006,
        "learning_rate": 0.00016153615959444976,
        "epoch": 0.430956107012445,
        "step": 5783
    },
    {
        "loss": 2.7072,
        "grad_norm": 2.4378116130828857,
        "learning_rate": 0.0001614806789797756,
        "epoch": 0.4310306282137268,
        "step": 5784
    },
    {
        "loss": 2.2711,
        "grad_norm": 3.485962152481079,
        "learning_rate": 0.00016142516792400408,
        "epoch": 0.43110514941500855,
        "step": 5785
    },
    {
        "loss": 2.3419,
        "grad_norm": 5.199365139007568,
        "learning_rate": 0.00016136962645462065,
        "epoch": 0.4311796706162903,
        "step": 5786
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.91257643699646,
        "learning_rate": 0.00016131405459912575,
        "epoch": 0.4312541918175721,
        "step": 5787
    },
    {
        "loss": 3.1269,
        "grad_norm": 1.3088138103485107,
        "learning_rate": 0.00016125845238503477,
        "epoch": 0.43132871301885384,
        "step": 5788
    },
    {
        "loss": 1.6128,
        "grad_norm": 6.456491947174072,
        "learning_rate": 0.0001612028198398781,
        "epoch": 0.43140323422013566,
        "step": 5789
    },
    {
        "loss": 2.0528,
        "grad_norm": 3.3178255558013916,
        "learning_rate": 0.00016114715699120136,
        "epoch": 0.4314777554214174,
        "step": 5790
    },
    {
        "loss": 2.5774,
        "grad_norm": 2.124157667160034,
        "learning_rate": 0.00016109146386656496,
        "epoch": 0.4315522766226992,
        "step": 5791
    },
    {
        "loss": 3.3316,
        "grad_norm": 3.59755539894104,
        "learning_rate": 0.00016103574049354446,
        "epoch": 0.43162679782398095,
        "step": 5792
    },
    {
        "loss": 2.9187,
        "grad_norm": 3.1995718479156494,
        "learning_rate": 0.00016097998689973037,
        "epoch": 0.4317013190252627,
        "step": 5793
    },
    {
        "loss": 2.8439,
        "grad_norm": 5.336294651031494,
        "learning_rate": 0.00016092420311272803,
        "epoch": 0.43177584022654447,
        "step": 5794
    },
    {
        "loss": 2.0457,
        "grad_norm": 3.933333396911621,
        "learning_rate": 0.0001608683891601578,
        "epoch": 0.43185036142782623,
        "step": 5795
    },
    {
        "loss": 1.5297,
        "grad_norm": 2.5305027961730957,
        "learning_rate": 0.00016081254506965505,
        "epoch": 0.431924882629108,
        "step": 5796
    },
    {
        "loss": 1.7508,
        "grad_norm": 2.6649813652038574,
        "learning_rate": 0.00016075667086887005,
        "epoch": 0.43199940383038976,
        "step": 5797
    },
    {
        "loss": 2.5389,
        "grad_norm": 2.8819000720977783,
        "learning_rate": 0.0001607007665854679,
        "epoch": 0.4320739250316715,
        "step": 5798
    },
    {
        "loss": 2.1013,
        "grad_norm": 3.8636093139648438,
        "learning_rate": 0.0001606448322471286,
        "epoch": 0.4321484462329533,
        "step": 5799
    },
    {
        "loss": 2.2224,
        "grad_norm": 3.0010440349578857,
        "learning_rate": 0.0001605888678815471,
        "epoch": 0.43222296743423505,
        "step": 5800
    },
    {
        "loss": 2.6359,
        "grad_norm": 3.3345115184783936,
        "learning_rate": 0.00016053287351643322,
        "epoch": 0.4322974886355168,
        "step": 5801
    },
    {
        "loss": 2.4707,
        "grad_norm": 2.421851873397827,
        "learning_rate": 0.00016047684917951165,
        "epoch": 0.4323720098367986,
        "step": 5802
    },
    {
        "loss": 2.1266,
        "grad_norm": 3.141263246536255,
        "learning_rate": 0.00016042079489852172,
        "epoch": 0.43244653103808034,
        "step": 5803
    },
    {
        "loss": 2.4191,
        "grad_norm": 2.58251953125,
        "learning_rate": 0.00016036471070121793,
        "epoch": 0.4325210522393621,
        "step": 5804
    },
    {
        "loss": 2.029,
        "grad_norm": 3.5033740997314453,
        "learning_rate": 0.00016030859661536923,
        "epoch": 0.43259557344064387,
        "step": 5805
    },
    {
        "loss": 2.5763,
        "grad_norm": 1.9365090131759644,
        "learning_rate": 0.00016025245266875962,
        "epoch": 0.43267009464192563,
        "step": 5806
    },
    {
        "loss": 2.3284,
        "grad_norm": 3.9905331134796143,
        "learning_rate": 0.00016019627888918788,
        "epoch": 0.4327446158432074,
        "step": 5807
    },
    {
        "loss": 2.1155,
        "grad_norm": 3.7523159980773926,
        "learning_rate": 0.00016014007530446738,
        "epoch": 0.43281913704448916,
        "step": 5808
    },
    {
        "loss": 2.5469,
        "grad_norm": 2.1316134929656982,
        "learning_rate": 0.00016008384194242633,
        "epoch": 0.4328936582457709,
        "step": 5809
    },
    {
        "loss": 2.3512,
        "grad_norm": 4.5702619552612305,
        "learning_rate": 0.0001600275788309078,
        "epoch": 0.4329681794470527,
        "step": 5810
    },
    {
        "loss": 2.7272,
        "grad_norm": 3.903651714324951,
        "learning_rate": 0.00015997128599776948,
        "epoch": 0.43304270064833444,
        "step": 5811
    },
    {
        "loss": 2.2162,
        "grad_norm": 3.208543062210083,
        "learning_rate": 0.00015991496347088376,
        "epoch": 0.4331172218496162,
        "step": 5812
    },
    {
        "loss": 1.9664,
        "grad_norm": 2.891045331954956,
        "learning_rate": 0.00015985861127813778,
        "epoch": 0.43319174305089797,
        "step": 5813
    },
    {
        "loss": 2.624,
        "grad_norm": 3.1846060752868652,
        "learning_rate": 0.00015980222944743337,
        "epoch": 0.43326626425217973,
        "step": 5814
    },
    {
        "loss": 1.5597,
        "grad_norm": 3.145035982131958,
        "learning_rate": 0.000159745818006687,
        "epoch": 0.4333407854534615,
        "step": 5815
    },
    {
        "loss": 1.8074,
        "grad_norm": 3.801375389099121,
        "learning_rate": 0.0001596893769838299,
        "epoch": 0.43341530665474326,
        "step": 5816
    },
    {
        "loss": 2.5122,
        "grad_norm": 3.6506218910217285,
        "learning_rate": 0.0001596329064068077,
        "epoch": 0.433489827856025,
        "step": 5817
    },
    {
        "loss": 2.4995,
        "grad_norm": 5.769778728485107,
        "learning_rate": 0.00015957640630358104,
        "epoch": 0.4335643490573068,
        "step": 5818
    },
    {
        "loss": 2.3363,
        "grad_norm": 1.7492833137512207,
        "learning_rate": 0.00015951987670212477,
        "epoch": 0.43363887025858855,
        "step": 5819
    },
    {
        "loss": 2.7809,
        "grad_norm": 2.4212191104888916,
        "learning_rate": 0.00015946331763042865,
        "epoch": 0.4337133914598703,
        "step": 5820
    },
    {
        "loss": 3.3562,
        "grad_norm": 5.761908054351807,
        "learning_rate": 0.00015940672911649698,
        "epoch": 0.4337879126611521,
        "step": 5821
    },
    {
        "loss": 2.5384,
        "grad_norm": 3.369952440261841,
        "learning_rate": 0.00015935011118834846,
        "epoch": 0.43386243386243384,
        "step": 5822
    },
    {
        "loss": 1.6366,
        "grad_norm": 3.0972678661346436,
        "learning_rate": 0.00015929346387401659,
        "epoch": 0.4339369550637156,
        "step": 5823
    },
    {
        "loss": 2.3482,
        "grad_norm": 2.839019298553467,
        "learning_rate": 0.00015923678720154927,
        "epoch": 0.43401147626499736,
        "step": 5824
    },
    {
        "loss": 2.7776,
        "grad_norm": 2.2261569499969482,
        "learning_rate": 0.00015918008119900895,
        "epoch": 0.4340859974662792,
        "step": 5825
    },
    {
        "loss": 2.1644,
        "grad_norm": 3.2306830883026123,
        "learning_rate": 0.00015912334589447268,
        "epoch": 0.43416051866756095,
        "step": 5826
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.1156296730041504,
        "learning_rate": 0.0001590665813160319,
        "epoch": 0.4342350398688427,
        "step": 5827
    },
    {
        "loss": 2.6401,
        "grad_norm": 2.0998282432556152,
        "learning_rate": 0.00015900978749179272,
        "epoch": 0.4343095610701245,
        "step": 5828
    },
    {
        "loss": 2.1875,
        "grad_norm": 2.1469528675079346,
        "learning_rate": 0.00015895296444987545,
        "epoch": 0.43438408227140624,
        "step": 5829
    },
    {
        "loss": 2.5805,
        "grad_norm": 2.2768778800964355,
        "learning_rate": 0.00015889611221841526,
        "epoch": 0.434458603472688,
        "step": 5830
    },
    {
        "loss": 2.1952,
        "grad_norm": 1.9236069917678833,
        "learning_rate": 0.00015883923082556133,
        "epoch": 0.43453312467396976,
        "step": 5831
    },
    {
        "loss": 2.2426,
        "grad_norm": 5.915872573852539,
        "learning_rate": 0.0001587823202994776,
        "epoch": 0.4346076458752515,
        "step": 5832
    },
    {
        "loss": 2.2209,
        "grad_norm": 2.952124834060669,
        "learning_rate": 0.00015872538066834238,
        "epoch": 0.4346821670765333,
        "step": 5833
    },
    {
        "loss": 2.3857,
        "grad_norm": 2.755028009414673,
        "learning_rate": 0.00015866841196034831,
        "epoch": 0.43475668827781505,
        "step": 5834
    },
    {
        "loss": 2.3779,
        "grad_norm": 2.7324864864349365,
        "learning_rate": 0.00015861141420370247,
        "epoch": 0.4348312094790968,
        "step": 5835
    },
    {
        "loss": 2.6672,
        "grad_norm": 3.2266194820404053,
        "learning_rate": 0.00015855438742662628,
        "epoch": 0.4349057306803786,
        "step": 5836
    },
    {
        "loss": 2.5141,
        "grad_norm": 1.7099815607070923,
        "learning_rate": 0.00015849733165735553,
        "epoch": 0.43498025188166034,
        "step": 5837
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.3845908641815186,
        "learning_rate": 0.00015844024692414054,
        "epoch": 0.4350547730829421,
        "step": 5838
    },
    {
        "loss": 2.6884,
        "grad_norm": 4.2549004554748535,
        "learning_rate": 0.0001583831332552457,
        "epoch": 0.43512929428422387,
        "step": 5839
    },
    {
        "loss": 2.6373,
        "grad_norm": 2.3291561603546143,
        "learning_rate": 0.00015832599067894997,
        "epoch": 0.43520381548550563,
        "step": 5840
    },
    {
        "loss": 1.9078,
        "grad_norm": 3.0244030952453613,
        "learning_rate": 0.00015826881922354632,
        "epoch": 0.4352783366867874,
        "step": 5841
    },
    {
        "loss": 1.9266,
        "grad_norm": 2.980337381362915,
        "learning_rate": 0.0001582116189173424,
        "epoch": 0.43535285788806916,
        "step": 5842
    },
    {
        "loss": 1.8945,
        "grad_norm": 4.1975483894348145,
        "learning_rate": 0.00015815438978865985,
        "epoch": 0.4354273790893509,
        "step": 5843
    },
    {
        "loss": 2.4559,
        "grad_norm": 4.202358722686768,
        "learning_rate": 0.00015809713186583482,
        "epoch": 0.4355019002906327,
        "step": 5844
    },
    {
        "loss": 1.125,
        "grad_norm": 2.9944491386413574,
        "learning_rate": 0.00015803984517721747,
        "epoch": 0.43557642149191445,
        "step": 5845
    },
    {
        "loss": 2.8793,
        "grad_norm": 2.5154216289520264,
        "learning_rate": 0.00015798252975117224,
        "epoch": 0.4356509426931962,
        "step": 5846
    },
    {
        "loss": 2.9596,
        "grad_norm": 2.9226157665252686,
        "learning_rate": 0.00015792518561607799,
        "epoch": 0.43572546389447797,
        "step": 5847
    },
    {
        "loss": 2.4132,
        "grad_norm": 2.3101351261138916,
        "learning_rate": 0.00015786781280032773,
        "epoch": 0.43579998509575973,
        "step": 5848
    },
    {
        "loss": 2.54,
        "grad_norm": 3.2240216732025146,
        "learning_rate": 0.00015781041133232853,
        "epoch": 0.4358745062970415,
        "step": 5849
    },
    {
        "loss": 3.1268,
        "grad_norm": 3.0096898078918457,
        "learning_rate": 0.00015775298124050168,
        "epoch": 0.43594902749832326,
        "step": 5850
    },
    {
        "loss": 2.3344,
        "grad_norm": 2.3991363048553467,
        "learning_rate": 0.0001576955225532828,
        "epoch": 0.436023548699605,
        "step": 5851
    },
    {
        "loss": 2.1956,
        "grad_norm": 4.499665260314941,
        "learning_rate": 0.00015763803529912152,
        "epoch": 0.4360980699008868,
        "step": 5852
    },
    {
        "loss": 2.4058,
        "grad_norm": 3.189288854598999,
        "learning_rate": 0.00015758051950648166,
        "epoch": 0.43617259110216855,
        "step": 5853
    },
    {
        "loss": 3.0674,
        "grad_norm": 2.168449640274048,
        "learning_rate": 0.00015752297520384126,
        "epoch": 0.4362471123034503,
        "step": 5854
    },
    {
        "loss": 2.5712,
        "grad_norm": 2.154212236404419,
        "learning_rate": 0.00015746540241969235,
        "epoch": 0.4363216335047321,
        "step": 5855
    },
    {
        "loss": 2.3482,
        "grad_norm": 2.2497506141662598,
        "learning_rate": 0.00015740780118254097,
        "epoch": 0.43639615470601384,
        "step": 5856
    },
    {
        "loss": 2.3213,
        "grad_norm": 3.7971813678741455,
        "learning_rate": 0.0001573501715209075,
        "epoch": 0.4364706759072956,
        "step": 5857
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.6299893856048584,
        "learning_rate": 0.00015729251346332628,
        "epoch": 0.43654519710857737,
        "step": 5858
    },
    {
        "loss": 2.8011,
        "grad_norm": 2.4716193675994873,
        "learning_rate": 0.00015723482703834573,
        "epoch": 0.43661971830985913,
        "step": 5859
    },
    {
        "loss": 2.9194,
        "grad_norm": 2.8067123889923096,
        "learning_rate": 0.00015717711227452812,
        "epoch": 0.43669423951114095,
        "step": 5860
    },
    {
        "loss": 2.1612,
        "grad_norm": 3.140559196472168,
        "learning_rate": 0.00015711936920045008,
        "epoch": 0.4367687607124227,
        "step": 5861
    },
    {
        "loss": 1.5328,
        "grad_norm": 3.324327230453491,
        "learning_rate": 0.000157061597844702,
        "epoch": 0.4368432819137045,
        "step": 5862
    },
    {
        "loss": 1.6974,
        "grad_norm": 3.495798349380493,
        "learning_rate": 0.0001570037982358885,
        "epoch": 0.43691780311498624,
        "step": 5863
    },
    {
        "loss": 2.1198,
        "grad_norm": 2.17055082321167,
        "learning_rate": 0.00015694597040262793,
        "epoch": 0.436992324316268,
        "step": 5864
    },
    {
        "loss": 2.7434,
        "grad_norm": 2.547279119491577,
        "learning_rate": 0.00015688811437355283,
        "epoch": 0.43706684551754976,
        "step": 5865
    },
    {
        "loss": 2.3924,
        "grad_norm": 2.5183231830596924,
        "learning_rate": 0.0001568302301773095,
        "epoch": 0.4371413667188315,
        "step": 5866
    },
    {
        "loss": 2.2973,
        "grad_norm": 2.736970901489258,
        "learning_rate": 0.0001567723178425584,
        "epoch": 0.4372158879201133,
        "step": 5867
    },
    {
        "loss": 2.3834,
        "grad_norm": 1.6984145641326904,
        "learning_rate": 0.00015671437739797384,
        "epoch": 0.43729040912139505,
        "step": 5868
    },
    {
        "loss": 2.6135,
        "grad_norm": 3.7448086738586426,
        "learning_rate": 0.00015665640887224403,
        "epoch": 0.4373649303226768,
        "step": 5869
    },
    {
        "loss": 2.5547,
        "grad_norm": 2.9283030033111572,
        "learning_rate": 0.00015659841229407096,
        "epoch": 0.4374394515239586,
        "step": 5870
    },
    {
        "loss": 2.6978,
        "grad_norm": 2.476107120513916,
        "learning_rate": 0.0001565403876921707,
        "epoch": 0.43751397272524034,
        "step": 5871
    },
    {
        "loss": 1.5946,
        "grad_norm": 3.71124267578125,
        "learning_rate": 0.00015648233509527314,
        "epoch": 0.4375884939265221,
        "step": 5872
    },
    {
        "loss": 3.1933,
        "grad_norm": 1.9762787818908691,
        "learning_rate": 0.00015642425453212214,
        "epoch": 0.43766301512780387,
        "step": 5873
    },
    {
        "loss": 2.8423,
        "grad_norm": 2.1273200511932373,
        "learning_rate": 0.0001563661460314751,
        "epoch": 0.43773753632908563,
        "step": 5874
    },
    {
        "loss": 2.5887,
        "grad_norm": 2.1580121517181396,
        "learning_rate": 0.00015630800962210356,
        "epoch": 0.4378120575303674,
        "step": 5875
    },
    {
        "loss": 2.0125,
        "grad_norm": 3.385509490966797,
        "learning_rate": 0.0001562498453327927,
        "epoch": 0.43788657873164916,
        "step": 5876
    },
    {
        "loss": 2.6863,
        "grad_norm": 2.9699604511260986,
        "learning_rate": 0.0001561916531923416,
        "epoch": 0.4379610999329309,
        "step": 5877
    },
    {
        "loss": 3.0792,
        "grad_norm": 2.596762180328369,
        "learning_rate": 0.00015613343322956307,
        "epoch": 0.4380356211342127,
        "step": 5878
    },
    {
        "loss": 2.1393,
        "grad_norm": 2.340330123901367,
        "learning_rate": 0.00015607518547328377,
        "epoch": 0.43811014233549445,
        "step": 5879
    },
    {
        "loss": 1.97,
        "grad_norm": 5.563319206237793,
        "learning_rate": 0.00015601690995234395,
        "epoch": 0.4381846635367762,
        "step": 5880
    },
    {
        "loss": 2.1291,
        "grad_norm": 4.167867660522461,
        "learning_rate": 0.00015595860669559783,
        "epoch": 0.43825918473805797,
        "step": 5881
    },
    {
        "loss": 2.5544,
        "grad_norm": 2.9397835731506348,
        "learning_rate": 0.00015590027573191332,
        "epoch": 0.43833370593933974,
        "step": 5882
    },
    {
        "loss": 2.2623,
        "grad_norm": 3.22946834564209,
        "learning_rate": 0.0001558419170901718,
        "epoch": 0.4384082271406215,
        "step": 5883
    },
    {
        "loss": 2.7776,
        "grad_norm": 2.1215755939483643,
        "learning_rate": 0.00015578353079926868,
        "epoch": 0.43848274834190326,
        "step": 5884
    },
    {
        "loss": 2.406,
        "grad_norm": 2.8828234672546387,
        "learning_rate": 0.000155725116888113,
        "epoch": 0.438557269543185,
        "step": 5885
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.938033103942871,
        "learning_rate": 0.00015566667538562717,
        "epoch": 0.4386317907444668,
        "step": 5886
    },
    {
        "loss": 2.3814,
        "grad_norm": 2.9459269046783447,
        "learning_rate": 0.00015560820632074774,
        "epoch": 0.43870631194574855,
        "step": 5887
    },
    {
        "loss": 2.4481,
        "grad_norm": 2.230370044708252,
        "learning_rate": 0.00015554970972242444,
        "epoch": 0.4387808331470303,
        "step": 5888
    },
    {
        "loss": 2.3593,
        "grad_norm": 2.304015636444092,
        "learning_rate": 0.00015549118561962103,
        "epoch": 0.4388553543483121,
        "step": 5889
    },
    {
        "loss": 2.7524,
        "grad_norm": 2.947266101837158,
        "learning_rate": 0.0001554326340413146,
        "epoch": 0.43892987554959384,
        "step": 5890
    },
    {
        "loss": 1.9671,
        "grad_norm": 3.038517713546753,
        "learning_rate": 0.00015537405501649598,
        "epoch": 0.4390043967508756,
        "step": 5891
    },
    {
        "loss": 3.0068,
        "grad_norm": 2.4003279209136963,
        "learning_rate": 0.00015531544857416956,
        "epoch": 0.43907891795215737,
        "step": 5892
    },
    {
        "loss": 2.0054,
        "grad_norm": 3.0148377418518066,
        "learning_rate": 0.0001552568147433533,
        "epoch": 0.43915343915343913,
        "step": 5893
    },
    {
        "loss": 2.383,
        "grad_norm": 3.8594577312469482,
        "learning_rate": 0.00015519815355307873,
        "epoch": 0.4392279603547209,
        "step": 5894
    },
    {
        "loss": 1.6909,
        "grad_norm": 2.432844638824463,
        "learning_rate": 0.000155139465032391,
        "epoch": 0.43930248155600266,
        "step": 5895
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.6540732383728027,
        "learning_rate": 0.00015508074921034867,
        "epoch": 0.4393770027572845,
        "step": 5896
    },
    {
        "loss": 2.4016,
        "grad_norm": 2.0137648582458496,
        "learning_rate": 0.00015502200611602383,
        "epoch": 0.43945152395856624,
        "step": 5897
    },
    {
        "loss": 2.7586,
        "grad_norm": 2.6046133041381836,
        "learning_rate": 0.00015496323577850212,
        "epoch": 0.439526045159848,
        "step": 5898
    },
    {
        "loss": 2.1911,
        "grad_norm": 2.3839821815490723,
        "learning_rate": 0.0001549044382268827,
        "epoch": 0.43960056636112976,
        "step": 5899
    },
    {
        "loss": 2.3279,
        "grad_norm": 2.5196993350982666,
        "learning_rate": 0.0001548456134902781,
        "epoch": 0.4396750875624115,
        "step": 5900
    },
    {
        "loss": 2.7605,
        "grad_norm": 5.03350830078125,
        "learning_rate": 0.00015478676159781445,
        "epoch": 0.4397496087636933,
        "step": 5901
    },
    {
        "loss": 1.9616,
        "grad_norm": 3.4372620582580566,
        "learning_rate": 0.00015472788257863123,
        "epoch": 0.43982412996497505,
        "step": 5902
    },
    {
        "loss": 2.256,
        "grad_norm": 3.1738455295562744,
        "learning_rate": 0.00015466897646188125,
        "epoch": 0.4398986511662568,
        "step": 5903
    },
    {
        "loss": 2.6731,
        "grad_norm": 2.4090023040771484,
        "learning_rate": 0.000154610043276731,
        "epoch": 0.4399731723675386,
        "step": 5904
    },
    {
        "loss": 2.804,
        "grad_norm": 2.1506295204162598,
        "learning_rate": 0.00015455108305236027,
        "epoch": 0.44004769356882034,
        "step": 5905
    },
    {
        "loss": 2.7353,
        "grad_norm": 3.33652663230896,
        "learning_rate": 0.0001544920958179621,
        "epoch": 0.4401222147701021,
        "step": 5906
    },
    {
        "loss": 2.1551,
        "grad_norm": 3.249528408050537,
        "learning_rate": 0.00015443308160274292,
        "epoch": 0.44019673597138387,
        "step": 5907
    },
    {
        "loss": 2.5365,
        "grad_norm": 3.628814935684204,
        "learning_rate": 0.0001543740404359227,
        "epoch": 0.44027125717266563,
        "step": 5908
    },
    {
        "loss": 2.5042,
        "grad_norm": 3.1584479808807373,
        "learning_rate": 0.00015431497234673475,
        "epoch": 0.4403457783739474,
        "step": 5909
    },
    {
        "loss": 2.6747,
        "grad_norm": 1.499550700187683,
        "learning_rate": 0.0001542558773644255,
        "epoch": 0.44042029957522916,
        "step": 5910
    },
    {
        "loss": 2.5414,
        "grad_norm": 2.394521713256836,
        "learning_rate": 0.00015419675551825475,
        "epoch": 0.4404948207765109,
        "step": 5911
    },
    {
        "loss": 2.0863,
        "grad_norm": 2.175981044769287,
        "learning_rate": 0.00015413760683749577,
        "epoch": 0.4405693419777927,
        "step": 5912
    },
    {
        "loss": 2.6318,
        "grad_norm": 1.438952922821045,
        "learning_rate": 0.00015407843135143497,
        "epoch": 0.44064386317907445,
        "step": 5913
    },
    {
        "loss": 2.2492,
        "grad_norm": 5.293542385101318,
        "learning_rate": 0.00015401922908937204,
        "epoch": 0.4407183843803562,
        "step": 5914
    },
    {
        "loss": 2.4397,
        "grad_norm": 3.2626700401306152,
        "learning_rate": 0.00015396000008062013,
        "epoch": 0.440792905581638,
        "step": 5915
    },
    {
        "loss": 1.8855,
        "grad_norm": 3.9201385974884033,
        "learning_rate": 0.00015390074435450525,
        "epoch": 0.44086742678291974,
        "step": 5916
    },
    {
        "loss": 2.7873,
        "grad_norm": 2.0595483779907227,
        "learning_rate": 0.00015384146194036692,
        "epoch": 0.4409419479842015,
        "step": 5917
    },
    {
        "loss": 1.4433,
        "grad_norm": 3.8644371032714844,
        "learning_rate": 0.0001537821528675578,
        "epoch": 0.44101646918548326,
        "step": 5918
    },
    {
        "loss": 2.8661,
        "grad_norm": 2.600034236907959,
        "learning_rate": 0.0001537228171654438,
        "epoch": 0.441090990386765,
        "step": 5919
    },
    {
        "loss": 2.9046,
        "grad_norm": 3.8919765949249268,
        "learning_rate": 0.00015366345486340398,
        "epoch": 0.4411655115880468,
        "step": 5920
    },
    {
        "loss": 2.997,
        "grad_norm": 3.488389492034912,
        "learning_rate": 0.00015360406599083042,
        "epoch": 0.44124003278932855,
        "step": 5921
    },
    {
        "loss": 1.784,
        "grad_norm": 3.981931447982788,
        "learning_rate": 0.0001535446505771286,
        "epoch": 0.4413145539906103,
        "step": 5922
    },
    {
        "loss": 1.539,
        "grad_norm": 3.7605273723602295,
        "learning_rate": 0.00015348520865171705,
        "epoch": 0.4413890751918921,
        "step": 5923
    },
    {
        "loss": 2.024,
        "grad_norm": 3.4910824298858643,
        "learning_rate": 0.00015342574024402743,
        "epoch": 0.44146359639317384,
        "step": 5924
    },
    {
        "loss": 2.9151,
        "grad_norm": 2.7077372074127197,
        "learning_rate": 0.0001533662453835044,
        "epoch": 0.4415381175944556,
        "step": 5925
    },
    {
        "loss": 2.1006,
        "grad_norm": 2.9673798084259033,
        "learning_rate": 0.00015330672409960593,
        "epoch": 0.44161263879573737,
        "step": 5926
    },
    {
        "loss": 2.5366,
        "grad_norm": 3.4748146533966064,
        "learning_rate": 0.00015324717642180283,
        "epoch": 0.44168715999701913,
        "step": 5927
    },
    {
        "loss": 2.4421,
        "grad_norm": 3.812624931335449,
        "learning_rate": 0.00015318760237957916,
        "epoch": 0.4417616811983009,
        "step": 5928
    },
    {
        "loss": 2.7314,
        "grad_norm": 3.8476624488830566,
        "learning_rate": 0.00015312800200243206,
        "epoch": 0.44183620239958266,
        "step": 5929
    },
    {
        "loss": 2.4326,
        "grad_norm": 5.356165409088135,
        "learning_rate": 0.0001530683753198716,
        "epoch": 0.4419107236008644,
        "step": 5930
    },
    {
        "loss": 2.3409,
        "grad_norm": 2.8079843521118164,
        "learning_rate": 0.00015300872236142076,
        "epoch": 0.44198524480214624,
        "step": 5931
    },
    {
        "loss": 1.9542,
        "grad_norm": 2.3309857845306396,
        "learning_rate": 0.0001529490431566158,
        "epoch": 0.442059766003428,
        "step": 5932
    },
    {
        "loss": 2.6178,
        "grad_norm": 2.44891357421875,
        "learning_rate": 0.0001528893377350058,
        "epoch": 0.44213428720470976,
        "step": 5933
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.532810688018799,
        "learning_rate": 0.00015282960612615298,
        "epoch": 0.4422088084059915,
        "step": 5934
    },
    {
        "loss": 2.4479,
        "grad_norm": 2.950162172317505,
        "learning_rate": 0.00015276984835963225,
        "epoch": 0.4422833296072733,
        "step": 5935
    },
    {
        "loss": 2.4994,
        "grad_norm": 3.793393611907959,
        "learning_rate": 0.00015271006446503175,
        "epoch": 0.44235785080855505,
        "step": 5936
    },
    {
        "loss": 2.182,
        "grad_norm": 2.520749568939209,
        "learning_rate": 0.00015265025447195238,
        "epoch": 0.4424323720098368,
        "step": 5937
    },
    {
        "loss": 2.131,
        "grad_norm": 2.7069694995880127,
        "learning_rate": 0.0001525904184100081,
        "epoch": 0.4425068932111186,
        "step": 5938
    },
    {
        "loss": 1.6256,
        "grad_norm": 3.978987455368042,
        "learning_rate": 0.00015253055630882564,
        "epoch": 0.44258141441240034,
        "step": 5939
    },
    {
        "loss": 1.7496,
        "grad_norm": 4.689752101898193,
        "learning_rate": 0.00015247066819804473,
        "epoch": 0.4426559356136821,
        "step": 5940
    },
    {
        "loss": 2.3944,
        "grad_norm": 4.572659015655518,
        "learning_rate": 0.00015241075410731786,
        "epoch": 0.44273045681496387,
        "step": 5941
    },
    {
        "loss": 2.4628,
        "grad_norm": 2.377422332763672,
        "learning_rate": 0.0001523508140663105,
        "epoch": 0.44280497801624563,
        "step": 5942
    },
    {
        "loss": 2.0324,
        "grad_norm": 3.298222064971924,
        "learning_rate": 0.000152290848104701,
        "epoch": 0.4428794992175274,
        "step": 5943
    },
    {
        "loss": 0.8657,
        "grad_norm": 5.580677032470703,
        "learning_rate": 0.00015223085625218032,
        "epoch": 0.44295402041880916,
        "step": 5944
    },
    {
        "loss": 2.241,
        "grad_norm": 2.817631483078003,
        "learning_rate": 0.00015217083853845252,
        "epoch": 0.4430285416200909,
        "step": 5945
    },
    {
        "loss": 2.2487,
        "grad_norm": 3.377821207046509,
        "learning_rate": 0.0001521107949932343,
        "epoch": 0.4431030628213727,
        "step": 5946
    },
    {
        "loss": 2.9457,
        "grad_norm": 3.6865596771240234,
        "learning_rate": 0.00015205072564625514,
        "epoch": 0.44317758402265445,
        "step": 5947
    },
    {
        "loss": 2.8465,
        "grad_norm": 2.5718324184417725,
        "learning_rate": 0.00015199063052725745,
        "epoch": 0.4432521052239362,
        "step": 5948
    },
    {
        "loss": 2.6034,
        "grad_norm": 2.558579206466675,
        "learning_rate": 0.00015193050966599616,
        "epoch": 0.443326626425218,
        "step": 5949
    },
    {
        "loss": 2.9618,
        "grad_norm": 4.302534580230713,
        "learning_rate": 0.00015187036309223919,
        "epoch": 0.44340114762649974,
        "step": 5950
    },
    {
        "loss": 2.6729,
        "grad_norm": 3.608952760696411,
        "learning_rate": 0.00015181019083576695,
        "epoch": 0.4434756688277815,
        "step": 5951
    },
    {
        "loss": 2.2655,
        "grad_norm": 2.5381221771240234,
        "learning_rate": 0.00015174999292637286,
        "epoch": 0.44355019002906326,
        "step": 5952
    },
    {
        "loss": 2.6596,
        "grad_norm": 2.923330545425415,
        "learning_rate": 0.00015168976939386268,
        "epoch": 0.443624711230345,
        "step": 5953
    },
    {
        "loss": 2.6503,
        "grad_norm": 3.2096056938171387,
        "learning_rate": 0.0001516295202680552,
        "epoch": 0.4436992324316268,
        "step": 5954
    },
    {
        "loss": 2.6971,
        "grad_norm": 2.118391990661621,
        "learning_rate": 0.00015156924557878165,
        "epoch": 0.44377375363290855,
        "step": 5955
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.7478299140930176,
        "learning_rate": 0.00015150894535588614,
        "epoch": 0.4438482748341903,
        "step": 5956
    },
    {
        "loss": 2.3969,
        "grad_norm": 2.5614144802093506,
        "learning_rate": 0.00015144861962922517,
        "epoch": 0.4439227960354721,
        "step": 5957
    },
    {
        "loss": 2.1776,
        "grad_norm": 2.553372383117676,
        "learning_rate": 0.00015138826842866796,
        "epoch": 0.44399731723675384,
        "step": 5958
    },
    {
        "loss": 2.2227,
        "grad_norm": 3.24975323677063,
        "learning_rate": 0.0001513278917840964,
        "epoch": 0.4440718384380356,
        "step": 5959
    },
    {
        "loss": 2.903,
        "grad_norm": 2.722386598587036,
        "learning_rate": 0.000151267489725405,
        "epoch": 0.44414635963931737,
        "step": 5960
    },
    {
        "loss": 2.6458,
        "grad_norm": 2.6191375255584717,
        "learning_rate": 0.00015120706228250068,
        "epoch": 0.44422088084059913,
        "step": 5961
    },
    {
        "loss": 2.6139,
        "grad_norm": 2.0853776931762695,
        "learning_rate": 0.00015114660948530316,
        "epoch": 0.4442954020418809,
        "step": 5962
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.405118942260742,
        "learning_rate": 0.00015108613136374452,
        "epoch": 0.44436992324316266,
        "step": 5963
    },
    {
        "loss": 1.8525,
        "grad_norm": 4.132530212402344,
        "learning_rate": 0.00015102562794776952,
        "epoch": 0.4444444444444444,
        "step": 5964
    },
    {
        "loss": 2.3478,
        "grad_norm": 3.9218997955322266,
        "learning_rate": 0.00015096509926733532,
        "epoch": 0.4445189656457262,
        "step": 5965
    },
    {
        "loss": 2.0197,
        "grad_norm": 2.546708822250366,
        "learning_rate": 0.00015090454535241174,
        "epoch": 0.444593486847008,
        "step": 5966
    },
    {
        "loss": 2.7672,
        "grad_norm": 1.9500027894973755,
        "learning_rate": 0.00015084396623298095,
        "epoch": 0.44466800804828976,
        "step": 5967
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.1625633239746094,
        "learning_rate": 0.00015078336193903762,
        "epoch": 0.4447425292495715,
        "step": 5968
    },
    {
        "loss": 2.1789,
        "grad_norm": 3.6835203170776367,
        "learning_rate": 0.00015072273250058894,
        "epoch": 0.4448170504508533,
        "step": 5969
    },
    {
        "loss": 2.5729,
        "grad_norm": 1.8533071279525757,
        "learning_rate": 0.0001506620779476546,
        "epoch": 0.44489157165213505,
        "step": 5970
    },
    {
        "loss": 2.6759,
        "grad_norm": 2.0925920009613037,
        "learning_rate": 0.00015060139831026662,
        "epoch": 0.4449660928534168,
        "step": 5971
    },
    {
        "loss": 2.5735,
        "grad_norm": 2.4084699153900146,
        "learning_rate": 0.0001505406936184694,
        "epoch": 0.4450406140546986,
        "step": 5972
    },
    {
        "loss": 2.5867,
        "grad_norm": 4.39946985244751,
        "learning_rate": 0.00015047996390231985,
        "epoch": 0.44511513525598034,
        "step": 5973
    },
    {
        "loss": 2.2968,
        "grad_norm": 3.3803930282592773,
        "learning_rate": 0.00015041920919188728,
        "epoch": 0.4451896564572621,
        "step": 5974
    },
    {
        "loss": 2.6407,
        "grad_norm": 3.2512476444244385,
        "learning_rate": 0.00015035842951725332,
        "epoch": 0.44526417765854387,
        "step": 5975
    },
    {
        "loss": 1.6973,
        "grad_norm": 3.232144832611084,
        "learning_rate": 0.0001502976249085121,
        "epoch": 0.44533869885982563,
        "step": 5976
    },
    {
        "loss": 2.9851,
        "grad_norm": 3.555412769317627,
        "learning_rate": 0.00015023679539576977,
        "epoch": 0.4454132200611074,
        "step": 5977
    },
    {
        "loss": 2.1879,
        "grad_norm": 3.478360414505005,
        "learning_rate": 0.0001501759410091451,
        "epoch": 0.44548774126238916,
        "step": 5978
    },
    {
        "loss": 2.4069,
        "grad_norm": 2.7364699840545654,
        "learning_rate": 0.0001501150617787691,
        "epoch": 0.4455622624636709,
        "step": 5979
    },
    {
        "loss": 1.8371,
        "grad_norm": 3.578326463699341,
        "learning_rate": 0.00015005415773478506,
        "epoch": 0.4456367836649527,
        "step": 5980
    },
    {
        "loss": 2.8235,
        "grad_norm": 2.9192135334014893,
        "learning_rate": 0.0001499932289073486,
        "epoch": 0.44571130486623445,
        "step": 5981
    },
    {
        "loss": 2.6508,
        "grad_norm": 3.4775753021240234,
        "learning_rate": 0.00014993227532662746,
        "epoch": 0.4457858260675162,
        "step": 5982
    },
    {
        "loss": 2.7273,
        "grad_norm": 4.202049732208252,
        "learning_rate": 0.00014987129702280188,
        "epoch": 0.445860347268798,
        "step": 5983
    },
    {
        "loss": 2.0137,
        "grad_norm": 2.3496298789978027,
        "learning_rate": 0.00014981029402606413,
        "epoch": 0.44593486847007974,
        "step": 5984
    },
    {
        "loss": 2.8596,
        "grad_norm": 2.052818536758423,
        "learning_rate": 0.0001497492663666189,
        "epoch": 0.4460093896713615,
        "step": 5985
    },
    {
        "loss": 1.6898,
        "grad_norm": 5.732460021972656,
        "learning_rate": 0.00014968821407468285,
        "epoch": 0.44608391087264326,
        "step": 5986
    },
    {
        "loss": 1.4665,
        "grad_norm": 1.6901603937149048,
        "learning_rate": 0.00014962713718048512,
        "epoch": 0.446158432073925,
        "step": 5987
    },
    {
        "loss": 2.251,
        "grad_norm": 6.4955949783325195,
        "learning_rate": 0.00014956603571426673,
        "epoch": 0.4462329532752068,
        "step": 5988
    },
    {
        "loss": 3.0946,
        "grad_norm": 2.7775604724884033,
        "learning_rate": 0.00014950490970628107,
        "epoch": 0.44630747447648855,
        "step": 5989
    },
    {
        "loss": 2.4894,
        "grad_norm": 3.0187127590179443,
        "learning_rate": 0.0001494437591867937,
        "epoch": 0.4463819956777703,
        "step": 5990
    },
    {
        "loss": 2.4486,
        "grad_norm": 1.9017999172210693,
        "learning_rate": 0.00014938258418608217,
        "epoch": 0.4464565168790521,
        "step": 5991
    },
    {
        "loss": 2.7222,
        "grad_norm": 2.2491180896759033,
        "learning_rate": 0.0001493213847344362,
        "epoch": 0.44653103808033384,
        "step": 5992
    },
    {
        "loss": 2.2711,
        "grad_norm": 3.252243757247925,
        "learning_rate": 0.00014926016086215765,
        "epoch": 0.4466055592816156,
        "step": 5993
    },
    {
        "loss": 1.8944,
        "grad_norm": 3.8310272693634033,
        "learning_rate": 0.00014919891259956053,
        "epoch": 0.44668008048289737,
        "step": 5994
    },
    {
        "loss": 2.6726,
        "grad_norm": 1.6729272603988647,
        "learning_rate": 0.0001491376399769709,
        "epoch": 0.44675460168417913,
        "step": 5995
    },
    {
        "loss": 2.5301,
        "grad_norm": 2.504850149154663,
        "learning_rate": 0.00014907634302472667,
        "epoch": 0.4468291228854609,
        "step": 5996
    },
    {
        "loss": 3.0469,
        "grad_norm": 2.3896126747131348,
        "learning_rate": 0.00014901502177317813,
        "epoch": 0.44690364408674266,
        "step": 5997
    },
    {
        "loss": 2.8459,
        "grad_norm": 2.939328193664551,
        "learning_rate": 0.00014895367625268735,
        "epoch": 0.4469781652880244,
        "step": 5998
    },
    {
        "loss": 2.2643,
        "grad_norm": 3.750584125518799,
        "learning_rate": 0.00014889230649362854,
        "epoch": 0.4470526864893062,
        "step": 5999
    },
    {
        "loss": 1.1473,
        "grad_norm": 2.294620990753174,
        "learning_rate": 0.00014883091252638784,
        "epoch": 0.44712720769058795,
        "step": 6000
    },
    {
        "loss": 1.464,
        "grad_norm": 3.144960403442383,
        "learning_rate": 0.00014876949438136352,
        "epoch": 0.4472017288918697,
        "step": 6001
    },
    {
        "loss": 2.3852,
        "grad_norm": 2.8141613006591797,
        "learning_rate": 0.00014870805208896552,
        "epoch": 0.44727625009315153,
        "step": 6002
    },
    {
        "loss": 1.9064,
        "grad_norm": 3.237931489944458,
        "learning_rate": 0.00014864658567961608,
        "epoch": 0.4473507712944333,
        "step": 6003
    },
    {
        "loss": 2.6623,
        "grad_norm": 2.7980382442474365,
        "learning_rate": 0.00014858509518374924,
        "epoch": 0.44742529249571505,
        "step": 6004
    },
    {
        "loss": 2.4252,
        "grad_norm": 4.225067615509033,
        "learning_rate": 0.00014852358063181087,
        "epoch": 0.4474998136969968,
        "step": 6005
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.7456769943237305,
        "learning_rate": 0.00014846204205425892,
        "epoch": 0.4475743348982786,
        "step": 6006
    },
    {
        "loss": 2.7052,
        "grad_norm": 2.7563188076019287,
        "learning_rate": 0.00014840047948156315,
        "epoch": 0.44764885609956034,
        "step": 6007
    },
    {
        "loss": 1.8796,
        "grad_norm": 3.1168742179870605,
        "learning_rate": 0.0001483388929442051,
        "epoch": 0.4477233773008421,
        "step": 6008
    },
    {
        "loss": 2.5627,
        "grad_norm": 3.0072214603424072,
        "learning_rate": 0.00014827728247267846,
        "epoch": 0.44779789850212387,
        "step": 6009
    },
    {
        "loss": 2.6663,
        "grad_norm": 3.888808012008667,
        "learning_rate": 0.0001482156480974884,
        "epoch": 0.44787241970340563,
        "step": 6010
    },
    {
        "loss": 1.8082,
        "grad_norm": 5.049743175506592,
        "learning_rate": 0.00014815398984915229,
        "epoch": 0.4479469409046874,
        "step": 6011
    },
    {
        "loss": 3.3476,
        "grad_norm": 3.479139804840088,
        "learning_rate": 0.00014809230775819895,
        "epoch": 0.44802146210596916,
        "step": 6012
    },
    {
        "loss": 2.2491,
        "grad_norm": 2.76522159576416,
        "learning_rate": 0.00014803060185516936,
        "epoch": 0.4480959833072509,
        "step": 6013
    },
    {
        "loss": 2.8075,
        "grad_norm": 2.956411600112915,
        "learning_rate": 0.00014796887217061614,
        "epoch": 0.4481705045085327,
        "step": 6014
    },
    {
        "loss": 1.8558,
        "grad_norm": 4.316417694091797,
        "learning_rate": 0.00014790711873510356,
        "epoch": 0.44824502570981445,
        "step": 6015
    },
    {
        "loss": 2.8825,
        "grad_norm": 2.3351547718048096,
        "learning_rate": 0.00014784534157920786,
        "epoch": 0.4483195469110962,
        "step": 6016
    },
    {
        "loss": 1.9635,
        "grad_norm": 3.155029296875,
        "learning_rate": 0.00014778354073351695,
        "epoch": 0.448394068112378,
        "step": 6017
    },
    {
        "loss": 1.9252,
        "grad_norm": 3.758805751800537,
        "learning_rate": 0.00014772171622863043,
        "epoch": 0.44846858931365974,
        "step": 6018
    },
    {
        "loss": 1.8943,
        "grad_norm": 2.220188617706299,
        "learning_rate": 0.00014765986809515952,
        "epoch": 0.4485431105149415,
        "step": 6019
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.549708843231201,
        "learning_rate": 0.00014759799636372738,
        "epoch": 0.44861763171622326,
        "step": 6020
    },
    {
        "loss": 1.6727,
        "grad_norm": 3.5730907917022705,
        "learning_rate": 0.0001475361010649688,
        "epoch": 0.448692152917505,
        "step": 6021
    },
    {
        "loss": 2.3867,
        "grad_norm": 3.1544768810272217,
        "learning_rate": 0.00014747418222952995,
        "epoch": 0.4487666741187868,
        "step": 6022
    },
    {
        "loss": 2.3383,
        "grad_norm": 2.851909875869751,
        "learning_rate": 0.00014741223988806908,
        "epoch": 0.44884119532006855,
        "step": 6023
    },
    {
        "loss": 2.1152,
        "grad_norm": 4.074575901031494,
        "learning_rate": 0.00014735027407125574,
        "epoch": 0.4489157165213503,
        "step": 6024
    },
    {
        "loss": 2.4524,
        "grad_norm": 2.8852546215057373,
        "learning_rate": 0.00014728828480977128,
        "epoch": 0.4489902377226321,
        "step": 6025
    },
    {
        "loss": 2.7454,
        "grad_norm": 2.7915468215942383,
        "learning_rate": 0.0001472262721343086,
        "epoch": 0.44906475892391384,
        "step": 6026
    },
    {
        "loss": 2.328,
        "grad_norm": 2.188141345977783,
        "learning_rate": 0.0001471642360755723,
        "epoch": 0.4491392801251956,
        "step": 6027
    },
    {
        "loss": 2.6268,
        "grad_norm": 2.891846179962158,
        "learning_rate": 0.00014710217666427837,
        "epoch": 0.44921380132647737,
        "step": 6028
    },
    {
        "loss": 2.2114,
        "grad_norm": 3.8013668060302734,
        "learning_rate": 0.00014704009393115441,
        "epoch": 0.44928832252775913,
        "step": 6029
    },
    {
        "loss": 2.8384,
        "grad_norm": 2.6837453842163086,
        "learning_rate": 0.0001469779879069397,
        "epoch": 0.4493628437290409,
        "step": 6030
    },
    {
        "loss": 1.5876,
        "grad_norm": 3.40578293800354,
        "learning_rate": 0.00014691585862238496,
        "epoch": 0.44943736493032266,
        "step": 6031
    },
    {
        "loss": 2.2242,
        "grad_norm": 3.747750997543335,
        "learning_rate": 0.00014685370610825245,
        "epoch": 0.4495118861316044,
        "step": 6032
    },
    {
        "loss": 1.9353,
        "grad_norm": 3.141845226287842,
        "learning_rate": 0.00014679153039531581,
        "epoch": 0.4495864073328862,
        "step": 6033
    },
    {
        "loss": 2.1983,
        "grad_norm": 3.060555934906006,
        "learning_rate": 0.00014672933151436035,
        "epoch": 0.44966092853416795,
        "step": 6034
    },
    {
        "loss": 2.163,
        "grad_norm": 2.8141250610351562,
        "learning_rate": 0.00014666710949618275,
        "epoch": 0.4497354497354497,
        "step": 6035
    },
    {
        "loss": 2.6804,
        "grad_norm": 2.966679573059082,
        "learning_rate": 0.0001466048643715912,
        "epoch": 0.4498099709367315,
        "step": 6036
    },
    {
        "loss": 2.4508,
        "grad_norm": 2.8287596702575684,
        "learning_rate": 0.00014654259617140531,
        "epoch": 0.4498844921380133,
        "step": 6037
    },
    {
        "loss": 2.2623,
        "grad_norm": 3.009219169616699,
        "learning_rate": 0.0001464803049264561,
        "epoch": 0.44995901333929506,
        "step": 6038
    },
    {
        "loss": 2.6135,
        "grad_norm": 2.5964698791503906,
        "learning_rate": 0.00014641799066758592,
        "epoch": 0.4500335345405768,
        "step": 6039
    },
    {
        "loss": 2.795,
        "grad_norm": 3.9537220001220703,
        "learning_rate": 0.00014635565342564866,
        "epoch": 0.4501080557418586,
        "step": 6040
    },
    {
        "loss": 2.5298,
        "grad_norm": 3.0487523078918457,
        "learning_rate": 0.00014629329323150963,
        "epoch": 0.45018257694314034,
        "step": 6041
    },
    {
        "loss": 1.8591,
        "grad_norm": 3.349199056625366,
        "learning_rate": 0.00014623091011604523,
        "epoch": 0.4502570981444221,
        "step": 6042
    },
    {
        "loss": 2.4928,
        "grad_norm": 2.310678005218506,
        "learning_rate": 0.00014616850411014348,
        "epoch": 0.45033161934570387,
        "step": 6043
    },
    {
        "loss": 2.6946,
        "grad_norm": 3.628951072692871,
        "learning_rate": 0.00014610607524470358,
        "epoch": 0.45040614054698563,
        "step": 6044
    },
    {
        "loss": 2.4513,
        "grad_norm": 2.8077940940856934,
        "learning_rate": 0.00014604362355063612,
        "epoch": 0.4504806617482674,
        "step": 6045
    },
    {
        "loss": 1.8244,
        "grad_norm": 2.7135000228881836,
        "learning_rate": 0.0001459811490588631,
        "epoch": 0.45055518294954916,
        "step": 6046
    },
    {
        "loss": 2.9545,
        "grad_norm": 2.9447338581085205,
        "learning_rate": 0.0001459186518003175,
        "epoch": 0.4506297041508309,
        "step": 6047
    },
    {
        "loss": 2.6077,
        "grad_norm": 2.9407122135162354,
        "learning_rate": 0.00014585613180594388,
        "epoch": 0.4507042253521127,
        "step": 6048
    },
    {
        "loss": 3.2975,
        "grad_norm": 5.163957118988037,
        "learning_rate": 0.0001457935891066979,
        "epoch": 0.45077874655339445,
        "step": 6049
    },
    {
        "loss": 2.7156,
        "grad_norm": 2.444167375564575,
        "learning_rate": 0.00014573102373354645,
        "epoch": 0.4508532677546762,
        "step": 6050
    },
    {
        "loss": 2.9446,
        "grad_norm": 3.1171305179595947,
        "learning_rate": 0.00014566843571746783,
        "epoch": 0.450927788955958,
        "step": 6051
    },
    {
        "loss": 2.4071,
        "grad_norm": 3.862987518310547,
        "learning_rate": 0.00014560582508945134,
        "epoch": 0.45100231015723974,
        "step": 6052
    },
    {
        "loss": 1.998,
        "grad_norm": 3.035797595977783,
        "learning_rate": 0.00014554319188049745,
        "epoch": 0.4510768313585215,
        "step": 6053
    },
    {
        "loss": 1.9304,
        "grad_norm": 2.948422908782959,
        "learning_rate": 0.00014548053612161802,
        "epoch": 0.45115135255980326,
        "step": 6054
    },
    {
        "loss": 2.931,
        "grad_norm": 3.0717926025390625,
        "learning_rate": 0.00014541785784383599,
        "epoch": 0.45122587376108503,
        "step": 6055
    },
    {
        "loss": 2.5371,
        "grad_norm": 3.456918478012085,
        "learning_rate": 0.0001453551570781854,
        "epoch": 0.4513003949623668,
        "step": 6056
    },
    {
        "loss": 2.3071,
        "grad_norm": 3.5798280239105225,
        "learning_rate": 0.0001452924338557114,
        "epoch": 0.45137491616364855,
        "step": 6057
    },
    {
        "loss": 2.8312,
        "grad_norm": 3.076880693435669,
        "learning_rate": 0.00014522968820747045,
        "epoch": 0.4514494373649303,
        "step": 6058
    },
    {
        "loss": 2.3254,
        "grad_norm": 3.153785228729248,
        "learning_rate": 0.00014516692016452976,
        "epoch": 0.4515239585662121,
        "step": 6059
    },
    {
        "loss": 2.2297,
        "grad_norm": 2.718332529067993,
        "learning_rate": 0.00014510412975796805,
        "epoch": 0.45159847976749384,
        "step": 6060
    },
    {
        "loss": 1.8914,
        "grad_norm": 2.7940673828125,
        "learning_rate": 0.00014504131701887477,
        "epoch": 0.4516730009687756,
        "step": 6061
    },
    {
        "loss": 2.2383,
        "grad_norm": 4.10159969329834,
        "learning_rate": 0.00014497848197835066,
        "epoch": 0.45174752217005737,
        "step": 6062
    },
    {
        "loss": 2.2997,
        "grad_norm": 3.5119075775146484,
        "learning_rate": 0.00014491562466750733,
        "epoch": 0.45182204337133913,
        "step": 6063
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.581956624984741,
        "learning_rate": 0.00014485274511746746,
        "epoch": 0.4518965645726209,
        "step": 6064
    },
    {
        "loss": 1.4277,
        "grad_norm": 1.989843726158142,
        "learning_rate": 0.00014478984335936494,
        "epoch": 0.45197108577390266,
        "step": 6065
    },
    {
        "loss": 2.192,
        "grad_norm": 2.4161155223846436,
        "learning_rate": 0.00014472691942434433,
        "epoch": 0.4520456069751844,
        "step": 6066
    },
    {
        "loss": 3.0076,
        "grad_norm": 3.367999792098999,
        "learning_rate": 0.00014466397334356136,
        "epoch": 0.4521201281764662,
        "step": 6067
    },
    {
        "loss": 2.9331,
        "grad_norm": 2.3877995014190674,
        "learning_rate": 0.0001446010051481828,
        "epoch": 0.45219464937774795,
        "step": 6068
    },
    {
        "loss": 1.8645,
        "grad_norm": 4.10027551651001,
        "learning_rate": 0.00014453801486938612,
        "epoch": 0.4522691705790297,
        "step": 6069
    },
    {
        "loss": 2.9234,
        "grad_norm": 3.404200792312622,
        "learning_rate": 0.00014447500253836,
        "epoch": 0.4523436917803115,
        "step": 6070
    },
    {
        "loss": 2.6183,
        "grad_norm": 2.971820592880249,
        "learning_rate": 0.00014441196818630378,
        "epoch": 0.45241821298159324,
        "step": 6071
    },
    {
        "loss": 2.5608,
        "grad_norm": 2.7751946449279785,
        "learning_rate": 0.00014434891184442796,
        "epoch": 0.452492734182875,
        "step": 6072
    },
    {
        "loss": 2.4986,
        "grad_norm": 3.4629032611846924,
        "learning_rate": 0.0001442858335439537,
        "epoch": 0.4525672553841568,
        "step": 6073
    },
    {
        "loss": 2.4711,
        "grad_norm": 2.149089813232422,
        "learning_rate": 0.00014422273331611313,
        "epoch": 0.4526417765854386,
        "step": 6074
    },
    {
        "loss": 2.5639,
        "grad_norm": 3.4394702911376953,
        "learning_rate": 0.00014415961119214937,
        "epoch": 0.45271629778672035,
        "step": 6075
    },
    {
        "loss": 2.3122,
        "grad_norm": 3.6560208797454834,
        "learning_rate": 0.00014409646720331607,
        "epoch": 0.4527908189880021,
        "step": 6076
    },
    {
        "loss": 2.8629,
        "grad_norm": 1.8946493864059448,
        "learning_rate": 0.00014403330138087797,
        "epoch": 0.45286534018928387,
        "step": 6077
    },
    {
        "loss": 2.6132,
        "grad_norm": 2.4429538249969482,
        "learning_rate": 0.00014397011375611064,
        "epoch": 0.45293986139056563,
        "step": 6078
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.949589252471924,
        "learning_rate": 0.00014390690436030023,
        "epoch": 0.4530143825918474,
        "step": 6079
    },
    {
        "loss": 2.4008,
        "grad_norm": 3.9423584938049316,
        "learning_rate": 0.00014384367322474372,
        "epoch": 0.45308890379312916,
        "step": 6080
    },
    {
        "loss": 2.0986,
        "grad_norm": 3.193366289138794,
        "learning_rate": 0.00014378042038074902,
        "epoch": 0.4531634249944109,
        "step": 6081
    },
    {
        "loss": 2.2771,
        "grad_norm": 4.531980037689209,
        "learning_rate": 0.0001437171458596347,
        "epoch": 0.4532379461956927,
        "step": 6082
    },
    {
        "loss": 1.7102,
        "grad_norm": 2.6146013736724854,
        "learning_rate": 0.00014365384969272993,
        "epoch": 0.45331246739697445,
        "step": 6083
    },
    {
        "loss": 2.17,
        "grad_norm": 3.18835186958313,
        "learning_rate": 0.00014359053191137484,
        "epoch": 0.4533869885982562,
        "step": 6084
    },
    {
        "loss": 2.7359,
        "grad_norm": 3.7589001655578613,
        "learning_rate": 0.00014352719254692004,
        "epoch": 0.453461509799538,
        "step": 6085
    },
    {
        "loss": 1.5869,
        "grad_norm": 3.0116705894470215,
        "learning_rate": 0.00014346383163072696,
        "epoch": 0.45353603100081974,
        "step": 6086
    },
    {
        "loss": 2.5176,
        "grad_norm": 2.1565017700195312,
        "learning_rate": 0.0001434004491941677,
        "epoch": 0.4536105522021015,
        "step": 6087
    },
    {
        "loss": 2.7429,
        "grad_norm": 2.30261492729187,
        "learning_rate": 0.00014333704526862498,
        "epoch": 0.45368507340338327,
        "step": 6088
    },
    {
        "loss": 2.4486,
        "grad_norm": 3.2516651153564453,
        "learning_rate": 0.00014327361988549213,
        "epoch": 0.45375959460466503,
        "step": 6089
    },
    {
        "loss": 1.9868,
        "grad_norm": 2.7182717323303223,
        "learning_rate": 0.00014321017307617308,
        "epoch": 0.4538341158059468,
        "step": 6090
    },
    {
        "loss": 2.5751,
        "grad_norm": 4.192476272583008,
        "learning_rate": 0.00014314670487208247,
        "epoch": 0.45390863700722855,
        "step": 6091
    },
    {
        "loss": 2.5989,
        "grad_norm": 3.14282488822937,
        "learning_rate": 0.0001430832153046456,
        "epoch": 0.4539831582085103,
        "step": 6092
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.4775986671447754,
        "learning_rate": 0.00014301970440529807,
        "epoch": 0.4540576794097921,
        "step": 6093
    },
    {
        "loss": 3.7143,
        "grad_norm": 5.225954055786133,
        "learning_rate": 0.00014295617220548622,
        "epoch": 0.45413220061107384,
        "step": 6094
    },
    {
        "loss": 2.2567,
        "grad_norm": 2.252321243286133,
        "learning_rate": 0.00014289261873666698,
        "epoch": 0.4542067218123556,
        "step": 6095
    },
    {
        "loss": 1.9071,
        "grad_norm": 2.705228805541992,
        "learning_rate": 0.00014282904403030772,
        "epoch": 0.45428124301363737,
        "step": 6096
    },
    {
        "loss": 2.0758,
        "grad_norm": 3.937620162963867,
        "learning_rate": 0.00014276544811788638,
        "epoch": 0.45435576421491913,
        "step": 6097
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.733633518218994,
        "learning_rate": 0.00014270183103089144,
        "epoch": 0.4544302854162009,
        "step": 6098
    },
    {
        "loss": 2.4869,
        "grad_norm": 2.0714404582977295,
        "learning_rate": 0.00014263819280082175,
        "epoch": 0.45450480661748266,
        "step": 6099
    },
    {
        "loss": 2.1711,
        "grad_norm": 3.076758623123169,
        "learning_rate": 0.00014257453345918662,
        "epoch": 0.4545793278187644,
        "step": 6100
    },
    {
        "loss": 1.3211,
        "grad_norm": 1.8582884073257446,
        "learning_rate": 0.00014251085303750588,
        "epoch": 0.4546538490200462,
        "step": 6101
    },
    {
        "loss": 1.9916,
        "grad_norm": 2.87375807762146,
        "learning_rate": 0.00014244715156730995,
        "epoch": 0.45472837022132795,
        "step": 6102
    },
    {
        "loss": 2.0381,
        "grad_norm": 4.566751480102539,
        "learning_rate": 0.00014238342908013936,
        "epoch": 0.4548028914226097,
        "step": 6103
    },
    {
        "loss": 2.8773,
        "grad_norm": 2.718787431716919,
        "learning_rate": 0.0001423196856075452,
        "epoch": 0.4548774126238915,
        "step": 6104
    },
    {
        "loss": 2.1775,
        "grad_norm": 3.493739366531372,
        "learning_rate": 0.000142255921181089,
        "epoch": 0.45495193382517324,
        "step": 6105
    },
    {
        "loss": 2.5493,
        "grad_norm": 3.2142703533172607,
        "learning_rate": 0.00014219213583234258,
        "epoch": 0.455026455026455,
        "step": 6106
    },
    {
        "loss": 2.3672,
        "grad_norm": 2.0686347484588623,
        "learning_rate": 0.00014212832959288824,
        "epoch": 0.45510097622773676,
        "step": 6107
    },
    {
        "loss": 2.6019,
        "grad_norm": 3.187335252761841,
        "learning_rate": 0.00014206450249431844,
        "epoch": 0.4551754974290186,
        "step": 6108
    },
    {
        "loss": 2.1721,
        "grad_norm": 2.8788270950317383,
        "learning_rate": 0.00014200065456823616,
        "epoch": 0.45525001863030035,
        "step": 6109
    },
    {
        "loss": 2.4427,
        "grad_norm": 2.4021334648132324,
        "learning_rate": 0.0001419367858462545,
        "epoch": 0.4553245398315821,
        "step": 6110
    },
    {
        "loss": 2.7785,
        "grad_norm": 2.9133636951446533,
        "learning_rate": 0.00014187289635999697,
        "epoch": 0.45539906103286387,
        "step": 6111
    },
    {
        "loss": 1.9981,
        "grad_norm": 2.688220500946045,
        "learning_rate": 0.0001418089861410975,
        "epoch": 0.45547358223414564,
        "step": 6112
    },
    {
        "loss": 2.3898,
        "grad_norm": 3.196342706680298,
        "learning_rate": 0.00014174505522119993,
        "epoch": 0.4555481034354274,
        "step": 6113
    },
    {
        "loss": 2.182,
        "grad_norm": 4.479272842407227,
        "learning_rate": 0.0001416811036319586,
        "epoch": 0.45562262463670916,
        "step": 6114
    },
    {
        "loss": 2.7484,
        "grad_norm": 3.6270089149475098,
        "learning_rate": 0.00014161713140503808,
        "epoch": 0.4556971458379909,
        "step": 6115
    },
    {
        "loss": 2.166,
        "grad_norm": 3.2516677379608154,
        "learning_rate": 0.00014155313857211309,
        "epoch": 0.4557716670392727,
        "step": 6116
    },
    {
        "loss": 2.7602,
        "grad_norm": 2.620417356491089,
        "learning_rate": 0.00014148912516486864,
        "epoch": 0.45584618824055445,
        "step": 6117
    },
    {
        "loss": 1.957,
        "grad_norm": 2.296945571899414,
        "learning_rate": 0.00014142509121499975,
        "epoch": 0.4559207094418362,
        "step": 6118
    },
    {
        "loss": 2.8609,
        "grad_norm": 2.2819910049438477,
        "learning_rate": 0.00014136103675421187,
        "epoch": 0.455995230643118,
        "step": 6119
    },
    {
        "loss": 1.9737,
        "grad_norm": 2.676945447921753,
        "learning_rate": 0.00014129696181422032,
        "epoch": 0.45606975184439974,
        "step": 6120
    },
    {
        "loss": 2.5691,
        "grad_norm": 2.7095017433166504,
        "learning_rate": 0.00014123286642675074,
        "epoch": 0.4561442730456815,
        "step": 6121
    },
    {
        "loss": 2.5147,
        "grad_norm": 2.0128846168518066,
        "learning_rate": 0.00014116875062353894,
        "epoch": 0.45621879424696327,
        "step": 6122
    },
    {
        "loss": 2.6326,
        "grad_norm": 2.0163304805755615,
        "learning_rate": 0.0001411046144363307,
        "epoch": 0.45629331544824503,
        "step": 6123
    },
    {
        "loss": 2.369,
        "grad_norm": 2.2591381072998047,
        "learning_rate": 0.00014104045789688186,
        "epoch": 0.4563678366495268,
        "step": 6124
    },
    {
        "loss": 2.3552,
        "grad_norm": 3.6244163513183594,
        "learning_rate": 0.00014097628103695848,
        "epoch": 0.45644235785080856,
        "step": 6125
    },
    {
        "loss": 2.3054,
        "grad_norm": 2.563361644744873,
        "learning_rate": 0.00014091208388833674,
        "epoch": 0.4565168790520903,
        "step": 6126
    },
    {
        "loss": 2.9123,
        "grad_norm": 2.289259195327759,
        "learning_rate": 0.00014084786648280257,
        "epoch": 0.4565914002533721,
        "step": 6127
    },
    {
        "loss": 2.113,
        "grad_norm": 3.571279287338257,
        "learning_rate": 0.00014078362885215217,
        "epoch": 0.45666592145465384,
        "step": 6128
    },
    {
        "loss": 2.1051,
        "grad_norm": 3.6321218013763428,
        "learning_rate": 0.00014071937102819182,
        "epoch": 0.4567404426559356,
        "step": 6129
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.483853578567505,
        "learning_rate": 0.0001406550930427375,
        "epoch": 0.45681496385721737,
        "step": 6130
    },
    {
        "loss": 2.973,
        "grad_norm": 3.222780466079712,
        "learning_rate": 0.00014059079492761547,
        "epoch": 0.45688948505849913,
        "step": 6131
    },
    {
        "loss": 2.7174,
        "grad_norm": 2.647416353225708,
        "learning_rate": 0.00014052647671466167,
        "epoch": 0.4569640062597809,
        "step": 6132
    },
    {
        "loss": 2.7679,
        "grad_norm": 2.585068941116333,
        "learning_rate": 0.00014046213843572238,
        "epoch": 0.45703852746106266,
        "step": 6133
    },
    {
        "loss": 3.0235,
        "grad_norm": 2.2990660667419434,
        "learning_rate": 0.0001403977801226534,
        "epoch": 0.4571130486623444,
        "step": 6134
    },
    {
        "loss": 2.6513,
        "grad_norm": 2.3308122158050537,
        "learning_rate": 0.0001403334018073207,
        "epoch": 0.4571875698636262,
        "step": 6135
    },
    {
        "loss": 2.4962,
        "grad_norm": 1.8223026990890503,
        "learning_rate": 0.00014026900352160016,
        "epoch": 0.45726209106490795,
        "step": 6136
    },
    {
        "loss": 2.0535,
        "grad_norm": 1.9667619466781616,
        "learning_rate": 0.00014020458529737738,
        "epoch": 0.4573366122661897,
        "step": 6137
    },
    {
        "loss": 1.702,
        "grad_norm": 3.0365028381347656,
        "learning_rate": 0.00014014014716654793,
        "epoch": 0.4574111334674715,
        "step": 6138
    },
    {
        "loss": 2.1888,
        "grad_norm": 3.692155599594116,
        "learning_rate": 0.00014007568916101737,
        "epoch": 0.45748565466875324,
        "step": 6139
    },
    {
        "loss": 2.7779,
        "grad_norm": 3.4364054203033447,
        "learning_rate": 0.00014001121131270084,
        "epoch": 0.457560175870035,
        "step": 6140
    },
    {
        "loss": 2.3622,
        "grad_norm": 2.589183807373047,
        "learning_rate": 0.0001399467136535235,
        "epoch": 0.45763469707131677,
        "step": 6141
    },
    {
        "loss": 2.5527,
        "grad_norm": 2.8229775428771973,
        "learning_rate": 0.00013988219621542013,
        "epoch": 0.45770921827259853,
        "step": 6142
    },
    {
        "loss": 2.557,
        "grad_norm": 3.290450096130371,
        "learning_rate": 0.0001398176590303356,
        "epoch": 0.45778373947388035,
        "step": 6143
    },
    {
        "loss": 2.8997,
        "grad_norm": 1.8567506074905396,
        "learning_rate": 0.00013975310213022426,
        "epoch": 0.4578582606751621,
        "step": 6144
    },
    {
        "loss": 2.6101,
        "grad_norm": 2.4296159744262695,
        "learning_rate": 0.00013968852554705047,
        "epoch": 0.4579327818764439,
        "step": 6145
    },
    {
        "loss": 2.3013,
        "grad_norm": 2.982851505279541,
        "learning_rate": 0.00013962392931278806,
        "epoch": 0.45800730307772564,
        "step": 6146
    },
    {
        "loss": 2.6094,
        "grad_norm": 2.2201004028320312,
        "learning_rate": 0.00013955931345942078,
        "epoch": 0.4580818242790074,
        "step": 6147
    },
    {
        "loss": 2.4344,
        "grad_norm": 3.6048483848571777,
        "learning_rate": 0.00013949467801894211,
        "epoch": 0.45815634548028916,
        "step": 6148
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.8033974170684814,
        "learning_rate": 0.00013943002302335523,
        "epoch": 0.4582308666815709,
        "step": 6149
    },
    {
        "loss": 2.23,
        "grad_norm": 3.1187517642974854,
        "learning_rate": 0.00013936534850467284,
        "epoch": 0.4583053878828527,
        "step": 6150
    },
    {
        "loss": 2.5686,
        "grad_norm": 2.393921136856079,
        "learning_rate": 0.00013930065449491738,
        "epoch": 0.45837990908413445,
        "step": 6151
    },
    {
        "loss": 2.0046,
        "grad_norm": 3.0892233848571777,
        "learning_rate": 0.00013923594102612102,
        "epoch": 0.4584544302854162,
        "step": 6152
    },
    {
        "loss": 2.7441,
        "grad_norm": 2.7407660484313965,
        "learning_rate": 0.00013917120813032564,
        "epoch": 0.458528951486698,
        "step": 6153
    },
    {
        "loss": 2.069,
        "grad_norm": 2.6375272274017334,
        "learning_rate": 0.00013910645583958245,
        "epoch": 0.45860347268797974,
        "step": 6154
    },
    {
        "loss": 2.8063,
        "grad_norm": 2.235379934310913,
        "learning_rate": 0.00013904168418595243,
        "epoch": 0.4586779938892615,
        "step": 6155
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.601409435272217,
        "learning_rate": 0.00013897689320150619,
        "epoch": 0.45875251509054327,
        "step": 6156
    },
    {
        "loss": 2.1511,
        "grad_norm": 1.2672590017318726,
        "learning_rate": 0.00013891208291832391,
        "epoch": 0.45882703629182503,
        "step": 6157
    },
    {
        "loss": 2.6275,
        "grad_norm": 2.5104777812957764,
        "learning_rate": 0.00013884725336849523,
        "epoch": 0.4589015574931068,
        "step": 6158
    },
    {
        "loss": 2.2909,
        "grad_norm": 3.099832534790039,
        "learning_rate": 0.00013878240458411944,
        "epoch": 0.45897607869438856,
        "step": 6159
    },
    {
        "loss": 2.8917,
        "grad_norm": 2.8230350017547607,
        "learning_rate": 0.0001387175365973053,
        "epoch": 0.4590505998956703,
        "step": 6160
    },
    {
        "loss": 2.7381,
        "grad_norm": 2.4324769973754883,
        "learning_rate": 0.00013865264944017095,
        "epoch": 0.4591251210969521,
        "step": 6161
    },
    {
        "loss": 2.6568,
        "grad_norm": 1.5444179773330688,
        "learning_rate": 0.00013858774314484424,
        "epoch": 0.45919964229823385,
        "step": 6162
    },
    {
        "loss": 2.0831,
        "grad_norm": 3.0252702236175537,
        "learning_rate": 0.00013852281774346246,
        "epoch": 0.4592741634995156,
        "step": 6163
    },
    {
        "loss": 2.1938,
        "grad_norm": 3.7198288440704346,
        "learning_rate": 0.00013845787326817224,
        "epoch": 0.45934868470079737,
        "step": 6164
    },
    {
        "loss": 2.4999,
        "grad_norm": 3.608167886734009,
        "learning_rate": 0.0001383929097511296,
        "epoch": 0.45942320590207913,
        "step": 6165
    },
    {
        "loss": 2.0779,
        "grad_norm": 3.1145827770233154,
        "learning_rate": 0.00013832792722450024,
        "epoch": 0.4594977271033609,
        "step": 6166
    },
    {
        "loss": 2.658,
        "grad_norm": 2.6268768310546875,
        "learning_rate": 0.00013826292572045914,
        "epoch": 0.45957224830464266,
        "step": 6167
    },
    {
        "loss": 2.4952,
        "grad_norm": 3.288367748260498,
        "learning_rate": 0.00013819790527119062,
        "epoch": 0.4596467695059244,
        "step": 6168
    },
    {
        "loss": 2.2253,
        "grad_norm": 2.5838124752044678,
        "learning_rate": 0.00013813286590888852,
        "epoch": 0.4597212907072062,
        "step": 6169
    },
    {
        "loss": 2.8682,
        "grad_norm": 3.94321608543396,
        "learning_rate": 0.0001380678076657559,
        "epoch": 0.45979581190848795,
        "step": 6170
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.4342246055603027,
        "learning_rate": 0.00013800273057400518,
        "epoch": 0.4598703331097697,
        "step": 6171
    },
    {
        "loss": 2.5793,
        "grad_norm": 2.415780782699585,
        "learning_rate": 0.00013793763466585818,
        "epoch": 0.4599448543110515,
        "step": 6172
    },
    {
        "loss": 1.9685,
        "grad_norm": 3.6904237270355225,
        "learning_rate": 0.00013787251997354608,
        "epoch": 0.46001937551233324,
        "step": 6173
    },
    {
        "loss": 2.3151,
        "grad_norm": 1.9633874893188477,
        "learning_rate": 0.00013780738652930928,
        "epoch": 0.460093896713615,
        "step": 6174
    },
    {
        "loss": 2.5405,
        "grad_norm": 2.166656494140625,
        "learning_rate": 0.0001377422343653974,
        "epoch": 0.46016841791489677,
        "step": 6175
    },
    {
        "loss": 2.9561,
        "grad_norm": 4.162422180175781,
        "learning_rate": 0.00013767706351406944,
        "epoch": 0.46024293911617853,
        "step": 6176
    },
    {
        "loss": 2.2517,
        "grad_norm": 2.5891337394714355,
        "learning_rate": 0.00013761187400759364,
        "epoch": 0.4603174603174603,
        "step": 6177
    },
    {
        "loss": 2.8162,
        "grad_norm": 3.6630563735961914,
        "learning_rate": 0.00013754666587824756,
        "epoch": 0.46039198151874206,
        "step": 6178
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.832991600036621,
        "learning_rate": 0.0001374814391583177,
        "epoch": 0.4604665027200239,
        "step": 6179
    },
    {
        "loss": 2.4919,
        "grad_norm": 4.678443431854248,
        "learning_rate": 0.0001374161938801001,
        "epoch": 0.46054102392130564,
        "step": 6180
    },
    {
        "loss": 1.7491,
        "grad_norm": 3.5878994464874268,
        "learning_rate": 0.0001373509300758997,
        "epoch": 0.4606155451225874,
        "step": 6181
    },
    {
        "loss": 2.2265,
        "grad_norm": 3.381186008453369,
        "learning_rate": 0.00013728564777803083,
        "epoch": 0.46069006632386916,
        "step": 6182
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.7822659015655518,
        "learning_rate": 0.0001372203470188169,
        "epoch": 0.4607645875251509,
        "step": 6183
    },
    {
        "loss": 2.5378,
        "grad_norm": 2.0070464611053467,
        "learning_rate": 0.00013715502783059048,
        "epoch": 0.4608391087264327,
        "step": 6184
    },
    {
        "loss": 2.1018,
        "grad_norm": 4.220664978027344,
        "learning_rate": 0.00013708969024569308,
        "epoch": 0.46091362992771445,
        "step": 6185
    },
    {
        "loss": 2.2222,
        "grad_norm": 2.8994903564453125,
        "learning_rate": 0.0001370243342964756,
        "epoch": 0.4609881511289962,
        "step": 6186
    },
    {
        "loss": 2.4432,
        "grad_norm": 2.609888792037964,
        "learning_rate": 0.00013695896001529796,
        "epoch": 0.461062672330278,
        "step": 6187
    },
    {
        "loss": 2.5727,
        "grad_norm": 2.438965320587158,
        "learning_rate": 0.00013689356743452896,
        "epoch": 0.46113719353155974,
        "step": 6188
    },
    {
        "loss": 1.6789,
        "grad_norm": 4.382780075073242,
        "learning_rate": 0.0001368281565865467,
        "epoch": 0.4612117147328415,
        "step": 6189
    },
    {
        "loss": 2.3689,
        "grad_norm": 3.695075035095215,
        "learning_rate": 0.00013676272750373828,
        "epoch": 0.46128623593412327,
        "step": 6190
    },
    {
        "loss": 2.5323,
        "grad_norm": 4.025108337402344,
        "learning_rate": 0.00013669728021849969,
        "epoch": 0.46136075713540503,
        "step": 6191
    },
    {
        "loss": 2.3674,
        "grad_norm": 3.3737940788269043,
        "learning_rate": 0.00013663181476323609,
        "epoch": 0.4614352783366868,
        "step": 6192
    },
    {
        "loss": 2.7254,
        "grad_norm": 3.0916197299957275,
        "learning_rate": 0.00013656633117036147,
        "epoch": 0.46150979953796856,
        "step": 6193
    },
    {
        "loss": 2.6955,
        "grad_norm": 2.0713789463043213,
        "learning_rate": 0.00013650082947229907,
        "epoch": 0.4615843207392503,
        "step": 6194
    },
    {
        "loss": 2.6725,
        "grad_norm": 3.6511521339416504,
        "learning_rate": 0.00013643530970148073,
        "epoch": 0.4616588419405321,
        "step": 6195
    },
    {
        "loss": 2.2648,
        "grad_norm": 2.757880449295044,
        "learning_rate": 0.00013636977189034755,
        "epoch": 0.46173336314181385,
        "step": 6196
    },
    {
        "loss": 1.773,
        "grad_norm": 4.71123743057251,
        "learning_rate": 0.00013630421607134947,
        "epoch": 0.4618078843430956,
        "step": 6197
    },
    {
        "loss": 2.3264,
        "grad_norm": 4.038713455200195,
        "learning_rate": 0.00013623864227694523,
        "epoch": 0.4618824055443774,
        "step": 6198
    },
    {
        "loss": 2.7674,
        "grad_norm": 3.2436537742614746,
        "learning_rate": 0.0001361730505396026,
        "epoch": 0.46195692674565914,
        "step": 6199
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.9681036472320557,
        "learning_rate": 0.00013610744089179825,
        "epoch": 0.4620314479469409,
        "step": 6200
    },
    {
        "loss": 0.7508,
        "grad_norm": 6.021252632141113,
        "learning_rate": 0.00013604181336601765,
        "epoch": 0.46210596914822266,
        "step": 6201
    },
    {
        "loss": 2.498,
        "grad_norm": 3.275514841079712,
        "learning_rate": 0.00013597616799475504,
        "epoch": 0.4621804903495044,
        "step": 6202
    },
    {
        "loss": 2.6548,
        "grad_norm": 1.9133515357971191,
        "learning_rate": 0.00013591050481051367,
        "epoch": 0.4622550115507862,
        "step": 6203
    },
    {
        "loss": 2.0898,
        "grad_norm": 2.4348180294036865,
        "learning_rate": 0.00013584482384580558,
        "epoch": 0.46232953275206795,
        "step": 6204
    },
    {
        "loss": 2.5118,
        "grad_norm": 1.88210928440094,
        "learning_rate": 0.00013577912513315142,
        "epoch": 0.4624040539533497,
        "step": 6205
    },
    {
        "loss": 1.8342,
        "grad_norm": 3.2284929752349854,
        "learning_rate": 0.00013571340870508096,
        "epoch": 0.4624785751546315,
        "step": 6206
    },
    {
        "loss": 2.6177,
        "grad_norm": 2.5648584365844727,
        "learning_rate": 0.0001356476745941324,
        "epoch": 0.46255309635591324,
        "step": 6207
    },
    {
        "loss": 2.4822,
        "grad_norm": 3.7163686752319336,
        "learning_rate": 0.0001355819228328529,
        "epoch": 0.462627617557195,
        "step": 6208
    },
    {
        "loss": 2.6882,
        "grad_norm": 2.27290678024292,
        "learning_rate": 0.0001355161534537983,
        "epoch": 0.46270213875847677,
        "step": 6209
    },
    {
        "loss": 2.7635,
        "grad_norm": 4.226205825805664,
        "learning_rate": 0.0001354503664895333,
        "epoch": 0.46277665995975853,
        "step": 6210
    },
    {
        "loss": 3.1329,
        "grad_norm": 2.7606894969940186,
        "learning_rate": 0.00013538456197263103,
        "epoch": 0.4628511811610403,
        "step": 6211
    },
    {
        "loss": 2.4491,
        "grad_norm": 2.4204766750335693,
        "learning_rate": 0.00013531873993567346,
        "epoch": 0.46292570236232206,
        "step": 6212
    },
    {
        "loss": 1.8819,
        "grad_norm": 4.489003658294678,
        "learning_rate": 0.00013525290041125127,
        "epoch": 0.4630002235636038,
        "step": 6213
    },
    {
        "loss": 2.4296,
        "grad_norm": 2.463798999786377,
        "learning_rate": 0.00013518704343196385,
        "epoch": 0.46307474476488564,
        "step": 6214
    },
    {
        "loss": 2.5077,
        "grad_norm": 3.2105982303619385,
        "learning_rate": 0.00013512116903041904,
        "epoch": 0.4631492659661674,
        "step": 6215
    },
    {
        "loss": 1.9354,
        "grad_norm": 1.6754944324493408,
        "learning_rate": 0.00013505527723923347,
        "epoch": 0.46322378716744916,
        "step": 6216
    },
    {
        "loss": 2.8747,
        "grad_norm": 2.923738718032837,
        "learning_rate": 0.00013498936809103226,
        "epoch": 0.4632983083687309,
        "step": 6217
    },
    {
        "loss": 2.48,
        "grad_norm": 2.8060879707336426,
        "learning_rate": 0.00013492344161844918,
        "epoch": 0.4633728295700127,
        "step": 6218
    },
    {
        "loss": 2.772,
        "grad_norm": 2.1307640075683594,
        "learning_rate": 0.00013485749785412666,
        "epoch": 0.46344735077129445,
        "step": 6219
    },
    {
        "loss": 2.1109,
        "grad_norm": 3.251340389251709,
        "learning_rate": 0.00013479153683071564,
        "epoch": 0.4635218719725762,
        "step": 6220
    },
    {
        "loss": 2.2181,
        "grad_norm": 2.525582790374756,
        "learning_rate": 0.00013472555858087555,
        "epoch": 0.463596393173858,
        "step": 6221
    },
    {
        "loss": 2.614,
        "grad_norm": 1.9960483312606812,
        "learning_rate": 0.00013465956313727428,
        "epoch": 0.46367091437513974,
        "step": 6222
    },
    {
        "loss": 2.1919,
        "grad_norm": 4.250482082366943,
        "learning_rate": 0.00013459355053258842,
        "epoch": 0.4637454355764215,
        "step": 6223
    },
    {
        "loss": 2.1289,
        "grad_norm": 2.396448850631714,
        "learning_rate": 0.000134527520799503,
        "epoch": 0.46381995677770327,
        "step": 6224
    },
    {
        "loss": 2.191,
        "grad_norm": 3.036074638366699,
        "learning_rate": 0.00013446147397071155,
        "epoch": 0.46389447797898503,
        "step": 6225
    },
    {
        "loss": 2.7574,
        "grad_norm": 2.861902952194214,
        "learning_rate": 0.00013439541007891583,
        "epoch": 0.4639689991802668,
        "step": 6226
    },
    {
        "loss": 2.7227,
        "grad_norm": 2.403310537338257,
        "learning_rate": 0.00013432932915682634,
        "epoch": 0.46404352038154856,
        "step": 6227
    },
    {
        "loss": 2.0909,
        "grad_norm": 4.22685432434082,
        "learning_rate": 0.00013426323123716193,
        "epoch": 0.4641180415828303,
        "step": 6228
    },
    {
        "loss": 2.3208,
        "grad_norm": 2.3301987648010254,
        "learning_rate": 0.00013419711635264983,
        "epoch": 0.4641925627841121,
        "step": 6229
    },
    {
        "loss": 1.6474,
        "grad_norm": 3.79506254196167,
        "learning_rate": 0.00013413098453602572,
        "epoch": 0.46426708398539385,
        "step": 6230
    },
    {
        "loss": 2.4509,
        "grad_norm": 3.2145235538482666,
        "learning_rate": 0.0001340648358200336,
        "epoch": 0.4643416051866756,
        "step": 6231
    },
    {
        "loss": 2.1659,
        "grad_norm": 5.10829496383667,
        "learning_rate": 0.00013399867023742586,
        "epoch": 0.4644161263879574,
        "step": 6232
    },
    {
        "loss": 2.9267,
        "grad_norm": 2.6932926177978516,
        "learning_rate": 0.00013393248782096314,
        "epoch": 0.46449064758923914,
        "step": 6233
    },
    {
        "loss": 1.9497,
        "grad_norm": 1.6374849081039429,
        "learning_rate": 0.00013386628860341475,
        "epoch": 0.4645651687905209,
        "step": 6234
    },
    {
        "loss": 2.5291,
        "grad_norm": 3.0938191413879395,
        "learning_rate": 0.00013380007261755794,
        "epoch": 0.46463968999180266,
        "step": 6235
    },
    {
        "loss": 2.6271,
        "grad_norm": 2.435021162033081,
        "learning_rate": 0.00013373383989617838,
        "epoch": 0.4647142111930844,
        "step": 6236
    },
    {
        "loss": 2.2589,
        "grad_norm": 2.9302818775177,
        "learning_rate": 0.0001336675904720701,
        "epoch": 0.4647887323943662,
        "step": 6237
    },
    {
        "loss": 2.8939,
        "grad_norm": 2.4697470664978027,
        "learning_rate": 0.00013360132437803537,
        "epoch": 0.46486325359564795,
        "step": 6238
    },
    {
        "loss": 2.3439,
        "grad_norm": 3.1196775436401367,
        "learning_rate": 0.0001335350416468848,
        "epoch": 0.4649377747969297,
        "step": 6239
    },
    {
        "loss": 2.6988,
        "grad_norm": 2.5242433547973633,
        "learning_rate": 0.000133468742311437,
        "epoch": 0.4650122959982115,
        "step": 6240
    },
    {
        "loss": 1.8297,
        "grad_norm": 3.297126293182373,
        "learning_rate": 0.000133402426404519,
        "epoch": 0.46508681719949324,
        "step": 6241
    },
    {
        "loss": 2.5252,
        "grad_norm": 2.5066444873809814,
        "learning_rate": 0.00013333609395896595,
        "epoch": 0.465161338400775,
        "step": 6242
    },
    {
        "loss": 2.3594,
        "grad_norm": 3.0505239963531494,
        "learning_rate": 0.0001332697450076212,
        "epoch": 0.46523585960205677,
        "step": 6243
    },
    {
        "loss": 1.8498,
        "grad_norm": 2.439690113067627,
        "learning_rate": 0.0001332033795833364,
        "epoch": 0.46531038080333853,
        "step": 6244
    },
    {
        "loss": 2.863,
        "grad_norm": 3.03676700592041,
        "learning_rate": 0.00013313699771897114,
        "epoch": 0.4653849020046203,
        "step": 6245
    },
    {
        "loss": 2.7127,
        "grad_norm": 3.4569203853607178,
        "learning_rate": 0.00013307059944739325,
        "epoch": 0.46545942320590206,
        "step": 6246
    },
    {
        "loss": 2.345,
        "grad_norm": 2.130286931991577,
        "learning_rate": 0.0001330041848014787,
        "epoch": 0.4655339444071838,
        "step": 6247
    },
    {
        "loss": 2.5207,
        "grad_norm": 2.3070619106292725,
        "learning_rate": 0.00013293775381411165,
        "epoch": 0.4656084656084656,
        "step": 6248
    },
    {
        "loss": 2.9876,
        "grad_norm": 3.198636770248413,
        "learning_rate": 0.00013287130651818413,
        "epoch": 0.46568298680974735,
        "step": 6249
    },
    {
        "loss": 1.6185,
        "grad_norm": 3.8072762489318848,
        "learning_rate": 0.0001328048429465964,
        "epoch": 0.46575750801102916,
        "step": 6250
    },
    {
        "loss": 2.7828,
        "grad_norm": 3.156606435775757,
        "learning_rate": 0.00013273836313225685,
        "epoch": 0.4658320292123109,
        "step": 6251
    },
    {
        "loss": 2.5958,
        "grad_norm": 4.270211696624756,
        "learning_rate": 0.00013267186710808168,
        "epoch": 0.4659065504135927,
        "step": 6252
    },
    {
        "loss": 2.3705,
        "grad_norm": 3.065765857696533,
        "learning_rate": 0.00013260535490699536,
        "epoch": 0.46598107161487445,
        "step": 6253
    },
    {
        "loss": 2.3336,
        "grad_norm": 2.7006330490112305,
        "learning_rate": 0.00013253882656193012,
        "epoch": 0.4660555928161562,
        "step": 6254
    },
    {
        "loss": 2.3002,
        "grad_norm": 2.0011038780212402,
        "learning_rate": 0.0001324722821058265,
        "epoch": 0.466130114017438,
        "step": 6255
    },
    {
        "loss": 2.1815,
        "grad_norm": 3.211164712905884,
        "learning_rate": 0.00013240572157163268,
        "epoch": 0.46620463521871974,
        "step": 6256
    },
    {
        "loss": 2.7022,
        "grad_norm": 2.7488794326782227,
        "learning_rate": 0.000132339144992305,
        "epoch": 0.4662791564200015,
        "step": 6257
    },
    {
        "loss": 2.4488,
        "grad_norm": 3.350949287414551,
        "learning_rate": 0.0001322725524008078,
        "epoch": 0.46635367762128327,
        "step": 6258
    },
    {
        "loss": 2.5814,
        "grad_norm": 2.20322322845459,
        "learning_rate": 0.00013220594383011314,
        "epoch": 0.46642819882256503,
        "step": 6259
    },
    {
        "loss": 2.1609,
        "grad_norm": 3.509368896484375,
        "learning_rate": 0.00013213931931320113,
        "epoch": 0.4665027200238468,
        "step": 6260
    },
    {
        "loss": 1.9397,
        "grad_norm": 3.078700542449951,
        "learning_rate": 0.00013207267888305983,
        "epoch": 0.46657724122512856,
        "step": 6261
    },
    {
        "loss": 2.7388,
        "grad_norm": 2.93505859375,
        "learning_rate": 0.00013200602257268498,
        "epoch": 0.4666517624264103,
        "step": 6262
    },
    {
        "loss": 2.3776,
        "grad_norm": 3.0168275833129883,
        "learning_rate": 0.00013193935041508044,
        "epoch": 0.4667262836276921,
        "step": 6263
    },
    {
        "loss": 2.7308,
        "grad_norm": 3.2684481143951416,
        "learning_rate": 0.00013187266244325758,
        "epoch": 0.46680080482897385,
        "step": 6264
    },
    {
        "loss": 2.6428,
        "grad_norm": 2.7886898517608643,
        "learning_rate": 0.00013180595869023603,
        "epoch": 0.4668753260302556,
        "step": 6265
    },
    {
        "loss": 2.2463,
        "grad_norm": 4.2951340675354,
        "learning_rate": 0.0001317392391890428,
        "epoch": 0.4669498472315374,
        "step": 6266
    },
    {
        "loss": 2.7651,
        "grad_norm": 2.632504463195801,
        "learning_rate": 0.00013167250397271306,
        "epoch": 0.46702436843281914,
        "step": 6267
    },
    {
        "loss": 2.271,
        "grad_norm": 3.4669878482818604,
        "learning_rate": 0.00013160575307428945,
        "epoch": 0.4670988896341009,
        "step": 6268
    },
    {
        "loss": 2.053,
        "grad_norm": 2.424786329269409,
        "learning_rate": 0.00013153898652682258,
        "epoch": 0.46717341083538266,
        "step": 6269
    },
    {
        "loss": 2.9579,
        "grad_norm": 2.8140149116516113,
        "learning_rate": 0.0001314722043633708,
        "epoch": 0.4672479320366644,
        "step": 6270
    },
    {
        "loss": 1.4503,
        "grad_norm": 2.9523346424102783,
        "learning_rate": 0.0001314054066170002,
        "epoch": 0.4673224532379462,
        "step": 6271
    },
    {
        "loss": 1.7623,
        "grad_norm": 2.2461819648742676,
        "learning_rate": 0.00013133859332078445,
        "epoch": 0.46739697443922795,
        "step": 6272
    },
    {
        "loss": 1.829,
        "grad_norm": 4.043522357940674,
        "learning_rate": 0.00013127176450780493,
        "epoch": 0.4674714956405097,
        "step": 6273
    },
    {
        "loss": 2.1056,
        "grad_norm": 3.321868658065796,
        "learning_rate": 0.00013120492021115085,
        "epoch": 0.4675460168417915,
        "step": 6274
    },
    {
        "loss": 1.7023,
        "grad_norm": 2.1411099433898926,
        "learning_rate": 0.00013113806046391907,
        "epoch": 0.46762053804307324,
        "step": 6275
    },
    {
        "loss": 2.7509,
        "grad_norm": 2.9926321506500244,
        "learning_rate": 0.00013107118529921393,
        "epoch": 0.467695059244355,
        "step": 6276
    },
    {
        "loss": 1.6343,
        "grad_norm": 4.045325756072998,
        "learning_rate": 0.00013100429475014762,
        "epoch": 0.46776958044563677,
        "step": 6277
    },
    {
        "loss": 2.0243,
        "grad_norm": 3.0512850284576416,
        "learning_rate": 0.0001309373888498397,
        "epoch": 0.46784410164691853,
        "step": 6278
    },
    {
        "loss": 2.5814,
        "grad_norm": 2.5705463886260986,
        "learning_rate": 0.00013087046763141758,
        "epoch": 0.4679186228482003,
        "step": 6279
    },
    {
        "loss": 1.9476,
        "grad_norm": 3.875375986099243,
        "learning_rate": 0.00013080353112801608,
        "epoch": 0.46799314404948206,
        "step": 6280
    },
    {
        "loss": 2.7229,
        "grad_norm": 3.453443765640259,
        "learning_rate": 0.00013073657937277778,
        "epoch": 0.4680676652507638,
        "step": 6281
    },
    {
        "loss": 1.3831,
        "grad_norm": 1.7611958980560303,
        "learning_rate": 0.00013066961239885264,
        "epoch": 0.4681421864520456,
        "step": 6282
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.6928787231445312,
        "learning_rate": 0.00013060263023939808,
        "epoch": 0.46821670765332735,
        "step": 6283
    },
    {
        "loss": 2.1072,
        "grad_norm": 3.874631881713867,
        "learning_rate": 0.00013053563292757923,
        "epoch": 0.4682912288546091,
        "step": 6284
    },
    {
        "loss": 2.5064,
        "grad_norm": 2.6262922286987305,
        "learning_rate": 0.00013046862049656874,
        "epoch": 0.46836575005589093,
        "step": 6285
    },
    {
        "loss": 3.4153,
        "grad_norm": 3.524460792541504,
        "learning_rate": 0.00013040159297954658,
        "epoch": 0.4684402712571727,
        "step": 6286
    },
    {
        "loss": 2.5926,
        "grad_norm": 2.0309441089630127,
        "learning_rate": 0.00013033455040970017,
        "epoch": 0.46851479245845445,
        "step": 6287
    },
    {
        "loss": 2.0075,
        "grad_norm": 3.9601542949676514,
        "learning_rate": 0.00013026749282022464,
        "epoch": 0.4685893136597362,
        "step": 6288
    },
    {
        "loss": 2.4151,
        "grad_norm": 3.373035192489624,
        "learning_rate": 0.00013020042024432233,
        "epoch": 0.468663834861018,
        "step": 6289
    },
    {
        "loss": 2.4633,
        "grad_norm": 2.905470132827759,
        "learning_rate": 0.0001301333327152031,
        "epoch": 0.46873835606229974,
        "step": 6290
    },
    {
        "loss": 2.5045,
        "grad_norm": 3.5346450805664062,
        "learning_rate": 0.0001300662302660842,
        "epoch": 0.4688128772635815,
        "step": 6291
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.9506109952926636,
        "learning_rate": 0.0001299991129301902,
        "epoch": 0.46888739846486327,
        "step": 6292
    },
    {
        "loss": 2.4841,
        "grad_norm": 3.0453226566314697,
        "learning_rate": 0.0001299319807407531,
        "epoch": 0.46896191966614503,
        "step": 6293
    },
    {
        "loss": 2.074,
        "grad_norm": 1.6779080629348755,
        "learning_rate": 0.00012986483373101222,
        "epoch": 0.4690364408674268,
        "step": 6294
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.53627872467041,
        "learning_rate": 0.00012979767193421431,
        "epoch": 0.46911096206870856,
        "step": 6295
    },
    {
        "loss": 2.33,
        "grad_norm": 2.677602767944336,
        "learning_rate": 0.00012973049538361338,
        "epoch": 0.4691854832699903,
        "step": 6296
    },
    {
        "loss": 2.5112,
        "grad_norm": 2.296881675720215,
        "learning_rate": 0.0001296633041124706,
        "epoch": 0.4692600044712721,
        "step": 6297
    },
    {
        "loss": 2.9387,
        "grad_norm": 3.5697693824768066,
        "learning_rate": 0.00012959609815405468,
        "epoch": 0.46933452567255385,
        "step": 6298
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.462371349334717,
        "learning_rate": 0.0001295288775416415,
        "epoch": 0.4694090468738356,
        "step": 6299
    },
    {
        "loss": 2.7612,
        "grad_norm": 2.7086689472198486,
        "learning_rate": 0.0001294616423085142,
        "epoch": 0.4694835680751174,
        "step": 6300
    },
    {
        "loss": 2.8968,
        "grad_norm": 2.10040020942688,
        "learning_rate": 0.00012939439248796304,
        "epoch": 0.46955808927639914,
        "step": 6301
    },
    {
        "loss": 2.0169,
        "grad_norm": 3.470238208770752,
        "learning_rate": 0.00012932712811328579,
        "epoch": 0.4696326104776809,
        "step": 6302
    },
    {
        "loss": 1.846,
        "grad_norm": 2.8920609951019287,
        "learning_rate": 0.000129259849217787,
        "epoch": 0.46970713167896266,
        "step": 6303
    },
    {
        "loss": 3.0491,
        "grad_norm": 3.090355396270752,
        "learning_rate": 0.00012919255583477882,
        "epoch": 0.4697816528802444,
        "step": 6304
    },
    {
        "loss": 2.3656,
        "grad_norm": 3.8545820713043213,
        "learning_rate": 0.00012912524799758043,
        "epoch": 0.4698561740815262,
        "step": 6305
    },
    {
        "loss": 2.3869,
        "grad_norm": 2.063932418823242,
        "learning_rate": 0.0001290579257395181,
        "epoch": 0.46993069528280795,
        "step": 6306
    },
    {
        "loss": 2.4834,
        "grad_norm": 2.168682098388672,
        "learning_rate": 0.0001289905890939252,
        "epoch": 0.4700052164840897,
        "step": 6307
    },
    {
        "loss": 2.1082,
        "grad_norm": 4.158262252807617,
        "learning_rate": 0.00012892323809414238,
        "epoch": 0.4700797376853715,
        "step": 6308
    },
    {
        "loss": 1.6854,
        "grad_norm": 3.659802198410034,
        "learning_rate": 0.00012885587277351732,
        "epoch": 0.47015425888665324,
        "step": 6309
    },
    {
        "loss": 2.1319,
        "grad_norm": 2.575364112854004,
        "learning_rate": 0.00012878849316540488,
        "epoch": 0.470228780087935,
        "step": 6310
    },
    {
        "loss": 2.4746,
        "grad_norm": 2.3584179878234863,
        "learning_rate": 0.00012872109930316673,
        "epoch": 0.47030330128921677,
        "step": 6311
    },
    {
        "loss": 2.3129,
        "grad_norm": 2.870302200317383,
        "learning_rate": 0.00012865369122017198,
        "epoch": 0.47037782249049853,
        "step": 6312
    },
    {
        "loss": 2.8907,
        "grad_norm": 2.2301411628723145,
        "learning_rate": 0.00012858626894979639,
        "epoch": 0.4704523436917803,
        "step": 6313
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.7587132453918457,
        "learning_rate": 0.0001285188325254231,
        "epoch": 0.47052686489306206,
        "step": 6314
    },
    {
        "loss": 2.0536,
        "grad_norm": 3.0885844230651855,
        "learning_rate": 0.00012845138198044195,
        "epoch": 0.4706013860943438,
        "step": 6315
    },
    {
        "loss": 1.5943,
        "grad_norm": 2.7823362350463867,
        "learning_rate": 0.00012838391734825005,
        "epoch": 0.4706759072956256,
        "step": 6316
    },
    {
        "loss": 1.8345,
        "grad_norm": 2.761350154876709,
        "learning_rate": 0.00012831643866225117,
        "epoch": 0.47075042849690735,
        "step": 6317
    },
    {
        "loss": 2.657,
        "grad_norm": 3.115704298019409,
        "learning_rate": 0.00012824894595585634,
        "epoch": 0.4708249496981891,
        "step": 6318
    },
    {
        "loss": 1.583,
        "grad_norm": 3.1158957481384277,
        "learning_rate": 0.00012818143926248348,
        "epoch": 0.4708994708994709,
        "step": 6319
    },
    {
        "loss": 2.2806,
        "grad_norm": 3.489548444747925,
        "learning_rate": 0.00012811391861555722,
        "epoch": 0.47097399210075264,
        "step": 6320
    },
    {
        "loss": 2.6478,
        "grad_norm": 3.5425140857696533,
        "learning_rate": 0.00012804638404850926,
        "epoch": 0.47104851330203446,
        "step": 6321
    },
    {
        "loss": 2.3533,
        "grad_norm": 3.6832356452941895,
        "learning_rate": 0.00012797883559477827,
        "epoch": 0.4711230345033162,
        "step": 6322
    },
    {
        "loss": 2.3031,
        "grad_norm": 2.9278390407562256,
        "learning_rate": 0.00012791127328780965,
        "epoch": 0.471197555704598,
        "step": 6323
    },
    {
        "loss": 2.3049,
        "grad_norm": 3.379807710647583,
        "learning_rate": 0.0001278436971610557,
        "epoch": 0.47127207690587974,
        "step": 6324
    },
    {
        "loss": 1.4683,
        "grad_norm": 1.554901123046875,
        "learning_rate": 0.00012777610724797555,
        "epoch": 0.4713465981071615,
        "step": 6325
    },
    {
        "loss": 1.7471,
        "grad_norm": 4.779822826385498,
        "learning_rate": 0.00012770850358203526,
        "epoch": 0.47142111930844327,
        "step": 6326
    },
    {
        "loss": 2.3423,
        "grad_norm": 2.007603883743286,
        "learning_rate": 0.0001276408861967075,
        "epoch": 0.47149564050972503,
        "step": 6327
    },
    {
        "loss": 2.4195,
        "grad_norm": 3.4592320919036865,
        "learning_rate": 0.000127573255125472,
        "epoch": 0.4715701617110068,
        "step": 6328
    },
    {
        "loss": 1.4729,
        "grad_norm": 3.112382173538208,
        "learning_rate": 0.00012750561040181496,
        "epoch": 0.47164468291228856,
        "step": 6329
    },
    {
        "loss": 2.1468,
        "grad_norm": 4.710006237030029,
        "learning_rate": 0.00012743795205922954,
        "epoch": 0.4717192041135703,
        "step": 6330
    },
    {
        "loss": 1.7748,
        "grad_norm": 3.083047866821289,
        "learning_rate": 0.0001273702801312157,
        "epoch": 0.4717937253148521,
        "step": 6331
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.2631161212921143,
        "learning_rate": 0.00012730259465128,
        "epoch": 0.47186824651613385,
        "step": 6332
    },
    {
        "loss": 2.3701,
        "grad_norm": 3.4817955493927,
        "learning_rate": 0.00012723489565293568,
        "epoch": 0.4719427677174156,
        "step": 6333
    },
    {
        "loss": 2.4161,
        "grad_norm": 3.463416814804077,
        "learning_rate": 0.0001271671831697027,
        "epoch": 0.4720172889186974,
        "step": 6334
    },
    {
        "loss": 2.6668,
        "grad_norm": 3.2412710189819336,
        "learning_rate": 0.00012709945723510778,
        "epoch": 0.47209181011997914,
        "step": 6335
    },
    {
        "loss": 3.0469,
        "grad_norm": 2.308190107345581,
        "learning_rate": 0.0001270317178826843,
        "epoch": 0.4721663313212609,
        "step": 6336
    },
    {
        "loss": 2.1654,
        "grad_norm": 3.167167901992798,
        "learning_rate": 0.0001269639651459721,
        "epoch": 0.47224085252254266,
        "step": 6337
    },
    {
        "loss": 1.6608,
        "grad_norm": 2.571965217590332,
        "learning_rate": 0.00012689619905851797,
        "epoch": 0.4723153737238244,
        "step": 6338
    },
    {
        "loss": 2.7639,
        "grad_norm": 2.8273069858551025,
        "learning_rate": 0.00012682841965387488,
        "epoch": 0.4723898949251062,
        "step": 6339
    },
    {
        "loss": 2.1153,
        "grad_norm": 4.061785697937012,
        "learning_rate": 0.0001267606269656028,
        "epoch": 0.47246441612638795,
        "step": 6340
    },
    {
        "loss": 2.3889,
        "grad_norm": 2.6632280349731445,
        "learning_rate": 0.00012669282102726804,
        "epoch": 0.4725389373276697,
        "step": 6341
    },
    {
        "loss": 2.7209,
        "grad_norm": 2.738165855407715,
        "learning_rate": 0.00012662500187244366,
        "epoch": 0.4726134585289515,
        "step": 6342
    },
    {
        "loss": 2.1514,
        "grad_norm": 3.691955327987671,
        "learning_rate": 0.000126557169534709,
        "epoch": 0.47268797973023324,
        "step": 6343
    },
    {
        "loss": 2.4845,
        "grad_norm": 2.6997289657592773,
        "learning_rate": 0.0001264893240476501,
        "epoch": 0.472762500931515,
        "step": 6344
    },
    {
        "loss": 2.5422,
        "grad_norm": 2.9583873748779297,
        "learning_rate": 0.00012642146544485948,
        "epoch": 0.47283702213279677,
        "step": 6345
    },
    {
        "loss": 2.6522,
        "grad_norm": 3.5315449237823486,
        "learning_rate": 0.0001263535937599363,
        "epoch": 0.47291154333407853,
        "step": 6346
    },
    {
        "loss": 2.7151,
        "grad_norm": 3.205967664718628,
        "learning_rate": 0.0001262857090264859,
        "epoch": 0.4729860645353603,
        "step": 6347
    },
    {
        "loss": 2.3505,
        "grad_norm": 3.392674684524536,
        "learning_rate": 0.00012621781127812018,
        "epoch": 0.47306058573664206,
        "step": 6348
    },
    {
        "loss": 2.0557,
        "grad_norm": 5.0482258796691895,
        "learning_rate": 0.00012614990054845765,
        "epoch": 0.4731351069379238,
        "step": 6349
    },
    {
        "loss": 2.5721,
        "grad_norm": 2.420414686203003,
        "learning_rate": 0.0001260819768711231,
        "epoch": 0.4732096281392056,
        "step": 6350
    },
    {
        "loss": 2.3822,
        "grad_norm": 2.8160438537597656,
        "learning_rate": 0.0001260140402797478,
        "epoch": 0.47328414934048735,
        "step": 6351
    },
    {
        "loss": 2.9801,
        "grad_norm": 3.1229565143585205,
        "learning_rate": 0.0001259460908079694,
        "epoch": 0.4733586705417691,
        "step": 6352
    },
    {
        "loss": 2.7109,
        "grad_norm": 2.2574541568756104,
        "learning_rate": 0.0001258781284894319,
        "epoch": 0.4734331917430509,
        "step": 6353
    },
    {
        "loss": 2.7317,
        "grad_norm": 3.5987672805786133,
        "learning_rate": 0.00012581015335778555,
        "epoch": 0.47350771294433264,
        "step": 6354
    },
    {
        "loss": 1.3552,
        "grad_norm": 4.470601558685303,
        "learning_rate": 0.00012574216544668714,
        "epoch": 0.4735822341456144,
        "step": 6355
    },
    {
        "loss": 2.5558,
        "grad_norm": 3.2242839336395264,
        "learning_rate": 0.00012567416478979982,
        "epoch": 0.4736567553468962,
        "step": 6356
    },
    {
        "loss": 2.1753,
        "grad_norm": 3.9543979167938232,
        "learning_rate": 0.00012560615142079287,
        "epoch": 0.473731276548178,
        "step": 6357
    },
    {
        "loss": 2.433,
        "grad_norm": 3.1701014041900635,
        "learning_rate": 0.00012553812537334177,
        "epoch": 0.47380579774945975,
        "step": 6358
    },
    {
        "loss": 2.4416,
        "grad_norm": 2.262723445892334,
        "learning_rate": 0.00012547008668112864,
        "epoch": 0.4738803189507415,
        "step": 6359
    },
    {
        "loss": 2.3972,
        "grad_norm": 3.454472303390503,
        "learning_rate": 0.00012540203537784157,
        "epoch": 0.47395484015202327,
        "step": 6360
    },
    {
        "loss": 2.6724,
        "grad_norm": 2.2252209186553955,
        "learning_rate": 0.00012533397149717512,
        "epoch": 0.47402936135330503,
        "step": 6361
    },
    {
        "loss": 2.2182,
        "grad_norm": 3.243008613586426,
        "learning_rate": 0.00012526589507282977,
        "epoch": 0.4741038825545868,
        "step": 6362
    },
    {
        "loss": 1.9676,
        "grad_norm": 3.6440629959106445,
        "learning_rate": 0.00012519780613851254,
        "epoch": 0.47417840375586856,
        "step": 6363
    },
    {
        "loss": 2.8032,
        "grad_norm": 3.710193634033203,
        "learning_rate": 0.00012512970472793638,
        "epoch": 0.4742529249571503,
        "step": 6364
    },
    {
        "loss": 2.48,
        "grad_norm": 4.081141948699951,
        "learning_rate": 0.00012506159087482055,
        "epoch": 0.4743274461584321,
        "step": 6365
    },
    {
        "loss": 2.2426,
        "grad_norm": 2.181936264038086,
        "learning_rate": 0.00012499346461289053,
        "epoch": 0.47440196735971385,
        "step": 6366
    },
    {
        "loss": 2.5388,
        "grad_norm": 3.0526678562164307,
        "learning_rate": 0.0001249253259758778,
        "epoch": 0.4744764885609956,
        "step": 6367
    },
    {
        "loss": 1.773,
        "grad_norm": 2.5542702674865723,
        "learning_rate": 0.00012485717499751998,
        "epoch": 0.4745510097622774,
        "step": 6368
    },
    {
        "loss": 2.2117,
        "grad_norm": 2.6490986347198486,
        "learning_rate": 0.00012478901171156088,
        "epoch": 0.47462553096355914,
        "step": 6369
    },
    {
        "loss": 1.5956,
        "grad_norm": 3.3503286838531494,
        "learning_rate": 0.0001247208361517504,
        "epoch": 0.4747000521648409,
        "step": 6370
    },
    {
        "loss": 2.8706,
        "grad_norm": 2.8330795764923096,
        "learning_rate": 0.00012465264835184454,
        "epoch": 0.47477457336612267,
        "step": 6371
    },
    {
        "loss": 3.0419,
        "grad_norm": 2.627028703689575,
        "learning_rate": 0.00012458444834560523,
        "epoch": 0.47484909456740443,
        "step": 6372
    },
    {
        "loss": 2.6372,
        "grad_norm": 1.7898062467575073,
        "learning_rate": 0.0001245162361668006,
        "epoch": 0.4749236157686862,
        "step": 6373
    },
    {
        "loss": 2.2742,
        "grad_norm": 3.426356554031372,
        "learning_rate": 0.00012444801184920463,
        "epoch": 0.47499813696996795,
        "step": 6374
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.544968366622925,
        "learning_rate": 0.00012437977542659754,
        "epoch": 0.4750726581712497,
        "step": 6375
    },
    {
        "loss": 1.8537,
        "grad_norm": 3.9223554134368896,
        "learning_rate": 0.00012431152693276532,
        "epoch": 0.4751471793725315,
        "step": 6376
    },
    {
        "loss": 2.5163,
        "grad_norm": 2.07521653175354,
        "learning_rate": 0.00012424326640150007,
        "epoch": 0.47522170057381324,
        "step": 6377
    },
    {
        "loss": 2.7145,
        "grad_norm": 2.9358890056610107,
        "learning_rate": 0.00012417499386659978,
        "epoch": 0.475296221775095,
        "step": 6378
    },
    {
        "loss": 2.8212,
        "grad_norm": 2.6615703105926514,
        "learning_rate": 0.00012410670936186845,
        "epoch": 0.47537074297637677,
        "step": 6379
    },
    {
        "loss": 2.2356,
        "grad_norm": 3.1260111331939697,
        "learning_rate": 0.000124038412921116,
        "epoch": 0.47544526417765853,
        "step": 6380
    },
    {
        "loss": 2.5838,
        "grad_norm": 2.7444515228271484,
        "learning_rate": 0.0001239701045781582,
        "epoch": 0.4755197853789403,
        "step": 6381
    },
    {
        "loss": 2.5441,
        "grad_norm": 2.3372459411621094,
        "learning_rate": 0.0001239017843668167,
        "epoch": 0.47559430658022206,
        "step": 6382
    },
    {
        "loss": 2.944,
        "grad_norm": 3.102273941040039,
        "learning_rate": 0.00012383345232091922,
        "epoch": 0.4756688277815038,
        "step": 6383
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.6381921768188477,
        "learning_rate": 0.00012376510847429903,
        "epoch": 0.4757433489827856,
        "step": 6384
    },
    {
        "loss": 2.6616,
        "grad_norm": 2.3402504920959473,
        "learning_rate": 0.00012369675286079553,
        "epoch": 0.47581787018406735,
        "step": 6385
    },
    {
        "loss": 2.8362,
        "grad_norm": 2.9042515754699707,
        "learning_rate": 0.00012362838551425372,
        "epoch": 0.4758923913853491,
        "step": 6386
    },
    {
        "loss": 2.4943,
        "grad_norm": 2.6771841049194336,
        "learning_rate": 0.00012356000646852463,
        "epoch": 0.4759669125866309,
        "step": 6387
    },
    {
        "loss": 2.4,
        "grad_norm": 2.6311700344085693,
        "learning_rate": 0.0001234916157574648,
        "epoch": 0.47604143378791264,
        "step": 6388
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.2075278759002686,
        "learning_rate": 0.00012342321341493687,
        "epoch": 0.4761159549891944,
        "step": 6389
    },
    {
        "loss": 2.0547,
        "grad_norm": 3.2517921924591064,
        "learning_rate": 0.00012335479947480897,
        "epoch": 0.47619047619047616,
        "step": 6390
    },
    {
        "loss": 2.0053,
        "grad_norm": 3.738680839538574,
        "learning_rate": 0.00012328637397095515,
        "epoch": 0.476264997391758,
        "step": 6391
    },
    {
        "loss": 2.8811,
        "grad_norm": 3.0356316566467285,
        "learning_rate": 0.00012321793693725506,
        "epoch": 0.47633951859303975,
        "step": 6392
    },
    {
        "loss": 2.4836,
        "grad_norm": 2.8085055351257324,
        "learning_rate": 0.0001231494884075942,
        "epoch": 0.4764140397943215,
        "step": 6393
    },
    {
        "loss": 2.5378,
        "grad_norm": 3.378042221069336,
        "learning_rate": 0.00012308102841586367,
        "epoch": 0.47648856099560327,
        "step": 6394
    },
    {
        "loss": 1.9576,
        "grad_norm": 4.0695953369140625,
        "learning_rate": 0.00012301255699596013,
        "epoch": 0.47656308219688504,
        "step": 6395
    },
    {
        "loss": 2.6049,
        "grad_norm": 4.227838039398193,
        "learning_rate": 0.0001229440741817861,
        "epoch": 0.4766376033981668,
        "step": 6396
    },
    {
        "loss": 2.48,
        "grad_norm": 2.4685041904449463,
        "learning_rate": 0.0001228755800072497,
        "epoch": 0.47671212459944856,
        "step": 6397
    },
    {
        "loss": 2.2164,
        "grad_norm": 2.6470303535461426,
        "learning_rate": 0.00012280707450626458,
        "epoch": 0.4767866458007303,
        "step": 6398
    },
    {
        "loss": 2.1253,
        "grad_norm": 2.582872152328491,
        "learning_rate": 0.00012273855771275017,
        "epoch": 0.4768611670020121,
        "step": 6399
    },
    {
        "loss": 2.4487,
        "grad_norm": 2.9902548789978027,
        "learning_rate": 0.0001226700296606312,
        "epoch": 0.47693568820329385,
        "step": 6400
    },
    {
        "loss": 2.201,
        "grad_norm": 4.980141639709473,
        "learning_rate": 0.00012260149038383823,
        "epoch": 0.4770102094045756,
        "step": 6401
    },
    {
        "loss": 2.5005,
        "grad_norm": 2.85123348236084,
        "learning_rate": 0.00012253293991630731,
        "epoch": 0.4770847306058574,
        "step": 6402
    },
    {
        "loss": 2.2766,
        "grad_norm": 4.3004865646362305,
        "learning_rate": 0.00012246437829198015,
        "epoch": 0.47715925180713914,
        "step": 6403
    },
    {
        "loss": 2.3707,
        "grad_norm": 3.911985397338867,
        "learning_rate": 0.00012239580554480368,
        "epoch": 0.4772337730084209,
        "step": 6404
    },
    {
        "loss": 2.2444,
        "grad_norm": 3.3895347118377686,
        "learning_rate": 0.0001223272217087305,
        "epoch": 0.47730829420970267,
        "step": 6405
    },
    {
        "loss": 2.3653,
        "grad_norm": 3.65470814704895,
        "learning_rate": 0.0001222586268177188,
        "epoch": 0.47738281541098443,
        "step": 6406
    },
    {
        "loss": 2.7771,
        "grad_norm": 3.524366617202759,
        "learning_rate": 0.00012219002090573216,
        "epoch": 0.4774573366122662,
        "step": 6407
    },
    {
        "loss": 2.2443,
        "grad_norm": 3.2620086669921875,
        "learning_rate": 0.0001221214040067396,
        "epoch": 0.47753185781354796,
        "step": 6408
    },
    {
        "loss": 2.4274,
        "grad_norm": 3.3445398807525635,
        "learning_rate": 0.0001220527761547155,
        "epoch": 0.4776063790148297,
        "step": 6409
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.4875760078430176,
        "learning_rate": 0.00012198413738363987,
        "epoch": 0.4776809002161115,
        "step": 6410
    },
    {
        "loss": 2.7561,
        "grad_norm": 1.7381609678268433,
        "learning_rate": 0.00012191548772749797,
        "epoch": 0.47775542141739324,
        "step": 6411
    },
    {
        "loss": 2.4703,
        "grad_norm": 2.110898017883301,
        "learning_rate": 0.00012184682722028049,
        "epoch": 0.477829942618675,
        "step": 6412
    },
    {
        "loss": 2.1989,
        "grad_norm": 3.290221929550171,
        "learning_rate": 0.00012177815589598355,
        "epoch": 0.47790446381995677,
        "step": 6413
    },
    {
        "loss": 2.2396,
        "grad_norm": 3.057623863220215,
        "learning_rate": 0.00012170947378860858,
        "epoch": 0.47797898502123853,
        "step": 6414
    },
    {
        "loss": 1.4917,
        "grad_norm": 3.9891719818115234,
        "learning_rate": 0.00012164078093216218,
        "epoch": 0.4780535062225203,
        "step": 6415
    },
    {
        "loss": 2.6501,
        "grad_norm": 3.6109085083007812,
        "learning_rate": 0.00012157207736065657,
        "epoch": 0.47812802742380206,
        "step": 6416
    },
    {
        "loss": 2.3797,
        "grad_norm": 3.2730371952056885,
        "learning_rate": 0.00012150336310810916,
        "epoch": 0.4782025486250838,
        "step": 6417
    },
    {
        "loss": 2.2948,
        "grad_norm": 2.015141725540161,
        "learning_rate": 0.00012143463820854258,
        "epoch": 0.4782770698263656,
        "step": 6418
    },
    {
        "loss": 1.9364,
        "grad_norm": 3.0348994731903076,
        "learning_rate": 0.0001213659026959847,
        "epoch": 0.47835159102764735,
        "step": 6419
    },
    {
        "loss": 2.3704,
        "grad_norm": 3.224601984024048,
        "learning_rate": 0.00012129715660446876,
        "epoch": 0.4784261122289291,
        "step": 6420
    },
    {
        "loss": 1.5031,
        "grad_norm": 4.521259307861328,
        "learning_rate": 0.00012122839996803327,
        "epoch": 0.4785006334302109,
        "step": 6421
    },
    {
        "loss": 2.6894,
        "grad_norm": 3.132061719894409,
        "learning_rate": 0.00012115963282072195,
        "epoch": 0.47857515463149264,
        "step": 6422
    },
    {
        "loss": 1.7733,
        "grad_norm": 3.1008219718933105,
        "learning_rate": 0.00012109085519658348,
        "epoch": 0.4786496758327744,
        "step": 6423
    },
    {
        "loss": 2.697,
        "grad_norm": 2.180867910385132,
        "learning_rate": 0.00012102206712967208,
        "epoch": 0.47872419703405616,
        "step": 6424
    },
    {
        "loss": 2.0047,
        "grad_norm": 3.364893913269043,
        "learning_rate": 0.00012095326865404686,
        "epoch": 0.47879871823533793,
        "step": 6425
    },
    {
        "loss": 2.433,
        "grad_norm": 3.363264560699463,
        "learning_rate": 0.00012088445980377221,
        "epoch": 0.4788732394366197,
        "step": 6426
    },
    {
        "loss": 2.1528,
        "grad_norm": 3.0869321823120117,
        "learning_rate": 0.00012081564061291775,
        "epoch": 0.4789477606379015,
        "step": 6427
    },
    {
        "loss": 1.411,
        "grad_norm": 3.697894334793091,
        "learning_rate": 0.00012074681111555806,
        "epoch": 0.4790222818391833,
        "step": 6428
    },
    {
        "loss": 3.1999,
        "grad_norm": 2.5263431072235107,
        "learning_rate": 0.00012067797134577276,
        "epoch": 0.47909680304046504,
        "step": 6429
    },
    {
        "loss": 1.7443,
        "grad_norm": 3.9168953895568848,
        "learning_rate": 0.0001206091213376468,
        "epoch": 0.4791713242417468,
        "step": 6430
    },
    {
        "loss": 2.1666,
        "grad_norm": 2.2676568031311035,
        "learning_rate": 0.00012054026112527002,
        "epoch": 0.47924584544302856,
        "step": 6431
    },
    {
        "loss": 2.9717,
        "grad_norm": 2.0701067447662354,
        "learning_rate": 0.00012047139074273746,
        "epoch": 0.4793203666443103,
        "step": 6432
    },
    {
        "loss": 2.8543,
        "grad_norm": 2.4905896186828613,
        "learning_rate": 0.00012040251022414899,
        "epoch": 0.4793948878455921,
        "step": 6433
    },
    {
        "loss": 2.294,
        "grad_norm": 3.0398669242858887,
        "learning_rate": 0.00012033361960360971,
        "epoch": 0.47946940904687385,
        "step": 6434
    },
    {
        "loss": 2.8749,
        "grad_norm": 1.9599007368087769,
        "learning_rate": 0.0001202647189152295,
        "epoch": 0.4795439302481556,
        "step": 6435
    },
    {
        "loss": 2.5836,
        "grad_norm": 2.573019504547119,
        "learning_rate": 0.00012019580819312348,
        "epoch": 0.4796184514494374,
        "step": 6436
    },
    {
        "loss": 2.6849,
        "grad_norm": 2.6204049587249756,
        "learning_rate": 0.00012012688747141147,
        "epoch": 0.47969297265071914,
        "step": 6437
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.538585901260376,
        "learning_rate": 0.0001200579567842185,
        "epoch": 0.4797674938520009,
        "step": 6438
    },
    {
        "loss": 2.7183,
        "grad_norm": 1.715293049812317,
        "learning_rate": 0.00011998901616567429,
        "epoch": 0.47984201505328267,
        "step": 6439
    },
    {
        "loss": 2.3123,
        "grad_norm": 3.795976400375366,
        "learning_rate": 0.00011992006564991368,
        "epoch": 0.47991653625456443,
        "step": 6440
    },
    {
        "loss": 2.1122,
        "grad_norm": 2.0149781703948975,
        "learning_rate": 0.00011985110527107635,
        "epoch": 0.4799910574558462,
        "step": 6441
    },
    {
        "loss": 2.3775,
        "grad_norm": 4.301729202270508,
        "learning_rate": 0.00011978213506330676,
        "epoch": 0.48006557865712796,
        "step": 6442
    },
    {
        "loss": 2.1971,
        "grad_norm": 3.722428798675537,
        "learning_rate": 0.00011971315506075434,
        "epoch": 0.4801400998584097,
        "step": 6443
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.6533215045928955,
        "learning_rate": 0.00011964416529757345,
        "epoch": 0.4802146210596915,
        "step": 6444
    },
    {
        "loss": 2.1541,
        "grad_norm": 2.9307687282562256,
        "learning_rate": 0.00011957516580792301,
        "epoch": 0.48028914226097325,
        "step": 6445
    },
    {
        "loss": 1.5883,
        "grad_norm": 3.4624030590057373,
        "learning_rate": 0.00011950615662596707,
        "epoch": 0.480363663462255,
        "step": 6446
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.242347478866577,
        "learning_rate": 0.00011943713778587424,
        "epoch": 0.48043818466353677,
        "step": 6447
    },
    {
        "loss": 2.1071,
        "grad_norm": 5.144193649291992,
        "learning_rate": 0.00011936810932181812,
        "epoch": 0.48051270586481853,
        "step": 6448
    },
    {
        "loss": 2.5179,
        "grad_norm": 2.87597918510437,
        "learning_rate": 0.00011929907126797681,
        "epoch": 0.4805872270661003,
        "step": 6449
    },
    {
        "loss": 2.5181,
        "grad_norm": 2.694239854812622,
        "learning_rate": 0.00011923002365853339,
        "epoch": 0.48066174826738206,
        "step": 6450
    },
    {
        "loss": 2.5077,
        "grad_norm": 1.8764890432357788,
        "learning_rate": 0.00011916096652767567,
        "epoch": 0.4807362694686638,
        "step": 6451
    },
    {
        "loss": 2.5,
        "grad_norm": 2.191537618637085,
        "learning_rate": 0.00011909189990959595,
        "epoch": 0.4808107906699456,
        "step": 6452
    },
    {
        "loss": 1.7664,
        "grad_norm": 2.8595776557922363,
        "learning_rate": 0.00011902282383849143,
        "epoch": 0.48088531187122735,
        "step": 6453
    },
    {
        "loss": 2.2285,
        "grad_norm": 2.895902633666992,
        "learning_rate": 0.00011895373834856405,
        "epoch": 0.4809598330725091,
        "step": 6454
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.3582801818847656,
        "learning_rate": 0.00011888464347402017,
        "epoch": 0.4810343542737909,
        "step": 6455
    },
    {
        "loss": 2.1349,
        "grad_norm": 2.371398448944092,
        "learning_rate": 0.0001188155392490709,
        "epoch": 0.48110887547507264,
        "step": 6456
    },
    {
        "loss": 2.7481,
        "grad_norm": 2.468886137008667,
        "learning_rate": 0.00011874642570793205,
        "epoch": 0.4811833966763544,
        "step": 6457
    },
    {
        "loss": 2.6255,
        "grad_norm": 3.4507172107696533,
        "learning_rate": 0.00011867730288482406,
        "epoch": 0.48125791787763617,
        "step": 6458
    },
    {
        "loss": 2.4565,
        "grad_norm": 3.3449862003326416,
        "learning_rate": 0.0001186081708139718,
        "epoch": 0.48133243907891793,
        "step": 6459
    },
    {
        "loss": 1.9601,
        "grad_norm": 2.4193341732025146,
        "learning_rate": 0.00011853902952960494,
        "epoch": 0.4814069602801997,
        "step": 6460
    },
    {
        "loss": 1.9369,
        "grad_norm": 6.330446720123291,
        "learning_rate": 0.00011846987906595743,
        "epoch": 0.48148148148148145,
        "step": 6461
    },
    {
        "loss": 2.4821,
        "grad_norm": 2.2507436275482178,
        "learning_rate": 0.00011840071945726803,
        "epoch": 0.4815560026827633,
        "step": 6462
    },
    {
        "loss": 2.7787,
        "grad_norm": 3.0078392028808594,
        "learning_rate": 0.00011833155073777993,
        "epoch": 0.48163052388404504,
        "step": 6463
    },
    {
        "loss": 2.6223,
        "grad_norm": 3.653017997741699,
        "learning_rate": 0.00011826237294174087,
        "epoch": 0.4817050450853268,
        "step": 6464
    },
    {
        "loss": 2.6003,
        "grad_norm": 2.0620367527008057,
        "learning_rate": 0.00011819318610340299,
        "epoch": 0.48177956628660856,
        "step": 6465
    },
    {
        "loss": 3.146,
        "grad_norm": 3.196161985397339,
        "learning_rate": 0.0001181239902570229,
        "epoch": 0.4818540874878903,
        "step": 6466
    },
    {
        "loss": 2.8932,
        "grad_norm": 3.074718713760376,
        "learning_rate": 0.0001180547854368618,
        "epoch": 0.4819286086891721,
        "step": 6467
    },
    {
        "loss": 2.8946,
        "grad_norm": 2.871689558029175,
        "learning_rate": 0.00011798557167718528,
        "epoch": 0.48200312989045385,
        "step": 6468
    },
    {
        "loss": 2.4977,
        "grad_norm": 3.1347312927246094,
        "learning_rate": 0.00011791634901226332,
        "epoch": 0.4820776510917356,
        "step": 6469
    },
    {
        "loss": 1.8211,
        "grad_norm": 3.120718479156494,
        "learning_rate": 0.00011784711747637028,
        "epoch": 0.4821521722930174,
        "step": 6470
    },
    {
        "loss": 2.4339,
        "grad_norm": 2.662536859512329,
        "learning_rate": 0.00011777787710378494,
        "epoch": 0.48222669349429914,
        "step": 6471
    },
    {
        "loss": 3.0014,
        "grad_norm": 2.959638833999634,
        "learning_rate": 0.00011770862792879057,
        "epoch": 0.4823012146955809,
        "step": 6472
    },
    {
        "loss": 2.4915,
        "grad_norm": 2.28192400932312,
        "learning_rate": 0.00011763936998567464,
        "epoch": 0.48237573589686267,
        "step": 6473
    },
    {
        "loss": 1.7626,
        "grad_norm": 2.950230360031128,
        "learning_rate": 0.00011757010330872915,
        "epoch": 0.48245025709814443,
        "step": 6474
    },
    {
        "loss": 2.3213,
        "grad_norm": 2.510664224624634,
        "learning_rate": 0.0001175008279322502,
        "epoch": 0.4825247782994262,
        "step": 6475
    },
    {
        "loss": 2.7219,
        "grad_norm": 2.24056077003479,
        "learning_rate": 0.0001174315438905382,
        "epoch": 0.48259929950070796,
        "step": 6476
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.5693752765655518,
        "learning_rate": 0.00011736225121789805,
        "epoch": 0.4826738207019897,
        "step": 6477
    },
    {
        "loss": 1.7781,
        "grad_norm": 5.103554725646973,
        "learning_rate": 0.00011729294994863893,
        "epoch": 0.4827483419032715,
        "step": 6478
    },
    {
        "loss": 2.476,
        "grad_norm": 3.2313616275787354,
        "learning_rate": 0.00011722364011707401,
        "epoch": 0.48282286310455325,
        "step": 6479
    },
    {
        "loss": 2.0827,
        "grad_norm": 2.3654818534851074,
        "learning_rate": 0.00011715432175752084,
        "epoch": 0.482897384305835,
        "step": 6480
    },
    {
        "loss": 1.7313,
        "grad_norm": 3.6604514122009277,
        "learning_rate": 0.00011708499490430127,
        "epoch": 0.48297190550711677,
        "step": 6481
    },
    {
        "loss": 1.9343,
        "grad_norm": 3.9296722412109375,
        "learning_rate": 0.00011701565959174132,
        "epoch": 0.48304642670839854,
        "step": 6482
    },
    {
        "loss": 2.559,
        "grad_norm": 1.819608449935913,
        "learning_rate": 0.0001169463158541712,
        "epoch": 0.4831209479096803,
        "step": 6483
    },
    {
        "loss": 1.564,
        "grad_norm": 5.447265148162842,
        "learning_rate": 0.00011687696372592514,
        "epoch": 0.48319546911096206,
        "step": 6484
    },
    {
        "loss": 2.1559,
        "grad_norm": 3.423261880874634,
        "learning_rate": 0.00011680760324134182,
        "epoch": 0.4832699903122438,
        "step": 6485
    },
    {
        "loss": 2.0299,
        "grad_norm": 3.6593332290649414,
        "learning_rate": 0.00011673823443476371,
        "epoch": 0.4833445115135256,
        "step": 6486
    },
    {
        "loss": 2.5974,
        "grad_norm": 3.631525754928589,
        "learning_rate": 0.00011666885734053764,
        "epoch": 0.48341903271480735,
        "step": 6487
    },
    {
        "loss": 2.2013,
        "grad_norm": 3.5571932792663574,
        "learning_rate": 0.00011659947199301458,
        "epoch": 0.4834935539160891,
        "step": 6488
    },
    {
        "loss": 1.6453,
        "grad_norm": 3.8299126625061035,
        "learning_rate": 0.00011653007842654942,
        "epoch": 0.4835680751173709,
        "step": 6489
    },
    {
        "loss": 2.7446,
        "grad_norm": 2.247147798538208,
        "learning_rate": 0.0001164606766755011,
        "epoch": 0.48364259631865264,
        "step": 6490
    },
    {
        "loss": 1.9573,
        "grad_norm": 3.0958805084228516,
        "learning_rate": 0.00011639126677423275,
        "epoch": 0.4837171175199344,
        "step": 6491
    },
    {
        "loss": 2.8387,
        "grad_norm": 2.182913303375244,
        "learning_rate": 0.00011632184875711154,
        "epoch": 0.48379163872121617,
        "step": 6492
    },
    {
        "loss": 2.5263,
        "grad_norm": 2.9795610904693604,
        "learning_rate": 0.00011625242265850864,
        "epoch": 0.48386615992249793,
        "step": 6493
    },
    {
        "loss": 1.4794,
        "grad_norm": 3.784775972366333,
        "learning_rate": 0.00011618298851279903,
        "epoch": 0.4839406811237797,
        "step": 6494
    },
    {
        "loss": 2.5169,
        "grad_norm": 1.9769301414489746,
        "learning_rate": 0.00011611354635436198,
        "epoch": 0.48401520232506146,
        "step": 6495
    },
    {
        "loss": 2.5705,
        "grad_norm": 3.16294527053833,
        "learning_rate": 0.00011604409621758045,
        "epoch": 0.4840897235263432,
        "step": 6496
    },
    {
        "loss": 2.1607,
        "grad_norm": 3.220367431640625,
        "learning_rate": 0.00011597463813684157,
        "epoch": 0.484164244727625,
        "step": 6497
    },
    {
        "loss": 2.2763,
        "grad_norm": 3.0842418670654297,
        "learning_rate": 0.0001159051721465363,
        "epoch": 0.4842387659289068,
        "step": 6498
    },
    {
        "loss": 2.4823,
        "grad_norm": 2.6135666370391846,
        "learning_rate": 0.00011583569828105954,
        "epoch": 0.48431328713018856,
        "step": 6499
    },
    {
        "loss": 2.5607,
        "grad_norm": 5.567383289337158,
        "learning_rate": 0.00011576621657480995,
        "epoch": 0.4843878083314703,
        "step": 6500
    },
    {
        "loss": 1.5344,
        "grad_norm": 3.117810010910034,
        "learning_rate": 0.0001156967270621903,
        "epoch": 0.4844623295327521,
        "step": 6501
    },
    {
        "loss": 2.803,
        "grad_norm": 2.147683620452881,
        "learning_rate": 0.00011562722977760719,
        "epoch": 0.48453685073403385,
        "step": 6502
    },
    {
        "loss": 1.8075,
        "grad_norm": 4.368869781494141,
        "learning_rate": 0.00011555772475547084,
        "epoch": 0.4846113719353156,
        "step": 6503
    },
    {
        "loss": 2.0559,
        "grad_norm": 3.9678096771240234,
        "learning_rate": 0.00011548821203019553,
        "epoch": 0.4846858931365974,
        "step": 6504
    },
    {
        "loss": 2.5934,
        "grad_norm": 2.390078067779541,
        "learning_rate": 0.0001154186916361994,
        "epoch": 0.48476041433787914,
        "step": 6505
    },
    {
        "loss": 2.4754,
        "grad_norm": 2.320448637008667,
        "learning_rate": 0.0001153491636079041,
        "epoch": 0.4848349355391609,
        "step": 6506
    },
    {
        "loss": 2.4699,
        "grad_norm": 3.1266984939575195,
        "learning_rate": 0.00011527962797973537,
        "epoch": 0.48490945674044267,
        "step": 6507
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.131625175476074,
        "learning_rate": 0.00011521008478612248,
        "epoch": 0.48498397794172443,
        "step": 6508
    },
    {
        "loss": 2.8294,
        "grad_norm": 2.3716554641723633,
        "learning_rate": 0.00011514053406149861,
        "epoch": 0.4850584991430062,
        "step": 6509
    },
    {
        "loss": 2.6977,
        "grad_norm": 2.5555102825164795,
        "learning_rate": 0.00011507097584030049,
        "epoch": 0.48513302034428796,
        "step": 6510
    },
    {
        "loss": 2.589,
        "grad_norm": 4.1536407470703125,
        "learning_rate": 0.00011500141015696876,
        "epoch": 0.4852075415455697,
        "step": 6511
    },
    {
        "loss": 2.8683,
        "grad_norm": 2.901613235473633,
        "learning_rate": 0.00011493183704594776,
        "epoch": 0.4852820627468515,
        "step": 6512
    },
    {
        "loss": 2.4823,
        "grad_norm": 2.6850106716156006,
        "learning_rate": 0.0001148622565416852,
        "epoch": 0.48535658394813325,
        "step": 6513
    },
    {
        "loss": 2.1992,
        "grad_norm": 2.591961622238159,
        "learning_rate": 0.0001147926686786328,
        "epoch": 0.485431105149415,
        "step": 6514
    },
    {
        "loss": 2.7379,
        "grad_norm": 2.905062198638916,
        "learning_rate": 0.00011472307349124585,
        "epoch": 0.4855056263506968,
        "step": 6515
    },
    {
        "loss": 1.7607,
        "grad_norm": 4.032814025878906,
        "learning_rate": 0.00011465347101398313,
        "epoch": 0.48558014755197854,
        "step": 6516
    },
    {
        "loss": 2.6306,
        "grad_norm": 3.1245269775390625,
        "learning_rate": 0.00011458386128130703,
        "epoch": 0.4856546687532603,
        "step": 6517
    },
    {
        "loss": 2.6642,
        "grad_norm": 3.7919721603393555,
        "learning_rate": 0.00011451424432768365,
        "epoch": 0.48572918995454206,
        "step": 6518
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.9862489700317383,
        "learning_rate": 0.00011444462018758276,
        "epoch": 0.4858037111558238,
        "step": 6519
    },
    {
        "loss": 1.7443,
        "grad_norm": 3.856600761413574,
        "learning_rate": 0.00011437498889547736,
        "epoch": 0.4858782323571056,
        "step": 6520
    },
    {
        "loss": 2.4159,
        "grad_norm": 3.439537525177002,
        "learning_rate": 0.00011430535048584432,
        "epoch": 0.48595275355838735,
        "step": 6521
    },
    {
        "loss": 2.5029,
        "grad_norm": 2.6421444416046143,
        "learning_rate": 0.00011423570499316379,
        "epoch": 0.4860272747596691,
        "step": 6522
    },
    {
        "loss": 2.006,
        "grad_norm": 3.228769302368164,
        "learning_rate": 0.0001141660524519196,
        "epoch": 0.4861017959609509,
        "step": 6523
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.1522791385650635,
        "learning_rate": 0.00011409639289659896,
        "epoch": 0.48617631716223264,
        "step": 6524
    },
    {
        "loss": 0.957,
        "grad_norm": 3.797346830368042,
        "learning_rate": 0.00011402672636169271,
        "epoch": 0.4862508383635144,
        "step": 6525
    },
    {
        "loss": 2.4458,
        "grad_norm": 2.763042688369751,
        "learning_rate": 0.00011395705288169496,
        "epoch": 0.48632535956479617,
        "step": 6526
    },
    {
        "loss": 2.0614,
        "grad_norm": 2.4358325004577637,
        "learning_rate": 0.00011388737249110326,
        "epoch": 0.48639988076607793,
        "step": 6527
    },
    {
        "loss": 2.1032,
        "grad_norm": 2.4898934364318848,
        "learning_rate": 0.0001138176852244187,
        "epoch": 0.4864744019673597,
        "step": 6528
    },
    {
        "loss": 2.2483,
        "grad_norm": 3.680208683013916,
        "learning_rate": 0.0001137479911161458,
        "epoch": 0.48654892316864146,
        "step": 6529
    },
    {
        "loss": 2.7632,
        "grad_norm": 4.497348785400391,
        "learning_rate": 0.00011367829020079237,
        "epoch": 0.4866234443699232,
        "step": 6530
    },
    {
        "loss": 2.7971,
        "grad_norm": 2.4468677043914795,
        "learning_rate": 0.0001136085825128695,
        "epoch": 0.486697965571205,
        "step": 6531
    },
    {
        "loss": 2.2807,
        "grad_norm": 1.9113966226577759,
        "learning_rate": 0.00011353886808689182,
        "epoch": 0.48677248677248675,
        "step": 6532
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.3964178562164307,
        "learning_rate": 0.00011346914695737725,
        "epoch": 0.48684700797376856,
        "step": 6533
    },
    {
        "loss": 2.8305,
        "grad_norm": 5.437706470489502,
        "learning_rate": 0.00011339941915884697,
        "epoch": 0.4869215291750503,
        "step": 6534
    },
    {
        "loss": 2.2681,
        "grad_norm": 4.375973224639893,
        "learning_rate": 0.00011332968472582561,
        "epoch": 0.4869960503763321,
        "step": 6535
    },
    {
        "loss": 2.4315,
        "grad_norm": 3.0230391025543213,
        "learning_rate": 0.00011325994369284088,
        "epoch": 0.48707057157761385,
        "step": 6536
    },
    {
        "loss": 2.909,
        "grad_norm": 3.2078299522399902,
        "learning_rate": 0.00011319019609442376,
        "epoch": 0.4871450927788956,
        "step": 6537
    },
    {
        "loss": 2.1116,
        "grad_norm": 3.166492223739624,
        "learning_rate": 0.00011312044196510866,
        "epoch": 0.4872196139801774,
        "step": 6538
    },
    {
        "loss": 2.2401,
        "grad_norm": 2.9295759201049805,
        "learning_rate": 0.00011305068133943323,
        "epoch": 0.48729413518145914,
        "step": 6539
    },
    {
        "loss": 2.63,
        "grad_norm": 3.3968546390533447,
        "learning_rate": 0.0001129809142519381,
        "epoch": 0.4873686563827409,
        "step": 6540
    },
    {
        "loss": 1.9935,
        "grad_norm": 3.5840187072753906,
        "learning_rate": 0.00011291114073716724,
        "epoch": 0.48744317758402267,
        "step": 6541
    },
    {
        "loss": 2.3832,
        "grad_norm": 2.6891636848449707,
        "learning_rate": 0.00011284136082966785,
        "epoch": 0.48751769878530443,
        "step": 6542
    },
    {
        "loss": 2.4509,
        "grad_norm": 2.6622302532196045,
        "learning_rate": 0.00011277157456399021,
        "epoch": 0.4875922199865862,
        "step": 6543
    },
    {
        "loss": 2.2548,
        "grad_norm": 3.1168136596679688,
        "learning_rate": 0.00011270178197468783,
        "epoch": 0.48766674118786796,
        "step": 6544
    },
    {
        "loss": 1.6506,
        "grad_norm": 3.7905874252319336,
        "learning_rate": 0.00011263198309631735,
        "epoch": 0.4877412623891497,
        "step": 6545
    },
    {
        "loss": 2.6331,
        "grad_norm": 3.844480514526367,
        "learning_rate": 0.00011256217796343844,
        "epoch": 0.4878157835904315,
        "step": 6546
    },
    {
        "loss": 2.638,
        "grad_norm": 2.27325177192688,
        "learning_rate": 0.0001124923666106138,
        "epoch": 0.48789030479171325,
        "step": 6547
    },
    {
        "loss": 1.9982,
        "grad_norm": 3.0520412921905518,
        "learning_rate": 0.0001124225490724094,
        "epoch": 0.487964825992995,
        "step": 6548
    },
    {
        "loss": 2.7539,
        "grad_norm": 1.6697452068328857,
        "learning_rate": 0.00011235272538339426,
        "epoch": 0.4880393471942768,
        "step": 6549
    },
    {
        "loss": 0.706,
        "grad_norm": 4.3915839195251465,
        "learning_rate": 0.00011228289557814034,
        "epoch": 0.48811386839555854,
        "step": 6550
    },
    {
        "loss": 2.3393,
        "grad_norm": 3.1735968589782715,
        "learning_rate": 0.00011221305969122254,
        "epoch": 0.4881883895968403,
        "step": 6551
    },
    {
        "loss": 2.919,
        "grad_norm": 2.0678603649139404,
        "learning_rate": 0.00011214321775721898,
        "epoch": 0.48826291079812206,
        "step": 6552
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.7185070514678955,
        "learning_rate": 0.00011207336981071071,
        "epoch": 0.4883374319994038,
        "step": 6553
    },
    {
        "loss": 2.4513,
        "grad_norm": 2.0201239585876465,
        "learning_rate": 0.00011200351588628178,
        "epoch": 0.4884119532006856,
        "step": 6554
    },
    {
        "loss": 2.9663,
        "grad_norm": 1.9448336362838745,
        "learning_rate": 0.00011193365601851902,
        "epoch": 0.48848647440196735,
        "step": 6555
    },
    {
        "loss": 2.8252,
        "grad_norm": 3.938063383102417,
        "learning_rate": 0.00011186379024201248,
        "epoch": 0.4885609956032491,
        "step": 6556
    },
    {
        "loss": 1.9916,
        "grad_norm": 3.1900136470794678,
        "learning_rate": 0.00011179391859135488,
        "epoch": 0.4886355168045309,
        "step": 6557
    },
    {
        "loss": 2.3187,
        "grad_norm": 3.2894773483276367,
        "learning_rate": 0.00011172404110114201,
        "epoch": 0.48871003800581264,
        "step": 6558
    },
    {
        "loss": 2.0474,
        "grad_norm": 1.3786029815673828,
        "learning_rate": 0.00011165415780597253,
        "epoch": 0.4887845592070944,
        "step": 6559
    },
    {
        "loss": 2.3673,
        "grad_norm": 7.501219272613525,
        "learning_rate": 0.00011158426874044794,
        "epoch": 0.48885908040837617,
        "step": 6560
    },
    {
        "loss": 2.4246,
        "grad_norm": 3.281792640686035,
        "learning_rate": 0.00011151437393917254,
        "epoch": 0.48893360160965793,
        "step": 6561
    },
    {
        "loss": 2.4386,
        "grad_norm": 3.109529972076416,
        "learning_rate": 0.00011144447343675354,
        "epoch": 0.4890081228109397,
        "step": 6562
    },
    {
        "loss": 2.4708,
        "grad_norm": 1.5866520404815674,
        "learning_rate": 0.00011137456726780111,
        "epoch": 0.48908264401222146,
        "step": 6563
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.8330161571502686,
        "learning_rate": 0.00011130465546692788,
        "epoch": 0.4891571652135032,
        "step": 6564
    },
    {
        "loss": 2.3116,
        "grad_norm": 3.6462481021881104,
        "learning_rate": 0.0001112347380687496,
        "epoch": 0.489231686414785,
        "step": 6565
    },
    {
        "loss": 1.5824,
        "grad_norm": 3.123894453048706,
        "learning_rate": 0.00011116481510788471,
        "epoch": 0.48930620761606675,
        "step": 6566
    },
    {
        "loss": 2.3134,
        "grad_norm": 3.4811580181121826,
        "learning_rate": 0.00011109488661895418,
        "epoch": 0.4893807288173485,
        "step": 6567
    },
    {
        "loss": 2.6244,
        "grad_norm": 4.089360237121582,
        "learning_rate": 0.00011102495263658209,
        "epoch": 0.48945525001863033,
        "step": 6568
    },
    {
        "loss": 1.976,
        "grad_norm": 2.2656288146972656,
        "learning_rate": 0.00011095501319539487,
        "epoch": 0.4895297712199121,
        "step": 6569
    },
    {
        "loss": 1.8628,
        "grad_norm": 4.5673065185546875,
        "learning_rate": 0.00011088506833002197,
        "epoch": 0.48960429242119385,
        "step": 6570
    },
    {
        "loss": 2.5116,
        "grad_norm": 2.0443267822265625,
        "learning_rate": 0.00011081511807509526,
        "epoch": 0.4896788136224756,
        "step": 6571
    },
    {
        "loss": 2.24,
        "grad_norm": 2.401270627975464,
        "learning_rate": 0.00011074516246524946,
        "epoch": 0.4897533348237574,
        "step": 6572
    },
    {
        "loss": 1.7867,
        "grad_norm": 2.784287452697754,
        "learning_rate": 0.00011067520153512199,
        "epoch": 0.48982785602503914,
        "step": 6573
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.946033000946045,
        "learning_rate": 0.00011060523531935259,
        "epoch": 0.4899023772263209,
        "step": 6574
    },
    {
        "loss": 2.4575,
        "grad_norm": 2.642294406890869,
        "learning_rate": 0.00011053526385258393,
        "epoch": 0.48997689842760267,
        "step": 6575
    },
    {
        "loss": 2.6405,
        "grad_norm": 3.7565746307373047,
        "learning_rate": 0.00011046528716946125,
        "epoch": 0.49005141962888443,
        "step": 6576
    },
    {
        "loss": 2.4788,
        "grad_norm": 3.3325045108795166,
        "learning_rate": 0.00011039530530463222,
        "epoch": 0.4901259408301662,
        "step": 6577
    },
    {
        "loss": 1.8418,
        "grad_norm": 3.2901692390441895,
        "learning_rate": 0.00011032531829274708,
        "epoch": 0.49020046203144796,
        "step": 6578
    },
    {
        "loss": 2.1459,
        "grad_norm": 2.805799961090088,
        "learning_rate": 0.00011025532616845875,
        "epoch": 0.4902749832327297,
        "step": 6579
    },
    {
        "loss": 2.0176,
        "grad_norm": 3.319380521774292,
        "learning_rate": 0.00011018532896642274,
        "epoch": 0.4903495044340115,
        "step": 6580
    },
    {
        "loss": 2.8186,
        "grad_norm": 2.8857412338256836,
        "learning_rate": 0.00011011532672129676,
        "epoch": 0.49042402563529325,
        "step": 6581
    },
    {
        "loss": 1.8104,
        "grad_norm": 4.215373992919922,
        "learning_rate": 0.00011004531946774137,
        "epoch": 0.490498546836575,
        "step": 6582
    },
    {
        "loss": 2.4003,
        "grad_norm": 3.2458577156066895,
        "learning_rate": 0.00010997530724041933,
        "epoch": 0.4905730680378568,
        "step": 6583
    },
    {
        "loss": 2.5876,
        "grad_norm": 1.835762858390808,
        "learning_rate": 0.00010990529007399606,
        "epoch": 0.49064758923913854,
        "step": 6584
    },
    {
        "loss": 2.2649,
        "grad_norm": 2.563889265060425,
        "learning_rate": 0.00010983526800313933,
        "epoch": 0.4907221104404203,
        "step": 6585
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.7665977478027344,
        "learning_rate": 0.00010976524106251942,
        "epoch": 0.49079663164170206,
        "step": 6586
    },
    {
        "loss": 2.6001,
        "grad_norm": 2.5650229454040527,
        "learning_rate": 0.00010969520928680894,
        "epoch": 0.4908711528429838,
        "step": 6587
    },
    {
        "loss": 2.6432,
        "grad_norm": 2.0004303455352783,
        "learning_rate": 0.0001096251727106828,
        "epoch": 0.4909456740442656,
        "step": 6588
    },
    {
        "loss": 2.7909,
        "grad_norm": 4.738502025604248,
        "learning_rate": 0.00010955513136881847,
        "epoch": 0.49102019524554735,
        "step": 6589
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.220508337020874,
        "learning_rate": 0.00010948508529589581,
        "epoch": 0.4910947164468291,
        "step": 6590
    },
    {
        "loss": 2.217,
        "grad_norm": 2.273890972137451,
        "learning_rate": 0.0001094150345265968,
        "epoch": 0.4911692376481109,
        "step": 6591
    },
    {
        "loss": 2.7113,
        "grad_norm": 2.35638165473938,
        "learning_rate": 0.00010934497909560594,
        "epoch": 0.49124375884939264,
        "step": 6592
    },
    {
        "loss": 0.8615,
        "grad_norm": 3.7133524417877197,
        "learning_rate": 0.00010927491903760989,
        "epoch": 0.4913182800506744,
        "step": 6593
    },
    {
        "loss": 2.932,
        "grad_norm": 2.840075731277466,
        "learning_rate": 0.00010920485438729773,
        "epoch": 0.49139280125195617,
        "step": 6594
    },
    {
        "loss": 1.7238,
        "grad_norm": 4.299083709716797,
        "learning_rate": 0.00010913478517936075,
        "epoch": 0.49146732245323793,
        "step": 6595
    },
    {
        "loss": 3.004,
        "grad_norm": 3.9145431518554688,
        "learning_rate": 0.00010906471144849262,
        "epoch": 0.4915418436545197,
        "step": 6596
    },
    {
        "loss": 2.2791,
        "grad_norm": 3.100522994995117,
        "learning_rate": 0.00010899463322938902,
        "epoch": 0.49161636485580146,
        "step": 6597
    },
    {
        "loss": 2.6156,
        "grad_norm": 2.560878276824951,
        "learning_rate": 0.00010892455055674791,
        "epoch": 0.4916908860570832,
        "step": 6598
    },
    {
        "loss": 1.7741,
        "grad_norm": 1.9991337060928345,
        "learning_rate": 0.00010885446346526962,
        "epoch": 0.491765407258365,
        "step": 6599
    },
    {
        "loss": 2.9845,
        "grad_norm": 2.085836887359619,
        "learning_rate": 0.00010878437198965659,
        "epoch": 0.49183992845964675,
        "step": 6600
    },
    {
        "loss": 2.3162,
        "grad_norm": 3.4670073986053467,
        "learning_rate": 0.00010871427616461334,
        "epoch": 0.4919144496609285,
        "step": 6601
    },
    {
        "loss": 2.1468,
        "grad_norm": 3.147343635559082,
        "learning_rate": 0.0001086441760248466,
        "epoch": 0.4919889708622103,
        "step": 6602
    },
    {
        "loss": 2.0999,
        "grad_norm": 2.217756509780884,
        "learning_rate": 0.00010857407160506523,
        "epoch": 0.49206349206349204,
        "step": 6603
    },
    {
        "loss": 2.4005,
        "grad_norm": 2.7731170654296875,
        "learning_rate": 0.00010850396293998029,
        "epoch": 0.49213801326477385,
        "step": 6604
    },
    {
        "loss": 2.0865,
        "grad_norm": 4.292611122131348,
        "learning_rate": 0.00010843385006430482,
        "epoch": 0.4922125344660556,
        "step": 6605
    },
    {
        "loss": 2.5809,
        "grad_norm": 4.2081475257873535,
        "learning_rate": 0.00010836373301275408,
        "epoch": 0.4922870556673374,
        "step": 6606
    },
    {
        "loss": 2.4213,
        "grad_norm": 3.569516658782959,
        "learning_rate": 0.00010829361182004531,
        "epoch": 0.49236157686861914,
        "step": 6607
    },
    {
        "loss": 2.9434,
        "grad_norm": 2.7333223819732666,
        "learning_rate": 0.00010822348652089769,
        "epoch": 0.4924360980699009,
        "step": 6608
    },
    {
        "loss": 2.698,
        "grad_norm": 3.6156628131866455,
        "learning_rate": 0.0001081533571500326,
        "epoch": 0.49251061927118267,
        "step": 6609
    },
    {
        "loss": 2.7375,
        "grad_norm": 1.990503191947937,
        "learning_rate": 0.0001080832237421735,
        "epoch": 0.49258514047246443,
        "step": 6610
    },
    {
        "loss": 2.4795,
        "grad_norm": 2.545210123062134,
        "learning_rate": 0.00010801308633204565,
        "epoch": 0.4926596616737462,
        "step": 6611
    },
    {
        "loss": 2.6611,
        "grad_norm": 2.124807834625244,
        "learning_rate": 0.0001079429449543763,
        "epoch": 0.49273418287502796,
        "step": 6612
    },
    {
        "loss": 2.9766,
        "grad_norm": 2.7430078983306885,
        "learning_rate": 0.00010787279964389483,
        "epoch": 0.4928087040763097,
        "step": 6613
    },
    {
        "loss": 1.7491,
        "grad_norm": 3.205873489379883,
        "learning_rate": 0.00010780265043533249,
        "epoch": 0.4928832252775915,
        "step": 6614
    },
    {
        "loss": 2.5794,
        "grad_norm": 3.5151519775390625,
        "learning_rate": 0.00010773249736342245,
        "epoch": 0.49295774647887325,
        "step": 6615
    },
    {
        "loss": 2.7328,
        "grad_norm": 2.6943275928497314,
        "learning_rate": 0.00010766234046289973,
        "epoch": 0.493032267680155,
        "step": 6616
    },
    {
        "loss": 1.9756,
        "grad_norm": 4.06147575378418,
        "learning_rate": 0.00010759217976850142,
        "epoch": 0.4931067888814368,
        "step": 6617
    },
    {
        "loss": 2.4695,
        "grad_norm": 2.614675998687744,
        "learning_rate": 0.00010752201531496626,
        "epoch": 0.49318131008271854,
        "step": 6618
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.7336678504943848,
        "learning_rate": 0.00010745184713703503,
        "epoch": 0.4932558312840003,
        "step": 6619
    },
    {
        "loss": 2.7884,
        "grad_norm": 1.6620948314666748,
        "learning_rate": 0.00010738167526945031,
        "epoch": 0.49333035248528206,
        "step": 6620
    },
    {
        "loss": 1.5736,
        "grad_norm": 1.9040923118591309,
        "learning_rate": 0.0001073114997469565,
        "epoch": 0.4934048736865638,
        "step": 6621
    },
    {
        "loss": 2.4913,
        "grad_norm": 4.081185817718506,
        "learning_rate": 0.00010724132060429966,
        "epoch": 0.4934793948878456,
        "step": 6622
    },
    {
        "loss": 1.9939,
        "grad_norm": 3.13032865524292,
        "learning_rate": 0.00010717113787622789,
        "epoch": 0.49355391608912735,
        "step": 6623
    },
    {
        "loss": 2.9202,
        "grad_norm": 2.061056613922119,
        "learning_rate": 0.00010710095159749098,
        "epoch": 0.4936284372904091,
        "step": 6624
    },
    {
        "loss": 2.5236,
        "grad_norm": 2.7998015880584717,
        "learning_rate": 0.00010703076180284042,
        "epoch": 0.4937029584916909,
        "step": 6625
    },
    {
        "loss": 1.6145,
        "grad_norm": 4.100260257720947,
        "learning_rate": 0.00010696056852702944,
        "epoch": 0.49377747969297264,
        "step": 6626
    },
    {
        "loss": 2.4489,
        "grad_norm": 3.575733184814453,
        "learning_rate": 0.00010689037180481314,
        "epoch": 0.4938520008942544,
        "step": 6627
    },
    {
        "loss": 1.9925,
        "grad_norm": 3.0274150371551514,
        "learning_rate": 0.00010682017167094807,
        "epoch": 0.49392652209553617,
        "step": 6628
    },
    {
        "loss": 2.7873,
        "grad_norm": 3.532787799835205,
        "learning_rate": 0.00010674996816019278,
        "epoch": 0.49400104329681793,
        "step": 6629
    },
    {
        "loss": 2.9546,
        "grad_norm": 2.4642834663391113,
        "learning_rate": 0.00010667976130730715,
        "epoch": 0.4940755644980997,
        "step": 6630
    },
    {
        "loss": 2.6314,
        "grad_norm": 2.2730658054351807,
        "learning_rate": 0.00010660955114705303,
        "epoch": 0.49415008569938146,
        "step": 6631
    },
    {
        "loss": 2.4278,
        "grad_norm": 2.8101043701171875,
        "learning_rate": 0.00010653933771419364,
        "epoch": 0.4942246069006632,
        "step": 6632
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.9015986919403076,
        "learning_rate": 0.00010646912104349408,
        "epoch": 0.494299128101945,
        "step": 6633
    },
    {
        "loss": 2.5501,
        "grad_norm": 2.3832859992980957,
        "learning_rate": 0.00010639890116972088,
        "epoch": 0.49437364930322675,
        "step": 6634
    },
    {
        "loss": 3.1272,
        "grad_norm": 2.329874277114868,
        "learning_rate": 0.00010632867812764215,
        "epoch": 0.4944481705045085,
        "step": 6635
    },
    {
        "loss": 2.525,
        "grad_norm": 3.1427552700042725,
        "learning_rate": 0.00010625845195202764,
        "epoch": 0.4945226917057903,
        "step": 6636
    },
    {
        "loss": 2.2871,
        "grad_norm": 3.931633472442627,
        "learning_rate": 0.00010618822267764875,
        "epoch": 0.49459721290707204,
        "step": 6637
    },
    {
        "loss": 1.9458,
        "grad_norm": 3.5217180252075195,
        "learning_rate": 0.00010611799033927807,
        "epoch": 0.4946717341083538,
        "step": 6638
    },
    {
        "loss": 2.5798,
        "grad_norm": 3.2656900882720947,
        "learning_rate": 0.00010604775497169013,
        "epoch": 0.4947462553096356,
        "step": 6639
    },
    {
        "loss": 2.6117,
        "grad_norm": 1.5047564506530762,
        "learning_rate": 0.0001059775166096606,
        "epoch": 0.4948207765109174,
        "step": 6640
    },
    {
        "loss": 1.2467,
        "grad_norm": 3.491807699203491,
        "learning_rate": 0.00010590727528796696,
        "epoch": 0.49489529771219914,
        "step": 6641
    },
    {
        "loss": 2.6657,
        "grad_norm": 2.542526960372925,
        "learning_rate": 0.00010583703104138783,
        "epoch": 0.4949698189134809,
        "step": 6642
    },
    {
        "loss": 2.638,
        "grad_norm": 2.969393730163574,
        "learning_rate": 0.00010576678390470356,
        "epoch": 0.49504434011476267,
        "step": 6643
    },
    {
        "loss": 2.3336,
        "grad_norm": 3.3501713275909424,
        "learning_rate": 0.0001056965339126957,
        "epoch": 0.49511886131604443,
        "step": 6644
    },
    {
        "loss": 2.534,
        "grad_norm": 2.1748783588409424,
        "learning_rate": 0.00010562628110014736,
        "epoch": 0.4951933825173262,
        "step": 6645
    },
    {
        "loss": 2.4261,
        "grad_norm": 2.232621669769287,
        "learning_rate": 0.000105556025501843,
        "epoch": 0.49526790371860796,
        "step": 6646
    },
    {
        "loss": 1.7758,
        "grad_norm": 2.5685577392578125,
        "learning_rate": 0.0001054857671525686,
        "epoch": 0.4953424249198897,
        "step": 6647
    },
    {
        "loss": 2.956,
        "grad_norm": 4.189478397369385,
        "learning_rate": 0.00010541550608711124,
        "epoch": 0.4954169461211715,
        "step": 6648
    },
    {
        "loss": 2.7121,
        "grad_norm": 2.410304546356201,
        "learning_rate": 0.00010534524234025942,
        "epoch": 0.49549146732245325,
        "step": 6649
    },
    {
        "loss": 2.4051,
        "grad_norm": 2.767727851867676,
        "learning_rate": 0.00010527497594680309,
        "epoch": 0.495565988523735,
        "step": 6650
    },
    {
        "loss": 2.7489,
        "grad_norm": 3.324357509613037,
        "learning_rate": 0.00010520470694153353,
        "epoch": 0.4956405097250168,
        "step": 6651
    },
    {
        "loss": 2.3539,
        "grad_norm": 3.1254446506500244,
        "learning_rate": 0.00010513443535924308,
        "epoch": 0.49571503092629854,
        "step": 6652
    },
    {
        "loss": 2.9051,
        "grad_norm": 2.4136953353881836,
        "learning_rate": 0.0001050641612347256,
        "epoch": 0.4957895521275803,
        "step": 6653
    },
    {
        "loss": 2.6925,
        "grad_norm": 3.3200879096984863,
        "learning_rate": 0.00010499388460277602,
        "epoch": 0.49586407332886207,
        "step": 6654
    },
    {
        "loss": 2.8007,
        "grad_norm": 3.255031108856201,
        "learning_rate": 0.00010492360549819065,
        "epoch": 0.49593859453014383,
        "step": 6655
    },
    {
        "loss": 2.4279,
        "grad_norm": 1.6068329811096191,
        "learning_rate": 0.00010485332395576702,
        "epoch": 0.4960131157314256,
        "step": 6656
    },
    {
        "loss": 1.8776,
        "grad_norm": 1.61967933177948,
        "learning_rate": 0.00010478304001030379,
        "epoch": 0.49608763693270735,
        "step": 6657
    },
    {
        "loss": 2.1629,
        "grad_norm": 2.836813449859619,
        "learning_rate": 0.00010471275369660087,
        "epoch": 0.4961621581339891,
        "step": 6658
    },
    {
        "loss": 2.9218,
        "grad_norm": 3.551917791366577,
        "learning_rate": 0.00010464246504945922,
        "epoch": 0.4962366793352709,
        "step": 6659
    },
    {
        "loss": 1.4737,
        "grad_norm": 3.269613027572632,
        "learning_rate": 0.0001045721741036811,
        "epoch": 0.49631120053655264,
        "step": 6660
    },
    {
        "loss": 2.2424,
        "grad_norm": 3.095780372619629,
        "learning_rate": 0.00010450188089406994,
        "epoch": 0.4963857217378344,
        "step": 6661
    },
    {
        "loss": 2.1286,
        "grad_norm": 2.9236340522766113,
        "learning_rate": 0.00010443158545543012,
        "epoch": 0.49646024293911617,
        "step": 6662
    },
    {
        "loss": 1.8135,
        "grad_norm": 3.411553144454956,
        "learning_rate": 0.00010436128782256722,
        "epoch": 0.49653476414039793,
        "step": 6663
    },
    {
        "loss": 2.3305,
        "grad_norm": 2.5994081497192383,
        "learning_rate": 0.00010429098803028787,
        "epoch": 0.4966092853416797,
        "step": 6664
    },
    {
        "loss": 2.93,
        "grad_norm": 3.2105422019958496,
        "learning_rate": 0.00010422068611339986,
        "epoch": 0.49668380654296146,
        "step": 6665
    },
    {
        "loss": 2.1426,
        "grad_norm": 3.9981539249420166,
        "learning_rate": 0.00010415038210671192,
        "epoch": 0.4967583277442432,
        "step": 6666
    },
    {
        "loss": 1.7911,
        "grad_norm": 4.645324230194092,
        "learning_rate": 0.00010408007604503402,
        "epoch": 0.496832848945525,
        "step": 6667
    },
    {
        "loss": 2.6401,
        "grad_norm": 2.960397720336914,
        "learning_rate": 0.00010400976796317686,
        "epoch": 0.49690737014680675,
        "step": 6668
    },
    {
        "loss": 2.3097,
        "grad_norm": 3.474187135696411,
        "learning_rate": 0.0001039394578959522,
        "epoch": 0.4969818913480885,
        "step": 6669
    },
    {
        "loss": 2.681,
        "grad_norm": 2.7497975826263428,
        "learning_rate": 0.00010386914587817296,
        "epoch": 0.4970564125493703,
        "step": 6670
    },
    {
        "loss": 2.2607,
        "grad_norm": 2.430387020111084,
        "learning_rate": 0.00010379883194465296,
        "epoch": 0.49713093375065204,
        "step": 6671
    },
    {
        "loss": 2.3712,
        "grad_norm": 2.8474888801574707,
        "learning_rate": 0.00010372851613020689,
        "epoch": 0.4972054549519338,
        "step": 6672
    },
    {
        "loss": 2.4793,
        "grad_norm": 2.4344656467437744,
        "learning_rate": 0.0001036581984696503,
        "epoch": 0.49727997615321556,
        "step": 6673
    },
    {
        "loss": 2.0112,
        "grad_norm": 3.2241454124450684,
        "learning_rate": 0.00010358787899779989,
        "epoch": 0.4973544973544973,
        "step": 6674
    },
    {
        "loss": 3.3573,
        "grad_norm": 3.07745623588562,
        "learning_rate": 0.00010351755774947313,
        "epoch": 0.49742901855577915,
        "step": 6675
    },
    {
        "loss": 2.4126,
        "grad_norm": 3.887075185775757,
        "learning_rate": 0.00010344723475948839,
        "epoch": 0.4975035397570609,
        "step": 6676
    },
    {
        "loss": 2.6492,
        "grad_norm": 2.5242180824279785,
        "learning_rate": 0.00010337691006266476,
        "epoch": 0.49757806095834267,
        "step": 6677
    },
    {
        "loss": 1.4458,
        "grad_norm": 3.6778738498687744,
        "learning_rate": 0.00010330658369382249,
        "epoch": 0.49765258215962443,
        "step": 6678
    },
    {
        "loss": 2.063,
        "grad_norm": 3.77005672454834,
        "learning_rate": 0.00010323625568778228,
        "epoch": 0.4977271033609062,
        "step": 6679
    },
    {
        "loss": 2.2859,
        "grad_norm": 3.2982630729675293,
        "learning_rate": 0.00010316592607936591,
        "epoch": 0.49780162456218796,
        "step": 6680
    },
    {
        "loss": 2.291,
        "grad_norm": 2.4749867916107178,
        "learning_rate": 0.00010309559490339593,
        "epoch": 0.4978761457634697,
        "step": 6681
    },
    {
        "loss": 2.5342,
        "grad_norm": 3.304060459136963,
        "learning_rate": 0.0001030252621946956,
        "epoch": 0.4979506669647515,
        "step": 6682
    },
    {
        "loss": 1.9563,
        "grad_norm": 3.063778877258301,
        "learning_rate": 0.00010295492798808876,
        "epoch": 0.49802518816603325,
        "step": 6683
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.5049958229064941,
        "learning_rate": 0.00010288459231840035,
        "epoch": 0.498099709367315,
        "step": 6684
    },
    {
        "loss": 2.5365,
        "grad_norm": 2.921168327331543,
        "learning_rate": 0.00010281425522045585,
        "epoch": 0.4981742305685968,
        "step": 6685
    },
    {
        "loss": 2.3792,
        "grad_norm": 2.437252998352051,
        "learning_rate": 0.0001027439167290815,
        "epoch": 0.49824875176987854,
        "step": 6686
    },
    {
        "loss": 2.5284,
        "grad_norm": 2.937328338623047,
        "learning_rate": 0.00010267357687910405,
        "epoch": 0.4983232729711603,
        "step": 6687
    },
    {
        "loss": 2.8309,
        "grad_norm": 1.5955945253372192,
        "learning_rate": 0.00010260323570535121,
        "epoch": 0.49839779417244207,
        "step": 6688
    },
    {
        "loss": 2.186,
        "grad_norm": 2.964656114578247,
        "learning_rate": 0.00010253289324265107,
        "epoch": 0.49847231537372383,
        "step": 6689
    },
    {
        "loss": 1.6376,
        "grad_norm": 3.1990320682525635,
        "learning_rate": 0.00010246254952583263,
        "epoch": 0.4985468365750056,
        "step": 6690
    },
    {
        "loss": 2.0472,
        "grad_norm": 3.28302264213562,
        "learning_rate": 0.0001023922045897252,
        "epoch": 0.49862135777628736,
        "step": 6691
    },
    {
        "loss": 2.6868,
        "grad_norm": 3.653654098510742,
        "learning_rate": 0.00010232185846915903,
        "epoch": 0.4986958789775691,
        "step": 6692
    },
    {
        "loss": 2.3259,
        "grad_norm": 4.909807205200195,
        "learning_rate": 0.00010225151119896465,
        "epoch": 0.4987704001788509,
        "step": 6693
    },
    {
        "loss": 1.7862,
        "grad_norm": 2.488006830215454,
        "learning_rate": 0.00010218116281397334,
        "epoch": 0.49884492138013264,
        "step": 6694
    },
    {
        "loss": 2.1805,
        "grad_norm": 2.8442838191986084,
        "learning_rate": 0.00010211081334901696,
        "epoch": 0.4989194425814144,
        "step": 6695
    },
    {
        "loss": 2.8682,
        "grad_norm": 2.6878585815429688,
        "learning_rate": 0.0001020404628389277,
        "epoch": 0.49899396378269617,
        "step": 6696
    },
    {
        "loss": 3.2427,
        "grad_norm": 3.5217397212982178,
        "learning_rate": 0.00010197011131853851,
        "epoch": 0.49906848498397793,
        "step": 6697
    },
    {
        "loss": 3.0199,
        "grad_norm": 2.080746650695801,
        "learning_rate": 0.00010189975882268272,
        "epoch": 0.4991430061852597,
        "step": 6698
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.994234800338745,
        "learning_rate": 0.0001018294053861941,
        "epoch": 0.49921752738654146,
        "step": 6699
    },
    {
        "loss": 2.3625,
        "grad_norm": 4.039865016937256,
        "learning_rate": 0.00010175905104390702,
        "epoch": 0.4992920485878232,
        "step": 6700
    },
    {
        "loss": 2.5996,
        "grad_norm": 3.2042102813720703,
        "learning_rate": 0.00010168869583065612,
        "epoch": 0.499366569789105,
        "step": 6701
    },
    {
        "loss": 2.0327,
        "grad_norm": 4.122024059295654,
        "learning_rate": 0.00010161833978127666,
        "epoch": 0.49944109099038675,
        "step": 6702
    },
    {
        "loss": 2.0688,
        "grad_norm": 2.736499309539795,
        "learning_rate": 0.00010154798293060415,
        "epoch": 0.4995156121916685,
        "step": 6703
    },
    {
        "loss": 1.7845,
        "grad_norm": 3.457632541656494,
        "learning_rate": 0.00010147762531347467,
        "epoch": 0.4995901333929503,
        "step": 6704
    },
    {
        "loss": 2.6171,
        "grad_norm": 3.248422145843506,
        "learning_rate": 0.00010140726696472443,
        "epoch": 0.49966465459423204,
        "step": 6705
    },
    {
        "loss": 2.4882,
        "grad_norm": 2.302079200744629,
        "learning_rate": 0.00010133690791919022,
        "epoch": 0.4997391757955138,
        "step": 6706
    },
    {
        "loss": 3.0101,
        "grad_norm": 2.6852152347564697,
        "learning_rate": 0.00010126654821170913,
        "epoch": 0.49981369699679556,
        "step": 6707
    },
    {
        "loss": 2.0882,
        "grad_norm": 2.639371871948242,
        "learning_rate": 0.00010119618787711862,
        "epoch": 0.4998882181980773,
        "step": 6708
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.1060574054718018,
        "learning_rate": 0.00010112582695025636,
        "epoch": 0.4999627393993591,
        "step": 6709
    },
    {
        "loss": 2.2722,
        "grad_norm": 2.9820969104766846,
        "learning_rate": 0.00010105546546596021,
        "epoch": 0.5000372606006409,
        "step": 6710
    },
    {
        "loss": 2.2895,
        "grad_norm": 2.6154966354370117,
        "learning_rate": 0.00010098510345906855,
        "epoch": 0.5001117818019226,
        "step": 6711
    },
    {
        "loss": 2.7009,
        "grad_norm": 1.5356073379516602,
        "learning_rate": 0.00010091474096441999,
        "epoch": 0.5001863030032044,
        "step": 6712
    },
    {
        "loss": 2.9511,
        "grad_norm": 2.9229743480682373,
        "learning_rate": 0.00010084437801685316,
        "epoch": 0.5002608242044861,
        "step": 6713
    },
    {
        "loss": 2.1658,
        "grad_norm": 2.761866569519043,
        "learning_rate": 0.00010077401465120718,
        "epoch": 0.500335345405768,
        "step": 6714
    },
    {
        "loss": 3.1815,
        "grad_norm": 3.1242120265960693,
        "learning_rate": 0.00010070365090232114,
        "epoch": 0.5004098666070497,
        "step": 6715
    },
    {
        "loss": 2.4022,
        "grad_norm": 2.993608236312866,
        "learning_rate": 0.00010063328680503453,
        "epoch": 0.5004843878083315,
        "step": 6716
    },
    {
        "loss": 2.7783,
        "grad_norm": 2.5472311973571777,
        "learning_rate": 0.00010056292239418683,
        "epoch": 0.5005589090096132,
        "step": 6717
    },
    {
        "loss": 2.8928,
        "grad_norm": 2.698603391647339,
        "learning_rate": 0.00010049255770461792,
        "epoch": 0.500633430210895,
        "step": 6718
    },
    {
        "loss": 2.3615,
        "grad_norm": 3.637176513671875,
        "learning_rate": 0.00010042219277116755,
        "epoch": 0.5007079514121767,
        "step": 6719
    },
    {
        "loss": 2.062,
        "grad_norm": 3.4519243240356445,
        "learning_rate": 0.00010035182762867568,
        "epoch": 0.5007824726134585,
        "step": 6720
    },
    {
        "loss": 2.8235,
        "grad_norm": 2.9930505752563477,
        "learning_rate": 0.00010028146231198238,
        "epoch": 0.5008569938147402,
        "step": 6721
    },
    {
        "loss": 2.7818,
        "grad_norm": 2.1803531646728516,
        "learning_rate": 0.00010021109685592797,
        "epoch": 0.5009315150160221,
        "step": 6722
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.9374382495880127,
        "learning_rate": 0.0001001407312953526,
        "epoch": 0.5010060362173038,
        "step": 6723
    },
    {
        "loss": 2.8433,
        "grad_norm": 3.089512825012207,
        "learning_rate": 0.00010007036566509649,
        "epoch": 0.5010805574185856,
        "step": 6724
    },
    {
        "loss": 2.7903,
        "grad_norm": 2.479182243347168,
        "learning_rate": 0.0001,
        "epoch": 0.5011550786198673,
        "step": 6725
    },
    {
        "loss": 2.6502,
        "grad_norm": 4.0683913230896,
        "learning_rate": 9.992963433490361e-05,
        "epoch": 0.5012295998211491,
        "step": 6726
    },
    {
        "loss": 2.3427,
        "grad_norm": 2.8994617462158203,
        "learning_rate": 9.985926870464741e-05,
        "epoch": 0.5013041210224308,
        "step": 6727
    },
    {
        "loss": 2.4557,
        "grad_norm": 3.3223683834075928,
        "learning_rate": 9.978890314407205e-05,
        "epoch": 0.5013786422237126,
        "step": 6728
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.9013428688049316,
        "learning_rate": 9.971853768801756e-05,
        "epoch": 0.5014531634249945,
        "step": 6729
    },
    {
        "loss": 2.667,
        "grad_norm": 3.3382492065429688,
        "learning_rate": 9.964817237132436e-05,
        "epoch": 0.5015276846262762,
        "step": 6730
    },
    {
        "loss": 2.509,
        "grad_norm": 4.057064056396484,
        "learning_rate": 9.957780722883254e-05,
        "epoch": 0.501602205827558,
        "step": 6731
    },
    {
        "loss": 1.6921,
        "grad_norm": 2.7617437839508057,
        "learning_rate": 9.95074422953821e-05,
        "epoch": 0.5016767270288397,
        "step": 6732
    },
    {
        "loss": 2.7907,
        "grad_norm": 2.3710601329803467,
        "learning_rate": 9.943707760581319e-05,
        "epoch": 0.5017512482301215,
        "step": 6733
    },
    {
        "loss": 2.5338,
        "grad_norm": 3.2639949321746826,
        "learning_rate": 9.936671319496544e-05,
        "epoch": 0.5018257694314032,
        "step": 6734
    },
    {
        "loss": 2.3959,
        "grad_norm": 2.4978585243225098,
        "learning_rate": 9.929634909767888e-05,
        "epoch": 0.501900290632685,
        "step": 6735
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.1743438243865967,
        "learning_rate": 9.92259853487929e-05,
        "epoch": 0.5019748118339667,
        "step": 6736
    },
    {
        "loss": 2.3382,
        "grad_norm": 2.2033119201660156,
        "learning_rate": 9.91556219831468e-05,
        "epoch": 0.5020493330352486,
        "step": 6737
    },
    {
        "loss": 2.5672,
        "grad_norm": 2.193657159805298,
        "learning_rate": 9.908525903558002e-05,
        "epoch": 0.5021238542365303,
        "step": 6738
    },
    {
        "loss": 2.8653,
        "grad_norm": 2.0837950706481934,
        "learning_rate": 9.90148965409314e-05,
        "epoch": 0.5021983754378121,
        "step": 6739
    },
    {
        "loss": 3.0155,
        "grad_norm": 3.057065010070801,
        "learning_rate": 9.894453453403981e-05,
        "epoch": 0.5022728966390938,
        "step": 6740
    },
    {
        "loss": 2.4477,
        "grad_norm": 3.445725679397583,
        "learning_rate": 9.887417304974374e-05,
        "epoch": 0.5023474178403756,
        "step": 6741
    },
    {
        "loss": 2.2996,
        "grad_norm": 1.9145628213882446,
        "learning_rate": 9.88038121228814e-05,
        "epoch": 0.5024219390416573,
        "step": 6742
    },
    {
        "loss": 2.7031,
        "grad_norm": 2.2141096591949463,
        "learning_rate": 9.873345178829089e-05,
        "epoch": 0.5024964602429391,
        "step": 6743
    },
    {
        "loss": 3.0464,
        "grad_norm": 2.373324155807495,
        "learning_rate": 9.866309208080987e-05,
        "epoch": 0.5025709814442209,
        "step": 6744
    },
    {
        "loss": 2.2728,
        "grad_norm": 2.2832324504852295,
        "learning_rate": 9.859273303527561e-05,
        "epoch": 0.5026455026455027,
        "step": 6745
    },
    {
        "loss": 1.4854,
        "grad_norm": 2.8400778770446777,
        "learning_rate": 9.852237468652545e-05,
        "epoch": 0.5027200238467844,
        "step": 6746
    },
    {
        "loss": 2.3967,
        "grad_norm": 3.7680466175079346,
        "learning_rate": 9.845201706939582e-05,
        "epoch": 0.5027945450480662,
        "step": 6747
    },
    {
        "loss": 1.7497,
        "grad_norm": 3.561018228530884,
        "learning_rate": 9.838166021872335e-05,
        "epoch": 0.5028690662493479,
        "step": 6748
    },
    {
        "loss": 2.6405,
        "grad_norm": 1.8464388847351074,
        "learning_rate": 9.83113041693439e-05,
        "epoch": 0.5029435874506297,
        "step": 6749
    },
    {
        "loss": 2.6216,
        "grad_norm": 1.929504156112671,
        "learning_rate": 9.824094895609301e-05,
        "epoch": 0.5030181086519114,
        "step": 6750
    },
    {
        "loss": 2.5336,
        "grad_norm": 4.079832077026367,
        "learning_rate": 9.817059461380594e-05,
        "epoch": 0.5030926298531933,
        "step": 6751
    },
    {
        "loss": 1.6339,
        "grad_norm": 1.7495100498199463,
        "learning_rate": 9.810024117731729e-05,
        "epoch": 0.503167151054475,
        "step": 6752
    },
    {
        "loss": 2.3042,
        "grad_norm": 3.419994831085205,
        "learning_rate": 9.802988868146153e-05,
        "epoch": 0.5032416722557568,
        "step": 6753
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.030118465423584,
        "learning_rate": 9.795953716107239e-05,
        "epoch": 0.5033161934570385,
        "step": 6754
    },
    {
        "loss": 2.8482,
        "grad_norm": 1.9386467933654785,
        "learning_rate": 9.788918665098306e-05,
        "epoch": 0.5033907146583203,
        "step": 6755
    },
    {
        "loss": 2.1994,
        "grad_norm": 3.0787949562072754,
        "learning_rate": 9.781883718602668e-05,
        "epoch": 0.503465235859602,
        "step": 6756
    },
    {
        "loss": 1.1272,
        "grad_norm": 3.202408790588379,
        "learning_rate": 9.774848880103529e-05,
        "epoch": 0.5035397570608838,
        "step": 6757
    },
    {
        "loss": 2.8981,
        "grad_norm": 4.313273906707764,
        "learning_rate": 9.767814153084098e-05,
        "epoch": 0.5036142782621655,
        "step": 6758
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.5931754112243652,
        "learning_rate": 9.760779541027481e-05,
        "epoch": 0.5036887994634474,
        "step": 6759
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.4171671867370605,
        "learning_rate": 9.753745047416738e-05,
        "epoch": 0.5037633206647291,
        "step": 6760
    },
    {
        "loss": 2.8316,
        "grad_norm": 3.641117811203003,
        "learning_rate": 9.746710675734895e-05,
        "epoch": 0.5038378418660109,
        "step": 6761
    },
    {
        "loss": 2.8243,
        "grad_norm": 1.9574480056762695,
        "learning_rate": 9.739676429464881e-05,
        "epoch": 0.5039123630672926,
        "step": 6762
    },
    {
        "loss": 2.7777,
        "grad_norm": 2.9704134464263916,
        "learning_rate": 9.732642312089597e-05,
        "epoch": 0.5039868842685744,
        "step": 6763
    },
    {
        "loss": 2.4395,
        "grad_norm": 3.6792449951171875,
        "learning_rate": 9.725608327091858e-05,
        "epoch": 0.5040614054698562,
        "step": 6764
    },
    {
        "loss": 2.7928,
        "grad_norm": 2.884896755218506,
        "learning_rate": 9.718574477954412e-05,
        "epoch": 0.5041359266711379,
        "step": 6765
    },
    {
        "loss": 2.4743,
        "grad_norm": 2.140085220336914,
        "learning_rate": 9.711540768159965e-05,
        "epoch": 0.5042104478724198,
        "step": 6766
    },
    {
        "loss": 2.7397,
        "grad_norm": 2.6674463748931885,
        "learning_rate": 9.704507201191118e-05,
        "epoch": 0.5042849690737015,
        "step": 6767
    },
    {
        "loss": 2.8791,
        "grad_norm": 3.7962684631347656,
        "learning_rate": 9.697473780530444e-05,
        "epoch": 0.5043594902749833,
        "step": 6768
    },
    {
        "loss": 2.6483,
        "grad_norm": 2.4708712100982666,
        "learning_rate": 9.69044050966041e-05,
        "epoch": 0.504434011476265,
        "step": 6769
    },
    {
        "loss": 2.5782,
        "grad_norm": 3.6060705184936523,
        "learning_rate": 9.683407392063405e-05,
        "epoch": 0.5045085326775468,
        "step": 6770
    },
    {
        "loss": 2.589,
        "grad_norm": 2.4508540630340576,
        "learning_rate": 9.676374431221774e-05,
        "epoch": 0.5045830538788285,
        "step": 6771
    },
    {
        "loss": 2.548,
        "grad_norm": 2.550478458404541,
        "learning_rate": 9.66934163061776e-05,
        "epoch": 0.5046575750801103,
        "step": 6772
    },
    {
        "loss": 2.7769,
        "grad_norm": 3.5445101261138916,
        "learning_rate": 9.662308993733525e-05,
        "epoch": 0.504732096281392,
        "step": 6773
    },
    {
        "loss": 1.7379,
        "grad_norm": 3.968388795852661,
        "learning_rate": 9.655276524051171e-05,
        "epoch": 0.5048066174826739,
        "step": 6774
    },
    {
        "loss": 1.9245,
        "grad_norm": 3.714715003967285,
        "learning_rate": 9.648244225052682e-05,
        "epoch": 0.5048811386839556,
        "step": 6775
    },
    {
        "loss": 2.2533,
        "grad_norm": 2.9164369106292725,
        "learning_rate": 9.641212100220013e-05,
        "epoch": 0.5049556598852374,
        "step": 6776
    },
    {
        "loss": 2.898,
        "grad_norm": 2.200775623321533,
        "learning_rate": 9.634180153034977e-05,
        "epoch": 0.5050301810865191,
        "step": 6777
    },
    {
        "loss": 2.6764,
        "grad_norm": 2.939532995223999,
        "learning_rate": 9.627148386979314e-05,
        "epoch": 0.5051047022878009,
        "step": 6778
    },
    {
        "loss": 2.4478,
        "grad_norm": 3.675723075866699,
        "learning_rate": 9.620116805534706e-05,
        "epoch": 0.5051792234890826,
        "step": 6779
    },
    {
        "loss": 3.1994,
        "grad_norm": 2.986860990524292,
        "learning_rate": 9.613085412182698e-05,
        "epoch": 0.5052537446903644,
        "step": 6780
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.7222800254821777,
        "learning_rate": 9.606054210404782e-05,
        "epoch": 0.5053282658916461,
        "step": 6781
    },
    {
        "loss": 2.4403,
        "grad_norm": 2.0364491939544678,
        "learning_rate": 9.599023203682323e-05,
        "epoch": 0.505402787092928,
        "step": 6782
    },
    {
        "loss": 2.6996,
        "grad_norm": 3.533813714981079,
        "learning_rate": 9.591992395496599e-05,
        "epoch": 0.5054773082942097,
        "step": 6783
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.2055866718292236,
        "learning_rate": 9.584961789328809e-05,
        "epoch": 0.5055518294954915,
        "step": 6784
    },
    {
        "loss": 1.9626,
        "grad_norm": 2.1220340728759766,
        "learning_rate": 9.57793138866001e-05,
        "epoch": 0.5056263506967732,
        "step": 6785
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.920254707336426,
        "learning_rate": 9.570901196971214e-05,
        "epoch": 0.505700871898055,
        "step": 6786
    },
    {
        "loss": 2.5885,
        "grad_norm": 2.4044740200042725,
        "learning_rate": 9.563871217743288e-05,
        "epoch": 0.5057753930993367,
        "step": 6787
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.7267282009124756,
        "learning_rate": 9.55684145445699e-05,
        "epoch": 0.5058499143006185,
        "step": 6788
    },
    {
        "loss": 2.9299,
        "grad_norm": 2.74161958694458,
        "learning_rate": 9.549811910593009e-05,
        "epoch": 0.5059244355019002,
        "step": 6789
    },
    {
        "loss": 1.8859,
        "grad_norm": 4.203718662261963,
        "learning_rate": 9.542782589631886e-05,
        "epoch": 0.5059989567031821,
        "step": 6790
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.32742977142334,
        "learning_rate": 9.535753495054079e-05,
        "epoch": 0.5060734779044638,
        "step": 6791
    },
    {
        "loss": 2.625,
        "grad_norm": 3.109308958053589,
        "learning_rate": 9.528724630339922e-05,
        "epoch": 0.5061479991057456,
        "step": 6792
    },
    {
        "loss": 2.4439,
        "grad_norm": 2.1584365367889404,
        "learning_rate": 9.521695998969622e-05,
        "epoch": 0.5062225203070273,
        "step": 6793
    },
    {
        "loss": 2.3472,
        "grad_norm": 2.8218846321105957,
        "learning_rate": 9.514667604423302e-05,
        "epoch": 0.5062970415083091,
        "step": 6794
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.2656893730163574,
        "learning_rate": 9.507639450180928e-05,
        "epoch": 0.5063715627095908,
        "step": 6795
    },
    {
        "loss": 2.4387,
        "grad_norm": 2.38651442527771,
        "learning_rate": 9.500611539722401e-05,
        "epoch": 0.5064460839108726,
        "step": 6796
    },
    {
        "loss": 2.4593,
        "grad_norm": 2.3428497314453125,
        "learning_rate": 9.493583876527449e-05,
        "epoch": 0.5065206051121544,
        "step": 6797
    },
    {
        "loss": 1.4156,
        "grad_norm": 4.126319885253906,
        "learning_rate": 9.486556464075689e-05,
        "epoch": 0.5065951263134362,
        "step": 6798
    },
    {
        "loss": 2.1994,
        "grad_norm": 2.5662901401519775,
        "learning_rate": 9.479529305846651e-05,
        "epoch": 0.506669647514718,
        "step": 6799
    },
    {
        "loss": 1.7536,
        "grad_norm": 3.7575416564941406,
        "learning_rate": 9.472502405319686e-05,
        "epoch": 0.5067441687159997,
        "step": 6800
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.9611533880233765,
        "learning_rate": 9.46547576597406e-05,
        "epoch": 0.5068186899172815,
        "step": 6801
    },
    {
        "loss": 2.3807,
        "grad_norm": 4.213642120361328,
        "learning_rate": 9.458449391288885e-05,
        "epoch": 0.5068932111185632,
        "step": 6802
    },
    {
        "loss": 2.5623,
        "grad_norm": 3.111173629760742,
        "learning_rate": 9.451423284743144e-05,
        "epoch": 0.506967732319845,
        "step": 6803
    },
    {
        "loss": 2.7551,
        "grad_norm": 3.4679031372070312,
        "learning_rate": 9.4443974498157e-05,
        "epoch": 0.5070422535211268,
        "step": 6804
    },
    {
        "loss": 2.3265,
        "grad_norm": 3.3310208320617676,
        "learning_rate": 9.437371889985273e-05,
        "epoch": 0.5071167747224086,
        "step": 6805
    },
    {
        "loss": 2.895,
        "grad_norm": 2.5194883346557617,
        "learning_rate": 9.430346608730433e-05,
        "epoch": 0.5071912959236903,
        "step": 6806
    },
    {
        "loss": 2.569,
        "grad_norm": 2.200857639312744,
        "learning_rate": 9.423321609529654e-05,
        "epoch": 0.5072658171249721,
        "step": 6807
    },
    {
        "loss": 2.6192,
        "grad_norm": 2.9926490783691406,
        "learning_rate": 9.416296895861213e-05,
        "epoch": 0.5073403383262538,
        "step": 6808
    },
    {
        "loss": 1.6696,
        "grad_norm": 3.560314178466797,
        "learning_rate": 9.409272471203305e-05,
        "epoch": 0.5074148595275356,
        "step": 6809
    },
    {
        "loss": 2.7373,
        "grad_norm": 2.2378265857696533,
        "learning_rate": 9.402248339033942e-05,
        "epoch": 0.5074893807288173,
        "step": 6810
    },
    {
        "loss": 2.5253,
        "grad_norm": 4.067753314971924,
        "learning_rate": 9.39522450283099e-05,
        "epoch": 0.5075639019300991,
        "step": 6811
    },
    {
        "loss": 2.0449,
        "grad_norm": 3.065568208694458,
        "learning_rate": 9.388200966072195e-05,
        "epoch": 0.5076384231313809,
        "step": 6812
    },
    {
        "loss": 2.3432,
        "grad_norm": 3.1370577812194824,
        "learning_rate": 9.381177732235128e-05,
        "epoch": 0.5077129443326627,
        "step": 6813
    },
    {
        "loss": 2.6451,
        "grad_norm": 3.251075267791748,
        "learning_rate": 9.374154804797238e-05,
        "epoch": 0.5077874655339444,
        "step": 6814
    },
    {
        "loss": 2.8239,
        "grad_norm": 2.592320680618286,
        "learning_rate": 9.367132187235794e-05,
        "epoch": 0.5078619867352262,
        "step": 6815
    },
    {
        "loss": 2.368,
        "grad_norm": 3.6414635181427,
        "learning_rate": 9.360109883027913e-05,
        "epoch": 0.5079365079365079,
        "step": 6816
    },
    {
        "loss": 2.8108,
        "grad_norm": 2.0868942737579346,
        "learning_rate": 9.353087895650593e-05,
        "epoch": 0.5080110291377897,
        "step": 6817
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.951988935470581,
        "learning_rate": 9.346066228580628e-05,
        "epoch": 0.5080855503390714,
        "step": 6818
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.99344801902771,
        "learning_rate": 9.3390448852947e-05,
        "epoch": 0.5081600715403533,
        "step": 6819
    },
    {
        "loss": 2.7758,
        "grad_norm": 2.6396548748016357,
        "learning_rate": 9.332023869269288e-05,
        "epoch": 0.508234592741635,
        "step": 6820
    },
    {
        "loss": 1.8041,
        "grad_norm": 3.4932475090026855,
        "learning_rate": 9.325003183980726e-05,
        "epoch": 0.5083091139429168,
        "step": 6821
    },
    {
        "loss": 1.9935,
        "grad_norm": 2.2845523357391357,
        "learning_rate": 9.317982832905195e-05,
        "epoch": 0.5083836351441985,
        "step": 6822
    },
    {
        "loss": 2.3396,
        "grad_norm": 2.7574009895324707,
        "learning_rate": 9.310962819518689e-05,
        "epoch": 0.5084581563454803,
        "step": 6823
    },
    {
        "loss": 2.1747,
        "grad_norm": 3.106264352798462,
        "learning_rate": 9.303943147297058e-05,
        "epoch": 0.508532677546762,
        "step": 6824
    },
    {
        "loss": 2.6703,
        "grad_norm": 2.85019588470459,
        "learning_rate": 9.29692381971597e-05,
        "epoch": 0.5086071987480438,
        "step": 6825
    },
    {
        "loss": 1.731,
        "grad_norm": 4.235898494720459,
        "learning_rate": 9.289904840250903e-05,
        "epoch": 0.5086817199493255,
        "step": 6826
    },
    {
        "loss": 2.4882,
        "grad_norm": 3.508321523666382,
        "learning_rate": 9.282886212377215e-05,
        "epoch": 0.5087562411506074,
        "step": 6827
    },
    {
        "loss": 3.1674,
        "grad_norm": 2.6952965259552,
        "learning_rate": 9.275867939570031e-05,
        "epoch": 0.5088307623518891,
        "step": 6828
    },
    {
        "loss": 2.0856,
        "grad_norm": 2.7376821041107178,
        "learning_rate": 9.268850025304354e-05,
        "epoch": 0.5089052835531709,
        "step": 6829
    },
    {
        "loss": 2.2928,
        "grad_norm": 3.9146077632904053,
        "learning_rate": 9.261832473054973e-05,
        "epoch": 0.5089798047544526,
        "step": 6830
    },
    {
        "loss": 2.6319,
        "grad_norm": 2.0684802532196045,
        "learning_rate": 9.254815286296494e-05,
        "epoch": 0.5090543259557344,
        "step": 6831
    },
    {
        "loss": 2.967,
        "grad_norm": 2.1097443103790283,
        "learning_rate": 9.247798468503375e-05,
        "epoch": 0.5091288471570161,
        "step": 6832
    },
    {
        "loss": 2.271,
        "grad_norm": 3.569678544998169,
        "learning_rate": 9.24078202314986e-05,
        "epoch": 0.5092033683582979,
        "step": 6833
    },
    {
        "loss": 2.0103,
        "grad_norm": 1.790152668952942,
        "learning_rate": 9.233765953710029e-05,
        "epoch": 0.5092778895595798,
        "step": 6834
    },
    {
        "loss": 1.8111,
        "grad_norm": 3.0964317321777344,
        "learning_rate": 9.226750263657763e-05,
        "epoch": 0.5093524107608615,
        "step": 6835
    },
    {
        "loss": 2.2365,
        "grad_norm": 3.1845576763153076,
        "learning_rate": 9.219734956466748e-05,
        "epoch": 0.5094269319621433,
        "step": 6836
    },
    {
        "loss": 1.7066,
        "grad_norm": 3.54750919342041,
        "learning_rate": 9.212720035610521e-05,
        "epoch": 0.509501453163425,
        "step": 6837
    },
    {
        "loss": 1.761,
        "grad_norm": 3.03609299659729,
        "learning_rate": 9.205705504562378e-05,
        "epoch": 0.5095759743647068,
        "step": 6838
    },
    {
        "loss": 2.4004,
        "grad_norm": 2.5043485164642334,
        "learning_rate": 9.198691366795437e-05,
        "epoch": 0.5096504955659885,
        "step": 6839
    },
    {
        "loss": 2.3812,
        "grad_norm": 2.3707470893859863,
        "learning_rate": 9.191677625782651e-05,
        "epoch": 0.5097250167672703,
        "step": 6840
    },
    {
        "loss": 2.5403,
        "grad_norm": 1.3686020374298096,
        "learning_rate": 9.184664284996735e-05,
        "epoch": 0.509799537968552,
        "step": 6841
    },
    {
        "loss": 2.5402,
        "grad_norm": 4.028604030609131,
        "learning_rate": 9.177651347910235e-05,
        "epoch": 0.5098740591698339,
        "step": 6842
    },
    {
        "loss": 2.9273,
        "grad_norm": 2.3508381843566895,
        "learning_rate": 9.170638817995477e-05,
        "epoch": 0.5099485803711156,
        "step": 6843
    },
    {
        "loss": 2.3594,
        "grad_norm": 3.3535101413726807,
        "learning_rate": 9.163626698724594e-05,
        "epoch": 0.5100231015723974,
        "step": 6844
    },
    {
        "loss": 2.1752,
        "grad_norm": 2.8111767768859863,
        "learning_rate": 9.156614993569522e-05,
        "epoch": 0.5100976227736791,
        "step": 6845
    },
    {
        "loss": 2.9858,
        "grad_norm": 2.0166025161743164,
        "learning_rate": 9.149603706001968e-05,
        "epoch": 0.5101721439749609,
        "step": 6846
    },
    {
        "loss": 2.1628,
        "grad_norm": 2.383859157562256,
        "learning_rate": 9.142592839493478e-05,
        "epoch": 0.5102466651762426,
        "step": 6847
    },
    {
        "loss": 2.3727,
        "grad_norm": 4.254386901855469,
        "learning_rate": 9.135582397515351e-05,
        "epoch": 0.5103211863775244,
        "step": 6848
    },
    {
        "loss": 1.7386,
        "grad_norm": 4.04748010635376,
        "learning_rate": 9.128572383538667e-05,
        "epoch": 0.5103957075788061,
        "step": 6849
    },
    {
        "loss": 2.4195,
        "grad_norm": 4.13104772567749,
        "learning_rate": 9.121562801034344e-05,
        "epoch": 0.510470228780088,
        "step": 6850
    },
    {
        "loss": 2.1896,
        "grad_norm": 3.0208234786987305,
        "learning_rate": 9.114553653473034e-05,
        "epoch": 0.5105447499813697,
        "step": 6851
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.2541842460632324,
        "learning_rate": 9.107544944325211e-05,
        "epoch": 0.5106192711826515,
        "step": 6852
    },
    {
        "loss": 2.4298,
        "grad_norm": 2.538930654525757,
        "learning_rate": 9.100536677061107e-05,
        "epoch": 0.5106937923839332,
        "step": 6853
    },
    {
        "loss": 2.3597,
        "grad_norm": 2.5916452407836914,
        "learning_rate": 9.09352885515074e-05,
        "epoch": 0.510768313585215,
        "step": 6854
    },
    {
        "loss": 2.4558,
        "grad_norm": 4.407433032989502,
        "learning_rate": 9.086521482063928e-05,
        "epoch": 0.5108428347864967,
        "step": 6855
    },
    {
        "loss": 2.0173,
        "grad_norm": 3.11545729637146,
        "learning_rate": 9.079514561270222e-05,
        "epoch": 0.5109173559877785,
        "step": 6856
    },
    {
        "loss": 2.1619,
        "grad_norm": 2.808128833770752,
        "learning_rate": 9.072508096239014e-05,
        "epoch": 0.5109918771890603,
        "step": 6857
    },
    {
        "loss": 2.784,
        "grad_norm": 1.5442712306976318,
        "learning_rate": 9.065502090439413e-05,
        "epoch": 0.5110663983903421,
        "step": 6858
    },
    {
        "loss": 2.5373,
        "grad_norm": 1.8368902206420898,
        "learning_rate": 9.058496547340316e-05,
        "epoch": 0.5111409195916238,
        "step": 6859
    },
    {
        "loss": 2.5299,
        "grad_norm": 2.9364542961120605,
        "learning_rate": 9.051491470410422e-05,
        "epoch": 0.5112154407929056,
        "step": 6860
    },
    {
        "loss": 1.8124,
        "grad_norm": 3.282353639602661,
        "learning_rate": 9.044486863118147e-05,
        "epoch": 0.5112899619941873,
        "step": 6861
    },
    {
        "loss": 2.3011,
        "grad_norm": 3.521984338760376,
        "learning_rate": 9.037482728931724e-05,
        "epoch": 0.5113644831954691,
        "step": 6862
    },
    {
        "loss": 2.845,
        "grad_norm": 2.263101577758789,
        "learning_rate": 9.030479071319117e-05,
        "epoch": 0.5114390043967508,
        "step": 6863
    },
    {
        "loss": 2.8686,
        "grad_norm": 2.1160635948181152,
        "learning_rate": 9.02347589374806e-05,
        "epoch": 0.5115135255980326,
        "step": 6864
    },
    {
        "loss": 2.5909,
        "grad_norm": 2.567780017852783,
        "learning_rate": 9.01647319968607e-05,
        "epoch": 0.5115880467993144,
        "step": 6865
    },
    {
        "loss": 2.5289,
        "grad_norm": 4.4364166259765625,
        "learning_rate": 9.009470992600402e-05,
        "epoch": 0.5116625680005962,
        "step": 6866
    },
    {
        "loss": 2.643,
        "grad_norm": 2.9193456172943115,
        "learning_rate": 9.00246927595807e-05,
        "epoch": 0.5117370892018779,
        "step": 6867
    },
    {
        "loss": 2.7079,
        "grad_norm": 3.28615140914917,
        "learning_rate": 8.995468053225872e-05,
        "epoch": 0.5118116104031597,
        "step": 6868
    },
    {
        "loss": 2.6272,
        "grad_norm": 1.9257915019989014,
        "learning_rate": 8.98846732787032e-05,
        "epoch": 0.5118861316044415,
        "step": 6869
    },
    {
        "loss": 2.108,
        "grad_norm": 3.7294914722442627,
        "learning_rate": 8.981467103357727e-05,
        "epoch": 0.5119606528057232,
        "step": 6870
    },
    {
        "loss": 1.903,
        "grad_norm": 3.2328670024871826,
        "learning_rate": 8.974467383154126e-05,
        "epoch": 0.512035174007005,
        "step": 6871
    },
    {
        "loss": 2.4863,
        "grad_norm": 2.7581558227539062,
        "learning_rate": 8.967468170725295e-05,
        "epoch": 0.5121096952082868,
        "step": 6872
    },
    {
        "loss": 2.8485,
        "grad_norm": 3.7722742557525635,
        "learning_rate": 8.960469469536787e-05,
        "epoch": 0.5121842164095686,
        "step": 6873
    },
    {
        "loss": 2.4488,
        "grad_norm": 2.625537395477295,
        "learning_rate": 8.953471283053876e-05,
        "epoch": 0.5122587376108503,
        "step": 6874
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.563446044921875,
        "learning_rate": 8.946473614741609e-05,
        "epoch": 0.5123332588121321,
        "step": 6875
    },
    {
        "loss": 1.5562,
        "grad_norm": 2.760917901992798,
        "learning_rate": 8.93947646806475e-05,
        "epoch": 0.5124077800134138,
        "step": 6876
    },
    {
        "loss": 2.6955,
        "grad_norm": 2.7051851749420166,
        "learning_rate": 8.932479846487805e-05,
        "epoch": 0.5124823012146956,
        "step": 6877
    },
    {
        "loss": 1.9494,
        "grad_norm": 3.097195863723755,
        "learning_rate": 8.925483753475056e-05,
        "epoch": 0.5125568224159773,
        "step": 6878
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.4797115325927734,
        "learning_rate": 8.918488192490469e-05,
        "epoch": 0.5126313436172591,
        "step": 6879
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.65944504737854,
        "learning_rate": 8.911493166997805e-05,
        "epoch": 0.5127058648185409,
        "step": 6880
    },
    {
        "loss": 2.4691,
        "grad_norm": 2.5994760990142822,
        "learning_rate": 8.904498680460516e-05,
        "epoch": 0.5127803860198227,
        "step": 6881
    },
    {
        "loss": 1.4902,
        "grad_norm": 3.2506625652313232,
        "learning_rate": 8.897504736341794e-05,
        "epoch": 0.5128549072211044,
        "step": 6882
    },
    {
        "loss": 2.8024,
        "grad_norm": 3.5467817783355713,
        "learning_rate": 8.890511338104585e-05,
        "epoch": 0.5129294284223862,
        "step": 6883
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.8593225479125977,
        "learning_rate": 8.883518489211533e-05,
        "epoch": 0.5130039496236679,
        "step": 6884
    },
    {
        "loss": 1.7144,
        "grad_norm": 1.5713837146759033,
        "learning_rate": 8.87652619312504e-05,
        "epoch": 0.5130784708249497,
        "step": 6885
    },
    {
        "loss": 2.1334,
        "grad_norm": 1.5509452819824219,
        "learning_rate": 8.86953445330722e-05,
        "epoch": 0.5131529920262314,
        "step": 6886
    },
    {
        "loss": 2.2881,
        "grad_norm": 1.898758053779602,
        "learning_rate": 8.862543273219892e-05,
        "epoch": 0.5132275132275133,
        "step": 6887
    },
    {
        "loss": 2.6921,
        "grad_norm": 2.32066011428833,
        "learning_rate": 8.855552656324646e-05,
        "epoch": 0.513302034428795,
        "step": 6888
    },
    {
        "loss": 2.3733,
        "grad_norm": 2.0500853061676025,
        "learning_rate": 8.848562606082742e-05,
        "epoch": 0.5133765556300768,
        "step": 6889
    },
    {
        "loss": 2.5063,
        "grad_norm": 3.296945810317993,
        "learning_rate": 8.841573125955208e-05,
        "epoch": 0.5134510768313585,
        "step": 6890
    },
    {
        "loss": 2.9805,
        "grad_norm": 3.772204875946045,
        "learning_rate": 8.834584219402748e-05,
        "epoch": 0.5135255980326403,
        "step": 6891
    },
    {
        "loss": 2.6353,
        "grad_norm": 4.020634174346924,
        "learning_rate": 8.827595889885796e-05,
        "epoch": 0.513600119233922,
        "step": 6892
    },
    {
        "loss": 2.4163,
        "grad_norm": 4.090795516967773,
        "learning_rate": 8.820608140864514e-05,
        "epoch": 0.5136746404352038,
        "step": 6893
    },
    {
        "loss": 1.3689,
        "grad_norm": 3.0207438468933105,
        "learning_rate": 8.813620975798753e-05,
        "epoch": 0.5137491616364855,
        "step": 6894
    },
    {
        "loss": 2.9283,
        "grad_norm": 2.7852344512939453,
        "learning_rate": 8.806634398148099e-05,
        "epoch": 0.5138236828377674,
        "step": 6895
    },
    {
        "loss": 2.5708,
        "grad_norm": 2.92397403717041,
        "learning_rate": 8.79964841137183e-05,
        "epoch": 0.5138982040390491,
        "step": 6896
    },
    {
        "loss": 1.9739,
        "grad_norm": 3.7920773029327393,
        "learning_rate": 8.792663018928923e-05,
        "epoch": 0.5139727252403309,
        "step": 6897
    },
    {
        "loss": 1.989,
        "grad_norm": 3.990582227706909,
        "learning_rate": 8.785678224278104e-05,
        "epoch": 0.5140472464416126,
        "step": 6898
    },
    {
        "loss": 2.5372,
        "grad_norm": 3.1007399559020996,
        "learning_rate": 8.778694030877755e-05,
        "epoch": 0.5141217676428944,
        "step": 6899
    },
    {
        "loss": 2.4661,
        "grad_norm": 3.977149248123169,
        "learning_rate": 8.771710442185968e-05,
        "epoch": 0.5141962888441761,
        "step": 6900
    },
    {
        "loss": 2.548,
        "grad_norm": 2.2899279594421387,
        "learning_rate": 8.764727461660575e-05,
        "epoch": 0.5142708100454579,
        "step": 6901
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.3846912384033203,
        "learning_rate": 8.757745092759056e-05,
        "epoch": 0.5143453312467396,
        "step": 6902
    },
    {
        "loss": 2.622,
        "grad_norm": 2.386582851409912,
        "learning_rate": 8.750763338938623e-05,
        "epoch": 0.5144198524480215,
        "step": 6903
    },
    {
        "loss": 2.161,
        "grad_norm": 2.739478588104248,
        "learning_rate": 8.743782203656166e-05,
        "epoch": 0.5144943736493032,
        "step": 6904
    },
    {
        "loss": 1.2582,
        "grad_norm": 3.2022597789764404,
        "learning_rate": 8.736801690368269e-05,
        "epoch": 0.514568894850585,
        "step": 6905
    },
    {
        "loss": 1.4506,
        "grad_norm": 2.4910495281219482,
        "learning_rate": 8.729821802531219e-05,
        "epoch": 0.5146434160518668,
        "step": 6906
    },
    {
        "loss": 2.1556,
        "grad_norm": 2.2168121337890625,
        "learning_rate": 8.722842543600974e-05,
        "epoch": 0.5147179372531485,
        "step": 6907
    },
    {
        "loss": 2.1871,
        "grad_norm": 3.39524507522583,
        "learning_rate": 8.715863917033218e-05,
        "epoch": 0.5147924584544303,
        "step": 6908
    },
    {
        "loss": 2.5007,
        "grad_norm": 4.123188495635986,
        "learning_rate": 8.708885926283286e-05,
        "epoch": 0.514866979655712,
        "step": 6909
    },
    {
        "loss": 1.6105,
        "grad_norm": 4.4692535400390625,
        "learning_rate": 8.701908574806192e-05,
        "epoch": 0.5149415008569939,
        "step": 6910
    },
    {
        "loss": 2.5302,
        "grad_norm": 4.078641891479492,
        "learning_rate": 8.694931866056679e-05,
        "epoch": 0.5150160220582756,
        "step": 6911
    },
    {
        "loss": 2.9455,
        "grad_norm": 2.950066089630127,
        "learning_rate": 8.687955803489128e-05,
        "epoch": 0.5150905432595574,
        "step": 6912
    },
    {
        "loss": 2.1157,
        "grad_norm": 3.6177124977111816,
        "learning_rate": 8.680980390557626e-05,
        "epoch": 0.5151650644608391,
        "step": 6913
    },
    {
        "loss": 2.2242,
        "grad_norm": 2.199542760848999,
        "learning_rate": 8.674005630715921e-05,
        "epoch": 0.5152395856621209,
        "step": 6914
    },
    {
        "loss": 2.3413,
        "grad_norm": 3.212548017501831,
        "learning_rate": 8.66703152741744e-05,
        "epoch": 0.5153141068634026,
        "step": 6915
    },
    {
        "loss": 2.6679,
        "grad_norm": 1.9604841470718384,
        "learning_rate": 8.660058084115304e-05,
        "epoch": 0.5153886280646844,
        "step": 6916
    },
    {
        "loss": 2.8347,
        "grad_norm": 1.8624523878097534,
        "learning_rate": 8.653085304262272e-05,
        "epoch": 0.5154631492659661,
        "step": 6917
    },
    {
        "loss": 2.7863,
        "grad_norm": 2.587017059326172,
        "learning_rate": 8.64611319131082e-05,
        "epoch": 0.515537670467248,
        "step": 6918
    },
    {
        "loss": 1.9321,
        "grad_norm": 2.9281420707702637,
        "learning_rate": 8.63914174871306e-05,
        "epoch": 0.5156121916685297,
        "step": 6919
    },
    {
        "loss": 2.2727,
        "grad_norm": 2.076604127883911,
        "learning_rate": 8.632170979920765e-05,
        "epoch": 0.5156867128698115,
        "step": 6920
    },
    {
        "loss": 2.4794,
        "grad_norm": 2.8510560989379883,
        "learning_rate": 8.625200888385422e-05,
        "epoch": 0.5157612340710932,
        "step": 6921
    },
    {
        "loss": 2.6501,
        "grad_norm": 1.5454314947128296,
        "learning_rate": 8.618231477558126e-05,
        "epoch": 0.515835755272375,
        "step": 6922
    },
    {
        "loss": 2.6126,
        "grad_norm": 3.124115228652954,
        "learning_rate": 8.611262750889676e-05,
        "epoch": 0.5159102764736567,
        "step": 6923
    },
    {
        "loss": 2.0304,
        "grad_norm": 2.7234880924224854,
        "learning_rate": 8.604294711830514e-05,
        "epoch": 0.5159847976749385,
        "step": 6924
    },
    {
        "loss": 1.4517,
        "grad_norm": 6.528742790222168,
        "learning_rate": 8.597327363830731e-05,
        "epoch": 0.5160593188762203,
        "step": 6925
    },
    {
        "loss": 2.4233,
        "grad_norm": 2.4373276233673096,
        "learning_rate": 8.590360710340106e-05,
        "epoch": 0.5161338400775021,
        "step": 6926
    },
    {
        "loss": 2.8697,
        "grad_norm": 2.0544168949127197,
        "learning_rate": 8.583394754808037e-05,
        "epoch": 0.5162083612787838,
        "step": 6927
    },
    {
        "loss": 1.4243,
        "grad_norm": 5.247896194458008,
        "learning_rate": 8.576429500683623e-05,
        "epoch": 0.5162828824800656,
        "step": 6928
    },
    {
        "loss": 2.1912,
        "grad_norm": 2.886113405227661,
        "learning_rate": 8.569464951415577e-05,
        "epoch": 0.5163574036813473,
        "step": 6929
    },
    {
        "loss": 2.795,
        "grad_norm": 3.9190828800201416,
        "learning_rate": 8.56250111045226e-05,
        "epoch": 0.5164319248826291,
        "step": 6930
    },
    {
        "loss": 2.6801,
        "grad_norm": 1.2462178468704224,
        "learning_rate": 8.555537981241725e-05,
        "epoch": 0.5165064460839108,
        "step": 6931
    },
    {
        "loss": 2.2119,
        "grad_norm": 2.851816177368164,
        "learning_rate": 8.548575567231637e-05,
        "epoch": 0.5165809672851926,
        "step": 6932
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.380039691925049,
        "learning_rate": 8.541613871869301e-05,
        "epoch": 0.5166554884864744,
        "step": 6933
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.6205389499664307,
        "learning_rate": 8.534652898601696e-05,
        "epoch": 0.5167300096877562,
        "step": 6934
    },
    {
        "loss": 1.7162,
        "grad_norm": 2.7482359409332275,
        "learning_rate": 8.527692650875417e-05,
        "epoch": 0.5168045308890379,
        "step": 6935
    },
    {
        "loss": 2.1191,
        "grad_norm": 3.6531059741973877,
        "learning_rate": 8.52073313213672e-05,
        "epoch": 0.5168790520903197,
        "step": 6936
    },
    {
        "loss": 2.4421,
        "grad_norm": 2.8735930919647217,
        "learning_rate": 8.513774345831487e-05,
        "epoch": 0.5169535732916014,
        "step": 6937
    },
    {
        "loss": 2.3629,
        "grad_norm": 2.8588297367095947,
        "learning_rate": 8.506816295405225e-05,
        "epoch": 0.5170280944928832,
        "step": 6938
    },
    {
        "loss": 2.4076,
        "grad_norm": 2.951396942138672,
        "learning_rate": 8.499858984303124e-05,
        "epoch": 0.5171026156941649,
        "step": 6939
    },
    {
        "loss": 1.8837,
        "grad_norm": 3.041255474090576,
        "learning_rate": 8.492902415969945e-05,
        "epoch": 0.5171771368954468,
        "step": 6940
    },
    {
        "loss": 2.5485,
        "grad_norm": 3.2384839057922363,
        "learning_rate": 8.485946593850141e-05,
        "epoch": 0.5172516580967286,
        "step": 6941
    },
    {
        "loss": 2.2808,
        "grad_norm": 2.8473641872406006,
        "learning_rate": 8.478991521387755e-05,
        "epoch": 0.5173261792980103,
        "step": 6942
    },
    {
        "loss": 2.68,
        "grad_norm": 2.8389828205108643,
        "learning_rate": 8.472037202026465e-05,
        "epoch": 0.5174007004992921,
        "step": 6943
    },
    {
        "loss": 2.6195,
        "grad_norm": 2.248865842819214,
        "learning_rate": 8.46508363920959e-05,
        "epoch": 0.5174752217005738,
        "step": 6944
    },
    {
        "loss": 1.8825,
        "grad_norm": 3.4531185626983643,
        "learning_rate": 8.458130836380061e-05,
        "epoch": 0.5175497429018556,
        "step": 6945
    },
    {
        "loss": 2.1884,
        "grad_norm": 2.6182944774627686,
        "learning_rate": 8.451178796980448e-05,
        "epoch": 0.5176242641031373,
        "step": 6946
    },
    {
        "loss": 2.4911,
        "grad_norm": 3.1091132164001465,
        "learning_rate": 8.444227524452925e-05,
        "epoch": 0.5176987853044192,
        "step": 6947
    },
    {
        "loss": 2.4396,
        "grad_norm": 2.680643320083618,
        "learning_rate": 8.437277022239283e-05,
        "epoch": 0.5177733065057009,
        "step": 6948
    },
    {
        "loss": 2.7535,
        "grad_norm": 2.6461029052734375,
        "learning_rate": 8.43032729378097e-05,
        "epoch": 0.5178478277069827,
        "step": 6949
    },
    {
        "loss": 2.7113,
        "grad_norm": 3.8843626976013184,
        "learning_rate": 8.423378342518999e-05,
        "epoch": 0.5179223489082644,
        "step": 6950
    },
    {
        "loss": 2.3215,
        "grad_norm": 2.893120050430298,
        "learning_rate": 8.41643017189405e-05,
        "epoch": 0.5179968701095462,
        "step": 6951
    },
    {
        "loss": 2.384,
        "grad_norm": 3.762190580368042,
        "learning_rate": 8.40948278534637e-05,
        "epoch": 0.5180713913108279,
        "step": 6952
    },
    {
        "loss": 1.9816,
        "grad_norm": 3.202885866165161,
        "learning_rate": 8.40253618631584e-05,
        "epoch": 0.5181459125121097,
        "step": 6953
    },
    {
        "loss": 2.4665,
        "grad_norm": 2.146421432495117,
        "learning_rate": 8.395590378241957e-05,
        "epoch": 0.5182204337133914,
        "step": 6954
    },
    {
        "loss": 2.612,
        "grad_norm": 2.4999067783355713,
        "learning_rate": 8.388645364563804e-05,
        "epoch": 0.5182949549146733,
        "step": 6955
    },
    {
        "loss": 2.3553,
        "grad_norm": 4.076694488525391,
        "learning_rate": 8.381701148720099e-05,
        "epoch": 0.518369476115955,
        "step": 6956
    },
    {
        "loss": 1.3544,
        "grad_norm": 2.6627755165100098,
        "learning_rate": 8.374757734149148e-05,
        "epoch": 0.5184439973172368,
        "step": 6957
    },
    {
        "loss": 2.8054,
        "grad_norm": 2.7112743854522705,
        "learning_rate": 8.367815124288842e-05,
        "epoch": 0.5185185185185185,
        "step": 6958
    },
    {
        "loss": 2.6859,
        "grad_norm": 2.453587293624878,
        "learning_rate": 8.360873322576727e-05,
        "epoch": 0.5185930397198003,
        "step": 6959
    },
    {
        "loss": 2.3848,
        "grad_norm": 2.33424973487854,
        "learning_rate": 8.3539323324499e-05,
        "epoch": 0.518667560921082,
        "step": 6960
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.930911064147949,
        "learning_rate": 8.34699215734506e-05,
        "epoch": 0.5187420821223638,
        "step": 6961
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.4964613914489746,
        "learning_rate": 8.340052800698545e-05,
        "epoch": 0.5188166033236455,
        "step": 6962
    },
    {
        "loss": 2.1635,
        "grad_norm": 2.344967842102051,
        "learning_rate": 8.333114265946232e-05,
        "epoch": 0.5188911245249274,
        "step": 6963
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.9592549800872803,
        "learning_rate": 8.326176556523633e-05,
        "epoch": 0.5189656457262091,
        "step": 6964
    },
    {
        "loss": 2.7284,
        "grad_norm": 1.7006042003631592,
        "learning_rate": 8.319239675865829e-05,
        "epoch": 0.5190401669274909,
        "step": 6965
    },
    {
        "loss": 2.3873,
        "grad_norm": 3.882767677307129,
        "learning_rate": 8.312303627407487e-05,
        "epoch": 0.5191146881287726,
        "step": 6966
    },
    {
        "loss": 2.2956,
        "grad_norm": 2.9176390171051025,
        "learning_rate": 8.305368414582889e-05,
        "epoch": 0.5191892093300544,
        "step": 6967
    },
    {
        "loss": 2.5348,
        "grad_norm": 2.4903202056884766,
        "learning_rate": 8.298434040825863e-05,
        "epoch": 0.5192637305313361,
        "step": 6968
    },
    {
        "loss": 2.3294,
        "grad_norm": 2.0969653129577637,
        "learning_rate": 8.291500509569875e-05,
        "epoch": 0.5193382517326179,
        "step": 6969
    },
    {
        "loss": 2.3996,
        "grad_norm": 3.091200351715088,
        "learning_rate": 8.284567824247924e-05,
        "epoch": 0.5194127729338996,
        "step": 6970
    },
    {
        "loss": 2.4388,
        "grad_norm": 3.2943131923675537,
        "learning_rate": 8.277635988292601e-05,
        "epoch": 0.5194872941351815,
        "step": 6971
    },
    {
        "loss": 1.8589,
        "grad_norm": 2.5207717418670654,
        "learning_rate": 8.27070500513611e-05,
        "epoch": 0.5195618153364632,
        "step": 6972
    },
    {
        "loss": 2.453,
        "grad_norm": 4.5933837890625,
        "learning_rate": 8.26377487821019e-05,
        "epoch": 0.519636336537745,
        "step": 6973
    },
    {
        "loss": 2.2248,
        "grad_norm": 4.715862274169922,
        "learning_rate": 8.256845610946182e-05,
        "epoch": 0.5197108577390267,
        "step": 6974
    },
    {
        "loss": 2.2417,
        "grad_norm": 2.6758792400360107,
        "learning_rate": 8.24991720677499e-05,
        "epoch": 0.5197853789403085,
        "step": 6975
    },
    {
        "loss": 2.3864,
        "grad_norm": 3.2053890228271484,
        "learning_rate": 8.242989669127086e-05,
        "epoch": 0.5198599001415903,
        "step": 6976
    },
    {
        "loss": 2.6059,
        "grad_norm": 2.175178289413452,
        "learning_rate": 8.236063001432537e-05,
        "epoch": 0.519934421342872,
        "step": 6977
    },
    {
        "loss": 2.1696,
        "grad_norm": 3.0947868824005127,
        "learning_rate": 8.229137207120939e-05,
        "epoch": 0.5200089425441539,
        "step": 6978
    },
    {
        "loss": 2.1471,
        "grad_norm": 4.221416473388672,
        "learning_rate": 8.222212289621507e-05,
        "epoch": 0.5200834637454356,
        "step": 6979
    },
    {
        "loss": 2.6379,
        "grad_norm": 2.2427947521209717,
        "learning_rate": 8.215288252362985e-05,
        "epoch": 0.5201579849467174,
        "step": 6980
    },
    {
        "loss": 2.702,
        "grad_norm": 2.065932512283325,
        "learning_rate": 8.208365098773672e-05,
        "epoch": 0.5202325061479991,
        "step": 6981
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.883213520050049,
        "learning_rate": 8.201442832281473e-05,
        "epoch": 0.5203070273492809,
        "step": 6982
    },
    {
        "loss": 2.363,
        "grad_norm": 2.4995627403259277,
        "learning_rate": 8.194521456313817e-05,
        "epoch": 0.5203815485505626,
        "step": 6983
    },
    {
        "loss": 2.8492,
        "grad_norm": 2.3204843997955322,
        "learning_rate": 8.187600974297714e-05,
        "epoch": 0.5204560697518444,
        "step": 6984
    },
    {
        "loss": 2.2615,
        "grad_norm": 2.5516936779022217,
        "learning_rate": 8.18068138965971e-05,
        "epoch": 0.5205305909531261,
        "step": 6985
    },
    {
        "loss": 2.5917,
        "grad_norm": 3.124633550643921,
        "learning_rate": 8.173762705825915e-05,
        "epoch": 0.520605112154408,
        "step": 6986
    },
    {
        "loss": 2.4472,
        "grad_norm": 2.080742120742798,
        "learning_rate": 8.16684492622201e-05,
        "epoch": 0.5206796333556897,
        "step": 6987
    },
    {
        "loss": 2.1552,
        "grad_norm": 3.710864782333374,
        "learning_rate": 8.159928054273191e-05,
        "epoch": 0.5207541545569715,
        "step": 6988
    },
    {
        "loss": 2.2543,
        "grad_norm": 3.0631003379821777,
        "learning_rate": 8.153012093404259e-05,
        "epoch": 0.5208286757582532,
        "step": 6989
    },
    {
        "loss": 2.476,
        "grad_norm": 2.4984829425811768,
        "learning_rate": 8.146097047039516e-05,
        "epoch": 0.520903196959535,
        "step": 6990
    },
    {
        "loss": 2.29,
        "grad_norm": 2.7777609825134277,
        "learning_rate": 8.139182918602814e-05,
        "epoch": 0.5209777181608167,
        "step": 6991
    },
    {
        "loss": 2.0359,
        "grad_norm": 2.951122999191284,
        "learning_rate": 8.132269711517596e-05,
        "epoch": 0.5210522393620985,
        "step": 6992
    },
    {
        "loss": 2.5664,
        "grad_norm": 2.2358851432800293,
        "learning_rate": 8.125357429206796e-05,
        "epoch": 0.5211267605633803,
        "step": 6993
    },
    {
        "loss": 2.8649,
        "grad_norm": 2.582965612411499,
        "learning_rate": 8.118446075092911e-05,
        "epoch": 0.5212012817646621,
        "step": 6994
    },
    {
        "loss": 2.0509,
        "grad_norm": 3.126511573791504,
        "learning_rate": 8.111535652597991e-05,
        "epoch": 0.5212758029659438,
        "step": 6995
    },
    {
        "loss": 1.9022,
        "grad_norm": 3.807426691055298,
        "learning_rate": 8.104626165143599e-05,
        "epoch": 0.5213503241672256,
        "step": 6996
    },
    {
        "loss": 2.7316,
        "grad_norm": 3.0554301738739014,
        "learning_rate": 8.097717616150858e-05,
        "epoch": 0.5214248453685073,
        "step": 6997
    },
    {
        "loss": 2.8657,
        "grad_norm": 2.7622690200805664,
        "learning_rate": 8.090810009040412e-05,
        "epoch": 0.5214993665697891,
        "step": 6998
    },
    {
        "loss": 2.139,
        "grad_norm": 3.087636947631836,
        "learning_rate": 8.083903347232433e-05,
        "epoch": 0.5215738877710708,
        "step": 6999
    },
    {
        "loss": 2.8689,
        "grad_norm": 2.36940336227417,
        "learning_rate": 8.076997634146663e-05,
        "epoch": 0.5216484089723527,
        "step": 7000
    },
    {
        "loss": 1.4858,
        "grad_norm": 3.882560968399048,
        "learning_rate": 8.070092873202314e-05,
        "epoch": 0.5217229301736344,
        "step": 7001
    },
    {
        "loss": 1.9146,
        "grad_norm": 4.122625827789307,
        "learning_rate": 8.063189067818189e-05,
        "epoch": 0.5217974513749162,
        "step": 7002
    },
    {
        "loss": 2.8068,
        "grad_norm": 3.50319766998291,
        "learning_rate": 8.056286221412577e-05,
        "epoch": 0.5218719725761979,
        "step": 7003
    },
    {
        "loss": 1.3503,
        "grad_norm": 3.56355881690979,
        "learning_rate": 8.049384337403294e-05,
        "epoch": 0.5219464937774797,
        "step": 7004
    },
    {
        "loss": 1.9997,
        "grad_norm": 2.456475257873535,
        "learning_rate": 8.042483419207702e-05,
        "epoch": 0.5220210149787614,
        "step": 7005
    },
    {
        "loss": 2.5223,
        "grad_norm": 2.4516122341156006,
        "learning_rate": 8.035583470242657e-05,
        "epoch": 0.5220955361800432,
        "step": 7006
    },
    {
        "loss": 2.3261,
        "grad_norm": 2.8339552879333496,
        "learning_rate": 8.02868449392457e-05,
        "epoch": 0.5221700573813249,
        "step": 7007
    },
    {
        "loss": 2.8275,
        "grad_norm": 2.803136110305786,
        "learning_rate": 8.021786493669335e-05,
        "epoch": 0.5222445785826068,
        "step": 7008
    },
    {
        "loss": 2.708,
        "grad_norm": 3.0074596405029297,
        "learning_rate": 8.014889472892366e-05,
        "epoch": 0.5223190997838885,
        "step": 7009
    },
    {
        "loss": 2.7256,
        "grad_norm": 2.2549211978912354,
        "learning_rate": 8.007993435008633e-05,
        "epoch": 0.5223936209851703,
        "step": 7010
    },
    {
        "loss": 2.4003,
        "grad_norm": 2.9871602058410645,
        "learning_rate": 8.001098383432567e-05,
        "epoch": 0.5224681421864521,
        "step": 7011
    },
    {
        "loss": 2.1661,
        "grad_norm": 3.1812541484832764,
        "learning_rate": 7.994204321578153e-05,
        "epoch": 0.5225426633877338,
        "step": 7012
    },
    {
        "loss": 2.6271,
        "grad_norm": 3.2239725589752197,
        "learning_rate": 7.987311252858855e-05,
        "epoch": 0.5226171845890156,
        "step": 7013
    },
    {
        "loss": 1.9346,
        "grad_norm": 2.5423507690429688,
        "learning_rate": 7.980419180687654e-05,
        "epoch": 0.5226917057902973,
        "step": 7014
    },
    {
        "loss": 2.1562,
        "grad_norm": 2.6111526489257812,
        "learning_rate": 7.973528108477053e-05,
        "epoch": 0.5227662269915792,
        "step": 7015
    },
    {
        "loss": 1.8453,
        "grad_norm": 3.7236387729644775,
        "learning_rate": 7.966638039639031e-05,
        "epoch": 0.5228407481928609,
        "step": 7016
    },
    {
        "loss": 2.2681,
        "grad_norm": 1.8713288307189941,
        "learning_rate": 7.959748977585104e-05,
        "epoch": 0.5229152693941427,
        "step": 7017
    },
    {
        "loss": 2.6407,
        "grad_norm": 1.7375837564468384,
        "learning_rate": 7.952860925726263e-05,
        "epoch": 0.5229897905954244,
        "step": 7018
    },
    {
        "loss": 1.7454,
        "grad_norm": 2.85758638381958,
        "learning_rate": 7.945973887472995e-05,
        "epoch": 0.5230643117967062,
        "step": 7019
    },
    {
        "loss": 3.2851,
        "grad_norm": 2.108077049255371,
        "learning_rate": 7.939087866235324e-05,
        "epoch": 0.5231388329979879,
        "step": 7020
    },
    {
        "loss": 2.4427,
        "grad_norm": 3.1264336109161377,
        "learning_rate": 7.93220286542272e-05,
        "epoch": 0.5232133541992697,
        "step": 7021
    },
    {
        "loss": 2.767,
        "grad_norm": 4.355632781982422,
        "learning_rate": 7.925318888444196e-05,
        "epoch": 0.5232878754005514,
        "step": 7022
    },
    {
        "loss": 2.7034,
        "grad_norm": 2.578212261199951,
        "learning_rate": 7.918435938708227e-05,
        "epoch": 0.5233623966018333,
        "step": 7023
    },
    {
        "loss": 2.1699,
        "grad_norm": 4.065328121185303,
        "learning_rate": 7.911554019622775e-05,
        "epoch": 0.523436917803115,
        "step": 7024
    },
    {
        "loss": 2.2549,
        "grad_norm": 2.6285760402679443,
        "learning_rate": 7.904673134595316e-05,
        "epoch": 0.5235114390043968,
        "step": 7025
    },
    {
        "loss": 2.6162,
        "grad_norm": 2.5497567653656006,
        "learning_rate": 7.897793287032801e-05,
        "epoch": 0.5235859602056785,
        "step": 7026
    },
    {
        "loss": 2.33,
        "grad_norm": 2.371619701385498,
        "learning_rate": 7.890914480341653e-05,
        "epoch": 0.5236604814069603,
        "step": 7027
    },
    {
        "loss": 1.7787,
        "grad_norm": 4.018857479095459,
        "learning_rate": 7.884036717927812e-05,
        "epoch": 0.523735002608242,
        "step": 7028
    },
    {
        "loss": 2.8348,
        "grad_norm": 2.064598321914673,
        "learning_rate": 7.877160003196665e-05,
        "epoch": 0.5238095238095238,
        "step": 7029
    },
    {
        "loss": 1.4945,
        "grad_norm": 2.905219554901123,
        "learning_rate": 7.870284339553125e-05,
        "epoch": 0.5238840450108055,
        "step": 7030
    },
    {
        "loss": 2.4721,
        "grad_norm": 2.1625475883483887,
        "learning_rate": 7.863409730401541e-05,
        "epoch": 0.5239585662120874,
        "step": 7031
    },
    {
        "loss": 2.6311,
        "grad_norm": 2.5120797157287598,
        "learning_rate": 7.856536179145746e-05,
        "epoch": 0.5240330874133691,
        "step": 7032
    },
    {
        "loss": 2.7042,
        "grad_norm": 3.416921377182007,
        "learning_rate": 7.849663689189086e-05,
        "epoch": 0.5241076086146509,
        "step": 7033
    },
    {
        "loss": 1.8669,
        "grad_norm": 3.305067539215088,
        "learning_rate": 7.84279226393434e-05,
        "epoch": 0.5241821298159326,
        "step": 7034
    },
    {
        "loss": 2.5586,
        "grad_norm": 2.6981053352355957,
        "learning_rate": 7.835921906783783e-05,
        "epoch": 0.5242566510172144,
        "step": 7035
    },
    {
        "loss": 1.5451,
        "grad_norm": 2.3312551975250244,
        "learning_rate": 7.82905262113915e-05,
        "epoch": 0.5243311722184961,
        "step": 7036
    },
    {
        "loss": 1.5746,
        "grad_norm": 4.368781089782715,
        "learning_rate": 7.822184410401646e-05,
        "epoch": 0.5244056934197779,
        "step": 7037
    },
    {
        "loss": 2.453,
        "grad_norm": 2.019744634628296,
        "learning_rate": 7.815317277971954e-05,
        "epoch": 0.5244802146210596,
        "step": 7038
    },
    {
        "loss": 2.2706,
        "grad_norm": 4.371212959289551,
        "learning_rate": 7.808451227250199e-05,
        "epoch": 0.5245547358223415,
        "step": 7039
    },
    {
        "loss": 1.8552,
        "grad_norm": 4.147506237030029,
        "learning_rate": 7.801586261636014e-05,
        "epoch": 0.5246292570236232,
        "step": 7040
    },
    {
        "loss": 1.4159,
        "grad_norm": 2.7396419048309326,
        "learning_rate": 7.794722384528457e-05,
        "epoch": 0.524703778224905,
        "step": 7041
    },
    {
        "loss": 2.3627,
        "grad_norm": 3.1797430515289307,
        "learning_rate": 7.787859599326042e-05,
        "epoch": 0.5247782994261867,
        "step": 7042
    },
    {
        "loss": 1.851,
        "grad_norm": 2.6808228492736816,
        "learning_rate": 7.780997909426786e-05,
        "epoch": 0.5248528206274685,
        "step": 7043
    },
    {
        "loss": 2.7773,
        "grad_norm": 4.234044551849365,
        "learning_rate": 7.774137318228117e-05,
        "epoch": 0.5249273418287502,
        "step": 7044
    },
    {
        "loss": 2.548,
        "grad_norm": 2.7050092220306396,
        "learning_rate": 7.767277829126952e-05,
        "epoch": 0.525001863030032,
        "step": 7045
    },
    {
        "loss": 2.0677,
        "grad_norm": 4.0304436683654785,
        "learning_rate": 7.760419445519641e-05,
        "epoch": 0.5250763842313139,
        "step": 7046
    },
    {
        "loss": 2.2232,
        "grad_norm": 3.12445068359375,
        "learning_rate": 7.753562170801987e-05,
        "epoch": 0.5251509054325956,
        "step": 7047
    },
    {
        "loss": 2.4782,
        "grad_norm": 3.1193675994873047,
        "learning_rate": 7.746706008369268e-05,
        "epoch": 0.5252254266338774,
        "step": 7048
    },
    {
        "loss": 2.1828,
        "grad_norm": 2.7479259967803955,
        "learning_rate": 7.739850961616172e-05,
        "epoch": 0.5252999478351591,
        "step": 7049
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.029069662094116,
        "learning_rate": 7.732997033936882e-05,
        "epoch": 0.5253744690364409,
        "step": 7050
    },
    {
        "loss": 2.1497,
        "grad_norm": 4.227036952972412,
        "learning_rate": 7.72614422872499e-05,
        "epoch": 0.5254489902377226,
        "step": 7051
    },
    {
        "loss": 1.979,
        "grad_norm": 4.412624359130859,
        "learning_rate": 7.719292549373533e-05,
        "epoch": 0.5255235114390044,
        "step": 7052
    },
    {
        "loss": 2.5529,
        "grad_norm": 2.2025067806243896,
        "learning_rate": 7.71244199927503e-05,
        "epoch": 0.5255980326402862,
        "step": 7053
    },
    {
        "loss": 2.7904,
        "grad_norm": 2.956170082092285,
        "learning_rate": 7.705592581821392e-05,
        "epoch": 0.525672553841568,
        "step": 7054
    },
    {
        "loss": 1.9755,
        "grad_norm": 3.34930682182312,
        "learning_rate": 7.698744300403989e-05,
        "epoch": 0.5257470750428497,
        "step": 7055
    },
    {
        "loss": 2.6504,
        "grad_norm": 2.8837902545928955,
        "learning_rate": 7.691897158413644e-05,
        "epoch": 0.5258215962441315,
        "step": 7056
    },
    {
        "loss": 2.3188,
        "grad_norm": 2.0893301963806152,
        "learning_rate": 7.685051159240582e-05,
        "epoch": 0.5258961174454132,
        "step": 7057
    },
    {
        "loss": 2.8796,
        "grad_norm": 2.07476544380188,
        "learning_rate": 7.678206306274497e-05,
        "epoch": 0.525970638646695,
        "step": 7058
    },
    {
        "loss": 2.577,
        "grad_norm": 3.9050674438476562,
        "learning_rate": 7.671362602904498e-05,
        "epoch": 0.5260451598479767,
        "step": 7059
    },
    {
        "loss": 2.6592,
        "grad_norm": 2.3564345836639404,
        "learning_rate": 7.664520052519105e-05,
        "epoch": 0.5261196810492585,
        "step": 7060
    },
    {
        "loss": 1.8744,
        "grad_norm": 3.5608971118927,
        "learning_rate": 7.65767865850632e-05,
        "epoch": 0.5261942022505403,
        "step": 7061
    },
    {
        "loss": 2.3295,
        "grad_norm": 2.5962769985198975,
        "learning_rate": 7.650838424253516e-05,
        "epoch": 0.5262687234518221,
        "step": 7062
    },
    {
        "loss": 2.7441,
        "grad_norm": 2.5061490535736084,
        "learning_rate": 7.64399935314754e-05,
        "epoch": 0.5263432446531038,
        "step": 7063
    },
    {
        "loss": 1.9454,
        "grad_norm": 3.5362629890441895,
        "learning_rate": 7.637161448574631e-05,
        "epoch": 0.5264177658543856,
        "step": 7064
    },
    {
        "loss": 2.018,
        "grad_norm": 1.758235216140747,
        "learning_rate": 7.630324713920449e-05,
        "epoch": 0.5264922870556673,
        "step": 7065
    },
    {
        "loss": 2.9081,
        "grad_norm": 3.635812520980835,
        "learning_rate": 7.623489152570099e-05,
        "epoch": 0.5265668082569491,
        "step": 7066
    },
    {
        "loss": 2.1831,
        "grad_norm": 3.272289514541626,
        "learning_rate": 7.616654767908079e-05,
        "epoch": 0.5266413294582308,
        "step": 7067
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.8197944164276123,
        "learning_rate": 7.609821563318331e-05,
        "epoch": 0.5267158506595127,
        "step": 7068
    },
    {
        "loss": 2.1218,
        "grad_norm": 3.5154178142547607,
        "learning_rate": 7.602989542184188e-05,
        "epoch": 0.5267903718607944,
        "step": 7069
    },
    {
        "loss": 2.5236,
        "grad_norm": 3.81801700592041,
        "learning_rate": 7.596158707888399e-05,
        "epoch": 0.5268648930620762,
        "step": 7070
    },
    {
        "loss": 2.0834,
        "grad_norm": 2.9980900287628174,
        "learning_rate": 7.589329063813159e-05,
        "epoch": 0.5269394142633579,
        "step": 7071
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.9858641624450684,
        "learning_rate": 7.582500613340018e-05,
        "epoch": 0.5270139354646397,
        "step": 7072
    },
    {
        "loss": 2.8847,
        "grad_norm": 3.6543123722076416,
        "learning_rate": 7.575673359849994e-05,
        "epoch": 0.5270884566659214,
        "step": 7073
    },
    {
        "loss": 2.3448,
        "grad_norm": 2.446603775024414,
        "learning_rate": 7.568847306723472e-05,
        "epoch": 0.5271629778672032,
        "step": 7074
    },
    {
        "loss": 2.3226,
        "grad_norm": 4.171216011047363,
        "learning_rate": 7.562022457340247e-05,
        "epoch": 0.5272374990684849,
        "step": 7075
    },
    {
        "loss": 1.5943,
        "grad_norm": 5.088870525360107,
        "learning_rate": 7.555198815079538e-05,
        "epoch": 0.5273120202697668,
        "step": 7076
    },
    {
        "loss": 2.6851,
        "grad_norm": 2.81099534034729,
        "learning_rate": 7.548376383319942e-05,
        "epoch": 0.5273865414710485,
        "step": 7077
    },
    {
        "loss": 2.5508,
        "grad_norm": 2.956129312515259,
        "learning_rate": 7.54155516543948e-05,
        "epoch": 0.5274610626723303,
        "step": 7078
    },
    {
        "loss": 2.239,
        "grad_norm": 2.3821351528167725,
        "learning_rate": 7.534735164815554e-05,
        "epoch": 0.527535583873612,
        "step": 7079
    },
    {
        "loss": 2.2731,
        "grad_norm": 2.7577221393585205,
        "learning_rate": 7.527916384824957e-05,
        "epoch": 0.5276101050748938,
        "step": 7080
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.9286131858825684,
        "learning_rate": 7.521098828843916e-05,
        "epoch": 0.5276846262761755,
        "step": 7081
    },
    {
        "loss": 2.2184,
        "grad_norm": 2.9178707599639893,
        "learning_rate": 7.514282500248e-05,
        "epoch": 0.5277591474774573,
        "step": 7082
    },
    {
        "loss": 2.0166,
        "grad_norm": 3.029261350631714,
        "learning_rate": 7.507467402412223e-05,
        "epoch": 0.5278336686787392,
        "step": 7083
    },
    {
        "loss": 2.3129,
        "grad_norm": 3.4027466773986816,
        "learning_rate": 7.500653538710952e-05,
        "epoch": 0.5279081898800209,
        "step": 7084
    },
    {
        "loss": 1.7076,
        "grad_norm": 4.283054828643799,
        "learning_rate": 7.493840912517942e-05,
        "epoch": 0.5279827110813027,
        "step": 7085
    },
    {
        "loss": 2.2469,
        "grad_norm": 3.111609697341919,
        "learning_rate": 7.487029527206366e-05,
        "epoch": 0.5280572322825844,
        "step": 7086
    },
    {
        "loss": 2.297,
        "grad_norm": 3.622547149658203,
        "learning_rate": 7.480219386148755e-05,
        "epoch": 0.5281317534838662,
        "step": 7087
    },
    {
        "loss": 2.5579,
        "grad_norm": 2.511340618133545,
        "learning_rate": 7.473410492717024e-05,
        "epoch": 0.5282062746851479,
        "step": 7088
    },
    {
        "loss": 1.4959,
        "grad_norm": 3.4315097332000732,
        "learning_rate": 7.466602850282496e-05,
        "epoch": 0.5282807958864297,
        "step": 7089
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.6660499572753906,
        "learning_rate": 7.459796462215839e-05,
        "epoch": 0.5283553170877114,
        "step": 7090
    },
    {
        "loss": 2.7162,
        "grad_norm": 3.1516945362091064,
        "learning_rate": 7.452991331887141e-05,
        "epoch": 0.5284298382889933,
        "step": 7091
    },
    {
        "loss": 1.7749,
        "grad_norm": 3.521397113800049,
        "learning_rate": 7.44618746266583e-05,
        "epoch": 0.528504359490275,
        "step": 7092
    },
    {
        "loss": 2.0837,
        "grad_norm": 3.3587987422943115,
        "learning_rate": 7.439384857920716e-05,
        "epoch": 0.5285788806915568,
        "step": 7093
    },
    {
        "loss": 2.2578,
        "grad_norm": 1.7940330505371094,
        "learning_rate": 7.43258352102002e-05,
        "epoch": 0.5286534018928385,
        "step": 7094
    },
    {
        "loss": 2.2243,
        "grad_norm": 2.7673795223236084,
        "learning_rate": 7.425783455331279e-05,
        "epoch": 0.5287279230941203,
        "step": 7095
    },
    {
        "loss": 2.5539,
        "grad_norm": 1.9534542560577393,
        "learning_rate": 7.418984664221447e-05,
        "epoch": 0.528802444295402,
        "step": 7096
    },
    {
        "loss": 1.6412,
        "grad_norm": 3.633460521697998,
        "learning_rate": 7.41218715105682e-05,
        "epoch": 0.5288769654966838,
        "step": 7097
    },
    {
        "loss": 2.3028,
        "grad_norm": 2.616575241088867,
        "learning_rate": 7.405390919203061e-05,
        "epoch": 0.5289514866979655,
        "step": 7098
    },
    {
        "loss": 2.6084,
        "grad_norm": 4.038024425506592,
        "learning_rate": 7.398595972025222e-05,
        "epoch": 0.5290260078992474,
        "step": 7099
    },
    {
        "loss": 1.9067,
        "grad_norm": 3.8819665908813477,
        "learning_rate": 7.391802312887683e-05,
        "epoch": 0.5291005291005291,
        "step": 7100
    },
    {
        "loss": 2.9077,
        "grad_norm": 3.0903990268707275,
        "learning_rate": 7.385009945154236e-05,
        "epoch": 0.5291750503018109,
        "step": 7101
    },
    {
        "loss": 2.1937,
        "grad_norm": 4.240517616271973,
        "learning_rate": 7.378218872187991e-05,
        "epoch": 0.5292495715030926,
        "step": 7102
    },
    {
        "loss": 2.6714,
        "grad_norm": 3.20052170753479,
        "learning_rate": 7.371429097351414e-05,
        "epoch": 0.5293240927043744,
        "step": 7103
    },
    {
        "loss": 1.9326,
        "grad_norm": 3.579082489013672,
        "learning_rate": 7.364640624006373e-05,
        "epoch": 0.5293986139056561,
        "step": 7104
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.270514726638794,
        "learning_rate": 7.357853455514043e-05,
        "epoch": 0.5294731351069379,
        "step": 7105
    },
    {
        "loss": 2.0473,
        "grad_norm": 3.19893217086792,
        "learning_rate": 7.35106759523499e-05,
        "epoch": 0.5295476563082196,
        "step": 7106
    },
    {
        "loss": 1.9319,
        "grad_norm": 3.2220845222473145,
        "learning_rate": 7.344283046529105e-05,
        "epoch": 0.5296221775095015,
        "step": 7107
    },
    {
        "loss": 2.5302,
        "grad_norm": 2.296588897705078,
        "learning_rate": 7.337499812755637e-05,
        "epoch": 0.5296966987107832,
        "step": 7108
    },
    {
        "loss": 2.3738,
        "grad_norm": 3.8090925216674805,
        "learning_rate": 7.330717897273199e-05,
        "epoch": 0.529771219912065,
        "step": 7109
    },
    {
        "loss": 2.7246,
        "grad_norm": 2.410648822784424,
        "learning_rate": 7.323937303439716e-05,
        "epoch": 0.5298457411133467,
        "step": 7110
    },
    {
        "loss": 2.5761,
        "grad_norm": 2.6423728466033936,
        "learning_rate": 7.317158034612513e-05,
        "epoch": 0.5299202623146285,
        "step": 7111
    },
    {
        "loss": 2.8462,
        "grad_norm": 2.1613175868988037,
        "learning_rate": 7.310380094148213e-05,
        "epoch": 0.5299947835159102,
        "step": 7112
    },
    {
        "loss": 1.7912,
        "grad_norm": 3.031810998916626,
        "learning_rate": 7.303603485402785e-05,
        "epoch": 0.530069304717192,
        "step": 7113
    },
    {
        "loss": 2.6652,
        "grad_norm": 2.2526097297668457,
        "learning_rate": 7.296828211731572e-05,
        "epoch": 0.5301438259184738,
        "step": 7114
    },
    {
        "loss": 2.159,
        "grad_norm": 3.9781129360198975,
        "learning_rate": 7.290054276489218e-05,
        "epoch": 0.5302183471197556,
        "step": 7115
    },
    {
        "loss": 1.958,
        "grad_norm": 3.175874948501587,
        "learning_rate": 7.283281683029731e-05,
        "epoch": 0.5302928683210373,
        "step": 7116
    },
    {
        "loss": 1.9896,
        "grad_norm": 2.815908193588257,
        "learning_rate": 7.276510434706441e-05,
        "epoch": 0.5303673895223191,
        "step": 7117
    },
    {
        "loss": 1.6349,
        "grad_norm": 3.240684747695923,
        "learning_rate": 7.269740534872003e-05,
        "epoch": 0.5304419107236009,
        "step": 7118
    },
    {
        "loss": 2.4534,
        "grad_norm": 2.825451374053955,
        "learning_rate": 7.262971986878433e-05,
        "epoch": 0.5305164319248826,
        "step": 7119
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.7783896923065186,
        "learning_rate": 7.256204794077052e-05,
        "epoch": 0.5305909531261644,
        "step": 7120
    },
    {
        "loss": 2.417,
        "grad_norm": 3.7906811237335205,
        "learning_rate": 7.249438959818508e-05,
        "epoch": 0.5306654743274462,
        "step": 7121
    },
    {
        "loss": 2.2247,
        "grad_norm": 3.695197820663452,
        "learning_rate": 7.24267448745281e-05,
        "epoch": 0.530739995528728,
        "step": 7122
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.9029910564422607,
        "learning_rate": 7.235911380329245e-05,
        "epoch": 0.5308145167300097,
        "step": 7123
    },
    {
        "loss": 1.9788,
        "grad_norm": 3.4171221256256104,
        "learning_rate": 7.229149641796478e-05,
        "epoch": 0.5308890379312915,
        "step": 7124
    },
    {
        "loss": 2.6358,
        "grad_norm": 2.0511603355407715,
        "learning_rate": 7.222389275202448e-05,
        "epoch": 0.5309635591325732,
        "step": 7125
    },
    {
        "loss": 2.1828,
        "grad_norm": 3.4394474029541016,
        "learning_rate": 7.215630283894432e-05,
        "epoch": 0.531038080333855,
        "step": 7126
    },
    {
        "loss": 2.6072,
        "grad_norm": 2.4345016479492188,
        "learning_rate": 7.208872671219038e-05,
        "epoch": 0.5311126015351367,
        "step": 7127
    },
    {
        "loss": 2.6932,
        "grad_norm": 3.2437033653259277,
        "learning_rate": 7.202116440522173e-05,
        "epoch": 0.5311871227364185,
        "step": 7128
    },
    {
        "loss": 2.7933,
        "grad_norm": 2.173555612564087,
        "learning_rate": 7.195361595149076e-05,
        "epoch": 0.5312616439377003,
        "step": 7129
    },
    {
        "loss": 2.8307,
        "grad_norm": 2.7350056171417236,
        "learning_rate": 7.188608138444288e-05,
        "epoch": 0.5313361651389821,
        "step": 7130
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.5319528579711914,
        "learning_rate": 7.181856073751651e-05,
        "epoch": 0.5314106863402638,
        "step": 7131
    },
    {
        "loss": 1.6214,
        "grad_norm": 2.2735166549682617,
        "learning_rate": 7.175105404414366e-05,
        "epoch": 0.5314852075415456,
        "step": 7132
    },
    {
        "loss": 2.3296,
        "grad_norm": 3.0371525287628174,
        "learning_rate": 7.168356133774878e-05,
        "epoch": 0.5315597287428273,
        "step": 7133
    },
    {
        "loss": 1.8113,
        "grad_norm": 2.1962568759918213,
        "learning_rate": 7.161608265174999e-05,
        "epoch": 0.5316342499441091,
        "step": 7134
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.8744423389434814,
        "learning_rate": 7.154861801955807e-05,
        "epoch": 0.5317087711453908,
        "step": 7135
    },
    {
        "loss": 2.4015,
        "grad_norm": 2.583374500274658,
        "learning_rate": 7.148116747457693e-05,
        "epoch": 0.5317832923466727,
        "step": 7136
    },
    {
        "loss": 2.0945,
        "grad_norm": 3.7621593475341797,
        "learning_rate": 7.141373105020362e-05,
        "epoch": 0.5318578135479544,
        "step": 7137
    },
    {
        "loss": 2.3891,
        "grad_norm": 3.9548768997192383,
        "learning_rate": 7.134630877982805e-05,
        "epoch": 0.5319323347492362,
        "step": 7138
    },
    {
        "loss": 2.6049,
        "grad_norm": 2.994760036468506,
        "learning_rate": 7.12789006968333e-05,
        "epoch": 0.5320068559505179,
        "step": 7139
    },
    {
        "loss": 1.95,
        "grad_norm": 3.5150225162506104,
        "learning_rate": 7.121150683459523e-05,
        "epoch": 0.5320813771517997,
        "step": 7140
    },
    {
        "loss": 3.2646,
        "grad_norm": 2.8901875019073486,
        "learning_rate": 7.114412722648264e-05,
        "epoch": 0.5321558983530814,
        "step": 7141
    },
    {
        "loss": 2.4019,
        "grad_norm": 2.321624279022217,
        "learning_rate": 7.107676190585764e-05,
        "epoch": 0.5322304195543632,
        "step": 7142
    },
    {
        "loss": 2.1808,
        "grad_norm": 3.2847704887390137,
        "learning_rate": 7.100941090607475e-05,
        "epoch": 0.5323049407556449,
        "step": 7143
    },
    {
        "loss": 2.1505,
        "grad_norm": 3.3774402141571045,
        "learning_rate": 7.094207426048193e-05,
        "epoch": 0.5323794619569268,
        "step": 7144
    },
    {
        "loss": 2.7784,
        "grad_norm": 3.088013172149658,
        "learning_rate": 7.087475200241958e-05,
        "epoch": 0.5324539831582085,
        "step": 7145
    },
    {
        "loss": 2.0918,
        "grad_norm": 3.1722400188446045,
        "learning_rate": 7.080744416522112e-05,
        "epoch": 0.5325285043594903,
        "step": 7146
    },
    {
        "loss": 0.9739,
        "grad_norm": 1.4759818315505981,
        "learning_rate": 7.074015078221303e-05,
        "epoch": 0.532603025560772,
        "step": 7147
    },
    {
        "loss": 1.3313,
        "grad_norm": 5.512397766113281,
        "learning_rate": 7.067287188671432e-05,
        "epoch": 0.5326775467620538,
        "step": 7148
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.1517131328582764,
        "learning_rate": 7.060560751203697e-05,
        "epoch": 0.5327520679633355,
        "step": 7149
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.8183218240737915,
        "learning_rate": 7.05383576914859e-05,
        "epoch": 0.5328265891646173,
        "step": 7150
    },
    {
        "loss": 2.556,
        "grad_norm": 2.3714840412139893,
        "learning_rate": 7.047112245835847e-05,
        "epoch": 0.532901110365899,
        "step": 7151
    },
    {
        "loss": 2.9936,
        "grad_norm": 2.167611598968506,
        "learning_rate": 7.040390184594533e-05,
        "epoch": 0.5329756315671809,
        "step": 7152
    },
    {
        "loss": 2.8932,
        "grad_norm": 2.687356948852539,
        "learning_rate": 7.033669588752947e-05,
        "epoch": 0.5330501527684627,
        "step": 7153
    },
    {
        "loss": 2.8779,
        "grad_norm": 2.4492578506469727,
        "learning_rate": 7.026950461638664e-05,
        "epoch": 0.5331246739697444,
        "step": 7154
    },
    {
        "loss": 2.8097,
        "grad_norm": 2.0594921112060547,
        "learning_rate": 7.020232806578571e-05,
        "epoch": 0.5331991951710262,
        "step": 7155
    },
    {
        "loss": 2.179,
        "grad_norm": 1.9357229471206665,
        "learning_rate": 7.013516626898775e-05,
        "epoch": 0.5332737163723079,
        "step": 7156
    },
    {
        "loss": 2.5211,
        "grad_norm": 3.5872199535369873,
        "learning_rate": 7.006801925924693e-05,
        "epoch": 0.5333482375735897,
        "step": 7157
    },
    {
        "loss": 1.6871,
        "grad_norm": 4.050198554992676,
        "learning_rate": 7.000088706980987e-05,
        "epoch": 0.5334227587748714,
        "step": 7158
    },
    {
        "loss": 2.1207,
        "grad_norm": 3.2462873458862305,
        "learning_rate": 6.993376973391583e-05,
        "epoch": 0.5334972799761533,
        "step": 7159
    },
    {
        "loss": 2.3662,
        "grad_norm": 2.762854814529419,
        "learning_rate": 6.986666728479694e-05,
        "epoch": 0.533571801177435,
        "step": 7160
    },
    {
        "loss": 2.2322,
        "grad_norm": 2.7323670387268066,
        "learning_rate": 6.979957975567762e-05,
        "epoch": 0.5336463223787168,
        "step": 7161
    },
    {
        "loss": 1.4529,
        "grad_norm": 4.11420202255249,
        "learning_rate": 6.973250717977538e-05,
        "epoch": 0.5337208435799985,
        "step": 7162
    },
    {
        "loss": 2.6772,
        "grad_norm": 2.593620777130127,
        "learning_rate": 6.96654495902999e-05,
        "epoch": 0.5337953647812803,
        "step": 7163
    },
    {
        "loss": 2.2253,
        "grad_norm": 4.874086856842041,
        "learning_rate": 6.959840702045345e-05,
        "epoch": 0.533869885982562,
        "step": 7164
    },
    {
        "loss": 2.7922,
        "grad_norm": 2.26893949508667,
        "learning_rate": 6.95313795034313e-05,
        "epoch": 0.5339444071838438,
        "step": 7165
    },
    {
        "loss": 3.012,
        "grad_norm": 3.005140542984009,
        "learning_rate": 6.946436707242074e-05,
        "epoch": 0.5340189283851255,
        "step": 7166
    },
    {
        "loss": 2.8592,
        "grad_norm": 3.3792262077331543,
        "learning_rate": 6.939736976060197e-05,
        "epoch": 0.5340934495864074,
        "step": 7167
    },
    {
        "loss": 2.1928,
        "grad_norm": 3.835231065750122,
        "learning_rate": 6.933038760114746e-05,
        "epoch": 0.5341679707876891,
        "step": 7168
    },
    {
        "loss": 1.9334,
        "grad_norm": 2.461118459701538,
        "learning_rate": 6.926342062722223e-05,
        "epoch": 0.5342424919889709,
        "step": 7169
    },
    {
        "loss": 2.5238,
        "grad_norm": 3.346933364868164,
        "learning_rate": 6.919646887198393e-05,
        "epoch": 0.5343170131902526,
        "step": 7170
    },
    {
        "loss": 2.228,
        "grad_norm": 2.527355909347534,
        "learning_rate": 6.91295323685824e-05,
        "epoch": 0.5343915343915344,
        "step": 7171
    },
    {
        "loss": 2.1522,
        "grad_norm": 2.405411958694458,
        "learning_rate": 6.906261115016032e-05,
        "epoch": 0.5344660555928161,
        "step": 7172
    },
    {
        "loss": 2.1361,
        "grad_norm": 4.1400957107543945,
        "learning_rate": 6.899570524985248e-05,
        "epoch": 0.5345405767940979,
        "step": 7173
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.664637804031372,
        "learning_rate": 6.892881470078604e-05,
        "epoch": 0.5346150979953797,
        "step": 7174
    },
    {
        "loss": 2.743,
        "grad_norm": 3.547731637954712,
        "learning_rate": 6.886193953608094e-05,
        "epoch": 0.5346896191966615,
        "step": 7175
    },
    {
        "loss": 2.1518,
        "grad_norm": 3.4532179832458496,
        "learning_rate": 6.879507978884909e-05,
        "epoch": 0.5347641403979432,
        "step": 7176
    },
    {
        "loss": 2.4638,
        "grad_norm": 2.723877191543579,
        "learning_rate": 6.872823549219509e-05,
        "epoch": 0.534838661599225,
        "step": 7177
    },
    {
        "loss": 2.4007,
        "grad_norm": 2.863520383834839,
        "learning_rate": 6.866140667921564e-05,
        "epoch": 0.5349131828005067,
        "step": 7178
    },
    {
        "loss": 1.9182,
        "grad_norm": 5.008880138397217,
        "learning_rate": 6.85945933829998e-05,
        "epoch": 0.5349877040017885,
        "step": 7179
    },
    {
        "loss": 2.2455,
        "grad_norm": 2.0290491580963135,
        "learning_rate": 6.852779563662919e-05,
        "epoch": 0.5350622252030702,
        "step": 7180
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.9548652172088623,
        "learning_rate": 6.846101347317751e-05,
        "epoch": 0.535136746404352,
        "step": 7181
    },
    {
        "loss": 1.9684,
        "grad_norm": 3.262746572494507,
        "learning_rate": 6.839424692571059e-05,
        "epoch": 0.5352112676056338,
        "step": 7182
    },
    {
        "loss": 2.4157,
        "grad_norm": 1.9403653144836426,
        "learning_rate": 6.832749602728705e-05,
        "epoch": 0.5352857888069156,
        "step": 7183
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.7639362812042236,
        "learning_rate": 6.826076081095716e-05,
        "epoch": 0.5353603100081973,
        "step": 7184
    },
    {
        "loss": 2.4474,
        "grad_norm": 2.9072017669677734,
        "learning_rate": 6.819404130976402e-05,
        "epoch": 0.5354348312094791,
        "step": 7185
    },
    {
        "loss": 2.113,
        "grad_norm": 2.8064019680023193,
        "learning_rate": 6.812733755674243e-05,
        "epoch": 0.5355093524107608,
        "step": 7186
    },
    {
        "loss": 2.4885,
        "grad_norm": 3.5703396797180176,
        "learning_rate": 6.80606495849196e-05,
        "epoch": 0.5355838736120426,
        "step": 7187
    },
    {
        "loss": 2.5892,
        "grad_norm": 3.060310125350952,
        "learning_rate": 6.799397742731505e-05,
        "epoch": 0.5356583948133244,
        "step": 7188
    },
    {
        "loss": 1.9254,
        "grad_norm": 3.3006343841552734,
        "learning_rate": 6.79273211169402e-05,
        "epoch": 0.5357329160146062,
        "step": 7189
    },
    {
        "loss": 2.2505,
        "grad_norm": 3.563598394393921,
        "learning_rate": 6.78606806867989e-05,
        "epoch": 0.535807437215888,
        "step": 7190
    },
    {
        "loss": 2.675,
        "grad_norm": 2.066545248031616,
        "learning_rate": 6.779405616988695e-05,
        "epoch": 0.5358819584171697,
        "step": 7191
    },
    {
        "loss": 3.5505,
        "grad_norm": 3.945742607116699,
        "learning_rate": 6.772744759919223e-05,
        "epoch": 0.5359564796184515,
        "step": 7192
    },
    {
        "loss": 2.5747,
        "grad_norm": 1.9925957918167114,
        "learning_rate": 6.766085500769503e-05,
        "epoch": 0.5360310008197332,
        "step": 7193
    },
    {
        "loss": 2.5024,
        "grad_norm": 3.1131012439727783,
        "learning_rate": 6.759427842836729e-05,
        "epoch": 0.536105522021015,
        "step": 7194
    },
    {
        "loss": 2.644,
        "grad_norm": 2.5985214710235596,
        "learning_rate": 6.752771789417353e-05,
        "epoch": 0.5361800432222967,
        "step": 7195
    },
    {
        "loss": 1.9627,
        "grad_norm": 3.5605945587158203,
        "learning_rate": 6.74611734380699e-05,
        "epoch": 0.5362545644235786,
        "step": 7196
    },
    {
        "loss": 2.0421,
        "grad_norm": 3.6016108989715576,
        "learning_rate": 6.739464509300469e-05,
        "epoch": 0.5363290856248603,
        "step": 7197
    },
    {
        "loss": 2.691,
        "grad_norm": 2.0621516704559326,
        "learning_rate": 6.732813289191835e-05,
        "epoch": 0.5364036068261421,
        "step": 7198
    },
    {
        "loss": 2.1928,
        "grad_norm": 3.3714759349823,
        "learning_rate": 6.726163686774316e-05,
        "epoch": 0.5364781280274238,
        "step": 7199
    },
    {
        "loss": 2.2262,
        "grad_norm": 3.3521382808685303,
        "learning_rate": 6.719515705340362e-05,
        "epoch": 0.5365526492287056,
        "step": 7200
    },
    {
        "loss": 1.96,
        "grad_norm": 3.769357442855835,
        "learning_rate": 6.712869348181596e-05,
        "epoch": 0.5366271704299873,
        "step": 7201
    },
    {
        "loss": 2.4546,
        "grad_norm": 3.237959384918213,
        "learning_rate": 6.706224618588836e-05,
        "epoch": 0.5367016916312691,
        "step": 7202
    },
    {
        "loss": 2.5668,
        "grad_norm": 2.5651423931121826,
        "learning_rate": 6.699581519852132e-05,
        "epoch": 0.5367762128325508,
        "step": 7203
    },
    {
        "loss": 2.5871,
        "grad_norm": 2.1005382537841797,
        "learning_rate": 6.69294005526067e-05,
        "epoch": 0.5368507340338327,
        "step": 7204
    },
    {
        "loss": 2.2697,
        "grad_norm": 2.821589946746826,
        "learning_rate": 6.686300228102887e-05,
        "epoch": 0.5369252552351144,
        "step": 7205
    },
    {
        "loss": 2.1663,
        "grad_norm": 3.967663526535034,
        "learning_rate": 6.679662041666362e-05,
        "epoch": 0.5369997764363962,
        "step": 7206
    },
    {
        "loss": 2.7114,
        "grad_norm": 3.2837486267089844,
        "learning_rate": 6.673025499237875e-05,
        "epoch": 0.5370742976376779,
        "step": 7207
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.1550729274749756,
        "learning_rate": 6.666390604103409e-05,
        "epoch": 0.5371488188389597,
        "step": 7208
    },
    {
        "loss": 2.5512,
        "grad_norm": 3.2976720333099365,
        "learning_rate": 6.659757359548101e-05,
        "epoch": 0.5372233400402414,
        "step": 7209
    },
    {
        "loss": 2.7368,
        "grad_norm": 2.319145917892456,
        "learning_rate": 6.653125768856304e-05,
        "epoch": 0.5372978612415232,
        "step": 7210
    },
    {
        "loss": 3.1082,
        "grad_norm": 2.9526867866516113,
        "learning_rate": 6.646495835311528e-05,
        "epoch": 0.5373723824428049,
        "step": 7211
    },
    {
        "loss": 2.3451,
        "grad_norm": 3.1685280799865723,
        "learning_rate": 6.639867562196457e-05,
        "epoch": 0.5374469036440868,
        "step": 7212
    },
    {
        "loss": 2.1492,
        "grad_norm": 3.8693366050720215,
        "learning_rate": 6.633240952792992e-05,
        "epoch": 0.5375214248453685,
        "step": 7213
    },
    {
        "loss": 2.5921,
        "grad_norm": 2.5911190509796143,
        "learning_rate": 6.62661601038217e-05,
        "epoch": 0.5375959460466503,
        "step": 7214
    },
    {
        "loss": 2.4278,
        "grad_norm": 2.889394521713257,
        "learning_rate": 6.61999273824421e-05,
        "epoch": 0.537670467247932,
        "step": 7215
    },
    {
        "loss": 1.0925,
        "grad_norm": 1.6322715282440186,
        "learning_rate": 6.613371139658529e-05,
        "epoch": 0.5377449884492138,
        "step": 7216
    },
    {
        "loss": 2.424,
        "grad_norm": 2.295867443084717,
        "learning_rate": 6.606751217903679e-05,
        "epoch": 0.5378195096504955,
        "step": 7217
    },
    {
        "loss": 2.0474,
        "grad_norm": 3.211913824081421,
        "learning_rate": 6.600132976257419e-05,
        "epoch": 0.5378940308517773,
        "step": 7218
    },
    {
        "loss": 2.6379,
        "grad_norm": 2.2294278144836426,
        "learning_rate": 6.593516417996645e-05,
        "epoch": 0.537968552053059,
        "step": 7219
    },
    {
        "loss": 2.0931,
        "grad_norm": 3.5911030769348145,
        "learning_rate": 6.586901546397429e-05,
        "epoch": 0.5380430732543409,
        "step": 7220
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.631532669067383,
        "learning_rate": 6.580288364735019e-05,
        "epoch": 0.5381175944556226,
        "step": 7221
    },
    {
        "loss": 2.4638,
        "grad_norm": 3.2605974674224854,
        "learning_rate": 6.573676876283801e-05,
        "epoch": 0.5381921156569044,
        "step": 7222
    },
    {
        "loss": 2.793,
        "grad_norm": 2.2489748001098633,
        "learning_rate": 6.567067084317368e-05,
        "epoch": 0.5382666368581862,
        "step": 7223
    },
    {
        "loss": 2.5419,
        "grad_norm": 2.768338680267334,
        "learning_rate": 6.560458992108425e-05,
        "epoch": 0.5383411580594679,
        "step": 7224
    },
    {
        "loss": 2.1884,
        "grad_norm": 3.8625118732452393,
        "learning_rate": 6.553852602928847e-05,
        "epoch": 0.5384156792607497,
        "step": 7225
    },
    {
        "loss": 1.9973,
        "grad_norm": 2.8901376724243164,
        "learning_rate": 6.547247920049699e-05,
        "epoch": 0.5384902004620314,
        "step": 7226
    },
    {
        "loss": 2.2327,
        "grad_norm": 3.595604658126831,
        "learning_rate": 6.540644946741155e-05,
        "epoch": 0.5385647216633133,
        "step": 7227
    },
    {
        "loss": 2.5055,
        "grad_norm": 3.821455478668213,
        "learning_rate": 6.534043686272576e-05,
        "epoch": 0.538639242864595,
        "step": 7228
    },
    {
        "loss": 2.6022,
        "grad_norm": 2.703519582748413,
        "learning_rate": 6.527444141912453e-05,
        "epoch": 0.5387137640658768,
        "step": 7229
    },
    {
        "loss": 2.391,
        "grad_norm": 3.2309067249298096,
        "learning_rate": 6.52084631692844e-05,
        "epoch": 0.5387882852671585,
        "step": 7230
    },
    {
        "loss": 1.8757,
        "grad_norm": 3.09228777885437,
        "learning_rate": 6.514250214587337e-05,
        "epoch": 0.5388628064684403,
        "step": 7231
    },
    {
        "loss": 1.9291,
        "grad_norm": 3.3387248516082764,
        "learning_rate": 6.507655838155078e-05,
        "epoch": 0.538937327669722,
        "step": 7232
    },
    {
        "loss": 1.8622,
        "grad_norm": 2.8923356533050537,
        "learning_rate": 6.50106319089678e-05,
        "epoch": 0.5390118488710038,
        "step": 7233
    },
    {
        "loss": 1.9012,
        "grad_norm": 5.542830467224121,
        "learning_rate": 6.494472276076663e-05,
        "epoch": 0.5390863700722855,
        "step": 7234
    },
    {
        "loss": 2.1666,
        "grad_norm": 2.7476611137390137,
        "learning_rate": 6.487883096958094e-05,
        "epoch": 0.5391608912735674,
        "step": 7235
    },
    {
        "loss": 1.295,
        "grad_norm": 2.95949125289917,
        "learning_rate": 6.481295656803614e-05,
        "epoch": 0.5392354124748491,
        "step": 7236
    },
    {
        "loss": 2.7668,
        "grad_norm": 2.9454731941223145,
        "learning_rate": 6.474709958874866e-05,
        "epoch": 0.5393099336761309,
        "step": 7237
    },
    {
        "loss": 2.5093,
        "grad_norm": 1.9088950157165527,
        "learning_rate": 6.468126006432656e-05,
        "epoch": 0.5393844548774126,
        "step": 7238
    },
    {
        "loss": 2.0725,
        "grad_norm": 2.8105247020721436,
        "learning_rate": 6.461543802736905e-05,
        "epoch": 0.5394589760786944,
        "step": 7239
    },
    {
        "loss": 2.1111,
        "grad_norm": 2.7102174758911133,
        "learning_rate": 6.454963351046672e-05,
        "epoch": 0.5395334972799761,
        "step": 7240
    },
    {
        "loss": 2.4219,
        "grad_norm": 2.2135019302368164,
        "learning_rate": 6.448384654620172e-05,
        "epoch": 0.539608018481258,
        "step": 7241
    },
    {
        "loss": 2.5534,
        "grad_norm": 2.3638112545013428,
        "learning_rate": 6.441807716714719e-05,
        "epoch": 0.5396825396825397,
        "step": 7242
    },
    {
        "loss": 2.4494,
        "grad_norm": 2.4950685501098633,
        "learning_rate": 6.435232540586763e-05,
        "epoch": 0.5397570608838215,
        "step": 7243
    },
    {
        "loss": 2.3592,
        "grad_norm": 2.390226125717163,
        "learning_rate": 6.428659129491912e-05,
        "epoch": 0.5398315820851032,
        "step": 7244
    },
    {
        "loss": 2.4084,
        "grad_norm": 2.5464422702789307,
        "learning_rate": 6.422087486684851e-05,
        "epoch": 0.539906103286385,
        "step": 7245
    },
    {
        "loss": 1.912,
        "grad_norm": 4.048999786376953,
        "learning_rate": 6.415517615419446e-05,
        "epoch": 0.5399806244876667,
        "step": 7246
    },
    {
        "loss": 2.6987,
        "grad_norm": 2.6581363677978516,
        "learning_rate": 6.408949518948636e-05,
        "epoch": 0.5400551456889485,
        "step": 7247
    },
    {
        "loss": 2.7533,
        "grad_norm": 3.1657357215881348,
        "learning_rate": 6.402383200524498e-05,
        "epoch": 0.5401296668902302,
        "step": 7248
    },
    {
        "loss": 2.5725,
        "grad_norm": 2.9926772117614746,
        "learning_rate": 6.395818663398244e-05,
        "epoch": 0.540204188091512,
        "step": 7249
    },
    {
        "loss": 2.9249,
        "grad_norm": 3.5025973320007324,
        "learning_rate": 6.389255910820177e-05,
        "epoch": 0.5402787092927938,
        "step": 7250
    },
    {
        "loss": 2.5046,
        "grad_norm": 3.2744808197021484,
        "learning_rate": 6.382694946039742e-05,
        "epoch": 0.5403532304940756,
        "step": 7251
    },
    {
        "loss": 2.302,
        "grad_norm": 2.6502621173858643,
        "learning_rate": 6.376135772305487e-05,
        "epoch": 0.5404277516953573,
        "step": 7252
    },
    {
        "loss": 2.5707,
        "grad_norm": 2.906592845916748,
        "learning_rate": 6.369578392865057e-05,
        "epoch": 0.5405022728966391,
        "step": 7253
    },
    {
        "loss": 1.8675,
        "grad_norm": 2.9899396896362305,
        "learning_rate": 6.363022810965248e-05,
        "epoch": 0.5405767940979208,
        "step": 7254
    },
    {
        "loss": 3.078,
        "grad_norm": 3.1269168853759766,
        "learning_rate": 6.356469029851921e-05,
        "epoch": 0.5406513152992026,
        "step": 7255
    },
    {
        "loss": 2.5239,
        "grad_norm": 2.2551519870758057,
        "learning_rate": 6.349917052770095e-05,
        "epoch": 0.5407258365004843,
        "step": 7256
    },
    {
        "loss": 2.4669,
        "grad_norm": 2.845679759979248,
        "learning_rate": 6.343366882963854e-05,
        "epoch": 0.5408003577017662,
        "step": 7257
    },
    {
        "loss": 2.186,
        "grad_norm": 3.260672092437744,
        "learning_rate": 6.336818523676394e-05,
        "epoch": 0.5408748789030479,
        "step": 7258
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.738856554031372,
        "learning_rate": 6.330271978150032e-05,
        "epoch": 0.5409494001043297,
        "step": 7259
    },
    {
        "loss": 2.3008,
        "grad_norm": 2.3576650619506836,
        "learning_rate": 6.323727249626174e-05,
        "epoch": 0.5410239213056115,
        "step": 7260
    },
    {
        "loss": 2.4081,
        "grad_norm": 3.6291189193725586,
        "learning_rate": 6.317184341345333e-05,
        "epoch": 0.5410984425068932,
        "step": 7261
    },
    {
        "loss": 2.9409,
        "grad_norm": 1.4236977100372314,
        "learning_rate": 6.310643256547112e-05,
        "epoch": 0.541172963708175,
        "step": 7262
    },
    {
        "loss": 2.5289,
        "grad_norm": 3.351384162902832,
        "learning_rate": 6.304103998470205e-05,
        "epoch": 0.5412474849094567,
        "step": 7263
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.2996304035186768,
        "learning_rate": 6.297566570352442e-05,
        "epoch": 0.5413220061107386,
        "step": 7264
    },
    {
        "loss": 2.766,
        "grad_norm": 2.4551589488983154,
        "learning_rate": 6.291030975430687e-05,
        "epoch": 0.5413965273120203,
        "step": 7265
    },
    {
        "loss": 2.0055,
        "grad_norm": 3.9738495349884033,
        "learning_rate": 6.284497216940955e-05,
        "epoch": 0.5414710485133021,
        "step": 7266
    },
    {
        "loss": 2.4399,
        "grad_norm": 3.23783802986145,
        "learning_rate": 6.277965298118308e-05,
        "epoch": 0.5415455697145838,
        "step": 7267
    },
    {
        "loss": 2.3409,
        "grad_norm": 3.2686994075775146,
        "learning_rate": 6.271435222196913e-05,
        "epoch": 0.5416200909158656,
        "step": 7268
    },
    {
        "loss": 2.7708,
        "grad_norm": 2.280592679977417,
        "learning_rate": 6.26490699241003e-05,
        "epoch": 0.5416946121171473,
        "step": 7269
    },
    {
        "loss": 2.3368,
        "grad_norm": 3.1210808753967285,
        "learning_rate": 6.258380611989988e-05,
        "epoch": 0.5417691333184291,
        "step": 7270
    },
    {
        "loss": 2.4886,
        "grad_norm": 2.2138397693634033,
        "learning_rate": 6.251856084168231e-05,
        "epoch": 0.5418436545197108,
        "step": 7271
    },
    {
        "loss": 2.4935,
        "grad_norm": 4.305192947387695,
        "learning_rate": 6.245333412175252e-05,
        "epoch": 0.5419181757209927,
        "step": 7272
    },
    {
        "loss": 1.8179,
        "grad_norm": 5.269746780395508,
        "learning_rate": 6.238812599240629e-05,
        "epoch": 0.5419926969222744,
        "step": 7273
    },
    {
        "loss": 2.4014,
        "grad_norm": 2.4405722618103027,
        "learning_rate": 6.232293648593057e-05,
        "epoch": 0.5420672181235562,
        "step": 7274
    },
    {
        "loss": 2.9539,
        "grad_norm": 3.1643295288085938,
        "learning_rate": 6.225776563460271e-05,
        "epoch": 0.5421417393248379,
        "step": 7275
    },
    {
        "loss": 2.7429,
        "grad_norm": 3.55503511428833,
        "learning_rate": 6.219261347069074e-05,
        "epoch": 0.5422162605261197,
        "step": 7276
    },
    {
        "loss": 2.4599,
        "grad_norm": 2.454698085784912,
        "learning_rate": 6.212748002645393e-05,
        "epoch": 0.5422907817274014,
        "step": 7277
    },
    {
        "loss": 1.8818,
        "grad_norm": 2.2968952655792236,
        "learning_rate": 6.206236533414178e-05,
        "epoch": 0.5423653029286832,
        "step": 7278
    },
    {
        "loss": 2.3626,
        "grad_norm": 3.282810926437378,
        "learning_rate": 6.199726942599487e-05,
        "epoch": 0.5424398241299649,
        "step": 7279
    },
    {
        "loss": 1.829,
        "grad_norm": 2.712418794631958,
        "learning_rate": 6.193219233424418e-05,
        "epoch": 0.5425143453312468,
        "step": 7280
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.431405544281006,
        "learning_rate": 6.186713409111149e-05,
        "epoch": 0.5425888665325285,
        "step": 7281
    },
    {
        "loss": 1.6368,
        "grad_norm": 2.953632354736328,
        "learning_rate": 6.180209472880941e-05,
        "epoch": 0.5426633877338103,
        "step": 7282
    },
    {
        "loss": 2.3657,
        "grad_norm": 4.406567096710205,
        "learning_rate": 6.173707427954083e-05,
        "epoch": 0.542737908935092,
        "step": 7283
    },
    {
        "loss": 2.5362,
        "grad_norm": 1.9074145555496216,
        "learning_rate": 6.167207277549977e-05,
        "epoch": 0.5428124301363738,
        "step": 7284
    },
    {
        "loss": 1.7964,
        "grad_norm": 2.9360060691833496,
        "learning_rate": 6.160709024887046e-05,
        "epoch": 0.5428869513376555,
        "step": 7285
    },
    {
        "loss": 1.924,
        "grad_norm": 4.503091335296631,
        "learning_rate": 6.154212673182779e-05,
        "epoch": 0.5429614725389373,
        "step": 7286
    },
    {
        "loss": 2.4409,
        "grad_norm": 4.993445873260498,
        "learning_rate": 6.147718225653757e-05,
        "epoch": 0.543035993740219,
        "step": 7287
    },
    {
        "loss": 3.0539,
        "grad_norm": 2.231735944747925,
        "learning_rate": 6.141225685515573e-05,
        "epoch": 0.5431105149415009,
        "step": 7288
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.261582136154175,
        "learning_rate": 6.134735055982907e-05,
        "epoch": 0.5431850361427826,
        "step": 7289
    },
    {
        "loss": 2.7731,
        "grad_norm": 3.531155824661255,
        "learning_rate": 6.128246340269478e-05,
        "epoch": 0.5432595573440644,
        "step": 7290
    },
    {
        "loss": 2.5252,
        "grad_norm": 1.7295796871185303,
        "learning_rate": 6.121759541588057e-05,
        "epoch": 0.5433340785453461,
        "step": 7291
    },
    {
        "loss": 2.7938,
        "grad_norm": 1.897852897644043,
        "learning_rate": 6.11527466315048e-05,
        "epoch": 0.5434085997466279,
        "step": 7292
    },
    {
        "loss": 2.6333,
        "grad_norm": 2.0513737201690674,
        "learning_rate": 6.108791708167605e-05,
        "epoch": 0.5434831209479096,
        "step": 7293
    },
    {
        "loss": 1.3271,
        "grad_norm": 3.5908589363098145,
        "learning_rate": 6.102310679849383e-05,
        "epoch": 0.5435576421491914,
        "step": 7294
    },
    {
        "loss": 2.6249,
        "grad_norm": 2.006927490234375,
        "learning_rate": 6.0958315814047665e-05,
        "epoch": 0.5436321633504733,
        "step": 7295
    },
    {
        "loss": 1.4675,
        "grad_norm": 2.1975836753845215,
        "learning_rate": 6.0893544160417574e-05,
        "epoch": 0.543706684551755,
        "step": 7296
    },
    {
        "loss": 2.5802,
        "grad_norm": 2.6649980545043945,
        "learning_rate": 6.0828791869674394e-05,
        "epoch": 0.5437812057530368,
        "step": 7297
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.706982970237732,
        "learning_rate": 6.076405897387893e-05,
        "epoch": 0.5438557269543185,
        "step": 7298
    },
    {
        "loss": 2.4813,
        "grad_norm": 3.461714744567871,
        "learning_rate": 6.069934550508263e-05,
        "epoch": 0.5439302481556003,
        "step": 7299
    },
    {
        "loss": 2.7191,
        "grad_norm": 2.693013906478882,
        "learning_rate": 6.063465149532723e-05,
        "epoch": 0.544004769356882,
        "step": 7300
    },
    {
        "loss": 2.4897,
        "grad_norm": 3.3750436305999756,
        "learning_rate": 6.056997697664478e-05,
        "epoch": 0.5440792905581638,
        "step": 7301
    },
    {
        "loss": 2.4205,
        "grad_norm": 3.0636019706726074,
        "learning_rate": 6.0505321981057895e-05,
        "epoch": 0.5441538117594455,
        "step": 7302
    },
    {
        "loss": 2.2063,
        "grad_norm": 2.822415828704834,
        "learning_rate": 6.044068654057917e-05,
        "epoch": 0.5442283329607274,
        "step": 7303
    },
    {
        "loss": 2.3859,
        "grad_norm": 2.5989086627960205,
        "learning_rate": 6.037607068721196e-05,
        "epoch": 0.5443028541620091,
        "step": 7304
    },
    {
        "loss": 1.6763,
        "grad_norm": 4.429825305938721,
        "learning_rate": 6.0311474452949624e-05,
        "epoch": 0.5443773753632909,
        "step": 7305
    },
    {
        "loss": 1.9282,
        "grad_norm": 3.3305530548095703,
        "learning_rate": 6.0246897869775686e-05,
        "epoch": 0.5444518965645726,
        "step": 7306
    },
    {
        "loss": 2.1223,
        "grad_norm": 4.201519966125488,
        "learning_rate": 6.018234096966441e-05,
        "epoch": 0.5445264177658544,
        "step": 7307
    },
    {
        "loss": 2.38,
        "grad_norm": 2.5518929958343506,
        "learning_rate": 6.01178037845799e-05,
        "epoch": 0.5446009389671361,
        "step": 7308
    },
    {
        "loss": 2.5728,
        "grad_norm": 2.0501582622528076,
        "learning_rate": 6.0053286346476555e-05,
        "epoch": 0.544675460168418,
        "step": 7309
    },
    {
        "loss": 2.3878,
        "grad_norm": 3.9964749813079834,
        "learning_rate": 5.998878868729922e-05,
        "epoch": 0.5447499813696997,
        "step": 7310
    },
    {
        "loss": 1.9894,
        "grad_norm": 3.392458438873291,
        "learning_rate": 5.9924310838982664e-05,
        "epoch": 0.5448245025709815,
        "step": 7311
    },
    {
        "loss": 2.4316,
        "grad_norm": 2.8373286724090576,
        "learning_rate": 5.9859852833452076e-05,
        "epoch": 0.5448990237722632,
        "step": 7312
    },
    {
        "loss": 2.5927,
        "grad_norm": 2.5166447162628174,
        "learning_rate": 5.979541470262273e-05,
        "epoch": 0.544973544973545,
        "step": 7313
    },
    {
        "loss": 2.6634,
        "grad_norm": 4.0187883377075195,
        "learning_rate": 5.973099647839986e-05,
        "epoch": 0.5450480661748267,
        "step": 7314
    },
    {
        "loss": 2.106,
        "grad_norm": 2.9916183948516846,
        "learning_rate": 5.966659819267931e-05,
        "epoch": 0.5451225873761085,
        "step": 7315
    },
    {
        "loss": 2.8327,
        "grad_norm": 2.8510544300079346,
        "learning_rate": 5.960221987734656e-05,
        "epoch": 0.5451971085773902,
        "step": 7316
    },
    {
        "loss": 2.4524,
        "grad_norm": 1.869629979133606,
        "learning_rate": 5.953786156427763e-05,
        "epoch": 0.545271629778672,
        "step": 7317
    },
    {
        "loss": 1.7869,
        "grad_norm": 4.790658950805664,
        "learning_rate": 5.947352328533833e-05,
        "epoch": 0.5453461509799538,
        "step": 7318
    },
    {
        "loss": 3.0366,
        "grad_norm": 2.7416985034942627,
        "learning_rate": 5.940920507238456e-05,
        "epoch": 0.5454206721812356,
        "step": 7319
    },
    {
        "loss": 2.5688,
        "grad_norm": 3.5678224563598633,
        "learning_rate": 5.9344906957262527e-05,
        "epoch": 0.5454951933825173,
        "step": 7320
    },
    {
        "loss": 2.4142,
        "grad_norm": 4.092337131500244,
        "learning_rate": 5.928062897180822e-05,
        "epoch": 0.5455697145837991,
        "step": 7321
    },
    {
        "loss": 2.6912,
        "grad_norm": 3.005467653274536,
        "learning_rate": 5.921637114784784e-05,
        "epoch": 0.5456442357850808,
        "step": 7322
    },
    {
        "loss": 2.5766,
        "grad_norm": 3.6720051765441895,
        "learning_rate": 5.915213351719753e-05,
        "epoch": 0.5457187569863626,
        "step": 7323
    },
    {
        "loss": 3.0284,
        "grad_norm": 1.7665705680847168,
        "learning_rate": 5.908791611166329e-05,
        "epoch": 0.5457932781876443,
        "step": 7324
    },
    {
        "loss": 2.606,
        "grad_norm": 2.75844407081604,
        "learning_rate": 5.9023718963041554e-05,
        "epoch": 0.5458677993889262,
        "step": 7325
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.6354689598083496,
        "learning_rate": 5.8959542103118115e-05,
        "epoch": 0.5459423205902079,
        "step": 7326
    },
    {
        "loss": 2.2949,
        "grad_norm": 2.7460038661956787,
        "learning_rate": 5.8895385563669334e-05,
        "epoch": 0.5460168417914897,
        "step": 7327
    },
    {
        "loss": 1.0652,
        "grad_norm": 3.3578994274139404,
        "learning_rate": 5.88312493764611e-05,
        "epoch": 0.5460913629927714,
        "step": 7328
    },
    {
        "loss": 2.6446,
        "grad_norm": 2.157942533493042,
        "learning_rate": 5.876713357324923e-05,
        "epoch": 0.5461658841940532,
        "step": 7329
    },
    {
        "loss": 2.9855,
        "grad_norm": 2.065152645111084,
        "learning_rate": 5.870303818577971e-05,
        "epoch": 0.546240405395335,
        "step": 7330
    },
    {
        "loss": 2.2965,
        "grad_norm": 3.907743453979492,
        "learning_rate": 5.863896324578815e-05,
        "epoch": 0.5463149265966167,
        "step": 7331
    },
    {
        "loss": 2.0339,
        "grad_norm": 3.5526533126831055,
        "learning_rate": 5.857490878500027e-05,
        "epoch": 0.5463894477978986,
        "step": 7332
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.800799608230591,
        "learning_rate": 5.851087483513145e-05,
        "epoch": 0.5464639689991803,
        "step": 7333
    },
    {
        "loss": 1.5978,
        "grad_norm": 4.49333381652832,
        "learning_rate": 5.844686142788687e-05,
        "epoch": 0.5465384902004621,
        "step": 7334
    },
    {
        "loss": 2.0887,
        "grad_norm": 1.9025598764419556,
        "learning_rate": 5.838286859496196e-05,
        "epoch": 0.5466130114017438,
        "step": 7335
    },
    {
        "loss": 1.8082,
        "grad_norm": 2.642913579940796,
        "learning_rate": 5.8318896368041485e-05,
        "epoch": 0.5466875326030256,
        "step": 7336
    },
    {
        "loss": 0.5729,
        "grad_norm": 2.544848680496216,
        "learning_rate": 5.825494477880009e-05,
        "epoch": 0.5467620538043073,
        "step": 7337
    },
    {
        "loss": 2.9991,
        "grad_norm": 2.603145122528076,
        "learning_rate": 5.819101385890256e-05,
        "epoch": 0.5468365750055891,
        "step": 7338
    },
    {
        "loss": 2.4435,
        "grad_norm": 3.858510971069336,
        "learning_rate": 5.812710364000299e-05,
        "epoch": 0.5469110962068708,
        "step": 7339
    },
    {
        "loss": 1.9546,
        "grad_norm": 3.0696256160736084,
        "learning_rate": 5.8063214153745535e-05,
        "epoch": 0.5469856174081527,
        "step": 7340
    },
    {
        "loss": 2.5422,
        "grad_norm": 3.591157913208008,
        "learning_rate": 5.799934543176391e-05,
        "epoch": 0.5470601386094344,
        "step": 7341
    },
    {
        "loss": 1.8604,
        "grad_norm": 3.5281152725219727,
        "learning_rate": 5.793549750568159e-05,
        "epoch": 0.5471346598107162,
        "step": 7342
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.6785638332366943,
        "learning_rate": 5.787167040711183e-05,
        "epoch": 0.5472091810119979,
        "step": 7343
    },
    {
        "loss": 2.2407,
        "grad_norm": 3.5023860931396484,
        "learning_rate": 5.780786416765738e-05,
        "epoch": 0.5472837022132797,
        "step": 7344
    },
    {
        "loss": 1.7099,
        "grad_norm": 3.5697295665740967,
        "learning_rate": 5.774407881891104e-05,
        "epoch": 0.5473582234145614,
        "step": 7345
    },
    {
        "loss": 2.6611,
        "grad_norm": 3.064354181289673,
        "learning_rate": 5.768031439245487e-05,
        "epoch": 0.5474327446158432,
        "step": 7346
    },
    {
        "loss": 2.8078,
        "grad_norm": 1.8588207960128784,
        "learning_rate": 5.761657091986065e-05,
        "epoch": 0.5475072658171249,
        "step": 7347
    },
    {
        "loss": 1.744,
        "grad_norm": 2.6760501861572266,
        "learning_rate": 5.7552848432690064e-05,
        "epoch": 0.5475817870184068,
        "step": 7348
    },
    {
        "loss": 1.2515,
        "grad_norm": 3.3772029876708984,
        "learning_rate": 5.7489146962494066e-05,
        "epoch": 0.5476563082196885,
        "step": 7349
    },
    {
        "loss": 3.0387,
        "grad_norm": 3.7824935913085938,
        "learning_rate": 5.742546654081341e-05,
        "epoch": 0.5477308294209703,
        "step": 7350
    },
    {
        "loss": 2.3554,
        "grad_norm": 2.76560640335083,
        "learning_rate": 5.7361807199178316e-05,
        "epoch": 0.547805350622252,
        "step": 7351
    },
    {
        "loss": 2.066,
        "grad_norm": 2.545152425765991,
        "learning_rate": 5.729816896910858e-05,
        "epoch": 0.5478798718235338,
        "step": 7352
    },
    {
        "loss": 2.1208,
        "grad_norm": 3.0786328315734863,
        "learning_rate": 5.723455188211363e-05,
        "epoch": 0.5479543930248155,
        "step": 7353
    },
    {
        "loss": 2.1475,
        "grad_norm": 3.4351320266723633,
        "learning_rate": 5.717095596969223e-05,
        "epoch": 0.5480289142260973,
        "step": 7354
    },
    {
        "loss": 2.3169,
        "grad_norm": 2.4569778442382812,
        "learning_rate": 5.710738126333304e-05,
        "epoch": 0.548103435427379,
        "step": 7355
    },
    {
        "loss": 2.3015,
        "grad_norm": 2.3908393383026123,
        "learning_rate": 5.7043827794513874e-05,
        "epoch": 0.5481779566286609,
        "step": 7356
    },
    {
        "loss": 2.5797,
        "grad_norm": 3.1628212928771973,
        "learning_rate": 5.698029559470197e-05,
        "epoch": 0.5482524778299426,
        "step": 7357
    },
    {
        "loss": 2.7086,
        "grad_norm": 3.5614209175109863,
        "learning_rate": 5.691678469535445e-05,
        "epoch": 0.5483269990312244,
        "step": 7358
    },
    {
        "loss": 2.3299,
        "grad_norm": 3.1021807193756104,
        "learning_rate": 5.685329512791746e-05,
        "epoch": 0.5484015202325061,
        "step": 7359
    },
    {
        "loss": 2.7562,
        "grad_norm": 2.3880443572998047,
        "learning_rate": 5.678982692382696e-05,
        "epoch": 0.5484760414337879,
        "step": 7360
    },
    {
        "loss": 2.8787,
        "grad_norm": 2.3201770782470703,
        "learning_rate": 5.672638011450797e-05,
        "epoch": 0.5485505626350696,
        "step": 7361
    },
    {
        "loss": 1.4471,
        "grad_norm": 4.220661640167236,
        "learning_rate": 5.666295473137504e-05,
        "epoch": 0.5486250838363514,
        "step": 7362
    },
    {
        "loss": 2.5838,
        "grad_norm": 1.6702255010604858,
        "learning_rate": 5.6599550805832325e-05,
        "epoch": 0.5486996050376332,
        "step": 7363
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.8922486305236816,
        "learning_rate": 5.653616836927298e-05,
        "epoch": 0.548774126238915,
        "step": 7364
    },
    {
        "loss": 2.3588,
        "grad_norm": 2.543027400970459,
        "learning_rate": 5.647280745307999e-05,
        "epoch": 0.5488486474401968,
        "step": 7365
    },
    {
        "loss": 1.8001,
        "grad_norm": 3.449775457382202,
        "learning_rate": 5.640946808862524e-05,
        "epoch": 0.5489231686414785,
        "step": 7366
    },
    {
        "loss": 2.8691,
        "grad_norm": 1.958041787147522,
        "learning_rate": 5.634615030727004e-05,
        "epoch": 0.5489976898427603,
        "step": 7367
    },
    {
        "loss": 2.3415,
        "grad_norm": 2.811166763305664,
        "learning_rate": 5.628285414036534e-05,
        "epoch": 0.549072211044042,
        "step": 7368
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.132673501968384,
        "learning_rate": 5.6219579619251004e-05,
        "epoch": 0.5491467322453238,
        "step": 7369
    },
    {
        "loss": 1.8398,
        "grad_norm": 2.7463934421539307,
        "learning_rate": 5.61563267752563e-05,
        "epoch": 0.5492212534466056,
        "step": 7370
    },
    {
        "loss": 2.5875,
        "grad_norm": 2.333678722381592,
        "learning_rate": 5.609309563969985e-05,
        "epoch": 0.5492957746478874,
        "step": 7371
    },
    {
        "loss": 2.3419,
        "grad_norm": 2.505831241607666,
        "learning_rate": 5.602988624388939e-05,
        "epoch": 0.5493702958491691,
        "step": 7372
    },
    {
        "loss": 1.8404,
        "grad_norm": 3.3545620441436768,
        "learning_rate": 5.596669861912204e-05,
        "epoch": 0.5494448170504509,
        "step": 7373
    },
    {
        "loss": 2.5212,
        "grad_norm": 3.497105598449707,
        "learning_rate": 5.590353279668401e-05,
        "epoch": 0.5495193382517326,
        "step": 7374
    },
    {
        "loss": 1.9271,
        "grad_norm": 3.2921531200408936,
        "learning_rate": 5.5840388807850655e-05,
        "epoch": 0.5495938594530144,
        "step": 7375
    },
    {
        "loss": 3.1667,
        "grad_norm": 2.541783332824707,
        "learning_rate": 5.577726668388687e-05,
        "epoch": 0.5496683806542961,
        "step": 7376
    },
    {
        "loss": 2.9065,
        "grad_norm": 2.0298874378204346,
        "learning_rate": 5.5714166456046255e-05,
        "epoch": 0.549742901855578,
        "step": 7377
    },
    {
        "loss": 2.0858,
        "grad_norm": 2.2606024742126465,
        "learning_rate": 5.565108815557203e-05,
        "epoch": 0.5498174230568597,
        "step": 7378
    },
    {
        "loss": 2.2465,
        "grad_norm": 1.8713164329528809,
        "learning_rate": 5.558803181369625e-05,
        "epoch": 0.5498919442581415,
        "step": 7379
    },
    {
        "loss": 2.8119,
        "grad_norm": 3.0984995365142822,
        "learning_rate": 5.552499746164002e-05,
        "epoch": 0.5499664654594232,
        "step": 7380
    },
    {
        "loss": 2.4735,
        "grad_norm": 2.502088785171509,
        "learning_rate": 5.5461985130613894e-05,
        "epoch": 0.550040986660705,
        "step": 7381
    },
    {
        "loss": 2.6673,
        "grad_norm": 2.741386890411377,
        "learning_rate": 5.539899485181723e-05,
        "epoch": 0.5501155078619867,
        "step": 7382
    },
    {
        "loss": 2.3426,
        "grad_norm": 3.306020975112915,
        "learning_rate": 5.533602665643865e-05,
        "epoch": 0.5501900290632685,
        "step": 7383
    },
    {
        "loss": 2.394,
        "grad_norm": 2.008951187133789,
        "learning_rate": 5.5273080575655744e-05,
        "epoch": 0.5502645502645502,
        "step": 7384
    },
    {
        "loss": 2.2644,
        "grad_norm": 2.6409387588500977,
        "learning_rate": 5.521015664063507e-05,
        "epoch": 0.550339071465832,
        "step": 7385
    },
    {
        "loss": 1.5935,
        "grad_norm": 2.398977041244507,
        "learning_rate": 5.5147254882532526e-05,
        "epoch": 0.5504135926671138,
        "step": 7386
    },
    {
        "loss": 1.2924,
        "grad_norm": 2.6903953552246094,
        "learning_rate": 5.508437533249267e-05,
        "epoch": 0.5504881138683956,
        "step": 7387
    },
    {
        "loss": 2.29,
        "grad_norm": 3.1064562797546387,
        "learning_rate": 5.502151802164937e-05,
        "epoch": 0.5505626350696773,
        "step": 7388
    },
    {
        "loss": 2.7606,
        "grad_norm": 2.5231361389160156,
        "learning_rate": 5.495868298112525e-05,
        "epoch": 0.5506371562709591,
        "step": 7389
    },
    {
        "loss": 1.7506,
        "grad_norm": 3.926164150238037,
        "learning_rate": 5.4895870242031935e-05,
        "epoch": 0.5507116774722408,
        "step": 7390
    },
    {
        "loss": 1.391,
        "grad_norm": 5.785586833953857,
        "learning_rate": 5.483307983547026e-05,
        "epoch": 0.5507861986735226,
        "step": 7391
    },
    {
        "loss": 2.229,
        "grad_norm": 3.269318103790283,
        "learning_rate": 5.4770311792529585e-05,
        "epoch": 0.5508607198748043,
        "step": 7392
    },
    {
        "loss": 2.454,
        "grad_norm": 3.7754855155944824,
        "learning_rate": 5.470756614428859e-05,
        "epoch": 0.5509352410760862,
        "step": 7393
    },
    {
        "loss": 1.5778,
        "grad_norm": 2.8494014739990234,
        "learning_rate": 5.4644842921814685e-05,
        "epoch": 0.5510097622773679,
        "step": 7394
    },
    {
        "loss": 2.42,
        "grad_norm": 3.2421469688415527,
        "learning_rate": 5.4582142156164e-05,
        "epoch": 0.5510842834786497,
        "step": 7395
    },
    {
        "loss": 2.6871,
        "grad_norm": 2.1114749908447266,
        "learning_rate": 5.451946387838199e-05,
        "epoch": 0.5511588046799314,
        "step": 7396
    },
    {
        "loss": 2.1898,
        "grad_norm": 2.735841989517212,
        "learning_rate": 5.4456808119502504e-05,
        "epoch": 0.5512333258812132,
        "step": 7397
    },
    {
        "loss": 2.1095,
        "grad_norm": 2.684342384338379,
        "learning_rate": 5.439417491054869e-05,
        "epoch": 0.5513078470824949,
        "step": 7398
    },
    {
        "loss": 2.5571,
        "grad_norm": 3.674237012863159,
        "learning_rate": 5.4331564282532185e-05,
        "epoch": 0.5513823682837767,
        "step": 7399
    },
    {
        "loss": 2.3847,
        "grad_norm": 3.3407845497131348,
        "learning_rate": 5.426897626645351e-05,
        "epoch": 0.5514568894850586,
        "step": 7400
    },
    {
        "loss": 2.8453,
        "grad_norm": 4.138448238372803,
        "learning_rate": 5.420641089330214e-05,
        "epoch": 0.5515314106863403,
        "step": 7401
    },
    {
        "loss": 2.1807,
        "grad_norm": 2.3107686042785645,
        "learning_rate": 5.414386819405619e-05,
        "epoch": 0.5516059318876221,
        "step": 7402
    },
    {
        "loss": 2.3341,
        "grad_norm": 3.2704408168792725,
        "learning_rate": 5.408134819968255e-05,
        "epoch": 0.5516804530889038,
        "step": 7403
    },
    {
        "loss": 2.6617,
        "grad_norm": 2.4155731201171875,
        "learning_rate": 5.401885094113701e-05,
        "epoch": 0.5517549742901856,
        "step": 7404
    },
    {
        "loss": 2.3182,
        "grad_norm": 3.486783742904663,
        "learning_rate": 5.3956376449363845e-05,
        "epoch": 0.5518294954914673,
        "step": 7405
    },
    {
        "loss": 2.1593,
        "grad_norm": 3.4916741847991943,
        "learning_rate": 5.389392475529644e-05,
        "epoch": 0.5519040166927491,
        "step": 7406
    },
    {
        "loss": 2.7017,
        "grad_norm": 2.1032721996307373,
        "learning_rate": 5.383149588985661e-05,
        "epoch": 0.5519785378940308,
        "step": 7407
    },
    {
        "loss": 2.738,
        "grad_norm": 2.1556012630462646,
        "learning_rate": 5.376908988395475e-05,
        "epoch": 0.5520530590953127,
        "step": 7408
    },
    {
        "loss": 2.7423,
        "grad_norm": 2.8103630542755127,
        "learning_rate": 5.370670676849039e-05,
        "epoch": 0.5521275802965944,
        "step": 7409
    },
    {
        "loss": 2.2998,
        "grad_norm": 3.7200961112976074,
        "learning_rate": 5.364434657435128e-05,
        "epoch": 0.5522021014978762,
        "step": 7410
    },
    {
        "loss": 1.0015,
        "grad_norm": 3.8841123580932617,
        "learning_rate": 5.3582009332414076e-05,
        "epoch": 0.5522766226991579,
        "step": 7411
    },
    {
        "loss": 2.435,
        "grad_norm": 3.3374619483947754,
        "learning_rate": 5.351969507354395e-05,
        "epoch": 0.5523511439004397,
        "step": 7412
    },
    {
        "loss": 2.3831,
        "grad_norm": 2.3722987174987793,
        "learning_rate": 5.3457403828594696e-05,
        "epoch": 0.5524256651017214,
        "step": 7413
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.7275190353393555,
        "learning_rate": 5.339513562840883e-05,
        "epoch": 0.5525001863030032,
        "step": 7414
    },
    {
        "loss": 2.2555,
        "grad_norm": 2.448112726211548,
        "learning_rate": 5.33328905038172e-05,
        "epoch": 0.552574707504285,
        "step": 7415
    },
    {
        "loss": 1.9234,
        "grad_norm": 3.082667589187622,
        "learning_rate": 5.327066848563966e-05,
        "epoch": 0.5526492287055668,
        "step": 7416
    },
    {
        "loss": 1.5791,
        "grad_norm": 3.9753835201263428,
        "learning_rate": 5.3208469604684284e-05,
        "epoch": 0.5527237499068485,
        "step": 7417
    },
    {
        "loss": 2.2163,
        "grad_norm": 3.5764009952545166,
        "learning_rate": 5.314629389174759e-05,
        "epoch": 0.5527982711081303,
        "step": 7418
    },
    {
        "loss": 2.459,
        "grad_norm": 2.985910415649414,
        "learning_rate": 5.308414137761506e-05,
        "epoch": 0.552872792309412,
        "step": 7419
    },
    {
        "loss": 2.2734,
        "grad_norm": 2.7973411083221436,
        "learning_rate": 5.302201209306027e-05,
        "epoch": 0.5529473135106938,
        "step": 7420
    },
    {
        "loss": 2.9529,
        "grad_norm": 3.0421884059906006,
        "learning_rate": 5.2959906068845636e-05,
        "epoch": 0.5530218347119755,
        "step": 7421
    },
    {
        "loss": 2.1176,
        "grad_norm": 3.543963670730591,
        "learning_rate": 5.289782333572173e-05,
        "epoch": 0.5530963559132573,
        "step": 7422
    },
    {
        "loss": 2.819,
        "grad_norm": 2.2934529781341553,
        "learning_rate": 5.283576392442773e-05,
        "epoch": 0.553170877114539,
        "step": 7423
    },
    {
        "loss": 1.5938,
        "grad_norm": 3.630805253982544,
        "learning_rate": 5.277372786569145e-05,
        "epoch": 0.5532453983158209,
        "step": 7424
    },
    {
        "loss": 2.6998,
        "grad_norm": 3.3334293365478516,
        "learning_rate": 5.2711715190228706e-05,
        "epoch": 0.5533199195171026,
        "step": 7425
    },
    {
        "loss": 2.2818,
        "grad_norm": 2.5811281204223633,
        "learning_rate": 5.26497259287443e-05,
        "epoch": 0.5533944407183844,
        "step": 7426
    },
    {
        "loss": 2.2348,
        "grad_norm": 5.1583356857299805,
        "learning_rate": 5.258776011193101e-05,
        "epoch": 0.5534689619196661,
        "step": 7427
    },
    {
        "loss": 3.02,
        "grad_norm": 3.4756569862365723,
        "learning_rate": 5.252581777047001e-05,
        "epoch": 0.5535434831209479,
        "step": 7428
    },
    {
        "loss": 2.2585,
        "grad_norm": 3.816410541534424,
        "learning_rate": 5.246389893503124e-05,
        "epoch": 0.5536180043222296,
        "step": 7429
    },
    {
        "loss": 2.6587,
        "grad_norm": 2.031043529510498,
        "learning_rate": 5.240200363627261e-05,
        "epoch": 0.5536925255235114,
        "step": 7430
    },
    {
        "loss": 1.232,
        "grad_norm": 3.2314040660858154,
        "learning_rate": 5.234013190484048e-05,
        "epoch": 0.5537670467247932,
        "step": 7431
    },
    {
        "loss": 2.4315,
        "grad_norm": 2.8628671169281006,
        "learning_rate": 5.227828377136965e-05,
        "epoch": 0.553841567926075,
        "step": 7432
    },
    {
        "loss": 2.6117,
        "grad_norm": 4.492983818054199,
        "learning_rate": 5.2216459266483076e-05,
        "epoch": 0.5539160891273567,
        "step": 7433
    },
    {
        "loss": 2.2589,
        "grad_norm": 3.8611297607421875,
        "learning_rate": 5.215465842079217e-05,
        "epoch": 0.5539906103286385,
        "step": 7434
    },
    {
        "loss": 2.4928,
        "grad_norm": 2.0211920738220215,
        "learning_rate": 5.209288126489651e-05,
        "epoch": 0.5540651315299202,
        "step": 7435
    },
    {
        "loss": 2.3315,
        "grad_norm": 3.636880397796631,
        "learning_rate": 5.2031127829383875e-05,
        "epoch": 0.554139652731202,
        "step": 7436
    },
    {
        "loss": 2.6102,
        "grad_norm": 2.9470767974853516,
        "learning_rate": 5.196939814483065e-05,
        "epoch": 0.5542141739324838,
        "step": 7437
    },
    {
        "loss": 2.0676,
        "grad_norm": 2.8220555782318115,
        "learning_rate": 5.190769224180099e-05,
        "epoch": 0.5542886951337656,
        "step": 7438
    },
    {
        "loss": 2.2325,
        "grad_norm": 2.113469362258911,
        "learning_rate": 5.184601015084774e-05,
        "epoch": 0.5543632163350474,
        "step": 7439
    },
    {
        "loss": 2.5449,
        "grad_norm": 2.4419264793395996,
        "learning_rate": 5.178435190251164e-05,
        "epoch": 0.5544377375363291,
        "step": 7440
    },
    {
        "loss": 2.7827,
        "grad_norm": 3.03373384475708,
        "learning_rate": 5.172271752732156e-05,
        "epoch": 0.5545122587376109,
        "step": 7441
    },
    {
        "loss": 2.3612,
        "grad_norm": 3.073843002319336,
        "learning_rate": 5.1661107055794885e-05,
        "epoch": 0.5545867799388926,
        "step": 7442
    },
    {
        "loss": 2.4262,
        "grad_norm": 2.619058609008789,
        "learning_rate": 5.159952051843687e-05,
        "epoch": 0.5546613011401744,
        "step": 7443
    },
    {
        "loss": 2.8028,
        "grad_norm": 2.9700188636779785,
        "learning_rate": 5.15379579457411e-05,
        "epoch": 0.5547358223414561,
        "step": 7444
    },
    {
        "loss": 2.1071,
        "grad_norm": 3.0803236961364746,
        "learning_rate": 5.147641936818917e-05,
        "epoch": 0.554810343542738,
        "step": 7445
    },
    {
        "loss": 2.2426,
        "grad_norm": 4.0197367668151855,
        "learning_rate": 5.141490481625074e-05,
        "epoch": 0.5548848647440197,
        "step": 7446
    },
    {
        "loss": 2.2712,
        "grad_norm": 2.8648059368133545,
        "learning_rate": 5.135341432038395e-05,
        "epoch": 0.5549593859453015,
        "step": 7447
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.139880418777466,
        "learning_rate": 5.129194791103444e-05,
        "epoch": 0.5550339071465832,
        "step": 7448
    },
    {
        "loss": 2.5065,
        "grad_norm": 3.123358726501465,
        "learning_rate": 5.1230505618636534e-05,
        "epoch": 0.555108428347865,
        "step": 7449
    },
    {
        "loss": 2.1998,
        "grad_norm": 3.14601731300354,
        "learning_rate": 5.116908747361218e-05,
        "epoch": 0.5551829495491467,
        "step": 7450
    },
    {
        "loss": 2.2958,
        "grad_norm": 3.00578236579895,
        "learning_rate": 5.110769350637149e-05,
        "epoch": 0.5552574707504285,
        "step": 7451
    },
    {
        "loss": 2.4042,
        "grad_norm": 3.1319944858551025,
        "learning_rate": 5.1046323747312696e-05,
        "epoch": 0.5553319919517102,
        "step": 7452
    },
    {
        "loss": 2.8962,
        "grad_norm": 2.420048236846924,
        "learning_rate": 5.0984978226821864e-05,
        "epoch": 0.555406513152992,
        "step": 7453
    },
    {
        "loss": 1.9163,
        "grad_norm": 1.8723629713058472,
        "learning_rate": 5.0923656975273323e-05,
        "epoch": 0.5554810343542738,
        "step": 7454
    },
    {
        "loss": 2.3527,
        "grad_norm": 3.1857972145080566,
        "learning_rate": 5.08623600230292e-05,
        "epoch": 0.5555555555555556,
        "step": 7455
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.6676530838012695,
        "learning_rate": 5.080108740043943e-05,
        "epoch": 0.5556300767568373,
        "step": 7456
    },
    {
        "loss": 1.7749,
        "grad_norm": 3.1939427852630615,
        "learning_rate": 5.0739839137842345e-05,
        "epoch": 0.5557045979581191,
        "step": 7457
    },
    {
        "loss": 2.3706,
        "grad_norm": 2.1396262645721436,
        "learning_rate": 5.067861526556377e-05,
        "epoch": 0.5557791191594008,
        "step": 7458
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.1715152263641357,
        "learning_rate": 5.061741581391784e-05,
        "epoch": 0.5558536403606826,
        "step": 7459
    },
    {
        "loss": 2.2067,
        "grad_norm": 3.1266939640045166,
        "learning_rate": 5.055624081320631e-05,
        "epoch": 0.5559281615619643,
        "step": 7460
    },
    {
        "loss": 2.406,
        "grad_norm": 3.629697561264038,
        "learning_rate": 5.04950902937189e-05,
        "epoch": 0.5560026827632462,
        "step": 7461
    },
    {
        "loss": 1.8257,
        "grad_norm": 5.098698139190674,
        "learning_rate": 5.0433964285733304e-05,
        "epoch": 0.5560772039645279,
        "step": 7462
    },
    {
        "loss": 2.5875,
        "grad_norm": 3.4267053604125977,
        "learning_rate": 5.0372862819514944e-05,
        "epoch": 0.5561517251658097,
        "step": 7463
    },
    {
        "loss": 2.5126,
        "grad_norm": 3.7042744159698486,
        "learning_rate": 5.0311785925317156e-05,
        "epoch": 0.5562262463670914,
        "step": 7464
    },
    {
        "loss": 1.5559,
        "grad_norm": 3.1375246047973633,
        "learning_rate": 5.025073363338117e-05,
        "epoch": 0.5563007675683732,
        "step": 7465
    },
    {
        "loss": 2.5622,
        "grad_norm": 2.4806466102600098,
        "learning_rate": 5.018970597393584e-05,
        "epoch": 0.5563752887696549,
        "step": 7466
    },
    {
        "loss": 2.6396,
        "grad_norm": 1.8613007068634033,
        "learning_rate": 5.012870297719816e-05,
        "epoch": 0.5564498099709367,
        "step": 7467
    },
    {
        "loss": 1.2788,
        "grad_norm": 3.1888370513916016,
        "learning_rate": 5.006772467337259e-05,
        "epoch": 0.5565243311722184,
        "step": 7468
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.219285011291504,
        "learning_rate": 5.0006771092651416e-05,
        "epoch": 0.5565988523735003,
        "step": 7469
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.8818130493164062,
        "learning_rate": 4.994584226521498e-05,
        "epoch": 0.556673373574782,
        "step": 7470
    },
    {
        "loss": 2.1748,
        "grad_norm": 3.466421604156494,
        "learning_rate": 4.988493822123088e-05,
        "epoch": 0.5567478947760638,
        "step": 7471
    },
    {
        "loss": 2.3877,
        "grad_norm": 3.793642044067383,
        "learning_rate": 4.982405899085491e-05,
        "epoch": 0.5568224159773456,
        "step": 7472
    },
    {
        "loss": 2.0659,
        "grad_norm": 3.871264934539795,
        "learning_rate": 4.976320460423032e-05,
        "epoch": 0.5568969371786273,
        "step": 7473
    },
    {
        "loss": 2.7142,
        "grad_norm": 2.6658151149749756,
        "learning_rate": 4.970237509148794e-05,
        "epoch": 0.5569714583799091,
        "step": 7474
    },
    {
        "loss": 2.3083,
        "grad_norm": 2.8030080795288086,
        "learning_rate": 4.964157048274667e-05,
        "epoch": 0.5570459795811908,
        "step": 7475
    },
    {
        "loss": 2.5112,
        "grad_norm": 2.684206247329712,
        "learning_rate": 4.958079080811266e-05,
        "epoch": 0.5571205007824727,
        "step": 7476
    },
    {
        "loss": 1.9287,
        "grad_norm": 3.5678160190582275,
        "learning_rate": 4.952003609768016e-05,
        "epoch": 0.5571950219837544,
        "step": 7477
    },
    {
        "loss": 1.8418,
        "grad_norm": 3.382734775543213,
        "learning_rate": 4.945930638153071e-05,
        "epoch": 0.5572695431850362,
        "step": 7478
    },
    {
        "loss": 2.3683,
        "grad_norm": 3.7957606315612793,
        "learning_rate": 4.939860168973343e-05,
        "epoch": 0.5573440643863179,
        "step": 7479
    },
    {
        "loss": 2.2775,
        "grad_norm": 2.6187241077423096,
        "learning_rate": 4.933792205234543e-05,
        "epoch": 0.5574185855875997,
        "step": 7480
    },
    {
        "loss": 2.9299,
        "grad_norm": 2.349076986312866,
        "learning_rate": 4.927726749941102e-05,
        "epoch": 0.5574931067888814,
        "step": 7481
    },
    {
        "loss": 2.6563,
        "grad_norm": 2.012019157409668,
        "learning_rate": 4.921663806096243e-05,
        "epoch": 0.5575676279901632,
        "step": 7482
    },
    {
        "loss": 2.6352,
        "grad_norm": 1.7599952220916748,
        "learning_rate": 4.915603376701914e-05,
        "epoch": 0.557642149191445,
        "step": 7483
    },
    {
        "loss": 1.9678,
        "grad_norm": 2.8185129165649414,
        "learning_rate": 4.909545464758827e-05,
        "epoch": 0.5577166703927268,
        "step": 7484
    },
    {
        "loss": 2.4997,
        "grad_norm": 1.996246576309204,
        "learning_rate": 4.903490073266472e-05,
        "epoch": 0.5577911915940085,
        "step": 7485
    },
    {
        "loss": 2.23,
        "grad_norm": 3.9834585189819336,
        "learning_rate": 4.8974372052230464e-05,
        "epoch": 0.5578657127952903,
        "step": 7486
    },
    {
        "loss": 2.6892,
        "grad_norm": 2.235991954803467,
        "learning_rate": 4.891386863625549e-05,
        "epoch": 0.557940233996572,
        "step": 7487
    },
    {
        "loss": 2.6574,
        "grad_norm": 2.268352746963501,
        "learning_rate": 4.885339051469691e-05,
        "epoch": 0.5580147551978538,
        "step": 7488
    },
    {
        "loss": 2.3762,
        "grad_norm": 2.7002737522125244,
        "learning_rate": 4.879293771749929e-05,
        "epoch": 0.5580892763991355,
        "step": 7489
    },
    {
        "loss": 1.75,
        "grad_norm": 3.8203177452087402,
        "learning_rate": 4.873251027459503e-05,
        "epoch": 0.5581637976004173,
        "step": 7490
    },
    {
        "loss": 2.5402,
        "grad_norm": 3.6584887504577637,
        "learning_rate": 4.867210821590359e-05,
        "epoch": 0.558238318801699,
        "step": 7491
    },
    {
        "loss": 2.6966,
        "grad_norm": 2.1150929927825928,
        "learning_rate": 4.861173157133208e-05,
        "epoch": 0.5583128400029809,
        "step": 7492
    },
    {
        "loss": 2.8438,
        "grad_norm": 1.9395400285720825,
        "learning_rate": 4.855138037077491e-05,
        "epoch": 0.5583873612042626,
        "step": 7493
    },
    {
        "loss": 3.0402,
        "grad_norm": 2.6820437908172607,
        "learning_rate": 4.84910546441139e-05,
        "epoch": 0.5584618824055444,
        "step": 7494
    },
    {
        "loss": 2.4491,
        "grad_norm": 3.111854076385498,
        "learning_rate": 4.843075442121837e-05,
        "epoch": 0.5585364036068261,
        "step": 7495
    },
    {
        "loss": 2.4774,
        "grad_norm": 3.8497023582458496,
        "learning_rate": 4.8370479731944896e-05,
        "epoch": 0.5586109248081079,
        "step": 7496
    },
    {
        "loss": 2.7704,
        "grad_norm": 2.096447229385376,
        "learning_rate": 4.831023060613733e-05,
        "epoch": 0.5586854460093896,
        "step": 7497
    },
    {
        "loss": 2.1311,
        "grad_norm": 4.490106582641602,
        "learning_rate": 4.825000707362722e-05,
        "epoch": 0.5587599672106714,
        "step": 7498
    },
    {
        "loss": 1.4989,
        "grad_norm": 3.8848891258239746,
        "learning_rate": 4.818980916423299e-05,
        "epoch": 0.5588344884119532,
        "step": 7499
    },
    {
        "loss": 1.0731,
        "grad_norm": 3.0751237869262695,
        "learning_rate": 4.812963690776086e-05,
        "epoch": 0.558909009613235,
        "step": 7500
    },
    {
        "loss": 2.1931,
        "grad_norm": 3.3259212970733643,
        "learning_rate": 4.806949033400387e-05,
        "epoch": 0.5589835308145167,
        "step": 7501
    },
    {
        "loss": 2.3676,
        "grad_norm": 2.5038487911224365,
        "learning_rate": 4.800936947274255e-05,
        "epoch": 0.5590580520157985,
        "step": 7502
    },
    {
        "loss": 2.715,
        "grad_norm": 2.7356221675872803,
        "learning_rate": 4.7949274353744846e-05,
        "epoch": 0.5591325732170802,
        "step": 7503
    },
    {
        "loss": 2.364,
        "grad_norm": 1.750187873840332,
        "learning_rate": 4.7889205006765715e-05,
        "epoch": 0.559207094418362,
        "step": 7504
    },
    {
        "loss": 2.5809,
        "grad_norm": 2.789386034011841,
        "learning_rate": 4.7829161461547514e-05,
        "epoch": 0.5592816156196437,
        "step": 7505
    },
    {
        "loss": 2.3322,
        "grad_norm": 3.247864246368408,
        "learning_rate": 4.776914374781974e-05,
        "epoch": 0.5593561368209256,
        "step": 7506
    },
    {
        "loss": 1.8111,
        "grad_norm": 3.2304933071136475,
        "learning_rate": 4.770915189529902e-05,
        "epoch": 0.5594306580222074,
        "step": 7507
    },
    {
        "loss": 1.9558,
        "grad_norm": 3.559976816177368,
        "learning_rate": 4.7649185933689544e-05,
        "epoch": 0.5595051792234891,
        "step": 7508
    },
    {
        "loss": 2.9206,
        "grad_norm": 3.0132179260253906,
        "learning_rate": 4.758924589268212e-05,
        "epoch": 0.5595797004247709,
        "step": 7509
    },
    {
        "loss": 2.1589,
        "grad_norm": 2.740748167037964,
        "learning_rate": 4.75293318019553e-05,
        "epoch": 0.5596542216260526,
        "step": 7510
    },
    {
        "loss": 2.1466,
        "grad_norm": 2.6600403785705566,
        "learning_rate": 4.7469443691174386e-05,
        "epoch": 0.5597287428273344,
        "step": 7511
    },
    {
        "loss": 2.4553,
        "grad_norm": 2.674037218093872,
        "learning_rate": 4.740958158999191e-05,
        "epoch": 0.5598032640286161,
        "step": 7512
    },
    {
        "loss": 2.3723,
        "grad_norm": 1.9453843832015991,
        "learning_rate": 4.734974552804764e-05,
        "epoch": 0.559877785229898,
        "step": 7513
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.7303857803344727,
        "learning_rate": 4.728993553496824e-05,
        "epoch": 0.5599523064311797,
        "step": 7514
    },
    {
        "loss": 2.339,
        "grad_norm": 3.4139211177825928,
        "learning_rate": 4.723015164036779e-05,
        "epoch": 0.5600268276324615,
        "step": 7515
    },
    {
        "loss": 1.8904,
        "grad_norm": 3.4828126430511475,
        "learning_rate": 4.7170393873847116e-05,
        "epoch": 0.5601013488337432,
        "step": 7516
    },
    {
        "loss": 2.2299,
        "grad_norm": 4.179931163787842,
        "learning_rate": 4.711066226499417e-05,
        "epoch": 0.560175870035025,
        "step": 7517
    },
    {
        "loss": 2.2358,
        "grad_norm": 3.3252499103546143,
        "learning_rate": 4.705095684338423e-05,
        "epoch": 0.5602503912363067,
        "step": 7518
    },
    {
        "loss": 2.5739,
        "grad_norm": 3.5953288078308105,
        "learning_rate": 4.699127763857919e-05,
        "epoch": 0.5603249124375885,
        "step": 7519
    },
    {
        "loss": 1.5253,
        "grad_norm": 2.304239273071289,
        "learning_rate": 4.6931624680128414e-05,
        "epoch": 0.5603994336388702,
        "step": 7520
    },
    {
        "loss": 2.8479,
        "grad_norm": 3.250826120376587,
        "learning_rate": 4.687199799756792e-05,
        "epoch": 0.560473954840152,
        "step": 7521
    },
    {
        "loss": 2.7446,
        "grad_norm": 1.619560956954956,
        "learning_rate": 4.6812397620420786e-05,
        "epoch": 0.5605484760414338,
        "step": 7522
    },
    {
        "loss": 2.2804,
        "grad_norm": 3.1161649227142334,
        "learning_rate": 4.675282357819718e-05,
        "epoch": 0.5606229972427156,
        "step": 7523
    },
    {
        "loss": 2.1561,
        "grad_norm": 3.1658577919006348,
        "learning_rate": 4.669327590039414e-05,
        "epoch": 0.5606975184439973,
        "step": 7524
    },
    {
        "loss": 1.8138,
        "grad_norm": 1.971037745475769,
        "learning_rate": 4.6633754616495626e-05,
        "epoch": 0.5607720396452791,
        "step": 7525
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.8752691745758057,
        "learning_rate": 4.657425975597265e-05,
        "epoch": 0.5608465608465608,
        "step": 7526
    },
    {
        "loss": 2.5517,
        "grad_norm": 2.424682378768921,
        "learning_rate": 4.651479134828292e-05,
        "epoch": 0.5609210820478426,
        "step": 7527
    },
    {
        "loss": 2.0623,
        "grad_norm": 3.368659496307373,
        "learning_rate": 4.64553494228714e-05,
        "epoch": 0.5609956032491243,
        "step": 7528
    },
    {
        "loss": 1.714,
        "grad_norm": 3.7276999950408936,
        "learning_rate": 4.639593400916963e-05,
        "epoch": 0.5610701244504062,
        "step": 7529
    },
    {
        "loss": 2.506,
        "grad_norm": 2.9989798069000244,
        "learning_rate": 4.633654513659602e-05,
        "epoch": 0.5611446456516879,
        "step": 7530
    },
    {
        "loss": 2.5024,
        "grad_norm": 2.448775291442871,
        "learning_rate": 4.627718283455621e-05,
        "epoch": 0.5612191668529697,
        "step": 7531
    },
    {
        "loss": 2.5786,
        "grad_norm": 3.2257027626037598,
        "learning_rate": 4.621784713244215e-05,
        "epoch": 0.5612936880542514,
        "step": 7532
    },
    {
        "loss": 2.3474,
        "grad_norm": 2.3541996479034424,
        "learning_rate": 4.6158538059633084e-05,
        "epoch": 0.5613682092555332,
        "step": 7533
    },
    {
        "loss": 1.6908,
        "grad_norm": 3.1963722705841064,
        "learning_rate": 4.609925564549482e-05,
        "epoch": 0.5614427304568149,
        "step": 7534
    },
    {
        "loss": 2.5674,
        "grad_norm": 2.099384307861328,
        "learning_rate": 4.603999991937989e-05,
        "epoch": 0.5615172516580967,
        "step": 7535
    },
    {
        "loss": 2.6035,
        "grad_norm": 4.2635650634765625,
        "learning_rate": 4.5980770910627933e-05,
        "epoch": 0.5615917728593784,
        "step": 7536
    },
    {
        "loss": 2.0189,
        "grad_norm": 2.656928539276123,
        "learning_rate": 4.592156864856497e-05,
        "epoch": 0.5616662940606603,
        "step": 7537
    },
    {
        "loss": 2.8822,
        "grad_norm": 3.7487902641296387,
        "learning_rate": 4.586239316250426e-05,
        "epoch": 0.561740815261942,
        "step": 7538
    },
    {
        "loss": 1.6282,
        "grad_norm": 4.330724716186523,
        "learning_rate": 4.580324448174531e-05,
        "epoch": 0.5618153364632238,
        "step": 7539
    },
    {
        "loss": 2.5612,
        "grad_norm": 2.3681085109710693,
        "learning_rate": 4.574412263557453e-05,
        "epoch": 0.5618898576645055,
        "step": 7540
    },
    {
        "loss": 2.4043,
        "grad_norm": 3.3505420684814453,
        "learning_rate": 4.5685027653265264e-05,
        "epoch": 0.5619643788657873,
        "step": 7541
    },
    {
        "loss": 2.5951,
        "grad_norm": 2.464529037475586,
        "learning_rate": 4.562595956407725e-05,
        "epoch": 0.5620389000670691,
        "step": 7542
    },
    {
        "loss": 2.3568,
        "grad_norm": 3.775696039199829,
        "learning_rate": 4.5566918397257105e-05,
        "epoch": 0.5621134212683508,
        "step": 7543
    },
    {
        "loss": 1.9281,
        "grad_norm": 2.348299980163574,
        "learning_rate": 4.550790418203801e-05,
        "epoch": 0.5621879424696327,
        "step": 7544
    },
    {
        "loss": 2.4353,
        "grad_norm": 2.9336485862731934,
        "learning_rate": 4.544891694763974e-05,
        "epoch": 0.5622624636709144,
        "step": 7545
    },
    {
        "loss": 1.5891,
        "grad_norm": 2.677164316177368,
        "learning_rate": 4.538995672326902e-05,
        "epoch": 0.5623369848721962,
        "step": 7546
    },
    {
        "loss": 2.7387,
        "grad_norm": 2.5883231163024902,
        "learning_rate": 4.533102353811871e-05,
        "epoch": 0.5624115060734779,
        "step": 7547
    },
    {
        "loss": 2.1009,
        "grad_norm": 3.1655490398406982,
        "learning_rate": 4.527211742136882e-05,
        "epoch": 0.5624860272747597,
        "step": 7548
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.7237708568573,
        "learning_rate": 4.5213238402185595e-05,
        "epoch": 0.5625605484760414,
        "step": 7549
    },
    {
        "loss": 2.6831,
        "grad_norm": 1.8300540447235107,
        "learning_rate": 4.515438650972185e-05,
        "epoch": 0.5626350696773232,
        "step": 7550
    },
    {
        "loss": 1.7798,
        "grad_norm": 3.53609561920166,
        "learning_rate": 4.5095561773117304e-05,
        "epoch": 0.562709590878605,
        "step": 7551
    },
    {
        "loss": 2.7696,
        "grad_norm": 3.0220725536346436,
        "learning_rate": 4.503676422149786e-05,
        "epoch": 0.5627841120798868,
        "step": 7552
    },
    {
        "loss": 2.4488,
        "grad_norm": 2.858215093612671,
        "learning_rate": 4.497799388397619e-05,
        "epoch": 0.5628586332811685,
        "step": 7553
    },
    {
        "loss": 2.0239,
        "grad_norm": 3.622257947921753,
        "learning_rate": 4.4919250789651376e-05,
        "epoch": 0.5629331544824503,
        "step": 7554
    },
    {
        "loss": 2.5968,
        "grad_norm": 2.1640164852142334,
        "learning_rate": 4.486053496760901e-05,
        "epoch": 0.563007675683732,
        "step": 7555
    },
    {
        "loss": 2.5518,
        "grad_norm": 3.4664387702941895,
        "learning_rate": 4.480184644692129e-05,
        "epoch": 0.5630821968850138,
        "step": 7556
    },
    {
        "loss": 2.4873,
        "grad_norm": 2.1312978267669678,
        "learning_rate": 4.474318525664679e-05,
        "epoch": 0.5631567180862955,
        "step": 7557
    },
    {
        "loss": 2.8727,
        "grad_norm": 2.057645320892334,
        "learning_rate": 4.468455142583047e-05,
        "epoch": 0.5632312392875773,
        "step": 7558
    },
    {
        "loss": 2.6367,
        "grad_norm": 3.835554838180542,
        "learning_rate": 4.462594498350407e-05,
        "epoch": 0.563305760488859,
        "step": 7559
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.924487829208374,
        "learning_rate": 4.456736595868538e-05,
        "epoch": 0.5633802816901409,
        "step": 7560
    },
    {
        "loss": 2.4634,
        "grad_norm": 2.7755355834960938,
        "learning_rate": 4.4508814380379005e-05,
        "epoch": 0.5634548028914226,
        "step": 7561
    },
    {
        "loss": 2.2857,
        "grad_norm": 3.0285797119140625,
        "learning_rate": 4.445029027757558e-05,
        "epoch": 0.5635293240927044,
        "step": 7562
    },
    {
        "loss": 2.5595,
        "grad_norm": 3.4075493812561035,
        "learning_rate": 4.439179367925228e-05,
        "epoch": 0.5636038452939861,
        "step": 7563
    },
    {
        "loss": 2.4793,
        "grad_norm": 2.403672695159912,
        "learning_rate": 4.433332461437285e-05,
        "epoch": 0.5636783664952679,
        "step": 7564
    },
    {
        "loss": 2.6033,
        "grad_norm": 2.230963706970215,
        "learning_rate": 4.4274883111887034e-05,
        "epoch": 0.5637528876965496,
        "step": 7565
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.450172185897827,
        "learning_rate": 4.421646920073131e-05,
        "epoch": 0.5638274088978315,
        "step": 7566
    },
    {
        "loss": 1.3681,
        "grad_norm": 3.456490993499756,
        "learning_rate": 4.415808290982826e-05,
        "epoch": 0.5639019300991132,
        "step": 7567
    },
    {
        "loss": 2.9144,
        "grad_norm": 2.818634510040283,
        "learning_rate": 4.409972426808674e-05,
        "epoch": 0.563976451300395,
        "step": 7568
    },
    {
        "loss": 2.3981,
        "grad_norm": 3.553781032562256,
        "learning_rate": 4.4041393304402204e-05,
        "epoch": 0.5640509725016767,
        "step": 7569
    },
    {
        "loss": 1.5257,
        "grad_norm": 2.6134543418884277,
        "learning_rate": 4.398309004765602e-05,
        "epoch": 0.5641254937029585,
        "step": 7570
    },
    {
        "loss": 2.1807,
        "grad_norm": 1.998288869857788,
        "learning_rate": 4.3924814526716266e-05,
        "epoch": 0.5642000149042402,
        "step": 7571
    },
    {
        "loss": 1.8618,
        "grad_norm": 4.073049545288086,
        "learning_rate": 4.3866566770436955e-05,
        "epoch": 0.564274536105522,
        "step": 7572
    },
    {
        "loss": 2.0433,
        "grad_norm": 4.001419544219971,
        "learning_rate": 4.3808346807658415e-05,
        "epoch": 0.5643490573068037,
        "step": 7573
    },
    {
        "loss": 2.7465,
        "grad_norm": 3.5820751190185547,
        "learning_rate": 4.375015466720733e-05,
        "epoch": 0.5644235785080856,
        "step": 7574
    },
    {
        "loss": 2.9806,
        "grad_norm": 2.200152635574341,
        "learning_rate": 4.369199037789643e-05,
        "epoch": 0.5644980997093673,
        "step": 7575
    },
    {
        "loss": 3.0564,
        "grad_norm": 1.9085360765457153,
        "learning_rate": 4.3633853968524943e-05,
        "epoch": 0.5645726209106491,
        "step": 7576
    },
    {
        "loss": 2.0899,
        "grad_norm": 3.4697461128234863,
        "learning_rate": 4.357574546787795e-05,
        "epoch": 0.5646471421119309,
        "step": 7577
    },
    {
        "loss": 2.0935,
        "grad_norm": 2.40413761138916,
        "learning_rate": 4.351766490472681e-05,
        "epoch": 0.5647216633132126,
        "step": 7578
    },
    {
        "loss": 2.5839,
        "grad_norm": 2.445345163345337,
        "learning_rate": 4.345961230782932e-05,
        "epoch": 0.5647961845144944,
        "step": 7579
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.894486665725708,
        "learning_rate": 4.340158770592903e-05,
        "epoch": 0.5648707057157761,
        "step": 7580
    },
    {
        "loss": 3.0256,
        "grad_norm": 3.224713087081909,
        "learning_rate": 4.3343591127756e-05,
        "epoch": 0.564945226917058,
        "step": 7581
    },
    {
        "loss": 2.2248,
        "grad_norm": 3.612445831298828,
        "learning_rate": 4.328562260202616e-05,
        "epoch": 0.5650197481183397,
        "step": 7582
    },
    {
        "loss": 2.1594,
        "grad_norm": 3.8443827629089355,
        "learning_rate": 4.322768215744156e-05,
        "epoch": 0.5650942693196215,
        "step": 7583
    },
    {
        "loss": 2.447,
        "grad_norm": 3.65342116355896,
        "learning_rate": 4.31697698226905e-05,
        "epoch": 0.5651687905209032,
        "step": 7584
    },
    {
        "loss": 1.8903,
        "grad_norm": 3.505751609802246,
        "learning_rate": 4.31118856264472e-05,
        "epoch": 0.565243311722185,
        "step": 7585
    },
    {
        "loss": 2.5189,
        "grad_norm": 2.5279059410095215,
        "learning_rate": 4.3054029597372104e-05,
        "epoch": 0.5653178329234667,
        "step": 7586
    },
    {
        "loss": 2.7644,
        "grad_norm": 3.047879934310913,
        "learning_rate": 4.299620176411157e-05,
        "epoch": 0.5653923541247485,
        "step": 7587
    },
    {
        "loss": 2.6506,
        "grad_norm": 1.8623014688491821,
        "learning_rate": 4.293840215529794e-05,
        "epoch": 0.5654668753260302,
        "step": 7588
    },
    {
        "loss": 2.8179,
        "grad_norm": 6.995032787322998,
        "learning_rate": 4.288063079954994e-05,
        "epoch": 0.5655413965273121,
        "step": 7589
    },
    {
        "loss": 1.5979,
        "grad_norm": 2.867323637008667,
        "learning_rate": 4.282288772547192e-05,
        "epoch": 0.5656159177285938,
        "step": 7590
    },
    {
        "loss": 2.5992,
        "grad_norm": 1.6509296894073486,
        "learning_rate": 4.276517296165431e-05,
        "epoch": 0.5656904389298756,
        "step": 7591
    },
    {
        "loss": 1.9561,
        "grad_norm": 3.6875109672546387,
        "learning_rate": 4.2707486536673725e-05,
        "epoch": 0.5657649601311573,
        "step": 7592
    },
    {
        "loss": 2.4414,
        "grad_norm": 3.257920026779175,
        "learning_rate": 4.264982847909245e-05,
        "epoch": 0.5658394813324391,
        "step": 7593
    },
    {
        "loss": 1.9469,
        "grad_norm": 4.099370956420898,
        "learning_rate": 4.2592198817459025e-05,
        "epoch": 0.5659140025337208,
        "step": 7594
    },
    {
        "loss": 1.7491,
        "grad_norm": 3.5858891010284424,
        "learning_rate": 4.253459758030776e-05,
        "epoch": 0.5659885237350026,
        "step": 7595
    },
    {
        "loss": 2.3214,
        "grad_norm": 3.966460943222046,
        "learning_rate": 4.247702479615874e-05,
        "epoch": 0.5660630449362843,
        "step": 7596
    },
    {
        "loss": 2.639,
        "grad_norm": 1.9288781881332397,
        "learning_rate": 4.241948049351834e-05,
        "epoch": 0.5661375661375662,
        "step": 7597
    },
    {
        "loss": 1.4941,
        "grad_norm": 3.878467082977295,
        "learning_rate": 4.236196470087844e-05,
        "epoch": 0.5662120873388479,
        "step": 7598
    },
    {
        "loss": 2.4873,
        "grad_norm": 2.1617910861968994,
        "learning_rate": 4.230447744671724e-05,
        "epoch": 0.5662866085401297,
        "step": 7599
    },
    {
        "loss": 2.5668,
        "grad_norm": 2.2304112911224365,
        "learning_rate": 4.2247018759498394e-05,
        "epoch": 0.5663611297414114,
        "step": 7600
    },
    {
        "loss": 1.7511,
        "grad_norm": 4.248552322387695,
        "learning_rate": 4.21895886676715e-05,
        "epoch": 0.5664356509426932,
        "step": 7601
    },
    {
        "loss": 2.4402,
        "grad_norm": 2.1517696380615234,
        "learning_rate": 4.213218719967229e-05,
        "epoch": 0.5665101721439749,
        "step": 7602
    },
    {
        "loss": 2.2734,
        "grad_norm": 2.5306038856506348,
        "learning_rate": 4.207481438392198e-05,
        "epoch": 0.5665846933452567,
        "step": 7603
    },
    {
        "loss": 2.5977,
        "grad_norm": 3.524817705154419,
        "learning_rate": 4.201747024882779e-05,
        "epoch": 0.5666592145465384,
        "step": 7604
    },
    {
        "loss": 2.1195,
        "grad_norm": 3.369138479232788,
        "learning_rate": 4.1960154822782624e-05,
        "epoch": 0.5667337357478203,
        "step": 7605
    },
    {
        "loss": 2.4182,
        "grad_norm": 3.241915702819824,
        "learning_rate": 4.190286813416523e-05,
        "epoch": 0.566808256949102,
        "step": 7606
    },
    {
        "loss": 1.9964,
        "grad_norm": 2.056286573410034,
        "learning_rate": 4.184561021134017e-05,
        "epoch": 0.5668827781503838,
        "step": 7607
    },
    {
        "loss": 2.5626,
        "grad_norm": 3.7338342666625977,
        "learning_rate": 4.178838108265758e-05,
        "epoch": 0.5669572993516655,
        "step": 7608
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.419114351272583,
        "learning_rate": 4.173118077645368e-05,
        "epoch": 0.5670318205529473,
        "step": 7609
    },
    {
        "loss": 2.5802,
        "grad_norm": 2.624260902404785,
        "learning_rate": 4.167400932105013e-05,
        "epoch": 0.567106341754229,
        "step": 7610
    },
    {
        "loss": 2.3763,
        "grad_norm": 2.745351791381836,
        "learning_rate": 4.1616866744754266e-05,
        "epoch": 0.5671808629555108,
        "step": 7611
    },
    {
        "loss": 2.688,
        "grad_norm": 2.804429054260254,
        "learning_rate": 4.1559753075859464e-05,
        "epoch": 0.5672553841567926,
        "step": 7612
    },
    {
        "loss": 2.4474,
        "grad_norm": 1.6149340867996216,
        "learning_rate": 4.150266834264442e-05,
        "epoch": 0.5673299053580744,
        "step": 7613
    },
    {
        "loss": 2.573,
        "grad_norm": 4.797003269195557,
        "learning_rate": 4.144561257337375e-05,
        "epoch": 0.5674044265593562,
        "step": 7614
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.1246869564056396,
        "learning_rate": 4.138858579629758e-05,
        "epoch": 0.5674789477606379,
        "step": 7615
    },
    {
        "loss": 2.6111,
        "grad_norm": 2.5280725955963135,
        "learning_rate": 4.13315880396517e-05,
        "epoch": 0.5675534689619197,
        "step": 7616
    },
    {
        "loss": 2.2223,
        "grad_norm": 2.5282819271087646,
        "learning_rate": 4.1274619331657646e-05,
        "epoch": 0.5676279901632014,
        "step": 7617
    },
    {
        "loss": 2.3569,
        "grad_norm": 1.5986863374710083,
        "learning_rate": 4.121767970052245e-05,
        "epoch": 0.5677025113644832,
        "step": 7618
    },
    {
        "loss": 2.5723,
        "grad_norm": 2.619036912918091,
        "learning_rate": 4.116076917443867e-05,
        "epoch": 0.567777032565765,
        "step": 7619
    },
    {
        "loss": 2.7332,
        "grad_norm": 3.574708938598633,
        "learning_rate": 4.1103887781584814e-05,
        "epoch": 0.5678515537670468,
        "step": 7620
    },
    {
        "loss": 2.1534,
        "grad_norm": 2.0857994556427,
        "learning_rate": 4.104703555012447e-05,
        "epoch": 0.5679260749683285,
        "step": 7621
    },
    {
        "loss": 2.2397,
        "grad_norm": 2.493213176727295,
        "learning_rate": 4.099021250820731e-05,
        "epoch": 0.5680005961696103,
        "step": 7622
    },
    {
        "loss": 2.305,
        "grad_norm": 2.4882800579071045,
        "learning_rate": 4.093341868396812e-05,
        "epoch": 0.568075117370892,
        "step": 7623
    },
    {
        "loss": 2.9141,
        "grad_norm": 2.091320037841797,
        "learning_rate": 4.087665410552731e-05,
        "epoch": 0.5681496385721738,
        "step": 7624
    },
    {
        "loss": 2.1063,
        "grad_norm": 3.3941047191619873,
        "learning_rate": 4.0819918800991083e-05,
        "epoch": 0.5682241597734555,
        "step": 7625
    },
    {
        "loss": 2.5545,
        "grad_norm": 3.9087722301483154,
        "learning_rate": 4.076321279845073e-05,
        "epoch": 0.5682986809747373,
        "step": 7626
    },
    {
        "loss": 1.8353,
        "grad_norm": 3.8543500900268555,
        "learning_rate": 4.070653612598343e-05,
        "epoch": 0.568373202176019,
        "step": 7627
    },
    {
        "loss": 2.5188,
        "grad_norm": 1.9619799852371216,
        "learning_rate": 4.0649888811651614e-05,
        "epoch": 0.5684477233773009,
        "step": 7628
    },
    {
        "loss": 1.9406,
        "grad_norm": 3.222820281982422,
        "learning_rate": 4.059327088350304e-05,
        "epoch": 0.5685222445785826,
        "step": 7629
    },
    {
        "loss": 2.9193,
        "grad_norm": 3.8403027057647705,
        "learning_rate": 4.0536682369571376e-05,
        "epoch": 0.5685967657798644,
        "step": 7630
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.5137133598327637,
        "learning_rate": 4.0480123297875204e-05,
        "epoch": 0.5686712869811461,
        "step": 7631
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.1068782806396484,
        "learning_rate": 4.0423593696418995e-05,
        "epoch": 0.5687458081824279,
        "step": 7632
    },
    {
        "loss": 2.5926,
        "grad_norm": 2.2342231273651123,
        "learning_rate": 4.036709359319231e-05,
        "epoch": 0.5688203293837096,
        "step": 7633
    },
    {
        "loss": 2.424,
        "grad_norm": 1.8885749578475952,
        "learning_rate": 4.0310623016170146e-05,
        "epoch": 0.5688948505849915,
        "step": 7634
    },
    {
        "loss": 2.8044,
        "grad_norm": 2.9363791942596436,
        "learning_rate": 4.025418199331302e-05,
        "epoch": 0.5689693717862732,
        "step": 7635
    },
    {
        "loss": 2.1038,
        "grad_norm": 4.0360636711120605,
        "learning_rate": 4.019777055256665e-05,
        "epoch": 0.569043892987555,
        "step": 7636
    },
    {
        "loss": 2.9839,
        "grad_norm": 3.6719462871551514,
        "learning_rate": 4.014138872186227e-05,
        "epoch": 0.5691184141888367,
        "step": 7637
    },
    {
        "loss": 2.6641,
        "grad_norm": 1.8601406812667847,
        "learning_rate": 4.008503652911633e-05,
        "epoch": 0.5691929353901185,
        "step": 7638
    },
    {
        "loss": 2.0801,
        "grad_norm": 3.261044979095459,
        "learning_rate": 4.0028714002230526e-05,
        "epoch": 0.5692674565914002,
        "step": 7639
    },
    {
        "loss": 2.3641,
        "grad_norm": 2.1829869747161865,
        "learning_rate": 3.997242116909222e-05,
        "epoch": 0.569341977792682,
        "step": 7640
    },
    {
        "loss": 2.0923,
        "grad_norm": 3.312467098236084,
        "learning_rate": 3.9916158057573616e-05,
        "epoch": 0.5694164989939637,
        "step": 7641
    },
    {
        "loss": 2.1791,
        "grad_norm": 3.806365489959717,
        "learning_rate": 3.985992469553263e-05,
        "epoch": 0.5694910201952456,
        "step": 7642
    },
    {
        "loss": 2.8899,
        "grad_norm": 3.1064693927764893,
        "learning_rate": 3.9803721110812134e-05,
        "epoch": 0.5695655413965273,
        "step": 7643
    },
    {
        "loss": 2.6585,
        "grad_norm": 1.8157079219818115,
        "learning_rate": 3.974754733124033e-05,
        "epoch": 0.5696400625978091,
        "step": 7644
    },
    {
        "loss": 2.5903,
        "grad_norm": 2.504019021987915,
        "learning_rate": 3.9691403384630776e-05,
        "epoch": 0.5697145837990908,
        "step": 7645
    },
    {
        "loss": 2.9579,
        "grad_norm": 2.0063586235046387,
        "learning_rate": 3.9635289298782097e-05,
        "epoch": 0.5697891050003726,
        "step": 7646
    },
    {
        "loss": 2.8207,
        "grad_norm": 2.679262161254883,
        "learning_rate": 3.9579205101478277e-05,
        "epoch": 0.5698636262016543,
        "step": 7647
    },
    {
        "loss": 1.7424,
        "grad_norm": 4.561868667602539,
        "learning_rate": 3.952315082048843e-05,
        "epoch": 0.5699381474029361,
        "step": 7648
    },
    {
        "loss": 2.9662,
        "grad_norm": 2.4322102069854736,
        "learning_rate": 3.9467126483566744e-05,
        "epoch": 0.570012668604218,
        "step": 7649
    },
    {
        "loss": 1.7604,
        "grad_norm": 3.329861879348755,
        "learning_rate": 3.9411132118452896e-05,
        "epoch": 0.5700871898054997,
        "step": 7650
    },
    {
        "loss": 2.0848,
        "grad_norm": 2.533635377883911,
        "learning_rate": 3.935516775287149e-05,
        "epoch": 0.5701617110067815,
        "step": 7651
    },
    {
        "loss": 2.1091,
        "grad_norm": 2.692067861557007,
        "learning_rate": 3.929923341453213e-05,
        "epoch": 0.5702362322080632,
        "step": 7652
    },
    {
        "loss": 2.2277,
        "grad_norm": 3.093008041381836,
        "learning_rate": 3.924332913112998e-05,
        "epoch": 0.570310753409345,
        "step": 7653
    },
    {
        "loss": 2.743,
        "grad_norm": 3.026808261871338,
        "learning_rate": 3.9187454930344904e-05,
        "epoch": 0.5703852746106267,
        "step": 7654
    },
    {
        "loss": 2.1517,
        "grad_norm": 3.4102020263671875,
        "learning_rate": 3.9131610839842224e-05,
        "epoch": 0.5704597958119085,
        "step": 7655
    },
    {
        "loss": 2.7028,
        "grad_norm": 1.8931504487991333,
        "learning_rate": 3.9075796887272046e-05,
        "epoch": 0.5705343170131902,
        "step": 7656
    },
    {
        "loss": 2.1174,
        "grad_norm": 3.1505227088928223,
        "learning_rate": 3.902001310026964e-05,
        "epoch": 0.5706088382144721,
        "step": 7657
    },
    {
        "loss": 2.77,
        "grad_norm": 2.3117570877075195,
        "learning_rate": 3.896425950645554e-05,
        "epoch": 0.5706833594157538,
        "step": 7658
    },
    {
        "loss": 2.3377,
        "grad_norm": 2.452404499053955,
        "learning_rate": 3.890853613343501e-05,
        "epoch": 0.5707578806170356,
        "step": 7659
    },
    {
        "loss": 2.5185,
        "grad_norm": 2.016270637512207,
        "learning_rate": 3.8852843008798686e-05,
        "epoch": 0.5708324018183173,
        "step": 7660
    },
    {
        "loss": 2.2927,
        "grad_norm": 2.4292144775390625,
        "learning_rate": 3.8797180160121984e-05,
        "epoch": 0.5709069230195991,
        "step": 7661
    },
    {
        "loss": 2.6372,
        "grad_norm": 2.916393756866455,
        "learning_rate": 3.874154761496527e-05,
        "epoch": 0.5709814442208808,
        "step": 7662
    },
    {
        "loss": 1.6435,
        "grad_norm": 2.8234505653381348,
        "learning_rate": 3.868594540087428e-05,
        "epoch": 0.5710559654221626,
        "step": 7663
    },
    {
        "loss": 2.6166,
        "grad_norm": 4.812574863433838,
        "learning_rate": 3.863037354537933e-05,
        "epoch": 0.5711304866234443,
        "step": 7664
    },
    {
        "loss": 1.9375,
        "grad_norm": 3.1894781589508057,
        "learning_rate": 3.857483207599595e-05,
        "epoch": 0.5712050078247262,
        "step": 7665
    },
    {
        "loss": 2.2193,
        "grad_norm": 3.0722036361694336,
        "learning_rate": 3.8519321020224483e-05,
        "epoch": 0.5712795290260079,
        "step": 7666
    },
    {
        "loss": 2.7271,
        "grad_norm": 3.3389179706573486,
        "learning_rate": 3.8463840405550267e-05,
        "epoch": 0.5713540502272897,
        "step": 7667
    },
    {
        "loss": 2.1474,
        "grad_norm": 3.5293853282928467,
        "learning_rate": 3.8408390259443626e-05,
        "epoch": 0.5714285714285714,
        "step": 7668
    },
    {
        "loss": 2.3489,
        "grad_norm": 1.813693881034851,
        "learning_rate": 3.83529706093596e-05,
        "epoch": 0.5715030926298532,
        "step": 7669
    },
    {
        "loss": 2.7174,
        "grad_norm": 2.664778470993042,
        "learning_rate": 3.8297581482738475e-05,
        "epoch": 0.5715776138311349,
        "step": 7670
    },
    {
        "loss": 2.6167,
        "grad_norm": 3.134964942932129,
        "learning_rate": 3.8242222907005134e-05,
        "epoch": 0.5716521350324167,
        "step": 7671
    },
    {
        "loss": 2.5857,
        "grad_norm": 2.5164029598236084,
        "learning_rate": 3.8186894909569304e-05,
        "epoch": 0.5717266562336984,
        "step": 7672
    },
    {
        "loss": 2.9312,
        "grad_norm": 1.5450023412704468,
        "learning_rate": 3.81315975178259e-05,
        "epoch": 0.5718011774349803,
        "step": 7673
    },
    {
        "loss": 2.3346,
        "grad_norm": 2.4166030883789062,
        "learning_rate": 3.807633075915433e-05,
        "epoch": 0.571875698636262,
        "step": 7674
    },
    {
        "loss": 2.7914,
        "grad_norm": 2.4722225666046143,
        "learning_rate": 3.802109466091906e-05,
        "epoch": 0.5719502198375438,
        "step": 7675
    },
    {
        "loss": 2.6034,
        "grad_norm": 2.559504508972168,
        "learning_rate": 3.796588925046923e-05,
        "epoch": 0.5720247410388255,
        "step": 7676
    },
    {
        "loss": 2.049,
        "grad_norm": 3.4005720615386963,
        "learning_rate": 3.7910714555138835e-05,
        "epoch": 0.5720992622401073,
        "step": 7677
    },
    {
        "loss": 3.1055,
        "grad_norm": 2.938913106918335,
        "learning_rate": 3.7855570602246736e-05,
        "epoch": 0.572173783441389,
        "step": 7678
    },
    {
        "loss": 1.8838,
        "grad_norm": 4.481274127960205,
        "learning_rate": 3.780045741909636e-05,
        "epoch": 0.5722483046426708,
        "step": 7679
    },
    {
        "loss": 2.7897,
        "grad_norm": 2.562976837158203,
        "learning_rate": 3.774537503297627e-05,
        "epoch": 0.5723228258439526,
        "step": 7680
    },
    {
        "loss": 2.2345,
        "grad_norm": 3.296553134918213,
        "learning_rate": 3.7690323471159485e-05,
        "epoch": 0.5723973470452344,
        "step": 7681
    },
    {
        "loss": 1.9841,
        "grad_norm": 3.257046699523926,
        "learning_rate": 3.7635302760903656e-05,
        "epoch": 0.5724718682465161,
        "step": 7682
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.5785704851150513,
        "learning_rate": 3.75803129294516e-05,
        "epoch": 0.5725463894477979,
        "step": 7683
    },
    {
        "loss": 2.3731,
        "grad_norm": 2.5507688522338867,
        "learning_rate": 3.752535400403045e-05,
        "epoch": 0.5726209106490797,
        "step": 7684
    },
    {
        "loss": 2.8193,
        "grad_norm": 3.418210029602051,
        "learning_rate": 3.7470426011852136e-05,
        "epoch": 0.5726954318503614,
        "step": 7685
    },
    {
        "loss": 2.0579,
        "grad_norm": 3.7159929275512695,
        "learning_rate": 3.741552898011344e-05,
        "epoch": 0.5727699530516432,
        "step": 7686
    },
    {
        "loss": 1.9937,
        "grad_norm": 2.901968240737915,
        "learning_rate": 3.73606629359955e-05,
        "epoch": 0.572844474252925,
        "step": 7687
    },
    {
        "loss": 1.6647,
        "grad_norm": 4.392822265625,
        "learning_rate": 3.730582790666446e-05,
        "epoch": 0.5729189954542068,
        "step": 7688
    },
    {
        "loss": 2.7663,
        "grad_norm": 3.4811060428619385,
        "learning_rate": 3.725102391927089e-05,
        "epoch": 0.5729935166554885,
        "step": 7689
    },
    {
        "loss": 2.5378,
        "grad_norm": 3.1604325771331787,
        "learning_rate": 3.719625100094988e-05,
        "epoch": 0.5730680378567703,
        "step": 7690
    },
    {
        "loss": 1.7672,
        "grad_norm": 5.666750431060791,
        "learning_rate": 3.714150917882153e-05,
        "epoch": 0.573142559058052,
        "step": 7691
    },
    {
        "loss": 2.0674,
        "grad_norm": 2.838352680206299,
        "learning_rate": 3.708679847999011e-05,
        "epoch": 0.5732170802593338,
        "step": 7692
    },
    {
        "loss": 2.1221,
        "grad_norm": 2.4794883728027344,
        "learning_rate": 3.703211893154488e-05,
        "epoch": 0.5732916014606155,
        "step": 7693
    },
    {
        "loss": 2.7502,
        "grad_norm": 2.8793909549713135,
        "learning_rate": 3.697747056055937e-05,
        "epoch": 0.5733661226618973,
        "step": 7694
    },
    {
        "loss": 1.9512,
        "grad_norm": 4.238346576690674,
        "learning_rate": 3.692285339409174e-05,
        "epoch": 0.5734406438631791,
        "step": 7695
    },
    {
        "loss": 2.5854,
        "grad_norm": 2.2230746746063232,
        "learning_rate": 3.6868267459184804e-05,
        "epoch": 0.5735151650644609,
        "step": 7696
    },
    {
        "loss": 1.8002,
        "grad_norm": 4.442351341247559,
        "learning_rate": 3.681371278286579e-05,
        "epoch": 0.5735896862657426,
        "step": 7697
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.3701159954071045,
        "learning_rate": 3.675918939214656e-05,
        "epoch": 0.5736642074670244,
        "step": 7698
    },
    {
        "loss": 2.0836,
        "grad_norm": 1.9865949153900146,
        "learning_rate": 3.670469731402342e-05,
        "epoch": 0.5737387286683061,
        "step": 7699
    },
    {
        "loss": 2.7688,
        "grad_norm": 4.3354716300964355,
        "learning_rate": 3.6650236575477046e-05,
        "epoch": 0.5738132498695879,
        "step": 7700
    },
    {
        "loss": 2.4895,
        "grad_norm": 2.117495536804199,
        "learning_rate": 3.659580720347295e-05,
        "epoch": 0.5738877710708696,
        "step": 7701
    },
    {
        "loss": 2.0745,
        "grad_norm": 2.6905901432037354,
        "learning_rate": 3.654140922496071e-05,
        "epoch": 0.5739622922721515,
        "step": 7702
    },
    {
        "loss": 2.3704,
        "grad_norm": 3.1293420791625977,
        "learning_rate": 3.648704266687474e-05,
        "epoch": 0.5740368134734332,
        "step": 7703
    },
    {
        "loss": 2.5621,
        "grad_norm": 4.060420989990234,
        "learning_rate": 3.643270755613362e-05,
        "epoch": 0.574111334674715,
        "step": 7704
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.8198678493499756,
        "learning_rate": 3.637840391964031e-05,
        "epoch": 0.5741858558759967,
        "step": 7705
    },
    {
        "loss": 2.7847,
        "grad_norm": 2.873506546020508,
        "learning_rate": 3.632413178428251e-05,
        "epoch": 0.5742603770772785,
        "step": 7706
    },
    {
        "loss": 1.933,
        "grad_norm": 3.499886989593506,
        "learning_rate": 3.6269891176932015e-05,
        "epoch": 0.5743348982785602,
        "step": 7707
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.469820499420166,
        "learning_rate": 3.621568212444518e-05,
        "epoch": 0.574409419479842,
        "step": 7708
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.2276971340179443,
        "learning_rate": 3.616150465366267e-05,
        "epoch": 0.5744839406811237,
        "step": 7709
    },
    {
        "loss": 2.5541,
        "grad_norm": 2.7799155712127686,
        "learning_rate": 3.6107358791409416e-05,
        "epoch": 0.5745584618824056,
        "step": 7710
    },
    {
        "loss": 2.0674,
        "grad_norm": 3.7855660915374756,
        "learning_rate": 3.6053244564495014e-05,
        "epoch": 0.5746329830836873,
        "step": 7711
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.8060624599456787,
        "learning_rate": 3.5999161999713115e-05,
        "epoch": 0.5747075042849691,
        "step": 7712
    },
    {
        "loss": 2.2681,
        "grad_norm": 2.840158462524414,
        "learning_rate": 3.59451111238416e-05,
        "epoch": 0.5747820254862508,
        "step": 7713
    },
    {
        "loss": 2.4957,
        "grad_norm": 2.6248133182525635,
        "learning_rate": 3.5891091963643066e-05,
        "epoch": 0.5748565466875326,
        "step": 7714
    },
    {
        "loss": 2.4689,
        "grad_norm": 4.2024359703063965,
        "learning_rate": 3.5837104545864e-05,
        "epoch": 0.5749310678888143,
        "step": 7715
    },
    {
        "loss": 2.7598,
        "grad_norm": 2.5209567546844482,
        "learning_rate": 3.578314889723551e-05,
        "epoch": 0.5750055890900961,
        "step": 7716
    },
    {
        "loss": 1.7412,
        "grad_norm": 2.408625364303589,
        "learning_rate": 3.572922504447266e-05,
        "epoch": 0.5750801102913778,
        "step": 7717
    },
    {
        "loss": 1.8411,
        "grad_norm": 1.9623068571090698,
        "learning_rate": 3.567533301427488e-05,
        "epoch": 0.5751546314926597,
        "step": 7718
    },
    {
        "loss": 2.454,
        "grad_norm": 2.9520304203033447,
        "learning_rate": 3.562147283332608e-05,
        "epoch": 0.5752291526939415,
        "step": 7719
    },
    {
        "loss": 2.9675,
        "grad_norm": 2.1336703300476074,
        "learning_rate": 3.556764452829392e-05,
        "epoch": 0.5753036738952232,
        "step": 7720
    },
    {
        "loss": 2.8234,
        "grad_norm": 1.8306424617767334,
        "learning_rate": 3.551384812583082e-05,
        "epoch": 0.575378195096505,
        "step": 7721
    },
    {
        "loss": 1.9879,
        "grad_norm": 3.6869773864746094,
        "learning_rate": 3.546008365257302e-05,
        "epoch": 0.5754527162977867,
        "step": 7722
    },
    {
        "loss": 2.0011,
        "grad_norm": 4.775603294372559,
        "learning_rate": 3.540635113514099e-05,
        "epoch": 0.5755272374990685,
        "step": 7723
    },
    {
        "loss": 1.9883,
        "grad_norm": 3.148597240447998,
        "learning_rate": 3.535265060013965e-05,
        "epoch": 0.5756017587003502,
        "step": 7724
    },
    {
        "loss": 2.1085,
        "grad_norm": 3.3965272903442383,
        "learning_rate": 3.529898207415775e-05,
        "epoch": 0.5756762799016321,
        "step": 7725
    },
    {
        "loss": 2.3507,
        "grad_norm": 2.833573818206787,
        "learning_rate": 3.524534558376842e-05,
        "epoch": 0.5757508011029138,
        "step": 7726
    },
    {
        "loss": 2.6506,
        "grad_norm": 4.725235462188721,
        "learning_rate": 3.5191741155528804e-05,
        "epoch": 0.5758253223041956,
        "step": 7727
    },
    {
        "loss": 1.9915,
        "grad_norm": 3.8665149211883545,
        "learning_rate": 3.5138168815980165e-05,
        "epoch": 0.5758998435054773,
        "step": 7728
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.1683003902435303,
        "learning_rate": 3.5084628591648003e-05,
        "epoch": 0.5759743647067591,
        "step": 7729
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.877004861831665,
        "learning_rate": 3.50311205090417e-05,
        "epoch": 0.5760488859080408,
        "step": 7730
    },
    {
        "loss": 1.8891,
        "grad_norm": 3.137235164642334,
        "learning_rate": 3.497764459465506e-05,
        "epoch": 0.5761234071093226,
        "step": 7731
    },
    {
        "loss": 2.2344,
        "grad_norm": 2.295872688293457,
        "learning_rate": 3.492420087496564e-05,
        "epoch": 0.5761979283106043,
        "step": 7732
    },
    {
        "loss": 2.4973,
        "grad_norm": 2.0092203617095947,
        "learning_rate": 3.48707893764351e-05,
        "epoch": 0.5762724495118862,
        "step": 7733
    },
    {
        "loss": 2.9163,
        "grad_norm": 2.238382339477539,
        "learning_rate": 3.481741012550941e-05,
        "epoch": 0.5763469707131679,
        "step": 7734
    },
    {
        "loss": 2.6366,
        "grad_norm": 2.7493293285369873,
        "learning_rate": 3.476406314861827e-05,
        "epoch": 0.5764214919144497,
        "step": 7735
    },
    {
        "loss": 2.4055,
        "grad_norm": 3.2645509243011475,
        "learning_rate": 3.4710748472175546e-05,
        "epoch": 0.5764960131157314,
        "step": 7736
    },
    {
        "loss": 2.6397,
        "grad_norm": 2.334970474243164,
        "learning_rate": 3.4657466122579076e-05,
        "epoch": 0.5765705343170132,
        "step": 7737
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.496975898742676,
        "learning_rate": 3.4604216126210654e-05,
        "epoch": 0.5766450555182949,
        "step": 7738
    },
    {
        "loss": 2.9806,
        "grad_norm": 1.7364704608917236,
        "learning_rate": 3.455099850943617e-05,
        "epoch": 0.5767195767195767,
        "step": 7739
    },
    {
        "loss": 2.2393,
        "grad_norm": 3.88211727142334,
        "learning_rate": 3.4497813298605266e-05,
        "epoch": 0.5767940979208585,
        "step": 7740
    },
    {
        "loss": 2.6291,
        "grad_norm": 2.3765597343444824,
        "learning_rate": 3.444466052005187e-05,
        "epoch": 0.5768686191221403,
        "step": 7741
    },
    {
        "loss": 2.7372,
        "grad_norm": 2.1470413208007812,
        "learning_rate": 3.439154020009363e-05,
        "epoch": 0.576943140323422,
        "step": 7742
    },
    {
        "loss": 2.7551,
        "grad_norm": 1.9902853965759277,
        "learning_rate": 3.433845236503196e-05,
        "epoch": 0.5770176615247038,
        "step": 7743
    },
    {
        "loss": 2.8167,
        "grad_norm": 1.9633123874664307,
        "learning_rate": 3.428539704115262e-05,
        "epoch": 0.5770921827259855,
        "step": 7744
    },
    {
        "loss": 2.6715,
        "grad_norm": 2.5641472339630127,
        "learning_rate": 3.423237425472496e-05,
        "epoch": 0.5771667039272673,
        "step": 7745
    },
    {
        "loss": 2.4575,
        "grad_norm": 3.351503849029541,
        "learning_rate": 3.417938403200225e-05,
        "epoch": 0.577241225128549,
        "step": 7746
    },
    {
        "loss": 2.8332,
        "grad_norm": 2.86777400970459,
        "learning_rate": 3.412642639922174e-05,
        "epoch": 0.5773157463298308,
        "step": 7747
    },
    {
        "loss": 2.7336,
        "grad_norm": 1.9591492414474487,
        "learning_rate": 3.4073501382604424e-05,
        "epoch": 0.5773902675311126,
        "step": 7748
    },
    {
        "loss": 2.1373,
        "grad_norm": 3.3195862770080566,
        "learning_rate": 3.402060900835533e-05,
        "epoch": 0.5774647887323944,
        "step": 7749
    },
    {
        "loss": 2.0506,
        "grad_norm": 2.513469696044922,
        "learning_rate": 3.396774930266311e-05,
        "epoch": 0.5775393099336761,
        "step": 7750
    },
    {
        "loss": 2.3955,
        "grad_norm": 2.2030887603759766,
        "learning_rate": 3.391492229170025e-05,
        "epoch": 0.5776138311349579,
        "step": 7751
    },
    {
        "loss": 1.9526,
        "grad_norm": 2.613072395324707,
        "learning_rate": 3.3862128001623314e-05,
        "epoch": 0.5776883523362396,
        "step": 7752
    },
    {
        "loss": 2.8878,
        "grad_norm": 2.1124677658081055,
        "learning_rate": 3.380936645857233e-05,
        "epoch": 0.5777628735375214,
        "step": 7753
    },
    {
        "loss": 2.1029,
        "grad_norm": 2.1189231872558594,
        "learning_rate": 3.375663768867143e-05,
        "epoch": 0.5778373947388032,
        "step": 7754
    },
    {
        "loss": 2.7345,
        "grad_norm": 3.655721664428711,
        "learning_rate": 3.370394171802828e-05,
        "epoch": 0.577911915940085,
        "step": 7755
    },
    {
        "loss": 2.0094,
        "grad_norm": 3.593810558319092,
        "learning_rate": 3.365127857273431e-05,
        "epoch": 0.5779864371413668,
        "step": 7756
    },
    {
        "loss": 1.162,
        "grad_norm": 2.8431923389434814,
        "learning_rate": 3.3598648278864874e-05,
        "epoch": 0.5780609583426485,
        "step": 7757
    },
    {
        "loss": 2.6742,
        "grad_norm": 2.469644069671631,
        "learning_rate": 3.354605086247886e-05,
        "epoch": 0.5781354795439303,
        "step": 7758
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.329498767852783,
        "learning_rate": 3.349348634961905e-05,
        "epoch": 0.578210000745212,
        "step": 7759
    },
    {
        "loss": 2.691,
        "grad_norm": 2.7433927059173584,
        "learning_rate": 3.344095476631183e-05,
        "epoch": 0.5782845219464938,
        "step": 7760
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.9525678157806396,
        "learning_rate": 3.338845613856719e-05,
        "epoch": 0.5783590431477755,
        "step": 7761
    },
    {
        "loss": 2.2562,
        "grad_norm": 3.1112377643585205,
        "learning_rate": 3.333599049237912e-05,
        "epoch": 0.5784335643490573,
        "step": 7762
    },
    {
        "loss": 2.8765,
        "grad_norm": 4.266323566436768,
        "learning_rate": 3.32835578537249e-05,
        "epoch": 0.5785080855503391,
        "step": 7763
    },
    {
        "loss": 2.3773,
        "grad_norm": 3.626289129257202,
        "learning_rate": 3.3231158248565816e-05,
        "epoch": 0.5785826067516209,
        "step": 7764
    },
    {
        "loss": 2.4468,
        "grad_norm": 2.0029103755950928,
        "learning_rate": 3.317879170284659e-05,
        "epoch": 0.5786571279529026,
        "step": 7765
    },
    {
        "loss": 2.7526,
        "grad_norm": 2.701350450515747,
        "learning_rate": 3.3126458242495464e-05,
        "epoch": 0.5787316491541844,
        "step": 7766
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.8210242986679077,
        "learning_rate": 3.3074157893424615e-05,
        "epoch": 0.5788061703554661,
        "step": 7767
    },
    {
        "loss": 2.0366,
        "grad_norm": 4.05968713760376,
        "learning_rate": 3.302189068152956e-05,
        "epoch": 0.5788806915567479,
        "step": 7768
    },
    {
        "loss": 2.5184,
        "grad_norm": 3.9314653873443604,
        "learning_rate": 3.296965663268958e-05,
        "epoch": 0.5789552127580296,
        "step": 7769
    },
    {
        "loss": 2.0542,
        "grad_norm": 2.7061426639556885,
        "learning_rate": 3.291745577276744e-05,
        "epoch": 0.5790297339593115,
        "step": 7770
    },
    {
        "loss": 2.0267,
        "grad_norm": 1.4905544519424438,
        "learning_rate": 3.286528812760936e-05,
        "epoch": 0.5791042551605932,
        "step": 7771
    },
    {
        "loss": 2.9709,
        "grad_norm": 2.776024580001831,
        "learning_rate": 3.281315372304551e-05,
        "epoch": 0.579178776361875,
        "step": 7772
    },
    {
        "loss": 1.5471,
        "grad_norm": 3.4216368198394775,
        "learning_rate": 3.276105258488906e-05,
        "epoch": 0.5792532975631567,
        "step": 7773
    },
    {
        "loss": 2.7988,
        "grad_norm": 2.7886605262756348,
        "learning_rate": 3.270898473893723e-05,
        "epoch": 0.5793278187644385,
        "step": 7774
    },
    {
        "loss": 2.5974,
        "grad_norm": 3.035243034362793,
        "learning_rate": 3.265695021097043e-05,
        "epoch": 0.5794023399657202,
        "step": 7775
    },
    {
        "loss": 2.3127,
        "grad_norm": 3.880462884902954,
        "learning_rate": 3.26049490267526e-05,
        "epoch": 0.579476861167002,
        "step": 7776
    },
    {
        "loss": 2.1326,
        "grad_norm": 4.498372554779053,
        "learning_rate": 3.25529812120313e-05,
        "epoch": 0.5795513823682837,
        "step": 7777
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.6858950853347778,
        "learning_rate": 3.250104679253747e-05,
        "epoch": 0.5796259035695656,
        "step": 7778
    },
    {
        "loss": 2.6706,
        "grad_norm": 1.971178650856018,
        "learning_rate": 3.244914579398548e-05,
        "epoch": 0.5797004247708473,
        "step": 7779
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.994633197784424,
        "learning_rate": 3.239727824207337e-05,
        "epoch": 0.5797749459721291,
        "step": 7780
    },
    {
        "loss": 1.8786,
        "grad_norm": 2.717134475708008,
        "learning_rate": 3.234544416248224e-05,
        "epoch": 0.5798494671734108,
        "step": 7781
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.432631731033325,
        "learning_rate": 3.2293643580877065e-05,
        "epoch": 0.5799239883746926,
        "step": 7782
    },
    {
        "loss": 1.8648,
        "grad_norm": 2.6185500621795654,
        "learning_rate": 3.224187652290591e-05,
        "epoch": 0.5799985095759743,
        "step": 7783
    },
    {
        "loss": 1.9349,
        "grad_norm": 3.309300661087036,
        "learning_rate": 3.2190143014200256e-05,
        "epoch": 0.5800730307772561,
        "step": 7784
    },
    {
        "loss": 2.401,
        "grad_norm": 2.741948127746582,
        "learning_rate": 3.213844308037522e-05,
        "epoch": 0.5801475519785378,
        "step": 7785
    },
    {
        "loss": 2.446,
        "grad_norm": 3.437441349029541,
        "learning_rate": 3.2086776747029046e-05,
        "epoch": 0.5802220731798197,
        "step": 7786
    },
    {
        "loss": 2.4869,
        "grad_norm": 2.6018471717834473,
        "learning_rate": 3.203514403974346e-05,
        "epoch": 0.5802965943811014,
        "step": 7787
    },
    {
        "loss": 2.251,
        "grad_norm": 3.0134146213531494,
        "learning_rate": 3.198354498408348e-05,
        "epoch": 0.5803711155823832,
        "step": 7788
    },
    {
        "loss": 2.4996,
        "grad_norm": 3.310751438140869,
        "learning_rate": 3.193197960559746e-05,
        "epoch": 0.5804456367836649,
        "step": 7789
    },
    {
        "loss": 2.527,
        "grad_norm": 3.088573455810547,
        "learning_rate": 3.188044792981718e-05,
        "epoch": 0.5805201579849467,
        "step": 7790
    },
    {
        "loss": 1.7104,
        "grad_norm": 6.189716815948486,
        "learning_rate": 3.1828949982257505e-05,
        "epoch": 0.5805946791862285,
        "step": 7791
    },
    {
        "loss": 2.4502,
        "grad_norm": 3.912736654281616,
        "learning_rate": 3.1777485788416984e-05,
        "epoch": 0.5806692003875102,
        "step": 7792
    },
    {
        "loss": 2.0678,
        "grad_norm": 3.031649112701416,
        "learning_rate": 3.17260553737771e-05,
        "epoch": 0.5807437215887921,
        "step": 7793
    },
    {
        "loss": 2.2435,
        "grad_norm": 3.3305439949035645,
        "learning_rate": 3.1674658763802645e-05,
        "epoch": 0.5808182427900738,
        "step": 7794
    },
    {
        "loss": 2.5195,
        "grad_norm": 1.7096989154815674,
        "learning_rate": 3.162329598394199e-05,
        "epoch": 0.5808927639913556,
        "step": 7795
    },
    {
        "loss": 2.5539,
        "grad_norm": 2.0481326580047607,
        "learning_rate": 3.157196705962632e-05,
        "epoch": 0.5809672851926373,
        "step": 7796
    },
    {
        "loss": 2.5229,
        "grad_norm": 3.9996497631073,
        "learning_rate": 3.152067201627039e-05,
        "epoch": 0.5810418063939191,
        "step": 7797
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.9725738763809204,
        "learning_rate": 3.146941087927203e-05,
        "epoch": 0.5811163275952008,
        "step": 7798
    },
    {
        "loss": 3.0004,
        "grad_norm": 3.226708173751831,
        "learning_rate": 3.1418183674012255e-05,
        "epoch": 0.5811908487964826,
        "step": 7799
    },
    {
        "loss": 2.0489,
        "grad_norm": 3.00776743888855,
        "learning_rate": 3.13669904258554e-05,
        "epoch": 0.5812653699977643,
        "step": 7800
    },
    {
        "loss": 2.4001,
        "grad_norm": 3.2384777069091797,
        "learning_rate": 3.131583116014879e-05,
        "epoch": 0.5813398911990462,
        "step": 7801
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.4890034198760986,
        "learning_rate": 3.126470590222325e-05,
        "epoch": 0.5814144124003279,
        "step": 7802
    },
    {
        "loss": 1.5619,
        "grad_norm": 3.4030659198760986,
        "learning_rate": 3.121361467739249e-05,
        "epoch": 0.5814889336016097,
        "step": 7803
    },
    {
        "loss": 2.5232,
        "grad_norm": 2.838343858718872,
        "learning_rate": 3.116255751095331e-05,
        "epoch": 0.5815634548028914,
        "step": 7804
    },
    {
        "loss": 1.9396,
        "grad_norm": 5.033957004547119,
        "learning_rate": 3.1111534428185985e-05,
        "epoch": 0.5816379760041732,
        "step": 7805
    },
    {
        "loss": 2.877,
        "grad_norm": 2.55607271194458,
        "learning_rate": 3.106054545435364e-05,
        "epoch": 0.5817124972054549,
        "step": 7806
    },
    {
        "loss": 1.5525,
        "grad_norm": 6.494225978851318,
        "learning_rate": 3.100959061470253e-05,
        "epoch": 0.5817870184067367,
        "step": 7807
    },
    {
        "loss": 2.7121,
        "grad_norm": 3.5561742782592773,
        "learning_rate": 3.095866993446216e-05,
        "epoch": 0.5818615396080185,
        "step": 7808
    },
    {
        "loss": 2.5941,
        "grad_norm": 2.85056734085083,
        "learning_rate": 3.0907783438844906e-05,
        "epoch": 0.5819360608093003,
        "step": 7809
    },
    {
        "loss": 3.0377,
        "grad_norm": 2.8515734672546387,
        "learning_rate": 3.08569311530465e-05,
        "epoch": 0.582010582010582,
        "step": 7810
    },
    {
        "loss": 2.007,
        "grad_norm": 3.0927772521972656,
        "learning_rate": 3.080611310224546e-05,
        "epoch": 0.5820851032118638,
        "step": 7811
    },
    {
        "loss": 1.6616,
        "grad_norm": 3.0713114738464355,
        "learning_rate": 3.075532931160339e-05,
        "epoch": 0.5821596244131455,
        "step": 7812
    },
    {
        "loss": 2.1893,
        "grad_norm": 5.390030860900879,
        "learning_rate": 3.070457980626519e-05,
        "epoch": 0.5822341456144273,
        "step": 7813
    },
    {
        "loss": 2.2553,
        "grad_norm": 3.402714967727661,
        "learning_rate": 3.065386461135844e-05,
        "epoch": 0.582308666815709,
        "step": 7814
    },
    {
        "loss": 1.8915,
        "grad_norm": 3.92209792137146,
        "learning_rate": 3.060318375199406e-05,
        "epoch": 0.5823831880169908,
        "step": 7815
    },
    {
        "loss": 2.509,
        "grad_norm": 3.1624693870544434,
        "learning_rate": 3.0552537253265715e-05,
        "epoch": 0.5824577092182726,
        "step": 7816
    },
    {
        "loss": 2.1105,
        "grad_norm": 2.6663050651550293,
        "learning_rate": 3.050192514025012e-05,
        "epoch": 0.5825322304195544,
        "step": 7817
    },
    {
        "loss": 2.3775,
        "grad_norm": 3.224074125289917,
        "learning_rate": 3.045134743800704e-05,
        "epoch": 0.5826067516208361,
        "step": 7818
    },
    {
        "loss": 2.1838,
        "grad_norm": 2.6615355014801025,
        "learning_rate": 3.040080417157909e-05,
        "epoch": 0.5826812728221179,
        "step": 7819
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.814687728881836,
        "learning_rate": 3.0350295365991975e-05,
        "epoch": 0.5827557940233996,
        "step": 7820
    },
    {
        "loss": 2.478,
        "grad_norm": 2.8607053756713867,
        "learning_rate": 3.0299821046254208e-05,
        "epoch": 0.5828303152246814,
        "step": 7821
    },
    {
        "loss": 2.4195,
        "grad_norm": 2.4216575622558594,
        "learning_rate": 3.02493812373572e-05,
        "epoch": 0.5829048364259631,
        "step": 7822
    },
    {
        "loss": 3.0701,
        "grad_norm": 3.168226480484009,
        "learning_rate": 3.0198975964275523e-05,
        "epoch": 0.582979357627245,
        "step": 7823
    },
    {
        "loss": 2.4901,
        "grad_norm": 3.1385369300842285,
        "learning_rate": 3.01486052519663e-05,
        "epoch": 0.5830538788285267,
        "step": 7824
    },
    {
        "loss": 2.2728,
        "grad_norm": 3.984710454940796,
        "learning_rate": 3.0098269125369904e-05,
        "epoch": 0.5831284000298085,
        "step": 7825
    },
    {
        "loss": 2.9578,
        "grad_norm": 1.6420960426330566,
        "learning_rate": 3.0047967609409344e-05,
        "epoch": 0.5832029212310903,
        "step": 7826
    },
    {
        "loss": 2.3743,
        "grad_norm": 3.5479736328125,
        "learning_rate": 2.9997700728990418e-05,
        "epoch": 0.583277442432372,
        "step": 7827
    },
    {
        "loss": 2.0917,
        "grad_norm": 2.830148935317993,
        "learning_rate": 2.9947468509002054e-05,
        "epoch": 0.5833519636336538,
        "step": 7828
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.8459746837615967,
        "learning_rate": 2.9897270974315782e-05,
        "epoch": 0.5834264848349355,
        "step": 7829
    },
    {
        "loss": 2.0797,
        "grad_norm": 3.178609609603882,
        "learning_rate": 2.9847108149786118e-05,
        "epoch": 0.5835010060362174,
        "step": 7830
    },
    {
        "loss": 1.5331,
        "grad_norm": 2.9909861087799072,
        "learning_rate": 2.9796980060250268e-05,
        "epoch": 0.5835755272374991,
        "step": 7831
    },
    {
        "loss": 2.5985,
        "grad_norm": 2.4977049827575684,
        "learning_rate": 2.97468867305282e-05,
        "epoch": 0.5836500484387809,
        "step": 7832
    },
    {
        "loss": 1.7186,
        "grad_norm": 3.236051321029663,
        "learning_rate": 2.9696828185422996e-05,
        "epoch": 0.5837245696400626,
        "step": 7833
    },
    {
        "loss": 2.3888,
        "grad_norm": 3.6393699645996094,
        "learning_rate": 2.9646804449720024e-05,
        "epoch": 0.5837990908413444,
        "step": 7834
    },
    {
        "loss": 2.7542,
        "grad_norm": 2.4531381130218506,
        "learning_rate": 2.9596815548187874e-05,
        "epoch": 0.5838736120426261,
        "step": 7835
    },
    {
        "loss": 2.7831,
        "grad_norm": 2.6959807872772217,
        "learning_rate": 2.954686150557763e-05,
        "epoch": 0.5839481332439079,
        "step": 7836
    },
    {
        "loss": 2.243,
        "grad_norm": 2.8985188007354736,
        "learning_rate": 2.94969423466231e-05,
        "epoch": 0.5840226544451896,
        "step": 7837
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.7917094230651855,
        "learning_rate": 2.944705809604099e-05,
        "epoch": 0.5840971756464715,
        "step": 7838
    },
    {
        "loss": 2.5004,
        "grad_norm": 3.5284698009490967,
        "learning_rate": 2.9397208778530583e-05,
        "epoch": 0.5841716968477532,
        "step": 7839
    },
    {
        "loss": 2.3123,
        "grad_norm": 2.8289942741394043,
        "learning_rate": 2.934739441877388e-05,
        "epoch": 0.584246218049035,
        "step": 7840
    },
    {
        "loss": 2.4942,
        "grad_norm": 3.1760549545288086,
        "learning_rate": 2.9297615041435656e-05,
        "epoch": 0.5843207392503167,
        "step": 7841
    },
    {
        "loss": 2.2114,
        "grad_norm": 2.0812888145446777,
        "learning_rate": 2.92478706711632e-05,
        "epoch": 0.5843952604515985,
        "step": 7842
    },
    {
        "loss": 2.5823,
        "grad_norm": 3.669970750808716,
        "learning_rate": 2.919816133258675e-05,
        "epoch": 0.5844697816528802,
        "step": 7843
    },
    {
        "loss": 2.6179,
        "grad_norm": 2.0770959854125977,
        "learning_rate": 2.914848705031894e-05,
        "epoch": 0.584544302854162,
        "step": 7844
    },
    {
        "loss": 2.5679,
        "grad_norm": 2.0915544033050537,
        "learning_rate": 2.9098847848955046e-05,
        "epoch": 0.5846188240554437,
        "step": 7845
    },
    {
        "loss": 1.7943,
        "grad_norm": 1.7407323122024536,
        "learning_rate": 2.904924375307324e-05,
        "epoch": 0.5846933452567256,
        "step": 7846
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.103637456893921,
        "learning_rate": 2.8999674787234022e-05,
        "epoch": 0.5847678664580073,
        "step": 7847
    },
    {
        "loss": 2.2919,
        "grad_norm": 3.324388027191162,
        "learning_rate": 2.8950140975980654e-05,
        "epoch": 0.5848423876592891,
        "step": 7848
    },
    {
        "loss": 2.3751,
        "grad_norm": 4.414203643798828,
        "learning_rate": 2.8900642343838934e-05,
        "epoch": 0.5849169088605708,
        "step": 7849
    },
    {
        "loss": 2.5143,
        "grad_norm": 2.1639299392700195,
        "learning_rate": 2.8851178915317212e-05,
        "epoch": 0.5849914300618526,
        "step": 7850
    },
    {
        "loss": 2.5198,
        "grad_norm": 3.1804282665252686,
        "learning_rate": 2.8801750714906517e-05,
        "epoch": 0.5850659512631343,
        "step": 7851
    },
    {
        "loss": 1.9645,
        "grad_norm": 3.6471660137176514,
        "learning_rate": 2.8752357767080252e-05,
        "epoch": 0.5851404724644161,
        "step": 7852
    },
    {
        "loss": 3.0346,
        "grad_norm": 3.6010913848876953,
        "learning_rate": 2.8703000096294652e-05,
        "epoch": 0.5852149936656978,
        "step": 7853
    },
    {
        "loss": 2.3342,
        "grad_norm": 2.2976953983306885,
        "learning_rate": 2.865367772698825e-05,
        "epoch": 0.5852895148669797,
        "step": 7854
    },
    {
        "loss": 2.4157,
        "grad_norm": 3.3811588287353516,
        "learning_rate": 2.860439068358205e-05,
        "epoch": 0.5853640360682614,
        "step": 7855
    },
    {
        "loss": 2.7437,
        "grad_norm": 2.079953908920288,
        "learning_rate": 2.8555138990479902e-05,
        "epoch": 0.5854385572695432,
        "step": 7856
    },
    {
        "loss": 2.2977,
        "grad_norm": 4.6100687980651855,
        "learning_rate": 2.850592267206772e-05,
        "epoch": 0.5855130784708249,
        "step": 7857
    },
    {
        "loss": 2.0594,
        "grad_norm": 3.36407732963562,
        "learning_rate": 2.8456741752714277e-05,
        "epoch": 0.5855875996721067,
        "step": 7858
    },
    {
        "loss": 2.578,
        "grad_norm": 1.9179973602294922,
        "learning_rate": 2.840759625677061e-05,
        "epoch": 0.5856621208733884,
        "step": 7859
    },
    {
        "loss": 2.6195,
        "grad_norm": 3.0443122386932373,
        "learning_rate": 2.8358486208570145e-05,
        "epoch": 0.5857366420746702,
        "step": 7860
    },
    {
        "loss": 1.9671,
        "grad_norm": 1.9880515336990356,
        "learning_rate": 2.830941163242903e-05,
        "epoch": 0.5858111632759521,
        "step": 7861
    },
    {
        "loss": 1.5329,
        "grad_norm": 3.894747257232666,
        "learning_rate": 2.8260372552645543e-05,
        "epoch": 0.5858856844772338,
        "step": 7862
    },
    {
        "loss": 2.6896,
        "grad_norm": 3.2826263904571533,
        "learning_rate": 2.8211368993500754e-05,
        "epoch": 0.5859602056785156,
        "step": 7863
    },
    {
        "loss": 2.1339,
        "grad_norm": 2.8775875568389893,
        "learning_rate": 2.8162400979257775e-05,
        "epoch": 0.5860347268797973,
        "step": 7864
    },
    {
        "loss": 3.0231,
        "grad_norm": 1.8854323625564575,
        "learning_rate": 2.8113468534162202e-05,
        "epoch": 0.5861092480810791,
        "step": 7865
    },
    {
        "loss": 1.9709,
        "grad_norm": 2.1640942096710205,
        "learning_rate": 2.8064571682442285e-05,
        "epoch": 0.5861837692823608,
        "step": 7866
    },
    {
        "loss": 2.0517,
        "grad_norm": 4.365406036376953,
        "learning_rate": 2.801571044830834e-05,
        "epoch": 0.5862582904836426,
        "step": 7867
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.040147304534912,
        "learning_rate": 2.796688485595321e-05,
        "epoch": 0.5863328116849243,
        "step": 7868
    },
    {
        "loss": 2.173,
        "grad_norm": 3.2166664600372314,
        "learning_rate": 2.7918094929552018e-05,
        "epoch": 0.5864073328862062,
        "step": 7869
    },
    {
        "loss": 2.6074,
        "grad_norm": 2.7741734981536865,
        "learning_rate": 2.7869340693262204e-05,
        "epoch": 0.5864818540874879,
        "step": 7870
    },
    {
        "loss": 1.9158,
        "grad_norm": 2.8346803188323975,
        "learning_rate": 2.7820622171223732e-05,
        "epoch": 0.5865563752887697,
        "step": 7871
    },
    {
        "loss": 2.9283,
        "grad_norm": 3.04152512550354,
        "learning_rate": 2.777193938755861e-05,
        "epoch": 0.5866308964900514,
        "step": 7872
    },
    {
        "loss": 2.0965,
        "grad_norm": 3.1609630584716797,
        "learning_rate": 2.7723292366371224e-05,
        "epoch": 0.5867054176913332,
        "step": 7873
    },
    {
        "loss": 2.6055,
        "grad_norm": 1.9172950983047485,
        "learning_rate": 2.7674681131748493e-05,
        "epoch": 0.5867799388926149,
        "step": 7874
    },
    {
        "loss": 3.089,
        "grad_norm": 2.6528208255767822,
        "learning_rate": 2.762610570775923e-05,
        "epoch": 0.5868544600938967,
        "step": 7875
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.3035531044006348,
        "learning_rate": 2.7577566118454934e-05,
        "epoch": 0.5869289812951785,
        "step": 7876
    },
    {
        "loss": 2.4271,
        "grad_norm": 2.7988431453704834,
        "learning_rate": 2.752906238786902e-05,
        "epoch": 0.5870035024964603,
        "step": 7877
    },
    {
        "loss": 2.8336,
        "grad_norm": 2.5004804134368896,
        "learning_rate": 2.7480594540017256e-05,
        "epoch": 0.587078023697742,
        "step": 7878
    },
    {
        "loss": 2.6113,
        "grad_norm": 3.6425940990448,
        "learning_rate": 2.7432162598897726e-05,
        "epoch": 0.5871525448990238,
        "step": 7879
    },
    {
        "loss": 1.5208,
        "grad_norm": 2.4535646438598633,
        "learning_rate": 2.73837665884906e-05,
        "epoch": 0.5872270661003055,
        "step": 7880
    },
    {
        "loss": 2.4889,
        "grad_norm": 3.408146619796753,
        "learning_rate": 2.73354065327584e-05,
        "epoch": 0.5873015873015873,
        "step": 7881
    },
    {
        "loss": 2.443,
        "grad_norm": 3.0369021892547607,
        "learning_rate": 2.728708245564574e-05,
        "epoch": 0.587376108502869,
        "step": 7882
    },
    {
        "loss": 2.0255,
        "grad_norm": 3.3528687953948975,
        "learning_rate": 2.7238794381079348e-05,
        "epoch": 0.5874506297041509,
        "step": 7883
    },
    {
        "loss": 2.5464,
        "grad_norm": 3.1372008323669434,
        "learning_rate": 2.7190542332968426e-05,
        "epoch": 0.5875251509054326,
        "step": 7884
    },
    {
        "loss": 2.5563,
        "grad_norm": 2.0478556156158447,
        "learning_rate": 2.7142326335203948e-05,
        "epoch": 0.5875996721067144,
        "step": 7885
    },
    {
        "loss": 2.8348,
        "grad_norm": 2.0578432083129883,
        "learning_rate": 2.709414641165947e-05,
        "epoch": 0.5876741933079961,
        "step": 7886
    },
    {
        "loss": 2.476,
        "grad_norm": 2.7971203327178955,
        "learning_rate": 2.7046002586190276e-05,
        "epoch": 0.5877487145092779,
        "step": 7887
    },
    {
        "loss": 1.2866,
        "grad_norm": 2.7478785514831543,
        "learning_rate": 2.6997894882633933e-05,
        "epoch": 0.5878232357105596,
        "step": 7888
    },
    {
        "loss": 2.173,
        "grad_norm": 1.4381719827651978,
        "learning_rate": 2.6949823324810274e-05,
        "epoch": 0.5878977569118414,
        "step": 7889
    },
    {
        "loss": 2.458,
        "grad_norm": 1.8181687593460083,
        "learning_rate": 2.690178793652095e-05,
        "epoch": 0.5879722781131231,
        "step": 7890
    },
    {
        "loss": 2.1878,
        "grad_norm": 4.466630458831787,
        "learning_rate": 2.6853788741549967e-05,
        "epoch": 0.588046799314405,
        "step": 7891
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.396509885787964,
        "learning_rate": 2.6805825763663274e-05,
        "epoch": 0.5881213205156867,
        "step": 7892
    },
    {
        "loss": 2.8863,
        "grad_norm": 2.6737592220306396,
        "learning_rate": 2.6757899026608802e-05,
        "epoch": 0.5881958417169685,
        "step": 7893
    },
    {
        "loss": 2.5413,
        "grad_norm": 3.803130626678467,
        "learning_rate": 2.671000855411686e-05,
        "epoch": 0.5882703629182502,
        "step": 7894
    },
    {
        "loss": 2.1082,
        "grad_norm": 2.939830780029297,
        "learning_rate": 2.666215436989935e-05,
        "epoch": 0.588344884119532,
        "step": 7895
    },
    {
        "loss": 2.3698,
        "grad_norm": 3.1964187622070312,
        "learning_rate": 2.661433649765066e-05,
        "epoch": 0.5884194053208138,
        "step": 7896
    },
    {
        "loss": 2.0431,
        "grad_norm": 5.195839881896973,
        "learning_rate": 2.6566554961046887e-05,
        "epoch": 0.5884939265220955,
        "step": 7897
    },
    {
        "loss": 1.651,
        "grad_norm": 3.007018566131592,
        "learning_rate": 2.6518809783746212e-05,
        "epoch": 0.5885684477233774,
        "step": 7898
    },
    {
        "loss": 2.1329,
        "grad_norm": 3.4667444229125977,
        "learning_rate": 2.6471100989388887e-05,
        "epoch": 0.5886429689246591,
        "step": 7899
    },
    {
        "loss": 2.4336,
        "grad_norm": 3.849717617034912,
        "learning_rate": 2.642342860159708e-05,
        "epoch": 0.5887174901259409,
        "step": 7900
    },
    {
        "loss": 2.529,
        "grad_norm": 2.524190664291382,
        "learning_rate": 2.6375792643974905e-05,
        "epoch": 0.5887920113272226,
        "step": 7901
    },
    {
        "loss": 2.2924,
        "grad_norm": 3.225550889968872,
        "learning_rate": 2.632819314010856e-05,
        "epoch": 0.5888665325285044,
        "step": 7902
    },
    {
        "loss": 1.8803,
        "grad_norm": 3.155402421951294,
        "learning_rate": 2.6280630113565986e-05,
        "epoch": 0.5889410537297861,
        "step": 7903
    },
    {
        "loss": 2.665,
        "grad_norm": 3.2590012550354004,
        "learning_rate": 2.623310358789738e-05,
        "epoch": 0.5890155749310679,
        "step": 7904
    },
    {
        "loss": 2.2029,
        "grad_norm": 2.115964651107788,
        "learning_rate": 2.6185613586634593e-05,
        "epoch": 0.5890900961323496,
        "step": 7905
    },
    {
        "loss": 2.7589,
        "grad_norm": 2.0525360107421875,
        "learning_rate": 2.6138160133291402e-05,
        "epoch": 0.5891646173336315,
        "step": 7906
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.605431079864502,
        "learning_rate": 2.6090743251363714e-05,
        "epoch": 0.5892391385349132,
        "step": 7907
    },
    {
        "loss": 2.1534,
        "grad_norm": 3.163069009780884,
        "learning_rate": 2.604336296432909e-05,
        "epoch": 0.589313659736195,
        "step": 7908
    },
    {
        "loss": 2.8642,
        "grad_norm": 2.8137078285217285,
        "learning_rate": 2.599601929564709e-05,
        "epoch": 0.5893881809374767,
        "step": 7909
    },
    {
        "loss": 2.5442,
        "grad_norm": 2.460228443145752,
        "learning_rate": 2.5948712268759123e-05,
        "epoch": 0.5894627021387585,
        "step": 7910
    },
    {
        "loss": 2.3965,
        "grad_norm": 2.9507222175598145,
        "learning_rate": 2.5901441907088387e-05,
        "epoch": 0.5895372233400402,
        "step": 7911
    },
    {
        "loss": 2.3807,
        "grad_norm": 1.8878061771392822,
        "learning_rate": 2.5854208234040058e-05,
        "epoch": 0.589611744541322,
        "step": 7912
    },
    {
        "loss": 2.3474,
        "grad_norm": 2.6174705028533936,
        "learning_rate": 2.5807011273000957e-05,
        "epoch": 0.5896862657426037,
        "step": 7913
    },
    {
        "loss": 2.3154,
        "grad_norm": 1.9218248128890991,
        "learning_rate": 2.575985104734001e-05,
        "epoch": 0.5897607869438856,
        "step": 7914
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.0153074264526367,
        "learning_rate": 2.5712727580407724e-05,
        "epoch": 0.5898353081451673,
        "step": 7915
    },
    {
        "loss": 1.6966,
        "grad_norm": 2.911597490310669,
        "learning_rate": 2.566564089553639e-05,
        "epoch": 0.5899098293464491,
        "step": 7916
    },
    {
        "loss": 2.279,
        "grad_norm": 3.8571155071258545,
        "learning_rate": 2.5618591016040284e-05,
        "epoch": 0.5899843505477308,
        "step": 7917
    },
    {
        "loss": 2.2052,
        "grad_norm": 2.355496883392334,
        "learning_rate": 2.5571577965215244e-05,
        "epoch": 0.5900588717490126,
        "step": 7918
    },
    {
        "loss": 2.2172,
        "grad_norm": 2.0730555057525635,
        "learning_rate": 2.5524601766339075e-05,
        "epoch": 0.5901333929502943,
        "step": 7919
    },
    {
        "loss": 2.5867,
        "grad_norm": 3.0311648845672607,
        "learning_rate": 2.5477662442671203e-05,
        "epoch": 0.5902079141515761,
        "step": 7920
    },
    {
        "loss": 2.6225,
        "grad_norm": 3.037771463394165,
        "learning_rate": 2.5430760017452705e-05,
        "epoch": 0.5902824353528578,
        "step": 7921
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.3253941535949707,
        "learning_rate": 2.538389451390665e-05,
        "epoch": 0.5903569565541397,
        "step": 7922
    },
    {
        "loss": 1.9037,
        "grad_norm": 2.372785806655884,
        "learning_rate": 2.533706595523755e-05,
        "epoch": 0.5904314777554214,
        "step": 7923
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.9711902141571045,
        "learning_rate": 2.529027436463196e-05,
        "epoch": 0.5905059989567032,
        "step": 7924
    },
    {
        "loss": 2.2734,
        "grad_norm": 4.161767959594727,
        "learning_rate": 2.5243519765257773e-05,
        "epoch": 0.5905805201579849,
        "step": 7925
    },
    {
        "loss": 2.3797,
        "grad_norm": 3.2581005096435547,
        "learning_rate": 2.5196802180264665e-05,
        "epoch": 0.5906550413592667,
        "step": 7926
    },
    {
        "loss": 2.4164,
        "grad_norm": 1.8701837062835693,
        "learning_rate": 2.515012163278423e-05,
        "epoch": 0.5907295625605484,
        "step": 7927
    },
    {
        "loss": 2.7251,
        "grad_norm": 3.7457823753356934,
        "learning_rate": 2.5103478145929405e-05,
        "epoch": 0.5908040837618302,
        "step": 7928
    },
    {
        "loss": 1.8643,
        "grad_norm": 2.8814897537231445,
        "learning_rate": 2.5056871742794973e-05,
        "epoch": 0.590878604963112,
        "step": 7929
    },
    {
        "loss": 2.6226,
        "grad_norm": 1.826390027999878,
        "learning_rate": 2.5010302446457255e-05,
        "epoch": 0.5909531261643938,
        "step": 7930
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.4207162857055664,
        "learning_rate": 2.496377027997422e-05,
        "epoch": 0.5910276473656755,
        "step": 7931
    },
    {
        "loss": 2.9077,
        "grad_norm": 3.8266942501068115,
        "learning_rate": 2.491727526638551e-05,
        "epoch": 0.5911021685669573,
        "step": 7932
    },
    {
        "loss": 1.709,
        "grad_norm": 4.212649822235107,
        "learning_rate": 2.48708174287123e-05,
        "epoch": 0.5911766897682391,
        "step": 7933
    },
    {
        "loss": 2.3512,
        "grad_norm": 2.8984291553497314,
        "learning_rate": 2.4824396789957304e-05,
        "epoch": 0.5912512109695208,
        "step": 7934
    },
    {
        "loss": 1.7288,
        "grad_norm": 2.6046957969665527,
        "learning_rate": 2.477801337310508e-05,
        "epoch": 0.5913257321708026,
        "step": 7935
    },
    {
        "loss": 2.2268,
        "grad_norm": 2.820796012878418,
        "learning_rate": 2.473166720112139e-05,
        "epoch": 0.5914002533720844,
        "step": 7936
    },
    {
        "loss": 2.9184,
        "grad_norm": 2.607717514038086,
        "learning_rate": 2.468535829695392e-05,
        "epoch": 0.5914747745733662,
        "step": 7937
    },
    {
        "loss": 2.5283,
        "grad_norm": 4.0515923500061035,
        "learning_rate": 2.4639086683531652e-05,
        "epoch": 0.5915492957746479,
        "step": 7938
    },
    {
        "loss": 1.5743,
        "grad_norm": 3.546306610107422,
        "learning_rate": 2.459285238376513e-05,
        "epoch": 0.5916238169759297,
        "step": 7939
    },
    {
        "loss": 1.6722,
        "grad_norm": 3.0286004543304443,
        "learning_rate": 2.4546655420546526e-05,
        "epoch": 0.5916983381772114,
        "step": 7940
    },
    {
        "loss": 2.6805,
        "grad_norm": 2.5344433784484863,
        "learning_rate": 2.4500495816749436e-05,
        "epoch": 0.5917728593784932,
        "step": 7941
    },
    {
        "loss": 1.8871,
        "grad_norm": 3.582280158996582,
        "learning_rate": 2.445437359522902e-05,
        "epoch": 0.5918473805797749,
        "step": 7942
    },
    {
        "loss": 2.5099,
        "grad_norm": 3.6312334537506104,
        "learning_rate": 2.4408288778821887e-05,
        "epoch": 0.5919219017810567,
        "step": 7943
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.052499771118164,
        "learning_rate": 2.436224139034604e-05,
        "epoch": 0.5919964229823385,
        "step": 7944
    },
    {
        "loss": 2.284,
        "grad_norm": 2.5032522678375244,
        "learning_rate": 2.4316231452601235e-05,
        "epoch": 0.5920709441836203,
        "step": 7945
    },
    {
        "loss": 2.38,
        "grad_norm": 2.230480194091797,
        "learning_rate": 2.427025898836832e-05,
        "epoch": 0.592145465384902,
        "step": 7946
    },
    {
        "loss": 2.224,
        "grad_norm": 4.183070182800293,
        "learning_rate": 2.422432402040997e-05,
        "epoch": 0.5922199865861838,
        "step": 7947
    },
    {
        "loss": 2.51,
        "grad_norm": 2.7630550861358643,
        "learning_rate": 2.4178426571469948e-05,
        "epoch": 0.5922945077874655,
        "step": 7948
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.75407338142395,
        "learning_rate": 2.4132566664273558e-05,
        "epoch": 0.5923690289887473,
        "step": 7949
    },
    {
        "loss": 1.4516,
        "grad_norm": 2.824497938156128,
        "learning_rate": 2.4086744321527677e-05,
        "epoch": 0.592443550190029,
        "step": 7950
    },
    {
        "loss": 2.8955,
        "grad_norm": 3.616098403930664,
        "learning_rate": 2.4040959565920283e-05,
        "epoch": 0.5925180713913109,
        "step": 7951
    },
    {
        "loss": 2.6801,
        "grad_norm": 3.111077308654785,
        "learning_rate": 2.399521242012105e-05,
        "epoch": 0.5925925925925926,
        "step": 7952
    },
    {
        "loss": 3.0156,
        "grad_norm": 3.306408166885376,
        "learning_rate": 2.3949502906780853e-05,
        "epoch": 0.5926671137938744,
        "step": 7953
    },
    {
        "loss": 2.6965,
        "grad_norm": 2.5556142330169678,
        "learning_rate": 2.390383104853182e-05,
        "epoch": 0.5927416349951561,
        "step": 7954
    },
    {
        "loss": 2.0279,
        "grad_norm": 4.294177532196045,
        "learning_rate": 2.3858196867987804e-05,
        "epoch": 0.5928161561964379,
        "step": 7955
    },
    {
        "loss": 2.3703,
        "grad_norm": 3.270972490310669,
        "learning_rate": 2.38126003877436e-05,
        "epoch": 0.5928906773977196,
        "step": 7956
    },
    {
        "loss": 2.7771,
        "grad_norm": 2.1508660316467285,
        "learning_rate": 2.3767041630375696e-05,
        "epoch": 0.5929651985990014,
        "step": 7957
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.8124754428863525,
        "learning_rate": 2.372152061844163e-05,
        "epoch": 0.5930397198002831,
        "step": 7958
    },
    {
        "loss": 2.5592,
        "grad_norm": 2.765470504760742,
        "learning_rate": 2.367603737448031e-05,
        "epoch": 0.593114241001565,
        "step": 7959
    },
    {
        "loss": 2.3382,
        "grad_norm": 3.309133529663086,
        "learning_rate": 2.3630591921012045e-05,
        "epoch": 0.5931887622028467,
        "step": 7960
    },
    {
        "loss": 1.8835,
        "grad_norm": 2.817828416824341,
        "learning_rate": 2.358518428053832e-05,
        "epoch": 0.5932632834041285,
        "step": 7961
    },
    {
        "loss": 2.7806,
        "grad_norm": 3.422008514404297,
        "learning_rate": 2.3539814475542e-05,
        "epoch": 0.5933378046054102,
        "step": 7962
    },
    {
        "loss": 2.0767,
        "grad_norm": 3.136202573776245,
        "learning_rate": 2.3494482528487106e-05,
        "epoch": 0.593412325806692,
        "step": 7963
    },
    {
        "loss": 2.4887,
        "grad_norm": 2.1662378311157227,
        "learning_rate": 2.3449188461818917e-05,
        "epoch": 0.5934868470079737,
        "step": 7964
    },
    {
        "loss": 1.618,
        "grad_norm": 5.028056621551514,
        "learning_rate": 2.340393229796416e-05,
        "epoch": 0.5935613682092555,
        "step": 7965
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.456319570541382,
        "learning_rate": 2.3358714059330567e-05,
        "epoch": 0.5936358894105372,
        "step": 7966
    },
    {
        "loss": 2.8711,
        "grad_norm": 2.2256453037261963,
        "learning_rate": 2.3313533768307083e-05,
        "epoch": 0.5937104106118191,
        "step": 7967
    },
    {
        "loss": 1.5831,
        "grad_norm": 5.281274318695068,
        "learning_rate": 2.3268391447264104e-05,
        "epoch": 0.5937849318131009,
        "step": 7968
    },
    {
        "loss": 1.711,
        "grad_norm": 1.8044979572296143,
        "learning_rate": 2.3223287118552984e-05,
        "epoch": 0.5938594530143826,
        "step": 7969
    },
    {
        "loss": 2.4764,
        "grad_norm": 2.3780527114868164,
        "learning_rate": 2.317822080450639e-05,
        "epoch": 0.5939339742156644,
        "step": 7970
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.6092169284820557,
        "learning_rate": 2.3133192527438108e-05,
        "epoch": 0.5940084954169461,
        "step": 7971
    },
    {
        "loss": 2.6857,
        "grad_norm": 3.2799487113952637,
        "learning_rate": 2.308820230964308e-05,
        "epoch": 0.5940830166182279,
        "step": 7972
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.803537130355835,
        "learning_rate": 2.30432501733975e-05,
        "epoch": 0.5941575378195096,
        "step": 7973
    },
    {
        "loss": 2.2679,
        "grad_norm": 2.5377273559570312,
        "learning_rate": 2.2998336140958532e-05,
        "epoch": 0.5942320590207915,
        "step": 7974
    },
    {
        "loss": 2.6736,
        "grad_norm": 2.1297872066497803,
        "learning_rate": 2.2953460234564738e-05,
        "epoch": 0.5943065802220732,
        "step": 7975
    },
    {
        "loss": 2.4592,
        "grad_norm": 3.128282308578491,
        "learning_rate": 2.2908622476435605e-05,
        "epoch": 0.594381101423355,
        "step": 7976
    },
    {
        "loss": 2.3527,
        "grad_norm": 2.3070781230926514,
        "learning_rate": 2.2863822888771634e-05,
        "epoch": 0.5944556226246367,
        "step": 7977
    },
    {
        "loss": 2.1423,
        "grad_norm": 3.431741714477539,
        "learning_rate": 2.2819061493754756e-05,
        "epoch": 0.5945301438259185,
        "step": 7978
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.5659148693084717,
        "learning_rate": 2.277433831354767e-05,
        "epoch": 0.5946046650272002,
        "step": 7979
    },
    {
        "loss": 2.2133,
        "grad_norm": 2.252565383911133,
        "learning_rate": 2.2729653370294434e-05,
        "epoch": 0.594679186228482,
        "step": 7980
    },
    {
        "loss": 1.6453,
        "grad_norm": 3.8799521923065186,
        "learning_rate": 2.2685006686119903e-05,
        "epoch": 0.5947537074297637,
        "step": 7981
    },
    {
        "loss": 3.2731,
        "grad_norm": 3.195411443710327,
        "learning_rate": 2.2640398283130082e-05,
        "epoch": 0.5948282286310456,
        "step": 7982
    },
    {
        "loss": 1.926,
        "grad_norm": 2.7576706409454346,
        "learning_rate": 2.2595828183412172e-05,
        "epoch": 0.5949027498323273,
        "step": 7983
    },
    {
        "loss": 2.2987,
        "grad_norm": 5.240665912628174,
        "learning_rate": 2.2551296409034173e-05,
        "epoch": 0.5949772710336091,
        "step": 7984
    },
    {
        "loss": 1.7681,
        "grad_norm": 2.895688533782959,
        "learning_rate": 2.250680298204534e-05,
        "epoch": 0.5950517922348908,
        "step": 7985
    },
    {
        "loss": 2.4668,
        "grad_norm": 2.72274112701416,
        "learning_rate": 2.246234792447576e-05,
        "epoch": 0.5951263134361726,
        "step": 7986
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.0148510932922363,
        "learning_rate": 2.241793125833651e-05,
        "epoch": 0.5952008346374543,
        "step": 7987
    },
    {
        "loss": 2.6895,
        "grad_norm": 2.4381909370422363,
        "learning_rate": 2.2373553005619918e-05,
        "epoch": 0.5952753558387361,
        "step": 7988
    },
    {
        "loss": 2.2043,
        "grad_norm": 3.423705816268921,
        "learning_rate": 2.232921318829898e-05,
        "epoch": 0.5953498770400178,
        "step": 7989
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.9669485092163086,
        "learning_rate": 2.228491182832785e-05,
        "epoch": 0.5954243982412997,
        "step": 7990
    },
    {
        "loss": 2.3298,
        "grad_norm": 3.1728932857513428,
        "learning_rate": 2.2240648947641552e-05,
        "epoch": 0.5954989194425814,
        "step": 7991
    },
    {
        "loss": 2.2244,
        "grad_norm": 3.292484998703003,
        "learning_rate": 2.2196424568156073e-05,
        "epoch": 0.5955734406438632,
        "step": 7992
    },
    {
        "loss": 1.7607,
        "grad_norm": 3.13075590133667,
        "learning_rate": 2.215223871176839e-05,
        "epoch": 0.5956479618451449,
        "step": 7993
    },
    {
        "loss": 2.4969,
        "grad_norm": 2.300067901611328,
        "learning_rate": 2.2108091400356345e-05,
        "epoch": 0.5957224830464267,
        "step": 7994
    },
    {
        "loss": 2.121,
        "grad_norm": 2.9339754581451416,
        "learning_rate": 2.206398265577865e-05,
        "epoch": 0.5957970042477084,
        "step": 7995
    },
    {
        "loss": 1.9862,
        "grad_norm": 2.9767415523529053,
        "learning_rate": 2.2019912499875127e-05,
        "epoch": 0.5958715254489902,
        "step": 7996
    },
    {
        "loss": 2.6214,
        "grad_norm": 3.1052775382995605,
        "learning_rate": 2.197588095446621e-05,
        "epoch": 0.595946046650272,
        "step": 7997
    },
    {
        "loss": 2.3663,
        "grad_norm": 2.9593758583068848,
        "learning_rate": 2.193188804135351e-05,
        "epoch": 0.5960205678515538,
        "step": 7998
    },
    {
        "loss": 2.5054,
        "grad_norm": 2.5355193614959717,
        "learning_rate": 2.1887933782319313e-05,
        "epoch": 0.5960950890528355,
        "step": 7999
    },
    {
        "loss": 1.7109,
        "grad_norm": 3.0629358291625977,
        "learning_rate": 2.1844018199126704e-05,
        "epoch": 0.5961696102541173,
        "step": 8000
    },
    {
        "loss": 2.5081,
        "grad_norm": 2.172119140625,
        "learning_rate": 2.180014131351985e-05,
        "epoch": 0.596244131455399,
        "step": 8001
    },
    {
        "loss": 1.8894,
        "grad_norm": 3.0725390911102295,
        "learning_rate": 2.1756303147223568e-05,
        "epoch": 0.5963186526566808,
        "step": 8002
    },
    {
        "loss": 1.2356,
        "grad_norm": 4.201506614685059,
        "learning_rate": 2.171250372194361e-05,
        "epoch": 0.5963931738579626,
        "step": 8003
    },
    {
        "loss": 2.5126,
        "grad_norm": 2.917261838912964,
        "learning_rate": 2.1668743059366492e-05,
        "epoch": 0.5964676950592444,
        "step": 8004
    },
    {
        "loss": 2.3626,
        "grad_norm": 3.4637091159820557,
        "learning_rate": 2.1625021181159454e-05,
        "epoch": 0.5965422162605262,
        "step": 8005
    },
    {
        "loss": 2.1813,
        "grad_norm": 3.3637118339538574,
        "learning_rate": 2.1581338108970806e-05,
        "epoch": 0.5966167374618079,
        "step": 8006
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.372732639312744,
        "learning_rate": 2.153769386442932e-05,
        "epoch": 0.5966912586630897,
        "step": 8007
    },
    {
        "loss": 2.4929,
        "grad_norm": 1.9888851642608643,
        "learning_rate": 2.1494088469144802e-05,
        "epoch": 0.5967657798643714,
        "step": 8008
    },
    {
        "loss": 2.7176,
        "grad_norm": 2.1917476654052734,
        "learning_rate": 2.145052194470767e-05,
        "epoch": 0.5968403010656532,
        "step": 8009
    },
    {
        "loss": 2.7108,
        "grad_norm": 2.274472713470459,
        "learning_rate": 2.1406994312689067e-05,
        "epoch": 0.5969148222669349,
        "step": 8010
    },
    {
        "loss": 2.4429,
        "grad_norm": 2.2480216026306152,
        "learning_rate": 2.1363505594641086e-05,
        "epoch": 0.5969893434682167,
        "step": 8011
    },
    {
        "loss": 2.4465,
        "grad_norm": 4.00696325302124,
        "learning_rate": 2.1320055812096263e-05,
        "epoch": 0.5970638646694985,
        "step": 8012
    },
    {
        "loss": 2.6345,
        "grad_norm": 5.426177978515625,
        "learning_rate": 2.127664498656813e-05,
        "epoch": 0.5971383858707803,
        "step": 8013
    },
    {
        "loss": 2.4706,
        "grad_norm": 2.894348382949829,
        "learning_rate": 2.123327313955079e-05,
        "epoch": 0.597212907072062,
        "step": 8014
    },
    {
        "loss": 2.4301,
        "grad_norm": 3.042119026184082,
        "learning_rate": 2.118994029251893e-05,
        "epoch": 0.5972874282733438,
        "step": 8015
    },
    {
        "loss": 2.5135,
        "grad_norm": 2.5788912773132324,
        "learning_rate": 2.1146646466928245e-05,
        "epoch": 0.5973619494746255,
        "step": 8016
    },
    {
        "loss": 2.017,
        "grad_norm": 2.7553398609161377,
        "learning_rate": 2.1103391684214767e-05,
        "epoch": 0.5974364706759073,
        "step": 8017
    },
    {
        "loss": 2.5296,
        "grad_norm": 3.0489015579223633,
        "learning_rate": 2.1060175965795525e-05,
        "epoch": 0.597510991877189,
        "step": 8018
    },
    {
        "loss": 2.7389,
        "grad_norm": 2.403977155685425,
        "learning_rate": 2.1016999333067944e-05,
        "epoch": 0.5975855130784709,
        "step": 8019
    },
    {
        "loss": 1.8484,
        "grad_norm": 3.078274726867676,
        "learning_rate": 2.0973861807410155e-05,
        "epoch": 0.5976600342797526,
        "step": 8020
    },
    {
        "loss": 2.7445,
        "grad_norm": 3.2077877521514893,
        "learning_rate": 2.0930763410181033e-05,
        "epoch": 0.5977345554810344,
        "step": 8021
    },
    {
        "loss": 2.2981,
        "grad_norm": 2.991777181625366,
        "learning_rate": 2.0887704162719933e-05,
        "epoch": 0.5978090766823161,
        "step": 8022
    },
    {
        "loss": 2.2917,
        "grad_norm": 3.8331685066223145,
        "learning_rate": 2.0844684086346966e-05,
        "epoch": 0.5978835978835979,
        "step": 8023
    },
    {
        "loss": 1.6031,
        "grad_norm": 3.2645065784454346,
        "learning_rate": 2.0801703202362754e-05,
        "epoch": 0.5979581190848796,
        "step": 8024
    },
    {
        "loss": 2.8051,
        "grad_norm": 2.327759027481079,
        "learning_rate": 2.0758761532048444e-05,
        "epoch": 0.5980326402861614,
        "step": 8025
    },
    {
        "loss": 2.7821,
        "grad_norm": 2.451693534851074,
        "learning_rate": 2.0715859096666047e-05,
        "epoch": 0.5981071614874431,
        "step": 8026
    },
    {
        "loss": 2.9022,
        "grad_norm": 1.9752557277679443,
        "learning_rate": 2.067299591745786e-05,
        "epoch": 0.598181682688725,
        "step": 8027
    },
    {
        "loss": 1.7708,
        "grad_norm": 1.5888981819152832,
        "learning_rate": 2.0630172015646777e-05,
        "epoch": 0.5982562038900067,
        "step": 8028
    },
    {
        "loss": 2.6557,
        "grad_norm": 2.0952935218811035,
        "learning_rate": 2.05873874124365e-05,
        "epoch": 0.5983307250912885,
        "step": 8029
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.403883695602417,
        "learning_rate": 2.0544642129010905e-05,
        "epoch": 0.5984052462925702,
        "step": 8030
    },
    {
        "loss": 1.6819,
        "grad_norm": 2.076828718185425,
        "learning_rate": 2.050193618653471e-05,
        "epoch": 0.598479767493852,
        "step": 8031
    },
    {
        "loss": 2.7342,
        "grad_norm": 2.4647600650787354,
        "learning_rate": 2.0459269606152963e-05,
        "epoch": 0.5985542886951337,
        "step": 8032
    },
    {
        "loss": 2.6471,
        "grad_norm": 2.7647318840026855,
        "learning_rate": 2.0416642408991284e-05,
        "epoch": 0.5986288098964155,
        "step": 8033
    },
    {
        "loss": 1.7016,
        "grad_norm": 5.402695178985596,
        "learning_rate": 2.0374054616155824e-05,
        "epoch": 0.5987033310976972,
        "step": 8034
    },
    {
        "loss": 2.707,
        "grad_norm": 2.3237438201904297,
        "learning_rate": 2.0331506248733088e-05,
        "epoch": 0.5987778522989791,
        "step": 8035
    },
    {
        "loss": 2.0923,
        "grad_norm": 3.8797106742858887,
        "learning_rate": 2.0288997327790336e-05,
        "epoch": 0.5988523735002608,
        "step": 8036
    },
    {
        "loss": 1.8756,
        "grad_norm": 3.554149627685547,
        "learning_rate": 2.0246527874375076e-05,
        "epoch": 0.5989268947015426,
        "step": 8037
    },
    {
        "loss": 2.0925,
        "grad_norm": 2.5499017238616943,
        "learning_rate": 2.0204097909515183e-05,
        "epoch": 0.5990014159028244,
        "step": 8038
    },
    {
        "loss": 2.5725,
        "grad_norm": 3.788677930831909,
        "learning_rate": 2.0161707454219303e-05,
        "epoch": 0.5990759371041061,
        "step": 8039
    },
    {
        "loss": 2.297,
        "grad_norm": 3.1037726402282715,
        "learning_rate": 2.0119356529476218e-05,
        "epoch": 0.5991504583053879,
        "step": 8040
    },
    {
        "loss": 1.9224,
        "grad_norm": 2.9073803424835205,
        "learning_rate": 2.007704515625539e-05,
        "epoch": 0.5992249795066696,
        "step": 8041
    },
    {
        "loss": 2.209,
        "grad_norm": 2.3749561309814453,
        "learning_rate": 2.0034773355506453e-05,
        "epoch": 0.5992995007079515,
        "step": 8042
    },
    {
        "loss": 2.5109,
        "grad_norm": 3.350475311279297,
        "learning_rate": 1.9992541148159538e-05,
        "epoch": 0.5993740219092332,
        "step": 8043
    },
    {
        "loss": 2.595,
        "grad_norm": 2.0790231227874756,
        "learning_rate": 1.995034855512532e-05,
        "epoch": 0.599448543110515,
        "step": 8044
    },
    {
        "loss": 2.8022,
        "grad_norm": 3.4457833766937256,
        "learning_rate": 1.9908195597294542e-05,
        "epoch": 0.5995230643117967,
        "step": 8045
    },
    {
        "loss": 1.519,
        "grad_norm": 4.391590595245361,
        "learning_rate": 1.9866082295538713e-05,
        "epoch": 0.5995975855130785,
        "step": 8046
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.2037267684936523,
        "learning_rate": 1.9824008670709426e-05,
        "epoch": 0.5996721067143602,
        "step": 8047
    },
    {
        "loss": 2.6655,
        "grad_norm": 1.8442906141281128,
        "learning_rate": 1.978197474363863e-05,
        "epoch": 0.599746627915642,
        "step": 8048
    },
    {
        "loss": 2.3687,
        "grad_norm": 4.293654918670654,
        "learning_rate": 1.9739980535138858e-05,
        "epoch": 0.5998211491169237,
        "step": 8049
    },
    {
        "loss": 2.435,
        "grad_norm": 3.7536368370056152,
        "learning_rate": 1.9698026066002706e-05,
        "epoch": 0.5998956703182056,
        "step": 8050
    },
    {
        "loss": 2.5202,
        "grad_norm": 2.8358614444732666,
        "learning_rate": 1.965611135700326e-05,
        "epoch": 0.5999701915194873,
        "step": 8051
    },
    {
        "loss": 2.3993,
        "grad_norm": 2.7682018280029297,
        "learning_rate": 1.961423642889384e-05,
        "epoch": 0.6000447127207691,
        "step": 8052
    },
    {
        "loss": 2.4531,
        "grad_norm": 2.3331732749938965,
        "learning_rate": 1.957240130240807e-05,
        "epoch": 0.6001192339220508,
        "step": 8053
    },
    {
        "loss": 2.8386,
        "grad_norm": 2.854990005493164,
        "learning_rate": 1.9530605998259932e-05,
        "epoch": 0.6001937551233326,
        "step": 8054
    },
    {
        "loss": 2.185,
        "grad_norm": 2.077186346054077,
        "learning_rate": 1.9488850537143567e-05,
        "epoch": 0.6002682763246143,
        "step": 8055
    },
    {
        "loss": 2.0385,
        "grad_norm": 3.0441126823425293,
        "learning_rate": 1.9447134939733614e-05,
        "epoch": 0.6003427975258961,
        "step": 8056
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.202542543411255,
        "learning_rate": 1.9405459226684753e-05,
        "epoch": 0.6004173187271779,
        "step": 8057
    },
    {
        "loss": 2.4271,
        "grad_norm": 2.0380592346191406,
        "learning_rate": 1.936382341863191e-05,
        "epoch": 0.6004918399284597,
        "step": 8058
    },
    {
        "loss": 1.3126,
        "grad_norm": 4.036860942840576,
        "learning_rate": 1.9322227536190507e-05,
        "epoch": 0.6005663611297414,
        "step": 8059
    },
    {
        "loss": 2.9929,
        "grad_norm": 2.8077597618103027,
        "learning_rate": 1.9280671599955968e-05,
        "epoch": 0.6006408823310232,
        "step": 8060
    },
    {
        "loss": 2.9421,
        "grad_norm": 3.197901725769043,
        "learning_rate": 1.923915563050389e-05,
        "epoch": 0.6007154035323049,
        "step": 8061
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.895843982696533,
        "learning_rate": 1.919767964839032e-05,
        "epoch": 0.6007899247335867,
        "step": 8062
    },
    {
        "loss": 2.3076,
        "grad_norm": 2.6123504638671875,
        "learning_rate": 1.915624367415131e-05,
        "epoch": 0.6008644459348684,
        "step": 8063
    },
    {
        "loss": 2.8085,
        "grad_norm": 1.9708530902862549,
        "learning_rate": 1.9114847728303208e-05,
        "epoch": 0.6009389671361502,
        "step": 8064
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.8863985538482666,
        "learning_rate": 1.907349183134247e-05,
        "epoch": 0.601013488337432,
        "step": 8065
    },
    {
        "loss": 2.4102,
        "grad_norm": 3.319197654724121,
        "learning_rate": 1.9032176003745693e-05,
        "epoch": 0.6010880095387138,
        "step": 8066
    },
    {
        "loss": 2.906,
        "grad_norm": 2.507763147354126,
        "learning_rate": 1.8990900265969868e-05,
        "epoch": 0.6011625307399955,
        "step": 8067
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.2117958068847656,
        "learning_rate": 1.894966463845175e-05,
        "epoch": 0.6012370519412773,
        "step": 8068
    },
    {
        "loss": 2.3255,
        "grad_norm": 3.424445629119873,
        "learning_rate": 1.890846914160864e-05,
        "epoch": 0.601311573142559,
        "step": 8069
    },
    {
        "loss": 2.3795,
        "grad_norm": 2.377502918243408,
        "learning_rate": 1.8867313795837704e-05,
        "epoch": 0.6013860943438408,
        "step": 8070
    },
    {
        "loss": 3.0749,
        "grad_norm": 2.9247372150421143,
        "learning_rate": 1.8826198621516267e-05,
        "epoch": 0.6014606155451225,
        "step": 8071
    },
    {
        "loss": 2.8849,
        "grad_norm": 3.1546950340270996,
        "learning_rate": 1.8785123639001856e-05,
        "epoch": 0.6015351367464044,
        "step": 8072
    },
    {
        "loss": 2.0558,
        "grad_norm": 3.2523043155670166,
        "learning_rate": 1.8744088868631948e-05,
        "epoch": 0.6016096579476862,
        "step": 8073
    },
    {
        "loss": 2.1449,
        "grad_norm": 4.178511619567871,
        "learning_rate": 1.8703094330724347e-05,
        "epoch": 0.6016841791489679,
        "step": 8074
    },
    {
        "loss": 2.6885,
        "grad_norm": 2.4576454162597656,
        "learning_rate": 1.8662140045576683e-05,
        "epoch": 0.6017587003502497,
        "step": 8075
    },
    {
        "loss": 2.6059,
        "grad_norm": 2.716588020324707,
        "learning_rate": 1.862122603346671e-05,
        "epoch": 0.6018332215515314,
        "step": 8076
    },
    {
        "loss": 1.9762,
        "grad_norm": 3.248485803604126,
        "learning_rate": 1.858035231465245e-05,
        "epoch": 0.6019077427528132,
        "step": 8077
    },
    {
        "loss": 2.2219,
        "grad_norm": 3.3004372119903564,
        "learning_rate": 1.8539518909371655e-05,
        "epoch": 0.6019822639540949,
        "step": 8078
    },
    {
        "loss": 2.4338,
        "grad_norm": 3.6847801208496094,
        "learning_rate": 1.8498725837842458e-05,
        "epoch": 0.6020567851553768,
        "step": 8079
    },
    {
        "loss": 2.0089,
        "grad_norm": 4.105456829071045,
        "learning_rate": 1.845797312026275e-05,
        "epoch": 0.6021313063566585,
        "step": 8080
    },
    {
        "loss": 2.2275,
        "grad_norm": 3.3537468910217285,
        "learning_rate": 1.841726077681052e-05,
        "epoch": 0.6022058275579403,
        "step": 8081
    },
    {
        "loss": 2.7138,
        "grad_norm": 2.224480390548706,
        "learning_rate": 1.8376588827643824e-05,
        "epoch": 0.602280348759222,
        "step": 8082
    },
    {
        "loss": 1.1532,
        "grad_norm": 6.723317623138428,
        "learning_rate": 1.833595729290063e-05,
        "epoch": 0.6023548699605038,
        "step": 8083
    },
    {
        "loss": 2.2403,
        "grad_norm": 2.811326503753662,
        "learning_rate": 1.8295366192698994e-05,
        "epoch": 0.6024293911617855,
        "step": 8084
    },
    {
        "loss": 2.2606,
        "grad_norm": 3.7651491165161133,
        "learning_rate": 1.825481554713686e-05,
        "epoch": 0.6025039123630673,
        "step": 8085
    },
    {
        "loss": 2.8402,
        "grad_norm": 2.0986549854278564,
        "learning_rate": 1.821430537629213e-05,
        "epoch": 0.602578433564349,
        "step": 8086
    },
    {
        "loss": 2.3279,
        "grad_norm": 3.5383942127227783,
        "learning_rate": 1.817383570022284e-05,
        "epoch": 0.6026529547656309,
        "step": 8087
    },
    {
        "loss": 2.827,
        "grad_norm": 2.316260814666748,
        "learning_rate": 1.81334065389668e-05,
        "epoch": 0.6027274759669126,
        "step": 8088
    },
    {
        "loss": 2.147,
        "grad_norm": 2.68452525138855,
        "learning_rate": 1.809301791254172e-05,
        "epoch": 0.6028019971681944,
        "step": 8089
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.1350595951080322,
        "learning_rate": 1.8052669840945514e-05,
        "epoch": 0.6028765183694761,
        "step": 8090
    },
    {
        "loss": 2.3954,
        "grad_norm": 2.5699691772460938,
        "learning_rate": 1.8012362344155653e-05,
        "epoch": 0.6029510395707579,
        "step": 8091
    },
    {
        "loss": 1.8752,
        "grad_norm": 2.954010009765625,
        "learning_rate": 1.7972095442129833e-05,
        "epoch": 0.6030255607720396,
        "step": 8092
    },
    {
        "loss": 1.8016,
        "grad_norm": 3.3989322185516357,
        "learning_rate": 1.793186915480548e-05,
        "epoch": 0.6031000819733214,
        "step": 8093
    },
    {
        "loss": 2.6611,
        "grad_norm": 2.125791549682617,
        "learning_rate": 1.7891683502099854e-05,
        "epoch": 0.6031746031746031,
        "step": 8094
    },
    {
        "loss": 2.39,
        "grad_norm": 3.8555452823638916,
        "learning_rate": 1.7851538503910303e-05,
        "epoch": 0.603249124375885,
        "step": 8095
    },
    {
        "loss": 1.9163,
        "grad_norm": 2.811682939529419,
        "learning_rate": 1.7811434180113818e-05,
        "epoch": 0.6033236455771667,
        "step": 8096
    },
    {
        "loss": 2.152,
        "grad_norm": 2.1483354568481445,
        "learning_rate": 1.7771370550567535e-05,
        "epoch": 0.6033981667784485,
        "step": 8097
    },
    {
        "loss": 2.7185,
        "grad_norm": 2.1793837547302246,
        "learning_rate": 1.7731347635108132e-05,
        "epoch": 0.6034726879797302,
        "step": 8098
    },
    {
        "loss": 2.568,
        "grad_norm": 3.2907843589782715,
        "learning_rate": 1.7691365453552224e-05,
        "epoch": 0.603547209181012,
        "step": 8099
    },
    {
        "loss": 2.4472,
        "grad_norm": 2.2799952030181885,
        "learning_rate": 1.765142402569644e-05,
        "epoch": 0.6036217303822937,
        "step": 8100
    },
    {
        "loss": 2.523,
        "grad_norm": 3.105731248855591,
        "learning_rate": 1.7611523371317006e-05,
        "epoch": 0.6036962515835755,
        "step": 8101
    },
    {
        "loss": 2.4872,
        "grad_norm": 3.479778528213501,
        "learning_rate": 1.7571663510170077e-05,
        "epoch": 0.6037707727848572,
        "step": 8102
    },
    {
        "loss": 1.8114,
        "grad_norm": 4.7315778732299805,
        "learning_rate": 1.7531844461991564e-05,
        "epoch": 0.6038452939861391,
        "step": 8103
    },
    {
        "loss": 2.5265,
        "grad_norm": 3.680070161819458,
        "learning_rate": 1.749206624649712e-05,
        "epoch": 0.6039198151874208,
        "step": 8104
    },
    {
        "loss": 1.9563,
        "grad_norm": 3.4433035850524902,
        "learning_rate": 1.7452328883382362e-05,
        "epoch": 0.6039943363887026,
        "step": 8105
    },
    {
        "loss": 2.784,
        "grad_norm": 2.882535219192505,
        "learning_rate": 1.7412632392322402e-05,
        "epoch": 0.6040688575899843,
        "step": 8106
    },
    {
        "loss": 2.1894,
        "grad_norm": 3.1971890926361084,
        "learning_rate": 1.7372976792972427e-05,
        "epoch": 0.6041433787912661,
        "step": 8107
    },
    {
        "loss": 2.371,
        "grad_norm": 3.1849184036254883,
        "learning_rate": 1.7333362104967177e-05,
        "epoch": 0.6042178999925478,
        "step": 8108
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.872495174407959,
        "learning_rate": 1.729378834792107e-05,
        "epoch": 0.6042924211938296,
        "step": 8109
    },
    {
        "loss": 2.3961,
        "grad_norm": 2.261767864227295,
        "learning_rate": 1.7254255541428554e-05,
        "epoch": 0.6043669423951115,
        "step": 8110
    },
    {
        "loss": 2.055,
        "grad_norm": 1.856624960899353,
        "learning_rate": 1.7214763705063485e-05,
        "epoch": 0.6044414635963932,
        "step": 8111
    },
    {
        "loss": 1.8525,
        "grad_norm": 2.4173424243927,
        "learning_rate": 1.7175312858379632e-05,
        "epoch": 0.604515984797675,
        "step": 8112
    },
    {
        "loss": 2.4727,
        "grad_norm": 3.1721882820129395,
        "learning_rate": 1.7135903020910382e-05,
        "epoch": 0.6045905059989567,
        "step": 8113
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.057393789291382,
        "learning_rate": 1.709653421216879e-05,
        "epoch": 0.6046650272002385,
        "step": 8114
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.6310033798217773,
        "learning_rate": 1.705720645164771e-05,
        "epoch": 0.6047395484015202,
        "step": 8115
    },
    {
        "loss": 2.374,
        "grad_norm": 2.5455822944641113,
        "learning_rate": 1.70179197588195e-05,
        "epoch": 0.604814069602802,
        "step": 8116
    },
    {
        "loss": 2.143,
        "grad_norm": 2.5620906352996826,
        "learning_rate": 1.697867415313643e-05,
        "epoch": 0.6048885908040837,
        "step": 8117
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.8568496704101562,
        "learning_rate": 1.6939469654030217e-05,
        "epoch": 0.6049631120053656,
        "step": 8118
    },
    {
        "loss": 2.3165,
        "grad_norm": 2.613938808441162,
        "learning_rate": 1.6900306280912214e-05,
        "epoch": 0.6050376332066473,
        "step": 8119
    },
    {
        "loss": 2.2041,
        "grad_norm": 2.810784101486206,
        "learning_rate": 1.6861184053173662e-05,
        "epoch": 0.6051121544079291,
        "step": 8120
    },
    {
        "loss": 2.7163,
        "grad_norm": 1.8258405923843384,
        "learning_rate": 1.6822102990185152e-05,
        "epoch": 0.6051866756092108,
        "step": 8121
    },
    {
        "loss": 1.9204,
        "grad_norm": 3.001513957977295,
        "learning_rate": 1.678306311129694e-05,
        "epoch": 0.6052611968104926,
        "step": 8122
    },
    {
        "loss": 2.1123,
        "grad_norm": 2.5651628971099854,
        "learning_rate": 1.6744064435839068e-05,
        "epoch": 0.6053357180117743,
        "step": 8123
    },
    {
        "loss": 1.7839,
        "grad_norm": 3.3729093074798584,
        "learning_rate": 1.6705106983120977e-05,
        "epoch": 0.6054102392130561,
        "step": 8124
    },
    {
        "loss": 1.5206,
        "grad_norm": 3.1029181480407715,
        "learning_rate": 1.6666190772431844e-05,
        "epoch": 0.6054847604143379,
        "step": 8125
    },
    {
        "loss": 2.183,
        "grad_norm": 3.590256929397583,
        "learning_rate": 1.6627315823040302e-05,
        "epoch": 0.6055592816156197,
        "step": 8126
    },
    {
        "loss": 2.5657,
        "grad_norm": 2.7895877361297607,
        "learning_rate": 1.6588482154194562e-05,
        "epoch": 0.6056338028169014,
        "step": 8127
    },
    {
        "loss": 2.4666,
        "grad_norm": 2.2438645362854004,
        "learning_rate": 1.6549689785122613e-05,
        "epoch": 0.6057083240181832,
        "step": 8128
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.775038957595825,
        "learning_rate": 1.6510938735031633e-05,
        "epoch": 0.6057828452194649,
        "step": 8129
    },
    {
        "loss": 2.2715,
        "grad_norm": 4.75587797164917,
        "learning_rate": 1.6472229023108686e-05,
        "epoch": 0.6058573664207467,
        "step": 8130
    },
    {
        "loss": 1.9688,
        "grad_norm": 3.6512415409088135,
        "learning_rate": 1.6433560668520176e-05,
        "epoch": 0.6059318876220284,
        "step": 8131
    },
    {
        "loss": 2.4744,
        "grad_norm": 2.2541377544403076,
        "learning_rate": 1.639493369041203e-05,
        "epoch": 0.6060064088233102,
        "step": 8132
    },
    {
        "loss": 2.03,
        "grad_norm": 3.7099153995513916,
        "learning_rate": 1.635634810790977e-05,
        "epoch": 0.606080930024592,
        "step": 8133
    },
    {
        "loss": 2.2459,
        "grad_norm": 2.75990891456604,
        "learning_rate": 1.6317803940118327e-05,
        "epoch": 0.6061554512258738,
        "step": 8134
    },
    {
        "loss": 2.645,
        "grad_norm": 2.095546007156372,
        "learning_rate": 1.62793012061223e-05,
        "epoch": 0.6062299724271555,
        "step": 8135
    },
    {
        "loss": 2.3228,
        "grad_norm": 2.44860577583313,
        "learning_rate": 1.624083992498554e-05,
        "epoch": 0.6063044936284373,
        "step": 8136
    },
    {
        "loss": 2.7275,
        "grad_norm": 2.2148163318634033,
        "learning_rate": 1.6202420115751447e-05,
        "epoch": 0.606379014829719,
        "step": 8137
    },
    {
        "loss": 2.0385,
        "grad_norm": 3.324139356613159,
        "learning_rate": 1.6164041797443073e-05,
        "epoch": 0.6064535360310008,
        "step": 8138
    },
    {
        "loss": 2.3502,
        "grad_norm": 3.104865312576294,
        "learning_rate": 1.612570498906264e-05,
        "epoch": 0.6065280572322825,
        "step": 8139
    },
    {
        "loss": 2.2624,
        "grad_norm": 2.78426456451416,
        "learning_rate": 1.6087409709592106e-05,
        "epoch": 0.6066025784335644,
        "step": 8140
    },
    {
        "loss": 2.3886,
        "grad_norm": 2.9001171588897705,
        "learning_rate": 1.6049155977992626e-05,
        "epoch": 0.6066770996348461,
        "step": 8141
    },
    {
        "loss": 2.7083,
        "grad_norm": 3.65525484085083,
        "learning_rate": 1.601094381320486e-05,
        "epoch": 0.6067516208361279,
        "step": 8142
    },
    {
        "loss": 2.068,
        "grad_norm": 2.095388174057007,
        "learning_rate": 1.5972773234148953e-05,
        "epoch": 0.6068261420374096,
        "step": 8143
    },
    {
        "loss": 2.5421,
        "grad_norm": 1.9531667232513428,
        "learning_rate": 1.5934644259724363e-05,
        "epoch": 0.6069006632386914,
        "step": 8144
    },
    {
        "loss": 1.9609,
        "grad_norm": 3.261233329772949,
        "learning_rate": 1.5896556908810034e-05,
        "epoch": 0.6069751844399732,
        "step": 8145
    },
    {
        "loss": 2.2024,
        "grad_norm": 2.532989025115967,
        "learning_rate": 1.5858511200264238e-05,
        "epoch": 0.6070497056412549,
        "step": 8146
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.116102695465088,
        "learning_rate": 1.582050715292458e-05,
        "epoch": 0.6071242268425368,
        "step": 8147
    },
    {
        "loss": 2.8283,
        "grad_norm": 2.2460079193115234,
        "learning_rate": 1.5782544785608256e-05,
        "epoch": 0.6071987480438185,
        "step": 8148
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.771883487701416,
        "learning_rate": 1.5744624117111606e-05,
        "epoch": 0.6072732692451003,
        "step": 8149
    },
    {
        "loss": 2.6224,
        "grad_norm": 1.5721873044967651,
        "learning_rate": 1.5706745166210324e-05,
        "epoch": 0.607347790446382,
        "step": 8150
    },
    {
        "loss": 1.8676,
        "grad_norm": 3.153878688812256,
        "learning_rate": 1.5668907951659674e-05,
        "epoch": 0.6074223116476638,
        "step": 8151
    },
    {
        "loss": 2.5558,
        "grad_norm": 4.850111961364746,
        "learning_rate": 1.5631112492193945e-05,
        "epoch": 0.6074968328489455,
        "step": 8152
    },
    {
        "loss": 2.3686,
        "grad_norm": 3.2022297382354736,
        "learning_rate": 1.5593358806527016e-05,
        "epoch": 0.6075713540502273,
        "step": 8153
    },
    {
        "loss": 2.3762,
        "grad_norm": 3.2049436569213867,
        "learning_rate": 1.555564691335195e-05,
        "epoch": 0.607645875251509,
        "step": 8154
    },
    {
        "loss": 2.4884,
        "grad_norm": 5.113078594207764,
        "learning_rate": 1.5517976831341063e-05,
        "epoch": 0.6077203964527909,
        "step": 8155
    },
    {
        "loss": 2.702,
        "grad_norm": 2.192324638366699,
        "learning_rate": 1.5480348579146143e-05,
        "epoch": 0.6077949176540726,
        "step": 8156
    },
    {
        "loss": 2.7069,
        "grad_norm": 2.81196928024292,
        "learning_rate": 1.5442762175398063e-05,
        "epoch": 0.6078694388553544,
        "step": 8157
    },
    {
        "loss": 1.4665,
        "grad_norm": 2.3850221633911133,
        "learning_rate": 1.5405217638707236e-05,
        "epoch": 0.6079439600566361,
        "step": 8158
    },
    {
        "loss": 2.2411,
        "grad_norm": 2.0975148677825928,
        "learning_rate": 1.5367714987663097e-05,
        "epoch": 0.6080184812579179,
        "step": 8159
    },
    {
        "loss": 1.9099,
        "grad_norm": 3.535534381866455,
        "learning_rate": 1.5330254240834363e-05,
        "epoch": 0.6080930024591996,
        "step": 8160
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.2934162616729736,
        "learning_rate": 1.5292835416769236e-05,
        "epoch": 0.6081675236604814,
        "step": 8161
    },
    {
        "loss": 2.6129,
        "grad_norm": 2.1616079807281494,
        "learning_rate": 1.5255458533994893e-05,
        "epoch": 0.6082420448617631,
        "step": 8162
    },
    {
        "loss": 2.4166,
        "grad_norm": 2.946807861328125,
        "learning_rate": 1.521812361101791e-05,
        "epoch": 0.608316566063045,
        "step": 8163
    },
    {
        "loss": 2.315,
        "grad_norm": 2.35680890083313,
        "learning_rate": 1.5180830666323998e-05,
        "epoch": 0.6083910872643267,
        "step": 8164
    },
    {
        "loss": 1.8048,
        "grad_norm": 2.143894910812378,
        "learning_rate": 1.5143579718378098e-05,
        "epoch": 0.6084656084656085,
        "step": 8165
    },
    {
        "loss": 2.733,
        "grad_norm": 2.0730462074279785,
        "learning_rate": 1.5106370785624413e-05,
        "epoch": 0.6085401296668902,
        "step": 8166
    },
    {
        "loss": 1.6486,
        "grad_norm": 3.062252998352051,
        "learning_rate": 1.5069203886486227e-05,
        "epoch": 0.608614650868172,
        "step": 8167
    },
    {
        "loss": 2.5723,
        "grad_norm": 2.4813408851623535,
        "learning_rate": 1.5032079039366209e-05,
        "epoch": 0.6086891720694537,
        "step": 8168
    },
    {
        "loss": 2.875,
        "grad_norm": 3.3438799381256104,
        "learning_rate": 1.4994996262646034e-05,
        "epoch": 0.6087636932707355,
        "step": 8169
    },
    {
        "loss": 1.9938,
        "grad_norm": 2.6494150161743164,
        "learning_rate": 1.4957955574686522e-05,
        "epoch": 0.6088382144720172,
        "step": 8170
    },
    {
        "loss": 1.7582,
        "grad_norm": 4.09646463394165,
        "learning_rate": 1.4920956993827861e-05,
        "epoch": 0.6089127356732991,
        "step": 8171
    },
    {
        "loss": 1.7049,
        "grad_norm": 2.8665108680725098,
        "learning_rate": 1.4884000538389186e-05,
        "epoch": 0.6089872568745808,
        "step": 8172
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.280942678451538,
        "learning_rate": 1.4847086226668872e-05,
        "epoch": 0.6090617780758626,
        "step": 8173
    },
    {
        "loss": 2.0154,
        "grad_norm": 3.2442636489868164,
        "learning_rate": 1.4810214076944396e-05,
        "epoch": 0.6091362992771443,
        "step": 8174
    },
    {
        "loss": 1.9698,
        "grad_norm": 3.897568464279175,
        "learning_rate": 1.4773384107472343e-05,
        "epoch": 0.6092108204784261,
        "step": 8175
    },
    {
        "loss": 2.6702,
        "grad_norm": 1.534300684928894,
        "learning_rate": 1.4736596336488473e-05,
        "epoch": 0.6092853416797078,
        "step": 8176
    },
    {
        "loss": 2.6157,
        "grad_norm": 2.4968345165252686,
        "learning_rate": 1.4699850782207525e-05,
        "epoch": 0.6093598628809896,
        "step": 8177
    },
    {
        "loss": 2.3017,
        "grad_norm": 3.084306001663208,
        "learning_rate": 1.4663147462823578e-05,
        "epoch": 0.6094343840822714,
        "step": 8178
    },
    {
        "loss": 2.7107,
        "grad_norm": 4.014954566955566,
        "learning_rate": 1.4626486396509575e-05,
        "epoch": 0.6095089052835532,
        "step": 8179
    },
    {
        "loss": 1.6079,
        "grad_norm": 3.6446752548217773,
        "learning_rate": 1.4589867601417528e-05,
        "epoch": 0.609583426484835,
        "step": 8180
    },
    {
        "loss": 2.3929,
        "grad_norm": 3.452636480331421,
        "learning_rate": 1.4553291095678767e-05,
        "epoch": 0.6096579476861167,
        "step": 8181
    },
    {
        "loss": 2.3236,
        "grad_norm": 3.2195658683776855,
        "learning_rate": 1.4516756897403416e-05,
        "epoch": 0.6097324688873985,
        "step": 8182
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.9444990158081055,
        "learning_rate": 1.4480265024680706e-05,
        "epoch": 0.6098069900886802,
        "step": 8183
    },
    {
        "loss": 2.5361,
        "grad_norm": 2.0681655406951904,
        "learning_rate": 1.4443815495579094e-05,
        "epoch": 0.609881511289962,
        "step": 8184
    },
    {
        "loss": 2.7125,
        "grad_norm": 2.397024154663086,
        "learning_rate": 1.4407408328145799e-05,
        "epoch": 0.6099560324912437,
        "step": 8185
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.0233585834503174,
        "learning_rate": 1.4371043540407293e-05,
        "epoch": 0.6100305536925256,
        "step": 8186
    },
    {
        "loss": 1.8874,
        "grad_norm": 2.7417562007904053,
        "learning_rate": 1.433472115036898e-05,
        "epoch": 0.6101050748938073,
        "step": 8187
    },
    {
        "loss": 2.4316,
        "grad_norm": 3.130398988723755,
        "learning_rate": 1.4298441176015121e-05,
        "epoch": 0.6101795960950891,
        "step": 8188
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.907197952270508,
        "learning_rate": 1.4262203635309302e-05,
        "epoch": 0.6102541172963708,
        "step": 8189
    },
    {
        "loss": 2.2416,
        "grad_norm": 2.566380739212036,
        "learning_rate": 1.4226008546193769e-05,
        "epoch": 0.6103286384976526,
        "step": 8190
    },
    {
        "loss": 1.9585,
        "grad_norm": 3.058964252471924,
        "learning_rate": 1.4189855926590034e-05,
        "epoch": 0.6104031596989343,
        "step": 8191
    },
    {
        "loss": 2.3178,
        "grad_norm": 3.1933019161224365,
        "learning_rate": 1.4153745794398377e-05,
        "epoch": 0.6104776809002161,
        "step": 8192
    },
    {
        "loss": 2.1642,
        "grad_norm": 3.430961847305298,
        "learning_rate": 1.4117678167498072e-05,
        "epoch": 0.6105522021014979,
        "step": 8193
    },
    {
        "loss": 1.8229,
        "grad_norm": 3.46597957611084,
        "learning_rate": 1.4081653063747446e-05,
        "epoch": 0.6106267233027797,
        "step": 8194
    },
    {
        "loss": 2.2749,
        "grad_norm": 3.242643117904663,
        "learning_rate": 1.4045670500983621e-05,
        "epoch": 0.6107012445040614,
        "step": 8195
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.2925570011138916,
        "learning_rate": 1.4009730497022877e-05,
        "epoch": 0.6107757657053432,
        "step": 8196
    },
    {
        "loss": 2.4331,
        "grad_norm": 2.1513514518737793,
        "learning_rate": 1.3973833069660181e-05,
        "epoch": 0.6108502869066249,
        "step": 8197
    },
    {
        "loss": 1.7731,
        "grad_norm": 1.649666428565979,
        "learning_rate": 1.3937978236669468e-05,
        "epoch": 0.6109248081079067,
        "step": 8198
    },
    {
        "loss": 1.7512,
        "grad_norm": 4.073122978210449,
        "learning_rate": 1.3902166015803808e-05,
        "epoch": 0.6109993293091884,
        "step": 8199
    },
    {
        "loss": 1.6796,
        "grad_norm": 3.7808830738067627,
        "learning_rate": 1.3866396424794847e-05,
        "epoch": 0.6110738505104703,
        "step": 8200
    },
    {
        "loss": 2.394,
        "grad_norm": 2.2640812397003174,
        "learning_rate": 1.383066948135341e-05,
        "epoch": 0.611148371711752,
        "step": 8201
    },
    {
        "loss": 1.4562,
        "grad_norm": 1.788489818572998,
        "learning_rate": 1.3794985203169031e-05,
        "epoch": 0.6112228929130338,
        "step": 8202
    },
    {
        "loss": 2.2863,
        "grad_norm": 3.2891602516174316,
        "learning_rate": 1.3759343607910125e-05,
        "epoch": 0.6112974141143155,
        "step": 8203
    },
    {
        "loss": 2.8794,
        "grad_norm": 3.158029317855835,
        "learning_rate": 1.3723744713224052e-05,
        "epoch": 0.6113719353155973,
        "step": 8204
    },
    {
        "loss": 2.3982,
        "grad_norm": 2.0045127868652344,
        "learning_rate": 1.3688188536736968e-05,
        "epoch": 0.611446456516879,
        "step": 8205
    },
    {
        "loss": 2.6263,
        "grad_norm": 3.0871236324310303,
        "learning_rate": 1.3652675096053935e-05,
        "epoch": 0.6115209777181608,
        "step": 8206
    },
    {
        "loss": 2.9092,
        "grad_norm": 3.324105978012085,
        "learning_rate": 1.3617204408758788e-05,
        "epoch": 0.6115954989194425,
        "step": 8207
    },
    {
        "loss": 1.8458,
        "grad_norm": 3.6615958213806152,
        "learning_rate": 1.3581776492414166e-05,
        "epoch": 0.6116700201207244,
        "step": 8208
    },
    {
        "loss": 2.3932,
        "grad_norm": 3.9790496826171875,
        "learning_rate": 1.3546391364561729e-05,
        "epoch": 0.6117445413220061,
        "step": 8209
    },
    {
        "loss": 2.3207,
        "grad_norm": 3.3211894035339355,
        "learning_rate": 1.351104904272168e-05,
        "epoch": 0.6118190625232879,
        "step": 8210
    },
    {
        "loss": 2.8719,
        "grad_norm": 2.3074300289154053,
        "learning_rate": 1.34757495443933e-05,
        "epoch": 0.6118935837245696,
        "step": 8211
    },
    {
        "loss": 3.0074,
        "grad_norm": 2.688612222671509,
        "learning_rate": 1.3440492887054435e-05,
        "epoch": 0.6119681049258514,
        "step": 8212
    },
    {
        "loss": 2.2981,
        "grad_norm": 2.713580846786499,
        "learning_rate": 1.340527908816177e-05,
        "epoch": 0.6120426261271331,
        "step": 8213
    },
    {
        "loss": 1.8033,
        "grad_norm": 2.8647685050964355,
        "learning_rate": 1.337010816515093e-05,
        "epoch": 0.6121171473284149,
        "step": 8214
    },
    {
        "loss": 1.7937,
        "grad_norm": 3.1824891567230225,
        "learning_rate": 1.3334980135436115e-05,
        "epoch": 0.6121916685296968,
        "step": 8215
    },
    {
        "loss": 2.585,
        "grad_norm": 2.8178768157958984,
        "learning_rate": 1.3299895016410314e-05,
        "epoch": 0.6122661897309785,
        "step": 8216
    },
    {
        "loss": 2.4504,
        "grad_norm": 2.2116665840148926,
        "learning_rate": 1.3264852825445417e-05,
        "epoch": 0.6123407109322603,
        "step": 8217
    },
    {
        "loss": 2.3787,
        "grad_norm": 3.3592803478240967,
        "learning_rate": 1.322985357989186e-05,
        "epoch": 0.612415232133542,
        "step": 8218
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.7680981159210205,
        "learning_rate": 1.3194897297079067e-05,
        "epoch": 0.6124897533348238,
        "step": 8219
    },
    {
        "loss": 2.6212,
        "grad_norm": 1.9706785678863525,
        "learning_rate": 1.3159983994314917e-05,
        "epoch": 0.6125642745361055,
        "step": 8220
    },
    {
        "loss": 2.4265,
        "grad_norm": 3.4618935585021973,
        "learning_rate": 1.3125113688886092e-05,
        "epoch": 0.6126387957373873,
        "step": 8221
    },
    {
        "loss": 2.0005,
        "grad_norm": 2.479947805404663,
        "learning_rate": 1.3090286398058149e-05,
        "epoch": 0.612713316938669,
        "step": 8222
    },
    {
        "loss": 2.1443,
        "grad_norm": 3.5868566036224365,
        "learning_rate": 1.3055502139075149e-05,
        "epoch": 0.6127878381399509,
        "step": 8223
    },
    {
        "loss": 3.0319,
        "grad_norm": 3.370236396789551,
        "learning_rate": 1.3020760929159937e-05,
        "epoch": 0.6128623593412326,
        "step": 8224
    },
    {
        "loss": 2.0894,
        "grad_norm": 1.8668217658996582,
        "learning_rate": 1.2986062785514019e-05,
        "epoch": 0.6129368805425144,
        "step": 8225
    },
    {
        "loss": 2.4301,
        "grad_norm": 1.9422979354858398,
        "learning_rate": 1.2951407725317566e-05,
        "epoch": 0.6130114017437961,
        "step": 8226
    },
    {
        "loss": 1.6398,
        "grad_norm": 3.368192434310913,
        "learning_rate": 1.291679576572945e-05,
        "epoch": 0.6130859229450779,
        "step": 8227
    },
    {
        "loss": 2.4713,
        "grad_norm": 3.5303990840911865,
        "learning_rate": 1.288222692388712e-05,
        "epoch": 0.6131604441463596,
        "step": 8228
    },
    {
        "loss": 2.3219,
        "grad_norm": 3.394707679748535,
        "learning_rate": 1.284770121690687e-05,
        "epoch": 0.6132349653476414,
        "step": 8229
    },
    {
        "loss": 3.1272,
        "grad_norm": 3.113184928894043,
        "learning_rate": 1.2813218661883441e-05,
        "epoch": 0.6133094865489231,
        "step": 8230
    },
    {
        "loss": 2.4242,
        "grad_norm": 2.1056902408599854,
        "learning_rate": 1.2778779275890196e-05,
        "epoch": 0.613384007750205,
        "step": 8231
    },
    {
        "loss": 1.4817,
        "grad_norm": 2.964107036590576,
        "learning_rate": 1.274438307597936e-05,
        "epoch": 0.6134585289514867,
        "step": 8232
    },
    {
        "loss": 2.2047,
        "grad_norm": 3.3821206092834473,
        "learning_rate": 1.2710030079181511e-05,
        "epoch": 0.6135330501527685,
        "step": 8233
    },
    {
        "loss": 1.9997,
        "grad_norm": 3.8233141899108887,
        "learning_rate": 1.2675720302505988e-05,
        "epoch": 0.6136075713540502,
        "step": 8234
    },
    {
        "loss": 2.9708,
        "grad_norm": 2.3025777339935303,
        "learning_rate": 1.2641453762940669e-05,
        "epoch": 0.613682092555332,
        "step": 8235
    },
    {
        "loss": 2.1096,
        "grad_norm": 2.9866135120391846,
        "learning_rate": 1.2607230477452014e-05,
        "epoch": 0.6137566137566137,
        "step": 8236
    },
    {
        "loss": 2.5844,
        "grad_norm": 1.909569501876831,
        "learning_rate": 1.2573050462985136e-05,
        "epoch": 0.6138311349578955,
        "step": 8237
    },
    {
        "loss": 2.0146,
        "grad_norm": 2.556697130203247,
        "learning_rate": 1.2538913736463597e-05,
        "epoch": 0.6139056561591772,
        "step": 8238
    },
    {
        "loss": 2.3487,
        "grad_norm": 2.593907594680786,
        "learning_rate": 1.2504820314789733e-05,
        "epoch": 0.6139801773604591,
        "step": 8239
    },
    {
        "loss": 2.05,
        "grad_norm": 3.6464309692382812,
        "learning_rate": 1.2470770214844251e-05,
        "epoch": 0.6140546985617408,
        "step": 8240
    },
    {
        "loss": 2.0348,
        "grad_norm": 2.804141044616699,
        "learning_rate": 1.24367634534864e-05,
        "epoch": 0.6141292197630226,
        "step": 8241
    },
    {
        "loss": 2.5931,
        "grad_norm": 3.010228157043457,
        "learning_rate": 1.2402800047554208e-05,
        "epoch": 0.6142037409643043,
        "step": 8242
    },
    {
        "loss": 2.4027,
        "grad_norm": 2.2832190990448,
        "learning_rate": 1.2368880013863937e-05,
        "epoch": 0.6142782621655861,
        "step": 8243
    },
    {
        "loss": 1.9623,
        "grad_norm": 4.033799171447754,
        "learning_rate": 1.2335003369210507e-05,
        "epoch": 0.6143527833668678,
        "step": 8244
    },
    {
        "loss": 2.8114,
        "grad_norm": 1.989410400390625,
        "learning_rate": 1.2301170130367445e-05,
        "epoch": 0.6144273045681496,
        "step": 8245
    },
    {
        "loss": 1.6862,
        "grad_norm": 4.783985614776611,
        "learning_rate": 1.2267380314086585e-05,
        "epoch": 0.6145018257694314,
        "step": 8246
    },
    {
        "loss": 1.9117,
        "grad_norm": 3.457549810409546,
        "learning_rate": 1.2233633937098465e-05,
        "epoch": 0.6145763469707132,
        "step": 8247
    },
    {
        "loss": 1.9156,
        "grad_norm": 2.674182176589966,
        "learning_rate": 1.2199931016111998e-05,
        "epoch": 0.6146508681719949,
        "step": 8248
    },
    {
        "loss": 2.8403,
        "grad_norm": 2.216541290283203,
        "learning_rate": 1.2166271567814502e-05,
        "epoch": 0.6147253893732767,
        "step": 8249
    },
    {
        "loss": 1.7478,
        "grad_norm": 2.5758581161499023,
        "learning_rate": 1.2132655608872023e-05,
        "epoch": 0.6147999105745585,
        "step": 8250
    },
    {
        "loss": 2.3406,
        "grad_norm": 2.5886013507843018,
        "learning_rate": 1.2099083155928804e-05,
        "epoch": 0.6148744317758402,
        "step": 8251
    },
    {
        "loss": 1.895,
        "grad_norm": 3.182607412338257,
        "learning_rate": 1.2065554225607767e-05,
        "epoch": 0.614948952977122,
        "step": 8252
    },
    {
        "loss": 0.8777,
        "grad_norm": 2.9970667362213135,
        "learning_rate": 1.2032068834510135e-05,
        "epoch": 0.6150234741784038,
        "step": 8253
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.378575086593628,
        "learning_rate": 1.1998626999215611e-05,
        "epoch": 0.6150979953796856,
        "step": 8254
    },
    {
        "loss": 2.9486,
        "grad_norm": 2.818326234817505,
        "learning_rate": 1.1965228736282375e-05,
        "epoch": 0.6151725165809673,
        "step": 8255
    },
    {
        "loss": 2.0361,
        "grad_norm": 3.01788592338562,
        "learning_rate": 1.1931874062246951e-05,
        "epoch": 0.6152470377822491,
        "step": 8256
    },
    {
        "loss": 3.0212,
        "grad_norm": 3.364345073699951,
        "learning_rate": 1.1898562993624373e-05,
        "epoch": 0.6153215589835308,
        "step": 8257
    },
    {
        "loss": 2.441,
        "grad_norm": 2.5055689811706543,
        "learning_rate": 1.186529554690804e-05,
        "epoch": 0.6153960801848126,
        "step": 8258
    },
    {
        "loss": 2.5787,
        "grad_norm": 2.26263689994812,
        "learning_rate": 1.1832071738569672e-05,
        "epoch": 0.6154706013860943,
        "step": 8259
    },
    {
        "loss": 2.6966,
        "grad_norm": 3.202054977416992,
        "learning_rate": 1.1798891585059602e-05,
        "epoch": 0.6155451225873761,
        "step": 8260
    },
    {
        "loss": 2.6452,
        "grad_norm": 1.9757906198501587,
        "learning_rate": 1.1765755102806275e-05,
        "epoch": 0.6156196437886579,
        "step": 8261
    },
    {
        "loss": 2.2904,
        "grad_norm": 4.820512294769287,
        "learning_rate": 1.1732662308216768e-05,
        "epoch": 0.6156941649899397,
        "step": 8262
    },
    {
        "loss": 3.1193,
        "grad_norm": 3.581580400466919,
        "learning_rate": 1.1699613217676364e-05,
        "epoch": 0.6157686861912214,
        "step": 8263
    },
    {
        "loss": 1.9756,
        "grad_norm": 2.8981525897979736,
        "learning_rate": 1.1666607847548728e-05,
        "epoch": 0.6158432073925032,
        "step": 8264
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.4989449977874756,
        "learning_rate": 1.163364621417592e-05,
        "epoch": 0.6159177285937849,
        "step": 8265
    },
    {
        "loss": 2.9872,
        "grad_norm": 5.120646953582764,
        "learning_rate": 1.1600728333878296e-05,
        "epoch": 0.6159922497950667,
        "step": 8266
    },
    {
        "loss": 2.2378,
        "grad_norm": 3.8101048469543457,
        "learning_rate": 1.1567854222954622e-05,
        "epoch": 0.6160667709963484,
        "step": 8267
    },
    {
        "loss": 2.3372,
        "grad_norm": 3.852965831756592,
        "learning_rate": 1.1535023897681918e-05,
        "epoch": 0.6161412921976303,
        "step": 8268
    },
    {
        "loss": 1.8994,
        "grad_norm": 3.9808387756347656,
        "learning_rate": 1.1502237374315505e-05,
        "epoch": 0.616215813398912,
        "step": 8269
    },
    {
        "loss": 2.5904,
        "grad_norm": 2.8683440685272217,
        "learning_rate": 1.1469494669089165e-05,
        "epoch": 0.6162903346001938,
        "step": 8270
    },
    {
        "loss": 2.4103,
        "grad_norm": 3.249126434326172,
        "learning_rate": 1.1436795798214773e-05,
        "epoch": 0.6163648558014755,
        "step": 8271
    },
    {
        "loss": 2.8564,
        "grad_norm": 4.328906059265137,
        "learning_rate": 1.1404140777882756e-05,
        "epoch": 0.6164393770027573,
        "step": 8272
    },
    {
        "loss": 2.309,
        "grad_norm": 2.292179822921753,
        "learning_rate": 1.1371529624261578e-05,
        "epoch": 0.616513898204039,
        "step": 8273
    },
    {
        "loss": 2.3224,
        "grad_norm": 4.938261032104492,
        "learning_rate": 1.1338962353498073e-05,
        "epoch": 0.6165884194053208,
        "step": 8274
    },
    {
        "loss": 1.8073,
        "grad_norm": 3.4320755004882812,
        "learning_rate": 1.130643898171746e-05,
        "epoch": 0.6166629406066025,
        "step": 8275
    },
    {
        "loss": 2.2697,
        "grad_norm": 3.420923948287964,
        "learning_rate": 1.1273959525023037e-05,
        "epoch": 0.6167374618078844,
        "step": 8276
    },
    {
        "loss": 2.2134,
        "grad_norm": 1.914239525794983,
        "learning_rate": 1.1241523999496439e-05,
        "epoch": 0.6168119830091661,
        "step": 8277
    },
    {
        "loss": 2.4868,
        "grad_norm": 2.334156036376953,
        "learning_rate": 1.1209132421197665e-05,
        "epoch": 0.6168865042104479,
        "step": 8278
    },
    {
        "loss": 1.9838,
        "grad_norm": 2.2834765911102295,
        "learning_rate": 1.1176784806164676e-05,
        "epoch": 0.6169610254117296,
        "step": 8279
    },
    {
        "loss": 1.354,
        "grad_norm": 3.436258554458618,
        "learning_rate": 1.1144481170414e-05,
        "epoch": 0.6170355466130114,
        "step": 8280
    },
    {
        "loss": 1.2975,
        "grad_norm": 3.0903985500335693,
        "learning_rate": 1.1112221529940159e-05,
        "epoch": 0.6171100678142931,
        "step": 8281
    },
    {
        "loss": 2.1911,
        "grad_norm": 3.684751272201538,
        "learning_rate": 1.1080005900715906e-05,
        "epoch": 0.6171845890155749,
        "step": 8282
    },
    {
        "loss": 2.4427,
        "grad_norm": 2.620762348175049,
        "learning_rate": 1.104783429869235e-05,
        "epoch": 0.6172591102168566,
        "step": 8283
    },
    {
        "loss": 2.3986,
        "grad_norm": 2.1499381065368652,
        "learning_rate": 1.1015706739798647e-05,
        "epoch": 0.6173336314181385,
        "step": 8284
    },
    {
        "loss": 2.5943,
        "grad_norm": 2.5837907791137695,
        "learning_rate": 1.098362323994223e-05,
        "epoch": 0.6174081526194202,
        "step": 8285
    },
    {
        "loss": 2.5473,
        "grad_norm": 2.63981032371521,
        "learning_rate": 1.0951583815008682e-05,
        "epoch": 0.617482673820702,
        "step": 8286
    },
    {
        "loss": 1.7032,
        "grad_norm": 4.660862922668457,
        "learning_rate": 1.0919588480861743e-05,
        "epoch": 0.6175571950219838,
        "step": 8287
    },
    {
        "loss": 2.946,
        "grad_norm": 2.8443596363067627,
        "learning_rate": 1.0887637253343386e-05,
        "epoch": 0.6176317162232655,
        "step": 8288
    },
    {
        "loss": 1.8742,
        "grad_norm": 4.461867809295654,
        "learning_rate": 1.0855730148273646e-05,
        "epoch": 0.6177062374245473,
        "step": 8289
    },
    {
        "loss": 2.6747,
        "grad_norm": 2.309844732284546,
        "learning_rate": 1.0823867181450876e-05,
        "epoch": 0.617780758625829,
        "step": 8290
    },
    {
        "loss": 2.4422,
        "grad_norm": 2.2784507274627686,
        "learning_rate": 1.0792048368651431e-05,
        "epoch": 0.6178552798271109,
        "step": 8291
    },
    {
        "loss": 2.591,
        "grad_norm": 3.429976224899292,
        "learning_rate": 1.0760273725629777e-05,
        "epoch": 0.6179298010283926,
        "step": 8292
    },
    {
        "loss": 2.055,
        "grad_norm": 2.9457626342773438,
        "learning_rate": 1.0728543268118706e-05,
        "epoch": 0.6180043222296744,
        "step": 8293
    },
    {
        "loss": 2.5337,
        "grad_norm": 3.1115944385528564,
        "learning_rate": 1.069685701182892e-05,
        "epoch": 0.6180788434309561,
        "step": 8294
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.565225601196289,
        "learning_rate": 1.0665214972449366e-05,
        "epoch": 0.6181533646322379,
        "step": 8295
    },
    {
        "loss": 2.5324,
        "grad_norm": 2.088956356048584,
        "learning_rate": 1.0633617165647048e-05,
        "epoch": 0.6182278858335196,
        "step": 8296
    },
    {
        "loss": 2.615,
        "grad_norm": 2.849100351333618,
        "learning_rate": 1.0602063607067026e-05,
        "epoch": 0.6183024070348014,
        "step": 8297
    },
    {
        "loss": 1.7952,
        "grad_norm": 2.418593406677246,
        "learning_rate": 1.0570554312332558e-05,
        "epoch": 0.6183769282360831,
        "step": 8298
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.6296894550323486,
        "learning_rate": 1.0539089297044846e-05,
        "epoch": 0.618451449437365,
        "step": 8299
    },
    {
        "loss": 1.947,
        "grad_norm": 3.2769641876220703,
        "learning_rate": 1.0507668576783359e-05,
        "epoch": 0.6185259706386467,
        "step": 8300
    },
    {
        "loss": 2.2339,
        "grad_norm": 3.1697428226470947,
        "learning_rate": 1.0476292167105495e-05,
        "epoch": 0.6186004918399285,
        "step": 8301
    },
    {
        "loss": 4.5658,
        "grad_norm": 3.9436841011047363,
        "learning_rate": 1.044496008354665e-05,
        "epoch": 0.6186750130412102,
        "step": 8302
    },
    {
        "loss": 2.0359,
        "grad_norm": 3.2726035118103027,
        "learning_rate": 1.0413672341620483e-05,
        "epoch": 0.618749534242492,
        "step": 8303
    },
    {
        "loss": 3.3785,
        "grad_norm": 2.992384195327759,
        "learning_rate": 1.0382428956818501e-05,
        "epoch": 0.6188240554437737,
        "step": 8304
    },
    {
        "loss": 2.4398,
        "grad_norm": 4.552674293518066,
        "learning_rate": 1.0351229944610407e-05,
        "epoch": 0.6188985766450555,
        "step": 8305
    },
    {
        "loss": 2.3927,
        "grad_norm": 3.9441230297088623,
        "learning_rate": 1.032007532044379e-05,
        "epoch": 0.6189730978463373,
        "step": 8306
    },
    {
        "loss": 2.6811,
        "grad_norm": 2.1713497638702393,
        "learning_rate": 1.0288965099744296e-05,
        "epoch": 0.6190476190476191,
        "step": 8307
    },
    {
        "loss": 1.6181,
        "grad_norm": 3.624607801437378,
        "learning_rate": 1.0257899297915697e-05,
        "epoch": 0.6191221402489008,
        "step": 8308
    },
    {
        "loss": 2.4772,
        "grad_norm": 2.5666909217834473,
        "learning_rate": 1.0226877930339662e-05,
        "epoch": 0.6191966614501826,
        "step": 8309
    },
    {
        "loss": 1.8937,
        "grad_norm": 5.186854839324951,
        "learning_rate": 1.0195901012375786e-05,
        "epoch": 0.6192711826514643,
        "step": 8310
    },
    {
        "loss": 2.4946,
        "grad_norm": 2.0254881381988525,
        "learning_rate": 1.016496855936191e-05,
        "epoch": 0.6193457038527461,
        "step": 8311
    },
    {
        "loss": 2.7895,
        "grad_norm": 2.253199815750122,
        "learning_rate": 1.0134080586613581e-05,
        "epoch": 0.6194202250540278,
        "step": 8312
    },
    {
        "loss": 2.6425,
        "grad_norm": 2.7919514179229736,
        "learning_rate": 1.0103237109424546e-05,
        "epoch": 0.6194947462553096,
        "step": 8313
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.91982102394104,
        "learning_rate": 1.0072438143066376e-05,
        "epoch": 0.6195692674565914,
        "step": 8314
    },
    {
        "loss": 2.4538,
        "grad_norm": 3.67923641204834,
        "learning_rate": 1.004168370278863e-05,
        "epoch": 0.6196437886578732,
        "step": 8315
    },
    {
        "loss": 2.1607,
        "grad_norm": 4.461233615875244,
        "learning_rate": 1.0010973803818879e-05,
        "epoch": 0.6197183098591549,
        "step": 8316
    },
    {
        "loss": 3.1118,
        "grad_norm": 4.244872570037842,
        "learning_rate": 9.980308461362541e-06,
        "epoch": 0.6197928310604367,
        "step": 8317
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.4587562084198,
        "learning_rate": 9.949687690603094e-06,
        "epoch": 0.6198673522617184,
        "step": 8318
    },
    {
        "loss": 2.6218,
        "grad_norm": 2.6508803367614746,
        "learning_rate": 9.919111506701851e-06,
        "epoch": 0.6199418734630002,
        "step": 8319
    },
    {
        "loss": 2.5242,
        "grad_norm": 2.8768534660339355,
        "learning_rate": 9.888579924798036e-06,
        "epoch": 0.6200163946642819,
        "step": 8320
    },
    {
        "loss": 2.8298,
        "grad_norm": 1.3569267988204956,
        "learning_rate": 9.858092960008936e-06,
        "epoch": 0.6200909158655638,
        "step": 8321
    },
    {
        "loss": 1.6107,
        "grad_norm": 3.2565603256225586,
        "learning_rate": 9.82765062742953e-06,
        "epoch": 0.6201654370668456,
        "step": 8322
    },
    {
        "loss": 2.4296,
        "grad_norm": 2.389010190963745,
        "learning_rate": 9.79725294213294e-06,
        "epoch": 0.6202399582681273,
        "step": 8323
    },
    {
        "loss": 2.7123,
        "grad_norm": 2.3897180557250977,
        "learning_rate": 9.766899919170014e-06,
        "epoch": 0.6203144794694091,
        "step": 8324
    },
    {
        "loss": 1.814,
        "grad_norm": 3.464092969894409,
        "learning_rate": 9.736591573569454e-06,
        "epoch": 0.6203890006706908,
        "step": 8325
    },
    {
        "loss": 1.8328,
        "grad_norm": 3.225273609161377,
        "learning_rate": 9.706327920337999e-06,
        "epoch": 0.6204635218719726,
        "step": 8326
    },
    {
        "loss": 2.1416,
        "grad_norm": 3.4269232749938965,
        "learning_rate": 9.67610897446013e-06,
        "epoch": 0.6205380430732543,
        "step": 8327
    },
    {
        "loss": 2.3446,
        "grad_norm": 2.8272368907928467,
        "learning_rate": 9.645934750898267e-06,
        "epoch": 0.6206125642745361,
        "step": 8328
    },
    {
        "loss": 1.8996,
        "grad_norm": 3.158327341079712,
        "learning_rate": 9.615805264592636e-06,
        "epoch": 0.6206870854758179,
        "step": 8329
    },
    {
        "loss": 1.8091,
        "grad_norm": 2.391932725906372,
        "learning_rate": 9.585720530461273e-06,
        "epoch": 0.6207616066770997,
        "step": 8330
    },
    {
        "loss": 2.3994,
        "grad_norm": 2.7832624912261963,
        "learning_rate": 9.555680563400237e-06,
        "epoch": 0.6208361278783814,
        "step": 8331
    },
    {
        "loss": 2.1555,
        "grad_norm": 2.7047953605651855,
        "learning_rate": 9.5256853782832e-06,
        "epoch": 0.6209106490796632,
        "step": 8332
    },
    {
        "loss": 1.6769,
        "grad_norm": 2.9546215534210205,
        "learning_rate": 9.495734989961847e-06,
        "epoch": 0.6209851702809449,
        "step": 8333
    },
    {
        "loss": 2.7618,
        "grad_norm": 2.666078567504883,
        "learning_rate": 9.465829413265536e-06,
        "epoch": 0.6210596914822267,
        "step": 8334
    },
    {
        "loss": 2.3901,
        "grad_norm": 2.798776865005493,
        "learning_rate": 9.435968663001482e-06,
        "epoch": 0.6211342126835084,
        "step": 8335
    },
    {
        "loss": 2.4835,
        "grad_norm": 2.8806326389312744,
        "learning_rate": 9.406152753954789e-06,
        "epoch": 0.6212087338847903,
        "step": 8336
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.607856035232544,
        "learning_rate": 9.376381700888248e-06,
        "epoch": 0.621283255086072,
        "step": 8337
    },
    {
        "loss": 2.4447,
        "grad_norm": 2.885427474975586,
        "learning_rate": 9.346655518542435e-06,
        "epoch": 0.6213577762873538,
        "step": 8338
    },
    {
        "loss": 2.6902,
        "grad_norm": 3.3003242015838623,
        "learning_rate": 9.316974221635855e-06,
        "epoch": 0.6214322974886355,
        "step": 8339
    },
    {
        "loss": 2.3475,
        "grad_norm": 2.3097405433654785,
        "learning_rate": 9.287337824864573e-06,
        "epoch": 0.6215068186899173,
        "step": 8340
    },
    {
        "loss": 1.915,
        "grad_norm": 3.14517879486084,
        "learning_rate": 9.257746342902663e-06,
        "epoch": 0.621581339891199,
        "step": 8341
    },
    {
        "loss": 2.3327,
        "grad_norm": 2.7529854774475098,
        "learning_rate": 9.228199790401771e-06,
        "epoch": 0.6216558610924808,
        "step": 8342
    },
    {
        "loss": 2.674,
        "grad_norm": 2.985924005508423,
        "learning_rate": 9.198698181991317e-06,
        "epoch": 0.6217303822937625,
        "step": 8343
    },
    {
        "loss": 2.1431,
        "grad_norm": 3.0946171283721924,
        "learning_rate": 9.169241532278627e-06,
        "epoch": 0.6218049034950444,
        "step": 8344
    },
    {
        "loss": 2.7882,
        "grad_norm": 1.7095081806182861,
        "learning_rate": 9.13982985584857e-06,
        "epoch": 0.6218794246963261,
        "step": 8345
    },
    {
        "loss": 2.2218,
        "grad_norm": 2.438133716583252,
        "learning_rate": 9.110463167263871e-06,
        "epoch": 0.6219539458976079,
        "step": 8346
    },
    {
        "loss": 2.3417,
        "grad_norm": 3.5109174251556396,
        "learning_rate": 9.081141481064926e-06,
        "epoch": 0.6220284670988896,
        "step": 8347
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.898888349533081,
        "learning_rate": 9.051864811769828e-06,
        "epoch": 0.6221029883001714,
        "step": 8348
    },
    {
        "loss": 2.6258,
        "grad_norm": 2.5101757049560547,
        "learning_rate": 9.022633173874472e-06,
        "epoch": 0.6221775095014531,
        "step": 8349
    },
    {
        "loss": 2.6652,
        "grad_norm": 2.631105422973633,
        "learning_rate": 8.993446581852305e-06,
        "epoch": 0.6222520307027349,
        "step": 8350
    },
    {
        "loss": 2.0544,
        "grad_norm": 4.2013702392578125,
        "learning_rate": 8.964305050154698e-06,
        "epoch": 0.6223265519040166,
        "step": 8351
    },
    {
        "loss": 2.4812,
        "grad_norm": 2.9062461853027344,
        "learning_rate": 8.935208593210498e-06,
        "epoch": 0.6224010731052985,
        "step": 8352
    },
    {
        "loss": 2.7837,
        "grad_norm": 1.8217109441757202,
        "learning_rate": 8.906157225426281e-06,
        "epoch": 0.6224755943065802,
        "step": 8353
    },
    {
        "loss": 1.8058,
        "grad_norm": 3.5984952449798584,
        "learning_rate": 8.87715096118642e-06,
        "epoch": 0.622550115507862,
        "step": 8354
    },
    {
        "loss": 2.2649,
        "grad_norm": 3.540797710418701,
        "learning_rate": 8.848189814852803e-06,
        "epoch": 0.6226246367091437,
        "step": 8355
    },
    {
        "loss": 1.792,
        "grad_norm": 3.4703774452209473,
        "learning_rate": 8.819273800765082e-06,
        "epoch": 0.6226991579104255,
        "step": 8356
    },
    {
        "loss": 2.3431,
        "grad_norm": 2.214730978012085,
        "learning_rate": 8.790402933240493e-06,
        "epoch": 0.6227736791117073,
        "step": 8357
    },
    {
        "loss": 2.4743,
        "grad_norm": 3.156586170196533,
        "learning_rate": 8.761577226573947e-06,
        "epoch": 0.622848200312989,
        "step": 8358
    },
    {
        "loss": 2.1612,
        "grad_norm": 2.8678057193756104,
        "learning_rate": 8.732796695038014e-06,
        "epoch": 0.6229227215142709,
        "step": 8359
    },
    {
        "loss": 1.8777,
        "grad_norm": 2.5260307788848877,
        "learning_rate": 8.704061352882808e-06,
        "epoch": 0.6229972427155526,
        "step": 8360
    },
    {
        "loss": 1.7897,
        "grad_norm": 4.679807186126709,
        "learning_rate": 8.675371214336258e-06,
        "epoch": 0.6230717639168344,
        "step": 8361
    },
    {
        "loss": 2.6823,
        "grad_norm": 2.3889095783233643,
        "learning_rate": 8.646726293603747e-06,
        "epoch": 0.6231462851181161,
        "step": 8362
    },
    {
        "loss": 2.0788,
        "grad_norm": 3.4143779277801514,
        "learning_rate": 8.618126604868226e-06,
        "epoch": 0.6232208063193979,
        "step": 8363
    },
    {
        "loss": 1.8181,
        "grad_norm": 3.3620364665985107,
        "learning_rate": 8.58957216229046e-06,
        "epoch": 0.6232953275206796,
        "step": 8364
    },
    {
        "loss": 2.7402,
        "grad_norm": 3.256237268447876,
        "learning_rate": 8.561062980008593e-06,
        "epoch": 0.6233698487219614,
        "step": 8365
    },
    {
        "loss": 1.7351,
        "grad_norm": 2.8690178394317627,
        "learning_rate": 8.532599072138548e-06,
        "epoch": 0.6234443699232431,
        "step": 8366
    },
    {
        "loss": 2.4937,
        "grad_norm": 3.7829906940460205,
        "learning_rate": 8.50418045277368e-06,
        "epoch": 0.623518891124525,
        "step": 8367
    },
    {
        "loss": 3.0275,
        "grad_norm": 2.062227964401245,
        "learning_rate": 8.475807135984936e-06,
        "epoch": 0.6235934123258067,
        "step": 8368
    },
    {
        "loss": 2.7866,
        "grad_norm": 2.7787420749664307,
        "learning_rate": 8.44747913582099e-06,
        "epoch": 0.6236679335270885,
        "step": 8369
    },
    {
        "loss": 2.2605,
        "grad_norm": 2.5662178993225098,
        "learning_rate": 8.419196466307866e-06,
        "epoch": 0.6237424547283702,
        "step": 8370
    },
    {
        "loss": 2.823,
        "grad_norm": 2.0057144165039062,
        "learning_rate": 8.390959141449228e-06,
        "epoch": 0.623816975929652,
        "step": 8371
    },
    {
        "loss": 2.2441,
        "grad_norm": 2.2955195903778076,
        "learning_rate": 8.362767175226382e-06,
        "epoch": 0.6238914971309337,
        "step": 8372
    },
    {
        "loss": 2.0788,
        "grad_norm": 3.314091920852661,
        "learning_rate": 8.334620581598007e-06,
        "epoch": 0.6239660183322155,
        "step": 8373
    },
    {
        "loss": 2.7975,
        "grad_norm": 1.8547930717468262,
        "learning_rate": 8.3065193745005e-06,
        "epoch": 0.6240405395334973,
        "step": 8374
    },
    {
        "loss": 2.2029,
        "grad_norm": 3.0709211826324463,
        "learning_rate": 8.278463567847638e-06,
        "epoch": 0.6241150607347791,
        "step": 8375
    },
    {
        "loss": 2.7504,
        "grad_norm": 2.1578400135040283,
        "learning_rate": 8.25045317553076e-06,
        "epoch": 0.6241895819360608,
        "step": 8376
    },
    {
        "loss": 1.8425,
        "grad_norm": 2.4814705848693848,
        "learning_rate": 8.222488211418744e-06,
        "epoch": 0.6242641031373426,
        "step": 8377
    },
    {
        "loss": 2.2353,
        "grad_norm": 2.9989535808563232,
        "learning_rate": 8.19456868935794e-06,
        "epoch": 0.6243386243386243,
        "step": 8378
    },
    {
        "loss": 1.7804,
        "grad_norm": 2.413085460662842,
        "learning_rate": 8.166694623172244e-06,
        "epoch": 0.6244131455399061,
        "step": 8379
    },
    {
        "loss": 2.432,
        "grad_norm": 1.9625051021575928,
        "learning_rate": 8.138866026663006e-06,
        "epoch": 0.6244876667411878,
        "step": 8380
    },
    {
        "loss": 2.1276,
        "grad_norm": 3.580857753753662,
        "learning_rate": 8.111082913609025e-06,
        "epoch": 0.6245621879424696,
        "step": 8381
    },
    {
        "loss": 2.0729,
        "grad_norm": 3.842031955718994,
        "learning_rate": 8.083345297766742e-06,
        "epoch": 0.6246367091437514,
        "step": 8382
    },
    {
        "loss": 1.7327,
        "grad_norm": 4.673705577850342,
        "learning_rate": 8.055653192869827e-06,
        "epoch": 0.6247112303450332,
        "step": 8383
    },
    {
        "loss": 1.4594,
        "grad_norm": 3.848262310028076,
        "learning_rate": 8.028006612629679e-06,
        "epoch": 0.6247857515463149,
        "step": 8384
    },
    {
        "loss": 2.1986,
        "grad_norm": 1.5839684009552002,
        "learning_rate": 8.000405570734982e-06,
        "epoch": 0.6248602727475967,
        "step": 8385
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.292877674102783,
        "learning_rate": 7.972850080851845e-06,
        "epoch": 0.6249347939488784,
        "step": 8386
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.049661159515381,
        "learning_rate": 7.94534015662397e-06,
        "epoch": 0.6250093151501602,
        "step": 8387
    },
    {
        "loss": 2.0235,
        "grad_norm": 3.182696580886841,
        "learning_rate": 7.917875811672392e-06,
        "epoch": 0.6250838363514419,
        "step": 8388
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.4841575622558594,
        "learning_rate": 7.890457059595625e-06,
        "epoch": 0.6251583575527238,
        "step": 8389
    },
    {
        "loss": 1.56,
        "grad_norm": 5.105620861053467,
        "learning_rate": 7.863083913969581e-06,
        "epoch": 0.6252328787540055,
        "step": 8390
    },
    {
        "loss": 2.7285,
        "grad_norm": 2.74014949798584,
        "learning_rate": 7.835756388347537e-06,
        "epoch": 0.6253073999552873,
        "step": 8391
    },
    {
        "loss": 1.9979,
        "grad_norm": 3.8667919635772705,
        "learning_rate": 7.808474496260387e-06,
        "epoch": 0.6253819211565691,
        "step": 8392
    },
    {
        "loss": 2.4691,
        "grad_norm": 2.4976906776428223,
        "learning_rate": 7.781238251216138e-06,
        "epoch": 0.6254564423578508,
        "step": 8393
    },
    {
        "loss": 2.0981,
        "grad_norm": 2.406557321548462,
        "learning_rate": 7.75404766670047e-06,
        "epoch": 0.6255309635591326,
        "step": 8394
    },
    {
        "loss": 1.4337,
        "grad_norm": 5.121332168579102,
        "learning_rate": 7.72690275617627e-06,
        "epoch": 0.6256054847604143,
        "step": 8395
    },
    {
        "loss": 2.2862,
        "grad_norm": 3.548976182937622,
        "learning_rate": 7.699803533083872e-06,
        "epoch": 0.6256800059616962,
        "step": 8396
    },
    {
        "loss": 2.1268,
        "grad_norm": 4.509543418884277,
        "learning_rate": 7.672750010841012e-06,
        "epoch": 0.6257545271629779,
        "step": 8397
    },
    {
        "loss": 2.4681,
        "grad_norm": 3.942955255508423,
        "learning_rate": 7.645742202842732e-06,
        "epoch": 0.6258290483642597,
        "step": 8398
    },
    {
        "loss": 2.2864,
        "grad_norm": 3.460533380508423,
        "learning_rate": 7.618780122461533e-06,
        "epoch": 0.6259035695655414,
        "step": 8399
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.3213796615600586,
        "learning_rate": 7.591863783047226e-06,
        "epoch": 0.6259780907668232,
        "step": 8400
    },
    {
        "loss": 2.0165,
        "grad_norm": 3.1438405513763428,
        "learning_rate": 7.564993197926884e-06,
        "epoch": 0.6260526119681049,
        "step": 8401
    },
    {
        "loss": 1.8675,
        "grad_norm": 5.13189172744751,
        "learning_rate": 7.53816838040512e-06,
        "epoch": 0.6261271331693867,
        "step": 8402
    },
    {
        "loss": 1.1065,
        "grad_norm": 1.5384337902069092,
        "learning_rate": 7.511389343763741e-06,
        "epoch": 0.6262016543706684,
        "step": 8403
    },
    {
        "loss": 2.7209,
        "grad_norm": 2.9086687564849854,
        "learning_rate": 7.484656101261878e-06,
        "epoch": 0.6262761755719503,
        "step": 8404
    },
    {
        "loss": 2.3786,
        "grad_norm": 3.505321502685547,
        "learning_rate": 7.457968666136117e-06,
        "epoch": 0.626350696773232,
        "step": 8405
    },
    {
        "loss": 1.3034,
        "grad_norm": 2.3553626537323,
        "learning_rate": 7.431327051600223e-06,
        "epoch": 0.6264252179745138,
        "step": 8406
    },
    {
        "loss": 2.4953,
        "grad_norm": 3.2692253589630127,
        "learning_rate": 7.4047312708453666e-06,
        "epoch": 0.6264997391757955,
        "step": 8407
    },
    {
        "loss": 2.4539,
        "grad_norm": 3.0638558864593506,
        "learning_rate": 7.37818133703998e-06,
        "epoch": 0.6265742603770773,
        "step": 8408
    },
    {
        "loss": 2.6508,
        "grad_norm": 2.768491744995117,
        "learning_rate": 7.351677263329782e-06,
        "epoch": 0.626648781578359,
        "step": 8409
    },
    {
        "loss": 2.4702,
        "grad_norm": 5.929802417755127,
        "learning_rate": 7.32521906283783e-06,
        "epoch": 0.6267233027796408,
        "step": 8410
    },
    {
        "loss": 2.3774,
        "grad_norm": 3.0161123275756836,
        "learning_rate": 7.2988067486644e-06,
        "epoch": 0.6267978239809225,
        "step": 8411
    },
    {
        "loss": 2.3881,
        "grad_norm": 2.4898414611816406,
        "learning_rate": 7.272440333887176e-06,
        "epoch": 0.6268723451822044,
        "step": 8412
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.5883078575134277,
        "learning_rate": 7.246119831560983e-06,
        "epoch": 0.6269468663834861,
        "step": 8413
    },
    {
        "loss": 2.5329,
        "grad_norm": 2.7312369346618652,
        "learning_rate": 7.219845254717927e-06,
        "epoch": 0.6270213875847679,
        "step": 8414
    },
    {
        "loss": 2.5085,
        "grad_norm": 3.5046145915985107,
        "learning_rate": 7.193616616367516e-06,
        "epoch": 0.6270959087860496,
        "step": 8415
    },
    {
        "loss": 2.4,
        "grad_norm": 2.645101308822632,
        "learning_rate": 7.167433929496304e-06,
        "epoch": 0.6271704299873314,
        "step": 8416
    },
    {
        "loss": 2.5402,
        "grad_norm": 2.233201265335083,
        "learning_rate": 7.141297207068265e-06,
        "epoch": 0.6272449511886131,
        "step": 8417
    },
    {
        "loss": 1.6435,
        "grad_norm": 3.443678617477417,
        "learning_rate": 7.115206462024538e-06,
        "epoch": 0.6273194723898949,
        "step": 8418
    },
    {
        "loss": 1.464,
        "grad_norm": 4.550448894500732,
        "learning_rate": 7.0891617072834426e-06,
        "epoch": 0.6273939935911766,
        "step": 8419
    },
    {
        "loss": 1.978,
        "grad_norm": 3.033565044403076,
        "learning_rate": 7.063162955740665e-06,
        "epoch": 0.6274685147924585,
        "step": 8420
    },
    {
        "loss": 2.7387,
        "grad_norm": 2.0089383125305176,
        "learning_rate": 7.037210220268953e-06,
        "epoch": 0.6275430359937402,
        "step": 8421
    },
    {
        "loss": 2.4479,
        "grad_norm": 3.118300676345825,
        "learning_rate": 7.011303513718481e-06,
        "epoch": 0.627617557195022,
        "step": 8422
    },
    {
        "loss": 2.171,
        "grad_norm": 2.438927412033081,
        "learning_rate": 6.985442848916412e-06,
        "epoch": 0.6276920783963037,
        "step": 8423
    },
    {
        "loss": 2.0303,
        "grad_norm": 3.8616857528686523,
        "learning_rate": 6.959628238667193e-06,
        "epoch": 0.6277665995975855,
        "step": 8424
    },
    {
        "loss": 2.297,
        "grad_norm": 2.3593106269836426,
        "learning_rate": 6.933859695752565e-06,
        "epoch": 0.6278411207988672,
        "step": 8425
    },
    {
        "loss": 1.8599,
        "grad_norm": 3.6845273971557617,
        "learning_rate": 6.9081372329312995e-06,
        "epoch": 0.627915642000149,
        "step": 8426
    },
    {
        "loss": 2.8759,
        "grad_norm": 4.1224188804626465,
        "learning_rate": 6.882460862939522e-06,
        "epoch": 0.6279901632014309,
        "step": 8427
    },
    {
        "loss": 2.4739,
        "grad_norm": 2.1365678310394287,
        "learning_rate": 6.8568305984903715e-06,
        "epoch": 0.6280646844027126,
        "step": 8428
    },
    {
        "loss": 2.518,
        "grad_norm": 2.046792984008789,
        "learning_rate": 6.831246452274209e-06,
        "epoch": 0.6281392056039944,
        "step": 8429
    },
    {
        "loss": 2.4425,
        "grad_norm": 2.8832192420959473,
        "learning_rate": 6.805708436958669e-06,
        "epoch": 0.6282137268052761,
        "step": 8430
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.531066417694092,
        "learning_rate": 6.780216565188402e-06,
        "epoch": 0.6282882480065579,
        "step": 8431
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.333031415939331,
        "learning_rate": 6.754770849585223e-06,
        "epoch": 0.6283627692078396,
        "step": 8432
    },
    {
        "loss": 2.0973,
        "grad_norm": 2.622523784637451,
        "learning_rate": 6.729371302748244e-06,
        "epoch": 0.6284372904091214,
        "step": 8433
    },
    {
        "loss": 1.9135,
        "grad_norm": 2.764709949493408,
        "learning_rate": 6.704017937253504e-06,
        "epoch": 0.6285118116104031,
        "step": 8434
    },
    {
        "loss": 2.265,
        "grad_norm": 3.652207612991333,
        "learning_rate": 6.678710765654406e-06,
        "epoch": 0.628586332811685,
        "step": 8435
    },
    {
        "loss": 2.8186,
        "grad_norm": 1.5671519041061401,
        "learning_rate": 6.6534498004812815e-06,
        "epoch": 0.6286608540129667,
        "step": 8436
    },
    {
        "loss": 2.2832,
        "grad_norm": 4.60081672668457,
        "learning_rate": 6.628235054241672e-06,
        "epoch": 0.6287353752142485,
        "step": 8437
    },
    {
        "loss": 1.881,
        "grad_norm": 2.830069065093994,
        "learning_rate": 6.603066539420233e-06,
        "epoch": 0.6288098964155302,
        "step": 8438
    },
    {
        "loss": 2.2821,
        "grad_norm": 3.187258720397949,
        "learning_rate": 6.5779442684787085e-06,
        "epoch": 0.628884417616812,
        "step": 8439
    },
    {
        "loss": 1.7266,
        "grad_norm": 2.6082937717437744,
        "learning_rate": 6.55286825385597e-06,
        "epoch": 0.6289589388180937,
        "step": 8440
    },
    {
        "loss": 2.8259,
        "grad_norm": 2.2362468242645264,
        "learning_rate": 6.527838507967976e-06,
        "epoch": 0.6290334600193755,
        "step": 8441
    },
    {
        "loss": 2.4875,
        "grad_norm": 3.4914298057556152,
        "learning_rate": 6.502855043207712e-06,
        "epoch": 0.6291079812206573,
        "step": 8442
    },
    {
        "loss": 2.8449,
        "grad_norm": 2.5296547412872314,
        "learning_rate": 6.477917871945416e-06,
        "epoch": 0.6291825024219391,
        "step": 8443
    },
    {
        "loss": 2.3295,
        "grad_norm": 1.7315622568130493,
        "learning_rate": 6.453027006528189e-06,
        "epoch": 0.6292570236232208,
        "step": 8444
    },
    {
        "loss": 2.9891,
        "grad_norm": 3.261720895767212,
        "learning_rate": 6.4281824592804295e-06,
        "epoch": 0.6293315448245026,
        "step": 8445
    },
    {
        "loss": 2.6715,
        "grad_norm": 2.0580947399139404,
        "learning_rate": 6.4033842425034186e-06,
        "epoch": 0.6294060660257843,
        "step": 8446
    },
    {
        "loss": 2.3755,
        "grad_norm": 3.119612455368042,
        "learning_rate": 6.378632368475534e-06,
        "epoch": 0.6294805872270661,
        "step": 8447
    },
    {
        "loss": 1.6736,
        "grad_norm": 4.273989200592041,
        "learning_rate": 6.353926849452308e-06,
        "epoch": 0.6295551084283478,
        "step": 8448
    },
    {
        "loss": 2.625,
        "grad_norm": 1.904651165008545,
        "learning_rate": 6.329267697666208e-06,
        "epoch": 0.6296296296296297,
        "step": 8449
    },
    {
        "loss": 2.3871,
        "grad_norm": 2.8473007678985596,
        "learning_rate": 6.304654925326814e-06,
        "epoch": 0.6297041508309114,
        "step": 8450
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.980356216430664,
        "learning_rate": 6.28008854462071e-06,
        "epoch": 0.6297786720321932,
        "step": 8451
    },
    {
        "loss": 1.8197,
        "grad_norm": 5.04645299911499,
        "learning_rate": 6.255568567711456e-06,
        "epoch": 0.6298531932334749,
        "step": 8452
    },
    {
        "loss": 2.6742,
        "grad_norm": 3.5925402641296387,
        "learning_rate": 6.2310950067398064e-06,
        "epoch": 0.6299277144347567,
        "step": 8453
    },
    {
        "loss": 1.1586,
        "grad_norm": 3.4503870010375977,
        "learning_rate": 6.206667873823302e-06,
        "epoch": 0.6300022356360384,
        "step": 8454
    },
    {
        "loss": 2.1877,
        "grad_norm": 3.0763869285583496,
        "learning_rate": 6.1822871810567225e-06,
        "epoch": 0.6300767568373202,
        "step": 8455
    },
    {
        "loss": 2.5675,
        "grad_norm": 2.7452175617218018,
        "learning_rate": 6.157952940511702e-06,
        "epoch": 0.6301512780386019,
        "step": 8456
    },
    {
        "loss": 2.4766,
        "grad_norm": 2.7225399017333984,
        "learning_rate": 6.133665164236912e-06,
        "epoch": 0.6302257992398838,
        "step": 8457
    },
    {
        "loss": 1.8428,
        "grad_norm": 2.9411368370056152,
        "learning_rate": 6.109423864258046e-06,
        "epoch": 0.6303003204411655,
        "step": 8458
    },
    {
        "loss": 2.6975,
        "grad_norm": 2.659299850463867,
        "learning_rate": 6.085229052577713e-06,
        "epoch": 0.6303748416424473,
        "step": 8459
    },
    {
        "loss": 2.1561,
        "grad_norm": 3.3314049243927,
        "learning_rate": 6.061080741175651e-06,
        "epoch": 0.630449362843729,
        "step": 8460
    },
    {
        "loss": 1.6673,
        "grad_norm": 2.356973886489868,
        "learning_rate": 6.0369789420084085e-06,
        "epoch": 0.6305238840450108,
        "step": 8461
    },
    {
        "loss": 2.0432,
        "grad_norm": 3.7905008792877197,
        "learning_rate": 6.01292366700954e-06,
        "epoch": 0.6305984052462925,
        "step": 8462
    },
    {
        "loss": 1.7478,
        "grad_norm": 2.9560768604278564,
        "learning_rate": 5.988914928089706e-06,
        "epoch": 0.6306729264475743,
        "step": 8463
    },
    {
        "loss": 2.7775,
        "grad_norm": 2.9149770736694336,
        "learning_rate": 5.964952737136364e-06,
        "epoch": 0.6307474476488562,
        "step": 8464
    },
    {
        "loss": 2.4018,
        "grad_norm": 3.048251152038574,
        "learning_rate": 5.9410371060139355e-06,
        "epoch": 0.6308219688501379,
        "step": 8465
    },
    {
        "loss": 2.4974,
        "grad_norm": 2.0424187183380127,
        "learning_rate": 5.917168046563915e-06,
        "epoch": 0.6308964900514197,
        "step": 8466
    },
    {
        "loss": 3.1758,
        "grad_norm": 2.197432041168213,
        "learning_rate": 5.893345570604614e-06,
        "epoch": 0.6309710112527014,
        "step": 8467
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.752579689025879,
        "learning_rate": 5.8695696899313315e-06,
        "epoch": 0.6310455324539832,
        "step": 8468
    },
    {
        "loss": 2.345,
        "grad_norm": 3.2040934562683105,
        "learning_rate": 5.845840416316284e-06,
        "epoch": 0.6311200536552649,
        "step": 8469
    },
    {
        "loss": 2.2595,
        "grad_norm": 3.1194608211517334,
        "learning_rate": 5.822157761508584e-06,
        "epoch": 0.6311945748565467,
        "step": 8470
    },
    {
        "loss": 2.2508,
        "grad_norm": 1.6674350500106812,
        "learning_rate": 5.79852173723433e-06,
        "epoch": 0.6312690960578284,
        "step": 8471
    },
    {
        "loss": 2.3626,
        "grad_norm": 2.8078575134277344,
        "learning_rate": 5.774932355196427e-06,
        "epoch": 0.6313436172591103,
        "step": 8472
    },
    {
        "loss": 2.2514,
        "grad_norm": 2.161111831665039,
        "learning_rate": 5.751389627074832e-06,
        "epoch": 0.631418138460392,
        "step": 8473
    },
    {
        "loss": 2.8519,
        "grad_norm": 2.705841302871704,
        "learning_rate": 5.727893564526299e-06,
        "epoch": 0.6314926596616738,
        "step": 8474
    },
    {
        "loss": 2.2731,
        "grad_norm": 3.633859634399414,
        "learning_rate": 5.7044441791844315e-06,
        "epoch": 0.6315671808629555,
        "step": 8475
    },
    {
        "loss": 2.0906,
        "grad_norm": 3.2319092750549316,
        "learning_rate": 5.681041482659899e-06,
        "epoch": 0.6316417020642373,
        "step": 8476
    },
    {
        "loss": 2.2234,
        "grad_norm": 3.41200852394104,
        "learning_rate": 5.657685486540032e-06,
        "epoch": 0.631716223265519,
        "step": 8477
    },
    {
        "loss": 2.3602,
        "grad_norm": 2.6620829105377197,
        "learning_rate": 5.634376202389236e-06,
        "epoch": 0.6317907444668008,
        "step": 8478
    },
    {
        "loss": 2.2731,
        "grad_norm": 4.704139709472656,
        "learning_rate": 5.611113641748688e-06,
        "epoch": 0.6318652656680825,
        "step": 8479
    },
    {
        "loss": 2.4912,
        "grad_norm": 3.4572293758392334,
        "learning_rate": 5.587897816136367e-06,
        "epoch": 0.6319397868693644,
        "step": 8480
    },
    {
        "loss": 2.1931,
        "grad_norm": 4.274931907653809,
        "learning_rate": 5.564728737047275e-06,
        "epoch": 0.6320143080706461,
        "step": 8481
    },
    {
        "loss": 2.6911,
        "grad_norm": 4.031264305114746,
        "learning_rate": 5.541606415953093e-06,
        "epoch": 0.6320888292719279,
        "step": 8482
    },
    {
        "loss": 2.4161,
        "grad_norm": 1.7805416584014893,
        "learning_rate": 5.518530864302574e-06,
        "epoch": 0.6321633504732096,
        "step": 8483
    },
    {
        "loss": 2.8095,
        "grad_norm": 2.226243495941162,
        "learning_rate": 5.495502093521066e-06,
        "epoch": 0.6322378716744914,
        "step": 8484
    },
    {
        "loss": 2.4451,
        "grad_norm": 2.636896848678589,
        "learning_rate": 5.472520115010871e-06,
        "epoch": 0.6323123928757731,
        "step": 8485
    },
    {
        "loss": 2.5257,
        "grad_norm": 2.934394598007202,
        "learning_rate": 5.449584940151186e-06,
        "epoch": 0.6323869140770549,
        "step": 8486
    },
    {
        "loss": 1.6359,
        "grad_norm": 3.842114210128784,
        "learning_rate": 5.426696580297929e-06,
        "epoch": 0.6324614352783366,
        "step": 8487
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.271467685699463,
        "learning_rate": 5.403855046783879e-06,
        "epoch": 0.6325359564796185,
        "step": 8488
    },
    {
        "loss": 2.2233,
        "grad_norm": 2.4042043685913086,
        "learning_rate": 5.381060350918632e-06,
        "epoch": 0.6326104776809002,
        "step": 8489
    },
    {
        "loss": 2.0204,
        "grad_norm": 1.661042332649231,
        "learning_rate": 5.3583125039885715e-06,
        "epoch": 0.632684998882182,
        "step": 8490
    },
    {
        "loss": 2.7445,
        "grad_norm": 2.2630317211151123,
        "learning_rate": 5.335611517256922e-06,
        "epoch": 0.6327595200834637,
        "step": 8491
    },
    {
        "loss": 2.2813,
        "grad_norm": 3.1446423530578613,
        "learning_rate": 5.312957401963636e-06,
        "epoch": 0.6328340412847455,
        "step": 8492
    },
    {
        "loss": 1.4616,
        "grad_norm": 4.296051979064941,
        "learning_rate": 5.290350169325586e-06,
        "epoch": 0.6329085624860272,
        "step": 8493
    },
    {
        "loss": 2.2927,
        "grad_norm": 1.3952981233596802,
        "learning_rate": 5.267789830536318e-06,
        "epoch": 0.632983083687309,
        "step": 8494
    },
    {
        "loss": 2.5627,
        "grad_norm": 2.0045950412750244,
        "learning_rate": 5.245276396766152e-06,
        "epoch": 0.6330576048885908,
        "step": 8495
    },
    {
        "loss": 2.3562,
        "grad_norm": 3.3363351821899414,
        "learning_rate": 5.222809879162305e-06,
        "epoch": 0.6331321260898726,
        "step": 8496
    },
    {
        "loss": 2.4449,
        "grad_norm": 2.136413335800171,
        "learning_rate": 5.2003902888486446e-06,
        "epoch": 0.6332066472911543,
        "step": 8497
    },
    {
        "loss": 2.2187,
        "grad_norm": 2.629763126373291,
        "learning_rate": 5.178017636925836e-06,
        "epoch": 0.6332811684924361,
        "step": 8498
    },
    {
        "loss": 1.8346,
        "grad_norm": 3.0833325386047363,
        "learning_rate": 5.155691934471329e-06,
        "epoch": 0.6333556896937179,
        "step": 8499
    },
    {
        "loss": 2.6592,
        "grad_norm": 2.8306450843811035,
        "learning_rate": 5.1334131925392934e-06,
        "epoch": 0.6334302108949996,
        "step": 8500
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.129096031188965,
        "learning_rate": 5.111181422160682e-06,
        "epoch": 0.6335047320962814,
        "step": 8501
    },
    {
        "loss": 1.9602,
        "grad_norm": 3.697601795196533,
        "learning_rate": 5.088996634343168e-06,
        "epoch": 0.6335792532975631,
        "step": 8502
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.1662936210632324,
        "learning_rate": 5.066858840071121e-06,
        "epoch": 0.633653774498845,
        "step": 8503
    },
    {
        "loss": 2.3288,
        "grad_norm": 2.0917553901672363,
        "learning_rate": 5.044768050305759e-06,
        "epoch": 0.6337282957001267,
        "step": 8504
    },
    {
        "loss": 2.0805,
        "grad_norm": 3.146595001220703,
        "learning_rate": 5.022724275984891e-06,
        "epoch": 0.6338028169014085,
        "step": 8505
    },
    {
        "loss": 2.3288,
        "grad_norm": 2.137735366821289,
        "learning_rate": 5.000727528023197e-06,
        "epoch": 0.6338773381026902,
        "step": 8506
    },
    {
        "loss": 2.2156,
        "grad_norm": 3.463212728500366,
        "learning_rate": 4.978777817311919e-06,
        "epoch": 0.633951859303972,
        "step": 8507
    },
    {
        "loss": 2.5191,
        "grad_norm": 2.3343892097473145,
        "learning_rate": 4.956875154719076e-06,
        "epoch": 0.6340263805052537,
        "step": 8508
    },
    {
        "loss": 2.4376,
        "grad_norm": 4.55594539642334,
        "learning_rate": 4.935019551089437e-06,
        "epoch": 0.6341009017065355,
        "step": 8509
    },
    {
        "loss": 2.474,
        "grad_norm": 2.1160717010498047,
        "learning_rate": 4.913211017244368e-06,
        "epoch": 0.6341754229078173,
        "step": 8510
    },
    {
        "loss": 1.3781,
        "grad_norm": 4.111509323120117,
        "learning_rate": 4.891449563982053e-06,
        "epoch": 0.6342499441090991,
        "step": 8511
    },
    {
        "loss": 2.6467,
        "grad_norm": 2.8530335426330566,
        "learning_rate": 4.8697352020772635e-06,
        "epoch": 0.6343244653103808,
        "step": 8512
    },
    {
        "loss": 2.535,
        "grad_norm": 2.494126558303833,
        "learning_rate": 4.848067942281464e-06,
        "epoch": 0.6343989865116626,
        "step": 8513
    },
    {
        "loss": 1.8873,
        "grad_norm": 5.352022647857666,
        "learning_rate": 4.826447795322897e-06,
        "epoch": 0.6344735077129443,
        "step": 8514
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.703990936279297,
        "learning_rate": 4.80487477190631e-06,
        "epoch": 0.6345480289142261,
        "step": 8515
    },
    {
        "loss": 2.4323,
        "grad_norm": 3.170536518096924,
        "learning_rate": 4.783348882713312e-06,
        "epoch": 0.6346225501155078,
        "step": 8516
    },
    {
        "loss": 2.5041,
        "grad_norm": 4.270558834075928,
        "learning_rate": 4.761870138402025e-06,
        "epoch": 0.6346970713167897,
        "step": 8517
    },
    {
        "loss": 2.3409,
        "grad_norm": 2.412087917327881,
        "learning_rate": 4.740438549607285e-06,
        "epoch": 0.6347715925180714,
        "step": 8518
    },
    {
        "loss": 2.3588,
        "grad_norm": 2.549083948135376,
        "learning_rate": 4.719054126940581e-06,
        "epoch": 0.6348461137193532,
        "step": 8519
    },
    {
        "loss": 2.7012,
        "grad_norm": 1.9463070631027222,
        "learning_rate": 4.69771688099e-06,
        "epoch": 0.6349206349206349,
        "step": 8520
    },
    {
        "loss": 2.1052,
        "grad_norm": 2.2290759086608887,
        "learning_rate": 4.67642682232039e-06,
        "epoch": 0.6349951561219167,
        "step": 8521
    },
    {
        "loss": 2.4698,
        "grad_norm": 3.26188588142395,
        "learning_rate": 4.655183961473086e-06,
        "epoch": 0.6350696773231984,
        "step": 8522
    },
    {
        "loss": 3.171,
        "grad_norm": 2.880622386932373,
        "learning_rate": 4.633988308966119e-06,
        "epoch": 0.6351441985244802,
        "step": 8523
    },
    {
        "loss": 1.2533,
        "grad_norm": 2.0287623405456543,
        "learning_rate": 4.612839875294217e-06,
        "epoch": 0.6352187197257619,
        "step": 8524
    },
    {
        "loss": 2.8384,
        "grad_norm": 3.207583427429199,
        "learning_rate": 4.5917386709286246e-06,
        "epoch": 0.6352932409270438,
        "step": 8525
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.573190927505493,
        "learning_rate": 4.570684706317196e-06,
        "epoch": 0.6353677621283255,
        "step": 8526
    },
    {
        "loss": 2.1573,
        "grad_norm": 4.021347522735596,
        "learning_rate": 4.549677991884527e-06,
        "epoch": 0.6354422833296073,
        "step": 8527
    },
    {
        "loss": 2.2701,
        "grad_norm": 3.10606050491333,
        "learning_rate": 4.528718538031674e-06,
        "epoch": 0.635516804530889,
        "step": 8528
    },
    {
        "loss": 1.3654,
        "grad_norm": 3.9590749740600586,
        "learning_rate": 4.507806355136368e-06,
        "epoch": 0.6355913257321708,
        "step": 8529
    },
    {
        "loss": 3.0147,
        "grad_norm": 2.7780659198760986,
        "learning_rate": 4.486941453552906e-06,
        "epoch": 0.6356658469334525,
        "step": 8530
    },
    {
        "loss": 2.5885,
        "grad_norm": 3.162306070327759,
        "learning_rate": 4.466123843612169e-06,
        "epoch": 0.6357403681347343,
        "step": 8531
    },
    {
        "loss": 2.6713,
        "grad_norm": 3.8350069522857666,
        "learning_rate": 4.445353535621666e-06,
        "epoch": 0.635814889336016,
        "step": 8532
    },
    {
        "loss": 2.093,
        "grad_norm": 3.746439218521118,
        "learning_rate": 4.424630539865415e-06,
        "epoch": 0.6358894105372979,
        "step": 8533
    },
    {
        "loss": 2.4909,
        "grad_norm": 3.5188820362091064,
        "learning_rate": 4.40395486660411e-06,
        "epoch": 0.6359639317385797,
        "step": 8534
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.465118169784546,
        "learning_rate": 4.383326526074927e-06,
        "epoch": 0.6360384529398614,
        "step": 8535
    },
    {
        "loss": 2.6438,
        "grad_norm": 2.7478058338165283,
        "learning_rate": 4.3627455284915855e-06,
        "epoch": 0.6361129741411432,
        "step": 8536
    },
    {
        "loss": 2.4423,
        "grad_norm": 2.7607173919677734,
        "learning_rate": 4.34221188404449e-06,
        "epoch": 0.6361874953424249,
        "step": 8537
    },
    {
        "loss": 2.0677,
        "grad_norm": 2.576748847961426,
        "learning_rate": 4.321725602900461e-06,
        "epoch": 0.6362620165437067,
        "step": 8538
    },
    {
        "loss": 1.5947,
        "grad_norm": 3.459855318069458,
        "learning_rate": 4.301286695202977e-06,
        "epoch": 0.6363365377449884,
        "step": 8539
    },
    {
        "loss": 2.1323,
        "grad_norm": 1.7232725620269775,
        "learning_rate": 4.2808951710719854e-06,
        "epoch": 0.6364110589462703,
        "step": 8540
    },
    {
        "loss": 2.0886,
        "grad_norm": 4.416935920715332,
        "learning_rate": 4.2605510406039664e-06,
        "epoch": 0.636485580147552,
        "step": 8541
    },
    {
        "loss": 2.516,
        "grad_norm": 2.214313507080078,
        "learning_rate": 4.240254313872028e-06,
        "epoch": 0.6365601013488338,
        "step": 8542
    },
    {
        "loss": 2.5235,
        "grad_norm": 1.8594906330108643,
        "learning_rate": 4.2200050009256535e-06,
        "epoch": 0.6366346225501155,
        "step": 8543
    },
    {
        "loss": 1.7387,
        "grad_norm": 4.3440165519714355,
        "learning_rate": 4.199803111791068e-06,
        "epoch": 0.6367091437513973,
        "step": 8544
    },
    {
        "loss": 2.8543,
        "grad_norm": 2.2557740211486816,
        "learning_rate": 4.179648656470791e-06,
        "epoch": 0.636783664952679,
        "step": 8545
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.849034309387207,
        "learning_rate": 4.1595416449439424e-06,
        "epoch": 0.6368581861539608,
        "step": 8546
    },
    {
        "loss": 2.7354,
        "grad_norm": 2.129621982574463,
        "learning_rate": 4.139482087166246e-06,
        "epoch": 0.6369327073552425,
        "step": 8547
    },
    {
        "loss": 1.5686,
        "grad_norm": 2.158731460571289,
        "learning_rate": 4.11946999306978e-06,
        "epoch": 0.6370072285565244,
        "step": 8548
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.901343584060669,
        "learning_rate": 4.0995053725632285e-06,
        "epoch": 0.6370817497578061,
        "step": 8549
    },
    {
        "loss": 2.6353,
        "grad_norm": 3.1145272254943848,
        "learning_rate": 4.079588235531706e-06,
        "epoch": 0.6371562709590879,
        "step": 8550
    },
    {
        "loss": 2.815,
        "grad_norm": 2.0230255126953125,
        "learning_rate": 4.059718591836825e-06,
        "epoch": 0.6372307921603696,
        "step": 8551
    },
    {
        "loss": 2.3747,
        "grad_norm": 3.7493481636047363,
        "learning_rate": 4.039896451316716e-06,
        "epoch": 0.6373053133616514,
        "step": 8552
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.7472410202026367,
        "learning_rate": 4.020121823785938e-06,
        "epoch": 0.6373798345629331,
        "step": 8553
    },
    {
        "loss": 2.0309,
        "grad_norm": 2.900075674057007,
        "learning_rate": 4.000394719035627e-06,
        "epoch": 0.6374543557642149,
        "step": 8554
    },
    {
        "loss": 2.3026,
        "grad_norm": 3.1593406200408936,
        "learning_rate": 3.980715146833269e-06,
        "epoch": 0.6375288769654966,
        "step": 8555
    },
    {
        "loss": 3.0461,
        "grad_norm": 3.3969860076904297,
        "learning_rate": 3.961083116922837e-06,
        "epoch": 0.6376033981667785,
        "step": 8556
    },
    {
        "loss": 2.673,
        "grad_norm": 3.124089241027832,
        "learning_rate": 3.941498639024866e-06,
        "epoch": 0.6376779193680602,
        "step": 8557
    },
    {
        "loss": 1.8943,
        "grad_norm": 3.749936819076538,
        "learning_rate": 3.921961722836242e-06,
        "epoch": 0.637752440569342,
        "step": 8558
    },
    {
        "loss": 2.246,
        "grad_norm": 3.3163676261901855,
        "learning_rate": 3.902472378030286e-06,
        "epoch": 0.6378269617706237,
        "step": 8559
    },
    {
        "loss": 2.6089,
        "grad_norm": 1.6537964344024658,
        "learning_rate": 3.883030614256866e-06,
        "epoch": 0.6379014829719055,
        "step": 8560
    },
    {
        "loss": 2.3208,
        "grad_norm": 3.057961940765381,
        "learning_rate": 3.863636441142215e-06,
        "epoch": 0.6379760041731872,
        "step": 8561
    },
    {
        "loss": 1.9856,
        "grad_norm": 2.9169955253601074,
        "learning_rate": 3.844289868289041e-06,
        "epoch": 0.638050525374469,
        "step": 8562
    },
    {
        "loss": 1.9823,
        "grad_norm": 3.0846476554870605,
        "learning_rate": 3.824990905276438e-06,
        "epoch": 0.6381250465757508,
        "step": 8563
    },
    {
        "loss": 2.692,
        "grad_norm": 1.9081467390060425,
        "learning_rate": 3.805739561659938e-06,
        "epoch": 0.6381995677770326,
        "step": 8564
    },
    {
        "loss": 2.3099,
        "grad_norm": 3.496196746826172,
        "learning_rate": 3.786535846971573e-06,
        "epoch": 0.6382740889783143,
        "step": 8565
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.1799895763397217,
        "learning_rate": 3.76737977071967e-06,
        "epoch": 0.6383486101795961,
        "step": 8566
    },
    {
        "loss": 2.3651,
        "grad_norm": 3.118572950363159,
        "learning_rate": 3.748271342389098e-06,
        "epoch": 0.6384231313808778,
        "step": 8567
    },
    {
        "loss": 2.635,
        "grad_norm": 2.470672845840454,
        "learning_rate": 3.729210571441022e-06,
        "epoch": 0.6384976525821596,
        "step": 8568
    },
    {
        "loss": 3.0822,
        "grad_norm": 3.16040301322937,
        "learning_rate": 3.7101974673130256e-06,
        "epoch": 0.6385721737834414,
        "step": 8569
    },
    {
        "loss": 1.954,
        "grad_norm": 5.08168888092041,
        "learning_rate": 3.69123203941919e-06,
        "epoch": 0.6386466949847232,
        "step": 8570
    },
    {
        "loss": 2.9902,
        "grad_norm": 2.4118094444274902,
        "learning_rate": 3.672314297149837e-06,
        "epoch": 0.638721216186005,
        "step": 8571
    },
    {
        "loss": 2.0587,
        "grad_norm": 2.076847791671753,
        "learning_rate": 3.6534442498718404e-06,
        "epoch": 0.6387957373872867,
        "step": 8572
    },
    {
        "loss": 2.2072,
        "grad_norm": 3.003174304962158,
        "learning_rate": 3.634621906928348e-06,
        "epoch": 0.6388702585885685,
        "step": 8573
    },
    {
        "loss": 1.5804,
        "grad_norm": 2.372148036956787,
        "learning_rate": 3.615847277638862e-06,
        "epoch": 0.6389447797898502,
        "step": 8574
    },
    {
        "loss": 2.1969,
        "grad_norm": 3.080968141555786,
        "learning_rate": 3.5971203712994005e-06,
        "epoch": 0.639019300991132,
        "step": 8575
    },
    {
        "loss": 2.8009,
        "grad_norm": 1.503891110420227,
        "learning_rate": 3.578441197182203e-06,
        "epoch": 0.6390938221924137,
        "step": 8576
    },
    {
        "loss": 2.2244,
        "grad_norm": 2.0888161659240723,
        "learning_rate": 3.559809764536004e-06,
        "epoch": 0.6391683433936955,
        "step": 8577
    },
    {
        "loss": 2.9459,
        "grad_norm": 2.4919090270996094,
        "learning_rate": 3.5412260825858025e-06,
        "epoch": 0.6392428645949773,
        "step": 8578
    },
    {
        "loss": 2.1561,
        "grad_norm": 2.929481029510498,
        "learning_rate": 3.522690160532971e-06,
        "epoch": 0.6393173857962591,
        "step": 8579
    },
    {
        "loss": 2.8759,
        "grad_norm": 3.106900453567505,
        "learning_rate": 3.5042020075552907e-06,
        "epoch": 0.6393919069975408,
        "step": 8580
    },
    {
        "loss": 1.3981,
        "grad_norm": 4.59731388092041,
        "learning_rate": 3.4857616328068166e-06,
        "epoch": 0.6394664281988226,
        "step": 8581
    },
    {
        "loss": 1.9497,
        "grad_norm": 3.173917531967163,
        "learning_rate": 3.467369045418001e-06,
        "epoch": 0.6395409494001043,
        "step": 8582
    },
    {
        "loss": 1.8227,
        "grad_norm": 3.4937920570373535,
        "learning_rate": 3.4490242544956143e-06,
        "epoch": 0.6396154706013861,
        "step": 8583
    },
    {
        "loss": 1.6001,
        "grad_norm": 2.4988784790039062,
        "learning_rate": 3.4307272691227132e-06,
        "epoch": 0.6396899918026678,
        "step": 8584
    },
    {
        "loss": 2.5075,
        "grad_norm": 1.3954838514328003,
        "learning_rate": 3.412478098358829e-06,
        "epoch": 0.6397645130039497,
        "step": 8585
    },
    {
        "loss": 1.9249,
        "grad_norm": 3.2141497135162354,
        "learning_rate": 3.3942767512396336e-06,
        "epoch": 0.6398390342052314,
        "step": 8586
    },
    {
        "loss": 1.8493,
        "grad_norm": 2.8559963703155518,
        "learning_rate": 3.376123236777284e-06,
        "epoch": 0.6399135554065132,
        "step": 8587
    },
    {
        "loss": 2.7618,
        "grad_norm": 1.8694286346435547,
        "learning_rate": 3.3580175639601476e-06,
        "epoch": 0.6399880766077949,
        "step": 8588
    },
    {
        "loss": 2.0903,
        "grad_norm": 3.3492276668548584,
        "learning_rate": 3.3399597417529295e-06,
        "epoch": 0.6400625978090767,
        "step": 8589
    },
    {
        "loss": 2.7177,
        "grad_norm": 2.271770477294922,
        "learning_rate": 3.321949779096656e-06,
        "epoch": 0.6401371190103584,
        "step": 8590
    },
    {
        "loss": 2.3373,
        "grad_norm": 3.392022132873535,
        "learning_rate": 3.3039876849086494e-06,
        "epoch": 0.6402116402116402,
        "step": 8591
    },
    {
        "loss": 2.1333,
        "grad_norm": 2.9066689014434814,
        "learning_rate": 3.2860734680825176e-06,
        "epoch": 0.6402861614129219,
        "step": 8592
    },
    {
        "loss": 2.5761,
        "grad_norm": 2.3029980659484863,
        "learning_rate": 3.2682071374881996e-06,
        "epoch": 0.6403606826142038,
        "step": 8593
    },
    {
        "loss": 2.742,
        "grad_norm": 2.821319818496704,
        "learning_rate": 3.2503887019718515e-06,
        "epoch": 0.6404352038154855,
        "step": 8594
    },
    {
        "loss": 1.8863,
        "grad_norm": 2.882016181945801,
        "learning_rate": 3.2326181703560387e-06,
        "epoch": 0.6405097250167673,
        "step": 8595
    },
    {
        "loss": 3.1939,
        "grad_norm": 3.769705295562744,
        "learning_rate": 3.214895551439512e-06,
        "epoch": 0.640584246218049,
        "step": 8596
    },
    {
        "loss": 2.9614,
        "grad_norm": 2.7615714073181152,
        "learning_rate": 3.1972208539972515e-06,
        "epoch": 0.6406587674193308,
        "step": 8597
    },
    {
        "loss": 2.4052,
        "grad_norm": 3.347817897796631,
        "learning_rate": 3.1795940867806683e-06,
        "epoch": 0.6407332886206125,
        "step": 8598
    },
    {
        "loss": 1.0897,
        "grad_norm": 1.3252977132797241,
        "learning_rate": 3.1620152585172814e-06,
        "epoch": 0.6408078098218943,
        "step": 8599
    },
    {
        "loss": 2.5531,
        "grad_norm": 2.5966269969940186,
        "learning_rate": 3.1444843779110167e-06,
        "epoch": 0.640882331023176,
        "step": 8600
    },
    {
        "loss": 2.5898,
        "grad_norm": 1.9786579608917236,
        "learning_rate": 3.1270014536419425e-06,
        "epoch": 0.6409568522244579,
        "step": 8601
    },
    {
        "loss": 2.3752,
        "grad_norm": 3.1138763427734375,
        "learning_rate": 3.1095664943664003e-06,
        "epoch": 0.6410313734257396,
        "step": 8602
    },
    {
        "loss": 2.1309,
        "grad_norm": 3.150050640106201,
        "learning_rate": 3.092179508717086e-06,
        "epoch": 0.6411058946270214,
        "step": 8603
    },
    {
        "loss": 1.8724,
        "grad_norm": 2.426485300064087,
        "learning_rate": 3.0748405053027673e-06,
        "epoch": 0.6411804158283032,
        "step": 8604
    },
    {
        "loss": 1.9573,
        "grad_norm": 2.7397422790527344,
        "learning_rate": 3.0575494927086446e-06,
        "epoch": 0.6412549370295849,
        "step": 8605
    },
    {
        "loss": 1.2786,
        "grad_norm": 3.3468925952911377,
        "learning_rate": 3.0403064794960244e-06,
        "epoch": 0.6413294582308667,
        "step": 8606
    },
    {
        "loss": 2.4329,
        "grad_norm": 2.461421489715576,
        "learning_rate": 3.023111474202456e-06,
        "epoch": 0.6414039794321484,
        "step": 8607
    },
    {
        "loss": 2.9047,
        "grad_norm": 2.316563129425049,
        "learning_rate": 3.0059644853418167e-06,
        "epoch": 0.6414785006334303,
        "step": 8608
    },
    {
        "loss": 2.371,
        "grad_norm": 3.0782229900360107,
        "learning_rate": 2.988865521404094e-06,
        "epoch": 0.641553021834712,
        "step": 8609
    },
    {
        "loss": 2.7923,
        "grad_norm": 1.9287967681884766,
        "learning_rate": 2.9718145908555705e-06,
        "epoch": 0.6416275430359938,
        "step": 8610
    },
    {
        "loss": 1.8153,
        "grad_norm": 3.081611156463623,
        "learning_rate": 2.9548117021387156e-06,
        "epoch": 0.6417020642372755,
        "step": 8611
    },
    {
        "loss": 2.5942,
        "grad_norm": 2.369338274002075,
        "learning_rate": 2.9378568636721835e-06,
        "epoch": 0.6417765854385573,
        "step": 8612
    },
    {
        "loss": 2.6443,
        "grad_norm": 2.4189581871032715,
        "learning_rate": 2.9209500838509153e-06,
        "epoch": 0.641851106639839,
        "step": 8613
    },
    {
        "loss": 1.9375,
        "grad_norm": 3.050222635269165,
        "learning_rate": 2.9040913710459583e-06,
        "epoch": 0.6419256278411208,
        "step": 8614
    },
    {
        "loss": 2.0202,
        "grad_norm": 3.146570920944214,
        "learning_rate": 2.8872807336046805e-06,
        "epoch": 0.6420001490424025,
        "step": 8615
    },
    {
        "loss": 2.6575,
        "grad_norm": 2.6004278659820557,
        "learning_rate": 2.8705181798505455e-06,
        "epoch": 0.6420746702436844,
        "step": 8616
    },
    {
        "loss": 1.5668,
        "grad_norm": 3.3006601333618164,
        "learning_rate": 2.8538037180832034e-06,
        "epoch": 0.6421491914449661,
        "step": 8617
    },
    {
        "loss": 2.887,
        "grad_norm": 2.4114632606506348,
        "learning_rate": 2.837137356578601e-06,
        "epoch": 0.6422237126462479,
        "step": 8618
    },
    {
        "loss": 2.3181,
        "grad_norm": 2.472029447555542,
        "learning_rate": 2.8205191035887814e-06,
        "epoch": 0.6422982338475296,
        "step": 8619
    },
    {
        "loss": 1.8355,
        "grad_norm": 4.484926700592041,
        "learning_rate": 2.8039489673419407e-06,
        "epoch": 0.6423727550488114,
        "step": 8620
    },
    {
        "loss": 2.6402,
        "grad_norm": 3.2661654949188232,
        "learning_rate": 2.787426956042549e-06,
        "epoch": 0.6424472762500931,
        "step": 8621
    },
    {
        "loss": 2.483,
        "grad_norm": 1.9581395387649536,
        "learning_rate": 2.7709530778711744e-06,
        "epoch": 0.6425217974513749,
        "step": 8622
    },
    {
        "loss": 2.4973,
        "grad_norm": 2.888119697570801,
        "learning_rate": 2.7545273409845696e-06,
        "epoch": 0.6425963186526567,
        "step": 8623
    },
    {
        "loss": 2.6929,
        "grad_norm": 2.2223801612854004,
        "learning_rate": 2.738149753515662e-06,
        "epoch": 0.6426708398539385,
        "step": 8624
    },
    {
        "loss": 2.5747,
        "grad_norm": 2.6866977214813232,
        "learning_rate": 2.7218203235734985e-06,
        "epoch": 0.6427453610552202,
        "step": 8625
    },
    {
        "loss": 1.936,
        "grad_norm": 2.9041950702667236,
        "learning_rate": 2.7055390592433895e-06,
        "epoch": 0.642819882256502,
        "step": 8626
    },
    {
        "loss": 0.358,
        "grad_norm": 0.8512142896652222,
        "learning_rate": 2.6893059685866307e-06,
        "epoch": 0.6428944034577837,
        "step": 8627
    },
    {
        "loss": 1.9757,
        "grad_norm": 3.447094440460205,
        "learning_rate": 2.6731210596408263e-06,
        "epoch": 0.6429689246590655,
        "step": 8628
    },
    {
        "loss": 2.8448,
        "grad_norm": 2.6719627380371094,
        "learning_rate": 2.6569843404196325e-06,
        "epoch": 0.6430434458603472,
        "step": 8629
    },
    {
        "loss": 2.5459,
        "grad_norm": 2.55472469329834,
        "learning_rate": 2.6408958189128697e-06,
        "epoch": 0.643117967061629,
        "step": 8630
    },
    {
        "loss": 2.5062,
        "grad_norm": 2.863229274749756,
        "learning_rate": 2.6248555030864873e-06,
        "epoch": 0.6431924882629108,
        "step": 8631
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.9446940422058105,
        "learning_rate": 2.6088634008825443e-06,
        "epoch": 0.6432670094641926,
        "step": 8632
    },
    {
        "loss": 1.8539,
        "grad_norm": 2.643199920654297,
        "learning_rate": 2.592919520219317e-06,
        "epoch": 0.6433415306654743,
        "step": 8633
    },
    {
        "loss": 1.7673,
        "grad_norm": 3.5712249279022217,
        "learning_rate": 2.577023868991102e-06,
        "epoch": 0.6434160518667561,
        "step": 8634
    },
    {
        "loss": 1.8729,
        "grad_norm": 3.1166622638702393,
        "learning_rate": 2.561176455068337e-06,
        "epoch": 0.6434905730680378,
        "step": 8635
    },
    {
        "loss": 2.7422,
        "grad_norm": 2.266939640045166,
        "learning_rate": 2.5453772862976345e-06,
        "epoch": 0.6435650942693196,
        "step": 8636
    },
    {
        "loss": 3.3133,
        "grad_norm": 3.450667142868042,
        "learning_rate": 2.5296263705016477e-06,
        "epoch": 0.6436396154706013,
        "step": 8637
    },
    {
        "loss": 2.1499,
        "grad_norm": 2.5582656860351562,
        "learning_rate": 2.513923715479216e-06,
        "epoch": 0.6437141366718832,
        "step": 8638
    },
    {
        "loss": 2.7103,
        "grad_norm": 2.2103216648101807,
        "learning_rate": 2.4982693290052096e-06,
        "epoch": 0.6437886578731649,
        "step": 8639
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.9254953861236572,
        "learning_rate": 2.4826632188306163e-06,
        "epoch": 0.6438631790744467,
        "step": 8640
    },
    {
        "loss": 2.81,
        "grad_norm": 2.1537909507751465,
        "learning_rate": 2.4671053926825562e-06,
        "epoch": 0.6439377002757285,
        "step": 8641
    },
    {
        "loss": 2.5481,
        "grad_norm": 2.101576566696167,
        "learning_rate": 2.4515958582641997e-06,
        "epoch": 0.6440122214770102,
        "step": 8642
    },
    {
        "loss": 1.956,
        "grad_norm": 2.8336191177368164,
        "learning_rate": 2.4361346232548487e-06,
        "epoch": 0.644086742678292,
        "step": 8643
    },
    {
        "loss": 1.292,
        "grad_norm": 3.3768231868743896,
        "learning_rate": 2.4207216953098353e-06,
        "epoch": 0.6441612638795737,
        "step": 8644
    },
    {
        "loss": 3.1449,
        "grad_norm": 2.0550806522369385,
        "learning_rate": 2.405357082060611e-06,
        "epoch": 0.6442357850808555,
        "step": 8645
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.434814453125,
        "learning_rate": 2.390040791114723e-06,
        "epoch": 0.6443103062821373,
        "step": 8646
    },
    {
        "loss": 2.7038,
        "grad_norm": 2.384608268737793,
        "learning_rate": 2.374772830055727e-06,
        "epoch": 0.6443848274834191,
        "step": 8647
    },
    {
        "loss": 2.2262,
        "grad_norm": 3.1611900329589844,
        "learning_rate": 2.3595532064433545e-06,
        "epoch": 0.6444593486847008,
        "step": 8648
    },
    {
        "loss": 2.3135,
        "grad_norm": 3.2291243076324463,
        "learning_rate": 2.3443819278132996e-06,
        "epoch": 0.6445338698859826,
        "step": 8649
    },
    {
        "loss": 1.9521,
        "grad_norm": 2.66941237449646,
        "learning_rate": 2.329259001677342e-06,
        "epoch": 0.6446083910872643,
        "step": 8650
    },
    {
        "loss": 2.5672,
        "grad_norm": 2.406492233276367,
        "learning_rate": 2.314184435523381e-06,
        "epoch": 0.6446829122885461,
        "step": 8651
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.1119223833084106,
        "learning_rate": 2.299158236815291e-06,
        "epoch": 0.6447574334898278,
        "step": 8652
    },
    {
        "loss": 2.1665,
        "grad_norm": 2.7422127723693848,
        "learning_rate": 2.284180412993053e-06,
        "epoch": 0.6448319546911097,
        "step": 8653
    },
    {
        "loss": 2.5162,
        "grad_norm": 2.2601866722106934,
        "learning_rate": 2.2692509714726695e-06,
        "epoch": 0.6449064758923914,
        "step": 8654
    },
    {
        "loss": 1.6972,
        "grad_norm": 2.682262659072876,
        "learning_rate": 2.254369919646182e-06,
        "epoch": 0.6449809970936732,
        "step": 8655
    },
    {
        "loss": 2.1118,
        "grad_norm": 2.5872280597686768,
        "learning_rate": 2.2395372648817304e-06,
        "epoch": 0.6450555182949549,
        "step": 8656
    },
    {
        "loss": 2.267,
        "grad_norm": 1.4198048114776611,
        "learning_rate": 2.2247530145234286e-06,
        "epoch": 0.6451300394962367,
        "step": 8657
    },
    {
        "loss": 2.5231,
        "grad_norm": 2.432183027267456,
        "learning_rate": 2.210017175891399e-06,
        "epoch": 0.6452045606975184,
        "step": 8658
    },
    {
        "loss": 1.4615,
        "grad_norm": 2.68424916267395,
        "learning_rate": 2.195329756281905e-06,
        "epoch": 0.6452790818988002,
        "step": 8659
    },
    {
        "loss": 2.3122,
        "grad_norm": 3.1032755374908447,
        "learning_rate": 2.1806907629671192e-06,
        "epoch": 0.6453536031000819,
        "step": 8660
    },
    {
        "loss": 1.8955,
        "grad_norm": 3.257302761077881,
        "learning_rate": 2.1661002031953316e-06,
        "epoch": 0.6454281243013638,
        "step": 8661
    },
    {
        "loss": 2.3906,
        "grad_norm": 3.6837496757507324,
        "learning_rate": 2.1515580841907745e-06,
        "epoch": 0.6455026455026455,
        "step": 8662
    },
    {
        "loss": 2.6572,
        "grad_norm": 4.4614362716674805,
        "learning_rate": 2.137064413153711e-06,
        "epoch": 0.6455771667039273,
        "step": 8663
    },
    {
        "loss": 1.9097,
        "grad_norm": 3.7111706733703613,
        "learning_rate": 2.122619197260489e-06,
        "epoch": 0.645651687905209,
        "step": 8664
    },
    {
        "loss": 2.4224,
        "grad_norm": 2.472290515899658,
        "learning_rate": 2.1082224436633324e-06,
        "epoch": 0.6457262091064908,
        "step": 8665
    },
    {
        "loss": 1.0047,
        "grad_norm": 3.1707210540771484,
        "learning_rate": 2.0938741594906162e-06,
        "epoch": 0.6458007303077725,
        "step": 8666
    },
    {
        "loss": 1.4232,
        "grad_norm": 4.965804576873779,
        "learning_rate": 2.079574351846614e-06,
        "epoch": 0.6458752515090543,
        "step": 8667
    },
    {
        "loss": 1.7803,
        "grad_norm": 2.8747847080230713,
        "learning_rate": 2.0653230278116055e-06,
        "epoch": 0.645949772710336,
        "step": 8668
    },
    {
        "loss": 2.6946,
        "grad_norm": 2.6487951278686523,
        "learning_rate": 2.0511201944419356e-06,
        "epoch": 0.6460242939116179,
        "step": 8669
    },
    {
        "loss": 1.764,
        "grad_norm": 2.643094778060913,
        "learning_rate": 2.036965858769868e-06,
        "epoch": 0.6460988151128996,
        "step": 8670
    },
    {
        "loss": 2.5082,
        "grad_norm": 2.4168484210968018,
        "learning_rate": 2.0228600278036857e-06,
        "epoch": 0.6461733363141814,
        "step": 8671
    },
    {
        "loss": 2.8215,
        "grad_norm": 3.3139889240264893,
        "learning_rate": 2.0088027085276353e-06,
        "epoch": 0.6462478575154631,
        "step": 8672
    },
    {
        "loss": 1.5075,
        "grad_norm": 2.4098868370056152,
        "learning_rate": 1.9947939079019596e-06,
        "epoch": 0.6463223787167449,
        "step": 8673
    },
    {
        "loss": 2.6273,
        "grad_norm": 2.319429874420166,
        "learning_rate": 1.9808336328628773e-06,
        "epoch": 0.6463968999180266,
        "step": 8674
    },
    {
        "loss": 2.0837,
        "grad_norm": 3.8114309310913086,
        "learning_rate": 1.9669218903225596e-06,
        "epoch": 0.6464714211193084,
        "step": 8675
    },
    {
        "loss": 2.3949,
        "grad_norm": 2.704383373260498,
        "learning_rate": 1.953058687169207e-06,
        "epoch": 0.6465459423205903,
        "step": 8676
    },
    {
        "loss": 2.4349,
        "grad_norm": 2.2108614444732666,
        "learning_rate": 1.9392440302669177e-06,
        "epoch": 0.646620463521872,
        "step": 8677
    },
    {
        "loss": 2.6924,
        "grad_norm": 4.164431095123291,
        "learning_rate": 1.9254779264557653e-06,
        "epoch": 0.6466949847231538,
        "step": 8678
    },
    {
        "loss": 2.5838,
        "grad_norm": 2.0207250118255615,
        "learning_rate": 1.9117603825518417e-06,
        "epoch": 0.6467695059244355,
        "step": 8679
    },
    {
        "loss": 1.2433,
        "grad_norm": 1.753839135169983,
        "learning_rate": 1.8980914053471132e-06,
        "epoch": 0.6468440271257173,
        "step": 8680
    },
    {
        "loss": 2.6521,
        "grad_norm": 3.1163930892944336,
        "learning_rate": 1.8844710016095667e-06,
        "epoch": 0.646918548326999,
        "step": 8681
    },
    {
        "loss": 1.3599,
        "grad_norm": 1.9432628154754639,
        "learning_rate": 1.8708991780830964e-06,
        "epoch": 0.6469930695282808,
        "step": 8682
    },
    {
        "loss": 2.2978,
        "grad_norm": 3.6423182487487793,
        "learning_rate": 1.857375941487549e-06,
        "epoch": 0.6470675907295625,
        "step": 8683
    },
    {
        "loss": 2.2229,
        "grad_norm": 3.052985429763794,
        "learning_rate": 1.8439012985187353e-06,
        "epoch": 0.6471421119308444,
        "step": 8684
    },
    {
        "loss": 2.5049,
        "grad_norm": 1.8926383256912231,
        "learning_rate": 1.830475255848385e-06,
        "epoch": 0.6472166331321261,
        "step": 8685
    },
    {
        "loss": 2.3746,
        "grad_norm": 3.2127015590667725,
        "learning_rate": 1.8170978201241474e-06,
        "epoch": 0.6472911543334079,
        "step": 8686
    },
    {
        "loss": 2.52,
        "grad_norm": 2.5075371265411377,
        "learning_rate": 1.8037689979696904e-06,
        "epoch": 0.6473656755346896,
        "step": 8687
    },
    {
        "loss": 1.8094,
        "grad_norm": 4.461679458618164,
        "learning_rate": 1.7904887959844684e-06,
        "epoch": 0.6474401967359714,
        "step": 8688
    },
    {
        "loss": 2.1504,
        "grad_norm": 2.8817594051361084,
        "learning_rate": 1.777257220744033e-06,
        "epoch": 0.6475147179372531,
        "step": 8689
    },
    {
        "loss": 2.7284,
        "grad_norm": 2.9018125534057617,
        "learning_rate": 1.76407427879971e-06,
        "epoch": 0.6475892391385349,
        "step": 8690
    },
    {
        "loss": 2.0907,
        "grad_norm": 3.407341480255127,
        "learning_rate": 1.7509399766788336e-06,
        "epoch": 0.6476637603398167,
        "step": 8691
    },
    {
        "loss": 2.1702,
        "grad_norm": 2.6776795387268066,
        "learning_rate": 1.7378543208846133e-06,
        "epoch": 0.6477382815410985,
        "step": 8692
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.8718167543411255,
        "learning_rate": 1.7248173178961657e-06,
        "epoch": 0.6478128027423802,
        "step": 8693
    },
    {
        "loss": 2.0765,
        "grad_norm": 3.0276715755462646,
        "learning_rate": 1.7118289741685944e-06,
        "epoch": 0.647887323943662,
        "step": 8694
    },
    {
        "loss": 2.1623,
        "grad_norm": 2.0611188411712646,
        "learning_rate": 1.6988892961328106e-06,
        "epoch": 0.6479618451449437,
        "step": 8695
    },
    {
        "loss": 2.1081,
        "grad_norm": 2.7670626640319824,
        "learning_rate": 1.6859982901956561e-06,
        "epoch": 0.6480363663462255,
        "step": 8696
    },
    {
        "loss": 3.0729,
        "grad_norm": 2.876603603363037,
        "learning_rate": 1.6731559627399363e-06,
        "epoch": 0.6481108875475072,
        "step": 8697
    },
    {
        "loss": 2.0043,
        "grad_norm": 1.7282744646072388,
        "learning_rate": 1.6603623201242757e-06,
        "epoch": 0.648185408748789,
        "step": 8698
    },
    {
        "loss": 2.738,
        "grad_norm": 2.783360481262207,
        "learning_rate": 1.647617368683252e-06,
        "epoch": 0.6482599299500708,
        "step": 8699
    },
    {
        "loss": 2.7162,
        "grad_norm": 2.9060757160186768,
        "learning_rate": 1.6349211147272946e-06,
        "epoch": 0.6483344511513526,
        "step": 8700
    },
    {
        "loss": 2.9766,
        "grad_norm": 1.684686541557312,
        "learning_rate": 1.62227356454272e-06,
        "epoch": 0.6484089723526343,
        "step": 8701
    },
    {
        "loss": 2.0155,
        "grad_norm": 3.9623301029205322,
        "learning_rate": 1.6096747243917741e-06,
        "epoch": 0.6484834935539161,
        "step": 8702
    },
    {
        "loss": 2.1689,
        "grad_norm": 3.3086373805999756,
        "learning_rate": 1.597124600512534e-06,
        "epoch": 0.6485580147551978,
        "step": 8703
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.0205812454223633,
        "learning_rate": 1.584623199118973e-06,
        "epoch": 0.6486325359564796,
        "step": 8704
    },
    {
        "loss": 2.9541,
        "grad_norm": 2.84906005859375,
        "learning_rate": 1.5721705264009733e-06,
        "epoch": 0.6487070571577613,
        "step": 8705
    },
    {
        "loss": 1.3519,
        "grad_norm": 3.2925400733947754,
        "learning_rate": 1.559766588524203e-06,
        "epoch": 0.6487815783590432,
        "step": 8706
    },
    {
        "loss": 2.5044,
        "grad_norm": 2.9229979515075684,
        "learning_rate": 1.5474113916303267e-06,
        "epoch": 0.6488560995603249,
        "step": 8707
    },
    {
        "loss": 2.8349,
        "grad_norm": 2.3141191005706787,
        "learning_rate": 1.5351049418367514e-06,
        "epoch": 0.6489306207616067,
        "step": 8708
    },
    {
        "loss": 1.2298,
        "grad_norm": 6.397623538970947,
        "learning_rate": 1.522847245236847e-06,
        "epoch": 0.6490051419628884,
        "step": 8709
    },
    {
        "loss": 2.3921,
        "grad_norm": 3.000152349472046,
        "learning_rate": 1.510638307899792e-06,
        "epoch": 0.6490796631641702,
        "step": 8710
    },
    {
        "loss": 2.4796,
        "grad_norm": 2.2795615196228027,
        "learning_rate": 1.4984781358705956e-06,
        "epoch": 0.649154184365452,
        "step": 8711
    },
    {
        "loss": 2.2089,
        "grad_norm": 3.521080493927002,
        "learning_rate": 1.486366735170197e-06,
        "epoch": 0.6492287055667337,
        "step": 8712
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.8864250183105469,
        "learning_rate": 1.4743041117953438e-06,
        "epoch": 0.6493032267680156,
        "step": 8713
    },
    {
        "loss": 2.1493,
        "grad_norm": 4.0705156326293945,
        "learning_rate": 1.4622902717186028e-06,
        "epoch": 0.6493777479692973,
        "step": 8714
    },
    {
        "loss": 2.462,
        "grad_norm": 3.919201612472534,
        "learning_rate": 1.4503252208884487e-06,
        "epoch": 0.6494522691705791,
        "step": 8715
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.471548318862915,
        "learning_rate": 1.4384089652291432e-06,
        "epoch": 0.6495267903718608,
        "step": 8716
    },
    {
        "loss": 2.1192,
        "grad_norm": 2.5190553665161133,
        "learning_rate": 1.4265415106408664e-06,
        "epoch": 0.6496013115731426,
        "step": 8717
    },
    {
        "loss": 2.8269,
        "grad_norm": 2.9820988178253174,
        "learning_rate": 1.41472286299954e-06,
        "epoch": 0.6496758327744243,
        "step": 8718
    },
    {
        "loss": 2.6426,
        "grad_norm": 3.1156668663024902,
        "learning_rate": 1.4029530281569614e-06,
        "epoch": 0.6497503539757061,
        "step": 8719
    },
    {
        "loss": 2.8138,
        "grad_norm": 2.6105308532714844,
        "learning_rate": 1.3912320119407795e-06,
        "epoch": 0.6498248751769878,
        "step": 8720
    },
    {
        "loss": 2.4401,
        "grad_norm": 3.5185792446136475,
        "learning_rate": 1.3795598201544525e-06,
        "epoch": 0.6498993963782697,
        "step": 8721
    },
    {
        "loss": 1.6863,
        "grad_norm": 3.8234333992004395,
        "learning_rate": 1.367936458577268e-06,
        "epoch": 0.6499739175795514,
        "step": 8722
    },
    {
        "loss": 2.4219,
        "grad_norm": 2.528841733932495,
        "learning_rate": 1.3563619329643229e-06,
        "epoch": 0.6500484387808332,
        "step": 8723
    },
    {
        "loss": 2.6105,
        "grad_norm": 2.505462408065796,
        "learning_rate": 1.3448362490465217e-06,
        "epoch": 0.6501229599821149,
        "step": 8724
    },
    {
        "loss": 2.6939,
        "grad_norm": 1.2531750202178955,
        "learning_rate": 1.3333594125306659e-06,
        "epoch": 0.6501974811833967,
        "step": 8725
    },
    {
        "loss": 2.0317,
        "grad_norm": 4.279794692993164,
        "learning_rate": 1.3219314290992434e-06,
        "epoch": 0.6502720023846784,
        "step": 8726
    },
    {
        "loss": 1.5932,
        "grad_norm": 3.5178120136260986,
        "learning_rate": 1.3105523044106837e-06,
        "epoch": 0.6503465235859602,
        "step": 8727
    },
    {
        "loss": 2.6271,
        "grad_norm": 3.0814077854156494,
        "learning_rate": 1.2992220440991466e-06,
        "epoch": 0.6504210447872419,
        "step": 8728
    },
    {
        "loss": 2.2314,
        "grad_norm": 1.9172923564910889,
        "learning_rate": 1.2879406537745775e-06,
        "epoch": 0.6504955659885238,
        "step": 8729
    },
    {
        "loss": 2.081,
        "grad_norm": 3.475277900695801,
        "learning_rate": 1.2767081390228087e-06,
        "epoch": 0.6505700871898055,
        "step": 8730
    },
    {
        "loss": 2.6376,
        "grad_norm": 1.9028023481369019,
        "learning_rate": 1.2655245054054133e-06,
        "epoch": 0.6506446083910873,
        "step": 8731
    },
    {
        "loss": 2.4416,
        "grad_norm": 1.837136149406433,
        "learning_rate": 1.254389758459773e-06,
        "epoch": 0.650719129592369,
        "step": 8732
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.8946409225463867,
        "learning_rate": 1.2433039036990668e-06,
        "epoch": 0.6507936507936508,
        "step": 8733
    },
    {
        "loss": 2.4811,
        "grad_norm": 2.3890676498413086,
        "learning_rate": 1.2322669466122483e-06,
        "epoch": 0.6508681719949325,
        "step": 8734
    },
    {
        "loss": 2.7272,
        "grad_norm": 2.6337382793426514,
        "learning_rate": 1.2212788926641017e-06,
        "epoch": 0.6509426931962143,
        "step": 8735
    },
    {
        "loss": 2.546,
        "grad_norm": 2.441399335861206,
        "learning_rate": 1.2103397472951306e-06,
        "epoch": 0.651017214397496,
        "step": 8736
    },
    {
        "loss": 1.2582,
        "grad_norm": 2.7020483016967773,
        "learning_rate": 1.1994495159217246e-06,
        "epoch": 0.6510917355987779,
        "step": 8737
    },
    {
        "loss": 2.4717,
        "grad_norm": 2.6031625270843506,
        "learning_rate": 1.188608203935948e-06,
        "epoch": 0.6511662568000596,
        "step": 8738
    },
    {
        "loss": 2.6207,
        "grad_norm": 2.3942501544952393,
        "learning_rate": 1.1778158167056964e-06,
        "epoch": 0.6512407780013414,
        "step": 8739
    },
    {
        "loss": 2.5608,
        "grad_norm": 3.8236775398254395,
        "learning_rate": 1.1670723595746613e-06,
        "epoch": 0.6513152992026231,
        "step": 8740
    },
    {
        "loss": 1.9644,
        "grad_norm": 2.6499342918395996,
        "learning_rate": 1.1563778378622436e-06,
        "epoch": 0.6513898204039049,
        "step": 8741
    },
    {
        "loss": 3.301,
        "grad_norm": 3.875676393508911,
        "learning_rate": 1.1457322568636852e-06,
        "epoch": 0.6514643416051866,
        "step": 8742
    },
    {
        "loss": 2.5027,
        "grad_norm": 2.5530192852020264,
        "learning_rate": 1.1351356218499477e-06,
        "epoch": 0.6515388628064684,
        "step": 8743
    },
    {
        "loss": 2.0939,
        "grad_norm": 3.621737241744995,
        "learning_rate": 1.124587938067745e-06,
        "epoch": 0.6516133840077502,
        "step": 8744
    },
    {
        "loss": 2.1847,
        "grad_norm": 3.8689186573028564,
        "learning_rate": 1.1140892107396216e-06,
        "epoch": 0.651687905209032,
        "step": 8745
    },
    {
        "loss": 1.9623,
        "grad_norm": 3.1100716590881348,
        "learning_rate": 1.1036394450638199e-06,
        "epoch": 0.6517624264103138,
        "step": 8746
    },
    {
        "loss": 2.3163,
        "grad_norm": 3.1377182006835938,
        "learning_rate": 1.093238646214345e-06,
        "epoch": 0.6518369476115955,
        "step": 8747
    },
    {
        "loss": 2.1779,
        "grad_norm": 2.7131783962249756,
        "learning_rate": 1.0828868193410224e-06,
        "epoch": 0.6519114688128773,
        "step": 8748
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.1157290935516357,
        "learning_rate": 1.0725839695693297e-06,
        "epoch": 0.651985990014159,
        "step": 8749
    },
    {
        "loss": 2.3686,
        "grad_norm": 2.590296745300293,
        "learning_rate": 1.062330102000586e-06,
        "epoch": 0.6520605112154408,
        "step": 8750
    },
    {
        "loss": 2.3111,
        "grad_norm": 2.6471457481384277,
        "learning_rate": 1.052125221711786e-06,
        "epoch": 0.6521350324167225,
        "step": 8751
    },
    {
        "loss": 2.4613,
        "grad_norm": 3.2892024517059326,
        "learning_rate": 1.0419693337557213e-06,
        "epoch": 0.6522095536180044,
        "step": 8752
    },
    {
        "loss": 2.4561,
        "grad_norm": 4.0512237548828125,
        "learning_rate": 1.0318624431608914e-06,
        "epoch": 0.6522840748192861,
        "step": 8753
    },
    {
        "loss": 2.7769,
        "grad_norm": 2.1458661556243896,
        "learning_rate": 1.021804554931538e-06,
        "epoch": 0.6523585960205679,
        "step": 8754
    },
    {
        "loss": 2.4201,
        "grad_norm": 3.510810613632202,
        "learning_rate": 1.0117956740476886e-06,
        "epoch": 0.6524331172218496,
        "step": 8755
    },
    {
        "loss": 2.5028,
        "grad_norm": 2.417468547821045,
        "learning_rate": 1.0018358054650345e-06,
        "epoch": 0.6525076384231314,
        "step": 8756
    },
    {
        "loss": 2.0242,
        "grad_norm": 3.14251446723938,
        "learning_rate": 9.91924954115031e-07,
        "epoch": 0.6525821596244131,
        "step": 8757
    },
    {
        "loss": 1.5842,
        "grad_norm": 5.555750370025635,
        "learning_rate": 9.820631249048973e-07,
        "epoch": 0.652656680825695,
        "step": 8758
    },
    {
        "loss": 2.4906,
        "grad_norm": 2.9801504611968994,
        "learning_rate": 9.722503227174939e-07,
        "epoch": 0.6527312020269767,
        "step": 8759
    },
    {
        "loss": 2.0346,
        "grad_norm": 3.240949869155884,
        "learning_rate": 9.624865524115234e-07,
        "epoch": 0.6528057232282585,
        "step": 8760
    },
    {
        "loss": 2.4755,
        "grad_norm": 2.5734307765960693,
        "learning_rate": 9.527718188213186e-07,
        "epoch": 0.6528802444295402,
        "step": 8761
    },
    {
        "loss": 3.067,
        "grad_norm": 2.9130704402923584,
        "learning_rate": 9.431061267569541e-07,
        "epoch": 0.652954765630822,
        "step": 8762
    },
    {
        "loss": 2.4289,
        "grad_norm": 2.025217294692993,
        "learning_rate": 9.334894810042461e-07,
        "epoch": 0.6530292868321037,
        "step": 8763
    },
    {
        "loss": 1.6895,
        "grad_norm": 3.3507845401763916,
        "learning_rate": 9.239218863247079e-07,
        "epoch": 0.6531038080333855,
        "step": 8764
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.8981263637542725,
        "learning_rate": 9.144033474555614e-07,
        "epoch": 0.6531783292346672,
        "step": 8765
    },
    {
        "loss": 2.6712,
        "grad_norm": 2.3271851539611816,
        "learning_rate": 9.049338691097476e-07,
        "epoch": 0.653252850435949,
        "step": 8766
    },
    {
        "loss": 2.7615,
        "grad_norm": 2.248136281967163,
        "learning_rate": 8.955134559758938e-07,
        "epoch": 0.6533273716372308,
        "step": 8767
    },
    {
        "loss": 2.1991,
        "grad_norm": 2.9035825729370117,
        "learning_rate": 8.861421127184133e-07,
        "epoch": 0.6534018928385126,
        "step": 8768
    },
    {
        "loss": 2.2828,
        "grad_norm": 2.6434786319732666,
        "learning_rate": 8.768198439773057e-07,
        "epoch": 0.6534764140397943,
        "step": 8769
    },
    {
        "loss": 2.5899,
        "grad_norm": 1.942583680152893,
        "learning_rate": 8.675466543683785e-07,
        "epoch": 0.6535509352410761,
        "step": 8770
    },
    {
        "loss": 2.1065,
        "grad_norm": 2.8282246589660645,
        "learning_rate": 8.583225484830592e-07,
        "epoch": 0.6536254564423578,
        "step": 8771
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.9218004941940308,
        "learning_rate": 8.491475308885055e-07,
        "epoch": 0.6536999776436396,
        "step": 8772
    },
    {
        "loss": 2.0909,
        "grad_norm": 2.863443374633789,
        "learning_rate": 8.400216061275945e-07,
        "epoch": 0.6537744988449213,
        "step": 8773
    },
    {
        "loss": 2.2307,
        "grad_norm": 3.2161953449249268,
        "learning_rate": 8.309447787188452e-07,
        "epoch": 0.6538490200462032,
        "step": 8774
    },
    {
        "loss": 2.2428,
        "grad_norm": 2.9707086086273193,
        "learning_rate": 8.219170531565179e-07,
        "epoch": 0.6539235412474849,
        "step": 8775
    },
    {
        "loss": 1.7122,
        "grad_norm": 1.5545382499694824,
        "learning_rate": 8.129384339105039e-07,
        "epoch": 0.6539980624487667,
        "step": 8776
    },
    {
        "loss": 2.0799,
        "grad_norm": 3.0568981170654297,
        "learning_rate": 8.040089254264138e-07,
        "epoch": 0.6540725836500484,
        "step": 8777
    },
    {
        "loss": 2.4075,
        "grad_norm": 2.1753478050231934,
        "learning_rate": 7.951285321255775e-07,
        "epoch": 0.6541471048513302,
        "step": 8778
    },
    {
        "loss": 2.5803,
        "grad_norm": 2.133049488067627,
        "learning_rate": 7.862972584049333e-07,
        "epoch": 0.6542216260526119,
        "step": 8779
    },
    {
        "loss": 1.9873,
        "grad_norm": 2.4991862773895264,
        "learning_rate": 7.775151086371279e-07,
        "epoch": 0.6542961472538937,
        "step": 8780
    },
    {
        "loss": 2.2939,
        "grad_norm": 2.762479066848755,
        "learning_rate": 7.687820871705276e-07,
        "epoch": 0.6543706684551756,
        "step": 8781
    },
    {
        "loss": 2.5709,
        "grad_norm": 3.0602867603302,
        "learning_rate": 7.600981983291067e-07,
        "epoch": 0.6544451896564573,
        "step": 8782
    },
    {
        "loss": 2.1494,
        "grad_norm": 3.473940134048462,
        "learning_rate": 7.514634464125703e-07,
        "epoch": 0.6545197108577391,
        "step": 8783
    },
    {
        "loss": 1.5566,
        "grad_norm": 3.2438066005706787,
        "learning_rate": 7.428778356962318e-07,
        "epoch": 0.6545942320590208,
        "step": 8784
    },
    {
        "loss": 1.7944,
        "grad_norm": 2.587139129638672,
        "learning_rate": 7.343413704311353e-07,
        "epoch": 0.6546687532603026,
        "step": 8785
    },
    {
        "loss": 1.8808,
        "grad_norm": 3.3403680324554443,
        "learning_rate": 7.258540548439441e-07,
        "epoch": 0.6547432744615843,
        "step": 8786
    },
    {
        "loss": 2.1688,
        "grad_norm": 2.1448774337768555,
        "learning_rate": 7.174158931370078e-07,
        "epoch": 0.6548177956628661,
        "step": 8787
    },
    {
        "loss": 2.2702,
        "grad_norm": 2.1488702297210693,
        "learning_rate": 7.090268894883734e-07,
        "epoch": 0.6548923168641478,
        "step": 8788
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.977576494216919,
        "learning_rate": 7.006870480516625e-07,
        "epoch": 0.6549668380654297,
        "step": 8789
    },
    {
        "loss": 2.6308,
        "grad_norm": 3.4474070072174072,
        "learning_rate": 6.923963729562277e-07,
        "epoch": 0.6550413592667114,
        "step": 8790
    },
    {
        "loss": 2.5402,
        "grad_norm": 2.242872714996338,
        "learning_rate": 6.84154868307052e-07,
        "epoch": 0.6551158804679932,
        "step": 8791
    },
    {
        "loss": 2.6632,
        "grad_norm": 2.598383903503418,
        "learning_rate": 6.759625381847712e-07,
        "epoch": 0.6551904016692749,
        "step": 8792
    },
    {
        "loss": 2.4098,
        "grad_norm": 2.6848912239074707,
        "learning_rate": 6.67819386645685e-07,
        "epoch": 0.6552649228705567,
        "step": 8793
    },
    {
        "loss": 2.6582,
        "grad_norm": 1.639329433441162,
        "learning_rate": 6.59725417721735e-07,
        "epoch": 0.6553394440718384,
        "step": 8794
    },
    {
        "loss": 1.5548,
        "grad_norm": 3.014993190765381,
        "learning_rate": 6.516806354204819e-07,
        "epoch": 0.6554139652731202,
        "step": 8795
    },
    {
        "loss": 1.9961,
        "grad_norm": 4.19462776184082,
        "learning_rate": 6.436850437251951e-07,
        "epoch": 0.6554884864744019,
        "step": 8796
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.1043221950531006,
        "learning_rate": 6.357386465947301e-07,
        "epoch": 0.6555630076756838,
        "step": 8797
    },
    {
        "loss": 2.259,
        "grad_norm": 4.169013023376465,
        "learning_rate": 6.278414479636286e-07,
        "epoch": 0.6556375288769655,
        "step": 8798
    },
    {
        "loss": 2.2098,
        "grad_norm": 3.3902666568756104,
        "learning_rate": 6.199934517420403e-07,
        "epoch": 0.6557120500782473,
        "step": 8799
    },
    {
        "loss": 2.5613,
        "grad_norm": 1.895868182182312,
        "learning_rate": 6.121946618157459e-07,
        "epoch": 0.655786571279529,
        "step": 8800
    },
    {
        "loss": 2.6222,
        "grad_norm": 2.4221158027648926,
        "learning_rate": 6.044450820462344e-07,
        "epoch": 0.6558610924808108,
        "step": 8801
    },
    {
        "loss": 3.0799,
        "grad_norm": 4.088564872741699,
        "learning_rate": 5.967447162705142e-07,
        "epoch": 0.6559356136820925,
        "step": 8802
    },
    {
        "loss": 2.5463,
        "grad_norm": 3.0054686069488525,
        "learning_rate": 5.890935683013465e-07,
        "epoch": 0.6560101348833743,
        "step": 8803
    },
    {
        "loss": 2.5304,
        "grad_norm": 2.6429450511932373,
        "learning_rate": 5.814916419270344e-07,
        "epoch": 0.656084656084656,
        "step": 8804
    },
    {
        "loss": 2.5443,
        "grad_norm": 2.9964535236358643,
        "learning_rate": 5.739389409115226e-07,
        "epoch": 0.6561591772859379,
        "step": 8805
    },
    {
        "loss": 2.0113,
        "grad_norm": 3.003685474395752,
        "learning_rate": 5.664354689944418e-07,
        "epoch": 0.6562336984872196,
        "step": 8806
    },
    {
        "loss": 1.9843,
        "grad_norm": 2.7544772624969482,
        "learning_rate": 5.589812298909869e-07,
        "epoch": 0.6563082196885014,
        "step": 8807
    },
    {
        "loss": 2.6566,
        "grad_norm": 3.427053213119507,
        "learning_rate": 5.515762272919722e-07,
        "epoch": 0.6563827408897831,
        "step": 8808
    },
    {
        "loss": 2.3823,
        "grad_norm": 2.4014413356781006,
        "learning_rate": 5.442204648638982e-07,
        "epoch": 0.6564572620910649,
        "step": 8809
    },
    {
        "loss": 2.3651,
        "grad_norm": 3.2282960414886475,
        "learning_rate": 5.369139462488182e-07,
        "epoch": 0.6565317832923466,
        "step": 8810
    },
    {
        "loss": 2.7227,
        "grad_norm": 2.745145559310913,
        "learning_rate": 5.296566750644383e-07,
        "epoch": 0.6566063044936284,
        "step": 8811
    },
    {
        "loss": 2.5806,
        "grad_norm": 3.0808651447296143,
        "learning_rate": 5.224486549040842e-07,
        "epoch": 0.6566808256949102,
        "step": 8812
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.6426548957824707,
        "learning_rate": 5.152898893366453e-07,
        "epoch": 0.656755346896192,
        "step": 8813
    },
    {
        "loss": 1.6549,
        "grad_norm": 4.516617298126221,
        "learning_rate": 5.081803819066977e-07,
        "epoch": 0.6568298680974737,
        "step": 8814
    },
    {
        "loss": 2.666,
        "grad_norm": 1.991408109664917,
        "learning_rate": 5.011201361343699e-07,
        "epoch": 0.6569043892987555,
        "step": 8815
    },
    {
        "loss": 2.441,
        "grad_norm": 2.076873302459717,
        "learning_rate": 4.941091555154431e-07,
        "epoch": 0.6569789105000372,
        "step": 8816
    },
    {
        "loss": 3.0061,
        "grad_norm": 2.5690412521362305,
        "learning_rate": 4.87147443521252e-07,
        "epoch": 0.657053431701319,
        "step": 8817
    },
    {
        "loss": 2.1579,
        "grad_norm": 3.1507794857025146,
        "learning_rate": 4.802350035987835e-07,
        "epoch": 0.6571279529026008,
        "step": 8818
    },
    {
        "loss": 2.3825,
        "grad_norm": 2.1477413177490234,
        "learning_rate": 4.733718391706221e-07,
        "epoch": 0.6572024741038826,
        "step": 8819
    },
    {
        "loss": 1.6357,
        "grad_norm": 4.362239360809326,
        "learning_rate": 4.6655795363491626e-07,
        "epoch": 0.6572769953051644,
        "step": 8820
    },
    {
        "loss": 2.0608,
        "grad_norm": 2.1539101600646973,
        "learning_rate": 4.597933503654894e-07,
        "epoch": 0.6573515165064461,
        "step": 8821
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.189274549484253,
        "learning_rate": 4.530780327116846e-07,
        "epoch": 0.6574260377077279,
        "step": 8822
    },
    {
        "loss": 2.6668,
        "grad_norm": 2.05867338180542,
        "learning_rate": 4.464120039984754e-07,
        "epoch": 0.6575005589090096,
        "step": 8823
    },
    {
        "loss": 2.6115,
        "grad_norm": 3.693125009536743,
        "learning_rate": 4.3979526752645495e-07,
        "epoch": 0.6575750801102914,
        "step": 8824
    },
    {
        "loss": 2.293,
        "grad_norm": 2.557990074157715,
        "learning_rate": 4.332278265717582e-07,
        "epoch": 0.6576496013115731,
        "step": 8825
    },
    {
        "loss": 2.3348,
        "grad_norm": 2.107402801513672,
        "learning_rate": 4.2670968438616174e-07,
        "epoch": 0.657724122512855,
        "step": 8826
    },
    {
        "loss": 2.6183,
        "grad_norm": 2.544304132461548,
        "learning_rate": 4.2024084419699514e-07,
        "epoch": 0.6577986437141367,
        "step": 8827
    },
    {
        "loss": 2.7081,
        "grad_norm": 3.009535789489746,
        "learning_rate": 4.138213092071963e-07,
        "epoch": 0.6578731649154185,
        "step": 8828
    },
    {
        "loss": 2.2919,
        "grad_norm": 3.370595932006836,
        "learning_rate": 4.074510825953004e-07,
        "epoch": 0.6579476861167002,
        "step": 8829
    },
    {
        "loss": 2.6446,
        "grad_norm": 2.375535249710083,
        "learning_rate": 4.011301675153956e-07,
        "epoch": 0.658022207317982,
        "step": 8830
    },
    {
        "loss": 2.1746,
        "grad_norm": 3.130103588104248,
        "learning_rate": 3.948585670971894e-07,
        "epoch": 0.6580967285192637,
        "step": 8831
    },
    {
        "loss": 2.1474,
        "grad_norm": 3.527937412261963,
        "learning_rate": 3.886362844459646e-07,
        "epoch": 0.6581712497205455,
        "step": 8832
    },
    {
        "loss": 2.1638,
        "grad_norm": 1.8688533306121826,
        "learning_rate": 3.8246332264254557e-07,
        "epoch": 0.6582457709218272,
        "step": 8833
    },
    {
        "loss": 2.5065,
        "grad_norm": 2.4014029502868652,
        "learning_rate": 3.763396847433875e-07,
        "epoch": 0.658320292123109,
        "step": 8834
    },
    {
        "loss": 2.0453,
        "grad_norm": 3.1157844066619873,
        "learning_rate": 3.702653737805095e-07,
        "epoch": 0.6583948133243908,
        "step": 8835
    },
    {
        "loss": 2.7276,
        "grad_norm": 3.856431484222412,
        "learning_rate": 3.6424039276149456e-07,
        "epoch": 0.6584693345256726,
        "step": 8836
    },
    {
        "loss": 2.3737,
        "grad_norm": 3.66064190864563,
        "learning_rate": 3.5826474466950086e-07,
        "epoch": 0.6585438557269543,
        "step": 8837
    },
    {
        "loss": 2.6813,
        "grad_norm": 2.5567142963409424,
        "learning_rate": 3.523384324632728e-07,
        "epoch": 0.6586183769282361,
        "step": 8838
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.700152635574341,
        "learning_rate": 3.464614590771298e-07,
        "epoch": 0.6586928981295178,
        "step": 8839
    },
    {
        "loss": 2.6786,
        "grad_norm": 2.585533380508423,
        "learning_rate": 3.4063382742095527e-07,
        "epoch": 0.6587674193307996,
        "step": 8840
    },
    {
        "loss": 2.6449,
        "grad_norm": 2.926719903945923,
        "learning_rate": 3.3485554038017453e-07,
        "epoch": 0.6588419405320813,
        "step": 8841
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.9250199794769287,
        "learning_rate": 3.2912660081583226e-07,
        "epoch": 0.6589164617333632,
        "step": 8842
    },
    {
        "loss": 2.3791,
        "grad_norm": 3.8437647819519043,
        "learning_rate": 3.2344701156451497e-07,
        "epoch": 0.6589909829346449,
        "step": 8843
    },
    {
        "loss": 2.1774,
        "grad_norm": 3.9555506706237793,
        "learning_rate": 3.1781677543835096e-07,
        "epoch": 0.6590655041359267,
        "step": 8844
    },
    {
        "loss": 1.9501,
        "grad_norm": 2.6450655460357666,
        "learning_rate": 3.1223589522508813e-07,
        "epoch": 0.6591400253372084,
        "step": 8845
    },
    {
        "loss": 2.6709,
        "grad_norm": 2.7301952838897705,
        "learning_rate": 3.0670437368797154e-07,
        "epoch": 0.6592145465384902,
        "step": 8846
    },
    {
        "loss": 1.3111,
        "grad_norm": 3.642488718032837,
        "learning_rate": 3.012222135658438e-07,
        "epoch": 0.6592890677397719,
        "step": 8847
    },
    {
        "loss": 2.6281,
        "grad_norm": 1.7332069873809814,
        "learning_rate": 2.957894175731113e-07,
        "epoch": 0.6593635889410537,
        "step": 8848
    },
    {
        "loss": 2.0375,
        "grad_norm": 3.260969400405884,
        "learning_rate": 2.9040598839973345e-07,
        "epoch": 0.6594381101423354,
        "step": 8849
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.7313227653503418,
        "learning_rate": 2.850719287112114e-07,
        "epoch": 0.6595126313436173,
        "step": 8850
    },
    {
        "loss": 2.5122,
        "grad_norm": 3.4474151134490967,
        "learning_rate": 2.7978724114861023e-07,
        "epoch": 0.659587152544899,
        "step": 8851
    },
    {
        "loss": 2.026,
        "grad_norm": 3.8802144527435303,
        "learning_rate": 2.7455192832857026e-07,
        "epoch": 0.6596616737461808,
        "step": 8852
    },
    {
        "loss": 2.1897,
        "grad_norm": 4.525922775268555,
        "learning_rate": 2.6936599284325126e-07,
        "epoch": 0.6597361949474626,
        "step": 8853
    },
    {
        "loss": 2.0367,
        "grad_norm": 2.7316572666168213,
        "learning_rate": 2.642294372603771e-07,
        "epoch": 0.6598107161487443,
        "step": 8854
    },
    {
        "loss": 2.2116,
        "grad_norm": 3.9104695320129395,
        "learning_rate": 2.591422641232355e-07,
        "epoch": 0.6598852373500261,
        "step": 8855
    },
    {
        "loss": 2.6482,
        "grad_norm": 2.659339427947998,
        "learning_rate": 2.54104475950645e-07,
        "epoch": 0.6599597585513078,
        "step": 8856
    },
    {
        "loss": 2.0369,
        "grad_norm": 2.9632489681243896,
        "learning_rate": 2.491160752369881e-07,
        "epoch": 0.6600342797525897,
        "step": 8857
    },
    {
        "loss": 2.7804,
        "grad_norm": 2.1293482780456543,
        "learning_rate": 2.4417706445216683e-07,
        "epoch": 0.6601088009538714,
        "step": 8858
    },
    {
        "loss": 2.4413,
        "grad_norm": 2.1755292415618896,
        "learning_rate": 2.3928744604166944e-07,
        "epoch": 0.6601833221551532,
        "step": 8859
    },
    {
        "loss": 2.6407,
        "grad_norm": 3.557185649871826,
        "learning_rate": 2.3444722242649265e-07,
        "epoch": 0.6602578433564349,
        "step": 8860
    },
    {
        "loss": 2.3273,
        "grad_norm": 4.022045612335205,
        "learning_rate": 2.2965639600318612e-07,
        "epoch": 0.6603323645577167,
        "step": 8861
    },
    {
        "loss": 1.8711,
        "grad_norm": 2.844965696334839,
        "learning_rate": 2.2491496914386346e-07,
        "epoch": 0.6604068857589984,
        "step": 8862
    },
    {
        "loss": 2.3741,
        "grad_norm": 1.8521760702133179,
        "learning_rate": 2.2022294419613565e-07,
        "epoch": 0.6604814069602802,
        "step": 8863
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.5208425521850586,
        "learning_rate": 2.155803234831999e-07,
        "epoch": 0.660555928161562,
        "step": 8864
    },
    {
        "loss": 2.579,
        "grad_norm": 2.5819857120513916,
        "learning_rate": 2.1098710930376188e-07,
        "epoch": 0.6606304493628438,
        "step": 8865
    },
    {
        "loss": 3.2784,
        "grad_norm": 2.7745983600616455,
        "learning_rate": 2.06443303932069e-07,
        "epoch": 0.6607049705641255,
        "step": 8866
    },
    {
        "loss": 2.8445,
        "grad_norm": 2.1019093990325928,
        "learning_rate": 2.0194890961791058e-07,
        "epoch": 0.6607794917654073,
        "step": 8867
    },
    {
        "loss": 1.7148,
        "grad_norm": 2.6710259914398193,
        "learning_rate": 1.9750392858659538e-07,
        "epoch": 0.660854012966689,
        "step": 8868
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.9301109313964844,
        "learning_rate": 1.9310836303900738e-07,
        "epoch": 0.6609285341679708,
        "step": 8869
    },
    {
        "loss": 2.6047,
        "grad_norm": 3.3927173614501953,
        "learning_rate": 1.8876221515151672e-07,
        "epoch": 0.6610030553692525,
        "step": 8870
    },
    {
        "loss": 2.1027,
        "grad_norm": 3.9853920936584473,
        "learning_rate": 1.844654870760354e-07,
        "epoch": 0.6610775765705343,
        "step": 8871
    },
    {
        "loss": 2.6091,
        "grad_norm": 3.3150477409362793,
        "learning_rate": 1.8021818094003939e-07,
        "epoch": 0.661152097771816,
        "step": 8872
    },
    {
        "loss": 2.0258,
        "grad_norm": 3.4164135456085205,
        "learning_rate": 1.7602029884649096e-07,
        "epoch": 0.6612266189730979,
        "step": 8873
    },
    {
        "loss": 2.7807,
        "grad_norm": 2.7632944583892822,
        "learning_rate": 1.7187184287389414e-07,
        "epoch": 0.6613011401743796,
        "step": 8874
    },
    {
        "loss": 1.1611,
        "grad_norm": 5.365356922149658,
        "learning_rate": 1.6777281507630583e-07,
        "epoch": 0.6613756613756614,
        "step": 8875
    },
    {
        "loss": 2.437,
        "grad_norm": 2.7180206775665283,
        "learning_rate": 1.6372321748326923e-07,
        "epoch": 0.6614501825769431,
        "step": 8876
    },
    {
        "loss": 2.5713,
        "grad_norm": 2.440253257751465,
        "learning_rate": 1.5972305209988047e-07,
        "epoch": 0.6615247037782249,
        "step": 8877
    },
    {
        "loss": 2.1225,
        "grad_norm": 3.6944022178649902,
        "learning_rate": 1.557723209067663e-07,
        "epoch": 0.6615992249795066,
        "step": 8878
    },
    {
        "loss": 1.9383,
        "grad_norm": 4.357394695281982,
        "learning_rate": 1.5187102586002866e-07,
        "epoch": 0.6616737461807884,
        "step": 8879
    },
    {
        "loss": 2.4882,
        "grad_norm": 2.0166015625,
        "learning_rate": 1.480191688913557e-07,
        "epoch": 0.6617482673820702,
        "step": 8880
    },
    {
        "loss": 2.3482,
        "grad_norm": 2.4552125930786133,
        "learning_rate": 1.4421675190791074e-07,
        "epoch": 0.661822788583352,
        "step": 8881
    },
    {
        "loss": 2.5348,
        "grad_norm": 3.1629152297973633,
        "learning_rate": 1.4046377679240995e-07,
        "epoch": 0.6618973097846337,
        "step": 8882
    },
    {
        "loss": 2.6973,
        "grad_norm": 3.417813777923584,
        "learning_rate": 1.3676024540307808e-07,
        "epoch": 0.6619718309859155,
        "step": 8883
    },
    {
        "loss": 2.5124,
        "grad_norm": 3.8744916915893555,
        "learning_rate": 1.3310615957362603e-07,
        "epoch": 0.6620463521871972,
        "step": 8884
    },
    {
        "loss": 2.4044,
        "grad_norm": 3.0854365825653076,
        "learning_rate": 1.2950152111333992e-07,
        "epoch": 0.662120873388479,
        "step": 8885
    },
    {
        "loss": 1.7355,
        "grad_norm": 2.9294557571411133,
        "learning_rate": 1.259463318069809e-07,
        "epoch": 0.6621953945897607,
        "step": 8886
    },
    {
        "loss": 2.4718,
        "grad_norm": 3.1141610145568848,
        "learning_rate": 1.2244059341484093e-07,
        "epoch": 0.6622699157910426,
        "step": 8887
    },
    {
        "loss": 1.888,
        "grad_norm": 3.702687978744507,
        "learning_rate": 1.189843076727315e-07,
        "epoch": 0.6623444369923244,
        "step": 8888
    },
    {
        "loss": 2.6276,
        "grad_norm": 2.9069907665252686,
        "learning_rate": 1.1557747629196147e-07,
        "epoch": 0.6624189581936061,
        "step": 8889
    },
    {
        "loss": 2.8188,
        "grad_norm": 3.685037136077881,
        "learning_rate": 1.1222010095938152e-07,
        "epoch": 0.6624934793948879,
        "step": 8890
    },
    {
        "loss": 1.1976,
        "grad_norm": 1.728982925415039,
        "learning_rate": 1.0891218333732856e-07,
        "epoch": 0.6625680005961696,
        "step": 8891
    },
    {
        "loss": 2.4016,
        "grad_norm": 2.351816415786743,
        "learning_rate": 1.0565372506365911e-07,
        "epoch": 0.6626425217974514,
        "step": 8892
    },
    {
        "loss": 1.5279,
        "grad_norm": 2.0026204586029053,
        "learning_rate": 1.0244472775173818e-07,
        "epoch": 0.6627170429987331,
        "step": 8893
    },
    {
        "loss": 2.5568,
        "grad_norm": 2.4288928508758545,
        "learning_rate": 9.928519299046146e-08,
        "epoch": 0.662791564200015,
        "step": 8894
    },
    {
        "loss": 2.501,
        "grad_norm": 2.750194787979126,
        "learning_rate": 9.617512234419978e-08,
        "epoch": 0.6628660854012967,
        "step": 8895
    },
    {
        "loss": 2.1417,
        "grad_norm": 3.390842914581299,
        "learning_rate": 9.31145173528658e-08,
        "epoch": 0.6629406066025785,
        "step": 8896
    },
    {
        "loss": 2.369,
        "grad_norm": 2.714329957962036,
        "learning_rate": 9.010337953185843e-08,
        "epoch": 0.6630151278038602,
        "step": 8897
    },
    {
        "loss": 2.6411,
        "grad_norm": 2.579949140548706,
        "learning_rate": 8.714171037208508e-08,
        "epoch": 0.663089649005142,
        "step": 8898
    },
    {
        "loss": 2.7636,
        "grad_norm": 2.210540771484375,
        "learning_rate": 8.42295113399727e-08,
        "epoch": 0.6631641702064237,
        "step": 8899
    },
    {
        "loss": 2.1155,
        "grad_norm": 1.9586354494094849,
        "learning_rate": 8.136678387744568e-08,
        "epoch": 0.6632386914077055,
        "step": 8900
    },
    {
        "loss": 2.5642,
        "grad_norm": 2.5909457206726074,
        "learning_rate": 7.855352940193683e-08,
        "epoch": 0.6633132126089872,
        "step": 8901
    },
    {
        "loss": 2.1675,
        "grad_norm": 3.302750825881958,
        "learning_rate": 7.578974930637639e-08,
        "epoch": 0.663387733810269,
        "step": 8902
    },
    {
        "loss": 2.8987,
        "grad_norm": 2.600320339202881,
        "learning_rate": 7.307544495919193e-08,
        "epoch": 0.6634622550115508,
        "step": 8903
    },
    {
        "loss": 2.4665,
        "grad_norm": 2.318629741668701,
        "learning_rate": 7.041061770435286e-08,
        "epoch": 0.6635367762128326,
        "step": 8904
    },
    {
        "loss": 2.262,
        "grad_norm": 2.49822735786438,
        "learning_rate": 6.77952688612704e-08,
        "epoch": 0.6636112974141143,
        "step": 8905
    },
    {
        "loss": 2.1777,
        "grad_norm": 3.448793649673462,
        "learning_rate": 6.522939972491982e-08,
        "epoch": 0.6636858186153961,
        "step": 8906
    },
    {
        "loss": 2.7191,
        "grad_norm": 2.0172994136810303,
        "learning_rate": 6.271301156571818e-08,
        "epoch": 0.6637603398166778,
        "step": 8907
    },
    {
        "loss": 1.1517,
        "grad_norm": 1.9957526922225952,
        "learning_rate": 6.024610562962441e-08,
        "epoch": 0.6638348610179596,
        "step": 8908
    },
    {
        "loss": 2.392,
        "grad_norm": 3.3091750144958496,
        "learning_rate": 5.782868313808365e-08,
        "epoch": 0.6639093822192413,
        "step": 8909
    },
    {
        "loss": 1.987,
        "grad_norm": 2.935772657394409,
        "learning_rate": 5.5460745288049566e-08,
        "epoch": 0.6639839034205232,
        "step": 8910
    },
    {
        "loss": 1.7656,
        "grad_norm": 2.975727081298828,
        "learning_rate": 5.314229325196207e-08,
        "epoch": 0.6640584246218049,
        "step": 8911
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.886308193206787,
        "learning_rate": 5.087332817774737e-08,
        "epoch": 0.6641329458230867,
        "step": 8912
    },
    {
        "loss": 2.2714,
        "grad_norm": 2.404116153717041,
        "learning_rate": 4.865385118886234e-08,
        "epoch": 0.6642074670243684,
        "step": 8913
    },
    {
        "loss": 2.4417,
        "grad_norm": 1.9471062421798706,
        "learning_rate": 4.648386338422794e-08,
        "epoch": 0.6642819882256502,
        "step": 8914
    },
    {
        "loss": 2.4912,
        "grad_norm": 2.708237648010254,
        "learning_rate": 4.4363365838295814e-08,
        "epoch": 0.6643565094269319,
        "step": 8915
    },
    {
        "loss": 2.2413,
        "grad_norm": 3.6933839321136475,
        "learning_rate": 4.2292359600970555e-08,
        "epoch": 0.6644310306282137,
        "step": 8916
    },
    {
        "loss": 2.7425,
        "grad_norm": 2.8671622276306152,
        "learning_rate": 4.027084569769857e-08,
        "epoch": 0.6645055518294954,
        "step": 8917
    },
    {
        "loss": 1.9991,
        "grad_norm": 3.2282371520996094,
        "learning_rate": 3.829882512937921e-08,
        "epoch": 0.6645800730307773,
        "step": 8918
    },
    {
        "loss": 2.9575,
        "grad_norm": 2.5538485050201416,
        "learning_rate": 3.6376298872420335e-08,
        "epoch": 0.664654594232059,
        "step": 8919
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.7913360595703125,
        "learning_rate": 3.4503267878760456e-08,
        "epoch": 0.6647291154333408,
        "step": 8920
    },
    {
        "loss": 2.0113,
        "grad_norm": 2.2918386459350586,
        "learning_rate": 3.2679733075757776e-08,
        "epoch": 0.6648036366346225,
        "step": 8921
    },
    {
        "loss": 0.7418,
        "grad_norm": 3.6160247325897217,
        "learning_rate": 3.0905695366334475e-08,
        "epoch": 0.6648781578359043,
        "step": 8922
    },
    {
        "loss": 2.3981,
        "grad_norm": 1.9311295747756958,
        "learning_rate": 2.9181155628854596e-08,
        "epoch": 0.6649526790371861,
        "step": 8923
    },
    {
        "loss": 1.9838,
        "grad_norm": 2.4091014862060547,
        "learning_rate": 2.750611471721287e-08,
        "epoch": 0.6650272002384678,
        "step": 8924
    },
    {
        "loss": 2.8262,
        "grad_norm": 3.7912731170654297,
        "learning_rate": 2.5880573460757007e-08,
        "epoch": 0.6651017214397497,
        "step": 8925
    },
    {
        "loss": 1.9279,
        "grad_norm": 3.905301094055176,
        "learning_rate": 2.4304532664365387e-08,
        "epoch": 0.6651762426410314,
        "step": 8926
    },
    {
        "loss": 2.3453,
        "grad_norm": 1.836064100265503,
        "learning_rate": 2.2777993108369367e-08,
        "epoch": 0.6652507638423132,
        "step": 8927
    },
    {
        "loss": 2.2256,
        "grad_norm": 3.770876884460449,
        "learning_rate": 2.1300955548619884e-08,
        "epoch": 0.6653252850435949,
        "step": 8928
    },
    {
        "loss": 2.6159,
        "grad_norm": 3.020796537399292,
        "learning_rate": 1.9873420716431945e-08,
        "epoch": 0.6653998062448767,
        "step": 8929
    },
    {
        "loss": 2.1933,
        "grad_norm": 3.7252461910247803,
        "learning_rate": 1.8495389318651246e-08,
        "epoch": 0.6654743274461584,
        "step": 8930
    },
    {
        "loss": 2.3126,
        "grad_norm": 2.684485912322998,
        "learning_rate": 1.7166862037565345e-08,
        "epoch": 0.6655488486474402,
        "step": 8931
    },
    {
        "loss": 2.6408,
        "grad_norm": 4.216660499572754,
        "learning_rate": 1.5887839530981385e-08,
        "epoch": 0.665623369848722,
        "step": 8932
    },
    {
        "loss": 2.1564,
        "grad_norm": 2.978492498397827,
        "learning_rate": 1.465832243217058e-08,
        "epoch": 0.6656978910500038,
        "step": 8933
    },
    {
        "loss": 2.7214,
        "grad_norm": 1.7687429189682007,
        "learning_rate": 1.3478311349934824e-08,
        "epoch": 0.6657724122512855,
        "step": 8934
    },
    {
        "loss": 2.5918,
        "grad_norm": 5.106114864349365,
        "learning_rate": 1.2347806868506784e-08,
        "epoch": 0.6658469334525673,
        "step": 8935
    },
    {
        "loss": 2.315,
        "grad_norm": 3.113962411880493,
        "learning_rate": 1.1266809547649803e-08,
        "epoch": 0.665921454653849,
        "step": 8936
    },
    {
        "loss": 2.3149,
        "grad_norm": 2.7299516201019287,
        "learning_rate": 1.0235319922602404e-08,
        "epoch": 0.6659959758551308,
        "step": 8937
    },
    {
        "loss": 2.4592,
        "grad_norm": 2.522122383117676,
        "learning_rate": 9.253338504089381e-09,
        "epoch": 0.6660704970564125,
        "step": 8938
    },
    {
        "loss": 1.7352,
        "grad_norm": 4.082946300506592,
        "learning_rate": 8.320865778310705e-09,
        "epoch": 0.6661450182576943,
        "step": 8939
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.7740626335144043,
        "learning_rate": 7.437902206985925e-09,
        "epoch": 0.666219539458976,
        "step": 8940
    },
    {
        "loss": 2.1867,
        "grad_norm": 3.07118558883667,
        "learning_rate": 6.604448227276461e-09,
        "epoch": 0.6662940606602579,
        "step": 8941
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.9784276485443115,
        "learning_rate": 5.8205042518633124e-09,
        "epoch": 0.6663685818615396,
        "step": 8942
    },
    {
        "loss": 2.6295,
        "grad_norm": 3.276113748550415,
        "learning_rate": 5.086070668902654e-09,
        "epoch": 0.6664431030628214,
        "step": 8943
    },
    {
        "loss": 2.4411,
        "grad_norm": 2.7324111461639404,
        "learning_rate": 4.4011478420258325e-09,
        "epoch": 0.6665176242641031,
        "step": 8944
    },
    {
        "loss": 2.05,
        "grad_norm": 1.3815431594848633,
        "learning_rate": 3.7657361103837776e-09,
        "epoch": 0.6665921454653849,
        "step": 8945
    },
    {
        "loss": 2.8113,
        "grad_norm": 1.898566484451294,
        "learning_rate": 3.1798357885692854e-09,
        "epoch": 0.6666666666666666,
        "step": 8946
    },
    {
        "loss": 2.5165,
        "grad_norm": 2.5058884620666504,
        "learning_rate": 2.6434471666947348e-09,
        "epoch": 0.6667411878679484,
        "step": 8947
    },
    {
        "loss": 2.075,
        "grad_norm": 4.503697395324707,
        "learning_rate": 2.156570510325473e-09,
        "epoch": 0.6668157090692302,
        "step": 8948
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.5975148677825928,
        "learning_rate": 1.71920606054643e-09,
        "epoch": 0.666890230270512,
        "step": 8949
    },
    {
        "loss": 2.8608,
        "grad_norm": 2.147272825241089,
        "learning_rate": 1.3313540339066067e-09,
        "epoch": 0.6669647514717937,
        "step": 8950
    },
    {
        "loss": 2.3187,
        "grad_norm": 2.8520383834838867,
        "learning_rate": 9.930146224412796e-10,
        "epoch": 0.6670392726730755,
        "step": 8951
    },
    {
        "loss": 2.5086,
        "grad_norm": 2.1513328552246094,
        "learning_rate": 7.041879936720008e-10,
        "epoch": 0.6671137938743572,
        "step": 8952
    },
    {
        "loss": 2.5275,
        "grad_norm": 2.3015897274017334,
        "learning_rate": 4.648742906177006e-10,
        "epoch": 0.667188315075639,
        "step": 8953
    },
    {
        "loss": 2.559,
        "grad_norm": 2.2147693634033203,
        "learning_rate": 2.7507363176138e-10,
        "epoch": 0.6672628362769207,
        "step": 8954
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.8796195983886719,
        "learning_rate": 1.3478611107231587e-10,
        "epoch": 0.6673373574782026,
        "step": 8955
    },
    {
        "loss": 2.0044,
        "grad_norm": 3.530611276626587,
        "learning_rate": 4.401179802826505e-11,
        "epoch": 0.6674118786794843,
        "step": 8956
    },
    {
        "loss": 2.15,
        "grad_norm": 2.509719133377075,
        "learning_rate": 2.7507375710555948e-12,
        "epoch": 0.6674863998807661,
        "step": 8957
    },
    {
        "loss": 2.5856,
        "grad_norm": 1.5612691640853882,
        "learning_rate": 0.0001999999889970499,
        "epoch": 0.6675609210820479,
        "step": 8958
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.501265287399292,
        "learning_rate": 0.00019999993123156844,
        "epoch": 0.6676354422833296,
        "step": 8959
    },
    {
        "loss": 2.4333,
        "grad_norm": 2.6037003993988037,
        "learning_rate": 0.00019999982395284662,
        "epoch": 0.6677099634846114,
        "step": 8960
    },
    {
        "loss": 2.4136,
        "grad_norm": 2.6915526390075684,
        "learning_rate": 0.0001999996671609376,
        "epoch": 0.6677844846858931,
        "step": 8961
    },
    {
        "loss": 3.0513,
        "grad_norm": 2.2690036296844482,
        "learning_rate": 0.00019999946085591904,
        "epoch": 0.667859005887175,
        "step": 8962
    },
    {
        "loss": 2.6617,
        "grad_norm": 2.4995980262756348,
        "learning_rate": 0.00019999920503789304,
        "epoch": 0.6679335270884567,
        "step": 8963
    },
    {
        "loss": 2.4488,
        "grad_norm": 3.0140292644500732,
        "learning_rate": 0.0001999988997069863,
        "epoch": 0.6680080482897385,
        "step": 8964
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.6040290594100952,
        "learning_rate": 0.00019999854486334995,
        "epoch": 0.6680825694910202,
        "step": 8965
    },
    {
        "loss": 2.0875,
        "grad_norm": 3.4547040462493896,
        "learning_rate": 0.00019999814050715968,
        "epoch": 0.668157090692302,
        "step": 8966
    },
    {
        "loss": 2.2904,
        "grad_norm": 5.364769458770752,
        "learning_rate": 0.00019999768663861577,
        "epoch": 0.6682316118935837,
        "step": 8967
    },
    {
        "loss": 2.625,
        "grad_norm": 2.325300455093384,
        "learning_rate": 0.0001999971832579429,
        "epoch": 0.6683061330948655,
        "step": 8968
    },
    {
        "loss": 2.7135,
        "grad_norm": 3.5738487243652344,
        "learning_rate": 0.00019999663036539034,
        "epoch": 0.6683806542961472,
        "step": 8969
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.0443410873413086,
        "learning_rate": 0.0001999960279612318,
        "epoch": 0.668455175497429,
        "step": 8970
    },
    {
        "loss": 2.1877,
        "grad_norm": 4.793271064758301,
        "learning_rate": 0.00019999537604576556,
        "epoch": 0.6685296966987108,
        "step": 8971
    },
    {
        "loss": 1.7014,
        "grad_norm": 1.5718659162521362,
        "learning_rate": 0.00019999467461931448,
        "epoch": 0.6686042178999926,
        "step": 8972
    },
    {
        "loss": 2.1218,
        "grad_norm": 4.186625957489014,
        "learning_rate": 0.00019999392368222574,
        "epoch": 0.6686787391012743,
        "step": 8973
    },
    {
        "loss": 2.5898,
        "grad_norm": 2.621978521347046,
        "learning_rate": 0.0001999931232348712,
        "epoch": 0.6687532603025561,
        "step": 8974
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.7106282711029053,
        "learning_rate": 0.00019999227327764722,
        "epoch": 0.6688277815038378,
        "step": 8975
    },
    {
        "loss": 2.5839,
        "grad_norm": 2.260075807571411,
        "learning_rate": 0.00019999137381097464,
        "epoch": 0.6689023027051196,
        "step": 8976
    },
    {
        "loss": 1.7392,
        "grad_norm": 1.9679049253463745,
        "learning_rate": 0.00019999042483529876,
        "epoch": 0.6689768239064013,
        "step": 8977
    },
    {
        "loss": 1.9387,
        "grad_norm": 3.141162395477295,
        "learning_rate": 0.00019998942635108952,
        "epoch": 0.6690513451076832,
        "step": 8978
    },
    {
        "loss": 1.4461,
        "grad_norm": 3.7046618461608887,
        "learning_rate": 0.00019998837835884123,
        "epoch": 0.6691258663089649,
        "step": 8979
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.1868698596954346,
        "learning_rate": 0.00019998728085907286,
        "epoch": 0.6692003875102467,
        "step": 8980
    },
    {
        "loss": 2.1097,
        "grad_norm": 4.166487693786621,
        "learning_rate": 0.00019998613385232775,
        "epoch": 0.6692749087115284,
        "step": 8981
    },
    {
        "loss": 2.6241,
        "grad_norm": 2.634984254837036,
        "learning_rate": 0.00019998493733917384,
        "epoch": 0.6693494299128102,
        "step": 8982
    },
    {
        "loss": 1.6608,
        "grad_norm": 3.22670841217041,
        "learning_rate": 0.0001999836913202036,
        "epoch": 0.6694239511140919,
        "step": 8983
    },
    {
        "loss": 2.8137,
        "grad_norm": 2.587273597717285,
        "learning_rate": 0.00019998239579603394,
        "epoch": 0.6694984723153737,
        "step": 8984
    },
    {
        "loss": 2.7946,
        "grad_norm": 2.939432382583618,
        "learning_rate": 0.0001999810507673063,
        "epoch": 0.6695729935166554,
        "step": 8985
    },
    {
        "loss": 2.2383,
        "grad_norm": 2.659560203552246,
        "learning_rate": 0.00019997965623468674,
        "epoch": 0.6696475147179373,
        "step": 8986
    },
    {
        "loss": 2.6402,
        "grad_norm": 2.4451961517333984,
        "learning_rate": 0.0001999782121988656,
        "epoch": 0.669722035919219,
        "step": 8987
    },
    {
        "loss": 2.2491,
        "grad_norm": 2.8670151233673096,
        "learning_rate": 0.00019997671866055797,
        "epoch": 0.6697965571205008,
        "step": 8988
    },
    {
        "loss": 2.0883,
        "grad_norm": 3.2369706630706787,
        "learning_rate": 0.00019997517562050332,
        "epoch": 0.6698710783217825,
        "step": 8989
    },
    {
        "loss": 2.1426,
        "grad_norm": 3.390280246734619,
        "learning_rate": 0.00019997358307946564,
        "epoch": 0.6699455995230643,
        "step": 8990
    },
    {
        "loss": 3.4045,
        "grad_norm": 2.7040958404541016,
        "learning_rate": 0.0001999719410382335,
        "epoch": 0.670020120724346,
        "step": 8991
    },
    {
        "loss": 2.3504,
        "grad_norm": 3.789738655090332,
        "learning_rate": 0.00019997024949761985,
        "epoch": 0.6700946419256278,
        "step": 8992
    },
    {
        "loss": 2.1535,
        "grad_norm": 2.1251277923583984,
        "learning_rate": 0.00019996850845846234,
        "epoch": 0.6701691631269096,
        "step": 8993
    },
    {
        "loss": 2.3807,
        "grad_norm": 4.096670627593994,
        "learning_rate": 0.00019996671792162288,
        "epoch": 0.6702436843281914,
        "step": 8994
    },
    {
        "loss": 2.0926,
        "grad_norm": 2.924710750579834,
        "learning_rate": 0.00019996487788798815,
        "epoch": 0.6703182055294732,
        "step": 8995
    },
    {
        "loss": 2.3089,
        "grad_norm": 3.661661386489868,
        "learning_rate": 0.00019996298835846913,
        "epoch": 0.6703927267307549,
        "step": 8996
    },
    {
        "loss": 2.3612,
        "grad_norm": 1.9611711502075195,
        "learning_rate": 0.00019996104933400142,
        "epoch": 0.6704672479320367,
        "step": 8997
    },
    {
        "loss": 2.8537,
        "grad_norm": 1.9220610857009888,
        "learning_rate": 0.00019995906081554508,
        "epoch": 0.6705417691333184,
        "step": 8998
    },
    {
        "loss": 1.9431,
        "grad_norm": 3.0940279960632324,
        "learning_rate": 0.0001999570228040847,
        "epoch": 0.6706162903346002,
        "step": 8999
    },
    {
        "loss": 2.0005,
        "grad_norm": 2.606288194656372,
        "learning_rate": 0.00019995493530062936,
        "epoch": 0.670690811535882,
        "step": 9000
    },
    {
        "loss": 3.306,
        "grad_norm": 3.133445978164673,
        "learning_rate": 0.00019995279830621265,
        "epoch": 0.6707653327371638,
        "step": 9001
    },
    {
        "loss": 2.7247,
        "grad_norm": 4.771549701690674,
        "learning_rate": 0.00019995061182189266,
        "epoch": 0.6708398539384455,
        "step": 9002
    },
    {
        "loss": 1.9755,
        "grad_norm": 2.959629774093628,
        "learning_rate": 0.00019994837584875203,
        "epoch": 0.6709143751397273,
        "step": 9003
    },
    {
        "loss": 2.79,
        "grad_norm": 3.10089111328125,
        "learning_rate": 0.00019994609038789782,
        "epoch": 0.670988896341009,
        "step": 9004
    },
    {
        "loss": 1.9571,
        "grad_norm": 3.856928586959839,
        "learning_rate": 0.00019994375544046166,
        "epoch": 0.6710634175422908,
        "step": 9005
    },
    {
        "loss": 1.6739,
        "grad_norm": 1.9167736768722534,
        "learning_rate": 0.00019994137100759964,
        "epoch": 0.6711379387435725,
        "step": 9006
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.9950724840164185,
        "learning_rate": 0.00019993893709049237,
        "epoch": 0.6712124599448543,
        "step": 9007
    },
    {
        "loss": 2.6299,
        "grad_norm": 3.6952669620513916,
        "learning_rate": 0.00019993645369034498,
        "epoch": 0.671286981146136,
        "step": 9008
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.101640224456787,
        "learning_rate": 0.0001999339208083871,
        "epoch": 0.6713615023474179,
        "step": 9009
    },
    {
        "loss": 2.0272,
        "grad_norm": 3.3212413787841797,
        "learning_rate": 0.0001999313384458728,
        "epoch": 0.6714360235486996,
        "step": 9010
    },
    {
        "loss": 2.5444,
        "grad_norm": 3.105552911758423,
        "learning_rate": 0.0001999287066040807,
        "epoch": 0.6715105447499814,
        "step": 9011
    },
    {
        "loss": 2.142,
        "grad_norm": 2.9160470962524414,
        "learning_rate": 0.00019992602528431393,
        "epoch": 0.6715850659512631,
        "step": 9012
    },
    {
        "loss": 2.3765,
        "grad_norm": 2.8769772052764893,
        "learning_rate": 0.0001999232944879001,
        "epoch": 0.6716595871525449,
        "step": 9013
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.9191198348999023,
        "learning_rate": 0.0001999205142161913,
        "epoch": 0.6717341083538266,
        "step": 9014
    },
    {
        "loss": 2.6206,
        "grad_norm": 2.447453737258911,
        "learning_rate": 0.0001999176844705641,
        "epoch": 0.6718086295551084,
        "step": 9015
    },
    {
        "loss": 2.5301,
        "grad_norm": 2.260845422744751,
        "learning_rate": 0.00019991480525241971,
        "epoch": 0.6718831507563902,
        "step": 9016
    },
    {
        "loss": 1.9966,
        "grad_norm": 3.379145622253418,
        "learning_rate": 0.0001999118765631836,
        "epoch": 0.671957671957672,
        "step": 9017
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.4958133697509766,
        "learning_rate": 0.00019990889840430594,
        "epoch": 0.6720321931589537,
        "step": 9018
    },
    {
        "loss": 2.2686,
        "grad_norm": 2.947178602218628,
        "learning_rate": 0.00019990587077726128,
        "epoch": 0.6721067143602355,
        "step": 9019
    },
    {
        "loss": 3.1169,
        "grad_norm": 4.128155708312988,
        "learning_rate": 0.00019990279368354875,
        "epoch": 0.6721812355615172,
        "step": 9020
    },
    {
        "loss": 2.8622,
        "grad_norm": 3.138359308242798,
        "learning_rate": 0.00019989966712469182,
        "epoch": 0.672255756762799,
        "step": 9021
    },
    {
        "loss": 2.7546,
        "grad_norm": 2.048372983932495,
        "learning_rate": 0.00019989649110223865,
        "epoch": 0.6723302779640807,
        "step": 9022
    },
    {
        "loss": 2.2583,
        "grad_norm": 2.820077896118164,
        "learning_rate": 0.0001998932656177617,
        "epoch": 0.6724047991653626,
        "step": 9023
    },
    {
        "loss": 2.0764,
        "grad_norm": 2.124349355697632,
        "learning_rate": 0.0001998899906728581,
        "epoch": 0.6724793203666443,
        "step": 9024
    },
    {
        "loss": 2.1255,
        "grad_norm": 3.2947933673858643,
        "learning_rate": 0.00019988666626914934,
        "epoch": 0.6725538415679261,
        "step": 9025
    },
    {
        "loss": 2.2198,
        "grad_norm": 3.4095799922943115,
        "learning_rate": 0.00019988329240828146,
        "epoch": 0.6726283627692078,
        "step": 9026
    },
    {
        "loss": 2.5916,
        "grad_norm": 1.6237313747406006,
        "learning_rate": 0.00019987986909192492,
        "epoch": 0.6727028839704896,
        "step": 9027
    },
    {
        "loss": 2.6482,
        "grad_norm": 1.8962249755859375,
        "learning_rate": 0.0001998763963217748,
        "epoch": 0.6727774051717713,
        "step": 9028
    },
    {
        "loss": 2.2414,
        "grad_norm": 4.193851947784424,
        "learning_rate": 0.0001998728740995505,
        "epoch": 0.6728519263730531,
        "step": 9029
    },
    {
        "loss": 1.9137,
        "grad_norm": 3.0129284858703613,
        "learning_rate": 0.00019986930242699605,
        "epoch": 0.672926447574335,
        "step": 9030
    },
    {
        "loss": 2.799,
        "grad_norm": 2.246419668197632,
        "learning_rate": 0.00019986568130587985,
        "epoch": 0.6730009687756167,
        "step": 9031
    },
    {
        "loss": 2.4068,
        "grad_norm": 3.679323673248291,
        "learning_rate": 0.0001998620107379949,
        "epoch": 0.6730754899768985,
        "step": 9032
    },
    {
        "loss": 2.5198,
        "grad_norm": 3.7419917583465576,
        "learning_rate": 0.00019985829072515854,
        "epoch": 0.6731500111781802,
        "step": 9033
    },
    {
        "loss": 2.0077,
        "grad_norm": 4.273766040802002,
        "learning_rate": 0.00019985452126921268,
        "epoch": 0.673224532379462,
        "step": 9034
    },
    {
        "loss": 2.5334,
        "grad_norm": 2.03114914894104,
        "learning_rate": 0.0001998507023720238,
        "epoch": 0.6732990535807437,
        "step": 9035
    },
    {
        "loss": 2.7678,
        "grad_norm": 2.7480499744415283,
        "learning_rate": 0.00019984683403548263,
        "epoch": 0.6733735747820255,
        "step": 9036
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.1885716915130615,
        "learning_rate": 0.00019984291626150462,
        "epoch": 0.6734480959833072,
        "step": 9037
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.7924752235412598,
        "learning_rate": 0.00019983894905202952,
        "epoch": 0.6735226171845891,
        "step": 9038
    },
    {
        "loss": 2.638,
        "grad_norm": 2.291718006134033,
        "learning_rate": 0.00019983493240902163,
        "epoch": 0.6735971383858708,
        "step": 9039
    },
    {
        "loss": 2.7881,
        "grad_norm": 2.447213888168335,
        "learning_rate": 0.00019983086633446973,
        "epoch": 0.6736716595871526,
        "step": 9040
    },
    {
        "loss": 1.8143,
        "grad_norm": 3.3475325107574463,
        "learning_rate": 0.00019982675083038708,
        "epoch": 0.6737461807884343,
        "step": 9041
    },
    {
        "loss": 2.1294,
        "grad_norm": 3.8780434131622314,
        "learning_rate": 0.00019982258589881143,
        "epoch": 0.6738207019897161,
        "step": 9042
    },
    {
        "loss": 2.4301,
        "grad_norm": 1.814116358757019,
        "learning_rate": 0.00019981837154180488,
        "epoch": 0.6738952231909978,
        "step": 9043
    },
    {
        "loss": 2.3287,
        "grad_norm": 2.4237048625946045,
        "learning_rate": 0.0001998141077614542,
        "epoch": 0.6739697443922796,
        "step": 9044
    },
    {
        "loss": 2.7172,
        "grad_norm": 3.961961269378662,
        "learning_rate": 0.00019980979455987046,
        "epoch": 0.6740442655935613,
        "step": 9045
    },
    {
        "loss": 2.7529,
        "grad_norm": 2.1348257064819336,
        "learning_rate": 0.00019980543193918928,
        "epoch": 0.6741187867948432,
        "step": 9046
    },
    {
        "loss": 2.4829,
        "grad_norm": 2.5871403217315674,
        "learning_rate": 0.00019980101990157073,
        "epoch": 0.6741933079961249,
        "step": 9047
    },
    {
        "loss": 2.4833,
        "grad_norm": 2.942789077758789,
        "learning_rate": 0.00019979655844919942,
        "epoch": 0.6742678291974067,
        "step": 9048
    },
    {
        "loss": 1.9455,
        "grad_norm": 2.23050594329834,
        "learning_rate": 0.0001997920475842843,
        "epoch": 0.6743423503986884,
        "step": 9049
    },
    {
        "loss": 2.2283,
        "grad_norm": 3.1058363914489746,
        "learning_rate": 0.00019978748730905882,
        "epoch": 0.6744168715999702,
        "step": 9050
    },
    {
        "loss": 2.8463,
        "grad_norm": 2.9089601039886475,
        "learning_rate": 0.000199782877625781,
        "epoch": 0.6744913928012519,
        "step": 9051
    },
    {
        "loss": 2.4933,
        "grad_norm": 2.7696189880371094,
        "learning_rate": 0.00019977821853673318,
        "epoch": 0.6745659140025337,
        "step": 9052
    },
    {
        "loss": 2.3449,
        "grad_norm": 3.034374952316284,
        "learning_rate": 0.00019977351004422223,
        "epoch": 0.6746404352038154,
        "step": 9053
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.942950487136841,
        "learning_rate": 0.00019976875215057952,
        "epoch": 0.6747149564050973,
        "step": 9054
    },
    {
        "loss": 2.7011,
        "grad_norm": 2.798814296722412,
        "learning_rate": 0.00019976394485816083,
        "epoch": 0.674789477606379,
        "step": 9055
    },
    {
        "loss": 1.8369,
        "grad_norm": 2.744865894317627,
        "learning_rate": 0.0001997590881693464,
        "epoch": 0.6748639988076608,
        "step": 9056
    },
    {
        "loss": 2.5552,
        "grad_norm": 2.0377166271209717,
        "learning_rate": 0.0001997541820865409,
        "epoch": 0.6749385200089425,
        "step": 9057
    },
    {
        "loss": 2.8254,
        "grad_norm": 3.1525156497955322,
        "learning_rate": 0.00019974922661217354,
        "epoch": 0.6750130412102243,
        "step": 9058
    },
    {
        "loss": 2.298,
        "grad_norm": 2.4881908893585205,
        "learning_rate": 0.00019974422174869793,
        "epoch": 0.675087562411506,
        "step": 9059
    },
    {
        "loss": 2.459,
        "grad_norm": 2.3795595169067383,
        "learning_rate": 0.00019973916749859214,
        "epoch": 0.6751620836127878,
        "step": 9060
    },
    {
        "loss": 2.5979,
        "grad_norm": 4.383702754974365,
        "learning_rate": 0.00019973406386435868,
        "epoch": 0.6752366048140696,
        "step": 9061
    },
    {
        "loss": 2.3219,
        "grad_norm": 4.111484527587891,
        "learning_rate": 0.00019972891084852452,
        "epoch": 0.6753111260153514,
        "step": 9062
    },
    {
        "loss": 1.7344,
        "grad_norm": 2.7596631050109863,
        "learning_rate": 0.00019972370845364111,
        "epoch": 0.6753856472166331,
        "step": 9063
    },
    {
        "loss": 2.5792,
        "grad_norm": 3.652763605117798,
        "learning_rate": 0.00019971845668228432,
        "epoch": 0.6754601684179149,
        "step": 9064
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.6721144914627075,
        "learning_rate": 0.00019971315553705447,
        "epoch": 0.6755346896191967,
        "step": 9065
    },
    {
        "loss": 2.4388,
        "grad_norm": 2.437042713165283,
        "learning_rate": 0.00019970780502057633,
        "epoch": 0.6756092108204784,
        "step": 9066
    },
    {
        "loss": 2.2679,
        "grad_norm": 4.002171516418457,
        "learning_rate": 0.0001997024051354991,
        "epoch": 0.6756837320217602,
        "step": 9067
    },
    {
        "loss": 2.7477,
        "grad_norm": 3.9946229457855225,
        "learning_rate": 0.00019969695588449652,
        "epoch": 0.675758253223042,
        "step": 9068
    },
    {
        "loss": 2.3983,
        "grad_norm": 1.0873438119888306,
        "learning_rate": 0.0001996914572702666,
        "epoch": 0.6758327744243238,
        "step": 9069
    },
    {
        "loss": 2.4015,
        "grad_norm": 1.7123574018478394,
        "learning_rate": 0.0001996859092955319,
        "epoch": 0.6759072956256055,
        "step": 9070
    },
    {
        "loss": 2.3144,
        "grad_norm": 3.9645395278930664,
        "learning_rate": 0.00019968031196303945,
        "epoch": 0.6759818168268873,
        "step": 9071
    },
    {
        "loss": 2.486,
        "grad_norm": 3.148717164993286,
        "learning_rate": 0.0001996746652755606,
        "epoch": 0.676056338028169,
        "step": 9072
    },
    {
        "loss": 2.4756,
        "grad_norm": 2.9300010204315186,
        "learning_rate": 0.00019966896923589127,
        "epoch": 0.6761308592294508,
        "step": 9073
    },
    {
        "loss": 2.25,
        "grad_norm": 2.728639602661133,
        "learning_rate": 0.00019966322384685172,
        "epoch": 0.6762053804307325,
        "step": 9074
    },
    {
        "loss": 2.7495,
        "grad_norm": 1.1662695407867432,
        "learning_rate": 0.0001996574291112867,
        "epoch": 0.6762799016320143,
        "step": 9075
    },
    {
        "loss": 1.8304,
        "grad_norm": 3.1546788215637207,
        "learning_rate": 0.0001996515850320654,
        "epoch": 0.676354422833296,
        "step": 9076
    },
    {
        "loss": 2.8131,
        "grad_norm": 3.564577102661133,
        "learning_rate": 0.00019964569161208138,
        "epoch": 0.6764289440345779,
        "step": 9077
    },
    {
        "loss": 2.1721,
        "grad_norm": 3.0434467792510986,
        "learning_rate": 0.00019963974885425266,
        "epoch": 0.6765034652358596,
        "step": 9078
    },
    {
        "loss": 2.0823,
        "grad_norm": 3.298750638961792,
        "learning_rate": 0.00019963375676152172,
        "epoch": 0.6765779864371414,
        "step": 9079
    },
    {
        "loss": 2.8123,
        "grad_norm": 2.4559106826782227,
        "learning_rate": 0.00019962771533685542,
        "epoch": 0.6766525076384231,
        "step": 9080
    },
    {
        "loss": 1.8898,
        "grad_norm": 4.608096599578857,
        "learning_rate": 0.00019962162458324505,
        "epoch": 0.6767270288397049,
        "step": 9081
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.206089973449707,
        "learning_rate": 0.00019961548450370638,
        "epoch": 0.6768015500409866,
        "step": 9082
    },
    {
        "loss": 2.4253,
        "grad_norm": 3.6388511657714844,
        "learning_rate": 0.00019960929510127952,
        "epoch": 0.6768760712422685,
        "step": 9083
    },
    {
        "loss": 1.4615,
        "grad_norm": 5.180160045623779,
        "learning_rate": 0.00019960305637902912,
        "epoch": 0.6769505924435502,
        "step": 9084
    },
    {
        "loss": 1.0884,
        "grad_norm": 3.396902322769165,
        "learning_rate": 0.0001995967683400441,
        "epoch": 0.677025113644832,
        "step": 9085
    },
    {
        "loss": 2.066,
        "grad_norm": 3.477647542953491,
        "learning_rate": 0.00019959043098743794,
        "epoch": 0.6770996348461137,
        "step": 9086
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.9036943912506104,
        "learning_rate": 0.0001995840443243484,
        "epoch": 0.6771741560473955,
        "step": 9087
    },
    {
        "loss": 1.6878,
        "grad_norm": 4.416084289550781,
        "learning_rate": 0.0001995776083539378,
        "epoch": 0.6772486772486772,
        "step": 9088
    },
    {
        "loss": 2.6819,
        "grad_norm": 3.090723991394043,
        "learning_rate": 0.0001995711230793927,
        "epoch": 0.677323198449959,
        "step": 9089
    },
    {
        "loss": 1.9996,
        "grad_norm": 3.1097707748413086,
        "learning_rate": 0.0001995645885039243,
        "epoch": 0.6773977196512407,
        "step": 9090
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.776357412338257,
        "learning_rate": 0.00019955800463076798,
        "epoch": 0.6774722408525226,
        "step": 9091
    },
    {
        "loss": 2.2107,
        "grad_norm": 3.416550397872925,
        "learning_rate": 0.00019955137146318366,
        "epoch": 0.6775467620538043,
        "step": 9092
    },
    {
        "loss": 2.2872,
        "grad_norm": 3.0249688625335693,
        "learning_rate": 0.00019954468900445566,
        "epoch": 0.6776212832550861,
        "step": 9093
    },
    {
        "loss": 2.3597,
        "grad_norm": 2.709602117538452,
        "learning_rate": 0.00019953795725789265,
        "epoch": 0.6776958044563678,
        "step": 9094
    },
    {
        "loss": 1.701,
        "grad_norm": 1.3418618440628052,
        "learning_rate": 0.00019953117622682778,
        "epoch": 0.6777703256576496,
        "step": 9095
    },
    {
        "loss": 1.6854,
        "grad_norm": 4.584945201873779,
        "learning_rate": 0.00019952434591461853,
        "epoch": 0.6778448468589313,
        "step": 9096
    },
    {
        "loss": 2.6342,
        "grad_norm": 2.60284161567688,
        "learning_rate": 0.0001995174663246468,
        "epoch": 0.6779193680602131,
        "step": 9097
    },
    {
        "loss": 2.6235,
        "grad_norm": 3.1646041870117188,
        "learning_rate": 0.00019951053746031895,
        "epoch": 0.6779938892614948,
        "step": 9098
    },
    {
        "loss": 2.3529,
        "grad_norm": 2.585857629776001,
        "learning_rate": 0.00019950355932506566,
        "epoch": 0.6780684104627767,
        "step": 9099
    },
    {
        "loss": 2.9644,
        "grad_norm": 3.573770046234131,
        "learning_rate": 0.000199496531922342,
        "epoch": 0.6781429316640585,
        "step": 9100
    },
    {
        "loss": 2.7485,
        "grad_norm": 2.29840087890625,
        "learning_rate": 0.0001994894552556275,
        "epoch": 0.6782174528653402,
        "step": 9101
    },
    {
        "loss": 2.4252,
        "grad_norm": 2.857462167739868,
        "learning_rate": 0.0001994823293284261,
        "epoch": 0.678291974066622,
        "step": 9102
    },
    {
        "loss": 2.368,
        "grad_norm": 2.547875165939331,
        "learning_rate": 0.00019947515414426597,
        "epoch": 0.6783664952679037,
        "step": 9103
    },
    {
        "loss": 2.2242,
        "grad_norm": 3.7280936241149902,
        "learning_rate": 0.00019946792970669983,
        "epoch": 0.6784410164691855,
        "step": 9104
    },
    {
        "loss": 2.7766,
        "grad_norm": 2.647336721420288,
        "learning_rate": 0.00019946065601930476,
        "epoch": 0.6785155376704672,
        "step": 9105
    },
    {
        "loss": 1.9564,
        "grad_norm": 3.9151408672332764,
        "learning_rate": 0.00019945333308568218,
        "epoch": 0.6785900588717491,
        "step": 9106
    },
    {
        "loss": 1.995,
        "grad_norm": 3.857910633087158,
        "learning_rate": 0.0001994459609094579,
        "epoch": 0.6786645800730308,
        "step": 9107
    },
    {
        "loss": 2.557,
        "grad_norm": 3.532930850982666,
        "learning_rate": 0.0001994385394942822,
        "epoch": 0.6787391012743126,
        "step": 9108
    },
    {
        "loss": 2.6001,
        "grad_norm": 2.7899370193481445,
        "learning_rate": 0.00019943106884382953,
        "epoch": 0.6788136224755943,
        "step": 9109
    },
    {
        "loss": 2.1174,
        "grad_norm": 3.1645867824554443,
        "learning_rate": 0.00019942354896179898,
        "epoch": 0.6788881436768761,
        "step": 9110
    },
    {
        "loss": 1.7454,
        "grad_norm": 3.2276651859283447,
        "learning_rate": 0.00019941597985191381,
        "epoch": 0.6789626648781578,
        "step": 9111
    },
    {
        "loss": 2.6854,
        "grad_norm": 2.5662591457366943,
        "learning_rate": 0.00019940836151792176,
        "epoch": 0.6790371860794396,
        "step": 9112
    },
    {
        "loss": 2.6049,
        "grad_norm": 1.6487412452697754,
        "learning_rate": 0.00019940069396359497,
        "epoch": 0.6791117072807213,
        "step": 9113
    },
    {
        "loss": 1.9766,
        "grad_norm": 3.821484088897705,
        "learning_rate": 0.00019939297719272981,
        "epoch": 0.6791862284820032,
        "step": 9114
    },
    {
        "loss": 1.7107,
        "grad_norm": 2.989058494567871,
        "learning_rate": 0.00019938521120914718,
        "epoch": 0.6792607496832849,
        "step": 9115
    },
    {
        "loss": 1.7773,
        "grad_norm": 3.965517044067383,
        "learning_rate": 0.00019937739601669222,
        "epoch": 0.6793352708845667,
        "step": 9116
    },
    {
        "loss": 2.343,
        "grad_norm": 3.617218255996704,
        "learning_rate": 0.0001993695316192345,
        "epoch": 0.6794097920858484,
        "step": 9117
    },
    {
        "loss": 2.2417,
        "grad_norm": 2.594104051589966,
        "learning_rate": 0.00019936161802066793,
        "epoch": 0.6794843132871302,
        "step": 9118
    },
    {
        "loss": 2.595,
        "grad_norm": 2.56081223487854,
        "learning_rate": 0.00019935365522491083,
        "epoch": 0.6795588344884119,
        "step": 9119
    },
    {
        "loss": 1.9649,
        "grad_norm": 4.078721046447754,
        "learning_rate": 0.00019934564323590578,
        "epoch": 0.6796333556896937,
        "step": 9120
    },
    {
        "loss": 2.1328,
        "grad_norm": 3.100517988204956,
        "learning_rate": 0.00019933758205761985,
        "epoch": 0.6797078768909754,
        "step": 9121
    },
    {
        "loss": 2.5035,
        "grad_norm": 2.5322067737579346,
        "learning_rate": 0.00019932947169404438,
        "epoch": 0.6797823980922573,
        "step": 9122
    },
    {
        "loss": 2.3451,
        "grad_norm": 2.554047107696533,
        "learning_rate": 0.00019932131214919503,
        "epoch": 0.679856919293539,
        "step": 9123
    },
    {
        "loss": 1.9163,
        "grad_norm": 2.3493311405181885,
        "learning_rate": 0.00019931310342711188,
        "epoch": 0.6799314404948208,
        "step": 9124
    },
    {
        "loss": 1.737,
        "grad_norm": 3.364387035369873,
        "learning_rate": 0.00019930484553185932,
        "epoch": 0.6800059616961025,
        "step": 9125
    },
    {
        "loss": 3.0992,
        "grad_norm": 2.2373206615448,
        "learning_rate": 0.0001992965384675262,
        "epoch": 0.6800804828973843,
        "step": 9126
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.8023009300231934,
        "learning_rate": 0.00019928818223822548,
        "epoch": 0.680155004098666,
        "step": 9127
    },
    {
        "loss": 3.2929,
        "grad_norm": 4.175799369812012,
        "learning_rate": 0.00019927977684809468,
        "epoch": 0.6802295252999478,
        "step": 9128
    },
    {
        "loss": 2.387,
        "grad_norm": 4.288111209869385,
        "learning_rate": 0.00019927132230129557,
        "epoch": 0.6803040465012296,
        "step": 9129
    },
    {
        "loss": 2.108,
        "grad_norm": 4.240487098693848,
        "learning_rate": 0.0001992628186020143,
        "epoch": 0.6803785677025114,
        "step": 9130
    },
    {
        "loss": 2.0523,
        "grad_norm": 3.8318722248077393,
        "learning_rate": 0.0001992542657544613,
        "epoch": 0.6804530889037931,
        "step": 9131
    },
    {
        "loss": 3.2009,
        "grad_norm": 2.282721757888794,
        "learning_rate": 0.00019924566376287132,
        "epoch": 0.6805276101050749,
        "step": 9132
    },
    {
        "loss": 1.663,
        "grad_norm": 4.489572048187256,
        "learning_rate": 0.0001992370126315036,
        "epoch": 0.6806021313063566,
        "step": 9133
    },
    {
        "loss": 2.1577,
        "grad_norm": 1.496381163597107,
        "learning_rate": 0.0001992283123646415,
        "epoch": 0.6806766525076384,
        "step": 9134
    },
    {
        "loss": 2.6132,
        "grad_norm": 2.699056386947632,
        "learning_rate": 0.00019921956296659282,
        "epoch": 0.6807511737089202,
        "step": 9135
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.2735202312469482,
        "learning_rate": 0.00019921076444168976,
        "epoch": 0.680825694910202,
        "step": 9136
    },
    {
        "loss": 2.4759,
        "grad_norm": 2.712963819503784,
        "learning_rate": 0.00019920191679428866,
        "epoch": 0.6809002161114838,
        "step": 9137
    },
    {
        "loss": 2.7551,
        "grad_norm": 2.639967679977417,
        "learning_rate": 0.0001991930200287703,
        "epoch": 0.6809747373127655,
        "step": 9138
    },
    {
        "loss": 2.2277,
        "grad_norm": 3.5147905349731445,
        "learning_rate": 0.00019918407414953982,
        "epoch": 0.6810492585140473,
        "step": 9139
    },
    {
        "loss": 1.9677,
        "grad_norm": 3.13057017326355,
        "learning_rate": 0.0001991750791610265,
        "epoch": 0.681123779715329,
        "step": 9140
    },
    {
        "loss": 2.3628,
        "grad_norm": 2.6964850425720215,
        "learning_rate": 0.00019916603506768419,
        "epoch": 0.6811983009166108,
        "step": 9141
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.2436423301696777,
        "learning_rate": 0.00019915694187399083,
        "epoch": 0.6812728221178925,
        "step": 9142
    },
    {
        "loss": 2.1507,
        "grad_norm": 3.296870470046997,
        "learning_rate": 0.00019914779958444875,
        "epoch": 0.6813473433191743,
        "step": 9143
    },
    {
        "loss": 2.532,
        "grad_norm": 3.3038134574890137,
        "learning_rate": 0.00019913860820358468,
        "epoch": 0.681421864520456,
        "step": 9144
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.824183464050293,
        "learning_rate": 0.00019912936773594945,
        "epoch": 0.6814963857217379,
        "step": 9145
    },
    {
        "loss": 2.3467,
        "grad_norm": 3.1092679500579834,
        "learning_rate": 0.00019912007818611847,
        "epoch": 0.6815709069230196,
        "step": 9146
    },
    {
        "loss": 2.4585,
        "grad_norm": 3.687106132507324,
        "learning_rate": 0.00019911073955869118,
        "epoch": 0.6816454281243014,
        "step": 9147
    },
    {
        "loss": 2.3172,
        "grad_norm": 3.3989484310150146,
        "learning_rate": 0.00019910135185829148,
        "epoch": 0.6817199493255831,
        "step": 9148
    },
    {
        "loss": 1.9101,
        "grad_norm": 3.6306910514831543,
        "learning_rate": 0.0001990919150895675,
        "epoch": 0.6817944705268649,
        "step": 9149
    },
    {
        "loss": 2.0552,
        "grad_norm": 3.5069918632507324,
        "learning_rate": 0.00019908242925719175,
        "epoch": 0.6818689917281466,
        "step": 9150
    },
    {
        "loss": 2.5032,
        "grad_norm": 5.21359920501709,
        "learning_rate": 0.00019907289436586097,
        "epoch": 0.6819435129294285,
        "step": 9151
    },
    {
        "loss": 2.1387,
        "grad_norm": 2.650233030319214,
        "learning_rate": 0.00019906331042029613,
        "epoch": 0.6820180341307102,
        "step": 9152
    },
    {
        "loss": 1.7686,
        "grad_norm": 2.7809674739837646,
        "learning_rate": 0.0001990536774252426,
        "epoch": 0.682092555331992,
        "step": 9153
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.2625255584716797,
        "learning_rate": 0.00019904399538547,
        "epoch": 0.6821670765332737,
        "step": 9154
    },
    {
        "loss": 2.377,
        "grad_norm": 2.3102991580963135,
        "learning_rate": 0.00019903426430577223,
        "epoch": 0.6822415977345555,
        "step": 9155
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.1847147941589355,
        "learning_rate": 0.00019902448419096741,
        "epoch": 0.6823161189358372,
        "step": 9156
    },
    {
        "loss": 2.2775,
        "grad_norm": 4.56728458404541,
        "learning_rate": 0.0001990146550458981,
        "epoch": 0.682390640137119,
        "step": 9157
    },
    {
        "loss": 2.6265,
        "grad_norm": 2.1752877235412598,
        "learning_rate": 0.0001990047768754309,
        "epoch": 0.6824651613384007,
        "step": 9158
    },
    {
        "loss": 2.6073,
        "grad_norm": 3.1499364376068115,
        "learning_rate": 0.0001989948496844569,
        "epoch": 0.6825396825396826,
        "step": 9159
    },
    {
        "loss": 2.5715,
        "grad_norm": 2.6229217052459717,
        "learning_rate": 0.0001989848734778914,
        "epoch": 0.6826142037409643,
        "step": 9160
    },
    {
        "loss": 3.2435,
        "grad_norm": 4.443206787109375,
        "learning_rate": 0.00019897484826067387,
        "epoch": 0.6826887249422461,
        "step": 9161
    },
    {
        "loss": 2.558,
        "grad_norm": 3.2475149631500244,
        "learning_rate": 0.00019896477403776817,
        "epoch": 0.6827632461435278,
        "step": 9162
    },
    {
        "loss": 2.3452,
        "grad_norm": 2.9121174812316895,
        "learning_rate": 0.00019895465081416235,
        "epoch": 0.6828377673448096,
        "step": 9163
    },
    {
        "loss": 2.4783,
        "grad_norm": 3.7788376808166504,
        "learning_rate": 0.0001989444785948688,
        "epoch": 0.6829122885460913,
        "step": 9164
    },
    {
        "loss": 1.9434,
        "grad_norm": 4.665760040283203,
        "learning_rate": 0.0001989342573849241,
        "epoch": 0.6829868097473731,
        "step": 9165
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.2580466270446777,
        "learning_rate": 0.000198923987189389,
        "epoch": 0.6830613309486548,
        "step": 9166
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.9145281314849854,
        "learning_rate": 0.0001989136680133488,
        "epoch": 0.6831358521499367,
        "step": 9167
    },
    {
        "loss": 2.0414,
        "grad_norm": 5.014213562011719,
        "learning_rate": 0.00019890329986191273,
        "epoch": 0.6832103733512184,
        "step": 9168
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.340897560119629,
        "learning_rate": 0.0001988928827402144,
        "epoch": 0.6832848945525002,
        "step": 9169
    },
    {
        "loss": 2.0589,
        "grad_norm": 4.199628829956055,
        "learning_rate": 0.00019888241665341175,
        "epoch": 0.6833594157537819,
        "step": 9170
    },
    {
        "loss": 2.4976,
        "grad_norm": 2.559760093688965,
        "learning_rate": 0.0001988719016066868,
        "epoch": 0.6834339369550637,
        "step": 9171
    },
    {
        "loss": 2.2349,
        "grad_norm": 2.403426170349121,
        "learning_rate": 0.00019886133760524597,
        "epoch": 0.6835084581563455,
        "step": 9172
    },
    {
        "loss": 2.386,
        "grad_norm": 3.407052516937256,
        "learning_rate": 0.00019885072465431974,
        "epoch": 0.6835829793576272,
        "step": 9173
    },
    {
        "loss": 2.3863,
        "grad_norm": 4.087750434875488,
        "learning_rate": 0.00019884006275916306,
        "epoch": 0.6836575005589091,
        "step": 9174
    },
    {
        "loss": 2.6132,
        "grad_norm": 3.8321688175201416,
        "learning_rate": 0.00019882935192505488,
        "epoch": 0.6837320217601908,
        "step": 9175
    },
    {
        "loss": 1.8925,
        "grad_norm": 2.8536953926086426,
        "learning_rate": 0.00019881859215729853,
        "epoch": 0.6838065429614726,
        "step": 9176
    },
    {
        "loss": 2.6626,
        "grad_norm": 1.948469638824463,
        "learning_rate": 0.0001988077834612215,
        "epoch": 0.6838810641627543,
        "step": 9177
    },
    {
        "loss": 2.6134,
        "grad_norm": 3.3222203254699707,
        "learning_rate": 0.00019879692584217552,
        "epoch": 0.6839555853640361,
        "step": 9178
    },
    {
        "loss": 2.6648,
        "grad_norm": 2.0055181980133057,
        "learning_rate": 0.00019878601930553664,
        "epoch": 0.6840301065653178,
        "step": 9179
    },
    {
        "loss": 2.4391,
        "grad_norm": 3.345396041870117,
        "learning_rate": 0.0001987750638567049,
        "epoch": 0.6841046277665996,
        "step": 9180
    },
    {
        "loss": 2.3184,
        "grad_norm": 2.2765860557556152,
        "learning_rate": 0.00019876405950110487,
        "epoch": 0.6841791489678813,
        "step": 9181
    },
    {
        "loss": 2.3972,
        "grad_norm": 3.1607866287231445,
        "learning_rate": 0.00019875300624418505,
        "epoch": 0.6842536701691632,
        "step": 9182
    },
    {
        "loss": 1.6633,
        "grad_norm": 3.7804691791534424,
        "learning_rate": 0.0001987419040914183,
        "epoch": 0.6843281913704449,
        "step": 9183
    },
    {
        "loss": 2.7949,
        "grad_norm": 2.4057085514068604,
        "learning_rate": 0.00019873075304830158,
        "epoch": 0.6844027125717267,
        "step": 9184
    },
    {
        "loss": 2.6818,
        "grad_norm": 3.239138126373291,
        "learning_rate": 0.00019871955312035627,
        "epoch": 0.6844772337730084,
        "step": 9185
    },
    {
        "loss": 2.4597,
        "grad_norm": 2.9693939685821533,
        "learning_rate": 0.00019870830431312775,
        "epoch": 0.6845517549742902,
        "step": 9186
    },
    {
        "loss": 1.9595,
        "grad_norm": 3.0912952423095703,
        "learning_rate": 0.0001986970066321857,
        "epoch": 0.6846262761755719,
        "step": 9187
    },
    {
        "loss": 2.0658,
        "grad_norm": 3.677126884460449,
        "learning_rate": 0.00019868566008312397,
        "epoch": 0.6847007973768537,
        "step": 9188
    },
    {
        "loss": 2.0309,
        "grad_norm": 3.336578369140625,
        "learning_rate": 0.00019867426467156055,
        "epoch": 0.6847753185781355,
        "step": 9189
    },
    {
        "loss": 1.2646,
        "grad_norm": 3.2803196907043457,
        "learning_rate": 0.0001986628204031378,
        "epoch": 0.6848498397794173,
        "step": 9190
    },
    {
        "loss": 2.5416,
        "grad_norm": 2.3740835189819336,
        "learning_rate": 0.000198651327283522,
        "epoch": 0.684924360980699,
        "step": 9191
    },
    {
        "loss": 2.2457,
        "grad_norm": 2.3334944248199463,
        "learning_rate": 0.00019863978531840391,
        "epoch": 0.6849988821819808,
        "step": 9192
    },
    {
        "loss": 2.6788,
        "grad_norm": 2.67441463470459,
        "learning_rate": 0.00019862819451349823,
        "epoch": 0.6850734033832625,
        "step": 9193
    },
    {
        "loss": 2.862,
        "grad_norm": 2.3096096515655518,
        "learning_rate": 0.000198616554874544,
        "epoch": 0.6851479245845443,
        "step": 9194
    },
    {
        "loss": 2.2686,
        "grad_norm": 2.816699981689453,
        "learning_rate": 0.0001986048664073044,
        "epoch": 0.685222445785826,
        "step": 9195
    },
    {
        "loss": 2.6221,
        "grad_norm": 2.102658271789551,
        "learning_rate": 0.0001985931291175667,
        "epoch": 0.6852969669871078,
        "step": 9196
    },
    {
        "loss": 2.1456,
        "grad_norm": 3.2184722423553467,
        "learning_rate": 0.0001985813430111425,
        "epoch": 0.6853714881883896,
        "step": 9197
    },
    {
        "loss": 1.7,
        "grad_norm": 4.432486534118652,
        "learning_rate": 0.00019856950809386746,
        "epoch": 0.6854460093896714,
        "step": 9198
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.7729201316833496,
        "learning_rate": 0.00019855762437160146,
        "epoch": 0.6855205305909531,
        "step": 9199
    },
    {
        "loss": 2.1146,
        "grad_norm": 3.4073948860168457,
        "learning_rate": 0.00019854569185022842,
        "epoch": 0.6855950517922349,
        "step": 9200
    },
    {
        "loss": 2.1535,
        "grad_norm": 3.6644535064697266,
        "learning_rate": 0.00019853371053565666,
        "epoch": 0.6856695729935166,
        "step": 9201
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.316343307495117,
        "learning_rate": 0.0001985216804338184,
        "epoch": 0.6857440941947984,
        "step": 9202
    },
    {
        "loss": 3.0832,
        "grad_norm": 3.312634229660034,
        "learning_rate": 0.00019850960155067023,
        "epoch": 0.6858186153960801,
        "step": 9203
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.3960344791412354,
        "learning_rate": 0.00019849747389219272,
        "epoch": 0.685893136597362,
        "step": 9204
    },
    {
        "loss": 1.7848,
        "grad_norm": 4.116781711578369,
        "learning_rate": 0.00019848529746439072,
        "epoch": 0.6859676577986437,
        "step": 9205
    },
    {
        "loss": 2.2142,
        "grad_norm": 2.068223237991333,
        "learning_rate": 0.00019847307227329318,
        "epoch": 0.6860421789999255,
        "step": 9206
    },
    {
        "loss": 2.6532,
        "grad_norm": 2.3584094047546387,
        "learning_rate": 0.00019846079832495318,
        "epoch": 0.6861167002012073,
        "step": 9207
    },
    {
        "loss": 2.9184,
        "grad_norm": 1.1241374015808105,
        "learning_rate": 0.00019844847562544793,
        "epoch": 0.686191221402489,
        "step": 9208
    },
    {
        "loss": 1.9486,
        "grad_norm": 3.0973126888275146,
        "learning_rate": 0.00019843610418087884,
        "epoch": 0.6862657426037708,
        "step": 9209
    },
    {
        "loss": 2.7982,
        "grad_norm": 3.4728477001190186,
        "learning_rate": 0.00019842368399737145,
        "epoch": 0.6863402638050525,
        "step": 9210
    },
    {
        "loss": 2.0292,
        "grad_norm": 3.235427141189575,
        "learning_rate": 0.0001984112150810753,
        "epoch": 0.6864147850063343,
        "step": 9211
    },
    {
        "loss": 2.0223,
        "grad_norm": 3.531310558319092,
        "learning_rate": 0.00019839869743816423,
        "epoch": 0.6864893062076161,
        "step": 9212
    },
    {
        "loss": 2.8388,
        "grad_norm": 2.412994384765625,
        "learning_rate": 0.00019838613107483612,
        "epoch": 0.6865638274088979,
        "step": 9213
    },
    {
        "loss": 2.495,
        "grad_norm": 1.9780163764953613,
        "learning_rate": 0.00019837351599731297,
        "epoch": 0.6866383486101796,
        "step": 9214
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.797429084777832,
        "learning_rate": 0.00019836085221184096,
        "epoch": 0.6867128698114614,
        "step": 9215
    },
    {
        "loss": 1.9374,
        "grad_norm": 3.2105956077575684,
        "learning_rate": 0.00019834813972469028,
        "epoch": 0.6867873910127431,
        "step": 9216
    },
    {
        "loss": 1.5261,
        "grad_norm": 2.5780141353607178,
        "learning_rate": 0.00019833537854215536,
        "epoch": 0.6868619122140249,
        "step": 9217
    },
    {
        "loss": 2.4076,
        "grad_norm": 2.077305555343628,
        "learning_rate": 0.00019832256867055467,
        "epoch": 0.6869364334153066,
        "step": 9218
    },
    {
        "loss": 2.5557,
        "grad_norm": 2.3879234790802,
        "learning_rate": 0.0001983097101162308,
        "epoch": 0.6870109546165885,
        "step": 9219
    },
    {
        "loss": 2.4521,
        "grad_norm": 2.794668436050415,
        "learning_rate": 0.00019829680288555035,
        "epoch": 0.6870854758178702,
        "step": 9220
    },
    {
        "loss": 2.7049,
        "grad_norm": 2.223475456237793,
        "learning_rate": 0.00019828384698490425,
        "epoch": 0.687159997019152,
        "step": 9221
    },
    {
        "loss": 3.2205,
        "grad_norm": 4.2198405265808105,
        "learning_rate": 0.0001982708424207073,
        "epoch": 0.6872345182204337,
        "step": 9222
    },
    {
        "loss": 2.1889,
        "grad_norm": 3.0143227577209473,
        "learning_rate": 0.00019825778919939854,
        "epoch": 0.6873090394217155,
        "step": 9223
    },
    {
        "loss": 2.9047,
        "grad_norm": 2.8288321495056152,
        "learning_rate": 0.00019824468732744098,
        "epoch": 0.6873835606229972,
        "step": 9224
    },
    {
        "loss": 2.4502,
        "grad_norm": 3.2081007957458496,
        "learning_rate": 0.00019823153681132184,
        "epoch": 0.687458081824279,
        "step": 9225
    },
    {
        "loss": 2.388,
        "grad_norm": 3.3958382606506348,
        "learning_rate": 0.00019821833765755237,
        "epoch": 0.6875326030255607,
        "step": 9226
    },
    {
        "loss": 1.1362,
        "grad_norm": 4.156708240509033,
        "learning_rate": 0.00019820508987266786,
        "epoch": 0.6876071242268426,
        "step": 9227
    },
    {
        "loss": 2.8308,
        "grad_norm": 2.195843458175659,
        "learning_rate": 0.0001981917934632278,
        "epoch": 0.6876816454281243,
        "step": 9228
    },
    {
        "loss": 2.3015,
        "grad_norm": 2.4812843799591064,
        "learning_rate": 0.00019817844843581558,
        "epoch": 0.6877561666294061,
        "step": 9229
    },
    {
        "loss": 2.2653,
        "grad_norm": 2.749220609664917,
        "learning_rate": 0.00019816505479703888,
        "epoch": 0.6878306878306878,
        "step": 9230
    },
    {
        "loss": 1.6317,
        "grad_norm": 4.571382522583008,
        "learning_rate": 0.0001981516125535292,
        "epoch": 0.6879052090319696,
        "step": 9231
    },
    {
        "loss": 2.258,
        "grad_norm": 4.480052471160889,
        "learning_rate": 0.00019813812171194233,
        "epoch": 0.6879797302332513,
        "step": 9232
    },
    {
        "loss": 2.6251,
        "grad_norm": 2.268934965133667,
        "learning_rate": 0.00019812458227895795,
        "epoch": 0.6880542514345331,
        "step": 9233
    },
    {
        "loss": 1.9107,
        "grad_norm": 3.066452980041504,
        "learning_rate": 0.00019811099426127996,
        "epoch": 0.6881287726358148,
        "step": 9234
    },
    {
        "loss": 2.2067,
        "grad_norm": 4.243234157562256,
        "learning_rate": 0.00019809735766563615,
        "epoch": 0.6882032938370967,
        "step": 9235
    },
    {
        "loss": 2.8438,
        "grad_norm": 3.220522880554199,
        "learning_rate": 0.00019808367249877853,
        "epoch": 0.6882778150383784,
        "step": 9236
    },
    {
        "loss": 2.9884,
        "grad_norm": 3.404489517211914,
        "learning_rate": 0.00019806993876748298,
        "epoch": 0.6883523362396602,
        "step": 9237
    },
    {
        "loss": 2.5129,
        "grad_norm": 2.354215383529663,
        "learning_rate": 0.0001980561564785496,
        "epoch": 0.6884268574409419,
        "step": 9238
    },
    {
        "loss": 2.8639,
        "grad_norm": 2.114948272705078,
        "learning_rate": 0.00019804232563880243,
        "epoch": 0.6885013786422237,
        "step": 9239
    },
    {
        "loss": 1.9364,
        "grad_norm": 3.7738094329833984,
        "learning_rate": 0.00019802844625508957,
        "epoch": 0.6885758998435054,
        "step": 9240
    },
    {
        "loss": 2.6501,
        "grad_norm": 2.6860668659210205,
        "learning_rate": 0.00019801451833428315,
        "epoch": 0.6886504210447872,
        "step": 9241
    },
    {
        "loss": 2.9018,
        "grad_norm": 2.4152395725250244,
        "learning_rate": 0.0001980005418832793,
        "epoch": 0.6887249422460691,
        "step": 9242
    },
    {
        "loss": 2.4482,
        "grad_norm": 2.6411399841308594,
        "learning_rate": 0.0001979865169089983,
        "epoch": 0.6887994634473508,
        "step": 9243
    },
    {
        "loss": 2.1927,
        "grad_norm": 3.298471450805664,
        "learning_rate": 0.00019797244341838432,
        "epoch": 0.6888739846486326,
        "step": 9244
    },
    {
        "loss": 2.5075,
        "grad_norm": 4.907023906707764,
        "learning_rate": 0.00019795832141840562,
        "epoch": 0.6889485058499143,
        "step": 9245
    },
    {
        "loss": 2.2782,
        "grad_norm": 3.280027151107788,
        "learning_rate": 0.00019794415091605448,
        "epoch": 0.6890230270511961,
        "step": 9246
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.499042272567749,
        "learning_rate": 0.00019792993191834713,
        "epoch": 0.6890975482524778,
        "step": 9247
    },
    {
        "loss": 2.0955,
        "grad_norm": 3.2738656997680664,
        "learning_rate": 0.00019791566443232392,
        "epoch": 0.6891720694537596,
        "step": 9248
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.0268545150756836,
        "learning_rate": 0.00019790134846504913,
        "epoch": 0.6892465906550413,
        "step": 9249
    },
    {
        "loss": 1.4542,
        "grad_norm": 2.421971082687378,
        "learning_rate": 0.00019788698402361106,
        "epoch": 0.6893211118563232,
        "step": 9250
    },
    {
        "loss": 2.8035,
        "grad_norm": 3.0772898197174072,
        "learning_rate": 0.00019787257111512197,
        "epoch": 0.6893956330576049,
        "step": 9251
    },
    {
        "loss": 2.5967,
        "grad_norm": 2.64673113822937,
        "learning_rate": 0.00019785810974671825,
        "epoch": 0.6894701542588867,
        "step": 9252
    },
    {
        "loss": 2.6039,
        "grad_norm": 2.801422357559204,
        "learning_rate": 0.00019784359992556017,
        "epoch": 0.6895446754601684,
        "step": 9253
    },
    {
        "loss": 1.1019,
        "grad_norm": 3.5623390674591064,
        "learning_rate": 0.000197829041658832,
        "epoch": 0.6896191966614502,
        "step": 9254
    },
    {
        "loss": 2.6055,
        "grad_norm": 4.503904819488525,
        "learning_rate": 0.00019781443495374196,
        "epoch": 0.6896937178627319,
        "step": 9255
    },
    {
        "loss": 2.4628,
        "grad_norm": 2.00600004196167,
        "learning_rate": 0.0001977997798175224,
        "epoch": 0.6897682390640137,
        "step": 9256
    },
    {
        "loss": 2.2467,
        "grad_norm": 2.2249958515167236,
        "learning_rate": 0.0001977850762574295,
        "epoch": 0.6898427602652955,
        "step": 9257
    },
    {
        "loss": 2.6692,
        "grad_norm": 2.914398193359375,
        "learning_rate": 0.0001977703242807435,
        "epoch": 0.6899172814665773,
        "step": 9258
    },
    {
        "loss": 3.2002,
        "grad_norm": 2.714942455291748,
        "learning_rate": 0.00019775552389476864,
        "epoch": 0.689991802667859,
        "step": 9259
    },
    {
        "loss": 2.6844,
        "grad_norm": 2.4396653175354004,
        "learning_rate": 0.00019774067510683297,
        "epoch": 0.6900663238691408,
        "step": 9260
    },
    {
        "loss": 2.3051,
        "grad_norm": 3.6414263248443604,
        "learning_rate": 0.00019772577792428867,
        "epoch": 0.6901408450704225,
        "step": 9261
    },
    {
        "loss": 3.0008,
        "grad_norm": 2.8505005836486816,
        "learning_rate": 0.00019771083235451182,
        "epoch": 0.6902153662717043,
        "step": 9262
    },
    {
        "loss": 2.5201,
        "grad_norm": 4.333980560302734,
        "learning_rate": 0.00019769583840490245,
        "epoch": 0.690289887472986,
        "step": 9263
    },
    {
        "loss": 1.7134,
        "grad_norm": 1.950331449508667,
        "learning_rate": 0.00019768079608288455,
        "epoch": 0.6903644086742678,
        "step": 9264
    },
    {
        "loss": 2.6375,
        "grad_norm": 3.357011318206787,
        "learning_rate": 0.00019766570539590605,
        "epoch": 0.6904389298755496,
        "step": 9265
    },
    {
        "loss": 1.7233,
        "grad_norm": 4.900282382965088,
        "learning_rate": 0.00019765056635143892,
        "epoch": 0.6905134510768314,
        "step": 9266
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.1094861030578613,
        "learning_rate": 0.0001976353789569789,
        "epoch": 0.6905879722781131,
        "step": 9267
    },
    {
        "loss": 2.5342,
        "grad_norm": 1.8691881895065308,
        "learning_rate": 0.00019762014322004582,
        "epoch": 0.6906624934793949,
        "step": 9268
    },
    {
        "loss": 2.2401,
        "grad_norm": 3.875645399093628,
        "learning_rate": 0.00019760485914818344,
        "epoch": 0.6907370146806766,
        "step": 9269
    },
    {
        "loss": 2.1207,
        "grad_norm": 2.6449456214904785,
        "learning_rate": 0.0001975895267489593,
        "epoch": 0.6908115358819584,
        "step": 9270
    },
    {
        "loss": 2.567,
        "grad_norm": 3.388265609741211,
        "learning_rate": 0.000197574146029965,
        "epoch": 0.6908860570832401,
        "step": 9271
    },
    {
        "loss": 2.6226,
        "grad_norm": 2.3164026737213135,
        "learning_rate": 0.0001975587169988161,
        "epoch": 0.690960578284522,
        "step": 9272
    },
    {
        "loss": 2.6615,
        "grad_norm": 4.256718635559082,
        "learning_rate": 0.00019754323966315198,
        "epoch": 0.6910350994858037,
        "step": 9273
    },
    {
        "loss": 2.3381,
        "grad_norm": 2.966829776763916,
        "learning_rate": 0.00019752771403063594,
        "epoch": 0.6911096206870855,
        "step": 9274
    },
    {
        "loss": 2.7108,
        "grad_norm": 3.085407257080078,
        "learning_rate": 0.00019751214010895526,
        "epoch": 0.6911841418883672,
        "step": 9275
    },
    {
        "loss": 2.4794,
        "grad_norm": 2.6722218990325928,
        "learning_rate": 0.00019749651790582114,
        "epoch": 0.691258663089649,
        "step": 9276
    },
    {
        "loss": 2.8926,
        "grad_norm": 3.0153284072875977,
        "learning_rate": 0.00019748084742896856,
        "epoch": 0.6913331842909308,
        "step": 9277
    },
    {
        "loss": 2.303,
        "grad_norm": 3.667928457260132,
        "learning_rate": 0.00019746512868615653,
        "epoch": 0.6914077054922125,
        "step": 9278
    },
    {
        "loss": 3.0441,
        "grad_norm": 2.754944086074829,
        "learning_rate": 0.00019744936168516797,
        "epoch": 0.6914822266934944,
        "step": 9279
    },
    {
        "loss": 2.4617,
        "grad_norm": 2.3021600246429443,
        "learning_rate": 0.0001974335464338095,
        "epoch": 0.6915567478947761,
        "step": 9280
    },
    {
        "loss": 1.9941,
        "grad_norm": 3.4244627952575684,
        "learning_rate": 0.00019741768293991198,
        "epoch": 0.6916312690960579,
        "step": 9281
    },
    {
        "loss": 2.8918,
        "grad_norm": 2.5433363914489746,
        "learning_rate": 0.00019740177121132973,
        "epoch": 0.6917057902973396,
        "step": 9282
    },
    {
        "loss": 2.5727,
        "grad_norm": 2.83223032951355,
        "learning_rate": 0.00019738581125594126,
        "epoch": 0.6917803114986214,
        "step": 9283
    },
    {
        "loss": 2.3881,
        "grad_norm": 3.130765676498413,
        "learning_rate": 0.00019736980308164887,
        "epoch": 0.6918548326999031,
        "step": 9284
    },
    {
        "loss": 2.1794,
        "grad_norm": 2.914130210876465,
        "learning_rate": 0.0001973537466963787,
        "epoch": 0.6919293539011849,
        "step": 9285
    },
    {
        "loss": 3.047,
        "grad_norm": 2.2001774311065674,
        "learning_rate": 0.00019733764210808086,
        "epoch": 0.6920038751024666,
        "step": 9286
    },
    {
        "loss": 2.3725,
        "grad_norm": 3.2328948974609375,
        "learning_rate": 0.00019732148932472918,
        "epoch": 0.6920783963037485,
        "step": 9287
    },
    {
        "loss": 1.9184,
        "grad_norm": 3.3943684101104736,
        "learning_rate": 0.00019730528835432148,
        "epoch": 0.6921529175050302,
        "step": 9288
    },
    {
        "loss": 1.8533,
        "grad_norm": 4.713781833648682,
        "learning_rate": 0.00019728903920487935,
        "epoch": 0.692227438706312,
        "step": 9289
    },
    {
        "loss": 2.7681,
        "grad_norm": 5.215660095214844,
        "learning_rate": 0.00019727274188444834,
        "epoch": 0.6923019599075937,
        "step": 9290
    },
    {
        "loss": 2.0397,
        "grad_norm": 2.906520128250122,
        "learning_rate": 0.00019725639640109775,
        "epoch": 0.6923764811088755,
        "step": 9291
    },
    {
        "loss": 2.447,
        "grad_norm": 4.720401763916016,
        "learning_rate": 0.00019724000276292078,
        "epoch": 0.6924510023101572,
        "step": 9292
    },
    {
        "loss": 2.2928,
        "grad_norm": 2.569451093673706,
        "learning_rate": 0.00019722356097803438,
        "epoch": 0.692525523511439,
        "step": 9293
    },
    {
        "loss": 2.5807,
        "grad_norm": 2.4492027759552,
        "learning_rate": 0.0001972070710545795,
        "epoch": 0.6926000447127207,
        "step": 9294
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.941087245941162,
        "learning_rate": 0.00019719053300072084,
        "epoch": 0.6926745659140026,
        "step": 9295
    },
    {
        "loss": 2.7995,
        "grad_norm": 3.6781556606292725,
        "learning_rate": 0.0001971739468246469,
        "epoch": 0.6927490871152843,
        "step": 9296
    },
    {
        "loss": 3.3346,
        "grad_norm": 2.0226473808288574,
        "learning_rate": 0.00019715731253457002,
        "epoch": 0.6928236083165661,
        "step": 9297
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.6897578239440918,
        "learning_rate": 0.00019714063013872646,
        "epoch": 0.6928981295178478,
        "step": 9298
    },
    {
        "loss": 1.7874,
        "grad_norm": 4.098412990570068,
        "learning_rate": 0.00019712389964537613,
        "epoch": 0.6929726507191296,
        "step": 9299
    },
    {
        "loss": 2.3576,
        "grad_norm": 2.277240037918091,
        "learning_rate": 0.00019710712106280293,
        "epoch": 0.6930471719204113,
        "step": 9300
    },
    {
        "loss": 2.7436,
        "grad_norm": 3.4555270671844482,
        "learning_rate": 0.00019709029439931444,
        "epoch": 0.6931216931216931,
        "step": 9301
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.04457950592041,
        "learning_rate": 0.00019707341966324204,
        "epoch": 0.6931962143229748,
        "step": 9302
    },
    {
        "loss": 1.9998,
        "grad_norm": 3.6266238689422607,
        "learning_rate": 0.00019705649686294104,
        "epoch": 0.6932707355242567,
        "step": 9303
    },
    {
        "loss": 2.6189,
        "grad_norm": 2.6205646991729736,
        "learning_rate": 0.0001970395260067905,
        "epoch": 0.6933452567255384,
        "step": 9304
    },
    {
        "loss": 2.3591,
        "grad_norm": 3.1969716548919678,
        "learning_rate": 0.0001970225071031931,
        "epoch": 0.6934197779268202,
        "step": 9305
    },
    {
        "loss": 2.5163,
        "grad_norm": 2.6472978591918945,
        "learning_rate": 0.00019700544016057565,
        "epoch": 0.6934942991281019,
        "step": 9306
    },
    {
        "loss": 2.7576,
        "grad_norm": 2.300182819366455,
        "learning_rate": 0.00019698832518738843,
        "epoch": 0.6935688203293837,
        "step": 9307
    },
    {
        "loss": 2.3947,
        "grad_norm": 2.2188780307769775,
        "learning_rate": 0.0001969711621921056,
        "epoch": 0.6936433415306654,
        "step": 9308
    },
    {
        "loss": 1.8698,
        "grad_norm": 3.495046377182007,
        "learning_rate": 0.00019695395118322524,
        "epoch": 0.6937178627319472,
        "step": 9309
    },
    {
        "loss": 2.2636,
        "grad_norm": 2.8294484615325928,
        "learning_rate": 0.000196936692169269,
        "epoch": 0.693792383933229,
        "step": 9310
    },
    {
        "loss": 2.6595,
        "grad_norm": 2.0526278018951416,
        "learning_rate": 0.00019691938515878233,
        "epoch": 0.6938669051345108,
        "step": 9311
    },
    {
        "loss": 2.3476,
        "grad_norm": 2.5177435874938965,
        "learning_rate": 0.00019690203016033467,
        "epoch": 0.6939414263357926,
        "step": 9312
    },
    {
        "loss": 2.429,
        "grad_norm": 2.7646148204803467,
        "learning_rate": 0.00019688462718251887,
        "epoch": 0.6940159475370743,
        "step": 9313
    },
    {
        "loss": 2.5206,
        "grad_norm": 3.567653179168701,
        "learning_rate": 0.0001968671762339518,
        "epoch": 0.6940904687383561,
        "step": 9314
    },
    {
        "loss": 2.4741,
        "grad_norm": 2.918992280960083,
        "learning_rate": 0.00019684967732327402,
        "epoch": 0.6941649899396378,
        "step": 9315
    },
    {
        "loss": 2.6361,
        "grad_norm": 2.108478307723999,
        "learning_rate": 0.0001968321304591497,
        "epoch": 0.6942395111409196,
        "step": 9316
    },
    {
        "loss": 2.3675,
        "grad_norm": 2.743126630783081,
        "learning_rate": 0.00019681453565026702,
        "epoch": 0.6943140323422013,
        "step": 9317
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.6665198802948,
        "learning_rate": 0.00019679689290533762,
        "epoch": 0.6943885535434832,
        "step": 9318
    },
    {
        "loss": 1.9146,
        "grad_norm": 3.75880765914917,
        "learning_rate": 0.00019677920223309708,
        "epoch": 0.6944630747447649,
        "step": 9319
    },
    {
        "loss": 2.255,
        "grad_norm": 3.8912270069122314,
        "learning_rate": 0.0001967614636423046,
        "epoch": 0.6945375959460467,
        "step": 9320
    },
    {
        "loss": 2.1335,
        "grad_norm": 2.8084170818328857,
        "learning_rate": 0.00019674367714174313,
        "epoch": 0.6946121171473284,
        "step": 9321
    },
    {
        "loss": 1.8479,
        "grad_norm": 3.5363845825195312,
        "learning_rate": 0.00019672584274021935,
        "epoch": 0.6946866383486102,
        "step": 9322
    },
    {
        "loss": 1.8017,
        "grad_norm": 3.6502456665039062,
        "learning_rate": 0.00019670796044656368,
        "epoch": 0.6947611595498919,
        "step": 9323
    },
    {
        "loss": 2.1009,
        "grad_norm": 4.407638072967529,
        "learning_rate": 0.00019669003026963022,
        "epoch": 0.6948356807511737,
        "step": 9324
    },
    {
        "loss": 2.9383,
        "grad_norm": 3.0259721279144287,
        "learning_rate": 0.00019667205221829677,
        "epoch": 0.6949102019524555,
        "step": 9325
    },
    {
        "loss": 2.0674,
        "grad_norm": 3.083420515060425,
        "learning_rate": 0.00019665402630146482,
        "epoch": 0.6949847231537373,
        "step": 9326
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.020585775375366,
        "learning_rate": 0.0001966359525280597,
        "epoch": 0.695059244355019,
        "step": 9327
    },
    {
        "loss": 1.9565,
        "grad_norm": 4.52629280090332,
        "learning_rate": 0.0001966178309070302,
        "epoch": 0.6951337655563008,
        "step": 9328
    },
    {
        "loss": 2.247,
        "grad_norm": 3.4572927951812744,
        "learning_rate": 0.00019659966144734906,
        "epoch": 0.6952082867575825,
        "step": 9329
    },
    {
        "loss": 1.8442,
        "grad_norm": 3.7525675296783447,
        "learning_rate": 0.00019658144415801246,
        "epoch": 0.6952828079588643,
        "step": 9330
    },
    {
        "loss": 2.5934,
        "grad_norm": 1.9313549995422363,
        "learning_rate": 0.0001965631790480404,
        "epoch": 0.695357329160146,
        "step": 9331
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.709125518798828,
        "learning_rate": 0.00019654486612647666,
        "epoch": 0.6954318503614279,
        "step": 9332
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.2972190380096436,
        "learning_rate": 0.00019652650540238832,
        "epoch": 0.6955063715627096,
        "step": 9333
    },
    {
        "loss": 2.033,
        "grad_norm": 3.6895956993103027,
        "learning_rate": 0.00019650809688486663,
        "epoch": 0.6955808927639914,
        "step": 9334
    },
    {
        "loss": 0.936,
        "grad_norm": 4.386777877807617,
        "learning_rate": 0.00019648964058302607,
        "epoch": 0.6956554139652731,
        "step": 9335
    },
    {
        "loss": 2.0224,
        "grad_norm": 2.750786304473877,
        "learning_rate": 0.00019647113650600502,
        "epoch": 0.6957299351665549,
        "step": 9336
    },
    {
        "loss": 2.0799,
        "grad_norm": 2.157039165496826,
        "learning_rate": 0.0001964525846629655,
        "epoch": 0.6958044563678366,
        "step": 9337
    },
    {
        "loss": 2.3676,
        "grad_norm": 4.377341270446777,
        "learning_rate": 0.00019643398506309306,
        "epoch": 0.6958789775691184,
        "step": 9338
    },
    {
        "loss": 2.347,
        "grad_norm": 3.002068042755127,
        "learning_rate": 0.00019641533771559705,
        "epoch": 0.6959534987704001,
        "step": 9339
    },
    {
        "loss": 2.4879,
        "grad_norm": 1.2716561555862427,
        "learning_rate": 0.00019639664262971032,
        "epoch": 0.696028019971682,
        "step": 9340
    },
    {
        "loss": 1.4778,
        "grad_norm": 5.378369331359863,
        "learning_rate": 0.00019637789981468944,
        "epoch": 0.6961025411729637,
        "step": 9341
    },
    {
        "loss": 2.8311,
        "grad_norm": 2.772615909576416,
        "learning_rate": 0.00019635910927981456,
        "epoch": 0.6961770623742455,
        "step": 9342
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.161953926086426,
        "learning_rate": 0.00019634027103438957,
        "epoch": 0.6962515835755272,
        "step": 9343
    },
    {
        "loss": 2.456,
        "grad_norm": 2.4006030559539795,
        "learning_rate": 0.0001963213850877418,
        "epoch": 0.696326104776809,
        "step": 9344
    },
    {
        "loss": 2.8453,
        "grad_norm": 1.9970674514770508,
        "learning_rate": 0.00019630245144922238,
        "epoch": 0.6964006259780907,
        "step": 9345
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.5770609378814697,
        "learning_rate": 0.00019628347012820593,
        "epoch": 0.6964751471793725,
        "step": 9346
    },
    {
        "loss": 2.174,
        "grad_norm": 4.45006799697876,
        "learning_rate": 0.00019626444113409074,
        "epoch": 0.6965496683806542,
        "step": 9347
    },
    {
        "loss": 2.7369,
        "grad_norm": 2.6897380352020264,
        "learning_rate": 0.0001962453644762987,
        "epoch": 0.6966241895819361,
        "step": 9348
    },
    {
        "loss": 2.4625,
        "grad_norm": 3.437732696533203,
        "learning_rate": 0.00019622624016427524,
        "epoch": 0.6966987107832179,
        "step": 9349
    },
    {
        "loss": 2.1305,
        "grad_norm": 3.757533311843872,
        "learning_rate": 0.00019620706820748952,
        "epoch": 0.6967732319844996,
        "step": 9350
    },
    {
        "loss": 2.3517,
        "grad_norm": 2.6519272327423096,
        "learning_rate": 0.0001961878486154341,
        "epoch": 0.6968477531857814,
        "step": 9351
    },
    {
        "loss": 2.2552,
        "grad_norm": 3.3987908363342285,
        "learning_rate": 0.00019616858139762537,
        "epoch": 0.6969222743870631,
        "step": 9352
    },
    {
        "loss": 2.383,
        "grad_norm": 3.479670763015747,
        "learning_rate": 0.000196149266563603,
        "epoch": 0.6969967955883449,
        "step": 9353
    },
    {
        "loss": 2.1056,
        "grad_norm": 4.292192459106445,
        "learning_rate": 0.00019612990412293049,
        "epoch": 0.6970713167896266,
        "step": 9354
    },
    {
        "loss": 2.0423,
        "grad_norm": 3.4210455417633057,
        "learning_rate": 0.00019611049408519479,
        "epoch": 0.6971458379909085,
        "step": 9355
    },
    {
        "loss": 2.6772,
        "grad_norm": 2.161616325378418,
        "learning_rate": 0.00019609103646000642,
        "epoch": 0.6972203591921902,
        "step": 9356
    },
    {
        "loss": 2.132,
        "grad_norm": 2.9765822887420654,
        "learning_rate": 0.00019607153125699952,
        "epoch": 0.697294880393472,
        "step": 9357
    },
    {
        "loss": 1.5037,
        "grad_norm": 1.7607643604278564,
        "learning_rate": 0.0001960519784858318,
        "epoch": 0.6973694015947537,
        "step": 9358
    },
    {
        "loss": 1.2975,
        "grad_norm": 3.4242632389068604,
        "learning_rate": 0.0001960323781561844,
        "epoch": 0.6974439227960355,
        "step": 9359
    },
    {
        "loss": 2.1141,
        "grad_norm": 3.4466915130615234,
        "learning_rate": 0.00019601273027776214,
        "epoch": 0.6975184439973172,
        "step": 9360
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.7499778270721436,
        "learning_rate": 0.0001959930348602933,
        "epoch": 0.697592965198599,
        "step": 9361
    },
    {
        "loss": 2.7265,
        "grad_norm": 1.8445338010787964,
        "learning_rate": 0.0001959732919135297,
        "epoch": 0.6976674863998807,
        "step": 9362
    },
    {
        "loss": 2.54,
        "grad_norm": 2.66081166267395,
        "learning_rate": 0.00019595350144724683,
        "epoch": 0.6977420076011626,
        "step": 9363
    },
    {
        "loss": 2.5313,
        "grad_norm": 3.666217803955078,
        "learning_rate": 0.00019593366347124344,
        "epoch": 0.6978165288024443,
        "step": 9364
    },
    {
        "loss": 2.5623,
        "grad_norm": 3.4793732166290283,
        "learning_rate": 0.00019591377799534204,
        "epoch": 0.6978910500037261,
        "step": 9365
    },
    {
        "loss": 3.0999,
        "grad_norm": 2.182725429534912,
        "learning_rate": 0.00019589384502938857,
        "epoch": 0.6979655712050078,
        "step": 9366
    },
    {
        "loss": 1.9119,
        "grad_norm": 3.596925973892212,
        "learning_rate": 0.00019587386458325257,
        "epoch": 0.6980400924062896,
        "step": 9367
    },
    {
        "loss": 2.6257,
        "grad_norm": 2.996272087097168,
        "learning_rate": 0.0001958538366668269,
        "epoch": 0.6981146136075713,
        "step": 9368
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.3913302421569824,
        "learning_rate": 0.00019583376129002805,
        "epoch": 0.6981891348088531,
        "step": 9369
    },
    {
        "loss": 2.318,
        "grad_norm": 2.7423691749572754,
        "learning_rate": 0.0001958136384627961,
        "epoch": 0.6982636560101348,
        "step": 9370
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.2019951343536377,
        "learning_rate": 0.00019579346819509444,
        "epoch": 0.6983381772114167,
        "step": 9371
    },
    {
        "loss": 1.8897,
        "grad_norm": 3.7822093963623047,
        "learning_rate": 0.00019577325049691002,
        "epoch": 0.6984126984126984,
        "step": 9372
    },
    {
        "loss": 1.663,
        "grad_norm": 3.5105268955230713,
        "learning_rate": 0.0001957529853782533,
        "epoch": 0.6984872196139802,
        "step": 9373
    },
    {
        "loss": 1.9069,
        "grad_norm": 2.9731855392456055,
        "learning_rate": 0.0001957326728491582,
        "epoch": 0.6985617408152619,
        "step": 9374
    },
    {
        "loss": 2.6627,
        "grad_norm": 3.4408531188964844,
        "learning_rate": 0.00019571231291968215,
        "epoch": 0.6986362620165437,
        "step": 9375
    },
    {
        "loss": 2.5342,
        "grad_norm": 2.5720598697662354,
        "learning_rate": 0.00019569190559990603,
        "epoch": 0.6987107832178254,
        "step": 9376
    },
    {
        "loss": 1.8142,
        "grad_norm": 4.063268184661865,
        "learning_rate": 0.00019567145089993403,
        "epoch": 0.6987853044191072,
        "step": 9377
    },
    {
        "loss": 2.6651,
        "grad_norm": 2.2469570636749268,
        "learning_rate": 0.00019565094882989414,
        "epoch": 0.698859825620389,
        "step": 9378
    },
    {
        "loss": 2.0252,
        "grad_norm": 2.9178826808929443,
        "learning_rate": 0.00019563039939993746,
        "epoch": 0.6989343468216708,
        "step": 9379
    },
    {
        "loss": 2.331,
        "grad_norm": 2.572481870651245,
        "learning_rate": 0.0001956098026202388,
        "epoch": 0.6990088680229525,
        "step": 9380
    },
    {
        "loss": 2.0745,
        "grad_norm": 5.8411126136779785,
        "learning_rate": 0.0001955891585009962,
        "epoch": 0.6990833892242343,
        "step": 9381
    },
    {
        "loss": 2.6124,
        "grad_norm": 3.9628849029541016,
        "learning_rate": 0.00019556846705243126,
        "epoch": 0.699157910425516,
        "step": 9382
    },
    {
        "loss": 1.8884,
        "grad_norm": 2.060336112976074,
        "learning_rate": 0.0001955477282847891,
        "epoch": 0.6992324316267978,
        "step": 9383
    },
    {
        "loss": 2.7174,
        "grad_norm": 3.3971235752105713,
        "learning_rate": 0.00019552694220833797,
        "epoch": 0.6993069528280796,
        "step": 9384
    },
    {
        "loss": 2.5616,
        "grad_norm": 2.7684123516082764,
        "learning_rate": 0.00019550610883336994,
        "epoch": 0.6993814740293613,
        "step": 9385
    },
    {
        "loss": 2.5689,
        "grad_norm": 4.172328472137451,
        "learning_rate": 0.00019548522817020017,
        "epoch": 0.6994559952306432,
        "step": 9386
    },
    {
        "loss": 2.5662,
        "grad_norm": 2.3354358673095703,
        "learning_rate": 0.0001954643002291673,
        "epoch": 0.6995305164319249,
        "step": 9387
    },
    {
        "loss": 2.6913,
        "grad_norm": 2.47544002532959,
        "learning_rate": 0.00019544332502063365,
        "epoch": 0.6996050376332067,
        "step": 9388
    },
    {
        "loss": 1.9781,
        "grad_norm": 3.691389322280884,
        "learning_rate": 0.00019542230255498454,
        "epoch": 0.6996795588344884,
        "step": 9389
    },
    {
        "loss": 2.831,
        "grad_norm": 2.189561367034912,
        "learning_rate": 0.00019540123284262896,
        "epoch": 0.6997540800357702,
        "step": 9390
    },
    {
        "loss": 1.7816,
        "grad_norm": 3.628493547439575,
        "learning_rate": 0.00019538011589399922,
        "epoch": 0.6998286012370519,
        "step": 9391
    },
    {
        "loss": 2.7055,
        "grad_norm": 2.9682488441467285,
        "learning_rate": 0.00019535895171955106,
        "epoch": 0.6999031224383337,
        "step": 9392
    },
    {
        "loss": 2.2333,
        "grad_norm": 3.0276405811309814,
        "learning_rate": 0.00019533774032976339,
        "epoch": 0.6999776436396155,
        "step": 9393
    },
    {
        "loss": 1.8648,
        "grad_norm": 3.068511724472046,
        "learning_rate": 0.00019531648173513882,
        "epoch": 0.7000521648408973,
        "step": 9394
    },
    {
        "loss": 2.1597,
        "grad_norm": 2.458238363265991,
        "learning_rate": 0.00019529517594620312,
        "epoch": 0.700126686042179,
        "step": 9395
    },
    {
        "loss": 2.0111,
        "grad_norm": 3.559725761413574,
        "learning_rate": 0.00019527382297350552,
        "epoch": 0.7002012072434608,
        "step": 9396
    },
    {
        "loss": 2.0494,
        "grad_norm": 3.047308921813965,
        "learning_rate": 0.0001952524228276185,
        "epoch": 0.7002757284447425,
        "step": 9397
    },
    {
        "loss": 2.4905,
        "grad_norm": 4.992232799530029,
        "learning_rate": 0.00019523097551913803,
        "epoch": 0.7003502496460243,
        "step": 9398
    },
    {
        "loss": 2.1145,
        "grad_norm": 2.8347482681274414,
        "learning_rate": 0.00019520948105868335,
        "epoch": 0.700424770847306,
        "step": 9399
    },
    {
        "loss": 1.9169,
        "grad_norm": 4.120013236999512,
        "learning_rate": 0.00019518793945689708,
        "epoch": 0.7004992920485879,
        "step": 9400
    },
    {
        "loss": 2.1714,
        "grad_norm": 4.7747802734375,
        "learning_rate": 0.00019516635072444524,
        "epoch": 0.7005738132498696,
        "step": 9401
    },
    {
        "loss": 1.2662,
        "grad_norm": 2.1151158809661865,
        "learning_rate": 0.00019514471487201696,
        "epoch": 0.7006483344511514,
        "step": 9402
    },
    {
        "loss": 2.3369,
        "grad_norm": 3.2566580772399902,
        "learning_rate": 0.00019512303191032509,
        "epoch": 0.7007228556524331,
        "step": 9403
    },
    {
        "loss": 2.5926,
        "grad_norm": 2.6500658988952637,
        "learning_rate": 0.00019510130185010528,
        "epoch": 0.7007973768537149,
        "step": 9404
    },
    {
        "loss": 2.0348,
        "grad_norm": 2.7316839694976807,
        "learning_rate": 0.00019507952470211706,
        "epoch": 0.7008718980549966,
        "step": 9405
    },
    {
        "loss": 2.3087,
        "grad_norm": 2.7037832736968994,
        "learning_rate": 0.00019505770047714287,
        "epoch": 0.7009464192562784,
        "step": 9406
    },
    {
        "loss": 2.9478,
        "grad_norm": 2.355573892593384,
        "learning_rate": 0.0001950358291859886,
        "epoch": 0.7010209404575601,
        "step": 9407
    },
    {
        "loss": 2.6961,
        "grad_norm": 3.2672293186187744,
        "learning_rate": 0.0001950139108394835,
        "epoch": 0.701095461658842,
        "step": 9408
    },
    {
        "loss": 2.2789,
        "grad_norm": 3.0801327228546143,
        "learning_rate": 0.00019499194544848005,
        "epoch": 0.7011699828601237,
        "step": 9409
    },
    {
        "loss": 2.7287,
        "grad_norm": 3.661193370819092,
        "learning_rate": 0.00019496993302385395,
        "epoch": 0.7012445040614055,
        "step": 9410
    },
    {
        "loss": 1.9853,
        "grad_norm": 4.608469009399414,
        "learning_rate": 0.0001949478735765044,
        "epoch": 0.7013190252626872,
        "step": 9411
    },
    {
        "loss": 2.9445,
        "grad_norm": 3.1538829803466797,
        "learning_rate": 0.0001949257671173537,
        "epoch": 0.701393546463969,
        "step": 9412
    },
    {
        "loss": 2.5681,
        "grad_norm": 2.3709163665771484,
        "learning_rate": 0.00019490361365734744,
        "epoch": 0.7014680676652507,
        "step": 9413
    },
    {
        "loss": 2.4807,
        "grad_norm": 2.612126111984253,
        "learning_rate": 0.00019488141320745463,
        "epoch": 0.7015425888665325,
        "step": 9414
    },
    {
        "loss": 2.0903,
        "grad_norm": 3.8905112743377686,
        "learning_rate": 0.00019485916577866727,
        "epoch": 0.7016171100678142,
        "step": 9415
    },
    {
        "loss": 1.6174,
        "grad_norm": 4.0670061111450195,
        "learning_rate": 0.00019483687138200097,
        "epoch": 0.7016916312690961,
        "step": 9416
    },
    {
        "loss": 1.7317,
        "grad_norm": 2.052873373031616,
        "learning_rate": 0.00019481453002849426,
        "epoch": 0.7017661524703778,
        "step": 9417
    },
    {
        "loss": 2.1217,
        "grad_norm": 4.13107967376709,
        "learning_rate": 0.0001947921417292092,
        "epoch": 0.7018406736716596,
        "step": 9418
    },
    {
        "loss": 3.2737,
        "grad_norm": 3.0711300373077393,
        "learning_rate": 0.0001947697064952309,
        "epoch": 0.7019151948729414,
        "step": 9419
    },
    {
        "loss": 1.6537,
        "grad_norm": 2.4886984825134277,
        "learning_rate": 0.0001947472243376678,
        "epoch": 0.7019897160742231,
        "step": 9420
    },
    {
        "loss": 1.7609,
        "grad_norm": 3.4563727378845215,
        "learning_rate": 0.00019472469526765157,
        "epoch": 0.7020642372755049,
        "step": 9421
    },
    {
        "loss": 2.3399,
        "grad_norm": 4.076202392578125,
        "learning_rate": 0.00019470211929633707,
        "epoch": 0.7021387584767866,
        "step": 9422
    },
    {
        "loss": 1.6552,
        "grad_norm": 3.392672061920166,
        "learning_rate": 0.00019467949643490245,
        "epoch": 0.7022132796780685,
        "step": 9423
    },
    {
        "loss": 2.4716,
        "grad_norm": 3.389866590499878,
        "learning_rate": 0.0001946568266945489,
        "epoch": 0.7022878008793502,
        "step": 9424
    },
    {
        "loss": 2.7454,
        "grad_norm": 4.336813926696777,
        "learning_rate": 0.0001946341100865011,
        "epoch": 0.702362322080632,
        "step": 9425
    },
    {
        "loss": 2.4902,
        "grad_norm": 1.938081979751587,
        "learning_rate": 0.00019461134662200668,
        "epoch": 0.7024368432819137,
        "step": 9426
    },
    {
        "loss": 2.3642,
        "grad_norm": 2.9490997791290283,
        "learning_rate": 0.00019458853631233662,
        "epoch": 0.7025113644831955,
        "step": 9427
    },
    {
        "loss": 2.6278,
        "grad_norm": 2.257835865020752,
        "learning_rate": 0.00019456567916878505,
        "epoch": 0.7025858856844772,
        "step": 9428
    },
    {
        "loss": 2.369,
        "grad_norm": 3.236658811569214,
        "learning_rate": 0.00019454277520266936,
        "epoch": 0.702660406885759,
        "step": 9429
    },
    {
        "loss": 1.8628,
        "grad_norm": 3.225794553756714,
        "learning_rate": 0.00019451982442532986,
        "epoch": 0.7027349280870407,
        "step": 9430
    },
    {
        "loss": 2.5333,
        "grad_norm": 3.1206254959106445,
        "learning_rate": 0.00019449682684813044,
        "epoch": 0.7028094492883226,
        "step": 9431
    },
    {
        "loss": 1.7326,
        "grad_norm": 4.434518814086914,
        "learning_rate": 0.00019447378248245786,
        "epoch": 0.7028839704896043,
        "step": 9432
    },
    {
        "loss": 3.1654,
        "grad_norm": 3.9136338233947754,
        "learning_rate": 0.0001944506913397221,
        "epoch": 0.7029584916908861,
        "step": 9433
    },
    {
        "loss": 2.6253,
        "grad_norm": 3.212592363357544,
        "learning_rate": 0.0001944275534313565,
        "epoch": 0.7030330128921678,
        "step": 9434
    },
    {
        "loss": 2.454,
        "grad_norm": 3.845924139022827,
        "learning_rate": 0.00019440436876881718,
        "epoch": 0.7031075340934496,
        "step": 9435
    },
    {
        "loss": 1.7174,
        "grad_norm": 3.8718647956848145,
        "learning_rate": 0.00019438113736358376,
        "epoch": 0.7031820552947313,
        "step": 9436
    },
    {
        "loss": 1.5563,
        "grad_norm": 3.6302294731140137,
        "learning_rate": 0.0001943578592271589,
        "epoch": 0.7032565764960131,
        "step": 9437
    },
    {
        "loss": 2.3577,
        "grad_norm": 2.359276294708252,
        "learning_rate": 0.0001943345343710682,
        "epoch": 0.7033310976972948,
        "step": 9438
    },
    {
        "loss": 2.7575,
        "grad_norm": 2.6613008975982666,
        "learning_rate": 0.00019431116280686073,
        "epoch": 0.7034056188985767,
        "step": 9439
    },
    {
        "loss": 2.5315,
        "grad_norm": 2.7643327713012695,
        "learning_rate": 0.0001942877445461084,
        "epoch": 0.7034801400998584,
        "step": 9440
    },
    {
        "loss": 2.0463,
        "grad_norm": 2.2644195556640625,
        "learning_rate": 0.00019426427960040648,
        "epoch": 0.7035546613011402,
        "step": 9441
    },
    {
        "loss": 2.4982,
        "grad_norm": 3.61611270904541,
        "learning_rate": 0.00019424076798137314,
        "epoch": 0.7036291825024219,
        "step": 9442
    },
    {
        "loss": 2.2745,
        "grad_norm": 3.6129229068756104,
        "learning_rate": 0.0001942172097006498,
        "epoch": 0.7037037037037037,
        "step": 9443
    },
    {
        "loss": 2.7461,
        "grad_norm": 2.254368782043457,
        "learning_rate": 0.00019419360476990085,
        "epoch": 0.7037782249049854,
        "step": 9444
    },
    {
        "loss": 2.0761,
        "grad_norm": 3.7322025299072266,
        "learning_rate": 0.000194169953200814,
        "epoch": 0.7038527461062672,
        "step": 9445
    },
    {
        "loss": 2.1927,
        "grad_norm": 2.3769893646240234,
        "learning_rate": 0.00019414625500509982,
        "epoch": 0.703927267307549,
        "step": 9446
    },
    {
        "loss": 1.6706,
        "grad_norm": 2.400277853012085,
        "learning_rate": 0.0001941225101944921,
        "epoch": 0.7040017885088308,
        "step": 9447
    },
    {
        "loss": 2.6823,
        "grad_norm": 2.8218889236450195,
        "learning_rate": 0.00019409871878074762,
        "epoch": 0.7040763097101125,
        "step": 9448
    },
    {
        "loss": 2.2061,
        "grad_norm": 2.632469654083252,
        "learning_rate": 0.00019407488077564634,
        "epoch": 0.7041508309113943,
        "step": 9449
    },
    {
        "loss": 1.9353,
        "grad_norm": 3.0567946434020996,
        "learning_rate": 0.00019405099619099122,
        "epoch": 0.704225352112676,
        "step": 9450
    },
    {
        "loss": 2.4829,
        "grad_norm": 1.977468490600586,
        "learning_rate": 0.00019402706503860833,
        "epoch": 0.7042998733139578,
        "step": 9451
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.1284918785095215,
        "learning_rate": 0.00019400308733034675,
        "epoch": 0.7043743945152395,
        "step": 9452
    },
    {
        "loss": 2.5382,
        "grad_norm": 3.0711216926574707,
        "learning_rate": 0.00019397906307807856,
        "epoch": 0.7044489157165214,
        "step": 9453
    },
    {
        "loss": 2.7329,
        "grad_norm": 2.067598342895508,
        "learning_rate": 0.00019395499229369912,
        "epoch": 0.7045234369178032,
        "step": 9454
    },
    {
        "loss": 2.2936,
        "grad_norm": 2.7075140476226807,
        "learning_rate": 0.00019393087498912646,
        "epoch": 0.7045979581190849,
        "step": 9455
    },
    {
        "loss": 2.9134,
        "grad_norm": 3.8647842407226562,
        "learning_rate": 0.000193906711176302,
        "epoch": 0.7046724793203667,
        "step": 9456
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.3762011528015137,
        "learning_rate": 0.00019388250086718997,
        "epoch": 0.7047470005216484,
        "step": 9457
    },
    {
        "loss": 2.0751,
        "grad_norm": 4.518858909606934,
        "learning_rate": 0.00019385824407377764,
        "epoch": 0.7048215217229302,
        "step": 9458
    },
    {
        "loss": 2.3208,
        "grad_norm": 3.2503268718719482,
        "learning_rate": 0.00019383394080807546,
        "epoch": 0.7048960429242119,
        "step": 9459
    },
    {
        "loss": 2.3704,
        "grad_norm": 3.78513240814209,
        "learning_rate": 0.00019380959108211668,
        "epoch": 0.7049705641254937,
        "step": 9460
    },
    {
        "loss": 2.1533,
        "grad_norm": 4.127738952636719,
        "learning_rate": 0.0001937851949079577,
        "epoch": 0.7050450853267755,
        "step": 9461
    },
    {
        "loss": 1.2635,
        "grad_norm": 1.1005414724349976,
        "learning_rate": 0.00019376075229767786,
        "epoch": 0.7051196065280573,
        "step": 9462
    },
    {
        "loss": 3.0427,
        "grad_norm": 2.8292760848999023,
        "learning_rate": 0.00019373626326337948,
        "epoch": 0.705194127729339,
        "step": 9463
    },
    {
        "loss": 2.13,
        "grad_norm": 3.338933229446411,
        "learning_rate": 0.00019371172781718781,
        "epoch": 0.7052686489306208,
        "step": 9464
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.331223964691162,
        "learning_rate": 0.00019368714597125134,
        "epoch": 0.7053431701319025,
        "step": 9465
    },
    {
        "loss": 2.3098,
        "grad_norm": 3.310666561126709,
        "learning_rate": 0.00019366251773774115,
        "epoch": 0.7054176913331843,
        "step": 9466
    },
    {
        "loss": 2.8233,
        "grad_norm": 2.1707582473754883,
        "learning_rate": 0.0001936378431288516,
        "epoch": 0.705492212534466,
        "step": 9467
    },
    {
        "loss": 2.715,
        "grad_norm": 3.6028659343719482,
        "learning_rate": 0.00019361312215679984,
        "epoch": 0.7055667337357479,
        "step": 9468
    },
    {
        "loss": 2.3619,
        "grad_norm": 2.0179052352905273,
        "learning_rate": 0.00019358835483382608,
        "epoch": 0.7056412549370296,
        "step": 9469
    },
    {
        "loss": 2.1595,
        "grad_norm": 2.965839385986328,
        "learning_rate": 0.0001935635411721934,
        "epoch": 0.7057157761383114,
        "step": 9470
    },
    {
        "loss": 2.5638,
        "grad_norm": 3.006073236465454,
        "learning_rate": 0.00019353868118418785,
        "epoch": 0.7057902973395931,
        "step": 9471
    },
    {
        "loss": 2.872,
        "grad_norm": 3.2798712253570557,
        "learning_rate": 0.00019351377488211846,
        "epoch": 0.7058648185408749,
        "step": 9472
    },
    {
        "loss": 2.4193,
        "grad_norm": 3.328521728515625,
        "learning_rate": 0.0001934888222783171,
        "epoch": 0.7059393397421566,
        "step": 9473
    },
    {
        "loss": 2.033,
        "grad_norm": 3.5286154747009277,
        "learning_rate": 0.00019346382338513874,
        "epoch": 0.7060138609434384,
        "step": 9474
    },
    {
        "loss": 2.5739,
        "grad_norm": 3.0281882286071777,
        "learning_rate": 0.000193438778214961,
        "epoch": 0.7060883821447201,
        "step": 9475
    },
    {
        "loss": 2.8645,
        "grad_norm": 2.5001349449157715,
        "learning_rate": 0.00019341368678018464,
        "epoch": 0.706162903346002,
        "step": 9476
    },
    {
        "loss": 2.8491,
        "grad_norm": 3.216965675354004,
        "learning_rate": 0.00019338854909323327,
        "epoch": 0.7062374245472837,
        "step": 9477
    },
    {
        "loss": 2.3696,
        "grad_norm": 2.315887689590454,
        "learning_rate": 0.00019336336516655333,
        "epoch": 0.7063119457485655,
        "step": 9478
    },
    {
        "loss": 2.7774,
        "grad_norm": 3.4489922523498535,
        "learning_rate": 0.00019333813501261422,
        "epoch": 0.7063864669498472,
        "step": 9479
    },
    {
        "loss": 2.1084,
        "grad_norm": 3.128246545791626,
        "learning_rate": 0.00019331285864390826,
        "epoch": 0.706460988151129,
        "step": 9480
    },
    {
        "loss": 2.5661,
        "grad_norm": 2.196047782897949,
        "learning_rate": 0.0001932875360729505,
        "epoch": 0.7065355093524107,
        "step": 9481
    },
    {
        "loss": 1.4625,
        "grad_norm": 4.095388412475586,
        "learning_rate": 0.00019326216731227914,
        "epoch": 0.7066100305536925,
        "step": 9482
    },
    {
        "loss": 2.7388,
        "grad_norm": 3.258892059326172,
        "learning_rate": 0.00019323675237445495,
        "epoch": 0.7066845517549742,
        "step": 9483
    },
    {
        "loss": 2.3409,
        "grad_norm": 2.9732747077941895,
        "learning_rate": 0.0001932112912720617,
        "epoch": 0.7067590729562561,
        "step": 9484
    },
    {
        "loss": 1.6833,
        "grad_norm": 4.058376789093018,
        "learning_rate": 0.00019318578401770619,
        "epoch": 0.7068335941575378,
        "step": 9485
    },
    {
        "loss": 2.3745,
        "grad_norm": 3.839432716369629,
        "learning_rate": 0.00019316023062401763,
        "epoch": 0.7069081153588196,
        "step": 9486
    },
    {
        "loss": 2.3581,
        "grad_norm": 2.313084840774536,
        "learning_rate": 0.0001931346311036485,
        "epoch": 0.7069826365601013,
        "step": 9487
    },
    {
        "loss": 2.6144,
        "grad_norm": 1.9901237487792969,
        "learning_rate": 0.00019310898546927394,
        "epoch": 0.7070571577613831,
        "step": 9488
    },
    {
        "loss": 1.7539,
        "grad_norm": 4.065396785736084,
        "learning_rate": 0.00019308329373359196,
        "epoch": 0.7071316789626649,
        "step": 9489
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.5228171348571777,
        "learning_rate": 0.00019305755590932338,
        "epoch": 0.7072062001639466,
        "step": 9490
    },
    {
        "loss": 2.0949,
        "grad_norm": 5.005460262298584,
        "learning_rate": 0.00019303177200921171,
        "epoch": 0.7072807213652285,
        "step": 9491
    },
    {
        "loss": 2.6678,
        "grad_norm": 3.181814670562744,
        "learning_rate": 0.0001930059420460236,
        "epoch": 0.7073552425665102,
        "step": 9492
    },
    {
        "loss": 2.3359,
        "grad_norm": 1.834542989730835,
        "learning_rate": 0.00019298006603254818,
        "epoch": 0.707429763767792,
        "step": 9493
    },
    {
        "loss": 1.8411,
        "grad_norm": 3.6528220176696777,
        "learning_rate": 0.00019295414398159765,
        "epoch": 0.7075042849690737,
        "step": 9494
    },
    {
        "loss": 2.296,
        "grad_norm": 3.397782325744629,
        "learning_rate": 0.00019292817590600665,
        "epoch": 0.7075788061703555,
        "step": 9495
    },
    {
        "loss": 2.5704,
        "grad_norm": 2.1637344360351562,
        "learning_rate": 0.00019290216181863301,
        "epoch": 0.7076533273716372,
        "step": 9496
    },
    {
        "loss": 2.4094,
        "grad_norm": 3.1637096405029297,
        "learning_rate": 0.00019287610173235708,
        "epoch": 0.707727848572919,
        "step": 9497
    },
    {
        "loss": 1.4839,
        "grad_norm": 2.888284206390381,
        "learning_rate": 0.0001928499956600821,
        "epoch": 0.7078023697742007,
        "step": 9498
    },
    {
        "loss": 2.6451,
        "grad_norm": 2.759397506713867,
        "learning_rate": 0.0001928238436147339,
        "epoch": 0.7078768909754826,
        "step": 9499
    },
    {
        "loss": 2.3854,
        "grad_norm": 3.09474778175354,
        "learning_rate": 0.00019279764560926142,
        "epoch": 0.7079514121767643,
        "step": 9500
    },
    {
        "loss": 2.29,
        "grad_norm": 3.801764965057373,
        "learning_rate": 0.000192771401656636,
        "epoch": 0.7080259333780461,
        "step": 9501
    },
    {
        "loss": 2.5112,
        "grad_norm": 2.9903953075408936,
        "learning_rate": 0.000192745111769852,
        "epoch": 0.7081004545793278,
        "step": 9502
    },
    {
        "loss": 2.5597,
        "grad_norm": 2.231058359146118,
        "learning_rate": 0.00019271877596192634,
        "epoch": 0.7081749757806096,
        "step": 9503
    },
    {
        "loss": 2.4426,
        "grad_norm": 3.8817920684814453,
        "learning_rate": 0.00019269239424589868,
        "epoch": 0.7082494969818913,
        "step": 9504
    },
    {
        "loss": 2.2206,
        "grad_norm": 3.835149049758911,
        "learning_rate": 0.00019266596663483163,
        "epoch": 0.7083240181831731,
        "step": 9505
    },
    {
        "loss": 2.0189,
        "grad_norm": 4.297699451446533,
        "learning_rate": 0.00019263949314181017,
        "epoch": 0.7083985393844549,
        "step": 9506
    },
    {
        "loss": 2.3361,
        "grad_norm": 2.184422492980957,
        "learning_rate": 0.00019261297377994233,
        "epoch": 0.7084730605857367,
        "step": 9507
    },
    {
        "loss": 1.8781,
        "grad_norm": 2.9975225925445557,
        "learning_rate": 0.0001925864085623587,
        "epoch": 0.7085475817870184,
        "step": 9508
    },
    {
        "loss": 1.9601,
        "grad_norm": 3.02097749710083,
        "learning_rate": 0.00019255979750221254,
        "epoch": 0.7086221029883002,
        "step": 9509
    },
    {
        "loss": 2.4896,
        "grad_norm": 3.5045173168182373,
        "learning_rate": 0.0001925331406126799,
        "epoch": 0.7086966241895819,
        "step": 9510
    },
    {
        "loss": 2.8514,
        "grad_norm": 2.5152652263641357,
        "learning_rate": 0.00019250643790695942,
        "epoch": 0.7087711453908637,
        "step": 9511
    },
    {
        "loss": 2.38,
        "grad_norm": 2.1340909004211426,
        "learning_rate": 0.00019247968939827256,
        "epoch": 0.7088456665921454,
        "step": 9512
    },
    {
        "loss": 2.7255,
        "grad_norm": 2.82191801071167,
        "learning_rate": 0.00019245289509986336,
        "epoch": 0.7089201877934272,
        "step": 9513
    },
    {
        "loss": 1.7385,
        "grad_norm": 2.134603977203369,
        "learning_rate": 0.00019242605502499855,
        "epoch": 0.708994708994709,
        "step": 9514
    },
    {
        "loss": 2.7488,
        "grad_norm": 3.479355573654175,
        "learning_rate": 0.00019239916918696747,
        "epoch": 0.7090692301959908,
        "step": 9515
    },
    {
        "loss": 2.3915,
        "grad_norm": 3.123537540435791,
        "learning_rate": 0.00019237223759908226,
        "epoch": 0.7091437513972725,
        "step": 9516
    },
    {
        "loss": 2.4147,
        "grad_norm": 3.1475422382354736,
        "learning_rate": 0.00019234526027467764,
        "epoch": 0.7092182725985543,
        "step": 9517
    },
    {
        "loss": 2.4146,
        "grad_norm": 5.238472938537598,
        "learning_rate": 0.00019231823722711092,
        "epoch": 0.709292793799836,
        "step": 9518
    },
    {
        "loss": 2.7443,
        "grad_norm": 2.7081403732299805,
        "learning_rate": 0.00019229116846976202,
        "epoch": 0.7093673150011178,
        "step": 9519
    },
    {
        "loss": 1.833,
        "grad_norm": 3.7191267013549805,
        "learning_rate": 0.00019226405401603374,
        "epoch": 0.7094418362023995,
        "step": 9520
    },
    {
        "loss": 1.8798,
        "grad_norm": 4.532134532928467,
        "learning_rate": 0.0001922368938793512,
        "epoch": 0.7095163574036814,
        "step": 9521
    },
    {
        "loss": 2.6038,
        "grad_norm": 2.834547758102417,
        "learning_rate": 0.00019220968807316232,
        "epoch": 0.7095908786049631,
        "step": 9522
    },
    {
        "loss": 3.12,
        "grad_norm": 2.4117438793182373,
        "learning_rate": 0.00019218243661093761,
        "epoch": 0.7096653998062449,
        "step": 9523
    },
    {
        "loss": 2.4903,
        "grad_norm": 2.4507057666778564,
        "learning_rate": 0.00019215513950617013,
        "epoch": 0.7097399210075266,
        "step": 9524
    },
    {
        "loss": 2.7024,
        "grad_norm": 2.784040689468384,
        "learning_rate": 0.00019212779677237562,
        "epoch": 0.7098144422088084,
        "step": 9525
    },
    {
        "loss": 2.6077,
        "grad_norm": 3.0720412731170654,
        "learning_rate": 0.00019210040842309228,
        "epoch": 0.7098889634100902,
        "step": 9526
    },
    {
        "loss": 2.2881,
        "grad_norm": 2.6500635147094727,
        "learning_rate": 0.000192072974471881,
        "epoch": 0.7099634846113719,
        "step": 9527
    },
    {
        "loss": 2.1868,
        "grad_norm": 2.4134788513183594,
        "learning_rate": 0.00019204549493232528,
        "epoch": 0.7100380058126537,
        "step": 9528
    },
    {
        "loss": 2.885,
        "grad_norm": 2.9562621116638184,
        "learning_rate": 0.00019201796981803106,
        "epoch": 0.7101125270139355,
        "step": 9529
    },
    {
        "loss": 2.4443,
        "grad_norm": 2.882838249206543,
        "learning_rate": 0.00019199039914262703,
        "epoch": 0.7101870482152173,
        "step": 9530
    },
    {
        "loss": 2.6144,
        "grad_norm": 2.587472438812256,
        "learning_rate": 0.00019196278291976423,
        "epoch": 0.710261569416499,
        "step": 9531
    },
    {
        "loss": 2.7832,
        "grad_norm": 2.9596612453460693,
        "learning_rate": 0.00019193512116311642,
        "epoch": 0.7103360906177808,
        "step": 9532
    },
    {
        "loss": 2.5825,
        "grad_norm": 3.0376551151275635,
        "learning_rate": 0.00019190741388637982,
        "epoch": 0.7104106118190625,
        "step": 9533
    },
    {
        "loss": 2.7245,
        "grad_norm": 2.758333444595337,
        "learning_rate": 0.00019187966110327322,
        "epoch": 0.7104851330203443,
        "step": 9534
    },
    {
        "loss": 1.9295,
        "grad_norm": 2.121640920639038,
        "learning_rate": 0.00019185186282753787,
        "epoch": 0.710559654221626,
        "step": 9535
    },
    {
        "loss": 2.8961,
        "grad_norm": 1.8756672143936157,
        "learning_rate": 0.00019182401907293778,
        "epoch": 0.7106341754229079,
        "step": 9536
    },
    {
        "loss": 2.3535,
        "grad_norm": 3.1250078678131104,
        "learning_rate": 0.00019179612985325908,
        "epoch": 0.7107086966241896,
        "step": 9537
    },
    {
        "loss": 2.5281,
        "grad_norm": 5.918027877807617,
        "learning_rate": 0.00019176819518231078,
        "epoch": 0.7107832178254714,
        "step": 9538
    },
    {
        "loss": 2.9125,
        "grad_norm": 3.3346900939941406,
        "learning_rate": 0.00019174021507392417,
        "epoch": 0.7108577390267531,
        "step": 9539
    },
    {
        "loss": 2.5254,
        "grad_norm": 3.0428802967071533,
        "learning_rate": 0.00019171218954195323,
        "epoch": 0.7109322602280349,
        "step": 9540
    },
    {
        "loss": 2.0921,
        "grad_norm": 5.865776538848877,
        "learning_rate": 0.00019168411860027423,
        "epoch": 0.7110067814293166,
        "step": 9541
    },
    {
        "loss": 2.289,
        "grad_norm": 3.127258062362671,
        "learning_rate": 0.00019165600226278597,
        "epoch": 0.7110813026305984,
        "step": 9542
    },
    {
        "loss": 2.5911,
        "grad_norm": 3.2513577938079834,
        "learning_rate": 0.00019162784054340987,
        "epoch": 0.7111558238318801,
        "step": 9543
    },
    {
        "loss": 2.6264,
        "grad_norm": 2.2373735904693604,
        "learning_rate": 0.00019159963345608967,
        "epoch": 0.711230345033162,
        "step": 9544
    },
    {
        "loss": 2.0511,
        "grad_norm": 2.4754788875579834,
        "learning_rate": 0.0001915713810147917,
        "epoch": 0.7113048662344437,
        "step": 9545
    },
    {
        "loss": 2.3723,
        "grad_norm": 3.5409255027770996,
        "learning_rate": 0.00019154308323350453,
        "epoch": 0.7113793874357255,
        "step": 9546
    },
    {
        "loss": 1.6429,
        "grad_norm": 3.679011106491089,
        "learning_rate": 0.0001915147401262394,
        "epoch": 0.7114539086370072,
        "step": 9547
    },
    {
        "loss": 1.8314,
        "grad_norm": 3.681375026702881,
        "learning_rate": 0.00019148635170702992,
        "epoch": 0.711528429838289,
        "step": 9548
    },
    {
        "loss": 2.6932,
        "grad_norm": 3.0088672637939453,
        "learning_rate": 0.00019145791798993206,
        "epoch": 0.7116029510395707,
        "step": 9549
    },
    {
        "loss": 2.3123,
        "grad_norm": 2.303344249725342,
        "learning_rate": 0.00019142943898902437,
        "epoch": 0.7116774722408525,
        "step": 9550
    },
    {
        "loss": 2.7753,
        "grad_norm": 2.340195894241333,
        "learning_rate": 0.0001914009147184077,
        "epoch": 0.7117519934421342,
        "step": 9551
    },
    {
        "loss": 2.1824,
        "grad_norm": 2.065518617630005,
        "learning_rate": 0.0001913723451922053,
        "epoch": 0.7118265146434161,
        "step": 9552
    },
    {
        "loss": 2.3386,
        "grad_norm": 2.9790592193603516,
        "learning_rate": 0.00019134373042456292,
        "epoch": 0.7119010358446978,
        "step": 9553
    },
    {
        "loss": 2.6233,
        "grad_norm": 4.106950283050537,
        "learning_rate": 0.00019131507042964875,
        "epoch": 0.7119755570459796,
        "step": 9554
    },
    {
        "loss": 2.4209,
        "grad_norm": 2.436188220977783,
        "learning_rate": 0.00019128636522165314,
        "epoch": 0.7120500782472613,
        "step": 9555
    },
    {
        "loss": 1.4842,
        "grad_norm": 3.732959747314453,
        "learning_rate": 0.00019125761481478916,
        "epoch": 0.7121245994485431,
        "step": 9556
    },
    {
        "loss": 2.5047,
        "grad_norm": 3.8049280643463135,
        "learning_rate": 0.00019122881922329184,
        "epoch": 0.7121991206498248,
        "step": 9557
    },
    {
        "loss": 2.4261,
        "grad_norm": 3.343928813934326,
        "learning_rate": 0.00019119997846141898,
        "epoch": 0.7122736418511066,
        "step": 9558
    },
    {
        "loss": 2.976,
        "grad_norm": 3.1972715854644775,
        "learning_rate": 0.00019117109254345062,
        "epoch": 0.7123481630523884,
        "step": 9559
    },
    {
        "loss": 2.3838,
        "grad_norm": 2.7890026569366455,
        "learning_rate": 0.00019114216148368896,
        "epoch": 0.7124226842536702,
        "step": 9560
    },
    {
        "loss": 2.2967,
        "grad_norm": 3.5151867866516113,
        "learning_rate": 0.0001911131852964589,
        "epoch": 0.712497205454952,
        "step": 9561
    },
    {
        "loss": 2.8696,
        "grad_norm": 3.599916458129883,
        "learning_rate": 0.00019108416399610733,
        "epoch": 0.7125717266562337,
        "step": 9562
    },
    {
        "loss": 2.7505,
        "grad_norm": 3.3960788249969482,
        "learning_rate": 0.0001910550975970038,
        "epoch": 0.7126462478575155,
        "step": 9563
    },
    {
        "loss": 1.3866,
        "grad_norm": 3.382227897644043,
        "learning_rate": 0.00019102598611353997,
        "epoch": 0.7127207690587972,
        "step": 9564
    },
    {
        "loss": 1.7925,
        "grad_norm": 2.4463930130004883,
        "learning_rate": 0.00019099682956012987,
        "epoch": 0.712795290260079,
        "step": 9565
    },
    {
        "loss": 2.2414,
        "grad_norm": 3.916335344314575,
        "learning_rate": 0.00019096762795120983,
        "epoch": 0.7128698114613607,
        "step": 9566
    },
    {
        "loss": 2.5333,
        "grad_norm": 3.335325002670288,
        "learning_rate": 0.00019093838130123864,
        "epoch": 0.7129443326626426,
        "step": 9567
    },
    {
        "loss": 1.6602,
        "grad_norm": 3.9496982097625732,
        "learning_rate": 0.0001909090896246972,
        "epoch": 0.7130188538639243,
        "step": 9568
    },
    {
        "loss": 2.5202,
        "grad_norm": 2.7180118560791016,
        "learning_rate": 0.00019087975293608878,
        "epoch": 0.7130933750652061,
        "step": 9569
    },
    {
        "loss": 2.7585,
        "grad_norm": 1.7804609537124634,
        "learning_rate": 0.00019085037124993886,
        "epoch": 0.7131678962664878,
        "step": 9570
    },
    {
        "loss": 1.7648,
        "grad_norm": 2.869910717010498,
        "learning_rate": 0.00019082094458079544,
        "epoch": 0.7132424174677696,
        "step": 9571
    },
    {
        "loss": 1.8273,
        "grad_norm": 2.884944438934326,
        "learning_rate": 0.00019079147294322847,
        "epoch": 0.7133169386690513,
        "step": 9572
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.995116949081421,
        "learning_rate": 0.00019076195635183045,
        "epoch": 0.7133914598703331,
        "step": 9573
    },
    {
        "loss": 2.7637,
        "grad_norm": 2.577036142349243,
        "learning_rate": 0.000190732394821216,
        "epoch": 0.7134659810716149,
        "step": 9574
    },
    {
        "loss": 2.2716,
        "grad_norm": 3.8653817176818848,
        "learning_rate": 0.00019070278836602183,
        "epoch": 0.7135405022728967,
        "step": 9575
    },
    {
        "loss": 2.4396,
        "grad_norm": 3.481905698776245,
        "learning_rate": 0.00019067313700090735,
        "epoch": 0.7136150234741784,
        "step": 9576
    },
    {
        "loss": 2.6012,
        "grad_norm": 2.728177785873413,
        "learning_rate": 0.00019064344074055362,
        "epoch": 0.7136895446754602,
        "step": 9577
    },
    {
        "loss": 2.6629,
        "grad_norm": 3.449183702468872,
        "learning_rate": 0.00019061369959966445,
        "epoch": 0.7137640658767419,
        "step": 9578
    },
    {
        "loss": 2.8186,
        "grad_norm": 4.094855785369873,
        "learning_rate": 0.0001905839135929656,
        "epoch": 0.7138385870780237,
        "step": 9579
    },
    {
        "loss": 2.9493,
        "grad_norm": 2.3548295497894287,
        "learning_rate": 0.00019055408273520498,
        "epoch": 0.7139131082793054,
        "step": 9580
    },
    {
        "loss": 2.2738,
        "grad_norm": 4.793514728546143,
        "learning_rate": 0.000190524207041153,
        "epoch": 0.7139876294805872,
        "step": 9581
    },
    {
        "loss": 2.4593,
        "grad_norm": 1.855035662651062,
        "learning_rate": 0.00019049428652560192,
        "epoch": 0.714062150681869,
        "step": 9582
    },
    {
        "loss": 3.4679,
        "grad_norm": 4.030966758728027,
        "learning_rate": 0.00019046432120336656,
        "epoch": 0.7141366718831508,
        "step": 9583
    },
    {
        "loss": 2.4396,
        "grad_norm": 4.291478157043457,
        "learning_rate": 0.0001904343110892836,
        "epoch": 0.7142111930844325,
        "step": 9584
    },
    {
        "loss": 2.5874,
        "grad_norm": 2.137554407119751,
        "learning_rate": 0.00019040425619821206,
        "epoch": 0.7142857142857143,
        "step": 9585
    },
    {
        "loss": 2.5228,
        "grad_norm": 2.282510280609131,
        "learning_rate": 0.00019037415654503308,
        "epoch": 0.714360235486996,
        "step": 9586
    },
    {
        "loss": 2.5871,
        "grad_norm": 2.8029227256774902,
        "learning_rate": 0.00019034401214465004,
        "epoch": 0.7144347566882778,
        "step": 9587
    },
    {
        "loss": 2.5225,
        "grad_norm": 3.3099043369293213,
        "learning_rate": 0.0001903138230119884,
        "epoch": 0.7145092778895595,
        "step": 9588
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.2705135345458984,
        "learning_rate": 0.00019028358916199577,
        "epoch": 0.7145837990908414,
        "step": 9589
    },
    {
        "loss": 2.6661,
        "grad_norm": 3.318159341812134,
        "learning_rate": 0.00019025331060964183,
        "epoch": 0.7146583202921231,
        "step": 9590
    },
    {
        "loss": 1.6824,
        "grad_norm": 3.321925640106201,
        "learning_rate": 0.0001902229873699187,
        "epoch": 0.7147328414934049,
        "step": 9591
    },
    {
        "loss": 3.0064,
        "grad_norm": 3.6002936363220215,
        "learning_rate": 0.00019019261945784025,
        "epoch": 0.7148073626946866,
        "step": 9592
    },
    {
        "loss": 2.1871,
        "grad_norm": 2.9906511306762695,
        "learning_rate": 0.0001901622068884426,
        "epoch": 0.7148818838959684,
        "step": 9593
    },
    {
        "loss": 2.6719,
        "grad_norm": 2.8146252632141113,
        "learning_rate": 0.00019013174967678415,
        "epoch": 0.7149564050972501,
        "step": 9594
    },
    {
        "loss": 2.7627,
        "grad_norm": 3.2378323078155518,
        "learning_rate": 0.00019010124783794513,
        "epoch": 0.7150309262985319,
        "step": 9595
    },
    {
        "loss": 2.5944,
        "grad_norm": 3.7561280727386475,
        "learning_rate": 0.00019007070138702813,
        "epoch": 0.7151054474998138,
        "step": 9596
    },
    {
        "loss": 1.863,
        "grad_norm": 3.35585618019104,
        "learning_rate": 0.00019004011033915753,
        "epoch": 0.7151799687010955,
        "step": 9597
    },
    {
        "loss": 2.6753,
        "grad_norm": 2.3213977813720703,
        "learning_rate": 0.00019000947470948013,
        "epoch": 0.7152544899023773,
        "step": 9598
    },
    {
        "loss": 1.9977,
        "grad_norm": Infinity,
        "learning_rate": 0.00019000947470948013,
        "epoch": 0.715329011103659,
        "step": 9599
    },
    {
        "loss": 2.7104,
        "grad_norm": 1.906545639038086,
        "learning_rate": 0.00018997879451316453,
        "epoch": 0.7154035323049408,
        "step": 9600
    },
    {
        "loss": 1.9934,
        "grad_norm": 1.926344394683838,
        "learning_rate": 0.00018994806976540147,
        "epoch": 0.7154780535062225,
        "step": 9601
    },
    {
        "loss": 2.8254,
        "grad_norm": 3.2952754497528076,
        "learning_rate": 0.0001899173004814039,
        "epoch": 0.7155525747075043,
        "step": 9602
    },
    {
        "loss": 1.9945,
        "grad_norm": 3.1148056983947754,
        "learning_rate": 0.00018988648667640662,
        "epoch": 0.715627095908786,
        "step": 9603
    },
    {
        "loss": 2.3775,
        "grad_norm": 2.3484621047973633,
        "learning_rate": 0.00018985562836566652,
        "epoch": 0.7157016171100679,
        "step": 9604
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.1977643966674805,
        "learning_rate": 0.00018982472556446268,
        "epoch": 0.7157761383113496,
        "step": 9605
    },
    {
        "loss": 2.5129,
        "grad_norm": 3.6469221115112305,
        "learning_rate": 0.000189793778288096,
        "epoch": 0.7158506595126314,
        "step": 9606
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.355193853378296,
        "learning_rate": 0.00018976278655188943,
        "epoch": 0.7159251807139131,
        "step": 9607
    },
    {
        "loss": 2.5412,
        "grad_norm": 2.540644884109497,
        "learning_rate": 0.00018973175037118823,
        "epoch": 0.7159997019151949,
        "step": 9608
    },
    {
        "loss": 2.2221,
        "grad_norm": 3.069429874420166,
        "learning_rate": 0.00018970066976135912,
        "epoch": 0.7160742231164766,
        "step": 9609
    },
    {
        "loss": 2.21,
        "grad_norm": 3.1747171878814697,
        "learning_rate": 0.00018966954473779134,
        "epoch": 0.7161487443177584,
        "step": 9610
    },
    {
        "loss": 2.5091,
        "grad_norm": 2.2590041160583496,
        "learning_rate": 0.00018963837531589577,
        "epoch": 0.7162232655190401,
        "step": 9611
    },
    {
        "loss": 2.5065,
        "grad_norm": 2.3610363006591797,
        "learning_rate": 0.00018960716151110554,
        "epoch": 0.716297786720322,
        "step": 9612
    },
    {
        "loss": 2.471,
        "grad_norm": 2.761817455291748,
        "learning_rate": 0.0001895759033388756,
        "epoch": 0.7163723079216037,
        "step": 9613
    },
    {
        "loss": 1.934,
        "grad_norm": 3.079538345336914,
        "learning_rate": 0.0001895446008146828,
        "epoch": 0.7164468291228855,
        "step": 9614
    },
    {
        "loss": 1.9946,
        "grad_norm": 4.458316802978516,
        "learning_rate": 0.00018951325395402617,
        "epoch": 0.7165213503241672,
        "step": 9615
    },
    {
        "loss": 1.6072,
        "grad_norm": 3.459700107574463,
        "learning_rate": 0.00018948186277242644,
        "epoch": 0.716595871525449,
        "step": 9616
    },
    {
        "loss": 2.4099,
        "grad_norm": 2.3899290561676025,
        "learning_rate": 0.00018945042728542663,
        "epoch": 0.7166703927267307,
        "step": 9617
    },
    {
        "loss": 2.7964,
        "grad_norm": 2.7164700031280518,
        "learning_rate": 0.00018941894750859118,
        "epoch": 0.7167449139280125,
        "step": 9618
    },
    {
        "loss": 2.668,
        "grad_norm": 3.6706769466400146,
        "learning_rate": 0.000189387423457507,
        "epoch": 0.7168194351292942,
        "step": 9619
    },
    {
        "loss": 2.5712,
        "grad_norm": 2.95866322517395,
        "learning_rate": 0.00018935585514778255,
        "epoch": 0.7168939563305761,
        "step": 9620
    },
    {
        "loss": 2.048,
        "grad_norm": 2.9802300930023193,
        "learning_rate": 0.0001893242425950484,
        "epoch": 0.7169684775318578,
        "step": 9621
    },
    {
        "loss": 1.779,
        "grad_norm": 3.317366600036621,
        "learning_rate": 0.00018929258581495685,
        "epoch": 0.7170429987331396,
        "step": 9622
    },
    {
        "loss": 2.1094,
        "grad_norm": 2.1845219135284424,
        "learning_rate": 0.00018926088482318235,
        "epoch": 0.7171175199344213,
        "step": 9623
    },
    {
        "loss": 2.3954,
        "grad_norm": 4.038273811340332,
        "learning_rate": 0.00018922913963542102,
        "epoch": 0.7171920411357031,
        "step": 9624
    },
    {
        "loss": 2.2407,
        "grad_norm": 3.4941298961639404,
        "learning_rate": 0.000189197350267391,
        "epoch": 0.7172665623369848,
        "step": 9625
    },
    {
        "loss": 1.7213,
        "grad_norm": 1.8991270065307617,
        "learning_rate": 0.00018916551673483215,
        "epoch": 0.7173410835382666,
        "step": 9626
    },
    {
        "loss": 2.671,
        "grad_norm": 2.8251914978027344,
        "learning_rate": 0.00018913363905350637,
        "epoch": 0.7174156047395484,
        "step": 9627
    },
    {
        "loss": 2.7542,
        "grad_norm": 4.9099955558776855,
        "learning_rate": 0.00018910171723919743,
        "epoch": 0.7174901259408302,
        "step": 9628
    },
    {
        "loss": 2.4067,
        "grad_norm": 3.154791831970215,
        "learning_rate": 0.0001890697513077106,
        "epoch": 0.7175646471421119,
        "step": 9629
    },
    {
        "loss": 2.7677,
        "grad_norm": 2.566249132156372,
        "learning_rate": 0.00018903774127487351,
        "epoch": 0.7176391683433937,
        "step": 9630
    },
    {
        "loss": 2.4485,
        "grad_norm": 2.6300363540649414,
        "learning_rate": 0.0001890056871565353,
        "epoch": 0.7177136895446755,
        "step": 9631
    },
    {
        "loss": 2.706,
        "grad_norm": 2.869417905807495,
        "learning_rate": 0.00018897358896856695,
        "epoch": 0.7177882107459572,
        "step": 9632
    },
    {
        "loss": 1.6128,
        "grad_norm": 1.9309427738189697,
        "learning_rate": 0.0001889414467268614,
        "epoch": 0.717862731947239,
        "step": 9633
    },
    {
        "loss": 2.5574,
        "grad_norm": 3.7047622203826904,
        "learning_rate": 0.00018890926044733326,
        "epoch": 0.7179372531485207,
        "step": 9634
    },
    {
        "loss": 2.7516,
        "grad_norm": 2.9757988452911377,
        "learning_rate": 0.00018887703014591907,
        "epoch": 0.7180117743498026,
        "step": 9635
    },
    {
        "loss": 2.4815,
        "grad_norm": 1.6539390087127686,
        "learning_rate": 0.00018884475583857713,
        "epoch": 0.7180862955510843,
        "step": 9636
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.6759392023086548,
        "learning_rate": 0.00018881243754128747,
        "epoch": 0.7181608167523661,
        "step": 9637
    },
    {
        "loss": 2.4334,
        "grad_norm": 2.4590141773223877,
        "learning_rate": 0.00018878007527005185,
        "epoch": 0.7182353379536478,
        "step": 9638
    },
    {
        "loss": 2.6177,
        "grad_norm": 2.17928409576416,
        "learning_rate": 0.00018874766904089404,
        "epoch": 0.7183098591549296,
        "step": 9639
    },
    {
        "loss": 1.8869,
        "grad_norm": 3.5968103408813477,
        "learning_rate": 0.00018871521886985935,
        "epoch": 0.7183843803562113,
        "step": 9640
    },
    {
        "loss": 2.1139,
        "grad_norm": 3.021493911743164,
        "learning_rate": 0.00018868272477301495,
        "epoch": 0.7184589015574931,
        "step": 9641
    },
    {
        "loss": 2.4497,
        "grad_norm": 2.8157591819763184,
        "learning_rate": 0.00018865018676644964,
        "epoch": 0.7185334227587749,
        "step": 9642
    },
    {
        "loss": 3.0987,
        "grad_norm": 2.898996353149414,
        "learning_rate": 0.0001886176048662742,
        "epoch": 0.7186079439600567,
        "step": 9643
    },
    {
        "loss": 3.1224,
        "grad_norm": 2.1544644832611084,
        "learning_rate": 0.00018858497908862086,
        "epoch": 0.7186824651613384,
        "step": 9644
    },
    {
        "loss": 1.854,
        "grad_norm": 2.4092633724212646,
        "learning_rate": 0.00018855230944964379,
        "epoch": 0.7187569863626202,
        "step": 9645
    },
    {
        "loss": 2.0735,
        "grad_norm": 3.5868327617645264,
        "learning_rate": 0.0001885195959655188,
        "epoch": 0.7188315075639019,
        "step": 9646
    },
    {
        "loss": 2.3705,
        "grad_norm": 2.594878673553467,
        "learning_rate": 0.00018848683865244333,
        "epoch": 0.7189060287651837,
        "step": 9647
    },
    {
        "loss": 2.3876,
        "grad_norm": 3.1329991817474365,
        "learning_rate": 0.00018845403752663678,
        "epoch": 0.7189805499664654,
        "step": 9648
    },
    {
        "loss": 2.5858,
        "grad_norm": 2.6072683334350586,
        "learning_rate": 0.00018842119260433982,
        "epoch": 0.7190550711677473,
        "step": 9649
    },
    {
        "loss": 1.7144,
        "grad_norm": 4.81890344619751,
        "learning_rate": 0.0001883883039018152,
        "epoch": 0.719129592369029,
        "step": 9650
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.484450578689575,
        "learning_rate": 0.00018835537143534716,
        "epoch": 0.7192041135703108,
        "step": 9651
    },
    {
        "loss": 2.5724,
        "grad_norm": 3.609633684158325,
        "learning_rate": 0.00018832239522124157,
        "epoch": 0.7192786347715925,
        "step": 9652
    },
    {
        "loss": 2.8324,
        "grad_norm": 2.4207990169525146,
        "learning_rate": 0.00018828937527582616,
        "epoch": 0.7193531559728743,
        "step": 9653
    },
    {
        "loss": 2.286,
        "grad_norm": 2.6071696281433105,
        "learning_rate": 0.00018825631161545014,
        "epoch": 0.719427677174156,
        "step": 9654
    },
    {
        "loss": 2.2164,
        "grad_norm": 2.3703410625457764,
        "learning_rate": 0.00018822320425648434,
        "epoch": 0.7195021983754378,
        "step": 9655
    },
    {
        "loss": 1.897,
        "grad_norm": 2.1141374111175537,
        "learning_rate": 0.00018819005321532142,
        "epoch": 0.7195767195767195,
        "step": 9656
    },
    {
        "loss": 2.8067,
        "grad_norm": 2.928367853164673,
        "learning_rate": 0.00018815685850837545,
        "epoch": 0.7196512407780014,
        "step": 9657
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.1892738342285156,
        "learning_rate": 0.00018812362015208222,
        "epoch": 0.7197257619792831,
        "step": 9658
    },
    {
        "loss": 2.5955,
        "grad_norm": 1.887729525566101,
        "learning_rate": 0.0001880903381628993,
        "epoch": 0.7198002831805649,
        "step": 9659
    },
    {
        "loss": 2.3653,
        "grad_norm": 2.909233808517456,
        "learning_rate": 0.00018805701255730537,
        "epoch": 0.7198748043818466,
        "step": 9660
    },
    {
        "loss": 1.7293,
        "grad_norm": 2.3519937992095947,
        "learning_rate": 0.0001880236433518013,
        "epoch": 0.7199493255831284,
        "step": 9661
    },
    {
        "loss": 2.315,
        "grad_norm": 2.5033140182495117,
        "learning_rate": 0.0001879902305629091,
        "epoch": 0.7200238467844101,
        "step": 9662
    },
    {
        "loss": 2.8669,
        "grad_norm": 3.2337427139282227,
        "learning_rate": 0.0001879567742071727,
        "epoch": 0.7200983679856919,
        "step": 9663
    },
    {
        "loss": 2.623,
        "grad_norm": 2.708064317703247,
        "learning_rate": 0.00018792327430115736,
        "epoch": 0.7201728891869736,
        "step": 9664
    },
    {
        "loss": 2.2413,
        "grad_norm": 2.904411554336548,
        "learning_rate": 0.0001878897308614499,
        "epoch": 0.7202474103882555,
        "step": 9665
    },
    {
        "loss": 2.6355,
        "grad_norm": 3.078108787536621,
        "learning_rate": 0.00018785614390465894,
        "epoch": 0.7203219315895373,
        "step": 9666
    },
    {
        "loss": 2.121,
        "grad_norm": 2.9318370819091797,
        "learning_rate": 0.0001878225134474143,
        "epoch": 0.720396452790819,
        "step": 9667
    },
    {
        "loss": 2.5005,
        "grad_norm": 1.9621453285217285,
        "learning_rate": 0.00018778883950636775,
        "epoch": 0.7204709739921008,
        "step": 9668
    },
    {
        "loss": 2.3949,
        "grad_norm": 2.2260921001434326,
        "learning_rate": 0.0001877551220981921,
        "epoch": 0.7205454951933825,
        "step": 9669
    },
    {
        "loss": 2.7193,
        "grad_norm": 2.154273271560669,
        "learning_rate": 0.00018772136123958215,
        "epoch": 0.7206200163946643,
        "step": 9670
    },
    {
        "loss": 2.1093,
        "grad_norm": 4.582484722137451,
        "learning_rate": 0.0001876875569472539,
        "epoch": 0.720694537595946,
        "step": 9671
    },
    {
        "loss": 3.0409,
        "grad_norm": 3.3734772205352783,
        "learning_rate": 0.00018765370923794493,
        "epoch": 0.7207690587972279,
        "step": 9672
    },
    {
        "loss": 2.4584,
        "grad_norm": 3.1195263862609863,
        "learning_rate": 0.0001876198181284145,
        "epoch": 0.7208435799985096,
        "step": 9673
    },
    {
        "loss": 2.1656,
        "grad_norm": 2.8934273719787598,
        "learning_rate": 0.00018758588363544307,
        "epoch": 0.7209181011997914,
        "step": 9674
    },
    {
        "loss": 1.9065,
        "grad_norm": 3.541083574295044,
        "learning_rate": 0.00018755190577583272,
        "epoch": 0.7209926224010731,
        "step": 9675
    },
    {
        "loss": 1.9239,
        "grad_norm": 3.8430655002593994,
        "learning_rate": 0.0001875178845664071,
        "epoch": 0.7210671436023549,
        "step": 9676
    },
    {
        "loss": 2.4136,
        "grad_norm": 2.798804759979248,
        "learning_rate": 0.00018748382002401119,
        "epoch": 0.7211416648036366,
        "step": 9677
    },
    {
        "loss": 2.6221,
        "grad_norm": 3.2856991291046143,
        "learning_rate": 0.00018744971216551134,
        "epoch": 0.7212161860049184,
        "step": 9678
    },
    {
        "loss": 1.6516,
        "grad_norm": 4.284435272216797,
        "learning_rate": 0.00018741556100779574,
        "epoch": 0.7212907072062001,
        "step": 9679
    },
    {
        "loss": 2.6874,
        "grad_norm": 2.405219078063965,
        "learning_rate": 0.0001873813665677734,
        "epoch": 0.721365228407482,
        "step": 9680
    },
    {
        "loss": 2.5388,
        "grad_norm": 3.541616439819336,
        "learning_rate": 0.00018734712886237537,
        "epoch": 0.7214397496087637,
        "step": 9681
    },
    {
        "loss": 2.2818,
        "grad_norm": 3.201173782348633,
        "learning_rate": 0.0001873128479085537,
        "epoch": 0.7215142708100455,
        "step": 9682
    },
    {
        "loss": 2.8239,
        "grad_norm": 2.577606439590454,
        "learning_rate": 0.00018727852372328214,
        "epoch": 0.7215887920113272,
        "step": 9683
    },
    {
        "loss": 2.4036,
        "grad_norm": 5.643246650695801,
        "learning_rate": 0.00018724415632355567,
        "epoch": 0.721663313212609,
        "step": 9684
    },
    {
        "loss": 2.6495,
        "grad_norm": 2.755014419555664,
        "learning_rate": 0.00018720974572639066,
        "epoch": 0.7217378344138907,
        "step": 9685
    },
    {
        "loss": 2.5822,
        "grad_norm": 4.889953136444092,
        "learning_rate": 0.000187175291948825,
        "epoch": 0.7218123556151725,
        "step": 9686
    },
    {
        "loss": 2.3675,
        "grad_norm": 2.3416874408721924,
        "learning_rate": 0.00018714079500791787,
        "epoch": 0.7218868768164542,
        "step": 9687
    },
    {
        "loss": 2.5508,
        "grad_norm": 2.909860372543335,
        "learning_rate": 0.0001871062549207498,
        "epoch": 0.7219613980177361,
        "step": 9688
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.1128382682800293,
        "learning_rate": 0.00018707167170442266,
        "epoch": 0.7220359192190178,
        "step": 9689
    },
    {
        "loss": 2.5249,
        "grad_norm": 4.6185994148254395,
        "learning_rate": 0.00018703704537605988,
        "epoch": 0.7221104404202996,
        "step": 9690
    },
    {
        "loss": 2.4939,
        "grad_norm": 2.391630172729492,
        "learning_rate": 0.00018700237595280602,
        "epoch": 0.7221849616215813,
        "step": 9691
    },
    {
        "loss": 2.3079,
        "grad_norm": 2.2690324783325195,
        "learning_rate": 0.00018696766345182705,
        "epoch": 0.7222594828228631,
        "step": 9692
    },
    {
        "loss": 2.8322,
        "grad_norm": 3.0270543098449707,
        "learning_rate": 0.0001869329078903102,
        "epoch": 0.7223340040241448,
        "step": 9693
    },
    {
        "loss": 2.6209,
        "grad_norm": 3.095369815826416,
        "learning_rate": 0.0001868981092854642,
        "epoch": 0.7224085252254266,
        "step": 9694
    },
    {
        "loss": 2.4012,
        "grad_norm": 1.6869746446609497,
        "learning_rate": 0.00018686326765451886,
        "epoch": 0.7224830464267084,
        "step": 9695
    },
    {
        "loss": 1.6389,
        "grad_norm": 2.430063486099243,
        "learning_rate": 0.00018682838301472553,
        "epoch": 0.7225575676279902,
        "step": 9696
    },
    {
        "loss": 2.3988,
        "grad_norm": 2.982841968536377,
        "learning_rate": 0.00018679345538335671,
        "epoch": 0.7226320888292719,
        "step": 9697
    },
    {
        "loss": 1.7997,
        "grad_norm": 3.1619439125061035,
        "learning_rate": 0.0001867584847777061,
        "epoch": 0.7227066100305537,
        "step": 9698
    },
    {
        "loss": 2.3644,
        "grad_norm": 2.112670660018921,
        "learning_rate": 0.00018672347121508902,
        "epoch": 0.7227811312318354,
        "step": 9699
    },
    {
        "loss": 2.6009,
        "grad_norm": 2.65602445602417,
        "learning_rate": 0.00018668841471284155,
        "epoch": 0.7228556524331172,
        "step": 9700
    },
    {
        "loss": 2.7145,
        "grad_norm": 2.714467763900757,
        "learning_rate": 0.00018665331528832155,
        "epoch": 0.7229301736343989,
        "step": 9701
    },
    {
        "loss": 1.7964,
        "grad_norm": 3.785435676574707,
        "learning_rate": 0.00018661817295890777,
        "epoch": 0.7230046948356808,
        "step": 9702
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.3679637908935547,
        "learning_rate": 0.00018658298774200028,
        "epoch": 0.7230792160369626,
        "step": 9703
    },
    {
        "loss": 2.5153,
        "grad_norm": 5.4607720375061035,
        "learning_rate": 0.0001865477596550206,
        "epoch": 0.7231537372382443,
        "step": 9704
    },
    {
        "loss": 1.9773,
        "grad_norm": 3.5850634574890137,
        "learning_rate": 0.00018651248871541114,
        "epoch": 0.7232282584395261,
        "step": 9705
    },
    {
        "loss": 2.812,
        "grad_norm": 4.674964904785156,
        "learning_rate": 0.00018647717494063584,
        "epoch": 0.7233027796408078,
        "step": 9706
    },
    {
        "loss": 2.6881,
        "grad_norm": 2.6657001972198486,
        "learning_rate": 0.00018644181834817964,
        "epoch": 0.7233773008420896,
        "step": 9707
    },
    {
        "loss": 2.6985,
        "grad_norm": 2.3284049034118652,
        "learning_rate": 0.00018640641895554874,
        "epoch": 0.7234518220433713,
        "step": 9708
    },
    {
        "loss": 2.3169,
        "grad_norm": 1.4516087770462036,
        "learning_rate": 0.00018637097678027048,
        "epoch": 0.7235263432446531,
        "step": 9709
    },
    {
        "loss": 2.566,
        "grad_norm": 3.8008980751037598,
        "learning_rate": 0.0001863354918398936,
        "epoch": 0.7236008644459349,
        "step": 9710
    },
    {
        "loss": 1.1742,
        "grad_norm": 1.721030592918396,
        "learning_rate": 0.00018629996415198775,
        "epoch": 0.7236753856472167,
        "step": 9711
    },
    {
        "loss": 2.2625,
        "grad_norm": 2.174642562866211,
        "learning_rate": 0.00018626439373414387,
        "epoch": 0.7237499068484984,
        "step": 9712
    },
    {
        "loss": 2.1852,
        "grad_norm": 3.9501655101776123,
        "learning_rate": 0.00018622878060397396,
        "epoch": 0.7238244280497802,
        "step": 9713
    },
    {
        "loss": 2.004,
        "grad_norm": 1.9547410011291504,
        "learning_rate": 0.00018619312477911138,
        "epoch": 0.7238989492510619,
        "step": 9714
    },
    {
        "loss": 2.2063,
        "grad_norm": 2.685089588165283,
        "learning_rate": 0.00018615742627721044,
        "epoch": 0.7239734704523437,
        "step": 9715
    },
    {
        "loss": 1.2657,
        "grad_norm": 5.374072551727295,
        "learning_rate": 0.0001861216851159466,
        "epoch": 0.7240479916536254,
        "step": 9716
    },
    {
        "loss": 2.1644,
        "grad_norm": 2.288301706314087,
        "learning_rate": 0.00018608590131301652,
        "epoch": 0.7241225128549073,
        "step": 9717
    },
    {
        "loss": 2.5313,
        "grad_norm": 3.074519634246826,
        "learning_rate": 0.0001860500748861379,
        "epoch": 0.724197034056189,
        "step": 9718
    },
    {
        "loss": 2.9561,
        "grad_norm": 3.512554883956909,
        "learning_rate": 0.00018601420585304972,
        "epoch": 0.7242715552574708,
        "step": 9719
    },
    {
        "loss": 2.5269,
        "grad_norm": 2.613271713256836,
        "learning_rate": 0.0001859782942315117,
        "epoch": 0.7243460764587525,
        "step": 9720
    },
    {
        "loss": 2.7466,
        "grad_norm": 3.315422534942627,
        "learning_rate": 0.00018594234003930498,
        "epoch": 0.7244205976600343,
        "step": 9721
    },
    {
        "loss": 1.9875,
        "grad_norm": 3.351623058319092,
        "learning_rate": 0.00018590634329423164,
        "epoch": 0.724495118861316,
        "step": 9722
    },
    {
        "loss": 2.4268,
        "grad_norm": 2.518160343170166,
        "learning_rate": 0.00018587030401411475,
        "epoch": 0.7245696400625978,
        "step": 9723
    },
    {
        "loss": 2.3651,
        "grad_norm": 2.907470226287842,
        "learning_rate": 0.00018583422221679876,
        "epoch": 0.7246441612638795,
        "step": 9724
    },
    {
        "loss": 2.1694,
        "grad_norm": 3.264502763748169,
        "learning_rate": 0.00018579809792014878,
        "epoch": 0.7247186824651614,
        "step": 9725
    },
    {
        "loss": 2.4826,
        "grad_norm": 3.414635419845581,
        "learning_rate": 0.0001857619311420511,
        "epoch": 0.7247932036664431,
        "step": 9726
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.3272759914398193,
        "learning_rate": 0.0001857257219004132,
        "epoch": 0.7248677248677249,
        "step": 9727
    },
    {
        "loss": 2.8147,
        "grad_norm": 3.2594640254974365,
        "learning_rate": 0.00018568947021316344,
        "epoch": 0.7249422460690066,
        "step": 9728
    },
    {
        "loss": 2.346,
        "grad_norm": 3.716562032699585,
        "learning_rate": 0.00018565317609825109,
        "epoch": 0.7250167672702884,
        "step": 9729
    },
    {
        "loss": 1.7948,
        "grad_norm": 3.208507537841797,
        "learning_rate": 0.0001856168395736468,
        "epoch": 0.7250912884715701,
        "step": 9730
    },
    {
        "loss": 1.5715,
        "grad_norm": 3.323404312133789,
        "learning_rate": 0.0001855804606573417,
        "epoch": 0.7251658096728519,
        "step": 9731
    },
    {
        "loss": 2.5082,
        "grad_norm": 2.774505138397217,
        "learning_rate": 0.00018554403936734835,
        "epoch": 0.7252403308741336,
        "step": 9732
    },
    {
        "loss": 2.6621,
        "grad_norm": 2.1902379989624023,
        "learning_rate": 0.00018550757572169998,
        "epoch": 0.7253148520754155,
        "step": 9733
    },
    {
        "loss": 2.5182,
        "grad_norm": 2.358436346054077,
        "learning_rate": 0.00018547106973845114,
        "epoch": 0.7253893732766972,
        "step": 9734
    },
    {
        "loss": 2.5684,
        "grad_norm": 2.628279209136963,
        "learning_rate": 0.00018543452143567703,
        "epoch": 0.725463894477979,
        "step": 9735
    },
    {
        "loss": 1.8392,
        "grad_norm": 3.8771371841430664,
        "learning_rate": 0.00018539793083147384,
        "epoch": 0.7255384156792607,
        "step": 9736
    },
    {
        "loss": 1.618,
        "grad_norm": 4.751115798950195,
        "learning_rate": 0.0001853612979439589,
        "epoch": 0.7256129368805425,
        "step": 9737
    },
    {
        "loss": 2.5733,
        "grad_norm": 3.628767967224121,
        "learning_rate": 0.00018532462279127025,
        "epoch": 0.7256874580818243,
        "step": 9738
    },
    {
        "loss": 2.8107,
        "grad_norm": 2.354621171951294,
        "learning_rate": 0.00018528790539156716,
        "epoch": 0.725761979283106,
        "step": 9739
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.737097978591919,
        "learning_rate": 0.0001852511457630293,
        "epoch": 0.7258365004843879,
        "step": 9740
    },
    {
        "loss": 1.9216,
        "grad_norm": 2.606046199798584,
        "learning_rate": 0.00018521434392385782,
        "epoch": 0.7259110216856696,
        "step": 9741
    },
    {
        "loss": 1.8661,
        "grad_norm": 2.9794769287109375,
        "learning_rate": 0.0001851774998922744,
        "epoch": 0.7259855428869514,
        "step": 9742
    },
    {
        "loss": 1.748,
        "grad_norm": 2.688749313354492,
        "learning_rate": 0.00018514061368652172,
        "epoch": 0.7260600640882331,
        "step": 9743
    },
    {
        "loss": 2.5987,
        "grad_norm": 2.931504011154175,
        "learning_rate": 0.00018510368532486342,
        "epoch": 0.7261345852895149,
        "step": 9744
    },
    {
        "loss": 2.3827,
        "grad_norm": 5.188967227935791,
        "learning_rate": 0.00018506671482558393,
        "epoch": 0.7262091064907966,
        "step": 9745
    },
    {
        "loss": 2.148,
        "grad_norm": 3.2784323692321777,
        "learning_rate": 0.00018502970220698846,
        "epoch": 0.7262836276920784,
        "step": 9746
    },
    {
        "loss": 2.0549,
        "grad_norm": 3.5036983489990234,
        "learning_rate": 0.00018499264748740327,
        "epoch": 0.7263581488933601,
        "step": 9747
    },
    {
        "loss": 2.519,
        "grad_norm": 5.465689182281494,
        "learning_rate": 0.00018495555068517535,
        "epoch": 0.726432670094642,
        "step": 9748
    },
    {
        "loss": 2.78,
        "grad_norm": 2.848158836364746,
        "learning_rate": 0.00018491841181867246,
        "epoch": 0.7265071912959237,
        "step": 9749
    },
    {
        "loss": 2.4379,
        "grad_norm": 2.6574573516845703,
        "learning_rate": 0.00018488123090628348,
        "epoch": 0.7265817124972055,
        "step": 9750
    },
    {
        "loss": 2.2367,
        "grad_norm": 2.7477657794952393,
        "learning_rate": 0.0001848440079664176,
        "epoch": 0.7266562336984872,
        "step": 9751
    },
    {
        "loss": 2.7476,
        "grad_norm": 2.001187801361084,
        "learning_rate": 0.00018480674301750533,
        "epoch": 0.726730754899769,
        "step": 9752
    },
    {
        "loss": 2.3452,
        "grad_norm": 2.53930401802063,
        "learning_rate": 0.00018476943607799773,
        "epoch": 0.7268052761010507,
        "step": 9753
    },
    {
        "loss": 2.59,
        "grad_norm": 3.6714913845062256,
        "learning_rate": 0.00018473208716636655,
        "epoch": 0.7268797973023325,
        "step": 9754
    },
    {
        "loss": 1.3254,
        "grad_norm": 3.8535168170928955,
        "learning_rate": 0.00018469469630110464,
        "epoch": 0.7269543185036142,
        "step": 9755
    },
    {
        "loss": 1.9066,
        "grad_norm": 3.1860926151275635,
        "learning_rate": 0.00018465726350072535,
        "epoch": 0.7270288397048961,
        "step": 9756
    },
    {
        "loss": 2.4559,
        "grad_norm": 2.1545469760894775,
        "learning_rate": 0.00018461978878376293,
        "epoch": 0.7271033609061778,
        "step": 9757
    },
    {
        "loss": 2.1194,
        "grad_norm": 3.787362813949585,
        "learning_rate": 0.00018458227216877236,
        "epoch": 0.7271778821074596,
        "step": 9758
    },
    {
        "loss": 2.5809,
        "grad_norm": 1.8720641136169434,
        "learning_rate": 0.00018454471367432924,
        "epoch": 0.7272524033087413,
        "step": 9759
    },
    {
        "loss": 2.8898,
        "grad_norm": 2.973729133605957,
        "learning_rate": 0.00018450711331903004,
        "epoch": 0.7273269245100231,
        "step": 9760
    },
    {
        "loss": 2.2214,
        "grad_norm": 3.0293943881988525,
        "learning_rate": 0.000184469471121492,
        "epoch": 0.7274014457113048,
        "step": 9761
    },
    {
        "loss": 2.4628,
        "grad_norm": 3.333665132522583,
        "learning_rate": 0.000184431787100353,
        "epoch": 0.7274759669125866,
        "step": 9762
    },
    {
        "loss": 2.9184,
        "grad_norm": 1.9335405826568604,
        "learning_rate": 0.00018439406127427157,
        "epoch": 0.7275504881138684,
        "step": 9763
    },
    {
        "loss": 1.8291,
        "grad_norm": 1.3287529945373535,
        "learning_rate": 0.00018435629366192696,
        "epoch": 0.7276250093151502,
        "step": 9764
    },
    {
        "loss": 2.2071,
        "grad_norm": 2.9470736980438232,
        "learning_rate": 0.00018431848428201926,
        "epoch": 0.7276995305164319,
        "step": 9765
    },
    {
        "loss": 2.4506,
        "grad_norm": 2.730036973953247,
        "learning_rate": 0.00018428063315326906,
        "epoch": 0.7277740517177137,
        "step": 9766
    },
    {
        "loss": 2.4461,
        "grad_norm": 2.8224236965179443,
        "learning_rate": 0.0001842427402944178,
        "epoch": 0.7278485729189954,
        "step": 9767
    },
    {
        "loss": 2.8167,
        "grad_norm": 4.275567054748535,
        "learning_rate": 0.00018420480572422737,
        "epoch": 0.7279230941202772,
        "step": 9768
    },
    {
        "loss": 2.4495,
        "grad_norm": 3.745464563369751,
        "learning_rate": 0.0001841668294614804,
        "epoch": 0.7279976153215589,
        "step": 9769
    },
    {
        "loss": 2.5845,
        "grad_norm": 1.978553295135498,
        "learning_rate": 0.00018412881152498034,
        "epoch": 0.7280721365228408,
        "step": 9770
    },
    {
        "loss": 2.1909,
        "grad_norm": 3.2309136390686035,
        "learning_rate": 0.00018409075193355086,
        "epoch": 0.7281466577241225,
        "step": 9771
    },
    {
        "loss": 2.8364,
        "grad_norm": 5.407029628753662,
        "learning_rate": 0.00018405265070603675,
        "epoch": 0.7282211789254043,
        "step": 9772
    },
    {
        "loss": 2.0256,
        "grad_norm": 2.752563714981079,
        "learning_rate": 0.00018401450786130306,
        "epoch": 0.7282957001266861,
        "step": 9773
    },
    {
        "loss": 1.8184,
        "grad_norm": 2.9000885486602783,
        "learning_rate": 0.0001839763234182355,
        "epoch": 0.7283702213279678,
        "step": 9774
    },
    {
        "loss": 1.9765,
        "grad_norm": 3.391880512237549,
        "learning_rate": 0.00018393809739574062,
        "epoch": 0.7284447425292496,
        "step": 9775
    },
    {
        "loss": 2.6842,
        "grad_norm": 2.0191540718078613,
        "learning_rate": 0.0001838998298127452,
        "epoch": 0.7285192637305313,
        "step": 9776
    },
    {
        "loss": 2.8659,
        "grad_norm": 2.6664528846740723,
        "learning_rate": 0.00018386152068819688,
        "epoch": 0.7285937849318131,
        "step": 9777
    },
    {
        "loss": 2.6628,
        "grad_norm": 3.854612112045288,
        "learning_rate": 0.00018382317004106375,
        "epoch": 0.7286683061330949,
        "step": 9778
    },
    {
        "loss": 2.4878,
        "grad_norm": 3.6351656913757324,
        "learning_rate": 0.00018378477789033444,
        "epoch": 0.7287428273343767,
        "step": 9779
    },
    {
        "loss": 2.6338,
        "grad_norm": 2.0927822589874268,
        "learning_rate": 0.0001837463442550181,
        "epoch": 0.7288173485356584,
        "step": 9780
    },
    {
        "loss": 1.843,
        "grad_norm": 3.886345148086548,
        "learning_rate": 0.0001837078691541447,
        "epoch": 0.7288918697369402,
        "step": 9781
    },
    {
        "loss": 1.9602,
        "grad_norm": 3.1471290588378906,
        "learning_rate": 0.00018366935260676423,
        "epoch": 0.7289663909382219,
        "step": 9782
    },
    {
        "loss": 2.3961,
        "grad_norm": 2.589860677719116,
        "learning_rate": 0.00018363079463194773,
        "epoch": 0.7290409121395037,
        "step": 9783
    },
    {
        "loss": 2.428,
        "grad_norm": 2.9980757236480713,
        "learning_rate": 0.0001835921952487863,
        "epoch": 0.7291154333407854,
        "step": 9784
    },
    {
        "loss": 2.7331,
        "grad_norm": 3.795976161956787,
        "learning_rate": 0.000183553554476392,
        "epoch": 0.7291899545420673,
        "step": 9785
    },
    {
        "loss": 1.8249,
        "grad_norm": 5.0675554275512695,
        "learning_rate": 0.000183514872333897,
        "epoch": 0.729264475743349,
        "step": 9786
    },
    {
        "loss": 2.4334,
        "grad_norm": 1.852343201637268,
        "learning_rate": 0.00018347614884045403,
        "epoch": 0.7293389969446308,
        "step": 9787
    },
    {
        "loss": 2.1342,
        "grad_norm": 3.2824807167053223,
        "learning_rate": 0.00018343738401523655,
        "epoch": 0.7294135181459125,
        "step": 9788
    },
    {
        "loss": 2.0377,
        "grad_norm": 5.960583686828613,
        "learning_rate": 0.00018339857787743813,
        "epoch": 0.7294880393471943,
        "step": 9789
    },
    {
        "loss": 1.8127,
        "grad_norm": 5.230688095092773,
        "learning_rate": 0.00018335973044627315,
        "epoch": 0.729562560548476,
        "step": 9790
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.9412195682525635,
        "learning_rate": 0.00018332084174097597,
        "epoch": 0.7296370817497578,
        "step": 9791
    },
    {
        "loss": 2.406,
        "grad_norm": 3.162783622741699,
        "learning_rate": 0.00018328191178080188,
        "epoch": 0.7297116029510395,
        "step": 9792
    },
    {
        "loss": 2.8341,
        "grad_norm": 4.6658196449279785,
        "learning_rate": 0.0001832429405850263,
        "epoch": 0.7297861241523214,
        "step": 9793
    },
    {
        "loss": 1.85,
        "grad_norm": 3.06596040725708,
        "learning_rate": 0.0001832039281729451,
        "epoch": 0.7298606453536031,
        "step": 9794
    },
    {
        "loss": 2.8575,
        "grad_norm": 5.332700729370117,
        "learning_rate": 0.0001831648745638747,
        "epoch": 0.7299351665548849,
        "step": 9795
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.4123034477233887,
        "learning_rate": 0.0001831257797771518,
        "epoch": 0.7300096877561666,
        "step": 9796
    },
    {
        "loss": 2.4263,
        "grad_norm": 2.473630428314209,
        "learning_rate": 0.0001830866438321334,
        "epoch": 0.7300842089574484,
        "step": 9797
    },
    {
        "loss": 2.5675,
        "grad_norm": 2.9537298679351807,
        "learning_rate": 0.00018304746674819712,
        "epoch": 0.7301587301587301,
        "step": 9798
    },
    {
        "loss": 2.4717,
        "grad_norm": 3.3212473392486572,
        "learning_rate": 0.00018300824854474083,
        "epoch": 0.7302332513600119,
        "step": 9799
    },
    {
        "loss": 2.2924,
        "grad_norm": 3.331644296646118,
        "learning_rate": 0.00018296898924118255,
        "epoch": 0.7303077725612936,
        "step": 9800
    },
    {
        "loss": 2.6407,
        "grad_norm": 2.8259434700012207,
        "learning_rate": 0.00018292968885696116,
        "epoch": 0.7303822937625755,
        "step": 9801
    },
    {
        "loss": 2.644,
        "grad_norm": 3.0690243244171143,
        "learning_rate": 0.00018289034741153523,
        "epoch": 0.7304568149638572,
        "step": 9802
    },
    {
        "loss": 2.7908,
        "grad_norm": 2.093980073928833,
        "learning_rate": 0.00018285096492438424,
        "epoch": 0.730531336165139,
        "step": 9803
    },
    {
        "loss": 2.3369,
        "grad_norm": 2.5268452167510986,
        "learning_rate": 0.00018281154141500762,
        "epoch": 0.7306058573664207,
        "step": 9804
    },
    {
        "loss": 2.1162,
        "grad_norm": 3.9119811058044434,
        "learning_rate": 0.00018277207690292532,
        "epoch": 0.7306803785677025,
        "step": 9805
    },
    {
        "loss": 1.5666,
        "grad_norm": 4.038921356201172,
        "learning_rate": 0.00018273257140767748,
        "epoch": 0.7307548997689842,
        "step": 9806
    },
    {
        "loss": 2.6381,
        "grad_norm": 1.8242720365524292,
        "learning_rate": 0.00018269302494882447,
        "epoch": 0.730829420970266,
        "step": 9807
    },
    {
        "loss": 2.5192,
        "grad_norm": 3.060537815093994,
        "learning_rate": 0.00018265343754594727,
        "epoch": 0.7309039421715479,
        "step": 9808
    },
    {
        "loss": 1.7636,
        "grad_norm": 2.875520706176758,
        "learning_rate": 0.00018261380921864672,
        "epoch": 0.7309784633728296,
        "step": 9809
    },
    {
        "loss": 2.8806,
        "grad_norm": 3.616426944732666,
        "learning_rate": 0.0001825741399865441,
        "epoch": 0.7310529845741114,
        "step": 9810
    },
    {
        "loss": 2.1456,
        "grad_norm": 3.314729690551758,
        "learning_rate": 0.00018253442986928094,
        "epoch": 0.7311275057753931,
        "step": 9811
    },
    {
        "loss": 2.6009,
        "grad_norm": 3.506911039352417,
        "learning_rate": 0.0001824946788865192,
        "epoch": 0.7312020269766749,
        "step": 9812
    },
    {
        "loss": 2.2952,
        "grad_norm": 2.253176689147949,
        "learning_rate": 0.0001824548870579407,
        "epoch": 0.7312765481779566,
        "step": 9813
    },
    {
        "loss": 2.1274,
        "grad_norm": 2.9400172233581543,
        "learning_rate": 0.00018241505440324773,
        "epoch": 0.7313510693792384,
        "step": 9814
    },
    {
        "loss": 1.8978,
        "grad_norm": 4.1531219482421875,
        "learning_rate": 0.0001823751809421627,
        "epoch": 0.7314255905805201,
        "step": 9815
    },
    {
        "loss": 1.6947,
        "grad_norm": 3.391408920288086,
        "learning_rate": 0.00018233526669442836,
        "epoch": 0.731500111781802,
        "step": 9816
    },
    {
        "loss": 1.7066,
        "grad_norm": 5.258185863494873,
        "learning_rate": 0.0001822953116798075,
        "epoch": 0.7315746329830837,
        "step": 9817
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.6252520084381104,
        "learning_rate": 0.0001822553159180832,
        "epoch": 0.7316491541843655,
        "step": 9818
    },
    {
        "loss": 1.6309,
        "grad_norm": 3.556805372238159,
        "learning_rate": 0.00018221527942905864,
        "epoch": 0.7317236753856472,
        "step": 9819
    },
    {
        "loss": 2.5767,
        "grad_norm": 2.9399633407592773,
        "learning_rate": 0.00018217520223255714,
        "epoch": 0.731798196586929,
        "step": 9820
    },
    {
        "loss": 2.3675,
        "grad_norm": 2.6094653606414795,
        "learning_rate": 0.00018213508434842247,
        "epoch": 0.7318727177882107,
        "step": 9821
    },
    {
        "loss": 2.7843,
        "grad_norm": 3.132139205932617,
        "learning_rate": 0.00018209492579651792,
        "epoch": 0.7319472389894925,
        "step": 9822
    },
    {
        "loss": 2.5659,
        "grad_norm": 2.6800525188446045,
        "learning_rate": 0.00018205472659672762,
        "epoch": 0.7320217601907743,
        "step": 9823
    },
    {
        "loss": 2.753,
        "grad_norm": 2.626882553100586,
        "learning_rate": 0.0001820144867689554,
        "epoch": 0.7320962813920561,
        "step": 9824
    },
    {
        "loss": 2.0344,
        "grad_norm": 3.3903825283050537,
        "learning_rate": 0.00018197420633312526,
        "epoch": 0.7321708025933378,
        "step": 9825
    },
    {
        "loss": 2.0157,
        "grad_norm": 2.1058874130249023,
        "learning_rate": 0.00018193388530918147,
        "epoch": 0.7322453237946196,
        "step": 9826
    },
    {
        "loss": 2.1656,
        "grad_norm": 2.161721706390381,
        "learning_rate": 0.00018189352371708817,
        "epoch": 0.7323198449959013,
        "step": 9827
    },
    {
        "loss": 2.151,
        "grad_norm": 3.6940858364105225,
        "learning_rate": 0.00018185312157682984,
        "epoch": 0.7323943661971831,
        "step": 9828
    },
    {
        "loss": 3.0964,
        "grad_norm": 3.0924267768859863,
        "learning_rate": 0.00018181267890841088,
        "epoch": 0.7324688873984648,
        "step": 9829
    },
    {
        "loss": 3.0317,
        "grad_norm": 2.077401638031006,
        "learning_rate": 0.0001817721957318557,
        "epoch": 0.7325434085997466,
        "step": 9830
    },
    {
        "loss": 2.0253,
        "grad_norm": 4.22715950012207,
        "learning_rate": 0.00018173167206720885,
        "epoch": 0.7326179298010284,
        "step": 9831
    },
    {
        "loss": 2.4722,
        "grad_norm": 2.4964776039123535,
        "learning_rate": 0.000181691107934535,
        "epoch": 0.7326924510023102,
        "step": 9832
    },
    {
        "loss": 2.5044,
        "grad_norm": 3.2119388580322266,
        "learning_rate": 0.00018165050335391876,
        "epoch": 0.7327669722035919,
        "step": 9833
    },
    {
        "loss": 2.4755,
        "grad_norm": 2.8350675106048584,
        "learning_rate": 0.00018160985834546478,
        "epoch": 0.7328414934048737,
        "step": 9834
    },
    {
        "loss": 2.1938,
        "grad_norm": 3.515190362930298,
        "learning_rate": 0.0001815691729292976,
        "epoch": 0.7329160146061554,
        "step": 9835
    },
    {
        "loss": 1.9609,
        "grad_norm": 3.715597629547119,
        "learning_rate": 0.00018152844712556216,
        "epoch": 0.7329905358074372,
        "step": 9836
    },
    {
        "loss": 2.5799,
        "grad_norm": 2.2971692085266113,
        "learning_rate": 0.0001814876809544229,
        "epoch": 0.7330650570087189,
        "step": 9837
    },
    {
        "loss": 2.3693,
        "grad_norm": 2.7910051345825195,
        "learning_rate": 0.00018144687443606464,
        "epoch": 0.7331395782100008,
        "step": 9838
    },
    {
        "loss": 1.7988,
        "grad_norm": 4.907016277313232,
        "learning_rate": 0.000181406027590692,
        "epoch": 0.7332140994112825,
        "step": 9839
    },
    {
        "loss": 1.9831,
        "grad_norm": 2.402769088745117,
        "learning_rate": 0.00018136514043852947,
        "epoch": 0.7332886206125643,
        "step": 9840
    },
    {
        "loss": 1.8777,
        "grad_norm": 3.2380175590515137,
        "learning_rate": 0.00018132421299982187,
        "epoch": 0.733363141813846,
        "step": 9841
    },
    {
        "loss": 2.6193,
        "grad_norm": 2.705298900604248,
        "learning_rate": 0.00018128324529483343,
        "epoch": 0.7334376630151278,
        "step": 9842
    },
    {
        "loss": 1.6351,
        "grad_norm": 4.597621917724609,
        "learning_rate": 0.00018124223734384875,
        "epoch": 0.7335121842164096,
        "step": 9843
    },
    {
        "loss": 2.1658,
        "grad_norm": 2.3247005939483643,
        "learning_rate": 0.00018120118916717222,
        "epoch": 0.7335867054176913,
        "step": 9844
    },
    {
        "loss": 1.6448,
        "grad_norm": 2.5932443141937256,
        "learning_rate": 0.00018116010078512803,
        "epoch": 0.7336612266189732,
        "step": 9845
    },
    {
        "loss": 2.9629,
        "grad_norm": 2.2308998107910156,
        "learning_rate": 0.0001811189722180605,
        "epoch": 0.7337357478202549,
        "step": 9846
    },
    {
        "loss": 2.3106,
        "grad_norm": 3.3685226440429688,
        "learning_rate": 0.00018107780348633373,
        "epoch": 0.7338102690215367,
        "step": 9847
    },
    {
        "loss": 3.1147,
        "grad_norm": 2.379688024520874,
        "learning_rate": 0.00018103659461033155,
        "epoch": 0.7338847902228184,
        "step": 9848
    },
    {
        "loss": 2.3297,
        "grad_norm": 2.004182815551758,
        "learning_rate": 0.00018099534561045804,
        "epoch": 0.7339593114241002,
        "step": 9849
    },
    {
        "loss": 2.2778,
        "grad_norm": 3.944411516189575,
        "learning_rate": 0.00018095405650713682,
        "epoch": 0.7340338326253819,
        "step": 9850
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.7543537616729736,
        "learning_rate": 0.00018091272732081138,
        "epoch": 0.7341083538266637,
        "step": 9851
    },
    {
        "loss": 1.6275,
        "grad_norm": 3.6084542274475098,
        "learning_rate": 0.00018087135807194545,
        "epoch": 0.7341828750279454,
        "step": 9852
    },
    {
        "loss": 2.2333,
        "grad_norm": 3.212216854095459,
        "learning_rate": 0.00018082994878102195,
        "epoch": 0.7342573962292273,
        "step": 9853
    },
    {
        "loss": 2.2506,
        "grad_norm": 3.6235175132751465,
        "learning_rate": 0.00018078849946854422,
        "epoch": 0.734331917430509,
        "step": 9854
    },
    {
        "loss": 2.8685,
        "grad_norm": 3.202361822128296,
        "learning_rate": 0.000180747010155035,
        "epoch": 0.7344064386317908,
        "step": 9855
    },
    {
        "loss": 2.5327,
        "grad_norm": 2.855806827545166,
        "learning_rate": 0.00018070548086103716,
        "epoch": 0.7344809598330725,
        "step": 9856
    },
    {
        "loss": 2.33,
        "grad_norm": 2.639631509780884,
        "learning_rate": 0.00018066391160711314,
        "epoch": 0.7345554810343543,
        "step": 9857
    },
    {
        "loss": 2.2197,
        "grad_norm": 1.8954253196716309,
        "learning_rate": 0.00018062230241384518,
        "epoch": 0.734630002235636,
        "step": 9858
    },
    {
        "loss": 1.5794,
        "grad_norm": 3.2861623764038086,
        "learning_rate": 0.0001805806533018355,
        "epoch": 0.7347045234369178,
        "step": 9859
    },
    {
        "loss": 1.8457,
        "grad_norm": 2.906309127807617,
        "learning_rate": 0.00018053896429170578,
        "epoch": 0.7347790446381995,
        "step": 9860
    },
    {
        "loss": 2.304,
        "grad_norm": 2.2110331058502197,
        "learning_rate": 0.00018049723540409783,
        "epoch": 0.7348535658394814,
        "step": 9861
    },
    {
        "loss": 2.2567,
        "grad_norm": 3.176722288131714,
        "learning_rate": 0.00018045546665967273,
        "epoch": 0.7349280870407631,
        "step": 9862
    },
    {
        "loss": 2.5353,
        "grad_norm": 2.8387885093688965,
        "learning_rate": 0.00018041365807911172,
        "epoch": 0.7350026082420449,
        "step": 9863
    },
    {
        "loss": 2.5025,
        "grad_norm": 3.224302291870117,
        "learning_rate": 0.00018037180968311556,
        "epoch": 0.7350771294433266,
        "step": 9864
    },
    {
        "loss": 2.8614,
        "grad_norm": 3.136014699935913,
        "learning_rate": 0.0001803299214924047,
        "epoch": 0.7351516506446084,
        "step": 9865
    },
    {
        "loss": 0.822,
        "grad_norm": 3.9215664863586426,
        "learning_rate": 0.00018028799352771944,
        "epoch": 0.7352261718458901,
        "step": 9866
    },
    {
        "loss": 1.9449,
        "grad_norm": 4.227886199951172,
        "learning_rate": 0.00018024602580981968,
        "epoch": 0.7353006930471719,
        "step": 9867
    },
    {
        "loss": 2.4037,
        "grad_norm": 2.95688796043396,
        "learning_rate": 0.0001802040183594849,
        "epoch": 0.7353752142484536,
        "step": 9868
    },
    {
        "loss": 1.9551,
        "grad_norm": 3.8315694332122803,
        "learning_rate": 0.00018016197119751458,
        "epoch": 0.7354497354497355,
        "step": 9869
    },
    {
        "loss": 1.2284,
        "grad_norm": 5.632161617279053,
        "learning_rate": 0.00018011988434472745,
        "epoch": 0.7355242566510172,
        "step": 9870
    },
    {
        "loss": 2.5227,
        "grad_norm": 2.3056042194366455,
        "learning_rate": 0.0001800777578219621,
        "epoch": 0.735598777852299,
        "step": 9871
    },
    {
        "loss": 2.429,
        "grad_norm": 3.1556777954101562,
        "learning_rate": 0.00018003559165007696,
        "epoch": 0.7356732990535807,
        "step": 9872
    },
    {
        "loss": 1.8009,
        "grad_norm": 3.3724825382232666,
        "learning_rate": 0.00017999338584994958,
        "epoch": 0.7357478202548625,
        "step": 9873
    },
    {
        "loss": 1.4089,
        "grad_norm": 2.8642919063568115,
        "learning_rate": 0.00017995114044247766,
        "epoch": 0.7358223414561442,
        "step": 9874
    },
    {
        "loss": 2.0289,
        "grad_norm": 2.5116124153137207,
        "learning_rate": 0.00017990885544857824,
        "epoch": 0.735896862657426,
        "step": 9875
    },
    {
        "loss": 1.3702,
        "grad_norm": 3.852259635925293,
        "learning_rate": 0.00017986653088918785,
        "epoch": 0.7359713838587078,
        "step": 9876
    },
    {
        "loss": 2.4685,
        "grad_norm": 2.9174253940582275,
        "learning_rate": 0.000179824166785263,
        "epoch": 0.7360459050599896,
        "step": 9877
    },
    {
        "loss": 2.1909,
        "grad_norm": 2.645353078842163,
        "learning_rate": 0.00017978176315777933,
        "epoch": 0.7361204262612713,
        "step": 9878
    },
    {
        "loss": 2.5266,
        "grad_norm": 3.748046636581421,
        "learning_rate": 0.00017973932002773244,
        "epoch": 0.7361949474625531,
        "step": 9879
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.914473056793213,
        "learning_rate": 0.00017969683741613727,
        "epoch": 0.7362694686638349,
        "step": 9880
    },
    {
        "loss": 2.8948,
        "grad_norm": 2.4110894203186035,
        "learning_rate": 0.0001796543153440283,
        "epoch": 0.7363439898651166,
        "step": 9881
    },
    {
        "loss": 1.4994,
        "grad_norm": 4.771461009979248,
        "learning_rate": 0.00017961175383245955,
        "epoch": 0.7364185110663984,
        "step": 9882
    },
    {
        "loss": 2.6928,
        "grad_norm": 2.5703933238983154,
        "learning_rate": 0.0001795691529025048,
        "epoch": 0.7364930322676801,
        "step": 9883
    },
    {
        "loss": 2.766,
        "grad_norm": 3.0383975505828857,
        "learning_rate": 0.00017952651257525698,
        "epoch": 0.736567553468962,
        "step": 9884
    },
    {
        "loss": 1.6419,
        "grad_norm": 4.398342609405518,
        "learning_rate": 0.00017948383287182885,
        "epoch": 0.7366420746702437,
        "step": 9885
    },
    {
        "loss": 2.7113,
        "grad_norm": 2.9127466678619385,
        "learning_rate": 0.00017944111381335237,
        "epoch": 0.7367165958715255,
        "step": 9886
    },
    {
        "loss": 2.3945,
        "grad_norm": 2.4212145805358887,
        "learning_rate": 0.00017939835542097937,
        "epoch": 0.7367911170728072,
        "step": 9887
    },
    {
        "loss": 2.6866,
        "grad_norm": 2.539412260055542,
        "learning_rate": 0.00017935555771588066,
        "epoch": 0.736865638274089,
        "step": 9888
    },
    {
        "loss": 2.1784,
        "grad_norm": 4.122758865356445,
        "learning_rate": 0.000179312720719247,
        "epoch": 0.7369401594753707,
        "step": 9889
    },
    {
        "loss": 2.7052,
        "grad_norm": 3.1170990467071533,
        "learning_rate": 0.00017926984445228836,
        "epoch": 0.7370146806766525,
        "step": 9890
    },
    {
        "loss": 2.2746,
        "grad_norm": 2.9464893341064453,
        "learning_rate": 0.00017922692893623405,
        "epoch": 0.7370892018779343,
        "step": 9891
    },
    {
        "loss": 2.1731,
        "grad_norm": 2.530794620513916,
        "learning_rate": 0.00017918397419233322,
        "epoch": 0.7371637230792161,
        "step": 9892
    },
    {
        "loss": 2.6156,
        "grad_norm": 2.6175789833068848,
        "learning_rate": 0.00017914098024185377,
        "epoch": 0.7372382442804978,
        "step": 9893
    },
    {
        "loss": 2.3567,
        "grad_norm": 2.8727264404296875,
        "learning_rate": 0.00017909794710608377,
        "epoch": 0.7373127654817796,
        "step": 9894
    },
    {
        "loss": 3.0688,
        "grad_norm": 2.5453686714172363,
        "learning_rate": 0.00017905487480633013,
        "epoch": 0.7373872866830613,
        "step": 9895
    },
    {
        "loss": 2.2833,
        "grad_norm": 4.325636863708496,
        "learning_rate": 0.0001790117633639194,
        "epoch": 0.7374618078843431,
        "step": 9896
    },
    {
        "loss": 2.5627,
        "grad_norm": 2.02227520942688,
        "learning_rate": 0.00017896861280019755,
        "epoch": 0.7375363290856248,
        "step": 9897
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.45054292678833,
        "learning_rate": 0.0001789254231365297,
        "epoch": 0.7376108502869066,
        "step": 9898
    },
    {
        "loss": 1.319,
        "grad_norm": 2.2055482864379883,
        "learning_rate": 0.0001788821943943006,
        "epoch": 0.7376853714881884,
        "step": 9899
    },
    {
        "loss": 2.2042,
        "grad_norm": 3.7347328662872314,
        "learning_rate": 0.0001788389265949142,
        "epoch": 0.7377598926894702,
        "step": 9900
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.6880476474761963,
        "learning_rate": 0.0001787956197597937,
        "epoch": 0.7378344138907519,
        "step": 9901
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.461817979812622,
        "learning_rate": 0.00017875227391038175,
        "epoch": 0.7379089350920337,
        "step": 9902
    },
    {
        "loss": 2.9692,
        "grad_norm": 3.1490390300750732,
        "learning_rate": 0.00017870888906814053,
        "epoch": 0.7379834562933154,
        "step": 9903
    },
    {
        "loss": 2.5872,
        "grad_norm": 3.6050446033477783,
        "learning_rate": 0.00017866546525455092,
        "epoch": 0.7380579774945972,
        "step": 9904
    },
    {
        "loss": 2.5367,
        "grad_norm": 3.4359521865844727,
        "learning_rate": 0.00017862200249111373,
        "epoch": 0.7381324986958789,
        "step": 9905
    },
    {
        "loss": 2.7583,
        "grad_norm": 5.19323205947876,
        "learning_rate": 0.00017857850079934867,
        "epoch": 0.7382070198971608,
        "step": 9906
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.636808156967163,
        "learning_rate": 0.00017853496020079496,
        "epoch": 0.7382815410984425,
        "step": 9907
    },
    {
        "loss": 2.7023,
        "grad_norm": 2.6199393272399902,
        "learning_rate": 0.00017849138071701095,
        "epoch": 0.7383560622997243,
        "step": 9908
    },
    {
        "loss": 2.1899,
        "grad_norm": 2.1054885387420654,
        "learning_rate": 0.00017844776236957416,
        "epoch": 0.738430583501006,
        "step": 9909
    },
    {
        "loss": 2.2342,
        "grad_norm": 3.0064988136291504,
        "learning_rate": 0.00017840410518008164,
        "epoch": 0.7385051047022878,
        "step": 9910
    },
    {
        "loss": 2.6675,
        "grad_norm": 3.2195587158203125,
        "learning_rate": 0.0001783604091701493,
        "epoch": 0.7385796259035695,
        "step": 9911
    },
    {
        "loss": 2.3813,
        "grad_norm": 3.5976967811584473,
        "learning_rate": 0.00017831667436141277,
        "epoch": 0.7386541471048513,
        "step": 9912
    },
    {
        "loss": 2.4962,
        "grad_norm": 3.3723411560058594,
        "learning_rate": 0.00017827290077552618,
        "epoch": 0.738728668306133,
        "step": 9913
    },
    {
        "loss": 1.9844,
        "grad_norm": 3.0673651695251465,
        "learning_rate": 0.00017822908843416357,
        "epoch": 0.7388031895074149,
        "step": 9914
    },
    {
        "loss": 2.5626,
        "grad_norm": 2.284477949142456,
        "learning_rate": 0.00017818523735901778,
        "epoch": 0.7388777107086967,
        "step": 9915
    },
    {
        "loss": 2.5489,
        "grad_norm": 2.6275177001953125,
        "learning_rate": 0.00017814134757180078,
        "epoch": 0.7389522319099784,
        "step": 9916
    },
    {
        "loss": 2.7779,
        "grad_norm": 3.6143994331359863,
        "learning_rate": 0.00017809741909424406,
        "epoch": 0.7390267531112602,
        "step": 9917
    },
    {
        "loss": 1.9211,
        "grad_norm": 3.2147700786590576,
        "learning_rate": 0.00017805345194809793,
        "epoch": 0.7391012743125419,
        "step": 9918
    },
    {
        "loss": 1.6759,
        "grad_norm": 3.714186906814575,
        "learning_rate": 0.0001780094461551319,
        "epoch": 0.7391757955138237,
        "step": 9919
    },
    {
        "loss": 2.6151,
        "grad_norm": 2.254413604736328,
        "learning_rate": 0.00017796540173713483,
        "epoch": 0.7392503167151054,
        "step": 9920
    },
    {
        "loss": 1.7864,
        "grad_norm": 3.4406628608703613,
        "learning_rate": 0.00017792131871591452,
        "epoch": 0.7393248379163873,
        "step": 9921
    },
    {
        "loss": 2.496,
        "grad_norm": 3.286543130874634,
        "learning_rate": 0.00017787719711329775,
        "epoch": 0.739399359117669,
        "step": 9922
    },
    {
        "loss": 1.9787,
        "grad_norm": 3.567378044128418,
        "learning_rate": 0.00017783303695113092,
        "epoch": 0.7394738803189508,
        "step": 9923
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.133275032043457,
        "learning_rate": 0.00017778883825127873,
        "epoch": 0.7395484015202325,
        "step": 9924
    },
    {
        "loss": 2.8924,
        "grad_norm": 3.9622344970703125,
        "learning_rate": 0.00017774460103562573,
        "epoch": 0.7396229227215143,
        "step": 9925
    },
    {
        "loss": 2.4881,
        "grad_norm": 3.006279468536377,
        "learning_rate": 0.00017770032532607504,
        "epoch": 0.739697443922796,
        "step": 9926
    },
    {
        "loss": 2.4122,
        "grad_norm": 2.689649820327759,
        "learning_rate": 0.00017765601114454915,
        "epoch": 0.7397719651240778,
        "step": 9927
    },
    {
        "loss": 2.1745,
        "grad_norm": 3.2942328453063965,
        "learning_rate": 0.0001776116585129894,
        "epoch": 0.7398464863253595,
        "step": 9928
    },
    {
        "loss": 2.1128,
        "grad_norm": 3.6347994804382324,
        "learning_rate": 0.00017756726745335612,
        "epoch": 0.7399210075266414,
        "step": 9929
    },
    {
        "loss": 2.2383,
        "grad_norm": 2.3259670734405518,
        "learning_rate": 0.000177522837987629,
        "epoch": 0.7399955287279231,
        "step": 9930
    },
    {
        "loss": 2.4337,
        "grad_norm": 3.8345775604248047,
        "learning_rate": 0.0001774783701378063,
        "epoch": 0.7400700499292049,
        "step": 9931
    },
    {
        "loss": 2.4846,
        "grad_norm": 2.626866579055786,
        "learning_rate": 0.00017743386392590578,
        "epoch": 0.7401445711304866,
        "step": 9932
    },
    {
        "loss": 2.3716,
        "grad_norm": 2.734632968902588,
        "learning_rate": 0.00017738931937396357,
        "epoch": 0.7402190923317684,
        "step": 9933
    },
    {
        "loss": 2.1615,
        "grad_norm": 3.875023126602173,
        "learning_rate": 0.00017734473650403545,
        "epoch": 0.7402936135330501,
        "step": 9934
    },
    {
        "loss": 2.6856,
        "grad_norm": 2.4318180084228516,
        "learning_rate": 0.0001773001153381957,
        "epoch": 0.7403681347343319,
        "step": 9935
    },
    {
        "loss": 2.0349,
        "grad_norm": 4.0198588371276855,
        "learning_rate": 0.00017725545589853775,
        "epoch": 0.7404426559356136,
        "step": 9936
    },
    {
        "loss": 2.8867,
        "grad_norm": 3.6355464458465576,
        "learning_rate": 0.0001772107582071739,
        "epoch": 0.7405171771368955,
        "step": 9937
    },
    {
        "loss": 2.4821,
        "grad_norm": 3.427345037460327,
        "learning_rate": 0.00017716602228623555,
        "epoch": 0.7405916983381772,
        "step": 9938
    },
    {
        "loss": 2.5416,
        "grad_norm": 2.373645067214966,
        "learning_rate": 0.00017712124815787278,
        "epoch": 0.740666219539459,
        "step": 9939
    },
    {
        "loss": 2.7251,
        "grad_norm": 2.689493417739868,
        "learning_rate": 0.00017707643584425492,
        "epoch": 0.7407407407407407,
        "step": 9940
    },
    {
        "loss": 2.5492,
        "grad_norm": 2.770132064819336,
        "learning_rate": 0.00017703158536756987,
        "epoch": 0.7408152619420225,
        "step": 9941
    },
    {
        "loss": 2.6366,
        "grad_norm": 2.925330638885498,
        "learning_rate": 0.00017698669675002453,
        "epoch": 0.7408897831433042,
        "step": 9942
    },
    {
        "loss": 2.606,
        "grad_norm": 3.499159336090088,
        "learning_rate": 0.000176941770013845,
        "epoch": 0.740964304344586,
        "step": 9943
    },
    {
        "loss": 1.6831,
        "grad_norm": 3.616243839263916,
        "learning_rate": 0.00017689680518127553,
        "epoch": 0.7410388255458678,
        "step": 9944
    },
    {
        "loss": 2.3658,
        "grad_norm": 3.516474723815918,
        "learning_rate": 0.00017685180227458008,
        "epoch": 0.7411133467471496,
        "step": 9945
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.159841775894165,
        "learning_rate": 0.00017680676131604086,
        "epoch": 0.7411878679484313,
        "step": 9946
    },
    {
        "loss": 2.2855,
        "grad_norm": 2.2428886890411377,
        "learning_rate": 0.0001767616823279591,
        "epoch": 0.7412623891497131,
        "step": 9947
    },
    {
        "loss": 1.5248,
        "grad_norm": 3.5560858249664307,
        "learning_rate": 0.00017671656533265506,
        "epoch": 0.7413369103509948,
        "step": 9948
    },
    {
        "loss": 1.9392,
        "grad_norm": 3.2161662578582764,
        "learning_rate": 0.00017667141035246744,
        "epoch": 0.7414114315522766,
        "step": 9949
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.330967664718628,
        "learning_rate": 0.00017662621740975414,
        "epoch": 0.7414859527535584,
        "step": 9950
    },
    {
        "loss": 2.4389,
        "grad_norm": 2.8578052520751953,
        "learning_rate": 0.00017658098652689156,
        "epoch": 0.7415604739548401,
        "step": 9951
    },
    {
        "loss": 2.3693,
        "grad_norm": 2.6500320434570312,
        "learning_rate": 0.00017653571772627502,
        "epoch": 0.741634995156122,
        "step": 9952
    },
    {
        "loss": 2.7984,
        "grad_norm": 3.5649871826171875,
        "learning_rate": 0.00017649041103031844,
        "epoch": 0.7417095163574037,
        "step": 9953
    },
    {
        "loss": 3.1121,
        "grad_norm": 2.3000073432922363,
        "learning_rate": 0.00017644506646145492,
        "epoch": 0.7417840375586855,
        "step": 9954
    },
    {
        "loss": 2.1555,
        "grad_norm": 3.419808864593506,
        "learning_rate": 0.00017639968404213586,
        "epoch": 0.7418585587599672,
        "step": 9955
    },
    {
        "loss": 1.7058,
        "grad_norm": 3.8742144107818604,
        "learning_rate": 0.00017635426379483164,
        "epoch": 0.741933079961249,
        "step": 9956
    },
    {
        "loss": 2.0653,
        "grad_norm": 4.773871898651123,
        "learning_rate": 0.0001763088057420312,
        "epoch": 0.7420076011625307,
        "step": 9957
    },
    {
        "loss": 2.2275,
        "grad_norm": 3.4501419067382812,
        "learning_rate": 0.00017626330990624248,
        "epoch": 0.7420821223638125,
        "step": 9958
    },
    {
        "loss": 2.5641,
        "grad_norm": 2.4156410694122314,
        "learning_rate": 0.0001762177763099918,
        "epoch": 0.7421566435650943,
        "step": 9959
    },
    {
        "loss": 1.5078,
        "grad_norm": 3.6175050735473633,
        "learning_rate": 0.0001761722049758245,
        "epoch": 0.7422311647663761,
        "step": 9960
    },
    {
        "loss": 2.7232,
        "grad_norm": 3.6199417114257812,
        "learning_rate": 0.00017612659592630434,
        "epoch": 0.7423056859676578,
        "step": 9961
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.7961978912353516,
        "learning_rate": 0.00017608094918401379,
        "epoch": 0.7423802071689396,
        "step": 9962
    },
    {
        "loss": 1.5725,
        "grad_norm": 2.1247494220733643,
        "learning_rate": 0.0001760352647715543,
        "epoch": 0.7424547283702213,
        "step": 9963
    },
    {
        "loss": 2.007,
        "grad_norm": 3.867037773132324,
        "learning_rate": 0.00017598954271154537,
        "epoch": 0.7425292495715031,
        "step": 9964
    },
    {
        "loss": 2.3613,
        "grad_norm": 3.847642421722412,
        "learning_rate": 0.00017594378302662576,
        "epoch": 0.7426037707727848,
        "step": 9965
    },
    {
        "loss": 2.4891,
        "grad_norm": 4.483058929443359,
        "learning_rate": 0.0001758979857394525,
        "epoch": 0.7426782919740667,
        "step": 9966
    },
    {
        "loss": 2.9053,
        "grad_norm": 2.557725429534912,
        "learning_rate": 0.00017585215087270118,
        "epoch": 0.7427528131753484,
        "step": 9967
    },
    {
        "loss": 2.6959,
        "grad_norm": 2.805521249771118,
        "learning_rate": 0.00017580627844906638,
        "epoch": 0.7428273343766302,
        "step": 9968
    },
    {
        "loss": 2.359,
        "grad_norm": 2.8007190227508545,
        "learning_rate": 0.00017576036849126098,
        "epoch": 0.7429018555779119,
        "step": 9969
    },
    {
        "loss": 2.4144,
        "grad_norm": 3.4986019134521484,
        "learning_rate": 0.00017571442102201634,
        "epoch": 0.7429763767791937,
        "step": 9970
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.9701590538024902,
        "learning_rate": 0.0001756684360640828,
        "epoch": 0.7430508979804754,
        "step": 9971
    },
    {
        "loss": 2.0285,
        "grad_norm": 2.788529634475708,
        "learning_rate": 0.00017562241364022887,
        "epoch": 0.7431254191817572,
        "step": 9972
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.516444206237793,
        "learning_rate": 0.00017557635377324167,
        "epoch": 0.7431999403830389,
        "step": 9973
    },
    {
        "loss": 2.4679,
        "grad_norm": 3.103043556213379,
        "learning_rate": 0.0001755302564859273,
        "epoch": 0.7432744615843208,
        "step": 9974
    },
    {
        "loss": 2.7688,
        "grad_norm": 2.7572884559631348,
        "learning_rate": 0.0001754841218011096,
        "epoch": 0.7433489827856025,
        "step": 9975
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.723254442214966,
        "learning_rate": 0.00017543794974163163,
        "epoch": 0.7434235039868843,
        "step": 9976
    },
    {
        "loss": 2.5885,
        "grad_norm": 2.1929733753204346,
        "learning_rate": 0.00017539174033035456,
        "epoch": 0.743498025188166,
        "step": 9977
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.772852659225464,
        "learning_rate": 0.00017534549359015834,
        "epoch": 0.7435725463894478,
        "step": 9978
    },
    {
        "loss": 2.3722,
        "grad_norm": 3.2046589851379395,
        "learning_rate": 0.00017529920954394113,
        "epoch": 0.7436470675907295,
        "step": 9979
    },
    {
        "loss": 2.626,
        "grad_norm": 4.009394645690918,
        "learning_rate": 0.00017525288821461962,
        "epoch": 0.7437215887920113,
        "step": 9980
    },
    {
        "loss": 2.8763,
        "grad_norm": 2.126512289047241,
        "learning_rate": 0.0001752065296251292,
        "epoch": 0.743796109993293,
        "step": 9981
    },
    {
        "loss": 2.2088,
        "grad_norm": 3.4672858715057373,
        "learning_rate": 0.00017516013379842332,
        "epoch": 0.7438706311945749,
        "step": 9982
    },
    {
        "loss": 2.9833,
        "grad_norm": 1.717979907989502,
        "learning_rate": 0.00017511370075747433,
        "epoch": 0.7439451523958566,
        "step": 9983
    },
    {
        "loss": 3.1852,
        "grad_norm": 2.6004467010498047,
        "learning_rate": 0.00017506723052527242,
        "epoch": 0.7440196735971384,
        "step": 9984
    },
    {
        "loss": 2.1038,
        "grad_norm": 2.8730952739715576,
        "learning_rate": 0.0001750207231248268,
        "epoch": 0.7440941947984202,
        "step": 9985
    },
    {
        "loss": 2.5421,
        "grad_norm": 2.946917772293091,
        "learning_rate": 0.0001749741785791647,
        "epoch": 0.7441687159997019,
        "step": 9986
    },
    {
        "loss": 2.0185,
        "grad_norm": 3.2352092266082764,
        "learning_rate": 0.00017492759691133176,
        "epoch": 0.7442432372009837,
        "step": 9987
    },
    {
        "loss": 2.6381,
        "grad_norm": 3.0313053131103516,
        "learning_rate": 0.00017488097814439226,
        "epoch": 0.7443177584022654,
        "step": 9988
    },
    {
        "loss": 2.182,
        "grad_norm": 4.44901704788208,
        "learning_rate": 0.0001748343223014286,
        "epoch": 0.7443922796035473,
        "step": 9989
    },
    {
        "loss": 2.736,
        "grad_norm": 2.8183655738830566,
        "learning_rate": 0.00017478762940554153,
        "epoch": 0.744466800804829,
        "step": 9990
    },
    {
        "loss": 2.6714,
        "grad_norm": 3.2105231285095215,
        "learning_rate": 0.00017474089947985038,
        "epoch": 0.7445413220061108,
        "step": 9991
    },
    {
        "loss": 1.8492,
        "grad_norm": 3.857036828994751,
        "learning_rate": 0.00017469413254749263,
        "epoch": 0.7446158432073925,
        "step": 9992
    },
    {
        "loss": 1.7668,
        "grad_norm": 4.44472074508667,
        "learning_rate": 0.000174647328631624,
        "epoch": 0.7446903644086743,
        "step": 9993
    },
    {
        "loss": 2.5164,
        "grad_norm": 3.7706735134124756,
        "learning_rate": 0.00017460048775541891,
        "epoch": 0.744764885609956,
        "step": 9994
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.321624279022217,
        "learning_rate": 0.00017455360994206943,
        "epoch": 0.7448394068112378,
        "step": 9995
    },
    {
        "loss": 2.649,
        "grad_norm": 2.0215015411376953,
        "learning_rate": 0.00017450669521478664,
        "epoch": 0.7449139280125195,
        "step": 9996
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.889770984649658,
        "learning_rate": 0.0001744597435967994,
        "epoch": 0.7449884492138014,
        "step": 9997
    },
    {
        "loss": 2.7492,
        "grad_norm": 3.7437868118286133,
        "learning_rate": 0.00017441275511135488,
        "epoch": 0.7450629704150831,
        "step": 9998
    },
    {
        "loss": 1.7547,
        "grad_norm": 4.883566379547119,
        "learning_rate": 0.0001743657297817189,
        "epoch": 0.7451374916163649,
        "step": 9999
    },
    {
        "loss": 2.7149,
        "grad_norm": 1.5963184833526611,
        "learning_rate": 0.000174318667631175,
        "epoch": 0.7452120128176466,
        "step": 10000
    },
    {
        "loss": 2.3455,
        "grad_norm": 3.0576679706573486,
        "learning_rate": 0.00017427156868302535,
        "epoch": 0.7452865340189284,
        "step": 10001
    },
    {
        "loss": 2.8597,
        "grad_norm": 3.0525662899017334,
        "learning_rate": 0.00017422443296059013,
        "epoch": 0.7453610552202101,
        "step": 10002
    },
    {
        "loss": 1.7005,
        "grad_norm": 5.674880027770996,
        "learning_rate": 0.00017417726048720775,
        "epoch": 0.7454355764214919,
        "step": 10003
    },
    {
        "loss": 1.5539,
        "grad_norm": 3.3790104389190674,
        "learning_rate": 0.0001741300512862348,
        "epoch": 0.7455100976227736,
        "step": 10004
    },
    {
        "loss": 2.3779,
        "grad_norm": 3.413681745529175,
        "learning_rate": 0.00017408280538104624,
        "epoch": 0.7455846188240555,
        "step": 10005
    },
    {
        "loss": 2.3925,
        "grad_norm": 3.994659423828125,
        "learning_rate": 0.000174035522795035,
        "epoch": 0.7456591400253372,
        "step": 10006
    },
    {
        "loss": 2.4483,
        "grad_norm": 3.9880547523498535,
        "learning_rate": 0.00017398820355161217,
        "epoch": 0.745733661226619,
        "step": 10007
    },
    {
        "loss": 2.7417,
        "grad_norm": 3.0346412658691406,
        "learning_rate": 0.0001739408476742071,
        "epoch": 0.7458081824279007,
        "step": 10008
    },
    {
        "loss": 2.3444,
        "grad_norm": 3.011267900466919,
        "learning_rate": 0.00017389345518626728,
        "epoch": 0.7458827036291825,
        "step": 10009
    },
    {
        "loss": 1.6225,
        "grad_norm": 3.8913087844848633,
        "learning_rate": 0.00017384602611125816,
        "epoch": 0.7459572248304642,
        "step": 10010
    },
    {
        "loss": 1.8995,
        "grad_norm": 3.478139638900757,
        "learning_rate": 0.00017379856047266366,
        "epoch": 0.746031746031746,
        "step": 10011
    },
    {
        "loss": 3.0345,
        "grad_norm": 1.4270074367523193,
        "learning_rate": 0.00017375105829398537,
        "epoch": 0.7461062672330278,
        "step": 10012
    },
    {
        "loss": 1.4548,
        "grad_norm": 3.5699374675750732,
        "learning_rate": 0.00017370351959874317,
        "epoch": 0.7461807884343096,
        "step": 10013
    },
    {
        "loss": 2.2506,
        "grad_norm": 3.6119697093963623,
        "learning_rate": 0.00017365594441047527,
        "epoch": 0.7462553096355913,
        "step": 10014
    },
    {
        "loss": 2.3088,
        "grad_norm": 2.9310362339019775,
        "learning_rate": 0.00017360833275273735,
        "epoch": 0.7463298308368731,
        "step": 10015
    },
    {
        "loss": 2.5207,
        "grad_norm": 3.0361223220825195,
        "learning_rate": 0.00017356068464910375,
        "epoch": 0.7464043520381548,
        "step": 10016
    },
    {
        "loss": 1.6707,
        "grad_norm": 3.9397518634796143,
        "learning_rate": 0.00017351300012316654,
        "epoch": 0.7464788732394366,
        "step": 10017
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.9703316688537598,
        "learning_rate": 0.00017346527919853577,
        "epoch": 0.7465533944407183,
        "step": 10018
    },
    {
        "loss": 2.8339,
        "grad_norm": 2.366940975189209,
        "learning_rate": 0.00017341752189883983,
        "epoch": 0.7466279156420002,
        "step": 10019
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.796124219894409,
        "learning_rate": 0.00017336972824772478,
        "epoch": 0.746702436843282,
        "step": 10020
    },
    {
        "loss": 1.6533,
        "grad_norm": 1.610283613204956,
        "learning_rate": 0.00017332189826885493,
        "epoch": 0.7467769580445637,
        "step": 10021
    },
    {
        "loss": 2.9855,
        "grad_norm": 4.34974479675293,
        "learning_rate": 0.00017327403198591244,
        "epoch": 0.7468514792458455,
        "step": 10022
    },
    {
        "loss": 3.0613,
        "grad_norm": 3.6412723064422607,
        "learning_rate": 0.0001732261294225974,
        "epoch": 0.7469260004471272,
        "step": 10023
    },
    {
        "loss": 2.5989,
        "grad_norm": 3.6865074634552,
        "learning_rate": 0.00017317819060262793,
        "epoch": 0.747000521648409,
        "step": 10024
    },
    {
        "loss": 2.4044,
        "grad_norm": 3.9575157165527344,
        "learning_rate": 0.00017313021554974024,
        "epoch": 0.7470750428496907,
        "step": 10025
    },
    {
        "loss": 2.4511,
        "grad_norm": 2.8897182941436768,
        "learning_rate": 0.00017308220428768832,
        "epoch": 0.7471495640509725,
        "step": 10026
    },
    {
        "loss": 2.9413,
        "grad_norm": 2.530229091644287,
        "learning_rate": 0.00017303415684024405,
        "epoch": 0.7472240852522543,
        "step": 10027
    },
    {
        "loss": 2.9812,
        "grad_norm": 2.3665432929992676,
        "learning_rate": 0.0001729860732311972,
        "epoch": 0.7472986064535361,
        "step": 10028
    },
    {
        "loss": 1.6012,
        "grad_norm": 3.944810390472412,
        "learning_rate": 0.00017293795348435578,
        "epoch": 0.7473731276548178,
        "step": 10029
    },
    {
        "loss": 2.6719,
        "grad_norm": 2.3460566997528076,
        "learning_rate": 0.0001728897976235453,
        "epoch": 0.7474476488560996,
        "step": 10030
    },
    {
        "loss": 1.9382,
        "grad_norm": 3.272268056869507,
        "learning_rate": 0.00017284160567260926,
        "epoch": 0.7475221700573813,
        "step": 10031
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.385788679122925,
        "learning_rate": 0.00017279337765540925,
        "epoch": 0.7475966912586631,
        "step": 10032
    },
    {
        "loss": 2.4409,
        "grad_norm": 3.210754156112671,
        "learning_rate": 0.00017274511359582432,
        "epoch": 0.7476712124599448,
        "step": 10033
    },
    {
        "loss": 2.4886,
        "grad_norm": 3.0905396938323975,
        "learning_rate": 0.00017269681351775188,
        "epoch": 0.7477457336612267,
        "step": 10034
    },
    {
        "loss": 2.1935,
        "grad_norm": 2.907470941543579,
        "learning_rate": 0.00017264847744510653,
        "epoch": 0.7478202548625084,
        "step": 10035
    },
    {
        "loss": 2.5376,
        "grad_norm": 4.11519718170166,
        "learning_rate": 0.00017260010540182133,
        "epoch": 0.7478947760637902,
        "step": 10036
    },
    {
        "loss": 2.7013,
        "grad_norm": 5.138507843017578,
        "learning_rate": 0.00017255169741184668,
        "epoch": 0.7479692972650719,
        "step": 10037
    },
    {
        "loss": 2.8077,
        "grad_norm": 3.7956151962280273,
        "learning_rate": 0.000172503253499151,
        "epoch": 0.7480438184663537,
        "step": 10038
    },
    {
        "loss": 2.5244,
        "grad_norm": 3.5589559078216553,
        "learning_rate": 0.00017245477368772055,
        "epoch": 0.7481183396676354,
        "step": 10039
    },
    {
        "loss": 1.718,
        "grad_norm": 3.19482421875,
        "learning_rate": 0.00017240625800155922,
        "epoch": 0.7481928608689172,
        "step": 10040
    },
    {
        "loss": 1.8028,
        "grad_norm": 4.9160308837890625,
        "learning_rate": 0.00017235770646468858,
        "epoch": 0.7482673820701989,
        "step": 10041
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.7026121616363525,
        "learning_rate": 0.00017230911910114834,
        "epoch": 0.7483419032714808,
        "step": 10042
    },
    {
        "loss": 2.5586,
        "grad_norm": 2.7428839206695557,
        "learning_rate": 0.00017226049593499555,
        "epoch": 0.7484164244727625,
        "step": 10043
    },
    {
        "loss": 2.2406,
        "grad_norm": 4.35573148727417,
        "learning_rate": 0.00017221183699030505,
        "epoch": 0.7484909456740443,
        "step": 10044
    },
    {
        "loss": 2.1907,
        "grad_norm": 3.8311960697174072,
        "learning_rate": 0.00017216314229116976,
        "epoch": 0.748565466875326,
        "step": 10045
    },
    {
        "loss": 2.0761,
        "grad_norm": 3.195943593978882,
        "learning_rate": 0.0001721144118616996,
        "epoch": 0.7486399880766078,
        "step": 10046
    },
    {
        "loss": 1.6728,
        "grad_norm": 2.433162212371826,
        "learning_rate": 0.000172065645726023,
        "epoch": 0.7487145092778895,
        "step": 10047
    },
    {
        "loss": 1.7623,
        "grad_norm": 3.859626531600952,
        "learning_rate": 0.00017201684390828536,
        "epoch": 0.7487890304791713,
        "step": 10048
    },
    {
        "loss": 2.5091,
        "grad_norm": 2.144057035446167,
        "learning_rate": 0.00017196800643265033,
        "epoch": 0.748863551680453,
        "step": 10049
    },
    {
        "loss": 2.201,
        "grad_norm": 3.220720052719116,
        "learning_rate": 0.00017191913332329878,
        "epoch": 0.7489380728817349,
        "step": 10050
    },
    {
        "loss": 2.4466,
        "grad_norm": 3.4583277702331543,
        "learning_rate": 0.0001718702246044293,
        "epoch": 0.7490125940830166,
        "step": 10051
    },
    {
        "loss": 2.5303,
        "grad_norm": 2.9910061359405518,
        "learning_rate": 0.0001718212803002585,
        "epoch": 0.7490871152842984,
        "step": 10052
    },
    {
        "loss": 2.9174,
        "grad_norm": 2.1876296997070312,
        "learning_rate": 0.00017177230043501996,
        "epoch": 0.7491616364855801,
        "step": 10053
    },
    {
        "loss": 2.5724,
        "grad_norm": 3.9443180561065674,
        "learning_rate": 0.00017172328503296558,
        "epoch": 0.7492361576868619,
        "step": 10054
    },
    {
        "loss": 2.6905,
        "grad_norm": 2.781033515930176,
        "learning_rate": 0.00017167423411836415,
        "epoch": 0.7493106788881436,
        "step": 10055
    },
    {
        "loss": 2.2945,
        "grad_norm": 2.3837575912475586,
        "learning_rate": 0.00017162514771550255,
        "epoch": 0.7493852000894254,
        "step": 10056
    },
    {
        "loss": 2.8474,
        "grad_norm": 3.208068370819092,
        "learning_rate": 0.00017157602584868512,
        "epoch": 0.7494597212907073,
        "step": 10057
    },
    {
        "loss": 2.2902,
        "grad_norm": 4.1741862297058105,
        "learning_rate": 0.00017152686854223347,
        "epoch": 0.749534242491989,
        "step": 10058
    },
    {
        "loss": 2.6672,
        "grad_norm": 3.5238921642303467,
        "learning_rate": 0.00017147767582048723,
        "epoch": 0.7496087636932708,
        "step": 10059
    },
    {
        "loss": 1.7808,
        "grad_norm": 3.38346529006958,
        "learning_rate": 0.00017142844770780328,
        "epoch": 0.7496832848945525,
        "step": 10060
    },
    {
        "loss": 2.772,
        "grad_norm": 3.1004607677459717,
        "learning_rate": 0.00017137918422855595,
        "epoch": 0.7497578060958343,
        "step": 10061
    },
    {
        "loss": 1.721,
        "grad_norm": 3.4446096420288086,
        "learning_rate": 0.00017132988540713738,
        "epoch": 0.749832327297116,
        "step": 10062
    },
    {
        "loss": 0.9594,
        "grad_norm": 4.7872138023376465,
        "learning_rate": 0.0001712805512679569,
        "epoch": 0.7499068484983978,
        "step": 10063
    },
    {
        "loss": 2.6193,
        "grad_norm": 4.026644706726074,
        "learning_rate": 0.00017123118183544144,
        "epoch": 0.7499813696996795,
        "step": 10064
    },
    {
        "loss": 1.686,
        "grad_norm": 3.2546048164367676,
        "learning_rate": 0.00017118177713403566,
        "epoch": 0.7500558909009614,
        "step": 10065
    },
    {
        "loss": 2.1534,
        "grad_norm": 3.6251912117004395,
        "learning_rate": 0.00017113233718820108,
        "epoch": 0.7501304121022431,
        "step": 10066
    },
    {
        "loss": 2.5868,
        "grad_norm": 2.530848264694214,
        "learning_rate": 0.0001710828620224173,
        "epoch": 0.7502049333035249,
        "step": 10067
    },
    {
        "loss": 2.5161,
        "grad_norm": 2.448673725128174,
        "learning_rate": 0.000171033351661181,
        "epoch": 0.7502794545048066,
        "step": 10068
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.761885643005371,
        "learning_rate": 0.00017098380612900633,
        "epoch": 0.7503539757060884,
        "step": 10069
    },
    {
        "loss": 2.7976,
        "grad_norm": 2.7903072834014893,
        "learning_rate": 0.00017093422545042507,
        "epoch": 0.7504284969073701,
        "step": 10070
    },
    {
        "loss": 2.6241,
        "grad_norm": 2.765498161315918,
        "learning_rate": 0.000170884609649986,
        "epoch": 0.7505030181086519,
        "step": 10071
    },
    {
        "loss": 2.3064,
        "grad_norm": 2.6114234924316406,
        "learning_rate": 0.00017083495875225578,
        "epoch": 0.7505775393099337,
        "step": 10072
    },
    {
        "loss": 1.9199,
        "grad_norm": 2.5288875102996826,
        "learning_rate": 0.00017078527278181811,
        "epoch": 0.7506520605112155,
        "step": 10073
    },
    {
        "loss": 2.6883,
        "grad_norm": 5.615849018096924,
        "learning_rate": 0.0001707355517632741,
        "epoch": 0.7507265817124972,
        "step": 10074
    },
    {
        "loss": 2.8123,
        "grad_norm": 2.1170737743377686,
        "learning_rate": 0.00017068579572124214,
        "epoch": 0.750801102913779,
        "step": 10075
    },
    {
        "loss": 2.397,
        "grad_norm": 4.053129196166992,
        "learning_rate": 0.00017063600468035834,
        "epoch": 0.7508756241150607,
        "step": 10076
    },
    {
        "loss": 2.5913,
        "grad_norm": 2.9668891429901123,
        "learning_rate": 0.00017058617866527569,
        "epoch": 0.7509501453163425,
        "step": 10077
    },
    {
        "loss": 2.647,
        "grad_norm": 2.5280237197875977,
        "learning_rate": 0.00017053631770066476,
        "epoch": 0.7510246665176242,
        "step": 10078
    },
    {
        "loss": 2.8747,
        "grad_norm": 2.3539788722991943,
        "learning_rate": 0.0001704864218112132,
        "epoch": 0.751099187718906,
        "step": 10079
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.9275052547454834,
        "learning_rate": 0.00017043649102162632,
        "epoch": 0.7511737089201878,
        "step": 10080
    },
    {
        "loss": 2.5019,
        "grad_norm": 2.0058014392852783,
        "learning_rate": 0.0001703865253566263,
        "epoch": 0.7512482301214696,
        "step": 10081
    },
    {
        "loss": 1.7135,
        "grad_norm": 4.924320220947266,
        "learning_rate": 0.00017033652484095291,
        "epoch": 0.7513227513227513,
        "step": 10082
    },
    {
        "loss": 2.6071,
        "grad_norm": 2.6689343452453613,
        "learning_rate": 0.00017028648949936304,
        "epoch": 0.7513972725240331,
        "step": 10083
    },
    {
        "loss": 3.0975,
        "grad_norm": 2.5433616638183594,
        "learning_rate": 0.00017023641935663063,
        "epoch": 0.7514717937253148,
        "step": 10084
    },
    {
        "loss": 2.6528,
        "grad_norm": 2.012676954269409,
        "learning_rate": 0.00017018631443754744,
        "epoch": 0.7515463149265966,
        "step": 10085
    },
    {
        "loss": 2.0369,
        "grad_norm": 2.1279218196868896,
        "learning_rate": 0.0001701361747669216,
        "epoch": 0.7516208361278783,
        "step": 10086
    },
    {
        "loss": 2.4116,
        "grad_norm": 2.7656307220458984,
        "learning_rate": 0.0001700860003695792,
        "epoch": 0.7516953573291602,
        "step": 10087
    },
    {
        "loss": 2.2461,
        "grad_norm": 3.231821060180664,
        "learning_rate": 0.00017003579127036314,
        "epoch": 0.7517698785304419,
        "step": 10088
    },
    {
        "loss": 2.2267,
        "grad_norm": 3.8627562522888184,
        "learning_rate": 0.0001699855474941335,
        "epoch": 0.7518443997317237,
        "step": 10089
    },
    {
        "loss": 2.19,
        "grad_norm": 2.688526153564453,
        "learning_rate": 0.00016993526906576779,
        "epoch": 0.7519189209330054,
        "step": 10090
    },
    {
        "loss": 2.457,
        "grad_norm": 4.337137222290039,
        "learning_rate": 0.00016988495601016044,
        "epoch": 0.7519934421342872,
        "step": 10091
    },
    {
        "loss": 2.4946,
        "grad_norm": 3.004192590713501,
        "learning_rate": 0.00016983460835222295,
        "epoch": 0.752067963335569,
        "step": 10092
    },
    {
        "loss": 3.2615,
        "grad_norm": 2.1438040733337402,
        "learning_rate": 0.0001697842261168843,
        "epoch": 0.7521424845368507,
        "step": 10093
    },
    {
        "loss": 2.3817,
        "grad_norm": 2.3827807903289795,
        "learning_rate": 0.00016973380932909038,
        "epoch": 0.7522170057381325,
        "step": 10094
    },
    {
        "loss": 2.4523,
        "grad_norm": 3.974026918411255,
        "learning_rate": 0.00016968335801380397,
        "epoch": 0.7522915269394143,
        "step": 10095
    },
    {
        "loss": 2.2599,
        "grad_norm": 3.0789599418640137,
        "learning_rate": 0.00016963287219600551,
        "epoch": 0.7523660481406961,
        "step": 10096
    },
    {
        "loss": 2.8242,
        "grad_norm": 2.6302335262298584,
        "learning_rate": 0.0001695823519006918,
        "epoch": 0.7524405693419778,
        "step": 10097
    },
    {
        "loss": 2.3029,
        "grad_norm": 2.9568724632263184,
        "learning_rate": 0.00016953179715287737,
        "epoch": 0.7525150905432596,
        "step": 10098
    },
    {
        "loss": 0.9214,
        "grad_norm": 3.1027541160583496,
        "learning_rate": 0.00016948120797759332,
        "epoch": 0.7525896117445413,
        "step": 10099
    },
    {
        "loss": 1.7091,
        "grad_norm": 1.8003021478652954,
        "learning_rate": 0.00016943058439988827,
        "epoch": 0.7526641329458231,
        "step": 10100
    },
    {
        "loss": 1.7576,
        "grad_norm": 2.950680732727051,
        "learning_rate": 0.00016937992644482742,
        "epoch": 0.7527386541471048,
        "step": 10101
    },
    {
        "loss": 2.1598,
        "grad_norm": 4.177206039428711,
        "learning_rate": 0.00016932923413749316,
        "epoch": 0.7528131753483867,
        "step": 10102
    },
    {
        "loss": 1.7263,
        "grad_norm": 4.315560817718506,
        "learning_rate": 0.00016927850750298506,
        "epoch": 0.7528876965496684,
        "step": 10103
    },
    {
        "loss": 2.3152,
        "grad_norm": 1.8211380243301392,
        "learning_rate": 0.00016922774656641937,
        "epoch": 0.7529622177509502,
        "step": 10104
    },
    {
        "loss": 2.4842,
        "grad_norm": 3.9173715114593506,
        "learning_rate": 0.00016917695135292978,
        "epoch": 0.7530367389522319,
        "step": 10105
    },
    {
        "loss": 2.0507,
        "grad_norm": 3.5694634914398193,
        "learning_rate": 0.00016912612188766627,
        "epoch": 0.7531112601535137,
        "step": 10106
    },
    {
        "loss": 2.3751,
        "grad_norm": 3.127588987350464,
        "learning_rate": 0.00016907525819579648,
        "epoch": 0.7531857813547954,
        "step": 10107
    },
    {
        "loss": 2.1736,
        "grad_norm": 4.203052520751953,
        "learning_rate": 0.00016902436030250456,
        "epoch": 0.7532603025560772,
        "step": 10108
    },
    {
        "loss": 2.6322,
        "grad_norm": 3.2092039585113525,
        "learning_rate": 0.0001689734282329916,
        "epoch": 0.7533348237573589,
        "step": 10109
    },
    {
        "loss": 2.2215,
        "grad_norm": 4.203188419342041,
        "learning_rate": 0.00016892246201247603,
        "epoch": 0.7534093449586408,
        "step": 10110
    },
    {
        "loss": 1.6569,
        "grad_norm": 3.6104133129119873,
        "learning_rate": 0.0001688714616661927,
        "epoch": 0.7534838661599225,
        "step": 10111
    },
    {
        "loss": 2.373,
        "grad_norm": 3.7522449493408203,
        "learning_rate": 0.00016882042721939354,
        "epoch": 0.7535583873612043,
        "step": 10112
    },
    {
        "loss": 2.9784,
        "grad_norm": 3.735852003097534,
        "learning_rate": 0.00016876935869734747,
        "epoch": 0.753632908562486,
        "step": 10113
    },
    {
        "loss": 2.9949,
        "grad_norm": 1.9460692405700684,
        "learning_rate": 0.00016871825612534017,
        "epoch": 0.7537074297637678,
        "step": 10114
    },
    {
        "loss": 2.5652,
        "grad_norm": 2.101825475692749,
        "learning_rate": 0.00016866711952867407,
        "epoch": 0.7537819509650495,
        "step": 10115
    },
    {
        "loss": 2.236,
        "grad_norm": 3.0122039318084717,
        "learning_rate": 0.00016861594893266885,
        "epoch": 0.7538564721663313,
        "step": 10116
    },
    {
        "loss": 2.7031,
        "grad_norm": 3.3341665267944336,
        "learning_rate": 0.0001685647443626604,
        "epoch": 0.753930993367613,
        "step": 10117
    },
    {
        "loss": 1.8556,
        "grad_norm": 2.864595890045166,
        "learning_rate": 0.00016851350584400204,
        "epoch": 0.7540055145688949,
        "step": 10118
    },
    {
        "loss": 2.2996,
        "grad_norm": 4.945544242858887,
        "learning_rate": 0.00016846223340206344,
        "epoch": 0.7540800357701766,
        "step": 10119
    },
    {
        "loss": 2.5702,
        "grad_norm": 3.993258476257324,
        "learning_rate": 0.0001684109270622315,
        "epoch": 0.7541545569714584,
        "step": 10120
    },
    {
        "loss": 2.579,
        "grad_norm": 2.2572691440582275,
        "learning_rate": 0.00016835958684990955,
        "epoch": 0.7542290781727401,
        "step": 10121
    },
    {
        "loss": 2.8011,
        "grad_norm": 2.547192335128784,
        "learning_rate": 0.00016830821279051764,
        "epoch": 0.7543035993740219,
        "step": 10122
    },
    {
        "loss": 2.4033,
        "grad_norm": 2.8055508136749268,
        "learning_rate": 0.00016825680490949307,
        "epoch": 0.7543781205753036,
        "step": 10123
    },
    {
        "loss": 2.6367,
        "grad_norm": 3.6744673252105713,
        "learning_rate": 0.00016820536323228937,
        "epoch": 0.7544526417765854,
        "step": 10124
    },
    {
        "loss": 2.4649,
        "grad_norm": 2.497840642929077,
        "learning_rate": 0.00016815388778437703,
        "epoch": 0.7545271629778671,
        "step": 10125
    },
    {
        "loss": 1.8176,
        "grad_norm": 2.354804277420044,
        "learning_rate": 0.00016810237859124318,
        "epoch": 0.754601684179149,
        "step": 10126
    },
    {
        "loss": 2.69,
        "grad_norm": 2.2228238582611084,
        "learning_rate": 0.00016805083567839182,
        "epoch": 0.7546762053804308,
        "step": 10127
    },
    {
        "loss": 2.5982,
        "grad_norm": 3.1119186878204346,
        "learning_rate": 0.0001679992590713435,
        "epoch": 0.7547507265817125,
        "step": 10128
    },
    {
        "loss": 2.3992,
        "grad_norm": 3.1590917110443115,
        "learning_rate": 0.00016794764879563546,
        "epoch": 0.7548252477829943,
        "step": 10129
    },
    {
        "loss": 2.2102,
        "grad_norm": 3.1609835624694824,
        "learning_rate": 0.00016789600487682153,
        "epoch": 0.754899768984276,
        "step": 10130
    },
    {
        "loss": 1.5936,
        "grad_norm": 3.165395736694336,
        "learning_rate": 0.00016784432734047252,
        "epoch": 0.7549742901855578,
        "step": 10131
    },
    {
        "loss": 1.8694,
        "grad_norm": 3.8062939643859863,
        "learning_rate": 0.0001677926162121755,
        "epoch": 0.7550488113868395,
        "step": 10132
    },
    {
        "loss": 2.7839,
        "grad_norm": 2.9779317378997803,
        "learning_rate": 0.00016774087151753448,
        "epoch": 0.7551233325881214,
        "step": 10133
    },
    {
        "loss": 2.3247,
        "grad_norm": 3.666252374649048,
        "learning_rate": 0.00016768909328216988,
        "epoch": 0.7551978537894031,
        "step": 10134
    },
    {
        "loss": 2.261,
        "grad_norm": 3.146496295928955,
        "learning_rate": 0.00016763728153171872,
        "epoch": 0.7552723749906849,
        "step": 10135
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.358790159225464,
        "learning_rate": 0.00016758543629183495,
        "epoch": 0.7553468961919666,
        "step": 10136
    },
    {
        "loss": 2.4773,
        "grad_norm": 3.403592348098755,
        "learning_rate": 0.0001675335575881885,
        "epoch": 0.7554214173932484,
        "step": 10137
    },
    {
        "loss": 1.9022,
        "grad_norm": 3.2780299186706543,
        "learning_rate": 0.0001674816454464665,
        "epoch": 0.7554959385945301,
        "step": 10138
    },
    {
        "loss": 1.7867,
        "grad_norm": 2.8966329097747803,
        "learning_rate": 0.0001674296998923722,
        "epoch": 0.7555704597958119,
        "step": 10139
    },
    {
        "loss": 2.0213,
        "grad_norm": 2.5255157947540283,
        "learning_rate": 0.0001673777209516255,
        "epoch": 0.7556449809970937,
        "step": 10140
    },
    {
        "loss": 2.2856,
        "grad_norm": 2.8813211917877197,
        "learning_rate": 0.00016732570864996305,
        "epoch": 0.7557195021983755,
        "step": 10141
    },
    {
        "loss": 2.2748,
        "grad_norm": 3.6753768920898438,
        "learning_rate": 0.00016727366301313765,
        "epoch": 0.7557940233996572,
        "step": 10142
    },
    {
        "loss": 2.7016,
        "grad_norm": 2.6844699382781982,
        "learning_rate": 0.000167221584066919,
        "epoch": 0.755868544600939,
        "step": 10143
    },
    {
        "loss": 3.0057,
        "grad_norm": 2.5880675315856934,
        "learning_rate": 0.000167169471837093,
        "epoch": 0.7559430658022207,
        "step": 10144
    },
    {
        "loss": 2.2136,
        "grad_norm": 3.2366063594818115,
        "learning_rate": 0.00016711732634946212,
        "epoch": 0.7560175870035025,
        "step": 10145
    },
    {
        "loss": 2.3656,
        "grad_norm": 3.1048355102539062,
        "learning_rate": 0.00016706514762984518,
        "epoch": 0.7560921082047842,
        "step": 10146
    },
    {
        "loss": 2.5084,
        "grad_norm": 3.2513997554779053,
        "learning_rate": 0.0001670129357040778,
        "epoch": 0.756166629406066,
        "step": 10147
    },
    {
        "loss": 2.5138,
        "grad_norm": 3.514092206954956,
        "learning_rate": 0.0001669606905980117,
        "epoch": 0.7562411506073478,
        "step": 10148
    },
    {
        "loss": 1.4341,
        "grad_norm": 1.4144024848937988,
        "learning_rate": 0.00016690841233751513,
        "epoch": 0.7563156718086296,
        "step": 10149
    },
    {
        "loss": 2.0926,
        "grad_norm": 4.262031555175781,
        "learning_rate": 0.00016685610094847267,
        "epoch": 0.7563901930099113,
        "step": 10150
    },
    {
        "loss": 2.7312,
        "grad_norm": 2.7103071212768555,
        "learning_rate": 0.00016680375645678565,
        "epoch": 0.7564647142111931,
        "step": 10151
    },
    {
        "loss": 2.4151,
        "grad_norm": 3.1845390796661377,
        "learning_rate": 0.00016675137888837126,
        "epoch": 0.7565392354124748,
        "step": 10152
    },
    {
        "loss": 2.5034,
        "grad_norm": 3.0125327110290527,
        "learning_rate": 0.00016669896826916361,
        "epoch": 0.7566137566137566,
        "step": 10153
    },
    {
        "loss": 2.007,
        "grad_norm": 3.379913330078125,
        "learning_rate": 0.0001666465246251128,
        "epoch": 0.7566882778150383,
        "step": 10154
    },
    {
        "loss": 2.0913,
        "grad_norm": 3.5313327312469482,
        "learning_rate": 0.0001665940479821853,
        "epoch": 0.7567627990163202,
        "step": 10155
    },
    {
        "loss": 2.1901,
        "grad_norm": 3.841698408126831,
        "learning_rate": 0.0001665415383663643,
        "epoch": 0.7568373202176019,
        "step": 10156
    },
    {
        "loss": 2.1969,
        "grad_norm": 3.033428430557251,
        "learning_rate": 0.00016648899580364862,
        "epoch": 0.7569118414188837,
        "step": 10157
    },
    {
        "loss": 1.8244,
        "grad_norm": 3.322704553604126,
        "learning_rate": 0.00016643642032005413,
        "epoch": 0.7569863626201654,
        "step": 10158
    },
    {
        "loss": 2.778,
        "grad_norm": 2.4019582271575928,
        "learning_rate": 0.00016638381194161256,
        "epoch": 0.7570608838214472,
        "step": 10159
    },
    {
        "loss": 2.4324,
        "grad_norm": 3.781632423400879,
        "learning_rate": 0.00016633117069437197,
        "epoch": 0.7571354050227289,
        "step": 10160
    },
    {
        "loss": 1.2836,
        "grad_norm": 3.4446322917938232,
        "learning_rate": 0.0001662784966043969,
        "epoch": 0.7572099262240107,
        "step": 10161
    },
    {
        "loss": 2.0166,
        "grad_norm": 3.0528299808502197,
        "learning_rate": 0.000166225789697768,
        "epoch": 0.7572844474252926,
        "step": 10162
    },
    {
        "loss": 1.9499,
        "grad_norm": 2.7379398345947266,
        "learning_rate": 0.000166173050000582,
        "epoch": 0.7573589686265743,
        "step": 10163
    },
    {
        "loss": 2.6204,
        "grad_norm": 2.1989293098449707,
        "learning_rate": 0.00016612027753895228,
        "epoch": 0.7574334898278561,
        "step": 10164
    },
    {
        "loss": 2.9952,
        "grad_norm": 3.617496967315674,
        "learning_rate": 0.00016606747233900818,
        "epoch": 0.7575080110291378,
        "step": 10165
    },
    {
        "loss": 2.5362,
        "grad_norm": 2.404431104660034,
        "learning_rate": 0.00016601463442689508,
        "epoch": 0.7575825322304196,
        "step": 10166
    },
    {
        "loss": 2.6903,
        "grad_norm": 3.669106960296631,
        "learning_rate": 0.00016596176382877517,
        "epoch": 0.7576570534317013,
        "step": 10167
    },
    {
        "loss": 2.152,
        "grad_norm": 4.058840751647949,
        "learning_rate": 0.00016590886057082587,
        "epoch": 0.7577315746329831,
        "step": 10168
    },
    {
        "loss": 2.5438,
        "grad_norm": 2.4115476608276367,
        "learning_rate": 0.00016585592467924175,
        "epoch": 0.7578060958342648,
        "step": 10169
    },
    {
        "loss": 1.3671,
        "grad_norm": 3.2649123668670654,
        "learning_rate": 0.00016580295618023286,
        "epoch": 0.7578806170355467,
        "step": 10170
    },
    {
        "loss": 2.6033,
        "grad_norm": 2.1815714836120605,
        "learning_rate": 0.00016574995510002582,
        "epoch": 0.7579551382368284,
        "step": 10171
    },
    {
        "loss": 2.4697,
        "grad_norm": 4.046411514282227,
        "learning_rate": 0.0001656969214648631,
        "epoch": 0.7580296594381102,
        "step": 10172
    },
    {
        "loss": 2.2117,
        "grad_norm": 2.8057641983032227,
        "learning_rate": 0.0001656438553010033,
        "epoch": 0.7581041806393919,
        "step": 10173
    },
    {
        "loss": 2.7845,
        "grad_norm": 2.6714069843292236,
        "learning_rate": 0.00016559075663472142,
        "epoch": 0.7581787018406737,
        "step": 10174
    },
    {
        "loss": 2.806,
        "grad_norm": 2.924919843673706,
        "learning_rate": 0.00016553762549230815,
        "epoch": 0.7582532230419554,
        "step": 10175
    },
    {
        "loss": 2.9531,
        "grad_norm": 2.4301750659942627,
        "learning_rate": 0.00016548446190007077,
        "epoch": 0.7583277442432372,
        "step": 10176
    },
    {
        "loss": 1.9322,
        "grad_norm": 2.9129064083099365,
        "learning_rate": 0.00016543126588433187,
        "epoch": 0.7584022654445189,
        "step": 10177
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.5106499195098877,
        "learning_rate": 0.00016537803747143087,
        "epoch": 0.7584767866458008,
        "step": 10178
    },
    {
        "loss": 1.891,
        "grad_norm": 3.922870635986328,
        "learning_rate": 0.0001653247766877228,
        "epoch": 0.7585513078470825,
        "step": 10179
    },
    {
        "loss": 2.4277,
        "grad_norm": 2.955873727798462,
        "learning_rate": 0.0001652714835595787,
        "epoch": 0.7586258290483643,
        "step": 10180
    },
    {
        "loss": 2.6538,
        "grad_norm": 3.2940728664398193,
        "learning_rate": 0.00016521815811338598,
        "epoch": 0.758700350249646,
        "step": 10181
    },
    {
        "loss": 1.9114,
        "grad_norm": 3.855206251144409,
        "learning_rate": 0.0001651648003755477,
        "epoch": 0.7587748714509278,
        "step": 10182
    },
    {
        "loss": 1.7485,
        "grad_norm": 3.7934019565582275,
        "learning_rate": 0.0001651114103724829,
        "epoch": 0.7588493926522095,
        "step": 10183
    },
    {
        "loss": 1.9834,
        "grad_norm": 3.792682647705078,
        "learning_rate": 0.0001650579881306269,
        "epoch": 0.7589239138534913,
        "step": 10184
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.8608808517456055,
        "learning_rate": 0.0001650045336764308,
        "epoch": 0.758998435054773,
        "step": 10185
    },
    {
        "loss": 2.3458,
        "grad_norm": 3.6467671394348145,
        "learning_rate": 0.00016495104703636145,
        "epoch": 0.7590729562560549,
        "step": 10186
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.5653462409973145,
        "learning_rate": 0.00016489752823690217,
        "epoch": 0.7591474774573366,
        "step": 10187
    },
    {
        "loss": 1.8595,
        "grad_norm": 3.659656047821045,
        "learning_rate": 0.00016484397730455146,
        "epoch": 0.7592219986586184,
        "step": 10188
    },
    {
        "loss": 2.3827,
        "grad_norm": 3.7290163040161133,
        "learning_rate": 0.00016479039426582447,
        "epoch": 0.7592965198599001,
        "step": 10189
    },
    {
        "loss": 2.982,
        "grad_norm": 2.2862706184387207,
        "learning_rate": 0.0001647367791472518,
        "epoch": 0.7593710410611819,
        "step": 10190
    },
    {
        "loss": 2.2611,
        "grad_norm": 3.6914784908294678,
        "learning_rate": 0.00016468313197537997,
        "epoch": 0.7594455622624636,
        "step": 10191
    },
    {
        "loss": 2.5944,
        "grad_norm": 2.798349618911743,
        "learning_rate": 0.0001646294527767716,
        "epoch": 0.7595200834637454,
        "step": 10192
    },
    {
        "loss": 2.1655,
        "grad_norm": 3.8703975677490234,
        "learning_rate": 0.00016457574157800487,
        "epoch": 0.7595946046650272,
        "step": 10193
    },
    {
        "loss": 2.9028,
        "grad_norm": 2.9506916999816895,
        "learning_rate": 0.0001645219984056741,
        "epoch": 0.759669125866309,
        "step": 10194
    },
    {
        "loss": 2.8446,
        "grad_norm": 2.398216485977173,
        "learning_rate": 0.00016446822328638927,
        "epoch": 0.7597436470675907,
        "step": 10195
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.253521680831909,
        "learning_rate": 0.00016441441624677616,
        "epoch": 0.7598181682688725,
        "step": 10196
    },
    {
        "loss": 2.5194,
        "grad_norm": 2.350393533706665,
        "learning_rate": 0.00016436057731347627,
        "epoch": 0.7598926894701542,
        "step": 10197
    },
    {
        "loss": 2.9063,
        "grad_norm": 2.982118606567383,
        "learning_rate": 0.00016430670651314727,
        "epoch": 0.759967210671436,
        "step": 10198
    },
    {
        "loss": 2.2954,
        "grad_norm": 3.8646085262298584,
        "learning_rate": 0.00016425280387246223,
        "epoch": 0.7600417318727178,
        "step": 10199
    },
    {
        "loss": 2.8147,
        "grad_norm": 2.938174247741699,
        "learning_rate": 0.0001641988694181101,
        "epoch": 0.7601162530739995,
        "step": 10200
    },
    {
        "loss": 2.4725,
        "grad_norm": 4.493297576904297,
        "learning_rate": 0.00016414490317679552,
        "epoch": 0.7601907742752814,
        "step": 10201
    },
    {
        "loss": 2.0057,
        "grad_norm": 2.7898077964782715,
        "learning_rate": 0.00016409090517523913,
        "epoch": 0.7602652954765631,
        "step": 10202
    },
    {
        "loss": 2.8476,
        "grad_norm": 4.028269290924072,
        "learning_rate": 0.0001640368754401769,
        "epoch": 0.7603398166778449,
        "step": 10203
    },
    {
        "loss": 1.2421,
        "grad_norm": 4.7261643409729,
        "learning_rate": 0.00016398281399836092,
        "epoch": 0.7604143378791266,
        "step": 10204
    },
    {
        "loss": 1.9982,
        "grad_norm": 4.172596454620361,
        "learning_rate": 0.00016392872087655876,
        "epoch": 0.7604888590804084,
        "step": 10205
    },
    {
        "loss": 2.2129,
        "grad_norm": 3.8161604404449463,
        "learning_rate": 0.00016387459610155348,
        "epoch": 0.7605633802816901,
        "step": 10206
    },
    {
        "loss": 2.3606,
        "grad_norm": 6.363405227661133,
        "learning_rate": 0.0001638204397001444,
        "epoch": 0.760637901482972,
        "step": 10207
    },
    {
        "loss": 1.922,
        "grad_norm": 3.887521982192993,
        "learning_rate": 0.0001637662516991457,
        "epoch": 0.7607124226842537,
        "step": 10208
    },
    {
        "loss": 1.4463,
        "grad_norm": 2.47694993019104,
        "learning_rate": 0.0001637120321253879,
        "epoch": 0.7607869438855355,
        "step": 10209
    },
    {
        "loss": 2.9306,
        "grad_norm": 3.3356218338012695,
        "learning_rate": 0.00016365778100571685,
        "epoch": 0.7608614650868172,
        "step": 10210
    },
    {
        "loss": 1.6919,
        "grad_norm": 3.1269798278808594,
        "learning_rate": 0.00016360349836699393,
        "epoch": 0.760935986288099,
        "step": 10211
    },
    {
        "loss": 2.4012,
        "grad_norm": 3.1606528759002686,
        "learning_rate": 0.00016354918423609646,
        "epoch": 0.7610105074893807,
        "step": 10212
    },
    {
        "loss": 2.6186,
        "grad_norm": 4.275054931640625,
        "learning_rate": 0.00016349483863991693,
        "epoch": 0.7610850286906625,
        "step": 10213
    },
    {
        "loss": 2.6765,
        "grad_norm": 3.1674089431762695,
        "learning_rate": 0.00016344046160536384,
        "epoch": 0.7611595498919442,
        "step": 10214
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.595123767852783,
        "learning_rate": 0.00016338605315936093,
        "epoch": 0.761234071093226,
        "step": 10215
    },
    {
        "loss": 2.6097,
        "grad_norm": 4.097794055938721,
        "learning_rate": 0.0001633316133288476,
        "epoch": 0.7613085922945078,
        "step": 10216
    },
    {
        "loss": 2.4915,
        "grad_norm": 3.004821300506592,
        "learning_rate": 0.0001632771421407787,
        "epoch": 0.7613831134957896,
        "step": 10217
    },
    {
        "loss": 2.5093,
        "grad_norm": 2.2407610416412354,
        "learning_rate": 0.00016322263962212502,
        "epoch": 0.7614576346970713,
        "step": 10218
    },
    {
        "loss": 1.8457,
        "grad_norm": 3.382596015930176,
        "learning_rate": 0.00016316810579987208,
        "epoch": 0.7615321558983531,
        "step": 10219
    },
    {
        "loss": 2.6854,
        "grad_norm": 5.273184299468994,
        "learning_rate": 0.0001631135407010217,
        "epoch": 0.7616066770996348,
        "step": 10220
    },
    {
        "loss": 2.4051,
        "grad_norm": 3.900256872177124,
        "learning_rate": 0.00016305894435259064,
        "epoch": 0.7616811983009166,
        "step": 10221
    },
    {
        "loss": 2.2473,
        "grad_norm": 3.557826519012451,
        "learning_rate": 0.0001630043167816116,
        "epoch": 0.7617557195021983,
        "step": 10222
    },
    {
        "loss": 2.5391,
        "grad_norm": 3.0892934799194336,
        "learning_rate": 0.00016294965801513228,
        "epoch": 0.7618302407034802,
        "step": 10223
    },
    {
        "loss": 1.8179,
        "grad_norm": 3.193741798400879,
        "learning_rate": 0.00016289496808021597,
        "epoch": 0.7619047619047619,
        "step": 10224
    },
    {
        "loss": 1.5819,
        "grad_norm": 2.534703254699707,
        "learning_rate": 0.00016284024700394164,
        "epoch": 0.7619792831060437,
        "step": 10225
    },
    {
        "loss": 2.403,
        "grad_norm": 2.95068621635437,
        "learning_rate": 0.00016278549481340334,
        "epoch": 0.7620538043073254,
        "step": 10226
    },
    {
        "loss": 2.7035,
        "grad_norm": 3.3807733058929443,
        "learning_rate": 0.0001627307115357109,
        "epoch": 0.7621283255086072,
        "step": 10227
    },
    {
        "loss": 2.74,
        "grad_norm": 2.5295121669769287,
        "learning_rate": 0.0001626758971979889,
        "epoch": 0.7622028467098889,
        "step": 10228
    },
    {
        "loss": 2.186,
        "grad_norm": 3.3947699069976807,
        "learning_rate": 0.0001626210518273781,
        "epoch": 0.7622773679111707,
        "step": 10229
    },
    {
        "loss": 2.199,
        "grad_norm": 2.996856689453125,
        "learning_rate": 0.0001625661754510341,
        "epoch": 0.7623518891124524,
        "step": 10230
    },
    {
        "loss": 3.22,
        "grad_norm": 3.2002511024475098,
        "learning_rate": 0.00016251126809612785,
        "epoch": 0.7624264103137343,
        "step": 10231
    },
    {
        "loss": 2.4951,
        "grad_norm": 3.3114123344421387,
        "learning_rate": 0.00016245632978984602,
        "epoch": 0.762500931515016,
        "step": 10232
    },
    {
        "loss": 2.5695,
        "grad_norm": 2.499122381210327,
        "learning_rate": 0.0001624013605593903,
        "epoch": 0.7625754527162978,
        "step": 10233
    },
    {
        "loss": 1.9251,
        "grad_norm": 4.367404937744141,
        "learning_rate": 0.00016234636043197754,
        "epoch": 0.7626499739175796,
        "step": 10234
    },
    {
        "loss": 2.3386,
        "grad_norm": 3.5821590423583984,
        "learning_rate": 0.00016229132943484045,
        "epoch": 0.7627244951188613,
        "step": 10235
    },
    {
        "loss": 2.7275,
        "grad_norm": 3.371886730194092,
        "learning_rate": 0.00016223626759522648,
        "epoch": 0.7627990163201431,
        "step": 10236
    },
    {
        "loss": 2.3479,
        "grad_norm": 2.3690531253814697,
        "learning_rate": 0.00016218117494039844,
        "epoch": 0.7628735375214248,
        "step": 10237
    },
    {
        "loss": 2.9724,
        "grad_norm": 1.444602608680725,
        "learning_rate": 0.0001621260514976349,
        "epoch": 0.7629480587227067,
        "step": 10238
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.3981502056121826,
        "learning_rate": 0.00016207089729422877,
        "epoch": 0.7630225799239884,
        "step": 10239
    },
    {
        "loss": 2.5471,
        "grad_norm": 2.4508237838745117,
        "learning_rate": 0.0001620157123574891,
        "epoch": 0.7630971011252702,
        "step": 10240
    },
    {
        "loss": 2.9241,
        "grad_norm": 1.943496823310852,
        "learning_rate": 0.0001619604967147395,
        "epoch": 0.7631716223265519,
        "step": 10241
    },
    {
        "loss": 1.2278,
        "grad_norm": 3.4962942600250244,
        "learning_rate": 0.00016190525039331925,
        "epoch": 0.7632461435278337,
        "step": 10242
    },
    {
        "loss": 2.724,
        "grad_norm": 2.4345178604125977,
        "learning_rate": 0.00016184997342058252,
        "epoch": 0.7633206647291154,
        "step": 10243
    },
    {
        "loss": 2.7511,
        "grad_norm": 2.4446215629577637,
        "learning_rate": 0.00016179466582389866,
        "epoch": 0.7633951859303972,
        "step": 10244
    },
    {
        "loss": 1.8814,
        "grad_norm": 3.294144868850708,
        "learning_rate": 0.00016173932763065247,
        "epoch": 0.7634697071316789,
        "step": 10245
    },
    {
        "loss": 2.9578,
        "grad_norm": 3.2843875885009766,
        "learning_rate": 0.00016168395886824346,
        "epoch": 0.7635442283329608,
        "step": 10246
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.3408331871032715,
        "learning_rate": 0.0001616285595640869,
        "epoch": 0.7636187495342425,
        "step": 10247
    },
    {
        "loss": 2.3753,
        "grad_norm": 4.049685001373291,
        "learning_rate": 0.00016157312974561232,
        "epoch": 0.7636932707355243,
        "step": 10248
    },
    {
        "loss": 2.4693,
        "grad_norm": 3.643613815307617,
        "learning_rate": 0.00016151766944026518,
        "epoch": 0.763767791936806,
        "step": 10249
    },
    {
        "loss": 2.2279,
        "grad_norm": 4.24308443069458,
        "learning_rate": 0.0001614621786755056,
        "epoch": 0.7638423131380878,
        "step": 10250
    },
    {
        "loss": 2.3201,
        "grad_norm": 3.4029297828674316,
        "learning_rate": 0.00016140665747880882,
        "epoch": 0.7639168343393695,
        "step": 10251
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.8590173721313477,
        "learning_rate": 0.00016135110587766515,
        "epoch": 0.7639913555406513,
        "step": 10252
    },
    {
        "loss": 2.5715,
        "grad_norm": 2.632200241088867,
        "learning_rate": 0.0001612955238995802,
        "epoch": 0.764065876741933,
        "step": 10253
    },
    {
        "loss": 3.3331,
        "grad_norm": 3.2945504188537598,
        "learning_rate": 0.00016123991157207423,
        "epoch": 0.7641403979432149,
        "step": 10254
    },
    {
        "loss": 2.6512,
        "grad_norm": 2.179471492767334,
        "learning_rate": 0.00016118426892268288,
        "epoch": 0.7642149191444966,
        "step": 10255
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.200719118118286,
        "learning_rate": 0.0001611285959789566,
        "epoch": 0.7642894403457784,
        "step": 10256
    },
    {
        "loss": 2.3809,
        "grad_norm": 2.9166457653045654,
        "learning_rate": 0.00016107289276846083,
        "epoch": 0.7643639615470601,
        "step": 10257
    },
    {
        "loss": 2.453,
        "grad_norm": 3.1919801235198975,
        "learning_rate": 0.00016101715931877622,
        "epoch": 0.7644384827483419,
        "step": 10258
    },
    {
        "loss": 2.4415,
        "grad_norm": 3.8000576496124268,
        "learning_rate": 0.00016096139565749797,
        "epoch": 0.7645130039496236,
        "step": 10259
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.35872220993042,
        "learning_rate": 0.00016090560181223666,
        "epoch": 0.7645875251509054,
        "step": 10260
    },
    {
        "loss": 2.3456,
        "grad_norm": 3.1687240600585938,
        "learning_rate": 0.00016084977781061766,
        "epoch": 0.7646620463521872,
        "step": 10261
    },
    {
        "loss": 2.6225,
        "grad_norm": 4.351795196533203,
        "learning_rate": 0.0001607939236802811,
        "epoch": 0.764736567553469,
        "step": 10262
    },
    {
        "loss": 2.4318,
        "grad_norm": 2.5958101749420166,
        "learning_rate": 0.00016073803944888243,
        "epoch": 0.7648110887547507,
        "step": 10263
    },
    {
        "loss": 1.8332,
        "grad_norm": 3.1957530975341797,
        "learning_rate": 0.00016068212514409148,
        "epoch": 0.7648856099560325,
        "step": 10264
    },
    {
        "loss": 1.7676,
        "grad_norm": 3.254011869430542,
        "learning_rate": 0.00016062618079359356,
        "epoch": 0.7649601311573142,
        "step": 10265
    },
    {
        "loss": 1.6806,
        "grad_norm": 4.100320339202881,
        "learning_rate": 0.00016057020642508838,
        "epoch": 0.765034652358596,
        "step": 10266
    },
    {
        "loss": 2.3806,
        "grad_norm": 2.8581905364990234,
        "learning_rate": 0.00016051420206629073,
        "epoch": 0.7651091735598777,
        "step": 10267
    },
    {
        "loss": 2.0884,
        "grad_norm": 3.496119737625122,
        "learning_rate": 0.00016045816774493005,
        "epoch": 0.7651836947611595,
        "step": 10268
    },
    {
        "loss": 2.4553,
        "grad_norm": 3.2720329761505127,
        "learning_rate": 0.00016040210348875102,
        "epoch": 0.7652582159624414,
        "step": 10269
    },
    {
        "loss": 2.1569,
        "grad_norm": 2.075364828109741,
        "learning_rate": 0.00016034600932551273,
        "epoch": 0.7653327371637231,
        "step": 10270
    },
    {
        "loss": 1.7062,
        "grad_norm": 3.1755762100219727,
        "learning_rate": 0.0001602898852829893,
        "epoch": 0.7654072583650049,
        "step": 10271
    },
    {
        "loss": 2.6792,
        "grad_norm": 3.5264642238616943,
        "learning_rate": 0.00016023373138896943,
        "epoch": 0.7654817795662866,
        "step": 10272
    },
    {
        "loss": 2.3342,
        "grad_norm": 3.159754514694214,
        "learning_rate": 0.00016017754767125696,
        "epoch": 0.7655563007675684,
        "step": 10273
    },
    {
        "loss": 2.5914,
        "grad_norm": 3.5981638431549072,
        "learning_rate": 0.00016012133415767012,
        "epoch": 0.7656308219688501,
        "step": 10274
    },
    {
        "loss": 2.5565,
        "grad_norm": 2.434856653213501,
        "learning_rate": 0.00016006509087604224,
        "epoch": 0.765705343170132,
        "step": 10275
    },
    {
        "loss": 2.1072,
        "grad_norm": 3.8593926429748535,
        "learning_rate": 0.00016000881785422112,
        "epoch": 0.7657798643714137,
        "step": 10276
    },
    {
        "loss": 2.6782,
        "grad_norm": 2.379856824874878,
        "learning_rate": 0.00015995251512006923,
        "epoch": 0.7658543855726955,
        "step": 10277
    },
    {
        "loss": 2.3993,
        "grad_norm": 4.456056594848633,
        "learning_rate": 0.00015989618270146428,
        "epoch": 0.7659289067739772,
        "step": 10278
    },
    {
        "loss": 2.5362,
        "grad_norm": 3.0275402069091797,
        "learning_rate": 0.00015983982062629784,
        "epoch": 0.766003427975259,
        "step": 10279
    },
    {
        "loss": 2.0668,
        "grad_norm": 3.004192352294922,
        "learning_rate": 0.00015978342892247692,
        "epoch": 0.7660779491765407,
        "step": 10280
    },
    {
        "loss": 1.9617,
        "grad_norm": 3.2245476245880127,
        "learning_rate": 0.00015972700761792284,
        "epoch": 0.7661524703778225,
        "step": 10281
    },
    {
        "loss": 1.9623,
        "grad_norm": 4.267205715179443,
        "learning_rate": 0.00015967055674057144,
        "epoch": 0.7662269915791042,
        "step": 10282
    },
    {
        "loss": 2.244,
        "grad_norm": 4.193050861358643,
        "learning_rate": 0.0001596140763183737,
        "epoch": 0.766301512780386,
        "step": 10283
    },
    {
        "loss": 2.1763,
        "grad_norm": 3.5516610145568848,
        "learning_rate": 0.00015955756637929479,
        "epoch": 0.7663760339816678,
        "step": 10284
    },
    {
        "loss": 2.5358,
        "grad_norm": 2.2825701236724854,
        "learning_rate": 0.00015950102695131451,
        "epoch": 0.7664505551829496,
        "step": 10285
    },
    {
        "loss": 1.544,
        "grad_norm": 4.2609686851501465,
        "learning_rate": 0.0001594444580624276,
        "epoch": 0.7665250763842313,
        "step": 10286
    },
    {
        "loss": 2.8015,
        "grad_norm": 3.202859401702881,
        "learning_rate": 0.00015938785974064308,
        "epoch": 0.7665995975855131,
        "step": 10287
    },
    {
        "loss": 1.8563,
        "grad_norm": 3.985217809677124,
        "learning_rate": 0.0001593312320139845,
        "epoch": 0.7666741187867948,
        "step": 10288
    },
    {
        "loss": 2.0752,
        "grad_norm": 2.4830944538116455,
        "learning_rate": 0.00015927457491049046,
        "epoch": 0.7667486399880766,
        "step": 10289
    },
    {
        "loss": 2.2537,
        "grad_norm": 2.6682779788970947,
        "learning_rate": 0.0001592178884582133,
        "epoch": 0.7668231611893583,
        "step": 10290
    },
    {
        "loss": 2.3286,
        "grad_norm": 3.7366228103637695,
        "learning_rate": 0.00015916117268522065,
        "epoch": 0.7668976823906402,
        "step": 10291
    },
    {
        "loss": 2.4486,
        "grad_norm": 2.4914398193359375,
        "learning_rate": 0.00015910442761959414,
        "epoch": 0.7669722035919219,
        "step": 10292
    },
    {
        "loss": 2.4609,
        "grad_norm": 3.071272611618042,
        "learning_rate": 0.0001590476532894303,
        "epoch": 0.7670467247932037,
        "step": 10293
    },
    {
        "loss": 2.3041,
        "grad_norm": 2.737685203552246,
        "learning_rate": 0.00015899084972283988,
        "epoch": 0.7671212459944854,
        "step": 10294
    },
    {
        "loss": 2.9841,
        "grad_norm": 2.4278111457824707,
        "learning_rate": 0.00015893401694794812,
        "epoch": 0.7671957671957672,
        "step": 10295
    },
    {
        "loss": 1.6658,
        "grad_norm": 4.022950649261475,
        "learning_rate": 0.0001588771549928949,
        "epoch": 0.7672702883970489,
        "step": 10296
    },
    {
        "loss": 2.4321,
        "grad_norm": 3.3247764110565186,
        "learning_rate": 0.0001588202638858343,
        "epoch": 0.7673448095983307,
        "step": 10297
    },
    {
        "loss": 2.9408,
        "grad_norm": 2.4136524200439453,
        "learning_rate": 0.00015876334365493532,
        "epoch": 0.7674193307996124,
        "step": 10298
    },
    {
        "loss": 2.6213,
        "grad_norm": 3.036221742630005,
        "learning_rate": 0.00015870639432838046,
        "epoch": 0.7674938520008943,
        "step": 10299
    },
    {
        "loss": 2.7049,
        "grad_norm": 2.4636008739471436,
        "learning_rate": 0.00015864941593436767,
        "epoch": 0.767568373202176,
        "step": 10300
    },
    {
        "loss": 1.3206,
        "grad_norm": 4.145970344543457,
        "learning_rate": 0.0001585924085011086,
        "epoch": 0.7676428944034578,
        "step": 10301
    },
    {
        "loss": 2.9499,
        "grad_norm": 2.2160398960113525,
        "learning_rate": 0.00015853537205682948,
        "epoch": 0.7677174156047395,
        "step": 10302
    },
    {
        "loss": 1.6584,
        "grad_norm": 5.468585968017578,
        "learning_rate": 0.0001584783066297711,
        "epoch": 0.7677919368060213,
        "step": 10303
    },
    {
        "loss": 1.7893,
        "grad_norm": 2.508509635925293,
        "learning_rate": 0.0001584212122481883,
        "epoch": 0.7678664580073031,
        "step": 10304
    },
    {
        "loss": 1.5153,
        "grad_norm": 3.162503719329834,
        "learning_rate": 0.00015836408894035026,
        "epoch": 0.7679409792085848,
        "step": 10305
    },
    {
        "loss": 2.5171,
        "grad_norm": 2.6111485958099365,
        "learning_rate": 0.00015830693673454083,
        "epoch": 0.7680155004098667,
        "step": 10306
    },
    {
        "loss": 2.4711,
        "grad_norm": 3.972318410873413,
        "learning_rate": 0.00015824975565905785,
        "epoch": 0.7680900216111484,
        "step": 10307
    },
    {
        "loss": 2.1554,
        "grad_norm": 3.4835972785949707,
        "learning_rate": 0.00015819254574221344,
        "epoch": 0.7681645428124302,
        "step": 10308
    },
    {
        "loss": 2.7829,
        "grad_norm": 3.9283125400543213,
        "learning_rate": 0.00015813530701233442,
        "epoch": 0.7682390640137119,
        "step": 10309
    },
    {
        "loss": 2.6411,
        "grad_norm": 2.191948175430298,
        "learning_rate": 0.00015807803949776107,
        "epoch": 0.7683135852149937,
        "step": 10310
    },
    {
        "loss": 2.1684,
        "grad_norm": 3.4712400436401367,
        "learning_rate": 0.00015802074322684878,
        "epoch": 0.7683881064162754,
        "step": 10311
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.7180233001708984,
        "learning_rate": 0.00015796341822796675,
        "epoch": 0.7684626276175572,
        "step": 10312
    },
    {
        "loss": 2.6587,
        "grad_norm": 3.1916933059692383,
        "learning_rate": 0.00015790606452949828,
        "epoch": 0.7685371488188389,
        "step": 10313
    },
    {
        "loss": 1.7035,
        "grad_norm": 2.297588586807251,
        "learning_rate": 0.00015784868215984132,
        "epoch": 0.7686116700201208,
        "step": 10314
    },
    {
        "loss": 2.5264,
        "grad_norm": 3.254300832748413,
        "learning_rate": 0.0001577912711474075,
        "epoch": 0.7686861912214025,
        "step": 10315
    },
    {
        "loss": 2.695,
        "grad_norm": 2.9839422702789307,
        "learning_rate": 0.00015773383152062318,
        "epoch": 0.7687607124226843,
        "step": 10316
    },
    {
        "loss": 2.1687,
        "grad_norm": 4.061895847320557,
        "learning_rate": 0.00015767636330792843,
        "epoch": 0.768835233623966,
        "step": 10317
    },
    {
        "loss": 2.934,
        "grad_norm": 2.92744517326355,
        "learning_rate": 0.00015761886653777767,
        "epoch": 0.7689097548252478,
        "step": 10318
    },
    {
        "loss": 2.7938,
        "grad_norm": 2.4614598751068115,
        "learning_rate": 0.00015756134123863933,
        "epoch": 0.7689842760265295,
        "step": 10319
    },
    {
        "loss": 2.4305,
        "grad_norm": 2.834345817565918,
        "learning_rate": 0.00015750378743899628,
        "epoch": 0.7690587972278113,
        "step": 10320
    },
    {
        "loss": 2.2703,
        "grad_norm": 2.9250237941741943,
        "learning_rate": 0.00015744620516734516,
        "epoch": 0.769133318429093,
        "step": 10321
    },
    {
        "loss": 2.5661,
        "grad_norm": 3.3211629390716553,
        "learning_rate": 0.00015738859445219686,
        "epoch": 0.7692078396303749,
        "step": 10322
    },
    {
        "loss": 2.7395,
        "grad_norm": 2.370624542236328,
        "learning_rate": 0.00015733095532207623,
        "epoch": 0.7692823608316566,
        "step": 10323
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.2654013633728027,
        "learning_rate": 0.0001572732878055225,
        "epoch": 0.7693568820329384,
        "step": 10324
    },
    {
        "loss": 2.3174,
        "grad_norm": 4.685652732849121,
        "learning_rate": 0.00015721559193108853,
        "epoch": 0.7694314032342201,
        "step": 10325
    },
    {
        "loss": 2.6542,
        "grad_norm": 3.6511807441711426,
        "learning_rate": 0.00015715786772734167,
        "epoch": 0.7695059244355019,
        "step": 10326
    },
    {
        "loss": 2.1454,
        "grad_norm": 3.2740061283111572,
        "learning_rate": 0.00015710011522286293,
        "epoch": 0.7695804456367836,
        "step": 10327
    },
    {
        "loss": 2.582,
        "grad_norm": 3.2122573852539062,
        "learning_rate": 0.0001570423344462473,
        "epoch": 0.7696549668380654,
        "step": 10328
    },
    {
        "loss": 2.3286,
        "grad_norm": 2.329378366470337,
        "learning_rate": 0.0001569845254261044,
        "epoch": 0.7697294880393472,
        "step": 10329
    },
    {
        "loss": 2.4145,
        "grad_norm": 3.4467270374298096,
        "learning_rate": 0.00015692668819105682,
        "epoch": 0.769804009240629,
        "step": 10330
    },
    {
        "loss": 2.6139,
        "grad_norm": 2.297191619873047,
        "learning_rate": 0.00015686882276974204,
        "epoch": 0.7698785304419107,
        "step": 10331
    },
    {
        "loss": 2.2698,
        "grad_norm": 4.186034679412842,
        "learning_rate": 0.00015681092919081103,
        "epoch": 0.7699530516431925,
        "step": 10332
    },
    {
        "loss": 2.3537,
        "grad_norm": 1.6367237567901611,
        "learning_rate": 0.00015675300748292865,
        "epoch": 0.7700275728444742,
        "step": 10333
    },
    {
        "loss": 0.6039,
        "grad_norm": 3.638423442840576,
        "learning_rate": 0.00015669505767477407,
        "epoch": 0.770102094045756,
        "step": 10334
    },
    {
        "loss": 1.7972,
        "grad_norm": 2.184849977493286,
        "learning_rate": 0.00015663707979503993,
        "epoch": 0.7701766152470377,
        "step": 10335
    },
    {
        "loss": 2.2576,
        "grad_norm": 3.8947525024414062,
        "learning_rate": 0.00015657907387243313,
        "epoch": 0.7702511364483196,
        "step": 10336
    },
    {
        "loss": 2.1948,
        "grad_norm": 3.391737461090088,
        "learning_rate": 0.00015652103993567428,
        "epoch": 0.7703256576496013,
        "step": 10337
    },
    {
        "loss": 2.3876,
        "grad_norm": 4.47622537612915,
        "learning_rate": 0.00015646297801349789,
        "epoch": 0.7704001788508831,
        "step": 10338
    },
    {
        "loss": 2.1476,
        "grad_norm": 3.5670485496520996,
        "learning_rate": 0.0001564048881346521,
        "epoch": 0.7704747000521649,
        "step": 10339
    },
    {
        "loss": 2.6469,
        "grad_norm": 2.5255699157714844,
        "learning_rate": 0.00015634677032789945,
        "epoch": 0.7705492212534466,
        "step": 10340
    },
    {
        "loss": 2.6111,
        "grad_norm": 3.0931482315063477,
        "learning_rate": 0.00015628862462201576,
        "epoch": 0.7706237424547284,
        "step": 10341
    },
    {
        "loss": 2.0011,
        "grad_norm": 3.7800331115722656,
        "learning_rate": 0.00015623045104579095,
        "epoch": 0.7706982636560101,
        "step": 10342
    },
    {
        "loss": 2.8447,
        "grad_norm": 3.3271636962890625,
        "learning_rate": 0.00015617224962802853,
        "epoch": 0.770772784857292,
        "step": 10343
    },
    {
        "loss": 2.1419,
        "grad_norm": 2.3238906860351562,
        "learning_rate": 0.00015611402039754612,
        "epoch": 0.7708473060585737,
        "step": 10344
    },
    {
        "loss": 2.3194,
        "grad_norm": 2.3450679779052734,
        "learning_rate": 0.0001560557633831748,
        "epoch": 0.7709218272598555,
        "step": 10345
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.4674878120422363,
        "learning_rate": 0.0001559974786137595,
        "epoch": 0.7709963484611372,
        "step": 10346
    },
    {
        "loss": 2.038,
        "grad_norm": 3.5433425903320312,
        "learning_rate": 0.00015593916611815906,
        "epoch": 0.771070869662419,
        "step": 10347
    },
    {
        "loss": 2.6217,
        "grad_norm": 2.8977444171905518,
        "learning_rate": 0.0001558808259252457,
        "epoch": 0.7711453908637007,
        "step": 10348
    },
    {
        "loss": 2.803,
        "grad_norm": 3.056011199951172,
        "learning_rate": 0.0001558224580639059,
        "epoch": 0.7712199120649825,
        "step": 10349
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.9554574489593506,
        "learning_rate": 0.00015576406256303908,
        "epoch": 0.7712944332662642,
        "step": 10350
    },
    {
        "loss": 2.7,
        "grad_norm": 3.2723567485809326,
        "learning_rate": 0.00015570563945155907,
        "epoch": 0.771368954467546,
        "step": 10351
    },
    {
        "loss": 2.7011,
        "grad_norm": 2.783926248550415,
        "learning_rate": 0.00015564718875839292,
        "epoch": 0.7714434756688278,
        "step": 10352
    },
    {
        "loss": 2.6713,
        "grad_norm": 3.39920711517334,
        "learning_rate": 0.00015558871051248144,
        "epoch": 0.7715179968701096,
        "step": 10353
    },
    {
        "loss": 2.5955,
        "grad_norm": 3.7332265377044678,
        "learning_rate": 0.00015553020474277931,
        "epoch": 0.7715925180713913,
        "step": 10354
    },
    {
        "loss": 1.6071,
        "grad_norm": 2.4501636028289795,
        "learning_rate": 0.00015547167147825453,
        "epoch": 0.7716670392726731,
        "step": 10355
    },
    {
        "loss": 2.0227,
        "grad_norm": 3.6962223052978516,
        "learning_rate": 0.00015541311074788875,
        "epoch": 0.7717415604739548,
        "step": 10356
    },
    {
        "loss": 1.1799,
        "grad_norm": 4.4755988121032715,
        "learning_rate": 0.0001553545225806775,
        "epoch": 0.7718160816752366,
        "step": 10357
    },
    {
        "loss": 2.2951,
        "grad_norm": 3.5720760822296143,
        "learning_rate": 0.00015529590700562963,
        "epoch": 0.7718906028765183,
        "step": 10358
    },
    {
        "loss": 1.9163,
        "grad_norm": 3.9936234951019287,
        "learning_rate": 0.0001552372640517675,
        "epoch": 0.7719651240778002,
        "step": 10359
    },
    {
        "loss": 2.4818,
        "grad_norm": 3.511129856109619,
        "learning_rate": 0.00015517859374812747,
        "epoch": 0.7720396452790819,
        "step": 10360
    },
    {
        "loss": 2.2297,
        "grad_norm": 4.213140487670898,
        "learning_rate": 0.00015511989612375872,
        "epoch": 0.7721141664803637,
        "step": 10361
    },
    {
        "loss": 2.7189,
        "grad_norm": 4.12669563293457,
        "learning_rate": 0.00015506117120772466,
        "epoch": 0.7721886876816454,
        "step": 10362
    },
    {
        "loss": 2.1542,
        "grad_norm": 3.664829730987549,
        "learning_rate": 0.00015500241902910168,
        "epoch": 0.7722632088829272,
        "step": 10363
    },
    {
        "loss": 2.647,
        "grad_norm": 1.9968516826629639,
        "learning_rate": 0.00015494363961698025,
        "epoch": 0.7723377300842089,
        "step": 10364
    },
    {
        "loss": 2.2639,
        "grad_norm": 3.052229166030884,
        "learning_rate": 0.00015488483300046374,
        "epoch": 0.7724122512854907,
        "step": 10365
    },
    {
        "loss": 1.6874,
        "grad_norm": 4.3796281814575195,
        "learning_rate": 0.00015482599920866913,
        "epoch": 0.7724867724867724,
        "step": 10366
    },
    {
        "loss": 2.5679,
        "grad_norm": 3.2346835136413574,
        "learning_rate": 0.00015476713827072727,
        "epoch": 0.7725612936880543,
        "step": 10367
    },
    {
        "loss": 2.5911,
        "grad_norm": 3.116851329803467,
        "learning_rate": 0.0001547082502157818,
        "epoch": 0.772635814889336,
        "step": 10368
    },
    {
        "loss": 2.0682,
        "grad_norm": 3.5397286415100098,
        "learning_rate": 0.0001546493350729906,
        "epoch": 0.7727103360906178,
        "step": 10369
    },
    {
        "loss": 2.3044,
        "grad_norm": 3.750378131866455,
        "learning_rate": 0.0001545903928715239,
        "epoch": 0.7727848572918995,
        "step": 10370
    },
    {
        "loss": 2.4372,
        "grad_norm": 2.2058353424072266,
        "learning_rate": 0.00015453142364056635,
        "epoch": 0.7728593784931813,
        "step": 10371
    },
    {
        "loss": 2.7943,
        "grad_norm": 3.349777936935425,
        "learning_rate": 0.00015447242740931536,
        "epoch": 0.772933899694463,
        "step": 10372
    },
    {
        "loss": 2.2484,
        "grad_norm": 2.710009813308716,
        "learning_rate": 0.00015441340420698193,
        "epoch": 0.7730084208957448,
        "step": 10373
    },
    {
        "loss": 2.8458,
        "grad_norm": 2.006765365600586,
        "learning_rate": 0.00015435435406279026,
        "epoch": 0.7730829420970265,
        "step": 10374
    },
    {
        "loss": 2.6602,
        "grad_norm": 2.518043041229248,
        "learning_rate": 0.00015429527700597828,
        "epoch": 0.7731574632983084,
        "step": 10375
    },
    {
        "loss": 2.0445,
        "grad_norm": 2.378556728363037,
        "learning_rate": 0.00015423617306579667,
        "epoch": 0.7732319844995902,
        "step": 10376
    },
    {
        "loss": 2.2708,
        "grad_norm": 3.1800954341888428,
        "learning_rate": 0.00015417704227151002,
        "epoch": 0.7733065057008719,
        "step": 10377
    },
    {
        "loss": 1.8086,
        "grad_norm": 2.6232171058654785,
        "learning_rate": 0.00015411788465239577,
        "epoch": 0.7733810269021537,
        "step": 10378
    },
    {
        "loss": 2.2767,
        "grad_norm": 2.813114881515503,
        "learning_rate": 0.00015405870023774468,
        "epoch": 0.7734555481034354,
        "step": 10379
    },
    {
        "loss": 2.1445,
        "grad_norm": 1.8474618196487427,
        "learning_rate": 0.00015399948905686127,
        "epoch": 0.7735300693047172,
        "step": 10380
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.828115224838257,
        "learning_rate": 0.00015394025113906246,
        "epoch": 0.773604590505999,
        "step": 10381
    },
    {
        "loss": 1.6926,
        "grad_norm": 3.014676332473755,
        "learning_rate": 0.0001538809865136792,
        "epoch": 0.7736791117072808,
        "step": 10382
    },
    {
        "loss": 2.2346,
        "grad_norm": 2.917841672897339,
        "learning_rate": 0.00015382169521005524,
        "epoch": 0.7737536329085625,
        "step": 10383
    },
    {
        "loss": 1.8385,
        "grad_norm": 3.628251314163208,
        "learning_rate": 0.0001537623772575476,
        "epoch": 0.7738281541098443,
        "step": 10384
    },
    {
        "loss": 2.634,
        "grad_norm": 2.3127541542053223,
        "learning_rate": 0.00015370303268552666,
        "epoch": 0.773902675311126,
        "step": 10385
    },
    {
        "loss": 2.7307,
        "grad_norm": 4.395522117614746,
        "learning_rate": 0.0001536436615233757,
        "epoch": 0.7739771965124078,
        "step": 10386
    },
    {
        "loss": 1.5423,
        "grad_norm": 2.7512426376342773,
        "learning_rate": 0.00015358426380049153,
        "epoch": 0.7740517177136895,
        "step": 10387
    },
    {
        "loss": 1.7851,
        "grad_norm": 3.1682159900665283,
        "learning_rate": 0.00015352483954628383,
        "epoch": 0.7741262389149713,
        "step": 10388
    },
    {
        "loss": 2.7268,
        "grad_norm": 2.529059886932373,
        "learning_rate": 0.00015346538879017544,
        "epoch": 0.774200760116253,
        "step": 10389
    },
    {
        "loss": 3.1578,
        "grad_norm": 4.057423114776611,
        "learning_rate": 0.0001534059115616023,
        "epoch": 0.7742752813175349,
        "step": 10390
    },
    {
        "loss": 2.1294,
        "grad_norm": 3.8093810081481934,
        "learning_rate": 0.00015334640789001376,
        "epoch": 0.7743498025188166,
        "step": 10391
    },
    {
        "loss": 2.9504,
        "grad_norm": 3.7636563777923584,
        "learning_rate": 0.0001532868778048719,
        "epoch": 0.7744243237200984,
        "step": 10392
    },
    {
        "loss": 2.8687,
        "grad_norm": 2.0860488414764404,
        "learning_rate": 0.00015322732133565205,
        "epoch": 0.7744988449213801,
        "step": 10393
    },
    {
        "loss": 2.1982,
        "grad_norm": 2.7444629669189453,
        "learning_rate": 0.0001531677385118424,
        "epoch": 0.7745733661226619,
        "step": 10394
    },
    {
        "loss": 2.7322,
        "grad_norm": 4.211162567138672,
        "learning_rate": 0.00015310812936294464,
        "epoch": 0.7746478873239436,
        "step": 10395
    },
    {
        "loss": 2.1582,
        "grad_norm": 3.305539131164551,
        "learning_rate": 0.00015304849391847296,
        "epoch": 0.7747224085252254,
        "step": 10396
    },
    {
        "loss": 2.4144,
        "grad_norm": 3.584977388381958,
        "learning_rate": 0.00015298883220795503,
        "epoch": 0.7747969297265072,
        "step": 10397
    },
    {
        "loss": 1.7989,
        "grad_norm": 3.987504482269287,
        "learning_rate": 0.00015292914426093124,
        "epoch": 0.774871450927789,
        "step": 10398
    },
    {
        "loss": 2.6657,
        "grad_norm": 2.851379156112671,
        "learning_rate": 0.0001528694301069549,
        "epoch": 0.7749459721290707,
        "step": 10399
    },
    {
        "loss": 1.6302,
        "grad_norm": 4.48239803314209,
        "learning_rate": 0.0001528096897755928,
        "epoch": 0.7750204933303525,
        "step": 10400
    },
    {
        "loss": 2.4706,
        "grad_norm": 3.5929837226867676,
        "learning_rate": 0.0001527499232964239,
        "epoch": 0.7750950145316342,
        "step": 10401
    },
    {
        "loss": 1.8271,
        "grad_norm": 4.135869979858398,
        "learning_rate": 0.00015269013069904084,
        "epoch": 0.775169535732916,
        "step": 10402
    },
    {
        "loss": 2.3857,
        "grad_norm": 3.439166307449341,
        "learning_rate": 0.0001526303120130488,
        "epoch": 0.7752440569341977,
        "step": 10403
    },
    {
        "loss": 2.221,
        "grad_norm": 4.088442325592041,
        "learning_rate": 0.00015257046726806586,
        "epoch": 0.7753185781354796,
        "step": 10404
    },
    {
        "loss": 2.9128,
        "grad_norm": 2.94321608543396,
        "learning_rate": 0.00015251059649372333,
        "epoch": 0.7753930993367613,
        "step": 10405
    },
    {
        "loss": 2.0352,
        "grad_norm": 3.8850464820861816,
        "learning_rate": 0.00015245069971966507,
        "epoch": 0.7754676205380431,
        "step": 10406
    },
    {
        "loss": 1.6277,
        "grad_norm": 1.978204607963562,
        "learning_rate": 0.00015239077697554783,
        "epoch": 0.7755421417393248,
        "step": 10407
    },
    {
        "loss": 2.016,
        "grad_norm": 2.3951468467712402,
        "learning_rate": 0.0001523308282910416,
        "epoch": 0.7756166629406066,
        "step": 10408
    },
    {
        "loss": 2.2798,
        "grad_norm": 3.8138883113861084,
        "learning_rate": 0.00015227085369582877,
        "epoch": 0.7756911841418883,
        "step": 10409
    },
    {
        "loss": 2.0609,
        "grad_norm": 3.236514091491699,
        "learning_rate": 0.00015221085321960457,
        "epoch": 0.7757657053431701,
        "step": 10410
    },
    {
        "loss": 2.3403,
        "grad_norm": 4.371750354766846,
        "learning_rate": 0.00015215082689207764,
        "epoch": 0.775840226544452,
        "step": 10411
    },
    {
        "loss": 2.8997,
        "grad_norm": 2.0956015586853027,
        "learning_rate": 0.00015209077474296842,
        "epoch": 0.7759147477457337,
        "step": 10412
    },
    {
        "loss": 2.6033,
        "grad_norm": 2.7663381099700928,
        "learning_rate": 0.00015203069680201113,
        "epoch": 0.7759892689470155,
        "step": 10413
    },
    {
        "loss": 2.6173,
        "grad_norm": 3.121535301208496,
        "learning_rate": 0.00015197059309895208,
        "epoch": 0.7760637901482972,
        "step": 10414
    },
    {
        "loss": 1.549,
        "grad_norm": 3.9730024337768555,
        "learning_rate": 0.00015191046366355075,
        "epoch": 0.776138311349579,
        "step": 10415
    },
    {
        "loss": 2.3395,
        "grad_norm": 4.663444519042969,
        "learning_rate": 0.00015185030852557914,
        "epoch": 0.7762128325508607,
        "step": 10416
    },
    {
        "loss": 2.7414,
        "grad_norm": 1.6091059446334839,
        "learning_rate": 0.00015179012771482188,
        "epoch": 0.7762873537521425,
        "step": 10417
    },
    {
        "loss": 2.7319,
        "grad_norm": 2.441098690032959,
        "learning_rate": 0.00015172992126107673,
        "epoch": 0.7763618749534242,
        "step": 10418
    },
    {
        "loss": 2.1962,
        "grad_norm": 3.2992496490478516,
        "learning_rate": 0.00015166968919415357,
        "epoch": 0.776436396154706,
        "step": 10419
    },
    {
        "loss": 2.8342,
        "grad_norm": 2.2596142292022705,
        "learning_rate": 0.00015160943154387567,
        "epoch": 0.7765109173559878,
        "step": 10420
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.08431339263916,
        "learning_rate": 0.0001515491483400781,
        "epoch": 0.7765854385572696,
        "step": 10421
    },
    {
        "loss": 1.8831,
        "grad_norm": 3.2446951866149902,
        "learning_rate": 0.00015148883961260942,
        "epoch": 0.7766599597585513,
        "step": 10422
    },
    {
        "loss": 2.5069,
        "grad_norm": 3.755930185317993,
        "learning_rate": 0.0001514285053913303,
        "epoch": 0.7767344809598331,
        "step": 10423
    },
    {
        "loss": 2.3143,
        "grad_norm": 3.006404399871826,
        "learning_rate": 0.00015136814570611406,
        "epoch": 0.7768090021611148,
        "step": 10424
    },
    {
        "loss": 2.6735,
        "grad_norm": 2.782594919204712,
        "learning_rate": 0.00015130776058684705,
        "epoch": 0.7768835233623966,
        "step": 10425
    },
    {
        "loss": 1.922,
        "grad_norm": 3.523698329925537,
        "learning_rate": 0.00015124735006342773,
        "epoch": 0.7769580445636783,
        "step": 10426
    },
    {
        "loss": 2.8645,
        "grad_norm": 2.1287612915039062,
        "learning_rate": 0.00015118691416576727,
        "epoch": 0.7770325657649602,
        "step": 10427
    },
    {
        "loss": 1.7414,
        "grad_norm": 3.8529770374298096,
        "learning_rate": 0.00015112645292378964,
        "epoch": 0.7771070869662419,
        "step": 10428
    },
    {
        "loss": 2.861,
        "grad_norm": 3.4584295749664307,
        "learning_rate": 0.00015106596636743115,
        "epoch": 0.7771816081675237,
        "step": 10429
    },
    {
        "loss": 1.9689,
        "grad_norm": 3.2972989082336426,
        "learning_rate": 0.00015100545452664048,
        "epoch": 0.7772561293688054,
        "step": 10430
    },
    {
        "loss": 2.779,
        "grad_norm": 2.6768457889556885,
        "learning_rate": 0.0001509449174313794,
        "epoch": 0.7773306505700872,
        "step": 10431
    },
    {
        "loss": 1.4518,
        "grad_norm": 4.0903000831604,
        "learning_rate": 0.00015088435511162134,
        "epoch": 0.7774051717713689,
        "step": 10432
    },
    {
        "loss": 2.3361,
        "grad_norm": 1.529430866241455,
        "learning_rate": 0.000150823767597353,
        "epoch": 0.7774796929726507,
        "step": 10433
    },
    {
        "loss": 2.5459,
        "grad_norm": 2.394181489944458,
        "learning_rate": 0.00015076315491857308,
        "epoch": 0.7775542141739324,
        "step": 10434
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.8156152963638306,
        "learning_rate": 0.00015070251710529308,
        "epoch": 0.7776287353752143,
        "step": 10435
    },
    {
        "loss": 2.1825,
        "grad_norm": 2.9950053691864014,
        "learning_rate": 0.0001506418541875367,
        "epoch": 0.777703256576496,
        "step": 10436
    },
    {
        "loss": 2.21,
        "grad_norm": 3.9033432006835938,
        "learning_rate": 0.00015058116619533996,
        "epoch": 0.7777777777777778,
        "step": 10437
    },
    {
        "loss": 1.9662,
        "grad_norm": 2.4994089603424072,
        "learning_rate": 0.00015052045315875175,
        "epoch": 0.7778522989790595,
        "step": 10438
    },
    {
        "loss": 2.1343,
        "grad_norm": 3.4944937229156494,
        "learning_rate": 0.00015045971510783298,
        "epoch": 0.7779268201803413,
        "step": 10439
    },
    {
        "loss": 1.5722,
        "grad_norm": 3.3592004776000977,
        "learning_rate": 0.000150398952072657,
        "epoch": 0.778001341381623,
        "step": 10440
    },
    {
        "loss": 2.9911,
        "grad_norm": 2.1263630390167236,
        "learning_rate": 0.00015033816408330946,
        "epoch": 0.7780758625829048,
        "step": 10441
    },
    {
        "loss": 3.0757,
        "grad_norm": 3.536243438720703,
        "learning_rate": 0.00015027735116988878,
        "epoch": 0.7781503837841866,
        "step": 10442
    },
    {
        "loss": 3.3172,
        "grad_norm": 3.848268985748291,
        "learning_rate": 0.00015021651336250526,
        "epoch": 0.7782249049854684,
        "step": 10443
    },
    {
        "loss": 2.1152,
        "grad_norm": 3.4239003658294678,
        "learning_rate": 0.0001501556506912817,
        "epoch": 0.7782994261867501,
        "step": 10444
    },
    {
        "loss": 1.776,
        "grad_norm": 3.316319704055786,
        "learning_rate": 0.00015009476318635304,
        "epoch": 0.7783739473880319,
        "step": 10445
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.4619858264923096,
        "learning_rate": 0.00015003385087786696,
        "epoch": 0.7784484685893137,
        "step": 10446
    },
    {
        "loss": 2.6219,
        "grad_norm": 3.6839470863342285,
        "learning_rate": 0.00014997291379598293,
        "epoch": 0.7785229897905954,
        "step": 10447
    },
    {
        "loss": 2.1189,
        "grad_norm": 3.72162127494812,
        "learning_rate": 0.0001499119519708731,
        "epoch": 0.7785975109918772,
        "step": 10448
    },
    {
        "loss": 2.5249,
        "grad_norm": 3.2715723514556885,
        "learning_rate": 0.00014985096543272154,
        "epoch": 0.778672032193159,
        "step": 10449
    },
    {
        "loss": 2.5317,
        "grad_norm": 2.3958778381347656,
        "learning_rate": 0.00014978995421172458,
        "epoch": 0.7787465533944408,
        "step": 10450
    },
    {
        "loss": 2.5896,
        "grad_norm": 3.668175220489502,
        "learning_rate": 0.0001497289183380912,
        "epoch": 0.7788210745957225,
        "step": 10451
    },
    {
        "loss": 2.9069,
        "grad_norm": 2.4410805702209473,
        "learning_rate": 0.0001496678578420418,
        "epoch": 0.7788955957970043,
        "step": 10452
    },
    {
        "loss": 2.8223,
        "grad_norm": 3.2893574237823486,
        "learning_rate": 0.0001496067727538098,
        "epoch": 0.778970116998286,
        "step": 10453
    },
    {
        "loss": 2.1089,
        "grad_norm": 4.17601203918457,
        "learning_rate": 0.00014954566310364028,
        "epoch": 0.7790446381995678,
        "step": 10454
    },
    {
        "loss": 2.5844,
        "grad_norm": 3.5570859909057617,
        "learning_rate": 0.00014948452892179056,
        "epoch": 0.7791191594008495,
        "step": 10455
    },
    {
        "loss": 2.7143,
        "grad_norm": 4.968973159790039,
        "learning_rate": 0.00014942337023853033,
        "epoch": 0.7791936806021313,
        "step": 10456
    },
    {
        "loss": 2.574,
        "grad_norm": 3.3347771167755127,
        "learning_rate": 0.00014936218708414112,
        "epoch": 0.779268201803413,
        "step": 10457
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.792844533920288,
        "learning_rate": 0.00014930097948891687,
        "epoch": 0.7793427230046949,
        "step": 10458
    },
    {
        "loss": 3.003,
        "grad_norm": 2.3545923233032227,
        "learning_rate": 0.00014923974748316338,
        "epoch": 0.7794172442059766,
        "step": 10459
    },
    {
        "loss": 2.8657,
        "grad_norm": 2.5532078742980957,
        "learning_rate": 0.00014917849109719863,
        "epoch": 0.7794917654072584,
        "step": 10460
    },
    {
        "loss": 2.5635,
        "grad_norm": 2.827575206756592,
        "learning_rate": 0.00014911721036135254,
        "epoch": 0.7795662866085401,
        "step": 10461
    },
    {
        "loss": 1.6402,
        "grad_norm": 2.118715524673462,
        "learning_rate": 0.00014905590530596741,
        "epoch": 0.7796408078098219,
        "step": 10462
    },
    {
        "loss": 2.4946,
        "grad_norm": 2.6627397537231445,
        "learning_rate": 0.0001489945759613973,
        "epoch": 0.7797153290111036,
        "step": 10463
    },
    {
        "loss": 2.8423,
        "grad_norm": 2.5953168869018555,
        "learning_rate": 0.0001489332223580084,
        "epoch": 0.7797898502123854,
        "step": 10464
    },
    {
        "loss": 2.4304,
        "grad_norm": 3.933055877685547,
        "learning_rate": 0.0001488718445261787,
        "epoch": 0.7798643714136672,
        "step": 10465
    },
    {
        "loss": 3.1584,
        "grad_norm": 2.732966423034668,
        "learning_rate": 0.00014881044249629864,
        "epoch": 0.779938892614949,
        "step": 10466
    },
    {
        "loss": 2.4101,
        "grad_norm": 2.2581024169921875,
        "learning_rate": 0.0001487490162987703,
        "epoch": 0.7800134138162307,
        "step": 10467
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.3865153789520264,
        "learning_rate": 0.00014868756596400761,
        "epoch": 0.7800879350175125,
        "step": 10468
    },
    {
        "loss": 2.1204,
        "grad_norm": 2.430582046508789,
        "learning_rate": 0.00014862609152243694,
        "epoch": 0.7801624562187942,
        "step": 10469
    },
    {
        "loss": 2.0472,
        "grad_norm": 3.522096633911133,
        "learning_rate": 0.00014856459300449602,
        "epoch": 0.780236977420076,
        "step": 10470
    },
    {
        "loss": 2.6614,
        "grad_norm": 4.190732479095459,
        "learning_rate": 0.00014850307044063517,
        "epoch": 0.7803114986213577,
        "step": 10471
    },
    {
        "loss": 2.027,
        "grad_norm": 3.0236949920654297,
        "learning_rate": 0.00014844152386131573,
        "epoch": 0.7803860198226396,
        "step": 10472
    },
    {
        "loss": 2.3455,
        "grad_norm": 3.910546064376831,
        "learning_rate": 0.00014837995329701177,
        "epoch": 0.7804605410239213,
        "step": 10473
    },
    {
        "loss": 2.4145,
        "grad_norm": 2.0240492820739746,
        "learning_rate": 0.00014831835877820884,
        "epoch": 0.7805350622252031,
        "step": 10474
    },
    {
        "loss": 2.4018,
        "grad_norm": 3.932079315185547,
        "learning_rate": 0.0001482567403354042,
        "epoch": 0.7806095834264848,
        "step": 10475
    },
    {
        "loss": 1.6627,
        "grad_norm": 3.761256456375122,
        "learning_rate": 0.00014819509799910745,
        "epoch": 0.7806841046277666,
        "step": 10476
    },
    {
        "loss": 1.7718,
        "grad_norm": 3.1384081840515137,
        "learning_rate": 0.00014813343179983962,
        "epoch": 0.7807586258290483,
        "step": 10477
    },
    {
        "loss": 2.1233,
        "grad_norm": 4.2457404136657715,
        "learning_rate": 0.00014807174176813346,
        "epoch": 0.7808331470303301,
        "step": 10478
    },
    {
        "loss": 2.3233,
        "grad_norm": 2.1134707927703857,
        "learning_rate": 0.00014801002793453404,
        "epoch": 0.7809076682316118,
        "step": 10479
    },
    {
        "loss": 2.5206,
        "grad_norm": 2.8250248432159424,
        "learning_rate": 0.00014794829032959775,
        "epoch": 0.7809821894328937,
        "step": 10480
    },
    {
        "loss": 2.2528,
        "grad_norm": 3.5755341053009033,
        "learning_rate": 0.0001478865289838928,
        "epoch": 0.7810567106341755,
        "step": 10481
    },
    {
        "loss": 2.2306,
        "grad_norm": 4.085746765136719,
        "learning_rate": 0.00014782474392799962,
        "epoch": 0.7811312318354572,
        "step": 10482
    },
    {
        "loss": 2.3349,
        "grad_norm": 2.317976474761963,
        "learning_rate": 0.00014776293519250952,
        "epoch": 0.781205753036739,
        "step": 10483
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.5175085067749023,
        "learning_rate": 0.0001477011028080264,
        "epoch": 0.7812802742380207,
        "step": 10484
    },
    {
        "loss": 1.4791,
        "grad_norm": 5.59189510345459,
        "learning_rate": 0.00014763924680516523,
        "epoch": 0.7813547954393025,
        "step": 10485
    },
    {
        "loss": 3.1338,
        "grad_norm": 2.561127185821533,
        "learning_rate": 0.0001475773672145532,
        "epoch": 0.7814293166405842,
        "step": 10486
    },
    {
        "loss": 2.3278,
        "grad_norm": 3.746515989303589,
        "learning_rate": 0.0001475154640668288,
        "epoch": 0.7815038378418661,
        "step": 10487
    },
    {
        "loss": 1.9576,
        "grad_norm": 3.5560929775238037,
        "learning_rate": 0.00014745353739264219,
        "epoch": 0.7815783590431478,
        "step": 10488
    },
    {
        "loss": 2.8207,
        "grad_norm": 3.355022430419922,
        "learning_rate": 0.00014739158722265554,
        "epoch": 0.7816528802444296,
        "step": 10489
    },
    {
        "loss": 2.6816,
        "grad_norm": 3.1458981037139893,
        "learning_rate": 0.00014732961358754213,
        "epoch": 0.7817274014457113,
        "step": 10490
    },
    {
        "loss": 1.4637,
        "grad_norm": 3.2848892211914062,
        "learning_rate": 0.00014726761651798754,
        "epoch": 0.7818019226469931,
        "step": 10491
    },
    {
        "loss": 1.4737,
        "grad_norm": 3.0500755310058594,
        "learning_rate": 0.00014720559604468808,
        "epoch": 0.7818764438482748,
        "step": 10492
    },
    {
        "loss": 1.5316,
        "grad_norm": 2.4540674686431885,
        "learning_rate": 0.00014714355219835244,
        "epoch": 0.7819509650495566,
        "step": 10493
    },
    {
        "loss": 2.6205,
        "grad_norm": 3.059293031692505,
        "learning_rate": 0.00014708148500970042,
        "epoch": 0.7820254862508383,
        "step": 10494
    },
    {
        "loss": 0.6902,
        "grad_norm": 3.7628040313720703,
        "learning_rate": 0.00014701939450946345,
        "epoch": 0.7821000074521202,
        "step": 10495
    },
    {
        "loss": 2.4763,
        "grad_norm": 4.451351165771484,
        "learning_rate": 0.0001469572807283848,
        "epoch": 0.7821745286534019,
        "step": 10496
    },
    {
        "loss": 2.7654,
        "grad_norm": 3.6941111087799072,
        "learning_rate": 0.0001468951436972189,
        "epoch": 0.7822490498546837,
        "step": 10497
    },
    {
        "loss": 2.8621,
        "grad_norm": 2.108093738555908,
        "learning_rate": 0.00014683298344673166,
        "epoch": 0.7823235710559654,
        "step": 10498
    },
    {
        "loss": 1.2948,
        "grad_norm": 4.681341171264648,
        "learning_rate": 0.00014677080000770101,
        "epoch": 0.7823980922572472,
        "step": 10499
    },
    {
        "loss": 2.2559,
        "grad_norm": 2.8715522289276123,
        "learning_rate": 0.00014670859341091582,
        "epoch": 0.7824726134585289,
        "step": 10500
    },
    {
        "loss": 1.4278,
        "grad_norm": 1.6660200357437134,
        "learning_rate": 0.0001466463636871765,
        "epoch": 0.7825471346598107,
        "step": 10501
    },
    {
        "loss": 2.6947,
        "grad_norm": 3.680345058441162,
        "learning_rate": 0.00014658411086729533,
        "epoch": 0.7826216558610924,
        "step": 10502
    },
    {
        "loss": 2.1119,
        "grad_norm": 3.3875088691711426,
        "learning_rate": 0.00014652183498209534,
        "epoch": 0.7826961770623743,
        "step": 10503
    },
    {
        "loss": 2.0778,
        "grad_norm": 2.560058832168579,
        "learning_rate": 0.00014645953606241165,
        "epoch": 0.782770698263656,
        "step": 10504
    },
    {
        "loss": 2.3965,
        "grad_norm": 3.1768085956573486,
        "learning_rate": 0.0001463972141390904,
        "epoch": 0.7828452194649378,
        "step": 10505
    },
    {
        "loss": 2.6379,
        "grad_norm": 3.3564562797546387,
        "learning_rate": 0.0001463348692429891,
        "epoch": 0.7829197406662195,
        "step": 10506
    },
    {
        "loss": 1.7528,
        "grad_norm": 4.1079607009887695,
        "learning_rate": 0.00014627250140497697,
        "epoch": 0.7829942618675013,
        "step": 10507
    },
    {
        "loss": 2.582,
        "grad_norm": 2.432802677154541,
        "learning_rate": 0.00014621011065593417,
        "epoch": 0.783068783068783,
        "step": 10508
    },
    {
        "loss": 2.2102,
        "grad_norm": 1.722039818763733,
        "learning_rate": 0.0001461476970267526,
        "epoch": 0.7831433042700648,
        "step": 10509
    },
    {
        "loss": 2.2407,
        "grad_norm": 3.8282229900360107,
        "learning_rate": 0.00014608526054833517,
        "epoch": 0.7832178254713466,
        "step": 10510
    },
    {
        "loss": 2.7603,
        "grad_norm": 2.246896266937256,
        "learning_rate": 0.00014602280125159623,
        "epoch": 0.7832923466726284,
        "step": 10511
    },
    {
        "loss": 2.9139,
        "grad_norm": 2.8908162117004395,
        "learning_rate": 0.00014596031916746136,
        "epoch": 0.7833668678739101,
        "step": 10512
    },
    {
        "loss": 2.6857,
        "grad_norm": 3.708059310913086,
        "learning_rate": 0.00014589781432686763,
        "epoch": 0.7834413890751919,
        "step": 10513
    },
    {
        "loss": 2.4197,
        "grad_norm": 3.031789779663086,
        "learning_rate": 0.00014583528676076317,
        "epoch": 0.7835159102764736,
        "step": 10514
    },
    {
        "loss": 2.3697,
        "grad_norm": 3.938070297241211,
        "learning_rate": 0.00014577273650010744,
        "epoch": 0.7835904314777554,
        "step": 10515
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.9967461824417114,
        "learning_rate": 0.00014571016357587096,
        "epoch": 0.7836649526790372,
        "step": 10516
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.84725022315979,
        "learning_rate": 0.00014564756801903588,
        "epoch": 0.783739473880319,
        "step": 10517
    },
    {
        "loss": 2.63,
        "grad_norm": 2.8134679794311523,
        "learning_rate": 0.0001455849498605951,
        "epoch": 0.7838139950816008,
        "step": 10518
    },
    {
        "loss": 2.2018,
        "grad_norm": 2.7589304447174072,
        "learning_rate": 0.00014552230913155313,
        "epoch": 0.7838885162828825,
        "step": 10519
    },
    {
        "loss": 2.804,
        "grad_norm": 3.8469462394714355,
        "learning_rate": 0.00014545964586292536,
        "epoch": 0.7839630374841643,
        "step": 10520
    },
    {
        "loss": 2.6049,
        "grad_norm": 3.9508166313171387,
        "learning_rate": 0.00014539696008573823,
        "epoch": 0.784037558685446,
        "step": 10521
    },
    {
        "loss": 2.663,
        "grad_norm": 1.749443769454956,
        "learning_rate": 0.00014533425183103002,
        "epoch": 0.7841120798867278,
        "step": 10522
    },
    {
        "loss": 2.5769,
        "grad_norm": 3.0917422771453857,
        "learning_rate": 0.00014527152112984903,
        "epoch": 0.7841866010880095,
        "step": 10523
    },
    {
        "loss": 2.6108,
        "grad_norm": 2.3542237281799316,
        "learning_rate": 0.00014520876801325566,
        "epoch": 0.7842611222892913,
        "step": 10524
    },
    {
        "loss": 2.7775,
        "grad_norm": 3.0081844329833984,
        "learning_rate": 0.00014514599251232098,
        "epoch": 0.784335643490573,
        "step": 10525
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.073183298110962,
        "learning_rate": 0.00014508319465812703,
        "epoch": 0.7844101646918549,
        "step": 10526
    },
    {
        "loss": 2.2471,
        "grad_norm": 3.547511100769043,
        "learning_rate": 0.00014502037448176736,
        "epoch": 0.7844846858931366,
        "step": 10527
    },
    {
        "loss": 2.709,
        "grad_norm": 3.551576852798462,
        "learning_rate": 0.000144957532014346,
        "epoch": 0.7845592070944184,
        "step": 10528
    },
    {
        "loss": 2.7696,
        "grad_norm": 3.9722790718078613,
        "learning_rate": 0.0001448946672869786,
        "epoch": 0.7846337282957001,
        "step": 10529
    },
    {
        "loss": 2.6664,
        "grad_norm": 2.076179027557373,
        "learning_rate": 0.00014483178033079138,
        "epoch": 0.7847082494969819,
        "step": 10530
    },
    {
        "loss": 2.6545,
        "grad_norm": 1.929734468460083,
        "learning_rate": 0.00014476887117692176,
        "epoch": 0.7847827706982636,
        "step": 10531
    },
    {
        "loss": 1.9997,
        "grad_norm": 3.072835683822632,
        "learning_rate": 0.00014470593985651802,
        "epoch": 0.7848572918995455,
        "step": 10532
    },
    {
        "loss": 2.669,
        "grad_norm": 3.222977876663208,
        "learning_rate": 0.00014464298640073984,
        "epoch": 0.7849318131008272,
        "step": 10533
    },
    {
        "loss": 2.5082,
        "grad_norm": 5.021784782409668,
        "learning_rate": 0.00014458001084075709,
        "epoch": 0.785006334302109,
        "step": 10534
    },
    {
        "loss": 1.8581,
        "grad_norm": 3.0681796073913574,
        "learning_rate": 0.00014451701320775137,
        "epoch": 0.7850808555033907,
        "step": 10535
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.978951930999756,
        "learning_rate": 0.00014445399353291464,
        "epoch": 0.7851553767046725,
        "step": 10536
    },
    {
        "loss": 2.3909,
        "grad_norm": 1.5871913433074951,
        "learning_rate": 0.00014439095184745024,
        "epoch": 0.7852298979059542,
        "step": 10537
    },
    {
        "loss": 3.3009,
        "grad_norm": 3.3353259563446045,
        "learning_rate": 0.0001443278881825721,
        "epoch": 0.785304419107236,
        "step": 10538
    },
    {
        "loss": 1.7775,
        "grad_norm": 3.45515513420105,
        "learning_rate": 0.00014426480256950485,
        "epoch": 0.7853789403085177,
        "step": 10539
    },
    {
        "loss": 2.8347,
        "grad_norm": 1.5800601243972778,
        "learning_rate": 0.00014420169503948462,
        "epoch": 0.7854534615097996,
        "step": 10540
    },
    {
        "loss": 2.9883,
        "grad_norm": 2.622727870941162,
        "learning_rate": 0.00014413856562375772,
        "epoch": 0.7855279827110813,
        "step": 10541
    },
    {
        "loss": 2.4872,
        "grad_norm": 3.670395612716675,
        "learning_rate": 0.00014407541435358198,
        "epoch": 0.7856025039123631,
        "step": 10542
    },
    {
        "loss": 2.3778,
        "grad_norm": 3.053612470626831,
        "learning_rate": 0.00014401224126022515,
        "epoch": 0.7856770251136448,
        "step": 10543
    },
    {
        "loss": 1.9469,
        "grad_norm": 3.417116641998291,
        "learning_rate": 0.00014394904637496665,
        "epoch": 0.7857515463149266,
        "step": 10544
    },
    {
        "loss": 2.5895,
        "grad_norm": 3.182762622833252,
        "learning_rate": 0.00014388582972909626,
        "epoch": 0.7858260675162083,
        "step": 10545
    },
    {
        "loss": 2.6153,
        "grad_norm": 3.5683534145355225,
        "learning_rate": 0.00014382259135391445,
        "epoch": 0.7859005887174901,
        "step": 10546
    },
    {
        "loss": 2.3545,
        "grad_norm": 3.065446376800537,
        "learning_rate": 0.00014375933128073286,
        "epoch": 0.7859751099187718,
        "step": 10547
    },
    {
        "loss": 1.7367,
        "grad_norm": 3.6810302734375,
        "learning_rate": 0.0001436960495408735,
        "epoch": 0.7860496311200537,
        "step": 10548
    },
    {
        "loss": 2.8398,
        "grad_norm": 2.869161605834961,
        "learning_rate": 0.00014363274616566914,
        "epoch": 0.7861241523213354,
        "step": 10549
    },
    {
        "loss": 2.7459,
        "grad_norm": 2.269861936569214,
        "learning_rate": 0.00014356942118646358,
        "epoch": 0.7861986735226172,
        "step": 10550
    },
    {
        "loss": 2.0178,
        "grad_norm": 2.8344101905822754,
        "learning_rate": 0.00014350607463461094,
        "epoch": 0.7862731947238989,
        "step": 10551
    },
    {
        "loss": 2.1752,
        "grad_norm": 2.692099094390869,
        "learning_rate": 0.00014344270654147607,
        "epoch": 0.7863477159251807,
        "step": 10552
    },
    {
        "loss": 2.2783,
        "grad_norm": 2.5009396076202393,
        "learning_rate": 0.000143379316938435,
        "epoch": 0.7864222371264625,
        "step": 10553
    },
    {
        "loss": 2.4614,
        "grad_norm": 3.0290377140045166,
        "learning_rate": 0.00014331590585687343,
        "epoch": 0.7864967583277442,
        "step": 10554
    },
    {
        "loss": 2.1,
        "grad_norm": 3.8178372383117676,
        "learning_rate": 0.0001432524733281887,
        "epoch": 0.7865712795290261,
        "step": 10555
    },
    {
        "loss": 2.8353,
        "grad_norm": 2.3465704917907715,
        "learning_rate": 0.00014318901938378803,
        "epoch": 0.7866458007303078,
        "step": 10556
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.802004337310791,
        "learning_rate": 0.00014312554405508985,
        "epoch": 0.7867203219315896,
        "step": 10557
    },
    {
        "loss": 2.6466,
        "grad_norm": 2.663576364517212,
        "learning_rate": 0.0001430620473735227,
        "epoch": 0.7867948431328713,
        "step": 10558
    },
    {
        "loss": 1.8814,
        "grad_norm": 2.2729783058166504,
        "learning_rate": 0.00014299852937052574,
        "epoch": 0.7868693643341531,
        "step": 10559
    },
    {
        "loss": 2.4184,
        "grad_norm": 3.6173787117004395,
        "learning_rate": 0.00014293499007754908,
        "epoch": 0.7869438855354348,
        "step": 10560
    },
    {
        "loss": 2.2869,
        "grad_norm": 3.3094608783721924,
        "learning_rate": 0.000142871429526053,
        "epoch": 0.7870184067367166,
        "step": 10561
    },
    {
        "loss": 1.9327,
        "grad_norm": 5.322201728820801,
        "learning_rate": 0.00014280784774750837,
        "epoch": 0.7870929279379983,
        "step": 10562
    },
    {
        "loss": 2.5849,
        "grad_norm": 2.754715919494629,
        "learning_rate": 0.00014274424477339653,
        "epoch": 0.7871674491392802,
        "step": 10563
    },
    {
        "loss": 2.7475,
        "grad_norm": 2.916043519973755,
        "learning_rate": 0.00014268062063520963,
        "epoch": 0.7872419703405619,
        "step": 10564
    },
    {
        "loss": 2.6426,
        "grad_norm": 4.232417583465576,
        "learning_rate": 0.00014261697536444988,
        "epoch": 0.7873164915418437,
        "step": 10565
    },
    {
        "loss": 1.5217,
        "grad_norm": 3.5487964153289795,
        "learning_rate": 0.00014255330899263022,
        "epoch": 0.7873910127431254,
        "step": 10566
    },
    {
        "loss": 1.9743,
        "grad_norm": 2.717733860015869,
        "learning_rate": 0.0001424896215512738,
        "epoch": 0.7874655339444072,
        "step": 10567
    },
    {
        "loss": 2.602,
        "grad_norm": 4.3273797035217285,
        "learning_rate": 0.0001424259130719146,
        "epoch": 0.7875400551456889,
        "step": 10568
    },
    {
        "loss": 1.8103,
        "grad_norm": 3.4878430366516113,
        "learning_rate": 0.00014236218358609656,
        "epoch": 0.7876145763469707,
        "step": 10569
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.6290476322174072,
        "learning_rate": 0.00014229843312537442,
        "epoch": 0.7876890975482524,
        "step": 10570
    },
    {
        "loss": 2.5553,
        "grad_norm": 4.059159278869629,
        "learning_rate": 0.00014223466172131303,
        "epoch": 0.7877636187495343,
        "step": 10571
    },
    {
        "loss": 2.0827,
        "grad_norm": 3.2152199745178223,
        "learning_rate": 0.0001421708694054876,
        "epoch": 0.787838139950816,
        "step": 10572
    },
    {
        "loss": 1.7861,
        "grad_norm": 3.997065544128418,
        "learning_rate": 0.00014210705620948413,
        "epoch": 0.7879126611520978,
        "step": 10573
    },
    {
        "loss": 0.5675,
        "grad_norm": 3.3648226261138916,
        "learning_rate": 0.00014204322216489814,
        "epoch": 0.7879871823533795,
        "step": 10574
    },
    {
        "loss": 2.5662,
        "grad_norm": 2.7609784603118896,
        "learning_rate": 0.00014197936730333635,
        "epoch": 0.7880617035546613,
        "step": 10575
    },
    {
        "loss": 2.3539,
        "grad_norm": 3.647155523300171,
        "learning_rate": 0.00014191549165641523,
        "epoch": 0.788136224755943,
        "step": 10576
    },
    {
        "loss": 2.2169,
        "grad_norm": 3.484971284866333,
        "learning_rate": 0.00014185159525576156,
        "epoch": 0.7882107459572248,
        "step": 10577
    },
    {
        "loss": 1.9629,
        "grad_norm": 4.0207719802856445,
        "learning_rate": 0.00014178767813301287,
        "epoch": 0.7882852671585066,
        "step": 10578
    },
    {
        "loss": 2.1014,
        "grad_norm": 3.6562299728393555,
        "learning_rate": 0.0001417237403198163,
        "epoch": 0.7883597883597884,
        "step": 10579
    },
    {
        "loss": 3.1325,
        "grad_norm": 2.438894748687744,
        "learning_rate": 0.0001416597818478299,
        "epoch": 0.7884343095610701,
        "step": 10580
    },
    {
        "loss": 2.8126,
        "grad_norm": 3.4736087322235107,
        "learning_rate": 0.00014159580274872134,
        "epoch": 0.7885088307623519,
        "step": 10581
    },
    {
        "loss": 1.9141,
        "grad_norm": 4.021884918212891,
        "learning_rate": 0.00014153180305416884,
        "epoch": 0.7885833519636336,
        "step": 10582
    },
    {
        "loss": 2.0328,
        "grad_norm": 3.5059103965759277,
        "learning_rate": 0.00014146778279586065,
        "epoch": 0.7886578731649154,
        "step": 10583
    },
    {
        "loss": 2.598,
        "grad_norm": 2.4528250694274902,
        "learning_rate": 0.0001414037420054955,
        "epoch": 0.7887323943661971,
        "step": 10584
    },
    {
        "loss": 2.4668,
        "grad_norm": 3.075496196746826,
        "learning_rate": 0.00014133968071478194,
        "epoch": 0.788806915567479,
        "step": 10585
    },
    {
        "loss": 1.4022,
        "grad_norm": 4.449650287628174,
        "learning_rate": 0.00014127559895543888,
        "epoch": 0.7888814367687607,
        "step": 10586
    },
    {
        "loss": 2.8491,
        "grad_norm": 2.6194283962249756,
        "learning_rate": 0.00014121149675919514,
        "epoch": 0.7889559579700425,
        "step": 10587
    },
    {
        "loss": 2.4308,
        "grad_norm": 2.9803378582000732,
        "learning_rate": 0.00014114737415779003,
        "epoch": 0.7890304791713243,
        "step": 10588
    },
    {
        "loss": 2.5548,
        "grad_norm": 3.271489381790161,
        "learning_rate": 0.00014108323118297252,
        "epoch": 0.789105000372606,
        "step": 10589
    },
    {
        "loss": 1.1072,
        "grad_norm": 4.186457633972168,
        "learning_rate": 0.0001410190678665022,
        "epoch": 0.7891795215738878,
        "step": 10590
    },
    {
        "loss": 2.2199,
        "grad_norm": 3.949334144592285,
        "learning_rate": 0.00014095488424014824,
        "epoch": 0.7892540427751695,
        "step": 10591
    },
    {
        "loss": 2.1487,
        "grad_norm": 2.743035078048706,
        "learning_rate": 0.00014089068033568998,
        "epoch": 0.7893285639764513,
        "step": 10592
    },
    {
        "loss": 2.605,
        "grad_norm": 2.1407766342163086,
        "learning_rate": 0.00014082645618491716,
        "epoch": 0.789403085177733,
        "step": 10593
    },
    {
        "loss": 2.827,
        "grad_norm": 2.615730047225952,
        "learning_rate": 0.00014076221181962887,
        "epoch": 0.7894776063790149,
        "step": 10594
    },
    {
        "loss": 2.9122,
        "grad_norm": 2.6533970832824707,
        "learning_rate": 0.00014069794727163492,
        "epoch": 0.7895521275802966,
        "step": 10595
    },
    {
        "loss": 1.3157,
        "grad_norm": 4.007316589355469,
        "learning_rate": 0.00014063366257275467,
        "epoch": 0.7896266487815784,
        "step": 10596
    },
    {
        "loss": 2.2347,
        "grad_norm": 3.654771089553833,
        "learning_rate": 0.00014056935775481744,
        "epoch": 0.7897011699828601,
        "step": 10597
    },
    {
        "loss": 2.4527,
        "grad_norm": 2.125626564025879,
        "learning_rate": 0.00014050503284966296,
        "epoch": 0.7897756911841419,
        "step": 10598
    },
    {
        "loss": 1.4826,
        "grad_norm": 4.502563953399658,
        "learning_rate": 0.0001404406878891404,
        "epoch": 0.7898502123854236,
        "step": 10599
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.4421632289886475,
        "learning_rate": 0.00014037632290510895,
        "epoch": 0.7899247335867055,
        "step": 10600
    },
    {
        "loss": 2.6031,
        "grad_norm": 2.678802490234375,
        "learning_rate": 0.00014031193792943811,
        "epoch": 0.7899992547879872,
        "step": 10601
    },
    {
        "loss": 2.5217,
        "grad_norm": 3.3937604427337646,
        "learning_rate": 0.00014024753299400684,
        "epoch": 0.790073775989269,
        "step": 10602
    },
    {
        "loss": 2.214,
        "grad_norm": 2.760960578918457,
        "learning_rate": 0.00014018310813070399,
        "epoch": 0.7901482971905507,
        "step": 10603
    },
    {
        "loss": 2.0477,
        "grad_norm": 4.117573261260986,
        "learning_rate": 0.0001401186633714288,
        "epoch": 0.7902228183918325,
        "step": 10604
    },
    {
        "loss": 2.4015,
        "grad_norm": 3.290508270263672,
        "learning_rate": 0.0001400541987480895,
        "epoch": 0.7902973395931142,
        "step": 10605
    },
    {
        "loss": 2.1736,
        "grad_norm": 2.1768693923950195,
        "learning_rate": 0.00013998971429260502,
        "epoch": 0.790371860794396,
        "step": 10606
    },
    {
        "loss": 1.8448,
        "grad_norm": 3.6903674602508545,
        "learning_rate": 0.00013992521003690342,
        "epoch": 0.7904463819956777,
        "step": 10607
    },
    {
        "loss": 2.1497,
        "grad_norm": 4.943875789642334,
        "learning_rate": 0.00013986068601292318,
        "epoch": 0.7905209031969596,
        "step": 10608
    },
    {
        "loss": 2.502,
        "grad_norm": 3.232168197631836,
        "learning_rate": 0.00013979614225261212,
        "epoch": 0.7905954243982413,
        "step": 10609
    },
    {
        "loss": 3.0739,
        "grad_norm": 3.3025710582733154,
        "learning_rate": 0.0001397315787879278,
        "epoch": 0.7906699455995231,
        "step": 10610
    },
    {
        "loss": 2.1529,
        "grad_norm": 3.1261167526245117,
        "learning_rate": 0.000139666995650838,
        "epoch": 0.7907444668008048,
        "step": 10611
    },
    {
        "loss": 2.8025,
        "grad_norm": 3.415801763534546,
        "learning_rate": 0.00013960239287331966,
        "epoch": 0.7908189880020866,
        "step": 10612
    },
    {
        "loss": 2.8812,
        "grad_norm": 3.330991506576538,
        "learning_rate": 0.00013953777048736015,
        "epoch": 0.7908935092033683,
        "step": 10613
    },
    {
        "loss": 2.4748,
        "grad_norm": 2.2257232666015625,
        "learning_rate": 0.0001394731285249556,
        "epoch": 0.7909680304046501,
        "step": 10614
    },
    {
        "loss": 2.0164,
        "grad_norm": 2.9964170455932617,
        "learning_rate": 0.00013940846701811278,
        "epoch": 0.7910425516059318,
        "step": 10615
    },
    {
        "loss": 2.5033,
        "grad_norm": 3.2843503952026367,
        "learning_rate": 0.00013934378599884752,
        "epoch": 0.7911170728072137,
        "step": 10616
    },
    {
        "loss": 1.8405,
        "grad_norm": 3.4498703479766846,
        "learning_rate": 0.00013927908549918541,
        "epoch": 0.7911915940084954,
        "step": 10617
    },
    {
        "loss": 2.2252,
        "grad_norm": 3.4853875637054443,
        "learning_rate": 0.0001392143655511621,
        "epoch": 0.7912661152097772,
        "step": 10618
    },
    {
        "loss": 2.5986,
        "grad_norm": 2.6405444145202637,
        "learning_rate": 0.00013914962618682237,
        "epoch": 0.7913406364110589,
        "step": 10619
    },
    {
        "loss": 2.9635,
        "grad_norm": 2.1277894973754883,
        "learning_rate": 0.00013908486743822066,
        "epoch": 0.7914151576123407,
        "step": 10620
    },
    {
        "loss": 1.4794,
        "grad_norm": 9.113917350769043,
        "learning_rate": 0.00013902008933742142,
        "epoch": 0.7914896788136224,
        "step": 10621
    },
    {
        "loss": 2.4991,
        "grad_norm": 5.037575721740723,
        "learning_rate": 0.0001389552919164983,
        "epoch": 0.7915642000149042,
        "step": 10622
    },
    {
        "loss": 2.6724,
        "grad_norm": 2.901620388031006,
        "learning_rate": 0.00013889047520753448,
        "epoch": 0.7916387212161861,
        "step": 10623
    },
    {
        "loss": 2.1484,
        "grad_norm": 7.086219787597656,
        "learning_rate": 0.00013882563924262327,
        "epoch": 0.7917132424174678,
        "step": 10624
    },
    {
        "loss": 2.1316,
        "grad_norm": 1.98813796043396,
        "learning_rate": 0.00013876078405386646,
        "epoch": 0.7917877636187496,
        "step": 10625
    },
    {
        "loss": 2.622,
        "grad_norm": 3.7283456325531006,
        "learning_rate": 0.00013869590967337643,
        "epoch": 0.7918622848200313,
        "step": 10626
    },
    {
        "loss": 2.7393,
        "grad_norm": 3.381502628326416,
        "learning_rate": 0.00013863101613327444,
        "epoch": 0.7919368060213131,
        "step": 10627
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.078597068786621,
        "learning_rate": 0.00013856610346569138,
        "epoch": 0.7920113272225948,
        "step": 10628
    },
    {
        "loss": 2.4786,
        "grad_norm": 4.030839920043945,
        "learning_rate": 0.00013850117170276777,
        "epoch": 0.7920858484238766,
        "step": 10629
    },
    {
        "loss": 2.5481,
        "grad_norm": 4.26666259765625,
        "learning_rate": 0.00013843622087665326,
        "epoch": 0.7921603696251583,
        "step": 10630
    },
    {
        "loss": 2.8222,
        "grad_norm": 3.403754949569702,
        "learning_rate": 0.00013837125101950738,
        "epoch": 0.7922348908264402,
        "step": 10631
    },
    {
        "loss": 2.0209,
        "grad_norm": 5.028903007507324,
        "learning_rate": 0.00013830626216349874,
        "epoch": 0.7923094120277219,
        "step": 10632
    },
    {
        "loss": 2.5514,
        "grad_norm": 1.747348666191101,
        "learning_rate": 0.00013824125434080538,
        "epoch": 0.7923839332290037,
        "step": 10633
    },
    {
        "loss": 1.9556,
        "grad_norm": 3.57442045211792,
        "learning_rate": 0.0001381762275836147,
        "epoch": 0.7924584544302854,
        "step": 10634
    },
    {
        "loss": 2.4582,
        "grad_norm": 2.696962356567383,
        "learning_rate": 0.00013811118192412383,
        "epoch": 0.7925329756315672,
        "step": 10635
    },
    {
        "loss": 2.0118,
        "grad_norm": 3.716533899307251,
        "learning_rate": 0.00013804611739453893,
        "epoch": 0.7926074968328489,
        "step": 10636
    },
    {
        "loss": 2.3517,
        "grad_norm": 3.051969528198242,
        "learning_rate": 0.00013798103402707547,
        "epoch": 0.7926820180341307,
        "step": 10637
    },
    {
        "loss": 2.3701,
        "grad_norm": 3.991501808166504,
        "learning_rate": 0.00013791593185395835,
        "epoch": 0.7927565392354124,
        "step": 10638
    },
    {
        "loss": 2.5447,
        "grad_norm": 3.351731300354004,
        "learning_rate": 0.00013785081090742196,
        "epoch": 0.7928310604366943,
        "step": 10639
    },
    {
        "loss": 2.6878,
        "grad_norm": 3.4600419998168945,
        "learning_rate": 0.0001377856712197096,
        "epoch": 0.792905581637976,
        "step": 10640
    },
    {
        "loss": 2.034,
        "grad_norm": 3.2677130699157715,
        "learning_rate": 0.0001377205128230743,
        "epoch": 0.7929801028392578,
        "step": 10641
    },
    {
        "loss": 2.3567,
        "grad_norm": 2.711881399154663,
        "learning_rate": 0.00013765533574977798,
        "epoch": 0.7930546240405395,
        "step": 10642
    },
    {
        "loss": 2.4581,
        "grad_norm": 3.5370898246765137,
        "learning_rate": 0.00013759014003209184,
        "epoch": 0.7931291452418213,
        "step": 10643
    },
    {
        "loss": 2.7966,
        "grad_norm": 3.056702136993408,
        "learning_rate": 0.00013752492570229678,
        "epoch": 0.793203666443103,
        "step": 10644
    },
    {
        "loss": 2.565,
        "grad_norm": 2.3884172439575195,
        "learning_rate": 0.00013745969279268204,
        "epoch": 0.7932781876443848,
        "step": 10645
    },
    {
        "loss": 2.6569,
        "grad_norm": 1.624199628829956,
        "learning_rate": 0.00013739444133554696,
        "epoch": 0.7933527088456666,
        "step": 10646
    },
    {
        "loss": 2.4396,
        "grad_norm": 1.7222387790679932,
        "learning_rate": 0.00013732917136319954,
        "epoch": 0.7934272300469484,
        "step": 10647
    },
    {
        "loss": 2.5958,
        "grad_norm": 2.6749908924102783,
        "learning_rate": 0.00013726388290795694,
        "epoch": 0.7935017512482301,
        "step": 10648
    },
    {
        "loss": 2.376,
        "grad_norm": 2.363874912261963,
        "learning_rate": 0.0001371985760021459,
        "epoch": 0.7935762724495119,
        "step": 10649
    },
    {
        "loss": 2.269,
        "grad_norm": 3.632099151611328,
        "learning_rate": 0.00013713325067810173,
        "epoch": 0.7936507936507936,
        "step": 10650
    },
    {
        "loss": 2.3615,
        "grad_norm": 3.4155373573303223,
        "learning_rate": 0.00013706790696816937,
        "epoch": 0.7937253148520754,
        "step": 10651
    },
    {
        "loss": 2.1529,
        "grad_norm": 3.522523880004883,
        "learning_rate": 0.00013700254490470258,
        "epoch": 0.7937998360533571,
        "step": 10652
    },
    {
        "loss": 2.7207,
        "grad_norm": 1.2741024494171143,
        "learning_rate": 0.00013693716452006416,
        "epoch": 0.793874357254639,
        "step": 10653
    },
    {
        "loss": 1.8139,
        "grad_norm": 4.655154705047607,
        "learning_rate": 0.00013687176584662602,
        "epoch": 0.7939488784559207,
        "step": 10654
    },
    {
        "loss": 2.1665,
        "grad_norm": 2.3912811279296875,
        "learning_rate": 0.00013680634891676956,
        "epoch": 0.7940233996572025,
        "step": 10655
    },
    {
        "loss": 2.0198,
        "grad_norm": 3.495464324951172,
        "learning_rate": 0.00013674091376288428,
        "epoch": 0.7940979208584842,
        "step": 10656
    },
    {
        "loss": 1.8555,
        "grad_norm": 2.737192153930664,
        "learning_rate": 0.0001366754604173697,
        "epoch": 0.794172442059766,
        "step": 10657
    },
    {
        "loss": 1.984,
        "grad_norm": 1.9788973331451416,
        "learning_rate": 0.00013660998891263366,
        "epoch": 0.7942469632610478,
        "step": 10658
    },
    {
        "loss": 1.8202,
        "grad_norm": 2.8070294857025146,
        "learning_rate": 0.0001365444992810935,
        "epoch": 0.7943214844623295,
        "step": 10659
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.997760772705078,
        "learning_rate": 0.00013647899155517512,
        "epoch": 0.7943960056636113,
        "step": 10660
    },
    {
        "loss": 2.5838,
        "grad_norm": 2.7448225021362305,
        "learning_rate": 0.00013641346576731344,
        "epoch": 0.7944705268648931,
        "step": 10661
    },
    {
        "loss": 2.8689,
        "grad_norm": 1.9589616060256958,
        "learning_rate": 0.00013634792194995268,
        "epoch": 0.7945450480661749,
        "step": 10662
    },
    {
        "loss": 2.4645,
        "grad_norm": 4.0410966873168945,
        "learning_rate": 0.00013628236013554546,
        "epoch": 0.7946195692674566,
        "step": 10663
    },
    {
        "loss": 2.7469,
        "grad_norm": 2.3083715438842773,
        "learning_rate": 0.000136216780356554,
        "epoch": 0.7946940904687384,
        "step": 10664
    },
    {
        "loss": 2.6962,
        "grad_norm": 4.154220104217529,
        "learning_rate": 0.00013615118264544843,
        "epoch": 0.7947686116700201,
        "step": 10665
    },
    {
        "loss": 2.0912,
        "grad_norm": 3.399569511413574,
        "learning_rate": 0.00013608556703470875,
        "epoch": 0.7948431328713019,
        "step": 10666
    },
    {
        "loss": 2.0803,
        "grad_norm": 3.9690864086151123,
        "learning_rate": 0.0001360199335568232,
        "epoch": 0.7949176540725836,
        "step": 10667
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.4925012588500977,
        "learning_rate": 0.00013595428224428906,
        "epoch": 0.7949921752738655,
        "step": 10668
    },
    {
        "loss": 2.2525,
        "grad_norm": 3.0131430625915527,
        "learning_rate": 0.0001358886131296126,
        "epoch": 0.7950666964751472,
        "step": 10669
    },
    {
        "loss": 1.6108,
        "grad_norm": 1.5557880401611328,
        "learning_rate": 0.00013582292624530866,
        "epoch": 0.795141217676429,
        "step": 10670
    },
    {
        "loss": 2.4805,
        "grad_norm": 2.7231645584106445,
        "learning_rate": 0.00013575722162390084,
        "epoch": 0.7952157388777107,
        "step": 10671
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.9708316326141357,
        "learning_rate": 0.00013569149929792187,
        "epoch": 0.7952902600789925,
        "step": 10672
    },
    {
        "loss": 1.2734,
        "grad_norm": 3.335148334503174,
        "learning_rate": 0.00013562575929991293,
        "epoch": 0.7953647812802742,
        "step": 10673
    },
    {
        "loss": 2.6769,
        "grad_norm": 2.8187806606292725,
        "learning_rate": 0.00013556000166242392,
        "epoch": 0.795439302481556,
        "step": 10674
    },
    {
        "loss": 2.7029,
        "grad_norm": 2.6666982173919678,
        "learning_rate": 0.000135494226418014,
        "epoch": 0.7955138236828377,
        "step": 10675
    },
    {
        "loss": 2.5544,
        "grad_norm": 1.8513717651367188,
        "learning_rate": 0.00013542843359925008,
        "epoch": 0.7955883448841196,
        "step": 10676
    },
    {
        "loss": 2.2503,
        "grad_norm": 2.634892225265503,
        "learning_rate": 0.00013536262323870878,
        "epoch": 0.7956628660854013,
        "step": 10677
    },
    {
        "loss": 2.4607,
        "grad_norm": 4.0813422203063965,
        "learning_rate": 0.00013529679536897465,
        "epoch": 0.7957373872866831,
        "step": 10678
    },
    {
        "loss": 2.1752,
        "grad_norm": 3.712728261947632,
        "learning_rate": 0.00013523095002264155,
        "epoch": 0.7958119084879648,
        "step": 10679
    },
    {
        "loss": 2.6546,
        "grad_norm": 2.252812147140503,
        "learning_rate": 0.00013516508723231146,
        "epoch": 0.7958864296892466,
        "step": 10680
    },
    {
        "loss": 2.1963,
        "grad_norm": 4.220317363739014,
        "learning_rate": 0.00013509920703059513,
        "epoch": 0.7959609508905283,
        "step": 10681
    },
    {
        "loss": 2.3553,
        "grad_norm": 2.655379056930542,
        "learning_rate": 0.00013503330945011224,
        "epoch": 0.7960354720918101,
        "step": 10682
    },
    {
        "loss": 2.6974,
        "grad_norm": 2.305790662765503,
        "learning_rate": 0.00013496739452349064,
        "epoch": 0.7961099932930918,
        "step": 10683
    },
    {
        "loss": 2.0691,
        "grad_norm": 3.1830692291259766,
        "learning_rate": 0.00013490146228336725,
        "epoch": 0.7961845144943737,
        "step": 10684
    },
    {
        "loss": 2.2982,
        "grad_norm": 3.352299213409424,
        "learning_rate": 0.00013483551276238684,
        "epoch": 0.7962590356956554,
        "step": 10685
    },
    {
        "loss": 2.7925,
        "grad_norm": 2.5779380798339844,
        "learning_rate": 0.00013476954599320355,
        "epoch": 0.7963335568969372,
        "step": 10686
    },
    {
        "loss": 2.7207,
        "grad_norm": 3.028043031692505,
        "learning_rate": 0.00013470356200847956,
        "epoch": 0.7964080780982189,
        "step": 10687
    },
    {
        "loss": 2.6065,
        "grad_norm": 3.521454095840454,
        "learning_rate": 0.0001346375608408857,
        "epoch": 0.7964825992995007,
        "step": 10688
    },
    {
        "loss": 2.4629,
        "grad_norm": 3.1977930068969727,
        "learning_rate": 0.00013457154252310115,
        "epoch": 0.7965571205007824,
        "step": 10689
    },
    {
        "loss": 2.8192,
        "grad_norm": 2.16646409034729,
        "learning_rate": 0.00013450550708781405,
        "epoch": 0.7966316417020642,
        "step": 10690
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.683295965194702,
        "learning_rate": 0.0001344394545677204,
        "epoch": 0.796706162903346,
        "step": 10691
    },
    {
        "loss": 2.6633,
        "grad_norm": 2.740993022918701,
        "learning_rate": 0.00013437338499552526,
        "epoch": 0.7967806841046278,
        "step": 10692
    },
    {
        "loss": 2.6605,
        "grad_norm": 2.2443366050720215,
        "learning_rate": 0.0001343072984039417,
        "epoch": 0.7968552053059096,
        "step": 10693
    },
    {
        "loss": 2.1819,
        "grad_norm": 2.3887088298797607,
        "learning_rate": 0.0001342411948256912,
        "epoch": 0.7969297265071913,
        "step": 10694
    },
    {
        "loss": 2.3423,
        "grad_norm": 3.4896891117095947,
        "learning_rate": 0.00013417507429350423,
        "epoch": 0.7970042477084731,
        "step": 10695
    },
    {
        "loss": 2.8653,
        "grad_norm": 2.3163647651672363,
        "learning_rate": 0.0001341089368401187,
        "epoch": 0.7970787689097548,
        "step": 10696
    },
    {
        "loss": 2.2076,
        "grad_norm": 2.779636859893799,
        "learning_rate": 0.00013404278249828182,
        "epoch": 0.7971532901110366,
        "step": 10697
    },
    {
        "loss": 2.0208,
        "grad_norm": 3.37325119972229,
        "learning_rate": 0.0001339766113007486,
        "epoch": 0.7972278113123183,
        "step": 10698
    },
    {
        "loss": 1.6474,
        "grad_norm": 1.0650010108947754,
        "learning_rate": 0.0001339104232802825,
        "epoch": 0.7973023325136002,
        "step": 10699
    },
    {
        "loss": 2.9965,
        "grad_norm": 2.066002368927002,
        "learning_rate": 0.00013384421846965566,
        "epoch": 0.7973768537148819,
        "step": 10700
    },
    {
        "loss": 2.2914,
        "grad_norm": 3.1096746921539307,
        "learning_rate": 0.0001337779969016479,
        "epoch": 0.7974513749161637,
        "step": 10701
    },
    {
        "loss": 2.2701,
        "grad_norm": 3.4637367725372314,
        "learning_rate": 0.00013371175860904796,
        "epoch": 0.7975258961174454,
        "step": 10702
    },
    {
        "loss": 2.2263,
        "grad_norm": 1.649907112121582,
        "learning_rate": 0.00013364550362465248,
        "epoch": 0.7976004173187272,
        "step": 10703
    },
    {
        "loss": 2.822,
        "grad_norm": 2.9863297939300537,
        "learning_rate": 0.00013357923198126654,
        "epoch": 0.7976749385200089,
        "step": 10704
    },
    {
        "loss": 2.7555,
        "grad_norm": 2.5668013095855713,
        "learning_rate": 0.00013351294371170315,
        "epoch": 0.7977494597212907,
        "step": 10705
    },
    {
        "loss": 2.6242,
        "grad_norm": 3.968549966812134,
        "learning_rate": 0.00013344663884878413,
        "epoch": 0.7978239809225725,
        "step": 10706
    },
    {
        "loss": 2.0691,
        "grad_norm": 3.5981030464172363,
        "learning_rate": 0.00013338031742533905,
        "epoch": 0.7978985021238543,
        "step": 10707
    },
    {
        "loss": 2.445,
        "grad_norm": 3.0394225120544434,
        "learning_rate": 0.00013331397947420585,
        "epoch": 0.797973023325136,
        "step": 10708
    },
    {
        "loss": 1.6644,
        "grad_norm": 3.793942928314209,
        "learning_rate": 0.0001332476250282304,
        "epoch": 0.7980475445264178,
        "step": 10709
    },
    {
        "loss": 2.4764,
        "grad_norm": 2.769848585128784,
        "learning_rate": 0.00013318125412026732,
        "epoch": 0.7981220657276995,
        "step": 10710
    },
    {
        "loss": 2.4785,
        "grad_norm": 2.218787670135498,
        "learning_rate": 0.0001331148667831787,
        "epoch": 0.7981965869289813,
        "step": 10711
    },
    {
        "loss": 2.8322,
        "grad_norm": 2.5811076164245605,
        "learning_rate": 0.00013304846304983538,
        "epoch": 0.798271108130263,
        "step": 10712
    },
    {
        "loss": 1.9814,
        "grad_norm": 3.272667169570923,
        "learning_rate": 0.00013298204295311586,
        "epoch": 0.7983456293315448,
        "step": 10713
    },
    {
        "loss": 1.0436,
        "grad_norm": 3.5364789962768555,
        "learning_rate": 0.00013291560652590678,
        "epoch": 0.7984201505328266,
        "step": 10714
    },
    {
        "loss": 2.5229,
        "grad_norm": 3.2363839149475098,
        "learning_rate": 0.0001328491538011034,
        "epoch": 0.7984946717341084,
        "step": 10715
    },
    {
        "loss": 2.2582,
        "grad_norm": 2.5945703983306885,
        "learning_rate": 0.0001327826848116081,
        "epoch": 0.7985691929353901,
        "step": 10716
    },
    {
        "loss": 2.5294,
        "grad_norm": 3.487088680267334,
        "learning_rate": 0.00013271619959033225,
        "epoch": 0.7986437141366719,
        "step": 10717
    },
    {
        "loss": 2.4396,
        "grad_norm": 2.0998737812042236,
        "learning_rate": 0.00013264969817019474,
        "epoch": 0.7987182353379536,
        "step": 10718
    },
    {
        "loss": 2.7218,
        "grad_norm": 2.929680109024048,
        "learning_rate": 0.00013258318058412244,
        "epoch": 0.7987927565392354,
        "step": 10719
    },
    {
        "loss": 2.2998,
        "grad_norm": 4.627950668334961,
        "learning_rate": 0.00013251664686505073,
        "epoch": 0.7988672777405171,
        "step": 10720
    },
    {
        "loss": 2.4105,
        "grad_norm": 4.64330530166626,
        "learning_rate": 0.00013245009704592237,
        "epoch": 0.798941798941799,
        "step": 10721
    },
    {
        "loss": 2.3609,
        "grad_norm": 3.713926315307617,
        "learning_rate": 0.00013238353115968833,
        "epoch": 0.7990163201430807,
        "step": 10722
    },
    {
        "loss": 2.7173,
        "grad_norm": 3.4037723541259766,
        "learning_rate": 0.0001323169492393078,
        "epoch": 0.7990908413443625,
        "step": 10723
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.5618531703948975,
        "learning_rate": 0.0001322503513177475,
        "epoch": 0.7991653625456442,
        "step": 10724
    },
    {
        "loss": 1.5573,
        "grad_norm": 2.951295852661133,
        "learning_rate": 0.00013218373742798214,
        "epoch": 0.799239883746926,
        "step": 10725
    },
    {
        "loss": 2.148,
        "grad_norm": 2.61989426612854,
        "learning_rate": 0.00013211710760299483,
        "epoch": 0.7993144049482077,
        "step": 10726
    },
    {
        "loss": 2.414,
        "grad_norm": 1.7811198234558105,
        "learning_rate": 0.00013205046187577565,
        "epoch": 0.7993889261494895,
        "step": 10727
    },
    {
        "loss": 2.3263,
        "grad_norm": 2.3378701210021973,
        "learning_rate": 0.00013198380027932345,
        "epoch": 0.7994634473507712,
        "step": 10728
    },
    {
        "loss": 2.2885,
        "grad_norm": 3.3868706226348877,
        "learning_rate": 0.00013191712284664435,
        "epoch": 0.7995379685520531,
        "step": 10729
    },
    {
        "loss": 2.4334,
        "grad_norm": 3.0849833488464355,
        "learning_rate": 0.00013185042961075277,
        "epoch": 0.7996124897533349,
        "step": 10730
    },
    {
        "loss": 2.1355,
        "grad_norm": 4.057821750640869,
        "learning_rate": 0.00013178372060467062,
        "epoch": 0.7996870109546166,
        "step": 10731
    },
    {
        "loss": 2.3377,
        "grad_norm": 2.6179747581481934,
        "learning_rate": 0.0001317169958614275,
        "epoch": 0.7997615321558984,
        "step": 10732
    },
    {
        "loss": 2.561,
        "grad_norm": 2.5130176544189453,
        "learning_rate": 0.00013165025541406135,
        "epoch": 0.7998360533571801,
        "step": 10733
    },
    {
        "loss": 2.463,
        "grad_norm": 2.002751350402832,
        "learning_rate": 0.00013158349929561725,
        "epoch": 0.7999105745584619,
        "step": 10734
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.5566368103027344,
        "learning_rate": 0.0001315167275391488,
        "epoch": 0.7999850957597436,
        "step": 10735
    },
    {
        "loss": 2.2918,
        "grad_norm": 3.1553566455841064,
        "learning_rate": 0.0001314499401777163,
        "epoch": 0.8000596169610255,
        "step": 10736
    },
    {
        "loss": 2.7361,
        "grad_norm": 2.1067676544189453,
        "learning_rate": 0.00013138313724438868,
        "epoch": 0.8001341381623072,
        "step": 10737
    },
    {
        "loss": 2.544,
        "grad_norm": 1.709205150604248,
        "learning_rate": 0.00013131631877224227,
        "epoch": 0.800208659363589,
        "step": 10738
    },
    {
        "loss": 1.6881,
        "grad_norm": 2.507526159286499,
        "learning_rate": 0.0001312494847943609,
        "epoch": 0.8002831805648707,
        "step": 10739
    },
    {
        "loss": 1.5782,
        "grad_norm": 5.621493816375732,
        "learning_rate": 0.0001311826353438365,
        "epoch": 0.8003577017661525,
        "step": 10740
    },
    {
        "loss": 2.2992,
        "grad_norm": 3.241353988647461,
        "learning_rate": 0.00013111577045376828,
        "epoch": 0.8004322229674342,
        "step": 10741
    },
    {
        "loss": 2.28,
        "grad_norm": 5.112525939941406,
        "learning_rate": 0.00013104889015726315,
        "epoch": 0.800506744168716,
        "step": 10742
    },
    {
        "loss": 1.6726,
        "grad_norm": 5.259119987487793,
        "learning_rate": 0.00013098199448743602,
        "epoch": 0.8005812653699977,
        "step": 10743
    },
    {
        "loss": 2.5962,
        "grad_norm": 3.3274872303009033,
        "learning_rate": 0.00013091508347740888,
        "epoch": 0.8006557865712796,
        "step": 10744
    },
    {
        "loss": 2.8647,
        "grad_norm": 4.0954203605651855,
        "learning_rate": 0.00013084815716031157,
        "epoch": 0.8007303077725613,
        "step": 10745
    },
    {
        "loss": 2.3112,
        "grad_norm": 3.7247629165649414,
        "learning_rate": 0.0001307812155692818,
        "epoch": 0.8008048289738431,
        "step": 10746
    },
    {
        "loss": 2.1868,
        "grad_norm": 2.620434284210205,
        "learning_rate": 0.00013071425873746403,
        "epoch": 0.8008793501751248,
        "step": 10747
    },
    {
        "loss": 2.4216,
        "grad_norm": 2.6886816024780273,
        "learning_rate": 0.0001306472866980112,
        "epoch": 0.8009538713764066,
        "step": 10748
    },
    {
        "loss": 1.5794,
        "grad_norm": 3.4963607788085938,
        "learning_rate": 0.00013058029948408324,
        "epoch": 0.8010283925776883,
        "step": 10749
    },
    {
        "loss": 1.9607,
        "grad_norm": 3.7890477180480957,
        "learning_rate": 0.00013051329712884756,
        "epoch": 0.8011029137789701,
        "step": 10750
    },
    {
        "loss": 2.5203,
        "grad_norm": 3.5605287551879883,
        "learning_rate": 0.00013044627966547943,
        "epoch": 0.8011774349802518,
        "step": 10751
    },
    {
        "loss": 2.642,
        "grad_norm": 1.237399935722351,
        "learning_rate": 0.00013037924712716123,
        "epoch": 0.8012519561815337,
        "step": 10752
    },
    {
        "loss": 2.4356,
        "grad_norm": 3.3330483436584473,
        "learning_rate": 0.00013031219954708317,
        "epoch": 0.8013264773828154,
        "step": 10753
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.8866257667541504,
        "learning_rate": 0.00013024513695844258,
        "epoch": 0.8014009985840972,
        "step": 10754
    },
    {
        "loss": 2.5858,
        "grad_norm": 1.934694528579712,
        "learning_rate": 0.00013017805939444433,
        "epoch": 0.8014755197853789,
        "step": 10755
    },
    {
        "loss": 2.4857,
        "grad_norm": 3.074197769165039,
        "learning_rate": 0.0001301109668883006,
        "epoch": 0.8015500409866607,
        "step": 10756
    },
    {
        "loss": 2.1526,
        "grad_norm": 1.8888773918151855,
        "learning_rate": 0.0001300438594732314,
        "epoch": 0.8016245621879424,
        "step": 10757
    },
    {
        "loss": 2.5406,
        "grad_norm": 1.4946686029434204,
        "learning_rate": 0.00012997673718246358,
        "epoch": 0.8016990833892242,
        "step": 10758
    },
    {
        "loss": 2.6297,
        "grad_norm": 3.0060887336730957,
        "learning_rate": 0.00012990960004923162,
        "epoch": 0.801773604590506,
        "step": 10759
    },
    {
        "loss": 2.6365,
        "grad_norm": 1.9213663339614868,
        "learning_rate": 0.0001298424481067772,
        "epoch": 0.8018481257917878,
        "step": 10760
    },
    {
        "loss": 2.4344,
        "grad_norm": 4.236944198608398,
        "learning_rate": 0.00012977528138834977,
        "epoch": 0.8019226469930695,
        "step": 10761
    },
    {
        "loss": 2.7024,
        "grad_norm": 3.238008499145508,
        "learning_rate": 0.00012970809992720534,
        "epoch": 0.8019971681943513,
        "step": 10762
    },
    {
        "loss": 2.1637,
        "grad_norm": 3.6124355792999268,
        "learning_rate": 0.00012964090375660806,
        "epoch": 0.802071689395633,
        "step": 10763
    },
    {
        "loss": 2.396,
        "grad_norm": 4.027233600616455,
        "learning_rate": 0.00012957369290982876,
        "epoch": 0.8021462105969148,
        "step": 10764
    },
    {
        "loss": 2.527,
        "grad_norm": 3.174192190170288,
        "learning_rate": 0.00012950646742014566,
        "epoch": 0.8022207317981966,
        "step": 10765
    },
    {
        "loss": 2.2751,
        "grad_norm": 3.537365198135376,
        "learning_rate": 0.0001294392273208446,
        "epoch": 0.8022952529994783,
        "step": 10766
    },
    {
        "loss": 1.7981,
        "grad_norm": 4.614078521728516,
        "learning_rate": 0.00012937197264521787,
        "epoch": 0.8023697742007602,
        "step": 10767
    },
    {
        "loss": 2.2992,
        "grad_norm": 3.780766487121582,
        "learning_rate": 0.00012930470342656595,
        "epoch": 0.8024442954020419,
        "step": 10768
    },
    {
        "loss": 2.306,
        "grad_norm": 2.1608898639678955,
        "learning_rate": 0.00012923741969819572,
        "epoch": 0.8025188166033237,
        "step": 10769
    },
    {
        "loss": 2.3182,
        "grad_norm": 4.689015865325928,
        "learning_rate": 0.0001291701214934216,
        "epoch": 0.8025933378046054,
        "step": 10770
    },
    {
        "loss": 2.6207,
        "grad_norm": 2.2843856811523438,
        "learning_rate": 0.00012910280884556526,
        "epoch": 0.8026678590058872,
        "step": 10771
    },
    {
        "loss": 2.2249,
        "grad_norm": 2.6680734157562256,
        "learning_rate": 0.0001290354817879552,
        "epoch": 0.8027423802071689,
        "step": 10772
    },
    {
        "loss": 3.0818,
        "grad_norm": 2.5163211822509766,
        "learning_rate": 0.00012896814035392754,
        "epoch": 0.8028169014084507,
        "step": 10773
    },
    {
        "loss": 2.5672,
        "grad_norm": 3.264758348464966,
        "learning_rate": 0.00012890078457682507,
        "epoch": 0.8028914226097325,
        "step": 10774
    },
    {
        "loss": 1.9093,
        "grad_norm": 2.230672597885132,
        "learning_rate": 0.00012883341448999782,
        "epoch": 0.8029659438110143,
        "step": 10775
    },
    {
        "loss": 3.1456,
        "grad_norm": 2.708760976791382,
        "learning_rate": 0.0001287660301268028,
        "epoch": 0.803040465012296,
        "step": 10776
    },
    {
        "loss": 2.7777,
        "grad_norm": 2.357785701751709,
        "learning_rate": 0.0001286986315206045,
        "epoch": 0.8031149862135778,
        "step": 10777
    },
    {
        "loss": 2.6576,
        "grad_norm": 2.5082614421844482,
        "learning_rate": 0.00012863121870477403,
        "epoch": 0.8031895074148595,
        "step": 10778
    },
    {
        "loss": 1.8018,
        "grad_norm": 3.053962230682373,
        "learning_rate": 0.0001285637917126897,
        "epoch": 0.8032640286161413,
        "step": 10779
    },
    {
        "loss": 1.9064,
        "grad_norm": 3.4245200157165527,
        "learning_rate": 0.00012849635057773668,
        "epoch": 0.803338549817423,
        "step": 10780
    },
    {
        "loss": 2.4094,
        "grad_norm": 5.155368328094482,
        "learning_rate": 0.00012842889533330753,
        "epoch": 0.8034130710187048,
        "step": 10781
    },
    {
        "loss": 2.967,
        "grad_norm": 3.1228771209716797,
        "learning_rate": 0.0001283614260128014,
        "epoch": 0.8034875922199866,
        "step": 10782
    },
    {
        "loss": 2.131,
        "grad_norm": 2.170168876647949,
        "learning_rate": 0.0001282939426496245,
        "epoch": 0.8035621134212684,
        "step": 10783
    },
    {
        "loss": 2.5109,
        "grad_norm": 3.4588236808776855,
        "learning_rate": 0.0001282264452771903,
        "epoch": 0.8036366346225501,
        "step": 10784
    },
    {
        "loss": 2.2352,
        "grad_norm": 3.8312644958496094,
        "learning_rate": 0.00012815893392891865,
        "epoch": 0.8037111558238319,
        "step": 10785
    },
    {
        "loss": 1.7422,
        "grad_norm": 5.3522114753723145,
        "learning_rate": 0.000128091408638237,
        "epoch": 0.8037856770251136,
        "step": 10786
    },
    {
        "loss": 2.3525,
        "grad_norm": 2.268772602081299,
        "learning_rate": 0.0001280238694385789,
        "epoch": 0.8038601982263954,
        "step": 10787
    },
    {
        "loss": 2.4551,
        "grad_norm": 2.6129417419433594,
        "learning_rate": 0.00012795631636338555,
        "epoch": 0.8039347194276771,
        "step": 10788
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.9936847686767578,
        "learning_rate": 0.00012788874944610461,
        "epoch": 0.804009240628959,
        "step": 10789
    },
    {
        "loss": 1.7697,
        "grad_norm": 3.4855427742004395,
        "learning_rate": 0.00012782116872019046,
        "epoch": 0.8040837618302407,
        "step": 10790
    },
    {
        "loss": 2.1482,
        "grad_norm": 3.049149990081787,
        "learning_rate": 0.00012775357421910486,
        "epoch": 0.8041582830315225,
        "step": 10791
    },
    {
        "loss": 2.7252,
        "grad_norm": 2.311859607696533,
        "learning_rate": 0.0001276859659763159,
        "epoch": 0.8042328042328042,
        "step": 10792
    },
    {
        "loss": 2.0221,
        "grad_norm": 3.9888956546783447,
        "learning_rate": 0.00012761834402529853,
        "epoch": 0.804307325434086,
        "step": 10793
    },
    {
        "loss": 2.9724,
        "grad_norm": 3.059129476547241,
        "learning_rate": 0.0001275507083995348,
        "epoch": 0.8043818466353677,
        "step": 10794
    },
    {
        "loss": 1.6633,
        "grad_norm": 4.620616912841797,
        "learning_rate": 0.00012748305913251327,
        "epoch": 0.8044563678366495,
        "step": 10795
    },
    {
        "loss": 2.1696,
        "grad_norm": 3.492910861968994,
        "learning_rate": 0.0001274153962577291,
        "epoch": 0.8045308890379312,
        "step": 10796
    },
    {
        "loss": 2.5847,
        "grad_norm": 3.3344361782073975,
        "learning_rate": 0.00012734771980868486,
        "epoch": 0.8046054102392131,
        "step": 10797
    },
    {
        "loss": 2.7742,
        "grad_norm": 3.28391170501709,
        "learning_rate": 0.00012728002981888875,
        "epoch": 0.8046799314404948,
        "step": 10798
    },
    {
        "loss": 2.8468,
        "grad_norm": 2.626344680786133,
        "learning_rate": 0.00012721232632185685,
        "epoch": 0.8047544526417766,
        "step": 10799
    },
    {
        "loss": 2.3068,
        "grad_norm": 3.6311285495758057,
        "learning_rate": 0.00012714460935111095,
        "epoch": 0.8048289738430584,
        "step": 10800
    },
    {
        "loss": 1.704,
        "grad_norm": 3.7110400199890137,
        "learning_rate": 0.0001270768789401803,
        "epoch": 0.8049034950443401,
        "step": 10801
    },
    {
        "loss": 2.3986,
        "grad_norm": 3.2485833168029785,
        "learning_rate": 0.00012700913512260031,
        "epoch": 0.8049780162456219,
        "step": 10802
    },
    {
        "loss": 2.3558,
        "grad_norm": 2.0978505611419678,
        "learning_rate": 0.000126941377931913,
        "epoch": 0.8050525374469036,
        "step": 10803
    },
    {
        "loss": 2.964,
        "grad_norm": 1.863871693611145,
        "learning_rate": 0.0001268736074016674,
        "epoch": 0.8051270586481855,
        "step": 10804
    },
    {
        "loss": 2.4085,
        "grad_norm": 2.828281879425049,
        "learning_rate": 0.00012680582356541873,
        "epoch": 0.8052015798494672,
        "step": 10805
    },
    {
        "loss": 2.5935,
        "grad_norm": 3.0782694816589355,
        "learning_rate": 0.00012673802645672938,
        "epoch": 0.805276101050749,
        "step": 10806
    },
    {
        "loss": 1.8304,
        "grad_norm": 4.238050937652588,
        "learning_rate": 0.00012667021610916735,
        "epoch": 0.8053506222520307,
        "step": 10807
    },
    {
        "loss": 2.5254,
        "grad_norm": 3.3782641887664795,
        "learning_rate": 0.00012660239255630814,
        "epoch": 0.8054251434533125,
        "step": 10808
    },
    {
        "loss": 2.2387,
        "grad_norm": 1.7066315412521362,
        "learning_rate": 0.00012653455583173332,
        "epoch": 0.8054996646545942,
        "step": 10809
    },
    {
        "loss": 2.1231,
        "grad_norm": 2.7825145721435547,
        "learning_rate": 0.0001264667059690311,
        "epoch": 0.805574185855876,
        "step": 10810
    },
    {
        "loss": 2.132,
        "grad_norm": 2.487443685531616,
        "learning_rate": 0.000126398843001796,
        "epoch": 0.8056487070571577,
        "step": 10811
    },
    {
        "loss": 3.1287,
        "grad_norm": 4.794505596160889,
        "learning_rate": 0.00012633096696362944,
        "epoch": 0.8057232282584396,
        "step": 10812
    },
    {
        "loss": 1.9836,
        "grad_norm": 4.0872883796691895,
        "learning_rate": 0.00012626307788813886,
        "epoch": 0.8057977494597213,
        "step": 10813
    },
    {
        "loss": 2.3712,
        "grad_norm": 2.107697010040283,
        "learning_rate": 0.0001261951758089386,
        "epoch": 0.8058722706610031,
        "step": 10814
    },
    {
        "loss": 2.4047,
        "grad_norm": 2.243269920349121,
        "learning_rate": 0.00012612726075964906,
        "epoch": 0.8059467918622848,
        "step": 10815
    },
    {
        "loss": 2.4768,
        "grad_norm": 3.5231130123138428,
        "learning_rate": 0.00012605933277389704,
        "epoch": 0.8060213130635666,
        "step": 10816
    },
    {
        "loss": 2.5834,
        "grad_norm": 2.6810364723205566,
        "learning_rate": 0.0001259913918853164,
        "epoch": 0.8060958342648483,
        "step": 10817
    },
    {
        "loss": 3.1281,
        "grad_norm": 2.032231569290161,
        "learning_rate": 0.0001259234381275463,
        "epoch": 0.8061703554661301,
        "step": 10818
    },
    {
        "loss": 1.6452,
        "grad_norm": 3.1375057697296143,
        "learning_rate": 0.00012585547153423328,
        "epoch": 0.8062448766674118,
        "step": 10819
    },
    {
        "loss": 1.9757,
        "grad_norm": 3.042053699493408,
        "learning_rate": 0.0001257874921390297,
        "epoch": 0.8063193978686937,
        "step": 10820
    },
    {
        "loss": 2.4439,
        "grad_norm": 2.9336190223693848,
        "learning_rate": 0.00012571949997559426,
        "epoch": 0.8063939190699754,
        "step": 10821
    },
    {
        "loss": 2.6268,
        "grad_norm": 2.0691535472869873,
        "learning_rate": 0.00012565149507759237,
        "epoch": 0.8064684402712572,
        "step": 10822
    },
    {
        "loss": 2.634,
        "grad_norm": 2.0049543380737305,
        "learning_rate": 0.00012558347747869522,
        "epoch": 0.8065429614725389,
        "step": 10823
    },
    {
        "loss": 2.4579,
        "grad_norm": 3.1553962230682373,
        "learning_rate": 0.00012551544721258075,
        "epoch": 0.8066174826738207,
        "step": 10824
    },
    {
        "loss": 2.5984,
        "grad_norm": 3.055166244506836,
        "learning_rate": 0.00012544740431293295,
        "epoch": 0.8066920038751024,
        "step": 10825
    },
    {
        "loss": 2.1152,
        "grad_norm": 3.2687344551086426,
        "learning_rate": 0.000125379348813442,
        "epoch": 0.8067665250763842,
        "step": 10826
    },
    {
        "loss": 1.9339,
        "grad_norm": 4.162350654602051,
        "learning_rate": 0.00012531128074780428,
        "epoch": 0.806841046277666,
        "step": 10827
    },
    {
        "loss": 2.8455,
        "grad_norm": 2.285646677017212,
        "learning_rate": 0.00012524320014972287,
        "epoch": 0.8069155674789478,
        "step": 10828
    },
    {
        "loss": 1.7164,
        "grad_norm": 3.2135698795318604,
        "learning_rate": 0.0001251751070529065,
        "epoch": 0.8069900886802295,
        "step": 10829
    },
    {
        "loss": 2.8976,
        "grad_norm": 2.2762880325317383,
        "learning_rate": 0.00012510700149107027,
        "epoch": 0.8070646098815113,
        "step": 10830
    },
    {
        "loss": 1.6967,
        "grad_norm": 4.020068645477295,
        "learning_rate": 0.00012503888349793538,
        "epoch": 0.807139131082793,
        "step": 10831
    },
    {
        "loss": 2.1784,
        "grad_norm": 2.6307082176208496,
        "learning_rate": 0.0001249707531072295,
        "epoch": 0.8072136522840748,
        "step": 10832
    },
    {
        "loss": 1.5857,
        "grad_norm": 2.6894025802612305,
        "learning_rate": 0.00012490261035268607,
        "epoch": 0.8072881734853565,
        "step": 10833
    },
    {
        "loss": 2.5234,
        "grad_norm": 3.7606005668640137,
        "learning_rate": 0.00012483445526804493,
        "epoch": 0.8073626946866383,
        "step": 10834
    },
    {
        "loss": 0.9134,
        "grad_norm": 4.5276312828063965,
        "learning_rate": 0.00012476628788705182,
        "epoch": 0.8074372158879202,
        "step": 10835
    },
    {
        "loss": 2.579,
        "grad_norm": 2.6102211475372314,
        "learning_rate": 0.00012469810824345849,
        "epoch": 0.8075117370892019,
        "step": 10836
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.530668258666992,
        "learning_rate": 0.00012462991637102338,
        "epoch": 0.8075862582904837,
        "step": 10837
    },
    {
        "loss": 2.5609,
        "grad_norm": 1.9910262823104858,
        "learning_rate": 0.00012456171230350988,
        "epoch": 0.8076607794917654,
        "step": 10838
    },
    {
        "loss": 2.8512,
        "grad_norm": 2.6303298473358154,
        "learning_rate": 0.00012449349607468853,
        "epoch": 0.8077353006930472,
        "step": 10839
    },
    {
        "loss": 2.4591,
        "grad_norm": 4.0076494216918945,
        "learning_rate": 0.00012442526771833527,
        "epoch": 0.8078098218943289,
        "step": 10840
    },
    {
        "loss": 2.7125,
        "grad_norm": 2.2223167419433594,
        "learning_rate": 0.00012435702726823204,
        "epoch": 0.8078843430956107,
        "step": 10841
    },
    {
        "loss": 2.0355,
        "grad_norm": 3.1374313831329346,
        "learning_rate": 0.0001242887747581672,
        "epoch": 0.8079588642968925,
        "step": 10842
    },
    {
        "loss": 2.446,
        "grad_norm": 3.551347017288208,
        "learning_rate": 0.0001242205102219347,
        "epoch": 0.8080333854981743,
        "step": 10843
    },
    {
        "loss": 2.1744,
        "grad_norm": 2.5829813480377197,
        "learning_rate": 0.00012415223369333436,
        "epoch": 0.808107906699456,
        "step": 10844
    },
    {
        "loss": 2.1724,
        "grad_norm": 3.069293260574341,
        "learning_rate": 0.0001240839452061724,
        "epoch": 0.8081824279007378,
        "step": 10845
    },
    {
        "loss": 2.6632,
        "grad_norm": 2.3799004554748535,
        "learning_rate": 0.0001240156447942606,
        "epoch": 0.8082569491020195,
        "step": 10846
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.470560312271118,
        "learning_rate": 0.00012394733249141657,
        "epoch": 0.8083314703033013,
        "step": 10847
    },
    {
        "loss": 2.3886,
        "grad_norm": 2.4137394428253174,
        "learning_rate": 0.00012387900833146434,
        "epoch": 0.808405991504583,
        "step": 10848
    },
    {
        "loss": 1.7771,
        "grad_norm": 4.271541595458984,
        "learning_rate": 0.00012381067234823296,
        "epoch": 0.8084805127058649,
        "step": 10849
    },
    {
        "loss": 2.6878,
        "grad_norm": 4.253304958343506,
        "learning_rate": 0.00012374232457555817,
        "epoch": 0.8085550339071466,
        "step": 10850
    },
    {
        "loss": 1.5861,
        "grad_norm": 3.718329906463623,
        "learning_rate": 0.00012367396504728092,
        "epoch": 0.8086295551084284,
        "step": 10851
    },
    {
        "loss": 2.4226,
        "grad_norm": 2.4816365242004395,
        "learning_rate": 0.0001236055937972486,
        "epoch": 0.8087040763097101,
        "step": 10852
    },
    {
        "loss": 1.4924,
        "grad_norm": 3.133054256439209,
        "learning_rate": 0.00012353721085931377,
        "epoch": 0.8087785975109919,
        "step": 10853
    },
    {
        "loss": 2.2039,
        "grad_norm": 2.104280471801758,
        "learning_rate": 0.00012346881626733506,
        "epoch": 0.8088531187122736,
        "step": 10854
    },
    {
        "loss": 2.4037,
        "grad_norm": 3.513775587081909,
        "learning_rate": 0.00012340041005517706,
        "epoch": 0.8089276399135554,
        "step": 10855
    },
    {
        "loss": 2.3235,
        "grad_norm": 3.6739957332611084,
        "learning_rate": 0.00012333199225670976,
        "epoch": 0.8090021611148371,
        "step": 10856
    },
    {
        "loss": 2.2939,
        "grad_norm": 4.799971103668213,
        "learning_rate": 0.00012326356290580932,
        "epoch": 0.809076682316119,
        "step": 10857
    },
    {
        "loss": 2.2264,
        "grad_norm": 2.3084888458251953,
        "learning_rate": 0.0001231951220363569,
        "epoch": 0.8091512035174007,
        "step": 10858
    },
    {
        "loss": 2.3827,
        "grad_norm": 3.822789192199707,
        "learning_rate": 0.0001231266696822402,
        "epoch": 0.8092257247186825,
        "step": 10859
    },
    {
        "loss": 1.9619,
        "grad_norm": 4.466624736785889,
        "learning_rate": 0.00012305820587735209,
        "epoch": 0.8093002459199642,
        "step": 10860
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.4669785499572754,
        "learning_rate": 0.0001229897306555911,
        "epoch": 0.809374767121246,
        "step": 10861
    },
    {
        "loss": 1.9803,
        "grad_norm": 3.2531681060791016,
        "learning_rate": 0.0001229212440508618,
        "epoch": 0.8094492883225277,
        "step": 10862
    },
    {
        "loss": 2.5253,
        "grad_norm": 2.1226673126220703,
        "learning_rate": 0.0001228527460970741,
        "epoch": 0.8095238095238095,
        "step": 10863
    },
    {
        "loss": 2.3749,
        "grad_norm": 3.940314292907715,
        "learning_rate": 0.00012278423682814338,
        "epoch": 0.8095983307250912,
        "step": 10864
    },
    {
        "loss": 1.8619,
        "grad_norm": 4.2783684730529785,
        "learning_rate": 0.00012271571627799107,
        "epoch": 0.8096728519263731,
        "step": 10865
    },
    {
        "loss": 0.7247,
        "grad_norm": 3.9382853507995605,
        "learning_rate": 0.00012264718448054383,
        "epoch": 0.8097473731276548,
        "step": 10866
    },
    {
        "loss": 2.9788,
        "grad_norm": 3.851616859436035,
        "learning_rate": 0.0001225786414697339,
        "epoch": 0.8098218943289366,
        "step": 10867
    },
    {
        "loss": 2.285,
        "grad_norm": 3.5330936908721924,
        "learning_rate": 0.00012251008727949948,
        "epoch": 0.8098964155302183,
        "step": 10868
    },
    {
        "loss": 2.3989,
        "grad_norm": 3.20828914642334,
        "learning_rate": 0.00012244152194378357,
        "epoch": 0.8099709367315001,
        "step": 10869
    },
    {
        "loss": 2.6568,
        "grad_norm": 2.7311811447143555,
        "learning_rate": 0.00012237294549653544,
        "epoch": 0.8100454579327819,
        "step": 10870
    },
    {
        "loss": 1.5713,
        "grad_norm": 2.0817601680755615,
        "learning_rate": 0.0001223043579717093,
        "epoch": 0.8101199791340636,
        "step": 10871
    },
    {
        "loss": 2.2077,
        "grad_norm": 2.87849760055542,
        "learning_rate": 0.0001222357594032653,
        "epoch": 0.8101945003353455,
        "step": 10872
    },
    {
        "loss": 2.8034,
        "grad_norm": 2.771484375,
        "learning_rate": 0.00012216714982516878,
        "epoch": 0.8102690215366272,
        "step": 10873
    },
    {
        "loss": 2.5732,
        "grad_norm": 3.851161479949951,
        "learning_rate": 0.0001220985292713904,
        "epoch": 0.810343542737909,
        "step": 10874
    },
    {
        "loss": 2.3987,
        "grad_norm": 4.664026737213135,
        "learning_rate": 0.00012202989777590672,
        "epoch": 0.8104180639391907,
        "step": 10875
    },
    {
        "loss": 2.3645,
        "grad_norm": 2.9888994693756104,
        "learning_rate": 0.00012196125537269929,
        "epoch": 0.8104925851404725,
        "step": 10876
    },
    {
        "loss": 2.4002,
        "grad_norm": 2.4302940368652344,
        "learning_rate": 0.00012189260209575524,
        "epoch": 0.8105671063417542,
        "step": 10877
    },
    {
        "loss": 2.8361,
        "grad_norm": 2.7520110607147217,
        "learning_rate": 0.00012182393797906695,
        "epoch": 0.810641627543036,
        "step": 10878
    },
    {
        "loss": 1.8709,
        "grad_norm": 4.787796497344971,
        "learning_rate": 0.00012175526305663246,
        "epoch": 0.8107161487443177,
        "step": 10879
    },
    {
        "loss": 2.4731,
        "grad_norm": 2.898589849472046,
        "learning_rate": 0.00012168657736245488,
        "epoch": 0.8107906699455996,
        "step": 10880
    },
    {
        "loss": 1.8636,
        "grad_norm": 3.676947593688965,
        "learning_rate": 0.00012161788093054278,
        "epoch": 0.8108651911468813,
        "step": 10881
    },
    {
        "loss": 1.952,
        "grad_norm": 3.1135365962982178,
        "learning_rate": 0.00012154917379490984,
        "epoch": 0.8109397123481631,
        "step": 10882
    },
    {
        "loss": 2.3764,
        "grad_norm": 2.7562003135681152,
        "learning_rate": 0.00012148045598957547,
        "epoch": 0.8110142335494448,
        "step": 10883
    },
    {
        "loss": 2.124,
        "grad_norm": 3.337160348892212,
        "learning_rate": 0.00012141172754856387,
        "epoch": 0.8110887547507266,
        "step": 10884
    },
    {
        "loss": 2.7988,
        "grad_norm": 2.36563777923584,
        "learning_rate": 0.00012134298850590497,
        "epoch": 0.8111632759520083,
        "step": 10885
    },
    {
        "loss": 1.6147,
        "grad_norm": 3.9209794998168945,
        "learning_rate": 0.00012127423889563365,
        "epoch": 0.8112377971532901,
        "step": 10886
    },
    {
        "loss": 2.3013,
        "grad_norm": 4.693916320800781,
        "learning_rate": 0.0001212054787517899,
        "epoch": 0.8113123183545718,
        "step": 10887
    },
    {
        "loss": 3.0675,
        "grad_norm": 4.301969051361084,
        "learning_rate": 0.00012113670810841953,
        "epoch": 0.8113868395558537,
        "step": 10888
    },
    {
        "loss": 2.4056,
        "grad_norm": 3.155768871307373,
        "learning_rate": 0.00012106792699957263,
        "epoch": 0.8114613607571354,
        "step": 10889
    },
    {
        "loss": 2.2817,
        "grad_norm": 2.853181838989258,
        "learning_rate": 0.00012099913545930536,
        "epoch": 0.8115358819584172,
        "step": 10890
    },
    {
        "loss": 2.7051,
        "grad_norm": 3.5087227821350098,
        "learning_rate": 0.00012093033352167853,
        "epoch": 0.8116104031596989,
        "step": 10891
    },
    {
        "loss": 1.7712,
        "grad_norm": 3.521383047103882,
        "learning_rate": 0.00012086152122075812,
        "epoch": 0.8116849243609807,
        "step": 10892
    },
    {
        "loss": 2.4583,
        "grad_norm": 3.6213576793670654,
        "learning_rate": 0.00012079269859061559,
        "epoch": 0.8117594455622624,
        "step": 10893
    },
    {
        "loss": 2.1429,
        "grad_norm": 4.103930473327637,
        "learning_rate": 0.00012072386566532704,
        "epoch": 0.8118339667635442,
        "step": 10894
    },
    {
        "loss": 2.0406,
        "grad_norm": 2.976282835006714,
        "learning_rate": 0.00012065502247897412,
        "epoch": 0.811908487964826,
        "step": 10895
    },
    {
        "loss": 2.73,
        "grad_norm": 1.9874670505523682,
        "learning_rate": 0.00012058616906564327,
        "epoch": 0.8119830091661078,
        "step": 10896
    },
    {
        "loss": 2.8932,
        "grad_norm": 2.2893271446228027,
        "learning_rate": 0.00012051730545942606,
        "epoch": 0.8120575303673895,
        "step": 10897
    },
    {
        "loss": 2.399,
        "grad_norm": 3.8131420612335205,
        "learning_rate": 0.00012044843169441892,
        "epoch": 0.8121320515686713,
        "step": 10898
    },
    {
        "loss": 2.1581,
        "grad_norm": 3.0352840423583984,
        "learning_rate": 0.00012037954780472387,
        "epoch": 0.812206572769953,
        "step": 10899
    },
    {
        "loss": 2.874,
        "grad_norm": 2.5293970108032227,
        "learning_rate": 0.00012031065382444737,
        "epoch": 0.8122810939712348,
        "step": 10900
    },
    {
        "loss": 1.9766,
        "grad_norm": 2.200792074203491,
        "learning_rate": 0.00012024174978770115,
        "epoch": 0.8123556151725165,
        "step": 10901
    },
    {
        "loss": 2.6611,
        "grad_norm": 3.2112414836883545,
        "learning_rate": 0.00012017283572860168,
        "epoch": 0.8124301363737984,
        "step": 10902
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.2113540172576904,
        "learning_rate": 0.00012010391168127083,
        "epoch": 0.8125046575750801,
        "step": 10903
    },
    {
        "loss": 2.1773,
        "grad_norm": 3.903261184692383,
        "learning_rate": 0.00012003497767983502,
        "epoch": 0.8125791787763619,
        "step": 10904
    },
    {
        "loss": 1.8385,
        "grad_norm": 3.908658981323242,
        "learning_rate": 0.00011996603375842562,
        "epoch": 0.8126536999776436,
        "step": 10905
    },
    {
        "loss": 2.677,
        "grad_norm": 3.1888418197631836,
        "learning_rate": 0.00011989707995117927,
        "epoch": 0.8127282211789254,
        "step": 10906
    },
    {
        "loss": 2.1313,
        "grad_norm": 3.031329393386841,
        "learning_rate": 0.00011982811629223701,
        "epoch": 0.8128027423802072,
        "step": 10907
    },
    {
        "loss": 2.3329,
        "grad_norm": 2.018925428390503,
        "learning_rate": 0.00011975914281574538,
        "epoch": 0.8128772635814889,
        "step": 10908
    },
    {
        "loss": 2.425,
        "grad_norm": 2.019803047180176,
        "learning_rate": 0.00011969015955585487,
        "epoch": 0.8129517847827707,
        "step": 10909
    },
    {
        "loss": 1.9615,
        "grad_norm": 3.3585641384124756,
        "learning_rate": 0.00011962116654672179,
        "epoch": 0.8130263059840525,
        "step": 10910
    },
    {
        "loss": 2.4511,
        "grad_norm": 3.811812400817871,
        "learning_rate": 0.00011955216382250669,
        "epoch": 0.8131008271853343,
        "step": 10911
    },
    {
        "loss": 2.3547,
        "grad_norm": 4.120816230773926,
        "learning_rate": 0.00011948315141737494,
        "epoch": 0.813175348386616,
        "step": 10912
    },
    {
        "loss": 2.4521,
        "grad_norm": 3.6875431537628174,
        "learning_rate": 0.0001194141293654971,
        "epoch": 0.8132498695878978,
        "step": 10913
    },
    {
        "loss": 1.9897,
        "grad_norm": 2.0170936584472656,
        "learning_rate": 0.00011934509770104809,
        "epoch": 0.8133243907891795,
        "step": 10914
    },
    {
        "loss": 2.6648,
        "grad_norm": 3.7176122665405273,
        "learning_rate": 0.00011927605645820767,
        "epoch": 0.8133989119904613,
        "step": 10915
    },
    {
        "loss": 2.4325,
        "grad_norm": 2.2750866413116455,
        "learning_rate": 0.00011920700567116067,
        "epoch": 0.813473433191743,
        "step": 10916
    },
    {
        "loss": 1.9702,
        "grad_norm": 3.0629677772521973,
        "learning_rate": 0.00011913794537409623,
        "epoch": 0.8135479543930249,
        "step": 10917
    },
    {
        "loss": 2.8503,
        "grad_norm": 2.402787446975708,
        "learning_rate": 0.00011906887560120827,
        "epoch": 0.8136224755943066,
        "step": 10918
    },
    {
        "loss": 2.9456,
        "grad_norm": 1.5000962018966675,
        "learning_rate": 0.00011899979638669589,
        "epoch": 0.8136969967955884,
        "step": 10919
    },
    {
        "loss": 2.5366,
        "grad_norm": 2.269080400466919,
        "learning_rate": 0.00011893070776476192,
        "epoch": 0.8137715179968701,
        "step": 10920
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.707411766052246,
        "learning_rate": 0.0001188616097696148,
        "epoch": 0.8138460391981519,
        "step": 10921
    },
    {
        "loss": 2.4158,
        "grad_norm": 3.6976871490478516,
        "learning_rate": 0.00011879250243546694,
        "epoch": 0.8139205603994336,
        "step": 10922
    },
    {
        "loss": 2.443,
        "grad_norm": 3.3151438236236572,
        "learning_rate": 0.00011872338579653595,
        "epoch": 0.8139950816007154,
        "step": 10923
    },
    {
        "loss": 2.7461,
        "grad_norm": 3.3567910194396973,
        "learning_rate": 0.00011865425988704355,
        "epoch": 0.8140696028019971,
        "step": 10924
    },
    {
        "loss": 1.9032,
        "grad_norm": 3.4879393577575684,
        "learning_rate": 0.00011858512474121615,
        "epoch": 0.814144124003279,
        "step": 10925
    },
    {
        "loss": 2.1087,
        "grad_norm": 3.820625066757202,
        "learning_rate": 0.00011851598039328507,
        "epoch": 0.8142186452045607,
        "step": 10926
    },
    {
        "loss": 2.6569,
        "grad_norm": 2.688351631164551,
        "learning_rate": 0.00011844682687748567,
        "epoch": 0.8142931664058425,
        "step": 10927
    },
    {
        "loss": 2.2089,
        "grad_norm": 3.239931106567383,
        "learning_rate": 0.00011837766422805856,
        "epoch": 0.8143676876071242,
        "step": 10928
    },
    {
        "loss": 2.6574,
        "grad_norm": 3.7865207195281982,
        "learning_rate": 0.00011830849247924787,
        "epoch": 0.814442208808406,
        "step": 10929
    },
    {
        "loss": 1.9544,
        "grad_norm": 3.720597505569458,
        "learning_rate": 0.00011823931166530318,
        "epoch": 0.8145167300096877,
        "step": 10930
    },
    {
        "loss": 2.1003,
        "grad_norm": 2.439595937728882,
        "learning_rate": 0.00011817012182047809,
        "epoch": 0.8145912512109695,
        "step": 10931
    },
    {
        "loss": 2.194,
        "grad_norm": 3.624063014984131,
        "learning_rate": 0.00011810092297903058,
        "epoch": 0.8146657724122512,
        "step": 10932
    },
    {
        "loss": 2.5547,
        "grad_norm": 1.586395502090454,
        "learning_rate": 0.00011803171517522351,
        "epoch": 0.8147402936135331,
        "step": 10933
    },
    {
        "loss": 2.0708,
        "grad_norm": 3.8755486011505127,
        "learning_rate": 0.00011796249844332385,
        "epoch": 0.8148148148148148,
        "step": 10934
    },
    {
        "loss": 1.8569,
        "grad_norm": 4.584102153778076,
        "learning_rate": 0.00011789327281760296,
        "epoch": 0.8148893360160966,
        "step": 10935
    },
    {
        "loss": 2.4045,
        "grad_norm": 1.6537601947784424,
        "learning_rate": 0.00011782403833233689,
        "epoch": 0.8149638572173783,
        "step": 10936
    },
    {
        "loss": 2.5862,
        "grad_norm": 2.6954092979431152,
        "learning_rate": 0.00011775479502180582,
        "epoch": 0.8150383784186601,
        "step": 10937
    },
    {
        "loss": 2.4676,
        "grad_norm": 1.9106847047805786,
        "learning_rate": 0.00011768554292029426,
        "epoch": 0.8151128996199418,
        "step": 10938
    },
    {
        "loss": 1.7843,
        "grad_norm": 6.250659942626953,
        "learning_rate": 0.00011761628206209159,
        "epoch": 0.8151874208212236,
        "step": 10939
    },
    {
        "loss": 2.5003,
        "grad_norm": 2.4286997318267822,
        "learning_rate": 0.00011754701248149056,
        "epoch": 0.8152619420225053,
        "step": 10940
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.4498865604400635,
        "learning_rate": 0.00011747773421278923,
        "epoch": 0.8153364632237872,
        "step": 10941
    },
    {
        "loss": 2.2241,
        "grad_norm": 2.9289066791534424,
        "learning_rate": 0.00011740844729028941,
        "epoch": 0.815410984425069,
        "step": 10942
    },
    {
        "loss": 2.0961,
        "grad_norm": 4.763106822967529,
        "learning_rate": 0.00011733915174829719,
        "epoch": 0.8154855056263507,
        "step": 10943
    },
    {
        "loss": 1.0609,
        "grad_norm": 2.4553208351135254,
        "learning_rate": 0.00011726984762112332,
        "epoch": 0.8155600268276325,
        "step": 10944
    },
    {
        "loss": 2.4952,
        "grad_norm": 2.607123613357544,
        "learning_rate": 0.00011720053494308225,
        "epoch": 0.8156345480289142,
        "step": 10945
    },
    {
        "loss": 2.5417,
        "grad_norm": 3.0883264541625977,
        "learning_rate": 0.00011713121374849331,
        "epoch": 0.815709069230196,
        "step": 10946
    },
    {
        "loss": 2.4136,
        "grad_norm": 2.065962553024292,
        "learning_rate": 0.00011706188407167947,
        "epoch": 0.8157835904314777,
        "step": 10947
    },
    {
        "loss": 1.9847,
        "grad_norm": 2.942190408706665,
        "learning_rate": 0.00011699254594696816,
        "epoch": 0.8158581116327596,
        "step": 10948
    },
    {
        "loss": 2.4082,
        "grad_norm": 2.5216970443725586,
        "learning_rate": 0.0001169231994086908,
        "epoch": 0.8159326328340413,
        "step": 10949
    },
    {
        "loss": 3.4069,
        "grad_norm": 5.943499565124512,
        "learning_rate": 0.00011685384449118346,
        "epoch": 0.8160071540353231,
        "step": 10950
    },
    {
        "loss": 2.4065,
        "grad_norm": 3.882715940475464,
        "learning_rate": 0.00011678448122878582,
        "epoch": 0.8160816752366048,
        "step": 10951
    },
    {
        "loss": 2.078,
        "grad_norm": 4.574718475341797,
        "learning_rate": 0.00011671510965584194,
        "epoch": 0.8161561964378866,
        "step": 10952
    },
    {
        "loss": 1.5414,
        "grad_norm": 3.974858283996582,
        "learning_rate": 0.00011664572980669983,
        "epoch": 0.8162307176391683,
        "step": 10953
    },
    {
        "loss": 2.6507,
        "grad_norm": 2.7177765369415283,
        "learning_rate": 0.00011657634171571199,
        "epoch": 0.8163052388404501,
        "step": 10954
    },
    {
        "loss": 3.1506,
        "grad_norm": 3.360133647918701,
        "learning_rate": 0.00011650694541723446,
        "epoch": 0.8163797600417319,
        "step": 10955
    },
    {
        "loss": 2.5939,
        "grad_norm": 2.0862789154052734,
        "learning_rate": 0.00011643754094562786,
        "epoch": 0.8164542812430137,
        "step": 10956
    },
    {
        "loss": 1.2784,
        "grad_norm": 1.4079854488372803,
        "learning_rate": 0.00011636812833525656,
        "epoch": 0.8165288024442954,
        "step": 10957
    },
    {
        "loss": 1.9144,
        "grad_norm": 3.768050193786621,
        "learning_rate": 0.0001162987076204888,
        "epoch": 0.8166033236455772,
        "step": 10958
    },
    {
        "loss": 1.7233,
        "grad_norm": 2.7767436504364014,
        "learning_rate": 0.00011622927883569748,
        "epoch": 0.8166778448468589,
        "step": 10959
    },
    {
        "loss": 2.4434,
        "grad_norm": 1.6635971069335938,
        "learning_rate": 0.00011615984201525854,
        "epoch": 0.8167523660481407,
        "step": 10960
    },
    {
        "loss": 2.7114,
        "grad_norm": 3.7865982055664062,
        "learning_rate": 0.00011609039719355284,
        "epoch": 0.8168268872494224,
        "step": 10961
    },
    {
        "loss": 1.8374,
        "grad_norm": 4.690967559814453,
        "learning_rate": 0.00011602094440496461,
        "epoch": 0.8169014084507042,
        "step": 10962
    },
    {
        "loss": 2.7468,
        "grad_norm": 2.5783584117889404,
        "learning_rate": 0.00011595148368388209,
        "epoch": 0.816975929651986,
        "step": 10963
    },
    {
        "loss": 2.3305,
        "grad_norm": 3.320005178451538,
        "learning_rate": 0.00011588201506469781,
        "epoch": 0.8170504508532678,
        "step": 10964
    },
    {
        "loss": 2.5438,
        "grad_norm": 2.5944676399230957,
        "learning_rate": 0.00011581253858180771,
        "epoch": 0.8171249720545495,
        "step": 10965
    },
    {
        "loss": 1.9383,
        "grad_norm": 3.1281235218048096,
        "learning_rate": 0.00011574305426961214,
        "epoch": 0.8171994932558313,
        "step": 10966
    },
    {
        "loss": 1.8049,
        "grad_norm": 3.4947123527526855,
        "learning_rate": 0.0001156735621625149,
        "epoch": 0.817274014457113,
        "step": 10967
    },
    {
        "loss": 2.5997,
        "grad_norm": 2.1764485836029053,
        "learning_rate": 0.00011560406229492382,
        "epoch": 0.8173485356583948,
        "step": 10968
    },
    {
        "loss": 2.7429,
        "grad_norm": 3.3833258152008057,
        "learning_rate": 0.00011553455470125045,
        "epoch": 0.8174230568596765,
        "step": 10969
    },
    {
        "loss": 2.2425,
        "grad_norm": 3.9743871688842773,
        "learning_rate": 0.00011546503941591065,
        "epoch": 0.8174975780609584,
        "step": 10970
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.038931369781494,
        "learning_rate": 0.00011539551647332316,
        "epoch": 0.8175720992622401,
        "step": 10971
    },
    {
        "loss": 2.0562,
        "grad_norm": 2.986492156982422,
        "learning_rate": 0.00011532598590791153,
        "epoch": 0.8176466204635219,
        "step": 10972
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.7506535053253174,
        "learning_rate": 0.00011525644775410229,
        "epoch": 0.8177211416648036,
        "step": 10973
    },
    {
        "loss": 2.7701,
        "grad_norm": 2.6398277282714844,
        "learning_rate": 0.00011518690204632634,
        "epoch": 0.8177956628660854,
        "step": 10974
    },
    {
        "loss": 1.3562,
        "grad_norm": 1.895818829536438,
        "learning_rate": 0.0001151173488190179,
        "epoch": 0.8178701840673671,
        "step": 10975
    },
    {
        "loss": 2.664,
        "grad_norm": 3.85075306892395,
        "learning_rate": 0.00011504778810661496,
        "epoch": 0.8179447052686489,
        "step": 10976
    },
    {
        "loss": 1.3315,
        "grad_norm": 3.4325990676879883,
        "learning_rate": 0.00011497821994355952,
        "epoch": 0.8180192264699307,
        "step": 10977
    },
    {
        "loss": 2.5601,
        "grad_norm": 2.6303534507751465,
        "learning_rate": 0.00011490864436429683,
        "epoch": 0.8180937476712125,
        "step": 10978
    },
    {
        "loss": 2.5067,
        "grad_norm": 3.712127923965454,
        "learning_rate": 0.0001148390614032764,
        "epoch": 0.8181682688724943,
        "step": 10979
    },
    {
        "loss": 1.6067,
        "grad_norm": 4.124317169189453,
        "learning_rate": 0.00011476947109495054,
        "epoch": 0.818242790073776,
        "step": 10980
    },
    {
        "loss": 1.7364,
        "grad_norm": 2.6786530017852783,
        "learning_rate": 0.00011469987347377604,
        "epoch": 0.8183173112750578,
        "step": 10981
    },
    {
        "loss": 2.8159,
        "grad_norm": 2.5331692695617676,
        "learning_rate": 0.00011463026857421286,
        "epoch": 0.8183918324763395,
        "step": 10982
    },
    {
        "loss": 2.3786,
        "grad_norm": 2.93979549407959,
        "learning_rate": 0.00011456065643072453,
        "epoch": 0.8184663536776213,
        "step": 10983
    },
    {
        "loss": 2.6651,
        "grad_norm": 2.410061836242676,
        "learning_rate": 0.00011449103707777858,
        "epoch": 0.818540874878903,
        "step": 10984
    },
    {
        "loss": 1.9606,
        "grad_norm": 3.223151683807373,
        "learning_rate": 0.00011442141054984566,
        "epoch": 0.8186153960801849,
        "step": 10985
    },
    {
        "loss": 2.4546,
        "grad_norm": 3.6124889850616455,
        "learning_rate": 0.00011435177688140002,
        "epoch": 0.8186899172814666,
        "step": 10986
    },
    {
        "loss": 2.9293,
        "grad_norm": 4.276942729949951,
        "learning_rate": 0.00011428213610691987,
        "epoch": 0.8187644384827484,
        "step": 10987
    },
    {
        "loss": 2.3633,
        "grad_norm": 2.7880473136901855,
        "learning_rate": 0.00011421248826088648,
        "epoch": 0.8188389596840301,
        "step": 10988
    },
    {
        "loss": 1.978,
        "grad_norm": 2.6108667850494385,
        "learning_rate": 0.00011414283337778465,
        "epoch": 0.8189134808853119,
        "step": 10989
    },
    {
        "loss": 2.6412,
        "grad_norm": 4.876520156860352,
        "learning_rate": 0.0001140731714921032,
        "epoch": 0.8189880020865936,
        "step": 10990
    },
    {
        "loss": 2.5635,
        "grad_norm": 3.9815258979797363,
        "learning_rate": 0.00011400350263833346,
        "epoch": 0.8190625232878754,
        "step": 10991
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.4658961296081543,
        "learning_rate": 0.00011393382685097116,
        "epoch": 0.8191370444891571,
        "step": 10992
    },
    {
        "loss": 2.7948,
        "grad_norm": 2.7262113094329834,
        "learning_rate": 0.0001138641441645148,
        "epoch": 0.819211565690439,
        "step": 10993
    },
    {
        "loss": 2.1549,
        "grad_norm": 3.615327835083008,
        "learning_rate": 0.00011379445461346685,
        "epoch": 0.8192860868917207,
        "step": 10994
    },
    {
        "loss": 2.5664,
        "grad_norm": 2.292161464691162,
        "learning_rate": 0.00011372475823233278,
        "epoch": 0.8193606080930025,
        "step": 10995
    },
    {
        "loss": 2.1267,
        "grad_norm": 3.25938081741333,
        "learning_rate": 0.00011365505505562131,
        "epoch": 0.8194351292942842,
        "step": 10996
    },
    {
        "loss": 2.2964,
        "grad_norm": 3.1637344360351562,
        "learning_rate": 0.00011358534511784511,
        "epoch": 0.819509650495566,
        "step": 10997
    },
    {
        "loss": 1.8945,
        "grad_norm": 3.713890314102173,
        "learning_rate": 0.00011351562845351973,
        "epoch": 0.8195841716968477,
        "step": 10998
    },
    {
        "loss": 2.5471,
        "grad_norm": 5.474730014801025,
        "learning_rate": 0.00011344590509716415,
        "epoch": 0.8196586928981295,
        "step": 10999
    },
    {
        "loss": 2.6585,
        "grad_norm": 2.261384963989258,
        "learning_rate": 0.00011337617508330054,
        "epoch": 0.8197332140994112,
        "step": 11000
    },
    {
        "loss": 2.2356,
        "grad_norm": 3.8047409057617188,
        "learning_rate": 0.00011330643844645479,
        "epoch": 0.8198077353006931,
        "step": 11001
    },
    {
        "loss": 2.569,
        "grad_norm": 3.4309844970703125,
        "learning_rate": 0.00011323669522115568,
        "epoch": 0.8198822565019748,
        "step": 11002
    },
    {
        "loss": 1.8761,
        "grad_norm": 3.3236701488494873,
        "learning_rate": 0.00011316694544193533,
        "epoch": 0.8199567777032566,
        "step": 11003
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.7989628314971924,
        "learning_rate": 0.00011309718914332903,
        "epoch": 0.8200312989045383,
        "step": 11004
    },
    {
        "loss": 2.1462,
        "grad_norm": 3.682171583175659,
        "learning_rate": 0.00011302742635987567,
        "epoch": 0.8201058201058201,
        "step": 11005
    },
    {
        "loss": 1.7272,
        "grad_norm": 3.6881697177886963,
        "learning_rate": 0.00011295765712611681,
        "epoch": 0.8201803413071018,
        "step": 11006
    },
    {
        "loss": 0.9754,
        "grad_norm": 2.4021456241607666,
        "learning_rate": 0.00011288788147659779,
        "epoch": 0.8202548625083836,
        "step": 11007
    },
    {
        "loss": 2.7472,
        "grad_norm": 3.113006114959717,
        "learning_rate": 0.00011281809944586661,
        "epoch": 0.8203293837096653,
        "step": 11008
    },
    {
        "loss": 2.8128,
        "grad_norm": 3.0984997749328613,
        "learning_rate": 0.00011274831106847457,
        "epoch": 0.8204039049109472,
        "step": 11009
    },
    {
        "loss": 2.2713,
        "grad_norm": 3.233347177505493,
        "learning_rate": 0.00011267851637897657,
        "epoch": 0.8204784261122289,
        "step": 11010
    },
    {
        "loss": 4.1218,
        "grad_norm": 5.651248931884766,
        "learning_rate": 0.0001126087154119297,
        "epoch": 0.8205529473135107,
        "step": 11011
    },
    {
        "loss": 2.632,
        "grad_norm": 2.4661154747009277,
        "learning_rate": 0.00011253890820189508,
        "epoch": 0.8206274685147925,
        "step": 11012
    },
    {
        "loss": 1.9425,
        "grad_norm": 3.0222008228302,
        "learning_rate": 0.00011246909478343645,
        "epoch": 0.8207019897160742,
        "step": 11013
    },
    {
        "loss": 2.3963,
        "grad_norm": 3.966801643371582,
        "learning_rate": 0.00011239927519112059,
        "epoch": 0.820776510917356,
        "step": 11014
    },
    {
        "loss": 2.8884,
        "grad_norm": 2.39927339553833,
        "learning_rate": 0.00011232944945951771,
        "epoch": 0.8208510321186377,
        "step": 11015
    },
    {
        "loss": 2.3454,
        "grad_norm": 3.693013906478882,
        "learning_rate": 0.00011225961762320055,
        "epoch": 0.8209255533199196,
        "step": 11016
    },
    {
        "loss": 2.6679,
        "grad_norm": 2.8149819374084473,
        "learning_rate": 0.0001121897797167454,
        "epoch": 0.8210000745212013,
        "step": 11017
    },
    {
        "loss": 2.35,
        "grad_norm": 2.2002604007720947,
        "learning_rate": 0.00011211993577473121,
        "epoch": 0.8210745957224831,
        "step": 11018
    },
    {
        "loss": 1.6451,
        "grad_norm": 3.4062387943267822,
        "learning_rate": 0.00011205008583173995,
        "epoch": 0.8211491169237648,
        "step": 11019
    },
    {
        "loss": 2.585,
        "grad_norm": 4.0194220542907715,
        "learning_rate": 0.00011198022992235649,
        "epoch": 0.8212236381250466,
        "step": 11020
    },
    {
        "loss": 1.3953,
        "grad_norm": 3.2674522399902344,
        "learning_rate": 0.00011191036808116904,
        "epoch": 0.8212981593263283,
        "step": 11021
    },
    {
        "loss": 2.5711,
        "grad_norm": 3.5847651958465576,
        "learning_rate": 0.00011184050034276838,
        "epoch": 0.8213726805276101,
        "step": 11022
    },
    {
        "loss": 2.8113,
        "grad_norm": 2.3621416091918945,
        "learning_rate": 0.0001117706267417483,
        "epoch": 0.8214472017288919,
        "step": 11023
    },
    {
        "loss": 2.2452,
        "grad_norm": 3.2125155925750732,
        "learning_rate": 0.00011170074731270536,
        "epoch": 0.8215217229301737,
        "step": 11024
    },
    {
        "loss": 2.749,
        "grad_norm": 1.9684667587280273,
        "learning_rate": 0.00011163086209023945,
        "epoch": 0.8215962441314554,
        "step": 11025
    },
    {
        "loss": 2.3838,
        "grad_norm": 2.024779796600342,
        "learning_rate": 0.00011156097110895274,
        "epoch": 0.8216707653327372,
        "step": 11026
    },
    {
        "loss": 2.8013,
        "grad_norm": 2.856628179550171,
        "learning_rate": 0.00011149107440345081,
        "epoch": 0.8217452865340189,
        "step": 11027
    },
    {
        "loss": 2.3129,
        "grad_norm": 3.0091867446899414,
        "learning_rate": 0.00011142117200834168,
        "epoch": 0.8218198077353007,
        "step": 11028
    },
    {
        "loss": 2.7481,
        "grad_norm": 2.496368408203125,
        "learning_rate": 0.00011135126395823618,
        "epoch": 0.8218943289365824,
        "step": 11029
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.0521719455718994,
        "learning_rate": 0.00011128135028774851,
        "epoch": 0.8219688501378642,
        "step": 11030
    },
    {
        "loss": 2.0207,
        "grad_norm": 2.953909397125244,
        "learning_rate": 0.00011121143103149462,
        "epoch": 0.822043371339146,
        "step": 11031
    },
    {
        "loss": 2.1603,
        "grad_norm": 3.646040201187134,
        "learning_rate": 0.00011114150622409427,
        "epoch": 0.8221178925404278,
        "step": 11032
    },
    {
        "loss": 1.8197,
        "grad_norm": 2.9467713832855225,
        "learning_rate": 0.00011107157590016938,
        "epoch": 0.8221924137417095,
        "step": 11033
    },
    {
        "loss": 1.9473,
        "grad_norm": 3.8581740856170654,
        "learning_rate": 0.0001110016400943446,
        "epoch": 0.8222669349429913,
        "step": 11034
    },
    {
        "loss": 2.584,
        "grad_norm": 2.474839925765991,
        "learning_rate": 0.0001109316988412477,
        "epoch": 0.822341456144273,
        "step": 11035
    },
    {
        "loss": 2.5132,
        "grad_norm": 1.6269505023956299,
        "learning_rate": 0.00011086175217550878,
        "epoch": 0.8224159773455548,
        "step": 11036
    },
    {
        "loss": 1.9859,
        "grad_norm": 2.894413471221924,
        "learning_rate": 0.00011079180013176053,
        "epoch": 0.8224904985468365,
        "step": 11037
    },
    {
        "loss": 2.5504,
        "grad_norm": 2.5358030796051025,
        "learning_rate": 0.00011072184274463884,
        "epoch": 0.8225650197481184,
        "step": 11038
    },
    {
        "loss": 2.2203,
        "grad_norm": 4.664856433868408,
        "learning_rate": 0.00011065188004878171,
        "epoch": 0.8226395409494001,
        "step": 11039
    },
    {
        "loss": 2.6757,
        "grad_norm": 3.577425718307495,
        "learning_rate": 0.00011058191207882987,
        "epoch": 0.8227140621506819,
        "step": 11040
    },
    {
        "loss": 2.3978,
        "grad_norm": 2.560413122177124,
        "learning_rate": 0.00011051193886942712,
        "epoch": 0.8227885833519636,
        "step": 11041
    },
    {
        "loss": 2.2532,
        "grad_norm": 3.365159511566162,
        "learning_rate": 0.00011044196045521897,
        "epoch": 0.8228631045532454,
        "step": 11042
    },
    {
        "loss": 1.4057,
        "grad_norm": 3.4612181186676025,
        "learning_rate": 0.00011037197687085444,
        "epoch": 0.8229376257545271,
        "step": 11043
    },
    {
        "loss": 1.4929,
        "grad_norm": 3.9230332374572754,
        "learning_rate": 0.00011030198815098438,
        "epoch": 0.8230121469558089,
        "step": 11044
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.2304489612579346,
        "learning_rate": 0.00011023199433026283,
        "epoch": 0.8230866681570906,
        "step": 11045
    },
    {
        "loss": 2.0175,
        "grad_norm": 4.026343822479248,
        "learning_rate": 0.00011016199544334584,
        "epoch": 0.8231611893583725,
        "step": 11046
    },
    {
        "loss": 2.7822,
        "grad_norm": 3.5888259410858154,
        "learning_rate": 0.00011009199152489203,
        "epoch": 0.8232357105596543,
        "step": 11047
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.4085843563079834,
        "learning_rate": 0.00011002198260956291,
        "epoch": 0.823310231760936,
        "step": 11048
    },
    {
        "loss": 2.435,
        "grad_norm": 2.9515302181243896,
        "learning_rate": 0.00010995196873202189,
        "epoch": 0.8233847529622178,
        "step": 11049
    },
    {
        "loss": 2.8252,
        "grad_norm": 3.2592101097106934,
        "learning_rate": 0.00010988194992693555,
        "epoch": 0.8234592741634995,
        "step": 11050
    },
    {
        "loss": 2.8062,
        "grad_norm": 3.0865321159362793,
        "learning_rate": 0.00010981192622897199,
        "epoch": 0.8235337953647813,
        "step": 11051
    },
    {
        "loss": 2.2112,
        "grad_norm": 2.9732906818389893,
        "learning_rate": 0.00010974189767280258,
        "epoch": 0.823608316566063,
        "step": 11052
    },
    {
        "loss": 1.9897,
        "grad_norm": 3.6505067348480225,
        "learning_rate": 0.00010967186429310066,
        "epoch": 0.8236828377673449,
        "step": 11053
    },
    {
        "loss": 1.8451,
        "grad_norm": 2.6316702365875244,
        "learning_rate": 0.00010960182612454191,
        "epoch": 0.8237573589686266,
        "step": 11054
    },
    {
        "loss": 2.1189,
        "grad_norm": 2.540364980697632,
        "learning_rate": 0.00010953178320180475,
        "epoch": 0.8238318801699084,
        "step": 11055
    },
    {
        "loss": 1.5674,
        "grad_norm": 1.9083771705627441,
        "learning_rate": 0.00010946173555956966,
        "epoch": 0.8239064013711901,
        "step": 11056
    },
    {
        "loss": 2.6851,
        "grad_norm": 3.2581238746643066,
        "learning_rate": 0.00010939168323251932,
        "epoch": 0.8239809225724719,
        "step": 11057
    },
    {
        "loss": 1.6284,
        "grad_norm": 3.1722137928009033,
        "learning_rate": 0.00010932162625533923,
        "epoch": 0.8240554437737536,
        "step": 11058
    },
    {
        "loss": 1.9254,
        "grad_norm": 3.814356565475464,
        "learning_rate": 0.00010925156466271677,
        "epoch": 0.8241299649750354,
        "step": 11059
    },
    {
        "loss": 1.7285,
        "grad_norm": 1.8251770734786987,
        "learning_rate": 0.00010918149848934159,
        "epoch": 0.8242044861763171,
        "step": 11060
    },
    {
        "loss": 1.9847,
        "grad_norm": 3.4667251110076904,
        "learning_rate": 0.0001091114277699061,
        "epoch": 0.824279007377599,
        "step": 11061
    },
    {
        "loss": 2.3015,
        "grad_norm": 3.0231237411499023,
        "learning_rate": 0.00010904135253910413,
        "epoch": 0.8243535285788807,
        "step": 11062
    },
    {
        "loss": 2.576,
        "grad_norm": 2.437547445297241,
        "learning_rate": 0.00010897127283163262,
        "epoch": 0.8244280497801625,
        "step": 11063
    },
    {
        "loss": 2.9886,
        "grad_norm": 3.1654603481292725,
        "learning_rate": 0.00010890118868219017,
        "epoch": 0.8245025709814442,
        "step": 11064
    },
    {
        "loss": 2.8593,
        "grad_norm": 3.600947380065918,
        "learning_rate": 0.00010883110012547759,
        "epoch": 0.824577092182726,
        "step": 11065
    },
    {
        "loss": 1.7836,
        "grad_norm": 3.2833783626556396,
        "learning_rate": 0.00010876100719619833,
        "epoch": 0.8246516133840077,
        "step": 11066
    },
    {
        "loss": 2.1534,
        "grad_norm": 2.363236665725708,
        "learning_rate": 0.0001086909099290574,
        "epoch": 0.8247261345852895,
        "step": 11067
    },
    {
        "loss": 1.2777,
        "grad_norm": 2.1799237728118896,
        "learning_rate": 0.00010862080835876251,
        "epoch": 0.8248006557865712,
        "step": 11068
    },
    {
        "loss": 2.7549,
        "grad_norm": 3.143099546432495,
        "learning_rate": 0.00010855070252002315,
        "epoch": 0.8248751769878531,
        "step": 11069
    },
    {
        "loss": 1.8452,
        "grad_norm": 3.6393465995788574,
        "learning_rate": 0.000108480592447551,
        "epoch": 0.8249496981891348,
        "step": 11070
    },
    {
        "loss": 2.4482,
        "grad_norm": 3.0211849212646484,
        "learning_rate": 0.00010841047817605968,
        "epoch": 0.8250242193904166,
        "step": 11071
    },
    {
        "loss": 2.4679,
        "grad_norm": 3.3405680656433105,
        "learning_rate": 0.00010834035974026535,
        "epoch": 0.8250987405916983,
        "step": 11072
    },
    {
        "loss": 1.7009,
        "grad_norm": 3.499112606048584,
        "learning_rate": 0.00010827023717488584,
        "epoch": 0.8251732617929801,
        "step": 11073
    },
    {
        "loss": 1.8877,
        "grad_norm": 6.531608581542969,
        "learning_rate": 0.00010820011051464111,
        "epoch": 0.8252477829942618,
        "step": 11074
    },
    {
        "loss": 2.2,
        "grad_norm": 2.8925302028656006,
        "learning_rate": 0.00010812997979425301,
        "epoch": 0.8253223041955436,
        "step": 11075
    },
    {
        "loss": 2.0734,
        "grad_norm": 3.858258008956909,
        "learning_rate": 0.00010805984504844587,
        "epoch": 0.8253968253968254,
        "step": 11076
    },
    {
        "loss": 1.7366,
        "grad_norm": 3.6973981857299805,
        "learning_rate": 0.00010798970631194536,
        "epoch": 0.8254713465981072,
        "step": 11077
    },
    {
        "loss": 2.0205,
        "grad_norm": 3.8572678565979004,
        "learning_rate": 0.00010791956361947979,
        "epoch": 0.8255458677993889,
        "step": 11078
    },
    {
        "loss": 2.2886,
        "grad_norm": 3.7843446731567383,
        "learning_rate": 0.00010784941700577899,
        "epoch": 0.8256203890006707,
        "step": 11079
    },
    {
        "loss": 2.7618,
        "grad_norm": 3.02451491355896,
        "learning_rate": 0.00010777926650557463,
        "epoch": 0.8256949102019524,
        "step": 11080
    },
    {
        "loss": 1.4176,
        "grad_norm": 2.747361660003662,
        "learning_rate": 0.000107709112153601,
        "epoch": 0.8257694314032342,
        "step": 11081
    },
    {
        "loss": 2.2805,
        "grad_norm": 4.100750923156738,
        "learning_rate": 0.00010763895398459327,
        "epoch": 0.8258439526045159,
        "step": 11082
    },
    {
        "loss": 2.0948,
        "grad_norm": 4.129035949707031,
        "learning_rate": 0.00010756879203328937,
        "epoch": 0.8259184738057977,
        "step": 11083
    },
    {
        "loss": 1.9659,
        "grad_norm": 4.150139808654785,
        "learning_rate": 0.00010749862633442876,
        "epoch": 0.8259929950070796,
        "step": 11084
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.820974826812744,
        "learning_rate": 0.00010742845692275254,
        "epoch": 0.8260675162083613,
        "step": 11085
    },
    {
        "loss": 2.1108,
        "grad_norm": 3.911506414413452,
        "learning_rate": 0.00010735828383300423,
        "epoch": 0.8261420374096431,
        "step": 11086
    },
    {
        "loss": 2.1508,
        "grad_norm": 2.698421001434326,
        "learning_rate": 0.00010728810709992849,
        "epoch": 0.8262165586109248,
        "step": 11087
    },
    {
        "loss": 2.4691,
        "grad_norm": 3.213771104812622,
        "learning_rate": 0.00010721792675827245,
        "epoch": 0.8262910798122066,
        "step": 11088
    },
    {
        "loss": 1.6482,
        "grad_norm": 1.9350346326828003,
        "learning_rate": 0.0001071477428427845,
        "epoch": 0.8263656010134883,
        "step": 11089
    },
    {
        "loss": 1.9152,
        "grad_norm": 2.700545310974121,
        "learning_rate": 0.00010707755538821506,
        "epoch": 0.8264401222147701,
        "step": 11090
    },
    {
        "loss": 2.4348,
        "grad_norm": 3.929476261138916,
        "learning_rate": 0.00010700736442931604,
        "epoch": 0.8265146434160519,
        "step": 11091
    },
    {
        "loss": 1.913,
        "grad_norm": 3.4517157077789307,
        "learning_rate": 0.00010693717000084171,
        "epoch": 0.8265891646173337,
        "step": 11092
    },
    {
        "loss": 2.6603,
        "grad_norm": 2.785372257232666,
        "learning_rate": 0.00010686697213754713,
        "epoch": 0.8266636858186154,
        "step": 11093
    },
    {
        "loss": 1.9382,
        "grad_norm": 4.476788520812988,
        "learning_rate": 0.00010679677087418989,
        "epoch": 0.8267382070198972,
        "step": 11094
    },
    {
        "loss": 1.8254,
        "grad_norm": 3.611928939819336,
        "learning_rate": 0.00010672656624552873,
        "epoch": 0.8268127282211789,
        "step": 11095
    },
    {
        "loss": 2.3593,
        "grad_norm": 2.2934930324554443,
        "learning_rate": 0.00010665635828632448,
        "epoch": 0.8268872494224607,
        "step": 11096
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.903787851333618,
        "learning_rate": 0.0001065861470313393,
        "epoch": 0.8269617706237424,
        "step": 11097
    },
    {
        "loss": 1.6345,
        "grad_norm": 1.9858930110931396,
        "learning_rate": 0.00010651593251533695,
        "epoch": 0.8270362918250243,
        "step": 11098
    },
    {
        "loss": 1.6542,
        "grad_norm": 4.457174777984619,
        "learning_rate": 0.00010644571477308316,
        "epoch": 0.827110813026306,
        "step": 11099
    },
    {
        "loss": 2.6134,
        "grad_norm": 3.3578429222106934,
        "learning_rate": 0.00010637549383934482,
        "epoch": 0.8271853342275878,
        "step": 11100
    },
    {
        "loss": 2.1831,
        "grad_norm": 3.494696855545044,
        "learning_rate": 0.000106305269748891,
        "epoch": 0.8272598554288695,
        "step": 11101
    },
    {
        "loss": 2.2639,
        "grad_norm": 3.0066637992858887,
        "learning_rate": 0.00010623504253649137,
        "epoch": 0.8273343766301513,
        "step": 11102
    },
    {
        "loss": 2.557,
        "grad_norm": 2.514160394668579,
        "learning_rate": 0.00010616481223691817,
        "epoch": 0.827408897831433,
        "step": 11103
    },
    {
        "loss": 2.2526,
        "grad_norm": 3.2856156826019287,
        "learning_rate": 0.00010609457888494458,
        "epoch": 0.8274834190327148,
        "step": 11104
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.1896421909332275,
        "learning_rate": 0.00010602434251534528,
        "epoch": 0.8275579402339965,
        "step": 11105
    },
    {
        "loss": 1.532,
        "grad_norm": 2.4367263317108154,
        "learning_rate": 0.00010595410316289687,
        "epoch": 0.8276324614352784,
        "step": 11106
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.4235241413116455,
        "learning_rate": 0.00010588386086237703,
        "epoch": 0.8277069826365601,
        "step": 11107
    },
    {
        "loss": 1.8021,
        "grad_norm": 3.4589719772338867,
        "learning_rate": 0.00010581361564856487,
        "epoch": 0.8277815038378419,
        "step": 11108
    },
    {
        "loss": 1.7256,
        "grad_norm": 3.539227247238159,
        "learning_rate": 0.00010574336755624142,
        "epoch": 0.8278560250391236,
        "step": 11109
    },
    {
        "loss": 2.6412,
        "grad_norm": 2.3276801109313965,
        "learning_rate": 0.00010567311662018859,
        "epoch": 0.8279305462404054,
        "step": 11110
    },
    {
        "loss": 0.8393,
        "grad_norm": 3.5352258682250977,
        "learning_rate": 0.00010560286287518983,
        "epoch": 0.8280050674416871,
        "step": 11111
    },
    {
        "loss": 1.5596,
        "grad_norm": 3.071693181991577,
        "learning_rate": 0.00010553260635603051,
        "epoch": 0.8280795886429689,
        "step": 11112
    },
    {
        "loss": 2.4641,
        "grad_norm": 2.076655626296997,
        "learning_rate": 0.00010546234709749632,
        "epoch": 0.8281541098442506,
        "step": 11113
    },
    {
        "loss": 1.3325,
        "grad_norm": 4.817837238311768,
        "learning_rate": 0.00010539208513437538,
        "epoch": 0.8282286310455325,
        "step": 11114
    },
    {
        "loss": 2.0797,
        "grad_norm": 1.7716885805130005,
        "learning_rate": 0.00010532182050145639,
        "epoch": 0.8283031522468142,
        "step": 11115
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.9160821437835693,
        "learning_rate": 0.00010525155323352994,
        "epoch": 0.828377673448096,
        "step": 11116
    },
    {
        "loss": 2.6149,
        "grad_norm": 2.8271541595458984,
        "learning_rate": 0.00010518128336538753,
        "epoch": 0.8284521946493777,
        "step": 11117
    },
    {
        "loss": 1.6967,
        "grad_norm": 2.937687397003174,
        "learning_rate": 0.00010511101093182194,
        "epoch": 0.8285267158506595,
        "step": 11118
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.6467862129211426,
        "learning_rate": 0.0001050407359676276,
        "epoch": 0.8286012370519413,
        "step": 11119
    },
    {
        "loss": 1.9493,
        "grad_norm": 3.53403377532959,
        "learning_rate": 0.00010497045850759972,
        "epoch": 0.828675758253223,
        "step": 11120
    },
    {
        "loss": 2.036,
        "grad_norm": 3.2064011096954346,
        "learning_rate": 0.00010490017858653536,
        "epoch": 0.8287502794545049,
        "step": 11121
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.793667793273926,
        "learning_rate": 0.00010482989623923185,
        "epoch": 0.8288248006557866,
        "step": 11122
    },
    {
        "loss": 2.5192,
        "grad_norm": 2.1643147468566895,
        "learning_rate": 0.00010475961150048868,
        "epoch": 0.8288993218570684,
        "step": 11123
    },
    {
        "loss": 2.5514,
        "grad_norm": 2.5576910972595215,
        "learning_rate": 0.00010468932440510604,
        "epoch": 0.8289738430583501,
        "step": 11124
    },
    {
        "loss": 2.358,
        "grad_norm": 2.433475971221924,
        "learning_rate": 0.00010461903498788532,
        "epoch": 0.8290483642596319,
        "step": 11125
    },
    {
        "loss": 2.5399,
        "grad_norm": 3.499274730682373,
        "learning_rate": 0.000104548743283629,
        "epoch": 0.8291228854609136,
        "step": 11126
    },
    {
        "loss": 2.7753,
        "grad_norm": 2.6239848136901855,
        "learning_rate": 0.00010447844932714108,
        "epoch": 0.8291974066621954,
        "step": 11127
    },
    {
        "loss": 2.7621,
        "grad_norm": 2.9542477130889893,
        "learning_rate": 0.00010440815315322612,
        "epoch": 0.8292719278634771,
        "step": 11128
    },
    {
        "loss": 2.698,
        "grad_norm": 2.751268148422241,
        "learning_rate": 0.00010433785479669034,
        "epoch": 0.829346449064759,
        "step": 11129
    },
    {
        "loss": 2.4504,
        "grad_norm": 3.309248208999634,
        "learning_rate": 0.00010426755429234058,
        "epoch": 0.8294209702660407,
        "step": 11130
    },
    {
        "loss": 1.7519,
        "grad_norm": 2.612133264541626,
        "learning_rate": 0.00010419725167498484,
        "epoch": 0.8294954914673225,
        "step": 11131
    },
    {
        "loss": 2.7749,
        "grad_norm": 3.524878978729248,
        "learning_rate": 0.00010412694697943264,
        "epoch": 0.8295700126686042,
        "step": 11132
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.985215902328491,
        "learning_rate": 0.00010405664024049358,
        "epoch": 0.829644533869886,
        "step": 11133
    },
    {
        "loss": 1.8146,
        "grad_norm": 4.496453285217285,
        "learning_rate": 0.00010398633149297923,
        "epoch": 0.8297190550711677,
        "step": 11134
    },
    {
        "loss": 3.2148,
        "grad_norm": 2.3121657371520996,
        "learning_rate": 0.00010391602077170166,
        "epoch": 0.8297935762724495,
        "step": 11135
    },
    {
        "loss": 2.7279,
        "grad_norm": 3.1539828777313232,
        "learning_rate": 0.00010384570811147383,
        "epoch": 0.8298680974737312,
        "step": 11136
    },
    {
        "loss": 2.3659,
        "grad_norm": 3.0997154712677,
        "learning_rate": 0.00010377539354711012,
        "epoch": 0.8299426186750131,
        "step": 11137
    },
    {
        "loss": 2.0106,
        "grad_norm": 3.334876775741577,
        "learning_rate": 0.0001037050771134253,
        "epoch": 0.8300171398762948,
        "step": 11138
    },
    {
        "loss": 2.5127,
        "grad_norm": 2.8788535594940186,
        "learning_rate": 0.00010363475884523566,
        "epoch": 0.8300916610775766,
        "step": 11139
    },
    {
        "loss": 2.8424,
        "grad_norm": 1.2990052700042725,
        "learning_rate": 0.00010356443877735786,
        "epoch": 0.8301661822788583,
        "step": 11140
    },
    {
        "loss": 2.032,
        "grad_norm": 3.7877862453460693,
        "learning_rate": 0.00010349411694460971,
        "epoch": 0.8302407034801401,
        "step": 11141
    },
    {
        "loss": 2.5225,
        "grad_norm": 3.0998082160949707,
        "learning_rate": 0.00010342379338180971,
        "epoch": 0.8303152246814218,
        "step": 11142
    },
    {
        "loss": 2.0276,
        "grad_norm": 3.4533042907714844,
        "learning_rate": 0.00010335346812377768,
        "epoch": 0.8303897458827036,
        "step": 11143
    },
    {
        "loss": 2.3179,
        "grad_norm": 5.840789794921875,
        "learning_rate": 0.00010328314120533373,
        "epoch": 0.8304642670839854,
        "step": 11144
    },
    {
        "loss": 2.2978,
        "grad_norm": 3.3463690280914307,
        "learning_rate": 0.0001032128126612991,
        "epoch": 0.8305387882852672,
        "step": 11145
    },
    {
        "loss": 2.7461,
        "grad_norm": 3.427992343902588,
        "learning_rate": 0.00010314248252649555,
        "epoch": 0.8306133094865489,
        "step": 11146
    },
    {
        "loss": 2.7341,
        "grad_norm": 2.408844470977783,
        "learning_rate": 0.00010307215083574617,
        "epoch": 0.8306878306878307,
        "step": 11147
    },
    {
        "loss": 2.4335,
        "grad_norm": 3.4838876724243164,
        "learning_rate": 0.00010300181762387418,
        "epoch": 0.8307623518891124,
        "step": 11148
    },
    {
        "loss": 2.4757,
        "grad_norm": 2.8474087715148926,
        "learning_rate": 0.00010293148292570411,
        "epoch": 0.8308368730903942,
        "step": 11149
    },
    {
        "loss": 2.4871,
        "grad_norm": 3.545989513397217,
        "learning_rate": 0.00010286114677606089,
        "epoch": 0.8309113942916759,
        "step": 11150
    },
    {
        "loss": 2.7264,
        "grad_norm": 2.365370988845825,
        "learning_rate": 0.00010279080920977004,
        "epoch": 0.8309859154929577,
        "step": 11151
    },
    {
        "loss": 2.7537,
        "grad_norm": 3.0923829078674316,
        "learning_rate": 0.00010272047026165845,
        "epoch": 0.8310604366942395,
        "step": 11152
    },
    {
        "loss": 2.6155,
        "grad_norm": 3.2296533584594727,
        "learning_rate": 0.0001026501299665527,
        "epoch": 0.8311349578955213,
        "step": 11153
    },
    {
        "loss": 2.733,
        "grad_norm": 3.055992603302002,
        "learning_rate": 0.00010257978835928097,
        "epoch": 0.8312094790968031,
        "step": 11154
    },
    {
        "loss": 1.664,
        "grad_norm": 4.020088195800781,
        "learning_rate": 0.00010250944547467152,
        "epoch": 0.8312840002980848,
        "step": 11155
    },
    {
        "loss": 2.4041,
        "grad_norm": 2.042248249053955,
        "learning_rate": 0.00010243910134755333,
        "epoch": 0.8313585214993666,
        "step": 11156
    },
    {
        "loss": 1.8484,
        "grad_norm": 3.0768492221832275,
        "learning_rate": 0.00010236875601275632,
        "epoch": 0.8314330427006483,
        "step": 11157
    },
    {
        "loss": 1.974,
        "grad_norm": 2.958218812942505,
        "learning_rate": 0.00010229840950511068,
        "epoch": 0.8315075639019301,
        "step": 11158
    },
    {
        "loss": 2.2287,
        "grad_norm": 3.2904064655303955,
        "learning_rate": 0.00010222806185944706,
        "epoch": 0.8315820851032119,
        "step": 11159
    },
    {
        "loss": 2.5579,
        "grad_norm": 2.2593092918395996,
        "learning_rate": 0.00010215771311059723,
        "epoch": 0.8316566063044937,
        "step": 11160
    },
    {
        "loss": 2.1218,
        "grad_norm": 3.002890110015869,
        "learning_rate": 0.00010208736329339297,
        "epoch": 0.8317311275057754,
        "step": 11161
    },
    {
        "loss": 1.7266,
        "grad_norm": 2.4775147438049316,
        "learning_rate": 0.00010201701244266666,
        "epoch": 0.8318056487070572,
        "step": 11162
    },
    {
        "loss": 2.7736,
        "grad_norm": 3.379755973815918,
        "learning_rate": 0.0001019466605932517,
        "epoch": 0.8318801699083389,
        "step": 11163
    },
    {
        "loss": 1.8364,
        "grad_norm": 3.918755054473877,
        "learning_rate": 0.00010187630777998109,
        "epoch": 0.8319546911096207,
        "step": 11164
    },
    {
        "loss": 1.652,
        "grad_norm": 2.458115577697754,
        "learning_rate": 0.0001018059540376892,
        "epoch": 0.8320292123109024,
        "step": 11165
    },
    {
        "loss": 2.4425,
        "grad_norm": 3.59759521484375,
        "learning_rate": 0.00010173559940121019,
        "epoch": 0.8321037335121843,
        "step": 11166
    },
    {
        "loss": 2.2775,
        "grad_norm": 3.4150264263153076,
        "learning_rate": 0.00010166524390537922,
        "epoch": 0.832178254713466,
        "step": 11167
    },
    {
        "loss": 1.7943,
        "grad_norm": 3.220069646835327,
        "learning_rate": 0.00010159488758503144,
        "epoch": 0.8322527759147478,
        "step": 11168
    },
    {
        "loss": 2.5065,
        "grad_norm": 3.5996055603027344,
        "learning_rate": 0.00010152453047500248,
        "epoch": 0.8323272971160295,
        "step": 11169
    },
    {
        "loss": 2.1058,
        "grad_norm": 3.725693464279175,
        "learning_rate": 0.00010145417261012873,
        "epoch": 0.8324018183173113,
        "step": 11170
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.8542107343673706,
        "learning_rate": 0.00010138381402524633,
        "epoch": 0.832476339518593,
        "step": 11171
    },
    {
        "loss": 1.9684,
        "grad_norm": 3.0051662921905518,
        "learning_rate": 0.00010131345475519258,
        "epoch": 0.8325508607198748,
        "step": 11172
    },
    {
        "loss": 1.9092,
        "grad_norm": 2.5164456367492676,
        "learning_rate": 0.00010124309483480409,
        "epoch": 0.8326253819211565,
        "step": 11173
    },
    {
        "loss": 2.501,
        "grad_norm": 2.23810076713562,
        "learning_rate": 0.0001011727342989188,
        "epoch": 0.8326999031224384,
        "step": 11174
    },
    {
        "loss": 2.2447,
        "grad_norm": 2.738831043243408,
        "learning_rate": 0.00010110237318237435,
        "epoch": 0.8327744243237201,
        "step": 11175
    },
    {
        "loss": 2.5375,
        "grad_norm": 2.4674201011657715,
        "learning_rate": 0.00010103201152000869,
        "epoch": 0.8328489455250019,
        "step": 11176
    },
    {
        "loss": 2.7988,
        "grad_norm": 3.3386003971099854,
        "learning_rate": 0.0001009616493466605,
        "epoch": 0.8329234667262836,
        "step": 11177
    },
    {
        "loss": 2.4719,
        "grad_norm": 2.4983956813812256,
        "learning_rate": 0.00010089128669716821,
        "epoch": 0.8329979879275654,
        "step": 11178
    },
    {
        "loss": 2.487,
        "grad_norm": 2.5311946868896484,
        "learning_rate": 0.00010082092360637057,
        "epoch": 0.8330725091288471,
        "step": 11179
    },
    {
        "loss": 2.3156,
        "grad_norm": 3.4770586490631104,
        "learning_rate": 0.00010075056010910688,
        "epoch": 0.8331470303301289,
        "step": 11180
    },
    {
        "loss": 2.051,
        "grad_norm": 3.7508490085601807,
        "learning_rate": 0.00010068019624021634,
        "epoch": 0.8332215515314106,
        "step": 11181
    },
    {
        "loss": 1.9099,
        "grad_norm": 3.2716686725616455,
        "learning_rate": 0.0001006098320345382,
        "epoch": 0.8332960727326925,
        "step": 11182
    },
    {
        "loss": 2.5436,
        "grad_norm": 2.961278200149536,
        "learning_rate": 0.00010053946752691256,
        "epoch": 0.8333705939339742,
        "step": 11183
    },
    {
        "loss": 1.9285,
        "grad_norm": 3.498181104660034,
        "learning_rate": 0.00010046910275217858,
        "epoch": 0.833445115135256,
        "step": 11184
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.667285919189453,
        "learning_rate": 0.0001003987377451766,
        "epoch": 0.8335196363365377,
        "step": 11185
    },
    {
        "loss": 2.8177,
        "grad_norm": 2.1601452827453613,
        "learning_rate": 0.00010032837254074653,
        "epoch": 0.8335941575378195,
        "step": 11186
    },
    {
        "loss": 2.3126,
        "grad_norm": 3.3156487941741943,
        "learning_rate": 0.0001002580071737283,
        "epoch": 0.8336686787391012,
        "step": 11187
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.1256983280181885,
        "learning_rate": 0.00010018764167896247,
        "epoch": 0.833743199940383,
        "step": 11188
    },
    {
        "loss": 2.6778,
        "grad_norm": 2.864845037460327,
        "learning_rate": 0.00010011727609128889,
        "epoch": 0.8338177211416649,
        "step": 11189
    },
    {
        "loss": 2.2327,
        "grad_norm": 2.4001617431640625,
        "learning_rate": 0.00010004691044554824,
        "epoch": 0.8338922423429466,
        "step": 11190
    },
    {
        "loss": 2.6432,
        "grad_norm": 2.9826037883758545,
        "learning_rate": 9.997654477658069e-05,
        "epoch": 0.8339667635442284,
        "step": 11191
    },
    {
        "loss": 1.6643,
        "grad_norm": 2.3287229537963867,
        "learning_rate": 9.99061791192266e-05,
        "epoch": 0.8340412847455101,
        "step": 11192
    },
    {
        "loss": 2.8543,
        "grad_norm": 2.4155688285827637,
        "learning_rate": 9.983581350832616e-05,
        "epoch": 0.8341158059467919,
        "step": 11193
    },
    {
        "loss": 2.9982,
        "grad_norm": 2.6240854263305664,
        "learning_rate": 9.976544797871996e-05,
        "epoch": 0.8341903271480736,
        "step": 11194
    },
    {
        "loss": 2.8182,
        "grad_norm": 4.210667133331299,
        "learning_rate": 9.969508256524816e-05,
        "epoch": 0.8342648483493554,
        "step": 11195
    },
    {
        "loss": 2.7979,
        "grad_norm": 2.7697091102600098,
        "learning_rate": 9.962471730275095e-05,
        "epoch": 0.8343393695506371,
        "step": 11196
    },
    {
        "loss": 1.515,
        "grad_norm": 3.279205799102783,
        "learning_rate": 9.955435222606834e-05,
        "epoch": 0.834413890751919,
        "step": 11197
    },
    {
        "loss": 2.7428,
        "grad_norm": 1.8630341291427612,
        "learning_rate": 9.94839873700407e-05,
        "epoch": 0.8344884119532007,
        "step": 11198
    },
    {
        "loss": 2.4674,
        "grad_norm": 2.387112617492676,
        "learning_rate": 9.941362276950765e-05,
        "epoch": 0.8345629331544825,
        "step": 11199
    },
    {
        "loss": 2.7295,
        "grad_norm": 4.026368141174316,
        "learning_rate": 9.934325845930933e-05,
        "epoch": 0.8346374543557642,
        "step": 11200
    },
    {
        "loss": 2.5234,
        "grad_norm": 2.8931028842926025,
        "learning_rate": 9.927289447428524e-05,
        "epoch": 0.834711975557046,
        "step": 11201
    },
    {
        "loss": 1.5855,
        "grad_norm": 4.499475479125977,
        "learning_rate": 9.920253084927475e-05,
        "epoch": 0.8347864967583277,
        "step": 11202
    },
    {
        "loss": 2.8117,
        "grad_norm": 3.0482630729675293,
        "learning_rate": 9.913216761911765e-05,
        "epoch": 0.8348610179596095,
        "step": 11203
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.928783655166626,
        "learning_rate": 9.90618048186525e-05,
        "epoch": 0.8349355391608912,
        "step": 11204
    },
    {
        "loss": 2.8118,
        "grad_norm": 2.32853364944458,
        "learning_rate": 9.899144248271863e-05,
        "epoch": 0.8350100603621731,
        "step": 11205
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.7100706100463867,
        "learning_rate": 9.892108064615461e-05,
        "epoch": 0.8350845815634548,
        "step": 11206
    },
    {
        "loss": 1.7053,
        "grad_norm": 3.8232104778289795,
        "learning_rate": 9.885071934379876e-05,
        "epoch": 0.8351591027647366,
        "step": 11207
    },
    {
        "loss": 1.3608,
        "grad_norm": 2.3930156230926514,
        "learning_rate": 9.878035861048952e-05,
        "epoch": 0.8352336239660183,
        "step": 11208
    },
    {
        "loss": 2.7178,
        "grad_norm": 2.3811941146850586,
        "learning_rate": 9.870999848106453e-05,
        "epoch": 0.8353081451673001,
        "step": 11209
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.9549472332000732,
        "learning_rate": 9.863963899036168e-05,
        "epoch": 0.8353826663685818,
        "step": 11210
    },
    {
        "loss": 2.3642,
        "grad_norm": 1.8219916820526123,
        "learning_rate": 9.856928017321812e-05,
        "epoch": 0.8354571875698636,
        "step": 11211
    },
    {
        "loss": 2.7842,
        "grad_norm": 4.824404716491699,
        "learning_rate": 9.849892206447076e-05,
        "epoch": 0.8355317087711454,
        "step": 11212
    },
    {
        "loss": 3.1169,
        "grad_norm": 3.439513683319092,
        "learning_rate": 9.842856469895612e-05,
        "epoch": 0.8356062299724272,
        "step": 11213
    },
    {
        "loss": 2.2514,
        "grad_norm": 3.368999481201172,
        "learning_rate": 9.835820811151069e-05,
        "epoch": 0.8356807511737089,
        "step": 11214
    },
    {
        "loss": 2.5962,
        "grad_norm": 2.19526743888855,
        "learning_rate": 9.828785233697018e-05,
        "epoch": 0.8357552723749907,
        "step": 11215
    },
    {
        "loss": 2.4814,
        "grad_norm": 3.05540132522583,
        "learning_rate": 9.821749741017004e-05,
        "epoch": 0.8358297935762724,
        "step": 11216
    },
    {
        "loss": 2.9863,
        "grad_norm": 2.486989736557007,
        "learning_rate": 9.814714336594515e-05,
        "epoch": 0.8359043147775542,
        "step": 11217
    },
    {
        "loss": 2.727,
        "grad_norm": 2.7263569831848145,
        "learning_rate": 9.807679023913043e-05,
        "epoch": 0.8359788359788359,
        "step": 11218
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.925607681274414,
        "learning_rate": 9.800643806455979e-05,
        "epoch": 0.8360533571801178,
        "step": 11219
    },
    {
        "loss": 2.506,
        "grad_norm": 3.279627799987793,
        "learning_rate": 9.793608687706681e-05,
        "epoch": 0.8361278783813995,
        "step": 11220
    },
    {
        "loss": 2.297,
        "grad_norm": 2.9986183643341064,
        "learning_rate": 9.786573671148495e-05,
        "epoch": 0.8362023995826813,
        "step": 11221
    },
    {
        "loss": 2.3229,
        "grad_norm": 4.870265960693359,
        "learning_rate": 9.779538760264657e-05,
        "epoch": 0.836276920783963,
        "step": 11222
    },
    {
        "loss": 2.4703,
        "grad_norm": 2.4176156520843506,
        "learning_rate": 9.772503958538428e-05,
        "epoch": 0.8363514419852448,
        "step": 11223
    },
    {
        "loss": 2.9615,
        "grad_norm": 2.8151535987854004,
        "learning_rate": 9.765469269452908e-05,
        "epoch": 0.8364259631865266,
        "step": 11224
    },
    {
        "loss": 2.5859,
        "grad_norm": 3.428886651992798,
        "learning_rate": 9.758434696491247e-05,
        "epoch": 0.8365004843878083,
        "step": 11225
    },
    {
        "loss": 2.7477,
        "grad_norm": 3.674872636795044,
        "learning_rate": 9.751400243136478e-05,
        "epoch": 0.8365750055890901,
        "step": 11226
    },
    {
        "loss": 2.7105,
        "grad_norm": 3.2903287410736084,
        "learning_rate": 9.744365912871574e-05,
        "epoch": 0.8366495267903719,
        "step": 11227
    },
    {
        "loss": 2.55,
        "grad_norm": 3.3557348251342773,
        "learning_rate": 9.73733170917949e-05,
        "epoch": 0.8367240479916537,
        "step": 11228
    },
    {
        "loss": 2.7227,
        "grad_norm": 2.666057586669922,
        "learning_rate": 9.73029763554308e-05,
        "epoch": 0.8367985691929354,
        "step": 11229
    },
    {
        "loss": 2.5422,
        "grad_norm": 3.829049825668335,
        "learning_rate": 9.723263695445127e-05,
        "epoch": 0.8368730903942172,
        "step": 11230
    },
    {
        "loss": 2.354,
        "grad_norm": 3.0596415996551514,
        "learning_rate": 9.716229892368392e-05,
        "epoch": 0.8369476115954989,
        "step": 11231
    },
    {
        "loss": 2.1344,
        "grad_norm": 3.401970863342285,
        "learning_rate": 9.709196229795529e-05,
        "epoch": 0.8370221327967807,
        "step": 11232
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.829946756362915,
        "learning_rate": 9.702162711209122e-05,
        "epoch": 0.8370966539980624,
        "step": 11233
    },
    {
        "loss": 2.3153,
        "grad_norm": 4.035478591918945,
        "learning_rate": 9.695129340091736e-05,
        "epoch": 0.8371711751993443,
        "step": 11234
    },
    {
        "loss": 2.719,
        "grad_norm": 2.495927333831787,
        "learning_rate": 9.688096119925769e-05,
        "epoch": 0.837245696400626,
        "step": 11235
    },
    {
        "loss": 2.5316,
        "grad_norm": 3.029562473297119,
        "learning_rate": 9.68106305419364e-05,
        "epoch": 0.8373202176019078,
        "step": 11236
    },
    {
        "loss": 2.4231,
        "grad_norm": 3.2999143600463867,
        "learning_rate": 9.674030146377624e-05,
        "epoch": 0.8373947388031895,
        "step": 11237
    },
    {
        "loss": 3.1128,
        "grad_norm": 2.6202757358551025,
        "learning_rate": 9.666997399959968e-05,
        "epoch": 0.8374692600044713,
        "step": 11238
    },
    {
        "loss": 2.5182,
        "grad_norm": 3.207202911376953,
        "learning_rate": 9.659964818422801e-05,
        "epoch": 0.837543781205753,
        "step": 11239
    },
    {
        "loss": 2.3101,
        "grad_norm": 3.5964081287384033,
        "learning_rate": 9.652932405248174e-05,
        "epoch": 0.8376183024070348,
        "step": 11240
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.8832008838653564,
        "learning_rate": 9.645900163918093e-05,
        "epoch": 0.8376928236083165,
        "step": 11241
    },
    {
        "loss": 1.9559,
        "grad_norm": 1.718712329864502,
        "learning_rate": 9.638868097914421e-05,
        "epoch": 0.8377673448095984,
        "step": 11242
    },
    {
        "loss": 2.1298,
        "grad_norm": 3.7537646293640137,
        "learning_rate": 9.631836210719004e-05,
        "epoch": 0.8378418660108801,
        "step": 11243
    },
    {
        "loss": 2.3541,
        "grad_norm": 2.072746515274048,
        "learning_rate": 9.624804505813507e-05,
        "epoch": 0.8379163872121619,
        "step": 11244
    },
    {
        "loss": 2.5247,
        "grad_norm": 4.141229629516602,
        "learning_rate": 9.617772986679601e-05,
        "epoch": 0.8379909084134436,
        "step": 11245
    },
    {
        "loss": 1.6932,
        "grad_norm": 5.023311614990234,
        "learning_rate": 9.610741656798802e-05,
        "epoch": 0.8380654296147254,
        "step": 11246
    },
    {
        "loss": 2.2269,
        "grad_norm": 2.6761422157287598,
        "learning_rate": 9.603710519652543e-05,
        "epoch": 0.8381399508160071,
        "step": 11247
    },
    {
        "loss": 2.8029,
        "grad_norm": 2.7216403484344482,
        "learning_rate": 9.596679578722194e-05,
        "epoch": 0.8382144720172889,
        "step": 11248
    },
    {
        "loss": 1.7704,
        "grad_norm": 2.691070318222046,
        "learning_rate": 9.589648837488993e-05,
        "epoch": 0.8382889932185706,
        "step": 11249
    },
    {
        "loss": 2.1389,
        "grad_norm": 3.6419785022735596,
        "learning_rate": 9.582618299434074e-05,
        "epoch": 0.8383635144198525,
        "step": 11250
    },
    {
        "loss": 2.6946,
        "grad_norm": 2.450251340866089,
        "learning_rate": 9.575587968038516e-05,
        "epoch": 0.8384380356211342,
        "step": 11251
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.7038073539733887,
        "learning_rate": 9.568557846783252e-05,
        "epoch": 0.838512556822416,
        "step": 11252
    },
    {
        "loss": 2.6636,
        "grad_norm": 2.290109157562256,
        "learning_rate": 9.56152793914911e-05,
        "epoch": 0.8385870780236977,
        "step": 11253
    },
    {
        "loss": 1.5408,
        "grad_norm": 3.7280170917510986,
        "learning_rate": 9.55449824861687e-05,
        "epoch": 0.8386615992249795,
        "step": 11254
    },
    {
        "loss": 2.5152,
        "grad_norm": 2.475903272628784,
        "learning_rate": 9.547468778667108e-05,
        "epoch": 0.8387361204262612,
        "step": 11255
    },
    {
        "loss": 1.7239,
        "grad_norm": 3.7305378913879395,
        "learning_rate": 9.540439532780387e-05,
        "epoch": 0.838810641627543,
        "step": 11256
    },
    {
        "loss": 2.2971,
        "grad_norm": 3.1840600967407227,
        "learning_rate": 9.533410514437097e-05,
        "epoch": 0.8388851628288247,
        "step": 11257
    },
    {
        "loss": 2.6293,
        "grad_norm": 3.8523566722869873,
        "learning_rate": 9.526381727117528e-05,
        "epoch": 0.8389596840301066,
        "step": 11258
    },
    {
        "loss": 2.7196,
        "grad_norm": 2.6697325706481934,
        "learning_rate": 9.519353174301886e-05,
        "epoch": 0.8390342052313883,
        "step": 11259
    },
    {
        "loss": 2.8302,
        "grad_norm": 1.8661940097808838,
        "learning_rate": 9.512324859470215e-05,
        "epoch": 0.8391087264326701,
        "step": 11260
    },
    {
        "loss": 2.2145,
        "grad_norm": 3.3423609733581543,
        "learning_rate": 9.505296786102486e-05,
        "epoch": 0.8391832476339519,
        "step": 11261
    },
    {
        "loss": 3.2694,
        "grad_norm": 3.7830700874328613,
        "learning_rate": 9.498268957678515e-05,
        "epoch": 0.8392577688352336,
        "step": 11262
    },
    {
        "loss": 2.3446,
        "grad_norm": 3.280637741088867,
        "learning_rate": 9.491241377678015e-05,
        "epoch": 0.8393322900365154,
        "step": 11263
    },
    {
        "loss": 1.7109,
        "grad_norm": 3.37268328666687,
        "learning_rate": 9.484214049580553e-05,
        "epoch": 0.8394068112377971,
        "step": 11264
    },
    {
        "loss": 2.4052,
        "grad_norm": 2.16196346282959,
        "learning_rate": 9.477186976865615e-05,
        "epoch": 0.839481332439079,
        "step": 11265
    },
    {
        "loss": 2.4224,
        "grad_norm": 5.399324417114258,
        "learning_rate": 9.470160163012525e-05,
        "epoch": 0.8395558536403607,
        "step": 11266
    },
    {
        "loss": 2.4811,
        "grad_norm": 2.588392972946167,
        "learning_rate": 9.463133611500488e-05,
        "epoch": 0.8396303748416425,
        "step": 11267
    },
    {
        "loss": 1.4861,
        "grad_norm": 3.443906784057617,
        "learning_rate": 9.456107325808565e-05,
        "epoch": 0.8397048960429242,
        "step": 11268
    },
    {
        "loss": 1.3206,
        "grad_norm": 3.152194023132324,
        "learning_rate": 9.449081309415727e-05,
        "epoch": 0.839779417244206,
        "step": 11269
    },
    {
        "loss": 2.8306,
        "grad_norm": 3.712754011154175,
        "learning_rate": 9.442055565800758e-05,
        "epoch": 0.8398539384454877,
        "step": 11270
    },
    {
        "loss": 2.355,
        "grad_norm": 2.840679168701172,
        "learning_rate": 9.435030098442364e-05,
        "epoch": 0.8399284596467695,
        "step": 11271
    },
    {
        "loss": 2.4239,
        "grad_norm": 2.659346342086792,
        "learning_rate": 9.428004910819064e-05,
        "epoch": 0.8400029808480513,
        "step": 11272
    },
    {
        "loss": 2.3674,
        "grad_norm": 2.6471664905548096,
        "learning_rate": 9.42098000640925e-05,
        "epoch": 0.8400775020493331,
        "step": 11273
    },
    {
        "loss": 2.0562,
        "grad_norm": 3.6932716369628906,
        "learning_rate": 9.413955388691221e-05,
        "epoch": 0.8401520232506148,
        "step": 11274
    },
    {
        "loss": 2.563,
        "grad_norm": 2.3065264225006104,
        "learning_rate": 9.406931061143041e-05,
        "epoch": 0.8402265444518966,
        "step": 11275
    },
    {
        "loss": 2.5403,
        "grad_norm": 2.204371690750122,
        "learning_rate": 9.399907027242726e-05,
        "epoch": 0.8403010656531783,
        "step": 11276
    },
    {
        "loss": 1.9965,
        "grad_norm": 3.3129255771636963,
        "learning_rate": 9.39288329046809e-05,
        "epoch": 0.8403755868544601,
        "step": 11277
    },
    {
        "loss": 2.2399,
        "grad_norm": 2.9004929065704346,
        "learning_rate": 9.385859854296802e-05,
        "epoch": 0.8404501080557418,
        "step": 11278
    },
    {
        "loss": 2.425,
        "grad_norm": 1.9103825092315674,
        "learning_rate": 9.378836722206427e-05,
        "epoch": 0.8405246292570236,
        "step": 11279
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.912937641143799,
        "learning_rate": 9.371813897674327e-05,
        "epoch": 0.8405991504583054,
        "step": 11280
    },
    {
        "loss": 2.5741,
        "grad_norm": 2.0987913608551025,
        "learning_rate": 9.36479138417772e-05,
        "epoch": 0.8406736716595872,
        "step": 11281
    },
    {
        "loss": 2.4054,
        "grad_norm": 3.98295259475708,
        "learning_rate": 9.357769185193714e-05,
        "epoch": 0.8407481928608689,
        "step": 11282
    },
    {
        "loss": 2.8776,
        "grad_norm": 3.4591963291168213,
        "learning_rate": 9.350747304199214e-05,
        "epoch": 0.8408227140621507,
        "step": 11283
    },
    {
        "loss": 2.5197,
        "grad_norm": 3.3092427253723145,
        "learning_rate": 9.343725744670968e-05,
        "epoch": 0.8408972352634324,
        "step": 11284
    },
    {
        "loss": 1.8696,
        "grad_norm": 3.4003658294677734,
        "learning_rate": 9.336704510085623e-05,
        "epoch": 0.8409717564647142,
        "step": 11285
    },
    {
        "loss": 2.0041,
        "grad_norm": 3.1171886920928955,
        "learning_rate": 9.329683603919566e-05,
        "epoch": 0.8410462776659959,
        "step": 11286
    },
    {
        "loss": 1.8655,
        "grad_norm": 4.10682487487793,
        "learning_rate": 9.322663029649117e-05,
        "epoch": 0.8411207988672778,
        "step": 11287
    },
    {
        "loss": 1.292,
        "grad_norm": 3.0170581340789795,
        "learning_rate": 9.315642790750369e-05,
        "epoch": 0.8411953200685595,
        "step": 11288
    },
    {
        "loss": 2.1591,
        "grad_norm": 3.51564621925354,
        "learning_rate": 9.308622890699295e-05,
        "epoch": 0.8412698412698413,
        "step": 11289
    },
    {
        "loss": 2.2842,
        "grad_norm": 3.632652997970581,
        "learning_rate": 9.30160333297167e-05,
        "epoch": 0.841344362471123,
        "step": 11290
    },
    {
        "loss": 2.6118,
        "grad_norm": 3.741956949234009,
        "learning_rate": 9.294584121043086e-05,
        "epoch": 0.8414188836724048,
        "step": 11291
    },
    {
        "loss": 2.2221,
        "grad_norm": 2.516014814376831,
        "learning_rate": 9.287565258389016e-05,
        "epoch": 0.8414934048736865,
        "step": 11292
    },
    {
        "loss": 2.6719,
        "grad_norm": 3.334585428237915,
        "learning_rate": 9.2805467484847e-05,
        "epoch": 0.8415679260749683,
        "step": 11293
    },
    {
        "loss": 2.5689,
        "grad_norm": 3.379225254058838,
        "learning_rate": 9.273528594805278e-05,
        "epoch": 0.84164244727625,
        "step": 11294
    },
    {
        "loss": 2.4919,
        "grad_norm": 1.8298231363296509,
        "learning_rate": 9.266510800825605e-05,
        "epoch": 0.8417169684775319,
        "step": 11295
    },
    {
        "loss": 2.9148,
        "grad_norm": 2.833512783050537,
        "learning_rate": 9.259493370020464e-05,
        "epoch": 0.8417914896788137,
        "step": 11296
    },
    {
        "loss": 2.1397,
        "grad_norm": 3.276860475540161,
        "learning_rate": 9.252476305864407e-05,
        "epoch": 0.8418660108800954,
        "step": 11297
    },
    {
        "loss": 1.7124,
        "grad_norm": 3.43239688873291,
        "learning_rate": 9.245459611831791e-05,
        "epoch": 0.8419405320813772,
        "step": 11298
    },
    {
        "loss": 2.4499,
        "grad_norm": 3.873018503189087,
        "learning_rate": 9.238443291396841e-05,
        "epoch": 0.8420150532826589,
        "step": 11299
    },
    {
        "loss": 1.8596,
        "grad_norm": 2.9982242584228516,
        "learning_rate": 9.231427348033554e-05,
        "epoch": 0.8420895744839407,
        "step": 11300
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.919212818145752,
        "learning_rate": 9.22441178521574e-05,
        "epoch": 0.8421640956852224,
        "step": 11301
    },
    {
        "loss": 2.0476,
        "grad_norm": 3.8765976428985596,
        "learning_rate": 9.217396606417061e-05,
        "epoch": 0.8422386168865043,
        "step": 11302
    },
    {
        "loss": 2.8891,
        "grad_norm": 3.906398296356201,
        "learning_rate": 9.210381815110946e-05,
        "epoch": 0.842313138087786,
        "step": 11303
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.450031042098999,
        "learning_rate": 9.203367414770636e-05,
        "epoch": 0.8423876592890678,
        "step": 11304
    },
    {
        "loss": 2.3497,
        "grad_norm": 4.21732234954834,
        "learning_rate": 9.196353408869233e-05,
        "epoch": 0.8424621804903495,
        "step": 11305
    },
    {
        "loss": 2.0645,
        "grad_norm": 3.4889020919799805,
        "learning_rate": 9.189339800879542e-05,
        "epoch": 0.8425367016916313,
        "step": 11306
    },
    {
        "loss": 2.0532,
        "grad_norm": 3.4309098720550537,
        "learning_rate": 9.182326594274273e-05,
        "epoch": 0.842611222892913,
        "step": 11307
    },
    {
        "loss": 2.5723,
        "grad_norm": 1.6494388580322266,
        "learning_rate": 9.175313792525868e-05,
        "epoch": 0.8426857440941948,
        "step": 11308
    },
    {
        "loss": 1.8619,
        "grad_norm": 4.912144184112549,
        "learning_rate": 9.168301399106624e-05,
        "epoch": 0.8427602652954765,
        "step": 11309
    },
    {
        "loss": 2.7374,
        "grad_norm": 1.6028534173965454,
        "learning_rate": 9.161289417488587e-05,
        "epoch": 0.8428347864967584,
        "step": 11310
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.573171377182007,
        "learning_rate": 9.154277851143609e-05,
        "epoch": 0.8429093076980401,
        "step": 11311
    },
    {
        "loss": 1.1946,
        "grad_norm": 4.344757556915283,
        "learning_rate": 9.147266703543372e-05,
        "epoch": 0.8429838288993219,
        "step": 11312
    },
    {
        "loss": 2.1761,
        "grad_norm": 2.7480170726776123,
        "learning_rate": 9.140255978159314e-05,
        "epoch": 0.8430583501006036,
        "step": 11313
    },
    {
        "loss": 2.6001,
        "grad_norm": 3.707571029663086,
        "learning_rate": 9.13324567846267e-05,
        "epoch": 0.8431328713018854,
        "step": 11314
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.910018801689148,
        "learning_rate": 9.126235807924458e-05,
        "epoch": 0.8432073925031671,
        "step": 11315
    },
    {
        "loss": 2.0417,
        "grad_norm": 2.3961493968963623,
        "learning_rate": 9.119226370015521e-05,
        "epoch": 0.8432819137044489,
        "step": 11316
    },
    {
        "loss": 1.8465,
        "grad_norm": 2.8906497955322266,
        "learning_rate": 9.11221736820645e-05,
        "epoch": 0.8433564349057306,
        "step": 11317
    },
    {
        "loss": 1.5516,
        "grad_norm": 5.155043125152588,
        "learning_rate": 9.105208805967629e-05,
        "epoch": 0.8434309561070125,
        "step": 11318
    },
    {
        "loss": 0.8927,
        "grad_norm": 3.7276859283447266,
        "learning_rate": 9.098200686769212e-05,
        "epoch": 0.8435054773082942,
        "step": 11319
    },
    {
        "loss": 2.8719,
        "grad_norm": 2.764720916748047,
        "learning_rate": 9.091193014081178e-05,
        "epoch": 0.843579998509576,
        "step": 11320
    },
    {
        "loss": 2.1792,
        "grad_norm": 3.506235122680664,
        "learning_rate": 9.084185791373228e-05,
        "epoch": 0.8436545197108577,
        "step": 11321
    },
    {
        "loss": 2.9043,
        "grad_norm": 3.572007894515991,
        "learning_rate": 9.07717902211489e-05,
        "epoch": 0.8437290409121395,
        "step": 11322
    },
    {
        "loss": 2.7957,
        "grad_norm": 4.405428409576416,
        "learning_rate": 9.070172709775436e-05,
        "epoch": 0.8438035621134212,
        "step": 11323
    },
    {
        "loss": 2.6259,
        "grad_norm": 3.171104669570923,
        "learning_rate": 9.063166857823906e-05,
        "epoch": 0.843878083314703,
        "step": 11324
    },
    {
        "loss": 2.0742,
        "grad_norm": 2.657991647720337,
        "learning_rate": 9.05616146972917e-05,
        "epoch": 0.8439526045159848,
        "step": 11325
    },
    {
        "loss": 2.0436,
        "grad_norm": 3.525327205657959,
        "learning_rate": 9.049156548959765e-05,
        "epoch": 0.8440271257172666,
        "step": 11326
    },
    {
        "loss": 2.5235,
        "grad_norm": 2.8201117515563965,
        "learning_rate": 9.0421520989841e-05,
        "epoch": 0.8441016469185483,
        "step": 11327
    },
    {
        "loss": 1.5378,
        "grad_norm": 2.714207172393799,
        "learning_rate": 9.035148123270297e-05,
        "epoch": 0.8441761681198301,
        "step": 11328
    },
    {
        "loss": 2.4985,
        "grad_norm": 1.9808506965637207,
        "learning_rate": 9.028144625286235e-05,
        "epoch": 0.8442506893211118,
        "step": 11329
    },
    {
        "loss": 2.8172,
        "grad_norm": 2.6418581008911133,
        "learning_rate": 9.021141608499606e-05,
        "epoch": 0.8443252105223936,
        "step": 11330
    },
    {
        "loss": 2.4091,
        "grad_norm": 3.385723829269409,
        "learning_rate": 9.014139076377803e-05,
        "epoch": 0.8443997317236754,
        "step": 11331
    },
    {
        "loss": 1.9521,
        "grad_norm": 7.4801411628723145,
        "learning_rate": 9.00713703238804e-05,
        "epoch": 0.8444742529249571,
        "step": 11332
    },
    {
        "loss": 2.8886,
        "grad_norm": 3.170893430709839,
        "learning_rate": 9.000135479997241e-05,
        "epoch": 0.844548774126239,
        "step": 11333
    },
    {
        "loss": 2.3214,
        "grad_norm": 3.0158839225769043,
        "learning_rate": 8.993134422672105e-05,
        "epoch": 0.8446232953275207,
        "step": 11334
    },
    {
        "loss": 2.2515,
        "grad_norm": 2.897534132003784,
        "learning_rate": 8.986133863879071e-05,
        "epoch": 0.8446978165288025,
        "step": 11335
    },
    {
        "loss": 2.4439,
        "grad_norm": 3.7013492584228516,
        "learning_rate": 8.979133807084375e-05,
        "epoch": 0.8447723377300842,
        "step": 11336
    },
    {
        "loss": 2.4461,
        "grad_norm": 3.078000783920288,
        "learning_rate": 8.972134255753955e-05,
        "epoch": 0.844846858931366,
        "step": 11337
    },
    {
        "loss": 2.0491,
        "grad_norm": 2.437258720397949,
        "learning_rate": 8.965135213353522e-05,
        "epoch": 0.8449213801326477,
        "step": 11338
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.1927168369293213,
        "learning_rate": 8.958136683348519e-05,
        "epoch": 0.8449959013339295,
        "step": 11339
    },
    {
        "loss": 2.3019,
        "grad_norm": 2.7557384967803955,
        "learning_rate": 8.951138669204167e-05,
        "epoch": 0.8450704225352113,
        "step": 11340
    },
    {
        "loss": 2.0342,
        "grad_norm": 2.7692973613739014,
        "learning_rate": 8.944141174385394e-05,
        "epoch": 0.8451449437364931,
        "step": 11341
    },
    {
        "loss": 2.8041,
        "grad_norm": 2.804023027420044,
        "learning_rate": 8.93714420235691e-05,
        "epoch": 0.8452194649377748,
        "step": 11342
    },
    {
        "loss": 1.7961,
        "grad_norm": 3.9065394401550293,
        "learning_rate": 8.930147756583135e-05,
        "epoch": 0.8452939861390566,
        "step": 11343
    },
    {
        "loss": 2.3001,
        "grad_norm": 4.385608196258545,
        "learning_rate": 8.923151840528222e-05,
        "epoch": 0.8453685073403383,
        "step": 11344
    },
    {
        "loss": 2.787,
        "grad_norm": 3.05708646774292,
        "learning_rate": 8.91615645765612e-05,
        "epoch": 0.8454430285416201,
        "step": 11345
    },
    {
        "loss": 2.5055,
        "grad_norm": 3.4058094024658203,
        "learning_rate": 8.909161611430417e-05,
        "epoch": 0.8455175497429018,
        "step": 11346
    },
    {
        "loss": 2.2795,
        "grad_norm": 3.1602916717529297,
        "learning_rate": 8.902167305314534e-05,
        "epoch": 0.8455920709441836,
        "step": 11347
    },
    {
        "loss": 2.8461,
        "grad_norm": 1.9714542627334595,
        "learning_rate": 8.895173542771563e-05,
        "epoch": 0.8456665921454654,
        "step": 11348
    },
    {
        "loss": 1.9457,
        "grad_norm": 4.0344109535217285,
        "learning_rate": 8.888180327264334e-05,
        "epoch": 0.8457411133467472,
        "step": 11349
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.476689338684082,
        "learning_rate": 8.881187662255445e-05,
        "epoch": 0.8458156345480289,
        "step": 11350
    },
    {
        "loss": 1.8026,
        "grad_norm": 2.7559759616851807,
        "learning_rate": 8.874195551207178e-05,
        "epoch": 0.8458901557493107,
        "step": 11351
    },
    {
        "loss": 1.9539,
        "grad_norm": 2.922306776046753,
        "learning_rate": 8.867203997581543e-05,
        "epoch": 0.8459646769505924,
        "step": 11352
    },
    {
        "loss": 2.4269,
        "grad_norm": 1.879976749420166,
        "learning_rate": 8.860213004840313e-05,
        "epoch": 0.8460391981518742,
        "step": 11353
    },
    {
        "loss": 2.9801,
        "grad_norm": 3.194080114364624,
        "learning_rate": 8.853222576444948e-05,
        "epoch": 0.8461137193531559,
        "step": 11354
    },
    {
        "loss": 2.4506,
        "grad_norm": 3.4642879962921143,
        "learning_rate": 8.84623271585662e-05,
        "epoch": 0.8461882405544378,
        "step": 11355
    },
    {
        "loss": 2.2331,
        "grad_norm": 3.0418334007263184,
        "learning_rate": 8.83924342653628e-05,
        "epoch": 0.8462627617557195,
        "step": 11356
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.9462639093399048,
        "learning_rate": 8.832254711944498e-05,
        "epoch": 0.8463372829570013,
        "step": 11357
    },
    {
        "loss": 2.6322,
        "grad_norm": 2.9218287467956543,
        "learning_rate": 8.825266575541662e-05,
        "epoch": 0.846411804158283,
        "step": 11358
    },
    {
        "loss": 1.5796,
        "grad_norm": 3.4310224056243896,
        "learning_rate": 8.818279020787794e-05,
        "epoch": 0.8464863253595648,
        "step": 11359
    },
    {
        "loss": 2.274,
        "grad_norm": 3.3955636024475098,
        "learning_rate": 8.811292051142689e-05,
        "epoch": 0.8465608465608465,
        "step": 11360
    },
    {
        "loss": 1.4281,
        "grad_norm": 1.6703932285308838,
        "learning_rate": 8.804305670065814e-05,
        "epoch": 0.8466353677621283,
        "step": 11361
    },
    {
        "loss": 3.1936,
        "grad_norm": 2.3753604888916016,
        "learning_rate": 8.797319881016339e-05,
        "epoch": 0.84670988896341,
        "step": 11362
    },
    {
        "loss": 2.805,
        "grad_norm": 2.6013343334198,
        "learning_rate": 8.790334687453185e-05,
        "epoch": 0.8467844101646919,
        "step": 11363
    },
    {
        "loss": 1.8298,
        "grad_norm": 3.545485496520996,
        "learning_rate": 8.783350092834923e-05,
        "epoch": 0.8468589313659736,
        "step": 11364
    },
    {
        "loss": 2.7166,
        "grad_norm": 3.4823617935180664,
        "learning_rate": 8.776366100619895e-05,
        "epoch": 0.8469334525672554,
        "step": 11365
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.9264719486236572,
        "learning_rate": 8.769382714266049e-05,
        "epoch": 0.8470079737685372,
        "step": 11366
    },
    {
        "loss": 2.2843,
        "grad_norm": 4.002436637878418,
        "learning_rate": 8.762399937231129e-05,
        "epoch": 0.8470824949698189,
        "step": 11367
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.6686360836029053,
        "learning_rate": 8.755417772972523e-05,
        "epoch": 0.8471570161711007,
        "step": 11368
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.7932732105255127,
        "learning_rate": 8.748436224947318e-05,
        "epoch": 0.8472315373723824,
        "step": 11369
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.891378879547119,
        "learning_rate": 8.741455296612336e-05,
        "epoch": 0.8473060585736643,
        "step": 11370
    },
    {
        "loss": 2.7891,
        "grad_norm": 2.6034862995147705,
        "learning_rate": 8.734474991424042e-05,
        "epoch": 0.847380579774946,
        "step": 11371
    },
    {
        "loss": 2.4295,
        "grad_norm": 2.3766353130340576,
        "learning_rate": 8.727495312838602e-05,
        "epoch": 0.8474551009762278,
        "step": 11372
    },
    {
        "loss": 3.5682,
        "grad_norm": 3.498530864715576,
        "learning_rate": 8.720516264311919e-05,
        "epoch": 0.8475296221775095,
        "step": 11373
    },
    {
        "loss": 2.7036,
        "grad_norm": 3.5541861057281494,
        "learning_rate": 8.713537849299524e-05,
        "epoch": 0.8476041433787913,
        "step": 11374
    },
    {
        "loss": 1.6373,
        "grad_norm": 3.768587350845337,
        "learning_rate": 8.70656007125665e-05,
        "epoch": 0.847678664580073,
        "step": 11375
    },
    {
        "loss": 1.8372,
        "grad_norm": 4.463714122772217,
        "learning_rate": 8.699582933638267e-05,
        "epoch": 0.8477531857813548,
        "step": 11376
    },
    {
        "loss": 2.1735,
        "grad_norm": 3.872781991958618,
        "learning_rate": 8.692606439898923e-05,
        "epoch": 0.8478277069826365,
        "step": 11377
    },
    {
        "loss": 2.5571,
        "grad_norm": 2.970850706100464,
        "learning_rate": 8.685630593492955e-05,
        "epoch": 0.8479022281839184,
        "step": 11378
    },
    {
        "loss": 2.3633,
        "grad_norm": 2.041093349456787,
        "learning_rate": 8.678655397874317e-05,
        "epoch": 0.8479767493852001,
        "step": 11379
    },
    {
        "loss": 3.63,
        "grad_norm": 2.5071849822998047,
        "learning_rate": 8.671680856496645e-05,
        "epoch": 0.8480512705864819,
        "step": 11380
    },
    {
        "loss": 2.1099,
        "grad_norm": 2.8628461360931396,
        "learning_rate": 8.664706972813288e-05,
        "epoch": 0.8481257917877636,
        "step": 11381
    },
    {
        "loss": 2.4917,
        "grad_norm": 2.575777292251587,
        "learning_rate": 8.657733750277217e-05,
        "epoch": 0.8482003129890454,
        "step": 11382
    },
    {
        "loss": 1.4713,
        "grad_norm": 3.2510340213775635,
        "learning_rate": 8.65076119234113e-05,
        "epoch": 0.8482748341903271,
        "step": 11383
    },
    {
        "loss": 2.8051,
        "grad_norm": 3.442678213119507,
        "learning_rate": 8.643789302457359e-05,
        "epoch": 0.8483493553916089,
        "step": 11384
    },
    {
        "loss": 1.9308,
        "grad_norm": 2.832611322402954,
        "learning_rate": 8.636818084077909e-05,
        "epoch": 0.8484238765928906,
        "step": 11385
    },
    {
        "loss": 2.5643,
        "grad_norm": 2.3922245502471924,
        "learning_rate": 8.629847540654447e-05,
        "epoch": 0.8484983977941725,
        "step": 11386
    },
    {
        "loss": 1.851,
        "grad_norm": 1.708647608757019,
        "learning_rate": 8.622877675638347e-05,
        "epoch": 0.8485729189954542,
        "step": 11387
    },
    {
        "loss": 2.8434,
        "grad_norm": 4.2479023933410645,
        "learning_rate": 8.615908492480602e-05,
        "epoch": 0.848647440196736,
        "step": 11388
    },
    {
        "loss": 1.7651,
        "grad_norm": 3.6991238594055176,
        "learning_rate": 8.608939994631882e-05,
        "epoch": 0.8487219613980177,
        "step": 11389
    },
    {
        "loss": 2.546,
        "grad_norm": 2.2423205375671387,
        "learning_rate": 8.601972185542506e-05,
        "epoch": 0.8487964825992995,
        "step": 11390
    },
    {
        "loss": 2.17,
        "grad_norm": 3.48327898979187,
        "learning_rate": 8.595005068662489e-05,
        "epoch": 0.8488710038005812,
        "step": 11391
    },
    {
        "loss": 2.2959,
        "grad_norm": 3.8026936054229736,
        "learning_rate": 8.588038647441456e-05,
        "epoch": 0.848945525001863,
        "step": 11392
    },
    {
        "loss": 2.4397,
        "grad_norm": 4.029232501983643,
        "learning_rate": 8.581072925328736e-05,
        "epoch": 0.8490200462031448,
        "step": 11393
    },
    {
        "loss": 1.912,
        "grad_norm": 3.217313766479492,
        "learning_rate": 8.57410790577327e-05,
        "epoch": 0.8490945674044266,
        "step": 11394
    },
    {
        "loss": 2.39,
        "grad_norm": 2.2661163806915283,
        "learning_rate": 8.567143592223654e-05,
        "epoch": 0.8491690886057083,
        "step": 11395
    },
    {
        "loss": 2.539,
        "grad_norm": 2.500180959701538,
        "learning_rate": 8.560179988128193e-05,
        "epoch": 0.8492436098069901,
        "step": 11396
    },
    {
        "loss": 2.705,
        "grad_norm": 2.5338480472564697,
        "learning_rate": 8.553217096934736e-05,
        "epoch": 0.8493181310082718,
        "step": 11397
    },
    {
        "loss": 1.7847,
        "grad_norm": 2.157627582550049,
        "learning_rate": 8.54625492209088e-05,
        "epoch": 0.8493926522095536,
        "step": 11398
    },
    {
        "loss": 2.4496,
        "grad_norm": 3.6788055896759033,
        "learning_rate": 8.539293467043816e-05,
        "epoch": 0.8494671734108353,
        "step": 11399
    },
    {
        "loss": 2.4342,
        "grad_norm": 2.214993715286255,
        "learning_rate": 8.53233273524037e-05,
        "epoch": 0.8495416946121171,
        "step": 11400
    },
    {
        "loss": 2.8968,
        "grad_norm": 3.309783458709717,
        "learning_rate": 8.525372730127056e-05,
        "epoch": 0.849616215813399,
        "step": 11401
    },
    {
        "loss": 2.6247,
        "grad_norm": 2.5579304695129395,
        "learning_rate": 8.518413455149976e-05,
        "epoch": 0.8496907370146807,
        "step": 11402
    },
    {
        "loss": 1.8999,
        "grad_norm": 2.050212860107422,
        "learning_rate": 8.511454913754919e-05,
        "epoch": 0.8497652582159625,
        "step": 11403
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.545290470123291,
        "learning_rate": 8.504497109387279e-05,
        "epoch": 0.8498397794172442,
        "step": 11404
    },
    {
        "loss": 2.8298,
        "grad_norm": 3.3057162761688232,
        "learning_rate": 8.497540045492086e-05,
        "epoch": 0.849914300618526,
        "step": 11405
    },
    {
        "loss": 1.8552,
        "grad_norm": 2.928633213043213,
        "learning_rate": 8.490583725513996e-05,
        "epoch": 0.8499888218198077,
        "step": 11406
    },
    {
        "loss": 1.8278,
        "grad_norm": 3.165173053741455,
        "learning_rate": 8.48362815289736e-05,
        "epoch": 0.8500633430210895,
        "step": 11407
    },
    {
        "loss": 1.4055,
        "grad_norm": 4.842796802520752,
        "learning_rate": 8.476673331086047e-05,
        "epoch": 0.8501378642223713,
        "step": 11408
    },
    {
        "loss": 2.397,
        "grad_norm": 2.9092154502868652,
        "learning_rate": 8.46971926352366e-05,
        "epoch": 0.8502123854236531,
        "step": 11409
    },
    {
        "loss": 2.4814,
        "grad_norm": 2.0672502517700195,
        "learning_rate": 8.46276595365336e-05,
        "epoch": 0.8502869066249348,
        "step": 11410
    },
    {
        "loss": 2.619,
        "grad_norm": 2.3144283294677734,
        "learning_rate": 8.455813404917986e-05,
        "epoch": 0.8503614278262166,
        "step": 11411
    },
    {
        "loss": 2.692,
        "grad_norm": 2.7813289165496826,
        "learning_rate": 8.448861620759956e-05,
        "epoch": 0.8504359490274983,
        "step": 11412
    },
    {
        "loss": 2.546,
        "grad_norm": 3.97345232963562,
        "learning_rate": 8.441910604621314e-05,
        "epoch": 0.8505104702287801,
        "step": 11413
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.8068323135375977,
        "learning_rate": 8.434960359943763e-05,
        "epoch": 0.8505849914300618,
        "step": 11414
    },
    {
        "loss": 2.1014,
        "grad_norm": 3.2431421279907227,
        "learning_rate": 8.428010890168571e-05,
        "epoch": 0.8506595126313437,
        "step": 11415
    },
    {
        "loss": 2.4614,
        "grad_norm": 2.1062793731689453,
        "learning_rate": 8.421062198736687e-05,
        "epoch": 0.8507340338326254,
        "step": 11416
    },
    {
        "loss": 1.4938,
        "grad_norm": 4.9428486824035645,
        "learning_rate": 8.41411428908858e-05,
        "epoch": 0.8508085550339072,
        "step": 11417
    },
    {
        "loss": 2.5355,
        "grad_norm": 2.5193464756011963,
        "learning_rate": 8.407167164664432e-05,
        "epoch": 0.8508830762351889,
        "step": 11418
    },
    {
        "loss": 2.959,
        "grad_norm": 2.0938892364501953,
        "learning_rate": 8.400220828903974e-05,
        "epoch": 0.8509575974364707,
        "step": 11419
    },
    {
        "loss": 1.977,
        "grad_norm": 2.8507113456726074,
        "learning_rate": 8.393275285246553e-05,
        "epoch": 0.8510321186377524,
        "step": 11420
    },
    {
        "loss": 2.3044,
        "grad_norm": 5.61764669418335,
        "learning_rate": 8.386330537131163e-05,
        "epoch": 0.8511066398390342,
        "step": 11421
    },
    {
        "loss": 1.9819,
        "grad_norm": 3.625004768371582,
        "learning_rate": 8.379386587996362e-05,
        "epoch": 0.8511811610403159,
        "step": 11422
    },
    {
        "loss": 2.8194,
        "grad_norm": 2.12134051322937,
        "learning_rate": 8.372443441280312e-05,
        "epoch": 0.8512556822415978,
        "step": 11423
    },
    {
        "loss": 2.3187,
        "grad_norm": 2.17814040184021,
        "learning_rate": 8.365501100420819e-05,
        "epoch": 0.8513302034428795,
        "step": 11424
    },
    {
        "loss": 2.2602,
        "grad_norm": 3.7297122478485107,
        "learning_rate": 8.358559568855251e-05,
        "epoch": 0.8514047246441613,
        "step": 11425
    },
    {
        "loss": 2.5857,
        "grad_norm": 2.128170967102051,
        "learning_rate": 8.351618850020575e-05,
        "epoch": 0.851479245845443,
        "step": 11426
    },
    {
        "loss": 2.7511,
        "grad_norm": 3.769118309020996,
        "learning_rate": 8.344678947353409e-05,
        "epoch": 0.8515537670467248,
        "step": 11427
    },
    {
        "loss": 2.9516,
        "grad_norm": 4.388964653015137,
        "learning_rate": 8.337739864289866e-05,
        "epoch": 0.8516282882480065,
        "step": 11428
    },
    {
        "loss": 2.3545,
        "grad_norm": 3.4787604808807373,
        "learning_rate": 8.330801604265761e-05,
        "epoch": 0.8517028094492883,
        "step": 11429
    },
    {
        "loss": 2.3526,
        "grad_norm": 3.1281332969665527,
        "learning_rate": 8.323864170716426e-05,
        "epoch": 0.85177733065057,
        "step": 11430
    },
    {
        "loss": 2.7685,
        "grad_norm": 3.317553997039795,
        "learning_rate": 8.316927567076833e-05,
        "epoch": 0.8518518518518519,
        "step": 11431
    },
    {
        "loss": 2.1011,
        "grad_norm": 3.6226325035095215,
        "learning_rate": 8.309991796781514e-05,
        "epoch": 0.8519263730531336,
        "step": 11432
    },
    {
        "loss": 2.2346,
        "grad_norm": 2.4828298091888428,
        "learning_rate": 8.303056863264578e-05,
        "epoch": 0.8520008942544154,
        "step": 11433
    },
    {
        "loss": 1.9928,
        "grad_norm": 4.462813377380371,
        "learning_rate": 8.296122769959768e-05,
        "epoch": 0.8520754154556971,
        "step": 11434
    },
    {
        "loss": 2.6705,
        "grad_norm": 2.576652765274048,
        "learning_rate": 8.289189520300354e-05,
        "epoch": 0.8521499366569789,
        "step": 11435
    },
    {
        "loss": 2.4363,
        "grad_norm": 2.505392551422119,
        "learning_rate": 8.28225711771925e-05,
        "epoch": 0.8522244578582606,
        "step": 11436
    },
    {
        "loss": 2.5085,
        "grad_norm": 2.3813040256500244,
        "learning_rate": 8.275325565648868e-05,
        "epoch": 0.8522989790595424,
        "step": 11437
    },
    {
        "loss": 2.1423,
        "grad_norm": 3.503507375717163,
        "learning_rate": 8.268394867521286e-05,
        "epoch": 0.8523735002608243,
        "step": 11438
    },
    {
        "loss": 2.6313,
        "grad_norm": 2.426304340362549,
        "learning_rate": 8.2614650267681e-05,
        "epoch": 0.852448021462106,
        "step": 11439
    },
    {
        "loss": 2.6097,
        "grad_norm": 2.691091299057007,
        "learning_rate": 8.254536046820507e-05,
        "epoch": 0.8525225426633878,
        "step": 11440
    },
    {
        "loss": 2.6199,
        "grad_norm": 2.03539776802063,
        "learning_rate": 8.247607931109259e-05,
        "epoch": 0.8525970638646695,
        "step": 11441
    },
    {
        "loss": 2.2166,
        "grad_norm": 3.1663262844085693,
        "learning_rate": 8.240680683064716e-05,
        "epoch": 0.8526715850659513,
        "step": 11442
    },
    {
        "loss": 2.403,
        "grad_norm": 3.6093528270721436,
        "learning_rate": 8.233754306116759e-05,
        "epoch": 0.852746106267233,
        "step": 11443
    },
    {
        "loss": 1.3623,
        "grad_norm": 3.0434069633483887,
        "learning_rate": 8.226828803694891e-05,
        "epoch": 0.8528206274685148,
        "step": 11444
    },
    {
        "loss": 1.7826,
        "grad_norm": 3.3765807151794434,
        "learning_rate": 8.219904179228146e-05,
        "epoch": 0.8528951486697965,
        "step": 11445
    },
    {
        "loss": 2.3931,
        "grad_norm": 2.812513828277588,
        "learning_rate": 8.212980436145114e-05,
        "epoch": 0.8529696698710784,
        "step": 11446
    },
    {
        "loss": 1.9299,
        "grad_norm": 3.727818727493286,
        "learning_rate": 8.206057577874007e-05,
        "epoch": 0.8530441910723601,
        "step": 11447
    },
    {
        "loss": 2.2185,
        "grad_norm": 3.0658178329467773,
        "learning_rate": 8.199135607842508e-05,
        "epoch": 0.8531187122736419,
        "step": 11448
    },
    {
        "loss": 2.3123,
        "grad_norm": 3.723206043243408,
        "learning_rate": 8.192214529477952e-05,
        "epoch": 0.8531932334749236,
        "step": 11449
    },
    {
        "loss": 2.002,
        "grad_norm": 3.6911182403564453,
        "learning_rate": 8.185294346207173e-05,
        "epoch": 0.8532677546762054,
        "step": 11450
    },
    {
        "loss": 2.3665,
        "grad_norm": 2.9025933742523193,
        "learning_rate": 8.178375061456571e-05,
        "epoch": 0.8533422758774871,
        "step": 11451
    },
    {
        "loss": 2.0798,
        "grad_norm": 2.9540059566497803,
        "learning_rate": 8.171456678652139e-05,
        "epoch": 0.8534167970787689,
        "step": 11452
    },
    {
        "loss": 2.491,
        "grad_norm": 3.073021650314331,
        "learning_rate": 8.164539201219362e-05,
        "epoch": 0.8534913182800506,
        "step": 11453
    },
    {
        "loss": 1.8298,
        "grad_norm": 3.2507901191711426,
        "learning_rate": 8.157622632583339e-05,
        "epoch": 0.8535658394813325,
        "step": 11454
    },
    {
        "loss": 1.7306,
        "grad_norm": 3.9872586727142334,
        "learning_rate": 8.150706976168679e-05,
        "epoch": 0.8536403606826142,
        "step": 11455
    },
    {
        "loss": 2.9352,
        "grad_norm": 3.041738510131836,
        "learning_rate": 8.14379223539955e-05,
        "epoch": 0.853714881883896,
        "step": 11456
    },
    {
        "loss": 2.8734,
        "grad_norm": 2.4311647415161133,
        "learning_rate": 8.136878413699646e-05,
        "epoch": 0.8537894030851777,
        "step": 11457
    },
    {
        "loss": 2.3711,
        "grad_norm": 3.704434394836426,
        "learning_rate": 8.12996551449226e-05,
        "epoch": 0.8538639242864595,
        "step": 11458
    },
    {
        "loss": 2.2854,
        "grad_norm": 2.057689666748047,
        "learning_rate": 8.123053541200181e-05,
        "epoch": 0.8539384454877412,
        "step": 11459
    },
    {
        "loss": 2.1965,
        "grad_norm": 2.9629409313201904,
        "learning_rate": 8.116142497245752e-05,
        "epoch": 0.854012966689023,
        "step": 11460
    },
    {
        "loss": 2.7201,
        "grad_norm": 2.7373554706573486,
        "learning_rate": 8.109232386050844e-05,
        "epoch": 0.8540874878903048,
        "step": 11461
    },
    {
        "loss": 1.5306,
        "grad_norm": 3.364696502685547,
        "learning_rate": 8.102323211036904e-05,
        "epoch": 0.8541620090915866,
        "step": 11462
    },
    {
        "loss": 2.8697,
        "grad_norm": 2.30646014213562,
        "learning_rate": 8.095414975624866e-05,
        "epoch": 0.8542365302928683,
        "step": 11463
    },
    {
        "loss": 1.5655,
        "grad_norm": 5.019003391265869,
        "learning_rate": 8.088507683235253e-05,
        "epoch": 0.8543110514941501,
        "step": 11464
    },
    {
        "loss": 2.6083,
        "grad_norm": 2.625018358230591,
        "learning_rate": 8.081601337288073e-05,
        "epoch": 0.8543855726954318,
        "step": 11465
    },
    {
        "loss": 2.7304,
        "grad_norm": 3.208453416824341,
        "learning_rate": 8.074695941202872e-05,
        "epoch": 0.8544600938967136,
        "step": 11466
    },
    {
        "loss": 2.33,
        "grad_norm": 3.515650987625122,
        "learning_rate": 8.067791498398783e-05,
        "epoch": 0.8545346150979953,
        "step": 11467
    },
    {
        "loss": 2.27,
        "grad_norm": 3.3548147678375244,
        "learning_rate": 8.060888012294361e-05,
        "epoch": 0.8546091362992772,
        "step": 11468
    },
    {
        "loss": 1.3756,
        "grad_norm": 3.2319347858428955,
        "learning_rate": 8.053985486307792e-05,
        "epoch": 0.8546836575005589,
        "step": 11469
    },
    {
        "loss": 1.5743,
        "grad_norm": 3.6573851108551025,
        "learning_rate": 8.047083923856727e-05,
        "epoch": 0.8547581787018407,
        "step": 11470
    },
    {
        "loss": 2.7572,
        "grad_norm": 3.196840524673462,
        "learning_rate": 8.040183328358344e-05,
        "epoch": 0.8548326999031224,
        "step": 11471
    },
    {
        "loss": 2.1888,
        "grad_norm": 3.922081232070923,
        "learning_rate": 8.033283703229383e-05,
        "epoch": 0.8549072211044042,
        "step": 11472
    },
    {
        "loss": 2.572,
        "grad_norm": 2.7474799156188965,
        "learning_rate": 8.026385051886057e-05,
        "epoch": 0.854981742305686,
        "step": 11473
    },
    {
        "loss": 1.2795,
        "grad_norm": 2.9461073875427246,
        "learning_rate": 8.0194873777441e-05,
        "epoch": 0.8550562635069677,
        "step": 11474
    },
    {
        "loss": 2.9506,
        "grad_norm": 3.2881696224212646,
        "learning_rate": 8.012590684218808e-05,
        "epoch": 0.8551307847082495,
        "step": 11475
    },
    {
        "loss": 2.8171,
        "grad_norm": 3.0062623023986816,
        "learning_rate": 8.005694974724941e-05,
        "epoch": 0.8552053059095313,
        "step": 11476
    },
    {
        "loss": 2.2512,
        "grad_norm": 3.1469366550445557,
        "learning_rate": 7.99880025267678e-05,
        "epoch": 0.8552798271108131,
        "step": 11477
    },
    {
        "loss": 2.7197,
        "grad_norm": 2.979437828063965,
        "learning_rate": 7.991906521488169e-05,
        "epoch": 0.8553543483120948,
        "step": 11478
    },
    {
        "loss": 2.7893,
        "grad_norm": 2.2028005123138428,
        "learning_rate": 7.985013784572364e-05,
        "epoch": 0.8554288695133766,
        "step": 11479
    },
    {
        "loss": 2.4211,
        "grad_norm": 2.0320630073547363,
        "learning_rate": 7.978122045342224e-05,
        "epoch": 0.8555033907146583,
        "step": 11480
    },
    {
        "loss": 2.3027,
        "grad_norm": 3.6304454803466797,
        "learning_rate": 7.971231307210052e-05,
        "epoch": 0.8555779119159401,
        "step": 11481
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.7099051475524902,
        "learning_rate": 7.964341573587701e-05,
        "epoch": 0.8556524331172218,
        "step": 11482
    },
    {
        "loss": 2.2287,
        "grad_norm": 3.5256683826446533,
        "learning_rate": 7.957452847886495e-05,
        "epoch": 0.8557269543185037,
        "step": 11483
    },
    {
        "loss": 2.1493,
        "grad_norm": 3.061549186706543,
        "learning_rate": 7.950565133517249e-05,
        "epoch": 0.8558014755197854,
        "step": 11484
    },
    {
        "loss": 2.3379,
        "grad_norm": 3.793775796890259,
        "learning_rate": 7.943678433890325e-05,
        "epoch": 0.8558759967210672,
        "step": 11485
    },
    {
        "loss": 3.0042,
        "grad_norm": 2.7386269569396973,
        "learning_rate": 7.936792752415526e-05,
        "epoch": 0.8559505179223489,
        "step": 11486
    },
    {
        "loss": 2.5917,
        "grad_norm": 2.278562307357788,
        "learning_rate": 7.929908092502222e-05,
        "epoch": 0.8560250391236307,
        "step": 11487
    },
    {
        "loss": 2.6675,
        "grad_norm": 3.240649461746216,
        "learning_rate": 7.923024457559174e-05,
        "epoch": 0.8560995603249124,
        "step": 11488
    },
    {
        "loss": 2.8797,
        "grad_norm": 3.61425518989563,
        "learning_rate": 7.91614185099474e-05,
        "epoch": 0.8561740815261942,
        "step": 11489
    },
    {
        "loss": 2.2177,
        "grad_norm": 3.014103412628174,
        "learning_rate": 7.90926027621671e-05,
        "epoch": 0.8562486027274759,
        "step": 11490
    },
    {
        "loss": 2.0211,
        "grad_norm": 4.522963523864746,
        "learning_rate": 7.902379736632361e-05,
        "epoch": 0.8563231239287578,
        "step": 11491
    },
    {
        "loss": 2.4782,
        "grad_norm": 5.5257697105407715,
        "learning_rate": 7.895500235648501e-05,
        "epoch": 0.8563976451300395,
        "step": 11492
    },
    {
        "loss": 2.4207,
        "grad_norm": 3.5645041465759277,
        "learning_rate": 7.888621776671383e-05,
        "epoch": 0.8564721663313213,
        "step": 11493
    },
    {
        "loss": 2.421,
        "grad_norm": 3.054155111312866,
        "learning_rate": 7.881744363106744e-05,
        "epoch": 0.856546687532603,
        "step": 11494
    },
    {
        "loss": 2.3576,
        "grad_norm": 3.6188182830810547,
        "learning_rate": 7.874867998359848e-05,
        "epoch": 0.8566212087338848,
        "step": 11495
    },
    {
        "loss": 1.4206,
        "grad_norm": 2.533698558807373,
        "learning_rate": 7.867992685835388e-05,
        "epoch": 0.8566957299351665,
        "step": 11496
    },
    {
        "loss": 2.8257,
        "grad_norm": 3.5886173248291016,
        "learning_rate": 7.861118428937545e-05,
        "epoch": 0.8567702511364483,
        "step": 11497
    },
    {
        "loss": 2.5481,
        "grad_norm": 3.0449306964874268,
        "learning_rate": 7.854245231070032e-05,
        "epoch": 0.85684477233773,
        "step": 11498
    },
    {
        "loss": 2.9031,
        "grad_norm": 1.4575128555297852,
        "learning_rate": 7.847373095635937e-05,
        "epoch": 0.8569192935390119,
        "step": 11499
    },
    {
        "loss": 2.1394,
        "grad_norm": 3.0103986263275146,
        "learning_rate": 7.84050202603792e-05,
        "epoch": 0.8569938147402936,
        "step": 11500
    },
    {
        "loss": 2.5867,
        "grad_norm": 2.5988636016845703,
        "learning_rate": 7.833632025678061e-05,
        "epoch": 0.8570683359415754,
        "step": 11501
    },
    {
        "loss": 2.2159,
        "grad_norm": 4.154938220977783,
        "learning_rate": 7.826763097957908e-05,
        "epoch": 0.8571428571428571,
        "step": 11502
    },
    {
        "loss": 2.1002,
        "grad_norm": 3.9395389556884766,
        "learning_rate": 7.819895246278517e-05,
        "epoch": 0.8572173783441389,
        "step": 11503
    },
    {
        "loss": 2.061,
        "grad_norm": 3.310199737548828,
        "learning_rate": 7.81302847404036e-05,
        "epoch": 0.8572918995454206,
        "step": 11504
    },
    {
        "loss": 2.5837,
        "grad_norm": 2.344250202178955,
        "learning_rate": 7.80616278464343e-05,
        "epoch": 0.8573664207467024,
        "step": 11505
    },
    {
        "loss": 2.1341,
        "grad_norm": 2.905388593673706,
        "learning_rate": 7.799298181487142e-05,
        "epoch": 0.8574409419479841,
        "step": 11506
    },
    {
        "loss": 1.8764,
        "grad_norm": 3.3073489665985107,
        "learning_rate": 7.79243466797038e-05,
        "epoch": 0.857515463149266,
        "step": 11507
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.474627733230591,
        "learning_rate": 7.785572247491486e-05,
        "epoch": 0.8575899843505478,
        "step": 11508
    },
    {
        "loss": 2.7203,
        "grad_norm": 3.589874267578125,
        "learning_rate": 7.778710923448296e-05,
        "epoch": 0.8576645055518295,
        "step": 11509
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.5736799240112305,
        "learning_rate": 7.771850699238059e-05,
        "epoch": 0.8577390267531113,
        "step": 11510
    },
    {
        "loss": 1.7257,
        "grad_norm": 4.516915798187256,
        "learning_rate": 7.764991578257499e-05,
        "epoch": 0.857813547954393,
        "step": 11511
    },
    {
        "loss": 2.617,
        "grad_norm": 2.655425786972046,
        "learning_rate": 7.758133563902776e-05,
        "epoch": 0.8578880691556748,
        "step": 11512
    },
    {
        "loss": 2.1964,
        "grad_norm": 2.24657940864563,
        "learning_rate": 7.75127665956955e-05,
        "epoch": 0.8579625903569565,
        "step": 11513
    },
    {
        "loss": 2.4699,
        "grad_norm": 3.4986631870269775,
        "learning_rate": 7.744420868652867e-05,
        "epoch": 0.8580371115582384,
        "step": 11514
    },
    {
        "loss": 2.7132,
        "grad_norm": 2.1099584102630615,
        "learning_rate": 7.737566194547278e-05,
        "epoch": 0.8581116327595201,
        "step": 11515
    },
    {
        "loss": 1.8855,
        "grad_norm": 3.4692554473876953,
        "learning_rate": 7.730712640646748e-05,
        "epoch": 0.8581861539608019,
        "step": 11516
    },
    {
        "loss": 3.1062,
        "grad_norm": 3.13114070892334,
        "learning_rate": 7.723860210344682e-05,
        "epoch": 0.8582606751620836,
        "step": 11517
    },
    {
        "loss": 2.7743,
        "grad_norm": 2.381009101867676,
        "learning_rate": 7.717008907033983e-05,
        "epoch": 0.8583351963633654,
        "step": 11518
    },
    {
        "loss": 3.0312,
        "grad_norm": 2.32053279876709,
        "learning_rate": 7.7101587341069e-05,
        "epoch": 0.8584097175646471,
        "step": 11519
    },
    {
        "loss": 2.6405,
        "grad_norm": 1.8174443244934082,
        "learning_rate": 7.70330969495522e-05,
        "epoch": 0.8584842387659289,
        "step": 11520
    },
    {
        "loss": 1.792,
        "grad_norm": 4.177026271820068,
        "learning_rate": 7.69646179297011e-05,
        "epoch": 0.8585587599672106,
        "step": 11521
    },
    {
        "loss": 2.658,
        "grad_norm": 1.4394328594207764,
        "learning_rate": 7.689615031542182e-05,
        "epoch": 0.8586332811684925,
        "step": 11522
    },
    {
        "loss": 1.8466,
        "grad_norm": 2.600377082824707,
        "learning_rate": 7.682769414061513e-05,
        "epoch": 0.8587078023697742,
        "step": 11523
    },
    {
        "loss": 2.4206,
        "grad_norm": 4.580883502960205,
        "learning_rate": 7.675924943917571e-05,
        "epoch": 0.858782323571056,
        "step": 11524
    },
    {
        "loss": 1.8397,
        "grad_norm": 3.590571165084839,
        "learning_rate": 7.669081624499301e-05,
        "epoch": 0.8588568447723377,
        "step": 11525
    },
    {
        "loss": 2.2647,
        "grad_norm": 3.6791434288024902,
        "learning_rate": 7.662239459195042e-05,
        "epoch": 0.8589313659736195,
        "step": 11526
    },
    {
        "loss": 2.5013,
        "grad_norm": 2.3881337642669678,
        "learning_rate": 7.655398451392573e-05,
        "epoch": 0.8590058871749012,
        "step": 11527
    },
    {
        "loss": 2.4242,
        "grad_norm": 2.3146560192108154,
        "learning_rate": 7.648558604479087e-05,
        "epoch": 0.859080408376183,
        "step": 11528
    },
    {
        "loss": 1.8978,
        "grad_norm": 2.6110429763793945,
        "learning_rate": 7.641719921841247e-05,
        "epoch": 0.8591549295774648,
        "step": 11529
    },
    {
        "loss": 2.6354,
        "grad_norm": 5.47658634185791,
        "learning_rate": 7.634882406865092e-05,
        "epoch": 0.8592294507787466,
        "step": 11530
    },
    {
        "loss": 3.2856,
        "grad_norm": 3.647465705871582,
        "learning_rate": 7.628046062936101e-05,
        "epoch": 0.8593039719800283,
        "step": 11531
    },
    {
        "loss": 2.6161,
        "grad_norm": 2.1494197845458984,
        "learning_rate": 7.621210893439154e-05,
        "epoch": 0.8593784931813101,
        "step": 11532
    },
    {
        "loss": 2.4307,
        "grad_norm": 2.409105062484741,
        "learning_rate": 7.6143769017586e-05,
        "epoch": 0.8594530143825918,
        "step": 11533
    },
    {
        "loss": 2.5416,
        "grad_norm": 2.0284037590026855,
        "learning_rate": 7.607544091278154e-05,
        "epoch": 0.8595275355838736,
        "step": 11534
    },
    {
        "loss": 1.8582,
        "grad_norm": 2.9783802032470703,
        "learning_rate": 7.600712465380955e-05,
        "epoch": 0.8596020567851553,
        "step": 11535
    },
    {
        "loss": 2.9736,
        "grad_norm": 2.1070330142974854,
        "learning_rate": 7.59388202744959e-05,
        "epoch": 0.8596765779864372,
        "step": 11536
    },
    {
        "loss": 2.4652,
        "grad_norm": 2.929844379425049,
        "learning_rate": 7.587052780866004e-05,
        "epoch": 0.8597510991877189,
        "step": 11537
    },
    {
        "loss": 2.641,
        "grad_norm": 2.18471097946167,
        "learning_rate": 7.58022472901162e-05,
        "epoch": 0.8598256203890007,
        "step": 11538
    },
    {
        "loss": 2.622,
        "grad_norm": 2.7464139461517334,
        "learning_rate": 7.57339787526718e-05,
        "epoch": 0.8599001415902824,
        "step": 11539
    },
    {
        "loss": 2.7483,
        "grad_norm": 2.3197972774505615,
        "learning_rate": 7.566572223012925e-05,
        "epoch": 0.8599746627915642,
        "step": 11540
    },
    {
        "loss": 1.7094,
        "grad_norm": 3.98368763923645,
        "learning_rate": 7.559747775628436e-05,
        "epoch": 0.8600491839928459,
        "step": 11541
    },
    {
        "loss": 2.6658,
        "grad_norm": 1.7276954650878906,
        "learning_rate": 7.552924536492716e-05,
        "epoch": 0.8601237051941277,
        "step": 11542
    },
    {
        "loss": 2.1968,
        "grad_norm": 1.338112235069275,
        "learning_rate": 7.546102508984199e-05,
        "epoch": 0.8601982263954095,
        "step": 11543
    },
    {
        "loss": 2.9087,
        "grad_norm": 2.6055991649627686,
        "learning_rate": 7.539281696480677e-05,
        "epoch": 0.8602727475966913,
        "step": 11544
    },
    {
        "loss": 2.5162,
        "grad_norm": 3.2225143909454346,
        "learning_rate": 7.532462102359348e-05,
        "epoch": 0.8603472687979731,
        "step": 11545
    },
    {
        "loss": 2.622,
        "grad_norm": 3.185873508453369,
        "learning_rate": 7.525643729996841e-05,
        "epoch": 0.8604217899992548,
        "step": 11546
    },
    {
        "loss": 2.5239,
        "grad_norm": 2.7578134536743164,
        "learning_rate": 7.518826582769144e-05,
        "epoch": 0.8604963112005366,
        "step": 11547
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.988495111465454,
        "learning_rate": 7.512010664051634e-05,
        "epoch": 0.8605708324018183,
        "step": 11548
    },
    {
        "loss": 2.1709,
        "grad_norm": 3.1536500453948975,
        "learning_rate": 7.505195977219137e-05,
        "epoch": 0.8606453536031001,
        "step": 11549
    },
    {
        "loss": 2.4837,
        "grad_norm": 2.202617883682251,
        "learning_rate": 7.498382525645776e-05,
        "epoch": 0.8607198748043818,
        "step": 11550
    },
    {
        "loss": 2.5207,
        "grad_norm": 3.1917009353637695,
        "learning_rate": 7.491570312705152e-05,
        "epoch": 0.8607943960056637,
        "step": 11551
    },
    {
        "loss": 2.8569,
        "grad_norm": 2.935372829437256,
        "learning_rate": 7.484759341770187e-05,
        "epoch": 0.8608689172069454,
        "step": 11552
    },
    {
        "loss": 2.2854,
        "grad_norm": 1.7291295528411865,
        "learning_rate": 7.477949616213244e-05,
        "epoch": 0.8609434384082272,
        "step": 11553
    },
    {
        "loss": 1.4492,
        "grad_norm": 3.987539052963257,
        "learning_rate": 7.471141139406028e-05,
        "epoch": 0.8610179596095089,
        "step": 11554
    },
    {
        "loss": 2.2543,
        "grad_norm": 3.028261184692383,
        "learning_rate": 7.464333914719624e-05,
        "epoch": 0.8610924808107907,
        "step": 11555
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.201815366744995,
        "learning_rate": 7.45752794552454e-05,
        "epoch": 0.8611670020120724,
        "step": 11556
    },
    {
        "loss": 1.5539,
        "grad_norm": 4.100773334503174,
        "learning_rate": 7.450723235190605e-05,
        "epoch": 0.8612415232133542,
        "step": 11557
    },
    {
        "loss": 2.2687,
        "grad_norm": 3.825166702270508,
        "learning_rate": 7.4439197870871e-05,
        "epoch": 0.8613160444146359,
        "step": 11558
    },
    {
        "loss": 2.6311,
        "grad_norm": 1.7135549783706665,
        "learning_rate": 7.437117604582573e-05,
        "epoch": 0.8613905656159178,
        "step": 11559
    },
    {
        "loss": 2.0609,
        "grad_norm": 3.7803940773010254,
        "learning_rate": 7.430316691045053e-05,
        "epoch": 0.8614650868171995,
        "step": 11560
    },
    {
        "loss": 2.2699,
        "grad_norm": 2.92754864692688,
        "learning_rate": 7.423517049841879e-05,
        "epoch": 0.8615396080184813,
        "step": 11561
    },
    {
        "loss": 2.1977,
        "grad_norm": 3.008610725402832,
        "learning_rate": 7.41671868433978e-05,
        "epoch": 0.861614129219763,
        "step": 11562
    },
    {
        "loss": 2.2504,
        "grad_norm": 1.6034939289093018,
        "learning_rate": 7.409921597904832e-05,
        "epoch": 0.8616886504210448,
        "step": 11563
    },
    {
        "loss": 2.2743,
        "grad_norm": 3.650172472000122,
        "learning_rate": 7.403125793902524e-05,
        "epoch": 0.8617631716223265,
        "step": 11564
    },
    {
        "loss": 2.1517,
        "grad_norm": 3.953005790710449,
        "learning_rate": 7.396331275697652e-05,
        "epoch": 0.8618376928236083,
        "step": 11565
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.8055777549743652,
        "learning_rate": 7.389538046654433e-05,
        "epoch": 0.86191221402489,
        "step": 11566
    },
    {
        "loss": 2.6252,
        "grad_norm": 3.289123773574829,
        "learning_rate": 7.382746110136405e-05,
        "epoch": 0.8619867352261719,
        "step": 11567
    },
    {
        "loss": 2.3556,
        "grad_norm": 3.04440975189209,
        "learning_rate": 7.375955469506462e-05,
        "epoch": 0.8620612564274536,
        "step": 11568
    },
    {
        "loss": 2.7528,
        "grad_norm": 2.613748550415039,
        "learning_rate": 7.369166128126913e-05,
        "epoch": 0.8621357776287354,
        "step": 11569
    },
    {
        "loss": 2.5952,
        "grad_norm": 4.637982368469238,
        "learning_rate": 7.362378089359332e-05,
        "epoch": 0.8622102988300171,
        "step": 11570
    },
    {
        "loss": 1.8323,
        "grad_norm": 4.13202428817749,
        "learning_rate": 7.355591356564735e-05,
        "epoch": 0.8622848200312989,
        "step": 11571
    },
    {
        "loss": 2.5526,
        "grad_norm": 1.9175927639007568,
        "learning_rate": 7.348805933103447e-05,
        "epoch": 0.8623593412325806,
        "step": 11572
    },
    {
        "loss": 2.537,
        "grad_norm": 2.6849477291107178,
        "learning_rate": 7.342021822335139e-05,
        "epoch": 0.8624338624338624,
        "step": 11573
    },
    {
        "loss": 2.7845,
        "grad_norm": 2.1005380153656006,
        "learning_rate": 7.335239027618872e-05,
        "epoch": 0.8625083836351441,
        "step": 11574
    },
    {
        "loss": 2.8142,
        "grad_norm": 3.2516143321990967,
        "learning_rate": 7.328457552313003e-05,
        "epoch": 0.862582904836426,
        "step": 11575
    },
    {
        "loss": 2.5591,
        "grad_norm": 3.1504602432250977,
        "learning_rate": 7.321677399775289e-05,
        "epoch": 0.8626574260377077,
        "step": 11576
    },
    {
        "loss": 1.7383,
        "grad_norm": 3.7850723266601562,
        "learning_rate": 7.314898573362794e-05,
        "epoch": 0.8627319472389895,
        "step": 11577
    },
    {
        "loss": 2.1797,
        "grad_norm": 3.339150905609131,
        "learning_rate": 7.308121076431935e-05,
        "epoch": 0.8628064684402713,
        "step": 11578
    },
    {
        "loss": 2.8514,
        "grad_norm": 3.457453966140747,
        "learning_rate": 7.301344912338459e-05,
        "epoch": 0.862880989641553,
        "step": 11579
    },
    {
        "loss": 2.2003,
        "grad_norm": 3.08715558052063,
        "learning_rate": 7.294570084437494e-05,
        "epoch": 0.8629555108428348,
        "step": 11580
    },
    {
        "loss": 2.5983,
        "grad_norm": 3.3172595500946045,
        "learning_rate": 7.287796596083469e-05,
        "epoch": 0.8630300320441165,
        "step": 11581
    },
    {
        "loss": 2.9034,
        "grad_norm": 3.103584051132202,
        "learning_rate": 7.281024450630153e-05,
        "epoch": 0.8631045532453984,
        "step": 11582
    },
    {
        "loss": 2.5152,
        "grad_norm": 2.2142388820648193,
        "learning_rate": 7.274253651430651e-05,
        "epoch": 0.8631790744466801,
        "step": 11583
    },
    {
        "loss": 1.901,
        "grad_norm": 4.086596488952637,
        "learning_rate": 7.267484201837427e-05,
        "epoch": 0.8632535956479619,
        "step": 11584
    },
    {
        "loss": 2.5681,
        "grad_norm": 2.4083876609802246,
        "learning_rate": 7.260716105202239e-05,
        "epoch": 0.8633281168492436,
        "step": 11585
    },
    {
        "loss": 2.0526,
        "grad_norm": 3.4014108180999756,
        "learning_rate": 7.253949364876212e-05,
        "epoch": 0.8634026380505254,
        "step": 11586
    },
    {
        "loss": 2.9832,
        "grad_norm": 3.0490407943725586,
        "learning_rate": 7.247183984209769e-05,
        "epoch": 0.8634771592518071,
        "step": 11587
    },
    {
        "loss": 2.5922,
        "grad_norm": 2.273789405822754,
        "learning_rate": 7.24041996655266e-05,
        "epoch": 0.8635516804530889,
        "step": 11588
    },
    {
        "loss": 2.3967,
        "grad_norm": 2.312479257583618,
        "learning_rate": 7.233657315254014e-05,
        "epoch": 0.8636262016543707,
        "step": 11589
    },
    {
        "loss": 1.5788,
        "grad_norm": 4.271915912628174,
        "learning_rate": 7.226896033662184e-05,
        "epoch": 0.8637007228556525,
        "step": 11590
    },
    {
        "loss": 2.4482,
        "grad_norm": 2.431361436843872,
        "learning_rate": 7.220136125124941e-05,
        "epoch": 0.8637752440569342,
        "step": 11591
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.5466558933258057,
        "learning_rate": 7.213377592989328e-05,
        "epoch": 0.863849765258216,
        "step": 11592
    },
    {
        "loss": 2.6026,
        "grad_norm": 2.123466730117798,
        "learning_rate": 7.206620440601699e-05,
        "epoch": 0.8639242864594977,
        "step": 11593
    },
    {
        "loss": 2.3553,
        "grad_norm": 1.9816645383834839,
        "learning_rate": 7.199864671307767e-05,
        "epoch": 0.8639988076607795,
        "step": 11594
    },
    {
        "loss": 2.2259,
        "grad_norm": 3.7455782890319824,
        "learning_rate": 7.193110288452525e-05,
        "epoch": 0.8640733288620612,
        "step": 11595
    },
    {
        "loss": 2.0145,
        "grad_norm": 3.8001997470855713,
        "learning_rate": 7.186357295380273e-05,
        "epoch": 0.864147850063343,
        "step": 11596
    },
    {
        "loss": 3.2665,
        "grad_norm": 3.477510690689087,
        "learning_rate": 7.179605695434668e-05,
        "epoch": 0.8642223712646248,
        "step": 11597
    },
    {
        "loss": 2.6333,
        "grad_norm": 3.089078187942505,
        "learning_rate": 7.172855491958632e-05,
        "epoch": 0.8642968924659066,
        "step": 11598
    },
    {
        "loss": 2.9006,
        "grad_norm": 2.4644663333892822,
        "learning_rate": 7.166106688294401e-05,
        "epoch": 0.8643714136671883,
        "step": 11599
    },
    {
        "loss": 2.1355,
        "grad_norm": 4.1144561767578125,
        "learning_rate": 7.159359287783567e-05,
        "epoch": 0.8644459348684701,
        "step": 11600
    },
    {
        "loss": 1.887,
        "grad_norm": 1.920924425125122,
        "learning_rate": 7.152613293766936e-05,
        "epoch": 0.8645204560697518,
        "step": 11601
    },
    {
        "loss": 2.6368,
        "grad_norm": 2.3652355670928955,
        "learning_rate": 7.14586870958471e-05,
        "epoch": 0.8645949772710336,
        "step": 11602
    },
    {
        "loss": 2.2995,
        "grad_norm": 3.066800594329834,
        "learning_rate": 7.139125538576327e-05,
        "epoch": 0.8646694984723153,
        "step": 11603
    },
    {
        "loss": 2.0183,
        "grad_norm": 3.7235398292541504,
        "learning_rate": 7.132383784080583e-05,
        "epoch": 0.8647440196735972,
        "step": 11604
    },
    {
        "loss": 1.9396,
        "grad_norm": 3.6473755836486816,
        "learning_rate": 7.125643449435519e-05,
        "epoch": 0.8648185408748789,
        "step": 11605
    },
    {
        "loss": 2.4522,
        "grad_norm": 2.2429423332214355,
        "learning_rate": 7.118904537978489e-05,
        "epoch": 0.8648930620761607,
        "step": 11606
    },
    {
        "loss": 2.5174,
        "grad_norm": 2.5620508193969727,
        "learning_rate": 7.112167053046172e-05,
        "epoch": 0.8649675832774424,
        "step": 11607
    },
    {
        "loss": 2.7673,
        "grad_norm": 2.7328765392303467,
        "learning_rate": 7.105430997974496e-05,
        "epoch": 0.8650421044787242,
        "step": 11608
    },
    {
        "loss": 2.4565,
        "grad_norm": 2.919018268585205,
        "learning_rate": 7.098696376098736e-05,
        "epoch": 0.8651166256800059,
        "step": 11609
    },
    {
        "loss": 1.9555,
        "grad_norm": 3.3349783420562744,
        "learning_rate": 7.091963190753372e-05,
        "epoch": 0.8651911468812877,
        "step": 11610
    },
    {
        "loss": 2.4447,
        "grad_norm": 3.682976484298706,
        "learning_rate": 7.08523144527227e-05,
        "epoch": 0.8652656680825694,
        "step": 11611
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.2238543033599854,
        "learning_rate": 7.078501142988515e-05,
        "epoch": 0.8653401892838513,
        "step": 11612
    },
    {
        "loss": 2.3014,
        "grad_norm": 2.512916326522827,
        "learning_rate": 7.071772287234493e-05,
        "epoch": 0.865414710485133,
        "step": 11613
    },
    {
        "loss": 1.7626,
        "grad_norm": 3.006748914718628,
        "learning_rate": 7.065044881341906e-05,
        "epoch": 0.8654892316864148,
        "step": 11614
    },
    {
        "loss": 2.1993,
        "grad_norm": 2.8099050521850586,
        "learning_rate": 7.058318928641701e-05,
        "epoch": 0.8655637528876966,
        "step": 11615
    },
    {
        "loss": 2.0786,
        "grad_norm": 3.36099910736084,
        "learning_rate": 7.051594432464101e-05,
        "epoch": 0.8656382740889783,
        "step": 11616
    },
    {
        "loss": 2.5836,
        "grad_norm": 3.1687865257263184,
        "learning_rate": 7.044871396138653e-05,
        "epoch": 0.8657127952902601,
        "step": 11617
    },
    {
        "loss": 2.1134,
        "grad_norm": 3.9490556716918945,
        "learning_rate": 7.038149822994138e-05,
        "epoch": 0.8657873164915418,
        "step": 11618
    },
    {
        "loss": 2.8784,
        "grad_norm": 2.6096413135528564,
        "learning_rate": 7.031429716358617e-05,
        "epoch": 0.8658618376928237,
        "step": 11619
    },
    {
        "loss": 2.7568,
        "grad_norm": 2.8864731788635254,
        "learning_rate": 7.02471107955947e-05,
        "epoch": 0.8659363588941054,
        "step": 11620
    },
    {
        "loss": 2.5691,
        "grad_norm": 4.443368434906006,
        "learning_rate": 7.017993915923262e-05,
        "epoch": 0.8660108800953872,
        "step": 11621
    },
    {
        "loss": 2.0645,
        "grad_norm": 2.6293461322784424,
        "learning_rate": 7.011278228775927e-05,
        "epoch": 0.8660854012966689,
        "step": 11622
    },
    {
        "loss": 2.2581,
        "grad_norm": 3.655439615249634,
        "learning_rate": 7.004564021442584e-05,
        "epoch": 0.8661599224979507,
        "step": 11623
    },
    {
        "loss": 2.354,
        "grad_norm": 3.2276203632354736,
        "learning_rate": 6.99785129724769e-05,
        "epoch": 0.8662344436992324,
        "step": 11624
    },
    {
        "loss": 2.3026,
        "grad_norm": 2.597439765930176,
        "learning_rate": 6.99114005951492e-05,
        "epoch": 0.8663089649005142,
        "step": 11625
    },
    {
        "loss": 1.8943,
        "grad_norm": 3.53792142868042,
        "learning_rate": 6.984430311567215e-05,
        "epoch": 0.8663834861017959,
        "step": 11626
    },
    {
        "loss": 2.4006,
        "grad_norm": 2.9556174278259277,
        "learning_rate": 6.977722056726819e-05,
        "epoch": 0.8664580073030778,
        "step": 11627
    },
    {
        "loss": 2.5193,
        "grad_norm": 2.2077584266662598,
        "learning_rate": 6.971015298315191e-05,
        "epoch": 0.8665325285043595,
        "step": 11628
    },
    {
        "loss": 1.8919,
        "grad_norm": 3.0089523792266846,
        "learning_rate": 6.964310039653069e-05,
        "epoch": 0.8666070497056413,
        "step": 11629
    },
    {
        "loss": 1.8843,
        "grad_norm": 2.9543991088867188,
        "learning_rate": 6.957606284060434e-05,
        "epoch": 0.866681570906923,
        "step": 11630
    },
    {
        "loss": 2.7384,
        "grad_norm": 2.624913215637207,
        "learning_rate": 6.950904034856561e-05,
        "epoch": 0.8667560921082048,
        "step": 11631
    },
    {
        "loss": 2.8934,
        "grad_norm": 1.9341156482696533,
        "learning_rate": 6.94420329535994e-05,
        "epoch": 0.8668306133094865,
        "step": 11632
    },
    {
        "loss": 2.0151,
        "grad_norm": 2.8350658416748047,
        "learning_rate": 6.937504068888323e-05,
        "epoch": 0.8669051345107683,
        "step": 11633
    },
    {
        "loss": 2.6298,
        "grad_norm": 1.9558374881744385,
        "learning_rate": 6.930806358758708e-05,
        "epoch": 0.86697965571205,
        "step": 11634
    },
    {
        "loss": 2.24,
        "grad_norm": 3.462029457092285,
        "learning_rate": 6.924110168287372e-05,
        "epoch": 0.8670541769133319,
        "step": 11635
    },
    {
        "loss": 2.332,
        "grad_norm": 3.9042842388153076,
        "learning_rate": 6.917415500789797e-05,
        "epoch": 0.8671286981146136,
        "step": 11636
    },
    {
        "loss": 2.6703,
        "grad_norm": 3.1047797203063965,
        "learning_rate": 6.910722359580755e-05,
        "epoch": 0.8672032193158954,
        "step": 11637
    },
    {
        "loss": 2.6556,
        "grad_norm": 2.352104663848877,
        "learning_rate": 6.904030747974229e-05,
        "epoch": 0.8672777405171771,
        "step": 11638
    },
    {
        "loss": 1.6796,
        "grad_norm": 4.106790542602539,
        "learning_rate": 6.897340669283438e-05,
        "epoch": 0.8673522617184589,
        "step": 11639
    },
    {
        "loss": 2.8381,
        "grad_norm": 3.1215553283691406,
        "learning_rate": 6.8906521268209e-05,
        "epoch": 0.8674267829197406,
        "step": 11640
    },
    {
        "loss": 2.3562,
        "grad_norm": 2.447727680206299,
        "learning_rate": 6.883965123898281e-05,
        "epoch": 0.8675013041210224,
        "step": 11641
    },
    {
        "loss": 1.7733,
        "grad_norm": 2.7832162380218506,
        "learning_rate": 6.877279663826573e-05,
        "epoch": 0.8675758253223042,
        "step": 11642
    },
    {
        "loss": 2.6317,
        "grad_norm": 3.1280646324157715,
        "learning_rate": 6.870595749915952e-05,
        "epoch": 0.867650346523586,
        "step": 11643
    },
    {
        "loss": 1.7388,
        "grad_norm": 3.393282890319824,
        "learning_rate": 6.863913385475831e-05,
        "epoch": 0.8677248677248677,
        "step": 11644
    },
    {
        "loss": 2.0945,
        "grad_norm": 3.424964666366577,
        "learning_rate": 6.857232573814896e-05,
        "epoch": 0.8677993889261495,
        "step": 11645
    },
    {
        "loss": 2.389,
        "grad_norm": 3.609312057495117,
        "learning_rate": 6.850553318241002e-05,
        "epoch": 0.8678739101274312,
        "step": 11646
    },
    {
        "loss": 2.3847,
        "grad_norm": 3.1450672149658203,
        "learning_rate": 6.8438756220613e-05,
        "epoch": 0.867948431328713,
        "step": 11647
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.9791361093521118,
        "learning_rate": 6.83719948858212e-05,
        "epoch": 0.8680229525299947,
        "step": 11648
    },
    {
        "loss": 2.5936,
        "grad_norm": 2.9150094985961914,
        "learning_rate": 6.830524921109037e-05,
        "epoch": 0.8680974737312765,
        "step": 11649
    },
    {
        "loss": 2.6121,
        "grad_norm": 1.9480903148651123,
        "learning_rate": 6.82385192294683e-05,
        "epoch": 0.8681719949325584,
        "step": 11650
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.5080485343933105,
        "learning_rate": 6.81718049739955e-05,
        "epoch": 0.8682465161338401,
        "step": 11651
    },
    {
        "loss": 1.7643,
        "grad_norm": 3.76497220993042,
        "learning_rate": 6.810510647770422e-05,
        "epoch": 0.8683210373351219,
        "step": 11652
    },
    {
        "loss": 2.7847,
        "grad_norm": 2.063995122909546,
        "learning_rate": 6.80384237736191e-05,
        "epoch": 0.8683955585364036,
        "step": 11653
    },
    {
        "loss": 2.4625,
        "grad_norm": 2.9177305698394775,
        "learning_rate": 6.797175689475677e-05,
        "epoch": 0.8684700797376854,
        "step": 11654
    },
    {
        "loss": 2.433,
        "grad_norm": 3.3845460414886475,
        "learning_rate": 6.790510587412644e-05,
        "epoch": 0.8685446009389671,
        "step": 11655
    },
    {
        "loss": 2.9394,
        "grad_norm": 3.1166772842407227,
        "learning_rate": 6.783847074472914e-05,
        "epoch": 0.868619122140249,
        "step": 11656
    },
    {
        "loss": 2.8648,
        "grad_norm": 3.7658965587615967,
        "learning_rate": 6.77718515395579e-05,
        "epoch": 0.8686936433415307,
        "step": 11657
    },
    {
        "loss": 2.8183,
        "grad_norm": 3.2179548740386963,
        "learning_rate": 6.770524829159836e-05,
        "epoch": 0.8687681645428125,
        "step": 11658
    },
    {
        "loss": 2.2013,
        "grad_norm": 4.828383445739746,
        "learning_rate": 6.763866103382771e-05,
        "epoch": 0.8688426857440942,
        "step": 11659
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.53598690032959,
        "learning_rate": 6.75720897992159e-05,
        "epoch": 0.868917206945376,
        "step": 11660
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.7046267986297607,
        "learning_rate": 6.750553462072395e-05,
        "epoch": 0.8689917281466577,
        "step": 11661
    },
    {
        "loss": 2.3995,
        "grad_norm": 2.224041223526001,
        "learning_rate": 6.743899553130595e-05,
        "epoch": 0.8690662493479395,
        "step": 11662
    },
    {
        "loss": 2.1797,
        "grad_norm": 3.5369880199432373,
        "learning_rate": 6.737247256390742e-05,
        "epoch": 0.8691407705492212,
        "step": 11663
    },
    {
        "loss": 2.359,
        "grad_norm": 3.2376508712768555,
        "learning_rate": 6.730596575146593e-05,
        "epoch": 0.869215291750503,
        "step": 11664
    },
    {
        "loss": 2.168,
        "grad_norm": 3.5426852703094482,
        "learning_rate": 6.723947512691149e-05,
        "epoch": 0.8692898129517848,
        "step": 11665
    },
    {
        "loss": 2.628,
        "grad_norm": 3.454881429672241,
        "learning_rate": 6.717300072316559e-05,
        "epoch": 0.8693643341530666,
        "step": 11666
    },
    {
        "loss": 2.5452,
        "grad_norm": 2.205932140350342,
        "learning_rate": 6.710654257314177e-05,
        "epoch": 0.8694388553543483,
        "step": 11667
    },
    {
        "loss": 3.0189,
        "grad_norm": 2.562652349472046,
        "learning_rate": 6.704010070974594e-05,
        "epoch": 0.8695133765556301,
        "step": 11668
    },
    {
        "loss": 2.0685,
        "grad_norm": 3.1622540950775146,
        "learning_rate": 6.697367516587545e-05,
        "epoch": 0.8695878977569118,
        "step": 11669
    },
    {
        "loss": 3.4504,
        "grad_norm": 4.388403415679932,
        "learning_rate": 6.690726597441967e-05,
        "epoch": 0.8696624189581936,
        "step": 11670
    },
    {
        "loss": 1.8757,
        "grad_norm": 4.754248142242432,
        "learning_rate": 6.684087316826036e-05,
        "epoch": 0.8697369401594753,
        "step": 11671
    },
    {
        "loss": 2.3728,
        "grad_norm": 2.0723342895507812,
        "learning_rate": 6.677449678027022e-05,
        "epoch": 0.8698114613607572,
        "step": 11672
    },
    {
        "loss": 1.3479,
        "grad_norm": 5.035161972045898,
        "learning_rate": 6.670813684331478e-05,
        "epoch": 0.8698859825620389,
        "step": 11673
    },
    {
        "loss": 2.0305,
        "grad_norm": 2.8940367698669434,
        "learning_rate": 6.664179339025076e-05,
        "epoch": 0.8699605037633207,
        "step": 11674
    },
    {
        "loss": 3.0152,
        "grad_norm": 2.4610559940338135,
        "learning_rate": 6.657546645392723e-05,
        "epoch": 0.8700350249646024,
        "step": 11675
    },
    {
        "loss": 1.923,
        "grad_norm": 4.904570579528809,
        "learning_rate": 6.650915606718473e-05,
        "epoch": 0.8701095461658842,
        "step": 11676
    },
    {
        "loss": 2.2644,
        "grad_norm": 3.2326807975769043,
        "learning_rate": 6.644286226285552e-05,
        "epoch": 0.8701840673671659,
        "step": 11677
    },
    {
        "loss": 2.3077,
        "grad_norm": 3.0128133296966553,
        "learning_rate": 6.637658507376412e-05,
        "epoch": 0.8702585885684477,
        "step": 11678
    },
    {
        "loss": 2.4108,
        "grad_norm": 3.170234203338623,
        "learning_rate": 6.63103245327263e-05,
        "epoch": 0.8703331097697294,
        "step": 11679
    },
    {
        "loss": 1.9877,
        "grad_norm": Infinity,
        "learning_rate": 6.63103245327263e-05,
        "epoch": 0.8704076309710113,
        "step": 11680
    },
    {
        "loss": 1.5066,
        "grad_norm": 2.7226967811584473,
        "learning_rate": 6.624408067255017e-05,
        "epoch": 0.870482152172293,
        "step": 11681
    },
    {
        "loss": 1.8913,
        "grad_norm": 3.72072172164917,
        "learning_rate": 6.617785352603473e-05,
        "epoch": 0.8705566733735748,
        "step": 11682
    },
    {
        "loss": 2.6818,
        "grad_norm": 2.032550573348999,
        "learning_rate": 6.611164312597162e-05,
        "epoch": 0.8706311945748565,
        "step": 11683
    },
    {
        "loss": 2.568,
        "grad_norm": 2.4077653884887695,
        "learning_rate": 6.604544950514363e-05,
        "epoch": 0.8707057157761383,
        "step": 11684
    },
    {
        "loss": 2.4899,
        "grad_norm": 2.845773220062256,
        "learning_rate": 6.597927269632525e-05,
        "epoch": 0.8707802369774201,
        "step": 11685
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.717298984527588,
        "learning_rate": 6.591311273228304e-05,
        "epoch": 0.8708547581787018,
        "step": 11686
    },
    {
        "loss": 2.4382,
        "grad_norm": 2.438051700592041,
        "learning_rate": 6.584696964577485e-05,
        "epoch": 0.8709292793799837,
        "step": 11687
    },
    {
        "loss": 2.4157,
        "grad_norm": 1.962250828742981,
        "learning_rate": 6.57808434695502e-05,
        "epoch": 0.8710038005812654,
        "step": 11688
    },
    {
        "loss": 2.0338,
        "grad_norm": 3.0529308319091797,
        "learning_rate": 6.57147342363505e-05,
        "epoch": 0.8710783217825472,
        "step": 11689
    },
    {
        "loss": 1.4832,
        "grad_norm": 2.9305472373962402,
        "learning_rate": 6.56486419789085e-05,
        "epoch": 0.8711528429838289,
        "step": 11690
    },
    {
        "loss": 2.5079,
        "grad_norm": 2.8745179176330566,
        "learning_rate": 6.558256672994853e-05,
        "epoch": 0.8712273641851107,
        "step": 11691
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.113856554031372,
        "learning_rate": 6.5516508522187e-05,
        "epoch": 0.8713018853863924,
        "step": 11692
    },
    {
        "loss": 2.7897,
        "grad_norm": 1.7180601358413696,
        "learning_rate": 6.545046738833096e-05,
        "epoch": 0.8713764065876742,
        "step": 11693
    },
    {
        "loss": 2.3656,
        "grad_norm": 2.434096097946167,
        "learning_rate": 6.53844433610799e-05,
        "epoch": 0.8714509277889559,
        "step": 11694
    },
    {
        "loss": 2.1116,
        "grad_norm": 1.9488579034805298,
        "learning_rate": 6.531843647312441e-05,
        "epoch": 0.8715254489902378,
        "step": 11695
    },
    {
        "loss": 2.3468,
        "grad_norm": 3.753490447998047,
        "learning_rate": 6.525244675714647e-05,
        "epoch": 0.8715999701915195,
        "step": 11696
    },
    {
        "loss": 1.8033,
        "grad_norm": 4.473696231842041,
        "learning_rate": 6.518647424582005e-05,
        "epoch": 0.8716744913928013,
        "step": 11697
    },
    {
        "loss": 2.2103,
        "grad_norm": 3.915696620941162,
        "learning_rate": 6.512051897181001e-05,
        "epoch": 0.871749012594083,
        "step": 11698
    },
    {
        "loss": 2.3759,
        "grad_norm": 3.0788779258728027,
        "learning_rate": 6.505458096777326e-05,
        "epoch": 0.8718235337953648,
        "step": 11699
    },
    {
        "loss": 2.3725,
        "grad_norm": 3.803518295288086,
        "learning_rate": 6.498866026635771e-05,
        "epoch": 0.8718980549966465,
        "step": 11700
    },
    {
        "loss": 2.2341,
        "grad_norm": 2.5659639835357666,
        "learning_rate": 6.492275690020285e-05,
        "epoch": 0.8719725761979283,
        "step": 11701
    },
    {
        "loss": 2.4242,
        "grad_norm": 2.341036319732666,
        "learning_rate": 6.485687090193948e-05,
        "epoch": 0.87204709739921,
        "step": 11702
    },
    {
        "loss": 2.2759,
        "grad_norm": 2.9046952724456787,
        "learning_rate": 6.47910023041902e-05,
        "epoch": 0.8721216186004919,
        "step": 11703
    },
    {
        "loss": 2.3548,
        "grad_norm": 3.7000155448913574,
        "learning_rate": 6.472515113956854e-05,
        "epoch": 0.8721961398017736,
        "step": 11704
    },
    {
        "loss": 2.2933,
        "grad_norm": 3.789069175720215,
        "learning_rate": 6.465931744067957e-05,
        "epoch": 0.8722706610030554,
        "step": 11705
    },
    {
        "loss": 2.664,
        "grad_norm": 3.992793083190918,
        "learning_rate": 6.45935012401196e-05,
        "epoch": 0.8723451822043371,
        "step": 11706
    },
    {
        "loss": 1.4469,
        "grad_norm": 2.653440237045288,
        "learning_rate": 6.452770257047665e-05,
        "epoch": 0.8724197034056189,
        "step": 11707
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.525115489959717,
        "learning_rate": 6.44619214643295e-05,
        "epoch": 0.8724942246069006,
        "step": 11708
    },
    {
        "loss": 2.2747,
        "grad_norm": 2.6546525955200195,
        "learning_rate": 6.43961579542488e-05,
        "epoch": 0.8725687458081824,
        "step": 11709
    },
    {
        "loss": 2.2378,
        "grad_norm": 4.101053237915039,
        "learning_rate": 6.433041207279614e-05,
        "epoch": 0.8726432670094642,
        "step": 11710
    },
    {
        "loss": 1.3264,
        "grad_norm": 2.8592569828033447,
        "learning_rate": 6.426468385252427e-05,
        "epoch": 0.872717788210746,
        "step": 11711
    },
    {
        "loss": 2.7115,
        "grad_norm": 2.6155593395233154,
        "learning_rate": 6.419897332597778e-05,
        "epoch": 0.8727923094120277,
        "step": 11712
    },
    {
        "loss": 2.5291,
        "grad_norm": 2.037982225418091,
        "learning_rate": 6.41332805256916e-05,
        "epoch": 0.8728668306133095,
        "step": 11713
    },
    {
        "loss": 2.7855,
        "grad_norm": 3.2436583042144775,
        "learning_rate": 6.40676054841928e-05,
        "epoch": 0.8729413518145912,
        "step": 11714
    },
    {
        "loss": 2.5309,
        "grad_norm": 3.8017585277557373,
        "learning_rate": 6.400194823399905e-05,
        "epoch": 0.873015873015873,
        "step": 11715
    },
    {
        "loss": 2.3267,
        "grad_norm": 2.0800092220306396,
        "learning_rate": 6.393630880761933e-05,
        "epoch": 0.8730903942171547,
        "step": 11716
    },
    {
        "loss": 2.4668,
        "grad_norm": 3.645374298095703,
        "learning_rate": 6.387068723755409e-05,
        "epoch": 0.8731649154184365,
        "step": 11717
    },
    {
        "loss": 2.7269,
        "grad_norm": 2.819190502166748,
        "learning_rate": 6.38050835562945e-05,
        "epoch": 0.8732394366197183,
        "step": 11718
    },
    {
        "loss": 3.0606,
        "grad_norm": 2.5625734329223633,
        "learning_rate": 6.373949779632333e-05,
        "epoch": 0.8733139578210001,
        "step": 11719
    },
    {
        "loss": 2.1385,
        "grad_norm": 3.4114460945129395,
        "learning_rate": 6.367392999011409e-05,
        "epoch": 0.8733884790222819,
        "step": 11720
    },
    {
        "loss": 1.5685,
        "grad_norm": 3.5555450916290283,
        "learning_rate": 6.360838017013156e-05,
        "epoch": 0.8734630002235636,
        "step": 11721
    },
    {
        "loss": 2.0812,
        "grad_norm": 2.850102663040161,
        "learning_rate": 6.354284836883151e-05,
        "epoch": 0.8735375214248454,
        "step": 11722
    },
    {
        "loss": 2.1226,
        "grad_norm": 3.5233867168426514,
        "learning_rate": 6.347733461866119e-05,
        "epoch": 0.8736120426261271,
        "step": 11723
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.4699862003326416,
        "learning_rate": 6.341183895205815e-05,
        "epoch": 0.873686563827409,
        "step": 11724
    },
    {
        "loss": 1.9354,
        "grad_norm": 1.537980318069458,
        "learning_rate": 6.334636140145181e-05,
        "epoch": 0.8737610850286907,
        "step": 11725
    },
    {
        "loss": 2.5278,
        "grad_norm": 2.8813562393188477,
        "learning_rate": 6.328090199926199e-05,
        "epoch": 0.8738356062299725,
        "step": 11726
    },
    {
        "loss": 2.4494,
        "grad_norm": 3.200528621673584,
        "learning_rate": 6.321546077790003e-05,
        "epoch": 0.8739101274312542,
        "step": 11727
    },
    {
        "loss": 2.5459,
        "grad_norm": 3.1813933849334717,
        "learning_rate": 6.31500377697679e-05,
        "epoch": 0.873984648632536,
        "step": 11728
    },
    {
        "loss": 2.6743,
        "grad_norm": 2.731050729751587,
        "learning_rate": 6.308463300725854e-05,
        "epoch": 0.8740591698338177,
        "step": 11729
    },
    {
        "loss": 2.4444,
        "grad_norm": 2.8933403491973877,
        "learning_rate": 6.301924652275628e-05,
        "epoch": 0.8741336910350995,
        "step": 11730
    },
    {
        "loss": 2.6314,
        "grad_norm": 3.364206552505493,
        "learning_rate": 6.295387834863581e-05,
        "epoch": 0.8742082122363812,
        "step": 11731
    },
    {
        "loss": 2.154,
        "grad_norm": 3.0956432819366455,
        "learning_rate": 6.288852851726345e-05,
        "epoch": 0.874282733437663,
        "step": 11732
    },
    {
        "loss": 2.6581,
        "grad_norm": 2.827667474746704,
        "learning_rate": 6.282319706099555e-05,
        "epoch": 0.8743572546389448,
        "step": 11733
    },
    {
        "loss": 2.1501,
        "grad_norm": 2.7106714248657227,
        "learning_rate": 6.275788401218021e-05,
        "epoch": 0.8744317758402266,
        "step": 11734
    },
    {
        "loss": 2.3217,
        "grad_norm": 3.0938870906829834,
        "learning_rate": 6.269258940315595e-05,
        "epoch": 0.8745062970415083,
        "step": 11735
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.6277923583984375,
        "learning_rate": 6.262731326625211e-05,
        "epoch": 0.8745808182427901,
        "step": 11736
    },
    {
        "loss": 2.4369,
        "grad_norm": 2.4444310665130615,
        "learning_rate": 6.256205563378935e-05,
        "epoch": 0.8746553394440718,
        "step": 11737
    },
    {
        "loss": 1.5531,
        "grad_norm": 1.7340854406356812,
        "learning_rate": 6.249681653807868e-05,
        "epoch": 0.8747298606453536,
        "step": 11738
    },
    {
        "loss": 2.8358,
        "grad_norm": 2.587245225906372,
        "learning_rate": 6.2431596011422e-05,
        "epoch": 0.8748043818466353,
        "step": 11739
    },
    {
        "loss": 2.61,
        "grad_norm": 2.6474006175994873,
        "learning_rate": 6.236639408611242e-05,
        "epoch": 0.8748789030479172,
        "step": 11740
    },
    {
        "loss": 1.7607,
        "grad_norm": 3.4815778732299805,
        "learning_rate": 6.230121079443341e-05,
        "epoch": 0.8749534242491989,
        "step": 11741
    },
    {
        "loss": 2.0215,
        "grad_norm": 2.9406044483184814,
        "learning_rate": 6.223604616865917e-05,
        "epoch": 0.8750279454504807,
        "step": 11742
    },
    {
        "loss": 2.3406,
        "grad_norm": 4.11369514465332,
        "learning_rate": 6.217090024105525e-05,
        "epoch": 0.8751024666517624,
        "step": 11743
    },
    {
        "loss": 2.7588,
        "grad_norm": 3.5816636085510254,
        "learning_rate": 6.210577304387706e-05,
        "epoch": 0.8751769878530442,
        "step": 11744
    },
    {
        "loss": 1.8918,
        "grad_norm": 2.732069253921509,
        "learning_rate": 6.204066460937157e-05,
        "epoch": 0.8752515090543259,
        "step": 11745
    },
    {
        "loss": 2.453,
        "grad_norm": 2.7498905658721924,
        "learning_rate": 6.197557496977581e-05,
        "epoch": 0.8753260302556077,
        "step": 11746
    },
    {
        "loss": 2.0729,
        "grad_norm": 2.2320923805236816,
        "learning_rate": 6.191050415731807e-05,
        "epoch": 0.8754005514568894,
        "step": 11747
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.40464448928833,
        "learning_rate": 6.184545220421693e-05,
        "epoch": 0.8754750726581713,
        "step": 11748
    },
    {
        "loss": 2.8175,
        "grad_norm": 2.4342286586761475,
        "learning_rate": 6.17804191426816e-05,
        "epoch": 0.875549593859453,
        "step": 11749
    },
    {
        "loss": 1.893,
        "grad_norm": 2.47812819480896,
        "learning_rate": 6.171540500491233e-05,
        "epoch": 0.8756241150607348,
        "step": 11750
    },
    {
        "loss": 2.2482,
        "grad_norm": 3.0989480018615723,
        "learning_rate": 6.16504098230996e-05,
        "epoch": 0.8756986362620165,
        "step": 11751
    },
    {
        "loss": 2.6552,
        "grad_norm": 2.4157967567443848,
        "learning_rate": 6.15854336294247e-05,
        "epoch": 0.8757731574632983,
        "step": 11752
    },
    {
        "loss": 2.7174,
        "grad_norm": 2.8774096965789795,
        "learning_rate": 6.152047645605932e-05,
        "epoch": 0.87584767866458,
        "step": 11753
    },
    {
        "loss": 1.2118,
        "grad_norm": 3.292518138885498,
        "learning_rate": 6.145553833516613e-05,
        "epoch": 0.8759221998658618,
        "step": 11754
    },
    {
        "loss": 2.3783,
        "grad_norm": 2.800238847732544,
        "learning_rate": 6.139061929889799e-05,
        "epoch": 0.8759967210671437,
        "step": 11755
    },
    {
        "loss": 2.2347,
        "grad_norm": 2.9600064754486084,
        "learning_rate": 6.132571937939847e-05,
        "epoch": 0.8760712422684254,
        "step": 11756
    },
    {
        "loss": 2.9589,
        "grad_norm": 4.424222469329834,
        "learning_rate": 6.126083860880148e-05,
        "epoch": 0.8761457634697072,
        "step": 11757
    },
    {
        "loss": 1.9409,
        "grad_norm": 2.3421993255615234,
        "learning_rate": 6.11959770192319e-05,
        "epoch": 0.8762202846709889,
        "step": 11758
    },
    {
        "loss": 1.7795,
        "grad_norm": 3.092068910598755,
        "learning_rate": 6.113113464280455e-05,
        "epoch": 0.8762948058722707,
        "step": 11759
    },
    {
        "loss": 2.1936,
        "grad_norm": 1.509772777557373,
        "learning_rate": 6.10663115116253e-05,
        "epoch": 0.8763693270735524,
        "step": 11760
    },
    {
        "loss": 1.8271,
        "grad_norm": 2.9383301734924316,
        "learning_rate": 6.1001507657790026e-05,
        "epoch": 0.8764438482748342,
        "step": 11761
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.6787197589874268,
        "learning_rate": 6.093672311338514e-05,
        "epoch": 0.8765183694761159,
        "step": 11762
    },
    {
        "loss": 1.9832,
        "grad_norm": 3.2112553119659424,
        "learning_rate": 6.0871957910488e-05,
        "epoch": 0.8765928906773978,
        "step": 11763
    },
    {
        "loss": 2.4277,
        "grad_norm": 3.1373848915100098,
        "learning_rate": 6.0807212081165423e-05,
        "epoch": 0.8766674118786795,
        "step": 11764
    },
    {
        "loss": 3.0307,
        "grad_norm": 2.7066617012023926,
        "learning_rate": 6.074248565747558e-05,
        "epoch": 0.8767419330799613,
        "step": 11765
    },
    {
        "loss": 2.0147,
        "grad_norm": 1.9814536571502686,
        "learning_rate": 6.0677778671466534e-05,
        "epoch": 0.876816454281243,
        "step": 11766
    },
    {
        "loss": 2.5027,
        "grad_norm": 3.568488359451294,
        "learning_rate": 6.061309115517673e-05,
        "epoch": 0.8768909754825248,
        "step": 11767
    },
    {
        "loss": 2.8278,
        "grad_norm": 5.363282680511475,
        "learning_rate": 6.0548423140635246e-05,
        "epoch": 0.8769654966838065,
        "step": 11768
    },
    {
        "loss": 2.3229,
        "grad_norm": 3.8264005184173584,
        "learning_rate": 6.048377465986116e-05,
        "epoch": 0.8770400178850883,
        "step": 11769
    },
    {
        "loss": 2.0829,
        "grad_norm": 3.675445318222046,
        "learning_rate": 6.0419145744864255e-05,
        "epoch": 0.87711453908637,
        "step": 11770
    },
    {
        "loss": 2.438,
        "grad_norm": 3.1054060459136963,
        "learning_rate": 6.035453642764435e-05,
        "epoch": 0.8771890602876519,
        "step": 11771
    },
    {
        "loss": 2.89,
        "grad_norm": 2.051400661468506,
        "learning_rate": 6.028994674019157e-05,
        "epoch": 0.8772635814889336,
        "step": 11772
    },
    {
        "loss": 2.6413,
        "grad_norm": 2.4880597591400146,
        "learning_rate": 6.022537671448629e-05,
        "epoch": 0.8773381026902154,
        "step": 11773
    },
    {
        "loss": 1.9088,
        "grad_norm": 3.956033706665039,
        "learning_rate": 6.01608263824995e-05,
        "epoch": 0.8774126238914971,
        "step": 11774
    },
    {
        "loss": 2.2589,
        "grad_norm": 3.7383334636688232,
        "learning_rate": 6.009629577619207e-05,
        "epoch": 0.8774871450927789,
        "step": 11775
    },
    {
        "loss": 2.4394,
        "grad_norm": 2.2479214668273926,
        "learning_rate": 6.003178492751518e-05,
        "epoch": 0.8775616662940606,
        "step": 11776
    },
    {
        "loss": 2.3547,
        "grad_norm": 2.9163074493408203,
        "learning_rate": 5.9967293868410146e-05,
        "epoch": 0.8776361874953424,
        "step": 11777
    },
    {
        "loss": 2.5684,
        "grad_norm": 4.470251083374023,
        "learning_rate": 5.990282263080889e-05,
        "epoch": 0.8777107086966242,
        "step": 11778
    },
    {
        "loss": 1.9143,
        "grad_norm": 2.573362350463867,
        "learning_rate": 5.983837124663294e-05,
        "epoch": 0.877785229897906,
        "step": 11779
    },
    {
        "loss": 2.358,
        "grad_norm": 3.6949994564056396,
        "learning_rate": 5.977393974779455e-05,
        "epoch": 0.8778597510991877,
        "step": 11780
    },
    {
        "loss": 2.1314,
        "grad_norm": 2.5508787631988525,
        "learning_rate": 5.970952816619573e-05,
        "epoch": 0.8779342723004695,
        "step": 11781
    },
    {
        "loss": 2.4313,
        "grad_norm": 2.9075253009796143,
        "learning_rate": 5.964513653372866e-05,
        "epoch": 0.8780087935017512,
        "step": 11782
    },
    {
        "loss": 3.0295,
        "grad_norm": 3.2315282821655273,
        "learning_rate": 5.958076488227613e-05,
        "epoch": 0.878083314703033,
        "step": 11783
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.2752885818481445,
        "learning_rate": 5.951641324371011e-05,
        "epoch": 0.8781578359043147,
        "step": 11784
    },
    {
        "loss": 2.3963,
        "grad_norm": 3.2437403202056885,
        "learning_rate": 5.945208164989362e-05,
        "epoch": 0.8782323571055966,
        "step": 11785
    },
    {
        "loss": 2.6301,
        "grad_norm": 3.0643739700317383,
        "learning_rate": 5.93877701326792e-05,
        "epoch": 0.8783068783068783,
        "step": 11786
    },
    {
        "loss": 2.6921,
        "grad_norm": 3.605630397796631,
        "learning_rate": 5.932347872390944e-05,
        "epoch": 0.8783813995081601,
        "step": 11787
    },
    {
        "loss": 1.2367,
        "grad_norm": 3.9433674812316895,
        "learning_rate": 5.925920745541739e-05,
        "epoch": 0.8784559207094418,
        "step": 11788
    },
    {
        "loss": 3.0662,
        "grad_norm": 2.854917287826538,
        "learning_rate": 5.9194956359025734e-05,
        "epoch": 0.8785304419107236,
        "step": 11789
    },
    {
        "loss": 1.9642,
        "grad_norm": 2.7174112796783447,
        "learning_rate": 5.9130725466547185e-05,
        "epoch": 0.8786049631120053,
        "step": 11790
    },
    {
        "loss": 2.0373,
        "grad_norm": 3.294201135635376,
        "learning_rate": 5.906651480978478e-05,
        "epoch": 0.8786794843132871,
        "step": 11791
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.082425594329834,
        "learning_rate": 5.900232442053122e-05,
        "epoch": 0.878754005514569,
        "step": 11792
    },
    {
        "loss": 1.9386,
        "grad_norm": 3.3111770153045654,
        "learning_rate": 5.893815433056911e-05,
        "epoch": 0.8788285267158507,
        "step": 11793
    },
    {
        "loss": 2.3301,
        "grad_norm": 4.272182941436768,
        "learning_rate": 5.887400457167158e-05,
        "epoch": 0.8789030479171325,
        "step": 11794
    },
    {
        "loss": 1.6743,
        "grad_norm": 4.176321506500244,
        "learning_rate": 5.880987517560075e-05,
        "epoch": 0.8789775691184142,
        "step": 11795
    },
    {
        "loss": 2.2112,
        "grad_norm": 3.301748752593994,
        "learning_rate": 5.874576617410956e-05,
        "epoch": 0.879052090319696,
        "step": 11796
    },
    {
        "loss": 2.5245,
        "grad_norm": 2.7229042053222656,
        "learning_rate": 5.868167759894023e-05,
        "epoch": 0.8791266115209777,
        "step": 11797
    },
    {
        "loss": 2.4607,
        "grad_norm": 3.4998860359191895,
        "learning_rate": 5.8617609481825385e-05,
        "epoch": 0.8792011327222595,
        "step": 11798
    },
    {
        "loss": 2.5683,
        "grad_norm": 3.6258697509765625,
        "learning_rate": 5.855356185448707e-05,
        "epoch": 0.8792756539235412,
        "step": 11799
    },
    {
        "loss": 1.5643,
        "grad_norm": 3.018795967102051,
        "learning_rate": 5.848953474863727e-05,
        "epoch": 0.879350175124823,
        "step": 11800
    },
    {
        "loss": 2.0769,
        "grad_norm": 2.8184399604797363,
        "learning_rate": 5.842552819597818e-05,
        "epoch": 0.8794246963261048,
        "step": 11801
    },
    {
        "loss": 2.5324,
        "grad_norm": 2.306942939758301,
        "learning_rate": 5.836154222820121e-05,
        "epoch": 0.8794992175273866,
        "step": 11802
    },
    {
        "loss": 1.7584,
        "grad_norm": 3.5031447410583496,
        "learning_rate": 5.829757687698838e-05,
        "epoch": 0.8795737387286683,
        "step": 11803
    },
    {
        "loss": 2.1876,
        "grad_norm": 2.9789233207702637,
        "learning_rate": 5.8233632174010456e-05,
        "epoch": 0.8796482599299501,
        "step": 11804
    },
    {
        "loss": 2.6972,
        "grad_norm": 2.1176676750183105,
        "learning_rate": 5.816970815092895e-05,
        "epoch": 0.8797227811312318,
        "step": 11805
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.6646640300750732,
        "learning_rate": 5.810580483939463e-05,
        "epoch": 0.8797973023325136,
        "step": 11806
    },
    {
        "loss": 2.6244,
        "grad_norm": 2.2462949752807617,
        "learning_rate": 5.8041922271047966e-05,
        "epoch": 0.8798718235337953,
        "step": 11807
    },
    {
        "loss": 2.4621,
        "grad_norm": 2.801699161529541,
        "learning_rate": 5.797806047751957e-05,
        "epoch": 0.8799463447350772,
        "step": 11808
    },
    {
        "loss": 2.117,
        "grad_norm": 4.197883129119873,
        "learning_rate": 5.79142194904294e-05,
        "epoch": 0.8800208659363589,
        "step": 11809
    },
    {
        "loss": 1.4736,
        "grad_norm": 4.664574146270752,
        "learning_rate": 5.7850399341387076e-05,
        "epoch": 0.8800953871376407,
        "step": 11810
    },
    {
        "loss": 2.4108,
        "grad_norm": 3.5843610763549805,
        "learning_rate": 5.778660006199235e-05,
        "epoch": 0.8801699083389224,
        "step": 11811
    },
    {
        "loss": 2.0265,
        "grad_norm": 4.123728275299072,
        "learning_rate": 5.772282168383409e-05,
        "epoch": 0.8802444295402042,
        "step": 11812
    },
    {
        "loss": 2.5098,
        "grad_norm": 1.8817009925842285,
        "learning_rate": 5.765906423849104e-05,
        "epoch": 0.8803189507414859,
        "step": 11813
    },
    {
        "loss": 2.9455,
        "grad_norm": 3.540970802307129,
        "learning_rate": 5.75953277575319e-05,
        "epoch": 0.8803934719427677,
        "step": 11814
    },
    {
        "loss": 2.432,
        "grad_norm": 2.1098735332489014,
        "learning_rate": 5.7531612272514246e-05,
        "epoch": 0.8804679931440494,
        "step": 11815
    },
    {
        "loss": 2.0023,
        "grad_norm": 2.814523458480835,
        "learning_rate": 5.746791781498604e-05,
        "epoch": 0.8805425143453313,
        "step": 11816
    },
    {
        "loss": 2.3554,
        "grad_norm": 3.005842685699463,
        "learning_rate": 5.74042444164844e-05,
        "epoch": 0.880617035546613,
        "step": 11817
    },
    {
        "loss": 2.1794,
        "grad_norm": 2.21994686126709,
        "learning_rate": 5.734059210853599e-05,
        "epoch": 0.8806915567478948,
        "step": 11818
    },
    {
        "loss": 2.7651,
        "grad_norm": 2.16595721244812,
        "learning_rate": 5.7276960922657354e-05,
        "epoch": 0.8807660779491765,
        "step": 11819
    },
    {
        "loss": 2.8836,
        "grad_norm": 2.7733397483825684,
        "learning_rate": 5.7213350890354156e-05,
        "epoch": 0.8808405991504583,
        "step": 11820
    },
    {
        "loss": 2.3847,
        "grad_norm": 3.8040921688079834,
        "learning_rate": 5.714976204312204e-05,
        "epoch": 0.88091512035174,
        "step": 11821
    },
    {
        "loss": 2.7455,
        "grad_norm": 2.8545870780944824,
        "learning_rate": 5.708619441244583e-05,
        "epoch": 0.8809896415530218,
        "step": 11822
    },
    {
        "loss": 2.4342,
        "grad_norm": 2.4911949634552,
        "learning_rate": 5.702264802979991e-05,
        "epoch": 0.8810641627543035,
        "step": 11823
    },
    {
        "loss": 2.4327,
        "grad_norm": 2.607208728790283,
        "learning_rate": 5.695912292664807e-05,
        "epoch": 0.8811386839555854,
        "step": 11824
    },
    {
        "loss": 2.6495,
        "grad_norm": 3.847865343093872,
        "learning_rate": 5.68956191344439e-05,
        "epoch": 0.8812132051568671,
        "step": 11825
    },
    {
        "loss": 2.1015,
        "grad_norm": 3.4323270320892334,
        "learning_rate": 5.683213668463015e-05,
        "epoch": 0.8812877263581489,
        "step": 11826
    },
    {
        "loss": 2.3102,
        "grad_norm": 3.247798204421997,
        "learning_rate": 5.6768675608638955e-05,
        "epoch": 0.8813622475594307,
        "step": 11827
    },
    {
        "loss": 2.411,
        "grad_norm": 2.2355527877807617,
        "learning_rate": 5.67052359378919e-05,
        "epoch": 0.8814367687607124,
        "step": 11828
    },
    {
        "loss": 2.4852,
        "grad_norm": 2.8252172470092773,
        "learning_rate": 5.6641817703800296e-05,
        "epoch": 0.8815112899619942,
        "step": 11829
    },
    {
        "loss": 1.3332,
        "grad_norm": 2.9525327682495117,
        "learning_rate": 5.657842093776432e-05,
        "epoch": 0.881585811163276,
        "step": 11830
    },
    {
        "loss": 1.9326,
        "grad_norm": 3.1103978157043457,
        "learning_rate": 5.651504567117405e-05,
        "epoch": 0.8816603323645578,
        "step": 11831
    },
    {
        "loss": 2.4274,
        "grad_norm": 3.529707908630371,
        "learning_rate": 5.645169193540853e-05,
        "epoch": 0.8817348535658395,
        "step": 11832
    },
    {
        "loss": 2.4426,
        "grad_norm": 2.209562063217163,
        "learning_rate": 5.638835976183615e-05,
        "epoch": 0.8818093747671213,
        "step": 11833
    },
    {
        "loss": 2.8951,
        "grad_norm": 2.957768440246582,
        "learning_rate": 5.6325049181815135e-05,
        "epoch": 0.881883895968403,
        "step": 11834
    },
    {
        "loss": 2.1203,
        "grad_norm": 5.833253383636475,
        "learning_rate": 5.626176022669207e-05,
        "epoch": 0.8819584171696848,
        "step": 11835
    },
    {
        "loss": 2.4308,
        "grad_norm": 2.652576446533203,
        "learning_rate": 5.619849292780379e-05,
        "epoch": 0.8820329383709665,
        "step": 11836
    },
    {
        "loss": 1.5374,
        "grad_norm": 3.614926338195801,
        "learning_rate": 5.613524731647589e-05,
        "epoch": 0.8821074595722483,
        "step": 11837
    },
    {
        "loss": 2.4708,
        "grad_norm": 2.88181734085083,
        "learning_rate": 5.607202342402322e-05,
        "epoch": 0.88218198077353,
        "step": 11838
    },
    {
        "loss": 2.644,
        "grad_norm": 3.8000926971435547,
        "learning_rate": 5.6008821281750245e-05,
        "epoch": 0.8822565019748119,
        "step": 11839
    },
    {
        "loss": 1.4326,
        "grad_norm": 3.0145697593688965,
        "learning_rate": 5.594564092095016e-05,
        "epoch": 0.8823310231760936,
        "step": 11840
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.988553762435913,
        "learning_rate": 5.588248237290592e-05,
        "epoch": 0.8824055443773754,
        "step": 11841
    },
    {
        "loss": 2.6504,
        "grad_norm": 2.150582790374756,
        "learning_rate": 5.581934566888918e-05,
        "epoch": 0.8824800655786571,
        "step": 11842
    },
    {
        "loss": 2.727,
        "grad_norm": 3.0548152923583984,
        "learning_rate": 5.575623084016105e-05,
        "epoch": 0.8825545867799389,
        "step": 11843
    },
    {
        "loss": 2.7145,
        "grad_norm": 2.874819755554199,
        "learning_rate": 5.5693137917971616e-05,
        "epoch": 0.8826291079812206,
        "step": 11844
    },
    {
        "loss": 2.2454,
        "grad_norm": 2.4165420532226562,
        "learning_rate": 5.563006693356061e-05,
        "epoch": 0.8827036291825024,
        "step": 11845
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.997260808944702,
        "learning_rate": 5.556701791815606e-05,
        "epoch": 0.8827781503837842,
        "step": 11846
    },
    {
        "loss": 2.5172,
        "grad_norm": 5.316568374633789,
        "learning_rate": 5.550399090297598e-05,
        "epoch": 0.882852671585066,
        "step": 11847
    },
    {
        "loss": 2.4228,
        "grad_norm": 2.1265487670898438,
        "learning_rate": 5.544098591922682e-05,
        "epoch": 0.8829271927863477,
        "step": 11848
    },
    {
        "loss": 2.4455,
        "grad_norm": 2.255452871322632,
        "learning_rate": 5.537800299810474e-05,
        "epoch": 0.8830017139876295,
        "step": 11849
    },
    {
        "loss": 2.358,
        "grad_norm": 2.571166753768921,
        "learning_rate": 5.531504217079442e-05,
        "epoch": 0.8830762351889112,
        "step": 11850
    },
    {
        "loss": 2.2646,
        "grad_norm": 4.067320346832275,
        "learning_rate": 5.525210346846975e-05,
        "epoch": 0.883150756390193,
        "step": 11851
    },
    {
        "loss": 2.2397,
        "grad_norm": 4.275691032409668,
        "learning_rate": 5.518918692229399e-05,
        "epoch": 0.8832252775914747,
        "step": 11852
    },
    {
        "loss": 2.7271,
        "grad_norm": 2.3495097160339355,
        "learning_rate": 5.5126292563418946e-05,
        "epoch": 0.8832997987927566,
        "step": 11853
    },
    {
        "loss": 2.8077,
        "grad_norm": 2.1485817432403564,
        "learning_rate": 5.5063420422986014e-05,
        "epoch": 0.8833743199940383,
        "step": 11854
    },
    {
        "loss": 1.3714,
        "grad_norm": 4.09138822555542,
        "learning_rate": 5.500057053212479e-05,
        "epoch": 0.8834488411953201,
        "step": 11855
    },
    {
        "loss": 2.5458,
        "grad_norm": 2.7533717155456543,
        "learning_rate": 5.4937742921954694e-05,
        "epoch": 0.8835233623966018,
        "step": 11856
    },
    {
        "loss": 2.652,
        "grad_norm": 2.608109712600708,
        "learning_rate": 5.4874937623583555e-05,
        "epoch": 0.8835978835978836,
        "step": 11857
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.719742774963379,
        "learning_rate": 5.4812154668108226e-05,
        "epoch": 0.8836724047991653,
        "step": 11858
    },
    {
        "loss": 2.0464,
        "grad_norm": 3.501149892807007,
        "learning_rate": 5.474939408661488e-05,
        "epoch": 0.8837469260004471,
        "step": 11859
    },
    {
        "loss": 1.739,
        "grad_norm": 2.509544610977173,
        "learning_rate": 5.4686655910178186e-05,
        "epoch": 0.8838214472017288,
        "step": 11860
    },
    {
        "loss": 2.8058,
        "grad_norm": 2.7390291690826416,
        "learning_rate": 5.4623940169861765e-05,
        "epoch": 0.8838959684030107,
        "step": 11861
    },
    {
        "loss": 2.9583,
        "grad_norm": 3.2362654209136963,
        "learning_rate": 5.4561246896718465e-05,
        "epoch": 0.8839704896042925,
        "step": 11862
    },
    {
        "loss": 2.3632,
        "grad_norm": 2.6703763008117676,
        "learning_rate": 5.44985761217897e-05,
        "epoch": 0.8840450108055742,
        "step": 11863
    },
    {
        "loss": 2.8287,
        "grad_norm": 2.943693161010742,
        "learning_rate": 5.443592787610562e-05,
        "epoch": 0.884119532006856,
        "step": 11864
    },
    {
        "loss": 2.2125,
        "grad_norm": 3.965144395828247,
        "learning_rate": 5.437330219068583e-05,
        "epoch": 0.8841940532081377,
        "step": 11865
    },
    {
        "loss": 2.1932,
        "grad_norm": 2.4300169944763184,
        "learning_rate": 5.4310699096537875e-05,
        "epoch": 0.8842685744094195,
        "step": 11866
    },
    {
        "loss": 2.5613,
        "grad_norm": 3.482639789581299,
        "learning_rate": 5.424811862465895e-05,
        "epoch": 0.8843430956107012,
        "step": 11867
    },
    {
        "loss": 1.9286,
        "grad_norm": 2.487802505493164,
        "learning_rate": 5.4185560806034475e-05,
        "epoch": 0.884417616811983,
        "step": 11868
    },
    {
        "loss": 2.9276,
        "grad_norm": 3.130460500717163,
        "learning_rate": 5.412302567163908e-05,
        "epoch": 0.8844921380132648,
        "step": 11869
    },
    {
        "loss": 2.056,
        "grad_norm": 2.360285758972168,
        "learning_rate": 5.406051325243586e-05,
        "epoch": 0.8845666592145466,
        "step": 11870
    },
    {
        "loss": 2.6099,
        "grad_norm": 3.927781105041504,
        "learning_rate": 5.3998023579376665e-05,
        "epoch": 0.8846411804158283,
        "step": 11871
    },
    {
        "loss": 2.9383,
        "grad_norm": 3.0610334873199463,
        "learning_rate": 5.393555668340241e-05,
        "epoch": 0.8847157016171101,
        "step": 11872
    },
    {
        "loss": 2.3005,
        "grad_norm": 2.301074981689453,
        "learning_rate": 5.387311259544221e-05,
        "epoch": 0.8847902228183918,
        "step": 11873
    },
    {
        "loss": 2.0639,
        "grad_norm": 2.480497121810913,
        "learning_rate": 5.3810691346414566e-05,
        "epoch": 0.8848647440196736,
        "step": 11874
    },
    {
        "loss": 2.2036,
        "grad_norm": 2.909637689590454,
        "learning_rate": 5.374829296722581e-05,
        "epoch": 0.8849392652209553,
        "step": 11875
    },
    {
        "loss": 1.8596,
        "grad_norm": 1.554478406906128,
        "learning_rate": 5.3685917488771784e-05,
        "epoch": 0.8850137864222372,
        "step": 11876
    },
    {
        "loss": 2.7856,
        "grad_norm": 2.81721830368042,
        "learning_rate": 5.36235649419365e-05,
        "epoch": 0.8850883076235189,
        "step": 11877
    },
    {
        "loss": 2.2856,
        "grad_norm": 4.690958499908447,
        "learning_rate": 5.356123535759279e-05,
        "epoch": 0.8851628288248007,
        "step": 11878
    },
    {
        "loss": 2.1769,
        "grad_norm": 3.159414052963257,
        "learning_rate": 5.349892876660188e-05,
        "epoch": 0.8852373500260824,
        "step": 11879
    },
    {
        "loss": 2.7934,
        "grad_norm": 2.5446531772613525,
        "learning_rate": 5.3436645199814075e-05,
        "epoch": 0.8853118712273642,
        "step": 11880
    },
    {
        "loss": 2.6621,
        "grad_norm": 2.5794155597686768,
        "learning_rate": 5.337438468806777e-05,
        "epoch": 0.8853863924286459,
        "step": 11881
    },
    {
        "loss": 2.5172,
        "grad_norm": 2.5724270343780518,
        "learning_rate": 5.331214726219041e-05,
        "epoch": 0.8854609136299277,
        "step": 11882
    },
    {
        "loss": 2.205,
        "grad_norm": 4.289906024932861,
        "learning_rate": 5.324993295299769e-05,
        "epoch": 0.8855354348312094,
        "step": 11883
    },
    {
        "loss": 1.7446,
        "grad_norm": 2.6350507736206055,
        "learning_rate": 5.318774179129381e-05,
        "epoch": 0.8856099560324913,
        "step": 11884
    },
    {
        "loss": 2.6152,
        "grad_norm": 4.3367791175842285,
        "learning_rate": 5.312557380787202e-05,
        "epoch": 0.885684477233773,
        "step": 11885
    },
    {
        "loss": 2.4916,
        "grad_norm": 2.758815050125122,
        "learning_rate": 5.3063429033513275e-05,
        "epoch": 0.8857589984350548,
        "step": 11886
    },
    {
        "loss": 2.7703,
        "grad_norm": 2.270296812057495,
        "learning_rate": 5.3001307498987776e-05,
        "epoch": 0.8858335196363365,
        "step": 11887
    },
    {
        "loss": 3.0012,
        "grad_norm": 3.325021743774414,
        "learning_rate": 5.293920923505385e-05,
        "epoch": 0.8859080408376183,
        "step": 11888
    },
    {
        "loss": 2.945,
        "grad_norm": 3.063880443572998,
        "learning_rate": 5.287713427245826e-05,
        "epoch": 0.8859825620389,
        "step": 11889
    },
    {
        "loss": 2.7657,
        "grad_norm": 2.295619487762451,
        "learning_rate": 5.28150826419366e-05,
        "epoch": 0.8860570832401818,
        "step": 11890
    },
    {
        "loss": 2.7729,
        "grad_norm": 2.804577112197876,
        "learning_rate": 5.2753054374212396e-05,
        "epoch": 0.8861316044414635,
        "step": 11891
    },
    {
        "loss": 1.8726,
        "grad_norm": 3.600346803665161,
        "learning_rate": 5.269104949999815e-05,
        "epoch": 0.8862061256427454,
        "step": 11892
    },
    {
        "loss": 1.4436,
        "grad_norm": 4.049871444702148,
        "learning_rate": 5.2629068049994344e-05,
        "epoch": 0.8862806468440271,
        "step": 11893
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.2012383937835693,
        "learning_rate": 5.25671100548901e-05,
        "epoch": 0.8863551680453089,
        "step": 11894
    },
    {
        "loss": 2.1814,
        "grad_norm": 2.514723777770996,
        "learning_rate": 5.250517554536265e-05,
        "epoch": 0.8864296892465906,
        "step": 11895
    },
    {
        "loss": 2.4741,
        "grad_norm": 2.413195848464966,
        "learning_rate": 5.244326455207805e-05,
        "epoch": 0.8865042104478724,
        "step": 11896
    },
    {
        "loss": 2.1328,
        "grad_norm": 3.262385845184326,
        "learning_rate": 5.2381377105690355e-05,
        "epoch": 0.8865787316491542,
        "step": 11897
    },
    {
        "loss": 1.632,
        "grad_norm": 3.262603998184204,
        "learning_rate": 5.2319513236842076e-05,
        "epoch": 0.886653252850436,
        "step": 11898
    },
    {
        "loss": 2.0106,
        "grad_norm": 3.5260043144226074,
        "learning_rate": 5.225767297616391e-05,
        "epoch": 0.8867277740517178,
        "step": 11899
    },
    {
        "loss": 2.1584,
        "grad_norm": 2.482396125793457,
        "learning_rate": 5.219585635427524e-05,
        "epoch": 0.8868022952529995,
        "step": 11900
    },
    {
        "loss": 2.6057,
        "grad_norm": 2.378202438354492,
        "learning_rate": 5.213406340178324e-05,
        "epoch": 0.8868768164542813,
        "step": 11901
    },
    {
        "loss": 2.5587,
        "grad_norm": 2.879809856414795,
        "learning_rate": 5.207229414928393e-05,
        "epoch": 0.886951337655563,
        "step": 11902
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.6831252574920654,
        "learning_rate": 5.2010548627361076e-05,
        "epoch": 0.8870258588568448,
        "step": 11903
    },
    {
        "loss": 2.101,
        "grad_norm": 3.126462459564209,
        "learning_rate": 5.194882686658683e-05,
        "epoch": 0.8871003800581265,
        "step": 11904
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.617473602294922,
        "learning_rate": 5.188712889752201e-05,
        "epoch": 0.8871749012594083,
        "step": 11905
    },
    {
        "loss": 2.8886,
        "grad_norm": 3.259920597076416,
        "learning_rate": 5.182545475071484e-05,
        "epoch": 0.88724942246069,
        "step": 11906
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.2055439949035645,
        "learning_rate": 5.1763804456702545e-05,
        "epoch": 0.8873239436619719,
        "step": 11907
    },
    {
        "loss": 2.3287,
        "grad_norm": 2.4666287899017334,
        "learning_rate": 5.170217804601015e-05,
        "epoch": 0.8873984648632536,
        "step": 11908
    },
    {
        "loss": 1.5192,
        "grad_norm": 3.0002753734588623,
        "learning_rate": 5.164057554915069e-05,
        "epoch": 0.8874729860645354,
        "step": 11909
    },
    {
        "loss": 2.1132,
        "grad_norm": 2.641584634780884,
        "learning_rate": 5.1578996996625875e-05,
        "epoch": 0.8875475072658171,
        "step": 11910
    },
    {
        "loss": 1.7258,
        "grad_norm": 3.5592589378356934,
        "learning_rate": 5.151744241892511e-05,
        "epoch": 0.8876220284670989,
        "step": 11911
    },
    {
        "loss": 1.9864,
        "grad_norm": 2.647524118423462,
        "learning_rate": 5.145591184652601e-05,
        "epoch": 0.8876965496683806,
        "step": 11912
    },
    {
        "loss": 2.4926,
        "grad_norm": 3.0694987773895264,
        "learning_rate": 5.1394405309894563e-05,
        "epoch": 0.8877710708696624,
        "step": 11913
    },
    {
        "loss": 2.0827,
        "grad_norm": 4.281681060791016,
        "learning_rate": 5.133292283948459e-05,
        "epoch": 0.8878455920709442,
        "step": 11914
    },
    {
        "loss": 2.1966,
        "grad_norm": 2.3984804153442383,
        "learning_rate": 5.127146446573794e-05,
        "epoch": 0.887920113272226,
        "step": 11915
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.9420058727264404,
        "learning_rate": 5.121003021908506e-05,
        "epoch": 0.8879946344735077,
        "step": 11916
    },
    {
        "loss": 1.9457,
        "grad_norm": 3.219426155090332,
        "learning_rate": 5.11486201299435e-05,
        "epoch": 0.8880691556747895,
        "step": 11917
    },
    {
        "loss": 2.2012,
        "grad_norm": 2.766282081604004,
        "learning_rate": 5.108723422871983e-05,
        "epoch": 0.8881436768760712,
        "step": 11918
    },
    {
        "loss": 1.5479,
        "grad_norm": 3.8318119049072266,
        "learning_rate": 5.102587254580795e-05,
        "epoch": 0.888218198077353,
        "step": 11919
    },
    {
        "loss": 1.963,
        "grad_norm": 3.4880802631378174,
        "learning_rate": 5.096453511159028e-05,
        "epoch": 0.8882927192786347,
        "step": 11920
    },
    {
        "loss": 2.6222,
        "grad_norm": 2.3039894104003906,
        "learning_rate": 5.090322195643688e-05,
        "epoch": 0.8883672404799166,
        "step": 11921
    },
    {
        "loss": 2.7999,
        "grad_norm": 2.7736127376556396,
        "learning_rate": 5.0841933110705806e-05,
        "epoch": 0.8884417616811983,
        "step": 11922
    },
    {
        "loss": 2.5187,
        "grad_norm": 2.3585329055786133,
        "learning_rate": 5.078066860474337e-05,
        "epoch": 0.8885162828824801,
        "step": 11923
    },
    {
        "loss": 2.9322,
        "grad_norm": 2.5542781352996826,
        "learning_rate": 5.071942846888342e-05,
        "epoch": 0.8885908040837618,
        "step": 11924
    },
    {
        "loss": 2.3907,
        "grad_norm": 3.2681052684783936,
        "learning_rate": 5.065821273344833e-05,
        "epoch": 0.8886653252850436,
        "step": 11925
    },
    {
        "loss": 1.6217,
        "grad_norm": 3.677929401397705,
        "learning_rate": 5.059702142874748e-05,
        "epoch": 0.8887398464863253,
        "step": 11926
    },
    {
        "loss": 2.5606,
        "grad_norm": 2.904233455657959,
        "learning_rate": 5.0535854585079076e-05,
        "epoch": 0.8888143676876071,
        "step": 11927
    },
    {
        "loss": 2.559,
        "grad_norm": 4.100722312927246,
        "learning_rate": 5.0474712232728705e-05,
        "epoch": 0.8888888888888888,
        "step": 11928
    },
    {
        "loss": 2.7823,
        "grad_norm": 2.4766855239868164,
        "learning_rate": 5.0413594401969844e-05,
        "epoch": 0.8889634100901707,
        "step": 11929
    },
    {
        "loss": 2.0509,
        "grad_norm": 3.309730291366577,
        "learning_rate": 5.0352501123064155e-05,
        "epoch": 0.8890379312914524,
        "step": 11930
    },
    {
        "loss": 1.2166,
        "grad_norm": 3.681257963180542,
        "learning_rate": 5.029143242626083e-05,
        "epoch": 0.8891124524927342,
        "step": 11931
    },
    {
        "loss": 1.2734,
        "grad_norm": 3.1092848777770996,
        "learning_rate": 5.023038834179681e-05,
        "epoch": 0.889186973694016,
        "step": 11932
    },
    {
        "loss": 1.2053,
        "grad_norm": 1.17239511013031,
        "learning_rate": 5.016936889989728e-05,
        "epoch": 0.8892614948952977,
        "step": 11933
    },
    {
        "loss": 1.7581,
        "grad_norm": 3.228502035140991,
        "learning_rate": 5.010837413077485e-05,
        "epoch": 0.8893360160965795,
        "step": 11934
    },
    {
        "loss": 2.4861,
        "grad_norm": 3.4299490451812744,
        "learning_rate": 5.004740406462991e-05,
        "epoch": 0.8894105372978612,
        "step": 11935
    },
    {
        "loss": 2.8705,
        "grad_norm": 3.426203489303589,
        "learning_rate": 4.998645873165107e-05,
        "epoch": 0.889485058499143,
        "step": 11936
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.4230825901031494,
        "learning_rate": 4.992553816201393e-05,
        "epoch": 0.8895595797004248,
        "step": 11937
    },
    {
        "loss": 2.5311,
        "grad_norm": 3.4091291427612305,
        "learning_rate": 4.9864642385882575e-05,
        "epoch": 0.8896341009017066,
        "step": 11938
    },
    {
        "loss": 2.6855,
        "grad_norm": 2.116030693054199,
        "learning_rate": 4.980377143340843e-05,
        "epoch": 0.8897086221029883,
        "step": 11939
    },
    {
        "loss": 2.7283,
        "grad_norm": 2.876038074493408,
        "learning_rate": 4.974292533473052e-05,
        "epoch": 0.8897831433042701,
        "step": 11940
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.6986358165740967,
        "learning_rate": 4.9682104119975984e-05,
        "epoch": 0.8898576645055518,
        "step": 11941
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.6732099056243896,
        "learning_rate": 4.962130781925919e-05,
        "epoch": 0.8899321857068336,
        "step": 11942
    },
    {
        "loss": 1.9601,
        "grad_norm": 3.107008695602417,
        "learning_rate": 4.9560536462682585e-05,
        "epoch": 0.8900067069081153,
        "step": 11943
    },
    {
        "loss": 2.0458,
        "grad_norm": 2.563382387161255,
        "learning_rate": 4.949979008033596e-05,
        "epoch": 0.8900812281093972,
        "step": 11944
    },
    {
        "loss": 2.5228,
        "grad_norm": 2.5713298320770264,
        "learning_rate": 4.9439068702296834e-05,
        "epoch": 0.8901557493106789,
        "step": 11945
    },
    {
        "loss": 2.475,
        "grad_norm": 2.4544901847839355,
        "learning_rate": 4.937837235863022e-05,
        "epoch": 0.8902302705119607,
        "step": 11946
    },
    {
        "loss": 2.1143,
        "grad_norm": 3.7302358150482178,
        "learning_rate": 4.931770107938916e-05,
        "epoch": 0.8903047917132424,
        "step": 11947
    },
    {
        "loss": 2.1626,
        "grad_norm": 2.9153690338134766,
        "learning_rate": 4.925705489461378e-05,
        "epoch": 0.8903793129145242,
        "step": 11948
    },
    {
        "loss": 2.5379,
        "grad_norm": 4.1464738845825195,
        "learning_rate": 4.919643383433203e-05,
        "epoch": 0.8904538341158059,
        "step": 11949
    },
    {
        "loss": 2.7513,
        "grad_norm": 2.1981308460235596,
        "learning_rate": 4.913583792855928e-05,
        "epoch": 0.8905283553170877,
        "step": 11950
    },
    {
        "loss": 2.5769,
        "grad_norm": 3.2868664264678955,
        "learning_rate": 4.907526720729876e-05,
        "epoch": 0.8906028765183694,
        "step": 11951
    },
    {
        "loss": 2.7886,
        "grad_norm": 2.8396787643432617,
        "learning_rate": 4.90147217005408e-05,
        "epoch": 0.8906773977196513,
        "step": 11952
    },
    {
        "loss": 2.2195,
        "grad_norm": 3.821916103363037,
        "learning_rate": 4.895420143826369e-05,
        "epoch": 0.890751918920933,
        "step": 11953
    },
    {
        "loss": 2.5597,
        "grad_norm": 2.1238486766815186,
        "learning_rate": 4.8893706450432916e-05,
        "epoch": 0.8908264401222148,
        "step": 11954
    },
    {
        "loss": 1.7786,
        "grad_norm": 3.0668509006500244,
        "learning_rate": 4.883323676700135e-05,
        "epoch": 0.8909009613234965,
        "step": 11955
    },
    {
        "loss": 1.9977,
        "grad_norm": 4.963546276092529,
        "learning_rate": 4.877279241790986e-05,
        "epoch": 0.8909754825247783,
        "step": 11956
    },
    {
        "loss": 2.5669,
        "grad_norm": 2.8170948028564453,
        "learning_rate": 4.8712373433086e-05,
        "epoch": 0.89105000372606,
        "step": 11957
    },
    {
        "loss": 2.4924,
        "grad_norm": 1.6900876760482788,
        "learning_rate": 4.8651979842445515e-05,
        "epoch": 0.8911245249273418,
        "step": 11958
    },
    {
        "loss": 2.0288,
        "grad_norm": 3.8104536533355713,
        "learning_rate": 4.859161167589113e-05,
        "epoch": 0.8911990461286236,
        "step": 11959
    },
    {
        "loss": 2.1595,
        "grad_norm": 3.861919403076172,
        "learning_rate": 4.853126896331299e-05,
        "epoch": 0.8912735673299054,
        "step": 11960
    },
    {
        "loss": 2.2749,
        "grad_norm": 2.9863810539245605,
        "learning_rate": 4.847095173458899e-05,
        "epoch": 0.8913480885311871,
        "step": 11961
    },
    {
        "loss": 2.4761,
        "grad_norm": 2.673271417617798,
        "learning_rate": 4.841066001958391e-05,
        "epoch": 0.8914226097324689,
        "step": 11962
    },
    {
        "loss": 2.0894,
        "grad_norm": 2.3106353282928467,
        "learning_rate": 4.8350393848150446e-05,
        "epoch": 0.8914971309337506,
        "step": 11963
    },
    {
        "loss": 2.5002,
        "grad_norm": 2.6666316986083984,
        "learning_rate": 4.8290153250128144e-05,
        "epoch": 0.8915716521350324,
        "step": 11964
    },
    {
        "loss": 2.9948,
        "grad_norm": 2.6093530654907227,
        "learning_rate": 4.8229938255344155e-05,
        "epoch": 0.8916461733363141,
        "step": 11965
    },
    {
        "loss": 2.4039,
        "grad_norm": 2.982555866241455,
        "learning_rate": 4.816974889361275e-05,
        "epoch": 0.891720694537596,
        "step": 11966
    },
    {
        "loss": 1.8586,
        "grad_norm": 2.710071325302124,
        "learning_rate": 4.810958519473591e-05,
        "epoch": 0.8917952157388777,
        "step": 11967
    },
    {
        "loss": 2.653,
        "grad_norm": 2.630751609802246,
        "learning_rate": 4.804944718850252e-05,
        "epoch": 0.8918697369401595,
        "step": 11968
    },
    {
        "loss": 1.5537,
        "grad_norm": 4.3155670166015625,
        "learning_rate": 4.798933490468892e-05,
        "epoch": 0.8919442581414413,
        "step": 11969
    },
    {
        "loss": 2.2682,
        "grad_norm": 2.402214527130127,
        "learning_rate": 4.792924837305848e-05,
        "epoch": 0.892018779342723,
        "step": 11970
    },
    {
        "loss": 1.7448,
        "grad_norm": 2.6176772117614746,
        "learning_rate": 4.786918762336228e-05,
        "epoch": 0.8920933005440048,
        "step": 11971
    },
    {
        "loss": 2.3834,
        "grad_norm": 2.5186922550201416,
        "learning_rate": 4.7809152685338245e-05,
        "epoch": 0.8921678217452865,
        "step": 11972
    },
    {
        "loss": 2.2119,
        "grad_norm": 2.956294536590576,
        "learning_rate": 4.774914358871152e-05,
        "epoch": 0.8922423429465683,
        "step": 11973
    },
    {
        "loss": 2.5662,
        "grad_norm": 1.8982430696487427,
        "learning_rate": 4.768916036319481e-05,
        "epoch": 0.89231686414785,
        "step": 11974
    },
    {
        "loss": 1.9937,
        "grad_norm": 4.738988399505615,
        "learning_rate": 4.762920303848753e-05,
        "epoch": 0.8923913853491319,
        "step": 11975
    },
    {
        "loss": 2.4787,
        "grad_norm": 2.84281325340271,
        "learning_rate": 4.756927164427692e-05,
        "epoch": 0.8924659065504136,
        "step": 11976
    },
    {
        "loss": 2.5117,
        "grad_norm": 3.1151771545410156,
        "learning_rate": 4.750936621023643e-05,
        "epoch": 0.8925404277516954,
        "step": 11977
    },
    {
        "loss": 2.7909,
        "grad_norm": 2.0333878993988037,
        "learning_rate": 4.7449486766027584e-05,
        "epoch": 0.8926149489529771,
        "step": 11978
    },
    {
        "loss": 2.5674,
        "grad_norm": 1.9907137155532837,
        "learning_rate": 4.738963334129853e-05,
        "epoch": 0.8926894701542589,
        "step": 11979
    },
    {
        "loss": 2.7633,
        "grad_norm": 2.6769533157348633,
        "learning_rate": 4.732980596568457e-05,
        "epoch": 0.8927639913555406,
        "step": 11980
    },
    {
        "loss": 2.0945,
        "grad_norm": 4.144668102264404,
        "learning_rate": 4.7270004668808397e-05,
        "epoch": 0.8928385125568225,
        "step": 11981
    },
    {
        "loss": 2.8924,
        "grad_norm": 2.3976454734802246,
        "learning_rate": 4.72102294802795e-05,
        "epoch": 0.8929130337581042,
        "step": 11982
    },
    {
        "loss": 2.8307,
        "grad_norm": 1.3908990621566772,
        "learning_rate": 4.71504804296944e-05,
        "epoch": 0.892987554959386,
        "step": 11983
    },
    {
        "loss": 1.5951,
        "grad_norm": 3.7610206604003906,
        "learning_rate": 4.7090757546637045e-05,
        "epoch": 0.8930620761606677,
        "step": 11984
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.132450580596924,
        "learning_rate": 4.7031060860678114e-05,
        "epoch": 0.8931365973619495,
        "step": 11985
    },
    {
        "loss": 1.8532,
        "grad_norm": 1.486793875694275,
        "learning_rate": 4.69713904013752e-05,
        "epoch": 0.8932111185632312,
        "step": 11986
    },
    {
        "loss": 2.4962,
        "grad_norm": 1.804870843887329,
        "learning_rate": 4.691174619827348e-05,
        "epoch": 0.893285639764513,
        "step": 11987
    },
    {
        "loss": 1.947,
        "grad_norm": 3.5896904468536377,
        "learning_rate": 4.6852128280904296e-05,
        "epoch": 0.8933601609657947,
        "step": 11988
    },
    {
        "loss": 2.6878,
        "grad_norm": 2.6129775047302246,
        "learning_rate": 4.679253667878678e-05,
        "epoch": 0.8934346821670766,
        "step": 11989
    },
    {
        "loss": 1.8102,
        "grad_norm": 4.098527908325195,
        "learning_rate": 4.673297142142644e-05,
        "epoch": 0.8935092033683583,
        "step": 11990
    },
    {
        "loss": 2.2589,
        "grad_norm": 3.3934850692749023,
        "learning_rate": 4.667343253831623e-05,
        "epoch": 0.8935837245696401,
        "step": 11991
    },
    {
        "loss": 2.0251,
        "grad_norm": 3.297440528869629,
        "learning_rate": 4.6613920058935725e-05,
        "epoch": 0.8936582457709218,
        "step": 11992
    },
    {
        "loss": 2.7275,
        "grad_norm": 3.1223394870758057,
        "learning_rate": 4.6554434012751304e-05,
        "epoch": 0.8937327669722036,
        "step": 11993
    },
    {
        "loss": 2.1945,
        "grad_norm": 2.9787302017211914,
        "learning_rate": 4.649497442921672e-05,
        "epoch": 0.8938072881734853,
        "step": 11994
    },
    {
        "loss": 2.4203,
        "grad_norm": 2.3965439796447754,
        "learning_rate": 4.643554133777215e-05,
        "epoch": 0.8938818093747671,
        "step": 11995
    },
    {
        "loss": 1.431,
        "grad_norm": 3.251458168029785,
        "learning_rate": 4.637613476784517e-05,
        "epoch": 0.8939563305760488,
        "step": 11996
    },
    {
        "loss": 2.7598,
        "grad_norm": 3.275561809539795,
        "learning_rate": 4.63167547488495e-05,
        "epoch": 0.8940308517773307,
        "step": 11997
    },
    {
        "loss": 1.8172,
        "grad_norm": 4.643234729766846,
        "learning_rate": 4.625740131018645e-05,
        "epoch": 0.8941053729786124,
        "step": 11998
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.2450079917907715,
        "learning_rate": 4.619807448124379e-05,
        "epoch": 0.8941798941798942,
        "step": 11999
    },
    {
        "loss": 2.7205,
        "grad_norm": 2.4448063373565674,
        "learning_rate": 4.613877429139598e-05,
        "epoch": 0.8942544153811759,
        "step": 12000
    },
    {
        "loss": 2.3012,
        "grad_norm": 1.940119743347168,
        "learning_rate": 4.607950077000474e-05,
        "epoch": 0.8943289365824577,
        "step": 12001
    },
    {
        "loss": 2.4804,
        "grad_norm": 3.5124804973602295,
        "learning_rate": 4.602025394641823e-05,
        "epoch": 0.8944034577837394,
        "step": 12002
    },
    {
        "loss": 2.1307,
        "grad_norm": 2.610649824142456,
        "learning_rate": 4.596103384997138e-05,
        "epoch": 0.8944779789850212,
        "step": 12003
    },
    {
        "loss": 2.7081,
        "grad_norm": 2.6862339973449707,
        "learning_rate": 4.5901840509986216e-05,
        "epoch": 0.8945525001863031,
        "step": 12004
    },
    {
        "loss": 2.3978,
        "grad_norm": 2.742579460144043,
        "learning_rate": 4.5842673955771195e-05,
        "epoch": 0.8946270213875848,
        "step": 12005
    },
    {
        "loss": 2.4658,
        "grad_norm": 2.0536937713623047,
        "learning_rate": 4.5783534216621506e-05,
        "epoch": 0.8947015425888666,
        "step": 12006
    },
    {
        "loss": 2.2204,
        "grad_norm": 2.613499879837036,
        "learning_rate": 4.572442132181953e-05,
        "epoch": 0.8947760637901483,
        "step": 12007
    },
    {
        "loss": 2.6686,
        "grad_norm": 2.3645365238189697,
        "learning_rate": 4.566533530063348e-05,
        "epoch": 0.8948505849914301,
        "step": 12008
    },
    {
        "loss": 2.984,
        "grad_norm": 3.790771007537842,
        "learning_rate": 4.5606276182319165e-05,
        "epoch": 0.8949251061927118,
        "step": 12009
    },
    {
        "loss": 1.8335,
        "grad_norm": 5.398925304412842,
        "learning_rate": 4.5547243996118546e-05,
        "epoch": 0.8949996273939936,
        "step": 12010
    },
    {
        "loss": 2.3695,
        "grad_norm": 3.325566530227661,
        "learning_rate": 4.5488238771260314e-05,
        "epoch": 0.8950741485952753,
        "step": 12011
    },
    {
        "loss": 1.9868,
        "grad_norm": 2.9421679973602295,
        "learning_rate": 4.54292605369601e-05,
        "epoch": 0.8951486697965572,
        "step": 12012
    },
    {
        "loss": 1.8478,
        "grad_norm": 5.472578048706055,
        "learning_rate": 4.537030932241972e-05,
        "epoch": 0.8952231909978389,
        "step": 12013
    },
    {
        "loss": 2.3534,
        "grad_norm": 3.3604583740234375,
        "learning_rate": 4.53113851568281e-05,
        "epoch": 0.8952977121991207,
        "step": 12014
    },
    {
        "loss": 2.2813,
        "grad_norm": 2.933523416519165,
        "learning_rate": 4.525248806936042e-05,
        "epoch": 0.8953722334004024,
        "step": 12015
    },
    {
        "loss": 2.9248,
        "grad_norm": 2.2573866844177246,
        "learning_rate": 4.519361808917851e-05,
        "epoch": 0.8954467546016842,
        "step": 12016
    },
    {
        "loss": 2.9593,
        "grad_norm": 2.6020359992980957,
        "learning_rate": 4.5134775245430736e-05,
        "epoch": 0.8955212758029659,
        "step": 12017
    },
    {
        "loss": 2.3738,
        "grad_norm": 2.998542070388794,
        "learning_rate": 4.5075959567252335e-05,
        "epoch": 0.8955957970042477,
        "step": 12018
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.7292003631591797,
        "learning_rate": 4.5017171083764776e-05,
        "epoch": 0.8956703182055294,
        "step": 12019
    },
    {
        "loss": 2.7446,
        "grad_norm": 2.4866015911102295,
        "learning_rate": 4.495840982407616e-05,
        "epoch": 0.8957448394068113,
        "step": 12020
    },
    {
        "loss": 1.9397,
        "grad_norm": 3.342707395553589,
        "learning_rate": 4.4899675817280994e-05,
        "epoch": 0.895819360608093,
        "step": 12021
    },
    {
        "loss": 2.9975,
        "grad_norm": 2.82409405708313,
        "learning_rate": 4.4840969092460675e-05,
        "epoch": 0.8958938818093748,
        "step": 12022
    },
    {
        "loss": 2.6229,
        "grad_norm": 2.9728636741638184,
        "learning_rate": 4.478228967868251e-05,
        "epoch": 0.8959684030106565,
        "step": 12023
    },
    {
        "loss": 1.7423,
        "grad_norm": 4.938229560852051,
        "learning_rate": 4.4723637605000866e-05,
        "epoch": 0.8960429242119383,
        "step": 12024
    },
    {
        "loss": 2.8364,
        "grad_norm": 2.565282106399536,
        "learning_rate": 4.4665012900456216e-05,
        "epoch": 0.89611744541322,
        "step": 12025
    },
    {
        "loss": 2.5644,
        "grad_norm": 3.058455467224121,
        "learning_rate": 4.4606415594075436e-05,
        "epoch": 0.8961919666145018,
        "step": 12026
    },
    {
        "loss": 2.8203,
        "grad_norm": 2.980076789855957,
        "learning_rate": 4.4547845714872304e-05,
        "epoch": 0.8962664878157836,
        "step": 12027
    },
    {
        "loss": 1.9545,
        "grad_norm": 3.5735557079315186,
        "learning_rate": 4.448930329184625e-05,
        "epoch": 0.8963410090170654,
        "step": 12028
    },
    {
        "loss": 2.7354,
        "grad_norm": 3.54105806350708,
        "learning_rate": 4.443078835398388e-05,
        "epoch": 0.8964155302183471,
        "step": 12029
    },
    {
        "loss": 2.1812,
        "grad_norm": 2.183053970336914,
        "learning_rate": 4.437230093025777e-05,
        "epoch": 0.8964900514196289,
        "step": 12030
    },
    {
        "loss": 1.5837,
        "grad_norm": 2.1105685234069824,
        "learning_rate": 4.4313841049626794e-05,
        "epoch": 0.8965645726209106,
        "step": 12031
    },
    {
        "loss": 2.8162,
        "grad_norm": 2.8729636669158936,
        "learning_rate": 4.4255408741036584e-05,
        "epoch": 0.8966390938221924,
        "step": 12032
    },
    {
        "loss": 1.665,
        "grad_norm": 3.1069836616516113,
        "learning_rate": 4.41970040334188e-05,
        "epoch": 0.8967136150234741,
        "step": 12033
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.7644402980804443,
        "learning_rate": 4.41386269556914e-05,
        "epoch": 0.896788136224756,
        "step": 12034
    },
    {
        "loss": 2.7793,
        "grad_norm": 2.2681620121002197,
        "learning_rate": 4.408027753675902e-05,
        "epoch": 0.8968626574260377,
        "step": 12035
    },
    {
        "loss": 2.4936,
        "grad_norm": 3.052766799926758,
        "learning_rate": 4.402195580551225e-05,
        "epoch": 0.8969371786273195,
        "step": 12036
    },
    {
        "loss": 1.8091,
        "grad_norm": 4.048259735107422,
        "learning_rate": 4.396366179082797e-05,
        "epoch": 0.8970116998286012,
        "step": 12037
    },
    {
        "loss": 2.5612,
        "grad_norm": 3.930995464324951,
        "learning_rate": 4.390539552156985e-05,
        "epoch": 0.897086221029883,
        "step": 12038
    },
    {
        "loss": 1.6772,
        "grad_norm": 3.6083149909973145,
        "learning_rate": 4.384715702658689e-05,
        "epoch": 0.8971607422311648,
        "step": 12039
    },
    {
        "loss": 2.4619,
        "grad_norm": 3.1202619075775146,
        "learning_rate": 4.378894633471525e-05,
        "epoch": 0.8972352634324465,
        "step": 12040
    },
    {
        "loss": 2.1011,
        "grad_norm": 4.006961345672607,
        "learning_rate": 4.3730763474776736e-05,
        "epoch": 0.8973097846337283,
        "step": 12041
    },
    {
        "loss": 1.774,
        "grad_norm": 2.730067729949951,
        "learning_rate": 4.367260847557979e-05,
        "epoch": 0.89738430583501,
        "step": 12042
    },
    {
        "loss": 2.4551,
        "grad_norm": 3.3318071365356445,
        "learning_rate": 4.361448136591877e-05,
        "epoch": 0.8974588270362919,
        "step": 12043
    },
    {
        "loss": 2.3836,
        "grad_norm": 3.329181671142578,
        "learning_rate": 4.3556382174574174e-05,
        "epoch": 0.8975333482375736,
        "step": 12044
    },
    {
        "loss": 2.3667,
        "grad_norm": 6.901813983917236,
        "learning_rate": 4.349831093031309e-05,
        "epoch": 0.8976078694388554,
        "step": 12045
    },
    {
        "loss": 2.1514,
        "grad_norm": 2.358798027038574,
        "learning_rate": 4.34402676618882e-05,
        "epoch": 0.8976823906401371,
        "step": 12046
    },
    {
        "loss": 2.328,
        "grad_norm": 4.040657043457031,
        "learning_rate": 4.338225239803898e-05,
        "epoch": 0.8977569118414189,
        "step": 12047
    },
    {
        "loss": 2.4796,
        "grad_norm": 2.634819746017456,
        "learning_rate": 4.332426516749026e-05,
        "epoch": 0.8978314330427006,
        "step": 12048
    },
    {
        "loss": 2.2989,
        "grad_norm": 3.221790075302124,
        "learning_rate": 4.326630599895374e-05,
        "epoch": 0.8979059542439825,
        "step": 12049
    },
    {
        "loss": 2.6022,
        "grad_norm": 3.8381993770599365,
        "learning_rate": 4.3208374921126795e-05,
        "epoch": 0.8979804754452642,
        "step": 12050
    },
    {
        "loss": 2.0521,
        "grad_norm": 3.7709076404571533,
        "learning_rate": 4.315047196269287e-05,
        "epoch": 0.898054996646546,
        "step": 12051
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.8274097442626953,
        "learning_rate": 4.309259715232187e-05,
        "epoch": 0.8981295178478277,
        "step": 12052
    },
    {
        "loss": 1.7032,
        "grad_norm": 4.054176330566406,
        "learning_rate": 4.303475051866941e-05,
        "epoch": 0.8982040390491095,
        "step": 12053
    },
    {
        "loss": 2.7886,
        "grad_norm": 2.149362087249756,
        "learning_rate": 4.297693209037709e-05,
        "epoch": 0.8982785602503912,
        "step": 12054
    },
    {
        "loss": 2.4529,
        "grad_norm": 4.194698333740234,
        "learning_rate": 4.2919141896072935e-05,
        "epoch": 0.898353081451673,
        "step": 12055
    },
    {
        "loss": 1.9632,
        "grad_norm": 2.881608009338379,
        "learning_rate": 4.286137996437067e-05,
        "epoch": 0.8984276026529547,
        "step": 12056
    },
    {
        "loss": 2.0514,
        "grad_norm": 3.166130781173706,
        "learning_rate": 4.2803646323870004e-05,
        "epoch": 0.8985021238542366,
        "step": 12057
    },
    {
        "loss": 3.3854,
        "grad_norm": 3.8356003761291504,
        "learning_rate": 4.2745941003157094e-05,
        "epoch": 0.8985766450555183,
        "step": 12058
    },
    {
        "loss": 1.8375,
        "grad_norm": 3.706458568572998,
        "learning_rate": 4.268826403080328e-05,
        "epoch": 0.8986511662568001,
        "step": 12059
    },
    {
        "loss": 2.2276,
        "grad_norm": 3.3843564987182617,
        "learning_rate": 4.2630615435366694e-05,
        "epoch": 0.8987256874580818,
        "step": 12060
    },
    {
        "loss": 2.6229,
        "grad_norm": 3.8184194564819336,
        "learning_rate": 4.2572995245390744e-05,
        "epoch": 0.8988002086593636,
        "step": 12061
    },
    {
        "loss": 2.3363,
        "grad_norm": 3.519531488418579,
        "learning_rate": 4.2515403489405335e-05,
        "epoch": 0.8988747298606453,
        "step": 12062
    },
    {
        "loss": 2.7256,
        "grad_norm": 1.8740893602371216,
        "learning_rate": 4.245784019592591e-05,
        "epoch": 0.8989492510619271,
        "step": 12063
    },
    {
        "loss": 2.7458,
        "grad_norm": 2.5093188285827637,
        "learning_rate": 4.240030539345385e-05,
        "epoch": 0.8990237722632088,
        "step": 12064
    },
    {
        "loss": 2.1853,
        "grad_norm": 3.894437551498413,
        "learning_rate": 4.2342799110476726e-05,
        "epoch": 0.8990982934644907,
        "step": 12065
    },
    {
        "loss": 1.615,
        "grad_norm": 4.361886501312256,
        "learning_rate": 4.228532137546768e-05,
        "epoch": 0.8991728146657724,
        "step": 12066
    },
    {
        "loss": 2.3765,
        "grad_norm": 3.58160662651062,
        "learning_rate": 4.222787221688584e-05,
        "epoch": 0.8992473358670542,
        "step": 12067
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.385403871536255,
        "learning_rate": 4.2170451663176e-05,
        "epoch": 0.8993218570683359,
        "step": 12068
    },
    {
        "loss": 2.401,
        "grad_norm": 3.8483126163482666,
        "learning_rate": 4.211305974276921e-05,
        "epoch": 0.8993963782696177,
        "step": 12069
    },
    {
        "loss": 2.2689,
        "grad_norm": 2.822164535522461,
        "learning_rate": 4.205569648408197e-05,
        "epoch": 0.8994708994708994,
        "step": 12070
    },
    {
        "loss": 2.315,
        "grad_norm": 2.793867588043213,
        "learning_rate": 4.199836191551673e-05,
        "epoch": 0.8995454206721812,
        "step": 12071
    },
    {
        "loss": 2.426,
        "grad_norm": 2.3348801136016846,
        "learning_rate": 4.194105606546157e-05,
        "epoch": 0.899619941873463,
        "step": 12072
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.9088640213012695,
        "learning_rate": 4.188377896229073e-05,
        "epoch": 0.8996944630747448,
        "step": 12073
    },
    {
        "loss": 2.7817,
        "grad_norm": 3.9425294399261475,
        "learning_rate": 4.1826530634363756e-05,
        "epoch": 0.8997689842760266,
        "step": 12074
    },
    {
        "loss": 1.7723,
        "grad_norm": 3.446305274963379,
        "learning_rate": 4.17693111100264e-05,
        "epoch": 0.8998435054773083,
        "step": 12075
    },
    {
        "loss": 2.7037,
        "grad_norm": 2.9889025688171387,
        "learning_rate": 4.171212041760983e-05,
        "epoch": 0.8999180266785901,
        "step": 12076
    },
    {
        "loss": 1.8167,
        "grad_norm": 3.9626717567443848,
        "learning_rate": 4.165495858543087e-05,
        "epoch": 0.8999925478798718,
        "step": 12077
    },
    {
        "loss": 2.2415,
        "grad_norm": 3.4031729698181152,
        "learning_rate": 4.1597825641792566e-05,
        "epoch": 0.9000670690811536,
        "step": 12078
    },
    {
        "loss": 1.4581,
        "grad_norm": 1.358215570449829,
        "learning_rate": 4.154072161498287e-05,
        "epoch": 0.9001415902824353,
        "step": 12079
    },
    {
        "loss": 1.9405,
        "grad_norm": 3.318124532699585,
        "learning_rate": 4.148364653327618e-05,
        "epoch": 0.9002161114837172,
        "step": 12080
    },
    {
        "loss": 1.4826,
        "grad_norm": 3.941690444946289,
        "learning_rate": 4.142660042493214e-05,
        "epoch": 0.9002906326849989,
        "step": 12081
    },
    {
        "loss": 2.3518,
        "grad_norm": 2.4794554710388184,
        "learning_rate": 4.1369583318196036e-05,
        "epoch": 0.9003651538862807,
        "step": 12082
    },
    {
        "loss": 2.0922,
        "grad_norm": 3.4895431995391846,
        "learning_rate": 4.1312595241299156e-05,
        "epoch": 0.9004396750875624,
        "step": 12083
    },
    {
        "loss": 2.3838,
        "grad_norm": 3.189812421798706,
        "learning_rate": 4.125563622245786e-05,
        "epoch": 0.9005141962888442,
        "step": 12084
    },
    {
        "loss": 1.8327,
        "grad_norm": 3.553419589996338,
        "learning_rate": 4.11987062898747e-05,
        "epoch": 0.9005887174901259,
        "step": 12085
    },
    {
        "loss": 2.4467,
        "grad_norm": 2.903256416320801,
        "learning_rate": 4.114180547173743e-05,
        "epoch": 0.9006632386914077,
        "step": 12086
    },
    {
        "loss": 2.8204,
        "grad_norm": 2.751404285430908,
        "learning_rate": 4.10849337962195e-05,
        "epoch": 0.9007377598926894,
        "step": 12087
    },
    {
        "loss": 2.3306,
        "grad_norm": 3.3808467388153076,
        "learning_rate": 4.1028091291479856e-05,
        "epoch": 0.9008122810939713,
        "step": 12088
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.778047561645508,
        "learning_rate": 4.097127798566327e-05,
        "epoch": 0.900886802295253,
        "step": 12089
    },
    {
        "loss": 2.8487,
        "grad_norm": 1.83275306224823,
        "learning_rate": 4.091449390689981e-05,
        "epoch": 0.9009613234965348,
        "step": 12090
    },
    {
        "loss": 1.9466,
        "grad_norm": 3.938028335571289,
        "learning_rate": 4.085773908330506e-05,
        "epoch": 0.9010358446978165,
        "step": 12091
    },
    {
        "loss": 2.0413,
        "grad_norm": 4.766923904418945,
        "learning_rate": 4.080101354298012e-05,
        "epoch": 0.9011103658990983,
        "step": 12092
    },
    {
        "loss": 2.0219,
        "grad_norm": 2.542106866836548,
        "learning_rate": 4.074431731401188e-05,
        "epoch": 0.90118488710038,
        "step": 12093
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.1604723930358887,
        "learning_rate": 4.068765042447239e-05,
        "epoch": 0.9012594083016618,
        "step": 12094
    },
    {
        "loss": 2.3952,
        "grad_norm": 3.306598424911499,
        "learning_rate": 4.0631012902419175e-05,
        "epoch": 0.9013339295029436,
        "step": 12095
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.65977144241333,
        "learning_rate": 4.057440477589555e-05,
        "epoch": 0.9014084507042254,
        "step": 12096
    },
    {
        "loss": 2.4505,
        "grad_norm": 3.325388193130493,
        "learning_rate": 4.051782607292982e-05,
        "epoch": 0.9014829719055071,
        "step": 12097
    },
    {
        "loss": 2.767,
        "grad_norm": 2.4626901149749756,
        "learning_rate": 4.046127682153632e-05,
        "epoch": 0.9015574931067889,
        "step": 12098
    },
    {
        "loss": 2.7778,
        "grad_norm": 3.2343106269836426,
        "learning_rate": 4.0404757049713935e-05,
        "epoch": 0.9016320143080706,
        "step": 12099
    },
    {
        "loss": 1.5997,
        "grad_norm": 3.3149168491363525,
        "learning_rate": 4.034826678544782e-05,
        "epoch": 0.9017065355093524,
        "step": 12100
    },
    {
        "loss": 2.9758,
        "grad_norm": 2.5099294185638428,
        "learning_rate": 4.029180605670804e-05,
        "epoch": 0.9017810567106341,
        "step": 12101
    },
    {
        "loss": 2.4667,
        "grad_norm": 2.422712564468384,
        "learning_rate": 4.023537489145003e-05,
        "epoch": 0.901855577911916,
        "step": 12102
    },
    {
        "loss": 2.185,
        "grad_norm": 3.429933547973633,
        "learning_rate": 4.017897331761494e-05,
        "epoch": 0.9019300991131977,
        "step": 12103
    },
    {
        "loss": 2.3855,
        "grad_norm": 3.4808619022369385,
        "learning_rate": 4.012260136312893e-05,
        "epoch": 0.9020046203144795,
        "step": 12104
    },
    {
        "loss": 2.6816,
        "grad_norm": 3.432921886444092,
        "learning_rate": 4.006625905590349e-05,
        "epoch": 0.9020791415157612,
        "step": 12105
    },
    {
        "loss": 2.0381,
        "grad_norm": 4.117740631103516,
        "learning_rate": 4.000994642383579e-05,
        "epoch": 0.902153662717043,
        "step": 12106
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.9602341651916504,
        "learning_rate": 3.995366349480787e-05,
        "epoch": 0.9022281839183247,
        "step": 12107
    },
    {
        "loss": 2.8008,
        "grad_norm": 2.668342113494873,
        "learning_rate": 3.98974102966872e-05,
        "epoch": 0.9023027051196065,
        "step": 12108
    },
    {
        "loss": 2.7108,
        "grad_norm": 3.9935812950134277,
        "learning_rate": 3.984118685732687e-05,
        "epoch": 0.9023772263208883,
        "step": 12109
    },
    {
        "loss": 1.719,
        "grad_norm": 3.501201629638672,
        "learning_rate": 3.9784993204564556e-05,
        "epoch": 0.9024517475221701,
        "step": 12110
    },
    {
        "loss": 2.2355,
        "grad_norm": 2.8962011337280273,
        "learning_rate": 3.972882936622386e-05,
        "epoch": 0.9025262687234519,
        "step": 12111
    },
    {
        "loss": 2.6762,
        "grad_norm": 2.4967002868652344,
        "learning_rate": 3.967269537011316e-05,
        "epoch": 0.9026007899247336,
        "step": 12112
    },
    {
        "loss": 2.6267,
        "grad_norm": 3.2989776134490967,
        "learning_rate": 3.9616591244026457e-05,
        "epoch": 0.9026753111260154,
        "step": 12113
    },
    {
        "loss": 2.7907,
        "grad_norm": 2.436882734298706,
        "learning_rate": 3.956051701574257e-05,
        "epoch": 0.9027498323272971,
        "step": 12114
    },
    {
        "loss": 2.2747,
        "grad_norm": 2.9727654457092285,
        "learning_rate": 3.950447271302562e-05,
        "epoch": 0.9028243535285789,
        "step": 12115
    },
    {
        "loss": 3.0044,
        "grad_norm": 2.913705825805664,
        "learning_rate": 3.9448458363625185e-05,
        "epoch": 0.9028988747298606,
        "step": 12116
    },
    {
        "loss": 2.2783,
        "grad_norm": 3.363783597946167,
        "learning_rate": 3.939247399527559e-05,
        "epoch": 0.9029733959311425,
        "step": 12117
    },
    {
        "loss": 3.1166,
        "grad_norm": 2.885139226913452,
        "learning_rate": 3.9336519635696855e-05,
        "epoch": 0.9030479171324242,
        "step": 12118
    },
    {
        "loss": 2.8438,
        "grad_norm": 2.870041608810425,
        "learning_rate": 3.928059531259337e-05,
        "epoch": 0.903122438333706,
        "step": 12119
    },
    {
        "loss": 2.0992,
        "grad_norm": 4.127268314361572,
        "learning_rate": 3.9224701053655444e-05,
        "epoch": 0.9031969595349877,
        "step": 12120
    },
    {
        "loss": 2.4018,
        "grad_norm": 3.1438214778900146,
        "learning_rate": 3.916883688655807e-05,
        "epoch": 0.9032714807362695,
        "step": 12121
    },
    {
        "loss": 2.0918,
        "grad_norm": 2.8694183826446533,
        "learning_rate": 3.9113002838961254e-05,
        "epoch": 0.9033460019375512,
        "step": 12122
    },
    {
        "loss": 2.0036,
        "grad_norm": 2.8307628631591797,
        "learning_rate": 3.905719893851051e-05,
        "epoch": 0.903420523138833,
        "step": 12123
    },
    {
        "loss": 2.1528,
        "grad_norm": 3.233950138092041,
        "learning_rate": 3.900142521283605e-05,
        "epoch": 0.9034950443401147,
        "step": 12124
    },
    {
        "loss": 2.7136,
        "grad_norm": 4.052019119262695,
        "learning_rate": 3.8945681689553195e-05,
        "epoch": 0.9035695655413966,
        "step": 12125
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.8699634075164795,
        "learning_rate": 3.888996839626259e-05,
        "epoch": 0.9036440867426783,
        "step": 12126
    },
    {
        "loss": 2.2051,
        "grad_norm": 3.7228336334228516,
        "learning_rate": 3.88342853605496e-05,
        "epoch": 0.9037186079439601,
        "step": 12127
    },
    {
        "loss": 2.898,
        "grad_norm": 2.8388895988464355,
        "learning_rate": 3.877863260998461e-05,
        "epoch": 0.9037931291452418,
        "step": 12128
    },
    {
        "loss": 2.2393,
        "grad_norm": 2.714186906814575,
        "learning_rate": 3.872301017212349e-05,
        "epoch": 0.9038676503465236,
        "step": 12129
    },
    {
        "loss": 2.8932,
        "grad_norm": 2.426504373550415,
        "learning_rate": 3.8667418074506215e-05,
        "epoch": 0.9039421715478053,
        "step": 12130
    },
    {
        "loss": 2.4934,
        "grad_norm": 2.133399486541748,
        "learning_rate": 3.8611856344658616e-05,
        "epoch": 0.9040166927490871,
        "step": 12131
    },
    {
        "loss": 2.3542,
        "grad_norm": 2.7617270946502686,
        "learning_rate": 3.855632501009101e-05,
        "epoch": 0.9040912139503688,
        "step": 12132
    },
    {
        "loss": 1.9185,
        "grad_norm": 3.864474296569824,
        "learning_rate": 3.85008240982987e-05,
        "epoch": 0.9041657351516507,
        "step": 12133
    },
    {
        "loss": 2.4096,
        "grad_norm": 3.246793746948242,
        "learning_rate": 3.844535363676218e-05,
        "epoch": 0.9042402563529324,
        "step": 12134
    },
    {
        "loss": 1.859,
        "grad_norm": 1.5945323705673218,
        "learning_rate": 3.8389913652946507e-05,
        "epoch": 0.9043147775542142,
        "step": 12135
    },
    {
        "loss": 2.3057,
        "grad_norm": 3.7002251148223877,
        "learning_rate": 3.833450417430207e-05,
        "epoch": 0.9043892987554959,
        "step": 12136
    },
    {
        "loss": 1.3538,
        "grad_norm": 3.6982154846191406,
        "learning_rate": 3.827912522826374e-05,
        "epoch": 0.9044638199567777,
        "step": 12137
    },
    {
        "loss": 2.8239,
        "grad_norm": 2.495103120803833,
        "learning_rate": 3.8223776842251466e-05,
        "epoch": 0.9045383411580594,
        "step": 12138
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.614276170730591,
        "learning_rate": 3.8168459043669976e-05,
        "epoch": 0.9046128623593412,
        "step": 12139
    },
    {
        "loss": 2.2098,
        "grad_norm": 2.4285178184509277,
        "learning_rate": 3.81131718599091e-05,
        "epoch": 0.904687383560623,
        "step": 12140
    },
    {
        "loss": 2.9287,
        "grad_norm": 2.2265214920043945,
        "learning_rate": 3.805791531834326e-05,
        "epoch": 0.9047619047619048,
        "step": 12141
    },
    {
        "loss": 2.3149,
        "grad_norm": 2.879964828491211,
        "learning_rate": 3.800268944633175e-05,
        "epoch": 0.9048364259631865,
        "step": 12142
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.5227580070495605,
        "learning_rate": 3.7947494271218654e-05,
        "epoch": 0.9049109471644683,
        "step": 12143
    },
    {
        "loss": 2.2031,
        "grad_norm": 3.274123430252075,
        "learning_rate": 3.789232982033304e-05,
        "epoch": 0.90498546836575,
        "step": 12144
    },
    {
        "loss": 2.1105,
        "grad_norm": 2.476426839828491,
        "learning_rate": 3.783719612098846e-05,
        "epoch": 0.9050599895670318,
        "step": 12145
    },
    {
        "loss": 2.3891,
        "grad_norm": 3.2058019638061523,
        "learning_rate": 3.778209320048364e-05,
        "epoch": 0.9051345107683136,
        "step": 12146
    },
    {
        "loss": 1.9065,
        "grad_norm": 2.3450090885162354,
        "learning_rate": 3.772702108610171e-05,
        "epoch": 0.9052090319695953,
        "step": 12147
    },
    {
        "loss": 2.2242,
        "grad_norm": 1.962402582168579,
        "learning_rate": 3.7671979805110594e-05,
        "epoch": 0.9052835531708772,
        "step": 12148
    },
    {
        "loss": 2.5567,
        "grad_norm": 1.451873540878296,
        "learning_rate": 3.761696938476333e-05,
        "epoch": 0.9053580743721589,
        "step": 12149
    },
    {
        "loss": 2.5972,
        "grad_norm": 3.5078563690185547,
        "learning_rate": 3.756198985229697e-05,
        "epoch": 0.9054325955734407,
        "step": 12150
    },
    {
        "loss": 2.0364,
        "grad_norm": 3.928316354751587,
        "learning_rate": 3.750704123493404e-05,
        "epoch": 0.9055071167747224,
        "step": 12151
    },
    {
        "loss": 1.4546,
        "grad_norm": 4.184173583984375,
        "learning_rate": 3.745212355988119e-05,
        "epoch": 0.9055816379760042,
        "step": 12152
    },
    {
        "loss": 2.4196,
        "grad_norm": 2.685121536254883,
        "learning_rate": 3.739723685432993e-05,
        "epoch": 0.9056561591772859,
        "step": 12153
    },
    {
        "loss": 1.9095,
        "grad_norm": 3.861143112182617,
        "learning_rate": 3.734238114545665e-05,
        "epoch": 0.9057306803785677,
        "step": 12154
    },
    {
        "loss": 1.0824,
        "grad_norm": 4.170685768127441,
        "learning_rate": 3.7287556460422016e-05,
        "epoch": 0.9058052015798495,
        "step": 12155
    },
    {
        "loss": 1.4267,
        "grad_norm": 3.2942495346069336,
        "learning_rate": 3.72327628263717e-05,
        "epoch": 0.9058797227811313,
        "step": 12156
    },
    {
        "loss": 2.4285,
        "grad_norm": 2.705547332763672,
        "learning_rate": 3.717800027043576e-05,
        "epoch": 0.905954243982413,
        "step": 12157
    },
    {
        "loss": 2.546,
        "grad_norm": 2.668067455291748,
        "learning_rate": 3.712326881972894e-05,
        "epoch": 0.9060287651836948,
        "step": 12158
    },
    {
        "loss": 1.8329,
        "grad_norm": 4.829609394073486,
        "learning_rate": 3.706856850135041e-05,
        "epoch": 0.9061032863849765,
        "step": 12159
    },
    {
        "loss": 2.3794,
        "grad_norm": 2.8427040576934814,
        "learning_rate": 3.701389934238444e-05,
        "epoch": 0.9061778075862583,
        "step": 12160
    },
    {
        "loss": 2.4512,
        "grad_norm": 3.2027196884155273,
        "learning_rate": 3.695926136989909e-05,
        "epoch": 0.90625232878754,
        "step": 12161
    },
    {
        "loss": 2.85,
        "grad_norm": 3.1925320625305176,
        "learning_rate": 3.690465461094773e-05,
        "epoch": 0.9063268499888218,
        "step": 12162
    },
    {
        "loss": 1.2867,
        "grad_norm": 2.5629723072052,
        "learning_rate": 3.685007909256775e-05,
        "epoch": 0.9064013711901036,
        "step": 12163
    },
    {
        "loss": 1.3413,
        "grad_norm": 3.952432155609131,
        "learning_rate": 3.6795534841781475e-05,
        "epoch": 0.9064758923913854,
        "step": 12164
    },
    {
        "loss": 2.1019,
        "grad_norm": 2.693964958190918,
        "learning_rate": 3.674102188559545e-05,
        "epoch": 0.9065504135926671,
        "step": 12165
    },
    {
        "loss": 2.3836,
        "grad_norm": 2.749783515930176,
        "learning_rate": 3.6686540251000756e-05,
        "epoch": 0.9066249347939489,
        "step": 12166
    },
    {
        "loss": 2.7328,
        "grad_norm": 2.6273090839385986,
        "learning_rate": 3.663208996497317e-05,
        "epoch": 0.9066994559952306,
        "step": 12167
    },
    {
        "loss": 2.3158,
        "grad_norm": 3.1213631629943848,
        "learning_rate": 3.657767105447264e-05,
        "epoch": 0.9067739771965124,
        "step": 12168
    },
    {
        "loss": 2.6732,
        "grad_norm": 2.635781764984131,
        "learning_rate": 3.6523283546444065e-05,
        "epoch": 0.9068484983977941,
        "step": 12169
    },
    {
        "loss": 2.2753,
        "grad_norm": 4.424756050109863,
        "learning_rate": 3.646892746781607e-05,
        "epoch": 0.906923019599076,
        "step": 12170
    },
    {
        "loss": 2.4114,
        "grad_norm": 2.0685365200042725,
        "learning_rate": 3.641460284550242e-05,
        "epoch": 0.9069975408003577,
        "step": 12171
    },
    {
        "loss": 2.6672,
        "grad_norm": 1.3431872129440308,
        "learning_rate": 3.6360309706400944e-05,
        "epoch": 0.9070720620016395,
        "step": 12172
    },
    {
        "loss": 2.0716,
        "grad_norm": 3.756854295730591,
        "learning_rate": 3.6306048077393806e-05,
        "epoch": 0.9071465832029212,
        "step": 12173
    },
    {
        "loss": 1.9282,
        "grad_norm": 1.6176764965057373,
        "learning_rate": 3.6251817985348004e-05,
        "epoch": 0.907221104404203,
        "step": 12174
    },
    {
        "loss": 2.422,
        "grad_norm": 3.3205156326293945,
        "learning_rate": 3.619761945711442e-05,
        "epoch": 0.9072956256054847,
        "step": 12175
    },
    {
        "loss": 1.092,
        "grad_norm": 4.511041641235352,
        "learning_rate": 3.614345251952848e-05,
        "epoch": 0.9073701468067665,
        "step": 12176
    },
    {
        "loss": 2.2786,
        "grad_norm": 3.217372417449951,
        "learning_rate": 3.608931719941019e-05,
        "epoch": 0.9074446680080482,
        "step": 12177
    },
    {
        "loss": 1.9992,
        "grad_norm": 4.463246822357178,
        "learning_rate": 3.603521352356364e-05,
        "epoch": 0.9075191892093301,
        "step": 12178
    },
    {
        "loss": 1.9959,
        "grad_norm": 2.958634853363037,
        "learning_rate": 3.5981141518777206e-05,
        "epoch": 0.9075937104106118,
        "step": 12179
    },
    {
        "loss": 2.4254,
        "grad_norm": 1.9713890552520752,
        "learning_rate": 3.592710121182407e-05,
        "epoch": 0.9076682316118936,
        "step": 12180
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.1360387802124023,
        "learning_rate": 3.587309262946087e-05,
        "epoch": 0.9077427528131754,
        "step": 12181
    },
    {
        "loss": 2.4421,
        "grad_norm": 2.4313979148864746,
        "learning_rate": 3.581911579842937e-05,
        "epoch": 0.9078172740144571,
        "step": 12182
    },
    {
        "loss": 2.4725,
        "grad_norm": 3.0752158164978027,
        "learning_rate": 3.576517074545505e-05,
        "epoch": 0.9078917952157389,
        "step": 12183
    },
    {
        "loss": 2.7258,
        "grad_norm": 2.3572285175323486,
        "learning_rate": 3.571125749724808e-05,
        "epoch": 0.9079663164170206,
        "step": 12184
    },
    {
        "loss": 1.8827,
        "grad_norm": 4.039308071136475,
        "learning_rate": 3.565737608050259e-05,
        "epoch": 0.9080408376183025,
        "step": 12185
    },
    {
        "loss": 2.7765,
        "grad_norm": 2.92016863822937,
        "learning_rate": 3.5603526521896914e-05,
        "epoch": 0.9081153588195842,
        "step": 12186
    },
    {
        "loss": 2.4191,
        "grad_norm": 3.1143784523010254,
        "learning_rate": 3.5549708848093946e-05,
        "epoch": 0.908189880020866,
        "step": 12187
    },
    {
        "loss": 2.1939,
        "grad_norm": 3.04677414894104,
        "learning_rate": 3.549592308574046e-05,
        "epoch": 0.9082644012221477,
        "step": 12188
    },
    {
        "loss": 2.4769,
        "grad_norm": 4.2789387702941895,
        "learning_rate": 3.54421692614676e-05,
        "epoch": 0.9083389224234295,
        "step": 12189
    },
    {
        "loss": 2.0554,
        "grad_norm": 2.2873940467834473,
        "learning_rate": 3.538844740189046e-05,
        "epoch": 0.9084134436247112,
        "step": 12190
    },
    {
        "loss": 1.8242,
        "grad_norm": 2.2144880294799805,
        "learning_rate": 3.533475753360874e-05,
        "epoch": 0.908487964825993,
        "step": 12191
    },
    {
        "loss": 1.9644,
        "grad_norm": 2.9042115211486816,
        "learning_rate": 3.5281099683205954e-05,
        "epoch": 0.9085624860272747,
        "step": 12192
    },
    {
        "loss": 2.6816,
        "grad_norm": 2.2927451133728027,
        "learning_rate": 3.522747387724984e-05,
        "epoch": 0.9086370072285566,
        "step": 12193
    },
    {
        "loss": 2.7061,
        "grad_norm": 2.2947885990142822,
        "learning_rate": 3.5173880142292215e-05,
        "epoch": 0.9087115284298383,
        "step": 12194
    },
    {
        "loss": 1.8246,
        "grad_norm": 2.794893741607666,
        "learning_rate": 3.512031850486928e-05,
        "epoch": 0.9087860496311201,
        "step": 12195
    },
    {
        "loss": 2.8914,
        "grad_norm": 3.5404021739959717,
        "learning_rate": 3.5066788991500944e-05,
        "epoch": 0.9088605708324018,
        "step": 12196
    },
    {
        "loss": 2.7138,
        "grad_norm": 2.830611228942871,
        "learning_rate": 3.501329162869168e-05,
        "epoch": 0.9089350920336836,
        "step": 12197
    },
    {
        "loss": 1.9408,
        "grad_norm": 3.046083688735962,
        "learning_rate": 3.49598264429296e-05,
        "epoch": 0.9090096132349653,
        "step": 12198
    },
    {
        "loss": 2.8531,
        "grad_norm": 3.7749545574188232,
        "learning_rate": 3.4906393460687006e-05,
        "epoch": 0.9090841344362471,
        "step": 12199
    },
    {
        "loss": 3.0003,
        "grad_norm": 2.553158760070801,
        "learning_rate": 3.485299270842062e-05,
        "epoch": 0.9091586556375288,
        "step": 12200
    },
    {
        "loss": 1.7679,
        "grad_norm": 3.133850336074829,
        "learning_rate": 3.479962421257049e-05,
        "epoch": 0.9092331768388107,
        "step": 12201
    },
    {
        "loss": 2.4493,
        "grad_norm": 2.147730588912964,
        "learning_rate": 3.474628799956143e-05,
        "epoch": 0.9093076980400924,
        "step": 12202
    },
    {
        "loss": 1.9753,
        "grad_norm": 4.181503772735596,
        "learning_rate": 3.469298409580186e-05,
        "epoch": 0.9093822192413742,
        "step": 12203
    },
    {
        "loss": 2.325,
        "grad_norm": 2.378967761993408,
        "learning_rate": 3.463971252768413e-05,
        "epoch": 0.9094567404426559,
        "step": 12204
    },
    {
        "loss": 2.5506,
        "grad_norm": 2.8811378479003906,
        "learning_rate": 3.458647332158497e-05,
        "epoch": 0.9095312616439377,
        "step": 12205
    },
    {
        "loss": 2.4003,
        "grad_norm": 4.4221649169921875,
        "learning_rate": 3.4533266503864634e-05,
        "epoch": 0.9096057828452194,
        "step": 12206
    },
    {
        "loss": 2.6851,
        "grad_norm": 3.3413989543914795,
        "learning_rate": 3.448009210086779e-05,
        "epoch": 0.9096803040465012,
        "step": 12207
    },
    {
        "loss": 2.6906,
        "grad_norm": 3.3135721683502197,
        "learning_rate": 3.442695013892272e-05,
        "epoch": 0.909754825247783,
        "step": 12208
    },
    {
        "loss": 2.656,
        "grad_norm": 1.9483953714370728,
        "learning_rate": 3.4373840644341726e-05,
        "epoch": 0.9098293464490648,
        "step": 12209
    },
    {
        "loss": 2.183,
        "grad_norm": 3.3516488075256348,
        "learning_rate": 3.4320763643420996e-05,
        "epoch": 0.9099038676503465,
        "step": 12210
    },
    {
        "loss": 1.022,
        "grad_norm": 2.1147944927215576,
        "learning_rate": 3.4267719162440884e-05,
        "epoch": 0.9099783888516283,
        "step": 12211
    },
    {
        "loss": 2.4968,
        "grad_norm": 2.2568092346191406,
        "learning_rate": 3.421470722766531e-05,
        "epoch": 0.91005291005291,
        "step": 12212
    },
    {
        "loss": 1.8527,
        "grad_norm": 2.6942553520202637,
        "learning_rate": 3.4161727865342244e-05,
        "epoch": 0.9101274312541918,
        "step": 12213
    },
    {
        "loss": 2.6273,
        "grad_norm": 2.2768936157226562,
        "learning_rate": 3.4108781101703404e-05,
        "epoch": 0.9102019524554735,
        "step": 12214
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.6412060260772705,
        "learning_rate": 3.405586696296465e-05,
        "epoch": 0.9102764736567553,
        "step": 12215
    },
    {
        "loss": 1.0911,
        "grad_norm": 3.88254976272583,
        "learning_rate": 3.4002985475325344e-05,
        "epoch": 0.9103509948580372,
        "step": 12216
    },
    {
        "loss": 1.631,
        "grad_norm": 3.782602310180664,
        "learning_rate": 3.3950136664969e-05,
        "epoch": 0.9104255160593189,
        "step": 12217
    },
    {
        "loss": 2.4503,
        "grad_norm": 1.9804950952529907,
        "learning_rate": 3.389732055806272e-05,
        "epoch": 0.9105000372606007,
        "step": 12218
    },
    {
        "loss": 2.1874,
        "grad_norm": 3.090332508087158,
        "learning_rate": 3.3844537180757385e-05,
        "epoch": 0.9105745584618824,
        "step": 12219
    },
    {
        "loss": 2.2212,
        "grad_norm": 4.52340841293335,
        "learning_rate": 3.37917865591881e-05,
        "epoch": 0.9106490796631642,
        "step": 12220
    },
    {
        "loss": 2.2766,
        "grad_norm": 2.645195722579956,
        "learning_rate": 3.373906871947295e-05,
        "epoch": 0.9107236008644459,
        "step": 12221
    },
    {
        "loss": 2.0522,
        "grad_norm": 4.5775861740112305,
        "learning_rate": 3.368638368771462e-05,
        "epoch": 0.9107981220657277,
        "step": 12222
    },
    {
        "loss": 2.1845,
        "grad_norm": 2.154266834259033,
        "learning_rate": 3.363373148999907e-05,
        "epoch": 0.9108726432670095,
        "step": 12223
    },
    {
        "loss": 2.4258,
        "grad_norm": 3.3584160804748535,
        "learning_rate": 3.358111215239604e-05,
        "epoch": 0.9109471644682913,
        "step": 12224
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.8185982704162598,
        "learning_rate": 3.35285257009593e-05,
        "epoch": 0.911021685669573,
        "step": 12225
    },
    {
        "loss": 1.9662,
        "grad_norm": 4.868005275726318,
        "learning_rate": 3.347597216172603e-05,
        "epoch": 0.9110962068708548,
        "step": 12226
    },
    {
        "loss": 2.1227,
        "grad_norm": 2.8859548568725586,
        "learning_rate": 3.3423451560717056e-05,
        "epoch": 0.9111707280721365,
        "step": 12227
    },
    {
        "loss": 2.2565,
        "grad_norm": 4.628559589385986,
        "learning_rate": 3.337096392393728e-05,
        "epoch": 0.9112452492734183,
        "step": 12228
    },
    {
        "loss": 2.4888,
        "grad_norm": 2.419341564178467,
        "learning_rate": 3.331850927737495e-05,
        "epoch": 0.9113197704747,
        "step": 12229
    },
    {
        "loss": 2.7115,
        "grad_norm": 4.280259132385254,
        "learning_rate": 3.3266087647001984e-05,
        "epoch": 0.9113942916759818,
        "step": 12230
    },
    {
        "loss": 1.9082,
        "grad_norm": 2.7315173149108887,
        "learning_rate": 3.3213699058774336e-05,
        "epoch": 0.9114688128772636,
        "step": 12231
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.447927474975586,
        "learning_rate": 3.3161343538630916e-05,
        "epoch": 0.9115433340785454,
        "step": 12232
    },
    {
        "loss": 2.6751,
        "grad_norm": 2.252460241317749,
        "learning_rate": 3.3109021112494956e-05,
        "epoch": 0.9116178552798271,
        "step": 12233
    },
    {
        "loss": 2.6711,
        "grad_norm": 3.418081521987915,
        "learning_rate": 3.3056731806272844e-05,
        "epoch": 0.9116923764811089,
        "step": 12234
    },
    {
        "loss": 2.1857,
        "grad_norm": 2.1979684829711914,
        "learning_rate": 3.300447564585485e-05,
        "epoch": 0.9117668976823906,
        "step": 12235
    },
    {
        "loss": 2.7177,
        "grad_norm": 2.3485562801361084,
        "learning_rate": 3.2952252657114643e-05,
        "epoch": 0.9118414188836724,
        "step": 12236
    },
    {
        "loss": 2.5514,
        "grad_norm": 2.677619218826294,
        "learning_rate": 3.2900062865909445e-05,
        "epoch": 0.9119159400849541,
        "step": 12237
    },
    {
        "loss": 1.5721,
        "grad_norm": 2.975562334060669,
        "learning_rate": 3.2847906298080324e-05,
        "epoch": 0.911990461286236,
        "step": 12238
    },
    {
        "loss": 2.4844,
        "grad_norm": 3.5512380599975586,
        "learning_rate": 3.2795782979451505e-05,
        "epoch": 0.9120649824875177,
        "step": 12239
    },
    {
        "loss": 1.4646,
        "grad_norm": 6.151555061340332,
        "learning_rate": 3.274369293583124e-05,
        "epoch": 0.9121395036887995,
        "step": 12240
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.3269635438919067,
        "learning_rate": 3.2691636193010635e-05,
        "epoch": 0.9122140248900812,
        "step": 12241
    },
    {
        "loss": 1.8152,
        "grad_norm": 4.668926239013672,
        "learning_rate": 3.2639612776765016e-05,
        "epoch": 0.912288546091363,
        "step": 12242
    },
    {
        "loss": 2.7853,
        "grad_norm": 2.777113437652588,
        "learning_rate": 3.2587622712852696e-05,
        "epoch": 0.9123630672926447,
        "step": 12243
    },
    {
        "loss": 3.2538,
        "grad_norm": 3.6128334999084473,
        "learning_rate": 3.253566602701561e-05,
        "epoch": 0.9124375884939265,
        "step": 12244
    },
    {
        "loss": 1.927,
        "grad_norm": 2.723783493041992,
        "learning_rate": 3.248374274497942e-05,
        "epoch": 0.9125121096952082,
        "step": 12245
    },
    {
        "loss": 2.5778,
        "grad_norm": 2.3738934993743896,
        "learning_rate": 3.243185289245292e-05,
        "epoch": 0.9125866308964901,
        "step": 12246
    },
    {
        "loss": 2.4225,
        "grad_norm": 3.148401975631714,
        "learning_rate": 3.237999649512839e-05,
        "epoch": 0.9126611520977718,
        "step": 12247
    },
    {
        "loss": 2.6367,
        "grad_norm": 2.583864688873291,
        "learning_rate": 3.232817357868182e-05,
        "epoch": 0.9127356732990536,
        "step": 12248
    },
    {
        "loss": 2.2674,
        "grad_norm": 2.1414148807525635,
        "learning_rate": 3.227638416877237e-05,
        "epoch": 0.9128101945003353,
        "step": 12249
    },
    {
        "loss": 2.5679,
        "grad_norm": 2.9368276596069336,
        "learning_rate": 3.2224628291042526e-05,
        "epoch": 0.9128847157016171,
        "step": 12250
    },
    {
        "loss": 2.5803,
        "grad_norm": 2.384286403656006,
        "learning_rate": 3.217290597111862e-05,
        "epoch": 0.9129592369028989,
        "step": 12251
    },
    {
        "loss": 2.103,
        "grad_norm": 2.2869925498962402,
        "learning_rate": 3.212121723460968e-05,
        "epoch": 0.9130337581041806,
        "step": 12252
    },
    {
        "loss": 2.8079,
        "grad_norm": 2.3624632358551025,
        "learning_rate": 3.20695621071088e-05,
        "epoch": 0.9131082793054625,
        "step": 12253
    },
    {
        "loss": 2.6021,
        "grad_norm": 3.123434066772461,
        "learning_rate": 3.201794061419201e-05,
        "epoch": 0.9131828005067442,
        "step": 12254
    },
    {
        "loss": 2.5971,
        "grad_norm": 3.152039051055908,
        "learning_rate": 3.196635278141871e-05,
        "epoch": 0.913257321708026,
        "step": 12255
    },
    {
        "loss": 1.9032,
        "grad_norm": 3.6459975242614746,
        "learning_rate": 3.191479863433195e-05,
        "epoch": 0.9133318429093077,
        "step": 12256
    },
    {
        "loss": 2.4735,
        "grad_norm": 3.3713481426239014,
        "learning_rate": 3.1863278198457634e-05,
        "epoch": 0.9134063641105895,
        "step": 12257
    },
    {
        "loss": 2.8211,
        "grad_norm": 2.792513132095337,
        "learning_rate": 3.181179149930542e-05,
        "epoch": 0.9134808853118712,
        "step": 12258
    },
    {
        "loss": 2.8022,
        "grad_norm": 3.418210744857788,
        "learning_rate": 3.1760338562367975e-05,
        "epoch": 0.913555406513153,
        "step": 12259
    },
    {
        "loss": 2.4342,
        "grad_norm": 1.3560084104537964,
        "learning_rate": 3.170891941312133e-05,
        "epoch": 0.9136299277144347,
        "step": 12260
    },
    {
        "loss": 2.098,
        "grad_norm": 4.25631046295166,
        "learning_rate": 3.165753407702469e-05,
        "epoch": 0.9137044489157166,
        "step": 12261
    },
    {
        "loss": 1.8823,
        "grad_norm": 3.689119338989258,
        "learning_rate": 3.160618257952081e-05,
        "epoch": 0.9137789701169983,
        "step": 12262
    },
    {
        "loss": 2.8909,
        "grad_norm": 3.408914566040039,
        "learning_rate": 3.155486494603541e-05,
        "epoch": 0.9138534913182801,
        "step": 12263
    },
    {
        "loss": 2.3547,
        "grad_norm": 3.2210755348205566,
        "learning_rate": 3.150358120197756e-05,
        "epoch": 0.9139280125195618,
        "step": 12264
    },
    {
        "loss": 2.252,
        "grad_norm": 3.4412338733673096,
        "learning_rate": 3.1452331372739344e-05,
        "epoch": 0.9140025337208436,
        "step": 12265
    },
    {
        "loss": 2.7445,
        "grad_norm": 3.9194788932800293,
        "learning_rate": 3.140111548369648e-05,
        "epoch": 0.9140770549221253,
        "step": 12266
    },
    {
        "loss": 2.2017,
        "grad_norm": 3.131134510040283,
        "learning_rate": 3.1349933560207425e-05,
        "epoch": 0.9141515761234071,
        "step": 12267
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.6255180835723877,
        "learning_rate": 3.129878562761421e-05,
        "epoch": 0.9142260973246888,
        "step": 12268
    },
    {
        "loss": 2.2779,
        "grad_norm": 3.142638921737671,
        "learning_rate": 3.124767171124178e-05,
        "epoch": 0.9143006185259707,
        "step": 12269
    },
    {
        "loss": 1.2784,
        "grad_norm": 3.8914527893066406,
        "learning_rate": 3.119659183639818e-05,
        "epoch": 0.9143751397272524,
        "step": 12270
    },
    {
        "loss": 1.9107,
        "grad_norm": 2.778592586517334,
        "learning_rate": 3.114554602837508e-05,
        "epoch": 0.9144496609285342,
        "step": 12271
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.3088467121124268,
        "learning_rate": 3.1094534312446464e-05,
        "epoch": 0.9145241821298159,
        "step": 12272
    },
    {
        "loss": 1.5365,
        "grad_norm": 4.213085651397705,
        "learning_rate": 3.1043556713870235e-05,
        "epoch": 0.9145987033310977,
        "step": 12273
    },
    {
        "loss": 2.2588,
        "grad_norm": 1.9595454931259155,
        "learning_rate": 3.099261325788697e-05,
        "epoch": 0.9146732245323794,
        "step": 12274
    },
    {
        "loss": 1.469,
        "grad_norm": 3.4948315620422363,
        "learning_rate": 3.0941703969720334e-05,
        "epoch": 0.9147477457336612,
        "step": 12275
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.771254062652588,
        "learning_rate": 3.089082887457739e-05,
        "epoch": 0.914822266934943,
        "step": 12276
    },
    {
        "loss": 2.419,
        "grad_norm": 2.4214441776275635,
        "learning_rate": 3.083998799764787e-05,
        "epoch": 0.9148967881362248,
        "step": 12277
    },
    {
        "loss": 1.4031,
        "grad_norm": 4.293116092681885,
        "learning_rate": 3.078918136410495e-05,
        "epoch": 0.9149713093375065,
        "step": 12278
    },
    {
        "loss": 2.4327,
        "grad_norm": 2.034661293029785,
        "learning_rate": 3.073840899910457e-05,
        "epoch": 0.9150458305387883,
        "step": 12279
    },
    {
        "loss": 2.7111,
        "grad_norm": 2.264058828353882,
        "learning_rate": 3.068767092778576e-05,
        "epoch": 0.91512035174007,
        "step": 12280
    },
    {
        "loss": 2.1734,
        "grad_norm": 3.3770759105682373,
        "learning_rate": 3.0636967175270525e-05,
        "epoch": 0.9151948729413518,
        "step": 12281
    },
    {
        "loss": 2.2458,
        "grad_norm": 3.9685890674591064,
        "learning_rate": 3.058629776666423e-05,
        "epoch": 0.9152693941426335,
        "step": 12282
    },
    {
        "loss": 2.6083,
        "grad_norm": 3.8290836811065674,
        "learning_rate": 3.053566272705458e-05,
        "epoch": 0.9153439153439153,
        "step": 12283
    },
    {
        "loss": 2.0959,
        "grad_norm": 3.305675506591797,
        "learning_rate": 3.0485062081512938e-05,
        "epoch": 0.9154184365451971,
        "step": 12284
    },
    {
        "loss": 2.4647,
        "grad_norm": 2.4721932411193848,
        "learning_rate": 3.0434495855093148e-05,
        "epoch": 0.9154929577464789,
        "step": 12285
    },
    {
        "loss": 1.7501,
        "grad_norm": 3.642003297805786,
        "learning_rate": 3.0383964072832383e-05,
        "epoch": 0.9155674789477607,
        "step": 12286
    },
    {
        "loss": 1.9677,
        "grad_norm": 4.214544773101807,
        "learning_rate": 3.033346675975053e-05,
        "epoch": 0.9156420001490424,
        "step": 12287
    },
    {
        "loss": 2.4521,
        "grad_norm": 2.762577772140503,
        "learning_rate": 3.0283003940850296e-05,
        "epoch": 0.9157165213503242,
        "step": 12288
    },
    {
        "loss": 1.908,
        "grad_norm": 1.7332841157913208,
        "learning_rate": 3.02325756411177e-05,
        "epoch": 0.9157910425516059,
        "step": 12289
    },
    {
        "loss": 2.1583,
        "grad_norm": 3.296330213546753,
        "learning_rate": 3.0182181885521255e-05,
        "epoch": 0.9158655637528877,
        "step": 12290
    },
    {
        "loss": 2.5949,
        "grad_norm": 4.762688636779785,
        "learning_rate": 3.0131822699012847e-05,
        "epoch": 0.9159400849541695,
        "step": 12291
    },
    {
        "loss": 2.4771,
        "grad_norm": 2.6987626552581787,
        "learning_rate": 3.0081498106526575e-05,
        "epoch": 0.9160146061554513,
        "step": 12292
    },
    {
        "loss": 2.5871,
        "grad_norm": 2.0365142822265625,
        "learning_rate": 3.0031208132980083e-05,
        "epoch": 0.916089127356733,
        "step": 12293
    },
    {
        "loss": 2.7638,
        "grad_norm": 3.2436094284057617,
        "learning_rate": 2.998095280327351e-05,
        "epoch": 0.9161636485580148,
        "step": 12294
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.403729200363159,
        "learning_rate": 2.9930732142289764e-05,
        "epoch": 0.9162381697592965,
        "step": 12295
    },
    {
        "loss": 2.6228,
        "grad_norm": 2.2193386554718018,
        "learning_rate": 2.9880546174894962e-05,
        "epoch": 0.9163126909605783,
        "step": 12296
    },
    {
        "loss": 2.1332,
        "grad_norm": 3.3724660873413086,
        "learning_rate": 2.983039492593771e-05,
        "epoch": 0.91638721216186,
        "step": 12297
    },
    {
        "loss": 0.9802,
        "grad_norm": 4.618757247924805,
        "learning_rate": 2.9780278420249464e-05,
        "epoch": 0.9164617333631419,
        "step": 12298
    },
    {
        "loss": 2.4613,
        "grad_norm": 3.235614776611328,
        "learning_rate": 2.9730196682644717e-05,
        "epoch": 0.9165362545644236,
        "step": 12299
    },
    {
        "loss": 1.6794,
        "grad_norm": 4.205723285675049,
        "learning_rate": 2.9680149737920492e-05,
        "epoch": 0.9166107757657054,
        "step": 12300
    },
    {
        "loss": 1.7281,
        "grad_norm": 3.1596148014068604,
        "learning_rate": 2.9630137610856567e-05,
        "epoch": 0.9166852969669871,
        "step": 12301
    },
    {
        "loss": 2.168,
        "grad_norm": 3.001504421234131,
        "learning_rate": 2.958016032621591e-05,
        "epoch": 0.9167598181682689,
        "step": 12302
    },
    {
        "loss": 2.6046,
        "grad_norm": 2.2918004989624023,
        "learning_rate": 2.9530217908743452e-05,
        "epoch": 0.9168343393695506,
        "step": 12303
    },
    {
        "loss": 2.1748,
        "grad_norm": 2.039076566696167,
        "learning_rate": 2.9480310383167653e-05,
        "epoch": 0.9169088605708324,
        "step": 12304
    },
    {
        "loss": 2.325,
        "grad_norm": 2.7897896766662598,
        "learning_rate": 2.9430437774199173e-05,
        "epoch": 0.9169833817721141,
        "step": 12305
    },
    {
        "loss": 2.6392,
        "grad_norm": 3.597846508026123,
        "learning_rate": 2.9380600106531752e-05,
        "epoch": 0.917057902973396,
        "step": 12306
    },
    {
        "loss": 2.6505,
        "grad_norm": 1.8376420736312866,
        "learning_rate": 2.9330797404841558e-05,
        "epoch": 0.9171324241746777,
        "step": 12307
    },
    {
        "loss": 1.8298,
        "grad_norm": 1.8690003156661987,
        "learning_rate": 2.9281029693787444e-05,
        "epoch": 0.9172069453759595,
        "step": 12308
    },
    {
        "loss": 2.0448,
        "grad_norm": 3.3018617630004883,
        "learning_rate": 2.923129699801124e-05,
        "epoch": 0.9172814665772412,
        "step": 12309
    },
    {
        "loss": 1.9047,
        "grad_norm": 2.925900936126709,
        "learning_rate": 2.9181599342137023e-05,
        "epoch": 0.917355987778523,
        "step": 12310
    },
    {
        "loss": 2.131,
        "grad_norm": 2.4428086280822754,
        "learning_rate": 2.9131936750772047e-05,
        "epoch": 0.9174305089798047,
        "step": 12311
    },
    {
        "loss": 1.8368,
        "grad_norm": 4.148736000061035,
        "learning_rate": 2.9082309248505446e-05,
        "epoch": 0.9175050301810865,
        "step": 12312
    },
    {
        "loss": 2.8882,
        "grad_norm": 3.310659885406494,
        "learning_rate": 2.9032716859909747e-05,
        "epoch": 0.9175795513823682,
        "step": 12313
    },
    {
        "loss": 2.564,
        "grad_norm": 3.4733240604400635,
        "learning_rate": 2.8983159609539666e-05,
        "epoch": 0.9176540725836501,
        "step": 12314
    },
    {
        "loss": 2.2768,
        "grad_norm": 3.992983818054199,
        "learning_rate": 2.893363752193262e-05,
        "epoch": 0.9177285937849318,
        "step": 12315
    },
    {
        "loss": 2.1521,
        "grad_norm": 3.681173801422119,
        "learning_rate": 2.888415062160852e-05,
        "epoch": 0.9178031149862136,
        "step": 12316
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.380699396133423,
        "learning_rate": 2.883469893307017e-05,
        "epoch": 0.9178776361874953,
        "step": 12317
    },
    {
        "loss": 2.6675,
        "grad_norm": 1.9914416074752808,
        "learning_rate": 2.878528248080248e-05,
        "epoch": 0.9179521573887771,
        "step": 12318
    },
    {
        "loss": 1.9248,
        "grad_norm": 2.4569294452667236,
        "learning_rate": 2.873590128927335e-05,
        "epoch": 0.9180266785900588,
        "step": 12319
    },
    {
        "loss": 2.4056,
        "grad_norm": 3.959653854370117,
        "learning_rate": 2.8686555382932944e-05,
        "epoch": 0.9181011997913406,
        "step": 12320
    },
    {
        "loss": 2.0692,
        "grad_norm": 3.1698081493377686,
        "learning_rate": 2.8637244786213957e-05,
        "epoch": 0.9181757209926223,
        "step": 12321
    },
    {
        "loss": 2.9942,
        "grad_norm": 2.3196799755096436,
        "learning_rate": 2.8587969523531932e-05,
        "epoch": 0.9182502421939042,
        "step": 12322
    },
    {
        "loss": 2.7156,
        "grad_norm": 3.171466588973999,
        "learning_rate": 2.8538729619284312e-05,
        "epoch": 0.918324763395186,
        "step": 12323
    },
    {
        "loss": 2.6578,
        "grad_norm": 1.967677116394043,
        "learning_rate": 2.848952509785169e-05,
        "epoch": 0.9183992845964677,
        "step": 12324
    },
    {
        "loss": 2.0489,
        "grad_norm": 4.085853576660156,
        "learning_rate": 2.8440355983596678e-05,
        "epoch": 0.9184738057977495,
        "step": 12325
    },
    {
        "loss": 2.4311,
        "grad_norm": 3.41397762298584,
        "learning_rate": 2.8391222300864463e-05,
        "epoch": 0.9185483269990312,
        "step": 12326
    },
    {
        "loss": 1.978,
        "grad_norm": 3.6632866859436035,
        "learning_rate": 2.8342124073982914e-05,
        "epoch": 0.918622848200313,
        "step": 12327
    },
    {
        "loss": 2.8065,
        "grad_norm": 2.48555588722229,
        "learning_rate": 2.8293061327261982e-05,
        "epoch": 0.9186973694015947,
        "step": 12328
    },
    {
        "loss": 2.0556,
        "grad_norm": 3.276057243347168,
        "learning_rate": 2.8244034084994443e-05,
        "epoch": 0.9187718906028766,
        "step": 12329
    },
    {
        "loss": 2.8637,
        "grad_norm": 3.0570874214172363,
        "learning_rate": 2.8195042371455195e-05,
        "epoch": 0.9188464118041583,
        "step": 12330
    },
    {
        "loss": 1.9031,
        "grad_norm": 4.2389302253723145,
        "learning_rate": 2.8146086210901612e-05,
        "epoch": 0.9189209330054401,
        "step": 12331
    },
    {
        "loss": 2.0674,
        "grad_norm": 2.191887617111206,
        "learning_rate": 2.809716562757343e-05,
        "epoch": 0.9189954542067218,
        "step": 12332
    },
    {
        "loss": 2.0915,
        "grad_norm": 3.321732997894287,
        "learning_rate": 2.804828064569304e-05,
        "epoch": 0.9190699754080036,
        "step": 12333
    },
    {
        "loss": 2.8244,
        "grad_norm": 2.8968803882598877,
        "learning_rate": 2.799943128946483e-05,
        "epoch": 0.9191444966092853,
        "step": 12334
    },
    {
        "loss": 1.8018,
        "grad_norm": 4.519461631774902,
        "learning_rate": 2.7950617583075756e-05,
        "epoch": 0.9192190178105671,
        "step": 12335
    },
    {
        "loss": 1.6575,
        "grad_norm": 4.271174907684326,
        "learning_rate": 2.7901839550694996e-05,
        "epoch": 0.9192935390118488,
        "step": 12336
    },
    {
        "loss": 2.0289,
        "grad_norm": 3.512080430984497,
        "learning_rate": 2.7853097216474334e-05,
        "epoch": 0.9193680602131307,
        "step": 12337
    },
    {
        "loss": 2.8437,
        "grad_norm": 3.261599540710449,
        "learning_rate": 2.7804390604547493e-05,
        "epoch": 0.9194425814144124,
        "step": 12338
    },
    {
        "loss": 2.989,
        "grad_norm": 2.4720840454101562,
        "learning_rate": 2.7755719739030918e-05,
        "epoch": 0.9195171026156942,
        "step": 12339
    },
    {
        "loss": 2.4615,
        "grad_norm": 2.7137205600738525,
        "learning_rate": 2.7707084644023084e-05,
        "epoch": 0.9195916238169759,
        "step": 12340
    },
    {
        "loss": 1.9805,
        "grad_norm": 4.6106276512146,
        "learning_rate": 2.7658485343604656e-05,
        "epoch": 0.9196661450182577,
        "step": 12341
    },
    {
        "loss": 2.3689,
        "grad_norm": 2.5569188594818115,
        "learning_rate": 2.760992186183905e-05,
        "epoch": 0.9197406662195394,
        "step": 12342
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.944964647293091,
        "learning_rate": 2.7561394222771265e-05,
        "epoch": 0.9198151874208212,
        "step": 12343
    },
    {
        "loss": 2.9571,
        "grad_norm": 2.4207584857940674,
        "learning_rate": 2.75129024504292e-05,
        "epoch": 0.919889708622103,
        "step": 12344
    },
    {
        "loss": 2.1085,
        "grad_norm": 3.1967456340789795,
        "learning_rate": 2.746444656882263e-05,
        "epoch": 0.9199642298233848,
        "step": 12345
    },
    {
        "loss": 2.0812,
        "grad_norm": 4.282532691955566,
        "learning_rate": 2.741602660194357e-05,
        "epoch": 0.9200387510246665,
        "step": 12346
    },
    {
        "loss": 2.5071,
        "grad_norm": 1.756617784500122,
        "learning_rate": 2.7367642573766515e-05,
        "epoch": 0.9201132722259483,
        "step": 12347
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.558197498321533,
        "learning_rate": 2.731929450824786e-05,
        "epoch": 0.92018779342723,
        "step": 12348
    },
    {
        "loss": 2.4994,
        "grad_norm": 2.5660154819488525,
        "learning_rate": 2.727098242932623e-05,
        "epoch": 0.9202623146285118,
        "step": 12349
    },
    {
        "loss": 1.4551,
        "grad_norm": 1.6806623935699463,
        "learning_rate": 2.7222706360922723e-05,
        "epoch": 0.9203368358297935,
        "step": 12350
    },
    {
        "loss": 2.1524,
        "grad_norm": 3.5719237327575684,
        "learning_rate": 2.7174466326940283e-05,
        "epoch": 0.9204113570310754,
        "step": 12351
    },
    {
        "loss": 1.8616,
        "grad_norm": 4.147273540496826,
        "learning_rate": 2.7126262351264066e-05,
        "epoch": 0.9204858782323571,
        "step": 12352
    },
    {
        "loss": 2.1704,
        "grad_norm": 3.038027763366699,
        "learning_rate": 2.707809445776168e-05,
        "epoch": 0.9205603994336389,
        "step": 12353
    },
    {
        "loss": 2.3491,
        "grad_norm": 3.945159912109375,
        "learning_rate": 2.7029962670282304e-05,
        "epoch": 0.9206349206349206,
        "step": 12354
    },
    {
        "loss": 2.2153,
        "grad_norm": 3.4259378910064697,
        "learning_rate": 2.6981867012657823e-05,
        "epoch": 0.9207094418362024,
        "step": 12355
    },
    {
        "loss": 2.2283,
        "grad_norm": 3.3778738975524902,
        "learning_rate": 2.693380750870177e-05,
        "epoch": 0.9207839630374841,
        "step": 12356
    },
    {
        "loss": 2.3912,
        "grad_norm": 2.764232635498047,
        "learning_rate": 2.6885784182210162e-05,
        "epoch": 0.9208584842387659,
        "step": 12357
    },
    {
        "loss": 2.4582,
        "grad_norm": 4.758737564086914,
        "learning_rate": 2.6837797056960835e-05,
        "epoch": 0.9209330054400477,
        "step": 12358
    },
    {
        "loss": 1.2634,
        "grad_norm": 2.2045109272003174,
        "learning_rate": 2.6789846156713694e-05,
        "epoch": 0.9210075266413295,
        "step": 12359
    },
    {
        "loss": 2.7223,
        "grad_norm": 1.9273803234100342,
        "learning_rate": 2.6741931505210992e-05,
        "epoch": 0.9210820478426113,
        "step": 12360
    },
    {
        "loss": 1.5374,
        "grad_norm": 2.39213228225708,
        "learning_rate": 2.6694053126176654e-05,
        "epoch": 0.921156569043893,
        "step": 12361
    },
    {
        "loss": 2.7448,
        "grad_norm": 4.666682243347168,
        "learning_rate": 2.6646211043317125e-05,
        "epoch": 0.9212310902451748,
        "step": 12362
    },
    {
        "loss": 2.8226,
        "grad_norm": 2.299859046936035,
        "learning_rate": 2.6598405280320183e-05,
        "epoch": 0.9213056114464565,
        "step": 12363
    },
    {
        "loss": 2.6043,
        "grad_norm": 2.6776041984558105,
        "learning_rate": 2.655063586085632e-05,
        "epoch": 0.9213801326477383,
        "step": 12364
    },
    {
        "loss": 2.2338,
        "grad_norm": 2.3943841457366943,
        "learning_rate": 2.6502902808577647e-05,
        "epoch": 0.92145465384902,
        "step": 12365
    },
    {
        "loss": 2.3159,
        "grad_norm": 3.1610007286071777,
        "learning_rate": 2.6455206147118273e-05,
        "epoch": 0.9215291750503019,
        "step": 12366
    },
    {
        "loss": 1.5253,
        "grad_norm": 2.747546672821045,
        "learning_rate": 2.6407545900094565e-05,
        "epoch": 0.9216036962515836,
        "step": 12367
    },
    {
        "loss": 2.5382,
        "grad_norm": 3.750983953475952,
        "learning_rate": 2.635992209110456e-05,
        "epoch": 0.9216782174528654,
        "step": 12368
    },
    {
        "loss": 2.5445,
        "grad_norm": 2.0196118354797363,
        "learning_rate": 2.6312334743728296e-05,
        "epoch": 0.9217527386541471,
        "step": 12369
    },
    {
        "loss": 2.3697,
        "grad_norm": 5.0469489097595215,
        "learning_rate": 2.6264783881528032e-05,
        "epoch": 0.9218272598554289,
        "step": 12370
    },
    {
        "loss": 2.5361,
        "grad_norm": 3.3593833446502686,
        "learning_rate": 2.6217269528047606e-05,
        "epoch": 0.9219017810567106,
        "step": 12371
    },
    {
        "loss": 2.0043,
        "grad_norm": 3.506063938140869,
        "learning_rate": 2.6169791706812864e-05,
        "epoch": 0.9219763022579924,
        "step": 12372
    },
    {
        "loss": 2.0066,
        "grad_norm": 3.169799327850342,
        "learning_rate": 2.61223504413319e-05,
        "epoch": 0.9220508234592741,
        "step": 12373
    },
    {
        "loss": 2.9636,
        "grad_norm": 2.2587060928344727,
        "learning_rate": 2.6074945755094094e-05,
        "epoch": 0.922125344660556,
        "step": 12374
    },
    {
        "loss": 2.6788,
        "grad_norm": 2.224210262298584,
        "learning_rate": 2.6027577671571314e-05,
        "epoch": 0.9221998658618377,
        "step": 12375
    },
    {
        "loss": 2.5111,
        "grad_norm": 2.0686774253845215,
        "learning_rate": 2.598024621421695e-05,
        "epoch": 0.9222743870631195,
        "step": 12376
    },
    {
        "loss": 3.0077,
        "grad_norm": 2.3016936779022217,
        "learning_rate": 2.593295140646629e-05,
        "epoch": 0.9223489082644012,
        "step": 12377
    },
    {
        "loss": 2.5989,
        "grad_norm": 2.3042235374450684,
        "learning_rate": 2.5885693271736744e-05,
        "epoch": 0.922423429465683,
        "step": 12378
    },
    {
        "loss": 2.4188,
        "grad_norm": 3.724234104156494,
        "learning_rate": 2.5838471833427112e-05,
        "epoch": 0.9224979506669647,
        "step": 12379
    },
    {
        "loss": 2.2689,
        "grad_norm": 3.198559284210205,
        "learning_rate": 2.5791287114918484e-05,
        "epoch": 0.9225724718682465,
        "step": 12380
    },
    {
        "loss": 2.5096,
        "grad_norm": 2.432145833969116,
        "learning_rate": 2.5744139139573486e-05,
        "epoch": 0.9226469930695282,
        "step": 12381
    },
    {
        "loss": 1.8568,
        "grad_norm": 1.856433391571045,
        "learning_rate": 2.5697027930736616e-05,
        "epoch": 0.9227215142708101,
        "step": 12382
    },
    {
        "loss": 1.8782,
        "grad_norm": 3.569795608520508,
        "learning_rate": 2.5649953511734082e-05,
        "epoch": 0.9227960354720918,
        "step": 12383
    },
    {
        "loss": 2.875,
        "grad_norm": 2.395540475845337,
        "learning_rate": 2.5602915905874147e-05,
        "epoch": 0.9228705566733736,
        "step": 12384
    },
    {
        "loss": 2.468,
        "grad_norm": 2.5917134284973145,
        "learning_rate": 2.5555915136446618e-05,
        "epoch": 0.9229450778746553,
        "step": 12385
    },
    {
        "loss": 2.0126,
        "grad_norm": 3.0688512325286865,
        "learning_rate": 2.5508951226723054e-05,
        "epoch": 0.9230195990759371,
        "step": 12386
    },
    {
        "loss": 2.3248,
        "grad_norm": 3.2442450523376465,
        "learning_rate": 2.5462024199956757e-05,
        "epoch": 0.9230941202772188,
        "step": 12387
    },
    {
        "loss": 2.1742,
        "grad_norm": 2.6752915382385254,
        "learning_rate": 2.541513407938303e-05,
        "epoch": 0.9231686414785006,
        "step": 12388
    },
    {
        "loss": 2.6709,
        "grad_norm": 3.0866849422454834,
        "learning_rate": 2.536828088821852e-05,
        "epoch": 0.9232431626797823,
        "step": 12389
    },
    {
        "loss": 2.4164,
        "grad_norm": 2.4778892993927,
        "learning_rate": 2.532146464966194e-05,
        "epoch": 0.9233176838810642,
        "step": 12390
    },
    {
        "loss": 2.4086,
        "grad_norm": 3.3002045154571533,
        "learning_rate": 2.5274685386893472e-05,
        "epoch": 0.9233922050823459,
        "step": 12391
    },
    {
        "loss": 2.959,
        "grad_norm": 2.644864320755005,
        "learning_rate": 2.5227943123074972e-05,
        "epoch": 0.9234667262836277,
        "step": 12392
    },
    {
        "loss": 2.3002,
        "grad_norm": 1.952101707458496,
        "learning_rate": 2.518123788135035e-05,
        "epoch": 0.9235412474849095,
        "step": 12393
    },
    {
        "loss": 1.5639,
        "grad_norm": 3.719305992126465,
        "learning_rate": 2.5134569684844488e-05,
        "epoch": 0.9236157686861912,
        "step": 12394
    },
    {
        "loss": 2.7168,
        "grad_norm": 2.145150899887085,
        "learning_rate": 2.5087938556664647e-05,
        "epoch": 0.923690289887473,
        "step": 12395
    },
    {
        "loss": 2.3179,
        "grad_norm": 2.6521847248077393,
        "learning_rate": 2.5041344519899313e-05,
        "epoch": 0.9237648110887547,
        "step": 12396
    },
    {
        "loss": 2.5366,
        "grad_norm": 2.3577232360839844,
        "learning_rate": 2.4994787597618662e-05,
        "epoch": 0.9238393322900366,
        "step": 12397
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.3635363578796387,
        "learning_rate": 2.4948267812874703e-05,
        "epoch": 0.9239138534913183,
        "step": 12398
    },
    {
        "loss": 2.3339,
        "grad_norm": 2.456500291824341,
        "learning_rate": 2.490178518870073e-05,
        "epoch": 0.9239883746926001,
        "step": 12399
    },
    {
        "loss": 2.5272,
        "grad_norm": 2.865809440612793,
        "learning_rate": 2.4855339748112007e-05,
        "epoch": 0.9240628958938818,
        "step": 12400
    },
    {
        "loss": 2.2389,
        "grad_norm": 3.300565004348755,
        "learning_rate": 2.4808931514105106e-05,
        "epoch": 0.9241374170951636,
        "step": 12401
    },
    {
        "loss": 2.2235,
        "grad_norm": 1.966317057609558,
        "learning_rate": 2.4762560509658226e-05,
        "epoch": 0.9242119382964453,
        "step": 12402
    },
    {
        "loss": 1.6488,
        "grad_norm": 3.039098024368286,
        "learning_rate": 2.471622675773111e-05,
        "epoch": 0.9242864594977271,
        "step": 12403
    },
    {
        "loss": 2.4704,
        "grad_norm": 3.4931087493896484,
        "learning_rate": 2.4669930281265274e-05,
        "epoch": 0.9243609806990088,
        "step": 12404
    },
    {
        "loss": 2.5546,
        "grad_norm": 2.726168155670166,
        "learning_rate": 2.4623671103183555e-05,
        "epoch": 0.9244355019002907,
        "step": 12405
    },
    {
        "loss": 2.4859,
        "grad_norm": 2.318913459777832,
        "learning_rate": 2.4577449246390372e-05,
        "epoch": 0.9245100231015724,
        "step": 12406
    },
    {
        "loss": 2.8554,
        "grad_norm": 1.984460473060608,
        "learning_rate": 2.4531264733771596e-05,
        "epoch": 0.9245845443028542,
        "step": 12407
    },
    {
        "loss": 2.4105,
        "grad_norm": 2.1319797039031982,
        "learning_rate": 2.4485117588194895e-05,
        "epoch": 0.9246590655041359,
        "step": 12408
    },
    {
        "loss": 2.462,
        "grad_norm": 1.8103182315826416,
        "learning_rate": 2.4439007832509087e-05,
        "epoch": 0.9247335867054177,
        "step": 12409
    },
    {
        "loss": 2.3612,
        "grad_norm": 2.443377733230591,
        "learning_rate": 2.4392935489544556e-05,
        "epoch": 0.9248081079066994,
        "step": 12410
    },
    {
        "loss": 0.7556,
        "grad_norm": 2.5101006031036377,
        "learning_rate": 2.4346900582113417e-05,
        "epoch": 0.9248826291079812,
        "step": 12411
    },
    {
        "loss": 2.8174,
        "grad_norm": 2.9082486629486084,
        "learning_rate": 2.430090313300888e-05,
        "epoch": 0.924957150309263,
        "step": 12412
    },
    {
        "loss": 1.6564,
        "grad_norm": 2.536423444747925,
        "learning_rate": 2.4254943165006038e-05,
        "epoch": 0.9250316715105448,
        "step": 12413
    },
    {
        "loss": 2.4087,
        "grad_norm": 2.9901504516601562,
        "learning_rate": 2.420902070086084e-05,
        "epoch": 0.9251061927118265,
        "step": 12414
    },
    {
        "loss": 2.3421,
        "grad_norm": 2.614225387573242,
        "learning_rate": 2.4163135763311294e-05,
        "epoch": 0.9251807139131083,
        "step": 12415
    },
    {
        "loss": 2.3418,
        "grad_norm": 2.534280776977539,
        "learning_rate": 2.4117288375076375e-05,
        "epoch": 0.92525523511439,
        "step": 12416
    },
    {
        "loss": 2.5628,
        "grad_norm": 3.0801162719726562,
        "learning_rate": 2.4071478558856585e-05,
        "epoch": 0.9253297563156718,
        "step": 12417
    },
    {
        "loss": 2.7589,
        "grad_norm": 2.669938564300537,
        "learning_rate": 2.402570633733403e-05,
        "epoch": 0.9254042775169535,
        "step": 12418
    },
    {
        "loss": 2.5511,
        "grad_norm": 2.3062117099761963,
        "learning_rate": 2.3979971733171947e-05,
        "epoch": 0.9254787987182354,
        "step": 12419
    },
    {
        "loss": 1.9583,
        "grad_norm": 4.2629899978637695,
        "learning_rate": 2.3934274769014963e-05,
        "epoch": 0.9255533199195171,
        "step": 12420
    },
    {
        "loss": 2.393,
        "grad_norm": 2.2371954917907715,
        "learning_rate": 2.3888615467489296e-05,
        "epoch": 0.9256278411207989,
        "step": 12421
    },
    {
        "loss": 2.6283,
        "grad_norm": 2.3886337280273438,
        "learning_rate": 2.3842993851202277e-05,
        "epoch": 0.9257023623220806,
        "step": 12422
    },
    {
        "loss": 2.3094,
        "grad_norm": 2.0978541374206543,
        "learning_rate": 2.3797409942742586e-05,
        "epoch": 0.9257768835233624,
        "step": 12423
    },
    {
        "loss": 2.7284,
        "grad_norm": 3.937213897705078,
        "learning_rate": 2.3751863764680583e-05,
        "epoch": 0.9258514047246441,
        "step": 12424
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.0737528800964355,
        "learning_rate": 2.3706355339567286e-05,
        "epoch": 0.9259259259259259,
        "step": 12425
    },
    {
        "loss": 2.703,
        "grad_norm": 1.9790507555007935,
        "learning_rate": 2.3660884689935693e-05,
        "epoch": 0.9260004471272076,
        "step": 12426
    },
    {
        "loss": 2.2908,
        "grad_norm": 3.0901296138763428,
        "learning_rate": 2.3615451838299618e-05,
        "epoch": 0.9260749683284895,
        "step": 12427
    },
    {
        "loss": 2.3953,
        "grad_norm": 2.2489407062530518,
        "learning_rate": 2.357005680715454e-05,
        "epoch": 0.9261494895297713,
        "step": 12428
    },
    {
        "loss": 2.5588,
        "grad_norm": 2.532888412475586,
        "learning_rate": 2.3524699618976942e-05,
        "epoch": 0.926224010731053,
        "step": 12429
    },
    {
        "loss": 2.7781,
        "grad_norm": 3.085207462310791,
        "learning_rate": 2.3479380296224562e-05,
        "epoch": 0.9262985319323348,
        "step": 12430
    },
    {
        "loss": 1.9191,
        "grad_norm": 3.6684017181396484,
        "learning_rate": 2.3434098861336695e-05,
        "epoch": 0.9263730531336165,
        "step": 12431
    },
    {
        "loss": 2.288,
        "grad_norm": 6.694941520690918,
        "learning_rate": 2.3388855336733407e-05,
        "epoch": 0.9264475743348983,
        "step": 12432
    },
    {
        "loss": 1.3397,
        "grad_norm": 4.062027931213379,
        "learning_rate": 2.3343649744816554e-05,
        "epoch": 0.92652209553618,
        "step": 12433
    },
    {
        "loss": 2.5586,
        "grad_norm": 2.243971109390259,
        "learning_rate": 2.3298482107968557e-05,
        "epoch": 0.9265966167374619,
        "step": 12434
    },
    {
        "loss": 1.6324,
        "grad_norm": 2.1065564155578613,
        "learning_rate": 2.3253352448553645e-05,
        "epoch": 0.9266711379387436,
        "step": 12435
    },
    {
        "loss": 2.8712,
        "grad_norm": 3.1089534759521484,
        "learning_rate": 2.3208260788916902e-05,
        "epoch": 0.9267456591400254,
        "step": 12436
    },
    {
        "loss": 3.1229,
        "grad_norm": 2.2683558464050293,
        "learning_rate": 2.3163207151384613e-05,
        "epoch": 0.9268201803413071,
        "step": 12437
    },
    {
        "loss": 2.5099,
        "grad_norm": 1.69210684299469,
        "learning_rate": 2.31181915582645e-05,
        "epoch": 0.9268947015425889,
        "step": 12438
    },
    {
        "loss": 2.7114,
        "grad_norm": 2.780423641204834,
        "learning_rate": 2.3073214031845115e-05,
        "epoch": 0.9269692227438706,
        "step": 12439
    },
    {
        "loss": 2.3938,
        "grad_norm": 2.857548952102661,
        "learning_rate": 2.302827459439625e-05,
        "epoch": 0.9270437439451524,
        "step": 12440
    },
    {
        "loss": 2.1461,
        "grad_norm": 3.2597944736480713,
        "learning_rate": 2.2983373268169063e-05,
        "epoch": 0.9271182651464341,
        "step": 12441
    },
    {
        "loss": 2.4852,
        "grad_norm": 2.775399684906006,
        "learning_rate": 2.293851007539558e-05,
        "epoch": 0.927192786347716,
        "step": 12442
    },
    {
        "loss": 2.7846,
        "grad_norm": 2.6466779708862305,
        "learning_rate": 2.289368503828895e-05,
        "epoch": 0.9272673075489977,
        "step": 12443
    },
    {
        "loss": 2.3288,
        "grad_norm": 2.8065717220306396,
        "learning_rate": 2.2848898179043777e-05,
        "epoch": 0.9273418287502795,
        "step": 12444
    },
    {
        "loss": 2.5987,
        "grad_norm": 2.4069414138793945,
        "learning_rate": 2.280414951983516e-05,
        "epoch": 0.9274163499515612,
        "step": 12445
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.8536081314086914,
        "learning_rate": 2.275943908281992e-05,
        "epoch": 0.927490871152843,
        "step": 12446
    },
    {
        "loss": 2.6603,
        "grad_norm": 2.8775479793548584,
        "learning_rate": 2.2714766890135498e-05,
        "epoch": 0.9275653923541247,
        "step": 12447
    },
    {
        "loss": 1.5783,
        "grad_norm": 1.6062757968902588,
        "learning_rate": 2.2670132963900515e-05,
        "epoch": 0.9276399135554065,
        "step": 12448
    },
    {
        "loss": 2.8245,
        "grad_norm": 1.674628734588623,
        "learning_rate": 2.262553732621485e-05,
        "epoch": 0.9277144347566882,
        "step": 12449
    },
    {
        "loss": 2.1456,
        "grad_norm": 2.057225465774536,
        "learning_rate": 2.25809799991591e-05,
        "epoch": 0.9277889559579701,
        "step": 12450
    },
    {
        "loss": 2.1947,
        "grad_norm": 2.9868924617767334,
        "learning_rate": 2.253646100479523e-05,
        "epoch": 0.9278634771592518,
        "step": 12451
    },
    {
        "loss": 2.2244,
        "grad_norm": 3.093111515045166,
        "learning_rate": 2.2491980365165956e-05,
        "epoch": 0.9279379983605336,
        "step": 12452
    },
    {
        "loss": 2.2999,
        "grad_norm": 3.232330560684204,
        "learning_rate": 2.244753810229514e-05,
        "epoch": 0.9280125195618153,
        "step": 12453
    },
    {
        "loss": 2.3393,
        "grad_norm": 3.644890546798706,
        "learning_rate": 2.2403134238187452e-05,
        "epoch": 0.9280870407630971,
        "step": 12454
    },
    {
        "loss": 3.0526,
        "grad_norm": 3.5746355056762695,
        "learning_rate": 2.235876879482889e-05,
        "epoch": 0.9281615619643788,
        "step": 12455
    },
    {
        "loss": 2.2779,
        "grad_norm": 3.559999942779541,
        "learning_rate": 2.231444179418617e-05,
        "epoch": 0.9282360831656606,
        "step": 12456
    },
    {
        "loss": 2.3036,
        "grad_norm": 2.6242916584014893,
        "learning_rate": 2.2270153258207027e-05,
        "epoch": 0.9283106043669423,
        "step": 12457
    },
    {
        "loss": 2.2175,
        "grad_norm": 2.1477935314178467,
        "learning_rate": 2.2225903208820098e-05,
        "epoch": 0.9283851255682242,
        "step": 12458
    },
    {
        "loss": 2.1579,
        "grad_norm": 2.9347312450408936,
        "learning_rate": 2.218169166793518e-05,
        "epoch": 0.9284596467695059,
        "step": 12459
    },
    {
        "loss": 2.0303,
        "grad_norm": 2.709857940673828,
        "learning_rate": 2.2137518657442703e-05,
        "epoch": 0.9285341679707877,
        "step": 12460
    },
    {
        "loss": 2.1931,
        "grad_norm": 3.0812160968780518,
        "learning_rate": 2.209338419921435e-05,
        "epoch": 0.9286086891720694,
        "step": 12461
    },
    {
        "loss": 3.6393,
        "grad_norm": 5.237534999847412,
        "learning_rate": 2.2049288315102412e-05,
        "epoch": 0.9286832103733512,
        "step": 12462
    },
    {
        "loss": 2.2542,
        "grad_norm": 2.2305800914764404,
        "learning_rate": 2.2005231026940132e-05,
        "epoch": 0.9287577315746329,
        "step": 12463
    },
    {
        "loss": 2.0279,
        "grad_norm": 3.9078738689422607,
        "learning_rate": 2.1961212356541994e-05,
        "epoch": 0.9288322527759147,
        "step": 12464
    },
    {
        "loss": 3.166,
        "grad_norm": 3.377953290939331,
        "learning_rate": 2.191723232570272e-05,
        "epoch": 0.9289067739771966,
        "step": 12465
    },
    {
        "loss": 1.246,
        "grad_norm": 3.216553211212158,
        "learning_rate": 2.1873290956198532e-05,
        "epoch": 0.9289812951784783,
        "step": 12466
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.505610942840576,
        "learning_rate": 2.1829388269786155e-05,
        "epoch": 0.9290558163797601,
        "step": 12467
    },
    {
        "loss": 1.9019,
        "grad_norm": 3.0329105854034424,
        "learning_rate": 2.1785524288203152e-05,
        "epoch": 0.9291303375810418,
        "step": 12468
    },
    {
        "loss": 2.4179,
        "grad_norm": 2.574610471725464,
        "learning_rate": 2.1741699033168227e-05,
        "epoch": 0.9292048587823236,
        "step": 12469
    },
    {
        "loss": 1.9827,
        "grad_norm": 3.4719839096069336,
        "learning_rate": 2.169791252638055e-05,
        "epoch": 0.9292793799836053,
        "step": 12470
    },
    {
        "loss": 2.4101,
        "grad_norm": 3.449836254119873,
        "learning_rate": 2.165416478952019e-05,
        "epoch": 0.9293539011848871,
        "step": 12471
    },
    {
        "loss": 1.9822,
        "grad_norm": 3.5126771926879883,
        "learning_rate": 2.161045584424828e-05,
        "epoch": 0.9294284223861689,
        "step": 12472
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.268820285797119,
        "learning_rate": 2.1566785712206438e-05,
        "epoch": 0.9295029435874507,
        "step": 12473
    },
    {
        "loss": 1.9584,
        "grad_norm": 2.7640604972839355,
        "learning_rate": 2.15231544150171e-05,
        "epoch": 0.9295774647887324,
        "step": 12474
    },
    {
        "loss": 1.5972,
        "grad_norm": 2.3553476333618164,
        "learning_rate": 2.1479561974283835e-05,
        "epoch": 0.9296519859900142,
        "step": 12475
    },
    {
        "loss": 2.378,
        "grad_norm": 4.101287364959717,
        "learning_rate": 2.1436008411590292e-05,
        "epoch": 0.9297265071912959,
        "step": 12476
    },
    {
        "loss": 1.8577,
        "grad_norm": 4.322486400604248,
        "learning_rate": 2.139249374850155e-05,
        "epoch": 0.9298010283925777,
        "step": 12477
    },
    {
        "loss": 1.9074,
        "grad_norm": 3.1987688541412354,
        "learning_rate": 2.134901800656297e-05,
        "epoch": 0.9298755495938594,
        "step": 12478
    },
    {
        "loss": 2.0919,
        "grad_norm": 3.282724380493164,
        "learning_rate": 2.1305581207300983e-05,
        "epoch": 0.9299500707951412,
        "step": 12479
    },
    {
        "loss": 2.2712,
        "grad_norm": 2.8009238243103027,
        "learning_rate": 2.12621833722225e-05,
        "epoch": 0.930024591996423,
        "step": 12480
    },
    {
        "loss": 1.9297,
        "grad_norm": 2.5335023403167725,
        "learning_rate": 2.1218824522815105e-05,
        "epoch": 0.9300991131977048,
        "step": 12481
    },
    {
        "loss": 2.4852,
        "grad_norm": 3.2771501541137695,
        "learning_rate": 2.1175504680547363e-05,
        "epoch": 0.9301736343989865,
        "step": 12482
    },
    {
        "loss": 2.6639,
        "grad_norm": 2.245483160018921,
        "learning_rate": 2.1132223866868182e-05,
        "epoch": 0.9302481556002683,
        "step": 12483
    },
    {
        "loss": 2.0756,
        "grad_norm": 2.5109610557556152,
        "learning_rate": 2.1088982103207588e-05,
        "epoch": 0.93032267680155,
        "step": 12484
    },
    {
        "loss": 2.7114,
        "grad_norm": 1.9348522424697876,
        "learning_rate": 2.1045779410975595e-05,
        "epoch": 0.9303971980028318,
        "step": 12485
    },
    {
        "loss": 2.6246,
        "grad_norm": 3.380657196044922,
        "learning_rate": 2.1002615811563563e-05,
        "epoch": 0.9304717192041135,
        "step": 12486
    },
    {
        "loss": 1.5947,
        "grad_norm": 3.762464761734009,
        "learning_rate": 2.0959491326343106e-05,
        "epoch": 0.9305462404053954,
        "step": 12487
    },
    {
        "loss": 2.621,
        "grad_norm": 3.93746280670166,
        "learning_rate": 2.0916405976666497e-05,
        "epoch": 0.9306207616066771,
        "step": 12488
    },
    {
        "loss": 1.6104,
        "grad_norm": 3.637047052383423,
        "learning_rate": 2.087335978386685e-05,
        "epoch": 0.9306952828079589,
        "step": 12489
    },
    {
        "loss": 2.5224,
        "grad_norm": 2.496751308441162,
        "learning_rate": 2.08303527692577e-05,
        "epoch": 0.9307698040092406,
        "step": 12490
    },
    {
        "loss": 2.5466,
        "grad_norm": 2.535337209701538,
        "learning_rate": 2.0787384954133128e-05,
        "epoch": 0.9308443252105224,
        "step": 12491
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.6036226749420166,
        "learning_rate": 2.074445635976805e-05,
        "epoch": 0.9309188464118041,
        "step": 12492
    },
    {
        "loss": 1.6859,
        "grad_norm": 3.3996987342834473,
        "learning_rate": 2.0701567007417744e-05,
        "epoch": 0.9309933676130859,
        "step": 12493
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.438671827316284,
        "learning_rate": 2.0658716918318054e-05,
        "epoch": 0.9310678888143676,
        "step": 12494
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.164475679397583,
        "learning_rate": 2.0615906113685713e-05,
        "epoch": 0.9311424100156495,
        "step": 12495
    },
    {
        "loss": 2.4344,
        "grad_norm": 2.3366565704345703,
        "learning_rate": 2.0573134614717425e-05,
        "epoch": 0.9312169312169312,
        "step": 12496
    },
    {
        "loss": 2.5913,
        "grad_norm": 3.013183355331421,
        "learning_rate": 2.0530402442591013e-05,
        "epoch": 0.931291452418213,
        "step": 12497
    },
    {
        "loss": 2.0664,
        "grad_norm": 3.392834186553955,
        "learning_rate": 2.048770961846439e-05,
        "epoch": 0.9313659736194947,
        "step": 12498
    },
    {
        "loss": 2.7033,
        "grad_norm": 5.07661247253418,
        "learning_rate": 2.0445056163476374e-05,
        "epoch": 0.9314404948207765,
        "step": 12499
    },
    {
        "loss": 2.63,
        "grad_norm": 2.9874155521392822,
        "learning_rate": 2.0402442098745954e-05,
        "epoch": 0.9315150160220583,
        "step": 12500
    },
    {
        "loss": 3.0754,
        "grad_norm": 2.6213290691375732,
        "learning_rate": 2.0359867445372693e-05,
        "epoch": 0.93158953722334,
        "step": 12501
    },
    {
        "loss": 2.5183,
        "grad_norm": 2.964005708694458,
        "learning_rate": 2.0317332224436847e-05,
        "epoch": 0.9316640584246219,
        "step": 12502
    },
    {
        "loss": 1.933,
        "grad_norm": 3.1302671432495117,
        "learning_rate": 2.0274836456998937e-05,
        "epoch": 0.9317385796259036,
        "step": 12503
    },
    {
        "loss": 1.6439,
        "grad_norm": 3.504784107208252,
        "learning_rate": 2.0232380164099984e-05,
        "epoch": 0.9318131008271854,
        "step": 12504
    },
    {
        "loss": 2.3931,
        "grad_norm": 2.990603446960449,
        "learning_rate": 2.0189963366761434e-05,
        "epoch": 0.9318876220284671,
        "step": 12505
    },
    {
        "loss": 1.5007,
        "grad_norm": 2.727753162384033,
        "learning_rate": 2.0147586085985403e-05,
        "epoch": 0.9319621432297489,
        "step": 12506
    },
    {
        "loss": 1.165,
        "grad_norm": 4.102535724639893,
        "learning_rate": 2.0105248342754135e-05,
        "epoch": 0.9320366644310306,
        "step": 12507
    },
    {
        "loss": 2.5023,
        "grad_norm": 1.992881417274475,
        "learning_rate": 2.006295015803047e-05,
        "epoch": 0.9321111856323124,
        "step": 12508
    },
    {
        "loss": 2.3741,
        "grad_norm": 2.6350338459014893,
        "learning_rate": 2.002069155275753e-05,
        "epoch": 0.9321857068335941,
        "step": 12509
    },
    {
        "loss": 2.4702,
        "grad_norm": 2.2071924209594727,
        "learning_rate": 1.9978472547859117e-05,
        "epoch": 0.932260228034876,
        "step": 12510
    },
    {
        "loss": 2.361,
        "grad_norm": 2.785184144973755,
        "learning_rate": 1.993629316423906e-05,
        "epoch": 0.9323347492361577,
        "step": 12511
    },
    {
        "loss": 2.5686,
        "grad_norm": 1.497928261756897,
        "learning_rate": 1.9894153422781925e-05,
        "epoch": 0.9324092704374395,
        "step": 12512
    },
    {
        "loss": 1.9683,
        "grad_norm": 4.126983165740967,
        "learning_rate": 1.9852053344352396e-05,
        "epoch": 0.9324837916387212,
        "step": 12513
    },
    {
        "loss": 2.7485,
        "grad_norm": 4.012319087982178,
        "learning_rate": 1.9809992949795543e-05,
        "epoch": 0.932558312840003,
        "step": 12514
    },
    {
        "loss": 3.1061,
        "grad_norm": 3.468980550765991,
        "learning_rate": 1.976797225993704e-05,
        "epoch": 0.9326328340412847,
        "step": 12515
    },
    {
        "loss": 2.5844,
        "grad_norm": 1.8500940799713135,
        "learning_rate": 1.9725991295582402e-05,
        "epoch": 0.9327073552425665,
        "step": 12516
    },
    {
        "loss": 3.0413,
        "grad_norm": 3.3533949851989746,
        "learning_rate": 1.9684050077518036e-05,
        "epoch": 0.9327818764438482,
        "step": 12517
    },
    {
        "loss": 2.6958,
        "grad_norm": 2.1814842224121094,
        "learning_rate": 1.9642148626510306e-05,
        "epoch": 0.9328563976451301,
        "step": 12518
    },
    {
        "loss": 2.1652,
        "grad_norm": 2.9939217567443848,
        "learning_rate": 1.9600286963305937e-05,
        "epoch": 0.9329309188464118,
        "step": 12519
    },
    {
        "loss": 2.4795,
        "grad_norm": 3.372995376586914,
        "learning_rate": 1.9558465108632152e-05,
        "epoch": 0.9330054400476936,
        "step": 12520
    },
    {
        "loss": 2.3656,
        "grad_norm": 4.290268898010254,
        "learning_rate": 1.9516683083196175e-05,
        "epoch": 0.9330799612489753,
        "step": 12521
    },
    {
        "loss": 1.6614,
        "grad_norm": 2.8438050746917725,
        "learning_rate": 1.9474940907685824e-05,
        "epoch": 0.9331544824502571,
        "step": 12522
    },
    {
        "loss": 1.4273,
        "grad_norm": 1.2879812717437744,
        "learning_rate": 1.94332386027689e-05,
        "epoch": 0.9332290036515388,
        "step": 12523
    },
    {
        "loss": 2.9677,
        "grad_norm": 5.058930397033691,
        "learning_rate": 1.9391576189093607e-05,
        "epoch": 0.9333035248528206,
        "step": 12524
    },
    {
        "loss": 1.9686,
        "grad_norm": 2.320925235748291,
        "learning_rate": 1.934995368728828e-05,
        "epoch": 0.9333780460541024,
        "step": 12525
    },
    {
        "loss": 2.854,
        "grad_norm": 2.397982597351074,
        "learning_rate": 1.9308371117961742e-05,
        "epoch": 0.9334525672553842,
        "step": 12526
    },
    {
        "loss": 2.1116,
        "grad_norm": 3.062359571456909,
        "learning_rate": 1.9266828501702805e-05,
        "epoch": 0.9335270884566659,
        "step": 12527
    },
    {
        "loss": 2.0485,
        "grad_norm": 3.186971664428711,
        "learning_rate": 1.9225325859080624e-05,
        "epoch": 0.9336016096579477,
        "step": 12528
    },
    {
        "loss": 1.9371,
        "grad_norm": 3.7922556400299072,
        "learning_rate": 1.9183863210644358e-05,
        "epoch": 0.9336761308592294,
        "step": 12529
    },
    {
        "loss": 1.9579,
        "grad_norm": 4.318365097045898,
        "learning_rate": 1.9142440576923716e-05,
        "epoch": 0.9337506520605112,
        "step": 12530
    },
    {
        "loss": 2.6264,
        "grad_norm": 1.943912148475647,
        "learning_rate": 1.910105797842825e-05,
        "epoch": 0.9338251732617929,
        "step": 12531
    },
    {
        "loss": 3.1889,
        "grad_norm": 4.110898017883301,
        "learning_rate": 1.9059715435647985e-05,
        "epoch": 0.9338996944630747,
        "step": 12532
    },
    {
        "loss": 2.6792,
        "grad_norm": 3.5129339694976807,
        "learning_rate": 1.9018412969052902e-05,
        "epoch": 0.9339742156643565,
        "step": 12533
    },
    {
        "loss": 2.8889,
        "grad_norm": 2.5243985652923584,
        "learning_rate": 1.8977150599093117e-05,
        "epoch": 0.9340487368656383,
        "step": 12534
    },
    {
        "loss": 2.8988,
        "grad_norm": 2.1908111572265625,
        "learning_rate": 1.8935928346199217e-05,
        "epoch": 0.9341232580669201,
        "step": 12535
    },
    {
        "loss": 2.5544,
        "grad_norm": 2.4125847816467285,
        "learning_rate": 1.889474623078138e-05,
        "epoch": 0.9341977792682018,
        "step": 12536
    },
    {
        "loss": 2.3901,
        "grad_norm": 1.9703744649887085,
        "learning_rate": 1.8853604273230473e-05,
        "epoch": 0.9342723004694836,
        "step": 12537
    },
    {
        "loss": 2.2637,
        "grad_norm": 2.1800293922424316,
        "learning_rate": 1.8812502493917094e-05,
        "epoch": 0.9343468216707653,
        "step": 12538
    },
    {
        "loss": 1.9551,
        "grad_norm": 3.4438388347625732,
        "learning_rate": 1.8771440913192052e-05,
        "epoch": 0.9344213428720471,
        "step": 12539
    },
    {
        "loss": 2.6055,
        "grad_norm": 2.2952842712402344,
        "learning_rate": 1.8730419551386402e-05,
        "epoch": 0.9344958640733289,
        "step": 12540
    },
    {
        "loss": 2.4112,
        "grad_norm": 2.37632417678833,
        "learning_rate": 1.8689438428811112e-05,
        "epoch": 0.9345703852746107,
        "step": 12541
    },
    {
        "loss": 2.5382,
        "grad_norm": 2.399709939956665,
        "learning_rate": 1.8648497565757183e-05,
        "epoch": 0.9346449064758924,
        "step": 12542
    },
    {
        "loss": 2.4536,
        "grad_norm": 2.327099084854126,
        "learning_rate": 1.8607596982495945e-05,
        "epoch": 0.9347194276771742,
        "step": 12543
    },
    {
        "loss": 2.0912,
        "grad_norm": 3.0925490856170654,
        "learning_rate": 1.8566736699278554e-05,
        "epoch": 0.9347939488784559,
        "step": 12544
    },
    {
        "loss": 2.707,
        "grad_norm": 1.6456599235534668,
        "learning_rate": 1.8525916736336145e-05,
        "epoch": 0.9348684700797377,
        "step": 12545
    },
    {
        "loss": 2.214,
        "grad_norm": 3.1487436294555664,
        "learning_rate": 1.848513711388029e-05,
        "epoch": 0.9349429912810194,
        "step": 12546
    },
    {
        "loss": 2.6968,
        "grad_norm": 2.9317309856414795,
        "learning_rate": 1.844439785210199e-05,
        "epoch": 0.9350175124823012,
        "step": 12547
    },
    {
        "loss": 2.7624,
        "grad_norm": 1.852148413658142,
        "learning_rate": 1.840369897117282e-05,
        "epoch": 0.935092033683583,
        "step": 12548
    },
    {
        "loss": 2.1206,
        "grad_norm": 2.3099517822265625,
        "learning_rate": 1.8363040491243998e-05,
        "epoch": 0.9351665548848648,
        "step": 12549
    },
    {
        "loss": 1.8797,
        "grad_norm": 2.5776569843292236,
        "learning_rate": 1.8322422432446996e-05,
        "epoch": 0.9352410760861465,
        "step": 12550
    },
    {
        "loss": 2.9997,
        "grad_norm": 2.4830074310302734,
        "learning_rate": 1.828184481489309e-05,
        "epoch": 0.9353155972874283,
        "step": 12551
    },
    {
        "loss": 2.094,
        "grad_norm": 2.6510233879089355,
        "learning_rate": 1.824130765867349e-05,
        "epoch": 0.93539011848871,
        "step": 12552
    },
    {
        "loss": 2.2525,
        "grad_norm": 2.0657384395599365,
        "learning_rate": 1.8200810983859618e-05,
        "epoch": 0.9354646396899918,
        "step": 12553
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.932107448577881,
        "learning_rate": 1.816035481050257e-05,
        "epoch": 0.9355391608912735,
        "step": 12554
    },
    {
        "loss": 2.502,
        "grad_norm": 3.449204206466675,
        "learning_rate": 1.8119939158633736e-05,
        "epoch": 0.9356136820925554,
        "step": 12555
    },
    {
        "loss": 2.118,
        "grad_norm": 4.47808837890625,
        "learning_rate": 1.807956404826393e-05,
        "epoch": 0.9356882032938371,
        "step": 12556
    },
    {
        "loss": 2.7689,
        "grad_norm": 4.086397171020508,
        "learning_rate": 1.8039229499384436e-05,
        "epoch": 0.9357627244951189,
        "step": 12557
    },
    {
        "loss": 2.2745,
        "grad_norm": 3.507402181625366,
        "learning_rate": 1.79989355319661e-05,
        "epoch": 0.9358372456964006,
        "step": 12558
    },
    {
        "loss": 2.095,
        "grad_norm": 1.8036195039749146,
        "learning_rate": 1.795868216595976e-05,
        "epoch": 0.9359117668976824,
        "step": 12559
    },
    {
        "loss": 2.404,
        "grad_norm": 3.2867980003356934,
        "learning_rate": 1.791846942129628e-05,
        "epoch": 0.9359862880989641,
        "step": 12560
    },
    {
        "loss": 1.656,
        "grad_norm": 3.578291654586792,
        "learning_rate": 1.7878297317886227e-05,
        "epoch": 0.9360608093002459,
        "step": 12561
    },
    {
        "loss": 2.7385,
        "grad_norm": 2.2234063148498535,
        "learning_rate": 1.7838165875620085e-05,
        "epoch": 0.9361353305015276,
        "step": 12562
    },
    {
        "loss": 2.8165,
        "grad_norm": 2.5021286010742188,
        "learning_rate": 1.7798075114368364e-05,
        "epoch": 0.9362098517028095,
        "step": 12563
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.275632381439209,
        "learning_rate": 1.7758025053981264e-05,
        "epoch": 0.9362843729040912,
        "step": 12564
    },
    {
        "loss": 2.4212,
        "grad_norm": 1.7135238647460938,
        "learning_rate": 1.7718015714288783e-05,
        "epoch": 0.936358894105373,
        "step": 12565
    },
    {
        "loss": 2.0541,
        "grad_norm": 2.5110225677490234,
        "learning_rate": 1.7678047115101126e-05,
        "epoch": 0.9364334153066547,
        "step": 12566
    },
    {
        "loss": 1.6461,
        "grad_norm": 4.092787742614746,
        "learning_rate": 1.7638119276207688e-05,
        "epoch": 0.9365079365079365,
        "step": 12567
    },
    {
        "loss": 2.456,
        "grad_norm": 2.6432032585144043,
        "learning_rate": 1.759823221737832e-05,
        "epoch": 0.9365824577092182,
        "step": 12568
    },
    {
        "loss": 2.4182,
        "grad_norm": 2.976059913635254,
        "learning_rate": 1.7558385958362313e-05,
        "epoch": 0.9366569789105,
        "step": 12569
    },
    {
        "loss": 2.9658,
        "grad_norm": 3.5346524715423584,
        "learning_rate": 1.7518580518888782e-05,
        "epoch": 0.9367315001117819,
        "step": 12570
    },
    {
        "loss": 1.732,
        "grad_norm": 3.2221622467041016,
        "learning_rate": 1.747881591866687e-05,
        "epoch": 0.9368060213130636,
        "step": 12571
    },
    {
        "loss": 2.213,
        "grad_norm": 2.4012532234191895,
        "learning_rate": 1.7439092177385174e-05,
        "epoch": 0.9368805425143454,
        "step": 12572
    },
    {
        "loss": 1.727,
        "grad_norm": 4.518011093139648,
        "learning_rate": 1.7399409314712367e-05,
        "epoch": 0.9369550637156271,
        "step": 12573
    },
    {
        "loss": 2.7807,
        "grad_norm": 2.0811927318573,
        "learning_rate": 1.7359767350296685e-05,
        "epoch": 0.9370295849169089,
        "step": 12574
    },
    {
        "loss": 2.3799,
        "grad_norm": 4.321964263916016,
        "learning_rate": 1.7320166303766118e-05,
        "epoch": 0.9371041061181906,
        "step": 12575
    },
    {
        "loss": 2.2123,
        "grad_norm": 3.184253692626953,
        "learning_rate": 1.7280606194728377e-05,
        "epoch": 0.9371786273194724,
        "step": 12576
    },
    {
        "loss": 2.8464,
        "grad_norm": 2.2808682918548584,
        "learning_rate": 1.724108704277113e-05,
        "epoch": 0.9372531485207541,
        "step": 12577
    },
    {
        "loss": 2.8066,
        "grad_norm": 2.7927825450897217,
        "learning_rate": 1.7201608867461515e-05,
        "epoch": 0.937327669722036,
        "step": 12578
    },
    {
        "loss": 1.8387,
        "grad_norm": 3.586061477661133,
        "learning_rate": 1.7162171688346495e-05,
        "epoch": 0.9374021909233177,
        "step": 12579
    },
    {
        "loss": 2.7236,
        "grad_norm": 3.996671199798584,
        "learning_rate": 1.7122775524952606e-05,
        "epoch": 0.9374767121245995,
        "step": 12580
    },
    {
        "loss": 3.0707,
        "grad_norm": 3.576106548309326,
        "learning_rate": 1.708342039678634e-05,
        "epoch": 0.9375512333258812,
        "step": 12581
    },
    {
        "loss": 2.3075,
        "grad_norm": 2.55403995513916,
        "learning_rate": 1.7044106323333585e-05,
        "epoch": 0.937625754527163,
        "step": 12582
    },
    {
        "loss": 2.7714,
        "grad_norm": 2.2435712814331055,
        "learning_rate": 1.7004833324060132e-05,
        "epoch": 0.9377002757284447,
        "step": 12583
    },
    {
        "loss": 2.3757,
        "grad_norm": 3.301262617111206,
        "learning_rate": 1.696560141841126e-05,
        "epoch": 0.9377747969297265,
        "step": 12584
    },
    {
        "loss": 1.806,
        "grad_norm": 2.819563627243042,
        "learning_rate": 1.6926410625811918e-05,
        "epoch": 0.9378493181310082,
        "step": 12585
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.424960136413574,
        "learning_rate": 1.6887260965666963e-05,
        "epoch": 0.9379238393322901,
        "step": 12586
    },
    {
        "loss": 2.4856,
        "grad_norm": 3.285618543624878,
        "learning_rate": 1.6848152457360367e-05,
        "epoch": 0.9379983605335718,
        "step": 12587
    },
    {
        "loss": 2.2462,
        "grad_norm": 4.007180690765381,
        "learning_rate": 1.6809085120256273e-05,
        "epoch": 0.9380728817348536,
        "step": 12588
    },
    {
        "loss": 2.0584,
        "grad_norm": 3.675180196762085,
        "learning_rate": 1.6770058973698145e-05,
        "epoch": 0.9381474029361353,
        "step": 12589
    },
    {
        "loss": 2.9549,
        "grad_norm": 2.6466801166534424,
        "learning_rate": 1.6731074037008976e-05,
        "epoch": 0.9382219241374171,
        "step": 12590
    },
    {
        "loss": 2.2718,
        "grad_norm": 3.377567768096924,
        "learning_rate": 1.669213032949166e-05,
        "epoch": 0.9382964453386988,
        "step": 12591
    },
    {
        "loss": 2.0043,
        "grad_norm": 4.170475482940674,
        "learning_rate": 1.6653227870428355e-05,
        "epoch": 0.9383709665399806,
        "step": 12592
    },
    {
        "loss": 3.0115,
        "grad_norm": 2.85311222076416,
        "learning_rate": 1.6614366679081095e-05,
        "epoch": 0.9384454877412624,
        "step": 12593
    },
    {
        "loss": 3.0758,
        "grad_norm": 2.914102554321289,
        "learning_rate": 1.657554677469124e-05,
        "epoch": 0.9385200089425442,
        "step": 12594
    },
    {
        "loss": 2.1099,
        "grad_norm": 2.0514779090881348,
        "learning_rate": 1.6536768176479812e-05,
        "epoch": 0.9385945301438259,
        "step": 12595
    },
    {
        "loss": 2.4962,
        "grad_norm": 2.0018932819366455,
        "learning_rate": 1.6498030903647277e-05,
        "epoch": 0.9386690513451077,
        "step": 12596
    },
    {
        "loss": 2.9456,
        "grad_norm": 2.955876350402832,
        "learning_rate": 1.6459334975373987e-05,
        "epoch": 0.9387435725463894,
        "step": 12597
    },
    {
        "loss": 2.574,
        "grad_norm": 3.060516357421875,
        "learning_rate": 1.642068041081921e-05,
        "epoch": 0.9388180937476712,
        "step": 12598
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.2731196880340576,
        "learning_rate": 1.6382067229122334e-05,
        "epoch": 0.9388926149489529,
        "step": 12599
    },
    {
        "loss": 3.0328,
        "grad_norm": 2.347231864929199,
        "learning_rate": 1.6343495449401858e-05,
        "epoch": 0.9389671361502347,
        "step": 12600
    },
    {
        "loss": 1.8953,
        "grad_norm": 3.4348626136779785,
        "learning_rate": 1.6304965090756095e-05,
        "epoch": 0.9390416573515165,
        "step": 12601
    },
    {
        "loss": 2.4284,
        "grad_norm": 2.726816177368164,
        "learning_rate": 1.62664761722626e-05,
        "epoch": 0.9391161785527983,
        "step": 12602
    },
    {
        "loss": 2.3318,
        "grad_norm": 4.4345316886901855,
        "learning_rate": 1.6228028712978428e-05,
        "epoch": 0.93919069975408,
        "step": 12603
    },
    {
        "loss": 2.3481,
        "grad_norm": 3.472578287124634,
        "learning_rate": 1.6189622731940335e-05,
        "epoch": 0.9392652209553618,
        "step": 12604
    },
    {
        "loss": 2.7098,
        "grad_norm": 3.687833547592163,
        "learning_rate": 1.61512582481642e-05,
        "epoch": 0.9393397421566436,
        "step": 12605
    },
    {
        "loss": 1.9147,
        "grad_norm": 3.0325639247894287,
        "learning_rate": 1.6112935280645782e-05,
        "epoch": 0.9394142633579253,
        "step": 12606
    },
    {
        "loss": 2.4247,
        "grad_norm": 1.6201472282409668,
        "learning_rate": 1.6074653848359735e-05,
        "epoch": 0.9394887845592071,
        "step": 12607
    },
    {
        "loss": 2.5636,
        "grad_norm": 2.683351755142212,
        "learning_rate": 1.6036413970260666e-05,
        "epoch": 0.9395633057604889,
        "step": 12608
    },
    {
        "loss": 2.419,
        "grad_norm": 2.6196181774139404,
        "learning_rate": 1.5998215665282313e-05,
        "epoch": 0.9396378269617707,
        "step": 12609
    },
    {
        "loss": 1.6292,
        "grad_norm": 3.6800670623779297,
        "learning_rate": 1.5960058952337854e-05,
        "epoch": 0.9397123481630524,
        "step": 12610
    },
    {
        "loss": 2.7152,
        "grad_norm": 2.539292812347412,
        "learning_rate": 1.5921943850320064e-05,
        "epoch": 0.9397868693643342,
        "step": 12611
    },
    {
        "loss": 2.3044,
        "grad_norm": 2.7683465480804443,
        "learning_rate": 1.588387037810092e-05,
        "epoch": 0.9398613905656159,
        "step": 12612
    },
    {
        "loss": 2.1178,
        "grad_norm": 3.141418695449829,
        "learning_rate": 1.5845838554531732e-05,
        "epoch": 0.9399359117668977,
        "step": 12613
    },
    {
        "loss": 3.1134,
        "grad_norm": 3.107877254486084,
        "learning_rate": 1.580784839844348e-05,
        "epoch": 0.9400104329681794,
        "step": 12614
    },
    {
        "loss": 1.9272,
        "grad_norm": 4.078562259674072,
        "learning_rate": 1.5769899928646235e-05,
        "epoch": 0.9400849541694613,
        "step": 12615
    },
    {
        "loss": 2.5399,
        "grad_norm": 3.3884620666503906,
        "learning_rate": 1.573199316392948e-05,
        "epoch": 0.940159475370743,
        "step": 12616
    },
    {
        "loss": 2.6501,
        "grad_norm": 2.137277603149414,
        "learning_rate": 1.5694128123062313e-05,
        "epoch": 0.9402339965720248,
        "step": 12617
    },
    {
        "loss": 1.8117,
        "grad_norm": 3.4431869983673096,
        "learning_rate": 1.5656304824792646e-05,
        "epoch": 0.9403085177733065,
        "step": 12618
    },
    {
        "loss": 2.4575,
        "grad_norm": 2.2335381507873535,
        "learning_rate": 1.5618523287848286e-05,
        "epoch": 0.9403830389745883,
        "step": 12619
    },
    {
        "loss": 2.7891,
        "grad_norm": 1.899242639541626,
        "learning_rate": 1.5580783530935917e-05,
        "epoch": 0.94045756017587,
        "step": 12620
    },
    {
        "loss": 2.7139,
        "grad_norm": 3.6206841468811035,
        "learning_rate": 1.554308557274189e-05,
        "epoch": 0.9405320813771518,
        "step": 12621
    },
    {
        "loss": 1.8553,
        "grad_norm": 2.9339194297790527,
        "learning_rate": 1.550542943193163e-05,
        "epoch": 0.9406066025784335,
        "step": 12622
    },
    {
        "loss": 2.374,
        "grad_norm": 4.890291690826416,
        "learning_rate": 1.5467815127149853e-05,
        "epoch": 0.9406811237797154,
        "step": 12623
    },
    {
        "loss": 2.6757,
        "grad_norm": 1.8994395732879639,
        "learning_rate": 1.543024267702079e-05,
        "epoch": 0.9407556449809971,
        "step": 12624
    },
    {
        "loss": 2.1704,
        "grad_norm": 2.0871055126190186,
        "learning_rate": 1.539271210014762e-05,
        "epoch": 0.9408301661822789,
        "step": 12625
    },
    {
        "loss": 2.5929,
        "grad_norm": 2.567863941192627,
        "learning_rate": 1.5355223415113207e-05,
        "epoch": 0.9409046873835606,
        "step": 12626
    },
    {
        "loss": 1.6788,
        "grad_norm": 4.6589274406433105,
        "learning_rate": 1.531777664047913e-05,
        "epoch": 0.9409792085848424,
        "step": 12627
    },
    {
        "loss": 2.7341,
        "grad_norm": 2.423520565032959,
        "learning_rate": 1.528037179478671e-05,
        "epoch": 0.9410537297861241,
        "step": 12628
    },
    {
        "loss": 2.7609,
        "grad_norm": 2.2714009284973145,
        "learning_rate": 1.5243008896556255e-05,
        "epoch": 0.9411282509874059,
        "step": 12629
    },
    {
        "loss": 3.0801,
        "grad_norm": 3.4735219478607178,
        "learning_rate": 1.5205687964287352e-05,
        "epoch": 0.9412027721886876,
        "step": 12630
    },
    {
        "loss": 2.4698,
        "grad_norm": 3.7200326919555664,
        "learning_rate": 1.516840901645874e-05,
        "epoch": 0.9412772933899695,
        "step": 12631
    },
    {
        "loss": 2.2768,
        "grad_norm": 3.4576849937438965,
        "learning_rate": 1.513117207152861e-05,
        "epoch": 0.9413518145912512,
        "step": 12632
    },
    {
        "loss": 2.605,
        "grad_norm": 4.121382236480713,
        "learning_rate": 1.5093977147934035e-05,
        "epoch": 0.941426335792533,
        "step": 12633
    },
    {
        "loss": 2.0827,
        "grad_norm": 3.005448341369629,
        "learning_rate": 1.5056824264091585e-05,
        "epoch": 0.9415008569938147,
        "step": 12634
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.3508660793304443,
        "learning_rate": 1.5019713438396822e-05,
        "epoch": 0.9415753781950965,
        "step": 12635
    },
    {
        "loss": 2.4685,
        "grad_norm": 3.4231626987457275,
        "learning_rate": 1.4982644689224412e-05,
        "epoch": 0.9416498993963782,
        "step": 12636
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.598129987716675,
        "learning_rate": 1.4945618034928544e-05,
        "epoch": 0.94172442059766,
        "step": 12637
    },
    {
        "loss": 2.055,
        "grad_norm": 3.6909942626953125,
        "learning_rate": 1.490863349384205e-05,
        "epoch": 0.9417989417989417,
        "step": 12638
    },
    {
        "loss": 2.5424,
        "grad_norm": 2.623051643371582,
        "learning_rate": 1.4871691084277383e-05,
        "epoch": 0.9418734630002236,
        "step": 12639
    },
    {
        "loss": 1.7172,
        "grad_norm": 3.5160815715789795,
        "learning_rate": 1.4834790824525879e-05,
        "epoch": 0.9419479842015053,
        "step": 12640
    },
    {
        "loss": 2.7331,
        "grad_norm": 1.730778694152832,
        "learning_rate": 1.4797932732858e-05,
        "epoch": 0.9420225054027871,
        "step": 12641
    },
    {
        "loss": 2.015,
        "grad_norm": 2.7753989696502686,
        "learning_rate": 1.4761116827523558e-05,
        "epoch": 0.9420970266040689,
        "step": 12642
    },
    {
        "loss": 1.7541,
        "grad_norm": 3.233386993408203,
        "learning_rate": 1.4724343126751116e-05,
        "epoch": 0.9421715478053506,
        "step": 12643
    },
    {
        "loss": 2.7375,
        "grad_norm": 3.5506784915924072,
        "learning_rate": 1.46876116487487e-05,
        "epoch": 0.9422460690066324,
        "step": 12644
    },
    {
        "loss": 3.1524,
        "grad_norm": 2.34826397895813,
        "learning_rate": 1.4650922411703206e-05,
        "epoch": 0.9423205902079141,
        "step": 12645
    },
    {
        "loss": 2.5874,
        "grad_norm": 2.4684112071990967,
        "learning_rate": 1.4614275433780678e-05,
        "epoch": 0.942395111409196,
        "step": 12646
    },
    {
        "loss": 2.781,
        "grad_norm": 2.3750574588775635,
        "learning_rate": 1.457767073312616e-05,
        "epoch": 0.9424696326104777,
        "step": 12647
    },
    {
        "loss": 2.2319,
        "grad_norm": 2.8459808826446533,
        "learning_rate": 1.4541108327863973e-05,
        "epoch": 0.9425441538117595,
        "step": 12648
    },
    {
        "loss": 2.2205,
        "grad_norm": 4.09110164642334,
        "learning_rate": 1.4504588236097294e-05,
        "epoch": 0.9426186750130412,
        "step": 12649
    },
    {
        "loss": 2.6743,
        "grad_norm": 2.2796528339385986,
        "learning_rate": 1.4468110475908447e-05,
        "epoch": 0.942693196214323,
        "step": 12650
    },
    {
        "loss": 1.4176,
        "grad_norm": 2.6164474487304688,
        "learning_rate": 1.443167506535864e-05,
        "epoch": 0.9427677174156047,
        "step": 12651
    },
    {
        "loss": 2.7954,
        "grad_norm": 3.3458986282348633,
        "learning_rate": 1.439528202248841e-05,
        "epoch": 0.9428422386168865,
        "step": 12652
    },
    {
        "loss": 1.4717,
        "grad_norm": 3.553417921066284,
        "learning_rate": 1.4358931365316996e-05,
        "epoch": 0.9429167598181682,
        "step": 12653
    },
    {
        "loss": 2.8339,
        "grad_norm": 2.362297296524048,
        "learning_rate": 1.4322623111842947e-05,
        "epoch": 0.9429912810194501,
        "step": 12654
    },
    {
        "loss": 2.1593,
        "grad_norm": 3.2455272674560547,
        "learning_rate": 1.4286357280043583e-05,
        "epoch": 0.9430658022207318,
        "step": 12655
    },
    {
        "loss": 2.1618,
        "grad_norm": 3.427454948425293,
        "learning_rate": 1.425013388787525e-05,
        "epoch": 0.9431403234220136,
        "step": 12656
    },
    {
        "loss": 2.7871,
        "grad_norm": 2.321355104446411,
        "learning_rate": 1.4213952953273557e-05,
        "epoch": 0.9432148446232953,
        "step": 12657
    },
    {
        "loss": 2.7262,
        "grad_norm": 1.6653640270233154,
        "learning_rate": 1.4177814494152564e-05,
        "epoch": 0.9432893658245771,
        "step": 12658
    },
    {
        "loss": 1.9959,
        "grad_norm": 3.3664028644561768,
        "learning_rate": 1.4141718528405812e-05,
        "epoch": 0.9433638870258588,
        "step": 12659
    },
    {
        "loss": 2.7083,
        "grad_norm": 2.4424233436584473,
        "learning_rate": 1.4105665073905538e-05,
        "epoch": 0.9434384082271406,
        "step": 12660
    },
    {
        "loss": 2.5068,
        "grad_norm": 1.90651273727417,
        "learning_rate": 1.406965414850292e-05,
        "epoch": 0.9435129294284224,
        "step": 12661
    },
    {
        "loss": 1.8755,
        "grad_norm": 3.956139326095581,
        "learning_rate": 1.4033685770028282e-05,
        "epoch": 0.9435874506297042,
        "step": 12662
    },
    {
        "loss": 2.5837,
        "grad_norm": 2.382951021194458,
        "learning_rate": 1.399775995629068e-05,
        "epoch": 0.9436619718309859,
        "step": 12663
    },
    {
        "loss": 2.3732,
        "grad_norm": 2.0773937702178955,
        "learning_rate": 1.3961876725078082e-05,
        "epoch": 0.9437364930322677,
        "step": 12664
    },
    {
        "loss": 1.8676,
        "grad_norm": 3.402038335800171,
        "learning_rate": 1.3926036094157624e-05,
        "epoch": 0.9438110142335494,
        "step": 12665
    },
    {
        "loss": 0.972,
        "grad_norm": 2.668159246444702,
        "learning_rate": 1.3890238081275065e-05,
        "epoch": 0.9438855354348312,
        "step": 12666
    },
    {
        "loss": 2.4614,
        "grad_norm": 3.182401180267334,
        "learning_rate": 1.3854482704155103e-05,
        "epoch": 0.9439600566361129,
        "step": 12667
    },
    {
        "loss": 2.2542,
        "grad_norm": 2.89656925201416,
        "learning_rate": 1.3818769980501622e-05,
        "epoch": 0.9440345778373948,
        "step": 12668
    },
    {
        "loss": 2.7955,
        "grad_norm": 1.976016879081726,
        "learning_rate": 1.378309992799689e-05,
        "epoch": 0.9441090990386765,
        "step": 12669
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.729212999343872,
        "learning_rate": 1.3747472564302511e-05,
        "epoch": 0.9441836202399583,
        "step": 12670
    },
    {
        "loss": 1.7786,
        "grad_norm": 2.9027976989746094,
        "learning_rate": 1.3711887907058617e-05,
        "epoch": 0.94425814144124,
        "step": 12671
    },
    {
        "loss": 2.5549,
        "grad_norm": 3.886094808578491,
        "learning_rate": 1.367634597388452e-05,
        "epoch": 0.9443326626425218,
        "step": 12672
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.3076119422912598,
        "learning_rate": 1.364084678237806e-05,
        "epoch": 0.9444071838438035,
        "step": 12673
    },
    {
        "loss": 1.9884,
        "grad_norm": 2.039825677871704,
        "learning_rate": 1.360539035011602e-05,
        "epoch": 0.9444817050450853,
        "step": 12674
    },
    {
        "loss": 2.6228,
        "grad_norm": 2.7686147689819336,
        "learning_rate": 1.3569976694654163e-05,
        "epoch": 0.944556226246367,
        "step": 12675
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.2046031951904297,
        "learning_rate": 1.3534605833526815e-05,
        "epoch": 0.9446307474476489,
        "step": 12676
    },
    {
        "loss": 1.569,
        "grad_norm": 2.7006185054779053,
        "learning_rate": 1.3499277784247455e-05,
        "epoch": 0.9447052686489307,
        "step": 12677
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.523375988006592,
        "learning_rate": 1.3463992564307881e-05,
        "epoch": 0.9447797898502124,
        "step": 12678
    },
    {
        "loss": 2.6863,
        "grad_norm": 2.160322904586792,
        "learning_rate": 1.3428750191179174e-05,
        "epoch": 0.9448543110514942,
        "step": 12679
    },
    {
        "loss": 2.015,
        "grad_norm": 2.211369514465332,
        "learning_rate": 1.339355068231094e-05,
        "epoch": 0.9449288322527759,
        "step": 12680
    },
    {
        "loss": 2.0598,
        "grad_norm": 2.4476983547210693,
        "learning_rate": 1.3358394055131485e-05,
        "epoch": 0.9450033534540577,
        "step": 12681
    },
    {
        "loss": 2.2947,
        "grad_norm": 4.90017032623291,
        "learning_rate": 1.3323280327048171e-05,
        "epoch": 0.9450778746553394,
        "step": 12682
    },
    {
        "loss": 2.6759,
        "grad_norm": 2.6708598136901855,
        "learning_rate": 1.3288209515446893e-05,
        "epoch": 0.9451523958566213,
        "step": 12683
    },
    {
        "loss": 2.622,
        "grad_norm": 1.8616338968276978,
        "learning_rate": 1.325318163769228e-05,
        "epoch": 0.945226917057903,
        "step": 12684
    },
    {
        "loss": 2.3207,
        "grad_norm": 4.233229160308838,
        "learning_rate": 1.3218196711127928e-05,
        "epoch": 0.9453014382591848,
        "step": 12685
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.1068649291992188,
        "learning_rate": 1.3183254753075935e-05,
        "epoch": 0.9453759594604665,
        "step": 12686
    },
    {
        "loss": 1.8994,
        "grad_norm": 4.113746643066406,
        "learning_rate": 1.3148355780837151e-05,
        "epoch": 0.9454504806617483,
        "step": 12687
    },
    {
        "loss": 2.6953,
        "grad_norm": 3.054715871810913,
        "learning_rate": 1.311349981169142e-05,
        "epoch": 0.94552500186303,
        "step": 12688
    },
    {
        "loss": 2.202,
        "grad_norm": 3.6652395725250244,
        "learning_rate": 1.3078686862896772e-05,
        "epoch": 0.9455995230643118,
        "step": 12689
    },
    {
        "loss": 2.2891,
        "grad_norm": 3.6372933387756348,
        "learning_rate": 1.304391695169046e-05,
        "epoch": 0.9456740442655935,
        "step": 12690
    },
    {
        "loss": 2.3479,
        "grad_norm": 3.1273298263549805,
        "learning_rate": 1.3009190095288127e-05,
        "epoch": 0.9457485654668754,
        "step": 12691
    },
    {
        "loss": 1.8565,
        "grad_norm": 2.3066036701202393,
        "learning_rate": 1.2974506310884104e-05,
        "epoch": 0.9458230866681571,
        "step": 12692
    },
    {
        "loss": 2.8268,
        "grad_norm": 2.648127555847168,
        "learning_rate": 1.2939865615651626e-05,
        "epoch": 0.9458976078694389,
        "step": 12693
    },
    {
        "loss": 2.5234,
        "grad_norm": 1.8736883401870728,
        "learning_rate": 1.290526802674229e-05,
        "epoch": 0.9459721290707206,
        "step": 12694
    },
    {
        "loss": 2.6964,
        "grad_norm": 3.044668436050415,
        "learning_rate": 1.2870713561286641e-05,
        "epoch": 0.9460466502720024,
        "step": 12695
    },
    {
        "loss": 2.7069,
        "grad_norm": 3.8696532249450684,
        "learning_rate": 1.2836202236393624e-05,
        "epoch": 0.9461211714732841,
        "step": 12696
    },
    {
        "loss": 2.6052,
        "grad_norm": 2.3112640380859375,
        "learning_rate": 1.2801734069150951e-05,
        "epoch": 0.9461956926745659,
        "step": 12697
    },
    {
        "loss": 2.2348,
        "grad_norm": 2.2447142601013184,
        "learning_rate": 1.2767309076624877e-05,
        "epoch": 0.9462702138758476,
        "step": 12698
    },
    {
        "loss": 2.3293,
        "grad_norm": 3.008387327194214,
        "learning_rate": 1.2732927275860462e-05,
        "epoch": 0.9463447350771295,
        "step": 12699
    },
    {
        "loss": 2.4769,
        "grad_norm": 3.09134578704834,
        "learning_rate": 1.2698588683881208e-05,
        "epoch": 0.9464192562784112,
        "step": 12700
    },
    {
        "loss": 2.7718,
        "grad_norm": 2.8133230209350586,
        "learning_rate": 1.2664293317689257e-05,
        "epoch": 0.946493777479693,
        "step": 12701
    },
    {
        "loss": 2.2742,
        "grad_norm": 3.0356051921844482,
        "learning_rate": 1.263004119426533e-05,
        "epoch": 0.9465682986809747,
        "step": 12702
    },
    {
        "loss": 2.3913,
        "grad_norm": 1.962409496307373,
        "learning_rate": 1.2595832330568912e-05,
        "epoch": 0.9466428198822565,
        "step": 12703
    },
    {
        "loss": 1.3896,
        "grad_norm": 3.112063407897949,
        "learning_rate": 1.2561666743537758e-05,
        "epoch": 0.9467173410835382,
        "step": 12704
    },
    {
        "loss": 2.0235,
        "grad_norm": 2.8352136611938477,
        "learning_rate": 1.2527544450088524e-05,
        "epoch": 0.94679186228482,
        "step": 12705
    },
    {
        "loss": 2.3663,
        "grad_norm": 3.39848256111145,
        "learning_rate": 1.2493465467116206e-05,
        "epoch": 0.9468663834861017,
        "step": 12706
    },
    {
        "loss": 2.3861,
        "grad_norm": 3.405625343322754,
        "learning_rate": 1.2459429811494361e-05,
        "epoch": 0.9469409046873836,
        "step": 12707
    },
    {
        "loss": 2.3965,
        "grad_norm": 2.727473497390747,
        "learning_rate": 1.2425437500075342e-05,
        "epoch": 0.9470154258886653,
        "step": 12708
    },
    {
        "loss": 2.0409,
        "grad_norm": 3.151642322540283,
        "learning_rate": 1.2391488549689623e-05,
        "epoch": 0.9470899470899471,
        "step": 12709
    },
    {
        "loss": 2.0387,
        "grad_norm": 2.817779779434204,
        "learning_rate": 1.2357582977146631e-05,
        "epoch": 0.9471644682912288,
        "step": 12710
    },
    {
        "loss": 1.9956,
        "grad_norm": 1.4070360660552979,
        "learning_rate": 1.2323720799234017e-05,
        "epoch": 0.9472389894925106,
        "step": 12711
    },
    {
        "loss": 1.9083,
        "grad_norm": 5.453800201416016,
        "learning_rate": 1.2289902032718026e-05,
        "epoch": 0.9473135106937924,
        "step": 12712
    },
    {
        "loss": 2.2921,
        "grad_norm": 3.0755369663238525,
        "learning_rate": 1.2256126694343562e-05,
        "epoch": 0.9473880318950741,
        "step": 12713
    },
    {
        "loss": 1.923,
        "grad_norm": 2.6081130504608154,
        "learning_rate": 1.222239480083377e-05,
        "epoch": 0.947462553096356,
        "step": 12714
    },
    {
        "loss": 2.5565,
        "grad_norm": 2.205012083053589,
        "learning_rate": 1.2188706368890545e-05,
        "epoch": 0.9475370742976377,
        "step": 12715
    },
    {
        "loss": 2.4603,
        "grad_norm": 3.7654919624328613,
        "learning_rate": 1.2155061415194058e-05,
        "epoch": 0.9476115954989195,
        "step": 12716
    },
    {
        "loss": 2.0753,
        "grad_norm": 3.1242475509643555,
        "learning_rate": 1.212145995640307e-05,
        "epoch": 0.9476861167002012,
        "step": 12717
    },
    {
        "loss": 2.6892,
        "grad_norm": 2.3006927967071533,
        "learning_rate": 1.2087902009154627e-05,
        "epoch": 0.947760637901483,
        "step": 12718
    },
    {
        "loss": 1.3683,
        "grad_norm": 4.233528137207031,
        "learning_rate": 1.2054387590064542e-05,
        "epoch": 0.9478351591027647,
        "step": 12719
    },
    {
        "loss": 2.3243,
        "grad_norm": 3.0910234451293945,
        "learning_rate": 1.2020916715726837e-05,
        "epoch": 0.9479096803040465,
        "step": 12720
    },
    {
        "loss": 2.4714,
        "grad_norm": 3.9724364280700684,
        "learning_rate": 1.1987489402714025e-05,
        "epoch": 0.9479842015053283,
        "step": 12721
    },
    {
        "loss": 2.3702,
        "grad_norm": 1.8063594102859497,
        "learning_rate": 1.1954105667576998e-05,
        "epoch": 0.9480587227066101,
        "step": 12722
    },
    {
        "loss": 2.835,
        "grad_norm": 3.636530876159668,
        "learning_rate": 1.1920765526845268e-05,
        "epoch": 0.9481332439078918,
        "step": 12723
    },
    {
        "loss": 2.957,
        "grad_norm": 1.508975625038147,
        "learning_rate": 1.1887468997026585e-05,
        "epoch": 0.9482077651091736,
        "step": 12724
    },
    {
        "loss": 2.1895,
        "grad_norm": 3.5567948818206787,
        "learning_rate": 1.185421609460705e-05,
        "epoch": 0.9482822863104553,
        "step": 12725
    },
    {
        "loss": 2.077,
        "grad_norm": 3.9334115982055664,
        "learning_rate": 1.1821006836051452e-05,
        "epoch": 0.9483568075117371,
        "step": 12726
    },
    {
        "loss": 2.2668,
        "grad_norm": 1.7291789054870605,
        "learning_rate": 1.1787841237802589e-05,
        "epoch": 0.9484313287130188,
        "step": 12727
    },
    {
        "loss": 2.5085,
        "grad_norm": 3.362760066986084,
        "learning_rate": 1.1754719316282048e-05,
        "epoch": 0.9485058499143006,
        "step": 12728
    },
    {
        "loss": 2.3566,
        "grad_norm": 3.215654134750366,
        "learning_rate": 1.1721641087889323e-05,
        "epoch": 0.9485803711155824,
        "step": 12729
    },
    {
        "loss": 2.5113,
        "grad_norm": 2.3346495628356934,
        "learning_rate": 1.1688606569002736e-05,
        "epoch": 0.9486548923168642,
        "step": 12730
    },
    {
        "loss": 2.4641,
        "grad_norm": 2.0736136436462402,
        "learning_rate": 1.1655615775978667e-05,
        "epoch": 0.9487294135181459,
        "step": 12731
    },
    {
        "loss": 2.5484,
        "grad_norm": 2.1496284008026123,
        "learning_rate": 1.1622668725151898e-05,
        "epoch": 0.9488039347194277,
        "step": 12732
    },
    {
        "loss": 1.9784,
        "grad_norm": 3.382964849472046,
        "learning_rate": 1.1589765432835731e-05,
        "epoch": 0.9488784559207094,
        "step": 12733
    },
    {
        "loss": 1.328,
        "grad_norm": 4.0578107833862305,
        "learning_rate": 1.1556905915321558e-05,
        "epoch": 0.9489529771219912,
        "step": 12734
    },
    {
        "loss": 1.9444,
        "grad_norm": 3.601083755493164,
        "learning_rate": 1.1524090188879155e-05,
        "epoch": 0.9490274983232729,
        "step": 12735
    },
    {
        "loss": 2.3504,
        "grad_norm": 3.9531161785125732,
        "learning_rate": 1.14913182697568e-05,
        "epoch": 0.9491020195245548,
        "step": 12736
    },
    {
        "loss": 2.1946,
        "grad_norm": 3.727038860321045,
        "learning_rate": 1.1458590174180872e-05,
        "epoch": 0.9491765407258365,
        "step": 12737
    },
    {
        "loss": 2.5009,
        "grad_norm": 2.3550655841827393,
        "learning_rate": 1.1425905918356061e-05,
        "epoch": 0.9492510619271183,
        "step": 12738
    },
    {
        "loss": 2.7553,
        "grad_norm": 4.311495304107666,
        "learning_rate": 1.1393265518465602e-05,
        "epoch": 0.9493255831284,
        "step": 12739
    },
    {
        "loss": 2.4575,
        "grad_norm": 4.142083644866943,
        "learning_rate": 1.1360668990670598e-05,
        "epoch": 0.9494001043296818,
        "step": 12740
    },
    {
        "loss": 2.0115,
        "grad_norm": 3.2932538986206055,
        "learning_rate": 1.1328116351110806e-05,
        "epoch": 0.9494746255309635,
        "step": 12741
    },
    {
        "loss": 2.2481,
        "grad_norm": 4.424075603485107,
        "learning_rate": 1.1295607615903992e-05,
        "epoch": 0.9495491467322453,
        "step": 12742
    },
    {
        "loss": 2.7261,
        "grad_norm": 2.0083463191986084,
        "learning_rate": 1.126314280114642e-05,
        "epoch": 0.949623667933527,
        "step": 12743
    },
    {
        "loss": 1.527,
        "grad_norm": 2.8416733741760254,
        "learning_rate": 1.1230721922912423e-05,
        "epoch": 0.9496981891348089,
        "step": 12744
    },
    {
        "loss": 2.421,
        "grad_norm": 2.9298901557922363,
        "learning_rate": 1.1198344997254584e-05,
        "epoch": 0.9497727103360906,
        "step": 12745
    },
    {
        "loss": 2.7636,
        "grad_norm": 2.123746633529663,
        "learning_rate": 1.1166012040203888e-05,
        "epoch": 0.9498472315373724,
        "step": 12746
    },
    {
        "loss": 2.7879,
        "grad_norm": 2.558065176010132,
        "learning_rate": 1.113372306776933e-05,
        "epoch": 0.9499217527386542,
        "step": 12747
    },
    {
        "loss": 2.0094,
        "grad_norm": 2.9813289642333984,
        "learning_rate": 1.1101478095938433e-05,
        "epoch": 0.9499962739399359,
        "step": 12748
    },
    {
        "loss": 2.2484,
        "grad_norm": 2.7869715690612793,
        "learning_rate": 1.106927714067646e-05,
        "epoch": 0.9500707951412177,
        "step": 12749
    },
    {
        "loss": 1.7111,
        "grad_norm": 2.621967077255249,
        "learning_rate": 1.1037120217927365e-05,
        "epoch": 0.9501453163424994,
        "step": 12750
    },
    {
        "loss": 1.2641,
        "grad_norm": 3.7397491931915283,
        "learning_rate": 1.1005007343613028e-05,
        "epoch": 0.9502198375437813,
        "step": 12751
    },
    {
        "loss": 2.4088,
        "grad_norm": 4.635432243347168,
        "learning_rate": 1.0972938533633581e-05,
        "epoch": 0.950294358745063,
        "step": 12752
    },
    {
        "loss": 2.2241,
        "grad_norm": 2.8031795024871826,
        "learning_rate": 1.094091380386728e-05,
        "epoch": 0.9503688799463448,
        "step": 12753
    },
    {
        "loss": 2.598,
        "grad_norm": 2.5042552947998047,
        "learning_rate": 1.0908933170170744e-05,
        "epoch": 0.9504434011476265,
        "step": 12754
    },
    {
        "loss": 1.5145,
        "grad_norm": 4.986138343811035,
        "learning_rate": 1.0876996648378512e-05,
        "epoch": 0.9505179223489083,
        "step": 12755
    },
    {
        "loss": 1.8697,
        "grad_norm": 3.02376651763916,
        "learning_rate": 1.084510425430354e-05,
        "epoch": 0.95059244355019,
        "step": 12756
    },
    {
        "loss": 2.6852,
        "grad_norm": 2.853498697280884,
        "learning_rate": 1.0813256003736694e-05,
        "epoch": 0.9506669647514718,
        "step": 12757
    },
    {
        "loss": 2.6026,
        "grad_norm": 1.835963249206543,
        "learning_rate": 1.078145191244705e-05,
        "epoch": 0.9507414859527535,
        "step": 12758
    },
    {
        "loss": 2.1153,
        "grad_norm": 3.337232828140259,
        "learning_rate": 1.0749691996182032e-05,
        "epoch": 0.9508160071540354,
        "step": 12759
    },
    {
        "loss": 2.0994,
        "grad_norm": 5.077861309051514,
        "learning_rate": 1.0717976270666763e-05,
        "epoch": 0.9508905283553171,
        "step": 12760
    },
    {
        "loss": 1.7574,
        "grad_norm": 2.9645814895629883,
        "learning_rate": 1.0686304751604947e-05,
        "epoch": 0.9509650495565989,
        "step": 12761
    },
    {
        "loss": 2.2985,
        "grad_norm": 4.07752799987793,
        "learning_rate": 1.0654677454678108e-05,
        "epoch": 0.9510395707578806,
        "step": 12762
    },
    {
        "loss": 2.1818,
        "grad_norm": 3.2617897987365723,
        "learning_rate": 1.0623094395545918e-05,
        "epoch": 0.9511140919591624,
        "step": 12763
    },
    {
        "loss": 2.683,
        "grad_norm": 3.099851131439209,
        "learning_rate": 1.0591555589846259e-05,
        "epoch": 0.9511886131604441,
        "step": 12764
    },
    {
        "loss": 2.3809,
        "grad_norm": 2.1528942584991455,
        "learning_rate": 1.0560061053194947e-05,
        "epoch": 0.9512631343617259,
        "step": 12765
    },
    {
        "loss": 2.5907,
        "grad_norm": 1.8350517749786377,
        "learning_rate": 1.0528610801186056e-05,
        "epoch": 0.9513376555630076,
        "step": 12766
    },
    {
        "loss": 2.1767,
        "grad_norm": 3.04604172706604,
        "learning_rate": 1.0497204849391595e-05,
        "epoch": 0.9514121767642895,
        "step": 12767
    },
    {
        "loss": 2.1547,
        "grad_norm": 3.6861956119537354,
        "learning_rate": 1.0465843213361658e-05,
        "epoch": 0.9514866979655712,
        "step": 12768
    },
    {
        "loss": 2.4475,
        "grad_norm": 3.02390456199646,
        "learning_rate": 1.0434525908624382e-05,
        "epoch": 0.951561219166853,
        "step": 12769
    },
    {
        "loss": 2.5955,
        "grad_norm": 2.37093448638916,
        "learning_rate": 1.0403252950686083e-05,
        "epoch": 0.9516357403681347,
        "step": 12770
    },
    {
        "loss": 2.4962,
        "grad_norm": Infinity,
        "learning_rate": 1.0403252950686083e-05,
        "epoch": 0.9517102615694165,
        "step": 12771
    },
    {
        "loss": 2.3984,
        "grad_norm": 3.252659559249878,
        "learning_rate": 1.0372024355031008e-05,
        "epoch": 0.9517847827706982,
        "step": 12772
    },
    {
        "loss": 2.339,
        "grad_norm": 1.9647855758666992,
        "learning_rate": 1.0340840137121399e-05,
        "epoch": 0.95185930397198,
        "step": 12773
    },
    {
        "loss": 1.9999,
        "grad_norm": 3.2410836219787598,
        "learning_rate": 1.0309700312397563e-05,
        "epoch": 0.9519338251732617,
        "step": 12774
    },
    {
        "loss": 2.6825,
        "grad_norm": 2.4602580070495605,
        "learning_rate": 1.0278604896277965e-05,
        "epoch": 0.9520083463745436,
        "step": 12775
    },
    {
        "loss": 2.3386,
        "grad_norm": 2.9862728118896484,
        "learning_rate": 1.0247553904158824e-05,
        "epoch": 0.9520828675758253,
        "step": 12776
    },
    {
        "loss": 2.8476,
        "grad_norm": 3.571913242340088,
        "learning_rate": 1.021654735141464e-05,
        "epoch": 0.9521573887771071,
        "step": 12777
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.213226795196533,
        "learning_rate": 1.0185585253397711e-05,
        "epoch": 0.9522319099783888,
        "step": 12778
    },
    {
        "loss": 2.7817,
        "grad_norm": 3.058885335922241,
        "learning_rate": 1.015466762543832e-05,
        "epoch": 0.9523064311796706,
        "step": 12779
    },
    {
        "loss": 2.2302,
        "grad_norm": 3.4357612133026123,
        "learning_rate": 1.0123794482844995e-05,
        "epoch": 0.9523809523809523,
        "step": 12780
    },
    {
        "loss": 1.391,
        "grad_norm": 4.059717655181885,
        "learning_rate": 1.0092965840903767e-05,
        "epoch": 0.9524554735822341,
        "step": 12781
    },
    {
        "loss": 2.4676,
        "grad_norm": 3.9335451126098633,
        "learning_rate": 1.0062181714879104e-05,
        "epoch": 0.952529994783516,
        "step": 12782
    },
    {
        "loss": 2.5025,
        "grad_norm": 2.4315736293792725,
        "learning_rate": 1.003144212001318e-05,
        "epoch": 0.9526045159847977,
        "step": 12783
    },
    {
        "loss": 2.0734,
        "grad_norm": 3.0669031143188477,
        "learning_rate": 1.0000747071526106e-05,
        "epoch": 0.9526790371860795,
        "step": 12784
    },
    {
        "loss": 2.227,
        "grad_norm": 2.9971959590911865,
        "learning_rate": 9.970096584616118e-06,
        "epoch": 0.9527535583873612,
        "step": 12785
    },
    {
        "loss": 2.785,
        "grad_norm": 1.9753649234771729,
        "learning_rate": 9.939490674459262e-06,
        "epoch": 0.952828079588643,
        "step": 12786
    },
    {
        "loss": 2.4774,
        "grad_norm": 3.458484411239624,
        "learning_rate": 9.908929356209396e-06,
        "epoch": 0.9529026007899247,
        "step": 12787
    },
    {
        "loss": 1.6981,
        "grad_norm": 3.2380757331848145,
        "learning_rate": 9.87841264499859e-06,
        "epoch": 0.9529771219912065,
        "step": 12788
    },
    {
        "loss": 2.4694,
        "grad_norm": 3.6694912910461426,
        "learning_rate": 9.847940555936608e-06,
        "epoch": 0.9530516431924883,
        "step": 12789
    },
    {
        "loss": 2.8821,
        "grad_norm": 2.6106929779052734,
        "learning_rate": 9.81751310411111e-06,
        "epoch": 0.9531261643937701,
        "step": 12790
    },
    {
        "loss": 2.1309,
        "grad_norm": 3.5371923446655273,
        "learning_rate": 9.7871303045879e-06,
        "epoch": 0.9532006855950518,
        "step": 12791
    },
    {
        "loss": 2.1814,
        "grad_norm": 2.583840847015381,
        "learning_rate": 9.756792172410256e-06,
        "epoch": 0.9532752067963336,
        "step": 12792
    },
    {
        "loss": 2.8985,
        "grad_norm": 4.355605125427246,
        "learning_rate": 9.726498722599776e-06,
        "epoch": 0.9533497279976153,
        "step": 12793
    },
    {
        "loss": 2.8103,
        "grad_norm": 3.169278144836426,
        "learning_rate": 9.696249970155613e-06,
        "epoch": 0.9534242491988971,
        "step": 12794
    },
    {
        "loss": 2.5592,
        "grad_norm": 2.940675973892212,
        "learning_rate": 9.666045930055046e-06,
        "epoch": 0.9534987704001788,
        "step": 12795
    },
    {
        "loss": 2.5425,
        "grad_norm": 2.5335915088653564,
        "learning_rate": 9.635886617252999e-06,
        "epoch": 0.9535732916014606,
        "step": 12796
    },
    {
        "loss": 1.9969,
        "grad_norm": 2.577007532119751,
        "learning_rate": 9.605772046682272e-06,
        "epoch": 0.9536478128027424,
        "step": 12797
    },
    {
        "loss": 2.6347,
        "grad_norm": 2.490382432937622,
        "learning_rate": 9.575702233253725e-06,
        "epoch": 0.9537223340040242,
        "step": 12798
    },
    {
        "loss": 2.6617,
        "grad_norm": 4.041109561920166,
        "learning_rate": 9.545677191855762e-06,
        "epoch": 0.9537968552053059,
        "step": 12799
    },
    {
        "loss": 2.8312,
        "grad_norm": 3.769274950027466,
        "learning_rate": 9.515696937354945e-06,
        "epoch": 0.9538713764065877,
        "step": 12800
    },
    {
        "loss": 2.1046,
        "grad_norm": 3.016751766204834,
        "learning_rate": 9.485761484595269e-06,
        "epoch": 0.9539458976078694,
        "step": 12801
    },
    {
        "loss": 1.2884,
        "grad_norm": 4.6304497718811035,
        "learning_rate": 9.455870848398918e-06,
        "epoch": 0.9540204188091512,
        "step": 12802
    },
    {
        "loss": 2.9504,
        "grad_norm": 1.8480316400527954,
        "learning_rate": 9.426025043565657e-06,
        "epoch": 0.9540949400104329,
        "step": 12803
    },
    {
        "loss": 2.2306,
        "grad_norm": 2.4948649406433105,
        "learning_rate": 9.396224084873063e-06,
        "epoch": 0.9541694612117148,
        "step": 12804
    },
    {
        "loss": 2.337,
        "grad_norm": 4.141658782958984,
        "learning_rate": 9.366467987076688e-06,
        "epoch": 0.9542439824129965,
        "step": 12805
    },
    {
        "loss": 2.3375,
        "grad_norm": 3.5072555541992188,
        "learning_rate": 9.336756764909693e-06,
        "epoch": 0.9543185036142783,
        "step": 12806
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.4867212772369385,
        "learning_rate": 9.307090433083022e-06,
        "epoch": 0.95439302481556,
        "step": 12807
    },
    {
        "loss": 2.1654,
        "grad_norm": 3.3124778270721436,
        "learning_rate": 9.277469006285554e-06,
        "epoch": 0.9544675460168418,
        "step": 12808
    },
    {
        "loss": 2.3407,
        "grad_norm": 3.2035250663757324,
        "learning_rate": 9.247892499183797e-06,
        "epoch": 0.9545420672181235,
        "step": 12809
    },
    {
        "loss": 2.6152,
        "grad_norm": 1.9279091358184814,
        "learning_rate": 9.218360926421954e-06,
        "epoch": 0.9546165884194053,
        "step": 12810
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.355137586593628,
        "learning_rate": 9.188874302622253e-06,
        "epoch": 0.954691109620687,
        "step": 12811
    },
    {
        "loss": 1.7805,
        "grad_norm": 2.627546787261963,
        "learning_rate": 9.15943264238427e-06,
        "epoch": 0.9547656308219689,
        "step": 12812
    },
    {
        "loss": 1.9991,
        "grad_norm": 2.9975368976593018,
        "learning_rate": 9.130035960285709e-06,
        "epoch": 0.9548401520232506,
        "step": 12813
    },
    {
        "loss": 1.9197,
        "grad_norm": 5.771380424499512,
        "learning_rate": 9.1006842708817e-06,
        "epoch": 0.9549146732245324,
        "step": 12814
    },
    {
        "loss": 2.144,
        "grad_norm": 3.016709804534912,
        "learning_rate": 9.071377588705354e-06,
        "epoch": 0.9549891944258141,
        "step": 12815
    },
    {
        "loss": 2.0424,
        "grad_norm": 3.150805950164795,
        "learning_rate": 9.042115928267304e-06,
        "epoch": 0.9550637156270959,
        "step": 12816
    },
    {
        "loss": 2.0772,
        "grad_norm": 4.131710529327393,
        "learning_rate": 9.012899304055899e-06,
        "epoch": 0.9551382368283776,
        "step": 12817
    },
    {
        "loss": 2.3675,
        "grad_norm": 2.2282047271728516,
        "learning_rate": 8.983727730537395e-06,
        "epoch": 0.9552127580296594,
        "step": 12818
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.634779214859009,
        "learning_rate": 8.954601222155468e-06,
        "epoch": 0.9552872792309413,
        "step": 12819
    },
    {
        "loss": 2.1009,
        "grad_norm": 3.281620979309082,
        "learning_rate": 8.92551979333165e-06,
        "epoch": 0.955361800432223,
        "step": 12820
    },
    {
        "loss": 2.595,
        "grad_norm": 6.997435092926025,
        "learning_rate": 8.896483458465043e-06,
        "epoch": 0.9554363216335048,
        "step": 12821
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.206956624984741,
        "learning_rate": 8.867492231932606e-06,
        "epoch": 0.9555108428347865,
        "step": 12822
    },
    {
        "loss": 2.8354,
        "grad_norm": 2.5083911418914795,
        "learning_rate": 8.838546128088787e-06,
        "epoch": 0.9555853640360683,
        "step": 12823
    },
    {
        "loss": 2.6544,
        "grad_norm": 3.3285770416259766,
        "learning_rate": 8.809645161265757e-06,
        "epoch": 0.95565988523735,
        "step": 12824
    },
    {
        "loss": 1.8802,
        "grad_norm": 4.554575443267822,
        "learning_rate": 8.780789345773244e-06,
        "epoch": 0.9557344064386318,
        "step": 12825
    },
    {
        "loss": 2.6721,
        "grad_norm": 2.434138298034668,
        "learning_rate": 8.751978695898844e-06,
        "epoch": 0.9558089276399135,
        "step": 12826
    },
    {
        "loss": 2.4838,
        "grad_norm": 1.6392239332199097,
        "learning_rate": 8.723213225907545e-06,
        "epoch": 0.9558834488411954,
        "step": 12827
    },
    {
        "loss": 2.565,
        "grad_norm": 2.152045249938965,
        "learning_rate": 8.694492950042165e-06,
        "epoch": 0.9559579700424771,
        "step": 12828
    },
    {
        "loss": 2.5114,
        "grad_norm": 2.1561598777770996,
        "learning_rate": 8.665817882523009e-06,
        "epoch": 0.9560324912437589,
        "step": 12829
    },
    {
        "loss": 2.2318,
        "grad_norm": 2.445462942123413,
        "learning_rate": 8.637188037547995e-06,
        "epoch": 0.9561070124450406,
        "step": 12830
    },
    {
        "loss": 2.654,
        "grad_norm": 2.342029094696045,
        "learning_rate": 8.608603429292861e-06,
        "epoch": 0.9561815336463224,
        "step": 12831
    },
    {
        "loss": 2.4878,
        "grad_norm": 1.972974419593811,
        "learning_rate": 8.580064071910544e-06,
        "epoch": 0.9562560548476041,
        "step": 12832
    },
    {
        "loss": 2.3283,
        "grad_norm": 2.2909576892852783,
        "learning_rate": 8.551569979531982e-06,
        "epoch": 0.9563305760488859,
        "step": 12833
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.7110493183135986,
        "learning_rate": 8.52312116626549e-06,
        "epoch": 0.9564050972501676,
        "step": 12834
    },
    {
        "loss": 2.6444,
        "grad_norm": 1.9874091148376465,
        "learning_rate": 8.49471764619696e-06,
        "epoch": 0.9564796184514495,
        "step": 12835
    },
    {
        "loss": 1.9692,
        "grad_norm": 3.9748013019561768,
        "learning_rate": 8.466359433389991e-06,
        "epoch": 0.9565541396527312,
        "step": 12836
    },
    {
        "loss": 2.4628,
        "grad_norm": 1.8146177530288696,
        "learning_rate": 8.43804654188557e-06,
        "epoch": 0.956628660854013,
        "step": 12837
    },
    {
        "loss": 2.4676,
        "grad_norm": 3.0761613845825195,
        "learning_rate": 8.409778985702432e-06,
        "epoch": 0.9567031820552947,
        "step": 12838
    },
    {
        "loss": 2.3073,
        "grad_norm": 3.410393714904785,
        "learning_rate": 8.38155677883673e-06,
        "epoch": 0.9567777032565765,
        "step": 12839
    },
    {
        "loss": 1.6463,
        "grad_norm": 3.350466012954712,
        "learning_rate": 8.353379935262218e-06,
        "epoch": 0.9568522244578582,
        "step": 12840
    },
    {
        "loss": 2.4528,
        "grad_norm": 2.013624668121338,
        "learning_rate": 8.32524846893008e-06,
        "epoch": 0.95692674565914,
        "step": 12841
    },
    {
        "loss": 2.1442,
        "grad_norm": 3.1523008346557617,
        "learning_rate": 8.297162393769253e-06,
        "epoch": 0.9570012668604218,
        "step": 12842
    },
    {
        "loss": 2.9112,
        "grad_norm": 2.0324110984802246,
        "learning_rate": 8.269121723686013e-06,
        "epoch": 0.9570757880617036,
        "step": 12843
    },
    {
        "loss": 2.6535,
        "grad_norm": 2.323882579803467,
        "learning_rate": 8.24112647256422e-06,
        "epoch": 0.9571503092629853,
        "step": 12844
    },
    {
        "loss": 2.4978,
        "grad_norm": 2.586021900177002,
        "learning_rate": 8.213176654265198e-06,
        "epoch": 0.9572248304642671,
        "step": 12845
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.3675789833068848,
        "learning_rate": 8.185272282627899e-06,
        "epoch": 0.9572993516655488,
        "step": 12846
    },
    {
        "loss": 2.2645,
        "grad_norm": 2.757723331451416,
        "learning_rate": 8.157413371468669e-06,
        "epoch": 0.9573738728668306,
        "step": 12847
    },
    {
        "loss": 2.5023,
        "grad_norm": 1.6745833158493042,
        "learning_rate": 8.129599934581278e-06,
        "epoch": 0.9574483940681123,
        "step": 12848
    },
    {
        "loss": 2.5346,
        "grad_norm": 3.948354482650757,
        "learning_rate": 8.101831985737197e-06,
        "epoch": 0.9575229152693941,
        "step": 12849
    },
    {
        "loss": 2.3839,
        "grad_norm": 3.134866952896118,
        "learning_rate": 8.074109538685127e-06,
        "epoch": 0.9575974364706759,
        "step": 12850
    },
    {
        "loss": 2.7773,
        "grad_norm": 2.9096133708953857,
        "learning_rate": 8.046432607151533e-06,
        "epoch": 0.9576719576719577,
        "step": 12851
    },
    {
        "loss": 1.9977,
        "grad_norm": 4.123352527618408,
        "learning_rate": 8.018801204839954e-06,
        "epoch": 0.9577464788732394,
        "step": 12852
    },
    {
        "loss": 2.497,
        "grad_norm": 4.053637981414795,
        "learning_rate": 7.991215345431757e-06,
        "epoch": 0.9578210000745212,
        "step": 12853
    },
    {
        "loss": 2.2741,
        "grad_norm": 2.7066662311553955,
        "learning_rate": 7.963675042585572e-06,
        "epoch": 0.957895521275803,
        "step": 12854
    },
    {
        "loss": 2.5002,
        "grad_norm": 2.4694414138793945,
        "learning_rate": 7.936180309937413e-06,
        "epoch": 0.9579700424770847,
        "step": 12855
    },
    {
        "loss": 2.3882,
        "grad_norm": 2.9223482608795166,
        "learning_rate": 7.908731161100946e-06,
        "epoch": 0.9580445636783665,
        "step": 12856
    },
    {
        "loss": 2.3335,
        "grad_norm": 2.432121515274048,
        "learning_rate": 7.8813276096671e-06,
        "epoch": 0.9581190848796483,
        "step": 12857
    },
    {
        "loss": 2.8538,
        "grad_norm": 1.6651504039764404,
        "learning_rate": 7.85396966920422e-06,
        "epoch": 0.9581936060809301,
        "step": 12858
    },
    {
        "loss": 3.1776,
        "grad_norm": 2.87304425239563,
        "learning_rate": 7.826657353258204e-06,
        "epoch": 0.9582681272822118,
        "step": 12859
    },
    {
        "loss": 2.2201,
        "grad_norm": 2.795976161956787,
        "learning_rate": 7.799390675352225e-06,
        "epoch": 0.9583426484834936,
        "step": 12860
    },
    {
        "loss": 1.8554,
        "grad_norm": 3.5437841415405273,
        "learning_rate": 7.772169648986871e-06,
        "epoch": 0.9584171696847753,
        "step": 12861
    },
    {
        "loss": 2.0179,
        "grad_norm": 4.120339870452881,
        "learning_rate": 7.744994287640329e-06,
        "epoch": 0.9584916908860571,
        "step": 12862
    },
    {
        "loss": 2.5028,
        "grad_norm": 3.2286453247070312,
        "learning_rate": 7.717864604767766e-06,
        "epoch": 0.9585662120873388,
        "step": 12863
    },
    {
        "loss": 2.5275,
        "grad_norm": 3.0163559913635254,
        "learning_rate": 7.69078061380215e-06,
        "epoch": 0.9586407332886207,
        "step": 12864
    },
    {
        "loss": 1.3618,
        "grad_norm": 2.4963998794555664,
        "learning_rate": 7.663742328153556e-06,
        "epoch": 0.9587152544899024,
        "step": 12865
    },
    {
        "loss": 2.2833,
        "grad_norm": 2.2600386142730713,
        "learning_rate": 7.636749761209616e-06,
        "epoch": 0.9587897756911842,
        "step": 12866
    },
    {
        "loss": 2.0077,
        "grad_norm": 3.239788770675659,
        "learning_rate": 7.6098029263351855e-06,
        "epoch": 0.9588642968924659,
        "step": 12867
    },
    {
        "loss": 2.3344,
        "grad_norm": 3.081401824951172,
        "learning_rate": 7.5829018368724805e-06,
        "epoch": 0.9589388180937477,
        "step": 12868
    },
    {
        "loss": 2.853,
        "grad_norm": 2.265035629272461,
        "learning_rate": 7.556046506141201e-06,
        "epoch": 0.9590133392950294,
        "step": 12869
    },
    {
        "loss": 1.6562,
        "grad_norm": 3.353938579559326,
        "learning_rate": 7.529236947438212e-06,
        "epoch": 0.9590878604963112,
        "step": 12870
    },
    {
        "loss": 2.3208,
        "grad_norm": 3.3144583702087402,
        "learning_rate": 7.502473174037961e-06,
        "epoch": 0.9591623816975929,
        "step": 12871
    },
    {
        "loss": 2.6319,
        "grad_norm": 3.2724859714508057,
        "learning_rate": 7.475755199191836e-06,
        "epoch": 0.9592369028988748,
        "step": 12872
    },
    {
        "loss": 2.5741,
        "grad_norm": 4.108582973480225,
        "learning_rate": 7.449083036128957e-06,
        "epoch": 0.9593114241001565,
        "step": 12873
    },
    {
        "loss": 2.6532,
        "grad_norm": 2.7150354385375977,
        "learning_rate": 7.422456698055536e-06,
        "epoch": 0.9593859453014383,
        "step": 12874
    },
    {
        "loss": 1.7195,
        "grad_norm": 3.282614231109619,
        "learning_rate": 7.3958761981550825e-06,
        "epoch": 0.95946046650272,
        "step": 12875
    },
    {
        "loss": 2.129,
        "grad_norm": 2.4085097312927246,
        "learning_rate": 7.369341549588571e-06,
        "epoch": 0.9595349877040018,
        "step": 12876
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.4590587615966797,
        "learning_rate": 7.3428527654941574e-06,
        "epoch": 0.9596095089052835,
        "step": 12877
    },
    {
        "loss": 2.6367,
        "grad_norm": 2.458587169647217,
        "learning_rate": 7.316409858987206e-06,
        "epoch": 0.9596840301065653,
        "step": 12878
    },
    {
        "loss": 2.556,
        "grad_norm": 1.7973463535308838,
        "learning_rate": 7.290012843160599e-06,
        "epoch": 0.959758551307847,
        "step": 12879
    },
    {
        "loss": 2.6486,
        "grad_norm": 2.1297805309295654,
        "learning_rate": 7.263661731084303e-06,
        "epoch": 0.9598330725091289,
        "step": 12880
    },
    {
        "loss": 1.9164,
        "grad_norm": 2.9578497409820557,
        "learning_rate": 7.237356535805562e-06,
        "epoch": 0.9599075937104106,
        "step": 12881
    },
    {
        "loss": 1.9378,
        "grad_norm": 2.6818714141845703,
        "learning_rate": 7.211097270349099e-06,
        "epoch": 0.9599821149116924,
        "step": 12882
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.6278408765792847,
        "learning_rate": 7.1848839477165364e-06,
        "epoch": 0.9600566361129741,
        "step": 12883
    },
    {
        "loss": 2.085,
        "grad_norm": 3.3477540016174316,
        "learning_rate": 7.158716580887093e-06,
        "epoch": 0.9601311573142559,
        "step": 12884
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.5795063972473145,
        "learning_rate": 7.132595182817059e-06,
        "epoch": 0.9602056785155376,
        "step": 12885
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.9150218963623047,
        "learning_rate": 7.106519766439912e-06,
        "epoch": 0.9602801997168194,
        "step": 12886
    },
    {
        "loss": 2.3175,
        "grad_norm": 2.1154229640960693,
        "learning_rate": 7.080490344666546e-06,
        "epoch": 0.9603547209181011,
        "step": 12887
    },
    {
        "loss": 2.0029,
        "grad_norm": 3.543161630630493,
        "learning_rate": 7.054506930384896e-06,
        "epoch": 0.960429242119383,
        "step": 12888
    },
    {
        "loss": 2.5601,
        "grad_norm": 2.3714370727539062,
        "learning_rate": 7.028569536460305e-06,
        "epoch": 0.9605037633206648,
        "step": 12889
    },
    {
        "loss": 2.3727,
        "grad_norm": 3.287590742111206,
        "learning_rate": 7.002678175735167e-06,
        "epoch": 0.9605782845219465,
        "step": 12890
    },
    {
        "loss": 2.6871,
        "grad_norm": 2.665426015853882,
        "learning_rate": 6.976832861029137e-06,
        "epoch": 0.9606528057232283,
        "step": 12891
    },
    {
        "loss": 1.6292,
        "grad_norm": 3.8345067501068115,
        "learning_rate": 6.951033605139045e-06,
        "epoch": 0.96072732692451,
        "step": 12892
    },
    {
        "loss": 2.5721,
        "grad_norm": 3.4714207649230957,
        "learning_rate": 6.925280420839042e-06,
        "epoch": 0.9608018481257918,
        "step": 12893
    },
    {
        "loss": 2.7266,
        "grad_norm": 2.4292478561401367,
        "learning_rate": 6.899573320880303e-06,
        "epoch": 0.9608763693270735,
        "step": 12894
    },
    {
        "loss": 2.2454,
        "grad_norm": 3.1678240299224854,
        "learning_rate": 6.87391231799126e-06,
        "epoch": 0.9609508905283554,
        "step": 12895
    },
    {
        "loss": 2.5667,
        "grad_norm": 3.3178627490997314,
        "learning_rate": 6.848297424877492e-06,
        "epoch": 0.9610254117296371,
        "step": 12896
    },
    {
        "loss": 2.441,
        "grad_norm": 3.784435272216797,
        "learning_rate": 6.822728654221844e-06,
        "epoch": 0.9610999329309189,
        "step": 12897
    },
    {
        "loss": 2.3951,
        "grad_norm": 2.700859785079956,
        "learning_rate": 6.797206018684166e-06,
        "epoch": 0.9611744541322006,
        "step": 12898
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.528254985809326,
        "learning_rate": 6.771729530901638e-06,
        "epoch": 0.9612489753334824,
        "step": 12899
    },
    {
        "loss": 2.6692,
        "grad_norm": 3.4631361961364746,
        "learning_rate": 6.746299203488482e-06,
        "epoch": 0.9613234965347641,
        "step": 12900
    },
    {
        "loss": 2.3542,
        "grad_norm": 3.296220541000366,
        "learning_rate": 6.7209150490359914e-06,
        "epoch": 0.9613980177360459,
        "step": 12901
    },
    {
        "loss": 2.3257,
        "grad_norm": 2.3949859142303467,
        "learning_rate": 6.6955770801128694e-06,
        "epoch": 0.9614725389373276,
        "step": 12902
    },
    {
        "loss": 3.0503,
        "grad_norm": 3.809741258621216,
        "learning_rate": 6.670285309264579e-06,
        "epoch": 0.9615470601386095,
        "step": 12903
    },
    {
        "loss": 2.1261,
        "grad_norm": 2.067988395690918,
        "learning_rate": 6.645039749014037e-06,
        "epoch": 0.9616215813398912,
        "step": 12904
    },
    {
        "loss": 2.069,
        "grad_norm": 2.6128151416778564,
        "learning_rate": 6.619840411861111e-06,
        "epoch": 0.961696102541173,
        "step": 12905
    },
    {
        "loss": 1.5146,
        "grad_norm": 2.5953292846679688,
        "learning_rate": 6.5946873102827545e-06,
        "epoch": 0.9617706237424547,
        "step": 12906
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.877542495727539,
        "learning_rate": 6.5695804567332044e-06,
        "epoch": 0.9618451449437365,
        "step": 12907
    },
    {
        "loss": 1.9621,
        "grad_norm": 3.2376976013183594,
        "learning_rate": 6.544519863643606e-06,
        "epoch": 0.9619196661450182,
        "step": 12908
    },
    {
        "loss": 2.3528,
        "grad_norm": 3.144134044647217,
        "learning_rate": 6.519505543422322e-06,
        "epoch": 0.9619941873463,
        "step": 12909
    },
    {
        "loss": 1.2789,
        "grad_norm": 3.6085336208343506,
        "learning_rate": 6.494537508454756e-06,
        "epoch": 0.9620687085475818,
        "step": 12910
    },
    {
        "loss": 0.8071,
        "grad_norm": 3.2737956047058105,
        "learning_rate": 6.4696157711033766e-06,
        "epoch": 0.9621432297488636,
        "step": 12911
    },
    {
        "loss": 2.6998,
        "grad_norm": 3.67838716506958,
        "learning_rate": 6.444740343707722e-06,
        "epoch": 0.9622177509501453,
        "step": 12912
    },
    {
        "loss": 2.7338,
        "grad_norm": 2.6120846271514893,
        "learning_rate": 6.419911238584564e-06,
        "epoch": 0.9622922721514271,
        "step": 12913
    },
    {
        "loss": 2.8667,
        "grad_norm": 2.7125275135040283,
        "learning_rate": 6.395128468027423e-06,
        "epoch": 0.9623667933527088,
        "step": 12914
    },
    {
        "loss": 1.8824,
        "grad_norm": 3.4335639476776123,
        "learning_rate": 6.370392044307216e-06,
        "epoch": 0.9624413145539906,
        "step": 12915
    },
    {
        "loss": 2.338,
        "grad_norm": 2.6570825576782227,
        "learning_rate": 6.345701979671626e-06,
        "epoch": 0.9625158357552723,
        "step": 12916
    },
    {
        "loss": 2.241,
        "grad_norm": 2.8940486907958984,
        "learning_rate": 6.321058286345616e-06,
        "epoch": 0.9625903569565541,
        "step": 12917
    },
    {
        "loss": 2.7957,
        "grad_norm": 2.4252443313598633,
        "learning_rate": 6.29646097653106e-06,
        "epoch": 0.9626648781578359,
        "step": 12918
    },
    {
        "loss": 2.431,
        "grad_norm": 2.7440481185913086,
        "learning_rate": 6.271910062406816e-06,
        "epoch": 0.9627393993591177,
        "step": 12919
    },
    {
        "loss": 2.3692,
        "grad_norm": 2.052334785461426,
        "learning_rate": 6.247405556128971e-06,
        "epoch": 0.9628139205603994,
        "step": 12920
    },
    {
        "loss": 2.5735,
        "grad_norm": 2.3124260902404785,
        "learning_rate": 6.2229474698303865e-06,
        "epoch": 0.9628884417616812,
        "step": 12921
    },
    {
        "loss": 2.0837,
        "grad_norm": 2.9331514835357666,
        "learning_rate": 6.198535815621232e-06,
        "epoch": 0.9629629629629629,
        "step": 12922
    },
    {
        "loss": 1.7255,
        "grad_norm": 3.0962491035461426,
        "learning_rate": 6.1741706055883034e-06,
        "epoch": 0.9630374841642447,
        "step": 12923
    },
    {
        "loss": 2.131,
        "grad_norm": 2.9246673583984375,
        "learning_rate": 6.149851851795751e-06,
        "epoch": 0.9631120053655265,
        "step": 12924
    },
    {
        "loss": 1.9832,
        "grad_norm": 2.1127405166625977,
        "learning_rate": 6.1255795662845675e-06,
        "epoch": 0.9631865265668083,
        "step": 12925
    },
    {
        "loss": 2.6788,
        "grad_norm": 3.784261703491211,
        "learning_rate": 6.101353761072681e-06,
        "epoch": 0.9632610477680901,
        "step": 12926
    },
    {
        "loss": 2.5185,
        "grad_norm": 1.970499038696289,
        "learning_rate": 6.077174448155187e-06,
        "epoch": 0.9633355689693718,
        "step": 12927
    },
    {
        "loss": 2.584,
        "grad_norm": 3.584097146987915,
        "learning_rate": 6.0530416395040094e-06,
        "epoch": 0.9634100901706536,
        "step": 12928
    },
    {
        "loss": 1.8995,
        "grad_norm": 5.175537109375,
        "learning_rate": 6.028955347068044e-06,
        "epoch": 0.9634846113719353,
        "step": 12929
    },
    {
        "loss": 2.8115,
        "grad_norm": 3.0958499908447266,
        "learning_rate": 6.004915582773296e-06,
        "epoch": 0.9635591325732171,
        "step": 12930
    },
    {
        "loss": 2.7479,
        "grad_norm": 3.0471343994140625,
        "learning_rate": 5.9809223585226115e-06,
        "epoch": 0.9636336537744988,
        "step": 12931
    },
    {
        "loss": 2.6608,
        "grad_norm": 3.629570722579956,
        "learning_rate": 5.9569756861957425e-06,
        "epoch": 0.9637081749757807,
        "step": 12932
    },
    {
        "loss": 2.4357,
        "grad_norm": 3.6204776763916016,
        "learning_rate": 5.9330755776496165e-06,
        "epoch": 0.9637826961770624,
        "step": 12933
    },
    {
        "loss": 2.3709,
        "grad_norm": 2.615272045135498,
        "learning_rate": 5.9092220447177885e-06,
        "epoch": 0.9638572173783442,
        "step": 12934
    },
    {
        "loss": 2.7763,
        "grad_norm": 2.332505226135254,
        "learning_rate": 5.8854150992110555e-06,
        "epoch": 0.9639317385796259,
        "step": 12935
    },
    {
        "loss": 2.6364,
        "grad_norm": 2.4953372478485107,
        "learning_rate": 5.861654752916923e-06,
        "epoch": 0.9640062597809077,
        "step": 12936
    },
    {
        "loss": 1.7013,
        "grad_norm": 4.209834098815918,
        "learning_rate": 5.837941017600001e-06,
        "epoch": 0.9640807809821894,
        "step": 12937
    },
    {
        "loss": 2.6991,
        "grad_norm": 3.109168767929077,
        "learning_rate": 5.814273905001721e-06,
        "epoch": 0.9641553021834712,
        "step": 12938
    },
    {
        "loss": 2.8643,
        "grad_norm": 2.0974154472351074,
        "learning_rate": 5.790653426840365e-06,
        "epoch": 0.9642298233847529,
        "step": 12939
    },
    {
        "loss": 1.8841,
        "grad_norm": 1.754842758178711,
        "learning_rate": 5.767079594811309e-06,
        "epoch": 0.9643043445860348,
        "step": 12940
    },
    {
        "loss": 3.0108,
        "grad_norm": 2.9689972400665283,
        "learning_rate": 5.743552420586684e-06,
        "epoch": 0.9643788657873165,
        "step": 12941
    },
    {
        "loss": 2.4666,
        "grad_norm": 2.001601219177246,
        "learning_rate": 5.72007191581555e-06,
        "epoch": 0.9644533869885983,
        "step": 12942
    },
    {
        "loss": 2.4605,
        "grad_norm": 2.000312089920044,
        "learning_rate": 5.696638092123863e-06,
        "epoch": 0.96452790818988,
        "step": 12943
    },
    {
        "loss": 2.059,
        "grad_norm": 2.5514049530029297,
        "learning_rate": 5.673250961114529e-06,
        "epoch": 0.9646024293911618,
        "step": 12944
    },
    {
        "loss": 2.7766,
        "grad_norm": 2.311626434326172,
        "learning_rate": 5.649910534367275e-06,
        "epoch": 0.9646769505924435,
        "step": 12945
    },
    {
        "loss": 2.3532,
        "grad_norm": 3.2121710777282715,
        "learning_rate": 5.626616823438679e-06,
        "epoch": 0.9647514717937253,
        "step": 12946
    },
    {
        "loss": 2.023,
        "grad_norm": 2.864549398422241,
        "learning_rate": 5.603369839862183e-06,
        "epoch": 0.964825992995007,
        "step": 12947
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.218315362930298,
        "learning_rate": 5.580169595148222e-06,
        "epoch": 0.9649005141962889,
        "step": 12948
    },
    {
        "loss": 2.0471,
        "grad_norm": 3.0676991939544678,
        "learning_rate": 5.557016100783907e-06,
        "epoch": 0.9649750353975706,
        "step": 12949
    },
    {
        "loss": 2.5724,
        "grad_norm": 2.3369994163513184,
        "learning_rate": 5.533909368233381e-06,
        "epoch": 0.9650495565988524,
        "step": 12950
    },
    {
        "loss": 2.4025,
        "grad_norm": 2.6423873901367188,
        "learning_rate": 5.5108494089375015e-06,
        "epoch": 0.9651240778001341,
        "step": 12951
    },
    {
        "loss": 2.7597,
        "grad_norm": 2.6998326778411865,
        "learning_rate": 5.487836234313959e-06,
        "epoch": 0.9651985990014159,
        "step": 12952
    },
    {
        "loss": 1.718,
        "grad_norm": 2.4861843585968018,
        "learning_rate": 5.464869855757493e-06,
        "epoch": 0.9652731202026976,
        "step": 12953
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.4345349073410034,
        "learning_rate": 5.441950284639275e-06,
        "epoch": 0.9653476414039794,
        "step": 12954
    },
    {
        "loss": 2.3064,
        "grad_norm": 2.9067578315734863,
        "learning_rate": 5.419077532307715e-06,
        "epoch": 0.9654221626052611,
        "step": 12955
    },
    {
        "loss": 2.2628,
        "grad_norm": 3.1976592540740967,
        "learning_rate": 5.396251610087788e-06,
        "epoch": 0.965496683806543,
        "step": 12956
    },
    {
        "loss": 2.5752,
        "grad_norm": 3.0240557193756104,
        "learning_rate": 5.373472529281332e-06,
        "epoch": 0.9655712050078247,
        "step": 12957
    },
    {
        "loss": 2.4251,
        "grad_norm": 1.6340069770812988,
        "learning_rate": 5.35074030116709e-06,
        "epoch": 0.9656457262091065,
        "step": 12958
    },
    {
        "loss": 2.8048,
        "grad_norm": 2.2079386711120605,
        "learning_rate": 5.328054937000459e-06,
        "epoch": 0.9657202474103883,
        "step": 12959
    },
    {
        "loss": 2.5236,
        "grad_norm": 3.61148738861084,
        "learning_rate": 5.305416448013789e-06,
        "epoch": 0.96579476861167,
        "step": 12960
    },
    {
        "loss": 2.3622,
        "grad_norm": 1.9148553609848022,
        "learning_rate": 5.28282484541609e-06,
        "epoch": 0.9658692898129518,
        "step": 12961
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.0293257236480713,
        "learning_rate": 5.2602801403932035e-06,
        "epoch": 0.9659438110142335,
        "step": 12962
    },
    {
        "loss": 2.1752,
        "grad_norm": 2.8541030883789062,
        "learning_rate": 5.2377823441077e-06,
        "epoch": 0.9660183322155154,
        "step": 12963
    },
    {
        "loss": 2.1906,
        "grad_norm": 2.7732603549957275,
        "learning_rate": 5.215331467699081e-06,
        "epoch": 0.9660928534167971,
        "step": 12964
    },
    {
        "loss": 2.4026,
        "grad_norm": 2.9834189414978027,
        "learning_rate": 5.192927522283441e-06,
        "epoch": 0.9661673746180789,
        "step": 12965
    },
    {
        "loss": 2.6199,
        "grad_norm": 2.7370400428771973,
        "learning_rate": 5.170570518953732e-06,
        "epoch": 0.9662418958193606,
        "step": 12966
    },
    {
        "loss": 2.4911,
        "grad_norm": 2.5199625492095947,
        "learning_rate": 5.148260468779587e-06,
        "epoch": 0.9663164170206424,
        "step": 12967
    },
    {
        "loss": 2.5924,
        "grad_norm": 2.5531771183013916,
        "learning_rate": 5.125997382807535e-06,
        "epoch": 0.9663909382219241,
        "step": 12968
    },
    {
        "loss": 2.313,
        "grad_norm": 2.4061007499694824,
        "learning_rate": 5.103781272060659e-06,
        "epoch": 0.9664654594232059,
        "step": 12969
    },
    {
        "loss": 2.56,
        "grad_norm": 2.102722644805908,
        "learning_rate": 5.081612147538961e-06,
        "epoch": 0.9665399806244876,
        "step": 12970
    },
    {
        "loss": 2.5414,
        "grad_norm": 2.4879465103149414,
        "learning_rate": 5.059490020219082e-06,
        "epoch": 0.9666145018257695,
        "step": 12971
    },
    {
        "loss": 2.6347,
        "grad_norm": 1.8007296323776245,
        "learning_rate": 5.03741490105436e-06,
        "epoch": 0.9666890230270512,
        "step": 12972
    },
    {
        "loss": 2.7447,
        "grad_norm": 2.0209453105926514,
        "learning_rate": 5.0153868009750305e-06,
        "epoch": 0.966763544228333,
        "step": 12973
    },
    {
        "loss": 2.3571,
        "grad_norm": 3.1181752681732178,
        "learning_rate": 4.993405730887768e-06,
        "epoch": 0.9668380654296147,
        "step": 12974
    },
    {
        "loss": 2.0543,
        "grad_norm": 3.946363925933838,
        "learning_rate": 4.971471701676233e-06,
        "epoch": 0.9669125866308965,
        "step": 12975
    },
    {
        "loss": 2.4802,
        "grad_norm": 2.1686017513275146,
        "learning_rate": 4.949584724200662e-06,
        "epoch": 0.9669871078321782,
        "step": 12976
    },
    {
        "loss": 1.9463,
        "grad_norm": 2.9555675983428955,
        "learning_rate": 4.927744809297952e-06,
        "epoch": 0.96706162903346,
        "step": 12977
    },
    {
        "loss": 2.3911,
        "grad_norm": 3.2634170055389404,
        "learning_rate": 4.905951967781841e-06,
        "epoch": 0.9671361502347418,
        "step": 12978
    },
    {
        "loss": 2.4596,
        "grad_norm": 2.8476877212524414,
        "learning_rate": 4.884206210442654e-06,
        "epoch": 0.9672106714360236,
        "step": 12979
    },
    {
        "loss": 1.998,
        "grad_norm": 3.371711492538452,
        "learning_rate": 4.862507548047368e-06,
        "epoch": 0.9672851926373053,
        "step": 12980
    },
    {
        "loss": 2.3852,
        "grad_norm": 2.7414374351501465,
        "learning_rate": 4.840855991339799e-06,
        "epoch": 0.9673597138385871,
        "step": 12981
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.2075111865997314,
        "learning_rate": 4.819251551040293e-06,
        "epoch": 0.9674342350398688,
        "step": 12982
    },
    {
        "loss": 2.6406,
        "grad_norm": 3.5488643646240234,
        "learning_rate": 4.797694237845873e-06,
        "epoch": 0.9675087562411506,
        "step": 12983
    },
    {
        "loss": 2.2667,
        "grad_norm": 2.8479127883911133,
        "learning_rate": 4.7761840624304114e-06,
        "epoch": 0.9675832774424323,
        "step": 12984
    },
    {
        "loss": 2.1222,
        "grad_norm": 2.947857618331909,
        "learning_rate": 4.754721035444109e-06,
        "epoch": 0.9676577986437142,
        "step": 12985
    },
    {
        "loss": 2.2224,
        "grad_norm": 2.593505859375,
        "learning_rate": 4.733305167514157e-06,
        "epoch": 0.9677323198449959,
        "step": 12986
    },
    {
        "loss": 1.2674,
        "grad_norm": 1.5943689346313477,
        "learning_rate": 4.711936469244149e-06,
        "epoch": 0.9678068410462777,
        "step": 12987
    },
    {
        "loss": 2.4045,
        "grad_norm": 2.4452016353607178,
        "learning_rate": 4.690614951214533e-06,
        "epoch": 0.9678813622475594,
        "step": 12988
    },
    {
        "loss": 2.4437,
        "grad_norm": 2.261518716812134,
        "learning_rate": 4.669340623982243e-06,
        "epoch": 0.9679558834488412,
        "step": 12989
    },
    {
        "loss": 2.2429,
        "grad_norm": 3.7655601501464844,
        "learning_rate": 4.648113498080842e-06,
        "epoch": 0.9680304046501229,
        "step": 12990
    },
    {
        "loss": 2.3117,
        "grad_norm": 4.587648391723633,
        "learning_rate": 4.6269335840206875e-06,
        "epoch": 0.9681049258514047,
        "step": 12991
    },
    {
        "loss": 1.8985,
        "grad_norm": 2.9176812171936035,
        "learning_rate": 4.605800892288525e-06,
        "epoch": 0.9681794470526864,
        "step": 12992
    },
    {
        "loss": 2.5277,
        "grad_norm": 3.737922191619873,
        "learning_rate": 4.584715433347986e-06,
        "epoch": 0.9682539682539683,
        "step": 12993
    },
    {
        "loss": 1.5222,
        "grad_norm": 3.9363696575164795,
        "learning_rate": 4.563677217639007e-06,
        "epoch": 0.96832848945525,
        "step": 12994
    },
    {
        "loss": 2.7721,
        "grad_norm": 2.9394729137420654,
        "learning_rate": 4.542686255578421e-06,
        "epoch": 0.9684030106565318,
        "step": 12995
    },
    {
        "loss": 2.6127,
        "grad_norm": 3.221425771713257,
        "learning_rate": 4.5217425575595054e-06,
        "epoch": 0.9684775318578136,
        "step": 12996
    },
    {
        "loss": 2.5531,
        "grad_norm": 3.3091928958892822,
        "learning_rate": 4.500846133952108e-06,
        "epoch": 0.9685520530590953,
        "step": 12997
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.5733115673065186,
        "learning_rate": 4.479996995102853e-06,
        "epoch": 0.9686265742603771,
        "step": 12998
    },
    {
        "loss": 2.6011,
        "grad_norm": 2.8605873584747314,
        "learning_rate": 4.4591951513347604e-06,
        "epoch": 0.9687010954616588,
        "step": 12999
    },
    {
        "loss": 1.8963,
        "grad_norm": 3.290767192840576,
        "learning_rate": 4.438440612947459e-06,
        "epoch": 0.9687756166629407,
        "step": 13000
    },
    {
        "loss": 2.2955,
        "grad_norm": 2.8752498626708984,
        "learning_rate": 4.417733390217305e-06,
        "epoch": 0.9688501378642224,
        "step": 13001
    },
    {
        "loss": 1.66,
        "grad_norm": 2.9774386882781982,
        "learning_rate": 4.397073493397053e-06,
        "epoch": 0.9689246590655042,
        "step": 13002
    },
    {
        "loss": 2.3842,
        "grad_norm": 3.095034122467041,
        "learning_rate": 4.376460932716086e-06,
        "epoch": 0.9689991802667859,
        "step": 13003
    },
    {
        "loss": 2.1664,
        "grad_norm": 3.5364842414855957,
        "learning_rate": 4.355895718380454e-06,
        "epoch": 0.9690737014680677,
        "step": 13004
    },
    {
        "loss": 2.7487,
        "grad_norm": 2.037951946258545,
        "learning_rate": 4.33537786057252e-06,
        "epoch": 0.9691482226693494,
        "step": 13005
    },
    {
        "loss": 2.4327,
        "grad_norm": 3.7362027168273926,
        "learning_rate": 4.314907369451482e-06,
        "epoch": 0.9692227438706312,
        "step": 13006
    },
    {
        "loss": 2.6088,
        "grad_norm": 2.418217658996582,
        "learning_rate": 4.294484255152875e-06,
        "epoch": 0.9692972650719129,
        "step": 13007
    },
    {
        "loss": 2.6047,
        "grad_norm": 2.4060401916503906,
        "learning_rate": 4.274108527788834e-06,
        "epoch": 0.9693717862731948,
        "step": 13008
    },
    {
        "loss": 2.5425,
        "grad_norm": 3.6518585681915283,
        "learning_rate": 4.253780197448121e-06,
        "epoch": 0.9694463074744765,
        "step": 13009
    },
    {
        "loss": 1.7087,
        "grad_norm": 3.7496814727783203,
        "learning_rate": 4.233499274195874e-06,
        "epoch": 0.9695208286757583,
        "step": 13010
    },
    {
        "loss": 2.4076,
        "grad_norm": 3.3098385334014893,
        "learning_rate": 4.213265768073937e-06,
        "epoch": 0.96959534987704,
        "step": 13011
    },
    {
        "loss": 2.3637,
        "grad_norm": 2.6822447776794434,
        "learning_rate": 4.193079689100532e-06,
        "epoch": 0.9696698710783218,
        "step": 13012
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.8199317455291748,
        "learning_rate": 4.1729410472704625e-06,
        "epoch": 0.9697443922796035,
        "step": 13013
    },
    {
        "loss": 2.4518,
        "grad_norm": 3.321371555328369,
        "learning_rate": 4.152849852554963e-06,
        "epoch": 0.9698189134808853,
        "step": 13014
    },
    {
        "loss": 1.9709,
        "grad_norm": 2.808082342147827,
        "learning_rate": 4.132806114901944e-06,
        "epoch": 0.969893434682167,
        "step": 13015
    },
    {
        "loss": 2.0027,
        "grad_norm": 2.9694974422454834,
        "learning_rate": 4.112809844235654e-06,
        "epoch": 0.9699679558834489,
        "step": 13016
    },
    {
        "loss": 2.4863,
        "grad_norm": 3.217172384262085,
        "learning_rate": 4.092861050456919e-06,
        "epoch": 0.9700424770847306,
        "step": 13017
    },
    {
        "loss": 1.6294,
        "grad_norm": 4.033112525939941,
        "learning_rate": 4.072959743443006e-06,
        "epoch": 0.9701169982860124,
        "step": 13018
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.3554248809814453,
        "learning_rate": 4.053105933047763e-06,
        "epoch": 0.9701915194872941,
        "step": 13019
    },
    {
        "loss": 2.5568,
        "grad_norm": 2.8846359252929688,
        "learning_rate": 4.033299629101394e-06,
        "epoch": 0.9702660406885759,
        "step": 13020
    },
    {
        "loss": 2.2341,
        "grad_norm": 1.9614386558532715,
        "learning_rate": 4.013540841410735e-06,
        "epoch": 0.9703405618898576,
        "step": 13021
    },
    {
        "loss": 1.985,
        "grad_norm": 3.0979487895965576,
        "learning_rate": 3.993829579758956e-06,
        "epoch": 0.9704150830911394,
        "step": 13022
    },
    {
        "loss": 2.6604,
        "grad_norm": 2.5906519889831543,
        "learning_rate": 3.974165853905709e-06,
        "epoch": 0.9704896042924211,
        "step": 13023
    },
    {
        "loss": 1.998,
        "grad_norm": 2.4463939666748047,
        "learning_rate": 3.954549673587271e-06,
        "epoch": 0.970564125493703,
        "step": 13024
    },
    {
        "loss": 2.1629,
        "grad_norm": 3.411243438720703,
        "learning_rate": 3.934981048516107e-06,
        "epoch": 0.9706386466949847,
        "step": 13025
    },
    {
        "loss": 2.6535,
        "grad_norm": 3.1461193561553955,
        "learning_rate": 3.915459988381376e-06,
        "epoch": 0.9707131678962665,
        "step": 13026
    },
    {
        "loss": 2.9947,
        "grad_norm": 3.2567527294158936,
        "learning_rate": 3.895986502848581e-06,
        "epoch": 0.9707876890975482,
        "step": 13027
    },
    {
        "loss": 1.8118,
        "grad_norm": 4.672231674194336,
        "learning_rate": 3.876560601559642e-06,
        "epoch": 0.97086221029883,
        "step": 13028
    },
    {
        "loss": 2.1429,
        "grad_norm": 3.588487148284912,
        "learning_rate": 3.857182294133022e-06,
        "epoch": 0.9709367315001117,
        "step": 13029
    },
    {
        "loss": 2.6094,
        "grad_norm": 2.6382787227630615,
        "learning_rate": 3.8378515901634884e-06,
        "epoch": 0.9710112527013935,
        "step": 13030
    },
    {
        "loss": 1.8589,
        "grad_norm": 3.207263469696045,
        "learning_rate": 3.8185684992223745e-06,
        "epoch": 0.9710857739026754,
        "step": 13031
    },
    {
        "loss": 1.9946,
        "grad_norm": 2.955522060394287,
        "learning_rate": 3.799333030857333e-06,
        "epoch": 0.9711602951039571,
        "step": 13032
    },
    {
        "loss": 2.2843,
        "grad_norm": 2.605672597885132,
        "learning_rate": 3.780145194592477e-06,
        "epoch": 0.9712348163052389,
        "step": 13033
    },
    {
        "loss": 2.2194,
        "grad_norm": 2.5815253257751465,
        "learning_rate": 3.761004999928297e-06,
        "epoch": 0.9713093375065206,
        "step": 13034
    },
    {
        "loss": 2.1629,
        "grad_norm": 3.4704720973968506,
        "learning_rate": 3.741912456341845e-06,
        "epoch": 0.9713838587078024,
        "step": 13035
    },
    {
        "loss": 2.6242,
        "grad_norm": 2.6105101108551025,
        "learning_rate": 3.722867573286315e-06,
        "epoch": 0.9714583799090841,
        "step": 13036
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.69350528717041,
        "learning_rate": 3.7038703601915415e-06,
        "epoch": 0.9715329011103659,
        "step": 13037
    },
    {
        "loss": 2.1114,
        "grad_norm": 3.6952946186065674,
        "learning_rate": 3.6849208264636227e-06,
        "epoch": 0.9716074223116477,
        "step": 13038
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.7734344005584717,
        "learning_rate": 3.6660189814851532e-06,
        "epoch": 0.9716819435129295,
        "step": 13039
    },
    {
        "loss": 2.2975,
        "grad_norm": 3.4133100509643555,
        "learning_rate": 3.6471648346150134e-06,
        "epoch": 0.9717564647142112,
        "step": 13040
    },
    {
        "loss": 2.3497,
        "grad_norm": 2.744295597076416,
        "learning_rate": 3.6283583951884913e-06,
        "epoch": 0.971830985915493,
        "step": 13041
    },
    {
        "loss": 2.1633,
        "grad_norm": 3.695509672164917,
        "learning_rate": 3.6095996725173163e-06,
        "epoch": 0.9719055071167747,
        "step": 13042
    },
    {
        "loss": 2.9445,
        "grad_norm": 2.199021339416504,
        "learning_rate": 3.590888675889503e-06,
        "epoch": 0.9719800283180565,
        "step": 13043
    },
    {
        "loss": 2.7566,
        "grad_norm": 4.038364887237549,
        "learning_rate": 3.5722254145695745e-06,
        "epoch": 0.9720545495193382,
        "step": 13044
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.8857102394104004,
        "learning_rate": 3.5536098977981714e-06,
        "epoch": 0.97212907072062,
        "step": 13045
    },
    {
        "loss": 2.3122,
        "grad_norm": 2.9518117904663086,
        "learning_rate": 3.5350421347925655e-06,
        "epoch": 0.9722035919219018,
        "step": 13046
    },
    {
        "loss": 1.9604,
        "grad_norm": 3.2002546787261963,
        "learning_rate": 3.516522134746203e-06,
        "epoch": 0.9722781131231836,
        "step": 13047
    },
    {
        "loss": 2.6221,
        "grad_norm": 3.8776469230651855,
        "learning_rate": 3.4980499068289376e-06,
        "epoch": 0.9723526343244653,
        "step": 13048
    },
    {
        "loss": 2.5687,
        "grad_norm": 3.583806276321411,
        "learning_rate": 3.4796254601870305e-06,
        "epoch": 0.9724271555257471,
        "step": 13049
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.104862689971924,
        "learning_rate": 3.461248803942996e-06,
        "epoch": 0.9725016767270288,
        "step": 13050
    },
    {
        "loss": 2.3613,
        "grad_norm": 3.569598913192749,
        "learning_rate": 3.4429199471956885e-06,
        "epoch": 0.9725761979283106,
        "step": 13051
    },
    {
        "loss": 1.1963,
        "grad_norm": 4.0935258865356445,
        "learning_rate": 3.4246388990203936e-06,
        "epoch": 0.9726507191295923,
        "step": 13052
    },
    {
        "loss": 2.2405,
        "grad_norm": 2.4594385623931885,
        "learning_rate": 3.4064056684686154e-06,
        "epoch": 0.9727252403308742,
        "step": 13053
    },
    {
        "loss": 1.888,
        "grad_norm": 3.141573190689087,
        "learning_rate": 3.388220264568176e-06,
        "epoch": 0.9727997615321559,
        "step": 13054
    },
    {
        "loss": 2.243,
        "grad_norm": 2.1010313034057617,
        "learning_rate": 3.3700826963233955e-06,
        "epoch": 0.9728742827334377,
        "step": 13055
    },
    {
        "loss": 2.4935,
        "grad_norm": 2.520655393600464,
        "learning_rate": 3.3519929727146237e-06,
        "epoch": 0.9729488039347194,
        "step": 13056
    },
    {
        "loss": 2.5513,
        "grad_norm": 1.918468952178955,
        "learning_rate": 3.3339511026987844e-06,
        "epoch": 0.9730233251360012,
        "step": 13057
    },
    {
        "loss": 2.4574,
        "grad_norm": 2.866006851196289,
        "learning_rate": 3.3159570952089215e-06,
        "epoch": 0.9730978463372829,
        "step": 13058
    },
    {
        "loss": 2.415,
        "grad_norm": 3.726918935775757,
        "learning_rate": 3.298010959154552e-06,
        "epoch": 0.9731723675385647,
        "step": 13059
    },
    {
        "loss": 2.5187,
        "grad_norm": 3.5151562690734863,
        "learning_rate": 3.280112703421323e-06,
        "epoch": 0.9732468887398464,
        "step": 13060
    },
    {
        "loss": 2.2513,
        "grad_norm": 2.8189899921417236,
        "learning_rate": 3.262262336871236e-06,
        "epoch": 0.9733214099411283,
        "step": 13061
    },
    {
        "loss": 1.4981,
        "grad_norm": 3.947826862335205,
        "learning_rate": 3.244459868342653e-06,
        "epoch": 0.97339593114241,
        "step": 13062
    },
    {
        "loss": 2.0266,
        "grad_norm": 3.545450210571289,
        "learning_rate": 3.226705306650091e-06,
        "epoch": 0.9734704523436918,
        "step": 13063
    },
    {
        "loss": 2.3932,
        "grad_norm": 2.818995714187622,
        "learning_rate": 3.208998660584517e-06,
        "epoch": 0.9735449735449735,
        "step": 13064
    },
    {
        "loss": 2.8225,
        "grad_norm": 2.694525718688965,
        "learning_rate": 3.1913399389129407e-06,
        "epoch": 0.9736194947462553,
        "step": 13065
    },
    {
        "loss": 2.7785,
        "grad_norm": 2.5458452701568604,
        "learning_rate": 3.173729150378879e-06,
        "epoch": 0.9736940159475371,
        "step": 13066
    },
    {
        "loss": 2.0521,
        "grad_norm": 4.029555320739746,
        "learning_rate": 3.156166303701957e-06,
        "epoch": 0.9737685371488188,
        "step": 13067
    },
    {
        "loss": 2.8603,
        "grad_norm": 1.979956030845642,
        "learning_rate": 3.13865140757813e-06,
        "epoch": 0.9738430583501007,
        "step": 13068
    },
    {
        "loss": 2.8791,
        "grad_norm": 3.2658884525299072,
        "learning_rate": 3.121184470679561e-06,
        "epoch": 0.9739175795513824,
        "step": 13069
    },
    {
        "loss": 2.5444,
        "grad_norm": 2.961843729019165,
        "learning_rate": 3.1037655016547763e-06,
        "epoch": 0.9739921007526642,
        "step": 13070
    },
    {
        "loss": 2.2943,
        "grad_norm": 2.8085243701934814,
        "learning_rate": 3.0863945091283874e-06,
        "epoch": 0.9740666219539459,
        "step": 13071
    },
    {
        "loss": 2.355,
        "grad_norm": 4.475316047668457,
        "learning_rate": 3.069071501701437e-06,
        "epoch": 0.9741411431552277,
        "step": 13072
    },
    {
        "loss": 2.5873,
        "grad_norm": 2.0915417671203613,
        "learning_rate": 3.051796487951053e-06,
        "epoch": 0.9742156643565094,
        "step": 13073
    },
    {
        "loss": 2.4,
        "grad_norm": 3.3434860706329346,
        "learning_rate": 3.034569476430649e-06,
        "epoch": 0.9742901855577912,
        "step": 13074
    },
    {
        "loss": 2.7091,
        "grad_norm": 2.8138787746429443,
        "learning_rate": 3.01739047566999e-06,
        "epoch": 0.9743647067590729,
        "step": 13075
    },
    {
        "loss": 1.7769,
        "grad_norm": 3.238053321838379,
        "learning_rate": 3.000259494174795e-06,
        "epoch": 0.9744392279603548,
        "step": 13076
    },
    {
        "loss": 2.5074,
        "grad_norm": 2.4384264945983887,
        "learning_rate": 2.9831765404272905e-06,
        "epoch": 0.9745137491616365,
        "step": 13077
    },
    {
        "loss": 2.8162,
        "grad_norm": 2.237135887145996,
        "learning_rate": 2.9661416228857765e-06,
        "epoch": 0.9745882703629183,
        "step": 13078
    },
    {
        "loss": 2.4922,
        "grad_norm": 3.1186673641204834,
        "learning_rate": 2.9491547499847615e-06,
        "epoch": 0.9746627915642,
        "step": 13079
    },
    {
        "loss": 2.6061,
        "grad_norm": 2.2367255687713623,
        "learning_rate": 2.932215930135074e-06,
        "epoch": 0.9747373127654818,
        "step": 13080
    },
    {
        "loss": 2.5881,
        "grad_norm": 2.2821834087371826,
        "learning_rate": 2.915325171723604e-06,
        "epoch": 0.9748118339667635,
        "step": 13081
    },
    {
        "loss": 1.9217,
        "grad_norm": 3.0663604736328125,
        "learning_rate": 2.898482483113607e-06,
        "epoch": 0.9748863551680453,
        "step": 13082
    },
    {
        "loss": 1.9454,
        "grad_norm": 2.7280526161193848,
        "learning_rate": 2.8816878726443785e-06,
        "epoch": 0.974960876369327,
        "step": 13083
    },
    {
        "loss": 2.6312,
        "grad_norm": 2.533151865005493,
        "learning_rate": 2.8649413486315113e-06,
        "epoch": 0.9750353975706089,
        "step": 13084
    },
    {
        "loss": 1.8298,
        "grad_norm": 3.220595121383667,
        "learning_rate": 2.848242919366706e-06,
        "epoch": 0.9751099187718906,
        "step": 13085
    },
    {
        "loss": 2.3497,
        "grad_norm": 2.674513578414917,
        "learning_rate": 2.8315925931179708e-06,
        "epoch": 0.9751844399731724,
        "step": 13086
    },
    {
        "loss": 2.4043,
        "grad_norm": 4.839856147766113,
        "learning_rate": 2.814990378129412e-06,
        "epoch": 0.9752589611744541,
        "step": 13087
    },
    {
        "loss": 2.5045,
        "grad_norm": 1.9811666011810303,
        "learning_rate": 2.7984362826213083e-06,
        "epoch": 0.9753334823757359,
        "step": 13088
    },
    {
        "loss": 2.4995,
        "grad_norm": 2.6000030040740967,
        "learning_rate": 2.7819303147901264e-06,
        "epoch": 0.9754080035770176,
        "step": 13089
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.3975276947021484,
        "learning_rate": 2.765472482808551e-06,
        "epoch": 0.9754825247782994,
        "step": 13090
    },
    {
        "loss": 2.2958,
        "grad_norm": 2.9608588218688965,
        "learning_rate": 2.74906279482533e-06,
        "epoch": 0.9755570459795811,
        "step": 13091
    },
    {
        "loss": 1.7096,
        "grad_norm": 3.2445883750915527,
        "learning_rate": 2.73270125896552e-06,
        "epoch": 0.975631567180863,
        "step": 13092
    },
    {
        "loss": 2.242,
        "grad_norm": 2.79473614692688,
        "learning_rate": 2.716387883330207e-06,
        "epoch": 0.9757060883821447,
        "step": 13093
    },
    {
        "loss": 1.9228,
        "grad_norm": 3.4876325130462646,
        "learning_rate": 2.7001226759966524e-06,
        "epoch": 0.9757806095834265,
        "step": 13094
    },
    {
        "loss": 2.4894,
        "grad_norm": 2.185734510421753,
        "learning_rate": 2.683905645018381e-06,
        "epoch": 0.9758551307847082,
        "step": 13095
    },
    {
        "loss": 2.6455,
        "grad_norm": 2.7410497665405273,
        "learning_rate": 2.667736798424858e-06,
        "epoch": 0.97592965198599,
        "step": 13096
    },
    {
        "loss": 2.4741,
        "grad_norm": 2.7135608196258545,
        "learning_rate": 2.6516161442219136e-06,
        "epoch": 0.9760041731872717,
        "step": 13097
    },
    {
        "loss": 2.7211,
        "grad_norm": 2.671583652496338,
        "learning_rate": 2.635543690391351e-06,
        "epoch": 0.9760786943885535,
        "step": 13098
    },
    {
        "loss": 2.1241,
        "grad_norm": 4.041502475738525,
        "learning_rate": 2.6195194448911497e-06,
        "epoch": 0.9761532155898353,
        "step": 13099
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.0046727657318115,
        "learning_rate": 2.603543415655507e-06,
        "epoch": 0.9762277367911171,
        "step": 13100
    },
    {
        "loss": 2.8563,
        "grad_norm": 2.3531417846679688,
        "learning_rate": 2.58761561059464e-06,
        "epoch": 0.9763022579923989,
        "step": 13101
    },
    {
        "loss": 2.5916,
        "grad_norm": 2.3479580879211426,
        "learning_rate": 2.5717360375949073e-06,
        "epoch": 0.9763767791936806,
        "step": 13102
    },
    {
        "loss": 2.5058,
        "grad_norm": 3.762072801589966,
        "learning_rate": 2.5559047045188522e-06,
        "epoch": 0.9764513003949624,
        "step": 13103
    },
    {
        "loss": 2.0779,
        "grad_norm": 3.9185965061187744,
        "learning_rate": 2.54012161920506e-06,
        "epoch": 0.9765258215962441,
        "step": 13104
    },
    {
        "loss": 2.3319,
        "grad_norm": 3.2680537700653076,
        "learning_rate": 2.524386789468236e-06,
        "epoch": 0.9766003427975259,
        "step": 13105
    },
    {
        "loss": 1.975,
        "grad_norm": 6.162659645080566,
        "learning_rate": 2.5087002230992696e-06,
        "epoch": 0.9766748639988077,
        "step": 13106
    },
    {
        "loss": 2.5594,
        "grad_norm": 2.5659549236297607,
        "learning_rate": 2.4930619278650036e-06,
        "epoch": 0.9767493852000895,
        "step": 13107
    },
    {
        "loss": 2.8022,
        "grad_norm": 2.878939628601074,
        "learning_rate": 2.4774719115085445e-06,
        "epoch": 0.9768239064013712,
        "step": 13108
    },
    {
        "loss": 2.3556,
        "grad_norm": 2.2257540225982666,
        "learning_rate": 2.4619301817489616e-06,
        "epoch": 0.976898427602653,
        "step": 13109
    },
    {
        "loss": 1.6729,
        "grad_norm": 3.165180206298828,
        "learning_rate": 2.446436746281544e-06,
        "epoch": 0.9769729488039347,
        "step": 13110
    },
    {
        "loss": 1.1172,
        "grad_norm": 2.2972185611724854,
        "learning_rate": 2.4309916127775667e-06,
        "epoch": 0.9770474700052165,
        "step": 13111
    },
    {
        "loss": 1.3921,
        "grad_norm": 4.183287620544434,
        "learning_rate": 2.4155947888843676e-06,
        "epoch": 0.9771219912064982,
        "step": 13112
    },
    {
        "loss": 2.5392,
        "grad_norm": 2.2908899784088135,
        "learning_rate": 2.4002462822255046e-06,
        "epoch": 0.97719651240778,
        "step": 13113
    },
    {
        "loss": 1.934,
        "grad_norm": 3.188525676727295,
        "learning_rate": 2.3849461004004646e-06,
        "epoch": 0.9772710336090618,
        "step": 13114
    },
    {
        "loss": 2.5255,
        "grad_norm": 2.4079642295837402,
        "learning_rate": 2.369694250984944e-06,
        "epoch": 0.9773455548103436,
        "step": 13115
    },
    {
        "loss": 1.7889,
        "grad_norm": 2.86769700050354,
        "learning_rate": 2.354490741530524e-06,
        "epoch": 0.9774200760116253,
        "step": 13116
    },
    {
        "loss": 2.8965,
        "grad_norm": 3.1786506175994873,
        "learning_rate": 2.3393355795650385e-06,
        "epoch": 0.9774945972129071,
        "step": 13117
    },
    {
        "loss": 2.1705,
        "grad_norm": 2.2928528785705566,
        "learning_rate": 2.324228772592285e-06,
        "epoch": 0.9775691184141888,
        "step": 13118
    },
    {
        "loss": 1.7939,
        "grad_norm": 3.4946043491363525,
        "learning_rate": 2.3091703280921138e-06,
        "epoch": 0.9776436396154706,
        "step": 13119
    },
    {
        "loss": 2.3052,
        "grad_norm": 4.192038059234619,
        "learning_rate": 2.2941602535205163e-06,
        "epoch": 0.9777181608167523,
        "step": 13120
    },
    {
        "loss": 2.2885,
        "grad_norm": 1.7673696279525757,
        "learning_rate": 2.2791985563094143e-06,
        "epoch": 0.9777926820180342,
        "step": 13121
    },
    {
        "loss": 2.2927,
        "grad_norm": 2.4925754070281982,
        "learning_rate": 2.264285243866815e-06,
        "epoch": 0.9778672032193159,
        "step": 13122
    },
    {
        "loss": 2.0205,
        "grad_norm": 2.5931246280670166,
        "learning_rate": 2.2494203235768673e-06,
        "epoch": 0.9779417244205977,
        "step": 13123
    },
    {
        "loss": 2.1638,
        "grad_norm": 3.4172329902648926,
        "learning_rate": 2.2346038027996285e-06,
        "epoch": 0.9780162456218794,
        "step": 13124
    },
    {
        "loss": 2.3743,
        "grad_norm": 2.224993944168091,
        "learning_rate": 2.2198356888712303e-06,
        "epoch": 0.9780907668231612,
        "step": 13125
    },
    {
        "loss": 2.489,
        "grad_norm": 4.876706123352051,
        "learning_rate": 2.205115989103923e-06,
        "epoch": 0.9781652880244429,
        "step": 13126
    },
    {
        "loss": 2.1784,
        "grad_norm": 3.3639724254608154,
        "learning_rate": 2.1904447107858107e-06,
        "epoch": 0.9782398092257247,
        "step": 13127
    },
    {
        "loss": 2.6463,
        "grad_norm": 2.564655303955078,
        "learning_rate": 2.175821861181182e-06,
        "epoch": 0.9783143304270064,
        "step": 13128
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.892871141433716,
        "learning_rate": 2.16124744753029e-06,
        "epoch": 0.9783888516282883,
        "step": 13129
    },
    {
        "loss": 1.4011,
        "grad_norm": 2.1592886447906494,
        "learning_rate": 2.1467214770493625e-06,
        "epoch": 0.97846337282957,
        "step": 13130
    },
    {
        "loss": 2.6073,
        "grad_norm": 2.472774028778076,
        "learning_rate": 2.1322439569307464e-06,
        "epoch": 0.9785378940308518,
        "step": 13131
    },
    {
        "loss": 2.0814,
        "grad_norm": 2.7143099308013916,
        "learning_rate": 2.1178148943426734e-06,
        "epoch": 0.9786124152321335,
        "step": 13132
    },
    {
        "loss": 2.7927,
        "grad_norm": 3.807873249053955,
        "learning_rate": 2.103434296429496e-06,
        "epoch": 0.9786869364334153,
        "step": 13133
    },
    {
        "loss": 1.9186,
        "grad_norm": 2.9920997619628906,
        "learning_rate": 2.089102170311508e-06,
        "epoch": 0.978761457634697,
        "step": 13134
    },
    {
        "loss": 2.0324,
        "grad_norm": 3.449662208557129,
        "learning_rate": 2.074818523084998e-06,
        "epoch": 0.9788359788359788,
        "step": 13135
    },
    {
        "loss": 2.554,
        "grad_norm": 3.1061248779296875,
        "learning_rate": 2.0605833618222436e-06,
        "epoch": 0.9789105000372607,
        "step": 13136
    },
    {
        "loss": 1.9718,
        "grad_norm": 2.761824131011963,
        "learning_rate": 2.046396693571606e-06,
        "epoch": 0.9789850212385424,
        "step": 13137
    },
    {
        "loss": 1.9255,
        "grad_norm": 2.646491050720215,
        "learning_rate": 2.0322585253573223e-06,
        "epoch": 0.9790595424398242,
        "step": 13138
    },
    {
        "loss": 2.1461,
        "grad_norm": 4.240903854370117,
        "learning_rate": 2.0181688641796702e-06,
        "epoch": 0.9791340636411059,
        "step": 13139
    },
    {
        "loss": 2.5709,
        "grad_norm": 2.2778658866882324,
        "learning_rate": 2.004127717014892e-06,
        "epoch": 0.9792085848423877,
        "step": 13140
    },
    {
        "loss": 3.3441,
        "grad_norm": 3.244420289993286,
        "learning_rate": 1.9901350908152483e-06,
        "epoch": 0.9792831060436694,
        "step": 13141
    },
    {
        "loss": 2.3483,
        "grad_norm": 3.3590681552886963,
        "learning_rate": 1.976190992508897e-06,
        "epoch": 0.9793576272449512,
        "step": 13142
    },
    {
        "loss": 2.8964,
        "grad_norm": 3.332097291946411,
        "learning_rate": 1.9622954290000937e-06,
        "epoch": 0.9794321484462329,
        "step": 13143
    },
    {
        "loss": 2.2949,
        "grad_norm": 2.046569585800171,
        "learning_rate": 1.9484484071689346e-06,
        "epoch": 0.9795066696475148,
        "step": 13144
    },
    {
        "loss": 2.5639,
        "grad_norm": 2.75911545753479,
        "learning_rate": 1.934649933871524e-06,
        "epoch": 0.9795811908487965,
        "step": 13145
    },
    {
        "loss": 2.6496,
        "grad_norm": 2.468189239501953,
        "learning_rate": 1.920900015939997e-06,
        "epoch": 0.9796557120500783,
        "step": 13146
    },
    {
        "loss": 2.2575,
        "grad_norm": 4.087671756744385,
        "learning_rate": 1.9071986601823077e-06,
        "epoch": 0.97973023325136,
        "step": 13147
    },
    {
        "loss": 2.8324,
        "grad_norm": 2.8995022773742676,
        "learning_rate": 1.8935458733825074e-06,
        "epoch": 0.9798047544526418,
        "step": 13148
    },
    {
        "loss": 1.6803,
        "grad_norm": 3.2459921836853027,
        "learning_rate": 1.8799416623005217e-06,
        "epoch": 0.9798792756539235,
        "step": 13149
    },
    {
        "loss": 1.5372,
        "grad_norm": 3.5753116607666016,
        "learning_rate": 1.8663860336722071e-06,
        "epoch": 0.9799537968552053,
        "step": 13150
    },
    {
        "loss": 2.2894,
        "grad_norm": 3.845877170562744,
        "learning_rate": 1.8528789942094504e-06,
        "epoch": 0.980028318056487,
        "step": 13151
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.751175880432129,
        "learning_rate": 1.8394205505999906e-06,
        "epoch": 0.9801028392577689,
        "step": 13152
    },
    {
        "loss": 2.7754,
        "grad_norm": 2.448962450027466,
        "learning_rate": 1.8260107095075751e-06,
        "epoch": 0.9801773604590506,
        "step": 13153
    },
    {
        "loss": 2.7539,
        "grad_norm": 2.738252878189087,
        "learning_rate": 1.8126494775718483e-06,
        "epoch": 0.9802518816603324,
        "step": 13154
    },
    {
        "loss": 2.1718,
        "grad_norm": 3.3990840911865234,
        "learning_rate": 1.799336861408374e-06,
        "epoch": 0.9803264028616141,
        "step": 13155
    },
    {
        "loss": 2.55,
        "grad_norm": 2.087101697921753,
        "learning_rate": 1.7860728676086568e-06,
        "epoch": 0.9804009240628959,
        "step": 13156
    },
    {
        "loss": 1.4633,
        "grad_norm": 4.415199279785156,
        "learning_rate": 1.7728575027401884e-06,
        "epoch": 0.9804754452641776,
        "step": 13157
    },
    {
        "loss": 2.508,
        "grad_norm": 2.455817699432373,
        "learning_rate": 1.7596907733462897e-06,
        "epoch": 0.9805499664654594,
        "step": 13158
    },
    {
        "loss": 1.834,
        "grad_norm": 4.370000839233398,
        "learning_rate": 1.746572685946235e-06,
        "epoch": 0.9806244876667412,
        "step": 13159
    },
    {
        "loss": 2.1812,
        "grad_norm": 3.0126454830169678,
        "learning_rate": 1.7335032470352175e-06,
        "epoch": 0.980699008868023,
        "step": 13160
    },
    {
        "loss": 2.4656,
        "grad_norm": 1.836674690246582,
        "learning_rate": 1.7204824630843608e-06,
        "epoch": 0.9807735300693047,
        "step": 13161
    },
    {
        "loss": 2.8111,
        "grad_norm": 2.0065932273864746,
        "learning_rate": 1.7075103405406857e-06,
        "epoch": 0.9808480512705865,
        "step": 13162
    },
    {
        "loss": 1.9338,
        "grad_norm": 2.2301414012908936,
        "learning_rate": 1.6945868858270764e-06,
        "epoch": 0.9809225724718682,
        "step": 13163
    },
    {
        "loss": 2.746,
        "grad_norm": 3.9514341354370117,
        "learning_rate": 1.6817121053424144e-06,
        "epoch": 0.98099709367315,
        "step": 13164
    },
    {
        "loss": 2.8005,
        "grad_norm": 2.9518163204193115,
        "learning_rate": 1.6688860054613786e-06,
        "epoch": 0.9810716148744317,
        "step": 13165
    },
    {
        "loss": 1.7371,
        "grad_norm": 3.7427117824554443,
        "learning_rate": 1.6561085925346554e-06,
        "epoch": 0.9811461360757135,
        "step": 13166
    },
    {
        "loss": 1.9455,
        "grad_norm": 3.5902035236358643,
        "learning_rate": 1.6433798728886617e-06,
        "epoch": 0.9812206572769953,
        "step": 13167
    },
    {
        "loss": 2.1849,
        "grad_norm": 2.8732855319976807,
        "learning_rate": 1.6306998528258787e-06,
        "epoch": 0.9812951784782771,
        "step": 13168
    },
    {
        "loss": 1.5296,
        "grad_norm": 1.9270541667938232,
        "learning_rate": 1.6180685386245731e-06,
        "epoch": 0.9813696996795588,
        "step": 13169
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.7177283763885498,
        "learning_rate": 1.6054859365389086e-06,
        "epoch": 0.9814442208808406,
        "step": 13170
    },
    {
        "loss": 2.1596,
        "grad_norm": 3.467906951904297,
        "learning_rate": 1.5929520527989795e-06,
        "epoch": 0.9815187420821223,
        "step": 13171
    },
    {
        "loss": 2.8201,
        "grad_norm": 2.0983924865722656,
        "learning_rate": 1.5804668936107214e-06,
        "epoch": 0.9815932632834041,
        "step": 13172
    },
    {
        "loss": 2.5589,
        "grad_norm": 3.5387563705444336,
        "learning_rate": 1.5680304651558898e-06,
        "epoch": 0.981667784484686,
        "step": 13173
    },
    {
        "loss": 1.787,
        "grad_norm": 2.9263362884521484,
        "learning_rate": 1.5556427735922251e-06,
        "epoch": 0.9817423056859677,
        "step": 13174
    },
    {
        "loss": 1.628,
        "grad_norm": 4.103084564208984,
        "learning_rate": 1.543303825053266e-06,
        "epoch": 0.9818168268872495,
        "step": 13175
    },
    {
        "loss": 2.1744,
        "grad_norm": 2.759878396987915,
        "learning_rate": 1.531013625648392e-06,
        "epoch": 0.9818913480885312,
        "step": 13176
    },
    {
        "loss": 2.5044,
        "grad_norm": 2.2778403759002686,
        "learning_rate": 1.5187721814629685e-06,
        "epoch": 0.981965869289813,
        "step": 13177
    },
    {
        "loss": 2.5005,
        "grad_norm": 2.4266483783721924,
        "learning_rate": 1.506579498558025e-06,
        "epoch": 0.9820403904910947,
        "step": 13178
    },
    {
        "loss": 2.2899,
        "grad_norm": 1.930809736251831,
        "learning_rate": 1.4944355829706546e-06,
        "epoch": 0.9821149116923765,
        "step": 13179
    },
    {
        "loss": 2.5227,
        "grad_norm": 3.917987823486328,
        "learning_rate": 1.4823404407136366e-06,
        "epoch": 0.9821894328936582,
        "step": 13180
    },
    {
        "loss": 2.4692,
        "grad_norm": 2.315396547317505,
        "learning_rate": 1.4702940777757136e-06,
        "epoch": 0.98226395409494,
        "step": 13181
    },
    {
        "loss": 2.3411,
        "grad_norm": 2.612705945968628,
        "learning_rate": 1.4582965001214367e-06,
        "epoch": 0.9823384752962218,
        "step": 13182
    },
    {
        "loss": 2.5298,
        "grad_norm": 2.3273940086364746,
        "learning_rate": 1.446347713691154e-06,
        "epoch": 0.9824129964975036,
        "step": 13183
    },
    {
        "loss": 2.79,
        "grad_norm": 3.0070035457611084,
        "learning_rate": 1.4344477244011668e-06,
        "epoch": 0.9824875176987853,
        "step": 13184
    },
    {
        "loss": 1.793,
        "grad_norm": 3.846588134765625,
        "learning_rate": 1.4225965381434837e-06,
        "epoch": 0.9825620389000671,
        "step": 13185
    },
    {
        "loss": 1.8722,
        "grad_norm": 1.885072112083435,
        "learning_rate": 1.4107941607860996e-06,
        "epoch": 0.9826365601013488,
        "step": 13186
    },
    {
        "loss": 1.6943,
        "grad_norm": 3.2986502647399902,
        "learning_rate": 1.3990405981726517e-06,
        "epoch": 0.9827110813026306,
        "step": 13187
    },
    {
        "loss": 2.5279,
        "grad_norm": 1.8542838096618652,
        "learning_rate": 1.3873358561227955e-06,
        "epoch": 0.9827856025039123,
        "step": 13188
    },
    {
        "loss": 2.345,
        "grad_norm": 2.392162322998047,
        "learning_rate": 1.3756799404319064e-06,
        "epoch": 0.9828601237051942,
        "step": 13189
    },
    {
        "loss": 2.3045,
        "grad_norm": 3.3125710487365723,
        "learning_rate": 1.364072856871179e-06,
        "epoch": 0.9829346449064759,
        "step": 13190
    },
    {
        "loss": 2.7196,
        "grad_norm": 3.0167579650878906,
        "learning_rate": 1.3525146111877162e-06,
        "epoch": 0.9830091661077577,
        "step": 13191
    },
    {
        "loss": 3.0925,
        "grad_norm": 2.148195743560791,
        "learning_rate": 1.3410052091043512e-06,
        "epoch": 0.9830836873090394,
        "step": 13192
    },
    {
        "loss": 2.1325,
        "grad_norm": 2.632180690765381,
        "learning_rate": 1.3295446563197479e-06,
        "epoch": 0.9831582085103212,
        "step": 13193
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.8317906856536865,
        "learning_rate": 1.3181329585084557e-06,
        "epoch": 0.9832327297116029,
        "step": 13194
    },
    {
        "loss": 1.8972,
        "grad_norm": 3.1186251640319824,
        "learning_rate": 1.306770121320744e-06,
        "epoch": 0.9833072509128847,
        "step": 13195
    },
    {
        "loss": 1.922,
        "grad_norm": 4.030521392822266,
        "learning_rate": 1.2954561503827122e-06,
        "epoch": 0.9833817721141664,
        "step": 13196
    },
    {
        "loss": 1.3301,
        "grad_norm": 1.6111878156661987,
        "learning_rate": 1.284191051296335e-06,
        "epoch": 0.9834562933154483,
        "step": 13197
    },
    {
        "loss": 1.9487,
        "grad_norm": 4.466283321380615,
        "learning_rate": 1.2729748296392507e-06,
        "epoch": 0.98353081451673,
        "step": 13198
    },
    {
        "loss": 2.544,
        "grad_norm": 2.420506477355957,
        "learning_rate": 1.261807490965039e-06,
        "epoch": 0.9836053357180118,
        "step": 13199
    },
    {
        "loss": 2.551,
        "grad_norm": 2.8929738998413086,
        "learning_rate": 1.250689040802988e-06,
        "epoch": 0.9836798569192935,
        "step": 13200
    },
    {
        "loss": 2.4621,
        "grad_norm": 1.9854369163513184,
        "learning_rate": 1.2396194846582055e-06,
        "epoch": 0.9837543781205753,
        "step": 13201
    },
    {
        "loss": 2.5684,
        "grad_norm": 2.5360021591186523,
        "learning_rate": 1.2285988280116289e-06,
        "epoch": 0.983828899321857,
        "step": 13202
    },
    {
        "loss": 2.162,
        "grad_norm": 2.936797618865967,
        "learning_rate": 1.2176270763198828e-06,
        "epoch": 0.9839034205231388,
        "step": 13203
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.385678291320801,
        "learning_rate": 1.2067042350154988e-06,
        "epoch": 0.9839779417244205,
        "step": 13204
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.5715630054473877,
        "learning_rate": 1.195830309506707e-06,
        "epoch": 0.9840524629257024,
        "step": 13205
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.371706485748291,
        "learning_rate": 1.1850053051775444e-06,
        "epoch": 0.9841269841269841,
        "step": 13206
    },
    {
        "loss": 2.1446,
        "grad_norm": 3.1629509925842285,
        "learning_rate": 1.1742292273878131e-06,
        "epoch": 0.9842015053282659,
        "step": 13207
    },
    {
        "loss": 2.1059,
        "grad_norm": 3.08788800239563,
        "learning_rate": 1.1635020814731335e-06,
        "epoch": 0.9842760265295477,
        "step": 13208
    },
    {
        "loss": 1.7156,
        "grad_norm": 1.841099739074707,
        "learning_rate": 1.152823872744846e-06,
        "epoch": 0.9843505477308294,
        "step": 13209
    },
    {
        "loss": 2.4625,
        "grad_norm": 2.4709086418151855,
        "learning_rate": 1.142194606490099e-06,
        "epoch": 0.9844250689321112,
        "step": 13210
    },
    {
        "loss": 2.3076,
        "grad_norm": 2.187263250350952,
        "learning_rate": 1.1316142879717384e-06,
        "epoch": 0.9844995901333929,
        "step": 13211
    },
    {
        "loss": 1.9251,
        "grad_norm": 3.336448907852173,
        "learning_rate": 1.1210829224284958e-06,
        "epoch": 0.9845741113346748,
        "step": 13212
    },
    {
        "loss": 2.4962,
        "grad_norm": 2.4586195945739746,
        "learning_rate": 1.1106005150747334e-06,
        "epoch": 0.9846486325359565,
        "step": 13213
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.786435127258301,
        "learning_rate": 1.1001670711006884e-06,
        "epoch": 0.9847231537372383,
        "step": 13214
    },
    {
        "loss": 2.5675,
        "grad_norm": 2.157282590866089,
        "learning_rate": 1.0897825956722841e-06,
        "epoch": 0.98479767493852,
        "step": 13215
    },
    {
        "loss": 2.7074,
        "grad_norm": 2.613403081893921,
        "learning_rate": 1.079447093931174e-06,
        "epoch": 0.9848721961398018,
        "step": 13216
    },
    {
        "loss": 1.6033,
        "grad_norm": 2.2368814945220947,
        "learning_rate": 1.0691605709948982e-06,
        "epoch": 0.9849467173410835,
        "step": 13217
    },
    {
        "loss": 2.5244,
        "grad_norm": 2.345306396484375,
        "learning_rate": 1.058923031956527e-06,
        "epoch": 0.9850212385423653,
        "step": 13218
    },
    {
        "loss": 2.5051,
        "grad_norm": 2.4399750232696533,
        "learning_rate": 1.0487344818850942e-06,
        "epoch": 0.985095759743647,
        "step": 13219
    },
    {
        "loss": 2.2644,
        "grad_norm": 3.278653860092163,
        "learning_rate": 1.0385949258252426e-06,
        "epoch": 0.9851702809449289,
        "step": 13220
    },
    {
        "loss": 2.4546,
        "grad_norm": 1.9012688398361206,
        "learning_rate": 1.0285043687974006e-06,
        "epoch": 0.9852448021462106,
        "step": 13221
    },
    {
        "loss": 2.7678,
        "grad_norm": 3.3671534061431885,
        "learning_rate": 1.0184628157977494e-06,
        "epoch": 0.9853193233474924,
        "step": 13222
    },
    {
        "loss": 1.9886,
        "grad_norm": 2.896557092666626,
        "learning_rate": 1.0084702717982009e-06,
        "epoch": 0.9853938445487741,
        "step": 13223
    },
    {
        "loss": 2.8109,
        "grad_norm": 2.2590479850769043,
        "learning_rate": 9.985267417463417e-07,
        "epoch": 0.9854683657500559,
        "step": 13224
    },
    {
        "loss": 1.9244,
        "grad_norm": 1.3650425672531128,
        "learning_rate": 9.886322305656003e-07,
        "epoch": 0.9855428869513376,
        "step": 13225
    },
    {
        "loss": 2.0329,
        "grad_norm": 4.016716957092285,
        "learning_rate": 9.787867431550245e-07,
        "epoch": 0.9856174081526194,
        "step": 13226
    },
    {
        "loss": 2.4339,
        "grad_norm": 2.4988932609558105,
        "learning_rate": 9.689902843894595e-07,
        "epoch": 0.9856919293539012,
        "step": 13227
    },
    {
        "loss": 1.682,
        "grad_norm": 2.066953659057617,
        "learning_rate": 9.592428591194691e-07,
        "epoch": 0.985766450555183,
        "step": 13228
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.9303433895111084,
        "learning_rate": 9.49544472171271e-07,
        "epoch": 0.9858409717564647,
        "step": 13229
    },
    {
        "loss": 2.3569,
        "grad_norm": 2.2979702949523926,
        "learning_rate": 9.398951283469126e-07,
        "epoch": 0.9859154929577465,
        "step": 13230
    },
    {
        "loss": 2.4431,
        "grad_norm": 3.6577131748199463,
        "learning_rate": 9.302948324240501e-07,
        "epoch": 0.9859900141590282,
        "step": 13231
    },
    {
        "loss": 1.673,
        "grad_norm": 3.1632916927337646,
        "learning_rate": 9.207435891561477e-07,
        "epoch": 0.98606453536031,
        "step": 13232
    },
    {
        "loss": 2.1457,
        "grad_norm": 1.8670504093170166,
        "learning_rate": 9.112414032723227e-07,
        "epoch": 0.9861390565615917,
        "step": 13233
    },
    {
        "loss": 1.8969,
        "grad_norm": 4.193734645843506,
        "learning_rate": 9.017882794773891e-07,
        "epoch": 0.9862135777628736,
        "step": 13234
    },
    {
        "loss": 1.7981,
        "grad_norm": 3.051882266998291,
        "learning_rate": 8.923842224519474e-07,
        "epoch": 0.9862880989641553,
        "step": 13235
    },
    {
        "loss": 2.1456,
        "grad_norm": 2.816474676132202,
        "learning_rate": 8.830292368522176e-07,
        "epoch": 0.9863626201654371,
        "step": 13236
    },
    {
        "loss": 2.6839,
        "grad_norm": 2.371661424636841,
        "learning_rate": 8.737233273101942e-07,
        "epoch": 0.9864371413667188,
        "step": 13237
    },
    {
        "loss": 1.8919,
        "grad_norm": 3.5829532146453857,
        "learning_rate": 8.644664984334916e-07,
        "epoch": 0.9865116625680006,
        "step": 13238
    },
    {
        "loss": 2.3616,
        "grad_norm": 2.3693318367004395,
        "learning_rate": 8.552587548055102e-07,
        "epoch": 0.9865861837692823,
        "step": 13239
    },
    {
        "loss": 2.7728,
        "grad_norm": 2.1563968658447266,
        "learning_rate": 8.46100100985292e-07,
        "epoch": 0.9866607049705641,
        "step": 13240
    },
    {
        "loss": 2.1529,
        "grad_norm": 3.2044858932495117,
        "learning_rate": 8.369905415075541e-07,
        "epoch": 0.9867352261718458,
        "step": 13241
    },
    {
        "loss": 2.2805,
        "grad_norm": 4.225157260894775,
        "learning_rate": 8.279300808827883e-07,
        "epoch": 0.9868097473731277,
        "step": 13242
    },
    {
        "loss": 2.4012,
        "grad_norm": 2.996563196182251,
        "learning_rate": 8.189187235971063e-07,
        "epoch": 0.9868842685744095,
        "step": 13243
    },
    {
        "loss": 2.8152,
        "grad_norm": 2.4694154262542725,
        "learning_rate": 8.099564741123056e-07,
        "epoch": 0.9869587897756912,
        "step": 13244
    },
    {
        "loss": 2.8175,
        "grad_norm": 2.856379747390747,
        "learning_rate": 8.010433368659254e-07,
        "epoch": 0.987033310976973,
        "step": 13245
    },
    {
        "loss": 2.3436,
        "grad_norm": 3.147374391555786,
        "learning_rate": 7.921793162711244e-07,
        "epoch": 0.9871078321782547,
        "step": 13246
    },
    {
        "loss": 2.3239,
        "grad_norm": 2.5420639514923096,
        "learning_rate": 7.833644167167697e-07,
        "epoch": 0.9871823533795365,
        "step": 13247
    },
    {
        "loss": 2.4804,
        "grad_norm": 2.1366512775421143,
        "learning_rate": 7.745986425674589e-07,
        "epoch": 0.9872568745808182,
        "step": 13248
    },
    {
        "loss": 0.5486,
        "grad_norm": 1.6158266067504883,
        "learning_rate": 7.658819981633203e-07,
        "epoch": 0.9873313957821,
        "step": 13249
    },
    {
        "loss": 1.7173,
        "grad_norm": 1.6828209161758423,
        "learning_rate": 7.572144878203235e-07,
        "epoch": 0.9874059169833818,
        "step": 13250
    },
    {
        "loss": 2.0958,
        "grad_norm": 3.62054443359375,
        "learning_rate": 7.485961158299915e-07,
        "epoch": 0.9874804381846636,
        "step": 13251
    },
    {
        "loss": 1.4968,
        "grad_norm": 3.748398780822754,
        "learning_rate": 7.400268864596105e-07,
        "epoch": 0.9875549593859453,
        "step": 13252
    },
    {
        "loss": 2.2286,
        "grad_norm": 3.2494802474975586,
        "learning_rate": 7.315068039520534e-07,
        "epoch": 0.9876294805872271,
        "step": 13253
    },
    {
        "loss": 2.5043,
        "grad_norm": 2.5229427814483643,
        "learning_rate": 7.23035872525879e-07,
        "epoch": 0.9877040017885088,
        "step": 13254
    },
    {
        "loss": 3.1287,
        "grad_norm": 3.4552700519561768,
        "learning_rate": 7.146140963753656e-07,
        "epoch": 0.9877785229897906,
        "step": 13255
    },
    {
        "loss": 2.4074,
        "grad_norm": 2.2256197929382324,
        "learning_rate": 7.062414796703887e-07,
        "epoch": 0.9878530441910723,
        "step": 13256
    },
    {
        "loss": 2.2868,
        "grad_norm": 2.8628435134887695,
        "learning_rate": 6.979180265564988e-07,
        "epoch": 0.9879275653923542,
        "step": 13257
    },
    {
        "loss": 2.0621,
        "grad_norm": 3.815051317214966,
        "learning_rate": 6.896437411549106e-07,
        "epoch": 0.9880020865936359,
        "step": 13258
    },
    {
        "loss": 2.6096,
        "grad_norm": 2.8226590156555176,
        "learning_rate": 6.814186275625023e-07,
        "epoch": 0.9880766077949177,
        "step": 13259
    },
    {
        "loss": 2.1071,
        "grad_norm": 2.903599262237549,
        "learning_rate": 6.73242689851794e-07,
        "epoch": 0.9881511289961994,
        "step": 13260
    },
    {
        "loss": 3.018,
        "grad_norm": 4.426918029785156,
        "learning_rate": 6.651159320709588e-07,
        "epoch": 0.9882256501974812,
        "step": 13261
    },
    {
        "loss": 2.2223,
        "grad_norm": 2.7898669242858887,
        "learning_rate": 6.570383582438e-07,
        "epoch": 0.9883001713987629,
        "step": 13262
    },
    {
        "loss": 1.4469,
        "grad_norm": 3.681785821914673,
        "learning_rate": 6.490099723698406e-07,
        "epoch": 0.9883746926000447,
        "step": 13263
    },
    {
        "loss": 2.2761,
        "grad_norm": 3.9418463706970215,
        "learning_rate": 6.410307784241454e-07,
        "epoch": 0.9884492138013264,
        "step": 13264
    },
    {
        "loss": 2.6081,
        "grad_norm": 1.8044260740280151,
        "learning_rate": 6.331007803575095e-07,
        "epoch": 0.9885237350026083,
        "step": 13265
    },
    {
        "loss": 2.4517,
        "grad_norm": 3.0412042140960693,
        "learning_rate": 6.252199820963144e-07,
        "epoch": 0.98859825620389,
        "step": 13266
    },
    {
        "loss": 2.295,
        "grad_norm": 1.911466121673584,
        "learning_rate": 6.173883875425945e-07,
        "epoch": 0.9886727774051718,
        "step": 13267
    },
    {
        "loss": 2.3682,
        "grad_norm": 2.597726345062256,
        "learning_rate": 6.096060005740811e-07,
        "epoch": 0.9887472986064535,
        "step": 13268
    },
    {
        "loss": 1.6303,
        "grad_norm": 4.505691051483154,
        "learning_rate": 6.018728250440032e-07,
        "epoch": 0.9888218198077353,
        "step": 13269
    },
    {
        "loss": 2.3613,
        "grad_norm": 3.2553789615631104,
        "learning_rate": 5.941888647813864e-07,
        "epoch": 0.988896341009017,
        "step": 13270
    },
    {
        "loss": 2.4175,
        "grad_norm": 3.5247862339019775,
        "learning_rate": 5.865541235907767e-07,
        "epoch": 0.9889708622102988,
        "step": 13271
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.912651777267456,
        "learning_rate": 5.789686052523724e-07,
        "epoch": 0.9890453834115805,
        "step": 13272
    },
    {
        "loss": 2.8645,
        "grad_norm": 2.1263697147369385,
        "learning_rate": 5.714323135220357e-07,
        "epoch": 0.9891199046128624,
        "step": 13273
    },
    {
        "loss": 2.2734,
        "grad_norm": 2.918114423751831,
        "learning_rate": 5.63945252131215e-07,
        "epoch": 0.9891944258141441,
        "step": 13274
    },
    {
        "loss": 2.6504,
        "grad_norm": 2.5246498584747314,
        "learning_rate": 5.565074247870228e-07,
        "epoch": 0.9892689470154259,
        "step": 13275
    },
    {
        "loss": 2.2816,
        "grad_norm": 3.28572678565979,
        "learning_rate": 5.491188351721576e-07,
        "epoch": 0.9893434682167076,
        "step": 13276
    },
    {
        "loss": 2.5711,
        "grad_norm": 2.842109203338623,
        "learning_rate": 5.417794869449377e-07,
        "epoch": 0.9894179894179894,
        "step": 13277
    },
    {
        "loss": 1.9665,
        "grad_norm": 4.32355260848999,
        "learning_rate": 5.344893837393228e-07,
        "epoch": 0.9894925106192712,
        "step": 13278
    },
    {
        "loss": 2.2767,
        "grad_norm": 3.2535862922668457,
        "learning_rate": 5.272485291649143e-07,
        "epoch": 0.989567031820553,
        "step": 13279
    },
    {
        "loss": 1.8817,
        "grad_norm": 3.362484931945801,
        "learning_rate": 5.200569268068444e-07,
        "epoch": 0.9896415530218348,
        "step": 13280
    },
    {
        "loss": 2.3408,
        "grad_norm": 3.6813974380493164,
        "learning_rate": 5.129145802259538e-07,
        "epoch": 0.9897160742231165,
        "step": 13281
    },
    {
        "loss": 2.5617,
        "grad_norm": 3.60852313041687,
        "learning_rate": 5.058214929586025e-07,
        "epoch": 0.9897905954243983,
        "step": 13282
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.1148157119750977,
        "learning_rate": 4.987776685168699e-07,
        "epoch": 0.98986511662568,
        "step": 13283
    },
    {
        "loss": 1.9035,
        "grad_norm": 4.397294998168945,
        "learning_rate": 4.91783110388333e-07,
        "epoch": 0.9899396378269618,
        "step": 13284
    },
    {
        "loss": 2.9019,
        "grad_norm": 2.716829299926758,
        "learning_rate": 4.848378220362659e-07,
        "epoch": 0.9900141590282435,
        "step": 13285
    },
    {
        "loss": 2.401,
        "grad_norm": 2.481529951095581,
        "learning_rate": 4.779418068994957e-07,
        "epoch": 0.9900886802295253,
        "step": 13286
    },
    {
        "loss": 1.8905,
        "grad_norm": 4.137764930725098,
        "learning_rate": 4.710950683924353e-07,
        "epoch": 0.990163201430807,
        "step": 13287
    },
    {
        "loss": 2.3851,
        "grad_norm": 2.211164951324463,
        "learning_rate": 4.6429760990518436e-07,
        "epoch": 0.9902377226320889,
        "step": 13288
    },
    {
        "loss": 2.7668,
        "grad_norm": 3.5662779808044434,
        "learning_rate": 4.5754943480333936e-07,
        "epoch": 0.9903122438333706,
        "step": 13289
    },
    {
        "loss": 2.4896,
        "grad_norm": 2.4776883125305176,
        "learning_rate": 4.5085054642817204e-07,
        "epoch": 0.9903867650346524,
        "step": 13290
    },
    {
        "loss": 3.5724,
        "grad_norm": 4.117140769958496,
        "learning_rate": 4.4420094809650703e-07,
        "epoch": 0.9904612862359341,
        "step": 13291
    },
    {
        "loss": 2.5965,
        "grad_norm": 2.2119832038879395,
        "learning_rate": 4.376006431007662e-07,
        "epoch": 0.9905358074372159,
        "step": 13292
    },
    {
        "loss": 2.7266,
        "grad_norm": 3.173353433609009,
        "learning_rate": 4.3104963470901314e-07,
        "epoch": 0.9906103286384976,
        "step": 13293
    },
    {
        "loss": 2.7387,
        "grad_norm": 3.1659913063049316,
        "learning_rate": 4.245479261648311e-07,
        "epoch": 0.9906848498397794,
        "step": 13294
    },
    {
        "loss": 2.7946,
        "grad_norm": 2.0833041667938232,
        "learning_rate": 4.1809552068744487e-07,
        "epoch": 0.9907593710410612,
        "step": 13295
    },
    {
        "loss": 2.452,
        "grad_norm": 1.8492716550827026,
        "learning_rate": 4.116924214716544e-07,
        "epoch": 0.990833892242343,
        "step": 13296
    },
    {
        "loss": 2.2508,
        "grad_norm": 3.1318604946136475,
        "learning_rate": 4.053386316878349e-07,
        "epoch": 0.9909084134436247,
        "step": 13297
    },
    {
        "loss": 1.5751,
        "grad_norm": 5.398719787597656,
        "learning_rate": 3.990341544819587e-07,
        "epoch": 0.9909829346449065,
        "step": 13298
    },
    {
        "loss": 2.4654,
        "grad_norm": 2.8241069316864014,
        "learning_rate": 3.927789929755954e-07,
        "epoch": 0.9910574558461882,
        "step": 13299
    },
    {
        "loss": 2.3924,
        "grad_norm": 3.1307313442230225,
        "learning_rate": 3.865731502658454e-07,
        "epoch": 0.99113197704747,
        "step": 13300
    },
    {
        "loss": 2.3388,
        "grad_norm": 3.4201419353485107,
        "learning_rate": 3.8041662942546186e-07,
        "epoch": 0.9912064982487517,
        "step": 13301
    },
    {
        "loss": 1.7503,
        "grad_norm": 2.816950798034668,
        "learning_rate": 3.743094335027064e-07,
        "epoch": 0.9912810194500336,
        "step": 13302
    },
    {
        "loss": 2.2397,
        "grad_norm": 3.2995645999908447,
        "learning_rate": 3.682515655214713e-07,
        "epoch": 0.9913555406513153,
        "step": 13303
    },
    {
        "loss": 2.027,
        "grad_norm": 2.4268054962158203,
        "learning_rate": 3.6224302848121283e-07,
        "epoch": 0.9914300618525971,
        "step": 13304
    },
    {
        "loss": 2.408,
        "grad_norm": 1.786665439605713,
        "learning_rate": 3.5628382535692895e-07,
        "epoch": 0.9915045830538788,
        "step": 13305
    },
    {
        "loss": 2.5375,
        "grad_norm": 2.4939751625061035,
        "learning_rate": 3.503739590992372e-07,
        "epoch": 0.9915791042551606,
        "step": 13306
    },
    {
        "loss": 1.7933,
        "grad_norm": 2.0873749256134033,
        "learning_rate": 3.4451343263429693e-07,
        "epoch": 0.9916536254564423,
        "step": 13307
    },
    {
        "loss": 3.3247,
        "grad_norm": 2.3242099285125732,
        "learning_rate": 3.387022488638647e-07,
        "epoch": 0.9917281466577241,
        "step": 13308
    },
    {
        "loss": 2.4523,
        "grad_norm": 2.379793167114258,
        "learning_rate": 3.3294041066520565e-07,
        "epoch": 0.9918026678590058,
        "step": 13309
    },
    {
        "loss": 2.1039,
        "grad_norm": 2.992555856704712,
        "learning_rate": 3.2722792089123766e-07,
        "epoch": 0.9918771890602877,
        "step": 13310
    },
    {
        "loss": 1.4876,
        "grad_norm": 3.662104606628418,
        "learning_rate": 3.2156478237038714e-07,
        "epoch": 0.9919517102615694,
        "step": 13311
    },
    {
        "loss": 2.7228,
        "grad_norm": 3.857367992401123,
        "learning_rate": 3.159509979066444e-07,
        "epoch": 0.9920262314628512,
        "step": 13312
    },
    {
        "loss": 2.3288,
        "grad_norm": 2.645162343978882,
        "learning_rate": 3.1038657027959717e-07,
        "epoch": 0.992100752664133,
        "step": 13313
    },
    {
        "loss": 2.6336,
        "grad_norm": 1.7779170274734497,
        "learning_rate": 3.048715022443749e-07,
        "epoch": 0.9921752738654147,
        "step": 13314
    },
    {
        "loss": 2.7066,
        "grad_norm": 3.0111098289489746,
        "learning_rate": 2.994057965316488e-07,
        "epoch": 0.9922497950666965,
        "step": 13315
    },
    {
        "loss": 2.6225,
        "grad_norm": 3.0217409133911133,
        "learning_rate": 2.9398945584769854e-07,
        "epoch": 0.9923243162679782,
        "step": 13316
    },
    {
        "loss": 2.3343,
        "grad_norm": 2.873943567276001,
        "learning_rate": 2.886224828743123e-07,
        "epoch": 0.99239883746926,
        "step": 13317
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.9174582958221436,
        "learning_rate": 2.8330488026885315e-07,
        "epoch": 0.9924733586705418,
        "step": 13318
    },
    {
        "loss": 2.7754,
        "grad_norm": 2.070735216140747,
        "learning_rate": 2.7803665066425953e-07,
        "epoch": 0.9925478798718236,
        "step": 13319
    },
    {
        "loss": 2.9418,
        "grad_norm": 2.30666184425354,
        "learning_rate": 2.728177966689671e-07,
        "epoch": 0.9926224010731053,
        "step": 13320
    },
    {
        "loss": 2.0536,
        "grad_norm": 4.095561981201172,
        "learning_rate": 2.676483208670422e-07,
        "epoch": 0.9926969222743871,
        "step": 13321
    },
    {
        "loss": 2.7781,
        "grad_norm": 2.3638832569122314,
        "learning_rate": 2.625282258180373e-07,
        "epoch": 0.9927714434756688,
        "step": 13322
    },
    {
        "loss": 2.5247,
        "grad_norm": 3.3775556087493896,
        "learning_rate": 2.5745751405706896e-07,
        "epoch": 0.9928459646769506,
        "step": 13323
    },
    {
        "loss": 2.552,
        "grad_norm": 2.719865322113037,
        "learning_rate": 2.5243618809485116e-07,
        "epoch": 0.9929204858782323,
        "step": 13324
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.928356885910034,
        "learning_rate": 2.474642504175728e-07,
        "epoch": 0.9929950070795142,
        "step": 13325
    },
    {
        "loss": 2.2101,
        "grad_norm": 2.8233351707458496,
        "learning_rate": 2.425417034870203e-07,
        "epoch": 0.9930695282807959,
        "step": 13326
    },
    {
        "loss": 1.8608,
        "grad_norm": 3.4481093883514404,
        "learning_rate": 2.3766854974049956e-07,
        "epoch": 0.9931440494820777,
        "step": 13327
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.9407862424850464,
        "learning_rate": 2.3284479159086935e-07,
        "epoch": 0.9932185706833594,
        "step": 13328
    },
    {
        "loss": 1.4442,
        "grad_norm": 3.0350966453552246,
        "learning_rate": 2.2807043142653027e-07,
        "epoch": 0.9932930918846412,
        "step": 13329
    },
    {
        "loss": 3.0118,
        "grad_norm": 1.887668251991272,
        "learning_rate": 2.2334547161143582e-07,
        "epoch": 0.9933676130859229,
        "step": 13330
    },
    {
        "loss": 2.4639,
        "grad_norm": 3.0788450241088867,
        "learning_rate": 2.1866991448505902e-07,
        "epoch": 0.9934421342872047,
        "step": 13331
    },
    {
        "loss": 1.7289,
        "grad_norm": 2.9206879138946533,
        "learning_rate": 2.1404376236241474e-07,
        "epoch": 0.9935166554884864,
        "step": 13332
    },
    {
        "loss": 2.1297,
        "grad_norm": 3.1575424671173096,
        "learning_rate": 2.094670175340596e-07,
        "epoch": 0.9935911766897683,
        "step": 13333
    },
    {
        "loss": 1.8167,
        "grad_norm": 3.479218006134033,
        "learning_rate": 2.0493968226611427e-07,
        "epoch": 0.99366569789105,
        "step": 13334
    },
    {
        "loss": 2.5502,
        "grad_norm": 1.9092698097229004,
        "learning_rate": 2.0046175880018557e-07,
        "epoch": 0.9937402190923318,
        "step": 13335
    },
    {
        "loss": 3.0602,
        "grad_norm": 2.504243850708008,
        "learning_rate": 1.9603324935345557e-07,
        "epoch": 0.9938147402936135,
        "step": 13336
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.9975205659866333,
        "learning_rate": 1.916541561186147e-07,
        "epoch": 0.9938892614948953,
        "step": 13337
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.567108631134033,
        "learning_rate": 1.8732448126389525e-07,
        "epoch": 0.993963782696177,
        "step": 13338
    },
    {
        "loss": 2.0513,
        "grad_norm": 3.916247606277466,
        "learning_rate": 1.8304422693308232e-07,
        "epoch": 0.9940383038974588,
        "step": 13339
    },
    {
        "loss": 2.12,
        "grad_norm": 3.9247443675994873,
        "learning_rate": 1.7881339524543627e-07,
        "epoch": 0.9941128250987405,
        "step": 13340
    },
    {
        "loss": 2.2755,
        "grad_norm": 2.182213068008423,
        "learning_rate": 1.7463198829580363e-07,
        "epoch": 0.9941873463000224,
        "step": 13341
    },
    {
        "loss": 2.7591,
        "grad_norm": 2.5317013263702393,
        "learning_rate": 1.7050000815452827e-07,
        "epoch": 0.9942618675013041,
        "step": 13342
    },
    {
        "loss": 2.2734,
        "grad_norm": 3.407058000564575,
        "learning_rate": 1.6641745686748478e-07,
        "epoch": 0.9943363887025859,
        "step": 13343
    },
    {
        "loss": 2.4504,
        "grad_norm": 3.1523869037628174,
        "learning_rate": 1.6238433645608953e-07,
        "epoch": 0.9944109099038676,
        "step": 13344
    },
    {
        "loss": 2.2632,
        "grad_norm": 3.451338768005371,
        "learning_rate": 1.5840064891725625e-07,
        "epoch": 0.9944854311051494,
        "step": 13345
    },
    {
        "loss": 2.3021,
        "grad_norm": 2.3121376037597656,
        "learning_rate": 1.544663962234627e-07,
        "epoch": 0.9945599523064311,
        "step": 13346
    },
    {
        "loss": 1.8111,
        "grad_norm": 2.503343105316162,
        "learning_rate": 1.505815803226618e-07,
        "epoch": 0.994634473507713,
        "step": 13347
    },
    {
        "loss": 2.1616,
        "grad_norm": 3.0039236545562744,
        "learning_rate": 1.467462031383704e-07,
        "epoch": 0.9947089947089947,
        "step": 13348
    },
    {
        "loss": 2.3132,
        "grad_norm": 2.771836519241333,
        "learning_rate": 1.4296026656959172e-07,
        "epoch": 0.9947835159102765,
        "step": 13349
    },
    {
        "loss": 1.6391,
        "grad_norm": 3.5509824752807617,
        "learning_rate": 1.392237724908929e-07,
        "epoch": 0.9948580371115583,
        "step": 13350
    },
    {
        "loss": 2.0925,
        "grad_norm": 7.7498369216918945,
        "learning_rate": 1.3553672275230523e-07,
        "epoch": 0.99493255831284,
        "step": 13351
    },
    {
        "loss": 1.9587,
        "grad_norm": 5.637739181518555,
        "learning_rate": 1.3189911917943497e-07,
        "epoch": 0.9950070795141218,
        "step": 13352
    },
    {
        "loss": 2.1498,
        "grad_norm": 2.86024808883667,
        "learning_rate": 1.283109635733526e-07,
        "epoch": 0.9950816007154035,
        "step": 13353
    },
    {
        "loss": 2.0382,
        "grad_norm": 3.105236530303955,
        "learning_rate": 1.2477225771069247e-07,
        "epoch": 0.9951561219166853,
        "step": 13354
    },
    {
        "loss": 2.3743,
        "grad_norm": 3.488396406173706,
        "learning_rate": 1.2128300334357522e-07,
        "epoch": 0.995230643117967,
        "step": 13355
    },
    {
        "loss": 2.387,
        "grad_norm": 3.110766649246216,
        "learning_rate": 1.1784320219963007e-07,
        "epoch": 0.9953051643192489,
        "step": 13356
    },
    {
        "loss": 1.8678,
        "grad_norm": 1.6690170764923096,
        "learning_rate": 1.1445285598205013e-07,
        "epoch": 0.9953796855205306,
        "step": 13357
    },
    {
        "loss": 2.5009,
        "grad_norm": 4.919174671173096,
        "learning_rate": 1.1111196636947042e-07,
        "epoch": 0.9954542067218124,
        "step": 13358
    },
    {
        "loss": 2.0652,
        "grad_norm": 2.958054780960083,
        "learning_rate": 1.078205350161121e-07,
        "epoch": 0.9955287279230941,
        "step": 13359
    },
    {
        "loss": 1.7324,
        "grad_norm": 3.020052671432495,
        "learning_rate": 1.045785635516272e-07,
        "epoch": 0.9956032491243759,
        "step": 13360
    },
    {
        "loss": 2.4508,
        "grad_norm": 1.6507230997085571,
        "learning_rate": 1.0138605358126496e-07,
        "epoch": 0.9956777703256576,
        "step": 13361
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.3468353748321533,
        "learning_rate": 9.824300668571651e-08,
        "epoch": 0.9957522915269394,
        "step": 13362
    },
    {
        "loss": 2.4733,
        "grad_norm": 1.916094183921814,
        "learning_rate": 9.514942442120367e-08,
        "epoch": 0.9958268127282212,
        "step": 13363
    },
    {
        "loss": 2.5201,
        "grad_norm": 2.171503782272339,
        "learning_rate": 9.210530831946784e-08,
        "epoch": 0.995901333929503,
        "step": 13364
    },
    {
        "loss": 2.7226,
        "grad_norm": 2.316847085952759,
        "learning_rate": 8.91106598877589e-08,
        "epoch": 0.9959758551307847,
        "step": 13365
    },
    {
        "loss": 2.5051,
        "grad_norm": 3.1959869861602783,
        "learning_rate": 8.616548060881302e-08,
        "epoch": 0.9960503763320665,
        "step": 13366
    },
    {
        "loss": 2.7938,
        "grad_norm": 2.963472843170166,
        "learning_rate": 8.326977194089703e-08,
        "epoch": 0.9961248975333482,
        "step": 13367
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.9774675369262695,
        "learning_rate": 8.042353531776403e-08,
        "epoch": 0.99619941873463,
        "step": 13368
    },
    {
        "loss": 2.1704,
        "grad_norm": 2.958254337310791,
        "learning_rate": 7.762677214866454e-08,
        "epoch": 0.9962739399359117,
        "step": 13369
    },
    {
        "loss": 2.5039,
        "grad_norm": 3.4247660636901855,
        "learning_rate": 7.487948381840193e-08,
        "epoch": 0.9963484611371936,
        "step": 13370
    },
    {
        "loss": 2.4237,
        "grad_norm": 3.077876329421997,
        "learning_rate": 7.218167168722146e-08,
        "epoch": 0.9964229823384753,
        "step": 13371
    },
    {
        "loss": 2.7419,
        "grad_norm": 2.1769630908966064,
        "learning_rate": 6.953333709089905e-08,
        "epoch": 0.9964975035397571,
        "step": 13372
    },
    {
        "loss": 2.2749,
        "grad_norm": 2.3423969745635986,
        "learning_rate": 6.693448134071912e-08,
        "epoch": 0.9965720247410388,
        "step": 13373
    },
    {
        "loss": 2.4849,
        "grad_norm": 3.4569413661956787,
        "learning_rate": 6.438510572346345e-08,
        "epoch": 0.9966465459423206,
        "step": 13374
    },
    {
        "loss": 1.768,
        "grad_norm": 2.9184532165527344,
        "learning_rate": 6.188521150141125e-08,
        "epoch": 0.9967210671436023,
        "step": 13375
    },
    {
        "loss": 1.5312,
        "grad_norm": 4.050069808959961,
        "learning_rate": 5.943479991232792e-08,
        "epoch": 0.9967955883448841,
        "step": 13376
    },
    {
        "loss": 1.1146,
        "grad_norm": 2.2969369888305664,
        "learning_rate": 5.7033872169509615e-08,
        "epoch": 0.9968701095461658,
        "step": 13377
    },
    {
        "loss": 2.2256,
        "grad_norm": 3.798840284347534,
        "learning_rate": 5.4682429461716535e-08,
        "epoch": 0.9969446307474477,
        "step": 13378
    },
    {
        "loss": 2.4126,
        "grad_norm": 3.724355459213257,
        "learning_rate": 5.238047295325066e-08,
        "epoch": 0.9970191519487294,
        "step": 13379
    },
    {
        "loss": 1.6797,
        "grad_norm": 3.777088165283203,
        "learning_rate": 5.012800378386695e-08,
        "epoch": 0.9970936731500112,
        "step": 13380
    },
    {
        "loss": 2.4451,
        "grad_norm": 1.9846571683883667,
        "learning_rate": 4.792502306883995e-08,
        "epoch": 0.9971681943512929,
        "step": 13381
    },
    {
        "loss": 1.3236,
        "grad_norm": 4.499996662139893,
        "learning_rate": 4.5771531898930465e-08,
        "epoch": 0.9972427155525747,
        "step": 13382
    },
    {
        "loss": 2.6705,
        "grad_norm": 3.251621961593628,
        "learning_rate": 4.36675313404189e-08,
        "epoch": 0.9973172367538564,
        "step": 13383
    },
    {
        "loss": 2.6637,
        "grad_norm": 1.989086627960205,
        "learning_rate": 4.161302243504972e-08,
        "epoch": 0.9973917579551382,
        "step": 13384
    },
    {
        "loss": 2.8147,
        "grad_norm": 2.564242362976074,
        "learning_rate": 3.960800620009808e-08,
        "epoch": 0.99746627915642,
        "step": 13385
    },
    {
        "loss": 2.8483,
        "grad_norm": 2.6997265815734863,
        "learning_rate": 3.7652483628281e-08,
        "epoch": 0.9975408003577018,
        "step": 13386
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.283777952194214,
        "learning_rate": 3.5746455687868386e-08,
        "epoch": 0.9976153215589836,
        "step": 13387
    },
    {
        "loss": 2.7933,
        "grad_norm": 2.989109516143799,
        "learning_rate": 3.388992332259422e-08,
        "epoch": 0.9976898427602653,
        "step": 13388
    },
    {
        "loss": 2.6733,
        "grad_norm": 3.033859968185425,
        "learning_rate": 3.208288745166765e-08,
        "epoch": 0.9977643639615471,
        "step": 13389
    },
    {
        "loss": 2.3307,
        "grad_norm": 3.5592916011810303,
        "learning_rate": 3.032534896983963e-08,
        "epoch": 0.9978388851628288,
        "step": 13390
    },
    {
        "loss": 2.2225,
        "grad_norm": 2.966862201690674,
        "learning_rate": 2.8617308747314054e-08,
        "epoch": 0.9979134063641106,
        "step": 13391
    },
    {
        "loss": 2.1372,
        "grad_norm": 3.053738832473755,
        "learning_rate": 2.6958767629792215e-08,
        "epoch": 0.9979879275653923,
        "step": 13392
    },
    {
        "loss": 1.1579,
        "grad_norm": 3.5950558185577393,
        "learning_rate": 2.5349726438472777e-08,
        "epoch": 0.9980624487666742,
        "step": 13393
    },
    {
        "loss": 3.0719,
        "grad_norm": 3.1637179851531982,
        "learning_rate": 2.379018597004068e-08,
        "epoch": 0.9981369699679559,
        "step": 13394
    },
    {
        "loss": 1.5078,
        "grad_norm": 3.7049615383148193,
        "learning_rate": 2.2280146996689344e-08,
        "epoch": 0.9982114911692377,
        "step": 13395
    },
    {
        "loss": 2.3901,
        "grad_norm": 2.3972227573394775,
        "learning_rate": 2.081961026607626e-08,
        "epoch": 0.9982860123705194,
        "step": 13396
    },
    {
        "loss": 2.2515,
        "grad_norm": 4.361629009246826,
        "learning_rate": 1.9408576501378507e-08,
        "epoch": 0.9983605335718012,
        "step": 13397
    },
    {
        "loss": 1.5792,
        "grad_norm": 3.168724298477173,
        "learning_rate": 1.804704640122612e-08,
        "epoch": 0.9984350547730829,
        "step": 13398
    },
    {
        "loss": 3.1032,
        "grad_norm": 1.9321632385253906,
        "learning_rate": 1.6735020639757626e-08,
        "epoch": 0.9985095759743647,
        "step": 13399
    },
    {
        "loss": 2.2933,
        "grad_norm": 2.0422017574310303,
        "learning_rate": 1.5472499866608926e-08,
        "epoch": 0.9985840971756464,
        "step": 13400
    },
    {
        "loss": 1.9945,
        "grad_norm": 3.107405424118042,
        "learning_rate": 1.4259484706902193e-08,
        "epoch": 0.9986586183769283,
        "step": 13401
    },
    {
        "loss": 2.2866,
        "grad_norm": 6.2602057456970215,
        "learning_rate": 1.3095975761223678e-08,
        "epoch": 0.99873313957821,
        "step": 13402
    },
    {
        "loss": 2.232,
        "grad_norm": 2.6381449699401855,
        "learning_rate": 1.1981973605668107e-08,
        "epoch": 0.9988076607794918,
        "step": 13403
    },
    {
        "loss": 1.5654,
        "grad_norm": 3.1191539764404297,
        "learning_rate": 1.0917478791816482e-08,
        "epoch": 0.9988821819807735,
        "step": 13404
    },
    {
        "loss": 2.4997,
        "grad_norm": 3.5399770736694336,
        "learning_rate": 9.902491846747185e-09,
        "epoch": 0.9989567031820553,
        "step": 13405
    },
    {
        "loss": 1.8884,
        "grad_norm": 6.593589782714844,
        "learning_rate": 8.93701327300267e-09,
        "epoch": 0.999031224383337,
        "step": 13406
    },
    {
        "loss": 2.7356,
        "grad_norm": 2.525595188140869,
        "learning_rate": 8.021043548611663e-09,
        "epoch": 0.9991057455846188,
        "step": 13407
    },
    {
        "loss": 3.0138,
        "grad_norm": 2.202495574951172,
        "learning_rate": 7.154583127122472e-09,
        "epoch": 0.9991802667859006,
        "step": 13408
    },
    {
        "loss": 1.9118,
        "grad_norm": 2.5262362957000732,
        "learning_rate": 6.337632437536378e-09,
        "epoch": 0.9992547879871824,
        "step": 13409
    },
    {
        "loss": 2.0731,
        "grad_norm": 4.182428359985352,
        "learning_rate": 5.570191884363141e-09,
        "epoch": 0.9993293091884641,
        "step": 13410
    },
    {
        "loss": 2.1684,
        "grad_norm": 2.938135862350464,
        "learning_rate": 4.852261847565487e-09,
        "epoch": 0.9994038303897459,
        "step": 13411
    },
    {
        "loss": 2.1445,
        "grad_norm": 3.2781872749328613,
        "learning_rate": 4.1838426826368295e-09,
        "epoch": 0.9994783515910276,
        "step": 13412
    },
    {
        "loss": 2.4351,
        "grad_norm": 3.3635191917419434,
        "learning_rate": 3.5649347205235494e-09,
        "epoch": 0.9995528727923094,
        "step": 13413
    },
    {
        "loss": 2.0868,
        "grad_norm": 3.6825883388519287,
        "learning_rate": 2.9955382676694068e-09,
        "epoch": 0.9996273939935911,
        "step": 13414
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.2360455989837646,
        "learning_rate": 2.475653605993333e-09,
        "epoch": 0.999701915194873,
        "step": 13415
    },
    {
        "loss": 2.4277,
        "grad_norm": 3.391294002532959,
        "learning_rate": 2.00528099291164e-09,
        "epoch": 0.9997764363961547,
        "step": 13416
    },
    {
        "loss": 2.438,
        "grad_norm": 3.833284616470337,
        "learning_rate": 1.584420661326913e-09,
        "epoch": 0.9998509575974365,
        "step": 13417
    },
    {
        "loss": 1.6889,
        "grad_norm": 4.394411563873291,
        "learning_rate": 1.2130728196169118e-09,
        "epoch": 0.9999254787987182,
        "step": 13418
    },
    {
        "loss": 3.1407,
        "grad_norm": 4.211414337158203,
        "learning_rate": 8.912376516567733e-10,
        "epoch": 1.0,
        "step": 13419
    },
    {
        "train_runtime": 10713.445,
        "train_samples_per_second": 2.505,
        "train_steps_per_second": 1.253,
        "total_flos": 1.43393022033408e+17,
        "train_loss": 2.3625568564318002,
        "epoch": 1.0,
        "step": 13419
    }
]